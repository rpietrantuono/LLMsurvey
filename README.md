# From Testing to Evaluation of NLP and LLM Systems: An Analysis of Researchers and Practitioners Perspectives through Systematic Literature Review and Developers' Community Platforms Mining

Ali Asgari (Delft University of Technology)
Antonio Guerriero (University of Naples Federico II)
Roberto Pietrantuono (University of Naples Federico II)
Stefano Russo (University of Naples Federico II)

This replication package provides a detailed overview of our research focusing on testing and evaluating NLP/LLM Systems.

• PaperDetails.xlsx: Contains the list of papers used in our systematic literature review, detailing evaluation objectives (Quality Attributes), Models, Datasets, Tasks, Metrics, Methods, and Sub-methods.

• Secondary Bibliography-Papers Surveyed.pdf: Lists all surveyed papers referenced in our study.

• MSR_V2.zip: Focuses on the AllTime folder, which includes datasets mined from StackOverflow, DataScience StackExchange, and AI StackExchange, along with the applied queries. The dataset file names indicate the keywords used for each forum (e.g., gpt-3.Stack.csv represents searches for “gpt-3” on StackOverflow).

• V2_AlltimeCombined_NLPLLM_AIfroume_DataScience_Stackoverflow.csv and V3_MSR.NLPLLM.csv: Aggregated datasets containing all records mined, with the latter version filtered for relevance.

• V4_AllRecordsWCategoryWCommentCount.csv: The final dataset used in the MSR section, containing 3,177 categorized, ready for analysis.



This collection serves as a comprehensive foundation for exploring testing and evaluation techniques in NLP/LLM systems.
