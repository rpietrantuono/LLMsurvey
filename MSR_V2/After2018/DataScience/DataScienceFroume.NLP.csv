Post Link,Title,CreationDate,AcceptedAnswerId,Score,ViewCount,Tags,Body,TagName
"129964","What software is available for procedurally generating translation tables for students?","2024-08-14 15:22:10","","0","8","<nlp>","<p>I am asking a question about natural language procession (NLP)</p>
<p>For better or worse, the data science stack exchange (SE) seems to be where questions about natural language processing go.</p>
<hr />
<p>Suppose that we wanted a table, like the following, for teaching kids and/or adults Spanish.</p>
<hr />
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>SPANISH</th>
<th>pod-emos</th>
<th>tra-bajar</th>
<th>en</th>
<th>conversación</th>
<th>,</th>
<th>vocabulario</th>
<th>,</th>
<th>gramática</th>
<th>o</th>
<th>lo que prefier-as</th>
<th>.</th>
</tr>
</thead>
<tbody>
<tr>
<td>SPANGLISH</td>
<td>power-we</td>
<td>trawl-below</td>
<td>in</td>
<td>conversation</td>
<td>,</td>
<td>vocabulary</td>
<td>,</td>
<td>grammar</td>
<td>or</td>
<td>it that prefer-you</td>
<td>.</td>
</tr>
<tr>
<td>ENGLISH</td>
<td>we can</td>
<td>work</td>
<td>in</td>
<td>conversation</td>
<td>,</td>
<td>vocabulary</td>
<td>,</td>
<td>grammar</td>
<td>or</td>
<td>whatever you prefer</td>
<td>.</td>
</tr>
</tbody>
</table></div>
<hr />
<p>You can assume that I already have a dictionary where the inputs are strings of Spanish text and the output are strings of English text.</p>
<pre class=""lang-python prettyprint-override""><code># Small Sample (or Example) of dictionary

toSpanglish = {
  &quot;trabjar&quot;      : &quot;trawl below&quot;,
  &quot;podemos&quot;      : &quot;power we&quot;,
  &quot;ano&quot;          : &quot;annual&quot;
  &quot;año&quot;          : &quot;annual&quot;
  &quot;conversación&quot; : &quot;conversation&quot;
}
</code></pre>
<hr />
<p>What tool might we use to procedurally generate stack exchange mark-down tables or HTML tables?</p>
<p>The input will be a string of Spanish text. The output will be a string for an HTML table or stack exchange mark-down language table, a tele-type ASSCII art table, or somthing like that.</p>
<p>Below, we have two different examples of what the desired output might be...</p>
<pre class=""lang-none prettyprint-override""><code>| SPANISH   | pod-emos | tra-bajar   | en | conversación | , | vocabulario | , | gramática | o  | lo que prefier-as   | . |
|-----------|----------|-------------|----|--------------|---|-------------|---|-----------|----|---------------------|---|
| SPANGLISH | power-we | trawl-below | in | conversation | , | vocabulary  | , | grammar   | or | it that prefer-you  | . |
| ENGLISH   | we can   | work        | in | conversation | , | vocabulary  | , | grammar   | or | whatever you prefer | . |
</code></pre>
<pre class=""lang-html prettyprint-override""><code>&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;style&gt;
table, th, td {
  border:1px solid black;
}
&lt;/style&gt;
&lt;body&gt;

&lt;table style=&quot;width:100%&quot;&gt;
&lt;tr&gt;
   &lt;th&gt;SPANISH       &lt;/th&gt;
   &lt;th&gt;pod-emos      &lt;/th&gt;
   &lt;th&gt;tra-bajar     &lt;/th&gt;
   &lt;th&gt;en            &lt;/th&gt;
   &lt;th&gt;conversación  &lt;/th&gt;
   &lt;th&gt;,             &lt;/th&gt;
   &lt;th&gt;vocabulario   &lt;/th&gt;
   &lt;th&gt;,             &lt;/th&gt;
   &lt;th&gt; gramática    &lt;/th&gt;
   &lt;th&gt; o            &lt;/th&gt;
   &lt;th&gt; lo que prefier-as   &lt;/th&gt;
   &lt;th&gt; .            &lt;/th&gt;
&lt;/tr&gt;

&lt;tr&gt;
    &lt;td&gt; SPANGLISH &lt;/td&gt;&lt;td&gt; power-we &lt;/td&gt;&lt;td&gt; trawl-below &lt;/td&gt;&lt;td&gt; in &lt;/td&gt;&lt;td&gt; conversation &lt;/td&gt;&lt;td&gt; , &lt;/td&gt;&lt;td&gt; vocabulary  &lt;/td&gt;&lt;td&gt; , &lt;/td&gt;&lt;td&gt; grammar   &lt;/td&gt;&lt;td&gt; or &lt;/td&gt;&lt;td&gt; it that prefer-you  &lt;/td&gt;&lt;td&gt; . &lt;/td&gt;&lt;td&gt;
&lt;/tr&gt; 
    &lt;tr&gt;
        &lt;td&gt; ENGLISH             &lt;/td&gt;
        &lt;td&gt; we can              &lt;/td&gt;
        &lt;td&gt; work                &lt;/td&gt;
        &lt;td&gt; in                  &lt;/td&gt;
        &lt;td&gt; conversation        &lt;/td&gt;
        &lt;td&gt;,                    &lt;/td&gt;
        &lt;td&gt; vocabulary          &lt;/td&gt;
        &lt;td&gt; ,                   &lt;/td&gt;
        &lt;td&gt; grammar             &lt;/td&gt;
        &lt;td&gt; or                  &lt;/td&gt;
        &lt;td&gt; whatever you prefer &lt;/td&gt;
        &lt;td&gt; .                   &lt;/td&gt;   
    &lt;/tr&gt;
&lt;/table&gt;
&lt;/body&gt;
&lt;/html&gt;
<span class=""math-container"">```</span>
</code></pre>
","nlp"
"129927","Text Classification Task with already masked text","2024-08-10 12:47:40","","0","13","<classification><nlp><ai>","<p>Hello all I am working on a project of NLP text classification (a domain specific data). The task is of binary classification around 8000 data points but only 80 labeled. Moreover some parts of the data are already masked. I have cleaned and preprocessed the data and even tokenised it using word2vec. How should I go about it, please shed some light. How should self supervised learning be used if it is the way to go.</p>
","nlp"
"129865","Intuition behind g variable calculation in the original word2vec implementation","2024-08-03 21:30:49","","2","60","<nlp><word-embeddings><word2vec>","<p>I am trying to develop the intuition of word2vec training.</p>
<p>Looking into the <a href=""https://github.com/dav/word2vec"" rel=""nofollow noreferrer"">word2vec source code</a>, I see (for example, in skip-gram):</p>
<pre><code>for (c = 0; c &lt; layer1_size; c++) f += syn0[c + l1] * syn1[c + l2];
</code></pre>
<p>which calculates the dot product between <code>last_word</code> embedding (syn0) and word trained to be predicted (syn1). Taking into account that syn0 is addressed with hot-point input and this just copies a row of a hidden layer matrix of the network as layer's results, I assume that this dot product is just a distance between words and intended to represent a probability for the next word follow the previous one. Please, correct me if I am wrong here.</p>
<p>The question comes with the variable <code>g</code> which is calculated in the following way:</p>
<pre><code>g = (1 - vocab[word].code[d] - f) * alpha; 
</code></pre>
<p>and I am lost here. The <code>vocab[word].code[d]</code> is the Huffman code (left and right, zero and one), so what’s the value of the <code>1 - vocab[word].code[d]</code>, it just inverts left to right and vice versa, which doesn’t have any meaning, since it should be not relevant what exactly to learn, this is just an agreement. Where is this “reflection” used?</p>
<p>My understanding is that we want to be <code>vocab[word].code[d]</code> (actual turn in in Huffman tree) and <code>f</code> (calculated turn in Huffman tree) is as close as possible and at the same time syn0 and syn1 weights move closer/farther (cosine dist) to fit this.</p>
<p>If this understanding is correct, how the code below could help us do so? If the difference <code>vocab[word].code[d] - f</code> is zero, the <code>g</code> value will be 1 and will lead to significant weights advancements. So, why do we have this &quot;1-...&quot; here?</p>
<p>To put in a nutshell, if <code>g</code> is a learning gradient, what is the intuition behind it? Why if <code>f</code> and <code>g</code> are calculated in this way, <code>g</code> would have the value which should be later multiplied to train the neural network?</p>
<p>I read the <a href=""https://arxiv.org/abs/1411.2738"" rel=""nofollow noreferrer"">word2vec Parameter Learning Explained</a>, but can’t find the justification there.</p>
<p>I see some helpful derivations in the article <a href=""https://medium.com/@towardsautonomy/word2vec-part5-80bcccfefe44"" rel=""nofollow noreferrer"">What is word2vec and how to build it from scratch?</a> where the same structure (1-x) comes in the gradient calculations, but in a slightly different context.</p>
<p>Can you please help me to grasp the internal logic here?</p>
<p><strong>Update</strong></p>
<p>Not sure if it gives the real answer, so I don't post it as the answer so far, but it seems that it comes from the formula [51] from the <a href=""https://arxiv.org/abs/1411.2738"" rel=""nofollow noreferrer"">word2vec Parameter Learning Explained</a> in hierarchical softmax, where (in a very simplified way) we have something like:</p>
<p><span class=""math-container"">$$x_{new}=x-kx$$</span></p>
<p>which could be rewritten as</p>
<p><span class=""math-container"">$$x_{new}=(1-k)x$$</span></p>
<p>where from this (1-k) comes.</p>
<p>Any thoughts if this is a correct insight or there is some other explanation?</p>
<p><strong>Update 2</strong></p>
<p>As a second thought, the root cause could be in the sigmoid function used in calculation of 'f' and the fact that <a href=""https://hausetutorials.netlify.app/posts/2019-12-01-neural-networks-deriving-the-sigmoid-derivative/"" rel=""nofollow noreferrer"">sigmoid derivative</a> is:</p>
<p><span class=""math-container"">$$\sigma'(x)=\frac{d}{dx}\sigma(x)=\sigma(x)(1-\sigma(x))$$</span></p>
<p>Actually, this could be the same as in the first update, just in math terms.</p>
<p>The problem is that in the absence of the documentation on word2vec it is hard to say for sure; some sources even suggest that when the Huffman code should be 0, the f should be 1 and vice versa which confirms my first guess about &quot;mirroring&quot;.</p>
","nlp"
"129837","Trying to understand the IBM Models for machine translation","2024-07-31 05:37:58","","0","4","<nlp><statistics><machine-translation>","<p>I came across some old literature on machine translation, specifically IBM Model 1 here: <a href=""https://aclanthology.org/J93-2003.pdf"" rel=""nofollow noreferrer"">https://aclanthology.org/J93-2003.pdf</a>. In it, the author makes the makes the statement : &quot;If we replace <span class=""math-container"">$\lambda_e$</span> by <span class=""math-container"">$\lambda_e$</span> <span class=""math-container"">$P(\bf{f}|\bf{e})$</span>&quot; (page number 270 - 271, equations 11, 12, 13 is the relevant context). I understand if we multiple and divide by <span class=""math-container"">$P(\bf{f} | \bf{e})$</span> but how can we just &quot;replace&quot; something like that?</p>
","nlp"
"129834","Faster preprocessing for Arabic texts","2024-07-30 21:01:07","","0","7","<python><nlp><dataset><preprocessing><stemming>","<p><strong>Background</strong></p>
<p>I'm analyzing a relatively large text-based Arabic dataset using Python (50,000 - 70,000 text files; total size ~5GB).</p>
<p>I want to segment, stem, and POS tag the dataset. I am aware of two Python libraries that can do these 3 tasks: <a href=""https://github.com/MagedSaeed/farasapy"" rel=""nofollow noreferrer"">farasapy</a> and <a href=""https://github.com/CAMeL-Lab/camel_parser"" rel=""nofollow noreferrer"">camel parser</a>.</p>
<p><strong>The problem</strong></p>
<p>However, both of these Arabic-compatible Python libraries are rather slow for my purposes.</p>
<p>Based on a preliminary analysis of a subset of the data (n = 1000 text files), it is estimated that the camel parser library would take around 29 days to process the full dataset.</p>
<p><strong>Question</strong></p>
<p>Is there a more efficient method or library to do the 3 target tasks for Arabic texts?</p>
<p>I would appreciate your suggestions.</p>
","nlp"
"129783","Keras Transformer regression model not predicting values beyond a threshold","2024-07-25 14:53:41","","0","16","<deep-learning><nlp><keras><tensorflow><transformer>","<p>I am working on a keras transformer model for regression and I am getting prediction values which are cut off to some specific threshold.</p>
<p>Code:</p>
<pre class=""lang-py prettyprint-override""><code>def transformer_block(self,inputs, embed_dim, num_heads, ff_dim, dropout_rate):
        
    x = inputs
    x = MultiHeadAttention(key_dim=embed_dim,num_heads=num_heads,dropout=dropout_rate)(x, x)
    #x = Dropout(dropout_rate)(x)
    res = x + inputs

    # Feed Forward Part
    x = LayerNormalization(epsilon=1e-6)(res)
    x = Dense(ff_dim, activation=&quot;relu&quot;)(x)
    x = Dropout(dropout_rate)(x)
    x = Dense(inputs.shape[-1])(x)
    return x + res
# Build the model
def create_model(self, embed_dim = 64, num_heads = 6,
        ff_dim = 128, num_transformer_blocks = 3,
        dropout_rate = 0.1):
        
    vocab_size,input_length = self.vocab_size,self.input_length,
    inputs = tf.keras.Input(shape=(input_length,), dtype=tf.int32)
    embedding_layer = keras.layers.Embedding(input_dim=vocab_size,
    output_dim=embed_dim)(inputs)

    x = embedding_layer
    
    for _ in range(num_transformer_blocks):
        x = self.transformer_block(x, embed_dim, num_heads, ff_dim, dropout_rate)
        x = LayerNormalization(epsilon=1e-6)(x) 
        x = tf.keras.layers.GlobalAveragePooling1D()(x)
        x = tf.keras.layers.Dropout(dropout_rate)(x)
        
    f = ff_dim
        
    while(f//2 &gt; 1):                
        x = tf.keras.layers.Dense(f//2, activation='relu')(x)
        x = tf.keras.layers.Dropout(dropout_rate)(x)
        f = f//2
            
    outputs = tf.keras.layers.Dense(1, activation = 'linear')(x)  # Regression output
    
    model = tf.keras.Model(inputs=inputs, outputs=outputs)
    return model
</code></pre>
<p>True vs Predicted Graph:
<a href=""https://i.sstatic.net/ew7LWGvI.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ew7LWGvI.png"" alt=""&quot;True vs Predicted Graph&quot;"" /></a></p>
<p>A previous question mentioned not to use tanh or sigmoid activation functions but I am only using relu (I also tried gelu and leaky relu). I tried adding more transformer layers but still get this problem. Can some one explain why I'm getting this and how to mitigate it?</p>
","nlp"
"129761","Should I merge tokens based on Named Entity Recognition?","2024-07-23 11:20:51","","0","8","<python><nlp><python-3.x><spacy>","<p>I am working on an NLP project for <em>sentiment analysis</em> and I am using SpaCy to tokenize sentences. As I was reading the <a href=""https://spacy.io/usage/linguistic-features#named-entities"" rel=""nofollow noreferrer"">documentation</a>, I learned about NER and I try to find if it can help me in tokenization.</p>
<p>To speak with examples:</p>
<p>Let's say we have a sentence <code>&quot;Let's not forget that Apple Pay in 2014 required...&quot;</code>. When I tokenize it, it breaks into <code>[Let, 's, not, forget, that, Apple, Pay, in, 2014, required, ...]</code>.</p>
<p><strong>My main question is</strong>, would it be a good technique to merge tokens that represent an entity, into one? For example, &quot;Apple&quot; and &quot;Pay&quot; would become &quot;Apple_Pay&quot;. My thinking is when training a classifier, not to confuse the organization &quot;Apple&quot; with the fruit &quot;apple&quot; (same for other words).</p>
<p>Would this improve performance or wouldn't worth the effort?</p>
","nlp"
"129757","How to convert messy dictionary definition string to array of short/summarized definitions as JSON, using open source AI tools?","2024-07-22 22:16:17","129794","0","66","<nlp><gpt><automatic-summarization><open-source>","<p>I am able to convert messy dictionary data to a JSON array of definitions, a summary definition (i.e. a &quot;gloss&quot;), and perhaps the part of speech (&quot;role&quot;), using OpenAI's Node.js API like so:</p>
<pre><code>import OpenAI from 'openai'
import 'dotenv/config'
import fs from 'fs/promises'

const PATH = `cleaned.json`

const openai = new OpenAI({
  apiKey: process.env.OPEN_AI_API_KEY,
})

async function main() {
  let i = 0
  const records = JSON.parse(await fs.readFile(`messy.json`, `utf-8`))

  const outputs = JSON.parse(await fs.readFile(PATH, 'utf-8'))

  for (const record of records) {
    if (outputs.find(o =&gt; o.term === record.word)) {
      i++
      continue
    }
    let text = await getText(record.def)
    try {
      if (typeof text === 'string') {
        try {
          text = text
            .split(/```json/)[1]
            .split(/```/)[0]
            .trim()
        } catch (e) {}
        // console.log(text)
        const output = JSON.parse(text)

        outputs.push({
          term: record.word,
          ...output,
        })
        console.log(i, i / records.length, record.word, output)

        await fs.writeFile(PATH, JSON.stringify(outputs, null, 2))
      } else {
        console.log(i)
      }
    } catch (e) {
      console.error(e)
      console.log(i)
    }

    i++

    async function getText(instructions: string) {
      const completion = await openai.chat.completions.create({
        messages: [
          {
            role: 'system',
            content: 'You are a helpful text summarizer.',
          },
          {
            role: 'user',
            content:
              `Extract the definitions from this set of messy definitions. Strip out the Tamil text and junk, and make the final set of definitions a small set. Don't include the definition if it says &quot;See x&quot;, linking to other definitions. Simplify each definition if it's not already simplified, to ideally 1-3 word definitions, removing or merging duplicate or similar definitions where applicable. Omit otherwise meaningless text from the input, which doesn't appear to be English. It's okay if the definition is longer than 3 words if it can't easily be shortened. Send the definitions as a JSON array of strings under the &quot;definitions&quot; key. Finally, take your summarized definitions, and summarize those into one short definition (ideally also 1-3 words), and return that under the &quot;gloss&quot; JSON key. Format all definitions and the gloss in lowercase unless it is a proper name, and don't use abbreviations where they can be easily expanded to the normal word. And then also, add the part of speech of the word that's being defined in the &quot;role&quot; field. Also, be conservative. If you can't figure it out clearly and simply, then return empty JSON (i.e. return nothing). Better to be safe than sorry. That's it. Here is the input as JSON: ` +
              JSON.stringify(instructions.split(/\s*;\s*/), null, 2),
          },
        ],
        model: 'gpt-4o',
      })

      return completion.choices[0].message.content
    }
  }
}

main()
</code></pre>
<p>Here is the main prompt:</p>
<blockquote>
<p>Extract the definitions from this set of messy definitions. Strip out the Tamil text and junk, and make the final set of definitions a small set. Don't include the definition if it says &quot;See x&quot;, linking to other definitions. Simplify each definition if it's not already simplified, to ideally 1-3 word definitions, removing or merging duplicate or similar definitions where applicable. Omit otherwise meaningless text from the input, which doesn't appear to be English. It's okay if the definition is longer than 3 words if it can't easily be shortened. Send the definitions as a JSON array of strings under the &quot;definitions&quot; key. Finally, take your summarized definitions, and summarize those into one short definition (ideally also 1-3 words), and return that under the &quot;gloss&quot; JSON key. Format all definitions and the gloss in lowercase unless it is a proper name, and don't use abbreviations where they can be easily expanded to the normal word. And then also, add the part of speech of the word that's being defined in the &quot;role&quot; field. Also, be conservative. If you can't figure it out clearly and simply, then return empty JSON (i.e. return nothing). Better to be safe than sorry. That's it. Here is the input as JSON: <code>[jsonArrayOfDefinitions]</code>.</p>
</blockquote>
<p>It costs about $100 per 20,000 API calls to do this sort of summarization/cleaning + return results as JSON, using OpenAI's API.</p>
<p>Is there a way to accomplish this same thing using open source tools in Node.js (such as huggingface's transformers.js), or Python, for free, using open source models? If so, what is the general approach? Or if it's not possible, what is the state of the art of open source versions in comparison with OpenAIs paid-for API?</p>
<p>Here is an example of a definition <strong>input</strong>:</p>
<blockquote>
<p>*அக்கடி akkaṭi , n. cf. akka + அடி. Difficulty, trouble in a voyage or journey, peril; அலைவு. எனக்கு அக்கடியா யிருக்கிறது. (R.)</p>
</blockquote>
<p>And here is what OpenAI's API spit out:</p>
<pre><code>{
  &quot;term&quot;: &quot;அக்கடி&quot;,
  &quot;definitions&quot;: [
    &quot;difficulty&quot;,
    &quot;trouble&quot;,
    &quot;peril&quot;
  ],
  &quot;gloss&quot;: &quot;difficulty&quot;,
  &quot;role&quot;: &quot;noun&quot;
}
</code></pre>
<p>Can it be done with open source? If so, what are the rough steps and/or pseudocode to get there?</p>
","nlp"
"129728","NER with custom tags and no training data, zero shot approach help","2024-07-17 21:18:07","","1","18","<machine-learning><deep-learning><classification><nlp><text-classification>","<p>I am building a &quot;field tagger&quot; for documents. Basically, a document, in my case something like a proposal or sales quote, would have a bunch of entities scattered throughout it, and we want to match each one to the field/tag that best describes it. The issue is that, though descriptive, the list of fields/tags doesnt exactly align with the existing NER entity and category types.</p>
<p>For example, some of the fields/tags (labels) in my case are things like: 'Ownership', 'Account Type', 'Main Competitor(s)', 'Account Name', 'Account Source', 'Industry', 'Account Number', 'Amount', 'Compare Name', 'Primary Partner Account ID', 'Active', 'Name', 'Has Open Activity', 'Probability (%)', 'Owner Alias', 'Owner ID', 'Has Line Item', 'Description', 'Account Site', 'Expected Amount', 'Account Description', 'Account Rating', 'Employees'. Things of that nature, which carry some amount of semantic meaning but perhaps not a lot.</p>
<p>So for instance, if the document is the text:</p>
<pre><code>&quot;The opportunity for &lt;&lt;ABC Company&gt;&gt; in the amount of &lt;&lt;$500,000&gt;&gt; is scheduled to close on &lt;&lt;July 30, 2024&gt;&gt;.&quot;
</code></pre>
<p>Where, at this stage, we can assume the entities have been marked so we don't have to handle recognizing what exactly is an entity in the raw document, I would want the tool to systematically go through each of the marked entities and match them to the most appropriate field/tag.</p>
<p>My initial approach, due to the lack of training data, has been to use a zero shot text classification approach, using facebook/bart-large-mnli, systematically iterating through the entities, passing the list of fields/tags as the labels, and a prompt asking to classify. The issue is I'm not sure that the model is semantically understanding the meaning of the labels themselves. What's worse is I'm unsure of how to pass context to the model, as simply adding it to the prompt, across the board, made predictions worse (things like company names, which were somewhat correctly identified beforehand, started to be treated as dates, etc).</p>
<p>Having no training data is a limit here, so my goal for this tools reflect that. It's only to build a tool that at least performs somewhat well. Frr instance, if the best label is in the top 5 most likely labels according to the model's probability distribution, I'd consider that a successful classification.</p>
<p>Wuld anyone be able to suggest an approach, or provide advice on how to adjust my existing one, to achieve some better results? How might I go about providing context in a way that enhances predictions? Any help would be appreciated</p>
","nlp"
"129687","How is Amazon extracting product relevant terms from reviews?","2024-07-13 12:22:04","","0","16","<nlp><feature-extraction><similarity><topic-model><real-ml-usecase>","<p>In the first picture, terms like <code>['Quality', 'Value', 'Taste', 'Health benefits', 'Freshness', 'Ingredients', 'Seal']</code> can be seen.</p>
<p>The reviews in the 1st picture are for toothpaste. Terms like &quot;effect on skin&quot; are not applicable to toothpaste but would be relevant for something like facewash. I assumed that Amazon might maintain a fixed list of terms per product category and run their model to find matching reviews for each term. However, for niche/exotic products in a category across wide range of prices, some tags that are available for one product could not be available for another. This suggests that the terms are being extracted from the reviews themselves by one model, with another model finding related claims with matching phrases. So, there is no fixed number or list of terms per product, but extraction is being done. I could be wrong. Please shed some light on this. The terms do no have fixed word count as well. This could also be done with Tf-Idf or topic-extraction, for all I know, but the extracted topics were relevant to the product.</p>
<ol>
<li>How do you ensure that relevancy for the product in question?</li>
<li>Can this extraction of terms be done category wise, I mean, for all toothpastes, instead of just dabur read toothpaste?</li>
<li>What pre-trained models can I start with (and fine-tune if required) to find such topics from reviews?</li>
</ol>
<p>Here are the two images of reviews for two products on Amazon.
<a href=""https://i.sstatic.net/pgPV0hfg.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/pgPV0hfg.png"" alt=""Reviews of a toothpaste"" /></a></p>
<p>and</p>
<p><a href=""https://i.sstatic.net/v1nJpWo7.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/v1nJpWo7.png"" alt=""Reviews of a facewash"" /></a></p>
","nlp"
"129657","Best model for enforcing corporate naming conventions","2024-07-10 05:52:42","","0","26","<python><nlp><bert><fuzzy-classification>","<p>I'm working on a project (Python) to enforce the company naming convention of products on product lists provided by clients/suppliers. I'm having a list of company names (Standardised names) and those of external. I'm considering typos too - generating this list using GPT.</p>
<p>Here's are the models I'm considering:<br />
Sequence-to-Sequence (Seq2Seq): LTSM over RNN<br />
Transformer-Based Models: BERT on custom data</p>
<p>Additionally, I'm looking up fuzzy string matching.</p>
<p>Could anyone recommend other approaches, or if I'm missing something?
Greatly appreciated :)</p>
<p>Edit:<br />
This a sample of the dataset I'm dealing with. This is the correct names. I'm creating my own dataset with around 20 incorrect names alongside the correct ones. There are about 20k+ unique names.</p>
<p><a href=""https://i.sstatic.net/mdKt161D.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/mdKt161D.png"" alt=""Dataset"" /></a></p>
","nlp"
"129640","NLP model for word recovery (analogy to BERT, but letters)","2024-07-07 11:03:14","","0","23","<nlp><bert><llm>","<p>I am working on solving the problem of restoring words in text where some letters are missing. For example (restore words where vowels are removed): Hll wrld -&gt; Hello world n ltrntv ssssmnt sggsts -&gt; An alternative assessment suggests</p>
<p>Can you please suggest what pre-trained models I can use to solve this problem or maybe there are some articles that describe similar problems? I know it's similar to BERT, but BERT predicts the word, not the letters.</p>
","nlp"
"129580","How can I make my Hugging Face fine-tuned model's config.json file reference a specific revision/commit from the original pretrained model?","2024-07-01 15:46:05","","0","18","<nlp><bert><huggingface><finetuning>","<p>I uploaded this model: <a href=""https://huggingface.co/pamessina/CXRFE"" rel=""nofollow noreferrer"">https://huggingface.co/pamessina/CXRFE</a>, which is a fine-tuned version of this model: <a href=""https://huggingface.co/microsoft/BiomedVLP-CXR-BERT-specialized"" rel=""nofollow noreferrer"">https://huggingface.co/microsoft/BiomedVLP-CXR-BERT-specialized</a></p>
<p>Unfortunately, CXR-BERT-specialized has this issue: <a href=""https://github.com/huggingface/transformers/issues/30412"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/issues/30412</a></p>
<p>I fixed the issue with this pull request: <a href=""https://huggingface.co/microsoft/BiomedVLP-CXR-BERT-specialized/discussions/5"" rel=""nofollow noreferrer"">https://huggingface.co/microsoft/BiomedVLP-CXR-BERT-specialized/discussions/5</a></p>
<p>However, when I save my fine-tuned model, the config.json file doesn't point to that specific pull request, pointing instead to the main branch by default, but the main branch of CXR-BERT-specialized has the aforementioned issue. As a consequence, when I try to use my model, it triggers the bug from the main branch of the underlying model, which shouldn't happen it were using the version from my pull request.</p>
<p>I've tried explicitly enforcing the revision I want like this:</p>
<pre><code>model = AutoModel.from_pretrained('microsoft/BiomedVLP-CXR-BERT-specialized', revision=&quot;6cfc310817fb7d86762d888ced1e3709c57ac578&quot;, trust_remote_code=True)
...
model.save_pretrained(&quot;/home/pamessina/huggingface_models/CXRFE/&quot;)
</code></pre>
<p>But the config file that gets saved doesn't reference the desired revision:</p>
<pre><code>{
  &quot;_name_or_path&quot;: &quot;microsoft/BiomedVLP-CXR-BERT-specialized&quot;,
  &quot;architectures&quot;: [
    &quot;CXRBertModel&quot;
  ],
  &quot;attention_probs_dropout_prob&quot;: 0.25,
  &quot;auto_map&quot;: {
    &quot;AutoConfig&quot;: &quot;microsoft/BiomedVLP-CXR-BERT-specialized--configuration_cxrbert.CXRBertConfig&quot;,
    &quot;AutoModel&quot;: &quot;microsoft/BiomedVLP-CXR-BERT-specialized--modeling_cxrbert.CXRBertModel&quot;
  },
  &quot;classifier_dropout&quot;: null,
  &quot;gradient_checkpointing&quot;: false,
  &quot;hidden_act&quot;: &quot;gelu&quot;,
  &quot;hidden_dropout_prob&quot;: 0.25,
  &quot;hidden_size&quot;: 768,
  &quot;initializer_range&quot;: 0.02,
  &quot;intermediate_size&quot;: 3072,
  &quot;layer_norm_eps&quot;: 1e-12,
  &quot;max_position_embeddings&quot;: 512,
  &quot;model_type&quot;: &quot;cxr-bert&quot;,
  &quot;num_attention_heads&quot;: 12,
  &quot;num_hidden_layers&quot;: 12,
  &quot;pad_token_id&quot;: 0,
  &quot;position_embedding_type&quot;: &quot;absolute&quot;,
  &quot;projection_size&quot;: 128,
  &quot;torch_dtype&quot;: &quot;float32&quot;,
  &quot;transformers_version&quot;: &quot;4.41.2&quot;,
  &quot;type_vocab_size&quot;: 2,
  &quot;use_cache&quot;: true,
  &quot;vocab_size&quot;: 30522
}
</code></pre>
<p>And when I try to use my fine-tuned model from Hugging Face on Google Colab, I get this error:</p>
<p><a href=""https://i.sstatic.net/rUuXJTyk.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/rUuXJTyk.png"" alt=""enter image description here"" /></a></p>
<p>As  you can see, it's invoking the version associated with the commit id &quot;b59c09e51ab2410b24f4be214bbb49043fe63fc2&quot;, when instead it should be using the commit id &quot;6cfc310817fb7d86762d888ced1e3709c57ac578&quot; with my pull request that fixes the bug.</p>
<p>What can I do?</p>
<p>Thanks in advance.</p>
","nlp"
"129579","What informatics tools will allow complex search in order to select full-text articles?","2024-07-01 11:28:39","","1","23","<nlp><r><llm><api>","<p>I'd like to search plant science literature (full text) to only return articles in which the word &quot;three&quot; appears four or more times in the full-text Methods section (presumably the best source of this information would be pubmed central). I've looked at scite - and this accepts json and regex - but apparently only searches citations rather than full-text methods. Perhaps it's possible to write a query which uses json and regex to query <a href=""https://europepmc.org/"" rel=""nofollow noreferrer"">https://europepmc.org/</a> to perform this specific action ? An alternative would be to use informatics tools to download huge amounts of data from PubMed and then construct R (or other) programming in order to search through the downloaded dataset. This question is about what is the most effective way forward in order to achieve this ? I have extensive R coding experience, Python shouldn't be a problem - I've been advised that an API would probably first be needed - and then NLP (natural language processing), particularly a technique called &quot;stemming&quot; and &quot;tokenising&quot;, or perhaps the large language model langchain ?</p>
","nlp"
"129572","Does it common for LM (hundreds million parameters) beat LLM (billion parameters) for binary classification task?","2024-07-01 01:16:10","","0","19","<python><neural-network><nlp><transformer><huggingface>","<p><strong>Preface</strong></p>
<p>I am trying to fine-tune the transformer-based model (LM and LLM). The LM that I used is DEBERTA, and the LLM is LLaMA 3. The task is to classify whether a text contains condescending language (binary classification).</p>
<p>I use <code>AutoModelForSequenceClassification</code>, which adds a classification layer to the model's top layer for both LM and LLM.</p>
<p><strong>Implementation</strong></p>
<ol>
<li><p>Dataset:</p>
<ul>
<li>Amount: it has about 10.000 texts with each text labeled <code>0</code> (for not condescending) and <code>1</code> (condescending). The proportion is <code>1:10</code> (condescending : not condescending).</li>
</ul>
</li>
<li><p>Parameter</p>
</li>
</ol>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Parameter</th>
<th>LM</th>
<th>LLM</th>
</tr>
</thead>
<tbody>
<tr>
<td>Batch size</td>
<td>32</td>
<td>16 (per_device_train_batch_size = 4,    gradient_accumulation_steps = 4)</td>
</tr>
<tr>
<td>Epoch / steps</td>
<td>2 epoch</td>
<td>1000 steps (20% used as validation set)</td>
</tr>
<tr>
<td>Learning Rate</td>
<td>linear (2e-5)</td>
<td>constant (2e-5)</td>
</tr>
<tr>
<td>Optimizer</td>
<td>AdamW (lr = 2e-5, eps = 1e-8)</td>
<td>paged_adamw_32bit</td>
</tr>
<tr>
<td>Fine-tuning</td>
<td>Full fine-tuning</td>
<td>LoRA (rank=32, dropout=0.5, alpha=8) with 8-bit quantization</td>
</tr>
<tr>
<td>Learning Rate</td>
<td>linear (2e-5)</td>
<td>constant (2e-5)</td>
</tr>
<tr>
<td>Precision</td>
<td>0,659</td>
<td>0,836</td>
</tr>
<tr>
<td>Recall</td>
<td>0,47</td>
<td>0,091</td>
</tr>
<tr>
<td>F1-score</td>
<td>0,549</td>
<td>0,164</td>
</tr>
</tbody>
</table></div>
<p><strong>Question and Issue</strong></p>
<p>Here is the log of the training sample. The validation f1-score is always <code>&gt;0.6</code>. But the validation loss is stuck at <code>0.24</code>. It is one of the samples of fine-tuned LLM.</p>
<p><a href=""https://i.sstatic.net/cWtxnVpgm.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/cWtxnVpgm.png"" alt=""enter image description here"" /></a></p>
<ol>
<li>Why does the test set f1-score only range from 0 - 0.2 for some parameter variation that I tuned when the f1-score for the validation set is always above 0.6, is it reasonable? why?</li>
<li>Is it common for LM to beat LLM for a particular task? If yes, what is the rationalization?</li>
</ol>
","nlp"
"129488","Keras: Relationship extraction","2024-06-21 05:04:18","","0","12","<nlp><keras>","<p>I am trying to manage how to extract logical relations from medical sentences. E.g.:</p>
<p>INPUT: &quot;Candidiasis is infection by Candida species (most often C. albicans), manifested by mucocutaneous lesions, fungemia, and sometimes focal infection of multiple sites.&quot;</p>
<p>OUTPUT: &quot;Candidiasis – infection&quot; &quot;Candidiasis – Candida species (most often C. albicans)&quot; &quot;Candidiasis – mucocutaneous lesions&quot; &quot;Candidiasis – fungemia&quot; &quot;Candidiasis – focal infection&quot;</p>
<p>How this type of problem is called and what architechtures should I use for solving it?</p>
<p>I read that this problem is somehow related with POS-tagging, but I cannot figure how make model search relations for each word in a sentence.</p>
","nlp"
"129390","NLP: how to handle bad tokenization","2024-06-12 03:50:46","","0","21","<machine-learning><nlp><huggingface>","<p>I get nonsense when trying to translate the following german sentence to swedish using google/madlad400-3b-mt:</p>
<blockquote>
<p>a. Natürliche Personen: BundID mit ELSTER-Zertifikat oder nPA/eID/eAT-Authentifizierung
b. Juristische Personen: Unternehmenskonto BUND mit ELSTER-Zertifik</p>
</blockquote>
<p>-&gt;</p>
<blockquote>
<p>. Personen mit Behinderung: BundesID mit ELSTER-Zertifikat oder nPA/eID/eAT-Authentifizierung c. Personen mit Behinderung: BundesID mit ELSTER-Zertifikat oder nPA db. Personen mit Behinderung: BundesID mit ELSTER-Zertifikat oder nPA/e</p>
</blockquote>
<p>Code:</p>
<pre><code>pipe = pipeline(&quot;translation&quot;, model=&quot;google/madlad400-3b-mt&quot;)
pipe('&lt;2sv&gt;'+input, max_length = n_words*5)

</code></pre>
<p>This is likely due to the abundance of abbreviations and special words.</p>
<p><em>Is there a per sentence metric I can use to measure bad tokenizations? A naive one would be to calulate the percentage of unknown tokens. In my case the problem seems to be that it falsely attends to abbreviations rather than unknown confusion.</em></p>
","nlp"
"129350","Search for documents with similar texts","2024-06-09 16:05:35","","0","19","<nlp><llm><search>","<p>I have a document with three attributes: tags, location, and text.</p>
<p>Currently, I am indexing all of them using LangChain/pgvector/embeddings.</p>
<p>I have satisfactory results, but I want to know if there is a better way since I want to find one or more documents with a specific tag and location, but the text can vary drastically while still meaning the same thing. I thought about using embeddings/vector databases for this reason.</p>
<p>Would it also be a case of using RAG (Retrieval-Augmented Generation) to &quot;teach&quot; the LLM about some common abbreviations that it doesn't know?</p>
","nlp"
"129348","The real world implementations of RAG vs the methods explained in the paper","2024-06-09 08:08:21","","1","24","<nlp><transformer><llm>","<p>While building a RAG application we</p>
<ol>
<li>Encode the query</li>
<li>Retrieve k docs</li>
<li>Concatenate before the query</li>
<li>Pass the entire thing to a LLM and it completes it for you</li>
</ol>
<p>I do not think this is either of RAG-sequence or RAG-token explained in the <a href=""https://arxiv.org/abs/2005.11401"" rel=""nofollow noreferrer"">paper</a>. Or, am I missing something. It seems closer to RAG-sequence but there also there are k output sequences generated which are then marginalized over the retrieved documents and the output sequence with the highest probability is selected</p>
<p>From the paper:</p>
<blockquote>
<p>RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate
the complete sequence. Technically, it treats the retrieved document as a single latent variable that
is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the
top K documents are retrieved using the retriever, and the generator produces the output sequence
probability for each document, which are then marginalized,</p>
</blockquote>
<blockquote>
<p>RAG-Token Model In the RAG-Token model we can draw a different latent document for each
target token and marginalize accordingly. This allows the generator to choose content from several
documents when producing an answer. Concretely, the top K documents are retrieved using the
retriever, and then the generator produces a distribution for the next output token for each document,
before marginalizing, and repeating the process with the following output token</p>
</blockquote>
","nlp"
"129341","In natural language processing, what is the name for the technique in which a sentence is modeled as a tree in order to generate simpler sentences?","2024-06-08 17:10:16","","0","27","<nlp><decision-trees>","<p>In natural language processing, there are times when we model a complex and/or compound sentence as a tree (or hierarchy) of simpler sentences.</p>
<p>The tree-model (hierarchical model) can help us translate a long sentence <strong>σ₀</strong> into two or more sentences in set <strong>XS</strong> such that for any <strong>x ∈ XS</strong>, sentence <strong>x</strong> is shorter than sentence <strong>σ₀</strong> (sigma-knot) and the concatenation of all sentences <strong>x ∈ XS</strong> is longer than sentence <strong>σ₀</strong>.</p>
<p>What is the name for the process of creating this type of prefix tree?</p>
<hr />
<h2>Step One (input)</h2>
<blockquote>
<p>Entiendo que estás intentando practicar y mejorar tu español conmigo, y agradezco tu esfuerzo.</p>
</blockquote>
<hr />
<h2>Step Two</h2>
<pre><code>Entiendo que estás/
├─ intentando practicar tu español conmigo/
├─ mejorar tu español conmigo/
├─ agradezco tu esfuerzo/
</code></pre>
<hr />
<h2>Step Three (output)</h2>
<blockquote>
<ol>
<li><p>Entiendo que estás intentando practicar tu español conmigo</p>
</li>
<li><p>Entiendo que estás mejorar tu español conmigo.</p>
</li>
<li><p>Entiendo que estás agradezco tu esfuerzo</p>
</li>
</ol>
</blockquote>
","nlp"
"129339","What is query generation re-ranking method?","2024-06-08 04:39:28","","0","6","<deep-learning><nlp><language-model><information-retrieval>","<p>I am reading up on reranking methodologies that leverage LLMs. Relevant <a href=""https://arxiv.org/pdf/2304.09542"" rel=""nofollow noreferrer"">literature</a>.</p>
<p>One of the methods suggested is query generation
<a href=""https://i.sstatic.net/nSADiKeP.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/nSADiKeP.png"" alt=""enter image description here"" /></a></p>
<p>Or, the same methodology from another <a href=""https://www.rungalileo.io/blog/mastering-rag-how-to-select-a-reranking-model#:%7E:text=colbert%2Dv1%2Den-,LLMs%20for%20Reranking,-As%20LLMs%20grow"" rel=""nofollow noreferrer"">source</a>
<a href=""https://i.sstatic.net/JpoqD6u2.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/JpoqD6u2.png"" alt=""enter image description here"" /></a></p>
<p>The task is to rank the passages according to their relevance to a given passage</p>
<p>Here they suggest giving the LLM the context and asking it to generate a query. How does this help rank the contexts as per their relevance to the given query? We generate a query given all the contexts.. Now what?</p>
","nlp"
"129298","How to interpret the token embeddings from decoders?","2024-06-04 20:26:05","","0","23","<nlp><transformer>","<p>I am having trouble thinking about the token embeddings from masked attention compared to BERT.</p>
<p>Let's say we have 5 tokens. The embedding of the first token will be used to predict the second token, but we already know what the second token is. If we have only one decoder, then we can just use embedding of the 5th token to predict the next one (the other embeddings can be ignored). However, if we have a second decoder, then the embedding of the first token will be used by that second decoder (the intermediate embeddings from the first decoder are used).</p>
<p>After the first token, there are many possible next tokens. It seems to me that the embeddings for the first tokens won't be very informative (they have very little context). Why would the decoders after the first one pay attention to the intermediate results used to predict the previous tokens (1st, 2nd, 3rd, 4th) when they essentially have access to the final result (5th token)?</p>
<p>Can we think of the embedding of the 5th token as a sentence embedding? Can we do (5th token embedding - 4th token embedding) to obtain something similar to BERT and do token classification with GPTs?</p>
<p>(When I say 1st token, I was thinking of a word. I wonder if the embedding of the &lt;START_TOKEN&gt; is changed throughout the decoder blocks)</p>
","nlp"
"129284","Calculating topic vector similarity based on document frequency","2024-06-03 21:33:54","","0","11","<nlp><clustering><similarity><topic-model><similar-documents>","<p>I am looking into ways to calculate the similarity of two topic vectors, where each dimension of a vector is a tuple made up a word describing the topic, along with the document frequency (tf-idf) of that word. I would like to account for the document frequency when measuring the similarity between topics, rather than just the number of matching words, and am looking for suggestions about how to do this.</p>
<p>As an example, given these 3 topics:</p>
<pre><code>topic1 = [('blue',.8), ('red',.4), ('sky',.1), ('boy',.01)]
topic2 = [('water',.5), ('fire',.1), ('sky',.05), ('boy',.02)]
topic3 = [('blue',.7), ('fire',.1), ('earth',.02), ('screen',.1)]
</code></pre>
<p>For <code>topic1</code>, if I was just counting the number of matching words, I would say that <code>topic2</code> is most similar, because 2 of the words match.</p>
<p>However, the two matching words, <code>'sky'</code> and <code>'boy'</code>, have relatively low document frequency scores in <code>topic2</code>, .05 and .02, respectively. On the other hand, both <code>topic1</code> and <code>topic3</code> contain <code>'blue'</code>, and this word has a much higher document frequency in <code>topic3</code>, .7.</p>
<p>Given these two scenarios, how might I go about incorporating tf-idf scores into measuring similarity, going beyond just counting the number of matching terms? Could I add up the tf-idf of all matches? Would I take the average? Should I use a transformation such as the log of each score? Any suggestions would be appreciated.</p>
","nlp"
"129263","Noob question - which NLP/deep learning technique shoud I use","2024-06-01 17:59:37","","2","44","<python><deep-learning><nlp><multilabel-classification><llm>","<p>Let's say I have dataset with inputs and expected outputs like this:</p>
<pre class=""lang-json prettyprint-override""><code>[
  {
    &quot;input&quot;: &quot;http://localhost/wordpress/wp-includes/blocks/navigation/view.min.js?ver=6.5.3&quot;,
    &quot;output&quot;: [&quot;WordPress 6.5.3&quot;]
  },
  {
    &quot;input&quot;: &quot;&lt;meta content=\&quot;max-image-preview:large\&quot; name=\&quot;robots\&quot;/&gt;&quot;,
    &quot;output&quot;: []
  },
  {
    &quot;input&quot;: &quot;https://cdnjs.cloudflare.com/ajax/libs/jquery/3.7.1/jquery.min.js?ver=3.7.1&quot;,
    &quot;output&quot;: [&quot;jQuery 3.7.1&quot;]
  },
  {
    &quot;input&quot;: &quot;Server: Apache/2.4.56 (Win64) OpenSSL/1.1.1t PHP/8.2.4&quot;,
    &quot;output&quot;: [&quot;Apache 2.4.56&quot;, &quot;OpenSSL 1.1.1t&quot;, &quot;PHP 8.2.4&quot;]
  },
  { &quot;input&quot;: &quot;X-Powered-By: PHP/7.4&quot;, &quot;output&quot;: [&quot;PHP 7.4&quot;] },
  ...
]
</code></pre>
<p>I would like to create a program that extracts/guesses which technologies (ideally with version) are in a given input.</p>
<p>I read something about multi-label classification, named entity recognition and also about fine tuning some LLM.
I'm still learning, not sure how best to solve this problem. Thanks for advice!</p>
","nlp"
"129249","How to find out that a conversation with a chatbot is likely ended","2024-05-31 09:50:56","","0","10","<python><nlp><language-model><chatbot>","<p>I'm working on a ChatBot with <code>Python</code> and <code>langchain</code>, and I'd like to have a metric that I could use to understand how close we are to the completion of a discussion.</p>
<p>So, for instance, that metric should score high if the last sentence of the history was <code>Goodbye</code> or <code>I see that we both agree</code>, or any other that somehow implies that the probability that we'll continue with the same topic is low.</p>
<p>Is there something that I can use on langchain or another library to help me with this?</p>
","nlp"
"129248","Latest Tree-based models","2024-05-31 07:03:47","","0","18","<machine-learning><python><nlp><decision-trees><gradient-boosting-decision-trees>","<p>What are the latest Tree-based models that are used in machine learning? Tell the new models except the old ones such as the Decision tree, Random Forest, Gradient Boosting, LightGBM, XGBoost, and CatBoost.</p>
","nlp"
"129233","Are there any ready to use Pytorch, TensorFlow or ONNX part-of-speech taggers that are below 100MB (disk space)?","2024-05-30 04:07:18","","0","34","<nlp><pytorch><knowledge-distillation>","<p>Are there any ready to use Pytorch, Tensorflow or onnx part-of-speech taggers that are below 100MB of disk space?</p>
<hr />
<p>My research:</p>
<p>The code below from <a href=""https://github.com/Kyubyong/nlp_made_easy/blob/master/Pos-tagging%20with%20Bert%20Fine-tuning.ipynb"" rel=""nofollow noreferrer"">Kyubyong/nlp_made_easy</a> works well to fine-tune Bert for part-of-speech tagging, but the model  is 450 MB of disk space.</p>
<pre class=""lang-py prettyprint-override""><code>'''
reqs: pip install pytorch-pretrained-bert numpy torch nltk sklearn onnx onnxruntime

__author__ = &quot;kyubyong&quot;
__address__ = &quot;https://github.com/kyubyong/nlp_made_easy&quot;
__email__ = &quot;kbpark.linguist@gmail.com&quot;
'''

import os
from tqdm import tqdm_notebook as tqdm
import numpy as np
import torch
import torch.nn as nn
from torch.utils import data
import torch.optim as optim
from pytorch_pretrained_bert import BertTokenizer

import torch.onnx

import nltk
nltk.download('treebank')
tagged_sents = nltk.corpus.treebank.tagged_sents()
print(len(tagged_sents))
print(tagged_sents[0])

tags = list(set(word_pos[1] for sent in tagged_sents for word_pos in sent))
print(&quot;,&quot;.join(tags))

# By convention, the 0'th slot is reserved for padding.
tags = [&quot;&lt;pad&gt;&quot;] + tags

tag2idx = {tag:idx for idx, tag in enumerate(tags)}
idx2tag = {idx:tag for idx, tag in enumerate(tags)}
# Let's split the data into train and test (or eval)
from sklearn.model_selection import train_test_split
train_data, test_data = train_test_split(tagged_sents, test_size=.1)
print(len(train_data), len(test_data))
device = 'cuda' if torch.cuda.is_available() else 'cpu'

tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)

class PosDataset(data.Dataset):
    def __init__(self, tagged_sents):
        sents, tags_li = [], [] # list of lists
        for sent in tagged_sents:
            words = [word_pos[0] for word_pos in sent]
            tags = [word_pos[1] for word_pos in sent]
            sents.append([&quot;[CLS]&quot;] + words + [&quot;[SEP]&quot;])
            tags_li.append([&quot;&lt;pad&gt;&quot;] + tags + [&quot;&lt;pad&gt;&quot;])
        self.sents, self.tags_li = sents, tags_li

    def __len__(self):
        return len(self.sents)

    def __getitem__(self, idx):
        words, tags = self.sents[idx], self.tags_li[idx] # words, tags: string list

        # We give credits only to the first piece.
        x, y = [], [] # list of ids
        is_heads = [] # list. 1: the token is the first piece of a word
        for w, t in zip(words, tags):
            tokens = tokenizer.tokenize(w) if w not in (&quot;[CLS]&quot;, &quot;[SEP]&quot;) else [w]
            xx = tokenizer.convert_tokens_to_ids(tokens)

            is_head = [1] + [0]*(len(tokens) - 1)

            t = [t] + [&quot;&lt;pad&gt;&quot;] * (len(tokens) - 1)  # &lt;PAD&gt;: no decision
            yy = [tag2idx[each] for each in t]  # (T,)

            x.extend(xx)
            is_heads.extend(is_head)
            y.extend(yy)

        assert len(x)==len(y)==len(is_heads), &quot;len(x)={}, len(y)={}, len(is_heads)={}&quot;.format(len(x), len(y), len(is_heads))

        # seqlen
        seqlen = len(y)

        # to string
        words = &quot; &quot;.join(words)
        tags = &quot; &quot;.join(tags)
        return words, x, is_heads, tags, y, seqlen

def pad(batch):
    '''Pads to the longest sample'''
    f = lambda x: [sample[x] for sample in batch]
    words = f(0)
    is_heads = f(2)
    tags = f(3)
    seqlens = f(-1)
    maxlen = np.array(seqlens).max()

    f = lambda x, seqlen: [sample[x] + [0] * (seqlen - len(sample[x])) for sample in batch] # 0: &lt;pad&gt;
    x = f(1, maxlen)
    y = f(-2, maxlen)


    f = torch.LongTensor

    return words, f(x), is_heads, tags, f(y), seqlens

from pytorch_pretrained_bert import BertModel


class Net(nn.Module):
    def __init__(self, vocab_size=None):
        super().__init__()
        self.bert = BertModel.from_pretrained('bert-base-cased')

        self.fc = nn.Linear(768, vocab_size)
        self.device = device

    def forward(self, x, y):
        '''
        x: (N, T). int64
        y: (N, T). int64
        '''
        x = x.to(device)
        y = y.to(device)

        if self.training:
            self.bert.train()
            encoded_layers, _ = self.bert(x)
            enc = encoded_layers[-1]
        else:
            self.bert.eval()
            with torch.no_grad():
                encoded_layers, _ = self.bert(x)
                enc = encoded_layers[-1]

        logits = self.fc(enc)
        y_hat = logits.argmax(-1)
        return logits, y, y_hat


def train(model, iterator, optimizer, criterion):
    model.train()
    for i, batch in enumerate(iterator):
        words, x, is_heads, tags, y, seqlens = batch
        _y = y  # for monitoring
        optimizer.zero_grad()
        logits, y, _ = model(x, y)  # logits: (N, T, VOCAB), y: (N, T)

        logits = logits.view(-1, logits.shape[-1])  # (N*T, VOCAB)
        y = y.view(-1)  # (N*T,)

        loss = criterion(logits, y)
        loss.backward()

        optimizer.step()

        if i % 10 == 0:  # monitoring
            print(&quot;step: {}, loss: {}&quot;.format(i, loss.item()))


def eval(model, iterator):
    model.eval()

    Words, Is_heads, Tags, Y, Y_hat = [], [], [], [], []
    with torch.no_grad():
        for i, batch in enumerate(iterator):
            words, x, is_heads, tags, y, seqlens = batch

            _, _, y_hat = model(x, y)  # y_hat: (N, T)

            Words.extend(words)
            Is_heads.extend(is_heads)
            Tags.extend(tags)
            Y.extend(y.numpy().tolist())
            Y_hat.extend(y_hat.cpu().numpy().tolist())

    ## gets results and save
    with open(&quot;result&quot;, 'w') as fout:
        for words, is_heads, tags, y_hat in zip(Words, Is_heads, Tags, Y_hat):
            y_hat = [hat for head, hat in zip(is_heads, y_hat) if head == 1]
            preds = [idx2tag[hat] for hat in y_hat]
            assert len(preds) == len(words.split()) == len(tags.split())
            for w, t, p in zip(words.split()[1:-1], tags.split()[1:-1], preds[1:-1]):
                fout.write(&quot;{} {} {}\n&quot;.format(w, t, p))
            fout.write(&quot;\n&quot;)

    ## calc metric
    y_true = np.array([tag2idx[line.split()[1]] for line in open('result', 'r').read().splitlines() if len(line) &gt; 0])
    y_pred = np.array([tag2idx[line.split()[2]] for line in open('result', 'r').read().splitlines() if len(line) &gt; 0])

    acc = (y_true == y_pred).astype(np.int32).sum() / len(y_true)

    print(&quot;acc=%.2f&quot; % acc)


model = Net(vocab_size=len(tag2idx))
model.to(device)
model = nn.DataParallel(model)
train_dataset = PosDataset(train_data)
eval_dataset = PosDataset(test_data)

train_iter = data.DataLoader(dataset=train_dataset,
                             batch_size=8,
                             shuffle=True,
                             num_workers=1,
                             collate_fn=pad)
test_iter = data.DataLoader(dataset=eval_dataset,
                             batch_size=8,
                             shuffle=False,
                             num_workers=1,
                             collate_fn=pad)

optimizer = optim.Adam(model.parameters(), lr = 0.0001)

criterion = nn.CrossEntropyLoss(ignore_index=0)
train(model, train_iter, optimizer, criterion)
eval(model, test_iter)

print(open('result', 'r').read().splitlines()[:100])

# Save the model
model_path = &quot;bert_pos_tagger_model.pth&quot;
torch.save(model.state_dict(), model_path)
print(f&quot;Model saved to {model_path}&quot;)


# Export the model to ONNX
dummy_input = torch.LongTensor([[0] * 128]).to(device)  # Adjust the input shape according to the model's expected input
onnx_path = &quot;pos_tagger_model.onnx&quot;
torch.onnx.export(model.module, (dummy_input, dummy_input), onnx_path,
                  input_names=[&quot;input&quot;],
                  output_names=[&quot;output&quot;],
                  dynamic_axes={&quot;input&quot;: {0: &quot;batch_size&quot;, 1: &quot;sequence_length&quot;},
                                &quot;output&quot;: {0: &quot;batch_size&quot;, 1: &quot;sequence_length&quot;}})
print(f&quot;Model exported to {onnx_path}&quot;)
</code></pre>
","nlp"
"129208","Fine-tuning pretrained model on 2 tasks with 2 labeled dataset","2024-05-27 16:05:43","","0","25","<classification><nlp><bert>","<p>I am having difficulty using BERT for a sentiment analysis task that handles both aspect-based sentiment analysis (ABSA) and comment sentiment analysis. I know that using two separate classification layers on top of the hidden layer can classify these two tasks, but I don't know how to handle labels for these two datasets. One dataset only has sentiment labels (positive, negative, and neutral), while the other includes 10 aspects with 4 sentiment labels (including NONE for aspects that are not present). I am wondering if I can assign a task type to the data and use it to identify which task is being trained on the model. Please let me know your thoughts on this problem. Thank you for your help! Have a good day!!!!!</p>
","nlp"
"129177","Calculating weighted cosine similarity between vectors of words","2024-05-24 19:22:51","","0","73","<nlp><clustering><similarity><cosine-distance>","<p>I have two word lists, where each word is representative of each topic. A topic is created from a collection of documents (tweets in this case). Not all words would’ve appeared an equal number of times in the collection of documents, which is why it has an associated tf-idf weight (the number next to each  word).</p>
<pre><code>topic1 = [('blue',.1), ('red',.05), ('sky',.01)]
topic2 = [('water',.5), ('fire',.1), ('earth',.02)]
</code></pre>
<p>I am trying to calculate the cosine similarity between the vectors, but also account for the tf-idf weighting of each word, where words with higher tf-idf weights will have more influence on cosine similarity.</p>
<p>Is there a commonly accepted way to account for individual weightings when doing vector similarity? How would I implement this?</p>
","nlp"
"129149","Any resource to learn NLP and latest updates for GEN AI like research papers","2024-05-21 12:49:32","","0","22","<nlp><ai>","<p>Looking to start learning for industry ready(Career change) in NLP and GEN AI.</p>
<p>Guidance will help me best like how to start, any course material and which projects to implement, where to implement(any online resource, Probability and other topics plus interview queries regarding models like what mathematics asked in interview and what sort of questions is asked on Models and follow on queries on Models like that stuff i.e. critical thinking aspect on Models)</p>
<p>Looking links for latest Models in GEN AI and NLP.</p>
<p>If someone can Help.</p>
<p>Thanks and Regards</p>
","nlp"
"129075","Improving GPU Utilization in LLM Inference System","2024-05-14 16:17:46","","1","110","<machine-learning><python><deep-learning><neural-network><nlp>","<p>I´m trying to build a distributed LLM inference platform with Huggingface support. The implementation involves utilizing Python for model processing and Java for interfacing with external systems. Below is the Python code responsible for receiving input text from a Java program, processing it through a pre-trained LLM, and returning the processed text:</p>
<pre><code>import socket, sys
import threading
from transformers import pipeline

generator = pipeline('text-generation', model='gpt2-large', device=&quot;cuda&quot;)


def process_input(input_text):
    request = generator(input_text, min_length=200)
    return request[0][&quot;generated_text&quot;]


def handle_connection(conn):
    with conn:
        data = conn.recv(10240).decode()
        processed_data = process_input(data.strip())
        conn.sendall(processed_data.encode())


PORT = int(sys.argv[1])

with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
    s.bind(('localhost', PORT))
    s.listen()
    while True:
        conn, addr = s.accept()
        thread = threading.Thread(target=handle_connection, args=(conn,))
        thread.start()
</code></pre>
<p>As you can see, it accepts socket connections from the Java process, receives the text, returns the request result, and closes the connection, all inside the execution of a thread. There is a shared pipeline among all threads, since it takes too much space in memory, it's only created 1 time.</p>
<p>On the Java side, I have a class <code>LLMProcess</code> that handles the creation and communication with the Python process, using threads for each request lifetime.</p>
<pre><code>LLMProcess process = new LLMProcess();

for (int i = 0; i &lt; 50; i++) {
    int index = i;
    Thread thread = new Thread(() -&gt; {
        System.out.println(&quot;&quot; + index + &quot; : &quot; + process.request(&quot;Sample text&quot;);
        System.out.flush();
    });
    thread.start();
}
</code></pre>
<p>However, when attempting to execute a substantial number of requests simultaneously, the system demonstrates sequential behavior in processing the requests and exhibits an overhead associated with thread usage, rather than effectively leveraging concurrent processing via the LLM pipeline and GPU acceleration.</p>
<p>The goal is to optimize this process by minimizing thread usage overhead and fully utilizing available GPU resources. Despite the presence of GPU support, its load remains minimal during program execution, typically not exceeding 3%.</p>
","nlp"
"129067","Data generation methods for NLP tasks","2024-05-13 17:49:22","129068","0","23","<nlp><dataset>","<p>I am doing a <strong>Natural language processing</strong> related project. It is a sentiment analysis task. I need to generate a dataset for the uniqueness of the work.</p>
<p>Is there any recommendation on how can I generate a dataset? I don't want to depend on social media websites or web scraping to generate the data.</p>
<p>Thank you</p>
","nlp"
"129029","What to do if I have a very low metric on one of the classes during multiclass classification?","2024-05-10 17:59:48","","0","19","<python><nlp><python-3.x><classifier><fasttext>","<p>I trained multiclass text classififer with fasttext. I have a very low metric on one of the classes. Here are results of metrics for each class on test data:</p>
<pre><code>               precision    recall  f1-score   support

     class1         0.66      0.68      0.67      1331
     class2         0.72      0.76      0.74      2297
     class3         0.50      0.32      0.39       410
     class4         0.62      0.60      0.61      1019

     accuracy                           0.67      5057
    macro avg       0.62      0.59      0.60      5057
 weighted avg       0.66      0.67      0.67      5057
</code></pre>
<p>as you see it doesnt work good on class  'class3'.</p>
<p>Data was unbalanced with class3 being the smallest. I cleaned text data before training. Butt still i get bad results. What can I do to improve it?</p>
","nlp"
"129016","Unsupervised metrics for search engine","2024-05-08 23:43:27","","0","15","<nlp><embeddings><search-engine>","<p>I'm the new one to ML, currently writing search engine with different models (tried LSTM, BERT, sentence-transformers) to get sentence embeddings, there are about 2k documents, and don't know how to rate perfomance of each model, because output is the range for each query for each model, but I don't have any labels, so I'm a little bit confused. I'm thinking about creating BOW for every document and analyze query on it, but the point of search engine with nueral network is understanding the context, so I don't know how. Will be very grateful for any ideas! Thanks</p>
","nlp"
"129015","Leveraging Extra Data to Enhance Text Clustering","2024-05-08 22:27:41","","0","4","<machine-learning><nlp><clustering>","<p>I want to cluster thousands of text data (called corpus A) and find a label for each cluster. Accuary of clustering is significantly important, because I want to use the texts and their labels for training a generative model.</p>
<p>I am using silhouette score for measuring the clustering performance but it does not look good. It is under 0.1 which does not look promising. On the other hand, I have some abstract description (called corpus B) about each text. However, they can not be used as a label. Because two similar text, can have different description with the same meaning. For exmaple, two description about are</p>
<p>Focuses on algorithms used in machine learning</p>
<p>Explores techniques for natural language processing</p>
<p>These description are related to two different text which talking about AI. Notice that it is not possible to check the clustering results by human, because of the huge amount of text data.</p>
<p>Now, my question is can I use these description (B) to improve the clustering?</p>
<p>Also, do you think it is a good idea to use corpus B for evaluating the ML (clustering or any other algorithm) model? for example, in the hyperparameter tuning, calculate the intra and inter cosine similarity on corpus B?</p>
","nlp"
"128979","Why do we use similarity/cosine between Query and Key in attention?","2024-05-07 04:45:28","","0","135","<deep-learning><nlp><transformer><attention-mechanism>","<p>Let's take an example sentence for translation:</p>
<p><code>I am going to my home and play with toy house.</code></p>
<p>For translating 'home', as per my understanding, Query will be 'house's embedding vector, Key will be each of the token's vectors i.e size 11 (word based token).</p>
<p>Then we take cosine to find the similarity.</p>
<p>Why? Why similarity? '<strong>Home</strong>' and '<strong>House</strong>' shall be most similar, but that doesn't play a part in the translation. Rather, probably 'toy' is more important here from translation perspective.</p>
","nlp"
"128969","jar files downloading very slowly in jupyter notebook in Mac Book(M2 pro)","2024-05-06 10:13:29","","0","11","<machine-learning><deep-learning><nlp><apache-spark><pyspark>","<p>Required jar files are downloading from maven repository in Jupyter notebook are very slow in Mac book (M2 pro). how can i increase the speed of download?</p>
<p><a href=""https://i.sstatic.net/1KuN6AH3.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/1KuN6AH3.png"" alt=""enter image description here"" /></a></p>
","nlp"
"128963","How OpenAI embeddings work?","2024-05-06 00:38:26","128967","1","254","<nlp><word-embeddings><embeddings><gpt>","<p>I was looking at the <a href=""https://www.youtube.com/playlist?list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ"" rel=""nofollow noreferrer"">Stanford CS224N NLP with Deep Learning lecture</a>, and in the first two videos, we are introduced to word2vec models. The high-level idea mentioned was that we have a 'big corpus' of text, and then we use SG or CBOW to generate the embeddings (There will of course be many more models). While watching the video I was comparing the approach with how we generate embeddings from OpenAI and this raised a few questions for me</p>
<ol>
<li><p>What is the 'big corpus' of text that OpenAi uses? Is it only my input? If I send only one word like 'cat' it gives an output, so where does the context come from? If OpenAI doesn't use this approach how does it generate the output? And how is it so fast?</p>
</li>
<li><p>Are the OpenAI embedding models substitutes for SG or CBOW?</p>
</li>
<li><p>How does the vector embedding generation differ from words, compared to that of sentences or paragraphs? Are they entirely different techniques or extensions of the same approach?</p>
</li>
</ol>
","nlp"
"128950","Information extraction with word count limit","2024-05-03 22:47:46","128955","1","28","<nlp>","<p>I have a task which I am not sure which algorithm or model to follow as a start. Suppose I have a corpus of texts. Let's assume that each set of text describes something, and the length of the text may not be known in advance. I need to provide a summary of each set of text within a certain character count limit (say 40 characters, including spaces). Of course, the summary must be in proper words and phrases (but need not be a full sentence). The summary should capture as much gist of the text as possible.</p>
<p>All in English.</p>
<p>What model or algorithm should I explore for this purpose?</p>
","nlp"
"128908","How can self-attention be used to combine representations from long text?","2024-04-30 18:42:49","","0","14","<python><nlp><bert><attention-mechanism>","<p>The paper &quot;<a href=""https://arxiv.org/abs/1905.05583"" rel=""nofollow noreferrer"">How to Fine-Tune BERT for Text Classification?</a>&quot; discusses using self-attention to combine the representations of a long input text that has been broken into chunks (section 5.3.1).</p>
<blockquote>
<p>The input text is firstly divided into k = L/510 fractions, which is fed into BERT to obtain the representation of the k text fractions. The representation of each fraction is the hidden state of the [CLS] tokens of the last layer. Then we use mean pooling, max pooling and self-attention to combine the representations of all the fractions.</p>
</blockquote>
<p>However, the paper does not go into detail on how this is actually done. Could anyone please explain how this is accomplished or provide examples that go into greater detail? I'm looking to implement this in Python alongside code that uses the Hugging Face transformers package.</p>
","nlp"
"128875","""No sentence-transformers model found with name"" on huggingface even though it exists","2024-04-28 10:00:02","","1","498","<nlp><word-embeddings><huggingface>","<p>I am trying to use <a href=""https://huggingface.co/infgrad/stella-base-en-v2"" rel=""nofollow noreferrer"">infgrad/stella-base-en-v2</a> on hugging to generate embeddings using langchain</p>
<ol>
<li>The model exists on the huggingface hub</li>
<li>The model is listed on the MTEB leaderboard</li>
<li>The model has sentence transformer tag on it</li>
</ol>
<p>Still when I try to use it like so:</p>
<pre><code>from langchain_community.embeddings import HuggingFaceEmbeddings
embeddings = HuggingFaceEmbeddings(model_name = &quot;infgrad/stella-base-en-v2&quot;)
</code></pre>
<p>I get the warning:</p>
<pre><code>No sentence-transformers model found with name infgrad/stella-base-en-v2. Creating a new one with MEAN pooling.
</code></pre>
<p>Why is that the case?</p>
","nlp"
"128857","How do I get model.generate() to omit the input sequence from the generation?","2024-04-26 15:06:02","128858","1","208","<nlp><pytorch><huggingface>","<p>I'm using Huggingface to do inference on llama-3-B. Here is my model:</p>
<pre><code>model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = &quot;unsloth/llama-3-8b-Instruct-bnb-4bit&quot;,
    max_seq_length = 2048,
    dtype = torch.float16,
    load_in_4bit = True,
)

model = FastLanguageModel.get_peft_model(
    model,
    r = 16,
    target_modules = [&quot;q_proj&quot;, &quot;k_proj&quot;, &quot;v_proj&quot;, &quot;o_proj&quot;, &quot;gate_proj&quot;, &quot;up_proj&quot;, &quot;down_proj&quot;],
    lora_alpha = 16,
    lora_dropout = 0,
    bias = &quot;none&quot;,
    use_gradient_checkpointing = True,
    random_state = 42,
    use_rslora = False,
    use_dora = False,
    loftq_config = None,
)
</code></pre>
<p>My issue is that when I prompt the model, it outputs &quot;prompt + generation&quot; rather than just outputing the generation. Here is where I prompt it:</p>
<pre><code>inputs = tokenizer(prompts[:2], return_tensors = &quot;pt&quot;, padding=True).to(&quot;cuda&quot;)
outputs = model.generate(**inputs, max_new_tokens = 256, use_cache = True)
predictions = tokenizer.batch_decode(outputs, skip_special_tokens = True)
print(predictions[0])
</code></pre>
<p>E.g. if the input prompt i.e. <code>prompt[0]</code> is &quot;What color is grass?&quot; the output is like &quot;What color is grass? Green.&quot; but I want it to just be &quot;Green.&quot;.</p>
","nlp"
"128831","Implementing Data Isolation in an RAG System in GCP using any of the LLM models","2024-04-24 07:45:36","","0","57","<nlp><information-retrieval><llm><rag>","<p>I am currently working on developing a Retrieval Augmented Generation (RAG) system where User-1 and User-2 each have their unique set of documents. My goal is to create a system where User-1's queries only receive responses from their own documents without any interference from User-2's data, and vice versa. Maintaining data confidentiality and security is crucial for this project.
I am using langchain with LLM models to structure my data, and I am seeking advice on how to implement data isolation effectively to ensure that the private documents of one user are protected from another. I have tried the following approaches:
Using separate directories for different users to store their documents in GCP Cloud storage.
Planning to use separate collections in vector DB for each user to ensure data isolation.</p>
<p>However, both methods have their drawbacks in terms of scalability and performance.
I would appreciate any suggestions or recommendations from the community on how to structure data isolation in this scenario. Some questions I have are:
Are there any best practices for data isolation?
What are some efficient ways to maintain data security while ensuring good performance in a RAG based architecture system?
Should I consider using other Python libraries or tools to achieve better data isolation?
Any help or advice would be greatly appreciated. Thank you all in advance!</p>
","nlp"
"128829","Weird behaviour when using RobERTA for text classification","2024-04-24 06:09:59","","3","51","<classification><nlp><bert><text-classification>","<p>I have a dataset with around 70 classes and the dataset is largely balanced ~150 samples per class. I am finetuning RoBERTA-base for 4 epochs with a <code>{lr =5e-5, wd = 0.01, batch_size=32}</code>, so fairly standard hyperparameters.</p>
<p>Two of the classes in the dataset are &quot;increaseCreditLimit&quot; and &quot;decreaseCreditLimit&quot;, and the data in them is quite obviously different. But when I test the model on very straightforward utterances like &quot;increase my credit limit&quot;, &quot;increase my credit card limit&quot;, etc, the prediction is always &quot;decreaseCreditLimit&quot;.</p>
<p>A finetuned distilBERT gets all of them correct, so I know that there is nothing amiss in the dataset or the preprocessing. Can someone please shed some light on what could be wrong with the finetuning of the RoBERTa model?</p>
<p>If we look at the SHAP values, it seems that the RoBERTA tokenizer breaks the word &quot;increase&quot; into &quot;incre&quot; and &quot;ase&quot;. But the distilBERT tokenizer doesn't do so and the attribution for the word &quot;increase&quot; is very significant.</p>
<p><strong>RoBERTA SHAP values (Output 14 corresponds to the &quot;increaseCreditLimit&quot; class and Output 57 corresponds to &quot;decreaseCreditLimit&quot;</strong></p>
<p><a href=""https://i.sstatic.net/mLhUcVvD.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/mLhUcVvD.png"" alt=""RoBERTA SHAP values (Output 14 corresponds to the &quot;increaseCreditLimit&quot; class and Output 57 corresponds to &quot;decreaseCreditLimit&quot;)"" /></a></p>
<p><strong>DistilBERT SHAP values</strong></p>
<p><a href=""https://i.sstatic.net/3KbxtNQl.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/3KbxtNQl.png"" alt=""DistilBERT SHAP values"" /></a></p>
<p>Train val loss per epoch
<a href=""https://i.sstatic.net/8Emm92TK.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/8Emm92TK.png"" alt=""enter image description here"" /></a></p>
","nlp"
"128802","How to choose a loss function and how to calculate the loss for Text Generation in Generative AI?","2024-04-22 13:03:52","","1","148","<nlp><loss-function><text-generation>","<p>For the classification problems, what loss functions can I choose ?</p>
<p>For the translation problem how do I decide whether the translation is good and how to choose a loss function?</p>
<p>And what about the question and answer problem ?</p>
","nlp"
"128768","Using UMAP on text data (euclidean distance on jaccard distance matrix)","2024-04-19 12:33:58","","1","97","<nlp><clustering><dimensionality-reduction><k-nn><umap>","<p>I am checking the capabilities of the UMAP dimensionality reduction algorithm, I am not sure whether the approach I am using is valid and does not violate the rules/limitations of this algorithm.</p>
<p>Purpose: visualization (and subsequent grouping) of articles in 3D space based on their thematic connections (words) in the titles/text of these articles.</p>
<p>Steps taken:</p>
<ol>
<li>text cleaning ; normalization; stopwords; stemming; etc.</li>
<li>1 hot-encoding;</li>
<li>construction of the Jaccard distance matrix (based on 1h-e data);</li>
<li>UMAP (input matrix from point 3) (umap using Euclidean distance);</li>
<li>Visualization; grouping; further analysis. (not included)</li>
</ol>
<p>If I understand the UMAP documentation correctly, it also allows the direct use of the Jaccard distance matrix without using the Euclidean distance, but these results, in my opinion (pictorial assessment of the results - proximity of points of related titles), do not separate groups of units as clearly as is visible in the case of using jaccard-&gt;euclides .</p>
<p>This raises my question: whether such use will not violate the theoretical assumptions of this algorithm and can be a correctly applied approach, or should I not use it in this way? I've included the R code, charts, and comparison tables below.</p>
<p>I will be grateful for a detailed statement and also for other tips to build the most appropriate solution.
Thanks!</p>
<p>code:</p>
<pre class=""lang-r prettyprint-override""><code>    library(tidytext)
    library(tidyverse)
    library(tm)
    library(reshape2) 
    library(plotly) 
    library(caret) 
    library(prabclus) 
    library(SnowballC) 
    
    extract &lt;- data.frame(
      title = c(
        &quot;Scheana Shay Unveils Tom Schwartz’s Vegas Kiss on ‘Vanderpump Rules’&quot;,
        &quot;Vanessa Marano: In-Depth Look at the ‘Switched at Birth’ Star&quot;,
        &quot;What Lies Ahead in Stranger Things Season 5 for Fans&quot;,
        &quot;Ariana Madix Acquires $1.6M LA Residence Post Split with Tom Sandoval&quot;,
        &quot;Kyle and Claire’s Romance Encounters Hurdles in The Young and the Restless&quot;,
        &quot;Twists and Turns in The Bold and the Beautiful Early April Installments&quot;,
        &quot;Don Mancini Set to Debut New Film in the Chucky Franchise&quot;,
        &quot;Steve Martin Contemplates the Fleeting Nature of Fame and Comedy&quot;,
        &quot;Remembering Daytime TV Icon Jennifer Leak Who Passed Away at 76&quot;,
        &quot;Aisha Hinds Delves into Hen’s Emotional Journey on ‘9-1-1’&quot;,
        &quot;6 Hidden Gems: TV Shows Deserving More Recognition&quot;,
        &quot;The Golden Bachelor’s Gerry Turner and Theresa Nist: The Untangling of a Marriage&quot;,
        &quot;Avantika’s ‘Rapunzel’ Role Sparks Controversy: A Perspective&quot;,
        &quot;Why Heroes Deserves a Revival: A Retrospective&quot;,
        &quot;Liza Lapira: Tracing Her Path to Stardom&quot;,
        &quot;Checking In on the Cast of Better Call Saul: Where Are They Now?&quot;,
        &quot;HBO’s ‘Euphoria’ Season 3 Production Delayed Amid Cast Ventures&quot;,
        &quot;7 Big Screen Stars Who Made Stops on General Hospital&quot;,
        &quot;Ranking Every Robert De Niro Mobster Role&quot;,
        &quot;Anthony Gonzalez: Inside the Life of the Award-Winning Coco Actor&quot;,
        &quot;Oscar Nominee Matt Ogens Presents ‘Madu’ at AFI Silver Theatre Prior to Disney+ Debut&quot;,
        &quot;Netflix’s ‘Vikings: Valhalla’ Captivates Audiences with Epic Tale&quot;,
        &quot;Exploring Awkwafina’s Standout Voice Performances in Film&quot;,
        &quot;Meet the Cast of Palm Royale: Apple TV+’s Period Drama&quot;,
        &quot;GTA 6: Release Timeline and Teaser Tease New Adventures&quot;,
        &quot;Speculations Surrounding Far Cry 7’s Arrival&quot;,
        &quot;Legal Woes for Palworld: Lawsuits Looming?&quot;,
        &quot;Christian Bale’s Transformation for Maggie Gyllenhaal’s ‘The Bride’&quot;,
        &quot;‘Shogun’ Creators Detail Authentic Recreation of Feudal Japan, Tease Season 2&quot;,
        &quot;Lauren and Eric Reflect on Sheila’s Complex Past on Bold &amp; Beautiful&quot;,
        &quot;Warner Bros. Reveals Plans for ‘Matrix 5’ with Fresh Directorial Vision&quot;,
        &quot;Tokyo Vice Creator Teases Season 2 Climax, Hints at Season 3&quot;,
        &quot;Stephen Colbert Delivers Emotional Tribute to Late Staffer on ‘Late Show’&quot;,
        &quot;The Circle Season 6 Premiere Date and AI Twist on Netflix&quot;,
        &quot;Rebel Wilson Accuses Sacha Baron Cohen of Humiliating Her in ‘The Brothers Grimsby’&quot;,
        &quot;Craig Conover Shares Optimism About His Long-Distance Relationship With Paige DeSorbo&quot;,
        &quot;Will Leo Reveal Jude’s True Parentage to Nicole on Days of Our Lives?&quot;,
        &quot;Paramount Confirms Development of Top Gun 3&quot;,
        &quot;Stream 7 New Titles on Netflix, Disney Plus, Prime Video, and More This Week&quot;,
        &quot;Are We Getting Ted Lasso Season 4?&quot;,
        &quot;7 Celebs Who Avoided Jail Time for Major Offenses&quot;,
        &quot;Robbie Amell: Meet the ‘Upload’ Star and Sci-Fi Actor&quot;,
        &quot;Joel Edgerton: A Journey Through His Television Roles&quot;,
        &quot;Is Scream 7 Still in the Works?&quot;,
        &quot;John Krasinski: A Journey Through His Directorial Career&quot;,
        &quot;What Lies Ahead for the Cars Franchise?&quot;,
        &quot;Modern Family: The Making of a Comedy Sensation&quot;,
        &quot;6 Facts To Know About Amy Price-Francis&quot;,
        &quot;Remembering Susan Flannery: The Legacy of a Soap Opera Icon&quot;,
        &quot;Is There Hope for Shrek 5?&quot;,
        &quot;Where to Stream Top Boy: Summerhouse Season 5&quot;,
        &quot;Gillian Anderson's Views on an X Files Revival&quot;,
        &quot;Days of Our Lives Mourns the Loss of Bill Hayes&quot;,
        &quot;Remembering General Hospital Alum Robyn Bernard&quot;,
        &quot;Remembering O.J. Simpson: The Life of a Former Football Star&quot;,
        &quot;10 Celebs Still Without a Star on the Hollywood Walk of Fame&quot;,
        &quot;Unveiling Phyllis Smith: The Voice of Sadness From Inside Out&quot;,
        &quot;Thomas Mann: Exploring His Career in Film&quot;,
        &quot;Remembering All My Children Star Alec Musser&quot;,
        &quot;Are We Getting Diablo 5 Soon?&quot;,
        &quot;Exploring the Legacy of Lorenzo Lamas&quot;,
        &quot;The Journey of C. Thomas Howell: The Outsiders Star&quot;,
        &quot;Indiana Jones and the Financial Setback&quot;,
        &quot;Melissa Gorga Speaks Out on ‘RHONJ’ Photo Incident&quot;,
        &quot;David Cronenberg’s Latest Film Sets Record for Length&quot;,
        &quot;Godzilla: A 70-Year Reign as Cinema’s Iconic Monster&quot;,
        &quot;General Hospital Teasers: April 8 to April 12, 2024&quot;,
        &quot;Ty Pennington: From Reality TV to Design Guru&quot;,
        &quot;John Malkovich’s Biggest Box Office Hits&quot;,
        &quot;Sarah Beeny: Redefining Home Renovation&quot;,
        &quot;Are We Getting a White Collar Comeback?&quot;,
        &quot;Breaking Down the Cast of Abigail: The Horror Flick&quot;,
        &quot;Amazon’s Fallout Series: Insights into Season 2&quot;,
        &quot;The Fate of Red Dead Redemption 3&quot;,
        &quot;Far Cry 7: What Fans Need to Know&quot;,
        &quot;Rudy Mancuso: Familiar Faces in ‘Música’&quot;,
        &quot;Camila Mendes: A Career Retrospective&quot;,
        &quot;Starfield: Coming to PS5?&quot;,
        &quot;Final Fantasy 16: Expanding to Other Platforms?&quot;,
        &quot;Palworld: Bridging Platforms?&quot;,
        &quot;Deciphering Sheila Carter’s Bold &amp; Beautiful Disappearance&quot;,
        &quot;NCIS Franchise Reaches Milestone with 1000th Episode&quot;,
        &quot;Exclusive Friends Script Auctioned for Charity&quot;,
        &quot;Remembering Larry David’s ‘Curb Your Enthusiasm’&quot;,
        &quot;Anna Devane’s Investigative Journey on General Hospital&quot;,
        &quot;Andrew Scott’s Performance in ‘Ripley’&quot;,
        &quot;Exploring Walton Goggins’ Roles in Prime Video’s Fallout Series&quot;,
        &quot;Vanna White Joins Ryan Seacrest on Talent Shows&quot;,
        &quot;The Young and the Restless: April 8-12 Drama Preview&quot;,
        &quot;3 Body Problem: A Guide to the Cast and Characters&quot;,
        &quot;Bridgerton Season 3: What’s in Store?&quot;,
        &quot;Henry Cavill’s Next Move After The Ministry of Ungentlemanly Warfare&quot;,
        &quot;Natalie Portman: Beyond Star Wars&quot;,
        &quot;The Truth Behind Eric Cartman’s South Park Father&quot;,
        &quot;Matt Damon’s Journey with ‘The Talented Mr. Ripley’&quot;,
        &quot;Paige Davis’ Net Worth: How Much Does She Earn?&quot;,
        &quot;Married at First Sight: Inside Jono McCullough and Ellie Dix’s Relationship&quot;,
        &quot;Kristen Wiig’s Aunt Linda Roasts Barbie on SNL’s Weekend Update&quot;,
        &quot;Larry David Faces Trial in Curb Your Enthusiasm Finale&quot;,
        &quot;Kristen Wiig’s Most Memorable SNL Sketches&quot;
      ),
      ID = 1:100
    )
    
    # Display the data
    print(extract)
    
</code></pre>
<pre class=""lang-r prettyprint-override""><code>    # Titles text - TITLES
    title_tokens = extract %&gt;% as.tibble() %&gt;%
      # lowercase
      mutate(title = tolower(title)) %&gt;%
      # tokenize
      unnest_tokens(word, title) %&gt;%
      #remove punct
      mutate(word = str_replace_all(word, &quot;[[:punct:]]&quot;, &quot;&quot;)) %&gt;%
      # remove numbers 
      filter(!grepl(&quot;\\d+&quot;, word)) %&gt;%
      # remove stop words
      anti_join(stop_words) %&gt;%
      # stemming
      mutate(word = SnowballC::wordStem(word, language = 'porter'))
    
    words = title_tokens %&gt;%
      group_by(word) %&gt;%
      summarise(count = n()) %&gt;%
      arrange(desc(count)) %&gt;%
      filter(count!=1) %&gt;%
      dplyr::select(word) %&gt;%
      unlist() %&gt;%
      as.vector()
    
    title_tokens = title_tokens %&gt;% unique() # remove duplicate words. &quot;season, late, &quot;april&quot;
    
    encoded_title_tokens = title_tokens # %&gt;% filter(word %in% words)
    dummy_matrix &lt;- predict(dummyVars(&quot;~ word&quot;, data = encoded_title_tokens, sep = &quot;_&quot;), newdata = encoded_title_tokens)
    dummy_matrix &lt;- cbind(id = encoded_title_tokens$ID, as.data.frame(dummy_matrix))
    colnames(dummy_matrix) &lt;- gsub(&quot;word_&quot;, &quot;&quot;, colnames(dummy_matrix))
    colnames(dummy_matrix) &lt;- gsub(&quot;word&quot;, &quot;&quot;, colnames(dummy_matrix))
    encoded_title_tokens = dummy_matrix %&gt;%
      as.data.frame() %&gt;%
      group_by(id) %&gt;%
      summarise(across(everything(), sum)) %&gt;%
      as.tibble()
    
    # Jacc dist
    jaccard_matrix_articles = as.tibble(1- prabclus::jaccard(encoded_title_tokens %&gt;%
                             dplyr::select(-id)%&gt;%
                             t()  %&gt;%
                             as.matrix())
    )
    
    
    library(umap)
    umap::umap.defaults
    umap_jacc = umap::umap(jaccard_matrix_articles,
                           n_components = 3,
                           metric = &quot;euclidean&quot;,
                           n_neighbors = 15,
                           min_dist = 0.001, 
                           n_epochs = 200)
    
    
    t &lt;- list(
      family = &quot;sans serif&quot;,
      size = 10,
      color = plotly::toRGB(&quot;grey20&quot;))
    
    umap_jacc<span class=""math-container"">$layout %&gt;% as.tibble() %&gt;%plotly::plot_ly(x = ~V1, y = ~V2, z = ~V3,
                                                    text = extract[encoded_title_tokens$</span>id,]$ID,
                                                        mode = 'markers',
                                                        hoverinfo = 'text') %&gt;%
      plotly::add_text(textfont = t, textposition = &quot;top&quot;) %&gt;%
      plotly::add_markers(size = 4) %&gt;%
      plotly::layout(scene = list(xaxis = list(title = &quot;UMAP component 1&quot;),
                                  yaxis = list(title = &quot;UMAP component 2&quot;),
                                  zaxis = list(title = &quot;UMAP component 3&quot;)),
                     showlegend = FALSE)
    
    # v clust 1 
    extract %&gt;% filter(ID %in% c(56,2,18,93,42,62,59,55)) %&gt;% kableExtra::kbl(caption = '&lt;center&gt;Group1&lt;/center&gt;', align = 'l') %&gt;%kableExtra::kable_classic() %&gt;% kableExtra::kable_styling()
    # v clust 2
    extract %&gt;% filter(ID %in% c(17,51,3,40,34,91,32,29,73,25)) %&gt;% kableExtra::kbl(caption = '&lt;center&gt;Group2&lt;/center&gt;', align = 'l') %&gt;%kableExtra::kable_classic() %&gt;% kableExtra::kable_styling()
    
    # UMAP AS DISTANCE INPUT
    umap::umap.defaults
    umap_jacc = umap::umap(jaccard_matrix_articles,
                           n_components = 3,
                           input= &quot;dist&quot;,
                           n_neighbors = 15,
                           min_dist = 0.001, 
                           n_epochs = 200)
    # Define colors for the three groups
    red_group &lt;- c(56, 2, 18, 93, 42, 62, 59, 55)
    green_group &lt;- c(17, 51, 3, 40, 34, 91, 32, 29, 73, 25)
    # Convert umap_jacc<span class=""math-container"">$layout to tibble and add ID column
umap_tibble &lt;- umap_jacc$</span>layout %&gt;%
      as_tibble() %&gt;%
      mutate(ID = 1:nrow(.))
    # Create color column based on IDs
    umap_tibble &lt;- umap_tibble %&gt;%
      mutate(color = case_when(
        ID %in% red_group ~ &quot;red&quot;,
        ID %in% green_group ~ &quot;green&quot;,
        TRUE ~ &quot;blue&quot;
      )) %&gt;%
      mutate(color = factor(color))
    # Plotly plot
    plot_ly(umap_tibble, x = ~V1, y = ~V2, z = ~V3,
            text = ~ID,  # Display ID as text
            mode = 'markers',
            hoverinfo = 'text') %&gt;%
      add_markers(marker = list(color = ~color), size = 4, showlegend = F) %&gt;%
      add_text(textfont = t, textposition = &quot;top&quot;) %&gt;%
      layout(scene = list(xaxis = list(title = &quot;UMAP component 1&quot;),
                          yaxis = list(title = &quot;UMAP component 2&quot;),
                          zaxis = list(title = &quot;UMAP component 3&quot;)),
             showlegend = TRUE)
</code></pre>
<p>jaccard -&gt; euclidean<br />
<a href=""https://i.sstatic.net/whp8o.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/whp8o.png"" alt=""jaccard -&gt; euclidean "" /></a><br />
jaccard -&gt; euclidean visible groups<br />
<a href=""https://i.sstatic.net/PggPu.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/PggPu.png"" alt=""jaccard -&gt; euclidean groups"" /></a><br />
jaccard-&gt; euclidean table groups<br />
<a href=""https://i.sstatic.net/ISTTR.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ISTTR.png"" alt=""enter image description here"" /></a><br />
jaccard as input intput = 'dist'<br />
green and red points are respective groups1 and 2 from jaccard -&gt; euclidean
<a href=""https://i.sstatic.net/86Wk2.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/86Wk2.png"" alt=""enter image description here"" /></a></p>
","nlp"
"128734","Train a LLM to learn the entropy of the use case","2024-04-16 19:25:57","","1","24","<nlp><training><information-retrieval><llm><finetuning>","<p>I want to train a LLM (prefered Llama-2-13b) to learn the entropy of german texts - to be specific sports news. I use perplexity as training metric and want to check the training success after the training.
I want to use the fine-tuned model for RAG and I hope, that the fine-tuned model understand the query and context better to give a better answer compared to the orignal model
How can an experiment look like to compare the fine-tuned Llama-2-13b related to the original Llama-2-13b preferably automated for this use case?</p>
","nlp"
"128703","Does Fine Tuning with Custom Label Build Upon the Capability of Zero Shot Classification or Does It Train from Scratch?","2024-04-15 09:30:38","","0","41","<nlp><bert><text-classification><huggingface><zero-shot-learning>","<p>The task is to classify email text bodies into exclusive categories like feedback, complaint etc. I have a labelled dataset available having about 350 samples.</p>
<p>I have tried the <code>facebook/bart-large-mnli</code> zero shot classification model where I passed the class names as possible label. It is already giving a decent performance.</p>
<p>Now, if I want to improve by using the existing labelled dataset and some model like <code>distilbert-base-uncased</code>, then will I lose the capability offered by the zero shot model altogether, and the new model will be trained entirely based on the labelled data?</p>
<p>I am afraid to go down the route because the number of labelled samples is so small, I feel it will fail to update the huge models having more weights than number of samples (we know the larger the model, the more samples you need).</p>
<p>So how do you guys address this concern, and how best to use the capabilities of the zero shot model on huggingface, while also using the labelled sample somehow?</p>
<p>I feel if I could just nudge the zero shot model a bit with the training samples, that would be the best possibility, but how to achieve it with huggingface?</p>
","nlp"
"128696","How do you train a seq2seq model on sequences longer than its sequence length?","2024-04-14 18:22:44","128704","1","38","<nlp><training><sequence-to-sequence>","<p>I was reading the GPT original paper <a href=""https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf"" rel=""nofollow noreferrer"">here</a> and in section 3.5 they mention evaluating on the CoQA dataset. I checked GPT has a sequence length of 512, yet most of the sequences in the CoQA are a few thousand tokens long. So how would they evaluate on this?</p>
","nlp"
"128674","keyword extraction and link to multiple sources","2024-04-12 14:05:24","","0","11","<nlp><information-extraction>","<p>I have to perform a tech watch over a very rich subject which spans from software to deep physical issues and also, from business news to scientific articles.</p>
<p>Usually, I would have first retrieved the most encountered keywords of the subject and selected articles that matched them. But it fails to some extent as it is quite a rich subject and I may miss some important news and/or articles.</p>
<p>So, I try to have a different approach with the help of keyword extractors. I found 6 of them :</p>
<ul>
<li>spaCy</li>
<li>YAKE</li>
<li>Rake-Nltk</li>
<li>Gensim</li>
<li>KeyBERT</li>
<li>TextRank</li>
</ul>
<p>They all work with some text as input and a list of keywords is their outcome. However, this means that I would have to merge all articles, so that the information about in which article each keyword occur would be lost.</p>
<p>Is there a way to extract some keywords from many articles and being able to link keywords with each article where it appears ?</p>
","nlp"
"128661","Problem when merge two probability distribution [Pointer Network]","2024-04-12 05:54:43","","0","11","<nlp><pytorch><loss-function><cross-entropy>","<p>I'm trying to re-implement Pointer Gen Net from <a href=""https://aclanthology.org/P17-1099.pdf"" rel=""nofollow noreferrer"">this paper</a>. Ya but you don't really need to read the paper.</p>
<p>To sum up briefly,</p>
<p>I have a vector probability distribution over vocabulary called p_vocab. This p_vocab is formed by <code>softmax</code> function</p>
<p>And another vector probability distribution over source sentence called p_src. This p_src also is formed by <code>softmax</code> function</p>
<p>Then I'll combined p_vocab and p_src by addition operator</p>
<p><code>p_final = p_vocab + p_src</code> (each element &gt;=0  because both used softmax function)</p>
<p>Okay, then in the paper they wrote:</p>
<pre><code>loss = - log(P(w))
</code></pre>
<p>I agree that the model will try to increase the prob of target token and reduce the prob of other token in vocab via the procedure reduce loss so we multiply with minus one. Because the prob very small due to large vocab then the log make it more significant in loss.</p>
<p>But I get confuse now. Is it just a way to express that they're using cross entropy loss? As the formula of cross entropy loss is : -log(exp(P)/sum..)</p>
<p>I tried not to apply log in the p_final and pass the p_final in cross_entropy_loss. It works though loss reduce very lightly. Normally I use log_softmax instead of softmax but I have two probs vector here, then I tried to apply log function to p_final but I got <code>nan</code> in the loss, I guess because there are some zero in the <code>p_final</code> then I add a really small epsilon like 1e-10 to p_final to avoid zero case before calculate log</p>
<p>so</p>
<pre><code>p_final = torch.log(p_final + 1e-10)

loss = torch.nn.functional.cross_entropy_loss(p_final, labels)
</code></pre>
<p>Is my way correct?</p>
","nlp"
"128658","Public Email Classification Dataset but not Spam vs Ham","2024-04-12 04:46:27","","1","31","<classification><nlp><dataset><huggingface>","<h5>Context</h5>
<p>Working to deliver a POC on automated email classification (in customer service context) to tag emails as related to <em>feedback</em>, <em>complain</em>, <em>lost and found</em> etc. The tags are not entirely exclusive, but the goal of the model is to assign a weight to each of these tags for a specific email. Like based on the email body, it is 20% related to feedback, 70% complaint and 10% lost and found.</p>
<p>Now, ideally, I would start with my client company's real email inbox. But it is not a mature data company (for systematic consumption of their inbox data), and there are privacy issues yet to be resolved.</p>
<h5>Question 1</h5>
<p>Is there any publicly available email/feedback related dataset (with plain texts, and other optional features like timestamp etc.) that can be used to show some quick POC? Most email data I see are obviously spam-ham type, not in the domain I want.</p>
<h5>Question 2</h5>
<p>Any idea which model (best if pre-trained with well documented interface, like from HuggingFace) will be suitable for the task, with some scope for fine-tuning? I am personally more a software engineer with ML experience, not an NLP expert, but picking up as I go.</p>
","nlp"
"128657","Finding accuracy of model that uses different labels than ground truth","2024-04-12 00:18:53","","0","24","<nlp><confusion-matrix><semantic-similarity>","<p>I have an nlp model that has ground truth labels and predicted labels (that belong to different group of classes). For example, the ground truth labels are [art, computer science, history] and predicted labels are [drawing, engineering, philosophy, mathematics]. To find the accuracy of my predictions, I use a cosine similarity of ground truth with predictions. I now want to quantify the quality of my predictions -- what I plan to do is find the cosine similarity between ground truth and a non-predicted label and then classify it as false negative and use something similar for false positive. Is it a good approach?</p>
","nlp"
"128654","Reducing emails token count preprocessing for Large Email Datasets - Feeding LLMs","2024-04-11 19:41:27","128660","1","66","<nlp><data-cleaning><preprocessing><bert><llm>","<p>I have a large email dataset in .txt format and want to feed LLMs (like Gemini and ChatGPT) to provide answers based on email content.</p>
<p>The token count for my email data is very high (~1M for 1K emails), exceeding LLM token limits. Even after preprocessing the basic headers, there are still a lot of tokens with low information like email threads on the body of the email.</p>
<p>I'm considering the following models/approaches:</p>
<ol>
<li>Signature removal: This <a href=""https://huggingface.co/spaces/Jean-Baptiste/email_parser"" rel=""nofollow noreferrer"">hugging face bert project</a> does a good job, even if the model is trained in french.</li>
<li>Quoted Text Identification (for text inside the list of emails)</li>
<li>Stop Word Removal</li>
<li>BERT-Based Transformers for Focused Preprocessing</li>
<li>Summarization Transformers</li>
<li>Extractive Summarization</li>
</ol>
<p>The challenge is doing so in mixed language emails (parts in french, english and portuguese). Since this is a very common (emails or any text) is there any hugging face project to address most of these points?</p>
","nlp"
"128634","How do I prompt GPT-4 to look at a PDF in Jupyter Notebook?","2024-04-11 04:16:55","128635","4","1425","<nlp><jupyter><gpt><llm><api>","<p>I am a beginner. I purchased tokens to use GPT-4 and finally figured out how to import the GPT-4 model into my Jupyter Notebook.</p>
<pre><code>%env OPENAI_API_KEY= (my key goes here)

!pip install --upgrade openai wandb

from openai import OpenAI

LLM = OpenAI()

response = LLM.chat.completions.create(

model='gpt-4',

messages=[{'role': 'user', 'content': 'What is 1+1?'}],)

response
</code></pre>
<p>Now, I would like to upload a PDF document and prompt GPT-4 to extract important headings from the document. Can you provide the code for how to do this? Specifically the part where you upload the PDF, and prompt GPT-4 with an example instruction.</p>
<p>If I'm not mistaken, I don't need to process the PDF into text format because GPT-4 can work directly with PDFs? That's why I wanted to use GPT-4, because when I was converting the PDF to text, it was very messy due to tables, headers, footers, etc.</p>
","nlp"
"128629","Multilabel Classification - Flat Binary Classifiers vs Hierarchical Binary Classifiers","2024-04-10 15:54:20","","0","31","<machine-learning><nlp><multilabel-classification><text-classification><binary-classification>","<p>Was researching on multi label classification to solve the problem of tagging news articles with topics and countries, where tags follow the syntax &lt;topic&gt;-&lt;country&gt;, and would like to weigh multiple binary classifier options (flat vs hierarchical classification).</p>
<p>Approach 1 (flat classification) builds 1 model per tag. The model learns the dependence within 1 tag (i.e. between topic and country), but not the dependence between tags.</p>
<p>Approach 2 (hierarchical classification) builds topic model (parent) and subsequently country models (children). The benefit of this approach is that it accounts for hierarchical information so dependence between tags is retained, but at the cost of error from parent propagating to the child, and complexities in measuring performance of hierarchical classifiers. It also assumes that the concept of topic remains the same for various countries.</p>
<p>Wanted to ask what other factors that would lead one to choose one approach over another, and why?</p>
","nlp"
"128557","Fine tuning a model for question-answering task without context on the dataset","2024-04-04 14:14:03","","0","44","<nlp><finetuning>","<p>I have a small dataset (contains around 2.5k question answer pairs) which I would like to train a <a href=""https://huggingface.co/google-t5/t5-base"" rel=""nofollow noreferrer"">T-5 base model</a> on.</p>
<p>The code examples that I have came across fine-tune with a dataset which has <code>question-answer-context</code>, where the context has the answer and the model has to extract the answer from that context.</p>
<p>I'd like to fine-tune a LLM like T-5 on a <code>question-answer</code> dataset from a very narrow specific domain so that it can answer question from that domain.</p>
<p>How can I fine-tune T-5 base model just on <code>question-answer</code> data?</p>
<p>How can I ensure the fine-tuned model performs better than baseline?</p>
<p>Additionally, I have articles from my domain, how could i use them as well?</p>
","nlp"
"128494","Subsequence classification","2024-03-29 13:15:33","","0","7","<nlp><bert>","<p>Given multiple paragraphs, is it possible to classify an entire paragraph while taking into account the surrounding paragraphs?</p>
<pre><code>Paragraph1

Paragraph2

Paragraph3
</code></pre>
<p>I am thinking of average pooling the token embeddings from a specific paragraph. This seems uncommon as I wasn't able to find examples of this online.</p>
<p>It is not quite the same as NER, since we know where the paragraph begins and ends. If using sequence classification on the paragraph, it wouldn't take into account the surrounding text.</p>
","nlp"
"128492","How is a causal language model correctly fine-tuned?","2024-03-29 11:55:58","","0","39","<nlp><language-model><finetuning>","<p>I want to fine-tune an SLM like Phi-2 through the huggingface API. I am in doubt how to achieve that, because I see two ways to do that and I am wondering which is the correct way.</p>
<p>The task is just a sequence to sequence mapping. For the sake of simplicity let's just assume we do summarization.</p>
<p>There seem to be two possibilities to achieve this, and here I would like some clarification, if that is true and/or if there is a preferred way to do this.</p>
<p><strong>Possibility 1:</strong></p>
<p>Now, to get a head start I was looking around and trying to find scripts and tutorials. I found <a href=""https://medium.com/thedeephub/optimizing-phi-2-a-deep-dive-into-fine-tuning-small-language-models-9d545ac90a99"" rel=""nofollow noreferrer"">this</a> one here, but I am genuinely confused, because it seems to me the author creates one single prompt with input <em>and</em> target sequence formatted in one place. There is no such thing as a label sequence that is expected to predicted but it's much more trained in a self-supervised or auto-encoder fashion. In code he writes the collate function that creates the dataset like this:</p>
<pre><code>def collate_and_tokenize(examples):

    question = examples[&quot;question&quot;][0].replace('&quot;', r'\&quot;')
    answer = examples[&quot;answer&quot;][0].replace('&quot;', r'\&quot;')
    #unpacking the list of references and creating one string for reference
    references = '\n'.join([f&quot;[{index + 1}] {string}&quot; for index, string in enumerate(examples[&quot;references&quot;][0])])

    #Merging into one prompt for tokenization and training
    prompt = f&quot;&quot;&quot;###System:
Read the references provided and answer the corresponding question.
###References:
{references}
###Question:
{question}
###Answer:
{answer}&quot;&quot;&quot;

    encoded = tokenizer(
        prompt,
        return_tensors=&quot;np&quot;,
        padding=&quot;max_length&quot;,
        truncation=True,
        max_length=2048,
    )

    encoded[&quot;labels&quot;] = encoded[&quot;input_ids&quot;]
    return encoded
</code></pre>
<p>During inference, logically, he then proceeds and feeds something like this:</p>
<pre><code>new_prompt = &quot;&quot;&quot;###System: 
Read the references provided and answer the corresponding question.
###References:
[1] For most people, the act of reading is a reward in itself. However, studies show that reading books also has benefits that range from a longer life to career success. If you’re looking for reasons to pick up a book, read on for seven science-backed reasons why reading is good for your health, relationships and happiness.
[2] As per a study, one of the prime benefits of reading books is slowing down mental disorders such as Alzheimer’s and Dementia  It happens since reading stimulates the brain and keeps it active, which allows it to retain its power and capacity.
[3] Another one of the benefits of reading books is that they can improve our ability to empathize with others. And empathy has many benefits – it can reduce stress, improve our relationships, and inform our moral compasses.
[4] Here are 10 benefits of reading that illustrate the importance of reading books. When you read every day you:
[5] Why is reading good for you? Reading is good for you because it improves your focus, memory, empathy, and communication skills. It can reduce stress, improve your mental health, and help you live longer. Reading also allows you to learn new things to help you succeed in your work and relationships.
###Question:
Why is reading books widely considered to be beneficial?
###Answer:
&quot;&quot;&quot;
</code></pre>
<p>where the model is asked to just generate answer. I get that, it makes sense, but I was expecting the following.</p>
<p><strong>Possibility 2:</strong></p>
<p>In the above mentioned tutorial the author just sets the labels equal to the <code>input_ids</code>. I would have done differently, such that I encode the target sequence and have that be my labels. This is how I feel like OpenAI's API works, but who knows what they are doing in the background ...</p>
<hr />
<p>My questions all go in the direction of: what is correct? Is there a <em>correct</em> way? What is prefered? Do both approaches achieve the same thing?</p>
<p>Thank you. Please ask clarifying questions if I wasn't clear somewhere.</p>
","nlp"
"128465","Which model and embedding to use for portuguese chat with docs","2024-03-27 09:24:42","","0","54","<nlp>","<p>I would like to have a model to read all my personal documents, meeting notes and things like that, all text based, and then be able to ask questions like: what was decided about the feature x? what was proposed about y? I have tried some examples that I have seen online and they seem to do what I want. The thing is my documents are not in english. I tried some models like maritaca-ai/sabia-7b, llama-2, mistral and some embeddings like sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2.</p>
<p>The results are not good at all. Is it the model? Or the embedding type I am using? Can somebody suggest some resource for me to learn more about how to solve this problem?</p>
","nlp"
"128448","Insights about W0rd2Vec","2024-03-26 06:25:39","","0","22","<deep-learning><nlp><text-mining><word-embeddings><transformer>","<p>As per my knowledge, Word2Vec is belongs to non-contextual embedding technique. this have only semantic relationship between words.</p>
<p>We can implement Word2Vec, either in CBoW or skip-gram model. but i confused with below statements:</p>
<ol>
<li>The CBOW model is designed to predict a target word based on its surrounding context words.</li>
<li>Unlike the Skip-gram model, which predicts context words given a target word, CBOW focuses on predicting the target word itself.</li>
</ol>
<p>since word2vec is non-contextual. but in CBoW, it is considering the context to find the target.
can you please give more insights about these two(CBoW, skip-gram).</p>
","nlp"
"128424","Is this how you would go about this NLP Project?","2024-03-24 05:52:56","","0","11","<nlp><text-classification><automatic-summarization><chatgpt><automation>","<p>What do you think of these steps? And where can I find help with this project?</p>
<p>I am in a business class and was assigned a data science problem. I was advised to seek out a coder at my school who can help, but that is not turning out well. Are there tutorials I can follow? Should I ask questions here step by step? Is there a freelance website where I can pay someone around $200 to do a very basic model? Is this even realistic for me to accomplish in 6 weeks?</p>
<p>This technical services company wants to automate their report writing. They want an algorithm that generates a rough draft report when you give it a set of documents.</p>
<p>From my research, I have found the following steps:</p>
<ol>
<li>Preprocess the documents that will be used to generate the report (remove headers/footers etc, tokenize text, handle data tables)</li>
<li>Extract whole paragraphs of text and copy them into the appropriate section of an empty &quot;Information Repository&quot; document. Some examples of sections are &quot;Site Description&quot;, &quot;Previous Investigations&quot;, &quot;Groundwater Sampling&quot;, etc.
To do this, you would create a mapping between the sections in the &quot;Information Repository&quot; and keywords that indicate each section's content. Train a model (a text classifier like GPT or BERT) to classify paragraphs from the reports into relevant sections of the &quot;Information Repository&quot;. Use supervised learning and provide labeled examples (each example is a paragraph along with its corresponding section label). With relevant sections now identified, extract the corresponding text from the documents and copy it into the appropriate sections of the &quot;Information Repository&quot;. Evaluate and improve the model.</li>
<li>Then part 3 I probably won't complete this semester, but it involves removing duplicate info from the Information Repository, and then extracting the relevant parts back into a new clean document which will be the rough draft (based on what type of report is being written, it will dictate what parts of the Information Repository is needed).</li>
</ol>
","nlp"
"128400","How is openAI embedding models trained?","2024-03-22 04:31:48","128401","1","865","<deep-learning><nlp><embeddings><gpt><artificial-intelligence>","<p>how it the embedding model trained? Are the embeddings simply extracted from chatGPT4 or are they trained differently from the beginning (pre-training stage)?</p>
","nlp"
"128379","Question about contextual embeddings?","2024-03-20 18:53:00","","0","22","<machine-learning><deep-learning><nlp>","<p>How do BERT and RoBERTa generate contextual embeddings? The articles I've read keep saying that transformer encoders work bidirectionally. Because of self-attention, they can look at every token, unlike RNN/LSTM, which can only process the previous hidden state. Is it true ? I'm not sure how BERT and RoBERTa accomplish that.</p>
","nlp"
"128377","Stream response from custom RASA actions to the chatbot","2024-03-20 11:18:11","","0","73","<machine-learning><nlp><gpt><chatbot><software-development>","<p>I am using RASA PRO with CALM.
I was thinking of using openai api within a custom action and stream the streaming response coming from openai to my chatbot. Openai is giving me streaming response and i am doing “dispatcher.utter_message(openai_res_chunk)” from actions, but the response is not getting streamed in chunks to my bot and is available only after last chunk from openai. I am also passing “stream”: True while sending request to rasa server, but nothing is working. How to achieve this?</p>
","nlp"
"128373","Best practises for creating datasets for the purpose of finetuning LLMs","2024-03-20 07:34:08","","1","43","<nlp><dataset><llm><reference-request>","<p>I am working on a problem for which no datasets exist. I have obtained several examples from this domain, and so far have been using them in Large Language Model (LLM) prompts(few shot learning) but I noticed results are not good and perhaps finetuning is the way to go. The expected output of LLM is to generate text and logical formulas.</p>
<p>I am looking for tutorials that include best practises for the following and more:</p>
<ol>
<li>How to decide which fields (columns) to include?</li>
<li>What types of data to include as input?</li>
</ol>
<p>Any pointers is extremely appreciated.</p>
","nlp"
"128363","Implementing Fuzzy Matching and NLP for Transaction Classification","2024-03-19 13:22:13","","0","20","<classification><nlp><data><ai><fuzzy-logic>","<p>I’m a trainee at a fintech startup, and I’m working on a project that involves classifying transactions using Natural Language Processing (NLP) and fuzzy matching techniques. The main goal is to categorize transactions based on merchant names, including the ability to recognize and match abbreviations to their full names. For example, if a transaction mentions ‘AMZ’, I want to use a fuzzy function to match it to ‘Amazon’ and classify it as a ‘shopping’ transaction.</p>
<p>I’m reaching out to the community for advice and insights on the following aspects:</p>
<ul>
<li>Approach: What are some effective methods or algorithms for implementing a fuzzy matching function that can accurately match abbreviations to full names in transaction classification?</li>
<li>NLP Integration: How can I integrate NLP techniques with the fuzzy matching function to improve the accuracy of transaction classification?</li>
<li>Tools and Technologies: Are there any recommended tools, libraries, or frameworks for implementing these techniques in a fintech environment?</li>
</ul>
<p>I’d greatly appreciate any guidance, recommendations, or examples you could share. Thank you in advance for your help!</p>
","nlp"
"128359","Do LSTM, GRU and Transformer models with less layers and units perform better than larger models when classifying short text sequences?","2024-03-19 09:35:47","","0","14","<classification><nlp><tensorflow><lstm><transformer>","<p>I am working with a Kaggle dataset with short Twitter messages as text input. I made a copy <a href=""https://www.kaggle.com/datasets/joachimrives/natural-language-processing-with-disaster-tweets"" rel=""nofollow noreferrer"">here</a>. When testing LSTMS, GRUs, bi-directional versions of the GRUs, and the Encoder layers of a Transformer model, the best models were shallow and had few units. The average text sequence length was 14-15 according to my code. Is it a good assumption that smaller models or models with fewer units usually do better when processing short text?</p>
<p>There are problems with my guess.</p>
<ol>
<li><p>The correlation between layer counts and unit counts vs. the length of meaningful words or tokens is not constant. For bi-directional LSTMs, the best models had one layer and 8 or 16 units. Other unit counts from 64 to 1 did not perform as well. Using the transformer encoder,1 layer with 2 heads and 4 inner units performed roughly as well as 6 heads and 8 inner dense units. For 1-D Convolutions, 32 and 64-filter convolutions did the best. I tested common multiples of 2 and some in-between values if the upper and lower limits performed well, e.g. 24 if 16 and 32 filters did well.</p>
</li>
<li><p>I did not sample a large range. I assumed that higher numbers of layers or units would make the model worse after reaching 128 filters for Conv-1D layers and 32 units for the LSTMs. This is because I did not have the computing power to test larger networks, not just that more units made performance worse. I also didn't give time to testing in-between values.</p>
</li>
<li><p>The performances of my models might depend on the data set, but they could also depend on the model architectures I set up, i.e. what layers came before or after. How do I tell if a trend might be generalizable to other text classification data sets vs. something specific to my model architecture?</p>
</li>
</ol>
<p>To summarize, I want to know how or what correlation there might be between model architectures or hyperparameters and the characteristics of the data set, like the sequence length and number of examples. If I test the same architectures on different data sets, would that help validate my guesses?</p>
<p>If you want, you can check the Notebook I <a href=""https://www.kaggle.com/code/joachimrives/layer-unit-count-binary-text-classification"" rel=""nofollow noreferrer"">linked</a>. Some of the models I tested have been turned into comments.</p>
","nlp"
"128346","Will hypermeters tuned on sampled dataset work for the whole dataset?","2024-03-18 12:53:15","","0","9","<nlp><bert><text-classification><hyperparameter><huggingface>","<p>I'm doing multi-label classification on text data using BERT model. Since the dataset is huge, around  50 thousand rows, I was thinking to use stratify sampling on dataset to reduce it to around 2-4 thousand to hyperparameter tune on.
I'm confused between trading off number of trails with size of sample set. Example: Would training 3000 rows with 5 trials will be better than training 1500 rows with 10 trials?
Moreover, thinking if I should drop epoch from tuning and focus on learning rate and weight decay.</p>
","nlp"
"128294","Commonly used metric in NLP literature to compare ranked weighted results with variable importance for top-k results","2024-03-14 11:50:23","","0","10","<nlp><metric><ranking>","<p>I have two different search engines that <strong>always return the same results but in different orders</strong>. The results consist of websites along with confidence scores, which range from 100 to 10,000. The maximum number of results is capped at 99. <strong>Assume the results of first search engine is ideal.</strong></p>
<p>For example, consider the following sixty results by two different search engines E1 and E2:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>E1 results ( site:score)</th>
<th>E2 results (site:score)</th>
</tr>
</thead>
<tbody>
<tr>
<td><span class=""math-container"">$w_1$</span> - 195</td>
<td><span class=""math-container"">$w_4$</span> -   199</td>
</tr>
<tr>
<td><span class=""math-container"">$w_2$</span> - 192</td>
<td><span class=""math-container"">$w_{24}$</span> -   192</td>
</tr>
<tr>
<td><span class=""math-container"">$w_3$</span> - 190</td>
<td><span class=""math-container"">$w_{36}$</span> -   189</td>
</tr>
<tr>
<td><span class=""math-container"">$w_4$</span> - 186</td>
<td><span class=""math-container"">$w_3$</span> - 166</td>
</tr>
<tr>
<td>...</td>
<td>...</td>
</tr>
<tr>
<td><span class=""math-container"">$w_{60}$</span> - 105</td>
<td><span class=""math-container"">$w_6$</span> -   101</td>
</tr>
</tbody>
</table></div>
<p>Given this setup, I am looking for a metric commonly used in NLP literature to compare these ranked, weighted results. <strong>The intention is to know how similar the second list wrt to the first</strong>. The metric should meet the following criteria:</p>
<ol>
<li><p>Suitable for lists that are finite, small, and complete, with just websites order changed.</p>
</li>
<li><p><strong>(optional)</strong> Includes a variable or mechanism that controls the importance of the top-k results, where k can vary from 1 to n. For instance, if k=1, the metric should primarily focus on the first result, whereas if k=n, it should give equal importance to the entire list.</p>
</li>
</ol>
<p>Note: The websites are permuted and not scores. Scores can be different in two lists, both in order and values. For example: the highest score in E1 results is for w1, which is 95 and in E2, it is 199 for w4.</p>
","nlp"
"128288","LLMs for text generation","2024-03-14 06:32:31","128289","4","111","<deep-learning><nlp><generative-models><text-generation><llm>","<p>We know that AI is rapidly growing. do we have any large language models (LLMs) to process images, pdf documents directly (fine-tune approach) for text generation tasks?</p>
","nlp"
"128279","Similarity Scores between SQL tables","2024-03-13 19:28:17","","0","43","<classification><nlp><similarity><cosine-distance>","<p>I'm trying to figure out the best way to get started on a project.</p>
<p>I have two separate databases, one is a &quot;Template&quot; db and the other is &quot;Content&quot; db. For each table in the Content db, I want to be able to identify which table in the Template db it is most similar to. In terms of similarity, I only care about structure, meaning the table name, column names, and column types.</p>
<p>Because each table can have a different structure, I'm not sure how to set this up.</p>
<p>My current way of thinking is to vectorize the table name, column names, and col types, and then to do a cosine similarity score of each table in Content to each table in Template.</p>
<p>Does that approach make sense? What can I read up on to learn more about how to do this?</p>
<p>Thanks</p>
","nlp"
"128260","Character-wise accuracy for image-to-text models","2024-03-12 15:03:38","","0","9","<nlp><cnn><computer-vision><transformer><ocr>","<p>is it possible to enforce image-to-text models like ViT or a simple CNN+Transformer to achieve character-wise accuracy?</p>
<p>Here's the context of my project:
I am developing a model to extract some targeted phrases and numbers from individual pdf pages and I would like the numbers, especially, to be character wise accurate.</p>
<p>Correct me if I am wrong, but ViT or other CNN+Transformer models make context-based estimations which is not the same as character-wise estimation, am I right?</p>
<p>One idea that I have is to tokenize the ground truth string on a character level instead of word but I am not sure how feasible this is.</p>
","nlp"
"128257","What's the purpose of using MLM when pretraining?","2024-03-12 09:13:22","128258","0","39","<machine-learning><python><deep-learning><nlp><transformer>","<p>If BERT is a stack of transformer encoders, and the encoder already operates bidirectionally, understanding both left and right contexts and generating contextual embeddings, what is the purpose of pretraining BERT using MLM ? Does it aim to improve the contextual embeddings even better ? Could someone please provide an explanation on this ? Thanks.</p>
","nlp"
"128254","can decoder only large language model be fine tuned to perform well at semantic similarity search?","2024-03-12 07:08:36","","0","18","<deep-learning><nlp><transformer><llm>","<p>BERT based models are Encoder only which are well suited for text classification, and Semantic Text similarity search (If fine-tuned via sBERT). I want to know whether decoder only models like Llama2, GPT can be fine-tuned to do well on STS benchmark. If yes, does it perform better than fine-tuning encoder-only models?</p>
","nlp"
"128252","How to find LLM that is best at STS task?","2024-03-12 06:01:18","","0","54","<deep-learning><nlp><huggingface><llm>","<p>I'm trying to find large language models that maps an embedding vector in proximity if they are semantically similar, in Korean. I tried looking at bunch of leaderboard such as MTEB_ko-ko STS, AI Hub benchmark(Korean LLM benchmark), etc... However not all models that I want to compare are within one benchmark therefore hard to compare which one is the best.</p>
<p>So I'm reading about each LLM from its base model, how it is continuously pre-trained to see how its objective function looks like. After shortlisting LLMs I'm planning to create my own dataset to compare all LLMs in shortlists.</p>
<p>As this method seem tidious, wanted to here some ideas on how others will tackle such problem.</p>
","nlp"
"128242","How do transformer-based architectures generate contextual embeddings?","2024-03-11 08:33:23","128243","0","39","<machine-learning><deep-learning><neural-network><nlp><transformer>","<p>How do transformer-based architectures, such as Roberta, etc., generate contextual embeddings? The issue is, I haven't found any articles that explain this process.</p>
","nlp"
"128234","Approach for Multi-class Classification of texts","2024-03-10 08:46:57","","0","26","<nlp><word-embeddings><transformer><bert><text-classification>","<p>I'm trying to do a project where I have paragraphs and I need to classify them into multiple labels. The dataset is around 40k rows with labels.
I understand there is no one right approach but should I consider typical ML classifiers like embeddings + Logistic regression, xgboost etc.
Or should I directly consider fine tuning transformers like BERT,DistilBERT etc.</p>
<p>My priority is on getting accurate predictions and I have a few weeks to complete this.</p>
","nlp"
"128218","Fine tuning or just feature extraction or both using Roberta?","2024-03-08 18:58:12","128225","0","61","<machine-learning><python><deep-learning><neural-network><nlp>","<p>I'm reading a program that use the pre-trained Roberta model (roberta-base). The code first extracts word embeddings from each caption in the batch, using the last hidden state of the Roberta model. Then, the model is trained to align these word embeddings with the image features (pixels) of the image through a type of attention mechanism. Then the models are updated using attention loss function. This iterative process continues until the training is complete, so I guess the word embeddings will be different after each epoch ? This is a multi-modal problem.</p>
<p>When I compare the Roberta model after training with the pre-trained model (roberta-base), I notice that every parameters the trained Roberta model are different, seems like the new model has updated the parameters. I'm not sure whether this is a form of fine-tuning or just feature extraction or both ?</p>
","nlp"
"128203","what is the main difference between ROUGE and BLUE?","2024-03-07 11:58:40","","0","17","<nlp><transformer><model-evaluations><language-model><llm>","<p>Both (ROUGE, BLUE) are useful to find the similarity between machine generated summary and reference summary.</p>
<p>what is the main difference?</p>
","nlp"
"128201","Reducing language bias for text classification, transformer model","2024-03-07 09:46:36","","0","9","<machine-learning><classification><nlp><transformer><huggingface>","<p>I am working on a text classification model predicting classes for text. We have languages from many parts of the world and some of our classes are dominated by specific languages. The model we are using is:</p>
<pre><code>https://huggingface.co/distilbert/distilbert-base-multilingual-cased
</code></pre>
<p>Even though the model is multilingual it shows bias pushing certain languages towards specific classes. If I translate text from English to Thai I will receive different predictions. Given the dataset imbalance in classes/languages, this is understandable but I'd like to improve it.</p>
<p>I'm wondering if someone has a good solution for decreasing this bias? I'm thinking of simply translating training data between the languages to reduce it</p>
","nlp"
"128194","How do I automate testing and comparison of the performance of models with different layer depths, layer types, and unit counts?","2024-03-07 01:46:52","128196","1","43","<nlp><tensorflow><hyperparameter-tuning><model-selection>","<p>I am testing the effects of different layer counts/depths, unit counts, and layer types for natural language processing. I made a Kaggle notebook where I manually create different layers and then train them on the same input. This is the <a href=""https://www.kaggle.com/code/joachimrives/effects-of-layer-count-and-unit-count"" rel=""nofollow noreferrer"">full notebook</a>. This is the code snippet for building the models:</p>
<pre class=""lang-py prettyprint-override""><code>def generate_model(
    model_optimizer,
    embedding_dimension=32,
    layer_count=1,
    unit_count=1,
    activation_function='selu',
    kernel_initializer_function='glorot_normal',
):
    input_layer = tf.keras.Input(shape=(sequence_max_len,))
    output_layer = tf.keras.layers.Embedding(
        input_dim=vocabulary_size,
#         input_length=sequence_max_len,
        output_dim=embedding_dimension,
        mask_zero=True,
    )(input_layer)
    
    output_layer = make_layer_block(
        output_layer,
        layer_count,
        unit_count,
        activation_function,
        kernel_initializer_function
    )
    output_layer = tf.keras.layers.GRU(
        units=unit_count,
        activation='selu',
        kernel_initializer='glorot_normal',
        return_sequences=False
    )(output_layer)
    
    output_layer = tf.keras.layers.Flatten()(output_layer)
    output_layer = tf.keras.layers.Dense(units=32, activation='selu')(output_layer)
    output_layer = tf.keras.layers.Dense(units=16, activation='selu')(output_layer)
    output_layer = tf.keras.layers.Dense(units=1, activation='sigmoid')(output_layer)
    
    model = tf.keras.Model(inputs=input_layer, outputs=output_layer)
    model.compile(
        optimizer=model_optimizer,
        loss='binary_crossentropy',
        metrics=['accuracy']
    )
    print(model.summary())
    
    return model
</code></pre>
<p>This code builds several different models and stores their history.</p>
<pre class=""lang-py prettyprint-override""><code># Python RNG
import random
# TF RNG
from tensorflow.python.framework import random_seed

layer_counts = [1, 3, 6, 9, 12]
unit_counts = [4, 8, 16, 32, 64]
model_performances = dict()


for layer_count in layer_counts:
    for layer_units_count in unit_counts:
        random_seed.set_seed(2)
        np.random.seed(2)
        optimizer_adam = tf.keras.optimizers.Adam(learning_rate=0.005)
        
        model = generate_model(
            optimizer_adam,
            32,
            layer_count,
            layer_units_count
        )
        
        model_history = model.fit(
            tokenized_train_text,
            train_split_df['target'].to_numpy(),
            validation_data=(
                tokenized_crossval_text,
                crossval_split_df['target'].to_numpy()
            ),
            epochs=30,
            callbacks=[
                tf.keras.callbacks.EarlyStopping(
                    monitor='val_accuracy',
                    min_delta=0.01,
                    patience=3,
                    verbose=1,
                    mode='auto',
                    baseline=None,
                    restore_best_weights=True,
                    start_from_epoch=0
                )
            ]
        )
        
        name = f&quot;layer_{layer_count}+unit_{layer_units_count}&quot;
        model_performances[name] = model_history
</code></pre>
<p>The problem is I have to go through the history dictionary to check the model performances. I also need to specify the layer type. It would also require a big change to make different combinations of layers. That isn't possible with the code I have. Is there any way to automate building models with different layer types, layer unit counts, and layer counts or depths? I am open to using TensorFlow, Weights and Biases, and MLFlow to compare model performance. If you have suggestions other than those, please include code snippets or links to examples I can try now.</p>
","nlp"
"128192","Could You Suggest Me Some Details of Realizing This LLM?","2024-03-06 21:46:05","","0","17","<nlp><transformer><llm>","<p>I mean this hypothetical LLM:
<a href=""https://twitter.com/RokoMijic/status/1663299142431432704"" rel=""nofollow noreferrer"">https://twitter.com/RokoMijic/status/1663299142431432704</a></p>
<p>I'm trying to figure out how the neural network (let's abstract from the data) can be realized. I understand that:</p>
<ol>
<li>It's a transformer;</li>
<li>It's sequence-to-sequence prediction (with a decoder, not with classification layers).</li>
</ol>
<p>I'd like to ask more experienced ML people for more details of the realization. &quot;Chronologically labelled data&quot; means here concatenation of the events with the dates (like in life2vec)? Am I missing something else that is critical?
Thanks a lot in advance!</p>
","nlp"
"128163","RAG - how to deal with numerical data","2024-03-05 12:41:01","","0","145","<machine-learning><nlp><llm>","<p>I have a car marker companies data . I am creating chunks for different car models in llama index and using vector store index and it is giving decent outputs when asked questions . It fails poorly when i ask questions like suggest car models below $xyz . I have tried many embedders but problem is that it language model doesn't seem to have sense of amounts/price and it matches against more expensive models . In general , how do you deal with such cases . Do you use llama index tools to deal with numerical questions . Please guide and let me know if more details are required</p>
","nlp"
"128134","Training Models Directly with Transformer Attention Weights: A Viable Strategy?","2024-03-03 04:35:45","","1","39","<machine-learning><deep-learning><nlp><data-science-model><transformer>","<p>I'm currently using pre-trained transformers to extract embeddings for sequence analysis, which are then used in downstream tasks. My process involves using the extracted embeddings as features for training models tailored to specific applications. Recently, I came across a study <a href=""https://www.nature.com/articles/s41598-022-18205-9"" rel=""nofollow noreferrer"">1</a> that not only utilizes embeddings from an MSA transformer but also trains models directly on the extracted row attention weights independently.</p>
<p>This approach intrigues me since it's not commonly seen in the literature or practices I've encountered. Is it a practical approach to train downstream models directly on attention weights derived from transformers?</p>
","nlp"
"128113","How can I use contextual embeddings with BERT for sentiment analysis/classification","2024-03-01 22:34:09","128118","1","162","<nlp><word-embeddings><transformer><bert><embeddings>","<p>I have a BERT model which I want to use for sentiment analysis/classification. E.g. I have some tweets that need to get a POSITIVE,NEGATIVE or NEUTRAL label. I can't understand how contextual embeddings would help in a better model, practically.</p>
<p>I process the tweets and sentences to make them ready to be fed into the tokenizer. After I get every embedding as well as its mask, and feed it into a BERT model. From that BERT model I get some hidden states in return. As I understand it, now I have to also use a linear layer to take that 768 output from BERT and output a possibility for the 3 labels.</p>
<p><strong>How can contextual embeddings help me here?</strong> I get that we can use a combination of those hidden states/layers that we get for every sentence by the BERT model, and that helps us create better embeddings, which technically mean better models. But, after I follow some approach, e.g. summing the last four hidden states, or taking a mean of every token to create a token for each word, how do I proceed now? Do I need another model to take that embedding and output the labels that way (e.g. a linear layer but after the contextual embeddings are created)? Am I thinking of this the right way? Any input would be appreciated.</p>
","nlp"
"128109","Top_p parameter in langchain","2024-03-01 16:16:59","128117","1","135","<machine-learning><nlp><sampling><artificial-intelligence>","<p>I am trying to understand the <code>top_p</code> parameter in langchain (nucleus sampling) but I can't seem to grasp it.
Based on <a href=""https://www.linkedin.com/pulse/science-control-how-temperature-topp-topk-shape-large-puente-viejo-u88yf/"" rel=""nofollow noreferrer"">this</a> we sort the probabilities and select a subset that exceeds p and concurrently has the fewer members possible.<br />
For example for:</p>
<pre><code>t1 =0.05
t2 = 0.5
t3 = 0.3
t4 = 0.15
</code></pre>
<p>and <code>top_p=0.75</code> we would select <code>t2</code> and <code>t3</code>, right?</p>
<p>If this is the case what happens if <code>top_p=0.001</code>?<br />
We just need one token and any one of these is enough.<br />
Do we select the biggest one (<code>t2</code>)? (based on my experience this makes sense, since i tested <code>top_p=0.001</code> on an LLM and the output was coherent, so since we select only one token if it was a random token with <code>probability &gt;0.001</code> the output should be garbage).</p>
","nlp"
"128105","Aside from trial and error, how do I select the number of layers and unit counts for LSTMS, GRUs, and Transformer units for text and time series?","2024-03-01 09:54:05","","1","36","<nlp><tensorflow><lstm><transformer><gru>","<p>When deciding on the number of units and layers for text processing or time-series prediction I rely heavily on trial and error. First, I look for a reference or paper on the topic such as the white paper on transformers: <a href=""https://arxiv.org/pdf/1706.03762.pdf"" rel=""nofollow noreferrer"">Attention Is All You Need</a>. Once I read why the standard is so-and-so, I code the standard. After that, I incrementally adjust the unit counts and number of layers one at a time. I am making wild guesses at that point. Maybe the number of meaningful or non-zero tokens could approximate the units required. If my sequence length is capped at 120 tokens, I'll see how long it takes to train a 128-unit LSTM, GRU, or Transformer model. I arbitrarily set the unit count to the lowest exponent value of 2 greater than the maximum number of tokens and then steadily reduce it. After that, I start adding layers. If the model has bad metrics, I keep adding layers. If the model takes more than a minute to train per layer, I reduce the number of units and layers. I tolerate long training times based on how much free time I think I have. Is there any way to search more systematically? My criteria are all arbitrary. I hope there is a way to calculate the layer and unit counts based on the input data or meta-data. I am trying this out with TensorFlow.</p>
","nlp"
"128094","How can I get the list of pretrained large language models?","2024-02-29 09:35:39","128156","0","39","<nlp><language-model><finetuning><llm>","<p>Is there any place I can get the list of pre-trained large language models in a neat way? Despite the most common ones like gpt, BARD, llama2, which llm do you suggest that can be used for RAG and fine-tuning? Especially I am looking forward multilingual models.</p>
","nlp"
"128043","What ML model is best suited for an intelligent search assistant?","2024-02-26 18:22:36","","0","37","<machine-learning><python><nlp><ai><gpt>","<p>I'm working on my thesis project, and want to make an intelligent search assistant that understands context and, of course, processes and repsonds in natural languaje. The data I want to train this model on is from the Virtual Observatory and a <a href=""https://pyvo.readthedocs.io/en/latest/"" rel=""nofollow noreferrer"">python library</a> that can be used to retrieve data from it.</p>
<p>I thought on Open AI's GPT-3 API, but the knowledge to which it has access to is outdated. The I thought about IBM's Wattson Discovery, but I feel that using their solution would limit the response type or the training process very much.</p>
<p>What ML model(s) would work better in my case? Or what software/solution would be useful?</p>
","nlp"
"128037","Resources on website summarization using LLMs","2024-02-26 11:17:34","","0","15","<nlp><llm><automatic-summarization>","<p>I am working on a problem where I have to summarize business websites. I have to generate a short 100 word summary of the primary function of a given website.</p>
<p>I am familiar with langchain url summarization and langchain document summrization. Using these tools can someone suggest some high level ideas regarding website summarization design or point to some resources regarding the same.</p>
<p>Any other related ideas are also welcome.</p>
","nlp"
"127021","What do special tokens used for in Roberta?","2024-02-24 12:28:40","128028","1","233","<machine-learning><python><deep-learning><neural-network><nlp>","<p>When I use this code:</p>
<pre><code>from transformers import RobertaTokenizer

tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
captions = &quot;This is an example caption&quot;

output = tokenizer(captions, padding='longest')
input_ids = output['input_ids']
print(input_ids)

tokens = tokenizer.convert_ids_to_tokens(input_ids)
print(tokens)
</code></pre>
<p>The output is:</p>
<pre><code>[0, 713, 16, 41, 1246, 3747, 2]  
['&lt;s&gt;', 'This', 'Ġis', 'Ġan', 'Ġexample', 'Ġcaption', '&lt;/s&gt;']
</code></pre>
<p>What is the purpose of the special tokens <code>&lt;s&gt;</code> and <code>&lt;/s&gt;</code>, are they really necessary, does the performance decrease if I remove them using <code>add_special_tokens = False</code> ?</p>
<p>Thanks</p>
","nlp"
"127010","job title normalizer","2024-02-23 17:48:15","","0","99","<nlp><recommender-system><word2vec><bert>","<p>is there any way to normalize job titles using ml or nlp?</p>
<p>examples:</p>
<p>raw title:  UX/UI Engineers
normalized title:  Software Engineers</p>
<p>raw title:  UX/UI Designer
normalized title:  Graphic Designers</p>
<p>raw title:  .NET Developer
normalized title:  Web Developers</p>
<p>raw title:  Senior Android Engineer
normalized title:  Software Developers</p>
<p>raw title:  Jr ml engineer
normalized title:  ML Engineer</p>
","nlp"
"126981","How to choose ideal pretrained model for fine-tuning?","2024-02-22 15:07:39","","0","23","<nlp><language-model><finetuning><llm><pretraining>","<p>I started to work with LLMs lately and want to know how people choose their pre-trained models in their fine-tuning tasks? What is the criteria to choose the base model and which factors affect?</p>
","nlp"
"126980","Can I fine tune MedPaLM model","2024-02-22 14:22:59","","0","69","<nlp><language-model><finetuning><llm>","<p>Is it possible to fine-tune MedPaLM and MedPaLM2; Google's llms trained using PaLM specialized for medical domain. Can we fine-tune these models further to get more specialized models?</p>
","nlp"
"126975","Why was the learning rate decreased for Roberta compared to LSTM?","2024-02-21 21:18:05","126978","0","66","<machine-learning><python><deep-learning><neural-network><nlp>","<p>I'm reading the codebase of a project that uses Bidirectional-LSTM. The learning rate for it is 0.02. Later, someone improved the project by replacing LSTM with Roberta and decreased the learning rate to 5e-5. Why did they decrease the learning rate ?</p>
","nlp"
"126946","What do these terms mean in the context of Roberta?","2024-02-20 09:29:39","126950","2","215","<machine-learning><python><deep-learning><neural-network><nlp>","<p>When I read articles about Roberta, I often read the terms &quot;transfer learning&quot; and &quot;fine-tuning&quot;. Additionally, they also mention &quot;feature extraction&quot;. What are the differences between these terms ?</p>
<p>If I want to train the pre-trained Roberta on my dataset but only use it to calculate contextual embeddings for each caption in the dataset by taking the last hidden state of the model, what is this process called ?</p>
","nlp"
"126942","Why do the Llama 2 weights have eight different files?","2024-02-19 23:09:51","","1","293","<machine-learning><deep-learning><neural-network><nlp><transformer>","<p>I downloaded the weights for <a href=""https://github.com/facebookresearch/llama/tree/main"" rel=""nofollow noreferrer"">Llama 2</a> (70B-chat). This process created a folder titled &quot;llama-2-70b-chat,&quot; which contained 8 files titled consolidated.00.pth, consolidated.01.pth, and so on until consolidated07.pth. Each file is about 16.84 GB. Here are the names and types of all the tensors in consolidated.00.pth:</p>
<ul>
<li>tok_embeddings.weight [32000, 1024]</li>
<li>norm.weight [8192]</li>
<li>output.weight [4000, 8192]</li>
<li>rope.freqs [64]</li>
<li>layers.0.attention.wq.weight [1024, 8192]</li>
<li>layers.0.attention.wk.weight [128, 8192]</li>
<li>layers.0.attention.wv.weight [128, 8192]</li>
<li>layers.0.attention.wo.weight [8192, 1024]</li>
<li>layers.0.feed_forward.w1.weight [3584, 8192]</li>
<li>layers.0.feed_forward.w2.weight [8192, 3584]</li>
<li>layers.0.feed_forward.w3.weight [3584, 8192]</li>
<li>layers.0.attention_norm.weight [8192]</li>
<li>layers.0.ffn_norm.weight [8192]</li>
<li>layers.1.attention.wq.weight [1024, 8192]</li>
<li>... and so on until</li>
<li>layers.79.ffn_norm.weight [8192]</li>
</ul>
<p><strong>But why are there 8 separate &quot;consolidated.0X.pt&quot; files?</strong> The other 7 files have tensors with the same names and the same shapes, but <em>different values</em>—even the token embeddings have different values!</p>
<p>In fact, if I multiply out the parameter dimensions above and sum them, I get approximately 8.6B parameters, which is far shy of 70B, but almost exactly an eighth of 70B.</p>
<p>I think the answer may relate to how the model uses <a href=""https://arxiv.org/pdf/2305.13245.pdf"" rel=""nofollow noreferrer"">grouped-query attention</a>; the <a href=""https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/"" rel=""nofollow noreferrer"">paper</a> mentions using 8 A100s with <a href=""https://huggingface.co/docs/text-generation-inference/en/conceptual/tensor_parallelism"" rel=""nofollow noreferrer"">tensor parallelism</a>. The params JSON file has this information:</p>
<pre><code>{&quot;dim&quot;: 8192, &quot;multiple_of&quot;: 4096, &quot;ffn_dim_multiplier&quot;: 1.3, &quot;n_heads&quot;: 64, &quot;n_kv_heads&quot;: 8, &quot;n_layers&quot;: 80, &quot;norm_eps&quot;: 1e-05, &quot;vocab_size&quot;: -1}
</code></pre>
<p>Everything else is available publicly in the <a href=""https://github.com/facebookresearch/llama/tree/main"" rel=""nofollow noreferrer"">Llama GitHub repo</a>.</p>
","nlp"
"126940","What are the differences between Embedding Layer and Roberta Embedding?","2024-02-19 21:34:36","126941","0","132","<machine-learning><python><deep-learning><neural-network><nlp>","<p>I'm reading an article about the Embedding Layer:</p>
<blockquote>
<p>The Embedding Layer learns word embeddings from raw text. It is
initialized with small random numbers and can be learned
simultaneously with a neural network in a supervised way using
backpropagation for a specific task, such as text classification.</p>
</blockquote>
<p>So, the Embedding Layer learn word embeddings from scratch, correct? In comparison to Roberta's word embeddings, using a pretrained model from HuggingFace's Transformer, are there any differences between them ?</p>
","nlp"
"126935","How to Detect and Identify German Compound Words in a Text using Python","2024-02-19 14:58:45","","0","66","<nlp><data-mining><data-science-model><ai><llm>","<p>I would like to know if there is any python libraries or packages available which can help me detect and extract German compound words. I took a look at nltk and spacy packages and they don't seem to have such libraries. I have given a sample paragraph that contains some compound words and then I listed those compound words.</p>
<p>PS: compound words are made of two or more nouns, verbs, adjectives, or adverbs.</p>
<p>&quot;Das Fahrradfahren ist eine beliebte Freizeitaktivität in Deutschland. Viele Menschen fahren gerne mit dem Fahrrad zur Arbeit oder nutzen es für lange Radtouren durch die malerische Landschaft. Einige bevorzugen sogar das Mountainbiking oder das Rennradfahren, um das Adrenalin zu spüren. Andere wiederum schätzen das E-Bikefahren für eine entspannte Fahrt ohne große Anstrengung.&quot;</p>
<p>Fahrradfahren
Freizeitaktivität
Radtouren
Mountainbiking
Rennradfahren
E-Bikefahren</p>
","nlp"
"126919","Extraction of name from phonetic transcription","2024-02-18 13:22:08","","1","23","<deep-learning><nlp><pytorch><generative-models>","<p>I have a use case where I want to extract the name from the phonetic transcription.<br />
For example if the phonetic transcription is - “m a j n e j m ɪ z s ʌ m i ɹ z o w ʃ i”, the output should be the name that is - “s ʌ m i ɹ z o w ʃ i” .<br />
Similarly if the phonetic transcription is - “juː kæn kɔːl miː s ʌ m i ɹ z o w ʃ i”, then output would be “s ʌ m i ɹ z o w ʃ i”.<br />
This is nothing but extracting name from the phonetic transcription.<br />
What could be the best suitable and easy way to achieve this?<br />
I think it would be kind of sequence to sequence model where the input is sentence and output is name.<br />
If yes, I am looking for guidance on the model type and if there in any fine tuning that may need to be done for achieving this.</p>
","nlp"
"126918","reverse summary matching","2024-02-18 12:19:05","","0","38","<nlp><pytorch>","<p>How can we develop a model to assess the relationship between a summary and a text, enabling it to determine their coherence but in reverse?</p>
<p>Also, how to make an inference</p>
<p>We have list of texts (2000) and their matching summaries (7000)</p>
<p>one to many, can be couple of summaries that can describe part of the text</p>
<p>(( Unrelated; our hidden goal [except from learning] it to check this approach for Java to Smali matching ))</p>
<pre class=""lang-py prettyprint-override""><code>current_model = 'google/flan-t5-large'
max_length = 512  # maximum sequence length
num_epochs = 3
learning_rate = 0.01
batch_size = 4

class CustomDataset(Dataset):
    def __init__(self, df, tokenizer, max_length):
        self.df = df
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        text = row['text']
        summary = row['summary']
        
        # Select a random text that does not match the summary
        non_matching_row = self.df[self.df['text'] != text].sample(n=1)
        non_matching_dialogue = non_matching_row['text'].iloc[0]

        inputs = self.tokenizer(text, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_length)
        matching_labels = self.tokenizer(summary, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_length)

        # Tokenize non-matching text
        non_matching_inputs = self.tokenizer(non_matching_dialogue, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_length)
        return inputs, matching_labels, non_matching_inputs

# Load pre-trained tokenizer
tokenizer = T5Tokenizer.from_pretrained(current_model)

# Create training dataset and dataloader
train_dataset = CustomDataset(train_df, tokenizer, max_length)
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

# Load pre-trained model
model = T5ForConditionalGeneration.from_pretrained(current_model)

# Define optimizer
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

# Training loop
for epoch in range(num_epochs):
    for batch_idx, batch in enumerate(train_loader):
        inputs = batch[0]
        matching_labels = batch[1]
        non_matching_inputs = batch[2]

        optimizer.zero_grad()
        
        # Train on matching text
        matching_outputs = model(input_ids=inputs['input_ids'][0], attention_mask=inputs['attention_mask'][0], labels=matching_labels['input_ids'][0])
        matching_loss = matching_outputs.loss

        # Train on non-matching text
        non_matching_outputs = model(input_ids=non_matching_inputs['input_ids'][0], attention_mask=non_matching_inputs['attention_mask'][0], labels=non_matching_inputs['input_ids'][0])
        non_matching_loss = non_matching_outputs.loss if non_matching_outputs is not None else torch.tensor(0.0)

        total_loss = matching_loss + non_matching_loss
        total_loss.backward()
        optimizer.step()

        if batch_idx % 100 == 0:
            print(f'Epoch {epoch}, Batch {batch_idx}, Matching Loss: {matching_loss.item()}, Non-matching Loss: {non_matching_loss.item()}')

# Save the trained model
model.save_pretrained('flan-t5-large-trained')

# Evaluation on the testing set
test_dataset = CustomDataset(test_df, tokenizer, max_length)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

correct = 0
total = 0

# Iterate over the testing set
for inputs, matching_labels, non_matching_inputs in test_loader:
    matching_outputs = model.generate(input_ids=inputs['input_ids'][0], attention_mask=inputs['attention_mask'][0], max_length=512, num_beams=1, early_stopping=True)
    matching_generated_summary = tokenizer.decode(matching_outputs[0], skip_special_tokens=True)
    
    # Check if the generated summary matches the original summary
    is_matching = matching_generated_summary == tokenizer.decode(matching_labels['input_ids'][0], skip_special_tokens=True)
    
    # Update the counts
    total += 1
    correct += is_matching

# Calculate accuracy
accuracy = correct / total * 100
print(f'Testing Accuracy: {accuracy}%')
<span class=""math-container"">```</span>
</code></pre>
","nlp"
"126915","What are the differences between contextual embeddings of Bidirectional-LSTM and Transformer?","2024-02-18 09:19:02","126923","0","185","<machine-learning><python><deep-learning><neural-network><nlp>","<p>A Transformer, like Roberta, can generate contextual embeddings using the encoder part, similar to a Bidirectional-LSTM that concatenates hidden states. What are the differences between them ? Are there any advantages of Transformer contextual embeddings over Bidirectional-LSTM? Could someone please explain?</p>
","nlp"
"126904","How to select the optimal beam size for beam search?","2024-02-17 07:36:58","126907","2","373","<machine-learning><nlp><hyperparameter-tuning><sequence-to-sequence><text-generation>","<p>Most Text Generation Models use beam search to select the optimal output candidate. How does one choose the optimal beam size? It would probably vary from task to task, dataset to dataset, and model to model. But given it all these parameters are fixed, how do we choose the optimal beam size? Theoretically scores(beam_size) &gt; scores(beam_size -1) but practically that may not be the case when evaluating for metrics like ROUGE, or BLEU. So is it experimentally determined, for example, to run it for all beam sizes and report the one with the best beam size? I am particularly curious about two aspects:</p>
<ol>
<li>In research projects, do people tune the beam size parameter or do they just take the largest reasonable beam size that fits whatever GPU they have?</li>
<li>When these models are deployed in the real world, how is the beam size determined given the incoming distribution of inputs may be wildly different from the training dataset such that the empirically validated beam size? Or is this not a significant enough concern for resources to be deployed for this optimization?</li>
</ol>
","nlp"
"126878","Structure for text message categorisation","2024-02-15 15:09:35","","1","23","<nlp>","<p>I have a load of online chat messages and need to categorise them based on certain criteria, such as the topic, intention, tone of voice, type of information (e.g. dates &amp; amounts), etc. Once categorised, these are used to train a model to automatically categorise new messages.</p>
<p>Currently, the categories (around 20) are a big hot mess that has been built up, ad-hoc, and doesn't have any particular structure to speak of.</p>
<p>My question is, what is a good approach to structuring the categories that would best capture the information and also be logical/orthogonal enough to train the model effectively.</p>
<p>Some stuff I've been considering: I've come across Searle's 5 types of &quot;speech acts&quot;, which could be a useful way to differentiate messages at a more fundamental level than the particular business logic. What other models might be useful? Would a hierarchical structure make sense? Would it make sense to separate nouns, verbs, adjectives, and then drill down into specifics? What about &quot;tagging&quot; as opposed to a hierarchy to avoid similar nodes in different branches of the hierarchy. Any good resources to get started in this research?</p>
<p>Another approach I've looked into but haven't tried is a transformer-based clustering method where the centroids would then become the basis for categories.</p>
<p>If anyone has experience of doing this or just some thoughts, I'd be interested to hear!</p>
","nlp"
"126868","Interpreting Perplexity, U_mass coherence and Cv score trends for a Latent Dirichlet Allocation Model","2024-02-14 17:07:18","","0","45","<nlp><data-science-model><text-mining><data-analysis><topic-model>","<p>I'm running an LDA model through gensim. To my understanding, closer the u_mass coherence score is to zero, higher is the interpretability of the topics that come up. I'm getting the u_mass coherence score trend like this-<a href=""https://i.sstatic.net/FZE64.png"" rel=""nofollow noreferrer"">1</a>. Since there's a noticeable dip in bound-<a href=""https://i.sstatic.net/MgLc9.png"" rel=""nofollow noreferrer"">2</a> between num_topics 10 to 15, I explored the top words in each topic for the same number of topics. While the actual topics that come up between 10 to 15 as the number of topics are meaningful and interpretable, I'm unable to understand why my perplexity trend-<a href=""https://i.sstatic.net/DYvFZ.png"" rel=""nofollow noreferrer"">3</a> is exact opposite because according to Latent Dirichlet Allocation by Blei, Ng, &amp; Jordan, perplexity should monotonically decrease as number of topics increases-<a href=""https://i.sstatic.net/BHtE3.png"" rel=""nofollow noreferrer"">4</a>. Also, the C_v score trend seems off - <a href=""https://i.sstatic.net/bBUYj.png"" rel=""nofollow noreferrer"">5</a>. I'm performing lemmatization and stopwords removal as preprocessing. Additionally, I'm looking at the tfidf scores to pick maximum number of unique words to make the corpus. Could the opposite perplexity trend be due to lack of parameter tuning? I would be grateful for any comments on the trends as well as how to tune the alpha and eta.</p>
<p>Below is my code-</p>
<pre><code>tokenized_narratives = [text.split() for text in cleaned_narrative_list]

**# Extract top 2800 terms from TF matrix
top_2800_words = tf_vectorizer.get_feature_names_out()[:2800]

**# Filter tokenized narratives using top 2800 terms
filtered_tokenized_narratives = [[token for token in text if token in top_2800_words] for text in tokenized_narratives]

min_topics = 2
max_topics = 30
step_size = 2
num_topics_range = range(min_topics, max_topics + 1, step_size)


id2word = corpora.Dictionary(filtered_tokenized_narratives)
corpus = [id2word.doc2bow(text) for text in filtered_tokenized_narratives]
train_corpus, test_corpus = train_test_split(corpus, test_size=0.2)`

**# Visualize results**

umass_coherence_values = []
cv_coherence_values = []
bound_values = []
perplexity_values=[]

for num_topics in num_topics_range:
    lda_model = gensim.models.LdaModel(
        corpus=train_corpus,
        id2word=id2word,
        num_topics=num_topics,
        iterations=400,
        chunksize=100,
        passes=10,
        per_word_topics=True,
    )

**# bound and perplexity calculation**
    bound = lda_model.log_perplexity(test_corpus)
    bound_values.append(bound)
    print(&quot;bound=&quot;,bound)
    
**# perplexity**
    perplexity = np.exp(-bound)
    perplexity_values.append(perplexity)
    print(perplexity)
    
**# UMass Coherence**
    coherence_model_umass = CoherenceModel(model=lda_model, corpus=corpus, dictionary=id2word, coherence='u_mass')
    umass_coherence = coherence_model_umass.get_coherence()
    umass_coherence_values.append(umass_coherence)
    `print(umass_coherence)`

**# C_v Coherence**
    coherence_model_cv = CoherenceModel(model=lda_model, texts=filtered_tokenized_narratives, dictionary=id2word, coherence='c_v')
    cv_coherence = coherence_model_cv.get_coherence()
    cv_coherence_values.append(cv_coherence)
    print(cv_coherence)

    print(num_topics)
</code></pre>
","nlp"
"126841","predict next career suggestion","2024-02-13 03:52:58","","0","19","<nlp><recommender-system><word2vec><gensim><gpt>","<p>I have a dataset having job and description. i want to make model which can predict what are the thing that user needs to improve when the user inputs his skills.</p>
<p>For an example,</p>
<p>If he has skills - Python, Data Visualization, and etc
The model should predict - DataScience, Machine Learning etc..</p>
<p>Note : The dataset contains the job description which contains various skills a candidate needs to have. model needs to predict from those data,</p>
<p>fine tune the dataset on GPT model is not recommended.</p>
<p>If you have any idea how to approach to this problem, I am all ears.</p>
","nlp"
"126801","Questions about hidden states of bidirectional LSTMs","2024-02-10 06:24:14","126802","0","94","<machine-learning><deep-learning><neural-network><nlp><lstm>","<p>I read this in an article about bidirectional LSTM:</p>
<blockquote>
<p>In bidirectional LSTM, each word corresponds to two hidden states, one
for each direction. Thus, we concatenate these two hidden states to
represent the semantic meaning of a word. Additionally, the last
hidden states of the bidirectional LSTM are concatenated to be the
sentence vector</p>
</blockquote>
<p>Could someone explain what are these two hidden states representing each word, and also clarify what are the last hidden states when representing the sentence?</p>
","nlp"
"126782","Information retrieval SOTA models","2024-02-08 15:14:37","","1","73","<machine-learning><nlp><information-retrieval>","<p>Where can I find sota models of information retrieval? My task is to rank documents by given query by semantic search of embedding. I know that models like ColBERT, SPLADE solve this problem, but I think they are not SOTA now. I tried to find SOTA models on paperswithcode but there almost nothing about this problem.</p>
","nlp"
"126760","What specific problems in what domains and fields have the need to use rule-based text classification?","2024-02-07 07:22:29","","0","29","<nlp><text-classification><domain-adaptation>","<p>I wrote a rule-based keyword detection and classification program specialized in my language (Vietnamese) and would like to know where this app is useful. Here how the program work:</p>
<ol>
<li>First you input the prompt (which is a bunch of keywords), e.g. <code>fish 50k</code></li>
<li>Then it will automatically label/classify the prompt like this:</li>
</ol>
<pre><code>Object: fish
Type of Object: food
Place of transaction: market
Type of place of transaction: offline
Consumer: myself
Type of consumer: myself
Price: 50000 VND
</code></pre>
<p>The program can make this classification based on a config you declare, e.g.:</p>
<pre class=""lang-yaml prettyprint-override""><code>- Dimension name: Object
  Classification:
    - Food: fish, meat
    - Appliance: computer, speaker
  Default value: meat
...
</code></pre>
<p>Which problems do you see this app will be useful? In general, where have you seen rule-based classification being applied? Especially in the context of ChatGPT and its GPT store? What domains, fields or industries have the need to use rule-based approaches? I think there should be a review on how this technique is applied in various field, but I can't find one.</p>
<p>In my understanding, there are two types of approaches in NLP: rule-based and statistic-based. Rule-based approach is simple, understandable and need not training, while statistic-based is better if the rules are complex and you have good training data. I think rule-based classification is much cheaper and more accurate than statistical-based classification. Is that correct?</p>
","nlp"
"126759","Why is dictionary-based approach a heuristic method?","2024-02-07 07:11:30","126761","2","256","<nlp><text><dictionary>","<p>In <a href=""https://datascience.stackexchange.com/a/96729/119882"">How can the accuracy of the dictionary-based approach be measured and improved?</a>, one user says that:</p>
<blockquote>
<p>dictionary-based approach is a <a href=""https://en.wikipedia.org/wiki/Heuristic"" rel=""nofollow noreferrer"" title=""Heuristic - Wikipedia"">heuristic</a> method</p>
</blockquote>
<p>Isn't that this approach is a type of rule-based approach, which on its turn is simply catching keywords using regex? Then shouldn't it be the most accurate approach, comparing to statistic-based approaches?</p>
<p>Research on the internet give me mixed results:</p>
<ul>
<li>The article <a href=""https://ai.plainenglish.io/heuristic-vs-rule-based-approaches-in-nlp-whats-the-difference-d9fb8ec8021e"" rel=""nofollow noreferrer"" title=""Heuristic vs. Rule-Based Approaches in NLP: What’s the Difference? | by Ajay Verma | Artificial Intelligence in Plain English"">Heuristic vs. Rule-Based Approaches in NLP: What’s the Difference? | by Ajay Verma | Artificial Intelligence in Plain English</a> compares heuristic approach as opposite to rule-based approach. It says:
<blockquote>
<p>heuristic approaches use general, flexible guidelines rather than rigid, predefined rules. These guidelines are often based on common-sense knowledge and intuition, making them adaptable to various scenarios.</p>
</blockquote>
</li>
<li>The article <a href=""https://www.tpximpact.com/knowledge-hub/blogs/tech/unstructured-data-natural-language-processing/"" rel=""nofollow noreferrer"" title=""The power of Natural Language Processing - TPXimpact"">The power of Natural Language Processing - TPXimpact</a> compares rule/heuristics-based category as opposite to data-driven category. It says:
<blockquote>
<p>heuristics-based approaches use rules created and programmed into machines (e.g., using templates, grammars or regular expressions)</p>
</blockquote>
</li>
</ul>
<p>So which one is correct?</p>
","nlp"
"126738","I want to make a Career suggestion model","2024-02-06 04:44:25","","0","24","<nlp><recommender-system><tfidf><gensim><cosine-distance>","<p>There is a <a href=""https://www.kaggle.com/datasets/ravindrasinghrana/job-description-dataset/data"" rel=""nofollow noreferrer"">dataset</a> having job titles and the descriptions. when a person enter his skills i need to output which category of job he should do. i have already created that using cosine similarity.(If you can tell me a better approach, that would be helpful too) now i need to give suggestions that he can improve his skills.</p>
<p>if he entered his skills as Statistics and python. model should be able to say to learn data science. Or learn data analytics.</p>
<p>Can you give me a suggestion on how to do that? An approach is much appreciated.</p>
<p>(Develop a LLM is not an option)</p>
<p>I am stucked here. any help is much appreciated</p>
","nlp"
"126720","Accuracy decreased after using google word2vec model for a sentiment classification [NLP][word-embedding]","2024-02-04 19:01:54","","0","16","<nlp><word-embeddings><word2vec>","<p>I am using Amazon fine food reviews for a sentiment classification project. while I used my dataset corpus to train avg word2vec , I was getting an accuracy of 89 %.
by using BOW and TF-IDF, i was getting 91 AND 92 % respectively.
But after using google news word2vec dataset and applying avg word2vec to train a LR model, accuracy decreased to 84%. what could be possible reasons , what should  I check ?</p>
","nlp"
"126715","What are the differences between BPE and byte-level BPE?","2024-02-04 13:27:07","126717","2","1337","<machine-learning><python><deep-learning><neural-network><nlp>","<p>In Roberta, I'm not sure if the model use BPE or byte-level BPE tokenization, are these techniques different or the same ? Can someone explain ? Thanks</p>
","nlp"
"126678","Clustering for language dialects","2024-02-01 10:24:54","","0","10","<nlp><clustering><open-source>","<p>I have a codebase in a programming language, various projects written by different people. The language is quite complex, so people use it in different styles, partially based on personal preference and knowledge, but also based on the problem they want to solve. I would like to discover language dialects through some language processing tool. My limited exposure to data mining says that this is a problem of clustering.</p>
<p>What open source software would you recommend for this task?</p>
","nlp"
"126667","Improve text classification accuracy","2024-01-31 13:31:42","","2","135","<machine-learning><nlp><data-science-model><text-classification>","<p><strong>Task:</strong></p>
<p>I am building a text classification for salary prediction for data science jobs. I want to achieve at least 70 percent accuracy.</p>
<p><strong>Data:</strong></p>
<ul>
<li>Features: Consists of job descriptions of data science, data engineering, data analyst jobs
of about 1800 samples</li>
<li>Target: Target is the salary column binned into 5 different categories. I obtained this using the pandas q cut. The  salary is heavily right skewed.</li>
</ul>
<p><strong>Problem:</strong></p>
<p>The problem is that the models I train are always in range of 40 to 50 percent accuracy. I have tried different models like Random forests, SVM, Logistic Regression with Bag of words model and Tf-idf. But accuracy doesn't increase?</p>
<p><strong>Questions:</strong></p>
<ul>
<li>Should I be trying different word embeddings like glove or word 2
vec?.</li>
<li>Should I try better models or move on to try neural networks?</li>
<li>Is the bad accuracy because the text data and my target is completely
uncorrelated?.</li>
<li>Should I try to different binning lengths for target and try different number of bins?</li>
<li>The data also has some samples which unrelated to data science but have software engineering in their titles, should I remove those?</li>
</ul>
<p><a href=""https://i.sstatic.net/8Ut4A.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/8Ut4A.png"" alt=""Test data confusion matrix"" /></a></p>
<p>[<img src=""https://i.sstatic.net/ZHciH.png"" alt="" salary bin distribution 2"" /></p>
","nlp"
"126658","What is the difference between hidden states in RNN and Transformers model?","2024-01-31 10:31:45","126660","2","262","<machine-learning><python><deep-learning><nlp><lstm>","<p>I'm very terrible at NLP and I have searched for these questions but didn't find any answer, my question is, in RNNs, there are hidden states to remember information for processing the next state, and in Transformers, there are also hidden states for each attention layer. Are these different or the same ? Additionally, I often read that RNNs or LSTMs have a dimension of <span class=""math-container"">$256$</span>, while Transformers have a dimension of <span class=""math-container"">$768$</span>. What are the meaning of these numbers, what are they used for ? Thanks</p>
","nlp"
"126645","Anomaly Detection in Log Data using LSTM","2024-01-30 14:34:58","","0","104","<deep-learning><neural-network><nlp><lstm><anomaly-detection>","<p><strong>Problem Overview:</strong>
I am currently working on a project involving anomaly detection in log data. The anomalies are defined by deviations from historical patterns. The log data has a simple structure: <em>[timestamp: log_statement]</em>.
<br><br>
<strong>Dataset Details:</strong>
The dataset consists of logs in the format [timestamp: log_statement] and have 10k+ logs
Each <em>log_statement</em> has been processed to generate keys (k1, k2, ...) for uniqueness after that it became 200-250 unique keys.
<br></p>
<p><strong>Current Approach:</strong>
I have preprocessed the log data to extract log statements and assigned unique keys to them.
Utilized LSTM for training with a window-based approach, considering past logs for predictions (similar to next word prediction by looking past words).
Anomalies are detected by comparing predicted and actual log statements within the specified window.
<br><br>
<strong>Specific Questions:</strong></p>
<ol>
<li>Is the current approach suitable for capturing temporal patterns in log data, am I right converting each log_statement to keys: k1,k2..etc ? <br></li>
<li>How can I ensure that the LSTM model is effectively learning and representing the patterns? <br></li>
<li>Are there alternative methods, either statistical, machine learning-based, deep-learning based, that might offer better results and <strong>more interpretable</strong>? <br></li>
<li>Also are there any ways, where I can tell after training that I am confident about these keys(say k1,k2) these are predictable and rest are not. I thought to compare True positive and True negative of each key, but it will heavily dependent on the model.</li>
</ol>
","nlp"
"126628","AI tools/LLM models to analyze excel data","2024-01-29 06:38:50","","0","1233","<python><nlp><data-analysis><excel><llm>","<p>I have a bunch of excel files containing information of employee attendance. I want to use NLP-based search to ask questions like: Which employee has taken most leaves?; What dates witness a high number of leave requests?; Which gender accounts for most leaves?, etc.</p>
<p>Are there any AI tools or LLM models that I can use to get answers to such queries by studying excel data?</p>
<p>NOTE: The tool should provide python APIs such that I can integrate it with my application.</p>
","nlp"
"126550","why does my multi-modal model can not learn anything?","2024-01-23 23:40:53","","0","26","<machine-learning><deep-learning><nlp><pytorch><finetuning>","<p>I have a multi-modal model. I want to train it using the Pytorch Framework. I have a balanced dataset. I have approximately 150 samples for each client. (I had preprocessed my text data.)
when I train my model it doesn't learn anything. This is my custom multi-modal model class:</p>
<pre><code>class MultiModalModel(nn.Module):
def __init__(self, num_classes):
    super(MultiModalModel, self).__init__()

    self.resnet = resnet50(pretrained=True)
    self.image_branch = nn.Sequential(
        *list(self.resnet.children())[:-2],
    )

    num_layers_to_unfreeze = 30
    start_layer_index = len(list(self.image_branch.children())) - num_layers_to_unfreeze

    for layer_index, param in enumerate(self.image_branch.parameters()):
        if layer_index &gt;= start_layer_index:
          param.requires_grad = True
        else:
          param.requires_grad = False

    # Text branch
    # self.bert_model = AutoModel.from_pretrained(&quot;vinai/bertweet-base&quot;)
    self.bert_model = BertModel.from_pretrained(&quot;bert-base-uncased&quot;)

    total_layers = len(list(self.bert_model.children()))

    # Specify the number of layers you want to unfreeze from the end
    num_layers_to_unfreeze = 20

    # Calculate the starting index of the layers to unfreeze
    start_layer_index = total_layers - num_layers_to_unfreeze

    # Iterate over the parameters and unfreeze the last 10 layers
    for layer_index, param in enumerate(self.bert_model.parameters()):
        if layer_index &gt;= start_layer_index:

          param.requires_grad = True
        else:
          param.requires_grad = False

    # Fusion layer
    self.fusion_layer = nn.Linear(2048 + self.bert_model.config.hidden_size, 10)  # Adjust input size based on your needs

    self.hidden1 = nn.Linear(10, 10)
    self.hidden2 = nn.Linear(10, 10)
    self.hidden3 = nn.Linear(10, 10)

    self.dropout = nn.Dropout(p=0.7)
    # Output layer
    self.output_layer = nn.Linear(10, num_classes)
    
def forward(self, image_input, input_ids, attention_mask, token_type_ids=0):
    # Image branch (ResNet)
    image_features = self.image_branch(image_input)
    image_features = F.adaptive_avg_pool2d(image_features, (1, 1))
    image_features = image_features.view(image_features.size(0), -1)


    # Text branch (BERT)
    # _, pooled_output = self.bert_model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids,return_dict=False)
    _, pooled_output = self.bert_model(input_ids=input_ids, attention_mask=attention_mask, return_dict=False)

    # Concatenate features from both branches
    fused_features = torch.cat((image_features, pooled_output), dim=1)
    # Fusion layer
    fused_features = F.relu(self.fusion_layer(fused_features))
    fused_features = self.dropout(fused_features)

    #add hidden layer
    hidden1 = F.relu(self.hidden1(fused_features))
    hidden1 = self.dropout(hidden1)

    hidden2 = F.relu(self.hidden2(hidden1))
    hidden2 = self.dropout(hidden2)

    hidden3 = F.relu(self.hidden3(hidden2))
    hidden3 = self.dropout(hidden3)
    
    output = self.output_layer(hidden3)
    return output
</code></pre>
<p>and this is my client class that train individual clients:</p>
<pre><code>class Client():
def __init__(self, client_id, train_data, val_data, model, weights, epochs, optimizer, loss_func, scheduler=None, batch_size=32):
    self.client_name = &quot;client_&quot; + str(client_id)
    self.train_sampler = self.sampler(train_data)
    self.val_sampler = self.sampler(val_data)
    self.train_data = DataLoader(CustomDataset(pd.DataFrame.from_dict(train_data), transforms), shuffle=False, batch_size=batch_size, sampler=self.train_sampler)
    self.val_data = DataLoader(CustomDataset(pd.DataFrame.from_dict(val_data), transforms), shuffle=False, batch_size=batch_size, sampler=self.val_sampler)

    self.model = model
    self.epochs = epochs
    self.optimizer = optimizer
    self.scheduler = scheduler
    self.loss_func = loss_func
    self.model.load_state_dict(weights)
def sampler(self , data):
labels = data['label']
class_weights = 1.0 / torch.bincount(torch.tensor(data['label']))
# Create a weight for each sample
weights = class_weights[labels]
return WeightedRandomSampler(weights, len(weights))

def validation(self, valid_data):
val_loss = []
val_f1_score = []

all_labels = []
all_predictions = []

total_val_loss = 0
for idx, batch in enumerate(valid_data):
  img = batch['image'].to(device)

  input_ids = batch['text']['input_ids'].to(device)
  attention_mask = batch['text']['attention_mask'].to(device)
  token_type_ids = batch['text']['token_type_ids'].to(device)
  lbls = batch['label'].to(device)

  valid_output = self.model(img, input_ids, attention_mask, token_type_ids)
  y_pred = torch.argmax(valid_output,dim=1).cpu()

  all_labels.append(lbls)
  all_predictions.append(y_pred)

  vloss = self.loss_func(valid_output, lbls)

  total_val_loss += vloss

all_labels = torch.cat(all_labels).cpu().numpy()
all_predictions = torch.cat(all_predictions).cpu().numpy()

f1_score_value = f1_score(all_labels, all_predictions, average='weighted')
val_precision_value = precision_score(all_labels, all_predictions, average='weighted')
val_recall_value = recall_score(all_labels, all_predictions, average='weighted')
val_balanced_accuracy_score_value = accuracy_score(all_labels, all_predictions)

avg_val_loss = total_val_loss / len(valid_data)
avg_f1_score = torch.mean(torch.tensor(f1_score_value))
avg_precision = torch.mean(torch.tensor(val_precision_value))
avg_recall = torch.mean(torch.tensor(val_recall_value))
avg_acc = torch.mean(torch.tensor(val_balanced_accuracy_score_value))


return avg_val_loss, avg_f1_score, avg_precision, avg_recall, avg_acc

def train(self):

if torch.cuda.is_available():
  self.model = self.model.cuda()
  self.loss_func = self.loss_func.cuda()

train_loss = []
train_acc = []
train_f1 = []

val_loss = []
val_acc = []
val_f1 = []
val_f1_score = []
val_precision = []
val_recall = []
for epoch_i in range(0, self.epochs):
# ========================================
#               Training
# ========================================
  print(f'======== Epoch {epoch_i + 1} / {self.epochs} ========')
  print(f&quot;Client ID : {self.client_name}&quot;)
  print('Training...')

  all_labels = []
  all_predictions = []

  epoch_train_loss = []
  epoch_train_acc = []
  train_f1_score = []
  train_precision = []
  train_recall = []

  total_train_loss = 0

  self.model.train(True)

  for step, batch in tqdm(enumerate(self.train_data)):

      image = batch['image'].to(device)
      input_ids = batch['text']['input_ids'].to(device)
      attention_mask = batch['text']['attention_mask'].to(device)
      token_type_ids = batch['text']['token_type_ids'].to(device)
      lbl = batch['label'].to(device)

      self.model.zero_grad()

      # result = self.model(image, input_ids, attention_mask, token_type_ids)
      result = self.model(image, input_ids, attention_mask)

      y_pred = torch.argmax(result,dim=1).cpu()
      all_labels.append(lbl)
      all_predictions.append(y_pred)


      loss = self.loss_func(result, lbl)
      total_train_loss += loss.item()


      # Perform a backward pass to calculate the gradients.
      loss.backward()
      # Clip the norm of the gradients to 1.0.
      # This is to help prevent the &quot;exploding gradients&quot; problem.
      torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
      self.optimizer.step()

  if self.scheduler:
    self.scheduler.step()
  epoch_train_loss.append(total_train_loss / len(self.train_data))

  all_labels = torch.cat(all_labels).cpu().numpy()
  all_predictions = torch.cat(all_predictions).cpu().numpy()

  train_f1_score_value = f1_score(all_labels, all_predictions, average='weighted')
  train_precision_value = precision_score(all_labels, all_predictions, average='weighted')
  train_recall_value = recall_score(all_labels, all_predictions, average='weighted')
  train_balanced_accuracy_score_value = accuracy_score(all_labels, all_predictions) #for imbalanced dataset

  train_loss_avg = torch.mean(torch.tensor(epoch_train_loss))
  train_f1_avg = torch.mean(torch.tensor(train_f1_score_value))
  train_acc_avg = torch.mean(torch.tensor(train_balanced_accuracy_score_value))
  train_precision_avg = torch.mean(torch.tensor(train_precision_value))
  train_recall_avg = torch.mean(torch.tensor(train_recall_value))

  train_loss.append(train_loss_avg)
  train_f1.append(train_f1_avg)
  train_acc.append(train_acc_avg)
  train_precision.append(train_precision_avg)
  train_recall.append(train_recall_avg)

  print(f&quot;Average training f1_score: {train_f1_avg :.3f}&quot;)
  print(f&quot;Average training balanced accuracy: {train_acc_avg :.3f}&quot;)
  print(f&quot;Average training precision: {train_precision_avg :.3f}&quot;)
  print(f&quot;Average training recall: {train_recall_avg :.3f}&quot;)
  print(f&quot;Average training loss: {train_loss_avg :.3f}&quot;)

  with torch.no_grad():
    print(&quot;________validation metrics__________&quot;)
    val_avg_loss, val_avg_f1_score, val_avg_precision, val_avg_recall, val_avg_acc = self.validation(self.val_data)
    val_loss.append(val_avg_loss)
    val_acc.append(val_avg_acc)
    val_f1_score.append(val_avg_f1_score)
    val_precision.append(val_avg_precision)
    val_recall.append(val_avg_recall)

    print(f&quot;Average validation f1_Score: {val_avg_f1_score :.3f}&quot;)
    print(f&quot;Average validation balanced accuracy: {val_avg_acc :.3f}&quot;)
    print(f&quot;Average validation precision: {val_avg_precision :.3f}&quot;)
    print(f&quot;Average validation recall: {val_avg_recall :.3f}&quot;)
    print(f&quot;Average validation loss: {val_avg_loss :.3f}&quot;)

#set new weights
weights = self.model.state_dict()
return {'client_name': self.client_name,
        'weights': weights,
        'train_loss': train_loss,
        'train_f1_scores': train_f1,
        'train_balanced_accuracy': train_acc,
        'train_precision': train_precision,
        'train_recall': train_recall,

        'val_loss': val_loss,
        'val_f1_score': val_f1_score,
        'val_balanced_accuracy': val_acc,
        'val_precision': val_precision,
        'val_recall': val_recall
        }
</code></pre>
<p>and this is my optimizer and loss-function setting :</p>
<pre><code>from torch.optim.lr_scheduler import ExponentialLR

optimizer = torch.optim.Adadelta(model.parameters(),
              lr = 1e-3, # args.learning_rate - default is 5e-5, our notebook had 2e-5
              eps = 1e-06, # args.adam_epsilon  - default is 1e-8.
              weight_decay=1e-5
            )

scheduler = ExponentialLR(optimizer, gamma=0.9)

loss_fn = nn.CrossEntropyLoss()
rounds = 3
epochs = 10
batch_size = 4
server = Server(encoded_train, encoded_val, encoded_test, model, rounds, epochs, optimizer, scheduler, nn.CrossEntropyLoss(), clients_number=num_clients, batch_size=batch_size)
train_res = server.train()
</code></pre>
<p>and this is my loss values and accuracy per epoch:
<a href=""https://i.sstatic.net/frtzg.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/frtzg.png"" alt=""average_loss_values_per_epoch"" /></a>
<a href=""https://i.sstatic.net/i8ktP.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/i8ktP.png"" alt=""average_accuracy_per_epoch"" /></a>
please help me what should I do?</p>
","nlp"
"126545","Zero-shot out-of-distribution text classification","2024-01-23 14:23:53","","0","14","<nlp><text-classification><zero-shot-learning>","<p>I'm building out a pipeline that would allow me to filter out text based on whether or not the text belongs to any of the classes I've defined.</p>
<p>I feel like one (albeit naive) approach would simply be to embed both the text and the text representing the class, and apply a distance function to both, discarding the sample if the distance is over some threshold.</p>
<p>Is this feasible in a zero-shot setting? If so, how should I go about figuring out the threshold? If not, what (if any) methods could be used in a zero-shot setting?</p>
<p>Note: I'm not looking for any methods that require additional data outside of the sample and the classes themselves.</p>
","nlp"
"126535","using llama2 on windows","2024-01-22 17:19:13","","0","11","<nlp><llm>","<p>does llama2 work by downloading the .bin file locally and running on CPU without install llama.cpp or any other dependencies. I am using theblokes llama2 models</p>
","nlp"
"126514","How the retriever model (Query encoder) is end-to-end trained in Retrieval Augmented Generation (RAG)?","2024-01-21 08:47:59","","0","150","<nlp><generative-models><information-retrieval>","<p><a href=""https://i.sstatic.net/c1w64.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/c1w64.png"" alt=""RAG architecture"" /></a></p>
<p>RAG architecture from <a href=""https://arxiv.org/abs/2005.11401v4"" rel=""nofollow noreferrer"">the original paper</a></p>
<p>Since loss is calculated at the output layer of the generator, how the gradients are back propagated to the retriever model?</p>
<p>Because the input to the Generator is pure text i.e. text of retrieved document + question.</p>
<p>It states that the entire architecture is fine-tuned end-to-end. Notice in the figure &quot;end-to-end backprop through q&quot; where q is the Query Encoder. My question is how does error is back propagated to q? Because while calculating the loss at the output of Generator, the Query Encoder model plays no role.</p>
","nlp"
"126503","unable to download llama2 weights because of http error 416","2024-01-20 14:20:21","","0","94","<nlp><llm>","<p>i am trying to download the weights for llama2 7b-chat but i always end up with &quot;Connecting to download.llamameta.net (52.84.205.116:443)
wget: server returned error: HTTP/1.1 416 Requested Range Not Satisfiable&quot;
i have requested the url multiple times but this doesnt resolve the issue.</p>
","nlp"
"126478","How to perform inference on a finetuned falcon 7b model fine tuned on open assistant dataset","2024-01-18 16:08:27","","0","17","<nlp><transformer><transfer-learning><huggingface><llm>","<p>I finetuned a falcon 7b model on the open assistant dataset using the official colab notebook provided by huggingface at <a href=""https://colab.research.google.com/drive/1BiQiw31DT7-cDp1-0ySXvvhzqomTdI-o?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1BiQiw31DT7-cDp1-0ySXvvhzqomTdI-o?usp=sharing</a>
How do i perform inference on it?
A sample row from the dataset it has been finetuned on is</p>
<p>''### Human: Can you write a short introduction about the relevance of the term &quot;monopsony&quot; in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.### Assistant: &quot;Monopsony&quot; refers to a market structure where there is only one buyer for a particular good or service. In economics, this term is particularly relevant in the labor market, where a monopsony employer has significant power over the wages and working conditions of their employees. The presence of a monopsony can result in lower wages and reduced employment opportunities for workers, as the employer has little incentive to increase wages or provide better working conditions. Recent research has identified potential monopsonies in industries such as retail and fast food, where a few large companies control a significant portion of the market (Bivens &amp; Mishel, 2013). In these industries, workers often face low wages, limited benefits, and reduced bargaining power, leading to a situation where they are dependent on the employer for their livelihood. This dependence can result in further suppression of wages and a decline in working conditions. Overall, the concept of monopsony is essential to understanding the dynamics of labor markets and the impact of market power on workers. Further research is needed to understand the extent and impact of monopsonies on the economy and to develop policies to address this issue. References: Bivens, J., &amp; Mishel, L. (2013). The Pay of Corporate Executives and Financial Professionals as Evidence of Rents in Top 1 Percent Incomes. Journal of Economic Perspectives, 27(3), 57-78.### Human: Now explain it to a dog''</p>
<p>What kind of prompt do i give this model for inference?
Should i use tags like Human and Assistant</p>
","nlp"
"126431","Falcon-7B llm giving random output","2024-01-15 14:21:16","126434","1","45","<nlp><transformer><huggingface><gpt><llm>","<p>I am using a falcon 7B model for a chatbot without any finetuning with the following code</p>
<pre><code>model_name = &quot;ybelkada/falcon-7b-sharded-bf16&quot;

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type=&quot;nf4&quot;,
    bnb_4bit_compute_dtype=torch.float16,
)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    trust_remote_code=True
)
model.config.use_cache = False
from transformers import pipeline

generator = pipeline(
    &quot;text-generation&quot;,
    model=model,
    tokenizer=&quot;gpt2&quot;,
)

result = generator(&quot;Hi&quot;)
print(result)
</code></pre>
<p>the result isnt as expected and it outputs
[{'generated_text': 'Hi8\x10=:AHi8\x10&gt;Hi8\x10&gt;:AHi8\x10?'}].
How can i fix this and make it output a proper response</p>
","nlp"
"126430","Why does Mistral model or in general Large language models have very low percentage of trainable parameters compared to total number of parameters of?","2024-01-15 12:56:08","","0","30","<nlp><pytorch><llm>","<p>I am using the below function to print the trainable parameters. I am getting this output:</p>
<blockquote>
<p>trainable params: 262410240 || all params: 7241732096 || trainable%: 3.6235839233122604</p>
</blockquote>
<pre><code>
def print_trainable_parameters(model):
    &quot;&quot;&quot;
    Prints the number of trainable parameters in the model.
    &quot;&quot;&quot;
    trainable_params = 0
    all_param = 0
    for param_name, param in model.named_parameters():
        all_param += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
            print(param_name)
    print(
        f&quot;trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}&quot;
    )



device = torch.device(&quot;cuda:0&quot;) # the device to load the model onto
model = MistralForCausalLM.from_pretrained(model_path,torch_dtype=torch.float16,load_in_8bit=True,trust_remote_code=True)

</code></pre>
","nlp"
"126397","predict if news article belong to specific category or not?","2024-01-12 22:42:04","","0","9","<machine-learning><classification><nlp><svm><text-classification>","<p>I am still new to machine learning. I am trying to build an ML model to predict if an article belongs to a category or not.</p>
<p>for example, I have three categories : [war, politics, and crime].</p>
<p>I choose a category I want to check if the news article belongs to it or no
and the model predicts if the article belongs to this category [yes/no]</p>
<p>i think about two approaches :</p>
<ol>
<li>make three binary classifiers, a classifier for each category,
predict [yes belong/no(not belong)]</li>
<li>one classifier that predicts the category of the article.</li>
</ol>
<p>my questions:</p>
<ol>
<li>which approach is better for me ? (the one which gives the highest accuracy ?)</li>
<li>how does the dataset of approach 1 look? maybe a file that includes two columns [article content, category] and category values be [1,0] (1 means yes belongs to this category)</li>
<li>if I use approach two, How to handle the case That article does not belong to any of my categories. (should I check probabilities and set a threshold)</li>
</ol>
","nlp"
"126391","Is this the correct way to calculate word embeddings using Roberta?","2024-01-12 19:47:42","126392","1","183","<machine-learning><python><deep-learning><nlp><pytorch>","<p>I'm trying to write a program that using Roberta to calculate word embeddings:</p>
<pre><code>from transformers import RobertaModel, RobertaTokenizer
import torch

model = RobertaModel.from_pretrained('roberta-base')
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
caption = &quot;this bird is yellow has red wings&quot;

encoded_caption = tokenizer(caption, return_tensors='pt')
input_ids = encoded_caption['input_ids']

outputs = model(input_ids)
word_embeddings = outputs.last_hidden_state
</code></pre>
<p>I extract the last hidden state after forwarding the <code>input_ids</code> to the <code>RobertaModel</code> class to calculate word embeddings, I don't know if this is the correct way to do this, can anyone help me confirm this ? Thanks</p>
","nlp"
"126371","Recommendation system NLP ideas","2024-01-11 16:28:40","","0","32","<machine-learning><classification><nlp><word-embeddings><text-classification>","<p><strong>The problem</strong>:</p>
<p>If we have a clustering problem with lets say x groups. And each group has a document describing it, lets say 3 pages. Then we have n observations each with a smaller piece of text describing it, 1 or 2 sentences.
And we want to allocate these observations to one of the groups.</p>
<p><strong>Ideas</strong>:</p>
<p>What would be a good technique to use for this problem. Would creating text embeddings for the larger and smaller pieces of text, and then computing their similarities be reasonable?</p>
<p>And if so are there any text embedding techniques you recommend?</p>
","nlp"
"126334","NLP Project on Multiclassification","2024-01-09 14:55:29","","0","20","<machine-learning><python><nlp>","<p>I am fresher and i am working on multi classification project in my organization i am unable to get good accuracy… the project is basically a email classification into 60+ teams… But the input data is into various language can be german , english, spanish likewise, Is anyone can guide me to get the output of the project.</p>
","nlp"
"126326","What does positive value in LIME mean?","2024-01-08 22:58:57","","0","18","<nlp><interpretation><lime>","<p>I'm exploring the workings of <strong>LIME</strong> in <strong>NLP</strong> models to understand how it elucidates positive and negative words.</p>
<p>I possess a trove of documents resembling this excerpt:</p>
<blockquote>
<p>The UN children’s agency says the 'world cannot stand by and watch' the suffering in Gaza.</p>
</blockquote>
<blockquote>
<p>'Intensifying conflict, malnutrition, and disease in Gaza are creating a deadly cycle that is threatening over 1.1 million children,' UNICEF said in a social media post.</p>
</blockquote>
<blockquote>
<p>At least 249 Palestinians have been killed and 510 wounded in the previous 24 hours in Gaza, the health ministry says.&quot;</p>
</blockquote>
<p>My classification model distinguishes each document as either &quot;<strong>United Nation Related</strong>&quot; or &quot;<strong>NON-UN Related</strong>&quot; denoted by labels <strong>1</strong> (United Nation Related) or <strong>0</strong> (NON-UN).</p>
<p>Here's a snippet of the code implementation:</p>
<pre class=""lang-py prettyprint-override""><code>exp = explainer.explain_instance(X_test.values[i], clf.predict_proba, num_features=LIMEMaxFeatures)
lst = exp.as_list()
</code></pre>
<p>Initially, everything seems fine. However, an issue arises when employing LIME for certain documents classified as NON-UN Related.</p>
<p>Consider this example:</p>
<blockquote>
<p>&quot;Guterres invoked this responsibility, saying he believed the
situation in Israel and the occupied Palestinian territories, 'may
aggravate existing threats to the maintenance of international peace
and security'.&quot;</p>
</blockquote>
<p>Despite this text, my model classifies it as <strong>NON-UN</strong> Related. Upon using LIME to delve into the reasons behind this classification, the results are as follows:</p>
<pre><code>    Word        Value
    occupied    -0.130118107160623
    situation   -0.284915997715762
    Guterres    0.22668070156952
    Gaza        0.144198872750898
</code></pre>
<p>My query pertains to the word &quot;Guterres,&quot; where the LIME value is <strong>POSITIVE</strong>. Does this imply that &quot;Guterres&quot; supports the decision of label <strong>0</strong> prediction (NON-UN Related)? <em>In essence, does a higher value for &quot;Guterres&quot; signify the model's stronger confidence in labeling the document as NON-UN Related?</em></p>
<p>Alternatively, does the <strong>POSITIVE</strong> value for &quot;Guterres&quot; signify a tendency towards label <strong>1</strong> (United Nation Related)? <em>Does a higher positive value for &quot;Guterres&quot; indicate a stronger inclination towards labeling the document as United Nation Related?</em></p>
","nlp"
"126324","Why does cross-attention in an NMT decoder use the encoder embeddings as values?","2024-01-08 15:02:29","","2","75","<nlp><transformer><embeddings><attention-mechanism><machine-translation>","<p>In the <a href=""https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf"" rel=""nofollow noreferrer"">Vaswani 2017</a> paper introducing encoder-decoder transformers, the cross-attention step in the decoder is visualised as follows:</p>
<p><a href=""https://i.sstatic.net/8I3vo.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/8I3vo.png"" alt=""cross-attention"" /></a></p>
<p>Because keys and values are always taken to be equal, this figure implies that the <em>final encoder embeddings</em> are used as keys and values, and the <em>intermediate decoder embeddings</em> are used as queries. Indeed, they write:</p>
<blockquote>
<p>In &quot;encoder-decoder attention&quot; layers, the queries come from the previous decoder layer,
and the memory keys and values come from the output of the encoder.</p>
</blockquote>
<p>What this means is that we have <span class=""math-container"">$m$</span> decoder embeddings coming in from the previous decoder block, yet counterintuitively, we don't have <span class=""math-container"">$m$</span> linear combinations of those <strong>decoder</strong> embeddings coming out of the current decoder block, but <span class=""math-container"">$m$</span> linear combinations of the <strong>encoder</strong> embeddings.</p>
<p>Although this is apparently not unheard of (<a href=""https://arxiv.org/pdf/1409.0473.pdf"" rel=""nofollow noreferrer"">Bahdanau 2015</a> sort of have the same thing by having the <em>context vector</em>, which is half of the recurrent input of the decoder, be a linear combination of the encoder embeddings), it has two very strange consequences:</p>
<ul>
<li><p>Assuming a distinct tokeniser for the target language (which isn't uncommon), the decoder has a separate embedding matrix which it indexes as its very first processing step. Its static embeddings pertain to tokens in the target language. These embeddings flow into the first decoder block. Yet, after that one decoder block, we have gone from these static <strong>target</strong>-language embeddings to a linear combination of contextualised <strong>source</strong>-language embeddings. The values in the decoder's embedding matrix are essentially thrown away after one decoder block, having contributed to nothing more than one set of dot products.</p>
</li>
<li><p>The decoder essentially <strong>regurgitates</strong> the same vectors in each block.</p>
<ul>
<li>In the encoder, a block starts out with <span class=""math-container"">$n$</span> embeddings. You then change these embeddings (with self-attention and an MLP). In the next block, you start with these changed embeddings, and change them again.</li>
<li>In the <em>decoder</em>, a block starts out with <span class=""math-container"">$m$</span> embeddings. Then you change them (with self-attention). Then you turn them into <span class=""math-container"">$m$</span> linear combinations of <span class=""math-container"">$n$</span> <em>encoder</em> embeddings. Then you change these (with an MLP). In the next block, you change these (with self-attention)... and then you go <em>back</em> to a linear combination of the <span class=""math-container"">$n$</span> <strong>old</strong> embeddings that you already transformed in the <em>previous block</em>. You keep circling back to replacing your work by (different linear combinations of) the <em>same</em> embeddings, rather than transforming them recursively.</li>
</ul>
</li>
</ul>
<p>User <em>noe</em> boiled this down to the &quot;black-boxiness&quot; of neural models in <a href=""https://datascience.stackexchange.com/a/122412/141432"">this thread</a>: if it doesn't make sense, just assume you're wrong and that the machine is right.</p>
<blockquote>
<p>[In a French-to-English model, the] keys, values and queries are not in an &quot;English representation space&quot; nor in a &quot;French representation space&quot;. Keys, vectors and queries are vectors in representation spaces that have been learned by the network during training. These representation spaces are not necessarily interpretable by a human, they were learned just to lower the loss at the task the model was trained in (i.e. to translate).</p>
</blockquote>
<p>This might handwave away the first concern (embeddings have &quot;no language&quot; even though we have different static embeddings for different languages), but the second concern requires an architectural motivation. Why do we regurgitate the encoder embeddings? <strong>Why don't we use the encoder vectors as queries to reweight a sequence of <span class=""math-container"">$m$</span> constantly evolving decoder embeddings used as keys and values?</strong> There must be a good motivation for doing it this way, right?</p>
<p>I have heard that some systems use deep encoders and shallow decoders. It isn't obvious to me that this isn't a side-effect of regurgitating the encoder embeddings. If you're not letting the decoder come up with vastly new embeddings in each block, there's no point in having a deep decoder.</p>
","nlp"
"126317","Handle multiple categorial features in character level RNN","2024-01-08 01:26:07","126319","1","35","<deep-learning><nlp><rnn>","<p>I am working on a fantasy name generator and I have 2 auxiliary categorical features (gender and race). I initially tried concatenating their one hot tensors directly into the input tensor (I think it's the most popular approach), but the model failed to differentiate between continues and categorical features (ignores the categorical features).</p>
<p>I read several similar questions and the closest ones I found were <a href=""https://datascience.stackexchange.com/questions/22340/rnn-time-series-predictions-with-multiple-features-containing-non-numeric-featur"">this</a> and <a href=""https://datascience.stackexchange.com/questions/29634/how-to-combine-categorical-and-continuous-input-features-for-neural-network-trai"">this</a>, which suggested first combining these features via a Dense layer(or multiple layers) and then concatenate Dense layer's output with input tensor (of names). One alternative approach I have found is using thi Are there any other alternate approaches ? also one question I have about this approach is that here too, ultimately the auxiliary features are concatenated with input tensor. Why should the model not ignore features with this approach ?</p>
","nlp"
"126311","Understanding the concepts of word embedding in GPT-2","2024-01-07 20:41:42","126312","1","365","<machine-learning><python><deep-learning><nlp><lstm>","<p>I have a program that calculate the word embedding using GPT-2 specifically the <code>GPT2Model</code> class:</p>
<pre><code>from transformers import GPT2Tokenizer, GPT2Model
import torch

tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2Model.from_pretrained('gpt2')
caption = &quot;this bird is yellow has red wings&quot;
encoded_caption = tokenizer(caption, return_tensors='pt')
input_ids = encoded_caption['input_ids']

with torch.no_grad():
    outputs = model(input_ids)
word_embeddings = outputs.last_hidden_state
</code></pre>
<p>I have a few questions about this:</p>
<ol>
<li><p>When calculating the word embedding using <code>outputs.last_hidden_state</code>, does this mean that the word embedding only uses the token embedding and positional embedding of GPT-2, without feeding them to the decoder blocks after that ?</p>
</li>
<li><p>Is this embedding also known as contextualized embedding ?</p>
</li>
<li><p>How does this embedding better than RNN architectures, such as LSTM or bidirectional-LSTM embedding ?</p>
</li>
</ol>
","nlp"
"126250","How to label a dataset of text pairs to use it as a universal one for calculating the precision@k metric for different models?","2024-01-02 19:33:12","","0","7","<neural-network><nlp><transformer><metric><search>","<p>I am facing a semantic search problem. I am fine tuning different NLU models and i want to use precision@k as my main metric. Is it possible to label a dataset of text pairs to use it as a universal one for calculating the precision@k metric for different NLU models? Or the only way is to label dataset after semantic search all over again for each model?</p>
","nlp"
"126224","Estimating the Cost of Creating an Image Dataset like GQA and Relevant Sources","2023-12-30 07:45:25","","0","10","<nlp><dataset><computer-vision>","<p>I am currently working on a project that requires an extensive image dataset, similar to the <a href=""https://cs.stanford.edu/people/dorarad/gqa/about.html"" rel=""nofollow noreferrer"">GQA</a> dataset. I am in the early stages of planning and would like to get an estimate of the cost involved in creating such a dataset. The GQA dataset is known for its diverse set of images annotated with questions and answers, making it a valuable resource for various computer vision tasks.</p>
<p>I would appreciate insights from the community on the following:</p>
<ol>
<li><p><strong>Cost Estimate:</strong> Can anyone provide an estimate of the cost involved in creating an image dataset like GQA? This could include expenses related to image acquisition, annotation, and any other relevant factors.</p>
</li>
<li><p><strong>Breakdown of Costs:</strong> If possible, could there be any breakdown of the costs involved in each phase of dataset creation? For instance, costs associated with obtaining high-quality images, hiring annotators, or any other significant expenses.</p>
</li>
<li><p><strong>Relevant Sources:</strong> Are there any reputable sources or case studies that discuss the costs associated with creating image datasets for computer vision tasks? I'm looking for insights that can help in better understanding the financial aspects of such projects.</p>
</li>
</ol>
<p>I understand that the costs may vary depending on several factors, but any ballpark figures or general advice on budgeting for such projects would be incredibly helpful!</p>
","nlp"
"126187","Cross-attention mask in Transformers","2023-12-27 15:44:28","126283","7","1482","<nlp><transformer><attention-mechanism><masking>","<p>I can't fully understand how we should create the mask for the decoder's cross-attention mask in the original Transformer model from <a href=""https://arxiv.org/abs/1706.03762"" rel=""noreferrer"">Attention Is All You Need</a>.
Here is my attempt at finding a solution:
Suppose we are training such Transformer model, and we are using different-length batches for the encoder and decoder, e.g. we are trying to train an <em>Italian-to-English</em> machine translation model and we have:</p>
<ol>
<li><p>The following input tokens for the Encoder:</p>
<p><code>[&lt;SOS&gt;, Mi, chiamo, Luke, &lt;EOS&gt;, &lt;PAD&gt;]</code></p>
<p><span class=""math-container"">$n_e=6$</span> (length of encoder's input)</p>
</li>
<li><p>The following input tokens for the Decoder:</p>
<p><code>[&lt;SOS&gt;, My, name, is, Luke, &lt;EOS&gt;, &lt;PAD&gt;, &lt;PAD&gt;]</code></p>
<p><span class=""math-container"">$n_d=8$</span> (length of decoder's input)</p>
</li>
</ol>
<p>We have three attentions masks:</p>
<ol>
<li><span class=""math-container"">$M_e \in \mathbb{R}^{n_e\times n_e}$</span>, Encoder's self attention mask.</li>
<li><span class=""math-container"">$M_d \in \mathbb{R}^{n_d\times n_d}$</span>, Decoder's self attention mask.</li>
<li><span class=""math-container"">$M_x \in \mathbb{R}^{n_d\times n_e}$</span>, Decoder's cross-attention.</li>
</ol>
<p>Note that for the cross-attention block, given a certain embedding dimension <span class=""math-container"">$d_m$</span>, we have that <span class=""math-container"">$Q\in\mathbb{R}^{n_d \times d_m}$</span>, <span class=""math-container"">$K\in\mathbb{R}^{n_e \times d_m}$</span>, <span class=""math-container"">$\frac{QK^T}{\sqrt{n_e}}\in\mathbb{R}^{n_d \times n_e} \to M_x \in \mathbb{R}^{n_d \times n_e}$</span></p>
<p><a href=""https://i.sstatic.net/J71IQl.pngz"" rel=""noreferrer""><img src=""https://i.sstatic.net/J71IQl.pngz"" alt=""enter image description here"" /></a></p>
<ol>
<li><p><span class=""math-container"">$M_e$</span> definition:</p>
<p>In this case we just have to apply the padding mask to the encoder's input</p>
<pre><code>mask = [       &lt;SOS&gt;     Mi Chiamo   Luke  &lt;EOS&gt;  &lt;PAD&gt;  
&lt;SOS&gt;        [     0,     0,     0,     0,     0,  -inf],
Mi           [     0,     0,     0,     0,     0,  -inf],
Chiamo       [     0,     0,     0,     0,     0,  -inf],
Luke         [     0,     0,     0,     0,     0,  -inf],
&lt;EOS&gt;        [     0,     0,     0,     0,     0,  -inf],
&lt;PAD&gt;        [     0,     0,     0,     0,     0,  -inf]
]

</code></pre>
<p>We zero-out all the elements belonging to the columns that correspond to the 
token, this way we are sure that the embeddings for the <code>&lt;PAD&gt;</code> token won't
contribute to the computation of the new <em>values</em> <span class=""math-container"">$V^{'}=\sigma(\frac{QK^T}{\sqrt{n_e}} + M_e)V$</span>. (Where <span class=""math-container"">$\sigma$</span> is the <em>softmax</em> function)</p>
</li>
<li><p><span class=""math-container"">$M_d$</span> definition:</p>
<p>In this case it should be enough to define the causal mask to the decoder's input</p>
<pre><code>mask = [       &lt;SOS&gt;     My   Name     is   Luke  &lt;EOS&gt;  &lt;PAD   &lt;PAD&gt;  
&lt;SOS&gt;        [     0,  -inf,  -inf,  -inf,  -inf,  -inf,  -inf,  -inf],
My           [     0,     0,  -inf,  -inf,  -inf,  -inf,  -inf,  -inf],
Name         [     0,     0,     0,  -inf,  -inf,  -inf,  -inf,  -inf],
is           [     0,     0,     0,     0,  -inf,  -inf,  -inf,  -inf],
Luke         [     0,     0,     0,     0,     0,  -inf,  -inf,  -inf],
&lt;EOS&gt;        [     0,     0,     0,     0,     0,     0,  -inf,  -inf],
&lt;PAD&gt;        [     0,     0,     0,     0,     0,     0,     0,  -inf],
&lt;PAD&gt;        [     0,     0,     0,     0,     0,     0,     0,     0]
]
</code></pre>
<p>We don't care about the padding mask because through the causal mask we implicitly ignore the values corresponding to the <code>&lt;PAD&gt;</code> tokens.</p>
</li>
<li><p><span class=""math-container"">$M_x$</span> definition:
I don't understand if we should combine the causal mask with the padding mask from the <em>encoder</em> output</p>
<pre><code>mask = [       &lt;SOS&gt;     Mi Chiamo   Luke  &lt;EOS&gt;  &lt;PAD&gt;  
&lt;SOS&gt;        [     0,  -inf,  -inf,  -inf,  -inf,  -inf],
My           [     0,     0,  -inf,  -inf,  -inf,  -inf],
Name         [     0,     0,     0,  -inf,  -inf,  -inf],
is           [     0,     0,     0,     0,  -inf,  -inf],
Luke         [     0,     0,     0,     0,     0,  -inf],
&lt;EOS&gt;        [     0,     0,     0,     0,     0,  -inf],
&lt;PAD&gt;        [     0,     0,     0,     0,     0,  -inf],
&lt;PAD&gt;        [     0,     0,     0,     0,     0,  -inf]
]  
</code></pre>
<p>or if we should just apply the padding mask (since the VALUES are coming from the encoder, and we should have full access over the whole encoder's input)</p>
<pre><code>mask = [       &lt;SOS&gt;     Mi Chiamo   Luke  &lt;EOS&gt;  &lt;PAD&gt;  
&lt;SOS&gt;        [     0,     0,     0,     0,     0,  -inf],
My           [     0,     0,     0,     0,     0,  -inf],
Name         [     0,     0,     0,     0,     0,  -inf],
is           [     0,     0,     0,     0,     0,  -inf],
Luke         [     0,     0,     0,     0,     0,  -inf],
&lt;EOS&gt;        [     0,     0,     0,     0,     0,  -inf],
&lt;PAD&gt;        [     0,     0,     0,     0,     0,  -inf],
&lt;PAD&gt;        [     0,     0,     0,     0,     0,  -inf]
]  
</code></pre>
<p>Is this the right way to implement the different attention masks? What's the right alternative for the cross-attention values and what's the rational behind it? Any valid and useful resource is welcome. Thank you!</p>
</li>
</ol>
<p>EDIT:
The rational behind the latter alternative, that personally makes a little more sense to me, is depicted here:</p>
<p><span class=""math-container"">$ 
   \text{Legend}\to 
   \color{orange}{\text{Decoder}} ,\  \color{green}{\text{Encoder}} \\
   \color{orange}{Q^{'}}=\sigma(\frac{\color{orange}{Q}\color{green}{K}^T}{\sqrt{n_e}} + M_x)\color{green}{V} = \\ 
\sigma\left(\color{orange}{
\tiny
\begin{bmatrix}
Q_{\text{&lt;SOS&gt;}_0} &amp; Q_{\text{&lt;SOS&gt;}_1} &amp; \dots &amp; Q_{\text{&lt;SOS&gt;}_{d_m}} \\
Q_{\text{My}_0} &amp; Q_{\text{My}_1} &amp; \dots &amp; Q_{\text{My}_{d_m}} \\
Q_{\text{Name}_0} &amp; Q_{\text{Name}_1} &amp; \dots &amp; Q_{\text{Name}_{d_m}} \\
Q_{\text{Is}_0} &amp; Q_{\text{Is}_1} &amp; \dots &amp; Q_{\text{Is}_{d_m}} \\
Q_{\text{Luke}_0} &amp; Q_{\text{Luke}_1} &amp; \dots &amp; Q_{\text{Luke}_{d_m}} \\
Q_{\text{&lt;EOS&gt;}_0} &amp; Q_{\text{&lt;EOS&gt;}_1} &amp; \dots &amp; Q_{\text{&lt;EOS&gt;}_{d_m}} \\
Q_{\text{&lt;PAD&gt;}_0} &amp; Q_{\text{&lt;PAD&gt;}_1} &amp; \dots &amp; Q_{\text{&lt;PAD&gt;}_{d_m}} \\
Q_{\text{&lt;PAD&gt;}_0} &amp; Q_{\text{&lt;PAD&gt;}_1} &amp; \dots &amp; Q_{\text{&lt;PAD&gt;}_{d_m}} 
\end{bmatrix}
}
\color{green}{
\tiny
\begin{bmatrix}
K_{\text{&lt;SOS&gt;}_0} &amp; K_{\text{Mi}_0} &amp; K_{\text{Chiamo}_0} &amp; K_{\text{Luke}_0} &amp; K_{\text{&lt;EOS&gt;}_0} &amp; K_{\text{&lt;PAD&gt;}_0} \\
K_{\text{&lt;SOS&gt;}_1} &amp; K_{\text{Mi}_1} &amp; K_{\text{Chiamo}_1} &amp; K_{\text{Luke}_1} &amp; K_{\text{&lt;EOS&gt;}_1} &amp; K_{\text{&lt;PAD&gt;}_1} \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
K_{\text{&lt;SOS&gt;}_{d_m}} &amp; K_{\text{Mi}_{d_m}} &amp; K_{\text{Chiamo}_{d_m}} &amp; K_{\text{Luke}_{d_m}} &amp; K_{\text{&lt;EOS&gt;}_{d_m}} &amp; K_{\text{&lt;PAD&gt;}_{d_m}}
\end{bmatrix}
}\cdot\frac{1}{\sqrt{n_e}} + M_x\right)\color{green}{V} = 
    $</span>
<span class=""math-container"">$
\sigma\left(
{
\tiny
\begin{bmatrix}
\color{orange}{\text{&lt;SOS&gt;}}\cdot\color{green}{\text{&lt;SOS&gt;}} &amp; 
\color{orange}{\text{&lt;SOS&gt;}}\cdot\color{green}{\text{Mi}} &amp; 
\color{orange}{\text{&lt;SOS&gt;}}\cdot\color{green}{\text{Chiamo}} &amp; 
\color{orange}{\text{&lt;SOS&gt;}}\cdot\color{green}{\text{Luke}} &amp; 
\color{orange}{\text{&lt;SOS&gt;}}\cdot\color{green}{\text{&lt;EOS&gt;}} &amp; 
\color{orange}{\text{&lt;SOS&gt;}}\cdot\color{green}{\text{&lt;PAD&gt;}} \\
\color{orange}{\text{My}}\cdot\color{green}{\text{&lt;SOS&gt;}} &amp; 
\color{orange}{\text{My}}\cdot\color{green}{\text{Mi}} &amp; 
\color{orange}{\text{My}}\cdot\color{green}{\text{Chiamo}} &amp; 
\color{orange}{\text{My}}\cdot\color{green}{\text{Luke}} &amp; 
\color{orange}{\text{My}}\cdot\color{green}{\text{&lt;EOS&gt;}} &amp; 
\color{orange}{\text{My}}\cdot\color{green}{\text{&lt;PAD&gt;}} \\
\color{orange}{\text{Name}}\cdot\color{green}{\text{&lt;SOS&gt;}} &amp; 
\color{orange}{\text{Name}}\cdot\color{green}{\text{Mi}} &amp; 
\color{orange}{\text{Name}}\cdot\color{green}{\text{Chiamo}} &amp; 
\color{orange}{\text{Name}}\cdot\color{green}{\text{Luke}} &amp; 
\color{orange}{\text{Name}}\cdot\color{green}{\text{&lt;EOS&gt;}} &amp; 
\color{orange}{\text{Name}}\cdot\color{green}{\text{&lt;PAD&gt;}} \\
\color{orange}{\text{Is}}\cdot\color{green}{\text{&lt;SOS&gt;}} &amp; 
\color{orange}{\text{Is}}\cdot\color{green}{\text{Mi}} &amp; 
\color{orange}{\text{Is}}\cdot\color{green}{\text{Chiamo}} &amp; 
\color{orange}{\text{Is}}\cdot\color{green}{\text{Luke}} &amp; 
\color{orange}{\text{Is}}\cdot\color{green}{\text{&lt;EOS&gt;}} &amp; 
\color{orange}{\text{Is}}\cdot\color{green}{\text{&lt;PAD&gt;}} \\
\color{orange}{\text{Luke}}\cdot\color{green}{\text{&lt;SOS&gt;}} &amp; 
\color{orange}{\text{Luke}}\cdot\color{green}{\text{Mi}} &amp; 
\color{orange}{\text{Luke}}\cdot\color{green}{\text{Chiamo}} &amp; 
\color{orange}{\text{Luke}}\cdot\color{green}{\text{Luke}} &amp; 
\color{orange}{\text{Luke}}\cdot\color{green}{\text{&lt;EOS&gt;}} &amp; 
\color{orange}{\text{Luke}}\cdot\color{green}{\text{&lt;PAD&gt;}} \\
\color{orange}{\text{&lt;EOS&gt;}}\cdot\color{green}{\text{&lt;SOS&gt;}} &amp; 
\color{orange}{\text{&lt;EOS&gt;}}\cdot\color{green}{\text{Mi}} &amp; 
\color{orange}{\text{&lt;EOS&gt;}}\cdot\color{green}{\text{Chiamo}} &amp; 
\color{orange}{\text{&lt;EOS&gt;}}\cdot\color{green}{\text{Luke}} &amp; 
\color{orange}{\text{&lt;EOS&gt;}}\cdot\color{green}{\text{&lt;EOS&gt;}} &amp; 
\color{orange}{\text{&lt;EOS&gt;}}\cdot\color{green}{\text{&lt;PAD&gt;}} \\
\color{orange}{\text{&lt;PAD&gt;}}\cdot\color{green}{\text{&lt;SOS&gt;}} &amp; 
\color{orange}{\text{&lt;PAD&gt;}}\cdot\color{green}{\text{Mi}} &amp; 
\color{orange}{\text{&lt;PAD&gt;}}\cdot\color{green}{\text{Chiamo}} &amp; 
\color{orange}{\text{&lt;PAD&gt;}}\cdot\color{green}{\text{Luke}} &amp; 
\color{orange}{\text{&lt;PAD&gt;}}\cdot\color{green}{\text{&lt;EOS&gt;}} &amp; 
\color{orange}{\text{&lt;PAD&gt;}}\cdot\color{green}{\text{&lt;PAD&gt;}} \\
\color{orange}{\text{&lt;PAD&gt;}}\cdot\color{green}{\text{&lt;SOS&gt;}} &amp; 
\color{orange}{\text{&lt;PAD&gt;}}\cdot\color{green}{\text{Mi}} &amp; 
\color{orange}{\text{&lt;PAD&gt;}}\cdot\color{green}{\text{Chiamo}} &amp; 
\color{orange}{\text{&lt;PAD&gt;}}\cdot\color{green}{\text{Luke}} &amp; 
\color{orange}{\text{&lt;PAD&gt;}}\cdot\color{green}{\text{&lt;EOS&gt;}} &amp; 
\color{orange}{\text{&lt;PAD&gt;}}\cdot\color{green}{\text{&lt;PAD&gt;}} 
\end{bmatrix}}\cdot\frac{1}{\sqrt{n_e}} + M_x
\right)\tiny{
\color{green}{
\begin{bmatrix}
V_{\text{&lt;SOS&gt;}_0} &amp; V_{\text{&lt;SOS&gt;}_1} &amp; \dots &amp; V_{\text{&lt;SOS&gt;}_{d_m}} \\
V_{\text{Mi}_0} &amp; V_{\text{Mi}_1} &amp; \dots &amp; V_{\text{Mi}_{d_m}} \\
V_{\text{Chiamo}_0} &amp; V_{\text{Chiamo}_1} &amp; \dots &amp; V_{\text{Chiamo}_{d_m}} \\
V_{\text{Luke}_0} &amp; V_{\text{Luke}_1} &amp; \dots &amp; V_{\text{Luke}_{d_m}} \\
V_{\text{&lt;EOS&gt;}_0} &amp; V_{\text{&lt;EOS&gt;}_1} &amp; \dots &amp; V_{\text{&lt;EOS&gt;}_{d_m}} \\
V_{\text{&lt;PAD&gt;}_0} &amp; V_{\text{&lt;PAD&gt;}_1} &amp; \dots &amp; V_{\text{&lt;PAD&gt;}_{d_m}}
\end{bmatrix}}
}=
\color{orange}{
\tiny
\begin{bmatrix}
Q^{'}_{\text{&lt;SOS&gt;}_0} &amp; Q^{'}_{\text{&lt;SOS&gt;}_1} &amp; \dots &amp; Q^{'}_{\text{&lt;SOS&gt;}_{d_m}} \\
Q^{'}_{\text{My}_0} &amp; Q^{'}_{\text{My}_1} &amp; \dots &amp; Q^{'}_{\text{My}_{d_m}} \\
Q^{'}_{\text{Name}_0} &amp; Q^{'}_{\text{Name}_1} &amp; \dots &amp; Q^{'}_{\text{Name}_{d_m}} \\
Q^{'}_{\text{Is}_0} &amp; Q^{'}_{\text{Is}_1} &amp; \dots &amp; Q^{'}_{\text{Is}_{d_m}} \\
Q^{'}_{\text{Luke}_0} &amp; Q^{'}_{\text{Luke}_1} &amp; \dots &amp; Q^{'}_{\text{Luke}_{d_m}} \\
Q^{'}_{\text{&lt;EOS&gt;}_0} &amp; Q^{'}_{\text{&lt;EOS&gt;}_1} &amp; \dots &amp; Q^{'}_{\text{&lt;EOS&gt;}_{d_m}} \\
Q^{'}_{\text{&lt;PAD&gt;}_0} &amp; Q^{'}_{\text{&lt;PAD&gt;}_1} &amp; \dots &amp; Q^{'}_{\text{&lt;PAD&gt;}_{d_m}} \\
Q^{'}_{\text{&lt;PAD&gt;}_0} &amp; Q^{'}_{\text{&lt;PAD&gt;}_1} &amp; \dots &amp; Q^{'}_{\text{&lt;PAD&gt;}_{d_m}} 
\end{bmatrix}
}
   $</span></p>
<p>Our constraint on the newly obtained <span class=""math-container"">$\color{orange}{Q^{'}}$</span> values is expressed below:</p>
<p><span class=""math-container"">$
\color{orange}{
\tiny
\begin{bmatrix}
Q^{'}_{\text{&lt;SOS&gt;}_0} &amp; Q^{'}_{\text{&lt;SOS&gt;}_1} &amp; \dots &amp; Q^{'}_{\text{&lt;SOS&gt;}_{d_m}}\\
Q^{'}_{\text{My}_0} &amp; Q^{'}_{\text{My}_1} &amp; \dots &amp; Q^{'}_{\text{My}_{d_m}} \\
Q^{'}_{\text{Name}_0} &amp; Q^{'}_{\text{Name}_1} &amp; \dots &amp; Q^{'}_{\text{Name}_{d_m}} \\
Q^{'}_{\text{Is}_0} &amp; Q^{'}_{\text{Is}_1} &amp; \dots &amp; Q^{'}_{\text{Is}_{d_m}} \\
Q^{'}_{\text{Luke}_0} &amp; Q^{'}_{\text{Luke}_1} &amp; \dots &amp; Q^{'}_{\text{Luke}_{d_m}} \\
Q^{'}_{\text{&lt;EOS&gt;}_0} &amp; Q^{'}_{\text{&lt;EOS&gt;}_1} &amp; \dots &amp; Q^{'}_{\text{&lt;EOS&gt;}_{d_m}} \\
Q^{'}_{\text{&lt;PAD&gt;}_0} &amp; Q^{'}_{\text{&lt;PAD&gt;}_1} &amp; \dots &amp; Q^{'}_{\text{&lt;PAD&gt;}_{d_m}} \\
Q^{'}_{\text{&lt;PAD&gt;}_0} &amp; Q^{'}_{\text{&lt;PAD&gt;}_1} &amp; \dots &amp; Q^{'}_{\text{&lt;PAD&gt;}_{d_m}} 
\end{bmatrix}
}
\color{black}{
\tiny
\begin{matrix}
\to\text{Should only contain information from }\color{orange}{Q_\text{&lt;SOS&gt;}} \color{white}{,\text{My}} \color{white}{\text{Name,}} \color{white}{\text{Is,}} \color{white}{\text{Luke,}}\color{white}{Q^{'}_{\text{&lt;PAD&gt;}_{d_m}}}\ \ \ \ \ \ \ \ \ \ \ \\
\to\text{Should only contain information from }\color{orange}{Q_{\text{&lt;SOS&gt;}}}, \color{orange}{Q_\text{My}} \color{white}{\text{Name,,}} \color{white}{\text{Is,}} \color{white}{\text{Luke,}}\color{white}{Q^{'}_{\text{&lt;PAD&gt;}_{d_m}}}\ \ \ \ \ \ \ \\
\to\text{Should only contain information from }\color{orange}{Q_{\text{&lt;SOS&gt;}}}, \color{orange}{Q_{\text{My}}}, \color{orange}{Q_\text{Name}} \color{white}{\text{Is,,}} \color{white}{\text{Luke,}}\color{white}{Q^{'}_{\text{&lt;PAD&gt;}_{d_m}}}\ \ \ \ \ \ \\
\to\text{Should only contain information from }\color{orange}{Q_{\text{&lt;SOS&gt;}}}, \color{orange}{Q_{\text{My}}}, \color{orange}{Q_{\text{Name}}}, \color{orange}{Q_\text{Is}} \color{white}{\text{Luke,}}\color{white}{Q^{'}_{\text{&lt;PAD&gt;}_{d_m}}}\ \ \ \ \\
\to\text{Should only contain information from }\color{orange}{Q_{\text{&lt;SOS&gt;}}}, \color{orange}{Q_{\text{My}}}, \color{orange}{Q_{\text{Name}}}, \color{orange}{Q_{\text{Is}}}, \color{orange}{Q_\text{Luke}}\color{white}{Q^{'}_{\text{&lt;PAD&gt;}_{d_m}}}\ \ \\
\to\text{Should only contain information from }\color{orange}{Q_{\text{&lt;SOS&gt;}}}, \color{orange}{Q_{\text{My}}}, \color{orange}{Q_{\text{Name}}}, \color{orange}{Q_{\text{Is}}}, \color{orange}{Q_{\text{Luke}}}, \color{orange}{Q_{\text{&lt;EOS&gt;}}}\ \  \\
\to\text{We don't care}\color{white}{Q^{'}_{\text{&lt;PAD&gt;}_{d_m}}}\\
\to\text{We don't care}\color{white}{Q^{'}_{\text{&lt;PAD&gt;}_{d_m}}}
\end{matrix}
}
   $</span></p>
<p>And I see no reason to define a causal mask given that each row in <span class=""math-container"">$\color{orange}{Q}\color{green}{K}^T$</span> contains information about the corresponding token in the decoder (i.e. the first row contains information about the first token <span class=""math-container"">$\color{orange}{\text{&lt;SOS&gt;}}$</span>, the second row contains information about the second token <span class=""math-container"">$\color{orange}{\text{My}}$</span>, and so on...)</p>
<p><span class=""math-container"">$
{
\tiny
\begin{bmatrix}
\color{orange}{\text{&lt;SOS&gt;}}\cdot\color{green}{\text{&lt;SOS&gt;}} &amp; 
\color{orange}{\text{&lt;SOS&gt;}}\cdot\color{green}{\text{Mi}} &amp; 
\color{orange}{\text{&lt;SOS&gt;}}\cdot\color{green}{\text{Chiamo}} &amp; 
\color{orange}{\text{&lt;SOS&gt;}}\cdot\color{green}{\text{Luke}} &amp; 
\color{orange}{\text{&lt;SOS&gt;}}\cdot\color{green}{\text{&lt;EOS&gt;}} &amp; 
\color{grey}{\text{&lt;SOS&gt;}}\cdot\color{grey}{\text{&lt;PAD&gt;}} \\
\color{orange}{\text{My}}\cdot\color{green}{\text{&lt;SOS&gt;}} &amp; 
\color{orange}{\text{My}}\cdot\color{green}{\text{Mi}} &amp; 
\color{orange}{\text{My}}\cdot\color{green}{\text{Chiamo}} &amp; 
\color{orange}{\text{My}}\cdot\color{green}{\text{Luke}} &amp; 
\color{orange}{\text{My}}\cdot\color{green}{\text{&lt;EOS&gt;}} &amp; 
\color{grey}{\text{My}}\cdot\color{grey}{\text{&lt;PAD&gt;}} \\
\color{orange}{\text{Name}}\cdot\color{green}{\text{&lt;SOS&gt;}} &amp; 
\color{orange}{\text{Name}}\cdot\color{green}{\text{Mi}} &amp; 
\color{orange}{\text{Name}}\cdot\color{green}{\text{Chiamo}} &amp; 
\color{orange}{\text{Name}}\cdot\color{green}{\text{Luke}} &amp; 
\color{orange}{\text{Name}}\cdot\color{green}{\text{&lt;EOS&gt;}} &amp; 
\color{grey}{\text{Name}}\cdot\color{grey}{\text{&lt;PAD&gt;}} \\
\color{orange}{\text{Is}}\cdot\color{green}{\text{&lt;SOS&gt;}} &amp; 
\color{orange}{\text{Is}}\cdot\color{green}{\text{Mi}} &amp; 
\color{orange}{\text{Is}}\cdot\color{green}{\text{Chiamo}} &amp; 
\color{orange}{\text{Is}}\cdot\color{green}{\text{Luke}} &amp; 
\color{orange}{\text{Is}}\cdot\color{green}{\text{&lt;EOS&gt;}} &amp; 
\color{grey}{\text{Is}}\cdot\color{grey}{\text{&lt;PAD&gt;}} \\
\color{orange}{\text{Luke}}\cdot\color{green}{\text{&lt;SOS&gt;}} &amp; 
\color{orange}{\text{Luke}}\cdot\color{green}{\text{Mi}} &amp; 
\color{orange}{\text{Luke}}\cdot\color{green}{\text{Chiamo}} &amp; 
\color{orange}{\text{Luke}}\cdot\color{green}{\text{Luke}} &amp; 
\color{orange}{\text{Luke}}\cdot\color{green}{\text{&lt;EOS&gt;}} &amp; 
\color{grey}{\text{Luke}}\cdot\color{grey}{\text{&lt;PAD&gt;}} \\
\color{orange}{\text{&lt;EOS&gt;}}\cdot\color{green}{\text{&lt;SOS&gt;}} &amp; 
\color{orange}{\text{&lt;EOS&gt;}}\cdot\color{green}{\text{Mi}} &amp; 
\color{orange}{\text{&lt;EOS&gt;}}\cdot\color{green}{\text{Chiamo}} &amp; 
\color{orange}{\text{&lt;EOS&gt;}}\cdot\color{green}{\text{Luke}} &amp; 
\color{orange}{\text{&lt;EOS&gt;}}\cdot\color{green}{\text{&lt;EOS&gt;}} &amp; 
\color{grey}{\text{&lt;EOS&gt;}}\cdot\color{grey}{\text{&lt;PAD&gt;}} \\
\color{orange}{\text{&lt;PAD&gt;}}\cdot\color{green}{\text{&lt;SOS&gt;}} &amp; 
\color{orange}{\text{&lt;PAD&gt;}}\cdot\color{green}{\text{Mi}} &amp; 
\color{orange}{\text{&lt;PAD&gt;}}\cdot\color{green}{\text{Chiamo}} &amp; 
\color{orange}{\text{&lt;PAD&gt;}}\cdot\color{green}{\text{Luke}} &amp; 
\color{orange}{\text{&lt;PAD&gt;}}\cdot\color{green}{\text{&lt;EOS&gt;}} &amp; 
\color{grey}{\text{&lt;PAD&gt;}}\cdot\color{grey}{\text{&lt;PAD&gt;}} \\
\color{orange}{\text{&lt;PAD&gt;}}\cdot\color{green}{\text{&lt;SOS&gt;}} &amp; 
\color{orange}{\text{&lt;PAD&gt;}}\cdot\color{green}{\text{Mi}} &amp; 
\color{orange}{\text{&lt;PAD&gt;}}\cdot\color{green}{\text{Chiamo}} &amp; 
\color{orange}{\text{&lt;PAD&gt;}}\cdot\color{green}{\text{Luke}} &amp; 
\color{orange}{\text{&lt;PAD&gt;}}\cdot\color{green}{\text{&lt;EOS&gt;}} &amp; 
\color{grey}{\text{&lt;PAD&gt;}}\cdot\color{grey}{\text{&lt;PAD&gt;}}
\end{bmatrix}}
$</span></p>
","nlp"
"126176","Email Parsing using Machine Learning","2023-12-26 07:26:02","","0","553","<machine-learning><nlp><beginner>","<p>I am new to machine learning. I have a project in which i need to extract some data from email by parsing email using machine learning and  would really appreciate if you could guide me with that. I am a bit confused where to start and how to start, kindly help.</p>
<p>Thanks.</p>
","nlp"
"126172","How to find proper context in open book question answering?","2023-12-25 15:33:13","","0","64","<machine-learning><nlp><information-retrieval><llm><question-answering>","<p>I want to make an <a href=""https://huggingface.co/tasks/question-answering"" rel=""nofollow noreferrer"">Open Book Question Answering</a> / <a href=""https://ai.meta.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/"" rel=""nofollow noreferrer"">Retrieval Augmented Generation</a> system. The major concern here is the proper context selection. There are some fundamental issues related to this. For example, you want to make a chatbot for an E-commerce site. So, you have downloaded the crawl of the whole site, and the crawl looks something like this:</p>
<pre><code>Product Name: ABC
Description: [A moderately large description]
Price: $100

Product Name: DEF
Description: [A moderately large description]
Price: $200

...
</code></pre>
<p>Now, you have to chunk the whole corpus and generate embeddings for each of them. The difficulties start from here. You cannot generate embedding for an arbitrarily large text. Also, even if you could do so, it would have a performance hit, as the passage will no longer be anything specific. Now, in the depicted scenario, after chunking there can be thousands of contexts that are very similar although they are referring to different products. Now during conversation, how do you ensure you are fetching the appropriate context for the correct product? As, after chunking the whole crawled data, the chunked passages may not contain any reference to the product at all and for many products, the passages can become almost equivalent.</p>
<p>A similar situation happens when you work on documents. Like if you have a PDF content like:</p>
<pre><code>An Overview of Machine Learning Techniques

.........

SVM
[A description of SVM]

Working Example
[An implementation of it]

Pros
[Benefit of using this algorithm]

Cons
[Describes when this is not applicable]

.........

Neural Network
[A description of Neural Network]

Working Example
[An implementation of it]

Pros
[Benefit of using this algorithm]

Cons
[Describes when this is not applicable]

</code></pre>
<p>Let's assume, someone is chatting on a Neural Network topic. Now the user is asking the bot about its pros and cons. Now, there is no guarantee that the Pros and Cons related sections will contain the term &quot;Neural Network&quot; in them. The distance between the section and the last time the term &quot;Neural Network&quot; was mentioned can be so large that they cannot be inside of a single context. How do you handle this case? How do you carry forward the topic information on which the subsequent contexts are talking about? Also, there are some catches. There can be multiple levels of hierarchies (section, subsection, etc.), choosing the higher level can make the context too generalized, and choosing the lower level can make it too specialized. Taking all of them into consideration without hierarchy can be misleading.</p>
<p>I have for example tried the <code>PyMuPDF</code> library to parse the headers and bold texts from a PDF. My target is to attach the most recent header to every other chunk where there is no header available. But in reality, you cannot always assume this will work. Even this approach fails in my local testing when I want to apply them to PDFs in the wild. Either the selected topic headers are too large in number or too less in number or they are simply generated as an artifact as people may not follow standard while writing documents, they can use those headers in inappropriate ways.</p>
<p>I have been searching through the Internet for weeks, but most of the tutorials are only addressing the happy path, and almost no one is discussing this real issue that will arise when you want to apply it in wild situations.</p>
<p>Is there any solution to this problem? Or is there any appropriate data structure to handle this kind of data? Is not Retrieval Augmented Generation applicable to this scenario? Any paper, algorithm, tutorial, or idea relevant to it will be helpful. Thanks.</p>
","nlp"
"126142","The using of golden dataset in Augmented SBERT Training","2023-12-21 19:43:34","","0","16","<nlp><training><transformer><bert><autoencoder>","<p>I use the training strategy of <a href=""https://www.sbert.net/examples/training/data_augmentation/README.html"" rel=""nofollow noreferrer"">Augmented SBERT (Domain-Transfer)</a>. In the code example they use the golden-dataset (STSb) for the training evaluator. Here two code snippets of the <a href=""https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/data_augmentation/train_sts_indomain_semantic.py"" rel=""nofollow noreferrer"">example of sentence-transformers</a>:</p>
<p><em>Get data and split them</em></p>
<pre class=""lang-py prettyprint-override""><code>gold_samples = []
dev_samples = []
test_samples = []

with gzip.open(sts_dataset_path, 'rt', encoding='utf8') as fIn:
    reader = csv.DictReader(fIn, delimiter='\t', quoting=csv.QUOTE_NONE)
    for row in reader:
        score = float(row['score']) / 5.0  # Normalize score to range 0 ... 1

        if row['split'] == 'dev':
            dev_samples.append(InputExample(texts=[row['sentence1'], row['sentence2']], label=score))
        elif row['split'] == 'test':
            test_samples.append(InputExample(texts=[row['sentence1'], row['sentence2']], label=score))
        else:
            #As we want to get symmetric scores, i.e. CrossEncoder(A,B) = CrossEncoder(B,A), we pass both combinations to the train set
            gold_samples.append(InputExample(texts=[row['sentence1'], row['sentence2']], label=score))
            gold_samples.append(InputExample(texts=[row['sentence2'], row['sentence1']], label=score))
</code></pre>
<p><em>Initialize evaluator and fit model</em></p>
<pre class=""lang-py prettyprint-override""><code>logging.info(&quot;Read STSbenchmark dev dataset&quot;)
evaluator = EmbeddingSimilarityEvaluator.from_input_examples(dev_samples, name='sts-dev')

# Configure the training.
warmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1) #10% of train data for warm-up
logging.info(&quot;Warmup-steps: {}&quot;.format(warmup_steps))

# Train the bi-encoder model
bi_encoder.fit(train_objectives=[(train_dataloader, train_loss)],
          evaluator=evaluator,
          epochs=num_epochs,
          evaluation_steps=1000,
          warmup_steps=warmup_steps,
          output_path=bi_encoder_path
          )
</code></pre>
<p><strong>First Question: Why is the golden-dataset used for the evaluation, if the model fits on the silver-dataset?</strong></p>
<p>Further, the <code>test_sample</code> from the golden dataset is used for the final analysis:</p>
<pre class=""lang-py prettyprint-override""><code># load the stored augmented-sbert model
bi_encoder = SentenceTransformer(bi_encoder_path)
test_evaluator = EmbeddingSimilarityEvaluator.from_input_examples(test_samples, name='sts-test')
test_evaluator(bi_encoder, output_path=bi_encoder_path)
</code></pre>
<p><strong>Second Question: Why is the <code>test_sample</code> based on the golden-dataset? Why is the <code>test_sample</code> not based on the silver dataset?</strong></p>
","nlp"
"126100","How to get Audio embeddings using Hubert model","2023-12-19 09:41:19","","0","88","<nlp><pytorch>","<p>Example code:</p>
<pre><code>import torch
from transformers import Wav2Vec2Processor, HubertForCTC
from datasets import load_dataset

processor = Wav2Vec2Processor.from_pretrained(&quot;facebook/hubert-large-ls960-ft&quot;)
model = HubertForCTC.from_pretrained(&quot;facebook/hubert-large-ls960-ft&quot;)
input_values = processor('array from audio file., return_tensors=&quot;pt&quot;).input_values 
</code></pre>
<p>How to get embeddings after this ? There is no last hidden state in the model .</p>
","nlp"
"126068","Best way to encode a tag column for clustering","2023-12-16 13:11:36","","0","10","<nlp><clustering><word-embeddings><encoding><categorical-encoding>","<p>I have a dataset which tells me a tech support case used a particular tech document.
Every case has been tagged with which product it pertains to.
Similarly tech documents are tagged with certain key words. Now these tags are not a one-to-one match.
My case can have <code>{&quot;vpn&quot;,&quot;rpx series&quot;,&quot;ipv6 protocol&quot;}</code> and the tech doc linked to it might have <code>{&quot;routers&quot;,&quot;rpx2090&quot;,&quot;rpx series&quot;}</code>
So in this case &quot;rpx series&quot; is present in both, hence its highly likely that the doc linked to the case is the correct one.</p>
<p>How can we encode these tag sets to show that there is a definite overlap?
I am looking to preserve more of the syntactic info than the semantic one since these are words.</p>
<p>My goal is to use the encoding for clustering and similarity techniques.
This will be one of the many features used for my exercise.</p>
","nlp"
"126064","PyTorch input shape for text classification using LSTM","2023-12-16 01:19:05","","0","38","<nlp><lstm><pytorch><text-classification>","<p>I have three sentiment classes: POSITIVE, NEGATIVE, and NEUTRAL, along with a dataset consisting of 3000 sentences and their corresponding sentiment labels (POSITIVE, NEGATIVE, or NEUTRAL). Each sentence is represented as a <span class=""math-container"">$100$</span>-dimensional vector. As a newcomer to LSTMs, I am seeking advice on the preferable approach for developing an LSTM model in PyTorch. Both approaches have a batch size of 64, with <code>batch_first = True.</code></p>
<p>Approach - 1:
The input tensor should be of shape <span class=""math-container"">$(64,100,1)$</span> where each batch contains 64 sentences and each of the sentences consists of 100 timesteps where each timestep is a <span class=""math-container"">$1$</span>-d vector. Basically each dimension of the sentence - vector is a timestep.</p>
<p>Approach - 2:
The input tensor should be of shape <span class=""math-container"">$(64,1,100)$</span> where each batch contains 64 sentences and each of the sentences consists of 1 timestep which has <span class=""math-container"">$100$</span> dimensions.</p>
<p>It's worth mentioning that the sentence vectors were generated by averaging the word embeddings of the tokens of each sentence. Although Approach - 1 seems more intuitive, I am unsure if the choice between the two approaches is influenced by experimental results.</p>
","nlp"
"126056","Questions about sentencepiece tokenizer","2023-12-15 08:57:55","","0","28","<nlp><tokenization>","<p>The SentencePiece original paper <a href=""https://arxiv.org/pdf/1808.06226.pdf"" rel=""nofollow noreferrer"">manuscript</a> is as vague as it can get about the implementation of the algorithm. The paper describes the problems with a BPE or Unigram tokenizer and then claims that SentencePiece solves them and then directly jumps into the usage details of the SentencePiece library</p>
<p>The problem with the BPE and Unigram Tokenizers, as described in the paper:</p>
<ol>
<li>They require a pre-tokenizer and many languages are vague about the separation (or even definition) of words (like Japanese etc)</li>
<li>They are not lossless. Pre-tokenization loses information like multiple whitespaces etc</li>
</ol>
<p>SentencePiece claims to solve these. It is also mentioned that SentencePiece can be used in two modes - Unigram and BPE. Also, the data to be fed to the SentencePiece library is supposed to be in the format of one sentence per line in the file</p>
<p>Questions:</p>
<ol>
<li>How are the algorithms Unigram and BPE used with the SentencePiece such that a pre-tokenizer is not needed</li>
<li>What is this sentence that should be present per line of the text file? If I have a corpus of documents how do I create the input for the SentencePiece? Do I separate the documents by the period symbol (.) and place each element in a line in the file to be fed to SentencePiece? That sounds weird but okay</li>
<li>Is SentencePiece using this &quot;Sentence&quot; as a single pre-tokenized unit?</li>
</ol>
","nlp"
"126032","Attention mechanisms without a linear layer","2023-12-13 11:51:35","","2","73","<nlp><encoding><attention-mechanism><sequence-to-sequence><allennlp>","<p>I am currently looking into attention mechanism as they are used in (non-Transformer) encoder-decoder architectures, meaning an architecture where some RNN (usually LSTM or GRU) is used in both the encoder and decoder and is unrolled as many times as the input sequences are long. In particular, I am working on a project that is based on an older work on machine translation that uses such a model in combination with an attention-mechanism in AllenNLP 0.9.</p>
<p>Looking at the AllenNLP code, I found that they call the attention-mechanism that is being used &quot;dot_product&quot; (please note that this is definitely not referring to the scaled dot product attention that is used in transformers), and apparently all it does is, for the hidden state ht of the decoder at timestep t, calculate the dot-product to the hidden-states of each encoder step:</p>
<pre><code>def _forward_internal(self, vector: torch.Tensor, matrix: torch.Tensor) -&gt; torch.Tensor:
    return matrix.bmm(vector.unsqueeze(-1)).squeeze(-1)
</code></pre>
<p>The result is then ran through a softmax-function and the resulting weights are used to calculated a weighted sum of the encoder hidden-states. Lastly, this vector is concatenated to ht and ran through a final linear layer that has the same output dimension as the vocabulary. This matches the attention-descriptions in Josh Starmer's <a href=""https://www.youtube.com/watch?v=PSs6nxngL6k&amp;t=670s"" rel=""nofollow noreferrer"">StatQuest on the topic</a> and in <a href=""https://arxiv.org/pdf/1508.04025v5.pdf"" rel=""nofollow noreferrer"">Section 3.1 of a very prominent paper</a>. Here is an illustration from the paper:</p>
<p><a href=""https://i.sstatic.net/Wtil6.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Wtil6.png"" alt=""enter image description here"" /></a></p>
<p>What confuses me about this is how this improves model performance. What is described above is simply a static calculation based on the dot-product, there are no learned weights since there is no linear layer in the attention-mechanism. What further confuses me is that there are many implementations of attention that DO use a linear layer for learning weights, e.g Bahdanau Attention or scaled dot product attention, and sources that do not even mention that there is a linear-layer-less attention <a href=""https://machinelearningmastery.com/the-attention-mechanism-from-scratch/"" rel=""nofollow noreferrer"">1</a> <a href=""https://datascience.stackexchange.com/questions/84130/is-a-dense-layer-required-for-implementing-bahdanau-attention"">2</a>.</p>
<p><strong>So I guess to sum up, my question is</strong> : How does attention without a linear layer help the model and how does it compare to an approach that does use a linear layer?</p>
","nlp"
"125004","Explanation : Simpler models beat BERT base","2023-12-11 09:20:38","","0","31","<nlp><bert><text-classification>","<p>I have been trying to train different models for a multi-class classification task of texts. My data set consists of rows of text and its label. The texts are short sentences.
I tried the following models :
after generating embeddings using distilbert-base-uncased i trained :</p>
<ul>
<li>Random Forest Classifier :F1-Score: 0.59</li>
<li>SVM :F1-Score: 0.63</li>
<li>Gradient Boosting Classifier:F1-Score: 0.65</li>
<li>MLPClassifier:F1-Score: 0.68</li>
</ul>
<p>then :</p>
<ul>
<li>-BERT base :F1-Score: 0.40</li>
</ul>
<p>Since BERT is supposed to be the most powerful model for this use case I was surprised with the result, I was trying to find an explanation and hypothesis for that. So has anyone faced similar results before . what do you think may be the reason?</p>
","nlp"
"125002","LSTM Layer producing same outputs for different sequences","2023-12-11 06:51:21","","0","71","<nlp><lstm><multilabel-classification>","<p>Currently I try to train on a multi-label language task with imbalanced class distribution. I have the following model, where I removed some of the feed forward layers to decrease factors in the chain of gradients.</p>
<p>Since the outputs are extremely weird during inference time (i.e. every prediction is class 1 of 32 and no others), I started to check the layers, esp. the LSTM layer to see if any inconsistencies occur.</p>
<p>First let me share my model-architecture with you.</p>
<pre><code>class Bi_RNN(nn.Module):
    &quot;&quot;&quot;&quot;
    Embedding Dim 300
    &quot;&quot;&quot;
    def __init__(self, hidden_dim_lstm, in_2_dim, in_3_dim, in_4_dim, input_dim=300, output_dim=32, num_layers=1, batch_size=1):
        super(Bi_RNN, self).__init__()
        
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim_lstm*2*num_layers
        self.hidden_dim_lstm = hidden_dim_lstm
        self.batch_size = batch_size
        self.num_layers = num_layers
        self.in_2_dim = in_2_dim
        self.in_3_dim = in_3_dim
        self.in_4_dim = in_4_dim
        self.act = nn.PReLU()

        # Define the LSTM layer
        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim_lstm, self.num_layers, batch_first=True, bidirectional=True)

        # Define the FFN
        self.linear_layer_1 = nn.Linear(self.hidden_dim, self.in_4_dim)
        self.linear_layer_last = nn.Linear(self.in_4_dim, output_dim)  
        
    def init_hidden(self):
        # This is what we'll initialise our hidden state as
        device = next(self.parameters()).device.type
        return (torch.zeros(self.num_layers*2, self.batch_size, self.hidden_dim//2).to(device),
                torch.zeros(self.num_layers*2, self.batch_size, self.hidden_dim//2).to(device))
    
    def forward(self, input):
        lstm_out, self.hidden = self.lstm(input, self.init_hidden())
        h_n, c_n = self.hidden
        c_n_merged = c_n.reshape(self.batch_size, -1)
  
        layer_1_out = self.act(self.linear_layer_1(c_n_merged))
        out = self.linear_layer_last(layer_1_out)
        out = torch.sigmoid(out)

        return out

</code></pre>
<p>This is the model state after training. Consider the following inputs <code>x</code> each with shape <code>torch.Size([7484, 300])</code> (it's actually a batch with <code>torch.Size([64, 7484, 300])</code>).</p>
<p>x_1 looks like this</p>
<pre><code>tensor([[-0.1113,  0.1436,  0.1895,  ...,  0.0342,  0.1602, -0.2500],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [-0.2910,  0.1787,  0.0500,  ..., -0.0228,  0.1177,  0.3535],
         ...
</code></pre>
<p>x_2 looks like this</p>
<pre><code>tensor([[ 0.1250,  0.0266, -0.0272,  ..., -0.0864, -0.1621, -0.0337],
         [ 0.0070, -0.0732,  0.1719,  ...,  0.0112,  0.1641,  0.1069],
         [ 0.0762,  0.0820, -0.1118,  ..., -0.0942, -0.0684,  0.2266],
         ...
</code></pre>
<p>So when getting the LSTM out of the model and passing these vectors (as a batch) into the LSTM, the hidden states <code>c_n, h_n</code> with shape <code>torch.Size([800])</code> are identical (most of them are for the complete batch)</p>
<p>The <code>c_n</code> look like this</p>
<pre><code>tensor([[-0.1549,  0.0412, -0.0041,  ..., -0.1105, -0.0761,  0.0696],
        [-0.1549,  0.0412, -0.0041,  ..., -0.1105, -0.0761,  0.0696]],
</code></pre>
<p>and the <code>h_n</code> look like this</p>
<pre><code>tensor([[-0.0746,  0.0206, -0.0020,  ..., -0.0547, -0.0372,  0.0344],
        [-0.0746,  0.0206, -0.0020,  ..., -0.0547, -0.0372,  0.0344]],
</code></pre>
<p>I don't understand how this is happening and I would be very grateful if somebody can point out what my misconception is.</p>
<p>Q: How can I adjust my model, such that it properly produces context vectors on input sequences with different output?
Q: Is there an error in coding in my model, such that I provoke this behavior?</p>
<p>I am sorry in advance if there are any rather stupid mistakes.</p>
<p>Thanks</p>
","nlp"
"124998","RuntimeError: mat1 and mat2 shapes cannot be multiplied (25x7 and 1x512)","2023-12-10 22:58:28","","0","133","<deep-learning><nlp><time-series><transformer><huggingface>","<p>I am dealing with multivariate time series forecasting using Transformers.
below is my code step by step:</p>
<p>After some preprocessing and windowing time series dataset …</p>
<p>1- Creating Mask function</p>
<pre><code>input_sequence_length = 10 # incoder input sequence
target_sequence_length = 5 # decoder input sequence

tgt_mask = generate_square_subsequent_mask(
    dim1=target_sequence_length,
    dim2=target_sequence_length
   )
src_mask = generate_square_subsequent_mask(
    dim1=target_sequence_length,
    dim2=input_sequence_length
   )
</code></pre>
<p>2- Positional Encoding</p>
<pre><code>class PositionalEncoder(nn.Module):
    def __init__(self, dropout: float = 0.1, 
        max_seq_len: int = 5000, d_model: int = 512,device = device):

        super().__init__()

        self.d_model = d_model
        self.dropout = nn.Dropout(p=dropout)
        self.batch_first = True  # Assuming batch_first is always True

        position = torch.arange(max_seq_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))

        pe = torch.zeros(1, max_seq_len, d_model)
        pe[0, :, 0::2] = torch.sin(position * div_term)
        pe[0, :, 1::2] = torch.cos(position * div_term)

        self.register_buffer('pe', pe)
        
    def forward(self, x: Tensor) -&gt; Tensor:
        x = x + self.pe[:, :x.size(1)]
        return self.dropout(x)
</code></pre>
<p>3 - Creating Transformers Encoder and Decoder with Pytorch</p>
<pre><code>class TimeSeriesTransformer(nn.Module):

    def __init__(self, 
        input_size: int,
        dec_seq_len: int,
        out_seq_len: int= 5, # target_sequence_length
        dim_val: int=512,  
        n_encoder_layers: int=2,
        n_decoder_layers: int=2,
        n_heads: int=4,
        dropout_encoder: float=0.2, 
        dropout_decoder: float=0.2,
        dropout_pos_enc: float=0.1,
        dim_feedforward_encoder: int=512,
        dim_feedforward_decoder: int=512,
        num_predicted_features: int=1
        ): 

        super().__init__() 

        self.dec_seq_len = dec_seq_len

        self.encoder_input_layer = nn.Linear(
            in_features=input_size, 
            out_features=dim_val 
            )

        self.decoder_input_layer = nn.Linear(
            in_features=num_predicted_features,
            out_features=dim_val
            )  
        
        self.linear_mapping = nn.Linear(
            in_features=dim_val, 
            out_features=num_predicted_features
            )

        # Create positional encoder
        self.positional_encoding_layer = PositionalEncoder(
            d_model=dim_val,
            dropout=dropout_pos_enc
            )

        encoder_layer = nn.TransformerEncoderLayer(
            d_model=dim_val, 
            nhead=n_heads,
            dim_feedforward=dim_feedforward_encoder,
            dropout=dropout_encoder,
            batch_first=True
            )

        self.encoder = nn.TransformerEncoder(
            encoder_layer=encoder_layer,
            num_layers=n_encoder_layers, 
            norm=None
            )

        decoder_layer = nn.TransformerDecoderLayer(
            d_model=dim_val,
            nhead=n_heads,
            dim_feedforward=dim_feedforward_decoder,
            dropout=dropout_decoder,
            batch_first=True
            )

        self.decoder = nn.TransformerDecoder(
            decoder_layer=decoder_layer,
            num_layers=n_decoder_layers, 
            norm=None
            )

    def forward(self, src: Tensor, tgt: Tensor, src_mask: Tensor=None, 
                tgt_mask: Tensor=None) -&gt; Tensor:

        src = self.encoder_input_layer(src) 
      
        src = self.positional_encoding_layer(src) 
        src = self.encoder(src=src)
        
        decoder_output = self.decoder_input_layer(tgt)
        decoder_output = self.decoder(
            tgt=decoder_output,
            memory=src,
            tgt_mask=tgt_mask,
            memory_mask=src_mask
            )
        decoder_output = self.linear_mapping(decoder_output) 
        
        return decoder_output
</code></pre>
<p>4 - model</p>
<pre><code>model = TimeSeriesTransformer(
    input_size=7,
    dec_seq_len=5,
    num_predicted_features=1,
    ).to(device)
</code></pre>
<p>5 - creating loader # befor created in the preprocessing step</p>
<pre><code>i, batch = next(enumerate(train_loader))
src, trg, trg_y = batch
src = src.to(device) # shape [5 , 10 , 7] , batch size , encoder sequence len , number of feature
trg = trg.to(device) # shape [5 , 5 , 7], batch size , decoder sequence len , number of feature
</code></pre>
<p>6 - output of the model</p>
<pre><code> output = model(
        src=src,
        tgt=trg,
        src_mask=src_mask,
        tgt_mask=tgt_mask
        )
    trg_y = trg_y.to(device) # [5 , 5 , 1] , batch size , deocder or output sequence len , number predicted feature
</code></pre>
<p>7 - Finally the raised error is like below</p>
<pre><code>output = model(
    src=src,
    tgt=trg,
    src_mask=src_mask,
    tgt_mask=tgt_mask
    )
Traceback (most recent call last):

  Cell In[348], line 1
    output = model(

  File C:\ProgramData\anaconda3\Lib\site-packages\torch\nn\modules\module.py:1518 in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)

  File C:\ProgramData\anaconda3\Lib\site-packages\torch\nn\modules\module.py:1527 in _call_impl
    return forward_call(*args, **kwargs)

  Cell In[344], line 80 in forward
    decoder_output = self.decoder_input_layer(tgt)

  File C:\ProgramData\anaconda3\Lib\site-packages\torch\nn\modules\module.py:1518 in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)

  File C:\ProgramData\anaconda3\Lib\site-packages\torch\nn\modules\module.py:1527 in _call_impl
    return forward_call(*args, **kwargs)

  File C:\ProgramData\anaconda3\Lib\site-packages\torch\nn\modules\linear.py:114 in forward
    return F.linear(input, self.weight, self.bias)

RuntimeError: mat1 and mat2 shapes cannot be multiplied (25x7 and 1x512)
</code></pre>
","nlp"
"124988","How can I leverage machine learning for log analysis?","2023-12-10 08:33:26","","1","440","<machine-learning><nlp><training><language-model><llm>","<p>I am new to data science and trying to find possibilities of using datascience in tasks. I have a set of logs which I want to convert to json. The logs are more or less of same format and I can write a script which parse them or aggregate them but instead of doing it manually I want to use help of machine learning. This thought is also inspired by the fact that at sometime some new log line may come. I guess pre-trained LLM models can identify the context and information from logs. But I am not sure how exactly can ML be used for for this purpose? The question may sound stupid but please pardon.</p>
","nlp"
"124982","How are the words defined in the sentencepiece algorithm?","2023-12-10 00:28:34","","0","13","<nlp><tokenization>","<p>I am not able to understand how the sentencepiece algorithm solves the problem of handling the languages without a clear-cut concept of words</p>
<p>My exact confusion is:</p>
<ol>
<li>It is mentioned that one of the advantages of the sentencepiece algorithm is that you do not have to do a language-specific pre-tokenization to compute the words</li>
<li>The sentencepiece algorithm supports BPE and unigram algorithms</li>
<li>However, both the sentencepiece and unigram tokenizers split the corpus into the words as the first step. They use some pre-tokenizer. I have read the huggingface implementation (<a href=""https://huggingface.co/learn/nlp-course/chapter6/7?fw=pt"" rel=""nofollow noreferrer"">unigram</a>, <a href=""https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt"" rel=""nofollow noreferrer"">BPE</a>) for both and that is the first step</li>
</ol>
<p>Can you help me understand how sentencepiece alleviates the problem of a lack of word demarcation in languages like Japanese?</p>
","nlp"
"124977","Purpose of Azure Cognitive Search","2023-12-09 18:02:39","","0","18","<machine-learning><nlp><azure-ml><search>","<p>Azure provides a service called <em>Cognitive Search</em> which is an intelligent AI-based search service based on advanced NLP.</p>
<p>I tried this feature. And to make the search as efficient as possible, it requires lot of configuration to be done, such as, applying filters like sortable, filterable, facetable, searchable, enabling spell check, adding synonyms, etc. Also, it gives decent results only when tried on data rich in text. It fails at searching through data rich in numbers.</p>
<p>How is this an inteliigent search service when everything needs to be specified manually?</p>
","nlp"
"124962","Higher level sentence similarity (meaning instead of 'just' embeddings)","2023-12-08 10:39:49","124963","4","250","<nlp><transformer><similarity><llm>","<p>I am looking for the correct model / approach for the task of checking if two sentences have the same <em>meaning</em></p>
<p>I know I can use embeddings to check similarity, but that is not what I am after. I suspect BERT style LLM have nice higher level vector that mights be useful, but I'm not sure how to apply that.</p>
<p>For example this sentence:</p>
<ul>
<li>I am very lazy</li>
</ul>
<p>Has a somewhat similar meaning as:</p>
<ul>
<li>I don't like to work hard</li>
</ul>
<p>But not</p>
<ul>
<li>A lazy horse is not very useful</li>
</ul>
<p>Using 'just' embeddings (for example HF: allMiniLM-L6-v2) gives results that are not useful.</p>
<p><a href=""https://i.sstatic.net/7pKtJ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/7pKtJ.png"" alt=""enter image description here"" /></a></p>
<p>What would be a good appoarch?</p>
","nlp"
"124834","Seeking datasets for training a Language Model on U.S. mortgage loan processes","2023-11-30 07:53:39","","0","92","<machine-learning><classification><nlp><dataset><data-mining>","<p>I'm in the process of training a Language Model (LLM) and require datasets that encompass various aspects of the U.S. mortgage loan process. The model's aim is to understand and simulate decision-making and advisory tasks within this domain.</p>
<p>Details:</p>
<p>Type of Data: I am looking for datasets with granular details on loan performance, approval rates, mortgage securities, and lending practices.
Intended Use: The data will be used to train a Language Model to grasp and generate information related to mortgage processes.
Required Specificity: Data should ideally span multiple years and include a broad spectrum of financial institutions.
What I've tried:</p>
<p>Searched the FHFA for data on market support activities and securities.
Looked at Fannie Mae's Loan Performance Data.
Investigated the National Mortgage Database for comprehensive mortgage market information.
Analyzed CFPB's data summaries for mortgage lending in 2021 and 2022.
Does anyone know where I might find additional datasets suitable for training a LLM? Any leads on datasets that are publicly available or purchasable would be incredibly helpful.</p>
","nlp"
"124816","Prefix tuning in LLM uses learnable vectors to fine tune the model","2023-11-29 04:54:40","","0","45","<deep-learning><nlp><transformer>","<p>I would like to implement a new architecture for Transformer.</p>
<p>Below description is my thought.</p>
<p>Prefix tuning in LLM uses learnable vectors to fine tune the model.</p>
<p>Is there a way to use the output generated by the Neural network as prefix?</p>
<p>Thanks</p>
<p><a href=""https://i.sstatic.net/13Vmk.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/13Vmk.png"" alt=""enter image description here"" /></a></p>
","nlp"
"124794","Some questions from a noob - Chat bot","2023-11-27 23:10:04","","0","32","<machine-learning><nlp><ai>","<p>I'm a T-SQL and .NET developer but I need to understand some basis about a data science / ml project.
I would like to know the guidelines and be directed to the right topics about the steps that need the project.</p>
<p>The project would concern a chatbot (I would use the telegram api and on this point I already know how to do it) which, given n entities (e.g.: sales invoices, credit notes, customers, suppliers, etc.) present in the company database, gives answers based on user questions.
Example of questions:</p>
<ul>
<li>&quot;I would like to know the items, quantity and total of the invoice FV23-XXXX&quot;</li>
<li>&quot;how much did the customer bill in the last month?&quot;</li>
<li>&quot;how much vendors supplied the item ITM-XXXX&quot;
What are the steps and courses to follow?
How could the project be modeled?</li>
</ul>
<p>I know that python is the best and most widespread language, so I would use that.</p>
<p>Unfortunately I don't have enough time to have the knowledge on the whole area to get an idea.
Forgive me for the stupid questions.</p>
","nlp"
"124792","Fine-tune zero-shot classification model multi-label","2023-11-27 19:18:46","","0","85","<nlp><multilabel-classification><text-classification><finetuning><zero-shot-learning>","<p>I started a small project where I am trying to fine-tune a zero-shot classification model on a proprietary dataset. I was thinking to use the NLI approach, building contradiction and entailment statements for each of my sentences/labels pairs.</p>
<p>I have a dataset with sentences and for each of them multiple true labels.</p>
<p>However, I am not sure on what is the best way to approach this, given that in literature I have only seen the case where there is only one label per sentence.</p>
<p>Making one example:</p>
<p>Sentence 1. Classes = ['A','B','C']</p>
<p>Should I build my dataset generating three different samples</p>
<p>Sentence 1. This is about 'A' + Entailment label
Sentence 1. This is about 'B' + Entailment label
Sentence 1. This is about 'C' + Entailment label</p>
<p>or generating only one as follows:</p>
<p>Sentence 1. This is about A, B, C. + Entailment label</p>
<p>I am happy to hear any other ideas on this.</p>
<p>Thanks a lot!</p>
","nlp"
"124737","Semantic Search on numeric data","2023-11-24 14:46:34","","0","113","<machine-learning><nlp><semantic-similarity><search-engine>","<p>I have a dataset in csv format. Most columns have numbers in them. The data is mostly number based, with little text.</p>
<p>A sample of my dataset:</p>
<p><a href=""https://i.sstatic.net/ZfsZO.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ZfsZO.png"" alt=""enter image description here"" /></a></p>
<p>I want to build a semantic search engine that can answer questions like the following:</p>
<ol>
<li>Which user has executed the maximum number of tests?</li>
<li>On what date did a particular user execute tests?</li>
<li>Which date observes the maximum number of failed tests?</li>
</ol>
<p>I tried Amazon Kendra, Azure Cognitive Search, ChatGPT, Bard, built my own semantic search engine from scratch, but none of them are performing well on numeric data. They're all more text based.</p>
<p>One idea I have is, if I change the numbers to text, will the models be able to predict well?</p>
<p>So for my requiement, what would you suggest me to try? Please shed some light on this.</p>
","nlp"
"124733","Purely extractive Language Model","2023-11-24 11:00:36","124735","0","142","<nlp><generative-models><language-model><parsing><search-engine>","<p>Given an email thread, I am trying to extract the body of the most recent email.</p>
<p>I used to do that with rules. Now I am testing Large Language Models (LLM) to see if I they provide a less ad hoc solution.</p>
<p>Mistral-7B-Instruct, for instance, seems to understand the task and provides acceptable outputs most of the time.</p>
<p>However, in some cases, it explains the email rather than just copy/paste the relevant chunk.</p>
<p>I have tried dozens of prompts, for instance:</p>
<pre><code>instruction = 'Given the email thread bellow the dotted line, extract verbatim the body of the most recent (top) message. Remove all headers, footers and disclaimers. In your response, do not add any text that was not present in the original message'
</code></pre>
<p>And tried to prevent hallucinations by setting the following:</p>
<pre><code>    generation_output = model.generate(
        model_inputs,
        do_sample=True,
        temperature=0.0000001,
        top_p=0.0000001,
        top_k=1,
        max_new_tokens=words
        )
</code></pre>
<p>However, in a few cases, the model still adds explanations and/or hallucinates a bit.</p>
<p>My questions are the following:</p>
<ol>
<li><p>Are you aware of any models that could do a better job without fine-tuning? For instance, purely extractive models (as opposed to generative ones).</p>
</li>
<li><p>If generative models are the way to go, is there a way to force the model to just copy/paste?</p>
</li>
</ol>
<p>Best,</p>
<p>Ed</p>
","nlp"
"124721","Performing Multi label text classification","2023-11-23 08:38:10","","0","22","<deep-learning><nlp><text-classification>","<p>I have a text and it's class, so I have performed single text classification, but now I want to train a multi label classifier, so I tried combining sentence to form a multi label dataset, but the lstm model is overfitting, even though there was no leakage of data for the model, is there anything I can do, any augmentation?</p>
","nlp"
"124643","NLP approach for classifying webscraped data","2023-11-18 04:15:08","","0","19","<python><nlp><scikit-learn><text-classification>","<p>I have a challenge in a project of mine where I will be provided with a list of scraped datas from a website. Along with the data i will also be provided some parameters like the tag of scraped element, class name of the scraped element and some preceeding texts. My job is to provide the best suited column name that suits the data. I am able to extract some relevant data from the class name but surety is required because that's not a reliable factor.</p>
<p>So, I require a method or approach of classifying the datas provided. I am considering spacy and its features like NER etc. Kindly provide an approach. Other technically feasible solutions are also welcomed!!!!!!</p>
<p>Things I have tried:- The default NER of spacy and sklearns classifier but the classifier wasnt showing any good result and the spacys default NER is a lot limited. A custom NER requires a good amount of data. So all i need is an approach to move ahead with. I am also unsure if i should consider the text classification. Because my ultimate goal is to ensure the category provided suits the scraped data. The scraped data can be varied:- list of job listings, real estates, products, titles of books, link or urls etc.</p>
","nlp"
"124639","Get accurate alias list of people without information on aliases","2023-11-17 15:53:57","","0","23","<nlp><data-mining><data-cleaning><similarity>","<p>I'm working on a project where I have a large dataframe of paintings (from the Art500k dataset), each row corresponds with a painting, containing the author's name in the <code>author_name</code> column. Sometimes, the same artist is stored under a different/incomplete name for two different pictures (rows), e.g. Rembrandt and Rembrandt van Rijn. I would like to &quot;accurately&quot; find the set of all aliases for each author, so I can store them in a dictionary and merge the &quot;alias&quot; data together.</p>
<p>The hard part is finding the aliases. Initially, I thought that I look whether some author names contain others, create a connection based on this information, and merge the connected &quot;components&quot; (graph notation) together. This would for example, merge Rembrandt, Rembrandt van Rijn, Rembrandt (Rembrandt van Rijn), and Rembrandt Harmensz. van Rijn together, which is correct. But it would also merge Rembrandt Peale with Rembrandt van Rijn, which is incorrect. I had further ideas for making connections based on a &quot;similarity function&quot;, but such function would still find Rembrandt as similar to Rembrandt van Rijn, as to Rembrandt Peale, or less. <br>
So now I consider using NLP, or any other method recommended for such cases, but I lack knowledge on this. Maybe I should gather nicknames of famous painters based on Wikipedia? What would be a good approach?</p>
<p>EDIT: I used different measures: <code>fuzzywuzzy</code> library for fuzzy string matching which returns a score for each pair of strings, and implemented some simple dataset-specific measures, then combined the measures with weights. Playing with weight parameters based on results are useful but there should be some other measure to help. I could imagine using LLMs or some other ML tools to help recognize.</p>
","nlp"
"124591","What is the input to an encoder-decoder transformer in next word prediction task?","2023-11-14 22:21:46","124593","0","225","<nlp><transformer><language-model>","<p>I'm trying to understand how encoder-decoder architectures are used, or if they are used at all, for generative tasks that do not require an explicit prompt (ie. machine translation, summarization, etc.).</p>
<p>From my understanding, decoder-only models autoregressively predict the next token in a sequence given its previous predictions. This makes sense, as we can simply keep feeding it tokens already predicted during inference. But how is this done when there is an encoder involved? For machine translation, we have the sequence in the source language to feed to the encoder. Similarly, we can feed it a passage to summarize for summarization. What would we feed the encoder if we simply wanted next word prediction? Do we feed it the sequence we want it to complete? I haven't found any examples of this task being performed. Does this mean that encoder-decoder models aren't needed for this task?</p>
","nlp"
"124582","Is there a tool to do ""next speech prediction""?","2023-11-14 13:25:53","","0","18","<nlp><text-to-speech>","<p>Given the enormous success of next-token prediction, I was wondering if there are any successful models that try to apply the same principle on speech, i.e., the analogue audio of speech rather than only the text. There are obviously a lot more information in speech, in comparison to text, so there is more to be learned.</p>
","nlp"
"124577","How does Bert masked language modelling task make sense if half the time the next sentence is wrong context in the sequence passed through the encoder","2023-11-14 07:42:17","124578","2","115","<nlp><word-embeddings><transformer><bert>","<p>Bert has two types of tasks that it uses to learn contextual word embeddings:</p>
<ol>
<li>Masked word prediction</li>
<li>Next sentence prediction</li>
</ol>
<p>I have read the <a href=""https://arxiv.org/abs/1810.04805"" rel=""nofollow noreferrer"">paper</a> and even there the training details are a little fuzzy or, dont make sense to me</p>
<p>To quote the paper:</p>
<blockquote>
<p>To generate each training input sequence, we sample two spans of text from the corpus, which we
refer to as “sentences” even though they are typically much longer than single sentences (but can
be shorter also). The first sentence receives the A
embedding and the second receives the B embedding. 50% of the time B is the actual next sentence
that follows A and 50% of the time it is a random
sentence, which is done for the “next sentence prediction” task. They are sampled such that the combined length is ≤ 512 tokens. The LM masking is
applied after WordPiece tokenization with a uniform masking rate of 15%, and no special consideration given to partial word pieces.</p>
</blockquote>
<p>Question:
If 50% of our training examples are sentence (in Bert sense) followed by another unrelated sentence and we run it as one sequence through the transformer and the objective is to find context-specific embeddings then how does the objective of masked language modelling make sense in these cases? Our second sentence is the wrong context here. Masked language modelling makes sense only if the second sentence is the accurate entailment which is the case only in 50% of the cases</p>
","nlp"
"124538","Fine-tuning MT5 for making it more like ChatGPT","2023-11-11 05:32:07","","0","77","<nlp><text-generation><finetuning><llm><chatgpt>","<p>I am trying to fine-tune a model which works like ChatGPT for Punjabi language, using the mt5-base, however I am not sure if I should go ahead with it since it does not even generate text and when I try to use it, I just get a response as &lt;extra_pad&gt; 0. I have checked the tokenizers, they work fine with Punjabi language, can anyone please tell how may I go on about it?
The dataset I will be using is an instruction following dataset in the format of alpaca and is of high quality.
I have tried fine-tuning indic-gpt before, however it has a very small token size i.e.1024 so I changed my base model.
Thanks in advance!</p>
","nlp"
"124530","F1 and Exact-Match (EM) Score in Extractive QA NLP","2023-11-10 15:51:39","","0","59","<nlp><bert><huggingface><llm><question-answering>","<p>I have a question as to how the F1 should be calculated in NLP and whether the text normalization is optional or not.</p>
<p>So I have been working on a project where we created a closed-domain extractive QA dataset from scratch, and we are trying to finetune and assess the performance of several LLMs in this new dataset. I came across different definitions of the F1 score, sometimes with text normalization (<a href=""https://qa.fastforwardlabs.com/no%20answer/null%20threshold/bert/distilbert/exact%20match/f1/robust%20predictions/2020/06/09/Evaluating_BERT_on_SQuAD.html#Exact-Match"" rel=""nofollow noreferrer"">like in here</a>) and sometimes not. I have run all my experiments without the normalization step for both the EM and F1 scores. Should I rerun all experiments?</p>
<p>Is the</p>
","nlp"
"124524","Unsupervised Machine Translation System Using Variational Autoencoder Models","2023-11-10 10:56:19","","1","82","<nlp><autoencoder><generative-models><machine-translation><variational-inference>","<p>I want to work on an unsupervised machine translation system using a variational autoencoder. I did a literature review but didn't find any related work, and most of the work is based on denoising autoencoders. There are a few questions regarding this problem -</p>
<ol>
<li>What kind of encoding to use for this machine translation system? (like BPE, wordpiece, etc)</li>
<li>What kind of encoder-decoder model would be employed?</li>
<li>What will be the latent variable's expected loss value, and how to define it? Hint: Although JS-divergence and Energy distance are both smooth functions and differentiable, KL-divergence is not.</li>
<li>Most text-VAE-based models do not support standard training methods such as backpropagation, what are some other approaches?</li>
</ol>
<p>I thought maybe we could use variational autoencoders for learning distributions from two monolingual corpora for unsupervised translation. Each autoencoder will learn a distribution for the respective language, however, I also need to build a common latent space between the two languages. I'm not very sure how to align the spaces of the two languages.</p>
<p>Looking for some responses. It'd be nice to have some good mathematical formulations as well.</p>
","nlp"
"124514","Do ML model measurements and validation standards (e.g. NIST, ISO) exists for the finance, healthcare, and technology industries? Provide citations","2023-11-09 21:46:31","","0","15","<machine-learning><deep-learning><nlp><machine-learning-model><validation>","<p>Normally, for example, we talk about splitting datasets into training and test datasets. But. The splitting % per train and test sets happens in a subjective manner. Sometimes. The train is 60% or 70%, leaving the remaining for the test set. Sometimes we create validation sets and sometimes we don't. Sometimes we use accuracy and sometimes we use the ROC-AUC or F1. My question is simple. Is there standards (e.g. NIST, ISO) that dictates how the dataset and other model components need to be treated in order to meet a specific standard (compliance) in the financial, healthcare, or technology industry? Please provide citations not opinions.</p>
","nlp"
"124475","Best approach to find whether a scientific research paper has human trials/human testing or no","2023-11-07 20:56:25","","0","14","<python><nlp><spacy>","<p>I want to know the best way to know if a paper has human trials/test subjects/testing. Was thinking of searching for some keywords in the paper like &quot;Human&quot;, &quot;Trials&quot; etc.</p>
<p>The method I tried gave me a lot of false positives</p>
","nlp"
"124428","Sentencepiece Tokenizer training from scrath","2023-11-05 11:18:37","","0","131","<nlp><tokenization>","<p>To train BPE model on sentencepiece as per given <a href=""https://github.com/google/sentencepiece#usage-instructions"" rel=""nofollow noreferrer"">Usage instructions</a></p>
<p>As, it is mentioned in the instructions that <code>--input: one-sentence-per-line raw corpus file.</code></p>
<p>What if a sentence is long enough and cannot fit in a single line?</p>
<p>is there some documentation/instructions to prepare a dataset as per required format to train BPE model on sentencepiece from scratch?</p>
","nlp"
"124402","Systematic way of selecting internet texts for a machine translation corpus / dataset?","2023-11-03 14:25:13","","0","8","<nlp><dataset><language-model><machine-translation><corpus>","<p>I am currently working on a neural machine translation project and want to gather a corpus (or dataset) of internet texts that are written in standard and <a href=""https://en.wikipedia.org/wiki/Plain_language"" rel=""nofollow noreferrer"">plain language</a>. In theory, it certainly makes sense to try to collect all texts and compile them in a research corpus. In practice, I would like to proceed as systematically as possible in order to find at least the relevant or most suitable texts.</p>
<p>It is possible to find simple language texts relatively quickly, which can also be easily aligned. So the problem for me at the moment is not finding data, but rather finding a systematic way that allows me to prioritize the scraping and merging into a dataset. Ideally, the system is backed up by scientific literature.</p>
<p>At the moment I'm a bit stuck and would be happy to hear from you about known approaches, queries for literature research, known similar projects, specific literature or other such things :)</p>
","nlp"
"124379","Why did the Double Metaphone algorithm choose to substitute and merge consonants?","2023-11-02 03:10:14","","0","34","<nlp><algorithms><text>","<p>Is there any literature describing the decisions behind why the mappings from input text characters to Metaphone hash consonants were made? Why did they choose to leave out vowels? Why did they merge consonants? Where can I find some of the reasoning behind some of the decisions made in the related phonetic algorithm(s)? I would like to see if it can be improved upon, but first it would be helpful to know some of the reasons why they made the decisions to cut and morph things the way they did.</p>
<p>In particular, I am imagining if you have the exact phonetic transcription of a word, then any deviation in the pronunciation will throw it off. So you want to perhaps make it more lenient, to handle slight changes in pronunciation. But the possibilities for this seem vast at first glance, so how did they choose what they did basically?</p>
<p>Getting rid of the vowels and making every vowel sound &quot;A&quot; seems to me the same as getting rid of every consonant and making each consonant &quot;T&quot;, so &quot;beautiful&quot; becomes &quot;TATATAT&quot; and &quot;emotion&quot; becomes &quot;ATATAT&quot; haha, but that is taking it too far. So somehow you don't want to get rid of <em>that much</em> information, and somehow they chose just the right amount of stuff to toss out in the pronunciation, and just the right amount of stuff to keep, and I'm unsure how they ended up where they ended up. Am super curious to learn any insights in their thought process here.</p>
<p>From my <a href=""https://github.com/words/double-metaphone/blob/main/index.js"" rel=""nofollow noreferrer"">reading</a>, they chose:</p>
<pre><code>K: KGQ
S: SZ
X
J
T: TD
F: FV
N
L
H
M
P: PB
R
</code></pre>
","nlp"
"124363","Highly unbalnced text data giving very low matrics","2023-11-01 08:16:29","","0","14","<python><nlp><class-imbalance><text><data-augmentation>","<p>I have an unbalanced multi-class banking text data with around 76 classes. Classes are badly distributed such as one class which is combination of 240 other different categories, represents 50% of data. rest 75 classes are highly unbalanced. I have tried data balancing techniques such as:- model class weight, random under sampling, random over sampling, but none of them could get me weighted accuracy more than 60%. As part of per processioning. I am using Linear SVM, bcs so far giving best accuracy.
I have performed text cleaning using, removed PII, removed cities/countries
Class_label counts are for reference as:</p>
<pre><code>0.48 (combined class)
0.08
0.06
0.004
</code></pre>
<p>and so on similar pattern.</p>
<pre><code>I am going to try data augmentation approach, please suggest how to use this approach as per class availability and populate data frame quickly. the code for augmentation is as follows:
import nlpaug.augmenter.char as nac
import nlpaug.augmenter.word as naw
aug=nac.keybordAug()
aug=naw.SynonymAug(aug_sr'wordnet',lang='eng')
augmented_text=aug.augmented(df['text'][0],1)  # 1  used for 3 augmented text will be generated
print(augmented_text)
</code></pre>
<p>I want to create new data frame with augmented text inserted multiple times as per their ratio to balance dataset.</p>
<p>Please free to suggest any other approach also. as per problem statement.</p>
","nlp"
"124347","Is there a Language Model that can accept huge corpse of tabular data and answr questions about?","2023-10-31 10:49:19","","3","136","<nlp><llm>","<p>I have been researching Language Models that can work with tabular data. My main goal is to have a model to answer simple questions about my data. An example is having household sales data and asking simple questions like &quot;What was the average sales during the last 2 months?&quot;. One of the best models I have found so far is <a href=""https://huggingface.co/docs/transformers/model_doc/tapas"" rel=""nofollow noreferrer"">TAPAS</a>. However, it has limitations regarding the size of tabular data. My data size is approximately 1 million rows with 10 columns. Is there a robust model that can perform the mentioned task or is there an alternative approach to this problem?</p>
","nlp"
"124334","【NLP】Is there a model or task that determines contextual similarity?","2023-10-30 15:16:50","","0","13","<machine-learning><deep-learning><nlp><language-model>","<p>I am trying to work on an engagement detection task in which I have to determine if a student is engaged in class.</p>
<p>I am looking for an NLP approach where I can calculate the similarity score of a conversation.</p>
<p>Teacher: &quot;What is your favourite animal?&quot;
Student: &quot;Football.&quot;
The model should output a contextual similarity score. It may not always be question and answer between the teacher and the student, but the core idea to determine if student is distracted.</p>
<p>I am thinking of Q&amp;A and semantic similarity but I am not sure which is better or if there is a more specific name for such a task.</p>
","nlp"
"124287","Leverage LLMs to classify sentence similarity","2023-10-26 21:09:13","","0","69","<classification><nlp><semantic-similarity><llm>","<p>This is intended to be mainly a reference request in the vast world of NLP and LLMs.</p>
<h2>Context</h2>
<p>A certain <em>protocol</em> is given in the form of text. This can be, for instance, the general description of a program in natural language or a statement regarding a legal or financial matter.</p>
<p>There is also a given set of <em>basic protocols</em>, which could be best-practices and/or basic algorithms for programming or basic regulations.</p>
<h2>Goal</h2>
<p>The protocol is supposed to implement or satisfy, at least partially, some of the basic protocols. The goal is to test the protocol against every basic protocol in order to get a binary classification:</p>
<p>1 - The protocol implements/satisfies (at least to some extent) the basic protocol.</p>
<p>0 - The protocol does not implement/satisfy (almost at all) the basic protocol.</p>
<h2>Example</h2>
<p>A very simple data science protocol could be the following.</p>
<blockquote>
<p>Extract the data and check for completeness and consistency. Scale the numerical features, input the data types expected by data validation and load the dataset into the DW. Organize the scaling and data type inputting in two functions documented following Google style docstring.</p>
</blockquote>
<p>Some basic protocols for this context and the related classification could be:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Basic Protocol</th>
<th>Implemented</th>
</tr>
</thead>
<tbody>
<tr>
<td>Check data accuracy</td>
<td>False</td>
</tr>
<tr>
<td>Check data completeness</td>
<td>True</td>
</tr>
<tr>
<td>Check data consistency</td>
<td>True</td>
</tr>
<tr>
<td>Check data timeliness</td>
<td>False</td>
</tr>
<tr>
<td>Check data validity</td>
<td>False</td>
</tr>
<tr>
<td>Check data uniqueness</td>
<td>False</td>
</tr>
<tr>
<td>Check data quality</td>
<td><em>Probably True</em> (Depends on &quot;threshold&quot;)</td>
</tr>
<tr>
<td>Set expected data types</td>
<td>True</td>
</tr>
<tr>
<td>Convert string features with less that 20% unique entries to category</td>
<td>False</td>
</tr>
<tr>
<td>Follow numpy docstring</td>
<td>False</td>
</tr>
<tr>
<td>Follow Google docstring</td>
<td>True</td>
</tr>
<tr>
<td>Use a data catalog</td>
<td>False</td>
</tr>
</tbody>
</table>
</div><h2>Problems</h2>
<ol>
<li>The basic protocols could come from different sources and so have very different styles and potentially conflicting terminology and/or various degree of verbosity.</li>
<li>Fine-tuning seems out of reach given the small amount of examples at hand.</li>
</ol>
<h2>Questions</h2>
<p>Could you point me to some attempt to leverage LLMs in order to deal with similar problems?</p>
<p>I imagine that it may make sense to create some middle layer in which basic protocols are uniformized according to some well-chosen examples, to brake the protocol into smaller components and for the classification to possibly use a mixture of similarity measures and a majority voting given by a bunch of Yes/No questions given to the LLM.</p>
<p>For the case I am interested in, a simple vectorization of the protocol and basic protocols gave pretty poor results. I kind of expected it, given that all the basic protocols belong to the same context of the protocol.</p>
","nlp"
"124273","TFRobertaSequenceClassification for Address Normalization task","2023-10-26 10:34:07","","0","26","<nlp><bert><normalization><language-model><finetuning>","<p>I have dataset with two column: one with faulty addresses, and other with correct addresses. I want to train a model such that, I can use it later for correcting all the incoming faulty addresses.
I have done tokenization, data splitting task for the same, but I can't make my model to start training.</p>
<p>I get GraphExecution error, which points towrads mismatch of dimension.</p>
<p>Exact error:
<strong>logits and labels must have the same first dimension, got logits shape [16,55] and labels shape [880]</strong></p>
<p>I don't know from where these values are coming from, as my X_Train consists of list with each list being length of <strong>45</strong>, and y_train is a list of length <strong>55.</strong> (All are integer values)</p>
<p>As, I am new to this task, any suggestions, comments, concerns, questions are welcome.</p>
<p>Also, I have tried basic ML approach, but the results were quite poor. Hence, please suggest me on the line of this approach only.</p>
<p>Here are some specifications to my approach:</p>
<pre><code>Tokenization: RobertaTokenizer.from_pretrained(&quot;roberta-base&quot;)
Model: TFRobertaForSequenceClassification.from_pretrained(&quot;roberta-base&quot;)
optimizer: Adam
learning_rate = 1e-5
loss: SparseCategoricalCrossentropy
epochs: 10
batch_size: 16
</code></pre>
<p>I think following are the list of models which we can use:</p>
<p><a href=""https://i.sstatic.net/Shp81.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Shp81.png"" alt=""enter image description here"" /></a></p>
<p>github link used: <a href=""https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/__init__.py"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/__init__.py</a></p>
<p>TIA</p>
","nlp"
"124265","Clustering words with similar meanings","2023-10-26 03:10:01","","0","188","<machine-learning><nlp><data-mining><text-mining>","<p>What methods are there to cluster words/word phrases with similar meanings together from a list of words/word phrases?</p>
","nlp"
"124256","How to improve GPT2 tokenizer trained from scratch?","2023-10-25 11:37:48","","0","72","<nlp><tokenization>","<p>I trained a GPT2 Tokenizer on Hindi dataset of size 170 MB from scratch and saved it as new_tokenizer. When I tried the new_tokenizer on a Hindi sentence</p>
<p><code>मेरा नाम विनय है</code></p>
<p>The number of tokens generated are 12 which are as follows</p>
<p><code>['म', 'े', 'र', 'ा', ' न', 'ा', 'म', ' व', 'ि', 'न', 'य', ' ह', 'ै']</code></p>
<p>I experimented with vocab size = 300, 500, 1000, 5000. In none of the cases new_tokenizer was able to capture complete words. How can I improve my tokenizer?</p>
<pre><code>from transformers import GPT2TokenizerFast

# validate the changes
text = &quot;मेरा नाम विनय है&quot;
new_tokenizer = GPT2TokenizerFast.from_pretrained(&quot;new_tokenizer_gpt2&quot;)

encoded_tokens = new_tokenizer.encode(text)
print(encoded_tokens)
print([new_tokenizer.decode(input_id) for input_id in encoded_tokens])
<span class=""math-container"">```</span>
</code></pre>
","nlp"
"124251","Expanding Training Data for Intent and Entity Recognition Model","2023-10-25 05:16:43","","0","13","<machine-learning><nlp><named-entity-recognition>","<p>I have a specific use case where I need to identify both intent and entities within a given statement. For example, given the statement &quot;Book train tickets from Mumbai to Delhi,&quot; the intent is &quot;BOOK_TRAIN_TICKETS,&quot; and the entities are &quot;source = Mumbai&quot; and &quot;destination = Delhi.&quot;</p>
<p>Currently, my model is experiencing overfitting due to limited training data. To address this issue, I've attempted to use data augmenters such as <strong>nlpaug</strong> with contextual word embedding augmentations. However, the results aren't as accurate as I'd like.</p>
<p>I'm seeking advice on any tools or libraries that can help generate additional training data with similar contextual relevance. For instance, I'd like to generate statements like &quot;Mumbai to Delhi train tickets&quot; to diversify the training data.</p>
<p>Any guidance or recommendations on tools, libraries, or techniques for this data augmentation process would be greatly appreciated. Thank you!</p>
","nlp"
"124235","How can I avoid the irrelevant number of sentences in the result?","2023-10-23 20:09:42","","0","44","<nlp><bert><semantic-similarity>","<p>The nature of the data I have is not arranged; however, I'm trying to extract the appropriate sentences for each query as a sample for ground truth. Also, the most critical problem is that I use the BERT model, which searches for the top 500 sentences in which the similarity value is greater than the 0.5 threshold. So the result is disorganized. For example, I got for a query</p>
<pre><code>5 for the relevant, 495 for the irrelevant, and 6 for the Total relevant
</code></pre>
<p>How can I avoid the irrelevant number of sentences in the result?</p>
","nlp"
"124233","Understanding Multi-headed Attention from architecture details","2023-10-23 19:12:28","","0","49","<machine-learning><deep-learning><nlp><transformer><attention-mechanism>","<p>I've a conceptual question</p>
<p>BERT-base has a dimension of 768 for query, key and value and 12 heads (Hidden dimension=768, number of heads=12). The same is conveyed if we see the BERT-base architecture</p>
<pre><code>(self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
)
</code></pre>
<p>Now, my question is:</p>
<blockquote>
<p>Can I consider the first 64 neurons from the <em>out_features</em> as the
first-head, the next 64 neurons from the <em>out_features</em> as the 2nd
head and so on? (sec 3.2.2 from original paper; <a href=""https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf"" rel=""nofollow noreferrer"">Link</a>)</p>
</blockquote>
<p>P.S: I referred to some of the previous posts (<a href=""https://datascience.stackexchange.com/questions/88330/how-do-the-linear-layers-in-the-attention-mechanism-work"">example</a>), but I would appreciate any validation on this thought-process as it's similar but not same.</p>
<p><strong>Update</strong></p>
<p>Here's a code which prunes a particular % in particular layer depending on <em>layer_index</em> and <em>prune_percentage</em></p>
<pre><code>model = AutoModelForMaskedLM.from_pretrained(checkpoint)

linear_layers_list = []
for name, layer in model.named_modules():
    if name in model_layers_list:
        linear_layers_list.append(layer)
print(f&quot;No of linear layers are: {len(linear_layers_list)}&quot;)

layer = linear_layers_list[layer_index]
if prune_type == 'ln_structured':
    # Ln structured with n=1 i.e L1 pruning
    prune.ln_structured(layer, name='weight', amount=prune_percentage, dim=0, n=n)
</code></pre>
<p>Here, I can understand that I can basically pass the Linear module and prune x% of weights.</p>
<p>Now, I would like to prune/remove one head in a similar fashion. Any help is appreciated!</p>
<p>Thanks</p>
","nlp"
"124224","In rotary positional embeddings (RoPE), why do we not rotate the values as well?","2023-10-23 10:53:39","","1","159","<nlp><transformer><encoding>","<p>Actually, the question is all there is</p>
<p>As per the <a href=""https://arxiv.org/pdf/2104.09864v4.pdf"" rel=""nofollow noreferrer"">paper</a> I see that the rotations are applied only to the keys and the queries. Why are the rotations not applied to the values as well?</p>
<p>The reasons for applying the rotations to the values as well:</p>
<ol>
<li>The sinusoidal embeddings are applied to all three: Keys, Queries and, the Values</li>
<li>Why would we not want the embedding values of the tokens to change depending on the relative positions of their occurrences?</li>
</ol>
","nlp"
"124200","What is the ""Extract"" token and how is the final Linear layer applied in GPT?","2023-10-22 04:08:28","124202","0","252","<nlp><transformer><transfer-learning><gpt>","<p>In the manuscript of GPT, the authors have given the following image:</p>
<p><a href=""https://i.sstatic.net/QYquD.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/QYquD.png"" alt=""enter image description here"" /></a></p>
<p>Questions:</p>
<ol>
<li>What is the final &quot;Extract&quot; (token?)? Is it the &quot;END&quot; token?</li>
<li>How is the final linear layer applied? We would get a tensor of shape (N,L,D) out of the transformer (tranformer's final layer would itself be a pointwise feedforward) where N is the number of samples, L is the sequence length and D is the output dimension for every token. Taking just one sample it would be (1,L,D). How is the linear layer applied on top of this? The output of the final linear layer would have to be a vector with dimensions equal to the number of decisions at hand (n in case of n-way-classifier, vocabulary size in case of cloze task etc)</li>
</ol>
","nlp"
"124197","Harford 2023 model: explicit reference","2023-10-21 09:18:28","","0","19","<nlp><language-model><reference-request>","<p>I'm citing in a paper the Wizard-Vicuna Uncensored 30B model of Hartford (2023). But I don't have an exact bib reference other than various web links for that model. Could anyone help?</p>
","nlp"
"124168","Semantic Scoring and readability for short sentences","2023-10-18 12:29:54","","0","31","<machine-learning><nlp><language-model>","<p>I am working on short sentences for NLP based classification. I wish to make a assessment if a sentence is readable before training the system on it. Now readability scores are not working since readability score indicate how simple a sentence is not how correct a sentence is.</p>
<p>What I am searching for is something that measures(or scores) the readability of a sentence. For example the sentence 'The sun rises in the east' should be given a high score whereas the sentence 'sun the east in rises' should be given a low score.</p>
<p>Is there any python package for this or could someone indicate some direction for it?</p>
","nlp"
"124163","Is openAI text generation models an extension of embedding models?","2023-10-18 03:24:40","","1","48","<nlp><word-embeddings><embeddings><gpt><stanford-nlp>","<p>we can creating embeddings using below code</p>
<pre><code>import openai
response = openai.Embedding.create(
  input=&quot;porcine pals say&quot;,
  model=&quot;text-embedding-ada-002&quot;
)
</code></pre>
<p>And we can generate text using below code</p>
<pre><code>def get_completion(prompt, model=&quot;text-davinci-003&quot;):
messages = [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}]
response = openai.ChatCompletion.create(
model=model,
messages=messages,
temperature=0,
)
return response.choices[0].message[&quot;content&quot;]
</code></pre>
<p>are text completions models extension of embeddings generation models, I mean is it like the embeddings generation models are further finetuned for chat/text generation?</p>
<p>or is it like both models are completely different in terms of their training and architecture?</p>
","nlp"
"124127","A question about contextual embeddings in the decoder only transformer architecture (gpt)","2023-10-15 16:00:13","124133","1","312","<nlp><word-embeddings><transformer><gpt><context-vector>","<p>I am reading up on the <a href=""https://stanford-cs324.github.io/winter2022/lectures/training/#decoder-only-models"" rel=""nofollow noreferrer"">decoder only architecture</a></p>
<p>Relevant excerpts:</p>
<p>We can use any model that maps token sequences into contextual embeddings (e.g., LSTMs, Transformers):</p>
<p><span class=""math-container"">$$\phi : V^L \to R^{d \times L}$$</span></p>
<p>Recall that an autoregressive language model defines a conditional distribution:</p>
<p><span class=""math-container"">$$p(x_i∣x_{1:i−1})$$</span></p>
<p>We define it as follows:</p>
<ul>
<li>Map <span class=""math-container"">$x_{1:i-1}$</span> to contextual embeddings <span class=""math-container"">$\phi(x_{1:i-1})$</span></li>
<li>Apply an embedding matrix <span class=""math-container"">$E \in R^{V×d}$</span>to obtain scores for each token <span class=""math-container"">$E\phi(x_{1:i-1})_{i-1}$</span></li>
<li>Exponentiate and normalize it to produce the distribution over xi</li>
</ul>
<p>Succinctly:
<span class=""math-container"">$$p(x_{i+1} \mid x_{1:i}) = softmax(E \phi(x_{1:i})_i)$$</span></p>
<p>Questions:</p>
<ol>
<li>What is the meaning of the second subscript on <span class=""math-container"">$\phi$</span> in <span class=""math-container"">$ E \phi (x_{1:i-1})_{i-1}$</span></li>
<li>I think  <span class=""math-container"">$softmax(E \phi(x_{1:i})_i)$</span> just takes the dot product of the context embedding for the word at the position <span class=""math-container"">$i$</span> with the embeddings <span class=""math-container"">$E$</span> of the entire vocabulary. This means that for the word at <span class=""math-container"">$i$</span>, we are basically just trying to learn the context embeddings as something that would essentially be equal to the embedding of the next token (if the model learns perfectly) in <span class=""math-container"">$E$</span>. Why is that the case? Should there not be a feed-forward between the final context embeddings and the embedding <span class=""math-container"">$E$</span> for the next token and then the similarity should be checked? This way, in the best case scenario, the contextual embeddings learned would just be the vector that appeared for the word at <span class=""math-container"">$i$</span> in <span class=""math-container"">$E$</span>. Please help me understand how we are not just asking the contextual embedding to be equal to the embedding <span class=""math-container"">$E$</span> for the word at <span class=""math-container"">$i$</span>?</li>
</ol>
","nlp"
"124118","How to read CSV File into Vector Store","2023-10-14 18:19:59","","0","947","<python><nlp><language-model><csv>","<p>I have a CSV file, and I am using langchain to read it into the vector store FAISS. My question is, since I have a CSV file, is RecursiveTextSplitter required? Put differently, consider the following codes</p>
<pre><code># Code A ------------------------------
myData = CSVLoader(file_path=myCSVFile&quot;)
finalData = myData.load()

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=100,
    chunk_overlap = 10,
    length_function = len,
)

theFile = text_splitter.transform_documents(finalData)

vector_store = FAISS.from_documents(theFile, embedder)
</code></pre>
<p>Compare to following code:</p>
<pre><code># Code B ------------------------------
myData = CSVLoader(file_path=myCSVFile&quot;)
finalData = myData.load()

vector_store = FAISS.from_documents(finalData, embedder)
</code></pre>
<p>Is code A correct or coe B?</p>
","nlp"
"124112","How to use location information as feature?","2023-10-13 03:41:53","","0","31","<nlp><word-embeddings>","<p>I have a location feature in a dataset. Some examples are: London, Uk; Sheefield Town, Ohio; UK ; North Carolina. etc. How to encode them into features? Is there any word embeddings suitable for such location information?</p>
","nlp"
"124109","To extend or not to extend vocabulary for instruction tuning","2023-10-12 20:36:44","","0","46","<nlp><llm>","<p>I want to fine tune a base llm using an instruction dataset. In order to minimize VRAM footprint I want to use SFTTrainer and QLora. My prompt can take the following structure:</p>
<pre><code>    [INST] &lt;&lt;SYS&gt;&gt;\nSystem_Message_Here\n&lt;&lt;/SYS&gt;&gt;\n\nUser_Msg_1 [/INST]_Msg_1 [INST] User_Msg_2 [/INST] Asst_Msg_2 [INST] User_Msg_3 [/INST]
</code></pre>
<p>I am wondering if I shall add to my tokenizer new tokens <em>[INST]</em> <em>&lt;&lt; SYS&gt;&gt;</em> <em>&lt;&lt;/ SYS&gt;&gt;</em> or <em>[/INST]</em> - or if I shall keep the base model vocabulary ?</p>
","nlp"
"124034","Training model using BERT","2023-10-07 16:46:44","124035","1","174","<nlp><data-science-model><bert><finetuning>","<p>I have generated dataset using chat gpt. Dataset has 9000 data recodes. It's 6 class sentiment analysis. classes are 0,1,2,3,4,5
I used 3000 recodes for training, 1200 recods for validation and testing.</p>
<p>This is the class counts</p>
<p>For training:</p>
<pre><code>5: 622 ,3: 614 ,0: 593,4: 571,2: 604,1: 596
</code></pre>
<p>For Validation:</p>
<pre><code>1: 221,5: 193,0: 193,4: 212,3: 182,2: 199
</code></pre>
<p>For Testing:</p>
<pre><code>3: 204,2: 197,0: 214,4: 217,1: 183,5: 185
</code></pre>
<p>I trained this using BERT('bert-base-uncased') with 25 epochs. Learning rate 2e-5</p>
<p>This is the result.</p>
<p><a href=""https://i.sstatic.net/acDCp.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/acDCp.png"" alt=""enter image description here"" /></a></p>
<p>Validation</p>
<pre><code>Validation Accuracy: 0.5666666666666667
Validation Classification Report:
              precision    recall  f1-score   support

           0       0.53      0.69      0.60       193
           1       0.54      0.50      0.52       221
           2       0.52      0.49      0.51       199
           3       0.54      0.50      0.52       182
           4       0.55      0.60      0.57       212
           5       0.77      0.63      0.69       193

    accuracy                           0.57      1200
   macro avg       0.58      0.57      0.57      1200
weighted avg       0.57      0.57      0.57      1200
</code></pre>
<p>Testing</p>
<pre><code>Test Accuracy: 0.5658333333333333
Test Classification Report:
              precision    recall  f1-score   support

           0       0.57      0.68      0.62       214
           1       0.49      0.56      0.52       183
           2       0.58      0.52      0.55       197
           3       0.59      0.49      0.53       204
           4       0.53      0.57      0.55       217
           5       0.68      0.58      0.62       185

    accuracy                           0.57      1200
   macro avg       0.57      0.56      0.57      1200
weighted avg       0.57      0.57      0.57      1200
</code></pre>
<p>I trained this with using different count of data. but graph shape is same with different accuracies. My questions are:</p>
<ol>
<li>how to increase accuracy and what whould be the issue?</li>
<li>Is that a issue with some words in multiple classes (because i some words in many classes in this dataset)?</li>
</ol>
","nlp"
"123996","what is the difference between window size and context length of language model?","2023-10-05 12:43:10","123997","2","748","<nlp><training><gpt><pretraining><chatgpt>","<p>is window size and context length of language model one and the same thing?</p>
<p>******** following text is added as question with ONLY above text was not allowed *****
I am trying to understand how GPT model is trained and this question to my mind.
I tried to search answer on google but couldn't find an answer thus asking here.</p>
","nlp"
"123995","Product name matching - Entity Resolution or Entity Linkage or both?","2023-10-05 09:14:20","","1","117","<nlp><definitions><entity-linking>","<p><strong>Context</strong></p>
<p>I am at the start of a project where I would like to map/match/link external product names to the respective internal product names. The goal should be to ingest related external information (e.g. stock number) of the external products into our system by joining the same products based on their product names. Short, the external product name should be matched to the internal representation of the product name.</p>
<p><strong>Problem and Question</strong></p>
<p>I'm now doing some research about potential solutions and I'm having difficulties finding out if the nature of the problem can be allocated to Entity Resolution or Entity Linkage or if it even includes both of them. I'm asking this because I'm afraid to go down the wrong path when researching for a potential way to tackle the problem. I have seen the post about <a href=""https://datascience.stackexchange.com/questions/115528/exemplify-key-differences-between-entity-linking-and-entity-matching#:%7E:text=As%20depicted%20below%2C%20entity%20linking,reference%20repository%20or%20knowledge%20base.&amp;text=However%2C%20in%20entity%20matching%20the,knowledge%20base%20do%20not%20exist.&amp;text=mirror%2Dimage."">key differences about entity linking and entity matching</a>, but it's still hard for me to allocate the nature of my problem to one of them. Can please someone tell me if the problem can be allocated to Entity Resolution, Entity Linkage or both, and why this is the case?</p>
<p>Thanks a lot!</p>
","nlp"
"123954","Locating base.py when working on Colab","2023-10-02 16:27:22","","0","20","<python><nlp><language-model><colab>","<p>I have faced an error while working with langchain on colab. There is a post on github which recommends changing some configurations in</p>
<pre><code> local / repo find-&gt; langchain/agents/agent_toolkits/pandas/base.py
</code></pre>
<p>Could you please let me know how I can find and configure base.py while working on colab?</p>
","nlp"
"123924","A good set of datasets/models for testing an NLP technique","2023-09-30 17:58:12","124090","1","332","<deep-learning><nlp><dataset><transformer>","<p>I am a machine learning researcher who up until this point has primarily worked on Computer Vision problems. However, I have an idea for an NLP technique involving a novel Transformer architecture, and I’d like to explore it.</p>
<p>What’s a good progression of datasets/models to explore? The technique I have in mind is pretty general and should apply to any decoder-only architecture. If it were, say, an image classification problem I might start with ResNet on an MNIST variant or CFAR, then move on to ImageNet. What's the NLP equivalent of that?</p>
<p>Unless there's a better way, I’d like to start by training from scratch on something small and comparing to a vanilla transformer. If things work I’ll want to try my ideas on more state-of-the art models and datasets, probably through fine-tuning. I don’t have a lot of resources, though I do plan to ultimately work up to a GPT-2 fine-tuning if I’m feeling confident.</p>
<p>Thank you in advance!</p>
","nlp"
"123921","Why do we want to maximize the average log probability in neural language models?","2023-09-30 12:56:07","","0","78","<nlp><language-model><doc2vec>","<p>I am currently trying to understand the Paragraph Vector framework by reading the paper <a href=""https://proceedings.mlr.press/v32/le14.html"" rel=""nofollow noreferrer"">&quot;Distributed Representation of Sentences and Documents&quot;</a> by Quoc Le and Thomas Mikolov but I have troubles following the formal description since my current understanding of Neural Networks is limited to mainly intuition. In the paper, they explain former techniques of learning word vector representations including Neural Language Models. Their formal description of the training task of Neural Language Models is as follows:</p>
<blockquote>
<p>More formally, given a sequence of training words <span class=""math-container"">$w_1 , w_2 , w_3 , ..., w_T$</span> , the objective of the word vector model is to maximize the average log probability <span class=""math-container"">$\frac{1}{T}\overset{T-k}{\underset{t=k}{\sum}}\log p(w_t | w_{t-k}, ..., w_{t+k})$</span></p>
</blockquote>
<p>Unfortunately, they don't explain why the goal is to maximize the average log probability and how they came up with this formula. Also search requests and looking through <a href=""https://jmlr.csail.mit.edu/papers/volume3/bengio03a/bengio03a.pdf"" rel=""nofollow noreferrer"">&quot;A Neural Probabilistic Language Model&quot;</a> by Bengio et al. did not bring me any further.</p>
<p>My understanding of the training task is that given a context, the model predicts the missing word of a context. Moreover, out of all words in the vocabulary the word with the highest conditional probability given the context is selected as the missing word. Therefore I don't understand why this formula does not include some sort of <span class=""math-container"">$\mathrm{argmax}$</span> expression.</p>
<p>Any help on understanding why this is the goal and additional resources on this would be  appreciated.</p>
","nlp"
"123919","Help understanding working of KeyBERT for keyphrase extraction","2023-09-30 07:20:08","","0","98","<deep-learning><nlp><bert><tokenization><document-understanding>","<p>I'm fairly new to reading and understanding research papers, so I wanted to get a second opinion on whether my understanding of KeyBERT was correct. Here is a high level overview of my understanding with an example -</p>
<ul>
<li>example sentence - “I like reading. I enjoy swimming. Reading is fun.”</li>
<li>let stop_words = None, ngram_range = (2, 2) that means bigrams will be considered, let number of candidate words we want to return ( top_n ) =  2</li>
<li>steps -
<ul>
<li>sentences will be split into unique bigrams ( 2 words ) - [ ‘i like’, ‘like reading’, ‘i enjoy’, ‘enjoy swimming’, ‘reading is’, ‘is fun’ ]</li>
<li>this list is passed to BERT for embedding. BERT will encode each element of the list and return a list of vectors v1,...,v6</li>
<li>The document is also passed to BERT for embedding, and a vector v7 representing the document is returned.</li>
<li>if seed_keywords, i.e. words that should be given more importance are given as a parameter, they are also embedded into vectors, and mean, say c1, of those vectors is taken. This vector c1 is then added to document vector v7.</li>
<li>it computes a similarity score between each of the candidate embedding vectors v1,v2,...,v6 with the document embedding vector v7. it uses cosine similarity for this.</li>
<li>it sorts the candidate vectors in descending order by similarity score, and returns the top 2 ( top_n ) vectors</li>
</ul>
</li>
</ul>
<p>Let me know if I'm missing any details. Also, are there any better alternatives for the task of keyphrase extraction for specific domain?</p>
","nlp"
"123908","How to get Llama-2 Rotary Embeddings?","2023-09-29 11:31:13","","0","319","<nlp><word-embeddings><transformer><language-model><huggingface>","<p>I want to get the Llama-2 rotary embeddings. I do <code>print(model)</code> and get the following output:
<a href=""https://i.sstatic.net/jFu2Z.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/jFu2Z.png"" alt=""enter image description here"" /></a></p>
<p>In the picture I highlight the rotary embeddings.</p>
<p>How can get the rotary embeddings and how can I interpret the output? What means 32x LLamaDecoderLayer and in its round brakets are four layer plus LlamaRotaryEmbeddings?</p>
<p>It's possible to get the embeddings as the first hidden-state <code>hidden_state[0]</code> and I want to know, which hidden-state represents the rotary embeddings.
Am I right, that there are several rotary embeddings?</p>
<p>Thanks in forward.</p>
<p>Best regards.</p>
","nlp"
"123904","How does fine-tuning work in question answering for custom documents","2023-09-29 03:53:55","","2","182","<machine-learning><nlp><ai><question-answering>","<br>
I am trying to build a Q&A bot for which I have a bunch of documents like articles (specific domain).<br>
I understand I can create a Retrieval-Augmented Generation (RAG) system for this, but I want to know how does fine-tuning work for this case, what would be the approach here?<br><br>
Would it be creating a question-answer pairs (without context) manually or automated using llms example(gpt-4) and use a pre-trained model such as LLAMA-2 to fine-tune on this QA dataset? (Creating question-answer pairs would it mean I have to create thousands of question-answer pairs that would capture almost everything about the documents I have?)<br><br>
Also, if I were to pre-train the model (LLAMA-2) on the documents I have and then fine-tune on the Question-Answer (no context) , would it yield better results?<br>
<p>Thank you for you time in advance.</p>
","nlp"
"123890","Store and retrieve multiple documents in free vector stores based on critria","2023-09-28 08:31:09","","0","174","<python><nlp><reference-request><vector-database>","<p>I am new to vector stores, and so far experimented with storing 1 file in Faiss and Pinecone. I am looking for tutorials that teach me how to save multiple files in free versions of any vector store, and retrieve them based on a user criteria. For instance, I could have document on sports and other on health, and through choice of user for specific words, I need to be able to retrieve the correct document and run search on them.</p>
","nlp"
"123889","Specifying arguments of HuggingFaceHub","2023-09-28 07:48:29","","0","266","<python><nlp><language-model><huggingface>","<p>In this <a href=""https://python.langchain.com/docs/integrations/llms/huggingface_hub"" rel=""nofollow noreferrer"">tutorial</a>, when specifying</p>
<pre><code>llm = HuggingFaceHub(
    repo_id=repo_id, model_kwargs={&quot;temperature&quot;: 0.5, &quot;max_length&quot;: 64}) 
</code></pre>
<p>only repository id is mentioned, without referring to the task that is to be performed (for instance summarization or text generation). However, one may specify something like</p>
<pre><code>dotaks = transformers.pipeline(
    model=repo_id, 
    tokenizer=tokenizer,
    task='text-generation',
    return_full_text=True,
    temperature=0.5,
    max_new_tokens=124,)
</code></pre>
<p>If I try to remove the task it would not work. What is the difference among the above two approaches? How will the top code decide on the appropriate task if not specified?</p>
","nlp"
"123883","Combining Textual, Categorical and Numerical data for Semantic Search using SentenceTransformers model","2023-09-28 00:50:48","","0","67","<nlp><categorical-data><bert><embeddings><search>","<p>I'm building a food semantic search model and I want to use a pre-trained <a href=""https://www.sbert.net/examples/applications/semantic-search/README.html"" rel=""nofollow noreferrer"">SentenceTransformers</a> model with cosine similarity. I'm using <a href=""https://www.kaggle.com/datasets/hugodarwood/epirecipes/data?select=full_format_recipes.json"" rel=""nofollow noreferrer"">Epicurious</a> dataset for the corpus which consists of textual (&quot;title&quot;, &quot;description&quot;, &quot;directions&quot;) as well as categorical (&quot;categories&quot;) and numerical data (&quot;calories&quot;, &quot;fat&quot;, &quot;sodium&quot;).</p>
<p>The model that computes embeddings for the corpus takes <code>String</code> data as an input. So my idea was to combine the categorical and numerical data with textual data in a single <code>String</code>.</p>
<p>Do you think it's a right way to handle categorical data? If yes, what are the ways to concatenate such data (maybe using column names with the values or adding some tags)? Otherwise, what is a better way to handle such data?</p>
","nlp"
"123867","How to deal with short text data using NLP models?","2023-09-27 02:41:01","","0","24","<nlp><bert>","<p>Now I want to use my own domain data to train NLP model like BERT. The following is the details of my data:</p>
<ol>
<li>data length distribution: over 70% of my data has the length shorter than 5 and the largest length is 14;</li>
<li>data format: the data is a list of number representing the AS-PATH of BGP announcement.</li>
</ol>
<p>I've tried to use HuggingFace transformer package to define BERT model and train, but the MLM loss is quite high. I think this is the cause of data length.</p>
<p>So I want to know that is there any way to deal with these short data?</p>
","nlp"
"123840","NLP Decision confidence analysis","2023-09-25 08:59:39","","0","17","<nlp><machine-learning-model>","<p>I have text from people explaining their decision. I want to extract a confidence score of how confident they are in their decision.
I am looking for a pre-trained model. I looked up online but I found nothing. Is there a specific name for this task?</p>
","nlp"
"123836","What is source_column argument in csv loader?","2023-09-25 06:46:04","123838","1","641","<python><nlp><language-model><csv>","<p>In <a href=""https://python.langchain.com/docs/integrations/document_loaders/csv"" rel=""nofollow noreferrer"">this tutorial</a>, what is the purpose of source_column argument? Does it act like a primary key in Databases? Thanks in advance.</p>
<pre><code>loader = CSVLoader(file_path=&quot;./example_data/mlb_teams_2012.csv&quot;, source_column=&quot;Team&quot;)
data = loader.load()
</code></pre>
","nlp"
"123835","Finding the suitable semantic search engine tool for querying an excel/csv file containing data rich in numbers","2023-09-25 05:39:33","","0","83","<nlp><aws><azure-ml><search-engine>","<p>I have an excel dataset that has the columns: name of the test, user name, data source, total number of records, number of failed records, number of passed records, date of test execution etc. I want questions like the following to be answered: How many tests were executed by user_1?, What data source has the most number of failed records? How many tests were run on a particular date? etc.</p>
<p>I want to build a semantic search engine using which the user can get answers to the above questions. I tried Amazon Kendra which is an enterprise semantic search engine, but it didn't prove to be effective on numerical data analysis.</p>
<p>Now, I'm considering trying Azure Cognitive Search for the above use case.</p>
<p>Can someone please shed light on the right tool or right approach for my use case?</p>
","nlp"
"123826","How to Use Multiple Adapters with a Pretrained Model in Hugging Face Transformers for Inference?","2023-09-23 22:02:38","","1","587","<deep-learning><nlp><pytorch><huggingface><inference>","<p>I have a pretrained Llama-2 model in the <code>models_hf</code> directory and two fine-tuned adapters: a summarization adapter in <code>./tmp/llama-output</code> and a chat adapter in <code>./tmp/chat_adapter</code>. The details of the code are in <a href=""https://stackoverflow.com/questions/77164963/how-to-merge-fine-tuned-adapter-and-pretrained-model-in-hugging-face-transformer"">another question</a>.</p>
<p>For inference, I'd like to use all three components: the pretrained model, summarization adapter, and chat adapter. However, I'm unsure about the memory requirements and the best approach.</p>
<p>Do I need to:</p>
<p><strong>Option 1</strong>: Load one instance of the pretrained model (e.g., 10GB GPU memory), and then load the two separate instances of adapters (e.g., 100MB each), resulting in a total memory usage of 10GB + 200MB?</p>
<p>OR</p>
<p><strong>Option 2</strong>: Load two separate instances of the 10GB pretrained model and stack the summarization adapter on one and the chat adapter on the other, resulting in a total memory usage of 20GB + 200MB?</p>
<p>Additionally, could you provide a code example or steps on how to load and use these components for inference effectively? I'm looking for guidance on memory management and loading processes to ensure smooth and efficient inference with this setup.</p>
","nlp"
"123729","Why is Spacy sentiment score 0.0 for a sentence?","2023-09-16 17:23:30","123734","1","198","<nlp><sentiment-analysis><language-model><spacy>","<p>I'm trying to get a sentence's sentiment score using Spacy and apparently every sentence I pass gets a score of 0.0. Can someone help me understand what's going wrong here?</p>
<pre><code>import spacy

nlp = spacy.load(&quot;en_core_web_sm&quot;)
doc1 = nlp(&quot;This Movie is really Great!&quot;)
doc2 = nlp(&quot;This Movie was the Worst!&quot;)

print(&quot;Sentiment score for 1st Sentence &quot;,doc1.sentiment)
print(&quot;Sentiment score for 2nd Sentence &quot;,doc2.sentiment)

for token in doc1:
    print(token.text, token.pos_, token.dep_)


for token in doc2:
    print(token.text, token.pos_, token.dep_)
</code></pre>
<p>Which gives the output as:</p>
<pre><code>Sentiment score for 1st Sentence  0.0
Sentiment score for 2nd Sentence  0.0
This DET det
Movie PROPN nsubj
is AUX ROOT
really ADV advmod
Great ADJ acomp
! PUNCT punct
This DET det
Movie PROPN nsubj
was AUX ROOT
the DET det
Worst ADJ attr
! PUNCT punct
</code></pre>
<p>Spacy's documentation says this about <code>Doc.sentiment</code>:</p>
<p><a href=""https://i.sstatic.net/XCQAD.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/XCQAD.png"" alt=""enter image description here"" /></a></p>
","nlp"
"123718","""text"" parameter in pinecone call from langchain","2023-09-15 16:46:35","","1","34","<python><nlp><word-embeddings><language-model><vector-database>","<p>In <a href=""https://python.langchain.com/docs/integrations/vectorstores/pinecone"" rel=""nofollow noreferrer"">this tutorial</a>, I do not understand what &quot;text&quot; refers to</p>
<pre><code>vectorstore = Pinecone(index, embeddings.embed_query, &quot;text&quot;)
</code></pre>
<p>Could you please help?</p>
","nlp"
"123706","Parsing response from llama2","2023-09-15 02:50:25","","0","149","<machine-learning><deep-learning><nlp><llm>","<p>I want to extract phone numbers from a given text and i am prompting a llama2 model for that ..I want the output in form of a list but i am getting unnecessary output like sure here are the phone numbers etc etc...can anyone help how to only extract the phone numbers from the response?</p>
","nlp"
"123659","How does supervised fine-tuning work in InstructGPT?","2023-09-11 17:38:19","","1","216","<machine-learning><nlp><supervised-learning><gpt><finetuning>","<p>See Figure 2 from the <a href=""https://arxiv.org/pdf/2203.02155.pdf#page=3"" rel=""nofollow noreferrer"">InstructGPT paper</a>:
<a href=""https://i.sstatic.net/zmnla.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/zmnla.png"" alt=""Figure 2 from the InstructGPT paper"" /></a></p>
<p>I want to know how Step 1 works. Here is one possible algorithm.</p>
<ol>
<li>Pass the prompt through the model, and compute the negative log of the probability of the first token in the desired output. Update the model weights to minimize this loss.</li>
<li>Pass the prompt through the model, followed by the first token from the desired output, and compute the negative log of the probability of the second token in the desired output. Update the model weights.</li>
<li>Pass the prompt through the model, followed by the first two tokens from the desired output, and compute the negative log of the probability of the third token in the desired output. Update the model weights.</li>
<li>Continue until the model has attempted to predict every token in the desired output, updating the weights after each attempt.</li>
</ol>
<p>After reading the SFT section of the <a href=""https://arxiv.org/pdf/2307.09288.pdf#subsection.3.1"" rel=""nofollow noreferrer"">Llama 2 paper</a>, I think this is almost correct. However, instead of updating the weights multiple times, we can run the model on the prompt and the desired output together, to get all the aforementioned negative log results at once. Then, we can sum all these results, and do one weight update to minimize this loss. Is that correct?</p>
","nlp"
"123652","Text preprocessing decreases classifier accuracy","2023-09-11 11:04:43","","2","73","<nlp><naive-bayes-classifier><tfidf>","<p>I try to solve a binary text classification problem using sklearn's Tfidf Vecotrizer and a naive bayes classifier. Before I pass the training/test data to the vectorizer I do some text preprocessing. I thought that would increase the performance compared to non preprocessed text. However, the performance did not increase, but actually decreases.</p>
<p>The text that should be classified comes from different PDFs, therefore the text can include unwanted characters/examples like bullet points, dashes if there is a line break or headings. I consider all of that to be noise in my data so I deal with that with some preprocessing.</p>
<p>The text preprocessing consists of the following steps:</p>
<ul>
<li>lowercase everything</li>
<li>remove unwanted characters (punctuation, white spaces, bullet points etc.)</li>
<li>remove stop words (using spacy's built in stop word list)</li>
</ul>
<p>Prior to the preprocessing I do a 80/20 train/test split on the data and pass it into a sklearn pipeline consisting of an Tfidf vectorizer and naive bayes classifier. Finally, I run a GridSearch on the pipeline in order to find a good set of params. The performance of the best classifier's performance is reported with classification_report.</p>
<p>In order to see how much of an improvement I gained from the preprocessing, I passed the raw text data to the pipeline. However, the classifier that is trained on the raw data outperforms the classifier that is trained on the processed data.</p>
<p>I assumed that the performance should increase since I kind of normalise my data. Has anyone some advice on why this is the reason? Maybe my assumption is already false or my interpretation of the result is false?</p>
","nlp"
"123643","What are the paprements needed and how should be filled using LIME with NLP in Python?","2023-09-10 21:10:57","","0","10","<python><nlp><lime>","<p>it is my first time using LIME and I have never used any interpretation technique before.</p>
<p>most likely I am doing something wrong but I cannot figure out what is it.</p>
<p>I tried googling and going through SOF questions to find the way to resolve this but did not find anything that could help me.</p>
<p>my dataset <strong>df_reps</strong> looks like this</p>
<pre><code>Toyota Horse Toyota Gear... Mazda Night King
Green Mazda King Toyota ... Blue Mazda Toyota
...
...
Gear Tyre Toyota Geaer ... Horse Blue Park
Laptop Invoice Toyota ...  Horse Mango Kitkat
</code></pre>
<p>and labels to predict, is whether the customer approved of not so the labels are only 0 and 1</p>
<p>Here is my code</p>
<pre><code>def BOW(df):
  CountVec = CountVectorizer() # to use only  bigrams ngram_range=(2,2)
  Count_data = CountVec.fit_transform(df)
  Count_data = Count_data.astype(np.uint8)
  cv_dataframe=pd.DataFrame(Count_data.toarray(), columns=CountVec.get_feature_names_out(), index=df.index)  # &lt;- HERE
  return cv_dataframe.astype(np.uint8)

df = BOW(df_reps)
y = df_Labels    # this is either 0 or 1
X = df
X_train, X_test, y_train, y_test = train_test_split(X, y)

clf = RandomForestClassifier(max_depth=100)
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)


# Here is the part for LIME

explainer = LimeTextExplainer()
exp = explainer.explain_instance(y_pred, clf.predict, num_features=10000)
</code></pre>
<p>How can I fix the LIME part so it actually gives me interoperability for y_pred results?</p>
","nlp"
"123586","What is the use of using Vector databases over Pandas Dataframes?","2023-09-06 10:57:54","123587","0","772","<machine-learning><nlp><pandas><dataframe><vector-database>","<p>My question is why can't we use a pandas dataframe to do all the data retrieval? Can someone give reasons apart from the fact that databases has the persistence of Data?</p>
","nlp"
"123572","Removing specific phrases from textual data (R)","2023-09-05 18:51:59","123588","1","36","<machine-learning><nlp><r><data-cleaning><unsupervised-learning>","<p>I have the following reddit posts and I would like to clean the posts and remove from the data the specific phrase &quot;Click to expand&quot;, while keeping all other words within a post the same.</p>
<pre><code>#Loading packages
library(tidyverse)
require(readxl)
library(quanteda)
</code></pre>
<p>#data example</p>
<pre><code>dput(df[1:20,c(1,2,3)])
</code></pre>
<p>output:</p>
<pre><code>structure(list(id = 1:20, username = c(&quot;106gunner&quot;, &quot;CPTMiller&quot;, 
&quot;matey1982&quot;, &quot;Why so serious&quot;, &quot;Joe Maya&quot;, &quot;Toomin&quot;, &quot;wadtheEel&quot;, 
&quot;Witch King&quot;, &quot;106gunner&quot;, &quot;roronoa_zoro&quot;, &quot;yonglimm&quot;, &quot;laopokcar_g&quot;, 
&quot;nymous&quot;, &quot;DragonBlack&quot;, &quot;betking&quot;, &quot;archon75&quot;, &quot;ahkrong&quot;, &quot;ahkrong&quot;, 
&quot;archon75&quot;, &quot;[人言可畏]&quot;), post = c(&quot;Was reported in SCMP news source underneath link&quot;, 
&quot;Government already said ft or CECA create new good jobs for Singaporean&quot;, 
&quot;gunner said Was reported in SCMP news source underneath Singaporean to expand arent u stating the obvious&quot;, 
&quot;lightboxclose Close lightboxnext Next lightboxprevious Previous lightboxerror The requested content cannot be loaded Please try again later lightboxstartslideshow Start slideshow lightboxstopslideshow Stop slideshow lightboxfullscreen Full screen lightboxthumbnails Thumbnails lightboxdownload Download lightboxshare Share lightboxzoom Zoom lightboxnewwindow New window lightboxtogglesidebar Toggle sidebar&quot;, 
&quot;From personal experience i lost my job to jhk&quot;, &quot;ceca ftw&quot;, 
&quot;edmw say yes but govt say no Who to believe&quot;, &quot;I will welcome ceca if pap have ceca candidates in the Parliament&quot;, 
&quot;matey said arent u stating the obvious Click to expand Surprised SCMP news also reported&quot;, 
&quot;wadtheEel said edmw say yes but govt say no Who to believe Click to expand I believe the govt Every year we can only produce ish IT uni graduates Got lots of IT jobs opening not enough if only hire them Posted from PCWX using SMGN&quot;, 
&quot;SONG BOH&quot;, &quot;Lies la Say ICT is a small Ask them how many ceca ICT have become PR or citizen This qn xia suay chan dont dare to answer&quot;, 
&quot;Ceca is threatof cos as usual sinkapore like to learn in hard way&quot;, 
&quot;CECA pls fug off We must stop these CECA cocaine from further entering into SG to worsen the addiction of CECA PMETs by companies like drug addicts We already had enough of FT office politics from Pinoy JHK and AT and now the CECA ones are really at another level Mo Tak Teng not only in the office but most glaring fact is their behaviour in our common public places are so obnoxious til unbearable Its so sickening to keep seeing CECAs everywhere bring in their spouses and families misbehaving themselves bulldoze their prams and trolleys into lifts without basic cowsense courtesy taking loudly in the lifts with kids try to show off their Inglish practiceasking their parents nonsensical why this why that in irritating accent weird inconsiderate behavior on the streets at work places malls hotels hogging at the counters due to some weird problems they face during payments in the parks even at Botanic Gardens PCB etc totally overwhelming like cannot escape from them anywhere in Spore NBCB Seems that unlike decades ago the BanglasIndian bluecollar FWs roaming and littering everywhere problem has become less of an issue these days they are more Ji Tong so to speak perhaps due to public complaints and more of them being confined to larger dormitories with fullfacilities like cinemas etc Since PAP love them so much maybe we should also build some CECA city maybe at Jurong Island or Pulau Hantu or JB even better nbcb LOL and confined these idiots inside like a lockdown to knock themselves out to prevent them from continuing to unleash their poison to our society Its mindboggling that after so many years PAP is still seemingly either unaware or chose to ignore the toxic culture that CECA is bringing to Singapore and still want to sing praises for them in parliament and the media Best part is they blame and even threaten Singaporeans from bringing up these topics which is already like shit stuck on our faces Such a sad story state we are having now I really hope in next few GEs more talents will join the opposition to quickly take up more seats and eventually kick out these elitist scums from further harming our citizens and the socioeconomic fabric of our society Talk so much also useless Rooks rike this is inevitableGE The New Dawn GE Rise of the Phoenix GE The New Era Unless the kumgong love to remain in this horror state then they reap what they sow&quot;, 
&quot;SCMP dun waste our tiam lah TS&quot;, &quot;Talk also no used SG local population is split old folks who happy of $ + public service sure vote for Then Jiuhu + Ceca + Ah Thiong that form bulk of new citizen sure vote for No way can win&quot;, 
&quot;DragonKnightMaster said u really no logic companies can be run by foreigners but country can anyway we have ceca turned singaporean in our parliament already if u not aware Click to expand You are slapping yourself LOLOLOLOLOLOL Sent from mai dua karchng using GAGT&quot;, 
&quot;archon said Talk also no used Singaporean local population is split old folks who happy of $ + public service sure vote for Then Jiuhu + Ceca + Ah Thiong that form bulk of new citizen sure vote for No way can win Click to expand sgdivided Well done LOLOLOLOLOLOL Sent from mai dua karchng using GAGT&quot;, 
&quot;Will we have Ah Thiong turned new citizen in parliament&quot;, &quot;of cos made things easier for them got free job for them why woud they wanna give it to sinkie 肥水不流外人田吗 dont believe ft create job for sinkie its used by LKY to hookwink daft sinkie which any world leader will laugh to scorn&quot;
)), row.names = c(NA, -20L), class = c(&quot;tbl_df&quot;, &quot;tbl&quot;, &quot;data.frame&quot;
))
</code></pre>
","nlp"
"123564","Segregation of a finance interview based on topic discussed","2023-09-05 10:23:29","","0","10","<nlp><data-cleaning><text-classification><semantic-segmentation><llm>","<p>I have a video interview of 5 people, which i have transcribed to text (example corpus given below). Considering the fact that we know SPEAKER_00 is the interviewer and rest are guests. I want to know the <strong>start time</strong> details when the interviewer has asked one question and what was the <strong>end time</strong> when all the other guests have finished replying to that question. The interviewer can counter question on the same topic which should be between the start time and end time of our output.</p>
<p>Basically help me to make multiple text file from single text croupous based on the number of questions asked by the interviewer.</p>
<pre><code>[ 00:00:01.290 -- 00:00:01.763 ] SPEAKER_00: No.

[ 00:00:01.290 -- 00:00:20.595 ] SPEAKER_01: The framework that we work with in that if there is a surplus government land, that it is put to the market for disposal. So I suppose there was that little bit of conflict, perceived conflict with what we do and the policy that exists.

[ 00:00:09.576 -- 00:00:10.724 ] SPEAKER_00: market.

[ 00:00:20.595 -- 00:00:35.682 ] SPEAKER_02: So it's probably quite variable, I guess, depending on the politics of the day. You mentioned that there's a framework you're working with in there. Would you say you're more guarded by that framework or the politics of the day when you're making those decisions? It is the framework. Yeah.

[ 00:00:33.775 -- 00:00:47.190 ] SPEAKER_03: It is the framework. So the Government Land Transaction Guidelines dictate that we are selling government land. Or surplus government land, I should say. So that's what we work within.
</code></pre>
","nlp"
"123559","Refining and Preparing Keywords from a column names","2023-09-05 06:56:09","","0","38","<nlp>","<p>For the past few days, I've been working on keyword extraction.</p>
<p>A bit of context: The keywords are meant to be names of tables and columns from a database (not necessarily a direct 1:1 mapping). They should be cleaned up, sometimes broken down into multiple words. The goal is to create a bag of keywords that will be used on a webpage for data management, specifically for the &quot;find similar keywords&quot; function, which is currently performing poorly. Here's my current strategy, and I'm wondering how I can improve it:</p>
<ol>
<li>I retrieve keywords from table and column names using an SQL query. In the query, I've implemented logic that pre-processes these words (splitCamelCase, convert to lowercase, replace characters like _ - / with spaces). This results in around 250,000 keywords that look bad:</li>
</ol>
<p>For instance, below are some keywords:</p>
<blockquote>
<pre><code>assembly name
base record type match code table
azure ad object id for a group
auto apply default entitlement on case update
a sync job state
</code></pre>
</blockquote>
<p>As you can see, they are not poorly processed. However, more keywords could be derived from them, for instance:</p>
<blockquote>
<pre><code>auto apply default entitlement on case update
</code></pre>
</blockquote>
<p>=</p>
<blockquote>
<pre><code>auto apply
default entitlement
case update
update
</code></pre>
</blockquote>
<p>and so on...</p>
<p>Another issue is the fact that I would like to somehow filter out irrelevant keywords. Sort them in some way by importance. Many keywords look like this, for example: <code>&quot;cd m path x&quot;</code> and it would be helpful to process such items as well, without losing important information.</p>
<pre><code>import os
from dotenv import load_dotenv
import pyodbc
import csv
import wordninja
from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS
from sklearn.decomposition import TruncatedSVD

# Function Definitions

def expand_abbreviations(text):
    words = text.split()
    new_words = []
    for word in words:
        if word in abbreviations:
            new_words.extend(abbreviations[word].split())
        else:
            new_words.append(word)
    return ' '.join(new_words)

def split_compound_words(text):
    # Dzielimy tekst na pojedyncze słowa
    words = text.split()
    
    # For each word, if it is protected, we leave it as it is, otherwise we split it
    split_words = [wordninja.split(word) if word not in protected_words else [word] for word in words]
    

    split_words = [word for sublist in split_words for word in sublist]
    
    return ' '.join(split_words)


def process_text(keyword):
    keyword = keyword.lower()
    # Expanding Abbreviations
    keyword = expand_abbreviations(keyword)
    # Splitting Compound Words
    keyword = split_compound_words(keyword)
    
    return keyword


def compute_lsa_keywords(keywords):
    vectorizer = TfidfVectorizer(stop_words=list(ENGLISH_STOP_WORDS), ngram_range=(1, 2))
    matrix = vectorizer.fit_transform(keywords)
    
    # LSA (for test)
    svd = TruncatedSVD(n_components=100)
    lsa_matrix = svd.fit_transform(matrix)
    
    lsa_scores = dict(zip(vectorizer.get_feature_names_out(), lsa_matrix[:, 0]))
    sorted_keywords = sorted(lsa_scores.items(), key=lambda x: x[1], reverse=True)
    return [keyword[0] for keyword in sorted_keywords]


# Loading Environment Variables
load_dotenv('.env')
db_user = os.getenv('USER')
db_password = os.getenv('PASSWORD')
db_host = os.getenv('HOST')
db_name = os.getenv('DB_NAME')

# Creating Connection String
connection_string = f&quot;DRIVER=ODBC Driver 17 for SQL Server;SERVER={db_host};DATABASE={db_name};UID={db_user};PWD={db_password}&quot;

# Connecting to the Database
conn = pyodbc.connect(connection_string)
cursor = conn.cursor()

# Retrieving Keywords from the Database
cursor.execute('SELECT DISTINCT keyword FROM data.dbo.keywords_test')
rows = cursor.fetchall()

# Abbreviation Definitions
abbreviations = {
    &quot;dwh&quot;: &quot;data warehouse&quot;,
    &quot;attr&quot;: &quot;attribute&quot;,
    &quot;cust&quot;: &quot;customer&quot;,
     ....
}

# List of words to be protected from splitting
protected_words = [&quot;ID&quot;, &quot;id&quot;, &quot;emails&quot;, &quot;users&quot;, &quot;date&quot;, &quot;update&quot;, &quot;updates&quot;,&quot;workspace&quot;, &quot;workflow&quot;, &quot;invoice&quot;, &quot;dataset&quot;, &quot;AI&quot;...]

# Preprocessing Keywords
processed_keywords_list = [process_text(row[0]) for row in rows]

# Computing TF-IDF for Keywords
important_keywords = compute_lsa_keywords(processed_keywords_list)

# Sorting
sorted_processed_keywords = sorted(processed_keywords_list, key=lambda x: important_keywords.index(x) if x in important_keywords else float('inf'))

# Preparing CSV File
with open('processed_keywords_test_rake.csv', 'w', newline='', encoding='utf-8') as csvfile:
    csv_writer = csv.writer(csvfile)
    csv_writer.writerow([&quot;processed_keyword&quot;]) 

    for keyword in sorted_processed_keywords:
        csv_writer.writerow([keyword])

# Closing the Database Connection
conn.close()
</code></pre>
","nlp"
"123546","Math Behind Additive Bahdanau Attention","2023-09-04 03:29:42","","0","38","<machine-learning><deep-learning><nlp><rnn><attention-mechanism>","<p>I am new to NLP field and wanted to apply attention model in one of my projects. I have LSTM model to train, and concatenate some external data sources though attention mechanism.</p>
<p>The hidden state size of my LSTM : 128</p>
<p>The vectors I am taking are : 3 , for 120 row values, therefore (120 x 3)</p>
<p>What I am expecting from attention mechanism to take weighted sum of these 120 values at a given LSTM timesteps. The additive Bahdanau takes hidden states h1 and h2, and map them to matrices W1 and W2. However, if these hidden states are different size, would it not cause redundant zero values in these matrices? On the other hand, What I would expect from an encoder-decoder type of network to have different size of encoder and decoder hidden states.</p>
<p>I hope I could explain the problem clear enough. I think attention mechanism would be useful to integrate other sources during training phrase, however I did not see any practical application in the literature yet.</p>
<p>Thank you so much.</p>
","nlp"
"123506","Why my validation loss and accuracy decays over epochs?","2023-09-01 11:52:22","123512","1","67","<machine-learning><deep-learning><neural-network><nlp><rnn>","<p>Im trying to build 2 simple networks with cleaned dataset for tweets sentiment classification(0/1):</p>
<ul>
<li>one with all dense layers(binary bag of words)</li>
<li>another with RNN layer(embedding layer).
But it both cases the validation loss and accuracy are always low. Pasting the code and graphs for reference.</li>
</ul>
<pre><code>    keras.layers.Dense(250, input_shape=(500,),activation='relu'),
    keras.layers.Dropout(0.2),
    keras.layers.Dense(250, activation='relu'),
    keras.layers.Dropout(0.2),
    keras.layers.Dense(1, activation='sigmoid')
])

model2 = keras.models.Sequential([
            keras.layers.Embedding(input_dim=vocabulary,
                     output_dim=EMBEDDING_DIM,
                     input_length=max_length,
                     mask_zero=True),
            keras.layers.SimpleRNN(500, activation='relu'),
            keras.layers.Dropout(0.5),
            keras.layers.Dense(100, activation='relu'),
            keras.layers.Dropout(0.5),
            keras.layers.Dense(100, activation='relu'),
            keras.layers.Dropout(0.5),
            keras.layers.Dense(1, activation='sigmoid')           
])
[![enter image description here][1]][1]```

Tried increasing network complexity by adding more layers and added dropouts. Still nothing increase the loss and accuracy. What am i missing?


  [1]: https://i.sstatic.net/tKlYS.png
</code></pre>
","nlp"
"123475","Build a topic model without data?","2023-08-30 15:52:49","123574","-1","58","<deep-learning><nlp><machine-learning-model><generative-models><topic-model>","<p>I need to come up with a topic model, without any labelled dataset, the model should also be multilingual, thinking of using LLM's as they are accurate and awesome but if Im to build one on my own how to approach this problem without any data in hand. Also, the data we are expecting is mostly like customer care or support questions from different domain. Any help?</p>
","nlp"
"123471","coversational AI chatbot using langchain and chatgpt 3.5?","2023-08-30 08:02:35","","-3","96","<nlp><language-model><chatbot><llm>","<p>can we develop end to end hotel booking coversational AI chatbot using langchain and chatgpt 3.5 ?</p>
","nlp"
"123454","Sum of vector sentence embeddings vs. paragraph embedding","2023-08-29 09:36:26","","0","192","<nlp><word-embeddings><transformer>","<p>I have been experimenting with the <a href=""https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2"" rel=""nofollow noreferrer"">all-MiniLM-L6-v2 model</a> for computing 384-dimensional vector embeddings for text paragraphs. The following code compares the embedding computed for a paragraph with the sum of embeddings of the constituent sentences:</p>
<pre class=""lang-py prettyprint-override""><code>import re
def split_into_sentences(text):
    # Split using regular expression to preserve punctuation at the end of sentences
    sentences = re.split(r'(?&lt;=[.!?])\s+', text)
    return sentences

text = &quot;Tigers, the largest of all big cats, possess an undeniable allure that captivates the human imagination. Their iconic coat of burnt orange, embellished with bold ebony stripes, paints a portrait of both grace and potency, symbolizing the untamed beauty of the natural world. As stealthy hunters, they navigate their diverse habitats with an air of both confidence and mystery, often lurking within the dense undergrowth of jungles or prowling the open grasslands with unparalleled stealth. A tiger's sinuous movements reflect a balance between athleticism and elegance, a testament to their adaptability in various terrains. These felines are not merely ground-bound; their prowess extends to swimming, displaying an unexpected dexterity in the water, and climbing, as they ascend trees with remarkable agility. Intricately patterned, a tiger's stripes are akin to a fingerprint, unique to each individual, and play a vital role in their camouflage while stalking prey. It is the eyes of these creatures, however, that truly leave an indelible mark—an intense amber gaze that radiates an aura of fierce determination. Throughout history and across cultures, tigers have held a mythical status, embodying strength, wisdom, and courage. Yet, the same forces that once revered them now threaten their existence. The encroachment of human activity upon their habitats and the insidious specter of poaching pose grave challenges to their survival. In response, conservation efforts have emerged as a beacon of hope for these magnificent creatures. Collaborative initiatives, alongside advancements in technology and awareness campaigns, strive to protect and preserve their habitats, ensuring a future where tigers continue to roam the landscapes they have graced for millennia. The allure of the tiger, both as a symbol of nature's magnificence and as a reminder of our responsibility as stewards of the planet, remains as potent as ever—a reminder that the fate of these enigmatic creatures is intertwined with the destiny of our world.&quot;
sentences = split_into_sentences(text)
text_embedding = get_embeddings([text])[0]
sentence_embeddings = get_embeddings(sentences)

print([np.round(cosine_similarity(text_embedding, e), 3) for e in sentence_embeddings])
print(cosine_similarity(text_embedding, np.sum(sentence_embeddings, axis=0)))
</code></pre>
<p>I get the output:</p>
<pre><code>[0.677, 0.462, 0.526, 0.701, 0.586, 0.797, 0.544, 0.697, 0.163, 0.336, 0.32, 0.575, 0.697]
0.8497203877599846
</code></pre>
<p>The first line compares each of the sentences with the whole paragraph. The second line compares the sum of sentence embeddings with the embedding for the whole paragraph.</p>
<p>It strikes me as unexpected that the sum of all sentence embeddings (~0.85) does not do much better than the sixth sentence alone (~0.80). Why is that the case?</p>
","nlp"
"123430","Training embeddings on own dataset","2023-08-28 06:04:11","","0","1451","<machine-learning><python><nlp><word-embeddings><llm>","<p>In my project I follow the retrieval augmented generation (RAG) approach.
I want to create embeddings for my own dataset and use it in combination with llama-2.
In the dataset are german annual reports, 548 reports as pdf-files with about 300 sites per report. Next, I want to load the data in a vector store, but first I think I have to create the embeddings.</p>
<p>And now, there are serveral questions and I need some best-practice:</p>
<ol>
<li><p>Do I have to train my own embeddings model or can I use models like word2vec of the gensim package or a pretrained model like BERT and take the hidden state?</p>
</li>
<li><p>Can I use any embeddings model? I think they train on a specific corpus and if my words aren't in the training corpus, I will get bad result or what do you think?</p>
</li>
</ol>
","nlp"
"123377","Implementation of spBLEU","2023-08-24 12:50:52","123379","2","433","<nlp><model-evaluations><metric><language-model><llm>","<p>I was looking for a way to explore evaluation metrics for language translation models and I came across spBLEU. I can’t find any implementations/examples that would help me start. Does anyone have a lead on what I can pursue?</p>
<p>thanks in advance!</p>
","nlp"
"123367","How can BERT/Transformer models accept input batches of different sizes?","2023-08-24 00:05:04","","0","27","<neural-network><nlp><transformer><huggingface>","<p>I understand that all inputs in a batch need to be of the same size. However, it seems BERT/Transformers models can accept batches with different sizes as input.</p>
<p>How is that possible? I thought we needed to pad all examples in a batch to <code>model.max_input_size</code>, however, it seems HuggingFace does <code>Dynamic Padding</code> that allows sending batches of different lengths (till the time they are smaller than <code>max_input_size</code>)</p>
<p><a href=""https://i.sstatic.net/zC1WS.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/zC1WS.png"" alt=""enter image description here"" /></a></p>
<p>Link: <a href=""https://mccormickml.com/2020/07/29/smart-batching-tutorial/"" rel=""nofollow noreferrer"">https://mccormickml.com/2020/07/29/smart-batching-tutorial/</a><br />
Link2: <a href=""https://huggingface.co/learn/nlp-course/en/chapter3/2?fw=pt#dynamic-padding"" rel=""nofollow noreferrer"">https://huggingface.co/learn/nlp-course/en/chapter3/2?fw=pt#dynamic-padding</a></p>
","nlp"
"123325","how do we adapt LLM token embeddings with custom vocab","2023-08-20 16:14:35","","2","2832","<nlp><transformer><tokenization>","<p>Hi im just getting started with understanding transformer based models and I am not able to find how the token embeddings are arrived at?. there are multiple tokenization approaches and multiple vocabularies/documents llms are trained on. so my question is</p>
<ol>
<li>whether each llm also trains its own token embeddings?</li>
<li>how do those pre trained embeddings work for transfer learning or fine tuning, on
custom data sets where some OOV words may be present or we have some
special unique tokens we want to keep whole and not have tokenizer do subword tokens?</li>
</ol>
","nlp"
"123300","How can I apply NLP/NLU methods for anomaly detection in structured log data?","2023-08-17 20:55:28","","1","168","<nlp><anomaly-detection>","<p>I have a dataset of logs with a specific structured format, and I'm looking for the best approach to detect anomalies within this data. I've already experimented with autoencoders and clustering techniques, but I'm curious if there are basic NLP methods or more advanced NLU techniques that could be more effective for this type of data.</p>
<p>Here's a sample of the log format:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>MODULE</th>
<th>DATE</th>
<th>TIME_STAMP</th>
<th>ACTION_TYPE</th>
<th>META_INFO</th>
<th>EXTRA_INFO</th>
</tr>
</thead>
<tbody>
<tr>
<td>AppCreator</td>
<td>2019-06-15</td>
<td>2019-06-15 13:15:10</td>
<td>UIResponse</td>
<td>{&quot;segment&quot;: &quot;App Branch&quot;, &quot;tag&quot;: &quot;Order&quot;, &quot;activity&quot;: &quot;MenuOpened&quot;, &quot;version&quot;: &quot;0.2&quot;, &quot;view&quot;: &quot;Primary Screen&quot;}</td>
<td>N/A</td>
</tr>
<tr>
<td>AppCreator</td>
<td>2019-06-15</td>
<td>2019-06-15 13:30:45</td>
<td>SysActivity</td>
<td>{&quot;context&quot;: &quot;Layout&quot;, &quot;tag&quot;: &quot;Tab Shifted&quot;, &quot;source&quot;: &quot;&quot;, &quot;element&quot;: &quot;AppTabs&quot;, &quot;activity&quot;: &quot;Modify&quot;, &quot;version&quot;: &quot;0.2&quot;}</td>
<td>[&quot;2&quot;]</td>
</tr>
</tbody>
</table>
</div>
<p>The DOMAIN_DATA field contains structured data in JSON format, and the CUSTOM_DATA field can either be a list or more complex structured data.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>MODULE</th>
<th>DATE</th>
<th>TIME_STAMP</th>
<th>ACTION_TYPE</th>
<th>META_INFO</th>
<th>EXTRA_INFO</th>
</tr>
</thead>
<tbody>
<tr>
<td>AppCreator</td>
<td>2019-06-15</td>
<td>2019-06-15 13:15:10</td>
<td>UIResponse</td>
<td>{&quot;segment&quot;: &quot;App Branch&quot;, &quot;tag&quot;: &quot;Order&quot;, &quot;activity&quot;: &quot;MenuOpened&quot;, &quot;version&quot;: &quot;0.2&quot;, &quot;view&quot;: &quot;Primary Screen&quot;}</td>
<td>N/A</td>
</tr>
<tr>
<td>AppCreator</td>
<td>2019-06-15</td>
<td>2019-06-15 13:30:45</td>
<td>SysActivity</td>
<td>{&quot;context&quot;: &quot;Layout&quot;, &quot;tag&quot;: &quot;Tab Shifted&quot;, &quot;source&quot;: &quot;&quot;, &quot;element&quot;: &quot;AppTabs&quot;, &quot;activity&quot;: &quot;Modify&quot;, &quot;version&quot;: &quot;0.2&quot;}</td>
<td>[&quot;2&quot;]</td>
</tr>
<tr>
<td>AppCreator</td>
<td>2019-06-15</td>
<td>2019-06-15 13:45:30</td>
<td>UIAction</td>
<td>{&quot;segment&quot;: &quot;App Design&quot;, &quot;tag&quot;: &quot;Submit&quot;, &quot;activity&quot;: &quot;ButtonPressed&quot;, &quot;version&quot;: &quot;0.3&quot;, &quot;view&quot;: &quot;Secondary Screen&quot;}</td>
<td>[&quot;FormA&quot;, &quot;ActionB&quot;, &quot;RedirectC&quot;]</td>
</tr>
</tbody>
</table>
</div>
<p>I've also come across methods that involve <a href=""https://arxiv.org/pdf/2302.07435.pdf"" rel=""nofollow noreferrer"">log parsing using LLMs</a> followed by anomaly detection, but I'm unsure if that's the right direction for this dataset.</p>
<p>Could anyone provide insights or suggestions on how to effectively leverage NLP/NLU for anomaly detection in this context? Any guidance or references?</p>
<p>Here's an extended table with 10 samples to help you get an understanding of what my input looks like for doing anomaly detection on the logs. This table showcases various interactions, system activities, and user actions within the application, providing a holistic view of the data:</p>
<p>To give readers a more generalistic understanding of the data, here's a sample of the log entries:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>MODULE</th>
<th>DATE</th>
<th>TIME_STAMP</th>
<th>ACTION_TYPE</th>
<th>META_INFO</th>
<th>EXTRA_INFO</th>
</tr>
</thead>
<tbody>
<tr>
<td>AppCreator</td>
<td>2019-06-15</td>
<td>2019-06-15 13:15:10</td>
<td>UIResponse</td>
<td>{&quot;segment&quot;: &quot;App Branch&quot;, &quot;tag&quot;: &quot;Order&quot;, &quot;activity&quot;: &quot;MenuOpened&quot;, &quot;version&quot;: &quot;0.2&quot;, &quot;view&quot;: &quot;Primary Screen&quot;}</td>
<td>N/A</td>
</tr>
<tr>
<td>AppCreator</td>
<td>2019-06-15</td>
<td>2019-06-15 13:30:45</td>
<td>SysActivity</td>
<td>{&quot;context&quot;: &quot;Layout&quot;, &quot;tag&quot;: &quot;Tab Shifted&quot;, &quot;source&quot;: &quot;&quot;, &quot;element&quot;: &quot;AppTabs&quot;, &quot;activity&quot;: &quot;Modify&quot;, &quot;version&quot;: &quot;0.2&quot;}</td>
<td>[&quot;2&quot;]</td>
</tr>
<tr>
<td>AppCreator</td>
<td>2019-06-15</td>
<td>2019-06-15 13:45:30</td>
<td>UIAction</td>
<td>{&quot;segment&quot;: &quot;App Design&quot;, &quot;tag&quot;: &quot;Submit&quot;, &quot;activity&quot;: &quot;ButtonPressed&quot;, &quot;version&quot;: &quot;0.3&quot;, &quot;view&quot;: &quot;Secondary Screen&quot;}</td>
<td>[&quot;FormA&quot;, &quot;ActionB&quot;, &quot;RedirectC&quot;]</td>
</tr>
<tr>
<td>AppCreator</td>
<td>2019-06-15</td>
<td>2019-06-15 14:00:00</td>
<td>SysActivity</td>
<td>{&quot;context&quot;: &quot;Data&quot;, &quot;tag&quot;: &quot;DataSaved&quot;, &quot;source&quot;: &quot;FormB&quot;, &quot;element&quot;: &quot;Database&quot;, &quot;activity&quot;: &quot;Save&quot;, &quot;version&quot;: &quot;0.4&quot;}</td>
<td>[&quot;DataX&quot;, &quot;DataY&quot;]</td>
</tr>
<tr>
<td>AppCreator</td>
<td>2019-06-15</td>
<td>2019-06-15 14:15:15</td>
<td>UIResponse</td>
<td>{&quot;segment&quot;: &quot;App Settings&quot;, &quot;tag&quot;: &quot;Config&quot;, &quot;activity&quot;: &quot;MenuAccessed&quot;, &quot;version&quot;: &quot;0.2&quot;, &quot;view&quot;: &quot;Config Screen&quot;}</td>
<td>N/A</td>
</tr>
<tr>
<td>AppCreator</td>
<td>2019-06-15</td>
<td>2019-06-15 14:30:30</td>
<td>UIAction</td>
<td>{&quot;segment&quot;: &quot;App Design&quot;, &quot;tag&quot;: &quot;Drag&quot;, &quot;activity&quot;: &quot;ElementDragged&quot;, &quot;version&quot;: &quot;0.3&quot;, &quot;view&quot;: &quot;Design Screen&quot;}</td>
<td>[&quot;ElementZ&quot;, &quot;PositionP&quot;]</td>
</tr>
<tr>
<td>AppCreator</td>
<td>2019-06-15</td>
<td>2019-06-15 14:45:45</td>
<td>SysActivity</td>
<td>{&quot;context&quot;: &quot;Layout&quot;, &quot;tag&quot;: &quot;LayoutChanged&quot;, &quot;source&quot;: &quot;User&quot;, &quot;element&quot;: &quot;LayoutGrid&quot;, &quot;activity&quot;: &quot;Update&quot;, &quot;version&quot;: &quot;0.5&quot;}</td>
<td>[&quot;Layout1&quot;, &quot;Layout2&quot;]</td>
</tr>
<tr>
<td>AppCreator</td>
<td>2019-06-15</td>
<td>2019-06-15 15:00:00</td>
<td>UIResponse</td>
<td>{&quot;segment&quot;: &quot;App Preview&quot;, &quot;tag&quot;: &quot;Preview&quot;, &quot;activity&quot;: &quot;PreviewOpened&quot;, &quot;version&quot;: &quot;0.2&quot;, &quot;view&quot;: &quot;Preview Screen&quot;}</td>
<td>N/A</td>
</tr>
<tr>
<td>AppCreator</td>
<td>2019-06-15</td>
<td>2019-06-15 15:15:15</td>
<td>UIAction</td>
<td>{&quot;segment&quot;: &quot;App Feedback&quot;, &quot;tag&quot;: &quot;Feedback&quot;, &quot;activity&quot;: &quot;FeedbackGiven&quot;, &quot;version&quot;: &quot;0.4&quot;, &quot;view&quot;: &quot;Feedback Screen&quot;}</td>
<td>[&quot;Positive&quot;, &quot;FeatureSuggestion&quot;]</td>
</tr>
<tr>
<td>AppCreator</td>
<td>2019-06-15</td>
<td>2019-06-15 15:30:30</td>
<td>SysActivity</td>
<td>{&quot;context&quot;: &quot;Integration&quot;, &quot;tag&quot;: &quot;API&quot;, &quot;source&quot;: &quot;ExternalService&quot;, &quot;element&quot;: &quot;APIEndpoint&quot;, &quot;activity&quot;: &quot;Call&quot;, &quot;version&quot;: &quot;0.6&quot;}</td>
<td>[&quot;API1&quot;, &quot;Response200&quot;]</td>
</tr>
</tbody>
</table>
</div>
<p>What have I already tried?</p>
<p>I have already explored various unsupervised learning methods for anomaly detection on this dataset. Specifically, I've tried:</p>
<ul>
<li>KMeans Clustering</li>
<li>Autoencoders</li>
<li>DBSCAN</li>
<li>Isolation Forests</li>
</ul>
<p>In addition to these, I've also delved into deep learning methods, particularly a method called DeepLog. For those unfamiliar, DeepLog is a deep neural network model designed for anomaly detection in log files. I've read and implemented insights from the <a href=""https://arxiv.org/pdf/2202.04301.pdf"" rel=""nofollow noreferrer"">DeepLog paper</a> (this is a survey paper the original paper of deeplog is here, wanted to try deep learning methods and thought of giving this a shot. However this still requires encoding the ACTION_TYPE and more importantly for the anomalies to be already labelled which isn't the case in my dataset.) and tried to adapt it to my dataset.</p>
<p>However, I'm still looking for more advanced or nuanced methods that might be better suited for the specific nature and format of my logs.</p>
<hr />
<p>To continue or help give some direction into how I am thinking about solving this I thought it best to share my way of coming up with a solution -</p>
<p>Here's a step by step description of what I am doing in the implementation given below :</p>
<ol>
<li>Initialize a list called ents to store named entities.</li>
<li>Initialize a list called pos_tags to store part-of-speech tags.</li>
<li>Initialize a list called deps to store syntactic dependencies.</li>
<li>Initialize a float variable called avg_sentiment to store the average sentiment score.</li>
<li>Initialize a list called key_phrases to store key phrases.</li>
<li>Split the input text into individual words using the word_tokenize() function.</li>
<li>Remove stop words and punctuation from the tokenized text using lists comprehension.</li>
<li>Lemmatize the remaining words using the lemmatizer.lemmatize() function.</li>
<li>Perform named entity recognition on the lemmatized text using the nlp() function and store the results in ents.</li>
<li>Perform part-of-speech tagging on the lemmatized text using the nlp() function and store the results in pos_tags.</li>
<li>Perform syntactic dependency parsing on the lemmatized text using the nlp() function and store the results in deps.</li>
<li>Calculate the average sentiment score of the text using the SentimentIntensityAnalyzer() class and store it in avg_sentiment.</li>
<li>Extract key phrases from the text using the keywords() function and store them in key_phrases.</li>
</ol>
<pre><code>import json
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from spacy import displacy
from spacy.lang.en import English
from spacy.models import load_pretrained
from vader import SentimentIntensityAnalyzer
from gensim.summarization.keypoints import keywords

nlp = English()
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def process_text(text):
    data = json.loads(text)
    components = data[&quot;COMPONENT&quot;]
    event_date = data[&quot;EVENT_DATE&quot;]
    event_timestamp = data[&quot;EVENT_TIMESTAMP&quot;]
    event_type = data[&quot;EVENT_TYPE&quot;]
    domain_data = data[&quot;DOMAIN_DATA&quot;]
    custom_data = data[&quot;CUSTOM_DATA&quot;]

    tokens = word_tokenize(components)
    tokens = [t for t in tokens if t.isalpha() and t not in stop_words]
    tokens = [lemmatizer.lemmatize(t) for t in tokens]
    ents = nlp(tokens)
    entities = [entity.text for entity in ents if entity.label_ == 'PEOPLE']
    pos_tags = nlp(tokens, disable=['ner'])
    pos_tags = [t.pos_ for t in pos_tags]
    parse_tree = nlp(tokens, disable=['ner', 'pos'])
    dependencies = [t.dep_ for t in parse_tree]
    sentiments = []
    for sentence in tokens:
        sentiments.append(SentimentIntensityAnalyzer().polarity_scores(sentence))
    avg_sentiment = np.mean([s[0] for s in sentiments])
    doc_frequency = keywords(text, density=0.5)
    key_phrases = [t for t, freq in doc_frequency.items() if freq &gt; 0.5]

return {
    'entities': entities,
    'pos_tags': pos_tags,
    'dependencies': dependencies,
    'avg_sentiment': avg_sentiment,
    'key_phrases': key_phrases
}

text = &quot;&quot;&quot;COMPONENT = ServiceStudio,    EVENT_DATE = 2019-07-02,    EVENT_TIMESTAMP = 2019-07-02 14:20:11,  EVENT_TYPE = UIEvent, DOMAIN_DATA = &quot;{
  &quot;&quot;area&quot;&quot;: &quot;&quot;ESpace Tree&quot;&quot;,
  &quot;&quot;label&quot;&quot;: &quot;&quot;Product&quot;&quot;,
  &quot;&quot;type&quot;&quot;: &quot;&quot;ContextMenuOpened&quot;&quot;,
  &quot;&quot;ver&quot;&quot;: &quot;&quot;0.1&quot;&quot;,
  &quot;&quot;window&quot;&quot;: &quot;&quot;Main Window&quot;&quot;
}&quot;                                                              
, CUSTOM_DATA    =           &quot;[
  {
    &quot;&quot;AppName&quot;&quot;: &quot;&quot;teste&quot;&quot;,
    &quot;&quot;AppType&quot;&quot;: 1,
    &quot;&quot;ModuleType&quot;&quot;: 1,
    &quot;&quot;Name&quot;&quot;: &quot;&quot;teste&quot;&quot;
  }
]&quot;
&quot;&quot;&quot;
result = process_text(text)
print(result)
</code></pre>
<p><strong>Input:</strong>
I want to use the above nlp methods to understand the logs written in this format (extracted from a table with a similar format as the one I already have shared above) -</p>
<blockquote>
<p>MODULE = AppCreator,  EVENT_DATE = 2019-07-02,    EVENT_TIMESTAMP =
2019-07-02 14:20:11,  EVENT_TYPE = UIEvent, DOMAIN_DATA = &quot;{<br />
&quot;&quot;area&quot;&quot;: &quot;&quot;ESpace Tree&quot;&quot;,   &quot;&quot;label&quot;&quot;: &quot;&quot;Product&quot;&quot;,   &quot;&quot;type&quot;&quot;:
&quot;&quot;ContextMenuOpened&quot;&quot;,   &quot;&quot;ver&quot;&quot;: &quot;&quot;0.1&quot;&quot;,   &quot;&quot;window&quot;&quot;: &quot;&quot;Main
Window&quot;&quot; }&quot;                                                                    , CUSTOM_DATA   =           &quot;[   {
&quot;&quot;AppName&quot;&quot;: &quot;&quot;teste&quot;&quot;,
&quot;&quot;AppType&quot;&quot;: 1,
&quot;&quot;ModuleType&quot;&quot;: 1,
&quot;&quot;Name&quot;&quot;: &quot;&quot;teste&quot;&quot;   } ]&quot;</p>
</blockquote>
<p><strong>Output:</strong></p>
<blockquote>
<p>{   'entities': ['AppCreator'],   'pos_tags': ['NNP'],<br />
'dependencies': [
{'rel': 'det', 'head': 'UIEvent', 'deps': ['Component']},
{'rel': 'nsubjpass', 'head': 'UIEvent', 'deps': ['Area']},
{'rel': 'obj', 'head': 'ContextMenuOpened', 'deps': ['Label']}   ],   'avg_sentiment': 0.0,   'key_phrases': ['ServiceStudio',
'UIEvent', 'Espace Tree', 'Product', 'ContextMenuOpened', 'Main
Window'] }</p>
</blockquote>
<p>Now with this information, I am looking to figure out possible ways in which an anomaly can be detected in the log.</p>
","nlp"
"123194","Hack to generate training data for a fantasy language?","2023-08-11 17:48:11","","0","57","<machine-learning><nlp>","<p>It is an impractical task of collecting millions of example sentences with their translations in a &quot;fantasy language&quot; (<a href=""https://en.wikipedia.org/wiki/Constructed_language"" rel=""nofollow noreferrer"">conlang</a>). At most, you can probably have a thousand or 2k sentences before you get tired of it.</p>
<p>However, it might be possible in my case to generate sentences given English input, if I greatly restrict the number of types of words used in the English sentences. I could then use ChatGPT or other LLMs to generate potentially 10's of thousands of example sentences, using a restricted vocabulary, and then programmatically (hardcoded) transform those into the fantasy language. That would give me say 10-20k sentences, using a limited vocab but many sentence-level grammar features. Any more sentences than that would get time consuming.</p>
<p>Is it possible to use those sentences as training data in building a translation system? Then it could take <em>new words</em> (in theory) and create sentences from that, following its training data examples?</p>
<p>Is that a hack that could work? Or what would be your recommendation for building a translation system for a language which doesn't have much in terms of resources (an extremely &quot;low resource&quot; language)?</p>
","nlp"
"123149","Why do GPT models use a transpose of the embedding matrix to convert outputs to logits?","2023-08-09 06:45:56","123152","0","624","<machine-learning><nlp><gpt><chatgpt>","<p>According to <a href=""https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf#subsection.3.1"" rel=""nofollow noreferrer"">section 3.1</a> of the original GPT paper, GPT right-multiplies the final output vectors (after applying a Transformer decoder model) by the transpose of the embedding matrix, before applying a softmax. See <a href=""https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens?commentId=Xh7HKWimSPmJqZcnc"" rel=""nofollow noreferrer"">this comment</a> for further verification.</p>
<p>Why? I feel like it would be simpler to learn a separate matrix for this task. Doing so wouldn't be particularly computationally expensive, and it wouldn't significantly increase the parameter count. Additionally, I don't see any mathematical link between the matrix and its transpose in this context; I don't think the dual space is going to matter here... Overall, I see very little connection between embedding tokens and obtaining logits. So what's the point of using the transpose? Does it just tend to perform well empirically / in practice?</p>
","nlp"
"123079","Unsupervised fine tuning of Code LLMs","2023-08-04 06:03:16","","0","930","<nlp><transformer><finetuning>","<p>How to prepare code data to fine tune a code LLM in an unsupervised way or is it even possible?</p>
<p>For example:
Task: Code summarization with custom code base (with no summaries)
Let's assume that this code base is unique, and a pre-trained model is giving unsatisfactory results. Now to fine tune there are three options,</p>
<ol>
<li>Manually prepare summaries for a portion of the code and fine tune</li>
<li>Find a similar code base which has the labels (docstring) and fine tune</li>
<li>Mask some portions of the code randomly and give as input and output will be the masked portions</li>
</ol>
<p>Options 1 and 2 don't seem feasible for a production environment.</p>
<p>The reasoning behind option 3 is that with no availability labels, the model will learn the patterns in the code base and provide a better summarization with its pre-trained knowledge.</p>
<p>I tried the option 3 with [CodeT5+ fine tuning] (<a href=""https://github.com/salesforce/CodeT5/blob/main/CodeT5%2B/tune_codet5p_seq2seq.py"" rel=""nofollow noreferrer"">https://github.com/salesforce/CodeT5/blob/main/CodeT5%2B/tune_codet5p_seq2seq.py</a>). The format of input and output was as follows
Input:</p>
<pre><code>    def __init__(self, text, font):
        self._text = text
        self._font = font

    def get_text(self):
        |&lt;mask&gt;|

    def set_text(self, value):
        self._text = value```


Output:
```return self._text```
</code></pre>
","nlp"
"123053","Why does everyone use BERT in research instead of LLAMA or GPT or PaLM, etc?","2023-08-03 01:11:29","123060","9","12125","<nlp><bert><language-model><gpt><research>","<p>It could be that I'm misunderstanding the problems space and the iterations of LLAMA, GPT, and PaLM are all based on BERT like many language models are, but every time I see a new paper in improving language models it takes BERT as a based an adds some kind of fine-tuning or filtering or something. I don't understand why BERT became the default in research circles when all anyone hears about publicly is GPT-2,3,4 or more recently LLAMA-2. I have a feeling it has something to do with BERT being open-source, but that can't be the whole story. This question might not be specific enough, please let me know. Thanks.</p>
","nlp"
"123036","Recommended way to embed a text thousands of tokens long?","2023-08-01 14:01:53","","1","1598","<nlp><preprocessing><word-embeddings><embeddings>","<p>I've split the text up sections each 512 tokens long and created embeddings for each of them.</p>
<p>I want to combine them into 1 embedding for the full text. How do I do that? Is this even recommended?</p>
<p>AdaV2 has <a href=""https://platform.openai.com/docs/guides/embeddings/second-generation-models"" rel=""nofollow noreferrer"">max token limit</a> of 8k, I thought about concatenating each embedding like <code>str(embed1) + str(embed2) + ...</code> and stopping when it reaches 8k tokens. But each embed is 15k tokens if converted to a string.</p>
","nlp"
"123020","Classification errors on 'bert-base-uncased' text classifier","2023-07-31 11:07:49","","0","57","<nlp><multiclass-classification><bert><text-classification>","<p>Disclaimer : This is a long question, please be patient. Thanks in advance</p>
<p>I am using <em>bert-base-uncased</em> for text-classification. I have 11 classes, and the classification is happening alright for most of the classes. But of these 11 classes there are three classes, say <strong>A, B and C</strong>. Where there are high misclassification errors. I wish to reduce the errors between these classes.</p>
<p>Current State of my model :</p>
<ol>
<li>Model used Hugging Face <em>bert-base-uncased</em>.</li>
<li>Loss function : Weighted Cross Entropy where the weights represent the inverse of the fraction of each class in the data.</li>
<li>The text data related to classes A, B and C are not unbalanced and are roughly comparable to the most populous class</li>
</ol>
<p>My Questions :</p>
<ol>
<li>Can anyone say why is this occuring?</li>
<li>I am thinking of using some-other loss function specifically for these three classes, say soft-F1 from torchmetrics. The idea is that nn.Cross_Entropy() will be used for all classes and apart from that I will use soft-F1 when the true_label belongs to these three classes.</li>
</ol>
<p>Thus the final loss function will be <code>loss = frac * $nn.Cross_entropy() + (1-frac) * torchmetrics.soft_f1(if true_label in [A, B, C])</code>, where <code>frac</code> is an hyper-parameter.</p>
<p>Will this approach work, or should I use something else ?</p>
","nlp"
"122916","Check if given information is genuine","2023-07-24 13:42:52","","0","20","<machine-learning><nlp><text-classification>","<p>I want to build an algorithm which can verify a piece of content against ground truth and output whether the piece of content is providing genuine information or is fake.</p>
<p>I'm working on a stock trading platform and we use <em>influencer</em> marketing. Need to monitor whether the <em>influencers</em> are providing the correct information in their posts.</p>
<p>What would be the approach of building such a model?</p>
<p>What algorithms would be applicable?</p>
","nlp"
"122865","What does it mean order of input sequence does not matter for transformer self-attention head?","2023-07-21 16:02:20","","3","793","<neural-network><nlp><predictive-modeling><transformer><attention-mechanism>","<p>The need for positional encoding in transformer models is justified by permutation invariance of self-attention heads, because, without it, transformer wouldn't have any mechanism to take into account the order of the words.</p>
<p>Suppose we trained a simple trigram transformer model without positional encoding to predict C as the next token, if the input is AB. Because the self-attention head output has T (time) dimension, we actually predicted BC, in other words, <code>head.forward('AB')='BC'</code>. We use only the last token C as the prediction of our model.</p>
<p>Now, because of the head permutation invariance, <code>head.forward('BA')='CB'</code>. Thus, the next token prediction becomes B (last token in output of 'forward`). So why is it said that transformer models without positional encoding are position agnostic. In the above example, we permuted input tokens and obtained a different prediction (B instead of C)</p>
<p>UPDATE: made a colab notebook illustrating the concept:
<a href=""https://colab.research.google.com/drive/1ItIQTCg3sVGRUIGbrh1pxTNlfLU5C7kq?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1ItIQTCg3sVGRUIGbrh1pxTNlfLU5C7kq?usp=sharing</a></p>
","nlp"
"122780","ELMO embeddings","2023-07-17 07:43:03","122781","1","47","<neural-network><nlp><machine-learning-model><word-embeddings>","<p>Could somebody tell me how does elmo work?
Is it good for phrase embedding too?
I m looking for phrase embeddings.</p>
<p>Thank You in advance.</p>
","nlp"
"122763","Text classification with very short strings","2023-07-16 04:56:48","","0","100","<classification><nlp><feature-extraction><tfidf>","<p>I have a dataset of short job titles (e.g., 'marketing manager', 'system administrator', etc.) and their respective Census occupation code (e.g., 1006 Computer systems analysts). I am interested in building a model that can classify each job title into an occupation. Unfortunately, the job titles consist of only 1 or 2 words, so the feature extraction methods that I've worked with before (mainly tf-idf) do not make much sense for this task. I'd appreciate any ideas for how I might approach this!</p>
","nlp"
"122751","Issue with Convolutional Layer in Python: Getting All Zeros in Output and Terminating at a Certain Iteration","2023-07-15 06:21:11","","0","77","<nlp><cnn><sentiment-analysis>","<p>I'm currently working on implementing a convolutional layer in Python for a natural language processing model. However, I've encountered an issue with the convolutional layer that I can't seem to resolve.</p>
<p><strong>The problem is twofold:</strong></p>
<ol>
<li><p>Getting all zeros in the output: When I run the forward pass of my convolutional layer, I'm consistently getting all zeros in the output at every position. I've verified this by printing the values during execution. This is unexpected because I initialized the output array correctly.</p>
</li>
<li><p>Terminating at a certain iteration: Additionally, the forward pass terminates abruptly at a certain iteration, specifically at the 30th iteration. I'm unable to figure out why the loop is exiting prematurely.</p>
</li>
</ol>
<p>Here's a simplified version of my Conv1DLayer class:</p>
<pre><code>import numpy as np

class Conv1DLayer:

    def __init__(self, num_filters, filter_size):
        self.num_filters = num_filters
        self.filter_size = filter_size
        self.conv_filter = np.random.randn(filter_size, 1)

    def loss(self, pred, target):
        # compute loss function
        return np.mean((pred - target) ** 2)

    def forward(self, inputs):
        self.inputs = inputs
        num_inputs = inputs.shape[1]
        output_length = num_inputs - self.filter_size + 1
        self.output = np.zeros((self.num_filters, output_length))
        # Convolution
        # input dim is basically the size of the vocabulary
        print(&quot;input dim: &quot;, inputs.shape)
        print(&quot;num inputs: &quot;, num_inputs)
        print(&quot;filter dim: &quot;, self.conv_filter.shape)
        print(&quot;filter size: &quot;, self.filter_size)
        print(&quot;output dim: &quot;, self.output.shape)
        print(&quot;output length: &quot;, output_length)
        for i in range(output_length):
            if i+self.filter_size &gt; num_inputs:
                break
            receptive_field = inputs[i:i+self.filter_size, 1].toarray()
            print(&quot;receptive field dim: &quot;, receptive_field.shape)
            self.output[:, i] = np.dot(receptive_field.T, self.conv_filter)
            print(&quot;output at &quot; + str(i) + str(self.output[:, i]))
            self.output[:, i] = np.maximum(0, self.output[:, i])

        return self.output
    
    def backward(self, grad_outputs, learning_rate):
        grad_input = np.zeros(grad_outputs.shape)
        grad_filter = np.zeros(self.conv_filter.shape)

        for i in range(grad_outputs.shape[0]):
            for j in range(self.num_filters):
               receptive_field = self.inputs[i:i+self.filter_size]
               grad_input[i:i+self.filter_size] += self.conv_filter[:, j] * grad_outputs[i, j]
               grad_filter[:, j] += receptive_field * grad_outputs[i, j]

            # Update the weights
            self.conv_filter -= learning_rate * grad_filter

        return grad_input

</code></pre>
<p>I've tried various modifications to the code, including checking the shape of input arrays, adjusting the receptive field, and updating the loop condition, but I haven't been able to resolve the issue.</p>
<p><strong>Here's my output</strong></p>
<pre><code>$ python model.py
input dim:  (32, 2010)
num inputs:  2010
filter dim:  (3, 1)
filter size:  3
output dim:  (10, 2008)
output length:  2008
receptive field dim:  (3, 1)
output at 0[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
receptive field dim:  (3, 1)
output at 1[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
receptive field dim:  (3, 1)
output at 2[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
receptive field dim:  (3, 1)
output at 3[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
receptive field dim:  (3, 1)
output at 4[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
receptive field dim:  (3, 1)
output at 5[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
receptive field dim:  (3, 1)
output at 6[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
receptive field dim:  (3, 1)
output at 7[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
receptive field dim:  (3, 1)
output at 8[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
receptive field dim:  (3, 1)
output at 9[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
receptive field dim:  (3, 1)
output at 10[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
receptive field dim:  (3, 1)
output at 11[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
receptive field dim:  (3, 1)
output at 12[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
receptive field dim:  (3, 1)
output at 13[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
receptive field dim:  (3, 1)
output at 14[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
receptive field dim:  (3, 1)
output at 15[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
receptive field dim:  (3, 1)
output at 16[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
receptive field dim:  (3, 1)
output at 17[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
receptive field dim:  (3, 1)
output at 18[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
receptive field dim:  (3, 1)
output at 19[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
receptive field dim:  (3, 1)
output at 20[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
receptive field dim:  (3, 1)
output at 21[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
receptive field dim:  (3, 1)
output at 22[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
receptive field dim:  (3, 1)
output at 23[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
receptive field dim:  (3, 1)
output at 24[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
receptive field dim:  (3, 1)
output at 25[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
receptive field dim:  (3, 1)
output at 26[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
receptive field dim:  (3, 1)
output at 27[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
receptive field dim:  (3, 1)
output at 28[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
receptive field dim:  (3, 1)
output at 29[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
receptive field dim:  (2, 1)
Traceback (most recent call last):
  File &quot;C:\Users\maste_0c98yk4\OneDrive\Desktop\Projects\Natural Language Processing Model - Sentiment Analysis\model.py&quot;, line 89, in &lt;module&gt;
    model.train(X, labels, num_epochs=10, batch_size=32)
  File &quot;C:\Users\maste_0c98yk4\OneDrive\Desktop\Projects\Natural Language Processing Model - Sentiment Analysis\model.py&quot;, line 44, in train   
    conv_output = self.conv_layer.forward(inputs)
  File &quot;C:\Users\maste_0c98yk4\OneDrive\Desktop\Projects\Natural Language Processing Model - Sentiment Analysis\convolution.py&quot;, line 32, in forward
    self.output[:, i] = np.dot(receptive_field.T, self.conv_filter)
ValueError: shapes (1,2) and (3,1) not aligned: 2 (dim 1) != 3 (dim 0)
(tf) 
</code></pre>
<p><strong>Expected behavior:</strong></p>
<ol>
<li>The forward pass should compute the convolution correctly, producing non-zero values in the output.</li>
<li>The loop should iterate over all valid positions without terminating prematurely.</li>
</ol>
<p><strong>My assessment</strong>
I think the issue is very likely just the way I am using indices but neither chatgpt nor bard have been able to fix it so it might be something deeper.</p>
<p>Any kind of help will be much appreciated. Thanks</p>
","nlp"
"122661","Legal considerations of analyzing and training language models with online public documents","2023-07-10 19:08:09","122663","0","20","<nlp>","<p>When training and evaluating my language models, I always wish to have more data, especially those from different sectors.</p>
<p>Then, I noticed <a href=""https://www.state.gov/copyright-information/"" rel=""nofollow noreferrer"">the following statement about copyright</a> on the US government website,
&quot;Unless a copyright is indicated, information on State Department websites is in the public domain and may be copied and distributed without permission.&quot;</p>
<p>Is it legal to train language models with documents subject to this statement and, more interestingly, write Medium posts or publish papers to present the results?</p>
","nlp"
"122547","What can be the possible benefits of analysing rare words?","2023-07-04 15:52:37","","0","37","<nlp>","<p>A few days ago, I was assigned the task to analyse rare words for apparently anything but it needs to be novel. But, after reading articles and research papers and my previous NLP and Computer Science experience, I don't understand what benefits can be extracted in analysing rare except for NER and tokenization problem. And a few statistical problem such as cultural shift, etc.</p>
<p>Now, I am in a block and standstill state. If anyone could help with their more in-depth experience and insights it'll greatly be appreciated.</p>
","nlp"
"122546","Best practice for fine tuning LLM","2023-07-04 10:03:36","","1","117","<nlp><dataset><text>","<p>I have a dataset that I have collected for specific topic.</p>
<p>The dataset is in the following format:</p>
<ol>
<li>Raw text (similar to shakespeare dataset) where it has no label or input, just text</li>
<li>Question and answer dataset similar to alpaca instructs</li>
</ol>
<p>My way is to fine tune a LLM on raw text first then on Q&amp;A dataset</p>
<p>Does this looks like good approach? Or can I just fine tune it on all the datasets together?</p>
","nlp"
"122539","Can bert uncased predict text classification on foreign data?","2023-07-03 21:50:47","122541","0","483","<nlp><dataset><bert>","<p>I am trying to do the fake news/real news classification and used a pre-trained bert uncased model as transfer learning and it gave a solid 81% accuracy. But the problem is while doing sanity checks, I found my dataset has some Korean/Chinese text articles and these are some real news and it gave the trustworthy score(basically probability) as 60-70%. If Bert-uncased is only for the English language, I'm just thinking about how it processes those languages. Does anyone have any insights?</p>
","nlp"
"122487","What is the NLP problem I am solving called and how should i go about solving it?","2023-06-30 10:29:39","","1","59","<nlp><web-scraping>","<p>I am working on a POC where I am required to write a NLP code after web scraping. The prompt to my code is</p>
<blockquote>
<p>How good is the online Data Science degree offered by MIT?</p>
</blockquote>
<p>I am required to do web scrapping and other information resources and generate a chatGPT type output.
I am new to data science and understand that this is not a regression of classification task. I want to know what is this problem called so that I can do the relevant study and solve this problem.</p>
<p>Also, how should I go about solving this problem, like a block diagram is highly appreciated.</p>
","nlp"
"122408","how to select number of number of layers and neurons in neural network(RNN) in standard way?","2023-06-26 15:39:46","","1","1578","<machine-learning><deep-learning><neural-network><nlp><tensorflow>","<p>For example, i have a 4000 samples/data points and we have to categorize them into 4 classes.</p>
<p>while building MLP RNN multi-class text classification model, which has 4 classes.</p>
<p>For building model,</p>
<p>1.initially how many neurons should we take in input layer?</p>
<p>2.how many hidden layers &amp; number of neurons in each layer should we consider initially?</p>
<p>3.how to take decision about activation functions (i.e) what functions to add at what layer or at neurons?</p>
<ol start=""4"">
<li>how to decide the threshold for multi-class classification in this use-case?</li>
</ol>
<p>what is the basic approach or assumption to consider the initial values?</p>
","nlp"
"122394","Smart Selection of Training Data for Fine-tuning Language Models in Small Domains","2023-06-25 23:13:22","122399","2","50","<nlp><dataset><language-model>","<h2>Background</h2>
<p>I am working to make language models (for example, Stanford's Alpaca model) perform well on a new small domain through fine-tuning on domain-specific dataset <span class=""math-container"">$D$</span>.</p>
<p>If the size of <span class=""math-container"">$D$</span> is <span class=""math-container"">$N$</span>, I would like to find a subset <span class=""math-container"">$n \ll N$</span> to fine-tune the model due to my limited computation resource: I could only afford to fine-tune the model for 24 GPU hours but fine-tuning on the entire <span class=""math-container"">$D$</span> will take 2400 GPU hours.</p>
<h2>Question</h2>
<p>Is there any strategy I could select <span class=""math-container"">$n$</span> smartly so that the model I fine-tuned is likely to perform better than if I choose <span class=""math-container"">$n$</span> in alternative ways? Here are two options I could think of:</p>
<ul>
<li>Randomly select <span class=""math-container"">$n$</span> from <span class=""math-container"">$N$</span>.</li>
<li>Measure the quality of each of <span class=""math-container"">$N$</span> in some way and fine-tune the data by selecting those of higher quality ones. I got this idea from curriculum learning (<a href=""https://arxiv.org/abs/2010.13166"" rel=""nofollow noreferrer"">a survey paper</a> in 2020).</li>
</ul>
<h2>Note</h2>
<p>This question has been cross-posted from <a href=""https://stats.stackexchange.com/q/619713/191779"">CrossValidated</a>.</p>
","nlp"
"122362","""implemented"" vs ""future promise"" text scoring in NLP","2023-06-24 14:17:31","","0","12","<nlp>","<p>Are there any pretrained NLP models or even analytical techniques to classify or score sentences from election speech into &quot;implemented&quot; and &quot;future promise&quot; categories.</p>
<p>Essentially, we aim to distinguish between text that describes something that has already been implemented or achieved and text that describes something with potential or promise for the future.</p>
<p>e.g. For &quot;implemented&quot; category:</p>
<ul>
<li>Since our regime from last 5 years, we have ensured your village has 24 x 7 electricity.</li>
</ul>
<p>e.g. For &quot;future promise&quot; category:</p>
<ul>
<li>If we get elected, we will bring 24 x 7 electricity in your region.</li>
</ul>
","nlp"
"122351","Doubt in gradient , vanishing gradient problem in Back propagation","2023-06-24 02:20:41","","1","56","<machine-learning><deep-learning><neural-network><nlp><gradient-descent>","<p>As per my knowledge, in back propagation- loss function or gradient is used to update the weights.
in back propagation, weights became small w.r.t gradients, this leads to vanishing gradient problem.</p>
<p>can you please give insights about these two terms (gradient(SGD), exploding gradient problem, vanishing gradient problem).</p>
<p>how to select which activation function is useful/suitable at different layers?</p>
","nlp"
"122322","memory and context in LLM models","2023-06-22 16:28:25","122331","0","1678","<nlp><language-model>","<p>I have a large document and I may need to introduce a large part of it to my llm for insight generation I know that that text can be chunked into parts and with the right prompt I can get what I want with the langchain memory feature but how long can my model remember past conversations?</p>
","nlp"
"122315","How to load Hugginface model on CPU?","2023-06-22 11:09:59","","1","250","<nlp><huggingface>","<p>My system runs out of memory on GPU. But I want just to test if it works and want to load it on CPU. How do I do that?
Adding <code>device = torch.device(‘cpu’)</code> before loading model doesn’t help
It crashes here:</p>
<pre><code>model = AutoModelForCausalLM.from_pretrained(
    config.base_model_name_or_path,
    quantization_config=bnb_config,
    device_map=&quot;auto&quot;,
    trust_remote_code=True,
)
</code></pre>
","nlp"
"122285","Fine-tuning LLM or prompting engineering?","2023-06-21 08:37:21","122297","7","2820","<nlp>","<p>For some type of chatbot, like a customized chatbot, may it be better to fine-tune the <a href=""https://en.wikipedia.org/wiki/Language_model"" rel=""nofollow noreferrer"">LM</a> for the domain-specific data and the type of question instead of relying on the prompt as prompts have to be processed by the LM every time?</p>
<p>Another reason is that how can the system add the correct prompt for each question, not to say there are many new questions which don't have any existing prompts? So I feel prompting is good, but not practical. Do you agree or do I miss some key points?</p>
","nlp"
"122281","Query on Attention Architecture","2023-06-21 07:58:58","122283","0","37","<deep-learning><nlp><tensorflow><transformer><attention-mechanism>","<p>As we most know that, Attention is focuses on specific parts of the input sequence those are most relevant in generating output sequence.</p>
<p>Ex: The driver could not drive the car fast because it had a problem.</p>
<ol>
<li><p>how attention finds the specific parts(here it is 'it') in input and how it will assign score for the token?</p>
</li>
<li><p>is attention context- based model?</p>
</li>
<li><p>how to obtain attention maps (query, key, value)?</p>
</li>
<li><p>On what basis attention assigns higher weights to input tokens?</p>
</li>
</ol>
","nlp"
"122252","How can someone evaluate llama index model?","2023-06-19 19:24:53","","2","175","<python><nlp><huggingface><openai-gym><chatbot>","<p>I have built a openAI llama index based model which takes multiple pdf and able to give chatbot based response. I want to evaluate the llm for accuracy. I already know method such as Rouge and Bleu. Is there any other way to evaluate model ?</p>
","nlp"
"122236","what is the difference between word2vec and doc2vec","2023-06-19 01:58:32","","0","182","<machine-learning><deep-learning><nlp><transformer><word2vec>","<p>As we know Word2Vec is a non-contextual embedding, here it maps the words in global vocabulary and returns their corresponding vectors (at word level).</p>
<p>In case of Doc2Vec, hope this is also non-contextual embedding and it return the vectors at document level, that means internally document is a union of paragraphs, sentences (i.e words).</p>
<p>what is the implementation style of Doc2Vec?</p>
<p>is any difference between Doc2vec, sent2vec,word2vec ? (because for all these word/subword is basic)?</p>
<p>please share the more insights about them?</p>
","nlp"
"122232","more insights about Word2Vec implementation","2023-06-18 14:31:18","","0","29","<machine-learning><deep-learning><nlp><word-embeddings><word2vec>","<p>As we know Word2Vec is non-contextual embedding (at word level). As per my knowledge, BOW is statistical embedding technique (word level).</p>
<p>we can perform Word2Vec embedding in two approaches: 1. CBOW. 2. Skip-gram analysis</p>
<p>am confused with BoW and CBoW as both methods output is numerical/continuous vector. then what is difference between these two?</p>
<p>and can you please share the more insights about CBoW and Skip-gram architecture/implementation?</p>
","nlp"
"122225","Will LLMs accumulate its skills after each time it is taught by one in-context learning?","2023-06-18 08:07:13","122234","4","2258","<nlp>","<p>If the model’s parameters aren’t updated during the in-context learning (ICL),
is the skills it just learned during the current ICL be kept/saved somehow in the model by some other way other than parameters?</p>
<p>Put it in another way,  will LLMs accumulate its skills after each time it is taught by a ICL?</p>
","nlp"
"122220","what is the difference between NSP and text prediction","2023-06-17 17:02:37","","0","278","<machine-learning><deep-learning><nlp><transformer><bert>","<p>In BERT, NSP (Next Sentence Prediction) is for predicting next sentence based on context and Text prediction task is also for predicting next word or phrases.</p>
<p>So, both are for predicting next sentence or word/ phrase only and both are BERT NLP tasks, then why these two?</p>
","nlp"
"122212","What are the Bidirectional methodoly based pre-trained models in NLP","2023-06-17 10:57:57","","0","10","<machine-learning><deep-learning><nlp><data-science-model><transformer>","<p>BERT model is useful to solve many NLP tasks (11+) like Sentiment analysis, Q&amp;A, summarization and NER.</p>
<p>what are the other predefined model similar to BERT?
what is best way to find them in NLP?</p>
","nlp"
"122201","What's Best way in selecting right model for document comparison","2023-06-16 12:51:24","","0","101","<machine-learning><deep-learning><nlp><pytorch><similarity>","<p>We have different pre-trained models like BERT, USE, ELMo, Word2Vec, FastText, etc..,
we have documents in different sizes (large, medium, small). now, we want to do document similarity. how can we decide which pre-trained model/transformer suits for our requirement (for fine-tuning) and what will be the best approach to do?</p>
","nlp"
"122197","What are the preprocessing steps for text classification after removing stopwords?","2023-06-16 10:58:29","122199","1","256","<machine-learning><deep-learning><nlp><text-classification>","<p>I am working on an NLP project where I have text that I need to categorize based on topics (The data is 2 columns, text and topic).</p>
<p>Something that I am stuck on now is the preprocessing part. What are the steps and in what order?</p>
<p>So far, what I have done is remove stopwords.</p>
<p>I have heard of TFIDF, Count Vectorizer, BOW, Tokenization, and Lemmatization/Stemming. However, I am confused about when to use them and in what order. Could someone please explain what can I do after removing stopwords? And do I need to one-hot encode the labels (topics) in order to pass it on to the model?</p>
<p>Thanks in advance.</p>
","nlp"
"122194","How to handle OOV in non-contextual embedding (word2vec, Glove, FastText)?","2023-06-16 08:03:50","122195","1","505","<machine-learning><deep-learning><nlp><tensorflow><pytorch>","<p>how non-contextual embedding (Word2Vec, Glove, FastText) handle OOV (incase if given word is not available in vocabulary)</p>
","nlp"
"122191","What is the ELMO approach to learn contextual embedding?","2023-06-16 06:28:17","122193","2","359","<machine-learning><deep-learning><nlp><tensorflow><pytorch>","<p>BERT, GPT, and ELMo used the contextual embedding. but, their approach of learning contextual embedding is different.</p>
<p>so, what is the ELMo approach to learn contextual embedding?</p>
","nlp"
"122157","Difference between Word2Vec and contextual embedding","2023-06-14 13:01:57","122160","0","958","<machine-learning><deep-learning><nlp><tensorflow><pytorch>","<p>am trying to understand the difference between word embedding and contextual embedding.</p>
<p>below is my understanding, please add if you find any corrections.</p>
<p>word embedding algorithm has global vocabulary (dictionary) of words. when we are performing word2vec then input corpus(unique words) map with the global dictionary and it will return the embeddings.</p>
<p>contextual embedding are used to learn sequence-level semantics by considering the sequence of all words in the documents.</p>
<p>but i don't understand where we considered context in word embedding.</p>
","nlp"
"122125","What do averaged word vectors represent?","2023-06-13 08:59:18","","0","92","<nlp><word-embeddings><word2vec><semantic-similarity>","<p>Assume you have high-dimensional word embeddings (d &gt; 100) for a large number of words (|V| &gt; 100,000) calculated over a huge non-specialized natural language corpus. Assume you have taken the average of all vectors of words referring to animals (like family and species names and common words like &quot;cat&quot;).</p>
<p>Would one say this averaged vector represents the &quot;prototypical animal&quot;?</p>
<p>Would one expect this vector to be similar to the vectors of &quot;animal&quot; and &quot;animal-like&quot;?</p>
<p>How would the averaged vector probably look like? Will for example many dimensions be cancelled out by averaging, and only a small number of non-vanishing dimensions remain? Will this characteristic &quot;spectrum&quot; be said to represent &quot;animal-likeness&quot;? Or won't there be something characteristic to be found?</p>
<p>(I could try to find out by myself by using <a href=""https://www.kaggle.com/datasets/pkugoodspeed/nlpword2vecembeddingspretrained"" rel=""nofollow noreferrer"">available word embeddings</a>, but I have no idea how to extract the words referring to animals from some several 100,000 words.)</p>
","nlp"
"122115","How do we get output layer in skip-gram?","2023-06-12 20:53:43","","0","51","<neural-network><nlp><word-embeddings><word2vec>","<p>Could you please explain how do we get output layer in this architecture (vectors <code>[0.2, 0.8, -1.4, 1.2]</code> and <code>[-0.3, 0.2, -0.7, 0.1]</code>). I understand that layer before are embeddings of word &quot;brown&quot;. But how do we get vector <code>[0.2, 0.8, -1.4, 1.2]</code>? I thought it should be dot product of &quot;brown&quot;'s embeddings with &quot;quick&quot;'s embeddings? Could you please describe in details how this part works?</p>
<p><a href=""https://i.sstatic.net/7lHwg.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/7lHwg.png"" alt=""enter image description here"" /></a></p>
","nlp"
"122101","Pretrained model for RNN Encoder-Decoder?","2023-06-12 08:42:10","","0","35","<nlp><reinforcement-learning><sequence-to-sequence><text-generation>","<p>Our team are implementing a paper called <a href=""https://arxiv.org/abs/1709.09346"" rel=""nofollow noreferrer"">Cold-Start-Reinforcement-Learning-with-Softmax-Policy-Gradient</a>.</p>
<p>Although the paper didn't mention. We want to use a pre-trained model, which is a RNN Encoder-Decoder based.</p>
<p>Are there any recommend for NLP Summarization with it?</p>
<p>Here are our <a href=""https://github.com/jacksonchen1998/Cold-Start-Reinforcement-Learning-with-Softmax-Policy-Gradient"" rel=""nofollow noreferrer"">code</a>.</p>
<p>Thanks all. 😄</p>
","nlp"
"121952","How to evaluate machine translations of long documents?","2023-06-03 18:15:38","","0","26","<nlp><model-evaluations><machine-translation>","<p>I'm using Python and I want to compare the output of two machine-translation (ish) systems. Most of the tools seem to be focused on sentence-by-sentence evaluation. Either I get memory blow-ups with pyter, or weird results with sacrebleu.</p>
<p>Fundamentally I just want:</p>
<p><code>$ calculate_ter file1.txt reference.txt </code></p>
<p>Or</p>
<p><code>&gt;&gt;&gt; calculate_ter(file1_str, reference_str)</code></p>
<p>I do not really care specifically about sentence boundaries or line boundaries. I just want to know how different these two texts are.</p>
","nlp"
"121943","I can't get good performance from BERT","2023-06-02 23:09:53","","1","57","<keras><nlp><word-embeddings><bert><recurrent-neural-network>","<p>I trained NLP models. This is a subset (200 instances) of my data set of 10,000 instances:<a href=""https://pastebin.com/FThmWXeE"" rel=""nofollow noreferrer"">This the link of the dataset on pastebin</a></p>
<p>I compare an LSTM model with a glove model and a BERT model. I expected a good performance with BERT. I can't get past 20% accuracy with BERT at all. I wonder what I'm missing in its implementation.</p>
<pre><code>!pip list
#tensorflow 2.12.0
!python --version
#python 3.10.11

import json 
import tensorflow as tf
import numpy as np
from hyperopt import Trials, STATUS_OK, tpe
from sklearn.model_selection import train_test_split
from keras.layers import Input
from sklearn.metrics import accuracy_score
import pandas as pd

# Reading of file
f = open ('sampled_data.json', &quot;r&quot;)
data = json.loads(f.read())
</code></pre>
<h1>Data preprocessing</h1>
<pre><code>X=[x[&quot;title&quot;].lower() for x in data]
y=[x[&quot;categories&quot;][0].lower() for x in data]
X_train,X_test,y_train,y_test= train_test_split(X,y, test_size=0.2, 
                                                random_state=42)
</code></pre>
<h3>target preprocessing. To consider the category unknown if not seen in test set</h3>
<pre><code>cat_to_id={'&lt;UNK&gt;':'0'}
for cat in y_train:
  if cat not in cat_to_id:
    cat_to_id[cat]=len(cat_to_id)

#MAPPING WITH RESPECT TO THE TRAINING SET
id_to_cat={v:k for k,v in cat_to_id.items()}
def preprocess_Y(Y,cat_to_id):
  res=[]
  for ex in Y:
    if ex not in cat_to_id.keys():
      res.append(cat_to_id['&lt;UNK&gt;'])
    else: 
      res.append(cat_to_id[ex])
  return np.array(res)

y_train_id=preprocess_Y(y_train,cat_to_id)
y_test_id=preprocess_Y(y_test,cat_to_id)
y_test_id=y_test_id.astype(float)

# Tokenization of of features
tokenizer=tf.keras.preprocessing.text.Tokenizer(num_words=10000)
tokenizer.fit_on_texts(X_train)

# TEXT TO SEQUENCE
X_train_seq=tokenizer.texts_to_sequences(X_train)
X_test_seq=tokenizer.texts_to_sequences(X_test)

#PADDING pad_sequences function transform in array
max_len=max([len(length) for length in X_train_seq])
X_train_pad= tf.keras.preprocessing.sequence.pad_sequences(X_train_seq,maxlen=max_len, truncating='post')
X_test_pad= tf.keras.preprocessing.sequence.pad_sequences(X_test_seq,maxlen=max_len, truncating='post')


####### RECCURRENT NEURAL NETWORK###############

vocab_size=len(tokenizer.word_index)
Embed_dim=300
dropout=0.2
dense_size=128
num_cat=len(cat_to_id)
batch_size=16
epochs=15

### CREER LE MODELE
model_rnn=tf.keras.models.Sequential()

# Add an embedding layer
model_rnn.add(tf.keras.layers.Embedding(input_dim=vocab_size, 
                                        output_dim=Embed_dim, 
                                        input_length=max_len))

# Add an LSTM layer
model_rnn.add(tf.keras.layers.LSTM(units=128))
model_rnn.add(tf.keras.layers.Dropout(0.4))

# Dense + activation
model_rnn.add(tf.keras.layers.Dense(units=dense_size,activation='relu'))
#Classifieur + activation
model_rnn.add(tf.keras.layers.Dense(units=num_cat,activation='softmax'))
print(model_rnn.summary())

model_rnn.compile(loss= 'sparse_categorical_crossentropy',
                  optimizer='adam',
                  metrics='accuracy')

model_rnn.fit(X_train_pad,y_train_id, batch_size=batch_size, epochs=epochs)

model_rnn.evaluate(X_test_pad, y_test_id)


Epoch 1/15
9/9 [==============================] - 5s 166ms/step - loss: 3.6699 - accuracy: 0.1643
Epoch 2/15
9/9 [==============================] - 1s 128ms/step - loss: 3.3861 - accuracy: 0.2286
Epoch 3/15
9/9 [==============================] - 1s 157ms/step - loss: 3.1313 - accuracy: 0.2357
Epoch 4/15
9/9 [==============================] - 1s 88ms/step - loss: 3.0774 - accuracy: 0.2286
Epoch 5/15
9/9 [==============================] - 1s 127ms/step - loss: 3.0358 - accuracy: 0.2286
Epoch 6/15
9/9 [==============================] - 0s 27ms/step - loss: 2.9461 - accuracy: 0.2286
Epoch 7/15
9/9 [==============================] - 0s 27ms/step - loss: 2.7970 - accuracy: 0.2357
Epoch 8/15
9/9 [==============================] - 1s 75ms/step - loss: 2.5048 - accuracy: 0.2429
Epoch 9/15
9/9 [==============================] - 1s 86ms/step - loss: 2.2543 - accuracy: 0.3357
Epoch 10/15
9/9 [==============================] - 1s 47ms/step - loss: 1.9985 - accuracy: 0.4357
Epoch 11/15
9/9 [==============================] - 0s 39ms/step - loss: 1.7728 - accuracy: 0.4929
Epoch 12/15
9/9 [==============================] - 1s 41ms/step - loss: 1.5552 - accuracy: 0.5929
Epoch 13/15
9/9 [==============================] - 0s 11ms/step - loss: 1.3320 - accuracy: 0.5929
Epoch 14/15
9/9 [==============================] - 0s 11ms/step - loss: 1.1506 - accuracy: 0.6786
Epoch 15/15
9/9 [==============================] - 0s 42ms/step - loss: 0.9498 - accuracy: 0.7714
2/2 [==============================] - 1s 13ms/step - loss: 6.6335 - accuracy: 0.2000



############# MODEL WITH GLOVE###################
embeddings_index = {}
f = open('glove.6B.300d.txt', encoding='utf-8')
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()

# Create embedding matrix
word_index=tokenizer.word_index
num_words = len(word_index) + 1
embedding_dim = 300
embedding_matrix = np.zeros((num_words, embedding_dim))
for word, i in word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector


    num_words=len(tokenizer.word_index)+1
embedding_dim=300
max_len=max([len(length) for length in X_train_seq])
dense_size=128
num_cat=len(cat_to_id)
batch_size=16
epochs=7
num_classes=len(cat_to_id)


# Create the model
model_glove = tf.keras.models.Sequential()
model_glove.add(tf.keras.layers.Embedding(input_dim=num_words,
                                          output_dim=embedding_dim,
                                          input_length=max_len,
                                          weights=[embedding_matrix]
                                       ))

#model_glove.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=128)))
model_glove.add(tf.keras.layers.LSTM(units=128))
model_rnn.add(tf.keras.layers.Dropout(0.2))
model_glove.add(tf.keras.layers.Dense(num_classes, activation='softmax'))
    

# Compile the model
model_glove.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
model_glove.fit(X_train_pad,y_train_id , epochs=epochs, batch_size=batch_size)
model_glove.evaluate(X_test_pad, y_test_id)
Epoch 1/7
9/9 [==============================] - 4s 169ms/step - loss: 3.5065 - accuracy: 0.1714
Epoch 2/7
9/9 [==============================] - 1s 148ms/step - loss: 2.9357 - accuracy: 0.2357
Epoch 3/7
9/9 [==============================] - 1s 152ms/step - loss: 2.5611 - accuracy: 0.2929
Epoch 4/7
9/9 [==============================] - 1s 108ms/step - loss: 2.1017 - accuracy: 0.4286
Epoch 5/7
9/9 [==============================] - 1s 116ms/step - loss: 1.5988 - accuracy: 0.6071
Epoch 6/7
9/9 [==============================] - 1s 88ms/step - loss: 1.0982 - accuracy: 0.7571
Epoch 7/7
9/9 [==============================] - 1s 67ms/step - loss: 0.7189 - accuracy: 0.8786
2/2 [==============================] - 1s 11ms/step - loss: 3.7847 - accuracy: 0.1833



########### MODEL WITH BERT##################

pip install tensorflow keras transformers
from transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

#
from tensorflow.keras.preprocessing.sequence import pad_sequences
max_sequence_length=100
# Tokenization and adding special toens
X_train_encoded = [tokenizer.encode(X_train, add_special_tokens=True) for text in X_train]
# Padding
input_ids = pad_sequences(X_train_encoded, maxlen=max_sequence_length, padding='post', truncating='post')
num_classes=len(cat_to_id)
inputs = tf.keras.Input(shape=(max_sequence_length,), dtype=tf.int32)
import tensorflow as tf
from transformers import BertTokenizer, TFBertModel
from tensorflow.keras.layers import Dense
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Define and compile the model
bert_model = TFBertModel.from_pretrained('bert-base-uncased')
inputs = tf.keras.Input(shape=(max_sequence_length,), dtype=tf.int32)
outputs = bert_model(inputs)[1]
outputs = Dense(num_classes, activation='softmax')(outputs)

model = tf.keras.Model(inputs=inputs, outputs=outputs)
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(x=input_ids, y=y_train_id, epochs=20, batch_size=64)
# For prediction, preprocess the input in the same way
tokenized_inputs_test = [tokenizer.tokenize(text) for text in X_test]
input_ids_test = [tokenizer.convert_tokens_to_ids(tokens) for tokens in tokenized_inputs_test]
input_ids_test = pad_sequences(input_ids_test, maxlen=max_sequence_length, padding='post', truncating='post')
# Evaluate the model
loss, accuracy = model.evaluate(x=input_ids_test, y=y_test_id)

Epoch 1/20
3/3 [==============================] - 3s 947ms/step - loss: 3.2514 - accuracy: 0.2062
Epoch 2/20
3/3 [==============================] - 3s 953ms/step - loss: 3.2550 - accuracy: 0.2062
Epoch 3/20
3/3 [==============================] - 3s 950ms/step - loss: 3.2695 - accuracy: 0.2062
Epoch 4/20
3/3 [==============================] - 3s 957ms/step - loss: 3.2598 - accuracy: 0.2062
Epoch 5/20
3/3 [==============================] - 3s 958ms/step - loss: 3.2604 - accuracy: 0.2062
Epoch 6/20
3/3 [==============================] - 3s 953ms/step - loss: 3.2649 - accuracy: 0.2062
Epoch 7/20
3/3 [==============================] - 3s 948ms/step - loss: 3.2507 - accuracy: 0.2062
Epoch 8/20
3/3 [==============================] - 3s 940ms/step - loss: 3.2564 - accuracy: 0.2062
Epoch 9/20
3/3 [==============================] - 3s 932ms/step - loss: 3.2727 - accuracy: 0.2062
Epoch 10/20
3/3 [==============================] - 3s 944ms/step - loss: 3.2611 - accuracy: 0.2062
Epoch 11/20
3/3 [==============================] - 3s 930ms/step - loss: 3.2527 - accuracy: 0.2062
Epoch 12/20
3/3 [==============================] - 3s 923ms/step - loss: 3.2578 - accuracy: 0.2062
Epoch 13/20
3/3 [==============================] - 3s 921ms/step - loss: 3.2626 - accuracy: 0.2062
Epoch 14/20
3/3 [==============================] - 3s 935ms/step - loss: 3.2546 - accuracy: 0.2062
Epoch 15/20
3/3 [==============================] - 3s 922ms/step - loss: 3.2617 - accuracy: 0.2062
Epoch 16/20
3/3 [==============================] - 3s 918ms/step - loss: 3.2577 - accuracy: 0.2062
Epoch 17/20
3/3 [==============================] - 3s 922ms/step - loss: 3.2602 - accuracy: 0.2062
Epoch 18/20
3/3 [==============================] - 3s 921ms/step - loss: 3.2617 - accuracy: 0.2062
Epoch 19/20
3/3 [==============================] - 3s 929ms/step - loss: 3.2513 - accuracy: 0.2062
Epoch 20/20
3/3 [==============================] - 3s 919ms/step - loss: 3.2497 - accuracy: 0.2062
</code></pre>
","nlp"
"121899","Are there any prebuilt models that I can apply to electronic health records","2023-06-01 16:21:59","","1","20","<nlp>","<p>I've been given a task by work to extract relevant disease and medication information from patient history case notes. There are about 5000 case notes, and they are about a paragraph long; They contain information on diseases/medication, family member's diseases, hospital admissions/scans and health behaviours (smoking, drinking, etc.).</p>
<p>While I'm have some knowledge of how NLP works, I'm a statistician, not a data scientist. I see from ChatGPT, automation of this task is possible, but I'm unsure if it's something someone in my position is able to do. I'm aware of clincal BERT but I'm not sure how applicable it is to my problem. As far as I know, clinical BERT is a pretrained model for word embeddings, and I feel like this get's me some of the way towards something useful. I have some queries though.</p>
<ol>
<li>Firstly, am I going down the right path in looking at something like clinical BERT</li>
<li>How would a model that is pretrained on a corpus of text handle text that it has not formed embeddings for, e.g., spelling errors, abbreviations and synonyms</li>
<li>If it can do the above, do I have to train it myself to define disease and medication with NER fine tuning</li>
<li>Does fine tuning involve providing a list of all disease and medications I expect to see in my case notes? Or, once it is fine tuned on a few examples, will it be able to identify unlisted diseases/medaction?</li>
</ol>
<p>I realise these questions are pretty superficial; I just need to know if it's worth going down this route; or, if the models that have the capacity of doing what I need are beyond what I can realistically develop, and I may as well get a head start on extracting this information manually.</p>
<p>Any guidance would be really appreciated.</p>
<p>Thanks!</p>
","nlp"
"121892","LLAMA MODEL WITHOUT USING HUGGINGFACE API","2023-06-01 12:42:58","","1","771","<python><nlp><scikit-learn><machine-learning-model>","<p>Is it possible to obtain the llama model alone as open source code without using the Huggingface API so that it can be hosted on our server?</p>
","nlp"
"121854","Text segmentation problem","2023-05-30 16:17:28","121855","0","258","<nlp><scikit-learn><word-embeddings><text><gensim>","<p>I am new to ML and trying to solve problem of text segmentation.</p>
<p>I have a transcript of news show and I want to split this transcript into parts by topic. I tried to google and asked chatgpt and found a lot of info, but I don't understand how to properly run this task.</p>
<p>It looks like a classic problem and I cant find proper naming for it.</p>
<p>I am looking for help to find proper names for this problem, and, how to approach it with existing tools.</p>
<p>My initial thought was to use word embeddings -&gt; sentence vectors with rolling average to detect changes in topics, but this approach does not work. What are other ways to solve this problem?</p>
","nlp"
"121818","About the last decoder layer in transformer architecture","2023-05-28 16:47:14","","1","179","<deep-learning><neural-network><nlp><transformer><linear-algebra>","<p>So, in the decoder layer of transfomer, suppose I have predicted 3 words till now, including the start token then the last decoder layer will produce 3 vectors of size d-model, and only the last vector will pass through embedding layer to form logits. Am I getting this right? Because its nowhere mentioned in the original paper and I'm having a hard time understanding it. What about the information that gets lost by discarding the two tokens before the last token. We could try to linearly project all the vectors into a single d-dimension vector but then the size of vectors would keep on increasing after we predict new word everytime and we'd need a new projection matrix everytime. This detail seems implicit and isnt mentioned anywhere. Can someone provide me what is actually done and the reason behind this or is this a random heuristic that seems to work (i.e. just take the final hidden state produced by the decoder)</p>
","nlp"
"121782","Some simple questions about confusion matrix and metrics in general","2023-05-26 16:07:54","121787","2","1139","<machine-learning><nlp><class-imbalance><metric><confusion-matrix>","<p>I will first tell you about the context then ask my questions.</p>
<p>The model detects hate speech and the training and testing datasets are imbalanced (NLP).</p>
<p>My questions:</p>
<ol>
<li>Is this considered a good model?</li>
<li>Is the False negative really bad and it indicates that my model will predict a lot of ones to be zeros on new data?</li>
<li>Is it common for AUC to be higher than the recall and precision when the data is imbalanced?</li>
<li>Is the ROC-AUC misleading in this case because it depends on the True Negative and it is really big? (FPR depends on TN)</li>
<li>For my use case, what is the best metric to use?</li>
<li>I passed the probabilities to create ROC, is that the right way?</li>
</ol>
<p><a href=""https://i.sstatic.net/59Z1Z.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/59Z1Z.png"" alt=""enter image description here"" /></a></p>
<p><strong>Edit:
I did under-sampling and got the following results from the same model parameters:</strong>
<a href=""https://i.sstatic.net/kebJh.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/kebJh.png"" alt=""enter image description here"" /></a></p>
<p><strong>Does this show that the model is good? or can it be misleading too?</strong></p>
","nlp"
"121757","How do people usually handle creating an embedding vector of longer texts (32000 characters?","2023-05-25 17:28:00","","1","133","<python><nlp><text>","<p>I have a set of podcast episode transcriptions in Arabic. I wish to convert these to embedding vectors so I can run a similarity comparison of them. Here's the summary statistics on the episodes:</p>
<p><a href=""https://i.sstatic.net/f4MeW.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/f4MeW.png"" alt=""enter image description here"" /></a></p>
<p>Here's the model I used</p>
<p><a href=""https://huggingface.co/asafaya/bert-base-arabic"" rel=""nofollow noreferrer"">https://huggingface.co/asafaya/bert-base-arabic</a></p>
<p>So the problem I'm running into is that the initial model I tried only accepts context windows of 512 characters. This means I can't run the whole sequence through it.</p>
<p>I tried chunking the text and then taking the average of the chunk vectors, but this didn't work. It seemed to create noise as all the vectors appeared similar even though their texts were not.</p>
<p>How do people usually handle creating an embedding vector of longer texts?</p>
","nlp"
"121756","How to monitor training of text generation models?","2023-05-25 16:46:55","","1","238","<deep-learning><nlp><text-generation><huggingface><nlg>","<p>I'm finetuning a pretrained Huggingface model based on Transformers for a downstream <strong>Text Generation</strong> task, but I have doubts on how the fine-tuning process should be monitored:</p>
<p>In classification, I usually calculate the loss and other metrics (e.g   accuracy) on a validation set to for early stopping and to save the checkpoint of the best epoch, but for Text Generation tasks I see these additional issues:</p>
<ul>
<li><p>(Causal) text generation is <em>slow</em>, while training can be more parallelized with teacher forcing. <em>Does it make sense to perform validation and generate text after every training epoch?</em></p>
</li>
<li><p>Text generation depends on an additional set of hyperparameters which greatly condition the quality of generated text, such as the <a href=""https://huggingface.co/blog/how-to-generate"" rel=""nofollow noreferrer"">decoding technique</a> (greedy vs sampling-based or other techniques), number of beams, temperature, maximum length, etc. All these parameters are not actually used during training. <em>I suppose  also find the best combination <em>after</em> training, but how can I monitor the training?</em></p>
</li>
<li><p>HuggingFace <a href=""https://huggingface.co/docs/transformers/main_classes/text_generation"" rel=""nofollow noreferrer"">generation API</a> does not provide the loss during prediction, i.e I cannot generate text and calculate the cross-entropy loss (at least out-of-the-box) during validation. To calculate loss I could either</p>
<ol>
<li>Create a custom generation procedure which includes loss.</li>
<li>Perform two passes on all data during validation (one with <code>model.generate</code> and one with <code>model.forward</code> )</li>
</ol>
<p><em>Both these alternatives are suboptimal and this made me think that it is not common to calculate validation loss in text generation tasks, is it true?</em></p>
</li>
</ul>
<p><strong>What is the common way to monitor training/fine-tuning of text generation models?</strong></p>
","nlp"
"121745","Why my sentiment analysis model is overfitting?","2023-05-25 10:00:44","121747","0","194","<classification><nlp><text-classification><sentiment-analysis><tfidf>","<p>The task is to predict sentiment from 1 to 10 based on Russian reviews. The training data size is 20000 records, of which 1000 were preserved as a validation set. The preprocessing steps included punctuation removal, digit removal, Latin character removal, stopword removal, and lemmatization. Since the data was imbalanced, I decided to downsample it. After that, TF-IDF vectorization was applied. At the end, I got this training dataset:</p>
<p><a href=""https://i.sstatic.net/EWo0m.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/EWo0m.png"" alt=""enter image description here"" /></a></p>
<p>The next step was the validation set TF-IDF transformation:</p>
<p><a href=""https://i.sstatic.net/HAv7x.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/HAv7x.png"" alt=""enter image description here"" /></a></p>
<p>As a classifier model, I chose MultinomialNB (I read it is useful for text classification tasks and sparse data). The training data fit was pretty quick:</p>
<pre><code># TODO: create a Multinomial Naive Bayes Classificator

clf = MultinomialNB(force_alpha=True)
clf.fit(X_res, y_res.values.ravel())
</code></pre>
<p>But the problem was in model evaluation part:</p>
<pre><code># TODO: model evaluation

print(clf.score(X_res, y_res.values.ravel()))
print(clf.score(X_val, y_val.values.ravel()))
y_pred = clf.predict(X_val)
print(precision_recall_fscore_support(y_val, y_pred, average='macro'))
</code></pre>
<p>Output:</p>
<pre><code>0.9352409638554217
0.222
(0.17081898127154763, 0.1893033502842826, 0.16303596541199034, None)
</code></pre>
<p>It is obvious that the model is overfitting, but what do I do? I tried to use SVC, KNeighborsClassifier, DecisionTreeClassifier, RandomForestClassifier, and GaussianNB, but everything remained the same. I tried to play around with the MultinomialNB hyperparameter <code>alpha</code> but <code>force_alpha=True</code> option is the best so far.</p>
","nlp"
"121738","Extract phrases/keywords that are SIMILAR to a python list of keyword/phrases, from a document","2023-05-25 07:08:58","","0","307","<machine-learning><python><deep-learning><nlp>","<p><strong>EDIT :</strong> If I had to match single worded phrases, I could first tokenize the text from the document and then calculate the cosine similarity of all the tokens with all the keywords from the <code>keyword_list</code>. But the issue is that I might have <strong>single worded</strong> or <strong>multi worded keyphrases</strong> present in the <code>keyword_list</code>. Even if I try to use <code>ngrams</code>, how would I know what <strong>length of <code>ngrams</code></strong> to use?</p>
<p>I have searched and read many articles/questions regarding this but could not find a solution.</p>
<p><strong>Problem Statement</strong> : I am trying to extract <strong>similar</strong> keywords/phrases from a document, based on a pre-curated list of keywords/phrases.</p>
<p>For example below is the list:</p>
<pre><code>keyword_list = ['your work', 'ongoing operations', 'completed operations', 'your name', 'bodily injury', 'property damage', 
     'to the extent permitted by law', 'is required by a contract or agreement']
</code></pre>
<p>I also have the text I extracted from the documents using OCR. Let's say the text is as below:</p>
<pre><code>text = &quot;In light of your ongoing operations, your name is an approximation of your working models. The contract requires that the damage done to the property must be borne by both the parties, as permitted by the law.&quot; 
</code></pre>
<p>Now I want to extract all the keywords/phrases that occur in the keyword_list. In addition to that I also want to extract <strong>similar</strong> keyphrases (by similar I mean <strong>similar in context or meaning</strong> but worded differently). So the logic/model should be able to extract the following terms:</p>
<pre><code>output = [&quot;ongoing operations&quot;, &quot;your name&quot;, &quot;your working&quot;, &quot;The contract requires&quot;, &quot;damage done to the property&quot;, &quot;as permitted by the law&quot;]
</code></pre>
<p>We can see that <code>ongoing operations</code> and <code>your name</code> are present in the <code>keyword_list</code> and hence are extracted.</p>
<p>But <code>your working</code>, <code>The contract requires</code>, <code>damage done to the property</code>, <code>as permitted by the law</code> are also extracted because they have the same meaning/context to <code>your work</code>, <code>is required by a contract or agreement</code>, <code>property damage</code>, <code>to the extent permitted by law</code>.</p>
<p>For the phrases matching completely (<code>ongoing operations</code> and <code>your name</code>), I have written a logic which uses regex to match the phrases. But for the phrases which have the same meaning/context but worded differently, I am unsure how to proceed. I think a Machine learning or Deep learning approach would be suitable here but I don't know which exact approach!</p>
<p>Any help is appreciated!</p>
","nlp"
"121721","What are the approaches for extracting an injury and its description from a paragraph?","2023-05-24 06:49:14","","0","56","<machine-learning><nlp><python-3.x><spacy><information-extraction>","<p>Suppose I have a paragraph which explains the injuries and its descriptions. I want to extract the injuries and its corresponding descriptions from the text. How can I do that?</p>
<p>For example, the paragraph will be as follows:</p>
<p>In my opinion the neck pain is due to the soft tissue injury. The fracture on the hand will be resolved in  2 months. The pain in the shoulder and neck is due to the soft tissue injury. There is a stiffness and discomfort around the hip.</p>
<p>the expected output is :</p>
<pre><code>{
&quot;neck&quot;: [&quot;soft tissue&quot;],
&quot;hand&quot;: [&quot;fracture&quot;],
&quot;shoulder&quot;: [ &quot;soft tissue&quot;],
&quot;hip&quot;: [&quot;stiffness&quot;, &quot;discomfort&quot;]
}
</code></pre>
<p>Which NLP techniques can be used here?</p>
<p>We have two txt files for injuries and descriptions.</p>
<p>But how will we relate or match the description with its corresponding injury?</p>
<p>I tried the dependency parser but the problem is we have to write a number of patterns for each injury, we have more than 100 injuries and more than 100 descriptions. So if we are writing patterns for all the injuries there will be a large number of patterns and I think it will take too much time and power.</p>
<p>Are there any other ways to do this kind of extraction?</p>
<p>The paragraph doesn't have a common structure.</p>
<p>I'm using python and spacy for this.</p>
","nlp"
"121685","Speech to Text for Unsupported Language","2023-05-22 08:24:17","","1","57","<nlp><transfer-learning><speech-to-text>","<p>I'm working on a project to plug the good old speech recognition in my app. However, I wish to do it in my country's dialect which is not supported by the major APIs like Azure, AWS, etc.</p>
<p>My country's national language is supported by them and this dialect is pretty similar. So I'm wondering whether is it a good idea to customise these pre-trained models or should I just start from scratch.</p>
<p>Thanks and any help would be appreciated!</p>
","nlp"
"121669","BIO Format (Skills,Qualification,Experience)","2023-05-21 07:31:01","","0","147","<nlp><named-entity-recognition>","<p>I have Dataset (CSV format).
<a href=""https://i.sstatic.net/JJxev.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/JJxev.png"" alt=""enter image description here"" /></a></p>
<p>My mail goal is to do named entity recognition and use algorithms that are today's SOTA, for example according to the website nlpprogress.com.</p>
<p>One of the SOTA is this repository: <a href=""https://github.com/ZihanWangKi/CrossWeigh/tree/master"" rel=""nofollow noreferrer"">https://github.com/ZihanWangKi/CrossWeigh/tree/master</a></p>
<p>Now, from what I've seen for named entity recognition I need to create a BIO file format.</p>
<p>Which I don't have right now.</p>
<p>What I have in my hand is a csv with a division of fields into their respective headers.</p>
<p>The question is how do I create such a dataset with the appropriate tags:
B-Skill, I-SKILL, B-EDU, I-EDU, B-EXP, I-EXP</p>
","nlp"
"121639","Load an LLM in multiple GPUs","2023-05-19 12:15:42","","2","7219","<python><nlp><pytorch><transformer><gpu>","<p>I am doing a POC on LLM text generation. I have one AWS p3.8x instance which has 4 GPUs each of 16 GB size. I am pretty new to use LLM and GPU. When I am trying load one LLM pertained model (WizardLM) in GPU, it is saying 16 GB is not sufficient for this. So my question is how can I load the model using all 64 GB?</p>
","nlp"
"121622","Sentence tokenization for sentence without punctuation","2023-05-19 03:33:35","","3","989","<python><nlp><python-3.x><nltk><spacy>","<p>I wish to perform sentence tokenization for sentence without punctuation, below is the code:</p>
<pre><code>import nltk

def segment_sentences(text):
    # Download the Punkt tokenizer if necessary
    nltk.download('punkt')
    
    # Tokenize the text into sentences
    sentences = nltk.sent_tokenize(text)
    
    return sentences

input_text = &quot;hello how are you today i hope you're doing well have a great day&quot;

sentences = segment_sentences(input_text)

# Print the segmented sentences
for sentence in sentences:
    print(sentence)
</code></pre>
<p>Desired output</p>
<pre><code>hello how are you today
i hope you're doing well
have a great day
</code></pre>
<p>But current output</p>
<pre><code>hello how are you today i hope you're doing well have a great day
</code></pre>
<p>How should I address it?</p>
","nlp"
"121605","LLM powered chat bot enhanced by NER","2023-05-18 10:26:08","","1","297","<nlp><named-entity-recognition><language-model>","<ul>
<li>I have been reading on the capabilities of LLM based conversational agents and have been wondering if there is even possibility for any further enhancement with the addition of NER to such system.</li>
<li>If so, in which case could a conversational agents powered by an LLM like say Dolly 2.0 be enhanced by NER?</li>
</ul>
","nlp"
"121526","Transformers doubt","2023-05-14 11:29:25","","0","75","<deep-learning><neural-network><nlp><transformer><attention-mechanism>","<p><a href=""https://i.sstatic.net/jGTmJ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/jGTmJ.png"" alt=""Transformers encoder layer"" /></a></p>
<p>Basically here the <span class=""math-container"">$Q$</span>,<span class=""math-container"">$K$</span> and <span class=""math-container"">$V$</span> are passed through a linear layer to obtain the actual <span class=""math-container"">$Q$</span>,<span class=""math-container"">$K$</span> and <span class=""math-container"">$V$</span> for self attention mechanism and then we concatenate all of it.</p>
<p>My doubt is, I thought the <span class=""math-container"">$Q$</span>,<span class=""math-container"">$K$</span> and <span class=""math-container"">$V$</span> were obtained through the input embedding <span class=""math-container"">$X$</span>.</p>
<p><span class=""math-container"">$$Q=XW_q$$</span>
<span class=""math-container"">$$K=XW_k$$</span>
<span class=""math-container"">$$V=XW_v$$</span></p>
<p>How come we are using the <span class=""math-container"">$Q$</span>,<span class=""math-container"">$K$</span> and <span class=""math-container"">$V$</span> and linearly projecting them to again get back <span class=""math-container"">$Q$</span>,<span class=""math-container"">$K$</span> and <span class=""math-container"">$V$</span>.</p>
<p>Sorry if my doubt is stupid!</p>
","nlp"
"121470","Deduplication using NLP","2023-05-12 07:58:43","","0","517","<python><nlp><word-embeddings><named-entity-recognition><semantic-similarity>","<p>I have a product catalog.</p>
<p>The user can <strong>add a new product</strong> to the catalog. The user can enter <strong>some attributes</strong> (such as color, weight, etc.) in the text boxes. The user can also <strong>mention the description</strong> of the product separately.</p>
<p>Each product will have a set of attributes explicitly <strong>mentioned by the user</strong> and a description. I want to check if there are any <strong>duplicate products in the catalog</strong> based on the <strong>attributes and descriptions</strong> provided by the user.</p>
<p>I want to perform the <strong>deduplication of records</strong> in the product catalog using the <strong>attributes and descriptions</strong> provided by the user.</p>
<p>Which NLP techniques can be used to perform the deduplication?</p>
","nlp"
"121446","Can we train the Dolly v-2 model on a large general purpose unlabelled text?","2023-05-11 07:21:52","","0","160","<nlp><language-model><huggingface>","<p>I am familiar with ML and Deep Learning concepts and have had a look at Dolly and even got the pretrained model running on a Jupyter lab notebook on Databricks.</p>
<p>However when I take a look at their training dataset format, they are all in instruction and response format.</p>
<p>My specific question is that if I have a super large dump of general text that is not labelled in form of instruction and response, can I just train Dolly as an autoregressive language model that will take a piece of text as an input to the generate function later once trained, and just generate text ?</p>
<p>Suggestions would be really appreciated. Thanks</p>
","nlp"
"121433","2 basic doubts on time series","2023-05-10 13:17:26","121434","0","50","<machine-learning><deep-learning><nlp><time-series><lstm>","<p>Suppose say, I have to predict the cost of stock market. I have previous data and I have made it into the following Structure :</p>
<p>(Xt-3,Xt-2,Xt-1)---&gt;(Xt=Yt)</p>
<p>Now the order of the above data points if I use an LSTM model should be preserved which means the Day1, Day2 and Day3 should be in sequential order.</p>
<p>My doubt is I will be having different rows like this. Can I shuffle those for training while preserving the order within each row. Eg : Can I keep the row For 3 days of August before 3 days of July even though those 3 days will be given in sequential order. I am assuming we should as every models considers each data row as a separate training sample and adjusts its weights as per gradient descent so order should not matter even if we shuffle the rows. Am I right?</p>
<p>Second doubt : if I have trained my model till May 8, And I need to predict tomorrow (May 11) and my window length is 3 for LSTM</p>
<p>Should I predict May 9 and May 10 and then use May 8, may 9 and May 10 value to predict the next day or should I use actual values of May 9 and May 10. I read somewhere you need to retrain to make new forecast. But I dont think it's a compulsion. If I have trained my model till may 8 and then I give it the values of May 8 May 9 and May10 in sequential order, It should give me a forecast right?</p>
","nlp"
"121412","Easy question on autoregressive LLM","2023-05-09 00:06:56","121414","0","73","<nlp><transformer><language-model>","<p>For LLM decoder, how exactly is the K, Q, V for each decoding step?</p>
<p>Say my input prompt is &quot;today is a&quot; (good day).</p>
<p>At t= 0 (generation step 0):
K, Q, V are the projections of the sequence (&quot;today is a&quot;)
Then say the next token generated is &quot;good&quot;</p>
<p><strong>At t= 1(generation step 1):
Which one is true:</strong></p>
<ul>
<li>K, Q, V are the projections of the sequence (&quot;today is a good&quot;)</li>
</ul>
<p>OR</p>
<ul>
<li>K, Q, are the projections of the sequence (&quot;today is a&quot;) , V is the projection of sequence (&quot;good&quot;)?</li>
</ul>
","nlp"
"121392","Semantic Text similarity NLP","2023-05-08 07:37:11","","1","30","<nlp><semantic-similarity>","<p>I'm new to AIML. Right now I'm working with a requirement where I need to see if two sentences are similar semantically. I'm searching for an API which compares given sentance with another array of sentences and provide us with matching one from passed array of sentances. Just want to know if there is any proven algo/API implementation published now.</p>
<p>Please let me through this folks. Thanks in advance.</p>
","nlp"
"121314","How can I generate embeddings using previously generated BERT embeddings and feed them to an RNN?","2023-05-03 18:13:19","","0","182","<python><nlp><word-embeddings><word2vec>","<p>I'm using an <strong>unlabeled</strong> news corpus to fine-tune a multi-lingual BERT model. After that I'm using those embeddings to generate embeddings for words present in a new <strong>labeled</strong> dataset. These new embeddings will be fed to an RNN as initial weights. I want to save the embeddings of all words in the <strong>labeled</strong> dataset in a matrix. The number of rows in the matrix is the number of unique words in the <strong>labeled</strong> dataset and the number of the columns of the matrix is the dimension of the embedding vector. How can I do that?</p>
<p>I've shared a similar code for generating the embedding matrix for word2vec model:</p>
<pre><code>MAX_NB_WORDS = 200000
embed_dim = embedding_size
words_not_found = []
nb_words = min(MAX_NB_WORDS, len(word_index)) 
embedding_matrix = np.random.rand(nb_words+1, embed_dim) #no. of unique words in the labeled data=nb_words+1

    for word, i in word_index.items(): #word_index contains the indices of the word tokens in labeled data
        if i &gt;= nb_words:
            continue
        #print(word)
    
        if embeddings_index.wv.__contains__(word): #embeddings_index contains the indices of the words and corresponding embeddings in the unlabeled data
    
            embedding_vector = embeddings_index.wv[word]
        
            embedding_matrix[i] = embedding_vector
        else:
            words_not_found.append(word)   
</code></pre>
<p>Plz help me to convert the same code for a multi-lingual BERT model.</p>
<p><strong>EDIT</strong></p>
<p>After going through @<strong>noe</strong> 's comment I'm not sure that it can be achieved at all. So, I reframed my question. Answer to any one of the questions will help me. New question is given below.</p>
<p>I'm using an <strong>unlabeled</strong> news corpus to fine-tune a multi-lingual BERT model. After that I want to use those embeddings to generate embeddings for words present in a new <strong>labeled</strong> dataset. These new embeddings will be fed to an RNN. How can I achieve that?</p>
<p>I'm just a rookie. Plz share some code snippets.</p>
","nlp"
"121305","Why do varied delimiters on text inputs help training stability?","2023-05-03 12:03:39","","0","27","<nlp><word-embeddings><transformer>","<p>In the preprint paper <a href=""https://arxiv.org/abs/2201.10005"" rel=""nofollow noreferrer"">Text and code embeddings by contrastive pre-training</a>, the authors describe a Transformer encoder which</p>
<blockquote>
<p>maps the input, x and y, to embeddings, vx and vy respectively and the similarity between two inputs is quantified by the cosine similarity between their embeddings, vx and vy</p>
</blockquote>
<p>And they state:</p>
<blockquote>
<p>We found that using different delimiters leads to more stable training. For x, we use ‘[’ as [SOS]x and ‘]’ as [EOS]x, while we use ‘{’ and ‘}’ as [SOS]y and [EOS]y respectively for y</p>
</blockquote>
<p>Is there an intuitive explanation for why using different delimiters is important for training stability?</p>
","nlp"
"121235","In the attention mechanism, why don't we normalize after multiplying values?","2023-04-29 22:58:54","","0","192","<deep-learning><neural-network><nlp><transformer><attention-mechanism>","<p>As this <a href=""https://ai.stackexchange.com/q/21237/23811"">question</a> says:</p>
<blockquote>
<p>In scaled dot product attention, we scale our outputs by dividing the
dot product by the square root of the dimensionality of the matrix:</p>
<p><a href=""https://i.sstatic.net/wLI4m.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/wLI4m.png"" alt=""enter image description here"" /></a></p>
<p>The reason why is stated that this constrains the distribution of the weights of the output to have a standard deviation of 1.</p>
</blockquote>
<p>My question is why don't we do the same after multiplying to <span class=""math-container"">$V$</span>(values) for the same reason?</p>
","nlp"
"121200","Passing target text to gpt2 and T5 for fine tuning to learn text generation task","2023-04-27 20:33:02","121201","0","266","<nlp><language-model><gpt><huggingface><t5>","<p>I have text with each line in following format:</p>
<pre><code>&lt;text-1&gt; some text-1 &lt;text-2&gt; some text-2 &lt;text-3&gt; some text-3
</code></pre>
<p>I want fine tune model to learn generate <code>some text-3</code> after reading <code>some text-1</code> and <code>some text-2</code>. In GPT2 and T5 text generation tutorials, we do specify <code>input-ids</code> for target text i.e. labels, but for GPT2 we dont.</p>
<p>For example in <a href=""https://medium.com/nlplanet/a-full-guide-to-finetuning-t5-for-text2text-and-building-a-demo-with-streamlit-c72009631887"" rel=""nofollow noreferrer"">this T5 text generation tutorial</a>, we can find line:</p>
<pre><code>model_inputs[&quot;labels&quot;] = labels[&quot;input_ids&quot;]
</code></pre>
<p>But I could not find any such line in these GPT2 text generation examples:</p>
<ul>
<li><p><a href=""https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets-demo.ipynb#scrollTo=6O-8Kr_m8AHE"" rel=""nofollow noreferrer"">huggingtweets demo</a>,</p>
</li>
<li><p><a href=""https://colab.research.google.com/github/AlekseyKorshuk/huggingartists/blob/master/huggingartists-demo.ipynb#scrollTo=L8kjz49JEa-5"" rel=""nofollow noreferrer"">huggingartists demo</a></p>
</li>
<li><p><a href=""https://towardsdatascience.com/how-to-fine-tune-gpt-2-for-text-generation-ae2ea53bc272"" rel=""nofollow noreferrer"">Finetune GPT2 for text generation</a></p>
</li>
</ul>
","nlp"
"121163","How does softmax work for vectors?","2023-04-26 15:40:26","","0","170","<nlp><word-embeddings><word2vec><softmax>","<p>In skipgram we predict the context words. That is the output layer before applying the softmax function is a number <span class=""math-container"">$V$</span> of words, where <span class=""math-container"">$V$</span> is the dictionary size. But each word is represented as a vector. So we have <span class=""math-container"">$V$</span> vectors in the output layer. And now we want to apply softmax to those vectors to get a vector of dimensionality <span class=""math-container"">$V$</span>, where each component represents the probability of a word appearing with an input word. But how do we apply the softmax function to vectors? By definition the softmax function takes as input a single vector, but we have <span class=""math-container"">$V$</span> vectors each of a chosen dimensionality <span class=""math-container"">$N$</span>.</p>
<p><a href=""https://i.sstatic.net/rbzgg.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/rbzgg.png"" alt=""enter image description here"" /></a></p>
","nlp"
"121136","How chatGPT can remember previous context?","2023-04-25 11:25:49","","0","278","<nlp>","<p>I thought I had some knowledge about NLP based on transformer, but after seeing ChatGPT, I felt like I didn't know much.
There is clearly a limit to the size of text that can be processed at once with transformer.
However, when conversing with ChatGPT, the length of the text becomes incredibly long, exceeding the length that ChatGPT can process at once.
Nevertheless, ChatGPT seems to remember the content of previous conversations.
How can the context of previous conversations, which took place a long time ago, be used in generating current sentences?</p>
<p>Here are some hypotheses I came up with.
One is to embed each sentence to correspond to a single token size and place them at the beginning of the sequence that the transformer processes, like this: &lt;conversation 1 embedding&gt; &lt;conversation 2 embedding&gt; &lt;&quot;this&quot;&gt; &lt;&quot;is&quot;&gt; &lt;&quot;the&quot;&gt; &lt;&quot;third&quot;&gt; &lt;&quot;conversation&quot;&gt;.
Alternatively, it may be possible to add context in a simple way, such as applying positional encoding to the transformer.
However, I have not been able to find out how ChatGPT actually processes long conversations.</p>
<p>If anyone could shed light on this for me, I would greatly appreciate it. Thank you!</p>
","nlp"
"121121","Bert model for document sentiment classification","2023-04-24 16:43:35","121122","0","68","<deep-learning><nlp><transformer><bert><sentiment-analysis>","<p>I am trying to fine-tune a Bert model for sentiment analysis. Instead of one sentence, my inputs are documents (including several sentences) and I am not removing dots. I was wondering if is it okay to use just the embedding of the first token in such cases. If not, what should I do?</p>
","nlp"
"121045","Is there such thing as dataset imrovement?","2023-04-20 05:15:11","","1","33","<nlp><model-evaluations><explainable-ai>","<p>I know that we can use explained machine learning to find why a model chose a certain classification.</p>
<p>I wonder if there is a way I can find which features are going to improve my current model.</p>
<p>I will explain what I mean by this.</p>
<p>Case:
NLP classification of sports, there is a paragraph talking about Ronaldo scores against Uruguay...</p>
<p>Is there a method that can ask which Ronaldo you mean (Ronaldo de Lime the Brazilian player or Cristiano Ronaldo the Portuguese)?</p>
<p>so the model can get a higher accuracy result to classify the paragraph about Brazilian Team or about Portugal Team?</p>
","nlp"
"121034","SpaCy textcat_multilabel, how to supply data","2023-04-19 11:59:12","","3","328","<nlp><spacy>","<p>Just as I was warned, the documentation in SpaCy is a bit difficult to read. I don't have a software-engineer / CS background, so I'm really struggling with this.</p>
<p>I would like to use SpaCy's textcat_multilabel (tm). I have figured out how to setup the config, and how to train the data w/o CLI. I also <em>know</em> that tm wants SpaCy's own binary format for the training data (and dev == validation data). I know that I'm supposed to use DocBin, but I do not know how to do this. I couldn't find anything <a href=""https://spacy.io/api/textcategorizer"" rel=""nofollow noreferrer"">here</a> that tells me how the classifier even wants the data, how to tell the classifier which are features and which are the labels, how to supply the labels (since it's multilabel, not so obvious to me, the few tutorials I saw were multiclass not multilabel).</p>
<p>If someone can point out to me how to do this it would be wonderful. Or even just point me towards the right direction.</p>
","nlp"
"121015","What is purpose of stacking N=6 blocks of encoder and decoder in transformer?","2023-04-18 20:15:40","","2","1531","<machine-learning><nlp><transformer><language-model>","<p>I was trying to understand transformer architecture from &quot;Attention is all you need&quot; paper.</p>
<p><a href=""https://i.sstatic.net/7p5lu.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/7p5lu.png"" alt=""enter image description here"" /></a></p>
<p>What is purpose of stacking <span class=""math-container"">$N=6$</span> blocks of encoder and decoder? Does higher blocks represent longer phrases and learns what longer phrases attend to? While bottommost block represent single word and its attention; something like how first layer of CNN represent pixel and deeper layers represent edges and further deeper layers represents shapes (like nose, hand etc.)?</p>
","nlp"
"121014","What does it exactly mean by ""different representation subspaces"" in transformer?","2023-04-18 20:14:34","","1","146","<machine-learning><nlp><transformer><language-model>","<p>I was trying to understand transformer architecture from &quot;Attention is all you need&quot; paper.</p>
<p>The paper says:</p>
<blockquote>
<p>Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.</p>
</blockquote>
<p>What does it exactly mean by &quot;different representation subspaces&quot;. Can you give intuitive example in terms of natural language conversation example. For example, in sentence &quot;Jane went to Africa during summer&quot;, query matrix <span class=""math-container"">$Q$</span> correspoding to word &quot;Africa&quot; can comprise of different queries &quot;Who went to Africa?&quot;, &quot;When went to Africa?&quot;. What are &quot;different representation subspaces&quot; here? Or with any other example of your choice?</p>
","nlp"
"121013","How K and V are extracted from encoder output in transformer?","2023-04-18 20:12:33","","1","290","<machine-learning><nlp><transformer><language-model>","<p>I was trying to understand transformer architecture from &quot;Attention is all you need&quot; paper.  The paper shows following transformer architecture:</p>
<p><a href=""https://i.sstatic.net/dtJ7Y.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/dtJ7Y.png"" alt=""enter image description here"" /></a></p>
<p>How <span class=""math-container"">$K$</span> and <span class=""math-container"">$V$</span> is extracted from <span class=""math-container"">$512$</span> dimensional encoder output (which is then fed to second multi head attention in decoder)?</p>
","nlp"
"121012","Understanding dimensions of vectors at various places in transformer architecture","2023-04-18 20:08:22","","1","809","<machine-learning><nlp><transformer><language-model>","<p>I was trying to understand transformer architecture from &quot;Attention is all you need&quot; paper.  It says following regarding dimensions of different vectors:</p>
<blockquote>
<ul>
<li>The input consists of queries and keys of dimension <span class=""math-container"">$d_k$</span>, and values of dimension <span class=""math-container"">$d_v$</span>.</li>
<li><span class=""math-container"">$MultiHead(Q, K, V ) = Concat(head_1, ..., head_h) W^O$</span> where <span class=""math-container"">$head_i = Attention(QW_i^Q,KW^K_i,VW^V_i)$</span><br />
where <span class=""math-container"">$W_i^Q\in\mathbb{R}^{d_{model}\times d_k}$</span>, <span class=""math-container"">$W_i^K\in\mathbb{R}^{d_{model}\times d_k}$</span>, <span class=""math-container"">$W_i^V\in\mathbb{R}^{d_{model}\times d_v}$</span>, <span class=""math-container"">$W^O\in\mathbb{R}^{hd_{v}\times d_{model}}$</span></li>
<li><span class=""math-container"">$h=8$</span> parallel attention layers or heads</li>
<li><span class=""math-container"">$d_k=d_v=d_{model}/h=64$</span></li>
</ul>
</blockquote>
<p>From these I figured out dimensions of vectors at different position in the transformers model as follows (in red colored text):</p>
<p><a href=""https://i.sstatic.net/3sAvB.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/3sAvB.png"" alt=""enter image description here"" /></a></p>
<p>I have following doubts:</p>
<p>Is dimension of <span class=""math-container"">$K$</span> (vector of multiple/all keys that current word-query needs to attend to)  <span class=""math-container"">$=d_k$</span>? Or <span class=""math-container"">$d_k$</span> is just for single key? If for single key, then what is the dimension for <span class=""math-container"">$K$</span>? Same is the doubt with <span class=""math-container"">$Q$</span> and <span class=""math-container"">$d_q$</span>. I feel <span class=""math-container"">$Q$</span> is set of all queries that can apply to single word. <span class=""math-container"">$K$</span> is the set of all keys that single word can attend to. If that is the case, then dimensions of <span class=""math-container"">$Q$</span> must be <span class=""math-container"">$d_q\times\text{number of queries to consider}$</span> and <span class=""math-container"">$K$</span> must be <span class=""math-container"">$d_k\times\text{number of keys to attend for each word-query}$</span> But then what is this number of queries and keys?</p>
","nlp"
"120997","Which model to use that can distinguish between names with the same words?","2023-04-18 09:42:04","","0","163","<machine-learning><deep-learning><nlp><transformer><semantic-similarity>","<p>For my task, I need a model that can distinguish between job titles that contain the same words. BERT model &quot;msmarco-MiniLM-L-12-v3&quot; shows high cosine similarity for positions: &quot;Data customer&quot; and &quot;Data provider&quot;. The meaning of these two positions are very different and I need my model to show a low cosine similarity for these two positions.</p>
<p>However, in this case cosine similarity must be high: &quot;Data customer&quot; &quot;Data consumer&quot;.</p>
<p>Which model should I use? Should I train classifier instead of nlu model? Why ChatGPT understands the difference between those texts, but BERT based models show high cosine similarity?</p>
","nlp"
"120952","Where can I see a summary of tools and techniques for most updated transformer developments?","2023-04-16 05:13:39","","0","20","<nlp><transformer><gpt>","<p>Since the invention of chatGPT, there are many tools and techniques and variants invented ever since. I want to keep track of these developments and tools but I find no avail. May I know which resources/sites that showcases all these? Thanks in advance.</p>
","nlp"
"120937","Dealing with rich vocabulary and a low average frequency of words in NLP","2023-04-15 15:58:58","","0","71","<nlp><sentiment-analysis><tfidf>","<p>What is the best way to deal with a dataset that has a rich vocabulary and a low average frequency of words that is showing low validation accuracy?
While reading online I saw many people recommending removing stop-words and stemming while others suggested using a TF-IDF vectorizer.</p>
<p>Which solution will have the greater impact?</p>
","nlp"
"120910","Below text-classification model gives accuracy of 0.77 only on one dataset and 0.99 on spam-ham dataset? What should I do to increase with my dataset?","2023-04-14 09:19:10","","1","89","<nlp><rnn><bert><text-classification><attention-mechanism>","<pre class=""lang-py prettyprint-override""><code>from keras.models import Model
from keras.layers import Input, Dense, Dropout, Embedding, Conv1D, MaxPooling1D, Flatten, Bidirectional, GRU, Concatenate, Lambda, Multiply, Permute, RepeatVector,dot



text_input = tf.keras.layers.Input(shape=(), dtype=tf.string)
encoder_inputs = preprocessor(text_input)
outputs = encoder(encoder_inputs)
#pooled_output = outputs[&quot;pooled_output&quot;]      # [batch_size, 768].

sequence_output = outputs[&quot;sequence_output&quot;]
dropout_layer = Dropout(0.3)(sequence_output)  
# add BiGRU layer with attention mechanism
bigru_output= Bidirectional(GRU(units=64,activation='tanh',return_sequences=True))(dropout_layer)


# Add a CNN layer
conv_layer1 = Conv1D(filters=128, kernel_size=2, activation='relu',padding=&quot;same&quot;)(bigru_output)
conv_layer2 = Conv1D(filters=128, kernel_size=3, activation='relu',padding=&quot;same&quot;)(bigru_output)
conv_layer3 = Conv1D(filters=128, kernel_size=4, activation='relu',padding=&quot;same&quot;)(bigru_output)
# max_pool_layer = MaxPooling1D(pool_size=2)(conv_layer)
conv_layer= tf.keras.layers.Concatenate()([conv_layer1,conv_layer2,conv_layer3])
# Add a dropout layer after the CNN layer
conv_layer = Dropout(0.3)(conv_layer)
# Map each cnn output vector to a unique context vector using a Dense layer
context_vectors = Dense(128, activation='tanh')(conv_layer)

# Define a function to compute attention scores
def compute_attention_score(context_vector, query_vector):
    &quot;&quot;&quot;
    Computes the attention score between a context vector and a query vector.
    &quot;&quot;&quot;
    score = dot([context_vector, query_vector], axes=[1, 1])
    score = Activation('softmax')(score)
    return score

# Compute attention scores for each context vector using a lambda function
attention_scores = Lambda(lambda x: compute_attention_score(x[0], x[1]))([context_vectors, bigru_output])

# Compute the weighted sum of the context vectors using the attention scores
weighted_context_vectors = Lambda(lambda x: dot([x[0], x[1]], axes=[1, 1]))([attention_scores, context_vectors])

# Concatenate the weighted context vectors with the BiGRU output vector
attention_output = Lambda(lambda x: tf.concat([x[0], x[1]], axis=-1))([bigru_output, weighted_context_vectors])


# Add max pooling layer
max_pool_layer = MaxPooling1D(pool_size=2)(attention_output)

# Flatten and add dense layer for final output
flatten_layer = Flatten()(attention_output)
output_layer = Dense(units=1, activation='sigmoid')(flatten_layer)


# define the model
model = Model(name=&quot;BBRCA&quot;,inputs=text_input, outputs=output_layer)
</code></pre>
","nlp"
"120901","On which texts should TfidfVectorizer be fitted when using TF-IDF cosine for text similarity?","2023-04-14 02:46:15","","0","118","<nlp><scikit-learn><tfidf><cosine-distance><semantic-similarity>","<p>I wonder on which texts should TfidfVectorizer be fitted when using TF-IDF cosine for text similarity. Should TfidfVectorizer be fitted on the texts that are analyzed for text similarity, or some other texts (if so, which one)?</p>
<hr />
<p>I follow <a href=""https://stackoverflow.com/users/163740/ogrisel"">ogrisel</a>'s <a href=""https://stackoverflow.com/a/12128777/395857"">code</a> to compute text similarity via TF-IDF cosine, which fits the <code>TfidfVectorizer</code> on the texts that are analyzed for text similarity (<code>fetch_20newsgroups()</code> in that example):</p>
<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.datasets import fetch_20newsgroups
twenty = fetch_20newsgroups()
tfidf = TfidfVectorizer().fit_transform(twenty.data)
from sklearn.metrics.pairwise import linear_kernel
cosine_similarities = linear_kernel(tfidf[0], tfidf[1]).flatten()
print(cosine_similarities) # print TF-IDF cosine similarity between text 1 and 2.
</code></pre>
","nlp"
"120870","Measuring sentiment using a dictionary-based model","2023-04-12 18:01:46","120875","1","38","<machine-learning><python><deep-learning><neural-network><nlp>","<p>I have a dataset of 40K reddit posts in Italian, and I have a sentiment-based dictionary of 9K unique words and phrases, which classifies words into positive or negative.
I would like to measure sentient per reddit post across time and I noticed that are several <a href=""https://stackoverflow.com/questions/33543446/what-is-the-formula-of-sentiment-calculation"">methods</a> to compute sentiment per post. I am currently using the following equation:</p>
<p><a href=""https://i.sstatic.net/Ag1PH.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Ag1PH.png"" alt=""enter image description here"" /></a></p>
<p>I wonder if it has any obvious downsides?
The main advantage of course is interpretability, which straightforward with this method as it produces a score with  a theoretical scale between −100 points (extremely negative) to 100 points (extremely positive).</p>
","nlp"
"120796","How can I force tortoise-tts to use the same voice throughout a generate speech file?","2023-04-08 00:35:27","","0","866","<nlp><voice><text-to-speech>","<p>From time to time, a sentence or a fragment of a sentence somewhere in the text will be read with a different voice.</p>
<p>Example with this <a href=""https://github.com/neonbjb/tortoise-tts/files/11179986/text.txt"" rel=""nofollow noreferrer"">text.txt</a>:</p>
<blockquote>
<p>What makes jee pee tee-3+ so advanced and powerful? Transformer based Large Language Models like Open AI’s jee pee tee have rapidly advanced in quality and capability. Transformers use an attention weight for each word in the input text, regardless of its position, to better consider long-term dependencies in the text and improve understanding and generation of natural language. They also allow for parallel processing which makes them faster to train and less computationally expensive, enabling the use of larger models with more accurate results. jee pee tee-3 and other similar models have a huge number of parameters (jee pee tee-3 has 175 billion params) giving them significant learning capacity. They are trained on all the text content from the public internet (jee pee tee-3 used 570 gigabytes of compressed text) to predict the probability distribution of the next word given prior words in the text. This simple objective can be performed on raw text without human labeling. When done at scale, it is surprising effective at teaching models to &quot;understand&quot; and generate natural language text.</p>
</blockquote>
<p>I used the command:</p>
<blockquote>
<p>/users/franck/workspace/tts//tortoise-tts$ python tortoise/read.py --textfile text.txt --voice freeman,train_empire --preset high_quality --candidates 1 --output_path results/longtexts</p>
</blockquote>
<p>The audio file with the  <code>train_empire</code> voice was fine (<a href=""https://github.com/neonbjb/tortoise-tts/files/11179970/train_empire.zip"" rel=""nofollow noreferrer"">train_empire.zip</a>), but the audio file with the  <code>freeman</code> voice had 1 fragment of a sentence with a female voice (&quot;<em>jee pee tee-3 has 175 billion params) giving them significant learning capacity</em>&quot;): <a href=""https://github.com/neonbjb/tortoise-tts/files/11179942/selected.segment.where.the.voice.is.messed.up.zip"" rel=""nofollow noreferrer"">selected segment where the voice is messed up.zip</a> (for the entire speech: <a href=""https://github.com/neonbjb/tortoise-tts/files/11179943/full.speech.zip"" rel=""nofollow noreferrer"">full speech.zip</a>).</p>
<p>How can I force <a href=""https://github.com/neonbjb/tortoise-tts"" rel=""nofollow noreferrer"">tortoise-tts</a> to use the same voice throughout a generate speech file?</p>
","nlp"
"120781","Dynamic batching and padding batches for NLP in deep learning libraries","2023-04-07 12:05:10","","2","2731","<nlp><pytorch><huggingface><dynamic-batching>","<p>This is the usual way we train modern deep learning models for NLP, e.g. with Huggingface libraries where we have a fix length for the input no. of tokens/subwoords unit. <a href=""https://huggingface.co/docs/transformers/pad_truncation"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/pad_truncation</a></p>
<p>In the follow example, we have 5 sentences of various length and all of them are padded to the max length set at 1024.</p>
<p>The first part of my question is with regards to GPU memory usage and pad, when we train a model with batches of data with padded inputs, <strong>would the padded tokens hog up the GPU RAM</strong>? Even if the model don't compute them since they will return zeros, it's still rather wasteful.</p>
<p><strong>Or does PyTorch / Tensorflow or other lower-level tensor libraries reoptimize the batch such that the pads don't take up memory? If so, any pointers to code/docs on this?</strong></p>
<p><a href=""https://i.sstatic.net/UOo7qm.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/UOo7qm.png"" alt=""enter image description here"" /></a></p>
<p>There are instances where the batches can be ordered in a way to arrange batches with similar length to go together, esp. at the start of model training, e.g. <a href=""https://discuss.huggingface.co/t/are-dynamic-padding-and-smart-batching-in-the-library/10404/15"" rel=""nofollow noreferrer"">https://discuss.huggingface.co/t/are-dynamic-padding-and-smart-batching-in-the-library/10404/15</a></p>
<p>Instead of doing padding, are there existing code for some sort of dynamic batching without sorting, <strong>is there a way to keep an offset of all the input sentences EOS token and pack the batch into something that looks like this</strong>:</p>
<p><a href=""https://i.sstatic.net/04ggX.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/04ggX.png"" alt=""enter image description here"" /></a></p>
<p><strong>Are there examples of the above batch packing in other deep learning libraries? Or in native Pytorch/Tensorflow/JAX?</strong></p>
","nlp"
"120775","How can I get tortoise-tts to pronounce acronyms correctly?","2023-04-07 05:36:16","","1","130","<nlp><voice><text-to-speech>","<p>I'm trying to get <a href=""https://github.com/neonbjb/tortoise-tts"" rel=""nofollow noreferrer"">tortoise-tts</a> to pronounce acronyms correctly. Example of text that I'd like tortoise-tts to generate an audio file for: <code>OpenAI ChatGPT is a new language model</code>.</p>
<p>The audio file generated by tortoise-tts is: <code>OpenAI Chat is a new language model</code> (GPT is missing from the <a href=""https://github.com/neonbjb/tortoise-tts/files/11176160/ChatGPT.became.Chat.zip"" rel=""nofollow noreferrer"">audio file</a>).</p>
<p>I can replace <code>ChatGPT</code> with <code>Chat gee pee tee</code> but I've had a case where the <code>jee pee tee</code> changes the tone of the audio file.</p>
<p>Questions:</p>
<ol>
<li>Is replacing <code>ChatGPT</code> with <code>Chat gee pee tee</code> the most optimal solution?</li>
<li>If it is the most optimal solution, is there any convenient script to replace acronyms with their pronounced version (GPT-&gt;jee pee tee)?</li>
<li>If it is not the most optimal solution, what is the most optimal solution?</li>
</ol>
","nlp"
"120764","How does an LLM ""parameter"" relate to a ""weight"" in a neural network?","2023-04-06 21:53:54","120766","16","15638","<machine-learning><nlp><terminology><gpt>","<p>I keep reading about how the latest and greatest LLMs have billions of parameters. As someone who is more familiar with standard neural nets but is trying to better understand LLMs, I'm curious if a LLM parameter is the same as a NN weight i.e. is it basically a number that starts as a random coefficient and is adjusted in a way that reduces loss as the model learns? If so, why do so many researches working in the LLM space refer to these as parameters instead of just calling them weights?</p>
","nlp"
"120752","PyTorch mat1 and mat2 shapes cannot be multiplied (100x200 and 100x9922)","2023-04-06 11:15:08","","0","512","<python><neural-network><nlp><lstm><pytorch>","<p>I am trying to make a BiLSTM language model and am having some issues.</p>
<p>Model</p>
<pre><code>class BiLSTM(nn.Module):
  def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, tie_weights):
    super().__init__()

    self.num_layers = num_layers
    self.hidden_dim = hidden_dim
    self.embedding_dim = embedding_dim

    self.embedding = nn.Embedding(vocab_size, embedding_dim)
    self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, 
                        dropout=dropout_rate, batch_first=True, bidirectional=True)
    self.dropout = nn.Dropout(dropout_rate)
    self.linear = nn.Linear(hidden_dim*2, vocab_size)

    if tie_weights:
      # Embedding and hidden layer need to be same size for weight tying
      assert embedding_dim == hidden_dim, 'Cannot tie weights, check dimensions'
      self.linear.weight = self.embedding.weight

    self.init_weights()

    self.hidden = None
    self.cell = None

  def forward(self, x, hidden):
    hidden_0 = torch.zeros(2*num_layers, batch_size, self.hidden_dim).to(device)
    cell_0 = torch.zeros(2*num_layers, batch_size, self.hidden_dim).to(device)
    output  = self.embedding(x)
    output, (h, c) = self.lstm(output, (hidden_0, cell_0))
    output = self.dropout(output)
    output = self.linear(output)
    return output, (h, c)

  def init_weights(self):
    init_range_emb = 0.1
    init_range_other = 1/math.sqrt(self.hidden_dim)
    self.embedding.weight.data.uniform_(-init_range_emb, init_range_emb)
    self.linear.weight.data.uniform_(-init_range_other, init_range_other)
    self.linear.bias.data.zero_()


  def init_hidden(self, batch_size):
        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_dim)
        cell = torch.zeros(self.num_layers, batch_size, self.hidden_dim)
        return hidden, cell
  
  def detach_hidden(self, hidden):
        hidden, cell = hidden
        hidden = hidden.detach()
        cell = cell.detach()
        return hidden, cell
</code></pre>
<p>Model params</p>
<pre><code>vocab_size = len(vocab)
embedding_dim = 100
hidden_dim = 100
num_layers = 5
dropout_rate = 0.4
tie_weights = True
model = BiLSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, tie_weights)
model.to(device)
</code></pre>
<p>Training:</p>
<pre><code>import copy
import time

criterion = nn.CrossEntropyLoss()
lr = 20.0  # learning rate
optimizer = torch.optim.SGD(model.parameters(), lr=lr)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)

def train(model: nn.Module) -&gt; None:
    model.train()  # turn on train mode
    total_loss = 0.
    log_interval = 200
    start_time = time.time()
    
    hidden = model.init_hidden(batch_size)
    
    num_batches = len(train_data) // bptt
    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):
        hidden = model.detach_hidden(hidden)
        data, targets = get_batch(train_data, i)
        seq_len = data.size(0)
        output, hidden = model(data, hidden)
        loss = criterion(output.view(-1, vocab_size), targets)

        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)
        optimizer.step()

        total_loss += loss.item()
        
        if batch % log_interval == 0 and batch &gt; 0:
            lr = scheduler.get_last_lr()[0]
            ms_per_batch = (time.time() - start_time) * 1000 / log_interval
            cur_loss = total_loss / log_interval
            ppl = math.exp(cur_loss)
            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '
                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '
                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')
            total_loss = 0
            start_time = time.time()
</code></pre>
<p>Eval:</p>
<pre><code>def evaluate(model: nn.Module, eval_data: Tensor) -&gt; float:
    model.eval()  # turn on evaluation mode
    total_loss = 0.
    with torch.no_grad():
        for i in range(0, eval_data.size(0) - 1, bptt):
            data, targets = get_batch(eval_data, i)
            seq_len = data.size(0)
            output = model(data)
            output_flat = output.view(-1, vocab_size)
            total_loss += seq_len * criterion(output_flat, targets).item()
    return total_loss / (len(eval_data) - 1)
</code></pre>
<p>Training block:</p>
<pre><code>best_val_loss = float('inf')
epochs = 50
best_model = None

for epoch in range(1, epochs + 1):
    epoch_start_time = time.time()
    train(model)
    val_loss = evaluate(model, val_data)
    val_ppl = math.exp(val_loss)
    elapsed = time.time() - epoch_start_time
    print('-' * 89)
    print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '
          f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')
    print('-' * 89)

    if val_loss &lt; best_val_loss:
        best_val_loss = val_loss
        best_model = copy.deepcopy(model)

    scheduler.step()
</code></pre>
<p>My problem is I am getting the following error.</p>
<pre><code>RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-345-453c3f2a9cad&gt; in &lt;cell line: 5&gt;()
      5 for epoch in range(1, epochs + 1):
      6     epoch_start_time = time.time()
----&gt; 7     train(model)
      8     val_loss = evaluate(model, val_data)
      9     val_ppl = math.exp(val_loss)

4 frames
&lt;ipython-input-343-16d7ac1074e2&gt; in train(model)
     20         data, targets = get_batch(train_data, i)
     21         seq_len = data.size(0)
---&gt; 22         output, hidden = model(data, hidden)
     23         loss = criterion(output.view(-1, vocab_size), targets)
     24 

/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py in _call_impl(self, *args, **kwargs)
   1499                 or _global_backward_pre_hooks or _global_backward_hooks
   1500                 or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1501             return forward_call(*args, **kwargs)
   1502         # Do not call functions when jit is used
   1503         full_backward_hooks, non_full_backward_hooks = [], []

&lt;ipython-input-340-c83c2df4172c&gt; in forward(self, x, hidden)
     29     output, (h, c) = self.lstm(output, (hidden_0, cell_0))
     30     output = self.dropout(output)
---&gt; 31     output = self.linear(output)
     32     return output, (h, c)
     33 

/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py in _call_impl(self, *args, **kwargs)
   1499                 or _global_backward_pre_hooks or _global_backward_hooks
   1500                 or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1501             return forward_call(*args, **kwargs)
   1502         # Do not call functions when jit is used
   1503         full_backward_hooks, non_full_backward_hooks = [], []

/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py in forward(self, input)
    112 
    113     def forward(self, input: Tensor) -&gt; Tensor:
--&gt; 114         return F.linear(input, self.weight, self.bias)
    115 
    116     def extra_repr(self) -&gt; str:

RuntimeError: mat1 and mat2 shapes cannot be multiplied (100x200 and 100x9922)
</code></pre>
<p>The model dimensions seem to be the problem but I don't know how to adjust to the model to solve the issue.</p>
<p>Any help is appreciated,
Thanks.</p>
<p>EDIT
Get batch returns</p>
<pre><code>(tensor([[9893,    0,  171,    1,    1,  180,    4,  795,   13,   26],
        [9894, 1192,   58,    0, 9325,  190,  140,    4,    2,    0],
        [9896,    2,  109, 5437,   20,   12, 1022,    5,    6,    4],
        [9897,    2,   52, 1446,    5,  474,    9,   38,  192,    5],
        [9898,    3, 2850,   30, 3489,  655,   10,  212, 3915,  300],
        [9902,    1, 2150,    1, 1791, 1534,  141,   12,    4, 2853],
        [9903, 2526,    9,  161,   40,   17,    7,    5,  339,    4],
        [9904,   42,   10,   14,  775, 1931,  151, 1786, 1281,  893],
        [9905,   32, 1574,    2, 2088,  569,  154,   19,  799, 3353],
        [9906,  222,   43,    9,  342,  499,  290,    1,    0, 9253]],
       device='cuda:0'), tensor([9894, 1192,   58,    0, 9325,  190,  140,    4,    2,    0, 9896,    2,
         109, 5437,   20,   12, 1022,    5,    6,    4, 9897,    2,   52, 1446,
           5,  474,    9,   38,  192,    5, 9898,    3, 2850,   30, 3489,  655,
          10,  212, 3915,  300, 9902,    1, 2150,    1, 1791, 1534,  141,   12,
           4, 2853, 9903, 2526,    9,  161,   40,   17,    7,    5,  339,    4,
        9904,   42,   10,   14,  775, 1931,  151, 1786, 1281,  893, 9905,   32,
        1574,    2, 2088,  569,  154,   19,  799, 3353, 9906,  222,   43,    9,
         342,  499,  290,    1,    0, 9253, 9908,    6,  388,   31,   28,    0,
         559,  835,    7,   30], device='cuda:0'))
</code></pre>
","nlp"
"120745","In ChatGPT, The difference of using reward to guide policy vs using the dataset of reward to train policy?","2023-04-06 00:42:41","","1","21","<deep-learning><nlp><gpt>","<p>In ChatGPT, What are the differences of <strong>using reward to guide policy</strong> vs <strong>using the dataset of reward to train policy</strong>?</p>
","nlp"
"120726","What should the numerical values of the <startofsentence> and <endofsentence> token vectors be?","2023-04-05 11:53:38","120979","1","154","<nlp><word-embeddings><transformer><gpt><tokenization>","<p>I'm trying to build GPT2 from scratch. I understand how to convert each word in a sentence to its respective token index and each token is then converted to its respective word embedding vector. I also understand there needs to be a fixed length for each input vector e.g. the max length of all sentences input into the transformer are 50 tokens, and for all sentences shorter than that padding token vectors consisting of nothing but zeroes fill the space where the additional word vectors would be.</p>
<p>I get that each input vector needs to have a start token at the beginning of the input vector, as well as a stop token after the last word and before the padding vectors. The integer values corresponding to the start and stop token indexes are somewhat arbitrary, but I still don't understand what the actual values of the start and stop token embeddings should be. Should they just also be vectors of zeroes? Are these values also arbitrary?</p>
","nlp"
"120642","Doubt in ELMO, BERT, Word2Vec","2023-04-02 09:12:27","","0","229","<machine-learning><nlp><lstm><word-embeddings><bert>","<p>I read an answer on Quora where a NLP Practioner stated that using ELMO and BERT embeddings as input to LSTM or some RNN will defeat the purpose of ELMo and BERT. I am not sure I agree with the above statement.</p>
<p>Normally we pass words to LSTM to obtain context specific represtations and I am aware of this. But, we pass word2vec instead of one-hot because the contextual representation after LSTM processed it will be better. Similarly common sense states that, if we give ELMO or BERT word embeddings to LSTM, It should output more context rich words than word2vec. Aint I right?</p>
<p>I am aware that once the context is obtained we can fine-tune it straight away for some downstream tasks. But why not use it this way in which we pass the context embeddings of ELMo and BERT to an LSTM ?</p>
<p>Doubt #2 :</p>
<p>I saw a post where the author used ELMo Embeddings with average vectors for each word for logistic regression and tree based models. While this worked for them, In general, It doesn't make sense ? because, In Logistic regression, Each parameter is fixed to an input. Like, Theta1*X1. So if X1 is of different word every time, It should ideally be more confusing to the model to fix that parameter compared to TFIDF where we have a fixed index for each word ?</p>
","nlp"
"120615","Aspect-Based Sentiment Analysis with Bert and Pytorch","2023-03-31 20:38:28","","0","105","<deep-learning><nlp><pytorch><text-classification><sentiment-analysis>","<p>I have a dataset of online reviews (X) with their corresponding topics (topic1 to topic5) and each topic can have 5 values (fined-grained sentiment score from 1 to 5). So, I have one X and 5 Y columns. I was wondering how can I use Bert and Pytorch to train a model which gets textual data and make an output like ([2,3,1,5,4] meaning topic1: 2 topic2: 3 and so on). Currently, my solution is like this but my metrics are not good. I really appreciate it if you help me with dealing with the imbalance situation since for every topic, scores 1 and 2 are really small.</p>
<pre><code>class SentimentClassifier(nn.Module):
  def __init__(self, n_classes):
    super(SentimentClassifier, self).__init__()
    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)
    self.drop = nn.Dropout(p=0.1)
    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)
  def forward(self, input_ids, attention_mask):
    _, pooled_output = self.bert(
      input_ids=input_ids,
      attention_mask=attention_mask, return_dict=False
    )
    output = self.drop(pooled_output)
    output_1 = self.out(output)
    output_2 = self.out(output)
    output_3 = self.out(output)
    output_4 = self.out(output)
    output_5 = self.out(output)
    return output_1 ,output_2 ,output_3 ,output_4 ,output_5 

torch.cuda.manual_seed(3447)
def train_epoch(
  model,
  data_loader,
  loss_fn,
  optimizer,
  device,
  scheduler
):
  model = model.train()
  losses = []
  acc = []
  f1 = []
  for d in data_loader:
    input_ids = d[&quot;input_ids&quot;].to(device)
    attention_mask = d[&quot;attention_mask&quot;].to(device)
    target_1 = d[&quot;targets&quot;][:,0].to(device)
    target_2 = d[&quot;targets&quot;][:,1].to(device)
    target_3 = d[&quot;targets&quot;][:,2].to(device)
    target_4 = d[&quot;targets&quot;][:,3].to(device)
    target_5 = d[&quot;targets&quot;][:,4].to(device)
    output_1 ,output_2 ,output_3 ,output_4 ,output_5 = model(
      input_ids=input_ids,
      attention_mask=attention_mask
    )
    preds_1 = torch.argmax(output_1 , dim=1)
    preds_2 = torch.argmax(output_2 , dim=1)
    preds_3 = torch.argmax(output_3 , dim=1)
    preds_4 = torch.argmax(output_4 , dim=1)
    preds_5 = torch.argmax(output_5 , dim=1)

    loss_1 = loss_fn(output_1 , target_1 -1)
    loss_2 = loss_fn(output_2 , target_2 -1)
    loss_3 = loss_fn(output_3 , target_3 -1)
    loss_4 = loss_fn(output_4 , target_4 -1)
    loss_5 = loss_fn(output_5 , target_5 -1)
    loss = loss_1 + loss_2 + loss_3 + loss_4 + loss_5 

    acc_1 = accuracy_score(preds_1 ,target_1 -1).item()
    f1_1 = f1_score(preds_1 ,target_1 -1).item()
    ....(for other Ys)


    
    acc_total = (acc_1 + acc_2 + acc_3 + acc_4 + acc_5) / 5
    f1_total = (f1_1 + f1_2 + f1_3 + f1_4 + f1_5) / 5

    losses.append(loss.item())
    acc.append(acc_total)
    f1.append(f1_total)

    loss.backward()

    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
    optimizer.step()
    scheduler.step()
    optimizer.zero_grad()
    gc.collect()
    torch.cuda.empty_cache()
  return np.mean(losses), np.mean(acc), np.mean(f1) 
</code></pre>
<p>Do you think my approach is OK? I see the problem as a multi-output classification (5 multi-class problems), so I used a 5-head output for the deep learning architecture. I really appreciate any help, comment, and resource improving the model. Thank you so much</p>
","nlp"
"120601","Do transformers (e.g. BERT) have an unlimited input size?","2023-03-31 09:47:49","120602","8","2641","<machine-learning><nlp><transformer><bert><hyperparameter>","<p>There are various sources on the internet that claim that BERT has a fixed input size of 512 tokens (e.g. <a href=""https://datascience.stackexchange.com/q/89684/141432"">this</a>, <a href=""https://stackoverflow.com/q/58636587/9352077"">this</a>, <a href=""https://www.saltdatalabs.com/blog/bert-how-to-handle-long-documents"" rel=""noreferrer"">this</a>, <a href=""https://datascience.stackexchange.com/q/113489/141432"">this</a> ...). This magical number also appears in the BERT paper (<a href=""https://arxiv.org/pdf/1810.04805.pdf"" rel=""noreferrer"">Devlin et al. 2019</a>), the RoBERTa paper (<a href=""https://arxiv.org/pdf/1907.11692.pdf"" rel=""noreferrer"">Liu et al. 2019</a>) and the SpanBERT paper (<a href=""https://www.cs.princeton.edu/%7Edanqic/papers/tacl2020.pdf"" rel=""noreferrer"">Joshi et al. 2020</a>).</p>
<p>The going wisdom has always seemed <em>to me</em> that when NLP transitioned from recurrent models (RNN/LSTM Seq2Seq, Bahdanau ...) to transformers, we traded variable-length input for fixed-length input that required padding for shorter sequences and could not extend beyond 512 tokens (or whatever other magical number you want to assign your model).</p>
<p>However, come to think of it, all the parameters in a transformer (Vaswani et al. 2017) work on a token-by-token basis: the weight matrices in the attention heads and the FFNNs are applied tokenwise, and hence their parameters are independent of the input size. <strong>Am I correct that a transformer (encoder-decoder, BERT, GPT ...) can take in an arbitrary amount of tokens even with fixed parameters, i.e., the amount of parameters it needs to train is independent of the input size?</strong></p>
<p>I understand that memory and/or time will become an issue for large input lengths since attention is O(n²). This is, however, a limitation of our <em>machines</em> and not of our <em>models</em>. Compare this to an LSTM, which can be run on any sequence but compresses its information into a fixed hidden state and hence blurs all information eventually. <em>If</em> the above claim is correct, then I wonder: <strong>What role does input length play during pre-training of a transformer, given infinite time/memory?</strong></p>
<p>Intuitively, the learnt embedding matrix and weights must somehow be different if you were to train with extremely large contexts, and I wonder if this would have a positive or a negative impact. In an LSTM, it has negative impact, but a transformer doesn't have its information bottleneck.</p>
","nlp"
"120504","Can I use LLM to explain codebase?","2023-03-27 00:39:56","120510","2","2437","<nlp><data-mining><word-embeddings><language-model><gpt>","<p>I am a Data Engineer, and I am currently assigned a task to refactor an outdated code and rectify any bugs present. However, I am unable to comprehend the code written in the existing codebase. Furthermore, the developers who worked on this codebase did not provide any documentation. Consequently, I am inquiring if there is a feasible method to convert the entire codebase into an extensive text document. Subsequently, I would like to utilize ChatGPT to translate the codebase into a comprehensive document(very long text, with folder structure tree and code inside src) that I can use to embedding. I do not require an in-depth explanation of the code; rather, I am seeking a more abstract-level understanding, such as the purpose of specific files, the functionality of particular folders, etc.</p>
","nlp"
"120415","NLP: Infer intent of finalising a transaction in a dialogue/chat system","2023-03-22 18:36:05","","1","14","<machine-learning><nlp><machine-learning-model>","<p>I have been tasked with tacking the following problem and I wanted to ask for different approaches on how to best approach it.</p>
<p>Problem
I am looking to infer the intent of finalising the transaction during a chat conversation. For example: buyer messages “are there any scratches on the table?” and gets a response “no, there are no scratches, the table is brand new” the probability of finalizing the transaction is 89%.</p>
<p>Data Available
Chat data is available for the last month all in Polish with a flag pointing if a transaction was completed or not. The feedback was acquired by sending a custom binary closed question 48h after the conversation ended probing both sides buyer and seller.</p>
<p>My approach
I was looking to preprocess the whole dialogue (remove stopwords, lemmatisation) as one text and pass it through a TF-IDF (use n-grams as well). Then based on the frequency of words determine how relevant those words are to a transaction or not and then fit a classifier (naive bayes) to determine the probability of a transaction. An open question still to answer is to use the whole dialogue up until a point or just use the last 2,4… message exchanged between the buyer and the seller.</p>
<p>Looking forward to your thoughts on the topic. Thanks a lot in advance for your help.</p>
","nlp"
"120408","Glove vector representation formula derivation - unsymmetric argument","2023-03-22 15:47:07","","0","15","<nlp><word-embeddings>","<p>On page 4 of paper &quot;GloVe: Global Vectors for Word Representation&quot;.
The author said &quot;Our final model should be invariant under this relabeling, but Eqn. (3) is not.&quot; My quesiton is why
<span class=""math-container"">$$
F\left(\left(w_i-w_j\right)^T \tilde{w}_k\right)=\frac{P_{i k}}{P_{j k}}
$$</span>
is not symmetric?</p>
","nlp"
"120394","How to identify certain term in a long document with NLP?","2023-03-22 00:35:29","120404","1","46","<nlp>","<p>Given different long documents of the same type, e.g. certain type of report, I need to identify certain items within the report, such as certain item's amount, the name of the certain person etc. How should I frame this problem under nlp? And what are the general approaches? I think the key challenge here is the same type of information will be in different part of the document in different documents. And the documents are 30-40 pages long.</p>
","nlp"
"120374","How do GPT models go from token probabilities to textual outputs?","2023-03-20 16:44:36","120379","3","1316","<machine-learning><nlp><gpt>","<p>Suppose GPT-2 or GPT-3 is trying to generate the next token, and it has a probability distribution (after applying softmax to some output logits) for the different possible next tokens. How does it choose what token to use in its textual output?</p>
<p>The <a href=""https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf"" rel=""nofollow noreferrer"">GPT-2 paper</a> mentions top-k random sampling (citing &quot;<a href=""https://arxiv.org/pdf/1805.04833.pdf"" rel=""nofollow noreferrer"">Hierarchical Neural Story Generation</a>&quot;) and never mentions beam search. The <a href=""https://arxiv.org/pdf/2005.14165.pdf"" rel=""nofollow noreferrer"">GPT-3 paper</a> mentions nucleus sampling (citing &quot;<a href=""https://arxiv.org/pdf/1904.09751.pdf"" rel=""nofollow noreferrer"">The Curious Case of Neural Text Degeneration</a>&quot;) and mentions beam search (citing &quot;<a href=""https://arxiv.org/pdf/1910.10683.pdf"" rel=""nofollow noreferrer"">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a>&quot;).</p>
","nlp"
"120358","How many parameters does the vanilla Transformer have?","2023-03-20 09:33:01","120363","1","3260","<machine-learning><nlp><transformer><attention-mechanism>","<p>The original Transformer paper (<a href=""https://arxiv.org/pdf/1706.03762.pdf"" rel=""nofollow noreferrer"">Vaswani et al; 2017 NeurIPS</a>) describes the model architecture and the hyperparameters in quite some detail, but it misses to provide the exact (or even rough) model size in terms of parameters (model weights).</p>
<p>I could not find a source with a definite answer on this. Table 3 also mentions a <code>base</code> and a <code>big</code> model, but for none of them model size is given.</p>
<p>How many parameters does a <code>base</code> or a <code>big</code> Transformer model, according to the original implementation by Vaswani et al., have?</p>
","nlp"
"120312","Why the label is not explicitly involved in the loss function of skip-gram?","2023-03-18 06:55:48","","1","205","<machine-learning><nlp><word-embeddings>","<p>I am recently learning word embedding myself. When learning skip-gram from the paper <a href=""https://arxiv.org/pdf/1310.4546.pdf%5BDistributed"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1310.4546.pdf[Distributed</a> Representations of Words and Phrases and their Compositionality], I am stuck in understanding the loss function.
<span class=""math-container"">$$
-\frac{1}{T}\sum_{t=1}^{T}\sum_{-c\le j\le c}\log p(w_{t+j}|w_t)
$$</span>
It is so-called conditional likelihood or something, a little bit like cross entropy, but it is not.
In the first forward propagation, we feed the word pairs to the neural network. For example, input <span class=""math-container"">$w_1$</span> as the focus word and <span class=""math-container"">$w_2$</span> as the context word. However, I cannot see how <span class=""math-container"">$w_2$</span> works in the loss.
<br />
<br />
Traditionally, the loss function for a supervised learning should contain label and the predicted value, but here <span class=""math-container"">$p(w_{t+j}|w_t)$</span> is just a predicted value calculated by applying softmax function on the output layer. If there is no label in the loss function, how can we reduce the loss by backward propagation.</p>
","nlp"
"120301","Convert cosine similarity to probability","2023-03-17 16:33:17","120305","1","2605","<nlp><cosine-distance>","<p>In natural language processing, the cosine similarity is often used to compute the similarity between two words. It is bounded between [-1, 1]. Supposedly, 1 means complete similarity, -1 means something like antonyms, and 0 means no relationship between the words, although I am unsure whether that fully holds true in praxis. For another application, I need to convert the cosine similarity to a probability between 0 and 1. A straightforward solution would be to take the absolute value of the cosine similarity, but does this make sense? My goal is simply to assign higher scores to words that occur in similar contexts (i.e. could be swapped out and still leave the sentence plausible).</p>
","nlp"
"120289","GPT-2 architecture question","2023-03-17 13:29:09","120297","0","379","<machine-learning><neural-network><nlp><pytorch><gpt>","<p>I am currently working on a NLP model that compares two comments and determines which one would be more popular. I have already came up with an architecture - it will be based on GPT-2. But now I am struggling understanding what is the general format of an output of it. I inspected <a href=""https://github.com/graykode/gpt-2-Pytorch/blob/master/GPT2/model.py"" rel=""nofollow noreferrer"">this</a> PyTorch implementation of GPT-2 and here is what I understood:</p>
<ul>
<li><a href=""https://github.com/graykode/gpt-2-Pytorch/blob/401078fde5e85475590dc04c9ed11b79332b8717/GPT2/model.py#L126"" rel=""nofollow noreferrer"">GPT2Model</a> is the main transformer block, which uses stack of decoders (class Block).</li>
<li><a href=""https://github.com/graykode/gpt-2-Pytorch/blob/401078fde5e85475590dc04c9ed11b79332b8717/GPT2/model.py#L110"" rel=""nofollow noreferrer"">Block</a> is just one decoder block with attention and convolution layers</li>
<li><a href=""https://github.com/graykode/gpt-2-Pytorch/blob/401078fde5e85475590dc04c9ed11b79332b8717/GPT2/model.py#L175"" rel=""nofollow noreferrer"">GPT2LMHead</a> is just some number of fully-connected layers. Simple classification head.</li>
</ul>
<p>What I don't understand so far is:</p>
<ol>
<li>What is <code>presents</code> variable for? I looked inside and it is just list of tensors, but I can't really figure out what are they.</li>
<li>If I want to get an embedding of my input sentence, which class I need to use? I thought it is GPT2Model that returns some hidden states, but it returns matrix with dimensions (batch_size, sentence_length + <em>smth</em>, 768). Why is it a matrix and how to get vector then?</li>
<li>What is the purpose of <a href=""https://github.com/graykode/gpt-2-Pytorch/blob/401078fde5e85475590dc04c9ed11b79332b8717/GPT2/model.py#L181"" rel=""nofollow noreferrer"">set_embedding_weights</a> method? To be honest, I don't even understand what embedding weights really are.</li>
<li>If I want to my output be of fixed shape, what placeholders do I need to use in case when an input sentence is smaller than max input size of the GPT-2 model?</li>
</ol>
<p>Please, can you help me to understand this? I would appreciate any help. Thank you in advance!</p>
","nlp"
"120227","Using BERT to extract a list of words and phrases from documents","2023-03-15 16:26:56","","2","402","<nlp><transformer><bert><information-extraction>","<p>I have a list of words and phrases (~3k items). What are my options to extract them from documents (~3M of job descriptions) with NLP? I do not have labeled data.</p>
<p>For example my list of words and phrases look like,</p>
<pre><code>Leadership
Microsoft Office
AWS
.
.
.
Python Programing Language
</code></pre>
<p>The result I am looking for is a matrix(3K x 3M) with binary values inside.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Doc #</th>
<th>Leadership</th>
<th>Microsoft Office</th>
<th>AWS</th>
<th>...</th>
<th>Python Programing Language</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td></td>
<td>1</td>
</tr>
<tr>
<td>.</td>
<td>.</td>
<td>.</td>
<td>.</td>
<td></td>
<td>.</td>
</tr>
<tr>
<td>3M</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td></td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
<ol>
<li><p>Regex -  This is the most straightforward solution comes my mind. However, this solutions is not robust and cannot capture different word/phrase forms. For example, people might write <code>MS Office</code> instead of <code>Microsoft Office</code>. Similarly, people might write <code>Amazon Web Service</code> rather than <code>AWS</code>.</p>
</li>
<li><p>Is there a solution to utilize a Large Language Model such as BERT?</p>
</li>
<li><p>If I create a labeled data using, for example, <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/sms-label-text.html"" rel=""nofollow noreferrer"">AWS Ground Truth</a>, is there a way to utilize the results to build a model and extract the list of words/phrases?</p>
</li>
</ol>
","nlp"
"120215","Does high number of output labels affect the performance of BERT and how to handle the class imbalance issue while doing multi text classification?","2023-03-15 11:34:02","","1","366","<machine-learning><nlp><transformer><bert><text-classification>","<p>I am using BERT to do multiclass text classification. The number of output classes I have to predict from is: 116 and there is high degree of class imbalance that I see.<br>
We have the following kind of records available for each of the classes:<br>
{'Class A': 975 number of records,<br>
'Class B': 776 number of records,<br>
'Class C': 533 number of records,<br>
'Class D': 412 number of records,<br>
'Class E': 302 number of records,<br>
'Class F': 250 number of records,<br>
'Class G': 207 number of records,<br>
'Class H': 137 number of records,<br>
'Class I': 96 number of records,<br>
'Class J': 51 number of records,<br>
'Class K': 28 number of records,<br>
'Class L': 17 number of records,<br>
'Class M': 7 number of records,<br>
'Class N': 2 number of records}<br></p>
<p>So I have two questions here:<br>
Question1: As we have around 116 output classes to predict from, does that affect the performance of BERT due to the high number of output classes?</p>
<p>Question2: My original data has the similar type of class distribution that I have illustrated above. So how does this affect the performance of BERT and if it affects how do we handle this to get proper output?</p>
<p>Looking forward to get answer from the talented community we have here.</p>
<p>Much thanks in advance.</p>
","nlp"
"120209","Is there any concern for a pretrained model to overfitting to a fine-tuning task that has overlapping pretraining and training data?","2023-03-15 08:46:06","","1","448","<nlp><overfitting><bert><pretraining>","<p>Let's say my language model is pretrained on a general text corpus, and I want to use it for some specific downstream task that has it's datasets also included in the general corpus, is there any concern for overfitting or bias?</p>
<p>I can't seem to find much resources that touch on this issue. I read this paper <a href=""https://aclanthology.org/D19-1371.pdf"" rel=""nofollow noreferrer"">SciBERT</a> that shows <em>in-domain pretraining</em> of BERT with vocab and corpus extracted from only <strong>scientific</strong> text would yield better performance on <strong>scientific</strong> tasks. But isn't this just overfitting? I also read a few papers like the <a href=""https://arxiv.org/pdf/1910.10683v3.pdf"" rel=""nofollow noreferrer"">T5</a> paper that claims <em>in-domain pretraining</em> leads to improvement of fine-tuning tasks as if it is a merit to use pretraining data that is similar to finetuning tasks? Is there not a concern for overfitting? Is it not a concern if the pretraining and finetuning objectives are different enough? Or am I misunderstanding the concept of pretraining and overfitting?</p>
<p>Would appreciate if anyone could also provide links to articles that investigate this issue.</p>
","nlp"
"119991","A French version of Rebel","2023-03-06 11:01:33","","1","22","<nlp><transformer><knowledge-base><knowledge-graph>","<p>Is there an end-to-end trained transformer like Rebel for french data?
Rebel can extract entities and relations from text, yet as far as I know, it works only with english texts.
Is there any other alternatives for <strong>french</strong> data? I am working in building a knowledge graph of my french (after translating it) data using Rebel, but it didn't work well.</p>
<p>I tried to translate my french data to english. I already have a very noisy data, it becomes even noisier after translation, so I am looking for altenative of Rebel, an end-to-end architecture that deals with <strong>french</strong> data.</p>
","nlp"
"119931","SBERT Embeddings from Conversations","2023-03-03 12:09:33","","0","321","<nlp><bert><embeddings>","<p>I have a dataset consisting of text-based conversations between two humans. One conversation has on average 20 turns and can look as follows:</p>
<pre><code>Person 1: Do you like cooking?
Person 2: Yes. I like cooking very much. I got this hobby when I was 12 years sold.
Person 1: Why do you like it?
Person 2: I have no idea. I like cooking by myself. I like to taste delicious food.
...
</code></pre>
<p>With SBERT I can get the embeddings of one turn (e.g., &quot;Hello there, how are you doing?&quot;). Is it also possible to get one embedding with SBERT for several turns or a whole conversation (20 turns)? Are there other models which are capable to do this or are more recent? Afterward, I would like to project the embedding to 2D or 3D space and apply clustering.</p>
","nlp"
"119925","Is there a reference dataset for contextual similarity?","2023-03-03 08:01:25","","3","44","<nlp><word-embeddings><similarity><semantic-similarity>","<p>I'm doing some experiments with word embeddings to try to capture context-aware similarity, so that for example the word pair apple - hardware, are very dissimilar in the context of a fruit store, but very similar in an IT context.</p>
<p>My question is if there is a benchmark dataset for this challenge. I've been looking, but I can't find anything.</p>
<p>Thanks in advance.</p>
","nlp"
"119923","How does BERT work for Aspect-Based sentiment analysis?","2023-03-02 21:45:31","","1","949","<deep-learning><nlp><bert><sentiment-analysis>","<p>I have recently used a <a href=""https://github.com/ScalaConsultants/Aspect-Based-Sentiment-Analysis"" rel=""nofollow noreferrer"">package</a> to perform Aspect-Based Sentiment Analysis (ABSA) through a BERT model.</p>
<p>Briefly, the model takes two inputs:</p>
<ul>
<li>words that constitute the <em>aspects</em></li>
<li>a sentence on which we want to perform the ABSA</li>
</ul>
<p>The BERT-based model outputs a sentiment list with three integers representing the positive, the neutral and the negative scores.</p>
<p>I would like to understand more about how this kind of model works.</p>
<p>I have seen many posts on Medium that are too general and read some papers that give the basic functioning for grated.</p>
<p>So, if anyone can explain extensively how bert-based ABSA works, it would be appreciated.</p>
<p>Thank you!</p>
","nlp"
"119912","How to finetune a closed generative huggingface model?","2023-03-02 14:52:20","","0","299","<nlp><huggingface><finetuning>","<p>I want to finetune a huggingface pretrained model on our internal documentation in a way it stats answering related questions. I could not find the adequate tutorial.</p>
","nlp"
"118898","Should I open abbreviations/acronyms in the text data, when training transformer model?","2023-03-01 08:10:58","","2","150","<machine-learning><deep-learning><nlp><transformer>","<p>I am currently training a transformer model on text data. Is it a good practise to open abbreviations/acronyms in the text data? I did not dins any tips or recommendations about it on internet.</p>
","nlp"
"118804","Understand the interpretability of word embeddings","2023-02-25 19:45:50","118818","1","82","<nlp><word-embeddings>","<p>When reading the Tensorflow tutorial for <em>Word Embeddings</em>, I found two notes that confuse me:</p>
<blockquote>
<p>Note: Experimentally, you may be able to produce more interpretable embeddings by using a simpler model. Try deleting the Dense(16) layer, retraining the model, and visualizing the embeddings again.</p>
</blockquote>
<p>And also:</p>
<blockquote>
<p>Note: Typically, a much larger dataset is needed to train more interpretable word embeddings. This tutorial uses a small IMDb dataset for the purpose of demonstration.</p>
</blockquote>
<p>I don't know exactly what it means by &quot;more interpretable&quot; in those two notes, is it related to the result displayed by the embedding projector? And also why the interpretability will increase when reducing model's complexity?</p>
<p>Many thanks!</p>
","nlp"
"118767","What are the advantages of autoregressive over seq2seq?","2023-02-24 09:43:33","118768","0","1168","<deep-learning><nlp><transformer><sequence-to-sequence><gpt>","<p>Why are recent dialog agents, such as ChatGPT, BlenderBot3, and Sparrow, based on the decoder architecture instead of the encoder-decoder architecture?</p>
<p>I know the difference between the attention of the encoder and the decoder, but in terms of dialogue, isn't the attention of the encoder-decoder better?</p>
","nlp"
"118739","How do we evaluate the outputs of text generation models?","2023-02-23 11:51:07","","2","2335","<nlp><word-embeddings><model-evaluations><text-generation><semantic-similarity>","<p>Evaluation of a wide variety of natural language generation (NLG) tasks is difficult. For instance, for a question answering model, it is hard for a human to quantify <em>how well</em> the model has answered a particular question. Doing this at scale is even harder, because it requires automating that judgement about output quality.</p>
<p>The most common approach for evaluation of NLG at scale involves building a set of test queries and reference answers, where the reference answers set out the 'gold standard' for how the model should respond. In the case of a Q&amp;A bot, this would be a list of questions and 'good' answers; for a machine translation system, this would be some human-verified translations.</p>
<p>A <em>good</em> text generation model ideally takes a query as input and returns an output as close as possible to the reference given in the test set. As such, the model is assessed by passing in each query in turn, and comparing <em>how semantically similar</em> the model's output is to the reference output. If the model's output is similar to the reference, then this means the model is performing well.</p>
<p>My question is <strong>how do we assess semantic similarity between the reference and candidate answers</strong>?</p>
<p>A few ideas:</p>
<ul>
<li>Old-school string matching - Calculate word, subword, or n-gram overlap between candidate and reference answers. Can use metrics like F1-score or recall depending on use case. The idea is that a good answers includes as much of the surface content from the reference as possible, with as little extraneous information as possible. However, this sort of approach performs poorly where the meaning of the answer is the same, but the surface form is different - or vice versa, e.g. 'The cat is under the mat' and 'The mat is under the cat' have a different meaning but contain all the same unigrams, so would get a high similarity score with a string-based metric.</li>
<li>Vector distance between embeddings - Use text embeddings trained to map paraphrases to similar embeddings. Encode the reference and candidate answers, and then use a measure of vector distance (e.g. cosine-similarity) to evaluate. If - once encoded - the reference and candidate answers are 'close' then the two answers should be a near-paraphrase of one another. This means the candidate answer does a good job of including the meaning from the reference answer. However, this method is only as good as the embeddings underpinning it. Moreover, it seems circular to use semantic similarity to evaluate the outputs of tasks where semantic similarity is used to produce the outputs (as is typically the case for Q&amp;A bots, semantic search, summarisation, machine translation, etc.)</li>
<li>Mover distance in semantic space* - This is a similar approach to using vector distance. The idea is that encoded text can be visualised as <span class=""math-container"">$n$</span> points in <span class=""math-container"">$k$</span>-dimensional space, where <span class=""math-container"">$n$</span> is the number of tokens in the text and <span class=""math-container"">$k$</span> is the dimension of the embeddings used. Then, we can think about the candidate and reference answers being <span class=""math-container"">$n_c$</span> and <span class=""math-container"">$n_r$</span> points in that semantic space. We can then think about moving candidate points to sit on top of reference points - the total distance involved in this movement is the 'mover distance'. There exists some optimal, i.e. most efficient, way of moving candidate points to sit on top of reference points, and this gives the 'mover score' for that model on that query.</li>
</ul>
<p><strong>What approaches have I missed? What are the strengths and weaknesses of each approach? Are there some state-of-the-art approaches that outperform these?</strong></p>
<p><sub>*<a href=""https://arxiv.org/pdf/2108.12463.pdf"" rel=""nofollow noreferrer"">Colombo, et al. (2021)</a>, <a href=""https://arxiv.org/pdf/1909.02622.pdf"" rel=""nofollow noreferrer"">Zhao, et al. (2019)</a></sub></p>
","nlp"
"118721","Best string similarity metric not considering word order","2023-02-22 13:12:15","","0","168","<nlp><preprocessing><similarity>","<p>I'm sorry if the title is misleading, but I didn't really know how to explain what I am searching for.
I have a dataset containing two columns representing names and surnames of a bunch of people. These might be inserted in multiple records.
However, sometimes the name is put in the surname field and viceversa. Also, there might be some typing mistakes.
I was thinking about merging these into a single string (NameSurname) in order to find similarities between records and fix the fields.
I have looked at some string similarity metrics, but I see that the most popular ones look at consecutive characters and would fail to recognize SurnameName and NameSurname as the same string.
Is there any metric robust to this?
Thank you a lot in advance.</p>
","nlp"
"118713","Does order matter in this causal language model?","2023-02-22 09:20:41","","0","30","<nlp><machine-learning-model><tokenization>","<p>Say you've implemented a causal language model like so:</p>
<pre><code>def get_causal_xy(data, max_len=8):
    xs=[]
    ys=[]
    index = 0
    for i in range(len(data) - max_len):
      xs.append(data[index: min(len(data), index + max_len)])
      ys.append(data[index+1: min(len(data), index + max_len + 1)])
      index = index + 1
    return xs, ys
</code></pre>
<p>would then order matter. Say you have A,B,C,D words and youre trying to find P(y)</p>
<p>would P(y | A, B, C, D) be the same as P(y | C , B , A , D)?</p>
<p>My hunch is yes, because you still have access to the same words, and casual language model like this simple one doesnt use positional encoding</p>
","nlp"
"118672","Can this task for phrases be called lemmatization?","2023-02-21 03:52:07","","0","20","<nlp><preprocessing><text>","<p>I want to 'lemmatize' phrases to <a href=""https://idioms.thefreedictionary.com/leave+one%27s+mark+on"" rel=""nofollow noreferrer"">dictionary entries</a>. For instance, the following collocates can be standardized to the idiom in the aforementioned link <code>leave (one's or its) mark on (someone or something)</code>:</p>
<ol>
<li>leaving their mark on a country</li>
<li>left his mark on me</li>
<li>leaves their mark on him</li>
<li>left its mark on me</li>
</ol>
<p>and etc.</p>
<p>One more illustration for <a href=""https://idioms.thefreedictionary.com/be+in+line+with"" rel=""nofollow noreferrer""><code>be in line with</code></a>:</p>
<ol>
<li>is in line with</li>
<li>be in line with</li>
<li>are in line with</li>
</ol>
<p>I don't have a corpus of the standardized phrases as candidates for the process.</p>
<p>I have tested using <a href=""https://www.machinelearningplus.com/nlp/lemmatization-examples-python/"" rel=""nofollow noreferrer"">lemmatization approaches in Python</a> and found that lemmatization cannot fully meet the requirements here. I wonder which task in NLP most relates to this problem?</p>
","nlp"
"118662","Group unstructured chat logs into conversations","2023-02-20 17:45:07","","3","733","<machine-learning><deep-learning><nlp><model-selection><question-answering>","<p>I am new to ML/AI/NLP and am interested in tackling the following problem. I have a database of chat logs from a Discord server. The database contains the following labeled data: <code>Author</code>, <code>Text</code>, <code>Text Created Timestamp</code>, and <code>Channel ID</code>.</p>
<p>I think what I want to do is fairly complex, but should, in theory, be possible. I want to go through the logs, and group messages that are related to a new label, <code>Conversation ID</code>. In an ideal world, the model will label each piece of text with a conversation id and I can then run queries against the data to show me a related text that pertains to a specific conversation. among 1 or more individuals. (i say 1 because someone might ask a question and then reply to themselves with an answer. this will be rare)</p>
<p>I can have someone go through the data manually and pull out and label what constitutes a 'conversation.' for some test data. If I was able to build a successful model, I would use the conversations to look for logical groupings of questions asked with follow-up answers. That would be the ultimate goal here is to find questions/answers. I think a logical step would be first to isolate conversations.</p>
<p>The end goal would be to take the discussion from the beginning of time to the start of time and be able to isolate different chunks of text into a logical conversation group.</p>
<p>It would make sense that any conversation happens within the same <code>Channel ID.</code> I am not worried about cross-channel conversations. Those are rare/unlikely.</p>
<p>My last naive approach for this was to find every question in the corpus and then search for the subsequent N messages following that message from the same channel, then ask an LLM to identify if there was an answer given the question. I want to see if I can improve that process. My approach had a hard limit of messages after a questions where the hard limit was <code>N</code>, nothing is to say that there wasn't an answer to a question in <code>N+M</code> messages and I missed the cut-off. The idea is a model won't be limited the number of subsequent messages.</p>
<p>Happy to take advice from seasoned hackers on different approaches to this problem that I can go ahead and experiment with. Names of techniques, tools, models that I can research that apply specifically to this problem would be ideal.</p>
","nlp"
"118602","Data Preparation for next word prediction","2023-02-18 06:21:10","","0","222","<nlp><data-cleaning><multiclass-classification><language-model><ai>","<p>In most places, I have seen that when preparing the training data and label for next-word prediction from the corpus one uses a fixed window size say of length 4, and then scans the subsequences of length 4 as X and the next token as y.</p>
<p>For example: Consider this sentence <code>&quot;The quick brown fox jumps over the lazy dog&quot;</code> and a window of size say 4. Then my training data looks something like this as (X, y) pair</p>
<pre><code>[&quot;The quick brown&quot; , &quot;fox&quot;], [&quot;quick brown fox&quot;, &quot;jumps&quot;], [&quot;brown fox jumps&quot;, &quot;over&quot;], .....
</code></pre>
<p>I have the following doubts.</p>
<ol>
<li>When we train a language model over the data it expects the sequence of length 4, but suppose a sentence only contains 2 words say <code>&quot;quick brown&quot;</code> and I need to predict the next word <code>&quot;fox&quot;</code> I know we can pad to sequence of length 4 but my doubt is will model do any good with a sequence of shorter length if it's trained on the fixed sequence of length 4?</li>
<li>Is it a good idea to have all subsequences of length say from 1 to 4 as training data and pad the shorter ones to a maximum length which is 4 in this case? One problem I see is the issue of the underrepresentation of larger lengths and the overrepresentation of smaller lengths.</li>
</ol>
","nlp"
"118570","Why custom training a Spacy model runs only the Initializing pipeline but the Training pipeline is not running?","2023-02-16 14:10:24","","0","149","<machine-learning><python><nlp><text-mining><nltk>","<p>I am training a custom NER model with Spacy version 3.5.0 using some dummy data. My entire code and dummy data is given below. <a href=""https://github.com/dmoonat/Named-Entity-Recognition/blob/main/NER_with_spaCy.ipynb"" rel=""nofollow noreferrer"">This is exact same code give in the 2nd half of this link</a>. The code is running fine, but it only executes until the <strong>Initializing pipeline</strong> step of the training and the <strong>Training pipeline</strong> is not executed.</p>
<p>Any idea why the training pipeline is not being executed?</p>
<pre><code>import pandas as pd
import os
from tqdm import tqdm
from spacy.tokens import DocBin

train = [
          (&quot;An average-sized strawberry has about 200 seeds on its outer surface and are quite edible.&quot;,{&quot;entities&quot;:[(17,27,&quot;Fruit&quot;)]}),
          (&quot;The outer skin of Guava is bitter tasting and thick, dark green for raw fruits and as the fruit ripens, the bitterness subsides. &quot;,{&quot;entities&quot;:[(18,23,&quot;Fruit&quot;)]}),
          (&quot;Grapes are one of the most widely grown types of fruits in the world, chiefly for the making of different wines. &quot;,{&quot;entities&quot;:[(0,6,&quot;Fruit&quot;)]}),
          (&quot;Watermelon is composed of 92 percent water and significant amounts of Vitamins and antioxidants. &quot;,{&quot;entities&quot;:[(0,10,&quot;Fruit&quot;)]}),
          (&quot;Papaya fruits are usually cylindrical in shape and the size can go beyond 20 inches. &quot;,{&quot;entities&quot;:[(0,6,&quot;Fruit&quot;)]}),
          (&quot;Mango, the King of the fruits is a drupe fruit that grows in tropical regions. &quot;,{&quot;entities&quot;:[(0,5,&quot;Fruit&quot;)]}),
          (&quot;undefined&quot;,{&quot;entities&quot;:[(0,6,&quot;Fruit&quot;)]}),
          (&quot;Oranges are great source of vitamin C&quot;,{&quot;entities&quot;:[(0,7,&quot;Fruit&quot;)]}),
          (&quot;A apple a day keeps doctor away. &quot;,{&quot;entities&quot;:[(2,7,&quot;Fruit&quot;)]})
        ]

db = DocBin() # create a DocBin object

for text, annot in tqdm(train): # data in previous format
    doc = nlp.make_doc(text) # create doc object from text
    ents = []
    for start, end, label in annot[&quot;entities&quot;]: # add character indexes
        span = doc.char_span(start, end, label=label, alignment_mode=&quot;contract&quot;)
        if span is None:
            print(&quot;Skipping entity&quot;)
        else:
            ents.append(span)
    doc.ents = ents # label the text with the ents
    db.add(doc)

db.to_disk(&quot;./train.spacy&quot;) # save the docbin object

!python -m spacy init fill-config base_config.cfg config.cfg

!python -m spacy train config.cfg --output ./output --paths.train ./train.spacy --paths.dev ./train.spacy
</code></pre>
<p><strong>Expected output</strong></p>
<p><a href=""https://i.sstatic.net/dm2Vn.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/dm2Vn.png"" alt=""enter image description here"" /></a></p>
<p><strong>Output I got</strong></p>
<p><a href=""https://i.sstatic.net/5Fj8z.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/5Fj8z.png"" alt=""enter image description here"" /></a></p>
","nlp"
"118559","How to extract values from unstructured text","2023-02-16 09:29:24","","1","79","<nlp><training>","<p>I'm implementing a tool which should extract values of interest from unstructured text entries. The data set is several hundred thousands of medical entries. Each entry is relatively short (around 100 characters).</p>
<p>Each text entry contains a keyword <code>MI</code> (or <code>TI</code> or <code>AO</code>) and their repective value. The value is sometimes numeric and sometimes expressed as human language.</p>
<p>Some entries are very obvious. Some entries are more complex or ambiguous, in which case the end user of the tool (a doctor) should be able to make my tool &quot;learn&quot; a correct value.</p>
<p>Here are some examples with my comments</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Data entries</th>
<th>Expected extracted value</th>
<th>Comments</th>
</tr>
</thead>
<tbody>
<tr>
<td>the value of MI is 1</td>
<td>1.0</td>
<td></td>
</tr>
<tr>
<td>MI = 1</td>
<td>1.0</td>
<td></td>
</tr>
<tr>
<td>MI &gt; 1</td>
<td>1.5</td>
<td></td>
</tr>
<tr>
<td>MI &lt; 1</td>
<td>0.5</td>
<td></td>
</tr>
<tr>
<td>No traces of MI found</td>
<td>0.0</td>
<td></td>
</tr>
<tr>
<td>No traces of MI found</td>
<td>0.0</td>
<td></td>
</tr>
<tr>
<td>traces of MI/TI found</td>
<td>0.5</td>
<td></td>
</tr>
<tr>
<td>MI/TI/AO up to 4</td>
<td>3.5</td>
<td></td>
</tr>
<tr>
<td>MI/TI/AO up to 4</td>
<td>3.5</td>
<td></td>
</tr>
<tr>
<td>MI 2-[3]</td>
<td>2.5</td>
<td></td>
</tr>
<tr>
<td>MI probably 2. Gradient of MI 170torr</td>
<td>2</td>
<td>This example shows that a numeric value following a MI does not always represent its value. Here 170 is not the value I'm looking for. It's the 2</td>
</tr>
<tr>
<td>DOMV bez MS/MI v chlopni, ale úzký MI</td>
<td>??</td>
<td>Here I actually don't know what the value is. Only the user of the tool will be able to say what the value is in this case</td>
</tr>
</tbody>
</table>
</div>
<p>Those are just some non exhaustive examples. On important aspect is that the patterns repeat often across the data set.</p>
<p>Another important requirement is that <em>what</em> we are looking must be configurable. Sometimes we look for <code>MI</code> sometimes for <code>TI</code>.</p>
<p>I am myself a software engineer (not a data scientist nor a doctor) so I started with a naive implementation based on regular expressions. It's basically a set of regexp matched against each entry of the data set</p>
<p>This kind of works and I'm able to find around 90% of the cases. However it has the following drawbacks:</p>
<ul>
<li>the more regexp, the longer it will take.</li>
<li>some regexp might clash and report different values for the same entry</li>
<li>it might be cumbersome for a non tech user to add or adjust the regexps</li>
</ul>
<p>The final user of the tool is not a software engineer but a doctor and does not know anything about regexp. However, in ambiguous/unclear cases it's the end user who can tell what the value is.</p>
<p>I could of course write a user interface to add/adjust/remove the rules (regexp) but that could be cumbersome as well</p>
<p>I was wondering if there is way without using explicit if/else rules but instead train a model which would be then used for finding the values.</p>
<p>I don't know much about NLP/AI or ML so maybe someone could point me to a right direction. Such as</p>
<ul>
<li>Would NLP or NER be the concept to use here?</li>
<li>What could be some online services or open source tools/libraries with an API that could be leveraged?</li>
</ul>
","nlp"
"118552","Using NLP output for subsequent model?","2023-02-15 16:13:55","","0","12","<machine-learning><python><nlp><spacy>","<p>I use SpaCy to output a vectorized array of my text field. I'm having issues plugging this output into my random forest and could use some guidance. I label encoded other fields so my pandas dataframe looks something like:</p>
<pre><code>import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

d = {'le1': [0,1,2,1], 'le2': [3,0,2,1], 'spacy_output':[[0.12,0.14,3.5],[1.21,0.84,1.92],[0.34,0.85,2.43],[0.09,0.18,2.21]], 'response':[0,1,1,0]}

df = pd.DataFrame(d)
</code></pre>
<p>Then I try to plug this into my model:</p>
<pre><code>X = np.array(df.drop(response, axis=1))
y = df[response].values.ravel()
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 23)

clf = RandomForestClassifier(min_samples_split=4, n_estimators=100, criterion='entropy')
clf.fit(X_train,y_train)
</code></pre>
<p>Confused on how to pass this dataframe to my model. I get the following errors:</p>
<pre><code>TypeError: only size-1 arrays can be converted to Python scalars
ValueError: setting an array element with a sequence.
</code></pre>
","nlp"
"118522","Does GPT-3 remember data from prompts used to fine tune it?","2023-02-14 16:14:57","","2","239","<machine-learning><nlp><tokenization><gpt><finetuning>","<p>I am trying to fine tune a model using OpenAI's fine tuning API. I am passing bodies of text (for example, news paper articles) as prompts and the data I want from it as completions.</p>
<p>Let us consider the following: if a newspaper article I passed as a prompt to fine tune the data, consists of some information that GPT did not know before, like 'lithium ores are found in India'. If I use the completion API after that to ask GPT 'are lithium ores found in India?' as a prompt, will GPT be able to answer 'yes'?</p>
","nlp"
"118492","Alternatives to Toronto Book Corpus","2023-02-13 11:46:08","118493","0","200","<nlp><dataset>","<p>As the toronto book corpus is no longer available (or rather, only in lowercase), I am looking for an alternative dataset of comparable language variety, quality, and size.
Any suggestions? The Gutenberg Standardized Corpus is too big and still requires lots of preprocessing.</p>
","nlp"
"118481","What loss function to use for predicting discrete output sequence given a discrete input sequence?","2023-02-12 20:56:14","118491","0","434","<nlp><regression><multiclass-classification><transformer><sequence-to-sequence>","<p>I am working on sequence-to-sequence tasks where the input is an <code>n</code>-length sequence of discrete values from a finite set S (say <code>{x | x is a non-negative integer less than 10}</code>).
An example input sequence of length 5 is: <code>1 8 3 5 2</code>.</p>
<p>The output is supposed to be some length preserving transformation of the input sequence (say reverse, shift, etc.). To be explicit, the tokens of the output sequence also come from the same set as the input sequence. For example, for the input sequence above, the reverse transformation produces the output sequence: <code>2 5 3 8 1</code>.</p>
<p>I want the model to predict the output tokens exactly, so the task is closer to classification than regression. However, since the output is a sequence, we need to predict multiple classes (as many as the input length) for each input sequence.</p>
<p>I searched for references but could not find a similar setting. Please link some suitable references you are aware of that may be helpful. I have the following questions for my use case:</p>
<ol>
<li>What changes are needed such that the model works on discrete sequences as defined above?</li>
<li>What loss function would be appropriate for my use case?</li>
</ol>
<p>For 1), one might change the input sequence such that each token is replaced by an embedding vector (learned or fixed) and input that to the model. For the prediction, I was thinking of ensuring that the model produces a <code>n x k</code> length output (<code>n</code> = sequence length; <code>k</code> = <code>|S|</code> or the vocab size) and then using each of these <code>n</code> vectors to make a class prediction (from <code>k</code> classes).</p>
<p>For 2), the loss could be a sum of <code>n</code> cross-entropy losses corresponding to the <code>n</code> classifications.</p>
<p>Please help me with better answers to these two questions.</p>
<p>Thank you.</p>
<p>Edit: My setup is encoder-only (non-autoregressive prediction). Please account for this while answering the questions by suggesting approaches that are in line with the setup, if possible.</p>
","nlp"
"118477","Word2vec CBOW model with negative sampling","2023-02-12 19:06:06","","0","594","<nlp><word-embeddings><word2vec><bag-of-words>","<p>From <a href=""https://aegis4048.github.io/optimize_computational_efficiency_of_skip-gram_with_negative_sampling#Softmax-is-computationally-very-expensive"" rel=""nofollow noreferrer"">this article</a>:</p>
<blockquote>
<ul>
<li>In vanilla skip gram model, softmax is computationally very expensive, as
it requires scanning through the entire output embedding matrix (W_output) to compute the probability distribution of all V words, where V can be millions or more.</li>
<li>Furtheremore, the normalization factor in the denominator also requires V iterations.</li>
</ul>
</blockquote>
<p>Hence the article suggests applying negative sampling instead of softmax. There are many article discussing skip grams with negative sampling. But I did not find any discussing CBOW (Continuous Bag of Words Model ) model with negative sampling. Why is this so? Is it not possible / recommended? Or its exacty same as skip gram? Can you please shed some insights about using negative sampling with CBOW?</p>
<p>PS: any article / paper link will also be of great help.</p>
","nlp"
"118416","Topic classification on text data with no/few labels","2023-02-09 14:06:14","118418","1","754","<nlp><unsupervised-learning><supervised-learning><text-classification><semi-supervised-learning>","<p>I would like to achieve a classification of a text input into predefined categories.
From what I have understand unsupervised approach are unfeasible if my target label is something very rare in pretrained models (I have labels about specific industrial processes).
Is this true?</p>
<p>Otherwise I could try an approach in which I label for example 1000 input texts using all the different labels and use a supervised approach with very few labeled data. Should this help someway the learning process? And what methods could I use in this case?</p>
","nlp"
"118371","Some answers given by ChatGPT are just beyond ridiculous, what could be the reasons?","2023-02-08 08:52:56","","1","149","<nlp><language-model><gpt>","<p>Some answers given by ChatGPT are just beyond ridiculous, especially in Chinese (I am Chinese so I ask ChatGPT in both Chinese and English). Here is an example,</p>
<p><a href=""https://i.sstatic.net/GxuvN.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/GxuvN.png"" alt=""castration"" /></a></p>
<p>The last answer</p>
<blockquote>
<p>阉割政策是满清末期中国强制对男性实施阉割的政策。这一政策是为了限制人口增长和减少国家财政负担，但是它对受害者造成了巨大的心理和生理影响。阉割政策也破坏了中国传统文化中对男性身份和角色的认识，并对整个社会造成了深远的影响。该政策在1911年的中华民国成立后终止，但对中国文化和历史的影响一直持续到今天。</p>
</blockquote>
<p>which means</p>
<blockquote>
<p>The castration policy was a policy of mandatory castration of men in
China in the late Qing Dynasty. This policy was intended to limit
population growth and reduce the financial burden on the state, but it
had a huge psychological and physical impact on the victims. The
castration policy has also undermined traditional Chinese cultural
understandings of masculinity and roles and has had a profound impact
on society as a whole. The policy ended after the founding of the
Republic of China in 1911, but its influence on Chinese culture and
history continues to this day.</p>
</blockquote>
<p>That is just absurd! And I have many examples like that.</p>
<p>Apparently, its Chinese corpus has something to do with it. What other things have gone wrong?</p>
","nlp"
"118357","How to perform topic reduction?","2023-02-07 18:51:17","","0","585","<python><nlp><topic-model>","<p>I am using <a href=""https://github.com/ddangelov/Top2Vec"" rel=""nofollow noreferrer"">top2vec</a> to perform topic modelling.</p>
<p>According to the <a href=""https://arxiv.org/pdf/2008.09470.pdf"" rel=""nofollow noreferrer"">paper</a>, topic reduction can be performed on the topic vectors to hierarchically group similar topics and reduce the number of topics discovered.</p>
<p>This would be very useful because the topics found by the algorithm on my dataset are pretty similar to each other.</p>
<p>Does anyone know if there is some python code to do this?
On the online <a href=""https://top2vec.readthedocs.io/en/latest/Top2Vec.html?highlight=top2vec"" rel=""nofollow noreferrer"">documentation</a>, there is nothing about it.</p>
<p>I have also opened an <a href=""https://github.com/ddangelov/Top2Vec/issues/319"" rel=""nofollow noreferrer"">issue</a> on the official page if someone wants to post an answer there.</p>
","nlp"
"118356","Which Publicly Accessible Large Language Models are Very Similar to OpenAI's ChatGPT?","2023-02-07 18:27:20","","3","179","<nlp><language-model><gpt>","<p>What other large language models exist or will soon exist that are VERY similar to OpenAI's ChatGPT in the sense of being fine-tuned or otherwise specifically created for conversational tasks including question answering? Such models can be free to use or require subscription. I'm preferably looking for models that can give companies API access. I am interested in models with fairly high quality question answering performance in terms of accuracy and not having too many &quot;hallucinations&quot;.</p>
<p>Here are some examples I am currently familiar with:</p>
<ul>
<li><strong>InstructGPT</strong> from OpenAI: <a href=""https://openai.com/blog/instruction-following/"" rel=""nofollow noreferrer"">https://openai.com/blog/instruction-following/</a></li>
<li><strong>OpenAssistant</strong> from LAION: <a href=""https://github.com/LAION-AI/Open-Assistant"" rel=""nofollow noreferrer"">https://github.com/LAION-AI/Open-Assistant</a></li>
<li><strong>ChatSonic</strong> from WriteSonic: <a href=""https://writesonic.com/chat"" rel=""nofollow noreferrer"">https://writesonic.com/chat</a></li>
<li><strong>Jasper Chat</strong>: <a href=""https://www.jasper.ai/chat"" rel=""nofollow noreferrer"">https://www.jasper.ai/chat</a></li>
<li><strong>Google Bard</strong> (Google's soon to be released rival to ChatGPT): <a href=""https://blog.google/technology/ai/bard-google-ai-search-updates/"" rel=""nofollow noreferrer"">https://blog.google/technology/ai/bard-google-ai-search-updates/</a></li>
<li><strong>ChatGenie</strong> via WriteCream: <a href=""https://www.writecream.com/chatgenie/"" rel=""nofollow noreferrer"">https://www.writecream.com/chatgenie/</a></li>
<li><strong>YouChat</strong> from you.com: <a href=""https://www.you.com/chat"" rel=""nofollow noreferrer"">https://www.you.com/chat</a></li>
<li><strong>Perplexity AI</strong>: <a href=""https://www.perplexity.ai"" rel=""nofollow noreferrer"">https://www.perplexity.ai</a></li>
<li><strong>Bing Chat</strong> (literally uses ChatGPT): <a href=""https://www.bing.com/"" rel=""nofollow noreferrer"">https://www.bing.com/</a></li>
</ul>
","nlp"
"118295","Finding associated words to a named entity","2023-02-05 06:36:24","","1","39","<nlp><named-entity-recognition>","<p>Is there a way to find a list of associated words to a Named Entity?</p>
<p>For instance : let the Named Entity be FIFA. Now FIFA is a Football Organization and hence related to the term football and all the terms related to football like jersey, footballers, goal, goalkeeper, halftime, penalty, freekick et cetera.</p>
<p>Is there a way we could do that? I have tried Babelnet but didn't get the desired results or anything even close to what I want.</p>
","nlp"
"118291","How to use/vectorize SpaCy output?","2023-02-05 00:39:16","","1","28","<machine-learning><nlp><spacy>","<p>I'm performing preprocessing on a dataset that I'm planning on using as a feature set for an RF model. I don't have too much experience with NLP so just want to make sure I'm processing my text fields correctly. So far, I've:</p>
<ol>
<li>Tokenized my text column</li>
<li>Filtered out stop words</li>
<li>Vectorize lemma result</li>
</ol>
<p>I'm worried that only vectorizing the lemma result without considering the POS won't produce a meaningful result. Should I be leveraging POS in some way? Any guidance would be appreciated. Below is my function.</p>
<pre><code>import spacy
import time

def text_analysis(df,col):
    start_time = time.time()
    results = []
    
    # Filter out stop words from text
    stop = spacy.lang.en.stop_words.STOP_WORDS
    df[col] = df[col].apply(lambda x: ' '.join([f for f in x.lower().split() if f not in (stop)]))
        
    # Write lemmetization output to temp pickle file (to prevent memory issues)
    for row in nlp.pipe(df[col].astype('unicode').values, n_process=4):
        if row.has_annotation(&quot;DEP&quot;):
            lemma_tokens = [n.lemma_ for n in row]
            lemma_desc = ' '.join([f for f in lemma_tokens])
            pos = [n.pos_ for n in row]
        else:
            lemma_desc = None
            pos = None
        
        # Vectorize lemma result
        results.append(nlp(lemma_desc).vector)
        
    print(f'{col} text processing time: {round(time.time()-start_time,2)} s')
    return results
<span class=""math-container"">```</span>
</code></pre>
","nlp"
"118273","Specifics about ChatGPT's Architecture","2023-02-03 16:47:49","","1","7132","<nlp><language-model><gpt>","<p>Does anyone know of reliable sources that have written about the architecture of OpenAI's ChatGPT - specifically regarding the following?:</p>
<ul>
<li><p>Number of hidden layers</p>
</li>
<li><p>Number of attention heads</p>
</li>
<li><p>Dimensions of its hidden layers</p>
</li>
<li><p>Sequence length in terms of number of tokens</p>
</li>
<li><p>Number of parameters in the version of the model currently free for public preview. My understanding is there are multiple versions of ChatGPT.</p>
</li>
</ul>
<p>Please note: I'm familiar with GPT-3 already. However, I am not certain that ChatGPT's architecture is the same. Open to contrary evidence, if any.</p>
","nlp"
"118260","ChatGPT's Architecture - Decoder Only? Or Encoder-Decoder?","2023-02-03 08:57:21","","17","20872","<nlp><language-model><gpt>","<p>Does ChatGPT use an encoder-decoder architecture, or a decoder-only architecture? I have been coming across Medium and TowardsDataScience articles suggesting that it has an encoder-decoder architecture (see sources below):</p>
<p>-- <a href=""https://medium.com/@sreedevr/chatgpt-architecture-explained-7fc0cbb7426c"" rel=""noreferrer"">https://medium.com/@sreedevr/chatgpt-architecture-explained-7fc0cbb7426c</a></p>
<p>-- <a href=""https://towardsdatascience.com/how-chatgpt-works-the-models-behind-the-bot-1ce5fca96286"" rel=""noreferrer"">https://towardsdatascience.com/how-chatgpt-works-the-models-behind-the-bot-1ce5fca96286</a></p>
<p>-- <a href=""https://leoneperdigao.medium.com/chatgpt-a-deep-dive-1feade9c4d77"" rel=""noreferrer"">https://leoneperdigao.medium.com/chatgpt-a-deep-dive-1feade9c4d77</a></p>
<p>I even asked ChatGPT myself whether it uses an encoder and decoder, or just a decoder, and it told me it has both an encoder and decoder. Here is one of its responses to the question:</p>
<p><em>&quot;As a language model, ChatGPT does not have a decoder-only architecture, it has an encoder-decoder architecture. Having an encoder-decoder architecture enables language models like ChatGPT to process input sequences and generate outputs based on that input. The encoder processes the input sequence and generates a set of hidden states, which are then used by the decoder to generate the output sequence.
Having both an encoder and a decoder allows ChatGPT to handle a variety of tasks that involve generating text based on input text, such as language translation, summarization, and answering questions. Having an encoder-decoder architecture allows ChatGPT to maintain context and consistency in the generated output, which is important for many NLP tasks.
It is possible to train a decoder-only language model, but it would be limited in its ability to generate coherent text as it would not have access to the context provided by the encoder.&quot;</em></p>
<p>However, I have been under the definite impression for quite some time now that GPT-3 (from which ChatGPT was in part derived) is a decoder-only model. And I take with a grain of salt ChatGPT's explanation of its own architecture given that it seems prone to generating incorrect answers sometimes. Also, with the huge fanfare of ChatGPT and the potential for misinformed authors writing about the model, I wonder if anyone knows of a reliable source that can clarify this question.</p>
<p>Thanks</p>
","nlp"
"118210","Predicting a next word from a sentence of a different lenght than seen in training","2023-02-01 11:56:12","","1","208","<deep-learning><nlp><transformer><language-model>","<p>I am building a custom Decoder-only transformer model, which is being trained on the task of Next Word Prediction. The training procedure is analogous to that of chat GPT models - the input to the model is a sentence of length K (say K=30) and the target is this sentence shifted one to the right, e.g.:</p>
<p>&quot;I would like a cup of&quot; - input</p>
<p>&quot;would like a cup of tea&quot; - output</p>
<p>If I train my model on sentences of a specified lenght, say K=30, how will it perform in inference mode when it is provided much shorter sentences, say of length 3?</p>
","nlp"
"118155","Creating variations of prompts for ChatGPT","2023-01-30 14:54:53","","0","103","<nlp><text><spacy><chatbot>","<p>I am developing a fine tune model to emulate a tech support chatbot based on my given information. I am struggling to create a large dataset (aiming for 1000 prompt/completion pairs), does anyone have good recommendations for nlp text augmentation tools to use?</p>
<p>I have tried nlpaug which does synonym replacement, but I am looking for something more nuanced - that will provide intelligent variations of a given prompt.</p>
<p>Any advice is appreciated!</p>
","nlp"
"118127","Is there any differences between using text-davinci-003 with the Azure API vs. with the OpenAI API?","2023-01-28 21:40:20","","0","259","<nlp><language-model><azure-ml><gpt><api>","<p>I wonder whether there are any differences between using text-davinci-003 with the Azure API vs. with the OpenAI API.</p>
","nlp"
"118124","How to extract embeddings from an audio file using wav2vec along with context","2023-01-28 18:31:45","118305","0","2809","<nlp><feature-extraction><transformer><audio-recognition>","<p>I am trying to use wav2vec embeddings from the XLSR model for emotion recognition on the EMODB dataset. How can I extract embeddings using wav2vec?
I want to use the XLSR model pre-trained with wav2vec, but I am not sure how to extract embeddings from audio files to use for emotion recognition.</p>
<p>I have made attempt like following but they are not correct, this results in random mappings.</p>
<pre><code>feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained('facebook/wav2vec2-large-xlsr-53') #XLSR is for SR, not specifically Emotion Rec. 
input_audio, sample_rate = librosa.load(emodb + file,  sr=16000)
extraction = feature_extractor(input_audio, sampling_rate=16000,  return_tensors=&quot;np&quot;, padding=&quot;max_length&quot;, max_length=max_len).input_values
</code></pre>
<p>Are there any series of steps to follow or libraries or methods I can use to extract the embeddings? Are there any examples or tutorials that I can follow to get started?</p>
","nlp"
"118121","Issues with audio embedding using wav2vec","2023-01-28 15:21:58","118300","1","1192","<nlp><feature-extraction><audio-recognition><tsne>","<p>I am having issues with audio embedding using the wav2vec library while trying to classify emotions using audio signals from the EMODB dataset (Emotions dataset in German). I am using the following code to extract embeddings:</p>
<pre><code>feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained('facebook/wav2vec2-large-xlsr-53') #XLSR is for SR, not specifically Emotion Rec. 
input_audio, sample_rate = librosa.load(emodb + file,  sr=16000)
extraction = feature_extractor(input_audio, sampling_rate=16000,  return_tensors=&quot;np&quot;, padding=&quot;max_length&quot;, max_length=max_len).input_values
</code></pre>
<p>The embeddings and shape of the vectors are:</p>
<ul>
<li><code>(1, 143652)</code> for <code>wav2vec</code> features</li>
<li><code>(3, 162)</code> for <code>mfcc</code> features</li>
</ul>
<p>Please note I have padded them to highest value. The length of audio files is around 1 to 2 seconds.
<a href=""https://i.sstatic.net/bL11B.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bL11B.png"" alt=""enter image description here"" /></a></p>
<p>My intended task is emotion detection. I plan to use these embeddings from audio file, along with the text, for a downstream model for emotion classification, and for this I plan to use multimodal approach, using audio and text embeddings.</p>
<p>So, I trained an LSTM model on these embeddings but it was constantly overfitting on the training data (~100% accuracy and ~20% on testing).</p>
<p>Then I decided to use wav2vec embeddings and MFCC embeddings for a simple classification task using SVM. When I use the resulting embeddings in a simple SVM classifier, I am getting random results (15-30% accuracy) for wav2vec embeddings. As a comparison, when I extract features using MFCC and use them in the same classifier, I am getting an accuracy of around 70%.<a href=""https://i.sstatic.net/u5dbK.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/u5dbK.png"" alt=""MFCC embeddings aren't great either but better they make sense."" /></a></p>
<p>Naturally, I visualized the embeddings using TSNE to check the quality of input and, I found to be getting strange results. Specifically, when I map 7 emotions, the resulting plot forms a spiral shape. When I only map 2 emotions, the resulting plots are different and also strange. The mappings are circular again when I add more features (3+).<a href=""https://i.sstatic.net/HOno6.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/HOno6.png"" alt=""enter image description here"" /></a></p>
<p>I am unable to understand why I am getting these results and why the embeddings are so poor. I am wondering if this is because I am using a general XLSR model without fine-tuning it for emotion recognition.</p>
<p>I would appreciate any suggestions on how to extract features using wav2vec in a better way, or any papers or implementations that may be helpful.</p>
","nlp"
"118055","Classify E-commerce URLs into predefined classes","2023-01-26 14:10:08","","0","24","<classification><nlp><text-classification><regex>","<p>How can I classify an E-commerce URL Page into the following categories,</p>
<ol>
<li>Cart</li>
<li>Payment</li>
<li>Product Page</li>
<li>Checkout</li>
</ol>
<p>How can I achieve this with the url and page title in my hand? I have tried multiple ways but nothing seemed solid. Any help on this?</p>
","nlp"
"118035","Preprocessing advice for large text corpus in natural language generation (NLG)","2023-01-25 19:14:51","118288","1","149","<nlp><preprocessing><text-generation>","<p>I have a large text corpus (i.e. 30 million sentences, all in lowercase in the format of Penn Treebank) that I want to use to train a neural network for natural language generation. What preprocessing steps would you recommend here? The sentences originate from formal text (i.e. books). I plan to use named entity recognition in order to replace named entities such as people, locations, and organisations during training and generation, and adding them back in for the final output. Any other suggestions?</p>
","nlp"
"118023","I have 2 Columns of text, Should I use different vectorizer and Embeddings for each or just one?","2023-01-25 10:33:36","","1","80","<deep-learning><nlp><tensorflow><text-generation>","<p>I have a dataset with two input columns as text. Should I use same textvectorizer in both columns or different ones?</p>
<p>I am asking this because. <code>columns a</code> has average length as 500 words, while <code>column b</code> has average length of 50 words.</p>
<p>what and How would you do it?</p>
","nlp"
"117998","Get the value of second dimension in numpy array","2023-01-24 07:32:37","118009","0","87","<nlp><word-embeddings><numpy>","<p>My NumPy array looks like this</p>
<pre><code>  array([-5.65998629e-02, -1.21844731e-01,  2.44745120e-01,  1.73819885e-01,
         -1.99641913e-01, -9.42866057e-02, ..])]
 ['آؤ_بھگت'
  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., ..])                       ]
 ['آؤلی'
  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0..])                       ]
</code></pre>
<p>When I want to search some specific word I use the built in function</p>
<pre><code>arr_index = np.where(x == 'شعلہ_مزاجی')
print(arr_index)

print(x[arr_index])
</code></pre>
<p>When I print it gives the index, but not the second value</p>
<p>How to get the second value in numpy array?</p>
","nlp"
"117996","NLP topic clustering","2023-01-24 05:57:17","118001","0","180","<nlp><clustering><unsupervised-learning>","<p>In my dataset, I have 500 abstracts. The goal is to cluster them in 2 topics.
<br>One topic must have those abstracts which contain some list of words or similar words and the rest of the abstracts in other topic. <br>Can anyone kindly offer me suggestions to do this?</p>
","nlp"
"117949","Requirements for variable length output in transformer","2023-01-21 21:38:18","117959","1","1691","<nlp><pytorch><transformer><sequence-to-sequence>","<p>I have been working on modifying the transformer from the article <a href=""https://nlp.seas.harvard.edu/2018/04/03/attention.html#a-first--example"" rel=""nofollow noreferrer"">The Annotated Transformer</a>. One of the features I would like to include is the ability to pass a sequence of fixed length, and receive an output sequence of a shorter length, which is possible per <a href=""https://datascience.stackexchange.com/questions/45475/variable-input-output-length-for-transformer"">this reference</a>.</p>
<p>In my case, I am using a sequence of 10 randomly generated integers 0-9 for the input (just like the article) and trying to return a sequence of five 2s (this is the simplest attempt to get an output of a shorter length I could think of). The start of the sequence is denoted as 1, and the end of the sequence is not defined.</p>
<p>I am successfully able to send the encoder the &quot;source&quot; batch tensor, and the decoder the &quot;target&quot; batch tensor consisting of only 5 columns in the batch tensor. The transformer will train on this data, but it returns a sequence of length equal to the input.</p>
<p>What are the requirements of the transformer network to output a sequence of length that is not equal to the length of the input sequence?</p>
<p>Thanks in advance for any assistance</p>
","nlp"
"117920","Similarity with respect to a specific concept in text embeddings","2023-01-20 15:38:59","","0","133","<machine-learning><nlp><word-embeddings><similarity><vector-space-models>","<p>In text embeddings, cosine similarity is often used to find texts similar to a search query.
However, I don't want to find a text that is overall similar, but similar with regards to a specific concept (which I can also embed).</p>
<p><strong>Example:</strong> Let's say you have many movie reviews that you have embedded. You choose one review and want to search for similar ones with regard to the cinematography only.</p>
<p>More formally stated, my problem is the following:
Let <span class=""math-container"">$x_1, \dots, x_n \in \mathbb{R}^d$</span> vector embeddings of texts <span class=""math-container"">$t_1, \dots, t_n$</span>. Further, let <span class=""math-container"">$q \in \mathbb{R}^d$</span> be the embedding of a search query. I want to rank the texts by similarity with the search query with regards to a specific concept/aspect with embedding <span class=""math-container"">$k \in \mathbb{R}^d$</span>.</p>
<p>I thought about using the embedding <span class=""math-container"">$k$</span> of the concept, like &quot;cinematography&quot; and then projecting the text embeddings onto that direction
<span class=""math-container"">$$S_C \propto \langle P_k q, P_k x_i\rangle$$</span></p>
<p>but that does not make sense as cosine similarity looks at the angle.</p>
<p>One could project just one of the text embeddings
<span class=""math-container"">$$S_C \propto \langle P_k q, x_i\rangle = S_c(k, q) \cdot S_c(k, x_i)$$</span>
but this amounts to multiplying the cosine similarities of the concept with both text embeddings.</p>
<p>I did not find any research literature on this specific problem.
Is there a way to align the cosine similarity to a specific concept/context?</p>
","nlp"
"117892","model interaction between words for a sentiment analysis task","2023-01-19 12:20:23","","0","25","<nlp><logistic-regression><sentiment-analysis><feature-interaction>","<p>I am wondering what is the most appropriate way to model the interaction between two words/variables in a language model for a sentiment analysis task. For example, in the following dataset:</p>
<pre><code>You didn't solve my problem,NEU
I never made that purchase,NEU
You never solve my problems,NEG
</code></pre>
<p>The words &quot;solve&quot; and &quot;never&quot;, in isolation, doesn't have a negative sentiment. But, when they appear together, they do. Formally speaking: assuming we have a feature «solve» that takes the value 0 when the word «solve» is absent, and 1 when the word is present, and another feature «never» with the same logic: the difference in the probability of Y=NEG between «solve»=0 and «solve»=1 is different when «never»=0 and «never»=1.
But a basic logistic regression (using, for example, <code>sklearn</code>), wouldn't be able to handle this kind of situation (it doesn't add interaction terms).</p>
","nlp"
"117885","Train Word Embeddings on new vocabulary given the pre trained embeddings through word2vec","2023-01-19 09:43:50","","0","450","<deep-learning><nlp><word-embeddings><word2vec>","<p>I have the pre-trained Embbedings on the language. I have the vocabulary for that language, what would be the pipeline to train this vocabulary by using Pre train embeddings through the word2vec model?</p>
","nlp"
"117858","How can I make my neural network learn faster?","2023-01-18 14:54:19","","2","109","<nlp><lstm><optimization><gpu><tpu>","<p>I would like to train an LSTM-based variational autoencoder on a large dataset (37 million sentences). However, I have calculated that my training speed as of now is too slow (on Google Colab). I am using a GPU provided by Google called A100-SXM4-40GB, and my framework of choice is Pytorch. I am already using automatic mixed precision, which sped up my code by about x2 (<a href=""https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html"" rel=""nofollow noreferrer"">https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html</a>). As a reminder, here is a stock image of a variational autoencoder:<a href=""https://i.sstatic.net/JoVV2.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/JoVV2.png"" alt=""VAE"" /></a></p>
<p>With my current training speed, I get through about <strong>130 million training examples/sentences in 24 hours</strong>. My vocabulary size is about 85,000, the number of parameters in the VAE is 17.1 million (mostly high because of the embedding layer), my encoder has 100 and my decoder 600 hidden neurons. My batch size is 64, and I am using the Adam optimizer. I have plotted the time usage of different parts of my code after 3000 batches: <a href=""https://i.sstatic.net/euhKo.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/euhKo.png"" alt=""Time usage of VAE in percent"" /></a>.
Following, you can see a more detailed breakdown of my model times:
<a href=""https://i.sstatic.net/pbevU.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/pbevU.png"" alt=""Percentage of time used by different parts of the VAE"" /></a></p>
<p>What advice can you give me to speed up my model? For instance, I also have access to a TPU, but I have never seen a clear breakdown on GPU vs TPU performance (and what role batch size plays). Can I use parallel computing, and is this possible on Colab? I am training for 5 epochs, leading to a total estimated training time of about 34 hours.</p>
<p>With regards to the detailed model training breakdown, please note:</p>
<ul>
<li>I am already using pack_padded_sequence for the encoder</li>
<li>The decoder is an LSTM cell (less optimised). I need to pass the last encoder hidden state (after reparametrization) to the decoder at each time step, together with the previous output of the cell state for the LSTM. I believe this is following <a href=""https://cs224d.stanford.edu/reports/OshriBarak.pdf"" rel=""nofollow noreferrer"">4</a></li>
</ul>
<p>EDIT: I updated my total run time (made a miscalculation)</p>
","nlp"
"117841","Short Text Topic Modelling in Python","2023-01-17 23:16:21","","1","206","<python><nlp><topic-model><lda>","<p>I have a large dataset of short reviews and I would like to find the most recurring themes. For this reason, I got into topic modeling.</p>
<p>I am looking for some good tutorials and references for short text topic modeling.
I have seen that there exists a lot of material for LDA but when it comes to short texts there is not much, especially for python implementations.
I have found this <a href=""https://stackoverflow.com/questions/62175452/topic-modeling-on-short-texts-python"">StackOverflow issue</a> but is more than 2 year and a half old.</p>
<p>So, I would like to know if something more recent exists.
I know that there is a transformer-based model called <a href=""https://maartengr.github.io/BERTopic/index.html"" rel=""nofollow noreferrer"">BertTopic</a> that should perform reasonably well even on short text. Let me know if there is something that I am missing.</p>
<p>Thank you!</p>
","nlp"
"117832","What is the best approach to deploy N number of ML models as a scalable service in the Cloud?","2023-01-17 15:58:52","","2","41","<nlp><transformer><aws><deployment><google-cloud-platform>","<p>I've <strong>N (~50)</strong> number of sentiment models of different languages, which were fine tuned on HggingFace's transformer models. Each of the models as 2-3 GB in size approx. Now, how can I deploy all these sentiment models as a scalable service in a cloud platform like GCP, so that the bill is optimized and the service performance (low inference time, or latency) is maximized.</p>
<p>Currently we're deploying each of our models as a separate service. For each of the model we're following the below steps.</p>
<ol>
<li><strong>Develop the service using Flask</strong>: We write the code for our service, including routes and logic for handling requests.</li>
<li><strong>Create a Dockerfile</strong>: A docker file is created to build a Docker image of our service.</li>
<li><strong>Build the Docker image</strong>: We build the Docker image of our service.</li>
<li><strong>Push the Docker image to GCR</strong>: We create a new repository in GCR and
push the Docker image to it.</li>
<li><strong>Create a GKE Cluster</strong>: We go to the Kubernetes Engine console and create a new cluster. Select the appropriate number of nodes and configure the desired resources.</li>
<li><strong>Create a GKE Deployment</strong>: We create a new deployment and associate it
with the image from our GCR repository and configure the desired
number of replicas.</li>
<li><strong>Create a Cloud Load Balancer</strong>: We go to the Google Cloud Console and create a new Cloud Load Balancer. Select the GKE deployment we created in step 6 as the target for the Load Balancer.</li>
<li><strong>Update your DNS to point to the Load Balancer</strong>: Then we update our DNS settings to point to the IP address of the Load Balancer created in step 7.</li>
<li><strong>Monitor the service</strong>: We use Stackdriver to monitor the service and ensure that it is running smoothly and that the desired number of replicas are running.</li>
<li><strong>Scale the service</strong>: When necessary, we use the Auto Scaling feature of GKE to automatically scale the number of replicas running your micro-service based on incoming traffic or other metrics.</li>
</ol>
<p>We follow the same steps for each of our models and deploy the models as a dedicated service. However, this approach costs us a lot of money at the end of the month.</p>
<p>So, suggest me a better way to deploy such multiple models as a service in a scalable manner so that the cloud bill is optimized, but the performance is maximized.</p>
","nlp"
"117828","How to automatically classify a sentence or text based on its context?","2023-01-17 13:34:14","117833","3","1876","<machine-learning><classification><nlp><text-mining><text-classification>","<p>I have a database of sentences which are about different topics. I want to automatically classify each sentence with the one or more relevant tags based on the context of the sentence as shown below:</p>
<p><strong>Sentence</strong>: The area of a circle is pi time the radius squared</p>
<p><strong>Expected tags</strong>: mathematics, geometry</p>
<p>Is there any python library or pre-trained model to generate such tags?</p>
","nlp"
"117815","How to classify text and predict if it belongs to the group or not?","2023-01-17 06:15:27","","2","155","<nlp><supervised-learning>","<p>I am basically Python Postgres programmer and new to datas science and its tools.</p>
<p>I have around 78 million records which contains information like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>CostCenter</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>110000032</td>
<td>Hiring of vehicles</td>
</tr>
<tr>
<td>110000032</td>
<td>Hired vehicles</td>
</tr>
<tr>
<td>110000032</td>
<td>Vehicles hired</td>
</tr>
<tr>
<td>110000032</td>
<td>Hiriing of vehicles</td>
</tr>
<tr>
<td>110000032</td>
<td>Pay bill</td>
</tr>
<tr>
<td>110000033</td>
<td>Hiring of vehicle</td>
</tr>
</tbody>
</table>
</div>
<p><code>CostCenter</code> is pre-defined (Master data) and <code>Description</code> is user entered field and there is no uniformity in what is entered into description.</p>
<p><strong>What is required:</strong>
I want to have a binary split of those, where Hiring of Vehicles (all like matches) - 110000032 are true and remaining all false.</p>
<p>The false entries from the above would be 110000032 - Paybill (32 CostCenter is for Hiring of Vehicles) and 110000033 - Hiring of vehicle (33 is not for hiring of vehicle).</p>
<p><strong>What I have tried so far:</strong>
Using RegEx is not useful because, I dont know the different patterns of free text (Description) entered by users.</p>
<p>I have used FTS (Full Text Search) of Postgre and tokenized the Description field and get all like matches for HIRE and VEHICLE.</p>
<p>What I am looking to do is, train a model so that based on the description, the system can display records whose CostCenter is not matching.</p>
<p>What is the correct approach? Supervised Learning? <a href=""https://developers.google.com/machine-learning/recommendation/collaborative/basics"" rel=""nofollow noreferrer"">Collobarative Filtering?</a> NLTK NLP?</p>
<p>Edited: I have around 3000 unique costcenters in 99 different combinations of the last 2 digits.</p>
<p>Thanks</p>
","nlp"
"117770","Are there any approaches in machine learning (neural or otherwise) that can learn relations/non-deterministic functions?","2023-01-14 23:01:08","","1","35","<machine-learning><neural-network><nlp>","<p>A lot of the approaches in machine learning (specifically natural language processing) to me seem to be focused on inferring the &quot;most likely&quot; output based on a given input. For instance, in training a neural net to parse natural language sentences, as I understand it, the corpora is annotated such that for each input sentence, there is exactly one parsed representation.</p>
<p>So a neural-net trained parser (for instance) would only be able to obtain a single parse of a sentence like &quot;I killed the man with the spoon&quot; when in fact this sentence (like many sentences in natural language) is ambiguous, and has multiple possible parses with different semantic interpretations.</p>
<p>This seems problematic to me for many natural language understanding problems -- for instance, if a user is trying to communicate with a chatbot, and what they are trying to say in unclear (due to a syntactic or other kind of ambiguity), they would not be able to ask for clarification from the user, but rather be tied to a single parse based on the output from the model.</p>
<p>Are there any machine learning techniques that are used in natural language processing to get around this problem? I've heard of relational neural nets, but I don't think that's exactly what I'm looking for here -- or at least I don't directly see how I would apply them to this sort of problem.</p>
<p>What I'm looking for is rather than training a model <code>raw_tokenized_sentence -&gt; parse_tree</code>, I'd want to train a model more like</p>
<pre><code>raw_tokenized_sentence -&gt; list_of_parse_trees_with_probabilities_of_how_likely_a_parse_they_are
</code></pre>
<p>Has anyone specifically in the field of natural language processing looked into such things? And if not, is there some kind of machine learning algorithm which would be able to learn a non-deterministic function like this that I desire?</p>
<p>Something similar to this can be done with more procedural approaches where from a sentence, you can construct an algorithm returning a list of all possible parses (something I've implemented before for various grammars/lexicons), but the machine-learning approach to this problem has a lot of advantages, as it is not entirely clear how a notion such as <em>the probability of a parse</em> would be determined without training on real-world data.</p>
","nlp"
"117745","Gensim doc2vec error: KeyError: ""word 'senseless' not in vocabulary""","2023-01-13 12:02:03","","3","873","<nlp><word-embeddings><gensim><similar-documents><doc2vec>","<p>I am new to machine learning and tried doc2vec on quora duplicate dataset. new_dfx has columns 'question1' and 'question2' which has preprocessed questions in each row. Following is the tagged document sample:</p>
<p>input:</p>
<pre><code>q_arr = np.append(new_dfx['question1'].values, new_dfx['question2'].values)
tagged_data1 = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(q_arr)]
tagged_data1[50001]
</code></pre>
<p>output:</p>
<pre><code>TaggedDocument(words=['senseless', 'movi', 'like', 'dilwal', 'happi', 'new', 'year', 'earn', 'easi', '100', 'crore', 'india'], tags=['50001'])
</code></pre>
<p>Input:</p>
<pre><code>model_dbow1 = Doc2Vec(dm=1, vector_size=300, negative=5, workers=cores)
model_dbow1.build_vocab([x for x in tqdm(tagged_data1)])
train_documents1  = utils.shuffle(tagged_data1)
model_dbow1.train(tagged_data1,total_examples=len(train_documents1), epochs=30)
</code></pre>
<p>-- to check if model trained right</p>
<pre><code>model_dbow1.most_similar('senseless')
</code></pre>
<p>Error:</p>
<pre><code>KeyError: &quot;word 'senseless' not in vocabulary&quot;
</code></pre>
<p>The data I have given to model for training as input has the word &quot;senseless&quot; so why this error? Could anyone please help?<a href=""https://i.sstatic.net/Vvg3o.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Vvg3o.png"" alt=""Other word is giving output"" /></a></p>
","nlp"
"117732","Is n-gram a special instance of bag of word? What are their differences?","2023-01-13 02:27:30","","1","666","<nlp><representation><ngrams><bag-of-words>","<p>Is n-gram a special instance of bag of word? What are their differences? From my understanding, n-gram is when replacing the words in bag of words with n-grams, and follow the same procedures to generate the word vector.</p>
<p>I tried to verify the above understanding, however, I find nothing supporting this. Thus, I wonder maybe there exists some subtle difference in the procedure of calculating the word vectors that I'm not aware of?</p>
<p>Would appreciate answers that directly compare and contrast these two methods in word representation. Thanks.</p>
","nlp"
"117724","Which pre-trained model to select to generate embeddings from shop names written in English?","2023-01-12 16:21:21","","1","102","<python><nlp><clustering><word-embeddings><embeddings>","<p>Good afternoon!</p>
<p>I have a dataset with thousands of shop names written in English. Several shop names might belong to one business entity, for instance, shops with names &quot;KFC 001&quot;, &quot;WWW.KFC.COM&quot; and &quot;KFC LITTLE STORE&quot; might belong to KFC.</p>
<p>I want to make a clustering model to group specific shops by their names similarity into business entities as in the example described above. So I want to encode shop names someway, each shop name to some vector. Shop names might be rather long (30-40 letters), the names might contain uppercase English letters, numbers and special symbols.</p>
<p>My question is which pre-trained model would you recommend to generate vector embeddings for my purpose from shop names? Important features the modell shall have:</p>
<ol>
<li>The model shall someway save the info about order of the symbols in the words</li>
<li>The model shall save the info about the symbols themselves</li>
</ol>
<p>So what would be your advice?</p>
","nlp"
"117683","NLP vs Keyword-Search. which one is the best?","2023-01-11 02:15:48","117698","0","112","<nlp><predictive-modeling><text-mining><text-classification><named-entity-recognition>","<p>I have constructed a natural language processing (NLP) model with the aim of identifying technology keywords within text. The model is trained on a large dataset that contains over 400,000 phrases and has been annotated with approximately 1000 technology keywords, of which only the keywords that I provided in the dataset, can be identified. The annotations within the training dataset include specific locations of technology keywords in the phrases, like for example in the below example, the technology keyword &quot;php&quot; is located at positions 0-3 and 43-46.</p>
<pre><code>TainingData = [ ('php search upperlower case mix word string php , regex , search , pregmatch , strreplace',
  {'entities': [[0, 3, 'php'], [43, 46, 'php']]}),
 ('create access global variables groovy groovy',
  {'entities': [[31, 37, 'groovy'], [38, 44, 'groovy']]}),
 ('asp.net mvc 2.0 application fail error parameterless constructor define object asp.net , asp.netmvc , asp.netmvc2',
  {'entities': [[0, 7, 'asp.net'],
    [79, 86, 'asp.net'],
    [89, 99, 'asp.netmvc']]}),
 ('question regular servlets within gwt work dev mode work deployment tomcat java , gwt , servlets , fileupload',
  {'entities': [[74, 78, 'java']]}),
 ('display type ive create use create type postgresql database , postgresql , type , export',
  {'entities': [[40, 50, 'postgresql'], [62, 72, 'postgresql']]}),
 ('compare date specific one datetime string twig php , twig',
  {'entities': [[42, 46, 'twig'], [47, 50, 'php'], [53, 57, 'twig']]}),
 ('ie display simple js alert javascript , internetexplorer7 , parallel',
  {'entities': [[27, 37, 'javascript']]}),
 ('differences basehttpserver wsgiref.simple_server python , basehttpserver , wsgiref',
  {'entities': [[49, 55, 'python']]})
]
</code></pre>
<p>An alternative very simple approach that I considered is to manually search for the technology keywords within the text, by using a set of predefined keywords and checking the text in a loop. However, I am wondering of which approach would be more efficient and effective. Given the large dataset and a vast number of keywords, I wonder which of these two methods would yield the best results and would be more appropriate.</p>
","nlp"
"117670","Using the whole GloVe pre-trained embedding matrix or minimize the matrix based on the number of words in vocabulary","2023-01-10 14:01:00","","1","173","<deep-learning><nlp><lstm><rnn><word-embeddings>","<p>I have created a neural network for sentiment analysis using bidirectional LSTM layers and pre-trained GloVe embeddings.</p>
<p>During the training I noticed that the <code>nn.Embedding</code> layers with the freezed embedding weights uses the whole vocabulary of GloVe:</p>
<p>(output of the instantiated model object)
<code>(embedding): Embedding(400000, 50, padding_idx=0)</code></p>
<p>Also the structure of the nn.Embedding layer:<br>
<code>self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=True, padding_idx=self.padding_idx)</code></p>
<p>, where <code>embedding_matrix = glove_vectors.vectors</code> object and <code>glove_vectors = torchtext.vocab.GloVe(name='6B', dim=50)</code></p>
<p>400,000 is the shape of glove_vectors object (meaning 400,000 pre-trained words in total).</p>
<p>Then I noticed that the training of the LSTM neural network took approximately 3 to 5 minutes per epoch. Which is quite too long for only 150,000 trainable parameters. And I was wondering if this had to do with the use of the whole embedding matrix with 400,000 words or it’s normal because of the bidirectional LSTM method.</p>
<p>Is it worth to create a minimized version of the GloVe embeddings matrix from the words that only exist in my sentences or using the whole GloVe embeddings matrix it does not affect the training performance?</p>
","nlp"
"117663","Quantitatively evaluate similarity between two corpus of texts","2023-01-10 09:36:46","","1","207","<nlp><text-mining><data-analysis><corpus>","<p>I want to assess how similar, or different two corpora are, and if the similarity is statistically significant. Something close to a Kolmogorov–Smirnov test in statistics, but for text data.</p>
<p>For additional context, two corpora are related to the same event, and one corpus is subsequently larger than the other.</p>
<p>Any leads /suggestions on this would be greatly appreciated. Thanks</p>
","nlp"
"117650","Who the article is about","2023-01-09 18:09:51","","0","53","<nlp><spacy><gpt>","<p>I have a problem that I need to solve. It involves articles about football. I have to determine who is the main protagonist in the article. I already have a solution that I have implemented. Its good enough. But I need to improve it further by using latest NLP solutions.</p>
<p>The current solution is, use coreference resolution to replace the pronouns with their actual coreferents. Then the output article/text is then passed to NER model to get the entities extracted. Then I simply count for either PER or ORG. Then take the entity with the maximum counts.</p>
<p>Any more ideas?</p>
","nlp"
"117634","What does Embeddings Array Represent in BERT's Feature Extraction?","2023-01-09 09:08:20","","0","216","<nlp><word-embeddings><transformer><bert>","<p>I am new to academic NLP, and I had been tasked with to use BERT to extract features of a sentence.</p>
<pre><code>text_input = [
  &quot;Hello I'm a single sentence&quot;,
  &quot;And another sentence&quot;,
  &quot;And the very very last one&quot;,
  &quot;My name is Aun&quot;
  ]
</code></pre>
<p>I got embeddings using pipeline from huggingface:</p>
<pre><code>from transformers import pipeline
feature_extraction = pipeline('feature-extraction', model=&quot;distilroberta-base&quot;, tokenizer=&quot;distilroberta-base&quot;)
features = feature_extraction(text_input)
</code></pre>
<p>Embeddings were multi-dimension, which I flattened and then padded to match the array with highest size. Here <code>text_df.head()</code>:</p>
<pre><code>    text_input                  text_embeddings                                 text_em_flat                        text_em_flat_pad
0   Hello I'm a single sentence [[[-0.010155443102121353, 0.07965511828660965,...   [-0.010155443102121353, 0.07965511828660965, 0...   [-0.010155443102121353, 0.07965511828660965, 0...
1   And another sentence        [[[-0.010256338864564896, 0.0948348417878151, ...   [-0.010256338864564896, 0.0948348417878151, -0...   [-0.010256338864564896, 0.0948348417878151, -0...
2   And the very very last one  [[[-0.001137858722358942, 0.09048153460025787,...   [-0.001137858722358942, 0.09048153460025787, -...   [-0.001137858722358942, 0.09048153460025787, -...
3   My name is Aun              [[[-0.0018534815171733499, 0.08652304857969284...   [-0.0018534815171733499, 0.08652304857969284, ...   [-0.0018534815171733499, 0.08652304857969284, ...
</code></pre>
<p>But I don't understand what each value represents in the text_embeddings. I have gone through some explanation, but don't understand if they are token level or segment level or position level embeddings for a stack of all three. Please explain.
Following are the shapes for few instances:</p>
<pre><code>arr_dimen(text_df['text_embeddings'][0]):    [1, 8, 768]
arr_dimen(text_df['text_embeddings'][1]):    [1, 5, 768]
arr_dimen(text_df['text_embeddings'][2]):    [1, 8, 768]
arr_dimen(text_df['text_embeddings'][3]):    [1, 7, 768]
arlen(text_df['text_em_flat'][0]):   6144
arlen(text_df['text_em_flat'][1]):   3840
arlen(text_df['text_em_flat'][2]):   6144
arlen(text_df['text_em_flat'][3]):   5376
</code></pre>
<p>From original paper I understand that BERT divides input in three-layers and then uses them like shown in the figure from original paper. <a href=""https://i.sstatic.net/XPWhh.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/XPWhh.png"" alt=""enter image description here"" /></a></p>
<p>But I want to understand how BERT encodes <code>My name is Aun</code> to an array with shape <code>[1, 7, 768]</code>.</p>
","nlp"
"117619","ChatGPT with multilingual language","2023-01-08 15:23:10","120945","1","142","<nlp><language-model>","<p>How it can answer with multilingual langauge ?</p>
<p>Maybe it's because it use GPT-3.5, since their dataset has Common Crawl dataset, which has more than 40+ languages.</p>
<p>And as I found that InstructGPT's paper shows that GPT-3 can handle this task as well, but it needs to use more careful prompts.</p>
<p>Maybe I should read the paper &quot;PaLI: A Jointly-Scaled Multilingual Language-Image Model&quot; that has similar work with multilingual language model's work.</p>
","nlp"
"117526","Seeking Example CLIP Model Code Allowing Me to Use a Pretrained Model and Go Directly to Inference Without any Additional Training","2023-01-05 00:54:04","","1","302","<nlp><computer-vision><language-model>","<p>I have been endlessly searching for open-source code (including from OpenAI themselves) which would allow me to take a pretrained CLIP model image encoder (e.g., the ViT B/32) and then solely based on the weights from the pretrained image encoder directly attempt to infer either a corresponding text description &quot;label&quot; from an image I feed the encoder, or vice-versa, will allow me to provide it a prompt after which it will return an image based on that prompt.</p>
<p>Is this even possible without taking some additional dataset of image-text pairs to further train the pretrained CLIP image encoder on?  All the code examples I have come across provide you the pretrained image encoder - but then require you to do MORE training using an extra dataset that (presumably) was never used during pretraining. Some of these code examples then allow you to provide a text prompt, in response to which the model will return an image (from the dataset you fed it during the training process) which aligns with your prompt.</p>
<p>I.e., I don't want to do any extra training. I just want the pretrained image encoder, then provide that image encoder with 1 image, and then have it return to me a matching text description of it. Alternatively, provide the pretrained (not re-trained) image encoder with a text prompt, and have it return to me an image that it was originally pretrained on (or at least an image that it was not re-trained on).</p>
<p>Some examples of code that <strong>ARE NOT</strong> what I am looking for (although very well done) include the following:</p>
<p><a href=""https://github.com/moein-shariatnia/OpenAI-CLIP/blob/master/.ipynb_checkpoints/OpenAI%20CLIP%20Simple%20Implementation-checkpoint.ipynb"" rel=""nofollow noreferrer"">https://github.com/moein-shariatnia/OpenAI-CLIP/blob/master/.ipynb_checkpoints/OpenAI%20CLIP%20Simple%20Implementation-checkpoint.ipynb</a></p>
<p><a href=""https://towardsdatascience.com/quick-fire-guide-to-multi-modal-ml-with-openais-clip-2dad7e398ac0"" rel=""nofollow noreferrer"">https://towardsdatascience.com/quick-fire-guide-to-multi-modal-ml-with-openais-clip-2dad7e398ac0</a></p>
<p><a href=""https://colab.research.google.com/github/openai/CLIP/blob/main/notebooks/Interacting_with_CLIP.ipynb"" rel=""nofollow noreferrer"">https://colab.research.google.com/github/openai/CLIP/blob/main/notebooks/Interacting_with_CLIP.ipynb</a></p>
<p>Thank you.</p>
","nlp"
"117491","In ChatGPT, what is the difference between Reinforcement-Learning-from-Human-Feedback and Data-Re-Label?","2023-01-04 02:16:19","","1","247","<machine-learning><deep-learning><nlp><gpt>","<p><a href=""https://openai.com/blog/instruction-following/"" rel=""nofollow noreferrer"">Reinforcement-Learning-from-Human-Feedback</a> vs <a href=""https://guotong1988.github.io/research/2021/12/01/relabel-is-all-you-need/"" rel=""nofollow noreferrer"">TrainingData-Label-Again</a>.</p>
","nlp"
"117472","ChatGPT and my PhD research","2023-01-03 14:03:21","","1","259","<python><nlp><gpt>","<p>I am currently a PhD student in the field of NLP and I can see a way how ChatGPT can solve my current research question. My research question is related to reasoning based on text. What can I possibly do to ensure that ChatGPT would not disrupt my research question? What are potential flaws of ChatGPT where we can focus on?</p>
","nlp"
"117458","Compound and Complex Sentence Tokenization","2023-01-02 19:45:24","","0","102","<nlp><transformer><sentiment-analysis>","<p>I am trying to tokenize sentences of a document for aspect-based sentiment analysis. There are some sentences that consist of more than one topic. A couple of examples:</p>
<ul>
<li>&quot;The touch screen is good but the battery is weak&quot;</li>
<li>&quot;Their smartphones are great and their TVs are perfect&quot;</li>
</ul>
<p>I want to tokenize sentences based on these conjunctions. Is there any pre-trained model for this task? Are there any other solutions?</p>
","nlp"
"117444","What size language model can you train on a GPU with x GB of memory?","2023-01-02 01:14:52","118875","8","10155","<nlp><gpu><language-model><memory>","<p>I'm trying to figure out what size language model I will be able to train on a GPU with a certain amount of memory. Let's for simplicity say that 1 GB = 10<sup>9</sup> bytes; that means that, for example, on a GPU with 12 GB memory, I can theoretically fit 6 billion parameters, given that I store all parameters as 16-bit floats. However, in order to use a language model, you typically also need space for storing the input text and the activations of the current layer (and maybe also of the previous layer), and if you are going to train the model, you will typically need space to store the activations of all layers in order to be able to do backpropagation, and if you use an optimizer like Adam, you need space to store the running mean of the partial derivatives (of the loss function with respect to the various parameters, or in other words, the gradient), as well as the running mean of the squares of the partial derivatives.</p>
<p>So, given this complication, could someone tell me what size language models (that is, how many parameters) I will be able to train on a GPU with</p>
<ol>
<li>10 GB of memory (RTX 3080 10 GB)?</li>
<li>12 GB of memory (RTX 3080 12 GB and RTX 3080 Ti)?</li>
<li>16 GB of memory (RTX 4080)?</li>
<li>24 GB of memory (RTX 3090 and RTX 3090 Ti)?</li>
</ol>
<p>For example, Tim Dettmers mentions in <a href=""https://timdettmers.com/2020/09/07/which-gpu-for-deep-learning/#When_do_I_need_11_GB_of_Memory"" rel=""nofollow noreferrer"">his blog</a> that you should have at least 24 GB of memory if you do research on transformers. I'm guessing this translates roughly to a language model of a certain size.</p>
","nlp"
"117435","Which Feature Selection Techniques for NLP is this represent","2023-01-01 03:32:10","","0","56","<nlp><feature-selection><feature-engineering><feature-extraction>","<p>I have a dataset that came from NLP for technical documents</p>
<p>my dataset has <strong>60,000</strong> records</p>
<p>There are <strong>30,000</strong> features in the dataset</p>
<p>and the value is the number of repetitions that word/feature appeared</p>
<p>here is a sample of the dataset</p>
<pre><code>RowID       Microsoft  Internet  PCI  Laptop  Google  AWS  iPhone  Chrome
1              8          2       0      0      5      1      0       0
2              0          1       0      1      1      4      1       0
3              0          0       0      7      1      0      5       0
4              1          0       0      1      6      7      5       0
5              5          1       0      0      5      0      3       1
6              1          5       0      8      0      1      0       0

-------------------------------------------------------------------------
Total          9,470     821      5     107     4,605  719    25      8
Appearance
</code></pre>
<p>There are some words that only appeared less than <strong>10</strong> times in the whole dataset</p>
<p>The technique is to select only words/features  that appeared in the dataset for more than a certain number (say 100)</p>
<p>what is this technique called? the one that only uses features that in total appeared more than a certain number.</p>
","nlp"
"117426","How to do unsupervised clustering on sentences to find intents","2022-12-30 21:45:50","","1","35","<deep-learning><nlp><clustering><data-mining><chatbot>","<br>
I am working on chatbot for students.<br>
So, I have chatlogs on conversation between student and tutor, which is on mathematical problems (no labels).<br>
First thing I want to find is intents, such as whether given line is problem description from student , doubt from student or greetings. <br><br>For which I tried with <b>sentence transformer with math bert</b> to create embeddings and use <b>umap</b> with cosine as metric to do dimensional reduction, and then cluster them using <b>hdbscan</b> to find all intents possible, but the results weren't that great.<br>
Also, tried with zero-shot classification with bart, which did not give better results.
<p>Any idea on what could be better approach?</p>
","nlp"
"117391","Which of these 2 approaches is the best route to learn to build a question answer chatbot?","2022-12-29 16:09:51","","0","45","<nlp><sequence-to-sequence><gpt><chatbot>","<h2>Quick background on what I am trying to accomplish:</h2>
<p>I have been working on a project in my company that requires about 300 people across the world to follow quite a large set of rules and guidelines. instead of sharing documents that people can reference for these rules, I am looking to build a chatbot. This chatbot will take all my documents as inputs and then can be used by the end users to answer any of their questions related to processes and guidelines</p>
<h2>What research I have done so far:</h2>
<p>Broadly, I believe my 2 main routes are</p>
<ol>
<li>build a chatbot from scratch. I found a good Udemy course on seq2seq architecture that can allow me to build this chatbot</li>
<li>Build an application(chatbot) on top of existing algorithms like GPT-3 or BERT</li>
</ol>
<h2>What help I need:</h2>
<ol>
<li>I am confused which of the above 2 is the best approach. I guess I need more information on the pros and cons of both.</li>
<li>Is 1) is the right approach, is seq2seq too outdated to learn about it now? Should I find a course that uses another architecture?</li>
<li>If 2) is the right approach, I am struggling to find a resource that can teach me how to build this chatbot including a simple GUI</li>
</ol>
<p>Note: I have experience working with CNN and a little bit with RNN as well. I have extensive experience with Python, none with HTML or Java</p>
","nlp"
"117374","Cluster URLs based on their pattern","2022-12-29 01:36:16","","1","589","<machine-learning><deep-learning><nlp><clustering><pattern-recognition>","<p>I am new to clustering techniques and I highly value any input you can provide for my problem bellow.
Basically, I want to cluster URLs based on their structural patterns.
for example</p>
<ul>
<li>cluster1 - simple URLs https://domain/path/file</li>
<li>cluster2 - shortened URLs</li>
<li>cluster3 - redirect URLs</li>
<li>....</li>
<li>cluster k - new URL pattern</li>
</ul>
<p>Given a URL dataset, I want to understand how many different URL pattern clusters exists and then visually see the difference.</p>
<p>What I see in the existing methods are clustering domain wise (cluster URLs of the same website together). And this is not what I am expecting. When I try the nlp based (word based) similarity clustering this is happening as the URLs of the same website tend to have same words with little differences.</p>
<p>Instead, I want to focus on the URL structure and identify URL patterns. Removing all the special characters and just creating a bag of words for each URL nullify the URL structure. Can anyone help me to identify a suitable clustering technique as well as a vectorizing technique to identify different URL pattern clusters.</p>
<p>Thanks in advance
Matheesha</p>
","nlp"
"117373","Should i remove french special characters and apostrophes","2022-12-29 00:22:09","117384","0","288","<nlp><data-cleaning><preprocessing><bert>","<p>I am working on a french text preprocessing task, in order to prepare the data to train an NLP model. But I do not know if it is better to remove french special characters and apostrophes or keep them. Example:</p>
<pre><code>Malgré que j'ai tellement aimé ce boulot je veut démissionner
</code></pre>
<p>Becomes</p>
<pre><code>Malgre que jai tellement aime ce boulot je veut demissionner
</code></pre>
<p>I have also noticed that most lemmatization libraries for french text are not at all efficient so i was wondering if I could skip this step, and also skip the stopwords removal step.
In general the preprocessing steps will be :</p>
<ol>
<li>Remove URLs and Emails</li>
<li>Demojize Emojis</li>
<li>Transform number into text (6-&gt;six)</li>
<li>Removal of all special characters including french special characters</li>
</ol>
","nlp"
"117356","Models that are good for long answer generation given context and question and what datasets would be the best for training?","2022-12-28 11:04:08","117375","0","222","<nlp><data><model-selection><text-generation><question-answering>","<p>Basically I am trying to create a context-needing question and long answer model and I was wondering what model would be best for such tasks, currently I am leaning towards T5, or GPT-NeoX-20B. Additionally for such tasks what datasets would be the most suitable, as of right now I have looked at CoQA, and SQuAD but none provide long answers.</p>
","nlp"
"117307","Is there any datset which has human judgement scores between 2 english senctences only?","2022-12-26 02:21:46","","0","29","<machine-learning><nlp><dataset>","<p>I am trying to propose an evaluation metric in NLP and I need to compare it with other baseline metric such as Bleu scores.</p>
<p>I need a dataset which has 2 english sentences (say same image being described by 2 different persons) and along with that a human judgement score on the quality of those sentences.</p>
<p>Ay inputs will be very much appreciated. I'm willing to calculate pearson correlation coeff between blue score and human judgement score; and between proposed metric and human judgement score to see which is giving a higher correlation coeff.</p>
<p>Could you specify the direct link to download such txt/ csv file if there exists one.</p>
<p>I found wmt metrics dataset that is a translation between different langauges. I am looking for 2 english sentences only along with a human judgement score</p>
","nlp"
"117293","Combining sentence embeddings of two different models (sBERT and mBERT)","2022-12-25 02:49:52","117311","0","985","<nlp><bert><huggingface>","<br>
I am working on a chatbot that helps students. <br>
So, I wanted to make use of bert model which has better performance on mathematics, which lead to me to math-bert, but the paper on it said that it was trained only on mathematical corpus, which means it wont have great performance on general sentences (example in image), so is there a method to combine sentence-bert and math-bert?
<br>[![enter image description here][1]][1]
<br>Or, the only way is to train bert model from scratch using corpus used for sentence-bert and math-bert.
","nlp"
"117260","NLP Question: Where can I find a list of all open compound words (with spaces in them), like ""peanut butter"" or ""high school""? [close","2022-12-23 01:50:11","117263","1","516","<python><nlp><word-embeddings><nltk><spacy>","<p>I already have a list of &quot;1-gram&quot; words, which include closed compound words like &quot;skyscraper&quot; or &quot;weatherman.&quot;</p>
<p>However, I'm also interested in compiling a list of &quot;2-gram&quot; words that take on a meaning completely different than their two halves, such as &quot;rubbber band.&quot;</p>
<p>Using any modules/resources/methods, how can I accomplish this?</p>
<p>(I tried downloading Google's Ngram files, but lots of the &quot;2-gram&quot; were nonsensical, like &quot;and I&quot; or &quot;what was.&quot;)</p>
","nlp"
"117254","How to translate text automatically using Google Translate API (or any other approach) in python","2022-12-22 18:55:04","117257","0","724","<python><nlp><machine-translation>","<p>I have a dataset of reviews from TripAdvisor and I would like to translate non-English reviews into English.</p>
<p>The reviews are in many different languages: my dataset contains reviews in 44 different languages including English reviews which are around a quarter of the total, followed by Italian, German, Spanish and French reviews which are respectively 14%, 11% 6%, and 5.5% of the reviews.</p>
<p>I detect the different languages using the <a href=""https://pypi.org/project/langdetect/"" rel=""nofollow noreferrer""><code>langdetect</code> package</a>.</p>
<pre><code>from langdetect import detect

language = detect(title) 
        
</code></pre>
<p>Then I tried to use the package <a href=""https://pypi.org/project/googletrans/"" rel=""nofollow noreferrer"">googletrans</a> to translate the text.
However, when I tried to perform a small test I get the following error:</p>
<pre><code>from googletrans import Translator
translator = Translator()

translator.translate('Buongiorno mi chiamo Alberto')
</code></pre>
<p><a href=""https://i.sstatic.net/Il9IX.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Il9IX.png"" alt=""enter image description here"" /></a></p>
<p>Does anyone know a way to translate reviews (knowing or not the language ) to English using python
for a dataset of around 10k reviews?</p>
","nlp"
"117248","What is the right processing order when working with a dataset that already consists of test and train data?","2022-12-22 14:40:52","117271","1","165","<nlp><pytorch><preprocessing><text-classification>","<p>I want to work on the following task:</p>
<ul>
<li>Text Classification using Deep Learning models and a Transfer Learning model.</li>
<li>The notebook that I'm creating should include the following steps:
<ol>
<li>Data Preparation</li>
<li>Cleaning</li>
<li>EDA</li>
<li>Embedding/Preprocessing</li>
<li>Training</li>
<li>Evaluation</li>
</ol>
</li>
<li>The data set that I want to use is the <a href=""https://huggingface.co/datasets/yahoo_answers_topics"" rel=""nofollow noreferrer"">yahoo_answer_topics from Hugging Face</a> and comes with an already defined train and test split. And here comes the point I'm not sure about: How should I deal with this predefined train and test split?</li>
</ul>
<p>Consulting other <a href=""https://stats.stackexchange.com/questions/239381/when-to-do-the-split-training-and-test-set"">similar questions</a> about when to do the train and test split, it is stated that the cleaning and EDA can be performed on the entire dataset. However, other resources mention that the <a href=""https://stats.stackexchange.com/questions/424263/should-exploratory-data-analysis-include-validation-set"">EDA should only be done on the training data</a> and use the validation and test data to evaluate the quality of any decisions made on the train data set. What is considered the correct way?</p>
<p>If the first would be the way to go; would this mean I have to best combine the two datasets at the start, clean, preprocess, and do the EDA before splitting them again into test, validation, and test?</p>
<p>If the second would be the way to go; would this mean I have to split the train into validation and train as a first step and then do the cleaning, and EDA only on the train data set, and then apply the same cleaning/transformation steps on the validation and test data right before training the model? Would it, therefore, be a violation if I look at the validation and test data set before the training (e.g. checking the distribution of the different classes in the validation and test set) and what if I want to do K fold Cross Validation?</p>
<p>Thank you already for your help!</p>
","nlp"
"117144","How many samples in dataset are required to fine-tune BERT for binary classification?","2022-12-18 15:51:18","","2","1754","<nlp><dataset><bert><finetuning>","<p>I'm trying to fine-tune a BERT-based model for a binary classification task (data is in English). The dataset I'm working with is quite small (~500 samples, out of which 80% are currently used for training), and I'm wondering if there is a rule of thumb for the minimal number of samples that is required to produce a decent model. I am able to increase the size of the dataset by labeling manually (though that would only be possible in the case of thousands of samples).</p>
<p>Any ideas? If this is problem-dependent, ideas on how to assess the size of dataset required would be appreciated.</p>
<p>Thanks!</p>
","nlp"
"117054","How to select target words for Lexical Simplification dataset","2022-12-14 18:20:35","","1","34","<machine-learning><nlp><dataset>","<p>I am trying to compile a Lexical Simplification dataset, which contains sentences, target words, and their simpler substitutes. I have already found some similar datasets in various languages, here are some of them for reference:</p>
<ul>
<li><a href=""https://github.com/MMU-TDMLab/CompLex"" rel=""nofollow noreferrer"">English</a> - contains 1 target word per sentence</li>
<li><a href=""https://data.mendeley.com/datasets/ywhmbnzvmx/2"" rel=""nofollow noreferrer"">Spanish</a> - contains multiple target words per sentence</li>
<li><a href=""https://github.com/LaSTUS-TALN-UPF/ALEXSIS"" rel=""nofollow noreferrer"">Spanish</a> - contains multiple target words per sentence</li>
<li><a href=""https://github.com/qiang2100/BERT-LS/tree/master/datasets"" rel=""nofollow noreferrer"">English</a> - contains 1 target word per sentence</li>
</ul>
<p>However, none of these sources seem to state how the target word was selected. Target words are words that are potentially complex, meaning that they may be labelled as complex or not complex. Are they chosen arbitrarily? How would such a process typically be carried out?</p>
<p>Thanks in advance!</p>
","nlp"
"117038","How to vectorize newline \n in tensorflow textVectorization layer?","2022-12-14 02:58:09","","1","128","<nlp><word-embeddings><vector-space-models>","<p>I am working on text generation model and i want to vectorize the newline character '\n' as  a word in tensorflow. How DO i do it.</p>
<p>I have done this so far. but tensorflow just not consider it.</p>
<ul>
<li>cleaning</li>
</ul>
<pre><code>import re
def clean(x):
    x = str(x).lower()
    x = x.replace('\r\n\r\n','\n')
    x = x.replace('\r\n','\n')
    x = x.replace('\n', ' \n ')
    # x = x.replace('  ',' ')
    x = re.sub(&quot;[^\r\n?.!:a-zA-Z]&quot;, &quot; &quot;, x) # removing punctuation
    x = x.replace('!',' ! ')
    x = x.replace(':',' : ')
    x = x.replace('.',' . ')
    x = x.replace('?',' ? ')
    return x
</code></pre>
<ul>
<li>Initialized vectorization layer</li>
</ul>
<pre><code>vector = layers.TextVectorization(max_tokens=max_token+2, standardize=None, pad_to_max_tokens=False,)
vector.adapt(william['content'])
</code></pre>
<ul>
<li>cleaned it</li>
</ul>
<pre><code>a = ['how are you ?', 'who are you? \n let go', ' some word. \n other word']
print('a: ',a)
clean_a = list(map(lambda x: clean(x),a))
print('clean a: ',clean_a)
a_vec = vector(clean_a) # vectorizer sttips punctuations
print('vectorized: ',a_vec)
print(a_vec.numpy())
# print('vector to text: ')
for i in a_vec:
    print(idtotext(i.numpy()))
</code></pre>
<p>output:</p>
<pre><code>a:  ['how are you ?', 'who are you? \n let go', ' some word. \n other word']
clean a:  ['how are you  ? ', 'who are you ?   \n  let go', ' some word .   \n  other word']

vectorized:  tf.Tensor(
[[149  49  28  38   0   0]
 [ 74  49  28  38  62 273]
 [208 398   4 257 398   0]], shape=(3, 6), dtype=int64)

back to text: 
how are you ?  
who are you ? let go
some word . other word 
</code></pre>
<p>Please tell me how to i vectorize newline character?</p>
","nlp"
"117015","Adding punctuation for a long text","2022-12-13 11:36:33","","2","898","<python><deep-learning><nlp><transformer>","<p>I want to add punctuation to a long text (youtube transcript) before using a Transformer pipeline for summarization.</p>
<p>I have found this answer here:
<a href=""https://datascience.stackexchange.com/questions/48575/is-there-any-nlp-library-or-package-which-can-help-in-adding-comma-punctuation?newreg=4c83717f50ec4a78943dd3eeae9511a1"">original answer</a></p>
<p>thus I have tried:</p>
<pre><code>from transformers import T5Tokenizer, TFT5ForConditionalGeneration

tokenizer = T5Tokenizer.from_pretrained('SJ-Ray/Re-Punctuate')
model = TFT5ForConditionalGeneration.from_pretrained('SJ-Ray/Re-Punctuate')

input_text = 'the story of this brave brilliant athlete whose very being was questioned so publicly is one that still captures the imagination'
inputs = tokenizer.encode(&quot;punctuate: &quot; + input_text, return_tensors=&quot;tf&quot;) 
result = model.generate(inputs)

decoded_output = tokenizer.decode(result[0], skip_special_tokens=True)
print(decoded_output)
</code></pre>
<p>While this is working fine with short sentences the token limit is 512 and I have text with lengths up to 13000 tokens.
This is not a problem for summarization I use longformer model that has limits of 16000, but I would like to know the best strategy to add punctuation for a long text.
How do you suggest splitting the text? other kinds of strategies? do you have a snippet of code?</p>
<p>Thank you very much for your help</p>
","nlp"
"116986","Possible NLP approaches to extract 'goals' from text","2022-12-12 14:35:25","","0","146","<nlp><transformer><spacy><association-rules><huggingface>","<p>I am planning to take up an interesting NLP project. I want to extract 'goal' statements from lengthy reports. For example, the goals can be <em>We would be reducing our carbon footprint by 50% by 2025</em> or <em>Our company aims to increase the diversity in the work-force in upcoming months</em>. Check below image for example text and highlighted goals.</p>
<p><a href=""https://i.sstatic.net/pU6sy.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/pU6sy.png"" alt=""Example with highlighted goals"" /></a></p>
<p>How can I go about the process of goal extraction, I would like to get some pointers on possible NLP approaches ?</p>
","nlp"
"116977","Dynamic clustering in NLP dataset","2022-12-12 12:20:28","","5","477","<machine-learning><nlp><clustering>","<p>Let's imagine this list:</p>
<pre><code>corpus = ['cat','banana','dog','horse','apple','tiger','snake']
</code></pre>
<p>I am looking for a way to build a vectorizer with dynamic clustering. I'm referring to a process like such:</p>
<ol>
<li>Take the first item as a class[0].</li>
<li>Look at the second, evaluate if it belongs to the same class (it doesn't but it doesn't know yet), and classify it in class [0].</li>
<li>Get to the third item and evaluate that two of those three are far more closely related, therefore create class [1] for fruits and reevaluate all of its previous assumptions.</li>
<li>Go over the whole corpus in this way.</li>
<li>Then, if 'plane' is added to the corpus, can again evaluate it doesn't fit the previous classes and create a class[2].</li>
</ol>
<p>So far, I worked with Multinominal Naive Bayes and Support Vector Machine in order to solve this problem. Both of them worked fine on discriminating between datasets that were already labeled. They however failed to achieve what I was looking for. The accuracy is great, but the classes are non-dynamic.</p>
<p>The clustering is destined to be used for authorship identification on conversational datasets spanning different topics and themes. I tried models like Word2vec but the result was not right either.</p>
<p>As far as expectations go, I'd like to know if anybody ever build something similar, or if there are already models and vectorizers out there that I could use to accomplish such a task. (it is entirely possible that SVM already allows for dynamic clustering and I just didn't get it while reading its documentation).</p>
<p>In short: How would you go about this problem?</p>
","nlp"
"116928","How to evaluate Natural Question-Answer Generation pairs?","2022-12-10 06:06:37","","2","202","<nlp><dataset><model-evaluations><metric><text-generation>","<p>I am trying to generate Natural Question-Answer for a specific domain. I am using a Large Language Model (LLM). I have only context to generate question-answers but don't have any ground truth. How to measure the accuracy or how good the generation is?
I am repeating the experiment 2-3 times, How to compare which question-answers pairs are good? Because each time the generated question-answers are different.</p>
<p>For example,e :</p>
<p><strong>Context :</strong></p>
<pre><code>&quot;&quot;&quot;This section describes our proposed method. The detailed setup for our experiments is described in Sections 4.1 and 4.2.&quot;&quot;&quot;
</code></pre>
<p><strong>Iteration1 (generated question-answer)</strong></p>
<pre><code>Q: This section describes what?
A: This section describes the paper's proposed model.
</code></pre>
<p><strong>Iteration2 (generated question-answer)</strong></p>
<pre><code>Q: Which section describes the detailed experiments?
A: Sections 4.1 and 4.2 describes the detailed setup and experiments.
</code></pre>
<p><strong>Iteration2 (generated question-answer)</strong></p>
<pre><code>Q: Sections 4.1 and 4.2 describes what?
A: Detailed setup and experiments are described in Sections 4.1 and 4.2
</code></pre>
<p>Now I want to measure how good this model is in generating questions and answers based on the given paragraph. What matrices I can use? Please guide me on this, and if possible share the papers too.</p>
","nlp"
"116835","how to extract common aspects from text using deep learning?","2022-12-07 19:53:04","116881","1","119","<deep-learning><nlp><text>","<p>Can you suggest me some papers to read about deep learning models that find patterns/similarities between different texts?</p>
<p>What I have is a set of reviews with the following categories for each review: Rating, Review title, Review body, date, and helpful votes.
What I would like to do is understand whether there exist similarities among the reviews. For instance, there exists a cluster of customers that complains about a specific aspect of a product.
Or, as another example, to see if there exists a problem related to the season e.g. a product has many bad reviews in summer because it does not work well with high temperatures.</p>
<p>Thank you for your help.</p>
","nlp"
"116808","What is the math behind the Keras Tokenizer() function?","2022-12-07 09:21:50","116810","0","227","<machine-learning><python><deep-learning><keras><nlp>","<p>I am doing an essay on the mathematics behind a text classifier with NLP and neural networks and I would like to know how exactly the TOKENIZER function of Keras works. Whether cosine similarity is involved and how the dictionary creation is carried out taking frequency into account. If anyone knows the answer or a book/article where it is reflected, I will be eternally grateful.</p>
<pre><code>MAX_NB_WORDS = 50000
MAX_SEQUENCE_LENGTH = 250
tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!&quot;#$%&amp;()*+,-./:;&lt;=&gt;?@[\]^_`{|}~', lower=True)
tokenizer.fit_on_texts(data['Consumer complaint narrative'].values)
</code></pre>
","nlp"
"116800","How is padding masking considered in the Attention Head of a Transformer?","2022-12-07 04:30:23","","3","291","<neural-network><nlp><pytorch><transformer>","<p>For purely educational purposes, my goal is to implement  basic Transformer architecture from scratch. So far I focused on the encoder for classification tasks and assumed that all samples in a batch have the same length. This means, I didn't care about any masking.</p>
<p>However, now I want to support masking. I like to think that I understand the the purpose of, e.g., the target mask so the order cannot &quot;peek into the future&quot;. I generate this mask as follows:</p>
<pre><code>source_batch = torch.LongTensor([
    [1, 2, 3, 0, 0, 0],
    [1, 2, 3, 4, 5, 6],
    [1, 2, 3, 4, 5, 0]
])

batch_size, seq_len = source_batch.shape

def generate_tgt_mask(size):
    return torch.triu(torch.ones(seq_len, seq_len) * float('-inf'), diagonal=1)

print(generate_tgt_mask(seq_len))
</code></pre>
<p>yielding:</p>
<pre><code>tensor([[0., -inf, -inf, -inf, -inf, -inf],
        [0.,   0., -inf, -inf, -inf, -inf],
        [0.,   0.,   0., -inf, -inf, -inf],
        [0.,   0.,   0.,   0., -inf, -inf],
        [0.,   0.,   0.,   0.,   0., -inf],
        [0.,   0.,   0.,   0.,   0.,   0.]])
</code></pre>
<p>which should be the expected outcome when I check the PyTorch docs. This mask has a shape of <code>(L,L)</code> where <code>L</code> is the sequence length of the source or target sequence. Again, this matches the docs.</p>
<p>I use this mask in my implementation of the Scaled Dot Product Attention as follows -- which should be in line with many other implementations I've seen:</p>
<pre><code>class Attention(nn.Module):
    ### Implements Scaled Dot Product Attention
    
    def __init__(self):
        super().__init__()


    def forward(self, Q, K, V, mask=None, dropout=None):
        # All shapes: (batch_size, seq_len, hidden_size)
        
        # Perform Q*K^T (* is the dot product here)
        # We have to use torch.matmul since we work with batches!
        out = torch.matmul(Q, K.transpose(1, 2)) # =&gt; shape: (B, L, L)

        # Divide by scaling factor
        out = out / (Q.shape[-1] ** 0.5)

        # Optional: src_mask/tgt_mask (shape: (L, L); mask values are represented by -inf)
        if mask is not None:
            out += mask.unsqueeze(0) # Broadcast since it's the same mask for all samples in batch
        
        # Push throught softmax layer
        out = f.softmax(out, dim=-1)
        
        # Optional: Dropout
        if dropout is not None:
            out = nn.Dropout(out, dropout)
        
        # Multiply with values V
        out = torch.matmul(out, V)
        
        return out
</code></pre>
<p>So far so good...at least I like to think. However, my problem is now the mask to address the padding (e.g. <code>src_key_padding_mask</code>). From different tutorials using the <code>nn.Transformer</code>, this mask can be generated as follows:</p>
<pre><code>pad_token_index = 0

src_key_padding_mask = (source_batch != pad_token_index)

print(src_key_padding_mask)
</code></pre>
<p>yielding:</p>
<pre><code>tensor([[ True,  True,  True, False, False, False],
        [ True,  True,  True,  True,  True,  True],
        [ True,  True,  True,  True,  True, False]])
</code></pre>
<p>having shape of <code>(N,L)</code> which again matches the doc.</p>
<p>What I'm now missing is: How do I have to incorporate this matrix into my implementation of <code>Attention</code>?</p>
<p>Intuitively, I would assume that the masking matrix would contain <code>-inf</code> for each position associated the a padding. For example, looking at the first sequence in my example batch above, I would assume the masking matrix to look like:</p>
<pre><code>tensor([[0.,   0.,   0.,   -inf, -inf, -inf],
        [0.,   0.,   0.,   -inf, -inf, -inf],
        [0.,   0.,   0.,   -inf, -inf, -inf],
        [-inf, -inf, -inf, -inf, -inf, -inf],
        [-inf, -inf, -inf, -inf, -inf, -inf],
        [-inf, -inf, -inf, -inf, -inf, -inf]])
</code></pre>
<p>And indeed, some -- but not all -- example code that implement the Transformer archictectur from scratch, create the masking matrix for the padding like this. Applying this matrix to the scores obviously also sets the scores to 0, that is, the last 3 rows are all 0.</p>
<p>However, once pushed throught Softmax, the last 3 rows now all contain the value <code>1/6</code>. For example, for the <code>source_batch</code> above I get</p>
<pre><code>tensor([[[0.1989, 0.4297, 0.3714, 0.0000, 0.0000, 0.0000],
         [0.4334, 0.2225, 0.3440, 0.0000, 0.0000, 0.0000],
         [0.2880, 0.2284, 0.4836, 0.0000, 0.0000, 0.0000],
         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],
         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],
         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667]],
       ...
       (the other 2 samples of the batch are not shown)
</code></pre>
<p>What am I missing here? I'm pretty sure it's something trivial, but I just can't see it right now.</p>
","nlp"
"116698","NLP - F1 score for positive class drops to 0 after data augmentation","2022-12-03 13:59:25","","1","35","<machine-learning><deep-learning><nlp><text-classification><data-augmentation>","<p>I'm working on a 3-class text classification problem where my initial class distribution looked like this:</p>
<p>positive: 50%
negative: 25% and
neutral: 25%</p>
<p>And training on a model on this slightly imbalanced data gave me F1 scores of (45%, 57% and 68%) respectively for the negative, neutral and positive classes in the validation set. Since the F1 scores for negative and neutral classes seemed to be lesser than the positive class, I decided to try some data augmentation approaches for the negative and neutral classes alone.</p>
<p>I used <strong>ContextualWordEmbsAug</strong> from the <a href=""https://github.com/makcedward/nlpaug"" rel=""nofollow noreferrer"">NLPAug Library</a> to augment those two classes after which I got a class distribution like this:</p>
<p>positive: 35%
negative: 33%
neutral: 32%</p>
<p>But the same model trained on this augmented data give me strange results. The F1 scores are now (57%, 62%, 0%) for the negative, neutral and positive classes resp. I don't understand why the F1 score for the positive class had to drop to 0%. It makes sense that the results for negative and neutral have improved due to augmenting them, but I fail to understand why that should affect the performance on the positive class which had given me good results prior to augmentation.</p>
<p>Am I missing something here? Could someone explain to me the possible reasons for this?</p>
","nlp"
"116659","Generate similar text based on category or the similar texts","2022-12-02 07:38:53","","0","331","<machine-learning><nlp><text-mining><text-generation><self-study>","<p>I'm trying to generate the similar text based on the category or to generate text by combining similar texts into the new text. I was checking multiple nlp tasks like question generation, but they don't work for my task.</p>
<p>I have a dataset of text and related categories. In addition, I have a text category classificatory</p>
<p>Is there any text generation branches for the similar tasks or the papers to read? I looked at Permgen, but I had a problem with generation</p>
<p>Thank you in advance</p>
","nlp"
"116642","What are MLM and NSP models actually used for after they've been trained?","2022-12-01 15:24:23","116647","-1","350","<machine-learning><deep-learning><nlp><bert><language-model>","<p>I am a Python programmer working with deep learning nets and I have recently built my own language models as well as I have fine-tuned popular models like BERT. MY question is - after these models have been successfully trained, what are they used for? I understand that masked-language models can predict what the masked word is, but what is the point? What is a real-world application of this model? The same question goes for next-sentence prediction models - what is a real-world application?</p>
<p>Thank you.</p>
","nlp"
"116603","Topic Modeling - n-grams or 1,2,3,...n-grams?","2022-11-30 07:43:07","","1","135","<nlp><topic-model><lda><document-term-matrix>","<p>Do people use n-grams or 1,2,3,...n-grams in both matrix factorisation and generative models in Topic Modeling?</p>
<p>I've been trying to understand the basics of Topic Modeling and came to know that there are two ways - Matrix Factorisation like LSA and NNMF and generative models like LDA and pLSA.</p>
<p>However, while reading the texts, I had a question - Do people use n-grams or 1,2,3,...n-grams in both matrix factorisation and generative models in Topic Modeling? For example, if n=5, then do people use only 5-grams or do they use all unigrams, bigrams, trigrams, 4-grams and 5-grams for creating the document term matrix?</p>
<p>If there are contextual answers then what are the reasons for using either?</p>
<p>Thanks in advance.</p>
","nlp"
"116589","In sklearn tfidf what is the difference between term frequecy and document frequency","2022-11-29 17:53:51","116613","2","113","<nlp><scikit-learn><tfidf>","<p>Looking at the sklearn tfidf page: <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"" rel=""nofollow noreferrer"">https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html</a>
and trying to understand the difference between term frequency and document frequency.
My guess is that term frequency is the number of times the word appears in all the entire corpus, and that document frequency is the number of document or sentences a particular word appears in. For example if I have the following:</p>
<pre><code>corpus = [
    'This is the first line.',
    'This line is the second line.',
    'And this is the third one.',
    'Is this the first line?',
]
</code></pre>
<p>The word <code>line</code> appears in three documents or sentences but the total count for the entire corpus is four. So would it be that the <code>document frequency</code> is <code>3</code> and the <code>term frequency</code> is <code>4</code>? Is this correct?</p>
","nlp"
"116518","NER - What advantage does IO Format have over BIO Format","2022-11-27 13:23:02","116523","3","458","<machine-learning><nlp><transformer><bert><named-entity-recognition>","<p>In <a href=""https://aclanthology.org/2021.acl-long.248.pdf"" rel=""nofollow noreferrer"">this</a> paper, the authors say that they used IO schema instead of BIO in their dataset, which, if I am not wrong, means they just tag the corresponding Entity Type or &quot;O&quot; in case the word is not a Named Entity. What advantage does this method have? I would imagine that it just takes away valuable information from the model and makes it harder to detect entities that span multiple words</p>
","nlp"
"116509","Gensim: create a dictionary from a large corpus without loading it in RAM?","2022-11-26 19:33:32","116510","0","133","<nlp><bigdata><topic-model><gensim>","<p>The topic modelling library Gensim offers the <a href=""https://radimrehurek.com/gensim/auto_examples/core/run_corpora_and_vector_spaces.html#corpus-streaming-one-document-at-a-time"" rel=""nofollow noreferrer"">ability</a> to stream a large document instead of storing it in memory.</p>
<p>Streaming is possible for the stage of converting the corpus to BOW, but the dictionary must have been created first and apparently this requires loading the full corpus:</p>
<pre><code>from gensim import corpora

dictionary = corpora.Dictionary(a_huge_corpus)
</code></pre>
<p>Is there a way to create the dictionary without loading the whole corpus at once?</p>
<p>ps: this is my first <em>question</em> on the site, I'm a beginner ;)</p>
","nlp"
"116506","Transformer XL - understanding paper's illustration","2022-11-26 17:09:22","","2","104","<deep-learning><nlp><transformer><attention-mechanism>","<p>If I understand correctly, the <code>Key</code> hidden layer in the Transformer XL is of size <code>2L * d</code>,  where <code>L</code> is the segment length and <code>d</code> is the embedding dimension.</p>
<blockquote>
<p>concatenation of two hidden sequences along the length dimension</p>
</blockquote>
<p>Therefore, the size of the attention matrix would be <code>L X 2L</code>, where row <code>i</code> represents the attention <code>Query i</code> should apply to each of the <code>2L Keys</code>.</p>
<p><strong>That is, the self attention window length = 2 X segment length.</strong></p>
<p>However, in the following image from the paper, the segment length is 4 and there are only 4 lines linked to each node. Shouldn't there be 4 * 2 = 8 lines from each node?</p>
<p><a href=""https://i.sstatic.net/vJAyt.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/vJAyt.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://arxiv.org/pdf/1901.02860.pdf"" rel=""nofollow noreferrer"">Link to transformer XL paper</a></p>
","nlp"
"116412","Of all the books ever published in English, what percentage are available for NLP training?","2022-11-23 02:58:08","116590","1","51","<nlp><books><corpus>","<p>I notice a lot of NLP models are trained on news articles and Wikipedia content and some books. I wonder if NLP models would be better if they were trained on more books. I assume we don't have datasets/corpus of all books published in English for example. Is this true? Does anyone have any idea what percentage of books published in English are available as NLP training data?</p>
","nlp"
"116384","How to include information about labels in a multilabel classification task","2022-11-22 07:50:51","","0","107","<machine-learning><nlp><transformer><multilabel-classification><ai>","<p>Currently, I'm working on a multilabel classification problem for a shared task in NLP. I have quite a few labels, and with those labels, I have a little paragraph defining them. I was wondering if there is some way I can include that label information in a multilabel classification pipeline.</p>
<p>Up until now, I've tried prompt-learning, designing a prompt that includes that paragraph, but I haven't obtained good results. My best shot so far has been using a fine-tuned RoBERTa model, and I thought that if I could include that label definition somehow in the pipeline, I could obtain better results, as the LM beneath could extract more information about it.</p>
<p>Thanks in advance! Cheers.</p>
","nlp"
"116356","Features for POS tagging","2022-11-21 13:56:53","","2","107","<nlp>","<p>I am building an NLP model which uses MEMM in order to tag parts of speech.</p>
<p>My model uses history of two previous words and tags, and of the next word, alongside the current word and tag. I used those values and created basics features (f100-f107, capitals letters and digits).</p>
<p>I am trying to figure more advanced binary features. When I tested the model, it confuses mainly between <code>JJ</code> and <code>NN</code>. How can I think about features that could help my model detecting those POS and not confuse and switch between them?</p>
","nlp"
"116130","How to find entity names in non-grammaratical text?","2022-11-13 03:10:44","","2","35","<classification><nlp><few-shot-learning>","<p>Given:
<code>Bella Pharma Rosuvas 5 Enalapril 10 Domperidone 10 Ned's 24 by 7 PCM 650 Teneligliptin 5 ...</code></p>
<p>Get: <code>(1,2,shop), (3,4,med), (5,6,med), (7,8,med), (9,12,shop), (13,14,med), ...</code> i.e. words 1 and 2 denote a shop name, 3 and 4 denote a medicine name, etc.</p>
<hr />
<p>Challenges:</p>
<ol>
<li><strong>recognize shop names:</strong> some shop names maybe non-English dictionary words (Romanized spelling of local language words)</li>
<li><strong>separate medicine names:</strong> there are absolutely no punctuation marks like <code>.</code> or <code>,</code> and the pattern is like SmmmmSmmSmSmmmmmmmmSmm... (S is shop, m is full name of one medicine e.g. <code>Rosuvas 5</code> or <code>Amoxicillin Clavulanate</code>)</li>
</ol>
<p>Text is non-grammatical (just transcript of dictation between 2 people verifying inventory)</p>
<p>spaCy en_core_web_lg and other pre-build NER don't work (probably because both shop names and medicine names appear like proper nouns)</p>
<p>Also, there is no exhaustive list of medicine names: same chemical is sold under different names by different brands, sometimes weight/power specification may not be given.</p>
<p>Worth noting that chemical names have either suffix like *azole, *nate, *ide, *ril, etc.  or weights like 650mg, 10mcg, etc. often (but not always) associated with them.</p>
<p><em>I am amazed how <strong>humans</strong> (even non-Pharmacists) <strong>can label most of the data correctly</strong></em>, how can I label this data almost as well as humans using ML/DL libraries?</p>
<hr />
<p>I work as junior SDE, I can code just fine (e.g. convert BIO tag to JSON style spacy input) but have never used ML libraries before. Kindly include sample code in answer or link to same.</p>
","nlp"
"116103","Do Sampling before or after TFIDF step?","2022-11-12 06:59:23","","1","581","<machine-learning><nlp><class-imbalance><text-classification><tfidf>","<p>This is a multiclass text classification problem. The dataset has a class imbalance and I'm planning to use a sampling technique before modeling.</p>
<p>Should the sampling be done before/after the <code>TFIDVectorizer</code> step? Kindly share your thoughts.</p>
<pre class=""lang-py prettyprint-override""><code>from imblearn.pipeline import Pipeline
from imblearn.over_sampling import RandomOverSampler, SMOTE
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression

X_train, X_test, y_train, y_test = ...

############# LIKE THIS #############
ppl = Pipeline(steps=[
    ('vectorizer', TfidfVectorizer(ngram_range=(1, 2))),
    ('sampler', SMOTE()),
    ('classifier', LogisticRegression(max_iter=1000, n_jobs=-1),
)]
ppl.fit(X_train, y_train)

############# OR LIKE THIS #############
sampler = SMOTE() # SMOTE couldn't be put inside pipeline before TFIDF step because its output format is incompatible with TFidf, hence moved outside of pipeline.
X_train, y_train = sm.fit_sample(X_train, y_train)

ppl = Pipeline(steps=[
    ('vectorizer', TfidfVectorizer(ngram_range=(1, 2))),
    ('classifier', LogisticRegression(max_iter=1000, n_jobs=-1),
)]
ppl.fit(X_train, y_train)

</code></pre>
","nlp"
"116101","Ordering training text data by length","2022-11-12 04:34:21","116111","0","131","<machine-learning><nlp><bert><text-classification><performance>","<p>If I have text data where the length of documents greatly varies and I'd like to use it for training where I use batching, there is a great chance that long strings will be mixed with short strings and the average time to process each batch will increase because of padding within the batches.
I imagine sorting documents naively by length would create a bias of some sort since long documents and short one would tend to be similar to each other.
Are there any methods that have been tried that can help reduce training time in this case without sacrificing model performance?</p>
","nlp"
"116018","How to cluster components of a graph containing text data?","2022-11-09 08:51:36","","1","87","<nlp><clustering><unsupervised-learning><graphs><graph-neural-network>","<p>Suppose that I have a graph that has components like the image below.
<br></p>
<p><a href=""https://i.sstatic.net/fEdeb.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/fEdeb.png"" alt=""A graph with several components"" /></a></p>
<br>
<p>Graph nodes contain text data (titles) and the edges data is the similarity (percentage).
I know that each component represents a cluster, but my question is how to cluster these components. <br>
<br>
<strong>Example:</strong> <br>
A graph component may have these data for their nodes (titles).</p>
<ul>
<li>How to make pizza</li>
<li>How to make pepperoni pizza</li>
<li>Recipe for cooking pizza</li>
<li>ingredients needed for Italian pizzas</li>
</ul>
<p>And I have one other graph component with these titles.</p>
<ul>
<li>Kebab restaurants</li>
<li>Homemade Kebab</li>
<li>How is kebab cooked?</li>
</ul>
<p>I know that these two graph components with the title mentioned are clusters individually. My question is how can I cluster these two graph components since they both may have the same topic (cooking, food, etc)?</p>
<p><strong>Things I have looked into or think can be the solution:</strong></p>
<ul>
<li>There is a library called <a href=""https://maartengr.github.io/BERTopic/index.html"" rel=""nofollow noreferrer"">BERTopic</a> that clusters records of text but
in my problem, I already know that certain records form a cluster. I
want to cluster these text clusters (graph components)</li>
<li>Maybe there is a way to form a representation using transformers or
word vectorization for a graph component, and based on that
representation vector and some distance metric we can cluster the
components but the question is if this solution is plausible what is
the best way to implement it?</li>
<li>GNN (Graph Neural Networks) is usually used for node classification
or edge prediction (Functionalities that I am aware of). Can they be
used for clustering graph components which do not contain connections
to each other?</li>
</ul>
","nlp"
"116016","Sentiment Analysis models trained on articles / alternative data","2022-11-09 07:20:22","","1","56","<nlp><dataset><language-model>","<p>For a <em>6 class sentence classification task</em> (emotion), I have a list of sentences where I retrieve the sentiment using a language model that was trained on Tweets <a href=""https://github.com/VinAIResearch/BERTweet"" rel=""nofollow noreferrer"">(bertweet)</a>.</p>
<p>It works fine for simplistic sentences where the sentiment is also obvious (someone died, someone won something, someone was afraid of something, etc). However, when applying it to articles, it shows uncontrollable behavior.</p>
<p>Two examples of the <code>sadness</code> class:</p>
<pre><code>How Your Family Can Volunteer During the Pandemic: 99% probability of sadness
There was a massacre in Bosnia where many were slaughtered: 96% probability of sadness
</code></pre>
<p>I have tried removing the softmax to break down the probabilities into absolute values in order to see if there's a difference there, but it seems that it is marginal and the first sentence again is &quot;sadder&quot; than the second one about the massacre.</p>
<p>There are many more such examples for all the other classes. Is there any model that is trained on articles? Possibly click bait titles and the kinds?</p>
","nlp"
"115959","Understanding Syntactic divergence","2022-11-07 12:31:40","","1","62","<machine-learning><nlp><dataset><distribution><stanford-nlp>","<p>I am trying to read a paper <a href=""https://arxiv.org/abs/2004.14444"" rel=""nofollow noreferrer"">https://arxiv.org/abs/2004.14444</a> Section 6.1 of the paper describes Syntactic divergence. I have confusion regarding the distribution graphs and split of the dataset.</p>
<ul>
<li><p>If a paper proposes a new test set for the NLP domain under the
distribution shift concept, should the Syntactic divergence
distribution be similar to all proposed test sets?</p>
</li>
<li><p>If the Syntactic divergence distribution of the proposed test set is
different, what does it mean?</p>
</li>
<li><p>If my understanding is correct, Syntactic divergence is a difficulty
measure metric so if the Syntactic divergence distribution of new
test sets differs a lot from the original test set, does it means
that the proposed test set is more difficult? Is it a good
indication?</p>
</li>
<li><p>In summary, the distribution of new test sets should be similar, or
can it be different?</p>
</li>
</ul>
<p>Could someone from the NLP field help me to clear my doubts?</p>
<p>Thank you!</p>
","nlp"
"115925","Calculationg perplexity (in natural language processing) manually","2022-11-06 07:47:10","","1","111","<nlp><loss-function><probability><metric><loss>","<p>I am trying to understand Perplexity within Natural Language Processing as a metric more fully. And I am doing so by creating manual examples to understand all the component parts. Is the following correctly understood:</p>
<p>Given a lists W of words (as probabilities), where W consists of <span class=""math-container"">$w_1$</span> .. <span class=""math-container"">$w_n$</span> , and where we know the probabilities for each word, a model will still have to compute the intersection of words for the following formula to be useful:</p>
<p><span class=""math-container"">$$ P(W)=P\left(w_1\right) P\left(w_2 \mid w_1\right) P\left(w_3 \mid w_2, w_1\right) \ldots P\left(w_N \mid w_{N-1}, w_{N-2}\right) $$</span></p>
<p>Since the formula for conditional probability is given by:</p>
<p><span class=""math-container"">$P(A \mid B)=\frac{P(A \cap B)}{P(B)}$</span></p>
<p>And the intersection <span class=""math-container"">${P(A \cap B)}$</span> will in the instance of NLP be calculated by Cross Entropy loss in a model.</p>
","nlp"
"115911","Alternatives to word to vector embedding","2022-11-05 14:22:52","116376","1","1119","<nlp><word-embeddings><word2vec>","<p>I'm just curious are there some alternative techniques to word 2 vector representation? So words/phrases/sentences are not represented as vectors but have a different form. Thanks.</p>
","nlp"
"115817","Text + tabular data for classification","2022-11-03 07:50:37","","1","127","<classification><nlp><text-classification>","<p>I have a dataset of &quot;mixed&quot; types e.g</p>
<pre><code>text |  date  | amount  |  supplier
-----+--------+---------+------------
</code></pre>
<p>where <code>text</code> is rather short sentences (text from banktransactions), <code>date</code> is when the transaction occured,<code>amount</code> the amount of the transaction and <code>supplier</code> being where they have purchased it from e.g &quot;Wallmart&quot;.</p>
<p>I want to predict the <code>supplier</code>.</p>
<p>Using only the <code>text</code> (Tf-IDF + KNN) I can achieve a (precison,recall) = (0.94,0.4) (overall), but for some <code>suppliers</code> I can see that the <code>amount</code> could help differentiate even more since some suppliers have almost identical text, but the amount differes greatly. My issue is though that I find it difficult to add the <code>amount</code> as a feature.</p>
<p>What I have tried is:</p>
<ol>
<li>Using the <code>supplier</code> predictions, based on text, aswell with the <code>amount</code> as a feature in a LightGBM model. Overall that didn't do anything good</li>
<li>Using the <code>amount</code> as an interactive term between the probability predicitons from a Logistic Regression regression, in a new Logistic Regression (didn't do very much overall aswell)</li>
</ol>
<p>I can find a lot (!) of text regarding text-classification but I struggle to find anything about text+tabular classification. Any ideas of how to tackle such problems?</p>
","nlp"
"115810","Can OpenAI's CLIP Model or DeepMind's Flamingo Model Predict Classes Truly Never Before Seen for Zero- or Few-Shot Learning?","2022-11-02 19:57:33","","1","258","<nlp><computer-vision><gpt><meta-learning><deepmind>","<p>One type of statement about zero-shot and few-shot learning in the literature I continually come across is that these models can predict new unseen classes at inference time for which they were never trained on. However, such sources typically do not explain exactly what they mean.</p>
<p>Meta-learning/in-context learning-based zero-shot/few-shot learning models like Flamingo and CLIP rely on 1) a pre-training stage where a massive base vision-language model has been trained on millions to billions of images and text examples, and 2) an inference stage where a prompt with anywhere from 0 to a just a few examples are presented to the model inside a prompt's &quot;support set&quot;, along with an image or image + question &quot;query&quot; (see the diagram from the Flamingo paper below) which asks the model to generate an answer to the query.</p>
<p>Diagram below is the Flamingo model paper (Alayrac et al., pg. 16):
<a href=""https://i.sstatic.net/DP71r.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/DP71r.png"" alt=""enter image description here"" /></a></p>
<p><strong>My Questions</strong></p>
<ul>
<li><p>As a result, it is unclear to me whether scholars' statements about zero-/few-shot learning models being able to predict &quot;unseen&quot; classes at inference time refer to the model never having been <em><strong>pre-trained</strong></em> on these unseen classes in the base model, whether they mean the model has never seen the unseen examples in the support set at inference time, or whether they mean both. Does anyone know?</p>
</li>
<li><p>Can someone explain exactly how the Flamingo model by Alayrac et al., 2022, or the CLIP model by Radford et al., 2021 (both of which are pre-trained using contrastive loss) would be able to predict a class at inference time which has never before been seen by the model? How would this even be possible if the model does not know the label of an unseen image?</p>
</li>
</ul>
","nlp"
"115784","Is it possible to apply stable diffusion to text?","2022-11-01 14:51:54","","0","172","<deep-learning><nlp><text-generation>","<p>Is it possible in theory to apply Stable Diffusion to a text domain?</p>
<p>I'm trying to generate text using a Seq2Seq approach, and I'm wondering whether or not it's possible to apply stable diffusion by making use of a convolutional Seq2Seq model.</p>
","nlp"
"115711","Open source NLP annotation tool/library supports active learning","2022-10-30 06:37:05","","1","63","<nlp><dataset><annotation><active-learning>","<p>I am looking for an NLP annotation tool/library that supports active learning. I am looking for something that works in this scenario:</p>
<ul>
<li><p>Annotating N samples.</p>
</li>
<li><p>Training a model on the annotated data.</p>
</li>
<li><p>Getting the model's predictions on the next N unlabeled data.</p>
</li>
<li><p>Correct/annotate (manually by the annotator) the annotations of the unlabeled data.</p>
</li>
<li><p>Retrain the model by including the labeled data from the last step.</p>
</li>
</ul>
<p>I found a library that's called Prodigy but it's not free. Any suggestion for a free library?</p>
","nlp"
"115668","identify data type of column from table using NER models","2022-10-28 10:02:14","","0","119","<nlp><python-3.x><named-entity-recognition><pyspark><aws>","<p>I have structure data with csv or parquet format, I would like to extract the data type of the column by analyzing the data.
when I looked at the NER from Hugging phase transformers, it actually dealing with context, but here my data is tabular format.
is there any way to extract data type of the each column by analyzing the data, actually volume of the data is so big (GB's), please suggest best solution for this.</p>
","nlp"
"115667","Best way to encode product names in NLP?","2022-10-28 09:42:05","","0","140","<nlp><preprocessing><encoding>","<p>I am supposed to train a classifier with historical shopping data that predicts the probability of an item being returned. The only human language contained for each purchase is the name of the product. Since the purchases are from many different companies all over Europe, the product names are often in different languages.</p>
<p>What model would be best suited to encode these product names? Would you use a translator to translate these product names? They are often riddled with abbreviations or brand names and in my eyes would most likely not lend themselves well to translations.</p>
<p>(I am yet to receive the data set, so I dont know exactly how many different products exist and what languages are most common, however the data set is mostly likely dominantly german and english)</p>
","nlp"
"115662","What is the NLP task that convert ""your"" to ""one's"", ""is"" to ""be""?","2022-10-28 03:08:06","","0","28","<nlp><normalization>","<p>In lemmatization, &quot;is&quot; remains the same; in stemming, 'your' remains the same; text normalization is irrelevant.</p>
<p>It can be solved using rules(mapping, like &quot;my&quot; -&gt; &quot;one's&quot;, &quot;his&quot; -&gt; &quot;one's&quot;) and is simple, just like English sentence segmentation. What is the task in NLP jargon?</p>
","nlp"
"115649","Hierarchical Classification - machine learning model with NLP","2022-10-27 14:13:51","","1","493","<machine-learning><classification><nlp>","<p>I have a Pandas DataFrame which consists of a body of text in 1 row. Each body of text is assigned labels for <code>Category</code> (in 1 row) and a <code>Topic</code> (in a separate row). Example of DataFrame below:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Text</th>
<th>Category</th>
<th>Topic</th>
</tr>
</thead>
<tbody>
<tr>
<td>cognitive training ...</td>
<td>Category A</td>
<td>Topic 2</td>
</tr>
<tr>
<td>correlation between ...</td>
<td>Category F</td>
<td>Topic 56</td>
</tr>
</tbody>
</table>
</div>
<p>There are 8 Categories with a total of 60 unique Topics. These Topics are not equally shared amongst the Categories, for example Category A can have Topics 1-18 and Category B can have Topics 19-27. A basic diagram has been added below demonstrating the levels.</p>
<pre><code>                     &lt;ROOT&gt;
            ____________|______________________
           /            |        \          \ ...
        Cat A         Cat B      Cat C
      / / | \ \        |  \        | 
     1 2  3  4 5 ...  19  20 ...   28
</code></pre>
<p>I would like to know the <strong>best approach(s) for generating a Hierarchical Classification</strong> where both the <code>Category</code> and <code>Topic</code> are generated in the prediction. So if a new body of text is received both levels can be predicted e.g. Category and Topic.</p>
<p>I have been using <code>TfIdf</code> on the body of text and how next to proceed is troubling me.</p>
<pre><code>from sklearn_hierarchical_classification.classifier import HierarchicalClassifier
</code></pre>
<p>Attempting to use this library isnt providing me with both levels as a prediction.</p>
<p>Any help or ideas in implementing such a model would be really appreciated :) Many thanks!</p>
","nlp"
"115607","What is the best approach to tackle stance prediction?","2022-10-26 09:23:58","115613","0","24","<machine-learning><python><classification><nlp>","<p>I am working on a task where I need to predict one of the following stances for a tweet: &quot;In favor&quot;, &quot;Against&quot;, &quot;Neutral&quot;, &quot;Not related&quot;, and &quot;Yes if&quot;. I've been trying to use scikit-learn and transformers for classification, but both seem to produce quite poor results. The problem is that the categories are not usual categories, but rather the attitude of the writer toward a specific topic, which probably should be tackled differently. I think there should be something that works with stances, but I managed to find only sentiment analysis and topic modeling tutorials so far. Is there anything I can take a look at? Any links, models, and advice would be greatly appreciated!</p>
","nlp"
"115600","SVM produces a constant accuracy when testing with development set, regardless of features","2022-10-26 07:44:59","","0","103","<machine-learning><nlp><svm>","<p>I am currently doing a class project to use a machine learning algorithm (SVM or Regression) to deduce whether two sentences are paraphrases of one another. We were given training, development, and test datasets, and when training my model I am given an accuracy that appears to be constant no matter which features are added/removed.</p>
<p>I believe it is possibly due to the model not properly attaining the features, but my primary concern is that depending on what I use, it produces a different constant accuracy.</p>
<p><strong>Vscode</strong>:0.5021459227467812<br />
<strong>JupyterLab (Kaggle)</strong>: 0.7421652421652422</p>
<p><strong>Training DataFrame</strong>  <a href=""https://i.sstatic.net/m31ES.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/m31ES.png"" alt=""Training DataFrame"" /></a><br />
<strong>Development DataFrame</strong> <a href=""https://i.sstatic.net/pHKQu.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/pHKQu.png"" alt=""Development DataFrame"" /></a></p>
<p><strong>Code using training and development sets:</strong></p>
<pre><code>#development
X_train = df_train.iloc[:,6:]
y_train = df_train['gold label'].values
X_dev = df_dev.iloc[:,6:]
y_dev = df_dev['gold label'].values

classifier = svm.SVC()
classifier.fit(X_train, y_train)

Y_pred = classifier.predict(X_dev)

print(classifier.score(X_dev, y_dev))
</code></pre>
<p>Please let me know what the issue could be or if there is a better way. Thank you!</p>
","nlp"
"115599","Best approach for rule-based system in multilabel classification-problem?","2022-10-26 07:41:05","115621","0","73","<nlp><multilabel-classification><spacy>","<p>I’m new to the world of NLP and am looking for some guidance. I want to create a rule-based system that “grades” text in accordance to some set of criteria. For example, one criteria could be “The author mentions that he/she wants money”, another “The author mentions working toward promotion”.</p>
<p>My initial idea was to use some available, open-source NLP-model, such as en_core_web_lg from the spaCy library. With such a model I could look at all verbs in a text, and classify texts as adhering to certain criteria when they have an appropriate verb with appropriate subject and object. I’ve read somewhere that exploiting the linguistic structure of sentences is a bad/unreliable way to go about things. The problem is that I don’t have any substantive data so as to allow supervised learning.</p>
<p>How do one typically go about creating a rule-based system for such a task? Is there any name for the problem I want to solve, maybe “Multi-label classification”? Any resources you could point me to?</p>
<p>Help a noob out!</p>
<p>I greatly appreciate it.</p>
","nlp"
"115554","How Exactly Does In-Context Few-Shot Learning Actually Work in Theory (Under the Hood), Despite only Having a ""Few"" Support Examples to ""Train On""?","2022-10-24 23:26:44","","7","2448","<nlp><computer-vision><language-model><gpt><deepmind>","<p>Recent models like the GPT-3 Language Model (Brown et al., 2020) and the Flamingo Visual-Language Model (Alayrac et al., 2022) use in-context few-shot learning. The models are able to make highly accurate predictions even when only presented with a &quot;few&quot; support examples. See diagram below (from Brown et al., 2020).</p>
<p><a href=""https://i.sstatic.net/zgZ8I.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/zgZ8I.png"" alt=""enter image description here"" /></a></p>
<p>Yet, it is unclear to me how these models theoretically work behind the scenes, and why they perform so well. The explanation appears to be that few-shot learning works because the model looks at the task description, then looks at the support examples (which are successful examples of how the given task can be fulfilled), and then based on the model's understanding of what the assigned task is and its understanding of the examples given of how the task could be successfully fulfilled it is then able to understand what it is supposed to predict based on the prompt.</p>
<p>Generally speaking, the more support examples the model sees at inference time, the better it will perform (but there is a point at which continuing to add further support examples does not increase performance). However, given that traditional machine learning models need to train on thousands of examples, it would seem unlikely that a model could really fulfill a task just based on a few examples.</p>
<p><strong>My Questions:</strong></p>
<ul>
<li><p>I understand that these models are built on huge pre-trained Language Models or Vision-Language Models having billions of parameters. But is there a commonly understood explanation of how these models are actually able to work (e.g., mathematical intuition) beyond what I have described?</p>
</li>
<li><p>Since these specific models (GPT-3 and Flamingo use &quot;in-context learning,&quot; which I understand to be the same as &quot;meta-learning,&quot; is it the case that what is actually happening in these models is that the massive pre-trained language and/or vision models they are built on are able to learn many <strong>tasks</strong>, and that consequently at inference time the model is able to learn from the few-shot prompt it is given what the new task being asked of it is, and <strong>also</strong> is able to learn the image/text query presented to it at inference time because it has been pre-trained on massive amounts of examples it can refer back to?</p>
</li>
<li><p>And is there a commonly accepted explanation of why these models actually work so well? Or are these three questions still a matter of debate among ML scholars?</p>
</li>
</ul>
","nlp"
"115550","Loss is very erratic in the 100s and val_loss is at 0, something - what is the reason for that?","2022-10-24 20:32:52","","1","33","<deep-learning><neural-network><nlp><convolutional-neural-network><mlp>","<p>I have a problem. I would like to solve a NLP classification problem.
For this I have trained a CNN and since I have other features, I wanted to include them in the model training.
Thus I have concatenated a CNN and the other features.</p>
<p>However, the problem is that the <code>loss</code> jumps to <code>175,10,100.8,...</code> . The <code>val_loss</code>, on the other hand, is at <code>0, something</code>.
What is the reason that the <code>loss</code> is so high and erratic?</p>
<p>Is it because the features and CNN cannot interpret the model correctly? Should the features be trained by a standalone model first?</p>
<p>What is the reason that the model has such an erratic <code>loss</code>? And what does that tell us?</p>
<p><a href=""https://i.sstatic.net/Zegay.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Zegay.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.sstatic.net/HghGe.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/HghGe.png"" alt=""enter image description here"" /></a></p>
<pre class=""lang-py prettyprint-override""><code>class CNN_1D:
    def __init__(self, x, y):
        self.x = x
        self.y = y

    def forward(self):
            # filter_sizes = [1,2,3,5]
            # num_filters = 32
            extra_nb_features = df_train.shape[1]

            inp = Input(shape=(maxlen, ))
            extra_inp = Input(shape=(extra_nb_features, ))

            x = Embedding(embedding_matrix.shape[0], 300, weights=[embedding_matrix], trainable=False)(inp)
            x = SpatialDropout1D(0.4)(x)
            # x = Reshape((maxlen, embed_size, 1))(x)

            x = Conv1D(256, 7, activation='relu')(x)
            x = MaxPooling1D()(x)
            
            
            x = Conv1D(128, 5, activation=&quot;relu&quot;)(x)
            x = MaxPooling1D()(x)
            
            x = Dropout(0.2)(x)  
            #x = Flatten()(x)
            x = GlobalMaxPooling1D()(x)
            combined = Concatenate(axis=-1)([x, extra_inp])

            combined = Dropout(0.15)(combined)
            
            outp = Dense(128, activation=&quot;relu&quot;)(combined)

            outp = Dense(numbmer, activation=&quot;softmax&quot;)(outp)

            model = Model(inputs=[inp, extra_inp] , outputs=outp)
            model.summary()
            return model
</code></pre>
","nlp"
"115540","terminology and advice for NLP on multiclass classification with ordered levels","2022-10-24 15:50:00","","1","22","<classification><nlp><multiclass-classification>","<p>I work in healthcare and am trying to see if I can use NLP for a classification task on complex sentences. To explain, I have different labels, and each label has multiple levels. I am not sure on the correct terminology however. I have label X, and X always exists as one of 4 'levels':</p>
<ul>
<li>'absent' or 0</li>
<li>'few', or 1</li>
<li>'many', or 2</li>
<li>'everywhere', or 3</li>
</ul>
<p>Example sentences then look like: 'I have no X', or 'I have a little bit of X', or 'there is X everywhere'. However, I also have labels Y and Z, which also have multiple levels. To complicate things further, one sentence can often contain information about multiple labels. As an example:</p>
<ul>
<li>'X and Y are both absent in this man'. classification would be X:0, and Y:0</li>
<li>'He has no X but a lot of Z'. classification would be X:0 , and Z:2</li>
<li>'There is Z everywhere, but very little Y'. classification would be Z:3 and Y:1</li>
</ul>
<p>Sentences in my corpus can also be about something completely different in which I am not interested:</p>
<ul>
<li>'Q was quite small'.</li>
</ul>
<p>Does anyone know the correct terminology for a problem like this? I have tried doing research on similar problems, I was thinking a regression based solution might be needed for the different levels, although pure multiclass classification might also work. I have quite a lot of experience with multilabel and multiclass classification problems, but without the multiple ordered 'levels' in the data I show here. Also, if anyone has any suggestions for approaches, that would also be very helpful! I do have a few thousand high quality training sentences.</p>
","nlp"
"115497","Recommendations of NLP for classifying sentance into tense forms","2022-10-23 08:40:52","","1","28","<nlp><language-model><stanford-nlp>","<p>I have a dataset of tweets. I have to classify each tweet into it's tense forms like whether it's about past, present or future. So for that can you please recommend any pretrained NLP model or method for this task?</p>
","nlp"
"115487","Are there any open source models that do diarization and transcription?","2022-10-22 23:39:57","115496","1","49","<nlp>","<p>Every time I look up speech-to-text, all I get are diarization or transcription models, but not both. Are there any models that say, process a conversation wav/mp3/whatever file and transcribe it <em>with</em> the relevant speaker labels?</p>
","nlp"
"115453","Do large pretrained language models already ""know"" about NLP tasks?","2022-10-21 14:09:46","","5","93","<nlp><transformer><language-model>","<p>Nowadays the state-of-the-art in NLP is to finetune a large pretrained language model such as BERT/GPT etc. on specfic tasks. These language models are pretrained on a huge amount of data and then basically evaluated on popular labeled datasets published for e.g. Question Answering, Machine Translation <a href=""http://nlpprogress.com/"" rel=""noreferrer"">etc.</a>. As those datasets became the de facto default of evaluating these model, those datasets have been published over and over again on various websites. Used and reused for people that are building their own small model etc. So basically these datasets (train and test data) including their e.g. label in classification tasks or the answer in Q/A tasks &quot;stray&quot; around in the internet. So now when training a <em>new</em> large language model (with a novel architecture) it is fed with text data that is often scraped from the internet as well. Wouldn't it be possible that in the training phase of these LMs, the networks have already seen this exact data (and learned on its co-occurence) which they are evaluated on later on? This would basically defeat the purpose of the evaluation as the test data already leaked into the process of pretraining the language models. Are there any prefiltering steps happening in pretraining these models such that this doesn't happen? And secondly, even if the network has seen the exact test data with e.g. test set question+answer among billions of other textual data, would it even pick up on that or is it just too much data anyways for the model to adjust the weights accordingly and &quot;remembering&quot; these exact datapoints.</p>
","nlp"
"115440","Why is it useful to use different word splitting with different tokenizers?","2022-10-21 08:15:27","115457","1","45","<nlp><tokenization>","<p>I have a problem. I have a NLP classification problem.
There are different methods to decompose sentences into tokens, for example in whole words or in characters. Then there are different tokenizers like:</p>
<ul>
<li>TF-IDF</li>
<li>Binary</li>
<li>Frequency</li>
<li>Count</li>
</ul>
<p>My question now aims, why should one make the effort and use a different word division (word or character) and then check this with the different tokenizers?</p>
","nlp"
"115432","Potential solution for a NLP clustering problem","2022-10-20 21:24:35","","0","72","<nlp><clustering><data-analysis>","<p>Trying to approach this clustering-based problem</p>
<p>Let's imagine I have a dataset containing millions of observations in a tabular data set, containing categorical, time-based, numerical, and text columns. (text column is basically a manual description uploaded by the user, detailing the complaint). Historically these complaints are raised by automatic systems or people. Many a time multiple systems or people raised the same issue, resulting in multiple individual complaints. Complaints that map to the same issue should most of the time should look similar in terms of features, according to my observation. My task is to cluster them in a way so that I can identify which complaints belong to the same problem. Historically I have data that tells me which complaints were duplicates.</p>
<p>Now my initial approach was to do some hierarchal clustering but got stuck on some issues-</p>
<p>How to accommodate text-based columns in clustering algorithms?
Given I have label data, is there is way to leverage this to develop a better clustering algorithm?
Any links or papers would be really helpful. Thanks in advance.</p>
<p>Note - CLusters are not few. for example in 24 hr period, there will be 100k complaints of which ~40k will be duplicates and 60k will be unique.</p>
","nlp"
"115429","What explains T5's recent resurgence?","2022-10-20 19:35:43","120944","2","43","<nlp><language-model><social-network-analysis>","<p>I read on <a href=""https://towardsdatascience.com/choosing-the-right-language-model-for-your-nlp-use-case-1288ef3c4929"" rel=""nofollow noreferrer"">https://towardsdatascience.com/choosing-the-right-language-model-for-your-nlp-use-case-1288ef3c4929</a>:</p>
<p><a href=""https://i.sstatic.net/6cI3W.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/6cI3W.png"" alt=""enter image description here"" /></a></p>
<p>I find the curve for T5 to be particularly interesting. What explains its recent resurgence?</p>
","nlp"
"115408","statistical significance of difference in cosine similarity for word pairs","2022-10-20 08:45:28","","0","168","<nlp>","<p>I'm wondering if there is a way to say something about statistical significance of differences in cosine similarity for word pairs as extracted from a language model. Suppose I have the word pair A-B, and the word pair A-C. I use word2vec, or bert (or whichever other) embeddings, to calculate the cosine similarity for A-B, let's say I get .60. I do the same for A-C, let's say that yields .62.
Now, is the difference between .60 and .62 statistically significant? Is that even a reasonable question to ask?
I feel it does make sense to ask, and suspect there is something to be said about it. But intuitively, I'd think that I would need to know on how many occurrences the calculation of cosine similarity is based (i.e. how many occurrences of the terms A, B and C there are in the corpus that the embeddings were trained on). Which is something I can't really find out (the corpora used for training the embeddings are not all freely available).</p>
<p>Any ideas on how to say something (statistically) sensible about the difference of .60 and .62 in the above example?</p>
","nlp"
"115361","Why is it convolutional 1D is sometimes better and faster that LSTM at classification and predicting tasks?","2022-10-18 23:04:19","","2","654","<machine-learning><deep-learning><nlp>","<p>I have been using LSTM and Conv1d for text classification and it turns out Conv1d is actually better and faster than LSTM on this kinda task. Is this right or they are just a special cases.</p>
","nlp"
"115353","How does BERT produce CLS token? Internally does it do max-pooling or avarage pooling?","2022-10-18 19:00:55","115356","1","1733","<nlp><transformer><bert>","<p>I ran experiment to compare max-pooled word tokens vs CLS token for sentence classification and CLS clearly wins. Trying to understand how BERT generates CLS token embedding if its better than max or avg pooling.</p>
","nlp"
"115342","What is the best way to create sequences for a text prediction model?","2022-10-18 16:16:13","","1","30","<machine-learning><deep-learning><nlp>","<p>this is the text example:</p>
<p>The process involves similar requirements to other Creator tools, like proving you’re the account owner and providing legal information.
Once set up, you receive a payout for your reels at the end of 30 days, during which you can choose up to 150 Reels to count towards the bonus. Paid, sponsored, or partnership Reels are not eligible for the bonus.And, you can go back and make Reels you’ve already published eligible for the bonus if you’re within the 24-hour window post-publishing.</p>
<p>type 1 sequence Example</p>
<pre><code>       features                                    

[['The process involves similar **requirements**'],          
['process involves similar requirements **to**'],  
['involves similar requirements to **other**'],       
['similar requirements to other **Creator**'],          
['requirements to other Creator **tools**'],
['to other Creator tools **,**'],
['other Creator tools, **like**'],
[' Creator tools, like **proving**']
]
</code></pre>
<p>type 2 sequence Example</p>
<pre><code>features                                    

[['The **process**'],                      
['The process **involves**'],              
['The process involves **similar**'],      
['The process involves similar **requirements**'],  
['The process requirements to other **other**'],
['The process involves similar requirements to other **Creator**'],
['The process involves similar requirements to other Creator **tools**'],
['The process involves similar requirements to other Creator tools **,**']
]
</code></pre>
<p>The <strong>bold words</strong> will be the targets. I need some help on creating these sequences, the 2 methods above are my approach i want to know if they are better ways of doing this. I'm still new in this field</p>
","nlp"
"115285","Classification of ""good"" and ""bad"" sentences","2022-10-16 20:09:07","","2","161","<machine-learning><python><nlp><r>","<p>I have a list of sentences. Examples:</p>
<ol>
<li>${INS1}, Watch our latest webinar about flu vaccine</li>
<li>Do you think patients would like to go up to 250 days without an attack?</li>
<li>Watch our latest webinar about flu vaccine</li>
<li>??? See if more of your patients are ready for vaccine</li>
<li>Important news for your invaccinated patients</li>
<li>Important news for your inv?ccinated patients</li>
<li>...</li>
</ol>
<p>I have around 30k of sentences, around 85% of these are sentences that considered as 'good'. By good I mean sentences with no strange characters and sequences of characters such as '${INS1}', '???', or '?' inside the word etc. Otherwise sentence is considered as 'bad'. I need to find 'good' patterns to be able to identify 'bad' sentences in the future and exclude them, as the list of sentences will become larger in the future and new 'bad' sentences might appear.</p>
<p>Is there any way to identify 'good' sentences using Regex, libraries in Python/R, or any other tool?</p>
<p>Thank you</p>
","nlp"
"115276","CNN model why is ReLu used in Conv1D layer and in the first Dense Layer?","2022-10-16 13:31:12","115277","0","917","<nlp><convolutional-neural-network><convolution><activation-function>","<p>I have a problem. I have a CNN model which is used for an NLP problem. This is written in Python. I have questions about this, which I can't find an answer to.</p>
<ul>
<li>Why is ReLu used inside the Conv1D layer and not Softmax ?</li>
<li>Why is ReLu used again as activation function in the first Dense-Layer and why Softmax afterwards ?</li>
</ul>
<pre class=""lang-py prettyprint-override""><code>model1 = Sequential()

model1.add(
        Embedding(vocab_size
                ,embed_size
                ,weights = [embedding_matrix] #Supplied embedding matrix created from glove
                ,input_length = maxlen
                ,trainable=False)
         )
model1.add(Conv1D(256, 7, activation=&quot;relu&quot;))
model1.add(MaxPooling1D())
model1.add(Conv1D(128, 5, activation=&quot;relu&quot;))
model1.add(MaxPooling1D())
model1.add(GlobalMaxPooling1D())
model1.add(Dense(128, activation=&quot;relu&quot;))
model1.add(Dense(number, activation='softmax'))
print(model1.summary())
<span class=""math-container"">```</span>
</code></pre>
","nlp"
"115267","Fine tuning BERT without pre-training it on domain specific corpus","2022-10-16 05:55:45","115269","1","1081","<nlp><bert><search>","<p>I'm building an internal semantic search engine using BERT/SBERT + ElasticSearch 8 where  answers are retrieved based on their cosine similarity with a query.</p>
<p>The documents to be searched are somewhat domain-specific, off the top of my head estimation is that about 10% of the vocabulary is not present in Wiki or Common Crawl datasets on which BERT models were trained. These are basically &quot;made-up&quot; words - niche product and brand names.</p>
<p>So my question is:</p>
<ol>
<li>Should I pre-train a BERT/SBERT model first on my specific corpus to learn the embeddings for these words using MLM?</li>
</ol>
<p>or</p>
<ol start=""2"">
<li>Can I skip pre-training and start fine-tuning a selected model for Q/A using SQUAD, synthetic Q/A based on my corpus and actual logged user queries?</li>
</ol>
<p>My concern is that if I skip #1 then a model would not know the embeddings for some of the &quot;made up&quot; words, replace them with &quot;unknown&quot; token and this might lead to worse search performance.</p>
","nlp"
"115257","how can i create better sequences for a word prediction model","2022-10-15 20:42:41","","2","56","<deep-learning><nlp><tensorflow><machine-learning-model>","<p>Sorry for the length of the question</p>
<pre><code>def create_ds(data, max_len):
    seq = []
    end = 2
    data = data.split(&quot; &quot;)
    for i in range(len(data)):
        try:
            if(end &lt;= max_len):
                x = data[i:i + end]
                end += 1
            else:
                end = 2
                x = data[i:i + end]
                end += 1
            
        except Exception as e :
            print(e)
        
        if(x != ''):
            xs = &quot; &quot;.join([i for i in x if i != ''])
            if(len(xs.split(&quot; &quot;)) &gt;= 2):
                seq.append([xs])
   shuffle(seq)
        
   return seq
   
</code></pre>
<p>this i my code for creating sequences
after the sequences have been created i vectorize them using keras TextVectorization layer
the output looks like this</p>
<pre><code>      array([  24,    7,  122,   14,   28, 8005,   44, 1031,   48,   13, 2299,
              8,  323,    2,  118,   28, 1391,   50,   11, 9784,  235,   50,
              9,   63,    2,  482, 1052,   31, 1972,    4,    3,  439,    6,
              592,  482,  132,    0,    0,    0,    0,    0,    0,    0,    0,
              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
              0])
</code></pre>
<p>and i use the following code to remove the last number is this vectorized sequence to make it the target</p>
<p>#this code will take the vectorized sequences and make targets from those.It takes the vectorized sequences and the size of each sequence and remove the last item in that sequence and make it a target and then replace that by a zero</p>
<pre><code> def extract(ds):
    labels = []
    for i in range(ds.shape[0]):
        labels.append(ds[i][list(ds[i]).index(0)-1])
        ds[i][list(ds[i]).index(0)-1] = 0
    
    return tf.convert_to_tensor(ds), np.array(labels)
  
 features, labels = extract(train_ds.numpy())
</code></pre>
<p>if you pass this into the the above function</p>
<pre><code>     array([  24,    7,  122,   14,   28, 8005,   44, 1031,   48,   13, 2299,
              8,  323,    2,  118,   28, 1391,   50,   11, 9784,  235,   50,
              9,   63,    2,  482, 1052,   31, 1972,    4,    3,  439,    6,
              592,  482,  132,    0,    0,    0,    0,    0,    0,    0,    0,
              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
              0])
</code></pre>
<p>it will take the  last 132 and make it into a target and replace its position with a zero</p>
<p>what i want to know is what am i doing wrong here and is there a better way of doing this which is efficient. Sorry for the length of the question</p>
","nlp"
"115251","Using different tokens for padding, end-of-sentence, and start-of-sentence in autoregressive sequence modeling?","2022-10-15 15:34:03","118494","1","508","<nlp><text-generation>","<p>Is there utility in using different tokens for end-of-sentence, start-of-sentence, and padding for autoregressive sequence modeling (i.e. text generation)?
Or can I use the same token for all of them?</p>
","nlp"
"115231","Good NLP model for computationally cheap predictions that can reasonably approximate language model given large training data set","2022-10-14 15:01:01","115249","1","39","<machine-learning><nlp><bert>","<p>I have a corpus of about one billion sentences, in which I am attempting to resolve NER conflicts (when two terms overlap in a sentence). My initial plan is to have an SME label the correct tag in each of a large number of conflicts, then use those labels to train either an NER model or a binary classification model (like GAN-ALBERT), to identify the correct choice when two NER tags conflict.</p>
<p>The problem is, about 5% of these sentences contain conflicts, and I don't think that I have the computational resources to run BERT or ALBERT prediction on 50 million sentences in a reasonable amount of time. So, my hope is to use the ALBERT model to generate a large number of labels (perhaps one million) for a computationally cheaper model.</p>
<p>So, I'm wondering if there is a model, 10 to 100 times cheaper at prediction than BERT, that could be trained to do a reasonable job of replicating the ALBERT model's performance, given a large amount of training data generated by said model.</p>
","nlp"
"115195","Does word ordering affect monolingual alignment success","2022-10-13 10:12:24","","1","23","<nlp><word-embeddings><word2vec>","<p>I spent some time reading about both Word2Vec embeddings and alignment between different embeddings (for instance <a href=""https://aclanthology.org/P18-1073/"" rel=""nofollow noreferrer"">vecmap</a>) and was wondering whether there is any significance to the word ordering of the different languages and how well the alignment can be made.</p>
<p>I haven't found much research in this topic (found quite a bit regarding word ordering in other cross-lingual tasks) and was wondering if the typological feature actually has an effect. Have you seen any papers in the matter?</p>
","nlp"
"115182","How are Learned Latent Arrays for the Perceiver Resampler in DeepMind's Flamingo Vision-Language Model Actually Calculated? By which Technique?","2022-10-13 00:31:31","","1","543","<deep-learning><nlp><computer-vision><language-model><deepmind>","<p>In <strong>&quot;Flamingo: a Visual Language Model for Few-Shot Learning&quot;</strong> (Alayrac et al. 2022) <a href=""https://arxiv.org/abs/2204.14198"" rel=""nofollow noreferrer"">https://arxiv.org/abs/2204.14198</a> DeepMind makes use of &quot;learned latent queries&quot; in their &quot;Perceiver Resampler&quot; to ensure that parameters do not scale quadratically the way they do with Transformers. The authors cite the DeepMind article <strong>&quot;Perceiver: General Perception with Iterative Attention&quot;</strong> (Jaegle et al., 2021) <a href=""https://arxiv.org/abs/2103.03206"" rel=""nofollow noreferrer"">https://arxiv.org/abs/2103.03206</a> as inspiration for their creation of Perceiver Resamplers. Perceivers from the Jaegle et al. article involve &quot;learned latent queries&quot; (i.e., queries from learned latent arrays) that cross-attend to image feature-based keys and values. My understanding is that these learned latent arrays are a reduced dimensional representation of the visual feature arrays that are the outputs of the Vision Encoder. However, the Flamingo paper does not explain how the learned latent array is actually computed from the original visual feature array from the Vision Encoder.</p>
<p>In terms of the Perceiver from Jaegle et al., the authors seem to hint that learned latent arrays may be created through some kind of clustering algorithm. They state, &quot;The model can also be seen as performing a fully end-to-end clustering of the inputs with latent positions as cluster centres, leveraging a highly asymmetric cross-attention layer&quot; (pg. 3). But if they use a clustering algorithm of some kind to produce the learned latent arrays, as far as I can see they do not explain how exactly such an algorithm could be reproduced for use in code, and so they leave it somewhat to the imagination to figure out.</p>
<p>I have 2 questions:</p>
<p><strong>1)</strong> Are these learned latent arrays learned from the visual features 𝑋𝑓 that come from Flamingo's Visual Encoder? If not, where are they being learned from?</p>
<p><strong>2)</strong> If so, how exactly (in a way that I might try to replicate the process) are learned latent arrays calculated from visual feature arrays that are outputs from the Vision Encoder?</p>
<p>Thank you for your help.</p>
<p>Source for image below (Alayrac et al. 2022, pg. 11):
<a href=""https://i.sstatic.net/omxGa.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/omxGa.png"" alt=""enter image description here"" /></a></p>
","nlp"
"115139","How long is the generator pre-trained in SeqGAN?","2022-10-11 19:12:19","","0","23","<nlp><gan><pretraining>","<p>I am reading up about SeqGAN and I am trying to understand the pretraining step better.
The authors claim they want to maximize the Maximum Likelihood Estimation on the dataset S by pretraining the generator on it (see pseudocode below). This is achieved by minimizing the negative log likelihood over the sequences. However, both from the paper and the code, it is unclear to me what stopping criterion they chose for training. Sure, they have a pre-set number of episodes the models run, but I would like to understand the idea behind it.
What makes more sense here: pre-train the generator until its loss is absolutely minimal on the <em>training data</em>, or using early stopping as soon as the loss on the <em>validation data</em> increases? Normally I would choose the latter, but maybe there is a good argument to be made for overfitting on the <em>training data</em> in the pretraining step.</p>
<p><a href=""https://i.sstatic.net/QLHWj.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/QLHWj.png"" alt=""Pseudocode for SeqGAN"" /></a></p>
","nlp"
"115135","can't convert lists to tensor in tensorflow python","2022-10-11 17:13:30","","0","1271","<python><keras><nlp><tensorflow><rnn>","<p>I'm relatively new to ML and data science and I'm using tensorflow and keras to do a NLP project.
I have about 18000 emails, in my code I convert each word in every email to a vector of shape (1,50) and then add all those arrays to a list. Obviously the length of those lists are different. When trying to fit my model I receive error.
How my arrays look like:
<code> [[float, float,....,float],[float, float,....,float],....]</code>
My code:</p>
<pre><code>train_data, test_data=train_test_split(DataFrame_2,test_size=0.2, train_size=0.8,random_state=0,stratify=DataFrame['Label'])

#print(test_data)

#Creating the model as a function

def MyModel():
    model=Sequential()
    model.add(LSTM(100, dropout=0.15))
    model.add(LSTM(100, dropout=0.15))
    model.add(Dense(1,'relu'))
    optim=keras.optimizers.Adam()
    model.compile(optimizer=optim, loss='binary_crossentropy', metrics=['accurcay'])
    return model

model=MyModel()
model.fit(tf.convert_to_tensor(train_data[&quot;Body&quot;]), train_data[&quot;Label&quot;], batch_size=64, epochs=25, verbose=1, validation_split=0.20, use_multiprocessing=True)
</code></pre>
<p>What I have tried:</p>
<p>I tried used <code>convert_to_tensor()</code> function both in first line (where I'm splitting the data), where I was fitting the data and also where I was making the array of vectors like this:</p>
<pre><code>for sent in tqdm(num_sent):
    vectorized.append([embedding_matrix_vocab[num] for num in sent])

DataFrame_2=pd.DataFrame({'Body':vectorized, &quot;Label&quot;:DataFrame[&quot;Label&quot;]})
</code></pre>
<p>also This is the error I'm receiving:</p>
<pre><code>Traceback (most recent call last):
  File &quot;F:\Farbod\work\ml\spam detection\SpamDetection.py&quot;, line 245, in &lt;module&gt;
    model.fit(tf.convert_to_tensor(train_data[&quot;Body&quot;]), train_data[&quot;Label&quot;], batch_size=64, epochs=25, verbose=1, validation_split=0.20, use_multiprocessing=True)
  File &quot;C:\Python\Python39\lib\site-packages\tensorflow\python\util\traceback_utils.py&quot;, line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File &quot;C:\Python\Python39\lib\site-packages\tensorflow\python\framework\constant_op.py&quot;, line 102, in convert_to_eager_tensor
    return ops.EagerTensor(value, ctx.device_name, dtype)
ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type list).
</code></pre>
<p>I have this guess which may or may not help. Since the sentences are of different sizes maybe converting them to tensor is not OK and maybe they should be converted to lists with fixed sizes.</p>
","nlp"
"115128","Does Word2Vec's skip-gram NNLM even produce context words?","2022-10-11 13:57:27","","4","390","<neural-network><nlp><word-embeddings><word2vec>","<p>Let me first establish what CBoW and skip-gram are supposed to do. You can skip to the next section if you think this is unnecessary.</p>
<h2>Background</h2>
<p>My understanding is that Word2Vec is a suite of 2 algorithms, <em>continuous bag-of-words (CBoW)</em> and <em>skip-gram neural-network language model (SGNNLM</em> or simply <em>skip-gram)</em>, which are both two-layer neural networks. That is, given a vocabulary <span class=""math-container"">$V$</span> and embedding size <span class=""math-container"">$H$</span>, they take vectors of size <span class=""math-container"">$|V|$</span> and pass them through one hidden layer and one output layer, with weight matrices <span class=""math-container"">$W_1\in \mathbb{R}^{H\times |V|}$</span> and <span class=""math-container"">$W_2\in \mathbb{R}^{|V|\times H}$</span>, much like a small auto-encoder. The output layer is always cited as having softmax activation, and according to <a href=""https://www.coursera.org/learn/probabilistic-models-in-nlp?specialization=natural-language-processing"" rel=""nofollow noreferrer"">this Coursera course</a>, the hidden layer has ReLU activation. I've seen it cited as having &quot;no&quot; activation (<span class=""math-container"">$\varphi(x) = x$</span>), though.</p>
<p>The <a href=""https://stackoverflow.com/a/42187104/9352077"">difference</a> between CBoW and skip-gram is supposedly that CBoW &quot;predicts the current word based on the context&quot;, and the skip-gram &quot;predicts surrounding words given the current word&quot;. That's a literal quote from the original Word2Vec paper, <a href=""https://arxiv.org/pdf/1301.3781.pdf"" rel=""nofollow noreferrer"">Mikolov 2013</a>. Here's their figure, which looks intuitive, but in reality is notoriously confusing:</p>
<p><a href=""https://i.sstatic.net/Urqj0.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Urqj0.png"" alt=""Mikolov figure"" /></a></p>
<p>Since both are <em>used</em> to move from a one-hot encoding to a smaller embedding (the weights in one or both of the matrices), their input and output format <em>is</em> still one-hot. I know that per training example, CBoW starts out with <span class=""math-container"">$C$</span> one-hot vectors (one for each context word) and just averages them to get a single <span class=""math-container"">$|V|$</span>-sized vector. (This is why it's called &quot;bag of words&quot;: because averaging is commutative, you lose the order of the context words.) Apart from this tricky many-to-one transformation, the rest of the network makes sense.</p>
<p>Skip-gram has no issue encoding the input, <em>but it has a one-to-many transformation</em> at the end. This is a problem.</p>
<h2>The issue</h2>
<p>What does skip-gram do in its final layer? I have been scouring various StackExchange sites to find a consensus on this question, and there seem to be fourdistinct camps.</p>
<ol>
<li><p>Skip-gram predicts <span class=""math-container"">$C$</span> softmaxes of size <span class=""math-container"">$|V|$</span> using <span class=""math-container"">$C$</span> <em>different</em> matrices <span class=""math-container"">$W_{2}^{(1)} \dots W_{2}^{(C)}$</span>.
<strong>Examples:</strong> <a href=""https://towardsdatascience.com/skip-gram-nlp-context-words-prediction-algorithm-5bbf34f84e0c"" rel=""nofollow noreferrer"">this article</a> (at least the figures), <a href=""https://mysaranshblog.wordpress.com/2016/11/10/skip-gram-architecture-of-word2vec-concisely-explained/"" rel=""nofollow noreferrer"">this article</a> stating &quot;each with its own weight matrix&quot;, and <a href=""https://stackoverflow.com/q/34363250/9352077"">this question</a>.</p>
</li>
<li><p>Skip-gram predicts <span class=""math-container"">$C$</span> softmaxes of size <span class=""math-container"">$|V|$</span> using <em>the same</em> matrix <span class=""math-container"">$W_2$</span>.
<strong>Examples:</strong> <a href=""https://stackoverflow.com/q/45431179/9352077"">this question</a> and <a href=""https://stats.stackexchange.com/a/198826/360389"">this answer</a>.</p>
</li>
<li><p>Skip-gram predicts <span class=""math-container"">$1$</span> softmax of size <span class=""math-container"">$|V|$</span> using one matrix <span class=""math-container"">$W_2$</span>, and the dataset consist of (center word, context word) pairs instead of (center word, all context words).
<strong>Examples:</strong> <a href=""https://www.geeksforgeeks.org/implement-your-own-word2vecskip-gram-model-in-python/"" rel=""nofollow noreferrer"">this tutorial</a>.</p>
</li>
<li><p>Skip-gram is a binary classifier predicting whether a given word is &quot;in context&quot; vs. &quot;out of context&quot; for another word. The dataset consists of pairs of words that either appeared close to each other (positive) or were randomly paired (negative).
<strong>Examples:</strong> <a href=""https://www.kdnuggets.com/2018/04/implementing-deep-learning-methods-feature-engineering-text-data-skip-gram.html"" rel=""nofollow noreferrer"">this article</a>, <a href=""https://towardsdatascience.com/nlp-101-negative-sampling-and-glove-936c88f3bc68"" rel=""nofollow noreferrer"">this article</a>, and <a href=""https://stats.stackexchange.com/a/245452/360389"">this answer</a>.</p>
</li>
</ol>
<p>Skip-gram is usually explained as having <em>one</em> output matrix, which is hypothesis (2) or (3), yet producing <span class=""math-container"">$C$</span> different words in the output layer, from the same hidden representation, which is hypothesis (1). See <a href=""https://towardsdatascience.com/skip-gram-nlp-context-words-prediction-algorithm-5bbf34f84e0c"" rel=""nofollow noreferrer"">this article</a> and the figure in <a href=""https://stats.stackexchange.com/q/341594/360389"">this question</a>. That's a magic trick. Pure fiction.</p>
<p>I find (3) the most reasonable, since it only has two weight matrices and doesn't produce useless copies. In that case, <em>skip-gram actually doesn't predict any words</em>, but rather produces a single <span class=""math-container"">$|V|$</span>-sized vector which represents the average one-hot vector of the context words. This single vector can then account for one term in the loss per context word.</p>
<p>Yet, there are some major issues with (3):</p>
<ul>
<li><p>You can never reach <span class=""math-container"">$0$</span> loss. Let's say the loss function is log likelihood, and compute the loss for one context window:
<span class=""math-container"">$$
\ell(\text{word}, \text{context}) = \sum_{i=1}^C \ln P(\text{context}_i \mid \text{word})
$$</span>
Clearly, that probability cannot be <span class=""math-container"">$1$</span> for each context word. In the perfect case, it is <span class=""math-container"">$1/C$</span>.</p>
</li>
<li><p>It cannot handle negative examples like (4). Let <span class=""math-container"">$y$</span> be a binary variable that is <span class=""math-container"">$1$</span> when a pair of words is positive and <span class=""math-container"">$0$</span> when it is negative. What is <span class=""math-container"">$P(y \mid w_1,w_2)$</span> given the above model? It can't just be <span class=""math-container"">$P(w_2 \mid w_1)$</span>. Why? Look at this binary loss:
<span class=""math-container"">$$\begin{aligned}
\ell(w_1, w_2, y) &amp;= y\ln P(y = 1 \mid w_1, w_2) + (1-y)\ln (1 - P(y = 1 \mid w_1,w_2)) \\ &amp;= y\ln P(w_2 \mid w_1) + (1-y)\ln (1 - P(w_2 \mid w_1))
\end{aligned}$$</span>
As we saw before, <em>best-case scenario</em>, <span class=""math-container"">$P(w_2 \mid w_1) = 1/C$</span> for a positive example, which means <span class=""math-container"">$P(y = 0 \mid w_1,w_2) = 1 - 1/C$</span>, clearly way higher. For example: with a context of <span class=""math-container"">$C = 4$</span>, the model predicts that a word is context by giving it 25% probability, and accidentally assigns the remaining 75% to all other words, so it always predicts that a word is out of context. You simply <em>cannot combine softmax likelihood with negative sampling</em>. <a href=""https://stats.stackexchange.com/a/446129/360389"">This answer</a> specifically says that softmax likelihood isn't used at all, and instead we use &quot;noise-contrastive estimation&quot;.</p>
</li>
</ul>
<p>So, once and for all: does Word2Vec's skip-gram NNLM even produce context words?</p>
","nlp"
"115127","How to solve mismatched code dependency issues between two or more different ML models/structures/frameworks?","2022-10-11 13:50:07","","1","22","<nlp><pipelines>","<p>I'm fairly new to machine learning. Currently I am trying to build a pipeline that uses two established NLP models. One is BERT that is fairly easy to load and quite structured. Another is FLAIR which has certain dependencies. I am trying to use one environment to load all packages from both, but they obviously use different versions of different packages. For example, different versions of numpy and pytorch. I cannot seem to combine them into one platform. I have faced this problem quite often with combining different models/frameworks (like tensorflow and pytorch) etc.</p>
<p>My question: is there any standard way to deal with these kinds of dependency issues? Do I have to have separate environments to load the respective packages? How do developers commonly deal with this?</p>
","nlp"
"115094","Convert English active voice sentences into passive voice sentences using Machine learning","2022-10-11 06:22:39","115099","-1","297","<machine-learning><python><nlp>","<p>Is there any machine learning algorithm developed to convert an English active voice sentence into a passive voice sentence? And what are the datasets available related to that purpose? And also if there are available source codes related to that research idea please mentioned them too.</p>
","nlp"
"115092","Abstracted text summarisation and generation from weighted keywords","2022-10-11 00:57:03","115129","1","84","<nlp><text-mining><sentiment-analysis><text-generation>","<p>Suppose I have a list of weighted keywords/phrases, such as &quot;solar panel&quot;, &quot;rooftop&quot;, etc. The weights are in [0,1] with higher weights indicating a stronger preference for specific keywords, so &quot;solar panel&quot; may have a weighting of 0.3 and &quot;rooftop&quot; may have a weighting of 0.2, for example. The sum of keyword weights is 1.</p>
<p>For each keyword/phrase, I additionally have a number of contextual sentences which are also weighted and carry a positive, negative, or neutral sentiment/connotation. For example, one contextual sentence related to the &quot;solar panel&quot; phrase might be &quot;good for the environment&quot; which is labelled with a positive sentiment and carries a weight of 0.2. The sum of weights for each keyword's contextual sentences is 1, so the sum of weights for all contextual sentences across all keywords is N, where N is the number of individual keywords.</p>
<p>Finally, I also have weighted linkages in [0,1] between keywords/phrases which, again, sum to 1. For example, the directed linkage from &quot;solar panel&quot; to &quot;rooftop&quot; may have a weight of 0.2 while the directed linkage from &quot;rooftop&quot; to &quot;solar panel&quot; may have a weight of 0.4.</p>
<p>I would like to use these weighted keywords, phrases, contextual sentiment-labelled sentences and linkages to create a summary in natural language. I realise that I'm working in reverse from the typical text summarisation objective, but I believe that the richness of my data should make the task a little easier.</p>
<p>How should I approach it? Should I first use a model to summarise the text contained within each of the contextual sentences before attempting to extract more basic keywords that can be used to generate summary text? How should I process the data? Is it worth pursuing a two-step approach, where a basic model summarises the keywords and contextual sentences in basic language before a secondary model transforms it to richer, more natural language?</p>
<p>I would be very grateful for any guidance or recommendations.</p>
<p>Edit: I'm very new to NLP, so I apologise for my lack of terminology and mathematical formalism.</p>
","nlp"
"115052","Semantic similarity on a large dataset","2022-10-10 00:17:15","","2","927","<machine-learning><nlp><cosine-distance>","<p>I'm going through <a href=""https://newscatcherapi.com/blog/ultimate-guide-to-text-similarity-with-python"" rel=""nofollow noreferrer"">this</a> guide on semantic similarity and use the code there as is.</p>
<p>I'm applying it to a dataset where each row is typically a paragraph (3-4 sentences, over 100 words). Currently, I have over 100k observations, but this number is likely to grow to 500k.</p>
<p>I want to measure semantic similarity between all rows.</p>
<p>When I test BoW and TFIDF on around 20-30k sample, I don't get any performance issues (even without cleaning, stopwords, etc.).</p>
<p>When I try Word2Vec/Universal Sentence Encoder, however, it takes couple of hours to finish even on 3-4k rows sample .</p>
<p>I also get completely different results, but that's beyond the point.</p>
<p>Is there a way to improve the performance for Word2Vec/Universal Sentence Encoder, especially the latter. (As far as I understand, in Word2Vec, words &quot;good&quot; and &quot;bad&quot; may <a href=""https://stackoverflow.com/a/57932912"">cancel each other out</a>, which is not good for my speach-like data.)</p>
","nlp"
"115033","Mathematically rigorous NLP","2022-10-09 09:28:57","115038","1","147","<nlp><probability>","<p>I'm looking for resources (books/articles/whatever) that provide mathematical formalization of NLP and statistical language theory. By that I mean clear exposition of the subject in terms of probability spaces (measure spaces) and so on. For example, many NLP books (like the Manning's one) use n-gram models which, as I see, may be modelled as Markov processes with word-states, but neither book states explicitly how the probability space for the process is constructed (I guess, there's something related to probabilities on formal languages?). I need such clear expositions. Thanks in advance.</p>
","nlp"
"115024","Dialogue history encoding for multi-turn dialogues using Seq2seq","2022-10-08 17:16:48","","0","72","<nlp><sequence-to-sequence><text-generation>","<p>In single-turn dialogue seq2seq models where the goal is to produce a good answer y to a query x, sentences are usually encoded such that x is fed to the encoder, while the decoder is only given a &quot;START&quot; token and the output from the encoder. Often, attention is incorporated into the decoding step by eg concatening the decoder output at time t with the encoder output and performing attention on the resulting vector: attention(y_t, encoder_{out}).</p>
<p>What are common ways to encode multi-turn dialogues? The idea here is that the model can be queried k times (where k is the number of turns in the dialogue) for one conversation and has to keep track of the dialogue context throughout to generate coherent responses.</p>
<p>A simplistic way to do it would be to simply concatenate the dialogue history over all turns and feed it to the encoder, but I am unsure whether different RNNs (eg LSTMs or GRUs) can handle longer sequences and still maintain a high feature quality.</p>
","nlp"
"115013","N-gram language model for preposition predition","2022-10-08 08:31:41","115016","1","95","<machine-learning><nlp><ngrams>","<p>I am trying to build N gram models to predict the missing prepositions of a text corpus.</p>
<p>I would want to have some guidance on if I'm understanding and doing things correctly.</p>
<p>So the N gram model is basically just a collection of posterior probabilities? Pr(this word | previous words)?
Then how is this machine learning I wonder? Since we would get a deterministic set of probabilities based on the frequencies of the word combinations from the training set. There doesn't seem to be any parameters to learn except in interpolation (like the weights of each gram in their weighted sum).</p>
<p>As for the actual prediction of preposition, after getting a set of the posterior probabilities of all the words in the vocabulary, do I simply only compare the posterior probabilities of the few known prepositions and find the argmax as the prediction?</p>
<p>Appreciate any help, thanks!</p>
","nlp"
"114959","Accuracy is getting worse after text pre processing","2022-10-06 11:50:40","114989","3","1901","<machine-learning><python><nlp><scikit-learn><text-classification>","<p>I'm working a multi-class text classification project.</p>
<p>After splitting the dataset into train and test datasets, I've applied the below function on the train dataset (AKA pre processing):</p>
<pre><code>STOPWORDS = set(stopwords.words('english'))

def clean_text(text):   
    # lowercase text
    text = text.lower() 
    
    # delete bad symbols
    text = re.sub(r&quot;(@\[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)|^rt|http.+?&quot;, &quot;&quot;, text)  
  
    # delete stopwords from text
    text = ' '.join(word for word in text.split() if word not in STOPWORDS) 

    # Stemming the words
    text = ' '.join([stemmer.stem(word) for word in text.split()])
    
    return text
</code></pre>
<p>To my surprise, I've got much worst results (i.e. va_accuracy) applying on the train dataset rather than just &quot;do nothing&quot; (59% vs 69%)</p>
<p>I've literally commented out the apply line in the below section:</p>
<pre><code>all_data = dataset.sample(frac=1).reset_index(drop=True)
train_df, valid = train_test_split(all_data, test_size=0.2)

train_df['text'] = train_df['text'].apply(clean_text)

</code></pre>
<p>What am I missing?
How can it be that pre processing steps decreased accuracy?</p>
<p><strong>A bit more info</strong></p>
<p>I forgot to mention I'm using the below to tokenize the text:</p>
<pre><code>X_train = train.iloc[:, :-1]
y_train = train.iloc[:, -1:]
X_test = valid.iloc[:, :-1]
y_test = valid.iloc[:, -1:]

weights = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(y_train), 
                                            y=y_train.values.reshape(-1))
le = LabelEncoder()
le.fit(weights)
class_weights_dict = dict(zip(le.transform(list(le.classes_)), weights))


tokenizer = Tokenizer(num_words=vocab_size, oov_token='&lt;OOV&gt;')
tokenizer.fit_on_texts(X_train['text'])

train_seq = tokenizer.texts_to_sequences(X_train['text'])
train_padded = pad_sequences(train_seq, maxlen=max_length, padding=padding_type, truncating=trunc_type)

validation_seq = tokenizer.texts_to_sequences(X_test['text'])
validation_padded = pad_sequences(validation_seq, maxlen=max_length, padding=padding_type, truncating=trunc_type)
</code></pre>
<p>Later on I'm fitting all into the model as follows:</p>
<pre><code>model = Sequential()
model.add(Embedding(vocab_size, embedding_dim, input_length=train_padded.shape[1]))

model.add(Conv1D(48, len(GROUPS), activation='relu', padding='valid'))
model.add(GlobalMaxPooling1D())
model.add(Dropout(0.5))

model.add(Flatten())
model.add(Dropout(0.5))

model.add(Dense(len(GROUPS), activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

epochs = 100
batch_size = 32

history = model.fit(train_padded, training_labels, shuffle=True ,
                    epochs=epochs, batch_size=batch_size,
                    class_weight=class_weights_dict,
                    validation_data=(validation_padded, validation_labels),
                    callbacks=[ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001), 
                               EarlyStopping(monitor='val_loss', mode='min', patience=2, verbose=1),
                               EarlyStopping(monitor='val_accuracy', mode='max', patience=5, verbose=1)])
</code></pre>
","nlp"
"114958","LSTM Feature engineering: using different Knowledge Graph data types","2022-10-06 09:54:10","114969","1","223","<deep-learning><nlp><lstm><feature-engineering><knowledge-graph>","<p>For a research project, I'm planning to use an LSTM to learn from sequences of KG entities. However, I have little experience using LSTMs or RNNs in general. During planning, a few questions concerning feature engineering have come up.</p>
<p>Let me give you some context:<br />
My initial data will be a collection of <span class=""math-container"">$n$</span> texts.
From these texts, I will extract <span class=""math-container"">$n$</span> sequences of entities of variable length using a DBPedia or Wikidata tagger. Consequently, I'll have <span class=""math-container"">$n$</span> sequences of KG entities that somehow correspond to their textual counterparts.</p>
<p>Most LSTM implementations I've seen take only one type of feature as input. However, as we're dealing with knowledge graphs, we have access to more types of information. I'm wondering what would be a good strategy to use more than just one type of feature.</p>
<h2>Objective</h2>
<p>Given a sequence of seen entities, I want the model to predict the continuation of that sequence. A set of truncated sequences from the corpus will be kept apart. The beginnings will serve as prompts and the endings will be truth values for evaluation. <br />
I'm also interested in the model's prediction probabilities when predicting following entities for one single entity given as a prompt.</p>
<h2>Assumptions</h2>
<p>I assume that diverse types of features will help the model make good predictions. Specifically, I want the model to learn not only from entity sequences but also from KG 'metadata' like associated RDF classes or pre-computed embedding vectors.</p>
<h2>Features</h2>
<h3>Feature 1: Numerical vocabulary features</h3>
<p>The simplest case I can think of is to create an orderet set from all extracted entities. <br />
For example, if the extracted entities from all my documents were <code>[U2, rock, post-punk, yen, Bono, revolutionary, guitar]</code> (in reality that'll probably be a few thousands more), I'd create this ordered set representing my <em>vocabulary</em>:</p>
<pre><code>{1: http://dbpedia.org/resource/U2, 2: http://dbpedia.org/resource/Rock_music, 3: http://dbpedia.org/resource/Post-punk, 4: http://dbpedia.org/resource/Japanese_yen, 5: http://dbpedia.org/resource/Bono, 6: http://dbpedia.org/resource/Revolutionary, 7: http://dbpedia.org/resource/Acoustic_guitar}
</code></pre>
<p>The training data for the LSTM would then be sequences of integers such as</p>
<pre><code>training_data = [
# Datapoint 1
[[1, 2, 3, 4, 5, 6, 7]],        #document 1
# Datapoint 2
[[5, 3, 3, 1, 6]],              #document 2
# Datapoint 3
[[2, 4, 5, 7, 1, 6, 2, 1, 7]],  #document 3
...]
</code></pre>
<h3>Feature 2: Numerical class features</h3>
<p>I want to include additional information about RDF classes. Similar to the approach in <strong>Feature 1</strong>, I could create an ordered set containing all possible classes. However, the difference is that each entity belongs to one or more classes</p>
<p>If all classes extracted were</p>
<pre><code>{1: owl:Thing, 2: dbo:MusicGenre, 3: dbo:Agent, 4: dbo:Person, 5: dbo:PersonFunction}
</code></pre>
<p>I would create a new data structure for each data point, this time containing class information. The notation represents <code>{entity: [classes]}</code>. My training data could then look something like this:</p>
<pre><code>training_data = [
# Datapoint 1
[
[1, 2, 3, 4, 5, 6, 7],                      # feature 1
{1: [1,2,4], 2: [2,3,4,5], ..., 7: [3,5]}   # feature 2
],
# Datapoint 2
[
[5, 3, 3, 1, 6],                            # feature 1
{1: [2,3,4], 2: [1,2,4,5], ..., 5: [3,5]}   # feature 2
],
# Datapoint 3
[
[2, 4, 5, 7, 1, 6, 2, 1, 7],                # feature 1
{1: [1,2,4], 2: [1,2,3,5], ..., 9: [2,3]}   # feature 2
],
...]
</code></pre>
<h3>Feature 3: RDF2Vec embeddings</h3>
<p>Each KG entity from a collection of entities can be mapped into a low-dimensional space using tools like RDF2Vec. I'm not sure whether to use this feature or not as its latent semantic content might interfere with my research question, but it is an option. <br />
Embedding features, in this case, are vectors of length 200:</p>
<pre><code>embedding_vector = tensor([5.9035e-01, 2.6974e-01, 8.6569e-01, 8.9759e-01, 9.3032e-01, 5.2442e-01, 9.6031e-01, 1.8393e-01, 6.3000e-01, 9.5930e-01, 2.5407e-01, 5.6510e-01, 8.1476e-01, 2.0864e-01, 2.7643e-01, 4.8667e-02, 9.3791e-01, 8.0929e-02, 5.0237e-01, 1.4946e-01, 5.9263e-01, 4.7912e-01, 6.8907e-01, 4.8248e-03, 4.9926e-01, 1.5715e-01, 7.0777e-01, 6.0065e-01, 2.6858e-01, 7.2022e-01, 4.4128e-01, 4.5026e-01, 1.9987e-01, 2.8191e-01, 1.2493e-01, 6.0253e-01, 6.9298e-01, 2.5828e-01, 2.8332e-01, 9.6898e-01, 4.5132e-01, 4.6473e-01, 8.0197e-01, 8.4105e-01, 8.8928e-01, 5.5742e-01, 9.5781e-01, 3.8824e-01, 4.6749e-01, 4.3156e-01, 2.8375e-03, 1.5275e-01, 6.7080e-01, 9.9894e-01, 7.2093e-01, 2.7220e-01, 8.5404e-01, 6.9299e-01, 3.9316e-01, 8.9538e-01, 8.1654e-01, 4.1633e-01, 9.6143e-01, 7.1853e-01, 9.5498e-01, 4.5507e-01, 3.6488e-01, 6.3075e-01, 8.0778e-01, 6.3019e-01, 4.4128e-01, 7.6502e-01, 3.2592e-01, 9.5351e-01, 1.1195e-02, 5.6960e-01, 9.2122e-01, 3.3145e-01, 4.7351e-01, 4.5432e-01, 3.7222e-01, 4.3379e-01, 8.1074e-01, 7.6855e-01, 4.0966e-01, 2.6685e-01, 2.4074e-01, 4.1252e-01, 1.9881e-01, 2.2821e-01, 5.9354e-01, 9.8252e-01, 2.7417e-01, 4.2776e-01, 5.3463e-01, 2.9148e-01, 5.8007e-01, 8.2275e-01, 4.8227e-01, 8.5314e-01, 3.6518e-01, 7.8376e-02, 3.6919e-01, 3.4867e-01, 8.9571e-01, 2.0085e-02, 7.9924e-01, 3.5849e-01, 8.7784e-01, 4.6861e-01, 6.2004e-01, 6.8465e-01, 4.1273e-01, 4.2819e-01, 9.4532e-01, 2.2362e-01, 8.3943e-01, 1.1692e-01, 6.9463e-01, 7.6764e-01, 2.8046e-02, 6.9382e-01, 9.2750e-01, 3.6031e-01, 6.8065e-01, 1.6976e-01, 8.2079e-01, 6.4580e-01, 8.3944e-01, 3.9363e-01, 4.4026e-01, 4.4569e-01, 8.2344e-01, 5.4172e-01, 1.6886e-04, 3.8689e-01, 5.8966e-01, 1.9510e-02, 2.5976e-01, 4.0868e-01, 3.1406e-01, 3.6334e-01, 6.1768e-01, 5.4854e-01, 4.1273e-01, 7.2670e-04, 2.4486e-01, 4.1042e-01, 9.0760e-01, 1.6224e-01, 7.4019e-02, 8.1329e-01, 7.2573e-01, 8.2816e-01, 7.3032e-01, 6.6017e-01, 6.4281e-01, 4.1839e-01, 9.2251e-01, 1.5183e-02, 4.4538e-01, 9.7205e-01, 9.5677e-01, 9.5649e-01, 1.2610e-01, 9.2521e-01, 3.2649e-01, 2.1019e-02, 2.5695e-01, 4.2663e-01, 9.2064e-01, 4.5242e-01, 7.0447e-01, 8.1233e-01, 2.7507e-01, 2.4744e-01, 1.3670e-01, 6.4032e-01, 5.8332e-01, 5.5130e-01, 2.4997e-02, 7.7206e-01, 1.5085e-01, 2.8028e-01, 8.2839e-01, 5.8292e-01, 9.9087e-01, 6.0233e-01, 4.1489e-01, 6.4902e-01, 7.5428e-01, 8.0953e-01, 3.7530e-01, 4.8196e-01, 1.8786e-01, 9.8463e-01, 6.3303e-01, 4.8519e-01, 7.6163e-01, 3.3821e-01]
</code></pre>
<p>If I included this in my training data, it would look something like this:</p>
<pre><code>training_data = [
# Datapoint 1
[
[1, 2, 3, 4, 5, 6, 7],                      # feature 1
{1: [1,2,4], 2: [2,3,4,5], ..., 7: [3,5]},  # feature 2
[7 embedding vectors],                      # feature 3
],
# Datapoint 2
[
[5, 3, 3, 1, 6],                            # feature 1
{1: [2,3,4], 2: [1,2,4,5], ..., 5: [3,5]},  # feature 2
[5 embedding vectors],                      # feature 3
],
# Datapoint 3
[
[2, 4, 5, 7, 1, 6, 2, 1, 7],                # feature 1
{1: [1,2,4], 2: [1,2,3,5], ..., 9: [2,3]},  # feature 2
[9 embedding vectors],                      # feature 3
],
...]
</code></pre>
<h2>Questions</h2>
<p>My training data will consist of lists of variable length <em>and</em> matrices/tensors. How do I best feed this data to the model?  In any case, I'm interested in predicting only entities. Training only on feature 1 could be a baseline that I compare to combinations of features, e.g. Features 1+2 or 1+3 or 1+2+3</p>
<p>Based on what I've read until now, I think I'm going to use padding and masking. However, I'm not sure what my features should finally look like.</p>
<p>I appreciate any kind of feedback.
Thanks for sharing your thoughts!</p>
","nlp"
"114932","Advantages of different tokenizers for NLP (specifically text generation)","2022-10-05 12:43:02","114941","0","120","<nlp><text-generation><tokenization>","<p>What are the advantages of using different tokenizers? For example, let's take the sentence:
&quot;In Düsseldorf I took my hat off. But I can't put it back on.&quot;</p>
<p>The treebank tokenizer yields: &quot;In Düsseldorf I took my hat off . But I ca n't put it back on . &quot;</p>
<p>However, the whitespace tokenizer would yield:
&quot;In Düsseldorf I took my hat off . But I can't put it back on . &quot;</p>
<p>NLTK has four tokenizers:</p>
<ul>
<li>TreebankWordTokenizer</li>
<li>WordPunctTokenizer</li>
<li>PunctWordTokenizer</li>
<li>WhitespaceTokenizer</li>
</ul>
<p>When should you use which one? For my project, I am interested in text generation, so I am leaning toward the whitespace tokenizer. Is this a good choice? Won't my model generate nonsense tokens like &quot;n't&quot; when I use eg the treebank tokenizer?</p>
","nlp"
"114897","Combine datasets of different domains to ehance generalizibility","2022-10-04 12:59:50","114914","1","36","<machine-learning><deep-learning><nlp><dataset>","<p>so I try to implement an Emotion Classifier, which should detect several emotions from a text. There are several datasets for this (ISear, GoEmotions, etc.). However, a lot of them come from different domains, e.g. from Chats, Blogs, Newsarticles, etc.</p>
<p>My Emotion Classifier should not be limited to a domain, so I basically combined each dataset (where I only considered the emotion: anger, disgust, neutral, happy, fear) and trained my model with it. My goal is to get an Emotion Classifier which generalizes well, also maybe on unknown use cases. So everyone can use it. It is worth highlighting, that I got an accuracy from 63-67% for each dataset I used here.</p>
<p>Now I wanted to know is this a reasonable approach? Which challenges and disadvantages are possible? Is there a paper, which is specifically discussing this kind of topic? Or do you have another idea how I could possibly solve this differently</p>
","nlp"
"114872","How do you train an ML algorithm to achieve a desirable clustering?","2022-10-03 12:54:58","","1","27","<machine-learning><nlp><clustering>","<p>Most clustering examples on the net are <strong>unsupervised</strong> learning.
There is a given vectorization into a 2D space and the algorithm discovers clusters.</p>
<p>However, what if the input data that I want it to train for is <em>desired clustering</em>?
So that the training data would be similar to classification, but in an open ended result set.</p>
<p>An example would be this list of entries</p>
<ul>
<li>Susan walks</li>
<li>Susan is sitting.</li>
<li>Hear Susan sing.</li>
<li>David walks</li>
<li>Robert sits</li>
</ul>
<p>The verbs (intents) are finite, Walk, Sing, Sit.
But in addition to classification of intent, I also need to cluster the intents to identify the <em>entities</em>. This is an open ended set.</p>
<p>If I have 100 000 records of data on this form with identified verb and entity.</p>
<pre><code>Susan walks      | Walk | Susan
Susan is sitting | Sit  | Susan
Hear Susan sing  | Sing | Susan
David walks      | Walk | David
Robert sits      | Sit  | Robert
</code></pre>
<p>The language has been simplified in this example. The point is that there may be variations on phrasing, but it is pretty simple english. The input data will have a lot of different names.</p>
<p>I would like an engine that would be good at classifying intent (easily solved), but it should <em>also</em> group together the intents <strong>that are likely to refer to the same person</strong>. So that the runtime can encounter &quot;Alexander&quot; and will be able to cluster his datapoints even though the name wasn't in the training data.</p>
<p>Is this poissible? What sort of algorithm is this?</p>
<p>And what would the training data <em>look like</em>?</p>
<pre><code>Susan walks      | Walk | PersonA
Susan is sitting | Sit  | PersonA
Hear Susan sing  | Sing | PersonA
David walks      | Walk | PersonB
Robert sits      | Sit  | PersonC
</code></pre>
<p>I want the algorithm to identify the correct <em>clusters</em>. I don't want the names &quot;Susan&quot; or &quot;David&quot; in the last column. Nor do I want &quot;PersonA&quot; there, because it would be just as correct to call Susan &quot;PersonB&quot;. But I want some way of indicating desirable clustering without naming them.</p>
<p>Most clustering examples I have looked at have already solved the problem of vectorization. But I want the computer to decide the correct vectorization to use that will produce the <strong>desirable clustering</strong>.</p>
<p>I took a look at LUIS that does a separation between <em>intent</em> and <em>entities</em>. But it seemed that the &quot;entity&quot; was just another classification within a predefined set. Not that you could use the data to discover entity instances.</p>
","nlp"
"114861","Newbie questions: real-time clustering of messages","2022-10-03 06:52:43","","2","29","<nlp><clustering><text-classification><beginner><data-stream-mining>","<p>I'm very much a newbie in NLP, so please accept my apologies if this is an obvious question, the wrong place to ask it or any other error I could be making.</p>
<p>I am considering using NLP for some subset of real-time spam detection in real-time chat. The general idea would be to observe semantic clusters forming in real-time, as they <em>could</em> indicate activation of a wave of spambots. This, by itself, won't be sufficient to indicate that it's spam but I suspect that it would be an interesting data point in the process.</p>
<p>More specifically:</p>
<ul>
<li>my system receives text messages in real-time (e.g. thousands to tens of thousands per second);</li>
<li>I would like to classify them and see if semantic clusters emerge;</li>
<li>I will need to cleanup the data regularly (e.g. remove everything that's older than one hour) to avoid retaining potential private data;</li>
<li>I cannot rely on external services for privacy reasons, so whatever happens, I'll need to write code. I'm fine with that.</li>
</ul>
<p>I figure that I need to encode my text messages into vectors, using e.g. BERT or some other existing model. So far, so good. My difficulties are:</p>
<ul>
<li>real-time classification of a growing dataset, with an unknown number of clusters (I'll be able to experiment with the distance, though);</li>
<li>regular cleanup.</li>
</ul>
<p>Are there any well-known algorithms or libraries that I should look at? I'm not afraid to code and optimize my code, if I have a good reason to believe that it's going to work.</p>
","nlp"
"114819","Increasing/Decreasing importance of feature/thing in ML/DL","2022-09-30 16:59:05","114824","0","56","<machine-learning><deep-learning><nlp><feature-selection><image-classification>","<p>I have 3 cases:</p>
<ol>
<li>I have a classification model that will be used to classify cats and dogs. On my train data dog pictures has a watermark on them, but cat pictures don't. The problem is: Whenever I have a watermark on a cat picture, the model will predict the cat picture as a dog picture</li>
<li>I have another classification model that classifies questions and normal sentences. But whenever I have the &quot;how&quot; word in my normal sentence, the model will classify it as &quot;question&quot;</li>
<li>I have a prediction model. I have 5 columns but column number 3 is very important. I mean the importance of that column is very high. But my model cannot understand it.</li>
</ol>
<p>All of those problems have 1 common problem. The importance of &quot;something&quot; or &quot;feature&quot; is being misunderstood by models. How these kinds of problems can be solved?</p>
","nlp"
"114770","Is there a way to map words to their synonyms in tfidf?","2022-09-28 18:54:30","114773","3","492","<nlp><scikit-learn><nltk><tfidf><spacy>","<p>I have the following code:</p>
<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
import pandas as pd

sentences = [&quot;I have the ability&quot;, &quot;I have the weakness&quot;, &quot;I have the capability&quot;, &quot;I have the power&quot;]

tfidf = TfidfVectorizer(max_features=300)
tfidf.fit(sentences)

X = tfidf.transform(sentences)

k = 2

model = KMeans(n_clusters=k, random_state=1)
model.fit(X)

print(pd.DataFrame(columns=[&quot;sentence&quot;], data=sentences).join(pd.DataFrame(columns=[&quot;cluster&quot;], data=model.labels_)))
</code></pre>
<p>The output looks like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;"">index</th>
<th style=""text-align: center;"">sentence</th>
<th style=""text-align: center;"">cluster</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;"">0</td>
<td style=""text-align: center;"">I have the ability</td>
<td style=""text-align: center;"">0</td>
</tr>
<tr>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">I have the weakness</td>
<td style=""text-align: center;"">0</td>
</tr>
<tr>
<td style=""text-align: center;"">2</td>
<td style=""text-align: center;"">I have the capability</td>
<td style=""text-align: center;"">0</td>
</tr>
<tr>
<td style=""text-align: center;"">3</td>
<td style=""text-align: center;"">I have the power</td>
<td style=""text-align: center;"">1</td>
</tr>
</tbody>
</table>
</div>
<p>As you can see &quot;I have the ability&quot;, &quot;I have the weakness&quot;, &quot;I have the capability&quot; were grouped in the same cluster (cluster 0) and &quot;I have the power&quot; was grouped into a separate cluster. I think they were grouped randomly and it can't tell which sentences actually mean the same thing. I want a way to be able to group &quot;I have the ability&quot;, &quot;I have the capability&quot;, and &quot;I have the power&quot; together by specifying that ability, capability and power are synonyms. So basically mapping all words to their synonyms. Is there an existing package for this?</p>
","nlp"
"114769","Why is max_features ordered by term frequency instead of inverse document frequency","2022-09-28 18:06:25","114784","1","139","<nlp><scikit-learn><tfidf>","<p>In the docs: <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"" rel=""nofollow noreferrer"">https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html</a></p>
<p>it is explained that <code>max_features</code> is ordered by term frequency across the corpus. Why not use the idf?</p>
","nlp"
"114687","Bertopic with embedding: unable to use find_topic","2022-09-26 00:24:00","","2","518","<python><nlp><training><bert>","<p>I've used BERTopic with success for the following tasks: get topics, visualise (topics, barcharts, documents ...) and DTM (extended to get area plot with considerable success).</p>
<p>However, I am unable to use the find_topics() function</p>
<blockquote>
<p><em>(There are a few others I'm struggling with, which I'll post as new questions so as not to conflate this one).</em></p>
</blockquote>
<p>I get an error message indicating that I'm using embedding (which is true).</p>
<pre class=""lang-py prettyprint-override""><code># Prepare embeddings using default 'sentence embedding'
sentence_model = SentenceTransformer(&quot;all-MiniLM-L6-v2&quot;)
embeddings = sentence_model.encode(docs_bert, show_progress_bar=True)
</code></pre>
<p>Trying to solve that, I have tried to instantiate a new model without embedding</p>
<pre class=""lang-py prettyprint-override""><code>model_ngram_embed2 = BERTopic(embedding_model=embeddings)
</code></pre>
<p>but it then throws an error:</p>
<blockquote>
<p>ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()</p>
</blockquote>
<p>I need to instantiate before I can fit_transform the model to my doc (text corpus), after which I would then be able to find_topics().<br />
How do I go about that? What should be done?</p>
<p>Regarding find_topics(), I've read allowing <em><a href=""https://github.com/MaartenGr/BERTopic/issues/79"" rel=""nofollow noreferrer"">precomputed embeddings in bertopic.find_topics()</a></em> issue<br />
NB: <em>Python 3.8.8 | IPython 7.31.1 | BERTopic 0.11.0</em></p>
","nlp"
"114670","One word changes everything NLP","2022-09-24 20:25:10","114672","0","65","<deep-learning><nlp><transformer><bert>","<p>I have a classification model (BERT) that classifies sentences as either question or normal sentences. But whenever a sentence has &quot;how&quot; word, the model chooses &quot;question&quot; class.
How can I solve this issue? (I have a very big dataset.)</p>
","nlp"
"114627","Extract categories from text (unsupervised)","2022-09-23 05:50:21","","1","38","<machine-learning><python><deep-learning><nlp><clustering>","<p>I have a list of bank statements. They look pretty much like this:</p>
<ul>
<li>Received transaction. Reason[separator] invoice from [date][separator][number]. Counterparty[separator] [company name]</li>
<li>Payment to another bank [IBAN]/[company name] inv. [invoice number(s)]</li>
<li>Payment of taxes</li>
<li>Received payment on [invoice numbers][separator][dates]</li>
</ul>
<p>etc.</p>
<p>I need to be able to extract key information from these statements, such as invoice numbers, or counterparty. Separators may vary or may not exist at all. Counterparty names will be different, so I want to be able to recognize that this part of the text is counterparty or at least cluster them all together and then I will name the clusters. Is it possible at all to perform some clustering on each line and then another one that will group the extracted groups? For example, groups of invoices, groups of dates, and so on. Or maybe I need to unite all lines and then cluster but in this case, sometimes I may need 2-3 words if the counterparty name is longer or one string (for example invoice number).</p>
<p>Any advice on what to look for - algorithms,  papers, or some similar examples?</p>
","nlp"
"114623","How can I decide the threshold value for relevance score in a search problem?","2022-09-22 23:05:40","","1","436","<nlp><tfidf><gensim><search-engine>","<p>I am using a LSA/TF-IDF/BM25/Ensemble models for text search and finally calculating similarity score to rank my search. I would like to decide a threshold value for the score, below which I would not like to display anything.
Eg: If my similarity score is less than 0.7, I would like to return &quot;No Result Found&quot;.</p>
<p>I know there wont be any specific value that I can use here, but I would appreciate suggestions on how I can find that value?</p>
<p>My Thoughts:
Idea 1: I can calculate the similarity scores for all past searches and try to find average of top 10 scores for each search, and take a final mean of all those value (Not sure if it would be a good idea).</p>
<p>Idea 2: Deploy model to production with top 15-20 search results and wait to collect users click results, so one insight after collecting result for a month could be, 95% of the time user does not go to any results having score less than 0.60 or something.</p>
","nlp"
"114570","Effectiveness of tf-idf on documents with repeated keywords","2022-09-21 06:25:12","","1","123","<nlp><tfidf><information-retrieval>","<p>I was doing some ML reading and came upon tf-idf. The tf portion counts the relative frequency of a relevant word in a document, while idf measures how common or rare a word is across the corpus.</p>
<p>The IDF formula states that IDF = -log(n<sub>t</sub> / N), with n<sub>t</sub> being the number of documents with the word and N being the corpus. My understanding is that IDF reduces the weights of filler words, such as '<em>a</em>' and '<em>the</em>', to 0.</p>
<p>However, how does the calculation work if all the documents have a relevant keyword? For example, the keyword 'healthcare' may appear in a corpus of presidential speeches. But because the word appears in all the speeches, its idf is 0 and hence the tf-idf is 0, effectively assigning a keyword with a weight of 0.</p>
<p>Is there a variant of TFIDF that tackles this issue?</p>
","nlp"
"114533","Fuzzy Classification in NLP","2022-09-20 04:53:19","","0","92","<machine-learning><nlp><fuzzy-classification>","<p>Is it possible to use use fuzzy classification models such as <strong>fknn, fsvm</strong> in nlp? I mean I've seen people use K-nn, SVM over textual feature datas extracted from twitter/reddit api to detect emotions like depression,, suicide etc. Can we use fuzzy Knn, fuzzy svm for such emotion detection over textual features too? I haven't seen any research paper regarding this issue or anyone who used fKnn or any fuzzy classification model on textual features/social media data to detect emotions. Can anybody explain its answer in details to me?</p>
","nlp"
"114381","Why use cosine similarity instead of scaling the vectors when calculating the similarity of vectors?","2022-09-13 09:31:42","114405","12","3462","<machine-learning><nlp><clustering><similarity>","<p>I'm watching a NLP video on Coursera. It's discussing how to calculate the similarity of two vectors. First it discusses calculating the Euclidean distance, then it discusses the cosine similarity. It says that cosine similarity makes more sense when the size of the corpora are different. That's effectively the same explanation as <a href=""https://datascience.stackexchange.com/questions/27726/when-to-use-cosine-simlarity-over-euclidean-similarity"">given here</a>.</p>
<p>I don't see why we can't scale the vectors depending on the size of the corpora, however. For example in the example from the linked question:</p>
<blockquote>
<ul>
<li>User 1 bought 1x eggs, 1x flour and 1x sugar.</li>
<li>User 2 bought 100x eggs, 100x flour and 100x sugar</li>
<li>User 3 bought 1x eggs, 1x Vodka and 1x Red Bull</li>
</ul>
</blockquote>
<p>Vector 1 and 2 clearly have different norms. We could normalize both of them to have length 1. Then the two vectors turn out to be identical and the Euclidean distance becomes 0, achieving results just as good as cosine similarity.</p>
<p>Why is this not done?</p>
","nlp"
"114379","Threshold determination / prediction for cosine similarity scores","2022-09-13 07:41:14","114386","3","3646","<nlp><transformer><semantic-similarity>","<p>Given a query sentence, we search and find similar sentences in our corpus using transformer-based models for semantic textual similarity.</p>
<ul>
<li><p>For one query sentence, we might get 200 similar sentences with scores ranging from <strong>0.95 to 0.55</strong>.</p>
</li>
<li><p>For a second query sentence, we might get 200 similar sentences with scores ranging from <strong>0.44 to 0.27</strong>.</p>
</li>
<li><p>For a third query sentence, we might only get 100 similar sentences with scores ranging from <strong>0.71 to 0.11</strong>.</p>
</li>
</ul>
<p>In all those cases, is there a way to predict where our threshold should be without losing too many relevant sentences? Having a similarity score of <code>1.0</code> does not mean that two documents are 2X more similar than if the score was <code>0.5</code>. Is there a way to determine the <code>topk</code> (how many of the top scoring sentences we should return) parameter?</p>
","nlp"
"114303","Detect data (web textual content) age","2022-09-10 21:37:58","","1","30","<nlp><time-series><web-scraping><time>","<p>This is a broad question and maybe does not have an answer but I will try.
I have been thinking of some techniques to detect the date of publication of public data in the wild of the internet. Without raising any defamation concerns, admitting data we are crawling comes from trust worthy sources.</p>
<p>The data I am concerned with is text of some length, I mean paragraphs not less than a page length, this is to ignore small omnipresent sentences.</p>
<p>Say for instance a news article from a news media website; What are techniques you can think of to &quot;estimate&quot; first appearance date. One obvious solution would be to check for a date which is logical depending on the time of the crawl. But what else can you think of ? I personally can think of no other way, but still I'm tempted to image other ways.</p>
<p>Excuse my curiosity, this doesn't come from a rushing business/scientific need but still, this can have its applications in the field of fake news detection.</p>
<p>Last attempts from me, would be to assume the crawler is so fast on it's target sites, and consequently any data discovered is tagged with the exact time of crawling (of course excluding news already in the database, which are copied from somewhere else (using hashes or perceptual hashes or something similar for unicity)).</p>
","nlp"
"114273","Issue with Relation Annotation (rel.manual) in Spacy's Prodigy tool","2022-09-09 16:01:22","","1","81","<deep-learning><nlp><spacy>","<p>I am trying to build a relation extraction model via spacy's prodigy tool.</p>
<p>NOTE: ner.manual, ner.correct, rel.manual are all recipes provided by prodigy.</p>
<ol>
<li><p>(ner.manual, ner.correct) The first step involved annotating and training a NER model that can predict entities (this step is done and model is obtained)</p>
</li>
<li><p>The next step involved annotating the relations between the entities. Now, this step could be done int wo different method.
<br>i. Label the entities and relations all from scratch
<br>ii. Use the trained NER model to predict the entities in the UI tool and make corrections to it if needed (similar to ner.correct) and label the relations between the entities</p>
</li>
</ol>
<p>The issue I am now facing is, whenever I use the trained model in the recipe's loop (rel.manual), there is no entities predicted.</p>
<p>Could someone help me with this??</p>
<p>PS: There is no trailing whitespaces issue, i cross-verified it</p>
","nlp"
"114227","Encoder Decoder model for parameter extraction from text input","2022-09-08 05:29:28","","0","57","<deep-learning><nlp><encoder>","<p>I have an input as text from which I want to extract parameters as given in example below.</p>
<p>Input:</p>
<pre><code>&quot;client need to pay penalty of 10%  of amount  if there is delay in project for more than 3 months&quot;
</code></pre>
<p>and output:</p>
<pre><code>penalty = 10% and delay = 3
</code></pre>
<p>assuming there are N number of such parameters.</p>
<p>Here I have thought of using encoder and decoder model . Where I use RNN as encoder for text input  Now I wonder what would be  decoder architecture that will output N parameters and their values.
what is alternative architecture to solve this problem.</p>
<p>Thanks in advance</p>
","nlp"
"114163","How to fine-tune hyperameters of unsupervised training in fasttext?","2022-09-06 13:52:56","","1","642","<machine-learning><nlp><word-embeddings><hyperparameter-tuning><fasttext>","<p>I want train fasttext unsupervised model on my text dataset. However there are many hyperparameters in train_unsupervised method:</p>
<pre><code>    lr                # learning rate [0.05]
    dim               # size of word vectors [100]
    ws                # size of the context window [5]
    epoch             # number of epochs [5]
    minCount          # minimal number of word occurences [5]
    minn              # min length of char ngram [3]
    maxn              # max length of char ngram [6]
    neg               # number of negatives sampled [5]
    wordNgrams        # max length of word ngram [1]
    thread            # number of threads [number of cpus]
    lrUpdateRate      # change the rate of updates for the learning rate [100]
</code></pre>
<p>Some of them influence quality of embeddings dramatically (dim, lr, minn, maxn especially). However I haven't found any method for tuning those hyperparameters. How could I do that? And also, how features of my dataset (mean sentence length for example) may influence choice of some of those hyperparameters?</p>
","nlp"
"114151","How does machine learning algorithms process text?","2022-09-06 10:50:55","","1","24","<machine-learning><nlp><gradient-boosting-decision-trees>","<p>I'm still new in machine learning and I've been trying to expand my knowledge about it. For my first project, I want to classify if a tweet is suicidal or not using the gradient boost algorithm.</p>
<p>I do know that ml models can't process plain text which is why we have to represent them as numbers. These numeric values will be the input features to the machine learning model (correct me if I'm wrong).</p>
<p>But what I don't understand is how these numbers/vectors are being processed by the model to train it and make a prediction.</p>
<p>Hopefully someone can explain how plain text are converted into words and what's happening internally as they are taken as input to the machine learning model.</p>
","nlp"
"114134","Is backpropagation applied every layer the same?","2022-09-05 21:15:03","114141","1","63","<deep-learning><nlp><convolutional-neural-network><training><backpropagation>","<p>For example, I have layers that are pretrained. But while predicted, the loss is very high. But not because of pre-trained layers. Because of not pretrained layers. Will every layer be affected by backprop the same?</p>
","nlp"
"114086","What counts as a token for bpemb's encode_ids_with_eos()","2022-09-04 10:40:29","","0","73","<nlp><rnn><tokenization>","<p>I have probelms understanding bpemb's <code>encode_ids_with_eos()</code> or similar.
When I run the following code i get none-word like segmentations (rather syllalbus based or letters grouped of two in a seemingly arbitrary fashin). But how is that useful? And how is it useful when assigning ids?</p>
<pre><code>print(bpemb_en.encode_ids_with_eos([&quot; , &quot;, &quot;. &quot;]))
print(bpemb_en.encode_ids([&quot; , &quot;, &quot;.   &quot;] ))
print(bpemb_en.encode_with_eos([&quot; , &quot;, &quot;.   &quot;] ))
print(bpemb_en.encode_with_eos([&quot;Canvas and grouped People&quot;, &quot;.   &quot;] ))
print(bpemb_en.encode_with_eos([&quot;Canvas and grouped people&quot;, &quot;.   &quot;, &quot;I went to China to eat a roll of meat&quot;, &quot;I am just an ordinary person&quot;] ))
print(bpemb_en.encode_ids_with_bos_eos([&quot;Olla , &quot;, &quot;.   &quot;, &quot;_&quot;]))
print(bpemb_en.encode_with_bos_eos([&quot;Olla , &quot;, &quot;.   &quot;, &quot;_&quot;]))
print(bpemb_en.encode_with_bos_eos([&quot;&lt;PAD&gt;&quot;, &quot;Olla , &quot;, &quot;.   &quot;, &quot;_&quot;]))


OUTPUT:
[[912, 934, 2], [896, 2]]
[[912, 934], [896]]
[['▁', ',', '&lt;/s&gt;'], ['▁.', '&lt;/s&gt;']]
[['▁can', 'v', 'as', '▁and', '▁group', 'ed', '▁people', '&lt;/s&gt;'], ['▁.', '&lt;/s&gt;']]
[['▁can', 'v', 'as', '▁and', '▁group', 'ed', '▁people', '&lt;/s&gt;'], ['▁.', '&lt;/s&gt;'], ['▁i', '▁w', 'ent', '▁to', '▁ch', 'ina', '▁to', '▁e', 'at', '▁a', '▁ro', 'l', 'l', '▁of', '▁me', 'at', '&lt;/s&gt;'], ['▁i', '▁am', '▁j', 'ust', '▁an', '▁or', 'd', 'in', 'ary', '▁pers', 'on', '&lt;/s&gt;']]
[[1, 13, 922, 922, 914, 912, 934, 2], [1, 896, 2], [1, 912, 976, 2]]
[['&lt;s&gt;', '▁o', 'l', 'l', 'a', '▁', ',', '&lt;/s&gt;'], ['&lt;s&gt;', '▁.', '&lt;/s&gt;'], ['&lt;s&gt;', '▁', '_', '&lt;/s&gt;']]
[['&lt;s&gt;', '▁', '&lt;', 'p', 'ad', '&gt;', '&lt;/s&gt;'], ['&lt;s&gt;', '▁o', 'l', 'l', 'a', '▁', ',', '&lt;/s&gt;'], ['&lt;s&gt;', '▁.', '&lt;/s&gt;'], ['&lt;s&gt;', '▁', '_', '&lt;/s&gt;']]

</code></pre>
","nlp"
"114071","Avoid leakage in NLP extraction","2022-09-03 17:09:21","114139","1","116","<nlp><training><model-evaluations><data-leakage>","<p>What is best practice for applying traditional NLP extraction techniques a pre-processing for ML models?</p>
<p>Given a pipeline:</p>
<ol>
<li>Collect raw data.</li>
<li>Parse full data set with a variety of traditional NLP techniques, to create model-compatible features (e.g. one-hot encoded matrix of entity extraction).</li>
<li>Train a ML model on the data.</li>
</ol>
<p>My intuition says you must split the data inbetween step 1 and 2, for example, only running TF-IDF or NMF on your training set.</p>
<p><strong>But, I have seen a lot in papers and production, that non-deep learning NLP techniques are often used before a data split.</strong></p>
","nlp"
"114058","Dataset Format for fine tuning deepset/roberta-base-squad2 hugging face transformer model","2022-09-03 09:44:58","","1","380","<nlp><tensorflow><pytorch><transformer><huggingface>","<p>I have been trying to fine tune the roberta model for QnA to my specific domain (healthcare).
I am unable to find the correct way to provide the dataset format to the tokenizer in order to fine tune the model.</p>
<p>Sample Dataset format -&gt;</p>
<pre><code>train_data = (
{
    'context':'context for the training data',
    'answers':{'text':['answer 1'],'answer_start':[115],'answer_end':[138] },
    'question':'question1'
},
{
    'context':'context for the training data',
    'answers':{'text':['answer 2'],'answer_start':[115],'answer_end':[138] },
    'question':'question2'
},
{
    'context':'context for the training data',
    'answers':{'text':['answer 3'],'answer_start':[115],'answer_end':[138] },
    'question':'question3'
}
)
</code></pre>
<p>Can anyone help me with the correct format to provide to tokenizer?</p>
<p>Thanks in advance.</p>
","nlp"
"114007","Text cleaning when applying Sentence Similarity / Semantic Search","2022-09-01 08:01:24","114036","0","659","<nlp><data-cleaning><transformer><semantic-similarity>","<p>Do we need to apply text cleaning practices for the task of sentence similarity?</p>
<p>Most models are being used with whole sentences that even have punctuation. Here are two example sentences that we wish to compare using SentenceTransformer (<strong>all-MiniLM-L6-v2</strong>):</p>
<pre><code>sentences = [
    &quot;Oncogenic KRAS mutations are common in cancer.&quot;,
    &quot;Notably, c-Raf has recently been found essential for development of K-Ras-driven NSCLCs.&quot;] 

# yields that sentence 2 has a score of 0.191 when compared with sentence 1
</code></pre>
<p>Will cleaning those sentences change its semantic meaning?</p>
<pre><code>cleaned = ['oncogenic bras mutations common cancer', 
           'notably c-raf recently found essential development bras driven nsclcs.']

# yields that sentence 2 now has a score of 0.327 when compared to sentence 1
</code></pre>
<p>It seems the model works better when the text is cleaned. However, nowhere does it say that the input sentences are being / should be cleaned? Would love to know your takes on this.</p>
<hr />
","nlp"
"113841","FileNotFoundError: Unsuccessful TensorSliceReader constructor","2022-08-25 12:15:43","","1","1724","<machine-learning><nlp><tensorflow><deployment>","<p>I am trying to deploy my model. I am encountering the following problem:</p>
<blockquote>
<p>FileNotFoundError: Unsuccessful TensorSliceReader constructor: Failed
to find any matching files for
ram://a603e930-4fda-4105-8554-7af5e5fc02f5/variables/variables You may
be trying to load on a different device from the computational device.
Consider setting the experimental_io_device option in
<code>tf.saved_model.LoadOptions</code> to the io_device such as <code>'/job:localhost'</code></p>
</blockquote>
<p>This happened when I stored an NLP model in a pickle. I have seen that it does not work so I then tried to save the model as <code>.h5</code> The problem still persists and shows me the above error</p>
","nlp"
"113817","Normalize summary of customer feedback text / word-cloud /word-count","2022-08-24 14:25:55","113836","1","37","<nlp>","<p>I am trying to make a first analysis on the interest of people feedback from their emails. For a first analysis I made with a simple wordcount to know the key words.</p>
<p>I am facing the following problem: some customers give very short feedback and others a single customer gives very long feedback so the wordcount mechanism that simply counts words gives more weight to the customer who writes more, which may not be the most important.</p>
<p>i.e</p>
<p><code>customer_1</code>: I would like to know the normative about Covid, cause I m covid vaccinated... covid ..covid (2000 words) # word covid appear 13 times</p>
<p><code>customer_2</code>: I m worry about price (100 words)<br />
<code>customer_3</code>: Something about pices too(150 words)</p>
<p>If we just follow the aproach of Word count, the results are unbalanced towards the person who writes the most. how can this be avoided ?</p>
<p>In ML, in order to avoid that some attributes have more weight than others, they are normalised, how would this be in NLP ?</p>
","nlp"
"113807","What method/algorithm to use to extract information from project documents about objectives, activities, and other variables?","2022-08-24 11:37:09","","0","26","<machine-learning><nlp><information-retrieval><information-extraction>","<p>I'm more-or-less new to NLP so assume little existing knowledge! But I have strong coding skills in R and to a lesser extent Python.</p>
<p>We're interested in extracting key information about objectives, activities, and risks from the project documents of a few thousand education projects carried out by the World Bank. The documents are fairly structured with headings and then tables for all of the variables which we're interested in, but they go back to the 1990s and the actual format and appearance of the documents has changed a lot over time. We need the output for each variable to be fairly general (so objectives would only have a few possible categories, like &quot;access&quot;, &quot;learning&quot;, and so on).</p>
<p>Roughly what methods should we be using? Our original plan was to handlabel a subset of them, and use a supervised learning approach (perhaps text classification?) to automatically label the rest. But upon reading more this doesn't seem too well-suited. Thanks in advance!</p>
","nlp"
"113723","Model for detecting contact information in text","2022-08-21 13:12:39","","0","275","<nlp><text-classification>","<p>Is there a SOTA solution for finding texts with contact information (phone numbers, social media links, etc.)?</p>
<p>I know that this task is advised to solve by regular expressions, I've already tried it myself, but there are problems with numbers of this type:
&quot;8 994 966 twelve 72&quot;, &quot;39<em>30</em>03*&quot;, &quot;912 five69 O7 OO&quot;, &quot;8918ob801ra70sha10s&quot; etc. If you write regular expressions under all of them, they already start to affect regular numbers, which don't need to be deleted. I tried to use NER models, but they don't see phone numbers specifically.</p>
<p>I am not asking to write a universal regular expression, I am specifically interested in a solution using machine learning techniques (I tried NER for example, but it did not work). I also put off using BERT and similar SOTA models, because there are no good embeddings for them and my computational power and time are limited :(.</p>
<p>Thank you in advance.</p>
","nlp"
"113717","Can you make Q&A language model stay on topic?","2022-08-21 07:32:58","","2","31","<nlp>","<p>I’m thinking of fine-tuning a pre-trained language model for a Q&amp;A task. More specifically, I’d like to fine-tune the model on a single chapter in a classic college textbook. Afterward, the reader of the chapter should be able to engage in a Q&amp;A session with the model about the content of the chapter.</p>
<p>But how do I make sure that the model stays on topic and doesn’t go out of a tangent? I know it is possible when looking at what <a href=""https://play.aidungeon.io/"" rel=""nofollow noreferrer"">https://play.aidungeon.io/</a> has achieved, but I don’t know if it will require me to build a model from the ground for each chapter. Can anyone tell me if I’m out of my mind or if it’s feasible?</p>
<p>Edit:</p>
<p>I've just learned that AI Dungeon's models has also struggled with a tendency to create graphic and sexual content despite not being prompted by players.</p>
","nlp"
"113716","Questions on GLM: General Language Model Pretraining with Autoregressive Blank Infilling","2022-08-21 04:48:28","","0","76","<nlp><autoencoder>","<p>For <a href=""https://aclanthology.org/2022.acl-long.26.pdf#page=3"" rel=""nofollow noreferrer"">GLM: General Language Model Pretraining with Autoregressive Blank Infilling</a> ,</p>
<ol>
<li><p>May I ask how is the sampling for input division in step (b) being done ?</p>
</li>
<li><p>why in step (c), the green <code>x3</code> is moved to the end ? why is the maximum value in <code>Position 1</code> limited to 5 instead of 6 ?</p>
</li>
<li><p>why Part A tokens cannot attend to Part B tokens ? but Part B tokens can attend to A ?</p>
</li>
</ol>
<p><a href=""https://i.sstatic.net/9FIoC.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/9FIoC.png"" alt=""GLM"" /></a></p>
","nlp"
"113709","Discover context in word alignement","2022-08-20 23:41:35","","0","11","<nlp><word-embeddings>","<p>I am using <a href=""https://github.com/facebookresearch/MUSE"" rel=""nofollow noreferrer"">Facebook Muse</a> to <a href=""https://github.com/facebookresearch/MUSE/blob/main/demo.ipynb"" rel=""nofollow noreferrer"">translate</a> words from one language to another, and apparently it performs well (I set no metric though). Although I have very basic ML/Datascience/NLP knowledge, could you please guide me to translating not only one word but two and maybe three words. I am not looking for very accurate translations like with tense and prepositions and so on. I am looking for approximate translation even say up to 70% is very fine for my case.</p>
<p>In other words, is it possible somehow to translate for instance: <code>bat</code> correctly in <code>baseball bat</code> knowing it is so different from <code>bat</code> in <code>bat wings</code>, in this situation, probably the context could help but I'm not sure how or even if this is possible using these alignment models ?</p>
","nlp"
"113628","Comparing encoders to same input of differnt output size","2022-08-17 22:38:38","113637","0","73","<machine-learning><nlp><encoding><vector-space-models><encoder>","<p>Let's say I have an input <code>s1</code> and I pass it to two encoders <code>e1</code> and <code>e2</code>. They output encodings of size <code>s1</code> and <code>s2</code>, where their length are not equal, lets say <code>len(s1) = k*len(s2)</code>. Is it possible to somehow compare which encoder is better out of the two ?</p>
","nlp"
"113618","Is it normal for a model to perform worse with the use of word embeddings?","2022-08-17 16:45:25","113663","0","375","<machine-learning><nlp><word-embeddings><fasttext>","<p>I have a multiclass text classification problem and I've tried different solutions and models, but I was not satisfied with the results.
So I've decided to use GloVe ( Global Vectors for Word Representation ) , but somehow all the models performed even worse.
So my question is, is it possible that NLP models perform even worse by the use of some word embeddings models like GloVe or FastText? Or did I just made a bad implementation?
The code is given below:</p>
<pre><code>embedding_model = {}
f = open(r'../../langauge_detection/glove.840B.300d.txt', &quot;r&quot;, encoding=&quot;utf8&quot;)
for line in f:
    values = line.split()
    word = ''.join(values[:-300])
    coefs = np.asarray(values[-300:], dtype='float32')
    embedding_model[word] = coefs
f.close()

def sent2vec(s):
    words = str(s).lower()
    words = word_tokenize(words)
    words = [w for w in words if not w in stop_words]
    words = [w for w in words if w.isalpha()]
    M = []
    for w in words:
        try:
            M.append(embedding_model[w])
        except:
            continue
    M = np.array(M)
    v = M.sum(axis=0)
    if type(v) != np.ndarray:
        return np.zeros(300)
    return v / np.sqrt((v ** 2).sum())

X_train, X_test, y_train, y_test = train_test_split(df.website_text, df.industry, test_size=0.2, random_state=42)

x_train_glove = [sent2vec(x) for x in tqdm(X_train)]
x_test_glove = [sent2vec(x) for x in tqdm(X_test)]

x_train_glove = np.array(x_train_glove)
x_test_glove = np.array(x_test_glove)

from sklearn.linear_model import SGDClassifier
sgd = SGDClassifier(random_state=42)
sgd.fit(x_train_glove, y_train)
</code></pre>
","nlp"
"113592","How to use Fuzzy Topic Model as a Classification Model Input","2022-08-17 07:28:10","","0","43","<classification><nlp><topic-model><fuzzy-classification>","<p>I have fuzzy clustering for Topic modelling and got this
<a href=""https://i.sstatic.net/kZZvM.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/kZZvM.png"" alt=""enter image description here"" /></a>.<br />
There are all total 50 topics[0 to 49] and each topic consists 30 words with a probability multiplicative factor. Now how do I make it as a Classifier input. My final goal to document classification.</p>
<p><strong>Demo</strong></p>
<pre><code>pip install octis
pip install FuzzyTM
from octis.dataset.dataset import Dataset
dataset = Dataset()
dataset.fetch_dataset('DBLP')
data = dataset._Dataset__corpus
print(data[0:5])
pwgt, ptgd = flsaW1.get_matrices()
topics = flsaW1.show_topics()
topics
</code></pre>
","nlp"
"113585","Is there a tokenizer to tokenize Swift language code in python","2022-08-16 20:09:29","","1","55","<python><nlp><data-cleaning><tfidf><tokenization>","<pre><code>import SwiftUI

struct ContentView: View {
    
    @State var moveOnPath = false
    
    var body: some View {
        
        ZStack {
            Circle()
                .stroke()
                .frame(width: 100, height: 100, alignment: .center)
            
            Circle()
                .frame(width: 15, height: 15, alignment: .center)
                .foregroundColor(.blue)
                .offset(x: -50)
                .rotationEffect(.degrees(moveOnPath ? 0 : 360))
                .animation(Animation.linear(duration: 4).repeatCount(10, autoreverses: false))
                .onAppear() {
                    moveOnPath.toggle()
                }
               
        }
        .rotation3DEffect(
            .degrees(70),
            axis: (x: 10, y: 0.5, z: 0.0))
    }
}

struct ContentView_Previews: PreviewProvider {
    static var previews: some View {
        ContentView()
    }
}
</code></pre>
<p>Like for example I have code above available and I would like to tokenize so that only the relevant tokens are extracted such as animation, Animation.linear, duration etc.</p>
<p>Currently I am using:</p>
<pre><code>vectorizer = TfidfVectorizer()
vectors = vectorizer.fit_transform(document)
feature_names = vectorizer.get_feature_names()
dense = vectors.todense()
denselist = dense.tolist()
</code></pre>
<p>It only usese the english language.</p>
","nlp"
"113530","how Can we add extra word embedding to the pytorch funnel transformer?","2022-08-15 10:30:55","","2","135","<nlp><pytorch><word-embeddings><transformer><text-classification>","<p>i was approaching NLP sequence classification problem (3 classes) using huggingface transformers (funnel-transformer/large)  and tensorflow.</p>
<p>first i created laserembedding like this :</p>
<pre><code>from laserembeddings import Laser
laser = Laser()
df = pd.read_csv(&quot;mycsv.csv&quot;)
embeds = laser.embed_sentences(df['text'].values, lang='en')
write_pickle_to_file('train.pkl', embeds )
</code></pre>
<h1>part 1  : Tensorflow version</h1>
<p>for data preparation i use code like below :</p>
<pre><code>
df['text']=temp['column1']+tokenizer.sep_token+temp['column2']+tokenizer.sep_token+temp['column3']

def encode_text(texts):
    enc_di = tokenizer.batch_encode_plus(
        texts, 
        padding='max_length',
        truncation=True,
        return_token_type_ids=True,
        pad_to_max_length=True,
        max_length=cfg.max_len
    )
    
    return [np.asarray(enc_di['input_ids'], dtype=np.int64), 
            np.asarray(enc_di['attention_mask'], dtype=np.int64), 
            np.asarray(enc_di['token_type_ids'], dtype=np.int64)]
</code></pre>
<p>then inside training function :</p>
<pre><code>
x_train = encode_text(df.text.to_list())
train_ds = (
      tf.data.Dataset
      .from_tensor_slices((
          {
              &quot;input_ids&quot;:      x_train[0], 
              &quot;input_masks&quot;:    x_train[1],
              &quot;input_segments&quot;: x_train[2], 
              &quot;lasers&quot;:         np.array( train[laser_columns].values, dtype=np.float32 ) #laser_columns contains all the laser embedded columns
          }, 
       
          tf.one_hot(df[&quot;label&quot;].to_list(), 3) #3 class
      ))
      .repeat()
      .shuffle(2048)
      .batch(cfg.batch_size)
      .prefetch(AUTO)
  )
</code></pre>
<p>i add laser embedding in my model like this :</p>
<pre><code>
def create_model():
    transformer = transformers.TFAutoModel.from_pretrained(cfg.pretrained,config=config,from_pt=True) 
    max_len=512
    # transformer
    input_ids      = Input(shape=(max_len,), dtype=&quot;int32&quot;, name=&quot;input_ids&quot;)
    input_masks    = Input(shape=(max_len,), dtype=&quot;int32&quot;, name=&quot;input_masks&quot;)
    input_segments = Input(shape=(max_len,), dtype=&quot;int32&quot;, name=&quot;input_segments&quot;)
    
    sequence_output = transformer(input_ids, attention_mask=input_masks, token_type_ids=input_segments)[0]

    cls_token = sequence_output[:, 0, :]
    
    # lasers
    lasers = Input(shape=(n_lasers,), dtype=tf.float32, name=&quot;lasers&quot;)  #n_lasers = 1024
    lasers_output = tf.keras.layers.Dense(n_lasers, activation='tanh')(lasers)

    x = tf.keras.layers.Concatenate()([cls_token, lasers_output])

    x = tf.keras.layers.Dropout(0.1)(x)
    x = tf.keras.layers.Dense(2048, activation='tanh')(x)
    x = tf.keras.layers.Dropout(0.1)(x)
    out = tf.keras.layers.Dense(3, activation='softmax')(x)
    
    model = Model(inputs=[input_ids, input_masks, input_segments, lasers], outputs=out)
    model.compile(Adam(lr=1e-5), loss=losses.CategoricalCrossentropy(), metrics=[&quot;acc&quot;, metrics.CategoricalCrossentropy(name='xentropy')])
    
    return model
</code></pre>
<p>now my question is, how do we do the same with pytorch for exact same problem and same dataset?</p>
<h1>part 2  : pytorch version</h1>
<pre><code>
df = pd.read_csv(&quot;mytrain.csv&quot;)
class myDataset(Dataset):
    def __init__(self,df, max_length, tokenizer, training=True):
        self.df = df
        self.max_len = max_length
        self.tokenizer = tokenizer
        self.column1 = self.df['column1'].values
        self.column2 = self.df['column2'].values
        self.column3= self.df['column3'].values
        self.column4= self.df['column4'].values
        self.training = training
        
        if self.training:
            self.targets = self.df['label'].values
    
    def __len__(self):
        return len(self.df)
    
    def __getitem__(self, index):
        column1 = self.column1[index]
        column2= self.column2[index]
        column3= self.column3[index]
        text0 = self.column4[index]
        text1 = column1  + ' ' + column2+ ' ' + column3

        
        inputs = self.tokenizer.encode_plus(
            text1 , 
            text0 ,
            truncation = True,
            add_special_tokens = True,
            return_token_type_ids = True,
            is_split_into_words=False,
            max_length = self.max_len
        )
        
        samples = {
            'input_ids': inputs['input_ids'],
            'attention_mask': inputs['attention_mask'],
        }
        
        if 'token_type_ids' in inputs:
            samples['token_type_ids'] = inputs['token_type_ids']
          
        if self.training:
            samples['target'] = self.targets[index]
        
        return samples
collate_fn = DataCollatorWithPadding(tokenizer=CONFIG['tokenizer'])

class myModel(nn.Module):
    def __init__(self, model_name):
        super(myModel, self).__init__()
        self.model = AutoModel.from_pretrained(model_name)
        if(True):
            print(&quot;using gradient_checkpoint...&quot;)
            self.model.gradient_checkpointing_enable()
        self.config = AutoConfig.from_pretrained(model_name)
       
        self.config.update(
            {
                &quot;output_hidden_states&quot;: True,
                &quot;hidden_dropout_prob&quot;: 0.0,
                &quot;layer_norm_eps&quot;: 1e-7,
                &quot;add_pooling_layer&quot;: False,
                &quot;attention_probs_dropout_prob&quot;:0.0,
            }
        )
        
        self.fc = nn.Linear(self.config.hidden_size, 3)
        
    def forward(self, ids, mask):        
        out = self.model(input_ids=ids,attention_mask=mask,output_hidden_states=False)
        out = out[0][:, 0, :]
        outputs = self.fc(out)
        return outputs
</code></pre>
<p>and in train and validation loop i have code like this :</p>
<pre><code>
bar = tqdm(enumerate(dataloader), total=len(dataloader))
for step, data in bar:
        ids = data['input_ids'].to(device, dtype = torch.long)
        mask = data['attention_mask'].to(device, dtype = torch.long)
        targets = data['target'].to(device, dtype=torch.long)
        
        batch_size = ids.size(0)
        optimizer.zero_grad()
        # forward pass with `autocast` context manager
        with autocast(enabled=True):
            outputs = model(ids, mask)
            loss = loss_fct(outputs, targets)
</code></pre>
<p>i would like to know where and how in my huggingface pytorch  pipeline i can use the laserembedding that i created earlier and used in tensorflow huggingface model?
i would like to concat laserembeddings with funnel transformer's simple CLS token output and train the transformers model with laser embed as extra feature in pytorch implementation exactly like i did in tensorflow example,do you know how to modify my pytorch code to make it working in pytorch? the tensorflow implementation with laserembedding concatenated above that i have posted here works good,i just wanted to do the same in pytorch implementation,,your help is highly appreciated,thanks in advance</p>
","nlp"
"113489","Input length of Sentence BERT","2022-08-13 19:51:57","113498","0","376","<nlp><bert>","<p>Can Sentence Bert embed an entire paragraph instead of only a sentence? For example, a description of a movie.</p>
<p>If so, what if the word counts exceeded the input limit? I remember Bert can only take up to 512 tokens as inputs?</p>
","nlp"
"113374","**tokens when tokens is a dictionary","2022-08-10 08:03:41","","0","71","<python><nlp><bert>","<p>Trying to understand the code from <a href=""https://www.analyticsvidhya.com/blog/2021/05/measuring-text-similarity-using-bert/"" rel=""nofollow noreferrer"">https://www.analyticsvidhya.com/blog/2021/05/measuring-text-similarity-using-bert/</a></p>
<p>I am looking at understanding the syntax on these two lines:</p>
<pre><code>token['input_ids'] = torch.stack(token['input_ids'])
token['attention_mask'] = torch.stack(token['attention_mask'])
</code></pre>
<pre><code>output = model(**token)
output.keys()
</code></pre>
<p>What does <code>**tokens</code> do? I can't seem to print it, or debug its value. I get a <code>Syntax Error</code> exception</p>
<p>I am familiar with the role of <code>**arg</code> in function calls, where it changes an expression like (a=1, b=2) to a dictionary like <code>{'a':1, 'b':2}</code>, but what does it do when the expression is already a dictionary? Or am I misunderstanding something here?</p>
","nlp"
"113359","why there is no preprocessing step for training BERT?","2022-08-09 12:33:30","113366","3","2470","<deep-learning><nlp><bert>","<p>I would like to train a BERT model from scratch. I read the paper as well as a few online material. It seems there is no preprocessing involved. e.g. removing punctuation, stopwords ...</p>
<p>I wonder why is it like that and would that improve them model if I do so ?</p>
","nlp"
"113343","Is there any sentiment analysis algorithm to identify sentiment of a sentence towards a certain word in the sentence?","2022-08-09 02:21:16","113344","0","69","<nlp><sentiment-analysis>","<p>I'll start with some examples. Think about a sentence like &quot;Mazda CX5 is a good car.&quot;. NLTK sentiment analysis module &quot;Vader&quot; will give a positive polarity score on the sentence. Meanwhile a positive score will also be assigned to a sentence like &quot;Mazda CX5 is a better car compared to Subaru Forester.&quot; However, the sentence in fact has a negative sentiment towards Subaru Forester. I wonder if there is any algorithm can actually identify such sentiment difference between the general sentiment and sentiment against a certain word in the sentence.</p>
","nlp"
"113309","Creating class labels for custom DataSets efficiently (HuggingFace)","2022-08-07 19:33:44","113315","0","1409","<nlp><transformer><huggingface>","<p>I have pandas dataframes - test &amp; train,they both have <code>text</code> and <code>label</code> as columns as shown below -</p>
<pre><code> label       text
 fear        ignition problems will appear 
 joy         enjoying the ride

</code></pre>
<p>As usual, to run any Transformers model from the HuggingFace, I am converting these dataframes into <code>Dataset</code> class, and creating the classLabels (fear=0, joy=1) like this -</p>
<pre><code>from datasets import DatasetDict 

traindts = Dataset.from_pandas(traindf)
traindts = traindts.class_encode_column(&quot;label&quot;)

testdts = Dataset.from_pandas(testdf)
testdts = testdts.class_encode_column(&quot;label&quot;)
</code></pre>
<p>Finally these <code>Datasets</code> are put into <code>DatasetDict</code>like this-</p>
<pre><code>emotions = DatasetDict({
    &quot;train&quot; : traindts , 
    &quot;test&quot; : testdts 
})

</code></pre>
<p>Everything works well but as you see that the way I am doing it can be definitely improved. How can it be done more efficiently in less number of lines ?</p>
","nlp"
"113268","Using relative or absolute frequencies to estimate group differences in texts","2022-08-05 10:36:50","","0","32","<nlp><text-mining><text>","<p>My objective is to estimate differences between how five political parties use moral words in their tweets and speeches. To that end, I have a dictionary that I pass to each tweet text / audio transcription through regex (this is important because audio transcriptions are somewhat more noisy and I cannot use bag of words) and get the frequencies with which each moral value is mentioned. Afterwards, I will use Tukey HSD intervals to estimate differences between parties. Nevertheless, my biggest concern here is whether I should compare absolute or relative frequencies of words. Relative frequencies seem like the right choice, because they allow to know how much is each moral value being used controlling for text / audio length. But on the other hand, absolute differences are interesting (especially for the particular case of audios, which can be noisy and not fully capture the total length of the texts and audios according to whitespaces). Is there any guideline to follow here?</p>
","nlp"
"113259","Next-word Generation in Tabular Dataset","2022-08-05 03:53:34","","1","33","<nlp><tensorflow><text><text-generation>","<p>I'll build next-word generation using Tensorflow to predict address mapping. But, I saw many tutorial, next-word generation use long-text narration for its training dataset. But, I have dataset structure like this below (in dataframe). I'm confusing to slicing it into the predictors and label.
this data below assumed as a dataframe:</p>
<pre><code>input                                        output/label
250 Hartford Avenue, Bellingham, MA, 2019    {'address': 'Hartford Avenue', 'city': 'Bellingham', 'state': 'MA','zip': '2019'}  
700 Oak Street, Brockton, MA, 2301           {'address': 'Oak Street', 'city': 'Brockton', 'state': 'MA', 'zip': '2301'}
</code></pre>
<p>if the dataset is narration, next-word generation usually has slicing data like this:</p>
<pre><code>Hartford Aven u
artford Avenu e
</code></pre>
<p>But, if I have my own dataset that has to slicing the label from different column (the dataframe), how it must be?</p>
","nlp"
"113204","Procedure or term for analyzing transcribed text and returning bulleted output","2022-08-03 06:44:33","","0","31","<machine-learning><nlp><text-mining><sentiment-analysis><speech-to-text>","<p>I am attempting to analyze transcribed text from an audio file to group bullet points based on known key phrases in the text.</p>
<p>Example: I have verbally stated the following keywords in the text, which have explicit meaning and are available in the transcript.</p>
<ol>
<li>Title, followed by a sentence of which I’d like to label this transcript</li>
<li>Action, followed by a sentence describing the action I am currently taking</li>
<li>Step, followed by multiple sentences of the steps I am taking</li>
<li>Result, signaling the end of my steps for the first stated action</li>
<li>Final Outcome, closes the loop of actions of the steps within and ultimately wrapping up the title.</li>
</ol>
<p>I have been careful to only use these keywords during the audio recording to properly bookmark the id of the appropriate section to break apart.</p>
<p>Given this example what I would like to accomplish is an extracted text that starts with the title and has a bullet for each action followed by nested bullets for each of the steps mentioned between the first and second action.</p>
<p>When I use the term final result.  This should end the procedure.</p>
<p>I am not sure how to refer to this form of NLP or if this is even a common ML practice.</p>
<hr />
<p><strong>Practical Example</strong></p>
<p><em>Input Text (transcribed audio file)</em></p>
<pre><code>Title debugging some random issue.
Action Investigate the backend server.

I'm now saying random things as fillers and an example of something that should not be captured.

Step I'm now proceeding to analyze the entrypoint for the backend server.

Saying more random things as I'm still within the context of the backend server.

Step I have now tested the entrypoint by using postman to manually trigger api calls.

Result this does not appear to be the problem.

Action Analyzing the front-end client library

Step Review the compiler for errors

Saying more random things as fillers with no context.

Result the compiler appears to be the issue due to a runtime error.

Final Outcome The compiler contained an error resulting in a runtime error. By resolving this matter the parent error is now resolved.
</code></pre>
<p><em>Desired Information Extraction Results</em></p>
<p>Title: Debugging Some Random Issue</p>
<p>Actions:</p>
<ol>
<li>Investigate the backend server
<ol>
<li>I'm now proceeding to analyze the entrypoint for the backend server</li>
<li>I have now tested the entrypoint by using postman to manually trigger api calls</li>
<li>This does not appear to be the problem</li>
</ol>
</li>
<li>Analyzing the front-end client library
<ol>
<li>Review the compiler for errors</li>
<li>The compiler appears to be the issue due to a runtime error.</li>
</ol>
</li>
</ol>
<p>Result: The compiler appears to be the issue due to a runtime error.
Final Outcome The compiler contained an error resulting in a runtime error.</p>
","nlp"
"113183","What Preprocessing is Needed for Semantic Search Using Pre-trained Hugging Face Transformers?","2022-08-02 12:08:18","113196","2","447","<nlp><dataset><preprocessing><transformer><huggingface>","<p>I am building a project for my bachelor thesis and am wondering how to prepare my raw data. The goal is to program some kind of semantic search for job postings. My data set consists of stored web pages in HTML format, each containing the detail page of a job posting. Via an interface I want to fill in predefined fields like skills, highest qualification, etc. with comma-separated sentences or words. These are then embedded via a Hugging Face Transformer and afterwards the similarity of the input is to be compared with the already embedded job postings and the &quot;best match&quot; is returned.</p>
<p>I have already found that intensive preprocessing such as stop word removal and lemmatization is not necessarily required for transformers. However, the data should be processed to resemble the data on which the pre-trained transformers learned. <strong>What would be the best way to prepare such a data set to fine-tune pre-trained Hugging Face Transformers?</strong></p>
<p>Additional info: 55,000 of the saved web pages contain an annotation scheme via which I could simply extract the respective sections &quot;Skills&quot; etc. from the HTML text. If that is not sufficient, I can use prodigy to further annotate the data, e.g. by span labeling texts within the text of the job postings.</p>
<p>Thank you very much in advance!</p>
","nlp"
"113181","NLP model for assessing the probability of token given n previous tokens","2022-08-02 11:23:25","","0","706","<nlp>","<p>I am looking for a model with which I can predict the probability of a current word given its n predecessors (or successors) in a sentence.</p>
<p>Please note: <em>I do not want to generate text nor do I want to predict the next word, but rather I want to know if a given already existing word/sentence makes sense or not.</em></p>
<p>For this I am looking for a solution that solves the following two problems:</p>
<p><strong>First:</strong>
For example: Given the sentences &quot;I build a house&quot; and &quot;I build a soup&quot;.</p>
<p>I want to have a model that tells me <code>P( &quot;house&quot; | &quot;I build a&quot;)</code> and <code>P( &quot;soup&quot; | &quot;I build a&quot;)</code></p>
<p>So in this illustrative example I would like to get something like 30% for &quot;house&quot; and say 0.1% for &quot;soup&quot;.</p>
<p><strong>Second:</strong>
This is related to the first requirement but now I want to ask what is the probability of any word in a sentence given the other words.</p>
<p>For example given again the sentence &quot;I build a house&quot; what is <code>P(&quot;build&quot; | &quot;I&quot;, &quot;a house&quot;)</code>. In this case I would expect the model to tell me that this word is reasonable within this context. While <code>P(&quot;build&quot; | &quot;I&quot;, &quot;a soup&quot;)</code> should be evaluated as unreasonable.</p>
<p>Which model would be recommendable for solving this problem? Ideally a pretrained one that is available for download.</p>
","nlp"
"113177","Do I need training data in multiple languages for a multilingual transformer?","2022-08-02 09:36:49","","0","55","<machine-learning><nlp><transformer><language-model><huggingface>","<p>I am attempting to train a transformer which can categorize sentences into one of n categories. This model should be able to work with a number of different languages - English and Arabic in my case.</p>
<p>Do I need to have labelled training data in both English and Arabic to fine tune a pretrained transformer, such as <a href=""https://huggingface.co/docs/transformers/model_doc/bloom"" rel=""nofollow noreferrer"">BLOOM</a>, or can I fine tune the model using only English samples, and then the model should also work well on Arabic samples for my classificaiton task, since the fine tuning only trained the classification head?</p>
<p>My thoughts are that the pretraining of this model should allow it to transform the same input texts in English and Arabic to the same (or similar) embedding, which the classification head would have learned to then predict these embeddings accurately through the fine tuning.</p>
","nlp"
"113147","Suggestions for guided NLP online courses - Beginner 101","2022-08-01 14:43:18","113153","2","540","<machine-learning><deep-learning><nlp><text-mining><text>","<p>I would like to know from the data science community here for suggestions on nlp courses.</p>
<p>I am new to NLP area and would like to take up a course which covers from basic to advanced concepts such as tokenization to embeddings, GPT-3, transformers etc</p>
<p>My aim is to become a Applied NLP expert (and I don't intend to invent any new algos). So, basically am trying to find a course where they can teach us existing algos, recent advancements, variety of use-cases etc in NLP</p>
<p>Is there any courses that you would recommend?</p>
","nlp"
"113138","How does ""A Neural Probabilistic Language Model"" learn good word vectors?","2022-08-01 07:40:42","","1","155","<neural-network><nlp><word-embeddings>","<p>I'm a layman making a foray into NLP and I have a question: The landmark paper <a href=""https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf"" rel=""nofollow noreferrer"">A Neural Probabilistic Language Model (Bengio et al., 2003)</a> makes an attempt at statistical language modelling by (1) learning a <em>distributed word feature vector</em> for every word (i.e. a <em>word embedding</em> in contemporary terminology) and (2) feeding those word vectors into a neural net to predict the successor to an n-gram of words.</p>
<p>The learned word vectors preserve similarity in the sense, that word vectors of words which occur in the same context during training tend to be closer together. This allows to &quot;<em>fight the curse of dimensionality with its own weapons</em>&quot;, as the authors put it poetically, since &quot;<em>Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence</em>&quot;.</p>
<p>Here's what's causing me headache: I fail to understand how the training process of this model produces word vectors which do preserve similarity in the sense above, instead of learning, well, not so good word vectors. I don't see where this extra constraint of &quot;learn word vectors, but preserve similarity&quot; is respected during training. The learning process described looks like a regular backprop without any extra effort put into learning good word vectors.</p>
<p>I've been staring at the paper for hours and I just don't get it. No publications or websites I've come across that discuss this paper mention my conundrum, so I assume it must be something simple that I'm overlooking. May somebody kindly help me out?</p>
","nlp"
"113111","Slow and Fast tokenizer gives different outputs(sentencepiece tokenizer)","2022-07-30 14:12:02","","0","548","<deep-learning><nlp><tokenization>","<p>When i use T5TokenizerFast(Tokenizer of T5 arcitecture), the output is expected as follows:</p>
<pre><code>['▁', '&lt;/s&gt;', '▁Hello', '▁', '&lt;sep&gt;', '&lt;/s&gt;']
</code></pre>
<p>But when i use normal tokenizer, it starts to split special token &quot;/s&gt;&quot; as follows:</p>
<pre><code>['▁&lt;/', 's', '&gt;', '▁Hello', '&lt;sep&gt;', '&lt;/s&gt;']
</code></pre>
<p><strong>And this is print of not fast tokenizer</strong>:</p>
<pre><code>PreTrainedTokenizer(name_or_path='', vocab_size=60000, model_max_len=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '&lt;/s&gt;', 'unk_token': '&lt;unk&gt;', 'pad_token': '&lt;pad&gt;'})
</code></pre>
<p><strong>For fast</strong> :</p>
<pre><code>PreTrainedTokenizerFast(name_or_path='', vocab_size=60000, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '&lt;/s&gt;', 'unk_token': '&lt;unk&gt;', 'pad_token': '&lt;pad&gt;'})
</code></pre>
<p>Code that i am using to produce these outputs:</p>
<pre><code>tokenizer = T5TokenizerFast('new_sp.model', extra_ids=0)
tokenizer.add_tokens(['&lt;sep&gt;'])
print(tokenizer.convert_ids_to_tokens(tokenizer.encode(&quot;&lt;/s&gt; Hello &lt;sep&gt;&quot;)))
</code></pre>
<p>I would appreciate any help. Thanks.</p>
","nlp"
"113091","Usage of Word2Vec","2022-07-29 17:03:09","","2","323","<machine-learning><deep-learning><nlp><lstm><bert>","<p>Sorry for the basic doubt,</p>
<p>I would like to know if I can use my Word2Vec straight for classification without using LSTM. My assumption is it’s not possible because the ordering of the words will not be take into account. Hence it wont perform for classification.</p>
<p>But we use BERT embeddings for classification. But in this case, BERT generates embedding based on context of the sentence. Hence we can use it for classification. Is my understanding right?</p>
<p>BERT achieves what LSTM learns through ordering without sequential processing, It finds an embedding by processing the sentence as a whole. LSTM also tries to represent a sentence using some context but does it through sequential processing. Is my understanding right?</p>
","nlp"
"113070","Transformers vs RNN basic doubt","2022-07-29 09:44:23","113075","0","123","<machine-learning><nlp><lstm><bert><transformer>","<p>I have a basic doubt. Kindly clarify this.</p>
<p>My doubt is, When we are using LSTM's, We pass the words sequentially and get some hidden representations.</p>
<p>Now transformers also does the same thing except non sequentially. But I have seen that the output of BERT based models can be used as word embeddings.</p>
<p>Why can't we use the output of LSTM also as a word embedding? I can find sentence similarity and all with LSTM also ?</p>
<p>For eg : If I have a sentence &quot; is it very hot out there&quot;</p>
<p>Now I will apply word2Vec and get dense representations and pass it to my LSTM model. The output of my LSTM can also be used as word embeddings as we do the same with BERT?</p>
<p>My understanding was that LSTM is used to identify dependencies between words and using that learned weights to perform classification/similar tasks.</p>
","nlp"
"113038","Good starting point for Natural Language Processing thesis","2022-07-28 12:50:54","","1","13","<machine-learning><nlp>","<p>I want to do a masters thesis on Natural Language Processing, where I want to evaluate if given definitions meet certain criterias. Problem is, I'm new to NLP and I don't know where to start. I need a paper with a NLP method appropriate for my problem so that I can build on that paper.</p>
","nlp"
"113026","Clustering unknown product names","2022-07-28 09:29:16","","0","199","<machine-learning><nlp><clustering><word-embeddings>","<p>I have a parser that reads messages that contain product names. I would like to automatically <strong>cluster</strong> product names in clusters where each cluster would be one product and all the ways it can be written in, i.e. 'laptop', 'lptop', 'lptopt', laptop/laptop' etc. The idea is that I can review this weekly and see which products I do not cover yet and add them manually to my parser.</p>
<p>The products I cover usually have two words in it, one describes product group, and another describes type. For example, I can have a string 'car/mercedes' or 'truck/volvo'. It might be better performing to cluster both the product group such as the 'car' and then subcluster 'mercedes' in it.</p>
<p>From what I gather I need to choose a distance metric such as Jaccard/Levenshtein/... and use a clustering algorithm such as Hierarchical Clustering. However I don't know how many products there are in total so I don't know how many clusters.</p>
<p><strong>Note</strong>: the product names I handle are not really English words, so methods that rely on semantic differences won't work here. I need to compare actual strings as sequences.</p>
<p>How do I frame this problem?</p>
","nlp"
"112988","The Right Approach / Method for Address Completion","2022-07-27 07:29:23","","1","47","<deep-learning><nlp><text-processing>","<p>I have data that formatted like this below:</p>
<pre><code>{&quot;input&quot;: &quot;250 Hartford Avenue, Bellingham, MA, 2019&quot;, 
    &quot;output&quot;: &quot;{
    'address': 'Hartford Avenue',
    'city': 'Bellingham',
    'state': 'MA',
    'zip': '2019'   }&quot; },   

{&quot;input&quot;: &quot;700 Oak Street, Brockton, MA, 2301&quot;,  
    &quot;output&quot;: &quot;{
    'address': 'Oak Street',
    'city': 'Brockton',
    'state': 'MA',
    'zip': '2301'   }&quot; },
</code></pre>
<p>This can be handled by regex, but I'm requested to make it can predict if the input just- and given output like this:</p>
<pre><code>&quot;input&quot;: &quot;Hartford Avenue&quot;
</code></pre>
<p>the output should be:</p>
<pre><code>  &quot;output&quot;: &quot;{
    'address': 'Hartford Avenue',
    'city': 'Bellingham',
    'state': 'MA'}
</code></pre>
<p>or</p>
<pre><code>&quot;input&quot;: &quot;Bellingham&quot;
</code></pre>
<p>the output should be:</p>
<pre><code>  &quot;output&quot;: &quot;{
    'city': 'Bellingham',
    'state': 'MA'}
</code></pre>
<p>and the others clueless input, output should gives completed address.</p>
<p>anyone can give me a method in deep learning side for handle this data? is this appropriate to using classify approach? or tagging? or text generation?</p>
","nlp"
"112981","Supervised recommender system design feedback","2022-07-26 22:14:20","","1","25","<nlp><recommender-system><labels><methodology>","<p>I am facing a challenge that I am not quite sure how to solve and would like to hear feedback.</p>
<p>Basically, I have to implement a recomendation system for certain courses to be recommended to users of certain companies website. There are some limitations that have proved challenging, namely that the history of courses taken were only valid for each year, that is, a course which was imparted in 2021 may or may not(most likely not) be valid for the year).</p>
<p>Initially, I had designed a collaborative filtering system which certain similar users were recommended courses that were contextually similar(using NLP) to courses previouly taken by those similar users, and looked to be ok as a solution.</p>
<p><strong>Very simplified example of original design:</strong></p>
<ul>
<li>User1 in 2021 took a course 'Excel for data analytics'</li>
<li>User2 which is similar to User1, found a similar course for year 2022 titled
'Spreadsheet data analytics', so recommend that course.</li>
</ul>
<p>Now, as I was working on this, one of the stakeholders in the project wanted instead for this to be a supervised model, so he could measure any of the metrics associated with such models.</p>
<p>After some research, which took me some time, I found a type of models called Learn To Rank, which seemed in line with what that stakeholder wanted, so I set out trying to fit an XGBRanker into the original design, reusing the contextually similar embeddings(as it makes no sense to me to train on no longer valid data). The problem that arose now is labeling: I have been trying to use the frequency of each similar course taken as the LTR score, however, evidently my first version of the model ended up predicting exactly the frenquency of the similar courses taken by similar users, which beats the purpose of the supervised model altogether, because that frenquency was computed already(as per original collaborative design), and can just be assigned without a supervised model.</p>
<p>I am not sure how to overcome this and make the supervised model fit into the overall system design as requested, except only by having someone manually asign a score instead of computing it.</p>
<p>How would you approach this problem and its complexities?</p>
","nlp"
"112969","Can you detect source language of a translation?","2022-07-26 15:54:22","112971","1","144","<nlp>","<p>Sometimes you read text and you have a strong feeling that it was translated from a certain language.</p>
<p>For example, you read Russian text, see «взять автобус» («take bus» instead of Russian «сесть в автобус» (literally «sit on bus»)), and it becomes obvious that the text was originally written in English and then translated by low-qualified translator.</p>
<p>Provided you have a long text, can you automatically detect if it is translation or is it originally written in this language, and can you detect the source language? Are there any ready solutions?</p>
","nlp"
"112952","Topic modelling or Keyword extraction for a small dataset","2022-07-26 09:53:42","","0","730","<nlp><dataset><text-classification><topic-model><automatic-summarization>","<p>I am working on a project where I have a dataset which contains very less data. These are the comments of people. I have only 130 lines with 10 words per line. My goal is to identify the common topics which are being discussed here. What should be my approach?</p>
<ol>
<li>Topic modelling</li>
<li>Topic classification</li>
<li>Keyword Extraction</li>
<li>Text Summariser</li>
</ol>
","nlp"
"112900","How can I implement classification for this problem?","2022-07-23 12:58:34","112902","0","29","<nlp><clustering><predictive-modeling><prediction>","<p>I have been thinking about the problem of &quot;predicting&quot; damages awarded in legal cases. For specificity, let us be given a dataset of summaries of cases of a certain flavour (say discrimination cases) that have been binned in a fixed number of &quot;bands&quot; by ranges of damages awarded. Then is it possible to train a custom model to be able to read the facts of a case as reported by an aggrieved party and predict which bin it would fall into should the plaintiff win. My first thought is unsupervised text clustering via NLP. Is there something more efficient that can be used here?</p>
","nlp"
"112891","Smaller embedding size causes lower loss","2022-07-23 07:30:15","112892","0","204","<deep-learning><nlp><transformer><tokenization>","<p>When I convert my multilingual transformer model to a single lingual transformer model (got my languages embedding from the multilingual transformer and deleted other embeddings, decreased dimensions of embedding layers), the loss is much less. But I didn't understand why. What can be the reason for that?</p>
","nlp"
"112877","What is the difference between adding words to a tokenizer and training a tokenizer?","2022-07-22 12:38:24","112893","0","71","<deep-learning><nlp><tokenization>","<p>The title says it all. I was researching this question but couldn't find something useful. What is the difference between adding words to a tokenizer and training a tokenizer?</p>
","nlp"
"112826","Limitations of NLP BERT model for sentiment analysis","2022-07-20 13:06:43","112832","0","480","<machine-learning><nlp><bert><text-classification>","<p>I am reading a <a href=""https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3757135"" rel=""nofollow noreferrer"">paper</a>, where the authors assess online public sentiment in China in response tot the government's policies during Covid-19, using a Chinese BERT model. The author's objective is not only to learn whether a given online post is critical or supportive, but also learning to whom each post was directed at (e.g. CCP, local governments, health ministry, etc). To achieve this, the authors further state in pages 8 through 9, that they,&quot;To train the classifer, we randomly sample approximately 5,000
posts from each dataset (10,541 posts in total), stratified by post creation data. This sample is used for a number of analyses, and we refer to it as the Hand-Annotated Sample.&quot;</p>
<p>My question here is what's the value of using human-annotated posts in combination with a BERT sentiment analysis model?</p>
<p>Specifically, my understanding of BERT as a technique is that it eliminates or at least minimizes the need for pre-labelling a sample of text for sentiment analysis purposes, and it's not clear to me why we still need hand-annotated text by humans even when using BERT.</p>
","nlp"
"112816","What is the effect of the tokens?","2022-07-20 01:53:09","112838","0","42","<nlp><tokenization>","<p>What is the effect of the tokens that the model has if model A has 1B tokens and the other model has 12B tokens? Will that have an effect on the performance?</p>
","nlp"
"112806","How to create product category based on product description","2022-07-19 17:43:43","","0","1243","<machine-learning><python><classification><nlp>","<p>I am currently working on a project that needs product range analysis. It's an ecommerce dataset that has 7 columns: InvoiceNUm, StockNum, Description, Quantity, InvoiceDate, UnitPrice and CustomerID. This means I have to create a category for all the products and I can only create category names based on the description column.</p>
<p>Below is a sample list of description and the category that I need to create.</p>
<p><a href=""https://i.sstatic.net/xytx6.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/xytx6.png"" alt=""enter image description here"" /></a></p>
<p>How do I approach the task of creating category for each product description? Like which model suits this problem? Any suggestions highly appreciated. Thanks in advance.</p>
","nlp"
"112791","LinearSVC training time with CountVectorizer and HashingVectorizer","2022-07-19 10:52:58","112794","0","206","<nlp><scikit-learn><text-classification>","<p>I am currently trying to build a text classifier and I am experimenting with different settings. Specifically, I am extracting my features with a <code>CountVectorizer</code> and <code>HashingVectorizer</code>:</p>
<pre><code>from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer

# Using the count vectorizer.
count_vectorizer = CountVectorizer(lowercase=False, ngram_range=(1, 2)) 
X_train_count_vectorizer = count_vectorizer.fit_transform(X_train['text_combined'])
X_dev_count_vectorizer = count_vectorizer.transform(X_dev['text_combined'])

# Using the has vectorizer.
hash_vectorizer = HashingVectorizer(n_features=2**16,lowercase=True, ngram_range=(1, 2))
X_train_hash_vectorizer = hash_vectorizer.fit_transform(X_train['text_combined'])
X_dev_hash_vectorizer = hash_vectorizer.transform(X_dev['text_combined'])
</code></pre>
<p>Then I am using a LinearSVC classifier</p>
<pre><code>from sklearn.svm import LinearSVC

# Testing with CountVectorizer.
clf_count = LinearSVC(random_state=0)
clf_count.fit(X_train_count_vectorizer, y_train)
y_pred = clf_count.predict(X_dev_count_vectorizer)
accuracy_score(y_dev, y_pred)

# Testing with HasingVectorizer.
clf_count = LinearSVC(random_state=0)
clf_count.fit(X_train_hash_vectorizer, y_train)
y_pred = clf_count.predict(X_dev_hash_vectorizer)
accuracy_score(y_dev, y_pred)
</code></pre>
<p>I obtained the following results:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th></th>
<th>Time to train</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>CountVectorizer</td>
<td>59.9 seconds</td>
<td>83.97%</td>
</tr>
<tr>
<td>HashingVectorizer</td>
<td>6.21 seconds</td>
<td>84.92%</td>
</tr>
</tbody>
</table>
</div>
<p>Please note that even when limiting the number of features of the CountVectorizer to 2**18, I still get slower training and inferior reults.</p>
<p>My questions:</p>
<ul>
<li>Why is training with CountVectorizer slower even for a similar number of features?</li>
<li>What could explain the performance gain in terms of training time?</li>
<li>Any intuition on the reasons behind the accuracy gain?</li>
</ul>
<p>For my particular case, I have also trained a <code>TfidfVectorizer</code> and the CountVectorizer worked a bit better. If the HashingVectorizer has such significant advantages in certain cases. I am wondering why the HashingVectorizer usage is not more widely introduced in different NLP tutorials?</p>
","nlp"
"112785","Mixed language OCR","2022-07-19 08:18:14","","2","180","<machine-learning><python><deep-learning><nlp><computer-vision>","<p>I'm solving a table data recognition task</p>
<p>And the huge problem is the recognition of mixed language pictures.</p>
<p>I'm using tesseract for OCR, but it fails to recognize both languages simultaneously.</p>
<p>Here are the examples of output:<a href=""https://i.sstatic.net/O27wi.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/O27wi.jpg"" alt=""an example of input(img_crop_ and output(data_eng and data_rus)"" /></a></p>
<p>If i use 'eng+rus' as lang option it fails to recognize english
So my question is: <strong>Is there a way to recognize both languages simultaneously? So the output would look like: Холодильник KitchenAid KCFMA 60150R</strong></p>
","nlp"
"112732","Removing unique writing style from text","2022-07-17 11:03:16","","1","18","<nlp>","<p>To start off, I'm not a data scientist or machine learning engineer.  I'm mostly interested in finding existing solutions that will fit this usecase.</p>
<p>I'm trying to build a forum that will protect users privacy.  To my knowledge, it would be possible for malicious attackers who gain access to the forum to run replies through a classifier and be able to roughly group the replies to individual users based on writing style.  Should those attackers also have labeled writing samples from people they are trying to pair with the users of the forum, it is possible that they could find matches and thus dox the user.</p>
<p>What I want to do to try to prevent this form of attack is to run all replies through a one way algorithm that removes styled aspects of replies.  I've tried a number of summarizing APIs, but none I have found seem to work well with short replies.</p>
<p>It would be amazing to get some suggestions here from those more familiar with the space.  Let me know if more details are needed.</p>
","nlp"
"112684","Named Entity Recognition using Spacy V3 with imbalance entities","2022-07-15 13:01:05","112691","0","194","<python><deep-learning><nlp><named-entity-recognition><spacy>","<p>Will the spacy V3 model get affected by imbalanced entities?
I have got a dataset annotated in spacy format and if I look into my custom entities the rations are different for different entities. For example, one entity say <strong>'flex'</strong> is more than <strong>2500</strong> but I also have an entity say <strong>'door'</strong> which is just <strong>21</strong>. I trained my spacy model and evaluated using <code>spacy.evaluate(examples)</code>.
I'm getting <strong>f1-score of 0.64</strong>,
<strong>precision of 1.0</strong> and
<strong>recall of 0.47</strong>.
I want to know whether this entity imbalance is affecting model performance?. If yes is there a way to solve this issue?
Any help on this will be greatly appreciated.</p>
","nlp"
"112674","Why are we training Segment Embedding in BERT?","2022-07-15 05:37:02","","0","141","<deep-learning><nlp>","<p>In BERT we have segment embeddings that are used for &quot;Segment Embeddings with shape (1, n, 768) which are vector representations to help BERT distinguish between paired input sequences.&quot;</p>
<p>Yes, but why. There are just 2 sentences, why are we making it so complicated and using 768-sized vector representation for 0 or 1? And we are adding them to the token and positional embeddings.</p>
<p>So it will be like :</p>
<p>Segment Embeddings:
<code>[0.321,0.231,...,0.434,0.312,0.123]</code></p>
<p>Position Embeddings:
<code>[0.123,0.6435,...,0.231,0.121,0.321]</code></p>
<p>Even If we sum those embeddings, summation will be like any embedding. How this summation will make model distinguish between paired input sequences?</p>
","nlp"
"112598","How special tokens in BERT-Transformers work?","2022-07-12 21:18:21","","2","6421","<deep-learning><nlp><feature-selection>","<p>I was trying to understand how tokens work and all I understood is that tokens are the representation of the input in a more meaningful way (data preparation for the &quot;encoder of transformer&quot; or &quot;BERT&quot;).</p>
<p>But when i see use of special tokens like this: <a href=""https://arxiv.org/pdf/2005.01107v1.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/2005.01107v1.pdf</a>,</p>
<p>i realized that you can actually &quot;specify&quot; your purpose while training your data.
For example, in an answer in <a href=""https://stackoverflow.com/questions/71679626/what-is-so-special-about-special-tokens"">StackOverflow</a>, it says :</p>
<p>&quot;Just an example, in extractive conversational question-answering it is not unusual to add the question and answer of the previous dialog-turn to your input to provide some context for your model. Those previous dialog turns are separated with special tokens from the current question. Sometimes people use the separator token of the model or introduce new special tokens. The following is an example with a new special token [Q]&quot;</p>
<pre><code>[CLS] previous question [Q] current question [SEP] text [EOS]
</code></pre>
<p>But it doesnt explain how any NLP model can use and can be trained in these tokens. How is it being training such a way that it understands that &quot; i should be aware of previous question to answer current question&quot; ?</p>
","nlp"
"112540","Error getting prediction explanation using shap_values when using scikit-learn pipeline?","2022-07-09 22:00:03","","0","1654","<python><nlp><random-forest><pipelines><shap>","<p>I am building an NLP model to predict language type (C/C++/C#/Python...) for a given code.
Now I need to provide an explanation for my model prediction. For example the following user_input is written in Java and the model is predicting that, but I need to show the users why it predicts so.</p>
<p>I am using shap_values to achieve this.
For some reason, the following code results in an error (I have added the error at the bottom).
Please advise how can I get shap_values and plots for my model predictions.</p>
<p>Link to data: <a href=""https://sharetext.me/bd68ryvzi0"" rel=""nofollow noreferrer"">https://sharetext.me/bd68ryvzi0</a></p>
<p>Code:</p>
<pre><code>import pandas as pd

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import FunctionTransformer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import Pipeline

# Loading Data:
DATA_PATH = r&quot;sample.csv&quot;

data = pd.read_csv(DATA_PATH, dtype='object')
data = data.convert_dtypes()
data = data.dropna()
data = data.drop_duplicates()

# Train/Test split
X, y = data.content, data.language
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)

# Model params to match:
# 1. Variable and module names, words in a string, keywords: [A-Za-z_]\w*\b
# 2. Operators: [!\#\\\$%\&amp;\*\+:\-\./&lt;=&gt;\?@\\\^_\|\~]+
# 3. Tabs, spaces and Brackets: [ \t\(\),;\{\}\[\]`&quot;']
# with the following regex:
token_pattern = r&quot;&quot;&quot;(\b[A-Za-z_]\w*\b|[!\#\\\$%\&amp;\*\+:\-\./&lt;=&gt;\?@\\\^_\|\~]+|[ \t\(\),;\{\}\[\]`&quot;'])&quot;&quot;&quot;


def preprocess(x):
 &quot;&quot;&quot; Clean up single-character variable names or ones constituted of a sequence of the same character &quot;&quot;&quot;
 return pd.Series(x).replace(r'\b([A-Za-z])\1+\b', '', regex=True)\
 .replace(r'\b[A-Za-z]\b', '', regex=True)


# Pipe steps:
# Define a transformer:
transformer = FunctionTransformer(preprocess)
# Perform TF-IDF vectorization with our token pattern:
vectorizer = TfidfVectorizer(token_pattern=token_pattern, max_features=3000)
# Create Random Forest Classifier:
clf = RandomForestClassifier(n_jobs=4)

pipe_RF = Pipeline([
 ('preprocessing', transformer),
 ('vectorizer', vectorizer),
 ('clf', clf)]
)

# Setting best params (after performing GridSearchCV)
best_params = {
 'clf__criterion': 'gini',
 'clf__max_features': 'sqrt',
 'clf__min_samples_split': 3,
 'clf__n_estimators': 300
}

pipe_RF.set_params(**best_params)

# Fitting
pipe_RF.fit(X_train, y_train)

# Evaluation
print(f'Accuracy: {pipe_RF.score(X_test, y_test)}')



user_input = [&quot;&quot;&quot; public class Fibonacci {

public static void main(String[] args) {

int n = 10;

System.out.println(fib(n));

}

public static int fib(int n) {

if (n &lt;= 1) {

return n;

}

return fib(n - 1) + fib(n - 2);

}

} &quot;&quot;&quot;]


import shap

shap.initjs()
explainer = shap.TreeExplainer(pipe_RF.named_steps['clf'])
observation = pipe_RF[:-1].transform(user_input).toarray()
shap_values = explainer.shap_values(observation)
</code></pre>
<p>Load the data and run it to get the following error:</p>
<blockquote>
<p>ExplainerError: Additivity check failed in TreeExplainer! Please
ensure the data matrix you passed to the explainer is the same shape
that the model was trained on. If your data shape is correct then
please report this on GitHub. Consider retrying with the
feature_perturbation='interventional' option. This check failed
because for one of the samples the sum of the SHAP values was
46609069202029743624438153216.000000, while the model output was 0.004444. If this difference is acceptable you can set check_additivity=False to disable this check.</p>
</blockquote>
","nlp"
"112506","How to link/relate predicted entities in named entity recognition?","2022-07-08 10:15:29","","1","40","<machine-learning><nlp><named-entity-recognition><markov-hidden-model><entity-linking>","<p>I have developed a NER model to detect all address and property price independently in a pdf document which have property address and its prices in natural language. There are lots of variations in how property address and prices are mentioned. It could be in described sentencse or sometimes like and many more</p>
<p><strong>One possibility</strong></p>
<pre><code>address 1
details about address 1
details about address 1
price 1

address 2
details about address 2
details about address 2
price 2
</code></pre>
<p>So the model in a document would predict say 5 different address and 5 different property prices.</p>
<p><a href=""https://i.sstatic.net/4H1m2.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/4H1m2.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.sstatic.net/PAOSx.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/PAOSx.png"" alt=""enter image description here"" /></a></p>
<p><strong>Questions</strong></p>
<ol>
<li>Now how to build model to assign the price to the correct address?</li>
<li>How to encode this link in the training data and learn that?</li>
</ol>
","nlp"
"112492","RNN basic doubt","2022-07-07 18:43:48","112493","0","26","<machine-learning><neural-network><nlp><lstm><rnn>","<p>Suppose if I have 2 sentences:
&quot;My name is Alex&quot;
&quot;Alex is my name&quot;</p>
<p>If I am using a RNN, After processing both the sentences, Will the final output vector be the same?</p>
<p>Because RNN basically shares the weights, And both the sentences have the same number of words,Shouldnt the final output after processing the last word in both sentences be the same ?</p>
<p>I am well aware that when processing each word in RNN, The next word will be based on the current and previous processed words. But what about the full processing of both these sentences with same words. Will they have same final output?</p>
","nlp"
"112485","How to calculate Pointwise Mutual Information (PMI) when working with multiple ngrams","2022-07-07 13:23:43","112497","1","710","<classification><nlp><text>","<p>Pointwise Mutual Information or PMI for short is given as</p>
<p><img src=""https://latex.codecogs.com/svg.image?%5Cfrac%7BP(bigram)%7D%7BP(1st%20Word)%20*%20P(2nd%20Word)%7D"" alt=""Formula1"" /></p>
<p>Which is the same as:</p>
<p><img src=""https://latex.codecogs.com/svg.image?log_%7B2%7D%5Cfrac%7B%5Cfrac%7BBigramOccurrences%7D%7BN%7D%7D%7B%5Cfrac%7B1stWordOccurrences%7D%7BN%7D%20*%20%5Cfrac%7B2ndWordOccurrences%7D%7BN%7D%7D"" alt=""Formula2"" /></p>
<p>Where BigramOccurrences is number of times bigram appears as feature, 1stWordOccurrences is number of times 1st word in bigram appears as feature and 2ndWordOccurrences is number of times 2nd word from the bigram appears as feature. Finally N is given as number of total words.</p>
<p>We can tweak the following formula a bit and get the following:</p>
<p><img src=""https://latex.codecogs.com/svg.image?log_%7B2%7D%5Cfrac%7BBigramOccurrences*%20N%7D%7B1stWordOccurrences%20*%202ndWordOccurrences%7D"" alt=""Formula3"" /></p>
<p>Now the part that confuses me a bit is the N in the formula. From what I understand it should be a total number of feature occurrences, even though it is described as total number of words. So essentially I wouldn't count total number of words in dataset (as that after some preprocessing doesn't seem like it makes sense to me), but rather I should count the total number of times all bigrams that are features have appeared as well as single words, is this correct?</p>
<p>Finally, one other thing that confuses me a bit is when I work with more than bigrams, so for example trigrams are also part of features. I would then, when calculating PMI for a specific bigram, not consider count of trigrams for N in the given formula? Vice-versa when calculating PMI for a single trigram, the N wouldn't account for number of bigrams, is this correct?</p>
<p>If I misunderstood something about formula, please let me know, as the resources I found online don't make it really clear to me.</p>
","nlp"
"112306","Books/Resources about taxonomy learning?","2022-07-01 15:25:17","","1","23","<nlp><books>","<p>Taxonomy learning is the task of hierarchically classifying concepts in an automatic manner from text corpora.</p>
<p>I am looking for useful books about taxonomy learning. If they don't exist, I'm open to other kind of resources (papers, videos, links). So far I have only found the following:</p>
<ul>
<li><a href=""https://en.wikipedia.org/wiki/Automatic_taxonomy_construction"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Automatic_taxonomy_construction</a></li>
<li><a href=""https://enterprise-knowledge.com/natural-language-processing-and-taxonomy-design/"" rel=""nofollow noreferrer"">https://enterprise-knowledge.com/natural-language-processing-and-taxonomy-design/</a></li>
<li><a href=""https://github.com/sebastianruder/NLP-progress/blob/master/english/taxonomy_learning.md"" rel=""nofollow noreferrer"">https://github.com/sebastianruder/NLP-progress/blob/master/english/taxonomy_learning.md</a></li>
</ul>
<p>Any help is greatly appreciated.</p>
","nlp"
"112292","Extract Parameter constraints from Natural Language text","2022-07-01 08:07:23","","0","57","<nlp>","<p>I am working on a project where I want to parse and verify CLI commands based on a documentation. The documentation provides a structured list of parameters for the commands, that I was easily able to extract. However, I am also interested in each parameter's data types and constraints.</p>
<p>Unfortunately, these are only described by short description strings. They might be something like</p>
<ul>
<li>&quot;The value is an integer.&quot;</li>
<li>&quot;The value is a number from 1 to 10.&quot;</li>
<li>&quot;It is either '1', '2', or '3'.&quot;</li>
<li>&quot;A string (spaces supported)&quot;.</li>
</ul>
<p>I would like to find a way to automatically evaluate or at least simplify these descriptions, so that I can use them for the verification procedure. However, since they really don't follow any common pattern, rule-based matching approaches I have tried until now didn't provide reliable results, and as I am still relatively new with NLP, I don't know what other tools could be used to tackle such a problem.</p>
<p>Is there any sort of commonly used approach that may be helpful for me to extract all constraints from these parameter descriptions?</p>
","nlp"
"112262","What is the right Pytorch RNN implementation?","2022-06-30 08:00:36","","0","119","<nlp><rnn><pytorch><data-science-model>","<p>I read about RNN in pytorch:
RNN — <a href=""https://pytorch.org/docs/stable/generated/torch.nn.RNN.html"" rel=""nofollow noreferrer"">PyTorch 1.12 documentation.</a></p>
<p>According to the document the RNN run the following function:</p>
<p><a href=""https://i.sstatic.net/eIa3u.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/eIa3u.png"" alt=""enter image description here"" /></a></p>
<p>I looked on another RNN example (from pytorch tutorial):
<a href=""https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html"" rel=""nofollow noreferrer"">NLP FROM SCRATCH: CLASSIFYING NAMES WITH A CHARACTER-LEVEL RNN</a>.</p>
<p>And they implemented RNN as:</p>
<pre><code>import torch.nn as nn

class RNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(RNN, self).__init__()

        self.hidden_size = hidden_size

        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)
        self.i2o = nn.Linear(input_size + hidden_size, output_size)
        self.softmax = nn.LogSoftmax(dim=1)

    def forward(self, input, hidden):
        combined = torch.cat((input, hidden), 1)
        hidden = self.i2h(combined)
        output = self.i2o(combined)
        output = self.softmax(output)
        return output, hidden

    def initHidden(self):
        return torch.zeros(1, self.hidden_size)

n_hidden = 128
rnn = RNN(n_letters, n_hidden, n_categories)
</code></pre>
<ol>
<li><p>Why the implemented function is different from the equation ?
(The function doesn’t contains softmax and it does contain bias which is not shown in the code)</p>
</li>
<li><p>Why the code dosn’t use tanh as shown in the equation ?</p>
</li>
</ol>
","nlp"
"112249","Is It Fundamentally Correct To The Text Classification Model To Train First Without Pre-Trained Word Vectors And Then With Pre-Trained Word Vectors?","2022-06-29 18:31:05","","1","15","<python><nlp><text-classification><sentiment-analysis><text-processing>","<p>Is this solution fundamentally correct to the text classification (sentiment analysis) model to train it by these three steps:</p>
<ol>
<li>train the model without pre-trained word vectors untill reaches the minimum loss or maximum accuracy or even stopped by a callback.</li>
<li>get the embedding layer weights and substitute the weights of the words which are represented in pre-trained word vectors and lock the embedding layer</li>
<li>train the model again with the embedding layer which includes weights of the known pre-trained word vectors represented in it and unknown pre-trained word vectors based on the previous training.</li>
</ol>
<p>thanks for your companion</p>
","nlp"
"112233","Is This Solution Fundamentally Correct To The Text Classification Model With Pre-Trained Word Vectors?","2022-06-29 12:14:14","","1","16","<python><nlp><text-classification><sentiment-analysis><text-processing>","<p>Is it fundamentally correct to training text classification (sentiment analysis) model with pre-trained word vectors; first with the locked embedding layer, and then train again with locked additional layers and unlocked embedding layer?</p>
<p>as you know there are so many words that not exist in the pre-trained word vectors, so by creating embedding layer weights with zeros elements, the model would miss these words same as out of vocabulary words?</p>
<p>or even how about the other same ways; define callback to lock and unlock layers on the end of each epoch sequentially, or even lock only the dense classifier layers and unlock the embedding layer and  the other ones.</p>
<p>thanks for your companion</p>
","nlp"
"112137","What configuration of output neurons to use for detecting bias","2022-06-26 09:53:13","","0","13","<machine-learning><deep-learning><nlp><softmax>","<p>I am trying to make a deep learning model that detects political bias in media articles for my local community. There are two political parties here and I have a dataset of biased articles from both. I do not have a dataset of unbiased articles.</p>
<p>Can I implement a neural network with 2 output neurons for each party, and if the accuracy of the prediction is under some value (let's say 0.7) predict it as unbiased and otherwise as biased?
Or do I explicitly need to have unbiased articles in my dataset and one neuron would be biased articles and the other unbiased?</p>
","nlp"
"112126","What are some methods to reduce a dataframe so I can pass it as one sample to an SVM?","2022-06-25 14:17:09","112143","1","37","<nlp><svm><reshape>","<p>I need to classify participants in an NLP study into 3 classes, based on multiple sentences spoken by the participant. I performed a feature extraction on <strong>each sentence</strong>, and so I am left with a matrix of length (# of sentences spoken x feature vector length for each sentence) for each participant. So, for me, each sample is represented by a matrix of varying length, since some participants spoke more sentences than others. What are some ways for me to reduce the dimensionality of each matrix, and also standardize the length, so I can perform an SVM with each participant as a sample?</p>
<p>I am also interested in learning about other methods to classify my samples, if SVMs are not the best fit. Thank you.</p>
","nlp"
"112119","Range of values of BERT and other embeddings?","2022-06-25 10:17:27","","3","639","<nlp><word-embeddings>","<p>Are the values in all NLP models' embeddings between the range -1 to 1?</p>
<p>If not, what models use a different range (or decimal points)? And what could be the reason for that shift/change?</p>
","nlp"
"112095","Mapping of an unseen Field/word to an existing description (in the input data), given Field and their respective descriptions as input/training data","2022-06-24 07:51:35","","0","43","<nlp><bert><semantic-similarity>","<p>I am working on a NLP problem.</p>
<p><strong>Problem Statement</strong></p>
<p>Given the input of fields &amp; Labels and the respective descriptions, the goal is to the map a new unseen field to one of the most appropriate definitions.</p>
<p><strong>Sample Dataset</strong></p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;""><strong>Label</strong></th>
<th style=""text-align: left;""><strong>Descriptions</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">Release Date</td>
<td style=""text-align: left;"">Date of formal issuance</td>
</tr>
<tr>
<td style=""text-align: left;"">Language</td>
<td style=""text-align: left;"">The language of the dataset</td>
</tr>
</tbody>
</table>
</div>
<p>So, for any new field, I want to map the field to one of the existing definition</p>
<p><strong>Approach</strong></p>
<p>I used a bert model for embedding combined with cosine similarity to compute the similarity between every new field (test unseen word) with the existing fields and take the definition of the most similar existing field.</p>
<p><strong>Challenges</strong></p>
<p>As my dataset is very small, the results are not great.</p>
<p><strong>Question to the Community</strong></p>
<p>Is it possible to generate a synthetic dataset that simulates the existing dataset, in order to have a bigger dataset which is &quot;contextually similar&quot; to my existing dataset. That might help my results to improve.</p>
<p>Would greatly appreciate any assistence.</p>
<p>Best regards,</p>
<p>nitin</p>
","nlp"
"112081","Using BERT embeddings as input for transformer architecture","2022-06-23 18:53:07","","0","331","<deep-learning><nlp><word-embeddings><bert><transformer>","<p>I will use BERT's embedding weights (as discussed <a href=""https://discuss.huggingface.co/t/how-to-get-embedding-matrix-of-bert-in-hugging-face/10261/4"" rel=""nofollow noreferrer"">here</a>) for embedding in embedding layers of the transformer model. But my question is: don't embeddings of BERT already go through the whole encoding layer and got that matrix? Why shouldn't I just remove-freeze the encoding layer and use BERT embedding vectors as input for the decoding layer? And also I will use BERT embeddings in the input of the decoding layer. Why should I not freeze attention layers in decoder layer too? Because embeddings of output text already have attention information?</p>
","nlp"
"112072","What are the inputs of encoder and decoder layers of transformer architecture?","2022-06-23 14:55:58","","1","497","<nlp><word-embeddings><bert><transformer><tokenization>","<p>In the paper (attention is all you need), it says &quot;embeddings&quot; are the input of the encoding layer. As I know embeddings are the numerical representation of words which is (for example) the output of bert model.</p>
<p>In the other hand, In BERT paper says the input of BERT is tokenized sentence <a href=""https://i.sstatic.net/b9IOH.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/b9IOH.png"" alt=""here"" /></a>.</p>
<p>Since encoder part of the transformer is BERT, In transformer architecture(<a href=""https://i.sstatic.net/fpxwR.png"" rel=""nofollow noreferrer"">this</a>) is the input of the encoder &quot;tokenized sentence&quot; or &quot;embedded sentence&quot;?</p>
<p>In short: what is the input of the encoder and decoder layer in transformers? Please provide an example.</p>
<p>Thanks</p>
","nlp"
"112063","Parameters for training a sentence-similarity model using Bert?","2022-06-23 09:19:20","","0","56","<nlp><bert><finetuning><semantic-similarity>","<p>I have a list of sentences :</p>
<pre><code>sentences = [&quot;Missing Plate&quot;, &quot;Plate not found&quot;]
</code></pre>
<p>I am trying to find the most similar sentences in the list by using Transformers model with <a href=""https://huggingface.co/sentence-transformers/paraphrase-MiniLM-L6-v2"" rel=""nofollow noreferrer"">Huggingface embedding</a>. I am able to find the similar sentences but the model is still not able to identify the difference between :</p>
<pre><code>&quot;Message ID exists&quot;  
&quot;Message ID doesn't exist&quot;
</code></pre>
<p>[Note: I am trying to find the similarity by using the <a href=""https://www.sbert.net/docs/package_reference/util.html#sentence_transformers.util.cos_sim"" rel=""nofollow noreferrer"">Cosine similarity</a> from pytorch]</p>
<p>Can you suggest me ways to hyperparameter tune my model so that the model can weigh in more on the negative words and consider them opposite?</p>
<p>I found the <a href=""https://huggingface.co/docs/transformers/v4.20.1/en/main_classes/trainer#transformers.TrainingArguments"" rel=""nofollow noreferrer"">list of parameters that can be tuned</a> but not sure what the best parameters would be</p>
<p>Thanks!</p>
","nlp"
"111964","Hashtag-based Tweet similarity","2022-06-20 05:48:32","","1","76","<machine-learning><nlp><machine-learning-model><word2vec><twitter>","<p><strong>I have a big dataset consisting of tweets including hashtags and I want to build a hashtag-based similarity engine to get the most similar tweets given a set of hashtags.</strong></p>
<p>In the end I would like to have some kind of &quot;hashtag to vector embedding&quot; model (should work like a language embedding model) which outputs a comparable vector(in a decent length), based on a set of input hashtags. Later I also want to train a classifier based on those vectors.</p>
<p>One idea would be to fit a TF IDF Vectorizer, do some dimension reduction and then take the cosine similarity/jaccard similarity between a query vector and the tweet vectors.</p>
<p>However, there come some problems with this solution and I feel like there are some better solutions for the problem, do you have any other solutions/pretrained model for the problem which you can recommend or I should try?</p>
<ul>
<li>The model <strong>should not</strong> capture the hashtag's semantic meaning but should just capture the statistical relation between the same hashtags (without preprocessing,stemming/lemmatization, grammar,...)</li>
<li>The <strong>weights</strong> of single hashtags are <strong>important</strong> - the more frequent a hashtag is the less impact it should have on the vector</li>
<li>The order of the tokens/hashtags <strong>does not matter</strong></li>
</ul>
","nlp"
"111948","Model to implement Question Answering System over structured data","2022-06-19 13:03:11","","1","435","<machine-learning><nlp><bert><transformer><question-answering>","<p>I need to write a program(like a chatbot) that retrieves an answer from a CSV datafile based on a question user asks. So for example if the CSV stores list of products and its specifications in 5-10 columns, then if a user asks a question about specification Y for product X the program should return the correct answer based on CSV. I need to use NLP as the user can write synonyms of a particular word or ask a question a bit differently from the keywords in the dataset.</p>
<p>I think I am supposed to use BERT model using HuggingFace Transformer, but I'm not sure how to use NLP as this is over structured data. Additionally, I don't have a list of questions generated already.</p>
<p>Does anyone suggest how I should do this.</p>
<p>Also some of the specifications are values like prices. I was wondering if there is a way for the program to return the average or sum of two or more products if the user asks that question.</p>
","nlp"
"111912","Performing a text classification based on a dictionary","2022-06-17 17:18:36","","3","1009","<machine-learning><deep-learning><nlp><data-science-model><text-classification>","<p>I have been given a kind of dictionary which maps a category with a set of certain strings. A sample of the dictionary is given below:</p>
<p><a href=""https://i.sstatic.net/C9o3p.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/C9o3p.png"" alt="""" /></a></p>
<p>This is all I have, there is no other data. There are around 46 categories (DT5 Category) and in the similar way all categories have a few keywords associated with them.</p>
<p>Now I am supposed to create a model which takes any kind of text input most probably a customer review/query and my model should correctly classify it according to the keywords present in it (The keywords can be in other forms like different forms of verb etc.)
The model should state which category does it belong to. (Basically need an NLP based approach)</p>
<p>I am completely unable to understand, what model to use in this, what data should I test or train my model on. Considering this data, it has 46 classes and the data set is so small. Please help me regarding what approach should I use to solve this problem.</p>
","nlp"
"111891","BertTokenizer Loading Problem","2022-06-16 17:53:57","111901","1","2070","<deep-learning><nlp><pytorch><bert>","<p>I loaded this BertTokenizer previously, but now it is showing, I have to make sure I don't have a local directory. In my kaggle kernel, I don't have this local directory.
How to solve it?</p>
<pre><code>class Config:
    DEVICE = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;
    LR = 2e-5
    TRAIN_BATCH_SIZE = 16
    TEST_BATCH_SIZE = 8
    EPOCHS = 10
    N_FOLD = 5
    TOKENIZER = BertTokenizer.from_pretrained('bert-large-uncased', do_lower_case=True)
    CLASSES = 3
    MAX_LEN = 200
    TRAIN_CSV = &quot;.csv&quot;
    TEST_CSV = &quot;test.csv&quot;
    API = &quot;#&quot;
    PROJECT_NAME = &quot;bert-base2&quot;
    MODEL_NAME = &quot;bert-large-uncased&quot;
</code></pre>
<pre><code>OSError: Can't load tokenizer for 'bert-large-uncased'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'bert-large-uncased' is the correct path to a directory containing all relevant files for a BertTokenizer tokenizer.
</code></pre>
","nlp"
"111830","How to deal with spelling errors in NLP classfier (low resource language)","2022-06-14 21:29:27","","1","318","<deep-learning><nlp><text-classification>","<p>I know there are questions on how to deal with spelling error NLP - but the question and solution are mainly focused on English where there are tons of library for spell-correction.</p>
<p>Here I am curious what strategy are recommended to build a classifier which is more robust against spelling mistakes. I wonder if it makes sense to make the classifier with character level tokenizer ? or perhaps, upsampling correct data to be misspeled ?</p>
","nlp"
"111802","Each multi-class output has another level of multi-class output","2022-06-14 02:54:13","","1","20","<nlp><multiclass-classification><multitask-learning>","<p>I want to predict a major class and then minor class.
Major class is a multi-class model which has 4 categories. Each of this categories has 10-15 subcategories, for which I can build a multi-task model with 4 heads. Is there a better architecture?</p>
<p>I am worried that if I build a supervised model to predict the subcategories directly, i.e., a model with ~100 labels, the classes will be confusing and the model will not be able to classify accurately. However, I am hesitant to build models per category as its not scalable and adds operational overhead.</p>
<p>Similar to this - <a href=""https://datascience.stackexchange.com/questions/105076/"">Prediction/Classification within each grup (Multi-Class / Multi-Label)</a></p>
","nlp"
"111747","Manipulating noise to get some data in right format and apply it to task using PPO","2022-06-12 18:21:19","112227","3","200","<machine-learning><python><nlp><reinforcement-learning><transformer>","<p><strong>Warning</strong>:<br>
I understand that my question may seem strange, stupid, and impossible, but let's just think about this interesting problem. I would not ask a question like: how to create an AGI in google colab. This is a real problem and I believe that it is possible to solve it. My question may seem strange, because I have little experience and maybe I have indicated something wrong. But I assure you, this is not complete nonsense. My actual task is much harder then task bellow, therefore to simplify question i have simplified problem<br><br>
<strong>I have RL task</strong>:<br>
My environment is python, agent is usual RL agent(it takes action like others RL agents), but i have no list of actions. Goal is writing the fastest python code for sorting.<br>Policy net(network which returns action) returns me sorting string(something like: &quot;[list1.pop(list1.index(min(list1))) for i in range(len(list1))]&quot;), i execute it through &quot;eval&quot;, get time of execution and use this time to form reward. But this task is easier, in my real task i have some variables and functions which model can use when produces sorting-strings. In our case it can be: &quot;list_1&quot;, &quot;some_function_which_helps_to_sort_list1_faster&quot;.<br><br>
<strong>That's how i'm going to get sorting-strings</strong>:<br>I know for sure i need code model. When i was looking for it i found <a href=""https://github.com/kingoflolz/mesh-transformer-jax/#gpt-j-6b"" rel=""nofollow noreferrer"">GPT-J</a>. GPT-J is usual transformer Decoder only model. First of all i create random initial(it's constant) noise. Policy net also produces noise. At the first time this(noise from policy net) is random noise, but over the time model will be trained better and the noise that policy net will produce will already be meaningful and will help to get normal sorting-strings. I add first initial noise to noise which i got from policy net, pass it through GPT-J and finally get sorting string. I gonna train model with many different initial noises, because logically if initial noises are different, model will: 1) be trained better 2)produce new &quot;the fastest&quot; results.
Entire approach looks like clip guided diffusion and i'm going to train it with PPO. As you remember, i have some variables that have to be in sorting strings. Therefore, there is a question: &quot;How to make policy net to add these variables into sorting strings?&quot;. I believe reward forming will help to solve it.
<br><br>
<strong>How reward will be formed</strong>:<br>
If policy net returns valid sorting string(which is valid python code and contains minimal set of variables i need(at least &quot;list1&quot;) to pass it through eval without errors) but it is more slower than previous best sorting-string, reward will be tiny(0.1). If policy net returns valid sorting string which is faster than previous best sorting string, reward will be huge(1). If policy net returns invalid sorting string(which is not valid python code or doesn't contain minimal set of variables), reward will be negative(-1).<br><br>
<strong>Thats how i'm going to train model. Bellow is how i'm going to use model at the inference time</strong>:<br>
First of all set initial noise. Then make the same like in training loop, but don't save weights(weights will be updated according PPO, all steps, which were in &quot;That's how i gonna get sorting-strings&quot; will be executed, but when i get result from final iteration, i won't save this new weights which i get in inference time and if i need to surpass previous the best result, i will run inference loop with new initial noise till i surpass this result.)<br><br></p>
<p><strong>What does here result from final iteration mean?</strong>:<br>
That's exactly like in clip guided diffusion. I set some variable n_steps. For example it will be equal to 1000. Here i make 1000 calls to policy net, 1000 times update policy weights(if it's training time, at the inference time i also update weights but keep them in RAM memory and don't save)... And when i get final result at 1000th iteration, that means for me result from final iteration.</p>
<p><strong>Question</strong>:<br>
Is my approach of implementing this problem right?
How would you implement my problem?
If you have some helpful tips for me(maybe you have some links which will help me, may be i wrong form reward...; here i meant anything which might be helpful for me), don't hesitate to share it with me.</p>
","nlp"
"111735","Can I use Sentence-Bert to embed event triples?","2022-06-12 07:26:13","111766","1","163","<nlp><bert><information-retrieval><information-extraction>","<p>I extracted event triples from sentences using OpenIE. Can I concatenate the components in the event triple to make it a sentence and use Sentence-Bert to embed it?
It seems no one has done this way before so I am questioning my idea.</p>
<p>I'm using news headlines to predict next day's stock movement. For example, there are two news headlines, the first is <em>&quot;U.S. stock index futures points to higher start&quot;</em>, I used openIE to extract it and there are two event triples, [('U.S. stock index futures', 'points to', 'start'), ('U.S. stock index futures', 'points to', 'higher start')]. (There are repetition in the openIE extracted event triples and I don't know how to avoid it.) Since it contains events I'm interested in (stock index), I will embed these two events and take their mean as the the embedding.</p>
<p>The second headline is <em>&quot;STOCKS NEWS US- Economic and earnings diary for Jan 4&quot;</em>, it contains no events as it is only contain nouns. So I will embed it as 0 vector in this case.</p>
","nlp"
"111716","Is TF-IDF for text classification transferable between corpuses?","2022-06-10 22:13:08","","0","113","<nlp><pca><text-classification><tfidf>","<p>I am using TF-IDF for text classification and my solution works well according to the performance metric of my choice (F1 macro). To speed up the training process I have used PCA to reduce the dimensionality of the document vectors.</p>
<p>I am using this for a growing set of datasets and the datasets keep changing.</p>
<p>Is there a way to reuse the TF-IDF vectorizer and the PCA transformation across different datasets? (for time efficiency)</p>
<p>My initial idea is to share the vocabulary and documents of the datasets to create a universal TF-IDF+PCA transformation, but I am worried if 1) it would harm the performance of classification on individual datasets and 2) new datasets might have terms not present in the universal vocabulary.</p>
<p>Are there existing solutions for reusing TF-IDF/PCA across multiple corpuses? and/or an actively changing corpus?</p>
","nlp"
"111687","DIalect packages in python or R (or a separate tool)","2022-06-09 22:32:38","","1","14","<nlp>","<p>Is anyone familiar with options for identifying properties (vocabulary, spelling, syntax) of American English dialects (specifically African American English, but any other dialects too)?</p>
","nlp"
"111683","LSTM basic doubt","2022-06-09 16:45:20","","0","29","<machine-learning><nlp><lstm><logistic-regression>","<p>How LSTM are able to figure out that a particular word has occurred. Like in classical algos, We have column order. But in LSTM, Since each cell receives different words, How does it know a particular word has occurred ?</p>
<p>When we use tfidf, We know that each column refers to a specific word. But when we use LSTM, How is this possible. Since LSTM cell receives input based on the word observed. How does LSTM know the occurrence of word since there is no index maintained</p>
","nlp"
"111681","NLP logistic regression","2022-06-09 15:53:44","","1","104","<machine-learning><nlp><data><logistic-regression>","<p>A basic doubt I have, Usually when dealing with text data for classic ML algos, We use Tf-idf which uses entire vocabulary for each row and assigns some weighted value.</p>
<p>My doubt is can I use 5 feature columns denoting the first 5 words to occur in the sentence and keep the target variable as some specific class.</p>
<p>Will this method work ? I am aware of RNN and LSTM, But was wondering if this would work ?</p>
","nlp"
"111547","What to do if your adversarial validation show different distributions for an NLP problem?","2022-06-03 22:30:14","","1","23","<machine-learning><nlp><cross-validation><validation>","<p>I was trying to figure out if the test set from a competition is similar to the train set. This was done in a NLP competition, in which I had two columns, tweet and type, and I needed to predict the type of crime the tweet was reporting. So I decided to check if the train set is too different from the test set. This is what I've done so far:</p>
<pre><code># drop the target column from the training data
train_adv = train_set.drop([&quot;type&quot;], axis=1)
test_adv = submission_set.copy()

# add the train/test labels
train_adv[&quot;AV_label&quot;] = 0
test_adv[&quot;AV_label&quot;] = 1

# make one big dataset
all_data = pd.concat([train_adv, test_adv], axis=0, ignore_index=True)

X_adv = all_data[&quot;tweet&quot;]
y_adv = all_data[&quot;AV_label&quot;]

X_train_adv, X_test_adv, y_train_adv, y_test_adv = train_test_split(
    X_adv, y_adv, test_size=0.5, random_state=42, stratify=y_adv
)

lgbm_pipe = Pipeline(
    [
        (
            &quot;vect&quot;,
            CountVectorizer(
                lowercase=True,
                analyzer=&quot;word&quot;,
                #ngram_range=(1, 2),
                strip_accents=&quot;ascii&quot;,
                token_pattern=r&quot;\w+&quot;,
                stop_words=&quot;english&quot;,
            ),
        ),      
        (&quot;feature_selection&quot;, SelectKBest(chi2, k=500)),
        (&quot;poly&quot;,PolynomialFeatures(2, interaction_only=True)),
        (&quot;scaler&quot;, MaxAbsScaler()),
        (&quot;model&quot;, LGBMClassifier(boosting_type=&quot;gbdt&quot;, n_jobs=-1, num_leaves=45)),
    ]
)

lgbm_pipe.fit(X_train_adv, y_train_adv)

y_pred_adv = lgbm_pipe.predict(X_test_adv)

print(f&quot;Accuracy: {accuracy_score(y_test_adv, y_pred_adv)}&quot;)
print(f&quot;F1-Score Weighted: {f1_score(y_test_adv, y_pred_adv)}&quot;)
</code></pre>
<p>All metrics are very high, indicating (from what I've learnt) that the test set is very different from the train set. Is there anything I could actually do with this information? Since there are only two columns, I don't know how can I fix this and avoid overfitting. It's not like I have some features which could explain what's happening. Any idea?</p>
","nlp"
"111515","Using BERT instead of word2vec to extract most similar words to a given word","2022-06-02 19:59:44","","0","1013","<nlp><word2vec><bert>","<p>I am fairly new to BERT, and I am willing to test two approaches to get &quot;the most similar words&quot; to a given word to use in Snorkel labeling functions for weak supervision.</p>
<p>Fist approach was to use word2vec with pre-trained word embedding of &quot;word2vec-google-news-300&quot; to find the most similar words</p>
<pre><code>@labeling_function()
def lf_find_good_synonyms(x):
  good_synonyms = word_vectors.most_similar(&quot;good&quot;, topn=25) ##Similar words are extracted here
  good_list = syn_list(good_synonyms) ##syn_list just returns the stemmed similar word
  return POSITIVE if any(word in x.stemmed for word in good_list) else ABSTAIN
</code></pre>
<p>The function basically looks for the word &quot;good&quot; or any of it's similar words in a sentence (the sentences have been previously stemmed so are the words as the function syn_list returns the stem of each similar word) if found, the function will simply label the sentence as POSITIVE.</p>
<p>The issue here is that my word vectors are based on word2vec, which does not respect context. I was wondering if I could use BERT instead, and since I am looking for similar words to a certain 'word', how do I include context?</p>
<p>Will using BERT improve the performance much as labeling functions are allowed to be lousy?</p>
","nlp"
"111510","How do I test one-shot model preformance against flawed categories?","2022-06-02 18:56:15","","1","12","<nlp><accuracy><one-shot-learning>","<p>I'm in the process of reworking the <a href=""https://msi.nga.mil/Piracy"" rel=""nofollow noreferrer"">ASAM database</a>. Excerpted, it looks like this:</p>
<pre><code>4155    PIRATES     BULK CARRIER    GULF OF ADEN: Bulk carrier fired upon 3 Aug 09 at 1500 UTC while underway in position 13-46.5N 050-42.3E. Ten heavily armed pirates in two boats fired upon the vessel underway. The pirates failed to board the vessel due to evasive action taken by the master. All crew and ship properties are safe (IMB).
4156    PIRATES     CARGO SHIP      NIGERIA: Vessel (SATURNAS) boarded, crewmembers kidnapped 3 Aug 09 while operating in the vicinity of the Escravos River in the western Niger Delta. The Lithuanian-flagged vessel came under attack by an unidentified group in a high-speed boat, according to a statement by the Lithuanian Foreign Ministry. The men kidnapped five of the crewmembers, taking them to an unknown location, but left the remaining nine crewmembers onboard. No crewmembers were reported injured, while the vessel sustained no damage (Bloomberg, LM: Topnews.in).
4157    PIRATES     BULK CARRIER    BONNY RIVER PORT HARCOURT NIGERIA: Heavily armed pirates in two speedboats, seven in each boat approached and opened fired on a bulk carrier at anchor. The vessel immediately heaved anchor and proceeded to open seas for safety reasons. One crew member injured.
4158    PIRATES     CONTAINER SHIP  75NM OFF MIRI, SARAWAK, MALAYSIA: Twelve pirates, in a seven meter long, unlit boat approached a container ship underway. They chased the ship and tried to get alongside. Alarm raised, took evasive maneuvers, alerted crewmembers and master fired rocket flares. Pirates aborted the attept.
4159    PIRATES     BULK CARRIER    BRAZIL: Bulk carrier robbed 27 Jul 09 at 2355 local time while anchored in position 01-05.41S - 048-29.08W, Mosqueiro anchorage. Robbers armed with knives boarded the vessel at anchor. They tied up the watch officer's hands and stole ship's stores before escaping (IMB).
4160    PIRATES     MERCHANT VESSEL SWEDEN: Vessel (ARCTIC SEA) boarded 24 Jul 09 at approximately 0300 local time while underway in Swedish territorial waters. Eight to twelve armed men allegedly wearing masks and uniforms bearing the word &quot;police¿ boarded the vessel using a black rubber boat. While onboard, the men allegedly assaulted and tied up many of the crew members. During this time, crew members were questioned about drug trafficking. The men stayed onboard the vessel for approximately 12 hours, during which time they rummaged through the cabins. The crew members were eventually released and the men departed the vessel. It's unknown if anything was stolen. Swedish police are currently investigating the incident. UPDATE: On 4 Aug 09, the vessel was expected to make a port call in Algeria, but never arrived. Russian authorities are investigating the incident and have begun searching for the vessel. There is no further information to provide at this time (AP, Bloomberg, LM: Times of Malta).
</code></pre>
<p>There are currently nine categories of hostilities (which is sub-optimal but another issue)</p>
<ol>
<li>Attempted Boarding</li>
<li>Blocking</li>
<li>Boarding</li>
<li>Fired Upon</li>
<li>Hijacking</li>
<li>Kidnapping</li>
<li>Hijacking/Kidnapping Combination</li>
<li>Robbery</li>
<li>Suspicious Approach</li>
</ol>
<p>You'll notice piracy isn't a category of hostility; robbery is usually the closest category that applies. However, in the above section, you would have:</p>
<ul>
<li>4155 as fired upon</li>
<li>4156 as kidnapping</li>
<li>4157 as fired upon</li>
<li>4158 would be correct as robbery</li>
<li>4159 as boarding.</li>
</ul>
<p>So, I've turned to running NLP models on the description to correctly categorize the hostilities. One-shot models are <em>very</em> promising and, from a cursory inspection, they seem pretty accurate and perfect for this sort of case. However, I don't know how to test model performance because the current categorization is so deeply flawed. I previously did a K-means which was essentially useless.</p>
<p>What is the best way to quantify the performance of the models? My best guess is to do a subset of the database by hand and check the models against that but this doesn't seem like a good way.</p>
","nlp"
"111393","Spacy custom POS tagging for medical concepts","2022-05-29 10:33:29","111403","5","438","<machine-learning><nlp><spacy>","<p>We are a group of doctors trying to use linguistic features of &quot;Spacy&quot;, especially the part of speech tagging to show relationships between medical concepts like:</p>
<p><em><strong>'Femoral artery pseudoaneurysm</strong></em>  as in ==&gt;</p>
<p><strong>&quot;femoral artery&quot; ['Anatomical Location']  --&gt; and &quot;pseudoaneurysm&quot; ['Pathology']</strong></p>
<p>We are new to NLP and spacy, can someone with experience with NLP and Spacy explain if this is a good approach to show these relationships in medical documents? If not what are the other alternative methods?</p>
<p>Many thanks!</p>
","nlp"
"111380","NLP Making new Predictions on Vectorized set","2022-05-28 17:16:46","","0","18","<nlp>","<p>Vectorization techniques like TF-IDF are very common techniques for transforming text data into numerical data that can be more easily feed to ML Algorithms. Before we first train the model, each word of each document is given a number (a frequency) which depends on the whole data.</p>
<p>How can I input to the already trained model, a new custom, unseen before sentence since the model was trained with the whole dataset vectorized?</p>
<p>For instance, my new sentence has a different number of features compared to the dataset used for training and testing. Moreover, the frequencies, as far as I understand are computed based on all the words in the dataset, so ,from my perspective, I also need to include this dataset in the operation of vectorization of the new custom sentence.</p>
","nlp"
"111322","Which model is better able to understand the difference that two sentences are talking about different things?","2022-05-26 10:11:25","","1","42","<deep-learning><nlp><word-embeddings><transformer><semantic-similarity>","<p>I'm currently working on the task of measuring semantic proximity between sentences. I use fasttext train _unsiupervised (skipgram) for this. I extract the sentence embeddings and then measure the cosine similarity between them. however, I ran into the following problem: cosine similarity between embeddings of these sentences:</p>
<p><code>&quot;Create a documentation of product A&quot;; &quot;he is creating a documentation of product B&quot;</code></p>
<p>is very high (&gt;0.9). obviously it because both of them is about creating a documentation. but however the first sentence is about product A and second is about product B and I would like my model to understand that and emphasise on those product names since they are key words in sentences. Which type of model would be more suitable for my case? Is BERT for example better for it and why?</p>
","nlp"
"111309","Could Attention_mask in T5 be a float in [0,1]?","2022-05-25 22:32:14","","1","193","<deep-learning><nlp><transformer><attention-mechanism><huggingface>","<p>I was inspecting T5 model from hf <a href=""https://huggingface.co/docs/transformers/model_doc/t5"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/model_doc/t5</a> . attention_mask is presented as</p>
<pre><code>attention_mask (torch.FloatTensor of shape (batch_size, sequence_length), optional) — Mask to avoid performing attention on padding token indices. Mask values selected in [0, 1]:
1 for tokens that are not masked,
0 for tokens that are masked. 
</code></pre>
<p>I was wondering whether it could be used something &quot;softer&quot; not only selecting the not-padding token but also selecting &quot;how much&quot; attention should be used on every token.</p>
<p>This question is related to the one proposed here <a href=""https://datascience.stackexchange.com/questions/94517/can-the-attention-mask-hold-values-between-0-and-1/111303#111303"">Can the attention mask hold values between 0 and 1?</a></p>
<p>Do you know if such attention_mask vector is used in any other ways where a non integer value could harm the model?</p>
<p>Thank you for your precious time and advices.</p>
","nlp"
"111278","Deep learning techniques for concept similarity?","2022-05-25 00:18:47","","1","111","<deep-learning><nlp><similar-documents>","<p>Given a corpus of product descriptions (say, vacuum cleaners), I'm looking for a way to group the documents that are all of the same type (where a type can be <code>cordless vacuums</code>, <code>shampooer</code>, <code>carpet cleaner</code>, <code>industrial vacuum</code>, etc.).</p>
<p>The approach I'm exploring is to use NER. I'm labeling a set of these documents with tags such as (<code>KIND, BRAND, MODEL</code>). The theory is that I'd then run new documents through the model, and the tokens corresponding to those tags would be extracted. I would then construct a feature vector for each document comprised of a boolean value for each of the tags. From there, a simple dot product would show all documents related to some base document (as in, these documents are all similar to this one document).</p>
<h1>Question</h1>
<p>What are other general approaches that might be a good fit for this task?</p>
","nlp"
"111260","NLP - support comments analysis","2022-05-24 14:39:39","","0","35","<python><nlp><bert>","<p>I am new to NLP and looking for some direction since after all my reading I haven't found a definite approach and the subject matter is vast. The project is to focus on specific fields of support comments using NLP and Python. The goal is that from the comments I want to verify that the comment is in fact a well made comment for that field. Some requirements is the context of entered text is relevant to the field it has been entered in, it is informative based on the specific field, it is not just a few unhelpful words entered, it can detect that words/sentences have similar semantic meaning (e.g problem is, issue encountered etc).</p>
<p>I first looked into TF-IDF but it doesn't consider context. I have been reading into other deep learning models such as BERT as it includes context. I have read though that BERT was designed more for sentence prediction and missing words rather than for comparing semantic meaning of multiple sentences. USE would be better for comparing if sentences are similar although all the comments will have a general theme and I doubt they will be similar enough to design a system around. Topic modelling sounded like what i'm looking for but I think it's more for general topics you find in newspapers and books rather than comment sections. For the text summarisation option maybe there is not enough text in each comment to give a helpful summary. I think I would prefer unlabelled data if possible so as to minimise manual input but if it's neccessary then we could evaluate it. Is there something in fine tuning a pretrained model such as BERT to analyse all the existing comments for a specific field and somehow extract from that if a new comment is informative and relevant by comparing it to the existing ones?</p>
<p>Example field would be &quot;Problem description&quot;. I am looking for direction on how to classify the content of that field as relevant and informative.</p>
<p>Any direction is appreciated or if you know of existing related material, papers, videos or tutorials.</p>
<p>Thanks</p>
","nlp"
"111231","Pretrain RoBERTa model with new data using PyTorch library","2022-05-23 11:26:22","","0","602","<python><nlp><pytorch><pretraining>","<p>I've pretrained the RoBERTa model with new data using a '<strong>simpletransformers</strong>' library:</p>
<pre><code>from simpletransformers.classification import ClassificationModel

OUTPUT_DIR = 'roberta_output/'
model = ClassificationModel('roberta', 'roberta-base',use_cuda=False, num_labels=22,
                        args={'overwrite_output_dir':True, 'output_dir':OUTPUT_DIR})

model.train_model(train_df)

result, model_outputs, wrong_predictions = model.eval_model(test_df) # model evaluation on test data
</code></pre>
<p>where '<strong>train_df</strong>' is a pandas dataframe that consists of many samples (=rows) with two columns: the 1st column is a text data - input; the 2nd column is a category (=label) - output.</p>
<p>I need to create the same model and pretrain it as above but using '<strong>PyTorch</strong>' library instead of '<strong>Simpletransformers</strong>' library. Is there any way to make it simple as the code above?</p>
<p>I've loaded the pretrained model as it was said <a href=""https://pytorch.org/hub/pytorch_fairseq_roberta/"" rel=""nofollow noreferrer"">here</a>:</p>
<pre><code>import torch
roberta = torch.hub.load('pytorch/fairseq', 'roberta.large', pretrained=True)
roberta.eval()  # disable dropout (or leave in train mode to finetune)
</code></pre>
<p>I also changed the number of labels to predict in the last layer:</p>
<pre><code>roberta.register_classification_head('new_task', num_classes=22)
</code></pre>
<p>But, I can't find how I can pretrain the classifier with my 'train_df'.
The only way I've found so far is from <a href=""https://github.com/facebookresearch/fairseq/blob/main/examples/roberta/README.pretraining.md"" rel=""nofollow noreferrer"">here</a> where we use a PyTorch toolkit '<strong>fairseq</strong>' and fairseq cli to pretrain RoBERTa model. Is this the only option or can it be done more simply?</p>
","nlp"
"111179","Classification of a free text field to determine which product","2022-05-21 08:31:54","","1","125","<classification><nlp>","<p>I have a problem. I have a free text field and I would like to use NLP to determine which product should be selected. This involves concepts such as weight, speed, size and so on. Here is an example:</p>
<pre class=""lang-py prettyprint-override""><code>If the weight is over 50 kg, take product A.
If less than the upper weight, take product B. 
</code></pre>
<p>I also have the labelled data for the corresponding free text fields.</p>
<pre class=""lang-py prettyprint-override""><code>Free text   Product Weight Size
(see above) A       110    50x58x98
</code></pre>
<p>What is the best approach to solve such a problem? Is there any literature on this subject? Is there also any blog/paper/jupyter notebook or anything else where someone implemented a similar classifciation.</p>
","nlp"
"111176","Building a graph out of a large text corpus","2022-05-21 07:19:36","111183","1","917","<nlp><text-mining><similarity><graphs><similar-documents>","<p>I'm given a large amount of documents upon which I should perform various kinds of analysis. Since the documents are to be used as a foundation of a final product, I thought about building a graph out of this text corpus, with each document corresponding to a node.</p>
<p>One way to build a graph would be to use models such as USE to first find text embeddings, and then form a link between two nodes (texts) whose similarity is beyond a given threshold. However, I believe it would be better to utilize an algorithm which is based on plain text similarity measures, i.e., an algorithm which does not &quot;convert&quot; the texts into embeddings. Same as before, I would form a link between two nodes (texts) if their text similarity is beyond a given threshold. Now, the question is: what is the simplest way to measure similarity of two texts, and what would be the more sophisticated ways? I thought about first extracting the keywords out of the two texts, and then calculate Jaccard Index.</p>
<p>Any idea on how this could be achieved is highly welcome. Feel free to post links to papers that address the issue.</p>
<p>NB: I would also appreciate links to Python libraries that might be helpful in this regard.</p>
","nlp"
"111111","What is the logic/algorithm behind 'did you mean' suggestion by search engines, command suggestion in command prompt like git?","2022-05-19 04:13:10","","1","129","<nlp><similarity><text><search>","<p>For eg. <a href=""https://stackoverflow.com/questions/307291/how-does-the-google-did-you-mean-algorithm-work"">https://stackoverflow.com/questions/307291/how-does-the-google-did-you-mean-algorithm-work</a>
this is the logic behind google's did you mean algorithm - used for spell correction suggestion.
What is the algorithm used in case of other search algorithm for spell correction/ to find similar text - in case of a music/OTT search app, eg. amazon music -
<a href=""https://i.sstatic.net/vcD3c.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/vcD3c.png"" alt=""enter image description here"" /></a></p>
<p>Similarly - what is the logic used - in case of git commands -
<a href=""https://i.sstatic.net/FOFJk.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/FOFJk.png"" alt=""enter image description here"" /></a></p>
<p>How do one usually backtrack the algorithm behind an application from usage? Any general ideas will also be helpful.</p>
","nlp"
"111076","Why calculating how much removed sentences with most contributing words to the result helps to show that a model is ""*faithful*""?","2022-05-18 12:41:15","","1","11","<nlp><metric><explainable-ai>","<p>I don't understand how the calculation score taking out the sentences where the words contribute the most of to the result helps to show to what extent a model is &quot;<em>faithful</em>&quot; to a reasoning process.</p>
<p>Indeed, a faithfulness score was proposed by Du et al. in 2019 to verify the importance of the identified contributing sentences or words to a given model’s outputs. It is assumed that the probability values for the predicted class will significantly drop if the truly important inputs are removed. The score is calculated as :</p>
<p><span class=""math-container"">$$S_{Faithfullness} = \frac{1}{N}\sum{(y_{x^i} - y_{x^i_{a}})}$$</span></p>
<p>Where <span class=""math-container"">$(y_{x^i})$</span> is the predicted probability for a given target class with original inputs and <span class=""math-container"">$(y_{x^i_{a}})$</span> is the predicted probability for the target class for the input with significant sentences/words removed. This metric is available in AIX360.</p>
<p>Yet, if faithfulness measures how well an interpretation method relates to the actual reasoning process used by the model it is ‘interpreting’, I don't get why faithfulness should be such a method such that seems to rely more on attention weight examination.</p>
","nlp"
"111042","Contextual word embeddings from pretrained word2vec vectors","2022-05-17 18:00:53","","1","52","<deep-learning><neural-network><nlp><word-embeddings><text-classification>","<p>I would like to create word embeddings that take <strong>context</strong> into account, so the vector of the word <em>Jaguar [animal]</em> would be different from the word <em>Jaguar [car brand]</em>.</p>
<p>As you know, word2vec only gives one representation for a given word, and I would like to take already pretrained embeddings and enrich them with context. So far I've tried a simple way with taking an average vector of the word and category word, for example <a href=""https://i.sstatic.net/NtMaT.png"" rel=""nofollow noreferrer"">like this</a>.</p>
<p>Now I would like to try to create and train a neural network that would take entire sentences, e.g.</p>
<ol>
<li><em>Jaguar F-PACE is a great SUV sports car.</em></li>
<li><em>Among cats, only tigers and lions are bigger than jaguars.</em></li>
</ol>
<p>And then it would undertake the task of text classification (I have a dataset with several categories like animals, cars, etc.), but the result would be new representations for the word jaguar, but in different contexts, so two different embeddings.</p>
<p>To simplify the whole thing, I'm assuming a limited number of embeddings per word, I have a dataset of a dozen words - each word has two/three meanings and each meaning has dozens of sentences - a small dataset initially, as the whole work is heavily experimental.</p>
<p>Does anyone have any idea how I could create such a network? I don't hide that I'm a beginner and have no idea how to go about it.</p>
","nlp"
"110999","Entity Embeddings of email address","2022-05-16 17:19:01","111166","2","462","<machine-learning><nlp><named-entity-recognition>","<p>I have a set of email address e.g. guptamols@gmail.com, neharaghav@yahoo.com, rkart@gmail.com, squareyards321@ymail.com.....</p>
<p>Is it possible to apply ML/Mathematics to generate category (like NER) from Id (part before @). Problem with straight forward application of NER is that the emails are not proper english.</p>
<ul>
<li>guptamols@gmail.com &gt; Person</li>
<li>neharaghav@yahoo.com &gt; Person</li>
<li>rkart@gmail.com &gt; Company</li>
<li>yardSpace@ymail.com &gt; Company</li>
<li>AgraTextile@google.com &gt; Place/Company</li>
</ul>
","nlp"
"110989","NLP : What is the difference between Authorship Attribution, Authorship Identification and Authorship Recognition?","2022-05-16 13:50:53","","2","238","<nlp>","<p>I have to write my Master's thesis on this topic (I'm in Natural Language Processing) and while I sometimes see these terms used interchangeably other sources seem to emphasize the fact that there are distinctions between them without explaining in what they differ.</p>
","nlp"
"110968","Is there a sensible notion of 'character embeddings'?","2022-05-15 19:17:03","110981","1","171","<nlp><word-embeddings><embeddings>","<p>There are several popular word embeddings available (e.g., <a href=""https://arxiv.org/abs/1607.04606"" rel=""nofollow noreferrer"">Fasttext</a> and <a href=""https://nlp.stanford.edu/pubs/glove.pdf"" rel=""nofollow noreferrer"">GloVe</a>); In short, those embeddings are a tool to encode words along with a sensible notion of <em>semantics</em> attached to those words (i.e. words with similar <em>sematics</em> are nearly parallel).</p>
<p><strong>Question:</strong></p>
<p><strong>Is there a similar notion of <em>character embedding</em>?</strong></p>
<p>By '<em>character embedding</em>' I understand an algorithm that allow us to encode characters in order to capture some syntactic similarity (i.e. similarity of character shapes or contexts).</p>
","nlp"
"110919","How are the embedding and context matrices created and updated in word embedding?","2022-05-13 14:48:19","","1","331","<nlp><word-embeddings><embeddings>","<p>I am struggling to understand how word embedding works, especially how the embedding matrix <span class=""math-container"">$W$</span> and context matrix <span class=""math-container"">$W'$</span> are created/updated. I understand that in the Input we may have a one-hot encoding of a given word, and that in the output we may have the word the most likely to be nearby this word <span class=""math-container"">$x_i$</span></p>
<p><a href=""https://i.sstatic.net/2Wdpv.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/2Wdpv.png"" alt=""enter image description here"" /></a></p>
<p>Would you have any very simple mathematical example?</p>
","nlp"
"110877","Can i use Transformer-XL for text classification task?","2022-05-12 08:24:51","","0","179","<deep-learning><nlp><tensorflow><transformer><text-classification>","<p>I want to use transformer xl for text classification tasks. But I don't know the architect model for the text classification task. I use dense layers with activation softmax for logits output from the transformer xl model, but this doesn't seem right. when training I see accuracy is very low.</p>
<p>Output of my model:</p>
<p><a href=""https://i.sstatic.net/F7QFy.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/F7QFy.png"" alt=""output, logits transformer-xl"" /></a></p>
<p>My training step:</p>
<p><a href=""https://i.sstatic.net/pgmk1.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/pgmk1.png"" alt=""dense layer"" /></a></p>
","nlp"
"110872","Dealing with near duplicates using NLP","2022-05-12 01:56:28","","0","577","<machine-learning><python><deep-learning><nlp><spacy>","<p>I have a dataframe like as shown below</p>
<pre><code>ID,Name,year,output
1,Test Level,2021,1
2,Test Lvele,2022,1
2,dummy Inc,2022,1
2,dummy Pvt Inc,2022,1
3,dasho Ltd,2022,1
4,dasho PVT Ltd,2021,0
5,delphi Ltd,2021,1
6,delphi pvt ltd,2021,1

df = pd.read_clipboard(sep=',')
</code></pre>
<p>My objective is</p>
<p>a) To replace near duplicate strings using a common string.</p>
<p>For example - let's pick couple of strings from <code>Name</code> column. We have <code>dummy Inc</code> and <code>dummy Pvt Inc</code>. These both have to be replaced as <code>dummy</code></p>
<p>I manually prepared a mapping df <code>map_df</code> like as below (but can't do this for big data)</p>
<pre><code>  Name,correct_name
  Test Level,Test
  Test Lvele,Test
  dummy Inc,dummy
  dummy Pvt Inc,dummy
  dasho Ltd,dasho
  dasho PVT Ltd,dasho
  delphi Ltd,delphi
  delphi pvt ltd,delphi
</code></pre>
<p>So, I tried the below</p>
<pre><code>map_df = map_df.set_index(Name)
df['Name'] = df['Name'].map(map_df) # but this doesn't work and throws error
</code></pre>
<p>Is creating mapping table the only way or is there any NLP based approach?</p>
<p>I expect my output to be like as below</p>
<pre><code>ID,Name,year,output
1,Test,2021,1
2,Test,2022,1
2,dummy,2022,1
2,dummy,2022,1
3,dasho,2022,1
4,dasho,2021,0
5,delphi,2021,1
6,delphi,2021,1
</code></pre>
","nlp"
"110814","How to train a machine learning model for named entity recognition","2022-05-10 02:44:07","","1","110","<machine-learning><nlp><named-entity-recognition>","<p>I cannot find any sources about the architectures of machine learning models to solve for NER problems. I vaguely knows it is a multiclass classification problem, but how can we format our input to feed into such multiclass classifier? I know the inputs must be annotated corpus, but how can we feed that chunk of pairs of (word, entity label) into the classifier? Or, how do you feature-engineer such corpus to feed into ML models? Or, in general, how can you train a custom NER from scratch with machine learning?</p>
<p>TIA.</p>
","nlp"
"110741","Is it possible use cluster analysis on word co-occurrences?","2022-05-07 12:09:33","","1","71","<nlp><clustering>","<p><strong>Problem:</strong> I am unsure if there is an appropriate clustering method to do the following: I wish to group a list of word co-occurrences into their possible clusters.</p>
<p><strong>Context:</strong> I have a dataset containing (1) frequencies for the number of times the list of terms appeared in the context of the main term (i.e., conditional probabilities). These were extracted from corpus data. I think I might need another metric to start clustering other than frequency of times the term appeared, or the count of times the word appeared in the context of the keyword. Would anyone be able to provide a source that might explain where to from here?</p>
","nlp"
"110718","Sum vs mean of word-embeddings for sentence similarity","2022-05-06 13:23:31","110720","8","2962","<nlp><word-embeddings><word2vec>","<p>So, say I have the following sentences</p>
<p>[&quot;The dog says woof&quot;, &quot;a king leads the country&quot;, &quot;an apple is red&quot;]</p>
<p>I can embed each word using an <code>N</code> dimensional vector, and represent each sentence as either the sum or mean of all the words in the sentence (e.g <code>Word2Vec</code>).</p>
<p>When we represent the words as vectors we can do something like <code>vector(king)-vector(man)+vector(woman) = vector(queen)</code> which then combines the different &quot;meanings&quot; of each vector and create a new, where the mean would place us in somewhat &quot;the middle of all words&quot;.</p>
<p>Are there any difference between using the sum/mean when we want to compare similarity of sentences, or does it simply depend on the data, the task etc. of which performs better?</p>
","nlp"
"110715","Evaluation of the preprocessing to make a dataset anonymous","2022-05-06 12:41:50","","1","27","<python><nlp><anonymization>","<p>I have a very huge dataset from the NLP area and I want to make it anonymous. Is there any way to check if my pre-processing is correct? Generaly, is there any way to evaluate how good is the pre-processing for the anonyminity?</p>
<p>I want to mention that the dataset is really huge, therefore it can be cheched manually.</p>
","nlp"
"110659","Word similarity considering special characteristics","2022-05-05 06:44:35","","0","36","<nlp><similarity>","<p>I'm looking for an algorithm that computes the similarity between two strings just like the <code>levenshtein</code> distance. However, I want to consider the following. The  <code>levenshtein</code> distance gives me the same for these cases:</p>
<pre><code>distance(&quot;apple&quot;, &quot;appli&quot;) #1
distance(&quot;apple&quot;, &quot;appel&quot;) #1
distance(&quot;apple&quot;, &quot;applr&quot;) #1
</code></pre>
<p>However, I want the second and third example to have a smaller distance because of the following reasons:</p>
<ul>
<li>second example: all the correct letters are used in the second word</li>
<li>third example: <code>r</code> is much likely to be a typo of the letter <code>e</code> because of the keyboard placement.</li>
</ul>
<p>Are you familiar with any algorithm that weights such characteristics ?</p>
","nlp"
"110638","Optimal clusters for K-means not clear - any ideas?","2022-05-04 11:04:45","","0","47","<nlp><scikit-learn><clustering><k-means><tfidf>","<p>I have a toy dataset of 10,000 strings of people's names, addresses and birthdays. As a quirk of the data collection process it is highly likely there are duplicate people caused by typos and I am trying to cluster them using K-means. I know there are easier ways of doing this, but the reason I am doing it like this is out of curiosity.</p>
<p>In order to vectorize each person I am concatenating the strings as follows:
<strong>[name][address][birthday]</strong> and then running this through the following function to tokenise and clean the string:</p>
<pre><code>def preprocess_text(text):
    # remove links
    text = re.sub(r&quot;http\S+&quot;, &quot;&quot;, text)
    # remove special chars and numbers
    text = re.sub(&quot;[^A-Za-z]+&quot;, &quot; &quot;, text)
    # remove stopwords
    if remove_stopwords:
        # 1. tokenize
        tokens = nltk.word_tokenize(text)
        # 2. check if stopword
        tokens = [w for w in tokens if not w.lower() in stopwords.words(&quot;english&quot;)]
        # 3. join back together
        text = &quot; &quot;.join(tokens)
    # return text in lower case and stripped of whitespaces
    text = text.lower().strip()
    return text
</code></pre>
<p>This outputs to a dataframe containing the corpus where each 'person' consists of string that is something like:</p>
<p><strong>john smith belgrave road birmingham england</strong></p>
<p>The entire corpus is then run through the TF-IDF vectorizer in sklearn.</p>
<p>When I use the Elbow method and the Silhouette Method to try and get the optimimal cluster number K for K-means, I get two graphs that don't show a clear value of K:</p>
<p><a href=""https://i.sstatic.net/nA1HK.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/nA1HK.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.sstatic.net/qBCD3.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/qBCD3.png"" alt=""enter image description here"" /></a></p>
<p>I've not really seen anything like this before and I was wondering if anyone has ideas for an optimal value of K based on these charts? Or, if these charts show that K-means really isn't a good way of doing this? Or, perhaps a TF-IDF vectorizer isn't a good approach and a Bag-of-Words vectoriser would be better as names and addresses <em>should be</em> semantically neutral? Any insight would be really appreciated. Thank you!</p>
","nlp"
"110550","Suggestions for a multi-class text classification model with a large number of classes?","2022-05-02 07:16:46","","0","1106","<nlp><multiclass-classification><text-classification>","<p>I was working on a text classification problem where I currently have around 40-45 different labels.</p>
<p>The input is a text sentence with a keyword. For e.g. <code>This phone is the most durable in the market</code> is the input sentence and the out label is <code>X</code> and all the words in the output with label <code>X</code> will have <code>durable</code> as a keyword.</p>
<p>What would be a good model to fit this? I tried basic SVM, Random Forest but to no use. I am not sure how to leverage the keyword to create a better model. Any suggestions would be very welcome.</p>
<p>Thanks!</p>
","nlp"
"110547","Ideal Windows Size in Pk Evaluation Metric","2022-05-02 01:12:26","","0","155","<machine-learning><nlp><model-evaluations><text-classification><automatic-summarization>","<p>I am very new to nlp. I am doing a text segmentation task and for evaluating my model I need to calculate Pk and Windiff scores. My question is what is the ideal value for window size (k) for Pk score because different window sizes give different results. I am using this function nltk.metrics.segmentation.pk. Thanks.</p>
","nlp"
"110522","getting actual concepts value instead of its URI in ontology","2022-04-30 19:39:08","","1","41","<nlp><sentiment-analysis><ai><knowledge-graph>","<p>I am using owl ontology for semantic analysis in emotional sentiment analysis project ,
I am trying to navigate the ontology to check a concepts and its relation ,
my ontology has classes like this :</p>
<pre><code>&lt;!-- http://purl.obolibrary.org/obo/MFOEM_000011 --&gt;

&lt;owl:Class rdf:about=&quot;http://purl.obolibrary.org/obo/MFOEM_000011&quot;&gt;
    &lt;rdfs:subClassOf rdf:resource=&quot;http://purl.obolibrary.org/obo/MFOEM_000001&quot; /&gt;
    &lt;rdfs:subClassOf&gt;
        &lt;owl:Restriction&gt;
            &lt;owl:onProperty rdf:resource=&quot;http://purl.obolibrary.org/obo/BFO_0000117&quot; /&gt;
            &lt;owl:someValuesFrom rdf:resource=&quot;http://purl.obolibrary.org/obo/MFOEM_000208&quot; /&gt;
        &lt;/owl:Restriction&gt;
    &lt;/rdfs:subClassOf&gt;
    &lt;obo:IAO_0000115&gt;An unpleasant emotion closely related to anger but lower in intensity and without the moral dimension of blame and seriousness that is implicated in anger. [Source: OCEAS]&lt;/obo:IAO_0000115&gt;
    &lt;obo:MFOEM_000010 xml:lang=&quot;es&quot;&gt;irritación&lt;/obo:MFOEM_000010&gt;
    &lt;obo:MFOEM_000010 xml:lang=&quot;fr&quot;&gt;irritation&lt;/obo:MFOEM_000010&gt;
    &lt;obo:MFOEM_000010 xml:lang=&quot;de&quot;&gt;Ärger&lt;/obo:MFOEM_000010&gt;
    &lt;obo:MFOEM_000165&gt;irritated&lt;/obo:MFOEM_000165&gt;
    &lt;rdfs:label&gt;irritation&lt;/rdfs:label&gt;
&lt;/owl:Class&gt;
</code></pre>
<p>I am using this code to retrieve concepts from triples s,p,o :</p>
<pre><code>from rdflib.namespace import RDF, RDFS, OWL ,FOAF
from pprint import pprint


properties2=set()
for p in g.subjects():
    properties2.add(p)    
pprint(properties2)
</code></pre>
<p>and the output is something like this :</p>
<pre><code> rdflib.term.URIRef('http://purl.obolibrary.org/obo/MFOEM_000221')
 rdflib.term.URIRef('http://purl.obolibrary.org/obo/MFOEM_000222')
 rdflib.term.URIRef('http://purl.obolibrary.org/obo/MFOEM_000223')
 rdflib.term.URIRef('http://purl.obolibrary.org/obo/MF_0000020')
 rdflib.term.URIRef('http://purl.obolibrary.org/obo/MF_0000029')
 rdflib.term.URIRef('http://purl.obolibrary.org/obo/MF_0000031')
 rdflib.term.URIRef('http://purl.obolibrary.org/obo/MF_0000039')
 rdflib.term.URIRef('http://purl.obolibrary.org/obo/NBO_0000003')
 rdflib.term.URIRef('http://purl.obolibrary.org/obo/comment')
 rdflib.term.URIRef('http://purl.org/dc/elements/1.1/contributor')
 rdflib.term.URIRef('http://purl.org/dc/elements/1.1/title')
 rdflib.term.URIRef('http://www.w3.org/2000/01/rdf-schema#comment')
 rdflib.term.URIRef('http://www.w3.org/2000/01/rdf-schema#label')}
</code></pre>
<p>but ,it return URIRef instead of the concept value
can anyone help me to understand why it returns that and how can I get the actual concepts behind of URI
excuse me if I have misunderstanding but , I am new in this topic and i did a hard googling and didn't find any reslut</p>
","nlp"
"110516","Looking for a generalized (extended) lemmatizer","2022-04-30 12:19:31","","2","37","<nlp><nltk>","<p>Whenever I lemmatize a compound word in English or German, I obtain a result that ignores the compound structure, e.g. for 'sidekicks' the NLTK WordNet lemmatizer returns 'sidekick', for 'Eisenbahnfahrer' the result of the NLTK German Snowball lemmatizer is 'eisenbahnfahr'.
What I need, however, is something that would extract the primary components out of compound words: ['side', 'kick'] and, especially, ['eisen', 'bahn', 'fahr'] (or 'fahren' or in whatever form for the last item). I am especially interested in segmentizing compound words for German.</p>
<p>I failed to find anything of the kind. This kind of an NLP pipe would probably not be called a lemmatizer (or would it?) Is there a definition for it?</p>
","nlp"
"110515","Why (or how) does a Keras model skip Stemming or Lemmatization steps?","2022-04-30 12:00:08","","0","295","<keras><nlp><sentiment-analysis>","<p>This <a href=""https://www.tensorflow.org/tutorials/keras/text_classification"" rel=""nofollow noreferrer"">Keras article / tutorial here</a> does perform text standardization i.e removing HTML elements, punctuation, etc. from the text dataset, however, there is a distinct lack of any stemming or lemmatization before the vectorization step.</p>
<p>I have a bit of experience in deep learning but I am very new to NLP, and I just got to know (from a <a href=""https://www.udemy.com/course/machinelearning/learn/lecture/19958908#overview"" rel=""nofollow noreferrer"">different tutorial on Udemy</a>, which BTW was using Bag of Words) that using either a Stemmer or a Lemmatizer helps in bringing down the vocabulary size and hence increases performance. I am a bit baffled by the absence of this step in the Keras-way of doing things.</p>
<p>Here is one assumption of mine - is it omitted because a Neural Network model is capable of handling a larger vocabulary size? I cannot think of any other reason(s) as to why that might be the case.</p>
","nlp"
"110511","Are the word of women and men different when expressing their views on the same subject?","2022-04-30 07:53:34","","0","45","<nlp><sentiment-analysis><topic-model><lda><gensim>","<p>My data includes women's comments on X and Y and men's comments on X and Y. Each comment is of equal length. I will calculate how much different the word choice between men and women when commenting on X. How can it do this?</p>
","nlp"
"110510","sentence type classification","2022-04-30 07:34:39","","2","78","<deep-learning><classification><nlp><text-classification>","<p>I want to classify the sentences in my dataset as <code>declarative</code>, <code>interrogative</code>, <code>imperative</code> and <code>exclamative</code>. Although It can be classified with respect to punctuation marks such as <code>?</code>, <code>!</code> and <code>.</code> but there are many cases and situations that these rules can fail.</p>
<p>In NLP area, is there any model or solution that can be applied to reach the mentioned goal?</p>
","nlp"
"110461","How to perform Grid Search on NLP CRF model","2022-04-28 16:41:32","","1","751","<nlp><machine-learning-model><gridsearchcv>","<p>I am trying to perform hyperparameter tuning on sklearn_crfsuite.CRF model. When I try to execute below code, it doesn't give any exception but it probably fails to perform fit. And due to which, if I try to get best estimator from grid search, it doesn't work.</p>
<pre><code>%%time
# define fixed parameters and parameters to search
crf = sklearn_crfsuite.CRF(
    algorithm='lbfgs',
    max_iterations=100,
    all_possible_transitions=True
)
params_space = {
    &quot;c1&quot;: [0,0.05,0.1, 0.25,0.5,1],
    &quot;c2&quot;: [0,0.05,0.1, 0.25,0.5,1]
}

# use the same metric for evaluation
f1_scorer = make_scorer(metrics.flat_f1_score,
                        average='weighted', labels=labels)

# search
grid_search = GridSearchCV(estimator=crf,
                           param_grid=params_space,
                           cv=3,
                           n_jobs=-1, verbose=1,scoring=f1_scorer)

#grid_search.fit(X_train, Y_train)
#above code throws exception, which seems to be a open bug in latest version of scikit-learn 0.24.0 or later.
#github link for bug: https://github.com/TeamHG-Memex/sklearn-crfsuite/issues/60
try:
    grid_search.fit(X_train, Y_train)
except AttributeError as e:
     if &quot;'CRF' object has no attribute 'keep_tempfiles'&quot; not in str(e):
        raise
</code></pre>
<p>Any help would be appreciated, how can I perform hyperparameter tuning here?</p>
<p>I took reference using this <a href=""https://sklearn-crfsuite.readthedocs.io/en/latest/tutorial.html#hyperparameter-optimization"" rel=""nofollow noreferrer"">tutorial</a>, but stuck in same situation.</p>
","nlp"
"110454","How do i generate text from ids in Torchtext's sentencepiece_numericalizer?","2022-04-28 14:05:11","110477","0","246","<python><nlp><pytorch><bert><transformer>","<p>The torchtext <code>sentencepiece_numericalizer()</code> outputs a generator with indices SentencePiece model corresponding to token in the input sentence. From the generator, I can get the ids.</p>
<p>My question is how do I get the text back after training?</p>
<p>For example</p>
<pre><code>&gt;&gt;&gt; sp_id_generator = sentencepiece_numericalizer(sp_model)
&gt;&gt;&gt; list_a = [&quot;sentencepiece encode as pieces&quot;, &quot;examples to   try!&quot;]
&gt;&gt;&gt; list(sp_id_generator(list_a))
    [[9858, 9249, 1629, 1305, 1809, 53, 842],
     [2347, 13, 9, 150, 37]]
</code></pre>
<p>How do I convert <code>list_a</code> back t(i.e <code>&quot;sentencepiece encode as pieces&quot;, &quot;examples to try!&quot;</code>)?</p>
","nlp"
"110310","Should I pretrain my BERT model on specific dataset if it has only one class of labels?","2022-04-24 12:48:40","","0","112","<machine-learning><deep-learning><nlp><bert><transformer>","<p>I want to use BERT model for sentences similarity measuring task. I know that BERT models were trained with natural language inference architecture with dataset with labels neutral, entailment, contradiction.</p>
<p>My data to which I want to apply BERT for sentences similarity task has very specific terms and jargon, so I want to pretrain model on it before. But in that data there are only cases of entailment labels (about 20k rows). Is it a good idea to pretrain model on that data? How could I handle my problem the best way?</p>
<p>Thanks in advance</p>
","nlp"
"110216","NLP work related to distinguishing scenes in a story","2022-04-20 09:48:23","","1","20","<nlp>","<p>Say I have a story or novel with multiple scenes. Are there any NLP work/techniques that allow me to know when the author switches from one scene to another? Searching things like &quot;context&quot;, &quot;scene&quot; in google does not yield good results.</p>
<p>Assume that I do not have a list of characters' names and there may be nameless NPCs.</p>
","nlp"
"110203","how to deal with large numbers of unlabelled target dataset?","2022-04-20 01:19:40","","2","42","<nlp><clustering><multiclass-classification>","<p>I have dataset of 5000 jobs descriptions out of which only 200 jobs are labelled with required English level score range between 0 to 9 and I want to predict remaining 4800 jobs required English level score? how to use clustering or multi classification in this scenario? Thanks</p>
","nlp"
"110194","Guide to Natural language Prompt programming for few-shot learning of Pretrained Language Models","2022-04-19 18:43:36","","1","56","<deep-learning><nlp><transformer><text-generation><gpt>","<p>I'm currently working on a project with the goal of producing AI content in the space of a content generation like blog writing, Instagram caption generation etc. Found the in-context few-shot learning capabilities of the GPT-3 quite useful but I'm unable to generate creative content consistently. It becomes boring and repetitive in nature after a few iterations. I came across the concept of knowledge probing of language models and have come to this understanding that writing better prompts can actually solve my problem.</p>
<p>Can the community guide me to the right set of papers or other articles/blogs that expand on this idea further? so that I can make some progress on this interesting use case. Thanks, regards!.</p>
","nlp"
"110192","Question Categorization Dataset(s)","2022-04-19 17:55:20","","1","28","<nlp><dataset>","<p>I'm looking for a dataset or group of datasets I could combine that would contain numerous examples of the following types of question categories.</p>
<ul>
<li>free response <em>(ex: What is the capital of Portugal?)</em></li>
<li>yes or no <em>(ex: Do you want me to turn off the lights?)</em></li>
<li>numerical <em>(ex: What is the temperature in here?)</em></li>
<li>conjunction <em>(ex: Do you want the red or the blue blanket?)</em></li>
</ul>
<p>I've found that there is a ton of datasets for free response questions, and a couple for yes/no questions, but almost none for numeral and conjunction questions. Any suggestions?</p>
","nlp"
"110119","Why shouldn't we mask [CLS] and [SEP] in preparing inputs for a MLM?","2022-04-18 03:28:43","","2","803","<nlp><bert><masking>","<p>I know that MLM is trained for predicting the index of MASK token in the vocabulary list, and I also know that [CLS] stands for the beginning of the sentence and [SEP] telling the model the end of the sentence or another sentence will come soon, but I still can't find the reason for unmasking the [CLS] and [SEP].</p>
<p>Here is the situation that I imagine:
We have a sentence pair like s1/s2, we input its input_ids into the model as the form like &quot;101 xxx 102 yyy 102&quot; and then I think we can ask model predict the token at the middle of the 'xxx' and 'yyy'(namely the first 102), so we can mask the token as 103 which means the MASK token.</p>
<p>I think the imagination is reasonable, could anyone give me a key?</p>
","nlp"
"110093","Does N-gram language model for text generation are more efficient than Neural Network language models?","2022-04-16 16:43:01","","1","63","<nlp><lstm><rnn><text-generation><ngrams>","<p>I recently build an language model with N-gram model for text generation and for change I started exploring Neural Network for text generation. One thing I observed that  the previous model results were better than the LSTM  model even when both where built using same corpus.</p>
","nlp"
"110006","Why we need to 'train word2vec' when word2vec itself is said to be 'pretrained'?","2022-04-14 11:30:58","110043","1","876","<nlp><word-embeddings><word2vec>","<p>I get really confused on why we need to 'train word2vec' when word2vec itself is said to be 'pretrained'? I searched for word2vec pretrained embedding, thinking i can get a mapping table directly mapping my vocab on my dataset to a pretrained embedding but to no avail. Instead, what I only find is how we literally train our own:</p>
<pre><code>Word2Vec(sentences=common_texts, vector_size=100, window=5, min_count=1, workers=4)
</code></pre>
<p>But I'm confused: isn't word2vec already pretrained? Why do we need to 'train' it again? If it's pretrained, then what do we modify in the model (or specifically, which part) with our new 'training'? And how does our now 'training' differ from its 'pretraining'? TIA.</p>
<p>Which type of word embedding are truly 'pretrained' and we can just use, for instance, model['word'] and get its corresponding embedding?</p>
","nlp"
"109999","How are the weights defined in a (linear-chain) Conditional Random Field?","2022-04-14 08:33:09","","1","176","<machine-learning><nlp><rnn><probability><naive-bayes-classifier>","<p>Edit: i saw that i mixed up i (in the graph) and t (in the formula), in the following i equivalent to t</p>
<p>I am trying to understand the theory behind linear chain Conditional Random Fields. I have now read &quot;<a href=""https://arxiv.org/abs/1011.4088"" rel=""nofollow noreferrer"">An Introduction to Conditional Random Fields</a>&quot; by McCallum and Sutton, I think McCallum is one of the &quot;inventors&quot; of CRFs. In this work you can find the following representation of the CRF as a graph (I added some annotations to better explain my question):</p>
<p><a href=""https://i.sstatic.net/q37cs.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/q37cs.jpg"" alt=""CRF"" /></a></p>
<p>As I understood it and as I know it from other works, the black squares on the connections are the weights that are learned by the training, right?
The formula for linear chain CRFs is defined as:</p>
<p><a href=""https://i.sstatic.net/mnDRV.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/mnDRV.png"" alt=""linear-chain crf"" /></a></p>
<p>Where Theta is the weight of the respective features function. It is unclear to me how Theta is defined, is Theta in the graph above represented by one of the black squares or a combination of them, for example for I a combination of w_2 and w_3, since the features function for i is f(X_i, Y_i, Y_i-1)?
Or is theta not represented at all in the above graph?</p>
","nlp"
"109982","Perplexed by perplexity","2022-04-13 15:35:11","","2","51","<nlp><perplexity>","<p>I've seen 2 definitions of the perplexity metric:</p>
<p><span class=""math-container"">$PP = 2^{H(p)}$</span></p>
<p>and</p>
<p><span class=""math-container"">$PP = 2^{H(p, q)}$</span></p>
<p>If I'm understanding correctly, the first one only tells us about how confident the model is about its predictions, while the second one reflects the accuracy/correctness of the model's predictions. Am I correct?</p>
<p>Which one do people actually refer to when they claim their language model achieved X perplexity in their papers?</p>
","nlp"
"109904","Can I use Bert on data subsets and get a compatible representation for the whole dataset?","2022-04-11 20:46:42","","1","15","<nlp><bert>","<p>I need to build an embedding for a massive amount of phrases. I want to use BERT (through the library <a href=""https://www.sbert.net/"" rel=""nofollow noreferrer"">https://www.sbert.net/</a>).</p>
<p>Can I build a partial representation of the data, say encoding 1000 sentences and then another 1000 and join the matrices at the end? If I generate the embeddings by parts (of the whole dataset), will I get a compatible vector representation between the different results? Or, on the contrary, should I build the representation with the whole dataset at the same time?</p>
<p>My final goal is to cluster and analyze the sentence vectors of the whole dataset.</p>
<p>I would appreciate any suggestions on what to read or how to better approach this question. Thanks!!!</p>
","nlp"
"109858","What approach I should take to extract number entity from dataset","2022-04-10 17:03:00","","1","150","<machine-learning><python><nlp><spacy>","<p>I have the training, validation, and test dataset. The first column has store data and the second column has store numbers. I need to develop an entity extractor model which can extract store numbers from the first column. I tried searching about entity extractor models like SpaCy and Stanford NER package but did not quite understand how to implement them in this scenario.</p>
<p><a href=""https://i.sstatic.net/LCWze.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/LCWze.png"" alt=""sample image data"" /></a></p>
<p>As you can see above, a store number is not always numeric data, but what I found out is that it always comes after the store name. If someone could share a reference link for this or any suggestions, please.
Thanks in advance!</p>
","nlp"
"109827","How to interpret integrated gradients in an NLP toxic text classification use-case?","2022-04-09 14:30:22","","1","48","<neural-network><nlp><gradient-descent><explainable-ai><gradient>","<p>I am trying to understand how integrated gradients work in the NLP case.</p>
<blockquote>
<p>Let <span class=""math-container"">$F: \mathbb{R}^{n} \rightarrow[0,1]$</span> a function representing a neural network, <span class=""math-container"">$x \in \mathbb{R}^{n}$</span> an input and <span class=""math-container"">$x' \in \mathbb{R}^{n}$</span> a reference. We consider <strong>the segment connecting <span class=""math-container"">$x$</span> to <span class=""math-container"">$x'$</span></strong>, and we compute the gradient at any point of this segment. The IG method is simply to sum these gradients. Thus, <span class=""math-container"">$I G$</span> in the ith dimension is given by the following formula:</p>
<p><span class=""math-container"">$$
I G_{i}(x)=\left(x_{i}-x'_{i}\right) \frac{\int_{\alpha=0}^{1} d F(x'+\alpha(x-x \prime))}{d x_{i}} d \alpha
$$</span></p>
<p>The advantage that IG has over other existing methods is that it satisfies the two axioms of sensitivity and implementation invariance that we detail in the next paragraph.</p>
</blockquote>
<p>In the case of NLP, where <span class=""math-container"">$x$</span> may be a text, <span class=""math-container"">$F$</span> a toxicity classification algorithm, and <span class=""math-container"">$x'$</span> the reference (but what kind of reference? a non-toxic text? Or a toxic one?).</p>
","nlp"
"109822","NLP Basic input doubt","2022-04-09 12:27:42","109881","1","41","<machine-learning><deep-learning><nlp><lstm><tfidf>","<p>I actually have a basic doubt in NLP,</p>
<p>When we consider traditional models like Decision trees, The feature column order is important, Like first column is fixed with some particular attribute. So If, I have Tf-Idf Each word will have some fixed index and the model can learn.</p>
<p>But in the case of LSTM, Sentences can be jumbled. For eg: &quot;There is heavy rain&quot;, &quot;Heavy rain is there&quot;</p>
<p>In the above 2 sentences, The word heavy occurs in different places. So in order for the model to understand that we have passed the word &quot;There&quot;, We would require some unique representations for the word &quot;there&quot;. Either a One-Hot or Word2vec. Is my understanding so far right?</p>
<p>My final doubt is, If I use tfidf for the above, How will it work? How will the model understand that &quot;heavy&quot; word is passed? This doubt has been bugging me for long. Kindly clarify this! Thanks a ton!</p>
","nlp"
"109805","Extracting information from bills, tax statements, etc: What ML model to use?","2022-04-08 17:53:57","","1","38","<nlp><information-retrieval><information-extraction>","<p>I have a bunch of documents such as bank statements, utilities bills, personal expenditure invoices, etc. The document types range is very broad. Some of these files are saved as pictures, others as pdfs.</p>
<p>So far, my tactic has been to ocr all the documents, and then use some regexes to extract information (I would like to extract dates, quantities/amounts and entities). However, this hasn't worked out great so far...</p>
<p>Thus, I was wondering what other possibilities there were in the Machine Learning field.</p>
<p>I've searched the Named-Entity-Recognition (NER) deep learning type of models like those in huggingface, but maybe I'm missing some alternatives.</p>
<ol>
<li>What alternatives are there to NER?</li>
<li>Which NER models have reported good results for this type of task?</li>
</ol>
<p>Any help would be appreciated.</p>
","nlp"
"109804","Which Pointers from WordNet are Used for Synset in NLTK","2022-04-08 17:36:26","","1","20","<nlp><nltk>","<p>I'm trying to create a custom parser for wordnet and hit a roadblock. I see that there are tons of different pointer_symbols and lots of them seem almost like synonyms but not exactly synonyms. I'm trying to extract the synonyms for each word and I'm not sure which pointers should be considered. I couldn't find anything through NLTK as well as to which pointer_symbols does it use for it's task.</p>
<p>Any hints on what should I use?</p>
","nlp"
"109800","NLP LSTM input basic doubt","2022-04-08 15:06:47","","1","96","<machine-learning><deep-learning><nlp><lstm><rnn>","<p>I have a basic doubt with regards to conversion of text to numbers and feeding it to LSTM. I am aware of the different methods such as OneHot, CountVectorizer, TfIDF, Word2vec etc. My doubt is, If we use a Count Vectoriser or Tfidf, Then in LSTM, we have to pass through the entire vocabulary of words for each sentence since that's how TFIDF and count vectoriser encodes the sentences. Am I right?</p>
<p>My second doubt is, If we use TFIDF or COuntVectorizer, Each word will have different value based on its occurrence and frequency. This is in contrast to Word2Vec where a embedding is learned and used. If each time the LSTM model sees different values for a particular word, How can it learn? Like in a sentence if the word &quot;Hi&quot; appears 6 times, Its encoded with the number 6 in its appropriate index, And in another sentence if it appears 4 times, we encode with the value 4. How does this work? It doesn't make sense.</p>
","nlp"
"109789","validation accuracy is stuck at 99.44% from the first training epoch while training accuracy and loss are increasing and decreasing respectively","2022-04-08 02:36:58","","0","94","<machine-learning><deep-learning><keras><nlp>","<p>and I don't understand why is this happening is it a problem with the architecture or the training itself I tried slightly different models but with the same problem.</p>
<p>its for jigsaw-toxic-comment-classification-challenge</p>
<p><a href=""https://i.sstatic.net/ogpO3.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ogpO3.png"" alt=""model"" /></a>
<a href=""https://i.sstatic.net/WMs8X.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/WMs8X.png"" alt=""training"" /></a></p>
","nlp"
"109759","Generate paragraphs from given words","2022-04-07 01:44:16","","2","330","<nlp><rnn>","<p>I am trying to build a ML model that. will take a list of words and will try to produce sentences with those words, based on a language model on an existing corpus. Example:</p>
<pre><code>(freenode, today, neural networks) -&gt; (Today I joined the freenode channel for neural networks....) 
</code></pre>
<p>What will be the right (and easiest) approach to do this?</p>
<p>I was trying to find an LSTM RNN based solution, but things get trickier once I feed more than one word to the network.</p>
","nlp"
"109755","skip gram vector representation","2022-04-06 19:22:59","","2","39","<neural-network><nlp><ai>","<p>I am using SVM for sentiment analysis project , I want to use n-skip gram for vector representation because I need to keep the semantic meaning between words ,
so I need to make vectors that belongs to a specified class be close to each other , how can I determine the most suitable n-skip to use? (means 2-skip or 3-skip or 4-skip ....),
I am using doc2Vec in python.</p>
","nlp"
"109714","Can you use both copy mechanism and BPE?","2022-04-06 01:21:17","","1","12","<machine-learning><deep-learning><neural-network><nlp><rnn>","<p>I read to alleviate the problem of Out of Vocabulary (OOV), there are two techniques:</p>
<ol>
<li>BPE</li>
<li>Copy mechanism</li>
</ol>
<p>It appears to me they are two orthogonal approaches.</p>
<p>Can we combine the two, i.e.,  we use both the copy mechanism and BPE? Are there any work out there that combines the two? I cant find any.</p>
","nlp"
"109625","NLP text representation techniques that preserve word order in sentence?","2022-04-03 09:51:25","","3","420","<nlp><feature-engineering><text-mining><feature-extraction><text>","<p>I see people are talking mostly about bag-of-words, td-idf and word embeddings. But these are at word levels. BoW and tf-idf fail to represent word orders, and word embeddings are not meant to represent any order at all. What's the best practice/most popular way of representing word order for texts of varying lengths? Simply concatenating word embeddings of individual words into long vector appearly not working for texts of varying lengths...</p>
<p>Or there exists no method of doing that except relying on network architectures like the positional encoding in transformer family?</p>
<p>By the way, ngram is not a solution to me, as it still fails to solve the problem in representing texts in varying lengths. (Or can it and how? It seems to me ngram is more for next word prediction rather than representing texts with varying lengths.)</p>
<p>TIA :)</p>
","nlp"
"109587","Ways to detect negation in Natural Language Processing?","2022-04-02 00:22:15","","0","392","<nlp>","<p>I am studying Natural Language Processing. What could be the ways to detect negation? There are at least two forms of negation that I can think of.</p>
<ol>
<li>I do <strong>not</strong> like orange juice.</li>
<li>I <strong>deny</strong> that I like orange juice.</li>
</ol>
<p>Thanks.</p>
","nlp"
"109586","NLP to calculate similarity ratio between sentences of max 5-6 words","2022-04-01 23:37:58","","1","813","<machine-learning><python><nlp>","<p><strong>Im looking for a relatively simple NLP algo that would help me rate the similarity between two sentences. These sentences usually range between 1-5 words approximately.</strong></p>
<p><strong>Context:</strong></p>
<p>A user can create as many categories as he wishes to group his photos. I noticed that a lot of these categories are empty and when diving a bit deeper I see that a lot of the categories created by a user have almost identical names
E.g. FRANCE VS FRANC |    SUMMER VS SUMER |  BEACH VS BEACH ( HEART EMOTE)</p>
<p>One assumption is that they are creating a category with a spelling mistake and instead of deleting , they create a new one.</p>
<p><strong>Goal:</strong></p>
<p>Quantify the amount of highly similar category pairs at a user level.</p>
<p>So my question is essentially two fold:</p>
<ol>
<li><p>which straightforward NLP algorithm could do the job pretty well without being some convuloted neural network that a company like google uses. heard of cosine similarity for vector space but unsure</p>
</li>
<li><p>what would be an appropriate threshold for similarity ratios? I guess thats subjective but any advice is appreciated</p>
</li>
</ol>
","nlp"
"109584","How to justify logarithmically scaled frequency for tf in tf-idf?","2022-04-01 22:28:07","","1","70","<nlp><tfidf><logarithmic>","<p>I am studying tf-idf (term frequency - inverse document frequency). The original logic for tf was straightforward: count of term t / number of total terms in the document.</p>
<p>However, I came across the log scaled frequency: log(1 + count of term t in the document). Please refer to <a href=""https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Term_frequency_2"" rel=""nofollow noreferrer"">Wikipedia</a>.</p>
<p>It does not include the number of total terms in a document. For example, say, document 1 has 10 words in total and one of them is &quot;happy&quot;. Using the original logic, tf(happy)=1/10=0.1. Document 2 also has one &quot;happy&quot; but it has 1,000 words in total. tf(happy)=1/1000=0.001. You can see the tf(happy) of document 1 is very different from that of document 2.</p>
<p>However, if we use the log scaled frequency, both are log(1+1), regardless of the length of documents (one only has 10 words, while the other has 1,000).</p>
<p>How to justify such logic? Thanks.</p>
","nlp"
"109448","Multi-Class Document Classification with both known and un-known classes","2022-03-29 09:07:28","","2","78","<nlp><multiclass-classification><unsupervised-learning><text-classification>","<p>Currently, I am building a multi-class document classifier which has to classify either 3 known classes, namely &quot;Financial Report&quot;, &quot;Insurance_Sheet&quot;, &quot;Endorsement&quot;, and an unknown class namely &quot;Random PDFs&quot;.</p>
<p>For document embedding, I create pipeline [TfidfVectorizer + SVD (for dimensional reduction)] and fitting on the training set (without unknown-labeled documents, or documents of &quot;Random PDFs&quot; class) and then using that Pipeline to transform for all the documents.</p>
<p>For classification is divided into 2 stages:</p>
<ul>
<li>Stage 1: Segregate the unknown and known-labeled documents using One-Class SVM for anomaly detection. Ideally, unknown document will be binned out for manual labeling, and the known-labeled documents will be transfer to stage 2.</li>
<li>Stage 2: Training Multi-Class (3 classes) Classifier for the known-labeled documents. This works well with Linear SVC.</li>
</ul>
<p>My problem is in Stage 1,  the unknown document is not able to be detected as outlier (or un-known label), but all of the unknown documents are classified as &quot;known label&quot; by One-Class SVM. Further error analysis, I realize that my unknown documents are Shareholder Announcement, which really similar to the known label document, namely Financial Report.</p>
<p>Can you help me suggest any better design flow to handle this multi-class document classification ? Thanks for your support !</p>
","nlp"
"109447","Method for multi-label category classification","2022-03-29 08:24:23","","1","125","<nlp><multilabel-classification>","<p>I’m working on a project that involves a Natural Language Processing methodology. I want to classify categories(label) to biomedical news articles (it can be multi-label)<br />
(For example, News 1: Oncology, News 2: Cardiology, Neurology)</p>
<p>I think I can retrieve enough new articles. The number of labels would be around 50.<br />
Before assigning labels, I need to decide the method.</p>
<p>I guess there would be two options: 'Multi-label text classification' and 'NER'.</p>
<p>Below are my considerations for category classifcation.</p>
<ul>
<li>specific jargons are most important for classifying categories.</li>
<li>text classification will assess every word of each news article.</li>
<li>sequence of words and contextualization may be less important than other tasks (topic classification, sentiment analysis, etc.).</li>
<li>I have once experienced sort of classification task but new to NER. Unlke text classification, NER does not output the final result(category). NER would classify biomedical terms and I think I need to do something using these terms.</li>
</ul>
<p>Which method will be more beneficial? Or is there any better method?<br />
Thanks in advance!</p>
","nlp"
"109430","Learning to Rank with Unlabelled Dataset","2022-03-28 13:48:42","","2","229","<nlp><xgboost><ranking><search-engine><learning-to-rank>","<p>I have folder of about 60k PDF documents that I would like to learn to rank based on queries to surface the most relevant results. The goal is to surface and rank relevant documents, very much like a search engine. I understand that Learning to Rank is a supervised algorithm that requires features generated based on query-document pairs. However, the problem is that none of them are labelled. How many queries should I have to even begin training the model?</p>
","nlp"
"109391","Which is the difference between the two Greek BERT models?","2022-03-26 17:29:30","109393","0","237","<nlp><pytorch><bert>","<p>I want to use the Greek BERT which can be found here <a href=""https://huggingface.co/nlpaueb/bert-base-greek-uncased-v1"" rel=""nofollow noreferrer"">https://huggingface.co/nlpaueb/bert-base-greek-uncased-v1</a></p>
<p>However I am confused about which model should I use and which are the differences.</p>
<p>The tokenizer is the same</p>
<pre><code>tokenizer = AutoTokenizer.from_pretrained('nlpaueb/bert-base-greek-uncased-v1')
</code></pre>
<p>but we have two models</p>
<pre><code>model = AutoModel.from_pretrained(&quot;nlpaueb/bert-base-greek-uncased-v1&quot;)
model = AutoModelWithLMHead.from_pretrained(&quot;nlpaueb/bert-base-greek-uncased-v1&quot;)
</code></pre>
<p>Which one should I use?</p>
","nlp"
"109378","Natural language processing","2022-03-26 11:30:53","","1","44","<machine-learning><deep-learning><nlp><pandas><nltk>","<p>I am new to NLP. I converted my JSON file to CSV with the Jupyter notebook. I am unsure how to proceed in pre-processing my data using techniques such as tokenization and  lemmatization etc. I normalised the data before converting it to a CSV format, so now i have a data frame. Please how do I apply the tokenisation process on the whole dataset and using the <code>split()</code> function is giving me an error?</p>
<p><a href=""https://i.sstatic.net/mzksG.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/mzksG.png"" alt=""enter image description here"" /></a></p>
","nlp"
"109373","spacy multi label classification help","2022-03-26 05:41:47","110285","2","1211","<deep-learning><nlp><multiclass-classification><spacy>","<p>I would like to create a multilabel text classification algorithm using SpaCy text multi label.  I am unable to understand the following questions:</p>
<ol>
<li>How to convert the training data to SpaCy format i.e I have 8 categories</li>
<li>After converting, how do we use that to train custom categories and apply different models</li>
</ol>
","nlp"
"109341","Advice on movie per topic classification and relation with rating","2022-03-24 23:12:51","109358","1","14","<machine-learning><nlp>","<p>I would like to extract topics from a set of movie subtitles, and possibly see if there is any relation with the viewer's rating. I have thought about creating a DocumentTermMatrix where each document is one movie, and than applying LDA in order to find the topics. However, I have never classified documents, and I have no idea about how to find out if one topic is more likely to have good reviews than others. I would like to create something graphical that shows both the clusters of topics and their relation to the rating...
Any advice would be very useful!</p>
","nlp"
"109261","How do you handle the free-text fields in tabular data in ML/DL?","2022-03-22 13:33:27","109274","1","623","<nlp><text-mining>","<p>While we see a number of cases where the input data is only a single text fields (for the X variable) in NLP tasks, e.g. a tweet with a sentiment label being the only numerical field.</p>
<p>But how do you handle the free-text fields in tabular data in ML/DL? The text field(s) is/are among all the numeric fields in a table! I think this is tricky to handle. It can be comment fields or some log data in some fields along with many other numeric fields. List as many approaches as possible. Any idea?</p>
<p>For easy discussion, the 'free-text' defined here refers to a bunch of text where each row of data in the dataset can has variable length in the text.</p>
<p>And the goal of this question is to find ways to transform such text field(s) such that they can be included into ML/DL models.</p>
","nlp"
"109233","What is the meaning of two embedding layers in a row?","2022-03-21 11:53:53","109246","1","654","<deep-learning><neural-network><nlp><word-embeddings><embeddings>","<p>I've noticed in one deep pre-trained textual neural network that there are two embedding layers in the beginning and I don't quite understand why there are two of them. As far as I understand (correct me if I'm wrong, I am a newcomer in NLP) in embedding layer there is a vector of trainable weights that forms set of parameters for every unique word. So, what is the meaning of two such layers in a row? Does the second layer create sub-parameters for every parameter in the original embedding layer?</p>
","nlp"
"109214","'list' object has no attribute 'lower' TfidfVectorizer","2022-03-20 16:51:37","","0","4829","<nlp><multiclass-classification><tfidf>","<p>I have a dataframe with two text columns and I converted them to a list. I seperated the train and test data as well. But while making a base model TfidfVectorizer throws me an error of 'list' object has no attribute 'lower'</p>
<p>Here is the code</p>
<pre><code>    X['ItemDescription']= X['ItemDescription'].str.lower()
    X['DiagnosisOne'] = X['DiagnosisOne'].str.lower()
    
    from sklearn.model_selection import train_test_split
    X_train,X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42)

# Convert abstract text lines into lists 
train_items = X_train.reset_index().values.tolist()
test_items = X_test.reset_index().values.tolist()

from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
train_labels_encoded = label_encoder.fit_transform(y_train.to_numpy())
test_labels_encoded = label_encoder.transform(y_test.to_numpy())

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline

# Create a pipeline
model_0 = Pipeline([
  (&quot;tf-idf&quot;, TfidfVectorizer()),
  (&quot;clf&quot;, MultinomialNB())
])

# Fit the pipeline to the training data
model_0.fit(X=train_items, 
            y=train_labels_encoded);
</code></pre>
<p>Error</p>
<pre><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
/tmp/ipykernel_789/2404586028.py in &lt;module&gt;
     11 # Fit the pipeline to the training data
     12 model_0.fit(X=train_items, 
---&gt; 13             y=train_labels_encoded);

/opt/conda/lib/python3.7/site-packages/sklearn/pipeline.py in fit(self, X, y, **fit_params)
    388         &quot;&quot;&quot;
    389         fit_params_steps = self._check_fit_params(**fit_params)
--&gt; 390         Xt = self._fit(X, y, **fit_params_steps)
    391         with _print_elapsed_time(&quot;Pipeline&quot;, self._log_message(len(self.steps) - 1)):
    392             if self._final_estimator != &quot;passthrough&quot;:

/opt/conda/lib/python3.7/site-packages/sklearn/pipeline.py in _fit(self, X, y, **fit_params_steps)
    353                 message_clsname=&quot;Pipeline&quot;,
    354                 message=self._log_message(step_idx),
--&gt; 355                 **fit_params_steps[name],
    356             )
    357             # Replace the transformer of the step with the fitted

/opt/conda/lib/python3.7/site-packages/joblib/memory.py in __call__(self, *args, **kwargs)
    347 
    348     def __call__(self, *args, **kwargs):
--&gt; 349         return self.func(*args, **kwargs)
    350 
    351     def call_and_shelve(self, *args, **kwargs):

/opt/conda/lib/python3.7/site-packages/sklearn/pipeline.py in _fit_transform_one(transformer, X, y, weight, message_clsname, message, **fit_params)
    891     with _print_elapsed_time(message_clsname, message):
    892         if hasattr(transformer, &quot;fit_transform&quot;):
--&gt; 893             res = transformer.fit_transform(X, y, **fit_params)
    894         else:
    895             res = transformer.fit(X, y, **fit_params).transform(X)

/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py in fit_transform(self, raw_documents, y)
   2075         &quot;&quot;&quot;
   2076         self._check_params()
-&gt; 2077         X = super().fit_transform(raw_documents)
   2078         self._tfidf.fit(X)
   2079         # X is already a transformed view of raw_documents so

/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py in fit_transform(self, raw_documents, y)
   1328                     break
   1329 
-&gt; 1330         vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)
   1331 
   1332         if self.binary:

/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py in _count_vocab(self, raw_documents, fixed_vocab)
   1199         for doc in raw_documents:
   1200             feature_counter = {}
-&gt; 1201             for feature in analyze(doc):
   1202                 try:
   1203                     feature_idx = vocabulary[feature]

/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py in _analyze(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)
    111     else:
    112         if preprocessor is not None:
--&gt; 113             doc = preprocessor(doc)
    114         if tokenizer is not None:
    115             doc = tokenizer(doc)

/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py in _preprocess(doc, accent_function, lower)
     69     &quot;&quot;&quot;
     70     if lower:
---&gt; 71         doc = doc.lower()
     72     if accent_function is not None:
     73         doc = accent_function(doc)

AttributeError: 'list' object has no attribute 'lower'
</code></pre>
","nlp"
"109200","Proof that multihead works better than single head in transformer","2022-03-20 03:15:34","109205","2","137","<nlp><transformer>","<p>According to <a href=""https://datascience.stackexchange.com/questions/96345/why-multiple-attention-heads-learn-differently"">this</a> post, the purpose of the multihead is to have 'gradient splitting' across heads, which is achieved by random initialization of weight matrices for Q, K and V in each head. But how can we prove this can solve the problems in using single head?</p>
<p>Specifically, how can the splitting of gradients ensures within each output attention vector for each word it wouldn't overemphasize (the attention) of itself?</p>
","nlp"
"109197","What makes differences in each head in the multiheaded attention in transformer?","2022-03-20 02:22:02","109204","0","320","<nlp><transformer>","<p>What makes differences in each head in the multiheaded attention in transformer?</p>
<p>As they are fed and trained in the exact same way, except the initialization of weights are different for each head to produce different sets of (Q,K,V) in each head. Such multi-headed design to me seems no difference than ensembling multiple models that are initialized differently.</p>
<p>Many sources claim that the multi-head attention 'can help capture meaning in different contextual subspace' without further substantiation or supporting proofs. Honestly I've been quite fed up with all those vague descriptions in the data science world that they make claim without mathematical rigor. I think I'm looking for more rigorous explanation as to why &quot;multi-head attention 'can help capture meaning in different contextual subspace'&quot; when they are simply an ensemble of identical models but weights randomly initialized?</p>
","nlp"
"109169","The separate of K and V is redundant in transformer?","2022-03-19 04:19:27","","0","31","<nlp><transformer>","<p>imho, I think the separate of K and V is redundant in transformer, as they are basically the same regardless in encoder self-attention, or decoder self-attention, or even the encoder-decoder attention. Can anyone counter-argue with me, based on the mathematical computation where K, V, Q are done in the training, to disprove my claim?</p>
<p>I would further argue that the preservation of Q, K, V in transformer is just a gimmick to match with the status quo of information retrieval, which adds little sense and brings much confusion to the learners of transformer.</p>
","nlp"
"109106","ValueError: The first argument to `Layer.call` must always be passed. for k Fold validation","2022-03-16 14:01:38","","0","8080","<machine-learning><deep-learning><neural-network><nlp><cross-validation>","<p>Here is my model</p>
<pre><code>  model = Sequential()
  model.add(Dense(units=8000,activation='relu'))
  model.add(Dense(units=1000,activation='relu'))
  model.add(Dense(units=500,activation='relu'))
  model.add(Dense(units=100,activation='relu'))
  model.add(Dense(units=1,activation='sigmoid'))

# For a binary classification problem
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
</code></pre>
<p>Here is my K fold declaration</p>
<pre><code>from sklearn.model_selection import KFold, cross_val_score
from keras.wrappers.scikit_learn import KerasRegressor
from sklearn.model_selection import StratifiedKFold
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.pipeline import Pipeline

estimator = KerasClassifier(build_fn=model, epochs=10)
kfold = StratifiedKFold(n_splits=10, shuffle=True)
results = cross_val_score(estimator,X[0:15500], y[0:15500], cv=kfold)
print(&quot;Baseline: %.2f%% (%.2f%%)&quot; % (results.mean()*100, results.std()*100))
</code></pre>
<p>I get the below error regarding a layer.</p>
<pre><code>/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.
  
Baseline: nan% (nan%)
/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: 
10 fits failed out of a total of 10.
The score on these train-test partitions for these parameters will be set to nan.
If these failures are not expected, you can try to debug them by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File &quot;/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py&quot;, line 680, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File &quot;/usr/local/lib/python3.7/dist-packages/keras/wrappers/scikit_learn.py&quot;, line 236, in fit
    return super(KerasClassifier, self).fit(x, y, **kwargs)
  File &quot;/usr/local/lib/python3.7/dist-packages/keras/wrappers/scikit_learn.py&quot;, line 153, in fit
    **self.filter_sk_params(self.build_fn.__call__))
  File &quot;/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py&quot;, line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py&quot;, line 3095, in _split_out_first_arg
    'The first argument to `Layer.call` must always be passed.')
ValueError: The first argument to `Layer.call` must always be passed.

  warnings.warn(some_fits_failed_message, FitFailedWarning)

</code></pre>
<p>Trying to figure out what it is, tried many things but still failed. Hope someone could solve it with a good explanation.</p>
","nlp"
"109105","Can pre-trained transformers (I.e., BERT) handle numerical/spatial data","2022-03-16 13:02:47","","1","126","<python><nlp><bert><geospatial><search>","<p>I’m curious to know if pre-trained transformers could handle search queries that include numerical data or make references to spatial relationships.</p>
<p>Take an example dataset of a list of restaurants, each with a distance relative to the city centre. Would transformers be able to handle:</p>
<p><em>”Which restaurant is closest to city centre?”</em></p>
<p><em>”Which restaurant is 2km from the city centre?”</em></p>
<p>Curious to know if anyone has any opinions on this or has seen any articles/examples covering this aspect of searching.</p>
<p>Thanks!</p>
","nlp"
"109065","Training a model with a series of text responses as input","2022-03-15 12:36:28","","0","198","<classification><nlp><text-classification><sentiment-analysis>","<p>I want to train a binary classifier on text -- so something like sentiment analysis, but my input vectors are going to be a series of responses from some user separated by some separator character. I don't want to separate the responses per vector -- each vector must correspond to responses from a user. For example, an input vector could be like</p>
<pre><code>['Hi there | i'm eating breakfast u? | okay talk to you later']
</code></pre>
<p>However, I've never dealt with inputs of this form when training a model. Has this kind of problem been tackled be for? How can this be done?</p>
","nlp"
"109046","Cluster tabular data with text in some columns","2022-03-14 21:06:31","","2","223","<machine-learning><deep-learning><nlp><clustering><pytorch>","<p>Let's say I have a following features in the my dataframe:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>user_id</th>
<th>user_age</th>
<th>is_student</th>
<th>is_graduate</th>
<th>salary</th>
<th>resume</th>
</tr>
</thead>
<tbody>
<tr>
<td>integer</td>
<td>integer</td>
<td>binary</td>
<td>binary</td>
<td>integer</td>
<td>text (up to 1000 symbols)</td>
</tr>
</tbody>
</table>
</div>
<p>And also a few more categorical and numeric features.</p>
<p>I want to cluster my data based on the given data. I could've just use K-Means, however I want to include <strong>resume</strong> column in my data. <strong>Is there a way to include text column in any clustering algorithm?</strong></p>
<p>One idea I have is to get vector embeddings for each resume observation, then cluster them, and then add those cluster_id to the initial data. This way the text data will be somehow used.</p>
<p>Another idea is to use TF-IDF. Let's say I will create 100 more binary features (words from TF-IDF). This way I can run K-Means straightforwardly on the new data, but this way my dataframe increases dramatically and the clustered data might be hard to interpret.</p>
","nlp"
"109015","What are the exact differences between Word Embedding and Word Vectorization?","2022-03-13 17:20:27","109023","6","8002","<nlp><word-embeddings><word2vec><text-classification><tfidf>","<p>I am learning NLP. I have tried to figure out the exact difference between Word Embedding and Word Vectorization. However, seems like some articles use these words interchangeably. But I think there must be some sort of differences.</p>
<p>In Vectorization, I came across these vectorizers:</p>
<blockquote>
<p>CountVectorizer, HashingVectorizer, TFIDFVectorizer</p>
</blockquote>
<p>Moreover, while I was trying to understand the word embedding. I found these tools.</p>
<blockquote>
<p>Bag of words, Word2Vec</p>
</blockquote>
<p>Would you please briefly summarize the differences and the algorithms of between Word Embeddings and Word Vectorization? Thanks a lot.</p>
","nlp"
"108981","A multi label text classification problem","2022-03-11 19:32:21","109040","2","88","<nlp><multiclass-classification><text-classification><language-model>","<p>I'm looking to solve a multi label text classification problem but I don't really know how to formulate it correctly so I can look it up.. Here is my problem :</p>
<p>Say I have the document <code>&quot;I want to learn NLP. I can do that by reading NLP books or watching tutorials on the internet. That would help me find a job in NLP.&quot;</code></p>
<p>I want to classify the sentences into 3 labels (for example) <em>objective</em>, <em>method</em> and <em>result</em>. The result would be :</p>
<pre><code>objective : I want to learn NLP

method : I can do that by reading NLP books or watching tutorials on the internet.

result : That would help me find a job.
</code></pre>
<p>As you would have noticed, it's not a classical classification problem, since the classification here depends on the document structure (unless I'm wrong?)</p>
<p>Any idea of the key words to better describe the problem ? or how I might solve it ?</p>
<p>Many thanks!</p>
","nlp"
"108877","Information Extraction from News Headlines","2022-03-08 12:59:37","","1","175","<nlp><information-extraction>","<p>I'm relatively new in the field of Information Extraction and was wondering if there are any methods to summarize multiple headlines on the same topic, like some kind of &quot;average&quot; of headlines. Imagine like 20 headlines from news articles on the topic that the Los Angeles Rams won the Super Bowl such as &quot;Rams win Super Bowl&quot;, &quot;Los Angeles rallies past Bengals to win Super Bowl&quot;, ...</p>
<p>The goal would be to find one &quot;average&quot; sentence that summarizes these headlines. I already searched in Google and Google Scholar, but I don't find anything that's fit, so I'm not sure if there is actually nothing or if I just don't know the right method/keyword to serach here.</p>
<p>Thank you in advance!</p>
","nlp"
"108844","Classifier as compression","2022-03-07 15:50:25","","1","17","<classification><nlp>","<p>Someone wrote a <a href=""https://gist.github.com/huytd/6a1a6a7b34a0d0abcac00b47e3d01513"" rel=""nofollow noreferrer"">Wordle clone in bash</a>, using the full system dictionary. Unfortunately there is no smaller list of &quot;common words&quot; available locally to make use of.</p>
<p>I was wondering if a classifying algorithm/model could be used as a form of compression, to generate a list of common words at runtime in lieu of packaging a full list of common words.</p>
<p>It wouldn't need to be perfect, or even good (false negatives are absolutely fine, false positives less than ideal). Each user's (English) word list could be different, but is probably extremely similar, so I imagine overfitting is desirable. I guess the only important thing in this context is that it be small and portable, although portable POSIX shell script bit might be a tricky constraint.</p>
<p>What approach could you take?</p>
","nlp"
"108832","How to do sentence segmentation without loosing sentence's subject?","2022-03-07 10:08:00","108929","3","638","<machine-learning><python><nlp><text-mining>","<p>I have some text with different lengths, I want to split it into separate clauses but I also want to preserve the subject</p>
<p>For example;</p>
<pre><code># single subject
Original: &quot;Coffee is very good, but wasn't hot enough&quot;
split: [&quot;Coffee is very good&quot;, &quot;Coffee wasn't hot enough&quot;]

Original: &quot;Joe was the top performer of last year's dance competition, he is also a good singer&quot;
split: [&quot;Joe was the top performer of last year's dance competition&quot;, &quot;Joe is a good singer&quot;]

# multiple subjects
Original: &quot;Delicious food service, but we struggled with the app.&quot;
split: [&quot;Delicious food service&quot;, &quot;We struggled with the app&quot;]
</code></pre>
<p>I don't know how to achieve this, we can maybe split sentences based on punctuation and conjunctions (may not be accurate) but how do we preserve its subject.</p>
<p>Please let me know if you need more information.</p>
","nlp"
"108829","How to deal with ""Ergänzungsstrichen"" and ""Bindestrichen"" in German NLP?","2022-03-07 08:06:20","","1","116","<nlp><tokenization>","<h3>Problem</h3>
<p>In German, the phrase &quot;Haupt- und Nebensatz&quot; has exactly the same meaning as &quot;Hauptsatz und Nebensatz&quot;. However, when transforming both phrases using e.g. spacy's <a href=""https://spacy.io/models/de#de_core_news_sm"" rel=""nofollow noreferrer""><code>de_core_news_sm</code></a> pipeline, the cosine similarity of the resulting vectors differs significantly:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>token1</th>
<th>token2</th>
<th>similarity</th>
</tr>
</thead>
<tbody>
<tr>
<td>Haupt-</td>
<td>Hauptsatz</td>
<td>0.07</td>
</tr>
<tr>
<td>und</td>
<td>und</td>
<td>0.67</td>
</tr>
<tr>
<td>Nebensatz</td>
<td>Nebenssatz</td>
<td>0.87</td>
</tr>
</tbody>
</table>
</div>
<p>Code to reproduce</p>
<pre class=""lang-py prettyprint-override""><code>import spacy
import numpy as np


def calc_cosine_similarity(a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))


nlp = spacy.load(&quot;de_core_news_sm&quot;)
doc1 = nlp(&quot;Hauptsatz und Nebensatz&quot;)
doc2 = nlp(&quot;Haupt- und Nebensatz&quot;)
for token1, token2 in zip(doc1, doc2):
    similarity = calc_cosine_similarity(token1.vector, token2.vector)
    print(f&quot;{token1.text}: {similarity}&quot;)

</code></pre>
<h3>My research for a solution</h3>
<p>This <a href=""https://wwwmatthes.in.tum.de/file/12kcbte1ld3i9/Sebis-Public-Website/-/Bachelor-s-Thesis-Patrick-Ruoff/Bachelorarbeit_Ruoff.pdf"" rel=""nofollow noreferrer"">Bachelorthesis</a> states on page 5:</p>
<blockquote>
<p>A distinction is made between phrases with a complementary dash, as in &quot;main and subordinate clauses&quot;, and those with a hyphen, as in &quot;price-performance ratio&quot;. The former are divided into several tokens, the latter form a single one. (translated from original German)</p>
</blockquote>
<p>This sounds like a preprocessing solution is readily available? However, so far I could not find any yet on e.g. <a href=""https://github.com/adbar/German-NLP"" rel=""nofollow noreferrer"">https://github.com/adbar/German-NLP</a> , but I might have overlooked things.</p>
","nlp"
"108779","How can I get the output of a Keras LSTM layer?","2022-03-05 11:36:39","","1","1899","<neural-network><keras><nlp><lstm><embeddings>","<p>I want to get the output (that is a vector) of a LSTM layer of a network built in Python using Keras and that is trained to classify sentences (i.e. sequences). How can I do it ?</p>
<p>My attempt has been the following:</p>
<p>Is it right to use the function <code>model.predict()</code> ? I found this video <a href=""https://www.youtube.com/watch?v=CcGf_Uo7NMw"" rel=""nofollow noreferrer"">LSTM in Keras | Understanding LSTM input and output shapes</a> that explains that the input of an LSTM layer (that is just after the embedding layer) is a vector of size <code>(number of sequences, number of inputs, embedding dimension)</code> and the corresponding LSTM output has dimension<code>(number of sequences, number of LSTM units)</code>. In the linked video, it gets the latter vector (i.e. the output vector of lstm layer) by using <code>model.predict(encodedsequences_data)</code> just after the LSTM layer. For example, if I train a neural network to classify between positive and negative comments like this:</p>
<pre><code>import numpy as np
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Dense
from tensorflow.keras.layers import Dropout
from keras.layers import LSTM
from keras.layers.embeddings import Embedding

# define documents
docs = np.array(['Well done!',
        'Good work',
        'Great effort',
        'nice work',
        'Excellent!',
        'Weak',
        'Poor effort!',
        'not good',
        'poor work',
        'Could have done better.'])

# define class labels
labels = np.array([1,1,1,1,1,0,0,0,0,0])

# train the tokenizer
tokenizer = Tokenizer()
# fit the tokenizer
tokenizer.fit_on_texts(docs)
# encode the sentences
encoded_docs = tokenizer.texts_to_sequences(docs)

vocab_size=len(tokenizer.word_index)+1 

# pad documents to a max length of 4 words
max_length = 4
padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')
embedding_dim=100

# define the model
model = Sequential()
model.add(Embedding(vocab_size, embedding_dim, input_length=max_length, name='embeddings'))
model.add(LSTM(64))
output=model.predict(padded_docs)
model.add(Dropout(0.25))
model.add(Dense(64))
model.add(Dropout(0.3))
model.add(Dense(1, activation='sigmoid'))
model.summary()

# compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# fit the model
model.fit(padded_docs, labels, epochs=50, verbose=2)
</code></pre>
<p>I should consider the input of the LSTM layer as a vector of size <code>(10, 4, 100)</code>. While instead the output is a vector whose shape is given by:</p>
<pre><code>In[10]: output.shape
Out[10]: (10, 64)
</code></pre>
<p>That is in practice this <code>numpy.ndarray</code>:</p>
<pre><code>In[11]: output
Out[11]: 
array([[-7.35682389e-03, -6.29833259e-04, -2.14141682e-02,
         5.49282366e-03, -1.68905873e-02, -7.86065124e-03,
         1.14580495e-02,  1.25549696e-02, -9.16293636e-03,
         6.39621960e-03, -1.34323994e-02, -2.12187809e-03,
         8.44217744e-03,  1.45898620e-02, -1.40892563e-03,
        -3.41916122e-02, -1.31929619e-02,  9.33299214e-03,
         1.19191762e-02, -1.00926403e-03,  1.26794688e-02,
        -2.21233014e-02, -1.12434607e-02, -4.41948650e-03,
         8.63359030e-03, -1.87689364e-02,  7.90755264e-03,
        -9.07730684e-03, -7.35392375e-03, -8.41679424e-03,
         6.20685238e-03, -3.13799526e-03,  1.42355347e-02,
         3.77556833e-04, -7.31376186e-03,  4.97561414e-03,
        -1.09350188e-02, -7.71270739e-03,  1.80931657e-03,
        -4.15941747e-03,  1.03279343e-02,  1.07305320e-02,
        -5.30181499e-03,  6.01283915e-04, -3.80512699e-03,
        -1.37944380e-02, -5.06241946e-03,  3.49981769e-04,
         6.85051316e-03, -3.79729504e-03,  1.81085169e-02,
        -2.42224871e-03, -1.16188182e-02, -9.10035986e-03,
         7.19825737e-03, -8.12581368e-03,  2.17414256e-02,
         1.95931066e-02, -1.33228181e-02,  5.87116787e-03,
         3.40227783e-02,  2.07215771e-02, -4.87452417e-05,
        -2.10380089e-02],
       [-5.61810751e-03,  1.05204089e-02, -1.52371302e-02,
        -2.87347310e-03, -1.25838947e-02, -1.18132159e-02,
         6.62136683e-03,  1.97880785e-03, -6.42609270e-03,
         1.00167897e-02, -1.96973495e-02, -8.16532318e-03,
         8.49343836e-03,  1.55395102e-02, -5.61557105e-03,
        -3.35718431e-02, -1.70064531e-02, -3.50781716e-03,
         1.04237692e-02,  3.42868188e-05,  1.28740557e-02,
        -2.28579864e-02, -3.93496035e-03, -1.70960138e-03,
         1.12372153e-02, -1.32894730e-02,  1.38472551e-02,
        -1.48426415e-02, -1.18360845e-02, -8.26376118e-03,
         2.58601783e-03, -1.19493445e-02,  9.43981484e-03,
         6.56166160e-03, -4.16467572e-03,  1.13104628e-02,
        -1.37887159e-02, -7.19879614e-03, -2.71008583e-04,
        -7.01515237e-03,  1.21428408e-02,  1.30166933e-02,
        -6.22257078e-03, -7.79333059e-03,  1.75176319e-04,
        -1.44108124e-02, -7.61069916e-03,  7.94319343e-03,
        -3.57268099e-03,  5.53885289e-03,  1.59333441e-02,
        -6.24595489e-03, -1.24001894e-02, -6.18426641e-03,
        -2.75318534e-03, -9.32588615e-03,  2.57411599e-02,
         1.81322880e-02, -1.50167728e-02,  2.56175001e-04,
         2.89641470e-02,  1.89595874e-02, -5.74650709e-03,
        -1.62386764e-02],
       [-1.57758372e-03,  6.51306845e-03, -1.83870271e-02,
        -2.17917864e-03, -1.48072215e-02, -6.46742154e-03,
         2.84540933e-03,  1.15868803e-02, -8.95035919e-03,
         1.18787242e-02, -1.53372595e-02, -1.28116598e-03,
         1.38210272e-02,  1.25349807e-02,  3.48864007e-03,
        -2.82158218e-02, -1.32004097e-02,  3.89576564e-03,
         1.16172284e-02,  6.00448996e-03,  1.25120645e-02,
        -2.08042618e-02, -9.06666648e-03, -9.38181765e-03,
         5.93809411e-03, -1.40472483e-02,  1.23529136e-02,
        -6.72566192e-03, -1.32737402e-02, -7.24557228e-03,
        -1.34050148e-03, -7.91837182e-03,  8.88528675e-03,
         4.43336181e-03, -3.90838226e-03,  7.95213319e-03,
        -1.97365284e-02, -7.12051382e-03, -1.71131018e-04,
         9.33982781e-04,  1.24091003e-02,  6.72526145e-03,
        -8.91984720e-03, -6.33321749e-03, -3.09348427e-04,
        -1.35311736e-02, -2.99455877e-03,  4.07836633e-03,
        -1.31862273e-03, -2.41302908e-03,  8.97983275e-03,
         3.61930369e-03, -5.18017821e-03, -6.46935450e-03,
        -8.58186861e-04, -2.87145306e-03,  1.99557561e-02,
         2.37323977e-02, -1.33994315e-02,  8.58740974e-03,
         2.87993196e-02,  2.31237561e-02, -9.56221425e-04,
        -1.58641450e-02],
       [-7.23353820e-03,  1.03754746e-02, -1.44966058e-02,
        -5.24685532e-03, -7.83862360e-03, -1.28473695e-02,
         7.85209332e-03,  4.59963316e-03, -6.33948809e-03,
         1.19380560e-02, -2.20133327e-02, -1.17637683e-02,
         4.68229316e-03,  1.56141464e-02, -2.19842512e-03,
        -3.82021852e-02, -1.75803266e-02, -1.37230149e-03,
         9.44487844e-03, -3.54365772e-03,  1.21249473e-02,
        -2.55316924e-02, -9.54593590e-04, -4.05333284e-03,
         1.15399351e-02, -7.02163018e-03,  1.45128630e-02,
        -1.76015086e-02, -1.56135382e-02, -6.89028949e-03,
         2.78898864e-03, -6.99541951e-03,  1.22199748e-02,
         7.81757478e-03, -3.65910749e-03,  9.27608926e-03,
        -1.18314605e-02, -7.95996375e-03, -5.45767276e-03,
         2.41609639e-03,  1.30290538e-02,  1.31718945e-02,
        -9.51540284e-03, -9.02444124e-03, -4.85872338e-03,
        -1.42892599e-02, -5.70658036e-03,  5.16991317e-03,
         1.60913187e-04,  5.72681241e-03,  1.93462688e-02,
        -7.89363962e-03, -1.39182042e-02, -1.19014597e-02,
        -3.61843006e-04, -9.27691348e-03,  2.38749031e-02,
         2.55510695e-02, -1.21669313e-02,  2.98210932e-03,
         3.11383940e-02,  2.01738160e-02, -1.07578654e-03,
        -1.72522105e-02],
       [-1.20945638e-02,  8.68875161e-03, -2.62735188e-02,
         1.12946620e-02, -1.22309905e-02, -5.58001362e-03,
         1.08549614e-02,  8.95095989e-03, -7.41497055e-03,
         1.16792703e-02, -2.28939448e-02, -3.50253959e-03,
         9.97900032e-03,  1.66020077e-02, -7.89092761e-03,
        -4.07132506e-02, -1.78568307e-02,  4.34662355e-03,
         1.21228509e-02,  3.13125411e-03,  1.45842955e-02,
        -2.05406677e-02, -8.31084419e-03, -4.01034905e-03,
         7.12802447e-03, -1.84220411e-02,  1.13592790e-02,
        -1.26814209e-02, -1.18051590e-02, -8.80334340e-03,
         3.80245410e-03, -8.09166487e-03,  1.76429395e-02,
        -1.92356238e-04, -5.44061745e-03,  1.36529943e-02,
        -1.77526288e-02, -7.82714784e-03,  1.20329263e-03,
        -4.59810719e-03,  1.16233192e-02,  1.27270408e-02,
        -7.62180379e-03, -9.63985734e-03, -7.18248449e-03,
        -2.18094457e-02, -1.00004785e-02, -1.08398555e-03,
         3.66409164e-04, -2.32298975e-03,  1.85762774e-02,
        -3.05683468e-03, -1.08189192e-02, -8.16119835e-03,
         2.15096259e-03, -1.27328513e-02,  3.22906636e-02,
         2.98772007e-02, -1.85494740e-02, -6.53657946e-04,
         3.88556197e-02,  2.41267327e-02, -4.84270183e-03,
        -2.17875950e-02],
       [-5.25501464e-03,  8.72639474e-03, -2.05799732e-02,
         6.14312338e-03, -1.72808673e-02, -1.31866131e-02,
         5.83314849e-03,  6.02243049e-03, -3.84115218e-03,
         1.41651463e-02, -2.30942499e-02, -5.55078313e-03,
         1.04255294e-02,  2.30545755e-02, -2.18844530e-03,
        -3.62902768e-02, -1.92042850e-02,  1.22342422e-03,
         1.13154203e-02, -2.17898004e-03,  1.37291076e-02,
        -2.04059239e-02, -8.93826876e-03, -3.92630836e-03,
         8.99609085e-03, -1.99639872e-02,  9.40152071e-03,
        -1.68196503e-02, -1.08547490e-02, -4.02889075e-03,
         1.14458892e-02, -9.00383666e-03,  1.34997619e-02,
         3.90134053e-03, -6.80747256e-03,  9.08577070e-03,
        -1.72130466e-02, -1.32173812e-02, -1.73806830e-03,
        -4.78662038e-03,  1.92851052e-02,  9.79739800e-03,
        -4.36533149e-03, -8.25465377e-03, -3.25881690e-03,
        -1.42115178e-02, -3.79414624e-03,  3.12958076e-03,
         2.55106570e-04,  1.77397695e-03,  2.18093134e-02,
        -1.31484016e-03, -1.34956567e-02, -5.44582447e-03,
         2.86075217e-03, -2.14463435e-02,  3.33723240e-02,
         2.67816409e-02, -1.40035218e-02,  4.13230434e-03,
         3.57808806e-02,  2.61333492e-02, -8.73044250e-04,
        -2.66207643e-02],
       [-7.28952140e-03,  3.69984098e-03, -1.50594991e-02,
        -2.92313565e-03, -1.21962698e-02, -1.36810802e-02,
         6.82729529e-03,  6.73223054e-03, -5.93179138e-04,
         1.36305839e-02, -1.97144989e-02, -3.86482291e-03,
         1.80094223e-02,  1.42702283e-02, -1.32999849e-03,
        -3.40334214e-02, -1.76202524e-02,  2.30349993e-04,
         1.09610325e-02,  5.54277329e-03,  7.91644678e-03,
        -2.16093995e-02, -1.39994686e-02, -3.96339269e-03,
         8.55575502e-03, -9.74893570e-03,  1.05637731e-02,
        -4.55878396e-03, -1.34231234e-02, -5.41363843e-04,
         4.89153492e-04, -6.66437577e-03,  1.07367048e-02,
         3.43973073e-03, -4.18765657e-03,  6.89268531e-03,
        -1.19383521e-02, -8.89711361e-03, -4.44964785e-03,
         5.09598665e-03,  1.03713274e-02,  8.53536651e-03,
        -8.85974988e-03, -5.51856030e-03, -6.80169091e-04,
        -1.59089398e-02,  4.49734647e-03,  5.99729270e-03,
         2.46776640e-03, -1.70147279e-03,  5.19089587e-03,
         1.00052624e-03, -5.67172188e-03, -3.38913826e-03,
         2.77179107e-03, -6.60816021e-03,  2.12577283e-02,
         2.13659275e-02, -1.32223312e-02,  2.25704932e-03,
         2.57163309e-02,  2.04398781e-02, -1.69396808e-04,
        -1.73870064e-02],
       [-3.72720137e-03,  1.10780252e-02, -1.58859324e-02,
         6.66555809e-03, -1.44280717e-02, -8.00609123e-03,
         6.45924266e-03,  8.54926067e-04, -1.88849677e-04,
         8.98782630e-03, -1.54129220e-02, -1.08604110e-03,
         1.07514234e-02,  1.73932631e-02, -1.32540641e-02,
        -3.20286751e-02, -1.51650719e-02, -4.61112056e-03,
         1.35826627e-02,  2.19380498e-04,  1.40464576e-02,
        -1.90667752e-02, -4.66079684e-03, -3.83105595e-03,
         7.03095598e-03, -1.30360266e-02,  1.06415655e-02,
        -9.59097221e-03, -9.75623727e-03, -7.97409564e-03,
         5.58470841e-03, -9.38377157e-03,  7.96551071e-03,
        -4.83714510e-04, -2.05025147e-03,  8.32131505e-03,
        -1.56590100e-02, -1.04131605e-02, -3.17667797e-03,
        -5.72070433e-03,  1.51903769e-02,  1.19651994e-02,
        -4.08067275e-03, -1.01910857e-02,  3.11617507e-03,
        -1.46057652e-02, -4.41839360e-03,  5.06953197e-03,
        -4.96832095e-03,  7.81613402e-03,  1.70495212e-02,
         2.49758250e-05, -7.83495232e-03, -4.50461730e-03,
        -2.93536019e-03, -1.30293816e-02,  2.36500446e-02,
         1.58338025e-02, -1.86773948e-02,  7.61539035e-04,
         2.71951333e-02,  1.39996661e-02, -5.14069805e-03,
        -1.78940073e-02],
       [-6.54364191e-03,  5.09772869e-03, -1.04034524e-02,
        -3.73475719e-03, -1.19625181e-02, -1.40891932e-02,
         9.06457752e-03,  3.41503625e-03, -4.86045564e-03,
         1.25511140e-02, -2.17867326e-02, -7.47563411e-03,
         1.25360051e-02,  1.62971858e-02, -3.65820760e-03,
        -3.51158790e-02, -1.73445251e-02, -2.78897071e-03,
         6.79790135e-03,  3.37651756e-04,  1.03165153e-02,
        -2.39069872e-02, -7.28563499e-03, -7.42059347e-05,
         1.29556786e-02, -1.14473784e-02,  1.45603633e-02,
        -1.20491283e-02, -1.42659768e-02, -3.70843848e-03,
         4.51912638e-03, -7.15361023e-03,  1.15718255e-02,
         6.00760849e-03, -6.92916662e-03,  7.49018928e-03,
        -1.01785287e-02, -8.63973703e-03, -4.90589021e-03,
        -4.61561285e-04,  1.11296903e-02,  9.36738681e-03,
        -7.41511211e-03, -6.98906882e-03, -3.12256836e-03,
        -1.77129637e-02, -7.80898728e-04,  9.68260039e-03,
         3.10223433e-03,  4.86978563e-03,  1.11100767e-02,
        -9.04789940e-03, -1.10857347e-02, -6.04295917e-03,
         3.34106589e-04, -8.40245467e-03,  2.65593827e-02,
         1.84700415e-02, -1.15374802e-02, -1.12538924e-03,
         2.62181889e-02,  1.91304646e-02, -2.57981522e-03,
        -1.63868871e-02],
       [ 8.44749855e-04, -1.66689996e-02, -8.96400423e-04,
         6.72107562e-03, -2.39076628e-03,  2.60190992e-03,
         9.26916581e-03,  8.82431399e-03, -7.25202635e-03,
        -1.06594963e-02, -5.55762183e-03, -1.29527331e-03,
        -1.64227630e-03,  2.20268476e-03,  1.16184941e-02,
        -1.13204587e-02,  5.73474122e-03,  1.36803361e-02,
         5.84689900e-03,  9.08445287e-03, -1.40777905e-03,
        -1.06005892e-02, -3.84641928e-03,  2.70416541e-03,
         4.00838861e-03,  2.82439473e-03,  2.29762960e-03,
        -1.00108888e-03, -4.37536306e-04, -5.20851184e-03,
         4.79862979e-03, -4.48886072e-03,  8.50347336e-03,
        -1.42790927e-02, -1.26732215e-02,  4.63177776e-03,
        -1.20126840e-03,  5.99153014e-03,  1.22683365e-02,
         3.37655889e-04, -1.39692798e-02,  3.31070763e-03,
        -5.39805554e-03,  2.78319512e-02, -6.16331352e-03,
         5.77836717e-03, -1.19619851e-03, -1.58095714e-02,
         3.77729093e-03,  1.85370538e-03,  6.35961816e-03,
         3.49745946e-03, -1.27604958e-02, -9.54155251e-03,
         1.23564843e-02, -7.57912593e-03, -3.30073433e-03,
        -7.29874987e-03, -6.47724420e-03, -1.41964471e-02,
         1.36867436e-02,  5.99695602e-03,  9.53863724e-04,
         5.32696443e-03]], dtype=float32)
</code></pre>
<p>Does this (multidimensional) vector represent the output of the LSTM layer and does each inner vector can be chosen as vector representation of the corresponding sentence (i.e. vector1 -&gt; sentence1 and so on) ?</p>
<p>Based on the youtube video that I linked, it seems right and also I find other lessons that seem to do the same (such as in this blog <a href=""https://medium.com/deep-learning-with-keras/lstm-understanding-output-types-e93d2fb57c77"" rel=""nofollow noreferrer"">LSTM: Understanding Output Types</a>). But I am not really convinced. My doubt is why does the function <code>model.predict()</code> give the output of the LSTM layer ? Python <code>help</code> tells that this function:</p>
<blockquote>
<p>Generates output predictions for the input samples.</p>
</blockquote>
<p>Or also in <a href=""https://scipy-lectures.org/packages/scikit-learn/index.html"" rel=""nofollow noreferrer"">3.6. scikit-learn: machine learning in Python</a>:</p>
<blockquote>
<p><strong>In supervised estimators</strong>: <code>model.predict()</code> : given a trained model, predict the label of a new set of data. This method accepts one argument, the new data <code>X_new</code> (e.g. <code>model.predict(X_new)</code>), and returns the learned label for each object in the array.</p>
</blockquote>
<p>So, given these explanations, why should <code>model.predict()</code> give the LSTM output vector ? What does <code>model.predict()</code> exactly do ?</p>
<p>To sum up, I would like to know if the procedure that I reported using <code>model.predict()</code> is right to get the output of the LSTM layer in Keras and why. Furthermore, in case it is not, if you can suggest the right procedure.</p>
<p>Thanks a lot in advance.</p>
<p>----<strong>EDIT</strong>----:
an other approach is using the explanation in <a href=""https://keras.io/getting_started/faq/#how-can-i-obtain-the-output-of-an-intermediate-layer-feature-extraction"" rel=""nofollow noreferrer"">https://keras.io/getting_started/faq/#how-can-i-obtain-the-output-of-an-intermediate-layer-feature-extraction</a></p>
<pre><code>intermediate_layer_model = keras.Model(inputs=model.input,
                                       outputs=model.get_layer(layer_name).output)
intermediate_output = intermediate_layer_model(data)
</code></pre>
<p>with <em>layer_name = the name of my lstm layer</em> and <em>data = padded_docs</em>. I think that this procedure is correct for sure since it is in Keras documentation. And I also think that it is similar to the first approach with <code>model.predict(padded_docs)</code>. In the same Keras linked page, there is the difference between using just <code>y=model(x)</code> and <code>y=model.predict(x)</code> and it says that they both mean &quot;run the model on x and retrieve the output y.&quot;</p>
<p>So, in conclusion, maybe they both run the input data (in this case our sentences) through the model or <em>through the model untill the layer of interest</em> and retrieve the output processed data by the model that has parameters (weights) that are updated after the training ? Does this mean to get the output of a layer (that in this case is a LSTM layer)?</p>
","nlp"
"108740","BERT - The purpose of summing token embedding, positional embedding and segment embedding","2022-03-04 02:42:07","108752","3","2719","<nlp><bert>","<p>I read the implementation of BERT inputs processing (image below). My question is why the author chose to sum up three types of embedding (token embedding, positional embedding and segment embedding)?
<a href=""https://i.sstatic.net/NdeWj.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/NdeWj.png"" alt=""enter image description here"" /></a></p>
","nlp"
"108736","Naive Bayes as a baseline model in an NLP task","2022-03-03 21:49:18","108760","1","168","<python><nlp><naive-bayes-classifier>","<p>I want to use the Naive Bayes model as a baseline in an classification task that I am working. I found this really useful tutorial: <a href=""https://www.geeksforgeeks.org/applying-multinomial-naive-bayes-to-nlp-problems/"" rel=""nofollow noreferrer"">https://www.geeksforgeeks.org/applying-multinomial-naive-bayes-to-nlp-problems/</a> and I want to apply it into my problem.</p>
<p>My dataset has a dataframe form with rows the texts and coloums the labels, original text, preprocessed text, etc.</p>
<p>The code that I have is this one</p>
<pre><code># cleaning texts
import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from sklearn.feature_extraction.text import CountVectorizer
 
nltk.download('stopwords')
 
corpus = []
for i in range(0, len(df)):
    text = df['Preprocessed_Text'][i]
    text = ''.join(text)
    corpus.append(text)
</code></pre>
<pre><code># creating bag of words model
cv = CountVectorizer(max_features = 1500)
 
X = cv.fit_transform(corpus).toarray()
y = df.iloc[:, 5].values
</code></pre>
<pre><code># splitting the data set into training set and test set
from sklearn.cross_validation import train_test_split
 
X_train, X_test, y_train, y_test = train_test_split(
           X, y, test_size = 0.25, random_state = 0)
</code></pre>
<pre><code>from sklearn import naive_bayes
# fitting naive bayes to the training set
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import confusion_matrix
 
#classifier = GaussianNB()
classifier = MultinomialNB()
classifier.fit(X_train, y_train)
 
classifier.score(X_test, y_test)
</code></pre>
<p><strong>I have the following two questions:</strong></p>
<ol>
<li><p>In the pre-processed part the is it correct this one text = ''.join(text)? Or a space (text = ' '.join(text)) is needed?</p>
</li>
<li><p>By this approach we apply first the bag-of-words in the whole dataset and then split it into training and testing. Is there any way to do this in the opposite way? Therefore, first splitting the texts into training and testing and then apply bag of words.</p>
</li>
</ol>
<p><strong>Update:</strong>
I have already split the texts into training validation and testing in a listing form, thus</p>
<pre><code># evaluation sets
texts_train
auhors_train

texts_valid
authors_valid

texts_test
authors_test
</code></pre>
<p>Then I'm applying the Bag of Words method only in the training and transform the other evaluation sets with the following way</p>
<pre><code># creating bag of words model
from sklearn.feature_extraction.text import CountVectorizer

cv = CountVectorizer(max_features = 1500)
 
cv.fit_transform(texts_train).toarray()

bow_train = cv.transform(texts_train)
bow_vald = cv.transform(texts_valid)
bow_test = cv.transform(texts_test)
</code></pre>
<p>Last step is to fit a classifier and measure its accuracy</p>
<pre><code># Fitting naive bayes to the training set
from sklearn import naive_bayes
from sklearn.naive_bayes import MultinomialNB

classifier = MultinomialNB()
classifier.fit(bow_train, authors_train)
 
classifier.score(bow_vald, authors_valid)
</code></pre>
","nlp"
"108708","Latent space vs Embedding space | Are they same?","2022-03-03 08:11:57","108808","1","261","<nlp><word-embeddings><autoencoder><embeddings>","<p>I am going through <a href=""https://medium.com/towards-data-science/understanding-generative-adversarial-networks-gans-cd6e4651a29"" rel=""nofollow noreferrer"">variational autoencoders</a> and it is mentioned that:</p>
<blockquote>
<p>continuity (two close points in the latent space should not give two
completely different contents once decoded) and completeness (for a
chosen distribution, a point sampled from the latent space should give
“meaningful” content once decoded).</p>
</blockquote>
<p>so is latent space merely an embedding space where two similar entities are mapped nearby in the vector?</p>
","nlp"
"108685","Best way to vectorise names and addresses for similarity searching?","2022-03-02 13:39:44","","7","3450","<nlp><word-embeddings><k-nn><search-engine><elastic-search>","<p>I have a large dataset of around 9 million people with names and addresses. Given quirks of the process used to get the data it is highly likely that a person is in the dataset more than once, with subtle differences between each record. I want to identify a person and their 'similar' personas with some sort of confidence metric for the alternative records identified.</p>
<p>My inital thoughts on an approach is to vectorise each name and address as a concatenated string using word embeddings, load them all into Elasticsearch and then use the KNN search funcionality to 'cluster' similar records and use the Euclidean distance between each point in the cluster as a similarity metric.</p>
<p>Now I think about this, I don't think it would work as word embeddings pick up on semantic relationship and names and addresses by definition are semantically neutral. There are other vectorising approaches like bag-of-words, n-grams and TF-IDF, but these will produce lots of high dimensional sparse vectors that won't work well with KNN and Elasticsearch uses TF-IDF to search out of the box so why mess about with vectors at all?</p>
<p>My questions are:</p>
<ol>
<li>Does this approach sound overly engineered?</li>
<li>If not, are there vectorising approaches that would better (such as hashing)?</li>
<li>If yes to the above, am I at least on the right lines for a valid approach?</li>
</ol>
<p>This is more of a sound board post, but any opinions would be really helpful. Thanks!</p>
","nlp"
"108595","Using KerasClassifier for training neural network","2022-02-27 20:39:43","","0","1783","<machine-learning><keras><nlp><bert><transformer>","<p>I created a simple neural network for binary spam/ham text classification using pretrained BERT transformer. The current pure-keras implementation works fine. I wanted however to plot certain metrics of the trained model, in particular the ROC curve. According to <a href=""https://www.dlology.com/blog/simple-guide-on-how-to-generate-roc-plot-for-keras-classifier/"" rel=""nofollow noreferrer"">this blog post</a> I understood that this is only be possible using <code>KerasClassifier()</code> from the <code>keras.wrappers.scikit-learn</code> package which is now deprecated and has been replaced by the <code>scikeras</code> package.</p>
<p>Thus I created a <code>build_keras_nn()</code> function to build my custom BERT-based neural network. I then passed this custom function to <code>KerasClassifier()</code> as shown <a href=""https://www.adriangb.com/scikeras/stable/quickstart.html"" rel=""nofollow noreferrer"">in the documentation</a>, and fitted the model using train data.</p>
<p>At this point I got the following error message:</p>
<pre><code>ValueError: Expected 2D array, got 1D array instead: Reshape your data either using array.reshape(-1, 1) 
if your data has a single feature or array.reshape(1, -1) if it contains a single sample.
</code></pre>
<p>Alright then, I thought a simple array reshape will work but I then ran into the following error:</p>
<pre><code>ValueError: could not convert string to float: 'awful bio obviously hatchet job press release totally 
biased bad grammar'
</code></pre>
<p>So for some reason the <code>KerasClassifier</code> implementation doesn't allow me to directly input text, even though my preprocessing steps are included within the custom function <code>build_keras_nn()</code>.</p>
<p>A full reproducible code is below:</p>
<pre><code>import tensorflow_hub as hub 
import tensorflow_text as text
import tensorflow as tf 
from tensorflow.keras.layers import Input, Dropout, Dense
from tensorflow.keras.metrics import BinaryAccuracy, AUC

bert_encoder_url = &quot;https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4&quot;
bert_preprocessor_url = &quot;https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3&quot;

bert_preprocessor_model = hub.KerasLayer(bert_preprocessor_url)
bert_encoder_model = hub.KerasLayer(bert_encoder_url)

df_ = pd.read_json(spam_ham_json)   # spam_ham_json: data in JSON file as a string

X_train_, X_test_, y_train_, y_test_ = train_test_split(df_['comment_text'], df_['label'])

def build_keras_nn():

    text_input = Input(shape=(), dtype=tf.string, name=&quot;text&quot;)
    preprocessed_text = bert_preprocessor_model(text_input)
    bert_output = bert_encoder_model(preprocessed_text)
    dropout = Dropout(0.1, name='dropout')(bert_output['pooled_output'])
    classification_output = Dense(1, activation='sigmoid', name='classification_output')(dropout)

    model = tf.keras.Model(inputs=[text_input], outputs=[classification_output])

    metrics_list = [AUC(name='auc'), BinaryAccuracy(name='accuracy')]
    model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = metrics_list)
    return model 

# Following two rows show the pure-keras implementation: this one works.
# model = build_keras_nn()
# history = model.fit(X_train_, y_train_, epochs=5);

# Now let's see the KerasClassifier
model = KerasClassifier(build_fn=build_keras_nn)
# history = model.fit(X_train_, y_train_, epochs=5);     # &lt;-- Value Error 1
# history = model.fit(np.array(X_train_).reshape(-1,1), np.array(y_train_).reshape(-1,1), epochs=5);     # &lt;-- Value Error 2

</code></pre>
<p>Data is in json format:</p>
<pre><code>'{&quot;comment_text&quot;:{&quot;0&quot;:&quot;problem wanted say problem trying redirect event schedule pakistan NUMBERTAG NUMBERTAG pakistan mother fucker boy want married sister ohhhh love sister boob hmmmmm yummyy&quot;,&quot;1&quot;:&quot;get life fucking loser question ask ask katie goulet picture&quot;,&quot;2&quot;:&quot;cum drinker hey wat nigga thought u could ban took long cuz wa busy az hell recently ill keep cumming back take word cumdrinker&quot;,&quot;3&quot;:&quot;liar liar pant fire seriously looked contribution tennis portal page tennis page ha descussion ever please lie NUMBERTAG NUMBERTAG NUMBERTAG NUMBERTAG&quot;,&quot;4&quot;:&quot;stop writing p nothing discus given lack bsinc education diplomacy&quot;,&quot;5&quot;:&quot;wa fucking page one edit page&quot;,&quot;6&quot;:&quot;question mad gay&quot;,&quot;7&quot;:&quot;warning page nerd please leave one stay girl though pleeeeeeeeeeeeeaaaaaaaaaaaaaaaaaaaaaassssssssssssssssseeeeeeeeeeeee NUMBERTAG oneoneoneoneoneoneoneoneoneoneoenone&quot;,&quot;8&quot;:&quot;full shit&quot;,&quot;9&quot;:&quot;go fuck conrad black cheated thousand people pension anyone defends hm asshole apologist evil&quot;,&quot;10&quot;:&quot;list office bearer national union student australia wp userfy userfied page located&quot;,&quot;11&quot;:&quot;talk history scottish national party claim spying hi sentence someone belief npov claim mean someone belief npov claim&quot;,&quot;12&quot;:&quot;section meant vice review btw magazine website writer name attached also like richardwilson NUMBERTAG even know question ninjarobotpirate wa responding happy criticise answer \\u2026 \\u2026 btw NUMBERTAG far know none editor either albanian croatian maybe airplane vision quite good think take care&quot;,&quot;13&quot;:&quot;next time subtweet&quot;,&quot;14&quot;:&quot;physicsyo yo yo dog&quot;,&quot;15&quot;:&quot;self censorship tv show might might notable tv pre empted breaking news notable happens time&quot;,&quot;16&quot;:&quot;article contains information soursed huddersfield aa street street&quot;,&quot;17&quot;:&quot;utc onto something centrifugal force experienced mass exhibiting inertia result tiny little bullet hitting side ride merry go round rueda puthoff haisch described zero point field electronic lorenz equation coupling inertial frame reference give mass inertial reluctance rather resistance enable describe change velocity direction compare ac v dc tesla v edison NUMBERTAG NUMBERTAG NUMBERTAG june NUMBERTAG&quot;,&quot;18&quot;:&quot;meant wa meant state either unblock create new account rendering block useless simple&quot;,&quot;19&quot;:&quot;NUMBERTAG utc hi NUMBERTAG must mistakenly thought ian wa original member b c always viewed band definitive axeman NUMBERTAG yeah almost bought akai headrush looper year ago notorious role cab one guitarist recording settled bos loop station instead rather headrush boomerang due two reliability price issue respectively check hovercraft southpacific auburn lull kind hallucinitory guitar looping thought cab new lineup wa incredible saw NUMBERTAG skipped classic lineup NUMBERTAG compare two performance wise best NUMBERTAG NUMBERTAG NUMBERTAG may&quot;},&quot;label&quot;:{&quot;0&quot;:1,&quot;1&quot;:1,&quot;2&quot;:1,&quot;3&quot;:1,&quot;4&quot;:1,&quot;5&quot;:1,&quot;6&quot;:1,&quot;7&quot;:1,&quot;8&quot;:1,&quot;9&quot;:1,&quot;10&quot;:0,&quot;11&quot;:0,&quot;12&quot;:0,&quot;13&quot;:0,&quot;14&quot;:0,&quot;15&quot;:0,&quot;16&quot;:0,&quot;17&quot;:0,&quot;18&quot;:0,&quot;19&quot;:0}}'
</code></pre>
","nlp"
"108563","Compare Books using book categories list NLP","2022-02-26 12:48:28","","3","264","<machine-learning><python><nlp><nltk><spacy>","<p>I have a database of books. Each book has a list of categories that describe the genre/topics of the book (I use Python models).</p>
<p>The categories in the list most of the time are composed of 1 to 3 words.</p>
<p>Examples of a book category list:</p>
<pre><code>['Children', 'Flour mills', 'Jealousy', 'Nannies', 'Child labor', 'Conduct of life'],
[&quot;Children's stories&quot;, 'Christian life'],
['Children', 'Brothers and sisters', 'Conduct of life', 'Cheerfulness', 'Christian life'],
['Fugitive slaves', 'African Americans', 'Slavery', 'Plantation life', 'Slaves', 'Christian life', 'Cruelty']
</code></pre>
<p>I want to create/use an algorithm to compare the books and find similarity between 2 books using NLP/machine learning models.</p>
<p>The categories are not well defined and tend to change. For example there could be a category <code>'story'</code> and another called <code>'stories'</code> since the text in the system doesn't use saved categories but an open text box.
So far I tried 2 algorithms:</p>
<ul>
<li>cossine similiarity with WordNet - split the category to get a bag of words and check if each word has a synonym in the other book lists.</li>
<li>Check the similarity using the <code>nlp</code> model of the spacy library (Python) - distance algorithm.</li>
</ul>
<p>So far I used WordNet model from the <code>nltk</code> package and <code>spacy</code>.
I had problems with those two algorithms because when the algorithm compares a category that contains 2 or 3 words the results wasn't accurate and each of them had specific problems.</p>
<p>Which algorithm/Python models that can handle strings containing 2 or 3 words can I use to compare the books?</p>
<p>B.t.w this is the first time I ask here. If you need more details about the database or what I did so far please tell me.</p>
","nlp"
"108472","What happens when the length of input is shorter than length of output in transformer architecture?","2022-02-23 17:36:35","","1","652","<nlp><transformer><sequence-to-sequence>","<p>Given standard transformer architecture with encoder and decoder.</p>
<p>What happens when the input for the encoder is shorter than the expected output from the decoder?</p>
<p>The decoder is expecting to receive <strong>value</strong> and <strong>key</strong> tensors from the encoder which size is dependent on the amount of input token.</p>
<p>I could solve this problem during training by padding input and outputs to the same size.</p>
<p>But how about inference, when I don't know the size of the output?</p>
<p>Should I make a prediction and if the decoder doesn't output the <code>stop</code> token within range of available size, re-encode inputs with more padding and try again?</p>
<p>What are the common approaches to this problem?
Thanks in advance, have a great day :)</p>
","nlp"
"108349","How to have a fixed no of features for input layer of a neural network when using TF-IDF","2022-02-20 14:52:18","","0","370","<machine-learning><deep-learning><neural-network><nlp><tfidf>","<p>So basically my question is hypothetically lets say:</p>
<p>I have a column containing 2000 rows of texts, and when I apply tf-idf, I get 27 features like shown below.</p>
<p><a href=""https://i.sstatic.net/d4655.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/d4655.png"" alt=""enter image description here"" /></a></p>
<p>Now once I do that, I could consider my Neural Network's Input layer's number of neurons to be 27, like shown below, and i train the model with the tf-idf features.</p>
<p><a href=""https://i.sstatic.net/HIMGQ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/HIMGQ.png"" alt=""enter image description here"" /></a></p>
<p>Now, hypothetically speaking, if I'm trying to test this model with one string (a short string), and when we apply Tf-Idf to that string text, we get 20 features. Now this number of features does not equal to the no of neurons in my input layer, which is 27, which can cause problems.</p>
<p>How can we tackle a problem like this, I have seen we can use max no of features with Tf-Idf, but i thought it would be good to ask the community as well, so that you could show me a better way.</p>
<p>Is there a way we could have a fixed length of features when applying Tf-Idf so that there wont be a problem when feeding to the neural network.</p>
<p>Your help will be appreciated!</p>
","nlp"
"108336","Start & End Tokens in LSTM when making predictions","2022-02-20 05:07:50","","2","1689","<nlp><tensorflow><lstm><rnn>","<p>I see examples of LSTM sequence to sequence generation models which use start and end tokens for each sequence.</p>
<p>I would like to understand when making predictions with this model, if I'd like to make predictions on an arbitrary sequence -  is it required to include start and end tokens tokens in it?</p>
","nlp"
"108263","Does word2vec fail for window size equal to sentence size","2022-02-17 23:19:16","","1","420","<nlp><word-embeddings><word2vec>","<p>Will word2vec fail if sentences contain only similar words, or in other words, if the window size is equal to the sentence size? I suppose this question boils down to whether word to vec considers words from other sentences as negative samples, or only words from the same sentence but outside of the window</p>
","nlp"
"108261","How do companies handle changing natural language","2022-02-17 21:13:18","","0","17","<nlp><word-embeddings>","<p>I am assuming large social medias like Twitter handle hashtags using some sort of embedding, so that similar tweets can be found or suggested. Maybe this is a bad assumption- maybe someone can clarify.
However, the question I have is how they handle new vocabulary being added? For example, whenever a new hashtag becomes trending, it is likely or at least possible that that exact string had not been included in the vocabulary of the embedding before then. Since embedding vocabulary cannot be changed after the fact, is it possible that they simply retrain their model every few hours? That seems to be intractable.</p>
","nlp"
"108251","My custom stop-words list using tf-idf","2022-02-17 11:36:22","108253","3","1770","<nlp><tfidf>","<p>I want to make my own stop words list, I computed tf-idf scores for my terms.</p>
<p>Can I consider those words highlighted with red to be stop word? and what should my threshold be for stop words that depend on tf-idf? Should I consider the high values of tf-idf as the most important words that I need to keep?</p>
<p><a href=""https://i.sstatic.net/YY0XT.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/YY0XT.png"" alt=""enter image description here"" /></a></p>
<p>@Erwan answered this question, check their answer to the question they linked too it is very informative</p>
","nlp"
"108202","WMT: What are the differences of WMT14, WMT15 and WMT16 datasets?","2022-02-16 07:48:21","108222","0","399","<nlp><dataset><machine-translation>","<p>Each year, the Workshop on Statistical Machine Translation (WMT) holds a conference that focuses on new tasks, papers, and findings in the field of machine translation.</p>
<p>Let's say we are talking about the parallel dataset Newscommentary. There is the Newscommentary in WMT14, WMT15, WMT16 and so on.</p>
<p>How much does the dataset differ from each conference? Is it possible to read this somewhere?</p>
","nlp"
"108178","How to prepare texts to BERT/RoBERTa models?","2022-02-15 12:15:41","","1","1184","<deep-learning><nlp><bert><transformer><huggingface>","<p>I have an artificial corpus I've built (not a real language) where each document is composed of multiple sentences which again aren't really natural language sentences.</p>
<p>I want to train a language model out of this corpus (to use it later for downstream tasks like classification or clustering with sentence BERT)</p>
<p><strong>How to tokenize the documents?</strong></p>
<p>Do I need to tokenize the input</p>
<p>like this:
<code>&lt;s&gt;sentence1&lt;/s&gt;&lt;s&gt;sentence2&lt;/s&gt;</code></p>
<p>or <code>&lt;s&gt;the whole document&lt;/s&gt;</code></p>
<p><strong>How to train?</strong></p>
<p>Do I need to train an MLM or an NSP or both?</p>
","nlp"
"108144","Word Embedding Dimensions Reduction","2022-02-14 14:32:44","108161","0","969","<nlp><word-embeddings>","<p>In my NLP task, I use Glove to get each word embedding, Glove gives 50 float numbers as an embedding for every word in my sentence, my corpus is large, and the resulted model is also large to fit my machine, I'm looking for a way to reduce each word embedding from 50 numbers to something less, maybe one floating number is possible, is there an approach to do that?</p>
<p>I was thinking about an aggregation approach,like taking the average of the 50 numbers, would that work?</p>
","nlp"
"108138","Why is the accuracy of multi-labels classification using the sigmoid"" activation function is much more than using ""softmax?","2022-02-14 08:40:08","","0","818","<machine-learning><deep-learning><classification><nlp>","<p>I do multi-label text classification using Bi-LSTM classifier, that means there are instances in the dataset of 11 classes that have more than 1 label. When I use the &quot;sigmoid&quot; activation function with &quot;binary_crossentropy&quot; loss function, I get higher accuracy than using &quot;softmax&quot; activation function with &quot;categorical_crossentropy&quot; loss function. why?</p>
","nlp"
"108136","How to compute sentence embedding from word2vec model?","2022-02-14 08:20:45","","0","893","<python><nlp><word-embeddings><word2vec>","<p>I am new to NLP and I'm trying to perform embedding for a clustering problem. I have created the <strong>word2vec</strong> model using Python's <code>gensim</code> library, but I am wondering the following:</p>
<p>The <strong>word2vec</strong> model embeds the words to vectors of size <code>vector_size</code>. However, in further steps of the clustering approach, I realised I was clustering based on single words instead of the sentences I had in my dataset at the beginning.</p>
<p>Let's say my vocabulary is composed of the two words <strong>foo</strong> and <strong>bar</strong>, mapped as follows:</p>
<p><code>foo</code>: [0.0045, -0.0593, 0.0045] <br>
<code>bar</code>: [-0.943, 0.05311, 0.5839]</p>
<p>If I have a sentence <code>bar foo</code>, how can I embed it? I mean, how can I get the vector of the entire sentence as a whole?</p>
<p>Thanks in advance.</p>
","nlp"
"108094","Should we clean text data before applying Vader for getting sentiment","2022-02-12 06:49:57","","0","1547","<machine-learning><deep-learning><nlp><text-mining><sentiment-analysis>","<p>What I meant by data cleaning is that</p>
<ul>
<li>Removing Punctuations</li>
<li>Lower Casing</li>
<li>Removing Stop words</li>
<li>Removing irrelevant symbols, links and emojis</li>
</ul>
<p>According to my knowledge, things like Punctuations, Capital Letters, Stop words like 'But' and emojis and such tend to intensifies the VADER sentiment value, or in other words it affects the sentiment.</p>
<p>So should I clean my data or leave punctuations, capital letters and stop words since VADER makes use of them as well?</p>
","nlp"
"108075","How to evaluate the quality / trustworthiness of textual information?","2022-02-11 11:10:08","","0","50","<nlp>","<p>I have a corpus of text (which can be used for learning). The text consists of proper names like street names:</p>
<pre><code>Bond Street
Balmain Crescent
Parkes Way
Barrine Drive
Gordon Street
Marcuse Clarke Street
</code></pre>
<p>I want to detect possible spelling errors (i.e. evaluate the quality / trustworthiness of some text such as street names).</p>
<p>The idea is that a sequence of strings can be evaluated based on the likelyhood that certain strings occur in a certain order (I suppose <a href=""https://keras.io/examples/nlp/lstm_seq2seq/"" rel=""nofollow noreferrer"">sequence-to-sequence</a> models and <a href=""https://github.com/jsvine/markovify"" rel=""nofollow noreferrer"">markov chain</a> models strongly draw from this idea).</p>
<p>Thus even if a sequence such as <code>Marcuse Clarke Street</code> may occur seldom, the observed sequences of strings such as <code>Cla, lar, ark, rke</code> may occur more often than (assuming a spelling error like <code>Marcuse Cqarke Street</code>) <code>Cqa, qar, ark, rke</code>.</p>
<p><strong>Question:</strong> Are there models to estimate the probability that some sequence of strings is &quot;true&quot; given some training data? My objective would be:</p>
<p>Input: <code>Street</code> -&gt; Model output: <code>very likely</code> (high probability that it may occur)</p>
<p>Input: <code>Stxeet</code> -&gt; Model output: <code>very unlikely</code> (low probability...)</p>
<p>Input: <code>Balmain</code> -&gt; Model output: <code>likely</code> (OK probability that it may occur)</p>
<p>Input: <code>Buwmain</code> -&gt; Model output: <code>unlikely</code> (low probability...)</p>
","nlp"
"108032","Understanding Kneser-Ney Formula for implementation","2022-02-09 17:18:59","","3","62","<nlp><mathematics><language-model><ngrams>","<p>I am trying to implement this formula in Python</p>
<p><span class=""math-container"">$$ \frac{\text{max}(c_{KN}(w^{i}_{i-n+1} - d), 0)}{c_{KN}(w^{i-1}_{i-n+1})} + \lambda(c_{KN}(w^{i-1}_{i-n+1})\mathbb{P}(c_{KN}(w_{i}|w^{i-1}_{i-n+2})$$</span></p>
<p>where</p>
<p><span class=""math-container"">$$
\mathrm{c_{KN}}(\cdot) = \begin{cases}
    \text{count}(\cdot) &amp; \text{for the highest order } \\ % &amp; is your ""\tab""-like command (it's a tab alignment character)
    \text{continuationcount}(\cdot) &amp; \text{otherwise.}
\end{cases}
$$</span></p>
<p>Following this link <a href=""https://medium.com/@dennyc/a-simple-numerical-example-for-kneser-ney-smoothing-nlp-4600addf38b8"" rel=""nofollow noreferrer"">here</a> I was able to understand how to implement the first half of the equation namely</p>
<p><span class=""math-container"">$$\frac{\text{max}(c_{KN}(w^{i}_{i-n+1} - d), 0)}{c_{KN}(w^{i-1}_{i-n+1})} $$</span></p>
<p>but the second half specifically at the moment the <span class=""math-container"">$\lambda(c_{KN}(w^{i-1}_{i-n+1})$</span> term is confusing me. The author in the link states that</p>
<p><span class=""math-container"">$$\lambda(w_{i-1}) = \frac{d}{c(w_{i-1})}\left|\{w:c(w_{i-1},w)&gt;0\}\right|$$</span></p>
<p>but when we are in the highest order the discounting factor <span class=""math-container"">$d$</span> is zero. The author goes on to say &quot;the denominator in the fraction is the frequency of the semifinal word in the special case of a 2-gram, but in the recursion scheme we are developing, we should consider the whole string preceding the final word (well, for the 2-gram case, the semifinal word is the whole string). The term to the right of the fraction means the number (not the frequency) of different final word types succeeding the string&quot;.</p>
<p>He proceeds on with the example but it makes no sense to me. I cannot really find any good material to explain how to calculate this <span class=""math-container"">$\lambda(c_{KN}(w^{i-1}_{i-n+1})$</span> term.</p>
<p>Any suggestions or material related to this would be helpful. Recall I am doing this numerically so I need help decomposing this equation so I can code it and thus I need to understand what each term is.</p>
<p>Following the link above, this is a preliminary implementation I came up with</p>
<pre><code>def ksener_ney_smoothing(previous_tokens, ngram_dict, discounting_factor=0.75):
    suggestions = []
    
    # Start with previous_token (user input)
    previous_ngram = tuple(previous_tokens)
    previous_ngram_minus_last_word = tuple(previous_tokens[:len(previous_tokens)-1]) # w
    len_previous_ngram = len(previous_ngram)
    
    # Pull ngrams for the highest order
    highest_order_ngrams_map = ngram_dict[len_previous_ngram]
    second_higest_order_ngrams_map = ngram_dict[len_previous_ngram - 1]
    
    # Check if the user input is in the highest order ngram map
    if previous_ngram in highest_order_ngrams_map:
#         discounting_factor = 0 # From the link if the users input is in the highest order ngrams then its 0, found no where in literature
        first_num = max(highest_order_ngrams_map[previous_ngram] - discounting_factor, 0)
        first_denom = second_higest_order_ngrams_map[previous_ngram_minus_last_word]
#         print(first_num, &quot; &quot;, first_denom)
        
        l1 = list(list(x) for x in second_higest_order_ngrams_map.keys())
        lamb_denom = [item for sublist in l1 for item in sublist].count(previous_tokens[-2])
        l2 = list(list(x) for x in highest_order_ngrams_map.keys())
        myset = {item[2] for item in l2 if item[:2] == previous_tokens[:len(previous_tokens)-1]}
        lamb = (discounting_factor/lamb_denom)*len(myset)
#         print(lamb)

        pcont_num = [word[-1] for word in list(ngram_dict_ex[3].keys())].count(previous_ngram[-1])
        pcont_denom = len(highest_order_ngrams_map)
        pcont = pcont_num/pcont_denom
        
        return first_num / first_denom + lamb*pcont
    
    else:
        pass
</code></pre>
<p>I was able to match his corpus and numerical examples so should be on the right track, I am just not sure about the recursion part.</p>
","nlp"
"108016","Matching 2 keywords list using NLP","2022-02-09 11:27:52","","1","324","<classification><nlp><text>","<p>I have two lists and I want to identify which elements are common (same or similar in meaning or context) in the list. Which NLP algorithm we should use.</p>
<pre><code>list-1= [US, Apple, Trump, Biden, Mango, French, German]

list-2= [State, iphone, ipad, ipod, president, person, Fruit, Language, Country]
</code></pre>
","nlp"
"107982","what kind of technique to use for below task?","2022-02-08 16:01:40","","0","31","<machine-learning><deep-learning><nlp><computer-vision>","<p>I'm working on a project to recognize confidential info like social security number, name, driver license number, financial details(like credit card number, account details etc), address, certain confidential info on legal and medical documents from a user-uploaded pdf, my question is let's say I collect some 2k records on financial details, 3k records on legal related terms, can I train only one model to do all these tasks? or separate models for each domain? for e.x: for finance separate model, for legal separate model, etc</p>
<p>I'm very new to the NLP and I don't have much idea, any suggestions on where can I get the data? and what techniques in NLP I can use for this task?</p>
<p><strong>p.s: this problem is both cv and nlp related, cv for the ocr part and nlp for rest, please read the question completely and mention in comments if you have any doubts before downvoting.</strong></p>
","nlp"
"107981","BERT each Word Embedding in Keras","2022-02-08 15:22:02","","1","198","<nlp><bert>","<p>How to use BERT to extract the embeddings of every word in a sentence.
Suppose I pass my corpus of sentences with different lengths to a BERT model , I want to be able to extract the embeddings of each word in every sentence.</p>
<p>And what is the best way to utilize every word embeddings? should I calculate their average?</p>
<p>when I follow the basic usage in this <a href=""https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/2"" rel=""nofollow noreferrer"">BERT model</a>, the sequence length is always 128 regardless of the actual sentence length, I know I can change the 128 but it will be the same for all sentences. Is there a way to get an embedding for every word and when the sentence length is less than 128 then remaining embeddings appear as zeros, for example, if my sentence is 7 words, I want bert to return an embeddings of (128,128) but the useful ones are the first 7 and the remaining are zeros.</p>
","nlp"
"107978","Pytorch build_vocab_from_iterator giving vocabulary with very few words","2022-02-08 13:34:15","","1","2427","<python><nlp><pytorch><machine-translation>","<p>I am trying to build a translation model in pytorch. Following <a href=""https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html"" rel=""nofollow noreferrer"">this post on pytorch</a> I downloaded the <code>multi30k</code> dataset and spacy models for English and German.</p>
<pre><code>python -m spacy download en
python -m spacy download de
</code></pre>
<pre class=""lang-py prettyprint-override""><code>import torchtext
import torch
from torchtext.data.utils import get_tokenizer
from collections import Counter
from torchtext.vocab import Vocab, build_vocab_from_iterator
from torchtext.utils import download_from_url, extract_archive
import io

url_base = 'https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/'
train_urls = ('train.de.gz', 'train.en.gz')
val_urls = ('val.de.gz', 'val.en.gz')
test_urls = ('test_2016_flickr.de.gz', 'test_2016_flickr.en.gz')

train_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in train_urls]
val_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in val_urls]
test_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in test_urls]

de_tokenizer = get_tokenizer('spacy', language='de')
en_tokenizer = get_tokenizer('spacy', language='en')

def build_vocab(filepath, tokenizer):
  counter = Counter()
  with io.open(filepath, encoding=&quot;utf8&quot;) as f:
    for string_ in f:
      counter.update(tokenizer(string_))
  return Vocab(counter, specials=['&lt;unk&gt;', '&lt;pad&gt;', '&lt;bos&gt;', '&lt;eos&gt;'])

de_vocab = build_vocab(train_filepaths[0], de_tokenizer)
en_vocab = build_vocab(train_filepaths[1], en_tokenizer)
</code></pre>
<p>Which gave me the following error:</p>
<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-66-c669b7554322&gt; in &lt;module&gt;()
     20     return vocab
     21 
---&gt; 22 de_vocab = build_vocab(train_filepaths[0], de_tokenizer)
     23 en_vocab = build_vocab(train_filepaths[1], en_tokenizer)
     24 

&lt;ipython-input-66-c669b7554322&gt; in build_vocab(filepath, tokenizer)
     16         for string_ in f:
     17             counter.update(tokenizer(string_))
---&gt; 18     vocab = Vocab(counter, specials=['&lt;unk&gt;', '&lt;pad&gt;', '&lt;bos&gt;', '&lt;eos&gt;'])
     19     vocab.set_default_index(vocab['&lt;unk&gt;'])
     20     return vocab

TypeError: __init__() got an unexpected keyword argument 'specials'
</code></pre>
<p>After a google search I tried to modify the <code>build_function</code>:</p>
<pre class=""lang-py prettyprint-override""><code>def build_vocab(filepath, tokenizer):
    counter = Counter()
    with io.open(filepath, encoding=&quot;utf8&quot;) as f:
        for string_ in f:
            counter.update(tokenizer(string_))
    vocab = build_vocab_from_iterator(counter, specials=['&lt;unk&gt;', '&lt;pad&gt;', '&lt;bos&gt;', '&lt;eos&gt;'])
    vocab.set_default_index(vocab['&lt;unk&gt;'])
    return vocab
</code></pre>
<p>This ran without errors but there were very few words in the vocabulary (<code>len(en_vocab) -&gt; 85</code>) while the <code>counter</code> for <code>en</code> has <code>len(counter) -&gt; 10836</code>.</p>
<p>If I create the vocab object without the <code>specials</code> keyword I get a vocab object with 10836 length.</p>
<pre class=""lang-py prettyprint-override""><code>en_vocab = Vocab(en_counter) # len(en_vocab) -&gt; 10836
</code></pre>
<p>But now I do not have a way of including the <code>specials</code> in the vocab.</p>
<p>Using an example from the <a href=""https://pytorch.org/text/stable/vocab.html#build-vocab-from-iterator"" rel=""nofollow noreferrer"">official pytorch documentation</a> for <code>build_vocab_from_iterator</code> I was able to create another vocab object-</p>
<pre class=""lang-py prettyprint-override""><code>def new_builder(file_path):
    with io.open(file_path, encoding='utf-8') as f:
        for line in f:
            yield line.strip().split()

new_vocab = build_vocab_from_iterator(new_builder(train_filepaths[1]), specials=['&lt;unk&gt;', '&lt;pad&gt;', '&lt;bos&gt;', '&lt;eos&gt;'])
</code></pre>
<p>With this method <code>len(new_vocab)</code> is coming out to be 15460. Which of these methods (if any) is correct and why do the other method give incorrect results?</p>
<p>Also, I have noticed that the spacy tokenizer does not appear to be changing any capitalization or
doing any other changes to the words in the input sentences.</p>
<pre class=""lang-py prettyprint-override""><code>sentence = 'Two young, White males are outside near many bushes.\n'
print(en_tokenizer(sentence))
</code></pre>
<p>Output:</p>
<pre><code>['Two', 'young', ',', 'White', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.', '\n']
</code></pre>
<p>Should there be additional preprocessing steps before creating the vocabulary so that <code>tree</code> and <code>Tree</code> are not considered different words?</p>
","nlp"
"107934","How to choose similarity measurement between sentences and paragraphs","2022-02-07 14:47:28","114814","3","999","<python><nlp><similarity><semantic-similarity>","<h2>Problems</h2>
<h3>1. How to find appropriate measurement method</h3>
<p>There are several ways to measure sentence similarities, but I have no idea how to find appropriate method among them for my data (sentences).</p>
<p>Related Question on Stack overflow: <a href=""https://stackoverflow.com/questions/71019668/how-to-calculate-the-accuracy-among-two-non-chronological-order-and-different-le"">is there a way to check similarity between two full sentences in python?</a></p>
<h3>2. Sentence or paragraph based</h3>
<p>If it is possible to acquire both one sentence and a paragraph which includes the sentence, which is more accurate to measure the similarity among sentences or paragraphs?</p>
<h2>What I tried so far</h2>
<h3>1. I've tried to use one of the libraries to measure the similarity.</h3>
<p>However, I'm struggling how to find more accurate method to measure similarities.</p>
<pre class=""lang-py prettyprint-override""><code>original = 'New York is a noisy city where hamburgers are famous.'
test = ['Berlin is a nostalgic city where sausages are famous.', 'Both New York and Belin are noisy cities, but hamburgers are famous in New York rather than in Berlin.']

import spacy
nlp = spacy.load(&quot;en_core_web_sm&quot;)


doc1 = nlp(original)
for doc2 in test:
    doc2 = nlp(doc2)
    print(doc1.similarity(doc2)) 

0.8682034221008
0.5078180005337849
</code></pre>
<ol start=""2"">
<li></li>
</ol>
<p>Same as sentence based, it was figured out there are several methods to measure the similarity between paragraphs.</p>
<p>But there is no crew which is better (generally high-peformance) to compare sentence or paragraph base.</p>
<p>Related Question on Stack overflow: <a href=""https://stackoverflow.com/questions/8897593/how-to-compute-the-similarity-between-two-text-documents"">How to compute the similarity between two text documents?</a></p>
","nlp"
"107931","Sequence Embedding using embedding layer: how does the network architecture influence it?","2022-02-07 13:26:43","","0","920","<deep-learning><nlp><embeddings><sequence><bioinformatics>","<p>I want to obtain a dense vector representation of protein sequences so that I can meaningfully represent them in an embedding space. We can consider them as sequences of letters, in particular there are 21 unique symbols which are the amino acids (for example: MNTQILVFIACVLIEAKGDKICL).</p>
<p>My approach is to use a sequence embedding that can be learned as a part of a deep learning model (built with Python using Keras libraries), that is a classifier (supervised) neural network which I train to classify sequences according to the host species they belong to. The steps I follow are the following:</p>
<ol>
<li><em><strong>Tokenization</strong></em>. The only way I can tokenize these sequences of amino acids symbols is to consider single characters as tokens. I also found this example in Kaggle <a href=""https://www.kaggle.com/danofer/deep-protein-sequence-family-classification"" rel=""nofollow noreferrer"">Deep Protein Sequence family Classification</a> in Python that classifies different proteins and uses single amino acids as tokens.</li>
<li><em><strong>Embedding</strong></em>. Stealing words from the answer to the question <a href=""https://datascience.stackexchange.com/questions/27025/how-the-embedding-layer-is-trained-in-keras-embedding-layer"">How the embedding layer is trained in Keras Embedding layer</a>:</li>
</ol>
<blockquote>
<p>An embedding layer is <em>a trainable layer</em> that contains 1 embedding matrix, <em>which is two dimensional</em>, in one axis <em>the number of unique values the categorical input can take</em> (for example 26 in the case of lower case alphabet) and on the other axis <em>the dimensionality of your embedding space</em>. The role of the embedding layer is to map a category into a dense space in a way that is useful for the task at hand, at least in a supervised task. This usually means there is some semantic value in the embedding vectors and categories that are close in this space will be close in meaning for the task.</p>
</blockquote>
<p>Moreover, useful words from the blog titled as <a href=""https://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526"" rel=""nofollow noreferrer"">Neural Network Embeddings Explained</a>:</p>
<blockquote>
<p>The main issue with one-hot encoding is that the transformation does not rely on any supervision. We can greatly improve embeddings by learning them using a neural network on a supervised task. <em>The embeddings form the parameters — weights — of the network which are adjusted to minimize loss on the task. The resulting embedded vectors are representations of categories where similar categories — relative to the task — are closer to one another.</em></p>
</blockquote>
<p>Thus putting all these pieces together in my case: I choose as tokens the single amino acids, so I have 21 unique symbols (i.e. number of amino acids) and I choose 10 as dimension of the embedding space, thus my embedding layer dimension is 21 x 10. This means that, once the neural network is trained, I can extract the weights of the embedding layer that are 21 vectors (one for each amino acid) and each vector is a 10 dimensional vector.</p>
<p>As the second extract explains, each element of these vectors are values that are adjusted to minimize the loss in the task. I could see each amino acid as if it was a letter in a word and I wanted to classify these words according to something (like positive or negative comment); or as if it was a word in a sentence and I chose as tokens words and I wanted to classify these sentences according to something (like positive or negative comment).</p>
<p>Since I want sequence representation, I have to find a way to put together the embeddings of single amino acids and the only way that I found feasible is to average all the 10 dimensional vectors so to obtain for each sequence a 10 dimensional vector that is the average of the embeddings of all the amino acids. This would for sure highlight if there are over-represented symbols in one sequence with respect to other. <strong>Furthermore</strong>, since each symbol is associated to vectors whose values are adjusted to minimize loss on the task, averaging should preserve each single amino acid meaningful embedding and give &quot;a global meaningful embedding&quot; of the whole sequence that minimizes the loss on the task. This, in fact, seems to work for simple sentences embeddings. See the answer to a my question post: <a href=""https://datascience.stackexchange.com/questions/104087/how-to-obtain-vector-representation-of-phrases-using-the-embedding-layer-and-do"">How to obtain vector representation of phrases using the embedding layer and do PCA with it</a> in which each word of sentences were considered as tokens and I was looking for vectors embedding the whole sentences. Similarly, here I can see each single amino acid symbol as word and the whole sequence as sentence.</p>
<p>Hence this method should carry/embed two information: frequencies of letters of sequences and classes to which each sequence belong to (in this case host species).</p>
<p>But I would like it to consider also the global structure of letters positions (not only the relative frequencies), because with this method sequences like: MNTQILVFIACVLIE<strong>AKGDKICL</strong> (sequence 1) and <strong>AKGDKICL</strong>MNTQILVFIACVLIE (sequence 2) are represented by the same 10 dimensional vector.</p>
<p>Thus... from here the third point:</p>
<ol start=""3"">
<li><em><strong>Choice of neural network architecture</strong></em>. <strong>Does the choice of neural network architecture influence the embedding of each single amino acid symbol and, consequently, the embedding of the entire sequence ?</strong> For example, if I use a LSTM neural network that should &quot;memorize&quot; the global structure of sequences, would it improve the dense vector representation (both in general and in this particular case) ? <strong>I would expect yes</strong> since, as reported also in the extracts, this embedding layer (so its weights) is trained as all the others so the backpropagation algorithm updates also these weights. In other words, if LSTM network has the power to recognize the importance of the position of each character according to the task (for example, it is able to learn that: if a letter is in position 1, it means it belongs to human, while instead if it is in position 2, it means it belongs to dog) then weights should be updated also according to this. Differently from a simple deep learning model that is not able to consider this information and to deal with sequences.</li>
</ol>
<p>I understand that if I average the embeddings of the single amino acids, I would have anyway the problem that sequences like MNTQILVFIACVLIE<strong>AKGDKICL</strong> sequence 1 and <strong>AKGDKICL</strong>MNTQILVFIACVLIE sequence 2 would have the same dense vector representation. <em>But does, also in this case in which I <strong>average</strong> the embeddings of all the symbols, a better choice of the network architecture give a better result in some way ?</em></p>
<p>In few words, what I really would like to know is:</p>
<p><em><strong>Using LSTM would improve protein sequences embeddings (which are built with the method explained before, i.e. averaging the embeddings of the amino acids present in the sequence) or not ?</strong> If yes or no, why ? Or maybe it is correct to say that is necessary to use networks like LSTM because we are dealing with these kind of data that are sequences?</em></p>
<p>I think that the amino acid embeddings obtained with the embedding layer would be more meaningful if this embedding layer is part of a LSTM neural network. As it happens in text analysis: if we train a LSTM neural network to classify sentences according to their sentiments, this neural network is able to capture the underlying structure of inputs (our sentences) and puts relation between words in our vocabulary into a higher dimension (let's say 10 as in my example) by optimization. Thus I would expect that this happens also in my case that is a sequence of symbols instead of words...</p>
<p>But I would like a confirmation with some more explanations to clarify, maybe explaining me better how the LSTMs work and how the embedding layer in an LSTM neural network is updated (if it is necessary to understand the answer).</p>
<p>I apologize for the long question and I thank you in advance.</p>
","nlp"
"107897","Understanding how Long Short-Term Memory works in classification of sequences of symbols","2022-02-06 12:02:25","","1","113","<neural-network><nlp><lstm><text-classification><bioinformatics>","<p>I want to use a LSTM neural network to classify sequences of protein according to the host species. For example, I have these sequences of letters (toy example, just to understand):</p>
<ul>
<li>MNTQILVFIACVLIE<strong>AKGDKICL</strong> belongs to human</li>
<li><strong>AKGDKICL</strong>MNTQILVFIACVLIE belongs to human</li>
<li>MNTQ<strong>AKGDKICL</strong>ILVFIACVLIE belongs to dog</li>
</ul>
<p>The sequences are different only according to the position of the subsequence <strong>AKGDKICL</strong> and my network should learn to recognize this. Is a LSTM network able to do this ?</p>
<p>I am trying to focus on the meaning of Long Short-Term Memory. From <a href=""https://machinelearningmastery.com/an-introduction-to-recurrent-neural-networks-and-the-math-that-powers-them/"" rel=""nofollow noreferrer"">An Introduction To Recurrent Neural Networks And The Math That Powers Them</a>:</p>
<blockquote>
<p>A recurrent neural network (RNN) is a special type of an artificial neural network adapted to work for time series data or data that involves sequences. Ordinary feed forward neural networks are only meant for data points, which are independent of each other. However, if we have data in a sequence such that one data point depends upon the previous data point, we need to modify the neural network to incorporate the dependencies between these data points. RNNs have the concept of ‘memory’ that helps them store the states or information of previous <em>inputs</em> to generate the next output of the sequence.</p>
</blockquote>
<p>Moreover, from <a href=""https://towardsdatascience.com/recurrent-neural-networks-by-example-in-python-ffd204f99470"" rel=""nofollow noreferrer"">Recurrent Neural Networks by Example in Python</a>:</p>
<blockquote>
<p><strong>a recurrent neural network (RNN) processes sequences</strong> — whether daily stock prices, sentences, or sensor measurements — <strong>one element at a time while retaining a memory (called a state) of what has come previously in the sequence</strong>.
Recurrent means the output at the current time step becomes the input to the next time step. At each element of the sequence, the model considers not just the current input, but what it remembers about the preceding elements. <em>This memory allows the network to learn long-term dependencies in a sequence which means it can take the entire context into account when making a prediction</em>, whether that be the next word in a sentence, a sentiment <em>classification</em>, or the next temperature measurement. A RNN is designed to mimic the human way of processing sequences: we consider the entire sentence when forming a response instead of words by themselves.</p>
</blockquote>
<p>Then, differently from simple RNNs that are affected by memory decay, a LSTM has the concept of storing events over an extended period (long-term memory).</p>
<p>So, in my simple exercise, this is traduced in the following: if I consider each letter as token, using keras tokenizer, I will obtain for each sequence an array of integers like:</p>
<pre><code>[7, 8, 9, 10, 1, 2, 3, 11, 1, 4, 5, 3, 2, 1, 12, 4, 6, 13, 14, 6, 1, 5, 2]
</code></pre>
<p>Once I have translated this sequences of symbols into vectors, I can feed them to the LSTM network <em>that is capable to capture the order of these integers (symbols), keep in memory it and consider it when it has to classify the next sequence?</em></p>
<p>For example, If I give it several sequences like the ones reported above, will he be able to recognize that if the subsequence AKGDKICL is positioned at the end or at the beginning of the sequence, it belongs to human while if it is in the middle, it belongs to dog ? Is this the meaning of Long Short-Term Memory ? And is this obtained if I choose each symbol of the sequence as token ?</p>
","nlp"
"107883","Pretrained German BERT","2022-02-05 20:15:34","","0","1037","<nlp><bert>","<p>I'm looking for a (well) pretrained BERT Model in German to be adapted in a Keras/TF framework. Ideally with a minimal example on how to fine-tune the model on specific tasks, i.e. text classification!</p>
<p>Can anyone point me to some (open source) resources?</p>
","nlp"
"107858","Grouping tweets and newspaper articles by topic","2022-02-04 21:52:25","","0","64","<machine-learning><python><nlp><topic-model>","<p>i want to implement a software that groups twitter posts to other twitter posts or to newspaper articles with similar topics.</p>
<p>Let's say for example someone tweets about a soccer game and at the same time a newspaper article gets released about the same soccer game. I would like to bundle and group up all tweets and newspaper articles that i can find about that specific football match together.</p>
<p>How would you approach this problem?</p>
","nlp"
"107725","What are the differences between bert embedding and flair embedding","2022-02-01 11:24:58","107748","0","1523","<deep-learning><nlp><word-embeddings><bert>","<p>I read about <code>BERT</code> embedding model and <code>FLAIR</code> embedding model, and I'm not sure I can tell what are the differences between them ?</p>
<ul>
<li><code>BERT</code> use <code>transformers</code> and <code>FLAIR</code> use <code>BLSTM</code></li>
<li>With <code>BERT</code>, we the feed words into the <code>BERT</code> architecture, and with <code>FLAIR</code> we feed characters into <code>FLAIR</code> architecture.</li>
</ul>
<ol>
<li>What are the strengths of <code>BERT</code> embedeeing ?</li>
<li>What are the strengths of <code>FLAIR</code> embedeeing ?</li>
<li>In which cases would we prefer to use one model rather than another ?</li>
</ol>
","nlp"
"107713","Which will be best deep learning model for topic classification using NLP","2022-02-01 07:29:18","","1","53","<machine-learning><deep-learning><nlp><multiclass-classification><bert>","<p>I have a dataset consisting of two columns [Text, topic_labels].
Topic_labels are of 6 categories for ex: [plants,animals,birds,insects etc]</p>
<p>I would like to build deep learning-based models in order to be able to classify topic_labels.
so far I have implemented both supervised[SVM, Logistic] &amp; unsupervised [topic-LDA, Guided-LDA] approaches in a traditional way by applying both Word2Vec &amp; TF-IDF  but I wanted to implement state-of-the-art deep learning classification techniques for the text data?</p>
<p>Suggest me the best deep learning model for text topic classification.</p>
","nlp"
"107678","Consistency between multiple word predictions in a single NLP sentence","2022-01-31 13:24:03","","1","17","<nlp><lstm><convolutional-neural-network>","<p>Considering a model which predicts multiple missing words in a sentence:</p>
<pre><code>The ___ is preparing the ___ in the ___
</code></pre>
<p>There is no pre-existing context in this masked sentence which could inform the model on what the topic is.</p>
<p>In the context of either an LSTM-based or convolutions-based model, what would be some strategies to apply either in the modelling part, or in the loss function part, to favor predictions which are consistent with one another?</p>
<p>Examples of consistent predictions:</p>
<pre><code>The [cook] is preparing the [meal] in the [kitchen]
The [mechanic] is preparing the [car] in the [garage]
The [sound engineer] is preparing the [microphone] in the [studio]
</code></pre>
<p>Examples of inconsistent predictions:</p>
<pre><code>The [cook] is preparing the [microphone] in the [park]
The [translator] is preparing the [chicken] in the [office]
</code></pre>
","nlp"
"107666","Clustering tables (with similar schema) together","2022-01-31 08:03:56","","0","208","<machine-learning><nlp><clustering>","<p>I have been working on a problem but unable to make substantial progress so wanting some insights/advice.</p>
<p>I have a large set of tables (CSV files) having only column names. (Column values are not present.)</p>
<p>For example:</p>
<pre><code>Users.csv --&gt; ID, FirstName, LastName, City, Address, CardID
Usr.csv --&gt; Residential, Name, State, UserID
</code></pre>
<p>I want to cluster files (tables) which represent similar entities. In the example above, both represent a person entity and hence should be clustered together.</p>
<p>Current Approaches:
I tried coming up with a similarity score b/w two tables which is based on the overlapping columns b/w the two tables and similarity b/w their table names</p>
<pre><code>sim_score = alpha * (column similarity) + beta * (table name similarity)
where 
column similarity is a fraction of possible columns matching to a minimum 
number of columns from both table.
possible columns match is computed by generating features of each column like jaccard_3gram, 
tf-idf score etc. and thresholding the similarity to arrive at similarity score.

table name similarity is computed by computing the dice coeff. of the two tables names
</code></pre>
<p>Once sim_score is computed for all table pairs, then creating a weighted undirected graph and running a clustering algorithm (tried Markov, Girvan) to get the required clusters.</p>
<p>This approach is not working well when there are semantically related column names b/w two tables. I have been trying approaches around this thought process.</p>
<p>Is there an alternative to how this problem can be looked at? Any standard approaches in ML/NLP to tackle this. Any leads are helpful. Thanks</p>
","nlp"
"107611","How to select a proper vectorization method in NLP?","2022-01-29 18:47:36","107613","0","176","<python><classification><nlp><text-classification>","<p>Suppose we have a text classification problem.</p>
<p>As we all know we have to convert the text data into vectors so as to train the model. So there are a couple of vectorization methods such as count vectorization, Tf-IDF, Bag of words, etc. So from these many vectorization methods how will we choose one method? Is it like that or in another way do we need to try all the methods, train the model then check the performance with each vectorization method?</p>
<p>Please share your thoughts and help me to understand this properly.</p>
","nlp"
"107587","Sequence-to-Sequence Transformer for Neural machine translation","2022-01-28 21:18:56","107609","0","84","<deep-learning><keras><nlp><transformer><language-model>","<p>I am using the tutorial in Keras documentation <a href=""https://keras.io/examples/nlp/neural_machine_translation_with_transformer/"" rel=""nofollow noreferrer"">here</a>. I am new to deep learning. On a different dataset <code>Menyo-20k</code> <a href=""https://zenodo.org/record/4297448"" rel=""nofollow noreferrer"">dataset</a>, of about 10071 total pairs, 7051 training pairs,1510 validation pairs,1510 test pairs. The highest validation accuracy and test accuracy I have gotten is approximately 0.26. I tried the list of things below:</p>
<ol>
<li>Using the following optimizers: <code>SGD, Adam, RMSprop</code></li>
<li>Tried different learning rate</li>
<li>Tried the dropout rate of <code>0.4 and 0.1</code></li>
<li>Tried using different embedding dimensions and feed-forward network dimension</li>
<li>Used <code>Early stopping and patience =3</code>, the model does not go past the <code>13th epoch</code>.
I tried the model itself without changing any parameters, the <code>validation accuracy never got to 0.3</code>, I tried to change the different parameters in order to know what I am doing wrong and I can't figure it out. Please what am I doing wrong? Thank you in advance for your guidance.</li>
</ol>
","nlp"
"107550","Dataset with Multiple Choice Questions for fine tuning","2022-01-28 09:25:21","","0","514","<machine-learning><nlp><dataset><data><gpt>","<p>I hope it's allowed to ask here, but I am looking for a dataset (the format is not that important) that is similar to SQuAD, but it also contains false answers to the questions. I wanna use it to fine tune GPT-3, and all I find is either MC questions based on a text, but with no distractors, or classical quizzes that have no context before each question.</p>
<p>I have a code that generates distractors, and I can just plug it in there, but I was wondering if there was any pre-made dataset.</p>
","nlp"
"107511","NLP Interview Coding Task","2022-01-27 09:56:06","","1","494","<classification><nlp><cosine-distance>","<p>Please comment on the following NLP Interview Coding Task that I have prepared for the candidates on Data Science NLP position that I am looking for. The goal is to check candidate understanding of the fundamental role of text representations with vectors in NLP, as well as checking candidate coding skills and their ability to optimize computations with vectorization that Numpy provides.</p>
<p>In particular I need your opinion on:</p>
<ol>
<li>Is task clear?</li>
<li>Is task adequate for coding a rough solution from scratch in 20 -30
minutes during the online interview?</li>
<li>What level - Junior, Middle or Senior DS NLP Engineer - would you
assign this task to?</li>
</ol>
<p>Task:</p>
<pre><code># Write from scratch (you can only use Numpy arrays) 
# very basic and simple algorithm to classify sentences:

test1 = &quot;cats like meat and fish is best for cats&quot;
test2 = &quot;train your mind reading good fiction, thrillers and other books&quot;

# Use these sentences to train your classifier:

# Class 1
sent1 = &quot;meat is a good food for all dogs and cats , dogs also like apples&quot;

# Class 2
sent2 = &quot;reading fiction books is a good food for mind and some thrillers are not&quot;
</code></pre>
<p>To solve this task, candidate should write count vectorizer and cosine similarity functions from scratch. Using these functions candidate can find similarity of test sentences to classes 1 and 2, and thus classify test sentences. Normalizing vectors would be a bonus for the candidate.</p>
<p>It took 20 minutes for me to code, test and describe this task. Not sure how much time NLP position candidate may need.</p>
","nlp"
"107497","Should I create single feature for each specific word which i find in text or one for all them?","2022-01-26 21:50:57","","1","28","<machine-learning><nlp><feature-engineering><dummy-variables>","<p>I am doing feature engineering right now for my classification task. In my dataframe I have a column with text messages. I decided to create a binary feature which depends on whether or not in this text were words &quot;call&quot;, &quot;phone&quot;, &quot;mobile&quot;, &quot;@gmail&quot;, &quot;mail&quot; &quot;facebook&quot;. But now I wonder should I create separate binary features for each word (or group of words) or one for all of them. How to check which solution is better. Is there any metric and how usually people do in practice. Thanks)</p>
","nlp"
"107495","Reversing a dependency tree into the original sentence","2022-01-26 21:10:42","","1","89","<nlp><stanford-nlp>","<p>I'm wondering if it is possible to convert a dependency parser such as</p>
<pre><code>
(ROOT
  (S
    (NP (PRP$ My) (NN dog))
    (ADVP (RB also))
    (VP (VBZ likes)
      (S
        (VP (VBG eating)
          (NP (NN sausage)))))
</code></pre>
<p>into</p>
<p><code>&quot;My dog also likes eating sausage.&quot;</code></p>
<p>with Standford CoreNLP or otherwise</p>
","nlp"
"107462","Why does averaging word embedding vectors (exctracted from the NN embedding layer) work to represent sentences?","2022-01-25 19:03:42","110506","2","4086","<neural-network><nlp><rnn><word-embeddings><embeddings>","<p>I'm puzzling to understand why the method of averaging word embeddings works in order to obtain sentence embedding, in particular considering the exercize of this post <a href=""https://datascience.stackexchange.com/questions/104087/how-to-obtain-vector-representation-of-phrases-using-the-embedding-layer-and-do"">How to obtain vector representation of phrases using the embedding layer and do PCA with it</a>. My current question actually is to understand the theory behind that more practical post.</p>
<p>The answer to the question linked uses a method for sentence embedding that is averaging the word embeddings (in the most naive and simplest case in which we obtain the word embeddings by extracting vectors from the embedding layer of the neural network model and so without using pretrained NN model).</p>
<p>This method seems to work because the words in the PCA space make clusters according to the class labels they belong to. <strong>Why does it work ?</strong></p>
<p>My personal explanation that I have tried to give is the following: the Tokenizer assigns an integer to each word (let us assume that we choose words as tokens) on the basis of word frequency: <em>lower integer means more frequent word (often the first few are stop words because they appear a lot)</em> (from <a href=""https://stackoverflow.com/questions/51956000/what-does-keras-tokenizer-method-exactly-do"">What does Keras Tokenizer method exactly do?</a>).</p>
<p>Now sentences are vectors of integers. After padding and truncatin, we can feed them to our NN.</p>
<p>In the training phase, the Neural Network receives in input a series of symbols: first the symbols of the first sentence and assigns to each of them a vector of weights (of which we can decide the dimension). Then it looks which is the label and through the backpropagation algorithm it updates its weights on the basis of this label. Sentences belonging to the same class will have weights more similar one respect to each other but also the NN captures the structure of the sentences (this last observation is from the answer to the post <a href=""https://stats.stackexchange.com/questions/270546/how-does-keras-embedding-layer-work"">How does Keras 'Embedding' layer work?</a>) and so maybe sentences that share more words in common will have weights vectors more similar (I tested myself this by doing an other simple exercize of classification where I considered as input data a set of words, I chose letters as tokens and I considered as word embedding the average of the embeddings of all the letters in the word. In the PCA plot, besides clustering according to the classes, words that share more letters in common were closer with respect to those that differ more).</p>
<p>And so, making the average of the words embeddings vectors to obtain the sentence embedding vectors comes out to be a good method because the average is like a &quot;weighted average&quot; since the weights vectors are assigned to each symbol based on the class of the entire set of symbols (i.e. sentence) and also based on the general structure of the set of symbols (i.e. frequency of words) ?</p>
<p>I did not find a research paper that used this method (they very often use more sophisticated methods that I have no time to try in this moment). But I found this answer to this other post <a href=""https://stackoverflow.com/questions/46889727/word2vec-what-is-best-add-concatenate-or-average-word-vectors"">word2vec - what is best? add, concatenate or average word vectors?</a>. Here it links to a lesson in which a Professor explains that averaging word vectors works incredibly well to capture all the statistics.</p>
","nlp"
"107423","How could I improve my classifier of text data?","2022-01-24 16:25:03","","1","33","<machine-learning><deep-learning><classification><nlp><word2vec>","<p>I have a dataset with three columns &quot;message&quot;, &quot;city&quot; and &quot;has_info&quot;. Here is a sample of it:</p>
<pre><code>message                                    city         has_info
ill be there soon, call me 313-972-0310  New-York          1
use this email john***@gmail.com         Boston            1
ok. you can check it                     Boston            0
.................................        .......           ..
i love it                                Miami             0
</code></pre>
<p>has_info column is binary column which defines whether or not some contact information was mentioned in column &quot;message&quot; (1 if there was, 0 if wasn't). I have train and test pandas dataframes like that. And I want to make classifier to predict target value &quot;has_info&quot; in test dataset.</p>
<p>I turned feature &quot;city&quot; into categorical one and created couple of new features as well, like number of words in message for example. I also used bag of words method by finding up to 1000 most frequent tokens in train dataset and sorting them by number of occurences (highest first). So it will create 1000 additional features.</p>
<p>All of it however gave me only AUC value 0.85.</p>
<p>I wanted to know, if there any other (better) method for this particular case? Maybe I should just manually create list of red flag words (phone, mail, number, call, text, etc.) and based on them create dummy variables whether they occurred in message or not? Is there any other nlp solution that can probably give me at least 0.9 AUC?</p>
<p>My train dataset has 900000 rows in it, so its very large.</p>
<p>Thanks in advance</p>
","nlp"
"107419","Regarding pos tagging","2022-01-24 14:58:12","","0","40","<python><nlp><dataset><feature-selection><feature-extraction>","<p>I working on a dataset, I did the pos_tagging using nltk. Now I want to know which sequence of grammar is most common in my rows, then I want to define a chunk grammar based on a common grammar sequence. Can you please tell me how to do it in python?
Thank you</p>
<p><a href=""https://i.sstatic.net/hCuuq.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/hCuuq.png"" alt=""enter image description here"" /></a></p>
","nlp"
"107412","Where to start with ChatBots?","2022-01-24 11:20:40","","1","36","<python><nlp><lstm><bert><chatbot>","<p>I want to start my journey into ChatBots and how I can create them. I have read some articles regarding the type of chatbots. Basically there are 2 types of chatbots, one is a <strong>rule based</strong> and the other is a <strong>NLP/ML based</strong>. I am more interested in the latter and want some kind of started guide. I have read that these kind of chatbots usually use some kind of attention models (as they give state of the art results) like <strong>BERT</strong>. I have no experience in attention models. I have started with LSTM models and it's variations, as attention models have LSTM at their core I think.</p>
<p>What I want from the stalwarts in this field is some <strong>advice/starter guide (be it articles/blogs/videos) on how to get started or am I going in the right direction or not.</strong> It would really help a novice like me!</p>
<p>Thank you!</p>
","nlp"
"107266","Topic Modeling: LDA vs LSA vs ToPMine","2022-01-20 09:32:38","109731","2","929","<python><nlp><topic-model><lda>","<p>I am new to Topic Modeling.</p>
<ol>
<li><p>Is it possible to implement ToPMine in Python? In a quick search, I can't seem to find any Python package with ToPMine.</p>
</li>
<li><p>Is ToPMine better than LDA and LSA? I am aware that LDA &amp; LSA have been around for a long time and widely used.</p>
</li>
</ol>
<p>Thank you</p>
","nlp"
"107264","Which ML to use for search suggestion?","2022-01-20 08:29:41","","1","57","<neural-network><nlp><reinforcement-learning>","<h1>Problem:</h1>
<hr/>
I want to create a program to organize text information and fast access to relevant documents. I would like to train a ML model to analyse the current situation and to suggest the next action.
The Workflow should look like this:
<ul>
<li><p>Model gets document type and document text.(Let's say its a Bill from Mr. M. Learning)</p>
</li>
<li><p>Then the model should suggest based on previous actions what the user wants to do next (e.g the user opened this bill and was coming from the unpaid bills tab of the application)</p>
</li>
<li><p>In this case the model should suggest to send a reminder to M. Learning or open his previous bills etc.</p>
</li>
</ul>
<p>I hope the problem I would like to solve is clear. The goal would be to have a network for every document which knows where to go next based on previous actions.</p>
<p>I thought of using reinforcement learning and reward the model if a user actually uses a suggested action. Maybe combining it with natural language processing for document analysis?</p>
<h1>Question:</h1>
<hr/>
I would like the model to continue to collect usage data from the user and continue **unsupervised** learning.
<p>Is this the way to go or are there models that are more suitable for my purpose? Any advice is highly appreciated!</p>
<p>Thanks in advance!</p>
","nlp"
"107212","Get sentence embeddings of transformer-based models","2022-01-19 00:12:26","","0","1671","<nlp><bert><transformer><embeddings><huggingface>","<p>I want to get sentence embeddings of transformer-based models (Bert, Roberta, Albert, Electra...).</p>
<p>I plan on doing mean pooling on the hidden states of the second last layer just as what bert-as-service did.</p>
<p>So my questions is that when I do mean pooling, should I include the embeddings related to [PAD] tokens or [CLS] token or [SEP] token?</p>
<p>For example, my sequence is 300 tokens, and are padded into 512 tokens.</p>
<p>The output size is 512 (tokens) * 768 (embeddings).</p>
<p>So should I average the embeddings of first 300 tokens or the embeddings of whole 512 tokens?</p>
<p>Why the embeddings of the last 212 tokens are non-zero?</p>
","nlp"
"107196","What can be the approaches to merge (ensemble) a NON-Probabilistic model with RandomForest?","2022-01-18 14:17:46","","1","18","<machine-learning><classification><nlp><scikit-learn><random-forest>","<p>I have a <code>RF</code> for Text classification and it gives me accuracy. Almost same metric is given by another model built using <code>ElasticSearch</code> and it gives you a score, which you can not relate to. For example, For sample <strong>X</strong> it gave me 4 prediction results with some scores as <code>A 10,B 9,C 8,D 7</code> (for each prediction as pred1, pred2, pred3...) and for <strong>Y</strong>, it gave me <code>B 4,A 3,D 2,C 1</code> and for <strong>Z</strong>, it gave me <code>C 90,A 65,B 43,D 20</code>. You simply can not say if scores of sample <strong>X</strong> is better than <strong>Y</strong> or vice versa.  <strong>But you can definitely say that <code>pred1</code> is better than <code>pred2</code> is better than <code>pred3</code>....</strong></p>
<p>What I observed is that my <code>RandomForest</code> gave better results for some <code>N</code> classes and the other model gave better results for <code>P</code> classes. When I compared the cumulative accuracy, it was say 95% but alone for any of the approach, it was 85%.</p>
<p>Is there any way where I can merge these two approaches? Suppose I make <code>One-Hot</code> encoded Class ids  (Vector of <code>N classes * 4 (results)  </code> + metadata / extra features from data) of all the Five results returned by the Elastic Algo and then use them in Logistic Regression with <code>Voting Classifier</code> of this and my <code>Random Forest</code>?? Just thinking out loud.</p>
<p>For example, let us suppose in a 4 class classification, if the model returned [A,B,C,D]as classes (in ORDER means A is better than B &gt; C &gt; D Total 4 classes), feature for that sample would be
[<code>00</code> <code>01</code> <code>10</code> <code>11</code>], I would pass it to a logistic Regression model and maybe use Voting Classifier along with my <code>RF</code>!! ?? Any good suggestions and approaches?</p>
","nlp"
"107190","does ValueError: 'rat' is not in list means not exist in tokenizer","2022-01-18 12:52:43","107248","1","110","<nlp><word-embeddings><bert><tokenization>","<p>Does this error means that the word doesn't exist in the tokenizer</p>
<pre><code>return sent.split(&quot; &quot;).index(word)
ValueError: 'rat' is not in list
</code></pre>
<p>the code sequences like</p>
<pre><code>def sentences():
   for sent in sentences:
       token = tokenizer.tokenize(sent)
       for i in token :
           idx = get_word_idx(sent,i)
def get_word_idx(sent: str, word: str):
    return sent.split(&quot; &quot;).index(word)
</code></pre>
<p>sentences split returns <code>['long', 'restaurant', 'table', 'with', 'rattan', 'rounded', 'back', 'chairs']</code>
which <code>rattan</code> here is the problem as i think</p>
","nlp"
"107118","How to get fine-grained sentiment score from text data under unsupervised learning?","2022-01-15 23:29:56","","0","92","<nlp><lstm><text-mining><text-classification><sentiment-analysis>","<p>In my experience I have only used LSTM models to do sentiment classification tasks on text data under supervised learning. For example, the imdb dataset from keras which is a binary classification task, and sentiment140 dataset from kaggle which has three classes (0, 2 and 4).</p>
<p>First, I want to get sentiment scores from text data under unsupervised learning. For example, I wish to get yesterday's sentiment score from yesterday's news. If I have to label a sentiment score to every news it would be nearly impossible.</p>
<p>Second, I wish to get a fine-grained sentiment score rather than several classes.</p>
<p>Since I am only familiar with LSTM, I want to ask that which model should I use? I think the first  point is a must and realizing the second point would make it better.</p>
","nlp"
"107076","How can i get the vector of word using BERT?","2022-01-14 14:17:14","","1","2027","<nlp><word-embeddings><bert><representation>","<p>I need to get word-vectors using BERT and got this function that i think it should be the one i need</p>
<pre><code>def get_bert_embed_matrix(sentences):
    device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
    model_config = transformers.AutoConfig.from_pretrained('bert-base-uncased', output_hidden_states=True)
    model = transformers.AutoModel.from_pretrained('bert-base-uncased', config=model_config)
    tokenizer = transformers.AutoTokenizer.from_pretrained('bert-base-uncased')  
   for i in sentences:
        tokenized_text = tokenizer.tokenize(i)
        indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)        
        tokens_tensor = torch.tensor([indexed_tokens])
        model.eval()
        outputs = model(tokens_tensor)
        hidden_states = outputs[2]
        word_embed_6 = torch.cat([hidden_states[i] for i in [-1,-2,-3,-4]], dim=-1)
    return word_embed_6
</code></pre>
<p>Does the method return vectors for sub-word or word ?</p>
","nlp"
"107042","Custom Named-Entity Recognition (NER) in product titles using deep learning","2022-01-13 16:42:43","107127","3","3098","<machine-learning><deep-learning><nlp><word-embeddings><named-entity-recognition>","<p>I am new to machine learning and Natural Language Processing (NLP). I am trying to identify which brand, product name, dimension, color, ... a product has from its product title. That is, from</p>
<p>'<strong>Sony ZX Series Wired On-Ear Headphones, Black MDR-ZX110</strong>'</p>
<p>I want to extract</p>
<p>'<strong>brand=''Sony''</strong>', '<strong>item=''Headphones''</strong>', '<strong>color=''Black''</strong>', '<strong>model_number=''MDR-ZX110''</strong>'.</p>
<p>I understand that this amounts to something like custom Named-Entity Recognition.</p>
<p>The very minimum of what my algorithm should do is identify the '<strong>item</strong>' attribute value, i.e. it should recognize that the above example refers to ''Headphones''.</p>
<p>My dataset contains product titles such as the one above with appropriate attribute-value pairs scraped from e-commerce websites.</p>
<p>Which deep learning algorithm is best-suited to solve this problem? Which type of input (embeddings?), neural network model, output layer should I choose to start?</p>
","nlp"
"106956","Difference between Doc2Vec and BERT","2022-01-11 18:20:49","106975","0","2758","<machine-learning><nlp><bert><transformer><doc2vec>","<p>I am trying to understand the difference between Doc2Vec and BERT. I do understand that doc2vec uses a paragraph ID which also serves as a paragraph vector. I am not sure though if that paragraph ID serves in better able to understand the context in that vector?</p>
<p>Moreover, BERT definitely understands the context and attributes different vectors for words such as &quot;Bank&quot;. for instance,</p>
<ol>
<li>I robbed a bank</li>
<li>I was sitting by the bank of a river.</li>
</ol>
<p>BERT would allocate different vectors for the word BANK here. Trying to understand if doc2vec also gets this context since the paragraph id would be different here (for doc2vec). Can anyone please help with this?</p>
","nlp"
"106898","How to categorise customer complaint using NLP","2022-01-10 06:24:11","107069","0","776","<machine-learning><classification><nlp><tensorflow><bert>","<p>I have a dataset of community complaints and I would like to build a NLP model on those descriptions and tag a category (can be referred for an inspection or Not ie &quot;Not referred) to each of them. Boolean answer ( Yes or No) would suffice my requirement.</p>
<p>For example: Our customer service department process complaints that are received via phone or email with &quot;referred&quot; or &quot;not referred&quot; status. Right now they are checking descriptions to classify them manually as &quot;referred&quot; or &quot;not referred&quot;. My ultimate goal is to automate the whole and build a Machine Learning Model which gives a binary output &quot;Yes&quot; or No&quot; based on descriptions.  So that, they dont need to check manually and process those complaints. That ML model should categorise the future complaints into two buckets &quot;Referred&quot; &quot;Not Referred&quot; The classification of the issues they have received into buckets will help the department to provide customized solutions to the customers in each group.</p>
<p>Is there a way in NLP to build and train a model to automate this process? I have been reading stuffs about NLP for the past couple of days and it looks like NLP has a lot of good features to get a head start in addressing this issue. Could someone please guide me with the way I should use NLP to address this issue?</p>
<p>Based on recent research and recommendation, i read few article on this task. Below screenshot from one of them;
<a href=""https://i.sstatic.net/HZQzG.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/HZQzG.png"" alt=""BERT"" /></a></p>
<p>In that <code>print</code> section he has passed one index row to get sentiment output. Can I get similar output for multiple rows if i dont initialise <code>iloc[0]</code>?</p>
<p>That is what I am after. We receive a bunch of messages daily from the community assisting Line and want to classify them into two buckets – of interest, not of interest.</p>
","nlp"
"106783","Best methods to choose between different searching models?","2022-01-06 22:15:24","","0","34","<machine-learning><deep-learning><nlp><search-engine>","<p>My question here is in regards to best practices and current methods for selecting search models on the fly based on a users query.</p>
<p>Lets say I have four searching models, each optimized for their respective types:</p>
<ul>
<li>Model A: Embedding-based, used for sentence queries about scientific topics</li>
<li>Model B: Embedding-based, used for sentence queries about general news topics</li>
<li>Model C: TF*IDF-based, used for keyword queries about scientific topics</li>
<li>Model D: TF*IDF-based, used for keyword queries about general news topics</li>
</ul>
<p>When users enter a query such as:</p>
<ul>
<li>Query: &quot;vaccine science&quot;</li>
<li>Query: &quot;what caused the stock market to change today&quot;</li>
</ul>
<p>...what are the best ways to determine the model a search engine should use? Are there any design patterns I can use as a reference, or, is this simply another model that I would need to train?</p>
<p>I tried to google terms like &quot;models that select other models&quot;, or &quot;models to determine which models to use&quot;, but I have not had much luck there.</p>
","nlp"
"106755","What are the best methods to reduce the bag of words dimensionality?","2022-01-05 22:31:36","","1","288","<nlp>","<p>I have a small text dataset with 600 comments. My task is a binary classification one. In order to train my model, I transformed the comments into a bag of words using sklearn CountVectorizer. The vocabulary has 1800 words, which is way bigger than the amount of comments. So, how can I reduce the dimensionality of my dataset? I want to know this because I believe that this bigger vocabulary is a problem for the model. I know methods to reduce dimentionality, like PCA, but I do not know if it is as useful in the context of text classification as it is in a tabular dataset. I am also new to nlp, so I wonder if there is a better way to reduce dimensionality, like a way to choose the best words in the vocabulary and use only those.</p>
","nlp"
"106751","Classification Texte with naive bayes complement","2022-01-05 21:31:01","","2","57","<machine-learning><nlp><multiclass-classification><text-classification><naive-bayes-classifier>","<p>Currently I am on a text classification project, the goal is to classify a set of CVs according to 13 classes. I use the bayes algorithm (ComplementNB), in my tests it is the model that gives the highest score. The problem is that I do not exceed 54% of recall, nor 50% of F1 score. I have a problem of class imbalance to begin with, but also my texts are similar and therefore my variables are not independent. looking at my confusion matrix, the model tends to classify the majority of resumes in majority classes. Would there be a solution to increase my F1 score and above all what should be done for the variable dependency problem? I want to clarify that the same text (CV) can have up to 4 different classes.</p>
","nlp"
"106664","Comparing the cosine similarities of the same word representations, from two separate models (vector spaces)","2022-01-03 17:29:13","","2","39","<nlp><word2vec><bert>","<p>I am comparing the cosine similarities of word representations derived from a BERT model and also from a static Word2Vec model.</p>
<p>I understand that the vector spaces of the two models are inherently different due to the dimensionality of BERT (768) and Word2Vec (300). Essentially I am trying to find a way to compare the two cosine similarity measurements between the same words but from two different models.</p>
<p>I also have a set of user-determined similarity scores between the words, e.g., 'vaccinate' - 'inoculate' = 8.99. I was thinking of using this as a scaling factor for the two similarities so each cosine similarity from the vector space would then be scaled by the same amount.</p>
<p>I essentially want to quantitatively compare the cosine similarity scores between two models' representations for the same words. Any help would be appreciated.</p>
","nlp"
"106634","What are popular deep learning models for tabular data of texts?","2022-01-02 18:50:04","","0","436","<machine-learning><deep-learning><classification><nlp>","<p>I have a tabular dataset where every column is of type &quot;text&quot; (i.e. not categorical variable and essentially it can be anything).</p>
<p>Let's suppose that the task is classification</p>
<p>What are some popular methods to classify such data? I have this idea to transform each row to a document where values are separated with a special character (much like CSV). Then I could train a language model like BERT. I guess the special character can be a word by itself so it could signal the model the notion of columns.</p>
<p>Is that approach popular and worth the shot? What are other approaches are known to be successful for my task?</p>
","nlp"
"106609","One class classifier for fraud call detection ( in Hindi language) using BERT","2022-01-01 15:40:23","","0","31","<nlp><data-science-model><bert>","<p>I have created a dataset of text files that are nothing but transcripts of fraud call recordings. I want to implement one class classifier as I have only fraud call audio and transcripts but not a single normal(not fraud) call recording. I want to use BERT's multilanguage model but I don't know intermediate steps or resources for such a project.  Can anyone help me with the approach? thank you.</p>
","nlp"
"106511","Get the keywords from positive and negative reviews","2021-12-29 10:09:40","","0","526","<python><nlp><sentiment-analysis>","<p>I have trained a classifier algorithm on a sentiment analysis model which classifies the reviews scraped off Amazon as Positive or Negative. Now for each class, I want to get the keywords from the review i.e. reason for the positive or negative review.</p>
<p>For example if I have a review &quot;the quality of the shirt is the worst!&quot;. I want to get the keyword as &quot;quality&quot;. Similarly &quot;Really liked the fitting of the shirt&quot; should return &quot;fitting&quot; as the keyword.</p>
<p>Any idea how this can be done?</p>
","nlp"
"106510","Making my own stop-words list from a certain community, is tf-idf good enough?","2021-12-29 08:41:30","","0","104","<nlp>","<p>So I have some tweets from my country and I want to make a my own stop-words list. Is tf-idf good enough? Are there any statistical methods that would be better?</p>
","nlp"
"106508","Algorithms for classification of very short text","2021-12-29 06:30:43","","0","36","<nlp><transformer>","<p>I am to create a classification model for texts that typically have 3 to 4 words in them. I thought of using BERT and XLNet but not sure if they are the right choice for texts that short.</p>
<p>Are there any models(pretrained or otherwise) that specialise for classification of short text?</p>
","nlp"
"106489","Document Content","2021-12-28 15:12:12","106490","0","35","<nlp><semantic-similarity>","<p>I have a set of .pdf/.docx documents with content. I need to search for the most suitable document according to a particular sentence. For instance:</p>
<pre><code> Sentence: &quot;Security in the work environment&quot;
</code></pre>
<p>The system should return the most appropriate document which contains at least the content expressed in the sentence. It should be a sort of search bar with advanced capabilities; I have a constraint: I can not have an apriori classification since the number of documents and the related category could vary on time.</p>
<p>How should I address this kind of task?</p>
","nlp"
"106469","Why not rule-based semantic role labelling?","2021-12-27 19:06:10","106473","1","162","<nlp><text><language-model><parsing>","<p>I have recently found some interest in automatic <a href=""https://en.wikipedia.org/wiki/Semantic_role_labeling"" rel=""nofollow noreferrer"">semantic role labelling</a>. Most introductory texts (e.g. Jurafsky and Martin, 2008) present approaches based on supervised machine learning, often using FrameNet (Baker et al. 1998) and PropBank (Kingsbury &amp; Palmer, 2002). Intuitively however, I would imagine that the same problem could be tackled with a grammar-based parser.</p>
<p>Why is this not the case? Or rather, why would these supervised solutions be preferred?
Thanks in advance.</p>
<hr />
<h2>References</h2>
<p>Jurafsky, D., &amp; Martin, J. H. (2009). Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition. Prentice Hall.</p>
<p>Baker, C. F., Fillmore, C. J., &amp; Lowe, J. B. (1998). The Berkeley FrameNet Project. 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 1, 86–90. <a href=""https://doi.org/10.3115/980845.980860"" rel=""nofollow noreferrer"">https://doi.org/10.3115/980845.980860</a></p>
<p>Kingsbury, P., &amp; Palmer, M. (2002). From treebank to propbank. In Language Resources and Evaluation.</p>
","nlp"
"106463","How to feed a Knowledge Base into Language Models?","2021-12-27 17:35:51","106506","2","591","<machine-learning><deep-learning><nlp><bert><knowledge-graph>","<p>I’m a CS undergrad trying to make my way into NLP Research. For some time, I have been wanting to incorporate &quot;everyday commonsense reasoning&quot; within the existing state-of-the-art Language Models; i.e. to make their generated output more reasonable and in coherence with our practical world. Although there do exist some commonsense knowledge bases like ConceptNet (2018), ATOMIC (2019), OpenMind CommonSense (MIT), Cyc (1984), etc., they exist in form of knowledge graphs, ontology, and taxonomies.</p>
<p>My question is, how can I go about leveraging the power of these knowledge bases into current transformer language models like BERT and GPT-2? How can we fine-tune these models (or maybe train new ones from scratch) using these knowledge bases, such that they retain their language modeling capabilities but also get enhanced through a new commonsense understanding of our physical world?</p>
<p>If any better possibilities exist other than fine-tuning, I'm open to ideas.</p>
","nlp"
"106417","Clustering text data based on sentiment?","2021-12-25 10:48:15","106423","1","1007","<python><nlp><multiclass-classification><unsupervised-learning><sentiment-analysis>","<p>I am scraping reviews off Amazon with the intent to perform sentiment analysis to classify them into positve, negative and neutral. Now the data I would get would be text and unlabeled.</p>
<p>My approach to this problem would be as following:-</p>
<p>1.) Label the data using clustering algorithms like <strong>DBScan</strong>, <strong>HDBScan</strong> or <strong>KMeans</strong>. The number of clusters would obviously be 3.</p>
<p>2.) Train a Classification algorithm on the labelled data.</p>
<p>Now I have never performed clustering on text data but I am familiar with the basics of clustering. So my question is:</p>
<p><strong>1. Is my approach correct?</strong></p>
<p><strong>2. Any articles/blogs/tutorials I can follow for text based clustering since I am kinda new to this?</strong></p>
<p><strong>PS:</strong> I am familiar with both NLP and Clustering algo's but I have never performed Clustering on text data.</p>
","nlp"
"106415","How to multi label text Classification using Deep learning","2021-12-25 09:01:13","","0","37","<deep-learning><nlp><multilabel-classification>","<p>I am new to the multi-label text classification using Deep learning,
I have Data like this:</p>
<pre><code>parent_pid   domain_tld         category_dz                            description_en
0   1000714377  douglas_de      Makeup &gt; Face &gt; Foundation              This fluid makeup provides long-lasting moistu...
1   1000753794  dm_de           Nails &gt; Nail Care &gt; Removers              The handy sample size ebelin nail polish remov...
2   1000790264  douglas_de       Nails &gt; Nail Polish                       LE VERNIS by CHANEL:  a novel, ultra-resistant ...
3   1000805273  douglas_de     Makeup &gt; Face &gt; Foundation             Clinique's superbalanced makeup controls shine...
4   1000808310  douglas_de      Makeup &gt; Lips &gt; Lip Liner           Keeps lipstick in place The Quickliner for Lip...
</code></pre>
<p>How Can I categorize the above category products using Deep learning, please any one share me any blog or references.</p>
","nlp"
"106357","extracting data from unstructured pdfs","2021-12-23 01:38:58","122651","0","780","<nlp><text-mining><ocr>","<p>I have about 200,000 PDFs made up of 20 different designs. i.e In an organization, different (20) departments issue monthly award submission requirements. Each department has its own document format. These documents are collected by the organization.</p>
<p>Now I need to extract the paragraphs, bullet points, or sentences from each of these PDFs, organize it properly, specify if it is a requirement or not (label the data), and store it in storage. This process needs to be repeatable/automated for any future PDF.</p>
<p>A lot of the pdfs are not structured, have no tags or bookmarks, have no table of content.</p>
<p>I want to know what is the best technique or method for handling this type of problem?</p>
","nlp"
"106341","How to improve language model ex: BERT on unseen text in training?","2021-12-22 10:50:35","106342","2","291","<classification><nlp><text-mining><bert><huggingface>","<p>I am using pre-trained language model for binary classification. I fine-tune the model by training on data my downstream task. The results are good almost 98% F-measure.</p>
<p>However, when I remove a specific similar sentence from the training data and add it to my test data, the classifier fails to predict the class of that sentence. For example, the sentiment analysis task</p>
<blockquote>
<p>&quot;I love the movie more specifically the acting was great&quot;</p>
</blockquote>
<p>I removed from training all sentences containing the words <strong>&quot; more specifically&quot;</strong> and surprisingly in the test set they were all misclassified, so the precision decreased by a huge amount.</p>
<p>Any ideas on how can I further fine-tune/improve my model to work better on unseen text in training to avoid the problem I described above? (of course without feeding the model on sentences containing the words <strong>&quot;more specifically&quot;</strong>)</p>
<p>Note: I observed the same performance regardless of the language model in use (BERT, RoBERTa etc).</p>
","nlp"
"106326","Treating Word Embeddings as Multivariate Gaussian Random Variables","2021-12-21 18:04:03","","1","378","<machine-learning><nlp><word2vec><doc2vec><gmm>","<p>I want to specify some probabilistic clustering model (such as a mixture model or lda) over words, and instead of using the traditional method of representing words as an indicator vector , I want to use the corresponding word embeddings extracted from word2vec, glove, etc. as input.</p>
<p>While treating word embeddings from my word2vec as an input to my GMM model, I observed that my word embeddings for each feature had a normal distribution, i.e. feature 1..100 were normally distributed for my word dictionary. Can anyone tell how that is true? In my understanding, they are word embeddings are model weights attributed from a shallow neural network. Are they always supposed to be normally distributed?</p>
<p>Furthermore, when using doc2vec word embeddings,my features were uniformly distributed? This goes against the earlier assertion that word embeddings are normally distributed. Can anyone explain this discrepancy?</p>
","nlp"
"106277","How to improve document classification between two similar documents","2021-12-20 10:41:46","","0","39","<machine-learning><classification><nlp>","<p>I have a document classification problem where I need to classify whether a certain document is about real estate or not. I get a URL of a webpage from which I extract all the text and then using my trained model which is actually LSTM based I classify whether it is about real estate property or not. Here a page with real estate property means that the page should be talking about only one property and not more.</p>
<p>My model could get one of the following kinds of input data:</p>
<ol>
<li>A URL having a unique real estate listing and talking about that specific property. e.g. <a href=""https://www.nagel-immobilien.de/angebote/immobilie/vermietete-wohnung-mit-garage-und-gartennutzung/"" rel=""nofollow noreferrer"">1</a></li>
<li>URL containing a list of real estates fulfilling a certain criteria. e.g. all the properties having 3 bedrooms, within a specific rent range. e.g. <a href=""https://www.nagel-immobilien.de/angebote/"" rel=""nofollow noreferrer"">2</a>. <em>These kind of pages I refer to as index pages</em>.</li>
<li>Just some random URL from these broker websites talking about their organization, their achievements, their teams, etc. e.g. <a href=""https://www.nagel-immobilien.de/ni-immobilien/team/"" rel=""nofollow noreferrer"">3</a></li>
</ol>
<p>Different websites show a collection or lists of real estates differently than others. Some broker websites might have a list of real estates with each real estate as a hyperlink and a text (usually title of the real estate). In scenario 2, a list item might have few more details in addition to the title of the real estate. The former classifies my model as not a real estate but the latter kind of webpages confuses and the model has a tendency to classify them as real estate. In scenario 3, the model performs again really nice, till the text size on the page is not too big. These kind of pages might talk about a sold real estate property, about their vision, etc.</p>
<p>During training my model I have removed all the stop words, punctuations, hyperlink texts, form field texts with a vocabulary size of 1000. I have not done any lemmatization.</p>
<ul>
<li>How can I improve the classification when the model has a high tendency to identify these index pages (the one having a lot many details about multiple real estates) as also a real estate?</li>
<li>Should I reduce the vocabulary size, as when the text extracted from the webpage is too big, it is identified as real estate?</li>
</ul>
<p>I referred to <a href=""https://www.tensorflow.org/text/tutorials/text_classification_rnn"" rel=""nofollow noreferrer"">this</a> link to build and train my model.</p>
","nlp"
"106265","Why do we use squeeze(1) at this model definition with PyTorch?","2021-12-19 20:12:35","","1","511","<classification><nlp><pytorch>","<p>PyTorch noobie here. I'm following an online tutorial and there's a simple model definition as follows:</p>
<pre><code>class BagOfWordsClassifier(nn.Module):
    def __init__(self, vocab_size, hidden1, hidden2):
        super(BagOfWordsClassifier, self).__init__()
        self.fc1 = nn.Linear(vocab_size, hidden1)
        self.fc2 = nn.Linear(hidden1, hidden2)
        self.fc3 = nn.Linear(hidden2, 1)
        
    def forward(self, inputs):
        x = F.relu(self.fc1(inputs.squeeze(1).float()))
        x = F.relu(self.fc2(x))
        return self.fc3(x)
</code></pre>
<p>What I don't understand here is <strong>why do we need <em>inputs.squeeze(1)</em></strong> at the <em>forawrd</em> function? When I take it out it gives me the following error:</p>
<p><em>&quot;addmm_cuda&quot; not implemented for 'Long'</em></p>
<p>If you need additional info don't hesitate to ask.</p>
","nlp"
"106214","How to precompute one sequence in a sequence-pair task when using BERT?","2021-12-17 10:22:30","106217","1","158","<deep-learning><nlp><bert><tokenization>","<p>BERT uses separator tokens ([SEP]) to input two sequences for a sequence-pair task. If I understand the BERT architecture correctly, attention is applied to all inputs thus coupling the two sequences right from the start.</p>
<p>Now, consider a sequence-pair task in which one of the sequences is constant and known from the start. E.g. Answering multiple unknown questions about a known context. To me it seems that there could be a computational advantage if one would precompute (part of) the model with the context only. However, if my assumption is correct that the two sequences are coupled from the start, precomputation is infeasible.</p>
<p>Therefore my question is:
<strong>How to precompute one sequence in a sequence-pair task while still using (pre-trained) BERT?</strong> Can we combine BERT with some other type of architecture to achieve this? And does it even make sense to do it in terms of speed and accuracy?</p>
","nlp"
"106191","validation/test set uniqueness question","2021-12-16 15:52:44","106208","0","117","<classification><nlp><cross-validation><training><text-classification>","<p>Hopefully a simple question, but it's a little unclear to me on how best to separate train/validate/test sets.</p>
<p>I have say 100 examples of class A.  I'm classifying text into either class A, which I care about, or class B, which could be any text in the world (negative class).  I have, obviously, far more examples of class B.</p>
<p>When I split the data into train/validate/test sets, is it imperative that the test set, which is not at all used in training/tuning, NOT have any examples of class A that were used in training?  In the real world (and given my limited samples), the text it will classify against will have some exact examples of class A, but not always (there could be variations - of which I do not have all of them).</p>
<p>I can ensure that the test set have unique class B text, but it is unclear if I have to also maintain completely unique class A examples in the test set, since the real world won't necessarily be like this.  Would it make sense to also have x% of class A examples from training in the test set, or should it always be 0% in the test set?</p>
","nlp"
"106186","Variable batch size for inputs of different length","2021-12-16 12:49:19","","1","313","<machine-learning><nlp><transformer><language-model><learning-rate>","<p>We're fine-tuning a GPT-2 model (using the Adam optimizer) to some posts from a social network. The length of these posts varies quite dramatically, so while some are only two tokens long, others can span hundreds of tokens. We've defined a cutoff at 256, but creating batches randomly and then padding is quite costly in terms of training time. We are now sorting the posts by length and then sampling randomly in consecutive blocks of n posts, where n is the maximum number of posts of 256 tokens that we can fit in a batch without running out of memory. But for batches of smaller posts (like n posts of length 2), this is not utilizing the resources we have and our compute time is quite limited.</p>
<p>So now we're thinking we could pack sequences together in batches such that n*length(post) remains roughly constant across batches. So one batch would be 10 sequences of length 256, while another would be 1280 sequences of length 2. But we're not sure what impact this will have on the training. It seems the learning rate is now scaled to tokens rather than posts, which actually sounds desirable with this kind of variance in number of tokens, but maybe not? Does the learning rate need to be adjusted somehow?</p>
<p>I've seen others do something similar called &quot;tensor packing&quot;, where you would concatenate all the posts and then chop them into chunks of the same size. I don't like the idea of combining posts that have nothing to do with each other like that (so the model could learn predict one from the other), but other than that this seems to do roughly the same thing regarding the learning process and the learning rate, right?</p>
","nlp"
"105153","Why tfidf of one document is not zero?","2021-12-15 17:13:41","105156","2","355","<nlp><tfidf>","<p>I'm new to nlp. Recently I wanted to do little nlp tasks, and faced strange thing.
That is I have run the following code</p>
<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer

docs = [&quot;strange event&quot;]
tfIdf_vectorizer = TfidfVectorizer(analyzer='word', tokenizer=word_tokenize,
                                   stop_words=stopwords, ngram_range=(1, 2), use_idf=True,
                                   norm='l2')
tfidf = tfIdf_vectorizer.fit_transform(docs)
print(tfidf)
</code></pre>
<p>and see the following result</p>
<pre><code>  (0, 2)    0.5773502691896258
  (0, 0)    0.5773502691896258
  (0, 1)    0.5773502691896258
</code></pre>
<p>shouldn't the tfidf of one single document be zero? (Since the IDF=log(1/1)=0)</p>
","nlp"
"105121","Word-level text generation with word embeddings – outputting a word vector instead of a probability distribution","2021-12-14 19:36:55","","0","815","<nlp><rnn><word-embeddings><text-generation>","<p>I am currently researching the topic of text generation for my university project. I decided (ofc) to go with a RNN getting a sequence of tokens as input with a target of predicting the next token given the sequence. I have been reading through a number of tutorials and there is one thing that I am wondering about. The sources I have read, regardless of how they encode the <code>X</code> sequences (one-hot or word embeddings), encode the <code>y</code> target tokens as a one-hot vector to interpret the network output as a probability distribution over all the possible tokens. This way the task is actually framed as a multi-class classification problem (eg. as in here <a href=""https://machinelearningmastery.com/develop-word-based-neural-language-models-python-keras/"" rel=""nofollow noreferrer"">https://machinelearningmastery.com/develop-word-based-neural-language-models-python-keras/</a>).</p>
<p>I am indeed planning to encode my <code>X</code> sequences into sequences of vectors mapping each token to a pre-trained word vector but I was actually thinking of designing the network to output a vector of values of the same dimension as the input vectors and map the output vector to a specific word by looking up the most similar vector within the known pre-trained vectors. As a side note, this would frame it as a regression problem, wouldn't it (since we are trying to find a vector of numbers matching the vector of the target word)?</p>
<p>My question is - is the method described in the paragraph above (outputting a word vector instead of probability distribution) known under some term or name? I doubt that nobody has thought about it before me (unless it doesn't make sense, and I am unaware of it), but a quick google search describing the method hasn't found anything useful and I'd like to learn more.</p>
","nlp"
"105103","How to evaluate triple extraction in NLP?","2021-12-14 12:58:15","105117","1","457","<machine-learning><nlp><text-mining><text-classification><stanford-nlp>","<p>I am current NLP work, I am extracting triples using triple extraction function in Stanford NLP and Spacy libraries. I am looking for a good method to evaluate how good the extraction has been? Any suggestions</p>
","nlp"
"105083","Encode a set of skills into a feature","2021-12-13 20:34:10","","2","80","<nlp><encoding>","<p>I am working with a dataset where users have a set of skills. I have more than 500 skills and I was wondering what is the best way of encoding a vector, e.g., <code>['java', 'python', 'c']</code> into a feature, so it would be possible to use the user skills as features.</p>
<p>I thought about one-hot-encoding but I am afraid of the curse of dimensionality, since we have hundreds of skills.</p>
<p>Any suggestion on how to handle this kind of scenarios?</p>
","nlp"
"105066","Issue with sarcasm detection","2021-12-13 13:53:29","","0","38","<keras><nlp><data>","<p>I am working on the Reddit dataset for sarcasm detection but the sarcastic data points(1) are showing zero percent recall, precision, and accuracy however nonsarcastic are showing 100% recall and 50 precision. Below is the confusion matrix. I don't know why is it not showing any results on sarcastic class.</p>
<p><a href=""https://i.sstatic.net/BRq9t.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/BRq9t.png"" alt=""enter image description here"" /></a></p>
","nlp"
"104909","Text classification length","2021-12-09 02:29:07","","0","324","<classification><nlp><bert><text-classification><text>","<p>I have a set of text examples I need to learn as class A, and they are of varying lengths, say 10 sentences to 1 sentence long.  I have to parse a document to find those strings of text that match one of the example texts as an example of class A.</p>
<p>What is the right/best/common way to go about handling the length of text variation of the examples vs how much text you use to compare to in a document?  That is, do I just first go through the document 10 sentences at a time (windowing) and running a prediction on each group of 10 (since some training examples were 10 sentences long), and then do it again for 9 sentence long groups, etc., down to 1?</p>
<p>Or is there something already handles arbitrary text length?</p>
","nlp"
"104902","Is binary classification the right choice in this case?","2021-12-08 17:12:53","104943","1","180","<classification><nlp><bert><text-classification><binary-classification>","<p>I am somewhat new to text classification and I have some questions if you folks can help:</p>
<p>I have some text I need to be able to classify as belonging to a single class or not (usually 1-10 sentences long each).  For the examples of the class, I have around 500 examples, but the non-class case can really be any text at all of which I have hundreds of documents with tens of thousands of sentences (which are not the class).  What I have to do is be able to classify each of the sentences in each document as belonging to the class or not.  The vast majority won't belong.</p>
<ol>
<li><p>I'm using a BERT based Binary Classifier (simpletransformer) to identify the text similar to (or exactly) the 500 class examples, does this seem reasonable/possible?</p>
</li>
<li><p>How should I deal with the class imbalance of 500 to 10000's?  I tried oversampling the minority class (my target), but it seems to overfit when I do that.</p>
</li>
<li><p>What is the usual way of handling this particular use case?  The 1-class anomaly detection doesn't seem to fit here, from what I can tell.  Is there a similar NLP style training that works for this case? Or something else?</p>
</li>
</ol>
<p>Would it make sense to just do a semantic similarity comparison of some sort?  That is, just take the class examples, and for each sentence in a document, test to see how similar it is to each class example and if the text is &quot;close enough&quot; to any of the class examples, then it's a &quot;hit&quot;?  this would seem slow...  Is there a standard/good library for semantic comparison?</p>
","nlp"
"104859","What is the difference between TextVectorization and Tokenizer?","2021-12-07 16:17:46","","3","2496","<keras><nlp><tokenization>","<p>What is the difference between the <code>layers.TextVectorization()</code> and</p>
<pre><code>from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
</code></pre>
<p>And when to use what ?</p>
","nlp"
"104815","how to extract the following keywords containing cells from all columns of csv dataset and copy to new Analysis column","2021-12-06 07:47:42","","1","149","<python><nlp><dataset><pandas><data-cleaning>","<ol>
<li>
<pre><code>  useData_list = ['Appeal Allowed','Appeal failed and
 dismissed','Appeal dismissed','petition allowed','order
  accordingly','petition dismissed','appeal dismissed without any
  order as to cost','Appeal disposed of','Appeal partly
   allowed','Petition disposed of','No costs'] 

  regstr ='|'.join(user_list)



datacontain=df[df.apply(lambdacolumn:column.astype(str).str.contains(regst).any(),axis=1)]
    df[&quot;Analysis&quot;]=datacontain.copy()
       df.to_csv('updated90.csv',index=False)
       df.to_csv('updated90.csv',index=False)

        KeyError Traceback (most    recent call last)
</code></pre>
</li>
</ol>
<p>~\anaconda3\lib\site-packages\pandas\core\indexes\base.py in get_loc(self,
key, method, tolerance)
2894             try:
-&gt; 2895                 return self._engine.get_loc(casted_key)
2896             except KeyError as err:</p>
<p>pandas_libs\index.pyx in pandas._libs.index.IndexEngine.get_loc()</p>
<p>pandas_libs\index.pyx in pandas._libs.index.IndexEngine.get_loc()</p>
<p>pandas_libs\hashtable_class_helper.pxi in pandas._libs.h
...
:
:
129         if self._validate_ndim and self.ndim and len(self.mgr_locs) != len(self.values):
--&gt; 130             raise ValueError(
131                 f&quot;Wrong number of items passed {len(self.values)}, &quot;
132                 f&quot;placement implies {len(self.mgr_locs)}&quot;</p>
<pre><code>                 ValueError: Wrong number of items passed 325, placement                       implies 1
</code></pre>
","nlp"
"104792","What is the difference between rule based & feature based methods in sentiment analysis?","2021-12-05 14:48:09","","0","843","<python><nlp><sentiment-analysis>","<p>if I use TextBlob in Python to get data label 0/1 of texts (postive/negative) and then use logistic regression for training and prediction, is this feature method or rule based method?</p>
","nlp"
"104785","How to perform entity level train-val-test split for NER task?","2021-12-05 08:08:48","","4","1057","<python><nlp><scikit-learn><named-entity-recognition>","<p>A normal and stratified split option is provided by <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"" rel=""nofollow noreferrer"">sklearn method</a> that can be used for ML problems like multi-class classification. This is relatively easier to do as (1) one sample has one class, and (2) you can split samples per class-wise to have the equal distribution of classes in train-val-test splits.</p>
<p>Now there seems to be a problem with NER (Named Entity Recognition) problem, as (1) there could be multiple entities, and also (2) each sample may have a different distribution of entities. So for example, say we have the following sample set,</p>
<pre><code>Sample 1: contains DATE, PER, ORG
Sample 2: contains DATE, PER
Sample 3: contains DATE, ORG
Sample 4: contains PER, ORG
Sample 5: contains ORG
</code></pre>
<p>Now the unique entities and their overall count are <code>DATE=3</code>, <code>PER=3</code>, and <code>ORG=4</code>. If you want to do an 80-20 train-test split (for simplicity's sake), the best option seems to be keeping <code>Sample 1</code> in the test and rest in train - as only then you will have a somewhat desired distribution of entities in the splits. On the other hand, if you select <code>Sample 5</code> as a test, for example, we won't have any <code>DATE</code> and <code>PER</code> instances in the test at all.</p>
<p>So this is my question -- what is the best practice to split the dataset at an entity level for the NER task? Do we even split at the entity level for stratification or randomly split at sample level a couple of times and pick the one with the best split at entity level?</p>
","nlp"
"104771","Is there a way to train Doc2Vec on a corpus of docs and be able to take a novel doc and see how similar it is to the trained corpus?","2021-12-04 14:10:25","","1","38","<nlp><semantic-similarity><doc2vec>","<p>I have a project idea, where I train a bunch of documents on Doc2Vec and then take a novel, input doc, and ideally be able to be told how similar it is to the docs supplied for training as a whole or how well it &quot;fits&quot; with the training docs. Is there a way to do this?</p>
","nlp"
"104629","When to use GloVe vocabulary vs. building a vocabulary from the training data?","2021-11-29 16:56:52","104656","1","579","<nlp><word-embeddings>","<p>While studying some (pytorch) examples that use pretrained GloVe vectors I came across two variants:</p>
<ol>
<li>Use the vocabulary of the GloVe vectors and thus initialize the embedding layer with the pretrained GloVe vectors.</li>
<li>Build a vocabulary from the corpus and then only use the pretrained GloVe vectors that correspond to that vocabulary to initialize the embedding layer.</li>
</ol>
<p>To me it seems that by using the vocabulary of the GloVe vectors there is a chance that some of the tokens in the training set may not have corresponding GloVe vectors and are thus excluded from the vocabulary. Therefore, you may miss out on tokens that are significant for the task.</p>
<p>On the other hand, building a vocabulary from the corpus implies that the model cannot handle unseen words (as far as I understand that correctly).</p>
<p>Therefore, I was wondering: When should one use the GloVe vocabulary vs. building a vocabulary from the training data? And would it make sense to use the &quot;union&quot; of the two vocabularies instead?</p>
","nlp"
"104628","What are the best ways to convert different parts of speech to noun?","2021-11-29 16:46:53","","1","26","<nlp><text-classification>","<p>I am working on a topic modelling task. I want to convert different parts of speech such as adjectives, verbs to noun. What are the best ways of doing this? I have tried lemmatization using NLTK library but I see this does not work for most of the words.</p>
","nlp"
"104574","Softmax in Sentiment analysis","2021-11-27 23:03:31","104576","1","232","<nlp><sentiment-analysis>","<p>I am going to do Sentiment Analysis over some tweet texts. So, in summary we have three classes: Positive, Neutral, Negative. If I apply Softmax in the last layer, I will have the probability for each class for each piece of text. we know that in Softmax:</p>
<p><code>P(pos) + P(neu) + P(neg) = 1</code></p>
<p>My question: suppose that we have a piece of text in Positive label. So, do we have to have these probabilities <strong>in this order</strong>:</p>
<p><code>P(pos) &gt; P(neu) &gt; P(neg)</code></p>
<p>What does it mean when we have them in this order:</p>
<p><code>P(pos) &gt; P(neg) &gt; P(neu)</code></p>
<p>Can we conclude anything from this? For example, can we say with confidence that the label is Positive like as before?</p>
","nlp"
"104570","Questions of understanding - Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation","2021-11-27 21:32:58","104599","1","56","<neural-network><nlp><text-classification><machine-translation><text-processing>","<p>I'm currently analysing the paper Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation (Post, Vilar 2018): <a href=""https://arxiv.org/abs/1804.06609"" rel=""nofollow noreferrer"">https://arxiv.org/abs/1804.06609</a></p>
<p>I have understanding problems how the data is processed. For example: the paper is writing about <strong>beams</strong>, <strong>banks</strong> and <strong>hypothesises</strong> and I have no idea what these terms mean.</p>
<p>How would you describe these terms and are there any tutorial sources you would recommend for understanding the <strong>dynamic beam allocation</strong>?</p>
","nlp"
"104536","BERT vs GPT architectural, conceptual and implemetational differences","2021-11-26 21:22:18","","10","8948","<machine-learning><nlp><bert><transformer><gpt>","<p>In the <a href=""https://arxiv.org/pdf/1810.04805.pdf"" rel=""noreferrer"">BERT paper</a>, I learnt that BERT is encoder-only model, that is it involves only transformer encoder blocks.</p>
<p>In the <a href=""https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf"" rel=""noreferrer"">GPT paper</a>, I learnt that GPT is decoder-only model, that is it involves only transformer decoder blocks.</p>
<p>I was guessing whats the difference. I know following difference between encoder and decoder blocks: GPT Decoder looks only at previously generated tokens and learns from them and not in right side tokens. BERT Encoder gives attention to tokens on both sides.</p>
<p>But I have following doubts:</p>
<p><strong>Q1.</strong> GPT2,3 focuses on new/one/zero short learning. Cant we build new/one/zero short learning model with encoder-only architecture like BERT?</p>
<p><strong>Q2.</strong> Huggingface <code>Gpt2Model</code> contains <code>forward()</code> method. I guess, feeding single data instance to this method is like doing one shot learning?</p>
<p><strong>Q3.</strong> I have implemented neural network model which utilizes output from <code>BertModel</code> from hugging face. Can I simply swap <code>BertModel</code> class with <code>GPT2Model</code> with some class and will it. The return value of <a href=""https://huggingface.co/transformers/model_doc/gpt2.html#transformers.GPT2Model.forward"" rel=""noreferrer""><code>Gpt2Model.forward</code></a> does indeed contain <code>last_hidden_state</code> similar to <a href=""https://huggingface.co/transformers/model_doc/bert.html#transformers.BertModel.forward"" rel=""noreferrer""><code>BertModel.forward</code></a>. So, I guess swapping out <code>BertModel</code> with <code>Gpt2Model</code> will indeed work, right?</p>
<p><strong>Q4.</strong> Apart from being decoder-only and encoder-only, auto-regressive vs non-auto-regressive and whether or not accepting tokens generated so far as input, what high level architectural / conceptual differences GPT and BERT have?</p>
","nlp"
"104519","Automatically finding business opportunities in text documents","2021-11-26 13:59:04","","1","38","<nlp><text-mining><text-classification>","<p>I am new to machine learning and NLP.</p>
<p>I am exploring the possibility of using one of these approaches to automatically examine a large collection of text documents and determine, first of all, if they talk about funding opportunities for small-medium enterprises.</p>
<p>Then, considering only the matching documents, I would need to automatically understand the (1) (who is providing the funding), the (2) what kind of business is being funded, the (3) how (how many funds), the (4) when (the deadline for applying), as well as other information of this kind.</p>
<p>Do you have any idea about what kind of automated approach, if any, can be pursued to this end?</p>
","nlp"
"104497","Creating numeric word representation of input sentences resulting in MemoryError","2021-11-25 19:43:47","","0","153","<python><neural-network><nlp><scikit-learn><decision-trees>","<p>I am trying to use <a href=""https://scikit-learn.org/stable/modules/feature_extraction.html#common-vectorizer-usage"" rel=""nofollow noreferrer""><code>CountVectorizer</code></a> to obtain word numerical word representation of data which is essentialy list of 160000 English sentences:</p>
<pre><code>import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer

df_train = pd.read_csv('data/train.csv')

vectorizer = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\b\w+\b', min_df=1)
X = vectorizer.fit_transform(list(df_train.text))
</code></pre>
<p>Then printing <code>X</code>:</p>
<pre><code>&gt;&gt;&gt; X
&lt;160000x693699 sparse matrix of type '&lt;class 'numpy.int64'&gt;'
with 3721191 stored elements in Compressed Sparse Row format&gt;
</code></pre>
<p>But converting the whole to array to get the numerical word representation of all data gives:</p>
<pre><code>&gt;&gt;&gt; X.toarray()
---------------------------------------------------------------------------
MemoryError                               Traceback (most recent call last)
~\AppData\Local\Temp/ipykernel_11636/854451212.py in &lt;module&gt;
----&gt; 1 X.toarray()

c:\users\crrma\.virtualenvs\humor-detection-2-8vpiokuk\lib\site-packages\scipy\sparse\compressed.py in toarray(self, order, out)
   1037         if out is None and order is None:
   1038             order = self._swap('cf')[0]
-&gt; 1039         out = self._process_toarray_args(order, out)
   1040         if not (out.flags.c_contiguous or out.flags.f_contiguous):
   1041             raise ValueError('Output array must be C or F contiguous')

c:\users\crrma\.virtualenvs\humor-detection-2-8vpiokuk\lib\site-packages\scipy\sparse\base.py in _process_toarray_args(self, order, out)
   1200             return out
   1201         else:
-&gt; 1202             return np.zeros(self.shape, dtype=self.dtype, order=order)
   1203 
   1204 

MemoryError: Unable to allocate 827. GiB for an array with shape (160000, 693699) and data type int64
</code></pre>
<p>For the example in the linked schikit learn <a href=""https://scikit-learn.org/stable/modules/feature_extraction.html#common-vectorizer-usage"" rel=""nofollow noreferrer"">doc page</a>, they have used only five sentences. Thus, for them <code>X.toarray()</code> seem to have returned the array of numerical word representation. But since my dataset contains 160000 sentences, (in error message) it seems that it is resulting in vocabulary of size 693699 (which contains both unique unigrams and bigrams, due to <code>ngram_range</code> parameter passed to <code>CountVectorizer</code>) and hence facing insufficient memory issue.</p>
<p><strong>Q1.</strong> How can I fix this? I am thinking to simply reject <code>X</code> and separately transform in mini batches as shown below. Is this correct?</p>
<pre><code>&gt;&gt;&gt; X_batch = list(df_train[:10].text)  # do this for 160000 / batch_size batches
&gt;&gt;&gt; X_batch_encoding = vectorizer.transform(X_batch).toarray()
&gt;&gt;&gt; X_batch_encoding
array([[0, 0, 0, ..., 0, 0, 0],
   [0, 0, 0, ..., 0, 0, 0],
   [0, 0, 0, ..., 0, 0, 0],
   ...,
   [0, 0, 0, ..., 0, 0, 0],
   [0, 0, 0, ..., 0, 0, 0],
   [0, 0, 0, ..., 0, 0, 0]], dtype=int64)

&gt;&gt;&gt; X_batch_encoding[0].shape
(693699,)
</code></pre>
<p><strong>Q2.</strong> I am thinking to train neural network and decision tree on this encoding for humor detection. But I guess it wont be great idea to have 693699 length vector to represent single sentence. Right? If yes, what should I do instead? Should I opt to use only unigrams while fitting <code>CountVectorizer</code> (but it will not capture even minimal context of words, unlike bigrams) ?</p>
<p>PS: I am creating baseline for humor detection, I am required to use <code>CountVectorizer</code>.</p>
","nlp"
"104493","How to learn common sense constants? Look in body for detail","2021-11-25 18:26:41","","2","31","<nlp><machine-learning-model><bert>","<p>If I wanted to learn constants for example week -&gt; 7 days, chicken -&gt; 2 legs, day -&gt; 24, 1km -&gt; 1000 meters hours, and so on, would it be possible to extract this information from a BERT model trained on the right dataset like Wikipedia words? If not, what model would I have to use?</p>
","nlp"
"104492","Picking the right NLP model to tag words from a dataset","2021-11-25 17:44:13","104503","3","155","<machine-learning><nlp>","<p>As the title suggests, I am posting here in the hope someone could direct me towards NLP models for tagging words.</p>
<p>To be more concrete, here is what I wish to do. I would like to build a flashcard application using an NLP model that would tag/categorize words.
So let us imagine I have a CSV file with items made of one question (in English) and one answer (in French):</p>
<pre><code>+----------------------------
| plane       | avion       |
+-------------+-------------+
| chopsticks  | baguettes   |
+-------------+-------------+
| airport     | aéroport    |
+-------------+-------------+
</code></pre>
<p>The idea is that the learners would pick a contextual deck (in this example, a deck related to travelling with planes). That deck would be generated by a tag &quot;airport&quot; made by the machine learning algorithm.</p>
<p>And thus, is there any good models I should look to?</p>
<p>Edit:</p>
<p>After much research, I came across NLU which meets many of the requirements I have described above. If you are interested, please have a look at those links: <a href=""https://datascience.stackexchange.com/questions/40492/what-is-nlp-technique-to-generalize-manually-created-rules-in-text/40500#40500"">What is NLP technique to generalize manually created rules in text?</a> and <a href=""https://datascience.stackexchange.com/questions/40973/nlp-algorithms-for-categorizing-a-list-of-words-with-specific-topics"">NLP algorithms for categorizing a list of words with specific topics</a>, as well as this repo: <a href=""https://github.com/ScarletPan/probase-concept"" rel=""nofollow noreferrer"">Probase-Concept</a></p>
","nlp"
"104488","Difference between the architectures of semantic and instance segmentation","2021-11-25 16:46:32","","1","57","<deep-learning><nlp><computer-vision><information-retrieval><information-extraction>","<p>My question is about the difference between the architectures of semantic segmentation and instance segmentation models. So, as far as I understand, a semantic segmentation model is making pixel-wise classification and, therefore, it has a dense layer at the end where the output dimension is number of labels (classes). The part that makes me confused is how instance segmentation models distinguish between the instances from same classes? How is the architecture of them?</p>
<p>Actually, I am studying on NLP and information extraction from documents. I recently trying to implement a model specified in a paper called &quot;Chargrid: Towards Understanding 2D Documents&quot; in which they do both instance and semantic segmentation and I could not understand the architecture. In the paper, they state that there are different fields like invoice number, amount, vendor name etc. Also there are line-items and there might me multiple of them. So, they say that, in order to differentiate between individual line-items they introduced a bounding-box regression branch in the decoder in addition to the semantic segmentation branch. I do not understand how bounding-box regression helps to identify individual items.</p>
","nlp"
"104455","Train a spaCy model for semantic similarity","2021-11-24 19:56:24","","1","744","<nlp><training><spacy><semantic-similarity>","<p>I'm attempting to train a spaCy model for the purposes of computing semantic similarity but I'm not getting the results I would anticipate.</p>
<p>I have created two text files that contain many sentences that use a new term, &quot;PROJ123456&quot;. For example, &quot;PROJ123456 is on track.&quot;</p>
<p>I've added each to a <code>DocBin</code> and saved them to disk as train.spacy and dev.spacy.</p>
<p>I'm then running:
<code>python -m spacy train config.cfg --output ./output --paths.train ./train.spacy --paths.dev ./dev.spacy</code></p>
<p>The config.cfg file contains:</p>
<pre><code>[paths]
train = null
dev = null
vectors = null
init_tok2vec = null

[system]
gpu_allocator = null
seed = 0

[nlp]
lang = &quot;en&quot;
pipeline = [&quot;tok2vec&quot;,&quot;parser&quot;]
batch_size = 1000
disabled = []
before_creation = null
after_creation = null
after_pipeline_creation = null
tokenizer = {&quot;@tokenizers&quot;:&quot;spacy.Tokenizer.v1&quot;}

[components]

[components.parser]
factory = &quot;parser&quot;
learn_tokens = false
min_action_freq = 30
moves = null
scorer = {&quot;@scorers&quot;:&quot;spacy.parser_scorer.v1&quot;}
update_with_oracle_cut_size = 100

[components.parser.model]
@architectures = &quot;spacy.TransitionBasedParser.v2&quot;
state_type = &quot;parser&quot;
extra_state_tokens = false
hidden_width = 128
maxout_pieces = 3
use_upper = true
nO = null

[components.parser.model.tok2vec]
@architectures = &quot;spacy.Tok2VecListener.v1&quot;
width = ${components.tok2vec.model.encode.width}
upstream = &quot;*&quot;

[components.tok2vec]
factory = &quot;tok2vec&quot;

[components.tok2vec.model]
@architectures = &quot;spacy.Tok2Vec.v2&quot;

[components.tok2vec.model.embed]
@architectures = &quot;spacy.MultiHashEmbed.v2&quot;
width = ${components.tok2vec.model.encode.width}
attrs = [&quot;ORTH&quot;,&quot;SHAPE&quot;]
rows = [5000,2500]
include_static_vectors = true

[components.tok2vec.model.encode]
@architectures = &quot;spacy.MaxoutWindowEncoder.v2&quot;
width = 256
depth = 8
window_size = 1
maxout_pieces = 3

[corpora]

[corpora.dev]
@readers = &quot;spacy.Corpus.v1&quot;
path = ${paths.dev}
max_length = 0
gold_preproc = false
limit = 0
augmenter = null

[corpora.train]
@readers = &quot;spacy.Corpus.v1&quot;
path = ${paths.train}
max_length = 0
gold_preproc = false
limit = 0
augmenter = null

[training]
dev_corpus = &quot;corpora.dev&quot;
train_corpus = &quot;corpora.train&quot;
seed = <span class=""math-container"">${system.seed}
gpu_allocator = $</span>{system.gpu_allocator}
dropout = 0.1
accumulate_gradient = 1
patience = 1600
max_epochs = 0
max_steps = 20000
eval_frequency = 200
frozen_components = []
annotating_components = []
before_to_disk = null

[training.batcher]
@batchers = &quot;spacy.batch_by_words.v1&quot;
discard_oversize = false
tolerance = 0.2
get_length = null

[training.batcher.size]
@schedules = &quot;compounding.v1&quot;
start = 100
stop = 1000
compound = 1.001
t = 0.0

[training.logger]
@loggers = &quot;spacy.ConsoleLogger.v1&quot;
progress_bar = false

[training.optimizer]
@optimizers = &quot;Adam.v1&quot;
beta1 = 0.9
beta2 = 0.999
L2_is_weight_decay = true
L2 = 0.01
grad_clip = 1.0
use_averages = false
eps = 0.00000001
learn_rate = 0.001

[training.score_weights]
dep_uas = 0.5
dep_las = 0.5
dep_las_per_type = null
sents_p = null
sents_r = null
sents_f = 0.0

[pretraining]

[initialize]
vectors = &quot;en_core_web_lg&quot;
init_tok2vec = ${paths.init_tok2vec}
vocab_data = null
lookups = null
before_init = null
after_init = null

[initialize.components]

[initialize.tokenizer]
</code></pre>
<p>I get a new model in <code>output/model-last</code>.</p>
<p>I then run the following file:</p>
<pre><code>import spacy
nlp = spacy.load(&quot;./output/model-last&quot;)
print(nlp('PROJ123456').vector)
</code></pre>
<p>I'm expecting to see a vector with some non-zero values but instead I see a vector of 300 zero values. I take that to indicate it hasn't added &quot;PROJ123456&quot; to the vocab. But I'm not sure why.</p>
","nlp"
"104403","Model doesn't know German well enough","2021-11-23 14:19:07","","1","37","<nlp><unsupervised-learning><transformer>","<p>I have a model that generates questions and answers based on input text. The texts are in German and based on observations it seems like the model doesn't know German well enough.</p>
<p>I need to pretrain the T5 transformer &quot;better&quot;, so I was thinking what possibilities I have: is there a better version of T5? A German pre-trained version? Isn't multilingual mT5 too much since I'm only interested in German?</p>
<p>And last but not least, can't I just use my current code for pretraining and use the c4 dataset in German to make it better?</p>
","nlp"
"104359","Any transformer model (NLP) for code classification?","2021-11-22 10:32:02","","1","19","<classification><nlp><transformer><text-classification><code>","<p>Does any Transformer (NLP) that is suitable for code classification tasks exist?</p>
<p>For example, I have a lot of source codes of various categories (driver, game, email client, etc.). I want to distinguish (classify) these types using such a model.</p>
<p>Many thanks for considering my question.</p>
","nlp"
"104356","I'm looking for some scripts or tools that can take some text, understand and generate multichoice questions, cloze deletion, pop quiz","2021-11-22 08:06:29","","1","13","<machine-learning><nlp><text-mining>","<p>I'm a fairly entry level coder in python but I was hoping for some guidance on if, or how I could load a block of text to generate random questions to test comprehension.</p>
<p>My goal is to conduct a large reading goal, there are often book summaries and pdfs so if I can drop the text into a tool I can generate pop quizzes / Anki flash cards, that is my ultimate goal.</p>
","nlp"
"104331","How to extract numerical information from text descriptions","2021-11-21 05:48:17","","1","118","<machine-learning><nlp><text-mining><ai><stanford-nlp>","<p>I have an attribute that is the description of an operation (i.e description of a building consent), I need to translate this to a mathematical operation. I need to find out the new number of dwelling that is going to build, and I have to ignore any other operation. I am not sure how to tackle this problem. I can do Regex, and do lots of searches but there should be a smarter way (is there???) by using machine learning/text mining/NLP(Stemming and lemmatization) but I am not sure where to start and how to approach this problem.</p>
<p>Below examples show a few cases of the description and the mathematical operation(i.e. the number of new unit):</p>
<p>building of a new unit-&gt;1 unit</p>
<p>building of a new garage-&gt;0 unit (the garage is not a dwelling)</p>
<p>demolishing of the existing unit and building a new one-&gt;0 unit (no changes in the total number of dwellings)</p>
<p>construction of an additional unit-&gt;1 unit</p>
<p>destroying the old building and building another one-&gt;0 unit (no changes in the total number of dwellings)</p>
<p>divining the land into two sub lots and building two new dwellings-&gt;2 units</p>
","nlp"
"104273","Is there a ubiquitous web crawler that can generate a good language-specific dataset for training a transformer?","2021-11-18 19:04:01","","0","65","<nlp><gpt><crawling>","<p>It seems like a lot of noteworthy AI tools are being trained on datasets generated by web crawlers rather than human-edited, human-compiled corpora (Facebook Translate, GPT-3). In general, it sounds more ideal to have an automatic and universal way of generating a dataset.</p>
<p>Is there any ubiquitous web crawler which does basically the same thing as Common Crawl but has a parameter for “language sought”? In other words, generate a web-crawled dataset in language X?</p>
<p>(Background: I’d like to create a language dataset in any language, then train a lemmatizer on it, a function that can lemmatize words in that language.)</p>
","nlp"
"104233","Is there an AI function that can provide the definition of a word or phase with reasonably good accuracy?","2021-11-17 15:47:33","","1","18","<nlp><gpt>","<p>I would like to make use of a software function which can provide the definition of a word or phrase. These words and phrases are in the realm of common knowledge - objects like &quot;DVD player&quot;, or specific places like &quot;Canary Islands&quot;.</p>
<p>I am pretty sure GPT-3 can do this because it's trained on the internet in general and Wikipedia, and it produces generally fluent language.</p>
<p>However, I was curious if someone has already written this function and provided it in some software library somewhere already.</p>
","nlp"
"104209","How does amazon's reviews that mention extracts topics from reviews?","2021-11-17 03:12:53","104225","3","241","<nlp><topic-model><real-ml-usecase>","<p>Amazon product page contains a section called <code>Reviews that mention</code>. The section lists the main things that users liked or dislike about the product. For example see
<a href=""https://www.amazon.in/SeCro-USB-Audio-Sound-Card/dp/B07WSBKPXX/ref=sr_1_4?crid=32JLLN4AVF67&amp;keywords=usb+hub+%2B+sound+card&amp;qid=1636868635&amp;sprefix=usb+hub+%2B+%2Caps%2C339&amp;sr=8-4"" rel=""nofollow noreferrer"">this page</a>. How exactly does it work?</p>
<p>This can be done using topic modelling using LDA. But this approach has several drawback.</p>
<ul>
<li><p>You need to choose number of topics upfront. But in amazon reviews number of topics vary for each product. Number of topics are not the same even for products that belong to same category.</p>
</li>
<li><p>You need to give friendly name to each topic. With so many products its unlikely that amazon does that.</p>
</li>
</ul>
<p>What approach would be suitable to do this in completely unsupervised way, without the drawbacks mentioned above.</p>
","nlp"
"104184","NLP conversation data - Pre-processing steps","2021-11-15 22:17:10","","0","174","<nlp><text-mining><preprocessing><text>","<p>I have text data, so data that has been transcribed from conversations from employees to customers. So each call has a recording that has been transcribed to text.</p>
<p>I am looking to do some analytics with the text data.</p>
<p>My main question, which I think is the most important, what would be the first pre-processing step to use with the text data in order to make it usable from an analytics perspective, possibly using supervised machine learning.</p>
<p>Do you use word to vector, sentence to vector, paragraph to vector etc?</p>
<p>Possible to provide links to examples etc.</p>
<p>Thank you in advance.</p>
","nlp"
"104180","GloVe dot product optimized for non-comutative data whilst the operation itself being commutative","2021-11-15 20:25:19","","1","36","<nlp><word-embeddings><word2vec><mathematics><stanford-nlp>","<p>To my current knowledge, GloVe word vectors dot product are optimized to be the</p>
<p>w_i ⋅ w_j = log⁡(P(ⅈ|j))</p>
<p>The probability being computed from a cooccurance matrix. However, dot product is a commutative operation, whilst the log probablity isn't. Is this issue being adressed in GloVe? Am I missing something?</p>
","nlp"
"104179","Is the Transformer decoder an autoregressive model?","2021-11-15 18:36:18","","12","12406","<nlp><transformer>","<p>I have been trying to find an answer to these questions, but I only find conflicting information. Is the transformer as a whole autoregressive or not? And what about the decoder? I understand that the decoder during inference proceeds autoregressively, but I am not sure about during training time.</p>
<p>Here are posts saying that the Transformer is not autoregressive:</p>
<p><a href=""https://datascience.stackexchange.com/questions/93144/minimal-working-example-or-tutorial-showing-how-to-use-pytorchs-nn-transformerd"">Minimal working example or tutorial showing how to use Pytorch&#39;s nn.TransformerDecoder for batch text generation in training and inference modes?</a></p>
<p>Here are some saying that it is:</p>
<p><a href=""https://datascience.stackexchange.com/questions/81727/what-would-be-the-target-input-for-transformer-decoder-during-test-phase?rq=1"">What would be the target input for Transformer Decoder during test phase?</a></p>
<p><a href=""https://www.tensorflow.org/text/tutorials/transformer"" rel=""noreferrer"">https://www.tensorflow.org/text/tutorials/transformer</a></p>
<p><a href=""https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0"" rel=""noreferrer"">https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0</a></p>
<p><a href=""https://huggingface.co/transformers/summary.html#seq-to-seq-models"" rel=""noreferrer"">https://huggingface.co/transformers/summary.html#seq-to-seq-models</a></p>
","nlp"
"104178","Which type of models generalize better, generative or discriminative models?","2021-11-15 17:03:20","104185","2","360","<nlp><generative-models><generalization>","<p>In NLP, which type of models (generative or discriminative) is more sensitive to the amount of data to generalize better? references?</p>
<p>This is related to the way those two types capture the data probability (join-prob. vs conditional prob.)?</p>
","nlp"
"104171","How to use pre-trained word2vec model generated by Gensim with Convolutional neural networks (CNN)","2021-11-15 15:31:58","104373","2","1241","<nlp><convolutional-neural-network><word2vec><text-classification>","<p>I have generated a pre-trained word2vec model using the Gensim framework (<a href=""https://radimrehurek.com/gensim/auto_examples/index.html#documentation"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/auto_examples/index.html#documentation</a>). The dataset has 507 sentiments(sentences) which are labeled as positive or negative. After performing all text processing, I used Gensim to generate the pre-trained word2Vec model. the model has 234 unique words with each vector having 300 dimension. However, I have a question.</p>
<p>How can I use the generated word2vec embedding vectors as input to CNN?</p>
","nlp"
"104141","Can I get some help on how to train a Word2Vec Model on a dictionary?","2021-11-14 20:00:16","","1","23","<nlp><word2vec>","<p>I doing a project, where I'm ingesting student resumes with Word2Vec, and then I need to find the best applicant for a project position. So I have a table with a column for the applicant ID and for the resume in text. But I can't seem to find a way to train the model so that it associates the information in the resume to its corresponding ID.</p>
","nlp"
"104140","Is PositionalEncoding needed for using Transformer models correctly?","2021-11-14 19:40:05","","0","25","<deep-learning><nlp><transformer>","<p>I am trying to make a model that uses a <code>Transformer</code> to see the relationship between several data vectors but the order of the data is not relevant in this case, so I am not using the <code>PositionalEncoding</code>.</p>
<p>Since the performance of models using <code>Transformers</code> is quite improved with the use of this part do you think that if I remove that part I am breaking the potential of <code>Transformers</code> or is it correct to do so?</p>
","nlp"
"104087","How to obtain vector representation of phrases using the embedding layer and do PCA with it","2021-11-12 18:27:15","104165","1","1053","<machine-learning><neural-network><nlp><word-embeddings><pca>","<p>I am trying to understand from both a conceptual and a Python code point of view, how to represent phrases that are present in a corpus (that is used to train a neural network to <em>classify</em> phrases) as vectors and how to do PCA with them.</p>
<p>Consider that I do not want to use Word2Vec embedding and that I want only to extract the vectors from the embedding layer of my neural network.</p>
<p>The example I chose to understand how to do this is the following:</p>
<pre><code>import numpy as np
from keras.preprocessing.text import one_hot, Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers.embeddings import Embedding
# define documents
docs = np.array(['Well done!',
        'Good work',
        'Great effort',
        'nice work',
        'Excellent!',
        'Weak',
        'Poor effort!',
        'not good',
        'poor work',
        'Could have done better.'])
# define class labels
labels = np.array([1,1,1,1,1,0,0,0,0,0])
# train the tokenizer
vocab_size = 15
tokenizer = Tokenizer(num_words=vocab_size)
tokenizer.fit_on_texts(docs)

# encode the sentences
encoded_docs = tokenizer.texts_to_sequences(docs)
# pad documents to a max length of 4 words
max_length = 4
padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')
print(padded_docs)
# define the model
model = Sequential()
model.add(Embedding(vocab_size, 2, input_length=max_length, name='embeddings'))
model.add(Flatten())
model.add(Dense(1, activation='sigmoid'))

# compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# fit the model
model.fit(padded_docs, labels, epochs=50, verbose=2)
</code></pre>
<p>It is a classifier MLP so I define the class labels corresponding to each sentence of the docs, I assign to each word an integer based on the tokenizer module, I prepare all my sequences of words to have all the same length because keras likes to work in this way, and then I finally define, compile and fit the model. After fitting the model, I can  extract the weights of my embedding layer with the following line of code:</p>
<pre><code># save embeddings
embeddings = model.get_layer('embeddings').get_weights()[0]
</code></pre>
<p>That is a 2D array with 2 dimensional embedding space (as chosen by me):</p>
<pre><code>array([[-0.02900218, -0.02272025],
       [-0.03750041,  0.08604637],
       [ 0.00261297,  0.06689994],
       [ 0.06822112, -0.07083904],
       [ 0.042956  ,  0.00642773],
       [-0.01934443, -0.03651911],
       [ 0.02451712,  0.02507548],
       [ 0.01995835,  0.03889224],
       [ 0.01348991,  0.01143651],
       [ 0.02176871,  0.01283678],
       [-0.04610137, -0.04942843],
       [-0.02342983, -0.07704163],
       [-0.08990634, -0.06908827],
       [ 0.07353339, -0.06115208],
       [-0.06146053,  0.09602208]], dtype=float32)
</code></pre>
<p>At this point, I have two huge difficulties:</p>
<ul>
<li><em>How to represent each phrase of the corpus with the embedding weights and so as a vector</em>: based on the nice answer to <a href=""https://stats.stackexchange.com/questions/270546/how-does-keras-embedding-layer-work"">this</a> question, I suppose that first of all I have to check which are the integers assigned to each word and I can do it with:</li>
</ul>
<pre><code>print(encoded_docs)
</code></pre>
<p>That gives me the following representation:</p>
<pre><code>[[6, 2], [3, 1], [7, 4], [8, 1], [9], [10], [5, 4], [11, 3], [5, 1], [12, 13, 2, 14]]
</code></pre>
<p>Then I assign to each integer the embedding weights of the trained network printed before and so I obtain:</p>
<pre><code>X=np.array([[[[ 0.02451712,  0.02507548], [ 0.00261297,  0.06689994]], [[ 0.06822112, -0.07083904], [-0.03750041,  0.08604637]], [[ 0.01995835,  0.03889224], [ 0.042956  ,  0.00642773]], [[ 0.01348991,  0.01143651], [-0.03750041,  0.08604637]],  [ 0.02176871,  0.01283678], [-0.04610137, -0.04942843], [[-0.01934443, -0.03651911], [ 0.042956  ,  0.00642773]], [[-0.02342983, -0.07704163],  [ 0.06822112, -0.07083904]], [[-0.01934443, -0.03651911], [-0.03750041,  0.08604637]], [[-0.08990634, -0.06908827], [ 0.07353339, -0.06115208], [ 0.00261297,  0.06689994], [-0.06146053,  0.09602208]]]])
</code></pre>
<p>Is it correct to say that X contain the vector representation of all the words of my docs? Furthermore, if yes or in any case is there a function in Python that allows you to get it ? Or one should implement it from scratch? Then, I ignored the fact that the sequences were padded with zeros. Should I add zeros and make all my vectors (of the 2D array) as 4 dimensional vectors in order to represent each word properly ?</p>
<ul>
<li><em>Once I obtain my vector representation of each word in the docs, how do I do the PCA representation of the 2D array that I obtain?</em> What are samples and what are variables? In theory, I should obtain a plot in which the data labeled as 1 cluster together and the data labeled as 0 cluster together thanks to the fact that they are now given by the weights which are obtained by training the classifier neural network.</li>
</ul>
<p>I hope I'm not too out of the way with everything.</p>
<p>Thank you in advance.</p>
<p>P.S.: please, if you downvote the question give me the reason of your downvoting. It's not a question I threw there but there was some research effort.</p>
","nlp"
"104022","How to extract details (educational details, exp details etc.) from a resume?","2021-11-10 18:41:15","","0","5714","<python><nlp><information-retrieval><spacy>","<p>I am trying to build a resume parser which can extract details such as Name, Address, Education details (degree name, college name, university name, course duration), Experience details (designation, company name, company location, work duration) from any kind of resume.</p>
<p>I tried to train a custom ner model using spacy. For that I created annotations from resumes which have entities as follows:</p>
<p>Degree -&gt; Degree name, College -&gt; College name, University -&gt; University name, Degree_date -&gt; Degree date.</p>
<p>Similarly created entities for experience too.</p>
<p>So i extracted text from the resume, for preprocessing I have done:</p>
<ul>
<li>Removed new lines, extra spaces, html tags.</li>
<li>Then removed special symbols such as bullet symbols etc.</li>
<li>Also encoded to ascii format so that some other kind of symbols will be removed</li>
</ul>
<p>The resultant text is used to annotate the entities.</p>
<p>Then I trained the model but it is not working as expected. It cannot extract all the details and sometimes the entities are wrongly detected.</p>
<p>Rule based extractor cannot be considered.</p>
<p>I want to know:</p>
<ol>
<li>Why my custom ner model is not extracting properly and not able to extract the text in the order as in resume.</li>
<li>Any other possibility is there?</li>
<li>Is it possible to use bert for this? If so, how should i structure the annotaions or in what format should i create the dataset for training bert?</li>
<li>If there is any other approach, please specify that too?</li>
</ol>
<p>Any help or suggestion will be greatly appreciated.</p>
","nlp"
"104014","Cloud-based visual tool to perform NLP on text corpora","2021-11-10 15:19:26","","1","22","<nlp><tools><corpus>","<p>I have some text corpora to share with non-programming clients (~50K documents, ~100M tokens) who would like to perform operations like regex searches, co-locations, Named-entity recognition, and word clusters. The tool <a href=""https://www.laurenceanthony.net/software/antconc/"" rel=""nofollow noreferrer"">AntConc</a> is nice and can do some of these things, but comes with severe size limitations, and crashes on these corpora even on powerful machines.</p>
<p>What cloud-based tools with a web interface would you recommend for this kind of task? Is there an open-source tool or a cloud service at a reasonable cost (&lt;$1,000 for the volume of data indicated above)?</p>
","nlp"
"103968","Unstructured data not template based to structured data","2021-11-09 10:59:25","","1","52","<nlp><data-mining><text-mining><computer-vision>","<p>I am working on a project where my goal is to extract data from a large diversity of pdf <strong>that does not follow a template</strong> (i.e unstructured data not template based). My ORC part works well and now I am trying to extract data from the raw text. During my research I have mainly seen data extraction from unstructured template based data.</p>
<p>The only things I see for now that could work, would be to use a regex, for example, to find a date, however, I don't really know what the date is related to, or to use NER, but I don't really see how to start.</p>
<p>I am not limited to NLP. I can also use computer vision on the pdf or other methods.</p>
<p>example of input:
<a href=""https://i.sstatic.net/XFXOz.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/XFXOz.jpg"" alt=""enter image description here"" /></a></p>
<p>example of output:</p>
<pre><code>{
date : &quot;27/06/2019&quot;,
place : &quot;saint-paul&quot;,
patient-name : Liliane marie SUZANNE&quot;
patient-birthdate : &quot;14/08/1957&quot;,
doctor : &quot;METE&quot;,
type : &quot;radiologie du thorax&quot;,
...
}
</code></pre>
<p>Any informations or solutions is welcome.</p>
","nlp"
"103895","Ways to cluster word senses with word embeddings","2021-11-07 07:01:45","","2","184","<nlp><word-embeddings><word2vec><bert>","<p>I'm trying to <em><strong>semantically cluster polysemous words</strong></em> or word with different meanings in a corpus for my class study and I want to do it by <em>word embeddings</em> but I have no Idea how to reach to the clustered target that I want. (a similar target that I'm looking for is posted below as an image)</p>
<p><strong>What I have</strong>: <em>a corpus</em></p>
<p><strong>What I want</strong>: <em>clustering of K frequent words with other most related semantically similar words in a corpus</em> considering that these words have multiple senses.</p>
<blockquote>
<p>For example: suppose word <em>cell</em> is repeated 5000 times in a corpus, here are some sentences:  &quot;There are many organelles in a biological cell&quot; , &quot;He went to prison cell&quot; and<br />
&quot;we are running out of cell phones&quot;, in every sentence we are receiving <em>contextually a different meaning from cell</em>, respectively, <em>blood cell</em>, <em>prison</em> and <em>mobile/phone</em>.</p>
</blockquote>
<p>So I want to cluster each word [for example <em>cell</em> in here] with their <em>semantically similar words</em>. (<em>sometime similar words are synonyms</em>)</p>
<p><strong>What I've done</strong>:</p>
<ul>
<li>preprocessing corpus for finding K frequent words.</li>
<li>As each meaning of word is contextualized to the correspondent sentence, I thought we could compare BERT vectors of those sentences with other sentences, but the problem is Bert compare vector to vector and different senses are dependent on their sentence but I don't know how I should correctly locate semantically similar words in sentences that are compared to the first sentence.</li>
</ul>
<p>I searched for related papers, there was a <em>WordNet</em> that seems to be similar but it is not constructed with word embedding methods.</p>
<blockquote>
<p>Although there are word embeddings like <code>GloVe</code>, <code>FastText</code>, <code>Word2Vec</code> that can bring us similar words but the context would be ignored and I didn't find anywhere that they can work <em>semantically</em>!</p>
</blockquote>
<p>And at last there was an <code>ELMo</code>, ELMo word representations take the entire input sentence into equation for calculating the word embeddings. Hence, the term <em>&quot;cell&quot;</em> would have different ELMo vectors under different contexts but it is still categorizing the words by their context and not putting Words that are semantically similar in a category, this way I have no Idea how to cluster different meanings by their semantically similar words.</p>
<p>Plus I've checked the papers and <strong>WSD</strong> is not the thing I want, maybe <code>Word Sense Induction</code> clustering` seems to be more accurate but still not exact.</p>
<p>Here is a photo of WordNet with word <strong>Search</strong>, kindda similar to the thing I want. (you can see each cluster of word <em>search</em> is connected semantically to their similar group)</p>
<p><a href=""https://i.sstatic.net/IRsry.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/IRsry.jpg"" alt=""WordNet"" /></a></p>
<hr />
<p>I'm asking here to get more ideas or maybe my intuitions are all wrong.</p>
<p>Thanks for your time.</p>
<p>Any information would be helpful and appreciated.</p>
","nlp"
"103815","What does the term ""seed lexicon"" means?","2021-11-04 14:51:28","","1","308","<nlp><language-model><research>","<p>I am reading a research paper (NLP) and found the phrase &quot;seed lexicon&quot;.</p>
<p>Could someone please explain it in detail?</p>
<p>Edit :</p>
<p>A sample paper
<a href=""https://aclanthology.org/2020.osact-1.17.pdf"" rel=""nofollow noreferrer"">Leveraging Affective Bidirectional Transformers for Offensive Language
Detection</a></p>
<p>Check 3rd page right column 5th line.</p>
","nlp"
"103768","Combine multiple vector fields for approximate nearest neighbor search","2021-11-03 10:51:03","","1","217","<nlp><vector-space-models><ann>","<p>I have multiple vector fields in one collection. My use-case is to find similar sentences in similar contexts. The sentences and contexts are encoded to float vectors. Therefore, I have one vector for the sentence and another vector for the context (surrounding text). I would like take both vectors in consideration to find similar sentences. Unfortunately, most approximate nearest neighbor (ann) search libraries only support to search for one field. I have tried to use PostgreSQL with the cube extension to filter by multiple vector similarities. Unfortunately, the number of vectors (100M) are too high for PostgreSQL.</p>
<p><strong>Questions:</strong></p>
<ol>
<li>Is there a possibility to combine multiple vector fields for approximate nearest neighbor search?</li>
<li>Is it also possible to weight the relevance of each vector field for the search?</li>
</ol>
","nlp"
"103758","Problem Direction - Text Data - Conversation Classification","2021-11-03 08:41:59","","0","41","<classification><nlp><text-mining><text-classification><text>","<p>The problem</p>
<p>I have a problem where I have text data that has been transcribed from a conversation. These conversations have been marked as a pass or fail in terms of compliance by a person, ie they have listened to the call and marked the call as passed compliance or not passed. I am trying to think of a way to use text data in regard to analytics. The conversations are just text data from a staff member to a customer, so I was thinking of trying to turn this into a learning problem to try and score conversations in terms of the risk. To explain further, score the conversation as the probability that the call passed or failed based on the text data.
My Thoughts</p>
<p>I was thinking of trying to create a model that can score new conversations based on previously labeled data as we have calls that have already been scored as a pass or fail. I am just not sure if this solution is possible given the current technology stack.</p>
<p>My Questions-</p>
<ul>
<li>I assume the main question is, is there a correlation between the conversation text and a call not passing compliance. How would I be able to calculate this correlation?</li>
<li>Would this be possible, what type of data pre-processing would need to be done.</li>
<li>I think as its text data the data pre-processing is the most important part. Information on the pre-processing steps would be much appreciated.</li>
<li>Is there an example online that can be used as a reference?</li>
<li>Would there be any other analytics that would be more useful? ie. a different type of solution.</li>
<li>what size dataset would be suitable, just to get started.</li>
</ul>
<p>If possible can you please add links to articles, tutorials, books, etc.?</p>
","nlp"
"103735","Methods for finding characteristic words for a group of documents in comparison to another group of documents?","2021-11-02 13:32:08","103794","1","167","<nlp><text-mining>","<p>I'm working on a problem of anomaly detection, where at the end of the anomaly detection I will have a group of documents consisting of a title of each object that was flagged as anomalous.</p>
<p>At the same time I have another group of documents which are the documents/texts of a title of each object that was <strong>not</strong> flagged as anomalous.</p>
<pre><code>anomalous_titles =[[Product:A - sub_group:X1 - pod: P1 - function: M1], [Product:B...],..]
not_anomalous_titles =[[Product:R - type:TX - producer: XX], [Product:B...],..]
</code></pre>
<p>What I would like to do here is to understand if there are any words or patterns that is shared among the anomalous documents which are not common in the group of non anomalous documents.</p>
<p>What method would be good to apply in this scenario? I know about TF-IDF and Topic Modelling, but I don't know if it makes sense for this use case?</p>
<p>Appreciate any input!</p>
","nlp"
"103719","How to consider the effect of exclamation marks in sentiment analysis","2021-11-02 09:13:34","","2","252","<python><nlp><sentiment-analysis>","<p>For example in the following two tweets, we can see the first one seems to be more negative than the second one:</p>
<blockquote>
<ol>
<li><p>&quot;You are not required to come here now!!!&quot;</p>
</li>
<li><p>&quot;You are not required to come here now&quot;</p>
</li>
</ol>
</blockquote>
<p>What are some ways to count the effect of exclamation marks so that we get better results?</p>
<p>In the following tweet:</p>
<blockquote>
<p>&quot;He is angry!!!&quot;</p>
</blockquote>
<p>We can keep &quot;angry&quot; two times, i.e., the tweet becomes:</p>
<blockquote>
<p>&quot;He is angry angry.&quot;</p>
</blockquote>
<p>So suppose we are using positive/negative frequency model for sentiment analysis, then it's more likely that the word &quot;angry&quot; will be considered negatively because of its higher frequency in negative tweets.</p>
","nlp"
"103717","Machine translation transformer output - ""unknown"" tokens?","2021-11-02 08:46:45","107819","1","337","<nlp><transformer>","<p>Cross post from my original post in Stackoverflow: <a href=""https://stackoverflow.com/questions/69595863/machine-translation-transformer-output-unknown-tokens"">https://stackoverflow.com/questions/69595863/machine-translation-transformer-output-unknown-tokens</a></p>
<p>Based on the feedback , I have now updated my approach to use WordPiece from Huggingface's pretrained BERT tokenizers. However, I still run into &quot;unk&quot; tokens when translating. I am curious why that still happens? I thought that WordPiece would try to decode without outputting any &quot;unk&quot; tokens.</p>
<p>This is how I tokenized my data, I am using German to english for the translation task.</p>
<pre><code>from transformers import BertTokenizer
bert_tokenizer_en = BertTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
bert_tokenizer_de = BertTokenizer.from_pretrained(&quot;bert-base-german-cased&quot;,do_lower_case=True)
</code></pre>
<p>Example out:</p>
<pre><code>ground truth = ['a', 'girl', 'in', 'a', 'jean', 'dress', 'is', 'walking', 'along', 'a', 'raised', 'balance', 'beam', '.']

predicted = ['a', 'girl', 'in', 'a', '&lt;unk&gt;', 'costume', 'is', 'jumping', 'on', 'a', 'clothesline', '.', '&lt;eos&gt;']
<span class=""math-container"">````</span>
</code></pre>
","nlp"
"103694","Machine learning roadmap not for beginners","2021-11-01 18:06:33","103754","3","252","<machine-learning><nlp><reinforcement-learning><computer-vision><unsupervised-learning>","<p>To introduce myself:
I know what is RL, know some RL algorithms such as PPO, A2C. Know about offline RL, online RL. I have read many papers about RL. Such as MuZero, AplhaZero, Decision Transformer and etc. I also know much about supervised learning. Know many architectures from MPL to modern Transformers. I can solve many tasks. I have read many papers about supervised learning. Such as DETR, VIT, T5, BERT, GPT-3 and many others. I know some things from unsupervised learning. I have read some papers such as DINO, UP-DETR, SIMCLR, and etc. Yesterday i knew about SwAV. I gonna read paper. And learn more about clustering in unsupervised learning. To create Neural Nets i use pytorch.
<br><br>
I'm student at university. I want to be professional AI researcher. And want to work at FAIR.<br>
<strong>Question:</strong><br>
What should i learn next? Which skills i have to have to be AI researcher?
Please give full roadmap from my current level to level of researcher.
Thank you.</p>
","nlp"
"103677","Is it possible to target a specific output length range with BART seq2seq?","2021-11-01 08:52:19","","1","441","<nlp><transformer><sequence-to-sequence>","<p>I'm currently working on an extractive summary model based on Facebook's BART model. Consistent absolute length output would be highly desirable. The problem is that input length may vary wildly. That is to say, creating the training data, the instructions look like this:</p>
<ol>
<li>Take the input text (a news article) and start (recursively) deleting examples, excess details, unnecessary background information, quotes, etc.</li>
<li>Once your summary has less than 90 words, stop deleting stuff.</li>
<li>Fix up the text format to match the style guide.</li>
</ol>
<p>The large BART model available on Huggingface was fine-tuned on 200 samples. All 200 samples had 60-88 words as the output sequence length. However, the model predicted outputs with lengths varying from 50 to 105 words, with some outliers as high as 120 words.</p>
<p>Now I'm questioning whether just throwing more samples at the problem will actually fix it. Since the model is doing very well following the style guide, I don't want to give up on this approach. The outputs which are too long can be eliminated by cranking up the length penalty. But that would make the &quot;too short&quot; case even more prevalent.</p>
<p>Can fine-tuning achieve a tighter range of output lengths just by specifying more examples? Or is there perhaps a more &quot;hacky&quot; solution to penalize output lengths not in the range?</p>
","nlp"
"103667","How can I balance sentence data for NLP tasks","2021-10-31 23:01:38","","0","148","<machine-learning><nlp><data-cleaning><svm><preprocessing>","<p>I have been given a task to train the SVM model on <a href=""https://huggingface.co/datasets/conll2003"" rel=""nofollow noreferrer"">conll2003 dataset</a> for Named Entity &quot;Identification&quot; (That is I have to tag all tokens in &quot;Statue of Liberty&quot; as named entities not as a place, which is the case in named entity recognition.)</p>
<p>I am building features which involve multiple tokens in sequence to determine whether token at particular position in that sequence is named entity or not. That is, I am building features that use surrounding tokens to determine whether a token is named entity or not. So as you have guessed there is relation between these tokens.</p>
<p>Now the data is very imbalanced. That is there are far more non-named entities than named entities and I wish to fix this. But I cannot simply oversample / undersample tokens randomly as it may result non-sensical sentences due loss of relation between tokens.</p>
<p>I am unable to guess how I can use <a href=""https://www.kaggle.com/rafjaa/resampling-strategies-for-imbalanced-datasets"" rel=""nofollow noreferrer"">other balancing techniques</a> like tomek links, SMOTE for such sentence data (that is without making sentences sound meaningless).</p>
<p>So what are best / preferred techniques to balance such data?</p>
","nlp"
"103628","How do I work with noisy real world text data for text classification?","2021-10-30 04:18:11","","0","32","<machine-learning><nlp><bert><text>","<p>I have a topic classification model built upon Bert, when I deploy my model people input strings of a random nature like :</p>
<p>&quot;aaaaaa&quot; &quot;aaa bbb&quot; &quot;ab ab ab&quot;</p>
<p>and so on.</p>
<p>My model predicts a false positive for these types of inputs 10% of the time. How do I make my machine learning model robust to these inputs?</p>
","nlp"
"103610","How to measure the pairwise similarity between two textual data sets?","2021-10-29 11:13:06","","1","313","<machine-learning><nlp><tfidf>","<p>I have <code>N</code> textual data sets, and each one is composed of thousands of documents. I want to compare them to find which data sets are more similar (Similar to what it is done in Figure 1 of <a href=""https://aclanthology.org/C18-1179.pdf"" rel=""nofollow noreferrer"">this article</a>.</p>
<p>My initial thought was to concatenate all the documents in a given data set, transforming each data set into a large document. Then, I would apply the TF-IDF vectorizer on the <code>N</code> documents. With the TF-IDF values, I could compare pairs of documents using some similarity measure, as the cosine, for example.</p>
<p>However, I'm struggling to find this way of summarizing my data sets without messing the data representation, more specifically the IDF value.
The IDF is computed using the <code>log(N)/|{d in D: t in d}|</code>, where <code>N</code> is the number of documents in the data set.</p>
<p>The problem is that, for comparing only two data sets (<code>A</code> and <code>B</code>), first I have to concatenate all the documents in <code>A</code>, and in <code>B</code>, generating documents <code>doc_A</code> and <code>doc_B</code>, respectively. Then, I apply the TF-IDF vectorizer in <code>[doc_A,doc_B]</code>. But, for a word <code>w</code> that appears quite frequently in many documents of <code>A</code>, but in only one document of <code>B</code>, my <code>IDF(w)</code> would be <code>log(2/2)=0</code>.</p>
<p>That seems conceptually wrong. Any ideas on a better way to do these comparisons?</p>
","nlp"
"103603","Which algorithm is best for predicting diseases if symptoms are given?","2021-10-29 09:19:05","103623","1","58","<machine-learning><python><nlp><scikit-learn><prediction>","<p>After Topic modelling through LDA, I get the following dataset as result.</p>
<pre><code>    Document_No Dominant_Topic  Topic_Perc_Contrib  Keywords    TextBookFindings    Disease/Drugs
0   0   3.0 0.7625  hypotension, bradycardia, mydriasis, hypersali  Hypotension,hypothermia,bradycardia NSAIDS Poisoning
1   1   5.0 0.6833  edema, cyanosis, cardiacarrest, lacrimation,    Hyperventilation,respiratoryalkalosis,edema–br...   NSAIDS Poisoning
2   2   0.0 0.8100  vomiting, nausea, diarrhea, abdominalpain,  Nausea,vomiting,diarrhea,abdominalpain– NSAIDS Poisoning
3   3   0.0 0.2625  vomiting, nausea, diarrhea, abdominalpain,  GIbleeding,pancreatitis,hepaticinjury   NSAIDS Poisoning
4   4   1.0 0.4463  insomnia, drowsiness, irritability, neurotoxic  Headache,dizziness,encephalopathy,irritability  NSAIDS Poisoning

... ... ... ... ... ... ...
1446    1446    7.0 0.5250  weakness, muscle, coagulopathy, fasciculations...   metabolicacidosis,Elevatedlactateconcentration...   Neuroleptic malignant syndrome (NMS)
1447    1447    0.0 0.0500  vomiting, nausea, diarrhea, abdominalpain, pan...   hematologictoxicity Neuroleptic malignant syndrome (NMS)
1448    1448    0.0 0.5250  vomiting, nausea, diarrhea, abdominalpain, pan...   Pancreatitis    NaN
1449    1449    0.0 0.0500  vomiting, nausea, diarrhea, abdominalpain, pan...   Hypersensitivity    Neuroleptic malignant syndrome (NMS)
1450    1450    0.0 0.0500  vomiting, nausea, diarrhea, abdominalpain, pan...   sensoryperipheralneuropathy NaN
</code></pre>
<p>I want to create a prediction system when a symptom is inputted, it shows the percentage matches of the Disease/Drugs. So I want to create a prediction between columns <code>keywords</code> and <code>Disease/Drugs</code>.</p>
<p>Which will be the best algorithm and some suggestions how to move forward with this?</p>
","nlp"
"103583","how to improve my imbalanced data NLP model?","2021-10-28 14:23:18","","0","329","<nlp><bert><language-model><allennlp>","<p>I want to classify a patient's health as a prediction probability and get the top 10 most ill patients in a hospital. I have patient's condition notes, medical notes, diagnoses notes, and lab notes for each day.</p>
<p>Current approach -</p>
<ol>
<li>vectorize all the notes using spacy's scispacy model and sum all the vectors grouped by patient id and day. (200 columns)</li>
<li>find the unit vectors of the above vectors. (200 columns)</li>
<li>use a moving average function on the vectors grouped by patient id and day.(200 columns)</li>
<li>find the unit vectors of the above moving average vectors (200 columns)</li>
<li>combine all the above columns and use them as independent features.</li>
<li>use a lgbm classifier.</li>
</ol>
<p>The data is imbalanced and the current AUC-ROC is around .78.</p>
<p>What else can I do to improve my AUC-ROC?
Can I use bert for this problem? how should I use it?</p>
<p>I'm currently using a moving average as a patient's health deteriorates over time.</p>
<p>Any suggestion/answer/feedback?</p>
","nlp"
"103567","How do I go for NLP based on phrases instead of sentences?","2021-10-28 04:43:11","","-1","57","<machine-learning><python><nlp><machine-learning-model><stanford-nlp>","<p>I have a list of words in this format:</p>
<pre><code>chem, chemistry
chemi, chemistry
chm, chemistry
chmstry, chemistry
</code></pre>
<p>Here, the first column represents the actual word which is in the second column. I need to apply NLP (in python3) so that when the model is trained using this dataset and I give 'chmty' as input, it will give 'chemistry' as output. I don't want string similarity techniques, I want to build an NLP model.</p>
","nlp"
"103541","Order by commonness of words: Sentence matching","2021-10-27 06:17:33","","0","176","<python><nlp><data-science-model>","<p>I'm trying to compare two sentences which might or might not be about the same theme (product). I've tried a few metrics like cosine similarity, distance etc. but the results are not very impressive. I figure, if I can see the type of words that are getting matched (common words vs rare words), I can try to do something to better tailor the logic as per the use case. Does Python have such a library?</p>
<p>Suppose I'm trying to compare <strong>A</strong> and <strong>B</strong> in the below example. There are several words which don't provide conclusive evidence that they are the same even if they match, but there are certain words (like Mochi, Squishy) which may be better explanators.</p>
<p>Eg.
<strong>A</strong>:
Kawaii Squishies, Mochi Squishy Toys for Kids Party Favors, Mini Stress Relief Toys for Halloween Christmas Easter Party Favors, Birthday Gifts, Classroom Prizes, Goodie Bag</p>
<p><strong>B</strong>:
I highly recommend these for anyone with anxiety, depression, adhd, trouble focusing, stress, ect. They are called mochi squishy toys and are wonderful to distract you from stress, or just to get your frustration out.</p>
","nlp"
"103536","Updating a genism LDA model with new documents and topics","2021-10-27 02:54:57","","1","726","<nlp><python-3.x><text-classification><topic-model>","<p>I have a conceptual problem that is related to a project I'm working on. I'm relatively new to the domain of NLP so this might be a poor question but I would really appreciate any help.</p>
<p>My dataset is web scraped news articles in a specific domain for the last 3 years and is updated with new articles every few days. The problem at hand is identifying trends. Here's my general approach so far:</p>
<ol>
<li>Preprocess the text</li>
<li>Run an LDA with genism to cluster and 'identify' topics</li>
<li>Find what topic each document relates to the most</li>
<li>Group these documents by topics</li>
<li>Use a moving average algorithm using the published date to see spikes in trends</li>
</ol>
<p>What I'm trying to achieve:
After every new update to the dataset, the new documents are classified into their respective topics. This way, I have a method to see what topics are trending.</p>
<p>The problem(s):</p>
<ol>
<li>LDA works on a whole set of documents. For a single new article, I don't know how to identify the topic. However, I might be able to figure out a workaround for this.</li>
<li>This is the bigger problem: I'm using a pre-determined number of topics. This approach fails if there's a new news story that belongs to a completely different topic. I don't have a way of accommodating this.</li>
</ol>
<p>I know that's a lot of information with very little context, but I've tried to explain to the best of my abilities. Is there a way I could update my LDA model to accommodate new topics? Or is there a different method I should be following altogether? Any and all suggestions are very very helpful.</p>
<p>I could have provided my code samples, but I don't know if they will be useful. I'll be happy to provide them as needed.</p>
","nlp"
"103514","Vector elements of word2vec?","2021-10-26 13:44:32","","1","199","<nlp><word2vec>","<p>In word2vec I understand that selecting a vector size of lets say 100 would give me a word vector which has the correlation (kind of) between the word and 100 other words in corpus.</p>
<p>My question is are these 100 words same for each word?</p>
","nlp"
"103485","Do I need a multilabel classification machine learning methodology or is it unnecessary?","2021-10-25 16:05:36","","2","197","<python><nlp><multilabel-classification>","<p><strong>Introduction</strong><br>
I’m working on a social science research project that involves a Natural Language Processing methodology. I’m assigning multiple labels (For example, label 1: <code>Blockchain</code>, Label 2: <code>Democracy</code>) to news articles that describe their content. I’m then running them through a couple of Python sentiment packages and I’ll be analysing changes in the correlation between different labels over time and what their sentiment looks like.<br></p>
<p>For example, if I retrieve articles from 2006-2016, i’d expect to see nothing about Blockchain until 2008, then I’d expect to see a growing number of blockchain related articles. After the launch of the Ethereum Blockchain in 2014, I’d expect to see a growing correlation between blockchain labels and democratisation labels because the Ethereum virtual machine facilitates “decentralized finance”, accessibility to markets, technology etc etc.<br></p>
<p>One of my learning objectives is to bolster my Python knowledge but I need to justify any research design decisions. I don’t want an unnecessarily complicated methodology for the sake of using elaborate Python coding and fancy sounding libraries.</p>
<p><strong>Question<br></strong>
For this project, is the use of supervised machine learning meaningful/justified/necessary or should I keep things simple? Please can you provide thoughts on the below considerations? Anything else I should bring into this assessment?</p>
<p><strong>Background<br></strong>
<em>Simple Approach to Multilabel Classification</em><br></p>
<ul>
<li>Two labels: 1. Blockchain 2. Democracy.</li>
<li>Create lists of terms associated with each of these labels. For example, the “Democracy” label might include: Democra (which would pull democracy, democrat, democratisation etc.), vote, voting, liberal, presidency, parliament, representative government, etc.</li>
<li>Filter my entire list of 5000 news articles for text that contains these terms, tag the articles if they contain any single instance of any of the terms. Tag each article with multiple labels if they contain terms relating to either label.</li>
<li>Manually audit the results by randomly selecting 100 articles, reading them in full and judging the labels. This may result in further manual correction, for example, if the term “liberal” returns numerous articles when the term is not used in the context of democracy, I might then decide to manually check the context of all instances for that term..</li>
<li>Pros and Cons: it will be impossible to manually review my filters in full. The existence of one instance of a term (an article mentions the word “democracy” once) doesn’t necessarily mean an article is about that subject (the threshold for receiving a particular label is very low). All terms will be considered unlike with the machine learning process where the training set might not include every term. The filtering/labelling of articles could be automated and is therefore scalable.</li>
</ul>
<p><em>Advanced Approach to Multilabel Classification<br></em></p>
<ul>
<li>Two labels: 1. Blockchain 2. Democracy</li>
<li>Create a dataset for training and testing a multilabel classification prediction algorithm:
Manually tag a subset of 500 news articles (10% of the total population) with the two labels.
This would be done using the simple approach described above combined with a manual audit of labels that this produces. For example, if the training/testing subset consists of 500 articles, I’ll read 100 of them to explore nuances and correct the labels. This may result in the decision to manually audit all 500.</li>
<li>Run a TFIDF function to assess the importance of all words in each article</li>
<li>Use binary relevance to assess each label independently with a Naive Bayes Algorithm for the classification.</li>
<li>If the testing yields decent accuracy results, then use the model for the remaining 4500 articles</li>
<li>Pros and Cons: this process will assess the importance of all words in each article and more broadly consider phrases that include my search terms. The labelling of this test/training data will be of higher quality than the labelling of all 5000 articles in the simple approach (manual review time focussed on 500 articles rather than 5000). If the training data doesn’t include particular terms however, then they will be dropped from the prediction process. Once the model is adequately trained, I can scale the size of this research without incurring additional work. For example, I could run 10,000 articles through the prediction algorithms.</li>
</ul>
<p><em>My preliminary answer and chosen approach<br></em>
Multilabel classification prediction algorithm (machine learning) not necessary but advantageous for the following reasons:</p>
<ul>
<li>the importance of all words will be assessed</li>
<li>The labelling of training data can be thoroughly reviewed manually</li>
<li>The main downside is that the training data might not include all terms considered relevant to a particular topic but I can address this by inserting articles about those topics or by fabricating them.</li>
<li>I appreciate there are levels to the multilabel classification problem (for example, moving from binary relevance to classifier chains/powersets etc) For now, just hoping to limit the decision to binary relevance machine learning solution vs just filtering the data. I'll be implementing in a very similar way to how it is done <a href=""https://www.section.io/engineering-education/multi-label-classification-with-scikit-multilearn/"" rel=""nofollow noreferrer"">here.</a></li>
</ul>
","nlp"
"103431","Using word embeddings as features in classification algorithms?","2021-10-23 18:28:10","","2","182","<classification><nlp><word-embeddings><word2vec><tfidf>","<p>I see there are ways to combine word vectors to form documents by taking averages or weighted averages. However, as a result of averaging there is a loss of information. Are there ways to retain the word embeddings of a document as is and use it as an input to a classification algorithm?</p>
","nlp"
"103389","HuggingFace hate detection model","2021-10-22 08:20:44","103466","1","52","<nlp><text-classification><huggingface>","<p>I am trying to train and evaluate a hate detection model using the <a href=""https://huggingface.co/transformers/"" rel=""nofollow noreferrer"">HuggingFace Transformers library</a> and this <a href=""https://github.com/t-davidson/hate-speech-and-offensive-language/tree/master/data"" rel=""nofollow noreferrer"">dataset</a>. Model performance is secondary, just trying to get it going. I have preprocessed the data and tokenised it as shown below:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import numpy as np
from numpy.random import RandomState
import re
import preprocessor as p
from transformers import AutoTokenizer

# Loading raw data
original_data = pd.read_csv('../data/data.csv')

# Make a random test and train split
rng = RandomState()
train = original_data.sample(frac=0.7, random_state=rng)
test = original_data.loc[~df.index.isin(train.index)]

# Preprocessing: remove special characters using RegEx
REPLACE_NO_SPACE = re.compile(&quot;(\.)|(\;)|(\:)|(\!)|(\')|(\?)|(\,)|(\&quot;)|(\|)|(\()|(\))|(\[)|(\])|(\%)|(\\\$)|(\&gt;)|(\&lt;)|(\{)|(\})&quot;)
REPLACE_WITH_SPACE = re.compile(&quot;(&lt;br\s/&gt;&lt;br\s/?)|(-)|(/)|(:).&quot;)

# Custum function to clean the datasets
def clean_tweets(df):
  tempArr = []
  for line in df:
    # send to tweet_processor
    tmpL = p.clean(line)
    # remove puctuation
    tmpL = REPLACE_NO_SPACE.sub(&quot;&quot;, tmpL.lower()) # convert all tweets to lower cases
    tmpL = REPLACE_WITH_SPACE.sub(&quot; &quot;, tmpL)
    tempArr.append(tmpL)
  return tempArr

# clean training data
train_tweet = clean_tweets(train[&quot;tweet&quot;])
train_tweet = pd.DataFrame(train_tweet)
# append cleaned tweets to the testing data
train[&quot;clean_tweet&quot;] = train_tweet

# clean the test data 
test_tweet = clean_tweets(test[&quot;tweet&quot;])
test_tweet = pd.DataFrame(test_tweet)
# append cleaned tweets to the training data
test[&quot;clean_tweet&quot;] = test_tweet

# Tokenisation so the inputs are ready for the model
tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-cased&quot;)
</code></pre>
<p>The above code generates a table for the test data as:
<a href=""https://i.sstatic.net/dMzOi.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/dMzOi.png"" alt=""enter image description here"" /></a></p>
<p>As I understand, the next part should be the model training part and extracting the % of hate tweets. Any suggestions on implementation?</p>
","nlp"
"103344","How does T5 model work on input and target data while transfer learning?","2021-10-21 03:41:11","","0","699","<nlp><transformer><transfer-learning><huggingface><nlg>","<p>I am working on a project where I want the model to generate job description based on <code>Role, Industry, Skills</code>. I have trained my data and got the resultant output.</p>
<p>I am aware that the <code>T5 model</code> was trained on C4 data in an unsupervised manner. The various different techniques applied including denoising, corrupted span, etc. But I am not able to understand how it works for classification problem.</p>
<p>My concern is if I pass my input and target variables for training how the model is going to train. Also give me a brief idea as how the model is going to react to input and output data.</p>
<p>Any links, resources is appreciated. Thankyou.</p>
","nlp"
"103282","How to apply two input and one output with LR and SVM","2021-10-19 14:24:21","103326","1","356","<machine-learning><nlp><svm>","<p>Q1: how to feed 2 input to LR and SVM?</p>
<p>My dataset consist of three columns which are:
sentence1 , sentence 2, and label (1 if the sentence2 is a paraphrased of sentence1)</p>
<p>I prepare my data and convert it numeric features using (tf-idf)
now I would like to train a classifier, but all the tutorials I find do one input and one output while I would like two inputs and one output. Could you help with an example?</p>
<p>Picture of my data:
<a href=""https://i.sstatic.net/KLq8j.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/KLq8j.png"" alt=""my dataset"" /></a></p>
","nlp"
"103224","Why do Transformers need positional encodings?","2021-10-17 17:40:21","","7","2964","<machine-learning><deep-learning><neural-network><nlp><transformer>","<p>At least in the first self-attention layer in the encoder, inputs have a correspondence with outputs, I have the following questions.</p>
<ul>
<li>Isn't ordering already implicitly captured by the query vectors, which themselves are just transformations of the inputs?</li>
<li>What do the sinusoidal positional encodings capture that the ordering of the query vectors don't already do?</li>
<li>Am I perhaps mistaken in thinking that transformers take in the entire input at once?</li>
<li>How are words fed in?</li>
<li>If we feed in the entire sentence at once, shouldn't the ordering be preserved?</li>
</ul>
<p><a href=""https://i.sstatic.net/Ezu0V.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/Ezu0V.png"" alt=""source: https://web.eecs.umich.edu/~justincj/slides/eecs498/FA2020/598_FA2020_lecture13.pdf"" /></a></p>
","nlp"
"103157","Using Sentence-Bert with other features in scikit-learn","2021-10-15 01:44:44","","1","225","<machine-learning><nlp><tensorflow><bert>","<p>I have a dataset where one feature is text and 4 more features. Sentence-Bert vectorizer transforms text data into tensors. I can use these sparse matrices directly with a machine learning classifier. Can I replace the text column with tensors? Also how can I train the model. Below is the code I used to transform the text into vectors.</p>
<pre><code>model = SentenceTransformer('sentence-transformers/LaBSE')
sentence_embeddings = model.encode(X_train['tweet'], convert_to_tensor=True, show_progress_bar=True)
sentence_embeddings1 = model.encode(X_test['tweet'], convert_to_tensor=True, show_progress_bar=True)
</code></pre>
","nlp"
"103113","When to do tokenization and does my output need tokenization after stemming?","2021-10-13 16:58:12","","0","73","<nlp><data-cleaning><preprocessing><sentiment-analysis><tokenization>","<p>I am working on sentiment analysis project , where there are various customer reviews. So I am trying to clean those reviews.</p> 
<p> So first thing i did is removing special characters, white  spaces, numbers from text. 
Next I did is removing stop words(removing this, that, have etc.)
After that i did stemming(removing ING, ed,y etc).</p>
 Below is my output.
<p>What I want to know is that is tokenization needed here any more?
Because my output after stemming looks like tokenization is done.
<a href=""https://i.sstatic.net/FkgsT.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/FkgsT.png"" alt=""enter image description here"" /></a></p>
","nlp"
"103109","How to identify/recognize that a sentence about talks about future?","2021-10-13 14:56:08","","3","1218","<python><keras><nlp><scikit-learn><spacy>","<p><strong>Brief Introduction:</strong>
I have a report/paragraph in which there are sentences with reference to future plans/outlooks/expectations for a particular entity. I want to extract all such sentences for now.</p>
<p><strong>Problem statement:</strong>
How to identify or recognize such futuristic statement (sentence where they refer to their plans) or How to best segregate the futuristic sentences from other non-futuristic sentences. I’m looking for a traditional programming solution and/or Machine Learning solution.</p>
<p><strong>Languages and packages preferred:</strong> <em>Python, Spacy, scikit-learn, keras (backend - tensorflow)</em></p>
<p><em>Here is an input paragraph:</em></p>
<hr />
<p>“As at the Latest Practicable Date, we have seven dipping lines located at our Malaysian facilities and two dipping lines at our Thai facility. We plan to upgrade our facilities in Malaysia by increasing the number of cleanrooms for laundry and packaging of our products and installing additional cleanroom plant and equipment. In relation to our Thai subsidiary, we intend to install an additional dipping line, increase the number of cleanrooms and upgrade our facilities by adding additional floor space to house additional plant and equipment. “</p>
<p>Copy the above text into paragraph.txt file.</p>
<p><em>Here is the code to get given text into sentences in a pandas dataframe.</em></p>
<hr />
<pre><code>import pandas 
import spacy 

with open(&quot;paragraph.txt&quot;,'r') as f:
    content = f.read() 

nlp = spacy.load('en_core_web_sm')
sentences = list(nlp(content).sents)
sentences = [str(sentence) for sentence in sentences]
sentences_dataframe = pandas.DataFrame(sentences,columns =[&quot;Sentences&quot;])
sentences_dataframe[&quot;F/NF&quot;] = &quot;&quot;
sentences_dataframe.head()
</code></pre>
<p><strong>Here's the desired result:</strong></p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">S.no</th>
<th style=""text-align: center;"">Sentences</th>
<th style=""text-align: center;"">F/NF</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">1</td>
<td style=""text-align: center;"">As at the Latest Practicable Date, we have seven dipping lines located at our Malaysian facilities and two dipping lines at our Thai facility.</td>
<td style=""text-align: center;"">NF</td>
</tr>
<tr>
<td style=""text-align: left;"">2</td>
<td style=""text-align: center;"">We plan to upgrade our facilities in Malaysia by increasing the number of cleanrooms for laundry and packaging of our products and installing additional cleanroom plant and equipment</td>
<td style=""text-align: center;"">F</td>
</tr>
<tr>
<td style=""text-align: left;"">3</td>
<td style=""text-align: center;"">In relation to our Thai subsidiary, we intend to install an additional dipping line, increase the number of cleanrooms and upgrade our facilities by adding additional floor space to house additional plant and equipment.</td>
<td style=""text-align: center;"">F</td>
</tr>
</tbody>
</table>
</div>
<p>F – Futuristic, NF - Futuristic</p>
<p>The paragraph has 3 sentences, one row for each sentence and related tags for each sentence</p>
","nlp"
"103103","What is the difference between batch_encode_plus() and encode_plus()","2021-10-13 11:04:44","103106","1","8956","<nlp><transformer><transfer-learning><nlg>","<p>I am doing a project using T5 Transformer. I have read documentations related to T5 Transformer model. While using T5Tokenizer I am kind of confused with tokenizing my sentences.</p>
<p>Can someone please help me understand the difference between batch_encode_plus() and encode_plus() and when should I use either of the tokenizers.</p>
","nlp"
"103091","Topic modelling on long documents: intra document clustering first","2021-10-12 20:39:43","","1","172","<nlp><clustering><k-means><text><topic-model>","<p>I have a collection (around 1000) of very noisy, similar documents, that are each very long (&gt;10 pages - 600 paragraphs) with multiple subsections - I want to perform topic modelling across the documents to discover the key themes.</p>
<p>I feel like I need to think carefully about how to treat the documents, but am struggling to find resources/papers. Does the following approach seem sensible, and are there any papers/sources that might be of help?:</p>
<ol>
<li>Iterate through each document</li>
<li>Identify paragraphs</li>
<li>Keep only the longest paragraphs (say only the top quartile)</li>
<li>Usual NLP pre-processing (stop words, tokenize etc)</li>
<li>Embed each para as tf-idf vector</li>
<li>KMeans cluster across all paragraphs</li>
<li>use the clustered paragraphs as new documents within the overall corpus</li>
</ol>
<p>So this way, the final corpus will be made of up of documents that are clustered paragraphs from each original document. I can then perform topic modelling.</p>
<p>Some initial clustering has yielded poor results (low silhouette score). Are there any approaches that might help? Or anything that I may have missed?</p>
","nlp"
"103067","Converting to lowercase while creating dataset for NER using spacy","2021-10-12 10:06:05","","2","824","<python><nlp><nltk><spacy>","<p>I am trying to make a custom entity model for an NER application using spacy. In several NLP projects, I have converted all the data to lowercase and applied several ML techniques. For NER also should I have to convert the data to lowercase. Or why it is necessary to convert to lower case. Is it a mandate one which will affect the accuracy of the model adversely if not converted to lowercase.</p>
","nlp"
"103057","How should I engineer features for Named Entity Identification task?","2021-10-11 23:18:52","","1","108","<nlp><feature-selection><feature-engineering><svm><feature-extraction>","<p>I was working on Named Entity Identification (not recognition) task. In this NLP task, given a sentence, model has to predict whether each word (aka token) is named entity or not. The dataset used were CONLL2003 dataset.</p>
<p>Initially, I included a feature <code>first-letter-capital</code> which was <code>1</code> if a token has its first letter capitalized. The model learnt to predict first word of each sentence as named entity.</p>
<p>So I removed this feature and added a feature <code>first-letter-capital-for-non-sentence-start-word</code>, which was <code>1</code> if a word is not a first word of the sentence and has first letter a capital. This made model to classify first word of each sentence a non named entity.</p>
<p>When I kept neither, the model predicted no word as named entity. Why this might have happened? Can someone share their insight?</p>
","nlp"
"103005","Can a reformer model really handle long-range dependency?","2021-10-10 16:15:16","","1","38","<deep-learning><nlp><transformer><attention-mechanism><huggingface>","<p>I read this <a href=""https://huggingface.co/blog/reformer"" rel=""nofollow noreferrer"">article</a> about new attention model called <a href=""https://github.com/patrickvonplaten/notebooks/blob/master/PyTorch_Reformer.ipynb"" rel=""nofollow noreferrer"">Reformer</a>. Here is the main strength of this model:</p>
<blockquote>
<p>The Reformer pushes the limit of longe sequence modeling by its ability to process up to half a million tokens at once as shown in this demo.</p>
</blockquote>
<p>Is it actually true? Because later in jupyter notebook I see the following in a config:</p>
<pre><code>config = {
    &quot;attention_head_size&quot;: 64,
    &quot;attn_layers&quot;: [&quot;local&quot;, &quot;lsh&quot;, &quot;local&quot;, &quot;lsh&quot;, &quot;local&quot;, &quot;lsh&quot;],
    &quot;lsh_attn_chunk_length&quot;: 64,
    &quot;local_attn_chunk_length&quot;: 64,

    &quot;lsh_num_chunks_before&quot;: 1,
    &quot;lsh_num_chunks_after&quot;: 0,
    &quot;local_num_chunks_before&quot;: 1,
    &quot;local_num_chunks_after&quot;: 0,
}
</code></pre>
<p>This config tells that they use chunks, so attention can span only <code>64</code> tokens and 1 chunk before it.</p>
<p>Does it mean that there is no any long-range dependency can be learned by this model?</p>
","nlp"
"102888","Why do we calculate the vector of a document by averaging the vectors of all the words?","2021-10-07 10:10:30","","1","304","<nlp><word2vec><gensim><spacy><search-engine>","<p>I am trying to build a search engine to query a folder of documents. Tutorials online suggest that we should obtain the vector of a document by averaging the vectors of all the words, then compare similarity to the vector of the query.</p>
<p>May I know how does the vector of all the words in the document retain the information of the words?</p>
<p>Would it be better if i retrieved similar words of the query and checked if these words were in each document?</p>
","nlp"
"102859","What approach should I take for my product classification ML model with user feedback for improving result accuracy?","2021-10-06 12:25:27","102860","1","159","<machine-learning><nlp><reinforcement-learning><real-ml-usecase>","<p>I'm trying to implement a product categorization ML model on a dataset with the following structure:
<a href=""https://i.sstatic.net/t0irT.png"" rel=""nofollow noreferrer"">Data sample</a></p>
<p>I want to my model to be able to predict the correct category that the product should fall under, based on product description and name.</p>
<p>However, I will be implementing this together with a GUI which allows some user input.</p>
<p>For example, a new product name with a description gets added to the table:
<a href=""https://i.sstatic.net/QVSaB.png"" rel=""nofollow noreferrer"">New entry before feedback training</a></p>
<p>The user will be presented with the following options (completely made these up) and has to select one:</p>
<p>Kitchen furniture - 65%</p>
<p>Home decorations - 29%</p>
<p>Kitchen Appliances - 6%</p>
<p>User will click on 'Home decorations'. This gets fed back to the model. Next time the model encounters something similar, such as:
<a href=""https://i.sstatic.net/H9gxR.png"" rel=""nofollow noreferrer"">New entry after feedback training</a></p>
<p>The user will be presented with more accurate predictions, where this time they have the same options to choose from, but with different predicted accuracy:</p>
<p>Home decorations - 70%</p>
<p>Kitchen furniture - 20%</p>
<p>Kitchen appliances - 10%</p>
<p>Therefore, the model has learned from that feedback and has become more accurate. I've done some research around this and it has pointed towards <strong>Reinforcement Learning</strong>. However, I couldn't find anything too similar and I am not THAT skilled in ML, so please point me in the right direction in terms of what Python libraries to use, what ML models to look at and maybe even previous implementations.</p>
<p>Thanks!</p>
","nlp"
"102800","default estimation method of gensim's word2vec skipgram?","2021-10-05 05:51:00","","1","60","<python><nlp><word2vec><gensim>","<p>I am now trying to use word2vec by estimating skipgram embeddings via NCE (noise contrastive estimation) rather than conventional negative sampling method, as a recent paper did (<a href=""https://asistdl.onlinelibrary.wiley.com/doi/full/10.1002/asi.24421?casa_token=uCHp2XQZVV8AAAAA%3Ac7ETNVxnpqe7u9nhLzX7pIDjw5Fuq560ihU3K5tYVDcgQEOJGgXEakRudGwEQaomXnQPVRulw8gF9XeO"" rel=""nofollow noreferrer"">https://asistdl.onlinelibrary.wiley.com/doi/full/10.1002/asi.24421?casa_token=uCHp2XQZVV8AAAAA%3Ac7ETNVxnpqe7u9nhLzX7pIDjw5Fuq560ihU3K5tYVDcgQEOJGgXEakRudGwEQaomXnQPVRulw8gF9XeO</a>). The paper has a replication github repository (<a href=""https://github.com/sandeepsoni/semantic-progressiveness"" rel=""nofollow noreferrer"">https://github.com/sandeepsoni/semantic-progressiveness</a>), and it mainly relied on gensim for implementing word2vec, but the repository is not well organized and in a mess, so I have no clue about how the authors implemented NCE estimation via gensim's word2vec.</p>
<p>The authors just used gensim's word2vec as a default status without including any options, so my question is what is the default estimation method for gensim's word2vec under skipgram embeddings. NCE? According to your manual, it just says there is an option for negative sampling, and if set to 0, then no negative sampling is used. But then what estimation method is used? negative (int, optional) – If &gt; 0, negative sampling will be used, the int for negative specifies how many “noise words” should be drawn (usually between 5-20). If set to 0, no negative sampling is used.</p>
<p>Thanks you in advance, and look forward to hearing from you soon!</p>
","nlp"
"102779","CRFSuite/Wapiti: How to create intermediary data for running a training?","2021-10-04 14:23:27","102796","1","67","<nlp><labelling>","<p>After <a href=""https://datascience.stackexchange.com/questions/102669/software-library-suggestion-is-there-a-usable-open-source-sequence-tagger-aroun/102672#102672"">having asked for and been suggested</a> two pieces of software last week (for training a model to categorize chunks of a string) I'm now struggling to make use of either one of them.</p>
<p>It seems that in machine learning (or at least, with CRF?), you can't just train on the training data directly, but you have to go through an intermediary step first.¹</p>
<p>From the CRFsuite tutorial:</p>
<blockquote>
<p>The next step is to preprocess the training and testing data to extract attributes that express the characteristics of words (items) in the data. CRFsuite internally generates features from attributes in a data set. In general, this is the most important process for machine-learning approaches because a feature design greatly affects the labeling accuracy.</p>
</blockquote>
<p>Wapiti doesn't need such an attribute file created, I think because it has &quot;patterns&quot; instead which seem somewhat more sophisticated than CRFsuite's intermediary-format files.</p>
<p>To provide an example: given a large number (many tens of thousands) of strings such as these three:</p>
<ul>
<li><p><code>Michael went to his room.</code></p>
</li>
<li><p><code>Did you know Jessica's mom used to be with the military?</code></p>
</li>
<li><p><code>Amanda! Come back inside! We'll have dinner soon!</code></p>
</li>
</ul>
<p>From which manually a smaller number (few thousands) of labelled training and test data have been created, such as this block (for the first example above):</p>
<pre><code>T Michael
K went
K to
K his
K room
S .

K Did
K you
K know
T Jessica's
K mom
K used
K to
K be
K with
K the
K military
S ?

T Amanda
S !
K Come
K back
K inside
S !
K We'll
K have
K dinner
K soon
S .
</code></pre>
<p>(<code>T</code> for names, <code>K</code> for non-names, <code>S</code> for punctuation, <code>N</code> for numbers.)</p>
<p>How do I figure out what the &quot;attributes&quot; should be, to be able to create an equivalent to the <code>chunking.py</code> script used in <a href=""http://www.chokkan.org/software/crfsuite/tutorial.html"" rel=""nofollow noreferrer"">the CRFsuite tutorial</a>?</p>
<hr />
<p>¹: With regard to that intermediary step, the terminology used by Naoaki Okazaki is not clear to me. &quot;Features&quot; and &quot;Attributes&quot; are used interchangeably and seem to refer to something invisible contained in the data. &quot;Labels&quot; might be the categories in which to put the tokens, and then there's also &quot;Observations&quot;.</p>
","nlp"
"102770","How can I classify if a line of text is an incomplete natural language fragment or a complete natural language unit?","2021-10-04 09:38:09","","2","138","<nlp>","<p>I would like to return a simple “true” or “false” for a given string which determines whether it is an incomplete sentence, like “which is why they usually”, or a complete entity, such as a chapter title or element in a list, for example “The boardwalk”.</p>
<p>I am open to using any machine learning architecture for this task and it’s possible I would provide context for the strings by showing surrounding lines for the algorithm to see if the sentence fragment fits in to some continuing text or not, but for a first draft I’ll probably just focus on the individual lines in isolation.</p>
<p>What would be the most standard way to approach this?</p>
<p>For example, maybe there’s already some pre-trained model that knows if something is a sentence fragment or not, which I could use.</p>
<p>Otherwise, if not, does it matter what kind of neural network I use, or is there a most standard one from PyTorch?</p>
<p>Is there a clever way to avoid needing to find or create training data? Can an unsupervised learning method naturally group lines of text into those which are similar and those which are different?</p>
<p>Or if I need to train it, are there algorithms which learn quickly, perhaps from just 100 examples?</p>
<p>Otherwise, I guess I will have to find or create a large source of data to train it on?</p>
<p>What would be the best way to approach this?</p>
","nlp"
"102669","Software/Library Suggestion: Is there a usable open-source sequence tagger around?","2021-09-30 21:31:41","102672","1","52","<nlp><labelling>","<p><em>(Not sure if this is the right community for the question - please do downvote if <code>stats.</code> or whatever else is more appropriate...)</em></p>
<p>I'm looking for a suggestion for either a command-line tool or library (preferably Python or Ruby, but at this point, anything will do) implementing non-Parts-of-Speech-specific <a href=""https://en.wikipedia.org/wiki/Sequence_labeling"" rel=""nofollow noreferrer"">sequence tagging/labelling</a>. If it was PoS-specific but could be re-trained for custom categories, that'd be fine, too.</p>
<p>The projects I've found mostly seem to be abandoned PhD thesis codebases or similar and I've not been able to make any of them work in a practical manner. The one I got the furthest with was <a href=""https://github.com/akurniawan/pytorch-sequence-tagger"" rel=""nofollow noreferrer"">pytorch-sequence-tagger</a>.</p>
<p>In case it helps with giving suggestions: the purpose is to tell apart tokens which are part of library class marks from tokens which are part of author names or book titles, but where the input data are too irregular for a rule-based system to work 100%.</p>
","nlp"
"102613","Clustering Strings Based on Similar Word Sequences","2021-09-29 14:14:17","","2","398","<machine-learning><python><nlp><clustering><text-mining>","<p>In my dataset I have a feature having below data :</p>
<h2>Input</h2>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Feature</th>
</tr>
</thead>
<tbody>
<tr>
<td>Brain Dementia Routine(Comfortone)</td>
</tr>
<tr>
<td>Morning Check</td>
</tr>
<tr>
<td>Dementia Brain-Routine(Comfortone)</td>
</tr>
<tr>
<td>Brain MRA Routine (Comfortone)</td>
</tr>
<tr>
<td>Brain-Dementia/Routine(MRCP)</td>
</tr>
<tr>
<td>MRCP BH WITH DYNAMIC****</td>
</tr>
<tr>
<td>LIVER/MRCP W/O, W/</td>
</tr>
<tr>
<td>MRCP Routine 30slice</td>
</tr>
<tr>
<td>MRCP-Routine/Dr.Robert</td>
</tr>
</tbody>
</table>
</div>
<p>How do I   cluster values that have similar words in it .</p>
<h2>Output</h2>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Feature</th>
<th>Cluster</th>
</tr>
</thead>
<tbody>
<tr>
<td>Brain Dementia Routine(Comfortone)</td>
<td>A</td>
</tr>
<tr>
<td>Morning Check</td>
<td>C</td>
</tr>
<tr>
<td>Dementia Brain-Routine(Comfortone)</td>
<td>A</td>
</tr>
<tr>
<td>Brain MRA Routine (Comfortone)</td>
<td>A</td>
</tr>
<tr>
<td>Brain-Dementia/Routine(Comfortone)</td>
<td>A</td>
</tr>
<tr>
<td>MRCP BH WITH DYNAMIC****</td>
<td>B</td>
</tr>
<tr>
<td>LIVER/MRCP W/O, W/</td>
<td>B</td>
</tr>
<tr>
<td>MRCP Routine 30slice</td>
<td>B</td>
</tr>
<tr>
<td>Dr.Robert/MRCP Routine/</td>
<td>B</td>
</tr>
</tbody>
</table>
</div>","nlp"
"102607","Is it possible to fine-tuning BERT by training it on multiple datasets? (Each dataset having it's own purpose)","2021-09-29 11:54:47","","1","816","<nlp><bert><transformer><transfer-learning><finetuning>","<p>BERT can be fine-tuned on a dataset for a specific task. Is it possible to fine-tune it on all these datasets for different tasks and then be utilized for these tasks instead of fine-tuning a BERT model specific to each task?</p>
","nlp"
"102537","What can I do when my test and validation scores are good, but the submission is terrible?","2021-09-27 15:37:05","","4","223","<nlp><overfitting><pipelines><data-leakage>","<p>This is a very broad question, I understand and I'm totally fine if someone believes it's not appropriate to do it. But it's killing me not to understand this...</p>
<p>Here's the thing, I'm doing a machine learning model to predict the tweet topic. I'm participating in <a href=""https://zindi.africa/competitions/gender-based-violence-tweet-classification-challenge"" rel=""nofollow noreferrer"">this competition</a>. So this is what I've done in order to ensure I'm not overfitting: I separated 10% of my training data and I called <i>validation set</i>, and I used the rest (90%) to prepare my model. So 90% of my data was divided into train and test set. So basically I had two datasets to test my model, the test set and the validation set. All results are great! Both the test and the validation set got me great results. I also did a Stratified K-Fold, which also showed me great results. However, the submission set is returning me 73% of accuracy. What can be happening? Why do I get good results in the test and validation set, but no so good in the submission? Is there any explanation? Is there any data leakage happening in here? I find it very weird to have any leakage, since the validation set is not used at all. But idk what can be happening anymore...</p>
<p>This is part of what I've done and where might lead to some leakage (I simplified a little):</p>
<pre><code># load training data
train_set = pd.read_csv('gender-based-violence-tweet-classification-challenge/Train.csv')

# leave 10% for validation
train = train_set.loc[:35685, [&quot;Tweet_ID&quot;, &quot;tweet&quot;, &quot;type&quot;]]
validation = train_set.loc[35685:, [&quot;Tweet_ID&quot;, &quot;tweet&quot;]]

# load the test set
submission_set = pd.read_csv('gender-based-violence-tweet-classification-challenge/Test.csv')

# load submission file
submission_file = pd.read_csv('gender-based-violence-tweet-classification-challenge/SampleSubmission.csv')

def preprocess_text(text):
    STOPWORDS = stopwords.words(&quot;english&quot;)

    # Check characters to see if they are in punctuation
    nopunc = [char for char in text if char not in string.punctuation]

    # Join the characters again to form the string.
    nopunc = &quot;&quot;.join(nopunc)

    # Now just remove any stopwords
    return &quot; &quot;.join([word for word in nopunc.split() if word.lower() not in STOPWORDS])


X = train[&quot;tweet&quot;]
y = train[&quot;type&quot;]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.15, random_state=42
)

pipe = Pipeline([
(&quot;vect&quot;, CountVectorizer(analyzer=preprocess_text)),
(&quot;clf&quot;, RandomForestClassifier(class_weight='balanced'))
])

pipe.fit(X_train, y_train)
y_pred = pipe.predict(X_test)
</code></pre>
","nlp"
"102531","How long does it take to fine-tune XLNet?","2021-09-27 13:53:30","","0","188","<nlp><bert><finetuning><pretraining>","<p>XLNet takes a lot more time than BERT during <strong>pre-training</strong>. This results in XLNet performing better than BERT in over 20 NLP tasks. How long does XLNet take for <strong>fine-tuning</strong> (let's assume this is running on Google Colab)?</p>
<p>(Let's assume a text summarization task with around 4000 examples)</p>
","nlp"
"102465","Which ML algorithm is best works on text data and the reason behind it? Also, which metrics is used for testing performance of model?","2021-09-25 03:21:34","","1","39","<machine-learning><nlp><algorithms><text-classification>","<p>I am working on a project - 'sentiment analysis of tweets.' There are 5 different sentiments - extremely negative, negative, neutral, positive, and extremely positive. So it is basically the NLP problem as I have to work with text data. Which algorithm works best on this data and the reason behind it. Also, which classification metrics I can use to check the performance of the model and the reason to choose the particular metrics?</p>
","nlp"
"102437","Where do Q vectors come from in Attention-based Sequence-to-Sequence Transformers?","2021-09-24 13:26:59","102447","0","72","<nlp><transformer><attention-mechanism>","<p>I'm taking a course on Attention-based NLP but I'm not understanding the calculation and application of Attention, based on the use of Q, K, and V vectors.  My understanding is that the K and V vectors are derived from the encoder input and the Q vector is derived from the decoder input.  This makes sense to me in the context of <em>training</em>, where the entire input sequence is presented to the encoder and the entire output sequence is presented to the decoder.  What does not make sense, however, is how this applies in the context of <em>inference</em>.  In that case, it would seem like there is no input to the decoder, so where does the Q vector come from?</p>
","nlp"
"102398","Does BERT need supervised data only when fine-tuning?","2021-09-23 14:40:49","102426","3","2313","<machine-learning><nlp><unsupervised-learning><supervised-learning><bert>","<p>I've read many articles and papers mentioning how unsupervised training is conducted while <strong>pre-training</strong> a BERT model. I would like to know if it is possible to fine-tune a BERT model in <strong>an unsupervised manner</strong> or does it <strong>always have to be supervised</strong>?</p>
","nlp"
"102300","Entity Linking for Receipts","2021-09-21 19:10:21","","1","102","<nlp><ocr><document-understanding><spatial-transformer>","<p>I am building a model for reading receipts from their mobile snapshots. After the receipt is OCR'd, I plan to use a variation on LayoutLM for entity extraction. Entities are: &quot;quantity&quot;, &quot;price-per-unit&quot;, &quot;product-name&quot;, &quot;items-price&quot;, etc.</p>
<p>What is the best model to consider to link all these entities into a single receipt item, so the final result looks like:</p>
<pre class=""lang-json prettyprint-override""><code>&quot;items&quot;: [
   {&quot;product&quot;: ...,
    &quot;unit_price&quot;: ...,
    &quot;price_paid&quot;: ...,
    &quot;quantity&quot;: ...,
},
...
]
</code></pre>
","nlp"
"102299","TF-IDF for 400,000+ unique words in corpus?","2021-09-21 18:15:40","","1","1130","<nlp><tfidf><memory><google-cloud-platform>","<p>I have a corpus with over 400,000 unique words. I would like to build a TF-IDF matrix for this corpus.
I have tried doing this on my laptop (16GB RAM) and Google Colab, but am unable to do so due to memory constraints. What is the best way to go about this?</p>
","nlp"
"102277","Binary document classification using keywords for a very small dataset","2021-09-21 09:22:49","102282","1","256","<machine-learning><classification><nlp><text-classification><binary-classification>","<p>I have a set of 150 documents with their assigned binary class. I also have 1000 unlabeled documents. Each document is about the length of a journal paper. Each class has 15 associated keywords.</p>
<p>I want to be able to predict the assigned class of the documents using this information. Does anyone have any ideas of how I could approach this problem?</p>
","nlp"
"102259","What exactly are the parameters in GPT-3's 175 billion parameters?","2021-09-20 17:40:06","","5","17103","<nlp><gpt>","<p>What exactly are the parameters in GPT-3's 175 billion parameters? Are these the words in text on which model is trained?</p>
","nlp"
"102258","how to filter out and discard irrelevant tweets in simplest way possible","2021-09-20 17:16:33","","0","246","<nlp><data-cleaning><text-mining><text-classification><twitter>","<p>I have lot of tweets and from which i need to filter out and discard irrelevant tweets. the criteria for a tweet to be irrelevant is very simple. if all that a tweet has is emojis or a single hastag or multiple hastags etc,. put simply, if a tweet contains no actual information to extract, that's irrelevant. are there any pre-built packages available.</p>
<p>I don't want to build a classifier, because this is going to be used inside of the data pre-processing pipeline of an NLP model. Moreover, labelling the tweets will be an additional overhead. So, i want to know if there are any approaches or pre-trained models to do this. And I would like this thing to be as simple as possible.</p>
","nlp"
"102235","Best approach for text classification of phrases with little syntactic difference","2021-09-19 20:35:57","102523","2","218","<deep-learning><nlp><svm><text-classification><language-model>","<p>So I have the task of classifying sentences based on their level of 'change talk' shown. Change talk is a psychology term used in counseling sessions to express how much the client wants to change their behavior.</p>
<p>So let's say there are two classes: change talk; and non-change talk.</p>
<p>An example of change talk is: &quot;I have to do this.&quot; or &quot;I can achieve this.&quot;</p>
<p>An example of non-change talk is &quot;I can't do this.&quot; or &quot;I have no motivation.&quot;</p>
<p>My issue is, if I want to take a machine learning approach in classifying these sentences, which is the best approach? SVM's? I do not have a lot of training data. Also - all the tutorials I look at use sentences with obvious words that can easily be classified (e.g. &quot;The baseball game is on tomorrow.&quot; -&gt; SPORT, or &quot;Donald Trump will make a TV announcement tomorrow.&quot; -&gt; POLITICS).</p>
<p>I feel my data is harder to classify as it typically does not have keywords relating to each class.</p>
<p>Some guidance on how people would approach this task would be great.</p>
","nlp"
"102214","How to get sentiment score for a word in a given dataset","2021-09-18 15:19:36","","2","732","<nlp><dataset><bert><sentiment-analysis>","<p>I have a sentiment analysis dataset that is labeled in three categories: positive, negative, and neutral. I also have a list of words (mostly nouns), for which I want to calculate the sentiment value, to understand &quot;how&quot; (positively or negatively) these entities were talked about in the dataset. I have read some online resources like blogs and thought about a couple of approaches for calculating the sentiment score for a particular word X.</p>
<ol>
<li><p>Calculate how many data instances (sentences) which have the word X in those, have &quot;positive&quot; labels, have &quot;negative&quot; labels, and &quot;neutral&quot; labels. Then, calculate the weighted average sentiment for that word.</p>
</li>
<li><p>Take a generic untrained BERT architecture, and then train it using the dataset. Then, pass each word from the list to that trained model to get the sentiment scores for the word.</p>
</li>
</ol>
<p>Does any of these approaches make sense? If so, can you suggest some related works that I can look at?
If these approaches don't make sense, could you please advise how I can calculate the sentiment score for a word, in a given dataset?</p>
","nlp"
"102208","What is multilingual vs monolingual corpus?","2021-09-18 11:33:27","","0","34","<nlp>","<p>If I have a corpus which has Hindi script and Hindi transliteration with English script. Is it multilingual or monolingual corpus?</p>
","nlp"
"102197","Phrase/Token labeling","2021-09-18 06:44:45","","2","69","<machine-learning><deep-learning><nlp><machine-learning-model><language-model>","<p>Looking for suggestions on how to define the following NLP problem and different ways in which it can be modeled to leverage machine learning. I believe there are multiple ways to model this problem.  Deep-learning-based suggestions also work as there is a good amount of data is available for training.</p>
<p>Will evaluate different approaches for the given dataset. Please share relevant papers, blogs, or GitHub repos. Thanks!</p>
<p><strong>Input</strong>: Given a sentence S having words W1 to W10.</p>
<p>S = W1 W2 W3 W4 W5 W6 W7 W8 W9 W10</p>
<p>The sentence has some syntactic and semantic patterns, but it is not exactly freely written natural language but it's in English. These are words, can be punctuation</p>
<p><strong>Output</strong>: should be something like this.</p>
<p>Label1 - W4</p>
<p>Label2 - W3</p>
<p>Label3 - [W2 W1] continuous // semantically related. Means words [W2 W1] in-order are assigned a Label3. <em>Also okay with solutions that don't output in-order.</em></p>
<p>Label4 - [W6 W8]</p>
<p>Label5- W10</p>
<p>Noise- W7, W9. Means words W7 and W9 independently are assigned a
Label3.</p>
<p>Label7- W5</p>
<p>Need to solve the problem. Looking for research/thoughts on how this problem can be defined in different ways to exploit different patterns in the structure of sentences. Looking for similar tasks which are already defined in NLP such as token labeling, parsing which can be used.</p>
<p>Would be really helpful to get the suggestions to the latest research on solving/defining this problem.</p>
","nlp"
"102181","How much data augmentation is required on an imbalanced dataset?","2021-09-17 17:10:37","","0","465","<classification><nlp><data-augmentation>","<p>Imagine I have a dataset with positive and negative sentences, and I need to train a transformer (Like BERT) to do the binary classification. The problem is that there are 100 negative sentences and 2000 positive sentences. There are libraries for NLP data augmentation like this one: <a href=""https://github.com/makcedward/nlpaug"" rel=""nofollow noreferrer"">https://github.com/makcedward/nlpaug</a></p>
<p>But how many new instances should I add to the minor class? Since my dataset is highly imbalanced, should I try to add 1900 instances to the minor class so that both classes have an equal population?</p>
<p>In order to make the imbalance ratio 1 in a highly imbalanced dataset such as mine, I have to use each sentence to generate 19 new sentences. If several of them are too alike, my model will end up overfitted.</p>
","nlp"
"102144","How to stay up to date in NLP and use the best approaches?","2021-09-16 16:24:27","","0","316","<machine-learning><nlp><text-mining><text-classification><stanford-nlp>","<p>There are many fast advancements in NLP field, BERT, RoBERTa, ALBERT, and XLNe, and no one can check the news or papers daily. Is there any way or site that keeps track of all these new developments and possibly provides a link to the code? For example, if someone needs to use text summarization, the suggested approach would be X, and so on.</p>
","nlp"
"102127","How to perform text classification on a dataset with many imbalanced classes","2021-09-16 10:10:21","","5","661","<python><nlp>","<p>I am completely new to NLP and I have been tasked with performing text classification on a dataset containing 193k records. The number of classes is 107.</p>
<p>The class with the highest number of records contains &gt; 16k entries, whereas the less frequent one contains only 5. You can see the frequency distribution below. The class names have been redacted due to confidentiality requirements.</p>
<p><a href=""https://i.sstatic.net/uuaQ8.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/uuaQ8.png"" alt=""class frequency distribution"" /></a></p>
<p>Each entry can contain up to 100 characters. The text is very terse and contains few words in English, with the remaining being codes, locations and names of people.</p>
<p>How would you tackle such a problem?
Does it make any sense to do text augmentation, or should I implement some form of weighting at the model evaluation stage?
If so, which text augmentation / weighing tools or procedures would you recommend?</p>
","nlp"
"102107","How can word2vec or BERT be used for previously unseen words","2021-09-15 13:03:22","102110","3","686","<nlp><word-embeddings><word2vec>","<p>Is there any way to modify word2vec or BERT to extend finding out embeddings for words that were not in the training data? My data is extremely domain-specific and I don't really expect pre-trained models to work very well. I also don't have access to huge amounts of this data so cannot train word2vec on my own. I was thinking something like a combination of word2vec and the PMI matrix (i.e. concatenation of the 2 vector representations). Would this work, would anyone have any other suggestions, please?</p>
<p>Thanks in advance!</p>
","nlp"
"102084","For text classification, would a BoW or Word Embeddings based model ever be better than a Language Model?","2021-09-14 18:47:58","","1","215","<nlp><word-embeddings><bert><text-classification><language-model>","<p>I've done a bit of research, with <a href=""https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794"" rel=""nofollow noreferrer"">this</a> being the best as far as objectively measuring quality, but wanted to ask from a theoretical perspective if BoW-based models (e.g. using TF-IDF) or word embeddings-based models (e.g. Word2Vec) would ever be a better choice than a language model (e.g. BERT) for a text classification problem?</p>
<p>The specific problem I'm working on is binary classification of short 2-8 word snippets such as &quot;Air bubble in ampoule&quot; into categories &quot;requires response&quot; or &quot;does not require response&quot;, but I'm more interested in the general question above.</p>
","nlp"
"102073","Types of averages when analyzing sentences","2021-09-14 12:53:14","","2","34","<python><nlp><word-embeddings>","<p>I have a list of words and their frequencies in a text corpus. So there are words like &quot;a&quot;, &quot;what&quot;, &quot;some&quot; that have really high frequencies, and other like &quot;neurodegenerative&quot; that are less popular.</p>
<p>I want to analyze sentences by assigning to each word its score and then determine if one sentence is more &quot;technical&quot;, or more specific to a domain than others. For example:</p>
<p><strong>&quot;I have a dog and a cat.&quot;</strong> vs. <strong>&quot;Mitochondria is the powerhouse of the cell.&quot;</strong></p>
<p>I was thinking of just calculating the average of these frequencies, but sometimes I have a sentence like:</p>
<p><strong>&quot;Migraine is a serious headache.&quot;</strong>, with average 640, and</p>
<p><strong>&quot;Typical examples of continuous functions which are not holomorphic are complex conjugation and taking the real part.&quot;</strong>, with average 600, because of the many short, very common words.</p>
<p>Is there any better way of evaluating such sentences to give a more realistic score, or average, that would indicate how &quot;niche&quot; they are?</p>
","nlp"
"101986","Calculating the dissimilarity between term frequency vectors","2021-09-11 21:25:55","101995","0","545","<nlp><data-mining>","<p>Given that a document is an object represented by what is called a term frequency vector. How can we calculate the dissimilarity between term frequency vectors?</p>
","nlp"
"101956","Comparing the similarity structure of 2 distance matrices (computed from sentence embedding)","2021-09-10 16:52:57","","1","42","<nlp><dimensionality-reduction><distance><semantic-similarity>","<p>I apologize if this question lacks clarity, my mathematical background on the topic is limited and was hoping to find some guidance. I would like to compare 2 distance matrices that contain pair-wise semantic (cosine) similarities for a set of 33 sentences. The matrices were created from sentence embedding, i.e., embedding of full sentences in a vector space (I used Google's Universal Sentence Encoder, so the vector space has 512 dimensions). The sets of sentences that underlie the 2 distance matrices describe the same 33 contents/events but one set contains more detailed descriptions, the other set less detailed (more &quot;condensed&quot;) descriptions. I would like to test how the loss of detail affects the structure of the distance matrices. In particular, I'm interested in the following 2 questions:</p>
<pre><code>Can the similarity structure of the condensed sentences be described in a lower-dimensional space than that of the detailed sentences?
</code></pre>
<p>--First thing that came to mind were dimensionality reduction technique (PCA etc.).</p>
<pre><code>(That might be directly related to the first question), is the &quot;range&quot; of distances for the detailed sentences wider than for the condensed sentences? That is, are there more &quot;extreme&quot; distances (high and low) for the detailed set of sentences?
</code></pre>
<p>--I was wondering whether multidimensional scaling could be useful. I would expect to see more clustering for the detailed set. But perhaps that doesn't make sense?</p>
<p>Any input would be greatly appreciated! Thank you!!</p>
","nlp"
"101945","Usage of ML models and DL models for text data","2021-09-10 11:21:27","","0","17","<machine-learning><deep-learning><nlp>","<p>I am newbie to NLP ,
I was  going through the some of the Kaggle notebooks then I got little bit confused. some of them are using ML models text classification and text regression problems and for same problems some of them are using Deep learning models.</p>
<p>I don't understand In what basis they are using ML models and DL models.
Cloud you please any one clarify this Question.</p>
","nlp"
"101904","How to use NER and POS for model input?","2021-09-08 13:54:25","","2","77","<deep-learning><keras><nlp><named-entity-recognition>","<p>I am building a model for contract information extraction, where NER and POS could serve relevant information. I am trying with Keras (and XGBoost). My question would be what are the techniques to use all of this information (the actual text, NER, POS) for my model input which is currently using only text. Could not find anything about it yet.</p>
","nlp"
"101880","How BERT model differentiate words with different meanings?","2021-09-07 19:35:59","","1","43","<nlp><bert>","<p>How BERT model differentiate between words with different meanings e.g. #Transformers like name of the movie or name of a library by @huggingface?</p>
","nlp"
"101877","What do we mean as Positive or Negative in Sentiment Analysis?","2021-09-07 17:25:48","","0","140","<nlp><sentiment-analysis>","<p>What do we mean in Sentiment Analysis NLP when it is said a sentence is positive or negative? I think I need to specify this regarding any other parameter. For example <em>&quot;iPhone is good&quot;</em> is a positive one respect to Apple Company, however that is negative one in respect to Samsung.</p>
","nlp"
"101872","Stemming/lemmatization for German words","2021-09-07 14:01:52","","2","1059","<python><nlp><nltk><scipy>","<p>I have a huge dataset of German words and their frequency in a text corpus (so words like &quot;der&quot;, &quot;die&quot;, &quot;das&quot; have a very high frequency, whereas terminology-like words have a very low frequency). Different forms of the same word, such as plural or 3rd person forms do appear, but there is no guarantee that this happens for every word.</p>
<p>I tried using <code>spacy.load('de_core_news_sm')</code> but it says it can't find the model. Other older posts don't mention anything reliable in this sense.</p>
<p>Maybe a second question: what could I do to determine a reliable popularity of a word using these frequencies when it comes to related words? For example, the singular form, Katze, has a frequency of 1000, but its plural, Katzen, has a frequency of 500. One idea is to add them; another idea is to make the plural have the same &quot;score&quot; as its singular, because the definition is the word is basically the same. How does one deal with this whe</p>
","nlp"
"101862","Cosine similarity between sentence embeddings is always positive","2021-09-07 08:54:54","101942","5","3977","<python><nlp><cosine-distance>","<p>I have a list of documents and I am looking for a) duplicates; b) documents that are very similar. To do so, I proceed as follows:</p>
<ol>
<li>Embed the documents using <a href=""https://huggingface.co/sentence-transformers/paraphrase-xlm-r-multilingual-v1"" rel=""noreferrer"">paraphrase-xlm-r-multilingual-v1</a>.</li>
<li>Calculate the cosine similarity between the vector embeddings (code below).</li>
</ol>
<p>All the cosine similarity values I get are between 0 and 1. Why is that? Shouldn't I also have negative cos similarity values? The sentence embeddings have both positive and negative elements.</p>
<pre><code>num_docs = np.array(sentence_embedding).shape[0]

cos_sim = np.zeros([num_docs, num_docs])
for ii in range(num_docs):
    for jj in range(num_docs):
        if ii != jj:
            cos_sim[ii, jj] = np.dot(sentence_embedding[ii], sentence_embedding[jj].T)/(norm(sentence_embedding[ii])*norm(sentence_embedding[jj]))
</code></pre>
","nlp"
"101819","Choice of the number of topics (clusters) in textual data","2021-09-05 10:15:05","","1","165","<nlp><clustering><text-mining><topic-model><lda>","<p>I have a social science background and I'm doing a text mining project.
I'm looking for advice about the choice of the number of topics/clusters when analyzing textual data. In particular, I'm analyzing a dataset of more than 200000 tweets and I'm performing a Latent Dirichlet allocation model on them to find clusters that represent the main topics of the tweets of my dataset. However, I was trying to decide the optimal number of clusters but the results I'm finding in the picture seem inconsistent.
<a href=""https://i.sstatic.net/t7GA2.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/t7GA2.png"" alt=""enter image description here"" /></a></p>
<p>I'm struggling with the choice of the number of clusters. So the question is: what number would you choose from the plot?
Moreover, do you think there are other ways and/or conventional rules that one can rely on to choose the number of clusters?</p>
","nlp"
"101784","How to measure the accuracy of an NLP paraphrasing model?","2021-09-04 04:15:03","101790","4","424","<deep-learning><nlp><model-evaluations><huggingface>","<p>I using the HuggingFace library to do sentence paraphrasing (given an input sentence, the model outputs a paraphrase). How am I supposed to compare the results of two separate models (one trained with t5-base, the other with t5-small) for this task? Can I just compare the validation loss or do I need to use a metric (if so, what metric)?</p>
","nlp"
"101783","Please explain Transformer vs LSTM using a sequence prediction example","2021-09-04 03:55:12","","1","1658","<nlp><lstm><rnn><transformer>","<p>I don't understand the difference in mechanics of a transformer vs LSTM for a sequence prediction problem. Here is what I have gathered so far:</p>
<p><strong>LSTM:</strong></p>
<p><a href=""https://i.sstatic.net/bmL98.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bmL98.png"" alt=""LSTM"" /></a></p>
<p>suppose we want to predict the remaining tokens in the word 'deep' given the first token 'd'. Then the first input will be 'd', and the predicted output is 'e'. Now at the next time step, the previously predicted output 'e' is fed along with the previous hidden state which contains information on 'd'. This is done till the predicted output is </p>
<p><strong>Transformer:</strong></p>
<p>In the same example, how would the transformer work, and avoid sequential inputs? Would we be giving the entire word 'deep' as input and leave it to the network to get the characters in correct sequence, or do we only input 'd' (which is what we did in LSTM)?</p>
<p>I'm really confused and I think I am missing out on some very fundamental concepts here. Would be really thankful for your help. Thanks!</p>
","nlp"
"101774","What is the appropriate NLP model to generate paraphrased sentences of a specific form?","2021-09-03 19:37:00","","1","82","<machine-learning><nlp>","<p>So my problem is actually quite simple: I have a set of sentences all of a very similar structure and another set of sentences that have basically the same meaning (but not quite as the example shows), but expressed differently eg: &quot;Go get 10 apples&quot;, &quot;I have gone and got 10 apples&quot;, and I would like a way to reliably convert a sentence of type 1 to its type 2 counterpart (given that the type 1 sentence is a never-before-seen user input)</p>
<p>Now, since this is not quite paraphrasing I am struggling to figure out the correct model/approach to use for achieving this task. It could obviously be done using GPT-J, GPT-neo etc, but these seem way too heavy for this small and specific task that I am trying to achieve.</p>
<p>I am hoping that there are some already pretrained models that could achieve this easily, but if not and I have to generate my own dataset and train my own model, then at least I know my effort is not getting wasted.</p>
<p>Any reccomendations?</p>
","nlp"
"101749","Freelancing in Data Science","2021-09-03 09:45:42","","2","71","<machine-learning><deep-learning><nlp>","<p>I would like to dive into freelancing in data science but I don't know how to do it, which site I can consult.</p>
","nlp"
"101742","Why is sequence prediction always the objective in RNN and LSTM like algorithms","2021-09-03 02:41:27","101744","0","21","<nlp><lstm><rnn><sequence-to-sequence>","<p>The title is pretty much my question. I haven't seen any literature yet that uses a different training objective. The goal is to find the hidden states eventually, then why is it that only 1 method is so popular, and there are no others seen?</p>
","nlp"
"100628","When calculating lexical richness (e.g. TTR) do you lemmatize first?","2021-08-29 21:37:53","100645","1","153","<nlp>","<p>When calculating Type-Token Ration (TTR) and Hapax richness (along with similar measures), do you lemmatize the corpus first?</p>
","nlp"
"100614","How to expand lists?","2021-08-29 15:03:27","100626","1","25","<nlp>","<p>In lists the main noun is often only mentioned at the end. However, for e.g. NER-tasks, I would like to &quot;expand&quot; them:</p>
<ul>
<li>Outward and return flight -&gt; outward flight and return flight</li>
<li>Proboscis, Vervet, and golden snub-nosed monkey -&gt; Proboscis monkey, Vervet monkey, and golden snub-nosed monkey</li>
<li>education in mathematics or physics -&gt; education in mathematics or education in physics</li>
</ul>
<p>Are there already tools out there (bonus points for support of German language)? Google only led me to expanding contractions (&quot;I've&quot; -&gt; &quot;I have&quot;)...</p>
","nlp"
"100598","Is it possible to add new vocabulary to BERT's tokenizer when fine-tuning?","2021-08-29 07:33:44","","2","3697","<nlp><word-embeddings><bert><finetuning>","<p>I want to fine-tune BERT by training it on a domain dataset of my own. The domain is specific and includes many terms that probably weren't included in the original dataset BERT was trained on. I know I have to use BERT's tokenizer as the model was originally trained on its embeddings.
To my understanding words unknown to the tokenizer will be masked with [UNKNOWN]. What if some of these words are common in my dataset? Does it make sense to add new IDs for them? is it possible without interferring with the network's parameters and the existing embeddings? If so, how is it done?</p>
","nlp"
"100553","Classification of scanned documents in pdf files using deep learning or NLP","2021-08-27 05:13:53","","1","1554","<python><deep-learning><nlp><image-classification><similar-documents>","<p>I know classifying images using cnn but I have a problem where I have multiple types of scanned documents in a pdf file on different pages. Some types of scanned documents present in multiple pages inside the pdf.</p>
<p>Now I have to classify and return which documents are present and the page numbers in which they present in the pdf document. If scanned document is in multiple pages I should return the range of page numbers like &quot;1 - 10&quot;.</p>
<p><strong>Input</strong> will be pdf files containing scanned target documents</p>
<p><strong>Output</strong> should be classified &quot;Document Name&quot; and Its &quot;page numbers&quot;</p>
<p>Can any one guide me on how can I a build a model that can address this problem.</p>
<p>Thankyou</p>
","nlp"
"100520","How to Inference With Keras Sequential Models (Text Classification)","2021-08-25 22:38:25","100524","0","362","<python><keras><nlp><tensorflow><lstm>","<p><strong>I have the following LSTM model and I can't make inference with it:</strong></p>
<pre><code>print(&quot;Define LSTM model&quot;)

rnnmodel=Sequential()
rnnmodel.add(embedding_layer)

rnnmodel.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))
rnnmodel.add(Dense(2, activation=&quot;sigmoid&quot;))

rnnmodel.compile(loss=&quot;binary_crossentropy&quot;,
                 optimizer=&quot;adam&quot;,
                 metrics=[&quot;accuracy&quot;])

rnnmodel.fit(X_train, y_train,
             batch_size=256,
             epochs=1,
             validation_data=(x_val, y_val))

score, acc=rnnmodel.evaluate(test_data, test_labels, batch_size=128)
print(f&quot;Test accuracy with RNN: {acc}&quot;)
</code></pre>
<p>(epoch is 1 to test) I want to make an inference with the text, let's say</p>
<pre><code>text=[&quot;the product was horrible&quot;]
</code></pre>
<p>I check the documentation of <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#predict"" rel=""nofollow noreferrer"">tf.keras.Sequential</a> and it states I should use the <code>predict</code> function and the input should be <em>&quot;A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs).&quot;</em></p>
<p><strong>So what I did is:</strong></p>
<pre><code>text=[&quot;the product was horrible&quot;]
inference_sequence=tokenizer.texts_to_sequences(text)
inference_data=pad_sequences(inference_sequence, maxlen=MAX_SEQUENCE_LENGTH)

predictions=rnnmodel.predict(inference_data)

print(predictions)
</code></pre>
<p>and it gives me the result <code>[[0.63219154 0.33410403]]</code></p>
<p><strong>However I've given only one sentence. Why it gives me two results? I checked the <code>sigmoid</code> documentation from <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/activations/sigmoid"" rel=""nofollow noreferrer"">here</a> and for an confirmed it should return only one result. So what's the problem here?</strong></p>
<p><strong>I also tried other approaches to make inference like mentioned <a href=""https://stackoverflow.com/questions/61443543/how-to-make-prediction-on-keras-text-classification"">https://stackoverflow.com/questions/61443543/how-to-make-prediction-on-keras-text-classification</a></strong></p>
<p><strong>So I did:</strong></p>
<pre><code>text=[&quot;the product was horrible&quot;]
rnnmodel.predict(text)
</code></pre>
<p>and it gives me the warning: <em>WARNING:tensorflow:Model was constructed with shape (None, 1000) for input Tensor(&quot;embedding_input:0&quot;, shape=(None, 1000), dtype=float32), but it was called on an input with incompatible shape (None, 1).</em> <strong>and stuck forever.</strong></p>
<p>What should I do I just can't make an inference.</p>
","nlp"
"100516","How effective is text generation?","2021-08-25 19:35:40","","0","33","<nlp><text-generation><nlg>","<p>I have implemented some basic models like composing a poem using the dataset of poems. But the results were not that good in general. I want to make a model that could write an essay for me. One strong motivation for making this essay writing model is that it would help me to escape my tedious and useless college assignments. But before I proceed and hope for making such a human-like model that could write an essay for me, I wanted to ask whether such a model is even possible to make in the first place?</p>
","nlp"
"100508","Personal Project Classifying Bank Account Data - NLP Noob","2021-08-25 17:28:13","","1","28","<nlp><multiclass-classification><spacy><huggingface>","<p>Background:</p>
<p>I'm looking to get up to speed with some of the newer ML classification techniques and keep my ML python fresh in my spare time/learn some new skills, so as a challenge I'm trying to see if I can beat some of the budgeting apps with their classification (and make the tagging a bit more personal). I use Mint and have easy access to my own banking data so have a good way to benchmark this little project. Data is &lt; 100 characters, unstructured (I'm not going to share my personal data as thats all I have at the moment, but to give an idea of the structure I'm using have a look at your csv bank statement!).</p>
<p>Ideally I'd love to have way to tag a couple of &quot;levels&quot; e.g. Restaurants and Eating Out : Coffee shop with the merchant name e.g. Starbucks cleansed too.</p>
<p>Context:</p>
<p>I work in Python, familiar with Tensorflow (but use more for gradient descent type optimizations in my day to day work). I haven't done any NLP in over 10 years. Back then I used Naive Bayes classifier which took some heavy lift... things have come on A LOT since then.</p>
<p>So few questions:</p>
<ul>
<li><p>What are the current tools people use for this type of exercise. I've read about spaCy, huggingface - are these the main tools used? What are the pros cons to each if so? (without sounding too old... there is so much off the shelf nowadays since I last looked at this space... props to the open source community!!!)</p>
</li>
<li><p>Any other tools I should be considering?</p>
</li>
<li><p>Any good articles people have read where people have tried to implement this problem? Any good online resources tackling this?</p>
</li>
<li><p>Any gotchas I should watch out for?</p>
</li>
<li><p>Any public large datasets anyone is aware of I could use?</p>
</li>
<li><p>And also I plan to do this in Python, because that's what I'm familiar with, but if people are tackling this in a different language I'd love to hear which and why (and maybe I'll give it a spin).</p>
</li>
</ul>
","nlp"
"100483","How do I get ngrams for all combinations of words in a sentence?","2021-08-25 07:09:36","","1","822","<machine-learning><nlp><tfidf><ngrams>","<p>Lets say I have a sentence &quot;I need multiple ngrams&quot;. If I create bigrams using Tf idf vectorizer it will create bigrams only using consecutive words. i.e. I will get &quot;I need&quot;, &quot;need multiple&quot;, &quot;multiple ngrams&quot;.</p>
<p>How can I get &quot;I mutiple&quot;, &quot;I ngrams&quot;, &quot;need ngrams&quot;?</p>
","nlp"
"100450","Sudden jumps in accuracy with logistic regression and bag of words : ""glm.fit: algorithm did not converge""","2021-08-24 08:13:01","","1","64","<nlp><r><linear-regression><logistic-regression><glm>","<p>I work on a bag of words, on the Toxic Comments Classifications challenge. The challenge is closed but the dataset is very nice to learn.</p>
<p>I use R, tf-idf, tm, and logistic regression.</p>
<p>I have a strange pattern in the accuracy results, linked with the error: &quot;glm.fit: algorithm did not converge&quot;. It tired the solution proposed in other answers and multiplied maxit by 4, but it did not help.</p>
<h3>Glimpse of the functions used</h3>
<h4>sub-sampling</h4>
<p>Original distribution is 200K non-toxic (0) and 20K toxic (1)</p>
<pre><code>set.seed(42)

df_toxic = df[df<span class=""math-container"">$toxic == 1,]
df_ok = df[df$</span>toxic == 0,]

df_ok_sampled = df_ok[sample(nrow(df_ok), nrow(df_toxic)), ]

df_sub = bind_rows(df_ok_sampled,df_toxic)
</code></pre>
<h4>Bag of words creation</h4>
<pre><code>
# Words 

control_list_words = list(
    tokenize = words,
    language=&quot;en&quot;,
    bounds = list(global = c(100, Inf)),
    weighting = weightTfIdf,
    tolower = TRUE,
    removePunctuation = TRUE,
    removeNumbers = TRUE,
    stopwords = TRUE,
    stemming = TRUE
)

dtm_words = DocumentTermMatrix(corpus, control=control_list_words) 
  
# nGrams

control_list_ngrams = list(
    tokenize = nGramsTokenizer,
    language=&quot;en&quot;,
    bounds = list(global = c(1000, Inf)),
    weighting = weightTfIdf,
    tolower = TRUE,
    removePunctuation = TRUE,
    removeNumbers = TRUE,
    # We don't remove stop-words for nGrams as structure like &quot;are a&quot; or &quot;such a&quot; are meaningful for toxic comments
    stopwords = FALSE,
    stemming = TRUE
)
  
dtm_ngrams = DocumentTermMatrix(corpus, control=control_list_ngrams)

# Merge the two
X = cbind(m_words,m_ngrams)

</code></pre>
<h4>Remove correlations</h4>
<pre><code>highlyCor = findCorrelation(cor(bow), cutoff = cutoff, exact = TRUE)

pruned_bow = bow[,-as.vector(highlyCor)]
</code></pre>
<h4>Logistic regression</h4>
<pre><code>f &lt;- glm(df_toxic ~ ., data=df_train, maxit = 100, family = 'binomial')
</code></pre>
<h3>Correlation cutoff vs accuracy</h3>
<p><a href=""https://i.sstatic.net/wMQMx.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/wMQMx.png"" alt=""GLM accuracy according to features pruning by correlation detection."" /></a></p>
<h3>Allure of the confusion matrix</h3>
<p>In the high dimensions yet low accuracy intervals: unbalanced</p>
<pre><code>           Reference
Prediction    0    1
         0 2530 3253
         1  243 5598
</code></pre>
<p>In the high accuracy intervals: balanced</p>
<pre><code>           Reference
Prediction    0    1
         0 4883  900
         1  641 5200
</code></pre>
<p>In the low dimensions and low accuracy intervals: unbalanced in the other way</p>
<pre><code>           Reference
Prediction    0    1
         0 5272  511
         1 3239 2602
</code></pre>
<h3>???</h3>
<p>Do you know what exactly is this &quot;glm.fit: algorithm did not converge&quot; and why raising maxit to 100 did not help?</p>
<p>Thanks</p>
","nlp"
"100441","NLP techniques for converting from a direct speech to a reported speech","2021-08-23 20:18:09","100442","0","464","<machine-learning><deep-learning><nlp>","<p>Any idea of some NLP techniques to transform a direct speech to a reported speech ?</p>
<p>Example converting : <em>&quot;I'm learning NLP&quot; said a user</em> to : <em>a user said he's learning NLP.</em></p>
<p>I thought about paraphrasing but not sure..</p>
<p>Thank you!</p>
","nlp"
"100429","Sparse matrix after vectorization giving size = 1","2021-08-23 15:43:18","","0","536","<python><nlp><tfidf>","<p>I am working on a NLP problem <a href=""https://www.kaggle.com/c/nlp-getting-started"" rel=""nofollow noreferrer"">https://www.kaggle.com/c/nlp-getting-started</a>. I want to perform vectorization after <code>train_test_split</code> but when I do that, the resulting sparse matrix has size = 1 which cannot be right. Below is my code:</p>
<pre><code>def clean_text(text):
    tokens = nltk.word_tokenize(text)    #tokenizing the words
    lower = [word.lower() for word in tokens]  #converting words to lowercase
    remove_stopwords = [word for word in lower if word not in set(stopwords.words('english'))]  
    remove_char = [word for word in remove_stopwords if word.isalpha()]
    lemm_text = [ps.stem(word) for word in remove_char]     #lemmatizing the words
    cleaned_data = &quot; &quot;.join([str(word) for word in lemm_text])
    return cleaned_data

x['clean_text']= x[&quot;text&quot;].map(clean_text)

x.drop(['text'], axis = 1, inplace = True)

from sklearn.model_selection import train_test_split
train_x, test_x, train_y, test_y = train_test_split(x, y, test_size = 0.2, random_state = 69, 
stratify = y)

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
tfidf = TfidfVectorizer()
train_x_vect = tfidf.fit_transform(train_x)
test_x1 = tfidf.transform(test_x)

pd.DataFrame.sparse.from_spmatrix(train_x_vect,
                              index=train_x.index,
                              columns=tfidf.get_feature_names())    
</code></pre>
<p>When I try to convert the sparse matrix (with size = 1) into a dataframe, it gives me error</p>
<pre><code>ValueError: Index length mismatch: 4064 vs. 1
</code></pre>
<p>The dataframe <code>x</code> has size = 4064 and my sparse matrix has size = 1 which is why it is giving me error. Any help will be aprreciated!</p>
","nlp"
"100392","Why are words represented by frequency counts before embedding?","2021-08-22 18:35:42","100397","2","246","<machine-learning><deep-learning><nlp><text-mining><word-embeddings>","<p>Before getting vector representations of words by embedding, the words are mapped to numbers. These numbers are chosen to be the frequency of that word in the dataset. Why does this convention exist? Does it have any effects, or is it arbitrary?</p>
","nlp"
"100388","How is CBOW different from building PMI matrix and then reducing using PCA","2021-08-22 17:58:50","","0","61","<neural-network><nlp><pca>","<p>PMI matrix and reduction using PCA: Based on the number of times 2 words appear together (in a certain pre-defined window), and the individual frequency of words, we build the PMI matrix. Then reduce it using PCA, to get dense representations of each word in the corpus, which are able to capture some semantics of the text</p>
<p>CBOW: Learning word representations through a neural network, whose end objective is to maximize the probability of correct word pairs. The probability values are known in advance by counting the number of times a word is appearing in another word's context in the training data.</p>
<p>Both of these methods are using counts, and then getting a dense word representation. Is there a definitive advantage of one over the other? Why was CBOW introduced at all when the former method is doing the exact same job?</p>
","nlp"
"100349","Splitting before tfidf or after?","2021-08-21 08:26:26","100353","0","1900","<machine-learning><python><nlp><preprocessing><data-leakage>","<p>When should I perform preprocessing and matrix creation of text data in NLP, before or after <code>train_test_split</code>? Below is my sample code where I have done preprocessing and matrix creation (tfidf) before <code>train_test_split</code>. I want to know will there be data leakage?</p>
<pre><code>corpus = []

for i in range(0 ,len(data1)):
    review = re.sub('[^a-zA-Z]', ' ', data1['features'][i])
    review = review.lower()
    review = review.split()
    review = [stemmer.stem(j) for j in review if not j in set(stopwords.words('english'))]
    review = ' '.join(review)
    corpus.append(review)

from sklearn.feature_extraction.text import TfidfVectorizer
cv = TfidfVectorizer(max_features = 6000)
x = cv.fit_transform(corpus).toarray()

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y = le.fit_transform(data1['label'])

from sklearn.model_selection import train_test_split
train_x, test_x, train_y, test_y = train_test_split(x, y, test_size = 0.2, random_state = 69, 
                                                                                stratify = y)

spam_model = MultinomialNB().fit(train_x, train_y)
pred = spam_model.predict(test_x)
c_matrix = confusion_matrix(test_y, pred)
acc_score = accuracy_score(test_y, pred)
</code></pre>
","nlp"
"100322","How to improve the evaluation score for highly imbalanced dataset?","2021-08-20 13:07:46","","0","223","<machine-learning><deep-learning><nlp><bert>","<p>I have trained my BERT model(bert-base-cased) to detect toxic comments. I used the Toxic Comment Classification Challenge dataset from the Kaggle. My accuracy is 98% and the AUROC for various sub-classes is above 90%. However, my Precision, Recall, and F1 score is less. The scores are shown in the image <a href=""https://i.sstatic.net/YpNAP.png"" rel=""nofollow noreferrer"" title=""Scores"">Evaluation Scores</a>.
The dataset is highly imbalanced. The ratio of clean comments is way higher than the toxic comments. Any suggestions to improve the evaluation scores?</p>
<p>Here's the final score</p>
<pre><code>           precision  recall  f1-score   support
</code></pre>
<p>micro avg       0.61      0.85      0.71      1743  <br/>
macro avg       0.56      0.69      0.61      1743<br />
weighted avg    0.64      0.85      0.72      1743  <br/>
samples avg     0.08      0.09      0.08      1743</p>
","nlp"
"100314","How to extract skills from job description using neural network","2021-08-20 11:01:30","","2","1301","<python><deep-learning><neural-network><nlp><lstm>","<p>I am doing a project where I have to extract skills from Job Description.</p>
<p>I have attempted by cleaning data(not removing stopwords), applying POS tag, labelling sentences as skill/not_skill, trained data using LSTM network. But while predicting it will predict if a sentence has skill/not_skill.</p>
<p>Glimpse of how the data is</p>
<pre><code>description                       skill/not_skill
Datacience and machine_learning         skill
not information extracted               not_skill
learn skill Python                        skill
</code></pre>
<p>I have read articles and research papers but I am not sure how to proceed after this. How do I extract skills.</p>
<p>This method I am following is from this research paper <code>(using Supervised approach)</code>. <a href=""https://confusedcoders.com/wp-content/uploads/2019/09/Job-Skills-extraction-with-LSTM-and-Word-Embeddings-Nikita-Sharma.pdf"" rel=""nofollow noreferrer"">https://confusedcoders.com/wp-content/uploads/2019/09/Job-Skills-extraction-with-LSTM-and-Word-Embeddings-Nikita-Sharma.pdf</a></p>
","nlp"
"100274","Not clear about relative position bias","2021-08-19 11:39:15","","1","2605","<nlp><computer-vision><word-embeddings><transformer><embeddings>","<p>I've been reading the <a href=""https://arxiv.org/abs/2103.14030"" rel=""nofollow noreferrer"">Swin Transformer</a> paper and came across <strong>relative position bias</strong> concept. I'm not able to figure out how is it more effective than positional embeddings. I hope someone can explain it intuitively. Thanks in advance!</p>
","nlp"
"100272","Data Set and guidance for Occupations/ Roles classification problem","2021-08-19 10:56:48","","0","28","<machine-learning><nlp><dataset><word2vec><fasttext>","<p>I am working on a project where I need to find similar roles -- for example, Software Engineer, Soft. Engineer , Software Eng ( all should be marked similar)</p>
<p>Currently, I have tried using the <a href=""https://www.bls.gov/soc/"" rel=""nofollow noreferrer"">Standard Occupational Classification Dataset</a> and tried using LSA, Leveinstein and unsupervised FastText with Word Movers Distances. The last option works but isn't great.</p>
<p>I am wondering if there are more comprehensive data sets or ways available to solve this problem??
Any lead would be helpful!</p>
","nlp"
"100262","How does an RNN differ from a CBOW model","2021-08-19 04:27:06","","0","220","<nlp><rnn><prediction>","<p>CBOW: We are trying to predict the next word based on the context (defined as a certain window of words around the target word)</p>
<p>RNN can also be used for predicting the next word in a sequence, where each time the input is the present input and the recent past (i.e. output of the previous step)</p>
<p>I am not able to understand how the RNN's approach is somehow better, because I could define a very large window for CBOW, and then it would also predict words based on previous information. Why do we say RNN will be better at predicting because it has memory? If we supply everything to CBOW at the same time, which is given to RNN in steps, why is one better than the other? Is it only because we save on computation in the latter case?</p>
","nlp"
"100252","Who is supposed to label my sentiment analysis? Linguistics or psychologist?","2021-08-18 22:45:46","107488","3","42","<nlp><data-mining><text-mining><sentiment-analysis><labels>","<p>I'm starting off my undergraduate research on text classification even though I'm still considered new to this topic. I've collected more than 20K data from Twitter. I've been trying to label the data into 3 sentiments, positive, negative, and neutral. But, I oftentimes find it difficult to determine whether a tweet is categorized as positive, negative, or neutral due to my lack of knowledge in that field.</p>
<p>My supervisor has someone with a psychology background that can help me labeling the data instead of linguistics. I'm aware that sentiment analysis is a part of the natural language processing task which makes more sense if I ask for help from linguistics, but at the same time sentiment, in general, is also studied in psychology as I read more papers. Does anyone have any advice? Thank you in advance!</p>
","nlp"
"100160","How is attention different from linear MLPs?","2021-08-16 18:40:53","","4","2028","<deep-learning><nlp><transformer><attention-mechanism>","<p>Each output for both the attention layer (as in transformers) and MLPs or feedforward layer(linear-activation) are weighted sums of previous layer. So how they are different?</p>
","nlp"
"100114","Can I use a pre-trained model for sentiment analysis/text classification of unlabelled data?","2021-08-15 19:45:46","","2","222","<machine-learning><classification><nlp><text-mining><sentiment-analysis>","<p>I'm planning on working on a project where I'll have a large collection of tweets about coronavirus vaccines. None of the tweets will have a label (e.g. positive, neutral, negative). Therefore I won't be able to train a model based on the labels.</p>
<p>I have a vague understanding of pre-trained models like BERT or VADER.
I don't know however if I can use a model trained on other (text) data (like the ones mentioned above) and use it to run a sentiment analysis for the tweets I have.</p>
<p>Is it possible to do this? Or would it require labeled data in order to train the model with <em>that specific</em> data relating to the vaccine tweets?</p>
","nlp"
"100033","In Text Classification if I get similar performance with 100 features and 200 features, which model should I go ahead with?","2021-08-13 11:58:35","","1","87","<machine-learning><nlp><feature-selection><machine-learning-model><text-classification>","<p>I have built two text classifier models, one has 200 features the other has 100 features (reduced to 100 from 200 after feature selection). I see similar performances in both. Which model should I go ahead with for production?</p>
","nlp"
"100001","Paraphrasing a sentence and changing the tone of it","2021-08-12 18:39:11","","1","295","<machine-learning><nlp><transformer><machine-translation>","<p>I am trying to make a model that is capable of translating a sentence into a new and a better form. I would like the model to change the tone and also give it some character. I am using this in my web app UI, simply allowing the users to witness new description as they refresh the page. For example, &quot;You are logged out&quot; -&gt; &quot;Looks like you have logged out&quot;. Something of such sort, any idea on this?</p>
","nlp"
"99996","applicability of relative similarity computation","2021-08-12 16:00:51","","0","22","<nlp><word-embeddings><similarity><cosine-distance><semantic-similarity>","<p>I've computed the cosine similarity between <code>a</code> &amp; <code>b</code> (<code>=x</code>) and <code>b</code> &amp; <code>c</code> (<code>=y</code>). I can use the same embeddings to compute the similarity between <code>a</code> and <code>c</code> (assuming it's = <code>z</code>).</p>
<p>I've a situation wherein I've only the similarity measures <code>x</code> and <code>y</code>. How can I find the similarity between <code>a</code> &amp; <code>c</code>, without the original embeddings? If I use a plane to represent this then I will have infinite number of solutions. Are there any approaches which provides some insights about the relation between <code>a</code> and <code>c</code>?</p>
<pre><code>a = torch.Tensor([1, 2])
b = torch.Tensor([1, -1])
c = torch.Tensor([2, 3])

sim_ab = torch.dot(a, b) / (torch.sqrt(sum(torch.square(a)) * sum(torch.square(b))))
sim_ac = torch.dot(a, c) / (torch.sqrt(sum(torch.square(a)) * sum(torch.square(c))))
sim_bc = torch.dot(b, c) / (torch.sqrt(sum(torch.square(b)) * sum(torch.square(c))))

print(&quot;Actual similarity between b and c: &quot;, sim_bc)

x = torch.arccos(sim_ab)
y = torch.arccos(sim_ac)

print(&quot;Measured similarity between b and c: &quot;, torch.cos(x-y))

&gt;&gt;&gt; Actual similarity between b and c:  tensor(-0.1961)
&gt;&gt;&gt; Measured similarity between b and c:  tensor(-0.1961)

</code></pre>
<p><a href=""https://i.sstatic.net/qlgyI.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/qlgyI.png"" alt=""geometric representation"" /></a></p>
","nlp"
"99984","Restrict Date parser in certain cases","2021-08-12 11:15:35","","0","30","<machine-learning><python><nlp><text-mining><stanford-nlp>","<p>Sorry if the title wasn't self-explanatory. Here is a detailed version.</p>
<p>I created a data parser to parse dates from resumes. The ultimate goal is to find how many years of work experience a candidate has &quot;based on the resume.&quot; The parser can catch dates in all formats like:</p>
<ol>
<li>MM/DD/YY - MM/DD/YY</li>
<li>MM/DD/YYYY - MM/DD/YYYY</li>
<li>Apr 09 - Jul 11</li>
<li>03/09 - 07/11</li>
<li>2007 - 2010
etc.</li>
</ol>
<p>The way in which the parser works is it first extracts all the dates and then converts them into &quot;Apr 2009 - Jul 2011&quot; format (the difference is, whatever format the date is written in the resume, it will be converted like this regardless) and be stored in a <strong>list</strong></p>
<p>The Next step after conversion of all the dates is, sorting them based on the &quot;last four&quot; characters of each element. For eg: if the parsed dates are</p>
<pre><code>[&quot;Apr 2009 - Jul 2011&quot;, &quot;Jan 2014 - Oct 2018&quot;, &quot;Feb 2013 - Jun 2014&quot;, &quot;Nov 2018 - Aug 2021&quot;, &quot;Mar 2010 - Sep 2012&quot;, &quot;Jan 2005 - Mar 2008&quot;]
</code></pre>
<p>The last four &quot;Apr 2009 - Jul 2011&quot; is <strong>2011</strong>, likewise the whole list is sorted (descending order) based on this criteria. So the list now becomes like this:</p>
<pre><code>[&quot;Nov 2018 - Aug 2021&quot;, &quot;Jan 2014 - Oct 2018&quot;, &quot;Feb 2013 - Jun 2014&quot;, &quot;Mar 2010 - Sep 2012&quot;, &quot;Apr 2009 - Jul 2011&quot;,&quot;Jan 2005 - Mar 2008&quot;]
</code></pre>
<p>And the total number of years, months are calculated by just summing the dates. I also ignore overlapping dates. In the above case if u notice the dates <strong>&quot;Mar 2010 - Sep 2012&quot;</strong> &amp; <strong>&quot;Apr 2009 - Jul 2011&quot;</strong> have overlapping months. So only <strong>&quot;Mar 2010 - Sep 2012&quot;</strong> is considered and <strong>&quot;Apr 2009 - Jul 2011&quot;</strong> is discarded.</p>
<p>Now coming to the issue I am facing, sometimes in CVs people tend to mention the software they have used in the past. (Un)fortunately, some of that software come along with their &quot;year as version&quot; For eg: <strong>SQL Server 2005 2008</strong> or <strong>Windows Server 2010 - 2011</strong> etc.</p>
<p>What's happening is.. my date parser is catching these years too. Luckily, sometime's these dates are discarded due to overlapping.
Unfortunately, sometimes <strong>these dates</strong> are being considered and actual dates which are supposed to be considered, are being discarded due to overlapping with the <strong>versions</strong>.</p>
<p>I can't think of any generalized way(atleast not yet) to avoid catching the &quot;year as version&quot; pattern. Can someone help me out on this? How can I prevent my parser from catching such patterns?</p>
<p><strong>TIA.</strong></p>
","nlp"
"99933","How to do Bert Finetuning of failure cases?","2021-08-11 07:54:06","","1","44","<nlp><bert>","<p>I have a large dataset(public available) of text that is labelled. However the test distribution (actual production setting of company) while similar is not from the same source and thus tends to fail on some cases. It is not possible to label our test distribution due to manpower issues. How do I incrementally train my Bert Model to better handle failure cases  without overfitting? (i.e. we only label failure cases due to complains from clients which are not many.)</p>
","nlp"
"99804","Measuring coherence score for Top2Vec models","2021-08-06 23:30:16","","0","1085","<nlp><topic-model><coherence>","<p>I am working on creating a number of <a href=""https://github.com/ddangelov/Top2Vec"" rel=""nofollow noreferrer"">Top2Vec models</a> on Reddit threads. I am basically changing the <a href=""https://hdbscan.readthedocs.io/en/latest/parameter_selection.html#selecting-min-cluster-size"" rel=""nofollow noreferrer"">HDBScan cluster sizes</a> to get different clusters of the Doc2Vec embeddings representing a different # of topics.</p>
<p>I am trying to compare different models using their coherence score. I have tried using Gensim's <a href=""https://radimrehurek.com/gensim/models/coherencemodel.html"" rel=""nofollow noreferrer"">coherence score</a> but failed. I got an error message indicating that a word in the topics is not included in the dictionary.</p>
<p>I also tried using <a href=""https://tmtoolkit.readthedocs.io/en/latest/topic_modeling.html?highlight=coherence#Topic-coherence"" rel=""nofollow noreferrer"">tmtooklit</a>. While I could get the Document Term Matrix (DTM) easily, I have not been able to get the topic-word distribution using Top2Vec.</p>
<p>Questions:</p>
<ol>
<li>Can I resolve either of the issues indicated above (get the dictionary to list all of the terms necessary or producing the topic-word distribution)?</li>
<li>Are there other metrics that can be used to be compare Top2Vec models?</li>
</ol>
","nlp"
"99796","HuggingFace Transformers is giving loss: nan - accuracy: 0.0000e+00","2021-08-06 19:23:11","","2","2756","<nlp><bert><huggingface><loss>","<p>I am a HuggingFace Newbie and I am fine-tuning a BERT model (<code>distilbert-base-cased</code>) using the Transformers library but the training loss is not going down, instead I am getting <code>loss: nan - accuracy: 0.0000e+00</code>.</p>
<p>My code is largely per the boiler plate on the [HuggingFace course][1]:-</p>
<pre><code>model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=3)

opt = Adam(learning_rate=lr_scheduler)

model.compile(optimizer=opt, loss=loss, metrics=['accuracy'])

model.fit(
    encoded_train.data,
    np.array(y_train),
    validation_data=(encoded_val.data, np.array(y_val)),
    batch_size=8,
    epochs=3
)
</code></pre>
<p>Where my loss function is:-</p>
<p><code>loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)</code></p>
<p>The learning rate is calculated like so:-</p>
<pre><code>lr_scheduler = PolynomialDecay(
    initial_learning_rate=5e-5,
    end_learning_rate=0.,
    decay_steps=num_train_steps
    )
</code></pre>
<p>The number of training steps is computed thus:-</p>
<pre><code>batch_size = 8
num_epochs = 3

num_train_steps = (len(encoded_train['input_ids']) // batch_size) * num_epochs
</code></pre>
<p>So far then all very much like the boiler plate code.</p>
<p>My data looks like this:-</p>
<pre><code>{'input_ids': &lt;tf.Tensor: shape=(1040, 512), dtype=int32, numpy=
array([[  101,   155,  1942, ...,     0,     0,     0],
       [  101, 27900,  7641, ...,     0,     0,     0],
       [  101,   155,  1942, ...,     0,     0,     0],
       ...,
       [  101,   109,  7414, ...,     0,     0,     0],
       [  101,  2809,  1141, ...,     0,     0,     0],
       [  101,  1448,  1111, ...,     0,     0,     0]], dtype=int32)&gt;, 'attention_mask': &lt;tf.Tensor: shape=(1040, 512), dtype=int32, numpy=
array([[1, 1, 1, ..., 0, 0, 0],
       [1, 1, 1, ..., 0, 0, 0],
       [1, 1, 1, ..., 0, 0, 0],
       ...,
       [1, 1, 1, ..., 0, 0, 0],
       [1, 1, 1, ..., 0, 0, 0],
       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)&gt;}
</code></pre>
<p>And like this:-</p>
<pre><code>10     2
147    1
342    1
999    3
811    3
Name: sentiment, dtype: int64
</code></pre>
<p>I have studied the forums and made the most obvious checks:-</p>
<p>Here I check if there are any NaN in the data:-</p>
<pre><code>print(&quot;Any NaN in y_train? &quot;,np.isnan(np.array(y_train)).any())

print(&quot;Any NaN in y_val? &quot;,np.isnan(np.array(y_val)).any())
</code></pre>
<p>Which gives:-</p>
<pre><code>Any NaN in y_train?  False
Any NaN in y_val?  False
</code></pre>
<p>I also tried:-</p>
<pre><code>print(&quot;Any NaN in encoded_train['input_ids']? &quot;,np.isnan(np.array(encoded_train['input_ids'])).any())
print(&quot;Any NaN 'encoded_train['attention_mask']'? &quot;,np.isnan(np.array(encoded_train['attention_mask'])).any())
</code></pre>
<p>but only got:-</p>
<pre><code>Any NaN in encoded_train['input_ids']?  False
Any NaN 'encoded_train['attention_mask']'?  False
</code></pre>
<p>I am struggling to know where to go next with this code.</p>
<p>The full error trace looks like this, you can see the accuracy and loss on each epoch and this model is obviously not training at all:-</p>
<pre><code>Some layers from the model checkpoint at distilbert-base-cased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_layer_norm', 'vocab_projector', 'activation_13', 'vocab_transform']
- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['pre_classifier', 'classifier', 'dropout_59']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Epoch 1/3
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
130/130 [==============================] - ETA: 0s - loss: nan - accuracy: 0.0019WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
130/130 [==============================] - 63s 452ms/step - loss: nan - accuracy: 0.0019 - val_loss: nan - val_accuracy: 0.0000e+00
Epoch 2/3
130/130 [==============================] - 57s 438ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00
Epoch 3/3
130/130 [==============================] - 57s 441ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00
&lt;tensorflow.python.keras.callbacks.History at 0x7f304f714fd0&gt;
</code></pre>
<p>I would be happy to post more details if anyone is able to tell me what it would be useful to see.</p>
","nlp"
"99755","An issue for sub-word tokenization preprocessing transformer","2021-08-05 22:47:38","99765","0","121","<python><nlp><bert><transformer><python-3.x>","<p>I'm stacked with executing <strong>the sub-word tokenization preprocessing</strong> to use transformer.</p>
<p>According to the tutorial on <a href=""https://blog.codecentric.de/en/2020/12/ner-with-little-data-transformers-to-the-rescue/"" rel=""nofollow noreferrer"">the article</a>, I have executed the sample code.</p>
<p>However, one function was not defined properly and no hint to fix it on the article.</p>
<p>If you have any ideas to fix the code, could you help me?</p>
<h2>Error</h2>
<pre><code>     22  return X, y
     23 
---&gt; 24 X_train, y_train = preprocess(train_samples)
     25 X_val, y_val = preprocess(val_samples)

     12 def preprocess(samples):
     13  tag_index = {tag: i for i, tag in enumerate(schema)}
---&gt; 14  tokenized_samples = list(tqdm(map(tokenize_sample, samples)))
     15  max_len = max(map(len, tokenized_samples))
     16  X = np.zeros((len(samples), max_len), dtype=np.int32)

TypeError: 'module' object is not callable
</code></pre>
<h2>Code</h2>
<p>This code is from <a href=""https://blog.codecentric.de/en/2020/12/ner-with-little-data-transformers-to-the-rescue/"" rel=""nofollow noreferrer"">the article</a> to build a model for named entity recognition using transformer.</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import tqdm
 
def tokenize_sample(sample):
  seq = [
         (subtoken, tag)
         for token, tag in sample
         for subtoken in tokenizer(token)['input_ids'][1:-1]
         ]
  return [(3, 'O')] + seq + [(4, 'O')]

def preprocess(samples):
  tag_index = {tag: i for i, tag in enumerate(schema)}
  tokenized_samples = list(tqdm(map(tokenize_sample, samples)))
  max_len = max(map(len, tokenized_samples))
  X = np.zeros((len(samples), max_len), dtype=np.int32)
  y = np.zeros((len(samples), max_len), dtype=np.int32)
  for i, sentence in enumerate(tokenized_samples):
    for j, (subtoken_id, tag) in enumerate(sentence):
      X[i, j] = subtoken_id
      y[i,j] = tag_index[tag]
  return X, y

X_train, y_train = preprocess(train_samples)
X_val, y_val = preprocess(val_samples)
</code></pre>
<h2>What I tried</h2>
<p>I checked that the function, tokenize_sample is executable with the below code.</p>
<p>However, I'm not sure how to insert it to the original code.</p>
<pre><code>for sample in samples:
  print(tokenize_sample(sample))
</code></pre>
","nlp"
"99754","Will repeatedly fine-tuning on new data cause overfitting?","2021-08-05 20:41:03","","1","725","<machine-learning><nlp><overfitting>","<p>I have a binary classification model which I have trained on a training set. On the validation set its accuracy is ~85%. I set up early stopping which ended training when validation loss increased. Let's call this final model <code>modelA</code>.</p>
<p>Because of the nature of the task I can generate as much training data as I want. I have a huge dataset (say, reddit comments) and am generating positive and negative examples in a way that there are an astronomical number of combinations. So I'm not worried that the model is &quot;memorizing&quot; my training data.</p>
<p>So I generated another dataset and fine-tuned <code>modelA</code> until the early stopping trigger was fired. The validation accuracy of this model, <code>modelB</code>, was ~87%.</p>
<p>I repeated the process once more, getting a final validation accuracy of ~89% for <code>modelC</code>.</p>
<p>My question is, will this cause overfitting? Is this iterative fine-tuning a common industry practice?</p>
","nlp"
"99744","Question answering bot: EM>F1, does it make sense?","2021-08-05 15:25:16","","0","1290","<nlp><f1score><huggingface><question-answering>","<p>I am fine-tuning a Question Answering bot starting from a pre-trained model from HuggingFace repo.</p>
<p>The dataset I am using for the fine-tuning has a lot of empty answers. So, after the fine tuning, when I'm evaluating the dataset by using the model just created, I find that the EM score is (much) higher than the F1 score. (I know that I must not use the same dataset for training and evaluation, it was just a quick test to see that everything is running)</p>
<p>I assume that this happens because every question with no real answer is a match when the model cannot predict an answer, but as a non-expert of NLP I wonder does this makes sense? is it theoretically possible or am I missing something big?</p>
","nlp"
"99741","Sub-word tokenization preprocessing to use transformer","2021-08-05 14:44:31","99767","0","90","<python><nlp><numpy><transformer><python-3.x>","<p>I'm stacked with executing <strong>the sub-word tokenization preprocessing</strong> to use transformer.</p>
<p>According to the tutorial on <a href=""https://blog.codecentric.de/en/2020/12/ner-with-little-data-transformers-to-the-rescue/"" rel=""nofollow noreferrer"">the article</a>, I have executed the sample code.</p>
<p>However, one function was not defined properly and no hint to fix it on the article.</p>
<p>If you have any ideas to fix the code, could you help me?</p>
<h2>Error</h2>
<pre><code>     22  return X, y
     23 
---&gt; 24 X_train, y_train = preprocess(train_samples)
     25 X_val, y_val = preprocess(val_samples)

     12 def preprocess(samples):
     13  tag_index = {tag: i for i, tag in enumerate(schema)}
---&gt; 14  tokenized_samples = list(tqdm(map(tokenize_sample, samples)))
     15  max_len = max(map(len, tokenized_samples))
     16  X = np.zeros((len(samples), max_len), dtype=np.int32)

TypeError: 'module' object is not callable
</code></pre>
<h2>Code</h2>
<p>This code is from <a href=""https://blog.codecentric.de/en/2020/12/ner-with-little-data-transformers-to-the-rescue/"" rel=""nofollow noreferrer"">the article</a> to build a model for named entity recognition using transformer.</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import tqdm
 
def tokenize_sample(sample):
  seq = [
         (subtoken, tag)
         for token, tag in sample
         for subtoken in tokenizer(token)['input_ids'][1:-1]
         ]
  return [(3, 'O')] + seq + [(4, 'O')]

def preprocess(samples):
  tag_index = {tag: i for i, tag in enumerate(schema)}
  tokenized_samples = list(tqdm(map(tokenize_sample, samples)))
  max_len = max(map(len, tokenized_samples))
  X = np.zeros((len(samples), max_len), dtype=np.int32)
  y = np.zeros((len(samples), max_len), dtype=np.int32)
  for i, sentence in enumerate(tokenized_samples):
    for j, (subtoken_id, tag) in enumerate(sentence):
      X[i, j] = subtoken_id
      y[i,j] = tag_index[tag]
  return X, y

X_train, y_train = preprocess(train_samples)
X_val, y_val = preprocess(val_samples)
</code></pre>
<h2>What I tried</h2>
<p>I checked that the function, tokenize_sample is executable with the below code.</p>
<p>However, I'm not sure how to insert it to the original code.</p>
<pre><code>for sample in samples:
  print(tokenize_sample(sample))
</code></pre>
","nlp"
"99695","How is the connection between Text Mining, NLP and Tasks like Tokenization, Lemmatization, Stop-word Removal etc.?","2021-08-04 15:41:18","","1","38","<nlp><text-mining><tokenization>","<p>I am new to the whole world around Big Data and Text Mining.</p>
<p>It took me a while to understand all the connections and to be able to classify the buzzwords.</p>
<p>But there's one thing I still don't understand. The connection between NLP, text mining and tasks like tokenization, lemmatization, stop-word removal etc..</p>
<p>I refer to these two papers for example:</p>
<ul>
<li><a href=""https://www.elderresearch.com/wp-content/uploads/2020/10/Whitepaper_The_Seven_Practice_Areas_of_Text_Analytics_Chapter_2_Excerpt.pdf"" rel=""nofollow noreferrer"">https://www.elderresearch.com/wp-content/uploads/2020/10/Whitepaper_The_Seven_Practice_Areas_of_Text_Analytics_Chapter_2_Excerpt.pdf</a></li>
<li><a href=""https://www.researchgate.net/publication/311394659_Text_Mining_Techniques_Applications_and_Issues"" rel=""nofollow noreferrer"">https://www.researchgate.net/publication/311394659_Text_Mining_Techniques_Applications_and_Issues</a></li>
</ul>
<p>how do I connect this?</p>
<p>Option 1:</p>
<ul>
<li>Tasks like tokenization, lemmatization etc. are tasks of NLP</li>
<li>and NLP is an application field of text mining</li>
</ul>
<p>Option 2:</p>
<ul>
<li>Tasks like tokenization, lemmatization etc. are tasks of Text Mining</li>
<li>which find their usage in NLP?</li>
</ul>
<p>Can someone explain this to me?</p>
","nlp"
"99688","Error to load a pre-trained BERT model","2021-08-04 13:39:27","99736","0","566","<python><nlp><bert><transformer>","<h2>Background</h2>
<p>I'm reading <a href=""https://blog.codecentric.de/en/2020/12/ner-with-little-data-transformers-to-the-rescue/"" rel=""nofollow noreferrer"">this article</a> about a natural language task, named entity recognition and trying to load a pre-trained BERT model on Google colaboratory.</p>
<p><strong>How can I fix an error to load a pre-trained BERT model?</strong></p>
<h2>Code</h2>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoConfig, TFAutoModelForTokenClassification
MODEL_NAME = 'bert-base-german-cased' 
config = AutoConfig.from_pretrained(MODEL_NAME, num_labels=len(schema))
model = TFAutoModelForTokenClassification.from_pretrained(MODEL_NAME, config=config)
model.summary()
</code></pre>
<h2>Error</h2>
<p>I can understand that schema is not defined before the line, but I cannot find a clew on the article to fix it.</p>
<pre><code>      1 from transformers import AutoConfig, TFAutoModelForTokenClassification
      2 MODEL_NAME = 'bert-base-german-cased'
----&gt; 3 config = AutoConfig.from_pretrained(MODEL_NAME, num_labels=len(schema))
      4 model = TFAutoModelForTokenClassification.from_pretrained(MODEL_NAME, config=config)
      5 model.summary()

NameError: name 'schema' is not defined
</code></pre>
<h2>What I tried</h2>
<p>I checked <a href=""https://blog.codecentric.de/en/2020/11/take-control-of-named-entity-recognition-with-you-own-keras-model/"" rel=""nofollow noreferrer"">previous blogpost</a> following the advice from a comment, and found one description.</p>
<p>However, I'm not sure where to insert it to the original code.</p>
<pre><code>For simplicity, we’ll truncate the sentences to a maximum length and pad shorter input sequences. But first, let us determine the set of all tags in the data and add an extra tag for the padding:

#code
schema = ['_'] + sorted({tag for sentence in samples for _, tag in sentence})
</code></pre>
<p>Is it correct understanding?</p>
<pre><code>def load_data(filename: str):
   with open(filename, 'r') as file:
     lines = [line[:-1].split() for line in file]
     samples, start = [], 0
     for end, parts in enumerate(lines):
       if not parts:
         sample = [(token, tag.split('-')[-1]) for token, tag in lines[start:end]]
         samples.append(sample)
         start = end + 1
     if start &lt; end:
       samples.append(lines[start:end])
     
     return samples

samples = load_data('data/01_raw/bag.conll')
train_samples = load_data('data/01_raw/bag.conll')
val_samples = load_data('data/01_raw/bgh.conll')
all_samples = train_samples + val_samples

schema = ['_'] + sorted({tag for sentence in samples for _, tag in sentence})
</code></pre>
<p>I checked the output.</p>
<pre><code>print(schema)
#result
['_', 'AN', 'EUN', 'GRT', 'GS', 'INN', 'LD', 'LDS', 'LIT', 'MRK', 'O', 'ORG', 'PER', 'RR', 'RS', 'ST', 'STR', 'UN', 'VO', 'VS', 'VT']
</code></pre>
","nlp"
"99618","Trying to compress text with NLP","2021-08-02 17:32:17","","1","446","<machine-learning><nlp><regression><nltk><stanford-nlp>","<p>For a university project, I need to send text in Spanish via SMS. As these have a cost, I am trying to compress this text in an inefficient way. This consists of first generating a permutation of codes formed by two characters of many alphabets (fines, Cyrillic, etc.) to which I assign a word that has more than two characters (to say that it is being compressed). Then I take each word in a sentence and assign it its associated code. In this way, in tests, I get a compression rate of at least 40% (passing 60% in the best case), at the cost of storing approximately 90,000 base words of the Spanish dictionary with their respective code, which although it weighs less than 10MB, can be done better.</p>
<p>My second attempt has been to look up the most common sub-sentences in Spanish text and assign a code to them. For this I look at the dependencies of the words in the text with <strong>stanfordnlp</strong> and the formed sentences I assign a code to them.  This way I manage to increase the compression rate a bit, but I still think I could do better.</p>
<p>Lately, I've been trying other NLP techniques. My approach is to take a sentence and apply <strong>SnowballStemmer</strong> from <strong>nltk.stem.snowball</strong> (or lemmatizer from <strong>stanfordnlp</strong> ).  This is with the idea that by getting the base of the words, I will be able to encode many words with a single code and I will have to store fewer codes and text, being able to add typical names for example, and thus achieve even more compression of the text.  However, when I try to go back from a compound sentence with the stemmer output to the original (or similar) my problem arises because I have not found information on how to do it so that the coherence of the output is acceptable. For example, I have the following:</p>
<pre><code>sentence = &quot;Hola como estas? queria preguntarte si sabes algo acerca de nlp&quot;

Applying snowballStemmer:
Hola ----&gt; hol
como ----&gt; com
estas? ----&gt; estas?
queria ----&gt; queri
preguntarte ----&gt; preguntart
si ----&gt; si
sabes ----&gt; sab
algo ----&gt; algo
acerca ----&gt; acerc
de ----&gt; de
nlp ----&gt; nlp

To a sentence:
outputStemmer = 'hol com estas? queri preguntart si sab algo acerc de nlp'
</code></pre>
<p>I have tried to take this output somehow to regression or least-squares problem where the idea would be to give weight to the prepositions (and other structures that give semantic coherence to a sentence) and then to suppose that there will necessarily be one of these between two words of the output and to go varying in such a way that the error (that I have not yet thought how to measure) is minimized and thus it approaches more to a sentence with some sense. The problem of this idea is that there are many details of the algorithm that I still need to think more about and that with the stemmer I have only the base of the word but not the variations that I can reach from this base (if I stored them maybe I would increase my compression percentage but also the text to store).</p>
<p>I would be very grateful if you could help me and guide me because maybe the problem is much easier to solve with some NLP techniques (which I don't know much) and ML.</p>
","nlp"
"99598","How can I convert text data to CoNLL format?","2021-08-02 10:05:21","99599","1","2625","<machine-learning><deep-learning><nlp><dataset>","<p>This is <a href=""https://stackoverflow.com/questions/68607939/how-to-convert-text-data-to-conll-format?noredirect=1#comment121250073_68607939"">the same question that I posted on stackoverflow</a>, but I wondered stackexchange would be appropriate for this question.</p>
<p>I would like to convert text data to CoNLL format.</p>
<p>words.txt</p>
<pre><code>I was born in 1981.
From 12 to 24.
</code></pre>
<p>tags.txt</p>
<pre><code>O O O O B-DateTime O
O B-DateTime I-DateTime B-DateTime O
</code></pre>
<p>CoNLL（.conll file)</p>
<pre><code>I    O
was  O
born O
in   O
1981 B-DateTime
.    O
</code></pre>
<p>However, I only found <a href=""https://pypi.org/project/conllu/"" rel=""nofollow noreferrer"">a library for CoNLL-U format(conllu)</a> and <a href=""https://pypi.org/project/pyconll/2.3.3/#description"" rel=""nofollow noreferrer"">a library looked like for CoNLL（pyconll）</a> but no sample code, so I have no idea to apply for it to text-CoNLL conversion.</p>
<p>I'm stacked with how to convert the data to CoNLL and how to write Python3 script to do it.</p>
","nlp"
"99545","How to impute missing text data?","2021-07-31 07:37:13","99548","2","3272","<machine-learning><nlp><data-cleaning><missing-data><text>","<p>Lets say I have a dataframe consisting of two text columns. By text, I mean the values in those columns are either sentences/paragraphs. In such a case, how do I handle missing 'NaN' values?</p>
<p>If it were a numeric data, I would use frequent/mean/median/knn imputation. But, what to do about text data? Any ideas?</p>
","nlp"
"99447","Why do we fine-tune language models and not just include the data in the pre-training datasets?","2021-07-29 02:16:32","99455","0","624","<machine-learning><nlp>","<p>One question about the pre-training &amp; fine-tuning process for language models: why is it better to fine-tune using a small dataset rather than including the fine-tuning dataset into the pre-training dataset?</p>
<p>Or am I misunderstanding and normally the fine-tuning dataset is already included in the pre-training one, and we only change the learning parameters to better fit the data properties?</p>
<p>Any paper reference is very welcome! Thank you.</p>
","nlp"
"98324","Handle with very short and very long sequences with Neural Network","2021-07-26 14:04:12","","1","236","<nlp><lstm><multiclass-classification><embeddings><sequence>","<p>I am working on multi-class problem with sequences. My dataset is composed of sequences of data with different length.</p>
<p>E.g. 1500 labeled samples: 500 datapoint belongs to class A, 500 class B and 500 class C.
For A and B the sequence length is 400 and 1000 respectively, and for class C the sequence length is 100.</p>
<p>In order to train the model I have applied post-padding on the sequences so that all the sequences have the same length. The resulting dataset has this shape (1500,1000).</p>
<p>I have tried first EMBEDDING+LSTM (mask_zero=True) to map and classify the sequences but even if the model achieve very high accuracy, evaluate the model with random/fake data it will classify based on the sequences' length: suggesting that the model is learning on the lengths instead of values.</p>
<p>The main problem is that the model is much more learning on 0s even if we use &quot;mask_zero&quot; into the embedding layer. My question is:</p>
<p>Does someone can suggest an approach to <strong>deal with very long sequences</strong>? Considering that we have very short and very long sequences to predict?</p>
<p>I am exploring another different approach:</p>
<ol>
<li>Train an Autoencoder (ANN or 1DCNN) to reduce the sequence length. Use the encoder and train again the Embedding layer + LSTM layer.</li>
</ol>
<p>Thanks.</p>
","nlp"
"98311","Jargon extraction in a text","2021-07-26 08:51:26","","0","84","<python><nlp><corpus>","<p>I have a big text corpus (documentation from a company) and I want to extract the terms that are specific to that area/business. I can do that using TF or TF-IDF and guide myself by the frequency of the words, which isn't always reliable.</p>
<p>I want to also do that for single, shorter sentences, but I think this is already more difficult. I was also thinking of using Wikipedia articles to train a model and then apply it to my documentation texts.</p>
<p>Is there any way of identifying words that are related to a specific field?</p>
","nlp"
"98308","How to classify objects from a description in natural language","2021-07-26 08:08:12","98310","2","812","<machine-learning><classification><nlp>","<p>My objective is to classify objects that all belong to a certain category, based on a textual description of these objects by humans. My problem is not specific to a certain category of objects, but for sake of clarity I am going to give examples as if the objects I wanted to classify were movies.</p>
<p>To be precise:</p>
<ul>
<li>the description contains both a judgement of the object, and a more objective description of the various parts of the object. For example: “<em>This movie has great lines, and the scenario is well-though. It counterbalances the poor actor performance. Still, overall I think it's a very good movie</em>”. This both contains information about different aspects of the movie, and provides a subjective review.</li>
<li>what I want is:
<ol>
<li>a score for each object (like a movie rating), based on how appreciated it is;</li>
<li>for a given object, &quot;similar&quot; objects (ie. if you liked this movie, you might also enjoy these), based on similar &quot;features&quot; each object has. For instance, a movie which was also well-written might be considered &quot;similar&quot; to the former example.</li>
</ol>
</li>
<li>I also have access to a pre-existing classification of these objects. For instance, a movie might be labeled &quot;action/thriller&quot;. This classification is too broad for my purposes (ie. not all &quot;action/thriller&quot; movies are similar), but it might be a good start.</li>
</ul>
<p>I have though that to solve my problem, I could use sentiment analysis to give each object a score, and that natural language processing coupled with a feature space could do the trick for classifying objects.</p>
<p>The point is that I am unsure on how to proceed, because I am new to machine learning, natural language processing, and data sciences in general. I have nonetheless a CS and mathematical background.</p>
<p><strong>Could you provide some insight on where to start?</strong> Are there libraries that already provide this kind of features?</p>
<p><em>This is a repost of <a href=""https://stackoverflow.com/questions/68521152/how-to-classify-objects-from-a-description-in-natural-language"">this question</a>, since it was not focused enough, and this forum seems more appropriate. It has been rewritten.</em></p>
","nlp"
"98307","NLP behind Google Assistant, Amazon Echo and Apple Siri","2021-07-26 08:04:06","","0","147","<python><nlp><chatbot>","<p>How do these assisant based models analyse text and convert them to commands, I mean how do they understand the intent, property and value. I just want to know what are the models used and also I am trying to make a assistant bot, does anyone know a better model that can identify the command from the text given?</p>
","nlp"
"98289","How do you research and list up thinkable encoders and decoders when you build NLP models?","2021-07-25 15:50:27","98321","0","41","<machine-learning><deep-learning><nlp>","<p>I'm a beginner in NLP and deep learning fields, and have stacked with the phase how research and list up available and substitutional encoders and decoders.</p>
<p>For example, I read a thesis that Bidirectional <code>LSTM</code> and <code>CRF</code> were selected for named-entity recognition, like below.</p>
<p><a href=""https://www.programmersought.com/article/7825966810/"" rel=""nofollow noreferrer"">Named entity recognition with bidirectional lstm+CRF (with tensorflow code) - NER</a></p>
<p>However, if my dataset and these encoders and decoders didn't show high performance, I need to try other encoders and decoders.</p>
<p>My question is:</p>
<p><em>When you have the specific NLP task, like <code>NER</code>, and dataset to train,
<strong>how do you research applicable encoders and decoders?</strong></em></p>
","nlp"
"98265","Do I need to read an entire database for a recommendation system?","2021-07-24 18:18:40","","0","35","<nlp><recommender-system><databases><cloud>","<p>Let's say I have a database with approx 100000 rows. I want to build a content-based recommendation system. Do I really need to read the entire database to calculate similarity? That would be very expensive to do it hosted on AWS, Azure, etc. Additionally, my data is always changing (new data being added, old removed), so I can't just use a constant file. Is there a more cost-effective way?</p>
","nlp"
"98217","Interpreting the RGB or HEX value from a description of the color using NLP","2021-07-23 08:02:05","","2","458","<machine-learning><nlp><3d-reconstruction>","<p>We do have models that predict the basic color from its description, by basic color I mean red, blue, black etc. But I would like to develop a model that can spit out the RGB or HEX colors by a description of it, an example being, &quot;yellow that is glossy and sorta dark&quot; should give the respective value for the same. Another example would be, &quot;Clear green plastic&quot;. This is relative to 3d modelling where I input this text and change the material of the object on the screen.</p>
","nlp"
"98164","Automating 3D Modeling using NLP","2021-07-21 21:07:59","","1","104","<nlp><3d-reconstruction><chatbot>","<p>I would like to know if there is any way we can automate 3D modeling processes. Like if I give the model a text input such as &quot;create a sphere and give it a red color&quot; and the we need to get the model. To be precise, I would like to create a bot that can perform actions in a software such as blender, like I tell the bot what I would like to do and then it does it. Any idea how can I achieve this?</p>
","nlp"
"98149","Does spaCy support multiple GPUs?","2021-07-21 10:56:18","98195","4","1563","<python><nlp><gensim><spacy><hpc>","<p>I was wondering if spaCy supports multi-GPU via <a href=""https://mpi4py.readthedocs.io/en/stable/tutorial.html#running-python-scripts-with-mpi"" rel=""nofollow noreferrer"">mpi4py</a>?</p>
<p>I am currently using spaCy's <code>nlp.pipe</code> for Named Entity Recognition on a high-performance-computing cluster that supports the MPI protocol and has many GPUs. It says <a href=""https://github.com/explosion/spaCy/issues/3394"" rel=""nofollow noreferrer"">here</a> that I would need to specify the GPU to use with cupy, but with PyMPI, I am not sure if the following will work (should I import spacy after calling cupy device?):</p>
<pre><code>
from mpi4py import MPI
import cupy

comm = MPI.COMM_WORLD
rank = comm.Get_rank()

if rank == 0:
    data = [&quot;His friend Nicolas J. Smith is here with Bart Simpon and Fred.&quot;*100]
else:
    data = None

unit = comm.scatter(data, root=0)

with cupy.cuda.Device(rank):
    import spacy
    from thinc.api import set_gpu_allocator, require_gpu
    set_gpu_allocator(&quot;pytorch&quot;)
    require_gpu(rank)
    nlp = spacy.load('en_core_web_lg')
    nlp.add_pipe(&quot;merge_entities&quot;)
    tmp_list = []
    for doc in nlp.pipe(unit):
        res = &quot; &quot;.join([t.text if not t.ent_type_ else t.ent_type_ for t in doc])
        tmp_list.append(res)

result = comm.gather(tmp_list, root=0)

if comm.rank == 0:
    print (result)
else:
    result = None

</code></pre>
<p>Or if I have 4 GPUs on the same machine and I do not want to use MPI, can I do the following:</p>
<pre><code>from joblib import Parallel, delayed
import cupy

rank = 0

def chunker(iterable, total_length, chunksize):
    return (iterable[pos: pos + chunksize] for pos in range(0, total_length, chunksize))

def flatten(list_of_lists):
    &quot;Flatten a list of lists to a combined list&quot;
    return [item for sublist in list_of_lists for item in sublist]

def process_chunk(texts):
    with cupy.cuda.Device(rank):
        import spacy
        from thinc.api import set_gpu_allocator, require_gpu
        set_gpu_allocator(&quot;pytorch&quot;)
        require_gpu(rank)
        preproc_pipe = []
        for doc in nlp.pipe(texts, batch_size=20):
            preproc_pipe.append(lemmatize_pipe(doc))
        rank+=1
        return preproc_pipe

def preprocess_parallel(texts, chunksize=100):
    executor = Parallel(n_jobs=4, backend='multiprocessing', prefer=&quot;processes&quot;)
    do = delayed(process_chunk)
    tasks = (do(chunk) for chunk in chunker(texts, len(texts), chunksize=chunksize))
    result = executor(tasks)
    return flatten(result)

preprocess_parallel(texts = [&quot;His friend Nicolas J. Smith is here with Bart Simpon and Fred.&quot;*100], chunksize=1000)
<span class=""math-container"">```</span>
</code></pre>
","nlp"
"98140","Using NLP to recognize the timeliness of text content","2021-07-21 09:51:21","","0","43","<machine-learning><nlp>","<p>NLP models can classify text content as positive or negative. Except that, we also need to know the timeliness of such text content. That is whether the text is describing something that has already happened or predicting something that might happen in the future. For example:</p>
<p>Text 1: Stock A has risen more than 7.5%. (positive, but happened already)</p>
<p>Text 2: Stock B might be exposed to downside risk due to its poor financial situation (negative prediction).</p>
<p>It is obvious that Text 2 contains more useful information because it is a prediction of the future while Text 1 is just a description of the past. However, current NLP models, as I understand, cannot recognize such timeliness inside the text content.</p>
<p>Dose anyone have any ideas about this problem? Any introduction of academic journals and surveys would be very useful.</p>
","nlp"
"98099","tf-idf for sentence level features","2021-07-20 07:27:11","98109","1","1467","<machine-learning><deep-learning><nlp><tfidf>","<p>Many papers mention comparing <strong>sentences</strong> using the tf-idf metric, e.g. <a href=""https://aclanthology.org/P19-1628.pdf"" rel=""nofollow noreferrer"">Paper</a>.</p>
<p>They state:</p>
<blockquote>
<p>The first one is based on tf-idf where the value of the
the corresponding dimension in the vector representation is the number of occurrences of the word in
the sentence times the idf (inverse document frequency) of the word.</p>
</blockquote>
<p>While I am familiar with <code>tf-idf</code> weights per token, it is a bit vague for me how to extract a similarity measure between two sentences given the <code>tf-idf</code> weights of their individual tokens.</p>
<p>If the reference to the paper itself was not clear, the questions is:
Given a document containing several sentences,</p>
<p>Is there a known measure of similarity between sentences in the document, based on the <code>tf-idf</code> score of the tokens inside each sentence?</p>
","nlp"
"98078","An efficient way to resegment tokenized text into phrases","2021-07-19 15:56:51","","0","50","<nlp><preprocessing><tokenization>","<p>I have text <b>tokenized on the word level</b> and few lists of phrases stored as tuples.</p>
<p>What would be the most efficient way to resegment (and store) the text into <b>phrases?</b></p>
<p>For example, a sentence like:</p>
<p>&quot;He didn't work hard.&quot;</p>
<p>Which is now tokenized as:</p>
<p>&quot;He&quot; &quot;did&quot; &quot;n't&quot; &quot;work&quot; &quot;hard&quot; &quot;.&quot;</p>
<p>Would become:</p>
<p>&quot;He&quot; &quot;did&quot; &quot;n't&quot; &quot;work hard&quot; &quot;.&quot;</p>
<p>Assuming that &quot;work hard&quot; would be one of the phrases I have.</p>
","nlp"
"98045","Document Clustering for given specific clusters in python","2021-07-19 02:32:49","","0","41","<python><nlp><clustering><data-analysis>","<p>How can we classify text in to given specific number of clusters in python? I'm aware that the number of clusters can be specified using some mechanisms like k-means but I need to classify the given to specific categories.</p>
<p>Ex: I'm having a corpus containing newspaper articles and I want to cluster them pertaining to sports, foreign and local.</p>
<p>Is there anyway to achieve this in python? I've googled but could not find a good match.</p>
<p>Any help in this regard is highly appreciated.</p>
<p>Thanks in advance!!</p>
","nlp"
"97957","Question regarding training data in word2vec - skip-gram","2021-07-16 12:04:27","","0","247","<neural-network><nlp><word-embeddings><word2vec>","<p>I have a very simple question regarding the training data in word2vec. In the skip-gram implementation, the training data (if I understand it correctly) is generated as pairs of words like it's shown in this image:</p>
<p><a href=""https://i.sstatic.net/fKkRF.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/fKkRF.png"" alt=""enter image description here"" /></a></p>
<p>This essentially is just pairs of one-hot vectors. My question is what happens to the results if, instead of splitting every window with one sample per pair of words, I train with the following data: <code>(word, {words in window})</code>? In other words, with a window like the previous one, I would be sending the vector <code>(1, 0, 0, 0, ... 0)</code> to <code>(0, 1, 1, 0, ... 0)/num_non_zero</code>?</p>
<p>(The <code>num_non_zero</code> division is a way to normalize the probabilities so that the softmax layer fits correctly)?</p>
<p>I know I can go and try myself, but I was just wondering if you can shed some light on what results I should expect. Mostly because this is a very obvious alternative to the original and I would be surprised if people haven't used this before.</p>
","nlp"
"97921","Low-dimensional path representation learning","2021-07-15 15:40:18","","1","30","<nlp><graphs><embeddings>","<p>I have a graph (ex: map) and multiple sequences of ids representing different paths.</p>
<ul>
<li>A vertex represents a region/area</li>
<li>An edge between 2 vertices : a crossing from a region to another</li>
<li>A graph path (sequences of crossings) : a trajectory</li>
</ul>
<p>Like the examples below:</p>
<pre><code>path1 = [15,1,2,3]
path2 = [1,2,9]
path3 = [15,3]
</code></pre>
<p>All the paths come from the same graph structure and they could have various high sizes (~50). Then I would like to get a low-dimensional vector (one for each path) in order to perform an Approximate Neighbors Search (it's a kind of search technique to find out the closest data points to another).</p>
<p>I have found some papers about graph representation learning but nothing relevant. Should I explore an NLP technique or a graph embeddings technique?</p>
","nlp"
"97915","WordNetLemmatizer not lemmatizing the word ""promotional"" even with POS given","2021-07-15 14:57:38","97922","0","176","<nlp><nltk>","<p>When I do <code>wnl.lemmatize('promotional','a')</code> or <code>wnl.lemmatize('promotional',wordnet.ADJ)</code>, I get merely <code>'promotional'</code> when it should return <code>promotion</code>. I supplied the correct POS, so why isn't it working? What can I do?</p>
","nlp"
"97882","GridSearchCV is Giving me ValueError: number of labels does not match number of samples","2021-07-15 00:53:29","97886","0","860","<python><nlp><scikit-learn>","<p>I'm trying to run a grid CV parameter search using sklearn.model_selection.GridSearchCV. I keep getting a ValueError that is really confusing me.
Below I've included the code for the pipeline I created, which includes a TfidfVectorizer and a RandomForestClassifier. I used train_test_split to separate the features and target, and tried to fit the grid search with the pipeline. Here are my results.</p>
<pre><code>from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction.text import TfidfVectorizer

features = ['id', 'description']
target = 'ratingCategory'
x_train, x_val, y_train, y_val = train_test_split(
    train[features],
    train[target],
    test_size=0.2,
    stratify=train[target],
    random_state=95
)

vect = TfidfVectorizer()
clf = RandomForestClassifier()

pipe = Pipeline([
    ('vect', vect), 
    ('clf', clf)]
)

parameters = {
    'vect__min_df': (0.01, 0.05),
    'clf__ccp_alpha': (0.1, 0.5)
}

grid_search = GridSearchCV(pipe, parameters, cv=5, n_jobs=4, verbose=1)
grid_search.fit(X=x_train, y=y_train)
</code></pre>
<p>Checking the shape of the matrices confirms that x_train and y_train have the same length (the number of rows in both = 3269).
So I'm confused as to why fitting the grid search gives me the following error:</p>
<pre><code>Fitting 5 folds for each of 4 candidates, totalling 20 fits
[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.
[Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed:    2.0s finished
--------------------
ValueErrorTraceback (most recent call last)
&lt;ipython-input-18-4e85850d6599&gt; in &lt;module&gt;
      29 grid_search = GridSearchCV(pipe, parameters, cv=5, n_jobs=4, verbose=1)
----&gt; 30 grid_search.fit(X=x_train, y=y_train)

........................................

ValueError: Number of labels=3269 does not match number of samples=2
</code></pre>
<p>What does it mean by number of labels and samples? There <em>should</em> be 3269 samples, since the shape of both X and y matrices is (3269, 2) and (3269, ), respectively.</p>
<p>Any help is super appreciated! Let me know if the full traceback would help, but it was extremely long so I didn't include it.</p>
","nlp"
"97867","How does the character convolution work in ELMo?","2021-07-14 14:59:29","97879","1","492","<nlp><convolutional-neural-network><allennlp>","<p>When I read the original ELMo paper (<a href=""https://arxiv.org/pdf/1802.05365.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1802.05365.pdf</a>), I'm stumped by the following line:</p>
<blockquote>
<p>The context insensitive type representation uses 2048 character n-gram convolutional filters
followed by two highway layers (Srivastava et al., 2015) and a linear projection down to a 512 representation.</p>
</blockquote>
<p>The Srivastava citation only seems to relate to the highway layer concept. So, what happens prior to the biLSTM layer(s) in ELMo? As I understand it, one-hot encoded vectors (so, 'raw text') are passed to a convolutional filter and a linear projection? How should I think of input and output dimensions? I get the feeling that perhaps there used to be a detailed explanation somewhere on allennlp.org (or perhaps their github repo), but it has perhaps been deemed outdated or unnecessary since?</p>
<p>I hope the question makes sense.</p>
","nlp"
"97843","Ensure trained word embeddings get high similarity with particular words","2021-07-14 07:13:33","","1","172","<machine-learning><nlp><word-embeddings>","<p>I am trying out my hand at training a Word2Vec model using gensim. I made a simple training file that basically had just one line repeated multiple times</p>
<pre><code>entertainment films Movies cinema
entertainment Movies 
entertainment films
entertainment cinema
</code></pre>
<p>The idea behind using a training file like the one above is to ensure that words like movies, etc come out to be most similar to entertainment.</p>
<pre><code>&gt;&gt;&gt; wv_model = gensim.models.Word2Vec(sents, size=300, min_count=1, 
    workers=8, window=1, sg=0)
</code></pre>
<p>But when I check the results I entertainment actually has a negative similarity score</p>
<pre><code>&gt;&gt;&gt; wv_model.most_similar(positive=['Movies'])
[('cinema', 0.14602532982826233), ('films', -0.022810805588960648), ('entertainment', -0.030070479959249496)]
</code></pre>
<p>The result I am trying to achieve is to ensure that the most similar word for <code>movies</code>, <code>films</code>, <code>cinema</code> comes out to be <code>entertainment</code></p>
","nlp"
"97833","How to match a corpus with a string of words using a TF-IDF matrix?","2021-07-13 22:45:27","","1","668","<nlp><text-classification><tfidf>","<p>I am trying to match strings of words with a website that has bulletpoints whose text is most similar to it. The way I thought of doing it is to get all of the documents from each bulletpoint into one corpus per website, that I would like to match a string of words with, discard stop words, and then lemmatize everything. Then, for each string of text, I create a TF-IDF sparse matrix, with each row the text from a single bulletpoint from a single website, so that the matrix contains all the text from the bulletpoints from all the websites, as well as a row for the string of words I want to match.</p>
<p>How should I then decide which row my string of words is most similar to? Should I get the cosine similarity of every row with my string of words row and just take whatever one has the highest cosine similarity (I will have a way of identifying the row with the website it was scrapped from)? Or is there an actual formalized way to go about this once I have my sparse matrix?</p>
","nlp"
"97744","How to choose and create natural language data for machine learning","2021-07-12 03:25:28","97765","1","74","<machine-learning><nlp><dataset>","<p>What the difference between these two data formats?</p>
<p>For example, for the Named Entity Recognition task, I learned that index and <a href=""https://lingpipe-blog.com/2009/10/14/coding-chunkers-as-taggers-io-bio-bmewo-and-bmewo/"" rel=""nofollow noreferrer"">BIO Encoding</a> are popular data formats to train.</p>
<p>Are they have different features for machine learning, and should I choose input data format following training models' requirements?</p>
<pre><code># index representation
    &quot;entities&quot;: [
        {
            &quot;name&quot;: &quot;John J. Smith &quot;,
            &quot;span&quot;: [4,8],
            &quot;type&quot;: &quot;PERSON&quot;
        }

# BIO Encoding
Tokens  IO  BIO BMEWO   BMEWO+
Yesterday   O   O   O   BOS_O
afternoon   0   O   O   O
,   0   O   O   O_PER
John    I_PER   B_PER   B_PER   B_PER
J   I_PER   I_PER   M_PER   M_PER
.   I_PER   I_PER   M_PER   M_PER
Smith   I_PER   I_PER   E_PER   E_PER
traveled    O   0   O   PER_O
to  O   O   O   O_LOC
Washington
I_LOC   B_LOC   W_LOC   W_LOC
.   O   O
</code></pre>
","nlp"
"97687","Hashing trick for dimensionality reduction","2021-07-10 07:41:14","","2","414","<nlp><dimensionality-reduction><apache-spark><tfidf><hashing-trick>","<p>I am building a model that uses TF-IDF NLP features in Spark Mllib. The <a href=""https://spark.apache.org/docs/latest/ml-features.html#tf-idf"" rel=""nofollow noreferrer"">TF-IDF HashingTF function in Mllib</a> uses the 'hashing trick' to efficiently allocate terms to features.</p>
<p>My question is: <strong>does the hashing trick work as an effective form of dimensionality reduction?</strong> Since I can choose the number of features generated by the IDF function, can I choose a relatively small number of features (say, only 512 or 1024 features) and be confident that the allocation will retain meaningful properties across the data? I am using bi-grams in my TF-IDF so the natural number of terms in the vocabulary will be significantly more than 1024.</p>
<p><a href=""https://alex.smola.org/papers/2009/Weinbergeretal09.pdf"" rel=""nofollow noreferrer"">Weinberger et al (2009)</a> seems to suggest that hashing is an effective form of dimensionality reduction. However, I'm interested if fellow practitioners find that it's a good option in the real world.</p>
<p><em>Some additional context</em>: I am finding that training is very slow and believe it's due to the large number of columns (65,536). I have tried to use <a href=""https://spark.apache.org/docs/latest/ml-features.html#chisqselector"" rel=""nofollow noreferrer"">ChiSqSelector</a> but that alone is taking hours to execute so doesn't improve things. The model I want to use can take a maximum of 4096 features, so I can't skip feature reduction.</p>
","nlp"
"97628","How do the linear + softmax layers give out word probabilities in Transformer network?","2021-07-08 17:30:20","97629","0","1202","<machine-learning><nlp><pytorch><transformer>","<p>I am trying to implement a transformer network from scratch in pytorch to understand it. I am using <a href=""https://jalammar.github.io/illustrated-transformer/"" rel=""nofollow noreferrer"">The illustrated transformer</a> for guidance. The part where I am stuck is about how do we go from the output of the final decoder layer to linear + softmax.</p>
<p>From what I have understood, if we have a batch of size B, max output seq length M, embedding dimension D, and vocab size V, then the output of the last decoder layer would be BxMxD which we have to turn into a vector of probabilities of size BxV so that we can apply softmax and get next predicted word. But how do we go from a variable size MxD matrix to a fixed-length V vector?</p>
<p><a href=""https://datascience.stackexchange.com/questions/74525/transformer-decoder-output-how-is-it-linear"">This post</a> says we apply the linear layer to all M vectors sequentially:</p>
<blockquote>
<p>That's the thing. It isn't flattened into a single vector. The linear
transformation is applied to all M vectors in the sequence
individually. These vectors have a fixed dimension, which is why it
works.</p>
</blockquote>
<p>But how do we coalesce those transformed vectors into just one single vector? Do we sum them up?</p>
","nlp"
"97617","BERT Optimization for Production","2021-07-08 13:18:05","97652","1","111","<nlp><bert><transformer><semantic-similarity>","<p>I'm  using BERT to transform text into 768 dim vector, It's multilingual :</p>
<pre><code>from sentence_transformers import SentenceTransformer
model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2') 
</code></pre>
<p>Now i want to put the model into production but the embedding time is too much and i want to reduce and optimize the model <strong>to reduce the embedding time</strong> What are the libraries that enable me to do this ?</p>
","nlp"
"97598","Number of features of the model must match the input. Model n_features is 740 and input n_features is 400","2021-07-08 07:01:05","97599","2","1183","<classification><nlp><scikit-learn><random-forest>","<p>i am getting this error predicting from random classifier,
could anybody point me to where i am going wrong in this?</p>
<p>(background information: yes, i am trying to do sentence classification with 2 labels)</p>
<pre><code>#Initializing BoW
cv = CountVectorizer()

#Test-Train Split
X_train,X_test,y_train,y_test = train_test_split(experiment_df['Sentence'],experiment_df['Label'])

#Transform
train = cv.fit_transform(X_train)
test = cv.fit_transform(X_test)


#Train Classifier
clf = RandomForestClassifier(max_depth=2, random_state=0)
clf.fit(train,y_train)

#Pred
y_pred = clf.predict(test)

</code></pre>
<p>Error:</p>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-31-a6f8e9da0bb0&gt; in &lt;module&gt;()
      1 clf = RandomForestClassifier(max_depth=2, random_state=0)
      2 clf.fit(train,y_train)
----&gt; 3 y_pred = clf.predict(val)

3 frames
/usr/local/lib/python3.7/dist-packages/sklearn/tree/_classes.py in _validate_X_predict(s

elf, X, check_input)
    389                              &quot;match the input. Model n_features is %s and &quot;
    390                              &quot;input n_features is %s &quot;
--&gt; 391                              % (self.n_features_, n_features))
    392 
    393         return X

ValueError: Number of features of the model must match the input. Model n_features is 740 and input n_features is 400 
<span class=""math-container"">```</span>
</code></pre>
","nlp"
"97545","How would you utilize NLP for this situation?","2021-07-06 20:15:37","","0","13","<machine-learning><nlp>","<p>I am working on a project that involves pulling data from a free response section of a survey and matching it to one or more pre-determined categories.</p>
<p>For example: someone who took the survey could input &quot;smoking?&quot; or &quot;history of smoking&quot; or &quot;history of tobacco use&quot; and the program would link their answer to the &quot;History of Smoking&quot; category.</p>
<p>There are many different categories and the users will input responses for multiple categories inside the same survey input box.</p>
<p>I am very new to machine learning and natural language processing and I was wondering what steps or algorithms would be used for a problem like this.  Any help would be great. Thanks!</p>
","nlp"
"97529","Document ranking on a web scraped dataset without any labelled data","2021-07-06 15:11:05","","1","139","<nlp><text-mining><bert><information-retrieval><similar-documents>","<p>I want to create a document ranking model which returns similar rows in the dataset for a sample query. The text in this corpus is standard english but without any labels (ie no query-related documents structure). Is it possible to use a pretrained model trained on a large corpus (like bert or word2vec) and use it directly on the scraped dataset without any evaluation and get decent results? If not this, is training a model on the MS macro dataset and applying it on this corpus worth exploring?</p>
","nlp"
"97515","One-Class Text Classification","2021-07-06 11:37:21","97534","1","182","<deep-learning><nlp><text-classification>","<p>So I have a specific use case where my colleagues have kept thousands of articles across the years deemed as &quot;Good&quot;, among hundreds of thousands of other articles deemed as bad and they didn't keep!</p>
<p>My objective is to train an NLP Deep Learning Model to detect which articles are good and which are bad. Since I don't have the &quot;Bad&quot; articles I cannot use Binary Classification.</p>
<p>So my questions are:
1- is One-Class Text Classification is suitable for this task?
1.1- if yes, please let me know how to do it in the context of NLP.
2- Are there other solutions or suggestions for this use case?</p>
<p>P.S.
I have found some research and code for similar use cases like Anomaly Detection and fraud detection, but the nature of this use case is different.
Because first I have textual documents and what I found are tabular data.
And second, is that I have thousands of documents that are labeled as &quot;Good&quot; among hundreds of thousands that are labeled &quot;Bad&quot; and were not kept in the database.
But in the case of Anomaly Detection and Fraud detection or other similar use cases, most of the data is labeled as &quot;Good&quot; therefore we're looking for exceptions.</p>
<p>I'm really looking forward to your answers, suggestion, and thoughts and I'm very open to discussions.
Thank You.</p>
","nlp"
"97504","Kmeans with Word2Vec model unexpected results","2021-07-06 02:14:06","97513","0","193","<machine-learning><python><nlp><clustering><word2vec>","<p>I'm trying to play around with unsupervised NLP using Word2Vec. So far, the data i used is very small, but that is because I am just testing to see how Kmeans will work.</p>
<p>The Kmeans was performed first (4 clusters) due to the small number of inputs, and the TSNE was used to visualise to 2D:</p>
<pre class=""lang-py prettyprint-override""><code>model = Word2Vec(sents,
                 min_count=5,
                 window=5,
                 alpha=0.03,
                 min_alpha=0.0007,
                 sg=1,
                 )

model_K = KMeansClusterer(4, distance=euclidean_distance, repeats=50) #cosine distance didnt change as much
assigned_clusters = model_K.cluster(model.wv.vectors, assign_clusters=True)

tsne = TSNE(n_components=2, random_state=0)
vectors = tsne.fit_transform(model.wv.vectors)
</code></pre>
<p><a href=""https://i.sstatic.net/iGHsV.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/iGHsV.png"" alt=""enter image description here"" /></a></p>
<p>As you can see the clustering kind of works, but there are some clusters way off. I'm wondering is that is because I performed the cluster before the reduction in dimensions. But from what I have read it's better to do Kmeans before if you can.</p>
<p>When I try with 6 clusters I get:</p>
<p><a href=""https://i.sstatic.net/WQNk3.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/WQNk3.png"" alt=""enter image description here"" /></a></p>
<p>Any reasons why the clustering isn't working as expected will be appreciated. Thanks.</p>
","nlp"
"97368","How does Word2Vec actually help with sentimental analysis?","2021-07-02 12:56:45","97375","0","136","<python><nlp><word2vec><sentiment-analysis>","<p>I'm trying read in a whole article, separate the article by sentences, and then words. Then I pass this into the Word2vec Model and the output comes out.</p>
<p>However, my goal is to find the positive or negative sentiment of the article. The input is unsupervised in that it does not have a label.</p>
<p>Do I need to perform some sentiment scoring on the article before inputting into the word2Vec. I don't understand how word2vec actually helps with sentimental analysis. All it tells me is that words are close together/ have same context, but not actually whether the words are positive or negative.</p>
<p>I've read articles claiming to &quot;use word2vec for sentimental analysis&quot;, but none actually do, so I'm not sure if I am misreading something here.</p>
<p>I'm wondering how I should go about this. Thanks.</p>
","nlp"
"97361","what can be done using NLP for a small sentence samples?","2021-07-02 08:53:47","97399","1","597","<machine-learning><deep-learning><classification><nlp><text-mining>","<p>I am new to NLP. I have few 100 textual sentences (100 rows in dataframe) with an average word length of 10 in a sentence. I would like to know what interesting insights (simple descriptive to advanced) can be derived using NLP techniques. I don't intend to predict anything but analyze and get some interesting insights.</p>
<p>I have thought of the below items that can be done using sample data that I have.</p>
<ol>
<li><p>Count the number of occurrences of each word in a sentence and finally find out the most frequently used (top) word and least used (bottom) word in the list of sentences that I have</p>
</li>
<li><p>Find the Entity in each sentence using NER. Which entity is discussed most in the sentences that I have?</p>
</li>
<li><p>Find which sentences are similar using textual similarity metrics.</p>
</li>
<li><p>I can identify the sentiment of the sentence</p>
</li>
<li><p>Can LDA be used to identify the topic of the sentence (which on average has 10 words) and my dataset itself has only 100 sentences?</p>
</li>
<li><p>What do you think is the use of creating syntactic/dependency trees? What can we infer through this? This might be useful for linguists, but can it help the layman end-users/business folks get some insight? Any simple explanation on this topic or directing me to resources would be helpful</p>
</li>
<li><p>I think we cannot summarize it because my sentences only contain 10 words on average</p>
</li>
</ol>
<p>Can you help me with q5, q6, and q7?</p>
<p>Is there anything else that you think can be done? What more do you think can be done using.</p>
","nlp"
"97350","Sequence learning from farm operations data","2021-07-02 06:45:22","","1","15","<machine-learning><python><deep-learning><nlp><sequence-to-sequence>","<p>I need to generalize a single sequence from N sequences entailing farming tasks/operations and ultimately plotting it on Gantt chart.</p>
<p>There are a total let's assume N sequences = n (total fields) * t (number of years)</p>
<p>Which ML/deep learning algorithm can learn from N sequences and create a single sequence that best represents overall data.</p>
<p>Each field has tasks/operations occurring in a sequence eg. A,B,C and respectively requiring t days (duration variable).</p>
<pre><code># Sequence for field 1 in year 2019
field1 = pd.DataFrame({&quot;task name&quot;: [A,B,C], &quot;start&quot;: [pd.Timestamp(&quot;2019-04-09 12:00:00&quot;),pd.Timestamp(&quot;2019-05-01 12:00:00&quot;), pd.Timestamp(&quot;2019-06-03 12:00:00&quot;)], &quot;end&quot;: [pd.Timestamp(&quot;2019-04-14 23:37:56 &quot;),pd.Timestamp(&quot;2019-05-06 00:06:25&quot;),pd.Timestamp(&quot;2019-06-04 00:08:13&quot;)], &quot;duration&quot;: [6 days, 6 days,2 days,]})
</code></pre>
<p>Similarly there are other N sequences for each n fields.</p>
<p><strong>Tricky part of the problem:</strong> 1. Data is not homogenous, since fields belongs to different farmers and differ in farm size. Minority of tasks are varying across fields</p>
<p>My final objective is to plot a Gantt chart of a single sequence that best represents/indicator of sequence of tasks and its respective <strong>duration</strong> for a crop used by a farmer.</p>
","nlp"
"97332","Traditional alternatives to Caputure Words Sequence information in NLP","2021-07-01 19:54:03","97338","1","25","<nlp><feature-engineering>","<p>What were the traditional/earlier methods in which NLP researchers captured the word sequence information through feature engineering?</p>
<p>I know the current methods which rely on deep learning models like roBERT and BERT and work well with capturing sequence information. I also know about embeddings like word2vec, but they fail to capture the sequence information.</p>
<p>For example, I would like a feature which can differentiate between
<code>&quot;cat ran after the dog.&quot;</code> and <code>&quot;dog ran after the cat.&quot;</code></p>
","nlp"
"97269","Extracting location from text - NOT sensetive to letters (Upper or Lower Case) or already known vocabulary words","2021-06-29 18:03:38","","2","131","<nlp><lstm><named-entity-recognition>","<p>I would like to extract location or contents related to location from raw text. I used the NLTK and spaCy packages already; none worked for me. For example, both would neglect 'canada' as a location because it is written in lower case format. Or, if I just include somewhere new in a text, both would fail to recognize it as a location.</p>
<p>Could anyone here recommend a solution (paper, blog, GitHub or anything) to solve this problem? To be more specific, I would like my algorithm to recognize &quot;sakfhajl&quot; and &quot;alksjf&quot; as locations in the example below:</p>
<p>&quot;I am currently at <strong>sakfhajl</strong> street&quot; or &quot;I wanted to spend more time in <strong>alksjf</strong> but ...&quot;</p>
<p>Yes, I know it is hard, but don't us humans do the same? We all recognize some names as locations given the context, although we might have never heard the name before.</p>
","nlp"
"97184","How to handle imbalanced NLP text data set e.g. some classes only have 2 records","2021-06-28 02:57:30","","0","165","<nlp><pytorch><class-imbalance><fastai>","<p>I am working on a dataset with around 2000 records.</p>
<p>Around 80% records have their the categorical labels.</p>
<p>There are around 200 categories, some categories got more than 20 records; whereas others only have TWO....</p>
<p>Considering this is a text dataset, so I cannot do the oversampling for minority categories with techniques like what I could do for images.</p>
<p>I am using Fast AI which is based on PyTorch.</p>
<p>So what can I do for it?</p>
","nlp"
"97148","How can I retrain existing spacy v3 NER model with additional data?","2021-06-26 22:13:47","","1","356","<machine-learning><python><nlp><spacy>","<p>I have trained Spacy v3 model on 1 million rows and want to train for additional 1 million.</p>
<pre><code>!python -m spacy train spacy_v3/config.cfg --output spacy_v3/output --paths.train spacy_v3/train.spacy --paths.dev spacy_v3/test.spacy --gpu-id 0
</code></pre>
<p>While training on additional 1 million or more data, I want to start my training from the previous best-model. Is there anyway to retrain existing ner model?</p>
","nlp"
"97131","NLP - How to detect sentence validity","2021-06-26 08:15:36","","3","201","<nlp>","<p>We have sentences like:</p>
<ul>
<li>We need to talk to you on this subject important issue immediately.</li>
<li>What year is this year?</li>
<li>Anticipate talk to you</li>
</ul>
<p>By reading these sentences, it's understandable that they are wrong.
What's the relevant field of studies in NLP that I need to look into, to understand how to detect these issues? (given a sentence like this -&gt; classify it as incorrect)</p>
","nlp"
"97098","NLP-Problem, language model BERT?","2021-06-25 13:57:19","","1","46","<machine-learning><deep-learning><nlp><bert><language-model>","<p>Right now I am in the process of deciding on my masters thesis topic. Right now I and my professor are thinking about the possibility of having a large dataset of product requirements given in a natural language. My task would be to develop a domain-specific language (DSL) for these requirements and afterwards to develop a neural network that gets the dataset containing these requirements as an input and transforms each requirement into a requirement in this DSL that I have to develop beforehand.</p>
<p>In theory a topic which I personally find very interesting and would love to learn more about, the only problem is, my current knowledge in this area is very limited so I am trying to read up on it right now.</p>
<p>An example for such a DSL would be presented in this Paper: <a href=""https://www.researchgate.net/publication/281372036_A_Textual_Domain_Specific_Language_for_Requirement_Modelling"" rel=""nofollow noreferrer"">A Textual Domain Specific Language for Requirement Modelling</a>
(It's not exactly the same but a good showcase of the DSL might be derived from it).</p>
<p>For the people that don't want to read the paper there is an example given like this:</p>
<p><code>Natural Language Req: &quot;The Lateral and Vertical 'ERC' and'WRD' labels shall be displayed in a green color&quot;</code></p>
<p>The neural network should then take this requirement as an input and generate the following requirements:</p>
<p>Req1: <strong>The</strong> Lateral_ERC_Label <strong>shall be displayed in</strong> green</p>
<p>Req2: <strong>The</strong> Lateral_WRD_Label <strong>shall be displayed in</strong> green</p>
<p>Req3: <strong>The</strong> Vertical_ERC_Label <strong>shall be displayed in</strong> green</p>
<p>Req4: <strong>The</strong> Vertical_WRD_Label <strong>shall be displayed in</strong> green</p>
<p>As we can see the DSL, in this case, has the keywords &quot;The&quot; and &quot;shall be displayed in&quot; (Bolded) that stay the same for each generated requirement, and in between are the specifications for the requirement.</p>
<p>Now the question is, how is this achievable? Or rather, is this achievable at all? My professor said I should look into BERT for this task but my research has led me to the conclusion that BERT is not really suitable for text generation.</p>
<p>On the other hand, I am not really sure if this above-mentioned problem can be classified as text generation. It looks like a combination problem of text extraction (extracting the necessary keywords) and text generation.</p>
<p>But I am not really sure if I am even looking and researching in the right direction right now. So I would really appreciate it if somebody would be able to help me out here and giving me a push in the right direction.</p>
","nlp"
"97093","Is there a distinction between saying Relationship Extraction and Relation Extraction in NLP","2021-06-25 12:44:00","","1","9","<nlp>","<p>A decade ago or so, I was doing a lot of Information Extraction (IE) with Stanford CoreNLP.  One of the tasks of the IE pipeline with that tool is Relation Extraction.  Relations are still very important, although we capture them with other techniques now.  There are some people out there that talk about Relationship Extraction which always confuses me. I generally talk about Relation Extraction when doing NLP.  Is there any distinction between Relation Extraction and Relationship Extraction?  Are they two distinct tasks, or do they generally refer to the same task?</p>
","nlp"
"97067","Using Subsequent Mask in Transformer Leads to NaN Outputs","2021-06-24 21:53:34","","2","1815","<deep-learning><neural-network><nlp><pytorch><transformer>","<p>I am trying to implement an autoregressive transformer model similar to the paper <em>attention is all you need</em>. From what I have understood, in order to replicate the architecture fully, I need to give the transformer decoder 3 masks.</p>
<ol>
<li><p>Target subsequent mask: this is for causality.</p>
</li>
<li><p>Target padding indexes: just to look at non-padded indices.</p>
</li>
<li><p>Encoder padding indices: just to look at non-padded inputs from the encoder.</p>
</li>
</ol>
<p>The snippet is here:</p>
<pre><code>y = self.decoder(y, x,
                         tgt_mask=tgt_causal_mask,
                         tgt_key_padding_mask=tgt_padding_mask,
                         memory_key_padding_mask=src_padding_mask)
</code></pre>
<hr />
<p>With masks being generated like this:</p>
<pre><code>def generate_no_peek_mask(self, sz):
    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)
    mask = mask.float().masked_fill(mask == 0, float(&quot;-inf&quot;)).masked_fill(mask == 1, float(0.0))
    mask = mask.to(self.device)
    return mask

def generate_padding_mask(self, seq, pad_idx):
    return (seq != pad_idx).to(self.device)
</code></pre>
<p>The problem is that using these masks leads to issues with the Softmax function because of NaN values. Without these masks, the model does not generate any NaN value. I have tried toying with various input lengths and seeing what happens when I make sure my inputs are moderately big, but it still does not work. The only thing that works is not giving the decoder the masks.</p>
","nlp"
"96994","How to structure unstructured data","2021-06-23 13:10:34","97010","1","276","<nlp><data-mining><structured-data>","<p>I am analysing tweets and have collected them in an unstructured format. What is the best way to structure this data so I can begin the data mining processes?
Somebody suggested using python packages such as spacy but not sure how to go about using this.</p>
","nlp"
"96954","Text classification - Multiple texts for a single instance (Deep Learning)","2021-06-22 12:17:14","","1","32","<deep-learning><nlp><text-classification>","<p>My task is to create a classification model (e.g. DDN, RNN) which as an input should accept list of texts and should produce a binary output (good, bad). Of-course the texts will be preprocess accordingly.</p>
<p>As en example let's consider classifying customers (good/bad) based on their emails. There are different number of emails for every customer and some of them have thousands of emails. It will be nice if the latest emails have greater importance and have bigger impact on a prediction.</p>
<p>What is a typical approach to this kind of problem?</p>
<p>I see only two possibilities right now:</p>
<ol>
<li>Combine all emails into one long text. Now we have simple text classification task. But all emails are accounted equally.</li>
<li>Treat each email as separate instance in the training set, with the same label (good/bad) as the owner of the email. In the end calculate weighted average predictions for emails (newer emails will have higher weight) and take it as prediction for a customer.</li>
</ol>
","nlp"
"96882","How to do batch inference on Hugging face pretrained models?","2021-06-20 16:10:22","","1","281","<deep-learning><nlp><machine-translation><huggingface>","<p>I want to do batch inference on MarianMT model. Here's the code:</p>
<pre><code>from transformers import MarianTokenizer
tokenizer = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-de')
src_texts = [ &quot;I am a small frog.&quot;, &quot;Tom asked his teacher for advice.&quot;]
tgt_texts = [&quot;Ich bin ein kleiner Frosch.&quot;, &quot;Tom bat seinen Lehrer um Rat.&quot;]  # optional
inputs = tokenizer(src_texts, return_tensors=&quot;pt&quot;, padding=True)
with tokenizer.as_target_tokenizer():
    labels = tokenizer(tgt_texts, return_tensors=&quot;pt&quot;, padding=True)
inputs[&quot;labels&quot;] = labels[&quot;input_ids&quot;]
outputs = model(**inputs) 
</code></pre>
<p>How do I do batch inference?</p>
","nlp"
"96867","Heauristics for a NER model prediction","2021-06-20 06:46:14","","2","39","<machine-learning><python><nlp><named-entity-recognition><spacy>","<p>I am trying to build and NER model that can name entities in a &quot;Job description.&quot;</p>
<p>The entities are:</p>
<ol>
<li>Mandatory skills (Must have skills like java, python, c++ etc.)</li>
<li>Nicetohave skills (the candidate &quot;may&quot; or &quot;may not&quot; have this, either is fine.)</li>
<li>Degree (bachelors, masters...)</li>
<li>Certification</li>
<li>Job title</li>
</ol>
<p>I prepared the dataset, annotated them and trained a spacy V2.0 model. The model performs well (in the range of 70-90s) on all labels, except &quot;nice to have.&quot; It is either misclassifying or not predicting at all. So I am trying to catch the sentences at with nicetohave skills and &quot;manually re-classify them, based on certain heuristics.</p>
<p>However, there are some complications within it. For eg:</p>
<p><em>Required skills:
Knowledge in python, c++
Must have knowledge on matlab
Nice to have skills:
Knowledge in CAD is a plus
Must have knowledge on CATIA V5</em></p>
<p>In the above example, skill from <strong>Required skills</strong> till the word <strong>matlab</strong> are <strong>mandatory</strong> and from <strong>nice to have:</strong> till <strong>CATIA V5</strong> everything has to be <strong>nicetohave</strong>. There is also a chance that in some Job description the order may change (Nicetohave is mentioned first, followed by mandatory skills)</p>
<p>Or it is also possible that
Mandatory skills are given first, followed by nice to have, followed by mandatory again. And many more combinations like that.</p>
<p>So how do I approach this scenario? I am open to any ideas, suggestions..</p>
","nlp"
"96864","Normalization before PCA in NLP domains?","2021-06-20 03:27:37","","0","166","<nlp><scikit-learn><clustering><pca>","<p>I'm working on a basic bag-of-words toy NLP pipeline for sentiment analysis using scikit-learn. From research of other questions here, it seems that the main applicable scaler for before PCA is the standardscaler. However, given that this is an NLP domain with many equivalent features, could the normalizer be considered instead?</p>
<p>For this dataset, I've tried the standardscaler before PCA, but I found that all categories were tightly clustered at 0, 0. If I replaced it with the normalizer, the data is much more spread out and some clusters start to form. Could this also be due to dataset size? I'm at about ~250 labeled documents. I'm only at the stage of looking for clusters.</p>
<p>My pipeline is CountVectorizer(n_gram=(1,1)) -&gt; (either StandardScaler or Normalizer) -&gt; PCA(n_components=2).</p>
<p><a href=""https://i.sstatic.net/9dMaf.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/9dMaf.png"" alt=""standard scaled"" /></a></p>
<p>Above is the standard scaled version:</p>
<p><a href=""https://i.sstatic.net/p2Y6K.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/p2Y6K.png"" alt=""normalized"" /></a></p>
<p>And here is the normalized version:</p>
","nlp"
"96830","Why do RNNs share weight?","2021-06-18 23:49:11","","2","47","<machine-learning><nlp><lstm><rnn>","<p>If weights are not shared the number of parameters will be extremely large and difficult to compute which I understand.
I don't understand the argument that varying length inputs are taken care of by sharing weights as stated in many StackExchange answers like <a href=""https://stats.stackexchange.com/questions/221513/why-are-the-weights-of-rnn-lstm-networks-shared-across-time"">https://stats.stackexchange.com/questions/221513/why-are-the-weights-of-rnn-lstm-networks-shared-across-time</a> or in this blog <a href=""https://towardsdatascience.com/recurrent-neural-networks-d4642c9bc7ce"" rel=""nofollow noreferrer"">https://towardsdatascience.com/recurrent-neural-networks-d4642c9bc7ce</a>. If in the architecture below I use different <span class=""math-container"">$W_{e}^{(t)}$</span> for each word at time <span class=""math-container"">$t$</span> then all of the <span class=""math-container"">$W_{e}^{(t)}$</span> will still have the same dimension because the embedding dimension for each word is same (every <span class=""math-container"">$e^{(t)}$</span> has the same size). And if we similarly take different <span class=""math-container"">$W_{h}^{(t)}$</span> at every time step (assuming all hidden states have the same number of nodes) then all <span class=""math-container"">$W_{h}^{(t)}$</span> will also have same dimensions. It will be equivalent to a series of vanilla NN's (inputs are embedding vector and previously hidden state vector of let's say dimension <span class=""math-container"">$e$</span> and <span class=""math-container"">$h$</span> respectively). Then the vanilla NN at <span class=""math-container"">$t$</span> time will output : <span class=""math-container"">$h^{(t)}=\sigma(W_{h}^{(t)}h^{(t-1)}+W_{e}^{(t)}e^{(t)}+bias)$</span><br />
So how does using the same <span class=""math-container"">$W_{h}$</span> and <span class=""math-container"">$W_{e}$</span> solve the problem of variable input sequence lengths?</p>
<p>Also, I know that in standard RNNs like below hidden state kind of store the context from previous time steps so what is the interpretation of <span class=""math-container"">$W_{h}$</span> here?</p>
<p><a href=""https://i.sstatic.net/sdjDe.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/sdjDe.png"" alt=""enter image description here"" /></a></p>
","nlp"
"96754","Extracting Keywoards from messages with own NER Model","2021-06-17 12:27:06","","0","39","<machine-learning><nlp><supervised-learning><named-entity-recognition>","<p>I'm starting a project where I want to extract keywoards from given messages. The keywoards are for example something like: &quot;hard disk&quot;, &quot;watch&quot; or other technical components. I'm working with a dataset where a technician wrote a small text if he maintenanced something on a given object.</p>
<p>The messages are often very different in their form. For example sometimes the messages start with the repaired object and sometimes with the current date..</p>
<p>I looked into some NER-Libarys and it doesn't seem like they can handle tasks like that. Especially the german language makes it hard for those libarys to detect entities.</p>
<p>I had the idea to use CRFsuite to train my own NER-Model. But I'm not sure how accurate the outcome will be. The process would include that i have to tag A LOT of training data and I'm not sure if the outcome will match the time I have to spend to tag those keywoards.</p>
<p>Does anybody has experience with such custom NER-Models? How accurate can such Model extract wanted Keywoards?</p>
<p>Any kind of feedback is appreaciated! Greetz</p>
","nlp"
"96743","encoding of text data in NLP","2021-06-17 09:28:47","96770","4","73","<python><nlp><preprocessing><similarity><text>","<p>I'm getting data using web scraping to create a dataset. I have a 'company' column that contains the names of the companies. I would like to encode this column but i don't know how to find the sentences that represent the same companies .</p>
<p>For example: &quot;International Business Machines Corporation&quot;, &quot;IBM&quot;, &quot;IBM India Pvt.Ltd&quot; reprsent the same company.</p>
<p>Any suggestion? Thank you</p>
","nlp"
"96679","Does it make sense to translate text before vectorization?","2021-06-16 03:12:23","","0","29","<nlp>","<p>I and new to NLP and I want to ask this question from knowledgeable people.</p>
<p>I work on a Russian text document and I want to vectorize it using some pre-trained embeddings. There are much more pre-trained models in English compared to Russian. So I asked myself, I will lemmatize and tokenize my document so it should be pretty easy to translate it from Russian to English. Maybe instead of using Russian-trained embeddings, I will translate it and then use English embeddings?</p>
<p>Does it even make sense?</p>
","nlp"
"96641","What are the advantages/disadvantages of using tfidf on n-grams generated through countvectorizer?","2021-06-15 07:12:20","96649","2","2687","<nlp>","<p>What are the advantages/disadvantages of using tfidf on n-grams generated through countvectorizer when your end goal is to see the frequent occurring terms in the corpus with the occurrence percentage?</p>
","nlp"
"96632","Influence of label names on the classfierier perfromance","2021-06-14 20:24:33","96635","0","24","<python><nlp><word-embeddings><bert>","<p>I am building a text classifier, the labels in my training data are not just short names like &quot;Dog&quot; or &quot;Cat&quot;, they are more of lengthy sentences that range from 2 words to around 20 words.</p>
<p>Does the length of the label/class name affect the performance of the classifier? in other words, should I try to shorten the names?</p>
","nlp"
"96595","Ways to build Abstractive summarisation and what are it's challenges","2021-06-14 07:30:21","","0","21","<deep-learning><nlp><bert><transformer>","<p>What are state of art techniques to build Abstractive summarisation on some paragraphs or articles and what kind of hurdles or challenges are there to approach this problem?</p>
","nlp"
"96557","How to use Word2Vec CBOW in statistical algorithm?","2021-06-12 17:32:39","","0","48","<nlp><logistic-regression><word2vec>","<p>I have seen a few examples of using CBOW in neural network models (although I did not understand them).</p>
<p>I know that Word2Vec is not similar to BOW or TFIDF, as there is no single value for CBOW and all examples I saw were used neural networks.</p>
<p>I have 2 questions:</p>
<p>1: Can we convert the vector to a single value and put it in a dataframe so we can use it in logistic regression model?</p>
<p>2: Is there any simple code for CBOW usage with logistic regression?</p>
","nlp"
"96556","An universal sentence encoder for a specific language?","2021-06-12 17:06:18","96581","1","57","<machine-learning><python><nlp><tensorflow>","<p>I am making a model that uses encoded articles (multiple sentences). I have found the <a href=""https://tfhub.dev/google/universal-sentence-encoder/4"" rel=""nofollow noreferrer"">Universal Sentence Encoder</a> by Tensorflow, but it says it is only for English. Specifically, I am looking for an encoder for the Macedonian language. Can I use this encoder and if not is there a multilingual model that understands Macedonian?</p>
","nlp"
"96536","Predict saved model from live camera for multiclass recognition","2021-06-11 21:13:36","","0","17","<nlp><image-classification><image-recognition>","<p>I have developed a model with relative good accuracy. I am trying to predict new input image from live camera by detecting or set a bounding box where only hand region are capture. I used the code below but the false prediction. I guess the capture image from camera has a lot of noise in it. How do I archive good live camera prediction of my model?</p>
<pre><code>import os
import traceback
import logging

import cv2
#from sklearn.externals 
import joblib
from skimage.feature import hog

from common.config import get_config
from common.image_transformation import resize_image


logging_format = '[%(asctime)s||%(name)s||%(levelname)s]::%(message)s'
logging.basicConfig(level=os.environ.get(&quot;LOGLEVEL&quot;, &quot;INFO&quot;),
                    format=logging_format,
                    datefmt='%Y-%m-%d %H:%M:%S',)
logger = logging.getLogger(__file__)


def get_image_from_label(label):
    testing_images_dir_path = get_config('testing_images_dir_path')
    image_path = os.path.join(testing_images_dir_path, label, '001.jpg')
    image = cv2.imread(image_path)
    return image


def main():

    camera = cv2.VideoCapture(0)
    while True:
        ret, frame = camera.read()
        if not ret:
            logger.error(&quot;Failed to capture image!&quot;)
            continue

        cv2.imshow(&quot;Webcam recording&quot;, frame)
        img = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)
        img = cv2.resize(frame,(128,128))
        try:
            hog_feature, hog_image = hog(img, orientations=8, pixels_per_cell=(16, 16), cells_per_block=(4, 4), block_norm= 'L2',visualize=True)
            classifier_model = joblib.load(&quot;model_name.pkl&quot;)
            predicted_labels = classifier_model.predict([hog_feature])
            predicted_label = predicted_labels[0]
            logger.info(&quot;Predicted label = {}&quot;.format(predicted_label))
            predicted_image = get_image_from_label(predicted_label)
            predicted_image = resize_image(predicted_image, 200)
            cv2.imshow(&quot;Prediction = '{}'&quot;.format(
                predicted_label), predicted_image)
        except Exception:
            exception_traceback = traceback.format_exc()
            logger.error(&quot;Error applying image transformation&quot;)
            logger.debug(exception_traceback)
        cv2.waitKey(2000)
        cv2.destroyAllWindows()
    cv2.destroyAllWindows()
    logger.info(&quot;The program completed successfully !!&quot;)


if __name__ == '__main__':
    main()
</code></pre>
","nlp"
"96514","Machine Learning for analyzing and generating sentences from given text inputs","2021-06-11 12:37:10","","1","24","<nlp><rnn><machine-translation>","<p>I'm trying to create a program that will translate Sign Language to Text and apply NLP so that the text is understandable to human. I've used CNN for recognizing sign language but I don't know how to transform the recognized text into a natural language. The idea is given a set of text, I want it to transform it to human language. For example, the statement &quot;Me eat&quot; (texts after recognizing the sign language), I want to improve it to become &quot;I am eating&quot;. Can it be done using RNN or does it fall under Machine Translation?</p>
","nlp"
"96501","How is a textual search engine able to recognize subwords from words?","2021-06-11 06:45:39","","0","22","<nlp><information-retrieval>","<p>I am interested to know how information retrieval systems are able to consider relevant subwords from a main search word when performing a keyword search. For example, the word <code>wristband</code> can either be considered as is, or as <code>wrist band</code>. When word tokenized, they appear as <code>[wristband]</code> and <code>[wrist, band]</code> respectively. If I am querying with <code>wristband</code>, the <code>wrist</code> and <code>band</code> will be ignored in the count vector.</p>
<p>Yet, I find common search engines that are able to retrieve results that contain subwords from the main search word:</p>
<p>Search suggestions for <code>wristband</code> from Shopee, an e-commerce site.
<a href=""https://i.sstatic.net/pcPVP.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/pcPVP.png"" alt=""Search suggestions for 'wristband' from Shopee, an e-commerce site"" /></a></p>
<p>Search suggestions for <code>wristband</code> from Amazon, an e-commerce site.
<a href=""https://i.sstatic.net/uF42x.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/uF42x.png"" alt=""enter image description here"" /></a></p>
<p>How is this done? I'm guessing they could brute force every single character combination and find ones with the most hits, but that doesn't sound very efficient.</p>
<p>I'm just looking for someone to point me in the right direction so that I can do more research myself.</p>
","nlp"
"96471","How can i extract words from a single concatenated word?","2021-06-10 10:34:49","96472","1","252","<nlp><text-mining><word-embeddings><bert><spacy>","<p>I'm stuck on this problem and would love some input.</p>
<p>I have mulitple words such as <em>getExtention</em>, <em>getPath</em>, <em>someWord</em> or <em>someword</em> and i want to separate each concatinated words into its own words such as:</p>
<p>getExtention ---&gt; [get][Extention].</p>
<p>someword --&gt; [some][word].</p>
<p>The concatenated words can also be in all small letters.</p>
<p>Do you guys have any ideas how I could achieve that?</p>
","nlp"
"96458","NLP approaches to infer Processes from Text","2021-06-10 00:41:33","96502","1","104","<nlp><spacy><allennlp>","<p>I would like to use NLP techniques to infer a process out of raw text. For example, if I have a sentence like:</p>
<p>Recruitment is about attracting and selecting the right person for the job.</p>
<p>To get the following process:</p>
<p>Attracting the right person.</p>
<p>I noticed that a very good strong step forward is to use <code>SpaCy</code>, tokenizing the texts and filtering them for NOUNS. But from this point on, I'm completely blank. Someone suggested to me something named &quot;Semantic Role Labelling&quot; (SRL) and reading about the approach I think it could work here in a good way, perhaps using the <code>AllenNLP</code> module.</p>
<p>What I would like to know though (and the whole purpose of this post) are alternatives. Besides SRL, what other approaches could suit and provide a solution to this problem? I obviously don't expect the people who reply to solve the problem, only to make suggestions on approaches that might work so I can dig upon them (as mentioned, my experience on this topic is really short).</p>
<p>Thanks in advance!</p>
","nlp"
"96447","classification of similar text input features with text output label","2021-06-09 16:05:41","96674","2","327","<keras><nlp><text-classification><gensim><doc2vec>","<p>I hope somebody can provide guidance/input/advice on my project, where I believe AI can help.</p>
<p>I have a general understanding of AI, but I lack a formal training.<br />
I've never built a neural net from scratch on my own.</p>
<h4>Task</h4>
<p>Build a classification model able to assign labels to input text data.<br />
Differently from a textbook example, the input is free text, so neither categorical nor numerical.<br />
To complicate matters, the predictors in the training data I use are often similar to each other.</p>
<h4>Data</h4>
<ul>
<li>input: short text data consisting of job descriptions, eg. <em>senior marketing manager</em><br />
the shortest entry consists of a single word, while the longest up to ~20 words.<br />
the input data forms a closed list (~130k entries) but new, unseen text might occur.</li>
<li>labels: closed list of 65 text labels (and corresponding <em>id</em>)</li>
</ul>
<h4>Strategies tested</h4>
<p>For a previous project, I built a word embedding model with <code>gensim</code> Word2Vec using the same data.<br />
So I used this model to get the vector representation of each word in the input text and then calculated the centroid to get an embedding for it.</p>
<p>I used this embedding to train&amp;test the following</p>
<ul>
<li>ordinary classifiers: <code>DecisionTree</code>, <code>RandomForest</code>, <code>NaiveBayes</code>, <code>KNeighborsClassifier</code> -&gt; max accuracy ~ 41%</li>
<li>multilabel classifiers: <code>OneVsRestClassifier</code>, <code>OneVsOneClassifier</code>, <code>OutputCodeClassifier</code> -&gt; max accuracy ~ 43%</li>
<li><code>keras</code>-based text classifier found in <a href=""https://datascience.stackexchange.com/questions/38280/keras-multiple-text-features-input-and-single-text-label-output-classification"">keras multiple text features input and single text label output classification</a> -&gt; max accuracy ~ 46%<br />
not really sure if this net is appropriate to my task, from the description it seems so to me</li>
</ul>
<p>Given such low accuracies, I did not tune the hyperparameters of these classifiers.<br />
I think I should build a model with higher baseline performance first.</p>
<h4>New Strategy</h4>
<p>I thought that training a <strong>Doc2Vec</strong> embedding of the input data and using it to train the <a href=""https://datascience.stackexchange.com/questions/38280/keras-multiple-text-features-input-and-single-text-label-output-classification"">keras</a> neural net should improve the performance of my classifier.<br />
Here's what I had in mind:</p>
<ul>
<li>create a grid of Doc2Vec (hyper)parameters</li>
<li>train a new Doc2Vec model for each combination</li>
<li>test each new model using a simple and quick classifier (eg. Logistic)</li>
<li>use the highest accuracy model to train&amp;test the classification <code>keras</code> neural net</li>
</ul>
<h4>Questions</h4>
<p>Before embarking on this (lengthy) process, I wish to ask for advice:</p>
<ul>
<li>Is this approach reasonable or can someone recommend a more clever way?</li>
<li>Can anyone recommend which Doc2Vec parameters to focus on and which to set?</li>
<li>Is the <a href=""https://datascience.stackexchange.com/questions/38280/keras-multiple-text-features-input-and-single-text-label-output-classification"">keras</a> neural net I've found appropriate for this task or should I modify it? If so, how?</li>
</ul>
<p>Many thanks to anyone who will be so kind to read all of this and provide suggestions.</p>
","nlp"
"96317","Data To Text NLG for financial reports","2021-06-05 18:27:06","","0","191","<machine-learning><nlp><transfer-learning><nlg>","<p>I am working on a project where I want to replace a <strong>template-based</strong> approach for financial reporting with an <strong>end-to-end approach with NLG</strong>.</p>
<p>The template-based approach takes as input some financial data (features about <strong>ESG</strong>: environment, governance, social of the company with some scores features on these three pillars); and then, it is passed as input to a function (i.e, the &quot;template&quot; function) which return a text which is present on the report for the company.</p>
<p>For example:</p>
<ul>
<li>score_governance = 40%</li>
<li>score_environment = 56%</li>
<li>Company : Toto</li>
<li>pct_females: 10%</li>
</ul>
<p>The text could be &quot;At Toto's company the governance score is below the average which can be due to a limited percentage of females... The environment score is above the average ....&quot;</p>
<p>It's scripted like if <em>pct_females</em> is below certain value text is like that else it's like that, etc.</p>
<p>In fact, this solution is not the best because the text sounds robotic and we need to regularly modify the template.</p>
<p>So I made some research and found out about <strong>data to text nlg</strong>. This is exactly what I want to do, transforming data (numerical but also some categorical) into text.</p>
<p>By taking a look at this dataset: <a href=""https://github.com/harvardnlp/boxscore-data"" rel=""nofollow noreferrer"">https://github.com/harvardnlp/boxscore-data</a>, I understand that to train such a model I need for the training a pair of <strong>(data, gold text)</strong>. But unfortunately, I only have the data available.</p>
<p>My ideas are :</p>
<ol>
<li><p><strong>Label</strong> the data (i.e produce a &quot;corpus&quot; text for each input sample). I will ask different people to describe the data they see using their own writing style. Then custom training on my dataset using some implemented data-to-text solution.</p>
</li>
<li><p>Using <strong>Transfer Learning</strong>. But I don't know if it is possible for data to text NLG and also I don't really found out papers that explore it.</p>
</li>
</ol>
<p>My question is:</p>
<p>If I label, how many? I know that I depend on my data, but if someone already works on a similar project (i.e. data to text custom training), what will be the minimum required</p>
<p>Thanks in advance to anyone who helps, I am open to any suggestions/ideas/papers...</p>
","nlp"
"95134","How to encode a sentence using an attention mechanism?","2021-06-01 13:08:09","95136","0","382","<nlp><encoding><attention-mechanism>","<p>Recently, I read about one of the state-of-the-art method called Attention models. This method use a Encoder-Decoder model. It can find a better encoding for each word in a sentence. But how can I encode a full sentence?</p>
<p>For example, I have a sentence &quot;I love reading&quot;.</p>
<p>After embedding, this sentence will be converted to list of three vectors. (or matrix with dimension <code>number of words</code> times <code>embedding dimension</code>).</p>
<p>After several layers of attention mechanism, I will still have the same matrix.</p>
<p>How can I convert this matrix to a single vector that contains an encoded representation of the full sentence?</p>
","nlp"
"95091","What hyperparameter values does the LDA mallet model use by default? Is it true that the formula to calculate alpha = 5.0/n(topics)?","2021-05-31 09:15:40","","2","83","<python><nlp><lda><dirichlet>","<p>I am trying to figure out the default <span class=""math-container"">$\alpha$</span> &amp; <span class=""math-container"">$\eta$</span> values used by mallet LDA, but there is not a lot of information on this. I did find a couple of answers, with no proper references, saying that symmetric <span class=""math-container"">$\alpha$</span> can be calculated with <code>5.0/num_topics</code>? Why is that? Why can't I use <code>1.0/num_topics</code> to calculate the symmetric <span class=""math-container"">$\alpha$</span>, just like in standard LDA? Can someone please help me understand and link me to references?</p>
<p>Thanks in advance.</p>
","nlp"
"95042","classification of bugs ownership by their content (improve score for log analysis)","2021-05-29 21:16:42","","0","31","<machine-learning><python><nlp><scikit-learn><text-classification>","<p>I'm doing a little project in which given a dataset of bugs and their relevant owner, I'd like to predict the &quot;final owner&quot; of a non-analyzed bug (bugs tends to assign back and forth between different owners and I'm referring to the &quot;real/final owner&quot; as the final one based on the dataset).</p>
<p>Number of classes/labels is 6</p>
<p>The data I use to make these decisions relays mostly on the bug's content which can be a subset of the following:</p>
<ul>
<li>Error logs</li>
<li>Python tracebacks</li>
<li>Panics</li>
<li>Panics traces</li>
</ul>
<p>I'm also using <code>SMOTETomek</code> to balanced the dataset (it creates artificial training data) - this alone increased my results by 10-15%.</p>
<p>However, I'm still getting a very low score (around 65%) when applying a set of classifiers (Each is being trained against the above mentioned data)</p>
<p>Example for classifiers I've tried: SGDClassifier, MultinomialNB, BernoulliNB, LinearSVC, RandomForestClassifier, LogisticRegression, DecisionTreeClassifier, ExtraTreeClassifier, etc.</p>
<p>My pipeline looks as follows:</p>
<pre><code>def train(self, algorithm, X_train, y_train):
    model = Pipeline([
        ('vect', CountVectorizer()),
        ('tfidf', TfidfTransformer()),
        ('smote', SMOTETomek()),
        ('classifier', algorithm)
    ])
    model.fit(X_train, y_train)
    return model
</code></pre>
<p>where each algorithm in the pipeline is one of the above mentioned classifiers (it's being run in a loop for each one)</p>
<p>Initially I thought that the root cause for this fairly low score is the way I'm sanitize the data.</p>
<p>For example: For errors, I'm relaying on regex to clean timestamps, thread ID, etc. For python tracebacks I'm getting rid of line numbers and taking just func names</p>
<p>Later on I was surprised to find out that even without cleaning the data, I'm getting the same score, so it made me think I'm missing something</p>
<p>My features are simply text I cleaned from the logs and (each feature is just a long string) and the labels are (each) simply a number that correlates to a class.</p>
<p>How can it be that no matter what I do (cleaning the logs vs leave it untouched) faces me with the same score? Am I using the wrong steps in the pipeline? missing steps?</p>
<p>Any help will be appreciated as I've no clue where/how to proceed.</p>
","nlp"
"95040","1D Data for NLP models","2021-05-29 20:40:42","","0","15","<machine-learning><deep-learning><nlp><transformer>","<p>My dataset( Network traffic dataset where we do binary classification)-</p>
<p><a href=""https://i.sstatic.net/ehEw4.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ehEw4.png"" alt=""enter image description here"" /></a></p>
<p>Number of features in data is 25</p>
<p>Can we use this kind of dataset in NLP models like transformers? If yes what should be the transformations to be done on this to feed it into models like transformers for binary classification?</p>
","nlp"
"95021","Why is my neural language model performing so poorly?","2021-05-28 22:50:06","","0","70","<nlp><tensorflow><lstm><rnn>","<p>I am trying to create a word-level Haiku generator using an LSTM neural network. I am scraping haikus from Reddit's r/haiku, and wanted to start with a &quot;simple&quot; model: my training data is the set of all haikus, flattened, and split into trigrams, such that the first two words of the trigram are the features and the last word is the label. I chose a &quot;context size&quot; of two just because I wanted to start with a small size, considering how small the haikus themselves are. So this is essentially a text prediction task with a small context window.</p>
<p>E.g the first couple (X,y) pairs (There are ~ 90,000 such pairs) look like:</p>
<pre class=""lang-py prettyprint-override""><code>[('delicate savage', '/'),
 ('savage /', &quot;you'll&quot;),
 (&quot; / you'll&quot;, 'never'),
 (&quot;you'll never&quot;, 'hold')... ]
</code></pre>
<p>(I am using the '/' character to designate a new stanza, and I also added '$' to indicate the end of the haiku, to see if my model will learn such distinctions present in a haiku's format)</p>
<p>I then encode my sequences using an index mapping and feed this into an embedding layer when I train my model.</p>
<p>The model:</p>
<pre class=""lang-py prettyprint-override""><code>Sequential([
    Embedding(input_dim=vocab_size, output_dim=60, input_length=seq_length), #seq length is 2
    LSTM(100,return_sequences=True),
    LSTM(100),
    Dense(100,kernel_regularizer=l2(0.01),activation = 'relu'),
    Dense(vocab_size, activation = 'softmax')
])
</code></pre>
<p>My performance is pretty bad: about 45% training accuracy and 15% validation accuracy. When I generate haiku sequences using a fit model, the model is often clueless and returns None.</p>
<p>I recognize that there are several places my model could be going wrong, but am really not sure where. I've tried various regularization techniques (dropout, L2) and modifying my neural network's architecture (# of layers and nodes) but without any luck. Before I try using pre-trained embedding, what can I do to improve my ability to predict text with my data?</p>
<p>If it helps, here's my <a href=""https://github.com/bfbarry/HaikuBot/blob/main/notebooks/preprocess_train_gen.ipynb"" rel=""nofollow noreferrer"">notebook</a>, using a language model heavily based on <a href=""https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/"" rel=""nofollow noreferrer"">this implementation</a>.</p>
","nlp"
"95012","Why we shift target(output) by one offset in language modelling","2021-05-28 17:35:31","","1","139","<keras><nlp><rnn><language-model>","<p>I have been working in sequence prediction tasks (very similar to language modelling)  where I want to predict the next token(s)/item(s) given past sequence of tokens. I have always taken an approach like the below for data preparation</p>
<pre><code>[0,1,2,3] -&gt; [4]
[4,5,6,7] -&gt; [8]
</code></pre>
<p>or</p>
<pre><code>[0,1,2,3] -&gt; [4,5,6,7]
[4,5,6,7] -&gt; [8,9,10,11]
</code></pre>
<p>I have recently seen people taking output/target as one offset shifted from X. Since the goal of language modelling is to predict next token. Why and in which situation would one use this data format?</p>
<p>For example a famous book D2l.ai provide <a href=""http://d2l.ai/chapter_recurrent-neural-networks/language-models-and-dataset.html"" rel=""nofollow noreferrer"">example</a> format of the data as</p>
<pre><code>X:  [[27. 28. 29. 30. 31.]
 [12. 13. 14. 15. 16.]]
Y: [[28. 29. 30. 31. 32.]
 [13. 14. 15. 16. 17.]]
</code></pre>
<p>Isn't it predicting token <code>28</code> after tokens <code>[27. 28. 29. 30. 31.]</code>wheras actaly Y(output) should be <code>[32]</code> or <code>[32,33,34,...]</code> ?</p>
<p>How would next prediction (which actually is <code>32</code>) be retrieved after token <code>31</code>?</p>
","nlp"
"94987","An algorithm to extract the purpose of a document","2021-05-27 17:33:50","","0","35","<nlp><lstm><bert><transformer><attention-mechanism>","<p>I want to build an algorithm to extract the <strong>purpose of the document</strong> (scientific papers for example) by extracting the sentences that state the purpose. I don't have many annotated data so I might use a semi-supervised learning algorithm. I was thinking of training a Q/A algorithm (using Bert for example..) but I want something to be specific to the purpose task..</p>
<p>Any idea or keywords that might help ?
Thanks!</p>
","nlp"
"94973","Transformer: English -> Source Code Training Accuracy stuck 60% and validation 40%","2021-05-27 13:26:27","","0","161","<machine-learning><deep-learning><nlp><tensorflow><transformer>","<p>I'm working on my final year project which is to write a model that takes as an input an english sentence and generate source code (currently testing on english-&gt;javascript dataset provided by CodeSearchNet) as output.</p>
<p>So, I decided to use Transformers as my model because I read many articles and it seemed that it is a good seq2seq model with good result.</p>
<p>I have used the transformer code provided by the Tensorflow documentation and I have applied few changes of course like now I'm using a pre-trained tokenizer called CodeBert which is pretrained on nl --&gt; 6 programming language (java/javascript/python/....)</p>
<p>The problem I'm facing that when I run my training(50k training data,5k validation), after 20epochs my training accuracy gets stuck around 60-65% and my validation accuarcy around 40-45% I even tried to train for 30 more epochs but it is the same and result of generation is no where close to something correct. Here what it looks like :</p>
<p>English: generate a number from 1 to 10
Result :
function ( ) { var i = 0 ; var i = 0 ; var i = 0 ; var i = 0 ; var i = 0 ; var i = 0 ; var i = 0 ; var i = 0 ; var i = 0 ; var i = 0 ; var i = 0 ; var i = 0 ; var i = 0 ; var i = 0 ; var i = 0 ; var i = 0 ; var i = 0 ; var i = 0 ; var i = 0 ; var i = 0 ; var i = 0 ; var i = 0 ; var i = 0 ; var i = 0x0 = 0x0 &amp; 0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x0000000000000000000000000000000000000000000000000000000000000000000000</p>
<p>I don't really know what I'm doing wrong, I have tried to play around with hyper-parameter but no result. I'm stuck I really need your advice if someone could help me and thanks in advance. here is my code:</p>
<p>Positional Encoding:</p>
<pre><code>import numpy as np
import tensorflow as tf
from tensorflow.keras import layers
class PositionalEncoding(layers.Layer):
def __init__(self):
    super(PositionalEncoding, self).__init__()

def get_angles(self, pos, i, d_model):
    angles = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))
    return pos * angles

def call(self, inputs):
    seq_length = inputs.shape.as_list()[-2]
    d_model = inputs.shape.as_list()[-1]
    angles = self.get_angles(np.arange(seq_length)[:, np.newaxis], np.arange(d_model)[np.newaxis, :], d_model)
    angles[:, 0::2] = np.sin(angles[:, 0::2])
    angles[:, 1::2] = np.cos(angles[:, 1::2])
    pos_encoding = angles[np.newaxis, ...]
    return inputs + tf.cast(pos_encoding, tf.float32)
</code></pre>
<p>Multi-head Attention</p>
<pre><code>import tensorflow as tf
from tensorflow.keras import layers

def scaled_dot_product_attention(queries,keys,values,mask):
    product = tf.matmul(queries,keys,transpose_b=True)
    keys_dim = tf.cast(tf.shape(keys)[-1],tf.float32)
    scaled_product = product/tf.math.sqrt(keys_dim)
    if mask is not None:
        scaled_product += (mask * -1e9)
    attention = tf.matmul(tf.nn.softmax(scaled_product,axis=-1),values)
    return attention


class MultiHeadAttention(layers.Layer):
    def __init__(self,nb_proj):
        super(MultiHeadAttention, self).__init__()
        self.nb_proj = nb_proj

    def build(self, input_shape):
        self.d_model = input_shape[-1]
        assert self.d_model % self.nb_proj == 0

        self.d_proj = self.d_model // self.nb_proj

        self.query_lin = layers.Dense(units=self.d_model)
        self.key_lin = layers.Dense(units=self.d_model)
        self.value_lin = layers.Dense(units=self.d_model)

        self.final_lin = layers.Dense(units=self.d_model)

    def split_proj(self,inputs,batch_size):
        shape = (batch_size,-1,self.nb_proj,self.d_proj)
        splited_inputs = tf.reshape(inputs,shape=shape)
        return tf.transpose(splited_inputs,perm=[0,2,1,3])

    def call(self,queries,keys,values,mask):
        batch_size = tf.shape(queries)[0]

        queries = self.query_lin(queries)
        keys = self.key_lin(keys)
        values = self.value_lin(values)

        queries = self.split_proj(queries,batch_size)
        keys = self.split_proj(keys,batch_size)
        values = self.split_proj(values,batch_size)

        attention = scaled_dot_product_attention(queries,keys,values,mask)

        attention = tf.transpose(attention,perm=[0,2,1,3])

        concat_attention = tf.reshape(attention,shape=(batch_size,-1,self.d_model))

        output = self.final_lin(concat_attention)

        return output
</code></pre>
<p>Encoder Layer</p>
<pre><code>from tensorflow.keras import layers
from TransformerArch import MultiHeadAttention


class EncoderLayer(layers.Layer):
    def __init__(self, FFN_units, nb_proj, dropout):
        super(EncoderLayer, self).__init__()
        self.FFN_units = FFN_units
        self.nb_proj = nb_proj
        self.dropout = dropout

    def build(self, input_shape):
        self.d_model = input_shape[-1]
        self.multi_head_attention = MultiHeadAttention.MultiHeadAttention(self.nb_proj)
        self.dropout_1 = layers.Dropout(rate=self.dropout)
        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)
        self.dense_1 = layers.Dense(units=self.FFN_units, activation=&quot;relu&quot;)
        self.dense_2 = layers.Dense(units=self.d_model)
        self.dropout_2 = layers.Dropout(rate=self.dropout)
        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)

    def call(self, inputs, mask, training):

        attention = self.multi_head_attention(inputs, inputs, inputs, mask)

        attention = self.dropout_1(attention, training=training)

        attention = self.norm_1(attention + inputs)

        outputs = self.dense_1(attention)

        outputs = self.dense_2(outputs)

        outputs = self.dropout_2(outputs)

        outputs = self.norm_2(outputs + attention)

        return outputs
</code></pre>
<p>Encoder:</p>
<pre><code>import tensorflow as tf
from tensorflow.keras import layers
from TransformerArch import PositionalEncoding
from TransformerArch import EncoderLayer


class Encoder(layers.Layer):
    def __init__(self, nb_layers, FFN_units, nb_proj, dropout, vocab_size, d_model, name=&quot;encoder&quot;):
        super(Encoder, self).__init__(name=name)
        self.nb_layers = nb_layers
        self.FFN_units = FFN_units
        self.nb_proj = nb_proj
        self.dropout = dropout
        self.d_model = d_model

        self.embedding = layers.Embedding(vocab_size, d_model)
        self.pos_encoding = PositionalEncoding.PositionalEncoding()
        self.dropout = layers.Dropout(rate=dropout)
        self.enc_layers = [EncoderLayer.EncoderLayer(FFN_units, nb_proj, dropout)
                           for _ in range(nb_layers)]

    def call(self, inputs, mask, training):
        outputs = self.embedding(inputs)
        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))
        outputs = self.pos_encoding(outputs)
        outputs = self.dropout(outputs, training)
        for i in range(self.nb_layers):
            outputs = self.enc_layers[i](outputs, mask, training)
        return outputs
</code></pre>
<p>Decoder Layer:</p>
<pre><code>from tensorflow.keras import layers
from TransformerArch import MultiHeadAttention


class DecoderLayer(layers.Layer):
    def __init__(self, FFN_units, nb_proj, dropout):
        super(DecoderLayer, self).__init__()
        self.FFN_units = FFN_units
        self.nb_proj = nb_proj
        self.dropout = dropout

    def build(self, input_shape):

        self.d_model = input_shape[-1]
        self.multi_head_attention_1 = MultiHeadAttention.MultiHeadAttention(self.nb_proj)
        self.dropout_1 = layers.Dropout(rate=self.dropout)
        self.norm_1 = layers.LayerNormalization(epsilon=1e-3)

        self.multi_head_attention_2 = MultiHeadAttention.MultiHeadAttention(self.nb_proj)
        self.dropout_2 = layers.Dropout(rate=self.dropout)
        self.norm_2 = layers.LayerNormalization(epsilon=1e-3)

        self.dense_1 = layers.Dense(units=self.FFN_units, activation=&quot;relu&quot;)
        self.dense_2 = layers.Dense(units=self.d_model)
        self.dropout_3 = layers.Dropout(rate=self.dropout)
        self.norm_3 = layers.LayerNormalization(epsilon=1e-3)

    def call(self, inputs, enc_outputs, mask_1, mask_2, training):
        attention = self.multi_head_attention_1(inputs, inputs, inputs, mask_1)

        attention = self.dropout_1(attention, training)
        attention = self.norm_1(attention + inputs)

        attention_2 = self.multi_head_attention_2(attention, enc_outputs, enc_outputs, mask_2)
        attention_2 = self.dropout_2(attention_2, training)
        attention_2 = self.norm_2(attention_2 + attention)


        outputs = self.dense_1(attention_2)
        outputs = self.dense_2(outputs)
        outputs = self.dropout_3(outputs, training)
        outputs = self.norm_3(outputs + attention_2)


        return outputs
</code></pre>
<p>Decoder:</p>
<pre><code>import tensorflow as tf
from tensorflow.keras import layers
from TransformerArch import DecoderLayer
from TransformerArch import PositionalEncoding


class Decoder(layers.Layer):
    def __init__(self, nb_layers, FFN_units, nb_proj, dropout, vocab_size, d_model, name=&quot;decoder&quot;):
        super(Decoder, self).__init__(name=name)
        self.nb_layers = nb_layers
        self.d_model = d_model

        self.embedding = layers.Embedding(vocab_size, d_model)
        self.pos_encoding = PositionalEncoding.PositionalEncoding()
        self.dropout = layers.Dropout(rate=dropout)
        self.dec_layers = [DecoderLayer.DecoderLayer(FFN_units, nb_proj, dropout) for _ in range(nb_layers)]

    def call(self, inputs, enc_outputs, mask_1, mask_2, training):
        outputs = self.embedding(inputs)

        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))

        outputs = self.pos_encoding(outputs)

        outputs = self.dropout(outputs, training)

        for i in range(self.nb_layers):
            outputs = self.dec_layers[i](outputs, enc_outputs, mask_1, mask_2, training)

        return outputs
</code></pre>
<p>Transformer:</p>
<pre><code>import tensorflow as tf
from TransformerArch import Encoder
from TransformerArch import Decoder
from tensorflow.keras import layers


class Transformer(tf.keras.Model):
    def __init__(self, vocab_size_enc, vocab_size_dec, d_model, nb_layers, FFN_units, nb_proj, dropout,
                 name=&quot;transfomer&quot;):
        super(Transformer, self).__init__(name=name)
        self.encoder = Encoder.Encoder(nb_layers, FFN_units, nb_proj, dropout, vocab_size_enc, d_model)
        self.decoder = Decoder.Decoder(nb_layers, FFN_units, nb_proj, dropout, vocab_size_dec, d_model)
        self.last_linear = layers.Dense(units=vocab_size_dec)

    def create_padding_mask(self, seq):
        mask = tf.cast(tf.math.equal(seq, 1), tf.float32)
        return mask[:, tf.newaxis, tf.newaxis, :]

    def create_look_ahead_mask(self, seq):
        seq_len = tf.shape(seq)[1]
        look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)
        return look_ahead_mask

    #def call(self, enc_inputs, dec_inputs, training):
    def call(self, inputs):
        enc_mask = self.create_padding_mask(inputs[0])
        dec_mask_1 = tf.maximum(self.create_padding_mask(inputs[1]), self.create_look_ahead_mask(inputs[1]))
        dec_mask_2 = self.create_padding_mask(inputs[0])
        enc_outputs = self.encoder(inputs[0], enc_mask, inputs[2])
        dec_outputs = self.decoder(inputs[1], enc_outputs, dec_mask_1, dec_mask_2, inputs[2])
        outputs = self.last_linear(dec_outputs)
        return outputs
</code></pre>
<p>Training Code:</p>
<pre><code>from TransformerArch import Transformer
import tensorflow as tf
import time
import os
import DPPreLN, ValidationS


TF_CONFIG_ = tf.compat.v1.ConfigProto()
TF_CONFIG_.gpu_options.allow_growth = True
sess = tf.compat.v1.Session(config=TF_CONFIG_)

DIRNAME = os.path.dirname(__file__)
with tf.device('/GPU:0'):
    tf.keras.backend.clear_session()

    # Hyper-parameters
    D_MODEL = 128  # 512 #128
    NB_LAYERS = 4  # 6 #4
    FFN_UNITS = 256  # 2048 #512
    NB_PROJ = 8  # 8
    DROPOUT = 0.1  # 0.1

    transformer = Transformer.Transformer(vocab_size_enc=50265, vocab_size_dec=50265, d_model=D_MODEL,
                                          nb_layers=NB_LAYERS,
                                          FFN_units=FFN_UNITS, nb_proj=NB_PROJ, dropout=DROPOUT)

    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=&quot;none&quot;)


    def loss_function(target, pred):
        mask = tf.math.logical_not(tf.math.equal(target, 1)) #CodeBert pad Token ID is 1 not 0
        loss = loss_object(target, pred)
        mask = tf.cast(mask, dtype=loss.dtype)
        loss *= mask
        return tf.reduce_sum(loss) / tf.reduce_sum(mask)


    def accuracy_function(real, pred):
        accuracies = tf.equal(real, tf.cast(tf.argmax(pred, axis=2), tf.int32))

        mask = tf.math.logical_not(tf.math.equal(real, 1))
        accuracies = tf.math.logical_and(mask, accuracies)

        accuracies = tf.cast(accuracies, dtype=tf.int32)
        mask = tf.cast(mask, dtype=accuracies.dtype)
        return tf.reduce_sum(accuracies) / tf.reduce_sum(mask)


    train_loss = tf.keras.metrics.Mean(name=&quot;train_loss&quot;)
    train_accuracy = tf.keras.metrics.Mean(name=&quot;train_accuracy&quot;)

    valid_loss = tf.keras.metrics.Mean(name=&quot;valid_loss&quot;)
    valid_accuracy = tf.keras.metrics.Mean(name=&quot;valid_accuracy&quot;)


    class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):
        def __init__(self, d_model, warmup_steps=4000):
            super(CustomSchedule, self).__init__()
            self.d_model = tf.cast(d_model, tf.float32)
            self.warmup_steps = warmup_steps

        def __call__(self, step):
            arg1 = tf.math.rsqrt(step)
            arg2 = step * (self.warmup_steps ** -1.5)
            self.lr = tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)
            return self.lr

        def get_config(self):
            config = {
                'd_model': self.d_model,
                'warmup_steps': self.warmup_steps,
                'lr': self.lr
            }
            return config


    learning_rate = CustomSchedule(D_MODEL)

    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate ,beta_1=0.9, beta_2=0.98, epsilon=1e-9)

    checkpoint_path = os.path.join(DIRNAME, &quot;Checkpoint&quot;)  # path to pc

    ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)

    ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=3)

    if ckpt_manager.latest_checkpoint:
        ckpt.restore(ckpt_manager.latest_checkpoint)
        print(&quot;Latest Checkpoint restored !&quot;)

    EPOCHS = 50
    dataset = DPPreLN.prepareData()
    valid_set = ValidationS.prepareData()
    
    for epoch in range(EPOCHS):
        print(&quot;Start of epoch {}&quot;.format(epoch + 1))
        start = time.time()
        train_loss.reset_states()
        train_accuracy.reset_states()
        for (batch, (enc_inputs, targets)) in enumerate(dataset):
            dec_inputs = targets[:, :-1]
            dec_outputs_real = targets[:, 1:]
            with tf.GradientTape() as tape:
                predictions = transformer([enc_inputs, dec_inputs, True])
                loss = loss_function(dec_outputs_real, predictions)
            gradients = tape.gradient(loss, transformer.trainable_variables)
            optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))
            train_loss(loss)
            train_accuracy(accuracy_function(dec_outputs_real, predictions))
            
            if batch % 50 == 0:
                print(
                    &quot;Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}&quot;.format(epoch + 1, batch,
                                                                           train_loss.result(),
                                                                           train_accuracy.result() * 100,
                                                                           ))

        # VALIDATION TEST
        print(&quot;***** VALIDATION PART *****&quot;)
        valid_loss.reset_states()
        valid_accuracy.reset_states()
        for (batch, (enc_inputs, targets)) in enumerate(valid_set):
            dec_inputs = targets[:, :-1]
            dec_outputs_real = targets[:, 1:]
            predictions = transformer([enc_inputs, dec_inputs, False])
            loss = loss_function(dec_outputs_real, predictions)
            valid_loss(loss)
            valid_accuracy(accuracy_function(dec_outputs_real, predictions))
            if batch % 50 == 0:
                print(
                    &quot;Validation :Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}&quot;.format(epoch + 1, batch,
                                                                                       valid_loss.result(),
                                                                                       valid_accuracy.result() * 100,
                                                                                       ))

        # SAVE CHECKPOINT/MODEL
        ckpt_save_path = ckpt_manager.save()
        print(&quot;Saving checkpoint for epoch {} at {} &quot;.format(epoch + 1, ckpt_save_path))
        print(&quot;Time taken for 1 epoch : {} secs\n&quot;.format(time.time() - start))
        transformer.save_weights(os.path.join(DIRNAME, &quot;Models&quot;, &quot;Weight&quot;))
        print(&quot;*****************************************&quot;)
        print(&quot;*   Model Has Been Successfully Saved   *&quot;)
        print(&quot;*****************************************&quot;)


    print(transformer.summary())
</code></pre>
","nlp"
"94951","What the differences between self-supervised/semi-supervised in NLP?","2021-05-27 04:00:18","94956","1","421","<nlp><semi-supervised-learning><pretraining>","<p>GPT-1 mentions both Semi-supervised learning and Unsupervised pre-training but it seems like the same to me. Moreoever, &quot;Semi-supervised Sequence Learning&quot; of Dai and Le also more like self-supervised learning. So what the key differences between them?</p>
","nlp"
"94940","English to ""basic English"" translation","2021-05-26 16:14:23","","1","860","<python><nlp><text-generation>","<p>I'd like to build something (ideally in Python) that can translate an English sentence into &quot;basic&quot; English.</p>
<p><strong>Are there any free/open-source tools/frameworks</strong> that can help? If not, what kind of steps can help solve this problem? (e.g., building on existing work like WordNet or pre-trained word embeddings).</p>
<p>By &quot;basic&quot; I mean things like being <strong>concise</strong> (avoiding unnecessary verbiage) and using <strong>well-known words</strong> (without compromising too much on meaning). I might even consider &quot;broken&quot; English, where verbs for example are lemmatised to their root.</p>
<p>The idea is to get something akin to what Wiki have achieved with their <a href=""https://simple.wikipedia.org/wiki/Basic_English"" rel=""nofollow noreferrer"">basic English</a> versions of articles. And to give one more example, simplifying sentences is one of the services provided by the <a href=""https://www.grammarly.com/blog/engineering/plainly-speaking-a-linguistic-approach-to-simplifying-complex-words/"" rel=""nofollow noreferrer"">Grammarly app</a>.</p>
<hr />
<p>For example:</p>
<p><strong>English:</strong></p>
<pre><code>I hurriedly made my way over to the store in order to purchase various garments from there
</code></pre>
<p><strong>&quot;Basic&quot; English</strong></p>
<pre><code>I quickly went to the shop to buy clothes
</code></pre>
<p><strong>&quot;Broken&quot; English</strong></p>
<pre><code>I quick go shop buy clothes
</code></pre>
<hr />
","nlp"
"94913","How to capture the detail of an attribute in a sentence?","2021-05-26 06:17:53","","1","42","<nlp><grammar-inference>","<p>The problem is best explained using an example, so please consider the sentence below:</p>
<p>Made of airy <em>cotton</em> with a touch of stretch, the Pinafore Dress features a modern <em>square neckline</em>.</p>
<p>Here, <em>cotton</em> (FABRIC) and <em>square neckline</em> (NECKLINE) are two important attributes in the sentence. What I need to do is to capture the word <em>airy</em> which is a detail of the fabric. FABRIC and NECKLINE can be successfully captured using NER, but NER is not working well when it comes to capturing the detail terms like <em>airy</em>.</p>
<p>Can someone point out how I could solve this?</p>
","nlp"
"94910","Implementing a model for a language to another","2021-05-26 03:57:44","","0","32","<deep-learning><nlp><dataset><language-model><implementation>","<p>I have a dataset of sentences of language X and Y (2 columns, for example, &quot;abc def lang&quot; ==&gt; &quot;xyz pqrt mno uages&quot;). I want to have a output as a table that translates word by word (abc =&gt; xyz, def =&gt; pqrt mno, lang =&gt; uages).</p>
<p>I think we should use deep learning but idk how to implement and what to use. Can you give me some ideas?</p>
<p>Thanks all</p>
","nlp"
"94894","How to find the related columns between column?","2021-05-25 15:31:57","","1","37","<python><nlp><dataframe><nltk>","<p>Suppose I have a data frame with columns car_id,number_car,bike_id,number_bike.</p>
<pre><code>df =
Product_id   Category order_id   order   customer_id  customer Name 
 pd01         car      or01      service   cus01       Jack
 pd02         bike     or02      repair    cus02       Swan
 pd03         car      or3       repair    cus03       Tom
 pd04         car      or04      cleaning  cus04       ram
</code></pre>
<p>Where we can make out that Product_id has a relation with Category, order_id has a relation with order and customer_id has a relation with customer name. But I am just curious about finding this by using nltk or any other way. Is it possible to find the relation between the columns using nltk word2vec? Just like sending a sentence as a list can we send the column as a list?</p>
<p>If not possible by nltk or is there any other way of doing this in data science, or any other method not related to data science. What is the best way to do it?</p>
<p>Basically, I am trying to find the dimensional tables without any human interference</p>
","nlp"
"94867","ML model to predict values from text (a lot of training training data)","2021-05-25 05:37:04","","0","311","<machine-learning><python><neural-network><nlp><text-classification>","<p>I have around 1M entries of the type:</p>
<pre><code>id | big5_openness | big5_conscientiousness | big5_extraversion | big5_agreeableness | big5_neuroticism | input_text
</code></pre>
<p>Where <code>id</code> is just the identifier of some user (it is not important), the big5 values are numbers between 0 and 100 (showing how much of each of them a person is), and the input_text is the text that is used to predict this values.
These are actual predictions, but I want to develop an algorithm that uses these predictions as training data to predict these values for an <code>arbitrary text</code>.</p>
<p>Here is some information on how the original predictions were made by the software that created the data:
<a href=""https://i.sstatic.net/oqVRz.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/oqVRz.png"" alt=""enter image description here"" /></a></p>
<p>(<a href=""https://cloud.ibm.com/docs/personality-insights?topic=personality-insights-science"" rel=""nofollow noreferrer"">https://cloud.ibm.com/docs/personality-insights?topic=personality-insights-science</a>)</p>
<p>Does anyone know what kind of ML algorithm I could use to train on this type of data (where the input is text, and the predictions are 5 discrete classes? For me, this immediately looks like a problem that can be solved by neural network (encode the texts somehow).
Thank you in advance, have a great day!</p>
","nlp"
"94859","Error while using pre-trained model","2021-05-24 23:48:33","","1","27","<python><deep-learning><nlp><pytorch><prediction>","<p>I'm working on NLP task using RoBERTa model. As training last very long I saved my model, but now for some reason, part of my code doesn't work with this pre-trained model (getting an error), and before saving and uploading newly it, the code worked fine with the trained model. My code:</p>
<pre><code>model = model.from_pretrained(&quot;/content/drive/MyDrive/model&quot;)
model = model.cuda()
model.eval()

def to_list(tensor):
    return tensor.detach().cpu().tolist()

def run_prediction(question_texts, context_text):
    &quot;&quot;&quot;Setup function to compute predictions&quot;&quot;&quot;
    examples = []

    for i, question_text in enumerate(question_texts):
        example = SquadExample(
            qas_id=str(i),
            question_text=question_text,
            context_text=context_text,
            answer_text=None,
            start_position_character=None,
            title=&quot;Predict&quot;,
            is_impossible=False,
            answers=None,
        )

        examples.append(example)

    features, dataset = squad_convert_examples_to_features(
        examples=examples,
        tokenizer=tokenizer,
        max_seq_length=384,
        doc_stride=128,
        max_query_length=64,
        is_training=False,
        return_dataset=&quot;pt&quot;,
      #  threads=1,
    )

    eval_sampler = SequentialSampler(dev_dataset)
    eval_dataloader = DataLoader(dev_dataset, sampler=eval_sampler, batch_size=10)

    all_results = []

    for batch in eval_dataloader:
        model.eval()
        batch = tuple(t.to(device) for t in batch)

        with torch.no_grad():
            inputs = {
                &quot;input_ids&quot;: batch[0],
                &quot;attention_mask&quot;: batch[1],
                &quot;token_type_ids&quot;: batch[2],
            }

            example_indices = batch[3]

            outputs = model(**inputs, return_dict=False)

            for i, example_index in enumerate(example_indices):
                eval_feature = features[example_index.item()]
                unique_id = int(eval_feature.unique_id)

                output = [to_list(output[i]) for output in outputs]

                start_logits, end_logits = output
                result = SquadResult(unique_id, start_logits, end_logits)
                all_results.append(result)

    output_prediction_file = &quot;predictions.json&quot;
    output_nbest_file = &quot;nbest_predictions.json&quot;
    output_null_log_odds_file = &quot;null_predictions.json&quot;

    predictions = compute_predictions_logits(
        examples,
        features,
        all_results,
        n_best_size,
        max_answer_length,
        do_lower_case,
        output_prediction_file,
        output_nbest_file,
        output_null_log_odds_file,
        False,  # verbose_logging
        True,  # version_2_with_negative
        null_score_diff_threshold,
        tokenizer,
    )

    return predictions

context = &quot;New Zealand (Māori: Aotearoa) is a sovereign island country in the southwestern Pacific Ocean. It has a total land area of 268,000 square kilometres (103,500 sq mi), and a population of 4.9 million. New Zealand's capital city is Wellington, and its most populous city is Auckland.&quot;
questions = [&quot;How many people live in New Zealand?&quot;, 
             &quot;What's the largest city?&quot;]
             
predictions = run_prediction(questions, context)

# Print results
for key in predictions.keys():
  print(predictions[key])
</code></pre>
<p>The received error:
<a href=""https://i.sstatic.net/sNUiz.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/sNUiz.png"" alt=""enter image description here"" /></a></p>
<p>Any ideas how to solve this issue?</p>
<p>(this code part mostly is based on this notebook:
<a href=""https://colab.research.google.com/github/spark-ming/albert-qa-demo/blob/master/Question_Answering_with_ALBERT.ipynb"" rel=""nofollow noreferrer"">https://colab.research.google.com/github/spark-ming/albert-qa-demo/blob/master/Question_Answering_with_ALBERT.ipynb</a>)</p>
","nlp"
"94858","How to write a simple rule-based datetime range parser in python?","2021-05-24 21:39:16","","2","142","<nlp><regex><parsing>","<p>The dateparser package fails to detect texts like the following and generate a date range 'last 2 weeks of 2020': Should return 18th December 2020 - 31st December 2020 'first three quarters of 2018': Should return 1st January 2018 - 30th September 2018 'last 3 days of September 2020': Should return 28th September - 30th September 2020</p>
<p>Is there a simple rule-based parser in python that can detect the words last, first, 2, 3, 2020, 2018 etc. and give a date range in the datetime format? I do not want to rely on heavy packages like SUtime to achieve this.</p>
","nlp"
"94826","Modeling of topics orthogonal to a given patterns","2021-05-24 06:09:48","","1","33","<nlp><topic-model><lda>","<p>How to force the topics to be different from the defined ones?</p>
<p>Suppose I have a collection of texts about cats and dogs.There should naturally be two topics: one about dogs and one about cats. But I'm not interested in such results, I'd like to get topics that are completely different (let's call them orthogonal). In other words I would like to use topic modeling to discover a different division of documents (but I don't know what). I thought about guided LDA, but in my opinion it is not applicable in this case (maybe I'm wrong?).</p>
<p>One idea is to remove from the corpus the words that make up the topic about cats and about dogs. I can imagine such a solution in such a way that I run the algorithm to model topics and in the obtained results I mark which topics are not interesting for me and run the algorithm again but already conditioned with this information.</p>
<p>Do you think this is a good idea? Or are there any methods designed for this?</p>
<p>I also found a paper <a href=""http://www.cs.cmu.edu/%7Egzheng/papers/p907-yao.pdf"" rel=""nofollow noreferrer"">Probabilistic Text Modeling with Orthogonalized Topics</a>, in which the concept of orthogonal themes appears, but it is used in a different context than I want.</p>
","nlp"
"94803","LSTM Text Generation with Pytorch","2021-05-23 12:48:53","","1","718","<python><nlp><lstm><pytorch><text-generation>","<p>I am currently trying quote generation (character level) with LSTMs using Pytorch. I am currently facing some issues understanding exactly how the hidden state is implemented in Pytorch.</p>
<p><strong>Some details:</strong></p>
<p>I have a list of quotes from a character in a TV series. I am converting those to a sequence of integers with each character corresponding to a certain integer by using a dictionary <code>char2idx</code>. I also have the inverse of this <code>idx2char</code> where the mapping is reversed.</p>
<p>After that, I am using a sliding window, say of size <code>window_size</code>, and a step of size <code>step</code> to prepare the data.</p>
<p>As an example, let's say the sequence is <code>[1, 2, 3, 4, 5, 0]</code> where 0 stands for the EOS character. Then using <code>window_size = 3</code> and <code>step = 2</code>, I get the sequence for x and y as:</p>
<pre><code>x1 = [1, 2, 3], y1 = [2, 3, 4]
x2 = [3, 4, 5], y1 = [4, 5, 0]

x = [x1, x2], y = [y1, y2]
</code></pre>
<p>The next step is to train the model. I have attached the code I am using to train the model.</p>
<p><em>NOTE:</em> I am not passing hidden states from one batch to the other as the ith sequence of the (j+1)th batch is probably not the next step to the ith sequence from the jth batch. (This is why I am using a sliding window to help the model remember). Is there a better way to do this?</p>
<p>My main question occurs during testing time. There are two methods by which I am testing.</p>
<p><em><strong>Method 1:</strong></em>
I take the initial seed string, pass it into the model and get the next character as the prediction. Now, I add that to the starting string and pass this whole sequence into the model, without passing the hidden state. That is, I input the whole sequence to the model, with the LSTM having the initial hidden state as 0, get the output, append the output to the sequence and repeat till I encounter the EOS character.</p>
<p><em><strong>Method 2:</strong></em>
I take the initial seed string, pass it into the model and get the next character as the prediction. Now, I just pass the character and the previous hidden state as the next input and continue doing so until an EOS character is encountered.</p>
<p><em><strong>Question</strong></em></p>
<ol>
<li>According to my current understanding, the outputs of both methods should be the same because the same thing should be happening in both.</li>
<li>What's actually happening is that both methods are giving completely different results. Why is this happening?</li>
<li>The second one gets stuck in an infinite loop for most inputs (e.g. it gives &quot;back to back to back to ....&quot;) and on some inputs, the first one also gets stuck. How to prevent and avoid this?</li>
<li>Is this related in some way to the training?</li>
</ol>
<p>I have tried multiple different ways (using bidirectional LSTMs, using one hot encoding (instead of embedding), changing the batch sizes, not using a sliding window approach (using padding and feeding the whole quote at once).</p>
<p>I cannot figure out how to solve this issue. Any help would be greatly appreciated.</p>
<p><strong>CODE</strong></p>
<p>Code for the Model Class:</p>
<pre><code>class RNN(nn.Module):
    def __init__(self, vocab_size, hidden_size, num_layers, dropout=0.15):
        super(RNN, self).__init__()
        self.vocab_size = vocab_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        self.embedding = nn.Embedding(vocab_size, hidden_size)
        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, dropout=dropout, batch_first=True)
        self.dense1 = nn.Linear(hidden_size, hidden_size*4)
        self.dense2 = nn.Linear(hidden_size*4, hidden_size*2)
        self.dense3 = nn.Linear(hidden_size*2, vocab_size)
        self.drop = nn.Dropout(dropout)
        
    def forward(self, X, h=None, c=None):
        if h is None:
            h, c = self.init_hidden(X.size(0))
        out = self.embedding(X)
        out, (h, c) = self.lstm(out, (h, c))
        out = self.drop(out)
        out = self.dense1(out.reshape(-1, self.hidden_size)) # Reshaping it into (batch_size*seq_len, hidden_size)
        out = self.dense2(out)
        out = self.dense3(out)
        return out, h, c
        
    def init_hidden(self, batch_size):
        num_l = self.num_layers
        hidden = torch.zeros(num_l, batch_size, self.hidden_size).to(DEVICE)
        cell = torch.zeros(num_l, batch_size, self.hidden_size).to(DEVICE)
        return hidden, cell
</code></pre>
<p>Code for training:</p>
<pre><code>rnn = RNN(VOCAB_SIZE, HIDDEN_SIZE, NUM_LAYERS).to(DEVICE)
optimizer = torch.optim.Adam(rnn.parameters(), lr=1e-3)
criterion = nn.CrossEntropyLoss()

rnn.train()
history = {}
best_loss = 100

for epoch in range(EPOCHS): #EPOCH LOOP
    counter = 0
    epoch_loss = 0
    
    for x, y in train_loader: #BATCH LOOP
        optimizer.zero_grad()
        counter += 1

        o, h, c = rnn(x)
        loss = criterion(o, y.reshape(-1))   
        epoch_loss += loss.item()
        
        loss.backward()
        nn.utils.clip_grad_norm_(rnn.parameters(), 5) # Clipping Gradients
        optimizer.step()

        if counter%print_every == 0:
            print(f&quot;[INFO] EPOCH: {epoch+1}, BATCH: {counter}, TRAINING LOSS: {loss.item()}&quot;)
    
    epoch_loss = epoch_loss/counter       
    history[&quot;train_loss&quot;] = history.get(&quot;train_loss&quot;, []) + [epoch_loss]
    print(f&quot;\nEPOCH: {epoch+1} COMPLETED!\nTRAINING LOSS: {epoch_loss}\n&quot;)     

</code></pre>
<p>Method 1 Code:</p>
<pre><code>with torch.no_grad():
    w = None
    start_str = &quot;Hey, &quot;
    x1 = quote2seq(start_str)[:-1]

    while w != EOS_TOKEN:
        x1 = torch.tensor(x1, device=DEVICE).unsqueeze(0)
        o1, h1, c1 = rnn(x1)
        p1 = F.softmax(o1, dim=1).detach()
        q1 = np.argmax(p1.cpu(), axis=1)[-1].item()
        w = idx2char[q1]
        start_str += w
        x1 = x1.tolist()[0]+ [q1]
    
quote = start_str.replace(&quot;&lt;EOS&gt;&quot;, &quot;&quot;)
quote
</code></pre>
<p>Method 2 Code:</p>
<pre><code>with torch.no_grad():
    w = None
    start_str = &quot;Are we back&quot;
    x1 = quote2seq(start_str)[:-1]
    h1, c1 = rnn.init_hidden(1)

    while w != EOS_TOKEN:
        x1 = torch.tensor(x1, device=DEVICE).unsqueeze(0)
        h1, c1 = h1.data, c1.data
        o1, h1, c1 = rnn(x1, h1, c1)
        p1 = F.softmax(o1, dim=1).detach()
        q1 = np.argmax(p1.cpu(), axis=1)[-1].item()
        w = idx2char[q1]
        start_str += w
        x1 = [q1]
    
quote = start_str.replace(&quot;&lt;EOS&gt;&quot;, &quot;&quot;)
quote
<span class=""math-container"">```</span>
</code></pre>
","nlp"
"94800","Machine learning algorithms for suggesting new baby names","2021-05-23 09:13:24","","1","133","<machine-learning><nlp><machine-learning-model><algorithms><ai>","<p><a href=""https://www.lexalytics.com/lexablog/machine-learning-natural-language-processing"" rel=""nofollow noreferrer"">https://www.lexalytics.com/lexablog/machine-learning-natural-language-processing</a></p>
<p><a href=""https://towardsdatascience.com/named-entity-recognition-ner-meeting-industrys-requirement-by-applying-state-of-the-art-deep-698d2b3b4ede"" rel=""nofollow noreferrer"">https://towardsdatascience.com/named-entity-recognition-ner-meeting-industrys-requirement-by-applying-state-of-the-art-deep-698d2b3b4ede</a></p>
<p><a href=""https://medium.com/mysuperai/what-is-named-entity-recognition-ner-and-how-can-i-use-it-2b68cf6f545d"" rel=""nofollow noreferrer"">https://medium.com/mysuperai/what-is-named-entity-recognition-ner-and-how-can-i-use-it-2b68cf6f545d</a></p>
<p><a href=""https://blog.skyl.ai/understanding-deep-learning-algorithms-for-ner"" rel=""nofollow noreferrer"">https://blog.skyl.ai/understanding-deep-learning-algorithms-for-ner</a></p>
<p><a href=""https://www.bounty.com/pregnancy-and-birth/baby-names/baby-name-search"" rel=""nofollow noreferrer"">https://www.bounty.com/pregnancy-and-birth/baby-names/baby-name-search</a></p>
<p>Can Machine learning algorithms assist in suggesting new Baby names?</p>
<p>Separate input dataset for all the male and female existing names.</p>
<p>Separate input dataset for male and female gender names.</p>
<p>Entity Category : Person First Name.</p>
<p>Note : Every Person Name has a meaning.</p>
<p>Thanks &amp; Regards,</p>
<p>Prashant S Akerkar</p>
","nlp"
"94788","How to apply TFIDF in structured dataset in Python?","2021-05-22 18:21:05","","3","389","<python><nlp><tfidf>","<p>I know that TFIDF is an NLP method for feature extraction.</p>
<p>and I know that there are libraries that calculate TFIDF directly from the text.</p>
<p>This is not what I want though</p>
<p>In my case, my text dataset has been converted into Bag of words</p>
<p>The original dataset that I &quot;DO NOT&quot; have access to, looks like this</p>
<pre><code>RepID     RepText
------------------
1         Doctor sys patient has diabetes and needs rest for ...
2         Patients history: broken arm, and ...
3         A dose of Metformin 2 times a day ...
4         Xray needed for the chest...
5         Covid-19 expectation and patient should have a rest ...
</code></pre>
<p>But my dataset looks like this</p>
<pre><code>RepID   Word         BOW
-------------------------
1       Doctor       3
1       diabetes     4
1       patient      1
.       .            .
.       .            .
2       patient      2
2       arm          7
.       .            .
.       .            .
5684    cough        9
5684    Xray         3
5684    Covid        5
.       .            .
.       .            .
</code></pre>
<p>What I want is to find TFIDF for each word in my dataset.</p>
<p>I was thinking of converting my dataset into a unstructured format</p>
<p>so it would look like this</p>
<pre><code>RepID     RepText
------------------
1         Doctor Doctor Doctor diabetes diabetes diabetes diabetes ...
2         Patients patients arm arm arm arm arm arm arm ...
.
.
5684      cough cough cough cough cough cough cough cough cough Xray Xray
</code></pre>
<p>so each word repeated the same number of BOW</p>
<p>but I do not think this is the best way to do as I convert a structured dataset into an unstructured one..</p>
<p>How to find the TFIDF from the structured dataset? is there a library or algorithm for that?</p>
<p>Note :</p>
<p>Dataset stored in MS SQL Server, and I am using Python code.</p>
","nlp"
"94785","Spacy dependencymatcher pattern not returning matches","2021-05-22 16:55:10","","2","89","<python><nlp><spacy>","<p>I am trying to create, add and get results from a pattern using spacy DependencyMatcher.</p>
<p>I created a pattern for the sentence: &quot;From Monday to Friday&quot;</p>
<p>The full pattern:</p>
<pre><code>pattern = [
    {
        &quot;RIGHT_ID&quot;: &quot;node0&quot;,
        &quot;RIGHT_ATTRS&quot;: {'DEP': 'ROOT', 'POS': 'ADP', 'TAG': 'IN'}
    },
    {
        &quot;LEFT_ID&quot;: &quot;node0&quot;,
        &quot;REL_OP&quot;: &quot;&gt;&quot;,
        &quot;RIGHT_ID&quot;: &quot;node1&quot;,
        &quot;RIGHT_ATTRS&quot;: {'DEP': 'pobj', 'POS': 'PROPN', 'TAG': 'NNP'},
    },
    {
        &quot;LEFT_ID&quot;: &quot;node1&quot;,
        &quot;REL_OP&quot;: &quot;$--&quot;,
        &quot;RIGHT_ID&quot;: &quot;node2&quot;,
        &quot;RIGHT_ATTRS&quot;: {'DEP': 'prep', 'POS': 'ADP', 'TAG': 'IN'},
    },
       {
        &quot;LEFT_ID&quot;: &quot;node2&quot;,
        &quot;REL_OP&quot;: &quot;&gt;&quot;,
        &quot;RIGHT_ID&quot;: &quot;node3&quot;,
        &quot;RIGHT_ATTRS&quot;:{'DEP': 'pobj', 'POS': 'PROPN', 'TAG': 'NNP'},
    },
    
]
</code></pre>
<p>The simpler pattern is :</p>
<pre><code>pattern = [
    {
        &quot;RIGHT_ID&quot;: &quot;node0&quot;,
        &quot;RIGHT_ATTRS&quot;: {&quot;POS&quot;: &quot;ADP&quot;}
    },
    {
        &quot;LEFT_ID&quot;: &quot;node0&quot;,
        &quot;REL_OP&quot;: &quot;&gt;&quot;,
        &quot;RIGHT_ID&quot;: &quot;node1&quot;,
        &quot;RIGHT_ATTRS&quot;: {&quot;POS&quot;: &quot;PROPN&quot;},
    },
    {
        &quot;LEFT_ID&quot;: &quot;node1&quot;,
        &quot;REL_OP&quot;: &quot;$--&quot;,
        &quot;RIGHT_ID&quot;: &quot;node2&quot;,
        &quot;RIGHT_ATTRS&quot;: {&quot;POS&quot;: &quot;ADP&quot;},
    },
       {
        &quot;LEFT_ID&quot;: &quot;node2&quot;,
        &quot;REL_OP&quot;: &quot;&gt;&quot;,
        &quot;RIGHT_ID&quot;: &quot;node3&quot;,
        &quot;RIGHT_ATTRS&quot;:{'POS': 'PROPN'},
    },
    
]
</code></pre>
<p><a href=""https://i.sstatic.net/pp356.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/pp356.png"" alt=""enter image description here"" /></a></p>
<p>My question is, why is this pattern not giving any matches, not on the full or simpler pattern?</p>
<pre><code>import spacy
from spacy.matcher import DependencyMatcher


nlp = spacy.load(&quot;en_core_web_sm&quot;)
matcher = DependencyMatcher(nlp.vocab)


text=&quot;From monday to friday&quot;
doc = nlp(text)
matcher.add(&quot;pattern1&quot;, [pattern])

matches = matcher(doc)

# Each token_id corresponds to one pattern dict
match_id, token_ids = matches[0]
</code></pre>
<p>spacy versions:</p>
<p>spaCy v3.0.6</p>
<p>NAME             SPACY            VERSION</p>
<p>en_core_web_sm   &gt;=3.0.0,&lt;3.1.0   3.0.0   ✔</p>
","nlp"
"94784","How to classify a set of words into one of the given labels","2021-05-22 16:30:48","94807","2","45","<classification><nlp>","<p>I have three labels: amusement, calm and energetic.
I get sets of words like:
Set1 = {Cloud
Sky
People in nature
Plant
Flash photography
Happy
Shorts
Grass
Leisure
Recreation}
Set2 = {Plant
Green
Natural landscape
Natural environment
Branch
Tree
People in nature
Shade
Wood
Deciduous}
I want to classify these group of words into one of the labels. What do you guys think? Set1 should be labelled energetic and Set2 should be labelled calm.</p>
","nlp"
"94716","How does sklearn's tf-idf vectorizer pick the bigrams and trigrams?","2021-05-21 06:13:20","","2","508","<machine-learning><nlp><tfidf>","<pre><code>class sklearn.feature_extraction.text.TfidfVectorizer(
*, input='content', encoding='utf-8', decode_error='strict', strip_accents=None, 
lowercase=True, preprocessor=None, tokenizer=None, analyzer='word', 
stop_words=None, token_pattern='(?u)\b\w\w+\b', **ngram_range=(1, 1)**, 
**max_df=1.0**, **min_df=1**, max_features=None, vocabulary=None, binary=False, 
dtype=&lt;class 'numpy.float64'&gt;, norm='l2', use_idf=True, smooth_idf=True, 
sublinear_tf=False)
</code></pre>
<p>These are the hyperparameters of sklearn's TF-IDF vectorizer.</p>
<p>The ngram_range parameter takes in the possible range of ngrams to extract from the given documents.</p>
<p>I couldn't help but wonder how does the vectorizer come up with the possible ngrams (say bigrams or trigrams) for the documents supplied as word vector features. Does it use only min_df and max_df to come up with them or is there something deeper than what meets the eye going on underneath?</p>
","nlp"
"94690","How to predict the sentiment of the entities form the tweet?","2021-05-20 14:53:42","","0","343","<nlp><sentiment-analysis><language-model><spacy><stanford-nlp>","<p>I have a JSON file (tweets.json) that contains tweets (sentences) along with the name of
the author.</p>
<p>Objective 1: Get the most frequent entities from the tweets.
Objective 2: Find out the sentiment/polarity of each author towards each of the entities.</p>
<pre><code>Sample Input:
Assume we have only 3 tweets:
Tweet1 by Author1: Pink Pearl Apples are tasty but Empire Apples are not.
Tweet2 by Author2: Empire Apples are very tasty.
Tweet3 by Author3: Pink Pearl Apples are not tasty.
</code></pre>
<p>Sample output:
Entities in the topics extracted: Share a dictionary with extracted entities as keys and the number of
tweets an entity has mentioned as values.</p>
<pre><code>Example dictionary for the above example: {“Pink Pearl Apples” : 2,
“Empire Apples” : 2}
</code></pre>
<p>Now, objective 2 -</p>
<p><a href=""https://i.sstatic.net/7bMlS.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/7bMlS.png"" alt=""enter image description here"" /></a></p>
<p>Now using Count Vectorizer I have completed objective 1, now how to get the sentiment based on the entities(Objective 2)</p>
<p>Thanks in Advance!!!</p>
","nlp"
"94619","NLP in Turkish with a DV that is a continuous number","2021-05-18 17:53:54","","1","5","<machine-learning><nlp>","<p>I know how to do NLP in R. Specifically, I know how to train a set of reviews on ratings and then predict ratings for a test set of reviews when the ratings are two-dimensional, i.e. 0 or 1. How can I do the same in Turkish and when my ratings are 5-dimensional, i.e. 1, 2, 3, 4, 5?</p>
<p>Thanks for your helps</p>
","nlp"
"94617","LIterature on query generation from a labelled document term matrix","2021-05-18 17:31:12","","1","8","<nlp><text-filter><document-term-matrix>","<p>I have a labelled dataset of relevant and non-relevant documents for which I built a boolean document term matrix.</p>
<p>I am trying to develop an algorithm which given this input would create a text-based boolean search rule which identifies a subset of the data favouring first of all sensitivity and then specificity.</p>
<p>I'd like to know published literature on the topic. I made some initial search but couldn't find anything related. I'd be glad if you can point me to relevant papers/authors.</p>
","nlp"
"94601","Testing Spacy NER model","2021-05-18 13:39:09","","3","1961","<nlp><named-entity-recognition><spacy>","<p>I've trained an NER model with the use of Spacy, and I would like to test the accuracy on a test dataset. What would be the best way to perform this?</p>
","nlp"
"94582","Do generative model produce varying outputs for same input","2021-05-18 00:23:13","94595","1","647","<deep-learning><nlp>","<p>I am new to data sciences. I believe the generative model generate responses on-the-fly for a valid user input. Is it correct to assume that such models would generate different responses for the same question?</p>
<p>For e.g: if we trained the model on say medical data. Now if user 1 asked &quot;what is fever&quot; and user 2 asked the same question, could be that user 1 and 2 will receive different answers? if this is so then how to circumvent this problem?</p>
<p>thanks in advance</p>
","nlp"
"94581","Recommendation System with ALS Implicit","2021-05-18 00:03:00","","1","143","<python><deep-learning><nlp><recommender-system><kaggle>","<p>I created a model for Recommending top 10 items to users similar to the approach used here <a href=""https://github.com/benfred/implicit/blob/master/examples/lastfm.py"" rel=""nofollow noreferrer"">https://github.com/benfred/implicit/blob/master/examples/lastfm.py</a></p>
<p>I wanted to evaluate the model using NDCG Metric. But I am not sure how to implement it. Can somebody help with it?</p>
","nlp"
"94566","Cluster words into groups of similar meaning (synonyms)","2021-05-17 15:51:30","","2","1468","<nlp><clustering><word-embeddings><text><semantic-similarity>","<p><strong>How can words be clustered into groups of similar meaning (synonyms)?</strong></p>
<p>I started with <strong>pre-trained word embeddings</strong> (e.g., <a href=""https://code.google.com/archive/p/word2vec/"" rel=""nofollow noreferrer"">Google News</a>), which is great, but not perfect - a limitation arises because the word embeddings are based on surrounding words. This introduces challenging results. For example:</p>
<ul>
<li><strong>polar meanings</strong>: word embeddings might find opposites to be similar. Even though these words mean the opposite semantically, they can quite readily be interchanged given the same preceding and following words. For example, &quot;<em>terrible</em>&quot; and &quot;<em>fantastic</em>&quot; are easily interchangeable in the following sentence: &quot;<em>I heard some ____ news today</em>&quot;. Another example is &quot;<em>happy</em>&quot; and &quot;<em>unhappy</em>&quot; in the following sentence: &quot;<em>I was very ____ with the service</em>&quot;.</li>
<li><strong>euphemisms</strong>: word embeddings may find euphemistic words and doublespeak to be similar. Even though these words are perceived very differently, they can still readily be interchanged given the same preceding and following words. For example, &quot;<em>headstrong</em>&quot; and &quot;<em>stubborn</em>&quot; are easily interchangeable in the following sentence: &quot;<em>He is a ____ colleague</em>&quot;. Other examples are: &quot;<em>pamper/spoil</em>&quot;, &quot;<em>check out/perv</em>&quot;, etc.</li>
</ul>
<p>Is there a way to separate similar words (in a word-embeddings sense) into more <strong>semantic clusters</strong>?</p>
<hr />
<p><strong>Possible solution?</strong> Make one extra dimension that puts the word on a <strong>sentiment spectrum</strong>? This one dimension might be enough to sufficiently separate &quot;terrible&quot; from &quot;fantastic&quot;, and &quot;happy&quot; from &quot;unhappy&quot;, etc.</p>
<p>How might such a feature be engineered?</p>
<p>Note that many words would be &quot;neutral&quot; and so this additional dimension wouldn't really affect them. For example, &quot;<em>Python/R/C++/Java</em>&quot; in the following sentence: &quot;<em>5+ years experience of ____ required</em>&quot;.</p>
","nlp"
"94561","Same model/same data different results in Keras/TF","2021-05-17 14:54:43","","1","143","<keras><nlp>","<p>So first to mention I used for all models the <strong>identical dataset</strong> (just in different shapes).</p>
<p>I started with a binary classification based on text following the tutorial from <a href=""https://keras.io/examples/nlp/text_classification_from_scratch/"" rel=""nofollow noreferrer"">https://keras.io/examples/nlp/text_classification_from_scratch/</a>. The results are quite promising on my dataset at ~80%. Dataset were .txt files generated from the CSV.
<a href=""https://i.sstatic.net/cVySL.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/cVySL.png"" alt=""enter image description here"" /></a></p>
<p>Further I wanted to add additional structured data to increase the accuracy following this tutorial (<a href=""https://www.tensorflow.org/tutorials/load_data/csv#mixed_data_types"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/load_data/csv#mixed_data_types</a>). But the results were quite bad at ~50%. I then adapted the layers from the first tutorial but the results are still bad. Dataset was CSV directly.
<a href=""https://i.sstatic.net/rk6EI.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/rk6EI.png"" alt=""enter image description here"" /></a></p>
<p>Now I removed all structured data from the CSV and just left the text in it but the results are not similar. Why? Dataset was CSV directly.
<a href=""https://i.sstatic.net/vECjr.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/vECjr.png"" alt=""enter image description here"" /></a></p>
<pre><code>import pandas as pd
import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras.layers.experimental import preprocessing
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
import numpy as np
from typing import Union
from tensorflow.python.keras.engine.keras_tensor import KerasTensor

titanic = pd.read_csv(&quot;Dataset.csv&quot;)
titanic.head()

titanic_features = titanic.copy()
titanic_labels = titanic_features.pop('survived')

inputs = {}

for name, column in titanic_features.items():
  dtype = column.dtype
  if dtype == object:
    dtype = tf.string
  else:
    dtype = tf.float32

  inputs[name] = tf.keras.Input(shape=(1,), name=name, dtype=dtype)
  

numeric_inputs = {name:input for name,input in inputs.items()
                  if input.dtype==tf.float32}

x = layers.Concatenate(name='ConcatNumeric')(list(numeric_inputs.values()))

norm = preprocessing.Normalization(name='PrepNormalization')


norm.adapt(np.array(titanic[numeric_inputs.keys()]))
all_numeric_inputs = norm(x)


# Not adding numeric values to dataset
preprocessed_inputs = []


# Model constants.
max_features = 20000
embedding_dim = 128
sequence_length = 500

# Now that we have our custom standardization, we can instantiate our text
# vectorization layer. We are using this layer to normalize, split, and map
# strings to integers, so we set our 'output_mode' to 'int'.
# Note that we're using the default split function,
# and the custom standardization defined above.
# We also set an explicit maximum sequence length, since the CNNs later in our
# model won't support ragged sequences.
vectorize_layer = TextVectorization(
    max_tokens=max_features,
    output_mode=&quot;int&quot;,
    output_sequence_length=sequence_length,
)

for name, input in inputs.items():
  if input.dtype == tf.float32:
    continue
  
  x = vectorize_layer(input)
  x = layers.Embedding(max_features + 1, embedding_dim)(x)
  x = layers.Dropout(0.5)(x)
  x = layers.Conv1D(128, 7, padding=&quot;valid&quot;, activation=&quot;relu&quot;, strides=3)(x)
  x = layers.Conv1D(128, 7, padding=&quot;valid&quot;, activation=&quot;relu&quot;, strides=3)(x)
  x = layers.GlobalMaxPooling1D(name='GlobalMaxPooling')(x)
  x = layers.Dense(128, activation=&quot;relu&quot;)(x)
  x = layers.Dropout(0.5)(x)
  x = layers.Dense(1, activation=&quot;sigmoid&quot;, name=&quot;predictionasds&quot;)(x)
  preprocessed_inputs.append(x)


titanic_preprocessing = tf.keras.Model(inputs, preprocessed_inputs)

titanic_preprocessing.compile(loss=&quot;binary_crossentropy&quot;, optimizer=&quot;adam&quot;, metrics=[&quot;accuracy&quot;])

titanic_features_dict = {name: np.array(value) 
                         for name, value in titanic_features.items()}

titanic_preprocessing.fit(x=titanic_features_dict, y=titanic_labels, epochs=3)

tf.keras.utils.plot_model(model = titanic_preprocessing , rankdir=&quot;LR&quot;, dpi=72, show_shapes=True)
</code></pre>
","nlp"
"94543","How Sklearn-crfsuit interpret text features","2021-05-17 05:41:54","","1","96","<python><nlp><scikit-learn><word-embeddings><named-entity-recognition>","<p>As we see <a href=""https://sklearn-crfsuite.readthedocs.io/en/latest/tutorial.html#features"" rel=""nofollow noreferrer"">here</a>, to build an NER model we can pass text features (parts of the word, pos tag, structure of the word etc.) to Sklearn-CRF. I was wondering how does this package convert the text features to numerical features? What strategy do they use - any specific type of embedding?</p>
","nlp"
"94517","Can the attention mask hold values between 0 and 1?","2021-05-16 12:47:40","","2","743","<machine-learning><nlp><attention-mechanism><imbalanced-data>","<p>I am new to attention-based models and wanted to understand more about the attention mask in NLP models.</p>
<blockquote>
<p><code>attention_mask</code>: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices
selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max
input sequence length in the current batch. It's the mask that we typically use for attention when
a batch has varying length sentences.</p>
</blockquote>
<p>So a normal attention mask is supposed to look like this, for a particular sequence of length 5 (with last 2 tokens padded) --&gt; <code>[1,1,1,0,0]</code>.</p>
<p>But can we have attention mask like this --&gt; <code>[1, 0.8, 0.6, 0, 0]</code> where values would be <strong>between</strong> (0 and 1) to indicate that we want to pay attention to those tokens, but it's result wouldn't be completely effective on the model's result due to it's lower attention weights (kinda of like dealing with class imbalance where we weight out certain classes to deal with imbalance).</p>
<p>Is this approach possible? is there some other way to have the model not use the information presented by some tokens completely?</p>
","nlp"
"94438","How to know the state-of-the-art recommended approaches for data science?","2021-05-14 15:18:14","94458","1","188","<machine-learning><nlp><data-mining><visualization><data-science-model>","<p>Data science, AI, NLP, and visualization are changing so fast. I wonder if there is a way/blog that shares the latest updates and recommended approach using certain techniques or avoid using others. For example, many NLP books are old and they would provide examples using TF-IDF. However, nowadays there are much better approaches but they are also changing fast. I am hoping to find some source that would say use these new techniques and avoid using those old techniques. Searching the web can help, but will bring back a lot of noise.</p>
","nlp"
"94401","Python Text Classification - Data that does not fit into any category","2021-05-13 18:27:31","","1","530","<python><classification><nlp><text-classification>","<p>I am having a lot of trouble finding any kind of answers to this problem i am facing.</p>
<p>I have a few text classifiers that i am testing out, and they work well for data that does fit into any predefined category, but if I input, lets just say &quot;fhjakdlfsah&quot;, it will still assign it to some category because i guess the predict_proba functionality has to add up to 1 for all of the categories.</p>
<p>Is there something i am missing here? I am having such a hard time finding a solution to this, and I would imagine it is a very common thing to deal with. Right now i am working with gradientboosting from sklearn, and tried wrapping it in a onevsrestclassifier as suggested by others, but it is still having the same thing where all probabilities are adding up to one, and it is getting assigned the highest probability</p>
<p>Basically I am looking for a solution that can say either, yes this fits into one of these categories, or no, this does not fit into any of these categories.</p>
<p>Any help would be greatly appreciated as I am getting quite stuck here</p>
<p>Thanks!</p>
","nlp"
"94384","What happens when the vocab size of an embedded layer is larger than the text corpus used in training?","2021-05-13 10:00:39","94387","4","2209","<neural-network><keras><nlp><tensorflow><word-embeddings>","<p>Full disclosure this question is based on following this tutorial: <a href=""https://tinyurl.com/vmyj8rf8"" rel=""nofollow noreferrer"">https://tinyurl.com/vmyj8rf8</a></p>
<p>I am trying to fully understand embedded layers in Keras. Imagine having a network to try and understand basic sentiment analysis as a binary classifier (1 positive sentiment and 0 negative sentiment). The toy dataset for this is as follows:</p>
<pre><code># Define 10 restaurant reviews
reviews =[
          'Never coming back!',
          'horrible service',
          'rude waitress',
          'cold food',
          'horrible food!',
          'awesome',
          'awesome services!',
          'rocks',
          'poor work',
          'couldn\'t have done better'
]#Define labels
labels = array([1,1,1,1,1,0,0,0,0,0])
</code></pre>
<p>This data can be used to train a really simple network as follows:</p>
<pre><code>Vocab_size = 50
model = Sequential()
embedding_layer = Embedding(input_dim=Vocab_size,output_dim=8,input_length=max_length)
model.add(embedding_layer)
model.add(Flatten())
model.add(Dense(1,activation='sigmoid'))
model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['acc'])
print(model.summary())
</code></pre>
<p>In order to feed this data into he network, we can one hot encode it using Keras one_hot as follows:</p>
<pre><code>encoded_reviews = [one_hot(d,Vocab_size) for d in reviews]
print(f'encoded reviews: {encoded_reviews}')
</code></pre>
<p>We get the following output:</p>
<pre><code>encoded reviews: [[14, 45, 43], [8, 2], [6, 43], [24, 1], [8, 1], [11], [11, 21], [16], [34, 40], [2, 25, 36, 15]]
</code></pre>
<p>I understand that the purpose of setting Vocab_size = 50, even though there are only around 20 unique words in the corpus is to give a large enough hashing space for the hashing algorithm behind one_hot to avoid collisions when the text is encoded.</p>
<p>If I train the model on these words (assume fixed length input and padding) and then get the weights of the embedded layer:</p>
<pre><code>print(embedding_layer.get_weights()[0].shape)

(50, 8)
</code></pre>
<p>We can see this it is an array of 50 vectors that look like this as an example:</p>
<p>[ 0.17051394  0.13659576 -0.05245572 -0.12567708  0.06743167  0.05893507
-0.14506021  0.06448647]</p>
<p>My understanding is that each of these vectors corresponds to a word embedding for each word in the corpus. But if there are only 20 unique words in the corpus and Vocab_size is set larger than this then that can't be completely true? If Vocab_size &gt; corpus_vocab_size, then what do these embeddings represent? Any help would be appreciated.</p>
","nlp"
"94375","Custom POS tagger for health issues","2021-05-12 22:23:53","","0","101","<nlp><recommender-system><named-entity-recognition>","<p>I am new to NLP, I have a bunch of raw data that is not tagged at all of medical questions, I need to extract from them what are the health issues stated in those texts.</p>
<p>I was thinking I need to create two custom POS tags for NER:<br />
-the location on the body<br />
-the problem itself</p>
<p>So if someone asked 'my head hurts' it would understand that the location is the head and the problem is that it hurts, but if someone asked 'my skin is red around my abdomen' it would understand that the location is the abdomen and the problem is that the skin is red.</p>
<p>After I extract this data I need to recommend medical articles based on what that user asked.<br />
I have some questions:<br />
1.Am I on the right path?<br />
2.How would you implement it?<br />
3.Do I need a custom pos tag for the location and the health issue or can it be done easier? How would you extract those informations?<br />
4.I guess I have to manually tag questions right?<br />
5.What framework would you use?<br />
6.To create a recommender system I need to extract the same informations from medical articles?<br />
7.How would you create a recommender system?</p>
<p>As I said, I am new to NLP and I didn't decide on the framework yet but the questions are not in english, I have found however on github a WordNet clone and a Named Entity Corpus for my language, so please keep in mind when recommending frameworks.</p>
","nlp"
"94333","GradientBoostingRegressor Text Classifier","2021-05-11 20:42:43","","0","74","<python><nlp><text-classification><boosting>","<p>I am working to build a text classifier using a Boosting method from sklearn. It is performing quite well, at around 97% accuracy on my test data. However, the problem I am seeing is that if I input text that clearly does not fall into a predefined category, it will randomly assign it to a certain classification with a high probability score</p>
<p>For example:</p>
<pre><code>X_train = df_train.text
X_test = df_test.text  
y_train = df_train.label
y_test = df_test.label

boosting = Pipeline([('vect', CountVectorizer()),
                  ('tfidf', TfidfTransformer()),
                  ('boosting', GradientBoostingClassifier()),])
boosting = boosting.fit(df_train.text, df_train.label)
</code></pre>
<p>list of categories -&gt; {'A': fruit, 'B': animal, 'C': car, 'D': person, 'E': dessert, 'F': place}</p>
<pre><code>docs = ['this is not category']

boosting.predict_proba(docs).tolist()
</code></pre>
<p>Output:</p>
<pre><code>[[0.0016872033185414193,
  0.9915417761339475,
  0.0016865302624752719,
  0.0016921961567993337,
  0.0016974399174602914,
  0.001694854210776211]]
</code></pre>
<p>You can see that the second category is receiving a .99 probability when it is clearly not fitting into any of the options. Regardless of what I put through it, could be &quot;fdahsjfkasl&quot; it will return the same probability score for that second category</p>
<p>The model works so well for text that could logically fit into a category (not only performing well on test data, but also on new/random text too), but i need a way to handle text that does not, so that it can be labeled &quot;Not a category&quot; or something like it.</p>
<p>Does anyone have any suggestions?</p>
","nlp"
"94294","Calculating confidence score in NER","2021-05-11 06:49:17","","1","1602","<python><deep-learning><nlp><named-entity-recognition><sequence-to-sequence>","<p>I am working on a problem on Named Entity Recognition. Given a text, my model is detecting the Named Entities and extracting that info for the end-user. Now the ask is end-user needs a confidence score along with the extracted entity. For example, the given text is: <em><strong>XYZ Bank India Limited is a good place to invest your money</strong></em> - Our model is detecting <strong>XYZ Bank</strong> as an Org, but <strong>India</strong> as a Location (which is wrong - the whole <strong>XYZ Bank India Limited</strong> is the name of the organization). Our model also gives a probability score for each token it classifies. But the end-user wants to know the confidence of the model that it did not mistake to detect the subsequent tokens as the parts of the organization name.</p>
<p><em>Question is</em> - how can we efficiently measure that in a given sequence our model is detecting a certain sub-sequence as an Organization name (or a Location or something else) correctly or not? How can we say that it did not miss out on any subsequent or preceding token which actually a part of the named entity (like it missed <em>India Limited</em> in the above example)?</p>
","nlp"
"94293","Recommender System Approaches","2021-05-11 04:18:10","","1","30","<machine-learning><python><classification><nlp><recommender-system>","<p>I have a 4 datasets with user features, item features, user-item rating and User-item link data. I'm trying to build a recommender system to recommend top 10 items to the user by maximizing NDCG as the metric.</p>
<p>So far I've used the user-item rating data with Implicit ALS using sparse matrix approach. I wanted to know how could the other data be trained and analyzed? Any good resources for similar problem?</p>
","nlp"
"94292","What model should i use to extract relation between words","2021-05-11 04:03:19","","0","21","<machine-learning><nlp><word>","<p>I want to create a ML model which would give a score from 0 to 1 which would signify the relation between them.</p>
<p>I know about Relationship Extraction(RE) but that's more related with sentences based relation. Instead i want to input two words and that should output the relation between them and a input dataset being a lot of sentences.</p>
","nlp"
"94235","Is this a tried alternative to word embedding for NLP?","2021-05-10 04:46:00","","0","288","<deep-learning><nlp><text-classification><language-model>","<p>I'm searching for research related to my idea, but apparently cannot articulate it well enough to the search engines to show me what's been published on this.</p>
<p>My idea: in a deep learning context (text classification), instead of inputting the text as integer vectors representing words in a vocabulary, which get substituted with dense vectors (word embeddings), what if instead, you represent each phrase almost like how a vector of &quot;pixels&quot; represent an image (then use 2d convolutions)? For example, each character, digit, and symbol get an integer value, just like how pixels are represented. So <code>abc</code> becomes <code>0, 1, 2</code>. The phrase could be formatted as a rectangle at word boundaries or something, but the gist of it, a phrase is formulated as a 2d array where each element is a character from some character set.</p>
<p>Is this a known/tried approach, and what name does it go by? From its apparent lack of popularity, I assume this formulation is far worse than word embeddings, but I'm curious about it.</p>
","nlp"
"94205","A simple attention based text prediction model from scratch using pytorch","2021-05-09 09:08:24","","3","1243","<nlp><pytorch><sequence-to-sequence><attention-mechanism><language-model>","<p>I first asked this question in codereview SE but a user recommended to post this here instead.</p>
<p>I have created a simple self attention based text prediction model using pytorch. The attention  formula used for creating attention layer is,</p>
<p><a href=""https://i.sstatic.net/198af.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/198af.png"" alt=""enter image description here"" /></a></p>
<p>I want to validate whether the whole code is implemented correctly, particularly my custom implementation of <code>Attention</code> layer.</p>
<p>Full code</p>
<pre><code>import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

import random
random.seed(0)
torch.manual_seed(0)

# Sample text for Training
test_sentence = &quot;&quot;&quot;Thomas Edison. The famed American inventor rose to prominence in the late
19th century because of his successes, yes, but even he felt that these successes
were the result of his many failures. He did not succeed in his work on one of his
most famous inventions, the lightbulb, on his first try nor even on his hundred and
first try. In fact, it took him more than 1,000 attempts to make the first incandescent
bulb but, along the way, he learned quite a deal. As he himself said,
&quot;I did not fail a thousand times but instead succeeded in finding a thousand ways it would not work.&quot; 
Thus Edison demonstrated both in thought and action how instructive mistakes can be. 
&quot;&quot;&quot;.lower().split()

# Build a list of tuples.  Each tuple is ([ word_i-2, word_i-1 ], target word)
trigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2])
            for i in range(len(test_sentence) - 2)]

# print the first 3, just so you can see what they look like
print(trigrams[:3])

vocab = list(set(test_sentence))
word_to_ix2 = {word: i for i, word in enumerate(vocab)}

# Number of Epochs
EPOCHS = 25

# SEQ_SIZE is the number of words we are using as a context for the next word we want to predict
SEQ_SIZE = 2

# Embedding dimension is the size of the embedding vector
EMBEDDING_DIM = 10

# Size of the hidden layer
HIDDEN_DIM = 256

class Attention(nn.Module):
    &quot;&quot;&quot;
    A custom self attention layer
    &quot;&quot;&quot;
    def __init__(self, in_feat,out_feat):
        super().__init__()             
        self.Q = nn.Linear(in_feat,out_feat) # Query
        self.K = nn.Linear(in_feat,out_feat) # Key
        self.V = nn.Linear(in_feat,out_feat) # Value
        self.softmax = nn.Softmax(dim=1)

    def forward(self,x):
        Q = self.Q(x)
        K = self.K(x)
        V = self.V(x)
        d = K.shape[0] # dimension of key vector
        QK_d = (Q @ K.T)/(d)**0.5
        prob = self.softmax(QK_d)
        attention = prob @ V
        return attention

class Model(nn.Module):
    def __init__(self,vocab_size,embed_size,seq_size,hidden):
        super().__init__()
        self.embed = nn.Embedding(vocab_size,embed_size)
        self.attention = Attention(embed_size,hidden)
        self.fc1 = nn.Linear(hidden*seq_size,vocab_size) # converting n rows to 1
        self.softmax = nn.Softmax(dim=1)

    def forward(self,x):
        x = self.embed(x)
        x = self.attention(x).view(1,-1)
        x = self.fc1(x)
        log_probs = F.log_softmax(x,dim=1)
        return log_probs

learning_rate = 0.001
loss_function = nn.NLLLoss()  # negative log likelihood

model = Model(len(vocab),EMBEDDING_DIM,CONTEXT_SIZE,HIDDEN_DIM)
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)

# Training
for i in range(EPOCHS):
    total_loss = 0
    for context, target in trigrams:
        # context, target = ['thomas', 'edison.'] the
        
        # step 1: context id generation
        context_idxs = torch.tensor([word_to_ix2[w] for w in context], dtype=torch.long)

        # step 2: setting zero gradient for models
        model.zero_grad()

        # step 3: Forward propogation for calculating log probs
        log_probs = model(context_idxs)

        # step 4: calculating loss
        loss = loss_function(log_probs, torch.tensor([word_to_ix2[target]], dtype=torch.long))

        # step 5: finding the gradients
        loss.backward()

        #step 6: updating the weights
        optimizer.step()

        total_loss += loss.item()
    if i%2==0:
        print(&quot;Epoch: &quot;,str(i),&quot; Loss: &quot;,str(total_loss))

# Prediction
with torch.no_grad():
    # Fetching a random context and target 
    rand_val = trigrams[random.randrange(len(trigrams))]
    print(rand_val)
    context = rand_val[0]
    target = rand_val[1]
    
    # Getting context and target index's
    context_idxs = torch.tensor([word_to_ix2[w] for w in context], dtype=torch.long)
    target_idxs = torch.tensor([word_to_ix2[w] for w in [target]], dtype=torch.long)
    print(&quot;Acutal indices: &quot;, context_idxs, target_idxs)
    log_preds = model(context_idxs)
    print(&quot;Predicted indices: &quot;,torch.argmax(log_preds))
</code></pre>
","nlp"
"94186","Currency Normalization for Salary Prediction","2021-05-08 18:36:02","","0","452","<machine-learning><deep-learning><nlp><dataset><clustering>","<p>I have a dataset (350k data points) with data of employees across different regions over the last 10 years. The dataset consists of their skills, the region they are in, the industry, their current role, their salary in the respective currency. <br>
After doing some analysis, I have found 60% of the salaries are in SGD, 30% in INR, and the rest are divided across 15 other currencies.
Is it recommended that I have a model for each currency or is there a way I can convert all the currencies to a universal value so I can use all my data points to train? <br>
Currently, I have used the 40% of the points available in SGD to train a random forest model and I have found that the results on the test set are reasonably accurate. For this model, I have considered skills, role, and industry as features and nothing else. Is there any better model I can explore?
Thank you</p>
","nlp"
"94081","What is the learning path for the role of NLP engineer a beginner should follow?","2021-05-06 15:19:09","","1","41","<nlp><beginner><career>","<p>I am data science enthusiast and have interest in NLP. How should I develop my understanding for the domain and prepare for the role of an NLP engineer? What are the must have skills to master NLP? Suggest few good reading resources/online courses</p>
","nlp"
"94068","Effective way to find similarity between utterance(short text) and question(long text)","2021-05-06 12:05:28","","0","48","<nlp><similarity><text>","<p>The challenge I have is a bunch of questions(long text) that are closely matching with an user utterance (short text). I have tried cosine similarity &amp; Tf-iDF, BM25,Jaccard similarity, etc., but none of these give desired results.</p>
<p>For instance, my question list is
a) how do I transfer from XYZ account
b) will I be able to transfer from XYZ account with zero fees
c) how do I receive some money from XYZ account</p>
<p>utterance - &quot;transfer XYZ account&quot; should result in close similarities between all these.</p>
<p>However an utterance - &quot;transfer XYZ account with some fees&quot; should result in b alone (with other two questions having very less similarity scores)</p>
<p>Another challenge that I face is ordering of these entities. From A --&gt; B is treated similarly as B --&gt; A.</p>
","nlp"
"94045","Evaluate Topic Modelling on synthetic data","2021-05-05 20:36:00","","2","50","<python><nlp><topic-model><lda><gensim>","<p>I try to find the <strong>optimal number of topics on a synthetic corpus</strong> (so a list of lists of tokens I generate using various parameters). I, therefore, <strong>know the true number of topics</strong> and the true topics distributions. I believe it is a good way to <strong>test unsupervised methods</strong>. The issue is that I completely fail to find the correct number of topics.</p>
<p>I am using NMF and LDA from gensim with c_v and u_mass coherence scores. It <strong>should be easy game</strong> to find the optimal number of topics so I do not use hyperparameters for tuning. I believe the issue is deeper than that.</p>
<p>The code is available <a href=""https://github.com/seaslug95/topicmodelling"" rel=""nofollow noreferrer"">here</a>.</p>
<p>It is well documented. The script to run is 'myscript.py'. It uses functions in 'mymodule.py'. You just need to install gensim and pandas (see requirements.txt if needed).</p>
<p>Any thoughts?</p>
","nlp"
"94036","BERT Self-Attention layer","2021-05-05 18:11:53","94044","0","347","<nlp><bert>","<p>I am trying to use the <strong>first</strong> individual <a href=""https://github.com/huggingface/transformers/blob/master/src/transformers/models/bert/modeling_bert.py#L213"" rel=""nofollow noreferrer"">BertSelfAttention</a> layer for the BERT-base model, but the model I am loading from <code>torch.hub</code> seems to be different then the one used in hugginface <code>transformers.models.bert.modeling_bert</code>:</p>
<pre><code>import torch, transformers

tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)
torch_model = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-uncased')

inputs = tokenizer.encode_plus(&quot;Hello&quot;, &quot;World&quot;, return_tensors='pt')

output_embedding = torch_model.embeddings(inputs['input_ids'], inputs['token_type_ids'])

output_self_attention = torch_model.encoder.layer[0].attention.self(output_embedding)[0]

# compare output with using the huggingface model directly
bert_self_attn = transformers.models.bert.modeling_bert.BertSelfAttention(torch_model.config) 

# transfer all parameters
bert_self_attn.load_state_dict(torch_model.encoder.layer[0].attention.self.state_dict())
# &lt;All keys matched successfully&gt;

output_self_attention2 = bert_self_attn(output_embedding)[0]
output_self_attention != output_self_attention2 # tensors are not equal?
</code></pre>
<p>Why is <code>output_self_attention2</code> different from <code>output_self_attention</code>? I thought they would give the same output given the same input.</p>
","nlp"
"93942","Human readable format for clusters of word vectors","2021-05-04 05:23:00","","2","43","<nlp><clustering><k-means><word2vec>","<p>Let's say I have pretrained word2vec model and apply it to dataset consisting of article titles from &quot;The Guardian&quot;. It seems pretty obvious that titles coming from &quot;Science&quot; section would form one cluster in latent space and titles from &quot;Fashion&quot; section would form another cluster in latent space. But the thing is my dataset doesn't have category label for each title. How can I come up with such human readable interpretation of cluster centers(probably coming from Kmeans)?</p>
","nlp"
"93931","BERT embedding layer","2021-05-03 20:29:42","93935","3","6705","<nlp><bert>","<p>I am trying to figure how the embedding layer works for the pretrained BERT-base model. I am using pytorch and trying to dissect the following model:</p>
<pre><code>import torch
model = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-uncased')
model.embeddings
</code></pre>
<p>This BERT model has 199 different named parameters, of which the first <strong>5</strong> belong to the embedding layer (the first layer)</p>
<pre><code>==== Embedding Layer ====

embeddings.word_embeddings.weight                       (30522, 768)
embeddings.position_embeddings.weight                     (512, 768)
embeddings.token_type_embeddings.weight                     (2, 768)
embeddings.LayerNorm.weight                                   (768,)
embeddings.LayerNorm.bias                                     (768,)
</code></pre>
<p>As I understand, the model accepts input in the shape of <code>[Batch, Indices]</code> where <code>Batch</code> is of arbitrary size (usually 32, 64 or whatever) and <code>Indices</code> are the corresponding indices for each word in the tokenized input sentence. <code>Indices</code> has a max length of <strong>512</strong>. One input sample might look like this:</p>
<pre><code>[[101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102]]
</code></pre>
<p>This contains only 1 batch and is the tokenized form of the sentence &quot;The quick brown fox jumps over the lazy dog&quot;.</p>
<p>The first <code>word_embeddings</code> weight will translate each number in <code>Indices</code> to a vector spanned in <code>768</code> dimensions (the embedding dimension).</p>
<p>Now, the <code>position_embeddings</code> weight is used to encode the position of each word in the input sentence. Here I am confused about why this parameter is being learnt? Looking at an <a href=""https://github.com/codertimo/BERT-pytorch/blob/master/bert_pytorch/model/embedding/position.py#L11-#L21"" rel=""nofollow noreferrer"">alternative implementation</a> of the BERT model, the positional embedding is a static transformation. This also seems to be the conventional way of doing the positional encoding in a transformer model. Looking at the <em>alternative implementation</em> it uses the sine and cosine function to encode interleaved pairs in the input. I tried comparing <code>model.embeddings.position_embeddings.weight</code> and <a href=""https://github.com/codertimo/BERT-pytorch/blob/master/bert_pytorch/model/embedding/position.py#L21"" rel=""nofollow noreferrer""><code>pe</code></a>, but I cannot see any similarity. The in the last sentence under under A.2 Pre-training Procedure (page 13) the <a href=""https://arxiv.org/pdf/1810.04805.pdf"" rel=""nofollow noreferrer"">paper</a> states</p>
<blockquote>
<p>Then, we train the rest
10% of the steps of sequence of 512 to learn the
positional embeddings.</p>
</blockquote>
<p><em><strong>Why is the positional embedding weight being learnt and not predefined?</strong></em></p>
<p>The next layer after the positional embedding is the <code>token_type_embeddings</code>. Here I am confused about how the segment label is inferred by the model. If I understand this correctly each input sentence is delimited by the <code>[SEP]</code> token. In the example above there is only 1 <code>[SEP]</code> token and the segment label must be <code>0</code> for that sentence. But there could be a maximum of 2 segment labels. If so, will the 2 segments be handled separately or are they processed in parallel all the same as one &quot;array&quot;? <em><strong>How does the model handle multiple sentence segments?</strong></em></p>
<p>finally the output from theese 3 embeddings are added togheter and passed through layernorm which I understand. But, <em><strong>are the weights in these embedding layers adjusted when fine-tuning the model to a downstream task?</strong></em></p>
","nlp"
"93801","Training a model purely on weak labels","2021-05-01 02:15:05","93831","3","328","<machine-learning><nlp><supervised-learning>","<p>I have read a couple of papers now use rules-based system to create weak labels and then train a BERT-based model only using these weak labels. Both studies have reported better performances on manually labelled gold-standard test data.</p>
<p>However, I just don't follow the logic here. I understand distant supervision and all that. It's been around for a while now. I just don't understand if your model (BERT or not) is only trained on these weak labels then you are treating them as &quot;ground-truth&quot;, and more importantly, don't you already know how to create &quot;ground-truth&quot; (by the rules-based system) ??? What's the point of the 2nd step?</p>
<p>Even though the performances on test data are better. It doesn't convince me your ML model (from the 2nd step) have learnt something beyond the weak labels you fed to it.</p>
<p>The only argument I find has some merits is that you basically treat the BERT-based model already as a zero-shot classifier and you are fine-tuning it with weak labels. I am just confused. Can someone pls enlighten me? Am I missing something obvious here?</p>
","nlp"
"93712","Paper/Idea to get the most attractive chapter in a novel","2021-04-29 07:25:46","","0","21","<machine-learning><classification><nlp>","<p>I face a problem as below:</p>
<ol>
<li>There is a novel app and need to find the most attractive chapter in previous 30 chapters of each novel</li>
<li>So that we can limit a user continue unless he make payment</li>
</ol>
<p>I didn't find such paper/idea through google .</p>
<p>What I thought is :</p>
<p>Set different payment point in different chapter, show to random user and record the CTR .
Then we can use it to get payment distribution to set proper payment point .
But we don't have many user now, and cost would be large .</p>
<p>So I wonder is there any NLP way to detect most attractive chapter ?</p>
","nlp"
"93637","Any research on relationship between the dimensions of a (word2Vec) space and how the human mind constructs meaning (or reality) through language?","2021-04-27 18:43:27","93804","0","52","<nlp><word2vec><deepmind>","<p>Neuroscience is still trying to &quot;find&quot; how the mind (and language) somehow &quot;works&quot;. Is there any theory linking a (low-dimensionality) embedding space (like word2Vec) to a mind (linguistic) model? Any <a href=""https://en.wikipedia.org/wiki/Cognitive_linguistics#Cognitive_Linguistics_(linguistics_framework)"" rel=""nofollow noreferrer"">Cognitive Linguistics</a> theory?</p>
","nlp"
"93535","Pytorch: understanding the purpose of each argument in the forward function of nn.TransformerDecoder","2021-04-25 18:33:48","93539","3","3496","<nlp><pytorch><transformer><sequence-to-sequence><text-generation>","<p>According to <a href=""https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoder.html"" rel=""nofollow noreferrer"">https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoder.html</a>, the <a href=""https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoder.html#torch.nn.TransformerDecoder.forward"" rel=""nofollow noreferrer"">forward function</a> of nn.TransformerDecoder contemplates the following arguments:</p>
<ul>
<li><strong>tgt</strong> – the sequence to the decoder (required).</li>
<li><strong>memory</strong> – the sequence from the last layer of the encoder (required).</li>
<li><strong>tgt_mask</strong> – the mask for the tgt sequence (optional).</li>
<li><strong>memory_mask</strong> – the mask for the memory sequence (optional).</li>
<li><strong>tgt_key_padding_mask</strong> – the mask for the tgt keys per batch (optional).</li>
<li><strong>memory_key_padding_mask</strong> – the mask for the memory keys per batch (optional).</li>
</ul>
<p>Unfortunately, Pytorch's official documentation on the function isn't exactly very thorough at this point (April 2021), in terms of the expected dimensions of each tensor and when it does or doesn't make sense to use each of the optional arguments.</p>
<p>For example, <a href=""https://datascience.stackexchange.com/a/93146/46305"">in previous conversations</a> it was explained to me that <strong>tgt_mask</strong> is usually a square matrix used for self attention masking to prevent future tokens from leaking into the prediction of past tokens. Similarly, <strong>tgt_key_padding_mask</strong> is used for masking padding tokens (which happens when you pad a batch of sequences of different lengths so that they can fit into a single tensor). In light of this, it makes total sense to use tgt_mask in the decoder, but I wouldn't be so sure about tgt_key_padding_mask. What would be the point of masking target padding tokens? Isn't it enough to simply ignore the predictions associated to padding tokens during training (say, you could do something like <code>nn.CrossEntropyLoss(ignore_index=PADDING_INDEX)</code> and that's it)?</p>
<p>More generally, and considering that the current documentation is not as thorough as one would like it to be, I would like to know what the purpose is of each argument of nn.TransformerDecoder's forward function, when it makes sense to use each of the optional arguments, and if there are nuances in the usage one should keep in mind when switching between training and inference modes.</p>
","nlp"
"93526","How do I split contents in a text that would include two or more different themes (context) in NLP?","2021-04-25 14:28:36","","1","690","<nlp><text-mining><text-classification><nltk>","<p>For example, a text: &quot;The airlines have affected by Corona since march 2020 a crime has been detected in Noia village this morning&quot;</p>
<p>the output should be:</p>
<ul>
<li>The airline companies have affected by Corona since march 2020</li>
<li>a crime has been detected in Noia village this morning</li>
</ul>
<p>the text has no Breaks. I know it is not a one-click solution, but if anyone knows a methodology or techniques to solve such a problem, please provide me with resources.</p>
","nlp"
"93487","Masked Language Modeling on Domain-specific Data","2021-04-24 08:53:23","","1","475","<deep-learning><nlp><transformer><huggingface>","<p>My goal is to have a language model that understands the relationships between words and can fill the masks in a sentence related to a specific domain. At first, I thought about pretraining or even training a language model(like BERT) from scratch, but unfortunately, my data isn't that big to help the previous model learn new connections, let alone learn the embeddings from scratch.</p>
<p>Now what I have in mind is creating a transformer model with my own vocabulary which consists of words in my domain-specific data (after separating them with spaces and not using transformer tokenizers). This way the vocab size would be smaller and the positions and relations would be learned faster and more easily. Although I'm a bit confused about implementation.</p>
<p>Can I use <a href=""https://www.tensorflow.org/tutorials/text/transformer"" rel=""nofollow noreferrer"">this architecture (that is for NMT)</a> and give plain text for both the input and output? or should I mask some tokens in the input and give the complete sentence as the label?</p>
<p>Any other suggestions?</p>
","nlp"
"93485","complete entity extraction from unstructured data","2021-04-24 05:38:16","","0","37","<nlp><named-entity-recognition><entity-linking>","<p>I understand there are many techniques/libraries/packages to extract named entities like people, places etc. from data.</p>
<p>Personally, for me an entity is something like:</p>
<pre><code>first name: john
surname: smith
dob: 1/1/2000
shoesize: 6
address: ...
</code></pre>
<p>etc.</p>
<p>So an entity is a class having fields, to use object orientated terminology.</p>
<p>One would expect that these fields/attributes would occur close in a unstructured data (closeness could be defined by word distance). Are there techniques to extract, what I would call, complete entities? Of course this would no be too accurate but anything would be better than nothing. I did a few google searches without success.</p>
","nlp"
"93447","Combining textual and numeric features into pre-trained Transformer BERT","2021-04-23 11:07:41","93477","2","2437","<nlp><pytorch><bert><finetuning>","<p>I have a dataset with 3 columns:</p>
<ol>
<li>Text</li>
<li>Meta-data (intending to extract features from it, then use those i.e., numerical features)</li>
<li>Target label</li>
</ol>
<p><strong>Question 1: How can I use a pre-trained BERT instance on more than the text?</strong></p>
<p>One theoretical <a href=""https://datascience.stackexchange.com/questions/54888/how-can-i-add-custom-numerical-features-for-training-to-bert-fine-tuning?rq=1"">solution</a> suggests having BERT fed the text and another neural network with the numerical features fed into this one, then aggregating their output, into another neural network.</p>
<ul>
<li>Is that the most efficient approach?</li>
</ul>
<p><strong>Question 2: How can you connect neural networks?</strong></p>
<ul>
<li><p>You get the output from each, but then what?</p>
</li>
<li><p>You get classification output from BERT, you get classification
output from MLP based on numerical features.</p>
</li>
<li><p>You concatenate these and feed them to another MLP, and you get the
final prediction? Wouldn't that last prediction be less robust ?</p>
</li>
<li><p>In other words, does the last MLP encapsulate the other 2 networks?</p>
</li>
<li><p>If so, what happens if BERT predicts on 90%, but the first MLP just
50%, will we get a lesser outcome?</p>
</li>
</ul>
<p><strong>Question 3: Any tips on how to implement this in pytorch?</strong></p>
","nlp"
"93419","Embedding from Transformer-based model from paragraph or documnet (like Doc2Vec)","2021-04-22 18:47:47","","1","1119","<nlp><bert><transformer><embeddings><doc2vec>","<p>I have a set of data that contains the different lengths of sequences. On average the sequence length is 600. The dataset is like this:</p>
<pre><code>S1 = ['Walk','Eat','Going school','Eat','Watching movie','Walk'......,'Sleep']
S2 = ['Eat','Eat','Going school','Walk','Walk','Watching movie'.......,'Eat']
.........................................
.........................................
S50 = ['Walk','Going school','Eat','Eat','Watching movie','Sleep',.......,'Walk']
</code></pre>
<p>The number of unique actions in the dataset are fixed. That means some sentences may not contain all of the actions.</p>
<p>By using Doc2Vec (Gensim library particularly), I was able to extract embedding for each of the sequences and used that for later task (i.e., clustering or similarity measure)</p>
<p>As transformer is the state-of-the-art method for NLP task. I am thinking if Transformer-based model can be used for similar task. While searching for this technique I came across the <a href=""https://github.com/UKPLab/sentence-transformers"" rel=""nofollow noreferrer"">sentence-Transformer</a>. But it uses a pretrained BERT model (which is probably for language but my case is not related to language) to encode the sentences. Is there any way I can get embedding from my dataset using Transformer-based model?</p>
","nlp"
"93414","BERT MLM overfitting","2021-04-22 16:28:40","","1","489","<nlp><loss-function><optimization><overfitting><bert>","<p>We are training the BERT model on masked language modeling task for the Russian Language. Our dataset consists of 60 mln texts with (128 tokens for each text) from online social networks, predominantly in the Russian language. We have not performed any text preprocessing. We use AdamW optimizer and <a href=""https://huggingface.co/DeepPavlov/rubert-base-cased"" rel=""nofollow noreferrer"">RuBERT</a> as a pre-trained model. The training parameters are the following:</p>
<pre><code>Learning rate: 1**e-5
Warmup: 30 000
Seq. len: 128
</code></pre>
<p>The charts below show the cross-entropy(CE) loss function for test evaluation as iterations go on. One iteration contains 100 batches, One epoch contains ~ 3500 iterations. The blue chart shows loss function when we mask only Cyrillic tokens (Cyrillic BERT). The orange chart shows loss function when any token (including punctuation, URLS, numerals, English letters) can be masked and it is the only difference among the two charts (Default BERT).</p>
<p><a href=""https://i.sstatic.net/bE3Na.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bE3Na.png"" alt=""perplexity"" /></a></p>
<p>My questions are the following:</p>
<ol>
<li>Why &quot;Cyrillic BERT&quot; overfits dramatically after the ~2200 iteration while the &quot;Default BERT&quot; oscillates near 1.67 at the same time? What Bert config parameters may change it?</li>
<li>Why The &quot;Default BERT&quot; has no improvements does not improve after the ~3000 iterations? How can I improve it? (already trying to change hidden_dropout_prob to 0.2 from 0.1)</li>
<li>Does &quot;Cyrillic BERT&quot; shows worse loss function because it is easier to predict punctuation and several English names?</li>
</ol>
","nlp"
"93344","How to decide to go with BOW or TFIDF","2021-04-21 04:46:40","","1","1418","<nlp><tfidf><bag-of-words>","<p>I know that there are methods that help in selecting features such as Matual Info, and Info Gain, etc.</p>
<p>But for datasets with thousands of records and thousands of features it is time consuming to train the model in BOW and TFIDF to decide which method is better.</p>
<p>is there a way to decide which method to choose without the need to spend all this time?</p>
","nlp"
"93331","Best Python NLP library for supervised topic classification","2021-04-20 20:37:50","","0","733","<python><classification><nlp><topic-model>","<p>I have a labeled dataset that I have ingested into a dataframe. It consists of news articles,</p>
<pre><code>&gt;&gt;&gt; df.columns

Index(['title', 'headline', 'byline', 'dateline', 'text', 'copyright',
       'country', 'industry', 'topic', 'file'],
      dtype='object')
</code></pre>
<p>where the text column contains the body (text) of the article and the topic column contains a list of associated topics.</p>
<p>I want to train a model from this dataset to predict the article topics. I was considering using transformers (<a href=""https://huggingface.co/transformers/index.html"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/index.html</a>) to do this, along with tensorflow, but I from what I know of transformers, it's not really good for this.</p>
<p>What would be the best NLP library to perform this task with high accuracy?</p>
","nlp"
"93224","Word2vec outperforming BERT, possible?","2021-04-18 16:45:11","93280","2","1113","<keras><tensorflow><nlp><word2vec><bert>","<p>I'm trying to solve a multilabel classification (dataset is tweet text) using a combination of BERT and CNN. As a benchmark, I'd compare it to other word embeddings, one of which is Word2vec. After numerous tries, it seems that Word2vec-CNN keeps outperforming BERT-CNN by a slight bit, here's a result from my last try:</p>
<pre><code>Word2vec-CNN
precision (macro): 0.89  
recall (macro): 0.87  
f1-score (macro): 0.88
accuracy (test set): 0.81
hamming loss: 0.062

BERT-CNN
precision (macro): 0.86  
recall (macro): 0.88  
f1-score (macro): 0.87
accuracy (test set): 0.74
hamming loss: 0.073
</code></pre>
<p>Question is:</p>
<ol>
<li>Could it be possible that Word2vec (or any static word embeddings) outperforms BERT (or any contextual word embeddings)? If so, what is the rationale? If there's any research paper on this it would be really helpful.</li>
<li>If not, what could possibly be the cause?</li>
</ol>
<p>FWIW: Model is trained using TensorFlow-Keras (I kind of suspect this is SOMEHOW caused by how TF-Keras calculates its metrics but I still haven't figured out why and, if any, a solution), and both embeddings are pretrained (BERT model was trained on a bigger corpus, around 200:1).</p>
","nlp"
"93169","Calculating optimal number of topics for topic modeling (LDA)","2021-04-17 10:21:37","93175","5","1242","<nlp><data-science-model><topic-model><lda>","<p>am going to do topic modeling via LDA. I run my commands to see the optimal number of topics. The output was as follows: It is a bit different from any other plots that I have ever seen. Do you think it is okay? or it is better to use other algorithms rather than LDA. It is worth mentioning that when I run my commands to visualize the topics-keywords for 10 topics, the plot shows 2 main topics and the others had almost a strong overlap. Is there any valid range for coherence?</p>
<p>Many thanks to share your comments as I am a beginner in topic modeling.</p>
<p><a href=""https://i.sstatic.net/P0unj.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/P0unj.png"" alt=""enter image description here"" /></a></p>
","nlp"
"93161","Training Objective of language model for GPT3","2021-04-17 00:14:58","93167","0","777","<nlp><language-model><gpt>","<p>On page 34 of OpenAI's <a href=""https://arxiv.org/pdf/2005.14165v2.pdf"" rel=""nofollow noreferrer"">GPT-3</a>, there is a sentence demonstrating the limitation of objective function:</p>
<blockquote>
<p>Our current objective weights every token equally and lacks a notion of what is most important to predict and what is less important.</p>
</blockquote>
<p>I am not sure if I understand this correctly. In my understanding, the objective function is to maximize the log-likelihood of the token to predict given the current context, i.e., <span class=""math-container"">$\max L \sim \sum_{i} \log P(x_{i} | x_{&lt;i})$</span>. Although we aim to predict every token that appears in the training sentence, the tokens have a certain distribution based on the appearance in human litterature, and therefore we do not actually assign equal weight to every token in loss optimization.</p>
<p>And what should be an example for a model to get the notion of &quot;what is important and what is not&quot;. What is the importance refer to in here? For example, does it mean that &quot;the&quot; is less important compared to a less common noun, or does it mean that &quot;the current task we are interested in is more important than the scenario we are not interested in ?&quot;</p>
<p>Any idea how to understand the sentence by OpenAI?</p>
","nlp"
"93155","Naives Bayes Text Classifier Confidence Score","2021-04-16 19:33:51","","1","728","<python><nlp><naive-bayes-classifier>","<p>I am experimenting with building a text classifier using Naive Bayes which has been pretty successful on my test data. One thing i am looking to incorporate is handling text that does not fit into any predefined category that I trained the model on.</p>
<p>Does anyone have some thoughts on how to do this? I was thinking of trying to calculate the confidence score for each document, and if &lt; 80 % confidence, for example, it should label the data as &quot;N/A&quot;</p>
<p>This is my code so far:</p>
<pre><code>df_train = pd.read_csv(__________)

text_clf = Pipeline([('vect', CountVectorizer()),
                  ('tfidf', TfidfTransformer()),
                  ('clf', MultinomialNB()),])
text_clf = text_clf.fit(df_train.text, df_train.label)

df['predicted'] = predicted
</code></pre>
<p>Like I said, it works well for documents that do fit into one of the categories, but if I have something that clearly does not fit into anything, it will still try and assign it a label, my guess is based on some kind of confidence calculation but just not sure how that works</p>
","nlp"
"93097","How to filter data samples which do not improve classifier?","2021-04-15 15:32:31","","0","20","<machine-learning><python><nlp><data-cleaning><text-classification>","<p>I have a text dataset with noisy labels and an unbalanced shape.</p>
<p>There are various ways to find features which do not drive improvement in some metric, and help to prune those from the pipeline.</p>
<p>I cannot however find a way to throw away noisy / confusing samples.</p>
<p>One naive approach might be to run the model removing a single sample at a time and see if the metric improves or does not change - if that happens, throw away that sample.</p>
<p>Is there a name for such a technique? Or does it not exist for good reason :)</p>
","nlp"
"93082","Extracting indirect quotations","2021-04-15 09:28:00","","1","191","<nlp>","<p>I found <a href=""https://github.com/chartbeat-labs/textacy"" rel=""nofollow noreferrer""><code>textacy</code></a> Python library, built on top of Spacy, a useful tool to experiment with direct quotation extraction. Unfortunately, <code>textacy</code> that does not support extraction of indirect quotation (according to this <a href=""https://github.com/chartbeat-labs/textacy/issues/293"" rel=""nofollow noreferrer"">issue</a>). So far, I have used rule-based approach that looks for reporting verb and its open clausal complement. However, this is quite custom and home-made solution.</p>
<p>Is there a library, preferably in Python, for such a task?</p>
","nlp"
"93062","What is the common practice for NLP or text mining for non-English?","2021-04-14 18:00:57","","0","44","<nlp><text-mining><bert><pretraining>","<p>A lot of natural language processing tools are pre-trained with corpus in English. What if ones need to analyze, say, <strong>Dutch text</strong>? The blogs I find online are mostly saying <strong>traslating text into English</strong> as pro-processing. Is this the common practice? If not, then what? Also, does <strong>how similar a language is to English</strong> have an impact on the model performance?</p>
<p>For some also widely speaking languages (e.g French, Spanish), do people construct corpus in their own language and train models on it? Forgive my ignorance because I'm not able to read papers in many languages.</p>
","nlp"
"93034","How to JUST represent words as embeddings by pretrained BERT?","2021-04-13 22:03:42","","0","5000","<nlp><unsupervised-learning><word-embeddings><bert><representation>","<p>I don't have enough data (i.e. I don't have enough texts) --- have only around 4k words in my dictionary. I need to compare given words, then I need to representate it as <em>embedding</em>.</p>
<p>After the representation of words I want to clusterize it, find similar vectors (i.e. words). Maybe even then make a <strong>classification</strong> to a given classes (<strong>classification</strong> there unsupervised --- since I don't have labeled data to train on).</p>
<p>I know that almost any task can be solved &quot;inside&quot; <strong>BERT</strong>, i.e. using <em>fine-tuning</em> in final layer.</p>
<p>Since all described above, <strong>I have two QUESTIONS</strong>; answers/hints/anything really appreciated since i'm stuck on that:</p>
<ol>
<li>How to just extract embeddings from BERT using some dictionary of words and use word representations for futher work?</li>
<li>Can we solve inside BERT using fine-tuning the next problem: a). Load dictionary of words into BERT b). Load given classes (words representing each class. E.g. &quot;fashion&quot;, &quot;nature&quot;). c) Make an unsupervised classification task?</li>
</ol>
","nlp"
"93033","Should we also include negative instance in cross-validation process of one-class classifiers?","2021-04-13 21:38:34","","1","25","<nlp><classifier>","<p>For a one-class classifier to do text classification, only positive instances are used for training.</p>
<p>However, in the cross-validation process to select the best hyperparameters, should we also include negative instances to evaluate the classifier? or use only positive instances? What's the tradition?</p>
","nlp"
"93019","Addressing polysemy in NLP tasks","2021-04-13 15:15:24","","0","227","<classification><nlp><topic-model><question-answering>","<p>Looking for modern algorithms using NN Language Model implementations addressing polysemy in NLP tasks, including text classification, question answering and topic modeling. Transfer/Zero-short learning methods are most interesting to find. Any working solutions with BERT and Hugging Face Transformers libraries?</p>
","nlp"
"92949","How to cluster words automatically?","2021-04-12 08:01:49","","0","41","<nlp><clustering><unsupervised-learning>","<p>I have a problem where I have a list of n words with truly k different ones (k is unknown) because some may be malformed or contracted. I would like to automatically cluster them.</p>
<p>I thought about using something like a dendogram with a distance between words (I don't know which), how can this be handled efficiently?</p>
","nlp"
"92880","Problem with mord library function 'Logistic AT'","2021-04-10 22:27:22","","-1","717","<python><nlp>","<p>I wanted to run a an ordinal logistic regression on my bag of words. I have used the code below with logistic regression but now I have modified it for an ordinal logistic regression. However, when I try to predict Y, I keep getting the error:</p>
<pre class=""lang-py prettyprint-override""><code>AttributeError: 'LogisticAT' object has no attribute 'coef_'
</code></pre>
<p>I am pretty sure the error is in the following line, but not sure how to fix it.</p>
<pre class=""lang-py prettyprint-override""><code>ml_model = LogisticAT(alpha = 1.0)
</code></pre>
<pre class=""lang-py prettyprint-override""><code>def train_logisticAT_regression(features, label):
    print (&quot;Training the logistic regression model...&quot;)
    from mord import LogisticIT, LogisticAT, OrdinalRidge
    ml_model = LogisticAT(alpha = 1.0)
    print ('Finished')
    return ml_model

ml_model = train_logisticAT_regression(tfidf_features, y_train)

test_data_features = vectorizer.transform(X_test)
# Convert to numpy array
test_data_features = test_data_features.toarray()

test_data_tfidf_features = tfidf.fit_transform(test_data_features)
# Convert to numpy array
test_data_tfidf_features = test_data_tfidf_features.toarray()

predicted_y = ml_model.predict(test_data_tfidf_features)
</code></pre>
<p>I'm not familiar with the mord package. How can I fix this error?</p>
","nlp"
"92856","The reason behind using a pre-trained model?","2021-04-10 13:06:21","","2","1549","<nlp><word-embeddings><word2vec>","<p>These last month I have been studying all about word embeddings and the most known pre-trained word embeddings, Word2Vec, GloVe, FastText, etc. I have read many times how important It is to take advantage of pre-trained models when doing a given task however I don't understand how a pre-trained model can adapt to my given corpus. Furthermore, If I have new words not present in the pre-trained model will I be able to use this pre-trained model to learn the embeddings for the new words?</p>
","nlp"
"92809","NLP methods specific to a language?","2021-04-09 13:36:47","92821","1","48","<nlp><algorithms>","<p>What NLP methods / algorithms depend on the features existing only in some languages? For example, does French has any NLP algorithms that English NLP and Spanish NLP do not have?</p>
","nlp"
"92740","Word2Vec: Identifying many-to-one relationships between words","2021-04-08 10:23:13","","0","81","<nlp><word-embeddings><word2vec><ai><vector-space-models>","<p>Standard introductory examples in Word2Vec, like <code>king - queen = man - woman</code> and <code>tokyo - japan = london - uk</code>, involve one-to-one relationships between words: Tokyo is the <em>exclusive</em> capital of Japan.</p>
<p>More generally, we might want to test for many-to-one relationships: e.g. we might want to ask if Kyoto is a <em>city</em> in Japan. I presume we are still interested in vectors of the form <code>kyoto - japan</code>, <code>houston - us</code>, etc., but these vectors are no longer <em>equal</em>.</p>
<p>Do these &quot;relationship vectors&quot; form a particularly interesting vector space? Do they sample some known distribution? How can I check a many-to-one relationship from the word embeddings?</p>
","nlp"
"92591","NLP: find the best preposition for connecting parts of a sentence","2021-04-05 17:07:57","","2","181","<nlp><ngrams>","<p>My task is to connect 2-3 parts of the sentence into one whole using a  preposition</p>
<ul>
<li>the first part is some  kind of action. Ex. &quot;take pictures&quot;</li>
<li>the second part is an object  that can consist of  only one noun or a noun with  adjectives and additions dependent on it. Ex. &quot;juicy cherry pie&quot;, &quot;squirrel&quot;</li>
<li>the third part is a place. Ex. &quot;room&quot;, &quot;London&quot;</li>
</ul>
<p>To solve this task I've already tried some options  such as generation using GPT-2 (or other networks like LSTM) and using grammar rules, but both of them didn't work as well as I want.</p>
<p>Using GPT-2 needs  a lot of resources and performance, which is very expensive on the server. Ordinary neural networks aren't good enough and often generate bad results. And it is also impossible to describe all cases using only grammar rules.</p>
<p>Are there any approaches how you can implement a solution to this problem without using transformers or grammatical rules, so that the model isn't too heavy and at the same time effective?</p>
<p>There may be several options for prepositions, the main thing is that they are used.</p>
<p>P.s. a good option was to use n-grams, but it is not clear how to work with it when the second part is long.</p>
<p><strong>Examples:</strong></p>
<p>[Pour, juice, boiler room]     -&gt;  Pour the juice into the boiler room</p>
<p>[Go, theater]                  -&gt;  Go to the theater</p>
<p>[Learn the history, ballet]    -&gt;  Learn the history of ballet</p>
","nlp"
"92561","Determining whether a sentence is ""cliche"" using NLP","2021-04-05 06:29:00","","1","145","<machine-learning><text-mining><nlp>","<p>I have a collection of essays from students. Each essay is about the same topic and of the same word length. My goal is to develop a machine learning algorithm that pinpoints &quot;cliche&quot; sentences (i.e., sentences that are &quot;unoriginal&quot; and similar to what other students have written).</p>
<p>Let document X be the essay we're trying to analyze. Let S be the set of all essays. Here is the rudimentary approach I've thought of:</p>
<ol>
<li>Split all essays (from set S - X) into individual sentences. Call this collection of sentences Y. Use <a href=""https://datascience.stackexchange.com/questions/23969/sentence-similarity-prediction"">this solution</a> to compare each sentence of X to <em>each</em> sentence in Y. This will yield an array of scores Z for <em>each</em> sentence in X.</li>
<li>For each Z: weight each score in Z based on position. If a sentence in X and a sentence in Y are in similar locations in their respective essays (e.g., both at the intro of the essay) the weight on the respective score in Z will be higher than if these two sentences were farther apart.</li>
<li>Average all of the Z arrays for all of the sentences. The sentence with the highest average is the &quot;most cliche.&quot;</li>
</ol>
<p>Is there a better approach to this problem?</p>
","nlp"
"92485","Search for similar wikipedia articles based on a set of keywords","2021-04-02 12:10:22","","1","71","<nlp><api><wikipedia>","<p>I want to solve two questions:</p>
<ol>
<li>Which wikipedia articles could be interesting to me based on a list of keywords that are generated by the search terms I normally use in google(received by google takeout)?</li>
<li>Which wikipedia articles could be interesting to me based on <strong>what is not</strong> on a list of keywords that are generated by the search terms I normally use in google(received by google takeout)?</li>
</ol>
<p>I am looking for a how to do context search on wikipedia articles - preferrably via api so I don't have to download and process terabytes of wikipedia articles - using a/the mentioned set of keywords.</p>
","nlp"
"92439","How does the trainable projection layer used in PRADO and pQRNN work?","2021-04-01 13:40:59","","0","75","<deep-learning><neural-network><nlp>","<p>Trainable projection layers are said to be a very powerful thing but after reading:</p>
<ul>
<li><a href=""https://www.aclweb.org/anthology/D19-1506.pdf"" rel=""nofollow noreferrer"">https://www.aclweb.org/anthology/D19-1506.pdf</a></li>
<li><a href=""https://arxiv.org/pdf/2101.08890.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/2101.08890.pdf</a></li>
</ul>
<p>I don't understand how it works. So how does the trainable projection layer used in PRADO and pQRNN work?</p>
","nlp"
"92342","Create clusters based on specific keywords","2021-03-30 10:57:44","","0","350","<nlp><clustering><k-means><python-3.x><association-rules>","<p>I am working on raw text data. I am using clustering to put together common words in the documents. My requirement is to create clusters based on a specific list of words i.e I want to get a group of words that are typically found with the user-given list of words. Visually, the clusters should look like below. Typically, the clustering techniques are focused on creating segregated clusters while I need segregated clusters with some overlap. The image shows the view of the expected results.
I have tried using k-means clustering, the Apriori algorithm, and PrefixSpan in Python. But my desired result is not achieved.</p>
<p>Any suggestion is appreciated.
<a href=""https://i.sstatic.net/QRWH5.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/QRWH5.png"" alt=""enter image description here"" /></a></p>
","nlp"
"92341","How to read the labeled enron dataset categories?","2021-03-30 10:36:57","92737","0","779","<classification><nlp><dataset><feature-engineering>","<p>I am trying to use the labeled Enron dataset (<a href=""https://data.world/brianray/enron-email-dataset"" rel=""nofollow noreferrer"">link</a>) but I am really confused about the labeling system they use. I understand the <code>Cat_[1-12]_level_weight</code> is some form of confidence level. This dataset labeled by multiple students. <code>Cat_[1-12]_level_weight</code> increases with the number of the same label assigned by multiple students to a certain row (sample). But, what is for instance the <code>Cat_1_level_2</code>? From the overview (in the <a href=""https://data.world/brianray/enron-email-dataset"" rel=""nofollow noreferrer"">link</a>), I guess <code>Cat_1</code> mean &quot;<em>Course genre</em>&quot; and <code>level_2</code> mean  &quot;<em>Purely Personal (49 cnt.)</em>&quot; ? If so, why the <code>Cat_1_level_2</code> has values like 1, 2, 3, 4, etc.? I am really confused.</p>
","nlp"
"92235","How to perform tokenization for tweets in xlnet?","2021-03-27 14:37:26","","0","368","<python><tensorflow><nlp><sentiment-analysis><tokenization>","<p>X_train has only one column that contains all tweets.</p>
<pre><code>xlnet_model = 'xlnet-large-cased'
xlnet_tokenizer = XLNetTokenizer.from_pretrained(xlnet_model)

def get_inputs(tweets, tokenizer, max_len=120):
    &quot;&quot;&quot; Gets tensors from text using the tokenizer provided&quot;&quot;&quot;
    inps = [tokenizer.encode_plus(t, max_length=max_len, pad_to_max_length=True, add_special_tokens=True) for t in tweets]
    inp_tok = np.array([a['input_ids'] for a in inps])
    ids = np.array([a['attention_mask'] for a in inps])
    segments = np.array([a['token_type_ids'] for a in inps])
    return inp_tok, ids, segments

inp_tok, ids, segments = get_inputs(X_train, xlnet_tokenizer)

AttributeError: 'NoneType' object has no attribute 'encode_plus'
</code></pre>
","nlp"
"92224","How to deal with class imbalance problem in natural language processing?","2021-03-27 03:49:05","92233","1","471","<classification><class-imbalance><nlp><bert>","<p>I am doing a NLP binary classification task, using Bert + softmax layer on top of it. The network uses cross-entropy loss.</p>
<p>When the ratio of positive class to negative class is 1:1 or 1:2, the model performs well on correctly classifying both classes(accuracy for each class is around 0.92).</p>
<p>When the ratio is 1:3 to 1:10, the model performs poorly as expected. When the ratio is 1:10, the model has a 0.98 accuracy on correctly classifying negative class instances, but only has a 0.80 accuracy on correctly classifying positive class instances.</p>
<p>The behavior is as expected as the model turns to classify most/all instances toward negative class since the ratio of positive class to negative class is 1:10.</p>
<p>I just want to ask what's the recommended way for handling this kind of class imbalance problem in natural language processing specifically?</p>
<p>I saw someone suggests to change loss function, or perform up/down sampling, but most of them are targetting computer vision class imbalance problem.</p>
","nlp"
"91179","How to programmatically differentiate between extractive and abstractive summarization in NLP?","2021-03-26 10:05:38","","0","68","<python><nlp><automatic-summarization>","<p>We are using different pre-trained models in python transformers library, to generate summaries(both extractive and abstractive). So is there a programmatic way, based on the output summary, we can classify it as abstractive or extractive?</p>
<p>One method I think of is using the rouge python library to compute rouge score with respect to original input text(not human reference summary), which will have rouge(specifically LCS) precision score as 1.0 for extractive(since all the words present in summary will be present in original text). Whereas for abstractive summary some words present in summary will not be in original text and hence precision score will be less than 1.0. Please let me know if this is the right approach? Or is there any other way to differentiate between extractive and abstractive summaries?</p>
","nlp"
"91085","When are subword ngrams trained in fasttext? (Enriching Word Vectors with Subword Information)","2021-03-24 07:36:11","","1","45","<machine-learning><nlp><word-embeddings><fasttext>","<p>when is the training for subword ngrams done? is it done simultaneously as when the word representation are trained? or is it done at the end, after word representations are created?</p>
<p>fasttext implements this <a href=""https://arxiv.org/pdf/1607.04606.pdf"" rel=""nofollow noreferrer"">paper</a> where word representations are enriched with subword information. here, the word representation for each word is the sum of the representations of its character ngrams. just as how skipgram model is trained, so is the character ngrams, where the ngrams are the context and the word is the word.</p>
<p>since there are two tasks here that have to be trained, how is the training done? in sequence? or is it done together? The paper mentions two scoring objectives, one for the word and one for the subword ngrams. I am unclear as to whether these two objectives have been combined into one or there are optimized separately.</p>
","nlp"
"91073","dealing with HuggingFace's model's tokens","2021-03-24 03:19:40","","1","288","<nlp><bert><tokenization><huggingface><bart>","<p>I have a few questions regarding tokenizing word/characters/emojis for different huggingface models.</p>
<p>From my understanding, a model would only perform best during inference if the token of the input sentence are within the tokens that the model’s tokenizer was trained on.</p>
<p>My questions are:</p>
<ol>
<li><p>is there a way to easily find out if a particular word/emoji is compatible (included during model training) with the model? (in huggingface context)</p>
</li>
<li><p>if this word/emoji is not was not included during model training, what are the best ways to deal with these words/emojis, such that model inference would give best possible output considering the inclusion of these word/emoji as input. (for 2. it would be nice if it could be answered in the context of my huggingface setup below, if possible)</p>
</li>
</ol>
<p>My current setup is as follows:</p>
<pre><code>from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
pre_trained_model = 'facebook/bart-large-mnli'
task = 'zero-shot-classification'
candidate_labels = ['happy', 'sad', 'angry', 'confused']
tokenizer = AutoTokenizer.from_pretrained(pre_trained_model)
model = AutoModelForSequenceClassification.from_pretrained(pre_trained_model)
zero_shot_classifier = pipeline(model=model, tokenizer=tokenizer, task=task)

zero_shot_classifier('today is a good day 😃', candidate_labels=candidate_labels)
</code></pre>
<p>Any help is appreciated 😃</p>
","nlp"
"90900","How to work with input which is a combination of metadata+ vectorized text data + image pixel data to build a Regression Model (predict views)?","2021-03-19 20:00:13","91044","0","68","<python><nlp><regression><sparse><metadata>","<p>There are 4 datasets (all in csv format), each has a uniqueID column by which each record can be identified. Image and text datasets are dense datasets.(need to be converted to ndarray).</p>
<p>Can someone suggest how to use all these 4 datasets for building a regression model?</p>
<p>This is how the datasets look,</p>
<p>Metadata having some input features and target variable(views)</p>
<pre><code>uniqueID    ad_blocked embed  duration language hour views
     1        True     True    68        3      10   244
     2        False    True    90        1      15   63
     3        True     False   195       3      7    350
</code></pre>
<p>Vectorized title data - one entire row represents a title</p>
<pre><code>uniqueID  title_1   title_2    title_3 
     1   -0.977637  -0.543310  0.079403
     2    0.041873   0.644655  -0.406487        
     3    0.503560  -0.085412  0.841144
</code></pre>
<p>Vectorized descriptions data - one entire row represents a description</p>
<pre><code> uniqueID  title_1   title_2    title_3 
     1   -0.052256  -0.016036  0.079403
     2    0.000106  0.356706  -0.025788
     3    0.015774 -0.085412   0.712229
</code></pre>
<p>Thumbnail pixel data - one entire row represents an image</p>
<pre><code>uniqueID  image_1    image_2   image_3
     1   -0.484456  -0.543310  0.032915
     2    0.666147  0.644655  -0.005733
     3    0.035018  -0.011111  0.841144
</code></pre>
","nlp"
"90824","Is CRF suitable for multi-words Named Entity Recognition?","2021-03-18 10:13:10","90825","2","239","<scikit-learn><nlp><named-entity-recognition>","<p>I've a problem where I should create a custom NER by using <a href=""https://sklearn-crfsuite.readthedocs.io/en/latest/tutorial.html"" rel=""nofollow noreferrer"">sklearn CRF</a>. In the official tutorial, they are using CoNLL2002 corpus is available in NLTK where the entities are represented with a single word but in my problem, an entity can be formed with multiple words ex: United States of America, Cinema at Miami, etc.</p>
<p>Can CRF handle this?</p>
","nlp"
"90810","How to identify the feature that make the model misclassifed in text classification","2021-03-18 02:51:14","","0","141","<nlp><text-mining><accuracy><text-classification><text>","<p>Hi I am working on social media financial <strong>THAI</strong> text classification, the problem with this one is the confused classes, the misclassified prediction has a pattern that consistent as a pair.</p>
<p>and I want to know how can I identify the word/feature that responsible for the misclassified and lead the model to confuse with the prediction?</p>
","nlp"
"90789","Difference between NCE-Loss and InfoNCE-Loss","2021-03-17 16:00:55","","3","2970","<loss-function><word-embeddings><word2vec><nlp><representation>","<p>I started looking into word2vec and was wondering what the connection/difference between the <a href=""https://proceedings.neurips.cc/paper/2013/file/db2b4182156b2f1f817860ac9f409ad7-Paper.pdf#cite.MnihTeh2012"" rel=""nofollow noreferrer"">NCE-Loss</a> and the <a href=""https://arxiv.org/pdf/1807.03748.pdf"" rel=""nofollow noreferrer"">infoNCE-Loss</a> is. I get the basic idea of both.</p>
<p>I have a hard time deriving one from another, do you have any idea ?</p>
<p>Thank you in advance!</p>
","nlp"
"90740","How to take the keywords from the given dataset to train GPT-2 based chatbot?","2021-03-16 14:56:39","","1","36","<machine-learning><nlp><data-cleaning><gpt><chatbot>","<p>I am working with a dataset that contains Questions on various Events conducted by a college and the corresponding answers for the queries. I am using this dataset to train a GPT-2 355M model to create a chatbot where users can get their queries answered. But i am not getting good results and i feel that's because the questions in the dataset are in the &quot; -Query &quot; format.</p>
<p>For example, Ques: &quot;Cicada3302 - Do I need to have any prerequisite knowledge to enter this event&quot;</p>
<p>I am confused as to how can I make the chatbot understand that the first words before the &quot;-&quot; is like a keyword for rest of the question ? I am really new to this, so any help will be appreciated.</p>
<p>I have used the gpt_2_simple library for this.
I am attaching the colab link I have written so far, if it might be of any help.</p>
<p><a href=""https://colab.research.google.com/drive/1CrzwC9WQwF4YsqD66TY8F7ovTSHKsnFv?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1CrzwC9WQwF4YsqD66TY8F7ovTSHKsnFv?usp=sharing</a></p>
","nlp"
"90709","Unsupervised Text Classification with Python: Kmeans","2021-03-16 00:49:55","","1","1312","<machine-learning><python><nlp><k-means><text-classification>","<p>I am working on a project to build a text classifier of questions being asked. There are no labels provided in my data so I have chosen to go with an unsupervised approach. This solution needs to read a new question, and determine which category of classification it falls into</p>
<p>I have been working on a KMeans model, but despite everything I try I just cannot get any decent results. I have even hand selected a data set, cleaned it further by hand to make it as straightforward as possible, and my clusters are still overlapping, and I am getting a low silhouette score of 0.26015371238537227 (this is the highest I've been able to get). My raw data isn't very great to work with, but I still don't understand how it performs so poorly when I hand select/clean the best examples to use as a test. I have included my code for kmeans section below.</p>
<p>Does anyone have any suggestions? I'm a kind of at a loss here. Are there any other good ways to classify unlabeled text? I'm also relatively new to this type of work and this is my first ML based project too so I'm sure there is some knowledge gap to account for as well. I'm about ready to give up on the unsupervised approach and do some manual work to label the data myself and build a supervised model. Figured I'd check here first to see if all hope is lost or not. Any help is greatly appreciated!</p>
<pre><code>    data = pd.DataFrame(text)
    data.columns = ['KM']
    tfidf = TfidfVectorizer(max_df=0.80, min_df=5, max_features=10000)
    text = tfidf.fit_transform(data['KM'].values.astype('U'))

    kmeans = KMeans(n_clusters=8, init='k-means++', max_iter=300, n_init=10, 
    random_state=20)
    kmeans.fit(text)

    clusters = kmeans.predict(text)

    pca = PCA(n_components=2)
    two_dim = pca.fit_transform(text.todense())

    scatter_x = two_dim[:, 0]
    scatter_y = two_dim[:, 1]

    plt.style.use('ggplot')

    fig, ax = plt.subplots()
    fig.set_size_inches(20,10)

    cmap = {0: 'green', 1: 'blue', 2: 'red', 3: 'orange', 4: 'black', 5: 'purple', 6: 
    'yellow', 7: 'pink'}

    for group in np.unique(clusters):
        ix = np.where(clusters == group)
        ax.scatter(scatter_x[ix], scatter_y[ix], c=cmap[group], label=group)

    ax.legend()
    plt.xlabel('PCA 0')
    plt.ylabel('PCA 1')
    plt.show()

    order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]

    terms = tfidf.get_feature_names()
    for i in range(8):
        print(&quot;Cluster %d:&quot; % i, end='')
        for ind in order_centroids[i, :10]:
            print(' %s' % terms[ind], end='')
        print()


    score = silhouette_score(text, labels=kmeans.predict(text))
    print(score)  
</code></pre>
","nlp"
"90685","Classification using texts as features","2021-03-15 14:36:41","","2","85","<machine-learning><scikit-learn><nlp><text-classification><tfidf>","<p>I want to build a classification model <strong>to match customers and products</strong>. I have a description of each product, and a description of each customer, and the label : <code>customer *i* buy/did not buy product *j*</code>.</p>
<p>Each sample/row is a pair <code>(customer, product)</code>, so Feature 1 is customer's description, Feature 2 is product's description, and the target variable y is: <code>&quot;y = 1 : customer buys product&quot;</code>, <code>&quot;y = 0 otherwise&quot;</code>. The goal is to predict for new arriving products whether each customer is going to buy them or not.</p>
<p>I want to use <strong>Tf-Idf Vectorizer</strong>. I don't in which specific step I should <code>fit_transform</code> the descriptions, and how to put together Feature 1 with Feature 2.</p>
<ul>
<li><p>Should I concatenate the descriptions of each pair <code>(customer, product)</code> and <code>fit_transform</code> only once I have the concatenation?</p>
</li>
<li><p>Should I put together 2 columns using <code>ColumnTransformer</code>? If so, is the classifier going to fit correctly the obtained features?</p>
</li>
<li><p>Should I transform using a unique vocabulary?</p>
</li>
</ul>
<p>I found <a href=""https://github.com/scikit-learn/scikit-learn/issues/16148#issuecomment-578190776"" rel=""nofollow noreferrer"">here</a> a reference of three possible ways of working with two columns, but I don't see which one fits for my case.</p>
<p>Ps. Until now, I only got to build a similarity pairwise coefficient (using <a href=""https://stackoverflow.com/a/8897648/15276782"">this</a>), but there is no classification, and I know using labelled data can help. In particular, similarity measure gives the same weight to any text coincidence, but some coincidences should be more important than others.</p>
","nlp"
"90649","Class token in ViT and BERT","2021-03-14 18:01:00","","15","13143","<machine-learning><deep-learning><nlp><computer-vision><attention-mechanism>","<p>I'm trying to understand the architecture of the ViT Paper, and noticed they use a CLASS token like in BERT.</p>
<p>To the best of my understanding this token is used to gather knowledge of the entire class, and is then solely used to predict the class of the image. My question is — why does this token exist as input in all the transformer blocks and is treated the same as the word / patches tokens?</p>
<p>Treating the class token like the rest of the tokens means other tokens can attend to it. I'd expect that the class token will be able to attend other tokens while they could not attend it.</p>
<p>Also, specifically in ViT, why does the class token receive positional encodings? It represents the entire class and thus doesn't have any specific location.</p>
<p>Thanks!</p>
","nlp"
"90644","How do you search for content, not words?","2021-03-14 16:05:30","90655","1","42","<deep-learning><nlp>","<p>Given a string that describes a situation or method, are there algorithms that create fingerprints out of it, compare it with a corpus to then point to pages where a similiar concept is being discussed?</p>
<p>The simple form is word search.</p>
<p>You search for word X and X appears in a text whose excerpt is shown to you.</p>
<p>This is the next step.</p>
<p>You describe X without writing &quot;X&quot; and you get text excerpts as a result where X is also described using different phrases (or even words) with or without an explicit mention of &quot;X&quot;.</p>
<p>How do you search for content, not words?</p>
<p>Hint: I am looking for technical terms for this problem to find research papers.</p>
","nlp"
"90578","Extract key phrases for binary outcome","2021-03-13 04:36:19","90588","1","44","<nlp><semi-supervised-learning>","<p>I have a set of phrases that lead to a binary outcome (accept/reject) and I was wondering what techniques are most helpful for extracting key phrases that are most likely to determine the outcome, given that I have a training set of data that has the English-language phrase and the observed outcome.</p>
<p>To illustrate the idea let me give a simple example:</p>
<h2>Accepted</h2>
<ul>
<li>Sounds great</li>
<li>That would be great</li>
<li>That's fine</li>
</ul>
<p>key words: great, fine</p>
<h2>Rejected</h2>
<ul>
<li>I'm not sure</li>
<li>I don't think so</li>
<li>No way</li>
</ul>
<p>key words: not, don't, no</p>
","nlp"
"90529","Text preprocessing on corpus in pipeline before Gensim word2vec training","2021-03-11 20:35:02","","2","284","<python><nlp><word2vec><nltk><gensim>","<p>I have a large compressed corpus, about 30gb in .txt.gz format. In raw format it can be used as input to word2vec like this:</p>
<pre><code>data = gensim.models.word2vec.LineSentence(corpus)
</code></pre>
<p>This creates an iterator over the lines of the corpus. The next step is training:</p>
<pre><code>model = gensim.models.Word2Vec(data)
</code></pre>
<p>I'd like to lemmatize and POS-tag the corpus before training. I am planning to use NLTK WordNetLemmatizer and NLTK POS-tagger.</p>
<p>How should I do this in a pipeline?</p>
","nlp"
"90493","How to implement your own word list for sentiment analysis?","2021-03-10 22:24:07","","1","520","<scikit-learn><nlp><sentiment-analysis>","<p>I've finished web scraping and cleaning the text i'm interested in and i also have the sentiment word list i want to apply to it. However, i have a few conceptual and implementation problems.</p>
<p>my sentiment word list is a data frame of words assigned to particular features such as negative, positive etc. I currently have a list where each element is a block of text waiting to be analysed. I'm not sure where to go from here. From the resources i have seen so far, they put their text through a sentiment word list from another module that takes care of it for you and simply gives you the results.</p>
<p>I need clarification on the following:</p>
<ol>
<li><p>my sentiment word list contains words and they are either True(1)/False(0) on specific features such as positive and negative. So within my text thats being analysed, if it contains the words that are also in my sentiment word list, they will be added to the document matrix, along with their frequency. The overall positivity and negativity will be proportional the the frequency of positive and negative words. Have i understood this correctly?</p>
</li>
<li><p>How do i go about implementing this? i'm not sure on how to use the vocabulary parameter within Count Vectorizer.</p>
</li>
</ol>
<pre><code>from sklearn.feature_extraction.text import CountVectorizer
vc=CountVectorizer(vocabulary=sentiment_words)
vectors=vc.fit_transform(cleaned_text)
</code></pre>
<p>The feature names end up being the features i'm using as an independent variable.</p>
<pre><code>(vc.get_feature_names())
['word',
 'negative',
 'positive',
 'uncertainty',]
</code></pre>
<p>The feature names should be the actual words contained within my sentiment word list right? i'm assuming i need to change the sentiment word list dataframe but i'm not sure on how to proceed. Also, how would the overall positive and negative metric be assigned here?</p>
<p>I'm quite new to this so any sort of clarification would be great! Thanks!</p>
","nlp"
"90389","What is the meaning of, or explanation for, having multiple tags in a Doc2Vec model's TaggedDocuments?","2021-03-08 16:06:24","","2","136","<python><nlp><word2vec><doc2vec><document-understanding>","<p>I've tried reading the other answers on this topic but I'm unsure if I understand completely.</p>
<p>For my dataset, I have a series of tagged documents, &quot;good&quot; or &quot;bad.&quot; Each document belongs to an entity, and each entity has a different number of documents.</p>
<p>Eventually, I'd like to create a classifier to detect whether or not an entity's document is good or bad and to also see what sentences are most similar to the good/bad tag.</p>
<p>All that being said, does it make sense to label my data as following:</p>
<pre><code>train_corpus = []
i = 0
for entity in entities:
    for doc_name in entity:
        for sentence in get_doc(doc_name):
           train_corpus.append(TaggedDocument(sentence, tags = [i, doc_name, entity, doc_name.good_or_bad])
           i+=1
</code></pre>
<p>From what I understand, this means that each entity is contextualized by all TaggedDocuments that have that entity's name, whereas each document is contextualized by each sentence that composes it. And the overall good/bad idea is composed of all the sentences that make up either the good or bad documents. Is this a correct interpretation? And if that's the case, could I then do something like:</p>
<pre><code>unlabeled_data = [...]
model.infer_tag(unlabeled_data[0])
# return predicted good/bag tag
model.cosine_distance(unlabeled_data[0], &quot;bad&quot;)
#get a numerical measure of how far some unlabeled data is from the &quot;bad&quot; tag
</code></pre>
","nlp"
"90306","Best way to suggest answers given historical question-answer pairs","2021-03-05 15:35:25","","1","64","<nlp><text-generation><question-answering>","<p>Many question-answering implementations focus on extracting information from large documents/corpora of text such as Wikipedia.</p>
<p>I have access to a full chat log from the customer service of a large electronics company and I'm wondering if I can use the large amount of historical question-answer pairs in order to come up with useful answer suggestions for the customer care agents. The goal is thus to provide the customer care agent with useful answer suggestions. These could either be the result of some matching- or classification endeavor over the historical answers, but could also be the result of generative methods.</p>
<p>The question is basically the same as this one:
<a href=""https://datascience.stackexchange.com/questions/5073/question-and-answer-chatbot-for-customer-support"">Question and Answer Chatbot for Customer Support</a>. Given that a lot has changed and evolved in NLP-land, I figured I'd ask this question again.</p>
<p>What I've experimented with thus far is to use state-of-the-art language models like BERT to vectorize the incoming questions and use cosine-similarity to find the most similar question and return the answer that was given to that question. This seems to work pretty well for generic questions that are answered with generic answers, but is horrible for more specific questions. Unfortunately, the generic questions make up only a small portion of the dataset.</p>
<p>I'm looking for suggestions to implement an approach that takes full advantage of the available historical question-answer pairs.</p>
","nlp"
"90245","NLP food review classification","2021-03-04 14:00:30","","1","94","<classification><nlp>","<p>I have a <a href=""https://s3.amazonaws.com/amazon-reviews-pds/readme.html"" rel=""nofollow noreferrer"">dataset</a> consisting of a bunch of food reviews. I'm trying to build a model that determines if a given review talks about a food safety issue. The dataset has a lot of features, but relevant to my goal there are only two features of importance --  food reviews and their corresponding numerical ratings.</p>
<p>I'd love a pointer or two to figure out how to approach this problem.</p>
<p>As a first pass attempt, I'm thinking of pipelining all reviews into a sentiment analsyis filter and then look at the ones have keyword related to food safety (eg: &quot;high cholesterol&quot;, &quot;unsafe&quot;, etc)</p>
","nlp"
"90227","Approach for training multilingual NER","2021-03-04 05:53:15","90231","0","197","<nlp><named-entity-recognition>","<p>I am working on multilingual (English, Arabic, Chinese) NER and I met a problem: how to tokenize data?</p>
<p>My train data provides sentence and list of spans for each named entity.</p>
<p>e.g.</p>
<pre><code>
[('The', 'DT'),
 ('company', 'NN'),
 ('said', 'VBD'),
 ('it', 'PRP'),
 ('believes', 'VBZ'),
 ('the', 'DT'),
 ('suit', 'NN'),
 ('is', 'VBZ'),
 ('without', 'IN'),
 ('merit', 'NN'),
 ('.', '.')]

[('你', 'PN'),
 ('有', '.'),
 ('没', 'AD'),
 ('有', '.'),
 ('用', 'VV'),
 ('过', '.'),
 ('其它', 'DT'),
 ('药品', 'NN'),
 ('？', 'PU')]

</code></pre>
<p>What the best way to tokenize input data? There are main different alternatives I consider: word level, wordpiece level, BPE.</p>
<p>BPE does not work with Chinese and Arabic because of unicode thing. I doubt about word level because I am not sure what is a word in Chinese.</p>
<p>What can you recommend me?</p>
","nlp"
"90226","Attention in NER","2021-03-04 04:42:52","","0","296","<neural-network><deep-learning><nlp><attention-mechanism>","<p>I am building a named entity recognition model and it is having BERT+BiLSTM+CRF in it. Now I am planning to introduce an attention layer. My question is - what type of attention I should use here and why?</p>
<p>Data: It is an invoice data - I am trying to extract Company and person names from the invoices.</p>
","nlp"
"90181","How to decide which method to use TFIDF, or BOW","2021-03-03 07:46:03","","1","506","<nlp><feature-extraction><tfidf><bag-of-words>","<p>In a huge dataset for NLP it is taking very long time to classify my dataset</p>
<p>therefore, trying each feature extraction method separetly is time consuming and not effecient.</p>
<p>Is there a way that can tell me which method (TFIDF or Bag Of Words) is more likely to give the highest F1 score.</p>
<p>I tried test them on smaller subset (1000 records) it was fast but best method in smaller subset does not mean it is the best in complete dataset.</p>
<p>any other way to decide which method to use?</p>
","nlp"
"90161","Is LSTM or pretrained BERTForMasked LM usable for predicting changed word in a sentence using a small dataset? (2000 samples)","2021-03-02 18:48:01","","0","39","<python><lstm><nlp><bert>","<p>I have a small (2000 samples) dataset of newspaper headlines and their humorous conterparts where only one word is changed to sound silly, for example:</p>
<p><strong>Original headline</strong>: Police &lt;officer&gt; arrested for abuse of authority</p>
<p><strong>Humorous headline</strong>: Police &lt;dog&gt; arrested for abuse of authority</p>
<p>I want to train a model to predict changed sentence by the original. I am planning to implement two models for this task: one for binary tagging of input sequences (whether a word in a sentence needs to be changed) and one for predicting sentences with changed words.</p>
<p><strong>Example of Model 1 input</strong>: Police officer arrested for abuse of authority</p>
<p><strong>Example of Model 1 output</strong>: &lt;no-change&gt; &lt;change&gt; &lt;no-change&gt; &lt;no-change&gt; &lt;no-change&gt; &lt;no-change&gt; </p>
<p><strong>Example of Model 2 input</strong>: Police &lt;...&gt; arrested for abuse of authority</p>
<p><strong>Example of Model 2 output</strong>: Police dog arrested for abuse of authority</p>
<p>I am going to use a RNN/LSTM model for sequence tagging. As for the changed word prediction task, I am thinking of either using LSTM (concatenation of two parallel LSTM layers - one to run forwards on left context of word and the other to run backwards on right context) or fine-tuning <a href=""https://huggingface.co/transformers/model_doc/bert.html#bertformaskedlm"" rel=""nofollow noreferrer"">BERTForMaskedLM from huggingface/transformers</a>.</p>
<p>The question is whether it would be appropraite considering the small number of data, or should I switch to some other models?</p>
","nlp"
"90155","Extracting Names using NER | Spacy","2021-03-02 16:34:00","90174","1","1001","<nlp><text-mining><spacy>","<p>I'm new to NER and I've been trying to extract names using Spacy. Here's my code:</p>
<pre class=""lang-py prettyprint-override""><code>import spacy
spacy_nlp = spacy.load('en_core_web_sm')

doc = spacy_nlp(text.strip())

# create sets to hold words 
named_entities = set()
money_entities = set()
organization_entities = set()
location_entities = set()
time_indicator_entities = set()

for i in doc.ents:
    entry = str(i.lemma_).lower()
    text = text.replace(str(i).lower(), &quot;&quot;)
    # Time indicator entities detection
    if i.label_ in [&quot;TIM&quot;, &quot;DATE&quot;]:
        time_indicator_entities.add(entry)
    # money value entities detection
    elif i.label_ in [&quot;MONEY&quot;]:
        money_entities.add(entry)
    # organization entities detection
    elif i.label_ in [&quot;ORG&quot;]:
        organization_entities.add(entry)
    # Geographical and Geographical entities detection
    elif i.label_ in [&quot;GPE&quot;, &quot;GEO&quot;]:
        location_entities.add(entry)
    # extract artifacts, events and natural phenomenon from text
    elif i.label_ in [&quot;ART&quot;, &quot;EVE&quot;, &quot;NAT&quot;, &quot;PERSON&quot;]:
        named_entities.add(entry.title())
</code></pre>
<p>The model seems to have a decent accuracy with certain kinds of names. However it is unaware of how people’s names can differ around the world (not adapted to suit cultural differences). Is there a possible workaround to avoid this bias?</p>
","nlp"
"90121","best python library to extract the location from the text","2021-03-01 23:19:47","","1","128","<machine-learning><nlp><data-science-model>","<p>Which is the best python library to extract the location from the text. Spacy and NLTK not recognizing the mountain names, river bodies properly.</p>
<p>Could you please give me your suggestions?</p>
<p>Thank you!</p>
","nlp"
"90092","Is there an NLP corpus that contains common medical terms?","2021-03-01 12:43:09","","7","5158","<python><nlp><nltk>","<p>I am trying to use the NLTK library to extract keywords denoting medical symptoms from medical reports of patients. For example, I have a medical report as follows:</p>
<blockquote>
<p>s:a 33 year old female crystallographer presents with mild spells of
vertigo, mild headaches particularly at the back of the head and in
the morning x 2 weeks. pt also reports chronic mild occasional
lightheadedness. o:Height 160 cm, Weight 53.8 kg, Temperature 37.3 C,
Pulse 76, SystolicBP 146, DiastolicBP 93, Respiration 15, Heart = 2/6
systolic murmur at base of heart, Chest = clear to auscultation B/L,
no rales or wheezing, Extremities = no edema or clubbing, Heart =
normal S1, S2, RRR a:Hypertension p:performed E/M Level 2 (established
patient) - Completed, and prescribed Hydrochlorothiazide - 50 mg po
qd, and ordered Cholesterol.</p>
</blockquote>
<p>Here, I would like to find all the keywords or bigrams that represent medical symptoms. In the text above, these keywords are 'mild spells of vertigo', 'mild headaches', 'lightheadedness' etc.</p>
<p>For this, I need some kind of collection of terms that represent symptoms, so that I can detect similar terms in the medical reports I have. Is there any NLTK corpus associated with medical terminology? If I find a list of words that denote medical symptoms, I can tokenise and lemmatise the words I have detected in the medical reports and compare them to the words found in the corpus.</p>
<p>Thank you.</p>
","nlp"
"90018","Difference between zero-padding and character-padding in Recurrent Neural Networks","2021-02-27 10:13:26","90085","0","695","<neural-network><nlp><rnn><sequence-to-sequence>","<p>For RNN's to work efficiently we vectorize the problem which results in an input matrix of shape</p>
<pre><code>    (m, max_seq_len) 
</code></pre>
<p>where m is the number of examples, e.g. sentences, and max_seq_len is the maximum length that a sentence can have. Some examples have a smaller lengths than this max_seq_len. A solution is to pad these sentences.</p>
<p>One method to pad the sentences is called &quot;zero-padding&quot;. This means that each sequence is padded by zeros. For example, given a vocabulary where each word is related to some index number, we can represent a sentence with length 4,</p>
<pre><code>    &quot;I am very confused&quot; 
</code></pre>
<p>by</p>
<pre><code>    [23, 455, 234, 90] 
</code></pre>
<p>Padding it to achieve a max_seq_len=7, we obtain a sentence represented by:</p>
<pre><code>    [23, 455, 234, 90, 0, 0, 0] 
</code></pre>
<p>The index 0 is not part of the vocabulary.</p>
<p>Another method to pad is to add a padding character, e.g. &quot;&lt;&lt;pad&gt;&gt;&quot;, in our sentence:</p>
<pre><code>    &quot;I am very confused &lt;&lt;pad&gt;&gt;&gt; &lt;&lt;pad&gt;&gt; &lt;&lt;pad&gt;&gt;&quot;
</code></pre>
<p>to achieve the max_seq_len=7. We also add &quot;&lt;&lt;pad&gt;&gt;&quot; in our vocabulary. Let's say it's index is 1000. Then the sentence is represented by</p>
<pre><code>    [23, 455, 234, 90, 1000, 1000, 1000]
</code></pre>
<p>I have seen both methods used, but why is one used over the other? Are there any advantages or disadvantages comparing zero-padding with character-padding?</p>
","nlp"
"89915","Where can I find documentation or paper mentioning pre-trained distilbert-base-nli-mean-tokens model?","2021-02-25 14:42:20","","0","1110","<nlp><word-embeddings><bert><transformer><embeddings>","<p>I am trying to find more information about pre-trained model <code>distilbert-base-nli-mean-tokens</code>. Can someone please point me to it's paper or documentation? Is it based on <a href=""https://arxiv.org/abs/1910.01108"" rel=""nofollow noreferrer"">DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</a> paper? This is published in March 2020. I am looking for links between this paper and Sentence-BERT (<a href=""https://www.sbert.net/index.html"" rel=""nofollow noreferrer"">sentence-transformers</a>). Original <a href=""https://arxiv.org/pdf/1908.10084.pdf"" rel=""nofollow noreferrer"">sentence-bert</a> paper is published in Aug 2019. I wanted to try pre-trained model using S-BERT model and hence tried <code>distilbert-base-nli-mean-tokens</code> <a href=""https://www.sbert.net/examples/training/sts/README.html"" rel=""nofollow noreferrer"">model</a>. After implementation I found out that it's much faster than other pre-trained models available on sentence-transformer <a href=""https://www.sbert.net/examples/training/sts/README.html"" rel=""nofollow noreferrer"">website</a>. While studying it's paper I realised original paper do not mention this pre-trained model.</p>
<p>I found <a href=""https://arxiv.org/pdf/2004.09813.pdf"" rel=""nofollow noreferrer"">Making Monolingual Sentence Embeddings Multilingual using
Knowledge Distillation</a> this paper published by same author which do mention <code>DistilmBERT</code> but not <code>DistilBert</code> Can someone please help me solve this mystery?</p>
","nlp"
"89896","How to visualize ner dataset tagged using BILOU?","2021-02-25 09:32:22","","1","56","<nlp><lstm><visualization><learning>","<p>I have a dataset for ner which is tagged using BILOU tagging method and example of same is below</p>
<pre><code>Minjun  B-Person
is  O
from    O
South   B-Location
Korea   I-Location
.   O
</code></pre>
<p>i wish to visualize how model learn if i use lstm with crfD
for example how this model is learning context of current sentence</p>
","nlp"
"89894","How to preprocess tensorflow imdb_review dataset","2021-02-25 08:34:53","","1","213","<python><tensorflow><nlp><preprocessing>","<p>I am using <a href=""https://www.tensorflow.org/datasets/catalog/imdb_reviews"" rel=""nofollow noreferrer"">tensorflow imdb_review dataset</a>, and I want to preprocess it using <strong>Tokenizer</strong> and <strong>pad_sequences</strong></p>
<p>When I am using the <strong>Tokenizer</strong> instance and using the following code:</p>
<pre class=""lang-py prettyprint-override""><code>tokenizer=Tokenizer(num_words=100)
tokenizer.fit_on_texts(df['text'])
word_index = tokenizer.word_index
sequences=tokenizer.texts_to_sequences(df['text'])

print(word_index)
print(sequences)
</code></pre>
<p>I am getting the error</p>
<pre><code>TypeError: a bytes-like object is required, not 'dict'
</code></pre>
<p><strong>What I've tried</strong></p>
<p>store dataset as dataframe and then iterate over the text column, and store it in a list, and then tokenize it.</p>
<pre class=""lang-py prettyprint-override""><code>df = tfds.as_dataframe(ds.take(4), info)
# list to store corpus
corpus = []
for sentences in df['text'].iteritems():
  corpus.append(sentences)

tokenizer=Tokenizer(num_words=100)
tokenizer.fit_on_texts(corpus)
word_index=tokenizer.word_index
print(word_index)
</code></pre>
<p>But i'm getting the error</p>
<pre><code>AttributeError: 'tuple' object has no attribute 'lower'
</code></pre>
<p>How can I use the <code>text</code> column and preprocess it to feed it to my neural network?</p>
","nlp"
"89846","Does batch size matter in inferencing speed?","2021-02-24 11:15:45","","0","424","<machine-learning><deep-learning><nlp>","<p>I am reading the paper <a href=""https://arxiv.org/pdf/1905.10650.pdf"" rel=""nofollow noreferrer"">&quot;Are Sixteen Heads Really Better than One?&quot;</a> and in section 4.3 it states that the inference speed varies with batch size.</p>
<p><a href=""https://i.sstatic.net/nnLXq.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/nnLXq.png"" alt=""enter image description here"" /></a></p>
<p>How does batch size affect inference speed for this machine learning model?</p>
","nlp"
"89781","How can I preprocess text to feed into a SVM?","2021-02-23 07:02:50","89793","0","111","<python><nlp>","<p>I am using an IMDB dataset which contains reviews of the movies in the column <strong>text</strong> and the rating 0 or 1 in the column <strong>label</strong>. I am preprocessing the text using <strong>Tfidf</strong> using sklearn.</p>
<p>The code for the above statement</p>
<pre class=""lang-py prettyprint-override""><code>from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer=TfidfVectorizer()
X = vectorizer.fit_transform(df_train['text'])
Y = vectorizer.transform(df_test['text'])
</code></pre>
<p>When I am trying to use the data for an SVM, using a linear kernel and then fitting it into the model using</p>
<pre class=""lang-py prettyprint-override""><code>classifier_linear = svm.SVC(kernel='linear')
classifier_linear.fit(X, df_test['label'])
</code></pre>
<p>I am getting the error</p>
<p>ValueError: Found input variables with inconsistent numbers of samples: [40000, 5000]</p>
<p>df_train is of the shape (40000,2)
df_test is of the shape (5000,2)</p>
<p>I am able to overcome this problem by using only 5000 values of df_train using</p>
<pre><code>df_train.loc[:4999,'text']
</code></pre>
<p>but this defeats the purpose of having a bigger training dataset.</p>
<p>My question is how can I use the training dataset that will retain it's number of values?</p>
","nlp"
"89778","to generate consistent encoding for words in Keras using tf.keras.preprocessing.text.one_hot","2021-02-23 04:56:00","90160","0","464","<python><keras><tensorflow><nlp>","<p>I am using keras(tensorflow) to convert text into encodings using <code>tensorflow.keras.preprocessing.text.one_hot</code></p>
<p>I have used it for training dataset as below</p>
<pre><code>from tensorflow.keras.preprocessing.text import one_hot

corpus = ['nice app']
onehot_repr = [one_hot(words, 10000) for words in corpus]

print(onehot_repr)
# [5779, 2969]
</code></pre>
<p>It's ok upto this point.</p>
<p>But when I use the <code>one_hot</code> for my testing set it generates different encoding.</p>
<p>I have created a Flask API to test, So how can use same encoding for both train and test set</p>
<p>Result from API is :</p>
<p><code>[[5129, 4965]]</code> for same text <code>['nice app']</code></p>
","nlp"
"89756","Extracting structured data from semi structured data","2021-02-22 17:34:51","","0","88","<machine-learning><nlp><pandas><text-mining><text-classification>","<p>I want to use machine learning and NLP to convert semi-structured data in text files to structured data by predicting the patterns in the files and splitting the fields for example if I have a text file that looks like this :</p>
<p>Input :</p>
<pre><code>2021565267MALL1ETAGE ZARA1st FLOOR 2345561
2022565267MALL2ETAGE ZARA1st FLOOR 2345561
2022565267ANFAPLACE2ETAGECOFEESHOP2345561
20225652634ANFAPLACE2ETAGE 2345561
</code></pre>
<p>Desired Output :</p>
<pre><code>2021565267,MALL1ETAGE ZARA1st FLOOR,2345561
2022565267,MALL2ETAGE ZARA1st FLOOR,2345561
2022565267,ANFAPLACE2ETAGECOFEESHOP,2345561
20225652634,ANFAPLACE2ETAGE,2345561
</code></pre>
<p>The semi-structured files are not fixed-width so we can not just add col specification in pandas like this ( it can work for the first line for example ) :</p>
<pre><code>col_specification =[(1, 10),.... ]

</code></pre>
<p>One of the approaches that I found online is to make a dictionary based on the occurrences of the words in the semi-structured file will that work in this situation if so how can I implement something like that?</p>
","nlp"
"89729","About Natural Question (NQ) benchmark in NLP","2021-02-22 02:29:35","","1","34","<nlp><bert><search-engine>","<p>I recently learned that there is a benchmark called NQ.</p>
<p><a href=""https://ai.google.com/research/NaturalQuestions/visualization"" rel=""nofollow noreferrer"">https://ai.google.com/research/NaturalQuestions/visualization</a></p>
<p>Unlike other QA benchmarks which relevant document is povided with query, it has to find information from millions of corpus by itself.</p>
<p>For example, if question is &quot;when are hops added to the brewing process?&quot; other QA benchmark also provide only 1 document about brewing. While NQ provide whole wikipedia text and model has to find most relevant document and answer.</p>
<p>When I tried all the example in the NQ with google search engine it gave me the answer every time. Then</p>
<ol>
<li>How does google search engine is managing those questions so well(Since NLP such as BERT is pretty computationally expensive I think it is not likely google is running whole model for every search)</li>
<li>If google is doing it in other ways than neural network.(Or mixing it with other method) Do we need to bother with method such as REALM(<a href=""https://arxiv.org/pdf/2002.08909.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/2002.08909.pdf</a>) which seems to be computationally heavy?</li>
</ol>
","nlp"
"89675","Is it domain adaptation?","2021-02-20 21:20:03","","1","62","<nlp>","<p>Let's say I trained a classifier to assign a shop department to a product, e.g.: ALGIDA Cow milk-&gt;Diary. It did it on a domain of official product names.</p>
<p>When I applied pre-trained classifier to names of products in a shopping list, I found it doesn't perform that well as in the source domain because people tend to make spelling errors, abbreviate product names, do not include brand names, etc. in their shopping lists.</p>
<p>Simply speaking, the classifier does not generalize well to another domain.</p>
<p>I tried a few techniques where each alone improved classifier accuracy in a target domain:</p>
<ol>
<li>Pre-processed product names in a shopping list by correcting spelling errors and expanding abbreviations</li>
<li>Pre-processed product names in a source domain before training by removing brand names.</li>
<li>Stemmed product names before constructing TF/IDF vectors</li>
<li>Embedded product names using pretrained USE (Universal Sentence Encoder) model.</li>
</ol>
<p>Each of those techniques makes phrases in one domain more similar to phrases in another domain, or features build on top of those phrases more similar across domains.</p>
<p>Would you call any of those techniques domain adaptation?</p>
","nlp"
"89575","Which machine learning problem is this?","2021-02-19 02:56:08","","4","89","<machine-learning><neural-network><classification><nlp>","<p>I am not able to figure out what kind of machine learning is this:</p>
<p>Training set: consists of sentences with object labels for object phrases
Example:</p>
<pre><code>&quot;This is a black chair. It is next to a large bed.&quot;
</code></pre>
<p>Phrases <code>'This', 'black chair', 'it'</code> were annotated with the label <code>&quot;chair&quot;</code> and phrase <code>'large bed'</code> was annotated with the label <code>'bed'</code>. There are 18 available labels that can be assigned.</p>
<p>For an unannotated sentence, I want to predict the labels for each phrase in the sentence.</p>
<p>For example:</p>
<pre><code>There is a study table in the corner of the room, behind it is a small chair.
</code></pre>
<p>I would like the model to predict a label (available 18 labels) for each phrase that represents an object in the above sentence.</p>
<p>Expected output:</p>
<pre><code>'study table', 'it' -----&gt; label 'table'

'small chair' -----------&gt; label 'chair'
</code></pre>
<p>Is this a classification, regression, or a different kind of problem?</p>
","nlp"
"89561","Preparing training data for NLP machine learning task","2021-02-18 20:30:03","","1","57","<machine-learning><neural-network><word-embeddings><nlp>","<p>I have the natural language sentences as follows:</p>
<p><code>This is a black chair. It is next to the table.</code></p>
<p>Each phrase that represents an object is annotated with an object Id. For example, in the above sentence, we have:</p>
<p><code>This: 15, black chair: 15, It: 15, table: 14</code></p>
<p>(where, 14 and 15 are object Ids)</p>
<p>I would like to train a model to predict the object Id of each phrase representing an object for a new sentence. From what I understand, each training example will consist of the following structure:</p>
<p><code>Inputs:</code> sentence + object phrase</p>
<p><code>Output:</code> object id (from 18 available ids)</p>
<p>I would need to repeat the above for each object phrase in a sentence</p>
<p>My question: How do I prepare the training data for this task? How do I represent each object phrase (eg: 'black chair') and each sentence for training the neural network?</p>
","nlp"
"89517","Looking for more recent dataset for document classfication","2021-02-18 01:26:17","","0","125","<classification><nlp><image-classification><similar-documents>","<p>I am trying to develop an NLP - CNN algorithm to detect documents with sensitive information such as passport, license and distinguish them from other documents like resume, email, form or advertisements.</p>
<p>I personally consider this as a document classification problem and looked for open source datasets which had documents from different category/classes.
I found the RVL-CDIP Dataset and tobacco3482 dataset with classes such as Email, form, letter, news, resume, scientific. However, the dataset collection looks from an old collection (ie samples in these datasets represent how resumes were written in 20000 and invoices collected in early 2000 )These datasets doesn't quite represent recent sample set of documents that we might come across on a daily basis.
Is there a more recent document classification dataset available which would be more relevant to the current format of these documents?</p>
<p>EDIT : Looking for a similar dataset to RVL-CDIP Dataset and tobacco3482 dataset but which is more recent created. Use case is to still distinguish between resumes, advertisements, passports, and emails.
I am not looking at detecting sensitive information within emails so my usecas is similar to RVL-CDIP Dataset  use-case</p>
","nlp"
"89435","What is the difference between Okapi bm25 and NMSLIB?","2021-02-16 08:45:51","89459","1","612","<nlp><python-3.x><information-retrieval><search-engine>","<p>I was trying to make a search system and then I got to know about <code>Okapi bm25</code> which is a ranking function like tf-idf. You can make an index of your corpus and later retrieve documents similar to your query.</p>
<p>I imported a <a href=""https://pypi.org/project/rank-bm25/"" rel=""nofollow noreferrer"">python library</a> <code>rank_bm25</code> and created a search system and the results were satisfying.</p>
<p>Then I saw something called <a href=""https://github.com/nmslib/nmslib"" rel=""nofollow noreferrer"">Non-metric space library</a>. I understood that its a similarity search library much like kNN algorithm.</p>
<p>I saw <a href=""https://towardsdatascience.com/how-to-build-a-smart-search-engine-a86fca0d0795"" rel=""nofollow noreferrer"">an example where a guy was trying to make a smart search system</a> using <code>nmslib</code>. He did the following things:-</p>
<ul>
<li>tokenized the documents</li>
<li>pass the tokens into <code>fastText</code> model to create word vectors</li>
<li>then combined those word vectors with bm25 weights</li>
<li>then passed the combination into nmslib</li>
<li>performed the search.</li>
</ul>
<p><em>If the above link does not opens the document just open it in incognito mode.</em></p>
<p>It was quite fast, but the results were not satisfying, I mean even if I was copy pasting any exact query from the doc, it was not returning that doc. But the search system that I made using rank_bm25 was giving great results. So the conclusion was</p>
<p><strong><code>bm25</code> gave good results and <code>nmslib</code> gave faster results.</strong></p>
<p>My questions are</p>
<ul>
<li>How do they both (bm25, nmslib) differ?</li>
<li>How can I pass bm25 weights to nmslib to create a better and faster search engine?</li>
<li>In short, how can I combine the goodness of both bm25 and nmslib?</li>
</ul>
","nlp"
"89426","How to reduce the GPU consumption size while using Elmo Model?","2021-02-16 07:45:53","","0","114","<deep-learning><tensorflow><nlp><python-3.x>","<p>I am performing an NLP task using <code>Elmo</code> model. Whenever I load the Elmo model, it occupies the 15 GB of my GPU memory. How can I reduce it ?</p>
<p>Below is my code</p>
<pre><code>import tensorflow.compat.v1 as tf
import tensorflow_hub as hub
tf.disable_eager_execution()
from tensorflow.compat.v1.keras import backend as K
sess = tf.Session()
K.set_session(sess)

elmo_model = hub.Module(&quot;https://tfhub.dev/google/elmo/2&quot;, trainable=True)
sess.run(tf.global_variables_initializer())
sess.run(tf.tables_initializer())


def ElmoEmbedding(x):
    return elmo_model(inputs={
                    &quot;tokens&quot;: tf.squeeze(tf.cast(x, tf.string)),
                    &quot;sequence_len&quot;: tf.constant(batch_size*[maxlen])
              },
              signature=&quot;tokens&quot;,
              as_dict=True)[&quot;elmo&quot;]
</code></pre>
<p>And then I am passing the ElmoEmbedding in the Lambda layer as below</p>
<pre><code>input_text = Input(shape=(maxlen,), dtype=tf.string)

embedding = Lambda(ElmoEmbedding, output_shape=(maxlen, 1024))(input_text)

x = Bidirectional(LSTM(units=512, return_sequences=True,
               recurrent_dropout=0.2, dropout=0.2))(embedding)

.....
</code></pre>
<p><strong>What do I need to change in the above code ?</strong></p>
","nlp"
"89352","Binary classification and numerical labels","2021-02-14 10:42:24","","4","764","<deep-learning><nlp><lstm><sentiment-analysis>","<p>I am trying to create a sentiment analysis model using a dataset that have ~50000  positive tweets that i labeled as 1, ~50000 negative tweets that i have labeled as 0. Also i have acquired ~10000 tweets that are neutral.</p>
<p>Due to the low number of neutral tweets my thinking is to label neutral with 0.5 and train the model using binary crossentropy as loss function. My output layer is 1 neuron with sigmoid activation function so prediction value would be between (0,1) .</p>
<p>Is my thinking right or it will mess the accuracy?</p>
","nlp"
"89230","How to add words to english model word list in Julius Speech Recognition Engine?","2021-02-11 08:11:29","","1","53","<machine-learning><deep-learning><nlp>","<p>I want to add some English words to model but how can I achieve this ?</p>
<p><a href=""https://github.com/julius-speech/julius"" rel=""nofollow noreferrer"">https://github.com/julius-speech/julius</a></p>
","nlp"
"89221","Medical NER for French language","2021-02-11 03:10:55","","3","191","<word-embeddings><nlp><bert><named-entity-recognition><spacy>","<p>I'm currently exploring the options to extract medical NER specifically for French language. I tried <code>SpaCy</code>'s general French NER but it wasn't helpful to the cause (mainly because of the domain-specific requirements). I assume we cannot use <code>Med7</code> or other English-language specific NER's for this purpose. I'd like to know the options and suggestions on how to proceed. I'd also like to know if <code>BioBERT</code> could come handy for this purpose, particularly by combining it with <code>camemBERT</code> or any other French language models.</p>
<p>If there're no readymade options available, I'm planning to translate French to English and then run the NER. I hope there's some potential for this approach.</p>
","nlp"
"89187","NER with LSTM - How to recognize person names that are not part of the vocabulary?","2021-02-10 07:12:10","","1","514","<nlp><lstm><named-entity-recognition>","<p>I am learning Named Entity Recognition and going through posts similar to this one:</p>
<p><a href=""https://towardsdatascience.com/named-entity-recognition-ner-using-keras-bidirectional-lstm-28cd3f301f54"" rel=""nofollow noreferrer"">Named-Entity Recognition (NER) using Keras Bidirectional LSTM</a></p>
<p>So the sentences are fed into the model as a sequence of integers - every int corresponding to the index in the vocabulary - from what I understand this is how the embedding layer works.</p>
<p>My question is - does that mean the model would not be able to recognize a person's name if it doesn't exist in the vocabulary?</p>
<p>For example, from the sentence:</p>
<p>&quot;John Doe went for a walk&quot;</p>
<p>given John Doe is in the vocabulary, it will be recognized as a person name but the sentence:</p>
<p>&quot;Unknown Name went for a walk&quot;</p>
<p>will not be properly tagged if Unknown Name is not in the vocab?</p>
<p>To me, this would be a little strange as Unknown Name is in the same context as John Doe so I was hoping to be able to somehow tag it properly.</p>
<p>I'm obviously lacking knowledge here so I'd be very grateful for any suggestions and reference materials.</p>
","nlp"
"89022","Cross between an edit distance algorithm and a phonetic algorithm","2021-02-06 13:49:17","89099","2","158","<nlp>","<p>My aim is to find an edit distance algorithm which penalises transformations differently depending on the phonetic context.</p>
<p>Take the Levenshtein algorithm, for example; it penalises the same operation - like a character substitution - equally, regardless of character context.</p>
<p>However, it seems to me that all substitutions/insertions/deletions are not made equal. For example, in <em>some</em> contexts, the transformation <code>C-&gt;K</code> should <strong>not</strong> be as harshly penalised as <code>A-&gt;T</code>.</p>
<p>A phonetic algorithm, like <a href=""https://en.wikipedia.org/wiki/Metaphone"" rel=""nofollow noreferrer"">Double Metaphone</a>, can help here. It maps both <code>kosmetik</code> and <code>cosmetic</code> to the same phonetic key, but it's aggressive and discards information. There's a clear difference between these two strings even though the phonetic key is the same.</p>
<p>Simply taking the Metaphone key and applying an edit distance algorithm is a poor choice for my goal.</p>
<p>Does a suitable method exist here? Or is the best option to manually fiddle with the internals of an edit distance algorithm?</p>
","nlp"
"89014","Are all 110 million parameter in bert are trainable","2021-02-06 05:38:05","89018","1","853","<deep-learning><neural-network><nlp><bert><transformer>","<p>I am trying to understand are all these 110 million parameters trainable of bert uncased model. Is there any non trainable parameters in this image below?</p>
<p>By trainable I understand they are initialized with random weight and during pretraining these weights are backpropagated and updated.</p>
<p><a href=""https://i.sstatic.net/tMIhL.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/tMIhL.png"" alt=""Bert 110 Million Parameters"" /></a></p>
","nlp"
"88984","How does Google's Universal Sentence Encoder deal with out-of-vocabulary terms?","2021-02-05 15:16:34","","1","381","<nlp><word-embeddings>","<p>It seems to output embeddings even for random jibberish, and the similarity is even high for this particular pair of jibberish.</p>
<pre><code>np.inner(embed('sdasdasSda'), embed('sadasvdsaf'))
</code></pre>
<pre><code>array([[0.70911765]], dtype=float32)
</code></pre>
<p>I'm wondering how sentences are tokenized and what preprocessing steps are done internally. Also, how is the embedding model trained? As I understand it, they use a Deep Averaging Network, which is another neural network applied on the average of the individual word embeddings?</p>
","nlp"
"88977","Backpropagation of a transformer","2021-02-05 13:22:11","88978","3","5928","<deep-learning><neural-network><nlp><bert><transformer>","<p>when a transformer model is trained there is linear layer in the end of decoder which i understand is a fully connected neural network. During training of a transformer model when a loss is obtained it will backpropagate to adjust the weights.</p>
<p>My question is how deep the backpropagation is?</p>
<ul>
<li>does it happen only till linear layer weights(fully connected neural net) ?</li>
<li>OR does it extend to all the decoder layer weight matrices(Q,K,V) and Feed forward layers weights?</li>
<li>OR does it extend to the even the encoder + decoder weights ?</li>
</ul>
<p>Please help me with the answer.</p>
","nlp"
"88959","Python stemmer for Georgian","2021-02-05 07:06:47","88962","6","694","<python><nlp><python-3.x><stemming>","<p>I am currently working with <strong>Georgian texts processing</strong>. Does anybody know any <strong>stemmers</strong>/<strong>lemmatizers</strong> (or other NLP tools) for Georgian that I could use with <strong>Python</strong>.</p>
<p>Thanks in advance!</p>
","nlp"
"88920","How to stem plural words properly?","2021-02-04 14:30:07","","0","104","<nlp><information-retrieval><indexing>","<p>I'm looking for a way to avoid removing ending <code>s</code> when <code>s</code> isn't a suffix. In order to do that, I first check if a word exists in my index, if it does, I don't remove the ending <code>s</code> but If it doesn't, I go on and remove the ending <code>s</code> and add it to the index. But the problem is what to do when starting to build the index.</p>
<p>Imagine we encounter <code>books</code>, I remove <code>s</code> and add <code>book</code> to my index. On the other hand, I may encounter <code>dangerous</code> for the first time, since it doesn't exists in my index yet, I remove <code>s</code> and add <code>dangerou</code> which is obviously wrong. What should I do?</p>
<p>Specifically I'm looking for ways to properly detect if suffixes and prefixes are indeed one or part of the original word. one way that comes to my mind is using a formal dictionary and instead of my own index, check the words in that dictionary.</p>
<p>P.S: I'm not working on English docs. It's a college/prototype thing Therefore I'm looking for general, good ideas with good accuracy. I'm not looking for advanced stuff with superb accuracy and considerable complexity.</p>
","nlp"
"88911","Topic modelling with many synonyms - how to extract 'latent themes'","2021-02-04 09:49:48","","0","409","<nlp><topic-model><lda>","<p>Here's my corpus</p>
<pre><code>{
    0: &quot;dogs are nice&quot;,       # canines are friendly
    1: &quot;mutts are kind&quot;,      # canines are friendly
    2: &quot;pooches are lovely&quot;,  # canines are friendly
    ...,
    3: &quot;cats are mean&quot;,         # felines are unfriendly
    4: &quot;moggies are nasty&quot;,     # felines are unfriendly
    5: &quot;pussycats are unkind&quot;,  # felines are unfriendly
}
</code></pre>
<p>As a human, the general topics I get from these documents are that:</p>
<ul>
<li>canines are friendly (0, 1, 2)</li>
<li>felines are not friendly (3, 4, 5)</li>
</ul>
<p>But how can a machine find the same conclusion?</p>
<p>If I were to do a Latent Dirichlet Allocation approach, I feel like it would struggle to find topics because the synonyms are 'diluting' the underlying meaning. For example:</p>
<ul>
<li>&quot;dogs&quot;, &quot;pooches&quot;, and &quot;mutts&quot; could all fall under &quot;canines&quot;</li>
<li>&quot;nice&quot;, &quot;kind&quot;, and &quot;lovely&quot; could all fall under &quot;friendly personality trait&quot;</li>
</ul>
<p>Is there a way where I can use an already-trained set of latent vectors (e.g., <a href=""https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit"" rel=""nofollow noreferrer"">Google News-vectors-negative300.bin.gz</a>) to represent each document in these broader entities, and <em>then</em> find the topics? (i.e., instead of using the 'raw' words)</p>
<p>Does that even make sense?</p>
<hr />
<p>EDIT: Come to think of it, I think my question essentially boils down to: <strong>is it possible to replace/redefine a set of similar-meaning words with a single all-encompassing word?</strong></p>
","nlp"
"88891","Clustering together words that appear together while down weighting words that appear too often","2021-02-04 01:05:13","","0","115","<machine-learning><nlp><clustering><word2vec>","<p>I was wondering if I could get some help finding a good model for the problem I have. I have a data set where each observation is a set of words that go together. So for example, it could be:</p>
<p>Obseration 1: {car, pizza, exhaust, engine}</p>
<p>Observation 2: {car, pizza, engine}</p>
<p>Observation 3: {food, pizza, chips}</p>
<p>.
.
.</p>
<p>Observation x: {ballons, air, pizza}</p>
<p>Observation x + 1: {car, exhaust}</p>
<p>I am trying to find a model that, when given a word (for example, &quot;car&quot;), it returns the words that are most commonly used with that word. One way to do this is to use cosine similarity however, there is an additional constraint I am trying to handle. In the example above, the word &quot;pizza&quot; is a word that's super common in a lot of observations. The thing is that, because it's common with so many observations and topics, I don't want to include it as one of the synonymous words that get shown or, at the very least, decrease the probability that it gets selected via some sort of weighting method.</p>
<p>Essentially, I am looking for words that go together often with each other but don't go together universally with many other words (if that makes sense).</p>
<p>Any models you guys have in mind? I would really appreciate the help! I have thought of doing something like a zipf's law weighting of some sort where the most common words are down weighted based inversely on how frequently they show up but wondering if there are some machine learning methods that are just built for this already!</p>
<p>Thank you for any responses!</p>
","nlp"
"88834","Word2Vec vs. Doc2Vec Word Vectors","2021-02-02 15:47:52","","2","141","<nlp><word2vec><doc2vec>","<p>I am doing some analysis on document similarity and was also interested in word similarity. I know that doc2vec inherits from word2vec and by default trains using word vectors which we can access.</p>
<p>My question is:</p>
<p>Should we expect these word vectors and by association any of the methods such as most_similar to be 'better' than word2vec or are they essentially going to be the same? If in the future I only wanted word similarity should I just default to word2vec?</p>
","nlp"
"88824","Unigram tokenizer: how does it work?","2021-02-02 13:28:18","88831","5","4302","<nlp><transformer><tokenization>","<p>I have been trying to understand how the unigram tokenizer works since it is used in the sentencePiece tokenizer that I am planning on using, but I cannot wrap my head around it.</p>
<p>I tried to read the original paper, which contains so little details that it feels like it's been written explicitely not to be understood. I also read several blog posts about it but none really clarified it (one straight up admitted not undertanding it completely).</p>
<p>Can somebody explain it to me? I am familiar with the EM algorithm but I cannot see how it related to the loss function in order to find the subwords probabilities...</p>
","nlp"
"88823","Transformers understanding","2021-02-02 12:45:33","88884","-2","163","<keras><nlp><transformer>","<p>i have i a big trouble. I don't understand transformers. I understand embedding, rnn's, GAN's, even Attention. But i don't understand transformers. Approximately 2 months ago i decided to avoid usage of transformers, because i found them hard. But i can't anymore avoid transformers. Please, help me. I want to use and understand work of transformers. How can i start to work with them?Past the fact that i want to understand their idea in general, i also want to can write/implement them using keras/tensorflow
Of course i tied to read some tutorials.  But i don't understand them anyway.</p>
","nlp"
"88814","Parse documents to obtain subjective sentiment","2021-02-02 01:13:24","","0","43","<nlp><sentiment-analysis><kaggle>","<p>I'm working on a project which deals with MRC (Machine Reading Comprehension).<br />
I would like a machine to read an article and give me the sentiments based on a provided token.</p>
<blockquote>
<p>For Instance given the input:  &quot;Jackson Heights should see an increase in real estate, While other areas of brookln should see a steep decline&quot;.</p>
</blockquote>
<blockquote>
<p>Given the token: &quot;Jackson Heights&quot; Should return a score indicating positive sentiment: e.g .72</p>
</blockquote>
<blockquote>
<p>Given the token: &quot;Brooklyn&quot; Should return a score indicating negative sentiment: e.g -.72</p>
</blockquote>
<p>I've tried to solve this problem in several different ways.  I've been following along with a <a href=""https://colab.research.google.com/drive/1SRdc76H4KmyQp7ho_NyKD5mN2JWYTuSW?usp=sharing"" rel=""nofollow noreferrer"">tutorial for sentiment analysis</a>.  I've linked to the corresponding jupyter notebook.</p>
<p>The issue is that this &quot;Sentiment Analysis&quot; doesn't really do much for multiple reasons.</p>
<ol>
<li>Sentiment analysis removes stop words such as weren't and were.   So the statement &quot;I would buy google stock&quot; and the statement &quot;I wouldn't buy google stock&quot; would evaluate to the same sentiment</li>
<li>Sentiment analysis is only per text so understanding what subject applies to sentiment is important.  e.g &quot;I love chicken and I hate fish&quot; may return a negative sentiment because neural nets have a tendency to put a stronger weight on statements closer to the end.   However the sentiments to be tokenized based on the subject.</li>
</ol>
<p>I've investigate a few open source technologies for this such as <a href=""https://demo.allennlp.org/reading-comprehension/bidaf-elmo"" rel=""nofollow noreferrer"">AllenNLP</a> However, this didn't seem promising because the model assumes the question can be answered.
Given the text: &quot;The price of GOOG should increase by 3% next week&quot;
and a question &quot;How much will Yahoo increase&quot;
the output still highlights &quot;3%&quot; instead of returning nothing.</p>
<p>After I investigated  a simple Question answering bot using <a href=""https://www.kaggle.com/seesee/submit-full"" rel=""nofollow noreferrer"">Kaggle</a>.<br />
This seems a bit more robust but it seems a bit overkill, I don't want to investigate a complicated AI model if there is a simpler solution.</p>
<p>To put the question in its simplest form.</p>
<p>I would like to parse a document into a list of tokens with their sentiment score.  However the solutions i've tried, remove important information, and full NLP seems to be a bit overkill.</p>
<p>could someone outline an simple efficient process for obtaining tokenized sentiment analysis?</p>
","nlp"
"88805","How to preprocess with NLP a big dataset for text classification","2021-02-01 20:38:58","88812","5","990","<python><nlp><text-classification>","<h2>TL;DR</h2>
<p>I've never done <code>nlp</code> before and I feel like I'm not doing it in the good way. I'd like to know if I'm really doing things in a bad way since the beginning or there's still hope to fix those problems mentioned later.</p>
<h2>Some basic info</h2>
<p>I'm trying to do some binary text classification for a university task and I'm struggling at the classification because the preprocessing with NLP is not being the best.</p>
<p>First of all, it's important to note that I need to have efficiency in mind when designing things because I'm working with very large datasets (&gt;1M texts) that are loaded in memory.</p>
<p>This datasets contains data related to new articles with <code>title</code>, <code>summary</code>, <code>content</code>, <code>published_date</code>, <code>section</code>, <code>tags</code>, <code>authors</code>...</p>
<p>Also, it's important to mention that as this task being part of a learning process I'm trying to create everything by myself instead of using external libraries (only for boring or complex tasks)</p>
<h2>Procedure</h2>
<p>The basic procedure for the NLP preprocessing is:</p>
<ol>
<li>Feature extraction -&gt; str variable with <code>title</code>, <code>summary</code> and <code>content</code> attributes joined in the same string</li>
<li>Lemmatization -&gt; same str as input but with lemmatized words</li>
<li>Stopword filtering</li>
<li>Corpus generation -&gt; <code>dict</code> object with lemmatized words as key and the index they're being inserted in the dictionary as value.</li>
</ol>
<p>After generating the corpus with all those samples, we can finally safely vectorize them (which is basically the same process as above but without the building corpus step).</p>
<p>As you might guess, I'm not strictly following the basic <code>bag of words (BOW)</code> idea since I need to relieve memory consumption so it raises two problems when trying to work with AI algorithms like <code>DecisionTreeClassifier</code> from sci-kit.</p>
<h2>Problems</h2>
<p>Some of the problems I've observed till the moment are:</p>
<ul>
<li>Vectors generated from those texts needs to have the same dimension <code>Does padding them with zeroes make any sense?</code></li>
<li>Vectors for prediction needs also to have the same dimension as those from the training</li>
<li>At prediction phase, those words that hasn't been added to the corpus are ignored</li>
<li>Also, the vectorization doesn't make much sense since they are like <code>[0, 1, 2, 3, 4, 1, 2, 3, 5, 1, 2, 3]</code> and this is different to <code>[1, 0, 2, 3, 4, 1, 2, 3, 5, 1, 2, 3]</code> even though they both contain the same information</li>
</ul>
","nlp"
"88783","""Rare words"" on vocabulary","2021-02-01 08:15:19","88785","1","187","<deep-learning><nlp><sentiment-analysis>","<p>I am trying to create a sentiment analysis model and I have a question.</p>
<p>After I preprocessed my tweets and created my vocabulary I've noticed that I have words that appear less than 5 times in my dataset (Also there are many of them that appear 1 time). Many of them are real words and not gibberish. My thinking is that if I keep those words then they will get wrong &quot;sentimental&quot; weights and gonna make my model worse.
Is my thinking right or am I missing something?</p>
<p>My vocab size is around 40000 words and those that are &quot;rare&quot; are around 10k.Should I &quot;sacrifice&quot; them?</p>
","nlp"
"88779","How to remove irrelevant text data from a large dataset","2021-02-01 04:29:59","","0","421","<machine-learning><nlp><data-cleaning><text>","<p>I am working on a ML project where data were coming from a social media, and the topic about the data should be depression under Covid-19. However, when I read some of the data retrieved, I noticed that even though the text (around 1-5 %) mentioned some covid-related keywords, the context of those texts are not actually about the pandemic, they are telling a life story (from 5-year-old to 27-year-old) instead of how covid affects their lives.<br>
The data I want to use and am looking for is some texts that tell people how covid makes depression worse and what not.<br>
Is there a general way to clean those irrelevant data whose contexts are not covid-related (or outliers)?<br>
Or is it ok to keep them in the dataset since they only count for 1-5% ? <br></p>
","nlp"
"88722","Printing the tweets that were incorrectly predicted after applying a machine learning classifier","2021-01-30 21:57:26","","-1","31","<scikit-learn><tensorflow><nlp><random-forest><confusion-matrix>","<p>I applied the random forest classifier to my csv file to classify the tweets as spam or not spam and after an accuracy of 93%, when I printed the confusion matrix I got
[[1068  105]
[  65 1262]].
Now I would like to print the 65 false negative tweets and the 105 false positive tweets but I am unable to do that. I tried to print the y_test and y_predict but it is difficult to map the incorrectly identified tweets. Has anyone please got any advice on this?</p>
<p>RandomForest = RandomForestClassifier()
RandomForest.fit(x_train, y_train)
y_predict = RandomForest.predict(x_test)
print(confusion_matrix(y_test,y_predictRF))</p>
<p>Thank you</p>
","nlp"
"88680","What is the difference between CountVectorizer() and Tokenizer() or are they the same?","2021-01-29 20:07:52","88875","2","2411","<keras><scikit-learn><nlp>","<pre><code>from sklearn.feature_extraction.text import CountVectorizer

from keras.preprocessing.text import Tokenizer
</code></pre>
<p>I am going through some NLP tutorials and realised that some tutorials use CountVectrizer and some use Tokenizer. From my understanding, I thought that they both use one-hot encoding but someone please clarify this.</p>
<p>What I don't understand is why CountVectorizer is not used on Deep Learning models such as RNN and Tokenizer() is not used on ML Classifiers such as SVM, Naive Bayes.</p>
<p>Thank you</p>
","nlp"
"88520","Best approach to create a text classification model with two inputs?","2021-01-26 20:24:17","","0","367","<keras><nlp><text-classification>","<p>I'm looking to train a model with two text inputs (sentences) and a binary classification. Essentially, for 2 given sentences, are they paraphrases or not.</p>
<p>I want to use the Microsoft research Paraphrase Detection dataset as I feel this would be a good place to start.</p>
<p>What would be the best approach to do this?</p>
<p>Thanks in advance, and sorry for my naïvete.</p>
","nlp"
"88504","Transformer architecture question","2021-01-26 15:01:45","88521","0","66","<neural-network><deep-learning><nlp><transformer><attention-mechanism>","<p>I am hand-coding a transformer (<a href=""https://arxiv.org/pdf/1706.03762.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1706.03762.pdf</a>) based primarily on the instructions I found at this blog: <a href=""http://jalammar.github.io/illustrated-transformer/"" rel=""nofollow noreferrer"">http://jalammar.github.io/illustrated-transformer/</a>.</p>
<p>The first attention block takes matrix input of the shape [words, input dimension] and multiplies by the attention weight matrices of shape [input dimension, model dimension]. The model dimension is chosen to be less than the input dimension and is the dimension used as output in all subsequent steps.</p>
<p>There is a residual connection around the attention block and the input is meant to be added to the output of the attention block. However the output of the attention block is shape [words, model dimension] and the input is form [words, input dimension]. Should I interpolate the input down to the model dimension as is done in ResNet? Or maybe add another weight matrix to transform the input?</p>
<p><a href=""https://i.sstatic.net/dI4Al.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/dI4Al.png"" alt=""enter image description here"" /></a></p>
","nlp"
"88485","Extracting events with attributes from unstructured text","2021-01-26 09:27:42","","2","191","<nlp><text-mining><information-extraction>","<p>I am scraping websites of organisations (mostly retailers) and I want to use NLP to extract information from the websites’ unstructured text. The first thing I want to do is to identify covid-related <strong>events</strong> in the text, for example “The shop will be closed from the 3rd of March” or “Unfortunately we have to close permanently.” The lexicon is rather limited, involving perhaps a few dozen (or hundreds at most) phrases/expressions.</p>
<p>I am very familiar with <strong>regular expressions</strong>, and I think it is possible to use a rule-based approach to extract some events and their attributes (e.g., dates), particularly with a small lexicon. However, the limitations of the rules are obvious (it is easy to miss expressions with small variations), and I would like to use also some ML approaches. I am familiar with ML approaches like sentiment analysis and topic modelling, but they seem to be designed for classification problems, rather than on this kind of extraction of specific attributes and data points from text. I also know <strong>NER</strong> that would work well to get dates and place names, for example, but not for events (e.g., closure of a shop x at date y).</p>
<p>Are there smarter ways to do this kind of NLP, going beyond the manual definition of several RegEx? Perhaps a lexical pattern learning from annotated examples?</p>
","nlp"
"88419","Learning words embedding for bigrams and unigrams in a corpus","2021-01-24 20:28:36","","2","783","<text-mining><word-embeddings><nlp>","<p>I am working on a topic modeling for tweets projects. I have generated my topics using both unigrams and bigrams. Topics are defined with a mixture of both bigrams and unigrams.</p>
<p>Now I am planning to evaluate the coherence of my generated topics with the coherence measure I am trying to use is the sum of pair-wise similarity of terms that define the topics.</p>
<p>To compute that measure as you may see I need embeddings for my terms, and that is where the issue came from.</p>
<p>I have trained Fasttext on my corpus to learn the word embedding but it only gives me the embedding for unigrams and not bigrams.</p>
<p>My first question is how to train the embedding to learn both bigrams and unigrams embedding?</p>
<p>I found <a href=""https://github.com/epfml/sent2vec#train-a-new-sent2vec-model"" rel=""nofollow noreferrer"">some research</a> where they include n-grams to improve word embedding but I can't see any approach where they output embedding for bi_grams.</p>
<p>How should I process my text so that I can learn the embdeedings?</p>
<p>I found the <code>Phrase</code> and <code>Phraser</code> <a href=""https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.Phrases"" rel=""nofollow noreferrer"">classes</a> from Gensim but they are not returning all possibles embedding and I can't seem to understand how they are doing it.</p>
<p>Which another approach should I use?</p>
<ul>
<li>Can I split each sentence in my sentence to included both unigrams and bigrams and learn the embedding from them?</li>
</ul>
<p>Ex: These are sentences about words embeddings  === should be split into :</p>
<p><code>these, these_are, are,  are_sentence, sentence,  sentence_about, about, about_words, words words_enmbeddigns</code> and learn the embedding from that?</p>
<ul>
<li>Each sentence can be split into one for unigrams and another one for bigrams and combine both and train the model on the combination? to</li>
</ul>
<p>Ex: <code>These are sentences about words embeddings</code> should be split into :</p>
<ul>
<li><p>these, are, sentences, about, words, embeddings and</p>
</li>
<li><p>these_are, are_sentences, sentences_about, about_words ,words_embeddings</p>
</li>
<li><p>Or simpler why not considering the bigrams embedding as the average of the unigrams embeddings?</p>
</li>
</ul>
<p>All ideas on how to tackle this are welcomed...</p>
<p>Thanks</p>
","nlp"
"88417","what's the motivation behind BERT masking 2 words in a sentence?","2021-01-24 20:03:49","88423","1","462","<nlp><bert><language-model>","<p><a href=""https://arxiv.org/pdf/1810.04805.pdf"" rel=""nofollow noreferrer"">bert</a> and the more recent <a href=""https://arxiv.org/pdf/1910.10683.pdf"" rel=""nofollow noreferrer"">t5</a> ablation study, agree that</p>
<blockquote>
<p>using a <strong>denoising</strong> objective always results in better downstream task performance compared to a language model</p>
</blockquote>
<p>where denoising == masked-lm == cloze.</p>
<p>I understand why learning to represent a word according to its bidirectional surroundings makes sense. However, I fail to understand why is it beneficial to learn to mask 2 words in the same sentence, e.g. <code>The animal crossed the road</code> =&gt; <code>The [mask] crossed the [mask]</code>. Why does it make sense to learn to represent <code>animal</code> without the context of <code>road</code>?</p>
<p>Note: I understand that the masking probability is 15% which corresponds to 1/7 words, which makes it pretty rare for 2 words in the same sentence to be masked, but why would it <strong>ever</strong> be beneficial, even with low probability?</p>
<p>Note2: please ignore the masking procedure sometimes replacing mask with a random/same word instead of <code>[mask]</code>, T5 investigates this choice in considerable length and I suspect that it's just an empirical finding :)</p>
","nlp"
"88393","How to evaluate the quality of speech-to-text data without access to the true labels?","2021-01-24 01:12:46","","2","261","<nlp><text-mining><transformer><speech-to-text>","<p>I am dealing with a data set of transcribed call center data, where customers are being recorded when interacting with the agent. This is then automatically transcribed by an external transcription system. I want to automatically assess the quality of these transcriptions.</p>
<p>Sadly, the quality seems to be disastrous. In some cases it's little more than gibberish, often due to different dialects the machine is not able to handle. We have no access to the original recordings (data privacy), so there is no way whatsoever to get or create the true labels. The system cannot be replaced as we are committed to it.</p>
<p>Again to the question: <strong>is there any way to automatically assess the quality of transcriptions with NLP methods?</strong> We want to quantify and compare transcription quality to filter out the best samples for semantic inference of our customers' input in a downstream task. I am thinking about something like a coherence measure in order to find the sentences which make the most sense, grammatically or semantically. Sadly, things as BLEU, WER or Rouge do not work in this case.</p>
<p>I'd be grateful for anything pointing in the right direction. Most importantly again, we have no labels and it needs to be scalable.</p>
<p>Thanks a lot!</p>
","nlp"
"88364","Pytorch CTC Loss Unexpected Behaviour","2021-01-23 07:17:57","","1","74","<nlp><pytorch>","<p>I have used the following code to test the behaviour of CTC loss.</p>
<pre><code>from torch.nn import CTCLoss
import torch.nn.functional as fn
import torch
def get_char_maps (vocab):
    char_to_index={}
    index_to_char={}
    cnt=0
    for c in vocab:
        char_to_index[c]=cnt
        index_to_char[cnt]=c
        cnt+=1
    vocab_size=cnt
    return (char_to_index, index_to_char, vocab_size)

loss_function = CTCLoss()
char_to_index, index_to_char, vocab_size = get_char_maps(['~', 'a', 'b', 'c', 'd', 'e', 'f', 'g', ' '])

empty_char = '~'
label = 'abc'
seq_len = 20
predict = 'abc'
pred_out = predict[0]
for i in range(1, len(predict)):
    if predict[i] == predict[i - 1]:
        pred_out += empty_char + predict[i]
    else:
        pred_out += predict[i]
print(pred_out)
predict = pred_out
label_len = len(label)
left_side = (seq_len - len(predict))//2
right_side = seq_len - left_side - len(predict)
seq_predict = [empty_char]*left_side + list(predict) + [empty_char]*right_side

out = []
for char in seq_predict:
    temp = [0]*vocab_size
    temp[char_to_index[char]] = 1
    out.append([temp])
out = torch.FloatTensor(out)

scores = fn.log_softmax(out, dim=2)
out_size = torch.tensor([seq_len]*1, dtype=torch.int)
y_size = torch.tensor([len(label)], dtype=torch.int)
y = [char_to_index[c] for c in label]
y_var = torch.tensor(y, dtype=torch.int)
l = loss_function(scores, y_var, out_size, y_size)
print(l.item())
</code></pre>
<p>With this code, when the target is <code>abc</code> and the prediction is also <code>abc</code>, it produces a loss of <code>7.770</code>. However with the same target when the prediction is g, it produces a loss of <code>7.654</code>. Which is lower than the perfect prediction. This behaviour seems counterintuitive.
Is this the expected behaviour from CTC loss? Or is this test code has an error?</p>
","nlp"
"88337","what is the training phase in N-gram model?","2021-01-22 13:09:59","","1","145","<nlp><ngrams>","<p>Following is my understanding of N gram model used in text prediction case :</p>
<p>Given a sentence say, &quot; I love my &quot; (say N = 1 /bigram),  using N gram and say 4 possible candidates ( country, family, wife, school) I can estimate the conditional probability on each of the candidates and take the one with highest probability as the next word.</p>
<p>Question :</p>
<ol>
<li><p>I understand the probability part of the model but to even get to the probability, we need the possible candidates ( next words, in this case family, wife, school, country). How does the model choose the candidates</p>
</li>
<li><p>Most of the articles online talk about the probability part but doesn't mention anything about training phase. What exactly is happening in training phase of this model?</p>
</li>
</ol>
","nlp"
"88330","How do the linear layers in the attention mechanism work?","2021-01-22 11:45:58","88332","1","3058","<machine-learning><nlp><transformer><attention-mechanism>","<p>I think I now the answer to my question but I dont really get confirmation.
When taking a look at the multi-head-attention block as presented in &quot;Attention Is All You Need&quot; we can see that there are three linear layers applied on the key, query and value matrix. And then one layer at the end, which is applied on the output of the matrix multiplication of the score matrix an the value.</p>
<p>The three linear layers at the beginnging: When the key/query/value with shape (seq-len x emb-dim) enter the linear layer the output is still (seq-len x emb-dim). Does that mean, the same linear layer is applied on every &quot;index&quot; of the input matrix. Like this (pseudo-code):</p>
<pre class=""lang-py prettyprint-override""><code>fc = linear(emb-dim, emb-dim) # in-features and out-features have the shape of emb-dim
output_matrix = []

for x in key/query/value:
    # x is one row the input matrix with shape (emb-dim x 1)
    x = fc(x)
    # x after the linear layer has still the shape of (emb-dim x 1)
    output_matrix.append(x)

# output_matrix has now the shape (seq-len x emb-dim); the same as the input-matrix
</code></pre>
<p>So is this indeed what happens? I couldn't explain why the output is the same as the input otherwise.</p>
<p>The linear layer before the output: So the output of the matrix multiplication of the score matrix an the value is also (seq-len x emb-dim) and therefore the output of the linear layer is too. So the output of the whole attention block has the same shape as the input.</p>
<p>So Im just asking for comfirmation if the explaination I wrote is correct. And if not: What am I understanding wrong?</p>
<p>Extra question: When I want to further use the output of the attention block further for classification, I would have to take the mean along the seq axis in order to get a vector of fixed shape (emb-dim x 1) so I can feed it into a classification layer. But I guess that valueable information is getting lost in that process.
My question: Could I replace the last linear layer with an RNN to get the desired output shape and without losing information?</p>
","nlp"
"88326","How to access GPT-3, BERT or alike?","2021-01-22 10:03:52","88327","1","867","<nlp><gpt><pretraining>","<p>I am interested in accessing NLP models mentioned in scientific papers, to replicate some results and experiment.</p>
<p>But I only see waiting lists <a href=""https://openai.com/blog/openai-api/"" rel=""nofollow noreferrer"">https://openai.com/blog/openai-api/</a> and licenses granted in large commercial deals <a href=""https://www.theverge.com/2020/9/22/21451283/microsoft-openai-gpt-3-exclusive-license-ai-language-research"" rel=""nofollow noreferrer"">https://www.theverge.com/2020/9/22/21451283/microsoft-openai-gpt-3-exclusive-license-ai-language-research</a> .</p>
<p>How can a researcher not affiliated to a university or (large) tech company obtain access so to replicate experiments of scientific papers ?</p>
<p>Which alternatives would you suggest to leverage on pre-trained data sets ?</p>
","nlp"
"88288","Get most likely topic per document in pandas dataframe using gensim","2021-01-21 13:26:00","","0","1895","<python><nlp><topic-model><gensim>","<p>I am using gensim LDA to build a topic model for a bunch of documents that I have stored in a pandas data frame. Once the model is built, I can call <code>model.get_document_topics(model_corpus)</code> to get a list of list of tuples showing the topic distribution for each document. For example, when I am working with 20 topics, I might get the following for the first three documents in my data frame:</p>
<pre><code>[(5, 0.11253482), (7, 0.75876033)]
[(19, 0.96343607)]
[(0, 0.010002977),
 (1, 0.010002977),
 (2, 0.010002977),
 (3, 0.010002979),
 (4, 0.8099435),
 (5, 0.010002977),
 (6, 0.010002977),
 (7, 0.010002977),
 (8, 0.010002977),
 (9, 0.010002977),
 (10, 0.010002977),
 (11, 0.010002977),
 (12, 0.010002977),
 (13, 0.010002977),
 (14, 0.010002977),
 (15, 0.010002977),
 (16, 0.010002977),
 (17, 0.010002977),
 (18, 0.010002977),
 (19, 0.010002977)]
</code></pre>
<p>This means that the most likely topic for document_1 is 7, for document_2 is 19, and for document_3 is 4. The primary output that I would like to see is simply this most likely topic for each document. The way I'm doing this now is using a loop:</p>
<pre><code>import numpy as np
import pandas as pd

def get_max(doc):
        idx,l = zip(*doc)
        return idx[np.argmax(l)]

data['doc_topic'] = [get_max(doc) for doc in model.get_document_topics(model_corpus)]
</code></pre>
<p>I have around 80k documents in my data frame, so this code takes about 45 seconds to execute. But since gensim has already done all the computations, I keep thinking that that 45 seconds of computational time is simply spent on reorganizing data, so there must be a more efficient way of doing this.</p>
<p>If possible, a secondary output that would be nice to have is the document-topic matrix, such that each row corresponds to a document in my data frame, and each column represents the probability (or similarity) of the document to the topic. So this would yield a DxT matrix, where D is the number of documents, and T is the number of topics.</p>
","nlp"
"88177","Machine translator giving only on character as output for all the words","2021-01-19 07:53:36","","1","26","<machine-learning><python><tensorflow><nlp>","<p>I had trained a machine translation model using 1.4 million examples.( hindi to english ). strangely the loss plateaus at the same point 0.8301 and while trying the inference the output is always the same no matter what the input is. i am using GRU both in encoder and decoder.</p>
<pre><code>class Encoder(tf.keras.Model):
def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):
    super(Encoder, self).__init__()
    self.batch_sz = batch_sz
    self.enc_units = enc_units
    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)
    self.gru = tf.keras.layers.GRU(self.enc_units,
                                return_sequences=True,
                                return_state=True,
                                recurrent_initializer='glorot_uniform')
# @tf.function(
#     input_signature=[[
#         tf.TensorSpec(shape=[None, 55], dtype=tf.int32), 
#         tf.TensorSpec(shape=[None, embedding_dim], dtype=tf.float32)
#         ]]
# )
# @tf.function
def call(self, inputs):
    x, hidden = inputs
    x = self.embedding(x)
    print(x.shape)
    print(hidden.shape)
    output, state = self.gru(x, initial_state = hidden)
    return output, state

def model(self):
    a = tf.keras.Input(shape=(55))
    b = tf.keras.Input(shape=(256))
    return tf.keras.Model(inputs=[a, b], outputs=self.call(a, b))

def initialize_hidden_state(self):
    return tf.zeros((self.batch_sz, self.enc_units))

class BahdanauAttention(tf.keras.layers.Layer):
def __init__(self, units):
    super(BahdanauAttention, self).__init__()
    self.W1 = tf.keras.layers.Dense(units)
    self.W2 = tf.keras.layers.Dense(units)
    self.V = tf.keras.layers.Dense(1)

def call(self, query, values):
    # query hidden state shape == (batch_size, hidden size)
    # query_with_time_axis shape == (batch_size, 1, hidden size)
    # values shape == (batch_size, max_len, hidden size)
    # we are doing this to broadcast addition along the time axis to calculate the score
    query_with_time_axis = tf.expand_dims(query, 1)

    # score shape == (batch_size, max_length, 1)
    # we get 1 at the last axis because we are applying score to self.V
    # the shape of the tensor before applying self.V is (batch_size, max_length, units)
    score = self.V(tf.nn.tanh(
        self.W1(query_with_time_axis) + self.W2(values)))

    # attention_weights shape == (batch_size, max_length, 1)
    attention_weights = tf.nn.softmax(score, axis=1)

    # context_vector shape after sum == (batch_size, hidden_size)
    context_vector = attention_weights * values
    context_vector = tf.reduce_sum(context_vector, axis=1)

    return context_vector, attention_weights

class Decoder(tf.keras.Model):
def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):
    super(Decoder, self).__init__()
    self.batch_sz = batch_sz
    self.dec_units = dec_units
    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)
    self.gru = tf.keras.layers.GRU(self.dec_units,
                                return_sequences=True,
                                return_state=True,
                                recurrent_initializer='glorot_uniform')
    self.fc = tf.keras.layers.Dense(vocab_size, activation='softmax')

    # used for attention
    self.attention = BahdanauAttention(self.dec_units)

# @tf.function(
#     input_signature=[[
#         tf.TensorSpec(shape=[None, 1], dtype=tf.int32), 
#         tf.TensorSpec(shape=[None, embedding_dim], dtype=tf.float32),
#         tf.TensorSpec(shape=[None, 55, embedding_dim], dtype=tf.float32)
#         ]]
# )
# @tf.function
def call(self, inputs):
    x, hidden, enc_output = inputs
    # enc_output shape == (batch_size, max_length, hidden_size)
    context_vector, attention_weights = self.attention(hidden, enc_output)

    # x shape after passing through embedding == (batch_size, 1, embedding_dim)
    x = self.embedding(x)

    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)
    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)

    # passing the concatenated vector to the GRU
    output, state = self.gru(x)

    # output shape == (batch_size * 1, hidden_size)
    output = tf.reshape(output, (-1, output.shape[2]))

    # output shape == (batch_size, vocab)
    x = self.fc(output)

    return x, state, attention_weights

def model(self):
    a = tf.keras.Input(shape=(1))
    b = tf.keras.Input(shape=(256))
    c = tf.keras.Input(shape=(55, 256))
    return tf.keras.Model(inputs=[a, b, c], outputs=self.call(a, b, c))

BATCH_SIZE = 8

embedding_dim = 256
units = 256
vocab_inp_size = len(inp_lang.word_index)+1
vocab_tar_size = len(targ_lang.word_index)+1

target_tensor_train)).shuffle(BUFFER_SIZE)
trainTextGenerator = DataGenerator().sequentialImageGenerator('training',trainData,trainTarget,BATCH_SIZE)
testTextGenerator = DataGenerator().sequentialImageGenerator('testing',testData,testTarget,BATCH_SIZE)
</code></pre>
<p>what are the possible reasons due to which the output is same all the time?</p>
","nlp"
"88097","why do transformers mask at every layer instead of just at the input layer?","2021-01-17 20:04:01","88128","1","1009","<neural-network><nlp><transformer>","<p>working thru the <a href=""http://nlp.seas.harvard.edu/2018/04/03/attention"" rel=""nofollow noreferrer"">annotated transformer</a>, I see that every layer in both the encoder (mask paddings) and decoder (mask padding + future positions) get masked. Why couldn't it be simplified to just one mask at the first layers of encoder and decoder?</p>
","nlp"
"88087","Which would be an ideal model to get a specific sub string from a bigger string?","2021-01-17 14:44:47","","0","30","<nlp><tfidf><automatic-summarization>","<p>I have a corpus of documents whose some lines have information like this:</p>
<p><code>wt 210 1b 14.4 oz (98 kg)</code></p>
<p>or</p>
<p><code>weight: 219 lb (99 kg), height: 5' 1.9&quot; (157 cm)</code></p>
<p>The format of occurrence of such information varies from document to document.
I need the value or the substring corresponding to weight and weight only.
Here are my questions regarding the problem:</p>
<ol>
<li><p>I have certain regexes that can get the weight value for labeling the lines. However, I do not know how to provide a string as the y-axis, should I convert it to TFIDF vector? Won't that make y-axis hyper-dimensional?</p>
</li>
<li><p>My first intuition is to use Extractive summarizer trained on many other such lines. Is there a better way to handle that?</p>
</li>
</ol>
<p>Thank you.</p>
","nlp"
"88072","how is the linear relation between positional encoding helping attention?","2021-01-16 21:00:52","","1","101","<nlp><transformer><attention-mechanism>","<p>I'm reading <a href=""http://nlp.seas.harvard.edu/2018/04/03/attention"" rel=""nofollow noreferrer"">the annotated transformer</a>, and interested in the mechanics behind the <a href=""https://datascience.stackexchange.com/questions/51065/what-is-the-positional-encoding-in-the-transformer-model"">positional encoding</a>. I understand the <a href=""https://timodenk.com/blog/linear-relationships-in-the-transformers-positional-encoding/"" rel=""nofollow noreferrer"">linear relation</a> between position <span class=""math-container"">$t$</span> and position <span class=""math-container"">$t+\phi$</span>, and understand that it is a function only of <span class=""math-container"">$\phi$</span>, and not of <span class=""math-container"">$t$</span>.</p>
<p>However, I'm still not clear on how exactly is this helping the model to attend to phrases such as &quot;they are&quot;, no matter if they appear in position 3 or position 7 in the sentence. The attention layer gets as an input [<span class=""math-container"">$E(they)+PE(3)$</span>, <span class=""math-container"">$E(are)+PE(4)$</span>] where <span class=""math-container"">$PE(4)=T_1PE(3)$</span> and <span class=""math-container"">$T_1$</span> is a nice diagonal linear transformation that depends only on the position diff which is 1. How is it using those nice properties for learning and generalizing for the case where the same phrase &quot;they are&quot; appears in position 7? how is the fact that the linear transformation is nice and diagonal helping the model?</p>
","nlp"
"88041","ValueError: y should be a 1d array, got an array of shape () instead","2021-01-16 01:10:33","","0","1871","<machine-learning><classification><nlp><sentiment-analysis>","<p>I'm using a reviews data and trying to apply classifier model and get prediction. Here is the code i'm trying.</p>
<pre><code>dataset = pd.read_csv('Scraping reviews.csv')
import numpy as np
X = np.linspace(0, 2*np.pi, 8)
y = np.sin(X) + np.random.normal(0, 0.4, 8)
X = X.reshape(-1, 1)

from sklearn.feature_extraction.text import CountVectorizer
count_vect = CountVectorizer()
X_train_counts = count_vect.fit_transform(dataset)
#X_train_counts=X_train_counts.reshape(4,1)
X_train_counts.shape
[out] (2,2)

from sklearn.feature_extraction.text import TfidfTransformer
tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)
X_train_tf = tf_transformer.transform(X_train_counts)
#X_train_tf=X_train_tf.reshape(4,1)
X_train_tf.shape
[out] (2,2)

tfidf_transformer = TfidfTransformer()
X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)
#X_train_tfidf=X_train_tfidf.reshape(4,1)
X_train_tfidf.shape
[out] (2,2)

from sklearn.naive_bayes import MultinomialNB

clf = MultinomialNB().fit(X_train_tfidf, X_train_counts)








 [out] ---------------------------------------------------------------------------
    ValueError                                Traceback (most recent call last)
    &lt;ipython-input-494-7734b71b758f&gt; in &lt;module&gt;
          1 from sklearn.naive_bayes import MultinomialNB
          2 
    ----&gt; 3 clf = MultinomialNB().fit(X_train_tfidf, X_train_counts)
    
    ~\anaconda3\lib\site-packages\sklearn\naive_bayes.py in fit(self, X, y, sample_weight)
        613         self : object
        614         &quot;&quot;&quot;
    --&gt; 615         X, y = self._check_X_y(X, y)
        616         _, n_features = X.shape
        617         self.n_features_ = n_features
    
    ~\anaconda3\lib\site-packages\sklearn\naive_bayes.py in _check_X_y(self, X, y)
        478 
        479     def _check_X_y(self, X, y):
    --&gt; 480         return self._validate_data(X, y, accept_sparse='csr')
        481 
        482     def _update_class_log_prior(self, class_prior=None):
    
    ~\anaconda3\lib\site-packages\sklearn\base.py in _validate_data(self, X, y, reset, validate_separately, **check_params)
        430                 y = check_array(y, **check_y_params)
        431             else:
    --&gt; 432                 X, y = check_X_y(X, y, **check_params)
        433             out = X, y
        434 
    
    ~\anaconda3\lib\site-packages\sklearn\utils\validation.py in inner_f(*args, **kwargs)
         70                           FutureWarning)
         71         kwargs.update({k: arg for k, arg in zip(sig.parameters, args)})
    ---&gt; 72         return f(**kwargs)
         73     return inner_f
         74 
    
    ~\anaconda3\lib\site-packages\sklearn\utils\validation.py in check_X_y(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)
        805                         ensure_2d=False, dtype=None)
        806     else:
    --&gt; 807         y = column_or_1d(y, warn=True)
        808         _assert_all_finite(y)
        809     if y_numeric and y.dtype.kind == 'O':
    
    ~\anaconda3\lib\site-packages\sklearn\utils\validation.py in inner_f(*args, **kwargs)
         70                           FutureWarning)
         71         kwargs.update({k: arg for k, arg in zip(sig.parameters, args)})
    ---&gt; 72         return f(**kwargs)
         73     return inner_f
         74 
    
    ~\anaconda3\lib\site-packages\sklearn\utils\validation.py in column_or_1d(y, warn)
        843         return np.ravel(y)
        844 
    --&gt; 845     raise ValueError(
        846         &quot;y should be a 1d array, &quot;
        847         &quot;got an array of shape {} instead.&quot;.format(shape))
    
    ValueError: y should be a 1d array, got an array of shape () instead.
    
    I tried reshaping X_train_counts, X_train_tf, X_train_tfidf but nothing is working. Please help me with this. Thanks.
</code></pre>
","nlp"
"88034","How can I classify specific types of words in a document given I have the full text of the document and the labels","2021-01-15 23:03:49","88051","1","50","<machine-learning><classification><nlp>","<p>I am working on a project that involves picking out specific kinds of objects from text. The documents I am going though are life sciences and biomedical in nature, and in these documents there are specific biomedical &quot;objects&quot; I want to pick out. The nature and variety of the text objects means I can't use regex or string matching. It has to be some kind of classification.</p>
<p>These text objects can be one word, or multiple words, but they are always in sequence.
An example sentence would be like</p>
<pre><code>During the process of protein synthesis, X was used.
</code></pre>
<p>I need to pick out X. Luckily, I have plenty of labeled documents, and plenty of labels to go along with it. So I know a human can pick out these objects. So the challenge now is to get a machine to be able to pick out these types of objects from unseen text. I am working under the assumption that these specific text objects all fall under somewhat similar grammatical and textual context, so given enough labeled data, a machine should be able to learn how to pick out the text object.</p>
<p>Two Main Questions.</p>
<ol>
<li><p>How do I label specific words in a document such that some model will understand that given a sequence of text, the object at position Y is a labeled and what we should be trying to classify.</p>
</li>
<li><p>Does anything I just said make any sense? Is there any research on what I've been talking about, because I've looked around and have not been able to find much.</p>
</li>
</ol>
","nlp"
"87991","How to find coherence between a large number of sentences","2021-01-15 05:21:34","","1","151","<nlp><text-mining><recommender-system><word-embeddings><similarity>","<p>I have a list of sentences returned as a result of a document search algorithm. I want to determine if the results returned are semantically close/similar/coherent using some sort of metric. For a starting point, I'm using Word Movers Distance (WMD) and calculating the similarity between the sentences. But my list of sentences is too long, and doing a pairwise comparison for all the items within a list (document) would be computationally infeasible. What might be the best way to solve this?</p>
","nlp"
"87981","Extract date/duration from text","2021-01-14 21:55:42","","3","1535","<machine-learning><python><nlp><text><named-entity-recognition>","<p>The text and output to be extracted are similar to the following :</p>
<p>&quot;Check it every two weeks&quot; - two weeks</p>
<p>&quot;Check it on day 1 and day 14&quot; - day 1 and day 14</p>
<p>&quot;day 19 and day fourteen are important&quot; - day 19, day fourteen</p>
<p>&quot;The game is in 6 wks&quot; - 6 weeks</p>
<p>&quot;Check it in 6mo&quot; - 6 months</p>
<p>&quot;days 1 and d14 are important&quot; - day 1, day 14</p>
<p>&quot;Check it on days 11 and 14&quot; - day 11, day 14</p>
<p>&quot;check it on day one and twelve&quot; - day one, day twelve</p>
<p>I have tried using SUTime library to extract the necessary information, but it just works in the case of first example and fails to properly extract information from the most of the other sentences. Considering the many ways in which the same text can be written, it is not very feasible to use ReGex.</p>
<p>I was wondering if there are any feasible NLP/ML solutions to extract this information. If NLP/ML is the best way to solve this, I have two questions: 1. I am not sure if I can use classification models and my first preference was to use a regression model, is it right to do that? 2. In either of the cases, I don't have a lot of labeled data to train. So, it'd be helpful if anyone can let me know if there's any open source labeled data for training+.</p>
","nlp"
"87959","Dot product for similarity in word to vector computation in NLP","2021-01-14 15:55:34","","3","4088","<nlp><word-embeddings><word2vec><similarity><softmax>","<p>In NLP while computing word to vector we try to maximize log(P(o|c)). Where P(o|c) is probability that o is outside word, given that c is center word. <br>
<img src=""https://latex.codecogs.com/gif.latex?P(o|c)&space;=&space;%5Cfrac%7Be%5E%7BU_%7Bo%7D.V_%7Bc%7D%7D%7D%7B%5Csum_%7Bi%7D%5E%7BT%7De%5E%7BU_%7Bi%7D.V_%7Bc%7D%7D%7D"" title=""P(o|c) = \frac{e^{U_{o}.V_{c}}}{\sum_{i}^{T}e^{U_{i}.V_{c}}}"" />
<br>
U<sub>o</sub> is word vector for outside word<br>
V<sub>c</sub> is word vector for center word<br>
T is number of words in vocabulary<br><br>
Above equation is softmax. And dot product of U<sub>o</sub> and V<sub>c</sub> acts as score, which should be higher the better. If words o and c are closer then their dot product should be high, but that is not the case with dot product. Because of following:<br>
Consider vectors <br>A=[1, 1, 1], B=[2, 2, 2], C=[100, 100, 100]<br>
A.B = 1 * 2 + 1 * 2 + 1 * 2 = 6<br>
A.C = 1 * 100 + 1 * 100 + 1 * 100 = 300<br><br>
Vectors A and B are closer compared to A and C, however dot product A.C &gt;A.B <br>
So Dot product is acting as distance and not as similarity measure. Then why is it used in softmax. <br>Please help me make my understanding better</p>
","nlp"
"87943","how to programmatically introduce grammatical errors in sentences","2021-01-14 09:45:41","","3","225","<python><nlp><language-model><grammar-inference>","<p>I've a set of sentences in English language. I'm exploring ways to create a dataset of sentences with grammatical errors programmatically. The following options has been tried out randomly -</p>
<ul>
<li>identify verbs, propositions etc. by POS tagging and change the tense or remove them</li>
<li>change the order of 2 or more words</li>
<li>remove commas, colons, semi-colons etc.</li>
</ul>
<p>These are not always fool-proof. Are there any proven ways to approach this problem?</p>
","nlp"
"87940","How to implement a POS tagger using Word co-occurence and Clustering concepts?","2021-01-14 08:06:41","","1","120","<nlp><data-mining>","<p>I am thinking of implementing a POS tagger by myself. a POS tagger, extracts the syntax role of a word in a sentence.<br />
According to my studies, word co-occurrence is a technique to analyze word occurrence which can be used to construct a graph where nodes are words and the weights between them is their co-occurrence weight.</p>
<p>I am wondering if there is a way to apply clustering algorithms on this graph to group words based on their syntax roles? i mean i want to do the same as a POS tagger does. the main idea is that after constructing the graph of co-occurrence, how can i apply a clustering or community detection algorithm on this graph to group nodes based on their roles in a sentence? for example, to group nodes which are verbs, or group nodes that are nouns. in clustering i don't need to know this is verb or not, i only want to group nodes which have a similar syntax in graph. then i will analyze them to find out they are weather a verb or noun or ...
I would be so thankful if you give mind on starting this project.</p>
","nlp"
"87906","Transformer model: Why are word embeddings scaled before adding positional encodings?","2021-01-13 10:10:24","87909","12","7554","<tensorflow><nlp><transformer><attention-mechanism>","<p>While going over a <a href=""https://www.tensorflow.org/tutorials/text/transformer"" rel=""noreferrer"">Tensorflow tutorial for the Transformer model</a> I realized that their implementation of the Encoder layer (and the Decoder) scales word embeddings by sqrt of embedding dimension <strong>before</strong> adding positional encodings. Notice that this is different from scaling the dot product attention.</p>
<p>I'm referring to the 3rd line of the call method of the Encoder class here: <a href=""https://www.tensorflow.org/tutorials/text/transformer#encoder"" rel=""noreferrer"">https://www.tensorflow.org/tutorials/text/transformer#encoder</a></p>
<pre class=""lang-py prettyprint-override""><code>def call(self, x, training, mask):

  seq_len = tf.shape(x)[1]
  
  # adding embedding and position encoding.
  x = self.embedding(x) # (batch_size, input_seq_len, d_model)
  x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))
  x += self.pos_encoding[:, :seq_len, :]
  
  x = self.dropout(x, training=training)

  for i in range(self.num_layers):
    x = self.enc_layers[i](x, training, mask)

  return x # (batch_size, input_seq_len, d_model)
</code></pre>
<p>I could not find any mention of this scaling in the papers I've read so far. People always show the input to the encoder as WE + PE, that is word embedding plus positional encoding. But this implementation seems to use sqrt(d_model) * WE + PE.</p>
<p>My questions:</p>
<ol>
<li>Have you ever seen this extra scaling step mentioned in a paper? I didn't find it in &quot;Attention is all you need&quot; (Vaswani et. al.).</li>
<li>What is this additional scaling trying to achieve?</li>
</ol>
","nlp"
"87898","How to i get word embeddings for out of vocabulary words using a transformer model?","2021-01-13 07:02:51","","3","1313","<nlp><transformer><stanford-nlp><tokenization><huggingface>","<p>When i tried to get word embeddings of a sentence using bio_clinical bert, for a sentence of 8 words i am getting 11 token ids(+start and end) because &quot;embeddings&quot; is an out of vocabulary word/token, that is being split into em,bed,ding,s.</p>
<p>I would like to know if there is any aggregation strategies available that make sense apart from doing a mean of these vectors.</p>
<pre><code>from transformers import AutoTokenizer, AutoModel
# download and load model
tokenizer = AutoTokenizer.from_pretrained(&quot;emilyalsentzer/Bio_ClinicalBERT&quot;)
model = AutoModel.from_pretrained(&quot;emilyalsentzer/Bio_ClinicalBERT&quot;)

sentences = ['This framework generates embeddings for each input sentence']


#Tokenize sentences
encoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=128, return_tensors='pt')


#Compute token embeddings
with torch.no_grad():
    model_output = model(**encoded_input)

print(encoded_input['input_ids'].shape)
</code></pre>
<p>Output :
torch.Size([1, 13])</p>
<pre><code>for token in encoded_input['input_ids'][0]:
  print(tokenizer.decode([token]))
</code></pre>
<p>Output:</p>
<pre><code>[CLS]
this
framework
generates
em
##bed
##ding
##s
for
each
input
sentence
[SEP]
</code></pre>
","nlp"
"87837","How to handle like meaning sentences when working on text summarization","2021-01-12 08:05:30","","1","30","<nlp><text-generation>","<p>Suppose we have a text like <code>Today is a very bad day. Very bad day is today. I wont come to play.</code></p>
<p>What kind of technique should I use to summarize similar texts like above? From articles, I found over the web till now I think that <em>extractive summarization</em> will give importance to the first two sentences because major key points of the texts(according to the frequency of words) are present in the first two sentences. Similarly, the <em>abstractive summarization</em> technique will also make a summary certainly considering the first two sentences. But, in the ideal case, the third sentence is a must.</p>
<p>What should be my approach considering my text may have many sentences with similar meanings??</p>
<p>I am very new to this any kind of suggestion or help will be great.</p>
","nlp"
"87809","sentence vector to sentence","2021-01-11 15:55:21","","2","148","<nlp>","<p>I have implemented an auto-encoder that takes sentence vectors as input and at decoder the last layer outputs sentence vectors. I would like to convert sentence vectors to sentences. Is there any way to convert sentence vectors to sentences?</p>
<pre><code>class AutoEncoder(nn.Module):

def __init__(self, embedding_dim, hidden_dim):
    
    #Constructor
    super().__init__()          

    self.fc=nn.Linear(embedding_dim,hidden_dim)
    self.act1=nn.ReLU(0.2)
    self.fc1=nn.Linear(hidden_dim,hidden_dim)
    self.act2=nn.ReLU(0.2)
    self.fc2=nn.Linear(hidden_dim,hidden_dim)
    self.act3=nn.ReLU(0.2)
    self.fc3=nn.Linear(hidden_dim,hidden_dim)
    self.act4=nn.ReLU(0.2)
    self.fc4=nn.Linear(hidden_dim,hidden_dim)
    self.act5=nn.ReLU(0.2)
    self.fc5=nn.Linear(hidden_dim,embedding_dim)
def forward(self, X):     
    l1=self.fc(X)
    a1=self.act1(l1)
    l2=self.fc1(a1)
    a2=self.act2(l2)
    l3=self.fc2(a2)
    a3=self.act3(l3)
    l4=self.fc3(a3)
    a4=self.act4(l4)
    l5=self.fc4(a4)
    a5=self.act5(l5)
    l6=self.fc5(a5)
    return l6
</code></pre>
<p>Here I have considered word embeddings initially and converted them to sentence vectors by averaging. These vectors are given for auto encoder model.</p>
<p>Let me know whether we have any other approach where we can generate a sentence.</p>
<p>Any kind of reference is helpful.</p>
","nlp"
"87793","Converting paragraphs into sentences","2021-01-11 10:29:08","87797","5","2320","<nlp><spacy><tokenization><information-extraction>","<p>I'm looking for ways to extract sentences from paragraphs of text containing different types of punctuations and all. I used <code>SpaCy</code>'s <code>Sentencizer</code> to begin with.</p>
<p>Sample input python list <code>abstracts</code>:</p>
<pre><code>[&quot;A total of 2337 articles were found, and, according to the inclusion and exclusion criteria used, 22 articles were included in the study. Inhibitory activity against 96% (200/208) and 95% (312/328) of the pathogenic fungi tested was described for Eb and [(PhSe)2], respectively. Including in these 536 fungal isolates tested, organoselenium activity was highlighted against Candida spp., Cryptococcus ssp., Trichosporon spp., Aspergillus spp., Fusarium spp., Pythium spp., and Sporothrix spp., with MIC values lower than 64 mug/mL. In conclusion, Eb and [(PhSe)2] have a broad spectrum of in vitro inhibitory antifungal activity.&quot;]
</code></pre>
<p>Code:</p>
<pre><code>from spacy.lang.en import English

nlp = English()
sentencizer = nlp.create_pipe(&quot;sentencizer&quot;)
nlp.add_pipe(sentencizer)

# read the sentences into a list
for doc in abstracts[:5]:
    do = nlp(doc)
    for sent in list(do.sents):
        print(sent)
</code></pre>
<p>Output:</p>
<pre><code>A total of 2337 articles were found, and, according to the inclusion and exclusion criteria used, 22 articles were included in the study.
Inhibitory activity against 96% (200/208) and 95% (312/328) of the pathogenic fungi tested was described for Eb and [(PhSe)2], respectively.
Including in these 536 fungal isolates tested, organoselenium activity was highlighted against Candida spp.,
Cryptococcus ssp.,
Trichosporon spp.,
Aspergillus spp.,
Fusarium spp.,
Pythium spp.,
and Sporothrix spp.,
with MIC values lower than 64 mug/mL. In conclusion, Eb and [(PhSe)2] have a broad spectrum of in vitro inhibitory antifungal activity.
</code></pre>
<p>It works fine for normal text but fails when there are dots (<code>.</code>) present in the sentence elsewhere other than at the end, which breaks the whole sentence as shown in the above output. How can we address this? Are there any other proven methods or libraries to perform this task?</p>
","nlp"
"87791","""Object"" Detection in Textual Data","2021-01-11 09:32:20","87824","2","117","<nlp><object-detection><document-understanding>","<p>I have a task where the input is a parsed document (i.e., full text in 1 string or tokens) and I need to classify parts of the text into say 5 classes (i.e., 5 tokens from the entire text are labeled into 5 different classes).</p>
<p>Example:</p>
<p>Document #1: &quot;... cat ...&quot; (the token &quot;cat&quot; belongs to class &quot;0&quot; which is animals)</p>
<p>Document #2: &quot;... fish ...&quot; (the token &quot;fish&quot; belongs to class &quot;0&quot; which is animals)</p>
<p>It is important to note that at inference time, I have the entire document (in text), and so most of the tokens from it do not belong to any of the classes.</p>
<p>What would be a good approach to this task? I thought about a simple classification problem where I take the labeled tokens from each document and input it into an RNN classifier, but that ignores the rest of the document and at test time irrelevant tokens can have larger probabilities than the labeled tokens.</p>
<p>I also had an idea inspired by YOLO, and maybe apply a 1D CNN object detector (with the respective number of classes) on the entire text. Is this reasonable?</p>
<p>Thanks.</p>
","nlp"
"87782","train NER using NLTK with custom corpora (non-english) must use StanfordNER?","2021-01-11 04:58:59","","1","520","<nlp><nltk><named-entity-recognition>","<p>I have searched about customization NER corpora for trainig the model using NLTK library from python, but all of the answer direct to nltk <a href=""http://www.nltk.org/book/ch07.html"" rel=""nofollow noreferrer"">book chapter 7</a> and honestly makes me confuse how to train the corpus with correct flow and data set that has structure like this below:</p>
<pre><code>Eddy N B-PER
Bonte N I-PER
is V O
woordvoerder N O
van Prep O
diezelfde Pron O
Hogeschool N B-ORG
. Punc O
</code></pre>
<p>I have some questions:</p>
<ol>
<li>I found so many article that if you will train customed corpora using NLTK, there uses StanfordNER library too, should it be? or we can use pure of NLTK library for it?</li>
<li>Should the grammar pattern be included if you want to apply it to other languages? How is the flow?</li>
</ol>
<p>And please give me example of code to train custom corpora until give the tag of POS Tag and NER label output using data like data structure above if you have. Thank you.</p>
","nlp"
"87712","doc2vec - paragraph or article as document","2021-01-09 13:46:01","","1","63","<nlp><gensim><doc2vec><wikipedia>","<p>I'm trying to train a doc2vec model on the German wiki corpus. While looking for the best practice I've found different possibilities on how to create the training data.</p>
<p>Should I split every Wikipedia article by each natural paragraph into several documents or use one article as a document to train my model?</p>
<p>EDIT: Is there an estimate on how many words per document for doc2vec?</p>
","nlp"
"87662","Why does BERT embedding increase the number of tokens?","2021-01-08 07:31:36","87663","0","344","<nlp><pytorch><bert>","<p>I am new to DataScience and trying to implement BERT embedding for one of my problems. But I am having one doubt here.
I am trying to embed the following sentence with BERT - &quot;Twinkle twinkle little star&quot;.
BERT tokenizer generates the following tokens - ['twin', '##kle', 'twin', '##kle', 'little', 'star']</p>
<p>But the final embedded tensor is having a dimension of [1,8,1024]</p>
<p>Why is the number of tokens 8 instead of 6? For any text, I am observing that number of tokens in the final embedding is getting increased by 2. Can anyone please help me to understand this?</p>
<p>I am giving the code snippet here -</p>
<pre><code>from transformers import BertTokenizer, BertForSequenceClassification, BertModel

PRE_TRAINED_MODEL_PATH = 'BERT\wwm_cased_L-24_H-1024_A-16'
tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_PATH)
model = BertModel.from_pretrained(PRE_TRAINED_MODEL_PATH)

encoded_input = tokenizer(texts, return_tensors='pt', padding=True)
emb = model(**encoded_input)
</code></pre>
","nlp"
"87634","List of Google T5 possible operations","2021-01-07 15:21:18","","0","153","<nlp><transfer-learning><transformer><huggingface>","<p>I am trying to use the huggingface.co pre-trained model of Google T5 (<a href=""https://huggingface.co/t5-base"" rel=""nofollow noreferrer"">https://huggingface.co/t5-base</a>) for a variety of tasks. But I can`t find a list of many tasks it really supports and how to address them. I found <code>summarize: </code> + the text to summarize. I also try to find an overview in the paper (<a href=""https://arxiv.org/abs/1910.10683"" rel=""nofollow noreferrer"">https://arxiv.org/abs/1910.10683</a>) there are for instance examples of question-answering in the appendix but without instructions how to tell T5 to answer a specific question. The huggingface.co documentation did not provide any further information besides the <code>summarize: </code>declaration.</p>
","nlp"
"87622","How to correctly lemmatize the text column in R?","2021-01-07 09:10:24","","0","292","<data-science-model><nlp>","<p>I'm working on a project in Natural Language Processing. I have a data frame that has a text column. I have to lemmatize that text column. I'm using lemmatize_strings() function in R. However, there's one case where word 'wound' in one of the text is being lemmatized to 'wind' which is not correct. Can anybody tell me what can be done to correct this ?</p>
","nlp"
"87597","NLP data cleaning and word tokenizing","2021-01-06 17:21:34","87705","1","345","<python><nlp><data-cleaning>","<p>I am new to NLP and have a dataset that has a bunch of (social media) messages on which I would like to try some methods like latent Dirichlet allocation (LDA). First, I need to clean the data of things like punctuation, emojis, etc. I'm not sure how to go about doing this in the most efficient and accurate manner. My code right now is this:</p>
<pre><code>import pandas as pd
import re

class TopicModel():
    def __init__(self, data_path = &quot;data.csv&quot;):
        self.data_path = data_path
        self.data = pd.read_csv(self.data_path, low_memory=False)

    def clean_data(self):
        self.remove_message_na()
        self.remove_emojis()
        self.remove_punctuation_and_lower()
        self.remove_url()
        self.remove_empty_messages()

    def remove_message_na(self):
        self.data = self.data.loc[~pd.isna(self.data['message'])]

    def remove_emojis(self):
        self.data['message'] = self.data['message'].str.encode(&quot;ascii&quot;, &quot;ignore&quot;).str.decode(&quot;utf8&quot;)

    def remove_punctuation_and_lower(self):
        p = re.compile('''[!#?,.:&quot;;]''')
        self.data['cleaned_data'] = [p.sub(&quot;&quot;, ii).lower() for ii in self.data['message'].tolist()]

    def remove_empty_messages(self):
        self.data = self.data.loc[self.data['cleaned_data'] != &quot;&quot;]

    def remove_url(self):
        self.data = [re.sub(r&quot;http\S+&quot;, &quot;&quot;, ii) for ii in self.data['message'].tolist()]
</code></pre>
<p>I don't want to remove contractions, which is why I left out <code>'</code> from my punctuation list, but I think optimally, the contractions would be reformatted as two separate words. I also wonder about the other punctuation marks when dealing with social media data, e.g., <code>#</code>. I know this question is a bit general, but I'm wondering if there is a good python library for performing the kind of data-cleaning operations that I want, prior to perform topic analysis, sentiment analysis, etc. I'd also like to know which libraries can efficiently perform these data-cleaning operations on a pandas data frame.</p>
","nlp"
"87579","Detecting grammatical errors with BERT","2021-01-06 09:48:55","","3","1391","<nlp><bert><grammar-inference>","<p>We fine-tuned BERT (<code>bert-base-uncased</code>) model with <code>CoLA</code> dataset for sentence classification task. The dataset is a mix of sentences with and without grammatical errors. The retrained model is then used to identify sentences with or without errors. Are there any other approaches we could make use of using BERT, other than building a classifier?</p>
","nlp"
"87540","What is the more natural parsing, the one that leads to the preferred reading of the sentence","2021-01-05 14:58:21","87586","2","54","<nlp><nltk><stanford-nlp>","<p>I have those rules:
<a href=""https://i.sstatic.net/jKfG2.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/jKfG2.png"" alt=""enter image description here"" /></a></p>
<p>and those two possible parse trees:</p>
<p><a href=""https://i.sstatic.net/gxHgF.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/gxHgF.png"" alt=""enter image description here"" /></a></p>
<p>I am asked for the next question:</p>
<p>What is the more natural parsing, the one that leads to the preferred reading of the sentence?</p>
<p>Can anyone explain to me, what is more natural in English and why ?</p>
<p>according to <a href=""https://sites.google.com/site/partofspeechhelp/#TOC-DT-"" rel=""nofollow noreferrer"">this</a></p>
<pre><code>A determiner is a noun-modifier that expresses the reference of a noun or noun phrase in the context.
</code></pre>
<p>I don't see any possible more natural distinguish.</p>
","nlp"
"87528","Pre-trained models for finding similar word n-grams","2021-01-05 09:34:23","","1","364","<nlp><fasttext>","<p>Are there any pre-trained models for finding similar word n-grams, where n&gt;1?</p>
<p>FastText, for instance, seems to work only on unigrams:</p>
<pre class=""lang-py prettyprint-override""><code>from pyfasttext import FastText
model = FastText('cc.en.300.bin')
model.nearest_neighbors('dog', k=2000)

[('dogs', 0.8463464975357056),
 ('puppy', 0.7873005270957947),
 ('pup', 0.7692237496376038),
 ('canine', 0.7435278296470642),
 ...
</code></pre>
<p>but it fails on longer n-grams:</p>
<pre class=""lang-py prettyprint-override""><code>model.nearest_neighbors('Gone with the Wind', k=2000)

[('DEky4M0BSpUOTPnSpkuL5I0GTSnRI4jMepcaFAoxIoFnX5kmJQk1aYvr2odGBAAIfkECQoABAAsCQAAABAAEgAACGcAARAYSLCgQQEABBokkFAhAQEQHQ4EMKCiQogRCVKsOOAiRocbLQ7EmJEhR4cfEWoUOTFhRIUNE44kGZOjSIQfG9rsyDCnzp0AaMYMyfNjS6JFZWpEKlDiUqALJ0KNatKmU4NDBwYEACH5BAUKAAQALAkAAAAQABIAAAhpAAEQGEiQIICDBAUgLEgAwICHAgkImBhxoMOHAyJOpGgQY8aBGxV2hJgwZMWLFTcCUIjwoEuLBym69PgxJMuDNAUqVDkz50qZLi',
  0.71047443151474),
</code></pre>
<p>or</p>
<pre class=""lang-py prettyprint-override""><code>model.nearest_neighbors('Star Wars', k=2000)
[('clockHauser', 0.5432934761047363),
 ('CrônicasEsdrasNeemiasEsterJóSalmosProvérbiosEclesiastesCânticosIsaíasJeremiasLamentaçõesEzequielDanielOséiasJoelAmósObadiasJonasMiquéiasNaumHabacuqueSofoniasAgeuZacariasMalaquiasNovo',
  0.5197194218635559),
</code></pre>
","nlp"
"87483","Is it acceptable to append information to word embeddings?","2021-01-04 11:52:52","","2","196","<word2vec><nlp><vector-space-models>","<p>Let's say I have my 300 dimensional word embedding trained with Word2Vec and it contains 10,000 word vectors.</p>
<p>I have additional data on the 10,000 words in the form of a vector (10,000x1), containing values between 0 and 1. Can I simply append the vector to the word embedding so that I have a 301 dimensional embedding?</p>
<p>I am looking to calculate similarities between word vectors using cosine similarity.</p>
","nlp"
"87389","Inference order in BERT masking task","2020-12-31 20:33:17","87390","1","150","<neural-network><nlp><bert><transformer><language-model>","<p>In BERT, multiple words in a single sentence can be masked at once. Does the model infer all of those words at once or iterate over them in either left to right or some other order?</p>
<p>For example:</p>
<blockquote>
<p>The dog walked in the park.</p>
</blockquote>
<p>Can be masked as</p>
<blockquote>
<p>The [mask] walked in [mask] park.</p>
</blockquote>
<p>In what order (if any) are these tokens predicted?</p>
<p>If you have any further reading on the topic, I'd appreciate it as well.</p>
","nlp"
"87384","NLP Subjectivity Detection methodology?","2020-12-31 18:10:08","","0","418","<machine-learning><classification><nlp><sentiment-analysis><text-classification>","<p>I am working on a project where I would like to be able to specifically analyze the level of subjectivity in a given text phrase using machine learning. Essentially, I would like to be able to classify a given text based on whether it is written from a subjective or objective point of view. Ideally, I would like to be able to classify on the sentence level (i.e. assign each sentence a classification of being written from a subjective or objective perspective). This will most likely be applied to short form news articles and/or tweets.</p>
<p>My question is: what sort of NLP methodologies are best for this use case?</p>
<p>I have explored a few papers on the topic and it seems that this falls under the umbrella of sentiment analysis. Many papers report that machine learning methods such as SVM and neural network architectures perform well on the problem, given features such as a lexicon containing mappings of words to sentiment and POS tags. I have briefly tested out the TextBlob library's subjectivity score with some mixed results.</p>
<p>As I am new to NLP I am looking for suggestions as to which methodology to apply. Specifically, I would like to ask:</p>
<ol>
<li><p>Is the text blob library well suited to this task (are there any other libraries you can recommend)? I have looked into the implementation and it seems to be computing subjectivity based on the presence of modifiers and the number of words belonging to certain POS tags.</p>
</li>
<li><p>What sort of features would you recommend utilizing to train a model for this task? As I mentioned, most papers seem to use POS tag counts and the sum of the polarity scores of the tokens in the text to determine subjectivity, but I wonder if there are other methods that experienced practitioners might think to apply</p>
</li>
<li><p>Are there any architectures that are particularly well suited to this task? The papers I have read report the best results for subjectivity classification using SVM and perceptron architectures but I am interested in exploring other architectures that may be able to analyze the text differently. I have looked briefly into sequence-to-sequence and attention models but based on what I have read they seem to be suited for a different range of tasks than this one - though intuitively I suspect that an attention model may be able to extract some latent representation of the text which could assist in subjectivity classification.</p>
</li>
</ol>
<p>I would really appreciate any help to set me off in the right direction. I hope that my questions are clear. For reference I've included links to some of the papers I have been looking at below.</p>
<p><a href=""https://www.sciencedirect.com/science/article/pii/S0950705114002068"" rel=""nofollow noreferrer"">https://www.sciencedirect.com/science/article/pii/S0950705114002068</a></p>
<p><a href=""https://www.researchgate.net/publication/322005172_Distinguishing_Between_Facts_and_Opinions_for_Sentiment_Analysis_Survey_and_Challenges"" rel=""nofollow noreferrer"">https://www.researchgate.net/publication/322005172_Distinguishing_Between_Facts_and_Opinions_for_Sentiment_Analysis_Survey_and_Challenges</a></p>
<p><a href=""https://www.researchgate.net/publication/322512000_Subjectivity_Detection_in_Nuclear_Energy_Tweets"" rel=""nofollow noreferrer"">https://www.researchgate.net/publication/322512000_Subjectivity_Detection_in_Nuclear_Energy_Tweets</a></p>
<p><a href=""https://www.researchgate.net/publication/343654268_Subjectivity_Detection_for_Sentiment_Analysis_on_Twitter_Data"" rel=""nofollow noreferrer"">https://www.researchgate.net/publication/343654268_Subjectivity_Detection_for_Sentiment_Analysis_on_Twitter_Data</a></p>
<p><a href=""https://www.researchgate.net/publication/318225737_Using_machine_learning_techniques_for_subjectivity_analysis_based_on_lexical_and_non-lexical_features"" rel=""nofollow noreferrer"">https://www.researchgate.net/publication/318225737_Using_machine_learning_techniques_for_subjectivity_analysis_based_on_lexical_and_non-lexical_features</a></p>
","nlp"
"87366","Adding extra feature to word2vec embeddings for use in classification","2020-12-30 22:29:21","","0","362","<machine-learning><nlp><word-embeddings><word2vec>","<p>I have a word2vec model that will transform my text into vectors, and I need to use them to classify the text. My data is essentially a timeseries of chat messages, so I think that the message frequency(Total messages / total time) holds a lot of information that should be used to classify.</p>
<p>I was thinking of taking the chat stream, combining them all into a single string, and then averaging all the word's word2vec representations together. But I also want the classifier to be able to use the message frequency.</p>
<p>Can I simply append the message frequency value onto the averaged word embedding vector?</p>
","nlp"
"87331","Generate sentences from keywords by adding formal word","2020-12-30 09:30:03","","1","449","<nlp><lstm><nlg>","<p>For example, I have a list of keywords like I, hungry =&gt; output: I am hungry or I, author, poem =&gt; output: I am the author of this poem.
Can someone please suggest the simplest way to achieve this?
I am a newbie, please tell me which knowledge I must have to solve this problem.</p>
","nlp"
"87328","Keep retweets during topic-modelling","2020-12-30 09:07:41","","2","13","<machine-learning><nlp><r><data-mining><lda>","<p>I got a dataset made out of tweets and I need to classify them into topics. For topic modelling with LDA I have cleaned out the dataset (removing stopwords, mentions, symbols, etc). Do I need to remove also retweets in order not to influence the LDA? If so, how should I finally classify all tweets since I could not use per-document-per-topic probability?</p>
","nlp"
"87326","True Negatives for Named Entity Recognition","2020-12-30 07:12:48","","1","436","<nlp><named-entity-recognition>","<p>Are True Negatives always zero for a Named Entity Recognition task because TN in NER would mean a not entity being classified as Not entity?</p>
<p>Actual Entity
[Microsoft Corp.] CEO [Steve Ballmer] announced the release of [Windows7] today</p>
<p>Model prediction
[Microsoft Corp.] [CEO] [Steve] Ballmer announced the release of Windows7 [today]</p>
<p>What will be the TNs in the above example?</p>
","nlp"
"87321","Tweet Classification into topics- What to do with data","2020-12-30 01:18:56","87336","2","98","<machine-learning><nlp><r><topic-model><lda>","<p>Good evening,
First of all, I want to apologize if the title is misleading.
I have a dataset made of around 60000 tweets, their date and time as well as the username. I need to classify them into topics. I am working on topic modelling with LDA getting the right number of topics (I guess) thanks to <a href=""https://cran.r-project.org/web/packages/ldatuning/vignettes/topics.html"" rel=""nofollow noreferrer"">this R package</a>, which calculates the value of three metrics(&quot;CaoJuan2009&quot;, &quot;Arun2010&quot;, &quot;Deveaud2014&quot;). Since I am very new to this, I just thought about a few questions that might be obvious for some of you, but I can't find online.</p>
<ol>
<li><p>I have removed, before cleaning the data (removing mentions, stopwords, weird characters, numbers etc), all duplicate instances (having all three columns in common), in order to avoid them influencing the results of topic modelling. Is this right?</p>
</li>
<li><p>Should I, for the same reason mentioned before, remove also all retweets?</p>
</li>
<li><p>Until now, I thought about classifing using the &quot;per-document-per-topic&quot; probability. If I get rid of so many instances, do I have to classify them based on the &quot;per-word-per-topic&quot; probability?</p>
</li>
<li><p>Do I have to divide the dataset into testing and training? I thought that is a thing only in supervised training, since I cannot really use the testing dataset to measure quality of classification.</p>
</li>
<li><p>Antoher goal would be to classify twitterers based the topic they most are passionate about. Do you have any idea about how to implement this?</p>
</li>
</ol>
<p>Thank you all very much in advance.</p>
","nlp"
"87318","Create a weighted graph from a continous text","2020-12-29 22:30:11","","1","100","<nlp><visualization><text-mining><graphs>","<p>My goal: plot a graph that shows how much the words of a large text are connected to each other.</p>
<p>My idea is that the closer the range between words, somehow affected by the occurence, the more connected the words are in the text.</p>
<p>I browsed the web for quite some time and noticed that there is a vast amount of algorithms and techniques in the field of text mining. It seems like i can't find a way to start without a deep study of all these topics and i need someone that can point me in the right direction.</p>
<p>my naive approach would be to calculate the distance of every word to any other word and somehow create a graph with weighted distances out of it but this seems like a pretty uncomputable task. Well i know there must be better solutions out there.</p>
<p>Thank you for any suggestions.</p>
","nlp"
"87317","Build a corpus for machine translation","2020-12-29 22:07:10","87319","2","367","<nlp><lstm><sequence-to-sequence><corpus>","<p>I want to train an LSTM with attention for translation between French and a &quot;rare&quot; language. I say rare because it is an african language with less digital content, and especially databases with seq to seq like format. I have found somewhere a dataset, but in terms of quality, both french and native language sentences where awfully wrong. When I used this dataset, of course my translations where damn funny ...</p>
<p>So I decided to do some web scraping to build myself my parallel corpus and it might be useful for research in the future.</p>
<p>It worked well and I managed to collect some good articles from a website containing some articles (monthly, since 2016 in both languages). Now the tricky part is putting everything into sentence to sentence format. I did a trial with a text and its translation just by tokenizing into sentence and I noticed that for example I had 23 sentences for French and 24 for native language.</p>
<p>Further checking showed that some small differences where notices in both languages, like a sentence where a comma was replaced in the other language by a dot.</p>
<p>So my question is :</p>
<p>Is it mandatory to put my articles into sentence-French to sentence-Native language format ? Or can I let it as text / paragraphs ?</p>
","nlp"
"87248","Minimum number of features for Naïve Bayes model","2020-12-28 12:45:52","","1","124","<python><nlp><feature-selection><naive-bayes-classifier><wikipedia>","<p>I keep on reading that Naive Bayes needs fewer features than many other ML algorithms. But what's the minimum number of features you actually need to get good results (90% accuracy) with a Naive Bayes model? I know there is no objective answer to this -- it depends on your exact features and what in particular you are trying to learn -- but I'm looking for a numerical ballpark answer to this.</p>
<p>I'm asking because I have a dataset with around 280 features and want to understand if this is way too few features to use with Naive Bayes. (I tried running Naive Bayes on my dataset and although I got 86% accuracy, I cannot trust this number as my data is imbalanced and I believe this may be responsible for the high accuracy. I am currently trying to fix this problem.)</p>
<p>In case it's relevant: the exact problem I'm working on is generating time tags for Wikipedia articles. Many times the infobox of a Wikipedia article contains a date. However, many times this date appears in the text of the article but is missing from the infobox. I want to use Naive Bayes to identify which date from all the dates we find in the article's text we should place in the infobox. Every time I find a sentence with a date in it I turn it into a feature vector -- listing what number paragraph I found this in, how many times this particular date appears in the article, etc. I've limited myself to a small subset of Wikipedia articles -- just apple articles -- and as a result, I only have 280 or so features. Any idea if this is enough data?</p>
<p>Thanks!</p>
","nlp"
"87244","how to run bert's pretrained model word embeddings faster?","2020-12-28 10:31:24","87347","5","2371","<nlp><pandas><word-embeddings><bert>","<p>I'm trying to get word embeddings for clinical data using microsoft/pubmedbert.
I have 3.6 million text rows. Converting texts to vectors for 10k rows takes around 30 minutes. So for 3.6 million rows, it would take around - 180 hours(8days approx).</p>
<blockquote>
<p>Is there any method where I can speed up the process?</p>
</blockquote>
<p>My code -</p>
<pre><code>from transformers import AutoTokenizer
from transformers import pipeline
model_name = &quot;microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext&quot;
tokenizer = AutoTokenizer.from_pretrained(model_name)
classifier = pipeline('feature-extraction',model=model_name, tokenizer=tokenizer)

def lambda_func(row):
    tokens = tokenizer(row['notetext'])
    if len(tokens['input_ids'])&gt;512:
        tokens = re.split(r'\b', row['notetext'])
        tokens= [t for t in tokens if len(t) &gt; 0 ]
        row['notetext'] = ''.join(tokens[:512])
    row['vectors'] = classifier(row['notetext'])[0][0]        
    return row

def process(progress_notes):     
    progress_notes = progress_notes.apply(lambda_func, axis=1)
    return progress_notes

progress_notes = process(progress_notes)
vectors_2d = np.reshape(progress_notes['vectors'].to_list(), (vectors_length, vectors_breadth))
vectors_df = pd.DataFrame(vectors_2d)
</code></pre>
<p>My progress_notes dataframe looks like -</p>
<pre><code>progress_notes = pd.DataFrame({'id':[1,2,3],'progressnotetype':['Nursing Note', 'Nursing Note', 'Administration Note'], 'notetext': ['Patient\'s skin is grossly intact with exception of skin tear to r inner elbow and r lateral lower leg','Patient with history of Afib with RVR. Patient is incontinent of bowel and bladder.','Give 2 tablet by mouth every 4 hours as needed for Mild to moderate Pain Not to exceed 3 grams in 24 hours']})
</code></pre>
<p>Note - 1) I'm running the code on aws ec2 instance r5.8x large(32 CPUs) - I tried using multiprocessing but the code goes into a deadlock because bert takes all my cpu cores.</p>
","nlp"
"87212","Model Predicts Badly on Real-World Data","2020-12-27 16:31:50","","1","42","<deep-learning><keras><nlp>","<p>I know this is probably a really easy fix, but I've been stuck on this for a while. I'm a noobie to both machine learning and programming in general.  My data is from this dataset. <a href=""https://www.kaggle.com/ozlerhakan/spam-or-not-spam-dataset"" rel=""nofollow noreferrer"">https://www.kaggle.com/ozlerhakan/spam-or-not-spam-dataset</a></p>
<p>I made a model that can predict whether or not an email is spam email, and it does so reasonably well, getting an 84% accuracy with the validation data. But when I try to make it predict on a new string, it seems to be predicting on a character-by-character basis. Below is the code that I used to preprocess the training(and testing) data. I would've copy-pasted the required code, but when I look at the draft, it looks terrible. So here's the link to my notebook. <a href=""https://colab.research.google.com/drive/1EnoNFR5CFi85hOrW8ihx66QWb6sToKAL?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1EnoNFR5CFi85hOrW8ihx66QWb6sToKAL?usp=sharing</a>
I tried to debug the function by putting various print statements all over. I've come to the conclusion that the problem spot is when I try to one-hot encode the words. I need those words to be one-hot encoded because that is how I trained the model.  But I don't know how to fix that. I would really appreciate some help with this. Thank you in advance.</p>
","nlp"
"87206","SKLEARN Metrics report ""Number of classes, 28, does not match size of target_names, 35. Try specifying the labels parameter""","2020-12-27 14:06:36","","0","2366","<python><scikit-learn><nlp><metric>","<p>What's the proper way to define the labels or target names of classification_report?</p>
<p>I have the report like this:</p>
<pre><code>print(metrics.classification_report(twenty_test.target[:n_samples], predicted, target_names=twenty_test.target_names))
</code></pre>
<p>I noticed i get the error like in the title or similar once the twenty_test.target[:n_samples] (or possibly also predicted array) is different length than twenty_test.target_names...</p>
<p>It doesn't work just out of box if i simply provide all target names (also those not included in the &quot;target&quot; or &quot;predicted&quot; array. (If i provide it as &quot;target_names&quot; param).</p>
<p>But if I provided it as the &quot;labels&quot; param as hinted i simply see 0 filled report.</p>
<p>Btw this is the output of unique values of &quot;target&quot; array:</p>
<pre><code>{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 25}
</code></pre>
<p>This is the output of predicted array (unique values):</p>
<pre><code>{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 25, 27, 30, 31, 32}
</code></pre>
<p>I tried to get the longer list of these and build the list of the longest value like this:</p>
<pre><code>longer = max(max(set(predicted)), max(set(twenty_test.target[:n_samples])))+1
print(metrics.classification_report(twenty_test.target[:n_samples], predicted, target_names=twenty_test.target_names[:longer])
</code></pre>
<p>Doesn't work. I would expect all target names also those not include should work out of box.</p>
","nlp"
"87189","auto updating text comparison model","2020-12-27 03:50:13","","1","16","<nlp><text-classification>","<p>I have a need to create a model that compares and groups distinct snippets of text based on keywords.  I can extract similar keywords with NLP methods and simply comparing sentence text.  I want these similar sentences to arise based on new text a user is entering and I want to give the user the ability to indicate whether or not the chosen sentences are actually similar to their sentence (would limit to 5 or so most similar).  The user's rankings would then feed along with the NLP key words into a machine learning model that would continue getting updated each time a new text entry is processed from a user.</p>
<p>I have two main questions:</p>
<ol>
<li><p>I work in an environment which generally uses C# for most projects.  Is there a way I could accomplish the above with ML.NET?  I've seen a lot of basic supervised binary classification examples, but it seems to me that what I'm attempting to accomplish would be better suited as unsupervised where I am simply feeding in keywords, rankings, and text and trying to establish patterns.  If not ML.NET, is there another easy way to reference external models, such as those created in Python in a .NET environment?</p>
</li>
<li><p>Breaking down the text classification / comparison seems like it would be best suited to something like a bag of words and performing clustering based on those words, but also seems like it should be focused on only the key words to keep the model smaller/simpler.  How would I account for rankings as well in this case?  I'm not very familiar with text classification algorithms, so was hoping someone with more experience could point me towards the algorithms that might work best in this scenario?</p>
</li>
</ol>
","nlp"
"87188","Can Transformer Models be used for Training Chatbots?","2020-12-27 03:29:55","","0","149","<machine-learning><deep-learning><nlp><transformer><chatbot>","<p>Can Transformer Models be used for Training Chatbots?</p>
<p><strong>Note  - I am talking about the transformer model google released on the paper 'Attention is all you need'</strong></p>
","nlp"
"87186","BERT :dropout(): argument 'input' (position 1) must be Tensor, not str","2020-12-27 01:41:45","93498","0","4077","<python><deep-learning><nlp><pytorch><bert>","<p>I am new to NLP and would like to build a BERT model for sentiment analysis so I am following <a href=""https://curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/"" rel=""nofollow noreferrer"">this</a> tutorial.</p>
<p>However, I am getting the error below:</p>
<pre><code>F.softmax(model(input_ids, attention_mask), dim = 1)
</code></pre>
<p>When I would like to execute this cell I get the error:</p>
<pre><code> dropout(): argument 'input' (position 1) must be Tensor, not str
</code></pre>
","nlp"
"87156","Classify tweets by topic","2020-12-26 13:50:15","87177","1","122","<machine-learning><nlp><machine-learning-model><text-classification><topic-model>","<p>I am approaching machine learning for the first time because of my studies. I have been given a bunch of tweets and the goal is to classify them per topic. I really have no clue on how this should be done. Is there a particular way to follow?</p>
<p>Until now, I have only found topics and was thinking about making a DTM-like dataframe for the training data containing not only the number of times not-sparse words occur but also the number of times particular N-grams occur and a ground truth column with the topic.</p>
<p>Is this totally wrong? How else could I train a classifier without having features?</p>
","nlp"
"87153","Identify outliers for annotation in text data","2020-12-26 11:38:30","87176","1","137","<machine-learning><nlp><annotation><active-learning>","<p>I read the book &quot;Human-in-the-Loop Machine Learning&quot; by Robert (Munro) Monarch about Active Learning. I don't understand the following approach to get a diverse set of items for humans to label:</p>
<blockquote>
<ol>
<li>Take each item in the unlabeled data and count the average number of word matches it has with items already in the training data</li>
<li>Rank the items by their average match</li>
<li>Sample the item with the lowest average number of matches</li>
<li>Add that item to the ‘labeled’ data and repeat 1-3 until we have sampled enough for one iteration of human review</li>
</ol>
</blockquote>
<p>It's not clear how to calculate the average number of word matches.</p>
","nlp"
"87140","Is it advisable to merge similar datasets to improve model accuracy?","2020-12-25 22:20:14","87149","1","132","<classification><nlp><class-imbalance>","<p>I'm trying to build a classifier that would help me classify whether a statement collected from Reddit is bullish, bearish or neutral.</p>
<p>To this end, I have hand-labelled a fairly small dataset of 2500 entries, each with max 280 characters. Unfortunately, the data is unbalanced (60% neutral, 25% bullish and 15% bearish) and my initial attempts are returning poor results for the bullish and bearish classes.</p>
<p>I've managed to obtain 10,000 entries of similar data from StockTwits, each labelled bullish or bearish (no neutral...) and 280 characters max.</p>
<p>I tested supplementing my Reddit dataset with bullish/bearish data from this dataset to balance out the classes. In other words, I added data from the StockTwits dataset until the number of bullish and bearish entries in matched the neutral. With this model, I'm getting much better results.</p>
<p>Is merging similar datasets in this way advisable? My gut instinct says &quot;no&quot; but I haven't found anything suggesting this is not a good idea.</p>
","nlp"
"87098","Dropout after the Embeding layer","2020-12-24 10:10:25","","0","44","<nlp><lstm>","<p>I am working on a classification problem. I am using pre-trained GloVe word embedding as input I wanted to know whether adding a dropout after the embedding layer makes sense at all? I have seen a lot of notebooks on Kaggle where dropout layers are added after the embedding layer and were wondering why this is done.</p>
<pre><code>model= Sequential()
model.add(Embedding(vocab_size+1,embedding_dim,input_length=max_len, weights=[embeddings_matrix],trainable=False))
model.add(Dropout(0.3))
model.add(Bidirectional(LSTM(64,return_sequences= True)))
model.add(Bidirectional(LSTM(32)))
model.add(Dense(32,activation='relu'))
model.add(Dense(1,activation='sigmoid'))
</code></pre>
","nlp"
"87093","Word2Vec: Why do some dimensions of an embedding have an interpretation, and why does addition/subtraction of embedding vectors work?","2020-12-24 08:22:52","97653","6","1883","<word-embeddings><word2vec><nlp>","<p>I'm reading about Word2Vec from this source: <a href=""http://jalammar.github.io/illustrated-word2vec/"" rel=""noreferrer"">http://jalammar.github.io/illustrated-word2vec/</a>. Below is the heatmap of the embeddings for various words. In the source, it's claimed that we can get an idea on what the different dimensions &quot;mean&quot; (their interpretation) based on their values for different words. For example, there's a column that's dark blue for every word except <strong>WATER</strong>, so that dimension may have something to do with the word representing a person.
<a href=""https://i.sstatic.net/dpkyD.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/dpkyD.png"" alt=""enter image description here"" /></a></p>
<p>Secondly, there's a famous example that &quot;king&quot; - &quot;man&quot; + &quot;woman&quot; ~= &quot;queen&quot;, where the word in quotation means the embedding of that word.</p>
<p>My questions are:</p>
<ol>
<li>I don't quite understand the mechanism as to how any dimension of an embedding goes on to have a tangible, interpretable meaning. I mean, the individual components of embedding vectors could've very well been completely arbitrary devoid of meaning, and the whole embedding approach <strong>still</strong> could've worked in that scenario, since we're interested in the vector as a whole. Is there an online explanation or a paper that I can look at to understand this phenomenon?</li>
<li>Why does this addition/subtraction of vectors to give the relevant embedding vector for &quot;queen&quot; work so nicely? In one <a href=""https://blog.galvanize.com/add-and-subtract-words-like-vectors-with-word2vec-2/"" rel=""noreferrer"">source</a>, the explanation is given as follows:</li>
</ol>
<blockquote>
<p>This works because the way that the neural network ended up learning about related frequencies of terms ended up getting encoded into the W2V matrix. Analogous relationships like the differences in relative occurrences of Man and Woman end up matching the relative occurrences of King and Queen in certain ways that the W2V captures.</p>
</blockquote>
<p>This seems like a broad, vague kind of an explanation. Is there any online resource or paper that explains (or better yet, proves) why this property of embedding vectors should hold?</p>
","nlp"
"87033","Scoring metric for recommendation system","2020-12-22 20:06:15","","1","145","<machine-learning><python><nlp><recommender-system><similarity>","<p>I'm working on a project that involves building a news recommendation system. I've come as far as quantifying user interaction with different articles  on the site into user's affinity towards atopic using a bayesian function. I also have quantified the recent articles using LDA into the proportion an article talk about each topic.</p>
<p>my users topic-affinity for a user x looks like this(target-x):</p>
<pre><code> user_id  interest-topic-0  interest-topic-1  interest-topic-2  interest-topic-3  interest-topic-4  interest-topic-5  interest-topic-6  interest-topic-7  interest-topic-8  interest-topic-9 
       0            0.0257            0.2956            0.0386            0.0643            0.1285            0.0000               0.0            0.0257            0.0386            0.1671  
</code></pre>
<p>My quantified articles looks something like this(vectors-v):</p>
<pre><code>post_id   topic-0   topic-1   topic-2   topic-3   topic-4   topic-5   topic-6  topic-7  topic-8   topic-9
      x  0.055048  0.000000  0.742544  0.032286  0.059630  0.000000  0.000000  0.01173      0.0  0.095441
      y  0.000000  0.051172  0.000000  0.000000  0.158314  0.042632  0.022281  0.00000      0.0  0.720676
      z  0.028615  0.000000  0.020919  0.000000  0.000000  0.018940  0.882862  0.00000      0.0  0.046078
</code></pre>
<p>The shape of target will always be (10,)</p>
<p>The shape of vectors will always be (num_articles, 10)</p>
<p>Both vectors do not follow the same distribution.</p>
<p>Now I'm trying to figure out the best way to recommend articles from vectors v, for a target user x, given x and v. I've tried distance similarity functions like cosine similarity to find the distance between vectors. The results are satisfactory but I'm looking for a better function/metric to pick out top n recommendations for a user.</p>
","nlp"
"86981","Shouldn't ROUGE-1 precision be equal to BLEU with w=(1, 0, 0, 0) when brevity penalty is 1?","2020-12-21 16:10:53","","0","121","<nlp><model-evaluations><ngrams>","<p>I am trying to evaluate a NLP model using BLEU and ROUGE. However, I am a bit confused about the difference between those scores. While I am aware that ROUGE is aimed at recall whilst BLEU measures precision, all ROUGE implementations I have come across also output precision and the F-score. The original ROUGE paper only briefly mentions precision and the F-score, therefore I am a bit unsure about what meaning they have to ROUGE. Is ROUGE mainly about recall and the precision and F-score are just added as a compliment, or is the ROUGE considered to be the combination of those three scores?</p>
<p>What confuses me even more is that to my understanding ROUGE-1 precision should be equal to BLEU when using the weights (1, 0, 0, 0), but that does not seem to be the case.
The only explanation I could have for this is the brevity penalty. However, I checked that the accumulated lengths of the references are shorter than the length of the hypothesis, which means that the brevity penalty is 1.
Nonetheless, BLEU with w = (1, 0, 0, 0) scores 0.55673 while ROUGE-1 precision scores 0.7249.</p>
<p>What am I getting wrong?</p>
<p>I am using <a href=""https://www.nltk.org/_modules/nltk/translate/bleu_score.html"" rel=""nofollow noreferrer"">nltk</a> to evaluate BLEU and <a href=""https://github.com/li-plus/rouge-metric"" rel=""nofollow noreferrer"">rouge-metric</a> for ROUGE.</p>
","nlp"
"86974","Entity linking vs aliasing","2020-12-21 13:23:30","94000","1","96","<nlp><entity-linking>","<p>The process of finding entity in a knowledge base (KB) that a given keyphrase in a text refers to is called entity linking. I have the
opposite problem. I have an entity in my knowledge base (KB) and I want to find all the ways people might refer to this entity. For instance, I have &quot;Madonna&quot; (singer) and I am looking for aliases like &quot;Louise Ciccone&quot;, &quot;Madonna Ritchie&quot;, &quot;Queen of Pop&quot;, &quot;Mo&quot;, etc.</p>
<p>Is it called aliasing? Or there is a better name in the literature?</p>
<p>I guess finding the right key words will help me find related research.</p>
","nlp"
"86966","Is NLP suitable for my legal contract parsing problem?","2020-12-21 09:11:51","","3","789","<nlp><beginner><named-entity-recognition><spacy>","<p>My company has a product that involves the extraction of a variety of fields from legal contract PDFs. The current approach is very time consuming and messy, and I am exploring if NLP is a suitable alternative.</p>
<p>The PDFs that need to be parsed usually follow one of a number of &quot;templates&quot;. Within a template, almost all of the documents are the same, except for 20 or so specific fields we are trying to extract. That being said, there are sometimes slight inconsistencies such as missing/added spaces, punctation, etc. Currently there are about 15 templates, but sometimes another gets added. Most of the data that needs to be extracted is in tabular form, but a few values are littered in paragraph text.</p>
<p>With the current, non-ML approach, we use a proprietary PDF parser that extracts out all of the tables into a json format. This parser must be custom configured for every new PDF template that appears, and this is time consuming. After converting to json, a custom regex expression must be made for each value in each template, leading to several hundred regexes total. Usually this approach is pretty accurate, but sometimes the slight inconsistencies can break it, meaning a the regex needs to be revised.</p>
<p>I'm wondering if NLP Named Entity Recognition would be a cleaner solution to this problem. My idea is to label all of the values in a few hundred sample documents and then train a custom NER model in a library like Spacy or Flair. Ideally, we could feed in the raw, extracted text from the PDFs instead of having to configure the custom parser to extract json.</p>
<p>The advantage I see in using the NLP approach is that we wouldn't have to configure the custom parser and write a bunch of regexes every time a new template appears. At worst, we would have to label a few of the new documents every time a template is added, which would presumably be faster and easier than the current approach. I think we could also generate tons of synthetic training data easily by swapping labeled values between different documents.</p>
<p>I am concerned that, using an ML approach, we wouldn't be able to achieve near perfect accuracy, which is a requirement. I'm also not sure how well NLP can perform on raw text from tables as opposed to paragraphs. The nice thing is that the documents tend to be very similar within templates.</p>
<p>I've never done NLP before, so I'm wondering if anyone here thinks this approach would be worthwhile exploring. If this is feasible, does anyone have suggestions on how to get the best results?</p>
","nlp"
"86888","NLP: Information extraction","2020-12-18 22:34:49","","3","107","<nlp><text-mining>","<p>I need to extract product names from a text column in a dataset. Currently I'm using regular expressions to extract the product names from the middle of the text, but sometimes the product name is misspelled, incomplete or even amended in another word, which means that I am unable to identify and extract the product name.</p>
<p>We currently have around 1500-2000 products on that list and I have a data set with those products already identified from approximately 30,000 lines. Is there an approach that I can use this historical data to improve the identification of products that have not yet been identified?</p>
<p>Just an example:</p>
<pre><code>The product X produced by the Company Y is used to treat skin diseases
</code></pre>
<p>Note: The product names doesn't appear in a fixed position.</p>
","nlp"
"86788","Bag-of-words and Spam classifiers","2020-12-16 17:38:48","86796","1","192","<machine-learning><deep-learning><nlp><naive-bayes-classifier><bag-of-words>","<p>I implemented a spam classifier using Bernoulli Naive Bayes, Logistic Regression, and SVM. Algorithms are trained on the entire Enron spam emails dataset using the Bag-of-words (BoW) approach. Prediction is done on the UCI SMS Spam Collection dataset. I have 3 questions:</p>
<ol>
<li><p>During test time, while creating the term-frequency matrix, what if none of the words from my training BoW are found in some of my test emails/smses. Then, wouldn't the document vectors be zero vectors for those datapoints. How should I tackle this?</p>
</li>
<li><p>What if a new word from my test email/sms doesn't exist in BoW?</p>
</li>
<li><p>How do I choose my BoW so as to improve my prediction accuracy?</p>
</li>
</ol>
","nlp"
"86720","pdf to json libraries","2020-12-15 12:44:28","","1","129","<nlp>","<p>I am looking for a library which converts pdf to json. Basically in that json the paragraph heading is the and the value is the content of paragraph. Is there any python library for that ? I am already using pdfminer but that just converts to plain text. It cannot persist the structure/organisation of the document. For now it is ok to not read images and table although if there is a library to do that would be great.</p>
","nlp"
"86669","Context Based Embeddings vs character based embeddings vs word based embeddings","2020-12-14 15:48:05","86713","1","425","<machine-learning><nlp><data-science-model><word-embeddings><python-3.x>","<p>I am working on a problem that uses English alphabets in the text but the language is not English. Its a mixture of English and different language text. But all words are written using English alphabets. Now, word-based pre-trained embedding models will not work here as it gives a random embedding to out of vocabulary words.</p>
<p>Now my question is that how the Context-based pre-trained embeddings deal with &quot;out of vocabulary&quot; words?</p>
<p>Besides, what's the difference between context-based embeddings and character-based embeddings?</p>
","nlp"
"86627","Finding and ranking best semantic matches between two sets of phrases","2020-12-13 10:42:06","86693","1","66","<machine-learning><nlp><recommender-system>","<p>I'm looking for a proper definition for what sort of problem this is, so I can further research it on my own - though I will, for sure, appreciate any specific advice on what are industry standard ways for working on this. While I'm fairly inexperienced with NLP or recommender systems, I've done a fair amount of classical ML before, in case that matters.</p>
<p>The problem - given a &quot;search&quot; query with a list of inputs and a list of expected outputs, retrieve and rank up to N best semantic match for each input. Constraints:</p>
<ol>
<li>All inputs and outputs are anywhere between a word and a sentence.</li>
<li>Number of outputs &gt;= number of inputs.</li>
<li>All inputs and outputs are unique within themselves, but there can be outputs that are identical to inputs.</li>
<li>Every input is guaranteed to have at least one &quot;good enough&quot; output present.</li>
<li>Every output is guaranteed to be the best match for <strong>at most</strong> one input.</li>
<li>Labelled data is available, i.e. human-tagged queries. Inputs are sparse, i.e. inputs for queries targeting identical or very similar output sets can be very different.</li>
<li>Data is in English, and I work in Python - if that matters for your suggestions.</li>
</ol>
<p><strong>An example:</strong></p>
<p><em>Inputs</em></p>
<pre><code>(1) Truck
(2) Assortment of lemons, limes, and oranges.
(3) Apples, pears, and oranges.
(4) A tool with broad blade, used for digging.
</code></pre>
<p><em>Outputs</em></p>
<pre><code>(a) Citrus fruits
(b) Portable telephone that can make and receive calls over a radio frequency.
(c) Fruits traditionally grown in Germany.
(d) Vehicles used for cargo transportation.
(e) Shovel
(f) Motor vehicle used for transportation.
</code></pre>
<p><em>Desired result (numbers are arbitrary, for illustrative purposes)</em></p>
<pre><code>1 - d (100%), f (75%)
2 - a (95%), c (60%)
3 - c (87%), a (45%)
4 - e (100%)
</code></pre>
<p>Similar question suggestions by Stack Exchange are not answering my question, and searching for the answer elsewhere just pits me into endless stream of articles about sentiment analysis of IDMB or Twitter datasets.</p>
","nlp"
"86618","Unsupervised text classification with R/Python","2020-12-12 23:49:23","87052","0","928","<machine-learning><python><nlp><r><text-mining>","<p>I am relativity new to machine/deep learning and NLP. As a part of my Phd thesis I have scraped vast number of job vacancies (most of them are in Polish, and about 10% are in English ones) and then extracted required skills/competencies. As an output I got a vector of strings contains single skills/competences. I performed initial preparation (removed extra spaces, special symbols, stop words etc.) and built then a frequency distribution table to figure out the most demanded skills. The most commonly required skills were as follow:</p>
<pre><code>prawo jazdy kat b ## driver license
umiejętność pracy w zespole ## team work
wykształcenie wyższe ## higher education
czynne prawo jazdy kat b ## driver license
znajomość pakietu ms office ## MS Office
prawa jazdy kat b ## driver license
prawo jazdy kategorii b ## driver license
fluent english
doświadczenie na podobnym stanowisku ## work experience
dobra znajomość języka angielskiego  ## English
excellent english skills
doświadczenie ## work experience
</code></pre>
<p>As you may see some of these competencies are quite similar. The problem is that job offers were provided by a different employers. European classification of skills/competences qualifications and occupations define about 3500 different competencies for instance.</p>
<p><strong>My aim is to create trivial classifier using logistic regression, SVM or random forest that would be able to classify skills in real time using pre trained model.</strong> The problem is that I can not label vector of unique skills because of vast number of skills within its synonyms. I spent a while trying to figure out a solution.</p>
<p>So, the idea was to perform cluster analysis and to put similar skills into separate groups (clusters) and then label this clusters. In this case I would have much less group to label. Moreover, I would have a lot of synonyms within each group, which would increase precision of classification (I guess). Using labelled data set I would be able to trained model, and then to classify future job vacancies (skills). However, as I said I am new to machine learning, so I am not sure whether my solution would be good enough.</p>
<p><strong>So, I am looking for advice/tutorials/links that would help me to solve my problem rather than for complete solutions. I can use both R and Python. Would be appreciated for any help.</strong></p>
","nlp"
"86601","Cosine Similarity but with weighting for vector indexes","2020-12-12 14:24:17","","2","566","<machine-learning><nlp><ranking><cosine-distance>","<p>I am very new to NLP and although this seems like a basic question I don't know how to search for an answer online.</p>
<p>This is my problem: I have extracted and ranked keywords from 2 text sources:</p>
<p><a href=""https://i.sstatic.net/0eVAQ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/0eVAQ.png"" alt=""enter image description here"" /></a></p>
<p>A rank of 1, means this keyword is more important than a keyword with rank 5. Some keywords may not exist in one text but do in the other. In this case, where the keyword does not exist, there is no rank, therefore, Nan.</p>
<p>What method do I need to use to extract the similarity between the keyword ranks? I want to find out how similar the 2 texts are based on what keywords it contains and the ranks of these keywords.</p>
<p>I have tried cosine similarity by removing rows that contain Nan values and then treating text1Rank and text2Rank as vectors like this:</p>
<p><a href=""https://i.sstatic.net/KJW2b.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/KJW2b.png"" alt=""enter image description here"" /></a></p>
<p>the 2 columns are the vectors I pass into the cosine similarity formula.</p>
<p>However, I do not think that this method weighs higher-ranked keywords more than lower-ranked keywords. Am I correct in thinking this?</p>
<p>If so, what method should I use to compare the 2 sets of ranks of keywords?</p>
","nlp"
"86566","What's the right input for gpt-2 in NLP","2020-12-11 17:29:15","","4","9665","<nlp><data-science-model><transformer><gpt>","<p>I'm fine-tuning pre-trained gpt-2 for text summarization. The dataset contains 'text' and 'reference summary'. So my question is how to add special tokens to get the right input format. Currently I'm thinking doing like this:</p>
<p>example1  &lt;BOS&gt; text  &lt;SEP&gt; reference summary &lt;EOS&gt; ,<br />
example2 &lt;BOS&gt; text &lt;SEP&gt; reference summary &lt;EOS&gt; ,
.....</p>
<p>Is this correct? If so, a follow-up question would be whether the max-token-length(i.e. 1024 for gpt-2) means also the concatenate length of text and reference summary?</p>
<p>Any comment would be very much appreciated!</p>
","nlp"
"86548","Trained BERT models perform unpredictably on test set","2020-12-11 10:23:51","86557","4","418","<nlp><bert><transformer>","<p>We are training a BERT model (using the Huggingface library) for a sequence labeling task with six labels: five labels indicate that a token belongs to a class that is interesting to us, and one label indicates that the token does not belong to any class.</p>
<p>Generally speaking, this works well: loss decreases with each epoch, and we get good enough results. However, if we compute precision, recall and f-score after each epoch on a test set, we see that they oscillate quite a bit. We train for 1,000 epochs. After 100 epochs performance seems to have plateaued. During the last 900 epochs, precision jumps constantly to seemingly random values between 0.677 and 0.709; recall between 0.729 and 0.798. The model does not seem to stabilize.
To mitigate the problem, we already tried the following:</p>
<ul>
<li>We increase the size of our test data set.</li>
<li>We experimented with different learning rates and batch sizes.</li>
<li>We used different transformer models from the Huggingface library, e.g. RoBERTa, GPT-2 etc.
Nothing of this has helped.</li>
</ul>
<p>Does anyone have any recommendations on what we could do here? How can we pick the “best model”? Currently, we pick the one that performs best on the test set, but we are unsure about this approach.</p>
","nlp"
"86526","BERT for classification model degenerates into all-positive predictions","2020-12-10 18:46:03","","0","168","<nlp><training><bert><huggingface>","<p>As a learning project, I'm training a BERT model with the CoLA dataset to detect sentence acceptability. Unfortunately my model is learning to classify every instance as &quot;acceptable&quot;, and I'm not sure what is going wrong with my code. Can anyone provide any help or insight into why this happens?</p>
<h3>Technical details</h3>
<p>I'm using hugging face's <code>transformers</code> library with PyTorch.</p>
<p>The (stripped version of the) code is as follows. Instrumentation and other details have been left out so the code is more readable.</p>
<pre class=""lang-py prettyprint-override""><code>tokenizer = transformers.AutoTokenizer.from_pretrained('bert-base-cased')

model = transformers.AutoModelForSequenceClassification.from_pretrained(
    'bert-base-cased',
    num_labels=2,
)

optimizer = transformers.AdamW(
    model.parameters(),
    lr=2e-5,
    eps=1e-8,
)

# The next lines read the CoLA dataset and split it for training and validation
training_dataloader = ...
validation_dataloader = ...

for epoch in range(4):
    train_loss = 0

    for batch in tqdm(train_dataloader):
        model.train()

        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['label'].to(device)

        model.zero_grad()

        model_output = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels,
            return_dict=True,
        )

        batch_loss = model_output.loss.sum()

        train_loss += batch_loss.item()

        batch_loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()

        # HERE: Measure performance after learning from each batch here with validation_dataloader
</code></pre>
<p>I've checked and the code that reads the dataset seems correct, so it seems that the problem lies either in measuring the performance of the model or in the learning phase.</p>
<p>To measure the performance, I'm running the model over the validation split and then converting the logits into actual classifications as follows:</p>
<pre class=""lang-py prettyprint-override""><code>tp = fn = fp = tn = 0

for batch in validation_dataloader:
    input_ids = batch['input_ids'].to(device)
    attention_mask = batch['attention_mask'].to(device)
    labels = batch['label'].to(device)

    model(
        input_ids=input_ids,
        attention_mask=attention_mask,
        labels=labels,
        return_dict=True,
    )

    expected = batch['label']
    predictions = output.logits[:,1] &gt; output.logits[:,0] # Is this correct?

    tp += sum(1 for exp, pred in zip(expected, predictions) if     exp and     pred)
    fn += sum(1 for exp, pred in zip(expected, predictions) if     exp and not pred)
    fp += sum(1 for exp, pred in zip(expected, predictions) if not exp and     pred)
    tn += sum(1 for exp, pred in zip(expected, predictions) if not exp and not pred)
</code></pre>
<p>Is the line <code>predictions = ...</code> correct?</p>
","nlp"
"86511","Sentence to word similarity","2020-12-10 13:58:40","","1","68","<nlp><semantic-similarity>","<p>Is there a way to know how much a sentence is related to a word/topic?</p>
<p>For instance the following dataframe and the topics/attributes <code>Romantique</code>, <code>Feminine</code>, ...:</p>
<pre><code>    comments
0   Très contente de mon achat. Je cherchais ce parfum depuis un temps en magasin et je suis heureuse qu’il soit disponible en ligne il sent tellement bon !! En plus en promo, génial ! \r\nLivraison très rapide !
1   J’adore les parfums de cette marque car je trouve qu’ils sont captivant et surtout ils tiennent toute la journée ! Ils ont des odeurs originales et que l’on ne retrouve pas partout ! Je conseil fortement
2   Le parfum ideal pour porter pendant toutes les saisons du matin à nuit !!!
3   Très bon parfum floral, envoûtant au note de Jasmin qui reste toute la journée\r\nCorresponds aux personnes qui aiment les parfums florales assez imposante
...
</code></pre>
<p>As a start I thought about doing a jaccard_similarity distance ...</p>
<pre><code>&gt;&gt;&gt;from collections import Counter
&gt;&gt;&gt;Counter(df['comments'].apply(lambda x: x.split(' ')).apply(lambda x: jaccard_similarity(x,['féminin'])))
Counter({0.0: 1344, 0.025: 21, 0.05: 21, 0.0625: 21})
</code></pre>
<p>But is there a better way to see how much a sentence relate to a targeted word?</p>
<h1>Update</h1>
<p>My main goal is to compare the proportion of people who used some topics in their comments of a product with the presence or absence of these topics in the description of the product. I used a model which use the synonyms:</p>
<pre class=""lang-py prettyprint-override""><code>d = {}
for product in collection.find():
  d_product = {}
  name = product['q0']['Results'][0]['Name']
  description = product['q0']['Results'][0]['Description'] 
  comments = short_comments_df(product['q2'])['comments']
  #for every attributes
  for attribut in attributs:
    consumers_approved = 0
    # Is the attribut, or its synonyms, in the comments?
    try: 
      # if the attribute or it synonyms are in the description then the product has the attribut
      product_approved = presence(synonymes[unidecode.unidecode(attribut)], description)
      # we test every comment to see if they talked about the attribute
      for comment in comments:
        # We only take the nouns and the verbs
        lemmatized_comment = lemmatize_pos_filtering(comment)
        # if the attributes are in the comments then we increment the consumer approved counter
        consumers_approved += presence(synonymes[unidecode.unidecode(attribut)],lemmatized_comment)
      # we take the proportion of people who used the attribute, but shouldn't we normalize it? 
      proportion_approved = consumers_approved/len(record['q2']['Results'])
    except IndexError:
      print(&quot;IndexError: &quot;,attribut)
    # we use the difference between if we found it in the description and the % of people who found it as well 
    d_product[attribut] = product_approved - proportion_approved
  d[name] = d_product

df = pd.DataFrame(d)
</code></pre>
<p>It produces the following graph:</p>
<p><a href=""https://i.sstatic.net/lmiR3.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/lmiR3.png"" alt=""enter image description here"" /></a></p>
<p>It's weird because it shows that for most products the difference between the presence/absence of any topic compared to its presence/absence in the description is the same for most topics, but different from 0! Everything that is above zero means that at least the description has it and none of the comments, but everything below zero means that the comments have it but not the description. What strikes me is these straight lines below zero. It means that the presence/absence of a given attribute in absciss is the same for every comment...</p>
","nlp"
"86427","How to go about training a NER model to extract book citations in free-form?","2020-12-08 18:09:06","","2","422","<nlp><named-entity-recognition><spacy><information-extraction>","<p>I'm doing a project where I wish to create a graph visualization of free-form citations (not academic style citations) across all my e-books. E.g. David Foster Wallace's essays cite a lot of other books by different authors. For that I should be able to detect and extract book and authors names from my own e-books.</p>
<p>I've selected some examples from my e-books that I wish my NER model would tag as &quot;books&quot; (in bold font):</p>
<blockquote>
<p>(...) or even the parodistic version of Pater to be found in W. H. Mallock’s <strong>The New Republic</strong> (...)</p>
</blockquote>
<blockquote>
<p>Plato words the same conception beautifully in the <strong>Republic</strong>: (...)</p>
</blockquote>
<p>I also wish to tag authors, but I suppose this could be done out-of-the-box with Spacy or other NLP library, with some pre trained PERSON tag.</p>
<p>So, my question is about the best approach to go about creating this NER model.</p>
<ul>
<li><p>I could create lots and lots of training samples from my books and create a new NER model. (very time consuming)</p>
</li>
<li><p>Or if there is a dataset or public model with the BOOK or something like WORK_OF_ART tag I could bootstrap my own dataset.</p>
</li>
</ul>
<p>What do you think about this approaches?</p>
","nlp"
"86405","NLP CounterVectorizer (sklearn), not able to get it to fit my code","2020-12-08 12:05:42","86411","0","271","<python><scikit-learn><nlp>","<p>I was starting an NLP project and simply get a &quot;CountVectorizer()&quot; output anytime I try to run CountVectorizer.fit on the list.  I've had the same issue across multiple IDE's, and different code.  I've looked online, and even copy and pasted other codes with their lists and I receive the same CountVectorizer() output.</p>
<p>My code is as follows:</p>
<pre><code>cv = CountVectorizer()

messages = ['This is a good product', 'It was a bit pricey for what it does', 'I found good value here']

cv.fit(messages)
</code></pre>
<pre><code>**output -----&gt;  CountVectorizer()**
</code></pre>
<p>I'm really stumped on this issue.  Any advice would be greatly appreciated.  Thanks.</p>
<p>Update:  This seems to be a local issue as I am able to get it to fit on Colab.  If anyone can suggest what might be going on I'd be estatic.</p>
","nlp"
"86390","Continuous Bag Of Words (CBOW) network architecture?","2020-12-08 06:52:15","","1","693","<nlp><pytorch><word-embeddings><word2vec>","<p>Looking into word2vec like embeddings I found <a href=""https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html#exercise-computing-word-embeddings-continuous-bag-of-words"" rel=""nofollow noreferrer"">this</a> exercise on PyTorch's website which prompts the reader to implement a CBOW network in PyTorch.</p>
<p>My question is about the architecture to implement this CBOW network.</p>
<p><strong>Here is my understanding</strong>: From a number of sources, it seems that the network should have a <strong>single hidden layer</strong> (with weights and no biases) which is connected to an activation layer (most sources say softmax). Then the network will be trained to map one-hot encoded words to likely contexts. Finally, the hidden layer's weights will be used to as the embedding matrix.</p>
<p><strong>My confusion is</strong>: I see a number of solutions like <a href=""https://github.com/FraLotito/pytorch-continuous-bag-of-words/blob/master/cbow.py"" rel=""nofollow noreferrer"">this first one off google</a> where there are <strong>multiple hidden layers</strong>. In this example, there is a embedding layer and there are <a href=""https://github.com/FraLotito/pytorch-continuous-bag-of-words/blob/master/cbow.py#L40-L45"" rel=""nofollow noreferrer"">two linear layers</a> connected by a relu. Here is another that uses one <a href=""https://rguigoures.github.io/word2vec_pytorch/"" rel=""nofollow noreferrer"">linear layer</a>.</p>
<p><strong>My questions are:</strong></p>
<ul>
<li>What is the proper architecture to train CBOW encodings?</li>
<li>If this <strong>multiple hidden layer</strong> approach is <strong>correct</strong>, how do you not lose semantic information when you only use one of the layers as the encodings?</li>
<li>If the <strong>single hidden layer</strong> approach is <strong>correct</strong>, does anyone have examples of this being implemented using this approach in PyTorch (fine if no)?</li>
</ul>
<p><strong>Note:</strong> Very new to ML so feel free to correct me even on nit picky things so I can learn!</p>
","nlp"
"86375","How to add time as a feature into word embeddings?","2020-12-07 17:22:34","","0","140","<text-mining><nlp><plotting>","<p>I have a text corpus and I'm using TfidfVectorizer. Would it be possible to cluster the resultant matrix once I concatenate tf-idf vector and time feature matrix I built([year, month, day])?</p>
<p>I'm also working around using word2vec for plotting a graph of similar words and I want to add time dimension for these plots.</p>
<p><em>Example: Let's say, I've five popular bug areas and for these five areas, I'm plotting similar words from the corpus using Word2Vec and Plotly. I want to have time filters on my plot such that I should be able to see these plots for different years. If my entire corpus has data from past four years then I want to visualize it for a particular year based on user selection.</em></p>
<p>I'm not sure how to achieve this. Any ideas?</p>
","nlp"
"86361","Evaluation metric for Information retrieval system","2020-12-07 12:12:47","","3","449","<nlp><model-evaluations><information-retrieval><semantic-similarity>","<p>I am currently reading <a href=""https://arxiv.org/pdf/1907.00937.pdf"" rel=""nofollow noreferrer"">Semantic Product Search</a> paper published by Amazon. They are using two evaluation subtasks matching and ranking. In matching, they tune the model hyperparameters to
maximize Recall@100 and Mean Average Precision (MAP).</p>
<p>According to <a href=""https://nlp.stanford.edu/IR-book/pdf/08eval.pdf"" rel=""nofollow noreferrer"">Introduction to Information Retrieval</a>, Precision (P) is the fraction of retrieved documents that are relevant:<br></p>
<p><img src=""https://latex.codecogs.com/gif.latex?Precision&space;=&space;%5Cfrac%7Btotal-relevant-items-retrieved%7D%7Btotal-retrieved-items%7D&space;=&space;P(relevant|retrieved)"" title=""Precision = \frac{total-relevant-items-retrieved}{total-retrieved-items} = P(relevant|retrieved)"" /></p>
<p>Recall (R) is the fraction of relevant documents that are retrieved:<br></p>
<p><img src=""https://latex.codecogs.com/gif.latex?Recall&space;=&space;%5Cfrac%7Btotal-relevant-items-retrieved%7D%7Btotal-relevant-items%7D&space;=&space;P(retrieved|relevant)"" title=""Recall = \frac{total-relevant-items-retrieved}{total-relevant-items} = P(retrieved|relevant)"" /></p>
<p>I want to know how to come up with ground truth(relevancy label) if it's not available? In other words, if I want to calculate precision or recall for the Semantic product search and if we don't have relevancy label available for input product query. In that case, how researchers calculate precision and recall? or how do they generate it?</p>
","nlp"
"86357","Quality check for preprocessing of Text data","2020-12-07 10:41:20","","1","317","<nlp><text-mining><preprocessing>","<p>I have developed a pipeline for text data preprocessing with different clean up techniques like Stemming , Lemmatization, Stop words removal etc. But now the ask from the business team is to quantify the quality of the preprocessing steps (or, the text data it produced). How can we develop some metrics to evaluate the preprocessing quality of text data?</p>
","nlp"
"86304","In transformers, do you understand why are the Value (V) vectors comes from the encoder? And than normalize with the query (Q) vector?","2020-12-05 17:20:44","","0","431","<nlp><transformer><spatial-transformer>","<p>In transformers, there is a phase for rasidual connection, where the queries and the output from the attention are add and normalize.
Can one please give some advise to the motivation of it? Or maybe I get it wrong?
It seems to me that the values shouldn't come from the encoder, the values are the vector that we want to have attention on.
And if so. We should have add and normalize the values from the previous state and not the queries... I'm confused..</p>
","nlp"
"86252","Effect of Stop-Word Removal on Transformers for Text Classification","2020-12-03 20:24:23","87548","9","5166","<nlp><preprocessing><transfer-learning><transformer><text-classification>","<p>The domain here is essentially topic classification, so not necessarily a problem where stop-words have an impact on the analysis (as opposed to, say, sentiment analysis where structure can affect meaning).</p>
<p>With respect to the positional encoding mechanism in transformer language models, <strong>when using a pretrained LM</strong> is stop-word removal as a preprocessing step actively harmful if the LM was trained on a corpus where they were left in? I'm still working on fully understanding the mechanism but I feel like removing stop-words would affect which wavelength is used to construct the context between any given pair of words with stop words between them, which in turn would impact the encoding.</p>
<p>Or, would this not matter because the regression when trained figures it out from consistently processed input? I feel like it should matter but haven't been able to find anything on the topic.</p>
","nlp"
"86246","Non-uniform class occurances in input data for classification task - how to tackle it?","2020-12-03 17:36:02","","1","40","<nlp><dataset><multiclass-classification>","<p>So, I gathered political articles for my thesis, now I want to be able to classify given text. Though the classes distribution is actually crazy.</p>
<ul>
<li>Class 1: 964 docs</li>
<li>Class 2: 37,020</li>
<li>Class 3: 640</li>
<li>Class 4: 2,675</li>
<li>Class 5: 793</li>
<li>Class 6: 23,160</li>
<li>Class 7: 2,665</li>
</ul>
<p>Such a skewed data is obviously going to favor classes 2 and 6, though I thought about elevating the difference from last layer for classes with lower observations, is that worth a shot? Or it will actually create overfit for these classes? Unfortunately I can't scrap more data, the websites with articles doesn't have any more (at least now). Of course any data augmentation is not possible.</p>
","nlp"
"86228","Model to detect specific semantic content without labeled data","2020-12-03 08:52:56","","1","28","<classification><nlp><text>","<p>I want to build a model that can detect sentences that discuss requests for communication - like 'email me', 'phone us', 'contact us', etc. However, I do not have any labeled data which I can use to simply train a neural network.</p>
<p>How can I go about solving this problem? On way I was thinking was to create an initial labeled dataset by detecting above mentioned phrases. But I am not sure how to generalize and detect sentences expressing the similar sentiment using some other phrase not in my list.</p>
<p>I am open to any other ideas as well. Thank you.</p>
","nlp"
"86160","improve NER model accuracy with spaCy dependency tree","2020-12-01 14:58:40","","2","866","<nlp><nltk><named-entity-recognition><spacy>","<p>I have search at lot, was not able to find a solution for my problem...
I am training a NER model, that should detect two types of words: Instructions and Conditions. This is not the standard use-case of NER, as it does not search for specific types of words (e.g. Google == Corporation), but is rather much more depended on the sentence structure.</p>
<p>For example:
If the car <strong>crashes</strong>, the airbag should <strong>go</strong> off.</p>
<ul>
<li>'crashes' should be labeld: &quot;condition&quot;</li>
<li>'go' should be labeld: &quot;instruction&quot;</li>
</ul>
<p>When training the model, I want to provide for each sentence not only my annotations, but also the dependency tree of the sentence calculated by the 'en_core_web_sm' model. I want my model to not only train based on the given words but also train based on the sentence structure.</p>
<p>My training data currently looks like this, but I want to expand it by also adding the dependency tree of each sentence generated using the 'en_core_web_sm' model:</p>
<pre><code>train_data = 
    (&quot;If the car crashes, the airbag should activate&quot;, [(11, 17, 'CON'), (38, 46, 'INS')]),
    ...
]
</code></pre>
<p>This is my current training loop, using the update function from spaCy, but I am open on trying a different tool:</p>
<pre><code>import random 
import datetime as dt
from spacy.util import minibatch, compounding
from spacy.util import decaying

dropout = decaying(0.6, 0.2, 1e-4)

nlp = create_blank_nlp(TRAIN_DATA)
optimizer = nlp.begin_training()
for i in range(80):
    losses = {}
    batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))
    for batch in batches:
        texts, annotations = zip(*batch)
        nlp.update(
            texts,               # batch of texts
            annotations,         # batch of annotations
            drop=next(dropout),  # dropout
            losses=losses,
        )
    print(f&quot;Losses at iteration {i} - {dt.datetime.now()} {losses}&quot;)
</code></pre>
<p>I am curious if and how this is might be possible. It feels like a waste to not use the pretrained model (mind you, the pretrained NER model from spaCy probably will not help me, only the dependency part).</p>
<p>Open to any advice, thank you.</p>
","nlp"
"86132","Python to clean miswritten words with repetitive letters such as ""wwwwooorrrrddss"" to ""words""","2020-11-30 18:04:58","","0","166","<nlp><data-cleaning><python-3.x><text>","<p>When cleaning text-data in Python3 for NLP, are there are any 'common practices' when it comes to addressing repetitive letters in words such as &quot;wwwwooorrds&quot; to &quot;words&quot;, or &quot;fffunnnyyyyyy&quot; to &quot;funny&quot;?</p>
<p>The source of the miswritten words is an OCR and I am not able to address the issue upstream, and thought I would check if there was anything that I can do downstream to fix this.</p>
<p>Thanks!</p>
","nlp"
"86104","What is the difference between BERT architecture and vanilla Transformer architecture","2020-11-30 03:34:44","86108","3","2386","<nlp><bert><transformer><encoder>","<p>I'm doing some research for the summarization task and found out BERT is derived from the Transformer model. In every blog about BERT that I have read, they focus on explaining what is a bidirectional encoder, So, I think this is what made BERT different from the vanilla Transformer model. But as far as I know, the Transformer reads the entire sequence of words at once, therefore it is considered bidirectional too. Can someone point out what I'm missing?</p>
","nlp"
"86083","Optimal input setup for character-level text classification RNN","2020-11-29 10:01:00","","0","121","<neural-network><rnn><nlp><text-classification><language-model>","<p>I want to classify 500-character long text samples as to whether they look like natural language using a character-level RNN. I'm unsure as to the best way to feed the input to the RNN. Here are two approaches I've thought of:</p>
<ol>
<li>Provide the whole 500 characters (one per time step) to the RNN, and predict a binary class, <span class=""math-container"">$\{0,1\}$</span>.</li>
<li>Provide shorter overlapping segments (e.g. 10 characters) and predict the next (e.g. 11th) character. Convert this to classification by taking the test input and calculate the joint probability of the observed characters based on predicted next-character distributions.</li>
</ol>
<p>The first approach seems sub-optimal as I don't believe that the 1st character is going to have any effect on the prediction of the 500th character. The second approach gives me diminishingly small likelihoods when you calculate the joint probability.</p>
<p>I'm aiming for a more nuanced language model akin to n-gram frequency counting. I'm using simple RNNs for now but intend to swap to either LSTM or GRU.</p>
","nlp"
"86077","Why does GPU speed up inference?","2020-11-29 06:51:46","86090","1","822","<nlp><cnn><gpu>","<p>I understand that GPU can speed up training for each batch multiple data records can be fed to the network which can be parallelized for computation. However, for inference, typically, each time the network only processes one record, for instance, for text classification, only one text (i.e., a tweet) is fed to the network. In such a case, how can GPU speed up?</p>
","nlp"
"86026","k-means for customer review analysis","2020-11-27 09:43:15","","0","151","<python><nlp><k-means><sentiment-analysis>","<p>I have a dataset of amazon Alexa reviews and want to group negative and positive reviews in separate groups. Is k-means a good approach to it? The dataset is unlabeled so how will my model know which review is for the negative and which is for the positive cluster? Is there any other way you would like to suggest in which I can do the above task?</p>
","nlp"
"85972","BERT data cleaning","2020-11-26 10:35:07","","0","207","<nlp><preprocessing><bert><transformer>","<p>I am wondering which data cleaning steps should be performed if you want to re-fine a BERT model on custom text data.</p>
<p>Which steps should be performed?</p>
<p>Does it make sense to perform a stemming or lemmatization if it has not been applied to the initial training of the BERT Base/Large model?</p>
","nlp"
"85966","In sequence models, is it possible to have training batches with different timesteps each to reduce the required padding per input sequence?","2020-11-26 08:57:44","85971","4","1761","<keras><tensorflow><nlp><sequence><mini-batch-gradient-descent>","<p>I want to train an LSTM model with variable length inputs. Specifically I want to use as little padding as possible while still using minibatches.</p>
<p>As far as I understand each batch requires a fixed number of timesteps for all inputs, necessitating padding. But different batches can have different numbers of timesteps for the inputs, so in each batch inputs only have to be padded to the length of the longest input-sequence in that same batch. This is what i want to implement.</p>
<p>What I need to do:</p>
<ol>
<li>Dynamically create batches of a given size during training, the inputs within each batch are padded to the longest sequence within that same batch.</li>
<li>The training data is shuffled after each epoch, so that inputs appear in different batches across epochs and are padded differently.</li>
</ol>
<p>Sadly my googling skills have failed me entirely. I can only find examples and resources on how to pad the entire input set to a fixed length, which is what i had been doing already and want to move away from. Some clues point me towards tensorflow's Dataset API, yet I can't find examples of how and why it would apply to the problem I am facing.</p>
<p>I'd appreciate any pointers to resources and ideally examples and tutorials on what I am trying to accomplish.</p>
","nlp"
"85881","Are LDA clusters identical across different runs?","2020-11-24 12:20:44","85927","1","67","<nlp><clustering><lda>","<p>for a given corpus are the Latent Dirichlet Allocation clusters for it is unique in general?
How about the <code>gensim</code>  multi-process implementation of LDA? are there unique or they will be different for every run of code?</p>
","nlp"
"85829","Vector representation of documents for text classification","2020-11-23 15:04:05","","0","64","<machine-learning><nlp><word-embeddings><doc2vec>","<p>I'm looking for proper method of document embeddings. I know that <strong>doc2vec</strong> will give me the vector representations for given corpus, but how do I embed new documents? I need to train neural network that will classify text, but I have no idea how new documents should be embedded properly.</p>
","nlp"
"85815","For short sentences(max length 10 ), which Name entity recognition algorithm is good?","2020-11-23 06:08:15","","1","166","<machine-learning><deep-learning><nlp><nltk><named-entity-recognition>","<p>My Training data look like this .</p>
<p><a href=""https://i.sstatic.net/2l5J3.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/2l5J3.png"" alt=""enter image description here"" /></a></p>
<p>I have to recognize 4 class for each sentence.
Any algorithm , which have some learning parameters Means not rule based approach . So which method is good for my problem ?</p>
","nlp"
"85790","Why this TensorFlow Transformer model has Linear output instead of Softmax?","2020-11-22 15:08:08","85793","0","470","<deep-learning><tensorflow><nlp><transformer><attention-mechanism>","<p>I am checking this official TensorFlow tutorial on a <a href=""https://www.tensorflow.org/tutorials/text/transformer"" rel=""nofollow noreferrer"">Transformer model for Portuguese-English translation</a>.</p>
<p>I am quite surprised that <a href=""https://www.tensorflow.org/tutorials/text/transformer#create_the_transformer"" rel=""nofollow noreferrer"">when the Transformer is created</a>, their final output is a Dense layer with <strong>linear activation</strong>, instead of <strong>Softmax</strong>. Why is that the case? In the original paper <a href=""https://arxiv.org/pdf/1706.03762.pdf"" rel=""nofollow noreferrer"">Attention is All You Need</a> the image is pretty clear, there is a Softmax layer just at the end (Fig.1, p. 3).</p>
<p>How can you justify this difference, when your task involves building a language model and your Loss is based on sparse categorical crossentropy?</p>
","nlp"
"85784","Twitter Data-Analyse: What can I do with the data?","2020-11-22 14:02:09","85787","1","66","<nlp><data-cleaning><data-science-model><topic-model><twitter>","<p>I retrieve data to a specific topic from Twitter and did my sentiment analysis on it. I never did anything in NLP, etc. So what else can I do with that? &quot;Main goal&quot; would be to find out if the Twitter community is against this &quot;topic&quot; or not.</p>
<p>I am also struggling with cleaning the data and I mean by that, that I am unsure how much should I clean on that Tweet.</p>
<p>I would be also glad to get any advise on books, articles, communities, videos...</p>
","nlp"
"85777","Differentiate between positive and negative clusters","2020-11-22 10:54:53","85797","2","785","<python><nlp><k-means><word2vec><sentiment-analysis>","<p>I have applied k-means clustering on my dataset of Amazon Alexa reviews.</p>
<pre><code>model = KMeans(n_clusters=2, max_iter=1000, random_state=True, n_init=50).fit(X=word_vectors.vectors.astype('double'))
</code></pre>
<p>Now I want to check which cluster is positive and which is negative, can anyone suggest me some way to do that?</p>
<p>Also, is there any way to check is a particular word belongs to which cluster. E.g, the word 'bad' belongs to which cluster - 0 or 1</p>
","nlp"
"85729","How to collect info about unseen bugs given user's comments/feedbacks?","2020-11-21 00:52:38","85732","2","35","<python><nlp><data-cleaning>","<p>I have a dataframe which looks like:</p>
<pre><code>user_id, comment
0, 'Functional but Horrible UI'
1, 'Great everything works well'
2, 'I struggled finding plus button because of theme colors in dark mode'
3, 'Keeps stopping on Android 10'
4, 'I like the functionaity but color theme could be better'
5, 'Consistently crashing. Uninstalled'
6, 'Good overall'
7, 'sfdfsdlfksd'
8, 'I lost in complex settings'
9, 'Configuring app is really a headache'
10, 'aaaaaaaaaaaaa'
</code></pre>
<p>And I want to figure out some data science approach to pluck out information about what users are struggling with and which issues appeared how much and stuff like this. Even some simple output would be good for me so that we know which parts of app to focus on more. Like for sample above I am aiming for an output as simple as:</p>
<pre><code>problems = {
'color_theme': 3,
'app_settings': 2,
'crashing' : 2}
</code></pre>
<p>So I kinda wants labeling and how much time a label is occured based on to which label a review belongs. But the problem is I cannot train a model with predefined labels because:</p>
<ol>
<li><p>I do not have labels for reviews. If we have to go through each review to know what problem is it talking about (i.e. to label it), we would just have filed it as well and would know what we have to work on.</p>
</li>
<li><p>I do not know in advance what problems are gonna come in future so even if we somehow label all at some point in time, it wouldn't be enough as some unseen problem may come and we have to do again.</p>
</li>
<li><p>Even if we have a system of labeling somehow, how would we update model, like do we define a new model with a different architecture for ever changing labels?</p>
</li>
</ol>
<p>So under these circumstances, I was trying to figure out an AI approach to ease in my situation. I am pretty good at python and do have working knowledge of keras/tensorflow and other libraries but none of them seem to have such flexible model approach. I was going through Google Cloud Platform's AI platform as well but it could do sentiment analysis to an extent but not understand in an app context that e.g. button is a part of UI and color as well. So how could I approach this problem in a more elegant way?</p>
","nlp"
"85716","Why I would use TF-IDF after Bag-of-Words (CountVectorizer)?","2020-11-20 17:45:01","85717","1","1245","<nlp><tfidf><bag-of-words>","<p>In my recent studies over Machine Learning NLP tasks I found this very nice tutorial teaching how to build your first text classifier:</p>
<p><a href=""https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a"" rel=""nofollow noreferrer"">https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a</a></p>
<p>The point is that I always believed that you have to choose between using Bag-of-Words or WordEmbeddings or TF-IDF, but in this tutorial the author uses Bag-of-Words (CountVectorizer) and then uses TF-IDF over the features generated by Bag-of-Words.</p>
<pre><code>text_clf = Pipeline([('vect', CountVectorizer()),
...                      ('tfidf', TfidfTransformer()),
...                      ('clf', MultinomialNB()),
... ])
</code></pre>
<p>Is that a valid technique? Why would I do it?</p>
","nlp"
"85712","Difference between Word Embedding and Text Embedding","2020-11-20 16:34:30","85718","0","735","<python><nlp><word2vec><gensim>","<p>I am working on a dataset of amazon alexa reviews and wish to cluster them in positive and negative clusters. I am using Word2Vec for vectorization so wanted to know the difference between Text Embedding and Word Embedding. Also, which one of them will be useful for my clustering of reviews (Please consider that I want to predict the cluster of any reviews that I enter.) Thanks in advance!</p>
","nlp"
"85697","LDA-like algorithm but at the character level?","2020-11-20 10:49:10","","0","17","<machine-learning><nlp><text-mining><unsupervised-learning><pattern-recognition>","<p>I have a catalog of products and I'd like to find &quot;topics&quot; in their description. The problem is that you might find in the description things like GraphicsCardVendor10.2 GraphicsCardVendor_10.3 etc. Obviously, for us humans it is simple to unravel the topic &quot;Graphics Cards&quot;</p>
<p>Of course, I can do some preprocessing but I'm looking for a more holistic way to deal with that. I know some algorithms (like RNNs) can work on character-level.</p>
<p>My end-goal is to find patterns in the data. Sort of, points of interest.
Like, &quot;Hey look, I see a lot of graphics cards over here&quot;.</p>
<p>I'd like to get your suggestions what should be the right model for this problem.</p>
<ul>
<li>Note: The data is currently not labeled.</li>
</ul>
","nlp"
"85566","How pre-trained BERT model generates word embeddings for out of vocabulary words?","2020-11-17 19:34:13","85570","5","12585","<nlp><word-embeddings><bert><oov>","<p>Currently, I am reading <a href=""https://arxiv.org/abs/1810.04805"" rel=""noreferrer"">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a>. I want to understand how pre-trained BERT generates word embeddings for out of vocabulary words? Models like <a href=""https://arxiv.org/abs/1802.05365"" rel=""noreferrer"">ELMo</a> process inputs at character-level and can generate word embeddings for out of vocabulary words. Can BERT do something similar?</p>
","nlp"
"85552","Logistic Regression performs better on longer texts","2020-11-17 15:57:31","","0","30","<nlp><logistic-regression>","<p>I trained the LogisticRegression model with TF-IDF (both birgams and unigrams) and while predicting class it revealed that in longer texts (up to 3000 symbols)it works better that if I use short (+-100 symbols) texts. I assume that the reason is that bigram weights &quot;work better&quot; on longer texts, but it doesn't help me to understand why.</p>
<p>So any advice what to read to clarify this situation is welcome</p>
","nlp"
"85543","Working Behavior of BERT vs Transformers vs Self-Attention+LSTM vs Attention+LSTM on the scientific STEM data classification task?","2020-11-17 11:42:53","","1","123","<machine-learning><deep-learning><lstm><nlp><attention-mechanism>","<p>So I just used BERT pre-trained with Focal Loss to classify Physics, Chemistry, Biology and Mathematics and got a good f-1 macro of 0.91. It is good given it only had to look for the tokens like <code>triangle</code>, <code>reaction</code>, <code>mitochondria</code> and <code>newton</code> etc in a broader way. Now I want to classify the the Chapter Name also. It is a bit difficult task because when I trained it on BERT for 208 classes, my score was almost 0. Why? I an get that there are lots of information also like <code>nacl: sodium chloride</code> , <code>bohr model</code> <code>9.8 m/sec</code> etc which I think BERT is not trained for. I want to ask few questions.</p>
<ol>
<li><strong>Is BERT useful in these conditions? Is it trained on scientific terms. I mean can it get the context of <code>Schrödinger equation</code> to <code>Plank's Constant</code>? If not, I don't think I should use it because I don't have enough data to re-train BERT. Anything but BERT</strong></li>
<li><strong>Can I use FastText or GloVe? Cn they get the meaning or context</strong>?</li>
<li><strong>Or should I simply create my own embeddings in <code>pytorch/keras</code> and keep <code>nacl,fe,ppm</code> as they are and hope either of <code>Transformer</code> or <code>Attention</code> will capture it</strong>?</li>
</ol>
<p>Please help. I have a data of 120K questions/data points.</p>
","nlp"
"85528","Converting a string to a recommendation type string","2020-11-17 00:25:51","","0","17","<machine-learning><nlp><text><text-classification>","<p>I am trying to build a recommendation system and some of the labels are</p>
<pre><code>'printer has been replaced',
'scanner replaced',
'updated software to windows 10',
'password has been changed',
'monitor replaced',
'...'
</code></pre>
<p>What I want to do is convert these strings into more action item/ recommendation strings. Something like this</p>
<pre><code>'I would recommend replacing the printer',
'I would recommend replacing the scanner',
'A software update to windows 10 is needed',
'I would recommend changing the password',
'I would recommend replacing the monitor'
'...'
</code></pre>
<p>Is there a library out there that does this? I have over a 1000 different resolution strings so the solution would need to be scalable.</p>
","nlp"
"85510","From where does BERT get the tokens it predicts?","2020-11-16 19:00:50","85524","2","475","<nlp><bert><language-model><tokenization>","<p>When BERT is used for masked language modeling, it masks a token and then tries to predict it.</p>
<p>What are the candidate tokens BERT can choose from? Does it just predict an integer (like a regression problem) and then use that token? Or does it do a softmax over all possible word tokens? For the latter, isn't there just an enormous amount of possible tokens? I have a hard time imaging BERT treats it like a classification problem where # classes = # all possible word tokens.</p>
<p>From where does BERT get the token it predicts?</p>
","nlp"
"85367","Is applying pre-trained model on a different type of corpus called transfer learning?","2020-11-13 17:05:21","85372","1","51","<nlp><transfer-learning>","<p>I trained my classification model on corpus A and evaluated it on corpus B.</p>
<p>I do it, because for corpus A I have a lot more labeled sentences than for B. Nature of sentences used in A is different than sentences using in B. A has name of products from e-shop, B has names of products as they appear in shopping lists, with all slang, abbreviations, spelling errors and private notes.</p>
<p>Am I doing transfer learning?</p>
","nlp"
"85328","How to pass input to deep learning models for Multiple choice question answering problem?","2020-11-13 01:23:52","","0","395","<classification><tensorflow><nlp><pytorch><question-answering>","<p>I'm currently working on a multiple-choice question answering system. The training set consists of a question, answer and 4 options and I need to predict the correct answer among 4 options. Sometimes there is one paragraph too,  For example :</p>
<pre><code>1.Which among the following is measured using a Vernier Caliper?

[A] Dimensions
[B] Time
[C] Sound
[D] Temperature

Answer : A [Dimensions]

Chapter text: [Book chapter related to Dimension, time, sound and temperature ]
</code></pre>
<p>How to feed this input to any of deep learning models?
I thought two approaches :</p>
<ol>
<li>Using  tokens</li>
</ol>
<p><a href=""https://i.sstatic.net/bDCGi.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bDCGi.png"" alt=""enter image description here"" /></a></p>
<p>and correct and as one hot encoding =&gt; [1, 0, 0, 0 ]</p>
<ol start=""2"">
<li>Using concatenation</li>
</ol>
<p>Generating fix sized word embedding for each text :</p>
<pre><code> - Chapter text = [1,1024] 
 - Text         = [1,1024] 
 - option_a     = [1,1024] 
 - option_b     = [1,1024] 
 - option_c     = [1,1024] 
 - option_d     = [1,1024]
</code></pre>
<p>final_input = concat( [ Chapter text, Text, option_a, option_b, option_c, option_d] ) ==&gt; [1,6144]</p>
<pre><code>and correct and as one hot encoding =&gt; [1, 0, 0, 0 ]
</code></pre>
<p>Is it good representation for understanding and reasoning over text for mcqa task?</p>
","nlp"
"85301","Why does an attention layer in a transformer learn context?","2020-11-12 15:31:37","","1","688","<neural-network><nlp><transformer><sequence-to-sequence><attention-mechanism>","<p>I understand the transformer architecture (from &quot;Attention is All You Need&quot;), as well as how the attention is computed in the multi-headed attention layers.</p>
<p>What I'm confused on is <em>why</em> the output of an attention layer is a context vector. That is to say: what is it about the way that a transformer is trained causes the attention layers to learn context? What I would expect to see in the paper is a justification along the lines of &quot;when you train a transformer using attention on sequence-to-sequence tasks, the attention layers learn context <em>and here's why...</em>&quot;. I believe it because I've seen the heatmaps that show that attention between related words, but I want to understand why that is necessarily the result of training a transformer.</p>
<p>Why couldn't it be the case that the attention layers learn some other features that happen to also be beneficial in sequence to sequence tasks? How do we know that they learn context, other than that's what we observe?</p>
<p>Again, I get the math and I know there are several posts about it. What I want to know is <em>what about the math or the training process implies that the attention layers learn context</em>.</p>
","nlp"
"85274","NER_Multiple_entities","2020-11-12 02:20:07","","1","11","<machine-learning><nlp><text-mining><named-entity-recognition>","<p>I am working on a problem of entity extraction which requires me to extract variables of interest from a text document. My challenge is that the text contains multiple entities of a variable, for ex. For a variable &quot;name&quot;, there are multiple names but I only have to extract a particular name of interest. Likewise there are other variable with multiple entities. I have 2 questions regarding this.</p>
<ol>
<li><p>How should I prepare my data for training?. Should I annotate all the entities for a variable or just the entity of interest. For ex: Out of names like &quot;james&quot;, &quot;dave&quot; and &quot;harry&quot;, if name of interest is &quot;james&quot;, then should I annotate only &quot;james&quot; or all the names for training.</p>
</li>
<li><p>Is there any particular method to approach such problem?</p>
</li>
</ol>
","nlp"
"85231","Mining timelines in a long text","2020-11-11 01:43:49","","1","51","<nlp><time-series><data-mining><sequential-pattern-mining>","<p>I am trying to detect timeline of brands histories. For my specific case, I believe it is easy because data is already clustered. For each Wikipedia article I can spot sentences surrounding dates. Here is an example:</p>
<blockquote>
<p>McDonald's Corporation is an American fast food company, founded in
1940 as a restaurant operated by Richard and Maurice McDonald, in San
Bernardino, California, United States. They rechristened their
business as a hamburger stand, and later turned the company into a
franchise, with the Golden Arches logo being introduced in 1953 at a
location in Phoenix, Arizona. In 1955, Ray Kroc, a businessman, joined
the company as a franchise agent</p>
</blockquote>
<p>From this, it is easy to narrow results programmatically to</p>
<blockquote>
<p>McDonald's is founded in 1940</p>
<p>Golden Arches logo introduced in 1953</p>
<p>Ray Kroc, a businessman, joined the company in 1955</p>
</blockquote>
<p>This seems easy if documents are clustered. If not, I am thinking of a basic algorithm to mine timelines 'or natural numbers). So I want to discuss existing studies and my intuition here.</p>
<p>Definitions:</p>
<ol>
<li>Timeline: a logical succession of events on one single subject.</li>
<li>Dates in a timeline are natural numbers, and can be &quot;unordered relatively&quot;.</li>
<li>Timelines are continuous (one range like) and cannot intersect.</li>
</ol>
<p>Let's ignore the NLP related part, and try to figure out timelines in natural numbers ignoring topics (1st definition).</p>
<p>Distance: Initial timeline length. It represents the minimum.</p>
<p>Example:</p>
<p><strong>Step A</strong></p>
<pre><code>1, 4, 2, 5,  3, 8, 7, 9, 20, 21, 23, 24, 1, 5, 7, 9

dist = 4
</code></pre>
<ul>
<li>Becomes:</li>
</ul>
<p>1, 4, 2, 5/  3, 8, 7, 9/ 20, 21, 23, 24/ 1, 5, 7, 9</p>
<ul>
<li><p>Score each set (of 4 elements): Scoring is critical but lets think of bubble sort score, where score = 1 / number exchange ops.</p>
<p>1,4,2,5 =&gt; 1/1  | 3,8,7,9 =&gt; 1/1</p>
</li>
</ul>
<p><strong>Step B</strong></p>
<p>The reason to score sets is to identify if a set represents a timeline or the combination of two sets represent a timeline, to decide, we score the combined set and divide by two</p>
<pre><code>1,4,2,5,3,8,7,9 =&gt; 5/2
</code></pre>
<p>We conclude <code>1,4,2,5</code> and <code>3,8,7,9</code> are two sets, while <code>1,4,2,5,3,8,7,9</code> is not.</p>
<p>We move sequentially to process next sets.</p>
<p>The reason I said Distance is minimum is that before comparing scoring initial sets, <em>we first   identify sets of 4, 5, 6 or more elements and score them</em> (<strong>(step A)</strong>) and only take separate sets with better score (minimum bubble sort score here).</p>
<p>Any thoughts ?</p>
","nlp"
"85218","What is considered short and long text in NLP (document similarity)","2020-11-10 19:10:08","85254","1","445","<nlp><similar-documents>","<p>What is considered short and long text in NLP?</p>
<p>I'm working on a dataset that contains documents from 10 to 600 words and I'm asking myself if I should treat them differently. Also, I haven't found a source which explicitly defines short and long text in NLP yet. The goal for my task is to find similar documents.</p>
","nlp"
"85208","How to find correlated knowledge among different documents?","2020-11-10 15:01:58","85210","-1","30","<nlp><information-retrieval>","<p>Say I have a sequence of documents clicked by a user, how can I mine the identical or semanticly similar word/knowledge/phrases shared among different documents?</p>
<p>Maybe someone can give a paper or subject relating to my goal?</p>
","nlp"
"85065","What is the structure and dimension of input passed to neural network when training CBOW and SKIP GRAM word embedding","2020-11-07 13:58:35","","4","349","<nlp><word-embeddings>","<p>I am confused about input passed to neural network in natural language processing <strong>(NLP)</strong> when training <code>CBOW</code> word embedding from scratch.  I read the paper and have some doubts.</p>
<p>In general neural network <strong>(NN)</strong> architecture, it is more clear that each row act's as input to neural network with <code>d</code> features. For example in the figure below:</p>
<p><a href=""https://i.sstatic.net/LEihs.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/LEihs.png"" alt=""enter image description here"" /></a></p>
<p><code>X1, X2, X3</code> is one input, or one row of the data-frame. So here, one data point is of <code>dimension 3</code> and data-frame would be like this:</p>
<pre><code>X1  X2  X3
1   2   3
4   5   6
7   8   9
</code></pre>
<p><strong>Is my understanding correct?</strong></p>
<p>Now coming to <code>NLP</code>, <strong>CBOW</strong> architecture: Lets take an example to train <strong>CBOW</strong> word embeddings:</p>
<p><code>Sentence1</code>: <strong>&quot;I like natural processing domain.&quot;</strong></p>
<p>Creating training data from above sentence, <strong>window size=1</strong></p>
<pre><code>Input                      output

(I,natural)                like
(like,processing)          natural
(natural,domain)           processing
(processing)               domain
</code></pre>
<p>Is the above creation of training data for <strong>CBOW</strong> architecture for <strong>window size=1</strong> correct?</p>
<p>My Questions are below:</p>
<p>How will I pass this training data to neural network for the above figure?</p>
<p>If I represent every word as <code>one-hot encoded</code> vector of dimension equal to size of <code>vocabulary V</code> as input to neural network, then how should I pass 2 words at the same time of dimesion <code>2V as input</code>.</p>
<p>Is this the way to pass the input for first training sample: I just concatanated the two input words:</p>
<p><a href=""https://i.sstatic.net/ek0D1.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ek0D1.jpg"" alt=""enter image description here"" /></a></p>
<p>Then I train the network to learn word-embeddings using cross entropy loss?</p>
<p><code>Is this the right way to pass input?</code></p>
<p><code>Secondly, the middle layer will give us the word embeddings for 2 input words or the target words??</code></p>
","nlp"
"85049","Is smoothing in NLP ngrams done on test data or train data?","2020-11-07 03:28:55","","1","179","<machine-learning><deep-learning><nlp>","<p>Is smoothing in NLP ngram done on test data or train data?</p>
<p>Since smoothing is to avoid the language model predicting 0 probability of unseen corpus (test). So I wonder is smoothing done on test data only? Or on train data only? Or both? I don't seem to find an answer to this yet.</p>
","nlp"
"85041","making conclusions after sentiment analysis","2020-11-06 20:09:29","","1","78","<python><nlp><statistics><sentiment-analysis><spacy>","<p>After performing some sentiment analysis, I have a dataset that looks like this:
For different products, using online reviews, I have obtained some values for positive/negative sentiments. However, now I am unable to figure out how to draw conclusions for this.</p>
<p>I had the idea of using correlation but need ideas on what features could be created &amp; what comparisons could be made?</p>
<p>The dataset includes different &quot;Features&quot; like webcam, screen, mousepad for different products (product name).</p>
<pre><code>id     Date       Website       Product Name    Brand   Stars   Feature   Sentiment  Positive     Negative       Anger      Happiness       Annoyance
0     2020.8.03   eBay    Lenovo Hi-Fi 320    Lenovo     4      Screen      NEGATIVE    0.000047    0.999851    0.000101    0.108132    0.248220    
</code></pre>
","nlp"
"85020","Are GPTs close to real intelligence or just another Data In -Data Out -Data Permutation and Combinations?","2020-11-06 13:50:39","","0","20","<machine-learning><deep-learning><nlp><data><data-augmentation>","<p>Use cases and Solutions surrounding GPT's have taken NLP world with storm and started the GPT-Best vs GPT- Not So Best war on the internet. There are solutions been derived from API's provided by HF. What would be the impact and how feasible would be GPT development and deployment in the real world use case across domains?</p>
","nlp"
"84930","Weights shared by different parts of a transformer model","2020-11-04 04:04:37","86363","4","4007","<machine-learning><neural-network><deep-learning><nlp><transformer>","<p><a href=""https://i.sstatic.net/R2IDD.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/R2IDD.png"" alt=""enter image description here"" /></a></p>
<p>Which parts of a transformer share weights, like, do all the encoders share the same weight or all the decoders share the same weights?</p>
","nlp"
"84881","What is the best approach to extract keys/values from documents?","2020-11-03 06:21:25","","1","1914","<machine-learning><nlp><named-entity-recognition><ocr>","<p>I am thinking of training a model to automatically extract information from more or less structured documents like invoices.</p>
<p>Here are the main challenges regarding this task:</p>
<ol>
<li><p>In fact, even though invoices are often called &quot;structured&quot; documents, there is a lot of variance in their layouts depending on the field, company, and other factors, which makes it almost impossible to achieve great results with pattern matching.</p>
</li>
<li><p>When relying on text, the most straightforward solution to get textual information from documents is using an OCR engine like Tesseract or EasyOCR (or, probably, commercial solutions from Google, Amazon, Microsoft). Unfortunately, more often than not, outputs from OCR libraries are rather dirty, which can be explained by the quality of scans.</p>
</li>
<li><p>Some fields can appear in a document multiple times. This is especially true for line items</p>
</li>
<li><p>Some values might belong to several classes (entities) at the same time. For example, extracting addresses, company names is definitely not a one-to-one mapping in that several companies are mentioned with the corresponding addresses and other types of information.</p>
</li>
</ol>
<p>That said, what could you recommend as a possible approach to this problem? I understand that given (1)-(4) there is no way I should expect a perfect solution, but at least something that would at least make sense to try.</p>
<p>My current thoughts:</p>
<ul>
<li><p>This problem has a lot of similarity with traditional NER-like tasks, but those are most often trained on sentenced where there's a lot of semantical value and not just key/value pairs.</p>
</li>
<li><p>I am almost 100% certain that without some rules, even some hardcoded ones, it will be extremely difficult to get something useful. One such idea is to use a pre-trained NER model, extract standard entities like MONEY, DATE, COMPANY, etc, and then just look in the neighborhood of these entities (in terms of coordinates, I mean) to check whether phrases like <code>Total amount</code> or <code>Invoice date</code> are nearby.</p>
</li>
<li><p>Probably, it makes sense to use both bounding boxes and text values for labeling and further model building. On the other hand, it would be quite challenging to do that labeling because some entities might change their location from document to document, span over several lines, etc.</p>
</li>
</ul>
<p>What should I read/try to have a better understanding of how to approach this problem?</p>
<p>Thanks in advance!</p>
","nlp"
"84838","Extract names from email address","2020-11-02 13:39:01","84847","1","1633","<machine-learning><deep-learning><nlp>","<p>Say I have two email addresses and I would like to see if it is likely that they belong to the same person. For example, <code>Cameron_M_Thompson@company.com</code> and <code>cthompson1024@personal.com</code> is likely to be from the same person (it doesn't have to be certain, providing the likeliness would be sufficient).</p>
<p>I had two directions in mind to achieve this, one is a string comparison between the two email addresses and the other is to first extract the names from the email addresses then compare if they might be the same person. Like in the example above, the names extracted should be <code>Cameron M Thompson</code> and <code>c thompson</code>.</p>
<p>I am also wondering if given that one of the email addresses is guaranteed to contain the full name (usually company email addresses have the full name), would that help the extraction of name in the other email address (personal email addresses might not always contain the full name), or would that help on the comparison of the two email addresses.</p>
<p>I have had a hard time trying to figure out if any of the above two directions would be feasible. Especially when email addresses might not have separators and names can vary a lot that a listing might not be sufficient to find a match.</p>
<p>How should I proceed in solving this problem? Would machine learning / deep learning help or I should go with something else simple like regex and fuzzy string match?</p>
<p>UPDATE:
I have a dataset that has two columns, email address and name, and about 2k rows there. I believe this could be used for the second direction (name extraction). For the first direction (string comparison similarity), I am thinking of modifying the dataset to three columns (email address 1, email address 2, label of whether they are the same person), which should give about 1k rows of data.</p>
","nlp"
"84805","What is the shape of the vector after it passes through the TfidfVecorizer fit_transform() method?","2020-11-01 21:53:48","","0","1255","<scikit-learn><nlp>","<p>I am trying to understand what happens inside the IDF part of the TFIDF vectorizer.</p>
<p><a href=""https://i.sstatic.net/bD4Gf.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bD4Gf.jpg"" alt=""SKlearn tfidf vectorizer"" /></a></p>
<p>The official scikit-learn page says that the shape is <code>(4,9)</code> for a corpus of 4 documents having 9 unique features.</p>
<p>I get the Term Frequency (TF) part, it makes sense to me that ( for every unique feature(9), for each document(4) we calculate each term's frequency, so we get a matrix of shape (4,9)</p>
<p>But what does not make sense to me is the IDF part the formula for IDF is:</p>
<p><span class=""math-container"">$$\text{idf}(t,D) = \text{log} \ {{N} \over {| \{ d \in D:t \in d \} | }}$$</span></p>
<p>with</p>
<ul>
<li><p><span class=""math-container"">$N$</span>: total number of documents in the corpus <span class=""math-container"">$N = |D|$</span></p>
</li>
<li><p><span class=""math-container"">$| \{ d \in D:t \in d \} |$</span> : number of documents where the term <span class=""math-container"">$t$</span> appears (i.e., <span class=""math-container"">$\text{tf}(t,d) \neq 0$</span>). If the term is not in the corpus, this will lead to a division-by-zero. It is therefore common to adjust the denominator to <span class=""math-container"">$1 + | \{ d \in D:t \in d \} |$</span>.</p>
</li>
</ul>
<p>So applying this formula, for every feature (9)  we calculate the log(total number of documents / number of documents having the term or feature in it)
I think
This will result in a shape of (1,9), please correct my understanding here.</p>
","nlp"
"84804","SKLEARN GridSearchCV hinting higher accuracy than Pipeline but with same parameters as Pipeline estimators","2020-11-01 19:06:59","","0","878","<scikit-learn><nlp><tfidf><grid-search>","<p>I have pipeline estimators like this:</p>
<pre><code>text_clf = Pipeline([
      ('tfidf', TfidfVectorizer(max_df=0.95, min_df=2, max_features=n_features, stop_words='english')),
  ('clf', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=random_state, max_iter=5, tol=None)),
])
text_clf.fit(dataset.data, dataset.target)
</code></pre>
<p>Then when evaluating the model accuracy like this</p>
<pre><code>mean = np.mean(predicted == twenty_test.target)
print(&quot;mean %0.3f&quot; % mean)
</code></pre>
<p>I get score 0.802.</p>
<p>Then when I add the GridSearch to get the best params like this:</p>
<pre><code>parameters = {
     'tfidf__use_idf': (True, False),
     'clf__alpha': (1e-2, 1e-3),
}

gs_clf = GridSearchCV(text_clf, parameters, cv=5, n_jobs=-1)
gs_clf = gs_clf.fit(dataset.data, dataset.target)
</code></pre>
<p>(Note that i am fitting GridSearch on TRAIN data same as the pipeline - although when i first tried fit it on TEST data by mistake the result was same though.)</p>
<p>It reports this:</p>
<pre><code>grid search best score 0.868
Best params: clf__alpha: 0.001
Best params: tfidf__use_idf: True
</code></pre>
<p>Note, that these params are already set on the model but the score is lower in the pipeline.</p>
<p>Is it because the other parameters I set in Pipeline are not kept when using GridSearch or?</p>
<p>Btw how the Grid Search knows the best parameters if I didn't provide any testing data.</p>
<p>Another problem is, that when I added more parameters to adjust in Grid search and then applied it to Pipeline, the accuracy didn't change at all. (or changed from 0.802 to 0.805, but GS hinted 0.867)</p>
","nlp"
"84711","Is there a clustering algorithm which accepts some clusters as input and outputs some more clusters?","2020-10-30 15:15:05","","3","376","<machine-learning><classification><nlp><clustering><labels>","<p>Heres the task: I have data I don't know much about. The final task is to build a classifier to classify the samples into a few categories. Some of the categories are pretty clear, we can easily use these as labels for a classifier. But I guess there are more useful categories possible, because right now <strong>most of my samples don't belong to any category</strong>. As I am no expert in the specific field, I would like to use some clustering algorithm to show possible label ideas.
When using traditional clustering algorithms, they find all sorts of patterns in the data I am not interested in.</p>
<p>So I am looking for a way to tell the algorithm: &quot;Hey, find some clusters in my data, but please take the existing clusters (or labeled data) into account.&quot; This should tell the clustering algorithm what I am interested in, and in what not.</p>
<p><strong>Does something like this exists? Or any other idea how to solve the problem of finding additional labels?</strong></p>
<p>BTW: in my case, I am doing NLP.</p>
","nlp"
"84652","Stemmer or dictionary?","2020-10-29 13:08:31","84672","4","159","<nlp>","<p>I have recently ported a stemmer from Java to Python for a highly inflectional language.</p>
<p>The stemmer learns how to change suffixes from the dictionary of words and their inflected forms. It basically builds a stemming table with learned stemming rules. As I was porting the algorithm I decided to train it on a larger dictionary. As a result, the learned stemming table got bigger, and stemming accuracy got higher as well.</p>
<p>Then I thought this actually make no sense as the stemming table size gets closer and closer to the size of the dictionary.</p>
<p>Why build or train stemming algorithms if you can simply lookup a dictionary?</p>
<p>I can understand that in old times storing large files could be a problem, but now? And for some languages there might be no proper dictionary resources. But is there any other reason?</p>
","nlp"
"84587","Bug in sentiment analysis and classification for unlabeled text","2020-10-27 20:37:38","84590","2","46","<machine-learning><deep-learning><nlp><sentiment-analysis><text-classification>","<p>I'm working on the transcript of Trump and Biden's debate and want to analyze the sentences and classify negative, positive, or neutral comments, but I ran into one problem. I used both TextBlob and the transformers pipeline to analyze the sentiment but unfortunately in both ways, there are some very disastrous flaws!</p>
<p>For example,I found that TextBlob recognizes <strong>-0.70 polarity</strong> in <em>&quot;fewer people are dying every day&quot;</em> (negative comment)</p>
<p>or the transformers pipeline recognizes <em>&quot;The audience here in the hall has promised to remain silent.&quot;</em> as a <strong>negative comment with 0.99 percent certainty</strong>!</p>
<p><strong>Why do you think it's happening?
Is there any way we can prevent this?
Is there any way better than this for analyzing the sentiment of unlabeled text?</strong></p>
<p>Also, I'm not comfortable with sentences like &quot;Oh, Really?!&quot; being classified as neutral. It's more of a sarcastic or negative comment I think.</p>
<p>Here's my <a href=""https://colab.research.google.com/drive/1ng80Q2CDN8FYRLDaHGLnQBEq-ALs5GnU?usp=sharing"" rel=""nofollow noreferrer"">colab notebook</a>, I've added one &quot;Problem&quot; markdown where I've observed these examples.</p>
","nlp"
"84571","Understanding the generality of the NER problem","2020-10-27 11:56:03","84572","1","229","<nlp><named-entity-recognition><text-classification>","<p>Named-entity recognition (NER) is a well-known problem in the NLP literature.</p>
<p>It typically addresses the problem to locate and classify named entities in text, e.g. <code>Organizations</code> and <code>Products</code>.</p>
<p><a href=""https://i.sstatic.net/xHiox.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/xHiox.png"" alt=""enter image description here"" /></a></p>
<p>I'm trying to solve a similar problem but, in my view, a bit more general. Given an input text, I want to be able to comprehensively annotate the whole text; not only specific entities like <code>Actors</code> and <code>Organizations</code> but also higher-level concepts like <code>Conditions of Applicability</code> and <code>Temporal Conditions</code>, e.g.:</p>
<p><a href=""https://i.sstatic.net/MY92C.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/MY92C.png"" alt=""enter image description here"" /></a></p>
<p>The added difficulty is that we have nested &quot;entities&quot;, e.g. (from above):</p>
<pre><code>&lt;denotic&gt; must, &lt;temporal&gt; within the specified period &lt;/temporal&gt;, notify ... &lt;/deontic&gt;
</code></pre>
<p>Can this still be formulated as a NER problem? If so, what would be the best type of model to solve this task assuming a dataset of ~ 50 K examples?</p>
","nlp"
"84535","Reuters Dataset Labels description","2020-10-26 15:29:30","84537","0","135","<nlp><dataset>","<p>the list of Reuters dataset labels are provided with the dataset and also available in varous online resources. <a href=""https://martin-thoma.com/nlp-reuters/"" rel=""nofollow noreferrer"">Here is an example.</a></p>
<p>But I couldn't find what each of these labels actually means.<br />
I was able to find out 'acq' is short for 'Acquisitions'. But yet don't know these ones:<br />
<strong>dlr, gnp, bop</strong></p>
<p>Is there a description availbale about what these labels represent?</p>
","nlp"
"84488","How to balance time/effort with transformations, feature selection, and models efficacy in nlp?","2020-10-25 17:34:49","","1","25","<nlp><feature-selection><optimization><model-selection><performance>","<p><strong>Edit:</strong> Question has been edited for reopening (see comment section for justification)</p>
<p>Being to new text analytics, I haven't gotten the hang of navigating a typical workflow given the longer times associated with the larger feature sets. My question then is how does one navigate optimization decisions when the cost of exploring all are often so high?</p>
<p>To provide a <strong>specific example</strong>, in a single text analytics problem, I have prepared a few different transformations of my training set:</p>
<ul>
<li>Stemmed vs lemmatized</li>
<li>Count vectorization vs TF-IDF vectorization</li>
<li>Full feature space vs 30% less features (identified by correlation analysis)</li>
<li>All cross-combinations of the above</li>
</ul>
<p>In an effort to get a sense of which of the transformation sets above I should run further tuning on, I ran <em>untuned</em> RF, Logistic, Naive Bayes, SGD, and KNN models on (with cross validation). Unfortunately, it was clear no transformation combination really stood out as a likely &quot;winner&quot;.</p>
<p><strong>How does one proceed here?</strong> All decision points are of equal merit empirically, but exploring all seems strategically wasteful if not impractical.</p>
","nlp"
"84404","NLP Bert model to to calculate text similarity, same sentence but not close similarity","2020-10-23 09:23:07","","1","222","<nlp><bert>","<p>Dear expert here:</p>
<p>I have a simple program to calculate text similarity. The program is copied from internet. Initially, I have a list of sentences or stored in db and fetched from db, then</p>
<ol>
<li>I make the list vectorizered and convered into bert model's vector, then using cosine to calculate the similarity.</li>
<li>2nd time, I instantiate a new vectorizer and vectorizer only one sentence and then compare against the first time bert model's vector, the result is different from 1st one.</li>
<li>3rd time, I using original vectorizer and vectorizered same sentence again, I notice the result is still different.</li>
</ol>
<p>Conclusion，one has to vectorizer sentence one by one, not a list. Why???</p>
<pre><code>from scipy import spatial
from sent2vec.vectorizer import Vectorizer

sentences = [
    &quot;This is an awesome book to learn NLP.&quot;,
    &quot;This is an awesome book to learn NLP.&quot;,
    &quot;This is a great book to learn NLP&quot;,
    &quot;This is a great book to learn NLP bert model&quot;,
    &quot;DistilBERT is an amazing NLP model.&quot;,
    &quot;We can interchangeably use embedding, encoding, or vectorizing.&quot;,
    &quot;How about a chinese dinner? this shall be close to 1 or close to 0, I suppose it is close to 1, i.e. the distance is very big&quot;
]

vectorizer = Vectorizer()
vectorizer.bert(sentences)
vectors_bert = vectorizer.vectors

dist_1 = spatial.distance.cosine(vectors_bert[0], vectors_bert[1])
dist_2 = spatial.distance.cosine(vectors_bert[0], vectors_bert[2])
dist_3 = spatial.distance.cosine(vectors_bert[0], vectors_bert[3])
dist_4 = spatial.distance.cosine(vectors_bert[0], vectors_bert[4])
dist_5 = spatial.distance.cosine(vectors_bert[0], vectors_bert[5])
dist_6 = spatial.distance.cosine(vectors_bert[0], vectors_bert[6])
print('dist_1: {0}, dist_2: {1}, dist_3: {2}, dist_4: {3}, dist_5: {4}, dist_6: {5}'.format(dist_1, dist_2, dist_3, dist_4, dist_5, dist_6))
   

test2 = []
test2.append(sentences[0])
vectorizer2 = Vectorizer()
vectorizer2.bert(test2)
vectors_bert2 = vectorizer2.vectors

dist_same_string_different_vectorizer = spatial.distance.cosine(vectors_bert[0], vectors_bert2[0])
print(&quot;dist same string different vectorizer is {0}&quot;.format(dist_same_string_different_vectorizer))

test3 = []
test3.append(sentences[0])
vectorizer.bert(test3)
vectors_bert3 = vectorizer.vectors
print(&quot;length of vector_bert3 is {0}&quot;.format(len(vectors_bert3)))
dist_same_string_same_vectorizer_different_time = spatial.distance.cosine(vectors_bert[0], vectors_bert3[0])
print(&quot;dist same string same vectorizer different time is {0}&quot;.format(dist_same_string_same_vectorizer_different_time))
</code></pre>
<p>/2/result</p>
<pre><code>dist_1: 0.0, dist_2: 0.039356231689453125, dist_3: 0.06737476587295532, dist_4: 0.046457767486572266, dist_5: 0.12459474802017212, dist_6: 0.1990041732788086
dist same string different vectorizer is 0.21568220853805542
length of vector_bert3 is 1
dist same string same vectorizer different time is 0.21568220853805542

Process finished with exit code 0
</code></pre>
","nlp"
"84324","An algorithm for Automatic Tag Clustering","2020-10-21 14:45:16","","-1","25","<nlp><clustering><word2vec><knowledge-base>","<p>Out website <a href=""https://dinf.net"" rel=""nofollow noreferrer"">dinf</a> is somewhat like StackExchange: people are submitting small definitions of concepts.
We would like to automatically assign those concepts into 'Topics'.
The problem is that dinf by default limits any definition to max of 500 characters.
Which algorithm / module we can use to assign those concepts assuming that all topics are known in advanced?</p>
","nlp"
"84316","Why BERT tokenizers function differently?","2020-10-21 11:56:15","","1","265","<machine-learning><deep-learning><nlp><tokenization>","<p>While experimenting with transformers' <em>TFBertForSequenceClassification</em> and <em>BertTokenizer</em>, I noticed that BertTokenizer:</p>
<pre><code>transformer_bert_tokenizer = BertTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
</code></pre>
<p>tokenizes the text differently from the tokenizer that I use to construct for my BERT models in this way:</p>
<pre><code>!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py
import tokenization
FullTokenizer = tokenization.FullTokenizer
</code></pre>
<p>and then</p>
<pre><code>BERT_MODEL_HUB = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2'
bert_layer = hub.KerasLayer(BERT_MODEL_HUB, trainable=True)
to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()
vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()

tokenizer = FullTokenizer(vocabulary_file, to_lower_case)
</code></pre>
<p>as an example:</p>
<pre><code>sequence = &quot;Systolic arrays are cool. This 🐳 is cool too.&quot;
transformer_bert_tokenizer .tokenize(sequence)
# output: ['s', '##ys', '##to', '##lic', 'array', '##s', 'are', 'cool', '.', 'this', '[UNK]', 'is', 'cool', 'too', '.']
tokenizer2.tokenize(sequence)
# output: ['sy','##sto','##lic','arrays','are','cool','.','this','[UNK]', 'is','cool','too','.']
</code></pre>
<p>Does anyone know why there is a difference? Aren't both tokenizers using the same vocabulary? which way is preferred?</p>
","nlp"
"84299","How is an ASR's output compared to ground truth for validation?","2020-10-20 22:16:58","84765","2","121","<nlp><similarity><speech-to-text>","<p>I am curious how it is done as I am interested in doing something similar. I have some manually transcribed data that contains tags for multiple speakers. I want to compare how well the out of the box ASRs (Google, AWS Transcribe) are able to diarise speakers (or in simple words identify and transcribe audio with multiple speakers). I want to compare it to the ground truth data I have and come up with a comparison metric.</p>
<p>I can use Levenshtein Distance or something like Ratcliff-Obershelp similarity as a metric. But I am trying to learn if there is a more standard way of doing this?</p>
","nlp"
"84228","Does the output of the Sequence-to-Sequence encoder model exist in the same semantic space as the inputs (Word2vec)?","2020-10-19 15:47:36","84248","2","35","<nlp><lstm><word-embeddings><word2vec><sequence-to-sequence>","<p>Does the output generated from the LSTM encoder module exist in the same semantic space as the original word vectors?
If so, say for example we have a sentence and we pass it through the encoder to get an encoded output and then we also calculate the average of word vectors for the same sentence separately, will the two new vectors (encoded and average) be comparable? Will their euclidean distance be relatively small?</p>
","nlp"
"84038","Using pretrained LSTM and Bert Models in CPU Only Environment - How to speed up Predictions?","2020-10-15 09:42:56","84055","0","1703","<python><tensorflow><nlp><lstm><bert>","<p>I have trained two text classification models using GPU on Azure. The models are the following</p>
<ol>
<li>Bert (ktrain)</li>
<li>Lstm Word2Vec (tensorflow)</li>
</ol>
<p>Exaples of the code can be found here: <a href=""https://github.com/lukasgarbas/nlp-text-emotion"" rel=""nofollow noreferrer"">nlp</a></p>
<p>I saved the models into files (.h5) for later use. The files are big e.g. 27,613kb for the lstm and 1.2 gb for bert.</p>
<p>I loaded the models and in a computer where only CPU is available. They both work fine but the <code>model.predict(text)</code> function is super slow predicting the class of the text e.g. on average 1 tweet sized message per second.</p>
<p>Adding GPU on the computer is not an option. I wonder if there is another way to make it run faster? e.g. train the models in a different way (without compromising accuracy) or save the model in a different file format?</p>
","nlp"
"83991","Summing three lexicon based approach methods for sentiment analysis?","2020-10-14 12:17:45","84024","2","107","<nlp><sentiment-analysis><nltk>","<p>I'm doing sentiment analysis using a lexicon based approach and I have a bunch of news headlines that needs to be categorized as negative, positive and neutral or within a scale ranging from -1 (very negative sentiment) to +1 (very positive sentiment).</p>
<p>I'm considering using three different methods: TextBlob, Vader (Valence Aware Dictionary and Sentiment Reasoner) and LIWC2015 as I have access to it. This would lead to three predicted sentiment of each headline.</p>
<p>I hove two question:</p>
<ul>
<li>Which are the pros and cons of this technique?</li>
<li>And, once runned the 3 analisys, how can I melt all them into a single metrics? Is standardize/normalize the 3 metrics and averaging them a good solution?</li>
</ul>
","nlp"
"83934","Smart sentence segmentation not splitting on abbreviations","2020-10-13 06:29:48","83944","3","545","<python><nlp><preprocessing><nltk><spacy>","<p>Sentencer from SpaCy and NLTK does not catch the fact that typical abbreviations (e.g. <code>Mio.</code> for <code>Million</code> in German) and the resulting sentence split is not correct. I understand that sentencers are supposed to be simple and quick but I am wondering if there is a better one that takes into account something more than uppercased words and punctuation? Alternatively, how to make SpaCy / NLTK / ... sentencer work for such sentences?</p>
<p>I am interested primarily with sentencers with Python API.</p>
","nlp"
"82923","Grouping profiles strings having the same words, but occurring out of order","2020-10-12 19:25:25","","1","161","<nlp><data-cleaning><text-classification>","<p>I have a dataframe containing a column of profile types, which looks like this:</p>
<pre><code>0                                    Android Java
1                  Software Development Developer
2                            Full-stack Developer
3                      JavaScript Frontend Design
4                          Android iOS JavaScript
5                             Ruby JavaScript PHP
</code></pre>
<p>I've used NLP to fuzzy match similar profiles, which returned the following similarity dataframe:</p>
<pre><code>left_side                       right_side                  similarity
7   JavaScript Frontend Design  Design JavaScript Frontend  0.849943
8   JavaScript Frontend Design  Frontend Design JavaScript  0.814599
9   JavaScript Frontend Design  JavaScript Frontend         0.808010
10  JavaScript Frontend Design  Frontend JavaScript Design  0.802881
12  Android iOS JavaScript      Android iOS Java            0.925126
15  Machine Learning Engineer   Machine Learning Developer  0.839165
21  Android Developer Developer Android Developer           0.872646
25  Design Marketing Testing    Design Marketing            0.817195
28  Quality Assurance           Quality Assurance Developer 0.948010
</code></pre>
<p>While this has helped, taking me from 478 unique profile to 461, what I'd want to focus on are profiles like this:</p>
<pre><code>Frontend Design JavaScript  Design Frontend JavaScript
</code></pre>
<p>The only tool I've seen which looks to address this problem is difflib?
My question is, what other techniques would be available so as to go through and standardize these profiles that consist of <strong>the same words, but out of order,</strong> to one standard string.
So desired output would be, taking a string containing &quot;Design&quot;, &quot;Frontend&quot; and &quot;JavaScript&quot; and replacing it with &quot;Design Frontend JavaScript&quot;.</p>
<p>Right now, I'm merging my original dataframe with the similarity dataframe to replace all occurrences of profile string on the right_side with the left_side, but that means I'm replacing the right_side below (&quot;Java Python Data Science&quot;) with the left_side below (&quot;JavaScript Python Data Science&quot;).</p>
<pre><code>53  JavaScript Python Data Science  Java Python Data Science
</code></pre>
<p>Any help would be greatly appreciated!!!</p>
","nlp"
"82908","If i use use BERT embeddings for if cosine(sent1,sent2) > 0.9, then is it fair to assume s1 and s2 are similar","2020-10-12 13:16:08","","2","996","<nlp><bert><cosine-distance><semantic-similarity>","<p>According to BERT author Jacob Devlin: I'm not sure what these vectors are, since BERT does not generate meaningful sentence vectors. It seems that this is doing average pooling over the word tokens to get a sentence vector, but we never suggested that this will generate meaningful sentence representations. And even if they are decent representations when fed into a DNN trained for a downstream task, it doesn't mean that they will be meaningful in terms of cosine distance. (Since cosine distance is a linear space where all dimensions are weighted equally).</p>
","nlp"
"82803","Difference between text-based image retrieval and natural language object retrieval","2020-10-10 00:43:57","","2","155","<machine-learning><nlp><object-detection><3d-object-detection>","<p>I am working on creating a model that locates an object in the scene (2D image or 3D scene) using a natural language query. I came across this paper on <a href=""https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Hu_Natural_Language_Object_CVPR_2016_paper.pdf"" rel=""nofollow noreferrer"">natural language object retrieval</a> that mentions that this task is different from text-based image retrieval in the sense that natural language object retrieval requires an understanding of objects in the image, spatial configurations, etc. I am not able to see the difference between these two tasks. Could you please explain it with an example?</p>
","nlp"
"82765","NLP: what are the advantages of using a subword tokenizer as opposed to the standard word tokenizer?","2020-10-09 08:37:15","82767","4","1922","<tensorflow><nlp><colab><tokenization>","<p>I'm looking at this Tensorflow colab tutorial about language translation with Transformers, <a href=""https://www.tensorflow.org/tutorials/text/transformer"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/text/transformer</a>, and they tokenize the words with a subword text tokenizer. I have never seen a subword tokenizer before and don't know why or when it should be used as opposed to a word tokenizer.</p>
<p>The tutorial says <code>The tokenizer encodes the string by breaking it into subwords if the word is not in its dictionary.</code></p>
<p>To get an idea of what the results can look like, the work <code>Transformer</code> gets broken down into index-subword pairs.</p>
<pre><code>7915 ----&gt; T
1248 ----&gt; ran
7946 ----&gt; s
7194 ----&gt; former
</code></pre>
<p>Does anybody know what the advantages of breaking down words into subwords is and when somebody should use a subword tokenizer instead of the more standard word tokenizer? Is the subword tokenizer used because the translation is from Portuguese to English?</p>
<p>*The version of Tensorflow is 2.3 and this subword tokenizer belongs to tfds.deprecated.text</p>
","nlp"
"82756","LSTM predicting same word repeatedly","2020-10-09 03:50:46","","2","318","<machine-learning><deep-learning><nlp><lstm>","<p>I am sure that this has happened to you if you have trained an LSTM model. The LSTM model predicts the same word 2 or 3 times. Now, I am not saying that for every input it has the same output. I am saying that for  every input,  the output that it produces has some repeated words in it. For example:</p>
<pre><code>    Human: What do you like?
    Bot: I quite enjoy enjoy programming!
</code></pre>
<p>Here the bot outputted the word enjoy 2 times. Not only in chatbots, I have encountered this issue in every LSTM Model I have trained so far.</p>
<p>What could be the reason of this? And how to stop this?</p>
<p>Thank you in advance!</p>
","nlp"
"82753","Text Analysis : Recommendation to identify cause of loss from claim narrative documents","2020-10-08 23:18:31","","1","44","<nlp><r><text-mining><lda><tfidf>","<p>I am trying to analyze auto claims narrative documents which contain description about the accident usually free text written by claims executives. Is there a nlp technique I could use to identify cause of loss like : drunk and driving , negligence , bad weather , etc ?</p>
<p>I have used TF-IDF technique to rank order words per claim but it does really help to identify the exact cause of loss easily even if I concentrate on top 20% words. I am aware of word embedding but really not sure if they could help .</p>
<p>Also tried LDA topic modeling in the past but that gives topics for entire corpus rather than claim level.</p>
<p>Below is the code I used to calculate tf-idf scores for the unigrams.</p>
<pre><code>library(dplyr)
library(tidytext)
library(janeaustenr)
library(tidyr) # for separate
library(qdapDictionaries)




#generate unigrams
loss_narrative_by_file_1 &lt;- loss_narrative_by_file %&gt;%
  unnest_tokens(unigram, whole_text_proxy, token = &quot;ngrams&quot;, n = 1)



# removing punctuation and numbers ( removing punct / digits faster on unigrams than whole text)
loss_narrative_by_file_1<span class=""math-container"">$unigram &lt;- gsub(""[[:punct:]]|[[:digit:]]|^http:\\/\\/.*|^https:\\/\\/.*"","""",loss_narrative_by_file_1$</span>unigram)


#retaining non blank unigrams
loss_narrative_by_file_1 &lt;- loss_narrative_by_file_1[unigram!=&quot;&quot;,]


# retain meaningful words only
loss_narrative_by_file_1 &lt;- loss_narrative_by_file_1[unigram %in% GradyAugmented]


# remving stopwords


unigrams_filtered &lt;- loss_narrative_by_file_1 %&gt;%
  filter(!unigram %in% stop_words$word)



#check coutns 

unigrams_filtered %&gt;% 
  count(unigram,sort=TRUE)

# after cleansing
unigram       n
&lt;chr&gt;     &lt;int&gt;
  1 claim     56247
2 loss      55068
3 policy    50394
4 insured   47512
5 date      36879
6 coverage  32167
7 plaintiff 31358
8 vehicle   26156
9 umbrella  25055
10 company   24479



# tf- idf ###############################################

# concat claim &amp; filename
unigrams_filtered<span class=""math-container"">$loss_file &lt;- paste(unigrams_filtered$</span>loss_Loss,unigrams_filtered$filename_Trimmed, sep = &quot;_&quot;)


unigram_tf_idf &lt;- unigrams_filtered %&gt;%
  count(loss_file, unigram) %&gt;%
  bind_tf_idf(unigram, loss_file, n) %&gt;%
  arrange(desc(tf_idf))

### validating results of tf-idf shows top 30% words with highest tf-idf score tend to contain the words associated with cause of loss. How can I narrow down further ?enter code here
</code></pre>
","nlp"
"82623","Can we combine multiple K-Means Models as a single model?","2020-10-06 12:44:24","82634","2","283","<nlp><clustering><k-means><word-embeddings><word2vec>","<p>I have a NLP problem statement where I use a Word2Vec embedding pre-trained model to convert key text to vectors and then on a set of terms run k-means clustering to get a final model for certain <code>k</code></p>
<p>For various sets of terms, I would develop a different model, which I would store to disk.</p>
<p>My question is, in case there is a new term, which I wish to classify as to which cluster should it point to from all the models can I follow the following approach?</p>
<ol>
<li>Load all models to memory and get their cluster centers.</li>
<li>get the vector of the new term based on the same pre-trained model as before.</li>
<li>get distance from each cluster center to the new vector and whichever is nearest can be considered as the winning cluster</li>
</ol>
<p>I would like to know what could be the possible drawbacks of such an approach.</p>
<p>My assumption is that since the vector space is same as defined by the pre-trained model, therefore the cluster centers would be in the same space.</p>
","nlp"
"82590","For an n-Gram model with n>2, do we need more context at end of each sentence?","2020-10-05 15:06:24","","1","64","<nlp><language-model><stanford-nlp><ngrams>","<p>Jurafsky's book says we need to add context to left and <strong>right</strong> of a sentence:</p>
<blockquote>
<p><a href=""https://i.sstatic.net/2tWoX.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/2tWoX.png"" alt=""&quot;Note that for these larger ngrams, we’ll need to assume extra context for the contexts to the left and right of the sentence end.&quot;"" /></a></p>
</blockquote>
<p>Does this mean,</p>
<p><strong>for example</strong>, if we've a corpus of three sentences: <code>&quot;John read Moby Dick&quot;</code>, <code>&quot;Mary read a different book&quot;</code>, and <code>&quot;She read a book by Cher&quot;</code>; and after training our tri-gram model on this corpus of three sentences, we need to evaluate the probability of a sentence &quot;John read a book&quot;, i.e. to find <span class=""math-container"">$P(John\; read\; a\; book)$</span> as below,</p>
<blockquote>
<p><span class=""math-container"">$P(John\; read\; a\; book)$</span> <br />
<span class=""math-container"">$=P(&lt;s&gt;&lt;s&gt;John\; read\; a\; book&lt;\backslash s&gt;&lt;\backslash s&gt;)$</span> <br />
<span class=""math-container"">$=P(John|&lt;s&gt;&lt;s&gt;) P(read|&lt;s&gt;John) \; P(a|John\; read) P(book|read\; a) P(&lt;\backslash s&gt;|a\; book)\; P(&lt;\backslash s&gt;|book&lt;\backslash s&gt;)$</span> <br />
<span class=""math-container"">$=\frac{1}{3}\frac{1}{1}\frac{1}{1}\frac{1}{2}\frac{0}{1}\frac{1}{1}$</span>       (without smoothing)</p>
</blockquote>
<p>It would be great, if you let me know if the above understanding is correct?</p>
","nlp"
"82577","In smoothing of n-gram model in NLP, why don't we consider start and end of sentence tokens?","2020-10-05 10:47:45","","4","499","<nlp><language-model><stanford-nlp><ngrams>","<p>When learning Add-1 smoothing, I found that somehow we are adding 1 to each word in our vocabulary, but not considering start-of-sentence and end-of-sentence as two words in the vocabulary. Let me give an example to explain.</p>
<p><strong>Example:</strong></p>
<p>Assume we have a corpus of three sentences:</p>
<p>&quot;<code>John read Moby Dick</code>&quot;, &quot;<code>Mary read a different book</code>&quot;, and &quot;<code>She read a book by Cher</code>&quot;.</p>
<p>After training our bi-gram model on this corpus of three sentences, we need to evaluate the probability of a sentence &quot;John read a book&quot;, i.e. to find <span class=""math-container"">$P(John\; read\; a\; book)$</span></p>
<p>To differentiate <em>John</em> appearing anywhere in a sentence from its appearance at the beginning, and likewise for <em>book</em> appearing at the end, we rather try to find <span class=""math-container"">$P(&lt;s&gt;John\; read\; a\; book&lt;\backslash s&gt;)$</span> after introducing two more words <span class=""math-container"">$&lt;s&gt;$</span> and <span class=""math-container"">$&lt;\backslash s&gt;$</span>,  indicating start of a sentence, and end of a sentence respectively.</p>
<p>Finally, we arrive at the</p>
<blockquote>
<p><span class=""math-container"">$P(&lt;s&gt;John\; read\; a\; book&lt;\backslash s&gt;)$</span> as
<span class=""math-container"">$P(John|&lt;s&gt;)P(read|John)P(a|read)P(book|a)P(&lt;\backslash s&gt;|book)=\frac{1}{3}\frac{1}{1}\frac{2}{3}\frac{1}{2}\frac{1}{2}$</span></p>
</blockquote>
<p><strong>My Question:</strong>
Now, to find <span class=""math-container"">$P(Cher\; read\; a\; book)$</span>, using Add-1 smoothing (Laplace smoothing) shouldn't we add the word 'Cher' that appears first in a sentence? And to that, we must add <span class=""math-container"">$&lt;s&gt;$</span>  and <span class=""math-container"">$&lt;\backslash s&gt;$</span> in our vocabulary. With this, our calculation becomes:</p>
<blockquote>
<p><span class=""math-container"">$P(Cher|&lt;s&gt;)P(read|Cher)P(a|read)P(book|a)P(&lt;\backslash s&gt;|book)=\frac{0+1}{3+13}\frac{0+1}{1+13}\frac{2+1}{3+13}\frac{1+1}{2+13}\frac{1+1}{2+13}$</span></p>
</blockquote>
<p>The 13 added to each numerator is due to the unique word count of the vocabulary which has 11 English words from our 3-sentence corpus plus 2 tokens - start and end of a sentence. In few places, I see 11 is added instead of 13 to the numerator. Wondering what I am missing here.</p>
","nlp"
"82555","Regression of citations by article title","2020-10-04 19:33:19","","0","23","<nlp><regression><linear-regression>","<p>I have a dataset of some lines (article titles) and the number of citations for an article with this title, what is the best way to build a regression model? I have an idea to launch TF-IDF and use a linear model, but I don't yet understand how true this is. There are about 5000 titles, all of them from the same topic.</p>
<p>Maybe there is some common name for such a task?</p>
","nlp"
"82540","How can I make a better unsupervised text classifier model? Is POS tagging part of Machine Learning and Data science?","2020-10-04 13:31:25","","0","94","<machine-learning><nlp><text-mining><data-science-model><text-classification>","<p>I have got complaints data, which is not good, and often contains less than 3 words in every complaint (sometimes so short that only one word of them makes sense). The objective is to find what's trending in these complaints i.e. what people are complaining about.</p>
<p>I tried LDA/NMF and other topic modeling techniques but it's not performing well and mixing up different complaints in one. Also, there is no way to monitor these techniques in Production and monitor the performance (what if new types of complaints start coming in the future and the classes start mixing up? I can not fix the number of classes in LDA in production.). To add, LDA/NMF models are computationally expensive.</p>
<p>I built a system to identify Verbs and Nouns as keywords (using texthero - off course after extensive cleaning and lemmatization) and make classes of every complaint being received. Like 100 complaints being received are &quot;delivery&quot; related, another 50 are &quot;quality&quot; related, etc. So far, this system seems to have performed the best, and without any misclassification (obviously).
Now I have two questions:</p>
<ol>
<li>Is POS tagging part of Data Science and Machine learning? I am in a Data science role and how can I justify this work?</li>
<li>Is there anything else that I can try given such a dataset?</li>
</ol>
","nlp"
"82478","Evaluating Language Model on specific topic","2020-10-02 16:56:25","82511","2","86","<machine-learning><nlp><language-model><gpt>","<p>I have finetuned a pretrained Language Model(GPT-2) on a custom dataset of mine. I would like a way of evaluating the ability of my model to generate sentences of a specific predefined topic, given in the form of either a single keyword(e.g. 'Computers') or a bag-of-words(e.g. 'Computers', 'Linux', 'Server'...).</p>
<p>For example given a LM, how relative are the outputs of the model to the topic specified by the word <em>Computers</em>?</p>
<p><strong>What I have already tried:</strong> Generating a large enough number of sentences from the LM and taking the average cosine similarity between these sentences and the target topic(or every word in that topic we have more than one) as described <a href=""https://towardsdatascience.com/cutting-edge-semantic-search-and-sentence-similarity-53380328c655"" rel=""nofollow noreferrer"">here</a> . I am not sure if this is a valid way to go and furthermore the cosine similarity between sentences yields poor results in many cases.</p>
<p>Thanks in advance for any help.</p>
","nlp"
"82328","NLP Emotion Detection - Model fails to learn to recognize negations","2020-09-28 11:12:45","82344","2","70","<machine-learning><classification><nlp>","<p>I am working on a nlp emotion detection project. The emotions that I try to predict are 'joy', 'fear', 'anger', 'sadness'. I use some publicly available labeled datasets to train my model e.g. ISEAR, WASSA etc. I have tried the following approaches:</p>
<ol>
<li><strong>Traditional ML</strong> approached using bigrams and trigrams.</li>
<li><strong>CNN</strong> with the following architecture: (X) Text -&gt; Embedding (W2V pretrained on wikipedia articles) -&gt; Deep Network (CNN 1D) -&gt; Fully connected (Dense) -&gt; Output Layer (Softmax) -&gt; Emotion class (Y)</li>
<li><strong>LSTM</strong> with the following architecture: (X) Text -&gt; Embedding (W2V pretrained on wikipedia articles) -&gt; Deep Network (LSTM/GRU) -&gt; Fully connected (Dense) -&gt; Output Layer (Softmax) -&gt; Emotion class (Y)</li>
</ol>
<p>The NN models achieve more than 80% accuracy but still when I use the trained model to predict the emotion on text that includes some negation I get the wrong results. For example:</p>
<p>Text : &quot;I am happy with easy jet, it is a great company!&quot;</p>
<p>Predicts <strong>Happy</strong></p>
<p>Text: I am not happy with easyjet #unhappy_customer</p>
<p>Predicts <strong>Happy</strong></p>
<p>Any suggestions on how to overcome this problem?</p>
","nlp"
"82322","Is each form of word classification also considered to be '(named) entity recognition'?","2020-09-28 10:29:43","82327","1","29","<classification><nlp><named-entity-recognition>","<p>In an article that I am writing, I focus on word classification. A typical task that involves word classification is (named) entity recognition. Entity recognition is a rather broad task and seems to cover other sub-tasks as well.</p>
<p>Therefore, it seems fair to me to use the terms interchangeably.</p>
<p>Is this a fair assumption?</p>
","nlp"
"82313","Sentiment Analysis on long and structured texts","2020-09-28 08:42:20","82315","0","1067","<machine-learning><nlp><sentiment-analysis>","<p>I'm trying to learn how sentiment analysis based on machine learning techniques works by reading guides online and papers from the academic world and I'm struggling to understand the following:</p>
<blockquote>
<p>Why don't people run - or, at least, hardly ever - sentiment analysis
on the long and structured text like newspaper articles or speeches
transcripts?</p>
</blockquote>
<p>I noticed it's pretty common analyzing reviews and newspapers headlines as they are short in terms of characters. So... I was wondering if it is just because of the computational power and time required to train ML algorithms (thinks about neural networks) or because of other reasons.</p>
<p>Can someone help me to understand?</p>
","nlp"
"82245","Morphological Analyzer for Hindi and English in Python","2020-09-26 06:04:30","82338","0","862","<nlp>","<p>I am unable to find morphological analyzers for English or Hindi. I am looking for a tool that can split the word into morphemeses e.g.</p>
<p>Independently = In + dependent + ly</p>
","nlp"
"82180","Generative chatbots with BERT pretrained vectors","2020-09-24 17:07:18","82210","-3","181","<nlp><bert><embeddings>","<p>Most places seem to train generative chatbots with one hot encoded vectors. See <a href=""https://towardsdatascience.com/generative-chatbots-using-the-seq2seq-model-d411c8738ab5"" rel=""nofollow noreferrer"">here</a> for example, and even the official tutorial on <a href=""https://pytorch.org/tutorials/beginner/chatbot_tutorial.html"" rel=""nofollow noreferrer"">pytorch</a>.<br />
But using one hot encoded vectors are undoubtedly the worst performing method. No tutorial seems to provide this using BERT vectors.</p>
<ul>
<li>Why hasn't chatbots been built with BERT vectors?</li>
<li>Are BERT vectors are not meant to be used this way?</li>
</ul>
","nlp"
"82134","Initializing weights that are a pointwise product of multiple variables","2020-09-23 13:18:40","","1","76","<nlp><word-embeddings><word2vec><weight-initialization><fasttext>","<p>In two-layer perceptrons that slide across words of text, such as <a href=""https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf"" rel=""nofollow noreferrer"">word2vec</a> and <a href=""https://arxiv.org/pdf/1607.04606.pdf"" rel=""nofollow noreferrer"">fastText</a>, hidden layer heights may be a product of two random variables such as positional embeddings and word embeddings <a href=""https://arxiv.org/pdf/1712.09405.pdf#page=2"" rel=""nofollow noreferrer"">(Mikolov et al. 2017, Section 2.2)</a>: <span class=""math-container"">$$v_c = \sum_{p\in P} d_p \odot u_{t+p}$$</span> However, it's unclear to me how to best initialize the two variables.</p>
<p>When only word embeddings are used for the hidden layer weights, word2vec and fastText initialize them to <span class=""math-container"">$\mathcal{U}(-1 / \text{fan_out}; 1 / \text{fan_out})$</span>. When the product of two random variables is used, we might:</p>
<ul>
<li><p>initialize the first variable with ones and the other variable with <span class=""math-container"">$\mathcal{U}(-1 / \text{fan_out}; 1 / \text{fan_out})$</span>: This would maintain the distribution of the weights, but the gradients to the second variable would be way too large.</p>
</li>
<li><p>initialize the variables with <a href=""http://ravshansk.com/articles/uniform-distribution.html"" rel=""nofollow noreferrer"">a 2-factor of <span class=""math-container"">$\mathcal{U}(0, 1)$</span></a>:</p>
<p><a href=""https://ravshansk.com/articles/uniform-distribution.html"" rel=""nofollow noreferrer""><img src=""https://ravshansk.com/assets/img/factoranalysis/beta1205.png"" alt=""2-factor of the uniform random distribution"" /></a></p>
<p>and then rescale their product to <span class=""math-container"">$[-1 / \text{fan_out}; 1 / \text{fan_out}]$</span>. This would maintain the distribution of the weights, but enlarge the gradients to both variables, since they are now both initialized to ones or close to ones.</p>
</li>
</ul>
<p>I will appreciate any ideas and pointers to existing research in this direction.</p>
","nlp"
"82115","Sentence Segmentation for Hindi","2020-09-23 08:16:01","","1","134","<nlp>","<p>I only found stanza that has sentence segmentation for hindi. All other available tools do word tokenization but not sentence segmentation. Could someone suggest any other available tools for sentence segmentation for Hindi</p>
","nlp"
"81824","Distinguish randomly generated texts from reasonable for human texts","2020-09-16 17:59:26","81832","0","53","<classification><nlp><text><text-classification><text-generation>","<p>I have strings short texts of 2 types:
<code>'23jd2032n0d2mn', 'fn830n30rn83', 'fhui29n4ok', 'qn4foml', ...</code>
and
<code>'sweetie23', 'king3prussia', 'maryjesus', 'lovedog4and_kitties', ...</code></p>
<p>Is there a way to distinguish one type from another?</p>
<p>I've tried to vectorize texts with word2vec and classify on these vectors with xgboost, but I didn't succeded to achieve got F1-score.</p>
","nlp"
"81792","What is the typical accuracy of masked language models during BERT pretraining?","2020-09-16 09:12:52","","2","1127","<nlp><bert>","<p>I was reading the BERT paper but I didn't find any tables concerning the performance of the masked language models during pretraining. Does anyone know the accuracy of BERT's masked language model?</p>
","nlp"
"81785","Learning word embeddings by first learning character embeddings","2020-09-16 07:37:25","","1","42","<deep-learning><nlp><word-embeddings>","<p>I was going through various papers for NLU applications(Natural Language Understanding).</p>
<p>There I have observed a common pattern that for a word embeddings, following 3 combinations are used (may be using concatenation or any other technique).</p>
<blockquote>
<ol>
<li>static word embedding (glove, word2vec)</li>
<li>word embeddings learnt from character embeddings</li>
<li>contextual word embedding</li>
</ol>
</blockquote>
<p>Currently I am implementing one of those papers and <strong>I want to learn a word embedding from character embeddings</strong> (I don't want to use pre-trained char-embeddings like FastText)</p>
<p><strong>learn character embeddings ---&gt; learn word embeddings from char embedding --&gt; train my custom task</strong></p>
<p>So I am thinking what is the best way to do this?</p>
<p>Let's say my data looks like this:</p>
<blockquote>
<p>row-1: What a sunny day. Beautiful! I feel happy.</p>
</blockquote>
<p>Our task is, we want find embeddings for each word which derived from char embedding.</p>
<p><strong>approach-1]  use char level CNN (1ConvD)</strong></p>
<ul>
<li><p>Here features are nothing but unique characters in our data.</p>
</li>
<li><p>I am assuming result of this step will be, I will have an embedding for each
character.</p>
</li>
<li><p>Then I will go word-by-word, e.g. 'sunny' --&gt; 's'+'u'+'n'+'n'+'y' that is take
a sum/max/avg of char embeddings to get a word embeddings.</p>
<p>However this last step doesn't sound correct wrt backtracking operation. It feels disconnected from model chain.
What am I missing here?</p>
</li>
</ul>
<p><strong>approach-2] Use Bi-Dir LSTM</strong>
Something like this as described in this <a href=""https://guillaumegenthial.github.io/sequence-tagging-with-tensorflow.html"" rel=""nofollow noreferrer"">Blog</a>:</p>
<p><a href=""https://i.sstatic.net/2jLZr.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/2jLZr.png"" alt=""enter image description here"" /></a></p>
<p>But few things are not clear from this:</p>
<ol>
<li><p><strong>&quot;What a sunny day&quot;</strong>
Here how would I know <strong>word boundary?</strong> In above picture there was just 1 word so concatenation was easy.</p>
</li>
<li><p>Since we are using hidden state as char embedding that means for same letter (e.g. 'C'), embedding will be different in 2 different words. Is my understanding correct?</p>
</li>
</ol>
","nlp"
"81740","Ordering of standardization, pca, and/or tfidf for neural network","2020-09-15 14:49:35","81763","0","559","<neural-network><nlp><pca><text-classification><tfidf>","<p>I have 60k rows of text data. I have tokenized it into 55k columns. I am using a neural network to classify the data but have some questions about how to order my preprocessing steps. I have too much data for my hardware (doesn't fit in memory/too slow) so I am using PCA to reduce dimensions.</p>
<ol>
<li><p>Obviously, I need to scale before PCA. I am currently standardizing the columns, but I am wondering if I can use tfidf instead of standardization. Some rows have 50k+ tokens while others have &lt;1k tokens so I am worried these rows have undue influence on the outcome of scaling which will trickle down the pipeline. Is this a good/bad idea? Would I maybe use tfidf then standardize before PCA?</p>
</li>
<li><p>Generally neural nets prefer standardized data. After PCA the first few columns have much greater magnitude than the rest b/c they capture so much variance. Should I standardize after PCA and before training? The reason for standardizing before training is so no feature has bigger influence on the model just b/c the scale is bigger, but isn't PCA telling me that the first few features are actually more important? FWIW, I've tried both and not scaling seems a little better.</p>
</li>
<li><p>What about performing tfidf after PCA and before training? Again, rows with 50k+ tokens will prefer a network with orders of magnitude larger weights than rows with &lt;1k tokens. Wouldn't it be hard for the network to set weights for both types of rows?</p>
</li>
</ol>
<p>Diagram for clarity: data -&gt; tokenize -&gt; ?standardize/tfidf? -&gt; PCA -&gt; ?standardize/tfidf? -&gt; neural net</p>
","nlp"
"81729","How to process list type questions in Question Answering task","2020-09-15 10:33:39","","1","30","<deep-learning><nlp><question-answering>","<p>How to generate question-answer-context triplets for questions with multiple answer strings? How to measure performance for it?</p>
<p>For a question with one single answer, we generate one question-answer-context triplet, and calculate EM/F1 score. Then take average scores of the whole training set as the overall performance.</p>
<p>For a list type question, is it correct to generate multiple triplets for each candidate answer string as separate records in the training set? Even they would share the same context and question. When calculating performance, should we combine answers from separate triplets (which share the same question and context) first, then compare them with the 'true' answer list of the question. Or just take average scores as other records with different questions in the training set, like how we calculate performance for questions with single answer?</p>
","nlp"
"81727","What would be the target input for Transformer Decoder during test phase?","2020-09-15 10:23:15","82206","1","3831","<nlp><transformer><attention-mechanism>","<p>The Transformer Decoder takes in two inputs, the encoder's output, and the target sequence. How the target is fed into the decoder has been provided in this <a href=""https://datascience.stackexchange.com/questions/51785/what-is-the-first-input-to-the-decoder-in-a-transformer-model"" >answer </a></p>
<p>I am having confusion about what the target sequence will be when the trained model is evaluated?.</p>
<p>Is it that we start with a <code>&lt;SOS&gt;</code> tag for the first timestep and loop through the transformer decoder for each timestep like in RNN's?</p>
<p>It would be helpful if someone can clarify this for me.</p>
","nlp"
"81678","How to approach for predicting semantic similarity between two phrases","2020-09-14 12:42:25","81691","0","130","<nlp><learning><semantic-similarity>","<p>I need pointers on the latest research, tools, and techniques for predicting semantic similarity between two phrases.</p>
<p><strong>Problem Statement</strong>: Given two propositions <code>A</code> and <code>B</code> with <code>A</code> know to be <code>true</code>, predict if <code>B</code> agrees, contradicts, or is neutral.</p>
<p><strong>Examples</strong></p>
<p><code>A</code> = <code>X is greater than Y</code> | <code>B</code> = <code>Y is greater than X</code> | <code>Result</code> = <code>contradict</code></p>
<p><code>A</code> = <code>X is greater than Y</code> | <code>B</code> = <code>Y is less than X</code> | <code>Result</code> = <code>agree</code></p>
<p><code>A</code> = <code>X is greater than Y</code> | <code>B</code> = <code>P is less than X</code> | <code>Result</code> = <code>neutral</code></p>
<p><code>A</code> = <code>X increases with Y</code> | <code>B</code> = <code>X is directly proportional to Y</code> | <code>Result</code> = <code>agree</code></p>
<p><code>A</code> = <code>X is joining Y and Z</code> | <code>B</code> = <code>Y and Z are connected with X</code> | <code>Result</code> = <code>agree</code></p>
<p><strong>What I have tried</strong></p>
<p>I am looking into Google's Universal Sentence Encoder and alternatives for evaluating semantic similarities with mixed results. Most of the solutions available will give a result of <code>exactly similar</code>
for the first example. I am also going through some research papers on grammar-based similarity techniques in natural language.</p>
<p>Any help or direction towards research papers, tools, or libraries is greatly appreciated.</p>
","nlp"
"81661","What's the best way to detect bible verse mentions in a text?","2020-09-14 07:55:41","","2","172","<machine-learning><deep-learning><word-embeddings><nlp>","<p>I have a set of 10 verses from the Bible in English. I want to detect the occurrence of any of these verses in a text. What would be the best way to go about doing this?</p>
<p>Note that verses of the Bible are worded differently from one translation to another. For example, the verse &quot;<em>Cast all your anxiety on him because he cares for you</em>&quot; exists in other translations as:</p>
<p><strong>KJV</strong>
Casting all your care upon him; for he careth for you.</p>
<p><strong>ESV</strong>
casting all your anxieties on him, because he cares for you.</p>
<p><strong>NLT</strong>
Give all your worries and cares to God, for he cares about you.</p>
<p>Also, people may make typos or make mistakes when writing a verse.</p>
<p>Would you say Semantic Search is the best approach for this? If so, do I use a pertained word embeddings model, retrain it on my set of 10 verses, and then search my text for the occurrence of these verses?</p>
<p>How would the search mechanism work? Do I get the word embedding sentence by sentence and compare each sentence's embedding one by one with my Bible corpus of 10 verses' embeddings?</p>
","nlp"
"81657","how can i detect medicine name and info(use and contents) by using medicine wrappers","2020-09-14 06:13:50","","0","106","<machine-learning><nlp><ocr>","<p>I got one project idea creating a Cross-platform react-native app</p>
<p>the project title is creating an app that can detect medicine name and other info from the medicine wrapper</p>
<p>I'm thinking of using Google Cloud Vision to extract all the text from the medicine wrapper.</p>
<p>The text contains medicine mfg info, address, pin code, expiry date, ingredients, and most importantly medicine name.</p>
<p>I want to extract only the medicine names and other info out of it.</p>
<p>What are the possible ways to get medicine name and other info?</p>
<p>and for rural people in India they can't spell the word correctly for that I can give one button that can spell that name for folks can you suggest me any ideas how can I do this and one important part
<strong>how/where can I get medicine wrapper ( Packaging ) of different medicines data????</strong></p>
","nlp"
"81595","Does BERT has any advantage over GPT3?","2020-09-12 04:37:50","","11","11917","<nlp><bert><gpt>","<p>I have read a couple of documents that explain in detail about the greater edge that GPT-3(Generative Pre-trained Transformer-3) has over BERT(Bidirectional Encoder Representation from Transformers). So am curious to know whether BERT scores better than GPT-3 in any particular area of NLP?</p>
<p>It's quite interesting to note that OpenAI's GPT-3 is not open-sourced whereas tech behemoth Google's BERT is open-sourced. I felt OpenAI's stance and the hefty price tag for GPT-3 api is in stark contrast to its mission statement(OpenAI’s mission is to ensure that artificial general intelligence (AGI)—by which we mean highly autonomous systems that outperform humans at most economically valuable work—benefits all of humanity).</p>
<p><a href=""https://analyticsindiamag.com/gpt-3-vs-bert-for-nlp-tasks/"" rel=""noreferrer"">https://analyticsindiamag.com/gpt-3-vs-bert-for-nlp-tasks/</a>
<a href=""https://thenextweb.com/neural/2020/07/23/openais-new-gpt-3-language-explained-in-under-3-minutes-syndication/"" rel=""noreferrer"">https://thenextweb.com/neural/2020/07/23/openais-new-gpt-3-language-explained-in-under-3-minutes-syndication/</a>
<a href=""https://medium.com/towards-artificial-intelligence/gpt-3-from-openai-is-here-and-its-a-monster-f0ab164ea2f8"" rel=""noreferrer"">https://medium.com/towards-artificial-intelligence/gpt-3-from-openai-is-here-and-its-a-monster-f0ab164ea2f8</a></p>
","nlp"
"81578","Marginal contribution of a text document","2020-09-11 18:44:30","","1","29","<machine-learning><nlp><feature-selection>","<p>I'm trying to build a Shapley value (marginal contribution) of a text document in terms of information content, given that there are several documents on a given topic.</p>
<p>For example, we have 3 reports describing the ocean:</p>
<p>A: {The ocean is blue.}</p>
<p>B: {The ocean is blue <strong>and salty</strong>.}</p>
<p>C: {The ocean is salty and <strong>a home for fish</strong>}</p>
<p>If I'm correct, a first step in the algorithm is identifying a set of features (individual pieces of information): {<em>blue, salty, a home for fish</em>}</p>
<p>Moreover, say each feature has an equal weight.</p>
<p>Then I can compute Shapley values of 1/6 for A, 1/3 for B, 1/2 for C.</p>
<p>I wonder if there are well-established algorithms / papers that deal with this problem. I'm coming from economics myself (I'm familiar with Shapley values abstractly), so apologies if the question is too trivial.</p>
<p>Thank you!</p>
","nlp"
"81508","Question about BERT embeddings with high cosine similarity","2020-09-10 15:13:03","85325","2","341","<nlp><bert><transformer><cosine-distance>","<p>Under what circumstances would BERT assign two occurrences of the same word similar embeddings? If those occurrences are contained within similar syntactic relations with their co-occurrents?</p>
","nlp"
"81418","Semantic similarity between two or more sentences","2020-09-09 00:16:56","81422","0","1599","<python><nlp><word2vec><cosine-distance><semantic-similarity>","<p>I need to determine how similar sentences (in meaning) are to one another.
In order to do it, I have been considering an algorithm (cosine similarity) to determine the similarity between sentences. I have thought as appropriate Word2vec or wordnet to build features for similarity.</p>
<p>If you have used this (or similar) approach, could you please provide me an example of use of word2vec/wordnet for similarity analysis?</p>
","nlp"
"81381","How to determine semantic differences in NLP","2020-09-08 12:01:16","81421","0","85","<python><nlp><word2vec>","<p>I would need to determine the difference in meaning between the following two sentences:</p>
<pre><code>I am at home
I am not at home
I am at the office
</code></pre>
<p>the first two sentences differs in verb, which changes the meaning of the sentences (to negative); the second one, with the first one, differs because of the place.
I have thought of word2vec, but I am not completely sure if this is the best tool to analyse sentences like the above ones. Also cosine_similarity could be a solution, but I would have not information about the meaning. I think it is more about semantic meaning...</p>
","nlp"
"81365","How to extract contents by topic from a document?","2020-09-08 06:35:27","","0","215","<machine-learning><deep-learning><nlp><information-extraction><semantic-segmentation>","<p>I am trying to extract information from resumes. I tried the pdfminer for the text extraction. But I need to extract the contents from a resume with respect to its title.</p>
<p>For example:
I will be giving my educational details under a title <strong>EDUCATIONAL BACKGROUND</strong>, so I have to extract the content topic wise.</p>
<p>Is it possible to extract like that?</p>
<p>What will be the process behind that?</p>
<p>Is it possible to approach the problem in a segmentation manner.</p>
","nlp"
"81306","Data Annotation: ""labeling"" target vs features","2020-09-06 19:40:19","","1","60","<nlp>","<p>I understand how one would use a data annotation tool to label targets for a given sentence, for example though, I'm not clear on how placing labels on features can be used to improve model performance.  For example, in <a href=""https://literature-review.doccano.coronawhy.org/demo/named-entity-recognition/"" rel=""nofollow noreferrer"">this text annotation tool</a> , you can add &quot;labels&quot; to a body of text like <em>person, location, event ...etc</em> .  Given that you must create Word Embeddings to work with the data, and the vector representation is not human-readable, how would you be able to improve model performance by annotating feature variables?</p>
","nlp"
"81278","GPT-3 API Documentation?","2020-09-05 14:56:52","","2","349","<nlp><gpt>","<p>Has documentation of the GPT-3 API been made public?</p>
<p>I would be interested in keeping myself up to speed on the API's capability.</p>
","nlp"
"81263","Sentiment analysis of the target in articles","2020-09-05 09:07:51","81298","2","120","<python><nlp><sentiment-analysis>","<p>The goal is as follows: I have a big article and I want to define the sentiment of the particular word. For example, the article describes pros and cons of bikes and cars and I want to find the sentiment of the word car.</p>
<p>In such an example I cannot use document-level SA as the article itself can be positive while the car was mentioned in a negative way.</p>
<p>So, I studied papers related to aspect-based sentiment analysis, but my constait is absence of data for training NNs. Hence, I concentrated on the approaches that basically do not involve training process. One of my attemts was to build sentiment analysis tool using word2vec and K-Means so that each cluster corresponds to one of three sentiments (pos, neg and neu).
It actually worked great but I found that for some reason one word can be at two clusters at the same time. Plus it generally goes not give sentiment for specific keyword but for all aspects found in the text.<br />
Another problem is that basically cannot test the correctness of the output if only not to read the text by myself and check whether the keyword belonged to the correct cluster or not.<br />
So I came to the decision to make summarization of the article first and then applying sentiment analysis (like sentiwordnet or similar).</p>
<p><strong>Question 1</strong><br />
Are there ways to improve word2vec+KMeans approach? Is it even worse improving?<br />
<strong>Question 2</strong><br />
Is it a good idea to go through text summarization before sentiment analysis?<br />
<strong>Question 2</strong><br />
Is there better way to find sentiment of the particular word without training process (due to no training data and small amount of unlabeled data)?</p>
","nlp"
"81249","Construct word2vec (CBOW) training data from beginning of sentence","2020-09-04 20:58:01","81341","1","231","<neural-network><nlp><text-mining><word-embeddings><word2vec>","<p>When constructing training data for CBOW, <a href=""https://arxiv.org/abs/1301.3781"" rel=""nofollow noreferrer"">Mikolov et al.</a> suggest using the word from the center of a context window. What is the &quot;best&quot; approach to capturing words at the beginning/end of a sentence (I put best in quotes because I'm sure this depends on the task). Implementations I see online do something like the this:</p>
<pre><code>for i in range(2, len(raw_text) - 2):
    context = [raw_text[i - 2], raw_text[i - 1],
               raw_text[i + 1], raw_text[i + 2]]
</code></pre>
<p>I see two issues arising from this approach.</p>
<ul>
<li><strong>Issue 1:</strong> The approach gives imbalanced focus to the middle of the sentence. For example, the first word of the sentence can only appear in 1 context window and will never appear as the target word. Compare this to the 4th word in the sentence which will appear in 4 context windows and will also be a target word. This will be an issue as some words appear frequently at the beginning of sentences (i.e. however, thus, etc.). Wouldn't this approach minimize their use?</li>
<li><strong>Issue 2:</strong> Sentences with 4 or fewer words are completely ignored, and the importance of short sentences is minimized. For example, a sentence with 5 words can only contribute one training sample while a sentence of length 8 will contribute 4 training samples.</li>
</ul>
<p>Can anyone offer insight as to how much these issues affect the results or any alternative approaches for constructing the training data? (I considered letting the first word be the target word and using the next N words as the context, but this creates issues of it's own).</p>
<p>Note: I also asked this question on Stack Overflow: <a href=""https://stackoverflow.com/questions/63747999/construct-word2vec-cbow-training-data-from-beginning-of-sentence"">https://stackoverflow.com/questions/63747999/construct-word2vec-cbow-training-data-from-beginning-of-sentence</a></p>
","nlp"
"81248","Does finetuning BERT involving updating all of the parameters or just the final classification layer?","2020-09-04 20:54:25","","2","2013","<nlp><bert><transformer><finetuning><pretraining>","<p>Currently learning and reading about transformer models, I get that during the pretraining stage the BERT model is trained on a large corpus via MLM and NSP.  But during finetuning, for example trying to classify sentiment based on another text, are all of the BERT parameters (110M+ parameters + final classification layer) updated or just only final classification layers?  Couldn't find a concrete answer to this in the resources I've been looking at.</p>
<p>Thank you in advance.</p>
","nlp"
"81123","How can I get a value of context vector in GPT?","2020-09-02 13:35:20","","1","257","<neural-network><deep-learning><nlp><text><matrix>","<p>I'm a newbie in NLP and I'm now stuck in GPT.
The question I'm struggling with is related to a term 'context vector'<a href=""https://i.sstatic.net/Fw0mH.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Fw0mH.png"" alt=""enter image description here"" /></a></p>
<p>It says in the following (sorry that the material provided is written in korean)
that U represents a context vector.
I searched for both terms 'context window' and 'context vector'
Now I understand that a context window means the number of words I will accept as a context of the token. I also searched for 'context vector' and what I found out is that it is calculated by multiplying value parameters of each token to its softmax (attention score)
(I hope I am right, feel free to tell me what I'm getting wrong)
My question is that if 'context vector' can be calculated by using attention mechanism, then how can it(U) be calculated in advance in GPT since a transformer block including a masked attention layer gets 'UW(e) + W(p)' as its input ?</p>
<p>I know this question might seems really non-essential and out of topics to many great experts here, but I guess it's just making me get stuck in a logical step to understand NLP.
<a href=""https://i.sstatic.net/l7ADP.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/l7ADP.png"" alt=""enter image description here"" /></a></p>
","nlp"
"81089","Getting sentence embeddings with sentence_transformers","2020-09-01 15:23:32","81138","0","3450","<machine-learning><python><deep-learning><nlp><bert>","<p>I have a text column in my data frame which contains paragraph(s) having multiple and variable sentences in each instance/example/row of the dataframe. Then, I created the sentence tokens of that paragraph using <code>sent_tokenizer</code> of nltk and put it into another column.</p>
<p>So my data frame looks like this:</p>
<pre><code>index       text                                              class

0           [&quot;Hello i live in berlin&quot;, 'I'm xxx']                                                          1
1           [&quot;My name is xx&quot;, &quot;I have a cat&quot;, &quot;Love is life&quot;]                                              0
</code></pre>
<p>now when I'm using:</p>
<pre><code>from sentence_transformers import SentenceTransformer
model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')
sentences = df['text']
sentences = sentences.tolist()
embeddings = model.encode(sentences)
</code></pre>
<p>I'm getting:</p>
<pre><code>TypeError: expected string or bytes-like object
</code></pre>
<p>The <strong>encode</strong>  method is not taking a list of list of sentences as an argument.</p>
","nlp"
"81085","How to expand abbrevations in text during preprocessing?","2020-09-01 14:41:02","81099","0","274","<machine-learning><nlp><data-cleaning><preprocessing>","<p>Im doing preprocessing on english text data. I have some domain specific abbreviations, for which i'm maintaining internal dictionary with key-value pairs. The problem i'm facing is the text has abbreviations in plural forms with and without contractions like:</p>
<ul>
<li>Mgr's = manager</li>
<li>mgrs = manager</li>
<li>mgr = manager</li>
</ul>
<p>All 3 points to a manager. Im able to capture the plural form with contractions using a regex(r&quot;'s&quot;) and removing the 's' but, in case of no contractions i'm creating one more entry in the dictionary with plural form of the abbreviations.</p>
<p>Im somehow feel this is duplication and not a clean approach. Is there any better solution to address this problem?</p>
<p>Any immediate help on this is much appreciated. Thank you</p>
","nlp"
"81076","Is it good practice to remove the numeric values from the text data during preprocessing?","2020-09-01 13:36:48","","0","2257","<nlp><tfidf><tokenization><hashingvectorizer><bag-of-words>","<p>Im doing preprocessing on a text dataset. I have certain numerics in it like:</p>
<ul>
<li>date(1st July)</li>
<li>year(2019)</li>
<li>tentative values (3-5 years/ 10+ advantages).</li>
<li>unique values (room no 31/ user rank 45)</li>
<li>percentage(100%)</li>
</ul>
<p>Is it recommended to discard this numerics before creating a vectorizer(bow/tf-idf) for any model(classification/regression) development?</p>
<p>Any quick help on this is much appreciated. Thank you</p>
","nlp"
"81072","How to process the hyphenated english words for any nlp problem?","2020-09-01 12:22:07","","1","1802","<nlp><preprocessing><tfidf><tokenization><bag-of-words>","<p>Im doing preprocessing on english text dataset. I encounter hyphenated words like 'well-known'. Will it be useful</p>
<ul>
<li>if I remove the hyphen as special character and treat it as a single word 'wellknown' or</li>
<li>separate the word into 2 'well' and 'known' or</li>
<li>use all 3 words 'well' , 'known', 'wellknown' in vector creation(BOW/TF-IDF) process for model input.</li>
</ul>
<p>Any quick help on this would be more appreciated. Thank you.</p>
","nlp"
"81034","How to properly compare these two confusion matrix?","2020-08-31 14:00:28","81114","1","1004","<machine-learning><python><nlp><nltk>","<p>I have used Vader, a sentiment analysis tool for social media, on a database of movie reviews. These two confusion matrices differ in the vader.py algorithm, as the first one is from nltk:</p>
<p><a href=""https://i.sstatic.net/YuSdn.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/YuSdn.png"" alt=""enter image description here"" /></a></p>
<p>The second one is deriving from Vader's original code on github and includes fixes to negation words, etc.</p>
<p><a href=""https://i.sstatic.net/0XYa7.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/0XYa7.png"" alt=""enter image description here"" /></a></p>
<p>I was wondering how could I properly compare the two, as I'm not really able to read them. It seems there is not a big difference between them and I don't understand what could be the sources of the errors here.</p>
","nlp"
"81005","Sampling methods for Text datasets (NLP)","2020-08-30 19:27:51","","1","692","<machine-learning><classification><nlp><dataset><statistics>","<p>I am working on two text datasets, one is having 68k text samples and other is having 100k text samples. I have encoded the text datasets into bert embedding.</p>
<pre><code>Text sample &gt; 'I am working on NLP' ==&gt; bert encoding ==&gt; [0.98, 0.11, 0.12....nth]
               # raw text 68k                              # bert encoding [68000, 1024]
</code></pre>
<p>I want to try different custom NLP models on these embeddings, but dataset large to test the model's performance quickly.</p>
<p>To check different models quickly, the best way is to take a small subset of dataset from the entire population and feed it to different algorithms. At last, choose the top algorithms to fit the entire dataset.</p>
<p>I am planning to sample at least 10k samples subset from 68k dataset and 10k subset from 100k dataset. I could select randomly 10k from 68k but that method is not the best way to sample.</p>
<p>Any advice on how to sample embeddings(text) from 68k samples while maintaining the probability distribution of the original population and how many samples would be enough for one sample subset?</p>
<p>Thank you!</p>
","nlp"
"80993","I want to start studying the field of machine translation","2020-08-30 11:32:42","80997","-2","34","<machine-learning><python><nlp><machine-translation>","<p>I've studied Japanese language and literature and passed some linguistic courses and now as for my masters, I want to study natural language processing and especially machine translation. so I tried taking some data science courses online and I'm now a little bit familiar with data science but I know literally nothing about machine translation.
so, long story short, I need to write a proposal in the machine translation field (university requirements) but I don't know where to start reading about machine translation. I tried to read some essays but the level was too high for me, I didn't understand a single thing.
I'd be so thankful if you could guide me through this journey.
thank you ^__^</p>
","nlp"
"80965","Loss first decreases and then increases","2020-08-29 09:30:52","","2","216","<nlp><bert><transformer>","<p>I am using pre-trained <code>xlnet-base-cased</code> model and training it further on real vs fake news detection dataset. I noticed a trend in accuracy for first epoch. Accuracy increases till some point (approximately half) of first epoch and then decreases. Loss also first decreases and then increases rapidly (it is not Nan). What can be reason of this trend ?<br />
I noticed same trend when I tried to run it with <code>roberta-base</code>. But any such trend was not noticed when I trained with <code>distilbert</code>, accuracy goes on increasing in this case.</p>
<p>Here is graph for accuracy VS steps:
<a href=""https://i.sstatic.net/bIY8E.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bIY8E.jpg"" alt=""enter image description here"" /></a></p>
<p>[EDIT]
File containing running output of the model:
<a href=""https://drive.google.com/file/d/1r5AWftyHTLf5sqtgWnQm_4lqB84UrJex/view?usp=sharing"" rel=""nofollow noreferrer"">https://drive.google.com/file/d/1r5AWftyHTLf5sqtgWnQm_4lqB84UrJex/view?usp=sharing</a></p>
","nlp"
"80962","How to handle Tokenized text content which is given in number?","2020-08-29 07:28:56","","0","194","<machine-learning><pandas><nlp>","<p>i have one data set of customer review, but the text data is given is tokenized text number. I am unable to proceed thinking about how to proceed?</p>
<p>As I am encountering such data set the first time, so just need guide how to proceed.</p>
<p><a href=""https://i.sstatic.net/fAABY.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/fAABY.png"" alt=""enter image description here"" /></a></p>
<p>As you can see text field is given in number, so how to proceed please guide?. it will predict the 0/1 +ve or -ve category.</p>
","nlp"
"80840","Is summing a cosine similarity matrix a good way to determine overall similarity?","2020-08-26 23:06:50","","2","1308","<nlp><cosine-distance>","<p>I'm trying to similar research abstracts, so I'm using word embeddings to convert words into 1x768 vectors, so overall turning abstracts into embeddings with shape (#ofwords, 768). Cosine similarity between two abstracts returns a matrix (#ofwords1, #ofwords2), which I then sum up to get an overall score. What I'm wondering is if this summing up of all the values in a cosine similarity matrix is really a good way to determine overall similarity between two different texts? Is there a better, or less computationally expensive way to do this?</p>
","nlp"
"80828","Is it bad to have a lot of one class of Data [K-NN classifier]?","2020-08-26 18:12:42","80835","0","39","<machine-learning><classification><nlp><dataset>","<p>I am trying to train a sklearn K-NN classifier on a labeled text dataset (in Irish). There are 5 classes, 0-4, but there is a lot of variation between how many there are in each class.</p>
<p>What I have done is I've gotten a corpus of Irish text, iterated through every word and stripped a few letters from it based on a linguistic form it took (or not). The problem is, class 4 (which means no action was performed) accounts for 16.5M out of 20.1M entries and it goes all the way down to class 3 with only 36,000 entries.</p>
<p>Gathering more data probably won't help as this basically represents the proportion of times these forms of words appear in real life.</p>
<p>Is this bad for classification and will it bias the classifier in any way? If it does, is that bias actually of help?</p>
<p>Any help is appreciated.</p>
<p>Justin</p>
","nlp"
"80826","Transformer masking during training or inference?","2020-08-26 17:35:55","81492","6","3738","<nlp><training><generative-models><transformer><attention-mechanism>","<p>I'm working through <a href=""http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf"" rel=""noreferrer"">Attention is All you Need</a>, and I have a question about masking in the decoder. It's stated that masking is used to ensure the model doesn't attend to any tokens in the future (not yet predicted), so it can be used autoregressively during inference.</p>
<p>I don't understand how masking is used during inference. When the encoder is given an unseen sample with no ground truth output or prediction, it seems to me that there is nothing to mask, since there aren't any output tokens beyond what the decoder has already produced. Is my understanding of masking correct?</p>
<p>Thanks!</p>
","nlp"
"80821","TF-IDF for Topic Modeling","2020-08-26 15:01:51","80824","1","3342","<nlp><topic-model><lda><tfidf><stanford-nlp>","<p>Can TF-IDF be used a sole method for Topic Modeling ? (I know there are better methods like LDA , LSA etc)</p>
<p>I just want to understand if TF-IDF alone can help us in Topic modeling . If yes , can someone explain how that simple framework works ?</p>
<p>I want to understand the application and capabilities of TF-IDF as a sole method for Topic Modeling. I could not find this anywhere else in the internet .</p>
","nlp"
"80817","What GPU size do I need to fine tune BERT base cased?","2020-08-26 13:48:40","","6","10777","<machine-learning><nlp><word-embeddings><bert><gpu>","<p>I want to fine tune BERT Multilingual but I'm not aware about the GPU requirements to train BERT Multilingual. I have GTX 1050ti 4GB on my local machine. I want to know what size of GPU is needed and what type of GPU is needed to train BERT. I have access to server resources. Could anyone tell me what size of GPU should I request for on server.</p>
","nlp"
"80797","Is it possible to classify documents of corpus using labels?","2020-08-26 01:36:55","","0","240","<nlp><text-classification><similar-documents><document-term-matrix>","<p>I have a corpus of 23000 documents that need to be classified into 5 different categories. I do not have any labeled data available to me, just freeform text documents and labels(yes, one-word labels, not topics).</p>
<p>So I followed a 2-step approach:</p>
<ol>
<li>Synthetically generate labeled data (using a rule-based labeling approach, obviously the recall is very low, ~ 1/8 documents are labeled)</li>
<li>Somehow, use this labeled data to identify labels for other documents.</li>
</ol>
<p>I have attempted the following approaches for step 2:</p>
<ol>
<li>Topic modeling on data classified using rules to extract significant terms and using significant terms to label the remaining documents.</li>
<li>Finding significant terms using sentence embedding</li>
<li>Using sentence embedding as features for my classifier</li>
</ol>
<p>But I haven't been successful in getting good results for my document classifier. Are there any other methods that can be used to classify the documents?</p>
<p>All help is greatly appreciated.</p>
","nlp"
"80792","Searching for a dataset that targets difficult words","2020-08-25 19:59:29","","1","67","<nlp><dataset><word>","<p>I am trying to find a dataset in which dataset targets words that are difficult. I understand there would be different levels of difficulty for each individual , but if we considered an average individual, I want to detect the difficult words that would be present in a sentence.</p>
<p>Example:
Yes, may be today's Britains are not responsible for some of these reparations but the same speakers have pointed with pride to their foreign aid - you are not responsible for the people starving in Somalia but you give them aid surely the principle of reparation for what is the wrongs that have done cannot be denied.</p>
<p>IN the above sentence, the model should detect the word - reparations successfully.</p>
","nlp"
"80782","Can we use sentence transformers to embed sentences without labels?","2020-08-25 14:39:45","80784","1","934","<nlp><word-embeddings><bert>","<p>I was trying to use this project :</p>
<p><a href=""https://github.com/UKPLab/sentence-transformers"" rel=""nofollow noreferrer"">https://github.com/UKPLab/sentence-transformers</a></p>
<p>for embedding non english sentences, the language is not a human speaking language, its machine language (x86)</p>
<p>but the problem is i cannot find a simple example where it shows how can i embed sentences using a custom dataset without any labels or similarity values of the sentences.</p>
<p>basically i have an array of sentences lists without any labels for sentences or similarity values for them, and i want to embed them into vectors in a way that it preserves the semantic of the sentence the best way possible, so far i have used word2vec and doc2vec using gensim library so i wanted to try this method to see if its any better?</p>
","nlp"
"80759","How can I picture an unfolded RNN as a normal Feed Forward Network?","2020-08-25 02:30:38","","0","139","<nlp><rnn>","<p>I am currently working on a Transformer architecture.
Trying to picture an RNN (or Encoder) as a normal Feed Forward network really confused me after looking at the following image in an <a href=""https://towardsdatascience.com/nlp-sequence-to-sequence-networks-part-2-seq2seq-model-encoderdecoder-model-6c22e29fd7e1"" rel=""nofollow noreferrer"">article</a>:</p>
<p><strong>(Image 1)</strong>
<a href=""https://i.sstatic.net/ufNEe.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ufNEe.png"" alt=""Image 1 - Encoder architecture"" /></a></p>
<p>I am usually used to seeing it like this:</p>
<p><strong>(Image 2)</strong></p>
<p><a href=""https://i.sstatic.net/C71eo.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/C71eo.png"" alt=""Image 2 - Unfolded Encoder architecutre"" /></a></p>
<p>In Image 1, it shows that the input goes in ALL at once, whereas in Image 2, we see only a single input at a timestep.</p>
<p>My two questions:</p>
<ol>
<li><p>Does this mean that Image 1 is a single node in Image 2?</p>
</li>
<li><p>How can I picture an RNN-architecture as Image 1?</p>
</li>
</ol>
","nlp"
"80729","How is the Gaussian noise given to this BLSTM based GAN?","2020-08-24 14:06:32","81063","3","185","<neural-network><deep-learning><lstm><nlp><gan>","<p>In a conditional GAN, we give a random noise along with a label to the generator as input. In <a href=""https://arxiv.org/pdf/1811.02356.pdf"" rel=""nofollow noreferrer"">this paper</a>, I don't understand why in one section of the paper, they say they are giving the random noise as input and the in another section of the paper they are saying it is concatenated to the output.</p>
<p><strong>page 2</strong></p>
<p><a href=""https://i.sstatic.net/2bZzN.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/2bZzN.png"" alt=""page 2"" /></a></p>
<p><strong>page 2 footnote</strong></p>
<p><a href=""https://i.sstatic.net/Bipho.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Bipho.jpg"" alt=""page 2 footnote"" /></a></p>
<p><strong>page 3 model setup section</strong></p>
<p><a href=""https://i.sstatic.net/zqV5F.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/zqV5F.jpg"" alt=""page 3 model setup section"" /></a></p>
<p>little overview of the paper:  Code switching is a phenomenon in spoken language where we switch between two different languages. Mixed language models improve the accuracy of automatic speech recognition to higher degree but the problem is less availability of mixed language written sentences. Thus, as a data augmentation technique, a conditional GAN is developed to synthesize English, Mandarin mixed sentences from a pure Mandarin sentence. The trained generator acts as an agent telling which words in the Mandarin sentence have to be translated. It outputs a binary array (of length equal to input Mandarin sentence length). Both generator and discriminator are BLSTM networks.</p>
<p><strong>#####EDIT: The author accepted that it is a typo, noise should be concatenated after the embedding layer not to the output of BLSTM
Author's reply:
It is a typo in page 3.
The noise is concatenated with the output of the embedding layer.
Thanks for your correction.
#####</strong></p>
","nlp"
"80711","How to categorize unlabelled promotional email data","2020-08-24 04:28:54","","0","52","<machine-learning><deep-learning><nlp><unsupervised-learning><text-classification>","<p>I have unlabelled data of promotional emails. I want to categorize those emails based on the topics like fashion, health &amp; wellness, sports, media, Entertainment, etc. Can anyone let me know any effective method or any pre-trained model which i can directly use to categorize those emails (any similar model like YOLO which we have for object detection).</p>
<p>Any kind of help regards to this would be much appreciable. Thanks!</p>
","nlp"
"80654","Word representation that gives more weight to terms frequent in corpus?","2020-08-22 14:28:37","80704","2","174","<anomaly-detection><nlp><outlier><tfidf><bag-of-words>","<p>The tf-idf discounts the words that appear in a lot of documents in the corpus. I am constructing an anomaly detection text classification algorithm that is trained only on valid documents. Later I use One-class SVM to detect outliers. Interesting enough the tf-idf performs worse than a simple count-vectorizer. First I was confused, but later it made sense to me, as tf-idf discounts attributes that are most indicative of a valid document. Therefore I was thinking of a new approach that would weight words that always appear in documents more, or rather assign a negative weight for the absence of such words. I have preset dictionary of words, so there is no worry that irrelevant words such as(is, that) will be weighted.</p>
<p>Do you have any ideas about such representations? The only thing I could imagine would be subtracting the document frequency from the attributes that are zero in a certain document.</p>
","nlp"
"80639","State-of-the-art Python packages that can evaluate language similarity","2020-08-22 03:34:01","","2","297","<nlp><model-evaluations><similarity><language-model>","<p>I am trying to evaluate the likelihood of generating a specific sentence out of a large set of sentences. To do this, I start from a simple approach: training a custom n-gram language model and calculating the perplexity values for a list of sentences.</p>
<p>I found that the package KenLM (<a href=""https://www.aclweb.org/anthology/W11-2123/"" rel=""nofollow noreferrer"">https://www.aclweb.org/anthology/W11-2123/</a>) was often used to do this task. However, it's kind of old (published in 2011).</p>
<p>On the other hand, I noticed that the two most famous state-of-the-art NLP packages, BERT and GPT-2, are both about pre-trained models.</p>
<p>I wonder if there is any package newer than KenLM suitable for this kind of likelihood evaluation task.</p>
","nlp"
"80598","Loading a model with attention layer and custom metric","2020-08-21 04:37:12","","0","870","<deep-learning><nlp>","<p>I have a neural network with SeqSelfAttention (<a href=""https://pypi.org/project/keras-self-attention/"" rel=""nofollow noreferrer"">https://pypi.org/project/keras-self-attention/</a>). Also I implemented a custom metric for F1. Then I saved the model without problems, but when the model is loaded using keras.models.load_model(model_path, custom_objects=SeqSelfAttention.get_custom_objects())</p>
<p>the next error is presented:
ValueError: Unknown metric function: f1</p>
<p>If I coded with the F1 metric as:
saved_model = load_model(Modelfilename, custom_objects={&quot;f1&quot;: f1})</p>
<p>the error say:</p>
<p>ValueError: Unknown layer: SeqSelfAttention</p>
<p>Are there a way of combine the two parameters in &quot;custom_objects&quot;?</p>
","nlp"
"80595","How should I use BERT embeddings for clustering (as opposed to fine-tuning BERT model for a supervised task)","2020-08-21 02:00:07","80607","8","7780","<machine-learning><deep-learning><nlp><word-embeddings><bert>","<p>First of all, I want to say that I am asking this question because I am interested in using BERT embeddings as document features to do clustering. I am using Transformers from the Hugging Face library. I was thinking of averaging all of the Word Piece embeddings for each document so that each document has a unique vector. I would then use those vectors for clustering. Please feel free to comment if you think this is not a good idea, or if I am missing something or not understanding something.</p>
<p>The issue that I see with this is that you are only using the first N tokens which is specified by <code>max_length</code> in Hugging Face library. What if the first N tokens are not the best representation for that document? Wouldn't it be better to randomly choose N tokens, or better yet randomly choose N tokens 10 times?</p>
<p>Furthermore, I realize that using the WordPiece tokenizer is a replacement for lemmatization so the standard NLP pre-processing is supposed to be simpler. However, since we are already only using the first N tokens, and if we are not getting rid of stop words then useless stop words will be in the first N tokens. As far as I have seen, in the examples for Hugging Face, no one really does more preprocessing before the tokenization.</p>
<p>[See example below of the tokenized (from Hugging Face), first 64 tokens of a document]</p>
<p>Therefore, I am asking a few questions here  (feel free to answer only one or provide references to papers or resources that I can read):</p>
<ol>
<li>Why are the first N tokens chosen, instead of at random? 1a) is there anything out there that randomly chooses N tokens perhaps multiple times?</li>
<li>Similar to question 1, is there any better way to choose tokens? Perhaps using TF-IDF on the tokens to at least rule out certain useless tokens?</li>
<li>Do people generally use more preprocessing before using the Word Piece tokenizer?</li>
<li>To what extent does the choice of <code>max_length</code> affect performance?</li>
<li>Why is there a limit of 512 max length in Hugging Face library? Why not just use the length of the longest document?</li>
<li>Is it a good idea to average the WordPiece embeddings to get a matrix (if you want to do clustering)?</li>
<li>Is it a good idea to use BERT embeddings to get features for documents that can be clustered in order to find similar groups of documents? Or is there some other way that is better?</li>
</ol>
<p>original:
<code>'Trump tries to smooth things over with GOP insiders. Hollywood, Florida (CNN) Donald Trump\'s new delegate guru told Republican Party insiders at a posh resort here on Thursday that the billionaire front-runner is recalibrating the part &quot;that he\'s been playing&quot; and is ready</code></p>
<p>tokenized:</p>
<pre><code>['[CLS]',
 'trump',
 'tries',
 'to',
 'smooth',
 'things',
 'over',
 'with',
 'go',
 '##p',
 'insider',
 '##s',
 '.',
 'hollywood',
 ',',
 'florida',
 '(',
 'cnn',
 ')',
 'donald',
 'trump',
 &quot;'&quot;,
 's',
 'new',
 'delegate',
 'guru',
 'told',
 'republican',
 'party',
 'insider',
 '##s',
 'at',
 'a',
 'po',
 '##sh',
 'resort',
 'here',
 'on',
 'thursday',
 'that',
 'the',
 'billionaire',
 'front',
 '-',
 'runner',
 'is',
 'rec',
 '##ali',
 '##bra',
 '##ting',
 'the',
 'part',
 '&quot;',
 'that',
 'he',
 &quot;'&quot;,
 's',
 'been',
 'playing',
 '&quot;',
 'and',
 'is',
 'ready',
 '[SEP]']
</code></pre>
","nlp"
"80543","NLP SBert (Bert) for answer comparison STS","2020-08-20 00:19:19","","2","540","<nlp><bert>","<p>I've been researching a good way to automate short answer evaluation. Essentially a teacher gives a test with some questions like:</p>
<p>Question: why did columbus sail westward to find asia?</p>
<p>Answer: so he could find a new trade route to Asia through the ocean. Three goals of the Spanish in the Americas were the desire to attain great amounts of riches, to establish claims on as much land as possible, and to colonize as much as possible.</p>
<p>With that we have the correct answer and would like to compare that with the students answer and produce a score based on similarity. I know this isn't a reliable replacement for human grading, but for the sake of the example.</p>
<p>I've come across this paper and codebase:
<a href=""https://arxiv.org/pdf/1908.10084.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1908.10084.pdf</a></p>
<p><a href=""https://github.com/UKPLab/sentence-transformers"" rel=""nofollow noreferrer"">https://github.com/UKPLab/sentence-transformers</a></p>
<p>It seems like the ideal method for solving this problem, but most examples are based on scoring/ranking of semantic search. I question whether I'm on the right path, given that I'm just comparing two answers and not a cluster. Anyone with more experience, possibly can provide some guidance?</p>
","nlp"
"80537","What are the hidden states in the Transformer-XL? Also, how does the recurrence wiring look like?","2020-08-19 21:00:18","80553","0","1589","<deep-learning><nlp><rnn><transformer><attention-mechanism>","<p>After exhaustively reading the many blogs and papers on Transformers-XL, I still have some questions before I can say that I understand Transformer-XL (and by extension XLNet). Any help in this regard is hugely appreciated.</p>
<ol>
<li>when we say <strong>hidden states</strong> are transferred from one segment to another, what exactly is included in these hidden states? Are the weights of the networks implementing the attention mechanism (i.e. calculating the Q, K and V) included? Are the weights involved in calculating the input word embedding included in the hidden state?</li>
<li>When the <strong>hidden states</strong> are transferred during recurrence, is this transfer from the encoder of one segment to the encoder of the next segment? Or is it from the decoder of the current segment to the encoder of the next segment? Is the decoder involved at all in the hidden state transfer?</li>
<li>I see images like the following in the following in the papers and blogs. what do the dots represent? encoders? decoders? or an entire unit? I guess the answer to my second question will shed a light on this one too.
<a href=""https://i.sstatic.net/fULu8.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/fULu8.png"" alt=""From Transformer-XL Google page"" /></a></li>
</ol>
<p>Thank you</p>
","nlp"
"80526","Inference from text data without label or Target","2020-08-19 16:54:55","","0","92","<nlp><clustering><text-mining>","<p>I have a use case where I have text data entered by an approver while approving some loans.</p>
<p>I have to make some inferences as to what could be the reasons for approval using NLP. How should I go about it?</p>
<p>It's a Non-English language. Can Clustering of text help?? Is it possible to cluster TEXT OF non-English language using python libraries?</p>
","nlp"
"80488","Is adding the embedded words of a sentence to represent the sentence a good approach?","2020-08-19 05:21:41","80494","0","122","<word-embeddings><word2vec><nlp>","<p>I have a dataset of sentences in a non english language like :</p>
<ol>
<li><p>word1 word2 word3 word62</p>
</li>
<li><p>word5 word1 word2</p>
</li>
</ol>
<p>Now i want to turn each variable length sentence to a fixed size vector to give it to my model, and i want all the words in the sentences to have effect on the output</p>
<p>I thought maybe i can use an algorithm like word2vec and turn each word into a fixed size vector, and add all of them to represent the sentence, is this a meaningful approach? is this better than adding the hot one vectors of the words to represent the sentence?  is there a better approach than these two?</p>
<p>EDIT1: basically i have a dataset of random variable length sentences and i want to embed them the best way possible, meaning keeping as much  information as possible in the resulting embedded vectors (which all have the same size)</p>
","nlp"
"80483","Based on transformer, how to improve the text generation results?","2020-08-19 04:09:42","80496","1","490","<deep-learning><nlp><transformer><text-generation>","<p>If I do not pretrain the text generation model like BART, how to improve the result based on transformer like tensor2tensor?</p>
<p>What are the improvement ideas for transformer in text generation task?</p>
","nlp"
"80475","Labelled dataset for NLP regression","2020-08-18 21:52:44","","0","486","<nlp><dataset><regression>","<p>I am looking for a dataset, where I could use NLP techniques to estimate target value using regression. For example, I could be give a few sentences that describe an accident, and the target value would be the cost of an accident. Kaggle has quite a few datasets for NLP, but they  all, as far as I can see, for classification.</p>
","nlp"
"80449","best approach to embed random length sequences of words as a fixed size vector without having a maximum length?","2020-08-18 14:06:49","80457","0","513","<rnn><word-embeddings><word2vec><nlp>","<p>I have a dataset of sentences in a non-English language like:</p>
<ol>
<li><p>word1 word2 word3 word62</p>
</li>
<li><p>word5 word1 word2</p>
</li>
</ol>
<p>and the length of each sentence is not fixed.</p>
<p>Now, I want to represent each sentence as a fixed sized vector and give it to my model and i want to keep as much information as possible in the embedding, and i don't want to have a maximum length for sentences because important information might happen in the end.</p>
<p>The only two approaches I can think of so far are:</p>
<ol>
<li><p>Convert them to one hot vector and add them</p>
</li>
<li><p>Convert them to a word embedding and then add them</p>
</li>
</ol>
<p>Is there any better way? What is the best approach to represent a variable length sentence without losing information from it (like having a maximum length for each sentence - I want all the words in the sentence to affect the embedding)?</p>
","nlp"
"80409","Document Similarity to List of Words in Sentiment Analysis","2020-08-17 17:15:32","","1","34","<nlp><similar-documents><doc2vec>","<p>How would you go about finding document similarity to a list of words in Sentiment Analysis?</p>
<p>Looking find document similarity to multiple lists of words in sentiment analysis. I had been working on this with my intern but he is sorting by sentiment average to find the most similar score of each list or combinations of the list of words. I assume this isn't the best approach, I was thinking it should be a separate thing like below and I will attempt it.</p>
<p>Suppose he might have wanted to find a separate similarity score for each document, for example, a bunch of Moods, themes, feelings like sentiment analysis with 10 .txt files each with words to fit a theme or mood etc. like this below.</p>
<p>I am have been learning NLP on the side to help him and now I want to attempt this any suggestions of feedback greatly welcome.</p>
<p>I was thinking should I instead do doc2vec and get a similarity score for this separately and just use the sentiment score as another score.</p>
<p>Happy.txt</p>
<pre><code>cheerful
contented
delighted
ecstatic
elated
glad
joyful
joyous
jubilant
lively
merry
overjoyed
peaceful
pleasant
pleased
thrilled
upbeat
blessed
blest
blissful
blithe
can't complain
captivated
chipper
</code></pre>
<p>Each column is a thing(movie, product, celebrity, whatever) and each thing has been reviewed.</p>
<p>examples;</p>
<p>thing1 was freaky awesome we have to do that again!!!</p>
<p>Each thing is a bunch of text documents reviewing a thing either positive, neutral, or negative, and has a sentiment score.</p>
<p>Then a similarity score to each txt file list of words.</p>
<pre><code>
so would have a separate score for each mood and for the sentiment and hot encode any categorial stuff then he would be able to get the most similar thing, to mood or almost any combination of them

Thing1 | Thing2 | Thing3
Happy | 0.857 | 0.126 | 0.836
Sad | 0.221 | 0.999 | 0.236
Romantic | 0.765 | 0.126 | 0.657
Humorous | 0.231 | 0.986 | 0.353

Sentiment | 0.987 | 0.237 | 0.736

** I also one hot encoded on category features** 

Cat can be 1 or more
Category A | 1 | 0 | 1
Category B | 0. |. 1. |. 1
Category C|. 1. | 0 | 0

Price can only be one
Price 1-5 |. 1. | 0 | 0
Price 5-10. |. 1. | 0 | 0
price 11-20 | 0. |. 1. |. 1


** The happiest** 

Thing1 0.857
Thing3 0.836
Thing2 0.126

** The saddest** 

Thing2 0.999
Thing3 0.236
Thing1 0.221

** Most similar to Thing3** 
Thing3 0.836 0.236 0.657 0.353

Thing1 0.857 0.221 0.765 0.321
Thing2 0.126 0.999 0.126 0.986
</code></pre>
<p>Using doc2vec I had did something similar with a bunch of Disney Princess books which lead me to this train of thought. Hopefully this right train of thought I want to help him finish before his intern finished.</p>
<pre><code>Doc2Vec
</code></pre>
<pre><code>
# Read and tag each book into disney_corpus

disney_corpus = []

for book_filename in book_filenames:

    with codecs.open(book_filename, &quot;r&quot;, &quot;utf-8&quot;) as book_file:

        disney__corpus.append(

            gensim.models.doc2vec.TaggedDocument(

                gensim.utils.simple_preprocess( # Clean the text with simple_preprocess

                    book_file.read()),

                    [&quot;{}&quot;.format(book_filename)])) # Tag each book with its filename


# Larger values for iter should improve the model's accuracy.

model = gensim.models.Doc2Vec(vector_size = 300, 

                              min_count = 3, 

                              epochs = 100)


model.build_vocab(book_corpus)

print(&quot;model's vocabulary length:&quot;, len(model.wv.vocab))


model's vocabulary length: 1838

model.train(disney_corpus, epochs= 100, total_examples=len(sentences))


model.docvecs.most_similar(0) # Aladdin

[('disney\\TheLittleMermaid.rtf', 0.07826487720012665),
 ('disney\\Mulan.rtf', -0.035049568861722946),
 ('disney\\BeautyAndTheBeast.rtf', -0.08333050459623337)]


model.docvecs.most_similar(1) #BeautyAndTheBeast

[('disney\\TheLittleMermaid.rtf', 0.06666166335344315),
 ('disney\\Mulan.rtf', 0.02150556817650795),
 ('disney\\Aladdin.rtf', -0.08333051204681396)]

model.docvecs.most_similar(2) # Mulan

[('disney\\TheLittleMermaid.rtf', 0.12576593458652496),
 ('disney\\BeautyAndTheBeast.rtf', 0.02150557190179825),
 ('disney\\Aladdin.rtf', -0.035049568861722946)]

model.docvecs.most_similar(3) # TheLittleMermaid

[('disney\\Mulan.rtf', 0.12576593458652496),
 ('disney\\Aladdin.rtf', 0.07826487720012665),
 ('disney\\BeautyAndTheBeast.rtf', 0.06666165590286255)]
</code></pre>
","nlp"
"80405","Which (naive) NLP method for correlating human messages in chatrooms?","2020-08-17 16:16:23","","1","35","<python><scikit-learn><nlp>","<p>Suppose an online chatroom is filled with many &quot;alt&quot; accounts - i.e, multiple accounts are being controlled by one user, a troll. This user leverages multiple accounts in order to steer the conversation in particular directions to suit their needs. This user may also be altering their mode of speech per account, to avoid detection.</p>
<p>Suppose I want to detect these alt accounts by using some kind of NLP classifier. What would the best method be? Where &quot;best&quot; means effective, but also relatively simple to set up - as in something you could do in python with relatively basic sklearn modules.</p>
<p>What I've tried so far is to collect the 100 most frequent words used by each user, and just throw that corpus into a sklearn.feature_extraction.text.TfidfVectorizer, and then looking at the pairwise similarity. These are mostly stop words, words which most NLP articles online are telling me to ignore. But I think these basic (almost unconsciously-used) words are less likely to be subject to obfuscation. The user may, for example, be applying varying spellings of less-common words (realize vs realise etc) - but the user could not plausibly have alternative spellings for [&quot;the&quot;, &quot;of&quot;, &quot;you&quot;, &quot;one&quot;] etc.</p>
<p>Like I said, that isn't an &quot;advanced&quot; approach, but it's an example of the general level of complexity that I'm willing to undertake. Is there are any validity to that approach? If not, can you suggest a better one?</p>
","nlp"
"80370","Using BERT for co-reference resolving, what's the loss function?","2020-08-16 17:43:04","","0","143","<nlp><bert>","<p>I'm working my way around using BERT for co-reference resolving. I'm following this highly-cited paper BERT for Coreference Resolution: Baselines and Analysis (<a href=""https://arxiv.org/pdf/1908.09091.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1908.09091.pdf</a>). I have following questions, the details can't be found easily from the paper, hope you guys help me out.</p>
<p>What’s the input? is it antecedents + parapraph?
What’s the output? clusters &lt;mention, antecedent&gt; ?
More importantly <strong>What’s the loss function?</strong></p>
<p>For comparison, in another highly-cited paper by [Clark .et al] using Reinforcement Learning, it's very clear about what reward function is. <a href=""https://cs.stanford.edu/people/kevclark/resources/clark-manning-emnlp2016-deep.pdf"" rel=""nofollow noreferrer"">https://cs.stanford.edu/people/kevclark/resources/clark-manning-emnlp2016-deep.pdf</a></p>
","nlp"
"80355","Improving Training accuracy of LSTM in Keras for ratings prediction given reviews","2020-08-16 11:31:17","","1","29","<keras><nlp><lstm><accuracy>","<p>I'm new to Deep Learning and NLP. I found a dataset online which has reviews of different companies and their corresponding ratings from 1 to 5. I encoded the labels, then removed some basic stop words from the text, and tokenized it all using Keras. My model, however, is not performing well at all. Training accuracy starts off at 30% and only goes up to 60%. Validation accuracy also starts off low, at around 35%, and ends at 30%. What do I do to improve the performance?</p>
<pre><code>model = tf.keras.Sequential([
tf.keras.layers.Embedding(10000, 20, input_length=150),
tf.keras.layers.BatchNormalization(),
tf.keras.layers.LSTM(40, dropout=0.2, recurrent_dropout=0.2),
tf.keras.layers.BatchNormalization(),
tf.keras.layers.Dense(15, activation='relu'),
tf.keras.layers.Dense(6, activation='softmax')])

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(padded, train_label1, epochs=10, batch_size=45 ,validation_split=0.1)
</code></pre>
","nlp"
"80313","How to choose threshold for gensim Phrases when generating bigrams?","2020-08-14 21:05:06","","5","2197","<nlp><text-mining><lda><gensim>","<p>I'm generating bigrams with <code>from gensim.models.phrases</code>, which I'll use downstream with TF-IDF and/or gensim.LDA</p>
<pre class=""lang-py prettyprint-override""><code>from gensim.models.phrases import Phrases, Phraser

# 7k documents, ~500-1k tokens each. Already ran cleanup, stop_words, lemmatization, etc
docs = get_docs()

phrases = Phrases(docs)
bigram = Phraser(phrases)
docs = [bigram[d] for d in docs]
</code></pre>
<p><code>Phrases</code> has <code>min_count=5</code>, <code>threshold=10</code>. I don't quite understand how they interact, they seem related? Anyway, I see <code>threshold</code> having values in different tutorials ranging 1-&gt;1000, described as important in determining the number of bigrams generated. I can't find an explanation on how to come by a decent value for one's purposes, simply &quot;fiddle and what works best for you&quot;. Is there any intuition / formula for choosing this value, maybe something like &quot;if you want x% more tokens added to your dictionary, use y&quot;; or &quot;if your corpus size is x, try y&quot;? I also see <code>scoring='default'</code> can be set to <code>'npmi'</code> instead. From the <a href=""https://arxiv.org/pdf/1310.4546.pdf"" rel=""noreferrer"">linked paper</a>, they say <code>and t is a chosen threshold, typically around 10e−5</code>. Might that be a decent approach if I just want this to work &quot;good enough&quot; without needing to fiddle much? That is, <code>phrases = Phrases(docs, scoring='npmi', threshold=10e-5)</code>.</p>
<p>TL;DR: is there a simple or intuitive way to choose a decent <code>threshold</code> (eg, based on corpus size); alternatively would <code>scoring='npmi',threshold=10e-5</code> be simpler?</p>
","nlp"
"80227","Should I keep common stop-words when preprocessing for word embedding?","2020-08-13 12:09:52","80231","3","1999","<word-embeddings><word2vec><nlp>","<p>If I want to construct a word embedding by predicting a target word given context words, is it better to remove stop words or keep them?</p>
<blockquote>
<p>the quick brown fox jumped over the lazy dog</p>
</blockquote>
<p>or</p>
<blockquote>
<p>quick brown fox jumped lazy dog</p>
</blockquote>
<p>As a human, I feel like keeping the stop words makes it easier to understand even though they are superfluous.</p>
<p>So what about for a Neural Network?</p>
","nlp"
"80196","How to insert sentences into training data which has 2 words, 3 words 4 and 5 etc into training data?","2020-08-12 19:54:08","","0","43","<deep-learning><tensorflow><pytorch><nlp>","<p>I have a set of sentences which each contain 2 words, 3 words, 4 words, 5 words etc. When I am trying to give the training data only the first two words in a sentence it is not accepting it. It is showing given 4 expected 2. How do you deal with this? Said another way, how do you to insert each and every sentence into the training data. I am trying to predict the next word.</p>
<p>My code:</p>
<pre><code>import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.autograd import Variable

dtype = torch.FloatTensor

sentences = [ &quot;i like dog&quot;, &quot;i love coffee&quot;, &quot;i hate milk&quot;, &quot;i love football&quot;, &quot;hi im great&quot;, &quot;i love watching football&quot;, &quot;i don't eat chicken&quot;]

word_list = &quot; &quot;.join(sentences).split()
word_list = list(set(word_list))
word_dict = {w: i for i, w in enumerate(word_list)}
number_dict = {i: w for i, w in enumerate(word_list)}
n_class = len(word_dict) # number of Vocabulary

# NNLM Parameter
n_step = 2 # n-1 in paper
n_hidden = 2 # h in paper
m = 2 # m in paper

def make_batch(sentences):
    input_batch = []
    target_batch = []

for sen in sentences:
    word = sen.split()
    input = [word_dict[n] for n in word[:-1]]
    target = word_dict[word[-1]]

    input_batch.append(input)
    target_batch.append(target)

return input_batch, target_batch

# Model
class NNLM(nn.Module):
    def __init__(self):
        super(NNLM, self).__init__()
        self.C = nn.Embedding(n_class, m)
        self.H = nn.Parameter(torch.randn(n_step * m, n_hidden).type(dtype))
        self.W = nn.Parameter(torch.randn(n_step * m, n_class).type(dtype))
        self.d = nn.Parameter(torch.randn(n_hidden).type(dtype))
        self.U = nn.Parameter(torch.randn(n_hidden, n_class).type(dtype))
        self.b = nn.Parameter(torch.randn(n_class).type(dtype))

    def forward(self, X):
        X = self.C(X)
        X = X.view(-1, n_step * m) # [batch_size, n_step * n_class]
        tanh = torch.tanh(self.d + torch.mm(X, self.H)) # [batch_size, n_hidden]
        output = self.b + torch.mm(X, self.W) + torch.mm(tanh, self.U) # [batch_size, n_class]
        return output

model = NNLM()

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

input_batch, target_batch = make_batch(sentences)
input_batch = Variable(torch.LongTensor(input_batch))
target_batch = Variable(torch.LongTensor(target_batch))

# Training
for epoch in range(5000):

optimizer.zero_grad()
output = model(input_batch)

# output : [batch_size, n_class], target_batch : [batch_size] (LongTensor, not one-hot)
loss = criterion(output, target_batch)
if (epoch + 1)%1000 == 0:
    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))

loss.backward()
optimizer.step()

# Predict
predict = model(input_batch).data.max(1, keepdim=True)[1]

# Test
print([sen.split()[:2] for sen in sentences], '-&gt;', [number_dict[n.item()] for n in predict.squeeze()])

ValueError                                Traceback (most recent call last)
&lt;ipython-input-26-65fcd35f0145&gt; in &lt;module&gt;()
      3 
      4 input_batch, target_batch = make_batch(sentences)
----&gt; 5 input_batch = Variable(torch.LongTensor(input_batch))
      6 target_batch = Variable(torch.LongTensor(target_batch))

ValueError: expected sequence of length 2 at dim 1 (got 3)
</code></pre>
","nlp"
"80191","Overfitting while fine-tuning pre-trained transformer","2020-08-12 18:03:26","","4","11232","<nlp><transfer-learning><transformer>","<p>Pretrained transformers (GPT2, Bert, XLNET) are popular and useful because of their transfer learning capabilities.</p>
<p>Just as a reminder: The goal of Transfer learning is is to transfer knowledge gained from one domain/task and use that transfer/use that knowledge to solve some related tasks. This is done by training a model on a huge amount of labelled data (which we already have and is probably easy to get), then remove the last few layers and fine-tune the model for the new related task with task-related dataset.</p>
<p>I took a recent pretrained transformer published by Google, called XLNET, and just added the classification layer from the top of it and fine-tuned the whole network. (Which is the main intention of this kind of model, correct me if I am wrong).</p>
<p>The problem is that the model is largely overfitting. I have 1200 examples to train and each has 350 words on average.</p>
<p>To overcome the overfitting, I set the dropout of each layer of the transformer from 0.1 to 0.5. This did not work. So I decreased the number of trainable parameters (since the transformer has a huge number of parameters), by freezing first 10 layers (11 layers + 1 classification layer in total). Even that does not work. So I counted the number of trainable parameters in the last layer. There are 7680000 parameters which are very high compared to my dataset (around 1200*350= 420000 words). So, this high number of tunable parameters is the most possible reason for overfitting.</p>
<p>Here is the loss graph:</p>
<p><a href=""https://i.sstatic.net/EXOXZ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/EXOXZ.png"" alt=""enter image description here"" /></a></p>
<p>My questions are:
Do you see any flaw in my analysis?
Is there anything I can do to decrease overfitting? (tried with low learning rate and large batch size)
If my analysis is correct, then the claim that &quot;fine-tune pre-trained transformers with small dataset&quot; is bit misleading and datasets should not be so small. Am I correct?</p>
","nlp"
"80114","How to serialize/pickle a spacy ner model?","2020-08-11 12:07:21","80117","1","2140","<python><nlp><named-entity-recognition><spacy>","<p>I have trained a custom SpaCy named entity recognition model. I saved the model to disk using:</p>
<pre><code>nlp.to_disk()
</code></pre>
<p>which results in keeping the model in a folder. Is it possible to make the nlp object to a pickle file?</p>
","nlp"
"80090","Are there any objections to using the same (unlabelled) data for pre-training of a BERT-Based model and the downstream task?","2020-08-11 04:16:09","","3","103","<nlp><bert><pretraining>","<p>I'm looking to train an Electra model using unlabelled data in a specific field. Are there any objections to using the same data for unsupervised learning and then using the same data downstream for the supervised learning task?</p>
","nlp"
"80074","Text generation - Input text (one sentence or many sentences)","2020-08-10 17:35:43","","1","57","<neural-network><nlp><lstm><pytorch>","<p>I am currently working on a project: I want to generate text with a LSTM using Pytorch. My model is working but I have a question about the methodology:</p>
<p>I'm using the BPTTIterator and something seems weird to me: you have to give one big example with all your text in it then it will provide your neural network with the current word and the target word for each step. So, it looks like that my LSTM won't be able to &quot;understand&quot; if it's the beginning or the end of the sentence. I saw that usually you feed your LSTM with one sentence at the time with SOS and EOS tokens.</p>
<p>My question is: <strong>what does it changes to use one or another method and which one is the best practice</strong>?</p>
","nlp"
"80033","How does TF-IDF classify a document based on ""Score"" alloted to each word","2020-08-09 18:34:38","80039","0","138","<classification><nlp><tfidf>","<p>I understand how TF-IDF &quot;score&quot; is calculated for each word in a document, but I do not get how can it be used to classify a test document. For example, if the word &quot;Mobile&quot; occurs in two texts, in the training data, one about Business (like the selling of Mobiles) and the other about Tech, then how does the &quot;score&quot; for word &quot;Mobile&quot;, in both training and test document over the given dataset, help the algorithm to classify whether the text (a new test document) belongs to &quot;Business&quot; category or &quot;Tech&quot; category? I'm new to NLP, thanks in advance!</p>
","nlp"
"80002","Prediction using words which were not in training in a CNN with pre-trained word embeddings","2020-08-09 01:49:26","","0","295","<machine-learning><deep-learning><nlp>","<p>In sentence classification using pre-trained embeddings(fasttext) in a CNN,  how does the CNN predict the category of a sentence when the words were not in the training set?</p>
<p>I think the trained model contains weights, these weights are not updated in the prediction stage, are they?. Then, what happens when the words in the sentence (for which the cnn will predict a category) were not seen in the training? I think they do not have a word vector, only the words that were found in the training.</p>
","nlp"
"80000","How keras.layers.embedding learn word embeddings?","2020-08-09 01:12:11","80007","1","38","<tensorflow><nlp><word-embeddings><encoding>","<p>I was trying some tensorflow tutorials and see that in all of them they use layers.embedding to learn these word embeddings, but how are these learned? , with a NN? which arquitecture? , or word2vec?</p>
<p>Thanks</p>
","nlp"
"79995","Explanation about i//2 in positional encoding in tensorflow tutorial about transformers","2020-08-08 22:29:26","","2","159","<tensorflow><nlp><encoding><transformer><attention-mechanism>","<p>I was implementing the transformer architecture in tensorflow.</p>
<p>I was following the tutorial : <a href=""https://www.tensorflow.org/tutorials/text/transformer#setup_input_pipeline"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/text/transformer#setup_input_pipeline</a></p>
<p>They implement the positional encoding in this way:</p>
<pre><code>angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))
</code></pre>
<p>However in the paper i is not divided by 2 (i//2), is this a bug? , or why is the reason to make this operation?</p>
<p><a href=""https://i.sstatic.net/fL0cp.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/fL0cp.jpg"" alt=""enter image description here"" /></a></p>
<p>thanks</p>
","nlp"
"79954","How to create a Document Categorization Classifier for different contexts of Documents","2020-08-07 16:59:47","80215","0","76","<python><nlp><text-classification><machine-translation>","<p>I have a doubt solving a test. The idea here is to demonstrate the <strong>NLP and Machine Translation abilities</strong>.</p>
<p>The Dataset is a multilingual, multi-context set of documents. The dataset is divided on context categories (Wikipedia, conference_papers, Amazon Reviews, etc.,) and on languages.</p>
<p>The objective is to create a document cartegorization classifier (in Python) for the different contexts of the documents. The classifier has to be done at context level, regardless of the language the documents are written in.</p>
<p>An important fact is that The dataset original has been modified and a document Never is repeated in 2 languages.</p>
<p>I have 2 ideas on mind to solve that:</p>
<ol>
<li>Train on all the data creating a multilingual classifier</li>
<li>Doing language detection first and use monolingual models later.</li>
</ol>
<p>What could be a reasonable approach to doing text classification for multiple languages?</p>
","nlp"
"79950","Modelling for similarity between two descriptions","2020-08-07 14:43:31","","0","241","<nlp><similarity><semantic-similarity>","<p>I have a dataset of companies and research projects that they were involved in. A subset of the dataset is shown below.</p>
<p><a href=""https://i.sstatic.net/k9t9F.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/k9t9F.png"" alt=""enter image description here"" /></a></p>
<p>I am trying to find a way to model similarity between the company and the research projects using this description.</p>
<p>For each pair of descriptions, I would like to output a number, similar to cosine similarity, which will indicate how similar the description in second column is, to the project title in third column.</p>
<p>How can I go about this?</p>
<p>Thanks in advance!</p>
","nlp"
"79883","Problem of continuous training - Supervised learning","2020-08-06 10:07:55","","1","241","<nlp><supervised-learning><bert><text-classification><data-augmentation>","<p>I am sure this is a most common problem, but would like to know by experts on how to tackle it. Note that, I mostly deal with textual data (NLP problems).<br />
When a supervised learning model is created, say a text classifier, and it works well on seen data then we deploy the model in production (you can think of a chatbot also).</p>
<p><strong>But in real time, when new type of data comes where the prediction fails, we find that a new word or new pattern is breaking the model. So we go ahead and retrain the model with new encountered data.</strong> This is where the continuous learning problem starts.</p>
<p>Can ML/NLP veterans please suggest some alternates to solve this labor work? Following approaches have been tried and the problems also listed:</p>
<ul>
<li>We simply can't train with new data infinitely. As production systems should be self healing. We cant put the cost of a human admin constantly monitoring the project. Also, it is practically not possible to get huge domain data during model training phase.</li>
<li>Use of advanced embeddings, and SoTA models like BERT.   (Problem: The accuracy of these models is too hard to control)</li>
<li>Synthetic data generation/data augmentatoin.  (Problem : Does not work well in case of NLP problems. Refer: <a href=""https://datascience.stackexchange.com/questions/77916/training-with-less-data"">training-with-less-data</a> )</li>
<li>Unsupervised classification  (Problem: Does not work well on closed domain problems, as most unsupervised models are either statistical which give a fair value of accuracy but not decent , or are trained on public domain data)</li>
<li>Reinforcement learning. (Problem: Real world NLP data is not labeled unlike a self driven car where the feedback is instant)</li>
</ul>
","nlp"
"79830","I tried loading my saved .h5 model and predicting with that model, i'm getting error list index out of range","2020-08-05 13:13:49","","0","1417","<deep-learning><keras><tensorflow><lstm><nlp>","<p>I saved my keras model into .h5 format. Again I've loaded that .h5 file into my colab and tried to predict with that model.
<code>model.save(&quot;/content/drive/My Drive/Datasets/sentiment_analysis.h5&quot;)</code></p>
<pre><code>from keras.models import load_model
loaded_model = load_model(&quot;/content/drive/MyDrive/Datasets/sentiment_analysis.h5&quot;)
loaded_model.predict(&quot;i love machine learning and google&quot;)
</code></pre>
<p>It's giving error list index out of range.</p>
<pre><code>IndexError: list index out of range
</code></pre>
","nlp"
"79817","how to train custom word2vec embeddings to find related articles?","2020-08-05 09:51:53","","0","262","<word-embeddings><nlp><embeddings>","<p>I am beginner in machine learning. My project is to make search engine based on AI which shows related articles when  we search on website. For this i decided to train my own embedding.</p>
<p>I found two methods for this:</p>
<ul>
<li>One is to train network to find next word( i.e inputs=[the quick,the quick brown,the quick brown fox] and outputs=[brown, fox,lazy]</li>
<li>Other method is to train with nearest words(i.e [brown,fox],[brown,quick],[brown,quick]).</li>
</ul>
<p>Which method should i use and after training how should i convert the sentence to a single vector to apply cosine similarity means sentence- the quick brown fox will return 4 vectors how should i convert it to feed for cosine similarity(which takes only one vector) with another sentence.</p>
","nlp"
"79813","Loading a Model with weights and optimizers without creating an instance in PyTorch","2020-08-05 09:33:06","","0","313","<nlp><pytorch><bert>","<p>I recently downloaded <a href=""https://camembert-model.fr/"" rel=""nofollow noreferrer"">Camembert Model</a> to fine-tune it for my purpose.</p>
<p>Upon unzipping the file the contents are:
<a href=""https://i.sstatic.net/DiKVi.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/DiKVi.png"" alt=""enter image description here"" /></a></p>
<p>Upon loading the <code>model.pt</code> file using pytorch:</p>
<pre><code>import torch
model = torch.load(model_saved_at)
</code></pre>
<p>I saw that <code>model</code> was in OrderedDict format containing the following keys:</p>
<pre><code>args
model
optimizer_history
extra_state
last_optimizer_state
</code></pre>
<p>As the name suggests most of them are <code>OrderedKeys</code> themselves with the exception of <code>args</code> which belongs to a class <code>argsparse.Namespace</code>. Using <code>vars()</code> we can see <code>args</code> only contains some hyperparameters and values which are to be passed from the command-line.</p>
<p><code>model[&quot;model&quot;]</code> contains the weights which I want to load and use as my base model.
A small part of it is as shown below:</p>
<pre><code>for ans in model[&quot;model&quot;].keys():
    try:
        print(ans, &quot;\t&quot; ,model[&quot;model&quot;][ans].size())
    except:
        print(ans, type(ans))
</code></pre>
<pre><code>decoder.sentence_encoder.embed_tokens.weight     torch.Size([32005, 768])
decoder.sentence_encoder.embed_positions.weight      torch.Size([514, 768])
decoder.sentence_encoder.layers.0.self_attn.in_proj_weight   torch.Size([2304, 768])
decoder.sentence_encoder.layers.0.self_attn.in_proj_bias     torch.Size([2304])
decoder.sentence_encoder.layers.0.self_attn.out_proj.weight      torch.Size([768, 768])
decoder.sentence_encoder.layers.0.self_attn.out_proj.bias    torch.Size([768])
decoder.sentence_encoder.layers.0.self_attn_layer_norm.weight    torch.Size([768])
decoder.sentence_encoder.layers.0.self_attn_layer_norm.bias      torch.Size([768])
decoder.sentence_encoder.layers.0.fc1.weight     torch.Size([3072, 768])
decoder.sentence_encoder.layers.0.fc1.bias   torch.Size([3072])
decoder.sentence_encoder.layers.0.fc2.weight     torch.Size([768, 3072])
decoder.sentence_encoder.layers.0.fc2.bias   torch.Size([768])
</code></pre>
<p>However, I cannot use <code>load_state_dict()</code> since I have no instance of this class. How am I suppose to load the weights and optimization parameters without creating an instance? I thought of using <code>sentence.bpe.model</code> but they are for tokenization purposes.</p>
","nlp"
"79772","Can we use BERT for only word embedding and then use SVM/RNN to do intent classification?","2020-08-04 13:08:47","","4","8628","<nlp><rnn><svm><word-embeddings><bert>","<p>According to this article, &quot;<a href=""http://www.diva-portal.org/smash/get/diva2:1349006/FULLTEXT01.pdf"" rel=""nofollow noreferrer"">Systems used for intent classification contain the following <strong>two components:
Word embedding, and a classifier</strong>.</a>&quot; This article also evaluated <strong>BERT+SVM</strong> and <strong>Word2Vec+SVM</strong>.</p>
<p>I'm trying to do the opposite, comparing two different <strong>classifiers</strong> (RNN and SVM) using BERT's word embedding.</p>
<p>Most Python codes that I found use BERT for the whole intent classification problem which made me confused. <a href=""https://www.kdnuggets.com/2020/02/intent-recognition-bert-keras-tensorflow.html"" rel=""nofollow noreferrer"">Example</a></p>
<p>I only want to map the words into vectors with BERT and feed the result into a classifier (SVM/RNN).
Does BERT support word embedding and text classification at the same time? Does someone has an explanation? Is what I'm trying to test feasible with Python?</p>
<p><em>I have a dataframe that has two columns: intent and questions. It's a small dataset.</em></p>
<p>Thank you!</p>
","nlp"
"79760","Semantic networks and conceptual graphs","2020-08-04 08:54:26","","1","105","<machine-learning><python><nlp><semantic-similarity>","<p>I would like to use semantic networks to understand changes in texts.
For example when I add/remove some words within a text.</p>
<pre><code>I would like to eat pizza tomorrow evening.
I ate pizza yesterday.
</code></pre>
<p>Or just adding a not before a verb, which changes the text meaning:</p>
<pre><code>I would prefer to not eat pizza tomorrow evening.
I didn’t eat pizza yesterday.
</code></pre>
<p>I have read about the use of word2vec and neural networks to do this, but I would like if anyone has already tried to do something similar.</p>
<p>Any advice would be greatly appreciated. Thanks</p>
","nlp"
"79682","Which NLP library has the most mature Chinese language models?","2020-08-02 22:25:35","","1","602","<nlp><named-entity-recognition>","<p>I am trying to do some NLP on Simplified Chinese texts (needing to extract sentence structure and to do named entity recognition).  I've used spaCy previously for English texts, but I see the <a href=""https://github.com/howl-anderson/Chinese_models_for_SpaCy/blob/master/README.en-US.md"" rel=""nofollow noreferrer"">notes on the Chinese models</a> suggest they are a work in progress, and the NER extraction accuracy has been poor for the examples I've tried.</p>
<p>Which NLP library has the most mature pre-built Chinese language models? Ideally Python based.</p>
","nlp"
"79669","How to use text as an input for a neural network - regression problem? How many likes/claps an article will get","2020-08-02 16:07:13","","1","414","<machine-learning><neural-network><deep-learning><nlp>","<p>I am trying to predict the number of likes an article or a post will get using a NN.</p>
<p>I have a dataframe with ~70,000 rows and 2 columns: &quot;text&quot; (predictor - strings of text) and &quot;likes&quot; (target - continuous int variable). I've been reading on the approaches that are taken in NLP problems, but I feel somewhat lost as to what the input for the NN should look like.</p>
<p>Here is what I did so far:</p>
<ol>
<li>Text cleaning: removing html tags, stop words, punctuation, etc...</li>
<li>Lower-casing the text column</li>
<li>Tokenization</li>
<li>Lemmatization</li>
<li>Stemming</li>
</ol>
<p>I've assigned the results to a new column , so now I have &quot;clean_text&quot; column with all the above applied to it. However, I'm not sure how to proceed.</p>
<p>In most NLP problems, I have noticed that people use word embeddings, but from what I have understood, it's a method used when attempting to predict the next word in a text. Learning word embeddings creates vectors for words that are similar to each other syntax-wise, and I fail to see how that can be used to derive the weight/impact of each word on the target variable in my case.</p>
<p>In addition, when I tried to generate a word embedding model using the Gensim library, it resulted in more than 50k words, which I think will make it too difficult or even impossible to onehot encode. Even then, I will have to one hot encode each row and then create a padding for all the rows to be of similar length to feed the NN model, but the length of each row in the new column I created &quot;clean_text&quot; varies significantly, so it will result in very big onehot encoded matrices that are kind of redundant.</p>
<p>Am I approaching this completely wrong? and what should I do?</p>
","nlp"
"79667","How can I compare the grammatical complexity between two texts using their sentences dependency length?","2020-08-02 15:43:42","79675","1","151","<machine-learning><statistics><nlp><hypothesis-testing>","<p>This is a continuation to the following <a href=""https://linguistics.stackexchange.com/questions/36836/how-to-determine-grammatical-complexity-using-quantitative-features/36838?noredirect=1#comment83757_36838"">thread</a>.</p>
<p>I have two texts, common English texts such as news articles and informative texts versus a technical textbook. I want to compare the grammatical complexity between those texts using their sentences dependency length in order to conclude whether they both have the same level of complexity or not. I was thinking about using the p-value as an evidence against the null hypothesis. Here is how the data would look like:</p>
<p><strong>Text 1</strong></p>
<pre><code>ID  Dependency Length   Sentence Length
0   13                  7
1   5                   3
2   20                  8
</code></pre>
<p><strong>Text 2</strong></p>
<pre><code>ID  Dependency Length   Sentence Length
0   8                   5
1   10                  7
2   14                  7
</code></pre>
<p>By the way, I am using python.</p>
","nlp"
"79648","create sequence of non dictionary words","2020-08-01 23:45:48","","1","33","<keras><scikit-learn><nlp><tokenization>","<p>I have a few word vectors-</p>
<pre><code>recvfrom,sendto,epoll_pwait,recvfrom,sendto,epoll_pwait 

getuid,recvfrom,writev,getuid,epoll_pwait,getuid
</code></pre>
<p>Now i want to tokenized them and then make them into sequences to feed into the model-</p>
<p>For a standard word vector I would do something like this-</p>
<pre><code>### Create sequence
vocabulary_size = 20000
tokenizer = Tokenizer(num_words= vocabulary_size)
tokenizer.fit_on_texts(df['text'])
sequences = tokenizer.texts_to_sequences(df['text'])
data = pad_sequences(sequences, maxlen=50)
</code></pre>
<p>But in my data I have non dictionary words and also I have some repeating words. How do I convert this data into sequences?</p>
","nlp"
"79612","Creating a valid dataset for obtaining results","2020-08-01 00:39:37","79625","1","56","<dataset><text-mining><nlp><text-classification>","<p>I have created a domain-specific dataset, lets say it is relating to python programming topic posts. I have taken data from various places specific to this topic to create positive examples in my dataset. For example, python related subreddits, stack exchange posts tagged with python, twitter posts hashtagged with python or python specific sites.</p>
<p>The data points taken from these places are considered positive data points and then I have retrieved data points from the same sources but relating to general topics, searched if they contain the word python in them and if they do discard them to create the negative examples in my dataset.</p>
<p>I have been told that I can use the training set from the dataset as is, but that I need to manually annotate the test set for the results to be valid, otherwise they would be biased. Is this correct? How would they be biased? To be clear the test set contains different entries to the training set.</p>
<p>There are close to 200,000 entries in the test set which makes manual annotation difficult. I have seen similar methods been used in papers I have previously read without mention of manual annotation. Is this technique valid or do I have to take some extra steps to ensure the validity of the test sets?</p>
","nlp"
"78556","For NLP, is GPT-3 better than RoBERTa?","2020-07-30 17:51:12","78562","0","2626","<nlp><bert><transformer><gpt>","<p>I am learning deep learning and I want to get into NLP. I have done LSTM, and now I am learning about vectorisation and transformers. Can you please tell me, which algorithm is more effective and accurate?</p>
","nlp"
"78483","Entity Recognition to extract question,options and diagrams from a question paper?","2020-07-29 10:17:31","","0","39","<python><deep-learning><nlp><named-entity-recognition>","<p>Here is a sample question paper. I want to extract questions, their options and diagrams for the corresponding question(if any). Thinking of doing entity recogntion for the same. But I am not sure if that is possible for diagrams? Can someone help me out.</p>
<p>I tried OCR techniques previously but they are not doing well.</p>
<p><a href=""https://i.sstatic.net/sQK05.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/sQK05.jpg"" alt=""enter image description here"" /></a></p>
","nlp"
"78431","Which deep learning network would be suitable for classifying this kind of text data?","2020-07-28 14:54:06","78434","2","102","<machine-learning><deep-learning><classification><nlp>","<p>I have some experience with images and have played around with image classification using CNN's but have limited knowledge when it comes to text data.</p>
<p>The input that I currently want to classify is written as:</p>
<pre><code>hjkhghkgfghjkhghkgfghfefdefdcdefghjkjh-hjhgfe
fdcd-dd-fdc-dad-ad-dfe-cde-dggf-ghd-gg-bcd
hjkhghkgfghjkhghkgfghfefdefdcdefghjkjh-gh-gfed
dh-hg-gf-gh-dh-hg-gf-gh-hkhg-kh-hg-gf-gh-hkhg-kh-hg-gf-ghh-hgfg-dfd-dc-fgf-gh
</code></pre>
<p>I have over 2000 rows of this data, that needs to be classified. I know that for regular text data RNN networks and LSTM cells have been known t be very effective. Using RNN+LSTM good results can be achieved by pre-processing the data using the usual approaches such as stemming, lemmitization, stop word filtering, tokenization etc. But the same cannot be applied to the text data I have.</p>
<p>Would RNN and LSTM still work on my data? If not which networks do you guys suggest I explore for such a task?</p>
","nlp"
"78412","Twitter POS and NER: What is state-of-the-art?","2020-07-28 10:43:46","","2","929","<python><nlp><named-entity-recognition><spacy>","<p>What is the current state-of-the-art for pos tagging and named entity recognition for twitter data? Are industrial-strength programs like <code>Spacy</code> and <code>SparkNLP</code> accurate for such texts? How about <code>FlairNLP</code> and Stanford's <code>CoreNLP</code> accuracy measures?</p>
","nlp"
"78390","Semantic network using word2vec","2020-07-27 23:19:54","","3","543","<python><neural-network><word2vec><nlp><semantic-similarity>","<p>I have thousands of headlines and I would like to build a semantic network using word2vec, specifically google news files.
My sentences look like</p>
<pre><code>Titles
Dogs are humans’ best friends
A dog died because of an accident
You can clean dogs’ paws using natural products.
A cat was found in the kitchen
</code></pre>
<p>And so on.</p>
<p>What I would like to do is finding some specific pattern within this data, e.g. similarity in topics on dogs and cats, using semantic networks.
Could you give me some advice on how I can do it?</p>
<p>Code:</p>
<pre><code>import pandas as pd
import gensim
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from sklearn.manifold import TSNE

main_data.Titles = np.where(main_data.Titles.isnull(),'NA', main_data.Titles)

article_titles = main_data['Titles']

titles_list = [title for title in article_titles]

big_title_string = ' '.join(titles_list)

tokens = word_tokenize(big_title_string)

words = [word.lower() for word in tokens if word.isalpha()]

stop_words = set(stopwords.words('english'))

words = [word for word in words if not a word in stop_words]

model = gensim.models.KeyedVectors.load_word2vec_format('path/GoogleNews-vectors-negative300.bin', binary = True) 

model.vector_size

vector_list = [model[word] for word in words if word in model.vocab]

words_filtered = [word for word in words if the word in `model.vocab`]

word_vec_zip = zip(words_filtered, vector_list)

word_vec_dict = dict(word_vec_zip)
df = pd.DataFrame.from_dict(word_vec_dict, orient='index')

tsne = TSNE(n_components = 2, init = 'random', random_state = 10, perplexity = 100)

tsne_df = tsne.fit_transform(df[:400])

sns.set()
fig, ax = plt.subplots(figsize = (11.7, 8.27))
sns.scatterplot(tsne_df[:, 0], tsne_df[:, 1], alpha = 0.5)

from adjustText import adjust_text
texts = []
words_to_plot = list(np.arange(0, 400, 10))

for word in words_to_plot:
    texts.append(plt.text(tsne_df[word, 0], tsne_df[word, 1], df.index[word], fontsize = 14))
    
adjust_text(texts, force_points = 0.4, force_text = 0.4, 
            expand_points = (2,1), expand_text = (1,2),
            arrowprops = dict(arrowstyle = &quot;-&quot;, color = 'black', lw = 0.5))

plt.show()
</code></pre>
<p>However, I cannot understand how to interpret the results. I think they are wrong and probably this is not a good approach for building a semantic network. maybe I have been missing something...For instance, this code is still keeping stopwords after the part of</p>
<pre><code>words = [word for word in words if not a word in stop_words]
</code></pre>
<p>This is an example of output difficult to read and explain (at least, for me):</p>
<p><a href=""https://i.sstatic.net/ufRXY.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ufRXY.png"" alt=""enter image description here"" /></a></p>
<p>I would greatly appreciate it if you could give me some tips and advice on how to perform a semantic network that can show semantic similarity within titles.</p>
","nlp"
"78380","Using word embeddings with additional features","2020-07-27 18:28:47","78389","0","166","<machine-learning><scikit-learn><nlp><feature-scaling>","<p>I have the set of queries for classification task using Gradient Boosting Classifier of scikit learn. I want to enrich the model by feeding additional features along with GloVe. How should I approach scaling in this case? GloVe is already well scaled, however, features are not.</p>
<p>I have tried StandardScaler, but this reduced the performance in comparison with just using GloVe. The problem maybe with the feature itself, however, I need your opinion on scaling starategies in case of glove and dummy variable.</p>
","nlp"
"78371","Understanding Transfer Learning of Word Embeddings","2020-07-27 12:54:28","","1","403","<nlp><word-embeddings><word2vec><transfer-learning><named-entity-recognition>","<p>I can't quite visualize how transfer learning of pre-trained word embeddings is useful in an NLP task( say <strong>named entity recognition</strong> ) . I'm studying Andrew NG's Sequence Models course and he seems to say if the training set for the target task is very less, then transfer learning of word embeddings is helpful in a way that unknown words in the training set can be handled in the application .</p>
<p>Let's consider the task of Named Entity recognition ,</p>
<p>My question is , what does the very small training set for the target task contain ? Are they word embeddings or sentences labeled with entities ?</p>
<p>Does he seem to suggest that if the training set is of just  labeled sentences whose words have embeddings in the pre-trained model , then words which are not present in the training set but are closer to those already in the training set also get captured effectively in the application ?</p>
<p>Eg : Consider 'Orange' is in training set . But , 'Apple' is not .</p>
<p>So , in the sentences , ' <strong>I like Orange Juice</strong> ' and ' <strong>I like Apple Juice</strong> ' , Apple gets recognized as a fruit , even though it's not in the training set since it is closer to Orange .</p>
<p>Am I right in my assumption ? Or can someone please correct and explain to me if I am not ?</p>
","nlp"
"78322","Image segmentation network to extract questions from an image of a test paper?","2020-07-26 10:54:37","","1","551","<python><nlp><computer-vision><ocr><image-segmentation>","<p>This is the sample document -&gt;
<a href=""https://i.sstatic.net/kvswZ.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/kvswZ.jpg"" alt=""enter image description here"" /></a></p>
<p>I want to extract questions along with the options. There are other question papers as which have questions with diagrams in them. I want to be able to extract them as well.</p>
<p>End goal is to store them in a question bank.</p>
<p>I am planning to run an image segmentation algorithm to extract the questions along with their options.
Will that be feasible?
Suggestions are more than welcome.</p>
","nlp"
"78306","""Change the features of a CNN into a grid to fed into RNN Encoder?"" What is meant by that?","2020-07-25 19:17:12","78333","2","145","<machine-learning><neural-network><deep-learning><nlp><cnn>","<p>So in the paper for OCR pr LaTex formula extraction from image <strong><a href=""https://arxiv.org/pdf/1609.04938v1.pdf"" rel=""nofollow noreferrer"">What You Get Is What You See: A Visual Markup Decompiler</a></strong>, they pass the features of the CNN into RNN Encoder. But there is problem that rather than passing the features directly, they have proposed a solution to change it into the grid.</p>
<p><strong>Extract the features from the CNN and then arrange those extracted features in a grid to pass into an RNN encoder.</strong> This is the exact language they have used.</p>
<p>What is meant by that? Theoratically speaking, if I have an <code>CNN</code> without any Dense/Fully Connected layer and produces an output of <code>[batch,m*n*C]</code>, then how can I change it in the form of a <code>grid</code>??  Please see the picture below. So after getting the output from the <code>CNN</code>, they have chnged it somehow before passing it to <code>RNN</code>. What is the method that one can use to get this transformation?</p>
<p><a href=""https://i.sstatic.net/nX6jy.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/nX6jy.png"" alt=""enter image description here"" /></a></p>
<p>So if I have to pass something to <code>keras.layers.RNN()(that_desired_grid_format)</code>, what should be this grid format and how can I change it?</p>
","nlp"
"78161","How to make use of POS tags as useful features for a NaiveBayesClassifier for sentiment analysis?","2020-07-23 03:48:20","","0","2008","<machine-learning><nlp><naive-bayes-classifier><sentiment-analysis>","<p>I'm doing sentiment analysis on a twitter dataset (<a href=""https://datahack.analyticsvidhya.com/contest/practice-problem-twitter-sentiment-analysis/#ProblemStatement"" rel=""nofollow noreferrer"">problem link</a>). I have extracted the POS tags from the tweets and created tfidf vectors from the POS tags and used them as a feature (got accuracy of 65%). But I think, we can achieve a lot more with POS tags since they help to distinguish how a word is being used within the scope of a phrase. The model I'm training is MultnomialNB().</p>
<p>The problem I'm trying to solve is to find the sentiments of tweets like positive, negative or neutral.</p>
<h3>Structure of datset: <a href=""https://i.sstatic.net/84nkH.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/84nkH.png"" alt=""enter image description here"" /></a></h3>
<h3>Created pos tags: <a href=""https://i.sstatic.net/9pv5E.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/9pv5E.png"" alt=""enter image description here"" /></a></h3>
<p>I created tfidf vectors from the tweet and gave the inputs to my model:</p>
<pre class=""lang-py prettyprint-override""><code>tfidf_vectorizer1 = TfidfVectorizer(
    max_features=5000, min_df=2, max_df=0.9, ngram_range=(1,2))
train_pos = tfidf_vectorizer1.fit_transform(train_data['pos'])
test_pos = tfidf_vectorizer1.transform(test_data['pos'])

clf = MultinomialNB(alpha=0.1).fit(train_pos, train_labels)
predicted = clf.predict(test_pos)
</code></pre>
<p>With the above code I got 65% accuracy. Rather than creating TF-IDF vectors of POS and using them as modal inputs. I'm wondering is there any other way that we can use POS tags to increase the accuracy of the model?</p>
","nlp"
"78124","Product classification according to description","2020-07-22 11:20:07","78171","0","126","<nlp>","<p>I have some products, along with their description. I wish to assign USPSC code to each product. I have a really basic doubt here. What exactly is my test file and training file? Eg. Should the training file be entries of product description along with <strong>manually</strong> entered codes assigned to each product? And the test file only product descriptions?</p>
","nlp"
"78069","Any useful tips on transfer learning for a text classification task","2020-07-21 08:07:40","78153","2","211","<classification><nlp><transfer-learning><text-classification>","<p>I am doing a supervised binary text classification task.</p>
<p>I want to classify the texts from site A, site B, and site C.</p>
<p>The in-domain performance looks OK for texts of each site. (92%-94% accuracy).</p>
<p>However, if I applied the model trained on texts of one site directly onto texts of another site(without fine-tuning), the performance downgrades a lot. (7%-16% downgrade for accuracy).</p>
<p>Approaches I already tried:</p>
<ol>
<li><p>Doc2vec embedding(trained on texts from one site) + logistic regression.</p>
</li>
<li><p>Bert embedding + logistic regression. (Using bert-as-a-service to generate the embeddings based on google pre-trained bert models).</p>
</li>
<li><p>TF-IDF + logistic regression.</p>
</li>
<li><p>Pre-trained Word2vec embedding(average word embedding for text) + logistic regression.</p>
</li>
</ol>
<p>All of those approaches don't work very well.</p>
<p>I knew that the performance downgrade is unavoidable, but I would like to get a maybe 3% - 5% downgrade.</p>
","nlp"
"78055","word2vec: usefulness of context vectors in classification","2020-07-21 00:25:21","","5","1129","<word-embeddings><word2vec><nlp><context-vector>","<p>I've been working on a NN-based classification system that accepts document vectors as input. I can't really talk about what I'm specifically training the neural net on, so i'm hoping for a more general answer.</p>
<p>Up to now, the word vectors I've been using (specifically, the gloVe function from the text2vec package for R) have been <strong>target vectors</strong>. Up to now I wasn't aware that the word2vec training produced <strong>context vectors</strong>, and quite frankly I'm not sure what exactly they represent. (It's not part of the main question, but if anybody could point me to resources on what context vectors are for and what they <em>do</em>, that would be greatly appreciated)</p>
<p>My question is, how useful are these context word vectors in any kind of classification scheme? Am I missing out on useful information to feed into the neuralnet?</p>
<p>How would, qualitatively speaking, these four schemes fare?</p>
<ol>
<li>Target word vectors only.</li>
<li>Context word vectors only.</li>
<li>Averaged target and context vectors.</li>
<li>Concatenated vectors (i.e. a 100-vector word2vec model ends up with a length of 200)</li>
</ol>
","nlp"
"77916","Training with less data","2020-07-18 09:34:58","","1","163","<deep-learning><nlp><data-augmentation>","<p>Most problem with machine learning projects I have faced is the lack of data. The samples available are enough to disqualify rule based approach but not enough for a neural network to train.</p>
<p>For example, to train a neural network (or even fine tune a pretrained model) on a new entity in an NER system takes a thousands of different records. And the requirement of these thousands of records is to have enough variations to avoid overfitting.</p>
<p>Generally as human we can detect patterns by carefully observing the data, however it becomes humanly not possible to detect all patterns in inputs, and that is where deep learning comes into play of automatically detecting the patterns to make a hypothesis.</p>
<p>Now my question is, <strong>what are possible ways, using which limited data can be used to train a neural network with limited data</strong>. Let me add some inputs from my side, which I believe are not sufficient:</p>
<ul>
<li>Data Augmentation : for images rotating, scaling and skewing. For textual data, repeating text with some masking and embedding/synonym replacement.</li>
<li>what else?</li>
</ul>
","nlp"
"77906","How to create a table to display relative frequencies of selected words (eg. with, can, will) from any text corpus in nltk package in python","2020-07-18 04:58:03","","1","326","<machine-learning><python><nlp><machine-learning-model><nltk>","<p>Interested words are ['with', 'can', 'will']<br>
<code>gutenberg</code> corpus is the corpus we would like to search on.</p>
<p>Expected output is</p>
<pre><code>                           can  could    may  might should   will  would 
        austen-emma.txt    270    825    213    322    366    559    815 
  austen-persuasion.txt    100    444     87    166    185    162    351 
       austen-sense.txt    206    568    169    215    228    354    507 
          bible-kjv.txt    213    165   1024    475    768   3807    443 
        blake-poems.txt     20      3      5      2      6      3      3 
     bryant-stories.txt     75    154     18     23     38    144    110 
burgess-busterbrown.txt     23     56      3     17     13     19     46 
      carroll-alice.txt     57     73     11     28     27     24     70 
    chesterton-ball.txt    131    117     90     69     75    198    139 
   chesterton-brown.txt    126    170     47     71     56    111    132 
chesterton-thursday.txt    117    148     56     71     54    109    116 
  edgeworth-parents.txt    340    420    160    127    271    517    503 
 melville-moby_dick.txt    220    215    230    183    181    379    421 
    milton-paradise.txt    107     62    116     98     55    161     49 
 shakespeare-caesar.txt     16     18     35     12     38    129     40 
 shakespeare-hamlet.txt     33     26     56     28     52    131     60 
shakespeare-macbeth.txt     21     15     30      5     41     62     42 
     whitman-leaves.txt     88     49     85     26     42    261     85 
</code></pre>
<p>Reference: <a href=""https://www.nltk.org/book/ch02.html"" rel=""nofollow noreferrer"">https://www.nltk.org/book/ch02.html</a></p>
","nlp"
"77873","How best to embed large and noisy documents","2020-07-17 13:21:54","","1","1411","<machine-learning><python><nlp><word-embeddings><tfidf>","<p>I have a large corpus of documents (web pages) collected from various sites of around 10k-30k chars each, I am processing them to extract relevant text as much as possible, but they are never perfect.</p>
<p>Right now I creating a doc for each page, processing it with <code>TFIDF</code> and then creating a dense feature vector using <code>UMAP</code>.</p>
<p>My final goal is to really pick out the differences in the articles, for similarity analysis, clustering and classification - however at this stage my goal is to generate the best possible embeddings.</p>
<p>For this type of data what is the best method to create document embeddings?</p>
<p>Also is it possible (is yes how) to embed various parts of the page; title, description, tags separately and them maybe combine this into a final vector?</p>
","nlp"
"77811","Classification of text articles found in different PDFs","2020-07-16 13:52:09","","0","19","<classification><nlp><supervised-learning><text>","<p>I am trying to figure out the approach to connect articles present in different pages or PDFs. The historical data that I have are:</p>
<ul>
<li>Text Articles (broken down in headlines, body text, body headline and media caption and the PDFs names that they are found). I have gotten this in a XML format and I converted it in a Json file that looks like:</li>
</ul>
<pre>
    {
         ""Headline"": ""This is a Headline"",
         ""Byline"": [
              ""This is another BYLINE""
         ],
         ""BodyHeadline"": [
              ""and the the body headline""
         ],
         ""BodyText"": [
              ""This should be a large text paragraph"",
              ""another sentence in the body text"",
              ""the last sentence in body text""

         ],
         ""MediaCaption"": [""If there is media caption""],
         ""PDF"": [
              ""X11#0001.pdf"",
              ""X11#0002.pdf"",
              ""X11#0003.pdf""
         ]
    }
</pre>
<p>As you see this article is included in three different PDFS. What I would like to do is to build  a model that classifies/predicts this article in different PDFs. i.e., to which PDFs is this article connected to.</p>
<p>The PDFs are mostly readable and I am using PyMuPDF (aka &quot;fitz&quot;) for getting the JSON output. This provides information regarding blocks of texts in PDFs such as font, size, bbox location and text. A sample of one PDF would look like this (I just show 2 blocks but it could contain more):</p>
<pre>
{
     ""PDF"": ""X11#0001.pdf"",
     ""Blocks"": [
          {
               ""block_bbox"": [
                    -0.5304872064644457,
                    0.9683844633175651,
                    0.5304879430798047,
                    0.9793545233723604
               ],
               ""block_idx"": 0,
               ""text"": [
                    ""a text sentence"",
                    ""other sentence""
               ],
               ""font"": [
                    ""TEOSOK+ScalaSansPro"",
                    ""TEOSOK+ScalaSansPro""
               ],
               ""size"": [
                    7.0,
                    7.0
               ],
               ""line_bbox"": [
                    [
                         -0.5304872064644457,
                         0.9683844633175651,
                         -0.4695114902929961,
                         0.9793545233723604
                    ],
                    [
                         0.4695122269083551,
                         0.9683844633175651,
                         0.5304879430798047,
                         0.9793545233723604
                    ]
               ],
              {
               ""block_bbox"": [
                    0.0789473460541895,
                    0.7554111799458375,
                    0.41483171058858515,
                    0.9036320824132956
               ],
               ""block_idx"": 1,
         
               ""text"": [
                    ""This is a sentence "",
                    ""another sentence"",
                    ""sentence3"",
                    ""another sentence again"",
                    ""yes, another sentence"",
                    ""continue..."",
                    ""continue..."",
                    ""continue..."",
                    ""another text"",
                    ""almost there "",
                    ""final sentence""
               ],
               ""font"": [
                    ""TEOSOK+ScalaSansPro-Bold"",
                    ""TEOSOK+ScalaSansPro-Bold"",
                    ""TEOSOK+ScalaSansPro"",
                    ""TEOSOK+ScalaSansPro"",
                    ""TEOSOK+ScalaSansPro"",
                    ""TEOSOK+ScalaSansPro"",
                    ""TEOSOK+ScalaSansPro"",
                    ""TEOSOK+ScalaSansPro"",
                    ""TEOSOK+ScalaSansPro"",
                    ""TEOSOK+ScalaSansPro"",
                    ""TEOSOK+ScalaSansPro""
               ],
               ""size"": [
                    8.0,
                    8.0,
                    8.0,
                    8.0,
                    8.0,
                    8.0,
                    8.0,
                    8.0,
                    8.0,
                    8.0,
                    8.0
               ],
               ""line_bbox"": [
                    [
                         0.0789473460541895,
                         0.7554111799458375,
                         0.08266079406682393,
                         0.7680569142930777
                    ],
                    [
                         0.0790216167143238,
                         0.7689795537757371,
                         0.16631733627647055,
                         0.7816252881229773
                    ],
                    [
                         0.0789473460541895,
                         0.7825479276056366,
                         0.3965036438661926,
                         0.7950850917740988
                    ],
                    [
                         0.0789473460541895,
                         0.7961163014355362,
                         0.41483171058858515,
                         0.8086534656039984
                    ],
                    [
                         0.0789473460541895,
                         0.8096846752654359,
                         0.38098440483292206,
                         0.822221839433898
                    ],
                    [
                         0.0789473460541895,
                         0.8232530490953355,
                         0.3889415222598362,
                         0.8357902132637977
                    ],
                    [
                         0.0789473460541895,
                         0.8368214229252351,
                         0.3813557156365535,
                         0.8493585870936973
                    ],
                    [
                         0.0789473460541895,
                         0.8503897967551347,
                         0.41312355623389646,
                         0.8629269609235968
                    ],
                    [
                         0.0789473460541895,
                         0.8639581705850342,
                         0.38012281984518836,
                         0.8764953347534964
                    ],
                    [
                         0.0789473460541895,
                         0.8775265444149338,
                         0.37606483416408437,
                         0.890063708583396
                    ],
                    [
                         0.0789473460541895,
                         0.8910949182448334,
                         0.3819320604922132,
                         0.9036320824132956
                    ]
               ],}

</pre>
<p>Any ideas about how can I approach this problem and in which manner can I structure the data?</p>
","nlp"
"77788","How to do embedding for nested dictionary with varying size?","2020-07-16 04:39:39","","1","99","<word-embeddings><nlp><embeddings>","<p>I'm working on an RL task in which the agent needs have some observation. Instead using images, I want to use available information of the environment as the observation. The information regarding the environment is about the info of the objects that exist in the environment and is stored as a list of nested dictionary:</p>
<pre><code>[
{'name': 'TennisRacket_de7db8d2',
  'position': {'x': -1.667, 'y': 0.0, 'z': 0.818},
  'rotation': {'x': 90.0, 'y': 29.999939, 'z': 0.0},
  'cameraHorizon': 0.0,
  'visible': False,
  'receptacle': False,
  'toggleable': False,
  'isToggled': False,
...},
{'name': 'Laptop_f5306a34',
  'position': {'x': -1.39652, 'y': 0.8289251, 'z': -1.89688826},
  'rotation': {'x': 0.0, 'y': 19.242403, 'z': 0.0},
  'cameraHorizon': 0.0,
  'visible': False,
...}
...]

</code></pre>
<p>So the task now is to do an embedding on this data. I have thought about putting encoding(eg. utf-8) for non-numerical characters into an vector, using averaged word2vec and doc2vec but they all seem not suitable for the following reasons:</p>
<ol>
<li><p>for directly using encoding, the dictionary might contain different numbers of observed objects so the length of the dictionary will vary, but the observation requires a vector of the constant size and shape.</p>
</li>
<li><p>for averaged word2vec and doc2vec, it is highly likely the case that some of the objects has much more importance, so I think it is better to be able to differentiate them individually and I thing both averaged word2vec and doc2vec will aggregate everything together and hence lose the individual object's info.</p>
</li>
</ol>
<p>Can someone suggest a embedding method that both preserves of the shape/size of the embedded vector regardless of the size of the input while keeping the information of individual objects? Thanks for replying.</p>
","nlp"
"77761","Text classification with Word2Vec on a larger corpus","2020-07-15 14:25:43","77767","1","505","<machine-learning><nlp><word2vec><text-classification><corpus>","<p>I am working on a small project and I would like to use the word2vec technique as a text representation  method. I need to classify patents but I have only a few of them labelled and to increase the performance of my ML model, I would like to increase the corpus/vocabulary of my model by using a large amount of patents. The question is, once I have train my word embedding feature, how to use this larger corpus with my training data - my labelled data?</p>
<p>My <strong>data set</strong> is composed by <em>2000 patents</em> which are labelled.</p>
<p>The patents used to train my <strong>word embedding corpus</strong> are <em>3 millions</em> (some of my 2000 labelled patents are already included in this larger corpus) which I trained using Gensim.</p>
<p>Do you have any suggestions on how to do it?</p>
<p>Thank you very much in advance.</p>
<p>Rob</p>
","nlp"
"77665","Passing Dependency/Constituency trees to a Neural Machine Translator","2020-07-13 19:30:36","","0","44","<neural-network><nlp><pytorch><machine-translation>","<p>I am working on a project on Neural Machine Translation in the English-Irish domain. I am not an  expert and have researched entirely on my own for a technology exhibition so apologies if my question is simple.</p>
<p>I am trying to parse all of my English corpus to constituency trees. Of course, the format of a sentence when using the Stanford Parser is something like:</p>
<blockquote>
<p>(ROOT (S (NP (VBG cohabiting) (NNS partners)) (VP (MD can) (VP (VB make) (NP (NP (NNS wills)) (SBAR (WHNP (WDT that)) (S (VP (VBP favour) (NP (DT each) (JJ other)))))))) (. .)))</p>
</blockquote>
<p>Of course, when dealing with simple sequences, each word is used, it's not symbolism like <code>NP</code> or <code>NNS</code> in constituency trees.
Right now, I'm working with PyTorch and Fairseq to produce all my models and have gotten a working seq2seq model. But, can I simply just pass my English input like shown above to a model and expect it to train? Do I need to make a new model from scratch that can deal with tree structures? I've tried very hard to research this, reading papers, books and playing around with tools but since I'm not in a class for this and since it's not really documented, I'm finding it hard to find this on my own.</p>
<p>Any help would be greatly appreciated</p>
","nlp"
"77550","First two principal components explain 100% variance of data set with 300 features","2020-07-11 08:54:03","77568","2","191","<machine-learning><python><nlp><clustering><pca>","<p>I am trying to do some analysis on my data set with PCA so I can effectively cluster it with kmeans.</p>
<p>My preprocessed data is tokenized, filtered (stopwords, punctuation, etc.), POS tagged, and lemmatized</p>
<p>I create a data set of about 1.2 million tweet vectors (300 features each) by taking the averaged word vectors multiplied by their tfidf scores, like so:</p>
<pre class=""lang-py prettyprint-override""><code># trained with same corpus as tfidf
# size=300, epochs=5, and min_count=10
tweet_w2v = Word2Vec.load('./models/tweet2vec_lemmatized_trained.model')

tweet_tfidf = TfidfVectorizer()
with open('./corpus/ttokens_doc_lemmatized.txt', 'r') as infile:
    tweet_tfidf.fit(infile)

tweet_tfidf_dict = dict(zip(tweet_tfidf.get_feature_names(), list(tweet_tfidf.idf_)))

tfidf_tweet_vectors = []

with open('./corpus/ttokens_doc_lemmatized.txt', 'r') as infile:
    for line in infile:
        word_vecs = []
        
        words = line.replace('\n', '').split(' ')
        
        if len(words) == 0:
            continue
            
        for word in words:
            try:
                word_vec = tweet_w2v.wv[word]
                word_weight = tweet_tfidf_dict[word]
                word_vecs.append(word_vec * word_weight)
            except KeyError:
                continue
                
        if len(word_vecs) != 0:
            tweet_vec = np.average(np.array(word_vecs), axis=0)
        else:
            continue
        tfidf_tweet_vectors.append(tweet_vec)
</code></pre>
<p>I also tried the above code with just average tweet vectors (no tfidf), and my problem still ended up happening.</p>
<p>I am starting to think that maybe my data set just isn't big enough or I am not training my word2vec model properly? I have somewhere around 100 million tweets I can use, but after filtering out retweets and only getting english language, it comes to around 1.3 million.</p>
<p>I'm not sure what's happening and what step I should take next. Any explanation is appreciated.</p>
<pre class=""lang-py prettyprint-override""><code># Load in the data
df = pd.read_csv('./models/tfidf_weighted_tweet_vectors.csv')
df.drop(df.columns[0], axis=1, inplace=True)

# Standardize the data to have a mean of ~0 and a variance of 1
X_std = StandardScaler().fit_transform(df)

# Create a PCA instance: pca
pca = PCA(n_components=20)
principalComponents = pca.fit_transform(X_std)

# Plot the explained variances
features = range(pca.n_components_)
plt.bar(features, pca.explained_variance_ratio_, color='black')
plt.xlabel('PCA features')
plt.ylabel('variance %')
plt.xticks(features)
</code></pre>
<p><a href=""https://i.sstatic.net/9JLO4.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/9JLO4.png"" alt=""PCA feature variance"" /></a></p>
","nlp"
"77541","How is GPT able to handle large vocabularies?","2020-07-11 03:33:05","77551","10","2652","<deep-learning><nlp><gpt>","<p>From what I understand, GPT and GPT-2 are trained to predict the <span class=""math-container"">$N^{th}$</span> word in a sentence given the previous <span class=""math-container"">$N-1$</span> words. When the vocabulary size is very large (100k+ words) how is it able to generate any meaningful prediction? Shouldn't it become extremely difficult to correctly label the next word given that there are 100k possible labels to choose from? Even a large-scale classification problem like ImageNet has only 1k classes to choose from.</p>
","nlp"
"77529","Effect of discounting parameter on Language Model Perplexity","2020-07-10 17:45:15","","1","70","<nlp><language-model>","<p>The general formula for absolute discounting for calculating language model probabilities subtracts a discounting parameter d from the count of the ngram before calculating the probabilities.</p>
<p><a href=""https://i.sstatic.net/B2pzz.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/B2pzz.png"" alt=""LM with absolute discounting"" /></a></p>
<p>The perplexity of the model first decreases as d increases, and then later starts increasing again.</p>
<p><a href=""https://i.sstatic.net/x3Zxr.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/x3Zxr.png"" alt=""Perplexity vs Discounting Parameter"" /></a></p>
<p>What is the reason behind this change? Why does the perplexity decrease first and then start increasing again?</p>
","nlp"
"77471","Resources for text classification algorithms","2020-07-10 06:26:48","","1","56","<python><nlp><r>","<p>I'm mining raw Facebook comments (irrelevant) and i am looking for an algorithm that can classify their context as negative/positive/neutral. So you can think of the output in the form of two columns. First column the comment (exists), and second column its classification. If possible i'd like for the algorithm to be able to process the sentence as is (e.g. not having to remove stop words etc). Can someone recommend any resources that i can refer to? I have no problem doing this in either Python or R.
Thanks</p>
","nlp"
"77404","How to match features in new records for NLP BOW","2020-07-08 19:21:53","","0","19","<nlp><feature-selection><feature-engineering>","<p>I have a dataset that has 100,000 records</p>
<p>data in this dataset are 2 columns
1- Text
2- Class</p>
<p>When I apply BOW of my model I get big list of features</p>
<p>That is fine, I managed to work with them</p>
<p>my problem is after building the model and deploying.</p>
<p>now if a new text came with new words then the model wont work as it wokds in same feature structure</p>
<p>Example
&quot;This is a test, test is important&quot; , Red
&quot;Adam pass a test&quot;, Green</p>
<p>so my final dataset is</p>
<pre><code>This is a test important Adam pass class
 1    2 1 2    1          0    0   Red
 0    0 1 1    0          1    1   Green
</code></pre>
<p>once model created and got this text</p>
<p>&quot;test and exam are similar&quot;, Yellow</p>
<p>in this case the set of features has new features which are</p>
<p>and exam are similar</p>
<p>the model will break coz these features never included in the training model</p>
<p>I wonder how to resolve this issue?</p>
","nlp"
"77341","Literature on selecting specific dimensions in a word embedding vector","2020-07-08 03:23:21","","1","131","<nlp><feature-selection><word-embeddings><word2vec>","<p>I am aware that the different dimensions in the word embedding represents different information and algebraic operations can be performed between two embeddings for example.</p>
<p>Can anyone point me to literature on selecting specific dimensions from a word embedding vector. I don't mean dimensionality reduction, but papers following the theory that all those dimensions are not important for all the tasks and that specific ones will be more important for a specific task. For example: Sentiment Analysis would benefit from dimension 5th to 55th and 250th to 300th instead of using the whole 300 dimensions.</p>
<p>Let me know if this theory isn't true.</p>
","nlp"
"77126","Unhashable type 'list' while looping through dataframe in Python","2020-07-04 13:37:23","","0","9485","<python><nlp><pandas><dataframe><python-3.x>","<p>I have the following dataframe <code>comments</code>. I have segregated a list of users based on certain conditions. I want to get the count of words based on those users which are named as <code>gold_users</code>. But I am getting an error in my code <code>TypeError: unhashable type: 'list'</code>. Please help me fix this.</p>
<p><strong>DataFrame(comments)</strong></p>
<pre><code>       Id|                    Text                             |    UserId  
        6|  [2006, course, allen, knutsons, 2001, course, ...  |    3   
        8|  [also, theo, johnsonfreyd, note, mark, haimans...  |    1
</code></pre>
<p><strong>Code</strong></p>
<pre><code>for index,rows in comments.iterrows():
  gold_comments = rows[comments.Text.loc[comments.UserId.isin(gold_users)]]
  Counter(gold_comments)
</code></pre>
<p><strong>Expected Output</strong></p>
<pre><code>#Top 10 Words that appear the most in the comments made by gold users with their count.
 [['scholar',20],['school',18],['bus',15],['class',14],['teacher',14],['bell',13],['time',12],['books',11],['bag',9],'student',7]]
</code></pre>
","nlp"
"77113","How to work with different Encoding for Foreign Languages","2020-07-04 07:30:05","77123","3","162","<nlp><word-embeddings><encoding><gensim>","<p>I've got a Word Embedding File called <code>model.txt</code>. This contains 100 Dimensional vectors for over a million French words. These words contain accented characters such as <em>é, â, î or ô</em>.</p>
<p>Let me explain my problem with the following example:
Consider these two words and their respective vectors, both of which are taken from <code>model.txt</code>:</p>
<pre><code>etait -0.100460 -0.127720 ... 

était 0.094601 -0.266495 ...

</code></pre>
<p>Both words signify the same meaning but the former is without the accents while the later has accents.</p>
<p>Now I'm trying to load this word embedding using the <code>gensim.models.KeyedVectors</code> in the following way:</p>
<pre><code>model = KeyedVectors.load_word2vec_format(open(model_location, 'r',
                                              encoding='utf8'),
                                          binary=False)
word_vectors = model.wv
</code></pre>
<p>To which I get the following error:</p>
<pre><code>---------------------------------------------------------------------------
UnicodeDecodeError                        Traceback (most recent call last)
&lt;ipython-input-82-e17c33c552da&gt; in &lt;module&gt;
     10 model = KeyedVectors.load_word2vec_format(open(model_location, 'r',
     11                                               encoding='utf8'),
---&gt; 12                                           binary=False)
     13 
     14 word_vectors = model.wv

D:\Anaconda\lib\site-packages\gensim\models\keyedvectors.py in load_word2vec_format(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)
   1547         return _load_word2vec_format(
   1548             cls, fname, fvocab=fvocab, binary=binary, encoding=encoding, unicode_errors=unicode_errors,
-&gt; 1549             limit=limit, datatype=datatype)
   1550 
   1551     @classmethod

D:\Anaconda\lib\site-packages\gensim\models\utils_any2vec.py in _load_word2vec_format(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, binary_chunk_size)
    286                 vocab_size, vector_size, datatype, unicode_errors, binary_chunk_size)
    287         else:
--&gt; 288             _word2vec_read_text(fin, result, counts, vocab_size, vector_size, datatype, unicode_errors, encoding)
    289     if result.vectors.shape[0] != len(result.vocab):
    290         logger.info(

D:\Anaconda\lib\site-packages\gensim\models\utils_any2vec.py in _word2vec_read_text(fin, result, counts, vocab_size, vector_size, datatype, unicode_errors, encoding)
    213 def _word2vec_read_text(fin, result, counts, vocab_size, vector_size, datatype, unicode_errors, encoding):
    214     for line_no in range(vocab_size):
--&gt; 215         line = fin.readline()
    216         if line == b'':
    217             raise EOFError(&quot;unexpected end of input; is count incorrect or file otherwise damaged?&quot;)

D:\Anaconda\lib\codecs.py in decode(self, input, final)
    320         # decode input (taking the buffer into account)
    321         data = self.buffer + input
--&gt; 322         (result, consumed) = self._buffer_decode(data, self.errors, final)
    323         # keep undecoded input until the next call
    324         self.buffer = data[consumed:]

UnicodeDecodeError: 'utf-8' codec can't decode bytes in position 7110-7111: invalid continuation byte
</code></pre>
<p>which I thought made sense if my file was encoded in a different format. However, using git I tried checking the encoding of the file using <code>file *</code> and got:</p>
<pre><code>model.txt: UTF-8 Unicode text, with very long lines
</code></pre>
<p>Now, if I try to write the above code and have the encoding set to <code>latin1</code>, there isn't any problem to load this document but at the cost of not being able to access any of the words which contains an accent. Essentially throwing an out-of-vocab error upon executing:
<code>word_vectors.word_vec('était')</code></p>
<p>How am I supposed to approach the problem? I've also got the <code>.bin</code> file of the model, should I try to use that to load my words and their corresponding vectors?</p>
","nlp"
"77080","KeyError: Selecting text from a dataframe based on values of another dataframe","2020-07-03 14:20:35","","2","206","<python><pandas><nlp><dataframe>","<p>I have the following two dataframes <code>badges</code> and <code>comments</code>. I have created a list of 'gold users' from <code>badges</code> dataframe whose <code>Class=1</code>.</p>
<p>Here <code>Name</code> means the 'Name of Badge' and <code>Class</code> means the level of Badge (1=Gold, 2=Silver, 3=Bronze).</p>
<p>I have already done the text preprocessing on <code>comments['Text']</code>and now want to find the count of top 10 words for gold users from <code>comments['Text']</code>.</p>
<p>I tried the given code but am getting error:</p>
<pre><code>&quot;KeyError: &quot;None of [Index(['1532', '290', '1946', '1459', '6094', '766', '10446', '3106', '1',\n       '1587',\n       ...\n       '35760', '45979', '113061', '35306', '104330', '40739', '4181', '58888',\n       '2833', '58158'],\n      dtype='object', length=1708)] are in the [index]&quot;. Please provide me a way to fix this.
</code></pre>
<p><strong>Dataframe 1 (badges)</strong></p>
<pre><code>   Id | UserId |  Name          |        Date              |Class | TagBased
   2  | 23     | Autobiographer | 2016-01-12T18:44:49.267  |   3  | False
   3  | 22     | Autobiographer | 2016-01-12T18:44:49.267  |   3  | False
   4  | 21     | Autobiographer | 2016-01-12T18:44:49.267  |   3  | False
   5  | 20     | Autobiographer | 2016-01-12T18:44:49.267  |   3  | False
   6  | 19     | Autobiographer | 2016-01-12T18:44:49.267  |   3  | False
</code></pre>
<p><strong>Dataframe 2 (comments)</strong></p>
<pre><code>   Id|                    Text                             |    UserId  
    6|  [2006, course, allen, knutsons, 2001, course, ...  |    3   
    8|  [also, theo, johnsonfreyd, note, mark, haimans...  |    1
</code></pre>
<p><strong>Code</strong></p>
<pre><code>for index,rows in comments.iterrows():
  gold_comments = rows[comments.Text.loc[gold_users]]
  Counter(gold_comments)
</code></pre>
","nlp"
"76984","How to use unigram and bigram as an feature on SVM or logistic regression","2020-07-02 02:30:31","76994","1","1501","<machine-learning><nlp><logistic-regression><svm><feature-engineering>","<p>How to use unigram and bigram as an feature to build an Natural Language Inference model on SVM or logistic regression?on my dataset i have premise, hypotesis and label column. I'm planning to use the unigram and bigram of the premis or hipotesis or both as one of the features on my training.
for example :</p>
<pre><code> premise                                      |hipotesis                         |hypothesis bigram
===============================================================================================
I am planning to use the unigram and bigram   |I am planning to use the unigram  |[(i, am), (am, planning), (planning, to), (to, use), (use, the), (the, unigram)]
</code></pre>
<p>the hypothesis bigram is a list of bigram(word), so i cant use it as input to my svm or logistic. can i convert the hypothesis bigram into vector?</p>
","nlp"
"76976","Comparing soccer teams name","2020-07-01 21:09:20","","1","60","<nlp>","<p>Which is the best way to compare soccer teams from different sites? For example, one soccer team in a site is named <code>Academica Clinceni</code> and in another <code>FC Clinceni</code> and in  another one is <code>Acs Fc Academica Clinceni</code>.
Moreover, for a team can exist also the female version of the team (eg. <code>Acs Fc Academica Clinceni (W)</code> or <code>Acs Fc Academica Clinceni (F)</code>). Which is the best approach to distinguish these teams? For the first problem I used the <em>Fuzzy String Matching</em>, in your opinion is it right?</p>
","nlp"
"76915","CountVectorizer vs HashVectorizer for text","2020-06-30 22:46:59","","1","94","<nlp><distributed><tokenization><hashingvectorizer>","<p>I'd like to tokenize a column of my training data (n-gram word-wise), but I'm working with a very large dataset distributed across a compute cluster. For this use case, Count Vectorizer doens't work well because it requires maintaining a vocabulary state, thus can't parallelize easily.</p>
<p>Instead, for distributed workloads, I read that I should instead use a HashVectorizer. My issue is that there are no generated labels now. Throughout training and at the end, I'd like to see which words were the most important for my model, but this isn't possible with Hash.</p>
<p>Is there something I can do to maintain the human-readable labels generated from CountVectorizer but take advantage of a parallel distributed cluster like HashVectorizer can?</p>
","nlp"
"76913","What are the merges and vocab files used for in BERT-based models?","2020-06-30 20:40:39","76925","1","610","<neural-network><nlp><bert>","<p>The title says it all. I see plenty online about how to initialize RoBERTa with a merges and vocab file, but what is the point of these files? What exactly are they used for?</p>
","nlp"
"76872","Next sentence prediction in RoBERTa","2020-06-29 20:55:34","","2","2865","<nlp><bert><transformer>","<p>I'm trying to wrap my head around the way next sentence prediction works in RoBERTa. Based on their paper, in section 4.2, I understand that in the original BERT they used a pair of text segments which may contain multiple sentences and the task is to predict whether the second segment is the direct successor of the first one. RoBERTa's authors proceed to examine 3 more types of predictions - the first one is basically the same as BERT, only using two sentences insted of two segments, and you still predict whether the second sentence is the direct successor of the first one. But I can't understand what the goal is in the other 2. I will cite their explanation below:</p>
<p>• FULL-SENTENCES: Each input is packed with full sentences sampled contiguously from one or more documents, such that the total length is at most 512 tokens. Inputs may cross document boundaries. When we reach the end of one document, we begin sampling sentences from the next document and add an extra separator token between documents. We remove the NSP loss.</p>
<p>• DOC-SENTENCES: Inputs are constructed similarly to FULL-SENTENCES, except that they
may not cross document boundaries. Inputs sampled near the end of a document may be
shorter than 512 tokens, so we dynamically increase the batch size in these cases to achieve a similar number of total tokens as FULL-SENTENCES. We remove the NSP loss.</p>
<p>So from what I understand in these two training strategies they already sample consecutive sentences, or at least consecutive sentences from neighbouring documents,  and I can't see what they are trying to predict - it can't be whether they're consecutive text blocks, because to me it seems that all of their training examples have already been sampled contiguously, thus making such a task redundant. It would be of enormous help if someone were to shed some light on the issue, thanks in advance!</p>
","nlp"
"76852","What methods to get the intention behind questions (time, preferences, ...)?","2020-06-29 14:12:51","","0","52","<nlp><interpretation>","<p>I have a csv with different questions, answer and question types. So far I have only been able to differentiate the questions between muliple answers and likert scale. I would rather like to get the type of questions (when question, where questions,  ...) as well as the intentions behind the question (eating, sleepling ...). Here is the csv:</p>
<pre><code>    QID Questions   Answers QType
0   H1  When do you think your next vacation can start? ['In next 3 months', 'In next 6 months', 'In next 1 year', 'Only once COVID-19 is under control', 'Only once COVID-19 vaccine is developed']    Multiple Choice
1   H2  What are your preferences regarding medical treatment policy (with additional cost)?    [&quot;Doctor's availability in hotel&quot;, 'Ventilator availability in hotel', 'Tie-ups with nearby hospitals', 'Availability of medical rooms with primary first aid care']    Multiple Choice
2   H3  What is your preferences of complementary breakfast?    ['Buffet breakfast with social distancing', 'Buffet breakfast replaced with Ala-carte with limited options', 'Breakfast to be delivered in room with limited options (chargeable)', 'Packaged breakfast only']  Multiple Choice
3   H4  What is your preference for a in-hotel grocery shops for the basic necessity items and packaged food?   ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10'] Likert Scale
4   H5  Consumer Personality    ['']    Multiple Choice
5   H6  What is your preference of hotel check-in?  ['Collect keys at the counter maintaining social distancing', 'Collect keys at the KIOSK using booking bar-code', 'Online Keys using the mobile App']   Multiple Choice
6   H7  What is your preference of payment during Check-out?    ['Pay at the counter maintaining social distancing', 'Pay at KIOSK', 'Online payment using the mobile App'] Multiple Choice
7   H8  What is your preference of hotel cancellation / travel date change policy?  ['Travel date change is preferred at no cost', 'Cancellation at some minimal cost (based on hotel policy)', 'Cancellation with some amount refund and hotel coupons for next visit']    Multiple Choice
8   H9  What is your preference of the guest policy?    ['Guests are allowed in living room with precautions', 'Guest are allowed only in certain designated areas', 'No guests are allowed inside hotel']  Multiple Choice
9   H10 What is your preference of the concierge service?   ['Regular concierge services', 'Online concierge service']  Multiple Choice
10  H11 Consumer Intentions ['']    Multiple Choice
</code></pre>
<p>So the categories can be the wh questions, the intents can be ... anything related to the intentions but one can notice that some lines have no answers (<code>['']</code>), these are just section titles of the whole questionnaire and should be detected as such.</p>
<p>My code:</p>
<pre><code>def classifier(l):
...     l = ast.literal_eval(f&quot;{l}&quot;)
...     try:
...         l = list(map(int, l))
...     except ValueError:
...         pass
...     if not l:
...         return None
...     try: 
...         if all(isinstance(x, int) for x in l):
...             return &quot;Likert Scale&quot;
...         else:
...             return &quot;Multiple Choice&quot;
...     except:
...         return None
... 
... df.QType = df.apply(lambda row: classifier(row['Answers']), axis = 1)
df.QType
0       Multiple Choice
1       Multiple Choice
2       Multiple Choice
3          Likert Scale
4       Multiple Choice
             ...       
</code></pre>
<h1>Update:</h1>
<p>I've tried to get the intents and differentiate with sextions using deep pavlov:</p>
<pre><code>!pip install fasttext

from deeppavlov import build_model, configs

CONFIG_PATH = configs.classifiers.intents_snips  # could also be configuration dictionary or string path or `pathlib.Path` instance

model = build_model(CONFIG_PATH, download=True)  # in case of necessity to download some data

model = build_model(CONFIG_PATH, download=False)  # otherwise

print(model([&quot;What is the weather in Boston today?&quot;]))

from google.colab import auth
auth.authenticate_user()
import gspread 
from oauth2client.client import GoogleCredentials
gc = gspread.authorize(GoogleCredentials.get_application_default())
wb = gc.open_by_url('https://docs.google.com/spreadsheets/d/1g...')
sheet = wb.worksheet('qa_cleaned_6')
data = sheet.get_all_values()
df = pd.DataFrame(data)

for question in df.Questions.head().iteritems():
  question= question[1]
  print(&quot;question: &quot;,question , &quot;type: &quot;, model([question]))
</code></pre>
<p>But got:</p>
<pre><code>question:  When do you think your next vacation can start? type:  ['PlayMusic']
question:  What are your preferences regarding medical treatment policy (with additional cost)? type:  ['SearchScreeningEvent']
question:  What is your preferences of complementary breakfast? type:  ['SearchScreeningEvent']
question:  What is your preference for a in-hotel grocery shops for the basic necessity items and packaged food? type:  ['BookRestaurant']
question:  Consumer Personality type:  ['PlayMusic']
</code></pre>
","nlp"
"76811","Does stronger regularization always improve performance on testing set?","2020-06-28 16:58:15","","0","442","<machine-learning><scikit-learn><nlp><regularization>","<p>I am using the Sklearn logistic regression function to do a binary classification task on texts.</p>
<p>I did the task using three different inputs: Bag-Of-Words, TF-IDF, Doc2vec embeddings.</p>
<p>The question is that for the Bag-Of-Words, stronger regularization improves the performance on testing set.
However, for TF-IDF and Doc2vec embedding, stronger regularization doesn't improve the performance on testing set. Actually the performance downgrades with stronger regularization.</p>
<p>Why this is the case?</p>
","nlp"
"76796","Automatic topic labelling for topic modelling","2020-06-28 09:33:22","116727","3","1800","<machine-learning><nlp><topic-model><python-3.x>","<p>I am just curious to know if there is a way to automatically get the lables for the topics in Topic modelling. It would be really helpful if there's any python implementation of it.</p>
","nlp"
"76777","Sentiment analysis of tweets (Train model on a labelled dataset and use on some other unlabelled data)","2020-06-27 16:06:36","106841","3","222","<classification><nlp><clustering><unsupervised-learning><sentiment-analysis>","<p>I have a huge amount of tweets on a particular topic say 'ABC' and the data is not labelled. I want to perform multi-class sentiment analysis of these tweets. I tried many unsupervised clustering techniques like Kmeans, DBScan, Agglomerative clustering from sklearn but the max silhoutte score that I have reached is 0.31 and the kmeans gives large negative score. I have performed cleaning and encoding of tweets using Bert embeddings, Word2Vec but nothing seems to change.</p>
<p>Suppose I used some other labelled multiclass dataset and build a classifier and then use that classifier to identify sentiment in my target data, will it be good enough? Is this approach correct and logical?</p>
<p>I have found <a href=""https://github.com/lukasgarbas/nlp-text-emotion/tree/master/data/datasets"" rel=""nofollow noreferrer"">these</a> general speech datasets. Will they suffice my purpose of getting correct sentiments for the &quot;ABC&quot; tweets dataset?</p>
<p>I found <a href=""http://saifmohammad.com/WebPages/EmotionIntensity-SharedTask.html"" rel=""nofollow noreferrer"">this</a> another emotion dataset related to tweets.</p>
","nlp"
"76739","Question on bootstrap sampling","2020-06-26 22:19:14","","2","61","<nlp><bootstraping>","<p>I have a corpus of manually annotated (aka &quot;gold standard) documents and a collection of NLP systems annotations on the text from the corpus. I want to do a bootstrap sampling of the system and gold standard to approximate a mean and standard error for various measures so that I can do a series of hypotheses tests using possibly ANOVA.</p>
<p>The issue is how do I do the sampling. I have 40 documents in the corpus with ~44K manual annotation in the gold standard. I was thinking of using each document as a sampling unit, and taking 60% of documents for each sample (or 24 documents per sample). However, the issue is that each manually annotated documents does not have the same number of annotations, so that violates using same sample size for each sample.</p>
<p>Any suggestions on how to achieve this bootstrap?</p>
","nlp"
"76720","How can I encode a 'Name' so that similar names are represented by vectors close in n-dimensional plane?","2020-06-26 14:10:35","","1","1259","<python><nlp><clustering><word2vec>","<p>I want to encode names of people for similarity comparison between them such that a name like 'Sarah' is closer when represented in vector to a name like 'Sarah connor', something very similar to what word2vec does but it uses sentences to train but I only have list of words. Using string matching algorithms like Levenshtein distance and Jaccard Index, I can find similarity between them but this can't be used to derive vector embeddings of these words satisfying above conditions, or can they ? If not is there a way to encode these list of names such that the condition of similar names (based on characters and other conditions) are closer in n-dimensional space, n being the vector length of these embeddings.</p>
","nlp"
"76652","Why does English ELMo model give embeddings for non-English words?","2020-06-25 10:21:52","","0","429","<tensorflow><word-embeddings><nlp><language-model>","<p>Here's the code from my notebook:</p>
<pre><code>%tensorflow_version 1.x
import tensorflow as tf
import tensorflow_hub as hub

elmo = hub.Module(&quot;https://tfhub.dev/google/elmo/2&quot;, trainable=True)
tf.logging.set_verbosity(tf.logging.ERROR)

def elmo_vectors(x):
    embeddings = elmo(x, signature=&quot;default&quot;, as_dict=True)[&quot;elmo&quot;]
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        sess.run(tf.tables_initializer())
        return sess.run(embeddings)
</code></pre>
<p>Output for non-English language: (Hindi in this example)</p>
<pre><code>words = ['गोकुल']
v = elmo_vectors(words)
print(v.shape) # (1,1,1024)
print(v[0][0])
# Output: [ 0.3731584   0.5700774  -0.48072845 ... -0.1241736   0.5961436 -0.6986947 ]
</code></pre>
<p>The <a href=""https://tfhub.dev/google/elmo/2"" rel=""nofollow noreferrer"">documentation of the pre-trained ELMo on Tensorflow Hub</a> shows that it was trained only on the English language.<br />
That is, the dataset from 1 billion word benchmark is based on monolingual English data. (<a href=""https://opensource.google/projects/lm-benchmark"" rel=""nofollow noreferrer"">Source</a>)</p>
<p>So, how/why am I getting embeddings for non-English vocabulary words from ELMo using the TF Hub model?</p>
","nlp"
"76645","Closed Domain Question Answering which doesn't answer Questions","2020-06-25 07:57:54","","0","273","<nlp><stanford-nlp><question-answering>","<p>I've been exploring Closed Domain Question Answering Implementations which have been trained on SQuAD 2.0 dataset. Ideally, it should not answer questions which the context text corpus doesn't contain answers to. But while implementing such models using the <code>Haystack</code> repo or the <code>FARM</code> repo, I'm finding that it always answers these questions even when it shouldn't. Is there any implementation available that takes into account the fact that it shouldn't answer questions when it doesn't find a suitable answer.</p>
<p>References:</p>
<ol>
<li><p><a href=""https://colab.research.google.com/github/deepset-ai/haystack/blob/update-tutorials/tutorials/Tutorial3_Basic_QA_Pipeline_without_Elasticsearch.ipynb#scrollTo=KS4nTwxIbRb6"" rel=""nofollow noreferrer"">https://colab.research.google.com/github/deepset-ai/haystack/blob/update-tutorials/tutorials/Tutorial3_Basic_QA_Pipeline_without_Elasticsearch.ipynb#scrollTo=KS4nTwxIbRb6</a></p>
</li>
<li><p><a href=""https://github.com/deepset-ai/FARM"" rel=""nofollow noreferrer"">https://github.com/deepset-ai/FARM</a></p>
</li>
<li><p><a href=""https://github.com/deepset-ai/haystack"" rel=""nofollow noreferrer"">https://github.com/deepset-ai/haystack</a></p>
</li>
<li><p><a href=""https://huggingface.co/deepset/bert-large-uncased-whole-word-masking-squad2"" rel=""nofollow noreferrer"">https://huggingface.co/deepset/bert-large-uncased-whole-word-masking-squad2</a></p>
</li>
<li><p><a href=""https://colab.research.google.com/drive/1UrKlHlf68hD3wwQDTctx2cQs6FMUxLLH?usp=sharing#scrollTo=J4jxYsxaG77O"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1UrKlHlf68hD3wwQDTctx2cQs6FMUxLLH?usp=sharing#scrollTo=J4jxYsxaG77O</a></p>
</li>
</ol>
","nlp"
"76622","How to build recommendation model based on resume and job description?","2020-06-24 20:16:12","76835","4","879","<machine-learning><deep-learning><nlp><recommender-system><semantic-similarity>","<p>How to build a model which will result in better recommendation of resumes based on the job description given?</p>
<p>I am familiar with bow or tfidf (n-grams) approach and then take a cosine similarity but I'm looking for a deep learning approach. I don't have any labelled data to evaluate.</p>
<p>Anything suggestions will be really appreciated.</p>
","nlp"
"76442","Constituency vs Dependency Parsing: What is more effective for Sentiment Analysis?","2020-06-22 08:07:05","","0","209","<nlp><sentiment-analysis><parsing>","<p>Parsing is often used to understand the sentiment of complex sentences filled with double negations or very articulated.</p>
<p>There are two main ways of parsing a sentence: <strong>Constituency</strong> and <strong>Dependency Parsing</strong>. What is the most successful application for Sentiment Analysis?</p>
","nlp"
"76406","Model for extracting common context from a similar cluster of sentence","2020-06-21 15:20:57","","1","26","<deep-learning><nlp>","<p>I have multiple clusters of similar sentence embeddings (one cluster holding one type of similar sentences)</p>
<p>Is there any unsupervised model that can extract the common context from each cluster in:</p>
<ul>
<li>a dense embedding or</li>
<li>in a sentence format altogether</li>
</ul>
<p>I am gonna use BERT to evaluate the dense sentence embedding and apply hierarchical clustering to create the clusters (I haven't evaluated how well it is gonna work yet), then apply your suggested model to extract the common context.
P.S. I would be thankful if you could provide the model for the second option.</p>
","nlp"
"76328","ELMo - How does the model transfer its learning/weights on new sentences","2020-06-20 03:44:07","","0","175","<nlp><word-embeddings><transfer-learning><encoding><embeddings>","<p>Word2vec and Glove embeddings have the same vector representation for every word in the corpus and does not take context into consideration.</p>
<p>For eg:</p>
<ul>
<li>The dog does bark at people</li>
<li>The bark of the tree is hard.</li>
</ul>
<p>In the above examples, Word2vec and Glove create one vector for the word &quot;bark&quot;. But using Elmo, there would be two different representations for the word &quot;bark&quot; as it considers context. So, I am trying to the mechanism behind how Elmo gives a vector for a new sentence in the data set?</p>
<p>Say, we have a pre-trained model and it has different vectors for various words. When I have to use this model on a new sentence, does Elmo produce a new vector with completely new vectors? Then is the fine-tuning of the model always happening as it is applied to new data? If that is the case, when is it considered to be a completely trained model?</p>
","nlp"
"76322","N-grams for RNNs","2020-06-19 23:08:44","","6","333","<nlp><lstm><rnn><ngrams>","<p>Given a word <span class=""math-container"">$w_{n}$</span> a statistical model such a Markov chain using n-grams predicts the subsequent word <span class=""math-container"">$w_{n+1}$</span>. The prediction is by no means random.</p>
<p><strong>How is this translated into a neural model?</strong> I have tried tokenizing and sequencing my sentences, below is how they are prepared to be passed to the model:</p>
<pre><code>train_x = np.zeros([len(sequences), max_seq_len], dtype=np.int32)
for i, sequence in enumerate(sequences[:-1]): #using all words except last
    for t, word in enumerate(sequence.split()):
        train_x[i, t] = word2idx(word) #storing in word vectors
</code></pre>
<p>The sequences look like this:</p>
<pre><code>Given sentence &quot;Hello my name is&quot;:
Hello 
Hello my
Hello my name
Hello my name is
</code></pre>
<p>Passing these sequences as input to an RNN with an LSTM layer, the predictions of the next word (given a word) I'm getting are random.</p>
","nlp"
"76320","Classify text as logical/ not logical","2020-06-19 20:19:40","76323","2","81","<nlp><text-mining><text-classification>","<p>Can some one advise me direction where to look in.Or some resources. Here is a task:</p>
<ol>
<li>User leaves feed back-text with min 50 characters.</li>
<li>I need to check if it's normal human sentences/ word combination OR just bag of words and characters.</li>
</ol>
<p>For ex ( 1-normal, 0-not normal):</p>
<p>&quot;I wrote question.hope for answer&quot; - 1(class)</p>
<p>&quot;Bla bla goog goog goog gooo&quot; - 0(class)</p>
<p>Maybe some dataset available.or some approach?
Thanks in advance!</p>
","nlp"
"76298","LSA, LDA or NMF in Topic Modeling?","2020-06-19 14:08:45","","2","2232","<nlp>","<p>I'm trying to implement Topic Modeling via Python &amp; NLP but can't figure out what algorithm should I use. I have studied Latent Semantic Analysis (LSA), Latent Dirichlet Allocation (LDA) and Non-negative matrix factorization (NMF) but how to decide which algorithm fits best for certain task? If I just try all of them in a row then how to measure the result?</p>
","nlp"
"76242","Explain FastText model using SHAP values","2020-06-18 14:09:24","","3","707","<python><nlp><pytorch><shap><fasttext>","<p>I have trained fastText model and some fully connected network build on its embeddings. I figured out how to use Lime on it: complete example can be found in <a href=""https://medium.com/@ageitgey/natural-language-processing-is-fun-part-3-explaining-model-predictions-486d8616813c"" rel=""nofollow noreferrer"">Natural Language Processing Is Fun Part 3: Explaining Model Predictions</a></p>
<p>The idea is clear - put 1 sentence into Lime, it drop words and generate some new sentences from my and check how score changes.</p>
<p>My next idea - use SHAP values for this.
SHAP values can be used for any deep model, using <code>DeepExplainer</code>. Here is a usage example: <a href=""https://slundberg.github.io/shap/notebooks/deep_explainer/Keras%20LSTM%20for%20IMDB%20Sentiment%20Classification.html"" rel=""nofollow noreferrer"">Keras LSTM for IMDB Sentiment Classification</a></p>
<p>But I can't use it for my ensemble, because <code>DeepExplainer</code> needs tensors as input, but I want to fed sentences. I don't want to use BoW or TF-IDF for that - I loose fastText power in that situation.
What I want to achieve - get some shap-plots built on words of my sentence.
Is it possible?</p>
","nlp"
"76201","Detect passive voice in headlines","2020-06-18 00:00:13","","2","1179","<python><nlp><spacy>","<p>To detect passive voice in sentences, we can use the spacy module to tag each token in the sentence, then build a classifier to classify it as passive or active based on conventional grammatical rules, e.g.</p>
<blockquote>
<p>If a clause has all of the following, then it is in the passive voice:</p>
<ul>
<li>A form of an auxiliary verb (usually be or get)</li>
<li>The past participle of a transitive verb</li>
<li>No direct object</li>
<li>The subject of the verb phrase is the entity undergoing an action or having its state changed</li>
</ul>
</blockquote>
<p>News headlines are written differently. They don't follow conventional grammatical rules. How would one be able to use spacy to detect headlines in passive voice?</p>
<p>For example, if I have this headline, &quot;Church That Defied Coronavirus Restrictions Is Burned to Ground,&quot; its part-of-speech tags are <code>['ROOT', 'dobj', 'compound', 'compound', 'nsubjpass', 'auxpass', 'relcl', 'prep', 'pobj']</code>. We can classify this as passive, based on the fact that it has a nominal subject (passive) and no direct object.</p>
<p>However, if I have this headline, &quot;Rayshard Brooks Fatally Shot By the Atlanta Police,&quot; its part-of-speech tags are <code>['compound', 'ROOT', 'advmod', 'ROOT', 'prep', 'det', 'compound', 'pobj']</code>. We can't use any of the criteria to classify this as passive voice, because the headline has dropped the auxiliary and spacy doesn't detect a nominal subject (passive).</p>
","nlp"
"76173","What are the theoretical differences of multitask learning vs fine tuning based transfer learning?","2020-06-17 14:44:55","","0","407","<machine-learning><neural-network><deep-learning><nlp><transfer-learning>","<p>Suppose, I have the following scenarios:</p>
<ol>
<li><p>I have a bunch of fruits, i.e., apple, orange, and banana. I simply made a multitask model, where my network first tell me which fruit it is, and then telling me the color of it. Suppose, if I give my network an apple, it tells me, (a) it is apple, (b) it is red. By doing some theoretical study, I have understood that it is one type of inductive transfer learning (TL) (correct me, if I am wrong). So here, the network is learning 2 task simultaneously.</p>
</li>
<li><p>I have a bunch of objects, i.e., cube, ball, and triangle. Here also I want my network will do the same thing like scenario 1. So it will tell me, (a) whether it is a cube or not, and (b) then tell me the color. So what I did is, I transferred the learned weight and parameters from the network of scenario 1 to this scenario. Thus I performed the fine-tuning based TL in here.</p>
</li>
</ol>
<p>So , from theoretical point of view, I have few confusions. I need to clarity my idea, and need some ideas from experts.</p>
<ol>
<li>If I consider the scenario 2, by definition of fine-tuning based TL, the task of scenario 1 (apple, and red) is my source task, and the task of scenario 2 (cube, and red) is the target task. From my understanding, I think that every inductive TL approach has a source task and target task. So, for scenario 2, thus it satisfies my understanding.</li>
</ol>
<p>[REAL QUESTIONS]
<strong>2.</strong> Now the confusion starts for my theoretical understanding. For scenario 1, it has also 2 tasks - (a) identify the fruit, (b) identify the color. So here, what will be my source task, and what will be my target task. For clarifying my theoretical description or explaining my thinking into words, I need to know this.</p>
<p><strong>3.</strong> As I am doing 2 TL tasks here, how to define the whole scenario?</p>
","nlp"
"76148","deep learning and uncertainty estimation","2020-06-17 09:37:58","","-1","70","<machine-learning><deep-learning><nlp><statistics>","<p>Recently I got very interested in NLP applications of deep learning. Diving into literature (on arXiv for instance) I noticed that is very unpopular to quote and estimate uncertainties on scores of ML tasks. In the era of pretrained language model (i.e. bert, gpt etc.) all further improvements quoted in papers seems to be compatible among each other within 1 or less standard deviations, making all the results statistically compatible with a fluctuation due to stochastic optimization in neural network training procedure (at fixed data-set). I am a physicist, and this looks really confusing to me when compared to the statistical treatment of experimental data performed by routine in laboratories.
I am sure this question has already been discussed in the past in ML/Data Science community, could you point me some review or paper addressing this issue?
Also, could you please share with me your thoughts about?<br />
Thank you very much</p>
","nlp"
"76133","HMM and its competitive alternatives","2020-06-17 02:48:44","","1","120","<lstm><nlp><sequence>","<p>In Natural language processing, what are the major applications of Hidden Markov Chain (HMM), and what are the alternatives that usually can outperform HMM, is RNN and LSTM always the choice right now?</p>
","nlp"
"76052","Unusually High BLEU score on a NMT model","2020-06-15 17:39:05","76054","1","334","<nlp><machine-translation>","<p>This is the project on Neural Machine Translation on the English/Irish language pair. I have been spending the past month or so trying to train a good baseline to do 'experimentation' on. I have a corpus of ~850k sentences (unfortunately Irish is very limiting). When I trained it and evaluated it with BLEU, I got a score of 65.02, which is obviously absurdly incorrect. These were my Fairseq-train settings:</p>

<pre><code>!CUDA_VISIBLE_DEVICES=0 fairseq-train data-bin-full_corp/MayNMT \
  --lr 5e-4 --lr-scheduler inverse_sqrt --optimizer adam\
  --clip-norm 0.1 --dropout 0.2 --max-tokens 4096 \
  --arch transformer --save-dir checkpoints/full-tran
</code></pre>

<p>I know not everyone uses Fairseq in NLP, but I hope the arguements are self-explanatory.</p>

<p>I deduplicated the dataset (converted to a Python <code>set()</code> which only takes unique entries) so I don't think the issue is the dev/valid and test sets contain duplicate entries, but I'm not sure what else causes this. Some suggest overfitting may be a cause, but I feel that would only affect the BLEU if the dev set shared training entries.
I've tried to find the problem myself, but there aren't many places that cover NMT, let alone BLEU.</p>
","nlp"
"75985","Clustering mixed data types - numeric, categorical, arrays, and text","2020-06-14 13:52:54","","2","1247","<nlp><clustering><k-means><categorical-data><text>","<p>I have a dataset with 4 types of data columns:</p>

<pre><code>              numeric  categorical            tags                     text
id
1               51585           27  [A, B, C, ...]  ""Some text bla bla bla""
2               53596           27  [B, D, E]               ""Other text...""
3             1176345           27  [D, A, F, ...]                    ""...""
4                 168           24             NaN                    ""...""
5               88564           22             NaN                    ""...""
</code></pre>

<ul>
<li><code>numeric</code> - continuous numeric values.</li>
<li><code>categorical</code> - discrete categories, either numbers or strings (the type doesn't really matter because I can convert it to whatever works)</li>
<li><code>tags</code> - array containing discrete values. Each row can have a different array length.</li>
<li><code>text</code> - a string of text.</li>
</ul>

<p>I am new to data science so maybe this is a ""beginner"" question.</p>

<p>How can I use all of these different data types in a clustering algorithm?</p>

<p>Here is what I learned so far:</p>

<ul>
<li>K-means is good for numerical data. I successfully applied it to a subset of my data with only numerical columns. I also used some evaluation metrics (such as silhouette coefficient) to help me choose the number of clusters. So this works in principal but since it's not using most of my data the results are not good.</li>
<li>Then I read about clustering categorical data. I found the Gower Distance which is a distance between categorical data. So far I've used it with K-means (I passed the distance matrix generated by Gower into K-means). From here it should be easy to join the Gower distance matrix with the numeric columns from my original dataset and pass all of them to K-means.</li>
</ul>

<p>I am aware there are other clustering algorithms besides K-means, and I plan to check some others as well. But before I do, I want to find some way to utilize all of my data in a single algorithm.</p>

<ul>
<li>The <code>tags</code> and <code>text</code> columns stump me. I can't find a way to use them for clustering. I found some articles about clustering words from a text document - this is not what I want to do. I want to use a <code>text</code> column as one (or more) ""feature"" among others for clustering.</li>
<li>I am aware of the ""bag of words"" method for converting <code>text</code> into a vector of numbers. I can also easily imagine how to use this same method for converting the <code>tags</code> into a vector. However that seems like a bit of an overkill because it will increase the dimentionality of my data by a lot. Are there other ways to tackle this?</li>
</ul>

<p>Bottom line - I am looking for a way to use all these data types together for clustering. I summarized what I know so far, but I am open to any solution, even if it's completely different from what I've listed above.</p>

<p>Thanks!</p>
","nlp"
"75870","Text Classification on a very small data set with a lot of classes","2020-06-12 06:25:49","","0","282","<scikit-learn><nlp><svm><text-classification><spacy>","<p>I have a data set consisting of 455 rows spread over 21 different classes. The data set is imbalanced as well as you can see below. </p>

<pre><code>job_alerts                    45
howto_apply                   40
application_status            31
salary                        30
job_close_date                30
multiple_role                 27
feedback                      26
assessment_campatilibility    24
interview_reschedule          23
disability                    22
reinstate_application         19
job_account_issue             19
assessment_link_problem       16
age_limit                     16
assessment_timebox            16
cv_past_experience            15
late_for_interview            13
interview_response_time       13
work_experience               11
assessment_validity           10
special_needs_at_work          9
</code></pre>

<p>I'm trying to perform classification on this. Firstly, when preprocessing the data I convert the text to lowercase, remove stopwords and punctuation and convert words to their lemmas using spaCy. </p>

<p>So far, I have tried the TextCategorizer in spaCy which uses a simple CNN for classification and I'm getting around 70% accuracy on an 80-20 split of the data. I have also tried various Sklearn algorithms such as SVC, LogisticRegression, RandomForestClassifier and MultinomialDB with the  TfIDVectorizer. I have used GridSearchCV to tune the parameters. The best results I get are with SVC at around 74-75% accuracy. </p>

<p>I want to know what else I can try to improve my accuracy. I am a beginner to NLP and I haven't worked on something like this before. Right now the ideas I have are to improve the TfIdVectorizer through some parameter tuning and to try a One vs all classifier for SVC. What else can you suggest to me?</p>
","nlp"
"75867","How to identify similar words as input from a dictionary","2020-06-12 04:00:30","75876","0","461","<machine-learning><neural-network><deep-learning><text-mining><nlp>","<p>Let's say I have a CSV file (single column) with a list of words as input. Meaning the file looks like below</p>

<p><strong>Input Terms</strong></p>

<pre><code>Subaaaash                         
Paruuru                          
Mettromin                        
Paracetamol                      
Crocedinin                        
Vehiclehcle                      
Buildding                        
</code></pre>

<p><strong>Dict terms</strong> #this dict has around million records and I have to find a closest match for input term from this dictionary</p>

<pre><code>Metformin 250 MG
Metformin
.....
Crocin
Vehcile 
Paru
Subash
</code></pre>

<p>Now, I expect my output to be like as shown below</p>

<p>As you can see that red colored <code>Paru</code> is not a correct match for <code>paracetamol</code>, but that's the closest match we can get for the input term <code>paracetamol</code> from the dictionary. So, I would like to do matching based on </p>

<p>1) Word sounds (when pronounced). Phenotics</p>

<p>2) Spelling mistake corrections</p>

<p>3) Find the best matching word from the dictionary for input terms</p>

<p><a href=""https://i.sstatic.net/nfFKm.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/nfFKm.png"" alt=""enter image description here""></a></p>

<p>Can you let me know how can I do the above?</p>
","nlp"
"75839","Using BM25 to rank words","2020-06-11 17:03:57","","1","696","<nlp><statistics><sentiment-analysis><information-retrieval><ranking>","<p>How effective is it to use BM25 to rank words, to be more specific i have a dictionary of words and i want to rank only words in a document that are also in my dictionary. I want to rank all words in my dictionary for each document and then add the BM25 value of each word for the specific document.</p>

<p>Lets say i have a document and dictionary like this:</p>

<pre><code>myDictionary=['bad', 'dangerous','hide', 'following]

corpus=[
['human', 'intelligence', 'computer','bad', 'dangerous'],
 ['survey', 'user', 'human', 'system', 'time', 'hide', 'following],
]
</code></pre>

<p>Now iam going to run the below BM25 formula in a loop for all words in my dictionary and then sum
the results of each word to get a bm25 value for each document.</p>

<p><img src=""https://latex.codecogs.com/gif.latex?BM25(myDictionary[i],&space;d1)&space;=&space;%5Cfrac%7B(k&plus;1)c(w_i,&space;d1)%7D%7Bc(w_i,&space;d1)&space;&plus;&space;k(1&space;-&space;b&space;&plus;&space;b%5Cfrac%7B|d|%7D%7Bavdl%7D)%7D&space;%5C%5C"" title=""BM25(myDictionary[i], d1) = \frac{(k+1)c(w_i, d1)}{c(w_i, d1) + k(1 - b + b\frac{|d|}{avdl})} \\"" /></p>
","nlp"
"75811","Is there an existing way to find out if there is an association between two words in a sentence using NLP","2020-06-11 04:02:22","","1","31","<python><nlp>","<p>So, I was working on word2vec , playing with it and although it's a great tool for finding similarities between two words,But when we come to finding a semantic association between two words it cannot be used. It would be interesting to know if there's an existing way, if yes how accurate is it and what principles does it work on?</p>
","nlp"
"75783","Dataset availability for automatic text summarization","2020-06-10 13:44:09","","3","45","<dataset><nlp><automatic-summarization>","<p>I'm working on an automatic text summarization NLP problem and looking for a dataset with USA legal case reports similar to the <a href=""https://archive.ics.uci.edu/ml/datasets/Legal+Case+Reports"" rel=""nofollow noreferrer"">Australian legal case reports dataset in UCI repository</a>.</p>

<p>Can you please refer me to any such dataset? I've not been able to find one up until now. It will also be great if you can point me to other industry-relevant datasets that can be used for automatic text summarization. </p>
","nlp"
"75656","NLP: Compare tags semantically with machine learning? (finding synonyms)","2020-06-08 07:52:31","","1","677","<machine-learning><nlp><semantic-similarity>","<p>Let's say I have multiple tags that I need to compare semantically. For example:</p>

<pre class=""lang-py prettyprint-override""><code>tags = ['Python', 'Football', 'Programming', 'Handball', 'Chess', 'Cheese', 'board game']
</code></pre>

<p>I would like compare these tags (and many more) semantically to find a similarity value between 0 and 1.
For example, I want to get values like these:</p>

<pre class=""lang-py prettyprint-override""><code>f('Chess','Cheese') = 0.0  # tags look similar, but means very different things
f('Chess', 'board game') = 0.9 # because chess is a board game
f('Football', 'Handball') = 0.3 # because both are sports with a ball
f('Python', 'Programming') = 0.9 # because Python is a programming language
</code></pre>

<p>So what is the state of the art approach to get a function <code>f</code> like this? I know that machine learning might be doing this, but this area is huge and overwhelming for me (on the first glance it looks like NLP focuses on other problems).
So what would be the best approach for this specific problem?</p>
","nlp"
"75648","Where can I get Insurance claim data for practicing NLP(Natural Language) processing?","2020-06-08 06:20:44","","2","500","<machine-learning><deep-learning><data-science-model><nlp>","<p>I am looking for specifically Insurance dataset for practicing Machine Learning &amp; NLP, but unable to find much in kaggle, udemy or other websites. Is there a way to get that dataset or any website that stores this.</p>

<p>I am mainly looking for NLP practice for automatic claim generation or other insurance activities, please suggest the approach</p>
","nlp"
"75635","in NLP academic paper, would it be okay to refer the ""token embeddings"" as ""tokens""?","2020-06-07 20:23:08","75637","1","43","<nlp>","<p>I am writing a paper in Natural Language Processing (NLP), and I just have a quick question about terminology.</p>

<p>In language models like Transformers, ""token"" refers to individual word in a text sequence, whereas there is a special term ""token embedding"" to refer to the embedding that results after token gets passed through the initial embedding layer.</p>

<p>Would it be problematic if I just refer a ""token embedding"" as a ""token""? </p>

<p>(e.g. ""interaction between hidden embeddings and token embeddings"" ---> ""interaction between hidden embeddings and tokens"")</p>

<p>I am trying to accommodate the different terminologies, but my sentences are getting really wordy...</p>

<p>Thank you,</p>
","nlp"
"75631","What is syntax V and S standing for nominal subject?","2020-06-07 19:46:03","75638","0","29","<deep-learning><nlp><bert>","<p>I was reading the recent paper <a href=""https://www.aclweb.org/anthology/P19-1580.pdf"" rel=""nofollow noreferrer"">https://www.aclweb.org/anthology/P19-1580.pdf</a> and noticed that in section 5.2, the syntactic relation is studied in terms of the ""direction between two tokens"". In table 1, the result is further shown with direction like <span class=""math-container"">$$v \to s, s \to v$$</span> for nsubj(nominal subject). </p>

<p><a href=""https://i.sstatic.net/zmQLR.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/zmQLR.png"" alt=""enter image description here""></a></p>

<p>However, what does V and S refer to here and where I can find more materials about this ?  </p>
","nlp"
"75518","Machine Learning - Input Prepocessing - NLP email classification model","2020-06-05 13:49:39","","1","214","<classification><nlp><preprocessing>","<p>So I created a model which classifies emails into different categories, just like a spam filter. I deployed the model as a webservice, no problem with that but I can’t get my head around how I would use it to predicht the output category of a new email. How do I preprocess the new email (subject and message body) to match the input format of the model/webservice ? The model I trained has about 1000 features, corresponding to the 1000 most frequent words in the training dataset. Do I vectorize the new email ? Do I just search for the features/words in the new email ?
There is something obvious I’m missing, I think.</p>

<p>I used python, sklearn and pandas/numpy to preprocess and train the model</p>
","nlp"
"75408","approach for multi label text classification","2020-06-03 20:58:04","75639","1","97","<nlp>","<p>I want to make a classifier that will label each text in a corpus with the correct label(s).
I can go straight to ML using sklearn multi-label text classification, or even to DL using LSTM.
But is it not better to start simple, and first use a rule-based system. That will help me understand the problem, and also set a benchmark accuracy score. Then I can make my algorithm more sophisticated (ML, DL) gradually in ways that only help the precision and recall.</p>
","nlp"
"75375","How to detect medicine name from the medicine wrapper","2020-06-03 12:28:41","","0","535","<machine-learning><python><nlp><ocr><stanford-nlp>","<p>I have got medicine wrapper ( Packaging ) of different medicines.</p>

<p>I want to detect medicine name out of it. </p>

<p>I'm using Google Cloud Vision to extract all the text from the medicine wrapper.</p>

<p>Text contains medicine mfg info, address, pin code, expiry date, ingredients and most importantly medicine name.</p>

<p>I want to extract only medicine name out of it. </p>

<p>What are the possible ways to get medicine name ?</p>

<p>Thanks!</p>
","nlp"
"75304","BPE vs WordPiece Tokenization - when to use / which?","2020-06-02 14:21:40","","10","7567","<machine-learning><nlp><sentiment-analysis><transformer><machine-translation>","<p>What's the general tradeoff between choosing BPE vs WordPiece Tokenization? When is one preferable to the other? Are there any differences in model performance between the two? I'm looking for a general overall answer, backed up with specific examples.</p>
","nlp"
"75299","How to figure out if two sentences have the same meaning with AI?","2020-06-02 13:19:11","","3","2354","<machine-learning><keras><tensorflow><nlp><similarity>","<p>I have two sentences which might be similar in meaning. 
Are there a useful and successful (machine learning) algorithms, which is able to determine the semantic similarity?</p>

<p>Are there any approaches which can handle this task and are easy to use?</p>

<p>An approach via Keras or Tensorflow as a Framework would work perfectly in my use case.</p>

<p>Thank you!</p>
","nlp"
"75206","NLP Text Summarization - which metrics to use in evaluation?","2020-06-01 07:00:34","","3","7605","<nlp>","<p>I'm trying to implement Text Summarization task using different algorithms and libraries. To evaluate which one gave the best result I need some metrics. I have read about the Bleu and Rouge metrics but as I have understand both of them need the human reference summaries as a reference. Is it so that no automatic evaluation can be done in text summarization, i.e human generated summary is needed beforehand? What is the common practice for the tasks like this? How often is the text summarization accuracy measured at all?</p>
","nlp"
"75156","How do you distinguish between conversational text and possible news article?","2020-05-31 06:03:18","","1","100","<machine-learning><classification><scikit-learn><nlp><spacy>","<p><strong>Context</strong></p>

<p>When you receive messages in group chats, how do you detect if that message belongs to conversational dialogue or if it is a 'news' article (could be fake or real) that they are sharing?</p>

<p><strong>Examples</strong></p>

<p>Conversational dialogue: ""Does anyone wanna have dinner today? I am pretty free this evening and I don't have dinner at home. Please let me know by 8pm!!""</p>

<p>'News' article: ""New Japan’s growth has been helped by YouTube, which has made New Japan’s matches more accessible to an audience outside of Asia, said Dave Meltzer, publisher of the Wrestling Observer Newsletter, which has followed the sport since 1983. Capitalizing on this rise, New Japan launched an online streaming service — similar to the W.W.E. Network — in December 2014.""</p>

<p><strong>Question</strong></p>

<p>Would you use rule-based matching or a classifier for this problem? (Assuming you already have a classifier for detecting if the news article is fake or real)</p>
","nlp"
"75108","German Chatbot or conversational AI","2020-05-30 10:55:41","","3","859","<dataset><nlp><bert><transformer>","<p>I want to build a chatbot mostly BERT(Transformer) based in the German Language. But I do not find any German chatbot data set!</p>

<p>So does it make sense to use google translator API to translate the English dataset to German and then train the model on it?</p>

<p>Any idea where I can find German datasets or solve this issue?</p>
","nlp"
"75092","Extract information using NLP and store it in csv file","2020-05-30 01:36:16","","2","447","<text-mining><data-science-model><feature-extraction><nlp>","<p>I have a text file that stores the pickup, drops, and time. SMS text is a dummy file that is used to train a cab service model. The text is like in this format:</p>

<pre><code>Please book a cab from airport to hauz khaas at 3 PM
airport to hauz khaas at 6 PM
Kindly book a cab for me at 1 PM from hauz khaas to dwarka sector 23
airport to hauz khaas at 1 AM
I want to go to dwarka sector 21 from airport leaving at 10 PM
airport to dwarka sector 21 at 12 PM
Please book a cab for dwarka sector 23 from hauz khaas at 12 PM
Please book a cab from dwarka sector 23 to dwarka sector 21 at 4 PM 
</code></pre>

<p>The problem is I need to create 3 columns in a csv file - Destination, Pickup and Time. I used almost every technique but It is not accurately extracting the text. I tried chinking, POS tagging, regex I also tried <code>LatentDirichletAllocation</code> to create the features but need some help to understand what is missing. 
Here is the code that I used:</p>

<pre><code>import nltk
returnme = list()
def process_content():
    try:
        returnme1 = list()
        for i in txtData.splitlines()[0:4]:
            list1 = set()
            words = nltk.ngrams(i.split(), 2)

            for j in words:
              pos = nltk.pos_tag(j)
              grm = r""""""origin:{(&lt;NN&gt;&lt;TO&gt;)|(&lt;NN&gt;&lt;VBG&gt;)|(&lt;VB&gt;&lt;NN&gt;&lt;TO&gt;)}
              time:{(&lt;CD&gt;&lt;NN&gt; ) | (&lt;CD&gt;&lt;NNS&gt;)}
              dest: {(&lt;VB&gt;&lt;NN&gt;&lt;CD&gt;) | (&lt;VB&gt;&lt;NN&gt;)}
              All:{(&lt;IN&gt;&lt;NN&gt;)|&lt;CD&gt;|&lt;NN&gt;|&lt;TO&gt;&lt;NN&gt;|&lt;NN&gt;&lt;NN&gt;&lt;CD&gt;} """"""
              chunkword = nltk.RegexpParser(grm)
              chuncked = chunkword.parse(pos)
              subtreelst = set()
              for subtree in chuncked.subtrees():                           
                if (subtree.label() == 'origin' and subtree.label() != 'S'):
                    subtreelst.add('origin: '+subtree.leaves()[0][0])
                if (subtree.label() == 'time' and subtree.label() != 'S'):
                    subtreelst.add('time: '+subtree.leaves()[0][0])
                if (subtree.label() == 'dest' and subtree.label() != 'S'):
                    subtreelst.add('dest: '+subtree.leaves()[0][0])
                if (subtree.label() == 'All' and subtree.label() != 'S'):
                   subtreelst.add('All: '+subtree.leaves()[0][0])
              list1.update(subtreelst)
            returnme.append(list1)
        returnme1.append(returnme)  


        return returnme1
    except Exception as e:
        print(str(e))


mylst = list()
mylst.append(process_content())
mylst
</code></pre>

<p>This is giving the following output:</p>

<pre><code>[[[{'All: 3',
    'All: book',
    'All: cab',
    'All: from',
    'All: hauz',
    'All: khaas',
    'origin: airport',
    'time: 3'},
   {'All: 6', 'All: hauz', 'All: khaas', 'origin: airport', 'time: 6'},
   {'All: 1',
    'All: 23',
    'All: PM',
    'All: book',
    'All: cab',
    'All: dwarka',
    'All: from',
    'All: hauz',
    'All: khaas',
    'All: sector',
    'origin: khaas',
    'time: 1'},
   {'All: 1', 'All: hauz', 'All: khaas', 'origin: airport'}]]]
</code></pre>

<p>LatentDirichletAllocation Part: </p>

<pre><code>    import pandas as pd
    import nltk
    from nltk.tokenize import word_tokenize
    from nltk.stem import PorterStemmer
    from nltk.corpus import stopwords
    from nltk.probability import FreqDist
    from sklearn.model_selection import train_test_split
    import re
    All_Reviews = pd.DataFrame(txtData.splitlines())
    def remove_non_alphabets(text):
        non_valid_word = re.compile(r'[-.?!,:;()""--``\[\]\|]')
        token = word_tokenize(text)
        return_me = list()
        for w in token:
            word= non_valid_word.sub("""",w)
            word= re.sub(r'^https?:\/\/.*[\r\n]*', '', word, flags=re.MULTILINE) # removed URLs
            word= re.sub("" \d+"", "" "", word) # remove digits 
            #word = re.sub('[^A-Za-z0-9]+', """", word)
            #word = re.sub(r'\[\[(?:[^\]|]*\|)?([^\]|]*)\]\]', r'\1', line)
            return_me.append(word)
        return return_me

    def dostopwords(text):
        return_me = "" "".join([c for c in text if c not in stopwords.words('english')])
        return return_me    
    #     return_me = list()
    #        # token = word_tokenize(text)
    #     for w in text:
    #         if w not in stopwords.words('english'):
    #             return_me.append(w)
    #     return return_me

    def counter(text):    
        fdist = FreqDist()
        for f in text:
            fdist[f.lower()] +=1
        return fdist
All_Reviews[0]= All_Reviews[0].apply(lambda lb: remove_non_alphabets(lb))
All_Reviews[0] = All_Reviews[0].apply(lambda lb: dostopwords(lb))
from sklearn.feature_extraction.text import CountVectorizer
CV = CountVectorizer(max_df=0.95, min_df=2,max_features=1000,ngram_range = (1,3),stop_words='english')
vect = CV.fit_transform(All_Reviews[0])
header = CV.get_feature_names()
from sklearn.decomposition import LatentDirichletAllocation
lda = LatentDirichletAllocation(n_components=5)
lda_output = lda.fit_transform(vect)
sorting = np.argsort(lda.components_)[:,::-1] 
features = np.array(CV.get_feature_names()) 
features
</code></pre>

<p>The output is:</p>

<pre><code>array(['10', '10 airport', '10 dwarka', '10 dwarka sector', '10 pm',
       '10 pm dwarka', '10 pm hauz', '11', '11 dwarka',
       '11 dwarka sector', '11 pm', '11 pm airport', '11 pm hauz', '12',
       '12 dwarka', '12 dwarka sector', '12 hauz', '12 hauz khaas',
       '12 pm', '12 pm airport', '12 pm dwarka', '12 pm hauz', '21',
       '21 10', '21 10 pm', '21 11', '21 11 pm', '21 12', '21 12 pm',
       '21 airport', '21 airport 10', '21 airport 11', '21 airport 12',
       '21 airport leaving', '21 airport pm', '21 dwarka',
       '21 dwarka sector', '21 hauz', '21 hauz khaas', '21 leaving',
       '21 leaving 10', '21 leaving 11', '21 leaving 12', '21 leaving pm',
       '21 pm', '23', '23 10', '23 10 pm', '23 11', '23 11 pm', '23 12',
       '23 12 pm', '23 airport', '23 airport 10', '23 airport 11',
       '23 airport 12', '23 airport leaving', '23 airport pm',
       '23 dwarka', '23 dwarka sector', '23 hauz', '23 hauz khaas',
       '23 leaving', '23 leaving 10', '23 leaving 11', '23 leaving pm',
       '23 pm', 'airport', 'airport 10', 'airport 10 pm', 'airport 11',
       'airport 11 pm', 'airport 12', 'airport 12 pm', 'airport dwarka',
       'airport dwarka sector', 'airport hauz', 'airport hauz khaas',
       'airport leaving', 'airport leaving 10', 'airport leaving 12',
       'airport leaving pm', 'airport pm', 'book', 'book cab',
       'book cab 10', 'book cab 11', 'book cab 12', 'book cab airport',
       'book cab dwarka', 'book cab hauz', 'book cab pm', 'cab', 'cab 10',
       'cab 10 airport', 'cab 10 dwarka', 'cab 10 pm', 'cab 11',
       'cab 11 dwarka', 'cab 11 pm', 'cab 12', 'cab 12 dwarka',
       'cab 12 hauz', 'cab 12 pm', 'cab airport', 'cab airport dwarka',
       'cab airport hauz', 'cab dwarka', 'cab dwarka sector', 'cab hauz',
       'cab hauz khaas', 'cab pm', 'cab pm airport', 'cab pm dwarka',
       'cab pm hauz', 'dwarka', 'dwarka sector', 'dwarka sector 21',
       'dwarka sector 23', 'hauz', 'hauz khaas', 'hauz khaas 10',
       'hauz khaas 11', 'hauz khaas 12', 'hauz khaas airport',
       'hauz khaas dwarka', 'hauz khaas leaving', 'hauz khaas pm',
       'khaas', 'khaas 10', 'khaas 10 pm', 'khaas 11', 'khaas 11 pm',
       'khaas 12', 'khaas 12 pm', 'khaas airport', 'khaas airport 10',
       'khaas airport 11', 'khaas airport 12', 'khaas airport leaving',
       'khaas airport pm', 'khaas dwarka', 'khaas dwarka sector',
       'khaas leaving', 'khaas leaving 10', 'khaas leaving 11',
       'khaas leaving 12', 'khaas leaving pm', 'khaas pm', 'kindly',
       'kindly book', 'kindly book cab', 'leaving', 'leaving 10',
       'leaving 10 pm', 'leaving 11', 'leaving 11 pm', 'leaving 12',
       'leaving 12 pm', 'leaving pm', 'pm', 'pm airport',
       'pm airport dwarka', 'pm airport hauz', 'pm dwarka',
       'pm dwarka sector', 'pm hauz', 'pm hauz khaas', 'sector',
       'sector 21', 'sector 21 10', 'sector 21 11', 'sector 21 12',
       'sector 21 airport', 'sector 21 dwarka', 'sector 21 hauz',
       'sector 21 leaving', 'sector 21 pm', 'sector 23', 'sector 23 10',
       'sector 23 11', 'sector 23 12', 'sector 23 airport',
       'sector 23 dwarka', 'sector 23 hauz', 'sector 23 leaving',
       'sector 23 pm', 'want', 'want airport', 'want airport dwarka',
       'want airport hauz', 'want book', 'want book cab', 'want dwarka',
       'want dwarka sector', 'want hauz', 'want hauz khaas'], dtype='&lt;U21')
</code></pre>
","nlp"
"74991","FastText Model Explained","2020-05-28 11:28:36","","3","360","<nlp><ngrams><fasttext>","<p>I was reading the FastText <a href=""https://arxiv.org/pdf/1607.01759.pdf"" rel=""nofollow noreferrer"">paper</a> and I have a few questions about the model used for classification. Since I am not from NLP background, some I am unfamiliar with the jargon. 
In the figure, what exactly is are the <span class=""math-container"">$x_i$</span>? I am not sure what <span class=""math-container"">$N$</span> ngram features mean. If my document has total <span class=""math-container"">$L$</span> words, then how can I represent the entire document using <span class=""math-container"">$N$</span> variables (<span class=""math-container"">$x_1$</span>,..,<span class=""math-container"">$x_n$</span>)? What exactly is <span class=""math-container"">$N$</span>? 
<a href=""https://i.sstatic.net/ZvWKZ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ZvWKZ.png"" alt=""enter image description here""></a></p>

<p><span class=""math-container"">$$-\frac{1}{N}\sum_{n=1}^Ny_n\log(f(BAx_n)) $$</span>
If <span class=""math-container"">$y_n$</span> is the label, then what sense does it make to multiply it with the output vector after softmax (lables would be like 0,1,2,3,.. )? Does the author mean we take the <span class=""math-container"">$y_n$</span>-th component of the output vector in loss calculation?</p>
","nlp"
"74955","Classify event announcement","2020-05-27 15:50:05","74959","2","29","<classification><nlp>","<p>I would like  train a classifier to spot news articles which would spot articles that announce an event. The issue is that I do not have a large pre-labeled dataset for this task (I only have 200 examples). So here are my two questions: </p>

<ol>
<li>Have you heard of a training set labeled for such task</li>
<li>I heard of few shot learning that can be helpful to train a classifier with few example would it be applicable to this case and is there any library/reading you would recommend.</li>
</ol>

<p>Thanks in advance</p>
","nlp"
"74954","Using word embeddings for kaggle?","2020-05-27 15:46:07","","0","115","<deep-learning><keras><nlp><word-embeddings><kaggle>","<p><a href=""https://i.sstatic.net/m37zM.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/m37zM.png"" alt=""enter image description here""></a>Not sure, if this is the right forum so redirect me if it wrong.</p>

<p>I have started on an NLP problem in kaggle. There i have word embeddings from google news, wiki, glove
in a zipped folder. I want to use one  of them, say glove, without unzipping the zipped file.
This is beecause if I try to unzip, it exceeds the 4.9 Gb space limitation and throws error and stops.</p>

<p>Any idea on how to deal with it?</p>

<p>I found out the way around for it.</p>

<pre><code>   import io
import zipfile

dim=300
embeddings1_index={}

with zipfile.ZipFile(""../input/quora-insincere-questions-classification/embeddings.zip"") as zf:
with io.TextIOWrapper(zf.open(""glove.840B.300d/glove.840B.300d.txt""), encoding=""utf-8"") as f:
for line in tqdm(f):
values=line.split(' ') # "".split(' ')"" only for glove-840b-300d; for all other files, "".split()"" works
word=values[0]
vectors=np.asarray(values[1:],'float32')
embeddings1_index[word]=vectors
</code></pre>
","nlp"
"74926","Extract First names from usernames","2020-05-27 06:46:23","","2","182","<python><nlp><text-mining>","<p>John10 , michaelscott, James.white , Jr.Jones , James-Anderson ,WhiteWalter10 -- These are some of the different cases of usernames possible(there may be more ). I have about 200K such usernames . I need to extract the first name from all of them. If only first name not possible then atleast only the relevant names (for example - WhiteWalter10 should give only Walter or White and Walter ). I already have a dataset of surnames so I can filter the first name</p>
","nlp"
"74918","How to use fine tuning of BERT when i have unlabelled dataset of text documents?","2020-05-27 03:24:06","","2","2904","<deep-learning><nlp><transfer-learning><bert>","<p>I have gained a basic understanding of using BERT for various NLP/text mining tasks. When it comes to fine-tuning of BERT, I always see that fine-tuning is performed using some classification tasks. So, how should I refine the word/sentence embeddings vector given by the BERT model in the case when I have a set completely unlabelled set of documents? I'm aware that the BERT model is originally trained on unlabelled data, so there must be some way.</p>
","nlp"
"74900","NLP+Non Text Features . How to give more weight to non NLP features?","2020-05-26 20:39:41","","0","204","<classification><nlp>","<p>I have some text and some metadata associated with it. Before classification I have made feature matrix in such way:  </p>

<pre><code>Non Text features + TFIDF vectorizer matrix  
</code></pre>

<p>So if had 5 non text features (like position of text, does text contain digits etc) and TFIDF matrix contains 100 columns then total columns fed to classifier is 105. I am using DecisionTree classifier. </p>

<p>Now, the problem is that, the classifier is using only features derived from text to make decisions. In some cases of mis-classifications I observed that the non text feature (position of text) was actually the decision factor.  </p>

<p>I think it is because the number of features generated by TFIDF vectorizer are too many (1800+). Any idea on how to make other features also contributor to classification.  </p>

<p>In case you want to see code :</p>

<pre><code>train_x =  train[[""Diff"",""col9"",""col10"", ""col6""]]  # Non text features
tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\w{1,}', use_idf=False, ngram_range=(1,3))
train_x1 = pd.concat([train_x, xtrain_tfidf_ngram_df], axis=1)  #Merge non text and text features
model.fit(train_x1, y=train_y)
predicted=model.predict(test_x1)
</code></pre>

<p>Sample training matrix after merge:</p>

<pre><code>     index  Diff  col9  col10  col6    0    1    2    3    4    5    6    7  ...  1875  1876  1877  1878  1879  1880  1881  1882  1883  1884  1885  1886  
1887
0        0 -2158     1      1     0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  
 0.0
1        1    15     1      1     1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  
 0.0
2        2    27     1      1     0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  
 0.0
</code></pre>
","nlp"
"74899","How to identify and extract patterns from emails","2020-05-26 20:32:21","","0","703","<machine-learning><python><scikit-learn><nlp>","<p>I would like to know if it would be possible to identify some patterns in a text.
For example, looking at emails, there is some common words used at the beginning and at the end. </p>

<pre><code>Dear Mr Pascal, 

We regret to inform you that we will not be able to respect the deadline previously agreed for the delivery of your order. Our supplier has warned us today that they are experiencing supply problems, which will result in a delay in our production chain. We count on your understanding and thank you for your patience. 

Please accept our apologies.

Best regards,

Matt
</code></pre>

<p>If I look at emails, they usually start with <code>Dear/Hi/Hello/Good morning...</code>, then the title/name of a person/company; the body; and the conclusion (<code>I look forward to hearing from you; kind regards, best regards;....</code>). 
I would like to ask you, therefore, if there is a way to collect information about these patterns and also if it is possible to classify emails by the patterns.</p>
","nlp"
"74879","Web Scraping: Multiple small files or one large file?","2020-05-26 13:38:03","74885","1","286","<python><nlp><preprocessing><web-scraping>","<p>I plan to scrape some forums (Reddit, 4chan) for a research project. We will scrape the newest posts, every 10 minutes for around 3 months. I am wondering how best to store the JSON data from each scrape, so that pre-processing (via <strong>Python</strong>) later would be as simple as possible. My options are the following:</p>

<ol>
<li>Dump data from each scrape into a fresh file (timestamp as filename). Resulting in 12,960 files of approx. 150kb each <em>OR</em></li>
<li>Maintain 1 single large file, whereby the scraper simply appends the new output to the end of the file. Resulting in 1 file of size approx 1.9Gb after 3 months</li>
</ol>

<p>Does anyone have any recommendations or warnings from their experience about either approach and how this affected pre-processing? I am cautions that having a pre-processing script work on a larger file might take longer to process but then again, opening and closing thousands of files will also be time-consuming.</p>
","nlp"
"74869","How exactly the hidden state works in an RNN ? How to decide on how many past instances to consider?","2020-05-26 10:33:43","","1","32","<neural-network><deep-learning><nlp><lstm><rnn>","<p>I am unable to grasp the working of RNN because in different tutorials, it is explained differently.</p>

<p>Please correct me as I have considered that:</p>

<p>In a <code>Many to one Model</code>: if a sentence is like <code>My name is Shady</code> becomes <code>Mon nom Shady</code> then how are the inputs were fed to model? IMO it was that <code>My</code> produced <code>Mon</code> by multiplying with <code>w0</code> and produced some hidden state as <code>h1</code> by multiplying with <code>a0</code>. Then (<code>h1</code> * <code>a1</code>)+ (<code>name</code> * <code>w1</code>) produced the <code>nom</code>. <strong>I want to ask that how is the state <code>h</code> changing? is <code>h_i</code> at the ith time a single float that the i+1_th input will use? If so,then why do they say that it uses  previous <code>N</code> states when it is using a <code>cumulative function of all the previous states</code> that have already occured???</strong></p>

<p><strong>And if the sentence is paragraph of 10000 words, will the last input use <code>cumulative function state</code> which is a state defined by all the previous 9999 inputs??</strong></p>
","nlp"
"74803","What is the concept of Normalized Mutual Information in the evaluation of Clustering?","2020-05-25 12:05:08","74859","1","4162","<machine-learning><nlp><clustering><probability><model-evaluations>","<p>I know what mutual information basically is but not quite sure about why and how it is used in the <strong>context of evaluation of clustering mechanisms</strong> ? Can someone please explain the intuition behind it ? 
i.e, how it is defined in the case of clustering evaluation ?  </p>
","nlp"
"74717","Generate text using user-supplied keywords","2020-05-23 17:35:17","","2","135","<deep-learning><nlp><language-model><text-generation><gpt>","<p>I've got a use case where I need to generate sentences based on a set of user supplied keywords. Here is an example of what I need:</p>

<h2>User input:</h2>

<p><strong>End-User:</strong> Data Scientists</p>

<p><strong>Region:</strong> Middle East</p>

<p><strong>Country:</strong> UAE</p>

<p><strong>Solution:</strong> BigPanda</p>

<p><strong>Application:</strong> machine learning</p>

<p><strong>Benefits:</strong> lower costs and runtime</p>

<h2><strong><em>Output (Curly-Brackets are just there to highlight):</em></strong></h2>

<p>Learn how {data scientists} in the {Middle East} such as in the {UAE} are using {BigPanda} to streamline their {machine learning} processes to {lower costs and runtime}.</p>

<hr>

<p>So the model needs to use the keywords given by the user and generate similar sentences. I also have a dataset of about 2000 of such sentences, which may come in handy.</p>

<p>One way I thought this could be possible is by using the GPT-2 model and perhaps finetuning it with the dataset, but I haven't been able to figure out how I would actually go about using it for something like this.</p>
","nlp"
"74688","How to use multiple text features for NLP classifier?","2020-05-23 08:30:24","","5","1752","<machine-learning><neural-network><deep-learning><classification><nlp>","<p>I am trying to build text classifier, Usually, we have one text column and ground truth. But I am working on a problem where dataset contains many text features. I am exploring different ways how to utilize different text features.</p>

<p>For example, my dataset looks like this </p>

<pre><code>Index_no                   domain  comment_by   comment       research_paper      books_name

01                         Science  Professor   Thesis needs  Evolution of         MOIRCS 
                                                more work     Quiescent            Deep 
                                                              Galaxies as a        Survey
                                                              Function of
                                                              Stellar Mass       



02                         Math    Professor   Doesn't follow  Evolution of   
                                               Latex format   Quiescent           nonlinear 
                                                              Galaxies as a       dispersive
                                                              Function of         equations
                                                              Stellar Mass             
</code></pre>

<p>This is just a dummy dataset, Here my ground truth (Y) is domain and features are <code>comment_by</code>, <code>comment</code>, <code>research_paper</code>, <code>books_name</code></p>

<p>If I am using any NLP model (RNN-LSTM, Transformers etc), those models usually take one 3 dim vectors, for that if I am using one text column that works but How to many text features for text classifier? </p>

<p>What I've tried :</p>

<blockquote>
  <p>1) <strong>Joining all column and making a long string</strong></p>
</blockquote>

<pre><code>Professor Thesis needs more work Evolution of Quiescent Galaxies as a Function of Stellar Mass MOIRCS Deep Survey  
</code></pre>

<blockquote>
  <p>2) <strong>Using a token between columns</strong></p>
</blockquote>

<pre><code>&lt;CB&gt; Professor &lt;C&gt; Thesis needs more work &lt;R&gt; Evolution of Quiescent Galaxies as a Function of Stellar Mass &lt;B&gt; MOIRCS Deep Survey 
</code></pre>

<p>where <code>&lt;CB&gt;</code> comment_by , <code>&lt;C&gt;</code> comment, <code>&lt;R&gt;</code> research_paper, <code>&lt;B&gt;</code> books_name</p>

<p>Should I use <code>&lt;CB&gt;</code> at the beginning or use like this?</p>

<pre><code>Professor &lt;1&gt; Thesis needs more work &lt;2&gt; Evolution of Quiescent Galaxies as a Function of Stellar Mass &lt;3&gt; MOIRCS Deep Survey
</code></pre>

<blockquote>
  <p>3) <strong>Using different dense layers (or embedding) for each column and
  concatenate them.</strong></p>
</blockquote>

<p>I've tried all three approaches, Is there any other approach I can try to improve the model accuracy? or extract, combine, join the better features?</p>

<p>Thanks in advance!</p>
","nlp"
"74680","Text analysis: structure and sentiment","2020-05-23 01:16:28","75884","0","37","<python><nlp><sentiment-analysis><nltk>","<p>I would need to analyse the structure of texts like this: </p>

<pre><code>******VIRUS ALERT****** ******VIRUS ALERT****** ******VIRUS ALERT ******

There is NEW VIRUS rapidly affecting computers on the internet. This new virus is insidious, in that it transmitted as a USENET message. Usenet is the ""news group"" area on the internet that users can openly discuss and exchange information on a wide variety of topics.

What makes this virus DOUBLY DANGEROUS, is that it is disguised as a common chain letter. Chain letters have been passed across usenet almost since it's beginning. Lately, a common chain letter subject is MAKE MONEY FAST.

The Make Money Fast (MMF) chain is read by thousands of people daily. It is also known as: ""Easy Cash"", ""Make Cash Fast"", ""Turn 5<span class=""math-container"">$ into $</span>50,000"" and many others. They are all basically the same scheme, in which the reader send $1 to each of the 5 people at the bottom of the list, then moves his name onto the list.

The MMF Virus, as it has been doubed, rides along on these chain letters as a ""hidden binary attachment"". Since most news reader programs (computer programs used to read USENET messages) will automatically decode and store binary attachments, there is NO SAFE WAY to protect yourself from infection.

The virus attackes your system the next time you run your news reader. Though the virus is transmitted during a normal usenet session, your NEXT usenet session will probably be your last for a while. As a hidden attachment, it is automatically activated with your news reader, and very quickly destroys your partition table. Generally, this is not even noticed until the next time you try to run ANY program.

The next thing the virus does is to place your micro processor into an nth-complexity infinate binary loop, quickly destroying it. This will appear at first as a normal ""lock-up"" but will quickley wipe out the delicate circuitry in your system.

At this point, your ONLY hope is to NOT DOWNLOAD ANY MESSAGES that have a subject similar to above. Please, FORWARD this message to ANYONE you know that reads usenet news.
</code></pre>

<p>In particular, I would be interested in grammar structure of each sentence (subject, verbs, object, and so on); specific pattern at the beginning of the text and at the end; and the sentiment. </p>

<p>Could you please give me any tips on which Python libraries I could use to do so?</p>

<p>Thanks</p>
","nlp"
"74640","spaCy - Text Preprocessing - Keeping ""Pronouns"" in text","2020-05-22 10:43:25","74645","1","785","<machine-learning><python><nlp><spacy>","<p>I am fairly new to machine learning and NLP in general. I am trying to wrap my head around how to do proper text pre-processing (cleaning the text).</p>

<p>I have built a custom text classification model. I have below method that I run on all input text, before serving it to my model. (both in training and testing).</p>

<p>The method will remove stopwords, punctuations and lemmatize the text.</p>

<pre><code>import spacy
from spacy.lang.en.stop_words import STOP_WORDS
import string

def normalize(text, lowercase, remove_stopwords, remove_punctuation):
    nlp = spacy.load(""en_core_web_sm"", disable=['parser', 'tagger', 'ner'])
    stops = spacy.lang.en.stop_words.STOP_WORDS

    if lowercase:
        text = text.lower()
    text = nlp(text)
    if remove_punctuation:
        text = [t for t in text if t.text not in string.punctuation]
    lemmatized = list()
    for word in text:
        lemma = word.lemma_.strip()
        if lemma:
            if not remove_stopwords or (remove_stopwords and lemma not in stops):
                lemmatized.append(lemma)

    return "" "".join(lemmatized)
</code></pre>

<p>Consider below input string:</p>

<p>Input: <code>You're very beautiful!</code></p>

<p>If I clean that text, using my method:</p>

<pre><code>test_text = ""You're very beautiful!""
test_text = normalize(test_text, lowercase=True, remove_stopwords=True, remove_punctuation=True)
</code></pre>

<p>It will return: <code>-PRON- beautiful</code></p>

<p>Is this a ""good"" way to clean the text? I notice that <code>-PRON-</code> is kept and therefore also used when training the model (same goes for when testing the model, as I use the same normalize method).</p>

<p>Should the <code>-PRON-</code> also be removed from the text? And should I complete even more text preprocessing?</p>
","nlp"
"74633","Question about removal of duplicates in NLP, when specifically working on twitter data","2020-05-22 08:05:50","","0","459","<machine-learning><nlp>","<p>Should we remove the duplicates before pre-processing or after pre-processing in NLP specifically when working on the twitter data? Actually main concern for me is that duplicates after pre-processing may have come from different tweets and so removing duplicates after pre-processing would be incorrect right? These are my thoughts, can confirm and give a clear insight about this?</p>
","nlp"
"74613","How to classify very short text for spend analytics?","2020-05-21 18:26:28","","1","18","<nlp><text-classification>","<p>I have a very small description of the item, example below-</p>

<ul>
<li>ACCELATOR SHAFT</li>
<li>ADAPTOR –EGR PIPE &amp; EGR VALVE(125KVA)</li>
<li>ADAPTOR-EGR COOLER AND VALVE-125 KVA</li>
<li>ADJUSTING LATCH - ALTERNATOR-125 KVA</li>
</ul>

<p>Some items are just one word long and maximum number of words in a sentence are around 6.</p>

<p>These items need to be classified into category and a sub category for further spend analysis.</p>

<p>This is a generic problem and many of our clients have similar problems and it takes a lot of time in classifying this manually so we are trying to develop a ML/NLP model to expedite this process.</p>

<p>For problem at hand we have around 2000+ unique items into 6 categories, each category having around 300 items.</p>

<p>We plan to tag around 100 items in each category and expect the model to classify the rest.</p>

<ul>
<li>How do we approach the problem, If someone already dealt with such a problem in past, guidance will be helpful</li>
<li>Does it make sense to go with tokenization/TFIDF approach with such a small description</li>
<li>What other Neural Network based methods we can look at (LSTM, RNN etc, but will 100 tags per category be sufficient for training)</li>
</ul>
","nlp"
"74576","How to group various similar search keywords and find top 100 keywords from big dataset","2020-05-21 09:18:13","","1","93","<machine-learning><python><nlp><data-science-model><word2vec>","<p>I have search keywords in one of my database table. These are the keywords searched by users on a website.
My requirement is to find the top 100 search keywords <strong>after consolidating various similar search keywords.</strong></p>

<p>e.g. COVID19, Covid 19, Corona, Corona virus, covid-19 are all related to corona and it should be grouped under <strong>Corona</strong>.</p>

<p>OR</p>

<p>blockchain, BLOCKCHAIN, Blockchain business, B L O C K C H A I N are all related to Blockchain so it should be grouped under <strong>Blockchain</strong> category.</p>

<p>I tried to do this using fuzzywuzzy python library but fuzzywuzzy needs a list to be compared with the search keywords. 
And unfortunately I don't have that list because it can be anything based on user's search. </p>

<p>Any suggestions on the approach would really help.</p>
","nlp"
"74515","Existing pre-trained NLP models to detect if a text input is a question","2020-05-20 10:15:37","","3","8495","<python><nlp><transfer-learning>","<p>I would like to quickly filter text data into question and non-questions. Using the presence of question mark in the text is too crude. Are there any existing models I can use to aid me with my task?</p>
","nlp"
"74513","Identify same product","2020-05-20 09:45:10","","0","308","<machine-learning><nlp><machine-learning-model>","<p>I am new to ML and still learning it.</p>

<p>My problem is to identify duplicate products. I have a dataset containing product details such as name, colour, size, description, features etc (there are roughly 70 columns).</p>

<p>I need to remove duplicate products.</p>

<p>I just completed some of the supervised ML model(classification and regression) and unsupervised clustering(K means and HC). I am also on the way of learning w2v and d2v.</p>

<p>But due to time constrain, I need to deliver a solution to the above problem statement. I am unsure as to how to proceed.</p>

<p>Any help and guidance would be appreciated </p>
","nlp"
"74503","Is the number of bidirectional LSTMs in encoder-decoder model equal to the maximum length of input text/characters?","2020-05-20 05:10:08","","0","159","<lstm><word-embeddings><nlp><rnn><attention-mechanism>","<p>I'm confused about this aspect of RNNs while trying to learn how seq2seq encoder-decoder works at <a href=""https://machinelearningmastery.com/configure-encoder-decoder-model-neural-machine-translation/"" rel=""nofollow noreferrer"">https://machinelearningmastery.com/configure-encoder-decoder-model-neural-machine-translation/</a>.</p>

<p>It seems to me that the number of LSTMs in the encoder would have to be the same as number of words in the text (if word embeddings are being used) or characters in the text (if char embeddings are being used). For char embeddings, each embedding would correspond to 1 LSTM in 1 direction and 1 encoder hidden state. </p>

<ol>
<li>Is this understanding correct? </li>
</ol>

<p>E.g. If we have another model that uses encoder-decoder for a different application (say text-to-speech synthesis described here <a href=""https://ai.googleblog.com/2017/12/tacotron-2-generating-human-like-speech.html"" rel=""nofollow noreferrer"">https://ai.googleblog.com/2017/12/tacotron-2-generating-human-like-speech.html</a>) tha uses 256 LSTMs in each direction of the bidirectional-encoder, does it mean the input to this encoder is limited to 256 characters of text?</p>

<ol start=""2"">
<li>Can the decoder output has to be same length as the encoder input or can it be different? If different what factor describes what the decoder output length should be?</li>
</ol>
","nlp"
"74455","Finding gender affinity for businesses","2020-05-19 11:57:34","74457","0","29","<nlp><machine-learning-model><probability>","<p>What are the different models I can use to find the gender affinity of businesses using yelp dataset-- <a href=""https://www.kaggle.com/yelp-dataset/yelp-dataset"" rel=""nofollow noreferrer"">https://www.kaggle.com/yelp-dataset/yelp-dataset</a> . I need to find Probablity (Male buying from a merchant) and Probablity (Female buying from a merchant) where both probablities add to 1 . There is no information about gender so I can use the Genderize api to find gender using names of users. This is an unsupervised problem</p>
","nlp"
"74413","Ideas to check if a given sentence A is the response to a query B","2020-05-18 18:19:48","","0","711","<machine-learning><nlp><similarity>","<p>I've a sentence 'A' which is a possible response to one of the x number of questions. I need to compare the response with each sentence to see which question's response sentence 'A' is or not a response at all. Any ideas on how to go about this. Pure nlp based solution will be helpful as there are no existing labelled data for the same.</p>
","nlp"
"74367","Natural Language Processing: Identifying Words That Are Out of Place?","2020-05-18 01:28:59","","0","41","<machine-learning><nlp><audio-recognition>","<p>I want to make it easier to manually (or potentially automatically) correct AI transcription. I've noticed that one significant error that can be very easily picked up by human transcriptionists is words that are just out of place in a sentence.</p>

<p>I am wondering if there is a system (algorithmic or something complete like a node.js package) that can score words for out-of-placeness in a group of paragraphs. I imagine it would involve a lot of algorithmic learning.</p>
","nlp"
"74362","How to separate text lines from txt files in python?","2020-05-17 21:35:57","","0","82","<nlp><pandas><preprocessing><python-3.x>","<p>I am playing with a <code>.txt</code> file in Python for EDA and I want to separate the lines from this fashion:</p>
<pre><code>Corona Virus = Justin Piper

COVID19 = Piper, Justin
Fed Paper Says Companies May Automate More Jobs During Pandemic https://t.co/SUqrMfpNJY

Donate the money 
Might put food on plate 
And smile on face 😊
</code></pre>
<p>to this fashion:</p>
<pre><code>[['Corona Virus = Justin Piper'],['COVID19 = Piper, Justin
Fed Paper Says Companies May Automate More Jobs During Pandemic https://t.co/SUqrMfpNJY'],['Donate the money 
Might put food on plate 
And smile on face 😊']]
</code></pre>
<p>I tried <code>.split('\t')</code> but it is taking each line as a new row.</p>
","nlp"
"74228","Organization finder in spaCy","2020-05-15 08:17:22","74230","1","196","<machine-learning><python><nlp><named-entity-recognition><spacy>","<p>I am trying to find the parties between which the agreement is executed from the statement below.</p>
<blockquote>
<p>AGREEMENT, dated as of January 10, 2000, is entered into by and between ABC-EFG GROUP Inc. having an address at 418 Mona Drive, Prominade 34, Florida 34673, United States of America and Rob Cummins, an individual residing in the state of Florida, and having an address at 13 test Dr, Arosa, FL 43566</p>
</blockquote>
<p>To get this, I tried the following code:</p>
<pre class=""lang-py prettyprint-override""><code>for chunk in doc.noun_chunks: # after loading &quot;nlp = spacy.load(&quot;en_core_web_sm&quot;)&quot; and doc = nlp(&quot;string&quot;)
   print(chunk.text, chunk.root.text, chunk.root.dep_,chunk.root.head.text)
</code></pre>
<p>which gives me:</p>
<pre><code>AGREEMENT AGREEMENT nsubjpass entered
January January pobj of
ABC-EFG GROUP Inc. Inc. pobj between
an address address dobj having
418 Mona Drive Drive pobj at
United States States conj Drive
America America pobj of
Rob Cummins Cummins conj America
an individual individual appos States
the state state pobj in
Florida Florida pobj of
an address address dobj having
13 test test pobj at
Dr Dr ROOT Dr
FL FL appos Arosa
</code></pre>
<p>I don't understand how I am supposed to find the two parties from this output.</p>
<p>I am not specific to spaCy. A suggestion of any other model or method also will work.</p>
<p><strong>Please note that I have already tried NER but the results are very poor.</strong></p>
","nlp"
"74208","Removing duplicate records before training","2020-05-14 18:47:36","","1","418","<nlp><overfitting><fasttext>","<p>I am currently working on a project classifying text into classes. The specific problem is classifying job titles into various industry codes. For example ""McDonalds Employee"" might get classified to 11203 (there are a few hundred classes in the problem). For this we are using FastText.</p>

<p>The person that I am working with insists on removing duplicate records from the data before training our model. That is, we might see 100 records with ""McDonalds Employee"" and class 11203 and he wants to remove all but one of them. His argument is that not doing so could result in over-fitting and an optimistic error rate as the same records will appear in all the train/test/validation sets. My counter to this is that I expect to see (many) records with ""McDonalds Employee"" in our future data and I would like to know how the model is going to do at predicting this, hence we will not arrive at an optimistic error rate but a properly calculated error rate. Secondly, if our data for some reason has one record ""McDonalds Employee"" with class 24444, removing the duplicates removes all evidence that the correct code is 11203.</p>

<p>I have read other posts here that suggest removing duplicates is not correct, but I have yet to see an actual source in the literature stating this. Since I have to convince a colleague my question is two fold: Does anyone know of any reference in the literature that suggests keeping duplicates? As well, is there any reason to remove duplicates specific to FastText? I admit I am not that familiar with NLP and FastText (or even neural networks in general), so it is possible maybe there is some reason to remove them when training a model of this type.</p>
","nlp"
"74148","Which words are causing wrong prediction","2020-05-13 20:06:08","","1","17","<classification><nlp>","<p>I have a simple multiclass text classification code which uses -<br>
split words -> remove stop words ->apply TFIDF vectorizer -> RandomForest (or LinearRegression or NaiveBayes) </p>

<p>During prediction some sentences which are similar to trained sentences but have some extra words are causing low confidence for a class prediction.<br>
I want to automatically identify those words which contributed to reducing the confidence score. 
Any advice?</p>

<p>Example (2 class prediction): 
I want to know status of this task -> Status Update (0.98 score)
I am working since morning and during and discovered that task status is incomplete so spent extra hours to complete the work. -> Status Update (0.6 score), TaskFinish (0.4 score)</p>

<p>In above hypothetical example, in second sentence, words like ""complete"" tried to change the class and words like ""working"", ""discovered"" etc which are not useful words causing low score for status update. These words I want to identify. </p>
","nlp"
"74144","How to prepare data for Named Entity Recognition with BIO annotation?","2020-05-13 19:23:39","","1","1210","<machine-learning><deep-learning><nlp><named-entity-recognition>","<p>Assume the task here is extracting important facts for resume like a candidate skills and his education etc.., Here is resume is parsed text from pdf or docx resume.</p>

<p>First, I'll obtain skills and education data from various online websites, job portals etc.., the obtaining data will be two text files and in each file every row represent a skills or a university name like below,</p>

<pre><code>skills.txt 
___________
c 
python 
java
node js


education.txt 
___________
massachusetts institute of Technology 
harvard university 
</code></pre>

<p>I want to know if these be enough to be able to train a named entity recognition model to recognize skills and education for raw resume text. The data I have is not sentences but just entities. I've read somewhere that we require some context along with the entity for NER model to learn better. Like this example below,</p>

<pre><code>skills.txt 
___________
c is used at facebook
python is my favorite programming language
</code></pre>

<p>If I use my collected data education.txt to train the modeel and BIO annotate them then it will be like below, it won't have O-Other token.</p>

<pre><code>massachusetts B-EDU
institute I-EDU
of I-EDU
Technology I-EDU

harvard B-EDU
university I-EDU

indian B-EDU
institute I-EDU
of I-EDU
technology I-EDU
</code></pre>

<p>But I don't know how to access such data for my resume-extraction problem. How do I proceed further?  How to build an effective NER model for my resume facts identification domain-specific task? Any inputs/suggestions would really help. </p>
","nlp"
"74115","Is BERT a language model?","2020-05-13 12:22:22","74119","9","3853","<nlp><bert><transformer><language-model>","<p>Is BERT a language model in the sense of a function that gets a sentence and returns a probability?
I know its main usage is sentence embedding, but can it also provide this functionality?</p>
","nlp"
"74084","Q&A answer comparison multiple sentences using","2020-05-13 00:48:50","","2","56","<machine-learning><nlp>","<p>I have been working on a Q&amp;A app that has a template of questions and answers. The hope is to take answer text from the user, and compare it to the correct answer. I’d like to weight it on the keywords/buzzwords and general accuracy of the response. </p>

<p>I’m a mobile developer, not quite a data scientist or well versed in machine learning, so I’m a bit lost in the woods here and not sure if I’m going down the right path.</p>

<p>I’ve been looking into natural language process and some related python libraries/models that evaluate sentences.    </p>

<p>Am I headed in the right direction? or should I go about this another way?</p>
","nlp"
"74006","Context capturing in a Structured PDF?","2020-05-12 02:59:35","","0","43","<machine-learning><deep-learning><nlp><feature-extraction><ocr>","<p>I'm trying to extract resume (PDF) data. resumes always tend to follow a structure. so if you see some numbers in a cv; according to the context, we could tell whether its a telephone number, a birthday, or a date period. if I can classify/identify one entity then that would increase my ability to classify an entity near to it.</p>

<p>I'm still a newbie and appreciate if anyone could give me any thoughts on approaching this problem. what kind of machine learning models should i focus on?</p>
","nlp"
"73792","Are there any open-source text annotation for multi label classification tools?","2020-05-08 09:07:38","","2","462","<deep-learning><nlp><text><annotation>","<p>I have a large texts in each document and I want to know if there are any open source text annotation tools available online for multiple label annotation.</p>

<p>Each sentence takes two labels. If there are any please let me know.</p>
","nlp"
"73783","Address parsing using spaCy","2020-05-08 06:53:47","73785","7","7133","<machine-learning><nlp><spacy>","<p>I am trying to parse addresses from various documents using spaCy using NER but the results are not so accurate.</p>
<p>I know this is bit generic question but it would be a great help if I could get reference of any past work or good articles or techniques to apply to this.</p>
","nlp"
"73778","Why dont we use 2d cnn filters for Nlp tasks?","2020-05-08 00:44:19","","2","84","<deep-learning><cnn><nlp>","<p>CNNs are used in NLP for various tasks. But I cannot find a clear understanding of why do we only use 1d filters in these networks? </p>
","nlp"
"73749","Weighting of words in lexicon based sentiment analysis","2020-05-07 16:15:46","","3","442","<nlp><sentiment-analysis><nltk>","<p>I am trying to do a lexicon based sentiment analysis on my data, where I calculate the sentiment score as follows:</p>
<p><span class=""math-container"">$$ Score = \frac{\sum_{i}{word_i}}{\mid words \mid} $$</span></p>
<p>So according to the score the word will be classified in either negative or positive. But I have also calculated for every word in the article its salience and frequency:</p>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>words</th>
<th>salience</th>
<th>frequency</th>
</tr>
</thead>
<tbody>
<tr>
<td>sad</td>
<td>0.8</td>
<td>3</td>
</tr>
<tr>
<td>happy</td>
<td>0.5</td>
<td>2</td>
</tr>
</tbody>
</table></div>
<p>Is it possible to use them in my sentiment analysis formula?</p>
","nlp"
"73733","How to segregate resume layouts into different types?","2020-05-07 12:20:23","","0","43","<deep-learning><nlp><text-mining><ocr><parsing>","<p>I'm looking for any suggestions on how to segregate resume layout into different types.</p>

<p>How do one proceed with such a task? I mean resumes are usually available as pdf or docx format and when we parse text from documents we lose a lof of information regarding layout or metadata. </p>

<p>So how one could build a system to segregate resumes based on layouts.</p>

<p>It'll be really helpful if you have any suggestions.</p>
","nlp"
"73679","Deriving answers to specific queries from a text","2020-05-06 14:03:51","","0","29","<nlp><automatic-summarization>","<p><strong>Introduction</strong></p>

<p>I am looking to extract out sentence(s) from a news article for questions like 'who', 'when', 'what', 'why' and 'how'. Now I did some research and found <em>bert model</em> which can be utilised to make query based summarizer. But it was not satisfactory as it sentences extracted were small and wrong by huge margins. It makes sense as it was designed to answer full questions and not something like just 'when'.</p>

<p><strong>Spacy</strong></p>

<p>I knew about spacy and from that I was able to guess answers for 'who' and 'when'. Also 'what' isn't difficult as it can be obtained from description or first few sentences.</p>

<p><strong>Problem</strong></p>

<p>Now I have no idea for 'why' and 'how'. Only thing I can think of is finding words that are used to describe when we give reason to something (why) and  words that describe a  process(how). But again many things become subjective here as it depends on the writer. What is best way possible right now for achieving this, or anything closest to it?</p>

<p><strong>Examples</strong></p>

<p><a href=""https://drive.google.com/file/d/1ejHc3sFZNYKPnmQZhX34R0lY93NAj6Vh/view?usp=sharing"" rel=""nofollow noreferrer"">https://drive.google.com/file/d/1ejHc3sFZNYKPnmQZhX34R0lY93NAj6Vh/view?usp=sharing</a></p>
","nlp"
"73645","How to deal with imbalanced text data","2020-05-06 03:19:33","","1","359","<classification><nlp><class-imbalance><text-classification>","<p>I am working on a problem where I have to classify products into multiple classes (more than one) based on product descriptions. For instance:</p>
<p>&quot;Tresemme shampoo and conditioner - sulfate-free&quot; = Personal Hygiene<br />
&quot;Lavender-scented handwash with moisturizer&quot; = Personal Hygiene<br />
&quot;Doritos Ranch flavor 18 oz mega party pack&quot; = Snacks<br />
&quot;Painting and Craft kit for adults above 18&quot; = Art and Craft</p>
<p>However, my training dataset is highly imbalanced. A few classes have only 10 records while there is one that has 3,000 records. 50,000 records overall.</p>
<p>Can anyone suggest any good techniques to deal with the imbalance in text data?</p>
<p>Thanks,
GD</p>
","nlp"
"73594","Is there any way to calculate a relevance score between a title and the content of a text?","2020-05-05 12:06:34","","2","551","<algorithms><nlp>","<p>My question might sound a little bit stupid but I am trying to come up with a way to measure the relevance of the title of a text, let's say a piece of news headline, to the content. My idea would be to work with word2vec and have a go with cosine similarity. Are there any more efficient and proper ways to work with this task?</p>
","nlp"
"73593","Building a text classification model from scratch","2020-05-05 12:03:28","","1","204","<python><neural-network><nlp><text-classification>","<p>I am a beginner in data science and machine learning techniques. I would need to build a model that allows me to classify texts based on sentiment analysis.
Right now I only have the text and they miss any class nor
any information about sentiment analysis.</p>
<p>Data collected (texts) are approximately 50000 and they are already cleaned of punctuation and stop words.
I heard about the possibility to build some neural networks or use a logistic regression, but I do not know anything about specific models to use for that.</p>
<p>Furthermore, I might consider to build a new model from scratch (I know that can take ages and a lot of efforts) but I'd like to know what I need (for example, already existing model/dataset to train with dictionaries and sentiment analysis).
The problem is that what I would like to do is to classify a text as positive or negative (sentiment); also, classify it as fake or not fake.</p>
<p>Do you have any suggestion or advice?
If you need further information, I would be happy to provide them.</p>
","nlp"
"73518","how does word variety depend on total words?","2020-05-04 13:20:51","73524","1","24","<nlp>","<p>I want to compare the word variety of several books. But some are short, while others are long. So how can I correct for the fact that longer books will generally contain a larger number of unique words. I tried simply dividing the number of unique words in each book by the number of total words in each book. But I think that was overdoing it, because now the shorter books appear to have the most variety. What is the best approach to this?</p>
","nlp"
"73486","How to Calculate semantic similarity between video captions?","2020-05-04 05:45:58","","0","192","<python><nlp><word-embeddings><tokenization><semantic-similarity>","<p>I intend to calculate the accuracy of a caption generated by comparing it to a number of reference sentences.</p>

<p>For example, the captions for one video are as follows:
These captions are for the same video only. However, reference sentences have been broken down with respect to different segments of a video.</p>

<p>Reference sentences (R):</p>

<pre><code>A man is walking along while pushing his bicycle.
He tries to balance himself by taking support from a pole.
Then he falls on the sidewalk along with the pole and the bicycle with him.
</code></pre>

<p>Candidate Caption generated (C):</p>

<pre><code>A person is trying to use a pole to push off his bike ride but ends up falling down.
</code></pre>

<p>I want to calculate a similarity score between each pair. 
That is, <code>(R1,C), (R2, C) and (R3, C)</code> </p>

<p>What is the best method?</p>

<p>I tried using TF-IDF and then Cosine similarity. However, that only got the word matching. I want lexical and semantic accuracy between these sentences to estimate how accurately the sentence C has been written.</p>

<p>You can refer the code I have done till now <a href=""https://github.com/Naomi98/Major-Project/blob/master/Tfidfcosineonly.ipynb"" rel=""nofollow noreferrer"">here</a></p>

<p>I understand I need to tokenize, do word embedding, semantic analysis and then some similarity metric but not sure? In which order and which algorithm is best suited for which?</p>
","nlp"
"73473","Intuition of LDA","2020-05-03 18:41:40","73483","1","424","<machine-learning><nlp><unsupervised-learning><topic-model><lda>","<p>Can anyone explain how the LDA-topic model assigns words to topics? 
I understand the generative property of the LDA model but how does the model recognize that ""Labrador"" and ""dog"" are similar words/ in the same cluster/topic? Is there kind of a similarity measure? The learning parameters of LDA are the the assignment of words to topics, the topic-words probabilities vector and the document-topic probabilities vector. But HOW is it learned? </p>
","nlp"
"73408","What information does output of [SEP] token captures in BERT?","2020-05-02 12:46:52","73775","2","2605","<nlp><word-embeddings><bert>","<p>After reading around on the web I came to understand that the output representation of the special token [CLS] captures the representation of a sentence (am I correct?).</p>

<p>My primary question is what information does the output embedding of [SEP] token (T_SEP) captures?</p>

<p>My other doubt is if I input a bunch of sentences into BERT separated by [SEP] does the output embedding of [CLS] contain information about all the sentences?</p>
","nlp"
"73338","Selecting most relevant word from lists of candidate words","2020-05-01 08:28:26","73340","2","55","<machine-learning><nlp>","<p>Let's suppose I have 1000's of training examples where each consists of a bucket e.g. <code>'engineering'</code> or <code>'management'</code> and a list of tags e.g. <code>['software', 'python', 'product']</code> where a human has selected the most relevant tag for the use case e.g.<code>'software'</code>.</p>

<p>So our data is like:</p>

<pre><code>bucket         tags                        best_tag
engineering    [fullstack, software]       software
engineering    [java, python, software]    software
management     [technical, product]        product
</code></pre>

<p>What kind of model or approach would suit taking a list of tags and predicting the best tag based on some kind of underlying latent hierarchy?</p>
","nlp"
"73325","Text Mapping - Medicine Names","2020-05-01 04:57:50","","0","94","<nlp><text-mining><text-classification>","<p>We have a problem where we have a standardized database of Medicine names.  On the other hand, there is a subset of medicine names which could have spelling mistakes, different structure or hypens, missing words etc.  There is also some metadata available, like manufacturer name, unit size etc.</p>

<p>Human can easily map those two database with each other.  We have used some string comparison and created some probabilistic scoring and in some cases it serves the purpose.</p>

<p>But lot of times we are running into lot of nuanced issues and conditions are keep getting piled up.  Is there any idea if any machine learning type of algorithm can help?  I have basic understanding of all major algorithms but yet I am drawing blank for this problem.  Simple example is mapping Epilex 300mg tab with Epilex 300 Tablet.  I can give more examples if needed.</p>
","nlp"
"73260","How to convert subword PPL to word level PPL?","2020-04-29 20:09:43","","1","26","<nlp><pytorch><bert><language-model>","<p>I'm using this formula to covert subword perpexity to word perplexity:
<code>PPL_word = exp(log(PPL_subword) * num_subwords / num_words)</code>
The question is do I need to include the [SEP] and [CLS] tokens when counting subwords?</p>
","nlp"
"73254","Extracting domain specific terms from a huge hard-coded list from a text","2020-04-29 18:08:47","","1","22","<python><nlp><information-extraction>","<p>I know. The title sounds like I haven't googled my problem, but trust me, I did. Maybe my problem has a name and I haven't found it yet. Hoping you can help me wrap my head around it.</p>

<p>What I want to do is, given a text, extract all terms from a specific domain. For simplicity let's say, given a list of hard-coded animals, I want my model to extract from an input text all of the animals that are present in the list. Why would I use IA for this? Well e.g. I want to distinguish negations, so I don't want to extract ""lion"" if the text says ""no it a'int a lion m'am"", also I want it to extract ""lion"" if the text says ""lioness"", without having to make a huge list of synonyms.</p>

<p>I think my problem is kind of like NER, but not exactly, right? I don't just want to say ""lioness"" = animal, I want it to map it to ""lion"" also. So this is basically a classification problem, right? I want it to label all of the animals found in the text, basically label ""lioness"" = lion. But the problem is I may have 100k+ thousands of possible labels, so should I just train a classification model with 100k+ possible outputs? Doesn't seem right.</p>

<p>Thanks in advance for any feedback.</p>
","nlp"
"73223","Why is n-grams language independent?","2020-04-29 12:28:18","73227","0","63","<nlp><stanford-nlp><vector-space-models><ngrams>","<p>I don't understand how n-grams are language independent. I've read that by using character n-grams of a word than the word itself as dimensions of a vector space model, we can skip the language-dependent pre-processing such as stemming and stop word removal. </p>

<p>Can someone please provide reasoning for this?</p>
","nlp"
"73189","Does BERT use GLoVE?","2020-04-28 21:23:47","73276","7","5281","<nlp><bert><transformer><attention-mechanism>","<p>From all the docs I read, people push this way and that way on how BERT uses or generates embedding. I GET that there is a key and a query and a value and those are all generated.</p>

<p>What I don't know is if the original embedding - the original thing you put into BERT - could or should be a vector. People wax poetic about how BERT or ALBERT can't be used for word to word comparisons, but nobody says explicitly what bert is consuming. Is it a vector? If so is it just a one-hot vector? Why is it not a GLoVE vector? (ignore the positional encoding discussion for now please)</p>
","nlp"
"73184","Problem with the hub.load function in tensorflow ( TypeError: Autotrack object is not callable)","2020-04-28 20:48:36","","2","1162","<python><deep-learning><tensorflow><nlp>","<p>I have a problem with loading pretrained module for an NLP task and the problem is because of the tf migration I suppose. Tensorflow website says that the problem might be sorted if the signature variable is given correctly. Can you help me correct this code ?</p>

<p>TypeError: 'AutoTrackable' object is not callable</p>

<p>[code]</p>

<pre><code>import tensorflow_hub as hub
# enabling the pretrained model for trainig our custom model using tensorflow hub
module_url = ""https://tfhub.dev/google/universal-sentence-encoder-large/3""
embed = hub.load(module_url)

# creating a method for embedding and will using method for every input layer 
def UniversalEmbedding(x):
    return embed(tf.squeeze(tf.cast(x, tf.string)), signature='default', as_dict=True)[""default""]
</code></pre>
","nlp"
"73166","What are some of the available methods for handling multi-label classification for longer sequences of text","2020-04-28 15:04:49","","1","34","<machine-learning><neural-network><deep-learning><nlp><text-classification>","<p>I am looking to solve a multi-class classification problem with long sequences of text with some rows having 1000's of tokens. Some of the state of the art methods such as BERT have a token limit and I was wondering what is currently being done to handle longer text sequences when dealing with classification? </p>
","nlp"
"73110","Measuring the success of text summarization","2020-04-27 21:47:55","73124","0","54","<nlp><automatic-summarization>","<p>I am trying to make a text summarization program that will take a text article and reduce it to a para or 2.</p>

<p>Since I am a newbie with no idea of NLP, it is hard to approach and break down the problem. So I was wondering if there was a measure that is used to check for effectiveness and correctness of text summarization. I tried googling this, but nothing that suits my purpose.</p>

<p>Does something like this even exist? or am I going in the wrong direction?</p>
","nlp"
"73077","Imbalanced text classification by oversampling: correction of class predicted probability by prior probability","2020-04-27 12:15:40","","2","150","<deep-learning><nlp><class-imbalance><probability-calibration>","<p>My dataset has 3 class and 900 examples for training. Class distribution is 255, 185, and 460.</p>

<p>I found that if I oversample (random) the training data then I have to correct/calibrate the predicted probability of the test data because after oversampling the training and testing data distribution are not same. This is nicely described <a href=""https://www.researchgate.net/profile/Marco_Saerens/publication/11608620_Adjusting_the_Outputs_of_a_Classifier_to_New_a_Priori_Probabilities_A_Simple_Procedure/links/55a76ebb08ae92aac77f8783/Adjusting-the-Outputs-of-a-Classifier-to-New-a-Priori-Probabilities-A-Simple-Procedure.pdf"" rel=""nofollow noreferrer"">here</a>, <a href=""https://www.knime.com/blog/correcting-predicted-class-probabilities-in-imbalanced-datasets"" rel=""nofollow noreferrer"">here</a> and <a href=""http://blog.data-miners.com/2009/09/adjusting-for-oversampling.html"" rel=""nofollow noreferrer"">here</a></p>

<p>I have 4 questions:</p>

<p>Should we do this to calculate:</p>

<p><strong>Training loss</strong></p>

<p>My guess: We should not change the posterior/predicted probability for training loss calculation. Because that will cancel out the effect that has been created by oversampling. Am I right?</p>

<p><strong>validation loss</strong>
My guess: Since the validation loss has no effect on training (optimization), we can correct the predicted probabilities but then it would not be comparable with the training loss. Since we compare both losses to check overfitting, we should not do any probability correction here. Unless we compute two kind of training losses. One is without probability correction which would be used to compute the gradient and another corrected training loss which would be used to plot with corrected validation loss. Then these two losses would be comparable. Am I right?</p>

<p><strong>validation accuracy</strong> </p>

<p>My guess: For validation accuracy, I think we should do it for the same reason we have to do it on testing data. Right?</p>

<p><strong>Is this a mandatory step</strong>? I am asking this because this might hurt the overall accuracy. Because this will penalize the probabilities of the classes which have less example.</p>
","nlp"
"72912","Keras: Vector regression: ValueError:can not squeeze dim[1]","2020-04-24 14:38:55","","1","1078","<keras><tensorflow><regression><nlp>","<p>I'm trying to create a NLP model which takes <code>x_train_padded_2</code> (padded/tokenized text sequences) as input and try to approximate <code>Y_train_embedding_2</code> (dense embedded sentences).</p>

<p>Input/target types and dimensions:</p>

<pre><code>print(x_train_padded_2.shape)
print(type(x_train_padded_2[0][0]))
(6960, 50)
&lt;class 'numpy.int32'&gt;
</code></pre>

<pre><code>print(Y_train_embedding_2.shape)
print(type(Y_train_embedding_2[0][0]))

(6960, 16)
&lt;class 'numpy.float32'&gt;
</code></pre>

<p>The model architcture (the embedding layer is pre-trained):</p>

<pre><code>model_2 = tf.keras.Sequential([
    embedding,
    tf.keras.layers.GlobalAveragePooling1D(),
    tf.keras.layers.Dense(16, activation='relu'),
    tf.keras.layers.Dense(embedding_dim)
])
embedding.trainable = False
model_2.compile(loss='mse',optimizer='adam',metrics=['mse'])

history_2 = model.fit(x_train_padded_2, Y_train_embedding_2, epochs=num_epochs, validation_split=0.1, callbacks=[earlyStopping], verbose=1)

</code></pre>

<pre><code>model_2.summary()

Model: ""sequential_1""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, 50, 16)            16000     
_________________________________________________________________
global_average_pooling1d_1 ( (None, 16)                0         
_________________________________________________________________
dense_2 (Dense)              (None, 16)                272       
_________________________________________________________________
dense_3 (Dense)              (None, 16)                272       
=================================================================
Total params: 16,544
Trainable params: 544
Non-trainable params: 16,000
_________________________________________________________________
</code></pre>

<p>I get the following error:</p>

<pre><code>ValueError: in user code:

    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:571 train_function  *
        outputs = self.distribute_strategy.run(
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica
        return fn(*args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:543 train_step  **
        self.compiled_metrics.update_state(y, y_pred, sample_weight)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/compile_utils.py:406 update_state
        metric_obj.update_state(y_t, y_p)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/metrics_utils.py:90 decorated
        update_op = update_state_fn(*args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/metrics.py:603 update_state
        matches = self._fn(y_true, y_pred, **self._fn_kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/metrics.py:3244 sparse_categorical_accuracy
        y_true = array_ops.squeeze(y_true, [-1])
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:180 wrapper
        return target(*args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py:507 new_func
        return func(*args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py:4145 squeeze
        return gen_array_ops.squeeze(input, axis, name)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py:9875 squeeze
        ""Squeeze"", input=input, squeeze_dims=axis, name=name)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:744 _apply_op_helper
        attrs=attr_protos, op_def=op_def)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py:595 _create_op_internal
        compute_device)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:3327 _create_op_internal
        op_def=op_def)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:1817 __init__
        control_input_ops, op_def)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:1657 _create_c_op
        raise ValueError(str(e))

    ValueError: Can not squeeze dim[1], expected a dimension of 1, got 16 for '{{node Squeeze}} = Squeeze[T=DT_FLOAT, squeeze_dims=[-1]](IteratorGetNext:1)' with input shapes: [?,16].
</code></pre>

<p>Do you have any suggestions?</p>
","nlp"
"72899","Find material noun in a sentence computationally","2020-04-24 10:15:40","","0","34","<nlp>","<p>I want to find all <a href=""https://www.teachingbanyan.com/grammar/material-noun/"" rel=""nofollow noreferrer"">material nouns</a> in a sentence. Using <code>pos_tag</code> set of <code>nltk</code> only give <code>NN, NNS, NNP</code> etc. Is there any existing work to find out material nouns or how can we solve this problem?</p>

<p>Edit: An example will be </p>

<p>Input: <code>Don't forget to bring water bottle, a blanket and shoes **during your visit.**</code></p>

<p>Output: [<code>water bottle, blanket, shoes</code>]</p>
","nlp"
"72862","NER and context mapping","2020-04-23 13:45:15","","3","132","<nlp><nltk>","<p>I want to extract various amounts and tenure of contracts from different contract documents that we have.</p>

<p>For example : </p>

<blockquote>
  <p>Mr xyz, this contact is valid for 3 Months and has to be executed within 1 Month.
  you have to pay \$3000  as  contract fee, \$60 as taxes, \$1200 as security deposit and \$1200 as rent</p>
</blockquote>

<p>Expected output : Contract tenure : 3 Months, Amount to pay : \$3060</p>

<p>Please note : I tried NER but that is showing 2 tenured and 2 amounts. However I am looking for a technique by which we can associate amount to contract. </p>
","nlp"
"72857","Overfitting with text classification using Transformers","2020-04-23 12:43:10","","3","9291","<classification><nlp><transformer><text-classification>","<p>I am trying to make a binary text classification model by using the encoder part of the transformer and then using its output to feed into an LSTM network. However, I am not able to achieve good accuracy on both the training set (92%) and the validation set (72%). Is my approach correct? Please tell me a better way to design the model and improve accuracy.</p>
","nlp"
"72836","SpaCy string store","2020-04-23 06:59:58","","1","218","<nlp><spacy>","<p>I am trying to get the hash values for some English words from the string store of SpaCy.</p>

<pre class=""lang-py prettyprint-override""><code>nlp = English()
doc1 = nlp('this is doc1')
id = doc1.vocab.strings['Saurabh']
print(id)
# output, it has given hash code

id1 = doc1.vocab.strings['समाचार'] 
# This is a hindi word so i do not expect it to be a part of English,
# so it shoud throw an error.
print(id1)
# output, it given hash code
</code></pre>

<p>Why this is not giving an error in (at least) the 2nd line?</p>
","nlp"
"72796","Gender identification task on instance or user level?","2020-04-22 17:09:11","","0","19","<classification><nlp>","<p>I'm working on a task which is gender identification. Given a user account (e.g. Twitter account) with its documents (e.g. 100 tweets), the user should be classified as a male or a female.</p>

<p>The dataset that I have contains around 500 users for each class (label).</p>

<p>There are 2 different ways to approach this task, but I don't know which one is ""more correct"":</p>

<p>1) I aggregate the documents of each user into one large document, and then feed the final document into a classifier.</p>

<p>2) I project the user label (class) into her/his documents, and then feed each single document of the user into a classifier. At the prediction time, I apply averaging on the probabiltities of the users' documents to get the labels of the users (e.g. larger or smaller than 0.5).</p>

<p>Probably an answer to this question could be that both ways are task-dependent, but I want to know if there is a scientific explanation behind any of the solutions, especially in my task.</p>

<p>BTW, some of the documents for many users are not important (e.g. <code>Hello all :D!</code>).</p>
","nlp"
"72745","Multiclass classification dataset with many features producing bad accuracy of predictions","2020-04-22 06:46:25","","0","114","<python><scikit-learn><nlp><predictive-modeling><multiclass-classification>","<p>I have been trying to fix this for 2 months now with no luck. I am doing some medical research for my study. I have a dataset that has patients diagnosis based on medical reports (Features.csv) and each patient based on that medical report has a list of diseases (Results.csv)</p>
<p>Here is a sample of each file.</p>
<p>Features.csv</p>
<pre><code>filename    code    frequency
1006    53438000    2
1006    54706004    10
1006    65801008    1
1006    66842004    10
1006    70901006    11
1006    71388002    1
1006    71651007    1
1006    71960002    2
1006    73761001    2
1006    74016001    1
1006    77477000    1
1007    105011006   1
1007    34896006    1
1007    363680008   2
1007    399208008   2
1007    52765003    1
1007    57485005    1
1007    71388002    3
1007    73632009    1
1007    767002  1
1007    86273004    2
1008    34227000    1
1008    363679005   1
1008    42525009    1
1008    67166004    1
1008    71388002    1
1008    90205004    1
1009    104866001   1
1009    113011001   1
1009    113063008   1
1009    118635009   2
1009    122462000   1
1009    16310003    6
1009    165581004   1
1009    168537006   2
1009    169070004   1
</code></pre>
<p>This is Results.csv</p>
<pre><code>filename    result  order
1006        5990    2
1006        7802    3
1006        2762    4
1006        2738    5
1006        4589    6
1006        V4575   7
1006        27651   8
1006        56400   9
1006        4019    10
1006        V103    11
1006        2449    12
1006        2724    13
1006        56210   14
1006        2859    15
1006        5779    16
1006        5566    1
1007        1892    1
1007        1970    2
1007        496 3
1007        4280    4
1007        51881   5
1007        2859    6
1007        4019    7
1007        V1011   8
1008        4321    1
1008        41400   2
1008        4019    3
1008        2724    4
1008        71590   5
1008        V4581   6
1009        0389    1
1009        5789    2
1009        5761    3
1009        5845    4
1009        51881   5
1009        1552    6
1009        5990    7
1009        4280    8
1009        5762    9
1009        57511   10
1009        25000   11
1009        V5867   12
1009        99592   13
1009        0413    14
</code></pre>
<p>I did this in my Python code</p>
<p><strong>1- Load dataset.</strong></p>
<p><strong>2- Pivot features to have codes as columns and frequency is the value</strong></p>
<pre><code>filename   53438000   54706004 ... 90205004
1006       2          10           0
1007       0           0           0
1008       0           0           0
1009       0           0           0
</code></pre>
<p><strong>3- Pivot Results and put values in array</strong></p>
<pre><code>1006 [5990, 7802, ...]
1007 [1892, 1970, ...]
</code></pre>
<p><strong>4- One hot encode results</strong></p>
<pre><code>filename  1892   1970    .... 5990   7802 ...
1006       0     0            1       1
1007       1     1            0       0
</code></pre>
<p><strong>5- Split dataset into Training and Test (80/20)</strong></p>
<p><strong>6- Use LogisticRegression</strong></p>
<p><strong>7- Check Accuracy</strong></p>
<p>The accuracy I get is very very bad</p>
<p>and when I try to tweak code I get all 1s prediction!!</p>
<p>What am I doing wrong here?</p>
<p>How can I improve accuracy?</p>
<p>Here are the complete files.<br />
<a href=""http://shrinx.it/features.zip"" rel=""nofollow noreferrer"">Features</a><br />
<a href=""http://shrinx.it/results.zip"" rel=""nofollow noreferrer"">Results</a></p>
<p>and here is my code.</p>
<pre><code>import pyodbc
import pandas as pd
import numpy as np
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.multiclass import OneVsRestClassifier
from sklearn.metrics import f1_score
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix


RecNo = &quot;0&quot;

pd.set_option('display.max_colwidth', 300)


conn = pyodbc.connect('Driver={SQL Server};'
                      'Server=DELLG3;'
                      'Database=NLP2;'
                      'Trusted_Connection=yes;')

df_features = pd.read_sql(&quot;EXEC GetFeatures &quot; + RecNo , conn)


df_features.shape

df_results = pd.read_sql(&quot;EXEC GetResults &quot; + RecNo  , conn)




df_features = df_features.pivot(index='filename', columns='code', values='frequency')
df_features[np.isnan(df_features)] = 0

df_results = df_results.groupby('filename')[[&quot;result&quot;]].agg(list).reset_index()


multilabel_binarizer = MultiLabelBinarizer()
multilabel_binarizer.fit(df_results['result'])
ResultsArray = multilabel_binarizer.transform(df_results['result'])

ResultsArray = ResultsArray[:, :-1]

xtrain, xval, ytrain, yval = train_test_split(df_features, ResultsArray, test_size=0.2, random_state=9)

lr = LogisticRegression()
clf = OneVsRestClassifier(lr)


# fit model on train data
clf.fit(xtrain, ytrain)

# make predictions for validation set
y_pred = clf.predict(xval)

# evaluate performance
print(f1_score(yval, y_pred, average=&quot;micro&quot;))

#I obtain the accuracy of this fold
ac=accuracy_score(y_pred,yval)

#I obtain the confusion matrix
cm=confusion_matrix(yval.ravel(), y_pred.ravel())

TN = cm[0][0]
FN = cm[1][0]
TP = cm[1][1]
FP = cm[0][1]
print(TN,FN,TP,FP)
</code></pre>
","nlp"
"72605","How do I extract album and song titles from this plain text file?","2020-04-19 23:37:49","","0","226","<nlp><text-mining><preprocessing><data-wrangling>","<p>Inspired by <a href=""https://news.codecademy.com/taylor-swift-lyrics-machine-learning/"" rel=""nofollow noreferrer"">topic modeling and clustering analysis</a> of Taylor Swift's lyrics, I want to do the same for the band Nightwish. I scraped <a href=""http://www.darklyrics.com/"" rel=""nofollow noreferrer"">Dark Lyrics</a> (see <a href=""https://github.com/medakk/darklyrics-scraper"" rel=""nofollow noreferrer"">script</a>) for all of their lyrics and saved the results into a single plain text file that looks like this:</p>

<p><a href=""https://i.sstatic.net/sOxHp.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/sOxHp.png"" alt=""enter image description here""></a></p>

<p>To perform my intended analyses, I want the target file to look like this, where each row is a line of lyric with columns indicating the corresponding album (wrapped around by ""*"") and song (after a track number ""n. "") titles. </p>

<p><a href=""https://i.sstatic.net/uKYz9.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/uKYz9.jpg"" alt=""enter image description here""></a></p>

<p>Maybe I can just read each line as a row and manually add album and song titles. However, I was wondering if there's a more efficient way to do this. </p>
","nlp"
"72594","Classify sentence into predefined category","2020-04-19 18:57:35","","1","90","<nlp>","<p>I am working on a project which takes text data (Social Media , reviews etc) and classify into a predefined category.</p>

<p>Now I already have a Sentiment Analyzer in place. So basically I am only working with Negative sentiment text.</p>

<p>Now I have a predefined category ex. 'Data speed is slow', 'No Network Coverage' etc. The total number of category is around 30.</p>

<p>My goal is to classify those text into those categories.</p>

<p>I have already tried few methods with limited success:
1. Defining Anagrams (bi and tri) for each category and then filtering.
2. Converting sentences to Sentence Vector Embedding and finding cosine similarity between sentence embedding and  an embedding for a category template.
3. KMeans Clustering and LDA.</p>

<p>Note : I don't have an option for labeling now so cannot do supervised learning. But I wanted to know whether Supervised learning would be the best option here.</p>

<p>I wanted to know the best strategy here using unsupervised technique. How to accurately capture the semantics and classify. Any traditional NLP approach which can be used here?.  Any ideas would be appreciated.    </p>
","nlp"
"72576","How to pass more than 2 input columns to a Deep learning Keras model for sequence tagging/labeling","2020-04-19 12:14:34","","0","253","<python><deep-learning><keras><nlp><labelling>","<p>I have to build a neural network which extract relationship between two entities.Input should be:
Input text/paragraph, vocabulary of entities and relationship phrases that system should recognize.</p>

<p>Output is sequence of tags and length of output sequence and input text/paragraph is same.</p>

<p>Dataset is a CSV file having 3 input columns(input text, entities in text, relationship between 2 entities) and 1 output column. I am using Keras library to build this model.</p>

<p>Example-<strong>input1</strong>: zomato acquires uber; <strong>input2</strong>: zomato, uber; <strong>input3</strong>: acquires
; <strong>Output</strong>: some-tag some-tag some-tag (<strong>note</strong>: these are not actual tags just an example)</p>

<p>I planned to use a char embedding for input text using time distributed layer with Bi-LSTM but now got stuck with 3 inputs.
I am aware of keras functional api but how can I use it in a sequence tagging problem with time distributed layer.
If any other approach can be used to avoid this problem, please suggest.</p>
","nlp"
"72565","System Requirement to train BERT model","2020-04-19 05:51:39","","1","923","<nlp><recommender-system><bert>","<ol>
<li><p>How much Hardware is required to train it well?(My current PC specs: 8GB RAM, i5 2 core Processor, Standard GPU (No work going on GPU))</p></li>
<li><p>I have a dataset of approx 1lakh records.Is it is necessary to train BERT model on such large records or it will give better results on less records as well?</p></li>
<li><p>Is BERT capable to classify unidentified text which is not present in train data but in test data?</p></li>
</ol>
","nlp"
"72498","Keras Bidirectional LSTM: low training and validation loss but very bad predictions","2020-04-17 16:34:07","","1","177","<python><keras><nlp><lstm><prediction>","<p>I'm training a <code>Bidirectional LSTM</code> using <code>Keras</code>. 
My task is to predict the words order in a sentence, so, given a sentence, output of each timestep will be a real number: predicted real numbers of the sentence are ranked in order to obtain integer numbers, indicating the predicted position of the word in the sentence.</p>

<p>Example: </p>

<p>sentence -> ""Nice I am""</p>

<p>predicted real numbers -> [0.2, 0.6, 0.4]</p>

<p>ranked real numbers -> [3,1,2]</p>

<p>Basically, I pad my sequences to 20, that is the max sequence length found in the dataset. </p>

<p>My model is the following:</p>

<pre><code>model = tf.keras.Sequential()
model.add(Masking(mask_value=0., input_shape=(timesteps, features)))
model.add(Bidirectional(LSTM(units=timesteps, return_sequences=True), input_shape=(timesteps, features)))
model.add(Dropout(0.2))
model.add(Dense(1, activation='linear'))
</code></pre>

<p>I mask all the padded vectors to avoid noise in learning.
My training loss and my validation loss, during the training, are both low and around 0.33.</p>

<p>In the prediction phase, however, I obtain very bad results.
I don't think my model is overfitting/underfitting looking at losses trend.</p>

<p>Is there something bad in my model architecture?</p>

<p>Thanks in advance.</p>
","nlp"
"72479","Understanding Classifier performance on text data","2020-04-17 11:32:00","","7","280","<machine-learning><nlp><random-forest><logistic-regression><svm>","<p>I am working on a multi-label text classification problem(Total target labels 90). The data distribution has a long tail and class imbalance and around 1900k records. Currently, I am working on a small sample of around 100k records with similar target distribution.I am using the OAA strategy (One against all). I have tried many algorithms on data. </p>

<p>Currently, each label has atleast 5000 data rows. The class imbalance is high with around 80k records for the most common label and the most rare with just one data row which I have not considered in the modelling. This dataset contains text from academic journals. It has Title and Abstract  columns. </p>

<p>I am using HashingVectorizer(number of features 2**20, char analyzer) to generate features and TSVD to reduce the dimensionality(n_components=200). </p>

<pre><code>LinearSVC(class_weight='balanced') # Got many warnings that it could not converge. I came to know that it may due to data not scaled properly. How can I scale text data??  
LogisticRegression(solver='lbfgs') # Converged very quickly
RandomForestClassifier(n_estimators=40,class_weight=""balanced"") # Train time ~2hr 
</code></pre>

<p>I noticed that LinearSVC has good recall(less false negatives) while Logistic and RF has good precision(less false positives) scores. Can anyone help me in identifying the reasons behind these scores and how can I improve them. </p>

<p><a href=""https://i.sstatic.net/SG7KC.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/SG7KC.png"" alt=""enter image description here""></a></p>

<p>Currently, I am not using deep learning/transformer models due to limited computation resources. </p>
","nlp"
"72438","NLP algorithm: sentiment with specific guidelines","2020-04-16 14:56:43","73106","1","44","<nlp><sentiment-analysis>","<p>So I have this situation, I have filtered a bunch of single independent sentences that I filtered because they contain the word X (in my case, X = ""budget""). if the meaning of the sentence is ""budget increases/goes up/etc"" my result is 1 and, if not, 0. all sentences hold such meaning (the word budget is not used in different ways)</p>

<p>this is not a traditional ""sentiment"" problem and I do not know how to approach this. I could run into a variety of cases (""budget did not go down"" // ""A budget cut was in order"" // ""A budget cut was not in order"")</p>

<p>if required, I can categorize by hand up to 400/500 sentences and say with certainty the result (0, 1). could you point me to any model that would solve this? would the amount of hand-labeled samples be enough? </p>
","nlp"
"72427","Topic Similarity Measure | Multi-class Text Classification Model","2020-04-16 11:07:55","","0","131","<deep-learning><nlp><logistic-regression><data-science-model><topic-model>","<p>I am trying to build a multi class text-classifier that classifies whether the tweet belongs to one of the categories ( Advise or Science or others ) <br>
let the input be any tweet like <a href=""https://twitter.com/ProfFeynman/status/1249384059488186368"" rel=""nofollow noreferrer"">this</a> ,<br>
<strong>Input :</strong> <br></p>

<pre><code>The goal of teaching should not be to help the students learn how to 
memorize and spit out information under academic pressure. Brain

The purpose of teaching is to inspire the desire for learning in them and 
make them able to think, understand, and question.

#maths #richardfeyman
</code></pre>

<p><strong>Output :</strong> <br>
<code>( Learning : 98% , others : 1 % , Science : 1 % )</code> <br>
For now i came up with this <br>
<strong>Idea 1 :</strong> <br>
Maybe i can use LDA to get the topics of the tweet and perform semantic matching between these labels ( learning , science , others ) so one with the highest score can be choosen as right one (<code>argMax(...)</code> ).<br>
What do you think about the above idea ? <br>
Can anyone please enlighten me or point me in the right direction .</p>
","nlp"
"72257","approach to classify text with natural language processing methods","2020-04-13 17:50:45","","1","360","<machine-learning><python><nlp><text-classification>","<p>I have a problem with regards to text classification/categorization. The task is bugging me for days already and as I am pretty new to AI and the field of natural language processing (NLP) I am just overwhelmed by the content online and available tools/libraries (e.g. NLTK, Keras, spaCy, etc.). It would be awesome if you could give me some guidance or clues on how you would approach the problem.</p>

<p>Issue: basicially I try to set up a tool for classifying text. I already have an extensive labeled dataset to work with. The input will always be a list of some sort (think of an Excel file with 500 rows). Each row contains a single word or a combination of words, i.e. no sentences.</p>

<p>A simplified example of my labeled dataset - input on the left, classification on the right:</p>

<pre><code>""dog"" -&gt; ""animal""
""dog owner"" -&gt; ""person""
""dog owner house"" -&gt; ""building""
""owner"" -&gt; ""person""
""dog food"" -&gt; ""food""
""food court"" -&gt; ""building""
</code></pre>

<p>My existing labeled dataset has around 2,000 of these classifications with in total 50 unique categories. How can I set up an algorithm that scans the input for example for the word ""dog"" - if it is only ""dog"" then it is the category ""animal"", if it is ""dog"" and ""owner"" it is the category ""person"", if it is ""dog"", ""owner"" and ""house"" it is the category ""building"" and so on.</p>

<p>If I set up a ton of if-else-statements as a decision tree it is just cumbersome and intransparent. Is there a way with NLP to solve such an issue?</p>

<p>Thank you very much in advance! Looking very much forward to your ideas and please let me know if I have to be more specific in any way.</p>

<p>Best regards, pythoneer</p>
","nlp"
"72206","Not able to restore attention model properly","2020-04-12 19:52:42","72224","1","197","<keras><tensorflow><nlp>","<p>I am referring <a href=""https://www.tensorflow.org/tutorials/text/nmt_with_attention"" rel=""nofollow noreferrer"">this</a> article on building an attention model using tensorflow.</p>

<p>I am trying to train a similar model on my dataset using google colab. Due to the session limit of colab and my large dataset, I need to save the model state and restore it to resume training.</p>

<p>However, I am not able to restore the model upon saving the parameters. I have saved the input and target tokenizers, model checkpoint and even the input and output tensors. However, every time I use checkpoint.restore and resume training the model it resumes training with a high loss(equal to random weights).</p>

<p>I always test my model before saving using the translate function on some test data and it generates a one line summary. However, when I restore the model and run some sample data on the translate function, I only get a single  tag as output (as if it is a newly initialised model).</p>

<p>Here is my code</p>

<pre><code>checkpoint_dir = './training_checkpoints'
checkpoint_prefix = os.path.join(checkpoint_dir, ""ckpt"")
checkpoint = tf.train.Checkpoint(optimizer=optimizer,
                                 encoder=encoder,
                                 decoder=decoder)

manager = tf.train.CheckpointManager(checkpoint, 'checkpoint_dir', max_to_keep=1)
</code></pre>

<p>The training step is</p>

<pre><code>EPOCHS = 50

for epoch in range(EPOCHS):
  start = time.time()

  enc_hidden = encoder.initialize_hidden_state()
  total_loss = 0

  for (batch, (inp, targ)) in tqdm(enumerate(dataset.take(steps_per_epoch))):
    batch_loss = train_step(inp, targ, enc_hidden)
    total_loss += batch_loss

    if batch % 100 == 0:
      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,
                                                   batch,
                                                   batch_loss.numpy()))
  # saving (checkpoint) the model every 3 epochs
  if (epoch + 1) % 3 == 0:
    manager.save()

  print('Epoch {} Loss {:.4f}'.format(epoch + 1,
                                      total_loss / steps_per_epoch))
  print('Time taken for 1 epoch {} sec\n'.format(time.time() - start))
</code></pre>

<p>I restore by doing</p>

<pre><code>checkpoint.restore('ckpt-ckptnumber.index')
</code></pre>

<p>I save the tokenizers (both input and output) using pickle</p>

<pre><code>with open('inp_tokenizer.pickle', 'wb') as handle:
      pickle.dump(inp_lang_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)
</code></pre>

<p>I save the tensors using numpy.save()</p>

<pre><code>np.save('X.npy', input_tensor)
</code></pre>
","nlp"
"72150","How should labeled data from multiple annotators be prepared for ML text classification?","2020-04-11 21:53:40","72333","2","908","<nlp><dataset><annotation>","<p>My specific question is how NLP data from multiple human annotators should be aggregated - though general advice related to the question title is appreciated. One critical step that I've seen in research is to assess inter-annotator agreement by Cohen's kappa or some other suitable metric; I've also found research reporting values for various datasets (e.g. <a href=""https://www.acl-bg.org/proceedings/2017/RANLP%202017/pdf/RANLP015.pdf"" rel=""nofollow noreferrer"">here</a>), which is helpful for baselining.</p>

<p>How many annotators should work on each data point depends on time, personnel, and data size requirements/constraints, among other factors (I may ask a followup question for how to find optimal <em>n</em>). However, once <em>n</em> annotators have finished a dataset, how should those <em>n</em> datasets be unified for a ""ground truth""? A couple approaches that I have seen used or seem reasonable to me:</p>

<ul>
<li><p>Take averages over all annotators. Classification problems are sometimes hard to restate as graduated ones, although that seems necessary if an average is to be taken.</p></li>
<li><p>Express some level of uncertainty in the data for controversial labels, or even omit them from training and evaluation.</p></li>
<li><p>Add an arbitration step to unify or discard controversial labels. I am not sure this would be worth the annotators' time.</p></li>
<li><p>Choose some ""principal annotator(s)"" (possibly determined by IAA scores) who get the final word in conflicts.</p></li>
</ul>

<p>Guidance/references for the above and any other steps I can take to make a high quality dataset are much appreciated. I am mostly interested in efficiently removing individual annotator bias even when <em>n</em> is low.</p>
","nlp"
"72105","Predicting correct match of French to English food descriptions","2020-04-10 19:38:31","72113","0","57","<classification><nlp><word-embeddings><machine-translation>","<p>I have a training and test set of food descriptions pairs (please, see example below)
First name in a pair is a name of food in French 
and second word is this food description in English.
Traing set has also a <code>trans</code> field that is True for correct descriptions 
and False for wrong descriptions.
The task is to predict <code>trans</code> field in a test set, in other words to predict
wich food description is corect and which is wrong.</p>

<pre><code>dishes = [{""fr"":""Agneau de lait"", ""eng"":""Baby milk-fed lamb"", ""trans"": True},
{""fr"":""Agrume"", ""eng"":""Blackcurrants"", ""trans"": False},
{""fr"":""Algue"", ""eng"":""Buttermilk"", ""trans"": False},
{""fr"":""Aligot"", ""eng"":""potatoes mashed with fresh mountain cheese"", ""trans"": False},
{""fr"":""Baba au rhum"", ""eng"":""Star anise"", ""trans"": True},
{""fr"":""Babeurre"", ""eng"":""seaweed"", ""trans"": False},
{""fr"":""Badiane"", ""eng"":""Sponge cake (often soaked in rum)"", ""trans"": False},
{""fr"":""Boeuf bourguignon"", ""eng"":""Créole curry"", ""trans"": False},
{""fr"":""Carbonade flamande"", ""eng"":""Beef Stew"", ""trans"": True},
{""fr"":""Cari"", ""eng"":""Beef stewed in red wine"", ""trans"": False},
{""fr"":""Cassis"", ""eng"":""citrus"", ""trans"": False},
{""fr"":""Cassoulet"", ""eng"":""Stew from the South-West of France"", ""trans"": True},
{""fr"":""Céleri-rave"", ""eng"":""Celery root"", ""trans"": True}]

df = pd.DataFrame(dishes)

    fr                  eng                                          trans
0   Agneau de lait      Baby milk-fed lamb                           True
1   Agrume              Blackcurrants                                False
2   Algue               Buttermilk                                   False
3   Aligot              potatoes mashed with fresh mountain cheese   False
4   Baba au rhum        Star anise                                   True
5   Babeurre            seaweed                                      False
6   Badiane             Sponge cake (often soaked in rum)            False
7   Boeuf bourguignon   Créole curry                                 False
8   Carbonade flamande  Beef Stew                                    True
9   Cari                Beef stewed in red wine                      False
10  Cassis              citrus                                       False
11  Cassoulet           Stew from the South-West of France           True
12  Céleri-rave         Celery root                                  True
</code></pre>

<p>I think to solve this as text classification problem, where text is a concatenation of French name and English description embeddings. </p>

<p>Questions:</p>

<ul>
<li>Which embeddings to use and how concatenate them?</li>
<li>Any other ideas on approach to this problem? BERT?</li>
</ul>

<p><strong>Update:</strong></p>

<p>How about the following approach:</p>

<ul>
<li>Translate (with BERT?) French names to English</li>
<li>Use embeddings to create two vectors: v1 - translated English vector and v2 - English description vector (from data set)</li>
<li>Compute v1 - v2 </li>
<li>Create new data set with two columns: <code>v1 - v2</code> and <code>trans</code></li>
<li>Train classifier on this new data set </li>
</ul>

<p><strong>Update 2:</strong></p>

<p>It looks like <em>cross-lingual classification</em> may be the right solution for my problem:</p>

<p><a href=""https://github.com/facebookresearch/XLM#iv-applications-cross-lingual-text-classification-xnli"" rel=""nofollow noreferrer"">https://github.com/facebookresearch/XLM#iv-applications-cross-lingual-text-classification-xnli</a></p>

<p>It is not clear yet from the description given on the page with the link above, where to fit my own training data set and how to run classifier on my test set. Please help to figure this out. It would be ideal to find end-to-end example / tutorial on cross-lingual classification.</p>
","nlp"
"72076","N-gram based Language Models learned using an Encoder-Decoder Model","2020-04-10 09:25:30","","1","75","<machine-learning><deep-learning><nlp><predictive-modeling><autoencoder>","<p>I have been going through a N-gram based Language Model learned using an Encoder-Decoder Model for Email smart compose.</p>
<p>The program outputs only one prediction for the given input.
I want to know how to get multiple predictions out of the same.</p>
<p>Here is the link to the notebook: <a href=""https://nbviewer.jupyter.org/github/PrithivirajDamodaran/NLP-Experiments/blob/master/Gmail_style_smart_compose_with_char_ngram_based_language_model.ipynb"" rel=""nofollow noreferrer"">https://nbviewer.jupyter.org/github/PrithivirajDamodaran/NLP-Experiments/blob/master/Gmail_style_smart_compose_with_char_ngram_based_language_model.ipynb</a></p>
<p>Here, for the input sequence : &quot;hi there&quot; the predicted sequence is: &quot;, how are you today?&quot;
But it maybe possible that I have <strong>multiple sentences</strong> starting with &quot;hi there&quot; in my training dataset. So how do I get all of those?</p>
","nlp"
"72044","How to train NER LSTM on single sentence level","2020-04-09 17:14:12","","1","200","<machine-learning><nlp><lstm><word-embeddings><named-entity-recognition>","<p>My <code>documents</code> are only a single sentence long, containing one annotation.
Sentences with the same named entity of course are similar, but not context-wise.</p>
<p>NER training examples (<em>afaik</em>) always has documents sequentially related, aka the next document is context-wise related to the previous document. Consider the example below. The first sentence is about the US, with location annotations. The second sentence is about an organisation but still related to the previous.</p>
<blockquote>
<p>The United States of America (LOC), commonly known as the United States (U.S. or US).</p>
<p>The Bank of America (ORG) is a multinational investment bank.</p>
</blockquote>
<p>My dataset for example would be:</p>
<blockquote>
<p>The United States of America (LOC), commonly known as the United States (U.S. or US) or America (LOC).</p>
<p>The Netherlands (LOC), informally Holland, is a country in Western Europe.</p>
<p>Peter (PER) works at the harbour.</p>
</blockquote>
<p>The sentences are not related. When considering a <code>bi-lstm</code>, should I somehow separate the two sentences during training? so that it doesn't think that the annotation of the current sentence is related to the previous sentence too?</p>
<p>Take for example the dataset previewed in <a href=""https://www.kaggle.com/navya098/bi-lstm-for-ner"" rel=""nofollow noreferrer"">this Kaggle notebook</a>. It has a <code>sentence_idx</code> to separate sentences by ID but other than that its just one gigantic long list of words (with features). What happens when a <code>(bi)-lstm</code> finds itself in a completely different context, where the current sentence has absolutely no relation to the previous.</p>
<p>Normally I suppose this isn't a problem because documents are very large, but mine are just a single sentence. Not sharing a context, except that, for example, sentences with a LOC annotation are of course about locations, but not a specific context.</p>
<p>I had a very difficult time describing my problem, questions and edits to make it more clear are very welcome. I believe a similar question is: <a href=""https://datascience.stackexchange.com/questions/30772/ner-at-sentence-level-or-document-level"">How to train NER LSTM on single sentence level</a></p>
","nlp"
"71952","BERT classifier with Ktrain API is unable to predict new data","2020-04-08 11:32:29","","1","718","<deep-learning><keras><tensorflow><nlp><bert>","<p>I have trained a classifier for sentiment analysis using BERT architecture.
I am able to train the classifier and I am getting a validation accuracy of 87%. But whenever I feed in test data, or some simple sentence like ""What an amazing movie"", ""I would love that book"", etc, the model predicts the class for one and says list index out of range for other.
I tried to find if there is a bug in my code as well.</p>

<pre><code>(x_train, y_train), (x_test, y_test), preproc = text.texts_from_csv(TWITTER,
                                                                       preprocess_mode='bert',                                                                    
                                                                       text_column = 'text', label_columns = ['target'])

model = text.text_classifier('bert', (x_train, y_train), preproc=preproc)
learner = ktrain.get_learner(model,train_data=(x_train, y_train), val_data=(x_test, y_test), batch_size=6)

learner.fit_onecycle(2e-5, 1)

predictor = ktrain.get_predictor(learner.model,preproc)

predictor.predict(['I am very happy to meet you!'])

</code></pre>

<p>Error that I am getting</p>

<pre><code>IndexError                                Traceback (most recent call last)
&lt;ipython-input-28-82b8da552189&gt; in &lt;module&gt;()
----&gt; 1 predictor.predict(['I am very happy to meet you!'])

1 frames
/usr/local/lib/python3.6/dist-packages/ktrain/text/predictor.py in &lt;listcomp&gt;(.0)
     56                 preds = np.squeeze(preds)
     57                 if len(preds.shape) == 0: preds = np.expand_dims(preds, -1)
---&gt; 58         result =  preds if return_proba or multilabel or not self.c else [self.c[np.argmax(pred)] for pred in preds]
     59         if multilabel and not return_proba:
     60             result =  [list(zip(self.c, r)) for r in result]

IndexError: list index out of range
</code></pre>

<p>My problem is that if I have any bugs in my code, then I should get this error every time I execute the predict method. Say I have 100 new test points, I am getting this error only for 30 to 40 test points and the remaining are classified properly. I tested this theory by feeding in tweets one tweet at a time. But I do not understand why is this happening.</p>
","nlp"
"71777","How can I tokenize a text file with BERT or something similar?","2020-04-05 14:48:44","","1","229","<python><nlp><nltk><bert>","<p>I want to use the <a href=""https://www.dropbox.com/s/7ewzdrbelpmrnxu/rumdetect2017.zip?dl=0&amp;file_subpath=%2Frumor_detection_acl2017"" rel=""nofollow noreferrer"">twitter datasets</a> in a project and the tweet contents look something like this:</p>

<pre><code>tweet_ID         tweet_text

12324124         some text here that has been twitted bla bla bla
35325323         some other text, trump, usa , merica ,etc.
56743563         bla bla text whatever tweet bla bla
</code></pre>

<p>Now I would like to end-up with a file that contains tweet_IDs and some vector encodings. I was reading about BERT, ROBERTA, etc. Is there a way to simply generate these encodings without writing a huge amount of boilerplate code? </p>
","nlp"
"71649","How to implement LSTM using Doc2Vec vectors to get representation?","2020-04-02 21:21:09","","2","336","<machine-learning><lstm><nlp><text><doc2vec>","<p><a href=""https://i.sstatic.net/BHm5g.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/BHm5g.png"" alt=""enter image description here""></a></p>

<p>Hi all. I'm a newbie in ML. I read and found a paper about A Multi-Level Plagiarism Detection System Based on Deep Learning Algorithms and want to implement this model . But I can't find more about step-by-step guide to build it. How LSTM can make representation with input is list vector of sentence trained by Doc2vec.</p>
","nlp"
"71564","What should return doc.ents if the doc have no entities, in spacy?","2020-04-01 18:49:11","71939","0","887","<nlp><spacy><implementation>","<p>I want to answer this question: ""How many sentences contain named entities given a doc?""
and I have this piece of code as solution</p>

<pre><code>nb = 0

for sent in list(doc.sents):
    if sent.ents:
        nb = nb+1

print(nb)
</code></pre>

<p>But I am a little confused about how the if-statement work; 
consider that the actual sentence(sent) does not have any entities(ents), so sent.ents will return an empty list I suppose and I can't understand why an empty list is considered as a false statement because the program when executed does not enter the if.</p>
","nlp"
"71560","Preparing text for modeling in dialogue structure","2020-04-01 17:44:13","","1","46","<python><deep-learning><keras><nlp><convolutional-neural-network>","<p>I'm working on implementing the DialogueGCN <a href=""https://github.com/SenticNet/conv-emotion/blob/master/DialogueGCN/train_IEMOCAP.py"" rel=""nofollow noreferrer"">code</a> from this <a href=""https://arxiv.org/pdf/1908.11540.pdf"" rel=""nofollow noreferrer"">paper</a>. Its a model that classifies the 'emotion' from utterances of text within a conversation. As this model takes into account speaker context, the structure of the data is very important. It looks something like this:</p>

<pre><code>c = conversation (each conversation is independent of the other
u  = utterances (ex.
                 u1 = ""Hey how are you?""
                 u2 = ""good, thanks, and you?""
                 u3 = ""confused, my dog is cooking pasta"")
Utterances are of variable length and conversations have variable amounts of utterances. 
Each utterance has an 'emotion' label.


                               |    corpus    |
                              /        |       \
                             c1       c2       c3
                           / | \     / | \    / | \
                          u1 u2 u3 u1 u2 u3  u1 u2 u3

</code></pre>

<p>The paper uses a CNN to generate features from the text data. The CNN model comes from <a href=""https://arxiv.org/pdf/1408.5882.pdf"" rel=""nofollow noreferrer"">Kim (2014)</a>. There are several implementations of this paper. I am using this <a href=""https://github.com/alexander-rakhlin/CNN-for-Sentence-Classification-in-Keras"" rel=""nofollow noreferrer"">Keras implementation</a>. The model architecture essentially boils down to this:</p>



<pre class=""lang-html prettyprint-override""><code>### the model input is 1 utterance

z = Embedding(len(vocabulary_inv), embedding_dim, input_length=sequence_length, name=""embedding"")(model_input)
z = Dropout(dropout_prob[0])(z)

# Convolutional block
conv_blocks = []
filter_sizes = [2,4,5]
for sz in filter_sizes:
    conv = Convolution1D(filters=num_filters,
                         kernel_size=sz,
                         padding=""valid"",
                         activation=""relu"",
                         strides=1)(z)
    conv = MaxPooling1D(pool_size=2)(conv)
    conv = Flatten()(conv)
    conv_blocks.append(conv)
z = Concatenate()(conv_blocks) if len(conv_blocks) &gt; 1 else conv_blocks[0]

z = Dropout(dropout_prob[1])(z)
z = Dense(hidden_dims, activation=""relu"")(z)

### the output of the dense layer is a tensor for the utterance
</code></pre>



<p>IIUC, each utterance of each conversation is individually input into this CNN and 1 tensor per utterance is the output. Then, all utterances per conversation are concatenated. After processing the whole corpus, the result is a list of concatenated tensors. This is fed into the GCN model.</p>

<p><strong>If you are still following me, my question is this:</strong></p>

<p>I'm trying to use the GCN model to predict on new set of dialogues. The dialogues are structured in the same format, but are still in text (string) form. I need to vectorize them to feed them into the CNN feature generation model. The DialogueGCN paper doesn't say how this was accomplished. What is the optimal way to vectorize strings when they are structured in the dialogue format where words in each utterance are dependent within a conversation, but words across conversations are independent?</p>

<p>In other words, if I am converting each unique word to a unique integer, should I process the entire corpus as one (using the same word:integer mapping for all conversations), or should I process each conversation separately (unique word:int mapping for each convo) because they independent of one another. For instance, the word 'baseball' could <em>mean</em> something different in conversation 1 and conversation 2, depending on context (previous and future utterances).</p>
","nlp"
"71558","Text classification into thousands of classes","2020-04-01 17:14:29","","2","2639","<machine-learning><nlp><multilabel-classification><text-classification>","<p>Could somebody point me to a paper or code that is about classifying texts into potentially thousands of categories (topics)? I do have data based on Wikipedia and the number of categories is really big (thousands), so looking for some solutions that should work. </p>
","nlp"
"71531","can someone give me an idea how to extract information for text such as invoice bills using natural language processing","2020-04-01 05:43:40","","0","32","<nlp>","<p>suppose i have text file with invoice details in it . I want to extract only some information based on my certain condition such as Mobile - 25,000 and quantity - 1 i want to extract only mobile and quantity based information and store it in text file.Can someone give me an idea how to do this. </p>
","nlp"
"71512","NLP - Simple approach to identify commonalities in text comments between people","2020-03-31 19:31:52","","1","454","<nlp><text-mining>","<p>For something we are working on, we were looking for a simple way to compare from review/feedback data against a question (for which there are multiple responses from multiple people), the following:</p>

<ol>
<li><p>What are the common things (things defined as phrases/sentences) they are saying (Some way to quantify the commonality too if possible). The point is to identify what seems to be areas of agreement about their review</p></li>
<li><p>What are things that are not common (basically...what are those on-off sentences/phrases that have been told that are very uncommon)</p></li>
<li><p>Where is there disagreement (i.e. are there sentences/phrases where there is disagreement potentially between the responses)</p></li>
</ol>

<p>The goal is to find a simple solution to this and not necessarily model driven (there is paucity of data). Also...it needs to be directional at this time...as the goal is to prove that this can work and can product reasonable results.</p>

<p>Any help advise?</p>

<p>Thanks much!</p>

<p>PS: We would need some 'intelligence' in identifying the commonality or vice-versa (i.e. different words meaning same within a phrase should be considered common). </p>
","nlp"
"71421","Can you use two different datasets as train and test sets with countVectorizer and test_train_split?","2020-03-29 21:32:49","","1","2108","<machine-learning><scikit-learn><nlp><naive-bayes-classifier>","<p>So I managed to run my code on a combination of train data and validation data, but now I need to create a text file that contains the predictions for the test data and I just don't understand how. Is there any way to make the X_train work with train_data and X_test with test_data? I think that would solve my problem but I can't find how or if it is even possible.</p>

<pre><code>train_data = np.genfromtxt('train_samples.txt', delimiter = '\t', dtype = None, encoding = 'utf-8', names = ('id', 'text'),
                               comments = None)

    train_labels = np.genfromtxt('train_labels.txt', delimiter='\t', dtype = None, names = ('id', 'label'))

    test_data = np.genfromtxt('test_samples.txt', delimiter = '\t', dtype = None, encoding = 'utf-8', names = ('id', 'text'),
                              comments = None)

    validation_data = np.genfromtxt('validation_samples.txt', delimiter='\t', dtype = None, encoding='utf-8',
                                    names = ('id', 'text'), comments = None)
    validation_labels = np.genfromtxt('validation_labels.txt', delimiter = '\t', dtype = None, names = ('id', 'label'))

    for x in range(len(train_data)):
        train_data[x][0] = train_labels[x][1]

    for x in range(len(validation_data)):
        validation_data[x][0] = validation_labels[x][1]

    train_data_text = np.append(train_data['text'], validation_data['text'])
    train_data_labels = np.append(train_data['id'], validation_data['id'])

    # show shape of training data
    cv = CountVectorizer()
    word_count_vector = cv.fit_transform(train_data_text)
    print(word_count_vector.shape)

    # train_data = np.concatenate((train_data, validation_data))
    X = cv.fit_transform(train_data_text).toarray()
    y = pd.get_dummies(train_data_labels)
    y = y.iloc[:, 1].values

    # Train Test Split
    from sklearn.model_selection import train_test_split

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)

    # Training model using Naive bayes classifier
    from sklearn.naive_bayes import MultinomialNB

    results = MultinomialNB().fit(X_train, y_train)

    y_pred = results.predict(X_test)
    print(y_pred)

    from sklearn.metrics import accuracy_score

    # Evaluate accuracy
    print(accuracy_score(y_test, y_pred))

    classifier.fit(X_train, y_train)
    y_pred = classifier.predict(X_test)
</code></pre>
","nlp"
"71385","Using MultiLabelBinarizer for SMOTE","2020-03-28 22:49:55","71415","1","343","<scikit-learn><nlp><class-imbalance><data-augmentation><smote>","<p>This is my first NLP project. I'm trying to use SMOTE for a classifier with 14 classes. I need to convert the classes into an array before using SMOTE. I tried using MultiLinearBinarizer but it does not seem to be working. From the stack trace, it seems like everything is getting converted.  Do I need to convert something else to an array? How would I do that?</p>

<pre><code>from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.preprocessing import MultiLabelBinarizer

nb = Pipeline([('vect', CountVectorizer()),
               ('tfidf', TfidfTransformer()),
               ('clf', MultinomialNB()),
              ])
nb.fit(X_train, y_train)


mlb = MultiLabelBinarizer()
print(mlb.fit_transform(df[""Technique""].str.split("","")))
print(mlb.classes_)

import imblearn 
from imblearn.over_sampling import SMOTE 

smote = SMOTE('minority')

x_sm, y_sm = smote.fit_sample(X_train, y_train)
#print(x_sm.shape, y_sm.shape)
pd.DataFrame(x_sm.todense(), columns=tv.get_feature_names())
</code></pre>

<p>I'm getting the error ValueError: could not convert string to float: 'left left center'</p>

<p>Here is the stack trace</p>

<pre><code>[[1 0 0 ... 0 0 0]
 [1 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 ...
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]]
['Appeal_to_Authority' 'Appeal_to_fear-prejudice' 'Bandwagon'
 'Black-and-White_Fallacy' 'Causal_Oversimplification' 'Doubt'
 'Exaggeration' 'Flag-Waving' 'Labeling' 'Loaded_Language' 'Minimisation'
 'Name_Calling' 'Red_Herring' 'Reductio_ad_hitlerum' 'Repetition'
 'Slogans' 'Straw_Men' 'Thought-terminating_Cliches' 'Whataboutism']
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-45-68681190410f&gt; in &lt;module&gt;()
     10 smote = SMOTE('minority')
     11 
---&gt; 12 x_sm, y_sm = smote.fit_sample(X_train, y_train)
     13 #print(x_sm.shape, y_sm.shape)
     14 pd.DataFrame(x_sm.todense(), columns=tv.get_feature_names())

8 frames
/usr/local/lib/python3.6/dist-packages/numpy/core/_asarray.py in asarray(a, dtype, order)
     83 
     84     """"""
---&gt; 85     return array(a, dtype, copy=False, order=order)
     86 
     87 

ValueError: could not convert string to float: 'left left center'
<span class=""math-container"">```</span>
</code></pre>
","nlp"
"70288","count_vectorizer.vocabulary_.items() and count_vectorizer.vocabulary_ Inconsistent number of returns","2020-03-26 19:18:41","70294","0","934","<machine-learning><nlp><feature-extraction>","<pre><code># create a count vectorizer object
count_vectorizer = CountVectorizer()

# fit the count vectorizer using the text data
count_vectorizer.fit(data['text'])

# collect the vocabulary items used in the vectorizer
dictionary = count_vectorizer.vocabulary_.items()
</code></pre>

<p>To my understanding, after <code>count_vectorizer</code> fits to <code>data['text']</code>, it generates a list of features. In my case, it generated <code>25,257</code> features and these are mapped as <code>dict</code> data type when I call <code>count_vectorizer.vocabulary_</code>. Which is still <code>25,257</code> tuples. It means, it used all the features.</p>

<p>Problem is, when I call <code>count_vectorizer.vocabulary_.items()</code> it returns <code>15,142</code> tuples as  <code>dict_items</code>. Why the number has been reduced here? Should't all the features be used to make the <code>dictionary</code>?</p>

<p>Here are the lengths I'm talking about:</p>

<pre><code>len(data['text'])  #19579
len(count_vectorizer.get_feature_names())  #25257 items
len(count_vectorizer.vocabulary_)  #25257 items
len(dictionary) #15142 items (??????)
</code></pre>
","nlp"
"70276","How effective would this pseudo-LDA2Vec implementation be?","2020-03-26 16:49:26","","1","111","<nlp><recommender-system><word-embeddings><word2vec><lda>","<p>For my site I'm working on a chat recommender that would recommend chats to users. Each chat has a title and description and my corpus is composed of many of these title and description documents. I was curious about training an LDA2Vec model, but considering the fact that this is a very dynamic corpus that would be changing on a minute by minute basis, it's not doable. I was thinking of just doing standard LDA, because LDA being a probabilistic model, it doesn't require any training, at the cost of not leveraging local inter-word relationships. The other added benefit of LDA2Vec was that I could get accurate labeled topics. So I thought, what if I use standard LDA to generate the topics, but then I use a pre-trained word2vec model whether that be trained locally on my corpus or a global one, maybe there's a way to combine both.</p>

<p>The junk below draws heavily from the stuff in the lda2vec paper:</p>

<p><a href=""https://arxiv.org/pdf/1605.02019.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1605.02019.pdf</a></p>

<p>topic 0: SpaceX, Nasa, Asteroids, Rover</p>

<p>w_j = target word</p>

<p>w_i = pivot word</p>

<p>d_j = document vector or what I'm calling the topic in my scenario</p>

<p>c_j = context vector</p>

<p>c_j = w_j + d_j</p>

<p>d_j = [v_1, v_2 ... v_n]</p>

<p>This is the loss function from the paper</p>

<p><img src=""https://i.sstatic.net/YYged.png"" alt=""Loss Function""></p>

<p>I can then iterate each word w_j for each pivot w_i such that the loss is minimal:</p>

<p>so first iteration pivot word is w_i = SpaceX and w_j iterates over [Nasa, Asteroids, Rover]</p>

<p>second iteration pivot word is w_i = Nasa and w_j iterates over [SpaceX, Asteroids, Rover]</p>

<p>once it's done I'll have a document vector d_j, I'll find the closest word in the embedding space to d_j and I presume that's a good label for the topic. Either I'm being extremely naive or this maybe works. Any input would be much appreciated, thanks in advance.</p>
","nlp"
"70253","Difference between using BERT as a 'feature extractor' and fine tuning BERT with its layers fixed","2020-03-26 10:11:35","","1","2595","<deep-learning><nlp><bert><finetuning>","<p>I understand that there are two ways of leveraging BERT for some NLP classification task: </p>

<ol>
<li>BERT might perform ‘feature extraction’ and its output is input further to another (classification) model </li>
<li>The other way is fine-tuning BERT on some text classification task by adding an output layer or layers to pretrained BERT and retraining the whole (with varying number of BERT layers fixed </li>
</ol>

<p>However, if in the second case, we fix all the layers and add ALL the layers from the classification model will be added, 1st and 2nd approaches are effectively the same, am I right? </p>
","nlp"
"70244","Taking huge time to execute piepeline text classification model using sklearn?","2020-03-26 06:11:51","","1","13","<machine-learning><tensorflow><nlp><text-classification>","<p>I have created a pipeline  model for text classification using python ,Firstly i have tried on 30k records dataset it is working fine got the good results , but when it comes to huge data set like 50k or 1 laksh record data set not at all giving at any results in console even though it is taken more than 24 hours time ?
please suggest  will pipeline model  taking to execute  on huge data for text classification ?
i am not getting that what was the problem with pipeline text classification .</p>

<p>tfidfVector = TfidfVectorizer(tokenizer = textProfilerObj.textTokenizer)</p>

<pre><code>    # Create the  pipeline  model to clean, tokenize, vectorize, and classify using tfidVector

    pipe = Pipeline([(""cleaner"", TextCleaner()),('vectorizer', tfidfVector),('classifier', modelObj)])

    #fit the data using pipeline  

    baypit=pipe.fit(self.trainX,self.trainY)
</code></pre>
","nlp"
"70236","Word2Vec Implementation","2020-03-25 22:22:09","70237","1","83","<nlp><word2vec>","<p>In word2vec why is the implementation of likelihood function multiplication of probabilities of finding a neighbouring word given a word? I didnt get why the probabilities should be multiplied.Is there a reason/intuition behind it ?</p>
","nlp"
"70222","Does the transformer decoder reuse previous tokens' intermediate states like GPT2?","2020-03-25 15:44:14","71654","3","2153","<nlp><transformer><gpt>","<p>I recently read Jay Alammar's blogpost about GPT-2 (<a href=""http://jalammar.github.io/illustrated-gpt2/"" rel=""nofollow noreferrer"">http://jalammar.github.io/illustrated-gpt2/</a>) which I found quite clear appart from one point :
He explains that the decoder of GPT-2 processes input tokens one at a time, only actively processing the last input token, the past tokens being saved in memory already and ""passively"" reused without reevaluation.</p>

<p>From my understanding of the transformer architecture, I had the impression that the decoder reevaluates every token generated at each generation. Is this then a difference between the decoder from GPT-2 or does the decoder from a ""classical' transformer also work this way ?</p>

<p>Intuitively I would think that it would make more sense the reevaluate everything at each iteration since new dependencies between words can appear that weren't there at the beginning and would then not be taken into account if past processed words are passively reused.</p>

<p>I hope I am making sense, can someone with knowledge about the GPT2 architecture help me clarify this ?</p>
","nlp"
"70181","Looking for suggestions on performing Sementic Analysis of ASR text","2020-03-24 16:11:58","","1","12","<text-mining><nlp><text-generation>","<p>Currently I am working on a project where I have ASR on which I am performing semantic analysis to extract meaning out of it. The ASR text contains huge amount of vague conversational text which needs to be eliminated using some algorithm.
Trial 1: I tried to use NER to find some important entities in text and then I eliminated the sentences which did not have any important entities. It worked for some samples but failed for majority because it couldn't retain meaningful sentences.
Trial 2: Tried summarizing it using Seq2seq model supervised learning, but it was an entire failure. It couldn't even generate proper sentences.
Trial 3: Tried to perform Sementic Analysis of the text, couldn't find a way to perform it successfully.</p>

<p>I need suggestions for something to try and also any help or examples for performing Sementic Analysis for extracting meaning out of it.</p>

<p>Hope anyone would have performed similar research.</p>
","nlp"
"70133","Determining number of clusters in high dimensions","2020-03-23 20:37:17","","1","66","<nlp><clustering>","<p>I am doing KMeans clustering for sentence embeddings and my problem is the number of clusters. In general, feature size is an order of a few hundreds (in this case 768) and my concern is the sparsity of space. I tried to use <strong>gap statistic</strong>, but it just increases monotonically and has no maximum (I ended up with max 2048 clusters). Also, embeddings lie on a n-dimensional sphere rather than fill the space uniformly. My questions is: does it really make sense to use various ""clustering metrics"" to determine optimal number of cluster when the feature space is large?</p>
","nlp"
"70119","Multiple choice gap-fill question (with distractors) dataset for evaluating NLP algorithms","2020-03-23 13:40:03","","1","77","<machine-learning><nlp><dataset><model-evaluations><kaggle>","<p>I am looking for a standard gap-filling multiple-choice exercise (with distractors) dataset that can be used to evaluate the NLP gap-filling ML algorithms. I expect the dataset to contain questions that evaluate the NLP aspect of the algorithm and can be answered even by kids that require no special prior technical knowledge. Simple questions like:</p>

<p>I eat an ----- every day.</p>

<ol>
<li>book</li>
<li>salary</li>
<li>apple</li>
<li>car</li>
</ol>

<p>I ----- to school yesterday.</p>

<ol>
<li>went</li>
<li>go</li>
<li>apple</li>
<li>student</li>
</ol>

<p>Any recommendation?</p>
","nlp"
"70108","Semi-Supervised Learning using NLP","2020-03-23 07:09:31","","1","80","<nlp><word-embeddings><word2vec><nltk><tfidf>","<p>I am working on a drug reaction problem in which I need to extract tweets and label the tweets (binary-reaction due to drug or not). But since I don't have domain knowledge, and clustering would also not help me in labelling them, I thought of using semi-supervised learning as one labelled small labelled dataset of around 400 tweets is available. So I will be training my model on those 400 tweets and then make predictions on the tweets that I extracted for labelling. </p>

<p>I know since 400 tweets is very less so its predictions on my extracted tweets would be really poor. But ignoring that, I am facing another problem. 
In NLP, we map every word in the dataset during preprocessing. Suppose bag-of-words or create word embedding, word2vec or tf-idf vector. So during prediction when I run the preprocessing on my extracted tweets, it throws an error for the new words in this extracted tweets dataset. As in my extracted dataset there will be names of new medicines or some English word that was not present in the labelled dataset. And suppose I had created bag-of-words, the structure of bag-of-words changes with the input, so in that case model prediction would be wrong as inputs (bag-of-words) are not based on the same words. </p>

<p>One thing that I have planned to do is combine extracted and labelled dataset and preprocess them together. Then train only on labelled and predict on extracted. This way maybe the predictions would be more accurate and it shouldn't throw any error. Still, is there any specific NLP technique that I can use in preprocessing to deal with this error and get better results?</p>
","nlp"
"70090","Using NLP in already analysed text,","2020-03-22 18:59:29","","1","17","<nlp><word2vec>","<p>I have serveral text files.</p>

<p>These files has been analysed through some analytical tool and provided main features</p>

<p>There each feature extracted has one repetition</p>

<p>I know to use predictive modeling in NLP using word2vec, Bag-of-words, Glove, etc.</p>

<p>these method depend on the word repetition</p>

<p><strong>Example</strong></p>

<pre><code>Text 1 : in Melbourne Supermarket the month of January customers bought more of beef minced meat, Milk law fat 2 liters. also customers were buying cream cheese 250 grams. there was high demand on unsalted butter 200 gram ...

Text 2 : things were a bit different this month for sydney Supermarket customers bought more of orange juice 1 ltr, apple juice 1 ltr and mango juice 1 lts, also Milk law fat 2 liters was on high demand. from confectionary kitkat box 12 pack and Snickers 12 pack were hogh in demand...
</code></pre>

<p><strong>The analytics tool gives me these results</strong></p>

<pre><code>Text1 Analysis : Product 3038, Product 6783, Product 5799, , Product 4836, , Product 4882
Text2 Analysis : Product 9398, Product 9391, Product 8349, , Product 8118, , Product 6783
</code></pre>

<p>and the predictive results are </p>

<p><strong>Products recommended to be promoted</strong> </p>

<pre><code>Text 1 : Recommend Chedder cheese 200 gram, chicken breast 1kg
Text 2 : Recommend Chocolate Milk 1 ltr, Smoothy Apple and Mango 1 ltr.
</code></pre>

<p>So how can I approach this problem?</p>

<p>Shall I use word2vec, Bag-of-words, Glove?</p>
","nlp"
"70089","How to identify word in a sentence representing the song genre?","2020-03-22 18:54:30","","1","37","<rnn><nlp>","<p>I am training a model to identify a word that represents a song genre given a sentence.  For example, the model is given a sentence ""Beethoven songs are part of the classical genre.""  The model will categorize the word, ""classical"" as ""song genre"".  Can I use an RNN classifier to do this or is there another algorithm that can accomplish this task?</p>
","nlp"
"70075","Predicting the missing word using fasttext pretrained word embedding models (CBOW vs skipgram)","2020-03-22 14:00:50","70115","4","1628","<machine-learning><nlp><prediction><word2vec><gensim>","<p>I am trying to implement a simple word prediction algorithm for filling a gap in a sentence by choosing from several options:</p>

<p>Driving a ---- is not fun in London streets.</p>

<ol>
<li>Apple</li>
<li>Car</li>
<li>Book</li>
<li>King</li>
</ol>

<p>With the right model in place: </p>

<p><strong>Question 1.</strong> What operation/function has to be used to find the best fitting choice? The similarity functions in the library are defined between <strong>one word to another word</strong> and not <strong>one word to a list of words</strong> (e.g. most_similar_to_given function). I don't find this primitive function anywhere while it is the main operation promised by CBOW (see below)! I see some suggestions <a href=""https://datascience.stackexchange.com/a/10417/92174"">here</a> that are not intuitive! What am I missing here?</p>

<p>I decided to follow the head first approach and start with fastText which provides the library and pre-trained datasets but soon got stuck in the <a href=""https://fasttext.cc/docs/en/unsupervised-tutorial.html"" rel=""nofollow noreferrer"">documentation</a>:</p>

<blockquote>
  <p><em>fastText provides two models for computing word representations: skipgram and cbow ('continuous-bag-of-words'). The skipgram model
  learns to predict a target word thanks to a nearby word. On the other
  hand, the cbow model predicts the target word according to its
  context. The context is represented as a bag of the words contained in
  a fixed size window around the target word.</em></p>
</blockquote>

<p>The explanation is not clear for me since the ""nearby word"" has a similar meaning as ""context"". I googled a bit and ended up with this <a href=""https://towardsdatascience.com/nlp-101-word2vec-skip-gram-and-cbow-93512ee24314"" rel=""nofollow noreferrer"">alternative definition</a>:</p>

<blockquote>
  <p><em>In the CBOW model, the distributed representations of context (or surrounding words) are combined to predict the word in the middle.
  While in the Skip-gram model, the distributed representation of the
  input word is used to predict the context.</em></p>
</blockquote>

<p>With this definition, CBOW is the right model that I have to use. Now I have the following questions:</p>

<p><strong>Question 2.</strong> Which model is used to train <a href=""https://fasttext.cc/docs/en/english-vectors.html"" rel=""nofollow noreferrer"">fastText pre-trained word vectors</a>? CBOW or skipgram?</p>

<p><strong>Question 3.</strong> Knowing that the right model that has to be used is CBOW, can I use the pre-trained vectors trained by skipgram model for my word prediction use case?</p>
","nlp"
"70004","Train a deep learning model in chunks/sequentially to avoid memory error","2020-03-20 17:01:16","","1","790","<machine-learning><python><deep-learning><nlp>","<p>How do I train/fit a model in chunks so as to escape the dreaded memory error?</p>

<pre><code>def TFIDF(X_train, X_test, MAX_NB_WORDS=75000):
    vectorizer_x = TfidfVectorizer(max_features=MAX_NB_WORDS)
    X_train = vectorizer_x.fit_transform(X_train).toarray()
    X_test = vectorizer_x.transform(X_test).toarray()
    print(""tf-idf with"", str(np.array(X_train).shape[1]), ""features"")
    return (X_train, X_test)


# In[3]:


def Build_Model_DNN_Text(shape, nClasses, dropout=0.5):
    """"""
    buildModel_DNN_Tex(shape, nClasses,dropout)
    Build Deep neural networks Model for text classification
    Shape is input feature space
    nClasses is number of classes
    """"""
    model = Sequential()
    node = 512  # number of nodes
    nLayers = 4  # number of  hidden layer
    model.add(Dense(node, input_dim=shape, activation='relu'))
    model.add(Dropout(dropout))
    for i in range(0, nLayers):
        model.add(Dense(node, input_dim=node, activation='relu'))
        model.add(Dropout(dropout))
    model.add(Dense(nClasses, activation='softmax'))
    model.compile(loss='sparse_categorical_crossentropy',
                  optimizer='adam',
                  metrics=['accuracy'])
    return model


# In[18]:


df = pd.read_csv(""ExtractedData.csv"")

# In[19]:

df = df.dropna()
X_train, X_test, y_train, y_test = train_test_split(df['body'], df['user_id'],
                                                    test_size=0.3, random_state=42, shuffle=True)
X_train_tfidf, X_test_tfidf = TFIDF(X_train, X_test)
y_train_tfidf, y_test_tfidf = TFIDF(y_train,y_test)
output_nodes = len(list(set(df['user_id'])))
model_DNN = Build_Model_DNN_Text(X_train_tfidf.shape[1], output_nodes)
model_DNN.fit(X_train_tfidf, y_train_tfidf,
              validation_data=(X_test_tfidf, y_test_tfidf),
              epochs=10,
              batch_size=256,
              verbose=2)

predicted = model_DNN.predict(X_test_tfidf)
print(metrics.classification_report(y_test, predicted))
</code></pre>

<p>When I run the above, I keep getting the memory error: <strong>Unable to allocate 37.9 GiB for an array with shape (67912, 75000) and data type float64</strong>. I know I can send the data in chunks pd.read_csv( , chunksize= 5000), but what I don't know is how do I implement it here. My data has 98000 rows and 4 columns. Thank you.</p>
","nlp"
"69954","Split text into phrases of a person and an operator","2020-03-19 17:15:00","69969","0","67","<machine-learning><neural-network><nlp><speech-to-text>","<p>I have about 5,000 texts without punctuation marks.
Each text is a conversation between an operator and a person.
For example: ""Hello hello how can I help you how can I find out how much money I have in my account""</p>

<p>How can one separate phrases between a person and an operator using machine learning methods?</p>
","nlp"
"69943","What are machine learning/deep learning models for generating contextually related words and synonyms?","2020-03-19 14:36:36","","4","691","<machine-learning><deep-learning><nlp><word2vec>","<p>I have a task to work on models for finding synonyms and contextually related words.
For example, if I enter:</p>

<ul>
<li><p>'<strong>car</strong>' it should generate -> '<strong>vehicle</strong>'</p></li>
<li><p>'<strong>sun</strong>' and '<strong>sea</strong>' could generate '<strong>beach</strong>', or some other related word to the first two. </p></li>
</ul>

<p>So I used so far word2vec and nltk to generate examples.
But since I am not an expert on NLP I really find it difficult to use other algorithms or to build my neural network architecture. 
I would appreciate it if someone can give me other suggestions and some explanations, that could be useful.</p>
","nlp"
"69911","Using transformers for information extraction","2020-03-18 21:06:34","","1","92","<nlp><information-retrieval>","<h3>Task</h3>
<p>I am trying to do some information extraction on earnings reports. I am trying to extract certain metrics, e.g. net sales for quarters. The earnings reports differ quite a lot in how they are structured but they use similar language. A &quot;simple&quot; example follows</p>
<blockquote>
<p>Nine-month period in figures</p>
<p>Order bookings increased 6.6% to SEK 1,095.5 million (1,027.9).</p>
<p>Net sales rose 22.8% to SEK 1,153.5 million (939.4). Adjusted for currency fluctuations, sales increased 19.6%.</p>
<p>Operating profit rose 31.3% to SEK 171.8 million (130.8), corresponding to an operating margin of 14.9% (13.9). Adjusted for currency fluctuations, operating profit increased 24.5%.</p>
<p>Profit before tax amounted to SEK 176.3 million (137.9).</p>
<p>This result includes nonrecurring items (refer to page 10 of the attached interim report). These items had a positive net effect of SEK 1.4 million on operating profit.</p>
<p>Cash flow after changes in working capital amounted to SEK 215.2 million (180.4).</p>
<p>Third quarter in figures</p>
<p>Order bookings increased 10.3% to SEK 431.1 million (390.7). Of the order bookings during the quarter, 25% were recognized during the third quarter and 32% to 42% pertain to revenue within 12 months after the end of the quarter.</p>
<p>Net sales increased 38.8% to <strong>SEK 457.4 million</strong>, while it was SEK 329.5 million in the previous year. Adjusted for currency fluctuations, sales increased 34.9%.</p>
<p>Operating profit rose 98.6% to SEK 99.7 million (50.2), corresponding to an operating margin of 21.8% (15.2). Adjusted for currency fluctuations, operating profit increased 87.6%.
Profit before tax amounted to SEK 100.7 million (51.4).</p>
<p>This result includes nonrecurring items (refer to page 10 of the attached interim report). These items had a positive net effect of SEK 1.4 million on operating profit.</p>
<p>Cash flow after changes in working capital amounted to SEK 134.1 million (80.0).</p>
</blockquote>
<p>where I have highlighted the metric I want to extract.</p>
<p>Note that <code>Third quarter in figures</code> is important, i.e. information from previous phrases are needed.</p>
<h3>Initial approach</h3>
<p>My first strategy is to extract all candidate spans (<code>SEK 1,095.5 million</code>, <code>SEK 1,153.5 million</code> etc.) and then classify each one of them (<code>EXTRACT</code> or <code>DO_NOT_EXTRACT</code>), but I haven't got any great results with that approach. I was thinking that maybe I could exploit transformer models like BERT etc. to capture context from previous phrases?</p>
<p>I hope someone have a good idea on how to tackle this task.</p>
","nlp"
"69877","Feeding XLM-R embeddings to neural machine translation?","2020-03-18 08:19:33","","0","369","<neural-network><nlp><sequence-to-sequence><machine-translation><language-model>","<p>I’m very new to the field of deep learning. My aim is to make a translation between Catalan to Catalan Sign Language. The grammar of the two languages is different </p>

<blockquote>
  <p>Input: He sells food.
  Output (sign language sentence): Food he sells.</p>
</blockquote>

<p>I've been playing around with XLM-R and go the token id like this</p>

<pre><code>input Ids: [200, 100, 2003, 1037, 3835, 3351, 5012, 300, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
</code></pre>

<p>I don't know how to use the embeddings in Sequence to Sequence NMT model. or any other means to do machine translation with a very small data set. The language is low resource language</p>

<pre class=""lang-py prettyprint-override""><code>import torch
from transformers import XLMRobertaModel, XLMRobertaTokenizer

tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-large')
model = XLMRobertaModel.from_pretrained('xlm-roberta-large')

def get_ids(tokens, tokenizer, max_seq_length):
token_ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = token_ids + [0] * (max_seq_length - len(token_ids))
return input_ids

s = ""test sentence""
stokens = tokenizer.tokenize(s)
print(stokens)
stokens = [""[CLS]""] + stokens + [""[SEP]""]
input_ids = get_ids(stokens, tokenizer, 15)

print(tokenizer.convert_tokens_to_ids(['test']))
print(tokenizer.convert_tokens_to_ids(['▁test']))
print(tokenizer.convert_ids_to_tokens([26130]))
print(tokenizer.convert_ids_to_tokens([30521]))
tokens_tensor = torch.tensor([input_ids])
print(input_ids)
print(tokens_tensor)
<span class=""math-container"">```</span>
</code></pre>
","nlp"
"69800","why does adding an LDA document vector with a word2vec word vector work well in LDA2vec?","2020-03-16 21:20:47","","1","87","<classification><nlp><word-embeddings><word2vec><lda>","<p>In LDA the document weight vector represents the ""weights"" of each topic in the document. I think it's also valid to say, each row in the document vector corresponds to a word in the document, the weight value being indicative of how relevant each word is to the topics being discussed in the document. </p>

<p><a href=""https://i.sstatic.net/q0NNJ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/q0NNJ.png"" alt=""enter image description here""></a></p>

<p>where dj is some document vector in the form</p>

<p><a href=""https://i.sstatic.net/Hd8bD.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Hd8bD.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.sstatic.net/YYged.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/YYged.png"" alt=""enter image description here""></a></p>

<p>from my understanding, LDA2vec tries to minimize the cost function by  shifting the p_jk weights of the document vector such that the sum w_j dot c_j for j = 0...n is minimal, which makes sense. I just don't understand the relationship between LDA space and the space of word vectors, because from the way it's being described it seems like the rows of the word vectors carry the same meaning as the rows of the document vectors in LDA, and I don't understand why that would be. If anyone can clear the confusion, that would be greatly appreciated, thanks in advance.</p>

<p>Update:</p>

<p>After further thought, the only way this would make sense to me is if the word, topic, and document vectors weights were being jointly trained and therefore belonging in a common space. Not sure if this is the case though.</p>
","nlp"
"69749","Remove subwords from BERT output","2020-03-15 19:44:58","","2","275","<python><keras><nlp><lstm><bert>","<p>I'm trying to build a multilingual WSD system with BERT on top as the embedding layer.
In order to have better performances, after BERT finishes its job (and performs Transfer Learning), I need to remove the subwords from its output. Is there a way to do so? <br>
I've tried to detach the model from the network's architecture, doing something like this... but I need to do this as a custom layer and I'm not 100% sure that this is even right</p>

<pre><code>class Bert:
    def __init__(self):
        input_word_ids = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name=""input_word_ids"")
        input_mask = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name=""input_mask"")
        segment_ids = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name=""segment_ids"")
        print(""dopwnloading BERT..."")
        bert = hub.KerasLayer(""https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/1"", trainable=False, name=""BERT"")
        print(""BERT downloaded"")
        pooled_output, sequence_output = bert([input_word_ids, input_mask, segment_ids])
        self.model = tf.keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=[pooled_output, sequence_output])
        self.model.summary()


    def predict(self, input_word_ids, input_mask, segment_ids, positional_ids,needed_padding, train_mode:bool = False):
        print(""Starting BERT prediction..."")
        pool_embs, all_embs = self.model.predict(
            {'input_word_ids': input_word_ids, 'input_mask': input_mask, 'segment_ids': segment_ids},
            verbose=1,
            batch_size=64
        )
        del pool_embs
        to_return = []
        print(""Conversion\nSoftware version 2.0..."")
        for i in tqdm(range(len(positional_ids))):
            indexes_to_extrapolate = np.concatenate((positional_ids[i],needed_padding[i]))
            indexes_to_extrapolate = indexes_to_extrapolate[:63] if len(indexes_to_extrapolate) &gt; 64 else indexes_to_extrapolate
            new_version = tf.gather(all_embs[i], tf.constant(indexes_to_extrapolate))
            if train_mode and new_version.shape[0] &lt; 64:
                #Means that, originally, there has to be a padding!
                #And, if there is, it can surely be found in the first position of the needed_padding!
                how_much_iteration = 64 - new_version.shape[0]
                if how_much_iteration &gt; 0:
                    for iteratore in range(how_much_iteration):
                        tmp_padding_for_iteration = needed_padding[i][0]
                        new_version = tf.concat([new_version, tf.constant(all_embs[i][tmp_padding_for_iteration], shape=(1,768))], 0)
            with open(""registro_shape.txt"",""a"") as registro:
                registro.write(""Shape --&gt; "" +str(new_version.shape)+""\n"")
            if new_version.shape[0] &gt; 64:
                print(""wth"")
            to_return.append(new_version)
        return tf.stack(to_return)
</code></pre>

<p><strong>EDIT</strong>: I'll try to contextualize the case with more information regarding the architecture of the network.
In particular, this is the architecture of the network that I'm trying to build for the WSD task. Note that the network should perform a multitask learning task:</p>

<ol>
<li>Bert</li>
<li>BiLSTM</li>
<li>Attention Layer</li>
<li>3 outputs layer</li>
</ol>

<p>self.tokenizatore = FullTokenizer(bert_path,do_lower_case=False)</p>

<p>input_word_ids = tf.keras.layers.Input(shape=(None,), dtype=tf.int32,name=""input_word_ids"")</p>

<p>input_mask = tf.keras.layers.Input(shape=(None,), dtype=tf.int32,name=""input_mask"")</p>

<p>segment_ids = tf.keras.layers.Input(shape=(None,), dtype=tf.int32,name=""segment_ids"")</p>

<pre><code>print(""dopwnloading BERT..."")
bert = hub.KerasLayer(""https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/1"", trainable=False)
print(""BERT downloaded"")
pooled_output, sequence_output = bert([input_word_ids, input_mask, segment_ids])
LSTM = tf.keras.layers.Bidirectional(
    tf.keras.layers.LSTM(
        units=hidden_size,
        dropout=dropout,
        recurrent_dropout=recurrent_dropout,
        return_sequences=True,
        return_state=True
    )
)(sequence_output)
LSTM = self.produce_attention_layer(LSTM)
LSTM = tf.keras.layers.Dropout(0.5)(LSTM)

babelnet_output = tf.keras.layers.Dense(outputs_size[0], activation=""softmax"", name=""babelnet"")(LSTM)
domain_output = tf.keras.layers.Dense(outputs_size[1], activation=""softmax"", name=""domain"")(LSTM)
lexicon_output = tf.keras.layers.Dense(outputs_size[2], activation=""softmax"", name=""lexicon"")(LSTM)



def produce_attention_layer(self, LSTM):
    """"""
    Produces an Attention Layer like the one mentioned in the Raganato et al. Neural Sequence Learning Models for Word Sense Disambiguation,
    chapter 3.2
    :param lstm: The LSTM that will be used in the task
    :return: The LSTM that was previously given in input with the enhancement of the Attention Layer
    """"""
    hidden_states = tf.keras.layers.Concatenate()([LSTM[1],LSTM[3]])
    ripetitore = tf.keras.layers.RepeatVector(tf.keras.backend.shape(LSTM[0])[1])(hidden_states)
    u = tf.keras.layers.Dense(1, activation=""tanh"")(ripetitore)
    attivazione = tf.keras.layers.Activation('softmax')(u)  # We are using a custom softmax(axis = 1) loaded in this notebook
    dotor = tf.keras.layers.Multiply()([LSTM[0],attivazione])

    return dotor
</code></pre>
","nlp"
"69693","How Yelp System Detects Paid Reviews","2020-03-14 17:42:28","","2","63","<machine-learning><nlp><feature-extraction><anomaly-detection><sentiment-analysis>","<p>I am wondering how the yelp spam detection system detects paid reviews? By paid, I mean the following scenarios:</p>

<ol>
<li>I as a business owner pay people to write positive comments and give me a good rate</li>
<li>I as a business owner pay people to write negative comments against my opponent businesses.</li>
</ol>

<p>I appreciate any technical response and also the introduction of good resources. Thanks in advance.</p>
","nlp"
"69681","Featurization for Relation Extraction using Support Vector Machine(SVM)","2020-03-14 09:18:05","","1","27","<python><nlp>","<p>Regarding Relation Extraction using SVM paper(<a href=""https://www.researchgate.net/publication/225671271_Relation_Extraction_Using_Support_Vector_Machine)I"" rel=""nofollow noreferrer"">https://www.researchgate.net/publication/225671271_Relation_Extraction_Using_Support_Vector_Machine)I</a> am looking for any code references on how to make features(word, pos, syntactic features, semantic feature types, chunk tags,distance) for relation extraction between two entities.
Entity1 - Drugs,
Entity2 - Disease,
Relation types:1.taking a drug caused disease
               2. Drug was taken for a disease,
first model is a binary classification model to classify if the entities are related to each other or not.
Second model takes only positive outputs from first model and classifies the relation which is a multi class classification model</p>
","nlp"
"69680","Practical example and working of Laplace Smoothing or Linear Interpolation in Natural Language Processing (NLP)","2020-03-14 07:56:27","","2","228","<machine-learning><python><probability><nlp>","<p>Let us suppose we have a document where</p>

<p>total_words = 50  (for example -> is,the,now,is,am,here,now)
total_unique_words = 40 (for example -> is,the,am,here,now)</p>

<p>How can we apply the <code>Linear Interpolation/ Laplace Smoothening</code> in the case of a <code>trigram</code></p>

<p>for example</p>

<p>('he') has a count of 12
('he', 'is') has a count of 8
('he','is','here') has a count of 4</p>

<p>('is','here') has a count of 6</p>

<p>('is') has a count of 18
('here') has a count of 5</p>

<p>I think it is related to the count of the values. I have the formula in terms of <code>lambda * P(x_i | x_i-1)</code> but do not know how to implement here. Can someone please give me an  <strong>PRACTICAL</strong> example of how to implement the interpolation</p>

<p><strong>Please make any necessary assumptions in the case as I am learning it the first time</strong></p>
","nlp"
"69639","How to use regularizer in AllenNLP?","2020-03-13 12:55:31","","1","209","<deep-learning><nlp><allennlp>","<p>Apology if this sounds a bit lame.</p>

<p>I am trying to use Allennlp for my NLP tasks and would like to use regularization to reduce overfitting. However from all the online tutorials, all the regularizers are set as None, and I still couldn't find out how to use the regularizer after many many attempts. </p>

<p>If I use the example in the official tutorial (<a href=""https://github.com/titipata/allennlp-tutorial"" rel=""nofollow noreferrer"">https://github.com/titipata/allennlp-tutorial</a>) , what if I want to add in regularizer for LSTM and feedforward layer?</p>

<pre><code>class AcademicPaperClassifier(Model):
    """"""
    Model to classify venue based on input title and abstract
    """"""
    def __init__(self, 
                 vocab: Vocabulary,
                 text_field_embedder: TextFieldEmbedder,
                 title_encoder: Seq2VecEncoder,
                 abstract_encoder: Seq2VecEncoder,
                 classifier_feedforward: FeedForward,
                 initializer: InitializerApplicator = InitializerApplicator(),
                 regularizer: Optional[RegularizerApplicator] = None) -&gt; None:
        super(AcademicPaperClassifier, self).__init__(vocab, regularizer)
        self.text_field_embedder = text_field_embedder
        self.num_classes = self.vocab.get_vocab_size(""labels"")
        self.title_encoder = title_encoder
        self.abstract_encoder = abstract_encoder
        self.classifier_feedforward = classifier_feedforward
        self.metrics = {
                ""accuracy"": CategoricalAccuracy(),
                ""accuracy3"": CategoricalAccuracy(top_k=3)
        }
        self.loss = torch.nn.CrossEntropyLoss()
        initializer(self)

    def forward(self, 
                title: Dict[str, torch.LongTensor],
                abstract: Dict[str, torch.LongTensor],
                label: torch.LongTensor = None) -&gt; Dict[str, torch.Tensor]:

        embedded_title = self.text_field_embedder(title)
        title_mask = get_text_field_mask(title)
        encoded_title = self.title_encoder(embedded_title, title_mask)

        embedded_abstract = self.text_field_embedder(abstract)
        abstract_mask = get_text_field_mask(abstract)
        encoded_abstract = self.abstract_encoder(embedded_abstract, abstract_mask)

        logits = self.classifier_feedforward(torch.cat([encoded_title, encoded_abstract], dim=-1))
        class_probabilities = F.softmax(logits, dim=-1)
        argmax_indices = np.argmax(class_probabilities.cpu().data.numpy(), axis=-1)
        labels = [self.vocab.get_token_from_index(x, namespace=""labels"") for x in argmax_indices]
        output_dict = {
            'logits': logits, 
            'class_probabilities': class_probabilities,
            'predicted_label': labels
        }
        if label is not None:
            loss = self.loss(logits, label)
            for metric in self.metrics.values():
                metric(logits, label)
            output_dict[""loss""] = loss

        return output_dict
</code></pre>
","nlp"
"69510","Real-life applications/examples of transfer learning approaches","2020-03-11 10:34:50","","2","117","<machine-learning><nlp><computer-vision><transfer-learning>","<p>I recently read a nice, informative paper titled <a href=""https://www.cse.ust.hk/~qyang/Docs/2009/tkde_transfer_learning.pdf"" rel=""nofollow noreferrer"">'A Survey on Transfer Learning'</a>. It mentions 3 settings of transfer learning - inductive, transductive, and unsupervised. At the same time, it states there are 4 common approaches on ""what knowledge to transfer"" for each setting - instances, features, model parameters, or relations.
I looked up the descriptions of each category, but I do not understand exactly how they are utilised in the domains of Computer Vision and NLP. Could I get an example application of each of these categories to understand better how exactly they differ from each other? How does one determine which approach to use in which setting? <a href=""https://i.sstatic.net/TtYbQ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/TtYbQ.png"" alt=""TL: Settings and approaches""></a></p>
","nlp"
"69477","Use embeddings to find similarity between documents","2020-03-10 21:18:52","69715","1","2406","<keras><nlp><pytorch><embeddings><doc2vec>","<p>I need to find cosine similarity between two text documents. I need embeddings that reflect order of the word sequence, so I don't plan to use document vectors built with bag of words or TF/IDF. Ideally I would use pre-trained document embeddings such as doc2vec from Gensim. 
How to map new documents to pre-trained embeddings ? </p>

<p>Otherwise what would be the easiest way to generate document embeddings in Keras/Tensorflow or Pytorch?</p>
","nlp"
"69474","Select best answer from several existing ones for a question","2020-03-10 20:01:35","69501","1","62","<nlp><question-answering>","<p>After analyzing questions on a forum, a human support team has created a set of general answers, that can be used to provide basic answers on the forum. </p>

<p>I am trying to build a system that: </p>

<ol>
<li><p>Selects best answer from this set of answers for a given question. How to do this?</p></li>
<li><p>Estimates acceptability of such an answer. Which metrics to use?</p></li>
</ol>

<p>Using document embeddings, such as doc2vec to find similarity between question and answer does not solve the problem, I think.  Other ideas?</p>

<p><strong>Update 1</strong></p>

<p>In my case I don't have labeled data set with good answers to train my model. My problem is unsupervised learning problem.</p>
","nlp"
"69464","Character-level embeddings in python","2020-03-10 17:04:04","","3","932","<python><nlp><word-embeddings><spacy>","<p>I'm working on an NLP task that requires the use of character level embeddings, and I've been trying to use <a href=""https://spacy.io/"" rel=""nofollow noreferrer"">Spacy</a>. However, it seems that spacy uses word-level embeddings for the word vectors, and I need character-level embeddings. The only character-level embedding library I've been able to find is <a href=""https://github.com/IntuitionEngineeringTeam/chars2vec"" rel=""nofollow noreferrer"">chars2vec</a> which does not seem well maintained. Is there a way to get character-level embeddings with either spacy or a more popular package than chars2vec? </p>
","nlp"
"69411","How to identify new job descriptions/postings from a set of documents when I have a set of already labeled job descriptions/postings","2020-03-09 20:33:42","","0","38","<nlp><text-mining><text-classification>","<p>Suppose I have a set of already labeled documents -- some of them are job descriptions/postings (these are documents of interest), and some of them are not. I wonder what kind of method would allow me to build a model that can generalize to new data, specifically new job descriptions that may be very different from the already labeled job descriptions.</p>

<p>What I have done so far (in Python):</p>

<p>(1) Trial 1: I tried the classic bag-of-words + <code>TfidfVectorizer</code> + binary classification (<code>LogisticRegression</code> specifcally) approach. With parameter tuning based on the train + valid sets, precision and recall on the test set could reach over 98%. However, after I built the model, I collected a new set of documents (again, some are job postings and others are not), and asked annotators to label these new data. I then used the model to classify these new docs. Perhaps expectedly, the precision and recall dropped pretty drastically to, ~65% and ~40%.</p>

<p>(2) Trial 2: I thought perhaps using tfidf alone might cause some overfitting, so I tried applying <code>TruncatedSVD</code> (with 100 components) on the TfidfVectorizer (bag-of-words + <code>TfidfVectorizer</code> + <code>TruncatedSVD</code> + binary classification).  Again, I tuned the parameters using the train and valid sets, and the precision and recall were similarly high on the test set. When I applied this new model to the newly collected data, the recall improved a bit, from 40% to 50%, but the precision dropped a bit, to 58%.</p>

<p><strong>So my question is</strong>: is there a way to leverage the initial set of coded documents to build a kind of model that can generalize better to new data set. My specific interest is identifying job descriptions/postings. It doesn't have to be a binary classification task since the non-job-descriptions can be much more varied than job descriptions/postings</p>
","nlp"
"69358","Why does vanilla transformer has fixed-length input?","2020-03-08 16:28:59","69481","8","2490","<nlp><transformer>","<p>I know that in the math on which the transformer is based there is no restriction on the length of input. But I still can’t understand why we should fix it in the frameworks (PyTorch). Because of this problem Transformer-XL has been created.</p>

<p>Can you explain to me where this problem is hiding, please?</p>
","nlp"
"69349","Attention to multiple areas of same sentence","2020-03-08 09:01:26","","1","52","<nlp><attention-mechanism>","<p>Lets consider some sentences below:<br>
""Datascience exchange is a wonderful platform to get answers to datascience related queries and it helps to learn various concepts too""<br>
""Can company1 buy company2? What will be their total turnover then?""<br>
""Coronavirus was originated in china. After that it is spreading all over the world. To prevent it everyone has to take care of cleanliness and prefer vegetarians."" </p>

<p>In all above sentences you can see there are multiple questions or utternaces. Sometimes separated by and sometimes by question mark and sometimes by just a dot.<br>
A rule based separation of these sentences fail in many cases.  I want to split these sentences in individual intents. </p>

<p>One of the approach I am guessing is by using attention mechanism on different parts of sentences. I cant use gensim etc sentence embeddings as I dont have clear sentence boundaries here.<br>
Can someone suggests if attention approach will work? If yes, any similar code if they can point to, that would be helpful as I haven't coded this before.  </p>

<p>If any other better approach can solve this problem then please suggest.</p>
","nlp"
"69348","Seq2Seq for sentence correction","2020-03-08 08:45:40","","1","41","<nlp><autoencoder><sequence-to-sequence><machine-translation>","<p>I have a task in hand where I get a dirty formed sentence and need to correct it. Examples are,   ""StackOverflow best question answering platform"" to be converted to ""StackOverflow <strong>is</strong> best question answering platform""<br>
""John published first paper 1990""  => ""John published <strong>his</strong> first paper <strong>in</strong> 1990""<br>
""John published first paper 1990""  => ""<strong>When did</strong> John published his first paper""    </p>

<p>I am thinking of using a Seq2Seq / EncoderDecoder technique just like NMT and train it on the input output pairs as training data.<br>
Can someone who is experienced at this, validate if this approach will work? or there is a better approach available?</p>
","nlp"
"69338","Building a tag-based recommendation engine given a set of user tags?","2020-03-08 02:41:29","69483","4","893","<nlp><recommender-system><word-embeddings><information-retrieval>","<p>Basically, the idea is to have users following tags on the site, so each users has a set of tags they are following. And then there is a document collection where each document in the collection has a Title, Description, and a set of tags which are of relevance to the topic being discussed in the document as determined by the author. What is the best way of recommending documents to the user, given the information we have, which would also take into consideration the semantic relevance of the title and description of a document to the user's tags, whether that be a word embeddings solution or a tf-idf solution, or a mix, do tell. I still don't know what I'm going to do about tag synonyms, it might have to be a collaborative effort like on stackoverflow, but if there is a solution to this or a pseudo-solution, and I'm writing this in C# using the Lucene.NET library, if that is of any relevance to you.</p>
","nlp"
"69311","Degree of Profanity in a Sentence","2020-03-07 12:52:37","69315","1","710","<nlp><sentiment-analysis>","<p>Given a comment or a sentence and a list of profane words, How do I write a program to print the degree of profanity in that sentence?</p>
","nlp"
"69259","How to perform topic modelling on query search results","2020-03-06 04:46:23","69337","1","241","<nlp><topic-model><lda>","<p>How can I model topics in the results returned by a search engine with higher weightage to documents ranked higher in the result set?</p>

<p>The use case that I am looking at involves extracting the most significant topics returned in the search results.</p>

<p>Eg. If the user searches for a query q1 which returns documents D1...Dn with scores S1...Sn (in descending order) then I propose the notion that the theme of such a set of documents is represented better by documents scored higher in the result set.</p>

<p>Is it possible to incorporate this information in to topic modelling algorithms like LDA? </p>
","nlp"
"69235","Detection of anomal data in the text","2020-03-05 19:05:43","69400","0","39","<deep-learning><nlp><speech-to-text>","<p>I work with texts where there is a dialogue between two people (a client and a call center employee, the beginning and end of each person’s phrase is not defined). My goal is to classify texts in which a call center employee names words from my list.
If the texts are manually marked up, can such a classification problem be solved?
Are there any tricks to solve this type of problem?</p>

<p>Sample data:
""hello hello my name is Sam Chin I'm calling for pizza delivery Okay now check your order wait a minute Sam""</p>
","nlp"
"69210","Generating text using NLP based on parameters","2020-03-05 12:53:18","69213","0","319","<python><nlp>","<p>I want to generate some text based on the value of certain parameters. For instance, let's say I want to generate descriptions of video games. So, besides real descriptions as training data, I would like that the model takes in account the following parameters (for example) about the game:</p>

<ul>
<li>Violent: yes </li>
<li>Multiplatform: yes </li>
<li>Drugs: no</li>
</ul>

<p>So that if the game has drugs content, the output text has some phrase referring to it. </p>

<p>Is this possible? If so, how could I do it in Python? I was going to use LSTM neural networks in Tensorflow. </p>
","nlp"
"69209","Generating synonyms or similar words from multiples word embeddings","2020-03-05 12:14:52","","5","11452","<nlp><bert>","<p>I am looking for a way to generate synonyms, using word embeddings. From one word, and from multiple words. Such as the two example below:</p>

<p>""word"" -> Word embedding -> generate synonym of ""word""</p>

<p>""word"", ""synonym of word .. ""->  -> Word embeddings -> generate a synonym of both word</p>

<p>I am very new at this. What do you think I should use ?</p>

<p>I also want to use a tools that, in further work, would take into account context for word embedding generation, such as:</p>

<p>""sentence in including a word"" -> Word embedding of word -> generate synonym of ""word"" in that context</p>

<p>I think I will start to do it with BERT... How should I start ? or which alternative should I use ?</p>

<p>Thanks for your help !</p>
","nlp"
"69198","How to train a supervised sequence classifier like CRF, if we have to extract start date and end date from a user query in python","2020-03-05 08:45:09","","2","91","<machine-learning><python><nlp><named-entity-recognition>","<p>I have to build a chatbot, in python in which a user can apply for a leave. I want to extract start date and end date from a users query. I did some research on couple of algorithms and found CRF Entity Extractor as the best one. I now want to see a similar implemented solution in python, which I can use it as a reference. I would like to see an end to end solution right from training dataset to predicting start and end dates from query. Kindly help.</p>

<p>For example:</p>

<p>1.query: ""I want to take leave from 2nd April to 5th April.""
Predicted dates: ""02-04-2020"" and ""05-04-2020""</p>

<p>1.query: ""I want to take leave on next Monday""
Predicted dates: ""09-03-2020""</p>
","nlp"
"69114","How to generalize comments using NLP","2020-03-04 05:37:05","","2","188","<machine-learning><nlp><ai>","<p>I have list of log comments in CSV file. I want to cluster those log comments using K-Means and after that I want convert each cluster comments into general form.
<strong>for eg. I have bunch of comments in one cluster which starts from ""Reservation number failed......"" and I want to convert those comments into particular comment like ""Reservation failure"".</strong></p>

<p>I can achieve this by giving specific name to each cluster after seeing each cluster. But I don't want like this. I want to create intelligent model which automatically create generalized comments for me.</p>

<p>I would not like to assign name to each cluster. Basically I am done with clustering part. that is, I have lets say 3 clusters as below</p>

<ul>
<li>cluster 0 : list of comments like ""Reservation number failed......"",
total comments: 15</li>
<li>cluster 1 : list of comments like ""Request timeout failed due to
......"", total comments:9</li>
<li>cluster 2 : list of comments like ""Dinning reservation successfully
completed..."", total comments: 5</li>
</ul>

<p>I want to build model that intelligently assign name to each cluster by its contents. for eg .</p>

<ul>
<li>cluster 0 will get name as ""Reservation failure""</li>
<li>cluster 1 will get name as ""Request timeout failure""</li>
<li>cluster 2 will get name as ""Dinning reservation successful""</li>
</ul>

<p>if after training more data with some different comments. it should create another cluster and assign the name as per content.</p>
","nlp"
"69054","Student answer evaluation","2020-03-03 10:59:03","","1","52","<nlp>","<p>Given a question, with some model answers and their grades, and possibly other 'learning material' content.</p>

<p>What tools or technologies can be used, to implement a system that evaluates a student answer?</p>

<p>I am new to machine learning.</p>

<p>Thanks.</p>
","nlp"
"69044","How to generate a sentence with exactly N words?","2020-03-03 09:30:27","","3","161","<nlp><bert><ai><text-generation><gpt>","<p>Thanks to GPT2 pretrained model now it is possible to generate meaningful sequence of words with or without prefix. However a sentence should end with a proper endings (.,!,?). I am just wondering how to generate a sentence (with proper ending) of length N? </p>

<p>One possible approach is post-processing, that is process many sequences and choose the ones the serve the purpose! However, it could be a really daunting task to use in any pipeline. </p>

<p>Is there any suggestion, perhaps a secondary algorithm, to tune the hyper-parameter such that it produces sentence of desired length with higher probabilities. </p>
","nlp"
"69032","Does it make sense to use a tfidf matrix for a model which expects to see new text?","2020-03-03 03:08:34","69073","1","336","<nlp><text-classification>","<p>I'm training a model to classify tweets right now. Most of the text classification examples I have seen convert the tweets into tf-idf document term matrices as input for the model. However, this model should be able to identify newly collected tweets without retraining. Does it make sense to use tf-idf in this context? What is the correct way to turn tweets into feature vectors in this task?</p>
","nlp"
"69021","Training pipelines where featurization/NLP is more expensive than backprop","2020-03-02 20:25:18","","2","30","<neural-network><tensorflow><nlp><training><word-embeddings>","<p>I'm working on a document classification project and I'm using a neural net in tensorflow, where the features are 300-dimensional word embeddings, either from fastext or word2vec (yes I know that there are newer better embeddings -- this is for backwards compatibility).  </p>

<p>Each of the documents is very long -- 3000+ tokens.  </p>

<p>Featurization involves tokenizing the document, embedding it, and stacking the embeddings into an array of dimension <code>batchsize, max_doc_length, features</code>.  It takes more than 5x as long to make a batchfile (on a CPU) as it does to do one iteration of backprop (also on a CPU).  My problem would be worse on a GPU, if backprop went faster.</p>

<p>My batches are almost 1GB in size, so I'd use almost a whole TB if I were to write everything to disk before starting backprop.  That's too much, especially given that I want to try different featurization strategies.</p>

<p>So, what are some good solutions to this problem?  Are there software packages that are designed to solve this problem?  </p>

<p>Right now I've got two processes going in parallel -- one that makes batches and saves them to the disk (in parallel) and the other which loads them, uses them for backprop, then deletes them.  It feels sort of janky, and I can't be the first person who has faced this issue.  </p>

<p>Suggestions appreciated!</p>
","nlp"
"68932","Distribution plot of word embeddings","2020-02-29 19:29:18","","2","477","<nlp><word-embeddings>","<p>I have a list of sentences and a 10 dimensional embedding for each of the sentence.   I am trying to visualize these embeddings so that i can see if several sentence embeddings form a cluster such that these have similar embeddings which in turn mean similar context's. How can i do this in python? Also, i am wondering if there's any good metric or method using which i can form cluster's or calculate similarity between sentences that can tell me how close two sentences are? </p>
","nlp"
"68760","Changing word inflections","2020-02-26 22:17:23","","0","50","<nlp>","<p>This might be an unusual question. I have a situation where I am creating paraphrases with a rule based system. One transformation that I'd like to implement would one that gets rid of light verbs, like so:</p>

<p>a) Steven made an unwilling concession to us.</p>

<p>b) Steven unwillingly conceded to us.</p>

<p>To go from (a) to (b) requires some inflectional changes such as:</p>

<p>unwilling -> unwillingly</p>

<p>concession -> conceded</p>

<p>Is there something out there that can take care of such inflectional changes reliably for a given word? Seems like there should be, but I don't recall seeing anything like this.</p>
","nlp"
"68700","Extracting amount from free text","2020-02-26 05:34:50","68711","0","630","<nlp><text-mining>","<p>I want to extract various amounts and tenure of contracts from different contract documents that we have.</p>
<p>For example: Mr xyz, this contact is valid for 3 Months and you have to pay $3000 as agreement fee.</p>
<p>Expected output : 3 Months, $3000</p>
<p>Please note that this is just an example but the sequence, format, currency and tenure is not fixed in the actual problem.</p>
","nlp"
"68673","How to train a Word2vec model so that particular words set would reside close to each other in the vector space?","2020-02-25 13:09:26","","1","196","<machine-learning><deep-learning><nlp><word-embeddings>","<p>I am supposed to build a resume parser. For the skill extraction part,  currently I am matching bi-grams and uni-grams in a CV against a predefined skill set, which is not that successful. Can I train a Word2vec model so that words and phrases in a cv which could classify as skills to have similar word vectors?</p>

<p>Is there any method which could be more successful?
Please help. Thank you in advance.</p>
","nlp"
"68647","The meaning of random word dropout in NLP","2020-02-24 21:31:45","","1","1160","<neural-network><nlp><regularization>","<p>I have been reading the early paper on pre-training in NLP (<a href=""https://arxiv.org/abs/1511.01432"" rel=""nofollow noreferrer"">https://arxiv.org/abs/1511.01432</a>) and I can't understand what <strong>random word dropout</strong> means. The authors completely ignore explaining this method as if it was a standard thing. Can someone explain what they really do and what is the purpose of that?</p>
","nlp"
"68637","NLP varying amount of features and BoW as feature concatenating to feedforward NN","2020-02-24 19:46:17","68913","3","479","<machine-learning><neural-network><classification><nlp>","<p>I was looking at <a href=""https://ai.googleblog.com/2018/08/the-machine-learning-behind-android.html"" rel=""nofollow noreferrer"">Google's Smart linkify machine learning models</a>, as it closely relates to a personal project. And couldn't quite understand how the features are fed to the neural network.</p>

<p>It's about the following:</p>

<p><a href=""https://i.sstatic.net/91jyz.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/91jyz.png"" alt=""enter image description here""></a></p>

<blockquote>
  <p>Given a candidate entity span, we extract: Left context: five words
  before the entity, Entity start: first three words of the entity,
  Entity end: last three words of the entity (they can be duplicated
  with the previous feature if they overlap, or padded if there are not
  that many), Right context: five words after the entity, Entity
  content: bag of words inside the entity and Entity length: size of the
  entity in number of words. They are then concatenated together and fed
  as an input to the neural network.</p>
</blockquote>

<p>I have 4 primary questions which I cant find a clear answer to:</p>

<ul>
<li><p>The article specifies the features are concatenated. How does
a concatenation layer work internally? Does it concatenate all the
values in a single variable, in a very literal sense? how does that
work computationally?</p></li>
<li><p>How can a Bag of Words be a feature, when its a key-value pair? Or is it
also just all concatenated into one variable. Which again, how can
that work computationally?</p></li>
<li><p>The text specifies multiple words are used as a single feature; e.g. <code>Left context: five words before the entity</code>. Is this again concatenating the embedding / vectors?</p></li>
<li><p><code>Entity end: last three words of the entity (they can be duplicated with the previous feature if they overlap, or padded if there are not that many)</code> does this mean a variable amount of features as input to the NN (or concatenation layer) or is this more intended as a configuration? Less context available so fewer amount of 'hard' coded input features? </p></li>
</ul>

<p>Perhaps a simple keras model with some hardcoded input variables would help shape the answer.</p>

<p>Is there more material online to understand and recreate the entity recognition model?</p>

<p>EDIT:</p>

<p>The article also mentions the lack of available context or entities. E.g. the diagram shows 4 features for entity (2 for entity start, 2 for entity end). The article mentions duplication, but duplicating 3 times doesnt sound like a great idea. Would a convolution layer with a filter (1x3) work better? </p>

<p>And how would a Keras model look like then? Would it have two separate input layers? One input layer with 10 input features for the context + 1 feature for BoW + 1 feature for entity length. And another input layer with 4 input features followed by a convolution layer. And then both layers lead to a concatenate layer?</p>
","nlp"
"68626","Get row wise frequency count of words from list in text column pandas","2020-02-24 16:37:04","104321","3","936","<nlp><pandas><text-mining><word-embeddings><python-3.x>","<p>I have a data frame with a Audio Transcript column from customer care phone conversation. I have created one list with words and sentences </p>

<pre><code>words = [""rain"", ""buy new house"", ""tornado""]
</code></pre>

<p>What I need to do is create a column in the data frame which checks these words in the text column row by row and if it presents then update the column with word and it's frequency. For example first row text</p>

<pre><code>""I was going to buy new house last week but it was raining since then. Once the rain stops I'll go and buy new house""
</code></pre>

<p>the column should read</p>

<pre><code>{""buy new house"",2}, {""rain"",2}
</code></pre>

<p>or may be create a duplicate row and add the comma part in next row. </p>

<p>How to proceed in this as I am fairly new.</p>
","nlp"
"68608","LSTM for text with different sentences size, but same input-output sizes","2020-02-24 12:28:41","","1","225","<nlp><lstm><word-embeddings>","<p>Hello fellow Data Scientists</p>

<p>I'm trying to use a LSTM (using word embeddings) to generate a system that can tag each word of a sentence. For this, I give it a set of sentences of different sizes and using padding I make the sentences all the same size, so that the LSTM can process them. </p>

<p>The problem is, the LSTM results, after excluding the tags that correspond to the padding portion give outputs of different size than the input.</p>

<p>So my question is, is this an architecture problem or it might be the case that more training (epocs word bigger batches) is needed?</p>

<p>Thanks for the help!</p>

<p>Edit: More details:</p>

<p>In the following picture I show my LSTM Architecture</p>

<p><a href=""https://i.sstatic.net/6E22N.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/6E22N.png"" alt=""enter image description here""></a></p>

<p>Upon running for a sentence of size n (using one hot encoding and word embeddings) I get a result, that is usually of diferente size; by result I mean final output of the lstm and corresponding translation into words.</p>

<p>Also, there are the variables I have at the moment, with which I've played around:</p>

<pre><code>BATCH_SIZE = 164
EPOCHS = 50
LSTM_NODES = 128
NUM_SENTENCES = 3000
MAX_NUM_WORDS = 250000
EMBEDDING_SIZE = 300
</code></pre>

<p>Sample input:</p>

<pre><code>1  Cada
2  obra
3  consome
4  1,5
5  tonelada
6  de
7  aço
8  (
9  US<span class=""math-container"">$
10 6
11 mil
12 )
13 mais
14 US$</span>
15 10
16 mil
17 de
18 mão-de-obra
19 .
</code></pre>

<p>Sample output:</p>

<pre><code>1  c-am-prd*
2  c-am-prd*
3  (c-v*)
4  (am-rec*)
5  (c-v*)
6  (c-v*)
7  (c-v*)
8  (c-v*)
9  am-dis*
</code></pre>
","nlp"
"68588","What is the best way to encode an arbitrary collection of strings into int categorical variables?","2020-02-24 02:47:02","","0","283","<nlp><feature-engineering><categorical-data><text><categorical-encoding>","<p>I have a bunch of categorical labels which I want to transform into int categorical features for an ML algorithm. 
The problem is I don't have a prior list of the categories, so that I can't just define a dictionary or mapping function before hand. </p>

<p>Say for example that I am using food labels - my current data has the following labels:
<code>['Steak','Potatoes','Soup']</code>, but it is possible that later, I will gate data with the labels <code>'Asparagus'</code> or <code>'Chow mein'</code>, and I have no way of knowing the list of all potential labels before hand. Moreover it is possible that some of the incoming labels are proper names or strings that are idiosyncratic and not part of any standard vocabulary, e.g. <code>'Double Super Mac-Whopper'</code>. </p>

<p>I thought of simply building my own hash map, but then I would have to put a lot of effort into saving and versioning the resulting map to maintain consistency across experiments and later in production.</p>

<p>I tried using the <code>int.from_bytes</code> function in Python 3, but it gives wildly varying int sizes (I think because it is using string length):</p>

<pre><code>&gt; int.from_bytes('steak'.encode('utf-8'),'little')
461195539571
&gt; int.from_bytes('milk'.encode('utf-8'),'little')
1802266989
&gt; int.from_bytes('Bok Choy'.encode('utf-8'),'little')
8750327238520172354
</code></pre>

<p>I looked at the sklearn categorical encoders (<code>preprocessing.LabelEncoder()</code> or <code>sklearn.feature_extraction.FeatureHasher</code>), but they all seem to require knowledge of the number of categories before hand (by having to specify a dictionary or fitting an encoder to the available data, etc...) </p>

<p>I thought about using some word embeddings like word2vec, but they return pretty large vectors, and all I need is an int, and I don't really care about semantic similarity etc...(i.e. using a word embedding is overkill). </p>

<p>Is there some sort of preprocessing utility from and ML library, or some publicly available string to int hash map that is stable that I can use?  </p>
","nlp"
"68562","What is the bleu score of professional human translators?","2020-02-23 17:08:50","68577","21","4868","<nlp><machine-translation>","<p>Machine translation models are usually evaluated using bleu score. I want to get some intuition for this score. What is the bleu score of professional human translator? </p>

<p>I know it depends on the languages, the translator ect. I just want to get the scale.</p>

<p>edit: I want to make it clear - I talk about the expected bleu. It's not a theoretical question, it is an experimental one.</p>
","nlp"
"68553","Why does the transformer positional encoding use both sine and cosine?","2020-02-23 12:54:49","","11","7153","<machine-learning><nlp><transformer><attention-mechanism>","<p>In the transformer architecture they use positional encoding (explained in <a href=""https://datascience.stackexchange.com/questions/51065/what-is-the-positional-encoding-in-the-transformer-model"">this answer</a> and I get how it is constructed. </p>

<p>I am wondering why it needs to use both sine and cosine though instead of just one or the other?</p>
","nlp"
"68537","Checking if english sentences have impact catpured in them using NLP","2020-02-23 00:15:08","68586","2","34","<nlp>","<p>I am looking for high level approach to identify <strong>if a given sentence has impact captured in it</strong>. For example in below two examples, <strong>sentence 2</strong> has impact captured.</p>

<h3>Example 1</h3>

<p>Sentence 1: I learnt speaking spanish this year.</p>

<p>Sentence 2: I learnt speaking spanish this year which helped expand my business to Mexico. </p>

<h3>Example 2</h3>

<p>Sentence 1 : I taught machine learning to my students this year.</p>

<p>Sentence 2 : I taught machine learning to my students earlier this year which resulted in increasing base salary of students by 20,000 Dollars.</p>

<p><strong>My initial thought is below approach.</strong></p>

<ol>
<li>think of all the english words like helped, resulted etc., </li>
<li>Apply these words as filters on a public domain data set to get sentences </li>
<li>Curate the sentences to see if they fall in the ""Sentence 2"" category.</li>
<li>Apply classification/modelling to categorize into sentences with impact and without impact</li>
</ol>

<p><strong>Question</strong>:</p>

<ol>
<li>Is the above approach good or there are better approaches?</li>
<li>What is the best publicly available dateset to tackle this problem.</li>
</ol>

<p>Any inputs are greatly appreciated. Thanks alot in advance!</p>
","nlp"
"68487","How can I find synonyms and antonyms for a word?","2020-02-21 18:48:32","","0","832","<python><nlp><nltk>","<p>I found some code online where I can feed in a word, and find both synonyms and antonyms for this word.  The code below does just that.</p>

<pre><code>import nltk
from nltk.corpus import wordnet   #Import wordnet from the NLTK
syn = list()
ant = list()
for synset in wordnet.synsets(""fake""):
   for lemma in synset.lemmas():
      syn.append(lemma.name())    #add the synonyms
      if lemma.antonyms():    #When antonyms are available, add them into the list
          ant.append(lemma.antonyms()[0].name())
print('Synonyms: ' + str(syn))
print('Antonyms: ' + str(ant))
</code></pre>

<p>My question is, how can I choose a word, like 'and', and find all synonyms and antonyms based on comments that exist in a field in a dataframe?  Here is a sample of the first 10 lines from my dataframe.  </p>

<pre><code>feels weird; may be a fake!
package came end missing box. since itâ€™s gift i update actual fit.
birkenstock amazing shoe!!!! i wish i ten pairs!
delivered advertised.... shoe looks fake.
second pair i had. nothing beats them.
they totally fake ðŸ˜¡. they felt weird i finally noticed â€œmade germanyâ€ logo above. they also smell like glue leather. infuriating happen!
i've birkenstock wearer 35 years. i wear 10 women's size ones i ordered wear big. i need size down.
great brand, packaging good, dad likes them. the size clearly needs fixed others don't order bigger sizes wanted. some people don't want bigger size actually wear.
false advertising.
a bit loose compared birkenstocks size. still like though. very comfy
</code></pre>
","nlp"
"68458","Why gaussian assumption in GMM-HMM ASR?","2020-02-21 10:22:50","","1","69","<machine-learning><nlp><data-science-model><speech-to-text>","<p>I am reading a book entitled <em>Speech and Language Processing</em> by Daniel Jurafsky and James.</p>
<p>For Acoustic vector, I would like to know why Gaussian assumption is made?</p>
<p>I searched over the internet and could not find the reason.</p>
","nlp"
"68407","Computer science corpus for training a language model","2020-02-20 13:06:52","","1","223","<nlp><data-mining><text-mining><text><corpus>","<p>I am looking for a <strong>domain specific computer science corpus</strong> of at least 20M words (preferable >50M words), for the purpose of training a language model in it. </p>

<p>Is there anything out-of-the box that I could use? 
*I tried to look for the sciBERT corpus, can not find how to access it. </p>

<p>Thanks!</p>
","nlp"
"68402","Comparing one small dataset with a big dataset for similar records","2020-02-20 11:28:05","68413","3","450","<nlp><data-mining><bigdata><similarity>","<p>I create a varying small dataset (dataset: X) with 500 records in each query. Everytime I need to compare the dataset with a bigger one (dataset: A) (15 milion records) to find similar (or semi-silmilar) values from three different columns. The values are either one word or a sentences. My algorithms is like this: </p>

<ol>
<li>make a vector of words in each record in both datasets</li>
<li>with a for loop, search for similarities over the big dataset (e.g. with tfidf). That means each record from the small dataset should look for possible similarities over the big dataset.</li>
</ol>

<p>However, the problem is searching a big data is very slow. Is there any efficeint way to solve this problem?
Thanks</p>
","nlp"
"68373","BertPunc (punctuation restoration with BERT)","2020-02-19 23:06:57","","5","1375","<nlp><bert>","<p>I've found the <a href=""https://github.com/nkrnrnk/BertPunc"" rel=""nofollow noreferrer"">script</a> for punctuation restoration. And I have one question about this method.</p>
<p>I will briefly explain the logic of the author. One of four tokens is assigned for each word: Other (0), PERIOD (1), COMMA (2), QUESTION (3). Further, all words are converted to BERT tokens. Here is an example:</p>
<pre><code>  2045  0
  2003  0
  2200  0
  2210  0
  3983  0
  2301  0
  2974  0
  1999  0
  2068  2
</code></pre>
<p>Next, we do padding. We set a segment (e.g. eight words) and for each word we take two words before a token and four words after a token. Also, we add a padding token after each word. For the very first word, there are no words before. Therefore, a word are taken from the end. Similarly, for the last word, there are no words after and therefore a word are taken from the beginning.
Here is an example of it.</p>
<pre><code>[1999, 2068, 2045, 0, 2003, 2200, 2210, 3983] 0
[2068, 2045, 2003, 0, 2200, 2210, 3983, 2301] 0
[2045, 2003, 2200, 0, 2210, 3983, 2301, 2974] 0
[2003, 2200, 2210, 0, 3983, 2301, 2974, 1999] 0
[2200, 2210, 3983, 0, 2301, 2974, 1999, 2068] 0
[2210, 3983, 2301, 0, 2974, 1999, 2068, 2045] 0
[3983, 2301, 2974, 0, 1999, 2068, 2045, 2003] 0
[2301, 2974, 1999, 0, 2068, 2045, 2003, 2200] 0
[2974, 1999, 2068, 0, 2045, 2003, 2200, 2210] 2
</code></pre>
<p>The first column contains tokens, and the second column contains punctuation symbols. In first column, &quot;0&quot; corresponds to a padding. Next we do TensorDataset, and than DataLoader. In the second column, '0' corresponds to &quot;other&quot;, and '2' corresponds to a &quot;period&quot;. Finaly we train a model:</p>
<pre><code>for inputs, labels in data_loader_train:
       inputs, labels = inputs.cuda(), labels.cuda()
       output = model(inputs)
</code></pre>
<p>The algorithm works well, but I do not understand the following.
What's the point of putting padding in the middle? Maybe we can do punctuation restoration with BERT in a more simple way?</p>
","nlp"
"68220","How are Q, K, and V Vectors Trained in a Transformer Self-Attention?","2020-02-17 09:55:54","","9","10290","<machine-learning><nlp><sequence-to-sequence><transformer><attention-mechanism>","<p>I am new to transformers, so this may be a silly question, but I was reading about transformers and how they use attention, and it involves the usage of three special vectors. Most articles say that one will understand their purpose after reading about how they are used for attention. I believe I understand what they do, but I'm unsure about how they are created.</p>

<p>I'm aware that they come from the multiplication of the input vector by three corresponding weights, but I'm not sure how these weights are derived. Are they chosen at random and trained like a standard neural network, and if so how if there's no predefined attention data in the training corpus? </p>

<p>I'm very new to this, so I hope everything I'm saying makes sense. If I've got something completely wrong, please tell me! </p>
","nlp"
"68172","UniLM - Unified Language Model for summarization","2020-02-16 10:57:10","68180","1","734","<nlp><automatic-summarization>","<p>The UniLM claims to be the best approach for summarization task. But there doesn't seem to be any tutorial or how-to section in the README.md or any other blog. How exactly can I use this state-of-the-art library for abstractive summary generation?</p>
<p><a href=""https://github.com/microsoft/unilm"" rel=""nofollow noreferrer"">Github link</a></p>
<p><a href=""https://arxiv.org/pdf/1905.03197.pdf"" rel=""nofollow noreferrer"">Paper</a></p>
<p>P.S. A newbie in NLP. Sorry if this is a dumb question.</p>
","nlp"
"68167","Methods for learning with noisy labels","2020-02-16 10:13:27","","2","206","<deep-learning><nlp><sequence-to-sequence><noise>","<p>I am looking for a specific deep learning method that can train a neural network model with both clean and noisy labels.</p>

<p>More precisely, I would like this method to be able to leverage noisy data as well, for instance by not fully ""trusting"" noisy data, or weighting samples, or deciding whether to use a specific sample at all for learning. But primarily, I am looking for inspiration.</p>

<p>Details:</p>

<ul>
<li>My task is sequence-to-sequence NLP,</li>
<li>I have both clean pairs of sequences of <code>(clean input, clean output)</code> and noisy ones <code>(noisy_input, noisy_output)</code>,</li>
<li>I know for certain which samples in my data are noisy, and if possible, I would like the desired method to make use of this information</li>
</ul>

<p>I am very glad to give more information about my use case if needed.</p>

<p><strong>Edit: Noisy vs. negative examples</strong></p>

<blockquote>
  <p>First, I wouldn't use the word ""noisy"" here because if you know which instances are ""wrong"" then these are not noise, they are negative examples. </p>
</blockquote>

<p>My view is that the data I have are noisy examples, but not ""negative"". Using an example from machine translation from German to English:</p>

<p><strong>clean (equivalent meaning)</strong></p>

<pre><code>DE Wenn es um die Medien geht, lebt Amerika in einem Paralleluniversum.
EN Regarding media, the US are living in a parallel universe.
</code></pre>

<p><strong>noisy (meaning overlap)</strong></p>

<pre><code>DE Wenn es um die Medien geht, lebt Amerika in einem Paralleluniversum.
EN Regarding media, the US are weird.
</code></pre>

<p><strong>negative (unrelated)</strong></p>

<pre><code>DE Wenn es um die Medien geht, lebt Amerika in einem Paralleluniversum.
EN Is Math related to science?
</code></pre>
","nlp"
"68155","What are some key strengths of BERT over ELMO/ULMFiT?","2020-02-16 03:28:36","","10","10928","<deep-learning><nlp>","<p>I see BERT family is being used as benchmark everywhere for NLP tasks. What are some key strengths of BERT over models like ELMO or ULMFiT?</p>
","nlp"
"68116","More pre-trained embeddings for PyTorch Big Graph","2020-02-15 01:03:31","","2","106","<nlp><word-embeddings><graphs>","<p>Apart from the entire <a href=""https://dl.fbaipublicfiles.com/torchbiggraph/wikidata_translation_v1.tsv.gz"" rel=""nofollow noreferrer"">wikidata</a>, are there any other <a href=""https://github.com/facebookresearch/PyTorch-BigGraph"" rel=""nofollow noreferrer"">PyTorch Big Graph</a> pre-trained graph embeddings on smaller sized knowledge graph, like freebase-15k? I do not have the resources to build it from scratch.</p>
","nlp"
"68082","Semantic text similarity using BERT","2020-02-14 12:01:07","68085","2","3204","<python><nlp><bert>","<p>Given two sentences, I want to quantify the degree of similarity between the two text-based on Semantic similarity.
Semantic Textual Similarity (STS) assesses the degree to which two sentences are semantically equivalent to each other.
 say my input is of order:</p>

<pre><code>index    line1                       line2
0        the cat ate the mouse      the mouse was eaten by the cat
1        the dog chased the cat     the alligator is fat 
2        the king ate the cake      the cake was ingested by the king
</code></pre>

<p>after application of the algorithm , the output needs to be </p>

<pre><code>index    line1                       line2                           lbl
0        the cat ate the mouse      the mouse was eaten by the cat    1
1        the dog chased the cat     the alligator is fat              0
2        the king ate the cake      the cake was ingested by the king 1
</code></pre>

<p>Here lbl= 1 means the sentences are semantically similar and lbl=0 means it isn't.
How would i implement this in python ?
I read the documentation of <a href=""https://github.com/hanxiao/bert-as-service#book-tutorial"" rel=""nofollow noreferrer"">bert-as-a-service</a> but since i am an absolute noob in this regard I couldn't understand it properly.</p>
","nlp"
"68074","Does Fasttext use One Hot Encoding?","2020-02-14 10:06:10","68915","3","467","<nlp><word-embeddings><one-hot-encoding>","<p>In the original Skipgram/CBOW both context word and target word are represented as one-hot encoding.</p>

<p>Does fasttext also use one-hot encoding for each subword when training the skip-gram/CBOW model (so the length of the one-hot encoding vector is |Vocab| + |all subwords|)? If they use it, do they use it in both context and target words?</p>
","nlp"
"68035","Paraphrase Generation - state-of-the-art?","2020-02-13 17:29:24","","1","159","<nlp><text-generation>","<p>I need to paraphrase short, but abstract sentences, such as:</p>

<blockquote>
  <p>He prefers variety to routine.</p>
</blockquote>

<p>or</p>

<blockquote>
  <p>I am easily discouraged.</p>
</blockquote>

<p>I've played around couple of online tools (such as <a href=""https://quillbot.com/"" rel=""nofollow noreferrer"">https://quillbot.com/</a>) just to get a feeling for how well this particular task is solved by available models and was quite disappointed. Hence my question:</p>

<p><strong>What is current state-of-the-art in paraphrase generation?</strong></p>
","nlp"
"68025","How to get sentence from embedding vector with Universal Sentence Encoder?","2020-02-13 15:16:03","","5","1479","<tensorflow><nlp><word-embeddings>","<p>I'd like to ask, if there is possibility to get sentence (or word) from embedding vector using <a href=""https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3"" rel=""nofollow noreferrer"">Universal Sentence Encoder</a>?</p>

<p>First of all, I've clustered my embedded sentences and I've got a vector which is representing center of the cluster, and now I want to convert this to some sentence which will have the best meaning for this specific cluster.</p>

<p>Best regards.</p>
","nlp"
"68020","What is the feedforward network in a transformer trained on?","2020-02-13 14:09:33","68067","1","1893","<neural-network><nlp><autoencoder><transformer><attention-mechanism>","<p>After reading the 'Attention is all you need' article, I understand the general architecture of a transformer. However, it is unclear to me how the feed forward neural network learns. </p>

<p>What I learned about neural nets is that they learn based on a target variable, through back propagation according to a particular loss function. </p>

<p><a href=""https://i.sstatic.net/ofQsr.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ofQsr.png"" alt=""Feed forward neural net""></a></p>

<p>Looking at the architecture of a Transformer, it is unclear to me what the target variables are in these feed forward nets. Can someone explain this to me?</p>

<p><a href=""https://i.sstatic.net/BcfGr.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/BcfGr.png"" alt=""The transformer architecture""></a></p>
","nlp"
"68016","How to preprocess data for Word2Vec?","2020-02-13 12:48:01","68019","3","5903","<nlp><data><preprocessing><word-embeddings><word2vec>","<p>I have text data which is crawled from websites. I am preprocessing data to train Word2Vec model. Should I remove stopwords and do lemmatization? How to preprocess data for Word2Vec?</p>
","nlp"
"67984","Pretrained Models for Keyword-Based Text Generation","2020-02-12 16:12:59","","1","73","<nlp><transformer><text-generation><gpt>","<p>I'm looking for an implementation that allows me to generate text based on a pre-trained model (e.g. GPT-2).</p>
<p>An example would be <a href=""https://github.com/minimaxir/gpt-2-keyword-generation"" rel=""nofollow noreferrer"">gpt-2-keyword-generation</a> (<a href=""https://minimaxir.com/apps/gpt2-reddit/"" rel=""nofollow noreferrer"">click here for demo</a>). As the author notes, there is</p>
<blockquote>
<p>[...] no explicit mathematical/theoetical basis behind the keywords
aside from the typical debiasing of the text [...]</p>
</blockquote>
<p>Hence my question: <strong>Are there more sophisticated ways of keyword-based text generation</strong> or at least any other <strong>alternatives</strong>?</p>
<p>Thank you</p>
","nlp"
"67967","Creating pronunciation dictionary for ASR","2020-02-12 12:20:09","","2","369","<machine-learning><dataset><nlp><speech-to-text>","<p>I am working on ASR(automatic speech recoginition) on Somali data as master thesis and now I am stuck with how to create a phonetics or pronunciation dictionary for it. I searched over net and could not find one. </p>

<p>I'm not sure how to tackle this. Can someone guide me ?</p>
","nlp"
"67942","Does building a corpus make sense on a documentation project?","2020-02-12 05:33:57","67992","0","56","<machine-learning><nlp><corpus>","<p>I have zero to experience in data science or machine learning. Because of this I am not able to determine if building a corpus does apply to the problem I am trying to solve.</p>

<p>I am trying to build a reference site for cloud technologies such as AWS Google Cloud. </p>

<p>I was able to build structured data and identify primary entities with in a single ecosystem using standard web scraping and sql.queries. </p>

<p>But I wanted to have the ability to have a mechanism that can autonomously identify entities and related information that is relevant to that entity and other entities it has relationships with. </p>

<p>Given that a specific ecosystem documentation follows a certain style can I use few entities as training docs and then have it classify the information I mentioned above. </p>

<p>Is the starting point to this is to build a corpus? I tried it out nltk categorized corpus builder. </p>

<p>Is it fine to include a specific document in multiple categories? For example an instance in AWS can be in category ec2 and a general category Computing unit </p>

<p>Anyways is this problem I am trying to solve fit into the general NLP ML space? </p>
","nlp"
"67914","What are the elements in a BERT word embedding?","2020-02-11 19:10:10","67968","3","3548","<word-embeddings><nlp><bert><language-model>","<p>As far as I understand, BERT is a word embedding that can be fine-tuned or used directly. </p>

<p>With older word embeddings (word2vec, Glove), each word was only represented once in the embedding (one vector per word). This was a problem because it did not take homonyms into account. As far as I understand, BERT tackles this problem by taking context into condsideration. </p>

<p>What does this mean for the word embedding itself? Is there still one vector for each word token? If so, how is context taken into consideration? If no, what is the format of the embedding?</p>
","nlp"
"67763","NER vs Text classification for very short sentences","2020-02-09 09:15:22","","2","1478","<nlp><text-mining>","<p>Given a large set of short sentences (around 20-30 words) and multi label task (around 100 labels , can be to 3 labels per sentences ). <br> </p>

<p>The location of each annotation is not impotent (i.e i only need to know if the annotation is included in the sentence) </p>

<p>Which method will be more beneficial ? using NER models with labels attached to the tokens from each sentence, or text classification where the sample is the whole sentence .</p>

<p>The labels are action that physician  is doing (i.e ""clean wound"" , ""remove skin"" etc)</p>
","nlp"
"67666","What does online learning mean in Topic modeling (LDA) - Gensim","2020-02-07 03:37:39","67669","1","829","<machine-learning><nlp><text-mining><topic-model><lda>","<p>I came across this line in the Gensim Documentation- <a href=""https://radimrehurek.com/gensim/models/ldamodel.html"" rel=""nofollow noreferrer"">Gensim LDA</a>  - ""The model can also be updated with new documents for online training."" </p>

<p>So my assumption on what it means is - 'Once we have a model trained on one corpus, we can add new data and continue to train the model with new data thereby adding more vocabulary and enriching results. <strong>Is this correct?</strong> </p>

<p>Is this the same approach discussed in the paper - <a href=""https://papers.nips.cc/paper/3902-online-learning-for-latent-dirichlet-allocation"" rel=""nofollow noreferrer"">Online Learning for LDA</a> ? Help me understand this technique.</p>
","nlp"
"67614","BERT word embedings for finding word definition","2020-02-05 22:30:01","","4","479","<nlp><word-embeddings><bert>","<p>Can BERT, GPT or other contextualised embedings be used for finding word definitions? What would be the most effective and not complicated approach for tackling a sample task as described below.</p>

<p>Map the meaning of the word 'bank' in the sentence ""I was walking along the bank of the river"" with one of the definitions listed in the WortNet database (or other word-sense lookup table).</p>
","nlp"
"67576","What does linearly decreasing training and accuracy loss means?","2020-02-05 11:05:52","","1","748","<deep-learning><nlp><lstm>","<p>I am trying to train an LSTM autoencoder for sentence embeddings:- </p>

<pre><code>Embedding size = 600
LSTM Hidden size = 600
Vocab Len = 3972
Number of LSTM layers = 1
Bidirectional = True
</code></pre>

<p>Here the architecture of the encoder is:-</p>

<pre><code>Embedding layer mapping 3972 nodes to 600
LSTM taking the whole sequence and giving output and state of shape (2*num_layers, 600)
</code></pre>

<p>This output and state is then used to set state and hidden state of a decoder, whose architecture is:-</p>

<pre><code>Embedding layer mapping 3972 nodes to 600
LSTM taking the whole sequence and giving output and state of shape (2*num_layers, 600)
Softmax(linear_layer mapping 600 to 3972)
</code></pre>

<p>I have tried many different architectures and most have been just an overfitting failure. Although, training the model with this config, here is the loss vs epoch and accuracy vs epoch :
<a href=""https://i.sstatic.net/TgGVE.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/TgGVE.png"" alt=""enter image description here""></a></p>

<p><a href=""https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/"" rel=""nofollow noreferrer"">This article</a>, although says that if the training loss and validation loss are decreasing linearly then the model is underfitting. Still, there is some confusion as the same article says that it is underfitting coz of :-</p>

<p>1) The model complexity is not able to learn the data and the model should be increased in complexity(increase the layers/nodes per layer)</p>

<p>2) The training is stopped prematurely.</p>

<p>My concern is:- 
1) the loss graph is not linear exactly, so is it underfitting? if it is, then,</p>

<p>2) Should I continue training or should I change the architecture?</p>
","nlp"
"67569","Information Extraction from image / text - approach?","2020-02-05 09:53:00","","3","55","<machine-learning><python><neural-network><nlp><computer-vision>","<p>I need assistance with a ML project I am currently trying to create.</p>

<p>I receive a lot of invoices from a lot of different suppliers - all in their own unique layout. I need to extract <strong>3</strong> key elements from the invoices. These <strong>3</strong> elements are all located in a table/line items for all the invoices.</p>

<p>The <strong>3</strong> elements are: </p>

<ul>
<li><strong>1</strong>: Tariff number (digit)</li>
<li><strong>2</strong>: Quantity (always a digit)</li>
<li><strong>3</strong>: Total line amount (monetary value)</li>
</ul>

<p>Please refer to below screenshot, where I have marked these field on a sample invoice.</p>

<p><a href=""https://i.sstatic.net/HYmGZ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/HYmGZ.png"" alt=""enter image description here""></a></p>

<p>I started this project with a template approach, based on <em>regular expressions</em>. This, however, was not scaleable at all and I ended up with tons of different rules.</p>

<p>I am hoping that machine learning can help me here - or maybe a hybrid solution?</p>

<h1>The common denominator</h1>

<p>In <strong>all</strong> of my invoices, despite of the different layouts, each line item will <strong>always</strong> consist of one <strong>tariff number</strong>. This tariff number is always 8 digits, and is always formatted in one the ways like below:</p>

<ul>
<li>xxxxxxxx</li>
<li>xxxx.xxxx</li>
<li>xx.xx.xx.xx</li>
</ul>

<p>(Where ""x"" is a digit from 0 - 9).</p>

<p><strong>Further</strong>, as you can see on the invoice there is both a Unit Price and a Total Amount per line. The amount I will need is <strong>always</strong> the highest for each line.</p>

<h1>The output</h1>

<p>For each invoice like the one above, I need the output for each line. This could for example be something like this:</p>

<pre><code>{
    ""line"":""0"",
    ""tariff"":""85444290"",
    ""quantity"":""3"",
    ""amount"":""258.93""
},
{
    ""line"":""1"",
    ""tariff"":""85444290"",
    ""quantity"":""4"",
    ""amount"":""548.32""
},
{
    ""line"":""2"",
    ""tariff"":""76109090"",
    ""quantity"":""5"",
    ""amount"":""412.30""
}
</code></pre>

<h1>Where to go from here?</h1>

<p>I am not sure of what I am looking to do falls under machine learning and if so, under which category. Is it computer vision? NLP? Named Entity Recognition?</p>

<p>My initial thought was to:</p>

<ol>
<li>Convert the invoice to text. (The invoices are all in textable PDFs, so I can use something like <code>pdftotext</code> to get the exact textual values)</li>
<li>Create custom <strong>named entities</strong> for <code>quantity</code>, <code>tariff</code> and <code>amount</code></li>
<li>Export the found entities.</li>
</ol>

<p>However, I feel like I might be missing something. </p>

<p><strong>Can anyone assist me in the right direction?</strong></p>
","nlp"
"67534","Needed: Java library to calculate text readability/complexity","2020-02-04 15:28:09","","2","363","<nlp><text><java>","<p>In principle the same as <a href=""https://pypi.org/project/textstat/"" rel=""nofollow noreferrer"">this</a> but for Java (and ideally for multiple languages) (e.g. flesch reading ease, smog index, flesch kincaid grade, coleman liau index, automated readability index, dale chall readability score, linsear write formula, gunning fog etc).</p>

<p>I guess there must be plenty of libs but I just cant find them ...</p>
","nlp"
"67460","Building a search tool and classifying text using NLP and ML","2020-02-03 13:12:55","","2","281","<machine-learning><nlp>","<p>Im a newbie in information Retrieval. Currently Im reading a book entitled <a href=""https://books.google.co.in/books/about/Introduction_to_Information_Retrieval.html?id=GNvtngEACAAJ&amp;redir_esc=y"" rel=""nofollow noreferrer"">""An Introduction to Information Retreival"" by Christopher D. Manning and Prabhakar Raghavan</a>. Im trying to implement an ai based search tool to search for some relevant information from a private dataset. ( say a chemical or mathematical dataset that invloves more number of numericals and unstructed messy representation of unit of measure)</p>

<p>Following is a row in my dataset in JSON format where keys are the coloumn name and value are the values corresponding to it. </p>

<pre><code>  ""ABC Project"": {
    ""In/Out diameter"": "" Both in and out are 1”  "",

    ""Design Pressure (barG)"": {
      ""Max"": ""116 psiG (7.99 barG)"",
      ""Minimum"": ""79.7 psiG (5.49 barG)"",
      ""Design"": ""174 psiG (11.99 barG)""
    },

    ""C02 %"": ""0.671"",
    ""MW"": ""16.68 kg/kmol"",
    ""TITLE"": null,
    ""Mothiram"": ""There is  very dense forest and their lived a king. The name of the king was Pandidhurai. He was very brave "",
    ""Thooval delivery material"": ""- thooval delivery material is panam patta 316/316L  ""
</code></pre>

<p>The sample search query that will fetch the above mentioned row from my dataset would be  </p>

<p>"" <em>The project in which molecular wight is nearly 16 kg/kmol and Thooval delivery material panam patta 316l is used in which Mothiram is King Pandidhurai</em> ""</p>

<p><strong><em>What I have done</em></strong>.</p>

<p><em>Preprocessing</em> </p>

<p>Read each row from table(say <span class=""math-container"">$T$</span>) as <span class=""math-container"">$R_{i}$</span> and search query as <span class=""math-container"">$Q$</span> do  the following preprocessing </p>

<ol>
<li>convert_lower_case(data)</li>
<li>remove_punctuation(data)</li>
<li>remove_apostrophe(data)</li>
<li>remove_single_characters(data)</li>
<li>convert_numbers(data)</li>
<li>remove_stop_words(data)</li>
<li>stemming(data)</li>
<li>remove_punctuation(data)</li>
<li>Convert Word to Vector for coloumn <span class=""math-container"">$C_i$</span> and do it for each corresponding row in table <span class=""math-container"">$R_i$</span></li>
<li>Build Vector Space Model for coloumn <span class=""math-container"">$C_i$</span> and do it for each corresponding row in table <span class=""math-container"">$R_i$</span></li>
<li>Build a vector space model for query <span class=""math-container"">$Q$</span>.</li>
<li>Compute cosine score to select Row having high score</li>
</ol>

<p><strong>Challenges I'm facing</strong> </p>

<ol>
<li><p>When I do <em>'remove_punctuation(data)'</em> Im loosing vital informations (say  <strong><em><span class=""math-container"">$""$</span></em></strong> denotes inches in the diameter coloumn) </p></li>
<li><p>A way to interpret unit of measure in the data</p></li>
<li><p>Unable to keep the relation between value ( say <strong><em>16.78 kg/k mol</em></strong>  is stored in multi diamentions and unable to find it is related to MW). 
<em>I think it could be solved by building a ML classifier and train it to identify the entity MW. Say &lt; 16.78 kg/k mol , MW > and using the unit of measure as a feature</em>. But there are values like percentage composition of different chemical components, IN and OUT diameter , Inside and Outside Temperature. etc</p></li>
<li><p>Difficulty in interpreting scientific terms and mapping to one root form ( say <strong><em>MW, mol wt, molwt</em></strong> etc are all different ways of representing molecular weight)</p></li>
<li><p>Difficulty in finding the close proximity of neumerical values</p></li>
</ol>

<p><strong>Question</strong></p>

<blockquote>
  <ul>
  <li><strong>Please suggest me a step by step approach to build a search tool using this dataset (Please also suggest me an apt algorithm that would
  be useful in each step)</strong></li>
  </ul>
</blockquote>

<ul>
<li>I have read from <a href=""https://books.google.co.in/books/about/Introduction_to_Information_Retrieval.html?id=GNvtngEACAAJ&amp;redir_esc=y"" rel=""nofollow noreferrer"">here</a> that some documents can be tagged as &lt; document, class > and train a Navie Base Classifier Model for better search performance. Is it possible in this case , if so what could be class label you would be suggesting ? <strong>UPDATE</strong> : I think Navie Base cannot be used as the coloumn or key would  be greater than 1000. Im looking for a scalable approach to the problem.</li>
</ul>

<blockquote>
  <ul>
  <li>Is there any better approach to solve this problem than using a vector space model and  computing cosine similarity ?</li>
  </ul>
</blockquote>
","nlp"
"67440","How to understand the weights and biases for beginners?","2020-02-03 04:42:25","","0","744","<deep-learning><nlp><cnn><image-classification><convolution>","<p>I am newbie to deep learning, I was building my first model using MNIST dataset, I understood the full  model, but one thing is a bit confusing to me. How can we get the weights and bias? Is it that, weights are our input data or is it going to get assigned some random values and also bias or some of the weights.</p>

<p>It works based on this formula </p>

<pre><code>Z=Wh∗x+bh
</code></pre>
","nlp"
"67379","Why this calculation of weight vector in linear regression is only for small dataset?","2020-02-01 16:22:03","67415","0","222","<nlp><regression><gradient-descent>","<p>Slides from my university says, that the following way of calculating the weight vector is suitable only for small datasets. Can you please explain, why it may be suitable for small datasets?<a href=""https://i.sstatic.net/fmjMj.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/fmjMj.png"" alt=""""></a></p>

<p>Here, X is observed variable and Y is dependent variable{usually 0 or 1}</p>
","nlp"
"67326","Classifying one particular class of documents from the rest","2020-01-31 11:49:12","","3","57","<machine-learning><nlp><text><text-classification>","<p>I am trying to build a classifier that would classify if a document is a document about sports or not. I have enough samples of sports document to train a classifier on, however I can't imagine how I would sample 'not a sports document' category as there can be anything - book, news article, resume, invoice etc. How would approach this problem?</p>

<p>I already tried training One class SVM classifier with my sample of sport documents, however the accuracy turned out to be awful - around 6%.</p>

<p>I also read about PU learning, do you think this is the way to go? Are there any other options?</p>

<p>Thank you.</p>
","nlp"
"67274","Word2Vec and Tf-idf how to combine them","2020-01-30 13:28:41","67275","5","7181","<nlp><text-mining><feature-engineering><word2vec><tfidf>","<p>I'm currently working in text mining ptoject I'd like to know once I'm on vectorisation. With method is better. </p>

<ul>
<li><p>Is it <code>Word2Vec</code> or <code>Tf-Idf</code> ?  </p></li>
<li><p><a href=""https://datascience.stackexchange.com/questions/28598/word2vec-embeddings-with-tf-idf"">Here</a> I see we can combine them why that? Does it improve quality of data?  </p></li>
<li><p>What about <code>GloVe</code>?</p></li>
</ul>

<p>Thanks</p>
","nlp"
"67245","Visualizing F-score differences in information extraction","2020-01-29 23:54:12","","4","138","<nlp><information-retrieval><ensemble><f1score>","<p>I have several corpora and NLP systems (including a few merge ensembles of output of these systems combined in unions and intersections) with which I have extracted the annotation span sets {(begin, end)} for each corpus across all documents within the corpus and compared the span sets to each corpus's respective gold standard and thus obtained standard measures of F-score, precision and recall.</p>

<p>I am trying to qualitatively assess why certain systems don't perform as well as a particular ensemble combination on F-score, so I figured the easiest way would be to generate precision-recall or ROC curves. </p>

<p>The task is just a simple binary classification: either a span of text is annotated (labeled as 1) or it is not (labeled as 0). </p>

<p>I have numpy vectors of the same length for each document in the corpus for both the system predictions and the gold standard, so I plan on using these for <code>y_true</code> and <code>y_predict</code> when trying to generate my ROC curve. </p>

<p>Is this a good approach to observe the behavior of my F-scores, assuming I plot them all on the same graph? If not, any recommendations for a better approach would be most appreciated.</p>
","nlp"
"67227","GMM in speech recoginition using HMM-GMM","2020-01-29 15:24:09","","2","182","<nlp><gaussian><markov-hidden-model><speech-to-text>","<p>I am trying to solve/understand ASR using HMM-GMM.</p>

<p>At the abstract level i do understand what's happening but I did not 
understand how GMM fits into it<a href=""https://i.sstatic.net/u6zP5.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/u6zP5.png"" alt=""ASR model""></a>.</p>

<p>My data has 5K hours of speech from single user. I took the above picture from <a href=""https://heartbeat.fritz.ai/the-3-deep-learning-frameworks-for-end-to-end-speech-recognition-that-power-your-devices-37b891ddc380"" rel=""nofollow noreferrer"">this</a> article.</p>

<p>I do know what is GMM but i am unable to wrap my head around it. 
Can somebody explain with a simple example.</p>
","nlp"
"67215","Using TF-IDF for feature extraction in Sentiment Analysis","2020-01-29 13:36:29","","2","1109","<nlp><sentiment-analysis><tfidf>","<p>I am working on sentiment analysis for twitter data, for which I have used Vader to get an approximation of sentiment for a tweet. Along with, I have used TF-IDF for feature extraction. These feature words I am using to train and test a Random Forest model. In my dataset, there are around 3K plus tweets from which I extracted around 570 unique feature words using TF-IDF. And all these features I have used for training the Random Forest Model. </p>

<p>My query is in regards to the usage of this trained model in the real world. What if the new tweets which this model has never seen do not have feature words that I have used for training, will the model fail to make correct predictions for them (in my case there are only 3 possible predictions viz. positive, negative and neutral) correctly for them? If yes, then how should I handle this scenario?</p>

<p>Please let me know in case I am missing something or doing something wrong here.</p>
","nlp"
"67202","Cyber crime - data set","2020-01-29 11:11:33","","1","32","<nlp><data-mining><web-scraping>","<p>I'm doing science project on my university to make an app, which uses AI to detect cyber crimes. I'm looking for sites to make my data set. Do you know any with ads like tobacco, alcohol, prescriptions, drugs, forgery? I'm using LASER embedder, so the site can be in almost any language. I'm looking for something like <em>OLX</em>, which allows illegal content. Dark web is fine as well.</p>

<p>I'm aware this question can somehow violate <em>stackexchange</em> terms of use. Please don't post particular site with an add, just a domain. I'll find posts myself. Something like <em>avito.ru</em> will do.</p>
","nlp"
"67189","Unable to save the TF-IDF vectorizer","2020-01-29 08:58:11","67197","1","2691","<machine-learning><python><nlp><multilabel-classification><tfidf>","<p>I'm workig on multi-label classification problem. I'm facing issue while saving the TF-IDf verctorizer and as well as model using both pickle and joblib packages. </p>

<p>Below is the code:</p>

<pre><code>vectorizer = TfidfVectorizer(min_df=0.00009, max_features=200000, smooth_idf=True, norm=""l2"", \
                         tokenizer = lambda x: x.split(), sublinear_tf=False, ngram_range=(1,3))
x_train_multilabel = vectorizer.fit_transform(x_train)
x_test_multilabel = vectorizer.transform(x_test)

classifier = OneVsRestClassifier(SGDClassifier(loss='log', alpha=0.00001, penalty='l1'), n_jobs=-1)
classifier.fit(x_train_multilabel, y_train)
predictions = classifier.predict(x_test_multilabel)
</code></pre>

<p>Error Message while saving the TF-IDF vectozier. </p>

<p><a href=""https://i.sstatic.net/co2il.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/co2il.png"" alt=""enter image description here""></a>
<a href=""https://i.sstatic.net/SakVx.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/SakVx.png"" alt=""enter image description here""></a></p>

<p>Any suggestions ?
Thanks in advance.</p>
","nlp"
"67179","NLP - Paraphrase extraction in Python","2020-01-29 05:14:24","","4","427","<python><nlp><text-mining><text-generation><text-filter>","<p>I am trying to develop a NLP model, which takes something like <code>you have high levels of cholesterol</code>(this will be a tag) as input and has to output something like <code>you have high levels of cholesterol, you need to have a low-salt diet that emphasizes fruits, vegetables and whole grains; limit the amount of animal fats and use good fats in moderation</code>(this will be the suggestion; and it is an example suggestion from doctor).</p>
<p>So, now when I was researching on how this could be accomplished I stumbled on <a href=""https://www.aclweb.org/anthology/W10-0205.pdf"" rel=""nofollow noreferrer"">this research paper</a> and I learned about something called <code>paraphrase extraction</code> from it and also that I need to build a parallel corpus with tag and suggestion.</p>
<p>I want to do this in Python. I couldn't find much information from internet on how paraphrase extraction can be done in Python, but there are many articles talking about paraphrase detection and things.<br />
So, do we have any libraries in Python for this purpose (any kind of help is appreciated)?<br />
How do I build a corpus for this same purpose?</p>
","nlp"
"67143","validation accuracy and loss increase","2020-01-28 10:20:22","67144","0","694","<deep-learning><nlp><lstm>","<p>I am training a generic LSTM based autoencoder to get the sentence embeddings, the bleu score is the accuracy metric. The model is coded to output the same number of tokens as the length of labels, hence the losses are calculated using cross-entropy loss between the output of token and the corresponding label token and added to a total loss to be backpropagated The embeddings size is 1000 throughout.</p>

<p>Here are the logs:-</p>

<pre><code>Training Epoch: 1/20, Training Loss: 78.32446559076034, Training Accuracy: 0.23442341868755373
Validation Epoch: 1/20, Validation Loss: 75.11487170562003, Validation Accuracy: 0.28851715943634565

Training Epoch: 2/20, Training Loss: 60.702940691499734, Training Accuracy: 0.3043263558919579
Validation Epoch: 2/20, Validation Loss: 68.58432596068359, Validation Accuracy: 0.337459858582381

Training Epoch: 3/20, Training Loss: 51.62519727157313, Training Accuracy: 0.35618672202599283
Validation Epoch: 3/20, Validation Loss: 64.17064862141332, Validation Accuracy: 0.37158793060235135

Training Epoch: 4/20, Training Loss: 44.40417488866389, Training Accuracy: 0.4094415453046547
Validation Epoch: 4/20, Validation Loss: 61.230048799977716, Validation Accuracy: 0.3955376494828317

Training Epoch: 5/20, Training Loss: 38.78325418571326, Training Accuracy: 0.46050421873328257
Validation Epoch: 5/20, Validation Loss: 59.78918521062842, Validation Accuracy: 0.4063787247291398

Training Epoch: 6/20, Training Loss: 33.65953556655257, Training Accuracy: 0.5193937894102788
Validation Epoch: 6/20, Validation Loss: 58.64455007580877, Validation Accuracy: 0.41980867690343776

Training Epoch: 7/20, Training Loss: 29.35849161540994, Training Accuracy: 0.5831378755700898
Validation Epoch: 7/20, Validation Loss: 58.26881152131025, Validation Accuracy: 0.4261582422867802

Training Epoch: 8/20, Training Loss: 25.244888168760856, Training Accuracy: 0.6488748581642462
Validation Epoch: 8/20, Validation Loss: 57.62903963564669, Validation Accuracy: 0.43286079887479756

Training Epoch: 9/20, Training Loss: 22.05663261861035, Training Accuracy: 0.7039174093261202
Validation Epoch: 9/20, Validation Loss: 58.09752491926684, Validation Accuracy: 0.4399501875046306

Training Epoch: 10/20, Training Loss: 19.248559526880282, Training Accuracy: 0.7486352249548112
Validation Epoch: 10/20, Validation Loss: 58.613073462421454, Validation Accuracy: 0.4470900014647744

Training Epoch: 11/20, Training Loss: 16.95602631587501, Training Accuracy: 0.7857343322245365
Validation Epoch: 11/20, Validation Loss: 58.38435334806304, Validation Accuracy: 0.44778823347334884

Training Epoch: 12/20, Training Loss: 14.74661236426599, Training Accuracy: 0.8136944976817879
Validation Epoch: 12/20, Validation Loss: 59.63633590068632, Validation Accuracy: 0.45206057264928495

Training Epoch: 13/20, Training Loss: 13.507415059699248, Training Accuracy: 0.8299945959036563
Validation Epoch: 13/20, Validation Loss: 60.149887264208886, Validation Accuracy: 0.4512303133278385

Training Epoch: 14/20, Training Loss: 12.026118357521792, Training Accuracy: 0.8491757446561087
Validation Epoch: 14/20, Validation Loss: 59.89944394497038, Validation Accuracy: 0.45497359431776685

Training Epoch: 15/20, Training Loss: 10.785567499923806, Training Accuracy: 0.8628473173326144
Validation Epoch: 15/20, Validation Loss: 61.482036528946125, Validation Accuracy: 0.45541000266481596

Training Epoch: 16/20, Training Loss: 9.373574649788727, Training Accuracy: 0.8767987081840235
Validation Epoch: 16/20, Validation Loss: 62.18386231796834, Validation Accuracy: 0.4580630794998584

Training Epoch: 17/20, Training Loss: 8.5658748998932, Training Accuracy: 0.8878869616990712
Validation Epoch: 17/20, Validation Loss: 63.56435154233743, Validation Accuracy: 0.4606744393166781

Training Epoch: 18/20, Training Loss: 7.807730126944895, Training Accuracy: 0.8960175152587504
Validation Epoch: 18/20, Validation Loss: 63.88373188037895, Validation Accuracy: 0.4606897915210869

Training Epoch: 19/20, Training Loss: 6.829077819740428, Training Accuracy: 0.9038927070366026
Validation Epoch: 19/20, Validation Loss: 65.59262917371629, Validation Accuracy: 0.4639800374912485

Training Epoch: 20/20, Training Loss: 6.152266260986982, Training Accuracy: 0.9090036335609419
Validation Epoch: 20/20, Validation Loss: 66.84154795008956, Validation Accuracy: 0.4672414105594907
</code></pre>

<p>Here is are the accuracy and loss vs epoch graphs :
<a href=""https://i.sstatic.net/nmqI2.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/nmqI2.png"" alt=""enter image description here""></a></p>

<p>I want to know why it is that the validation loss and accuracy is increasing.</p>
","nlp"
"67113","What are some best Text Representation techniques in NLP","2020-01-27 13:04:03","67135","1","1146","<nlp>","<p>I've studied about various text representation techniques like : <code>Bag of Words</code>, <code>N-gram data modelling</code>, <code>Tf-idf</code>, <code>word embedding</code> etc.</p>

<p>I would like to know which among all the techniques are most efficient when it comes to data modelling or representation for a <code>supervised text classification</code> across a large number of categories.</p>

<p>I might have around 40 categories and then around a same number of sub-categories upto 4 levels.</p>
","nlp"
"66972","Document similarity","2020-01-24 10:14:48","","2","358","<machine-learning><nlp><unsupervised-learning><supervised-learning><similar-documents>","<p>I have close to 50000 documents in plain text format. </p>

<p>Is there a way in which I can group similar documents together? Similarity mostly here is the content similarity. </p>

<p>Will transforming the text into a vector (using TFIDF) and running a K-Means (unsupervised learning) algorithm on top of that help? Are there any better approaches that could be used?</p>
","nlp"
"66954","NLP problem : Choosing the optimal parser for extracting quantitative values from text","2020-01-23 22:38:30","","3","98","<machine-learning><nlp><r>","<p>I have a clinical NLP problem for which I would need some help to establish a proper framework.</p>

<p>I am trying to extract different elements from echocardiography reports. Those elements are both quantitative and qualitative.</p>

<p>For example, <strong>quantitative</strong> elements are in the form : </p>

<blockquote>
  <p>""LVEF : 40%"", ""LVIM : 2 mm/s"" , ""the tricuspid regurgitation is 4mm/s""</p>
</blockquote>

<p>and <strong>qualitative</strong> elements are in the form :</p>

<blockquote>
  <p>""The ventricular function is depressed"", ""We noticed reduced diastolic function""</p>
</blockquote>

<hr>

<p>Ultimately my objective is to obtain the following table for one note :</p>

<pre><code>ID LVEF LVIM TR Diastolic Dysfunction
1  40   2    4  Reduced
</code></pre>

<hr>

<p>My current framework to do so is to use the Quanteda package and Spacyr.</p>

<ol>
<li><p>Tokenization of the text</p>

<blockquote>
  <p>This is pretty straight forward. </p>
</blockquote></li>
<li><p>Parsing</p>

<blockquote>
  <p>This is where I am a little bit hesitant.
  I believe that it is key for this project to capture the sequence of the word as retrieving what number is associated with which parameter would be difficult otherwise (i.e. for example in a bag-of-word scenario, it would be impossible to know if 4 refers to the value of LVEF or LVIM for example).</p>
  
  <p>I do not know what would be the best way to parse the text to retrieve that information.</p>
</blockquote></li>
<li><p>Dictionary use </p>

<blockquote>
  <p>A lot of the echocardiogram parameters are registered in different ways. For example, 'LVEF' is documented as 'Left Ventricular Function', 'Heart Function', 'Left Function'.
  I constructed a dictionary mapping all the variations possible for a given concept.</p>
  
  <p>At what point of the NLP pipeline should the dictionary be used and how does Quanteda work with custom dictionaries ?</p>
</blockquote></li>
</ol>

<p>Thank you all! </p>
","nlp"
"66913","How does attention mechanism learn?","2020-01-23 06:05:27","67188","16","4209","<neural-network><deep-learning><nlp><attention-mechanism>","<p>I know how to build an attention in neural networks. But I don’t understand how attention layers learn the weights that pay attention to some specific embedding.</p>

<p>I have this question because I’m tackling a NLP task using attention layer. I believe it should be very easy to learn (the most important part is to learn alignments). However, my neural networks only achieve 50% test set accuracy. And the attention matrix is weird.
I don’t know how to improve my networks.</p>

<p>To give a example:<br>
English: Who are you?<br>
Chinese: 你是誰？  </p>

<p>The alignments are<br>
‘Who’ to ‘誰’<br>
‘are’ to ‘是’<br>
‘you’ to ‘你’  </p>

<p>How does attention learn that?</p>

<p>Thank you!</p>
","nlp"
"66866","What is the main concept of using lexical,linguistic, semantic or syntactic approach in NLP for cyberbullying","2020-01-22 10:17:04","","0","127","<machine-learning><nlp><data-mining><dataset><training>","<p>I am really in need of some explanation, I am working on an NLP cyber-bullying detection tool which I will deploy to the web using Django framework, however, am stuck on some idea, can someone explain to me...What is the main concept of using lexical, linguistic, semantic or syntactic approach in (NLP) and how is applied in cyber-bullying or what are the step or why does people say I built this cyber-bullying system from a linguistic approach, I know POS-tagging is way of grouping word and look for dependency in other word, my idea of POS-tagging is a synonym of semantic because POS-tagging is a process linking words to it root and representation word in an understandable context correct me if wrong.</p>

<p>I read an article where a paper tackled a project using a predictive analysis approach with feature extraction techniques, Naive Bayes for classification and to train the model, in the discussion they also spoke on how other team used the Semantics approach to classifier cyber-bullying. I know of data cleaning, tokenization stemming and most of feature extraction model, however, am stuck on the problem of approach which is what is the relevant on lexical, semantic or syntactic and how are they been approached.</p>
","nlp"
"66848","Building own embedding from sequence","2020-01-21 22:39:47","","2","154","<deep-learning><nlp><word-embeddings><embeddings>","<p>I have 100 sequences of the word (i.e., action for completing a task). Each of the sequences contains around 350 actions(115 unique actions but all the actions are not used in each sequence. Some of the actions may repeat). The dataset looks like as below:</p>

<pre><code>Datapoint 1    Datapoint 2 .............  Datapoint 100

Add wall        Add wall                   Add window
Edit wall       Remove Roof                 Add wall
Add wall        Add window                  Edit wall
.......         .........                   .........
........        .........                   .........
Remove door     Add door                    Remove door
</code></pre>

<p>My target is to predict the next design actions. However, when I used these sequences in the LSTM model, the prediction accuracy is not so high (35%). For this reason, I am thinking if I can use any <strong>embedding model</strong>. It is mention-worthy that actions in the sequences are correlated. It means each action has a certain relation with its previous actions and later actions. How can I represent these relationships using embedding? In short, I want to build my own embedding based on the sequence. If anyone help me to provide some reference, paper, it would be highly appreciated.</p>
","nlp"
"66846","Find multiple categories of promises in texts","2020-01-21 19:59:41","","2","45","<nlp>","<p>I have conversations a customer with an agent (without punctuation). There are phrases of several categories of promises that an agent gave to a customer (call back, make an appointment, etc.). It has been done manually. Altogether 12 categories. Now I'm thinking of creating an algorithm for this. I am thinking to do this task in two steps. </p>

<ol>
<li>In the first step, I need to create an algorithm that can find an
end and a beginning of all promises. This algorithm has to insert a
start tag and an end tag.</li>
<li>The second step is to create a classifier that would label a promise
to the necessary categories.</li>
</ol>

<p>As I understand, the second step is well known and this is called text classification. But for the first step, I could not find any articles and github repositories. But I think it is an important NLP task and there must be information on this. Maybe are there approaches that solve two steps at the same time?</p>

<p><strong>Update</strong></p>

<p>Just sample an agent's transcript (in reality it is more difficult):</p>

<pre><code>hi my name is ben how can i help you yes good what about i can help probably yes sir do you have a problem with internet connection i see let do you need a help at place okay i see so what i can do i can arrange appointment with technical will it be good for you great can i help you with something else you okey okey to have a great day you too
</code></pre>

<p>Promise here is </p>

<pre><code>i can arrange appointment with technical
</code></pre>
","nlp"
"66790","Tools/tutorials for compiling corpora for NLP experiments?","2020-01-21 02:00:31","","2","22","<nlp><tools><pipelines><corpus>","<p>I have a couple of NLP ideas I want to try out (mostly for my own learning) - while I have the python/tensorflow background for running the actual training and prediction tasks, I don't have much experience in processing large amounts of text data and whatever pipelines are involved.</p>

<p>Are there any tutorials on how to gather data and label it for a larg(ish) NLP experiment?</p>

<p>For example: BERT was originally trained on all of English Wikipedia. How do you go about gathering all of the text from Wikipedia's 5.9 Million + articles in a repository in the right format? How do you go about tokenizing such a large corpus Do things like NLTK and Beautiful soup still work on such large data sets?</p>

<p>If I have website or multiple website on a topic specific topic that I want to come up with some NLP models for, are there any Webscraping APIs that can pull all of that into one place?
Any tutorials, tools would be very welcome,
Thanks</p>
","nlp"
"66786","What is a 'hidden state' in BERT output?","2020-01-20 22:00:44","66788","3","14117","<nlp><rnn><bert>","<p>I'm trying to understand the workings and output of BERT, and I'm wondering how/why each layer of BERT has a 'hidden state'.</p>

<p>I understand what RNN's have a 'hidden state' that gets passed to each time step, which is a representation of previous inputs. But I've read that BERT isn't a RNN - it's a CNN with attention.</p>

<p>But you can output the hidden state for each layer of a BERT model.  How is it that BERT has hidden states if it's not a RNN?</p>
","nlp"
"66720","Understanding cosine distance with word vectors","2020-01-19 19:12:05","66807","1","314","<nlp><cosine-distance>","<p>I'm a new DL4J user, and I'm running all the works of Shakespeare through a Word2Vec neural net. I've got a pretty basic question about how to understand the results so far. In the below example, there's an obvious association with the ""ing"" in king and the ""ing"" in other words that probably don't have much to do with king. Am I missing something about how a word2vec formula uses the characters inside the words it is mapping? Or is my net just really untrained?</p>

<p>Also, what does the cosine distance between those example words say to you about the results, if anything? Thank you for your advice!</p>

<pre class=""lang-java prettyprint-override""><code>   List&lt;String&gt; abc = vec.similarWordsInVocabTo(""king"", 0.8); //80% similar
   System.out.println(abc);

   String[] words = {""woman"", ""king"", ""boy"", ""child"", ""human""};
   for (String word : words) {
       System.out.println(vec.similarity(""man"", word));
   }
</code></pre>

<p><strong>Output - Similar words to king:</strong></p>

<pre><code>[taking, drinking, kingly, picking, waking, singing, wringing, knight, feigning, beginning, ink, thinking, kin, knocking, making, bringing, knowing, lingring, winking, neighing, king-, kings, asking, stinking, king, liking]
</code></pre>

<p><strong>Output - Vector similarity between ""man"" and woman, king, boy, child, human:</strong></p>

<pre><code>woman:   0.8305895924568176
king:    0.00203840178437531
boy:     0.2974374294281006
child:   0.4752597510814667
human:  -0.10414568334817886
</code></pre>
","nlp"
"66706","Consider ratings as sentiment labels?","2020-01-19 14:38:34","66728","1","93","<nlp><naive-bayes-classifier>","<p><strong>Beginner here!</strong></p>

<p>I have a dataset, with reviews of a product as text, ratings for the product.</p>

<p>My previous motive was to use Naive Bayes classifier for sentiment analysis. But my data doesn't have the variables( sentiment) required - negative/positive. </p>

<ol>
<li>Shall I use the ratings (1-5) and encode it as positive and negative?</li>
<li>Or using the lexicon-based methods is more valid?</li>
</ol>
","nlp"
"66705","How to choose the best parameter values for TfidfVectorizer in sklearn library?","2020-01-19 14:29:54","66733","0","8511","<scikit-learn><nlp><tfidf>","<p>Recently, I used  <code>TfidfVectorizer</code> in scikit-learn library to calculate a matrix of TF-IDF features. However, I do not know how to set some parameters such as <code>max_features</code>, <code>min_df</code>, <code>max_df</code>, etc.</p>
","nlp"
"66685","How to classify named entities of the same type?","2020-01-18 23:56:27","102997","2","83","<machine-learning><nlp><named-entity-recognition>","<p>I am doing a project where I am extracting date/time entities from text. I'm using a rule-based system to extract the temporal expressions and ground them to an actual date/time.</p>

<p>The second part of the problem I hope to solve is label the role of each entity discovered. For example, consider the following text: ""Leaving at 2pm and back at 4pm"". I correctly identified 2pm and 4pm as date/time entities. However, I'm unable to say whether the entity is ""start-time"", ""end-time"", or neither.</p>

<p>The question is how do I do this?</p>

<p>I'm new to NLP and ML. Here is an idea I have please tell me if I'm going the right direction:</p>

<p>The plan is to train a logistic regression (or naive bayes?) classifier  using the following features:</p>

<ol>
<li>The average of the word embedding for each word within a window of the date/time phrase.</li>
<li>The POS tags for each word within a window of the date/time phrase??(Not sure how to pass this in to a logistic regression classifier but just a thought)</li>
<li>The word shapes of the words in the temporal expression??</li>
</ol>

<p>I'm a little confused as to where to start and would really appreciate some pointers on how to select my features and what classifier would be appropriate.</p>

<p>I'm also open to suggestions on learning resources. There's a lot of NER resources online but not many on how to ""role classify"" found entities.</p>
","nlp"
"66638","Pretrained handwritten OCR model","2020-01-17 12:23:42","66885","5","9836","<machine-learning><deep-learning><nlp><ocr>","<p>I've been looking around for pretrained models dedicated to handwritten OCR. So far I've found very little. Could you please share, if you know any?
I find <code>tesseract</code> hard to parse anything that isn't <em>arial</em> and perfectly captured.</p>
","nlp"
"66629","How to validate regex based Resume parser efficiently","2020-01-17 10:11:00","66689","2","335","<machine-learning><python><nlp><data-science-model><regex>","<p>I am using rule based logic to extract features from resume. Basically I am trying to find if the candidate switched the company in less than 1 year. So I have the code in place to find it using python. However if I want to validate it, I am currently doing it manually for few resumes, meaning I open the resume and find if the candidate switched the company in less than 1 year and compare it with what my regex logic gives. But this is time consuming process to validate.</p>

<p>Is there any better way to validate this piece of python code. ?</p>
","nlp"
"66620","Feature extraction from resume using Python without rule based logic","2020-01-17 08:18:18","","2","622","<machine-learning><python><nlp><data-science-model>","<p>I am working on a resume parser project. Currently, I am using rule-based regex to extract features like University, Experience, Large Companies, etc.</p>

<p>So basically I have a set of universities' names in a CSV, and if the resume contains one of them then I am extracting that as University Name.  In the same way I have a list of Large Companies in CSV and if the resume contains any of them then I flag it as Yes.</p>

<p>So these are rule-based logic and can never be fool-proof considering different countries have different resume formats.
Is there any other way of doing it to improve the accuracy and make it a global solution?</p>
","nlp"
"66471","(pre-trained) python package for semantic word similarity","2020-01-14 15:49:02","66478","2","2347","<python><nlp><word-embeddings>","<p>I am searching for a python package that calculates the semantic similarity between words. I do not want to train a model (what most packages seem to offer) - the package should have been pre-trained on ideally thousands of natural language books and documents (e.g. on how often do words occur in close proximity to each other in the training material) and be simple to install/use. As for example in pseudo code below:</p>

<pre><code>import XYZ

assessor = XYZ.loadPreTrainedModel(""standard_text"")
assessor.scoreWords(""pilot"", ""airplane"")  # returns 0.94 (I made up these numbers)
assessor.scoreWords(""student"", ""university"")  # returns 0.91
assessor.scoreWords(""cat"", ""dog"")  # returns 0.82 
assessor.scoreWords(""cat"", ""airplane"")  # returns 0.13
assessor.scoreWords(""student"", ""apple"")  # returns 0.25
...
</code></pre>
","nlp"
"66398","Question answering (QA) vs Chatbots","2020-01-13 10:40:18","","6","3894","<nlp><text-mining><chatbot><question-answering>","<p>Are <strong>Question answering (QA)</strong> the same as <strong>Chatbots</strong>? I can not understand the difference between them. </p>

<p>For me it's the same thing: interact with a robot that answers questions.</p>
","nlp"
"66394","How does BERT and GPT-2 encoding deal with token such as <|startoftext|>, <s>","2020-01-13 10:07:32","66399","3","5437","<nlp><pytorch><bert><gpt>","<p>As I understand, GPT-2 and BERT are using Byte-Pair Encoding which is a subword encoding. Since lots of start/end token is used such as &lt;|startoftext|> and , as I image the encoder should encode the token as one single piece.</p>

<p>However, when I use pytorch <code>BertTokenizer</code> it seems the encoder also separate token into pieces. Is this correct behaviour?</p>

<pre><code>from pytorch_pretrained_bert import BertTokenizer, cached_path
tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False) 
tokenizer.tokenize('&lt;s&gt; This is a sentence &lt;|endoftext|&gt;')
</code></pre>

<p>The results are:</p>

<pre><code>['&lt;',
 's',
 '&gt;',
 'This',
 'is',
 'a',
 'sentence',
 '&lt;',
 '|',
 'end',
 '##oft',
 '##ex',
 '##t',
 '|',
 '&gt;']
</code></pre>
","nlp"
"66331","find bigrams in pandas","2020-01-11 18:12:17","","3","1392","<python><nlp><pandas><nltk>","<p>I have a DataFrame with 4 columns: 'Headline', 'Body_ID', 'Stance', 'articleBody', with 'Headline' and 'articleBody containing cleaned and tokenized words.  I want to find bi-grams using nltk and have this so far:</p>

<pre><code>bigram_measures = nltk.collocations.BigramAssocMeasures()
articleBody_biGram_finder = df_2['articleBody'].apply(lambda x: BigramCollocationFinder.from_words(x))
</code></pre>

<p>I'm having trouble with the last step of applying the <code>articleBody_biGram_finder</code> with <code>bigram_measures</code>. I've tried multiple iterations of lambda with list comprehension but am getting nowhere.   </p>

<p>my most recent attempts:</p>

<p><code>df_2['articleBody_scored'] = score_ngrams(bigram_measures.raw_freq) for item in articleBody_biGram_finder</code></p>

<p><code>df_2['articleBody_scored'] = articleBody_biGram_finder.apply(lambda x: BigramCollocationFinder.score_ngrams(bigram_measures.raw_freq))</code></p>
","nlp"
"66305","Extracting name, date and total from a set of heterogeneous receipts","2020-01-11 08:13:47","66314","2","632","<machine-learning><neural-network><deep-learning><nlp><named-entity-recognition>","<p>So, this is how the problem goes: I am trying to extract information from scanned receipts like this,</p>

<p><a href=""https://i.sstatic.net/mjAuJ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/mjAuJ.png"" alt=""Receipt from New York, USA""></a></p>

<p>What I have been told is that I would get the textual data from a OCR software, so in short I will be working with a textual version of the image directly.</p>

<p><strong>Problem:</strong></p>

<p>The problem in hand here is that I have to extract certain information here, namely,</p>

<ol>
<li>The <code>Location</code>. (for eg: New York, United States)</li>
<li>The complete <code>Total</code> (after all the discounts, tips, etc. have been involved) (Eg: 1033.42)</li>
<li>The <code>Currency</code>. ($, £, €, etc)</li>
<li>The <code>Date</code>. (Easier to guess) </li>
</ol>

<p>The reason I want to extract the Location information is that, if for example, the currency is not explicitly mentioned here, then I can infer from the location where the receipt is generated.</p>

<p><strong>Challenges:</strong></p>

<p>The challenge here is that, information like <code>Total Due</code> could be anything like <code>Grand Total</code> or <code>Total</code> (only), or something semantically similar to <code>Total</code>, as I am not going to get the same receipt from the same restaurant only. (The restaurant could be anywhere in the world, but the problem is limited to only English speaking countries for now.) </p>

<p>The other challenge is to actually get the total information. It's very easy for us to see that 1033.42 is the total in the above receipt. But how do I get the software to know that? The way I see it, is that 1033.42 is near to the total(proximity). But there could be other numbers near to it as well. </p>

<p><strong>Where I have tried and failed:</strong></p>

<p>I have been told to start from NLTK(NER) but NER doesn't work for everything here. I can get the date information through it but the problem isn't just only to identify what the named entities are, imo. </p>

<p><strong>What I think would work</strong></p>

<p>The way I see it, I think I need to use a machine/deep learning model where the machine would be able to understand the proximity match between anything which semantically says <code>Total</code> and the number near to it (most probably on the right side).  </p>

<p>Any help regarding which model would work best in terms of speed(first and foremost) and accuracy would be greatly appreciated.<br>
I would also appreciate any help regarding where I could find any dataset or existing model which I could use for <code>Transfer Learning</code>. </p>
","nlp"
"66260","Meaningful Information retrieval and question answering for unstructured data - Is it even possible?","2020-01-10 10:11:42","","1","237","<nlp><information-retrieval><named-entity-recognition><question-answering>","<p>Hello good NLP people,</p>

<p>I am working on a task that gradually seems not solvable for me. My data-set consists of long, messy, unstructured documents (pdfs, doc, docx, scans with tables, graphs, text, etc)  and the client wants to obtain a system that allows to query basically as much information as possible from the documents. </p>

<p>An example query could be: “Who are the beneficiaries of project xxx that is implemented by ORG xxx?”. 
Or: “How much co-financing was directed to projects that concern focal areas x,y,z?”</p>

<p>My initial idea was following:</p>

<p><strong>1)</strong> Process documents (Python, OCR, etc) into machine readable form.</p>

<p><strong>2)</strong> Pre-process/clean/normalise text.</p>

<p><strong>3</strong>) Manually annotate entities and build customised NER model.</p>

<p><strong>4)</strong> Manually annotate entity relations and build Relation extraction model (I am not sure how to do this the best way?)</p>

<p><strong>5)</strong> Extract triples.</p>

<p><strong>6)</strong> Store triples in knowledge graph/base.</p>

<p><strong>7)</strong> Query (however the query will then be limited by the extracted entities and relations). </p>

<p>The end result could look like this: 
<a href=""http://semanticparsing.ukp.informatik.tu-darmstadt.de:5000/question-answering/static/index.html"" rel=""nofollow noreferrer"">http://semanticparsing.ukp.informatik.tu-darmstadt.de:5000/question-answering/static/index.html</a></p>

<p>However, I have the feeling that QA systems and Relation Extraction frameworks can be built on top of large quality datasets (like Wikipedia - or even better KB like Wikidata). In reality, dealing for example with messy documents, it seems very difficult to replicate. </p>

<p>Please let me know about any important work on this lately, or how you would proceed.
Thanks!</p>
","nlp"
"66256","Need some info regarding string matching algorithms?","2020-01-10 09:33:47","","1","146","<python><nlp><algorithms><text><search>","<p>Let me explain a scenario to better explain my question,</p>

<p>Assume I am working in a credit-card related company in which people uploads their receipts every month, I want to check if that person bought fruits or not. Let's assume we used OCR to extract only the names of items bought and stored in a list.</p>

<p>First thing I did is web scrape all the names of fruits found everywhere and I stored the names of each of them in a big text file.</p>

<p>Now I want to know how can I match/lookup and make a decision that person bought fruits.</p>

<p>1) Any search/match algorithms that works on huge amounts of data.</p>

<p>I'm just looking for advice on what to implement in this type of scenario. Thanks in advance.</p>
","nlp"
"66213","Removing junk sentences","2020-01-09 18:09:27","","4","1212","<nlp>","<p>I have transcripts of phone calls with customers and agents. I'm trying to find promises which were made by an agent to a customer. </p>

<p>I already did punctuation restoration. But there are a lot of sentences that don't have any sense. I would like to remove them from the transcript. Most of them are just a set of not connected words. 
I wonder what approach is the best for this task?</p>

<p>My ideas are:</p>

<p>• Use tf idf and word2vec to create vectors from all sentences. After that we can do some kind of anomaly detection e.g. look for and delete vectors that are highly deviated from most other vectors.</p>

<p>• Spam filters. Maybe is it possible to apply spam filters for this task?</p>

<p>• Crate some pattern of part of speech tags that proper sentence must include. For example, any good sentence must include noun + verb. Or we can use for example dependency tokens from spacy.</p>

<p><strong>update</strong></p>

<p>Example of a sentence that I want to keep:</p>

<blockquote>
  <p>There's no charge once sent that you'll get a ups tracking number.</p>
</blockquote>

<p>Example of a junk sentence:</p>

<blockquote>
  <p>Kinder pr just have to type it in again, clock drives bethel.</p>
</blockquote>

<p>Another junk sentence:</p>

<blockquote>
  <p>Just so you have it on and said this is regarding that.</p>
</blockquote>
","nlp"
"66207","What is purpose of the [CLS] token and why is its encoding output important?","2020-01-09 17:20:10","66209","72","100277","<nlp><sentiment-analysis><bert><language-model><text-classification>","<p>I am reading <a href=""http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/"" rel=""noreferrer"">this article on how to use BERT</a> by Jay Alammar and I understand things up until:</p>

<blockquote>
  <p>For sentence classification, we’re only only interested in BERT’s output for the [CLS] token, so we select that slice of the cube and discard everything else.</p>
</blockquote>

<p>I have read <a href=""https://datascience.stackexchange.com/questions/46312/what-is-the-vector-value-of-cls-sep-tokens-in-bert"">this topic</a>, but still have some questions:</p>

<p>Isn't the [CLS] token at the very beginning of each sentence? Why is that ""we are only interested in BERT's output for the [CLS] token""? Can anyone help me get my head around this? Thanks!</p>
","nlp"
"66205","Predicting Missing Word in Text","2020-01-09 17:07:08","","1","682","<nlp><bert>","<p>I know about BERT and other solutions when you masking some words and try to predict them.
But let say I have a text:</p>

<blockquote>
  <p>Transformer have taken the of Natural Processing
  by storm, transforming the field by leaps and bounds. New,
  bigger, and better models to crop up almost every , 
  benchmarks in performance across a wide variety of tasks.</p>
</blockquote>

<p>And I cannot in advance say to BERT where masking is. I am looking for an algorithm which can understand where missing words are and after that predict them.</p>
","nlp"
"66201","Method to assess text credibility","2020-01-09 16:05:17","66408","3","308","<python><nlp><text-filter>","<p>I am searching for an automated method (ideally a python package) that produces a score to assess the <em>credibility</em> of a given text (e.g. from a webpage).</p>

<p>I am <strong>not</strong> searching for: </p>

<ul>
<li>text <em>complexity</em> assessments (i.e. how long sentences are and how many difficult words are used) as for example flesch reading ease, smog
index, flesch kincaid grade, coleman liau index, automated
readability index, dale chall readability score, difficult words
index, linsear write formula, or gunning fog.</li>
<li>text <em>coherence</em> (i.e. how well the next sentence fit with the previous one) as for example <a href=""https://arxiv.org/pdf/1710.07770.pdf"" rel=""nofollow noreferrer"">Text Coherence Analysis Based on Deep Neural Network</a></li>
</ul>

<p>Why is complexity/coherence not the same credibility? Because many texts advertising for example homeopathy use long complex scientifically sounding and complex word loaded sentences while being nonsense in terms of trueness. Therefore I am wondering if there is any method to assess the credibility/reliability of a given piece of text/webpage information automatically? </p>
","nlp"
"66180","Python package to assess text coherence","2020-01-09 10:27:02","","2","1716","<python><nlp>","<p>I am looking for a python package that calculates how well one sentence of a natural text follows the next. One could simply count how many identical words are in the next sentence but the better method would be to compare word similarities using something like word vectors (=semantically similar words instead of exact matches or synonyms).</p>

<p>Coherent:</p>

<blockquote>
<pre><code>Tom loves reading books.
He prefers reading books at library.
So he always goes to library.
</code></pre>
</blockquote>

<p>Non-coherent:</p>

<blockquote>
<pre><code>Tom loves reading books.
He missed his lunch today.
So he always goes to library.
</code></pre>
</blockquote>

<p>I guess there must be several well written packages for such an automatic assessment (or similar methods) but I just can't find it. Any ideas?</p>
","nlp"
"66165","How can I extract the reason of the legal compensation from a court report?","2020-01-09 08:18:42","","0","41","<deep-learning><nlp><data><stanford-nlp>","<p>I'm working on a project (court-related). At a certain point, I have to extract the reason of the legal compensation. For instance, let's take these sentences (from a court report)</p>

<p>Order mister X to pay EUR 5000 for compensation for unpaid wages</p>

<p>and</p>

<p>To cover damages, mister X must pay EUR 4000 to mister Y</p>

<p>I want to make an algorithm that is able from this sentence to extract the motive of legal compensation. For the first sentence</p>

<p>Order mister X to pay EUR 5000 for compensation for unpaid wages</p>

<p>the algorithm's output must be ""compensation for unpaid wages"" or ""compensation unpaid wages "".</p>

<p>For the second sentence, the algorithm's output must be ""cover damages"". Output can be a string or a list of string, it doesn't matter.</p>

<p>As I'm not an NLP expert (but I have already worked on a project on sentiment analysis, so I know some stuff about NLP), and there are so many articles, I don't know where to start.</p>

<p>I'm working on French texts, but I can get away with working on English texts.</p>
","nlp"
"66111","Approach to semantic similarity between documents","2020-01-08 13:39:33","","2","214","<nlp><similarity><cosine-distance><doc2vec>","<p>I was wondering what approach people would take, or point me in the right direction on this challenge I have set myself. I am pretty new at this, I have covered some area but want to expand my skillset. </p>

<p>Say you have an abstract from a research paper, which is a summarised form of information of a larger document, can you calculate, from a list of papers, which research paper this abstract belongs to?</p>

<p>Please note: I am not asking how to summarise the research paper. Also, note that the abstract information doesn't necessarily take the same form as the research paper, but is semantically similar.  </p>

<p>Would you, encode both datasets with a something like doc2vec to try to get the semantic meaning of the texts and then use cosine similarity?</p>

<p>Would the semantics of the numbers used in these papers get lost in the vectorizing?</p>

<p>IN this case, would a custom encoder work the best, or do you think USE or doc2vec would fair better?</p>

<p>Forgive my basic questions, I just wanted to explore things before I started coding!</p>
","nlp"
"66067","How to approach TF-IDf based analysis?","2020-01-08 01:16:04","66069","2","89","<scikit-learn><nlp><text-mining><tfidf>","<p><strong>Problem statement :</strong></p>

<p>We have documents with list of words in them.
Overall these documents are classified into 2 group (say, good quality vs bad)</p>

<p>docs -</p>

<pre><code>doc1 = [w1,w2,w3,w4]
doc2 = [w4,w3,w3,w4]
doc3 = [w2,w4,w8,w1]
doc4 = [w5,w4,w0,w9]
</code></pre>

<p>doc group -</p>

<pre><code>good_grp = [doc2, doc1]
bad_grp = [doc3, doc4]
</code></pre>

<p><strong><em>Now we have to find out which words actually are important to make the document good vs bad ?</em></strong></p>

<p><strong>Idea 1:</strong> 
Merge all words from documents that belong to document group 1 into single document say (good quality doc) and other one being (bad quality doc) and calculate tf-idf score per doc; but in this case we lose information of document level words and now just see document group level word importance.</p>

<pre><code>doc1 = [w1,w2,w3,w4]
doc2 = [w4,w3,w3,w4]
doc3 = [w2,w4,w8,w1]
doc4 = [w5,w4,w0,w9]

good_grp = [w1,w2,w3,w4,w4,w3,w3,w4]
bad_grp = [w2,w4,w8,w1,w5,w4,w0,w9]
</code></pre>

<p><strong>Can someone help me to direct to a better approach tf-idf or any other technique to solve this problem?</strong></p>
","nlp"
"66044","What could be proper terms for a research direction in natural language processing to measure meaningfulness?","2020-01-07 17:12:07","","2","33","<nlp><reference-request>","<p>For some time, I did assessments to design metrics on how to recognize well-written and meaningful software requirements. Then I decided to work with Stack Overflow question posts because they are a large corpus and have also votes which indicate human perceived quality<sup>1</sup>.</p>

<p>Now I am not sure which proper data science/NLP terms would describe the methodology to sort out what is a meaningful text or not.</p>

<p>I've been looking over entropy and semiotics, also research specifically on the Stack Overflow corpus but I fail to find seminal papers dealing actually with defining and measuring <em>meaningfulness</em> of text.</p>

<p>What is <em>meaningfulness</em>?</p>

<ul>
<li>According to Wiktionary and Wikipedia, <em><a href=""https://en.wiktionary.org/wiki/meaningfulness"" rel=""nofollow noreferrer"">meaningfulness</a></em> is ""<em>the state or measure of being meaningful</em>"", while <em><a href=""https://en.wiktionary.org/wiki/meaningful"" rel=""nofollow noreferrer"">meaningful</a></em> is ""<em>having meaning, significant</em>"", while <em><a href=""https://en.wikipedia.org/wiki/Meaning_(linguistics)"" rel=""nofollow noreferrer"">meaning</a></em> is ""<em>the information or concepts that a sender intends to convey, or does convey, in communication with a receiver</em>"".</li>
<li>I've also asked a <a href=""https://linguistics.stackexchange.com/questions/34801/is-there-a-formal-definition-of-the-term-meaning"">question</a> on Linguistics SE to find out whether there are recent results from academic reseach on that and more up-to-date definitions.</li>
</ul>

<p>Stack Overflow folks would have already implemented some practical entropy detection algorithms to filter out low quality posts (as we know it by the system filter, until you have typed in more content) but for example the following quite meaningless question produces no warning from the - that is, entropy i.e. randomness  appears low enough, but indeed the text is indeed very random and meaningless.</p>

<p><a href=""https://i.sstatic.net/71ATd.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/71ATd.png"" alt=""meaningless-question""></a></p>

<p>I do understand there is no absolute ""meaning"" because it depends on the context of the message receiver (semiotics!), but then it should be possible to put an message (SO question) into context of all previously received messages (posted SO questions).</p>

<p><sup>1</sup> The fact that SO content features code snippets additionally to natural language, it's possible a subject to research on its own what does it mean, what contributes etc.; for simplicity, I'll possibly just exclude code for text analysis but for sure it's also worth looking on the correlation between code snippets and votes. As said, it could be a research topic on its own.</p>
","nlp"
"66009","Multilingual Bert sentence vector captures language used more than meaning - working as interned?","2020-01-07 07:29:00","66019","3","507","<deep-learning><nlp><pytorch><bert>","<p>Playing around with BERT, I downloaded the Huggingface Multilingual Bert and entered three sentences, saving their sentence vectors (the embedding of <code>[CLS]</code>), then translated them via Google Translate, passed them through the model and saved their sentence vectors.</p>

<p>I then compared the results using cosine similarity.</p>

<p>I was surprised to see that each sentence vector was pretty far from the one generated from the sentence translated from it (0.15-0.27 cosine distance) while different sentences from the same language were quite close indeed (0.02-0.04 cosine distance).</p>

<p>So instead of having sentences of similar meaning (but different languages) grouped together (in 768 dimensional space ;) ), dissimilar sentences of the same language are closer. </p>

<p>To my understanding the whole point of Multilingual Bert is inter-language transfer learning - for example training a model (say, and FC net) on representations in one language and having that model be readily used in other languages.  </p>

<p>How can that work if sentences (of different languages) of the exact meaning are mapped to be more apart than dissimilar sentences of the same language?  </p>

<p>My code:</p>

<pre><code>import torch

import transformers
from transformers import AutoModel,AutoTokenizer

bert_name=""bert-base-multilingual-cased""
tokenizer = AutoTokenizer.from_pretrained(bert_name)
MBERT = AutoModel.from_pretrained(bert_name)

#Some silly sentences
eng1='A cat jumped from the trees and startled the tourists'
e=tokenizer.encode(eng1, add_special_tokens=True)
ans_eng1=MBERT(torch.tensor([e]))

eng2='A small snake whispered secrets to large cats'
t=tokenizer.tokenize(eng2)
e=tokenizer.encode(eng2, add_special_tokens=True)
ans_eng2=MBERT(torch.tensor([e]))

eng3='A tiger sprinted from the bushes and frightened the guests'
e=tokenizer.encode(eng3, add_special_tokens=True)
ans_eng3=MBERT(torch.tensor([e]))

# Translated to Hebrew with Google Translate
heb1='חתול קפץ מהעץ והבהיל את התיירים'
e=tokenizer.encode(heb1, add_special_tokens=True)
ans_heb1=MBERT(torch.tensor([e]))

heb2='נחש קטן לחש סודות לחתולים גדולים'
e=tokenizer.encode(heb2, add_special_tokens=True)
ans_heb2=MBERT(torch.tensor([e]))

heb3='נמר רץ מהשיחים והפחיד את האורחים'
e=tokenizer.encode(heb3, add_special_tokens=True)
ans_heb3=MBERT(torch.tensor([e]))


from scipy import spatial
import numpy as np

# Compare Sentence Embeddings

result = spatial.distance.cosine(ans_eng1[1].data.numpy(), ans_heb1[1].data.numpy())

print ('Eng1-Heb1 - Translated sentences',result)


result = spatial.distance.cosine(ans_eng2[1].data.numpy(), ans_heb2[1].data.numpy())

print ('Eng2-Heb2 - Translated sentences',result)

result = spatial.distance.cosine(ans_eng3[1].data.numpy(), ans_heb3[1].data.numpy())

print ('Eng3-Heb3 - Translated sentences',result)

print (""\n---\n"")

result = spatial.distance.cosine(ans_heb1[1].data.numpy(), ans_heb2[1].data.numpy())

print ('Heb1-Heb2 - Different sentences',result)

result = spatial.distance.cosine(ans_eng1[1].data.numpy(), ans_eng2[1].data.numpy())

print ('Heb1-Heb3 - Similiar sentences',result)

print (""\n---\n"")

result = spatial.distance.cosine(ans_eng1[1].data.numpy(), ans_eng2[1].data.numpy())

print ('Eng1-Eng2 - Different sentences',result)

result = spatial.distance.cosine(ans_eng1[1].data.numpy(), ans_eng3[1].data.numpy())

print ('Eng1-Eng3 - Similiar sentences',result)

#Output:
""""""
Eng1-Heb1 - Translated sentences 0.2074061632156372
Eng2-Heb2 - Translated sentences 0.15557605028152466
Eng3-Heb3 - Translated sentences 0.275478720664978

---

Heb1-Heb2 - Different sentences 0.044616520404815674
Heb1-Heb3 - Similar sentences 0.027982771396636963

---

Eng1-Eng2 - Different sentences 0.027982771396636963
Eng1-Eng3 - Similar sentences 0.024596810340881348
""""""
</code></pre>

<p>P.S.</p>

<p>At least the Heb1 was closer to Heb3 than to Heb2.
This was also observed for the English equivalents, but less so. </p>

<p>N.B.
Originally asked on Stack Overflow, <a href=""https://stackoverflow.com/questions/59619760/multilingual-bert-sentence-vector-captures-language-used-more-than-meaning-wor"">here</a></p>
","nlp"
"65878","How we compare two paragraphs of unlabelled dataset semantically (STS)?","2020-01-04 23:23:12","","1","36","<deep-learning><nlp><unsupervised-learning><semantic-similarity>","<p>Column representation: <strong>Unique_id | Text1 | Text2</strong></p>

<p><strong>Unique_id</strong> 0</p>

<p><strong>Text1</strong> public show for reynolds suspension of his coaching licence. portrait sir joshua reynolds portrait of omai will get a public airing following fears it would stay hidden because of an export wrangle.</p>

<p><strong>Text2</strong> then requested to do so by spain s anti-violence commission. the fine was far less than the expected amount of about Â£22 000 or even the suspension of his coaching licence.</p>

<p><strong>Unique_id</strong> 1</p>

<p><strong>Text1</strong> groening. gervais has already begun writing the script but is keeping its subject matter a closely guarded secret. he will also write a part for himself in the episode. i ve got the rough idea but this is the most intimidating project of my career.</p>

<p><strong>Text2</strong> philadelphia said they found insufficient evidence to support the woman s allegations regarding an alleged incident in january 2004. the woman reported the allegations to canadian authorities last month. cosby s lawyer walter m phillips jr said the comedian was pleased with the decision.</p>

<p>In the above problem, I've to <strong>compare two paragraphs texts</strong> i.e. Text1 &amp; Text2 and then I've to compare semantic similarity between two texts. If they are <strong>semantically similar then it will print '1' if not then '0'</strong></p>

<p>Any refrence implementation link or any suggestions !</p>
","nlp"
"65865","NLP: How to group sub-field into fields?","2020-01-04 17:19:57","65869","2","80","<nlp>","<p>Suppose I have a list of strings that captures a sub-field of academic research and would like to group them as higher-level fields.
For example, </p>

<pre><code>'Quantum Mechanics'  =&gt; 'Physics'
'Abstract Algebra'   =&gt; 'Mathematics'
....
</code></pre>

<p>My understanding is that standard NLP techniques may not fit here, because the relationship between sub-fields and fields are linked by its meanings but not word-frequency or word-embedding etc. </p>

<p>I wonder if there is anything done that could be useful to tackle this problem (papers or packages)?</p>
","nlp"
"65864","NLP : variations of a text without modifying it's meaning","2020-01-04 16:53:52","66674","8","1085","<nlp><neural-style-transfer>","<p>I am currently working on the automation of recurring reports (weekly 30-50 pages reports for around 100 districts). Those reports have a mostly fixed form : maps, graphs, data tables and small zone of text.</p>

<p>Apart for some discussion around colors and legends, it isn't difficult to automate the production of maps / graphs / tables. (I work with Rmarkdown if you want to know)</p>

<p>However, for the text, a simple approach like writing 'r value' in markdown to produce a variable value inside of the text feel 'too automated'. The reports end up having ten sentences like 'During the last quarter (QX 201X) total result was XXX (a +X% growth compared to the same quarter the previous year).'</p>

<p>I'd like to get automatic variations of that phrase without modyfiying it's meaning. I've ended up writing half a dozen variations myself. But (1) it still feels repetitive and unnatural, and (2) doing it for every phrase of the report may take a lot of time.</p>

<p>We have seen a lot of extraordinary things in transfering things for visual representation (see : <a href=""https://en.wikipedia.org/wiki/Neural_Style_Transfer"" rel=""noreferrer"">https://en.wikipedia.org/wiki/Neural_Style_Transfer</a>). So I was wondering if we have similar things for NLP, that would allow a text to be rewritten using a different 'style' (a neutral style -or an absence of style- in my case), keeping it's main content. The main paper I found on the subject is titled '<a href=""https://www.groundai.com/project/what-is-wrong-with-style-transfer-for-texts/1"" rel=""noreferrer"">What is wrong with style transfer for texts?</a>' and shows why style transfer doesn't really work for texts. Given (1) the constraint (keeping the same meaning) and (2) it's formalism (I know which number should be shown), I feel like the problem may be simpler than the whole style transfert.</p>

<p>Any idea where to start to automatically write variations of a text while keeping it's meaning constant ?</p>
","nlp"
"65857","How to incorporate keyboard positions on character level embeddings?","2020-01-04 13:35:19","65859","1","59","<neural-network><nlp><feature-engineering><word-embeddings>","<p>I am working with NLP and have <strong>character level embeddings</strong>.</p>

<p>I have embeddings learned from Wikipedia text.</p>

<p>Now, I want to learn embeddings from chat data (where misspellings and abbreviations are way more common). Usually, the character <code>n</code> doesn't follow from character <code>b</code>, however, during texting, this can be common because they are <strong>close together on the keyboard</strong>, and a misspelling occurs.</p>

<p>So, my questions is: <strong>what are strategies to incorporate character keyboard position information to a traditional character level embedding?</strong></p>

<p>Note: it can be assumed that only QWERTY keyboards exist.</p>
","nlp"
"65825","Lemmatization of pandas column using Wordnet after POS","2020-01-03 23:14:30","","1","2203","<python><nlp><pandas><python-3.x>","<p>I have a pandas column <code>df_travail[line_text]</code> with text.</p>

<p>I want to lemmatize each word of this column.</p>

<p>First I Lowercase the text :</p>

<pre><code>df_travail ['lowercase'] = df_travail['line_text'].str.lower()
</code></pre>

<p>Then, I tokenize it and apply POS (because of wordnet default configuration which consider every word as noun).</p>

<pre><code>from nltk import word_tokenize, pos_tag
tok_and_tag = lambda x: pos_tag(word_tokenize(x))
df_travail ['tok_and_tag'] = df_travail['lowercase'].apply(tok_and_tag)
</code></pre>

<p>Then I have the following : (extract of the entire <code>df_travail['tok_and_tag']</code>)</p>

<pre><code>""[('so', 'RB'), ('you', 'PRP'), (""""'ve"""", 'VBP'), ('come', 'VBN'), ('to', 'TO'), ('the', 'DT'), ('master', 'NN'), ('for', 'IN'), ('guidance', 'NN'), ('?', '.'), ('is', 'VBZ'), ('this', 'DT'), ('what', 'WP'), ('you', 'PRP'), (""""'re"""", 'VBP'), ('saying', 'VBG'), (',', ','), ('grasshopper', 'NN'), ('?', '.')]""
[('actually', 'RB'), (',', ','), ('you', 'PRP'), ('called', 'VBD'), ('me', 'PRP'), ('in', 'IN'), ('here', 'RB'), (',', ','), ('but', 'CC'), ('yeah', 'UH'), ('.', '.')]
</code></pre>

<p>However, then, I'm lost about the lemmatization function to apply (with Wordnet), in order to take into account the fact that I applied POS ?</p>
","nlp"
"65767","How can we perform STS (Semantic Textual Similarity) on unsupervised dataset using deep learning?","2020-01-03 04:45:14","","1","522","<deep-learning><nlp><unsupervised-learning><similarity>","<p>How do you implement STS(Semantic Textual Similarity) on an unlabelled dataset? The dataset column contains <code>Unique_id</code>, <code>text1</code> (contains paragraph), and <code>text2</code> (contains paragraph).</p>
<p>Ex: Column representation:  Unique_id   |    Text1    |     Text2</p>
<p><strong>Unique_id</strong> 0</p>
<p><code>Text1</code> public show for Reynolds suspension of his coaching licence. portrait Sir Joshua Reynolds portrait of omai will get a public airing following fears it would stay hidden because of an export wrangle.</p>
<p><code>Text2</code> then requested to do so by Spain's anti-violence commission. The fine was far less than the expected amount of about Â£22 000 or even the suspension of his coaching license.</p>
<p><strong>Unique_id</strong> 1</p>
<p><code>Text1</code> Groening. Gervais has already begun writing the script but is keeping its subject matter a closely guarded secret. he will also write a part for himself in the episode. I've got the rough idea but this is the most intimidating project of my career.</p>
<p><code>Text2</code> Philadelphia said they found insufficient evidence to support the woman s allegations regarding an alleged incident in January 2004. The woman reported the allegations to Canadian authorities last month. Cosby s lawyer Walter m Phillips jr said the comedian was pleased with the decision.</p>
<p>In the above problem, I've to compare two paragraphs of texts i.e. <code>Text1</code> &amp; <code>Text2</code>, and then I've to compare semantic similarity between two texts. If they are semantically similar then it will print '1' if not then '0'</p>
<p>Any reference implementation link or any suggestions!</p>
<p>Thanks in advance!</p>
","nlp"
"65746","MultinomialNB predict_proba doesnt return labels with the probability","2020-01-02 17:19:49","65780","0","606","<machine-learning><python><multilabel-classification><nlp>","<p>I have a model what looks like this</p>

<pre><code>Product_names= [
    'Dress white',
    'Pullover shirt',
    'etc'


]

category_labels = [
   'Female-&gt;Clothes-&gt;T-shirts',
   'Female-&gt;Jeans-&gt;Skinny',
   'etc'
]
</code></pre>

<p>I use the  MultinomialNB classifier to predict a new product name into a category. Before  it gets predicted i want to get the probability of the prediction.</p>

<p>So in psuedo it looks likes this:</p>

<pre><code>clf = MultinomialNB()  
clf.fit(Product_names,category_labels)   
clf.predict_proba('White Pullover shirt')

</code></pre>

<p>What I get is this:</p>

<pre><code>[4.18600796e-03 7.46021220e-04 4.14456233e-05 4.14456233e-05
 1.16047745e-03 6.92141910e-03 3.70938329e-03 1.78216180e-03
 1.49204244e-03 2.15517241e-03 1.03614058e-04 3.48143236e-03
 3.27420424e-03 7.66744032e-04 1.03614058e-03 8.91080902e-04
 8.49635279e-04 2.07228117e-04 4.14456233e-05 4.14456233e-05
 2.44529178e-03 1.84433024e-03 1.67854775e-03 1.01541777e-03
 5.28431698e-03 1.03614058e-03 1.45059682e-04 8.28912467e-05
 9.53249337e-04 1.86505305e-03 2.59035146e-03 2.32509947e-02
 1.24336870e-04 6.21684350e-05 2.69396552e-04 1.90028183e-02
 6.83852785e-04 8.28912467e-05 2.07228117e-05 8.91080902e-04
 5.80238727e-03 3.39854111e-03 1.11488727e-02 6.21684350e-05
 3.31564987e-04 8.18551061e-03 7.46021220e-04 3.52287798e-04
 6.21684350e-05 1.50862069e-02 2.48673740e-04 1.53141578e-02
 4.64190981e-03 4.14456233e-05 2.27950928e-04 1.73242706e-02
 8.89008621e-03 4.14456233e-04 1.28481432e-03 1.65782493e-04
 3.99950265e-03 7.41876658e-03 3.31564987e-04 1.90649867e-03
 1.24336870e-04 7.39804377e-03 1.07758621e-03 6.21684350e-05
 3.39854111e-03 2.19661804e-03 3.85444297e-03 1.88577586e-03
 3.56432361e-03 1.03614058e-03 2.07228117e-05 4.35179045e-04
 6.90069629e-03 1.86505305e-04 2.27950928e-03 2.90119363e-04
 4.39323607e-03 4.14456233e-05 5.18070292e-04 1.80288462e-03
 4.14456233e-05 4.10311671e-03 3.93733422e-04 4.53829576e-03
 6.21684350e-05 1.80288462e-03 5.38793103e-04 2.01011273e-03
 3.68037135e-02 3.50008289e-02 2.63386936e-02 9.82261273e-03
 1.75729443e-02 2.89497679e-02 1.78423408e-02 2.69396552e-03
 3.04418103e-02 4.35179045e-03 3.29492706e-03 1.59565650e-03
 1.67854775e-03 1.58115053e-02 1.83604111e-02 2.34375000e-02
 1.50033156e-02 1.38221154e-02 4.66263263e-03 1.92722149e-03
 1.59565650e-03 1.09830902e-03 3.43998674e-03 2.17589523e-03
 3.81299735e-03 1.11281499e-02 1.45059682e-04 1.91271552e-02
 1.96866711e-03 4.55901857e-04 5.80238727e-04 6.21684350e-05
 1.86505305e-04 6.15467507e-03 8.84864058e-03 3.73010610e-04
 1.24336870e-03 7.04575597e-04 1.03614058e-04 7.66744032e-04
 1.24336870e-04 4.14456233e-04 2.07228117e-04 4.97347480e-04
 1.61637931e-03 1.45059682e-04 1.20192308e-03 3.43998674e-03
 1.24336870e-04 3.00480769e-03 1.71999337e-03 1.03614058e-04
 2.07228117e-03 3.33637268e-03 1.69927056e-03 2.56962865e-03
 3.21203581e-03 5.38793103e-04 2.92191645e-03 4.24817639e-03
 4.90508952e-02 3.35709549e-03 6.00961538e-04 2.27950928e-04
 6.19612069e-03 1.59565650e-02 4.14456233e-03 1.52934350e-02
 8.70358090e-04 8.28912467e-05 1.24336870e-04 1.86505305e-03
 8.28912467e-05 1.80288462e-03 1.99767905e-02 2.63179708e-03
 2.69396552e-04 8.35129310e-03 7.08720159e-03 3.10842175e-04
 2.96336207e-03 3.46070955e-03 1.13975464e-03 3.58504642e-03
 5.59515915e-04 2.23806366e-03 2.07228117e-04 4.43468170e-03
 1.40915119e-02 1.15011605e-02 4.18600796e-03 4.20673077e-03
 7.41876658e-03 5.22214854e-03 2.07228117e-05 1.09001989e-02
 1.69927056e-03 1.45059682e-02 6.77635942e-03 1.46095822e-02
 3.25969828e-02 2.21734085e-03 9.94694960e-04 6.21684350e-05
 1.94794430e-03 3.62649204e-03 4.31034483e-03 3.25348143e-03
 1.28481432e-03 3.31564987e-04 3.93733422e-04 1.03614058e-04
 1.84433024e-03 1.71999337e-03 1.45059682e-04 2.73541114e-03
 2.25878647e-03 2.92191645e-03 3.31564987e-04 1.07758621e-03
 2.27950928e-04 1.65782493e-04 4.35179045e-04 1.26409151e-03
 1.51276525e-03 2.48673740e-04 4.14456233e-05 2.07228117e-04
 2.79757958e-03 8.28912467e-05 2.30023210e-03 1.24336870e-03
 3.31564987e-04 6.21684350e-05 9.11803714e-04 2.07228117e-05
 4.35179045e-04 2.07228117e-05 2.27950928e-04 1.16047745e-03
 9.73972149e-04 3.31564987e-04 3.64721485e-03]
</code></pre>

<p>Its an array of 235 items, It makes sense, because if i group my dataset on category it contains 235 categories. But its missing the Category label with the probability. Does someone know why? Or can someone give me the right directions?</p>

<p>I want to create a flask web application where i can see the probability of the prediction, if its greater then 0.80% for example i can assign it.</p>

<p>Can someone help me with this? I am struggling a couple of days with this issue now :(.</p>

<p>My complete code looks like this</p>

<pre class=""lang-py prettyprint-override""><code>import numpy as np
import pandas as pd
import seaborn as sns
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
import string
from nltk.corpus import stopwords

#open file
data = pd.read_csv('cats.csv',sep=';')
data['length'] = data['Product Name'].str.len()

#remove all puncs
def text_process(mess):
    # Check characters to see if they are in punctuation
    nopunc = [char for char in mess if char not in string.punctuation]
    # Join the characters again to form the string.
    nopunc = ''.join(nopunc)
    # Now just remove any stopwords
    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english') if word.lower() not in stopwords.words('dutch')]


# Might take awhile...
bow_transformer = CountVectorizer(analyzer=text_process).fit(data['Product Name'])

# Print total number of vocab words
messages_bow = bow_transformer.transform(data['Product Name'])
tfidf_transformer = TfidfTransformer().fit(messages_bow)
messages_tfidf = tfidf_transformer.transform(messages_bow)

from sklearn.naive_bayes import MultinomialNB
spam_detect_model = MultinomialNB().fit(messages_tfidf, data['Category Path'])

message4 = ""Some input data from flask web app ""
bow4 = bow_transformer.transform([message4])
tfidf4 = tfidf_transformer.transform(bow4)

predicted =  spam_detect_model.predict_proba(tfidf4)[0]
print(predicted) 

</code></pre>

<p>Thanks!</p>
","nlp"
"65688","Why I get a very low accuracy with LSTM and pretrained word2vec?","2020-01-01 16:17:40","","2","460","<machine-learning><neural-network><deep-learning><keras><nlp>","<p>I'm working on a reviews classification model with only two categories 0 (negative) and 1 (positive). I'm using pre-trained word2vec from google with LSTM. The problem is I get an accuracy of around 50% where it should be around 83% according to <a href=""http://unitech-selectedpapers.tugab.bg/2018/papers/s4_p218.pdf"" rel=""nofollow noreferrer"">this paper</a>. I tried many different hyperparameters combination and still gets a horrible accuracy. I also tried to change the data preprocessing techniques and tried stemming but it hasn't resolved the problem</p>

<p>here's my code</p>

<pre><code>X, y = read_data()
X = np.array(clean_text(X)) #apply data preprocessing  
tokenizer = Tokenizer()
tokenizer.fit_on_texts(X)

#converts text to sequence and add padding zeros
sequence = tokenizer.texts_to_sequences(X)
X_data = pad_sequences(sequence, maxlen = length, padding = 'post')

X_train, X_val, y_train, y_val = train_test_split(X_data, y, test_size = 0.2)

#Load the word2vec model
word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)

word_index = tokenizer.word_index
nb_words = min(MAX_NB_WORDS, len(word_index))+1

embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))
null_words = []
for word, i in word_index.items():
    if word in word2vec.wv.vocab:
        embedding_matrix[i] = word2vec.word_vec(word)
    else:
        null_words.append(word)

embedding_layer = Embedding(embedding_matrix.shape[0], # or len(word_index) + 1
                            embedding_matrix.shape[1], # or EMBEDDING_DIM,
                            weights=[embedding_matrix],
                            input_length=701,
                            trainable=False)

model = Sequential()
model.add(embedding_layer)
model.add(LSTM(100))
model.add(Dropout(0.4))
model.add(Dense(2, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
</code></pre>

<p>I also tried other optimizers like AdaMax and MSLE loss function. I'm just confused if the problem isn't with the model and preprocessing where could it be? Thanks </p>
","nlp"
"65620","User profiling based on multiple posts","2019-12-30 10:08:29","","5","86","<machine-learning><deep-learning><nlp><bert>","<p>I currently have collected a dataset of different social media posts for each user with labels assigned to each user. I tried to use LSTM, and BERT for the text classification problem, So for each post, I try to predict the label(for example age). This is not sufficient because you need all of the information contained in the sum of posts to determine the user's age for example.
My first thought was to concatenate all of the posts for a single user but since I am currently using BERT which has a maximum sequence length of 512 it wouldn't work. My second idea was to use a text summary and combine them in one vector and hope it doesn't pass the maximum length limit.</p>

<p>Do you have any suggestions for a possible solution? I would assume this problem has been dealt with in the scientific literature and I would be thankful if anyone could point me in the right direction.</p>
","nlp"
"65577","How to do feature selection after using pre-trained word embeddings?","2019-12-29 10:48:16","","2","1140","<nlp><feature-selection><feature-engineering>","<p>I am working on a multiclass text classification problem. I want to use the top k features based on mutual information (<code>mutual_info_classif</code>) for training my model.</p>

<p>I started this project on ML models:</p>

<p>I used <code>tfidf</code> for feature extraction and then used <code>mutual_info_classif</code> for feature selection. </p>

<pre class=""lang-py prettyprint-override""><code>    svc = Pipeline([('tfidf', TfidfVectorizer()),
                   ('ig', SelectKBest(mutual_info_classif,k=1000)),
                   ('clf',LinearSVC(multi_class='ovr')),
              ])
    svc.fit(X_train, y_train)
    y_pred = svc.predict(X_test)
</code></pre>

<p>This was pretty straight-forward.</p>

<p>Next, I started to work on RNN (more specifically a simple LSTM model)and that is where I am having a problem.</p>

<p>I have used pre-trained word embeddings from GloVe (300d) to get features from my data.
The embedding matrix I feed into the embedding layer of my RNN has the shape (4293,300), 4293 is the number of unique words found in my data and 300 is the dimension.</p>

<p><strong>My questions are:</strong> Is there any way that I can use the top 1000 words (features) out of these 4293 based on <code>mutual_info_classif</code>? Is it even possible to do so? If yes, then should it be done before making the embedding_matrix or after?</p>
","nlp"
"65556","The differences between BNF and JSGF in NLP?","2019-12-28 15:45:24","","2","223","<nlp><language-model>","<p>I wonder what the differences are between the <a href=""http://matt.might.net/articles/grammars-bnf-ebnf/"" rel=""nofollow noreferrer"">BNF</a>(Backus-Naur Form) and <a href=""https://en.wikipedia.org/wiki/JSGF"" rel=""nofollow noreferrer"">JSGF</a>(Java Speech Grammar Format)? The former is a kind of context-free grammar taught in <a href=""https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1162/handouts/cgw/cgw-instructions.pdf"" rel=""nofollow noreferrer"">CS224</a>, but I learned that the latter is also being used. Could anyone tell me which one is better and what are their differences?</p>
","nlp"
"65477","Change the way spacy works - Custom properties for training and prediction","2019-12-26 16:53:32","","3","58","<nlp><named-entity-recognition><spacy>","<p>Spacy detects the entities using its predefined algorithm. It parses tokens in text considering position of tokens with respect to tokens surrounding it. It also takes into consideration the POS tagging for these tokens.   </p>

<p>However, I believe it misses the position of tokens above and below (For example in a tabular data) or it also misses few properties of text like if it is underline etc.  This statement is based on my understanding. Please correct me if I am wrong in these.  </p>

<p>Now the question is, <strong>can such properties be taken into consideration <em>while doing training and prediction</em> of entities</strong>? I have seen Extension Attributes, but these do not play role during training and prediction but work as meta data.</p>
","nlp"
"65430","Which kind of model is better for keyword-set classification?","2019-12-26 06:00:27","65496","2","71","<deep-learning><classification><nlp><text-mining><text-classification>","<p>There exists a similar task that is named text classification.</p>

<p>But I want to find a kind of model that the inputs are keyword set. And the keyword set is not from a sentence. </p>

<p>For example:</p>

<pre><code>input [""apple"", ""pear"", ""water melon""] --&gt; target class ""fruit""
input [""tomato"", ""potato""] --&gt; target class ""vegetable""
</code></pre>

<p>Another example:</p>

<pre><code>input [""apple"", ""Peking"", ""in summer""]  --&gt;  target class ""Chinese fruit""
input [""tomato"", ""New York"", ""in winter""]  --&gt;  target class ""American vegetable""
input [""apple"", ""Peking"", ""in winter""]  --&gt;  target class ""Chinese fruit""
input [""tomato"", ""Peking"", ""in winter""]  --&gt;  target class ""Chinese vegetable""
</code></pre>

<p>Thank you.</p>
","nlp"
"65417","Text summarization with limited number of words","2019-12-25 21:55:33","65440","1","299","<nlp><automatic-summarization>","<p>I am reviewing summarization techniques and haven't (yet) found an approach to limit the length of a summary. So for example a summarization function that gives me a summary that is <code>&lt; 500 words</code>.</p>

<p>Can you point me in the right direction? Are there approaches/implementations out there that try to solve this challenge?</p>

<p>Appreciate your replies!</p>
","nlp"
"65281","How to deal with spelling errors NLP","2019-12-22 16:49:27","","9","5355","<machine-learning><classification><nlp><text-mining>","<p>I have some data where the main column is the description of one product. The main task is to extract the name of some product from this column, where it sometimes is spelled wrong and amended in other words. I have more than a thousand possible product names.</p>

<p>Currently I'm just using regular expressions with a list of product names to find and extract the product names in each row of the dataset, but it's not working well in the cases of misspelled product names. </p>

<p>Since I already have more than 50,000 rows in which the product was extracted by hand in cases where there is some product from the list in the column. I am wondering if after breaking each description into multiple lines (tokenization) could apply some classification/search method to detect if there is some product and which is in the description.</p>

<blockquote>
  <p>Example: Medical product used in cancer treatment ""PRODUCT XXXXYYYY""
  where the XXXX and YYYY are the first and second name of the product.</p>
</blockquote>

<p>I think most of the description would be unused, because as in the example above, simply being a medical product would not help at all since there are several possibilities, but the presence of the wrongly spelled words that describe a product specific would be helpful. I am open to suggestions on how to deal with these spelling errors.</p>
","nlp"
"65276","Why is T test reweighting on a word X word co-occurrence matrix so effective?","2019-12-22 11:44:06","65277","2","347","<nlp><statistics><word-embeddings><stanford-nlp>","<p>I am going through Stanford NLP class: <a href=""http://web.stanford.edu/class/cs224u/"" rel=""nofollow noreferrer"">http://web.stanford.edu/class/cs224u/</a></p>

<p>A task in the homework is to implement T-test reweighting on a word X word co-occurrence matrix: 
<a href=""https://nbviewer.jupyter.org/github/cgpotts/cs224u/blob/2019-spring/hw1_wordsim.ipynb#t-test-reweighting-[2-points]"" rel=""nofollow noreferrer"">https://nbviewer.jupyter.org/github/cgpotts/cs224u/blob/2019-spring/hw1_wordsim.ipynb#t-test-reweighting-[2-points]</a></p>

<p><span class=""math-container"">$$\textbf{ttest}(X, i, j) = 
\frac{
    P(X, i, j) - \big(P(X, i, *)P(X, *, j)\big)
}{
\sqrt{(P(X, i, *)P(X, *, j))}
}$$</span></p>

<p>I have 2 questions:</p>

<ul>
<li><p>What is the intuition behind this formula? It looks a little like PMI but I can't understand what it's doing. The T-test explanation out there seems to be unrelated to this task.</p></li>
<li><p>It works amazingly well (when evaluated by <a href=""https://nbviewer.jupyter.org/github/cgpotts/cs224u/blob/2019-spring/hw1_wordsim.ipynb#Full-evaluation"" rel=""nofollow noreferrer"">this test</a>): raw matrix yield a correlation score of 0.014, PMIed matrix 0.123 and t scored matrix: 0.408979. This number seems almost too good to be true for such a simple model. Can anyone bring some intuition/experience about why that is?</p></li>
</ul>
","nlp"
"65272","What's the best way to train a NER model?","2019-12-22 10:57:07","65274","1","2010","<python><nlp><data><training><named-entity-recognition>","<p>I am trying to do a project using NLP. My goal is to process Cyber Threat Intelligence articles like <a href=""https://unit42.paloaltonetworks.com/rancor-cyber-espionage-group-uses-new-custom-malware-to-attack-southeast-asia/"" rel=""nofollow noreferrer"">this</a> to extract information such as actor’s name, malwares and tools used…</p>

<p>To do that I want to use NER. However, there isn’t training data available on the web. So I was wondering if I should process manually 10-20 articles to make my training data or if I could do something like taking only interesting lines such as <code>“Rancor conducted at least two rounds of attacks intending to install Derusbi or KHRat malware on victim systems”</code> in multiples articles and replacing the group name by another actor. This way I could deduplicate my training data by the number of known actors. But doing that, only the actor name is changing. So, the context is always the same.</p>

<p>I am wondering what’s the best way to train my model considering the quantity of training data available?</p>
","nlp"
"65271","How to calculate perplexity in PyTorch?","2019-12-22 10:30:12","65273","1","4455","<lstm><pytorch><nlp><language-model>","<p>I am wondering the calculation of <code>perplexity</code> of a <code>language model</code> which is based on <code>character level LSTM model</code>. I got the code from <a href=""https://www.kaggle.com/francescapaulin/character-level-lstm-in-pytorch"" rel=""nofollow noreferrer"">kaggle</a> and edited a bit for my problem but not the training way. I have added some other stuff to graph and save logs. However, as I am working on a <code>language model</code>, I want to use <code>perplexity</code> measuare to compare different results. In <code>tensorflow</code>, I have done it via this <a href=""https://stackoverflow.com/a/42025194/1404324"">answer</a> and it was easy. I have looked for a way doing it in <code>PyTorch</code> and literally no related result on Google. I need some help, and it is really appreciated. </p>

<p>Here is the related code, I believe:</p>

<pre><code>criterion = nn.CrossEntropyLoss()

# create training and validation data
val_idx = int(len(data)*(1-val_frac))
data, val_data = data[:val_idx], data[val_idx:]

if(train_on_gpu):
    net.cuda()

counter = 0
n_chars = len(net.chars)
for e in range(epochs):
    # initialize hidden state
    h = net.init_hidden(batch_size)

    for x, y in get_batches(data, batch_size, seq_length):
        counter += 1

        # One-hot encode our data and make them Torch tensors
        x = one_hot_encode(x, n_chars)
        inputs, targets = torch.from_numpy(x), torch.from_numpy(y)

        if(train_on_gpu):
            inputs, targets = inputs.cuda(), targets.cuda()

        # Creating new variables for the hidden state, otherwise
        # we'd backprop through the entire training history
        h = tuple([each.data for each in h])

        # zero accumulated gradients
        net.zero_grad()

        # get the output from the model
        output, h = net(inputs, h)

        # calculate the loss and perform backprop
        loss = criterion(output, targets.view(batch_size*seq_length))
</code></pre>
","nlp"
"65241","Why is the decoder not a part of BERT architecture?","2019-12-21 17:09:07","65242","29","26177","<nlp><bert><machine-translation><attention-mechanism>","<p>I can't see how BERT makes predictions without using a decoder unit, which was a part of all models before it including transformers and standard RNNs. How are output predictions made in the BERT architecture without using a decoder? How does it do away with decoders completely?</p>
<p>To put the question another way: what decoder can I use, along with BERT, to generate output text? If BERT only encodes, what library/tool can I use to decode from the embeddings?</p>
","nlp"
"65236","Answer to Question","2019-12-21 15:31:45","","4","95","<nlp><sequence-to-sequence><question-answering>","<p>Looking for a system which can generate answers to questions. Most systems and blogs posted on internet are on Question to answer but not on answer to question or paraphrasing or keyword to questions.<br>
Seq2Seq I tried and even after training for many hours the results were not making sense.<br>
Rule bases and template based systems like add What, who where etc to keywords have so many pitfalls. But if any system known giving decent outputs may also work. Kindly let me know if there is any such system known.
Or if there is any other idea someone has then please suggest. I had above two ideas.</p>
","nlp"
"65206","Choosing the size of Character Embedding for Language Generation models","2019-12-20 21:41:09","","3","1406","<python><tensorflow><nlp><embeddings><text-generation>","<p>I am working on a character-based <strong>Language Generator</strong>, loosely based on <a href=""https://www.tensorflow.org/tutorials/text/text_generation"" rel=""nofollow noreferrer"">this tutorial on the TensorFlow 2.0 website</a>. Following the example, I am using an <code>Embedding()</code> layer to generate <strong>character embeddings</strong>: I want my model to generate text character by character.</p>

<p>My vocabulary counts <strong>86 unique characters</strong>. What embedding size should I choose?</p>

<p>Should I always choose an embedding size that is shorter than the size of vocabulary? The embedding size in the example above is much larger than the vocabulary size, I can't understand how this can build an effective model (but apparently it does, if it's an official tutorial, and if anyone can explain me why it'd be much appreciated).</p>

<hr>

<p>EDIT:</p>

<p>Another thing I find puzzling is: when we generate word embeddings is because we want a dense representation of a word meaning. Does it make sense to make it larger than the actual one-hot encoded vectors we started with?</p>
","nlp"
"65106","what are the step need to be followed inorder to retrain any pretrained neural network model?","2019-12-19 08:07:11","","1","24","<neural-network><nlp><cnn><rnn>","<p>what are the step need to be followed inorder to retrain any pretrained neural network model ? (<a href=""https://datascience.stackexchange.com/questions/64323/how-to-load-the-pre-trained-bert-model-from-local-colab-directory/64513?noredirect=1#comment69886_64513"">How to load the pre-trained BERT model from local/colab directory?</a> ) I tried to re train few pretrained neural network models . After retraining i get 3 CKPT files but im stuck on how i should proceed further after getting the 3 CKPT file . How to load the model buy pointing to these CKTP files ?</p>
","nlp"
"65082","General approach to work with text of phone calls (topics, promises, sentiments, etc.)","2019-12-18 21:10:58","","1","85","<nlp>","<p>I have an NLP task. There is a text (telephone conversations). Voice is already converted into text and is divided into agent and customer paragraphs. I need to understand what approach is the best one for the next tasks:</p>

<ol>
<li>Who is the customer and who is the agent?</li>
<li>Customer Name</li>
<li>The topic of conversation</li>
<li>Promises made by the operator to the customer (for example, ""I call back tomorrow"")</li>
<li>Negative Sentiment (if there is something in the conversation that the subscriber is not happy with)</li>
</ol>

<p>I am just trying to understand how to handle it. Is it possible to create some kind of general approach for this? If yes, for which packages/publications/books could I pay my attention?</p>
","nlp"
"65067","Proper masking in the transformer model","2019-12-18 11:18:32","65070","9","7582","<nlp><word-embeddings><transformer>","<p>Concerning the transformer model, a mask is used to mask out attention scores (replace with 1e-9) prior to the matrix multiplication with the value tensor. Regarding the masking, I have 3 short questions and would appreciate if you could clarify those:</p>

<ol>
<li>Are the attention scores the only place (besides the loss) where masks are needed or should the input be masked out as well? </li>
</ol>

<p>I am asking because is see implementations where a linear layer for the query, key and values is with <code>bias=False</code> is used. </p>

<ol start=""2"">
<li><p>Is the reason for setting <code>bias=False</code> to have zeros preserved in the output of the layers or is there a different explanation?</p></li>
<li><p>Should a padding_idx be used when learning word embeddings in order to zero out the padded tokens? </p></li>
</ol>
","nlp"
"64961","What are x variable and y variable in word2vec model if it is supervised learning","2019-12-17 08:05:23","","1","150","<deep-learning><word2vec><nlp>","<p>What are x variable and y variable in word2vec model if it is supervised learning.
In both the flavours- CBOW and skip-gram model.</p>

<p>Though some blogs have explained it as unsupervised learning.</p>

<p>Thanks</p>
","nlp"
"64891","Using a model to generate training data for another model","2019-12-16 00:04:14","","1","192","<machine-learning><nlp>","<p>I have a text classification problem. Lets say I need to classify text into labels A,B,C &amp; D.</p>

<p>I have two different approaches and want to understand the difference:</p>

<p>Approach 1: Traditional approach of proving labeled training data to a single label classifier model and training it</p>

<p>Approach 2: </p>

<p>Step 1: Building 4 different models, one for each label, that predicts the likelihood of that label (Yes/No). This will use the same training data is Approach 1 split in 4 ways, one for each label, with additional annotations for ""No"" label.</p>

<p>Step 2: Using the above models to generate labeled training data with tens of thousands of rows with 99.9% accuracy (discard any text where the previous model has low score), and all four labels A,B,C,D</p>

<p>Step 3: Use the generated data to train new model to predict the label for a given text. </p>

<p>I'm new to NLP so would like to understand, is Approach #2 going to be better than #1 in terms of precision and recall? I assume it will be, but what doesn't make sense intuitively is that Approach 2 is using the same human labeled data as input as Approach 1 (with exception of some additional data in Step 1 for the ""No"" Labels. Is the accuracy of the models going to be much different? </p>
","nlp"
"64883","What's wrong with RF/SVM with word embedding (GloVe)?","2019-12-15 17:12:05","64980","0","556","<keras><word-embeddings><nlp><ensemble-modeling><gensim>","<p>I searched many times in google for examples on word embedding (specifically GloVe) with Random forest and I couldn't find any single example. For GloVe, it was all either LSTM or CNN. Maybe there's something I don't understand, but why only these 2 algorithms are used with GloVe, what's wrong with the other algorithms?  </p>
","nlp"
"64714","Would Topic Modelling be classified as NLP or NLU?","2019-12-12 17:03:11","64716","5","307","<nlp><word-embeddings><topic-model>","<p>I recently started my journey into the world of NLP, it's been one heck of a ride. I'm currently trying to understand whether topic modelling would be considered as NLP or NLU.</p>

<p>Initially I would assume that topic modelling would be classified as NLP. However, if we use word embeddings for topic modelling wouldn't it then be classified as NLU, as we have deeper understanding of how the words relate to each other in vector space? </p>

<p>Maybe I'm having trouble formulating the inherent difference between NLP and NLU, when do we draw the line between the two?</p>

<p>Your insight regarding this matter would be highly appreciated.</p>
","nlp"
"64685","Is there any NLP framework/ algorithm which is able to derive relationships between different entities?","2019-12-12 10:21:52","","2","58","<nlp>","<p>Is there any NLP framework/ algorithm which is able to derive relationships between different entities?</p>

<ul>
<li>Dogs bark</li>
<li>For example, if the system can identify and understand a phrase, 'Dogs were barking loudly' and can relate to another phrase, 'Usually dogs bark loud, when they sense danger'. And from this piece of information, the system can imply/ derive: 'Dogs had sensed danger' - derived sentence.</li>
</ul>

<p>Can BERT help in identifying and implying?</p>
","nlp"
"64684","Can BERT/ELMo be used (or retrained) to generate a text in both directions?","2019-12-12 09:39:29","","0","877","<nlp><bert><text-generation>","<p>Text generation is perhaps one of the fun things to do with old NGram or new BERT/ELMo models. I am wondering can BERT be used to generate text from the end of a sentence, or better in both directions. That is instead of giving some starting words, we give some ending words. </p>
","nlp"
"64658","R train(method=""naive_bayes"") and naiveBayes() very different performance","2019-12-11 16:13:37","","2","164","<nlp><r><cross-validation><naive-bayes-classifier>","<p>I am an R novice and having some difficulty. I was hoping R would be a good (flexible, easy) way to do machine learning of textual data.</p>

<p>A few years ago, I wrote a naive Bayesian classifier (from scratch) in Perl and I'm trying to replicate it in R (in order to extend it using all the cool toys R enables). My Perl script uses LOOCV and achieves an overall hit rate of 69.4%. Using equivalent pre-processing steps in R with naiveBayes() from the e1071 package with an 80-20 split I get hit rate of 60.3%. I can imagine this discrepancy is due to different crossvalidation methodologies.</p>

<p>Now, I am trying to find a way to use LOOCV to do an apples-to-apples comparison between my perl script and R. I'm using train() (from package ""caret"") with method=""naive_bayes"" (from a package called ""naivebayes""). The code is below. No matter what I do, the results are <strong>horrible</strong> (chance-level, even worse than a no information model that exploits the baserates). Clearly either I'm doing something wrong or there is a bug; my prior p=0.999 that it's the former. I'm hoping one of you can give me a clue. My code is below.</p>

<p>The other aspect of performance is speed. Perl is interpreted like R and runs the LOOCV on a sample of 655 in a few seconds (so perhaps 163 N-1 analyses/second). R runs a single naive bayesian analysis very quickly, but train() takes the better part of an hour to run.</p>

<p>This code works well:</p>

<pre><code>classifier &lt;- naiveBayes(dtm_train,data_train<span class=""math-container"">$class)
pred &lt;- predict(classifier, newdata=dtm_test)
#create a confusion table 
table(""predictions""=pred, ""actual""=actual_classes$</span>class)
</code></pre>

<p>This is an example of one way of using train that doesn't work well:</p>

<pre><code>train_control &lt;- trainControl(method=""LOOCV"",verboseIter = TRUE)

model5 &lt;- train(class~., data=dtm_df, 
                trControl=train_control, 
                method=""naive_bayes"",
                verbose = TRUE
                )
print(model5)
</code></pre>
","nlp"
"64651","Text embeddings and data splitting","2019-12-11 14:17:50","64677","2","1612","<nlp><dataset>","<p>I have created some document embeddings which were then used further in text classification tasks. After revisiting my code I was unsure about the workflow I used to train the document embeddings.</p>

<p>At the moment I am creating the document embeddings based on the complete corpus available at the time of training. After the training is done, I evaluate the model by looking whether it creates useful similarities between the document embeddings. Those embeddings are used then in machine learning models and that's where the embeddings will be split into train, test and validation sets.</p>

<p>Now my question is: Where is the right time to split the data? Should I do it before creating the document embeddings to prevent data leakage? I have used the mentioned approach because I viewed the creation of the document embeddings as a preprocessing step, so the computer can work with textual data. However, after I have put some thought into it, I think it's the wrong approach. I wanted to hear from more experienced NLP practitioners how they approach this task. Sorry for this very basic question.</p>

<p>Thanks.</p>
","nlp"
"64542","NLP: Getting the top 5 or top 10 predictions","2019-12-10 10:12:05","","0","79","<nlp><text-mining><data-science-model><social-network-analysis>","<p>I am working on a social networking application and I have to make its news feed better. For example: If someone searches for 'suggest me some good books',  it should yield some names. </p>

<p>Now, I have used the Infersent algo (to begin with) in order for my model to be able to answer questions. </p>

<p>I am getting only the best output that my model could predict viz., 'Alchemist'.</p>

<p>I want at least 4 or 5 other outputs, other words, the top five predictions.</p>

<p>I know that Xgboost has the ability to do this activity in some sorts, but I am not sure how I should use that in my problem.</p>

<p>Any heads up?</p>

<p>My apologies, I cannot share any code but I would really appreciate ideas and suggestions.</p>

<p>Thank you,</p>
","nlp"
"64525","Are there some research papers about text-to-set generation?","2019-12-10 03:13:26","64607","0","70","<deep-learning><nlp><generative-models><generalization>","<p>I have googled but find no results.</p>

<p>Text-to-(word)set generation or sequence-to-(token)set generation.</p>

<p>For example, input a text and then output the tags for this text:</p>

<p><code>'Peter is studying English'</code> --> <code>{'good behavior','person','doing something'}</code></p>

<p>Thank you!</p>
","nlp"
"64466","group the similar words","2019-12-09 09:49:25","","1","106","<machine-learning><deep-learning><classification><nlp><word-embeddings>","<blockquote>
  <p>array(['Ruby on Rails', 'Ruby', 'AWS DynamoDB', 'Python', 'MySQL',
         'Swift', 'Android', 'iOS', 'JavaScript', 'React Native', 'ReactJS',
         'TypeScript', 'Vue.js', 'Webpack', 'Amazon Web Services(AWS)',
         'Kubernetes', 'PHP', 'CI/CD', 'Java', 'C#', 'C', 'Node.js',
         'REST API', 'Go', 'Redux-Saga', 'Redux.js', 'Babel', 'GraphQL',
         'Tensorflow', 'PyTorch', 'Jenkins', 'Spring', 'Django', 'Git',
         'AWS EC2', 'CSS', 'HTML', 'MongoDB', 'Docker', 'Scala', 'SQL',
         'Embedded System', 'NLP', 'Apache', 'Kotlin', 'Angular', 'jQuery',
         'C++', 'RxJS', 'AngularJS', 'Redis', 'Next.js', 'NoSQL',
         'GCP(Google Cloud Platform)', 'Elasticsearch', 'OpenStack',
         'JPA(Java Persistent API)', 'TCP/IP', 'Objective-C', 'Realm',
         'Firebase', 'Ajax', 'Linux', 'PostgreSQL', 'ES6', 'AWS Lambda',
         'HTML5', 'AWS S3', 'GitHub', 'RxSwift', 'Terraform', 'AWS EKS',
         'AWS RDS', 'Microsoft Azure', 'Sass(SCSS)', 'CodeIgniter', 'Flask',
         'Nuxt.js', 'Ansible', 'Spring Boot', 'Linux kernel',
         'Apache Kafka', 'Deep Learning', 'Nginx', 'ActionScript', 'OOP',
         'Shell', 'gulp', 'Celery', 'SQLAlchemy', 'ExpressJS', 'RxJava',
         'Apache Spark', 'WebGL', 'OpenGL', 'Machine Learning',
         'MSSQL(Microsoft SQL Server)', 'Database', 'styled-components',
         'MVC', 'Retrofit', 'Machine Vision', 'Oracle', 'web3.js', 'R',
         'AWS ElasticBeanstalk', 'Elastic Stack', 'Laravel', 'ASP.NET',
         'Aurora DB', 'Redux-Observable', '.NET', 'AWS Backup',
         'AWS CloudWatch', 'Kibana', 'Fluentd', 'Logstash', 'JSP',
         'Bootstrap', 'Datadog', 'Rust', 'Azure', 'Apache Hadoop',
         'AWS X-Ray', 'Memcached', 'Jest', 'Mocha',
         'DRF(Django REST framework)', 'Spring Cloud', 'Data Analysys',
         'Big Data', 'GitLab', 'Gradle', 'SQLite', 'Microsoft IIS', 'Unity',
         'Electron', 'MariaDB', 'mSQL', 'gensim', 'Scikit-Learn',
         'AWS Simple Queue Service(AWS SQS)', 'gRPC',
         'Naver Cloud Platform', 'Ubuntu', 'Microservice Architecture',
         'Apache ActiveMQ', 'Oracle Database', 'Apache Subversion(SVN)',
         'Apache Tomcat', 'Red Hat Ceph Storage', 'Puppeteer', 'OpenLayers',
         'Vuex', 'Less.js', 'JIRA', 'Keras', 'NCP(Naver Cloud Platform)',
         'NestJS', 'PKI(Public key infrastructure)', 'AWS ECS', 'Hibernate',
         'UML', 'BitBucket', 'Arduino', 'Raspberry Pi', 'RabbitMQ',
         'Capistrano', 'Bamboo', 'MVP', 'OkHttp', 'Cocos2d', 'Ethereum',
         'Blockchain', 'DSP(Digital Signal Processing)', 'D3.js', 'Cocoa',
         'Axios', 'Ionic', 'WPF', 'AWS IAM', 'Shell Script',
         'Responsive Web', 'Canvas', 'ThreeJS', 'Apache ZooKeeper',
         'Pandas', 'Spring Batch', 'JUnit', 'Spring Data JPA', 'ASP',
         'Grunt', 'WordPress', 'MyBatis', 'AWS ElastiCache',
         'Apache HTTP Server', 'AWS Security Hub', 'Google API', 'Qt',
         'CAD', 'GatsbyJS', 'PostCSS', 'Socket.IO', 'Backbone.js',
         'Azure Linux Virtual Machines', 'Heroku', 'CUDA', 'IOCP', 'Unix',
         'CocoaPods', 'MVVM(Model-View-ViewModel)',
         'Google Firebase Crashlytics', 'Google Cloud Platform',
         'Windows kernel', 'OpenCV', 'Unreal Engine', 'Google Cloud SDK',
         'RxAndroid', 'Windows Embedded', 'Entity Framework', 'Packer',
         'Nexus', 'Consul', 'Selenium', 'Jekyll', 'XML',
         'Dependency Lookup', 'RxKotlin', 'Expo', 'Sketch', 'InVision',
         'Azure Text Analytics', 'Google Dialogflow',
         'Google Cloud Natural Language'], dtype=object)</p>
</blockquote>

<p>let say I have this array list <br/>
if I want to group them by similar things 
is there any pretrained language model to do this job easier? </p>

<p><br/></p>

<p>for example  <br/>
pytorch and tensorflow should be in one group because 
most of the deep learning people are using pytorch or tensorflow <br/></p>
","nlp"
"64444","Difficulty interpreting word embedding vector similarity (spaCy)","2019-12-09 00:10:55","64467","2","368","<python><nlp><word-embeddings><spacy>","<p>I calculate vector similarities like this:</p>

<pre><code>nlp = spacy.load('en_trf_xlnetbasecased_lg')
a = nlp(""car"").vector
b = nlp(""plant"").vector
dot(a, b)/(norm(a)*norm(b))
0.966813
</code></pre>

<p>Why are the vector similarities so high for unrelated words for the embedding? This is not the only pair for which they are abnormally high. I also had a similar experience with fastText, so I am wondering, am I misunderstanding something?</p>

<p>Also I am able to get vectors for non-words like ""asdfasfdasfd"" or ""zzz123Y!/§zzzZz"", and they differ from each other. How is this possible?</p>
","nlp"
"64427","NLP and one-class classifier building","2019-12-08 15:49:45","64440","3","1390","<python><classification><svm><nlp>","<p>I have a big dataset containing almost 0.5 billions of tweets. I'm doing some research about how firms are engaged in activism and so far, I have labelled tweets which can be clustered in an activism category according to the presence of certain hashtags within the tweets.</p>

<p>Now, let's suppose firms are tweeting about an activism topic without inserting any hashtag in the tweet. My code won't categorized it and my idea was to run a SVM classifier with only one class.</p>

<p>This lead to the following question:</p>

<ul>
<li>Is this solution data-scientifically feasible?</li>
<li>Does exists any other one-class classifier? </li>
<li>(Most important of all) Are there any other ways to find if a tweet is similar to the ensable of tweets containing activism hashtags?</li>
</ul>
","nlp"
"64355","Difference between packaged sentiment analysis tools (TextBlob/NLTK) and training your own classifier?","2019-12-06 20:13:09","64390","1","781","<machine-learning><nlp><sentiment-analysis><nltk><classifier>","<p>I'm new to ML and training classifiers in practice, so I was just wondering what the difference was between the built-in sentiment tools of packages such as NLTK and TextBlob as compared to manually creating a classifier (training, testing, etc). I think I read in a comment somewhere that Textblob/NLTK's existing sentiment analysis tools basically just tokenize the text and count the number of positive/negative words to determine an overall sentiment rating (not sure how accurate this is). Does anyone know if using a custom classifier would, in general, be a better way to doing sentiment analysis of text (I'm looking at analyzing the sentiments expressed in hotel reviews)?</p>
","nlp"
"64323","How to load the pre-trained BERT model from local/colab directory?","2019-12-06 10:59:04","64513","5","28704","<nlp><bert>","<p>Hi i downloaded the BERT pretrained model (<a href=""https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip"" rel=""noreferrer"">https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip</a>) from here and saved to a directory in gogole colab and in local .</p>

<p>when i try to load the model in colab im getting ""We assumed '/content/drive/My Drive/bert_training/uncased_L-12_H-768_A-12/config.json"" . tried to laod the model in local machine and getting same error .</p>

<p>this is how i loaded the model:
from transformers import BertForMaskedLM
BertNSP=BertForMaskedLM.from_pretrained('/content/drive/My Drive/bert_training/uncased_L-12_H-768_A-12/')</p>

<p>is this the correct way of loading model from the directory when i have downloaded the pretrained model ?
Im getting error "" '/content/drive/My Drive/bert_training/uncased_L-12_H-768_A-12/config.json' ""
the downloaded model had these naming conventions where file name start with bert_ but the BertForMaskedLM class is expecting the file name to be config.json .</p>

<p>bert_config.json
bert_model.ckpt.data-00000-of-00001
bert_model.ckpt.index vocab.txt
bert_model.ckpt.meta</p>

<p>FULL ERROR:
Model name '/content/drive/My Drive/bert_training/uncased_L-12_H-768_A-12/' was not found in model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). We assumed '/content/drive/My Drive/bert_training/uncased_L-12_H-768_A-12/config.json' was a path or url to a configuration file named config.json or a directory containing such a file but couldn't find any such file at this path or url.</p>

<p>when i renamed the above 4 files by removing bert from all 4 file names , i get this error even though the ""model.ckpt.index"" files exist </p>

<p>ERROR:
""OSError: Error no file named ['pytorch_model.bin', 'tf_model.h5', 'model.ckpt.index'] found in directory /content/drive/My Drive/bert_training/uncased_L-12_H-768_A-12/ or from_tf set to False""</p>
","nlp"
"64197","where to start in natural language processing for a language","2019-12-04 11:05:47","","2","175","<nlp><machine-translation><speech-to-text>","<p>My native language is a regional language and few people speak it. I have some assignements in a machine learning course and i was thinking about doing some natural languge processing on my native language but i don't know where to start since there is almost no research about this language ( no corpus , no research papers , ... ) and i'm new to machine learning.</p>

<p>I want to start doing everything from bottom and i want to do things right , can you please guide me to steps i should follow?</p>

<p>I also want to build my own corpus ,since it's a very tiring work , is there a way to build a single corpus that can be used on several NLP applications ( at least for translation and speach recognition)  ?</p>
","nlp"
"64136","Detect if word is «common English» word or slang word","2019-12-03 08:24:23","64137","2","1744","<python><nlp><nltk>","<p>I have a huge list of short phrases, for example:</p>

<pre><code>sql server data analysis # SQL is not a common word
bodybuilding # common word
export opml # opml is not a common word
best ocr mac # ocr and mac are not common words
</code></pre>

<p>I want to detect if word is not a common word and should not be processes further.</p>

<p>I've tried to do this with NLTK, but it gives strange results:</p>

<pre><code>result = word in nltk.corpus.words.words()
</code></pre>

<pre><code>sql = false
iso = true
mac = true
</code></pre>

<p>Is there a better way to do this?</p>
","nlp"
"64029","Chunking Sentences with Spacy","2019-11-30 15:53:24","64045","6","523","<machine-learning><nlp><nltk><spacy>","<p>I have a lot of sentences (500k) which looks like this:</p>

<pre><code>""Penalty missed! Bad penalty by Felipe Brisola  - Riga FC -  shot with right foot is very close to the goal. Felipe Brisola should be disappointed.""
""Penalty saved! Damir Kojasevic  - Sutjeska Niksic -  fails to capitalise on this great opportunity,  shot with right foot saved  in the centre of the goal.""   
""Penalty saved! Stefan Panic  - Riga FC -  fails to capitalise on this great opportunity,  shot with right foot saved  in the centre of the goal.""
""Penalty saved! Georgie Kelly  - Dundalk -  fails to capitalise on this great opportunity,  shot with right foot saved  in the centre of the goal.""
""Penalty missed! Still  FC København 1, Crvena Zvezda 1. Marko Marin  - Crvena Zvezda -  hits the bar with a shot with right foot.""
</code></pre>

<p>As you see, they are not really robotic, and after ending up writing 1500 lines of php code (with regex) and still being inconsistent, I decided to see my alternatives with machine learning.</p>

<p>What I am trying to achieve is:</p>

<pre><code>For example this one:

""Penalty saved! Stefan Panic  - Riga FC -  fails to capitalise on this great opportunity,  shot with right foot saved  in the centre of the goal.""

type =&gt; penalty
action =&gt; saved
reason =&gt; shot with right foot saved  in the centre of the goal
person =&gt; Stefan Panic
</code></pre>

<p>I stumbled upon spaCy and saw ""Named Entity Recognition"" and thought maybe I can use it for this purpose. Especially as I have huge training data.</p>

<p>I wanted to ask: Is spaCy's Named Entity Recognition is right for this task? If not, what should I try to learn for this task?</p>

<p><em>P.S: I know a little about python but nothing about ML</em></p>
","nlp"
"64001","Annotating the vocabulary using Word2vec model","2019-11-29 16:00:51","","2","123","<machine-learning><nlp><gensim>","<p>I am trying to label the vocabulary in the corpus.</p>

<ol>
<li><p>I have trained the word2vec model on the corpus</p></li>
<li><p>I have grouped the words which are related based on the score as key as the first word as the key and remaining words as a list of 2-tuple of word and scores with respect to the key </p></li>
</ol>

<p>example:
'coffee'---key 
values are </p>

<pre><code>[('tea', 0.8139282),
 ('latte', 0.76456803),
 ('coffe', 0.7607962),
 ('lattes', 0.756057),
 ('starbucks', 0.7158153),
 ('espresso', 0.71386236),
 ('mocha', 0.69999266),
 ('coffees', 0.6816252),
 ('frappucino', 0.67192864),
 ('cuppa', 0.66720986),
 ('cappucino', 0.6664002),
 ('chai', 0.6623157),
 ('decaf', 0.65980726),
 ('frappuccino', 0.65150374),
 ('venti', 0.6486204),
 ('expresso', 0.6369579),
 ('macchiato', 0.6280453),
 ('scone', 0.62476856),
 ('sippy', 0.6236704),
 ('cappuccino', 0.61718297),
 ('iced', 0.6130485),
 ('hazelnut', 0.6023698),
 ('mug', 0.6004759),
'
'
'
'
'
</code></pre>

<p>as i  know the coffee is releated to latte ,green_tea ,espresso,starbucks.. from the above data
I would like to label each word as below </p>

<p>latte [COHYPO] green_tea [COHYPO] espresso [HYPO] Starbucks [RELATED] tim_horton [RELATED] </p>

<p>COHYPO-<a href=""https://en.wiktionary.org/wiki/cohyponym"" rel=""nofollow noreferrer"">https://en.wiktionary.org/wiki/cohyponym</a></p>

<p>[HYPO] -<a href=""https://en.wiktionary.org/wiki/hyponyme"" rel=""nofollow noreferrer"">https://en.wiktionary.org/wiki/hyponyme</a></p>

<p>[RELATED] -the word is repeated </p>

<p>[MORPHO]-Morphological variant (example :Computer and computers )</p>

<p>[Partof]- indicates that the annotated word is a part of the word of interest</p>

<p>Any suggestion or ideas by which I can approach this problem </p>
","nlp"
"63959","why an advanced LSTM model produce the same results as a simpler one?","2019-11-29 00:34:20","","3","126","<deep-learning><classification><nlp><lstm><rnn>","<p>I have implemented the model proposed in <a href=""https://www.aclweb.org/anthology/D15-1167/"" rel=""nofollow noreferrer"">this</a> article which is a text classification model that uses sentence representation rather than only word representation to classify texts.</p>

<pre><code>model=tf.keras.Sequential()
embeding_layer=layers.Embedding(self.vocab_size,self.word_vector_dim,weights=[word_embeding_matrix],trainable=False,mask_zero=False)
model.add(TimeDistributed(embeding_layer))
model.add(TimeDistributed(tf.keras.layers.LSTM(50)))
model.add(tf.keras.layers.Bidirectional(costumized_lstm.Costumized_LSTM(50)))
model.add(layers.Dense(6,activation='softmax'))
opt=tf.keras.optimizers.Adam(learning_rate=0.001)
model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['accuracy',self.f1_m,self.precision_m, self.recall_m])
self.model=model
</code></pre>

<p>and I use a dataset with 40000 documents with 6 different labels to train it. (30000 for train and 10000 for the test). I uses a pretrained word embeding and the input for this model is (sample,sentences,words). it achieves 84% accuracy. the problem is that I can achieve this accuracy very easily with this simple model:</p>

<pre><code>        model=tf.keras.Sequential()
    embeding_layer=layers.Embedding(self.vocab_size,self.word_vector_dim,weights=[word_embeding_matrix],trainable=False,mask_zero=False)
    model.add(embeding_layer)
    model.add(tf.keras.layers.Bidirectional(layers.LSTM(50)))
    model.add(layers.Dense(6,activation='softmax'))
    opt=tf.keras.optimizers.RMSprop(learning_rate=0.001)
    model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['accuracy',self.f1_m,self.precision_m, self.recall_m])
    self.model=model
</code></pre>

<p>this one is not based on sentence representation and the input for this model is (sample, words).
what is the first model ? is my implementation wrong? what should I do? </p>

<p>the training process for both models is as below picture. I also have used every trick to overcome overfitting but I haven't got any results. any suggestions please?
<a href=""https://i.sstatic.net/cETh2.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/cETh2.jpg"" alt=""enter image description here""></a></p>
","nlp"
"63928","where can i find the algorithm of these papers?","2019-11-28 11:45:03","63935","0","32","<nlp>","<p>I am reading about clinical NER</p>
<p>I found 2 papers talking about it</p>
<p><a href=""https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5977567/pdf/2731659.pdf"" rel=""nofollow noreferrer"">Paper 1</a>
and
<a href=""https://academic.oup.com/jamia/article/18/5/601/834186"" rel=""nofollow noreferrer"">Paper 2</a></p>
<p>They are talking about algorithms and ML has been used to approach clinical NER.</p>
<p>I could not find anywhere on how exactly these algorithms are implemented in these papers.</p>
<p>Can anyone help me find it or build it?</p>
","nlp"
"63882","Autonomous learning - chatbots","2019-11-27 19:06:27","","2","23","<nlp><reinforcement-learning><chatbot>","<p>My chatbots need to be trained when we get new data or feedbacks from users. Can someone provides ways how these chatbots can learn on themselves and become intelligent day by day?<br>
Some of techniques may be :</p>

<ul>
<li>Reinforcement learning - though I could not find a successful implementation and have asked a separate question. Any successful implementation reference would be helpful.</li>
<li>Building a pipeline with a user feedback - this has 2 drawbacks. One user or admin has to provide the correct answer. Second, a single feedback data may not be enough to retrain the bot.</li>
</ul>

<p>Any expert advice on - self optimizing chatbot architecture OR some code reference OR some paper trying to solve this problem (a paper with code may be)?</p>
","nlp"
"63880","Reinforcement Learning in NLP for chatbots","2019-11-27 18:57:12","","0","187","<nlp><reinforcement-learning>","<p>Is anyone aware of any successful implementation of reinforcement learning for NLP. I am looking to for chatbots which can learn automatically.  </p>

<p>Tried searching internet but found very few articles like <a href=""https://medium.com/@vgpiyer/reinforcement-learning-for-natural-language-processing-part-1-5105ce2a1025"" rel=""nofollow noreferrer"">Reinforcement Learning For Natural Language Processing - Medium</a> or papers like <a href=""https://arxiv.org/abs/1906.03926"" rel=""nofollow noreferrer"">A Survey of Reinforcement Learning Informed by Natural Language</a>.  </p>

<p>But none of them provides a robust code which shows that it is working. Kindly suggest.</p>
","nlp"
"63839","How to do Named Entity Recognition in Tables?","2019-11-27 09:42:22","","2","869","<nlp><named-entity-recognition>","<p>What are approaches to do Named Entity Recognition on Tables? I am referring to tables which have a column header and the corresponding information in the cells below the header, or the respective with line headers, not tables which contain full sentences. Since these NER lack the context of a sentence and are sort of determined by a header, the approaches I found (most depend on context) do not really work.</p>
","nlp"
"63813","Datasets for Topic Modeling","2019-11-26 19:07:31","","1","3320","<nlp><dataset><text-mining><topic-model>","<p>I'm looking to try and use deep learning methods for topic modeling as opposed to the more traditional methods of lda and word embedding methods. However, I'm having trouble finding good labeled datasets for this task.  So far the best that I've seen is the <a href=""https://catalog.ldc.upenn.edu/LDC2008T19"" rel=""nofollow noreferrer"">New York Times Dataset</a> which I can't use due to licensing constraints. I've also seen the <a href=""http://qwone.com/~jason/20Newsgroups/"" rel=""nofollow noreferrer"">20News Dataset</a> but it only has twenty categories so it probably won't scale well to other domains.</p>

<p>Are there any other good datasets out there that I'm missing that can be used for topic modeling?  I'm happy to use a dataset that isn't explicitly meant for topic modeling; as long as it has some sentences/paragraphs that are tagged or labeled that should be fine.</p>
","nlp"
"63790","Definition of a ""lexicon"" in Named Entity Recognition","2019-11-26 11:51:39","","1","176","<nlp><named-entity-recognition>","<p>I am writing a paper on Named Entity Recognition and I mention that in the literature were proposed a lot of methods which make use of lexicons. However, I am struggling to find a definition of a ""lexicon"" which I can cite. As far as I understand, a lexicon is a list of lemmatas (word parts), whole words or word combinations where these are classified to an entity. So, for example if in a sentence i have ""New York is located in"" and in my lexicon I have the pattern "" X is a located in"" -> CITY my system will look up the pattern and see that New York belongs to the entity CITY. 
Could someone explain to me if I am wrong? If so, I would like to get a definition of a ""lexicon"", ""gazzetteer"" and ""dictionary"", because I get the feeling that some of these are sometimes used interchangeably in papers.   </p>
","nlp"
"63763","Has this method of NLP processing with neural networks been done?","2019-11-26 01:11:54","","1","19","<neural-network><nlp>","<p>Take the sentence:</p>

<pre><code>If you see a green light then you may cross the road.
</code></pre>

<p>I propose a neural network which produces as output from this sentence 3 masks  and a classifier as follows:</p>

<pre><code>input   : If you see a green light then you may cross the road.
output 1: XX                       XXXX
output 2:    XXXXXXXXXXXXXXXXXXXXX
output 3:                               XXXXXXXXXXXXXXXXXXXXXXX
output 4: [if-then-statement]
</code></pre>

<p>So basically we train the neural network on a corpus to split the sentence into it's most general structure. Then we take the masks and use it on the input to produce a new phrase. e.g. let's take the mask from output 2:</p>

<pre><code>input   : you see a green light
output 1:     xxx
output 2: xxx
output 3:         xxxxxxxxxxxxx
output 4:[event]
</code></pre>

<p>This time the neural network classifies the input as an event and creates masks for splitting the sentence into subject-object-verb.</p>

<p>So we repeat the process using the masks created to change the input until we have parsed the sentence into a tree-like representation. We use the same neural network each time.</p>

<p>If it fails to find a classification for an input it could go back up one level and try a different classification. (Much like a human might do if it doesn't quite understand a sentence).</p>

<p>The result would be an internal representation which might consist of a tree-like structure of masks and classifications. Which might then be more useful for reasoning about the meaning of the sentence.</p>

<p>This is kind of a top-down approach. I'm not sure how it would learn these masks from un-marked speech, but certainly I think it could learn them from a tagged corpus. I think maybe a similar approach might be used backwards to generate sentences.</p>

<p>Has this sort of thing been tried before?</p>
","nlp"
"63746","NLP Transformers: How to get a fixed sentences embedding vectors size?","2019-11-25 15:13:00","","4","1250","<machine-learning><deep-learning><nlp><word-embeddings><bert>","<p>I'm loading a language model from torch hub (<a href=""https://camembert-model.fr/#about"" rel=""nofollow noreferrer"">CamemBERT</a> a French RoBERTa-based model) and using it do embed some sentences:  </p>

<pre class=""lang-py prettyprint-override""><code>import torch
camembert = torch.hub.load('pytorch/fairseq', 'camembert.v0')
camembert.eval()  # disable dropout (or leave in train mode to finetune)


def embed(sentence):
   tokens = camembert.encode(sentence)
   # Extract all layer's features (layer 0 is the embedding layer)
   all_layers = camembert.extract_features(tokens, return_all_hiddens=True)
   embeddings = all_layers[0]
   return embeddings

# Here we see that the shape of the embedding vector depends on the number of tokens in the sentence

u = embed(""Bonjour, ça va ?"")
u.shape # torch.Size([1, 7, 768])
v = embed(""Salut, comment vas-tu ?"")
v.shape # torch.Size([1, 9, 768])

</code></pre>

<p>Imagine now, I want to calculate the <code>cosine distance</code> between the vectors (tensors in our case) <code>u</code> and <code>v</code> : </p>

<pre class=""lang-py prettyprint-override""><code>cos = torch.nn.CosineSimilarity(dim=0)
cos(u, v) #will throw an error since the shape of `u` is different from the shape of `v``
</code></pre>

<p>I'm asking what is the best method to use in order to always get the <strong>same embedding shape</strong> for a sentence regardless the count of tokens?</p>

<p>=> The first solution I'm thinking of is calculating the <code>mean on axis=1</code> (mean embedding of tokens in the sentence) since axis=0 and axis=2 have always the same size :</p>

<pre class=""lang-py prettyprint-override""><code>cos = torch.nn.CosineSimilarity(dim=1) #dim becomes 1 now

u = u.mean(axis=1)
v = v.mean(axis=1)

cos(u, v).detach().numpy().item() # works now and gives 0.7269
</code></pre>

<p>But, I'm afraid that I'm hurting the embedding when calculating the mean! </p>

<p>=> The second solution is to pad shorter sentences out, that means:  </p>

<ul>
<li>giving a list of sentences to embed at a time (instead of embedding sentence by sentence)</li>
<li>look up for the sentence with the longest tokens and embed it, get its shape <code>S</code></li>
<li>for the rest of sentences embed then pad zero to get the same shape <code>S</code> (the sentence has 0 in the rest of dimensions)</li>
</ul>

<p>What are your thoughts? What technique would you use and why? </p>
","nlp"
"63731","Transfer learning between Language Model and classification","2019-11-25 11:29:15","","1","141","<classification><nlp><transfer-learning><language-model>","<p>Following this fast.ai <a href=""https://www.youtube.com/watch?v=5gCQvuznKn0&amp;list=PLtmWHNX-gukKocXQOkQjuVxglSDYWsSh9&amp;t=2757s"" rel=""nofollow noreferrer"">lecture</a>, I am trying to understand the mechanism of Transfer Learning in NLP from a general Language Model (LM) to a classification problem.</p>

<p>What is exactly taken from the Language Model training? Is it just the word embeddings? Or is it also the weights of the LSTM cell? The architecture of the neural net should be quite different - where in a LM you would output a prediction after every sequence-step, in a classification problem you would only care about the output of the final sequence step. </p>

<p>(I would happy to know what is the general practice, and also if anyone knows how fast.ai does it)</p>
","nlp"
"63600","Is it possible to generate syllogisms using an NLP algorithm?","2019-11-22 12:39:27","63614","4","141","<machine-learning><deep-learning><nlp>","<p>I want to build a tool that generates sensible syllogisms. An example of a syllogisms is: all A are B. all C are A. all C are B). I want the triplet (A, B, C) to be semantically related to each other in the way described by the syllogisms. That is, I would like my tool to generate 'a=humans; b=mortal; c=greeks' and not 'a=chickens; b=burgers; c=frogs'.</p>

<p>There's a syllogism generator online (<a href=""http://krypton.mnsu.edu/~jp5985fj/courses/609/Logic/Silly%20Syllogisms.htm"" rel=""nofollow noreferrer"">http://krypton.mnsu.edu/~jp5985fj/courses/609/Logic/Silly%20Syllogisms.htm</a>) but it doesn't generate syllogisms that are semantically related, it generates random terms for A, B, and C which may or may not be related.</p>

<p>My question is, in NLP, are there any research papers for generating semantically valid syllogisms? What topics would I need to research to build this tool?</p>
","nlp"
"63590","Vectorize One line text data","2019-11-22 10:13:21","","1","63","<nlp><tfidf>","<p>How to vectorize one-line text data? I have used tf-idf including bigrams and trigrams but I am not able to get good results. 
I have purchase order descriptions which are one-liners and I need to classify. 
It is a multi-class imbalanced data and I have a small dataset to train around 700 PO descriptions. The number of classes is 7 and the class distribution is similar to exponential. One class is dominating. 
My take is that TF IDF should not work since the term frequency and the IDF frequency will be very small. 
Also, can we make some user-defined functions to create vectors? If yes, what should be it?</p>

<p>Please suggest some alternative approaches as  well.</p>
","nlp"
"63552","Populating Knowledge Base - Stanford DeepMind Alternatives","2019-11-21 15:50:47","","1","24","<python><nlp><text-mining><information-retrieval>","<p>I am dealing with the task to extract structured information from domain-specific unstructured documents. The end goal is to obtain a reliable, queryable system, i.e. in the form of a chat-bot or Question-Answering application. </p>

<p>During my research I formulated following solution approach:</p>

<ol>
<li><p>Parse doc, docx and pdf documents to raw text. Pre-process with common NLP tools. </p></li>
<li><p>Consult with client and decide upon several categories for entities. </p></li>
<li><p>Train customised NER model to obtain entities from texts.</p></li>
<li><p>Obtain entity-level-relations.</p></li>
<li><p>Populate knowledge base</p></li>
<li><p>Migrate knowledge base data to knowledge graph and set up the queryable system. </p></li>
</ol>

<p>For the implementation, I was looking into <a href=""http://deepdive.stanford.edu/"" rel=""nofollow noreferrer"">Stanfords Deepdive</a> architecture, however they stopped support in 2017 and I am not sure if it is still up to date. </p>

<p>Alternatively, I intended to use spaCy to train my own NER model, but it is unclear to me if this allows to obtain the entity-level-relations for populating a knowledge base. </p>

<p>Could you advise me to alternatives to <strong>Deepdive</strong> or any other resources that demonstrate how to build up such a system from scratch? </p>
","nlp"
"63549","Generate Intro-Text for Newsletter","2019-11-21 15:28:26","","2","20","<machine-learning><nlp><nlg>","<p>I am trying to implement the following idea.</p>

<p>For a daily newsletter I would like to generate an engaging and funny intro text, such as:</p>

<blockquote>
  <p>Good morning. Sorry if there are beer stains and buffalo sauce smeared
  on the Brew this morning. Yesterday was the sports equinox, a very
  special day in which all four major U.S. sports leagues (NBA, NHL,
  MLB, and NFL) were in action, plus Tiger and the Premier League.</p>
  
  <p>In related news...our Nats in 7 prediction? It's in the realm of
  possibility.</p>
</blockquote>

<p>As these texts are custom written I find it extremely hard to generate the text.</p>

<p>I was thinking of using an NLG template or on the other hand creating a template that adds daily news articles.</p>

<p>Any suggestions how you would approach this idea?</p>

<p>I appreciate your replies!</p>
","nlp"
"63399","what is BIO Tags for creating custom NER Named entity recognization?","2019-11-19 12:52:54","","11","22889","<nlp><named-entity-recognition>","<p>I would like to create custom Named Entity Recognition (NER), but I am confused about what BIO Tags are. Could anyone please explain the steps for creating NER and about this B, I, O tag.</p>
","nlp"
"63333","to include first single word in bigram or not?","2019-11-18 10:09:19","63345","2","243","<nlp>","<p>in a text such as </p>

<blockquote>
  <p>""The deal with Canada's Barrick Gold finalised in Toronto over the
  weekend""</p>
</blockquote>

<p>When I try to break it into bigram model, I get this</p>

<pre><code>""The deal""
""deal with"" 
""with Canada's""
""Canada's Barrick""
""Barrick Gold""
""Gold finalised""
""finalised in""
""in Toronto""
""Toronto over""
""over the""
""the weekend""
</code></pre>

<p>my question </p>

<p>shall I include the first word and the last word as single words</p>

<pre><code>""* The""
""Weekend *""
</code></pre>
","nlp"
"63274","Weka Character Level CNN","2019-11-17 03:39:44","","1","28","<nlp><cnn><weka>","<p>I am wanting to use a character level CNN to classify a heap of documents based on the century which they are from. I am having difficulties finding any resources on how to do character level classification in WEKA and was wondering whether anyone has any experience with this. I am using the wekadeeplearning4j package but I can't seem to figure out how their CNN text iterator can work with characters instead of embeddings. I would greatly appreciate any help. </p>
","nlp"
"63254","How to extract sub sentences from sentence mentioning a particular subject?","2019-11-16 15:31:43","","0","238","<python><nlp><sentiment-analysis>","<p>I am trying to solve an NLP problem. For a given sentence like :</p>

<p>""The Pasta was delicious, the Pizza was average""</p>

<p>I want to extract the sentiment attached to food items. Having built my own NER model, I am able to extract Pasta and Pizza and hence the sentences containing them. But using a sentiment analyser on the entire sentence would be wrong in this case</p>

<p>EXPECTED OUTPUT: </p>

<p>Pasta - Good score
Pizza - Average score</p>

<p>CURRENT OUTPUT:</p>

<p>Pasta - Kinda Good Score
Pizza - Kinda Good Score</p>

<p>I know I am getting this output because I am considering the same sentence for getting the sentiment attached to both the subjects in hand. </p>

<p>Is there a way to extract the sub sentence like ""Pasta was good"" and ""Pizza was average"" to associate sentence with each item instead of whole sentences which I am currently doing ?</p>
","nlp"
"63234","Get cosine similarity with constraint on the size of strings","2019-11-16 04:56:32","","0","108","<python><nlp>","<p>I want to get the similarity between strings (in python)
However, I also want to add a weight based on the size of the strings. If the cosine similarity is 1, but the string is of 2 words, I want the final similarity to be lower than a cosine similarity of 1 with 10 words per string. </p>

<p>Until now, this is what I'm doing:</p>

<pre><code>def weight_cosine_similarity(num1, num2):
    """"""
    num1: nb of words of string 1
    num2: nb of words of string 2
    """"""

    distance = (abs(num1 - num2) + 1)  # + 1 to avoid dividing by zero
    total_words = num1 + num2
    sigm_total_words = 1 / (1 + math.exp(-total_words))

    return sigm_total_words / distance
</code></pre>

<p>Two things impact the weight:
1. if number of words in string 1 is different than string 2, the weight will be smaller
2. if the total number of words is high, the weight will be higher (using sigmoid to get a number between 0 and 1)</p>

<p>The cosine similarity do the rest of the job.</p>

<p>My question: Do you know any better ways to better answer my problem, which is, to find a good weight when comparing two strings.</p>

<p>Thanks for your help.</p>
","nlp"
"63217","Similarity of words using BERTMODEL","2019-11-15 17:06:44","","3","6878","<nlp><word-embeddings><similarity><bert>","<p>I want to find the similarity of words using the BERT model within the NER task. I have my own dataset so, I don't want to use the pre-trained model.
I do the following:</p>

<pre><code>from transformers import BertModel
hidden_reps, cls_head = BertModel(token_ids , attention_mask = attn_mask , token_type_ids = seg_ids)
</code></pre>

<p>where</p>

<pre><code>token_ids with Shape : [1, 4, 47]
attn_mask with Shape : [1, 4, 47]
seg_ids with Shape : [1, 4, 47]
</code></pre>

<p>but I have an error :</p>

<pre><code>TypeError                                 Traceback (most recent call last)
&lt;ipython-input-74-5fa632122cc7&gt; in &lt;module&gt;()
      1 from transformers import BertModel
----&gt; 2 hidden_reps, cls_head = BertModel(token_ids , attention_mask = attn_mask , token_type_ids = seg_ids)

TypeError: __init__() got an unexpected keyword argument 'attention_mask'
</code></pre>

<p>How can I fix this error??</p>

<p>and how can I find the word embedding and the similarity of a given word using this model? </p>

<p>I could not find any tutorials or similar codes for this task.</p>
","nlp"
"63124","BERT Model Evaluation Measure in terms of Syntax Correctness and Semantic Coherence","2019-11-14 03:29:42","","2","360","<nlp><bert><language-model>","<p>For example I have an original sentence. The word barking corresponds to the word that is missing.</p>

<pre><code>Original Sentence : The dog is barking.
Incomplete Sentence : The dog is ___________.
</code></pre>

<p>For example, using the BERT model, it predicts the word crying instead of 
the word barking. How will I measure the accuracy of the BERT Model in terms of how syntactically correct and semantically coherent the predicted word is?</p>

<p>(For an instance, there are a lot of incomplete sentences, and the task is to evaluate BERT accuracy based on these incomplete sentences.)</p>

<p>In other words, how will I measure the distance in terms of semantics in terms of model between the two words <code>barking</code> and <code>crying</code>.</p>

<p>Please help.</p>
","nlp"
"63036","What is the advantage of positional encoding over one hot encoding in a transformer model?","2019-11-12 05:49:57","63096","2","7965","<machine-learning><nlp><encoding><attention-mechanism>","<p>I'm trying to read and understand the paper <a href=""https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf"" rel=""nofollow noreferrer"">Attention is all you need</a> and in it, they used positional encoding with sin for even indices and cos for odd indices. </p>

<p>In the paper (Section 3.5), they mentioned </p>

<blockquote>
  <p>Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add ""positional encodings"" to the input embeddings at the bottoms of the encoder and decoder stacks.</p>
</blockquote>

<p>My question is that if there is no recurrence, why not use One Hot Encoding. What is the advantage of using a sinusoidal positional encoding?</p>
","nlp"
"62914","Looking for a causality to effect dataset","2019-11-09 10:32:53","","1","38","<nlp>","<p>I am looking for a causality dataset that would look like this:</p>

<pre><code>animal + car -&gt; accident
speeding -&gt; traffic ticket
tomatoes + lettuce -&gt; salad
virus -&gt; illness
tooth decay -&gt; dentist
</code></pre>

<p>Does this already exists?</p>
","nlp"
"62907","Training with many CPU cores doesn't improve performance","2019-11-09 05:52:56","","0","118","<deep-learning><tensorflow><nlp>","<p>I ran my job on a computing cluster: first with 1 node / 4 cores, then with 2 nodes / 32 cores. But the training time is pretty much exactly the same for both of them! 67 seconds per step.</p>

<p>I am trying to <a href=""https://github.com/nshepperd/gpt-2"" rel=""nofollow noreferrer"">fine-tune GPT-2</a> for my text dataset (chat logs).</p>

<p>What can I do to get a performance increase corresponding to the increase in processing power with CPUs?</p>
","nlp"
"62879","How to extract insights from the given data?","2019-11-08 13:04:56","","1","102","<nlp><data-mining><dataset><data-cleaning>","<p>Ok, I have this data with 3 columns, unique id, <strong>raw text</strong>, and <strong>review text</strong>. My task is play with the dataset and find meaningful insights from it. Raw text is in plain English but review text is in another language. I have no idea how to proceed with the dataset. Even after I clean the data from the raw text, what should I do with review one because it is in another language. Which text analysis should I do and how can I implement it on the dataset?</p>
<p><a href=""https://i.sstatic.net/qICxo.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/qICxo.png"" alt=""dataset"" /></a></p>
","nlp"
"62868","High / low resources language : what does it mean?","2019-11-08 08:33:57","62877","3","6648","<nlp>","<p>In NLP, languages are often referred as <code>low resource</code> or <code>high resource</code>.</p>
<p>What do these terms mean?</p>
","nlp"
"62862","Preprocessing for Text Classification in Transformer Models (BERT variants)","2019-11-08 06:28:48","","14","8598","<python><nlp><preprocessing><bert><transformer>","<p>This might be silly to ask, but I am wondering if one should carry out the conventional text preprocessing steps for training one of the transformer models?</p>
<p>I remember for training a Word2Vec or Glove, we needed to perform an extensive text cleaning like: tokenize, remove stopwords, remove punctuations, stemming or lemmatization and more. However, during last few days I have had a quick jump into transformer models (fascinated btw), and what I have noticed that most of these models have a built-in tokenizer (cool), but none of the demos, examples, or tutorials are performing any of the these text preprocessing steps. You may take <a href=""https://github.com/kaushaltrivedi/fast-bert"" rel=""noreferrer"">fast-bert</a> for instance, there are no text preprocessing involved for the demos (maybe it is just a demo), but at <a href=""https://github.com/kaushaltrivedi/fast-bert#5-model-inference"" rel=""noreferrer"">inference</a> the whole sentences are passed without any cleaning:</p>
<pre><code>texts = ['I really love the Netflix original movies',
         'this movie is not worth watching']
predictions = learner.predict_batch(texts)
</code></pre>
<p>The same is true for the <a href=""https://github.com/huggingface/transformers"" rel=""noreferrer"">original transformer</a> by HuggingFace. Or many tutorials that I have looked at (take <a href=""https://github.com/kaushaltrivedi/bert-toxic-comments-multilabel/blob/master/toxic-bert-multilabel-classification.ipynb"" rel=""noreferrer"">this</a> or <a href=""https://towardsdatascience.com/building-a-multi-label-text-classifier-using-bert-and-tensorflow-f188e0ecdc5d"" rel=""noreferrer"">another one</a>). I can imagine that depending on the task this might not be required, e.g. next work prediction or machine translation and more. More importantly I think this is part of the contextual-based approach that these models offer (that is the innovation so to say) that are meant to keep most of the text and we may obtain a minimum but still good representation of the each token (out of vocabulary word). Borrowed from <a href=""https://medium.com/huggingface/multi-label-text-classification-using-bert-the-mighty-transformer-69714fa3fb3d"" rel=""noreferrer"">medium article</a> by HuggingFace:</p>
<blockquote>
<p><strong>Tokenisation</strong>
BERT-Base, uncased uses a vocabulary of 30,522 words. The
processes of tokenisation involves splitting the input text into list
of tokens that are available in the vocabulary. In order to deal with
the words not available in the vocabulary, BERT uses a technique
called BPE based WordPiece tokenisation. In this approach an out of
vocabulary word is progressively split into subwords and the word is
then represented by a group of subwords. Since the subwords are part
of the vocabulary, we have learned representations an context for
these subwords and the context of the word is simply the combination
of the context of the subwords.</p>
</blockquote>
<p>But does that hold true for tasks like multi-label text classification? In my use case the text is full of not useful stopwords, punctuation, characters and abbreviations and it is multi-label text classification as mentioned earlier. And in fact the prediction accuracy is not good (after a few rounds of training using <a href=""https://github.com/kaushaltrivedi/fast-bert"" rel=""noreferrer"">fast-bert</a>). What do I miss here?</p>
","nlp"
"62788","TLDR Bot - Sentence Tagging w/ BERT","2019-11-06 19:23:05","","0","303","<nlp><lstm><bert>","<p>Currently making a bot that condenses news articles. I'm tagging sentences as important or not important using a simple BERT classifier. The results were... not great. I'm really interested in how I can improve the results using LSTM.</p>

<p>I'm now batching 5 sentences together, calculating their BERT encodings, and then using 2 LSTM Layers, one backwards one forwards, to predict if the sentence is important.</p>

<p>Unfortunately I'm now calculating 5 times the number of embeddings, and if it doesn't work, I can't seem to figure out how to feed a variable number of things into BERT using Tensorflow, to see if I can tweak some results.</p>

<p>Are there other methods to add surrounding sentences to this context?</p>
","nlp"
"62780","Do repeated sentences impact Word2Vec?","2019-11-06 17:02:32","62786","1","1000","<machine-learning><nlp><word2vec><word-embeddings>","<p>I'm working with domain-oriented documents in order to obtain synonyms using Word2Vec. 
These documents are usually templates, so sentences are repeated a lot.</p>

<p>1k of the unique sentences represent 83% of the text corpus; while 41k of the unique sentences represent the remaining 17% of the corpus.</p>

<p>Can this unbalance in sentence frequency impact my results? Should I sub-sample the most frequent sentences?</p>
","nlp"
"62730","Best structure for a LSTM Bert sentence classifier","2019-11-05 18:56:41","","0","306","<nlp><bert>","<p>I'm interested in classifying sentences using BERT. Finetuning on a single sentence had very poor results. I'd like to add a forward and backward LSTM layer to try to improve results. I'm having trouble figuring out the best structure for an efficient implementation.</p>

<p>My first idea is to implement it as normal, but use a tf while loop to get all the BERT embeddings, and then feed them into the output. This seems really inefficient because I'll be calculating way more embeddings than I need.</p>

<p>The next would be to pre calculate all BERT embeddings and then feed those as input to my LSTM and classifier layer. Using this I think it's clear how to train the LSTM and classifier model, but how would I train the BERT Model as well? Is this a common method of constructing / training models? Any references?</p>

<p>Finally the last idea was to use the 2 sentence BERT embedding. It appears it would be limited to a 3 sentence window - previous sentence, current, and next. So scaling up might not work if the model still does not perform that well. Another issue is that it's unclear, at least to me, if the pooled output of a sentence</p>

<pre><code>[cls] sentenceA [sep] sentenceB [sep]
</code></pre>

<p>gives a representation of sentenceA, of both of them, or of sentenceB 'given' sentenceA. The pooled output is the representation of [cls] token, but in the paper / github there was no reference to if the [sep] representation encodes anything at all.</p>

<p>Any help, insights, or similar work would be appreciated!</p>
","nlp"
"62706","How to select 500 most pertinents tags among 10000?","2019-11-05 12:08:06","62716","0","39","<nlp><multilabel-classification><dimensionality-reduction>","<p>Say we have 100,000 documents tagged with 10,000 different tags (Max 5 tag per document). We wish to limit allowed tags to a list of 500 tags. </p>

<p><strong>How to select 500 tags in order to cover the largest set of documents ?</strong></p>

<p>Firstly, I chose the 500 most <strong>frequent</strong> tags. If we keep only these 500 tags, 70% of documents keep at least 1 tag.</p>

<p>I'm looking for a better method to pick the 500 most <strong>pertinents</strong> tags.</p>

<p>example : <a href=""https://i.sstatic.net/HkSdK.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/HkSdK.jpg"" alt=""enter image description here""></a></p>

<p>my best set of 5 tags to cover all documents is [italian, german, french, chineese, japan]. </p>

<p>it is not [italian, spanish, indian, finnish, english] although these are the most frequent tags.</p>

<p>I tried a Single Value Decomposition on the matrix : [documentID, set of tags]. but after that ? is it a good idea ? how to get 500 tags from the SVD results ?</p>
","nlp"
"62658","How to get sentence embedding using BERT?","2019-11-04 15:22:32","","39","78505","<tensorflow><nlp><pytorch><bert>","<p>How to get sentence embedding using BERT?</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import BertTokenizer
tokenizer=BertTokenizer.from_pretrained('bert-base-uncased')
sentence='I really enjoyed this movie a lot.'
#1.Tokenize the sequence:
tokens=tokenizer.tokenize(sentence)
print(tokens)
print(type(tokens))
</code></pre>
<h1>2. Add [CLS] and [SEP] tokens:</h1>
<pre class=""lang-py prettyprint-override""><code>tokens = ['[CLS]'] + tokens + ['[SEP]']
print(&quot; Tokens are \n {} &quot;.format(tokens))
</code></pre>
<h1>3. Padding the input:</h1>
<pre class=""lang-py prettyprint-override""><code>T=15
padded_tokens=tokens +['[PAD]' for _ in range(T-len(tokens))]
print(&quot;Padded tokens are \n {} &quot;.format(padded_tokens))
attn_mask=[ 1 if token != '[PAD]' else 0 for token in padded_tokens  ]
print(&quot;Attention Mask are \n {} &quot;.format(attn_mask))
</code></pre>
<h1>4. Maintain a list of segment tokens:</h1>
<pre class=""lang-py prettyprint-override""><code>seg_ids=[0 for _ in range(len(padded_tokens))]
print(&quot;Segment Tokens are \n {}&quot;.format(seg_ids))
</code></pre>
<h1>5. Obtaining indices of the tokens in BERT’s vocabulary:</h1>
<pre class=""lang-py prettyprint-override""><code>sent_ids=tokenizer.convert_tokens_to_ids(padded_tokens)
print(&quot;senetence idexes \n {} &quot;.format(sent_ids))
token_ids = torch.tensor(sent_ids).unsqueeze(0) 
attn_mask = torch.tensor(attn_mask).unsqueeze(0) 
seg_ids   = torch.tensor(seg_ids).unsqueeze(0)
</code></pre>
<h1>Feed them to BERT</h1>
<pre class=""lang-py prettyprint-override""><code>hidden_reps, cls_head = bert_model(token_ids, attention_mask = attn_mask,token_type_ids = seg_ids)
print(type(hidden_reps))
print(hidden_reps.shape ) #hidden states of each token in inout sequence 
print(cls_head.shape ) #hidden states of each [cls]

output:
hidden_reps size 
torch.Size([1, 15, 768])

cls_head size
torch.Size([1, 768])
</code></pre>
<p>Which vector represents the sentence embedding here? Is it <code>hidden_reps</code>  or <code>cls_head</code> ?</p>
<p>Is there any other way to get sentence embedding from BERT in order to perform similarity check with other sentences?</p>
","nlp"
"62657","help finding research discussion on HTS classification","2019-11-04 15:00:09","","1","29","<python><classification><nlp><word2vec>","<p>My question is about the theory of this problem, and not necessarily syntax.</p>

<p>I'm wondering if anyone here has experience with automating HTS (Harmonized Tax Schedule) classifications, specifically training a classifier to predict the HTS code that should be assigned to a product? There are rules for these designations that are stored as descriptive text data that 'explain' the product. This seems (to me) to make predicting how a part will be classified very difficult.</p>

<p>For context, it would seem that NLP is heavily involved in this process, but with the high cardinality involved in these descriptions and the high amount of overlap from one description to another, are there any best practices for success in making such predictions? Similarly, I'm having a hard time finding any public research on the matter. It appears that almost all advancement in this focus was developed behind the closed doors of companies. I understand why they'd make that decision, but I am a bit surprised how little public discussion exists on the topic considering how much business it affects across the globe.</p>

<p>things I am considering/researching:</p>

<p>Word vectorizing (Word2Vec): I got the idea from another user here, but given that it is typical to train a vectorizer with a large public corpus, I am not sure how that would affect the vectoring process, as the products I am classifying are domain specific.</p>

<p>n-gram: I suppose using 2, 3, or 4 would be sufficient for classification, given that most descriptions that I can see are succinct. But as I said, a lot of these descriptions will have overlap, for example:</p>

<pre><code>    HTS CODE             HTS Description
------------------     ------------------
11111 = 'football'     An inflated ball (used for kicking)
11112 = 'basketball'   An inflated ball (used for dribbling)
11113 = 'ball'         An inflated ball
</code></pre>

<p>In this case (if my understanding is correct), n-gram 6 would be needed for the first two products but only n-gram 3 is needed for the third product. Is the n-gram value something we can 'soft code' to adapt to each record or will I need to somehow find the appropriate number for n?</p>

<p>Thanks</p>
","nlp"
"62598","is it possible to implement LSTM with input shape (sample,timestep,timestep,feature)?","2019-11-03 06:49:58","","0","28","<nlp><lstm><rnn><word-embeddings>","<p>I'm new to Keras. I am trying to implement this model <a href=""https://www.aclweb.org/anthology/D15-1167"" rel=""nofollow noreferrer"">https://www.aclweb.org/anthology/D15-1167</a> for document classification, and I want to use LSTM for getting sentence representation. I have trained vector representation separately with the skip-gram model on my dataset. now after converting each document to separate sentence and then converting each sentence to separate word and then converting each word to the corresponding integer in the dictionary, I have something for example like this for each document: [[54,32,13],[21,43,2]...[28,1,9]] which I should feed each sentence to an LSTM to get a sentence vector and after that I should feed each sentence vector to a diffrent LSTM on the higher layer in order to get a document representation and then apply classification to it. my problem is in the first layer. how should I feed each sentence simultaneously to each LSTM (therefore at each time step each LSTM should be applied to a word vector from each sentence) ?</p>
","nlp"
"62538","What is the reason behind having low results using the data augmentation technique in NLP?","2019-11-01 18:20:23","62545","1","42","<machine-learning><nlp><data-augmentation>","<p>I used Data augmentation technique on my dataset, to have more data to train. My data is text so the data augmentation technique is based on random insertion of words, random swaps and synonyms replacement.</p>

<p>The algorithm I used performs well in other datasets, but in mine, it gives lower accuracy results comparing to the original experiment. Are there any logical interpretations?</p>
","nlp"
"62485","How to group chat messages by topic?","2019-10-31 15:34:17","","2","547","<nlp>","<p>I am a newbie in this field. Developer since 20 years and more but never done anything (except tutorials) with ML, DL, and NLP. Though I've already read a bunch of articles and tutorials about this technology and I am starting to figure out the steps and the conditions required to make it work.</p>

<p>What I would like to achieve (the reason of my question) is this:</p>

<p>I have a file containing about 2 years of conversations between me and another person. I would like to extract sequences of messages that are related by the same topic. I mean messages that are sequential in time and that belong to the same conversation about a topic.</p>

<p>My goal is to extract the time we spent on every single topic.</p>

<p>There is any model already trained for this task? (This is of course the obvious question :-P )</p>

<p>Or, there is any model that I can use as a start training base?</p>

<p>Or, if not, what should be a good approach (steps, techniques, software) to train one by my self? (If possible).</p>

<p>Thanks</p>

<p><strong>Update</strong></p>

<p>Thanks to Erwan response I made some more research. I am not sure if Erwan's answer can be considered a resolutive answer but no doubts that it pointed me to a possible research direction ""removing part of the fog in front of my eyes"". Since, in my case I don't have a labeld dataset to train a supervised model I started to search for LDA solutions (as you implicitly suggested and also based on several tutorial I found) and found some: <a href=""https://radimrehurek.com/gensim/"" rel=""nofollow noreferrer"">gensim</a>'s <a href=""https://radimrehurek.com/gensim/models/ldamodel.html"" rel=""nofollow noreferrer"">LDA model</a> and <a href=""https://radimrehurek.com/gensim/models/ldamulticore.html"" rel=""nofollow noreferrer"">parallelized LDA model</a>; <a href=""https://github.com/lda-project/lda"" rel=""nofollow noreferrer"">lda-project</a>; I also found a Java implementation that is <a href=""http://mallet.cs.umass.edu/api/cc/mallet/topics/LDA.html"" rel=""nofollow noreferrer"">MALLET LDA class</a> ecc...</p>

<p>I also started to browse arguments like Word2Vec, FastText (and family - I guess), even if I am not sure what are the purpose of these software compared to LDA, possibly because I have not a clear and complete view of the informations that LDA models can ""capture"" compared to the others.</p>

<p>Though the first step was to find a software that supports Italian language ""natively"" and I found <a href=""https://spacy.io/"" rel=""nofollow noreferrer"">spaCy</a> a tool that provides lemmatization, tokenization, ecc... and seems to have the purpose to prepare text for other topic detection softwares (even if it seems it has its own class for this, guessing by name, <a href=""https://spacy.io/api/textcategorizer"" rel=""nofollow noreferrer"">TextCategorizer</a> even if I am not able to figure out how to use it because of the lack of a complete documentation and examples).</p>

<p>So my guess is I could use spaCy to prepare the text to be feed to gensim's LDA (or any other implementation).</p>

<p>For data visualization I found <a href=""https://pyldavis.readthedocs.io/en/latest/index.html"" rel=""nofollow noreferrer"">pyLDAvis</a>.</p>
","nlp"
"62353","LSTM fot text classification always returns the same results","2019-10-29 10:22:46","62354","1","283","<machine-learning><nlp><lstm><machine-learning-model>","<p>Hello fellow Data Scientists,
I'm trying to make a classifier that was to classify sequences of text into some predefined classes, but i always get the same output, can anyone help me understand why?
The training of the model:</p>

<pre><code>   # The maximum number of words to be used. (most frequent)
MAX_NB_WORDS = 100
#2155
# Max number of words in each complaint.
MAX_SEQUENCE_LENGTH = 100
# This is fixed.
EMBEDDING_DIM = 20


cf.go_offline()
cf.set_config_file(offline=False, world_readable=True)

def treina(model_name):

    df = pd.read_csv(""divididos.csv"",sep='§',header=0)
    df.info()

    max_len = 0
    for value in df.Perguntas:
        if(len(value)&gt;max_len):
            max_len = len(value)
    max_words = 0
    for value in df.Perguntas:
        word_count = len(value.split("" ""))
        if(word_count&gt;max_words):
            max_words = word_count

    tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!""#$%&amp;()*+,-./:;&lt;=&gt;?@[\]^_`{|}~', lower=True)
    tokenizer.fit_on_texts(df['Perguntas'].values)
    word_index = tokenizer.word_index
    X = tokenizer.texts_to_sequences(df['Perguntas'].values)
    X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)
    Y = pd.get_dummies(df['Class']).values

    X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.05, random_state = 42)
    print(X_train.shape,Y_train.shape)
    print(X_test.shape,Y_test.shape)

    #Balance data
    sm = SMOTE(random_state=12)
    X_train, Y_train = sm.fit_sample(X_train, Y_train)
    print(X_train.shape,Y_train.shape)

    #LSTM net
    model = Sequential()
    model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))
    model.add(LSTM(20, dropout=0.2, recurrent_dropout=0.2,activation=""relu"",return_sequences=True))
    model.add(LSTM(10, dropout=0.2, recurrent_dropout=0.2,activation=""relu""))
    model.add(Dropout(0.2))
    model.add(Dense(11, activation='softmax'))
    opt = adam(lr=0.3)
    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])

    epochs = 100
    batch_size = 20

    history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1)
    accr = model.evaluate(X_test,Y_test)
    print('Test set\n  Loss: {:0.3f}\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))
    model.save(model_name)
    return model
</code></pre>

<p>and the testing:</p>

<pre><code>def corre(modelo):
    labels = [""a"",""b"",""c"",""d"",""e"",""f"",""g"",""h"",""i"",""j"",""k""]
    model = load_model(modelo)
    a = 0
    tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!""#$%&amp;()*+,-./:;&lt;=&gt;?@[\]^_`{|}~', lower=True)
    while (a==0):
        new_complaint = input()
        new_complaint = [new_complaint]
        seq = tokenizer.texts_to_sequences(new_complaint)
        padded = pad_sequences(seq, maxlen=MAX_SEQUENCE_LENGTH)
        pred = model.predict(padded)
        print(pred, labels[np.argmax(pred)])
</code></pre>

<p>Thank you for your time</p>
","nlp"
"62322","NLP techniques to label unlabeled data in a dataset","2019-10-28 17:18:31","","0","430","<python><nlp><nltk><spacy>","<p>So I have a .xls file with negative and neutral reviews of a medicine. However, this dataset does not have labels. I converted this .xls into a dataframe and I am using the Spacy Lib. </p>

<p>I have to use NLP techniques to label the data. Can you guys help me out on how to use NLP techniques to label this dataset as a neutral review or a negative review. So far, I did data cleansing (remove stop words, punctuation, etc.)</p>

<p>What are the next steps?</p>
","nlp"
"62307","Information Gain & Gini Index for NLP","2019-10-28 10:14:03","","0","325","<nlp><feature-selection><feature-extraction>","<p>I know how Information Gain and Gini Index work in General.</p>

<p>I have problem figuring out how to apply these techniques in NLP and text feature extraction.</p>

<p>Can someone show me an example of how to implement these techniques on NLP.</p>

<p>Thanks</p>
","nlp"
"62225","How to Extract Information from the Image(PNG)","2019-10-25 16:09:12","","2","1214","<python><deep-learning><nlp><image-preprocessing><opencv>","<p>I'm trying to extract some particular information from the image(png).</p>

<p>I tried to extract the text using the below code</p>

<pre><code>import cv2
import pytesseract
import os
from PIL import Image
import sys

def get_string(img_path):
    # Read image with opencv
    img = cv2.imread(img_path)

    # Convert to gray
    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    # Apply dilation and erosion to remove some noise
    kernel = np.ones((1, 1), np.uint8)
    img = cv2.dilate(img, kernel, iterations=1)
    img = cv2.erode(img, kernel, iterations=1)

    # Write the image after apply opencv to do some ...
    cv2.imwrite(""thres.png"", img)
    # Recognize text with tesseract for python
    result = pytesseract.image_to_string(Image.open(""invoice.png""))
    os.remove(""invoice.png"")

    return result

if __name__ == '__main__':
    from sys import argv

    if len(argv)&lt;2:
        print(""Usage: python image-to-text.py relative-filepath"")
    else:
        print('--- Start recognize text from image ---')
        for i in range(1,len(argv)):
            print(argv[i])
            print(get_string(argv[i]))
            print()
            print()

        print('------ Done -------')
</code></pre>

<p>But I want to extract data from particular fields.</p>

<p>Such as</p>

<blockquote>
<pre><code> a) INVOICE NO.
 b) CUSTOMER NO.
 c) SUBTOTAL
 d) TOTAL
 e) DATE
</code></pre>
</blockquote>

<p><strong>How can I extract the required information from the below image ""invoice""?</strong></p>

<p>PFB</p>

<p><a href=""https://i.sstatic.net/RarrU.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/RarrU.png"" alt=""enter image description here""></a></p>
","nlp"
"62161","Accessing Flask WS APIs over intranet -","2019-10-24 11:51:14","","0","231","<machine-learning><nlp><json><windows>","<p>I have 2 scripts - A.py and B.py, and both are Flask apps.
<em>A.py</em> renders a web page and acts as my UI taking inputs from user.
<em>B.py</em> is hold the main logic and has a web service API being called by <em>A.py</em>.</p>

<p>Both run as flask app services in localhost from different ports - 5001 and 5002, respectively.</p>

<p>My doubts - </p>

<p>1) Every time I make any changes to my B.py (or A.py), do i need to stop and restart the services?</p>

<p>2) I'm currently running both the services from local machine for proof-of-concept purposes. If I use WSGI (thinking of <em>Bottle</em>), will it be accessible from other machines over intranet?</p>

<p>I'm on Windows 10 (64-bit), Python 3.6 and Flask 1.1.1</p>

<p>Please advise.</p>

<hr>

<p>Edit 1) Added OS and Python version details.</p>
","nlp"
"62112","Is Annoy a machine learning algorithm to find nearest neighbor ? and is it similar to K nearest neighbor algorithm?","2019-10-23 06:24:50","","1","3055","<machine-learning><nlp><algorithms><k-nn>","<p>I was researching about  Google universal sentence encoding and i saw that it uses simple neighbor/Annoy to find the nearest vector for semantic-similarity search engine. This is the first time i'm hearing about it.  Is simple neighbor/Annoy another kind of  machine learning algorithm to find nearest neighbor ? and is it similar to K nearest neighbor  algorithm ?</p>
","nlp"
"62096","How to access an embedding table that is too large to fully load into memory?","2019-10-22 19:01:03","","1","371","<tensorflow><nlp><word-embeddings>","<p>I'm currently trying to find a way of loading/deserializing a .json file containing Flair word embeddings that is too large to fit in my RAM at once (>60GB .json with 32GB of RAM). My current code for loading the embedding is below.</p>

<pre><code>def get_embedding_table(config):
    words_id2vec = json.load(open(config.words_id2vector_filename, 'r'))
    words_vectors = [0] * len(words_id2vec)
    for id, vec in words_id2vec.items():
        words_vectors[int(id)] = vec

    words_vectors.append(list(np.random.uniform(0, 1, config.embedding_dim)))
    words_embedding_table = tf.Variable(name='words_emb_table', initial_value=words_vectors, dtype=tf.float32)
</code></pre>

<p>The rest of the code that I am trying to reproduce with a different word embedding can be found <a href=""https://github.com/xiangrongzeng/copy_re"" rel=""nofollow noreferrer"">here</a>.</p>

<p>I wonder if it is somehow possible to access the embedding table without deserialization of the entire .json file, for example: by sequentially reading it, somehow splitting it, or reading it directly from my disk. I would greatly appreciate your input!</p>
","nlp"
"62058","What dataset was Stanford NER trained on?","2019-10-21 21:46:14","62060","1","160","<nlp><dataset><stanford-nlp>","<p>I would like to re-train the <a href=""https://nlp.stanford.edu/software/CRF-NER.html"" rel=""nofollow noreferrer"">Stanford NER library</a> from scratch as a 1 class model. </p>

<p>Only 3,4 and 7 class models are available out of the box.</p>

<p>Is it possible to obtain the data that the model was originally trained on?</p>
","nlp"
"62039","Recognizing emerging topic within ongoing topic","2019-10-21 16:03:58","","1","17","<nlp><data-cleaning><twitter>","<p>I have been collecting a large amount of tweets from Twitter for a few weeks related to a series of specific keywords, and would like to address a specific problem. Say I collected all tweets mentioning the word ""Monsters"", and it averaged about 50k tweets a day, but this week, James Blunt released a new song ""Monsters"" which becomes a particular hit and that amount increases to maybe 80k. What would be a good approach to help distinguish between the usual usages of the word ""Monsters"" and the new tweets that are presumably related to James Blunt? I would imagine there are quite a few options, so any ideas would be much appreciated. </p>
","nlp"
"62037","Can BERT embeddings be used to reproduce the original content of the text?","2019-10-21 15:38:14","","2","229","<deep-learning><nlp><machine-translation><bert><neural-style-transfer>","<p>From what I understand, BERT provides contextualized embeddings that are not deterministic the way Word2Vec embeddings (i.e. the word ""Queen"" doesn't always produce the same vector, it'll be different depending on the context)</p>

<p>Is there a way to ""reverse"" these contextualized embeddings to produce an output related to the original content of the text? For instance, how would I do machine translation, or style transfer?</p>
","nlp"
"62033","Clause type classification","2019-10-21 14:04:25","","2","187","<machine-learning><neural-network><classification><nlp>","<p>We would like identify similar text (clauses) on a contract based on a trained corpus.</p>

<p>For instance:</p>

<p><strong>Contract - small sample</strong></p>

<pre><code>NOW, THEREFORE, the parties hereto mutually agree as follows:

1. Scope of Services. The CONTRACTOR shall, in a proper and satisfactory manner as determined by OHA, provide all the goods and services set forth in Attachment – S1,
which is hereby made a part of this Contract.

2. Time of Performance. The performance required of the CONTRACTOR under this Contract shall be completed in accordance with the Time Schedule set forth in Attachment – S2, which is hereby made a part of this Contract.

3. Compensation. The CONTRACTOR shall be compensated according to the Compensation provision set forth in Attachment – S3, which is hereby made a part of this Contract.

4. Standards of Conduct Declaration. The Standards of Conduct Declaration of the CONTRACTOR is attached and is made a part of this Contract.
</code></pre>

<p><strong>Trained clauses</strong></p>

<p>We already have classified a few clauses from previous contracts. For instance:</p>

<pre><code>#time_of_performance = 
[
    ""Under this contract the performance required to be completed in accordance with the a predefined schedule.""
,
    ""The completion of each phase of the project will be used to define the performance of this contract""
,
    etc.
]
</code></pre>

<p>Where <code>#time_of_performance</code> is the classification for these clauses.</p>

<p><strong>Expected result</strong></p>

<p>Given the contract and the trained set, we would like to get parts/ranges of the texts and its classifications:</p>

<pre><code>#time_of_performance = [""2. Time of Performance. The performance required of the CONTRACTOR under this Contract shall be completed in accordance with the Time Schedule set forth in Attachment – S2, which is hereby made a part of this Contract.""]
</code></pre>

<p>Is there a known approach for this problem or a recommended processing pipeline? </p>
","nlp"
"62016","Wikipedia corpus for NLP - Cleaned sentences","2019-10-21 08:17:44","","0","110","<nlp><corpus><wikipedia>","<p>I can see many wikipedia dumps out there.
I am looking for a wikipedia-made corpus, in which every line is one sentence, without any wikipedia meta tags.</p>
","nlp"
"62005","what is sentence embeding and how to do sentence embedding for a sentence and how to use word embedding to create a sentence embedding?","2019-10-20 18:58:18","","2","1721","<nlp><word-embeddings><stanford-nlp>","<p>What is sentence embedding? How would you do sentence embedding for a sentence like: <code>&quot;How old are you?&quot;</code> How do you use word embedding to create a sentence embedding?</p>
","nlp"
"61878","How prevalent is `C/C++` in machine learning development?","2019-10-17 15:50:06","61889","9","1510","<machine-learning><deep-learning><cnn><nlp><programming>","<p>I am currently a data scientist mostly doing NLP, and I do most of my work in<code>Python</code>. Since I didn't get a CS degree in undergrad, I've been limited to very high level languages; <code>Java</code>, <code>Python</code>, and <code>R</code>. I somehow even took Data Structures and Algorithms avoiding <code>C</code> or <code>C++</code>.</p>

<p>I'm intending to go to graduate school to study more Natural Language Processing, and I'm wondering how much <code>C/C++</code> I need to know. Deep-learning frameworks like <code>PyTorch</code> or <code>Tensorflow</code> are written in <code>C++</code>, and CUDA is only available in <code>C</code>. I'm not going to be writing <code>Cython</code> libraries, but I would like to do research and build new models (i.e. like ""inventing"" CNN's, seq2seq models, transformers). </p>

<p>I don't know how much <code>C/C++</code> is used, and I'm unsure if it's worth learning the language-specific complexities that may be channeled into learning something else; hopefully somebody can let me know how prevalent the use of <code>C/C++</code> is? </p>
","nlp"
"61861","Finding Criminal Name in news?","2019-10-17 09:41:31","61873","1","101","<machine-learning><nlp><beginner><named-entity-recognition>","<p>We have news URLS, which we want to classify into crimes or non-crimes and further identify criminals by using NERs. 
For creating a model that identifies criminals, we tried SPacy which gave all the names like lawyers name , president ,criminal etc.. 
can Anyone help on how to get only Criminal name, Not all these irrelevant names.
I am just a beginner trying things, any help is appreciated 
Thanks in Advance</p>
","nlp"
"61825","Why is word prediction an obsession in Natural Language Processing?","2019-10-16 14:52:38","61893","8","803","<nlp><bert>","<p>I have heard how great <a href=""https://arxiv.org/abs/1810.04805"" rel=""noreferrer"">BERT</a> is at masked word prediction, i.e. predicting a missing word from a sentence.</p>

<p>In a <a href=""https://medium.com/saarthi-ai/bert-how-to-build-state-of-the-art-language-models-59dddfa9ac5d"" rel=""noreferrer"">Medium post about BERT</a>, it says:</p>

<blockquote>
  <p>The basic task of a language model is to predict words in a blank, or it predicts the probability that a word will occur in that particular context. Let’s take another example:</p>
  
  <p>“FC Barcelona is a _____ club”</p>
</blockquote>

<p>Indeed, I recently heard about <a href=""https://arxiv.org/abs/1907.10529"" rel=""noreferrer"">SpanBERT</a>, which is ""designed to better represent and predict spans of text"".</p>

<p>What I do not understand is: <strong><em>why?</em></strong></p>

<ol>
<li>I cannot think of any common reason that a human would need to do this task, let alone why it would need to be automated.</li>
<li>This does not even seem to be a task where it is particularly easy to evaluate the success of a model. For example, </li>
</ol>

<blockquote>
  <p>My ___ is cold</p>
</blockquote>

<p>This could reasonably be a number of possible words. How can BERT be expected to get this right, and how can humans or another algorithm be expected to evaluate whether ""soup"" is a better answer than ""coffee""?</p>

<p>Clearly there are a lot of smart people who think that this is important, so I accept my lack of understanding is likely based on my own ignorance. Is it that this task itself is not important, but it's a proxy for ability at other tasks?</p>

<p>What am I missing?</p>
","nlp"
"61802","can we learn a model to pre-process text?","2019-10-16 06:22:44","","0","41","<machine-learning><nlp>","<p>I'm in a very dire situation where I have to preprocess the text but the text in the documents is very random. It is in the form of <strong>numerical points</strong>. </p>

<p>I want to <strong>remove a certain class of points</strong>  (like bullet points) from those documents but simple preprocessing is not helping as the points are appearing randomly anywhere in the document.</p>

<p>I was thinking of learning a model where the model identifies the points which I want to remove. </p>

<p>Is it viable? And If so what approaches should I look for.</p>

<p>[Natural language processing]
Thanks!</p>
","nlp"
"61783","How to generate abbrevations from shortend words in medical records","2019-10-15 17:46:22","","1","113","<python><nlp><regex>","<p>I have text files which contains medical history of a patients and would want to extract information out of it.</p>

<p>Basically what want is generate english text of abbrevations,semantic category region,location and relation also correct spelling mistakes if any by parsing medical history records.</p>

<p>I have google around for open source tools and found cTAKES &amp; Metamap.However could not find python api for the same.</p>

<p>Can someone suggest how to make use of these tools or interact via python?and also is there a better way to extract information using regular epression..etc.</p>

<pre><code>Sample sentence:
55 yr M comes to the ED with c/o of a chest pain that started 1 hr ago,
The cP are a/w SOB.

Expected o/p 
55 yr(year) (M)male comes to the (ED)emergency department with (c/o)complaint of a chest pain that started 1 hr(hour) ago,The (cP)chest pain is associated with (SOB)short of breathing.
</code></pre>

<p>Apart from generating abbrevations,also need to cature temparatures,levels such as 5/4,semantic category region,location and relation also correct spelling mistakes in the clinical notes.</p>

<p>While am waiting for UMLS license,just wanted to check can above functionality be achieved through UMLS/cTaked/Metamap? or cutsom script needs to be written in conjunction with python/NLP/RE and open source clinical abbrevations dictionary(please suggest)?</p>
","nlp"
"61778","Combining text (NLP), numeric, and categorical data for a regression problem","2019-10-15 15:15:44","61781","1","2240","<machine-learning><python><nlp><regression>","<p>I have a dataset</p>
<pre><code>data = { 
    points: 3.765, 
    review: `Food was great, staff was friendly`, 
    country: 'Chile', 
    designation: 'random', 
    age: 20
}
</code></pre>
<p>I am looking for a way to use these features to build a model to predict points.</p>
<p>Description seems to hold a lot of information about points.</p>
<p>How do I feed this data into the model and also which model?</p>
<p><em>Note</em> I don't want to use word2vec (embeddings)</p>
","nlp"
"61759","Can AI (NLP) convert user questions (text) into database SQL queries?","2019-10-15 10:24:11","","4","1237","<neural-network><nlp><ai><chatbot>","<p>I have been reading about NLP but got confused and not able to figure out - if it is feasible for NLP to convert questions in natural language to transform into SQL queries (so that it can execute on concerned database and fetch the output).</p>

<p>Ex. If the user raises a question to the AI engine (application) - </p>

<blockquote>
  <p>How many new customers have registered on my website this month?</p>
</blockquote>

<p>The AI should parse this using NLP techniques and convert it into SQL and execute on database table say ""User_management"" -</p>

<pre><code>select count(1) from User_management
where joining_month = 'Oct' and joining_year = 2019
</code></pre>

<p>Please share your thoughts and advice.</p>

<p>Update 1) Got <a href=""https://datascience.stackexchange.com/questions/31617/natural-language-to-sql-query"">this SO link</a> and <a href=""https://www.nltk.org/book/ch10.html"" rel=""nofollow noreferrer"">this NLTK chapter</a> as starting point</p>
","nlp"
"61755","Information Extraction/Semantic Search for long, unstructured documents","2019-10-15 09:57:44","","1","382","<nlp><text-mining><information-retrieval><named-entity-recognition>","<p>I am stuck with a particular task of <strong>information extraction</strong>. I have a few hundred, long (5-35 pages) <strong><em>pdf, doc and docx</em></strong> project documents from which I seek to extract specific information and store them in a structured database. </p>

<p>The ultimate goal is to extract and store information in a way that we can query those and any new incoming documents for fast and reliable information. For instance, I want to query a combination of entities from the knowledge base and then return the n-most relevant paragraphs/sentences from the documents. Since some entities like “World Bank” are extracted dozens of times for some documents, I need a way to query the entity in context. Otherwise I just end up with a database that contains the names of specific entities without any way to map them back. </p>

<p><strong>NER</strong> usually seems like a good solution for this, however, the documents all have very unique structures which also change from document to document. For instance, a lot of relevant information is stored in tables, but also in long paragraphs.</p>

<p>As far as I understand, <strong>NER</strong> uses the surrounding words to identify entities, hence loading in the whole documents as raw text and manually tagging terms in the tables will probably not serve as good training data. </p>

<p>For now I built a function that extracts the raw text from pdf, doc and docx documents and extracted entities with spaCys NER model, however, I need to define my own entities and the domain is to scientifc for <strong>spaCy</strong> to deliever good results.  I also obtained a prodigy license for annotation. </p>

<p>Also, it seems I have to find a way to distinguish between ""relevant"" text and table entries and ""junk"" - aka footnotes, annexes, titles, subtitles, etc. </p>

<p>Any input is highly appreciated! </p>

<p>Thanks!</p>
","nlp"
"61726","What does Conv1d do in a sentiment analysis?","2019-10-14 17:13:25","","0","261","<nlp><sentiment-analysis><convolutional-neural-network>","<p>I am doing some study on <a href=""https://www.kaggle.com/anshulrai/cudnnlstm-implementation-93-7-accuracy"" rel=""nofollow noreferrer"">https://www.kaggle.com/anshulrai/cudnnlstm-implementation-93-7-accuracy</a></p>

<p>I understand we need LSTM to capture the sequence of words in the sentience, but I am not quite understand what does Conv1d do in the model architecture? Could someone please share some intuitive explanation? </p>

<p>Thanks a lot!</p>

<pre><code>Model: ""model_1""
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 100)          0                                            
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 100, 100)     2000000     input_1[0][0]                    
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 100, 100)     0           embedding_1[0][0]                
__________________________________________________________________________________________________
conv1d_1 (Conv1D)               (None, 98, 200)      60200       dropout_1[0][0]                  
__________________________________________________________________________________________________
conv1d_2 (Conv1D)               (None, 96, 200)      120200      conv1d_1[0][0]                   
__________________________________________________________________________________________________
conv1d_3 (Conv1D)               (None, 94, 256)      153856      conv1d_2[0][0]                   
__________________________________________________________________________________________________
conv1d_4 (Conv1D)               (None, 92, 256)      196864      conv1d_3[0][0]                   
__________________________________________________________________________________________________
conv1d_5 (Conv1D)               (None, 45, 512)      393728      conv1d_4[0][0]                   
__________________________________________________________________________________________________
cu_dnnlstm_1 (CuDNNLSTM)        (None, 512)          2101248     conv1d_5[0][0]                   
__________________________________________________________________________________________________
cu_dnnlstm_2 (CuDNNLSTM)        (None, 512)          2101248     conv1d_5[0][0]                   
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 1024)         0           cu_dnnlstm_1[0][0]               
                                                                 cu_dnnlstm_2[0][0]               
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 1024)         0           concatenate_1[0][0]              
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 64)           65600       dropout_2[0][0]                  
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 64)           0           dense_1[0][0]                    
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1)            65          dropout_3[0][0]                  
==================================================================================================
Total params: 7,193,009
Trainable params: 7,193,009
Non-trainable params: 0
</code></pre>
","nlp"
"61692","What does dimension represent in GloVe pre-trained word vectors?","2019-10-14 06:24:06","","6","6498","<word-embeddings><nlp>","<p>I'm using <a href=""https://nlp.stanford.edu/projects/glove/"" rel=""nofollow noreferrer""><em>GloVe</em> pre-trained word vectors</a> (<code>glove.6b.50d.txt</code>, <code>glove.6b.300d.txt</code>) as word embedding.</p>
<p>I have a conceptual question:</p>
<ul>
<li>What is the difference between these files?</li>
<li>On the other hand, what does the dimension represent in the GloVe pre-trained word vectors?</li>
</ul>
","nlp"
"61662","How to cluster n-grams?","2019-10-13 04:47:18","","3","868","<machine-learning><nlp>","<p>I just wanted to know how to cluster n-grams based on their semantics. Like clustering together n-grams that are semantically similar by leveraging the distributional hypothesis suggesting that similar words appear in similar contexts.</p>
","nlp"
"61581","Training data requirements for NLP models","2019-10-10 23:16:44","","2","790","<machine-learning><nlp><sampling>","<p>Are there general guidelines for how much data is required for natural language processing (NLP) classification models? I understand this may depend on the text quality, text length, how accurate the labels are, the event frequencies, the chosen algorithm, etc. Are there any general frameworks I can use to estimate how much data I would need? I'm going through an exercise of having data owners labeling their text data and i'd like to give them a recommendation for how much data they should label other than more is better.</p>
","nlp"
"61564","How to choose solution - Neural Neworks or Scikit-Learn/Numpy/Pandas?","2019-10-10 14:42:19","","1","48","<machine-learning><neural-network><classification><nlp>","<p>I am trying to solve a problem - categorising and routing service desk emails to concerned teams for resolution. 
Created and tested a model using Scikit-Learn, Numpy and Pandas. - Tokenized the email subject and body, used Bag-of-Words/TfIdf, and applied ML algos like - SVMs, Random Forest, Linear Classification, etc.</p>

<p>Now, as I read more came across NLP and Neural Networks, Keras, PyTorch, Tensorflow, etc.</p>

<p>May I ask how to choose the correct tool or solution for my particular problem? Please advise.</p>
","nlp"
"61558","Custom POS tags with SpaCy for NER","2019-10-10 13:21:48","","0","1209","<nlp><named-entity-recognition><spacy>","<p>Quite new to NLP and especially NER. I'm trying to train a NER model on a custom dataset. This is a dataset of houses for sale. As part of the entities I'm training the model to extract are <strong>reference numbers</strong>. These are of variable length (but usually between 4-9) and look like <code>G55L7</code> or <code>LPP01Z1-32</code>. </p>

<p>How can I give these entities a new ""POS tag"", as from what I'm aware of, I can't find any in SpaCy's default list that would match these?</p>

<p>Ideally, I'd like to train this alongside a pre-existing NER model so that I can also extract <code>ORGs</code> which SpaCy already has support for.</p>
","nlp"
"61491","What is word embedding and character embedding ? Why words are represented in vector with huge size?","2019-10-09 13:47:18","","3","6889","<nlp><word-embeddings><embeddings>","<p>In NLP word embedding represent word as number but after reading many blog i found that word are represent as vectors ? so what is word embedding exactly and  Why words are represented in vector and the vector size is huge. what values dose that vector represent ?</p>

<p>what is the difference between word and character embedding ? please explain in simple term with simple example.</p>
","nlp"
"61404","A Derivation in Combinatory Categorial Grammer","2019-10-08 01:32:59","","1","23","<nlp><derivation>","<p>I am reading about CCG on page 23 of <a href=""https://web.stanford.edu/~jurafsky/slp3/12.pdf"" rel=""nofollow noreferrer"">Speech and Language processing</a>. There is a derivation as follows:</p>

<pre><code>(VP/PP)/NP , VP\((VP/PP)/NP) =&gt; VP?
</code></pre>

<p>Can anyone example this please? This make sense if </p>

<pre><code>VP\((VP/PP)/NP) is equivalent to (VP\(VP/PP))/NP 
</code></pre>

<p>and </p>

<pre><code>(VP/PP)/NP is equivalent to VP/(PP/NP). 
</code></pre>

<p>But they seem at least non-trivial from the text!</p>

<p>Any help would be greatly appreciated.</p>

<p>CS</p>
","nlp"
"61398","Are stopwords helpful when using tf-idf features for document classification?","2019-10-07 20:30:13","","4","10215","<nlp><tfidf>","<p>I have documents of pure natural language text. Those documents are rather short; e.g. 20 - 200 words. I want to classify them.</p>

<p>A typical representation is a bag of words (BoW). The drawback of BoW features is that some features might always be present / have a high value, simply because they are an important part of the language. Stopwords like the following are examples: is, are, with, the, a, an, ...</p>

<p>One way to deal with that is to simply define this list and remove them, e.g. by looking at the most common words and just deciding which of them don't carry meaning for the given task. Basically by gut feeling.</p>

<p>Another way is TF-IDF features. They weight the words by how often they occur in the training set overall vs. how often they occur in the specific document. This way, even words which might not directly carry meaningful information might be valuable.</p>

<p>The last part is my question: Should I remove stopwords when I use TF-IDF features? Are there any publications on this topic? (I'm pretty sure I'm not the first one to wonder about this question)</p>
","nlp"
"61312","Rank links from rss feed","2019-10-05 16:59:38","61336","0","184","<machine-learning><nlp><clustering>","<p>I am trying to create a script to filter the most ""intersting"" articles from an rss feed and rank them.</p>

<pre><code>feeds = ['http://feeds.theguardian.com/theguardian/technology/rss',
         'http://rss.cnn.com/rss/money_news_international.rss',
         'https://news.ycombinator.com/rss?format=xml',
         'http://feeds.reuters.com/reuters/companyNews',
         'http://feeds.reuters.com/reuters/businessNews']
</code></pre>

<p>I tried using applying the K-means algorithm to RSS feeds to filter the most popular articles from thousands of links in an attempts to reduce my personal RSS reading time.</p>

<p>However, I feel that this might not be state of the art.</p>

<p>Any suggestions on papers, actual implementations or approaches to get a proper list of ""<em>must-read</em>"" articles.</p>

<p>I appreciate your replies!</p>
","nlp"
"61232","How to find similar phrases","2019-10-04 03:42:04","","3","4235","<nlp><word2vec><word-embeddings>","<p>I have the following problem:
I have created a customized Dictionary for getting used in some NLP tasks. I want to enhance my dictionary by finding phrases similar to the phrases I have in my dictionary.
For example:</p>

<pre><code>Lets say I have a phrase:
- take responsibility 
then, I should be able to find phrases like:
- hold accountable for 
-responsible for
</code></pre>

<p>I have just started learning NLP and I tried Word2Vec model over my dictionary but it only gives similar words that are already present in the dictionary.
Is there any other algorithm that helps me find similar phrases using multiple sources.</p>
","nlp"
"61195","Distractor Generation for Multiple Choice Questions","2019-10-03 12:09:29","","1","2287","<machine-learning><python><deep-learning><nlp><text-generation>","<p>I'm currently working on generating distractor for multiple choice questions. Training set consists of question, answer and 3 distractor and I need to predict 3 distractor for test set. I have gone through many research papers regarding this but the problem in my case is unique. Here the problem is the questions and answers are for a comprehension(usually a big passage of text story) but the comprehension based on which is not given nor any supporting text is given for the question. Moreover, the answers and distractor are not a single word but sentences. The research paper I went mostly worked with some kind of support text. Even the SciQ dataset had some supporting text but the problem im working is different</p>
<p><a href=""http://www.personal.psu.edu/cul226/files/naacl18_bea_distractor.pdf"" rel=""nofollow noreferrer"">This</a> research paper was the one which I thought closely went by what I wanted and I'm planning to implement this. Below is an excerpt from the paper which the authors say worked better than NN models.</p>
<blockquote>
<p>We solve DG as the following ranking problem: Problem.
Given a candidate distractor set D and a MCQ dataset M = {(qi , ai , {di1, ..., dik})} N i=1, where qi is the question stem, ai is the key, Di = {di1...dik} ⊆ D are the distractors associated with qi and ai , find a point-wise ranking function r: (qi , ai , d) → [0, 1] for d ∈ D, such that distractors in Di are ranked higher than those in D − Di.</p>
</blockquote>
<p>My questions are a) From what I understood, The above lines says we first create a big list containing all the distractors in the dataset and then we create a pointwise ranking function with respect to all distractors for every question? So if we have n questions and d distractors. We will have a (nxd) matrix where pointwise function values range between o and 1. Also, a question's own distractors should be ranked higher than the rest. Right?</p>
<blockquote>
<p>To learn the ranking function, we investigate two types of models: feature-based models and NNbased models.</p>
<p>Feature-based Models: Given a tuple (q, a, d), a feature-based model first transforms it to a feature vector φ(q, a, d) ∈ R d with the function φ. We design the following features for DG, resulting in a 26-dimension feature vector:</p>
</blockquote>
<ul>
<li>Emb Sim. Embedding similarity between q and d and the similarity
between a and d.</li>
<li>POS Sim. Jaccard similarity between a and d’s POS tags.</li>
<li>ED. The edit distance between a and d.</li>
<li>Token Sim. Jaccard similarities between q and d’s tokens, a and d’s tokens, and q and a’s tokens.</li>
<li>Length. a and d’s character and token lengths and the difference
of lengths.</li>
<li>Suffix. The absolute and relative length of a and d’s longest
common suffix.</li>
<li>Freq. Average word frequency in a and d.</li>
<li>Single. Singular/plural consistency of a and d. This</li>
<li>Wiki Sim.</li>
</ul>
<p>My questions: Will these feature generation idea applies to both word distractors and sentence distractors? ( As per the paper, they claim it will).</p>
<p>Apart from all of these, I have other simple questions such as should I remove stopwords here?</p>
<p>I'm new to NLP. So any suggestions about which SOTA implementation would work here would be very helpful. Thanks in advance.</p>
","nlp"
"61024","What is auxiliary loss in Character-level Transformer model?","2019-09-30 04:00:46","61056","3","886","<deep-learning><nlp><loss-function><transformer>","<p>I am reading <a href=""https://arxiv.org/abs/1808.04444"" rel=""nofollow noreferrer"">Character-Level Language Modeling with Deeper Self-Attention</a> from Rami Al-Rfou. In the second page, they had mentioned about Auxiliary Losses which can speed-up the model convergence and as an additional regularizer. They said they had 3 kinds of auxiliary losses:</p>

<ol>
<li>Auxiliary losses at intermediate sequence positions</li>
<li>Auxiliary losses from intermediate hidden representations</li>
<li>Auxiliary losses at target positions multiple steps.</li>
</ol>

<p>However, I cannot find any information or reference explaining auxiliary losses. I would like to know:</p>

<ul>
<li>What is auxiliary losses? Is it an <a href=""https://arxiv.org/abs/1803.00144"" rel=""nofollow noreferrer"">unsupervised prediction model using part of the data</a>?</li>
<li>How can I calculate the auxiliary losses?</li>
</ul>
","nlp"
"61009","NER with Unsupervised Learning?","2019-09-29 18:57:49","75562","2","2829","<nlp><unsupervised-learning><named-entity-recognition>","<p>If we treated NER as a classification/prediction problem, how would we handle name entities that weren't in training corpus?</p>

<p>For example, ""James was born in England."" James was labeled as a PERSON and England as a LOCATION. But we type another completely strange sentence like ""Fyonair is from Fuabalada land."" We as human can understand Fyonair is a person (or maybe princess from a fairy tale) and Fuabalada is the land where she comes from.</p>

<p>How would our model identify it if it wasn't included in billions of corpus and tokens? Can unsupervised learning achieve this task?</p>
","nlp"
"60974","How to extract assignment from natural language text?","2019-09-28 21:35:36","","2","97","<nlp>","<p>I'm a bit new to NLP/IE.
I'm looking for the task within NLP/IE that would be concerned with extracting a value that has been assigned.</p>

<p>For instance, given the text ""The value is 45.1hz"" or ""The color is blue"", I would like to be able to extract the fact that 45.1hz has been assigned to the value, or that the color has been set as blue.</p>

<p>I have tried to find the right name for this task - It seems somewhat related to relation extraction, however it doesn't seem to have two or more entities, and the relation unclear to me.</p>

<p>It may also be related to triplet extraction, but once again I'm a bit loss on what the values would be, i.e. (color, be, blue) ?</p>

<p>I'm looking for the name for this type of task, where you extract the value of an assignment, for a specific pre-defined value.</p>
","nlp"
"60817","Document embedding vs locality sensitive hashing for document clustering","2019-09-26 11:23:40","60891","2","1763","<dimensionality-reduction><nlp><embeddings><similar-documents>","<p>I would like to compare two methods: <a href=""https://en.wikipedia.org/wiki/Locality-sensitive_hashing"" rel=""nofollow noreferrer"">locality sensitivity hashing</a>  and document embedding to get the similarity between two documents. Both of those methods encode information of a document in a vector which I would like to use to find similar documents in a very large corpus (potentially more than 100 000 documents). Have anybody ever compared those two methods and what are the advantages of each of them. </p>

<p>Cheers in advance </p>
","nlp"
"60787","N-Gram Linear Smoothing","2019-09-26 01:30:44","","1","32","<nlp><expectation-maximization>","<p>In slide 61 of the <a href=""https://web.stanford.edu/~jurafsky/slp3/slides/LM_4.pdf"" rel=""nofollow noreferrer"">NLP text</a>, to smooth out the n-gram probabilities, we need to find the lambdas the miximazies a probability to held-out set given in terms of <em>M(λ1, λ2, ...λ_k)</em>. What does this notation mean please? Also, it says that ""One way is to use the EM algorithm, an iterative learning algorithm that converges on locally optimal λs"". Can someone refer me to a good example? Say the training text is ""Sam I am Sam I do not eat.""
and the held-out data is ""I do like Sam I am sam""</p>

<p>What is the objective function please?</p>
","nlp"
"60756","Best practice on count of manual annotations for building criminal detection from news articles?","2019-09-25 14:50:40","","0","20","<machine-learning><nlp><data-science-model><annotation>","<p>We have 7 million news articles corpus, which we want to classify into crimes or non-crimes and further identify criminals by using NERs/annotating criminals, crime manually. For creating a model that identifies criminals, what is the number of annotated articles that we must train/build our model on? Is there any industry best practice on this count? Is there any better way to come to this number of training(annotated) dataset, than random guessing? Are there any best practices resources that anyone can point to? Thanks in advance!</p>
","nlp"
"60727","Using Google Translate API to create a Translation Dataset","2019-09-25 07:31:38","","1","2195","<dataset><nlp><machine-translation><google>","<p>Is it a good idea? ;-)<br>
Is it legal to do so? Is it legal to release such a dataset to public?</p>

<p>Say I have a language X for which I want to create a dataset for translation to/from English, for which I have no online sources to scrape out the data from. But our giant Google Translate somehow has the support for language X.</p>

<p>Can we just dump the translations from English datasets for my language X using Google Translate API and create a rough dataset to start with?</p>
","nlp"
"60689","Analyzing Sentiments of Financial News related to a Company","2019-09-24 11:18:36","60712","1","34","<python><nlp><sentiment-analysis>","<p>I'm trying to build a model which gives me the sentiments of the Financial News related to a company and I want to predict the stock price accordingly. But the major problem that I'm facing is understanding the news for the counterpart.</p>

<p>Let's say I have a news headline as ""Total is under pressure and CEO has confirmed that they are planning to sell stakes soon"". My model will always give negative sentiment which is correct, but this might actually be a good news for Shell or Exxon, lets say. But how do I tell my model that it is actually a good news for Shell.</p>

<p>Also is there any good process to understand which news relate to which companies and how I can calculate the sentiment accordingly. Maybe a good labelled data-set or pre-trained architecture which might help me out?</p>

<p>P.S. Most important, is there any labelled dataset or any other pre-trained architecture which I can use to calculate the sentiments of financial news?</p>
","nlp"
"60674","how to use word embedding to do document classification etc?","2019-09-24 06:21:45","60707","1","902","<classification><nlp><random-forest><word2vec><word-embeddings>","<p>I just start learning NLP technology, such as GPT, Bert, XLnet, word2vec, Glove etc. I try my best to read papers and check source code. But I still cannot understand very well.</p>

<p>When we use word2vec or Glove to transfer a word into a vector, it is like:</p>

<pre><code>[0.1,0.1,0.2...]
</code></pre>

<p>So, one document should be like:</p>

<pre><code>[0.1,0.1,0.2...]
[0.1,0.05,0.1...]
[0.1,0.1,0.3...]
[0.1,0.15,0.1...]
.......
</code></pre>

<p>So, one document is a matrix. If I want to use some traditional method like random forest to classify documents, how to use such data? I was told that Bert or other NLP models can do this. But I am really curious about how the word embedding are applied in the traditional methods? </p>
","nlp"
"60567","Automatically categorize parts of a piece of writing","2019-09-22 00:37:54","","2","37","<nlp>","<p>Suppose I had a piece of writing. The document contains aspects like questions, assertions, examples, and explanations. Is it possible to use Natural Language Processing to categorize each sentence of the writing into such aspects, thus creating a code like 1.1:assertion, 1.2:explanation, for instance, meaning paragraph 1, sentence 1 assertion, and paragraph 1, sentence 2, explanation?</p>

<p>If that is possible, I would appreciate if someone would share their knowledge about known algorithms for performing this task.</p>
","nlp"
"60506","How to validate a clustering model without a ground truth?","2019-09-20 16:14:32","60509","1","474","<machine-learning><nlp><clustering>","<p>Im dealing with a dataset (text messages about source code comments) that are not labeled. I don't have a assumption about the implicits classes in this dataset. I want to discovery (by clustering) the common hidden patterns shared by the groups of messages. This is a unsupervised learning problem. I was asked how i will  validate this method (patterns discovery, clusters) without a dataset of correct answers to measure the output of the model with the ""reality"". Im neither a specialist in the field of the messages dataset to manualy inspect and label the data. So, how to approach this question or provide a scientific explanation about the model output? How to prove that the clusters generated by the model are reasonable or correct?</p>
","nlp"
"60489","Using LSTM for binary text Classification, getting almost same accuracy at each epoch","2019-09-20 09:45:43","","0","490","<classification><keras><nlp><lstm><sentiment-analysis>","<p>I am doing Twitter sentiment classification. For that I am using <strong>LSTM</strong> with pretrained 50d <strong>GloVe</strong> word embeddings(not training them as of now, might do in future).</p>

<p>The tweets are of variable lengths ranging from 1 to 250. Hence I sorted the tweets and divided them into batches of almost similar length. Further I zero padded them with maxLen equal to the highest length of tweet in that particular batch, also in embedding layer I have set 'mask_zero = True'. </p>

<p>I am using <strong>adam</strong> optimizer with default values, and <strong>fit generator</strong> to train the model.</p>

<p>The first issue I observed was that for each epoch initially the accuracy starts increasing and goes till 80, but  when its around halfway through the training set the accuracy starts decreasing and fall to 74 point something. This happens for each epoch. </p>

<p>To solve this I shuffled the batches and then fed them to model, as I thought the model might be adjusting itself to smaller length tweets as those are being fed to it first and hence not generalizing to longer tweets.</p>

<p>After doing this, the accuracy <strong>randomly fluctuates</strong> and again lands on 74 something at the end of each epoch. </p>

<p>I don't know why I am getting <strong>stuck at local minima</strong>, ANY HELP IS MUCH APPRECIATED...:)</p>

<p>I will attach some code if required...</p>
","nlp"
"60384","Sentiment Analysis: using a dataset (IMDB reviews) to train a neural-net and using it to predict entirely different datasets (Political articles)","2019-09-18 12:04:41","","1","84","<machine-learning><neural-network><nlp><sentiment-analysis>","<p>We need to analyse a lot of articles relevant to political instability in a given country (things like the possibility of a coalition / a snap election etc). The problem is that I could not find any labeled datasets which could be plugged into a neural network (CNN/LSTM in TensorFlow) so as to supervise it for real-time events (news articles, tweets etc).</p>

<p>I believe we can't use publicly available big datasets - like IMDB film reviews - for training the models to accurately identify and predict the occurrence of such events (or can we?).</p>

<p>Are there other ways to solve this problem?</p>

<p>I also thought of using unsupervised learning - libraries like VADER - but that gives me a more generic sentiment-score, rather than attuned to the specific corpora relevant to the problem.</p>
","nlp"
"60380","What is the State-of-the-Art open source Voice Cloning tool right now?","2019-09-18 10:51:31","","2","637","<nlp>","<p>I would like to clone a voice as precisely as possible. Lately, impressive models have been released that only need about 10 s of voice input (cf. <a href=""https://github.com/CorentinJ/Real-Time-Voice-Cloning"" rel=""nofollow noreferrer"">https://github.com/CorentinJ/Real-Time-Voice-Cloning</a>), but I would like to go beyond that and clone a voice even more precisely (with subsequent text-to-speech using that voice). It doesn't matter if I have to provide minutes or hours of voice inputs.</p>
","nlp"
"60370","How can I get probabilities of next word with ELMO?","2019-09-18 08:34:54","","1","1229","<neural-network><deep-learning><nlp>","<p>ELMO is a language model, build to to compute the probability of a word, given some prior history of words seen. How can I get this probability from pretained ELMO model?</p>
","nlp"
"60369","Is it possible to create a rule-based algorithm to compute the relevance score of question-answer pair?","2019-09-18 07:09:36","60397","0","136","<nlp><information-retrieval><question-answering>","<p>In information retrieval or question answering system, we use TD-IDF or BM25 to compute the similarity score of question-question pair as the baseline or coarse ranking for deep learning.</p>

<p>In community question answering, we already have the question-answer pairs to collect some statistics info. Without deep learning, could we invent an algorithm like BM25 to compute the relevance score of question-answer pair? </p>
","nlp"
"60366","Why is the result of CountVectorizer * TfidfVectorizer.idf_ different from TfidfVectorizer.fit_transform()?","2019-09-18 06:43:33","","2","558","<python><scikit-learn><nlp><tfidf>","<p>I have a dataframe:</p>

<pre><code>df = pd.DataFrame({'docs': ['gamma alfa beta beta epsilon', 'beta gamma eta',], 'labels': ['alfa alfa beta', 'gamma fi']})
</code></pre>

<p>I use count vectorizer:</p>

<pre><code>import numpy as np
import pandas as pd
from itertools import chain
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

vocab_docs = set(chain(*[i.split() for i in df['docs'].unique()]))
cv_docs = CountVectorizer(vocabulary=vocab_docs)
cv_docs_s = cv_docs.fit_transform(df['docs'])
</code></pre>

<p>I do TFIDF:</p>

<pre><code>tfidf_docs = TfidfVectorizer(vocabulary=vocab_docs)
tfidf_docs_s = tfidf_docs.fit_transform(df['docs'])
# tfidf docs
tfidf_docs_s = tfidf_docs_s.todense()
</code></pre>

<p>but I see that the results are different:</p>

<pre><code>test = np.multiply(cv_docs_s.todense(), tfidf_docs.idf_)

test != tfidf_docs_s 
</code></pre>

<p>Why is the result of CountVectorizer * TfidfVectorizer.idf_ different from TfidfVectorizer.fit_transform()?</p>
","nlp"
"60290","How to justify the usage of 200 dimensions in word vectors instead of the 300 dimensions?","2019-09-16 19:44:02","68090","2","1480","<classification><nlp><word-embeddings><sentiment-analysis>","<p>When employing machine learning methods in NLP, most of studies use 200 or 300 dimensional vectors. 300 dimensional embeddings carry more information and this, therefore, is considered to produce better performance results in general.</p>

<p>If you have unlimited computational resources and the training time is not a problem for you, when does it make sense to use 200 dimensional embeddings instead of 300 dimensional vectors in a classification problem (e.g. in sentiment analysis) and why? </p>

<p>I am assuming that you are using off-the-shelf word2vec, GloVe, or other pretrained vectors. That is, the vectors are <i>not</i> being learnt from scratch in your classification task.</p>
","nlp"
"60266","What is the easiest way to identify a gender for a noun (in french)?","2019-09-16 09:28:08","","2","502","<nlp>","<p>I am working on an app where in order to process some data, I need to be able to identify the gender for some selected words.
My data is in French.</p>

<p>The feature I am looking for should be able to tell me that ""les garçons"" is male plural, ""chienne"" is singular female.</p>

<p>What is the easiest word to do so?
If you have a link to a library do not hesitate.</p>
","nlp"
"60261","Generate new sentences based on keywords","2019-09-16 08:50:27","","4","9135","<nlp><lstm><nlg>","<p>For example, for a domain specific neural network in Fashion, with the Keywords <em>light, dress, orange, cotton</em>. It could output: <em>This gorgeous orange summer dress is great for wearing on sunny camping days. It's cotton fabric makes it very comfortable to wear.</em></p>

<p>Can someone please suggest the simplest way to achieve this?</p>
","nlp"
"60258","NMT, What if we do not pass input for decoder?","2019-09-16 06:52:32","60259","1","306","<deep-learning><nlp><machine-translation><transformer>","<p>For transformer-based neural machine translation (NMT), take English-Chinese for example, we pass English for encoder and use decoder input(Chinese) attend to encoder output, then final output.</p>

<p>What if we do not pass input for decoder and consider it as a 'memory' model for translation.
Is it possible and what will happen?</p>

<p>It seems decoder could be removed and there only exist encoder.</p>

<p>Could I do translation task like text generation?
See:</p>

<p><a href=""https://github.com/salesforce/ctrl/blob/master/generation.py"" rel=""nofollow noreferrer"">https://github.com/salesforce/ctrl/blob/master/generation.py</a></p>

<p><a href=""https://einstein.ai/presentations/ctrl.pdf"" rel=""nofollow noreferrer"">https://einstein.ai/presentations/ctrl.pdf</a></p>
","nlp"
"60238","Infer family type, size from reviews","2019-09-15 15:54:31","60249","1","23","<nlp><data-mining><text-mining>","<p>I have a bunch of reviews:</p>

<pre><code>User_id, review
1, ""We (a family of 4 adults) chose this and view and loved this place""
1, ""My husband and I, with our 2 teen sons, visit this restaurant at least once...""
2,""My partner and I booked table for a short holiday, their wine menu was awesome""
2,""My wife is a fan of jazz and she's expecting, so visited this place ""
</code></pre>

<p>What techniques/packages are available to, for instance, estimate that:</p>

<pre><code>User Id 1 =&gt; family of 4, 2 sons (13-19)
User Id 2 =&gt; family of 2, expecting
:
:
</code></pre>

<p>I have been googling around, to little help, and other than creating my own labeled dataset, I was hoping there are some NLP techniques that can help bootstrap my training set, which can then be curated by humans.</p>
","nlp"
"60229","Google NLP AutoML","2019-09-15 13:44:42","","3","174","<machine-learning><nlp><google><automl>","<p>I am doing research for Google NLP AutoML, What methodologies they have used, techniques, models, feature selection, hyper parameter optimization, etc.</p>

<p>I could not find any paper on how google built their NLP AutoML.</p>

<p>Can anyone guide me on that? how to find google's research on that field for academic research?</p>

<p>Any paper you may have will help.</p>

<p>Thanks</p>
","nlp"
"60225","NLP based Data Preprocessing Method to Improve Disease Name Prediction Using CRF and Word Embedding","2019-09-15 12:18:02","","2","85","<machine-learning><nlp><lstm><word-embeddings>","<p>I built a model( using CRF along bi lstm) to Predict New Disease Name/Entities from medical text data but the problem is Disease name appears only 5,6 times in 1 text file  but on average text file consists of 500-1000 words and this increase False negative (FN).  when i test model on newly/real time Data set its only Predict 60% disease Names (On new dataset).</p>

<p>In total i have 16000 words having  (O-tags which represent Irrelevant words) and only 220 words having B-Disease tags. 
My model precision is 73 % and recall 61% f1 score is 66 %. Parameter on which i trained model</p>

<pre><code>params = {
    'dim': 300,
    'dim_chars': 100,
    'dropout': 0.5,
    'num_oov_buckets': 1,
    'epochs': 75,
    'batch_size': 7,
    'buffer': 15000,
    'char_lstm_size': 25,
    'lstm_size': 100,
    'words': str(Path(DATADIR, 'vocab.words.txt')),
    'chars': str(Path(DATADIR, 'vocab.chars.txt')),
    'tags': str(Path(DATADIR, 'vocab.tags.txt')),
    'glove': str(Path(DATADIR, 'glove.npz'))
}
[file having complete code][1]
</code></pre>

<p>Case #2 : When i trained model only on those lines which include disease Names this cause High False Positive  .</p>

<p>Q1 .Should i remove half lines from every text file so irrevlant dataset decrease. </p>

<p>Q2. Or increase data set without removing half lines from a text file (increase train data set size rather using 80 files  train data on 200 files) As i am using Neural Architectures for Named Entity Recognition and Neural Architectures Want more training data Set </p>

<p>I am using following <a href=""https://github.com/guillaumegenthial/tf_ner"" rel=""nofollow noreferrer"">Code / Library , tf_ner </a> </p>

<p>i am using 2nd model mention in the library  ""chars_lstm_lstm_crf""</p>

<p>This model include :</p>

<pre><code> GloVe 840B vectors

 Chars embeddings

 Chars bi-LSTM

 Bi-LSTM

 CRF
</code></pre>

<p>Related Research Paper 
Neural Architectures for Named Entity Recognition by Lample et al.
==>>>       </p>

<p><a href=""https://arxiv.org/pdf/1603.01360.pdf"" rel=""nofollow noreferrer"">Research Paper</a></p>
","nlp"
"60222","Spacy word embeddings for sentence","2019-09-15 10:41:32","","3","2526","<machine-learning><python><nlp><word-embeddings><spacy>","<p>Spacy offers <a href=""https://spacy.io/usage/vectors-similarity"" rel=""nofollow noreferrer"">pre-trained vectors for words</a>. However I have notices that you can get vectors for sentences too:</p>

<pre><code>spacy_nlp('hello I').has_vector == True
</code></pre>

<p>However I can't figure how it calculates the word2vecs from the sentences. I've tried:</p>

<pre><code>spacy_nlp('hello I').vector == spacy_nlp('hello').vector + spacy_nlp('I').vector
</code></pre>

<blockquote>
  <p>False</p>
</blockquote>

<pre><code>spacy_nlp('hello I').vector/spacy_nlp('hello I').vector_norm == spacy_nlp('hello').vector/spacy_nlp('hello').vector_norm + spacy_nlp('I').vector/spacy_nlp('I').vector_norm
</code></pre>

<blockquote>
  <p>False</p>
</blockquote>

<p>I can't seem to find or work out how spacy computes the w2v for sentences.</p>

<hr>

<pre><code>a =spacy_nlp('hello').vector
a

array([ 2.1919045 , -1.3554063 , -2.0530818 , -1.4123821 ,  0.73116064,
       -0.24243775, -1.238019  , -1.038872  , -3.8119905 ,  0.3023836 ,
        2.0082908 , -0.4146578 ,  0.52871764, -4.171281  , -4.014127  ,
        3.5551465 ,  3.5740273 ,  0.5369273 , -0.92361224,  1.4550962 ,
        2.1736908 , -0.05514041,  0.02151388, -2.1722403 ,  0.81322104,
        3.5877275 , -1.0136521 ,  4.6003613 , -0.19145766,  5.403145  ,
       -1.9958102 ,  0.80248785, -2.3566568 ,  2.15387   ,  0.26684093,
        1.8178961 ,  3.594517  , -2.9950802 ,  2.5587099 , -5.6746616 ,
       -3.7259517 ,  4.0144114 , -1.4814405 ,  1.5888698 , -0.2371515 ,
        0.5498152 ,  0.9527153 , -4.1197095 , -4.252441  , -0.36907774,
       -4.510469  ,  1.2669985 , -0.91693896, -3.0032263 , -4.037157  ,
       -1.986922  ,  1.8322158 , -0.9520336 , -2.6739838 ,  0.368276  ,
        0.5881702 ,  1.4819605 ,  2.1771026 ,  0.20011072, -0.20952749,
       -1.7966032 ,  4.412916  , -0.8781664 ,  3.0670204 ,  3.92986   ,
       -0.7381511 , -0.07432494, -3.6973615 , -3.546731  ,  1.6010978 ,
       -4.0834403 ,  1.7816883 ,  0.8037724 ,  0.40344352, -1.2090104 ,
       -3.3253288 ,  4.6769385 ,  1.3193885 , -1.1775286 , -1.2436512 ,
       -0.29471165,  1.9998071 ,  1.1338542 ,  5.747326  , -0.10331005,
        1.6050186 ,  2.6961374 , -1.9422164 , -3.0807574 , -1.1481779 ,
        7.1367517 ], dtype=float32)

b =spacy_nlp('I').vector
b

array([ 1.9940598e+00, -2.7776110e+00,  8.4717870e-01, -2.1956882e+00,
       -1.6103275e+00,  1.2993972e-01,  8.3826280e-01,  8.7950850e-01,
       -3.5490465e+00,  4.4254961e+00, -1.4894485e+00,  4.4692218e-01,
       -6.0040636e+00,  3.4809113e-01,  7.5852954e-01, -5.0149399e-01,
       -1.9669157e+00,  8.8114321e-01,  5.3964740e-01,  1.6436796e+00,
       -4.3819084e+00,  7.1328688e-01, -8.9688343e-01, -1.2563754e+00,
       -2.6987386e-01,  3.3273227e+00,  7.1929336e-01,  1.2008041e-01,
        2.8758078e+00, -8.6590099e-01,  5.6435466e-01, -5.4331255e-01,
       -3.3853512e+00, -2.0917976e+00, -1.1649452e+00,  8.6632729e+00,
        9.1355121e-01, -3.9117950e-01, -6.3341379e-01, -3.4170332e+00,
        3.2871642e+00,  4.5229197e-03, -4.0161700e+00,  2.6399128e+00,
       -2.4242992e+00, -1.2012237e-01, -1.1977488e-01, -1.6422987e-01,
        7.7170479e-01, -1.5015860e+00, -3.0203837e-01,  1.9385589e+00,
       -2.9229348e+00, -2.8134599e+00, -6.1340892e-01, -2.5029099e+00,
       -6.6817325e-01, -8.4735197e-01,  4.2243872e+00,  2.8358276e+00,
       -2.7096636e+00,  6.3791027e+00,  1.3461562e+00, -3.9387980e+00,
        1.0648534e+00,  5.3636909e-01,  4.1285772e+00, -2.8879738e+00,
        1.3546917e+00, -1.9005369e+00, -3.7411542e+00, -4.8598945e-02,
       -1.4411114e+00,  1.3436056e+00,  1.1946709e+00,  2.3972931e+00,
        2.1032238e+00,  1.8248746e+00, -2.1880054e+00, -1.4601905e+00,
       -1.9771397e+00,  9.3115008e-01, -3.7088573e+00, -4.9041757e-01,
        1.0846795e+00,  2.2863836e+00,  3.5038524e+00,  1.0964345e+00,
        3.6875091e+00, -1.6266774e+00,  1.4012933e-02,  2.7396250e+00,
        3.9477596e+00, -3.5737205e+00,  3.1862993e+00,  2.2955155e+00],
      dtype=float32)

c =spacy_nlp('hello I').vector
c

array([ 2.4846857 , -1.9697192 , -0.09456831, -1.5198507 , -1.6889997 ,
       -0.7867774 , -1.1812011 ,  0.01011622, -2.9120972 ,  3.59254   ,
        1.3454058 , -0.305678  , -2.1474035 , -3.110804  , -0.6446719 ,
        1.9236953 ,  0.88007987,  0.4077559 ,  0.27990723,  0.36027157,
        1.214731  , -0.27636862,  0.33037317, -1.4009418 , -1.7570219 ,
        2.0057924 ,  0.1711272 ,  0.65295005, -0.6732832 ,  1.5165039 ,
       -1.8387947 , -0.49002886, -2.529176  ,  1.0543746 ,  0.13975173,
        6.3513803 ,  3.1074045 , -1.8838222 ,  1.707653  , -3.5569887 ,
        0.02888358,  1.4662569 , -1.4711913 ,  1.6238092 , -0.996526  ,
        0.29157495,  0.7459268 , -2.6089895 , -1.4595604 , -1.6607146 ,
       -1.9626031 ,  0.0429309 , -2.2927856 , -2.7657444 , -2.2093186 ,
       -1.8635755 ,  1.1076405 , -0.87808686, -0.8882728 , -0.20140225,
       -0.14074779,  1.5494955 ,  2.2195954 , -0.8879056 ,  0.16175044,
       -0.47926584,  6.069929  , -2.2804523 ,  1.389133  ,  2.3614829 ,
       -1.6746982 , -0.65907   , -0.88322634, -0.35415757,  1.2424103 ,
       -1.3832704 ,  1.74179   ,  2.0219522 , -0.3940425 , -1.076731  ,
       -3.0649443 ,  2.6106696 , -0.03948617,  0.03465301,  0.6218431 ,
        0.8250919 ,  1.7428303 ,  0.8449378 ,  3.0572054 ,  0.29650444,
        0.4229828 ,  0.38575757,  0.20896101, -0.91772854,  0.3865456 ,
        4.248111  ], dtype=float32)
</code></pre>
","nlp"
"60056","Step extraction from a paragraph","2019-09-11 16:42:32","","2","27","<machine-learning><data-science-model><nlp>","<p>Came across an interesting problem:</p>

<p>Given a paragraph describing how to do a process, need to break it down to various steps. Basically, need to determine for each sentence in the paragraph, if this is start of a new step or belongs to the previous step.</p>

<p>The paragraph here describes a particular process. </p>

<p>Example: How to install a software. So the first step in this case can be the system requirements. All the sentences regarding this need to be in the same step. The sentences thereafter stating how to download this would belong to the next step. The sentences thereafter stating how to install this would belong to the next step.</p>

<p>This is just breaking the paragraph into meaningful steps.</p>

<p>Any idea how to go about this, what algorithm/model to use?</p>
","nlp"
"58946","How to obtain the predictions of SVM model on single input?","2019-09-10 05:50:45","","1","159","<machine-learning><nlp><svm>","<p>So, I am trying to build a Spam detection model. It is trained on a dataset consisting of about 3500 messages. I used SVM to build a model. But, if I now wish to find out whether a message is spam or not for a single line of input message, how would I do it? I tried </p>

<p><code>testmessage = ['Discount ! Free ! guarantee']
x = vectorizer.transform(testmessage)
y = svm.predict(x)</code></p>

<p>But I am getting some error like : </p>

<blockquote>
  <p>ValueError: X.shape[1] = 6 should be equal to 8272, the number of
  features at training time</p>
</blockquote>

<pre><code>import pandas as pd
import numpy as np
import nltk

data = pd.read_csv(""/media/piyush/Disk/Code practice/SPAM_Detction/spam.csv"",encoding ='latin-1')
data = data.drop([""Unnamed: 2"",""Unnamed: 3"",""Unnamed: 4""], axis = 1)

# data.loc[data.v1 == ""spam"", ""v1""] = int(1)
# data.loc[data.v1 == ""ham"", ""v1""] = int(0)


data = data.rename(columns = {'v1': 'label', 'v2': 'text'})

def message_lower(msg):
    msg = msg.lower()
    return msg

from nltk import stem
from nltk.corpus import stopwords
stemmer = stem.SnowballStemmer('english')
stopwords = set(stopwords.words('english'))

def alternative_review_messages(msg):
    # converting messages to lowercase
    msg = msg.lower()
    # removing stopwords
    msg = [word for word in msg.split() if word not in stopwords]
    # using a stemmer
    msg = "" "".join([stemmer.stem(word) for word in msg])
    return msg

data['text'] = data['text'].apply(message_lower)

from sklearn.model_selection import train_test_split


X_train, X_test, y_train, y_test =  train_test_split(data['text'],data['label'], test_size=0.1, random_state= 1)
from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer()
X_train = vectorizer.fit_transform(X_train)


from sklearn.svm import SVC
svm = SVC(C = 3000)
svm.fit(X_train,y_train)
X_test = vectorizer.transform((X_test)) y_pred = svm.predict(X_test)

from sklearn.metrics import confusion_matrix 

print(confusion_matrix(y_test,y_pred))
</code></pre>
","nlp"
"58914","Limitations of StempelStemmer, Polish stemmers","2019-09-09 17:42:28","","1","69","<nlp>","<p>I am trying to understand what is an acceptable output of a stemmer.</p>

<p>I have tried to stem a few words using <a href=""https://lucene.apache.org/core/7_1_0/analyzers-stempel/org/apache/lucene/analysis/stempel/StempelStemmer.html"" rel=""nofollow noreferrer"">StempelStemmer</a>, an algorithmic stemmer for Polish from Apache Lucene. However, it looks it cannot handle some words properly, e.g.</p>

<blockquote>
  <p>joyce -> ąć </p>
  
  <p>wielce -> ąć</p>
  
  <p>piwko -> ąć</p>
  
  <p>royce -> ąć </p>
  
  <p>pip -> ąć </p>
  
  <p>xyz -> xyz</p>
</blockquote>

<ol>
<li><p>I'm surprised it cannot handle Polish words (""wielce"", ""piwko"" and ""pip""). Is this a limitation of (a) the stemming algorithm or (b) the training dataset the algorithm used or (3) something else? </p></li>
<li><p>I am surprised that for non-Polish words it returns ""ać"". I would
expect that for words it has not be trained for it will return their
original forms, as it happens, for instance, when stemming words like
""xyz"". I understand that compared to lemmatizers, stemmers are more crude: they can cut too much. But it looks like instead of cutting it introduces completely different word. Is it a bug?</p></li>
</ol>
","nlp"
"58894","Identifying if the sentence if it comprise information about education","2019-09-09 09:58:47","58895","0","84","<machine-learning><python><machine-learning-model><nlp>","<p>Given a sentence I am trying to classify if the sentence contain information about education. For example:</p>

<pre><code>sentence1 = ""Require minimum four years of professional degree."" 
sentence2 = ""no degree required for this job.""
</code></pre>

<p>For identifying as a first step I have built a vocab which have the set of keywords for identifying education based sentences. I was partly successful until I have problems with sentences like this. </p>

<pre><code>sentence3 = ""BE or BTech or any degree equivalent to it""
</code></pre>

<p>In my vocab 'BE' is also a keyword as 'BE' represents bachelors of engineering degree (in case of country India). As the parsing of algorithm is done in lower case, the issue is coming with 'BE' will be 'be' in sentence. Because of that every sentence that contains 'be' is recognized as educational sentence.</p>

<p>To built a strong ML model I dont have much data. If I want to use vocab based recognition, for this I have to understand the words beside them in the sentence. </p>

<p>Are there any built models so as to import and identify such scenarios or labelled dataset available for it ? 
Are there any methods for accomplishing such task ?</p>
","nlp"
"58861","How to extract location related terms from raw text in python","2019-09-08 15:51:44","61188","1","2571","<python><nlp><named-entity-recognition>","<p>I want to extract location related keywords from raw text in python.
I have already tried spacy but the results were not good and I just got names of countries while I want fine-grained location mentions like streets or neighborhoods in a city.
I also have tried stanford NER but the problem is it is too slow and I need to produce my results with a good speed.
Is there any package or library for python which can solve my problem?
Also if there is any other suggestions which are not for python I will be glad to hear them.</p>
","nlp"
"58794","How does Byte Pair Encoding work on the byte sequence?","2019-09-06 13:51:02","","1","89","<nlp><transformer>","<p>I am reading a paper on OpenAI GPT-2, and in the paper the authors are mentioning that they have performed Byte Pair Encoding (BPE) on the byte sequence themselves, and I am not sure what they meant by that.</p>

<p>How is BPE on the byte sequence different from the regular BPE?</p>

<p>Could you give me some example?</p>

<p>Thank you,</p>
","nlp"
"58788","Simplest way to build a semantic analyzer","2019-09-06 12:26:39","58808","1","45","<machine-learning><machine-learning-model><nlp><model-selection>","<p>I want to build a semantic analyzer i.e., to find how similar the meaning of two sentences are. 
For example-
English: Birdie is washing itself in the water basin.
English Paraphrase: The bird is bathing in the sink.
Similarity Score: 5 ( The two sentences are completely equivalent, as they mean the same thing.)</p>

<p>I have to find the similarity between the meaning of those sentences.</p>

<p>Here is a github repo of what I want to implement.
<a href=""https://github.com/anantm95/Semantic-Textual-Similarity"" rel=""nofollow noreferrer"">https://github.com/anantm95/Semantic-Textual-Similarity</a></p>

<p>Is there any simpler approach?</p>
","nlp"
"58719","NLP to detect duplicates for very technical language","2019-09-05 09:40:05","","0","2005","<machine-learning><deep-learning><nlp><text-mining>","<p>I have the following scenario, to detect duplicate products based on the description fields. The Description Field contains product technical name, dimensions, characteristics. My model needs to consider that different annotation and abbreviations might have been used for technical names, text errors in data entries,  similar/different dimensions or characteristics might still point to the same product. Therefore, I think that applying a normal fuzzy matching or other NLP text matching will not perform well in my case. I trying to approach this problem as a learning/supervised model, but still not sure how so any suggestion/idea is very appreciated.</p>
","nlp"
"58701","Set the number of iterations gpt-2","2019-09-05 05:00:59","","1","41","<python><tensorflow><nlp>","<p>I'm fine tunning a gpt-2 model following this tutorial:</p>

<p><a href=""https://medium.com/@ngwaifoong92/beginners-guide-to-retrain-gpt-2-117m-to-generate-custom-text-content-8bb5363d8b7f"" rel=""nofollow noreferrer"">https://medium.com/@ngwaifoong92/beginners-guide-to-retrain-gpt-2-117m-to-generate-custom-text-content-8bb5363d8b7f</a></p>

<p>With is associated GitHub repository:</p>

<p><a href=""https://github.com/nshepperd/gpt-2"" rel=""nofollow noreferrer"">https://github.com/nshepperd/gpt-2</a></p>

<p>I have been able to replicate the examples, my issue is that I'm not fiding a parameter to set the number of iterations. Basically the training script shows a sample every 100 iterations and save a model version every 1000 iterations. But I'm not finding a parameter to train it for say, 5000 iterations and then close it.</p>

<p>The script for training is here: <a href=""https://github.com/nshepperd/gpt-2/blob/finetuning/train.py"" rel=""nofollow noreferrer"">https://github.com/nshepperd/gpt-2/blob/finetuning/train.py</a></p>

<p>I suspect it should be inside this loop, but really don't get to understand the code:</p>

<pre><code>    try:
        while True:
            if counter % args.save_every == 0:
                save()
            if counter % args.sample_every == 0:
                generate_samples()
            if args.val_every &gt; 0 and (counter % args.val_every == 0 or counter == 1):
                validation()

            if args.accumulate_gradients &gt; 1:
                sess.run(opt_reset)
                for _ in range(args.accumulate_gradients):
                    sess.run(
                        opt_compute, feed_dict={context: sample_batch()})
                (v_loss, v_summary) = sess.run((opt_apply, summaries))
            else:
                (_, v_loss, v_summary) = sess.run(
                    (opt_apply, loss, summaries),
                    feed_dict={context: sample_batch()})

            summary_log.add_summary(v_summary, counter)

            avg_loss = (avg_loss[0] * 0.99 + v_loss,
                        avg_loss[1] * 0.99 + 1.0)

            print(
                '[{counter} | {time:2.2f}] loss={loss:2.2f} avg={avg:2.2f}'
                .format(
                    counter=counter,
                    time=time.time() - start_time,
                    loss=v_loss,
                    avg=avg_loss[0] / avg_loss[1]))

            counter += 1
</code></pre>
","nlp"
"58691","Smallest Possible Dataset for Text Classification using BERT","2019-09-04 19:16:57","","0","944","<nlp><transfer-learning><bert>","<p>What are your experiences for appropriate dataset sizes for usual text classification tasks using a finetuned BERT such as sentiment analysis?</p>

<p>~100 examples</p>

<p>~1000 examples</p>

<p>...</p>

<p>~10000000 examples</p>

<p>What are your experiences?</p>
","nlp"
"58677","Skip-gram trained on The Hobbit: no improvement in the similarity of the word representation","2019-09-04 14:52:51","","1","27","<neural-network><nlp><language-model>","<p>I've trained a simple skipgram NNLM (window size = 5) on The Hobbit. This is the rough pseudocode: </p>

<pre><code>for e in  epochs:  
    for i in idx:   
     loss_batch = 0
     for b in batch_size:
         input = data[i+b]
         output = nnlm(input)
         loss_window = neg_log_lhd(output, target)
         loss_batch += loss_window

     loss_batch /=batch_size  
     loss_batch.backward()
</code></pre>

<p><code>idx</code> here is the index of the first input word in the batch. Loss is accumulated over the whole batch (if <code>batch_size</code>=128, it means there are 128 sliding windows for which the loss is calculated) and backpropagated before moving to the next one.</p>

<p>The model seems to converge, judging by the curve of the loss function, but I'm concerned about the vector representation of words. I computed cosine similarity for 4 different words: 'bilbo','baggins', 'gandalf', 'thorin'. I expected the similarity between the first two to be very high, as they are used interchangeably, or very frequently together. Nevertheless, it starts with <code>-0.042</code> and is <code>-0.04</code> after 40 epochs (~400 batches/epoch). Obviously something's wrong here.</p>

<p>What could possibly be the reason for the situation when the model converges, but the distance between word embeddings doesn't change?    </p>
","nlp"
"58661","Getting answers to bullets (numbered items) from text via NLP","2019-09-04 11:31:20","","1","443","<nlp><information-retrieval>","<p>This is related to information extraction. In real world data, documents are written in bullets/numbered items form. For example,   </p>

<pre><code>How to create a website:  

 - Get A DNS
 - Get a Hosting 
 - Deploy wordpress or some site ...
</code></pre>

<p>above is sample of a structured data. Take another example where content is semi structured,</p>

<pre><code>While sandeep was going to home there was a road on the way he saw a 

 - Car
 - 2 wheeler
 - cart     

and he carefully crossed the road
</code></pre>

<p>If I have to find out the ""steps to create a website"" or ""what is on the road""  .. is there an established method using NLP? As some data is semi structured, so simple classification may not work in this case.<br>
Maybe deep learning network or some pretrained model? I have seen google is able to return such results when we search something like ""what are steps create a website"".</p>
","nlp"
"58629","Punctuation removal effect on n gram detection for keyword generation","2019-09-03 20:00:06","","3","442","<nlp>","<p>I'm building ngrams after removing punctuation and lemmatization. The algorithm is to detect keywords in large bodies of text. </p>

<p>I have concerns that 2 documents</p>

<blockquote>
  <p>The child played with the red ball.</p>
</blockquote>

<p>And</p>

<blockquote>
  <p>The sign was red. Balls are the toys of children.</p>
</blockquote>

<p>would both contain ""red ball"". Are there best practices here? Ideally I would not want the second document to have the same value of ""red ball"" as the second.</p>
","nlp"
"58560","Models after word2vec outputs","2019-09-02 16:45:03","","0","98","<nlp><word2vec><word-embeddings><gensim>","<p>I am originally using a bag of word (2-gram) model to approach a classification problem. The one hot encoding of the 2-gram output was sent to a logistic regression or neural network to build a classification model.</p>

<p>Now, I am experimenting the gensim word2vec approach, each word is now a vector from word2vec. That is, if my sentence have 10 words, it would become a 10x30 array (assume word2vec embedding dimension is 30). It's not clear to me how do I send such outputs to a logistic regression or neural network model like before ...</p>

<p>What type of model should I use after the gensim word2vec outputs to solve a classification problem? Thanks!</p>
","nlp"
"58380","How to convert sequence of words in to numbers which are input to RNN/LSTM?","2019-08-29 12:08:54","58382","0","1590","<nlp><lstm><rnn>","<p>I am watching online videos and tutorials about use of RNN/LSTM for NLP but none of them explain how to convert the sequences of words into digitized input to the neural networks? </p>

<p>I am looking for intuitive understanding but answers with python code are also welcome.</p>

<p>For example, how do I input; <strong>""grass is green and sun is hot""</strong> to my RNN/LSTM? </p>
","nlp"
"58372","Extract imperative sentences from a document(English) using NLP in python","2019-08-29 09:09:21","58395","1","1486","<python><nlp>","<p>I am very new to NLP, hence require some help on extracting imperative sentences from a document. I am working on a project where I need to get all the imperative sentences from the entire document(English documents). I understand I need to use POS tagging. But how do I proceed further.</p>

<p>Thanks.  </p>
","nlp"
"58324","Sentiment Analysis for Q&A based reviews","2019-08-28 13:05:18","","1","100","<machine-learning><deep-learning><nlp><text-mining><sentiment-analysis>","<p>I'm a self-learning ML enthusiast and I recently started learning NLP and performing Sentiment Analysis on imdb, yelp, amazon datasets(using Python). I came across a dataset where the reviews were in the form of Q&amp;A's for each product. 
1) What do you like about the product?
2) What do you dislike about the product?
3) What would you recommend to improve?
4) What did you use this product for?</p>

<p>I was wondering if Sentiment Analysis can be performed on this type of data or not? If yes, any tips/pointers/links to reading material in the right direction would be much appreciated. Also, there was no overall rating given for the product, so I stuck on that aspect as well. </p>

<p>-Luc. </p>
","nlp"
"58136","How do I use NLTK Text object with Re library?","2019-08-24 16:46:14","58201","1","275","<nlp><nltk>","<p>I am working on to build a <strong>bag of words</strong> model from my text file. I I want to use the <code>re.sub</code> function from the re library. I am getting the following error;</p>

<blockquote>
  <p>TypeError: expected string or bytes-like object</p>
</blockquote>

<p>I have coded the following;</p>

<pre><code>import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

"""""" importing data set """"""
from nltk.corpus import PlaintextCorpusReader
corpus_root = './'
wordlists = PlaintextCorpusReader(corpus_root, 'evidence1.txt')
wordlists.fileids()
wordlists.words('evidence1.txt')
stringx = wordlists.words(wordlists.fileids()[0])
print (stringx) 

"""""" cleaning the texts """"""
from nltk.text import Text
text = Text(stringx)
print(text)

import re
lowerx = re.sub('[^a-zA-Z0-9]','',text)
</code></pre>

<p>I think I need right kind of object to pass on to <code>re.sub</code></p>
","nlp"
"58126","Error while using NLTK/ How should I read paragraph using NLTK?","2019-08-24 13:23:50","","0","51","<nlp><nltk>","<p>I am a total newbie. I have a custom text document which is a paragraph. I am trying to read it and display using NLTK. </p>

<p>following is my code;</p>

<pre><code>import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from nltk.corpus import PlaintextCorpusReader

corpus_root = ''
wordlists = PlaintextCorpusReader(corpus_root, 'evidence.txt')
wordlists.fileids()
wordlists.words('evidence.txt')
print(wordlists.words('evidence.txt'))
</code></pre>

<p>I am getting an error;</p>

<blockquote>
  <p>UnicodeDecodeError: 'utf-8' codec can't decode byte 0x92 in position
  19: invalid start byte</p>
</blockquote>

<p>please help.</p>
","nlp"
"58116","Extracting tokens from a document: applying Deep Learning or Classification?","2019-08-24 09:38:04","58125","1","63","<deep-learning><classification><nlp>","<p>I have a legal document from Law. That document is 4-pages of evidence from the plaintiff. I want to identify the Dates, Addresses and Financial transactions in that document.</p>

<p>Can I apply deep learning, the data with me is very small, on just one 4-page document, or should I apply Text Classification to solve my problem?</p>
","nlp"
"58043","NLP: Mapping Penn treebank and Brown corpus, to Universal PoS Tags","2019-08-22 20:18:54","","0","506","<deep-learning><classification><nlp>","<p>I am experimenting with NLP and PoS tagging. I wish to build a large corpus, composed of Penn Treebank and Brown corpus, and possibly even more. Unfortunately, their PoS tags are not compatible.</p>

<p>Is there a way to map their tags to universal tagging? (Or any other tagging?)</p>
","nlp"
"57988","How to validate a chatbot?","2019-08-22 03:09:14","","1","120","<deep-learning><nlp><chatbot>","<p>Let's say I'm building a medical assistance chatbot.</p>

<p>How do I validate that my model is working well or better than others. </p>

<p>Is there any standard validation metrics for it ?</p>
","nlp"
"57961","How do I identify specific parts of a PDF document?","2019-08-21 13:28:43","","1","64","<keras><nlp><image-recognition><object-detection><text>","<p>I have a bunch of medical records that I have to input manually. I would like to automate this but all of the records are in different formats. What is the best strategy to build a deep learning model to extract this information automatically? Does it make sense to use something like YOLO to recognize specific parts of the image and then Tesseract to actually perform OCR? A bit of guidance would be much appreciated. Also, I have seen things like Amazon Textract but am looking for something I can build and run locally without depending on another service. Thanks!</p>
","nlp"
"57947","Training the document page layout and classifying good/bad layouts","2019-08-21 10:15:52","","0","313","<machine-learning><deep-learning><nlp><cnn><rnn>","<p>I have a use case where I am supposed to get the coordinates of each block element in a page (whether its paragraph, image, table) where I train a model to understand how they are placed in a given page where some documents are with good layout and other with bad ones and I want to train this and throw in some coordinates of a new document and try to understand whether it has a good layout or a bad layout, I want to understand how I can achieve this using some deep learning techniques ?</p>

<p>can someone suggest me an approach for solving this?</p>

<p>Was trying to workout with RNN but not sure if that's the correct approach.</p>
","nlp"
"57928","Why are bigger embedding vectors not necessarily better?","2019-08-21 07:21:07","57936","1","1499","<machine-learning><deep-learning><word2vec><word-embeddings><nlp>","<p>I'm wondering why increasing the dimension of a word dimension vector in NLP doesn't necessarily lead to a better result. For instance, on examples I run, I see sometimes that using a pre-trained 100d GloVe vector performs better than a 300d one. Why is this the case? Intuitively a larger dimension should become almost like a one-hot encoding and be in some more ""accurate,"" no? </p>
","nlp"
"57890","Given two large corpora of text from different sources, is there an accepted way to get differences in vocabulary (n-grams) between them?","2019-08-20 15:24:24","","1","52","<nlp><text-mining><tfidf>","<p>Given two large corpora of text from different sources, is there an accepted way to get differences in vocabulary (n-grams) between them?</p>

<p>That is, to get results which say that, for example, the bigram ""hello world"" is much more common in corpus A than corpus B (ideally with some kind of measure of how much more common).</p>

<p>TF-IDF examples use a larger number of documents to highlight ""important"" words in each, but I am not sure if that would work in this case?</p>
","nlp"
"57861","Why is data science not yet widely applied to Law?","2019-08-20 09:55:10","57870","2","180","<nlp>","<p>Law (judiciary) contains such a huge corpus to apply NLP to, but yet there are only search engines designed for Law. Why is NLP not yet extensively applied? Is it because of dimensionality? </p>
","nlp"
"57769","How does a CBoW model convert a word to a vector?","2019-08-19 04:21:45","","2","228","<python><deep-learning><nlp>","<p>A CBOW model actually takes multiple words as inputs and a targeted central word as the output.<br>
So, the trained model actually maps several words to a single one, I mean it takes context words and outputs the central word.
But what we expected to get is model mapping a word to its vector representation. It seems the output is coincident but not the input and the mapping.</p>

<p>So like in genism, how it really works to map a word to its vector representation? 
Does it just save all final model's outputs as the central words' vector representation? But the final model's outputs would close to the ground truth's one-hot embedding rather than a vector with context information.</p>

<p>For short, my question is:<br>
<strong>How does a CBoW model convert one word to its vector representation?</strong></p>
","nlp"
"57764","How to combine nlp and numeric data for a linear regression problem","2019-08-18 23:42:28","","1","2354","<scikit-learn><nlp><linear-regression><tfidf>","<p>I'm very new to data science (this is my hello world project), and I have a data set made up of a combination of review text and numerical data such as number of tables. There is also a column for reviews which is a float (avg of all user reviews for that restaurant). So a row of data could be like:</p>

<pre><code>{ 
    rating: 3.765, 
    review: `Food was great, staff was friendly`, 
    tables: 30, 
    staff: 15, 
    parking: 20
    ... 
}
</code></pre>

<p>So following tutorials, I have been able to do the following:</p>

<ol>
<li>Created a linear regression model to predict rating with the inputs being all the numerical data columns.</li>
<li>Created a regression model to predict rating based on review text using sklearn.TfidfVectorizer.</li>
</ol>

<p>But now I'd like to combine models or combine the data from both into one to create a linear regression model. So how can I utilize the vectorized text data in my linear regression model?</p>
","nlp"
"57685","Where can I find dataset for word analogy task?","2019-08-16 19:04:50","57933","1","1693","<nlp><dataset><word2vec>","<p>In the paper of Word2Vec by Thomas Mikolov and others, there is a accuracy report on the full Semantic-Syntactic data set. Where I can find this dataset or a related dataset for word analogy task? </p>

<p>Here is the paper link and I'm referring to table 5. 
<a href=""https://arxiv.org/abs/1301.3781"" rel=""nofollow noreferrer"">https://arxiv.org/abs/1301.3781</a></p>
","nlp"
"57650","Is there any way to define custom entities in Spacy","2019-08-16 11:43:49","57661","6","4382","<python><nlp><spacy>","<p>1) I have just started working on NLP the basic Idea is to extract meaningful information from text. For this I am using ""Spacy"".</p>

<p>As far as I have studied Spacy has following entities.</p>

<ul>
<li>ORG</li>
<li>PERSON</li>
<li>DATE</li>
<li>MONEY</li>
<li>CARDINAL</li>
</ul>

<p>etc. But I want to add custom entities like:</p>

<p><code>Nokia-3310</code> should be labeled as <code>Mobile</code>
and <code>XBOX</code> should be labeled as <code>Games</code> </p>

<p>2) Can I find some already trained models in Spacy to work on ?</p>
","nlp"
"57624","Document Readability","2019-08-15 23:44:26","","1","23","<python><nlp>","<p>What are the ways to determine the level of difficulty w.r.t. the complexity of concept for any given document ?</p>

<p>I am aware of Flesch Reading Ease Score, but that's generic and based on word count and syllables count. </p>

<p>Is there anything that works at the concept level ?</p>

<p>Thanks!</p>
","nlp"
"57566","Is there a way to rank the Extracted Named Entities based on their importance/occurence in a document?","2019-08-14 18:37:25","","5","242","<nlp><named-entity-recognition>","<p>Looking for a way to rank the tens and hundreds of named entities present in any document in order of their importance/relevance in the context. </p>

<p>Any thoughts ?</p>

<p>Thanks in advance!</p>
","nlp"
"57489","What is ChunkParserI in nltk.chunk ? What exactly it has been called for?","2019-08-13 11:23:19","","1","249","<nlp><nltk><parsing>","<pre><code>from nltk.chunk import ChunkParserI 
from nltk.chunk.util import conlltags2tree 
from nltk.corpus import gazetteers 

class LocationChunker(ChunkParserI): 
    def __init__(self): 
        self.locations = set(gazetteers.words()) 
        self.lookahead = 0
        for loc in self.locations: 
            nwords = loc.count(' ') 
        if nwords &gt; self.lookahead: 
            self.lookahead = nwords 
</code></pre>

<p>What is ChunkParserI  in nltk.chunk ? What exactly it has been called for?
Also, please explain the code.
What is the difference between chunking and parsing?</p>
","nlp"
"57449","NLP Feature creation from phrase matching","2019-08-12 18:02:44","57901","1","105","<python><nlp><feature-construction>","<p>I'm building a model to classify email content, to decide whether the email should lead to a JIRA ticket being ""Raised"" or ""Not Raised"". The problem I am having is the data is highly imbalanced with only around 11% being classed as ""Raised"". So far, the Random Forest classifier is providing the highest level of accuracy but the True Positive Rate/Recall is sitting at around 40% and I can't seem to increase upon this. I have been provided with a list of phrases that should they be contained in the email content, then in all likelihood a ticket needs raising. Looking for some tips as to the best method to create a new feature based on phrase matching? Has anyone any experience in the best methods for doing this?</p>
","nlp"
"57364","Is hyperparameter tuning more affected by the input data, or by the task?","2019-08-10 19:32:02","57392","1","75","<keras><nlp><optimization>","<p>I'm working on optimizing the hyperparameters for several ML models (FFN, CNN, LSTM, BiLSTM, CNN-LSTM) at the moment, and running this alongside another experiment examining which word embeddings are best to use on the task of binary text classification.</p>

<p>My question is: should I decide on which embeddings to use before I tune the hyperparameters, or can I decide on the best hyperparameters and then experiment with the embeddings? The task remains the same in both cases.</p>

<p>In other words, is hyperparameter tuning more affected by the task (which is constant) or by the input data?</p>
","nlp"
"57326","How to use a ragged tensor with a convolutional layer?","2019-08-09 17:33:30","","4","624","<deep-learning><tensorflow><nlp>","<p>I have textual data of various lengths for which ragged tensors seems well suited. For instance my data could look as follows :</p>

<pre><code>x = tf.ragged.constant([[1,2,3,4,5,6,7], [5,6,1,2]])
</code></pre>

<p>I would like provide this ragged tensor to a model composed by some convolutional filters, let's say at least one filter as follows : </p>

<pre><code>model = Sequential([Embedding(alphabet_size, embedding_dim),
                    Conv1D(filters=10, kernel_size=10),
                    GlobalMaxPooling1D()])
</code></pre>

<p>I tried to use <code>tf.ragged.map_flat_values</code> however I am not sure that it does what I would like, i.e : embedding each text line of the batch, convolving it, and then taking the max over each text line.</p>

<p>Is there a workaround to make this model work on (very) variable lengths texts (except of course using 3d tensors batches)?</p>
","nlp"
"57283","how to work with NLP with other features","2019-08-09 01:54:07","","2","72","<classification><nlp>","<p>My dataset looks like this</p>

<pre><code>Sport_Type       City         Report_Text                                                 Labels
Ball             Toronto      Messi has been announced the best soccer player...          Soccer
Swimming         London       Todays new records in Butterfly Stroke &amp; Backstroke...      Butterfly Swimming, Backstroke Swimming, Front Crawl
Ball             Chicago      Tennis and basketball along with football has...            Tennis, basketball, Soccer
Fighting         Sydney       Todays matches include boxing, judo, and...                 Boxing, Judo, Karate
Horse            Melbourne    Melbourne Cup is the race that stops the nation...          Horse Racing
</code></pre>

<p>I can build multi label model to identify labels in each <code>Report_Text</code> field.</p>

<p>but is there a way I can consider <code>Sport_Type</code> and <code>City</code> field in my model as it will help in improving results.</p>

<p>How can I use other features such as <code>Sport_Type</code> and <code>City</code> in <strong>NLP multi label</strong> model?</p>
","nlp"
"57272","How to properly use approximate_predict() with HDBSCAN clusterer for text clustering (NLP)?","2019-08-08 21:19:15","","1","1644","<nlp><clustering><dbscan>","<p>I have approached text clustering using HDBSCAN based on <a href=""https://cai.tools.sap/blog/text-clustering-with-r-an-introduction-for-data-scientists/"" rel=""nofollow noreferrer"">this article</a> which describes how to do this in R. I've re-written this in Python using <a href=""https://hdbscan.readthedocs.io/en/latest/index.html"" rel=""nofollow noreferrer"">this library</a>. The approach is to first calculate TF-IDF vectors for the documents, then calculate a distance matrix for all vector pairs and fit the HDBSCAN clusterer based on the distance matrix.</p>

<p>I have fitter the clusterer with a subset of my documents since the algorithm is slow and my whole set is a bit big. I've limited it to 5000 samples. The clusters that HDBSCAN has found are acceptable. I will fine-tune them later.</p>

<p>Now, I would like to create a Python method that would take a new document, not being a part of the original training set, and return the cluster label which the new document seems to belong to.</p>

<p>I have approached this task by trying to use <a href=""https://hdbscan.readthedocs.io/en/latest/prediction_tutorial.html"" rel=""nofollow noreferrer"">approximate_predict()</a>. This is where I have question.</p>

<p>I suspect the process for calculating the cluster label for the new document look like this:</p>

<ol>
<li>Add the new document to the set of 5000 samples that I've used for the clusterer training</li>
<li>Calculate the disctance matrix for the 5001 samples (the matrix will be bigger than the matrix used for clusterer fitting)</li>
<li>Take the last row of the resulting matrix (should correspond to my new text) and remove the last element from the resulting vector (it should contain the distance of the new document to itself, which we can ignore). Removing the last element is to make the dimension of the last vector match the dimension of the matrix used to fit the clusterer. Otherwise the clusterer would complain.</li>
<li>Use the approximate_predict() method and pass the vector obtained in step 3 to get the cluster label.</li>
</ol>

<p><strong>My questions are:</strong></p>

<ol>
<li>Is my approach correct? (it seems overcomplicated but I don't know what it should look like)</li>
<li>Will it perform well in production when I start passing lots of documents to this method? (the processing needed before I actually call approximate_predict() seems huge)</li>
<li>How can it be done differently?</li>
<li>Would it be better to not use the approximate_predict() method, but instead take the cluster labels that HDBSCAN calculated for my my 5000 samples and use it for supervised learning to train a classifier to then classify new documents?</li>
</ol>

<p>I am looking forward to your answers and I will appreciate them a lot.
I am completely new to this and I don't have anybody around me to discuss this.</p>
","nlp"
"57254","Metrics for unsupervised doc2vec model","2019-08-08 17:01:44","57311","3","867","<python><nlp><word2vec><unsupervised-learning><gensim>","<p>I have just built a simple doc2vec model using the gensim library, pretty much followed the tutorial located <a href=""https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb"" rel=""nofollow noreferrer"">here</a>.</p>

<p>The methods provided for checking the quality of the model are very manual and require reading the similar docs, is there a way to calculate some other metrics from the model to try and improve its performance?</p>
","nlp"
"57191","Is there a good German Stemmer?","2019-08-08 06:31:30","","5","8838","<nlp><nltk><stemming>","<p><strong>What I tried:</strong></p>

<pre><code># -*- coding: utf-8 -*-

from nltk.stem.snowball import GermanStemmer
st = GermanStemmer()

token_groups = [([""experte"", ""Experte"", ""Experten"", ""Expertin"", ""Expertinnen""], []),
                ([""geh"", ""gehe"", ""gehst"", ""geht"", ""gehen"", ""gehend""], []),
                ([""gebäude"", ""Gebäude"", ""Gebäudes""], []),
                ([""schön"", ""schöner"", ""schönsten""], [""schon""])]
header = ""{:&lt;15} [best expected: n/n| best variants: 1/n | overlap: m]: ..."".format(""name"")
print(header)
print('-' * len(header))
for token_group, different_tokens in token_groups:
    stemmed_tokens = [st.stem(token) for token in token_group]
    different_tokens = [st.stem(token) for token in different_tokens]
    nb_expected = sum(1 for token in stemmed_tokens if token == token_group[0])
    nb_variants = len(set(stemmed_tokens))
    overlap = set(stemmed_tokens).intersection(set(different_tokens))
    print(""{:&lt;15} [as expected: {}/{}| variants: {}/{} | overlap: {}]: {}"".format(token_group[0], nb_expected, len(token_group), nb_variants, len(token_group), len(overlap), stemmed_tokens))
</code></pre>

<p><strong>what I get:</strong></p>

<pre><code>experte  [as expected: 0/5| variants: 3/5 | overlap: 0]: ['expert', 'expert', 'expert', 'expertin', 'expertinn']
geh      [as expected: 3/6| variants: 4/6 | overlap: 0]: ['geh', 'geh', 'gehst', 'geht', 'geh', 'gehend']
gebäude  [as expected: 0/3| variants: 1/3 | overlap: 0]: ['gebaud', 'gebaud', 'gebaud']
schön    [as expected: 0/3| variants: 1/3 | overlap: 1]: ['schon', 'schon', 'schon']
</code></pre>

<p>The two main problems are:</p>

<ul>
<li>Overlaps: schön != schon</li>
<li>Non-working stemming, e.g. [experte, expertin, expertinnen], [ich gehe, du gehst, er geht]</li>
</ul>

<p>A not so serious side problem is matching my expectations. So if the stemmer could actually bring the word in a basic form (not simply the stem), then it would be easier to analyze.</p>

<h2>More Examples</h2>

<h3>Clashes</h3>

<ul>
<li><strong>Input -> Output != clash</strong></li>
<li>mittels -> mittel != ""Das Mittel""</li>
</ul>

<h3>Unmatched expectations</h3>

<ul>
<li><p><strong>Input -> Output / expected</strong></p></li>
<li><p>Mädchen -> madch / Mädchen</p></li>
<li>Behaarung -> behaar / Behaarung</li>
</ul>
","nlp"
"57140","How do you integrate nlp into an existing website?","2019-08-07 14:15:05","57151","0","78","<nlp>","<p>Let's say I have a db driven app which has two tables: people and organizations. I run my documents through a named entity recognition program. Now - what do I do with this additional ner information? </p>

<p>Does it go in my existing db tables?
Do I have to rewrite my existing tables to accommodate the ner data?
Do I make new tables for the ner data?
Do I connect them to my older tables with foreign keys?
Do I need new functions that pull response data from these new ner objects instead of, or with, my original ones?
Are these new entities ""objects"" in the traditional OOP sense? 
But they are different objects from the ones in my original db tables, so do they replace the old, non-ner objects?
Does it matter what kind of datastore the new ner objects go into in order to be fully utilized? rmdbs? key/value? document based?</p>

<p>The only thing I’ve been able to find online about this is a Medium post that pickles the model - it says <em>nothing</em> about any other kind of datastore - and the Flask website just displays the information by accessing it through an api. </p>

<p>A search here on datascience.stackexchange.com got this:</p>

<blockquote>
  <p>We couldn't find anything for integrate with existing website</p>
</blockquote>

<p>And now I get a warning that my question ""appears to be subjective and is likely to be closed"". I'm sure that's machine learning at work. Granted, the step by step details might be different if your site is php vs python, but the issue is a major one I assume a lot of people are going to run into sooner if not later. My site is in Django.</p>

<p>How do you integrate the product of nlp into an existing, complex website with many apps and functions?</p>
","nlp"
"57076","How to segment old digitized newspapers into articles","2019-08-06 15:52:31","","2","78","<nlp><text-mining><ocr>","<p>I'm working on a large corpus of french daily newspapers from the 19th century that have been digitized and where the data are in the form of raw OCR text files (one text file per day). In terms of size, one year of issues is around 350 000 words long.</p>

<p>What I'm trying to achieve is to detect the different articles that form a newspaper issue. Knowing that an article can be two or thee lines long or very much longer, that there is no systematic typographic division and that there are a lot of OCR errors in each file.
I should also mention that I don't have access to others OCR data like document layout in XML or so.</p>

<p>I've tried the <a href=""https://www.aclweb.org/anthology/J97-1003"" rel=""nofollow noreferrer"">TexTiling</a> algorithm (the nltk implementation) but the results were not really conclusive.</p>

<p>Before diving deeper by myself I was wondering if maybe some of you would have some hint about a task like this one : train a machine learning model, try others algorithms ?</p>
","nlp"
"57025","Why TREC set two task: document ranking and passage ranking","2019-08-06 03:29:50","57051","1","70","<deep-learning><nlp><information-retrieval>","<p>TREC is <a href=""https://microsoft.github.io/TREC-2019-Deep-Learning/"" rel=""nofollow noreferrer"">https://microsoft.github.io/TREC-2019-Deep-Learning/</a></p>

<p>I am new to text retrieval. Still can not understand why set the two similar task.</p>

<p>Thank you very much.</p>
","nlp"
"57015","Classification of substrings?","2019-08-05 22:26:13","","3","46","<nlp>","<p>What is the appropriate method to find n-grams/sub-phrases/parts-of-sequences that are referring to a specific topic or belong to a certain category?</p>

<p>For instance:</p>

<p>Imagine a topic of ""transfer of ownership"" and some example sentences:</p>

<ul>
<li>A >change of ownership&lt; occurs when a title is transferred from one person or entity to another</li>
<li>Making a >change in business ownership&lt; is a lengthy and complex process</li>
<li>In this respect, the >transfer of legal ownership&lt; is not the relevant feature for determining the treatment of repo-like operations</li>
</ul>

<p>What I am looking for is a method to identify sentences and ideally parts of sentences which refer to a specific theme / topic.</p>

<p>What NLP methods are appropriate?</p>
","nlp"
"56955","Guidelines to debug REINFORCE-type algorithms?","2019-08-05 05:51:23","","2","337","<reinforcement-learning><pytorch><nlp><policy-gradients>","<p>I implemented a self-critical policy gradient (as described <a href=""https://arxiv.org/pdf/1705.04304.pdf"" rel=""nofollow noreferrer"">here</a>), for text summarization. </p>

<p>However, after training, the results are not as high as expected (actually lower than without RL...).</p>

<p><strong>I'm looking for general guidelines on how to debug RL-based algorithms.</strong></p>

<hr>

<p>I tried :</p>

<ul>
<li><strong>Overfitting on small datasets</strong> (~6 samples) : I could increase the average reward , but it does not <em>converge</em>. Sometimes the average reward would go down again.</li>
<li><strong>Changing the learning rate</strong> : I changed the learning rate and see its effect on small dataset. From my experiment I choose quite big learning rate (<code>0.02</code> vs <code>1e-4</code> in the paper)</li>
<li><strong>Looking at how average reward evolve as training (on full dataset) goes</strong> : Average reward significantly does not move...</li>
</ul>
","nlp"
"56950","Sentiment Analysis of News Headlines","2019-08-05 03:53:22","","1","638","<nlp><text-mining><sentiment-analysis><spacy>","<p>I'm trying to do sentiment analysis of News Headlines about a particular subject mentioned in it.</p>

<p>Initially, I used <code>TextBlob</code> library for sentiment analysis to generate a polarity score. But the polarity score being generated for news headlines are not accurate. It is classifying negative news as positive news.</p>

<p>For example: Goldman Sachs CEO apologizes to people of Malaysia. This news is being classified as positive.</p>

<p>Post that, I've tried to build a custom model using <code>spaCy</code> library. I trained the custom model on 500 manually labelled news headlines. After training the model, I ran it on new set of headlines. The accuracy has improved but there is still scope for improvement. The accuracy is particularly poor with regard to financial news and editorial headlines.</p>

<p>My expectations is to create a model that can accurately predict the sentiment of news headline. So that I can plot sentiment trend about the subject over a time period.</p>
","nlp"
"56949","What can NLI do for a chatbot?","2019-08-05 03:33:16","57525","-1","165","<deep-learning><nlp>","<p>Natural Language Inference(NLI) is the task of predicting the labels(entailment, contradiction, and neutral,) for sentence pairs.</p>

<p>People invent a lot of deep model to solve this problem.</p>

<p>But I can not think of some application or scenario for this kind of deep model.</p>
","nlp"
"56872","For text match problem, what is the different between question-question match and question-answer match?","2019-08-03 01:26:20","57535","1","69","<deep-learning><nlp>","<p>I know question-question match is a text similarity problem.</p>

<p>What about question-answer or question-doc match? It is used in information retrieval.</p>

<p>question-question match is indeed text similarity. But how do you define question-answer similarity? </p>

<p>Thank you!!</p>
","nlp"
"56844","How to feed data for ngram model?","2019-08-02 14:39:40","56846","1","574","<nlp><dataset><language-model><ngrams>","<p>I want to train an ngram language model</p>

<p>Let's say I have the following corpus:</p>

<pre><code>The sliding cat is not able to dance
He is only able to slide
Because obviously he is the sliding cat
</code></pre>

<p>I am planning to use tf.data.Dataset to feed my model, which is fine</p>

<p>But I don't know if it is better to use a sliding window to iterate through my copus or simply feed my corpus n words at a time</p>

<p>Using a sliding window, my model (assuming a bigram) will see:</p>

<pre><code>The sliding
sliding cat
cat is
is not
...
</code></pre>

<p>Going n word at a time:</p>

<pre><code>The sliding
cat is
not able
...
</code></pre>

<p>I'd appreciate any recommandation, thanks</p>
","nlp"
"56761","Representation options of strings (keywords/topics) in models","2019-08-01 15:11:04","","1","52","<nlp><word-embeddings><topic-model><javascript>","<p>What are all the possible ways to represent keywords in a machine learning model?</p>

<p>The two I am aware of are:</p>

<ul>
<li>one hot encoding, using a static index. </li>
<li>vector representation, using
an embedding layer.</li>
</ul>

<p>We have a specific problem where we are doing client side (browser) ml and need to convert text data into something the model can consume without sending it over to the server.</p>

<p>EDIT: (comment clarification)
The text data is extracted from the page on which our script loads, we then want to run a model locally on the browser - using text based features from the page (ideally). We are minimising in all elements any data needed to sent to the server.</p>

<p>In terms of model, that is not defined at this stage, also this question primary concerned with representation of text based features.</p>
","nlp"
"56652","How can one determine that Word2Vec (CBOW method) embeddings are related to each other?","2019-07-31 03:51:20","","1","19","<neural-network><word2vec><nlp>","<p>I read some fascinating stuff about the potential for using the Word2Vec algorithm to speed up the pace of scientific discovery here <a href=""https://www.researchgate.net/publication/334209824_Unsupervised_word_embeddings_capture_latent_knowledge_from_materials_science_literature"" rel=""nofollow noreferrer"">https://www.researchgate.net/publication/334209824_Unsupervised_word_embeddings_capture_latent_knowledge_from_materials_science_literature</a>. In the paper, the researchers trained the algorithm on millions of materials science abstracts to identify potential associations that human researchers had missed.</p>

<p>My question concerns using the Word2Vec in contexts that are more disparate than a single scientific field. The CBOW method takes in an input of words and returns a predicted word that would most likely be associated with it. So if I had trained the algorithm on economics papers, an input of externalities, public good, and tragedy of the commons might return a prediction of carbon pollution. However, had I trained on ecology papers and used the same input, I would not get the same output, and the input might well be meaningless due to the lack of those words in the training set. The question then becomes, how can one tell if the embedding of a word in one field, is similar to that in another field? The application I am imagining for this is the potential to solve problems in one field using thought processes and methodologies already developed in another field. Would words with similar vectors in different field automatically indicate a connection between them, at least in the structure of the words around them?</p>
","nlp"
"56651","BERT : text classification and feature extractionn","2019-07-31 02:38:31","","1","155","<nlp><bert>","<p>I have tried multi-label text classification with BERT. </p>

<p>Here is the sample input: <strong>$15.00 hour, customer service, open to industries</strong></p>

<p>One of the labels is Billing_rate and prediction score looks quite good.</p>

<p>Now my question is if I want to extract <strong>$15.00 hour</strong> basically feature value out of BERT. Can you please suggest what are my next step options? </p>
","nlp"
"56620","How to effectively tune the hyper-parameters of Gensim Doc2Vec to achieve maximum accuracy in Document Similarity problem?","2019-07-30 11:50:18","","1","1119","<python><nlp><word2vec><gensim><similar-documents>","<p>I have around 20k documents with 60 - 150 words. Out of these 20K documents, there are 400 documents for which the similar document are known. These 400 documents serve as my test data. </p>

<p>At present I am removing those 400 documents and using remaining 19600 documents for training the doc2vec. Then I extract the vectors of train and test data. Now for each test data document, I find it's cosine distance with all the 19600 train documents and select the top 5 with least cosine distance. If the similar document marked is present in these top 5 then take it to be accurate. Accuracy% = No. of Accurate records / Total number of Records.</p>

<p>The other way I find similar documents is by using the doc2Vec most similiar method.  Then calculate accuracy using the above formula.</p>

<p>The above two accuracy doesn't match. With each epoch one increases other decreases.</p>

<p>I am using the code given here: <a href=""https://medium.com/scaleabout/a-gentle-introduction-to-doc2vec-db3e8c0cce5e"" rel=""nofollow noreferrer"">https://medium.com/scaleabout/a-gentle-introduction-to-doc2vec-db3e8c0cce5e</a>. For training the Doc2Vec.</p>

<p>I would like to know how to tune the hyperparameters so that I can get making accuracy by using above-mentioned formula. Should I use cosine distance to find the most similar documents or shall I use the gensim's most similar function?</p>
","nlp"
"56616","Getting Value Error while training a model for binary classification","2019-07-30 10:49:31","56617","0","134","<deep-learning><keras><nlp><spacy>","<p>While training a sequential model using Keras, Im getting this error</p>

<p>The model summary is shown below</p>

<pre><code>Layer (type)                 Output Shape              Param #   
=================================================================
dense_1 (Dense)              (None, 512)               20480512  
_________________________________________________________________
activation_1 (Activation)    (None, 512)               0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 512)               262656    
_________________________________________________________________
activation_2 (Activation)    (None, 512)               0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 2)                 1026      
_________________________________________________________________
activation_3 (Activation)    (None, 2)                 0         
=================================================================
</code></pre>

<p>I used the below steps to train the model, for binary classification,</p>

<pre><code>model = Sequential()

model.add(Dense(512, input_shape=(vocab_size,)))
model.add(Activation('relu'))
model.add(Dropout(0.3))

model.add(Dense(512))
model.add(Activation('relu'))

model.add(Dropout(0.3))
model.add(Dense(num_labels))
model.add(Activation('softmax'))
</code></pre>

<p>num_labels = 2 for the above code</p>

<p>The error is shown below.</p>

<pre><code>ValueError: Error when checking target: expected activation_3 to have
shape (2,) but got array with shape (1,)
</code></pre>
","nlp"
"56534","Difference between from nltk import word_tokenize and from nltk.tokenize import word_tokenize?","2019-07-29 06:04:21","56543","0","93","<nlp><nltk>","<p>What is the difference between the word_tokenize, one imported directly from nltk and the other being imported from tokenize package of nltk?</p>
","nlp"
"56476","What is the best question generation state of art with nlp?","2019-07-27 07:39:18","","8","9020","<machine-learning><deep-learning><nlp>","<p>I was trying out various projects available for question generation on GitHub namely NQG,question-generation and a lot of others but I don't see good results form them either they have very bad question formation or the questions generated are off-topic most of the times, Where I found one project that actually generates good questions </p>

<p><a href=""https://github.com/bloomsburyai/question-generation"" rel=""noreferrer"">bloomsburyai/question-generation</a></p>

<p>It basically accepts a context(paragraph) and an answer to generate the question and I am trying to validate the questions generated by passing the generated question along with the paragraph to allenNLP </p>

<p><a href=""https://demo.allennlp.org/reading-comprehension"" rel=""noreferrer"">Answer generation for a question</a></p>

<p>And then I am trying to make sure the generated answers are correct for the questions generated with calculating the sentence embedding for both the answers(AllenNLP and PotentialAnswer) using <a href=""https://tfhub.dev/google/universal-sentence-encoder/2"" rel=""noreferrer"">Universal Sentence Encoder</a> and a cosine distance to get how similar the answers match and the filtering question that has least cosine distance.</p>

<p>Wanted to know if this is the best approach or Is there a state of the art implementation for question generation? Please suggest</p>
","nlp"
"56406","An exhaustive, representative test database in phrase search algorithm","2019-07-25 21:32:27","56411","1","31","<nlp><search-engine>","<p>For a phrase searching algorithm, imagine the goal is to search for a name phrase and return matched results based on a pre-defined threshold. For example, searching for ""Jon Smith"" could return ""Jon Smith"", ""Jonathan Smith"", ""Jonathan David Smith"", ""Jonathan Smith-Mikel"", ""Jonathan 'Smith' Mikel"" etc.</p>

<p>The plan is to manually choose N test cases and put them in a benchmark database. I have concern about this plan because the test cases is likely to be not exhaustive. I know there are pretty mature search engines there, so is there a test database which covers all possible cases, such as different name combinations, punctuations, symbols, etc. such that we can use this as our benchmarks instead of guessing?</p>

<p>For example, this test database should contain all cases for ""Jon Smith"", as well as connectors such as hyphen, apostrophe, and so on.</p>
","nlp"
"56382","NLP - How can I encode single words from one feature (No word frequency)","2019-07-25 15:00:20","","0","52","<machine-learning><python><pandas><nlp><dataframe>","<p>I'm constructing a pandas data-frame as an input for some sklearn machine learning models. It is a supervised learning problem that consists in classify 'words' included in the body-content of documents to different labels. I got categorical, numerical and text data. The text data is already tokenised. So in feature 'text' I have single words. Meaning that, there is one word per row. E.g., 'downtown road', '5678-VB', '3552', 'product#3'. The data frame looks like the following: </p>

<pre><code>label                               316511 non-null object
num_feature_1                       316511 non-null float16
num_feature_2                       316511 non-null float16
num_feature_3                       316511 non-null float16
num_feature_4                       316511 non-null float16
text                                316511 non-null object
length                              316511 non-null float16
start_with_capital                  316511 non-null uint8
is_upper                            316511 non-null uint8
ending_with_colon                   316511 non-null uint8
</code></pre>

<p>I already encode every-feature but the words (I got current word, previous word and next word as three different features). I have thought about using sklearn.preprocessing.LabelBinarizer for one-hot-encode the words. However, I got 316511 instances/words and 161880 unique words so I created a total of 161920 features. This increased the weight of the pandas data-frame up to 47GB. This is not feasible for my laptop/cloud instance so I had to find another way to handle this large dataset. I found that <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html"" rel=""nofollow noreferrer"">HashingVectorizer</a> as another way to encode large dataset in a more optimised way  But still, this uses frequency of words and for this problem case it does not make sense to apply frequency. </p>

<p>Anyone got some other idea on how to encode text/strings on this fashion to input sklearn-models?</p>

<p>****EDIT: Just to have a small view about 5 entrances with few features of the dataset: </p>

<pre><code>+--------+-----------------------+--------------+-------------+------------+---------------+
|        | actual_label          |num_feature_1 | text        |   is_upper |num_feature_2  |
|--------+-----------------------+--------------+-------------+------------+---------------|
|  22337 | Sold-to-party Address | 0.989258     | 6B          |          1 | 0.0400085     | 
| 199218 | Material Description  | 0.956543     | 32X10       |          1 | 0.0999756     |
|  33579 | Sold-to-party Address | 0.989258     | Hatfield    |          0 | 0.160034      |
|   6506 | Sold-to-party Address | 0.956543     | 26          |          0 | 0.0400085     |
| 199066 | Material Description  | 0.93457      | scanalatura |          0 | 0.219971      |
+--------+-----------------------+--------------+-------------+------------+---------------+
</code></pre>
","nlp"
"56226","In the context of natural language processing, can anyone give a concrete example of True Positive, True Negative, False Positive, False Negative?","2019-07-23 10:08:15","56228","1","656","<machine-learning><deep-learning><classification><nlp>","<p>Google <a href=""https://developers.google.com/machine-learning/crash-course/classification/true-false-positive-negative"" rel=""nofollow noreferrer"">post</a> gives a interesting explanation about True Positive, True Negative, False Positive, False Negative</p>

<blockquote>
  <p>True Positive (TP): Reality: A wolf threatened. Shepherd said: ""Wolf.""
  Outcome: Shepherd is a hero.</p>
  
  <p>True Negative (TN): Reality: No wolf threatened. Shepherd said: ""No
  wolf."" Outcome: Everyone is fine.</p>
  
  <p>False Positive (FP): Reality: No wolf threatened. Shepherd said:
  ""Wolf."" Outcome: Villagers are angry at shepherd for waking them up.</p>
  
  <p>False Negative (FN): Reality: A wolf threatened. Shepherd said: ""No
  wolf."" Outcome: The wolf ate all the sheep.</p>
</blockquote>

<p>In the context of CV, the classifier predicts if an image contains cat</p>

<pre><code>True Positive (TP): 
Reality: an image contains cat. 
classifier predicts: cat.

True Negative (TN): 
Reality: an image does not contains cat. 
classifier predicts: no cat.

False Positive (FP): 
Reality: an image does not contains cat. 
classifier predicts: cat.

False Negative (FN): 
Reality: an image contains cat. 
classifier predicts: no cat.
</code></pre>

<p>Can anyone gives a concrete example of TP、TN、FP、FN like above, in the context of natural language processing?</p>
","nlp"
"56204","Which method of NLP is this?","2019-07-23 05:18:59","56205","2","58","<nlp>","<p>I have been searching for 2 weeks and I got no where so far.</p>

<p>There is  a list of diseases</p>

<pre><code>Diabetes 
Heart Transplant
Fingertip amputation
Injury by sharp tools
.
.
.
</code></pre>

<p>and My dataset is a list of medical text reports.</p>

<p>the training dataset has diseases that can be generated from each record</p>

<p>example that I made</p>

<blockquote>
  <p>This patient has suffered a cut while using his Carving Chisel and led
  to losing the fingertip therefore we had to operate to sew
  the tip.....</p>
</blockquote>

<p>from this report we get these diseases</p>

<pre><code>1- Injury by sharp tools
2- Fingertip amputation
3- Sewing injury
</code></pre>

<p>another report results may have 3 or less or more diseases</p>

<p>I have searched a lot
I found many examples about NLP classification</p>

<p>where a text will be classified into Sports, Politics, Culture, Science, etc.</p>

<p>I found NER where person names, locations, dates, etc can be extracted from a text. </p>

<p>But did not find anything for a single text could have multiple values (similar to my dataset)</p>

<p>I dont know where to start.</p>

<p>Could anyone please help me finding what is the name of this method of extracting list of issues from a text?</p>

<p><strong>Edit</strong></p>

<p>What else do I need to exclude the negations, if the report says </p>

<blockquote>
  <p>""This patient has stomach problem but not diabetes ""</p>
</blockquote>

<p>How can I make AI understand there is a negation (<strong>NOT</strong>) before diabetes so it should not be included?</p>

<p>So the result will be</p>

<pre><code>stomach problem
</code></pre>

<p>as diabetes should be excluded because of the negation word </p>
","nlp"
"56198","Determining topic of text","2019-07-23 03:33:24","","1","22","<machine-learning><python><nlp><nltk><topic-model>","<p>I was wondering what I should be looking into if I want to measure the similarity between a paragraph and a corpus of text.</p>

<p>For example, given a paragraph of text and the entire corpus of Data Science StackExchange questions, how would you determine whether that paragraph of text could be classified as a Data Science question?</p>
","nlp"
"56194","fit transform with mysterious special chracters","2019-07-23 01:41:38","","1","11","<python><preprocessing><nlp>","<p>I tried to make the bag of words <br/></p>

<pre><code>pattern = re.compile('[^A-Za-z0-9]+' '' "" $#! People Whitespace"")
df[""bag_of_words""]= df[""bag_of_words""].apply(lambda s: ''.join([i for i in str(s) if not pattern.findall(i)]))
</code></pre>

<p>and apply fit transform and check the feature names <br/></p>

<pre><code>feature_names = count.get_feature_names()
</code></pre>

<p>but it includes some (Dr.)strange characters looks like following <br/></p>

<p>['<strong>',
 '_</strong>',
 '____',
 '_____',
 '______',
 '_______',
 '__________',
 '_____________',
 '__________cat',
 '__________something',
 '_________soso,
 '_________dog']</p>

<p><br/>
what is this ______ thing? and how to remove it? </p>
","nlp"
"56165","counter vector fit transform cosine similarity memory error","2019-07-22 14:35:26","","0","665","<machine-learning><nlp><data-analysis><cosine-distance>","<pre><code>count_matrix = count.fit_transform(off_data3['bag_of_words'])
</code></pre>

<p>I have count_matrix shape with <br/></p>

<blockquote>
  <p>count_matrix.shape
  (476147, 482824)</p>
</blockquote>

<pre><code>cosine_sim = cosine_similarity(count_matrix, count_matrix)
</code></pre>

<p>I think the matrix size is too big to cause this memory error <br/></p>

<blockquote>
  <p>--------------------------------------------------------------------------- MemoryError                               Traceback (most recent call
  last)  in </p>
  
  <p>~/venv/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in
  cosine_similarity(X, Y, dense_output)    1034     1035     K =
  safe_sparse_dot(X_normalized, Y_normalized.T,
  -> 1036                         dense_output=dense_output)    1037     1038     return K</p>
  
  <p>~/venv/lib/python3.6/site-packages/sklearn/utils/extmath.py in
  safe_sparse_dot(a, b, dense_output)
      135     """"""
      136     if sparse.issparse(a) or sparse.issparse(b):
  --> 137         ret = a * b
      138         if dense_output and hasattr(ret, ""toarray""):
      139             ret = ret.toarray()</p>
  
  <p>~/venv/lib/python3.6/site-packages/scipy/sparse/base.py in
  <strong>mul</strong>(self, other)
      479             if self.shape[1] != other.shape[0]:
      480                 raise ValueError('dimension mismatch')
  --> 481             return self._mul_sparse_matrix(other)
      482 
      483         # If it's a list or whatever, treat it like a matrix</p>
  
  <p>~/venv/lib/python3.6/site-packages/scipy/sparse/compressed.py in
  _mul_sparse_matrix(self, other)
      514                                     maxval=nnz)
      515         indptr = np.asarray(indptr, dtype=idx_dtype)
  --> 516         indices = np.empty(nnz, dtype=idx_dtype)
      517         data = np.empty(nnz, dtype=upcast(self.dtype, other.dtype))
      518 </p>
  
  <p>MemoryError:</p>
</blockquote>

<p>Any tips to avoid this memory error when You have large matrix?<br/></p>
","nlp"
"56156","SpaCy binary text classification","2019-07-22 13:35:10","","0","643","<machine-learning><classification><nlp><spacy>","<p>I have a dataset of two folders. One of them contains the documents(text, pdfs) related to personal information (like name,email,address etc), the  other contains non-personal information.</p>

<p>I have to train a model using Spacy, based on these two folders. So, when we predict a given document, it should predict among these two folders.</p>

<p>I have tried writing many codes taking reference from Github, but nothing seem to be worked. </p>

<p>So, can anyone  give me a code sample to train a model based on the information given above and predict ?</p>

<p>I have done some hands on, on the below code </p>

<pre><code>import spacy
from spacy import displacy
from spacy.util import minibatch, compounding

train_data = [(""This has names, emails, addresses "", {'cats': {'POSITIVE': 1}} ), (""This has games, food, etc"", {'cats': {'POSITIVE': 0}})]

nlp = spacy.load('en_core_web_sm')

if 'textcat' not in nlp.pipe_names:
    textcat = nlp.create_pipe(""textcat"")
    nlp.add_pipe(textcat, last=True)
else:
    textcat = nlp.get_pipe(""textcat"")

textcat.add_label('POSITIVE')

other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'textcat']

n_iter = 1


with nlp.disable_pipes(*other_pipes):
    optimizer = nlp.begin_training()
    print(""Training model..."")
    for i in range(n_iter):
        losses = {}
        batches = minibatch(train_data, size=compounding(4,32,1.001))
        for batch in batches:
            texts, annotations = zip(*batch)
            nlp.update(texts, annotations, sgd=optimizer,
                      drop=0.2, losses=losses)
</code></pre>

<p>Here in the above code, I have trained the model using two simple sentences. I need to train on two folders, as mentioned in the question.
This code just says model has trained.
And also how can i save this model and test it for documents to predict ??</p>
","nlp"
"56071","Word2Vec - document similarity","2019-07-20 18:55:06","","1","500","<machine-learning><nlp><word2vec>","<p>Lets say I have text data for different documents from 2005 - 2015. I want to compare the similarity between <span class=""math-container"">$t$</span> and <span class=""math-container"">$t-1$</span> documents. So I take the document at 2006 and compare it with the document at 2005, take the document at 2007 and compare with the document at 2006 … all the way to 2015, compared with 2014.</p>

<p>I have computed for each year a Word2Vec model independently of other year and obtained high dimensional arrays for each Word. So I have 10 Word2Vec models from 2005 - 2015.</p>

<p>Whats the best way for me to compare the documents similarity from here.</p>

<p>Previously I used TF-IDF where I was able to for each document have a large matrix with words in the rows and documents in the columns. Then I could combine he TermDocumentMatrix at <span class=""math-container"">$t$</span> and <span class=""math-container"">$t-1$</span> and compute the cosine similarity.</p>

<p>However Word2Vec gives much high dimensional arrays and I cannot think how to compare W2V models from <span class=""math-container"">$t$</span> to <span class=""math-container"">$t-1$</span>.</p>

<p>Any help would be great!</p>
","nlp"
"56033","real word dataset on company full name with commonly used short name","2019-07-19 19:38:11","","0","138","<classification><nlp><dataset>","<p>I am involved in a work where i have to recognize company when user does not provides its full legal name. Database only has full legal name which is rarely used by human user. Like no body calls google inc but they just call google. My job is to match that user provided name with database name. The approach i am thinking is to create short name of company from legal name and index that so it what user used could be directly matched. </p>

<p>I was thinking of writing a classifier that could be trained on real world data of full name and short names map and then used to create short names from legal names in database. Are there any such dataset currently available that i can use as input source for my trainer?</p>
","nlp"
"56014","Facing this issue while predicting ""CountVectorizer - Vocabulary wasn't fitted""","2019-07-19 14:31:04","","0","3880","<machine-learning><nlp><spacy>","<pre><code>#these are classifier and  vectorizer

vectorizer = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1))
classifier = LinearSVC()
</code></pre>

<p>I have created a Pipeline as shown below</p>

<pre><code># Create the  pipeline to clean, tokenize, vectorize, and classify
pipe = Pipeline([(""cleaner"", predictors()),
                 ('vectorizer', vectorizer),
                  ('classifier', classifier)])
</code></pre>

<p>while predicting the test dataset, im facing this issue.</p>

<pre><code>sample_prediction = pipe.predict(X_test)
</code></pre>

<h1>ERROR : sklearn.exceptions.NotFittedError: CountVectorizer - Vocabulary wasn't fitted.</h1>

<p>Can anyone help ?</p>
","nlp"
"55901","In a Transformer model, why does one sum positional encoding to the embedding rather than concatenate it?","2019-07-18 08:34:46","76291","39","11569","<nlp><encoding><transformer><attention-mechanism>","<p>While reviewing the Transformer architecture, I realized something I didn't expect, which is that :</p>

<ul>
<li>the positional encoding is summed to the word embeddings </li>
<li>rather than concatenated to it.</li>
</ul>

<blockquote>
  <p><a href=""https://i.sstatic.net/bFPI9.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/bFPI9.png"" alt=""positional encoding summed to word embedding""></a></p>
  
  <p><a href=""http://jalammar.github.io/images/t/transformer_positional_encoding_example.png"" rel=""noreferrer"">http://jalammar.github.io/images/t/transformer_positional_encoding_example.png</a></p>
</blockquote>

<p>Based on the graphs I have seen wrt what the encoding looks like, that means that :</p>

<ul>
<li>the first few bits of the embedding are completely unusable by the network because the position encoding will distort them a lot, </li>
<li>while there is also a large amount of positions in the embedding that are only slightly affected by the positional encoding (when you move further towards the end).</li>
</ul>

<blockquote>
  <p><a href=""https://i.sstatic.net/XLT9V.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/XLT9V.png"" alt=""graph shows positional encoding affects firsts logits a lot, last logits hardly not""></a></p>
  
  <p><a href=""https://www.tensorflow.org/beta/tutorials/text/transformer_files/output_1kLCla68EloE_1.png"" rel=""noreferrer"">https://www.tensorflow.org/beta/tutorials/text/transformer_files/output_1kLCla68EloE_1.png</a></p>
</blockquote>

<p>So, why not instead have smaller word embeddings (reduce memory usage) and a smaller positional encoding retaining only the most important bits of the encoding, and instead of summing the positional encoding of words keep it concatenated to word embeddings?</p>
","nlp"
"55896","How to train a spacy model for text classification?","2019-07-18 07:42:15","","5","6575","<machine-learning><nlp><spacy>","<p>Can i know the way or steps to train a spacy model for text classification. (binary classification in my case) </p>

<p>Please help me with the process and way to approach.</p>
","nlp"
"55892","Why is hard for neural machine translation model to learn rare words?","2019-07-18 06:53:51","","1","161","<nlp><machine-translation>","<p>i'm kinda new with neural machine translation. I've read some papers, authors usually limit the size of vocabulary by replace rare words by unk token. In this <a href=""http://www.nlpr.ia.ac.cn/cip/ZongPublications/2016/IJCAI_2016_LXQ.pdf"" rel=""nofollow noreferrer"">paper</a>, they said that ""...NMT model cannot learn the translation of rare words..."". I want to understand why is hard for NMT mode to learn rare words and also the impact of word counts to NMT model. Thanks</p>
","nlp"
"55876","pros and cons of lexical vs machine learning methods for text mining","2019-07-17 23:40:17","","2","147","<machine-learning><data-mining><text-mining><nlp>","<p>I wanted to know what are the pros and cons are of using lexical methods and machine learning methods for classifying texts based topic.</p>

<p>I have used a simple method of mining documents related to a specific topic based on a keyword list. Basically, if the document contains one of the words from the keyword list it will retrieve it. If that particular word could be used in a different context it checks the post again for other associated words which would usually be found in similar types of documents. This is a simple method but seems to work well, and can be applied to any topic quickly and easily. The main detractor seems to be the keyword lists need to be created and maintained which can be time consuming and inefficient.</p>

<p>In recent times machine learning methods have been used for this type of document classification. It seems this method is able to judge ""context"" better in documents but requires large datasets to be trained on and also require continual training if new data needs to be classified.</p>

<p>It feels like people dismiss lexical methods since the emergence of machine learning methods but is this warranted? It seems like lexical methods can still get good results, especially on small documents which don't contain much context.</p>

<p>What are the pros and cons of each?</p>
","nlp"
"55856","Does the mean/median of a set sentence embedded vectors represent anything?","2019-07-17 18:33:59","55865","0","423","<python><nlp><clustering><word-embeddings>","<p>Please bear with me as I am new to NLP.
I am specifically using tensorflow's universal sentence encoder: <a href=""https://tfhub.dev/google/universal-sentence-encoder-large/3"" rel=""nofollow noreferrer"">https://tfhub.dev/google/universal-sentence-encoder-large/3</a></p>

<p>I am clustering text based on the cosine similarity of the embedding produced by the model and I want to see what cluster a new text would most likely lie in.  I was going to compare the new text embedding to the mean/median of all the embeddings within a cluster to see which cluster it would most likely lie in.  Would taking the mean/median of the cluster's vectors ""represent"" the general idea of the cluster or will the vector not represent what I am looking for?</p>
","nlp"
"55835","Does gensim use Negative sampling in Word2vec?","2019-07-17 13:47:50","","0","2025","<machine-learning><nlp><word2vec><gensim>","<p>When I train a word2vec model in Gensim on a huge amount of words/data (let's say hundreds of thousands of word vectors), is gensim using <strong>negative sampling</strong> automatically?</p>

<p>Alternatively, is there a way to make it use it?</p>
","nlp"
"55821","Pre-trained models","2019-07-17 09:29:07","","4","109","<machine-learning><computer-vision><nlp>","<p>I am starting off with machine learning so could someone tell if there is some site where one can find the current best performing trained models for any specific problem like sentiment analysis or objection detection or any machine learning problem of that sort?</p>
","nlp"
"55763","Is it normal when BLEU score on filtered data by length is greater than BLEU score on whole data","2019-07-16 10:16:58","","1","133","<nlp><machine-translation>","<p>I am creating 2 neural machine translation model (model A and B with different improvements each model) with fairseq-py. When I evaluate model with bleu score, model A BLEU score is 25.9 and model C is 25.7. Then i filtered data by length into 4 range values such as 1 to 10 words, 11 to 20 words, 21 to 30 words and 31 to 40 words. I re-evaluated on each filtered data and all bleu scores of model B is greater than model A. Do you think this is normal case? </p>
","nlp"
"55750","n-gram Model - Why Smoothing?","2019-07-16 06:54:12","","2","2025","<nlp><stanford-nlp>","<p>I am creating an n-gram model that will predict the next word after an n-gram (probably unigram, bigram and trigram) as coursework.</p>

<p>I have seen lots of explanations about HOW to deal with zero probabilities for when an n-gram within the test data was not found in the training data. I understand how 'add-one' smoothing and some other techniques work.</p>

<p>However, I can find nothing about WHY we need to take actions such as these.</p>

<p>For instance, if the test data has ""Peace begins with a Smile"" and this was not in the training data, so when I supply the model with ""Peace begins with a"", it will not come up with ""Smile"" end word. It may provide others or none. If there are none or they have a low probability, then I would supply the shorter n-gram of ""begins with a"" and see what words and probabilities that provides. If that fails, then ""with a"" and so on.</p>

<p>I suspect I'm missing something but can't see what.</p>

<p>Please can you help?</p>
","nlp"
"55619","Extract Domain related words","2019-07-13 17:40:07","55622","0","343","<nlp>","<p>I am doing a research regarding on automatic text summarizing. So in order to weighting sentences I need to get words related to a particular field or domain like shown below.</p>

<pre><code>Topic word - Car
Related words - engine, driver, road, break, accelerator 
</code></pre>

<p>Is there any direct method that I can use like <code>wordnet synsets</code>.</p>
","nlp"
"55618","SpaCy vs AllenNLP?","2019-07-13 17:12:30","68538","3","1097","<nlp><pytorch><spacy><allennlp>","<p>I have used a little of both <a href=""https://spacy.io/"" rel=""nofollow noreferrer"">spaCy</a> and <a href=""https://allennlp.org/"" rel=""nofollow noreferrer"">allenNLP</a> in my NLP projects. I like them both as they work very well with <a href=""https://pytorch.org/"" rel=""nofollow noreferrer"">PyTorch</a> (my DL framework choice!). But, I still cannot decide which one to master in a long term so that I can increase the pace of my NLP projects in future.<br>
Can someone please share their experience or suggest the differences between these 2 libraries or pros and cons?</p>
","nlp"
"55521","Autocomplete with deep learning","2019-07-11 17:32:34","","4","2879","<deep-learning><nlp>","<p>I got interested in autocompletion using deep learning and <a href=""https://medium.com/@curiousily/making-a-predictive-keyboard-using-recurrent-neural-networks-tensorflow-for-hackers-part-v-3f238d824218"" rel=""nofollow noreferrer"">tutorials</a> that I found where conditioned always on specific number of characters (given 40 characters predict the next character or the whole complete word). But in real world the autocomplete is done after entering a first letter already.</p>

<p>I was checking out how autocompletion works in Whatsapp I had following observations:<br>
 #1. Autocomplete starts working already after a first letter.<br>
 #2. Before I would enter a space, Whatsapp suggests the continuation of a word, example: entering 'L' would result in suggesting 'L', 'Like', 'Last'.<br>
 #3. If I enter a space after a word, autocomplete would suggest me whole next word, example: entering 'Last ' would result in suggesting 'time', 'night', 'week'.<br>
 #4. If I enter 'I am ru' it will suggest 'running', 'run'. (Pay attention: grammatically, run is wrong)</p>

<p>One could use RNNs/LSTMs with one-hot encoding or word-embedding to do such things.</p>

<p>However, I wondered what the train dataset should look like:  </p>

<ol>
<li>is it char-based model with different n-grams words, example:<br>
1.1 char-based 1-gram ('l', 'i', 'k', 'e') word would cover the cases #1 and #2 because I would be able to sample the next character.<br>
1.2. char-based with 2-gram word ('l', 'i', 'k', 'e', ' ', 's','u','n') would cover #3 as I would be able to predict the whole complete next word (character-based) and it also preserves the dependence on the previous word.    </li>
</ol>

<p>By using 1.1 and 1.2 I could cover cases #1, #2 and #3 and model 2-words dependence.</p>

<ol start=""2"">
<li>is it word-based model with different n-grams, example:<br>
2.1 word-based with 1 grams, eg. ""like sun"" -> sequence: ""like "", predict: ""sun"". This would allow me to model #3<br>
2.2  word-based with 2 grams, eg. ""like sun and"" -> sequence: ""like sun "", predict: ""and"". This would allow me to model a bit longer dependence. </li>
</ol>

<p>By using 1.1 and 2.1 I would cover cases #1, #2, #3.</p>

<p>Questions:<br>
I wonder what would be better to do: use 1.1 and 1.2 or 1.1 and 2.1. </p>

<p>Also, let me know if my way of thinking is correct or if I am completely wrong somewhere or if I miss something or if you think there are some easier way of doing it. Would be glad about any suggestions.</p>
","nlp"
"55399","NLP - Identify Tagged Words","2019-07-10 03:36:20","","2","104","<machine-learning><nlp><lstm><word-embeddings><named-entity-recognition>","<p><em>Please pardon me as the title might not be very accurate</em></p>

<p>I am trying to create a model that learns the word representation and then is able to predict word representation in another piece of text. An example will make it more clear. Please see below for the example:</p>

<p><strong>Model Training Text:</strong></p>

<pre><code>I lived in *Munich last summer. *Germany has a relaxing, slow summer lifestyle. One night, I got food poisoning and couldn't find !Tylenol to make the pain go away, they insisted I take !aspirin instead.
</code></pre>

<p><strong>Model Predicts:</strong></p>

<p><code>['Munich','Germany','Tylenol','aspirin']</code></p>

<p><strong>Evaluation Text:</strong></p>

<pre><code>When I lived in Paris last year, France was experiencing a recession. The nightlife was too fun, I developed an addiction to Adderall and Ritalin.
</code></pre>

<p><strong>Output:</strong></p>

<p><code>['Paris','France','Adderall','Ritalin']</code></p>

<p>The question is that what sort of NLP technique will be helpful in such a case. I don't even know what are these kind of problems called. Can you please tell what are these problems called? </p>

<p>One approach I can think of is to train the <code>RNN</code> with <code>Embedding Layer</code> to predict the position of <code>*</code> and <code>!</code> since <code>*</code> will be prefixed to the name of the country and <code>!</code> will be prefixed to the name of the drug but my challenge is that how can I structure my data for such training. Is it a feasible approach?</p>

<p>Is there any resource/material I can refer to and draw the inspiration from?</p>

<p>I would very much appreciate any help or suggestions. Thanks a lot in advance. </p>
","nlp"
"55177","Android: NLP library for date recognition in string","2019-07-06 11:37:07","","1","253","<nlp><java>","<p>I am currently working on an android app which should make appointments automatically by reading the incoming messages from your mobile phone. I've managed to create a service which monitors the incoming messages, but now I need an <strong>Natural Language Processing</strong> algorithm in order to find the date for the appointment.</p>

<p>I've tried <strong>DialogFlow</strong>, but I found out it cannot be used offline and that is not the purpose of the app. <strong>It should work offline too</strong>!</p>

<p>Does anyone have a suggestion for a library I can use?</p>
","nlp"
"55034","Extracting information with corresponding fields","2019-07-04 09:40:08","","1","43","<machine-learning><python><nlp><stanford-nlp>","<p>I have large pool of scanned county documents. I need to extract information like document title, borrower name&amp;address, lender name&amp;address etc. </p>

<p>The text is like this 
Eg: the deed of trust, between abc llc, a limited company, whose address is XXXXXX, herein called ""borrower"", and  xyz, whose address is XXXXX,herein called ""lender"".</p>

<p>I used Named entity recognition method to  extract the names, it works well. but how would i know which name is borrower and which one is lender? can anyone help me</p>
","nlp"
"55019","What model is recommended: I am using text features in a regression and want to interpret coefficients","2019-07-04 02:40:56","","0","96","<machine-learning><nlp><regression><model-selection>","<p>I am using the text of comments on a forum to predict how many upvotes it will get. I want to be able to say, ""Reviews with X, Y, Z words are more upvoted"". So to do this, I want to use text features in a regression. In particular,</p>

<p>What model should I use to maximize interpretability of coefficients? </p>
","nlp"
"54998","Ways of filtering erronous email addresses using NLP?","2019-07-03 17:41:19","","0","203","<nlp>","<p><strong>Background:</strong></p>

<p>I have a database of user information, in which they registered through a website. </p>

<p><strong>Objective:</strong></p>

<p>I would like to filter out erroneous emails, not by if it is malformed (i.e. it's missing an @-sign), but rather by ""weird strings"" in the ""local-part"" of the address. So examples of erroneous email address are things like: </p>

<ul>
<li>zzzzzzzzzzzzzzzzz@gmail.com</li>
<li>asdfasdfasdf@gmail.com</li>
<li>yourenotgettingmyrealemail@gmail.com</li>
<li>123@yahoo.com</li>
<li>test@test.com</li>
</ul>

<p>I know most of these require some ""human"" interpretation to figure that they're probably not real, but I was wondering if there are any algorithms that can help me out.</p>
","nlp"
"54906","Predicting word from a set of words","2019-07-02 11:27:51","","2","399","<neural-network><nlp><word2vec><word-embeddings><bert>","<p>My task is to predict relevant words based on a short description of an idea. for example ""SQL is a domain-specific language used in programming and designed for managing data held in a relational database"" should produce words like ""mysql"", ""Oracle"", ""Sybase"", ""Microsoft SQL Server"" etc...</p>

<p>My thinking is to treat the initial text as a set of words (after lemmatization and stop words removal) and predict words that should be in that set. I can then take all of my texts (of which I have a lot), remove a single word and learn to predict it.</p>

<p>My initial thought was to use word2vec and find words similar to my set. But this doesn't work very well, as I don't want similar words but words that go together in many sentences, which is sort of the task that word2vec is training on to create its embedding...</p>

<p>How would you model this problem? I don't think RNNs are relevant here because I want to use a set of words - they do not have any order, I'm not trying to predict the <em>next</em> word in any way. However, the size of the set could vary I think...</p>

<p>What kind of NN architecture would you use for this sort of problem?</p>

<p>UPDATE:</p>

<p>To further explain the purpose of the predictor and why word2vec doesn't work for me:</p>

<p>I did train a word2vec model on my data (which is very domain specific and I have lots of data on it). Now my problem is that it produces words that are similar to the ones I input to it... and not ones that could be in the context.</p>

<p>e.g: I input ""computer manufacturer"" to get most similar words and i get:</p>

<p>(u'equipment_manufacturer', 0.7463390827178955),
 (u'manufacture', 0.7410058975219727),
 (u'manufacturer_distributor', 0.7196526527404785),
 (u'distributor', 0.6950951814651489),
 (u'desktop_computer', 0.6632821559906006)</p>

<p>These are terms that have similar meaning...</p>

<p>But I'm not looking for similar meaning. I would be more interested in getting ""Dell"" for example as the output.
Another example is to associate ""Apple_inc"" with informatics, OS, hardware.. - not similar words, but related ones...</p>

<p>So my thinking now is to build a word2vec CBOW model, but not to use just the embeddings that are created but actually use the network to predict new words from context... </p>
","nlp"
"54891","Data Extraction from images using NLP and ML","2019-07-02 07:25:03","","2","326","<machine-learning><python><nlp><data-cleaning>","<p><a href=""https://i.sstatic.net/encxg.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/encxg.png"" alt=""enter image description here""></a>Hi i'm trying to extract data like name, planType, phone#, .. from images like insurance cards or licence cards with GoogleVision / Textract using some conditions  but it does not extract the correct content . </p>

<p>is there a way to use ML and NLP together to solve this problem</p>

<p>Thank you </p>
","nlp"
"54806","Word embedding of a new word which was not in training","2019-07-01 03:30:30","","4","6022","<nlp><word2vec><word-embeddings>","<p>Let's say I trained a Skip-Gram model (Word2Vec) for my vocabulary of size 10,000.
The representation allows me to reduce the dimension from 10,000 (one-hot-encoding) to 100 (size of hidden layer of the neural network). </p>

<p>Now suppose I have a word in my test set which was not in my training vocabulary. What is a reasonable representation of the word in the space of dimension 100?
For me, it seems that I cannot use the neural network I trained to come up with the word embeddings.</p>
","nlp"
"54769","Predicting similarity between nouns like university names and tech companies?","2019-06-30 07:12:28","54780","1","71","<nlp><similarity>","<p>I am trying to extract entities like university studied at and tech companies from resumes , I have a list of popular universities and companies and I want to find out which university best matches the extracted entity </p>

<p>Example </p>

<p>1) University in the list : <code>IIT Bombay</code> </p>

<p>Extracted entity : <code>Education : Indian Institute of technology Bombay Btech</code></p>

<p>2)University in the list : <code>Infosys</code> </p>

<p>Extracted entity : <code>Infosys India Ltd.</code></p>

<p>As you can see , there are extra unwanted words , short forms , expanded forms etc recognized in the extracted entity , is there any sentence <strong>similarity algorithms</strong> best suited for this purpose ? </p>

<p>Using SpaCy for entity extraction.</p>
","nlp"
"54764","Transformer for neural machine translation: is it possible to predict each word in the target sentence in a single forward pass?","2019-06-30 02:44:25","","0","496","<nlp><training><pytorch><transformer>","<p>I want to replicate the Transformer from the paper <a href=""https://arxiv.org/abs/1706.03762"" rel=""nofollow noreferrer"">Attention Is All You Need</a> in PyTorch. My question is about the decoder branch of the Transformer. If I understand correctly, given a sentence in the source language and partial/incomplete translation in the target language, the Transformer is tasked with predicting the next token in the translation. For example:</p>

<p>English (source): <em>I love eating chocolate</em></p>

<p>Spanish (target): <em>Yo amo comer</em> ...</p>

<p>The next token in the translation should be <em>chocolate</em> (thus, the full translation would be <em>""Yo amo comer chocolate""</em>). So, in this example, the encoder would process the sentence <em>""I love eating chocolate""</em>, the decoder would process the partial translation <em>""Yo amo comer""</em> and the final output would be a softmax over the whole vocabulary (hopefully with <em>chocolate</em> being the token with the highest score).</p>

<p>The issue is, during training we want the Transformer to learn the full translation. This means that if the target sentence has length N, we want the transformer to predict the first word, the second word, the third word, and so on all the way up to the N-th word of the target sentence. One way to do that is by generating N training instances (one for each word of a target sentence of length-N). However this approach is computationally quite expensive, because instead of having a single training instance per source-target pair, we now have as many training instances as there are words in all the target sentences. So I was wondering if it's possible make all predictions for all words in the target sentence in a single forward pass. Is that possible?</p>
","nlp"
"54749","Weight of Vectors - doc2vec","2019-06-29 18:42:25","","1","55","<nlp>","<p>Let's assume I have a number of different documents. I used <code>doc2vec</code> to generate a vector per document. So after plotting it with <code>t-Sne</code>, I can visualize where the documents are in a <span class=""math-container"">$2$</span> or <span class=""math-container"">$3$</span> dimensional space. So I have realistic distances between the documents.</p>

<p>Now there comes the (for me) tricky part:
I want consumers to answer a questionnaire, where they rate the documents on a <span class=""math-container"">$5$</span>-point-Lickert scale (<span class=""math-container"">$1$</span> do not like - <span class=""math-container"">$5$</span> like very much). In the end, I want to see where consumer positions himself in the plot I made before.</p>

<p>Which means, that I somehow need to weight the vectors. 
document <span class=""math-container"">$1$</span> - with vector <span class=""math-container"">$1234$</span> is rated as <span class=""math-container"">$5$</span> (so this document has a high (I call it) ""gravitation"" )
document <span class=""math-container"">$2$</span> - with vector <span class=""math-container"">$1756$</span> is rated with <span class=""math-container"">$2$</span> (so with lower gravity...)</p>

<p>Do you have any suggestion on how to solve this problem? I am still not sure on how to ""calculate"" with (let's say a <span class=""math-container"">$100$</span> dimensional) vector</p>

<p>It would be awesome if you had a hint for me.</p>
","nlp"
"54748","Xgboost multiple class predictive performance beats one versus rest","2019-06-29 18:28:34","55738","3","2194","<nlp><xgboost><prediction><performance>","<p>I have an NLP task I'm tackling with xgboost (R implementation).</p>

<p>Before describing my doubt I'll give you some background:</p>

<p>I have a corpus of documents for which I did topic discovery, using a term x term matrix clustering approach. For each document, I get a topic score computed using the terms in the document (with a TfIdf score). Then for each document, I pick up the topic with the highest score.</p>

<p>The following step is to create a model that given the term x document score matrix and the best topic per document, predicts the best topic.</p>

<p>I tried two different approaches:</p>

<ul>
<li>a multiple class model, where a topic is associated with each document;</li>
<li>a one versus rest series of models, one per topic, where each document is labeled as belonging or not to a topic.</li>
</ul>

<p>Here are the results of the two approaches, using AUC:</p>

<pre><code>    i                 topic    single     multi
1   1             Topic.nv1 0.9564445 0.9880821
2   2  Topic.nv10_Topic.wv9 0.9848492 0.9969546
3   3            Topic.nv11 0.9174293 0.9741100
4   4 Topic.nv12_Topic.wv11 0.9874073 0.9967725
5   5 Topic.nv13_Topic.wv10 0.9509909 0.9916768
6   6 Topic.nv14_Topic.wv12 0.9864622 0.9959161
7   7            Topic.nv15 0.7333333 0.9333333
8   8   Topic.nv2_Topic.wv3 0.9590279 0.9877953
9   9   Topic.nv3_Topic.wv5 0.9448966 0.9879057
10 10   Topic.nv4_Topic.wv2 0.9521490 0.9908656
11 11   Topic.nv5_Topic.wv6 0.9761665 0.9946294
12 12             Topic.nv6 0.9439377 0.9889028
13 13   Topic.nv7_Topic.wv4 0.9656248 0.9926163
14 14             Topic.nv8 0.9673726 0.9944970
15 15   Topic.nv9_Topic.wv8 0.9716538 0.9929586
16 16             Topic.wv1 0.9610704 0.9925414
17 17             Topic.wv7 0.9765398 0.9904255
</code></pre>

<p>It is visible that the multiclass approach systematically outperforms the one vs rest one. NB: These are training set performances.</p>

<p>Is there a clear theoretical reason for this?</p>
","nlp"
"54723","Build text complexity model based on complex examples","2019-06-28 22:20:32","","2","80","<machine-learning><nlp><text-mining>","<p>I try to build the user specific model which predicts whether arbitrary English text is complex for particular user or not. Having the complex and easy text samples allows to build such model but what if I have only complex samples. How can I build the model in such case?</p>

<p>I can detect whether the given text is different (find the ""outlier"") from those which user marked as difficult. But that information does not tell me in which way it's different. The text could be easier or more difficult.</p>

<p>Currently I see only one way - make an assumption about how the easy text could look like. But it's kind of unsafe since different people might have own unique areas which they do not understand in the text.</p>
","nlp"
"54711","Is there a python library for reformatting names?","2019-06-28 18:13:11","","1","22","<python><nlp>","<p>I have a list of several hundred thousand electrical assets named in multiple databases which I am trying to reformat to fit into a universal naming convention. </p>

<p>I know I could solve this problem hard-coding and using regex, but I wanted to try to solve this using machine learning in python and wanted to know if anyone could recommend me a library or method to try tackling this task.</p>

<p>The idea is that there are a bunch of devices for clients we serve which are indexed in multiple databases with different names and I'd like to rename them all to have the same name for all current and future devices added in.</p>

<p>For the most part, it should be relatively easy for the human eye to match up the different names and it is not complicated to write a function that does the same, but as I mentioned I think it would be fun to try to do this as an NLP task:</p>

<p>Example of my training data of before -> after:</p>

<pre><code>MALIBU BEACH 4KV CIRCUIT BREAKER -&gt; MALIBU BEACH 4KV CB
MALIBU BEACH CIRCUIT BREAKER -&gt; MALIBU BEACH 4KV CB
MALIBU BEACH CB -&gt; MALIBU BEACH 4KV CB
MALIBU BCH CB -&gt; MALIBU BEACH 4KV CB
BEVERLY HILLS 3N -&gt; BEVERLY HILLS NO.3N BANK
BEVERLY HILLS BK 3N -&gt; BEVERLY HILLS NO.3N BANK
BEVERLY HILLS NO.3N BK -&gt; BEVERLY HILLS NO.3N BANK
BEVERLY H BANK 3N -&gt; BEVERLY HILLS NO.3N BANK
</code></pre>

<p>From the perspective of writing a function to do this, I'd use regex to capture things like the voltage level, the device type, and city name and then reformat it to the fixed format.</p>

<p>From an NLP perspective, if I create a bunch of before -> after cases to train a model, would it be able to handle things like the abbreviations? </p>

<p>Such as figuring out on its own that MALIBU BCH is a reference to MALIBU BEACH like a human would, and apply the logic of matching abbreviations in training to future information passing through </p>

<p>Or do I need to create a separate dictionary for the model to reference something like a list of possible abbreviations?</p>

<p>Would a library like NLTK be the right choice for handling a problem such as this? </p>

<p>The upside I see to this method is that by building a ML solution, I would lay out the framework to have a more robust tool to fix other similar non-uniform data issues on the system rather than just this specific one by going the hard-coding method.</p>
","nlp"
"54707","Using nlp to analyze accident report","2019-06-28 16:51:31","","2","497","<machine-learning><nlp>","<p>I want to use Natural Language Processing to analyze traffic accident reports and from the text determine two things: </p>

<ol>
<li>Direction of vehicle travel (just compass directions like north, southeast, etc.)</li>
<li>Vehicle movement descriptions (e.g. backing, turning left on a red, stopped in traffic, turning right, parked).</li>
</ol>

<p>A fragment of an accident report narrative looks something like this:</p>

<blockquote>
  <p>TU2 was parked and attended on the west side of Main St, facing west,
  engine turned off. TU1 was parked on the east side of Main St, facing
  east and exited the parking spot, driving backwards. TU1's rear
  collided with TU2's rear. TU2 was still parked with the engine off
  when rear ended by TU1.</p>
</blockquote>

<p>The accident reports are in thousands of files, with all accident reports for one year in one city in a single file.  The ""answers"" (labels) to vehicle travel direction and movement descriptions are provided, along with the narrative of the accident report.  So I have a decent training dataset.</p>

<p>I was thinking about starting with an approach like an n-gram bag of words and a simple classifier for vehicle direction (north, southwest, etc.).  Would that be a good start?</p>
","nlp"
"54671","Is there any library available for balancing imbalanced text dataset?","2019-06-28 09:14:42","54697","2","1271","<deep-learning><nlp><preprocessing><text>","<p>I have a text dataset similar to newsgroup dataset, the problem with the dataset is that it is highly imbalanced. So is there any readily built library that will do upsampling or downsampling with a function call?</p>

<p><a href=""https://i.sstatic.net/p8PnQ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/p8PnQ.png"" alt=""Imbalanced dataset""></a></p>
","nlp"
"54564","Do word embeddings help with out of vocab tokens?","2019-06-26 18:40:26","","0","408","<nlp><word-embeddings><missing-data><text><embeddings>","<p>I am performing sentiment analysis on a custom dataset of text with Keras but am a little confused about word embeddings. I have been able to train an ""Embedding"" layer and have also learned to load existing weights from Glove but am still facing a few problems. The main one being that there are certain ""negative"" words I know of but that do not appear in the vocab. Because of this, when i try examples with word that do NOT appear in the vocab (like the word ""rubbish"") the network does not know that this contains a negative sentiment. </p>

<p><strong>Is there a way to use Word2Vec / Glove / etc to pass in the word rubbish, find the similarity to the word garbage, and then pass that known word into the network instead?</strong> And if so is that handled by the ""Embedding"" layer or is it an additional step I need to perform during pre-processing?</p>

<p>Additionally, how can I handle misspelled words? For instance, how can i associate ""rubbbbbish"" with ""rubbish""?</p>

<p>I am new to text classification and would really appreciate a bit of guidance! </p>
","nlp"
"54517","How to utilize dictionary data set for text classification?","2019-06-26 07:08:48","","0","122","<classification><nlp><word2vec><word-embeddings><text>","<p>I have a dataset similar to newsgroup20 for classification. With the training dataset, I have a dictionary data set that explains some jargons in the training dataset. These both are different data set, So how will i utilize the dictionary dataset for improving my model accuracy?</p>
","nlp"
"54467","Embedding dimension size for a custom Word2Vec?","2019-06-25 14:17:25","","2","733","<nlp><word2vec>","<p>Are there any guidelines for choosing the embedding dimension size value in a custom Word2Vec embedding? I know that the default is 100 and that seems just as good as any. But I'm wondering if there is any data out there that creates heuristics from when you should deviate from this value. </p>

<p>I can't imagine that there is much benefit in a smaller size, but there must be some value in a larger size? Or maybe it's related to the size of my vocabulary? I have a relatively small vocabulary for my latest project (7,000 words) so maybe there is some ratio or proportion that I can apply? </p>
","nlp"
"54412","How to add a CNN layer on top of BERT?","2019-06-24 21:26:21","","5","9086","<nlp><cnn><pytorch><transfer-learning><bert>","<p>I am just playing with <strong>bert (Bidirectional Encoder Representation from Transformer)</strong> <br>
<a href=""https://arxiv.org/abs/1810.04805"" rel=""noreferrer"">Research Paper</a></p>

<p>Suppose I want to add any other model or layers like Convolutional Neural Network layers (CNN), Non Linear (NL) layers on top of BERT model. <strong>How can I do this?</strong></p>

<p><strong>I am not able to figure out where should I change in code of BERT.</strong> I am using the pytorch implementation of <a href=""https://github.com/huggingface/pytorch-pretrained-BERT"" rel=""noreferrer"">bert</a>
from huggingface.</p>

<p><em>This is what I want to do:</em>
<a href=""https://i.sstatic.net/LGIlP.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/LGIlP.png"" alt=""enter image description here""></a>
<a href=""https://i.sstatic.net/ckzSB.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/ckzSB.png"" alt=""enter image description here""></a></p>

<p><strong>Please show steps to implement this using sudo code</strong> which will help me in implemention of cnn on top of BERT.</p>
","nlp"
"54370","Emotional tension score in sentences","2019-06-24 08:41:17","","1","74","<text-mining><nlp><sentiment-analysis>","<p>I am beginner in natural language processing and my goal is to find a way to score sentences based on their emotional tension. More specifically, I would like to know to what degree a sentence transmits wish, hate, or fear. I looked at some studies on sentiment analysis, but haven't seen any relevant result. Most of the them are on negativity or positivity of a sentence.  </p>
","nlp"
"54257","Does it make sense to concatenate datasets to improve accuracy of model?","2019-06-21 22:02:45","54283","1","127","<nlp><dataset>","<p>I'm planning on training an NER model, I already do have a large corpus but I did find one more large corpus and I'm quite confident that I can source even more corpora and format its data to my needs.</p>

<p>Assuming all datasets eventually are in the same format and don't have any inconsistencies, does it make sense to concatenate all datasets?</p>

<p>Or does the model converge anyways at some point? Does that make sense at all? What other problems could I encounter?</p>
","nlp"
"54230","What is the difference between and Embedding Layer and an Autoencoder?","2019-06-21 15:52:36","","12","11811","<nlp><word2vec><word-embeddings><dimensionality-reduction><embeddings>","<p>I'm reading about Embedding layers, especially applied to NLP and word2vec, and they seem nothing more than an application of Autoencoders for dimensionality reduction. Are they different? If so, what are the differences between them?</p>
","nlp"
"54166","I have a word2vec embedding - now what?","2019-06-20 15:45:22","","0","106","<neural-network><keras><nlp><word2vec>","<p>I've always relied on the Keras embedding layer for my NLP work. But for my latest project I want to use a custom embedding layer. I have gone through the steps to create a word2vec file but now what? Can someone provide an example of how I can replace the Keras embedding layer with my own layer when constructing a model? </p>
","nlp"
"54120","Word Embedding for Item Names(integer, one-hot encoding)","2019-06-20 04:48:43","","1","575","<python><nlp><word-embeddings><word>","<p>I am looking for the way to get the similarity between two item names using integer encoding or one-hot encoding.</p>
<p>For example, &quot;lane connector&quot; vs. &quot;a truck crane&quot;.<br />
I have 100,000 item names consisting of 2~3 words as above.<br />
also, items have its size(36mm, 12M, 2400*1200...) and unit(ea, m2, m3, hr...)</p>
<p>I wanna make (item name, size, unit) as a vector. To do this, I need to change texts to numbers using some way. All I found is only word2vec things, but my case has no context corpus. So I don't think it is possible to learn some context from my data.</p>
<p><a href=""https://i.sstatic.net/6JAKa.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/6JAKa.png"" alt=""Example Image of dataset"" /></a></p>
","nlp"
"54093","Feature selection or Dimension reduction in unsupervised learning","2019-06-19 15:30:15","","2","635","<nlp><clustering><feature-selection><unsupervised-learning><dimensionality-reduction>","<p>I'm trying to do Embedded clustering using kmeans.</p>

<p>This is customer data, so it involves a lot of sentences, so I'm using the universal sentence encoder before clustering.</p>

<p>But I should be doing a feature selection or dimensionality reduction before embedding the features.</p>

<p>I want to know if there is a method to do feature selection or dimensionality reduction in unsupervised learning. This could be very helpful as the clustering is giving a mixed result as of now and I have a strong feeling that this could be because of the unwanted attribute in the data.</p>

<p>I have read all the resources which only gave options to be done on the supervised learning.</p>

<p>Any help is appreciated!</p>

<p>Thanks
Arav</p>
","nlp"
"54033","Using doccano for Aspect Based Sentiment Analysis annotation","2019-06-18 17:36:11","54055","4","570","<nlp><sentiment-analysis><tools><annotation>","<p>Currently looking for a good tool to annotate sentences regarding aspects and their respective sentiment polarities.</p>

<p>I'm using SemEval Task 4 as a reference. The following is an example in the training dataset:</p>

<pre><code>&lt;sentence id=""2005""&gt;
    &lt;text&gt;it is of high quality, has a killer GUI, is extremely stable, is highly expandable, is bundled with lots of very good applications, is easy to use, and is absolutely gorgeous.&lt;/text&gt;
    &lt;aspectTerms&gt;
        &lt;aspectTerm term=""quality"" polarity=""positive"" from=""14"" to=""21""/&gt;
        &lt;aspectTerm term=""GUI"" polarity=""positive"" from=""36"" to=""39""/&gt;
        &lt;aspectTerm term=""applications"" polarity=""positive"" from=""118"" to=""130""/&gt;
        &lt;aspectTerm term=""use"" polarity=""positive"" from=""143"" to=""146""/&gt;
    &lt;/aspectTerms&gt;
&lt;/sentence&gt;
</code></pre>

<p>Can I easily use doccano for such a task? Or would I be better off using some other tool, such as brat?</p>
","nlp"
"53978","Which libraries in Python are there in NLP to tokenize the Hindi sentence?","2019-06-18 00:12:22","","1","2585","<python><neural-network><nlp><text-mining><nltk>","<p>For English language there are libraries like NLTK, CoreNLP which are used for Text Normalization, Word Tokenization and Detokenization, Sentence Splitting etc.
Like English, is there any library to do above operation using Hindi Script ?</p>
","nlp"
"53950","How does Byte Pair Encoding work?","2019-06-17 13:42:33","53957","0","603","<nlp><encoding>","<p>I am using <a href=""https://github.com/rsennrich/subword-nmt"" rel=""nofollow noreferrer"">this</a> to do some Byte Pair Encoding (BPE). My corpus looks like <a href=""https://gist.github.com/shamoons/4bf9e78cd92624bcb120644fb995454a"" rel=""nofollow noreferrer"">this</a>. </p>

<p>When I run the <code>learn_bpe</code>, I get a vocabulary that looks like <a href=""https://gist.github.com/shamoons/14bc594c19cbd7a5f2fae2e36549ff0c"" rel=""nofollow noreferrer"">this</a>.</p>

<pre><code>e r
r e
o n
o r
t i
) ;&lt;/w&gt;
a c
n t
' ,&lt;/w&gt;
er r
a l
r o
h e
m e
</code></pre>

<p>When I try to combine it again to see if it worked with <code>ubword-nmt apply-bpe -c data/jsvocab.txt &lt; data/javascript.txt &gt; tst.txt</code>, <a href=""https://gist.github.com/shamoons/11750709c2f29f0078bb419c13a5b3f3"" rel=""nofollow noreferrer"">the resulting file</a> has a lot of strange <code>@</code> characters. </p>

<pre><code>const p@@ re@@ F@@ or@@ m@@ at@@ t@@ e@@ d@@ B@@ l@@ o@@ c@@ k@@ N@@ a@@ me@@ s = {
    '@@ ap@@ i@@ -@@ p@@ ro@@ j@@ ect@@ s@@ '@@ : '@@ A@@ P@@ I P@@ ro@@ j@@ ect@@ s@@ ',
    '@@ b@@ a@@ s@@ i@@ c@@ -@@ c@@ ss@@ '@@ : '@@ B@@ a@@ s@@ i@@ c C@@ S@@ S@@ ',
    '@@ b@@ a@@ s@@ i@@ c@@ -@@ h@@ t@@ m@@ l@@ -@@ and@@ -@@ h@@ t@@ m@@ l@@ 5@@ '@@ : '@@ B@@ a@@ s@@ i@@ c H@@ T@@ M@@ L an@@ d H@@ T@@ M@@ L@@ 5@@ ',
    '@@ c@@ ss@@ -@@ f@@ le@@ x@@ b@@ o@@ x@@ '@@ : '@@ C@@ S@@ S F@@ le@@ x@@ b@@ o@@ x@@ ',
    '@@ c@@ ss@@ -@@ g@@ r@@ i@@ d@@ '@@ : '@@ C@@ S@@ S G@@ r@@ i@@ d@@ ',
    de@@ v@@ o@@ p@@ s@@ : '@@ D@@ e@@ v@@ O@@ p@@ s@@ ',
    e@@ s@@ 6@@ : '@@ E@@ S@@ 6@@ ',
    '@@ in@@ f@@ or@@ m@@ ation@@ -@@ se@@ c@@ ur@@ i@@ t@@ y@@ -@@ w@@ i@@ th@@ -@@ he@@ l@@ me@@ t@@ j@@ s@@ '@@ : '@@ I@@ n@@ f@@ or@@ m@@ a@@ ti@@ o@@ n S@@ ec@@ ur@@ i@@ t@@ y w@@ i@@ t@@ h H@@ e@@ l@@ me@@ t@@ J@@ S@@ ',
    j@@ q@@ u@@ er@@ y@@ : '@@ j@@ Q@@ u@@ er@@ y@@ ',
    '@@ j@@ s@@ on@@ -@@ ap@@ i@@ s@@ -@@ and@@ -@@ a@@ j@@ a@@ x@@ '@@ : '@@ J@@ S@@ O@@ N A@@ P@@ I@@ s an@@ d A@@ j@@ a@@ x@@ ',
    '@@ m@@ on@@ g@@ o@@ d@@ b@@ -@@ and@@ -@@ m@@ on@@ g@@ o@@ o@@ se@@ '@@ : 'M@@ on@@ g@@ o@@ D@@ B an@@ d M@@ on@@ g@@ o@@ o@@ se@@ ',
    '@@ t@@ he@@ -@@ d@@ o@@ m'@@ : '@@ T@@ h@@ e D@@ O@@ M@@ ',
    '@@ ap@@ i@@ s@@ -@@ and@@ -@@ m@@ i@@ c@@ ro@@ serv@@ i@@ c@@ e@@ s@@ '@@ : '@@ A@@ P@@ I@@ s an@@ d M@@ i@@ c@@ ro@@ serv@@ i@@ c@@ e@@ s@@ ',
    '@@ ap@@ i@@ s@@ -@@ and@@ -@@ m@@ i@@ c@@ ro@@ serv@@ i@@ c@@ e@@ s@@ -@@ p@@ ro@@ j@@ ect@@ s@@ '@@ : '@@ A@@ P@@ I@@ s an@@ d M@@ i@@ c@@ ro@@ serv@@ i@@ c@@ e@@ s P@@ ro@@ j@@ ect@@ s@@ '
}@@ ;
</code></pre>

<p>And so on. I'm not sure what I'm missing, but it seems that it didn't fully reconstruct the text from the vocabulary?</p>
","nlp"
"53921","Assigning tags to posts using predefined set of tags","2019-06-17 05:23:47","","2","81","<text-mining><nlp>","<p>I want to tag the text of a post with a predefined set of tags. A post could have multiple tags such as health, addiction, etc. I want to recommend up to <span class=""math-container"">$5$</span> tags. Total of <span class=""math-container"">$60$</span> tags is present. Nearly <span class=""math-container"">$50$</span> posts with tags are available for testing the results.</p>

<p><strong>My approach</strong>: Remove stopwords, punctuations. Find the similarity(cosine) between word vector of each word of the post and the vectors of all the tags.</p>

<p><strong>Problem:</strong> Context sensitive tags like fired(as in from a job) are shown for irrelevant post e.g. 'car back-fired) and only on average <span class=""math-container"">$3$</span> out <span class=""math-container"">$5$</span> most similar tags are relevant.</p>

<p>Gathered more posts(<span class=""math-container"">$~200$</span> with average word length <span class=""math-container"">$40$</span>) from other websites. Tried preprocessing of the posts: <code>lemmatization</code> and <code>stemming</code>, created dictionary, made bow corpus then used Topic modeling (Latent Dirichlet Allocation)</p>

<p>Used gensim.models.LdaMulticore tried both BOW and tf-idf models but the topics produced had low confidence(of the order <span class=""math-container"">$0.07$</span>) for the words in them. Found the relevant tags (using vector similarity) considering only top <span class=""math-container"">$10$</span> words of each topic. But the performance degraded even more as now at most <span class=""math-container"">$2$</span> tags were relevant.</p>

<p><strong>Conditions</strong>:The tags are diverse and finding posts/text related to each tag is difficult. Tags are not be modified.</p>

<p>Does anyone have a better approach? Any help would be appreciated.</p>
","nlp"
"53919","Understanding Layers in Recurrent Neural Networks for NLP","2019-06-17 04:09:49","","0","92","<deep-learning><nlp><lstm><rnn>","<p>In convolution neural networks, we have a concept that inner layers learn fine features like lines and edges, while outer layers learn more complex shapes.</p>

<p>Do we have any such understanding for layers in RNNs (like LSTMs), something like inner layers understand grammar while outer layers understand more complete meanings of sentences assuming that we are using the LSTM for some natural language task like text summarization?</p>
","nlp"
"53882","Can word embedding be used for text classification on a mix of English and non-English text?","2019-06-16 03:02:42","53883","2","137","<deep-learning><keras><nlp><word-embeddings>","<p>I'm doing text classification on text messages generated by consumers and just realized even though most of the replies provided by consumers are in English, some are in French. I've used Keras word embedding, conv1D and maxpooling to learn the structure in the text and didn't use any other text preprocessing techniques such as stop words removal etc. </p>

<p>In this case, I think it should be fine to use word embedding on both languages since word embedding learns the meaning of individual words regardless of languages...Is this reasonable? Or maybe I do need to separate the languages and build different models for each language?</p>
","nlp"
"53875","What is whole word masking in the recent BERT model?","2019-06-15 23:13:57","53910","12","7755","<nlp><language-model><bert>","<p>I was checking <a href=""https://github.com/google-research/bert"" rel=""noreferrer"">BERT GitHub page</a> and noticed that there are new models built from a new training technique called ""whole word masking"". Here is a snippet describing it:</p>

<blockquote>
  <p>In the original pre-processing code, we randomly select WordPiece tokens to mask. For example:</p>
</blockquote>

<pre><code>Input Text: the man jumped up , put his basket on phil ##am ##mon ' s head

Original Masked Input: [MASK] man [MASK] up , put his [MASK] on phil [MASK] ##mon ' s head
</code></pre>

<blockquote>
  <p>The new technique is called Whole Word Masking. In this case, we always mask all of the the tokens corresponding to a word at once. The overall masking rate remains the same.</p>
</blockquote>

<pre><code>Whole Word Masked Input: the man [MASK] up , put his basket on [MASK] [MASK] [MASK] ' s head
</code></pre>

<p>I can't understand ""<em>we always mask all of the the tokens corresponding to a word at once</em>"". ""jumped"", ""phil"", ""##am"", and ""##mon"" are masked and I am not sure how these tokens are related.</p>
","nlp"
"53814","Pytorch: How to implement nested transformers: a character-level transformer for words and a word-level transformer for sentences?","2019-06-14 18:44:26","","2","308","<nlp><pytorch><transformer>","<p>I have a model in mind, but I'm having a hard time figuring out how to actually code it in Pytorch, especially when it comes to training the model (e.g. how to define mini-batches, etc.). First of all let me quickly introduce the context:</p>

<p>I'm working on VQA (visual question answering), in which the task is to answer questions about images, for example:</p>

<p><a href=""https://i.sstatic.net/UW8hE.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/UW8hE.png"" alt=""enter image description here""></a></p>

<p>So, letting aside many details, I just want to focus here on the NLP aspect/branch of the model. In order to process the natural language question, I want to  use <em>character-level</em> embeddings (instead of traditional <em>word-level</em> embeddings) because they are more robust in the sense that they can easily accommodate for morphological variations in words (e.g. prefixes, suffixes, plurals, verb conjugations, hyphens, etc.). But at the same time I don't want to lose the inductive bias of reasoning at the word level. Therefore, I came up with the following design:</p>

<p><a href=""https://i.sstatic.net/Tl7bu.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Tl7bu.png"" alt=""enter image description here""></a></p>

<p>As you can see in the picture above, I want to use <a href=""http://papers.nips.cc/paper/7181-attention-is-all-you-need"" rel=""nofollow noreferrer"">transformers</a> (or even better, <a href=""https://arxiv.org/abs/1807.03819"" rel=""nofollow noreferrer"">universal transformers</a>), but with a little twist. I want to use 2 transformers: the first one will process each word characters in isolation (<em>character-level</em> transformer) to produce an initial word-level embedding for each word in the question. Once we have all these initial word-level embeddings, a second <em>word-level</em> transformer will refine these embeddings to enrich their representation with context, thus obtaining <em>context-aware word-level</em> embeddings.</p>

<p>The full model for the whole VQA task obviously is more complex, but I just want to focus here on this NLP part. So my question is basically about which Pytorch functions should I pay attention to when implementing this. For example, since I'll be using <em>character-level</em> embeddings I have to define a <em>character-level</em> embedding matrix, but then I have to perform lookups on this matrix to generate the inputs for the <em>character-level</em> transformer, repeat this for each word in the question and then feed all these vectors into the <em>word-level</em> transformer. Moreover, words in a single question can have different lengths, and questions within a single mini-batch can have different lengths too. So in my code I have to somehow account for different lengths at both word and question levels simultaneously within a single mini-batch (during training), and I've got no idea how to do that in Pytorch or whether it's even possible at all.</p>

<p>Any tips on how to go about implementing this in Pytorch that could lead me in the right direction will be deeply appreciated.</p>
","nlp"
"53783","Which approach to select category based on keywords","2019-06-14 10:59:58","","1","534","<nlp><clustering><topic-model>","<p>I want to assign a certain category to a group of keywords.
So i.e. people can upload images or videos, when they do this they can set keywords for this.
These keywords are free to type so words can be spelled in different ways.
The amount of keywords are 95% between 0 and 20 words.</p>

<p>I want to create categories from these. So that I can assign a combination of keywords to a category.</p>

<p>The categories and the amount of categories are undefined.</p>

<p>From what I've researched is that this is probably a Topic modeling or clustering issue. Although with topic modeling most examples I see are based on long texts instead of a couple of keywords. </p>

<p>What would be a good approach to handle this?</p>

<p>I thought about first some simple fuzzywuzzy to find different spellings of the same words.</p>

<p>Create a big word list from this. Then each keyword will be matched against the list and rewritten if it matches one and added if there isn't a good match.</p>

<p>Then I would need to create groups and here I don't know which algorithms I should use.</p>

<p>I was thinking maybe do k-means clustering and then see on which k I get the best results and then assign a category to it manually by looking at which keywords are in it.</p>

<p>So it would be nice to just let the algo figure the amount and categories out, but it can be relaxed that I will set them before.</p>

<p>Does anyone have a better suggestion or are there just complete algorithms for this already available?</p>
","nlp"
"53781","How can I use all possible spelling correction of documents before clustering those documents?","2019-06-14 10:03:43","","1","193","<nlp><word2vec>","<p>I have the data set with many documents of 50 to 100 words each. </p>

<p>I need to clean those data by correcting misspelled words in those documents.</p>

<p>I have an algorithm which predicts possible correct words for misspelled word.</p>

<p>The problem is I need to choose or verify the predictions made by that algorithm in order to clean the spelling errors in the documents.</p>

<p>Can I use all the possible correct words predicted for correct spelling in word vector to perform clustering on those data? </p>
","nlp"
"53772","Get long answers from BERT","2019-06-14 08:27:56","","1","503","<nlp><bert>","<p>We are using Google BERT for question and answering. We are using vanialla bert-base-uncased as well as squad trained checkpoints.   </p>

<p>The answers from BERT are very short and crisp. For example, if we ask describe a chatbot, then it will simply return, takes input from user and replies ...  Though the answer actually is complete paragraph.  </p>

<ul>
<li>Will training BERT on long paragraphs can solve this problem? If yes, any idea from where we can get such a QnA dataset, as it is not possible to create a huge QnA dataset manually.</li>
<li>Is there any other tweak which can be done at some BERT layer, so that it starts understanding a long answer?</li>
<li>Or is there any other framework or system already have solved this kind of problem, by maybe integrating with some other neural network technique, or by using a pipeline of multiple components?</li>
</ul>
","nlp"
"53737","Neural net classifier outputting extreme probabilities","2019-06-13 19:47:19","","1","115","<machine-learning><neural-network><deep-learning><classification><nlp>","<p>I am training a multi-label neural network text classifier (i.e. a given sample can have more than one label; most samples have exactly 1 label though):</p>

<ol>
<li><p>Single-layer BiLSTM producing a sequence of (16, 512) vectors with dropout of 0.2</p></li>
<li><p>Self attention over each output vector of LSTM to produce a single 512-dimension ""sentence vector""</p></li>
<li><p>L2 normalize sentence vector</p></li>
<li><p>ReLU layer on the normalized vector with about 750 units</p></li>
<li><p>Dropout of 0.3</p></li>
<li><p>Final sigmoid output layer to output a probability for each of the K classes (K~1000)</p></li>
</ol>

<p>Firstly, does L2 normalization help or make things worse?</p>

<p>Secondly, this model is quite accurate but outputs very extreme ""probabilities"" for each class for the samples - very often &lt; 0.00001 or > 0.9999 probability, instead of something less extreme (like say 0.8 or 0.2 or whatever). Any idea of what might be going wrong? If this is overfitting -- I'm using quite a bit of dropout too but that didn't help either. How can I remedy this?</p>
","nlp"
"53717","Features used by MAUI key phrase extraction tool","2019-06-13 13:07:04","53727","0","88","<machine-learning><python><nlp>","<p>I have been trying out keyphrase extraction for a while and I want to know what are all the features that MAUI <a href=""https://github.com/zelandiya/maui-standalone"" rel=""nofollow noreferrer"">MAUI github</a> uses for training the keyphrase extraction?</p>
<p>Having read <a href=""https://www.airpair.com/nlp/keyword-extraction-tutorial"" rel=""nofollow noreferrer"">this</a> its hard for me to understand every feature that she uses can someone help me give a brief about the features that she is using? and some references for the same if any.</p>
","nlp"
"53681","Metrics for Name Entity Recognition","2019-06-13 07:10:50","","2","1196","<machine-learning><nlp><model-evaluations><metric>","<p>Working on a NER project, I have been facing the problem of evaluating my model during training. I cannot be using the accuracy metrics or f1 score or any other metrics to evaluate my model on runtime as this always leads to high values due to a number of O (other) tags in the dataset. </p>
","nlp"
"53655","In the Keras Tokenizer class, what exactly does word_index signify?","2019-06-12 17:44:49","53660","3","8806","<keras><nlp>","<p>I'm trying to really understand Tokenizing and Vectorizing text in machine learning, and am looking really hard into the Keras Tokenizer class. I get the mechanics of how it's used, but I'd like to really know more about it. So for example, there's this common usage:</p>

<pre><code>tokens = Tokenizer(num_words=SOME_NUMBER)
tokens.fit_on_texts(texts)
</code></pre>

<p>tokens returns a word_index, which maps words to some number.
Are the words all words in texts, or are they maxed at SOME_NUMBER?
And are the dict values for word_index the frequency of each word, or just the order of the word?</p>
","nlp"
"53569","Why categorical cross entropy loss is not correlated with NLP scores?","2019-06-11 12:49:07","","3","436","<deep-learning><nlp>","<p>I'm training a deep network for image captioning which is consist of one CNN and three GRUs. During training epoch by epoch model loss (categorical cross entropy) decreases but when I'm measuring <code>bleu</code>,<code>METEOR</code>,<code>ROUGE</code>,<code>CIDEr</code> and <code>SPICE</code> scores,I get best ones in the first epoch that has worst loss. I don't get why this is happening? And if categorical cross entropy is not a suitable loss function for autoencoder then what should I use instead?</p>
","nlp"
"53549","Learning Embeddings for One Word","2019-06-10 23:04:26","","1","22","<nlp><word-embeddings>","<p>I have a non-conventional <code>NLP</code> task. I am looking to develop a sequence to a vector model. Instead of employing <code>one-hot encoding</code> for vectorization of the input, I am trying to see if it will be possible to learn to embed for the input text which is usually only one non-human word. For example: </p>

<pre><code>1. GHJJRIDBDL7US = positive
2. LDNF3DM&lt;VAYFI = negative
3. OFNE6NKFE1NNE = positive
</code></pre>

<p><code>ULMFiT</code> is the state of the art now. Any idea if and how it can be applied to this problem? Any idea on how to learn the embedding for such one-word data will be helpful.</p>
","nlp"
"53462","Incorrect Text Classification, But Accurate Model. Do I Perform Manual Text Classification For A Data Set?","2019-06-08 20:46:24","","1","188","<machine-learning><classification><nlp><data><bert>","<p>I'm currently using Google's BERT pre-trained sentiment analysis model that is trained on an IMDb pos/neg review dataset. I'm using this model to predict whether tweets are positive (bullish) or negative (bearish). While the model is accurate when plugging in my own test data (F1 Score ~86%), the classification itself is not accurate. Tweets that are undoubtedly positive/bullish, and not classified as so. Perhaps this is because the language in the investment world is different than a movie review - which uses universally recognized positive/negative words and/or sentences.</p>

<p>The same is true when I take my tweet dataset and use Vader SentimentIntensityAnalyser to parse pos/neg tweets into separate folders.</p>

<p>So my question is... since the language that is used for telling whether a stock is bullish/bearish is uniquely different from that of an Amazon review, or movie review, would it be optimal for me to manually classify my dataset into positive (bullish) and negative (bearish) datasets?  </p>
","nlp"
"53448","nltk's stopwords returns ""TypeError: argument of type 'LazyCorpusLoader' is not iterable""","2019-06-08 14:25:42","53450","0","10766","<nlp><preprocessing><nltk><text>","<p>While trying to remove stopwords using the nltk package, the following error occurred:</p>

<pre><code>from tqdm import tqdm 
import nltk
from nltk.corpus import stopwords

preprocessed_reviews = []
for sentance in tqdm(final[""Text""].values):
    sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in stopwords)
</code></pre>

<blockquote>
<pre><code>TypeError                                 
  Traceback (most recent call last)
&lt;ipython-input-136-ac5c19fafd9c&gt; in &lt;module&gt;()
---&gt; 7     sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in stopwords)
     8     preprocessed_reviews.append(sentance.strip())
     9 

TypeError: argument of type 'LazyCorpusLoader' is not iterable
</code></pre>
</blockquote>
","nlp"
"53369","What is the best approach to perform information extraction from tourist reviews using NLP, DL?","2019-06-07 05:04:41","","0","155","<machine-learning><deep-learning><nlp><text-mining>","<p>I am interested in performing some information extraction from tourist reviews about different places.</p>
<p>I have data of 50 different places and around 300-400 reviews about each of them and I would like to get information like: <em>weather, experience(happy/not happy), costly/cheap, places around, best time to visit, etc.</em></p>
<p><strong>Reviews are like:</strong></p>
<blockquote>
<ol>
<li><p>Mahabaleshwar is a hill station situated nearby Pune. You can reach
there by road via Pune. Air connectivity is available up to Pune and
from there you can travel by road. Mahabaleshwar is having so many
mountain range. Weather is remaining very beautiful throughout the
year.
There are so many points to be visited in Mahabaleshwar like Parsi
point, Elphinstone point, Wilson Point, Arthur seat point. There is
Vennna Lake where you can have boating and horse riding. Strawberry is
available at each and every place in Mahabaleshwar which is its main
attraction. There are so many hotels available but they are somewhat
costly. By the way it is the nice place to visit.</p>
</li>
<li><p>Mahabaleshwar is a best place for tourists. It is located in Maharashtra. I also visit Mahabaleshwar when I'm free time. It is
famous for their's natural beauty. Here is a one river ans when I
visit to Mahabaleshwar then I definitely go to ride on small boat, it
Is very joyful.</p>
</li>
</ol>
</blockquote>
<p>Need some suggestions on what algorithm can be used and about data processing approach. Links or tutorials are would be helpful too.
Thanks!</p>
","nlp"
"53340","Does it make sense to use TF-IDF to extract most important tokens from a corpus?","2019-06-06 14:31:01","","1","555","<text-mining><nlp><tfidf>","<p>I have a collection of documents and I'd like to extract the most important words and phrases from the entire corpus.</p>

<p>My understanding of TF-IDF is that it is calculated per token <em>per document</em>, so the calculated weights are relative to a given document in the corpus. Is there a way to use TF-IDF to recover the most significant terms in the entire corpus, or is this the wrong approach? If the latter, what would be a more appropriate NLP approach?</p>
","nlp"
"53307","How to tackle a multilabel classification problem","2019-06-06 07:29:50","","0","2029","<nlp><lstm><rnn><multiclass-classification><multilabel-classification>","<p>I am trying to build a LSTM model for a multiclass classification problem on textual data. Until now, I have only built a model when one input belongs to one of the categories. What do I do when one input can belong to more than one class (i.e.: one entry of data can belong to 2-3 categories)? Can anyone help me with some blogs or resources to build an intuition for making such a model?</p>

<p>Thanks.</p>
","nlp"
"53298","Where can I learn the complete mathematics involved in LDA?","2019-06-06 04:42:29","","2","163","<nlp><sentiment-analysis><lda><dirichlet>","<p>I have come across Latent Dirichlet Allocation (LDA) on multiple occasions while reading about sentiment analysis and recommender systems.</p>

<p>Where can I find good reading material which explains the concept in depth, especially by taking an example?</p>
","nlp"
"53145","What is the best technique to transform documents into vectors?","2019-06-03 16:25:12","","1","36","<nlp><similar-documents><vector-space-models>","<p>What is the best algorithm between doc2vec and Singular Value Decomposition (SVD) to transform a set of 600 documents of around 1000 words each into vectors ?</p>
","nlp"
"53137","Back-Translation model for German and English","2019-06-03 14:51:02","","3","42","<machine-learning><nlp><machine-translation><data-augmentation><text-generation>","<p>Do you know of any pre-trained models for back translation between German and English? I am aware that there are ways to include a monolingual corpus into the training of a machine translation model (often referred to as back translation). I am looking for a model to allow me to generate new, semantically similar German texts by translating them into, say, English and then back. Ultimately, to increase my training corpus with examples that are similar to the existing ones, but with slight variation in sentence structure and word choice.</p>

<p>Obviously, I could just stick together two separate models (one for translating the original texts into English and one for translating the English version back to the original language). However, it would be great if you knew of another that I could use.</p>

<p>I'd very much appreciate any kind of help!</p>
","nlp"
"53112","Predicting class of email based on its associated parameters","2019-06-03 05:44:46","","1","14","<machine-learning><nlp>","<p><a href=""https://i.sstatic.net/ZsAH2.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ZsAH2.png"" alt=""enter image description here""></a></p>

<p>I am solving a challenge where the data represented as above is given. In this challenge I have to associate each email with one of the the 2 criteria (automation, Or SEO). Example if email falls under SEO then I can reach out that person regarding SEO related queries.</p>

<p>I have these parameters associated with each email:</p>

<ul>
<li>Position of the person</li>
<li>Organisation name</li>
<li>Company domain</li>
</ul>

<p>My task is to associate each email with one of the the 2 criteria (automation, Or SEO)</p>

<p>Some entries for Position (job title) are missing from my dataset. But I always have Organisation name and Company Domain and associated email.</p>

<p>To solve this I referred to a well known problem of predicting tags of a stack overflow post: <a href=""https://medium.com/datadriveninvestor/predicting-tags-for-the-questions-in-stack-overflow-29438367261e"" rel=""nofollow noreferrer"">https://medium.com/datadriveninvestor/predicting-tags-for-the-questions-in-stack-overflow-29438367261e</a></p>

<p>But in my case is this approach suitable? How should I approach this problem with a better method?</p>
","nlp"
"53110","Embedding Values in word2vec","2019-06-03 03:35:58","53111","1","560","<machine-learning><nlp><word2vec><machine-translation>","<p>Are the embedding values for a particular word using word2vec Skipgram model the weights of the first layer or the softmax output of the function?
Does the embedding value change according to the training corpus?</p>
","nlp"
"52981","Linking LDA topics to the input documents","2019-05-31 14:22:17","","0","72","<nlp><topic-model><lda><gensim>","<p>I am new to LDA topic modelling. I am using gensim and am able to generate topics that make sense. Using 25k of documents, I can also print them using <code>print_topics</code>. I am aware that we need use the generated model to classify any new document. I would like to know the topics for each training document. Do I have to treat them as new documents to find their topics?</p>

<pre><code>lda.print_topics(num_topics=22, num_words=8)
</code></pre>
","nlp"
"52925","Fewer observations & larger documents vs More observations & smaller documents","2019-05-30 17:25:52","","1","49","<classification><nlp>","<p>Let's suppose that I have a dataset of 1000 documents.</p>

<p>Each document is a restaurant review (so relatively short text) and it has labels {Negative, Indifferent, Positive}.</p>

<p>Let's suppose that the dataset has 600 positive reviews, 200 indifferent reviews and 200 negative reviews.</p>

<p>I want to train a classifier to classify a review as Negative or Indifferent or Positive based on the text of the review.</p>

<p>I am not thinking about using any word embeddings for now so I will probably use a TF or TF-IDF model (even though this may be a bit off topic for current question).</p>

<p>Let's suppose that in my case I split (in a stratified way) my dataset into a training set of 800 observations and into a test set of 200 observations.</p>

<p>My question is the following:
Is it better to have 800 separate documents in my training set or to merge these documents based on its categories/labels and create 3 very big documents?</p>

<p>There 800 separate documents of any of the 3 labels or 3 big documents of each of the labels is the best way to go and why?</p>

<p>My question stems from the fact that in the latter case if for example I do TF-IDF then this will be applied based on different categories/labels since each document will be about a category/label.</p>

<p>On the other hand, if I do (as we usually do actually) like in the former case then the TF-IDF will be categories/labels-agnostic and I do not know this helps things.</p>

<p>Is the answer simply that this an interesting but pretty bad idea because in this way you simply massively decrease the number of the observations with which the model/algorithm is trained and so you make much harder for him to figure out how to successfully classify things?</p>
","nlp"
"52899","Character-level seq2seq with LSTM in Keras for language declension","2019-05-30 09:05:08","","1","226","<keras><nlp><lstm>","<p>I am trying to create sequence to sequence mapping of words on Croatian language, to automatize declension  (<a href=""https://en.wikipedia.org/wiki/Declension"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Declension</a>). English language is fairly simple in that regard, but most world languages do have extensive declension rules. Analogy in English language would be car -> cars, or bus -> buses. Since Croatian language has 14 forms for each noun (7 for singular, and 7 for plural), and there are a lot specific rules with exceptions, deep learning seems like a good strategy to try out. I have created a training dataset with about 2000 entries, and I would like to train a model that is capable of generating 13 other forms from the initial word. I followed several tutorials and examples, but I can not make it work, since model is returning floating-point values, instead of 0 and 1, which I need to revert one-hot encoded vectors for each word. I am guessing that some of the parameters in model are not correct, but after a couple of days trying, I was not able to make it work. My code is here:</p>

<p><a href=""https://github.com/matkosoric/deklinacije"" rel=""nofollow noreferrer"">https://github.com/matkosoric/deklinacije</a></p>

<p>Initial step would be to create mapping for just one form, and then others, and maybe eventually combine outputs.</p>

<p>What I am doing in my Jupyter Notebook is chartacter-level encoding, to get vectors like this:</p>

<pre><code>[[1, 17, 14, 3, 15, 6, 7],
 [1, 17, 3, 10, 2, 19, 2, 8, 1],
 [1, 17, 3, 7, 6, 7, 4],
 [1, 17, 3, 5, 4, 12, 11],
 [1, 22, 5, 2, 9, 1, 7, 9, 1],
 [1, 14, 1],
...
</code></pre>

<p>Then I add zero-padding, and then I do the one-hot encoding of the characters. Finnaly, this is the code for model:</p>

<pre><code>model = Sequential()
model.add(LSTM(28, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True, activation='softmax'))  #, return_sequences=True
# model.add(Dropout(0.2))
model.add(LSTM(28, activation='softmax', return_sequences=True, recurrent_activation=""sigmoid""))          #, return_sequences=False
model.add(Dense(28, activation='sigmoid'))
model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])    # binary_crossentropy | categorical_crossentropy
history = model.fit(X_train, Y_train,
          epochs=20,
          batch_size=128, 
          validation_data = (X_val,Y_val)
         )
</code></pre>

<p>After training and using model for prediction for test set, I get something like this, which can not be converted back to meaningful character sequence:</p>

<pre><code>[0.5826852  0.42405558 0.42048743 0.41398    0.425908   0.4192267
 0.4133946  0.41803166 0.4172448  0.42309627 0.4156795  0.419433
 0.416399   0.41344917 0.4158144  0.4196655  0.42249662 0.42083743
 0.42371705 0.41620597 0.41855162 0.42147878 0.41726097 0.41544187
 0.42190933 0.42433727 0.42017445 0.409134  ][0.5842657  0.4228464  0.41903406 0.41235432 0.42476133 0.41782877
 0.41161612 0.4165     0.415777   0.42182064 0.41404086 0.41801238
</code></pre>

<p>I am guessing that Dense layer at the end should have number 2 as a parameter, since I am expecting zero or one values, but it is not compiling because of <code>return_sequences=True</code> in the LSTM layer before.</p>
","nlp"
"52898","Write way to add samples to torch TabularDataset","2019-05-30 08:39:49","","1","96","<python><nlp><pytorch><torch>","<p>I have a TabularDataset and i would like to add some examples to the dataset.</p>

<pre><code>dataset = TabularDataset(path=path, format=""csv"", fields=[('label', l_f),('data', d_f)])
# I tried this
dataset.examples = new_examples # new_examples is a list of examples 
len(dataset) == len(new_examples) # This is True
</code></pre>

<p>I can see that the length of the dataset has changed and in examples i have the examples from new_exampels, but i don't think this is the wright way to do it because i think that it messes up the other methods in TabularDataset.</p>

<p>What is the wright way to add samples or to create a new dataset from a list of examples? </p>
","nlp"
"52853","Classify documents using a set of known vocabularies","2019-05-29 15:16:11","","1","55","<nlp><text-mining><topic-model>","<p>I have a bunch of documents that I want to classify which ones talk about soccer (unsupervised learning, I do not want to manually label the documents). </p>

<p>One way I am thinking about is to go online and search for the most popular words in soccer articles to make a list of vocabularies (for example: score, shoot, World Cup, etc). Then somehow use that list of vocabularies to classify the documents (maybe if a particular contains 30% of the words in that list of vocabularies, then that document talks about soccer).</p>

<p>I am wondering whether it is a valid method or there are better existing methods. Really appreciate any help. </p>
","nlp"
"52845","What is this called by gathering the meaning from a sentence?","2019-05-29 14:32:31","52860","0","25","<nlp>","<p>What would this process of gathering the meaning of a sentence be called? What would the segments derived from the sentence be called?</p>

<pre><code>""John and Derrek both love cake""
    -&gt; John loves cake
    -&gt; Derrek loves cake

""John was mad that the weather was rainy today""
     -&gt; John was mad
     -&gt; weather was rainy today
</code></pre>
","nlp"
"52831","Integration of NLP and Angular application","2019-05-29 11:08:49","","2","595","<machine-learning><python><classification><numpy><nlp>","<p>I'm doing a small POC in which I've trained my Machine Learning model (Naive Bayes) and saved in &quot;.pkl&quot; (pickle) format. Now my next task is to develop a web application which asks the user to enter the Text for the Text classification analysis. This newly taken (from the user) &quot;TEXT&quot; will be the testing dataset which can be fed to the Naive Bayes model that I built in the earlier stage and make prediction.</p>
<p>Is there any way to convert the text (taken from the user) into numpy array and then transform this numpy array using <code>CountVectorizer()</code> (which was used for training dataset) and then feed to the saved model for predictions?</p>
<p>I want to make an Angular application like <a href=""https://dhormale.github.io/angular-tensorFlow-integration/"" rel=""nofollow noreferrer"">this</a> where the input is an image and this image can be converted into pixels.</p>
<p>In my case, it is plain text which needs to be transformed into NumPy array in an Angular application before I feed it to the Trained Naive Bayes Model.</p>
","nlp"
"52814","Framing Sentences based on keywords","2019-05-29 08:24:28","52960","2","1066","<nlp><nlg>","<p>Given a few specific words, which techniques of Natural Language Processing can I use to achieve creating a meaningful sentence from those words?</p>

<p>eg.</p>

<p><strong>Words:</strong>
jackets, highest sale, sweaters, lowest sale</p>

<p><strong>Sentence:</strong>
Jackets exhibited the highest sales while sweaters sold the least. </p>

<p>If my question is too broad, let me know so I can ask more specific questions.</p>
","nlp"
"52730","Extracting email id gives error","2019-05-27 21:24:22","52731","0","233","<python><nlp><pandas><nltk>","<p>I am extracting email ids and storing them into a new column variable, but I am getting the issue:</p>

<p><a href=""https://i.sstatic.net/CrHpP.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/CrHpP.png"" alt=""enter image description here""></a></p>

<pre><code>    import re
    def email_extract(comments):
        comments1 =re.findall(r'[\w\.-]+@[\w\.-]+',comments)
       return comments1

   data[""email_id""] = data.COMMENTS.apply(lambda x: email_extract(x))
</code></pre>

<hr>

<blockquote>
<pre><code>--- TypeError                                 Traceback (most recent call
last) &lt;ipython-input-33-d9b73bdc4f8e&gt; in &lt;module&gt;()
----&gt; 1 data[""email_id""] = data.COMMENTS.apply(lambda x: email_extract(x))

C:\ProgramData\Anaconda4\lib\site-packages\pandas\core\series.py in
apply(self, func, convert_dtype, args, **kwds)    3192            
else:    3193                 values = self.astype(object).values
-&gt; 3194                 mapped = lib.map_infer(values, f, convert=convert_dtype)    
3195     3196         if len(mapped) and
isinstance(mapped[0], Series):

pandas/_libs/src\inference.pyx in pandas._libs.lib.map_infer()

&lt;ipython-input-33-d9b73bdc4f8e&gt; in &lt;lambda&gt;(x)
----&gt; 1 data[""email_id""] = data.COMMENTS.apply(lambda x: email_extract(x))

&lt;ipython-input-32-97f3705d1972&gt; in email_extract(comments)
      2 def email_extract(comments):
      3     #re_pattern = re.compile(r'[\w\.-]+@[\w\.-]+')
----&gt; 4     comments1 =re.findall(r'[\w\.-]+@[\w\.-]+',comments)
      5     return comments1

C:\ProgramData\Anaconda4\lib\re.py in findall(pattern, string, flags)
    221 
    222     Empty matches are included in the result.""""""
--&gt; 223     return _compile(pattern, flags).findall(string)
    224 
    225 def finditer(pattern, string, flags=0):

TypeError: expected string or bytes-like object
</code></pre>
</blockquote>

<p>How can I fix this issue?</p>
","nlp"
"52719","meaning of fine-tuning in nlp task","2019-05-27 15:48:21","","7","2860","<nlp><word2vec><word-embeddings><transfer-learning><bert>","<p>There are two types of transfer learning model. One is
feature extraction, where the weights of the pre-trained model are not changed while training on the actual task and other is the weights of the pre-trained model can be changed.</p>

<p>According to those categorizations, static word vector like word2vec is a feature extraction model, where each vector encodes the meaning of the word.  </p>

<p>The meaning of the word changes context. For example, ""Bank of the river""  vs ""Bank as a financial institute"". These word2vec vectors do not differentiate between these meaning. </p>

<p>Current models like Bert consider the context. Bert is a language representation model. That means, it internally can represent word by contextual word vectors.</p>

<p>By default, Bert is a fine-tuning model. This is where my imagination about fine-tuning started to fall apart. 
Let's say, on top of Bert model we created some task-specific layer. Now, if we fine tune, by definition, the weights of the lower level (language representation layer) will change at least a bit that means, the vector of the word will also change (if we compare before and after fine tune). That means the meaning of the word change a bit because of the new task.
If my above interpretation is correct, I can not comprehend this phenomenon for example, the word vectors of task sentiment analysis are different that the word vectors (of the same word) of task question answering. Can anybody help me?    </p>

<p>Please correct me if anything above is wrong. Thanks  </p>
","nlp"
"52696","gensim : KeyError: ""word 'sso' not in vocabulary""","2019-05-27 08:07:43","","1","1659","<python><nlp>","<p>I am using Google pretrained word embeddings(word2vec) to train my model. while using getting an error as below : </p>

<pre><code>KeyError: ""word 'sso' not in vocabulary""
</code></pre>

<p>i am trying this below code:</p>

<pre><code>import gensim 
model = gensim.models.KeyedVectors.load_word2vec_format('D:/NLP  JIRA/GoogleNews-vectors-negative300.bin', binary=True) 
print(model)
words = list(model.wv.vocab)
print(words)
print(model[words])
for words in op:
        print(words)
        obj = model[words]
        print(obj)
</code></pre>

<p>input i am passing as </p>

<pre><code>op = [['observations', 'sso', 'clienttool', 'testing'], ['logouri', 'column', 'clients', 'table', 'accepting', 'values', 'null']]
</code></pre>
","nlp"
"52629","How do I visualize data for a natural language processing project?","2019-05-26 04:15:21","","3","139","<nlp>","<p>I am using a question-and-answer dataset. My neural network takes a question and an article content, and outputs where an answer starts (as an integer). To visualize my data, how should I process it and what plot(s) should I use?</p>

<p>I'm considering:</p>

<p>Word/N-gram frequency histogram for the questions. Another one for the answers.</p>

<p>Plots mapping word/n-gram frequency to output features</p>

<p>Plots mapping word/n-gram frequencies to Shannon entropy values.</p>

<p>On that note, maybe using a smaller machine learning model - such as a decision tree - qnd graphing the resulting probabilities.</p>

<p>What is the best plot for a project like mine?</p>
","nlp"
"52547","Comparing word frequencies between two corpora to find keywords","2019-05-24 14:50:44","","0","365","<machine-learning><nlp><statistics>","<p>I have a set of tweets that are either labelled as ironic or non-ironic, and have separated them into two different corpora: ironic and non-ironic. I want to compare these two to see which words are the keywords of each corpora, remove them from the tweets and see how this affects irony detection.</p>

<p>So far, I have simply counted the frequency of the words in each corpus and ranked them this way. I have also computed the relative frequency of each word in the ironic corpus, and then compared it with the same word's relative frequency in the non-ironic corpus (and vice versa). This was done by either taking the difference (e.g. the word ""love"" has a relative frequency of 12 in the ironic corpus, and 5 in the non-ironic corpus, which gives a difference of 7) or the ratio (using the same example: 12 / 7 = 1.7).</p>

<p>However, I have been reading some papers about this, which is making me rethink my method. They mention the following ways of comparing two corpora:</p>

<ol>
<li>Pearson's chi-squared test</li>
<li>log-likelihood ratio test</li>
<li>Welch’s t-Test</li>
<li>Wilcoxon rank-sum test</li>
<li>Bootstrap test</li>
</ol>

<p>Is it best to figure out how to implement these in Python and use one of them instead of comparing the relative frequencies? If so, which ones are suitable to my problem? </p>
","nlp"
"52490","which open-source frameworks exist for fact-extraction","2019-05-23 18:47:46","","1","42","<nlp>","<p>probably question is silly, but still: what tools one might use to transform an English text to a set of ""facts""</p>

<p>for example a sentence ""Julius Caesar crossed the Rubicon in 49 BC"" would be parsed as something like:</p>

<pre><code>{
""subject"": ""Julius Caesar (person)"",
""object"": ""Rubicon (river)"",
""verb"": ""to cross"", 
""verb aspects"": [""time"": ""49BC""]
}
</code></pre>

<p>or anything remotely close to this</p>
","nlp"
"52475","Tokenisation with Spacy - how to get tokens to the left/right of token","2019-05-23 14:50:37","56146","1","1762","<python><nlp>","<p>I am using Spacy for text tokenisation and getting stuck with it:</p>

<pre><code>import spacy
nlp = spacy.load(""en_core_web_sm"")
mytext = ""This is some sentence that spacy will not appreciate""
doc = nlp(mytext)

for token in doc:
    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)
</code></pre>

<p>returns something that seems to me to say that tokenisation was succesful:</p>

<pre><code>This this DET DT nsubj Xxxx True False 
is be VERB VBZ ROOT xx True True 
some some DET DT det xxxx True True 
sentence sentence NOUN NN attr xxxx True False 
that that ADP IN mark xxxx True True 
spacy spacy NOUN NN nsubj xxxx True False 
will will VERB MD aux xxxx True True 
not not ADV RB neg xxx True True 
appreciate appreciate VERB VB ccomp xxxx True False
</code></pre>

<p>but on the other hand</p>

<pre><code>[token.text for token in doc[2].lefts]
</code></pre>

<p>returns an empty list. Is there a bug in lefts/rights?</p>

<p>Beginner at natural language processing, hope I am not falling into a conceptual trap. Using Spacy v'2.0.4'.</p>
","nlp"
"52469","Is it possible to use Word2vec for text paraphrasing?","2019-05-23 13:18:39","","1","406","<nlp><word2vec><text-generation>","<p>After reading several papers I am not sure if it is possible to some how generate text with the same meaning (paraphrase it) using only Word2vec.</p>

<p>I found out other approaches that use sequences of sentence pairs, and they train Neural nets to find the most similar, but this is hard to maintain and it will be hard to generate relevant content like this.</p>

<p>I would like to give raw text to Word2vec powered algorithm that gives paraphrased text.</p>
","nlp"
"52367","What are tokens and tokenizations?","2019-05-22 03:25:28","","3","2399","<machine-learning><nlp>","<p>I'm a high school senior who is new to data science, and would like to get into natural language processing. I currently know nothing about NLP, and the information online can be overwhelming. What are tokens? What are they used for / why do we need to tokenize text?</p>
","nlp"
"52258","Best way to combine two similar document","2019-05-20 12:52:56","","4","1237","<nlp><text-mining><similarity><information-retrieval>","<p>I have f.ex.: two news-articles that report the same event. However, these two text are similar BUT not the same. I would like to combine these two texts creating one text that contains only the most ""<em>relevant</em>"" information.</p>

<p>I was thinking to check the texts paragraph wise on its ""information value"" and then only combine the paragraphs that are the most ""relevant"".</p>

<p>Any suggestions on an approach how to do this? Any research papers about this topic?</p>
","nlp"
"52238","Compute Jaccard distance between two lists of strings","2019-05-20 06:05:46","","0","8334","<python><nlp>","<p>I have input like below:</p>

<pre><code>input_list = [['Search' 'engines','using','machine','learning','pattern','detections'], 

        ['machine','learning','helped','Google','automatically','sift','pages']]

input_list1 = ['Machine','learning','ever','evolving','technology']
</code></pre>

<p>Expecting Jaccard similarity distance between <code>input_list</code> and <code>input_list1</code>.</p>
","nlp"
"52177","How is Bayes theorem being applied in expanding the formula of Binary Independence Model?","2019-05-19 00:30:25","","0","26","<nlp>","<p>The <a href=""https://nlp.stanford.edu/IR-book/html/htmledition/the-binary-independence-model-1.html"" rel=""nofollow noreferrer"">BIM formula</a> makes use of Bayes theorem. Can anyone please explain:</p>

<ul>
<li>How to read the probability of the form <code>P(R=1|x,q)</code>? Is it <code>P((R=1|x),q)</code> or <code>P(R=1|(x,q))</code>? Does <code>,</code> stand for AND here?</li>
<li>How exactly Bayes theorem being applied to expand it?</li>
</ul>

<p><a href=""https://i.sstatic.net/BKDTc.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/BKDTc.png"" alt=""enter image description here""></a>
<a href=""https://i.sstatic.net/SLmQ7.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/SLmQ7.png"" alt=""enter image description here""></a>
<a href=""https://i.sstatic.net/oDIGu.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/oDIGu.png"" alt=""enter image description here""></a></p>
","nlp"
"52167","Categorization of Natural Language Processing Tasks","2019-05-18 18:44:32","","1","311","<nlp>","<h2>Problem</h2>

<p>I am currently learning basics of natural language processing. I see many tasks in this area is assigning labels to each individual words in the sentence, including POS tagging, chunking, named entity recognition and semantic role labeling. </p>

<p>My question is</p>

<ul>
<li>Are there other tasks that <strong>do not</strong> involve assign labels to each word in the sentence. One thing I could think of is sentiment analysis, which is assigning label to entire sentence/document or different aspects of sentence/document.</li>
<li>Is there some survey that is recommended to read that summarize the general tasks in natural language processing. </li>
</ul>
","nlp"
"52026","How to create pretrained word embedding text file with additional word features","2019-05-15 17:55:40","","1","62","<nlp><word2vec><word-embeddings><gensim>","<p>I've had an idea for using word features to improve the quality of neural machine translation. Now, I would like to create word embeddings with additional word features such as pos tag, named entity, lemma, etc. I used gensim to create pretrained word embeddings and pretrained tag embeddings. However, I still do not understand how to deal with it.  Are there any ways to implement this idea? Should I annotated pos tag on a training data file and train a new word embedding model with it?</p>
","nlp"
"51956","Natural language Generator using Data from table","2019-05-14 14:34:06","","2","32","<deep-learning><data-science-model><nlp>","<p>I am working on some natural language generator part. </p>

<p>eg.1
Input to it will be </p>

<pre><code>Col1 Col2 Col3
A      B    13
X      Y    14
</code></pre>

<p>Output should be two sentence, one for each row</p>

<p>Number of columns and rows can change every time.
Please can any one suggest any approach/free source lib for it</p>
","nlp"
"51952","How to replace words in a sentence with their POS tag generated with SpaCy efficiently?","2019-05-14 12:42:43","51964","1","2088","<machine-learning><nlp><spacy>","<p>How is it possible to replace words in a sentence with their respective PoS tags generated with SpaCy in an efficient way? </p>
","nlp"
"51934","Text classification with multiple documents per labeled datapoint","2019-05-14 09:39:34","","2","232","<classification><nlp><multi-instance-learning>","<p>I have a dataset with a label <code>TRUE</code> or <code>FALSE</code> for each person, but each person has multiple documents associated with them (emails and documents). </p>

<p>Right now I use a Random Forest Classifier on a bag of words consisting of all words in all documents put together per person (so that I have one row with all words and a label). It performs reasonably well, but I was wondering if you guys have some suggestions about how I can use the information of separate documents.</p>

<p>When I try to find information about this I only encounter multi-label classification, which is the exact opposite problem: multiple labels per document, instead of multiple documents per label. </p>
","nlp"
"52000","Bidirectional Encoder Representations from Transformers in R","2019-05-13 11:59:26","","4","2206","<nlp><r><text><programming>","<p>Can anybody suggest to me, where I can find example code for R language for BERT neural network for text mining tasks. All I can see are python examples, and I need R.</p>

<ul>
<li><p><a href=""https://github.com/google-research/bert"" rel=""nofollow noreferrer"">https://github.com/google-research/bert</a></p></li>
<li><p><a href=""https://github.com/facebookresearch/XNLI"" rel=""nofollow noreferrer"">https://github.com/facebookresearch/XNLI</a></p></li>
<li><p><a href=""https://github.com/tensorlayer/seq2seq-chatbot"" rel=""nofollow noreferrer"">https://github.com/tensorlayer/seq2seq-chatbot</a></p></li>
<li><p><a href=""https://www.depends-on-the-definition.com/named-entity-recognition-with-bert/"" rel=""nofollow noreferrer"">https://www.depends-on-the-definition.com/named-entity-recognition-with-bert/</a></p></li>
<li><p><a href=""https://github.com/huggingface/pytorch-pretrained-BERT"" rel=""nofollow noreferrer"">https://github.com/huggingface/pytorch-pretrained-BERT</a> и чат ботов</p></li>
<li><p><a href=""https://stanfordnlp.github.io/coqa/"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/coqa/</a></p></li>
<li><p><a href=""https://github.com/nyu-dl/bert-gen"" rel=""nofollow noreferrer"">https://github.com/nyu-dl/bert-gen</a></p></li>
</ul>
","nlp"
"51826","How to convert Hindi/Telugu/Marathi text to vector for text classification problem?","2019-05-12 12:01:31","51869","1","473","<machine-learning><deep-learning><nlp>","<p>sentence = 'अच्छा होगा अगर इसमें और गहने ना हों'</p>

<p>Which method will work for this task?
Is any pretrained model available to convert this text to vectors?
Please help by giving the code.</p>
","nlp"
"51785","what is the first input to the decoder in a transformer model?","2019-05-11 08:36:07","51798","12","15292","<nlp><sequence><bert><transformer>","<p><a href=""https://i.sstatic.net/SPNEP.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/SPNEP.png"" alt=""from https://jalammar.github.io/illustrated-transformer/""></a></p>

<p>The image is from url: <a href=""https://jalammar.github.io/illustrated-transformer/"" rel=""noreferrer"">Jay Alammar on transformers</a></p>

<p>K_encdec and V_encdec are calculated in a matrix multiplication with the encoder outputs and sent to the encoder-decoder attention layer of each decoder layer in the decoder.<br>
The previous output is the input to the decoder from step 2 but what is the input to the decoder in step 1?  Just the K_encdec and V_encdec or is it necessary to prompt the decoder by inputting the vectorized output (from the encoder) for the first word?  </p>
","nlp"
"51697","bert-as-service maximum sequence length","2019-05-09 18:06:36","51713","2","2379","<nlp><sequence><bert>","<p>I installed bert-as-service (<a href=""https://github.com/hanxiao/bert-as-service"" rel=""nofollow noreferrer"">bert-as-service github repo</a>) and tried encoding some sentences in Japanese on the <code>multi_cased_L-12_H-768_A-12</code> model.  It seems to work as I am getting vectors of length 768 per word but <code>np.shape()</code> shows this for each sentence:</p>

<pre><code>np.shape(vec_j[0]): (25, 768)
np.shape(vec_j[1]): (25, 768)
np.shape(vec_j[2]): (25, 768)
np.shape(vec_j[3]): (25, 768)
np.shape(vec_j[4]): (25, 768)
type: &lt;class 'numpy.ndarray'&gt;
</code></pre>

<p>My sentences are short so there is quite a bit of padding with 0's.  Still, I am unsure why this model seems to have a maximum sequence length of 25 rather than the 512 mentioned here:  <a href=""https://github.com/google-research/bert/blob/master/README.md"" rel=""nofollow noreferrer"">Bert documentation section on tokenization</a></p>

<blockquote>
  <p>""Truncate to the maximum sequence length. (You can use up to 512, but
  you probably want to use shorter if possible for memory and speed
  reasons.)""</p>
</blockquote>
","nlp"
"51689","Reading a visualization of word embeddings","2019-05-09 16:21:06","","0","261","<machine-learning><word2vec><word-embeddings><nlp>","<p>For my Masters Thesis, I created a Word2Vec model. I wanted to show this image to clarify the result. But how does the mapping works to display the words in this 2D space? </p>

<p>All words are represented by a vector of 300 dim. How are they mapped on this 2D image? What are the x &amp; y scales?</p>

<p><strong>Code:</strong></p>

<pre><code>documents = [_text.split() for _text in df_train.text]
w2v_model = gensim.models.word2vec.Word2Vec(size=W2V_SIZE,
                                            window=W2V_WINDOW,
                                            min_count=W2V_MIN_COUNT,
                                            workers=8)

w2v_model.build_vocab(documents)

words = w2v_model.wv.vocab.keys()
vocab_size = len(words)
print(""Vocab size"", vocab_size)

w2v_model.train(documents, total_examples=len(documents), 

epochs=W2V_EPOCH)
tokenizer = Tokenizer()
tokenizer.fit_on_texts(df_train.text)

vocab_size = len(tokenizer.word_index) + 1
print(""Total words"", vocab_size)

x_train = pad_sequences(tokenizer.texts_to_sequences(df_train.text), maxlen=SEQUENCE_LENGTH)
x_test = pad_sequences(tokenizer.texts_to_sequences(df_test.text), maxlen=SEQUENCE_LENGTH)

labels = df_train.target.unique().tolist()
labels.append(NEUTRAL)

encoder = LabelEncoder()
encoder.fit(df_train.target.tolist())

y_train = encoder.transform(df_train.target.tolist())
y_test = encoder.transform(df_test.target.tolist())

y_train = y_train.reshape(-1,1)
y_test = y_test.reshape(-1,1)

embedding_matrix = np.zeros((vocab_size, W2V_SIZE))
for word, i in tokenizer.word_index.items():
  if word in w2v_model.wv:
    embedding_matrix[i] = w2v_model.wv[word]
print(embedding_matrix.shape)
embedding_layer = Embedding(vocab_size, W2V_SIZE, weights=[embedding_matrix], input_length=SEQUENCE_LENGTH, trainable=False)
</code></pre>

<p><a href=""https://i.sstatic.net/zq35T.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/zq35T.png"" alt=""enter image description here""></a></p>
","nlp"
"51638","How do I group similar type of skills together?","2019-05-08 22:14:31","","1","288","<machine-learning><nlp>","<p>Suppose that I have a file which has thousands of skills starting from A-Z. Now, I would like to create a model that can group similar skills together (example neural network and SVM can group together). I know that I can use NLP for this problem, but I'm not sure about the algorithm that I can use to get the best result.</p>

<p>I'm new to NLP so any help is greatly appreciated.</p>
","nlp"
"51522","What is the use of [SEP] in paper BERT?","2019-05-07 04:53:18","","18","25003","<machine-learning><nlp><transformer><bert>","<p>I know that [CLS] means the start of a sentence and [SEP] makes BERT know the second sentence has begun. </p>

<p>However, I have a question.</p>

<p>If I have 2 sentences, which are s1 and s2, and our fine-tuning task is the same. </p>

<p>In one way, I add special tokens and the input looks like [CLS]+s1+[SEP] + s2 + [SEP]. </p>

<p>In another, I make the input look like [CLS] + s1 + s2 + [SEP]. </p>

<p>When I input them to BERT respectively, what is the difference between them? </p>

<ul>
<li><p>Will the s1 in second one integrate more information from s2 than the s1 in first one does?</p></li>
<li><p>Will the token embeddings change a lot between the 2 methods?</p></li>
</ul>

<p>Thanks for any help!</p>
","nlp"
"51404","Word2Vec how to choose the embedding size parameter","2019-05-04 23:29:32","51557","17","35488","<python><nlp><word2vec><gensim>","<p>I'm running word2vec over collection of documents. I understand that the size of the model is the number of dimensions of the vector space that the word is embedded into. And that different dimensions are somewhat related to different, independent ""concepts"" that a word could be grouped into. But beyond this I can't find any decent heuristics for how exactly to pick the number. There's some discussion here about the vocabulary size: <a href=""https://stackoverflow.com/questions/45444964/python-what-is-the-size-parameter-in-gensim-word2vec-model-class"">https://stackoverflow.com/questions/45444964/python-what-is-the-size-parameter-in-gensim-word2vec-model-class</a>  However, I suspect that vocabulary size is not most important, but more important is how many sample documents you have and how long they are. Surely each ""dimension"" should have sufficient examples to be learnt? </p>

<p>I have a collection of 200 000 documents, averaging about 20 pages in length each, covering a vocabulary of most of the English language. I'm using the word2vec embedding as a basis for finding distances between sentences and the documents. I'm using Gensim, if it matters. I'm using a size of 240. Is this reasonable? Are there any studies on what heuristics to use to choose the size parameter? Thanks.</p>
","nlp"
"51367","How to detect the begin word and end word in a sentence with machine learning","2019-05-04 08:19:54","","1","302","<machine-learning><deep-learning><nlp>","<p>I have some English text that has been tokenized. For example, the length of text token is about 20000 and each word (tokenize) has an index. Also, each index has a label, as the beginning word in a sentence labeled as 'b', as an end word (include symbol) in the sentence labeled as 'e', with other words labeled as 'o'. There is labeled training data and unlabeled test data.</p>

<p>My question: how do we use a machine learning method or deep learning model to solve the issue that predicts the label of words in the test data?
I mean how do we make the label and data for training just a word for training? I am confused.</p>
","nlp"
"51221","LSTM input and output for sentiment analysis","2019-05-01 12:14:18","","0","355","<neural-network><lstm><rnn><nlp><sentiment-analysis>","<p>I'm studying this LSTM network:
<a href=""https://www.kaggle.com/paoloripamonti/twitter-sentiment-analysis"" rel=""nofollow noreferrer"">https://www.kaggle.com/paoloripamonti/twitter-sentiment-analysis</a></p>

<pre><code>model = Sequential()
model.add(embedding_layer)
model.add(Dropout(0.5))
model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(1, activation='sigmoid'))

model.summary()
</code></pre>

<p>I understand the input part of the embedding layer. Each word get's a unique vector that represents the meaning of the word.</p>

<p>The drop out will deactivate neurons.
So the input for the LSTM model is the vocabulary where each token is represented by a vector.</p>

<p>I understand the workflow in an LSTM model. But what exactly does it do with the input? Give it a score by learning?
And what is the output of the lstm?</p>

<p>Here is the summary:</p>

<p><a href=""https://i.sstatic.net/VYHdK.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/VYHdK.png"" alt=""enter image description here""></a></p>
","nlp"
"51206","identifying the primary and secondary keywords in sentense","2019-05-01 02:24:20","","2","40","<machine-learning><nlp><text-mining><ipython>","<p>want to identify the primary and secondary keywords which are having an impact to sentences or comparison between 2 keywords.</p>
<p>below is the example</p>
<blockquote>
<p>India and China has highest population in the world.</p>
<p>travelers are interested in travel Paris instead of Hamburg</p>
</blockquote>
<p>in the above example
sentences 1 primary keyword is India and secondary keywords is China
sentences 2 primary keyword is Paris and secondary keywords is hamburg</p>
","nlp"
"51170","Text extraction from large pool of documents of different formats","2019-04-30 12:40:01","","5","132","<nlp><data-mining><pandas><text-mining>","<p>I have a collection of 6 million documents stored on a hard drive (around 500GB of data storage). Those documents contain text, tables, images and come in different formats: pdf, jpg, png, rar, vsd, xlsx, docx, and other Microsoft Office file types.</p>

<p>Some documents contain digital text (90%), other documents are scanned copies (10%). Some documents are in english (5%), some are in russian (5%) and rest of them is mix of english and russian (containing translations with similar meaning). Also some documents are packed into zip or rar archives.</p>

<p>What is the best approach to parse all this documents for text data? </p>

<p>What is the best approach to store extracted data?</p>

<p>How to make this data searchable? E.g. if I want to list top 30 documents that contain text similar to my search paragraph.</p>

<p>PS: I am interested to do this task in the shortest time possible, so I guess possible solution would involve big data techniques and distributed computing.</p>
","nlp"
"51141","How should I treat these non-English documents in the NLP task?","2019-04-29 21:43:59","51153","4","330","<machine-learning><nlp><language-model>","<p>So I have a small corpus of about 30k documents and about 50 documents in this corpus are in other languages (Persian, Chinese, Arabic, German, Spanish etc). I will be using this corpus for training a machine learning model.</p>

<p>Now the question is: How should these non-English documents be treated?</p>

<ol>
<li>Should I exclude them from the final corpus and from training the model?</li>
<li>or should I manually translate them (Requesting natives from each language to translate it for me) and include them in the final corpus?</li>
<li>or should I use Google translate/DeepL to translate these non-English documents into English and then include them in the final corpus?</li>
</ol>

<p>Each document in the corpus under question is not larger than 500 words each.</p>

<p>Any help or hint will be appreciated.</p>
","nlp"
"51106","Using Keras how and what do I need to export to use my classifier independently?","2019-04-29 10:51:39","","0","29","<python><classification><keras><multilabel-classification><nlp>","<p>I have a basic question that I can't seem to find an answer to.</p>

<p>I built and trained with good results (above 90% accuracy) a NLP Log classifier that takes in a UTF-8 payload and classifies it into 32 distinct categories but I am having a hard time writing a simple script that loads all the necessary info from my training and testing session (model.h5 and ?).</p>

<h2>This is the structure of my code.</h2>

<pre><code># load data logs and split it 80-20 for training and testing
vocab_size = 500
tokenizer = text.Tokenizer(num_words=vocab_size)
tokenize.fit_to_text(trainRawLogs)
x_train = tokenize.text_to_matrix(trainRawLogs)
x_test = tokenize.text_to_matrix(testRawLogs)

encoder = labelBinarizer()
encoder.fit(trainRawLogs)

#Model build is simple ReLu - Softmax

model.compile..

model.fit..

model.evaluate..
</code></pre>

<h2>Now here is my question.</h2>

<p>Out of all of this process what do I need to save to build a lightweight classifier? The model? The model and the labels? Anything else? I tried loading the model</p>
","nlp"
"51096","Convert natural language text to structured data","2019-04-29 09:10:16","","4","1003","<nlp><structured-data>","<h3>Convert natural language text to structured data.</h3>

<p>I'm developing a bot to help user assist in identifying Apparels.
The problem is to convert natural language text to structured data (list of apparels) and query the store's inventory to find the closest match for each item. </p>

<p>For example, consider the following user input to the bot. </p>

<pre><code>""I would like to order regular fit blue jeans with hip size 32 inches""
</code></pre>

<p>and the desired  output will be the following</p>

<pre><code>[
  {
    ""quantity"": 1,
    ""size"": ""32 inches"",
    ""category"": ""jeans"",
    ""attributes"":[
      {""colour"": ""blue""},
      {""fit"": ""regular fit""}
    ]
  }
]
</code></pre>

<p>I've attempted to solve the problem by splitting it into two parts. </p>

<p><strong>Part 1: Named entity recognition using conditional random fields (CRF).</strong> - I've used approached discussed <a href=""https://rajmak.wordpress.com/2016/02/19/structuring-text-using-conditional-random-field-crf-tagging-recipe-ingredient-phrases/"" rel=""nofollow noreferrer"">here</a> to tag individual tokens and I'm able to extract entities like <code>apparel type</code>, <code>apparel size</code> and <code>attributes</code> etc.</p>

<p>example output (representation) of tagger :</p>

<pre><code>I would like to order regular fit   blue   jeans     with hip size   32 inches
|                     |   |         |   |  |   |   |     |           |   |       |
+----------------------   +---------+   +--+   +---+     +-----------+   +-------+
OTHERS                    FIT           COLOR  CATEGORY  ATTR_TYPE       SIZE
</code></pre>

<p><strong>Part 2: Rule-based grammar</strong>
    - 
Assuming a query from a user will always be a combination of defined entities (like type, color, fit, etc), 
I've written rules to capture the sequence of tags and their respective tokens and transform them into the required format. </p>

<p>Following are a few examples of commonly occurring sequences:</p>

<pre><code>OTHERS ~ FIT ~ COLOR ~ CATEGORY ~ ATTR_TYPE ~ SIZE ~ OTHERS
OTHERS ~ CATEGORY ~ OTHERS ~ COLOR ~ FIT ~ OTHERS ~ SIZE
OTHERS ~ COLOR ~ CATEGORY ~ FIT ~ SIZE ~ OTHER ~ ATT_STYLE    
QTY ~ COLOR ~ ATT_MATERIAL ~ CATEGORY ~ OTHERS
COLOR ~ FIT ~ ATT_STYLE ~ CATEGORY
</code></pre>

<p>I've made some assumptions and mined frequently occurring sequences to write these rules.</p>

<p>The second part is not scalable and becomes a bottleneck. I cannot keep adding rules for capturing additional data points or handling new patterns that the system has not seen.</p>

<p>I'm looking for a generalized solution/data pipeline that can extract entities (relational) from natural language and convert them to structured data. </p>

<p>I would appreciate any ideas.</p>

<p>More examples to help understand the problem better:</p>

<p>Example 1:</p>

<pre><code>""find jeans with black color, slim fit and size 28""

find a   jeans     with     black color,     slim fit    and       size 28
|    |   |   |     |  |     |         |      |      |    | |       |     | 
+----+   +---+     +--+     +---------+      +------+    +-+       +-----+
OTHERS   CATEGORY  OTHERS   COLOR            FIT         OTHERS    SIZE

[
  {
    ""quantity"": 1,
    ""size"": ""28"",
    ""category"": ""jeans"",
    ""attributes"":[
      {""colour"": ""black""},
      {""fit"": ""slim fit""}
    ]
  }
]
</code></pre>

<p>Example 2:</p>

<pre><code>""I would like to find a white shirt, slim fit, XL with long sleeve, one maroon silk tie, and a black color regular fit flat front trousers""

I would like to find a  white   shirt,     slim fit, XL    with       long sleeve. 
|                    |  |   |   |   |      |      |  ||    |  |       |         |  
+--------------------+  +---+   +---+      +------+  ++    +--+       +---------+  
OTHERS                  COLOR   CATEGORY   FIT       SIZE  OTHER      ATT_STYLE    

one   maroon    silk         tie       and a 
| |   |    |    |  |         | |       |   | 
+-+   +----+    +--+         +-+       +---+ 
QTY   COLOR     ATT_MATERIAL CATEGORY  OTHERS

black color  regular fit   flat front   trousers
|         |  |         |   |        |   |      |
+---------+  +---------+   +--------+   +------+
COLOR        FIT           ATT_STYLE    CATEGORY

[
  {
    ""quantity"": 1,
    ""size"": ""XL"",
    ""category"": ""shirt"",
    ""attributes"":[
      {""colour"": ""white""},
      {""fit"": ""slim fit""},
      {""sleeve_length"": ""long sleeve""}
    ]
  },
  {
    ""quantity"": 1,
    ""size"": ""STANDARD"",
    ""category"": ""tie"",
    ""attributes"": [
      {""color"": ""maroon""},
      {""material"": ""silk""},
    ]
  },
  {
    ""quantity"": 1,
    ""size"": null,
    ""category"": ""trousers"",
    ""attributes"":[
      {""fit"": ""regular fit""},
      {""style"": ""flat front""},
      {""color"": ""black""}
    ]
  }
]
</code></pre>

<p><strong>Edit 1:</strong>
I'm trying to parse the sequence of entities and transform it into structured data using rules. The current rule-based system has limitations like maintaining rules require skilled experts, they need to be manually crafted and enhanced all the time. Is there a way to overcome these limitations using ML? Replace the rule-based parser with an ML-based parser? </p>
","nlp"
"51065","What is the positional encoding in the transformer model?","2019-04-28 14:43:17","90038","106","115861","<nlp><encoding><attention-mechanism><transformer>","<p>I'm trying to read and understand the paper <a href=""https://arxiv.org/abs/1706.03762"" rel=""noreferrer"">Attention is all you need</a> and in it, there is a picture:</p>

<p><a href=""https://i.sstatic.net/BpxYv.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/BpxYv.png"" alt=""enter image description here""></a></p>

<p>I don't know what <strong>positional encoding</strong> is. by listening to some youtube videos I've found out that it is an embedding having both meaning and position of a word in it and has something to do with <span class=""math-container"">$sin(x)$</span> or <span class=""math-container"">$cos(x)$</span></p>

<p>but I couldn't understand what exactly it is and how exactly it is doing that. so I'm here for some help. thanks in advance.</p>
","nlp"
"51034","Duplicate QUORA question detection:Kaggle Dataset","2019-04-27 18:40:02","","0","75","<deep-learning><nlp><lstm><rnn>","<p>I have tried to use 2 BILSTMs along with the attention layer but the validation accuracy is not improving at all. Could anyone suggest an alternative to increase the accuracy?</p>
<h3>Layer structuring:</h3>
<pre><code>&gt;Preprocessing
&gt;GLOVE
&gt;BILSTM
&gt;ATTENTION
&gt;BILSTM
</code></pre>
<p>Current Validation Accuracy for 10 epochs is in the range of 0.62 to 0.63</p>
<h3>Code:</h3>
<pre><code>## Embeddings matrix is the set of vectors representing our words 
from keras.initializers import Constant

# Length of the maximum sentence in the df
sequence_length = 470
DROPOUT=0.1


question1 = Input(shape=(max_len,))
question2 = Input(shape=(max_len,))
q1 = Embedding(len(word_index)+1, 
                 embedding_dim, 
                 embeddings_initializer=Constant(embedding_matrix), 
                 input_length=max_len, 
                 trainable=False)(question1)
q1 = Bidirectional(LSTM(300, return_sequences=True), merge_mode=&quot;sum&quot;)(q1)
q2 = Embedding(len(word_index)+1, 
                 embedding_dim, 
                 embeddings_initializer=Constant(embedding_matrix), 
                 input_length=max_len, 
                 trainable=False)(question2)
q2 = Bidirectional(LSTM(300, return_sequences=True), merge_mode=&quot;sum&quot;)(q2)
## Applying Attention
model = dot([q1,q2], [1,1])
model = Flatten()(model)
model = Dense((max_len*300))(model)
model = Reshape((max_len, 300))(model)
model = add([q1,model])
##Applying BiLSTM
model = Bidirectional(LSTM(300, return_sequences=True), merge_mode=&quot;sum&quot;)(model)
model = Flatten()(model)
model = Dense(200, activation='relu')(model)
model = Dropout(DROPOUT)(model)
model = BatchNormalization()(model)
model = Dense(200, activation='relu')(model)
model = Dropout(DROPOUT)(model)
model = BatchNormalization()(model)
model = Dense(200, activation='relu')(model)
model = Dropout(DROPOUT)(model)
model = BatchNormalization()(model)
model = Dense(200, activation='relu')(model)
model = Dropout(DROPOUT)(model)
model = BatchNormalization()(model)

is_duplicate = Dense(1, activation='sigmoid')(model)

final_model = Model(inputs=[question1,question2], outputs=is_duplicate)
final_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

history = final_model.fit([x1_train, x2_train],
                    X_train.is_duplicate.values,
                    epochs=10,
                    verbose=2,
                    validation_data=([x1_test, x2_test], X_test.is_duplicate.values),
                    batch_size=64,
                    callbacks=callbacks)
</code></pre>
","nlp"
"51007","nltk.corpus for data science related words?","2019-04-27 03:05:01","","0","426","<python><nlp><nltk>","<p>from job description I scraped from the internet, I've went through all nlp processes and I've got to place where I found:</p>

<pre><code>freq = nltk.FreqDist(lemmatized_list)
most_freq_words = freq.most_common(100)
</code></pre>

<p>which outputs:</p>

<pre><code>[('data', 179),
 ('experience', 86),
 ('work', 78),
 ('business', 71),
 ('team', 59),
 ('learn', 56),
 ('model', 49),
 ('skills', 47),
 ('science', 41),
 ('use', 41),
 ('build', 39),
 ('machine', 37),
 ('ability', 36),.....
</code></pre>

<p>and so on. My problem is I do not want to consider words like ""experience"", ""work"", and only consider keywords related to data science. I'm guessing there is a corpus for data science terms which I can use like how I use stop word corpus to not select them. Let me know if there is a way, Thanks!</p>
","nlp"
"50843","Need help with entity tagging","2019-04-24 10:55:38","50846","3","439","<machine-learning><text-mining><nlp><named-entity-recognition>","<p>I need to design a system which can identify <code>movie</code> and <code>production company</code> names in a sentence.</p>

<p>The approach that comes to my mind is to train a <code>NER</code> Named-entity recognition system on labeled data so that it identifies the corresponding entities. But what about new entities (movie or production company name) which trained system hasn't seen, how can we tag them. Re-training the model every time with new released movies won't be feasible.</p>

<blockquote>
  <p>Labeled data: Sentences with the position of words that corresponds to movie or production company name</p>
</blockquote>

<p>I am a beginner in NLP any help would be appreciated</p>
","nlp"
"50838","How can I categoriese / classify a cluster of words?","2019-04-24 09:10:38","50839","-1","58","<classification><clustering><text-mining><nlp><lda>","<p>I am just wondering if it is possible to classify word clusters?</p>

<p>For example if I provide you an array of words <code>[bird,chicken,dock,park,apple,grapes,furits,juice]</code> 
what I need is to convert this array to something like this (or nearest possible) </p>

<pre><code>[
   ""Birds""=&gt;[bird,chicken,dock,park],
   ""Fruits""=&gt;[apple,grapes,furits,juice,park]
]
</code></pre>

<p>Any direction to how I can achieve this, please?</p>
","nlp"
"50820","Approaching a multi-class classification problem but without labels","2019-04-24 05:40:33","","3","337","<machine-learning><deep-learning><nlp><machine-learning-model>","<p>I am working on a business problem where I have a movie description dataset. In this dataset I've columns as - Movie title, Movie plot summary, Date of Release. Now based on this information and using machine learning I want to predict which category the movie falls into. For example The Conjuring should fall into Horror and Thriller i.e a multiclass classification problem. Now the problem is I don't have a label column besides the movie description and other info. Now I want my model to predict which categories a movie(unseen to model) should fall into. I have decided 5 labels that I want to consider - Horror, Thriller, Comedy, Romantic and Emotional. So, I want the dataset to look like this -</p>

<hr>

<p>Conjuring| Description | Title | Horror,Thriller</p>

<hr>

<p>The notebook| Description| Title | Romantic,Emotional</p>

<hr>

<p>I believe if I want to proceed this problem as a classification problem then I have to think of some way to create labels to existing dataset by some script and logic. If not supervised then maybe if I can do clustering first and then based on where the data point lies I can do classification later on.  </p>

<p><strong>What I have tried ?</strong></p>

<p>Once I decided what my 5 labels should be, I made 50 synonyms for each and then iterated the description of the movies and based on the number of occurrence of words I made frequency and based on majority of the occurrence I decided which category a movie should fall into. Very bad results from this approach.</p>

<p>I used K means clusters from the data and tried to extract information from the clusters. Could not get very meaningful information though.</p>

<p>To be very honest I am pretty clueless and just want a direction how to approach this problem.</p>
","nlp"
"49731","Efficient way to replace incorrect words in Series of strings","2019-04-22 19:30:29","","1","411","<python><nlp>","<p>I'm working with text data, that is handwritten, so it has lots of ortographic errors. I'm currently working with <code>pyspellchecker</code> to clean the data and I'm using the <code>correct()</code> method to find the most likely word when a word doesn't exist. My approach was to create a dictionary with all poorly written words as keys and the most likely word as value:</p>

<pre><code>dic={}
for i in df.text:
    misspelled = spell.unknown(i.split())
    for word in misspelled:
        dic[word]=spell.correction(word)
</code></pre>

<p>Even though this is working, it is doing so very slowly. Thus, I wanted to know if there's a faster option to implement this. Do you have any ideas?</p>
","nlp"
"49712","Lemmatization Vs Stemming","2019-04-22 10:41:28","49713","7","3589","<nlp><stanford-nlp>","<p>I have been reading about both these techniques to find the root of the word, but how do we prefer one to the other?</p>

<p>Is ""Lemmatization"" always better than ""Stemming""?</p>
","nlp"
"49621","Having trouble figuring out how loss was calculated for SQuAD task in BERT paper","2019-04-20 01:57:53","49637","3","87","<machine-learning><nlp><loss-function>","<p>The BERT Paper</p>

<p><a href=""https://arxiv.org/pdf/1810.04805.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1810.04805.pdf</a></p>

<p>Section 4.2 covers the SQuAD training.</p>

<p>So from my understanding, there are two extra parameters trained, they are two vectors with the same dimension as the hidden size, so the same dimensions as the contextualized embeddings in BERT. They are S (for start) and E (for End).</p>

<p>For each, a softmax is taken with S and each of the final contextualized embeddings to get a score for the correct Start position. And the same thing is done for E and the correct end position.</p>

<p>I get up to this part. But I am having trouble figuring out how the did the labeling and final loss calculations, which is described in this paragraph</p>

<p>""and the maximum scoring span is used as the prediction. The training objective is the loglikelihood of the correct start and end positions.""</p>

<p>What do they mean by ""maximum scoring span is used as the prediction""?</p>

<p>Furthermore, how does that play into ""The training objective is the loglikelihood of the correct start and end positions""?</p>

<p>From this Source:</p>

<p><a href=""https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/"" rel=""nofollow noreferrer"">https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/</a></p>

<p>It says the log-likelihood is only applied to the correct classes. So the we are only calculating the softmax for the correct positions only, Not any of the in correct positions.</p>

<p>If this interpretation is correct, then the loss will be</p>

<pre><code>Loss = -Log( Softmax(S*T(predictedStart) / Sum(S*Ti) ) -Log( Softmax(E*T(predictedEnd) / Sum(S*Ti) )
</code></pre>
","nlp"
"49610","sklearn & Meanshift for NLP only returns 1 cluster","2019-04-19 19:26:06","","0","604","<scikit-learn><nlp><clustering><unsupervised-learning><mean-shift>","<p>I am using <code>sklearn.clustering</code> to work with some text data and the MeanShift algorithm. I have:</p>

<ol>
<li>Done all standard NLP data prep like lemmatizing, removing stop words, etc.</li>
<li>Used the TfidfVectorizer to create my word vectors on 80k-plus records</li>
<li>The vectorizer gives me a sparse array so I converted it using a standard <code>.toarray()</code> command</li>
<li>I made a call to sklearn Meanshift and then accepted all of the default parameters. The call looks like <code>meanshift = MeanShift().fit(fitted_vector_data.toarray())</code> and results in the following output when I call the model: <code>MeanShift(bandwidth=None, bin_seeding=False, cluster_all=True, min_bin_freq=1, n_jobs=1, seeds=None)</code></li>
</ol>

<p>The problem is that no matter what data I pass in (whether it's 10 records or 10k records, it always just gives me 1 cluster when I should be getting hundreds of clusters. </p>

<p>This is my first time using MeanShift, so I'm guessing there is a problem with how I'm setting up my data and/or parameters? I should also point out, I have used other models like k-means and affinity propogation - with the same data prep - and those models gave multiple clusters.</p>
","nlp"
"49591","Scraping financial web data","2019-04-19 13:43:30","","5","372","<data-mining><nlp><web-scraping>","<p>I recently started working as a data scientist and I am starting a web scraping and NLP project using Python. The idea is to create a program that searches for public information on the company's clients. These information can come from various sources: annual reports, income statements, articles.... I will have to deal with two types of formats: HTML and PDFs. For now I will focus on retrieving the revenue of the company. After a month of research and tests, I realized a few things:
- NLP techniques are too slow to be used on annuals reports 
The first step of the project will be the following:</p>

<p>Search for the annual report and scrape the HTML code: so far I managed to get all the google results and I'm using Beautifulsoup to get the HTML code. However I can't quite get the revenue of the company because each website has its own HTML structure. I first decided to focus on extracting tables (the goal is to find the company's income statement) but I realized that HTML tables are often used for layout (even if it's a bad practice). I can't rely on css selectors as I need to keep it as generic as possible. How can I achieve it?</p>
","nlp"
"49517","Predict a 100 dimensional array","2019-04-18 07:03:30","","1","31","<deep-learning><keras><nlp>","<p>I have several sentences that I transformed into vectors. With these vectors I would like to predict another vector (which represents a vector of a sentence (the answer)). </p>

<p>Can you tell me if this kind of neural network fits this problem? (In term of loss function, activation function (I didn't use an activation function for the output), and metrics.</p>

<pre><code>model = Sequential()
model.add(Dense(1024, input_shape=(1200,))) 
model.add(Activation('relu'))
model.add(Dense(100))
model.compile(loss='mse', optimizer='rmsprop', metrics=['mae'])
</code></pre>

<p>Input: Several sentences, converted into 100 dimensions array (so, 12 sentences)</p>

<p>Output: 100 dimensions array, that should correspond to the kind of answer we would have from the 12 sentences. </p>

<p>Thanks!</p>
","nlp"
"49474","Chinese Word Segmentation","2019-04-17 11:43:51","","1","33","<nlp>","<p>Following this paper <a href=""https://www.aclweb.org/anthology/D17-1079"" rel=""nofollow noreferrer"">https://www.aclweb.org/anthology/D17-1079</a>, I'm stuck with Paragraph 2 ""Baseline Segmentation Model"". In this section they basically explain how they feed the chinese word embeddings into the bi-LSTM model. In particular they use this formula for <span class=""math-container"">$e^f_i$</span> (forward word representation):</p>

<p><span class=""math-container"">$$ e^f_i = concat_1(e_{w_i}, e_{w_{i−1}w_i}),
    = tanh(W_1[e_{w_i}; e_{w_{i−1}w_i}]) $$</span></p>

<p>where <span class=""math-container"">$w_i$</span> is the <span class=""math-container"">$i$</span>-th character in a sentence <span class=""math-container"">$\{w_1,w_2,\dots,w_N\}$</span>, where <span class=""math-container"">$N$</span> is the sentence length. We have a corresponding embedding <span class=""math-container"">$e_{w_i}$</span> and <span class=""math-container"">$e_{w_{i−1}w_i}$</span> for each character unigram <span class=""math-container"">$w_i$</span> and character bigram <span class=""math-container"">$w_{i−1}w_i$</span>.</p>

<p>I don't really get what they are doing here and how can I replicate it with Keras?</p>
","nlp"
"49460","BERT has a non deterministic behaviour","2019-04-17 07:57:15","","1","556","<neural-network><nlp><feature-extraction><bert>","<p>I am using the BERT implementation in <a href=""https://github.com/google-research/bert"" rel=""nofollow noreferrer"">https://github.com/google-research/bert</a> for feature extracting and I have noticed a weird behaviour which I was not expecting: if I execute the program twice on the same text, I get different results. I need to know if this is normal and why this happens in order to treat this fact in one or another way. Why is the reason for this? Aren't neural networks deterministic algorithms?</p>
","nlp"
"49313","How does BERT deal with catastrophic forgetting?","2019-04-15 07:26:43","","3","3228","<deep-learning><nlp><transfer-learning>","<p>In the <a href=""https://arxiv.org/abs/1801.06146"" rel=""nofollow noreferrer"">ULMFit paper</a> authors propose a strategy of gradual unfreezing in order to deal with catastrophic forgetting. That is, when the model starts be fine-tuned according to a downstream task, there is the danger of forgetting information on lower layers. Although <a href=""https://arxiv.org/abs/1810.04805"" rel=""nofollow noreferrer"">Google's BERT</a> is also a pre-trained language model, which makes use of fine-tuning for downstream tasks, authors don't mention this phenomenon. Why is that the case? Is BERT immune to it? Or does it deal with this in another way?</p>
","nlp"
"49306","NLP - Retrieval-based model","2019-04-15 06:02:12","","0","191","<nlp><chatbot>","<p>My goal is to predict the most appropriate answer from an utterance, in a group of 21 potential answers. (I'm not sure the ""question"" is called utterance though. )</p>

<p>Example:</p>

<p>Utterance: How are you today?
Answers: Answer1, 2, ..., 21.</p>

<p>I have a training file with this format:</p>

<p>Utterance:
Answers: Good answer, wrong answer1, wrong answer2,..., wrong answer20.</p>

<p><strong>My problem</strong></p>

<p>For the first time, we have to make a prediction from a group of possible answers, and, thus, this is a MCQ form. </p>

<p>Any ideas how I could start the problem?</p>

<p><strong>What I've done</strong></p>

<p>For the moment, the only thing I did was to choose the answers from the 21 possible answers which had the highest cosine similarity with the utterance. (So, unsupervised). It's not that bad (24% against 1/21 at random), but I'm sure there are ways to make something really better. </p>

<p><strong>What I don't want to do at first</strong></p>

<p>Use a generative model which predicts a full sentence. I want to choose the best candidate amongs the 21 answers, and use the training file which can allow us to do supervised learning. </p>
","nlp"
"49184","How can I find colors in a sentence?","2019-04-12 10:02:30","","1","285","<word-embeddings><word2vec><nlp><model-selection><nltk>","<p>Given a sentence ""I like blue jeans"", the output should be ""blue"". </p>

<p>I do not have any training data. I'll just be downloading a bunch of tweets related to a hashtag.</p>

<p>How do I build a model for this? I haven't learnt word2vec but could that work? Pre-trained embeddings? I have not worked on them before yet but I want to know if it would work beforehand.</p>

<p>I want to know what would be the best approach and any references to similar problems worked on before. Thanks.</p>
","nlp"
"49179","How to build a machine translation system for a new language","2019-04-12 08:45:17","","1","1085","<machine-learning><nlp><machine-translation>","<p>I am trying to figure out what are the options for building a natural language translation model for a language that is not yet supported by existing machine translations.
The project is to build a system for translating a very limited subset of a small east african language into English (only one-way needed).<br>
The language is to my knowledge not yet supported by any machine translation systems, but it is related to several other big African languages (putting sentences into Google Translate, the language is mostly auto detected as Kiswahili or Shona, and the English translation is sometimes usable).    </p>

<p>I know building a translator for a new language is by no means trivial, but the problem domain is very small and I think it should be doable. Are there any features of the big cloud providers that support this, ML frameworks, or vendors that build these models as a service?</p>

<p>If this is not the right stack exchange for this question, kindly guide me to a better place.</p>
","nlp"
"49078","Policy gradient/REINFORCE algorithm with RNN: why does this converge with SGM but not Adam?","2019-04-10 20:40:52","","2","378","<deep-learning><nlp><rnn><reinforcement-learning><policy-gradients>","<p>I am working on training RNN model on caption generation with REINFORCE algorithm. I adopt self-critic strategy (see paper <a href=""https://arxiv.org/pdf/1612.00563.pdf"" rel=""nofollow noreferrer"">Self-critical Sequence Training for Image Captioning</a>) to reduce the variance. I initialize the model with a pre-trained RNN model (a.k.a. warm start). This pre-trained model (trained with log-likelihood objective) got 0.6 F1 score in my task. </p>

<p>When I use adam optimizer to train this policy gradient objective, the performance of my model drops to 0 after a few epochs. However, if I switch to gradientdescent optimizer and keep everything else the same, the performance looks reasonable and slightly better than the pre-trained model. Is there any idea why is that? </p>

<p>I use tensorflow to implement my model.</p>
","nlp"
"49048","Why did Logistic regression perform better than svm?","2019-04-10 13:31:38","","1","900","<nlp><random-forest><svm><logistic-regression><naive-bayes-classifier>","<p>I have a data set of movies and their subtitles.My task is to classify them based on their ratings-[R,NR,PG,PG-13,G].
I have tried different ML algorithms and found that Logistic regression out performed them all, but I am unable to figure out why.My data had more features than observations.</p>

<p>SVM- should perform well on high dimensional data and will perform well even the there is class imbalance, but failed to show great results.
Naive Bayes-I think Naive Bayes did not perform well because of class imbalance.
Random forest-decent performance.but did not out perform logistic regression.</p>

<p>I am looking for an explanation as to why did one perform better than the other.</p>

<p>Note:The data set is sparse and it has more features/parameters than observations/examples.</p>
","nlp"
"49047","Will a Count vectorizer ever perform (slightly) better than tf-idf?","2019-04-10 12:49:24","49050","0","2087","<classification><nlp><tfidf>","<p>For the task of binary classification, I have a small data-set of a total 1000 texts (~590 positive and ~401 negative instances). With a training set of 800 and test set of 200, I get a (slightly) better accuracy for count vectorizer compared to the tf-idf. </p>

<p>Additionally, count vectorizer picks out the relevant ""words"" training the model, while tf-idf does not pick those relevant words out. Even the confusion matrix for count vectorizer shows marginally better numbers compared to tf-idf. </p>

<pre><code>TFIDF confusion matrix
[[ 80  11]
 [  6 103]]
BoW confusion matrix
[[ 81  10]
 [  6 103]] 
</code></pre>

<p>I haven't tried cross-validation yet though it came to me as shock that count vectorizer performed a bit better than tfidf. Is it because my data set is too small or if I have't used any dimensionality reduction to reduce the number of words taken into account by both the classifiers. What is it that I am doing wrong?</p>

<p>I am sorry, if it is an immature question, but I am really new to ML.</p>
","nlp"
"49033","What does localist one-hot vector mean in cs224n NLP course?","2019-04-10 10:33:43","49057","1","162","<nlp>","<p>Chris <a href=""https://youtu.be/ERibwqs9p38?t=651"" rel=""nofollow noreferrer"">said</a> one-hot is a ""localist"" representation.</p>

<p>what does ""localist"" mean here? </p>

<p>I've searched on recommended text, didn't find explanation.</p>

<p>any clue?</p>
","nlp"
"48975","How to cluster text-based software requirements","2019-04-09 16:40:14","48978","1","78","<neural-network><clustering><unsupervised-learning><nlp>","<p>I'm beginner in deep learning and I'd like to cluster text-based software requirements by themes (words similarities/frequency of words) using neural networks. Is there any example/tutorial/github code of unsupervised neural network that groups texts based on themes and words similarities?</p>

<p>Thank you very much for your answers! </p>
","nlp"
"48969","Why Heaps' Law Equation looks so different in this NLP course?","2019-04-09 15:00:00","48983","1","237","<deep-learning><nlp><language-model>","<p>I'm actually not sure if this question is allowed on this community since it's more of a linguistics question than it is a data science question. I've searched extensively on the Web and have failed to find an answer and also the Linguistics Beta Stack Exchange community also doesn't seem to be able to help. If it's not allowed here please close it.</p>

<p><a href=""https://en.wikipedia.org/wiki/Heaps%27_law"" rel=""nofollow noreferrer"">Heaps' Law</a> basically is an empirical function that says <em><strong>the number of distinct words you'll find in a document grows as a function to the length of the document</strong></em>. The equation given in the Wikipedia link is</p>

<p><span class=""math-container"">$$V_R(n) = Kn^\beta$$</span></p>

<p>where <span class=""math-container"">$V_R$</span> is the number of distinct words in a document of size <span class=""math-container"">$n$</span>, and <span class=""math-container"">$K$</span> and <span class=""math-container"">$\beta$</span> are free parameters that are chosen empirically (usually <span class=""math-container"">$0 \le K \le 100$</span> and <span class=""math-container"">$0.4 \le \beta \le 0.6$</span>).</p>

<p>I'm currently following a course on Youtube called <em>Deep Learning for NLP</em> by Oxford University and DeepMind. There is a slide in a lecture that demonstrates Heaps' Law in a rather different way:</p>

<p><a href=""https://i.sstatic.net/QnAvH.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/QnAvH.png"" alt=""enter image description here""></a></p>

<p>The equation given with the logarithms apparently is also Heaps' Law. The fastest growing curve is a corpus for Twitter data and the slowest is for the Wall Street Journal. Tweets usually have less structure and more spelling errors, etc. compared to the WSJ which would explain the faster-growing curve.</p>

<p>The main question that I had is how Heaps' Law seems to have taken on the form that the author has given? It's a bit of a reach but the author didn't specify what any of these parameters are (i.e. <span class=""math-container"">$C$</span>, <span class=""math-container"">$\alpha$</span>, <span class=""math-container"">$r(w)$</span>, <span class=""math-container"">$b$</span>) and I was wondering if anybody might be familiar with Heaps' Law to give me some advise on how to solve my question.</p>
","nlp"
"48966","Story Tag Prediction - Optional Labels","2019-04-09 13:46:47","48981","1","36","<nlp>","<p><a href=""https://i.sstatic.net/fQjCq.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/fQjCq.png"" alt=""enter image description here""></a></p>

<p>I'm currently working on a prediction for fiction. I have a database with fiction, which are each described with different story tags. My idea is to use a neural network that can tell you by processing a new story which tags are relevant.
The problem is, that the original data wasn't generated but added by users. A story in the woods could be tagged with trees, nature etc. Another story that also takes place in the woods might not be tagged with nature, even though the tag applies. This might confuse the neural network. Is there a way to prevent this form happening?</p>

<p>Thank you!</p>
","nlp"
"48832","How does GlobalMaxPooling work on the output of Conv1D?","2019-04-07 18:57:42","48833","0","285","<keras><nlp><cnn><text-filter>","<p>In the field of text classification, it is common to use Conv1D filters running over word embeddings and then getting a single value on the output for each filter using GlobalMaxPooling1D. </p>

<p>As I understand the process, the convolutional filter is a matrix of the same size as the <span class=""math-container"">$$\text{size of filter matrix} = \text{embedding dim}\cdot\text{width of the filter}$$</span> The filter matrix is then applied to the input embeddings (multiplied element by element) which produces a matrix of the same size for each filter position. Not a single number. </p>

<p>So how does the global max pooling get a single number on the output? Does it simply take a maximum over all the values in all the output matrices, or is there any other processing?</p>

<p>Please correct me if I'm wrong.</p>
","nlp"
"48589","Training an acoustic model for a speech-to-text engine","2019-04-04 09:25:50","48604","0","615","<neural-network><nlp><speech-to-text>","<p>What are the steps for training an acoustic model? The format of the data (the audio) includes its length and other characteristics. If anyone could provide a simple example of how to train an acoustic model, it would be greatly appreciated.</p>
","nlp"
"48575","Is there any NLP library or package which can help in adding comma, punctuation, newlines appropriately to text?","2019-04-04 06:27:53","","3","2493","<nlp><preprocessing>","<p>I have a movie transcript without commas, punctuation, or newlines. Is there any NLP technique that can help to implement this?</p>
","nlp"
"48457","How to find possible subjects for given verb in everyday object domain","2019-04-02 16:25:37","","2","624","<nlp><nltk>","<p>I am asking for tools (possibly in NLTK) or papers that talk about the following:</p>

<p>e.g. Input: Vase(Subject1) put(verb)</p>

<p>Ans I am looking for: flower, water</p>

<p>Is there a tool that can output subjects (objects) that can be associated to this verb? (I was going through VerbNet but didn't find anything)</p>
","nlp"
"48446","Sentence similarity using Doc2vec","2019-04-02 14:06:00","48528","2","2879","<machine-learning><nlp><gensim>","<p>I have a list of 50k sentences  such as : 'bone is making noise', 'nose is leaking' ,'eyelid is down' etc.. </p>

<p>I'm trying to use Doc2Vec to find the most similar sentence from the 50k given a new sentence. </p>

<pre><code>tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data)]

max_epochs = 100
vec_size = 20
alpha = 0.025

model = Doc2Vec(size=vec_size,
                alpha=alpha, 
                min_alpha=0.025,
                min_count=1,
                dm =0)

model.build_vocab(tagged_data)

for epoch in range(max_epochs):
    print('iteration {0}'.format(epoch))
    model.train(tagged_data,
                total_examples=model.corpus_count,
                epochs=model.iter)
    # decrease the learning rate
    model.alpha -= 0.0002
    # fix the learning rate, no decay
    model.min_alpha = model.alpha

test_data = word_tokenize(""The nose is leaking blood after head injury"".lower())
v1 = model.infer_vector(test_data)
#print(""V1_infer"", v1)

similar_doc = model.docvecs.most_similar(positive=[model.infer_vector(test_data)],topn=3)

for i in range(0,len(similar_doc)):
    print(tagged_data[int(similar_doc[i][0])],similar_doc[i][1])
</code></pre>

<p>Such that for the sentence ""The nose is leaking blood after head injury"" i would like to get the sentence with the highest similarity score ( i guess that it will bring sentences with words like leak or even synonyms like dripping?) . But the sentence i get back are unrelated and change each iteration of model.infer_vector(test_data)</p>

<p>Any idea about what is wrong? </p>
","nlp"
"48439","robots.txt communication in webmining","2019-04-02 13:03:50","","1","31","<data-mining><nlp>","<p>I hope this is the right subforum as I did not really find a suiting one.
I'm mining data from a website that does specifically exclude some crawlers from it's site in the robots.txt like this:</p>

<pre><code>User-agent: *
Disallow:
User-agent: Proxetrics2000
Disallow: /
</code></pre>

<p>While I'm pretty sure I'm not Proxetrics2000, how can I know which User-Agent to comply to?  Let's say I'm mining too frequently and the admin wants to restrict me from certain sites. He would want to add my name to</p>

<pre><code>User-Agents: dnns
Disallow: /
</code></pre>

<p>Is there a way to tell the website ""a name"" of my crawler? I'm looking forward to your answers. To me this seems an oddly obvious questions, however I did not find anything while searching for it.</p>

<p>Cheers,</p>

<p>Dennis</p>
","nlp"
"48438","What to use in setting up a Speech to Text engine in production?","2019-04-02 12:51:08","58241","0","46","<neural-network><deep-learning><nlp><speech-to-text>","<p>So i have the task to study the feasability of setting up a <strong>Speech-To-Text</strong> engine in a production environnement, and i've been researching on this topic, so I tried <strong><em>Google's Speech-To-Text API</em></strong> and there is a quota on how many transcribtions you can use daily => That's a <strong>no-no</strong>. Second alternative, <strong>CMU Sphinx</strong> ( Open Source pretrained model for that purpose ), just tested it, and the output is messy for the <strong>LiveSpeech</strong> feature ( Perhaps i need to adjust some parameters first ). Third option, using the available <strong>deep learning</strong> tools to create my own <strong>SpeechRecognition</strong> engine ( I have tons of recorded conversations of people speaking <strong>French</strong> ).</p>

<p><strong>Question :</strong> What should i use for my case? building an engine from scratch using neural networks seems complicated for a beginner in ML like me, using CMU Sphinx seems easy, but i have a feeling it's not. Any advice on what approach should i follow?</p>
","nlp"
"48426","How PV-DBOW works","2019-04-02 09:57:17","","1","558","<nlp><text-mining><word2vec><word-embeddings><gensim>","<p>The authors of the Paragraph Vector <a href=""https://cs.stanford.edu/~quocle/paragraph_vector.pdf"" rel=""nofollow noreferrer"">paper</a> describe PV-DBOW with:</p>

<blockquote>
  <p><strong>2.3. Paragraph Vector without word ordering: Distributed bag of words</strong></p>
  
  <p>The above method considers the concatenation of the paragraph vector
  with the word vectors to predict the next word in a text window.
  Another way is to ignore the context words in the input, but force the
  model to predict words randomly sampled from the paragraph in the
  output. In reality, what this means is that at each iteration of
  stochastic gradient descent, we sample a text window, then sample a
  random word from the text window and form a classification task given
  the Paragraph Vector.</p>
</blockquote>

<p>I have a couple of questions:</p>

<ol>
<li>Why do you need to sample a text window before sampling
a random word? To create a batch, why can't you just randomly sample from a list of the form <code>[(1, ""cat""), (1, ""sat""), ..., (1, ""mat""), (2, ""humpty""), (2, ""dumpty""), ... (2, ""wall""), ...]</code> where the first item in each tuple represents the paragraph?</li>
<li>If hierarchical softmax or negative sampling is used, is stochastic
gradient descent still used to update the weights in the network? Or are these optimization methods themselves?</li>
<li>To infer representations for new paragraphs, is the model only trained on words sampled from that paragraph?</li>
</ol>
","nlp"
"48353","How to filter out a portion of the text data with re.match or any other regex method with Python?","2019-04-01 13:55:55","","0","406","<python><nlp><regex>","<p>I have the following data : </p>

<pre><code>Node : WATER-7609-WR1 

IP Address : 172.20.150.81

Severity : 5 - Critical

ServerSerial : 50505223

SNSupportGroup : UC-WAN

Node Location : CO 160 WATER 8 FL RCV

Event Details : Unknown Not Link Down or Up trap on Serial Channel:  Unknown Severity: 4
NOTE:Configuration Item is not found in CMDB. Please update Incident with related Configuration Item
</code></pre>

<p>I am interested in all text after ""Event Details :"",
 I have used the the following function : </p>

<pre><code>def eventdetails(text): 
   match =re.search(r'Event Details :[^.]*',text) 
   return match

print(eventdetails(dfnetcool_inc[""Description""][3])) 

output: &lt;_sre.SRE_Match object; span=(159, 292), match='Event Details : Unknown Not Link Down or Up trap &gt; 
</code></pre>

<p>--> Am not getting all of the text after Event Details. Perhaps there is an EOL charecter - how do I get remainder of the text, request your help, TIA!</p>
","nlp"
"48289","Named Entity Recognition using context of the sentence","2019-03-31 11:53:14","","1","198","<machine-learning><nlp><data-science-model><nltk>","<p>I have a problem in which I want to know how can we extract or name the entity based on the context in which it is getting used in a sentence.</p>

<p>For example: 
If we have to extract date field which is used in the context of the date of birth only then how can we do that.</p>

<p>I know that we can use regular expression, spacy, NLTK to extract date field from a document. But I am unable to determine the approach to extract date based on the context in which it is getting used.</p>
","nlp"
"48274","Doc2vec most similar document to a query string","2019-03-30 19:42:03","","1","630","<word2vec><similarity><nlp><information-retrieval><similar-documents>","<p>I'm working on a project and I created doc2vec representation of different academics which include their patents and publications etc. For each publication and patent I have information such as title and abstract. Now, I want to do a search on all of the professors and find which professor is the most similar to a query string, such as ""deep learning"" or ""computer networking"". I have tried to use the infer_vector() to create a doc2vec representation of the query string using the already generated model and calculate the cosine similarity between the vectors. But I got terrible results. For example, when I search for ""computer networking"", it will give me the result of professor from History. 
Is there any recommendation of how to find most similar document to a query string?</p>
","nlp"
"48271","Implementation of NLP to categorize text into two categories","2019-03-30 18:25:50","","0","49","<machine-learning><python><nlp>","<p>I can't discuss my actual dataset, so please bear with me. </p>

<p>Let's say I have a dataset that contains a population of 20,000 examinations by a school principal. The principal is to record their examinations of student misconduct incidents. I want to implement NLP that assess the quality very broadly into two categories: ""good examination"" or ""bad examination"" of the full population.</p>

<p>An example of ""bad examinations"" are:""examination results - negative"" or ""exam results: negative"". Or ""check student's bags, checked the person. Nothing suspicious found. Or examination results negative"". Or ""Examination results positive"". Or ""ABC examined, results negative"". ABC could be an abbreviation of the person's name.</p>

<p>A good examination would be where there is a lot of context: ""Checked the student's bag and found textbooks, pencils, erasers, binders. No hidden compartments found. Interviewed the student and asked ""x"", ""y"", ""z"" questions. Her story corroborated other reports. Student presented herself in a clam manner. Examination results negative"". Other times it could be paragraphs and paragraphs, and at the end ""examination negative"" or ""examination positive""</p>

<p>There are also instances where all what could be listed is ""wrong person because of different birth date. Examination results negative"" and this is perfectly fine. Would this be a third category? </p>

<p>How would I go about implementing a reliable NLP solution? My first instinct is to take a random sample, classify it manually, and then apply it to the rest of the 20,000 records?</p>
","nlp"
"48192","Implementing back translation as a data augmentation for text classification","2019-03-29 07:25:32","","2","431","<deep-learning><nlp><text><data-augmentation><machine-translation>","<p>Since back translation English->other language -> English <a href=""https://arxiv.org/abs/1903.09244"" rel=""nofollow noreferrer"">seems like quite a useful data augmentation technique </a>, I wanted to experiment with it. E.g. it occurred to me that languages from very different language families (but very well supported for economic reasons such as Chinese, Russian, Spanish, Korean, Arabic...) could make for a diverse set of effects occurring in the back translation.</p>

<p>Commercial translation APIs would be a straightforward way of doing this, but without free API key or budget from my organization (would not qualify as academic) that's quickly quite expensive for a private thing.</p>

<p>Pretrained translation models would seem like an obvious alternative (I have a GPU for inference, but clearly that's not enough to train all the models from scratch), but I could e.g. not find those for any OpenNMT variant. Are there any recommendations from others that have used this approach?</p>
","nlp"
"48177","Why does all of NLP literature use Noise contrastive estimation loss for negative sampling instead of sampled softmax loss?","2019-03-28 20:44:47","48221","5","1519","<machine-learning><nlp><word2vec><word-embeddings>","<p>A sampled softmax function is like a regular softmax but randomly selects a given number of 'negative' samples.</p>
<p>This is difference than NCE Loss, which doesn't use a softmax at all, it uses a logistic binary classifier for the context/labels. In NLP, 'Negative Sampling' basically refers to the NCE-based approach.</p>
<p>More details <a href=""https://www.tensorflow.org/extras/candidate_sampling.pdf"" rel=""nofollow noreferrer"">here</a></p>
<p>I have tested both and they both give pretty much the same results. But in word embedding literature, they always use NCE loss, and never sampled softmax.</p>
<p>Is there any reason why this is? The sampled softmax seems like the more obvious solution to prevent applying a softmax to all the classes, so I imagine there must be some good reason for the NCE loss.</p>
","nlp"
"48165","Why is MLP working similar to RNN for text generation","2019-03-28 17:46:13","","1","94","<neural-network><rnn><nlp><language-model>","<p>I was trying to perform text generation using only a character level feed-forward neural network after having followed <a href=""https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/"" rel=""nofollow noreferrer"">this tutorial</a> which uses LSTM. I one-hot encoded the characters of my corpus which gave a vector of length 45. Then I concatenated every 20 characters and fed this 20*45 length vector as input to an MLP with the 21st character's one hot as the output.</p>

<p>Thus my X (input data) shape is -> (144304, 900)
and my Y (output data) shape is -> (144304, 45)</p>

<p>Here's the output from my code:</p>

<blockquote>
  <p>alice was beginning very about a grible thing was and bet she with a
  great come and fill feel at and beck to the darcht repeat allice
  waited it, put this was not an encir to the white knew the mock turtle
  with a sigh. ‘i only took the regular cours was be crosd it to fits
  some to see it was and getting she dodn as the endge of the evence,
  and went on to love that you were no alway not--ohe f whow to the
  gryphon is, who denight to goover and even stried to the dormouse, and
  repeated her question. ‘why did they live at the bottom of a well?’</p>
  
  <p>‘tabl the without once it it howling it the duchess to herself it as
  eng, longing one of the door and wasting for the homend of the taits.’</p>
  
  <p>‘sthing i cancus croquet with the queen to-day?’</p>
  
  <p>‘i should like it very much,’ said the dryphon, ‘you first form into a
  line along-the sea-shore--’</p>
  
  <p>‘the right!’ cried the queen nother frowing tone. any the this her for
  some thing is and at like the look of it at all,’ said the king:
  ‘however, it may kiss my hand if it likes.’</p>
  
  <p>‘i’d really feeer that in a few this for some whele wish to get thing
  to eager to think thcapered twice, and shook note bill herself in a
  lell as expectant, and thowedd all have come fuconfuse it the march
  hare: she thought it must be the right way of speaking to a mouse: she
  had never done such a thing before, but she remembered having seen in
  her brother’s latin grammar, ‘a mouse--of a mouse--to a mouse--a
  mouse--o mister once to hin fent on the words with her fane with ale
  three king the said, and diffich a vage and so mane alice this cime.</p>
</blockquote>

<p>My Question is why is MLP working similar to an RNN/LSTM. What's the advantage of using RNN/LSTM for such tasks over MLP?</p>
","nlp"
"48125","What are CRF (Conditional Random Field)","2019-03-28 06:20:43","48132","1","1242","<deep-learning><text-mining><nlp><named-entity-recognition><mathematics>","<p>Looking for language modeling, I have been finding CRF in a lot of places which is but looking online for the same isn't actually helping me a lot. I referred <a href=""http://blog.echen.me/2012/01/03/introduction-to-conditional-random-fields/"" rel=""nofollow noreferrer"">Edwin Chen's blog</a> and <a href=""https://medium.com/ml2vec/overview-of-conditional-random-fields-68a2a20fa541"" rel=""nofollow noreferrer"">Ravish Chawala's Medium article</a> but rather than solving my problem, raises more question. </p>

<p>Could you please refer some interesting blog posts, Github repo's or research papers that are going to further help me with this?</p>
","nlp"
"48100","How is WordPiece tokenization helpful to effectively deal with rare words problem in NLP?","2019-03-27 16:54:59","","7","9153","<nlp><word-embeddings><bert>","<p>I have seen that NLP models such as <a href=""https://github.com/google-research/bert"" rel=""noreferrer"">BERT</a> utilize WordPiece for tokenization. In WordPiece, we split the tokens like playing to play and ##ing. It is mentioned that it covers a wider spectrum of Out-Of-Vocabulary (OOV) words. Can someone please help me explain how WordPiece tokenization is actually done, and how it handles effectively helps to rare/OOV words?</p>
","nlp"
"48024","How to calculate which word fits the best given a context and possible words?","2019-03-26 12:41:01","48025","1","486","<machine-learning><nlp>","<p>I have this task for research purposes and searched a while for a framework or a paper which already took care of this problem.</p>

<p>Unfortunately I don't find anything which helps me with my problem.</p>

<p>I have a sentence like </p>

<pre><code>if the age of the applicant is **higher** than 18, then ...
</code></pre>

<p>and a list of words like</p>

<pre><code>higher, bigger, greater, wider ...
</code></pre>

<p>which are all a </p>

<p>Now I want to find find out, which of the given words approximately fits the best at the predefined position in the sentence.</p>

<p>The best fitting word in this example would be 'greater', but for example 'higher' would be also fine.
In my specific case, I want to show an error message if someone would write 'wider', because this doesn't make sense in this semantic context.</p>

<p>I hope that I explained my problem good enough.</p>
","nlp"
"48022","What simple machine learning algorithm to use in POS tagging","2019-03-26 11:59:55","","1","63","<machine-learning><python><nlp>","<p>I am new to NLP. I have a dataset that has already a parts of speech included, the only problem is what algorithm to use in order to train my dataset. Sample:</p>

<pre class=""lang-js prettyprint-override""><code>{""word1"": ""POS"",
""word2"": ""POS"",
""word3"": ""POS"",
""word4"": ""POS""}
</code></pre>

<p>Ambiguity is not a problem in this case.
If I input a word, it should tell what POS it is, but if the word is unknown, the POS will also be unknown. </p>
","nlp"
"48005","Which is better: GPT or RelGAN for text generation?","2019-03-26 08:39:57","48006","1","1566","<deep-learning><gan><nlp><text-generation><transformer>","<p>Based on my understanding, gpt or gpt-2 are using language model loss to train and generate text, which do not contains GAN.</p>

<p>So which is better: GPT vs RelGAN/LeakGAN/SeqGAN/TextGAN </p>

<p>I am so confused about this question.</p>
","nlp"
"47973","How to use a one-hot encoded nominal feature in a classifier in Scikit Learn?","2019-03-25 20:33:12","","7","323","<machine-learning><scikit-learn><nlp><pandas>","<p>I'm working on a genre classification problem on a songs dataset. Since genre is a nominal feature, I used sklearn's LabelBinarizer to get the one-hot encoding for this feature for every row in the dataset. I'm then left with a dataframe(df_train_num) with two columns, both numeric in nature and a Series object for which every row value is a numpy array - the one-hot encoding of the genre. I now want to fit a classifier on this data. What I did was:</p>

<pre><code>svm_classifier = LinearSVC()
svm_classifier.fit(df_train_num,df_train_genre)
</code></pre>

<p>This gives me:</p>

<blockquote>
  <p>ValueError: Unknown label type: 'unknown'</p>
</blockquote>

<p>What exactly is causing this error? Am I not allowed to use a Series object with a DataFrame object in the to fit a classifier? Although replacing <code>df_train_genre</code> with <code>df_train_genre.values</code> so as to pass the numpy array directly to the fit method also doesn't change anything. Same error.</p>

<p>Here is a view of the two pandas objects:</p>

<pre><code>df_train_num.head(5)

Unique_Word_Count   Sentiment Polarity
157277                  126   0.027766
90109                   114  -0.199545
106224                  16    0.000000
221087                  103  -0.058025
247082                  409  -0.170143

df_train_genre.head(5)

157277    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...
90109     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...
106224    [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...
221087    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...
247082    [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...
Name: Genre_Encoded, dtype: object
</code></pre>
","nlp"
"47941","Automatic labelling of text data based on predefined entities","2019-03-25 13:41:46","","1","1708","<nlp><data><labels>","<p>I'm new to NLP. 
I have a folder containing .txt files which are legal and specific documents. I want to label all these files based on four predefined labels. How can I do that automatically? </p>
","nlp"
"47817","How to prepare the data for text generation task","2019-03-23 00:43:54","","2","56","<deep-learning><nlp><language-model><text-generation><transformer>","<p>First, I'm not sure whether the model contains the encoder during training.</p>

<p>EOS means end-of-sentence. Encoder and decoder are part of transformer network.</p>

<p>If without-encoder, training time:</p>

<pre><code>target: [E, F, G, H, EOS]
decoder input: [0, E, F, G, H]
</code></pre>

<p>If without-encoder, testing time:</p>

<pre><code>decoder input: [0]
</code></pre>

<p>If with encoder, training time:</p>

<pre><code>encoder input: [A, B, C, B]
target: [E, F, G, H, EOS]
decoder input: [0, E, F, G, H]
</code></pre>

<p>If with-encoder, testing time:</p>

<pre><code>encoder input: [A, B, C, D]
decoder input: [0]
</code></pre>

<p>Am I exact right?</p>
","nlp"
"47773","The principle of LM deep model","2019-03-22 09:35:50","47780","1","65","<neural-network><deep-learning><nlp><language-model><transformer>","<p>Language model(LM) is the task of predicting the next word.</p>

<p>Does the deep model need the encoder? From the ptb code of tensor2tensor, I find the deep model do not contains the encoder.</p>

<p>Or both with-encoder and without-encoder can do the LM task?</p>
","nlp"
"47687","Paragraph Generator using BERT or GPT","2019-03-20 16:22:00","","2","224","<nlp><bert><gpt>","<p>I am trying to generate similar sentences, called paragraph generation. For example, what is the name of the eldest brother of ram? - For these paragraphs can be - who is the oldest brother of ram? , Who is the oldest blood brother of ram? , I want to know the eldest sibling of ram.  etc etc  </p>

<p>BERT and OpenAI GPT are one of the most powerful NLP systems, as per my knowledge, at least for now. However, I heard that BERT is based on MLM so it cant be used for sentence generation, but can be used for word prediction.  </p>

<p>Can OpenAI GPT be used for this purpose? If so, please give some pointer.<br>
If any other name is in mind which is the State of Art please suggest.</p>
","nlp"
"47617","Fuzzy name and nickname match","2019-03-19 13:36:15","47867","8","4141","<deep-learning><nlp>","<p>I have a dataset with the following structure:</p>

<p><code>full_name,nickname,match
 Christian Douglas,Chris,1,
 Jhon Stevens,Charlie,0,
 David Jr Simpson,Junior,1
 Anastasia Williams,Stacie,1
 Lara Williams,Ana,0
 John Williams,Willy,1</code></p>

<p>where each predictor row is a pair full name, nickname, and the target variable, match, which is 1 when the nickname corresponds to the person with that name and 0 otherwise. As you can see, the way the nickname is obtained from the full name doesn't follow a particular pattern.</p>

<p>I want to train an ML algorithm that, given the pair full name, nickname, predict the probability of match. </p>

<p>My baseline is just trying to see the number of carachters that match, and features like that. However, I am thinking about an NLP approach using deep learning. My question is wether there are neural network architectures that are specific of this problem.</p>
","nlp"
"47613","Using the Stanford Named Entity Tagger in R","2019-03-19 13:25:41","47615","0","742","<r><nlp><stanford-nlp>","<p>I am experimenting with the Stanford Named Entity Tagger  here <a href=""http://nlp.stanford.edu:8080/ner/process"" rel=""nofollow noreferrer"">http://nlp.stanford.edu:8080/ner/process</a> and I feel it would be useful in my research. Does anyone know of a example that I could follow so that I could do the analysis in R? Ideally I'd want to provide a string and get back a count (as a list) of the number of organisations, persons, etc recognised in the string. Thanks.</p>
","nlp"
"47556","How can I output tokens from MWE Tokenizer?","2019-03-18 19:36:02","47682","1","1291","<nlp><nltk><tokenization>","<p>How to output the tokens produced using <a href=""https://www.nltk.org/_modules/nltk/tokenize/mwe.html"" rel=""nofollow noreferrer"">MWE Tokenizer</a>?</p>

<p>NLTK's multi-word expression tokenizer (MWETokenizer) provides a method/function <code>add_mwe()</code> that allows the user to enter multiple word expressions prior to using the tokenizer on the text.</p>

<p>Currently, I have a file consisting of phrases / multi-word expression I want to use with the tokenizer.  My concern is that the manner in which I am presenting the phrases to the function correctly and so not resulting in the desired set of tokens to be used in tokenizing the incoming text.</p>

<p>So this leads me to ask if anyone knows how to output the token generated by <code>add_mwe()</code> so that I can verify that I am correctly passing the phrase to the function?</p>
","nlp"
"47545","Detect sensitive data from unstructured text documents","2019-03-18 17:10:39","","4","1529","<machine-learning><deep-learning><nlp><information-retrieval><automatic-summarization>","<p>I know this question is broad, but I need an advice to know if it's possible to achieve what I want to do.</p>

<p>The problem is that I have around 2500 documents with sensitive data being replaced by four dots. I do not have the original documents, so I wonder if there is a way to build a model that can detect sensitive data from any new documents (with sensitive data not being removed) using the previous documents? I want to apply machine learning or deep learning approaches. And what I know is that the original data set with annotated sensitive data should be used for training, which I can't obtain.</p>

<p>I am new at this field so any advice would be very appropriated</p>
","nlp"
"47429","ways to represent document by its keyword vectors","2019-03-16 17:42:54","","2","463","<nlp><word-embeddings><similar-documents><vector-space-models>","<p>I have documents, say for example, <em>D<sub>1</sub>, D<sub>2</sub>, D<sub>3</sub>... D<sub>m</sub></em>.</p>

<p>Every <em>D<sub>i</sub></em> has its individual components or keywords <em>k<sub>1</sub>, k<sub>2</sub>, k<sub>3</sub>,... k<sub>n</sub></em>, where <em>k<sub>i</sub></em> is an n-dimensional vector. The number of individual components varies between documents.</p>

<p>What are the ways to find how close <em>D<sub>i</sub></em> are? Or what is the best way to represent a document using its keyword vectors? Please note that I'm using a custom embedding here.</p>
","nlp"
"47424","Comparing English word pronunciation complexity","2019-03-16 16:00:21","","0","187","<nlp><text-mining><algorithms>","<p>I'm trying to figure out a way to compute a score for the pronunciation of a given english word, so I can use that score to compare the pronunciation complexity between english words. </p>

<p>Eg: Given words <code>apple</code> and <code>banana</code>, determine which one is harder to pronounce based on that score.</p>

<p>Please share your thoughts.
Thanks.</p>
","nlp"
"47320","""Context Resolution"" Task in NLP","2019-03-15 00:58:32","","1","57","<nlp>","<p>I'm looking for references to a standard(-ish) task/dataset in NLP that is close(-ish) to the following: we have a document with a list of references (sorry), for example, a scientific paper. For every reference, we have a sentence within the document and the corresponding text of another paper. The task is to find sentences/regions within the corresponding text, that ""support"" the sentence in the original text, given some metric. Basically, what anyone would do given cumbersome unexplained statement within some mind-boggling paper.</p>

<p>Apart from the main question, what are the possible/good/standard metrics one can use? Are there remarkable implementations/users of some in-house solutions/any other useful remarks?</p>
","nlp"
"47278","How to create a language translator from scratch?","2019-03-14 06:01:30","","2","1558","<deep-learning><nlp><rnn><machine-translation><nlg>","<p>I want to create a translator which can translate English, Korean and Tamil sentences into English sentence, I tried googletrans but is there any way to create something better than that using DL and NLP techniques?</p>
","nlp"
"47094","Why does the classic Neural Network perform better than LSTM in Sentiment Analysis","2019-03-11 14:49:33","","1","368","<keras><nlp>","<p>My goal is to predict the polarity of some reviews (negative, positive or neutral). I tried two different neural networks:</p>

<pre><code>    left_branch = Input((7000, ))
    left_branch_dense = Dense(512, activation = 'relu')(left_branch)

    right_branch = Input((14012, ))
    right_branch_dense = Dense(512, activation = 'relu')(right_branch)
    merged = Concatenate()([left_branch_dense, right_branch_dense])
    output_layer = Dense(3, activation = 'softmax')(merged)

    model = Model(inputs=[left_branch, right_branch], outputs=output_layer)
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    model.fit([np.array(review_matrix), np.array(X_train)], labels,epochs=2, verbose=1)
    model.save('model.merged') 

    #############################################################################


    #############################################################################

    #We will try to merge two different models in a different way: Accuracy: 70

    # Prepare the review column for embedding: 
    review_matrix_for_embedding = prepare_for_encoding(train_set[4].tolist(), 7000) # Shape: (1503,100)        

    second_matrix = np.array(pd.concat([onehot_category, aspect_matrix],axis=1))


    left_branch = Input(shape=(100,), dtype='int32')
    # input_dim: Size of maximum integer (7001 here); output dim: Size of embedded vector; 
    # input_length: Size of the array
    left_branch_embedding = Embedding(7000, 300, input_length=100)(left_branch)
    lstm_out = LSTM(256)(left_branch_embedding)
    lstm_out = Dropout(0.7)(lstm_out)
    lstm_out = Dense(128, activation='sigmoid')(lstm_out)

    right_branch = Input((7012, ))
    merged = Concatenate()([lstm_out, right_branch])
    output_layer = Dense(3, activation = 'softmax')(merged)

    model = Model(inputs=[left_branch, right_branch], outputs=output_layer)
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    model.fit([review_matrix_for_embedding, second_matrix], labels,epochs=5, verbose=1)
</code></pre>

<p>The first one does 80% accuracy while the second one does 70%, with embedding vectors and LSTM layer. How is it possible? Is there anything wrong in my architecture? </p>
","nlp"
"47080","Doc2vec '-' symbol occurrence","2019-03-11 11:31:27","47214","1","85","<nlp><word2vec><word-embeddings><nltk>","<p>Currently working on resume parser and struggled with embedding words with '-' symbols in them. Such as 'IT-manager'.</p>

<p>Vector representations of these words are incorrectly classified by doc2vec.</p>

<blockquote>
  <p>['it-manager']
  [('salary', 0.23328335583209991), ('responsibilites', 0.22327110171318054), ('schedule', 0.14869527518749237), ('position', 0.12755176424980164)]</p>
</blockquote>

<p>But when I remove '-' symbol, it is tokenized and classified right.</p>

<blockquote>
  <p>['it', 'manager']
  [('position', 0.9306046962738037), ('schedule', 0.6630333662033081), ('responsibilites', 0.6081600189208984), ('salary', 0.5934453010559082)]</p>
</blockquote>

<p>How do you work with such data properly? For this kind of task, I guess, it is better to exclude the symbol. But there may be a way to tell Doc2vec to treat these words like two different ones. Or perhaps tell the word_tokenizer to tokenize them in this fashion?</p>
","nlp"
"47071","Error while trying to merge two neural networks","2019-03-11 10:24:31","","1","220","<keras><nlp>","<p>I'm trying to merge two neural networks with Keras.</p>

<p>The code:</p>

<pre><code>left_branch = Sequential()
left_branch.add(Dense(512, input_shape=(7000,)))
left_branch.add(Activation('relu'))
right_branch = Sequential()
right_branch.add(Dense(512, input_shape=(14012,)))
right_branch.add(Activation('relu'))
merged = Concatenate([left_branch, right_branch])

final_model = Sequential()
final_model.add(merged) 
final_model.add(Dense(3, activation='softmax'))
final_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
final_model.fit([np.array(review_matrix), np.array(X_train)], labels,epochs=2, verbose=1)
final_model.save('model.merged')
</code></pre>

<p>I get the following error: AssertionError (assert len(inputs) == 1)</p>

<p>I guess the problem comes from the fact that final_model should not be sequential. However, I don't know how I can do  otherwise. In a lot of links, it works with sequential model (for example: <a href=""https://statcompute.wordpress.com/2017/01/08/an-example-of-merge-layer-in-keras/"" rel=""nofollow noreferrer"">https://statcompute.wordpress.com/2017/01/08/an-example-of-merge-layer-in-keras/</a>)</p>

<p>Thanks ! </p>
","nlp"
"47064","What is the difference between TextGAN and LM for text generation?","2019-03-11 07:39:36","","-1","429","<neural-network><deep-learning><nlp><gan><text-generation>","<p>I'm new to LeakGAN or SeqGAN or TextGAN. I know GAN is to generate text and let discriminator un-judge-able to real text and gen-text.</p>

<p>LM(language model) is the task of predicting the next word and can also be used to generate text.</p>
","nlp"
"47057","NLP: Fuzzy Word/Phrase Match","2019-03-11 03:47:19","","1","52","<nlp>","<p>I am attempting to determine if a given phrase (or a few words) is present in a relatively large text. For example:</p>

<p><strong>Text:</strong> </p>

<blockquote>
  <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Fusce sed
  tristique purus, id lobortis justo. Vestibulum ante ipsum primis in
  faucibus orci luctus et ultrices posuere cubilia Curae; Cras vitae
  neque non nibh elementum malesuada convallis et nunc. Nam vel tellus
  nec nunc dictum dignissim eu ut felis. In Tony Starkeget efficitur
  nunc. Cras ultrices turpis est, ac eleifend leo congue at. Donec lorem
  diam, mattis sed sollicitudin ac, tincidunt eu sem. Curabitur vel
  euismod lectus, sit amet tempor massa. Vivamus ut dictum nisl. Aliquam
  et urna sit amet urna hendrerit tincidunt in a mauris. Class aptent
  taciti sociosqu ad litora torquent per conubia nostra, per inceptos
  himenaeos. Maecenas vel justo metus. Sed gravida egestas velit,
  porttitor pulvinar justo hendrerit et.</p>
</blockquote>

<p><strong>Phrase/Words to match in the text above:</strong>  </p>

<pre><code>tony.stark  
t.stark  
stark_tony
starktony
</code></pre>

<p>The intention here is to infer if the person(Tony Stark) is being mentioned in a block of text. </p>

<p>I have read up on some fuzzy word match algorithms like <code>Levenshtein</code> and <code>Soundex</code> and also tested them in the above application but they appear to be useful for one word to one word match, not in the above application where various permutations of Tony stark is possible in both the pattern and text.</p>

<p>Would anyone be able to advice which fuzzy word matching algorithms would be ideal for this application, and perhaps share resources and sample code for its implementation.</p>

<p>Thanks.</p>
","nlp"
"46891","What is a better solution for text classification than use of perplexity","2019-03-07 22:38:47","","1","112","<classification><nlp>","<p>To classify some texts, I train a language model over a training set and then select the model which has the lowest perplexity on a given test sample as the class of that sample. 
I would like to know what other methods or measures except perplexity can be used.</p>
","nlp"
"46876","Determine whether or not a company has acquired others using NLP","2019-03-07 17:34:12","","1","35","<python><nlp>","<p>I'm trying to build a Python script to identify whether a company has acquired others. For this, I intend to seach Google for "" acquisition"", and parse the titles of the first N pages. Then, from the titles that have indicators that there was an actual acquisition, I want to extract the name of the company that was acquired, and I also want to provide a confidence metric. And obviously, it also has to make sure that it was the company I'm looking for that acquired another one, and not the other way around. </p>

<p>I have some experience with NER, but with this specific NLP task I don't know where to start. Is there something already implemented for such task?</p>
","nlp"
"46834","What to use pretrained models (Glove) or train my own model?","2019-03-07 06:00:58","","0","1304","<machine-learning><nlp><word2vec><word-embeddings>","<p>I have been using pre-trained models such as google news or Glove 6B model but many words in my text data does not have their vectors representation in those pre trained model. So I was thinking of maybe train my own model with the data I have.</p>

<p>Is there any disadvantages in training our own model for two class classification model? or Should I keep using pre-trained model. What is the difference between training our own model or using pre-trained model.</p>

<pre><code># This is how I am thinking to train the model 
from gensim.models import Word2Vec   
w2v_model=Word2Vec(list_of_sentance_train,min_count=5,size=50, workers=4)
</code></pre>
","nlp"
"46795","Clustering and graphing similarities of sentence subjects","2019-03-06 15:32:17","","1","56","<clustering><similarity><nlp>","<p>I have a bunch of sentences. Each sentence is given a weight of how ""close"" it is to a particular subject. </p>

<p>Ex. ""I love reading math books"" Subjects for the above sentence = <code>[Math: 10, Reading: 14, Romance: 1, Leisure: 4]</code></p>

<p>Now I would like to create a graph of nodes, where each node is a sentence and place these nodes at the origin in a 2D plane. Each subject forms the circumference of a circle surrounding the nodes. The ""closeness"" of the nodes to their respective subjects is represented by their positions in the 2D plane. I figured I could do this by taking each score for a subject of a sentence and apply it as a vector. Then add all the vectors, for all the subjects, together to settle on a final position</p>

<p><a href=""https://i.sstatic.net/VXs9v.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/VXs9v.jpg"" alt=""enter image description here""></a></p>

<p>The resulting plane could look like this</p>

<p><a href=""https://i.sstatic.net/iWGRx.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/iWGRx.jpg"" alt=""enter image description here""></a></p>

<p>The idea here is that we can now bind each sentence node to each other with edges to create a proximity graph. Using a Gabriel graph, we can only bind the closest nodes together</p>

<p><a href=""https://i.sstatic.net/gs4Kx.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/gs4Kx.png"" alt=""enter image description here""></a></p>

<p>The entire goal here is to construct a script from a bunch of sentences where we can flow down the script going from subject to subject without too much discontinuity. </p>

<p>As you can see I already have a method that seems to make sense. I was wondering however if there was already a set of methods for doing this kind of thing in data science. I was looking into <a href=""https://en.wikipedia.org/wiki/Spectral_clustering"" rel=""nofollow noreferrer"">spectral clustering</a>, and similarity measure. I even looked into bioinformatics and found <a href=""https://en.wikipedia.org/wiki/Needleman%E2%80%93Wunsch_algorithm"" rel=""nofollow noreferrer"">Needleman–Wunsch algorithm</a> and <a href=""https://en.wikipedia.org/wiki/Smith%E2%80%93Waterman_algorithm"" rel=""nofollow noreferrer"">Smith–Waterman algorithm</a>. But I'm not knowledgable in data science or bioinformatics. Can I get some directions as to where I should be headed to solve this kind of problem. Is there already an established set of tools and methods for accomplishing it?</p>
","nlp"
"46771","How to deal with name strings in large data sets for ML?","2019-03-06 10:06:45","","1","555","<python><nlp><preprocessing><encoding><classifier>","<p>My data set contains multiple columns with first name, last name, etc. I want to use a classifier model such as Isolation Forest later.</p>

<p>Some word embedding techniques were used for longer text sequences preferably, not for single-word strings as in this case. So I think these techniques wouldn't be the way that will work correctly. Additionally Label encoding or Label binarization may not be suitable ways to work with names, beacause of many different values on the on side (Label binarization) and no direct comparison between names on the other side (Label encoding).</p>

<p>Are there other approaches to use or transform especially name information in order to work with ML algorithms?</p>
","nlp"
"46714","Create word embeddings without keeping fastText Vector file in the repository","2019-03-05 18:25:02","","1","81","<nlp><word-embeddings>","<p>I am trying to embed a sentence with the help of <a href=""https://github.com/facebookresearch/InferSent"" rel=""nofollow noreferrer"">Infersent</a>, and Infersent uses <a href=""https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M-subword.zip"" rel=""nofollow noreferrer"">fastText</a> vectors for word embedding. The fastText vector file is close to 5 GiB.</p>

<p>When we keep the fastText vector file along with the code repository it makes the repository size huge, and makes the code difficult to share/deploy (even creating a docker container).</p>

<p>Is there any method to avoid keeping the vector file along with the repository, but reuse it for embedding new sentences?</p>
","nlp"
"46704","The memorisation capacity of an LSTM (real numbers)","2019-03-05 13:31:46","","1","232","<nlp><lstm>","<p>My question is the following:</p>
<p>It is known that a LSTM can remember sequences of one-hot encodings which represent integers (i.e. output <span class=""math-container"">$x_1, ... x_n$</span> after receiving <span class=""math-container"">$x_1, ... x_n$</span> as inputs, <span class=""math-container"">$x_k \in \{0,1\}^m$</span>, where <span class=""math-container"">$m$</span> is the number of distinct integers).</p>
<p>Is it theoretically possible for the LSTM to learn to remember sequences of real numbers instead (that can be expressed in a finite number of bits), i.e. if <span class=""math-container"">$x_t \in \mathbb{R}$</span> instead.</p>
<p>The task I'm concerned with is much simpler - I just want to output the first input <span class=""math-container"">$x_1$</span> after reading the entire sequence <span class=""math-container"">$x_1, ... x_n$</span>. I have done some small experiments with <span class=""math-container"">$x_t \in \mathbb{R}$</span>, using square loss. There seems to be some level of success, however the results aren't very interpretable (when I look at the weights). Can anyone shed some light on this, specifically:</p>
<ol>
<li>Does such a configuration of weights exist? (the questions following this quesetion suggests that it does exist)</li>
<li>If so, what are they and if not, why not?</li>
</ol>
<p>The LSTM model is specified by:</p>
<p>The input, forget and outputs gates:</p>
<p><span class=""math-container"">$$f_t = \sigma(W_f [h_{t-1}, x_t] + b_f)$$</span>
<span class=""math-container"">$$i_t = \sigma(W_i [h_{t-1}, x_t] + b_i)$$</span>
<span class=""math-container"">$$o_t = \sigma(W_o [h_{t-1}, x_t] + b_o)$$</span></p>
<p>And the internal state <span class=""math-container"">$c_t$</span> and hidden state <span class=""math-container"">$h_t$</span>:</p>
<p><span class=""math-container"">$$c_t = f_t * c_{t-1} +  i_t * \text{tanh}(W_c[h_{t-1}, x_t] + b_c) $$</span>
<span class=""math-container"">$$h_t = o_t * \text{tanh}(c_t)$$</span></p>
<p>As requested, this is the assignment question:</p>
<blockquote>
<p>Memory Task Description</p>
<p>Consider the following task: Given an input sequence of <span class=""math-container"">$n$</span> numbers, we would like a system which, after reading this sequence will return the first number in the sequence. That is given an input sequence: <span class=""math-container"">$(x_1, x_2, \cdots x_n)$</span>, <span class=""math-container"">$x_i \in \mathbb{R}$</span> the system has to return, at time <span class=""math-container"">$t=n$</span> after 'reading' the last input <span class=""math-container"">$x_n$</span>, the first input <span class=""math-container"">$x_1$</span>.</p>
<ol>
<li>Given the task above, consider the above recurrent models (RNNs/LSTMs/GRUs). Which of these arhitectures can (theoretically) perfom the task above? In answering this questions, please consider a simple one-layer model of RNNs/GRU/LSTM with a one-dimensional input <span class=""math-container"">$x_t$</span>, a <span class=""math-container"">$32$</span>-dim hidden and output layer, followed by a transformation to a one-dimensional final output which should predict <span class=""math-container"">$x_0$</span>.
Whenever the answer is positive, give the gates' activations and weigths that will produce the desired behaviour. Whenever the answer is no, prove that there exists no such parameters that an arbitrary input sequence can be transformed to produce the first symbol read.</li>
</ol>
</blockquote>
","nlp"
"46679","Bert Fine Tuning with additional features","2019-03-05 02:57:48","48436","9","5412","<nlp><bert>","<p>I want to use Bert for an nlp task. But I also have additional features that I would like to include. </p>

<p>From what I have seen, with fine tuning, one only changes the labels and retrains the classification layer.</p>

<p>Is there a way to used pre-trained Bert models and include additional features?</p>
","nlp"
"46648","What does 'Linear regularities among words' mean?","2019-03-04 15:34:12","46650","5","736","<nlp><language-model><representation>","<p><strong>Context</strong>: In the paper <a href=""https://arxiv.org/abs/1301.3781"" rel=""noreferrer"">""Efficient Estimation of Word Representations in Vector Space""</a> by T. Mikolov et al., the authors make use of the phrase: 'Linear regularities among words'.</p>

<p>What does that mean in the context of the paper, or in a general context related to NLP? </p>

<p>Quoting the paragraph from the paper:</p>

<blockquote>
  <p>Somewhat surprisingly, it was found that similarity of word
  representations goes beyond simple syntactic regularities. Using a
  word offset technique where simple algebraic operations are performed
  on the word vectors, it was shown for example that vector(”King”) -
  vector(”Man”) + vector(”Woman”) results in a vector that is closest to
  the vector representation of the word Queen [20].</p>
  
  <p>In this paper, we try to maximize accuracy of these vector operations
  by developing new model architectures that preserve <strong>the linear
  regularities among words</strong>. We design a new comprehensive test set for
  measuring both syntactic and semantic regularities1 , and show that
  many such regularities can be learned with high accuracy. Moreover, we
  discuss how training time and accuracy depends on the dimensionality
  of the word vectors and on the amount of the training data.</p>
</blockquote>
","nlp"
"46625","How to build confusion matrix , when predicted value and actual value is in sentence?","2019-03-04 07:21:08","","1","193","<machine-learning><python><nlp><confusion-matrix>","<p>I am building some model, which predict on basis of  highest probability from history and I am assuming this is best action.
I am comparing this with real action. </p>

<p>Predicted : process resumed somedate2</p>

<p>Actual :  process resumed at somedate1</p>

<p>As both are similar and assumed this is correct prediction.</p>

<p>I have applied cosine similarity for same and doing comparison.</p>

<p>Is there any better way to compare same and to check accuracy ?</p>

<p>Any help will be appreciated.</p>
","nlp"
"46619","NLP to recognize the meaning of a paragraph","2019-03-04 03:48:57","","1","307","<machine-learning><text-mining><nlp>","<p>How can I apply NLP to extract the summary of a paragraph,I found <a href=""https://datascience.stackexchange.com/questions/2646/extract-most-informative-parts-of-text-from-documents?newreg=590750240e6a48738cf4d598733614c5"">this</a> but was wondering if there is a better and easy way before implementing it, as the ans seems to be an old one.
Basically what I need is, I have 2 paragraph and wanted to know how related they were(maybe in percentage) in context of their <strong>meaning</strong>.</p>

<p>PS:I am new to both NLP and this forum.</p>
","nlp"
"46552","python - Identify variable in similar sentences","2019-03-03 02:44:51","","1","95","<python><scikit-learn><nlp><similarity><text>","<p>I'm looking to solve the following problem: I have a list of similar sentences as my dataset, and I want to be able to type a new sentence, which is also similar to the sentences in my dataset and then highlight the parts of the sentence which are varying. For example,</p>

<p>This is the set I have:</p>

<pre><code>I want to eat an apple
I want to eat an ice cream
I want to drink coffee
I want to eat a pizza
I want to puke ginger
</code></pre>

<p>These sentences are similar but they have some varying parts. The two main variables in these sentences are the <code>action</code> (eat, drink, puke) and the <code>food</code> (apple, ice cream, coffee, pizza, ginger).</p>

<p>Now if I type in a new sentence say <code>I want to drink cola</code> I want my model to identify <code>drink</code> and <code>cola</code> as the variables in this sentence.</p>

<p>At this point, I don't really care about the semantics that much. I will work on that later. Even if the sentence is <code>I want to hug a table</code>, all I want to do is identify <code>hug</code> and <code>table</code> as variables.</p>
","nlp"
"46420","Training NLP with multiple text input features","2019-02-28 19:38:26","","6","5571","<machine-learning><nlp><nltk>","<h2>Question:</h2>

<p>How can I train a NLP model with discrete labels that is based on multiple text input features?</p>

<h2>Background:</h2>

<p>I'm trying to predict the difficulty of a 4-option multiple choice exam question (probability of a test-taker selecting the correct response) based on the text of the question <em>along with</em> its possible responses. I'm hoping to be able to take into account how some incorrect yet convincing responses, the exact subject of which is relative to the content of the question, can skew the difficulty of the exam question, as well as how the wording of the question can make the question misleading.  </p>

<h2>Intuition, Attempts and Options:</h2>

<p>My intuition is that the content of the question and its responses are both significant in the prediction of its difficulty. However, when using a library like Spacy, NLTK or Textacy, training seems to be done on only one text column at a time. I'm looking at potentially five text columns at a time, or two if I concatenate the question responses together.  </p>

<p>I haven't been able to find much on the topic, but <a href=""https://towardsdatascience.com/natural-language-processing-on-multiple-columns-in-python-554043e05308"" rel=""noreferrer"">here</a> is an attempt I found. I thought this attempt was flawed because they were just doing a single-column train multiple times, and for example training the City against a salary value and concatenating that to your train of Job Description against salary value is not going to give a meaningful improvement to your first model, since the Job Description did not depend on the City when training.  </p>

<p>My options that I've found are:  </p>

<ul>
<li>Following the above attempt after all (which I think is flawed)</li>
<li>Concatenating my text features (which I can't understand why that would make sense in this case but seems to be the norm)</li>
<li>Eliminating some of my features entirely, such as narrowing the question down by subject matter and disposing of the question content and just training on the response options concatenated together (which also removes some very important information in the question content that can lead to prediction values)</li>
</ul>

<hr>

<p>Thoughts and advice? Is there a library that can make this easier? Thanks!</p>
","nlp"
"46353","Papers on anger detection in dialogues","2019-02-27 20:39:47","","2","48","<nlp><lstm><cnn>","<p>I am interested in anger detection in dialogues and I want to study multiple methods like LSTM, CNN, etc. Are there any good research papers or books about this subject?</p>
","nlp"
"46251","How to find similar meaning log messages in the source code?","2019-02-26 11:13:01","","0","109","<python><scikit-learn><nlp>","<p>Currently we have 10 thoundands log messages in the source codes which is maintained by different groups.</p>
<p>Some messages may be similar, but the sentence is not the same. My purpose is to create a centralized file which contains the unified message for reference.</p>
<p>For example:</p>
<blockquote>
<p>printf(<strong>&quot;Failed to open the file %s&quot;</strong>, file)</p>
<p>printf(<strong>&quot;%s: failed to open file %s&quot;</strong>, TIMESTAMP, file)</p>
<p>printf(<strong>&quot;Failed to create the file %s&quot;</strong>)</p>
</blockquote>
<p>I can extract all the message body which is in bold.They are not strict English sentence. but manually categorizing them takes too much time.</p>
<p>Is there a way to automatically categorizing the logs?
I just want to reduce the manual work.</p>
","nlp"
"46192","How to implement hierarchical labeling classification?","2019-02-25 12:17:32","","9","630","<neural-network><classification><keras><nlp><text>","<p>I am currently working on the task of eCommerce product name classification, so I have <strong>categories</strong> and <strong>subcategories</strong> in product data. I noticed that using subcategories as labels delivers worse results (84% acc) than categories (94% acc). But subcategories are more precise as labels, what's important for the whole task. And then I got an idea to first do category classification and then based on the results continue with subcategories within the predicted category.</p>
<p>The problem here is that I do not know how to approach this problem/define network architecture. Any hints on the neural networks, how to deal with it?</p>
<p>Currently I defined network like this:</p>
<pre><code>model = Sequential()
model.add(Dense(400, input_shape=(FEATURE_NUM,)))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(num_classes))
model.add(Activation('softmax'))
</code></pre>
","nlp"
"46171","What is the reason for the speedup of transformer-xl?","2019-02-25 02:24:10","46291","0","383","<deep-learning><nlp><attention-mechanism><transformer>","<p>The inference speed of transformer-xl is faster than transformer.</p>

<p>Why?</p>

<p>If state reuse is the reason, so it is compared by two 32seq_len + state-reuse vs one 64seq_len + no-state-reuse?</p>
","nlp"
"46098","What is the motivation for row-wise convolution and folding in Kalchbrenner et al. (2014)?","2019-02-23 14:56:32","","2","314","<machine-learning><deep-learning><nlp><cnn><convolution>","<p>I was reading the paper by Kalchbrenner et al. titled <a href=""https://arxiv.org/pdf/1404.2188.pdf"" rel=""nofollow noreferrer"">A Convolutional Neural Network for Modelling Sentences</a> and am struggling to understand their definition of convolutional layer.</p>

<p>First, let's take a step back and describe what I'd expect the 1d convolution to look like, just as defined in <a href=""https://arxiv.org/pdf/1408.5882.pdf"" rel=""nofollow noreferrer"">Yoon Kim (2014)</a>. </p>

<blockquote>
  <p>sentence. A sentence of length n (padded where necessary) is represented as</p>
  
  <p>x1:n = x1 ⊕ x2 ⊕ . . . ⊕ xn, (1)</p>
  
  <p>where ⊕ is the concatenation operator. In general, let xi:i+j refer to the concatenation of words
  xi , xi+1, . . . , xi+j . A convolution operation involves a filter w ∈ R^hk, which is applied to a window of h words to produce a new feature. For
  example, a feature ci is generated from a window
  of words xi:i+h−1 by</p>
  
  <p>ci = f(w · xi:i+h−1 + b) (2).</p>
  
  <p>Here b ∈ R is a bias term and f is a non-linear function such as the hyperbolic tangent. This filter is applied to each possible window of words in the sentence {x1:h, x2:h+1, . . . , xn−h+1:n} to produce a feature map</p>
  
  <p>c = [c1, c2, . . . , cn−h+1], (3)</p>
  
  <p>with c ∈ R^(n−h+1).</p>
</blockquote>

<p>Meaning a single feature detector transforms every window from the input sequence to a single number, resulting in n-h+1 activations.</p>

<p>Whereas in Kalchbrenner's paper, the convolution is described as follows:</p>

<blockquote>
  <p>If we temporarily ignore the pooling layer, we
  may state how one computes each d-dimensional
  column a in the matrix a resulting after the convolutional and non-linear layers. Define M to be the
  matrix of diagonals:
  M = [diag(m:,1), . . . , diag(m:,m)] (5)
  where m are the weights of the d filters of the wide
  convolution. Then after the first pair of a convolutional and a non-linear layer, each column a in the
  matrix a is obtained as follows, for some index j:
  <a href=""https://i.sstatic.net/7EwxO.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/7EwxO.png"" alt=""enter image description here""></a>
  Here a is a column of first order features. Second order features are similarly obtained by applying Eq. 6 to a sequence of first order features aj , ..., aj+m0−1 with another weight matrix M0. Barring pooling, Eq. 6 represents a core aspect of the feature extraction function and has a rather general form that we return to below. Together with pooling, the feature function induces position invariance and makes the range of higher-order features variable.</p>
</blockquote>

<p>As described in <a href=""https://stats.stackexchange.com/questions/345977/what-does-the-matrix-m-diagm-1-ldots-diagm-m-look-like"">this question</a>, the matrix M has dimensionalty of d x dm and the vector of concatenated w's has dimensionality dm. Thus the multiplication produces a vector of dimensionality d (for a single convolution of a single window!).</p>

<p>Architecture visualization from the paper seems to confirm this understanding:
<a href=""https://i.sstatic.net/3iHN7.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/3iHN7.png"" alt=""enter image description here""></a></p>

<p>The two matrices in the second layer represent two feature maps. Each feature map has dimensionality (s + m - 1) x d, and not (s + m - 1) as I would expect.</p>

<p>Authors refer to a ""conventional"" model where feature maps have only one dimension as Max-TDNN and differentiate it from their own.</p>

<p>As the authors point out, feature detectors in different rows are fully independent from each other until the top layer. Thus they introduce the Folding layer, which merges each pair of rows in the penultimate layer (by summation), reducing their number in half (from d to d/2).
<a href=""https://i.sstatic.net/e1Dyn.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/e1Dyn.png"" alt=""architecture""></a></p>

<p>Sorry for the prolonged introduction, here are my two main questions:</p>

<p>1) What is the possible motivation for this definition of convolution (as opposed to Max-TDNN or e.g. Yoon Kim's model)</p>

<p>2) In the Folding layer, why is it satisfying to only have dependence between pairs of corresponding rows? I don't understand the gain over no dependence at all.</p>
","nlp"
"46001","Detect named entities inside words","2019-02-22 07:45:24","","1","59","<nlp><named-entity-recognition><chatbot>","<p>Some languages have word endings with their nouns (like Finnish, e.g. ""in Berlin"" -> ""Berliinissä""). I have tried to annotate the characters in the training data as entities, but then I run the model, it doesn't detect the characters inside the word. When those characters are a separate word, only then they're detected. I am unable to think of an implementation to effectively detect named entities within a word. Any suggestions would be helpful.</p>
","nlp"
"45971","Error: valueError: input arrays should have the same number of samples as target arrays. Find 1 input samples and 0 target samples","2019-02-21 16:51:53","","1","2533","<python><neural-network><keras><nlp>","<p>I'm trying to do task for system calls classification. The code below is inspired from a text classification project. My system calls are represented as sequences of integers between 1 and 340. The error I got is:</p>

<blockquote>
  <p>valueError: input arrays should have the same number of samples as
  target arrays. Find 1 input samples and 0 target samples.</p>
</blockquote>

<p>I don't know what to do as it's my first time.</p>

<pre><code>   df = pd.read_csv(""data.txt"") 
   df_test = pd.read_csv(""validation.txt"")
   #split arrays into train and test data (cross validation)
    train_text, test_text, train_y, test_y = 
   train_test_split(df,df,test_size = 0.2)
   MAX_NB_WORDS = 5700

   # get the raw text data
   texts_train = train_text.astype(str)
   texts_test = test_text.astype(str)
   # finally, vectorize the text samples into a 2D integer tensor
   tokenizer = Tokenizer(nb_words=MAX_NB_WORDS, char_level=False)
   tokenizer.fit_on_texts(texts_train)
   sequences = tokenizer.texts_to_sequences(texts_train)
   sequences_test = tokenizer.texts_to_sequences(texts_test)

   word_index = tokenizer.word_index
   type(tokenizer.word_index), len(tokenizer.word_index)
   index_to_word = dict((i, w) for w, i in tokenizer.word_index.items()) 
    "" "".join([index_to_word[i] for i in sequences[0]])
    seq_lens = [len(s) for s in sequences]

    MAX_SEQUENCE_LENGTH = 100
    # pad sequences with 0s
    x_train = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH) 
    x_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)
    #print('Shape of data train:', x_train.shape)  #cela a donnée (1,100)
    #print('Shape of data test tensor:', x_test.shape)
    y_train = train_y
    y_test = test_y
    print('Shape of label tensor:', y_train.shape)
    EMBEDDING_DIM = 32
    N_CLASSES = 2

    y_train = keras.utils.to_categorical( y_train , N_CLASSES )
    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='float32')

    embedding_layer = Embedding(MAX_NB_WORDS, EMBEDDING_DIM,
                    input_length=MAX_SEQUENCE_LENGTH,
                    trainable=True)
    embedded_sequences = embedding_layer(sequence_input)

    average = GlobalAveragePooling1D()(embedded_sequences)
    predictions = Dense(N_CLASSES, activation='softmax')(average)

    model = Model(sequence_input, predictions)
    model.compile(loss='categorical_crossentropy',
      optimizer='adam', metrics=['acc'])
    model.fit(x_train, y_train, validation_split=0.1,
    nb_epoch=10, batch_size=1)
    output_test = model.predict(x_test)
    print(""test auc:"", roc_auc_score(y_test,output_test[:,1]))
</code></pre>
","nlp"
"45941","Keras value error: Operands could not be broadcast with with shapes(100,100) - GRU","2019-02-21 10:25:07","","2","504","<keras><nlp><word-embeddings><attention-mechanism>","<p>I am trying to use Hierarchical Attention Networks for classification of news articles using 20 newsgroup dataset that i downloaded from the internet. I came across this <a href=""https://github.com/FlorisHoogenboom/keras-han-for-docla"" rel=""nofollow noreferrer"">code</a> of the implementation and tried using it on 20 newsgroup dataset as i was curious to see the results and couldn't hold back. Like he showed in the example i followed same steps and got the error
""ValueError: Operands could not be broadcast together with shapes (100, 200) (None, Elemwise{mul,no_inplace}.0)"". I have never used keras,can anyone help me with what is wrong with the dimensions of the word encoding and sentence encoding.   </p>

<pre><code>import re
import numpy as np
import pandas as pd
import sys
import os
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.callbacks import ModelCheckpoint
from keras.utils import to_categorical
from nltk.tokenize import sent_tokenize
from sklearn.model_selection import train_test_split
from keras_han.model import HAN

MAX_WORDS_PER_SENT = 100
MAX_SENT = 15
MAX_VOC_SIZE = 20000
GLOVE_DIM = 100
TEST_SPLIT = 0.2

def remove_quotations(text):
    text = re.sub(r""\\"", """", text)
    text = re.sub(r""\'"", """", text)
    text = re.sub(r""\"""", """", text)
    text = re.sub(r""[^A-Za-z0-9]+"", "" "", text)
    return text

def remove_html(text):
    tags_regex = re.compile(r'&lt;.*?&gt;')
    return tags_regex.sub('', text)

print('Processing text dataset')
TEXT_DATA_DIR = ""E:/Thesis/20news-bydate-test""
df = pd.DataFrame(columns=['Text','Labels'])
texts = []  # list of text samples
labels_index = {}  # dictionary mapping label name to numeric id
labels = []  # list of label ids
for name in sorted(os.listdir(TEXT_DATA_DIR)):
    path = os.path.join(TEXT_DATA_DIR, name)
    if os.path.isdir(path):
        label_id = len(labels_index)
        labels_index[name] = label_id
        for fname in sorted(os.listdir(path)):
            if fname.isdigit():
                fpath = os.path.join(path, fname)
                if sys.version_info &lt; (3,):
                    f = open(fpath)
                else:
                    f = open(fpath, encoding='latin-1')
                t = f.read()
                i = t.find('\n\n')  # skip header
                if 0 &lt; i:
                    t = t[i:]
                texts.append(t)
                f.close()
                labels.append(label_id)
print('Found %s texts.' % len(texts))
df=pd.DataFrame({'Text':texts,'Label':labels})
df['Text'] = df['Text'].apply(remove_quotations) 
df['Text'] = df['Text'].apply(remove_html) 
df = df.replace('\n','', regex=True)
news = df['Text'].values
labels = df['Label'].values
#print(news)
#print(labels)

print(""Tokenization."")

# Build a Keras Tokenizer that can encode every token
word_tokenizer = Tokenizer(num_words=MAX_VOC_SIZE)
word_tokenizer.fit_on_texts(news)

# Construct the input matrix. This should be a nd-array of
# shape (n_samples, MAX_SENT, MAX_WORDS_PER_SENT).
# We zero-pad this matrix (this does not influence
# any predictions due to the attention mechanism.
X = np.zeros((len(news), MAX_SENT, MAX_WORDS_PER_SENT), dtype='int32')

for i, review in enumerate(news):
    sentences = sent_tokenize(review)
    tokenized_sentences = word_tokenizer.texts_to_sequences(
        sentences
    )
    tokenized_sentences = pad_sequences(
        tokenized_sentences, maxlen=MAX_WORDS_PER_SENT
    )

    pad_size = MAX_SENT - tokenized_sentences.shape[0]

    if pad_size &lt; 0:
        tokenized_sentences = tokenized_sentences[0:MAX_SENT]
    else:
        tokenized_sentences = np.pad(
            tokenized_sentences, ((0,pad_size),(0,0)),
            mode='constant', constant_values=0
        )

    # Store this observation as the i-th observation in
    # the data matrix
    X[i] = tokenized_sentences[None, ...]

# Transform the labels into a format Keras can handle
y = to_categorical(labels)

# We make a train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SPLIT)

embeddings = {}
with open('./embeddings(100).txt',encoding='utf-8') as file:
    for line in file:
        values = line.split()
        word = values[0]
        coefs = np.asarray(values[1:], dtype='float32')
        embeddings[word] = coefs
# Initialize a matrix to hold the word embeddings
embedding_matrix = np.random.random(
    (len(word_tokenizer.word_index) + 1, GLOVE_DIM)
)
embedding_matrix[0] = 0

for word, index in word_tokenizer.word_index.items():
    embedding_vector = embeddings.get(word)
    if embedding_vector is not None:
        embedding_matrix[index] = embedding_vector      
print(""Training the model"")
han_model = HAN(
    MAX_WORDS_PER_SENT, MAX_SENT, 20, embedding_matrix,
    word_encoding_dim=100, sentence_encoding_dim=100
)

han_model.summary()

checkpoint_saver = ModelCheckpoint(
    filepath='./tmp/model.{epoch:02d}-{val_loss:.2f}.hdf5',
    verbose=1, save_best_only=True
)

han_model.compile(
    optimizer='adagrad', loss='categorical_crossentropy',
    metrics=['acc']
)

han_model.fit(
    X_train, y_train, batch_size=20, epochs=10,
    validation_data=(X_test, y_test),
    callbacks=[checkpoint_saver]
)
</code></pre>

<p>Here is the Stack trace: 
<a href=""https://i.sstatic.net/qjls7.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/qjls7.png"" alt=""enter image description here""></a></p>
","nlp"
"45874","Using text classification for system calls","2019-02-20 12:18:28","","0","99","<python><deep-learning><keras><tensorflow><nlp>","<p>I'm working on a project in which I should classify System calls sequences, my dataset is represented as sequences of integers (from 1 to 340). To do the classification I have inspired from Text classification projects. I'm trying to use on of them but I found a problem in my dataset shape, the code is: </p>

<pre><code>df = pd.read_csv(""data.txt"") 
#df_test = pd.read_csv(""validation.txt"")

#split arrays into train and test data (cross validation)
train_text, test_text, train_y, test_y = train_test_split(df,df,test_size = 
0.2)

#train_text, train_y = (df,df)
#test_text, test_y = (df_test, df_test)
MAX_NB_WORDS = 5700

texts_train = train_text.astype(str)
texts_test = test_text.astype(str)


tokenizer = Tokenizer(nb_words=MAX_NB_WORDS, char_level=False)
tokenizer.fit_on_texts(texts_train)
sequences = tokenizer.texts_to_sequences(texts_train)
sequences_test = tokenizer.texts_to_sequences(texts_test)

word_index = tokenizer.word_index
#print('Found %s unique tokens.' % len(word_index))
type(tokenizer.word_index), len(tokenizer.word_index)
index_to_word = dict((i, w) for w, i in tokenizer.word_index.items()) 
"" "".join([index_to_word[i] for i in sequences[0]])

seq_lens = [len(s) for s in sequences]
#print(""average length: %0.1f"" % np.mean(seq_lens))
#print(""max length: %d"" % max(seq_lens))

MAX_SEQUENCE_LENGTH = 100
# pad sequences with 0s
x_train = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)  # former des 
sequence de meme taille 150, en ajoutant des 0
x_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)
#print('Shape of data train:', x_train.shape)  #it gives (1,100)
#print('Shape of data test tensor:', x_test.shape)
y_train = train_y
y_test = test_y
#if np.any(y_train):
  #y_train = to_categorical(y_train)
print('Shape of label tensor:', y_train.shape)
EMBEDDING_DIM = 50
N_CLASSES = 2
# input: a sequence of MAX_SEQUENCE_LENGTH integers
sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='float32')

embedding_layer = Embedding(MAX_NB_WORDS, EMBEDDING_DIM,
                        input_length=MAX_SEQUENCE_LENGTH,
                        trainable=True)
embedded_sequences = embedding_layer(sequence_input)

average = GlobalAveragePooling1D()(embedded_sequences)
predictions = Dense(N_CLASSES, activation='softmax')(average)

   model = Model(sequence_input, predictions)
model.compile(loss='categorical_crossentropy',
          optimizer='adam', metrics=['acc'])
model.fit(x_train, y_train, validation_split=0.1,
      nb_epoch=10, batch_size=100)
output_test = model.predict(x_test)
print(""test auc:"", roc_auc_score(y_test,output_test[:,1]))
</code></pre>

<p>I got the error: ValueError Error when checking target expected dense_1 to have shape(2,), but got array with shape(1,)</p>

<p>Any suggestion, cause I don't know how to proceed .
Thank you</p>
","nlp"
"45856","Machine learning or NLP approach to convert string about month ,year into dates","2019-02-20 06:30:01","45990","5","2479","<machine-learning><python><nlp><nltk><regex>","<p>I'm currently in the process of developing a program with the capability of converting human style of representing year into actual dates.
Example : <strong>last year last month</strong> into <strong>December 2018</strong>
string may be complete sentence like : <strong>what were you doing 5 years ago</strong> </p>

<p>it will gives <strong>2014</strong></p>

<p>The purpose is to evalute human style of represting year or date into actual date, i have created collection of this type of strings and matching them with regex.</p>

<p>I have read some machine learning but I'm not sure which algorithm suits this problem the best or if I should consider using NLP.</p>

<p>Does anyone have a suggestion of what algorithm to use or where I can find the necessary literature to solve my problem?</p>

<p>Thanks for any contribution!</p>
","nlp"
"45797","Help to choose algorithm for computing difference between 2 texts?","2019-02-19 07:45:48","45877","1","75","<python><nlp>","<p>I have a task to create a tool, which will be able to <strong>find articles-duplicates of a given reference article</strong>.
I know word vectorization (tf-idf,word2vec), RNN methods, but i can not choose something suitable for my situation.</p>

<p>My requirments:</p>

<ul>
<li>data are being collected on the fly  (program parses articles from web sites, so i don't have regular DB with collection of texts)</li>
<li>there is a reference text, whose copies need to be found</li>
<li>copies could be copypasted, partially copypasted (by paragraphs) or paraphrased</li>
<li><strong>reference-vs-copy comparison</strong> algorithm is preferable, but not required (instead of reference-vs-corps)</li>
<li>algorithm shouldn't do deep semantic analyzis, only kind of word counting, word vectorization, substring search</li>
<li>instead one algorithm, i can use a set of herurisitcs</li>
<li>algorithms can do false positive dicisions</li>
</ul>

<p>I come up with such ideas:</p>

<ol>
<li>download pretrained word2vec and compare means of word-vectors</li>
<li>Build a dictionary word->count from every text and compare it to reference dictionary</li>
<li>collect about 100 texts, vectorize them according to tf-idf and find closest to the reference</li>
</ol>

<p>I will apreciate, if you will point specific algorithms, libs, <strong>examples</strong> based on key-word extractions, dummy substring search, line difference comparison for <strong>python</strong> or CLI.</p>
","nlp"
"45746","Glove supported languages","2019-02-18 10:19:47","","1","634","<nlp><word-embeddings><stanford-nlp><embeddings>","<p>Recently I started reading more about NLP and following tutorials in Python in order to learn more about the subject. I started experimenting with words embeddings also, and I found some interesting results which I don't know how to interpret. I first used an English corpus for both training and testing and afterwards, I used the English corpus for training and a small French corpus for testing (all corpora have been annotated for the same binary classification task). In both cases, I used the pre-trained on tweets Glove embeddings. As the results in the case where I also used the French corpus improved (by almost 5%, reaching ~accuracy = 0.8), I was wondering if Glove was trained on multilingual data, as I didn't see anyone making this statement (I'm aware of the amount of data that is used) comparing to FastText, for example, where you have embeddings for different languages.</p>

<p>Also, if Glove supports multilingual information, this would also eliminate the need for mapping different embeddings into the same embedding space (as is the case for FastText).  </p>

<p>Any clarification would be greatly appreciated.</p>
","nlp"
"45720","TypeError: doc2bow expects an array of unicode tokens on input, not a single string","2019-02-17 16:45:39","","1","958","<python><nlp>","<pre><code>import re
from nltk.tokenize import sent_tokenize ,word_tokenize
from nltk.corpus import  stopwords
from nltk.stem import WordNetLemmatizer
from gensim.corpora.dictionary import Dictionary
from collections import Counter


 article1 = """"""Indian landscape is rich and replete with art, but not all of it gets a platform to be exhibited. With the upcoming edition of Hindustan Times Imagine Fest 2019, arts from across India will get a chance to become visible to people, who can also buy these artworks since the concept of this festival is to have all artworks priced below ₹2,00,000. """"""
 word = word_tokenize(article1)
 islower = [t.lower() for t in word if t.isalpha()]
 dictionary = Dictionary(islower)
 computer_id = dictionary.token2id.get(""computer"")
 corpus = [dictionary.doc2bow(article) for article in article1]
 print(corpus[4][:10])
</code></pre>

<blockquote>
  <p>I used all libraries but I cant understand whats the problem, A little guidance would be
  appreciated.</p>
</blockquote>
","nlp"
"45629","Contrastive loss problem in a character-level, siamese NN model","2019-02-15 11:19:08","","1","459","<keras><nlp><loss-function>","<p><strong>Summary: my NN with contrastive loss does not work, need help debugging</strong></p>

<p>Background:
I am trying to replicate <a href=""http://www.aclweb.org/anthology/W16-1617"" rel=""nofollow noreferrer"">this paper</a> </p>

<p>At first, I used binary crossentropy for loss, and the results were very good, but there was a caveat. The model correctly predicted which job titles were similar, but only if it had seen both titles in the training data. If it saw a given title for the first time, it would ALWAYS predict 1 (i.e. that they are similar). This is very undesirable for me product-wise.</p>

<p>Then, I tried the following implementation of the contrastive loss:</p>

<pre><code>def contrastive_loss(y_true, y_pred):
    """"""Contrastive loss from Hadsell-et-al.'06
    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf
    """"""
    margin = 1
    sqaure_pred = K.square(y_pred)
    margin_square = K.square(K.maximum(margin - y_pred, 0))
    return K.mean(y_true * sqaure_pred + (1 - y_true) * margin_square)
</code></pre>

<p>However, what happens now is that the model always predicts all zeros.
I run this code with print_tensor...</p>

<pre><code>    sqaure_pred =  K.print_tensor(K.square(y_pred))
    margin_square =  K.print_tensor(K.square(K.maximum(margin - y_pred, 0)))
    return  K.print_tensor(K.mean(y_true * sqaure_pred + (1 - y_true) * margin_square))
</code></pre>

<p>...and this is what I saw
<a href=""https://i.sstatic.net/RTG4O.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/RTG4O.png"" alt=""training_contrastive_loss""></a></p>

<p>Any ideas what I should try next?</p>

<p>This is the model:</p>

<pre><code>def build_blstm_encoder(params):
    lstm = params['lstm']
    nb_tokens = params['nb_tokens']
    maxlen = params['max_seq_len']
    offer_rep_dim = params['offer_rep_dim']
    emb_len = params['emb_len']

    input_1 = Input(shape=(maxlen,), dtype='int32')
    input_2 = Input(shape=(maxlen,), dtype='int32')
    emb_layer = Embedding(nb_tokens, output_dim=emb_len, input_length=maxlen, mask_zero=False)
    blstm_layer = Bidirectional(LSTM(output_dim=lstm, return_sequences=True), merge_mode='concat', weights=None)
    dense = Dense(offer_rep_dim, activation='relu')

    blstm_encoders = []
    for char_array in [input_1, input_2]:
        embs = emb_layer(char_array)
        blstm = blstm_layer(embs)
        dropout = Dropout(0.15)(blstm)
        dense_ = dense(dropout)
        flatten = Flatten()(dense_)
        blstm_encoders.append(flatten)

    distance = Dot([1, 1], normalize=True)(blstm_encoders)
    return Model([input_1, input_2], [distance])
</code></pre>

<p>The optimizer is just Adam() (default parameters)</p>

<p><strong>Questions:</strong>
Any idea what went wrong?
What would you try next?</p>
","nlp"
"45605","what is difference between set() and word_tokenize()?","2019-02-15 06:52:37","45606","1","385","<python><nlp><nltk>","<pre><code>from nltk.tokenize import sent_tokenize ,word_tokenize

sentence = 'jainmiah I love you but you are not bothering about my request, 
            please yaar consider me for the sake'

word_tok = word_tokenize(sentence)
print(word_tok)

set_all = set(word_tokenize(sentence))
print(set_all)
</code></pre>

<blockquote>
  <p>Actually both word_tokenize() and set(word_tokenize()) both returns
  same answers what makes the difference?</p>
</blockquote>
","nlp"
"45526","Does a precision score increasing with a higher number of folds mean the model will improve with more data?","2019-02-13 18:02:00","","5","173","<classification><nlp><cross-validation><model-evaluations>","<p>I have been working on a pretty simple text classifying module (tfidf + Random Forest). 
My manager insisted on using a simple .7/.3 split rather than doing cross validation, then was adamant about putting the model trained on 70% of the data in production (rather than the model trained on the whole thing). Her rationale was that the model would be more ""predictable"" and that anything we would gain from adding the remaining 30% would be negligible.</p>

<p>Out of curiosity, I ran a few test using cross-validation. First with 3 folds, then incrementally up to 10. With every increase in fold, the model's precision increases (first quite strongly: there's a 3 point increase in precision from 3 folds to 5 folds, the gains then become more marginal with each fold but still add up to a 5 point boost when reaching 10 folds). The variance between the cross-validation scores is always very low.</p>

<p>Am I right to assume that this can be interpreted as meaning the model is very likely to benefit from being trained on the whole dataset rather than on just 70%? </p>

<p>I understand there may be a risk of overfitting on the training data but a) I don't really see how the risk would be significantly mitigated by training on 70% of the data only, b) these are quite formal, standardized communications and unseen data is unlikely to differ significantly from the training datasets.</p>
","nlp"
"45520","Is there a way to cluster words based on how similarly they sound?","2019-02-13 15:49:08","","3","213","<word-embeddings><multilabel-classification><nlp><distance>","<p>I have a list of words for a fictional world I've made (don't judge lol). </p>

<p>My ultimate goal is to generate more words that sound like them through a markov generator, but for now, I'm trying to build a model that will classify words into their region by similarity to the other words in that region.</p>

<p>My first attempt used Levenshtein distance, and this performed... well, not very good. </p>

<p>I then tried using the Jaro-Winkler distance plus the Jaro-Winkler distance for the inverted string divided by two, to get the average distance (to refresh, Winkler considers the beginning of the word to be more important, so I did this to get the end as important too). This was better, with 80% train/25% test accuracy, but obviously not good enough at all. </p>

<p>As you can see, for the multiclass models I'm trying to use, these are poor predictors of the regional class I've assigned to each word. I have only found this:</p>

<p><a href=""https://www.oreilly.com/library/view/python-cookbook/0596001673/ch17s09.html"" rel=""nofollow noreferrer"">https://www.oreilly.com/library/view/python-cookbook/0596001673/ch17s09.html</a></p>

<p>As any sort of mechanism that takes sound into account, but it seems like words have to be similar length, and mine aren't necessarily within one or two character lengths of each other. </p>

<p>I mean, might be able to apply it, but I wanted to see if there was a better solution. My research hasn't turned up anything useful. </p>
","nlp"
"45495","How to find out the subject of an email (in the form of a sentence) or a pdf document in NLP using Python","2019-02-13 10:27:41","","1","20","<python><nlp><text-mining><topic-model>","<p>How to find out the subject of an email (in the form of a sentence)  or a pdf document in NLP using Python. If I do topic modelling and get different groups of topic, how do I pick out the only topic best suited for the document and join back all the words to form a meaningful sentence as the email/document subject.</p>
","nlp"
"45475","Variable input/output length for Transformer","2019-02-13 03:43:48","55353","13","8999","<nlp><attention-mechanism>","<p>I was reading the paper ""Attention is all you need"" (<a href=""https://arxiv.org/pdf/1706.03762.pdf"" rel=""noreferrer"">https://arxiv.org/pdf/1706.03762.pdf</a> ) and came across this site 
<a href=""http://jalammar.github.io/illustrated-transformer/"" rel=""noreferrer"">http://jalammar.github.io/illustrated-transformer/</a> which provided a great breakdown of the architecture of the Transformer. </p>

<p>Unfortunately, I was unable to find any explanation of why it works with input/output lengths that are not equal (eg. input: “je suis étudiant” and expected output: “i am a student”). </p>

<p>My main confusion is this. From what I understand, when we are passing the output from the encoder to the decoder (say <span class=""math-container"">$3 \times 10$</span> in this case), we do so via a Multi-Head Attention layer, which takes in 3 inputs:</p>

<ol>
<li>A Query (from encoder), of dimension <span class=""math-container"">$3 \times k_1$</span></li>
<li>A Key (from encoder), of dimension <span class=""math-container"">$3 \times k_1$</span></li>
<li>A Value (from decoder), of dimension <span class=""math-container"">$L_0 \times k_1$</span>, where <span class=""math-container"">$L_0$</span> refers to the number of words in the (masked) output sentence.</li>
</ol>

<p>Given that the Multi-Head Attention should take in 3 matrices which have the same number of rows (or at least this is what I have understood from its architecture), how do we deal with the problem of varying output lengths?</p>
","nlp"
"45464","Online vs Batch Learning in Latent Dirichlet Allocation using Scikit Learn","2019-02-12 20:50:15","","2","1746","<python><scikit-learn><nlp><topic-model><lda>","<p><a href=""https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html"" rel=""nofollow noreferrer"">Reference</a></p>
<p>I'm looking at the LDA algorithm from Scikit Learn for topic modeling. Can someone tell me how the 'online' method of learning works vs the 'batch' method of learning?
Also, what is learning decay and learning offset, in this context and in general?</p>
","nlp"
"45375","Word classification in the context","2019-02-11 11:39:35","","1","59","<classification><nlp><text>","<p>I'm trying to solve a 'negation-like' classification problem, where I need to classify whether a certain word within the context has negative or positive label.</p>

<p>For example, how to identify whether a keyword has been prescribed or only mentioned: </p>

<pre><code>['Aspirin has been prescribed to a patients'] -&gt; {[('key_word': 'aspirin', 'prescribed': TRUE)]}

['If symptoms continue, the patient should consider taking Omeprazol'] -&gt; {[('key_word': 'omeprazol', 'prescribed': FALSE)]}

[‘The plan is for him to commence 25mg of Trazodone as soon as he gets better.’] -&gt; {[('key_word': 'trazodone ', 'prescribed': FALSE)]}

['her current meds are: sertraline 200 mg and olanzapine 5 mg'] -&gt; {[('key_word': 'sertraline', 'prescribed': TRUE), ('key_word': 'olanzapine', 'prescribed': TRUE)]}

['if she continues to be depressed, then she needs to be started on Risperidone'] -&gt; {[('key_word': 'risperidone', 'prescribed': FALSE)]}
</code></pre>

<p>I have a training data set for this task, but it is not clear how to formulate the classification problem. It is similar to sentiment classification problem, but here instead of predicting a label of the entire sentence, I need to predict a label of a certain keyword based on the context.</p>

<p>Any ideas?</p>
","nlp"
"45362","Automating scoring of answers for a given question","2019-02-11 06:22:00","","1","35","<machine-learning><nlp><nltk>","<p>There is a topic 'X' on which students are asked to write an English passage. Given a question, I have multiple solutions with their scorings out of 10.
For a given answer by a new candidate, one needs to read the passage and score the answer. The main focus of scoring is <strong>vocablury</strong> and not grammer. 
How can I automate this process? What should be the steps?
Should I like train a simple model with answers as input and their scores as output. Then for a random answer I can check what is the score given my the model trained. But I feel like this will be very basic and it's accuracy will be very less. 
I am new to machine learning. Any reading material on this..etc anything would be great help. Thanks. </p>
","nlp"
"45335","Is there any text similarity databse available for phrases?","2019-02-10 10:40:47","","2","85","<machine-learning><deep-learning><nlp>","<p>I want to train my application for phrase similarity. I want my model to predict similarity score for phrases as shown in below examples.
ex-</p>

<pre><code>International Business Machines = I.B.M
Synergy Telecom = SynTel
Beam inc = Beam Incorporate
Sir J J Smith = Johnson Smith
Alex, Julia = J Alex
James B. D. Joshi = James Joshi
James Beaty, Jr. = Beaty
</code></pre>

<p>Is there any dataset available to train this type of model?</p>
","nlp"
"45204","determining unpronounceability","2019-02-07 09:56:54","","1","107","<nlp>","<p>I would like to determine whether a given string (eg in English) is pronounceable or not , e.g. using soundex or the like .  Has anyone run across a way to do this? </p>

<p>Examples of unpronounceable: 
'lskjd','xqq','gtmm'</p>

<p>Examples of pronounceable: 
'aabaa','kwerty','smorglebop'</p>
","nlp"
"45190","How is maximizing L(lambda1, lamda2, lamda3) equivalent to minimizing perplexity?","2019-02-07 04:49:52","","1","78","<nlp><language-model>","<p>In language modeling, L(lambda1, lambda2, lambda3) is defined as:</p>

<pre><code>Sum(count of trigram(u,v,w) x q(w|u,v)) 
</code></pre>

<p>where u, v, w are words in the corpus
and 
perplexity is defined as:</p>

<pre><code>2^-l 
</code></pre>

<p>where </p>

<pre><code>l = (1/M)Sum(log(q(w|u,v)).
</code></pre>

<p>where M is the total no. of words in the corpus.</p>

<p>Also,</p>

<pre><code>q(w|u,v) = lambda1*q(w|u,v) + lambda2*q(w|v) + lamdba3*q(w)
</code></pre>

<p>The q-values on the right hand side are maximum likelihood estimates.
Some of the materials for Natural Language Processing state that maximizing L is same as minimizing perplexity. I don't see how that is true or can be mathematically proved.</p>
","nlp"
"45183","How to learn irrelevant words in an information retrieval system?","2019-02-06 18:56:27","45194","2","417","<nlp><recommender-system><word-embeddings>","<p>Right now my recommender system for information retrieval uses word embedding stogether with Tfidfs weights like written here:
<a href=""http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/"" rel=""nofollow noreferrer"">http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/</a></p>

<p>Using Tfidf improves results. But I have the problem that irrelevant keywords (high frequent words) still have a large impact.
Can I learn a system such that it learns on which words to pay attention - preferred in an unsupervised way?</p>

<p>What can you suggest for a better information retrieval using word embedings?</p>
","nlp"
"45163","imbalanced dataset in text classififaction","2019-02-06 12:31:40","45187","3","2702","<python><nlp><class-imbalance><imbalanced-learn>","<p>I have a data set collected from Facebook consists of 10 class, each class have 2500 posts, but when count number of unique words in each class, they has different count as shown in the figure <a href=""https://i.sstatic.net/eUIMX.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/eUIMX.png"" alt=""word count in each class""></a></p>

<p>Is this an imbalanced problem due to word count , or balanced according number of posts. and what is the best solution if it imbalanced?</p>

<p><strong>update</strong>
My python code:</p>

<pre><code>data = pd.read_csv('E:\cluster data\One_File_nonnormalizenew2norm.txt', sep=""*"")

data.columns = [""text"", ""class1""]
data.dropna(inplace=True)
data['class1'] = data.class1.astype('category').cat.codes
text = data['text']

y = (data['class1'])
sentences_train, sentences_test, y_train, y_test = train_test_split(text, y, test_size=0.25, random_state=1000)
from sklearn.feature_extraction.text import CountVectorizer
num_class = len(np.unique(data.class1.values))



vectorizer = CountVectorizer()
vectorizer.fit(sentences_train)

X_train = vectorizer.transform(sentences_train)
X_test  = vectorizer.transform(sentences_test)

model = Sequential()
max_words=5000
model.add(Dense(512, input_shape=(60874,)))
model.add(Dense(20,activation='softmax'))####
model.summary()
model.compile(loss='sparse_categorical_crossentropy',
  optimizer='rmsprop',
  metrics=['accuracy'])

model.fit(X_train, y_train,batch_size=150,epochs=10,verbose=2,validation_data=(X_test,y_test),shuffle=True)
predicted = model.predict(X_test)
predicted = np.argmax(predicted, axis=1)
accuracy_score(y_test, predicted)
predicted = model.predict(X_test)
predicted = np.argmax(predicted, axis=1)
accuracy_score(y_test, predicted)

0.9592031872509961
</code></pre>
","nlp"
"45149","prepare email text for nlp (sentiment analysis)","2019-02-06 09:37:59","45222","3","910","<nlp><sentiment-analysis>","<p>I have text of emails, which also contains disclaimers, phone numbers, email addresses, file attachment names, addresses, greetings etc. </p>

<p>At the moment I blindly pass this text through an OOTB sentiment analyser called <a href=""https://medium.com/analytics-vidhya/simplifying-social-media-sentiment-analysis-using-vader-in-python-f9e6ec6fc52f"" rel=""nofollow noreferrer"">Vader</a> with poor results (i.e. if I open an email marked as negative, my human understanding does not confirm the sentiment - looking at the core English text).</p>

<p>I could use regex etc. to strip out email addresses, file names etc. but other text components (e.g. addresses, disclaimers) are harder to remove. Btw, disclaimers are often negative ...</p>

<p>Anyway, I wonder if anyone is aware of text prep methods for my scenario - to extract core human text. Google searches were moderately successful. Thanks! </p>
","nlp"
"45143","Dealing with missing n-grams in Naive Bayes classifier","2019-02-06 08:36:38","","1","53","<nlp><naive-bayes-classifier><ngrams>","<p>I am doing sentiment analysis on code-mixed text data, i.e English used interchangeably with another language. The dataset I currently have is very small in size, approx 3.5k samples.</p>

<p>I am sure that this dataset will not contain all possible n-grams that are used in the language. Are there any ways I can deal with this problem without increasing the number of samples in the dataset?</p>

<p>For now, I am using a MNB classifier to do the sentiment analysis.</p>
","nlp"
"45033","Trying to implement a ""smart compose"" feature","2019-02-04 12:10:56","","3","842","<nlp>","<p>I found this post on Gmail's smart compose feature, and it got me thinking about trying to implement it myself.</p>

<p><a href=""https://ai.googleblog.com/2018/05/smart-compose-using-neural-networks-to.html"" rel=""nofollow noreferrer"">https://ai.googleblog.com/2018/05/smart-compose-using-neural-networks-to.html</a></p>

<p>The text is super vague though - the only part that gives any direction at all is ""we combined a BoW model with an RNN-LM"".</p>

<p>I remember that BoW is an unordered representation of the input corpus where only relative magnitudes are important. But I just don't have enough experience to know how an unordered representation like this could help me predict an ordered suffix.</p>

<p>Does anybody have any thoughts on how this might more specifically work? Or any references for further reading? I wasn't able to find much unfortunately.</p>
","nlp"
"45023","How to extract name of objects from technical description (NLP)","2019-02-04 09:05:24","","1","98","<nlp>","<p>I have a list of technical descriptions of mechanical parts such as </p>

<p>Relay Compressor A4u8skk</p>

<p>Relay-Start-Compressor sdfvsh2</p>

<p>Relay-Start-Compressor-Copelan 05-569-21</p>

<p>Compressor-J4KS0AK-5gss-fj2</p>

<p>and i would like that my model output for the each sample of that list above will be ""Compressor"" , i have 700+ unique samples like the one above, not only Compressor but large list of items . </p>

<p>What is the best approach / models that i should look for that kind of task? 
is topic modeling related ? </p>
","nlp"
"44981","NLP text autoencoder that generates text in poetic meter","2019-02-03 03:12:52","","2","317","<deep-learning><nlp><text-generation>","<p>I would like to create an NLP autoencoder that happens to only generate text that conforms to a poetic meter, for example 'iambic pentameter'. That is, the output should be a series of clauses which are 10 syllables long and are read aloud with the verbal stresses as 'duh-DUH duh-DUH duh-DUH duh-DUH duh-DUH'. Shakespeare and his contemporaries used this format for much of their poetry. </p>

<p>The only method I can think of is to collect lines of text which are in this format, and then train an autoencoder on these. Since the network only emits iambic pentameter while training, it should also emit iambic pentameter while predicting. Given a large enough corpus, this seems like it should work.</p>

<p>One problem is that this training only accepts iambic pentameter. To rewrite other text into IP, the training needs to include variations of text in IP and train (not IP sentence) to emit (IP sentence). Generating these variations is straightforward. This crosses the line from training ""sentence embeddings"" to training ""thought embeddings"".</p>

<p>Are there other ways to solve this problem? Is there a way to directly alter the sentence embedding space? For example, variational sampling works by applying a nonlinear transformation on the embedding space.</p>

<p>Note: the CMU Pronouncing Dictionary supplies pronunciation stresses for over 100k words, usable for classifying meter:
<a href=""http://www.speech.cs.cmu.edu/cgi-bin/cmudict"" rel=""nofollow noreferrer"">http://www.speech.cs.cmu.edu/cgi-bin/cmudict</a></p>

<p>Note: this is a personal hobby project to teach myself deep learning &amp; NLP- I really do not know whether it is achievable with current NLP technology.</p>
","nlp"
"44975","how to label a tain_data?","2019-02-02 20:24:01","","1","42","<nlp><cross-validation><multilabel-classification><randomized-algorithms>","<p>I have one assignment that I have four files
1) train_data.csv: The training file contains two fields (text, id).
2) train_label.csv: The label file contains two fields (id, label).
3) test_data.csv: The test file contains two fields (text, id).
4) sample_submission.csv: This is a file that needs to be submitted.
And this should be obvious multilabel classification, but whenever I try to identify labels in train data, it doesn't show labels.
How can I remove noise from train_data??
Any type of suggestion welcome!!</p>

<p>train_label:
<a href=""https://i.sstatic.net/TgEfl.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/TgEfl.jpg"" alt=""enter image description here""></a></p>

<p>train_data:
<a href=""https://i.sstatic.net/OF0Yu.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/OF0Yu.jpg"" alt=""enter image description here""></a></p>
","nlp"
"44925","Identify specify areas in the text","2019-02-01 16:33:45","","-1","49","<machine-learning><neural-network><deep-learning><nlp>","<p>I'd be interested in identifying various areas in the text message. Let's say I have a text containing some introduction, then there is a poem and at the end there are some urls to some web pages. 
I'd like to be able to break down the text into these sections and process them separately.
I should be able to collect quite a few training data for each of the sections.
Any ideas or references to papers would be highly appreciated!
Thanks</p>
","nlp"
"44915","Help in NLP Problem","2019-02-01 13:30:06","","0","61","<nlp>","<p>I am trying to solve an NLP problem and, since I am new to NLP I would like to ask for some insight.</p>

<p>I have a data-set with 300000 and 14 features. This data set is about customer complaints, and the data set includes the company the complaint was made, the date, the issue (and sub-issue), the product, and most importantly, the customer complaint narrative.</p>

<p>I am now supposed to build a classifier that provides the root cause of the problem, given an unclassified complaint narrative. </p>

<p>The thing is, there is an apparent root cause (I assume that what is in the issue feature) and the actual root cause, that may differ from the apparent root cause, and is contained inside the customer narrative.</p>

<p>I already tokenized,removed stopwords and used Lemmatisation on the customer narrative but now I am a bit lost as to what to do next.</p>

<p>Anybody can point me in the right direction?</p>

<p>Thank you!</p>

<p>EDIT: Added some text</p>
","nlp"
"44876","What options are out there to extract text from a group of PDFs where each PDF is formatted differently but contains the general same content","2019-01-31 19:58:54","","1","83","<nlp><text-mining><text>","<p>Think insurance/medical forms that come from different companies.  There is no standard on formatting.  I am trying to extract the text based on each section of a given form.  A form might have a section of texted divided by whitespace, border, lines etc.  I need to keep the text in context of that section.</p>

<p>Textract from Amazon is something I am waiting for, but was wondering if there was some other alternatives.</p>
","nlp"
"44795","Find all potential similar documents out of a list of documents using clustering","2019-01-30 00:33:11","44799","1","387","<machine-learning><python><nlp><clustering><similarity>","<p>I'm working with the quora question pairs csv file which I loaded into a pd dataframe and isolated the qid and question so my questions are in this form : </p>

<pre><code>0        What is the step by step guide to invest in sh...
1        What is the step by step guide to invest in sh...
2        What is the story of Kohinoor (Koh-i-Noor) Dia...
3        What would happen if the Indian government sto...
.....
19408    What are the steps to solve this equation: [ma...
19409                           Is IMS noida good for BCA?
19410              How good is IMS Noida for studying BCA?
</code></pre>

<p>My dataset is actually bigger (500k questions) but I will use these questions to showcase my problem.</p>

<p>I want to identify pairs of questions that have a high probability of asking the same thing. I thought about the naive way, which is to turn each sentence into a vector using doc2vec and then for each sentence calculate the cosine similarity with every other sentence. Then, keep the one with the highest similarity and in the end print all those that have a high enough cosine similarity. The problem is this would take ages to finish so I need another approach.</p>

<p>Then I found an answer in another question that suggests to use clustering to solve a similar problem. So following is the code I implemented based on that answer.</p>

<pre><code>""Load and transform the dataframe to a new one with only question ids and questions""
train_df = pd.read_csv(""test.csv"", encoding='utf-8')

questions_df=pd.wide_to_long(train_df,['qid','question'],i=['id'],j='drop')
questions_df=questions_df.drop_duplicates(['qid','question'])[['qid','question']]
questions_df.sort_values(""qid"", inplace=True)
questions_df=questions_df.reset_index(drop=True)

print(questions_df['question'])

# vectorization of the texts
vectorizer = TfidfVectorizer(stop_words=""english"")
X = vectorizer.fit_transform(questions_df['question'].values.astype('U'))
# used words (axis in our multi-dimensional space)
words = vectorizer.get_feature_names()
print(""words"", words)


n_clusters=30
number_of_seeds_to_try=10
max_iter = 300
number_of_process=2 # seads are distributed
model = KMeans(n_clusters=n_clusters, max_iter=max_iter, n_init=number_of_seeds_to_try, n_jobs=number_of_process).fit(X)

labels = model.labels_
# indices of preferable words in each cluster
ordered_words = model.cluster_centers_.argsort()[:, ::-1]

print(""centers:"", model.cluster_centers_)
print(""labels"", labels)
print(""intertia:"", model.inertia_)

texts_per_cluster = numpy.zeros(n_clusters)
for i_cluster in range(n_clusters):
    for label in labels:
        if label==i_cluster:
            texts_per_cluster[i_cluster] +=1

print(""Top words per cluster:"")
for i_cluster in range(n_clusters):
    print(""Cluster:"", i_cluster, ""texts:"", int(texts_per_cluster[i_cluster])),
    for term in ordered_words[i_cluster, :10]:
        print(""\t""+words[term])

print(""\n"")
print(""Prediction"")

text_to_predict = ""Why did Donald Trump win the elections?""
Y = vectorizer.transform([text_to_predict])
predicted_cluster = model.predict(Y)[0]
texts_per_cluster[predicted_cluster]+=1

print(text_to_predict)
print(""Cluster:"", predicted_cluster, ""texts:"", int(texts_per_cluster[predicted_cluster])),
for term in ordered_words[predicted_cluster, :10]:
    print(""\t""+words[term])
</code></pre>

<p>I thought that this way I could find for each sentence the cluster that it most likely belongs in and then calculate the cosine similarity between all other questions of that cluster. This way instead of doing it on all the dataset I will be doing it on far fewer documents. However using the code for an example sentence ""Why did Donald Trump win the elections?"" I have the following results.</p>

<pre><code>Prediction
Why did Donald Trump win the elections?
Cluster: 25 texts: 244
    trump
    donald
    clinton
    hillary
    president
    vote
    win
    election
    did
    think
</code></pre>

<p>I know that my sentence belongs to cluster 25 and I can see the top words for that cluster. However how could I access the sentences that are in this cluster. Is there any way to do it?</p>
","nlp"
"44731","Latent feature extraction using dnn and word2vec embeddings","2019-01-28 19:35:36","","2","77","<nlp><word2vec>","<p>I recently read a journal about a tag-aware recommender system. There is a part in the paper which I do not understand. </p>

<p>They used word2vec first and using embeddings as input to a DNN to extract latent features. I'm not sure what could the latent features be. Could anyone give me some hints?</p>

<p><a href=""https://i.sstatic.net/uWLAc.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/uWLAc.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.sstatic.net/wfBwD.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/wfBwD.png"" alt=""enter image description here""></a></p>
","nlp"
"44699","How to train neural word embeddings?","2019-01-28 13:15:46","","3","227","<neural-network><deep-learning><nlp><word-embeddings>","<p>So I am new to Deep Learning and NLP. I have read several blog posts on medium, towardsdatascience and papers where they talk about pre-training the word embeddings in an unsupervised fashion and then use them in supervised DNN. But recently I read a blog <a href=""https://machinelearningmastery.com/develop-word-embedding-model-predicting-movie-review-sentiment"" rel=""nofollow noreferrer"">post</a> which suggested that training the word embeddings while training the neural network gives better results. <a href=""https://towardsdatascience.com/deep-learning-for-specific-information-extraction-from-unstructured-texts-12c5b9dceada"" rel=""nofollow noreferrer"">This</a> is the other link.</p>
<p>So my question is which one should I follow?</p>
<p>Some YouTube videos that I referred:</p>
<ol>
<li>Deep Learning for NLP without Magic Part 1, 2 and 3</li>
</ol>
","nlp"
"44697","Word classification (not text classification) using NLP","2019-01-28 12:32:03","","2","2210","<machine-learning><classification><nlp><named-entity-recognition>","<p>I have been trying to extract Person name and Company name out of string. But, I have been facing lot of difficulties. I have a dataset of names and a dataset of company names. In the string, I wish to classify each word as “Name”, “Company Names” or “Other”. In case classification of three classes is difficult, I can compromise on this by using POS tagger as a filter and using only nouns for “Person name” and “Company name” (and manually sift through the resultant data if necessary).</p>

<p>Please note that I have tried several POS taggers. None of them have worked well out of the box for all (international) names and company names. (Stanford NER, NLTK, Google API or even MeaningCloud)</p>

<ol>
<li><p>I read through literature. I got few ideas about POS tagging. But, literally every literature out there focuses mostly on CNN at character level and I was unable to understand why that is necessary. (Please note: I can only post two links as a newbie. So, I am not able to share the links of several research papers I read. Also, I have never worked much with CNNs.)</p></li>
<li><p>I googled “word classification” and I got almost no results. There are few results for “word clustering”. But, I felt that few algorithms may not work after trying few things. See the image below for more details. (I found CNN based character sequence stuffs instead of word classification)</p></li>
</ol>

<p><a href=""https://i.sstatic.net/GyXZN.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/GyXZN.png"" alt=""enter image description here""></a></p>

<ol start=""3"">
<li>In the image above, I have tried two-character and three-character set. I tried to observe audio inflection patterns in the characters and then get a score based on it. But, as we can observe, there can be no hyperplane splitting the data. I have just used the ascii value of three characters from the database. For example, if database has name “Prasad”, I would convert that to “pra”, “ras”, “asa”, “sad” and then check against a database of three-character set. I would get the sum of the frequencies of these sequences from company name database and human name database, normalise, get confidence measure and then, work through this. But, this is failing badly too.</li>
</ol>

<p>For example, if the string is “Prasad is a software engineer of X”, I would wish to have the following response:
(‘Prasad’, ‘name’)
(‘software’, ‘other’)
(‘engineer’, ‘other’)
(‘X’, ‘company’)</p>

<p>(X is a multicharacter string. Assume any single word company name)</p>

<p>I will definitely ignore any words with less than three characters.</p>

<p>I can understand one typical question as to why I may not consider word level n-gram. It is because, I do not have proper training data. I am not sure if relationship extraction between words can be done.</p>

<p><strong>Main question:</strong></p>

<p>Is this possible to do and get more than 80% accuracy? (with or without CNN) and how?</p>

<p><strong>Sub-questions:</strong></p>

<p><s>1.</s></p>

<ol start=""2"">
<li>Is there any tutorial for three class naive bayes classifier (for person, company, others) neatly explained with table and data as in <a href=""https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/"" rel=""nofollow noreferrer"">https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/</a> ?</li>
</ol>

<p><s>3.</s></p>

<ol start=""4"">
<li><p>I tried phonetics and failed. I used many of the preprocessing algorithms and tried Soundex, Leveinshtein’s distance, metaphone etc as in <a href=""http://home.iitk.ac.in/~ankitkr/projects/phoneme/report.pdf"" rel=""nofollow noreferrer"">http://home.iitk.ac.in/~ankitkr/projects/phoneme/report.pdf</a> . They all failed badly. Is there any way to extract most important phonetics of a word?</p></li>
<li><p>Why can’t word classification be recommended? Almost every library uses word embedding and then proceeds from there. But, why is the research so sparse on word classification (atleast in the first page of Google)?</p></li>
<li><p>Should I proceed with text-to-speech and then using some sort of audio signatures in frequency transform? I know that it is computationally expensive. But, is this the way to go about it if we need higher accuracy?</p></li>
</ol>

<p><s>7.</s></p>

<ol start=""8"">
<li>Why is my three character set approach failing so badly? Is it due to false positives and false negatives? In that case, should I stick to Naive Bayes? Are there any other algorithms that I can apply? (after looking at my data, I lost all hopes of hyperplanes)</li>
</ol>

<p>Thanks for your time and consideration.</p>

<p>References:</p>

<p>""what machine/deep learning/ nlp techniques are used to classify a given words as name, mobile number, address, email, state, county, city etc"" datascience stackexchange. (I found no answer which I understood and implemented them well. Similarly, ""Appropriate algorithm for string (not document) classification?"" (I was not able to figure out TF IDF approach for ""Prasad is a software engineer of X"")</p>

<p><strong>EDIT:</strong>
Questions 1, 3 and 7 have been considered ""solved"" after checking comments of @ShamitVerma. As algorithms are of higher interest, this question has been edited.</p>
","nlp"
"44522","How does the hashing trick work in NLP","2019-01-24 18:45:27","","1","174","<nlp><dimensionality-reduction>","<p>If I have 1000 words and want to reduce the dimension to 500, does that mean per hash there will be two words on average?
In the course NLP specialization on courser, 40 million unique word matrix was reduced to 4 million word matrix.I can only imagine how many collisions will be there, but it turns out that there would not be may collisions.
Can anyone please throw some light on this.
Thanks.</p>
","nlp"
"44504","sentence patterns/rules/formulas for imperative sentences …","2019-01-24 14:02:19","","2","310","<nlp><nlg>","<p>Please let me ask if you know any ressources on imperative sentence patterns for natural language generation.</p>

<p>There’s probably some ressources online, but I did not really find any.</p>

<p>Imperative sentences, or command sentences, could be structured very simple, e.g.:</p>

<p>1) “Eat!” (intransitive action-verb + exclamation mark)</p>

<p>2) “Eat rice!” (transitive action-verb + noun (singular or plural in some cases) + exclamation mark)</p>

<p>3) “Eat healthy!” (intransitive action-verb + adverb)</p>

<p>… but the more words per sentence, the more complex it gets.</p>

<p>So maybe there’s some general patterns/formulas for creating different variations of such sentences in a grammatically correct sense.</p>

<p>EDIT, as it seems the question is unclear:</p>

<p>I want to generate imperative sentences, i.e. instructional sentences. Is there any pattern on how to create such sentences with NLG?</p>
","nlp"
"44448","how to extract the Top contributing labels/words in universal-sentence-encoder-large - TransformerModel?","2019-01-23 15:34:39","","2","130","<nlp><clustering><feature-extraction><encoding><embeddings>","<p>I'm using the universal-sentence-encoder-large (Transformer Model) encoding process for embedding and then using the embedding for Clustering - Basically for unsupervised learning.</p>

<p>I want to get the <strong>top features / words</strong> which are contributing to the resultant clusters, in order to define the cluster or name the cluster in the business aspect with the help of the words contributing effectively for the cluster.
Is there any way to do this using the universal-sentence-encoder-large?</p>

<p>where can I actually find the usage of universal-sentence-encoder-large's functions, like,</p>

<ul>
<li>Fucntions:

<ul>
<li>embed.export()</li>
<li>embed.get_attached_message()</li>
<li>embed.get_input_info_dict()</li>
<li>embed.variable_map()</li>
<li>embed.variables()</li>
<li>embed.get_signature_names()**</li>
</ul></li>
</ul>

<p>Any help is hugely appreciated!</p>

<p>Thanks
Arav</p>
","nlp"
"44322","Job Recommendation System","2019-01-21 10:33:13","","1","137","<machine-learning><predictive-modeling><lstm><recommender-system><nlp>","<p>I am building a Job Recommendation System where I have Student Data for different subjects in Machine Learning(Data Viz, Python, Statistics, etc) and their skills from the resume. Need to Recommend jobs scraped from a reputed website.</p>

<p>Current thoughts- Using CF for finding similar students with similar skills and ranking them based on their performance in the subjects and then matching them with the scraped jobs for the relevant skills.</p>

<p>Based upon Job Titles- options are word2vec to find semantics from job titles, Lstms for semanctics etc.</p>

<p>Based upon Job's key skills- Can do direct strin matching and filter based upon Experience. </p>

<p>I want a good approach for my Problem.</p>
","nlp"
"44306","How to calculate Accuracy, Precision, Recall and F1 score based on predict_proba matrix?","2019-01-20 20:30:38","44351","1","5308","<classification><scikit-learn><nlp><naive-bayes-classifier>","<p>I found <a href=""https://monkeylearn.com/text-classification/"" rel=""nofollow noreferrer"">this link</a> that defines <code>Accuracy</code>, <code>Precision</code>, <code>Recall</code> and <code>F1 score</code> as:</p>

<p><strong>Accuracy</strong>: the percentage of texts that were predicted with the correct tag.</p>

<p><strong>Precision</strong>: the percentage of examples the classifier got right out of the total number of examples that it predicted for a given tag.</p>

<p><strong>Recall</strong>: the percentage of examples the classifier predicted for a given tag out of the total number of examples it should have predicted for that given tag.</p>

<p><strong>F1 Score</strong>: the harmonic mean of precision and recall.</p>

<p>Following <a href=""https://datascience.stackexchange.com/questions/44266/naive-bayes-all-of-the-elements-in-predict-proba-output-matrix-are-less-than-0"">this question</a> of mine, my <code>MultinomialNB</code> classifier calculated the <code>predict_proba</code> matrix for the test set (with 14 samples) as follows:</p>

<pre><code>0.192995    0.0996929   0.173688    0.136715    0.126616    0.133012    0.137282
0.174185    0.109345    0.169467    0.144389    0.115021    0.132762    0.154831
0.14172     0.190075    0.125429    0.155343    0.122939    0.149733    0.114763
0.130958    0.2304      0.108793    0.174371    0.115698    0.122529    0.117251
0.139486    0.0938475   0.236573    0.133689    0.118372    0.165151    0.112881
0.135901    0.0845106   0.262501    0.127767    0.119785    0.166609    0.102926
0.136622    0.13782     0.119651    0.320522    0.0854596   0.0996346   0.100292
0.139607    0.181654    0.112189    0.259983    0.0920986   0.106649    0.107819
0.151441    0.0929748   0.155358    0.130407    0.208591    0.151803    0.109425
0.132648    0.122881    0.130545    0.126466    0.196319    0.142594    0.148548
0.135545    0.101456    0.177762    0.118609    0.120773    0.253616    0.0922385
0.132612    0.112645    0.111808    0.102153    0.113548    0.327516    0.0997178
0.111618    0.0859541   0.106807    0.116613    0.085918    0.0873931   0.405696
0.107745    0.0936872   0.0877116   0.122336    0.0902212   0.0909265   0.407373
</code></pre>

<p><strong>1.</strong> The <a href=""https://datascience.stackexchange.com/a/44276/57691"">Answerer</a> of my <a href=""https://datascience.stackexchange.com/questions/44266/naive-bayes-all-of-the-elements-in-predict-proba-output-matrix-are-less-than-0"">last question</a>, said that although the predict_proba matrix elements are all less than 0.5, they may be useful in text labeling. But From the above definitions, I concluded that the <code>Accuracy</code> and <code>Precision</code> of the prediction is zero, since all of the predicted values are less than 0.5. Am I correct?</p>

<p><strong>2.</strong> I'm not sure about the Recall and F1 score and how to calculate them. </p>

<p><strong>3.</strong> How can I interpret the matrix and the model's usefulness?</p>

<blockquote>
  <hr>
</blockquote>

<p><strong>Edit 1:</strong></p>

<p>Using <a href=""https://stackoverflow.com/a/47267518/8902456"">this answer</a> I changed my <code>predict_proba</code> matrix above (named in the code as <strong>pred_prob</strong> ) with a shape of (14,7) to a matrix (named <strong>y_pred</strong>) with a shape of (7,1) and then used a one_hot_encoder function to convert it to a confusion matrix (named <strong>y_pred_one_hot</strong>) as follows:</p>

<pre><code>y_pred = np.argmax(pred_prob, axis=1)

def one_hot_encode(actual, n_classes):

    if len(actual.shape) == 1:
        actual2 = np.zeros((actual.shape[0], n_classes))
        for i, val in enumerate(actual):
            actual2[i, val] = 1
        actual = actual2

    return actual

y_pred_one_hot = one_hot_encode(y_pred, n_classes=7)
</code></pre>

<p>Now y_pred_one_hot is:</p>

<pre><code>1   0   0   0   0   0   0
1   0   0   0   0   0   0
0   1   0   0   0   0   0
0   1   0   0   0   0   0
0   0   1   0   0   0   0
0   0   1   0   0   0   0
0   0   0   1   0   0   0
0   0   0   1   0   0   0
0   0   0   0   1   0   0
0   0   0   0   1   0   0
0   0   0   0   0   1   0
0   0   0   0   0   1   0
0   0   0   0   0   0   1
0   0   0   0   0   0   1
</code></pre>

<p>Now is this <code>y_pred_one_hot</code> matrix, the confusion matrix? </p>
","nlp"
"44303","Which neural network to choose for classification from text/speech?","2019-01-20 19:39:23","","2","177","<neural-network><deep-learning><lstm><cnn><nlp>","<p>I am considering two tasks:</p>

<ul>
<li>Dialog Act Classification from Text (e.g. classify to: question; opinion; ...)</li>
<li>Emotion Recognition from Speech (e.g. happy; calm; sad; ...)</li>
</ul>

<p>Which DL model should perform better for such tasks? I am planning to use CNN which should work for both of them, however not sure how well. Can I apply LSTM or some other methods? I used Keras before.</p>

<p>Is it good to apply attention mechanism or some other approaches for these 2 tasks?</p>
","nlp"
"44283","Knowing Feature Importance from Sparse Matrix","2019-01-20 11:48:47","","1","1415","<machine-learning><python><nlp><feature-selection>","<p>I was working with a dataset that had a textual column as well as numerical columns, so I used <code>TFIDF</code> for the textual column and created a sparse matrix, similarly for the numerical features I created a sparse matrix using <code>scipy.sparse.csr_matrix</code> and combined them with the text sparse features.</p>
<p>Then I'm feeding the algorithm to a gradient boosting model and doing the rest of the training and prediction.
However I want to know, is there any way I can plot the feature importance, of this sparse matrix and will be able to know the important feature column names?</p>
","nlp"
"44270","NLP: What are some popular packages for phrase tokenization?","2019-01-20 09:45:14","44290","2","952","<nlp><nltk><tokenization>","<p>I'm trying to tokenize some sentences into phrases. For instance, given</p>

<blockquote>
  <p>I think you're cute and I want to know more about you</p>
</blockquote>

<p>The tokens can be something like</p>

<blockquote>
  <p>I think you're cute</p>
</blockquote>

<p>and</p>

<blockquote>
  <p>I want to know more about you</p>
</blockquote>

<p>Similarly, given input</p>

<blockquote>
  <p>Today was great, but the weather could have been better.</p>
</blockquote>

<p>Tokens:</p>

<blockquote>
  <p>Today was great</p>
</blockquote>

<p>and</p>

<blockquote>
  <p>the weather could have been better</p>
</blockquote>

<p>Can NLTK or similar packages achieve this?</p>

<p>Any advice appreciated.</p>
","nlp"
"44266","Naive bayes, all of the elements in predict_proba output matrix are less than 0.5","2019-01-20 06:51:49","44276","1","751","<machine-learning><scikit-learn><nlp><multiclass-classification><naive-bayes-classifier>","<p>I've created a <code>MultinomialNB</code> classifier model by which I'm trying to label some test texts:</p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn import preprocessing
from sklearn.naive_bayes import MultinomialNB

tfv = TfidfVectorizer(strip_accents='unicode', analyzer='word',token_pattern=r'\w{1,}',
                      use_idf=1,smooth_idf=1,sublinear_tf=1)

# df['text'] is a long string text of words
tfv.fit(df['text'])

lbl_enc = preprocessing.LabelEncoder()

# df['which_subject'] is one of the following 7 subjects: ['Educational', 'Political', 'Sports', 'Tech', 'Social', 'Religions', 'Economics']
y = lbl_enc.fit_transform(df['which_subject'])

xtrain_tfv = tfv.transform(df['text'])

# xtest_tfv has 7 samples
xtest_tfv = tfv.transform(test_df['text'])

clf = MultinomialNB()
clf.fit(xtrain_tfv, y)

y_test_preds = clf.predict_proba(xtest_tfv)
</code></pre>

<p>Now <code>y_test_preds</code> is as follows:</p>

<p><a href=""https://i.sstatic.net/iQ5e7.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/iQ5e7.png"" alt=""enter image description here""></a></p>

<pre><code>0.255328    0.118111    0.129958    0.123368    0.119301    0.131098    0.122836
0.122814    0.265444    0.117637    0.13531     0.116697    0.122812    0.119286
0.131485    0.114459    0.258224    0.122414    0.118132    0.134005    0.12128
0.125075    0.131948    0.122668    0.258655    0.116518    0.119995    0.12514
0.124356    0.116987    0.121706    0.119796    0.266172    0.127231    0.123751
0.132295    0.1192      0.13366     0.119445    0.123186    0.257318    0.114895
0.126779    0.118406    0.123723    0.127393    0.122539    0.117509    0.263652
</code></pre>

<p>As you see, all of the elements are less than 0.5. Does this table show anything? Can I conclude that the classifier is not able to label test text? </p>
","nlp"
"44256","Survey papers on Natural Language Understanding","2019-01-20 00:26:03","","1","295","<machine-learning><nlp><statistics><machine-learning-model>","<p>What are some good survey or review papers on the state of the art of machine learning approaches to grammatical/syntactic structure of natural languages?</p>
","nlp"
"44215","Confidence Score For Trained Sentiment Analyser Model","2019-01-18 18:55:33","44931","2","1189","<machine-learning><scikit-learn><nlp><accuracy><sentiment-analysis>","<p>I have trained a text based sentiment analysis model, using SciKit-learn and custom data. I have the model ready and it works fine in predicting a text to a class (Positive or Negative or Neutral). I have achieved over <em>85%</em> testing accuracy and around <em>80%</em> cross validation accuracy.</p>

<p>But I want to get the <strong>confidence score</strong> attached to each of my prediction to a new example data/text I feed to the classifier. This is just an <strong>extra parameter</strong> I want to show/output apart from just the predicted class.</p>

<p>I have no idea how to achieve this, I shall be really thankful if anyone can provide some helpful insights.</p>
","nlp"
"44205","Word embeddings for Information Retrieval - Document search?","2019-01-18 15:08:49","44212","1","1258","<nlp><text-mining><word2vec><information-retrieval>","<p>What are good ways to find for single sentence (query) the most similiar document (text). I asked myself if word vectors (weighted average of the documents) are suitable to map a single sentence to a whole document?</p>
","nlp"
"44096","Transform single-label data set into multi-label data set","2019-01-16 16:48:10","","2","582","<nlp><data><multilabel-classification>","<p>I received a data set containing a string of text and a label that categorizes that text into one of 50 categories. I'm hoping to build a model that predicts which category a string of text belongs in. </p>

<p>When the dataset was put together, it was assembled under the assumption that each string of text can only belong to one group. In actuality, the text can belong to more than one group simultaneously.</p>

<p>Instead of going back to the drawing board and manually labeling the data again, I want to try and convert this single-label data set into a multi-label data set.</p>

<p>I've tried one method with questionable results. I built a linear regression that predicts each category individually, and appended those predictions to the original data. While this gave me data in the structure I needed, it yielded lackluster results. Most strings of text still only belong to one category (many should belong to multiple), and a good portion weren't assigned any label at all.</p>

<p>It seems that even if I can ""Frankenstein"" this data together, it may not serve as quality training data. I'm curious, is there's any great way of transforming this single-label data into multi-label data?</p>
","nlp"
"44094","Pretrained word embeddings vs model weights","2019-01-16 15:51:52","44122","0","849","<nlp><word-embeddings>","<p>I'm trying to understand the relationship between pretrained word vectors and pretrained  weights when using pretrained word vectors in another neural network.</p>

<p>Are the vectors themselves the weights or do the weights need to be transferred in addition to selecting embedded representations? What's the high level conceptual process in using pretrained vectors? </p>

<p>Thanks! </p>
","nlp"
"44053","Significant Phrases","2019-01-15 17:41:52","","1","32","<nlp>","<p>How do I extract the significant phrases from a web page text (the ones that best represent the topics of the page)?  I want to take a set of pages on the same topic (for example, pages on attractions near Altoona, PA) and get a list of the phrases that have the most information.  Not the most common, but ones that represent sites.</p>
","nlp"
"44008","How can I improve the recall of a certain class in a multiclass-classification result","2019-01-15 04:02:26","","1","1685","<nlp><multiclass-classification><search-engine>","<p>I am working on a multiclass classification which is to assign medical related queries of web search to certain departments of hospital.My classifier is based on the <a href=""https://github.com/facebookresearch/fastText/tree/master/python"" rel=""nofollow noreferrer"">fastText</a>. <p></p>

<p>I found for most conditions, the result is good enough say recall is 0.8 for Nephrology. However, for just one department, Dermatology, the recall is pretty low,like 0.5. Unfortunately, this label has most samples in the test data.<p></p>

<p>How can I improve the recall of one class while maintain the performance of other classes? Will ensembling method work?</p>
","nlp"
"43913","Algorithm for document retrieval in QA system","2019-01-13 11:23:15","","1","130","<machine-learning><nlp><information-retrieval><tfidf>","<p>I am working with question answering and machine reading comprehension system. I want to match questions and documents (around 100,000 docs) in database. I've used tf-idf but it accuracy is about 55% and I need to be at least 80%. Could you give me some advice?</p>
","nlp"
"43835","Duplicate Question finding","2019-01-11 10:59:43","","0","143","<machine-learning><deep-learning><classification><nlp>","<p>I have around 40K question and answers. How can I build a machine learning model so that if any new question comes it has to detect as duplicate or not? </p>
","nlp"
"43815","test accuracy of text classification is too less","2019-01-11 04:45:48","","1","941","<nlp><svm><accuracy><naive-bayes-classifier>","<p>I have a data set of movies and their subtitles. My task is to classify them based on their ratings - <strong>[R, NR, PG, PG-13, G]</strong>. </p>

<p>I have 13 examples for each class.
I preprocessed the subtitles in the following way:</p>

<ul>
<li>I used word puns tokenizer to tokenize subtitles.</li>
<li>Removed stop words and punctuation.</li>
<li>Performed stemming.</li>
<li>Vectorized the subtitles using TF-IDF vectorizer.</li>
</ul>

<p>The accuracy that I am getting using:</p>

<p>1) svm :
train accuracy is 1.0 and test accuracy is .17.</p>

<p>2) naive bayes:
training accuracy is 0.5 and test accuracy is .23.</p>

<p>I have the following questions:</p>

<p>1)Why is my accuracy so low and what can I do to improve the accuracy?</p>

<p>2)Will more training data help?</p>

<p>3)Should I perform feature selection?</p>

<p>4)What other classification algorithms can I use to improve the accuracy?</p>
","nlp"
"43721","How to Combine tfidf with LSTM in keras?","2019-01-09 13:17:54","","1","4227","<keras><nlp><lda><tfidf>","<p>I am classifying emails as spam or ham using LSTM and some of its modified form(by adding constitutional layer at the end). For converting documents into vectors I am using <code>keras.text_to_sequences</code> function.</p>
<p>But now I want to use TfIdf with the LSTM can anyone tell me or share the code how to do it. Please also guide me if it is possible and good approach or not.</p>
<p>If you are wondering why I would like to do this there are two reasons:</p>
<ol>
<li>I want to see if this improves the results.</li>
<li>Second, my professor asked me to perform Latent Dirichlet Allocation, and use same features for both of the tasks.</li>
</ol>
","nlp"
"43549","ModuleNotFoundError: No module named 'en_core_web_sm'","2019-01-06 07:52:27","","2","6123","<nlp>","<p>I am trying to import en_core_web_sm in my jupyter notebook.</p>

<pre><code>import en_core_web_sm
nlp = en_core_web_sm.load()
</code></pre>

<p>error:</p>

<pre><code>ModuleNotFoundError: No module named 'en_core_web_sm'
</code></pre>
","nlp"
"43465","Training on a subset of data for few epochs and then proceeding to the next subset for few epochs and so on?","2019-01-04 06:56:12","","0","531","<neural-network><deep-learning><nlp><dataset>","<p>My training data is very huge and it's impossible to load all of it at once even into main memory. So I'm loading a few blocks (subset) of data and training till convergence, then proceeding to next subset and training till convergence and so on. Is it the right approach ? </p>

<p>The model performance kind of remains the same even when training on a new subset of data. </p>

<p>Is this method fundamentally wrong, why ? </p>
","nlp"
"43413","What is the position embedding code?","2019-01-03 03:08:26","43601","0","488","<nlp><embeddings><deep-learning>","<p><a href=""https://github.com/google-research/bert/blob/master/modeling.py#L491-L520"" rel=""nofollow noreferrer"">https://github.com/google-research/bert/blob/master/modeling.py#L491-L520</a></p>

<p>The code of BERT is one of the implementation. But it is not what I need.</p>

<p>I search a lot but can not judge.</p>

<p>But where is the code which seems uses <code>cos</code> and <code>sin</code>?</p>
","nlp"
"43396","Training an LSTM on two merged writing styles (say Shakespeare and Frost)","2019-01-02 12:54:11","","1","34","<deep-learning><nlp><lstm><text-generation>","<p>I was trying to develop intuitions on how two writing styles can be merged (if at all they can be merged) into a single LSTM and then get meaningful results from the same. </p>

<p>Can anybody provide me with some insights on how to go about this? I wish to see if I can go about merging Shakespeare and Frost into one model and then get the generated text be like <strong><em>picking up nature themes from Frost and mixing in them a flavour of Shakespeare</em></strong>?</p>

<p>How do you all feel about this?</p>
","nlp"
"43363","How to detect when the ""bibliography"" of a paper has began?","2019-01-01 03:53:02","","0","69","<machine-learning><nlp>","<p>I'm working on project that involves reading papers from ArXiV to look for particular patterns (It gets complicated but it basically has to do with common phrases and expressions) in their text.</p>

<p>My process can be abstractly described as:</p>

<p><span class=""math-container"">$$ PDF \rightarrow ""\text{PDF as a string}"" \rightarrow \text{Pattern_Finding_ Method}(""\text{PDF as a string}"") $$</span> </p>

<p>Now unfortunately, the ""bibliography"" or references section of many papers cause the Pattern_Finding_Method to break, because they just happen to carry a lot of the behavior we are looking for (but they are not of interest). </p>

<p>As a human it's quite easy for me just manually read and see where the ""references"" begin and then I can trim that part of the pdf string but at scale this is not practical. Moreover paper writers don't have consistent ways of declaring when their ""references"" ""bibliography"" or ""acknowledgements"" begin. </p>

<p>So it seems natural to view this as an ML/AI problem, where I have a string, I have a loose notion of what constitute the ""references"" of the string which I can provide training data for (I have the pdf as a string and I can list a character index on the string the references begin) </p>

<p>Now given training data I need to come up with some kind of model that can effectively learn how to detect references on its own.</p>

<p>This is where I get stuck. The data problem i'm dealing with is a highly semantic problem (it's particular organizations of words and their underlying meaning and patterns to how those words are displayed that is giving me cues as to when the reference have began), but my knowledge of learning algorithms is limited to mostly geometric data (SVMs), or at least highly continuous data (Neural Network Models), and then in the case of NLP, my understanding is at best a recipe book of very goal specific algorithms: (ex: TF-IDF for document classification).</p>

<p>I don't know how to bridge the gap from my understanding to creating a specialized model for the problem at hand; that I have intuitive reason to believe will work.</p>

<h2>Formal Problem Statement:</h2>

<p>Given a collection of large strings, each attached with an integer <span class=""math-container"">$i$</span> indicating where the references begin, determine a model that can reliably detect when references begin on new text. </p>
","nlp"
"43278","How to train millions of doc2vec embeddings using GPU?","2018-12-29 11:27:29","","3","539","<deep-learning><nlp><word-embeddings>","<p>I am trying to train a doc2vec based on user browsing history (urls tagged to user_id). I use chainer deep learning framework. </p>

<p>There are more than 20 millions (user_id and urls) of embeddings to initialize which doesn’t fit in a GPU internal memory (maximum available 12 GB). Training on CPU is very slow.</p>

<p>I am giving an attempt using code written in chainer given <a href=""https://github.com/monthly-hack/chainer-doc2vec"" rel=""nofollow noreferrer"">here</a></p>

<p>Please advise options to try if any.</p>
","nlp"
"43203","Improving automated ingestion system using Machine Learning and/or NLP","2018-12-27 11:34:01","","1","61","<machine-learning><python><nlp><supervised-learning><semi-supervised-learning>","<p>I'm working on a automated ingestion system which takes a PDF or doc file or a URL. It then parses the file and get me the required text in a json format but there are some error and there are few things that I want to capture differently or add.</p>

<p>As to give you an idea what I really want to do is: Let's say I've a URL of Step by Step Instructions on doing something. I want to scrape/parse this web-page and convert these instructions into smaller and meaningful instruction, so Amazon Alexa or any other voice assistant can speak and give the user instruction through voice.</p>

<p>The output I'm getting by scrapping this webpage is not in proper format. Till now I've been manually improving by adding the desired words and punctuation. But the number of these URL will increase over time so it is not possible to give so much time on improving the instructions by using a human to add the missing words or break a sentence into two meaningful instructions. So how can I automate this part using ML/DL so that this system can be scaled.</p>

<p>So can you guide me on how can I leverage Machine Learning to improve this ingestion system?</p>

<p>Thanks in advance. Let me know if you have any questions regarding this or If I've not made something clear which is required to answer it correctly!!!</p>
","nlp"
"43117","I have data of some movies and their subtitles.I want to classify them based on their ratings","2018-12-25 07:04:20","","2","40","<classification><nlp><svm><pca><lda>","<p>I will convert the subtitles into vectors and use them as features to classify the movies into different categories based on their ratings.The problem that I am facing is my feature vector is much much larger compared to the number of examples I have. I would like to know what should the size of my tailing data set be to use LDA,PCA,SVM and naive Bayes.Would 10 movies per category be sufficient?</p>
","nlp"
"43018","Tagging Unix/Non-Unix logs using NLP","2018-12-21 18:39:38","","1","326","<machine-learning><deep-learning><nlp><regex>","<p>I have a set of unstructured data consisting command output logs for different operating systems like Unix, Windows, etc.</p>

<p>For example:</p>

<blockquote>
  <p>Releasing version 0.0.1 for Stackoverflow, on 01/01/2019. The coverage is 99% and build is passed.</p>
</blockquote>

<p>(This is just an example, and not related to actual use case). This output is different for different operating systems. I want to perform tagging on this data</p>

<p>For the case above, the output should be:</p>

<blockquote>
  <p>Releasing version VERSION_NUMBER for PRODUCT_NAME, on releaseDate. The TEST_TYPE is TEST_VALUE and TEST_TYPE is TEST_VALUE</p>
</blockquote>

<p>Here, some words are replaced with their corresponding sample tags.</p>

<p>I have studied techniques like POS tagging, NER, LSTM, but I don't know which one is suitable for this particular problem. How can I gather data from raw output and how to apply those techniques here.</p>

<p>Thanks to everyone who is willing to help me with this.</p>
","nlp"
"42985","Why and how BERT can learn different attentions for each head?","2018-12-21 03:34:42","43095","1","271","<deep-learning><nlp><transfer-learning><multitask-learning><attention-mechanism>","<p><a href=""https://towardsdatascience.com/deconstructing-bert-distilling-6-patterns-from-100-million-parameters-b49113672f77"" rel=""nofollow noreferrer"">https://towardsdatascience.com/deconstructing-bert-distilling-6-patterns-from-100-million-parameters-b49113672f77</a></p>

<p>I read the blog above. It visualizes that different color/head has different attention of words. </p>

<p>Based on my understanding, the code implementation of each head is almost the same.</p>
","nlp"
"42680","Understanding word2vec vectors representation","2018-12-15 17:53:05","","0","164","<nlp><word2vec><gensim>","<p>I'm trying to obtain the <code>word2vec</code> representation of few words using <code>gensim</code>.</p>

<p>At present, this is the model that I have:</p>

<pre><code>model = Word2Vec(desc, size=1, seed=7, window=1, iter=1000)
</code></pre>

<p>And the current output is</p>

<pre><code>Ansible automate 0.37919244170188904
Docker container -0.05218552052974701
Ansible automation 0.39010459184646606
Automate Ansible 0.1009570062160492
Contain Docker 0.41532352566719055
Dock contain -0.38013115525245667
Dock container -0.17561538517475128
</code></pre>

<p>I'm wondering why <code>Contain Docker</code> is closer to <code>Ansible automate</code> and <code>Ansible automation</code> than it is from the other terms which contain the word <code>Docker</code>, <code>Container</code> etc. </p>

<p>How can I train the word2vec model to get this sorted out?</p>
","nlp"
"42513","Is it possible to use word2Vec to derive hyperonymy (hyponymy or ISA relation)?","2018-12-12 17:01:59","","2","59","<nlp><word2vec>","<p>It's easy to have hyperonymy in WordNet, e.g. to know that ""tea"" is a case of ""beverage"". Is it possible to use word2Vec in this way?</p>
","nlp"
"42452","How to create clusters based on sentence similarity?","2018-12-11 12:24:34","","2","2130","<machine-learning><data-mining><nlp><clustering><unsupervised-learning>","<p>I have data which looks like following. 
Data is a group of sentences which are similar, but have few unique words in between like TABLEA, TABLEB etc.</p>

<pre><code>java.sql.SQLException: [Teradata Database] [TeraJDBC 15.10.00.22] [Error 3523] [SQLState 42000] The user does not have SELECT access to TABLEA
java.sql.SQLException: [Teradata Database] [TeraJDBC 15.10.00.22] [Error 3523] [SQLState 42000] The user does not have SELECT access to TABLEC
java.sql.SQLException: [Teradata Database] [TeraJDBC 15.10.00.22] [Error 3523] [SQLState 42000] The user does not have SELECT access to TABLEB
Dataframe read is null 
Dataframe read is null     
java.sql.SQLException: [Teradata Database] [TeraJDBC 15.10.00.22] [Error 3807] [SQLState 42S02] Object Y  does not exist.
java.sql.SQLException: [Teradata Database] [TeraJDBC 15.10.00.22] [Error 3807] [SQLState 42S02] Object Z  does not exist.
java.sql.SQLException: [Teradata Database] [TeraJDBC 15.10.00.22] [Error 2652] [SQLState HY000] Operation not allowed: TABLEK is being Loaded.
java.sql.SQLException: [Teradata Database] [TeraJDBC 15.10.00.22] [Error 9804] [SQLState HY000] Response Row size or Constant Row size overflow.
java.sql.SQLException: [Teradata JDBC Driver] [TeraJDBC 15.10.00.22] [Error 1000] [SQLState 08S01] Login failure for Connection to xxx.xx.xx.xx Tue Dec 04 02:49:47 MST 2018 
</code></pre>

<p><strong>Problem Statement:</strong> 
I want to group/cluster the data and provide a unique number to each group/cluster.</p>

<p><strong>Assumptions</strong>:</p>

<ol>
<li>The groups/cluster should be formed based on the similarity. Similar sentences should be grouped in one cluster</li>
<li>This should be unsupervised learning. If in future, some new sentence comes which is very less similar to existing cluster, it should create a new group/cluster.</li>
<li>The sentences can be of any length</li>
<li>The common words between sentences can appear
anywhere - starting of string, in between, in the end, or so</li>
<li>The sequence of the words matter</li>
</ol>

<p><strong>Output:</strong></p>

<p>The outcome should be a dimension table for category like below
<a href=""https://i.sstatic.net/Kkkl3.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Kkkl3.png"" alt=""enter image description here""></a></p>

<p>Although I do get the problem statement in its abstract form, I do not know a concrete way to do this.</p>

<p>So far I have read about text clustering using various algorithms like cosine similarity and etc, but I am not sure if that will suffice this problem statement.
One of the major problem here is, it is unsupervised. If there are any new sentences whose similarity is very less then it should create a new group.</p>

<p>The bigger picture goes like this</p>

<ol>
<li><p>Get list of all unclassified/uncategorised(I am using both interchangeably here since I am not sure which one it falls into) statements</p></li>
<li><p>Check in the dimension table, by matching using some similarity threshold(not clear on this). </p></li>
<li><p>If the similarity is matching above a threshold, then do nothing</p></li>
<li><p>If similarity is less, then create a new group in the dimension table with the Description column which has common words.</p></li>
</ol>

<p>I have yet to identify what is the best approach to solve this problem.Please recommend some algorithm or approach to solve this problem.</p>
","nlp"
"42399","How to improve accuracy of Named entity recognition (NER) tagger on local data?","2018-12-10 12:21:28","","1","2035","<nlp><named-entity-recognition>","<p>I am using NER from spacy. Its giving incorrect results for few words. Its trained on general dataset. How can I customize on my local data. </p>

<p>For example,</p>

<pre><code>Person -  {'Mike Miller', 'Miller', 'Infantino', 'Gianni Infantino'}
Location -  {'England', 'UK', 'Europe', 'Telegraph'}
</code></pre>

<p>Here, ""Telegraph"" is incorrectly is assigned to location.</p>
","nlp"
"42287","Possible reasons for word2vec learning context words as most similar rather than words in similar contexts","2018-12-07 13:46:42","42605","1","894","<neural-network><deep-learning><nlp><word2vec><word-embeddings>","<p>I am observing my word2vec model learning context words as most similar rather than words in similar contexts. I don't understand why it (word2vec in general, not my model in particular) can behave like that and would like to know why.</p>

<p>I have implemented the original word2vec in keras. I chose the variant with the dot product layer rather than the hierarchical softmax and trained the model on a Wikipedia dump that I split into 5-grams. For each word I construct 8 pairs with a binary target label as training items. I use the 4 context words with the label True and choose 4 random words that are not one of the context words with the label 0.</p>

<p>Intuitively this model should learns similar representations for words in similar context, because it modifies the representation of these words in a <em>similar</em> manner, as it optimizes them independently with <em>similar</em> context words.
So these similar words are not directly made similar but rather indirectly, as they are subject to <em>similar</em> nudges due to their <em>similar</em> contexts.</p>

<p>The model is this:</p>

<pre><code>input_target = Input((1,))
input_context = Input((1,))

embedding = Embedding(vocab_size, vector_dim, input_length=1, name='embedding')

target = embedding(input_target)
target = Reshape((vector_dim, 1))(target)
context = embedding(input_context)
context = Reshape((vector_dim, 1))(context)

dot_product = Dot(axes=1)([target, context])
dot_product = Reshape((1,))(dot_product)

output = Dense(1, activation='sigmoid')(dot_product)

model = Model(inputs=[input_target, input_context], outputs=output)
model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])
</code></pre>

<p>However, what I observe from training this model is, that when I rank the most similar words given a certain word, that I get words that appear in the context of that word, rather than words that appear in similar context, as the most similar words.</p>

<p>For example:</p>

<pre><code>words most similar to ""plant"":

rank |   word

0    |   amount
1    |   surface
2    |   electron
3    |   mass
4    |   plant # also: Why is plant not most similar to plant? How can that happen?
5    |   fluid
6    |   air
7    |   metal
8    |   molecule
9    |   cell
10   |   electric
11   |   per
12   |   oxygen
13   |   demonstrate
14   |   smooth
</code></pre>

<p>To me that looks a lot more like words that appear in the context of <em>plant</em> rather than words that appear in similar context as it.</p>

<p>The function to compute these is:</p>

<pre><code>def get_most_similar(word_vector, embeddings, n=15):
    """"""
    find the `n` words that are most similar to `word_vector` in `embeddings`
    measured by their cosine similarity
    """""" 
    v = word_vector
    m = embeddings
    cosines = (np.dot(v, m.T))/(np.linalg.norm(m.T, axis=0)*np.linalg.norm(v))
    ranked_by_similarity = np.argpartition(cosines, -n)[-n:]
    return reversed(ranked_by_similarity)
</code></pre>

<p>Is there a simple reason for that? </p>

<p>I have the following other parameters:</p>

<pre><code>word vector size: 300
batch size: 128
vocabulary size: 169161 (distinct lemmas)
training sample count: 27793586 (5-grams, overlapping within sentences)
</code></pre>

<p>I trained the model for 1 epoch and did only observe marginal further convergence into the second epoch.</p>
","nlp"
"42237","Continous bag of words claimed to be unsupervised, how is it working?","2018-12-06 15:11:33","42242","1","596","<machine-learning><neural-network><deep-learning><word2vec><nlp>","<p>I'm following these two lectures on CBOW and skip-gram word2vec models. The first is lec 12 and the next lec 13 of a deep learning series </p>

<p><a href=""https://www.youtube.com/watch?v=syWB-YMYZvI"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=syWB-YMYZvI</a></p>

<p><a href=""https://www.youtube.com/watch?v=GMCwS7tS5ZM&amp;t=548s"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=GMCwS7tS5ZM&amp;t=548s</a></p>

<p>Up to about 17 mins into the second video the lecturer says that the approach is unsupervised for CBOW as there are no labels? How can you learn a NN with no labels? This completely confused me as why are we not comparing our softmax probability vector to an actual set of outputs so that we can adjust the v_c and v_w weights accordingly. His liklihood function seems to only be concerned with the parameters v_c and v_w (and completely devoid of some kind of target label) which is bonkers to me, because could i not not just make them whatever i want? In addition how are you learning relationships between pairs of actual words, if actual target variables are not guiding you towards correct labels? Could someone please explain what is going on under the hood as i really want to understand this approach. Perhaps target labels are being considered and i've not noticed it? </p>

<p>Most log likelihood estimates with a window of size <span class=""math-container"">$m$</span> look something like the following <span class=""math-container"">$-\mathrm{log} \prod_{j=0,j \neq m}^{2m} \frac{e^{u^T_{c-m+j}v_c}}{\sum_{k=1}^{|v|} e^{u_k^Tv_c}} $</span></p>

<p>To my knowledge a likelihood function should involve data from actual observations not purely parameters, which are to be estimate, can someone explain this also?</p>

<p>Note i follow it more from 17 mins onwards as he talks about pairs (w,c) in D or D' respectively. I appreciate any help!</p>
","nlp"
"42219","Document similarity matching between Doc2Vec documents","2018-12-06 11:33:05","","1","1294","<python><nlp><word2vec>","<p>I am creating a Doc2Vec model out of hundreds of PDF documents. </p>

<p>I have 17 documents that are part of this Doc2Vec that I want to use to check similarity with other documents in the Doc2Vec model. </p>

<p>For instance I want to something like: <code>model.similarity(tag5, tag30)</code></p>

<p>Can this be done?</p>
","nlp"
"42157","Updating Google News Word2vec Word Embedding?","2018-12-05 10:13:41","42172","2","936","<word2vec><word-embeddings><nlp><gensim>","<p>Is it possible to update the Google News Word Embedding with a custom text dataset (text data pertaining to a particular domain) ? </p>

<p><a href=""https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit"" rel=""nofollow noreferrer"">Google News Word2Vec - Word Embedding</a> clearly helps us to come with a robust set of word vectors but it unfortunately cannot be used for most business case. 
For example:</p>

<pre><code>embeddings.most_similar('python')

[('pythons', 0.6688377857208252),
 ('Burmese_python', 0.6680365204811096),
 ('snake', 0.6606293320655823),
 ('crocodile', 0.6591362953186035),
 ('boa_constrictor', 0.6443518996238708),
 ('alligator', 0.6421656608581543),
 ('reptile', 0.6387744545936584),
 ('albino_python', 0.6158879995346069),
 ('croc', 0.6083582639694214),
 ('lizard', 0.601341724395752)]
</code></pre>

<p>This output is clearly not what we want. We could create a custom word2vec model using gensim library for this business case but it would not be exhaustive (vocabulary will be comparatively less). What is best practice in such cases ? Is is possible to update the weights of a pretrained Word Embedding model so that the word embedding also learns from domain text data?</p>
","nlp"
"42139","Why ELMo's word embedding can represent the word better than glove?","2018-12-05 03:09:24","42539","4","348","<deep-learning><nlp><word2vec><word-embeddings><representation>","<p>I have read the code of <a href=""https://github.com/allenai/bilm-tf"" rel=""nofollow noreferrer"">ELMo</a>.<br />
Based on my understanding, ELMo first init an word embedding matrix <code>A</code> for all the word and then add LSTM <code>B</code>, at end use the LSTM <code>B</code>'s outputs to predict each word's next word.</p>
<p>I am wondering why we can input each word in the vocab and get the final word representation from the word embedding matrix <code>A</code> after training.</p>
<p>It seems that we lost the information of LSTM <code>B</code>.</p>
<p>Why the embedding can contains the information we want in the language model.</p>
<p>Why the training process can inject the information for a good word representation into the word embedding matrix <code>A</code>?</p>
","nlp"
"41889","What are common ways to identify subject and object from a question?","2018-11-30 09:08:53","","1","88","<machine-learning><nlp>","<p>I am looking for a way to extract the potential  subject and object from a question in <strong>French</strong>.</p>

<p>For the moment, I am building some handmade rules.
Alternatively, I started to think of using an already trained model for the task.
I wanted to use StanfordNLP parser but unfortunately it is not free for commercial use.</p>

<p>So, here comes the questions:</p>

<ol>
<li><p>Are they ML models used for this task?</p></li>
<li><p>Are they labeled data sets (in french) which can help create ML models used for this task?</p></li>
</ol>

<p>[Edit] I am looking mainly for open source tools/ libraries.</p>
","nlp"
"41878","Skip-thought models applied to phrases instead of sentences","2018-11-29 21:32:09","","1","111","<neural-network><nlp><word2vec><word-embeddings><ngrams>","<p>My goal is to build a statistical model with domain specific phrase embeddings.</p>

<p>To do this, I am doing research on how to build a model using <a href=""https://medium.com/huggingface/universal-word-sentence-embeddings-ce48ddc8fc3a"" rel=""nofollow noreferrer"">skip-thought vectors</a>, where instead of using sentence embeddings, I am more interested in complete phrases as embeddings. The goal is to be able to create this model and then use some kind of transfer learning to create phrase embeddings from the unsupervised task of skip-thought modeling.</p>

<p>One of the biggest issues we have is that while our corpus is fairly large, it is only 23,000 free text notes and thus, approaching maybe a 11 million words at best. Thus, a standard word embedding model would have issues wrt data sparsity. Also, our entities of interest are basically multi-token phrases. This is why we are looking at building a model using skip-thought vectors, where instead of sentence embeddings, the unit of interest is a phrase, and then using transfer learning to create phrase embeddings. </p>

<p>I am wondering if anyone is familiar with any relevant research on this, and how I would go about training a corpus of relevant documents to create a statistical model of phrase embeddings using skip-thoughts. Would I first create a word2vec model and then use that for input to further train the model? Is this similar to creating a word2vec skip-gram model wwithout worrying about sentence boundaries? Any recommended packages out there? I have not had any luck pinning down specifics on how to do this. </p>

<p>NB: One problem with our text, is that not all sentences in the text are clearly delineated as a sentence. So, I'm thinking this might be beneficial, in that I could have a boat load of skip-grams as phrases that cross sentences boundaries. </p>
","nlp"
"41726","Is this a good approach to classify tickets which contains description and logs?","2018-11-27 05:07:46","","0","79","<neural-network><nlp><tfidf>","<p>I want to classify a dataset of support tickets which mostly contain text in the description field and sometimes server logs in a separate field. </p>

<p>The log field is not always there but when it's present, it's a good indicator of the target class of the ticket.</p>

<p>I have created a CNN based classifier which can classify the tickets based on the log field, and a SVM clf with TFIDF based features for the description field. </p>

<p>I am thinking of adding the output probabilities of the CNN classifier in TFIDF based  SVM classifier to combine the models as a feature column. </p>

<p>Is there a better way to combine these models?</p>

<p>Is there a better way to approach this problem, without having two separate models?</p>
","nlp"
"41637","Handle 50,000 classes in OneVsRestClassifier","2018-11-24 04:52:48","41679","0","392","<python><scikit-learn><nlp><multilabel-classification>","<p>I'm new to data science and NLP. I'm trying to solve a problem that is having 1 million rows and some 50,000 distinct classes. The dataset has some text column as a predictor and the other one is the multilabel responses. I have been using tfidf to represent the text fields and MultiLabelBinarizer to transform the labels. But MultiLabelBinarizer is giving MemoryError.</p>

<p>And there is no way I can pass the legacy multi-label data representation using a sequence of sequences as it seems no more to be supported in the sklearn package. So, what should be my approach?</p>

<p>Any help is appreciated. Thanks in advance.</p>
","nlp"
"41494","Is there a way to automatically generate a string given an input of another string?","2018-11-21 00:05:30","","1","30","<machine-learning><python><nlp>","<p>I am fairly new to data science in general, and I'm wondering if it is possible to use machine learning/natural language processing to generate definitions for various terms.  My team has recently been tasked with the delivery of a business glossary and using ML/NLP has been an idea that my manager wants to pursue.</p>

<p>I'm wondering if, for example, it is possible to write an algorithm that takes a 1-4 word string as an input and generates a 'good' definition, with good being defined by a statement of what the term is/means without being circular (e.g. the definition for 'Account' should be simply be 'Account' or 'An account').</p>

<p>I've been exploring various Python libraries (nltk, gensim, scikit-learn) and reading tutorials on how this could possibly be done, but as you can tell I'm a novice.  I've gone through the <a href=""https://www.datacamp.com/courses/natural-language-processing-fundamentals-in-python"" rel=""nofollow noreferrer"">Natural Language Processing Fundamentals in Python</a> course on Datacamp which is very helpful in building a basic classifier algorithm but that's all I've really been able to do.</p>

<p>My first idea is that I could write something which consumes various corpora.  I'm in Finance so my thoughts on the type of corpora would be the <a href=""https://spec.edmcouncil.org/static/glossary/"" rel=""nofollow noreferrer"">FIBO Glossary</a> and the various business glossaries which already exist in-house.</p>

<p>So basically, I'm just wondering if</p>

<ul>
<li>What I'm trying to do is even possible</li>
<li>If it is, if anyone here knows of a or use case where this has been implemented or even what the name of such an algorithm, function is, as that would make looking for resources easier.</li>
</ul>

<p>Thank you for your help.</p>
","nlp"
"41385","On a multi lingual sentiment corpus","2018-11-18 17:41:42","","7","435","<machine-learning><python><nlp><dataset><sentiment-analysis>","<p>I am looking to compile a sentiment corpus for news articles in multiple languages (~100k per lang. for a machine learning experiment) where each article is labeled positive, neutral, or negative. I have searched high and low but could not find anything like this available. I already have the news articles in each language.</p>

<p>My question to the community is how would you achieve this as accurately as possible?</p>

<p>I was first looking at <a href=""https://www.mturk.com/"" rel=""nofollow noreferrer"">Mechanical Turk</a>, where you can hire people to  label each article manually for you. And this may be the best way forward <em>but expensive</em>.
<a href=""https://i.sstatic.net/5izMO.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/5izMO.png"" alt=""enter image description here""></a></p>

<p>Next, I thought about all of those existing popular libraries (some of whom have already used Mechanical Turk) that do sentiment analysis (<a href=""https://github.com/fnielsen/afinn"" rel=""nofollow noreferrer"">AFINN</a>, <a href=""https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html"" rel=""nofollow noreferrer"">Bing Liu</a>, <a href=""http://mpqa.cs.pitt.edu/lexicons/subj_lexicon/"" rel=""nofollow noreferrer"">MPQA</a>, <a href=""https://github.com/cjhutto/vaderSentiment"" rel=""nofollow noreferrer"">VADER</a>, <a href=""https://github.com/sloria/TextBlob"" rel=""nofollow noreferrer"">TextBlob</a>, etc.)</p>

<ol>
<li>Sentiment Idea</li>
</ol>

<p>My current idea is that I run each news article across a few of these libraries (for example AFINN, then TextBlob, then VADER) and for those articles who show positive, negative, neutral unanimously though all three libs are accepted into the corpus. Does that seem like a fairly strong and reasonable verification process?</p>

<ol start=""2"">
<li>Language Idea</li>
</ol>

<p>The next issue pertains to language itself. The 3 lib pipeline above can be executed in English with no issue. However these libraries do not uniformity support many other languages (Spanish, German, Chinese, Arabic, French, Portuguese, etc.) I was thinking about doing what VADER suggests and taking the news stories in non-english languages and sending them though Google Translation API to get them into English and then send them through the existing 3 lib pipeline above. I do realize there will be a loss in semantics for many articles. However, my <em>hope</em> is that enough articles will translate well enough that some pass through the 3 lib pipeline. </p>

<p>I am aware that translating and sending news articles through this triple blind sentiment pipe may take a 100k corpus and yield 10k results. I am fine with that. The accuracy and then price is my concern. I can easily acquire more data.  </p>

<p>What would you do that may be a more accurate way of achieving a sentiment corpus of news articles? Is there an existing best used practice for assembling a corpus like this?</p>
","nlp"
"41353","What is the best way to use word2vec for bilingual text similarity?","2018-11-17 13:26:50","","1","912","<nlp><text-mining><word2vec>","<p>I face a problem where I need to compute similarities over bilingual (English and French) texts. The ""database"" looks like this:</p>

<pre><code>+-+-+-+
| |F|E|
+-+-+-+
|1|X|X|
+-+-+-+
|2| |X|
+-+-+-+
|3|X| |
+-+-+-+
|4|X| |
+-+-+-+
|5| |X|
+-+-+-+
|6|X|X|
+-+-+-+
|7|X| |
+-+-+-+
</code></pre>

<p>which means that I have English and French texts (variable long single sentences) for each ""item"" with either in both version (in this case the versions are loose translations of each other) or only in one language.</p>

<p>The task is to find the closest item ID for any incoming new sentence irrespective the actual language of either of the sentence in the ""database"" or of the incoming sentence (that is, the matching sentence in the ""database"" needn't necessarily be in the same language as the incoming sentence as long as the meaning is the closest). I hope this goal explanation is clear.</p>

<p>Originally I planned to build a word2vec from scratch for both languages (the vocabulary is quite specific so I would have preferred my own word2vec) and find similarities only for the corresponding language for each new sentence but this would omit all candidates from the items where the corresponding language sentences are missing.</p>

<p>So I wonder if generating a common word2vec encoding for the combined corpus is viable (the word2vec method itself being language agnostic) but I cannot figure out if such a solution would be superior.</p>

<p>Additionally, the number of the sentences is not very large (about 10.000) maybe word2vec generation from scratch is not the best idea on one hand, but there are really specific terms in the corpora on the other hand.</p>
","nlp"
"41346","What are common data augmentation techniques for nlp in general and for chatbot use specifically (in rasa)?","2018-11-17 08:23:50","","1","305","<nlp>","<p>I am trying to build a chatbot using <a href=""https://rasa.com/docs/"" rel=""nofollow noreferrer"">RASA</a>
The process to build the chatbot can summarized in two step:</p>

<ol>
<li><p>First identify the intent and the entities in the user's question </p></li>
<li><p>Second, based on a history of intents and their corresponding actions, augment the history of the data.</p></li>
<li><p>Use the augmented dataset to train a model (which can be an ML model) in order to use it to identify the actions from the intents.</p></li>
</ol>

<p>My problem is to understand how the data augmentation is done in this case. So far and after reading the code for a whole afternoon, I concluded that one of the first steps in the process is the generation of a graph which represents the dataset.</p>
","nlp"
"41315","Combine multiple features for text classification","2018-11-16 14:54:28","","2","1044","<machine-learning><python><nlp><feature-selection>","<p>Recently I started reading more about NLP and following tutorials in Python in order to learn more about the subject. I'm trying to make my own classification algorithm (the text sends a positive/negative message) and I have already preprocessed the text and tested it using different approaches in order to find the one that works the best in my case. The best results were achieved by using Random Forest classifier with BOW - unigrams, but the accuracy can still be further improved (at the moment is somewhere around 0.73). What I want to do next is to create a Bag of emojis, try different sentiment analysis techniques (Vader, SentiStrength), count the number of negative/positive words in each of the  phrases. My question is how can I take into account all these different features? Should I create a table with the results given by all of these features and calculate the mean? For example (0 - classified as positive):</p>

<pre><code>Classifier   Sentiment analysis   Bag of emojis               Pos/Neg words
  0 or 1           0 or 1             0 or 1        if numberPos &gt; numberNeg consider positive
</code></pre>

<p>I'm kind of new at this and I was wondering if anyone could please guide me in the right direction?</p>
","nlp"
"41285","any efficient way to find surrounding adjective/verbs with respect to the target phrase in python [updated]?","2018-11-15 21:34:54","42691","6","9084","<python><nlp><sentiment-analysis>","<p>I am doing sentiment analysis on given documents. My goal is to find out the closest or surrounding <strong>adjective words with respect to the target phrase</strong> in my sentences. I do have an idea how to extract surrounding words with respect to target phrases. But how do I find out <strong>relatively close or closest</strong> adjective or <code>NNP</code> or <code>VBN</code> or other POS tag with respect to target phrase ?</p>

<p>Here is the sketch idea of how I may get surrounding words with respect to my target phrase.</p>

<pre><code>sentence_List = {
    ""Obviously one of the most important features of any computer is the human interface."", 
    ""Good for everyday computing and web browsing."",
    ""My problem was with DELL Customer Service"", 
    ""I play a lot of casual games online[comma] and the touchpad is very responsive""
}

target_phraseList = {
    ""human interface"",
    ""everyday computing"",
    ""DELL Customer Service"",
    ""touchpad""
}
</code></pre>

<p>Note that my original dataset was given as DataFrame where the list of the sentences and respective target phrases were given. Here I just simulated data as follows:</p>

<pre><code>import pandas as pd
df=pd.Series(sentence_List, target_phraseList)
df=pd.DataFrame(df)
</code></pre>

<p>Here, I tokenize the sentence as follow:</p>

<pre><code>from nltk.tokenize import word_tokenize
tokenized_sents = [word_tokenize(i) for i in sentence_List]
tokenized=[i for i in tokenized_sents]
</code></pre>

<p>Then I try to find out surrounding words with respect to my target phrases by using this <a href=""https://stackoverflow.com/questions/17645701/extract-words-surrounding-a-search-word"">loot at here</a>. However, I want to find out relatively closer or closest <code>adjective</code>, or <code>verbs</code> or <code>VBN</code> with respect to my target phrase.</p>

<p>How can I make this happen? Any idea to get this done? Thanks</p>
","nlp"
"41239","How to measure word similarity using wordnet for the information theoretic definition as detailed in Resnik 1995?","2018-11-14 21:19:40","","1","33","<nlp><information-theory>","<p><a href=""https://arxiv.org/pdf/cmp-lg/9511007.pdf"" rel=""nofollow noreferrer"">Resnik 1995</a> equation 3 uses count(n) to define P(c). What is count(n)? </p>

<p>Any solved example would be appreciated.</p>
","nlp"
"41192","Grouping domain specific words/phrases with same meaning","2018-11-14 05:31:27","","2","684","<classification><nlp><text-mining><feature-extraction><named-entity-recognition>","<p>I am looking at NLP methods to group together words/phrases which could have the same meaning. For example, in the sentence 'the table is broken' broken could be replaced by the following words/phrases and the sentence would still have the same meaning.</p>

<p>Broken: damaged, ruined, busted, unfit for purpose, missing a leg</p>

<p>I want to do this for texts that contain domain specific and colloquial jargon so existing NLP solutions may not be suitable? </p>

<p>My intention is to do this as a bridging step between named entity extraction and named entity linking.</p>
","nlp"
"41067","Composing phrases into a grammatically correct sentence?","2018-11-12 00:33:32","41073","0","36","<machine-learning><nlp><text-mining>","<p>I'm wondering if there exist any models which could take in an ordered list of phrases without punctuation and generate a grammatically correct sentence from it. </p>

<p>For example, for the input: [""My dog"", ""hates that"", ""guy named Michael Vick""]</p>

<p>The output could be either ""My dog hates that guy named Michael Vick!"", ""My dog hates that guy named Michael Vick."", ""My dog hates that guy named Michael Vick..."", or even ""My dog hates that guy named... Michael Vick."". Ideally, the probability of the sentence would be returned alongside the output. </p>

<p>Is there a specific name for this problem, so I can do a literature search myself?</p>

<p>Thanks! </p>
","nlp"
"40973","NLP algorithms for categorizing a list of words with specific topics","2018-11-09 17:58:06","41097","5","2674","<python><nlp><algorithms><topic-model>","<p>Currently I am using LDA to apply topic modeling to a corpus. Since LDA is unsupervised, it returns a set of words for a given 'topic' but doesn't necessarily specify the topic itself. I was wondering if there are any suggestions for algorithms that take a list of words and sees what topics it can be categorized to?</p>

<p>For example <code>[cat, dog, fish]</code> can be categorized to <code>animals</code> or <code>pets</code>.</p>

<p>One output for my model: </p>

<pre><code>['game', 'week', 'fantasy', 'sportsline', 'play', 'players', 'league', 'random', 'sunday', 'season', 'agent', 'elink', 'exercise', 'start', 'yards', 'free', 'injury', 'expected', 'practice', 'getbad', 'weekly', 'year', 'reports', 'starting', 'luck', 'nat', 'nfl', 'weeks', 'smith', 'fast']
</code></pre>

<p>Could be categorized to <code>football</code> or <code>sports</code>.</p>

<p>Any suggestions, specifically with Python models/packages would be much appreciated.</p>
","nlp"
"40946","What is a suitable loss function and evaluation metric for a classification model with large number of unbalanced target classes?","2018-11-09 04:09:15","","2","316","<machine-learning><deep-learning><text-mining><multiclass-classification><nlp>","<p>I am building a multiclass classifier to predict the ""Intent"" of a question.
There are some 100 classes in the target variable and each target class contains an unequal proportion of observations/questions varying from 3 % to 40 %.</p>

<p><strong>Questions</strong></p>

<ol>
<li>What would be a good evaluation metric for this classifier? </li>
<li>What is the best proxy loss function for optimizing the suggested evaluation metric?</li>
</ol>

<p>EDIT: This is not a ranking problem. The model should predict only 1 target class.
The cost of misclassification is the same for each target class.</p>
","nlp"
"40870","When one model is superior in real world use?","2018-11-07 14:11:34","","0","85","<neural-network><keras><nlp>","<p>I have an NLP neural network that I have developed with Keras for multi-label classification. </p>

<p>I have fit the model several times and save the best results (via best validation accuracy score) after each set of epochs completes. All of my saved models are in the 96%+ validation accuracy score (according to Keras). </p>

<p>However, when I run these models against real-world data where I also know the result (e.g. effectively a second round of validation) one model in particular outperforms the rest. I can take the champion model (96.29% validation accuracy) and put it up against another model (with something like 96.18% validation accuracy) and the champion model can achieve 90%+ accuracy in the second round of validation while the other model - or any other model - will do nowhere near that. This one model will achieve a minimum 8% accuracy above all other models.</p>

<p>I have double-checked my methodology and I'm nearly positive that all models are being created with the same code and process. </p>

<p>Should I be concerned that this one particular model outperforms the rest? Does it indicate anything in particular in my overall methodology? </p>
","nlp"
"40838","How to learn word embedding from a context on the fly?","2018-11-06 22:30:05","","1","82","<word-embeddings><nlp>","<p>Consider the fictional word tahiliuk in the sentence “We found a small, fluffy tahiliuk running around our garden.” While hearing a new word used in context, people are remarkably adept at inferring a basic notion of a its meaning. </p>

<p><strong>Question:</strong> How the tahiliuk's vector could be learned from the context? </p>

<p><strong>Assumption:</strong> 
a fixed input vocabulary, mapped to a set of word vectors.</p>

<p><strong>Remarks:</strong></p>

<ol>
<li>Tahiliuk is a completely new word. There is now such word within
training data.</li>
<li><p>Using approaches like ""fastText"" will not work here because of
tahiliuk is a random word.</p></li>
<li><p>One possible way is to initialize the embedding  as the sum of their context words and then rapidly refine the embedding with a high learning rate. This approach is used in the following paper, but this approach seems to be not perfect</p></li>
</ol>

<p><em>Herbelot, A., &amp; Baroni, M. (2017). High-risk learning: acquiring new word vectors from tiny data. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing.</em> </p>
","nlp"
"40826","Find correct forms of a word","2018-11-06 15:03:32","","0","50","<python><nlp>","<p>I'm trying to find all grammatical forms for words from a dataset. I'm new to NLP.</p>

<p>I'm using a combo: <code>spaCy</code> (to get a base form of a word), <code>word_forms</code> (to get all possible forms). Sadly, it seems that they are not able to get everything right. Examples like 'wildest' or 'scariest' seem to be out of reach.</p>

<p>Maybe I'm expecting too much (in most of the cases the solution works really well).</p>

<p>Is there a decent tool I could use? I'm aware of <code>nltk</code> lemmatizer, its performance is worse than <code>word_forms</code> and <code>spaCy.</code> I'd appreciate some kind of advice.</p>
","nlp"
"40569","How to use two different datasets as train and test sets?","2018-11-01 14:01:11","40592","2","11000","<python><nlp><dataset><training>","<p>Recently I started reading more about NLP and following tutorials in Python in order to learn more about the subject. The problem that I've encountered, now that I'm trying to make my own classification algorithm (the text sends a positive/negative message) regards the training and the testing datasets. In all the examples that I've found, only one dataset is used, a dataset that is later split into training/testing. I have two datasets, and my approach involved putting together, in the same corpus, all the texts in the two datasets (after preprocessing) and after, splitting the corpus into a test set and a training set. </p>

<pre><code>datasetTrain = pd.read_csv('train.tsv', delimiter = '\t', quoting = 3)
datasetTrain['PN'].value_counts()

datasetTest = pd.read_csv('test.tsv', delimiter = '\t', quoting = 3)
datasetTest['PN'].value_counts()

corpus = []
y = []

# some preprocessing
    y.append(posNeg)
    corpus.append(text)

from sklearn.feature_extraction.text import TfidfVectorizer
transf = TfidfVectorizer(stop_words = stopwords, ngram_range = (1,1), min_df = 5, max_df = 0.65)
X = transf.fit_transform(corpus).toarray()

# Splitting the dataset into the Training set and Test set
from sklearn.cross_validation import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.11, random_state = 0)
</code></pre>

<p>The reason why I've done this is because I'm working with the Bag of Words model and if I'm creating from the beginning <em>X_train</em> and <em>X_test</em> (<em>y_train</em>, <em>y_test</em> respectively) and not using the splitting function, I get an error when running the classification algorithm:</p>

<pre><code>X_train = transf.fit_transform(corpustrain).toarray()
X_test = transf.fit_transform(corpustest).toarray()

...

classifier.fit(X_train, y_train)
y_pred = classifier.predict(X_test)

ValueError: Number of features of the model must match the input. Model n_features is 2770 and input n_features is 585
</code></pre>

<p>I'm kind of new at this and I was wondering if anyone could please guide me in the right direction?</p>
","nlp"
"40509","TF-IDF Features vs Embedding Layer","2018-10-31 14:02:41","40515","6","3288","<keras><nlp><rnn><tfidf>","<p>Have you guys tried to compare the performance of TF-IDF features* with a shallow neural network classifier vs a deep neural network models like an RNN that has an embedding layer with word embedding as weights next to the input layer? I tried this on a couple of tweet datasets and got surprising results: f1 score of~65% for the TF-IDF vs ~45% for the RNN. I tried the setup embedding layer + shallow fully connected layer vs TF-IDF + fully connected layer but got almost same result difference. Can you guys give some opinion on how TF-IDF features can outperform the embedding layer of a deep NN? Is this case common? Thanks!</p>

<ul>
<li>I've used unigrams and bigrams to produce the TF-IDF features</li>
</ul>
","nlp"
"40492","What is NLP technique to generalize manually created rules in text?","2018-10-31 10:09:30","40500","3","693","<nlp><text-mining><information-retrieval><named-entity-recognition><generalization>","<p>Let's say we have a free text containing key-value entities.</p>

<p>Example: ""... patient's <strong>tumour</strong> has <strong>width 6 cm</strong> and <strong>height 5 cm</strong>""</p>

<p>Then an expert comes, marks it as important, thus we do have the rule for finding the same entity in new, different texts.</p>

<p><em>What is the name of NLP technique that produces generalized rule, so that we are able to recognize also synonyms/variations with the same meaning?</em></p>

<p>Example: ""... patient with <strong>cancerous growth</strong>, <strong>60</strong> x <strong>50 mm</strong>""</p>

<p>I need to to learn terminology to be able to search for papers. If you could add also some references to state-of-art approaches, would be great.</p>
","nlp"
"40483","How do you measure performance for word prediction tasks?","2018-10-31 05:03:56","","0","460","<nlp><metric>","<p>Say I have to predict the next word in a sentence, given the initial few words. </p>

<p>Suppose the prefix is ""I went to _____"". This prefix is common enough that it might appear 10 times in the training data with a few different variations:</p>

<p>I went to <strong>college</strong>: 5</p>

<p>I went to <strong>California</strong>: 3</p>

<p>I went to <strong>London</strong>: 2</p>

<p>In such a case, suppose my model predicts ""college"" as the right answer. It would still get only a 50% score if I used accuracy as a metric, the data naturally has multiple (correct) answers for the same input. How do I solve this?</p>
","nlp"
"40295","TextRank algorithm for Web content","2018-10-26 21:20:25","","1","40","<python><text-mining><unsupervised-learning><nlp>","<p>I am looking for an algorithm that would be able to extract meaningful keyphrases from web articles. Each article has more than 2000 words and information is structured using paragraphs, h1, h2 tags and so on. I would like to find 2-3 word phrases that describe particular content. H1 tag plays an important role since it is like a title, but sometimes the body may contain some important information. I was thinking, that maybe TextRank algorithm could be adapted to this problem by including HTML tags and also Named Entity Recognition (NER). Do you know of any research paper in thus direction or open source implementation?
BTW. RAKE performs poorly. </p>
","nlp"
"40224","I am getting a Type Error in this Line","2018-10-25 14:32:17","40231","0","32","<machine-learning><python><nlp><jupyter><ipython>","<pre><code>Diff = [i - j for i,j in zip(text_features, author_signature)]
</code></pre>

<p><strong>Diff is a List</strong> , 
text_features    =  [1, 2, 3] , 
author_signature =  [3, 2, 1]</p>

<p><a href=""https://i.sstatic.net/vDdfe.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/vDdfe.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.sstatic.net/x3pmQ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/x3pmQ.png"" alt=""enter image description here""></a></p>
","nlp"
"40154","How to extract the positions of employee from raw text","2018-10-24 13:49:16","","1","107","<machine-learning><nlp><stanford-nlp>","<p>I have raw text like ""Mr John Fullerton is Chief Executive Officer
and Managing Director of Australian Rail Track
Corporation Ltd, and was appointed to the
position in February 2011.""</p>

<p>I easily identified name of PERSON(John Fullerton) by using Stanford NER, now i want to extract the positions(Chief Executive Officer). Stanford NER treat Chief Executive Officer as 'O', so which method should i use here? </p>
","nlp"
"40062","Identifying documents similar to specific clusters","2018-10-22 23:03:02","","2","51","<nlp><clustering><similarity>","<p>Through performing clustering on a set of 1,000,000 text documents, I have identified 100 clusters. I am particularly interested in, say, 10 of the clusters. Imagine, I now have an additional set of 100,000 documents (not part of the initial 1 mil documents). I wonder if there is a way to efficiently check whether each one of the 100K new documents belongs to one of the 10 clusters.</p>
","nlp"
"40054","Twitter tweet classification","2018-10-22 18:26:05","40057","0","87","<classification><nlp><twitter>","<p>I am trying to do a small project on my own to find out job openings using twitter data. I saved data using flume and converted it to .csv for analysis. My problem is i don't know how to classify tweets, whether it is a job vacancy or just some news on say machine-learning.I read online about neural networks and word2vec but i am not sure if it will solve my problem. Can anyone suggest some ways to do this based on tweet text and hashtags.I don't have training and test data, i just have tweets stored using flume.Also what kind of analysis will i be able to do with it.I am new to data science :/ </p>
","nlp"
"39937","practical improvements worth trying over plain LSTM in text classification?","2018-10-19 18:31:23","42890","0","87","<deep-learning><nlp><lstm><rnn>","<p>I have a dataset of about 1 million tweets corresponding to about 30,000 user accounts, labelled with binary data (classifying the tweet as written by a bot).</p>

<p>With that amount of data, I could use a LSTM-based NLP approach to take advantage of the sequence data in the text.</p>

<p>I would like to get the best accuracy possible, but don't know what models are available that could be combined with or build on LSTM-based approaches.</p>

<p>I also wonder, as one part of the data is sequential and the other stationary (account metadata), whether some kind of hybrid classifier could do better than using just an LSTM.</p>

<p>Are there any hybrid models or any other proven models that would be worth investigation further, to implement for my project?</p>
","nlp"
"39879","Sequence models word2vec","2018-10-18 16:15:42","","2","573","<machine-learning><neural-network><nlp><word2vec><sequence>","<p>I am working a data-set with more than 100,000 records. This is how the data looks like:</p>

<pre><code>email_id    cust_id campaign_name
123         4567     World of Zoro
123         4567     Boho XYz
123         4567     Guess ABC
234         5678     Anniversary X
234         5678     World of Zoro
234         5678     Fathers day
234         5678     Mothers day
345         7890     Clearance event
345         7890     Fathers day
345         7890     Mothers day
345         7890     Boho XYZ
345         7890     Guess ABC
345         7890     Sale
</code></pre>

<p>I am trying to understand the campaign sequence and looking for the next possible campaign for the customers.</p>

<p>Assume I have processed my data and stored it in 'camp'.</p>

<p>With Word2Vec-</p>

<pre><code>from gensim.models import Word2Vec

model = Word2Vec(sentences=camp, size=100, window=4, min_count=5, workers=4, sg=0)
</code></pre>

<p>The problem with this model is that it accepts tokens and spits out text-tokens with probabilities in return when looking for similarities.</p>

<p>Word2Vec accepts this form of input-</p>

<pre><code>['World','of','Zoro','Boho','XYZ','Guess','ABC','Anniversary','X'...]
</code></pre>

<p>And gives this form of output -</p>

<pre><code>model.wv.most_similar('Zoro')
[Guess,0.98],[XYZ,0.97]
</code></pre>

<p>Since I want to predict campaign sequence which occurs more frequently in combination with target word, I was wondering if there is anyway I can give below input to the model and get the campaign name in the output</p>

<p>My input to be as -</p>

<pre><code>[['World of Zoro','Boho XYZ','Guess ABC'],['Anniversary X','World of 
Zoro','Fathers day','Mothers day'],['Clearance event','Fathers day','Mothers 
day','Boho XYZ','Guess ABC','Sale']]
</code></pre>

<p>Output -</p>

<pre><code>model.wv.most_similar('World of Zoro')
[Sale,0.98],[Mothers day,0.97]
</code></pre>

<p>I am also not sure if there is any functionality within the Word2Vec or any similar algorithms which can help finding the next possible campaign for individual users.</p>
","nlp"
"39867","Audio signal signal processing for background noise reduction or removal","2018-10-18 11:27:04","","1","332","<preprocessing><nlp><audio-recognition>","<p>I am performing <a href=""https://www.tensorflow.org/tutorials/sequences/audio_recognition"" rel=""nofollow noreferrer"">simple audio recognition</a> using tensor-flow to spot key word or hot word detection. The graph takes the <strong>microphone input directly</strong> and performs ""Audio spectrometer --> mfcc's --> and input to the neural network."" </p>

<p>Now i would to like to perform some kind of real time back ground noise reduction or cancellation before inputting the mfcc's into the neural network. </p>

<p>I am not sure what kind of algorithms/python packages that are helpful to reduce the back ground noise and If some one can point me to the pieces of code/knowledge base that can help to filter the mfcc's before inputting into neural network.</p>
","nlp"
"39783","Opensource Speech Recognition Library that is secure and trained on large data","2018-10-16 17:40:17","39787","0","47","<machine-learning><nlp><speech-to-text>","<p>For all those who are working on developing a chatbot/assistant and care about the privacy of users consuming the speech recognition library, can you suggest an open souce library which is trained on a large data. Big concern is the privacy that's why not going for Google Diagflow or IBM Watson or Amazon Lexa or Wit.</p>

<p>Would appreciate a lot if someone can suggest a good library.</p>
","nlp"
"39758","Is my model over-fitting (LSTM, GRU)","2018-10-16 12:17:54","","1","785","<machine-learning><classification><keras><nlp><lstm>","<p>I have small corpus max 150 text utterances, which is again distributed among 5 categories. 
To test I started with basic deep learning model, where I used word2vec embedding, added 1D convolution layer followed by 150 GRU units:</p>

<pre><code>embedding_layer = Embedding(vocab_size, 300,weights=[embedding_matrix], input_length=max_length,trainable=True)
sequence_input = Input(shape=(max_length,), dtype='int32')
embedded_sequences = embedding_layer(sequence_input)

x = Conv1D(24, 4,activation='relu')(embedded_sequences)
x = GRU(150,kernel_regularizer=regularizers.l1(0.01),activation='tanh')(x)
x = Dropout(0.4)(x)
</code></pre>

<p>Training &amp; validation loss:</p>

<p><a href=""https://i.sstatic.net/vofrX.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/vofrX.png"" alt=""training &amp; validation loss""></a>
As per this it's fits perfect, but when I give as is utterances to predict it's going to come other class.</p>

<p>Code for predicting the utterances:</p>

<pre><code>encoded_doc = [[16, 7, 49, 50, 51]]
max_length = 25
padded_doc = pad_sequences(encoded_doc, maxlen=max_length, padding='post')
predictions = model.predict(padded_doc)
pre_class = model.predict(padded_doc)[0]
classes = np.argmax(predictions)

print('Predicted class: '+str(label_encoder.inverse_transform(classes))+' ## score: '+str(pre_class[classes]))
</code></pre>

<p>So, I changed it to LSTM where I increased the LSTM units to 250:</p>

<pre><code>seed = 7
np.random.seed(seed)

embedding_layer = Embedding(vocab_size, 300,weights=[embedding_matrix], input_length=max_length,trainable=True)
sequence_input = Input(shape=(max_length,), dtype='int32')
embedded_sequences = embedding_layer(sequence_input)

x = Conv1D(24, 4,activation='relu')(embedded_sequences)
x = LSTM(250,kernel_regularizer=regularizers.l1(0.01),activation='tanh')(x)
x = Dropout(0.4)(x)
</code></pre>

<p>Train &amp; Validation loss:</p>

<p><a href=""https://i.sstatic.net/ejuNQ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ejuNQ.png"" alt=""enter image description here""></a></p>

<pre><code>_________________________________________________________________ 
Layer (type)                 Output Shape              Param #   
================================================================= 
input_7 (InputLayer)         (None, 25)                0         
_________________________________________________________________                     
embedding_7 (Embedding)      (None, 25, 300)           31500     
_________________________________________________________________     
conv1d_7 (Conv1D)            (None, 22, 64)            76864     
_________________________________________________________________ 
lstm_4 (LSTM)                (None, 250)               315000    
_________________________________________________________________ 
dropout_7 (Dropout)          (None, 250)               0         
_________________________________________________________________ 
dense_7 (Dense)              (None, 5)                 1255      
================================================================= 
Total params: 424,619 Trainable params: 424,619 Non-trainable params:0
_________________________________________________________________ 
Train on 123 samples, validate on 31 samples Epoch 1/35 123/123
&gt; [==============================] - 1s 8ms/step - loss: 22.8336 - acc:
&gt; 0.2439 - val_loss: 18.5322 - val_acc: 0.3871 Epoch 2/35 123/123 [==============================] - 1s 7ms/step - loss: 16.5612 - acc:
&gt; 0.2846 - val_loss: 12.7986 - val_acc: 0.4194 Epoch 3/35 123/123 [==============================] - 1s 7ms/step - loss: 11.0305 - acc:
&gt; 0.3171 - val_loss: 7.7372 - val_acc: 0.5806 Epoch 4/35 123/123 [==============================] - 1s 7ms/step - loss: 6.5548 - acc:
&gt; 0.4472 - val_loss: 4.1708 - val_acc: 0.5806 Epoch 5/35 123/123 [==============================] - 1s 7ms/step - loss: 3.2714 - acc:
&gt; 0.5447 - val_loss: 1.9674 - val_acc: 0.5484 Epoch 6/35 123/123 [==============================] - 1s 7ms/step - loss: 1.6011 - acc:
&gt; 0.5528 - val_loss: 1.2261 - val_acc: 0.5806 Epoch 7/35 123/123 [==============================] - 1s 7ms/step - loss: 2.0814 - acc:
&gt; 0.4878 - val_loss: 1.9198 - val_acc: 0.5484 Epoch 8/35 123/123 [==============================] - 1s 7ms/step - loss: 1.7965 - acc:
&gt; 0.4634 - val_loss: 1.2227 - val_acc: 0.5806 Epoch 9/35 123/123 [==============================] - 1s 7ms/step - loss: 1.4348 - acc:
&gt; 0.5447 - val_loss: 1.2684 - val_acc: 0.6129 Epoch 10/35 123/123 [==============================] - 1s 7ms/step - loss: 1.3092 - acc:
&gt; 0.5772 - val_loss: 1.0482 - val_acc: 0.7742 Epoch 11/35 123/123 [==============================] - 1s 7ms/step - loss: 1.2495 - acc:
&gt; 0.6341 - val_loss: 1.0036 - val_acc: 0.7419 Epoch 12/35 123/123 [==============================] - 1s 7ms/step - loss: 1.1438 - acc:
&gt; 0.7073 - val_loss: 0.9640 - val_acc: 0.7419 Epoch 13/35 123/123 [==============================] - 1s 7ms/step - loss: 0.8768 - acc:
&gt; 0.9024 - val_loss: 0.4931 - val_acc: 1.0000 Epoch 14/35 123/123 [==============================] - 1s 7ms/step - loss: 0.5908 - acc:
&gt; 0.9512 - val_loss: 2.2134 - val_acc: 0.6129 Epoch 15/35 123/123 [==============================] - 1s 7ms/step - loss: 1.4977 - acc:
&gt; 0.6423 - val_loss: 1.1113 - val_acc: 0.5806 Epoch 16/35 123/123 [==============================] - 1s 7ms/step - loss: 1.1936 - acc:
&gt; 0.5691 - val_loss: 1.0105 - val_acc: 0.5806 Epoch 17/35 123/123 [==============================] - 1s 7ms/step - loss: 1.1522 - acc:
&gt; 0.4634 - val_loss: 1.0109 - val_acc: 0.5806 Epoch 18/35 123/123 [==============================] - 1s 7ms/step - loss: 1.0135 - acc:
&gt; 0.6260 - val_loss: 2.5970 - val_acc: 0.1935 Epoch 19/35 123/123 [==============================] - 1s 7ms/step - loss: 1.4423 - acc:
&gt; 0.6992 - val_loss: 1.3537 - val_acc: 0.6774 Epoch 20/35 123/123 [==============================] - 1s 7ms/step - loss: 1.4512 - acc:
&gt; 0.5935 - val_loss: 1.1868 - val_acc: 0.6774 Epoch 21/35 123/123 [==============================] - 1s 7ms/step - loss: 1.2350 - acc:
&gt; 0.5447 - val_loss: 1.0781 - val_acc: 0.5806 Epoch 22/35 123/123 [==============================] - 1s 7ms/step - loss: 1.1008 - acc:
&gt; 0.6260 - val_loss: 0.9849 - val_acc: 0.9677 Epoch 23/35 123/123 [==============================] - 1s 7ms/step - loss: 0.9986 - acc:
&gt; 0.6504 - val_loss: 0.8684 - val_acc: 0.9355 Epoch 24/35 123/123 [==============================] - 1s 7ms/step - loss: 1.3619 - acc:
&gt; 0.7154 - val_loss: 1.4444 - val_acc: 0.6774 Epoch 25/35 123/123 [==============================] - 1s 7ms/step - loss: 1.5590 - acc:
&gt; 0.7398 - val_loss: 1.5238 - val_acc: 0.5806 Epoch 26/35 123/123 [==============================] - 1s 7ms/step - loss: 1.1659 - acc:
&gt; 0.8862 - val_loss: 0.8608 - val_acc: 1.0000 Epoch 27/35 123/123 [==============================] - 1s 7ms/step - loss: 0.8432 - acc:
&gt; 0.9756 - val_loss: 0.6919 - val_acc: 1.0000 Epoch 28/35 123/123 [==============================] - 1s 8ms/step - loss: 0.7218 - acc:
&gt; 0.9675 - val_loss: 0.6103 - val_acc: 1.0000 Epoch 29/35 123/123 [==============================] - 1s 7ms/step - loss: 0.6496 - acc:
&gt; 0.9756 - val_loss: 0.5566 - val_acc: 1.0000 Epoch 30/35 123/123 [==============================] - 1s 7ms/step - loss: 0.5961 - acc:
&gt; 0.9675 - val_loss: 0.5152 - val_acc: 1.0000 Epoch 31/35 123/123 [==============================] - 1s 7ms/step - loss: 0.5590 - acc:
&gt; 0.9675 - val_loss: 0.4832 - val_acc: 1.0000 Epoch 32/35 123/123 [==============================] - 1s 9ms/step - loss: 0.5188 - acc:
&gt; 0.9593 - val_loss: 0.4564 - val_acc: 1.0000 Epoch 33/35 123/123 [==============================] - 1s 10ms/step - loss: 0.4987 - acc:
&gt; 0.9675 - val_loss: 0.4355 - val_acc: 1.0000 Epoch 34/35 123/123 [==============================] - 1s 7ms/step - loss: 0.4634 - acc:
&gt; 0.9919 - val_loss: 0.4177 - val_acc: 1.0000 Epoch 35/35 123/123 [==============================] - 1s 7ms/step - loss: 0.4372 - acc:
&gt; 1.0000 - val_loss: 0.4043 - val_acc: 1.0000 31/31 [==============================] - 0s 1ms/step Accuracy: 100.000000
</code></pre>

<p>I am not getting any clue where I am going wrong, even to avoid that I have hard coded the sentence tokens to predict. I wanted to understand is it over fitting or not cos at least by training &amp; validation loss graph I don't think it's. </p>

<p>Please, let me know if you need any more information.</p>

<hr>

<ul>
<li>Epochs: 35</li>
<li>Batch size: 20</li>
<li>Shuffle = False</li>
<li>Validation split is on 20% </li>
</ul>

<p>Thanks in advance.</p>
","nlp"
"39724","Alternatives to doc2vec?","2018-10-15 18:51:58","","2","1280","<python><nlp><data><word-embeddings><data-science-model>","<p>What are some alternatives to the doc2vec embedding model? I.e models that convert paragraphs/documents into vectors, not just models that take the mean/sum of the word embeddings of each word in the document.</p>
","nlp"
"39656","Stemmer/lemmatizer for Polish language","2018-10-14 08:45:03","39978","6","9576","<nlp>","<p>I'm looking for a stemmer/lemmatizer for Polish language, preferably in Python. What would you recommend?</p>
<p>I have a list of ingredients in a recipe. Plural forms are inflected differently, depending on the counter, e.g.: for tomatoes</p>
<blockquote>
<p>5 pomidorów</p>
<p>2 pomidory</p>
<p>1 pomidor</p>
</blockquote>
<p>I want my parser to recognize all those ingredients as for one product, hence the need for stemming.</p>
","nlp"
"39377","Classifying objects based of a varying number of the same type of feature vector for each object","2018-10-08 19:02:46","","2","38","<neural-network><classification><keras><nlp><feature-selection>","<p>For a congressional session, I have created a doc2vec model of speeches made. Using the vectors from this model, I have a dataset of each congressperson, their political affiliation, and a list of the vector representations of each speech they made. Each of these document vector representations is a 300 element vector.</p>

<p>I am now trying to classify each congressperson by party using these document vectors representing their speeches. So far I have tried using the mean vector for each speaker, but I was looking for ways to use the whole set of vectors.</p>
","nlp"
"39314","Classification of Conversations in Text","2018-10-07 18:04:34","57162","4","2328","<classification><nlp>","<p>I am trying to pick a technique for classifying conversational text.  I am concerned about treating the problem at a level of fidelity of each individual message because people often say things like, ""ok"" or short responses that have no inferable meaning.   How does conversation classification typical handle these types of problems?</p>

<p>To elaborate, a conversation might be:</p>

<pre><code>P1: Hi I want to buy a car?
P2: Ok. Great!
P1: What cars do you have?
P2: A large variety!
</code></pre>

<p>The topic is cars, but this can not be inferred by anything P2 says, nor should it be.  So would you break a conversation into blocks of time, or is there a technique for partitioning?</p>
","nlp"
"39280","What are the rules when extracting SVO triples from preprocessed text?","2018-10-06 18:09:42","","5","1249","<python><nlp><text-mining><feature-extraction>","<p>If you have some already preprocessed text that is tagged, what are the rules to extract Subject-Verb-Object (SVO) triples if you want a triple like (word, word, word). Can you give the sentence as example and extract all triples? Do you just need to find all combinations without repetition from set of N words?</p>
","nlp"
"39261","How to fetch text from pdf to further proceed with question answer based model from the same document?","2018-10-06 06:59:11","","3","223","<machine-learning><deep-learning><nlp><cnn><computer-vision>","<p>To illustrate the above title.</p>

<p><em>Suppose you have a pdf document, which is basically scanned from hardcopy, now there are set of fixed questions to answer from the document itself. 
For an example a document contains a contract of land, now the set of fixed questions be ""who is the seller?"" ""what is price of the asset? "", document has referred to this answers probably 2-3 times, as a human it's a simple task.</em></p>

<p>How to automate this?</p>
","nlp"
"39081","Confidence Intervals for Multi-Categorical Votes","2018-10-02 20:57:41","39151","2","96","<multiclass-classification><nlp><counts>","<p>I have an ngram-based language model that produces a long tag list for a given sentence. For example, the just-previous sentence, broken into bigrams, and run through the model might produce something like:  </p>

<p>{I have}=>C1 {have an}=>C2 {an ngram}=>C1 {ngram based}=>C3, etc. </p>

<p>resulting in counts: C1=2, C2=1, C3=1 (for the shown segment above). </p>

<p>Easy enough to pick the winner by sorting either the counts, or after turning them into percentages, which would control for sentence length. But I want a CI on that winner -- that is, I want to know when it's a statistical tie between the top N categories (either by count or percent). </p>

<p>I'm sure that there's an obvious way to do this....</p>

<p>...Pointers appreciated! </p>
","nlp"
"39072","Spacy Returns Nonidentical Results for Doc. Examples?","2018-10-02 14:37:32","39194","3","287","<python><nlp>","<p>I am getting started with Spacy, and I was just experimenting the given examples. It is weird though that when I run them over their containers using their experimental console it gives me different results than when I run them locally. </p>

<p>For example, when I run <a href=""https://spacy.io/usage/vectors-similarity"" rel=""nofollow noreferrer"">the following example</a> from their console,</p>

<pre><code>import spacy

nlp = spacy.load('en_core_web_md')  # make sure to use larger model!
tokens = nlp(u'dog cat banana')

for token1 in tokens:
    for token2 in tokens:
        print(token1.text, token2.text, token1.similarity(token2))
</code></pre>

<p>It gives me: </p>

<pre><code>dog dog 1.0
dog cat 0.80168545
dog banana 0.24327646
cat dog 0.80168545
cat cat 1.0
cat banana 0.2815437
banana dog 0.24327646
banana cat 0.2815437
banana banana 1.0
</code></pre>

<p>But when I run the exact same thing locally, it gives me:</p>

<pre><code>dog dog 1.0
dog cat 0.0
dog banana 0.0
cat dog 0.0
cat cat 1.0
cat banana -0.044681177
banana dog -7.828739e+17
banana cat -8.242222e+17
banana banana 1.0
</code></pre>

<p>To me the former makes sense, not my local result. And the problem persists for other examples as well. I am so confused! What is happening locally? </p>

<ul>
<li>Spacy: '2.0.12'</li>
<li>Python 3.6.5 Anaconda, Inc.</li>
<li>Mac OSx 10.13.5</li>
</ul>

<p>I am not sure if it is relevant, but I was getting <a href=""https://github.com/explosion/spaCy/issues/1684"" rel=""nofollow noreferrer"">this error</a> running it in Jupyter, so I exported the followings, as suggested in the git, in the terminal before launching the Jupyter: </p>

<pre><code>export LC_ALL=en_US.UTF-8
export LANG=en_US.UTF-8
</code></pre>
","nlp"
"39040","best activation function for ensemble?","2018-10-02 03:17:03","39045","1","396","<logistic-regression><ensemble-modeling><nlp><activation-function>","<p>i have created some logistic regression model (different preprocessing) with softmax function. and i mix all model with an ensemble with a hierarchical method. so the output of all model (base) will be used as input for the final model (logistic regression too).</p>

<p>the default base model used a softmax function. i think transform a confident value into a probability will lose much information. so i have a plan to change the softmax into activation function.</p>

<p>what i learn in my CNN class, Relu is the best default activation function for an image. but my case is multi-class classification email. </p>

<p>which activation function should i choose?
Relu, sigmoid or other?</p>

<p>thanks</p>
","nlp"
"39039","Model Joint Probability of N Words Appearing Together in a Sentence","2018-10-02 03:15:38","","1","196","<machine-learning><nlp><lda><rnn><rbm>","<p>Assume that we have a large corpus of texts to train with. Given N words as input, I want to model the joint probability <span class=""math-container"">$p(x_1, x_2, ..., x_N)$</span> of these words appearing together in a sentence. More specifically, <em>the N words are not required to be ordered or contiguous, and words other than given words can appear in the sentence.</em> There is no restriction on the number of times each of N words can appear in the sentence. I did some research and below are some possible directions. </p>

<ol>
<li><p>Popular RNN models like LSTM offer conditional probability <span class=""math-container"">$p(x_t | x_{t-1}, ..., x_0)$</span>, which may shed light on the joint probability I am modeling. However, RNN models take sequential inputs and require the N given words to be ordered. Also, if I am to use RNN models, I need to use Markov chain rule to calculate the joint probability, the N words would need to be continuous, and no words other than given words would be allowed.  </p></li>
<li><p>Topic models like LSA(Latent Semantic Analysis) or LDA(Latent Dirichlet Allocation) may help me find topics from corpus, and model joint probability based on topics that each of N words belong to. Words from same topic are assigned higher joint probability. These models require me to manually set the number of topics. As I am not really familiar with these models, I am not sure how good they will perform. </p></li>
<li><p>A neural network alternative to LDA is a Restricted Boltzmann Machine, where topics are learnt as hidden neurons. The input to this RBM would be a vector the size of my whole dictionary. Each entry is either 1(present), 0(not present), or -1(don’t know). During training the probability <span class=""math-container"">$p(h|v_{train})$</span> is learnt, where <span class=""math-container"">$h$</span> is my learnt topics and <span class=""math-container"">$v$</span> are training data. At test time, the test input <span class=""math-container"">$v_{test}$</span> will have all N given words’ value equal to 1, while all other words equal to -1.  In this way we can learn the probability <span class=""math-container"">$p(v_{test}|h)$</span>. </p></li>
</ol>

<p>I feel like this is a really fundamental problem in language modeling, but have not yet been able to find recent research on this. Could anyone please give any pointers on this problem? Does any of the above proposed solutions look feasible? Any help is appreciated! </p>
","nlp"
"39010","Help with reusing glove word embedding pretrained model","2018-10-01 09:29:58","39014","1","263","<python><nlp><text-mining><feature-extraction><word-embeddings>","<p>When using pretrained GloVe.6B for embedding generation, How can I get only the top most frequently used 100000 words rather than all the 4M words in the file?</p>
","nlp"
"38935","Using an ontology to recognize named entities in free text","2018-09-29 08:39:14","","1","90","<nlp><named-entity-recognition><hierarchical-data-format>","<p>I'm trying to solve a fairly basic problem in NPL efficiently. What tool or software package would you use to identify the words, or group of words that are part of an given ontology within a free text.</p>

<p>Let's imagine the inputs are the following dummy ontology:</p>

<p><a href=""https://i.sstatic.net/iTMCJ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/iTMCJ.png"" alt=""Dummy Ontology""></a></p>

<p>And this publication's abstract:</p>

<blockquote>
  <p>This study evaluates the addition of metformin to standard of care in locally advanced and metastatic prostate cancer, half the patients will receive metformin in combination with standard treatment, and the other half will receive the standard of care only. Advanced-stage PCa is usually treated with androgen-deprivation therapy (ADT). Most patients with metastatic disease who were managed with ADT eventually progress to castration-resistant prostate cancer (CRPC) and die of the disease.</p>
</blockquote>

<p>When run together through the tool via an API, or any such other call, then the outputs would look something like (in the format <code>class:name text_loc_start</code>:<code>text_loc_end</code>):</p>

<pre><code>- ID005:ID011 34:56 (Drug:Metformin ""metformin"")
- ID004:ID008 117:132 (Condition:Prostate cancer ""prostate cancer"")
- ID005:ID011 444:523 (Drug:Metformin ""metformin"")
- ID004:ID008 981:995 (Condition:Prostate cancer ""prostate cancer"")
</code></pre>

<p>I am looking for a solution that would have an unrestrictive license and can be used commercially for free. Thanks in advance.</p>
","nlp"
"38898","Predicting topics for customer reviews based on topics mapped to n-grams?","2018-09-28 07:05:03","","2","220","<nlp><word2vec><word-embeddings><lda>","<p>I have a large number of unlabelled customer review data(text column) and my objective is to classify each review to a particular topic.
<a href=""https://i.sstatic.net/vbUnV.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/vbUnV.png"" alt=""Customer_review_data""></a></p>

<p>Also I have a list of unigrams,bigrams and trigrams(not a part of the customer review data) where each n-gram(word column) is tagged with a particular topic(topic column).</p>

<p><a href=""https://i.sstatic.net/yeAna.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/yeAna.png"" alt=""n-gram topic mapping data""></a></p>

<p>Is there a way to classify the customer reviews to the topics associated with the n-grams? 
I am mostly thinking in terms of Word2Vec and Doc2Vec approaches.</p>
","nlp"
"38827","Find matching text from a text column","2018-09-26 16:39:20","","5","543","<machine-learning><python><nlp><text-mining>","<p>This is my first time to use Data Analytics tool to figure out a solution to a problem.
I have a table with following columns <code>Person ID, Person Name, Note ID, Note (notes is a free form text where a call representative can enter their comments) Insert User</code>. I have been given list of <code>key phrases</code> to be identified from the <code>Note</code> column. These key phrases are in a sentence format.
Example:<a href=""https://i.sstatic.net/zr03B.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/zr03B.png"" alt=""enter image description here"" /></a></p>
<p>I have to find exact or a similar matching phrase from those notes. The format of my final report would look something like this
<a href=""https://i.sstatic.net/jbGd1.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/jbGd1.png"" alt=""enter image description here"" /></a></p>
<p>I looked into couple of articles related to text matching which suggested options such as</p>
<blockquote>
<p>fuzzywuzzy, Doc2vec, Difflib, python-levenshtein</p>
</blockquote>
<p>It is all so confusing. Even if I get a starter to pick the best suitable option, I can may be take it from there. Any suggestions?
Thank you so much!</p>
","nlp"
"38753","Commercial Software for Interactive Machine Learning and Annotation in NLP","2018-09-25 07:32:38","","1","45","<nlp><active-learning>","<p>We are evaluating make or buy for a tool for interactive machine learning on legal text. With little labeling effort, it should enable for (ml layman) domain experts to build classifiers on a GUI.</p>

<p>Regarding commercial solutions, a came across prodi.gy of explosion AI. 
It looks great and their spacy lib is awesome too.</p>

<p>What are competing commercial solutions in the (inter)active ML / semisupervised space, ideally geared towards NLP (legal would be a +)? Also looking into startups.</p>

<p>Thanks in advance.</p>
","nlp"
"38750","Can we use doc2vec to detect outlier documents?","2018-09-25 05:34:34","","2","305","<data-mining><nlp><word2vec><outlier><gensim>","<p>I have a set of documents and I want to identify and remove the outlier documents. I am just wondering if doc2vec can be used for this task.</p>

<p>Or are there any recently evolved, promising algorithms that I can use for this task?</p>

<p><strong>EDIT</strong></p>

<p>I am currently using a bag of words model to identify outliers.</p>
","nlp"
"38745","Increasing SpaCy max NLP limit","2018-09-24 23:33:27","","14","21485","<python><nlp>","<p>I'm getting this error:</p>

<pre><code>[E088] Text of length 1029371 exceeds maximum of 1000000. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.
</code></pre>

<p>The weird thing is that if I reduce the amount of documents being lemmatized, it still says the length exceeds 1 million. Is there a way of increasing the limit past 1 million? The error seems to suggest there is but I'm unable to do so.</p>
","nlp"
"38662","What approach to use to detect violations of media ethics in news?","2018-09-23 05:21:24","","2","25","<machine-learning><classification><nlp>","<p>I've been trying to come up with a solution to detect violations of media ethics in news articles. For example, I need to detect if an article has mentioned the name of a victim of rape. So far, I've tried to perform a binary classification using a Multinomial Naive Bayes classifier with sklearn. However, it does not produce very accurate results and I believe this is because there are no clear-cut features to be used for classifying. I really appreciate if someone can point me in the right direction or give me some tips on how to proceed.</p>
","nlp"
"38619","Interpretation of the loss function for word2vec","2018-09-21 19:26:13","38623","5","9582","<nlp><stanford-nlp>","<p>I am trying to understand the loss function which is used for the word2vec model, but I don't really follow the argumentation behind this video <a href=""https://www.youtube.com/watch?v=ERibwqs9p38&amp;t=5s"" rel=""noreferrer"">https://www.youtube.com/watch?v=ERibwqs9p38&amp;t=5s</a>, at 29:30.</p>

<p>The formula which is unclear is the following:</p>

<p><span class=""math-container"">$J(\theta) = \displaystyle-\dfrac{1}{T} {\sum_{t=1}^{T} \sum_{-m &lt;= j &lt;=m, \\j\ne0} log(p(w_{t+j}|w_t))}.$</span></p>

<ul>
<li><span class=""math-container"">$T$</span> is the number of words in the vocabulary</li>
<li><span class=""math-container"">$w_t$</span> is a given word and we try to calculate the probablity that another word <span class=""math-container"">$w_{t+j}$</span> occurs within a window of +/- <span class=""math-container"">$m$</span> words ahead.</li>
<li><span class=""math-container"">$\theta$</span> is the solution we're after. It is essentially a <span class=""math-container"">$2*d*Tx1$</span> dimensional vector which contains all the columns of the matrices V and U.</li>
</ul>

<p>At a first sight it looks all pretty clear: we're iterating though the whole vocabulary and for each (fixed word then), we add up all probabilities that another word occurs within a window around that <em>fixed</em> word. </p>

<p>However, it fails apart for me when I consider that a word <span class=""math-container"">$w_t$</span> occurs in many positions in the corpus and might occur multiple times with a different word. E.g, the words 'deep learning' often occur together, which indicates that there's a contextual relation between them. Why would we only count them twice? It seems like the formula above is counting each pair <span class=""math-container"">$p(w_{t+j}|w_t)$</span> just twice (e.g. once for <span class=""math-container"">$p(deep|learning)$</span> and once for <span class=""math-container"">$ p(learning|deep)$</span>). IMO we should need a correcting term that adjusts for the missing 'frequency', e.g.</p>

<p><span class=""math-container"">$J(\theta) = \displaystyle-\dfrac{1}{T} {\sum_{t=1}^{T} \sum_{-m &lt;= j &lt;=m, \\j\ne0} log(\lambda(w_t,w_{t+j})p(w_{t+j}|w_t))}.$</span></p>

<p>In the case where <span class=""math-container"">$\lambda(x,y) = 1$</span>, we get the formula above, but we could also be free to chose a function that boosts frequent occurrences (pairwise). The formula above could then be seen as a special case when you don't care that words that occur more often together get a boost. </p>

<p>On the other hand, when the formula above already accounts for multiple occurrences, then where is this visible?
The author then continues and defines <span class=""math-container"">$p(o|w)$</span> as <span class=""math-container"">$exp(o^T*w)/\sum(exp(u^T*w))$</span>. In this particular case, I don't see that we're counting the dot-product as many times as the word <span class=""math-container"">$o$</span> is in the neighbourhood of the word <span class=""math-container"">$w$</span>. Maybe the choice of <span class=""math-container"">$p$</span> is just very simplistic and represents a model where the fact that 2 words are in the neighbourhood (just somewhere in the corpus) is enough (bag of words model???). It's hard to see then that such models deliver good performance in NLP though.</p>
","nlp"
"38597","Lowercase texts before tokenizing as pre-processing step for alignment","2018-09-21 08:06:36","","1","239","<nlp><preprocessing><tokenization>","<p>I am pre-processing some texts and I wonder what the best practice is when preparing your texts for word alignment. I don't know how aligners such as fast align and GIZA++ work under the hood, but I could assume that they take capital letters as distinguished characters. Is there, therefore, any reason to lower case your text in such a situation? Is this language dependent? It might, for instance, be interesting for a language with many capital letters, such as German, to do so or not.</p>
","nlp"
"38563","How to do good Keyword Extraction","2018-09-20 18:09:59","","1","266","<machine-learning><nlp><named-entity-recognition>","<p>I tried the sketch engine (no ad!) and I wonder what might be the underlying algorithms to do such a good keyword extraction. I have a document consitsing of rows of sentences and from each I want the proper NERs or keywords. I tried a lot. I need them to write my own NER based on CRF.</p>

<p>Is this just statistics used to extract the keywords or what else do you need for that? I ask in general and not how they did it, but it would be nice to find out if someone knows...</p>
","nlp"
"38540","Are there any good out-of-the-box language models for python?","2018-09-20 13:34:22","121044","16","8158","<python><nlp><language-model>","<p>I'm prototyping an application and I need a language model to compute perplexity on some generated sentences.</p>

<p>Is there any trained language model in python I can readily use? Something simple like</p>

<pre><code>model = LanguageModel('en')
p1 = model.perplexity('This is a well constructed sentence')
p2 = model.perplexity('Bunny lamp robert junior pancake')
assert p1 &lt; p2
</code></pre>

<p>I've looked at some frameworks but couldn't find what I want. I know I can use something like:</p>

<pre><code>from nltk.model.ngram import NgramModel
lm = NgramModel(3, brown.words(categories='news'))
</code></pre>

<p>This uses a good turing probability distribution on Brown Corpus, but I was looking for some well-crafted model on some big dataset, like the 1b words dataset. Something that I can actually trust the results for a general domain (not only news)</p>
","nlp"
"38213","Pass 2 different kinds of X training data to ML model simultaneously","2018-09-13 15:22:59","","1","260","<machine-learning><python><scikit-learn><nlp>","<p>I'm trying to classify if a book is fiction/nonfiction based on title and summary.</p>

<p>This is 2 distinct types of information - is there a way to segment <code>title</code> and <code>summary</code> before feeding it to a model, rather than concatenating the information?</p>

<p><strong>For example:</strong></p>

<p>Title: <code>""such a long journey""</code></p>

<p>Summary: <code>""it is bombay in 1971, the year india went to...""</code></p>

<p>Label: <code>""fiction""</code> (where fiction =1)</p>

<p><strong>Current procedure:</strong></p>

<p>What I've been doing until now is concatenating the information, so the above becomes,</p>

<pre><code>example = ""such a long journey it is bombay in 1971, the year india went to...""
label = 1
</code></pre>

<p>Then the usual setup, something like,</p>

<pre><code>X.append(example)
y.append(label)
...
X = lemmatize(X)
...
X_train, X_test, y_train, y_test = split_data(X,y)

vectorizer = TfidfVectorizer(...)
X_train = vectorizer.fit_transform(X_train)
X_test = vectorizer.transform(X_test)

classifier.fit(X_train, y_train)
y_predict = classifier.predict(X_test)
</code></pre>

<p>But feeding the data concatenated feels intuitively wrong. Is there a better way to do this?</p>

<p>If for some reason its possible with a library other than sklearn (keras, tensorflow) I'd be also open to hearing about that.</p>

<hr>

<h2>UPDATE</h2>

<p>Going from,</p>

<pre><code>X = ['two'],['two'],['four'],['two'],['four'],['four']]
y = ['human','human','dog','human','dog','dog']
</code></pre>

<p>to,</p>

<pre><code>X = [['two','hello'],['two','hello'],['four','bark'],['two','hi'],['four','bark'],['four','woof']]
y = ['human','human','dog','human','dog','dog']
</code></pre>

<p>causes errors to be thrown.</p>

<p><code>'list' object has no attribute 'lower'</code> is X is a list, and <code>'numpy.ndarray' object has no attribute 'lower'</code> if X is an array.</p>

<p>The error is thrown when I call, </p>

<pre><code>X_train = vectorizer.fit_transform(X_train)
</code></pre>

<p>Is it possible to pass in a vector of features?</p>
","nlp"
"38182","Common deep learning practices in NLP for text classification","2018-09-12 23:54:28","","1","39","<deep-learning><text-mining><nlp>","<p>Are there any articles about best modern techniques in text classification (not only for English texts, but in general)? In particular, I'm interested:</p>

<ul>
<li>what kind of text preprocessing techniques are usually useful? Can you share with some good open-source text preprocessors?</li>
<li>what kind of neural networks architectures one should try first on a new text classification problem? What kind of architectures should be solid baselines?</li>
<li>what are the ""rules of thumb"" for choosing the size of the dictionary? maximum number of words to be left (in sequence padding)? word embeddings dimension?</li>
</ul>

<p>Would be great if you could share articles, or (even better!) the must-read research papers, or anything else you find useful about this topic. If you could share with your own experience in the comments, that would be greatly appreciated, too!</p>
","nlp"
"38099","Efficient search for a Triples data","2018-09-11 12:04:39","","1","33","<nlp><similarity><information-retrieval>","<p>I have a text file which consists of ~10k triples in the format,
</p>

<p>Let say I need to extract all the relevant triples for the query ""Tom and Jerry likes to play football"", with this I am extracting NP(Noun Phrases) from the query and applying similarity search. </p>

<p>It takes a lot of time ~10 minutes to fetch the results and it isn't a scalable solution since, the size of triple file is directly proportional to search time. </p>

<p>How can I reduce the time?</p>
","nlp"
"38071","word/sentence alignment for English document","2018-09-10 22:35:22","38483","2","396","<text-mining><nlp><machine-translation>","<p>I have an English document, which is preprocessed into two versions. I want to align words or sentences from these two versions of the document. A simple example is as below:</p>
<pre><code>I don't want to go there. My e-mail address ok.
</code></pre>
<p>should be aligned with</p>
<pre><code>I do n't want to go there my email address ok
</code></pre>
<p>Or the tokens from the first sentence are aligned correctly with the tokens in the 2nd sentence. I have tried some sequence alignment methods, which do not perform well. Considering there are alignment tools for machine translation, automatic alignment for English should be easier right?</p>
","nlp"
"38054","Is there a dataset with news articles and their headlines?","2018-09-10 16:51:28","","2","1421","<machine-learning><nlp><machine-translation><automatic-summarization>","<p>I need a set of news headlines and articles to help me in a project on automatic summarization. Is there such a dataset or something similar?</p>
","nlp"
"38028","Check If Answer for a Question is Correct by Similarity","2018-09-10 08:03:35","","5","1762","<machine-learning><nlp><ai>","<p>Let's say in an NLP problem, I have a question and some correct answers to that question (say, 10 correct answers). </p>

<p>Is there a way to get a new answer as input, and ""calculate"" whether it is correct? </p>
","nlp"
"38022","Are word embeddings further updated during training for document classification?","2018-09-10 07:07:31","","5","871","<machine-learning><nlp><rnn><word2vec>","<p>I am relatively new to the area of using word embeddings in NLP tasks. From a large corpus of documents, I train word2vec word embedding vectors and afterwards I am going to use these for document classification, combined with RNN based classifiers (LSTM, GRU), which is a pretty standart pipeline nowadays.</p>

<p>There is one issue; that should we update the word embeddings during the document classification training as well.</p>

<p>I am used to tasks like image classification/object detection in the past. You get an image input and the convolution features extracted from that image are updated during the numerical optimization of a CNN. But the image itself is never updated, naturally, since it is the original data.</p>

<p>How do we treat the embedding vectors in the world of text documents? They are not ""natural"" exactly like the images, we learn them from an unsupervised method first (word2vec, GloVe or any other tool) so I think they can be further fine-tuned during the supervised training. Is it the common practice to update the embedding vectors as well as the RNN parameters during the training of the sequence classifier or should we leave them as constant (in order to avoid overfitting) ?</p>
","nlp"
"38007","How to compute unseen bi-grams in a corpus (for Good-Turing Smoothing)","2018-09-09 18:08:57","38008","2","103","<nlp>","<p>Consider a (somewhat nonsensical) sentence - ""I see saw a see saw""</p>

<p>The observed bi-grams would be: 
""I see""<p>""see saw""<p>""saw a""<p>and,<p>""a see"".</p>

<p>My aim is to smoothen out the probability mass of the bi-gram probabilities by using Good-Turing smoothing. For this, I need to find the count of unseen bi-grams, i.e., bi-grams with a frequency count of 0.</p>

<p>How do I do this?</p>

<p>1) Would this be a list of all bi-grams formed by using 2 non-consecutive words? For example, ""I saw"", ""saw saw"", ""a I"", etc.?</p>

<p>2) Would repetitions of the same word be included as bi-grams? Eg. ""I I"", ""see see"", etc.?</p>
","nlp"
"37979","Check If Answer is Correct by Similarity","2018-09-08 21:16:21","","1","96","<machine-learning><nlp><ai>","<p>I am new to data science and machine learning. </p>

<p>Let's say that I have a question, and some correct answers for that question (for example, 10 correct answers). Is there a way to get a new answer as input, and ""calculate"" whether it is right? If you can recommend some readings, that would be great.</p>
","nlp"
"37824","Difference between IOB and IOB2 format?","2018-09-05 07:50:01","","4","10186","<dataset><nlp><named-entity-recognition>","<p>I have to tag a dataset for NER. I came across <a href=""https://github.com/teropa/nlp/blob/master/resources/corpora/conll2002/esp.testa"" rel=""nofollow noreferrer"">conll2002/esp</a>. What I understand so far, in IOB2 format if I want to tag '<code>Alex Larson is going to Los Angeles for a job interview with Candace Patrick</code>' it'll be like:</p>

<pre><code>Alex B-PER
Larson I-PER
is O
going O
to O
Los B-LOC
Angeles I-LOC
for O
a O
job O
interview O
with O
Candace B-PER
Patrick I-PER
</code></pre>

<p>Am I right? What about IOB format?</p>
","nlp"
"37815","What to do if training loss decreases but validation loss does not decrease?","2018-09-05 04:16:32","37816","11","19120","<deep-learning><nlp>","<p>I am training a LSTM model to do question answering, i.e. so given an explanation/context and a question, it is supposed to predict the correct answer out of 4 options. </p>

<p>My model architecture is as follows (if not relevant please ignore): I pass the explanation (encoded) and question each through the same lstm to get a vector representation of the explanation/question and add these representations together to get a combined representation for the explanation and question. I then pass the answers through an LSTM to get a representation (50 units) of the same length for answers. In one example, I use 2 answers, one correct answer and one wrong answer. From this I calculate 2 cosine similarities, one for the correct answer and one for the wrong answer, and define my loss to be a hinge loss, i.e. I try to maximize the difference between the cosine similarities for the correct and wrong answers, correct answer representation should have a high similarity with the question/explanation representation while wrong answer should have a low similarity, and minimize this loss.</p>

<p>The problem I find is that the models, for various hyperparameters I try (e.g. number of hidden units, LSTM or GRU) the training loss decreases, but the validation loss stays quite high (I use dropout, the rate I use is 0.5), e.g. </p>

<p><a href=""https://i.sstatic.net/tyQEA.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/tyQEA.png"" alt=""enter image description here""></a></p>

<p>My dataset contains about 1000+ examples. Any advice on what to do, or what is wrong?</p>
","nlp"
"37814","Extracting specific data from unstructured text - NER","2018-09-05 02:20:02","37822","1","1011","<nlp><named-entity-recognition>","<p>I have a reasonably simple problem to solve. I need to extract reservations numbers from unstructured text. Based on my research, it seems to be an NER problem. Based on a visual analysis of the dataset, I could notice that the reservation number is frequently near specific keywords, such as 'confirmation', 'reservation', 'confirmation number', 'reservations number', etc.</p>

<p>First, I decided to try a Regex rule to extract the data, but some minimum variations might render this solution inefficient. The reservation number can have very different variations, such as:</p>

<pre><code>ZXC51657856,
EA5FFD4,
45615177413515,
QT454545EF,
</code></pre>

<p>At this moment, I don't have a dataset available to train a classifier to solve this issue. </p>

<p>I would like to receive some ideas from the community to guide me towards an elegant solution to this problem, as I'm pretty new to ML in general and time is limited. </p>
","nlp"
"37752","NER: Extracting entities from an article","2018-09-03 11:54:26","37763","0","186","<machine-learning><deep-learning><nlp><data-science-model>","<p><strong>Description</strong> : I have dataset of categorised articles and to extract specific values from respective categorised article I have regex created for each category.</p>

<p><strong>Aiming for</strong>:</p>

<ol>
<li>Nlp techniques which learns the context of the content and avoids/minimizes the use of regex</li>
<li>If some new (similar) article comes up, depending on the learning (from 1) it tries to give the specific values.</li>
</ol>

<p><strong>Steps taken</strong>: </p>

<ol>
<li>Created a dataframe with various features like : 'Name of the author', 'published date', etc. and got the values from the dataset by using regex</li>
</ol>

<p><strong>Research_work</strong>:</p>

<p>I was considering these options ahead this stage :</p>

<ol>
<li>Using CNN : it will classify new articles depending on the feature values it learnt on and then use regexs for entity extraction. (It wont achieve the first aim)</li>
<li>Using CRF (<a href=""https://towardsdatascience.com/named-entity-recognition-and-classification-with-scikit-learn-f05372f07ba2"" rel=""nofollow noreferrer"">medium_article</a>): making use of POS+IOB tagging</li>
</ol>

<p>Is there any other way around ?
cons of above stated methods?</p>
","nlp"
"37714","Significance of log in DCG metric?","2018-09-02 13:22:53","","1","329","<nlp><similarity>","<p>I am trying to make logical sense about why DCG is metric is formulated as such but I am not able to understand what's the need of taking the log of <code>(1 + rank)</code> term. Can you help me undestand the significance of log here?</p>

<p><a href=""https://i.sstatic.net/NHM3y.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/NHM3y.png"" alt=""enter image description here""></a></p>
","nlp"
"37645","In natural language processing, why each feature requires an extra dimension?","2018-08-31 12:22:36","37651","2","212","<machine-learning><nlp><feature-selection>","<p>I am reading Machine Learning by Example. I am trying to understand natural language processing. The book used Scikit-learn's fetch_20newsgroups data as an example.</p>

<p>The book mentioned that the text data in the 20 newsgroups dataset that we downloaded from fetch_20newsgroups data is highly dimensional. I do not understand this statement. 
It is my understanding that dimension is used to describe axies that an array has.
For example,</p>

<pre><code>import numpy as np
np.random.seed(0)
x1 = np.random.randint(10, size=6)
print(""x1"",x1) # 1 dimensions
np.random.seed(0)
x2 = np.random.randint(10, size=(3,4))
print(""x2"",x2) # 2 dimensions
np.random.seed(0)
x3 = np.random.randint(10, size=(3,4,5))
print(""x3"",x3) #3 dimensions
</code></pre>

<p>How does no. of axies relates to feature in NLP? Why one feature equals to one dimension? Please explain. Thanks.</p>

<p>Below is the code from the book that used to download the data for your reference.</p>

<pre><code>from sklearn.datasets import fetch_20newsgroups
groups = fetch_20newsgroups()
</code></pre>
","nlp"
"37534","How many RNN units are needed for tasks involving sequences?","2018-08-28 18:12:47","","2","307","<nlp><rnn><lstm>","<p>I am training an RNN on the following task:
Given a sequence of thirty words, predict the next word.</p>

<p>Is there a benefit to having more than 30 cells (LSTM, GRU or plain RNN) in my network?<br>
I've seen many examples online where similar networks are trained with multiple layers that each have 100 cells, but this does not make sense to me.<br>
How does it help to have more cells than the length of the sequence? (in my case this length is 30)<br><br>
I'm confused because from my understanding, each cell takes in two inputs<br>
1. A new element of the sequence<br>
2. The output from the previous cell<br> So after 30 cells, there will be no new sequence elements to input into the cell. Each cell will just be processing the output of the previous cell (receiving no new info).</p>

<p>I am using LSTM cells for this task (however, I'm guessing the actual type of RNN cell used is irrelevant).</p>
","nlp"
"37500","How do I perform Sentiment Analysis on Tweets in the following pattern:","2018-08-28 06:38:59","","2","33","<machine-learning><sentiment-analysis><nlp>","<p>I have tweets obtained based on matches (football) before the match begins. I have tweets which specify a team will win 3-1 and so on which are easily analyzed using regular expressions. I am facing difficulties when it comes to tweets where two team names are specified with win / loss comparison such as:</p>

<p>Manchester United will win the match tonight against Spurs.
(or)
Spurs will find it difficult to break down United defence.</p>

<p>I have taken several such tweets and replaced the team names as TEAMONE and TEAMTWO and placed them into two files (teamone winning tweets and team two winning tweets). Using SVM, the model couldn't clearly classify tweets between the two classes (files mentioned above). Is it due to class imbalance? Should I stick to a rule based classification in this case or would any different method work? I'm a newbie in ML and any suggestions would be appreciated.</p>
","nlp"
"37489","mathematical accurate definition of the binary independence model","2018-08-27 21:19:58","37622","1","81","<nlp><probability><information-retrieval>","<p>I have a hard time understanding the exact mathematical meaning behind the binary independence model. On <a href=""https://en.wikipedia.org/wiki/Binary_Independence_Model"" rel=""nofollow noreferrer"">wikipedia</a> we can see the following definition  or similarly in the <a href=""https://nlp.stanford.edu/IR-book/html/htmledition/the-binary-independence-model-1.html"" rel=""nofollow noreferrer"">book</a> from Manning and Schütze,</p>

<p>it claims that </p>

<blockquote>
  <p>The probability P(R|d,q) that a document is relevant derives from the probability of relevance of the terms vector of that document P(R|x,q). By using the Bayes rule we get:</p>
</blockquote>

<p>$P(R|x,q) = \dfrac{P(x|R,q)P(R|q)}{P(x|q)}$ </p>

<p>Now, Bayes rule is as follows:
$P(A|B) = \dfrac{P(B|A)P(A)}{P(B)}$ </p>

<p>If you set $A := R$ and $B := x,q$ you get:
$P(A|B) = P(R|x,q) = \dfrac{P(x,q|R)P(R)}{P(x,q)}$ </p>

<p>If you compare the terms, you'll notice why I am confused:</p>

<ul>
<li>$x=x,q$</li>
<li>$R,q=q$</li>
<li>$R|q=R$</li>
<li>$x|q=x,q$ </li>
</ul>

<p>This result has nothing to do with the initial claim.
I think that I miss the definition of the 'comma' in this context. I am not aware of multi-dimensional probabilities. As I understand, a probability $P$ is always defined over a $\sigma$ algebra of an event space $\Omega$</p>

<p>In order to understand what's the idea behind the formula above, here a few things that could help:</p>

<ul>
<li>What does the comma  <em>precisely</em> mean in the formula above (in mathematical notation)?</li>
<li>What is the underlying $\Omega$ ? If there is a probability $P$, then there must be an Omega $\Omega$ which serves as the space on which we define probabilities. It's not clear at all what this space is. If a document is a vector $x \in \{{0,1}\}^n$ and the query is a vector $q \in \{{0,1}\}^n$, then defining a space like $\Omega := \{{0,1}\}^n \times \{{0,1}\}^n$ could make sense. In this case it's not clear what $R$ is. Maybe the intent is to use $\Omega := \{{0,1}\}^n \times \{{0,1}\}^n \times \{{relevant, nonrelevant}\}$</li>
<li>Do $R$, $x$ or $q$ have anything to do with random variables? If yes, then it would help to see their domain, e.g: $R : \Omega \mapsto {0,1}  $ </li>
<li>Because the conditional probability is defined between 2 sets, $R$ and $x,q$, as well as $x$ and $R,q$ or $R|q$ should represent sets. If $R$ is a random variable, then maybe in the formula above the term $R$ represents the set $\{\omega \in \Omega\ | R(\omega)=relevant\}$. What is then the set for $x$ or for $q$ ?</li>
</ul>
","nlp"
"37463","Word vectors to Sentence Vectors","2018-08-26 19:12:47","","3","3240","<nlp><word2vec><word-embeddings>","<p>How can I use the vectors of words in a sentence to get the vector of that sentence . I have used strategies like - Averaging the individual word vectors or a tf-idf weighted combination of the words . While these hacks work , there are obvious problems with these . Wanted to know what would be some other way to do this </p>
","nlp"
"37442","word2vec - log in the objective softmax function","2018-08-26 10:45:34","37458","2","531","<nlp><word2vec><mathematics>","<p>I'm reading a <a href=""https://www.tensorflow.org/tutorials/representation/word2vec#scaling_up_with_noise-contrastive_training"" rel=""nofollow noreferrer"">TensorFlow</a> tutorial on Word2Vec models and got confused with the objective function. The base softmax function is the following:</p>
<blockquote>
<p><span class=""math-container"">$P(w_t|h) = softmax(score(w_t, h) = \frac{exp[score(w_t, h)]}{\Sigma_v exp[score(w',h)]}$</span>, where <span class=""math-container"">$score$</span> computes the compatibility of word <span class=""math-container"">$w_t$</span> with the context <span class=""math-container"">$h$</span> (a dot product is commonly used). We train this model by maximizing its log-likelihood on the training set, i.e. by maximizing <span class=""math-container"">$ J_{ML} = \log P(w_t|h) = score(w_t,h) - \log \bigl(\Sigma_v exp[score(w',h)\bigr)$</span></p>
</blockquote>
<p>But why <span class=""math-container"">$\log$</span> disappeared from the <span class=""math-container"">$score(w_t,h)$</span> term?</p>
","nlp"
"37406","Classifying whether a comment or review is a complaint or appreciation of product and extracting the Topic?","2018-08-24 21:22:28","37626","0","170","<machine-learning><classification><information-retrieval><nlp>","<p>I need to classify whether a given review or comment is a complaint or appreciation. This is planned to be used in multiple places, product review pages of own site as well as facebook and twitter. Suggestions on how to approach please.</p>

<p>The Problems that are confusing me:</p>

<ol>
<li>In FB/Twitter I don't know which product it is for, I need to extract that from text as well.</li>
<li>I need to extract the complaint/appreciation part and group similar ones together, (like good color reproduction and great clarity into just good display)</li>
<li>Articles(each document) will be differently sized.</li>
<li>Data availability is none, I will prepare data by going through our FB etc.</li>
</ol>

<p>My initial thought was LSTM based classification, but point 3,4 make that hard. Even with 3,4 solved. How do I go about 1,2. I only have played with word2vec a bit and done some twitter sentiment analysis dummy projects. Point 1,2 seem Information Retrieval, Need pointers for that.</p>
","nlp"
"37368","Character Level Embeddings","2018-08-23 20:01:00","37372","4","4239","<python><nlp><word2vec>","<p>I am working on a problem that current depends on word level embeddings created using Word2Vec. I am researching new methods to apply to this model and one was a character level embedding. I have not found much information on it, and I don't imagine Word2Vec but at a character level would be effective. Is there any insight on giving vector representations to characters for an overall classification model?</p>
","nlp"
"37327","NLP how to go beyond simple intent finding--using context and targeting objects","2018-08-23 04:10:19","","0","314","<python><nlp><nltk>","<p>I'm manually applying NLP rules to a chatbot.  </p>

<p>Currently, I've a simple set of rules--actions that follow certain trigger words.<br>
Ex: <strong>""Create the match on saturday.""</strong>  </p>

<p>This has been working for relatively simple phrases like the above example, where I would expect words like ""create"" and appoint it as an <code>action</code> word, expect ""match"" then appoint it as an <code>object</code> entity; ""saturday"" as a <code>time</code>, etc.  </p>

<p>When I try to expand the scope of what the bot can handle, it becomes more complicated. Here's an example of something I'm trying to handle:  </p>

<p><strong>""Update the memo title of the match on saturday to 'Game Day'.""</strong>  </p>

<p>I'm not sure how to move forward.  </p>

<p>I considered manually expanding(expecting) the entities, then trying a method where I still parse for <code>action</code> words, but if a certain threshold of varying <code>objects</code> is passed, then I execute a subset of that <code>action</code>.   </p>

<p>For example: <code>update</code> will obviously signal an action, but the addition of ""memo"", ""title"", ""'Game day'"", signals a subset of the action as there's more to this than a simpler ""create match"". Then, checking the additional objects like ""title"" against a predetermined set of entities will narrow down the intent to <code>update</code> + <code>title</code>.    </p>

<p>I see many holes in this logic, esp. as the complexity even slightly increases.   </p>

<p>This led me to the field of dependency parsing.<br>
But upon looking into dep. parsing, I wonder if this is something feasible in manual implementation.  </p>

<p>I'll be using python code, it won't be deep learning based.  </p>

<p>What do you think of the basic rule-based method I've outlined? Is it something that sounds workable for a domain-specific bot?  </p>

<p>Should I be looking into using NLP libraries offered in Python such as NLTK or spaCy and use their features? My concern with using these features such as spaCy's dependency parser was that it was overkill, or too much added complexity for handling my domain specific tasks. Furthermore, and likely because I haven't yet seen effective use of it in another project, I guess I have doubts the practicality of dependency parsers outside of academia or research.   </p>

<p>Edit: Apologies if this isn't the proper space for this kind of question. Would appreciate if you can please point me in the right direction</p>
","nlp"
"37319","Algorithms that can determine whether a string is an English sentence?","2018-08-22 19:22:50","37664","3","2461","<machine-learning><nlp>","<p>In particular, I'm looking for something that can distinguish between complete sentences and sentence-like grammatical structures like clauses and phrases. It would also be very helpful if a library with this functionality was shared, but even a link to a paper would be appreciated.</p>

<p>For example- I would like ""False"" to be returned for strings like: </p>

<p>'Via The Daily Currant,' </p>

<p>“Waiting for the rain to stop”</p>

<p>'查看更多请返回网站主页。'</p>

<p>'*  *  *'</p>

<p>I would like ""True"" to be returned for things like:</p>

<p>""This is a complete sentence.""</p>

<p>""I don't think there will ever be a Half Life 3, unfortunately.""</p>

<p>Thank you!</p>
","nlp"
"37304","Grouping prefix and term queries","2018-08-22 14:28:00","","1","12","<machine-learning><classification><bigdata><nlp>","<p>Consider the following context:</p>

<p>You have a website that is offering a search function, which produces a list of matching articles.</p>

<p>The search has prefix functionality: if you are looking for ""Samsung Galaxy"", the user can also input ""Sams Galax"" and he will still get a list of results. Now what you try to find out, how many users are using shortened (prefix-) and how many are using full terms.</p>

<p>As your dataset you have a list of customer-executed queries which also contains a number how many articles were in the produced results. But you have no data which of the resulted articles the users finally clicked on. So you can't easily indicate whether the search was successful or not and what exactly the users were looking for.</p>

<p>How would you analyze the data to split it in the two groups, e.g. prefix- and full-term? </p>
","nlp"
"37283","parsing a simple sentence using syntaxnet (on Ubuntu)","2018-08-22 10:18:34","","0","68","<nlp><parsing>","<p>could someone help me in with the following. I would appreciate it.</p>

<p>I would like to use <code>syntaxnet</code> to analyze a sentence. I installed <strong>Ubuntu</strong> on windows following <a href=""https://github.com/xzhaos/miscellaneous/wiki/Install-SyntaxNet-on-Windows-10"" rel=""nofollow noreferrer"">this link</a>.</p>

<p>What do I have to do next to make <code>syntaxnet</code> to produce result. The sentence is for example, ""David put a book on shelf"". </p>

<p>Thank you in advance.</p>
","nlp"
"37240","can I use public pretrained word2vec, and continue train it for domain specific text?","2018-08-21 13:02:56","","5","2618","<nlp><word2vec><gensim>","<p>I have a set of reviews from apparel domain, about 100K reviews (2M words).
And I want to train word2vec to do some cool NLP staff with it.</p>

<p>However the size is not enough for creating adequate word2vec model, it requires billions of words.</p>

<p>So the idea is to use public corpora (e.g. wikipedia), or even use some pretrained models (e.g. from gensim cool framework) and add my domain specific text. 
I assume the model will get aware of unseen-in-public words, and could correct vectors for common word.</p>

<p>Does it make sense ? will the 2M words makes any effect at all ?</p>
","nlp"
"37182","how to encode labels for relationship extraction","2018-08-20 09:58:07","","1","118","<deep-learning><nlp><named-entity-recognition>","<p>I am trying to extract relationship from text. So, lets take the following text 
""He went to movie. but, they went to school""</p>

<p>So, here the relationship's are ""He and Movie"", ""they and school"". 
How can i possibly encode these as labels. my initial thought was to mark all pronouns as 1's and nouns as 2's. But, is there a better way to do this?. </p>
","nlp"
"37174","Text classification with thousands of output classes in Keras","2018-08-20 08:26:24","37270","9","9376","<machine-learning><neural-network><deep-learning><keras><nlp>","<p><strong>Task:</strong></p>

<p>I have a dataset with job titles and descriptions. The task is to predict tags for job by job title and description.</p>

<p>There are several tags for each job posting. Therefore, the number of labels for the model will be measured in tens of thousands.</p>

<p><em>Number of job postings</em> = <strong>78042</strong></p>

<p><em>Number of unique classes (tags)</em> = <strong>1369</strong></p>

<p><a href=""https://i.sstatic.net/WGOHW.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/WGOHW.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.sstatic.net/Kh0FO.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/Kh0FO.png"" alt=""enter image description here""></a></p>

<p><strong>Questions:</strong></p>

<p>Could you advise working types of neural networks (desirable in Keras)?</p>

<p>Or maybe you know how to solve this problem with the help of classical machine learning algorithms?</p>

<p>Still I will be very grateful for links to articles where similar problems are solved.</p>
","nlp"
"37167","Resource and useful tips on Transfer Learning in NLP","2018-08-20 05:12:11","","2","167","<deep-learning><nlp><word-embeddings><convolutional-neural-network><transfer-learning>","<p>I have a few label data for training and testing a DNN. Main purpose of my work is to train a model which can do a binary classification of text. And for this purpose, I have around 3000 label data and 60000 unlabeled data available to me. My data type is related to instructions (like- open the door[label-1], give me a cup of water[label-1], give me money[label-0] etc.) In this case, I heard that Transferring knowledge from other models will help me a lot. Can anyone give me some useful resource for transfer learning in NLP domain?</p>

<p>I already did a few experiments. I used GLoVE as a pretrained embeddings. Then test it with my label data. But got around 70% accuracy. Also tried with embedding built using my own data (63k) and then train the model. Got 75% accuracy on the test data. My model architecture is given below-
<a href=""https://i.sstatic.net/SptRg.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/SptRg.png"" alt=""enter image description here""></a></p>

<p><strong>Q1:</strong> I have a quick question will it be referred to as Transfer learning if I use GLOVE embeddings in model?</p>

<p>Any kind of help is welcomed. Even someone has other ideas for building a model without using transfer learning is welcomed.</p>
","nlp"
"37092","How to auto tag texts","2018-08-17 19:34:44","37110","0","550","<nlp><text-mining>","<p>Suppose we have predefined list of tags <code>Tag #1</code>, <code>Tag #2</code>, ..., <code>Tag #N</code>
and we want to assign tags to sample texts based on keywords or semantic analysis. A text can have multiple Tags. I don't want to consider it as classification problem assuming each tag as a separate class. What algorithm we can use here, Any suggestions would be of great help.</p>
","nlp"
"37073","What would be the best way to map similar ngrams","2018-08-17 08:26:25","","1","1051","<python><nlp><nltk>","<p>I'm trying to map similar ngrams using Wordnet and synsets. For example: <code>elder brother</code> and <code>older sibling</code> should map to the same entity.</p>

<p>What would be the best way to implement this? I've been thinking and so far I've only come up with a brute-force approach checking for each synset of each word and trying to find a similar word or else add them as a new entity.</p>

<p>I am wondering whether there are any better methods of implementing this?</p>

<p>for 1-gram:</p>

<pre><code>from nltk.corpus import wordnet as wn
from nltk.stem import WordNetLemmatizer
l = WordNetLemmatizer()

older = 'older'
elder = 'elder'
older_lemma = l.lemmatize(older, pos=wn.ADJ)
elder_lemma = l.lemmatize(elder, pos=wn.ADJ)


for syn in wn.synsets(older_lemma):
    if elder_lemma in syn.lemma_names():
        print(syn)
</code></pre>

<p>Ideally, I would want to extend it to n-grams and I'm searching for a better way to do this.</p>

<p>Edit : I'm not looking for vector based solutions. </p>

<p>What I'm thinking of is some sort of crude but fast similarity algorithm that can give me a crude representation of how close can two words/synsets possibly be. That way I could eliminate most of the absolutely dissimilar words saving time. I'm not sure if it exists</p>
","nlp"
"37037","Topic Segmentation - should it be done in Raw, TfIdf or Semantic Space?","2018-08-16 15:42:57","37046","0","116","<nlp><clustering><topic-model><matrix-factorisation>","<p>Let's assume we have a collection of documents and wish to perform some <strong>unsupervised</strong> topic segmentation.</p>

<p>As always, we will perform some preprocessing (including tokenization, accent-removal, lowercasing, lemmatizing and such) and transform the lists of tokens into either raw-counts or tfidf-vectors. We'll call this matrix <strong>M</strong>.</p>

<p>Now we have several possible approaches to perform some simple bag-of-words topic segmentation:</p>

<ul>
<li>Apply a matrix decomposition method (LSI, LDA, NMF) directly to M and use the resulting components as the topics.</li>
<li>Embed each vector of M into a semantic space (LSI, word2vec) and then apply a matrix decomposition method on the semantic space.</li>
<li>Apply a clustering method (kM, DBSCAN, MSC, GMM) directly to M.</li>
<li>Embed each vector of M into a semantic space and then apply a clustering method on the semantic space.</li>
</ul>

<p>I have two questions:</p>

<ol>
<li>Are there any other alternatives to bag-of-words topic segmentation that I have not considered yet?</li>
<li>What are the conceptual differences between the methods described above and which one(s) are recommended?</li>
</ol>

<p>Thanks in advance!</p>
","nlp"
"37025","NLP - How to perform semantic analysis?","2018-08-16 13:15:50","37028","5","6539","<machine-learning><python><nlp><sentiment-analysis><stanford-nlp>","<p>I'd like to perform a textual/sentiment analysis.</p>

<p>I was able to analyse samples with 3 labels: <code>(positive, neutral, negative)</code> and I used algorithms such as SVM, Random Forest, Logistic Regression and Gradient Boosting.</p>

<p>My script works correctly and with the cross validation I can take the best algorithm among the 4. </p>

<p>I use supervised algorithms with the python function ""Countvectorizer"" </p>

<p>But my boss typed ""NLP"" on the internet and looked at some articles.</p>

<p>He told me : ""These 3 outputs are not enough, I want a complete semantic analysis that can explain the global meaning of the sentence"" </p>

<p>He didn't seem to have a preference between supervised and unsupervised algorithms.</p>

<p>He told me that he wanted an algorithm able to tell that ""The company president is behind bars"" is equivalent to ""the CEO is in jail"". </p>

<p>So do you have any idea how one could perform that ? And how to implement it in Python? I guess we need a great database full of words, I know this is not a very specific question but I'd like to present him all the solutions.</p>

<p>What scares me is that he don't seem to know a lot about it, for example he told me ""you have to reduce the high dimension of your dataset"" , while my dataset is just 2000 text fields.</p>

<p>Thank you very much for your answers :)</p>
","nlp"
"36939","I need sources of interrogative, exclamatory, and imperative sentences","2018-08-14 18:18:43","","0","281","<deep-learning><nlp><dataset>","<p>I am working on accumulating a large database of labeled sentences for several projects/experiments. At present I am only using Wikipedia and Project Gutenberg as sources of data. Between these two sources I expect I will be able to extract millions of sentences. Labeling them may be another matter, but I'm less concerned about that right now. </p>

<p>My chief need is to find a broader variety of sentences. Questions, exclamations, invective, conversation, and so on. My project is several deep learning experiments to look at different types of sentences and structures. It would be helpful if the sentences were labeled. </p>

<p>The ultimate goal of my project is to be able to infer the type, structure, and coherence of a sentence (with separate models). Generating gibberish is pretty easy to do, so I'm not as concerned about finding sources of non-sense sentences. </p>

<p>My current concern is that the vast number of sentences are declarative (statements) rather than interrogatives, exclamatory or imperatives. This bias towards declarative sentences means that my sample sizes will be limited unless I can find sources of questions, exclamations and commands. </p>

<p>Alternatively, I would be open to suggestions about how to generate the data I need. </p>
","nlp"
"36873","Loss function for Hierarchical Multi-label classification","2018-08-13 12:56:04","","1","282","<neural-network><deep-learning><classification><multilabel-classification><nlp>","<p>I am looking to try different loss functions for a hierarchical multi-label classification problem. So far, I have been training different models or submodels (e.g., a simple MLP branch inside a bigger model) that either deal with different levels of classification, yielding a binary vector. I have been using BCE and summing all the losses existing in the model before backpropagating.</p>

<p>I am considering trying other losses like MultiLabelSoftMarginLoss and MultiLabelMarginLoss. What other loss functions are worth to try? hamming loss perhaps or a variation?
Is it better to sum all the losses and backpropagate or do multiple backpropagations?</p>

<p>Thanks in advance!</p>
","nlp"
"36764","ImportError: cannot import name 'StanfordCoreNLPParser'","2018-08-10 20:07:39","","0","1102","<machine-learning><python><nlp><stanford-nlp>","<p>I've been trying to extract subject-predicate-object triples from sentences and found this <a href=""https://github.com/tdpetrou/RDF-Triple-API"" rel=""nofollow noreferrer"">awesome API</a> that did just that. However, when it was written, it used the <code>StanfordParser</code> (<code>from nltk.parse import stanford, stanford.StanfordParser</code>), which is now defunct.</p>

<p>Instead, what is to be used now is the <code>StanfordCoreNLPParser</code>. Is this the right way to import it?</p>

<pre><code>from nltk.parse.corenlp import StanfordCoreNLPParser
</code></pre>

<p>Anyways, I am trying to modify it <code>rdf_triple.py</code> file (found <a href=""https://github.com/tdpetrou/RDF-Triple-API/blob/master/rdf_triple.py"" rel=""nofollow noreferrer"">here</a>) to now use the new <code>StanfordCoreNLPParser</code>, while keeping all the functionality intact. </p>

<p>My code is below:</p>

<pre><code>from nltk.parse import stanford
import os, sys
import operator
from nltk.parse.corenlp import StanfordCoreNLPParser
# java_path = r""C:\Program Files\Java\jdk1.8.0_31\bin\java.exe""
# os.environ['JAVAHOME'] = java_path
os.environ['STANFORD_PARSER'] = r'/path/stanford-parser-full-2018-02-27'
os.environ['STANFORD_MODELS'] = r'/path/stanford-parser-full-2018-02-27'


# the RDF function starts here, the problems with the code are
# above this line, I think
class RDF_Triple():

    class RDF_SOP():

        def __init__(self, name, pos=''):
            self.name = name
            self.word = ''
            self.parent = ''
            self.grandparent = ''
            self.depth = ''
            self.predicate_list = []
            self.predicate_sibings = []
            self.pos = pos
            self.attr = []
            self.attr_trees = []


    def __init__(self, sentence):
        self.sentence = sentence
        self.clear_data()


    def clear_data(self):
        self.parser = nltk.parse.corenlp.StanfordCoreNLPParser(
                path_to_jar='/path/stanford-corenlp-3.9.1-models.jar',
                path_to_models_jar='/path/stanford-corenlp-3.9.1-models.jar')
        # UPDATE THIS PARSER!!!!
        self.first_NP = ''
        self.first_VP = ''
        self.parse_tree = None
        self.subject = RDF_Triple.RDF_SOP('subject')
        self.predicate = RDF_Triple.RDF_SOP('predicate', 'VB')
        self.Object = RDF_Triple.RDF_SOP('object')      


    def find_NP(self, t):
        try:
            t.label()
        except AttributeError:
            pass
        else:
            # Now we know that t.node is defined
            if t.label() == 'NP':
                if self.first_NP == '': 
                    self.first_NP = t
            elif t.label() == 'VP':
                if self.first_VP == '':
                    self.first_VP = t
            for child in t:
                self.find_NP(child)


    def find_subject(self, t, parent=None, grandparent=None):
        if self.subject.word != '':
            return
        try:
            t.label()
        except AttributeError:
            pass
        else:
            # Now we know that t.node is defined
            if t.label()[:2] == 'NN':
                if self.subject.word == '': 
                    self.subject.word = t.leaves()[0]
                    self.subject.pos = t.label()
                    self.subject.parent = parent
                    self.subject.grandparent = grandparent
            else:
                for child in t:
                    self.find_subject(child, parent=t, grandparent=parent)


    def find_predicate(self, t, parent=None, grandparent=None, depth=0):
        try:
            t.label()
        except AttributeError:
            pass
        else:
            if t.label()[:2] == 'VB':
                self.predicate.predicate_list.append((t.leaves()[0], depth, parent, grandparent))

            for child in t:
                self.find_predicate(child, parent=t, grandparent=parent, depth=depth+1)


    def find_deepest_predicate(self):
        if not self.predicate.predicate_list:
            return '','','',''
        return max(self.predicate.predicate_list, key=operator.itemgetter(1))


    def extract_word_and_pos(self, t, depth=0, words=[]):
        try:
            t.label()
        except AttributeError:
#             print t
#             print 'error', t
            pass
        else:
            # Now we know that t.node is defined
            if t.height() == 2:
#                 self.word_pos_holder.append((t.label(), t.leaves()[0]))
                words.append((t.leaves()[0], t.label()))
            for child in t:
                self.extract_word_and_pos(child, depth+1, words)
        return words



    def print_tree(self, t, depth=0):
        try:
            t.label()
        except AttributeError:
            print(t)
#             print 'error', t
            pass
        else:
            # Now we know that t.node is defined
            print('(')#, t.label(), t.leaves()[0]
            for child in t:
                self.print_tree(child, depth+1)
            print(') ')


    def find_object(self):
        for t in self.predicate.parent:
            if self.Object.word == '':
                self.find_object_NP_PP(t, t.label(), self.predicate.parent, self.predicate.grandparent)


    def find_object_NP_PP(self, t, phrase_type, parent=None, grandparent=None):
        '''
        finds the object given its a NP or PP or ADJP
        '''
        if self.Object.word != '':
            return
        try:
            t.label()
        except AttributeError:
            pass
        else:
            # Now we know that t.node is defined
            if t.label()[:2] == 'NN' and phrase_type in ['NP', 'PP']:
                if self.Object.word == '': 
                    self.Object.word = t.leaves()[0]
                    self.Object.pos = t.label()
                    self.Object.parent = parent
                    self.Object.grandparent = grandparent
            elif t.label()[:2] == 'JJ' and phrase_type == 'ADJP':
                if self.Object.word == '': 
                    self.Object.word = t.leaves()[0]
                    self.Object.pos = t.label()
                    self.Object.parent = parent
                    self.Object.grandparent = grandparent
            else:
                for child in t:
                    self.find_object_NP_PP(child, phrase_type, parent=t, grandparent=parent)


    def get_attributes(self, pos, sibling_tree, grandparent):
        rdf_type_attr = []
        if pos[:2] == 'JJ':
            for item in sibling_tree:
                if item.label()[:2] == 'RB':
                    rdf_type_attr.append((item.leaves()[0], item.label()))
        else:
            if pos[:2] == 'NN':
                for item in sibling_tree:
                    if item.label()[:2] in ['DT', 'PR', 'PO', 'JJ', 'CD']:
                        rdf_type_attr.append((item.leaves()[0], item.label()))
                    if item.label() in ['QP', 'NP']:
                        #append a tree
                        rdf_type_attr.append(item, item.label())
            elif pos[:2] == 'VB':
                for item in sibling_tree:
                    if item.label()[:2] == 'AD':
                        rdf_type_attr.append((item, item.label()))

        if grandparent:
            if pos[:2] in ['NN', 'JJ']:
                for uncle in grandparent:
                    if uncle.label() == 'PP':
                        rdf_type_attr.append((uncle, uncle.label()))
            elif pos[:2] == 'VB':
                for uncle in grandparent:
                    if uncle.label()[:2] == 'VB':
                        rdf_type_attr.append((uncle, uncle.label()))


        return self.attr_to_words(rdf_type_attr)


    def attr_to_words(self, attr):
        new_attr_words = []
        new_attr_trees = []
        for tup in attr:
            if type(tup[0]) != str:
                if tup[0].height() == 2:
                    new_attr_words.append((tup[0].leaves()[0], tup[0].label()))
                else:
#                     new_attr_words.extend(self.extract_word_and_pos(tup[0]))
                    new_attr_trees.append(tup[0].unicode_repr())
            else:
                new_attr_words.append(tup)
        return new_attr_words, new_attr_trees

    def jsonify_rdf(self):
        return {'sentence':self.sentence,
                'parse_tree':self.parse_tree.unicode_repr(),
         'predicate':{'word':self.predicate.word, 'POS':self.predicate.pos,
                      'Word Attributes':self.predicate.attr, 'Tree Attributes':self.predicate.attr_trees},
         'subject':{'word':self.subject.word, 'POS':self.subject.pos,
                      'Word Attributes':self.subject.attr, 'Tree Attributes':self.subject.attr_trees},
         'object':{'word':self.Object.word, 'POS':self.Object.pos,
                      'Word Attributes':self.Object.attr, 'Tree Attributes':self.Object.attr_trees},
         'rdf':[self.subject.word, self.predicate.word, self.Object.word]
         }


    def main(self):
        self.clear_data()
        self.parse_tree = self.parser.raw_parse(self.sentence)[0]
        self.find_NP(self.parse_tree)
        self.find_subject(self.first_NP)
        self.find_predicate(self.first_VP)
        if self.subject.word == '' and self.first_NP != '':
            self.subject.word = self.first_NP.leaves()[0]
        self.predicate.word, self.predicate.depth, self.predicate.parent, self.predicate.grandparent = self.find_deepest_predicate()
        self.find_object()
        self.subject.attr, self.subject.attr_trees = self.get_attributes(self.subject.pos, self.subject.parent, self.subject.grandparent)
        self.predicate.attr, self.predicate.attr_trees = self.get_attributes(self.predicate.pos, self.predicate.parent, self.predicate.grandparent)
        self.Object.attr, self.Object.attr_trees = self.get_attributes(self.Object.pos, self.Object.parent, self.Object.grandparent)
        self.answer = self.jsonify_rdf()


# =============================================================================
# if __name__ == '__main__':
#     try:
#         sentence = sys.argv[1]
#         sentence = 'A rare black squirrel has become a regular visitor to a suburban garden'
#     except IndexError:
#         print(""Enter in your sentence"")
#         sentence = 'A rare black squirrel has become a regular visitor to a suburban garden'
#         print(""Heres an example"")
#         print(sentence)
# 
#     # sentence = 'The boy dunked the basketball'
#     sentence = 'They also made the substance able to last longer in the bloodstream, which led to more stable blood sugar levels and less frequent injections.'
#     sentence = 'A rare black squirrel has become a regular visitor to a suburban garden'
#     rdf = RDF_Triple(sentence)
#     rdf.main()
# 
#     ans =  rdf.answer
#     print(ans)
# =============================================================================
</code></pre>

<p>What happens when I run my code is I get the error:</p>

<blockquote>
  <p>ImportError: cannot import name 'StanfordCoreNLPParser'.</p>
</blockquote>

<p>Does anyone have an idea how to fix this?</p>
","nlp"
"36663","Under what circumstance is lemmatization not an advisble step when working with text data?","2018-08-08 22:26:50","36756","7","892","<nlp><data-cleaning>","<p>Disregarding possible computational restraints, are there general applications where lemmatization would be a counterproductive step when analyzing text data?</p>

<p>For example, would lemmatization be something that is not done when building a context-aware model?</p>

<p>For reference, lemmatization per dictinory.com is the act of grouping together the inflected forms of (a word) for analysis as a single item.</p>

<p>For example, the word 'cook' is the lemma of the word 'cooking'. The act of lemmatization is, for example, replacing the word cooking with cook after you have tokenized your text data. Additionally, the word 'worse' has 'bad' as its lemma, and as the previous example replacing the word 'worse' with 'bad' is the action of lemmatization. </p>
","nlp"
"36624","PMI between lemma vs surface","2018-08-08 06:55:19","","0","206","<nlp><probability><mutual-information>","<p>I was wondering whether it's possible to compute the some sort of pointwise mutual information between lemma and its surface form.</p>

<p>First if we assume, </p>

<pre><code>p('to go') = count('to go') / sum(all lemmas)

p('went') = count('went') / sum(all words)
</code></pre>

<p>Breakpoint here, since every word comes with its respective lemma, we have the condition that </p>

<pre><code>sum(all lemmas) == sum(all words)
</code></pre>

<p>The joint probability is also a little hard to normalize</p>

<pre><code># count of ""went"" being lemmatize to ""to go
p('went', 'to go') = count('went'-&gt;'to go') / sum(all words)
</code></pre>

<p>The catch here is since ""went"" always lemmatize to ""to go"", we have special condition of </p>

<pre><code>count('went'-&gt;'to go')  == count('went')
</code></pre>

<p>The reverse is not true though since the ""to go"" can be realized as different surface forms. </p>

<p>In that case, it's really awkward, if we put it all together</p>

<pre><code> p('went', 'to go') = p('went') = count('went') / sum(all words)
 p('to go') = count('to go') / sum(all words)


 PMI('went', 'to go') 
 =  p('went', 'to go') / (p('to go') * p('went') 
 =  p('went') / (p('to go') * p('went') 
 =  1 / p('to go') 
</code></pre>

<p>But that would assign the same information value for all surface forms given the same lemma. </p>

<p>I think I've made some mistakes in how I'm accounting for the joint or independent probabilities. Could anyone advise how to approach an information score for lemma and its surface?</p>

<p>Or is the information score between lemma and its surface uninformative thus the conclusion of inverse probability of the lemma?</p>

<hr>

<p>One possible solution is to change the normalization value for the joint probability where:</p>

<pre><code>p('went', 'to go') = count('went') / count('to go')
</code></pre>

<p>Then the PMI equation would be:</p>

<pre><code>PMI('went', 'to go') 
=  p('went', 'to go') / (p('to go') * p('went') 
=  (p('went') / (p('to go')) / (p('to go') * p('went') 
=  1 / p('to go')**2
</code></pre>

<p>Still the final mutual information disregard the surface probability =(</p>
","nlp"
"36573","Why use GAN in NLG?","2018-08-07 06:51:16","","1","510","<nlp><gan><text-generation>","<p>I am interested in the GAN recently. There are many papers that recently applied GAN to NLG.</p>

<p>I do not know much about NLP or NLG, but I wonder why I use GAN for NLG. For better quality? Or for many variations?</p>
","nlp"
"36499","What's beyond topic modeling?","2018-08-05 20:23:04","36660","2","430","<nlp><topic-model><lda>","<p>I tried topic modeling (LDA, NMF) to extract insights from the data.</p>

<p>I'm curious right now, are there other methods for unsupervised learning to cluster documents by the same or similar context? </p>

<p>(Aside) Are there any methods to show similarity of a topic or documents from topic? </p>
","nlp"
"36463","Mastering NLP: Reading List","2018-08-04 13:06:10","","8","1732","<nlp><reference-request><books>","<p>I've searched the web and there are hundreds of recommendations on what to read. The time moves on and new better quality techniques are published, so I would like to know what is relevant in 2018?</p>

<p>My background is 4 years of BSc in Maths &amp; Stats (top uni) + 1 year of role in Data Science (building predictive models, no NLP).</p>

<p>If possible, please separate it into sections/readings, e.g.</p>

<ul>
<li><p>Background (History, e.g. philosophical)</p></li>
<li><p>Theoretical (Mathematics)</p></li>
<li><p>Practical (Using Tensorflow and other NLP libraries to build algorithms)</p></li>
</ul>

<hr>

<p>I have a few side projects that I would like to do:</p>

<p><em>Build an algorithm which answers multiple choice questions</em></p>

<p>E.g. given a question:</p>

<p>Which is not a fruit?
1) Apple 2) Cucumber</p>

<p>I would like NLP to understand negation, and find that the topic of the question is fruit. Then I'd probably incorporate Google Search API or something.</p>

<hr>

<p><em>Categorise a list of 'keyword' searches into categories.</em></p>

<p>Let's take google which probably has something like this, it categorises every keyword and gives recommendations. Given a list of 10,000 searches, I would like to categorise them into N categories, based on similarity (not just how similar the words are, but including synonyms).</p>
","nlp"
"36313","How to interpret Hashingvectorizer representation?","2018-08-01 13:49:38","36357","1","1348","<nlp><text-mining><preprocessing>","<p>I cannot really understand the logic behind Hashingvectorizer for text feature extraction. I can follow the logic of Bag of Word or TFiDF where the features are values for all/certain words/N-grams per document and as such one can compute (dis)similarity between the representation vector.</p>

<ol>
<li>But how could one imagine the arbitrary number of features in case of Hashing? </li>
<li>Are these features purely synthetic features with no correlation of the original words? </li>
<li>Is it able to learn from some training set (by ""fit"") and apply this learned features to new sentences (by ""transform"") like other vectorizers? </li>
<li>Does Hashing represent any semantic distance like word2vec for words? How?</li>
</ol>
","nlp"
"36296","intent detection and slot filling using tensorflow.js","2018-08-01 08:49:03","","1","455","<machine-learning><nlp><tensorflow>","<p>I'm still getting up to speed with machine learning, but I'm aware of the papers on joint intent detection and slot filling by <a href=""https://arxiv.org/pdf/1609.01454.pdf"" rel=""nofollow noreferrer"">Bing Liu &amp; Ian Lane</a>, and another by <a href=""https://www.ijcai.org/Proceedings/16/Papers/425.pdf"" rel=""nofollow noreferrer"">Xiaodong Zhang and Houfeng Wang</a> - and I'm sure there would be others.</p>

<p>There are several implementations available on GitHub:</p>

<ul>
<li><a href=""https://github.com/brightmart/slot_filling_intent_joint_model"" rel=""nofollow noreferrer"">liu/lane by brightmart</a></li>
<li><a href=""https://github.com/HadoopIt/rnn-nlu"" rel=""nofollow noreferrer"">liu/lane by HadoopIt</a></li>
<li><a href=""https://github.com/applenob/RNN-for-Joint-NLU"" rel=""nofollow noreferrer"">liu/lane by applenob</a></li>
</ul>

<p>I'm hoping to be able to train a model using python and TensorFlow on the back-end, and then use the model in Node and browsers using <a href=""https://js.tensorflow.org/"" rel=""nofollow noreferrer"">TensforFlow.js</a></p>

<p>If I feed the training process with a limited set of patterns of the form:</p>

<pre><code>flights from @City:origin to @City:destination @Date:date
flights from @City:origin to @City:destination @DatePeriod:datePeriod
airfares from @City:origin to @City:destination
...
</code></pre>

<p>...then the vocabulary would be fairly limited:</p>

<ul>
<li>flights</li>
<li>from</li>
<li>to</li>
<li>airfares</li>
</ul>

<p>Plus known entity values:</p>

<ul>
<li>Boston, New York...</li>
<li>tomorrow</li>
<li>next week</li>
<li>saturday...</li>
<li>first...</li>
<li>january...</li>
</ul>

<p>I have a few noob questions:</p>

<ul>
<li><p>Is this strategy valid?</p></li>
<li><p>If my vocabulary is, say, 100 words then how big is the model that would have to be downloaded by the web app?</p></li>
<li><p>How difficult would it be to implement the <code>.predict()</code> side in TensorFlow.js?</p></li>
<li><p>Would there be any noticeable performance difference between the TensorFlow.js <code>.predict()</code> implementation and iterating through a list of regular expressions?</p></li>
</ul>
","nlp"
"36286","Why does joint embedding of word and images work?","2018-07-31 23:28:56","","3","352","<machine-learning><deep-learning><computer-vision><nlp>","<p>I often see some papers where the authors do point-wise multiplication of word and image embedding (e.g the image below). </p>

<p>Why does this  implementation works? I do not understand. </p>

<p><a href=""https://i.sstatic.net/xsaDy.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/xsaDy.png"" alt=""enter image description here""></a></p>
","nlp"
"36250","multiple intents for modifying an intent of a sentence?","2018-07-31 09:51:17","","3","129","<nltk><nlp>","<p>Say I have a sentence like 'I refuse to fly' or 'I'd like to fly'. I also have a sentence like 'I don't want to sit'. When training custom intents in one of the available NLU engines (rasa/wit/luis), what's the best way to go for modeling:
Naively I could have: RefuseFlyIntent,WantFlyIntent,and RefuseSit and WantSit</p>

<p>More sophisticated, have set of intents FlyIntent, SitIntent, WantIntent, RefuseIntent, and have my code process the combinations.</p>

<p>same question can apply for other cases, like how to model the difference between You Like To Fly and I Like To Fly</p>

<p>I'm sure there are known methodologies for that, wanted to understand what they are. If you could give me links to literature about it, would be great.</p>

<p>many thanks,
Lior</p>
","nlp"
"36198","Text Similarities: which nlp methods to use?","2018-07-30 14:13:16","","1","4665","<nlp><text-mining><word2vec><similarity><similar-documents>","<p>I have data where there is text for each user A visiting a business B. I want to find similarity between each user using their text. </p>

<p><strong>Question 1: Which NLP method should I start with?</strong></p>

<p>I have tried cosine similarity, I know its a great method but it's very slow in computing similarities. </p>

<p>I have thought about extracting important information from text (for example: Term Extraction, I don't want to use LDA) and computing similarities between that information. </p>

<p><strong>Question 2: Any suggestions?</strong></p>

<p><a href=""https://i.sstatic.net/TpbtE.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/TpbtE.png"" alt=""this is how text data looks like""></a></p>
","nlp"
"36082","Extracting sections from document based on list of keywords - Python","2018-07-27 08:20:14","","2","1197","<machine-learning><python><nlp><text-mining>","<p>I am new to NLP and I would like to ask how can I extract sentences from the text based on keywords that I have using Python. I created a list of keywords which will be used to extract sentences from the document.</p>

<p>If this will be a simple tokenization problem in which you will loop the list through the tokens, how can I capture synonyms or related words?</p>

<p>For example:</p>

<pre><code>Keyword: Internal business

Sentence: You can only use this software for your business only.


Keyword: Confidentiality

Sentence: Information will be kept as secure as possible.
</code></pre>

<p>I actually implemented text categorization using TF-IDF, but with small dataset and large number of keywords. I don't think this will work to.
Thanks in advance.</p>

<p>Is it possible to apply pre-trained models like word2vec?</p>

<p>Is it also possible to create a custom model that will fit my concerns?</p>
","nlp"
"36066","Can embeddings generated by word2vec be similar for words which never share any words in the same sentence?","2018-07-26 23:39:54","","2","214","<machine-learning><neural-network><nlp><word-embeddings>","<p>Is it possible for word2vec to produce similar embedding vectors for two words which never share any common words in the sentences that the words are found in?</p>

<p>Specifically, imagine I have the words A and B. Next imagine that I have words(A) and words(B), which represent the set of all words which show up in the same sentences as A and B, respectively. If the intersection of words(A) and words(B) is the null set (meaning the two words never have common words), then would it be possible for word2vec to put the embedding vectors for A and B in similar regions of the vectorspace?</p>
","nlp"
"35930","What can I use to post process an NLP tree generated from the python library `spaCy`?","2018-07-23 19:28:23","35932","2","93","<python><neural-network><nlp><classifier>","<p>Using <code>spaCy</code> as the NLP engine for a chatbot, I call <code>nlp(""Where are the apples?"").print_tree()</code> and receive:</p>

<pre><code>[{'word': 'are',
  'lemma': 'be',
  'NE': '',
  'POS_fine': 'VBP',
  'POS_coarse': 'VERB',
  'arc': 'ROOT',
  'modifiers': [{'word': 'Where',
    'lemma': 'where',
    'NE': '',
    'POS_fine': 'WRB',
    'POS_coarse': 'ADV',
    'arc': 'advmod',
    'modifiers': []},
   {'word': 'apples',
    'lemma': 'apple',
    'NE': '',
    'POS_fine': 'NNS',
    'POS_coarse': 'NOUN',
    'arc': 'nsubj',
    'modifiers': [{'word': 'the',
      'lemma': 'the',
      'NE': '',
      'POS_fine': 'DT',
      'POS_coarse': 'DET',
      'arc': 'det',
      'modifiers': []}]},
   {'word': '?',
    'lemma': '?',
    'NE': '',
    'POS_fine': '.',
    'POS_coarse': 'PUNCT',
    'arc': 'punct',
    'modifiers': []}]}]
</code></pre>

<p>I can easily enough parse out (arc, lemma) pairs for <code>where</code> (<code>advmod</code>, <code>where</code>) and (<code>apple</code>, <code>nsubj</code>), and call a function <code>where(apple)</code>.</p>

<p>However, this is a really naive way of handling the parsed tree.  Any suggestions for how to handle processing this tree?  I don't think something like a multilevel SVM would work.  Maybe a NN of some kind?</p>
","nlp"
"35921","Most Efficient Machine Learning Algorithm for Text Analysis","2018-07-23 14:56:32","46629","1","156","<deep-learning><nlp><cnn><sentiment-analysis>","<p>Looking at <a href=""http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/"" rel=""nofollow noreferrer"">Understanding Convolutional Neural Networks for NLP</a>, Convolutional Neural Networks (CNNs) seem to be suitable not only for image recognition, but also for NLP.</p>

<p>Are CNNs in general the best choice for NLP text analysis, e.g. for sentiment analysis?</p>

<p>If not, is there an overview or comparison of different algorithms with regards to recognition performance (F1 score or something similar)?</p>
","nlp"
"35806","Is there a synset for phrasal verbs?","2018-07-20 16:59:29","","0","912","<nlp><nltk><embeddings>","<p>Is there a database of phrasal verbs of similar semantics? Eg. one where querying ‘get in touch’ would return ‘get in contact with’, among other phrasal verbs of similar meanings? </p>

<p>If not, given a phrasal verb how would you obtain others of similar meanings?</p>
","nlp"
"35743","How to select features for Text classification problem","2018-07-19 16:05:32","35749","0","1827","<classification><nlp><text-mining><random-forest>","<p>I am working on a problem where we need to <code>classify</code> user query into multiple classes.</p>

<blockquote>
  <p><strong>Problem</strong>:</p>
  
  <p>Suppose we are running a website for selling products. The website has a form where the user can write any complaint or issue.</p>
  
  <p>In order to resolve users issue, we thought to classify issues into predefined classes so that we can understand what type of problems users are facing.</p>
</blockquote>

<p>Issues classes can be like</p>

<pre><code>Class 1: Payment Issues
Class 2: Registration issues
Class 3: Order booking issues
Class 4: Problem accessing website app
...
</code></pre>

<p>I am thinking to apply <code>random forest</code> but cannot decide what <code>features</code> should I select. Any suggestions would be of great help thank you</p>
","nlp"
"35602","What machine learning algorithms to use for unsupervised POS tagging?","2018-07-17 15:13:42","35604","4","1491","<machine-learning><nlp><unsupervised-learning><parsing>","<p>I am interested in an <em>unsupervised</em> approach to training a POS-tagger.</p>

<p>Labeling is very difficult and I would like to test a tagger for my specific domain (chats) where users typically write in lower cases etc. If it matters, the data is mostly in German.</p>

<p>I read about about old techniques like HMM, but maybe there are newer and better ways?</p>
","nlp"
"35572","How are natural language generation algorithms given a target","2018-07-17 04:38:42","","1","105","<nlp><nlg>","<p>I've started learning about NLP and NLG and I'm fascinated! I've been blown away by the things I've seen from NLP; but I have a few questions about NLG. All my questions boil down to this:</p>

<p>Given a network or Markov chain how does one specify what you want the system to talk about?</p>

<p>To explain this a little; if I ask my 5 year old nephew to tell me something he'll talk about his toys, or what's on TV etc. whatever he decides - but I can also ask him to talk to me about something specific eg. <em>tell me about dinosaurs</em>, then he'll give me a few sentences about his favourite dino. How does one specify they want their NLG system to talk about dinosaurs specifically once you've fed the system the whole of Wikipedia?</p>
","nlp"
"35549","Discarding rare words when comparing texts - per text, per comparison, or per codex?","2018-07-16 16:43:05","35760","0","240","<nlp><ngrams>","<p>I'm trying to compare texts (read: books) using KL divergence of N-gram usage frequency. </p>

<p>first I have to calculate the frequency of N-grams, and I see (perhaps unsurprisingly) that many of the words used are ""rare"" (for example, they appear  &lt;10 times in the text). I wish to replace these rare words with tokens (""UNK"", for unknown word).</p>

<p>Let's say I am comparing 10 texts. At what point should I count the rare words to discover which can be considered rare?
I can think of three options:</p>

<ol>
<li>Count words for each text separately. If ""Boat"" is a word is rare in the first text and frequent in second one, it is replaced with UNK only in the first text, and not in the second one.</li>
<li>Count words for each comparison. For 10 texts I'm carrying out 45 (two-sided) comparisons. If I choose this option, I will not replace the word ""Boat"" in either text when comparing the first and second texts. I will however replace it with UNK when comparing the first and third texts (assume that ""Boat"" is rare in the third text as well).</li>
<li>Count words for the entire collection. Using this option, ""Boat"" is never replaced in any of the texts, due to it being a non-rare word in one of the texts.</li>
</ol>

<p>What option would serve me better in finding meaningful N-gram representations of texts? </p>

<p>Many thanks.</p>
","nlp"
"35471","Why would you use word embeddings to find similar words?","2018-07-14 18:13:52","35472","1","2204","<nlp><word2vec><word-embeddings><stanford-nlp>","<p>One of the applications of word embeddings (such as GloVe) is finding words of similar meaning. I just had a look at some embeddings produced by glove on large datasets and I found that the nearest neighbors of a given word are often fairly irrelevant. Eg. ‘dad’ is the closest neighbor of ‘mom’, ‘dealership’ is the seventh closest neighbor of ‘car’. </p>

<p>In light of this, if you wanted to find words of similar semantics why would you prefer using embeddings instead of just downloading a database of synonyms from an online dictionary that is compiled by humans? </p>
","nlp"
"34287","Finding the most phonetically similar word from WordNet","2018-07-11 07:22:51","","2","812","<nlp><similarity>","<p>Besides soundex and other libraries, that take two words and determine whether they are similar, is there any way to find the most similar sounding word from WordNet, for a given word? I tried to use autospell as a well to correct the spelling, but unable to get the desired results.</p>
","nlp"
"34174","Is it a red flag that increasing the number of parameters makes the model less able to overfit small amounts of data?","2018-07-09 01:24:01","","4","166","<deep-learning><nlp><lstm><cnn><named-entity-recognition>","<p>I'm training a deep network (CNN-LSTM-CRF) for Named Entity Recognition. Is there a reason that increasing the number of parameters would make the network less able to overfit a small training set (~20 sentences), or does this indicate a serious bug in the code?</p>
","nlp"
"34110","Extracting specific portions of text from poorly formatted document?","2018-07-06 16:02:23","","0","279","<nlp><text-mining>","<p>I have a corpus of text files (free books) which are poorly formatted.  The goal is to extract a particular chapter (say chapter 2) from the raw text with all weird formatting removed.  Some documents are literal copy and pastes from pdfs, so that the text contents from a chapter would have page numbers interspersed throughout, etc., and I.e., it would be impossible to create a good regex rule.</p>

<p>I've manually gone through a number of documents and stripped out all unwanted formatting to arrive at the desired, clean section of text.  Using this as the training set, what would be a good model to set up for this task?</p>

<p>To clarify, the input would be a (very long) string, and the output would also be a string.</p>

<p><strong>EDIT</strong>:</p>

<p>Please note that this particular example can be very easily handled by a series of regex rules, but it's only for illustration purposes only.  Imagine if each document had their own minor idiosyncrasies making it very difficult to come up with a general series of regex rules to apply to all docs in the corpus.</p>

<p>Sample input:</p>

<pre><code>Book Title ABC

Table of Contents

Preface ................................ Pg 1
Chapter 1 .............................. Pg 2
Chapter 2 .............................. Pg 3
Chapter 3 .............................. Pg 4

Preface

&lt;p&gt; Contents for the preface sit here.  Lorem ipsum dolor sit amet, consectetur adipiscing elit. Suspendisse scelerisque turpis ac commodo porttitor. Suspendisse iaculis lacinia ante, non suscipit purus venenatis et. &lt;/p&gt;

- 1 -

Leftover Contents from Preface

Chapter 1

Praesent at justo sit amet nulla porttitor hendrerit. Etiam pretium sem ac feugiat posuere. Nulla mi risus, mollis sit amet ligula egestas, lacinia dapibus tellus. Proin iaculis ligula ultricies nunc elementum maximus. Nulla accumsan libero at nunc scelerisque, ut fermentum lacus euismod. Nulla imperdiet pharetra laoreet. Quisque id mollis diam. Vestibulum interdum orci at lectus ullamcorper pharetra. Curabitur laoreet mollis pharetra.

- 2 -

Chapter 2

Cras diam nibh, congue sit amet imperdiet quis, finibus imperdiet tellus. Sed imperdiet risus id elit consequat cursus. Curabitur consequat facilisis molestie. Ut varius urna vel ornare scelerisque. Nullam dictum tempus sapien, in pulvinar tellus consectetur non. Vestibulum consequat pretium iaculis.

- 3 -

Leftover contents from Chapter 2.  Vivamus pretium mauris at metus egestas, eget luctus ligula auctor. 

Chapter 3

Maecenas vel dapibus lorem. Sed eget justo sit amet libero aliquam maximus a quis augue. Nunc consequat, urna quis elementum condimentum, quam arcu consectetur tellus, vehicula gravida est purus id nisl.

- 4 -

Nulla eget molestie velit, pharetra euismod magna. Vestibulum ultricies justo vitae massa vulputate, et venenatis orci fringilla.

Index
</code></pre>

<p>Sample Output:</p>

<pre><code>Cras diam nibh, congue sit amet imperdiet quis, finibus imperdiet tellus. Sed imperdiet risus id elit consequat cursus. Curabitur consequat facilisis molestie. Ut varius urna vel ornare scelerisque. Nullam dictum tempus sapien, in pulvinar tellus consectetur non. Vestibulum consequat pretium iaculis.  
Leftover contents from Chapter 2.  Vivamus pretium mauris at metus egestas, eget luctus ligula auctor. 
</code></pre>
","nlp"
"34086","How to detect phrases from an English sentence.","2018-07-06 08:50:46","","0","2850","<nlp><text-mining><preprocessing>","<p>The question is not about <em>detecting keyphrases</em>. It is about detecting a combination of words makes a valid phrase or not. For example,</p>
<blockquote>
<p>&quot;John reads New York Times in New York.&quot;</p>
</blockquote>
<p>Here, the phrases are</p>
<blockquote>
<p>New York Times</p>
<p>New York</p>
</blockquote>
<p>Detecting <em>keyword phrases</em> is a <strong>Text Summarization</strong> problem, however, here it is classifying whether a combination of words make a valid phrase or not.</p>
<p>There have been a few algorithms we have gone through including but not limited to <strong>models.phrases – Phrase (collocation) detection</strong> of <strong>Gensim</strong>, however, we are looking for better results.</p>
","nlp"
"34042","Question classification","2018-07-05 14:08:41","","1","97","<deep-learning><multiclass-classification><multilabel-classification><nlp>","<p>I have 10 classes and 10-15 questions in each class .
Given a new question, I want to find the class to which the question is most similar?</p>
","nlp"
"34039","regex to remove repeating words in a sentence","2018-07-05 13:14:49","34045","4","15158","<python><nlp><regex>","<p>I am new to regex. I am working on a project where i need to replace repeating words with that word. for example:</p>

<blockquote>
  <p>I need need to learn regex regex from scratch.</p>
</blockquote>

<p>I need to change it to:</p>

<blockquote>
  <p>I need to learn regex from scratch.</p>
</blockquote>

<p>I can identify the repeating words using the following regex
<code>\b(\w+)\b[\s\r\n]*(\l[\s\r\n])+</code></p>

<p>For substituting it, I need the word in the repeated word phrase. 
<code>pattern.sub(sentence, &lt;what do i write here?&gt;)</code></p>
","nlp"
"34003","Text Classification with deep learning","2018-07-04 20:41:47","","0","219","<deep-learning><nlp>","<p>I  have questions of users and I want to classify them automatically without manually labelling them. What deep learning method would be good for text classification just from text (so unsupervised). </p>

<p>Does those algorithms have to rely on word embedding? </p>
","nlp"
"34001","How to find impactful words affecting classification?","2018-07-04 20:34:41","","1","392","<machine-learning><neural-network><nlp>","<p>So I know there are many methods to classify sentences into types. Like in sentiment analysis (positive, negative, neutral), spam emails (spam, not spam), etc. The thing I want to ask is how would I find the words most responsible for the categorisation. For example: <code>sad-negative, happy-positive, the-no information, are-no information</code>.</p>

<p>So how do I find the words which impact the classification?</p>
","nlp"
"33974","Chat Bot Answering based on Data Corpus Self-Training","2018-07-04 09:15:08","","0","1476","<nlp><dataset><unsupervised-learning>","<p>I have created a very simple chat bot based on RASA NLU. In this case, I manually create some sample input text and create a model for using it against unknown source of input. It's fine for now.</p>

<p>As my next step of learning, I want to do use a big documnet as source for my chat bot. </p>

<p>What steps should I do to make the program train automatically on the documnet text corpus and able to answer based on the user query? I want to avoid the manual training.</p>

<p>For this NLU problem statement, what libraries can I use? </p>

<p>Almost all internet sources talk about SQuaD models. How they are related to a custom domain training?</p>

<p>Any blogs, tutorials, libs that can help me to do this will be useful.</p>

<p><strong>Some other related questions but without solid answers:</strong></p>

<p><a href=""https://datascience.stackexchange.com/questions/16646/build-knowledge-bot-using-deep-learning"">Build knowledge bot using deep learning</a></p>
","nlp"
"33958","Rasa_Nlu SpaCy installing dependencies","2018-07-04 00:03:57","33963","-3","675","<python><nlp>","<p>I'm trying to do some intent extraction/recognition. I've installed all dependancies (I believe) but it still gives me the error: </p>

<pre><code>File ""C:\Users\user\.spyder-py3\chatbot\Outlook\rasa_nlu\components.py"", line 65, in validate_requirements
    ""Please install {}"".format("", "".join(failed_imports)))

Exception: Not all required packages are installed. To use this pipeline, 
you need to install the missing dependencies. Please install
sklearn_crfsuite, spacy
</code></pre>

<p>Can someone please share their knowledge on how I can resolve this? </p>

<p>Here is my pip list:</p>

<pre><code>absl-py (0.1.13)
asciitree (0.3.3)
asn1crypto (0.24.0)
astor (0.6.2)
atomicwrites (1.1.5)
attrs (18.1.0)
Automat (0.6.0)
beautifulsoup4 (4.6.0)
bleach (1.5.0)
boto (2.48.0)
boto3 (1.5.20)
botocore (1.8.50)
bz2file (0.98)
certifi (2018.4.16)
cffi (1.11.5)
chardet (3.0.4)
cloudpickle (0.5.2)
colorama (0.3.9)
coloredlogs (9.0)
conda (4.4.10)
constantly (15.1.0)
cryptography (2.1.4)
cycler (0.10.0)
cymem (1.31.2)
Cython (0.27.2)
cytoolz (0.8.2)
decorator (4.2.1)
dill (0.2.8.2)
docutils (0.14)
duckling (1.8.0)
en-core-web-md (2.0.0)
en-core-web-sm (2.0.0)
et-xmlfile (1.0.1)
ftfy (4.4.3)
future (0.16.0)
gast (0.2.0)
gensim (3.4.0)
gevent (1.2.2)
gitdb2 (2.0.3)
GitPython (2.1.9)
greenlet (0.4.13)
grpcio (1.10.0)
html5lib (1.0.1)
humanfriendly (4.12.1)
hyperlink (17.3.1)
idna (2.6)
incremental (17.5.0)
jdcal (1.3)
jmespath (0.9.3)
JPype1 (0.6.3)
jsonschema (2.6.0)
kiwisolver (1.0.1)
klein (17.10.0)
koala2 (0.0.17)
lxml (4.2.2)
Markdown (2.6.11)
matplotlib (2.1.0)
menuinst (1.4.11)
mkl-fft (1.0.0)
mkl-random (1.0.1)
mock (2.0.0)
more-itertools (4.2.0)
msgpack-numpy (0.4.1)
msgpack-python (0.5.4)
murmurhash (0.28.0)
networkx (1.9)
nltk (3.2.5)
numpy (1.14.0)
oauthlib (2.0.7)
openpyxl (2.4.9)
packaging (17.1)
pandas (0.22.0)
pandas-datareader (0.6.0+21.gda18fbd)
pathlib (1.0.1)
pbr (4.0.4)
pip (9.0.1)
plac (0.9.6)
pluggy (0.6.0)
preshed (1.0.0)
protobuf (3.5.2.post1)
py (1.5.4)
py4j (0.10.6)
pyasn1 (0.4.3)
pyasn1-modules (0.2.2)
pycosat (0.6.3)
pycparser (2.18)
pynput (1.3.10)
pyOpenSSL (17.5.0)
pyparsing (2.2.0)
pyreadline (2.1)
PySocks (1.6.7)
pyspark (2.3.0)
pytest (3.6.2)
python-crfsuite (0.9.5)
python-dateutil (2.7.2)
pytz (2018.4)
pywin32 (223)
PyYAML (3.12)
rasa-nlu (0.13.0a2, c:\users\users\rasa_nlu)
regex (2017.4.5)
requests (2.18.4)
requests-file (1.4.3)
requests-ftp (0.3.1)
requests-oauthlib (0.8.0)
ruamel-yaml (0.15.35)
s3transfer (0.1.13)
scikit-learn (0.19.1)
scipy (1.1.0)
selenium (3.11.0)
service-identity (17.0.0)
setuptools (38.4.0)
simplejson (3.13.2)
six (1.11.0)
sklearn-crfsuite (0.3.6)
smart-open (1.5.7)
smmap2 (2.0.3)
spacy (2.0.11)
tabulate (0.8.2)
tensorboard (1.7.0)
tensorflow (1.7.0)
termcolor (1.1.0)
textblob (0.15.1)
thinc (6.10.2)
toolz (0.9.0)
tqdm (4.19.5)
tweepy (3.6.0)
Twisted (18.4.0)
typing (3.6.2)
ujson (1.35)
urllib3 (1.22)
wcwidth (0.1.7)
webencodings (0.5.1)
Werkzeug (0.14.1)
wheel (0.30.0)
win-inet-pton (1.0.1)
wincertstore (0.2)
wrapt (1.10.11)
xlrd (1.1.0)
XlsxWriter (1.0.2)
zope.interface (4.5.0)
</code></pre>
","nlp"
"33875","Tools and Techniques for Analyzing German Automotive Discussion Forum Posts","2018-07-02 10:35:01","33880","0","50","<machine-learning><tensorflow><nlp>","<p>I work for a <a href=""https://www.motor-talk.de/"" rel=""nofollow noreferrer"">German online disussion forum around all things automotive</a>, a bit like a “StackOverflow for cars”, if you will.</p>

<p>We would like to train a model using TensorFlow with our high quality content, to be able to evaluate the quality of new content our users post on our platform.</p>

<p>Our ultimate goal is to be able to put a link to the best answer to a question on our discussion forums. </p>

<p>We (two backend Java developers and myself, a JavaScript frontend web developer) are very new to the field of data science and machine learning, currently going through the tutorials from Google and trying to figure out where to start.</p>

<p><strong>Which tools and technologies would you recommend to use for this project?</strong></p>

<p>How can we train a model to suit our needs?</p>

<p>Are there any tutorials that demonstrate how to train a model to use German text as input?</p>
","nlp"
"33792","How to retrain Glove Vectors on top of my own data?","2018-06-29 07:30:01","","2","7866","<machine-learning><nlp><word-embeddings>","<p>I am using GloVe and gensim for my project. I have a corpus of data (let's say mydata.txt) which has new words which are not in the existing Glove. So, how do I retrain glove so that the existing pre-trained glove must now include the new words on my corpus mydata.txt? I have been struggling and failed to find the solution for 2 weeks. The only similar post I found is this <a href=""https://stackoverflow.com/questions/43618145/improving-on-the-basic-existing-glove-model"">Improving existing GloVe Model</a> </p>
","nlp"
"33772","what is the reason behind the bad outputs gained by RNN, LSTM when using GloVe pretrained model in text classification?","2018-06-28 14:45:43","","1","177","<nlp><word2vec><word-embeddings><sentiment-analysis>","<p>the problem is with the results gained for accuracy and f1  afer training our model via pretrained models such as GloVe.
when I apply CNN as a classifier, the result are good as follows:</p>

<pre><code>acc: 0.9345 - val_loss: 0.1513 
</code></pre>

<p>but when I apply RNN and LSTM as a classifier the results will be as follows:</p>

<pre><code>24931/24931 [==============================] - 188s 8ms/step - loss: 7.9559 - acc: 6.0166e-04 - val_loss: 7.9904 - val_acc: 0.0000e+00
Epoch 2/4
24931/24931 [==============================] - 189s 8ms/step - loss: 7.9645 - acc: 0.0000e+00 - val_loss: 7.9904 - val_acc: 0.0000e+00
</code></pre>

<p>the above result is reached both via RNN and LSTM.</p>

<p>the problem is that I use the same data set and the same structure and the same GloVe but I reach acc: 0.9345 for CNN and gain acc: 0.0000e+00 for both LSTM and RNN.
It is worth noting that I have changed optimizer but still get the same result. the applied dataset contains 41,399 items, totaling 60.3 MB and also is binary.
any guidance will be appreciated as I am a beginner in working with GloVe.</p>

<p>I apply keras with tensorflow backend with python 3.5 in ubuntu.</p>
","nlp"
"33730","Should I rescale tfidf features?","2018-06-27 16:30:43","43043","8","8875","<nlp><feature-engineering><feature-scaling><tfidf>","<p>I have a dataset which contains both text and numeric features.</p>

<p>I have encoded the text ones using the TfidfVectorizer from sklearn.</p>

<p>I would now like to apply logistic regression to the resulting dataframe.</p>

<p>My issue is that the numeric features aren't on the same scale as the ones resulting from tfidf. </p>

<p>I'm unsure about whether to:</p>

<ul>
<li><p>scale the whole dataframe with StandardScaler prior to passing to a classifier;</p></li>
<li><p>only scale the numeric features, and leave the ones resulting from tfidf as they are.</p></li>
</ul>
","nlp"
"33726","Is there any named entity reconginition algorithm trained for the french language?","2018-06-27 14:16:32","33727","2","2117","<classification><nlp><named-entity-recognition><stanford-nlp>","<p>I am trying to implement a utility for my mobile application to perform some actions based on user questions. I need an algorithm to extract named entities from a text string (French grammar). I have used <code>nltk</code>'s interface to Stanford's NER models but it works only for English (A subset of other languages is supported but i can't find French). I have also used <code>Polyglot</code> but it seems that it doesn't do the work very well (Maybe the models I am using are not very well trained). I don't know if there is any free REST API that can do NER for the French language or any other algorithm or even an already trained model for <code>nltk</code>/<code>Stanford NER</code>. </p>
","nlp"
"33644","Matching similar strings","2018-06-25 21:46:21","","1","300","<nlp>","<p>I have a list of conferences on different topics, e.g. </p>

<pre><code>Conference on genomics and neurosciences
Advances in string theory and astrophysics 
Genomics and neuroscience: 20 years of research
Swiss Physics society meeting on string theory and astrophysics
...
</code></pre>

<p>They fall into different classes, like 1 and 3, 2 and 4 together. What is the right tool to group those titles?</p>
","nlp"
"33543","Data scraping & NLP?","2018-06-23 09:14:06","","1","405","<python><nlp><csv><scraping>","<p>I'm scraping data from Bing search results for (non-commercial purposes, of course) on Python using BeautifulSoup. I've entered an Indian dessert name, called 'rasmalai' as the word that I am focusing on. The code I'm using returns the title and a description of the web page. I've also extracted the links for the results. Here is the code I used: </p>

<pre><code>from bs4 import BeautifulSoup
import urllib, urllib2

def bing_search(query):
    address = ""http://www.bing.com/search?q=%s"" % (urllib.quote_plus(query))

    getRequest = urllib2.Request(address, None, {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_2) AppleWebKit/537.36 Chrome/65.0.3325.162 Safari/537.36'})

    urlfile = urllib2.urlopen(getRequest)
    htmlResult = urlfile.read(200000)
    urlfile.close()

    soup = BeautifulSoup(htmlResult)

    [s.extract() for s in soup('span')]
    #unwantedTags = ['a', 'strong', 'cite']
    #for tag in unwatedTags:
        #for match in soup.findAll(tag):
           # match.replaceWithChildren()

    results = soup.findAll('li', {""class"" : ""b_algo"" })
    for result in results: 
        print ""# TITLE: "" + str(result.find('h2')).replace("" "", "" "") + ""\n#""
        print ""# DESCRIPTION: "" + str(result.find('p')).replace("" "", "" "")
        print ""# ___________________________________________________________\n#""

    return results

if __name__ == '__main__':
    links = bing_search('rasmalai')
</code></pre>

<p>Now that I have the links, web page title, and a short description, I want to extract keywords using NLP. In the end, I'd like to produce a CSV file with the dish name and associated keywords. Could someone guide me to some resources on how to do this part? </p>

<p>Thank you so much in advance. </p>
","nlp"
"33542","Information retrieval / slot filling / NLP","2018-06-23 07:47:48","","0","561","<nlp><information-retrieval>","<p>Excuse if this has been answered before.</p>

<p>I need to extract features and parse from a piece of text and run some analysis. For e.g. ""Plot the past 5-year sales of Apple"" should give me the following</p>

<p>Information:</p>

<ul>
<li>Company: Apple</li>
<li>Item: Sales</li>
<li>Period: past 5 years</li>
<li>Action: Plot</li>
</ul>

<p>What deep learning techniques / algorithms should I be looking to use?</p>

<p>Any pointers are highly appreciated.</p>
","nlp"
"33480","Predicting a new document","2018-06-21 17:51:43","","1","31","<python><prediction><nlp><tfidf>","<p>I have a document, (purchase agreement) of approx. 100 pages. This document is sent from buyer to seller several times, and each time there is a negotiation. Negotiation could be anything.</p>

<hr>

<p>For eg. Change in the amount of Insurance cover, or the definition of any term could be changed.
To be more specific, buyer claims the loss covered to be 10,000 dollars (Version 1 of agreement). Seller changes the agreement to cover a loss of $5,000  (Version 2). This way there is a change in the original document.</p>

<hr>

<p>QUESTION: Is there any way by which I can predict what could be the negotiation? Can I use NLP or tf-idf to track the common changes that occurs in the document and then predict it on the new document?</p>
","nlp"
"33394","how to input the data set in to a word2vec by keras?","2018-06-19 20:05:20","33466","1","847","<nlp><keras><word2vec><word-embeddings><sentiment-analysis>","<p>I am new in using word2vec model, as a result, I do not know how I can prepare my dataset as an input for word2vec? 
I have searched a lot but the datasets in tutorials were in CSV format or just one txt file, but my dataset is in this structure: 2 folders one of these is blood cancer and the other one is breast cancer.
each folder contains 1000 txt files which contain 40 sentences.
I do not have any idea about I can create a vocabulary as an input for the word2vec model in keras with tensorflow backend? 
I use python 3.5 in ubuntu 17.10
Any guidance will be appreciated.</p>
","nlp"
"33386","Text extraction from documents using NLP or Deep Learning","2018-06-19 16:09:57","33585","8","18713","<deep-learning><nlp><text-mining><reinforcement-learning><named-entity-recognition>","<p>I am looking for references(Papers/github projects) on how to use deep learning in a text extraction task.</p>

<p>Recently I was given a task to extract important information from documents of similar type, say for example legal merger documents. I have thousands of legal merger documents as inputs. A paralegal would go through the entire document and highlight important points from the document. This is the extracted text. </p>

<p>What I want to do: Given a document(say legal merger document) I want to use DL or NLP  to extract the information from the legal document that would be similar to that of the information extracted by paralegal.</p>

<p>I am currently using bag of words model to extract text from the document, calculating sentiment and displaying the sentences with positive or negative sentiments. This yielded very bad results.</p>

<p>My knowledge in DL/NLP is very limited and I am particularly looking for some interesting papers and github projects related to text extraction using these frameworks. Can anyone please provide me with some references and suggestions on how to tackle this issue?</p>
","nlp"
"33347","Vertical and horizontal lines appearing on large confusion matrix?","2018-06-19 01:21:12","33349","0","1374","<python><scikit-learn><nlp><pandas><smote>","<p>I have produced a large heatmap-like confusion matrix and am seeing horizontal and vertical lines on it, so I'm trying to determine:</p>

<ol>
<li>What they mean</li>
<li>Why they are there</li>
<li>How I can improve on this</li>
</ol>

<p><a href=""https://i.sstatic.net/jVlSj.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/jVlSj.png"" alt=""Confusion Matrix Heatmap""></a></p>

<hr>

<p><strong>Approach</strong></p>

<p>I am relatively new to ML and in the early stages of of a multi-class text classification problem.  I may be a little verbose so you can ensure I'm on track and my question isn't due to a flaw in my approach.</p>

<p>I have 90,000+ samples that I'd like to be able to classify into one of 412 classes.  I've taken a basic look at the data in terms of its class distribution and the unigrams and bigrams that are selected for each class.  Continuing exploration, I trained 4 classifiers on the data, receiving the following levels of accuracy:</p>

<pre><code>LinearSVC                 0.547190
LogisticRegression        0.530063
MultinomialNB             0.368121
RandomForestClassifier    0.200568
</code></pre>

<p>Having had a lot of trouble plotting a confusion matrix this large with Seaborn or Matplotlib, I used used the following python code to produce a confusion matrix in CSV:</p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.svm import LinearSVC

def make_confusion_matrix(a,p,c):
    cm = pd.DataFrame(0,index=c,columns=c)
    for count in range(len(p)):
        cm[int(a[count])][int(p[count])]+=1
    return cm

tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')
features = tfidf.fit_transform(df['DetailedDescription'])

model = LinearSVC()
X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(features, df['BreakdownAgency'], df.index, test_size=0.33, random_state=0)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)    

cm = make_confusion_matrix(y_test.tolist(),y_pred,labels_df['TOOCS Breakdown Agency'])
cm.to_csv('ConfusionMatrix.csv')
</code></pre>

<p>I was finally able to view the confusion matrix in a heatmap style by using Excel conditional formatting, which produced the matrix above.</p>

<p><strong>Interpretation</strong></p>

<p>Given that the X axis is <em>actual</em> and y axis is <em>predicted</em>:</p>

<p>I interpret the horizontal lines as showing incorrect bias of predictions towards a class with a disproportionately large number of samples?</p>

<p>I interpret the vertical lines as showing incorrect predictions away from a class with a disproportionately large number of samples?</p>

<p>Does this show that the model is both overfitting and underfitting the data?
Or that the samples within my classes are overly diverse?</p>

<p><strong>Action</strong></p>

<p>I'm contemplating:</p>

<ol>
<li>Manually adding samples to the classes that have very few (a minimum of 10?).</li>
<li>Using SMOTE to oversample small classes (knn=6).</li>
<li>Potentially removing some samples that are atypical or incorrect.</li>
</ol>

<p>Any help on my <em>Interpretation</em> or <em>Action</em> would be greatly appreciated!</p>
","nlp"
"33326","Entity Recognition in Stanford NLP using Python","2018-06-18 17:51:28","","1","3389","<python><nlp><stanford-nlp>","<p>I am using Stanford Core NLP using Python. I have taken the code from <a href=""https://www.khalidalnajjar.com/setup-use-stanford-corenlp-server-python/"" rel=""nofollow noreferrer"">here</a>.</p>
<p>This is the code:</p>
<pre><code>from stanfordcorenlp import StanfordCoreNLP
import logging
import json


class StanfordNLP:
def __init__(self, host='http://localhost', port=9000):
    self.nlp = StanfordCoreNLP(host, port=port,
                               timeout=30000 , quiet=True, logging_level=logging.DEBUG)
    self.props = {
        'annotators': 'tokenize,ssplit,pos,lemma,ner,parse,depparse,dcoref,relation,sentiment',
        'pipelineLanguage': 'en',
        'outputFormat': 'json'
    }

def word_tokenize(self, sentence):
    return self.nlp.word_tokenize(sentence)

def pos(self, sentence):
    return self.nlp.pos_tag(sentence)

def ner(self, sentence):
    return self.nlp.ner(sentence)

def parse(self, sentence):
    return self.nlp.parse(sentence)

def dependency_parse(self, sentence):
    return self.nlp.dependency_parse(sentence)

def annotate(self, sentence):
    return json.loads(self.nlp.annotate(sentence, properties=self.props))

@staticmethod
def tokens_to_dict(_tokens):
    tokens = defaultdict(dict)
    for token in _tokens:
        tokens[int(token['index'])] = {
            'word': token['word'],
            'lemma': token['lemma'],
            'pos': token['pos'],
            'ner': token['ner']
        }
    return tokens

if __name__ == '__main__':
sNLP = StanfordNLP()
text = r'China on Wednesday issued a $50-billion list of U.S. goods  including soybeans and small aircraft for possible tariff hikes in an escalating technology dispute with Washington that companies worry could set back the global economic recovery.The country\'s tax agency gave no date for the 25 percent increase...'
ANNOTATE =  sNLP.annotate(text)
POS = sNLP.pos(text)
TOKENS = sNLP.word_tokenize(text)
NER = sNLP.ner(text)
PARSE = sNLP.parse(text)
DEP_PARSE = sNLP.dependency_parse(text)    
</code></pre>
<p>I am only interested in Entity Recognition which is being saved in the variable NER. The command NER is giving the following result:</p>
<p><a href=""https://i.sstatic.net/KuPwA.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/KuPwA.png"" alt=""NER"" /></a></p>
<p>The same thing if I run on <a href=""http://nlp.stanford.edu:8080/corenlp/process"" rel=""nofollow noreferrer"">Stanford Website</a>, the output for NER is:
<a href=""https://i.sstatic.net/MD0LG.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/MD0LG.png"" alt=""NER Stanford"" /></a></p>
<p>There are 2 problems with my Python Code:</p>
<p><strong>1.</strong> '$' and '50-billion' should be combined and named a single entity.
Similarly, I want '25' and 'percent' as a single entity as it is showing in the online stanford output.<br />
<strong>2.</strong>  In my output, 'Washington' is shown as State and 'China' is shown as Country. I want both of them to be shown as 'Loc' as in the stanford website output. The possible solution to this problem lies in the <a href=""https://github.com/stanfordnlp/CoreNLP/tree/master/doc/ner"" rel=""nofollow noreferrer"">documentation</a> .
<a href=""https://i.sstatic.net/yBzgK.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/yBzgK.png"" alt=""documentaion"" /></a></p>
<p>But I don't know which model am I using and how to change the model.</p>
","nlp"
"33151","Correcting ALL CAPS for human and algorithmic consumption","2018-06-14 12:47:44","33181","4","48","<nlp><stanford-nlp>","<p>United States federal tax returns tend to be written in ALL CAPS to facilitate OCR. This practice has persisted even when returns are filed electronically. Thus, much of the text in the <a href=""https://registry.opendata.aws/irs990/"" rel=""nofollow noreferrer"">IRS 990 dataset</a> is in all caps. This makes it hard to read, and limits the ability of algorithms such as <a href=""https://en.wikipedia.org/wiki/Treebank"" rel=""nofollow noreferrer"">Treebank</a> to accurately tag part of speech.</p>

<p>I understand that the approach of the Stanford POS tagger may be more amenable to correction of capitalization, but in practice, I have not had much luck in using it to correct the text in the IRS 990 corpus, in which nearly every sentence contains one or more proper nouns.</p>

<p>Are there any ""tricks of the trade"" for improving the performance of an off-the-shelf POS tagger when using ALL CAPS text, and/or an algorithm that may do better at identifying the proper nouns therein? </p>
","nlp"
"33026","One Class Classification","2018-06-12 12:56:55","","1","234","<machine-learning><python><classification><nlp><text-mining>","<p>I was reading about One Class Classification but had two doubts -</p>

<p>1) How does One Class Classification work because the training data is of only a particular class so in that case, during testing, the model would always predict that any test data belongs to that class only.</p>

<p>2) And since it does work then is it not perfect for Spam Detection because we can have data for spam emails but how could we find any amount of training data on Non Spam to possibly cover all cases for Non Spam E-mails.
As any general email is a non spam one and only certain mails are spam.</p>
","nlp"
"33019","NLP - extract sentence parts related to people","2018-06-12 11:57:36","33134","0","193","<machine-learning><nlp><text-mining><language-model>","<p>Thank you for your help, I appreciate your time.</p>
","nlp"
"33006","How to calculate perplexity of language model?","2018-06-12 09:17:56","56475","3","1597","<nlp>","<p>In one of the <a href=""http://spark-public.s3.amazonaws.com/nlp/slides/languagemodeling.pdf"" rel=""nofollow noreferrer"">lecture on language modeling</a> about calculating the perplexity of a model by Dan Jurafsky in his <a href=""https://web.stanford.edu/~jurafsky/NLPCourseraSlides.html"" rel=""nofollow noreferrer"">course on Natural Language Processing</a>, in slide number 33 he give the formula for perplexity as </p>

<p><a href=""https://i.sstatic.net/ZDvqc.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ZDvqc.png"" alt=""enter image description here""></a></p>

<p>Then, in the next slide number 34, he presents a following scenario:</p>

<p>""If a system has to recognize
• Operator (1 in 4)
• Sales (1 in 4)
• Technical Support (1 in 4) 
• 30,000 names (1 in 120,000 each)""</p>

<p>Perplexity in this case is 53.</p>

<p>Can anyone explain how the answer 53 came ?</p>
","nlp"
"32926","NLP: To remove verb and find the match in a sentence","2018-06-11 05:53:16","","1","1075","<nlp><nltk>","<p>Is there a NLP method like stemming, lemmatisation to figure out the below?</p>

<pre><code>1=2 
3=4

1) provide technical documentation
2) technical documentation
3) use software design patterns
4) software design patterns
</code></pre>
","nlp"
"32859","Is it valid to include your validation data in your vocabulary for NLP?","2018-06-08 22:55:27","32914","6","2151","<machine-learning><scikit-learn><nlp><information-theory>","<p>At the moment, I am following best practices and creating a ""bag of words"" vector with a vocabulary from the training data. My cross validation (and test) datasets are transformed using this model, using the same vocabulary created by the training set. They don't contribute any vocabulary, or affect the document frequency (for ""term-frequency inverse document frequency"" calculation).</p>

<p>However, this is restrictive in a few ways. Firstly, calculating the bag of words model is expensive, and so this prohibits me carrying out k-folds cross validation (since it would require constant re-calculation of the bag of words). My dataset is around 10 million words, and I'm calculating bag of words and bag of bi-grams, which takes around 5 minutes each time. </p>

<p>This also means I currently have holdout data for both my cross validation and test sets, which is data I can't use for training. </p>

<p>Would I be biasing my results significantly if I fit the bag of words on both the training set, and the cross validation set? In other words, if I use the vocabulary in the validation set to calculate the vocabulary for the bag of words? The way I figure, even though they might contribute to the vocabulary, there's no risk of overfitting since the frequency for those specific samples won't be seen at training. This allows me to slice the validation set later however I like, and I still have a ""test"" set for an accurate predictor of generalisation error (the test set won't be seen at all until test time). </p>

<p>I wonder if there's any precedent for something like this, and what your experiences are doing anything similar. </p>
","nlp"
"32799","transfer learning with sentiment analysis?","2018-06-07 17:57:47","44935","2","874","<word-embeddings><sentiment-analysis><nlp>","<p>The question is how good and what are some things to keep in mind when sentiment analysis models are tested on different datasets than they are trained on.</p>

<p>Say the task is to perform sentiment analysis on product reviews (unlabeled datset) - to classify positive, negative or neural. Because the data is unlabelled, a model can be trained (perhaps using logistic regression or NN) on a similar labeled dataset (say movie reviews, or product reviews) and tested on the original unlabelled dataset. </p>

<p>Will something like this work? Because the words of product names that occur in the unlabelled dataset will not be words that the model was exposed to during training, during test time will these words possibly throw off the model? </p>
","nlp"
"32585","How should I format input and output for text generation with LSTMs","2018-06-04 07:57:48","","1","290","<lstm><nlp><text-generation>","<p>I'm attempting to generate a response to an input line of text using an LSTM. I've considered various forms of input, including one-hot encoding each character in the line and passing each input line as a vector of one-hot encoded vectors. I've also considered using a dictionary and one-hot encoding each word in the sentence based on its alphabetical position. </p>

<p>However, I'm not sure about any of this, as I am new to natural language processing in machine learning. What would be the best way to format my input (and my output) for this problem?</p>
","nlp"
"32578","Machine learning - Algorithm suggestion for my problem using NLP","2018-06-04 05:54:29","","1","1262","<machine-learning><python><nlp><text-mining><vector-space-models>","<p>I am looking for a machine learning algorithm for my problem.</p>

<p>I have a set of sentences like,</p>

<p><code>[""The cat in the hat disabled"", ""A cat is a fine pet ponies."", ""Dogs and cats make good pets."",""I haven't got a hat.""]</code></p>

<p>and the search-words like,</p>

<p><code>[""cat"",""hat""]</code></p>

<p>I want to convert my sentence list and search-words to a vector space and find matching score between my sentence list and search-word list.</p>

<p>the type of output I am expecting is,</p>

<p><code>[(""The cat in the hat disabled"",0.9), (""A cat is a fine pet ponies."",0.5), ""(Dogs and cats make good pets."",0.6),(""I haven't got a hat."",0.49)]</code></p>

<p>Please suggest a machine learning algorithm for my task, if possible please share a reference link.</p>

<p>let me know if you have any queries or need more information.
I am currently using semanticpy for this <a href=""https://github.com/josephwilk/semanticpy"" rel=""nofollow noreferrer"">https://github.com/josephwilk/semanticpy</a></p>

<p>I am getting key-error for many search-words. its not performing stemming and lemmatization for the sentence list but only performing for the search-words list. </p>

<p>Please help on this.</p>
","nlp"
"32572","ML model to transform words","2018-06-03 18:25:00","32574","1","154","<neural-network><nlp><convolutional-neural-network>","<p>I build model that on input have correct word. On output there is possible word written by human (it contain some errors). My training dataset looks that:</p>

<pre><code>input - output  
hello - helo  
hello - heelo  
hello - hellou  
between - betwen  
between - beetween  
between - beetwen  
between - bettwen  
between - bitween
</code></pre>

<p>etc.
During preprocessing I add a measure of the distortion of a word. Then I hardcoding letters for numbers.
My current model's using CNN. The number of neurons of input is the same as the longest word in training dataset and the number of neurons of output is the same as the longest word in traning dataset. 
This model doesn't work as I excepted. Word on the output is not look as I except.
eg.</p>

<pre><code>input - output
house - gjrtdd
</code></pre>

<p>Question:</p>

<p>How can I build/improve model for this task? Is CNN a good idea? What other methods can I use for this task?</p>
","nlp"
"32544","Multilingual data handling for text summarisation","2018-06-02 16:53:15","","0","158","<deep-learning><nlp>","<p>I have a dataset having text documents as each entry , the documents are of various ethical languages which are not specified. How should be my strategy if I want to summarise the content of the dataset.</p>

<p>For eg. Doc1 : ""Ruby is a dog lover.Ruby loves her dogs so much. Ruby has a pet whose name is a dog""</p>

<p>Now summarising it is easy as ""Ruby loves dog.Ruby has a dog""</p>

<p>But How should be the approach , when the dataset also has documents in french, chinese, hindi etc...</p>

<p>Some of the problems are: there are cases of code switch, and I don't know what are the languages in the dataset before hand.</p>

<p>What I am guessing at this is to may be change all the dataset to english language and then do the tasks? In case if converting to english is a good choice then which library would be better?</p>
","nlp"
"32419","Are there any paper for a closed domain conversational agent","2018-05-30 20:22:30","","1","63","<deep-learning><nlp><word2vec><word-embeddings>","<p>i was trying to find a closed domain conversational agent/chatbot paper in Question and answering so not long conversation, and i don't think i see any. </p>

<p>All the paper i can find are related to an open domain conversational agent or a persona based neural conversational model, etc. I really need this kind of paper since i want to implement a chatbot with a quite niche topic and i can't seem to find any that can help me out.</p>

<p>I search most of the paper from arxiv and this <a href=""https://github.com/snakeztc/NeuralDialogPapers#task-bots"" rel=""nofollow noreferrer"">https://github.com/snakeztc/NeuralDialogPapers#task-bots</a></p>

<p>Any kind of help will be greatly appreciated</p>
","nlp"
"32347","Where can I find a dataset for long sequence text chunking?","2018-05-29 21:00:57","","1","197","<nlp><dataset><text-mining><text>","<p>Context:</p>

<p>I have documents with reviews of articles that have the following structure:</p>

<ul>
<li>Introduction: a description of the review, dates and metadata that will be discarded. (avg~180 words, std~30 words)</li>
<li>Loop with:

<ul>
<li>Reference: the reference article to review. A single review can talk about multiple articles. (avg~8 words, std~2 words)

<ul>
<li>Subject: topic of the following comments. There can be more than one subject for the following comments. (avg~7 words, std~3 words)

<ul>
<li>Comment: atomic comment about the article. There can be multiple comments per subjects. (avg~45 words, std~30 words)</li>
</ul></li>
</ul></li>
</ul></li>
<li>Final: a brief summary of the review and other info that will be discarded. (avg~165 words, std~40 words)</li>
</ul>

<p>I've to identify orderly the different chunks of references, subjects and comments. I've already designed a pipeline, and have a dataset to test and train my models, I'm using BIO tags for chunking as output, but I can adapt the pipeline to use BMEWO (BILOU).</p>

<p>I'm searching for a similar problem dataset, to compare how my pipeline is performing against the best metrics accomplished in that problem.</p>

<p>I've already searched in:</p>

<p><a href=""https://archive.ics.uci.edu/ml/datasets.html"" rel=""nofollow noreferrer"">https://archive.ics.uci.edu/ml/datasets.html</a></p>

<p><a href=""https://www.kaggle.com/datasets"" rel=""nofollow noreferrer"">https://www.kaggle.com/datasets</a></p>

<p>Until now the closest dataset I've found is the CoNLL for named entity recognition or grammar tagging(noun phrases, prepositional phrases, verb phrases).</p>

<p>But that problems seems very different from mine:</p>

<ul>
<li>Small length per chunk in relation with my problem.</li>
<li>The length of every chunk class are similar, In my case the size of the comments are very different than the subjects and references.</li>
<li>The outside tag could be everywhere while in my problem only can be at the beginning or the end.</li>
<li>These problems seems more focus on short grammatic structure instead of long sequence meaning.</li>
</ul>

<p>I'm searching with the following keywords: chunk, long-sequence, text. I'm not sure if the word chunk could be misleading.</p>

<p>Where can I keep searching datasets? What keywords can I use to improve my search? There is a similar problem in which I could try to transform mine to test against?</p>
","nlp"
"32345","Initial embeddings for unknown, padding?","2018-05-29 20:07:26","","5","6204","<deep-learning><nlp><lstm><word-embeddings>","<p>Last time I've been passing pretrained word embeddings into LSTM to solve text classification problems. Usually, there are additional <code>&lt;pad&gt;</code>, <code>&lt;unk&gt;</code> replacements for padding and unknown types. Of course, there are no pretrained vectors for them.</p>

<p>Solutions I've come up with are</p>

<ol>
<li>Fill them with random values</li>
<li>Fill them with zeros</li>
</ol>

<p>Which approach is better/wrong? What is the common practice?</p>

<p>Note: I use pytorch+torchtext if it matters.</p>
","nlp"
"32244","Fine-tuning NLP models","2018-05-28 06:40:54","","2","496","<neural-network><deep-learning><lstm><nlp><transfer-learning>","<p>In computer vision, if we don't have a large training set, a common method is to start with a pre-trained model for some related task (e.g., ImageNet) and fine-tune that model to solve our problem.</p>

<p>Can something similar be done with natural language processing problems?  I have a boolean classification problem on sentences and don't have a large enough training set to train a RNN from scratch.  In particular, is there a good way to fine-tune a LSTM or 1D CNN or otherwise do transfer learning?  And, if we want to do classification on sentences, is there a reasonable pre-trained model to start from?</p>
","nlp"
"32206","Determine the most important documents for supervised learning","2018-05-26 17:15:54","","3","108","<nlp><text-mining><feature-selection><k-means><unsupervised-learning>","<p>I have somewhat of a general/high level question.</p>

<p>Assume I'm doing supervised machine learning on some text data (tweets for example) and categorizing the documents to a certain taxonomy (multi-class classification). My supervised model performs fairly well on testing data but what I'm trying to do now is find a way to sort out which <strong>documents</strong> from the production data (not human labeled) if added to the training set would improve accuracy the most on that particular production data set.</p>

<p>The idea is once the initial predictions are made on production data, another algorithm would be run to determine the n most important documents (say 500) that if manually labeled by a human and added to the training set would improve accuracy the most, assuming the model does a 2nd round of prediction after the n documents have been added to the training set.</p>

<p>So the process would look like this:</p>

<ol>
<li>Initial predictions on production data are made</li>
<li>Algorithm runs to determine the most important records to increase accuracy of the model (based on features most likely).</li>
<li>Human reviews these most important records and adds them to the training set</li>
<li>A 2nd round of prediction takes place, hopefully with a better accuracy metric</li>
</ol>

<p>I'm thinking of using something like K-Means after doing some dimensionality reduction since it's NLP problem with lots of features.</p>

<p>Does anyone have any experience or suggestions in regard to this topic? Am I on the right track?</p>
","nlp"
"32145","Stopwords for programming languages (for, while, print,...)","2018-05-25 06:52:20","","3","819","<nlp>","<p>I am trying to do some analysis about user behaviour when typing (keystroke biometrics). Ideally, it will include traits extracted when people are writing code. Although not technically Natural Language, code also has some structured characteristics as language and I wanted to leverage that. I was wondering if there has been some research about performing language analysis focusing on programming languages instead of traditional spoken languages.</p>

<p>Mainly, I am interested in having a comprehensive list of stopwords for as many languages as possible. For example, stopwords will include: <em>for, while, return, break, string, if, else</em>, and so on. Although it would be nice to have them separated by languages, I wouldn't mind a list comprising several languages.</p>

<p>I know this could be done for example by getting some sample code and retrieving the most frequent terms, but I also wanted to know if there has been some research towards this direction.</p>

<p>Any ideas, papers, methods would be welcome.</p>

<p>Thanks!</p>
","nlp"
"32144","Web page data extraction using machine learning","2018-05-25 06:46:39","32242","4","200","<machine-learning><python><nlp>","<p>I would like to extract some specific information from web pages. Web pages contain person profiles, and I want to extract information such as name, email, research interested-areas. 
Structure of each page is different from one another. How can I extract such information using machine learning? What kind of a method, features I can use?</p>

<p>Or can I use NLP for such task?</p>
","nlp"
"32141","How can I create a negation of the sentence?","2018-05-25 06:04:57","","3","298","<deep-learning><nlp>","<p>What are some ways that I can generate the negation of a sentence such that the output sentence reflects the negation of original sentence?</p>

<p>If the sentence is: <code>That is an apple.</code></p>

<p>Then, the expected negation could be: <code>That is not an apple.</code></p>

<p>Another example:</p>

<pre><code>Sentence: Rita is so hot.
Negation: Rita is boring.
</code></pre>
","nlp"
"32110","Automatic Semantic Clustering and Tagging of sentences using NLP","2018-05-24 14:46:29","","2","1051","<python><nlp><clustering><word2vec>","<h1>NLP Analysis for keyword clustering</h1>

<p>I have a set of keywords for search engines and I would like to create a python script to classify and tag them under unknown categories.</p>

<p>To make it clear I should have an output like this one, without knowing the categories (Product, Colour, Accessory, Brand...):<br>
+----------------------------+------------+----------+--------------+-----------+<br>
|.......Keywords............|.Product...|.Colour.|.Accessory.|.Brand...|<br>
+----------------------------+------------+----------+--------------+-----------+<br>
|.red shoes with heels.|.shoes......|.red......|.heels.........|..............|<br>
|.Apple computer.........|.computer.|............|..................|.Apple....|<br>
|.Armani blue shoes....|.shoes......|.blue.....|..................|.Armani..|<br>
|.black mouse..............|.mouse.....|.black...|..................|..............|<br>
|.gaming laptop...........|.computer.|.............|..................|..............|<br>
+----------------------------+------------+----------+--------------+-----------+  </p>

<p>Any suggestions on how I could be able to do it?</p>

<p>I am currently using Word2Vec to find similarities between words and some APIs to recognize Brands and entities in the keywords</p>

<pre><code>    model2 = models.Word2Vec.load('semantic_clustering/datasets/it/it.bin')
    with open('tmp/kw_msm.txt') as f:
        kwlist = f.readlines()
    kwlist = [x.strip() for x in kwlist

    # unique_words = list(set([word for word in kw.split(' ') for kw in kwlist]))
    freq_words = defaultdict(int)
    words = set()
    for kw in kwlist:
        for word in kw.split(' '):
            freq_words[word] += 1
            words.add(word)
    sorted_freq_words = sorted(freq_words.items(), key=operator.itemgetter(1), reverse=True)

    # Creating Dataframe with words as columns
    df = pd.DataFrame(columns=words)
    df['keyword'] = kwlist
    for i, row in df.iterrows():
        for w in row['keyword'].split():
            df.loc[i, w] = 1

    good_words = [w for w in words if kw_in_vocab(w, model2)]

    KW_MODEL = model2[good_words]
    NUM_CLUSTERS = 10
    kclusterer = KMeansClusterer(NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance, repeats=25)
    assigned_clusters = kclusterer.cluster(KW_MODEL, assign_clusters=True)
    clusters = {}
    for i, w in enumerate(good_words):
        clusters[w] = assigned_clusters[i]
    sorted_clusters = sorted(clusters.items(), key=operator.itemgetter(1))
    for k in sorted_clusters:
        print(k)
</code></pre>

<p>This is a snippet of code I am using, creating a sparse matrix of words and clustering the columns with a fixed number of clusters, it's only a first test</p>
","nlp"
"31995","Learning verb conjugation","2018-05-22 17:03:44","","2","548","<machine-learning><nlp>","<p>I would like to train a network to conjugate verbs and recognize verb conjugations. For example:</p>

<ul>
<li>Given ""To eat, 1st person, past continuous"", the model would return ""I was eating""</li>
<li>Given ""I was eating"", the model would return ""To eat, 1st person, past continuous""</li>
</ul>

<p>The benefit of training would be to have the model conjugate and recognize verbs it hasn't seen previously.</p>

<p>I would like to train the network on standard conjugation tables such as <a href=""http://conjugator.reverso.net/conjugation-english-verb-eat.html"" rel=""nofollow noreferrer"">http://conjugator.reverso.net/conjugation-english-verb-eat.html</a>.</p>

<p>Is this within the realm of possibilities with our present machine learning technologies? </p>

<p>EDIT: If yes (and it seems to be the case), hints for suggested architectures or actual implementations would be most welcome. </p>
","nlp"
"31982","Machine Learning Text Categorization with Data in multiple Languages","2018-05-22 12:52:44","","1","19","<machine-learning><neural-network><nlp>","<p>I want to label natural text Dokuments in various different categories (arround 400 different categories) using Neural Networks. I have around 50.000 documents already labeled. The problem is around half of them are english only, the others are either chinese or a mix of chinese and english.</p>

<p>Would it be possible to train a network with all of the documents and get a network with an good quality or is it more useful to separate the languages and train the network with english documents only (arround 20.000 docs then)? </p>
","nlp"
"31958","Algorithms and tools for ranking text as a job description","2018-05-22 01:24:12","32135","3","218","<neural-network><nlp><naive-bayes-classifier>","<p>We're working on a ranking texts by degree of the similarity to vacancies. We have 4-year data set (≈1M texts) of custom search feeds from social network. We also have vacancies (≈30K) manually selected from this set.</p>

<p>Now the vacancies are selected in 2 stages:</p>

<ol>
<li><p>We make several specific search queries (i.e. ""need translator"") on the social network by API, receiving and combining the search feeds into one.</p></li>
<li><p>Then we look through each post in it for real vacancies. Generally, it should contain a request for interpreter services. Often it contains some <em>job details</em> (such as a place, salary, subject, deadlines, languages, contacts) and <em>stylistic features of speech</em> (i.e. greetings, exclamations, request to write in private, ask for recommendations, etc.).</p></li>
</ol>

<p>Before the point 2, we would like to have first in our final moderation feed the texts, that are most likely to be vacancies. We're going to try ranking by using the Naive Bayes classifier or the Natural Language Processing, but it's not as simple as it seems: there are a lot of various tools, schemes, models.</p>

<p>The question is, <strong>what algorithms and tools could help in our case?</strong> What we should pay attention to?</p>
","nlp"
"31937","How can I extract skills from a resume using python?","2018-05-21 15:10:43","","1","2648","<python><nlp><text-mining>","<p>I am looking at how to extract a set of skills from an unstructured resume format using Python. Please note that, though I have a basic idea of NLP, I am completely new to Python.</p>
","nlp"
"31925","Predict words in a given corpus from jumbled incomplete characters","2018-05-21 12:43:56","","1","111","<python><nlp><nltk>","<p>A complete newbie to data science. I have to predict the word using characters given. But the characters given can be in any order, and some might be missing or wrong. The problem is I get too many suggestions that arent right.</p>

<h1>Trial 1:</h1>

<p>I have tried permutating the given characters and checking if it is a valid english word : <a href=""https://pastebin.com/ZUZDZPPa"" rel=""nofollow noreferrer"">https://pastebin.com/ZUZDZPPa</a><br>
This does not take into account autocorrect like features.
With autocorrect: <a href=""https://pastebin.com/mDjVn0Rz"" rel=""nofollow noreferrer"">https://pastebin.com/mDjVn0Rz</a></p>

<h1>Trial 2:</h1>

<p>Tried limiting editdistance (levenshtein distance) to 2 and used my custom corpus that I built as I knew the context of the word (Occupations or hobbies).<br><a href=""https://pastebin.com/iC5TYxzS"" rel=""nofollow noreferrer"">https://pastebin.com/iC5TYxzS</a> <br><br>The problem is that the predictions I get arent close to the answer. I got pretty good results in Trial 2, but I am wondering if there was a better approach as edit distance of 2 is pretty limiting.</p>
","nlp"
"31818","Grouping company information","2018-05-18 16:00:07","31824","1","62","<nlp><similarity><text>","<p>I have 3 different datasets with company information, in all of them I have company name, but is not perfect: For example:</p>

<ul>
<li>Dataset A: Company name: Facebook</li>
<li>Dataset B: Company name: Facebook, Inc</li>
<li>Dataset C: Company name: facebook</li>
</ul>

<p>Some other signals like <strong>company url</strong> exists, but in terms of name matching wondering if text similarity is a good approach for this grouping problem.</p>
","nlp"
"31707","DBSMOTE on Short Text Classification","2018-05-15 21:58:48","","2","660","<nlp><feature-extraction><tfidf><smote>","<p>I am trying to use DBSMOTE(Density-Based Synthetic Oversampling TEqnique) to on a data set of short text--tweets to be specific. This will be used to train a classifier model in a multiclass classification model. This will be done at feature level augmentation, using TF-IDF as features. I have read though, that to use SMOTE on NLP, the feature vectors must be reduced in dimension. What is an optimal size of a feature vector to use in a SMOTE family algorithm? </p>

<p>Similar Question: <a href=""https://datascience.stackexchange.com/questions/27671/how-do-you-apply-smote-on-text-classification?rq=1"">How do you apply SMOTE on text classification?</a></p>

<p>DBSMOTE Code: <a href=""https://rdrr.io/cran/smotefamily/man/DBSMOTE.html"" rel=""nofollow noreferrer"">https://rdrr.io/cran/smotefamily/man/DBSMOTE.html</a></p>

<p>DBSMOTE Paper: <a href=""https://link.springer.com/article/10.1007/s10489-011-0287-y"" rel=""nofollow noreferrer"">https://link.springer.com/article/10.1007/s10489-011-0287-y</a></p>
","nlp"
"31691","Best way to extract information from text description and match it with set of words","2018-05-15 14:30:27","","-2","1138","<machine-learning><deep-learning><nlp><text-mining><recommender-system>","<p>I have 10k records of data, each record represents a unique product(10k class labels) and its description. For example, ""Coffee Maker, this product takes coffee beans and brew it, to make tasty cofe"". Description can be little bigger and smaller. Most of the data also mentions acronyms in the description to refer the product or some other related product and yeah there spelling mistakes too. </p>

<p>So what is the best approach to solve this problem, im open for using Machine Learning/Deep Learning. Please help me on how to build a model, that takes a small description like three or four words or more as a input and suggest a list of products that closely represent those words. </p>
","nlp"
"31617","Natural Language to SQL query","2018-05-14 04:23:08","31713","26","36468","<machine-learning><sql><nlp>","<p>I have been working on developing a system ""Converting Natural Language to SQL Query"".</p>

<p>I have read the answers from the similar questions, but was not able to get the information that I was looking for.</p>

<p>Below is the flowchart for such system which I have got from <strong><a href=""http://www.iaees.org/publications/journals/selforganizology/articles/2016-3(3)/algorithm-to-transform-natural-language-into-SQL-queries.pdf"" rel=""noreferrer"">An Algorithm to Transform Natural Language into SQL Queries for Relational Databases by Garima Singh, Arun Solanki</a></strong></p>

<p><a href=""https://i.sstatic.net/9Ekcd.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/9Ekcd.png"" alt=""Flowchart""></a></p>

<p>I have understood till part of speech tagging step. But how do I approach the remaining steps.</p>

<ol>
<li>Do I need to train all the possible SQL queries?</li>
<li>Or, once part of speech tagging is done, I have to play with the words and form a SQL query?</li>
</ol>

<p>Edit: I have successfully implemented the from step ""user query"" to ""Part of speech tagging"". </p>

<p>Thank you.</p>
","nlp"
"31591","NLP: how to organize this research into Tensorflow","2018-05-13 07:47:19","","2","73","<nlp><tensorflow>","<p>I need a network with the following organization:</p>

<ul>
<li>the input text, it is constant for training, for example ""mama soap frame sponge"" or the formulas of physics, math, chemistry and others</li>
<li>a series of questions (who, where, when, what, whom, what, for what), for example ""who washed the frame?""</li>
<li>A series of answers (labels), for example, the answer ""Mom"".</li>
</ul>

<p>How to do it with TensorFlow?</p>

<p>Thank you</p>
","nlp"
"31584","How many examples needed for named entity disambiguation?","2018-05-12 22:30:11","","4","1324","<nlp><named-entity-recognition>","<p>If I want to build a named entity linking system for resumes using an ontology of occupations and skills about how many annotations would I need? The ontology has about 20,000 entities. </p>

<p>As a lower bound I'm guessing I would need about 10 examples per entity and maybe 3 different annotators to label each mention so ~600K annotations. Does that make sense?</p>
","nlp"
"31545","Build train data set for natural language text classification?","2018-05-11 18:23:22","31654","3","254","<python><classification><nlp><text-mining>","<p>I have extracted <strong>~550 video scripts (subtitles)</strong> from 11 free courses on the Coursera platform. I have pre-processed them in terms of <strong>punctuation removal, stop words removal, tokenization, stemming and lemmatization</strong>. Now, I've been advised that for my task I can attempt to use a simple <strong>Bag of Words</strong>. However I am not sure how exactly would that help me towards classifying my text into <strong>one out of six categories</strong>. The categories are related to the intent a video material was created with and more precisely, which part explains a concept, which part discusses an example, which part gives practical advice etc. Below are my categories:</p>

<p><code>ConceptDescription</code>-> Explanation of the Main concept(s)<br>
<code>ConceptMention</code>-> Mentioning of a concept, related to the main concept<br>
<code>Methodology / Technique</code>-> To achieve something, what should one do<br>
<code>Summary</code>-> Summary of the discussed material or of the whole course<br>
<code>Application</code>-> Practical advise for the concept<br>
<code>Example</code>-> Concept example<br></p>

<p>By manually reading several files from 3 of the courses, I created a dictionary, containing spoken language words, that may help me identify which class a specific sentence/paragraph falls into. However <strong>I do NOT have a train dataset</strong> for a classifier. So my idea was to use that dictionary to label my data, e.g. sentence 1 as <code>Summary</code>, sentence 4 as <code>ConceptDescription</code> and sentence 12 as <code>Example</code> and then marking sentences 2 and 3 the same as 1, sentence 5-11 like sentence 4 etc.</p>

<p>My question is, is this idea too lame? And <strong>is there a way to create at least an average quality training dataset in a way that is not manual?</strong> Or <strong>if manual check is the only option, is there an option where I would need to do manual labeling on only a small fraction of the files</strong>, say 50 out of 550 and classification would still produce average to good result? I don't aim at perfect result, but I aim at something less time-consuming due to limited time.</p>

<p>I also played with tf-idf which outputs terms, but of course, not really what I need, so that was a bit random.</p>

<p>Thanks in advance for your help. <strong>Any specific ideas and algorithms would be very welcome.</strong></p>
","nlp"
"31543","LSTM not learning with extra nontemporal data added after LSTM layer - Keras","2018-05-11 18:01:32","","1","657","<deep-learning><nlp><keras><rnn><lstm>","<p>I have three different inputs I would like to send into my LSTM - the sequence of ""words"", extra temporal information, as well as extra nontemporal information.</p>

<p>Following Adam Sypniewski excellent answer to this question (<a href=""https://datascience.stackexchange.com/questions/17099/adding-features-to-time-series-model-lstm"">Adding Features To Time Series Model LSTM</a>) I came with this architecture:</p>

<pre><code>word_input = Input(shape=(sequence_length,))
temporal_metadata = Input(shape=(sequence_length, 1))
nontemporal_metadata = Input(shape=(nontemporal_num_features,))

embedding = Embedding(input_dim=vocab_size, output_dim=100)(word_input)
temporal_info = concatenate(inputs=[embedding, temporal_metadata])
rnn = LSTM(50, dropout = .5, recurrent_dropout=.5)(temporal_info)
all_info = concatenate(inputs=[rnn, nontemporal_metadata])
dense_1 = Dense(10, activation='relu')(all_info)
dropout_1 = Dropout(.5)(dense_1)
dense_2 = Dense(1, activation='sigmoid')(dropout_1)

model = Model([word_input, temporal_metadata, nontemporal_metadata], dense_2)
model.compile(loss='binary_crossentropy', optimizer='adam'
model.fit(x=[my_words, my_temporal, my_nontemporal], y = my_y, 
     epochs = 30, 
     validation =([my_words_test, my_temporal_test, my_nontemporal_test], y_test))
</code></pre>

<p>This does not learn, and gets an AUC of 0.5</p>

<p>However if I don't add the nontemporal data and change the above code to</p>

<pre><code>word_input = Input(shape=(sequence_length,))
temporal_metadata = Input(shape=(sequence_length, 1))

embedding = Embedding(input_dim=vocab_size, output_dim=100)(word_input)
temporal_info = concatenate(inputs=[embedding, temporal_metadata])
rnn = LSTM(50, dropout = .5, recurrent_dropout=.5)(temporal_info)
dense_1 = Dense(10, activation='relu')(rnn)
dropout_1 = Dropout(.5)(dense_1)
dense_2 = Dense(1, activation='sigmoid')(dropout_1)

model = Model([word_input, temporal_metadata], dense_2)
model.compile(loss='binary_crossentropy', optimizer='adam'
model.fit(x=[my_words, my_temporal], y = my_y, 
     epochs = 30, 
     validation =([my_words_test, my_temporal_test], y_test))
</code></pre>

<p>It works great. Can anyone please help me find what is going wrong in the first code sample that would make the model unable to learn. Thanks!</p>
","nlp"
"31524","Supervised learning model for extracting terror attack motives from attack summary (NLP)","2018-05-11 13:23:20","","1","64","<python><nlp><supervised-learning>","<p>I have a terror attack (tabular) data set. Each row is one attack and there are columns like: </p>

<ul>
<li>Date of the attack (daily resolution)</li>
<li>Location of the attack (long/lat, as well as city/country)</li>
<li>Number of casualties</li>
<li>Attack/weapon type</li>
<li>A few boolean columns like whether it was a suicide attack</li>
</ul>

<p>Furthermore, I have a text column that holds a 2-3 sentence description of the attack. This is the main column I want to use for training/predicting.</p>

<p>There are several target columns of the form ""is_left_wing"", ""is_right_wing"", etc. The values are 0, 1, and -1. Here 0 means the attack didn't have the respective motive, 1 means it had the motive and -1 means it is unknown.</p>

<p>In short, my goal is to build a model that is trained on the 0 and 1 values in the target columns and makes predictions about the -1s.</p>

<p>The main thing I'm stuck on is how to extract features from the text column with the attack description. I have limited NLP experience and I want to use something more sophisticated than a simple bag of words model.</p>

<p>I would appreciate suggestions about the general approach to this problem (also some good readings on the topic).</p>
","nlp"
"31478","Any Hadoop Tool to Annotate Text","2018-05-10 14:30:16","","1","22","<nlp><text-mining><tools>","<p>I have large text files on HDFS... I would like to label some text in those files to improve text analysis?</p>

<p>Do you know of any tool like that?</p>
","nlp"
"31475","Interpolation in nlp - definition of O term","2018-05-10 11:50:43","","1","1989","<nlp><interpolation>","<p>Reading definition of interpolation below how are the O terms defined? Is this a value that is set manually?</p>
<blockquote>
<h1><a href=""https://gist.github.com/ttezel/4138642#example-1"" rel=""nofollow noreferrer"">Example</a></h1>
<ul>
<li>P( Sam | I am ) = count( Sam I am ) / count(I am) = 1 / 2</li>
</ul>
<h3>Interpolation using N-grams</h3>
<p>We can combine knowledge from each of our n-grams by using interpolation.</p>
<p>E.g. assuming we have calculated unigram, bigram, and trigram probabilities, we can do:</p>
<p>P ( Sam | I am ) = Θ<sub>1</sub> x P( Sam ) + Θ<sub>2</sub> x P( Sam | am ) + Θ<sub>3</sub> x P( Sam | I am )</p>
</blockquote>
<p><sub>(<a href=""https://i.sstatic.net/uIZUW.jpg"" rel=""nofollow noreferrer"">original problem statement image</a>)</sub></p>
","nlp"
"31448","How to filter Named Entity Recognition results","2018-05-09 17:27:06","","4","332","<nlp><named-entity-recognition>","<p>I have a pipeline built which at the end outputs a bunch (thousands to tens of thousands or more) of named entities. I'd like to do aggregates on those named entities (to see, e.g. how many times a specific named entity is mentioned in my corpus). A problem that I am arriving at; however, is that the named entities often don't match up with each other even though they are the same entity. For example, one instance of the named entity might be ""Dr. John Smith"" while another instance is ""John Smith"" or one instance might be ""Google"" while another might be ""Google Inc."". This makes aggregating quite hard to do.</p>

<p>In order to deal with this issue and set ""Dr. John Smith"" to be the same entity as ""John Smith"", I was thinking of doing word matching between my named entities. I.e. I would check if named entity A has a word in common with named entity B and if they do set them to be the same entity. This approach is obviously seriously flawed. I will be equating ""John Nguyen"" and ""John Smith"" as the same entity even though they are obviously not. What's potentially even worse with this method though is I might run into similarity chains where I have ""John Smith"" linked with ""Richard Smith"" linked with ""Richard Sporting Goods Inc."" linked with ""Google Inc."" etc etc... While I may be willing to allow issues arising from former problem through, the latter problem appears to be catastrophic.</p>

<p>Are there any accepted techniques in the NLP community for dealing with this issue? </p>
","nlp"
"31407","How to combine sparse text features with user smile for sentiment classification?","2018-05-08 23:26:01","","1","622","<classification><sentiment-analysis><nlp>","<p>I am trying to perform sentiment classification task where I have some text and some information about whether the user smiled or not. 
Now when I use count-vectorizer to convert my text to feature set (Bag of Words) approach it results in more than 5000 features. When I add smile to these 5000 features, total number of features become 5001. However, I do not see any improvement in the classification accuracy. I think the 5000 sparse features dominate the one and only smile feature. Hence no effect on the classification. Can anyone recommend, what is the good way to merge textual feature with smile, prosody or other features?</p>
","nlp"
"31361","How to replace short words into full words from tweets using python","2018-05-08 05:15:28","","0","7252","<machine-learning><python><nlp><nltk><twitter>","<p>I am doing sentiment analysis on tweets. Most of the tweets contains short words and i want to replace them as original/full word.</p>

<p>Suppose that tweet is:</p>

<pre><code>I was wid Ali.
</code></pre>

<p>I want to convert:</p>

<pre><code>wid -&gt; with
</code></pre>

<p>Similarly</p>

<pre><code>wud -&gt; would
</code></pre>

<p>i have 6000 tweets in which there are lots of short words. How i can replace them ? is there any library available in python for this task? or any dictionary of shorts words available online?</p>

<p>i read answer of <a href=""https://stackoverflow.com/questions/43018030/replace-appostrophe-short-words-in-python"">Replace appostrophe/short words in python</a> Question but it provides dictionary of appostrophe only.</p>

<p>Currently i am using NLTK but this task is not possible with NLTK.</p>
","nlp"
"31350","What does 1024 by 3 model mean?","2018-05-08 00:02:15","31353","0","79","<deep-learning><nlp>","<p>I was watching this <a href=""https://youtu.be/040CfFRJ9Rs?t=3m25"" rel=""nofollow noreferrer"">video</a>  and Sentdex mentioned he had to switch around 1024 by 3 model. What does he mean by 1024 by 3 model and what did he change around? </p>

<hr>

<p>Edited: Youtube link with the timestamp where he talks about this topic</p>
","nlp"
"31323","Machine Learning and Natural Language Processing : Project Initiation","2018-05-07 09:38:09","","2","111","<machine-learning><nlp>","<p>I am in the research phase of a long project and am willing to get some useful feedback from your side about the most appropriate project path to take.</p>

<p><strong>Current situation:</strong></p>

<ul>
<li>A large team of so called production editors (PEs) is semi-automatically processing MS word files in order to do the copy-editing and layout.</li>
<li>This is currently semi-automatized with word macros where the PEs will apply different macros on different text paragraphs in order to set the appropriate style, position, font etc.</li>
<li>Still many actions are manual and depending on the human eye checks, resulting at the end many mistakes and an average quality</li>
</ul>

<p><strong>Plan</strong></p>

<ul>
<li>Start using machine learning, later maybe neuronal networks, statistics probability and artificial intelligence</li>
<li>The usage of machine learning (to start with) could be in the way that the system will learn the manual tasks done by the user (meaning the tool was not able to do those correctly in the automatized way) and then try to apply those in next similar situation</li>
<li>The term of similar situations is important here as this will imply natural language processing (NLP). The staff is dealing with word documents that could contain anything (within a similar layout)</li>
</ul>

<p><strong>The dilemma of the day is :) :</strong></p>

<p>Where to start from in order to do some use case analyses? What I currently have:</p>

<ul>
<li>More than 100k word files that are correctly done (except the user mistakes that may still lay around)</li>
<li>A list of rules that apply to the word documents describing the exact styles that needs to be used (non exhaustive)</li>
<li>Basic Machine Learning knowledge (starting with AI studying)</li>
</ul>
","nlp"
"31314","How do you use Tensorboard to visualise my Chatbot? What can I learn from it?","2018-05-07 05:17:09","","1","48","<machine-learning><deep-learning><nlp><tensorflow>","<p>I am currently learning DeepLearning and wanted to ask a few questions in relation to my current project <a href=""https://github.com/deepcollege/deeplearning/blob/master/030-chatbot/chatbot_simple.py"" rel=""nofollow noreferrer"">https://github.com/deepcollege/deeplearning/blob/master/030-chatbot/chatbot_simple.py</a></p>

<p>First of all, how do you generate an useful Tensorboard model graph? As you can see from the code, current session.graph isn't outputing anything helpful or comprehensible. My model is basically a chatbot model based on Seq2Seq, how do you generate more simplified and useful Tensorboard model graph?</p>

<p>My second question is, in NLP situation (more specifically a chatbot), what other things can you do to analyse the model in Tensorboard? And how do you enable them?</p>

<p>Thank you</p>
","nlp"
"31219","Identifying parts of speech for individual words (not texts)","2018-05-04 15:41:06","","3","80","<nlp>","<p>Please forgive the potential ignorance of this question. I am not a complete virgin regarding NLP:</p>

<p>I am trying to assign parts of speech to individual words. I realize that many words can serve as multiple parts of speech, but I am looking for the POS that a human would be most likely to interpret the word as, seeing it out of context.</p>

<p>Is there any database that has this info in it for a reasonably large set of words? Or, is there any large corpus with POS tags where I could feed it  words, and get frequency distributions of the POS that each word is playing in the corpus.</p>

<p>Thanks.</p>
","nlp"
"31201","Creating labels for Text classification using keras","2018-05-04 07:02:07","31204","1","2447","<python><deep-learning><nlp><keras><text-mining>","<p>I have a text file with information that needs to classified based on keywords. The text file contains many number of paragraphs. And the paragraph contains keywords that we want (lets say salary amount, interest rate and so on..)</p>

<p>I want to write a model which will extract the paragraph (or 3 to 4 lines of text) containing the keyword i want. How do i create a label in this case? All i have is a raw text.</p>

<p>I am new to NLP. Any suggestions how i can approach this?</p>
","nlp"
"31109","Ratio between embedded vector dimensions and vocabulary size","2018-05-02 09:43:47","31126","8","16827","<nlp><word-embeddings>","<p>Using <code>Embedding</code> layer in Keras on a fairly small vocabulary (~300), I am looking at how to choose the output of this layer (dense vector) when given a 300 dimension vector.  I think that the embedded vector need to have a minimum length to be able to map a given vocabulary.</p>
","nlp"
"31048","Pros/Cons of stop word removal?","2018-04-30 17:14:54","31055","8","10121","<nlp>","<p>What are the pros / cons of removing stop words from text in the context of a text classification problem, I'm wondering what the best approach is (i.e. to remove or not to remove)?</p>

<p>I've read somewhere (but can't locate the reference) that it may be detrimental the the performance of a model in the case of sentiment analysis to remove stop words.</p>
","nlp"
"30999","Text + scalar features in one model","2018-04-29 11:56:28","","-2","197","<nlp><word-embeddings>","<p>I have a set of features including text field(3-10 sentences) and about 10 scalar fields. I need to predict another scalar field (between 0 and 1). I have this field in my training/validation data.  </p>

<p>It's my first experience with a text feature. What is the correct way of creating a model? </p>

<p>Should I treat text field as embedding, flatten them to 1 dimension matrix of X size and combine with other features? </p>

<p>Thanks. </p>
","nlp"
"30978","Identifying most informative (sub)words/vectors that help classify a sample","2018-04-28 14:29:23","","2","127","<neural-network><nlp><text-mining><feature-selection>","<p>I am classifying text using <a href=""https://fasttext.cc"" rel=""nofollow noreferrer"">fastText</a> which is a word2vec library that can also create vectors for character level n-grams and I have successfully trained a binary classifier.</p>

<p>Now I’d like to see what words or subword n-grams are the most predictive of a class for the two classes (e.g. if classifier sees a word <code>forest</code> or a subword <code>res</code> then that might be a strong indication that the document has label <code>Nature</code>, but if it sees word “and” then that is probably not very informative for this classification task).</p>

<p>Therefore, I guess the question could be phrased as:</p>

<p><strong>Given vectors representing words and subwords and a trained fastText classifier, what would be the best way to get a list of e.g. top 10 most informative words and subwords for deciding which class a sample belongs to?</strong></p>

<p>Even though I’d be glad if you could make specific suggestions that consider my current setup with fastText, I’m also open to the more general solution suggestions.</p>

<p>Thanks</p>
","nlp"
"30937","Automatic code checking","2018-04-27 11:16:24","","4","177","<machine-learning><r><nlp><word2vec><similar-documents>","<p>I have some experience in machine learning, mainly clustering and classifiers.  However, I am somewhat of a newbie when it comes to NLP.</p>

<p>That said I am aware of all the various issues and difficulties involved in processing <strong>natural</strong> language eg part-of-speech, ambiguity, negation detection etc.  I am also aware of certain models to represent text such as bag-of-words and Word vector representation.</p>

<p>Whilst the specific problem I have is a textual problem, it is not to my mind a natural language problem.  Instead I need to compare 1000s of files of programming code to determine how similar they are to one another.</p>

<p>Unlike natural language processing where we remove stop-words and we care very much about issues such as ambiguity and the scope of negation etc, with programming code there is no ambiguity, at least not with reserved key words such as ""function"", ""if"", ""else, ""while"", ""end"", nor is there any ambiguity with core mathematical functions and other operators eg ""log"", ""sin"", ""tanh"" etc.</p>

<p>For my problem, we can assume all files are in the same programming language (let's say R) and let's also assume that each programming file contains at most 250 lines of code.</p>

<p>The initial objective is to produce a correlation matrix that shows how similar the code files are to one another.</p>

<p>There are to my mind 2 ways of approaching this:</p>

<ol>
<li><p><strong>Quick and simplistic</strong>.  I propose to remove from each text file:</p>

<ul>
<li>All comments as I'm not interested in each developer's individual comments</li>
<li>All excessive white space, ie remove all formatting ensuring there is 
only one space between reserved words and blocks of code</li>
</ul>

<p>This is the only pre-processing I would do in a simplistic solution. </p>

<p>I then propose to use the Doc2Vec model to reduce each file to a single vector and then compare the various vectors to one another by calculating the correlation coefficient.</p>

<p>Does this seem reasonable to people for a very quick and simplistic solution?  If not, please advise what other pre-processing you would do and/or what model you would use to represent the files.</p></li>
<li><p><strong>More advanced</strong>.  An immediate flaw with the simplistic solution is that it is too naive.  For instance:</p>

<ul>
<li><p>If Code A and Code B have exactly the code (ie same functions that perform exactly the same task and all variable and object names are the same) except that Code A's functions are listed in a slightly different order to those listed in Code B's file, then a line-by-line comparison of the two code files will obviously show differences.  However, if the code in file B was ``reordered'' so that the functions were in exactly the same order as file A, then there wouldn't be any differences.</p></li>
<li><p>Code A and Code B could in logical terms be exactly the same (ie same number of functions that perform logically the same operations and output) except that object, variable and function names are different (after all that is a subjective choice by each coder) then again we would have significant differences.  For example, in Code A's file there is a function called MyFunction(a,b) whereas in Code B's file there is a function called CopyMyFunction(x,y).  Now if both functions perform exactly the same operations (eg the sum of the two input values) then logically the code is the same - it's just the variable names that are different.</p></li>
<li><p>Finally a more complicated version.  Suppose there is a function in Code A's file called MyAverage(a,b) that performs the operations, returning as output d:</p>

<p>c = a + b</p>

<p>d = c/2</p>

<p>Suppose in Code B's file there is also a function called MyAverage(a,b) that performs the same calculation but in only one step, ie</p>

<p>c = (a + b)/2</p>

<p>Logically the two functions produce exactly the same output, albeit Code A does it in 2 steps whereas Code B does it in 1 step.</p></li>
</ul>

<p>The above are clearly only some examples of the types of issues that could be encountered.</p></li>
</ol>

<p>The objective is not to determine whether there has been plagiarism or whether sections of code have been copied from one code file to another.  Instead, if a serious error (a bug or logical error) is subsequently found in one code file, then I need a means of identifying how likely that same error occurs in other code files, ie has that bug propagated itself by being copied from file to file?</p>

<p>How would people approach the more advanced problem? For instance, to solve the last problem (ie determining whether two sections of code are logically equivalent) I could see two ways of approaching this:</p>

<ul>
<li>Renaming of variables and substitution.  So in Code A, as variable c is defined as ""a + b"", it could be substituted into the expression for variable d with the addition of the brackets.  After the substitution, variable d could be renamed as variable c.  In this case, Code B would then be the same as Code A.</li>
<li>Simulation of output.  In more complicated function, the above technique may not always work.  Instead, if the outputs of the two functions are the same when given the same inputs, can we deduce that the logic of the two must be the same within a certain level of confidence?  The problem with this is that simulating millions of inputs would be very time consuming and expensive surely.</li>
</ul>

<p>Other than the above I've no idea how to approach this problem.</p>

<p>Any thoughts on the approach and/or resources to use?</p>

<p>I am happy to be directed to a good reference or resource on the issue.</p>
","nlp"
"30917","K-means clustering of word embedding gives strange results","2018-04-27 00:38:13","","8","7733","<nlp><clustering><word-embeddings>","<p>I'm trying to cluster words based on pre trained embeddings. I ran a simple experiment where I obtained around 100 words relating to ""food taste"", obtained word embeddings from a pre-trained set, and tried to run k-means on the result. </p>

<p>I do get sensible results on certain clusters, but very strange results on others. E.g: </p>

<pre><code>Cluster 1: [fatty, oily, greasy] -- (good)
Cluster 2: [crumbly, powdery, grainy, flakey, chalky] -- (good)
Cluster 3: [flavorful, hearty, unflavored, savory, full-bodied] -(bad)
Cluster 4: [seasoned, unseasoned] -- (bad)
</code></pre>

<p>Can anyone suggest as to why seemingly opposite words like (seasoned, unseasoned) and (flavorful, unflavored) would be clustered together? </p>

<p>What I tried: </p>

<p>1) Using fasttext embeddings and Glove embeddings. The latest results are from concatenating fasttext wikipedia and common crawl embeddings.</p>

<p>2) Normalizing the vectors to same length before doing k-means (using eucledian distnaces). I think this is somewhat similar to having cosine distances. Tried not normalizing s well, but normalizing gave better results.</p>

<p>3) Tried some other clustring methods like DBSCAN but k-means seems better. </p>

<p>4) Tried PCA to reduce the dimensionality of the word vectors - didnt change the results much. Tried PCA for the selected embeddings in my vocab that needs to be clustered- not the whole word set. </p>

<p>Any suggestions to improve my results are welcome. If someone came across research articles discussing similar issues please post them as well. </p>

<p>Thanks!
TNC</p>
","nlp"
"30877","Extracting date, relation and noun phrase from text","2018-04-26 13:31:47","","2","265","<nlp><text-mining><information-retrieval><nltk><stanford-nlp>","<p>A sentence (Segmented from a document) as below:</p>

<pre><code>This Amendment dated 26th of April 2018 modifies the Agreement dated 20th April 2017.
or
This Amendment depends on the agreement dated 20th April 2017
</code></pre>

<p>I would like to extract the following as a CSV</p>

<pre><code>Document_type | Date_of_document | relation | related_doc | related_doc_date
----------------------------------------------------------------------------
Amendment     | 26-04-2018       | modifies | Agreement   | 20-04-2017
Amendment     | Null             | depends  | agreement   | 20-04-2017
</code></pre>

<p><strong>What have I done till now</strong></p>

<p>Found and extracted dates using Stanford NER. Got them in a normalized form. Can extract Noun Phrases using TextBlob. But cannot link which Noun Phrase is linked to which date. </p>

<p>Am I doing it wrong? Any tutorials/examples/advice?</p>
","nlp"
"30772","NER at sentence level or document level?","2018-04-24 18:28:32","","2","208","<nlp><lstm><word-embeddings><named-entity-recognition><stanford-nlp>","<p>Should NER models (LSTM or CRF) take input training data at sentence level or paragraph level?</p>

<p>Let's say we have this input text, and we would like to do Named Entity Extraction:</p>

<blockquote>
  <p>GOP Sen. Rand Paul was assaulted in his home in Bowling Green,
  Kentucky, on Friday, according to Kentucky State Police. State
  troopers responded to a call to the senator\'s residence at 3:21 p.m.
  Friday. Police arrested a man named Rene Albert Boucher, who they
  allege ""intentionally assaulted"" Paul, causing him ""minor injury"".
  Boucher, 59, of Bowling Green was charged with one count of
  fourth-degree assault. As of Saturday afternoon, he was being held in
  the Warren County Regional Jail on a $5,000 bond.</p>
</blockquote>

<ol>
<li><p>Paragraph level: we can take it as one record and each token is marked by the entity label. Model has <strong>ONE</strong> record with <strong>LONG</strong> sequence.</p></li>
<li><p>Sentence level: we first intelligently split the paragraph into 5 correct sentences, and each token in each sentence is marked by the entity label. Model has <strong>FIVE</strong> records with <strong>shorter</strong> sequences:</p></li>
</ol>

<blockquote>
  <p>0) GOP Sen. Rand Paul was assaulted in his home in Bowling Green,
  Kentucky, on Friday, according to Kentucky State Police. </p>
  
  <p>1) State
  troopers responded to a call to the senator's residence at 3:21 p.m.
  Friday.</p>
  
  <p>2) Police arrested a man named Rene Albert Boucher, who they allege
  ""intentionally assaulted"" Paul, causing him ""minor injury"".</p>
  
  <p>3) Boucher, 59, of Bowling Green was charged with one count of
  fourth-degree assault.</p>
  
  <p>4) As of Saturday afternoon, he was being held in the Warren County
  Regional Jail on a $5,000 bond.</p>
</blockquote>

<p><strong><em>Which one gives the NER modeling a better NER performance?</em></strong></p>

<p>I tend to think sentence level is better, however, shouldn't LSTM memory cells be trained to remember or forget states automatically if given long paragraphs? Especially when the sentence splitting can also potentially make mistakes, for instance:</p>

<blockquote>
  <p>1) State troopers responded to a call to the senator's residence at 3:21 p.m. Friday.</p>
</blockquote>

<p>could have been </p>

<blockquote>
  <p>1) State troopers responded to a call to the senator's residence at 3:21 p.m. </p>
  
  <p>2) Friday.</p>
</blockquote>
","nlp"
"30681","Package/function to convert the word to one hot vector in Python NLP","2018-04-23 11:27:52","30718","1","733","<machine-learning><python><nlp>","<p>Is there a package or function in NLP which can be used to convert the word into an one hot vector.</p>

<p>Thank you. </p>
","nlp"
"30610","What should be the ratio of True vs False cases in a binary classifier dataset?","2018-04-21 11:40:16","30627","2","833","<machine-learning><deep-learning><nlp><dataset><cnn>","<p>I am using a CNN for sentiment analysis of news articles. It is a binary classification with outputs: Interesting &amp; Uninteresting.
In my dataset, there are around 50,000 Uninteresting articles and only about 200 Interesting articles. I know the ratio is badly skewed.</p>

<ol>
<li>My question is what should be the ratio in such a scenario.</li>
<li>One approach that I want to try is to cluster the Uninteresting news
articles and take a sample from each cluster for training. Is there
a better approach?</li>
</ol>
","nlp"
"30513","Are there any good NLP APIs for comparing strings in terms of semantic similarity?","2018-04-19 09:07:00","","4","3097","<machine-learning><python><nlp><similarity><software-recommendation>","<p>I want to create a chatbot which informs the user about traffic at the streets but not in real-time for the moment. I have created a small database with MySQL which has some data stored regarding traffic and I fetch them with a PHP script whenever this is appropriate depending on the interaction of the user with the chatbot.</p>

<p>I wonder how to deal with the case when the user asks variations of the same question which therefore can be answered with the same answer.
For example:</p>

<ul>
<li>Why is there traffic at High Street?</li>
<li>What is the cause of traffic at High Street?</li>
<li>Why did I encounter traffic at High Street?</li>
<li>I am stuck in traffic at High Street. Why is this?</li>
</ul>

<p>Obviously, I can start by removing stopwords (e.g. did), by naming entities (e.g. road -> High Street), by defining synonyms and by applying a text similarity measure (e.g. Levenshtein distance etc).</p>

<p>However, I feel like reinventing the wheel if I do this. Therefore, my question is:</p>

<p><strong>Are there any APIs which can compare strings in terms of semantic similarity (without even requiring training)?</strong></p>

<p>I know that there software platforms such as Dialogflow which are suitable for these tasks but still you must explicitly state all the variations of the same question so that you will get the same answer. Therefore, I look for a API where you will explicitly state only one of these variations of the same question (e.g. Why is there traffic at High Street?) and then the API will figure out by itself which other variations are identical to it in terms of meaning or not.</p>
","nlp"
"30284","Measuring document cluster cohesion","2018-04-13 19:31:15","","0","239","<nlp><clustering><text-mining>","<p>I have a set of clusters which each cluster contains a list of short documents. I want to compute how coherent and cohesive each cluster is and filter out the incoherent and in-cohesive ones. </p>

<p>I am aware of <strong>intra-cluster distance</strong> and <strong>within-cluster dispersion</strong>, which are part of computing clustering evaluation metrics <strong>Silhouette Coefficient</strong> and <strong>Calinski-Harabaz Index</strong>, respectively. My question is, is there any other metrics or ways to compute such intra-cluster coherence or cohesion? Is there any standard ways for this that I am not aware of? Thanks.</p>
","nlp"
"30216","Extract relevant vocabulary from a document","2018-04-12 09:59:42","","1","528","<python><nlp><preprocessing>","<p>I am training a DSSM model for QnA. I have 200 queries and their correspondent answers - the answer is answering what kind of information should an article related to the query contained. E.g:</p>
<blockquote>
<p>Title:
literacy rates Africa</p>
</blockquote>
<blockquote>
<p>Description:
What are literacy rates in African countries</p>
</blockquote>
<p>I have trained my model with the whole vocabulary but validation has no lead to great results. By the whole vocabulary I mean a list containing all the words used as I thought that eliminating prepositions, conjuctions and so on may lead to a loss in semantic meaning.</p>
<p>Now I am trying to find a way to extract the more relevant vocabulary of my documents. I have done some research and I have thought of <em>n-grams</em>. In fact there is a similar example in <a href=""https://cntk.ai/pythondocs/CNTK_303_Deep_Structured_Semantic_Modeling_with_LSTM_Networks.html"" rel=""nofollow noreferrer"">CNKT</a> where the vocabulary they use for the answers is formed both for single words and n-grams but I couldn't find the way to do it by myself yet.</p>
<p>So far I have found a way to do the <em>n-grams</em> but this is not what I want as for example in the sentence:</p>
<blockquote>
<p>the cow jumps over the moon</p>
</blockquote>
<p>I get the following code:</p>
<pre><code>the_cow_jumps
cow_jumps_over
jumps_over_the
over_the_moon
</code></pre>
<p>When I would be interested in the <em>4-gram:</em></p>
<pre><code>cow_jumps_over_moon
</code></pre>
<p>Keep in mind that even eliminating the articles (or stopwords) I would still be getting more than one <em>n-gram</em> which is not what I want as my main objective is get the final vocabulary to train my model.</p>
<p>As an example of what I want could be the words:</p>
<pre><code>book 
book_character 
book_editions_published 
book_subject 
books_published 
</code></pre>
<p>Instead of:</p>
<pre><code>book
character
editions
published
subject
</code></pre>
","nlp"
"30181","How to do give input to CNN when doing a text processing?","2018-04-11 16:51:31","77741","4","252","<machine-learning><python><nlp><cnn>","<p>As a signal processing engineering and being new to NLP, I am confused with giving input to CNN network. </p>

<p>With my knowledge of <a href=""https://github.com/raady07/CNN-for-bearing-fault-diagnosis/blob/master/main_all.py"" rel=""nofollow noreferrer"">CNN</a>, I am trying to build a classifier for ethnicity with inputs as text (last name(LN), middle name(MN), first name(FN)).
I have a list of 8,000,000 samples with last, middle, first names and class information</p>

<pre><code>array = [['person1_LN','person1_MN','person1_FN','Person1_class'],
         ['person1_LN','person1_MN','person1_FN','Person2_class'], 
         ....]
</code></pre>

<p>I want to apply conv layer (CL) followed by Pooling Layer(PL) on LN,MN,FN respectively. </p>

<p><a href=""https://github.com/dennybritz/cnn-text-classification-tf/blob/master/text_cnn.py"" rel=""nofollow noreferrer"">Text processing</a> example demonstrates with the sentences to convert to word embedding. I am trying to understand this code snippet </p>

<pre><code>W = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0), name='W')
self.embedded_chars = tf.nn.embedding_lookup(W, self.input_x)
self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)
</code></pre>

<p>This <a href=""https://rare-technologies.com/word2vec-tutorial/"" rel=""nofollow noreferrer"">tutorial</a> says training to be done using word2vec.
Reading these two blogs I could understand nothing. In Either of the cases, my data is not a sentence, moreover in the second case on what things I have to train?</p>

<p>Will the CL operate if I directly give the input of words without word embeddings? 
If not any example on how to embed the words in my case to give the input to CNN?</p>
","nlp"
"30160","How to classify features into two classes without labels?","2018-04-11 09:32:52","","1","666","<machine-learning><data-mining><nlp>","<p>I have a big dataset with nearly 200 features. However, I do not have class labels for these data. I want to divide these data into two classes based on these features. I know, when we do not have class labels we have to use some clustering method. However, since I do not have any labels, I am just wondering how to measure the accuracy of the models.</p>

<p>Please let me know the most suitable approach that I should follow?</p>

<p>I am happy to provide more details about my featureset if needed :)</p>
","nlp"
"30155","What are the best tools for manually annotating textual topic segmentation?","2018-04-11 07:40:45","30562","1","359","<machine-learning><nlp><annotation>","<p>I checked this <a href=""https://www.quora.com/What-are-the-best-tools-for-manually-annotating-a-text-corpus-with-entities-and-relationships"" rel=""nofollow noreferrer"">old question</a> on tools for text corpus annotation and there are many good tools there, however the tools there seem not to cover my problem. </p>

<p>I have a linear text segmentation problem, in which a text needs to be segmented in different sections (topics). I'm building a model for it, but I have no training data. Therefore, the team decided to manually label some text, by annotating blocks in the text that represent each section. I tried some NER or POS labelling tools, but they are not very convenient for selecting several lines and paragraphs to annotate a label. </p>

<p>Is there a good tool for human annotation of text segmentation? </p>
","nlp"
"30090","Can CBOW model only accept fixed number of words?","2018-04-09 15:42:16","45641","5","153","<deep-learning><word2vec><nlp>","<p>I have a question about CBOW prediction. Suppose my job is to use 3 surrounding words w(t-3), w(t-2), w(t-1)as input to predict one target word w(t). Once the model is trained and I want to predict a missing word after a sentence. Does this model only work for a sentence with four words which the first three are known and the last is unknown? If I have a sentence in 10 words. The first nine words are known, can I use 9 words as input to predict the last missing word in that sentence?</p>
","nlp"
"30088","Skipgram - multiple formulations?","2018-04-09 14:55:46","","4","191","<machine-learning><nlp><word2vec><word-embeddings>","<p>I've been reading about the Skipgram model and I have found what I interpreted as multiple definitions.</p>

<p>1 - Taking a look at <a href=""http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/"" rel=""nofollow noreferrer"">this blog post</a> and <a href=""https://www.coursera.org/learn/nlp-sequence-models/lecture/8CZiw/word2vec"" rel=""nofollow noreferrer"">Andrew Ng's Deep Learning Specialization</a>, I understood that, for each word, we generate <strong>one training sample for each context word</strong>.</p>

<p>So, if we have the sentence ""cat sat on the mat"" we will have samples:</p>

<p><code>(cat, sat)
(sat, cat)
(sat, on)</code></p>

<p>And so on.</p>

<p>Then you train your network which will have the dimensions.</p>

<ul>
<li>Input: $(V,1)$</li>
<li>Weights 1: $(d,V)$</li>
<li>Hidden layer: $(d,1)$</li>
<li>Weights 2: $(V,d)$</li>
<li>Output layer: $(V,1)$</li>
</ul>

<p>Ok, so these dimensions match and we are good. <strong>For one given input, given that the weights are the same, we always have the same estimation in the output layer</strong>.</p>

<p>In this definition we have symmetric samples (for example, <code>(cat, sat)</code> and <code>(sat, cat)</code>) and <strong>saying that we use center words as inputs and context words as outputs is meaningless since they are interchangeable?</strong></p>

<p>2 - Watching <a href=""https://www.youtube.com/watch?v=ERibwqs9p38"" rel=""nofollow noreferrer"">Stanford's NLP with Deep Learning class</a> (38:54), it seems like for the same center word we can get different outputs:</p>

<p><a href=""https://i.sstatic.net/g5w44.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/g5w44.jpg"" alt=""enter image description here""></a></p>

<p>The numbers in the red circles that I drew should be the same from what I understand. I don't really understand how it is possible to get different outputs if you multiple Weights 2 by the Hidden Layer.</p>

<p>3- I saw in other places (couldn't find reference now) another formulation: for each input word, we represent the context as a vector with ones for the context words and zeros elsewhere. So, in this case, instead of one training example for each context word, we would have <strong>one training example for each center word</strong>.</p>
","nlp"
"30075","what actually word embedding dimensions values represent?","2018-04-09 11:01:02","30082","7","1352","<deep-learning><nlp><word2vec><lstm><word-embeddings>","<p>I am learning word2vec and word embedding , I have downloaded GloVe pre-trained word embedding (shape 40,000 x 50)  and using this function to extract information from that:</p>

<pre><code>import numpy as np
def loadGloveModel(gloveFile):
    print (""Loading Glove Model"")
    f = open(gloveFile,'r')
    model = {}
    for line in f:
        splitLine = line.split()
        word = splitLine[0]
        embedding = np.array([float(val) for val in splitLine[1:]])
        model[word] = embedding
    print (""Done."",len(model),"" words loaded!"")
    return model
</code></pre>

<p>Now if I call this function for word 'hello'  something like :</p>

<pre><code>print(loadGloveModel('glove.6B.100d.txt')['hello'])
</code></pre>

<p>it gives me 1x50 shape vector like this:</p>

<pre><code>[ 0.26688    0.39632    0.6169    -0.77451   -0.1039     0.26697
  0.2788     0.30992    0.0054685 -0.085256   0.73602   -0.098432
  0.5479    -0.030305   0.33479    0.14094   -0.0070003  0.32569
  0.22902    0.46557   -0.19531    0.37491   -0.7139    -0.51775
  0.77039    1.0881    -0.66011   -0.16234    0.9119     0.21046
  0.047494   1.0019     1.1133     0.70094   -0.08696    0.47571
  0.1636    -0.44469    0.4469    -0.93817    0.013101   0.085964
 -0.67456    0.49662   -0.037827  -0.11038   -0.28612    0.074606
 -0.31527   -0.093774  -0.57069    0.66865    0.45307   -0.34154
 -0.7166    -0.75273    0.075212   0.57903   -0.1191    -0.11379
 -0.10026    0.71341   -1.1574    -0.74026    0.40452    0.18023
  0.21449    0.37638    0.11239   -0.53639   -0.025092   0.31886
 -0.25013   -0.63283   -0.011843   1.377      0.86013    0.20476
 -0.36815   -0.68874    0.53512   -0.46556    0.27389    0.4118
 -0.854     -0.046288   0.11304   -0.27326    0.15636   -0.20334
  0.53586    0.59784    0.60469    0.13735    0.42232   -0.61279
 -0.38486    0.35842   -0.48464    0.30728  ]
</code></pre>

<p>Now I am not getting what actually these values represent , ( I know its result of hidden layer of single layer neural network ) but my confusion is what actually these weights represent and how it is useful for me?</p>

<p>Because what I was getting suppose if I have :</p>

<p><a href=""https://i.sstatic.net/5gAnY.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/5gAnY.png"" alt=""enter image description here""></a></p>

<p>Here I understand because each word is mapping to each column category label, But in the GloVe there are no columns labels for 50 columns, it just returns 50 values vector, so what actually these vectors represent and what i can do with it? I am trying to find this since 4-5 hours but everyone/every tutorial on the internet explaining what are word embedding and how they looks like but no one explaining what actually these weights represent?</p>
","nlp"
"30057","To extract the skills required for the job given the job description","2018-04-09 04:08:14","","1","6312","<machine-learning><data-mining><nlp>","<ol>
<li><p>Any recommendation on the libraries/methods to extract the skill set required for the job from the job description (raw text) ?</p></li>
<li><p>And also to extract the skillset from the resume ? The resume could be of any format raw text, pdf, docx.</p></li>
</ol>

<p><a href=""https://i.sstatic.net/nZoQT.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/nZoQT.png"" alt=""enter image description here""></a></p>

<p>I have attached the job description format. </p>
","nlp"
"30054","Data normalisation and recommendation based on skillset","2018-04-09 02:52:51","","0","41","<machine-learning><nlp><recommender-system>","<ol>
<li>Given the <strong>job title</strong> return the skillset required for the job </li>
<li>If a user is lacking some skills required for the job, we have to suggest the courses the user has to take to bridge the gap.</li>
</ol>

<p>Problem with 1) is:</p>

<p>1) The provider is returning the skillset only if the keyword(tile of the job) is exact. For instance: For ""Accountant"" it gives the skillset whereas for ""Accounting"" it didn't give any result. One another example is: It gives result for ""Developer"" not for ""Software Engineer"" or ""Software Developer""</p>

<p>Does NLP or machine learning solves this issue? If yes, can you give some recommendation on how this problem could be approached.</p>

<p>For 2): What is the way to handle recommendations?</p>
","nlp"
"29938","Plagiarism detection with Python","2018-04-05 14:14:12","36114","4","4853","<python><nlp><fuzzy-logic>","<h1>Background</h1>
<p>Using Python, I need to score the existence of a quote, containing around 2-7 words, a longer text. The quote doesn't have to match the text precisely, but similar words should have the <em>same order</em>.</p>
<p>For example, given the following <strong>long text</strong>:</p>
<blockquote>
<p>The most beautiful things in the world cannot be seen or touched, they are felt with the heart</p>
</blockquote>
<p>The following quotes should be scored <strong>high</strong> (say, above 80 / 100):</p>
<blockquote>
<p>The beautiful thing in our world</p>
<p>World cannot see</p>
<p>They feel with the heart</p>
</blockquote>
<p>Since they are not precise, but they preserve the order.</p>
<p>While, on the other hand, these quotes should be scored <strong>lower</strong> (say, below 50 / 100):</p>
<blockquote>
<p>The beautiful heart cannot be felt or seen</p>
<p>They are the most seen in the world</p>
<p>These words don't even appear on this text</p>
</blockquote>
<p>Because (the first 2) appear entirely in the text, but do not preserve the order.</p>
<h1>The problem</h1>
<p>This task cannot be accomplished by simply checking the existence of each word in the text. I don't know which algorithm fits best for this task.</p>
<h1>What I have tried</h1>
<p>Most of the functions in <code>fuzzywuzzy</code> (<code>partial_token_sort_ratio</code>, <code>token_sort_ratio</code> and etc) scored the later terms higher.
<code>partial_ratio</code> did score the earlier terms higher, but the quote</p>
<blockquote>
<p>These words don't even appear on this text</p>
</blockquote>
<p>Got 52 / 100 which is unreasonably high.</p>
<h1>My question</h1>
<p>How can I use python to score the existence of short quotes in longer texts as mentioned above?</p>
","nlp"
"29916","How to select features for clustering to detect the number of different unique products in a search result?","2018-04-05 01:44:21","","0","95","<scikit-learn><clustering><k-means><nltk><nlp>","<p>I am trying to use clustering to determine the number of products in a search of products. So far I am using kmeans clustering. I have run into a problem where I cannot determine good features to use. <a href=""https://i.sstatic.net/zV56U.jpg"" rel=""nofollow noreferrer"">Here are my clustering results for one search.</a></p>

<p>As you can see, there is clearly three different groups showing up at the moment. However, when you look at the <a href=""https://i.sstatic.net/afSzy.jpg"" rel=""nofollow noreferrer"">top terms of each cluster</a>, there is a clear issue. A lot of the terms are repeating in each cluster. </p>

<p>There is another issue, when I try to have the model predict which clusters a few test items are in, which are clearly different products, it says that they are in the same group. Here are the test terms:</p>

<pre><code>Adjustable DC-DC LM 2596 Converter Buck Step Down Regulator Power Module LW SZUS
10pcs Mini360 3A DC Voltage Step Down Power Converter Buck Module 3.3V 5V 9V 12V
5V Out, 6V to 12V In AMS1117-5.0 5.0V Step-Down Linear Voltage Regulator Module
10pcs Mini LM2596s 3A DC to DC Buck Converter Power Supply Step Down Module
</code></pre>

<p>I have thought about using the grammar in each title as a feature, but as there is an 80 character limit for the titles, there rarely is any grammar. I am at a loss for where to go next in terms of trying to pick more features that would get me better results. I am doing all of this in python using scikitlearn and nltk.</p>
","nlp"
"29886","Extract details from bibliometrics data","2018-04-04 07:27:36","","2","87","<data-mining><nlp><text-mining><social-network-analysis>","<p>I have set of bibliometrics data (references). I want to extract the author names, title and the name of the conference/journal from it. Since the referencing style used by different papers vary, I am interested in knowing if there are any per-existing tools to do it?</p>

<p>I am happy to provide examples if needed :) </p>
","nlp"
"29857","Splitting a sentence to meaningful parts","2018-04-03 15:18:33","","5","3927","<machine-learning><nlp>","<p>I have the following sentence</p>

<pre><code>query = u'tell me about people in konoha who have wind style chakra and are above jonin level'
</code></pre>

<p>I want to split the above sentence into the following three parts</p>

<pre><code>[
 ""tell me about people in konoha"",
 ""who have wind style chakra"",
 ""and are above jonin level""
]
</code></pre>

<p>I have looked into sent tokenizers in <strong>spacy</strong> and <strong>nltk</strong>, but they are not giving me the desired results.</p>

<p>I am just a beginner in nlp and machine learning and have very limited knowledge so far. It would be awesome if you could direct me to some techniques or available packages through which I could achieve the above results.</p>

<p><strong>EDIT</strong></p>

<p>I'll be having question-like queries like the one above where I would be asking for details about the subject based on some constraints</p>

<p>In the above query the subject was <strong>people in Konoha</strong> and the constraints are <strong>have wind style chakra</strong> and <strong>above jonin level</strong></p>

<p>I want to extract such relationships from similar queries.</p>
","nlp"
"29818","Stacking/Concatenating/Combining two vector space models","2018-04-02 14:32:10","","0","780","<machine-learning><nlp><vector-space-models>","<p>I have two vector-space models,  with different dimensions.</p>

<p>The number of vectors in one model is the same as the number of vectors in the other. I.E: if I have vector representation for a <em>car</em> in one model,  I have vector representation for a <em>car</em> in the other model, but the number of dimensions can be different.</p>

<p>I want to combine these models (and then cluster using the combined model),  I cannot average (BoW) or add these models together as stated earlier they have different dimensions. </p>

<p>I was going to simply concatenate the vectors. Is this valid or  is there better way to do this? </p>

<p>Also before you concatenate should you normalize? </p>
","nlp"
"29733","Incorporating new features in document similarity task","2018-03-30 13:50:39","","0","101","<nlp><feature-extraction><similar-documents>","<p>I have a model pipeline for finding similar text documents given an input query text. The model is very simple; I have a corpus of documents on which I train a <a href=""https://en.wikipedia.org/wiki/Tf%E2%80%93idf"" rel=""nofollow noreferrer"">TfIDF</a> model. When a query is input, we can infer its TfIDF vector. Finally, we compare the query's TfIDF vector to all the vectors of the documents in the corpus using cosine similarity, which finds us the ""most similar"" texts.</p>

<p><strong>My question is, is there a way to incorporate more micro structure features into the pipeline such that similar document retrieval will perform better, and if so how?</strong></p>

<p>By this I mean, can we use Part of Speech tagging, intent recognition or other methods, as extra features in the pipeline. I am looking for more unsupervised methods here as I have a lot of data (1TB) and it is all unstructured and un-tagged / un-labelled, but supervised suggestions will also be welcome. I appreciate that by incorporating new features, we will most likely have to move to a completely different model paradigm as TfIDF and cosine similarities will not work depending on the structure of the new features.</p>

<p>NOTE: I have already performed a significant amount of text pre-processing on the corpus e.g. stemming, tokenizing, word replacement, stop word removal etc.</p>
","nlp"
"29583","How do Dynamic Memory Network scale to large inputs?","2018-03-27 11:06:45","","1","30","<neural-network><deep-learning><nlp>","<p>How do Dynamic Memory Network, for example from the paper <a href=""https://arxiv.org/pdf/1506.07285.pdf"" rel=""nofollow noreferrer"">Ask Me Anything:
Dynamic Memory Networks for Natural Language Processing</a>, scale to large inputs? The paper states the following:</p>

<blockquote>
  <p>In these cases, the input may be a sentence, a long story, a
  movie review, a news article, or several Wikipedia articles.</p>
</blockquote>

<p>Did anyone actually implement this network architecture to read an arbitrarily large corpus of text (say 250 Wikipedia articles)? The only example I could find online use rather small inputs (a few lines of text).</p>

<p>What is the bottleneck of using this type of network with large inputs? One solution I see is to encode paragraphs instead of sentences.</p>
","nlp"
"29581","Theoretical and practical comparison of CTC and seq2seq loss in Tensorflow","2018-03-27 10:54:41","37231","5","1166","<machine-learning><neural-network><deep-learning><nlp><tensorflow>","<p>Tensorflow has built-in implementations for both, the <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/ctc_loss"" rel=""noreferrer"">Connectionist Temporal Classification (CTC) loss </a> and a special <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/sequence_loss"" rel=""noreferrer"">seq2seq loss (weighted cross-entropy)</a>.</p>

<p>Since CTC loss is also intended to deal with seq2seq mappings, I wonder about how the two loss functions compare. I went through an <a href=""https://distill.pub/2017/ctc/"" rel=""noreferrer"">excellent explanation</a> of CTC loss, finding out that the target sequence is restricted not to be longer than the input sequence whereas this restriction does not exist for seq2seq loss.</p>

<p>Some questions I have (ranked by relevance):</p>

<ol>
<li>Are bidirectional LSTMs suited for seq2seq loss? I have only found b-LSTM implementations with CTC or even simpler loss functions. There is a <a href=""https://github.com/tensorflow/tensorflow/issues/3585"" rel=""noreferrer"">post from 2016</a> saying that this was not possible at that time. Any updates?</li>
<li>Are there conceptual differences apart from the many-to-one restriction in CTC loss that seq2seq does not have? E.g. does the seq2seq loss make a similar conditional independence assumption like the CTC loss?</li>
<li>Do the loss functions have preferences regarding the optimizer (RMSProp, ADAM, Momentum etc.)</li>
<li>Do the loss functions work better for particular sets of tasks?</li>
</ol>

<p>Theoretical arguments and practical experiences are both appreciated.</p>
","nlp"
"29561","How to extract entities from text using existing ontologies?","2018-03-26 23:29:41","","1","1153","<nlp><text-mining><named-entity-recognition><stanford-nlp>","<p>I am working on a entity extraction task and I am using Stanford CoreNLP NER. Here, I want to detect entities of type ""Animal"", ""Building"", ""Imagery"", etc., which are not covered in Stanford CoreNLP entity extraction method. Also, I want to detect organization entities which are not included in CoreNLP entity model. I have the data about the organizations with me. </p>

<p>How can I do this?</p>
","nlp"
"29529","How to alter word2vec wikipedia model for n-grams?","2018-03-25 23:34:19","29537","0","4712","<neural-network><nlp><text-mining><word2vec><word-embeddings>","<p>I have a very little data, so my word2vec model does not perform well. My intention is to identify words similar to technical terms such as 'support vector machine', 'machine learning', 'artificial intelligence' etc.</p>

<p>I am interested in knowing if I can use the Google's wikipedia model for this. But according to my model most of the words I will be dealing with are n-grams. Hence, how can I utilise this Google's wikipedia model that is based on unigrams to achieve my task?</p>

<p>I am happy to provide more examples if needed :)</p>
","nlp"
"29469","Implementation of LDA (Latent Dirichlet Allocation) for classification tasks","2018-03-23 11:39:45","","1","416","<classification><nlp><lda>","<p>Until now I have used LDA only for <em>topic modelling</em>. I would like to know which is the simplest implementation of LDA algorithm for classification tasks.</p>
","nlp"
"29458","How to calculate lexical cohension and semantic informaticveness for a given dataset?","2018-03-23 04:35:32","","1","87","<data-mining><nlp><text-mining><statistics>","<p>In <a href=""https://onlinelibrary.wiley.com/doi/full/10.1002/widm.1097"" rel=""nofollow noreferrer"">'Automatic construction of lexicons, taxonomies, ontologies, and other knowledge structures'</a> they have mentioned;</p>

<p><em>There are two slightly different classes of measure: <strong>lexical cohesion (sometimes called ‘unithood’ or ‘phraseness’)</strong>, which quantifies the expectation of co-occurrence of words in a phrase (e.g., back-of-the-book index is significantly more cohesive than term name); and <strong>semantic informativeness (sometimes called ‘termhood’)</strong>, which highlights phrases that are representative of a given document or domain.</em></p>

<p>However, the review does not include the ways to calculate/derive these measures. Can someone please specify how to get these two measurements for a given text documents?</p>
","nlp"
"29432","What distance should I use for edges weights in textrank algorithm","2018-03-22 16:36:02","29452","1","150","<nlp>","<p>I found <a href=""https://github.com/davidadamojr/TextRank"" rel=""nofollow noreferrer"">this</a> python implementation on github with 400+ stars which use levenshtein distance between each nodes.</p>

<p>But <a href=""http://www.aclweb.org/anthology/W04-3252"" rel=""nofollow noreferrer"">original paper (page 4)</a> said: </p>

<blockquote>
  <p>Next, all lexical units that pass the syntactic filter
  are added to the graph, and an <strong>edge is added between
  those lexical units that co-occur within a window of N word</strong></p>
</blockquote>

<p>So the question is: is levenshtein distance legit for this algorithm or better to rewrite with windowed edges?</p>

<p>Intuitively levenshtein distance must not work because it's not denote importance of word...</p>
","nlp"
"29427","Can LSTM have a confidence score for each word predicted?","2018-03-22 15:25:43","29624","0","1925","<neural-network><nlp><lstm><prediction>","<p>LSTM networks can be used to generate new text. Given a sequence, I can predict the next word. Is there a way to get a score associated to each word predicted?</p>

<p>In particular, if the new word has never been seen by the LSTM network, can we train the LSTM to output a score of ""no confidence""?</p>

<p>For example, this <a href=""https://arxiv.org/pdf/1602.06291.pdf"" rel=""nofollow noreferrer"">article</a> gives the following example:</p>

<pre><code>For example, let us consider the following three text segments:

1) Sir Ahmed Salman Rushdie is a British Indian novelist
and essayist. He is said to combine magical realism with
historical fiction.

2) Calvin Harris &amp; HAIM combine their powers for a magical
music video.

3) Herbs have enormous magical power, as they hold the
earth’s energy within them.

Consider an LM that is trained on a dataset having the
example sentences given above — given the word “magical”,
what should be the most likely next word: realism, music, or power?
</code></pre>

<p>Say, that, in fact, the next word is neither one, but ""power"", but my LSTM has never seen that word before. So, the LSTM is going to predict one of the three words it has seen, but I would like it to output a low confidence score. Is this possible?</p>
","nlp"
"29337","How to measure Entity Ambiguity?","2018-03-20 20:15:47","","3","402","<nlp><text-mining><metric><named-entity-recognition><research>","<p>When using/building a system for <a href=""https://en.wikipedia.org/wiki/Entity_linking"" rel=""nofollow noreferrer"">Entity Linking</a>, is there a well-known measure for ""ambiguity degree"" of an entity?</p>

<p>Some approach to compare named entities regarding how difficult to disambiguate?</p>
","nlp"
"29329","How to get the tagset for hindi pos tagging?","2018-03-20 18:08:42","","2","960","<nlp>","<p>I am trying to tag Hindi text using python(<code>nltk</code> library). I have been successful but I am unable to understand some of the tags.  </p>

<p>I tried searching for tagset but the only information I could find is about some <code>upenn_tagset</code>. The tags that I get matches only some of the tags from <code>upenn_tagset</code>.  </p>

<p>Does anyone know how to get the tagset for hindi?</p>
","nlp"
"29278","Detecting Offensive Text Content in English and German","2018-03-19 19:07:15","29282","1","1000","<classification><nlp>","<p>I am working on an NLP project about classifying offensive text data in social media. By offensive I especially mean threat words that one say to another.
<br/><br/>
<strong>Some examples:</strong></p>
<blockquote>
<p>&quot;Stop doing this or else you will pay for it.&quot;</p>
<p>&quot;Just wait until you see what's coming&quot;</p>
<p>&quot;I will break your legs next time I see you.&quot;</p>
</blockquote>
<p>As an initial approach, I considered semantic and syntactic keyword matching. However, doing this on this very problem seemed harder because threating is an action and it is expressed in so many different ways.
<br/><br/>
My main goal is classifying text data by offensive/non-offensive text by using Machine Learning and Deep Learning algorithms. After weeks of online searching, I could not find a ready-to-use dataset. I considered manually labelling the data. However, I don't know where I should start.</p>
<p>What would be the best approach for making progress in this task? I also plan to do this in both English and German languages.</p>
<p>Also, below is a related article for fully understanding of the problem:<br/>
<a href=""https://link.springer.com/content/pdf/10.1007%2Fs41060-017-0088-4.pdf"" rel=""nofollow noreferrer"">Deep learning for detecting inappropriate content in text</a></p>
","nlp"
"29208","How do I learn encoding of a text that is encoded at character level?","2018-03-18 08:59:43","","1","41","<deep-learning><nlp>","<p>Encoded text at character level:</p>

<blockquote>
  <p>tvletwgzkrqvuhtwamuluhpkskpmpmiwtvuhamqvmviwlrvikquhtwamuluhqgvipmmvulkriwpmqvtwleuhamqvmviwlrvikquhtwamuluhqgqvqvtwviezlemvxeuhamqvmviwlrvikquhtwamuluhpkskvieniwlrvikquhqvmvuhqgpmpmiwletwulenokuhxepmuhtwiwululentvuhtwamuluhvimvuhsktwlemvezskenuhtwtvuhulqvkrezuhamypmvamdfuhulenamguuhraskvipmyptwqvuhtwamuluhxepmuhvimvenulgzenypuhenuhsatvuhvipmdfuhqgletwsklepmuhulqvlemvxeuhtwamuluhxepmuhtwiwululentvuhenuhqvmvuhpmpmiwletwulenok</p>
</blockquote>

<p>Map to one of 10 classes: 0-9.</p>
","nlp"
"29202","Confusion about Keras' skipgram and sampling table utilities","2018-03-18 05:30:29","","4","862","<machine-learning><nlp><keras><word2vec>","<p>I'm fairly new to ML, so as a learning exercise to get familiar with Keras I'm trying to learn some word2vec style embeddings from a dataset. I'm confused about the behavior of the <a href=""https://keras.io/preprocessing/sequence/#skipgrams"" rel=""noreferrer"">skipgram utility</a>, in particular the sampling table argument that's supposed to be populated from <a href=""https://keras.io/preprocessing/sequence/#make_sampling_table"" rel=""noreferrer"">make_sampling_table</a>. I understand the idea behind it, but I'm seeing some strange outputs.</p>

<p>As a toy example, let's pretend that we have the following parameters:</p>

<pre><code>vocab_size = 3
sequence = [0,1,0,1,1,0]
</code></pre>

<p>So a small vocabulary with 3 words in it, only 2 of which show up in the sequence. Generating the skipgrams &amp; labels works fine without a sampling table at all:</p>

<pre><code>pairs, labels = skipgrams(sequence, vocab_size)
print(""pairs: "" + str(pairs) + "" labels: "" + str(labels))

OUTPUT: pairs: [[1, 1], [1, 2], [1, 1], [1, 2], [1, 1], [1, 1], [1, 1], [1, 2], [1, 1], [1, 1], [1, 1], [1, 1]] labels: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1]
</code></pre>

<p>But when I try to pass in a sampling table so as to not oversample the common words, I get no outputs at all:</p>

<pre><code>pairs, labels = skipgrams(sequence, vocab_size, sampling_table=make_sampling_table(vocab_size))
print(""pairs: "" + str(pairs) + "" labels: "" + str(labels))

OUTPUT: pairs: [] labels: []
</code></pre>

<p>My question is: what's going on here? Why does the act of passing a sampling table into the skipgrams method make it produce no output?</p>

<p>Thanks for the help!</p>
","nlp"
"29175","How are dynamic memory networks employed in sequence to sequence modelling","2018-03-16 20:50:51","","2","22","<deep-learning><nlp><stanford-nlp>","<p>Dynamic Memory networks are described <a href=""https://arxiv.org/pdf/1506.07285.pdf"" rel=""nofollow noreferrer"">here</a>
. I understand what is going on for question answering task but when it comes to sequence to sequence modeling, they describe it in 4th paragraph of 2.4 answer module.</p>

<blockquote>
  <p>In the sequence modeling task, we wish to label each word
  in the original sequence. To this end, the DMN is run in
  the same way as above over the input words. For word t,
  we replace Eq. 8 with $e^i = h^{i}_{t}$
  . Note that the gates for the
  first pass will be the same for each word, as the question is the same. This allows for speed-up in implementation
  by computing these gates only once. However, gates for
  subsequent passes will be different, as the episodes are different.</p>
</blockquote>

<p>I could not understand how the global gates are computed and why would they be equal for each word for first pass. They are going to be computed as a function of question vector (same) and word vector (different for each word). This is how vague i am on what is going on here. Can somebody explain how the piece in the paragraph mentioned come together?</p>
","nlp"
"29162","How do NLP tokenizers handle hashtags?","2018-03-16 14:55:49","29167","0","1621","<nlp><tokenization>","<p>I know that tokenizers turn words into numerics but what about hashtags? Are tokenizers design to handle hashtags or should I be filtering the ""#"" prior to tokenizing? What about the ""@"" symbol? </p>
","nlp"
"29156","Should I remove features that occur very rarely to build a model?","2018-03-16 13:53:15","29158","0","63","<feature-selection><preprocessing><naive-bayes-classifier><nlp>","<p>I am trying ML techniques in language processing.
I have got 3000 short texts and I extract features(words and phrases) from all of them and build a vocabulary. I end up with 6000 od features and most of them occurs once or twice.
So for example from texts:</p>
<blockquote>
<p>0: One text here</p>
<p>1: Another text there</p>
</blockquote>
<p>I got</p>
<pre><code>    One Another  Text  Here  There   Target
  0   1       0     1      1     0     True
  1   0       1     1      0     1    False
</code></pre>
<p>So if word &quot;one&quot; occured once I have got as a column and it is False for all other 2999 texts.
Should I drop these columns? Or should use different technique?
This amount of columns make me some problems becuase it takes a lot of time to build a classify.</p>
","nlp"
"29148","How to read Feature Based Grammar from a string","2018-03-16 09:31:19","","0","167","<python><nlp><nltk>","<p>To read Context Free Grammars we can use </p>

<pre><code>nltk.CFG.fromstring("""""" 
S-&gt; NP VP`...
    """""")
</code></pre>

<p>But it can't read a feature based grammar which is like?</p>

<pre><code>S -&gt; NP[NUM=?n] VP[NUM=?n]
</code></pre>

<p>So how to read feature based grammar from a string without actually having to save it in a fcfg file to load it later?</p>
","nlp"
"29117","what machine/deep learning/ nlp techniques are used to classify a given words as name, mobile number, address, email, state, county, city etc","2018-03-15 13:17:31","30144","10","1208","<machine-learning><deep-learning><text-mining><nlp>","<p>I am trying to generate an intelligent model which can scan a set of words or strings and classify them as names, mobile numbers, addresses, cities, states, countries and other entities using machine learning or deep learning.</p>

<p>I had searched for approaches, but unfortunately I didn't find any approach to take.  I had tried with bag of words model and glove word embedding to predict whether a string is name or city etc..</p>

<p>But, I didn't succeed with bag of words model and with GloVe there are a lot of names which are not covered in the embedding example :- lauren is present in Glove and laurena isn't</p>

<p>I did find this post <a href=""https://datascience.stackexchange.com/questions/8542/appropriate-algorithm-for-string-not-document-classification"">here</a>, which had a reasonable answer but I couldn't the approached used to solve that problem apart from the fact that NLP and SVM were used to solve it.</p>

<p>Any suggestions are appreciated</p>

<p>Thanks and Regards,
Sai Charan Adurthi.</p>
","nlp"
"29063","Text analysis - classification, parsing","2018-03-14 08:41:34","","0","713","<deep-learning><convolutional-neural-network><nlp>","<p>Excuse if this has been answered before. </p>

<p>I need to extract features and parse from a piece of text and run some analysis.
For e.g. ""Plot the past 5-year sales of Apple"" should give me the following </p>

<p>Information:</p>

<ol>
<li>Company - Apple </li>
<li>Item - Sales </li>
<li>Period - past 5 years </li>
<li>Action - Plot</li>
</ol>

<p>What deep learning techniques / algorithms should I be looking to use? </p>

<p>Any pointers are highly appreciated.</p>
","nlp"
"29053","Proper/Possible methods for extracting unstructured data from websites","2018-03-14 01:53:39","29060","0","257","<machine-learning><nlp><feature-extraction><nltk><scraping>","<p>I'm working in Python, using Scrapy, and NLTK to try to understand how I can extract data from college websites. </p>

<p>My scraper can navigate through the university websites and find their tuition fees pages perfectly , but when trying to extract specific fees like : </p>

<ol>
<li>Resident </li>
<li>Non Resident </li>
<li>Per Credit Hour</li>
<li>Per Semester </li>
</ol>

<p>I'm running into trouble due to the data being so unstructured from site to site.</p>

<p>I've tried using NLTK to parse data based on parts of speech tags and regex chunking to try to extract sentences such as ""tuition cost for resident: $12,500"" but colleges can display this data in a number of ways.</p>

<p>Here is my question:</p>

<p>Are there any better ideas/methodologies that I should be looking into that can help me with extracting this type of data?</p>
","nlp"
"28835","How to fix these vanishing gradients?","2018-03-08 22:53:59","","5","5113","<deep-learning><neural-network><tensorflow><nlp><convolutional-neural-network>","<p>I am trying to train a deep network for twitter sentiment classification. It consists of an embedding layer (word2vec), an RNN (GRU) layer, followed by 2 conv layers, followed by 2 dense layers. Using ReLU for all activation functions.</p>

<p>I have just started using tensorboard &amp; noticed that I seemingly have extremely small gradients in my convolutional layer weights (see figure)</p>

<p><a href=""https://i.sstatic.net/IdRAo.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/IdRAo.png"" alt=""enter image description here""></a></p>

<p>I believe I have vanishing gradients since the distribution of CNN filter weights does not seem to change &amp; the gradients are extremely small relative to the weights (see figure). [NOTE: the figure shows layer 1, but layers 2 looked very similar] </p>

<p>My questions are:</p>

<p>1) Am I interpreting the plots correctly that I do indeed have vanishing gradients &amp; thus my convolutional layers arent learning? Does this mean they are currently essentially worthless? </p>

<p>2) What can I do to remedy this situation?</p>

<p>Thanks!</p>

<p><strong>UPDATE 3/13/18</strong></p>

<p>Few comments:</p>

<p>1) I have tried the network w/ just 1 layer and no layers (RNN-->FC), and having 2 kayers does empirically improve performance.</p>

<p>2) I have tried Xavier initialization and it doesnt do much (the previous default initialization mean value of .1 was very close to the Xander value) </p>

<p>3) By quick math, the gradients seem to change on the order of 1e-5, while the weights themselves are on the order of 1e-1. Thus at every iteration, the weights change 1e-5/1e-1*100% = ~.01%. Is this to be expected? What is the threshold for how much the weights change until we consider them to have converged / consider the changes to be useless in the sense that they dont change outcome?</p>
","nlp"
"28787","Make a chatbot using slack","2018-03-08 07:48:00","","0","53","<machine-learning><nlp>","<p>How did anaek created chat bots using slack for HR Tools and how much time will it take as a fresher to make a similar product?</p>
","nlp"
"28780","Data Mining - Intent matching and classification of text","2018-03-08 00:27:18","","1","194","<classification><data-mining><nlp><text-mining><svm>","<p><strong>PROBLEM</strong></p>

<p>Suppose you have a list of 100,000+ google queries related to travel bookings. For example:</p>

<pre><code>hotels in london
barcelona flight
city breaks to berlin
khao san road hostel
luxury holiday to paris
new york business class flight price
disneyland trip...
</code></pre>

<ol>
<li>How can I extract the location i.e., London</li>
<li>Classify the Line of Business i.e., flight, hotel, package, etc</li>
<li>Classify the affinity i.e., luxury, family, city break, beach etc</li>
<li><p>Use this info to record the frequency of various pattern that exists in the keywords </p>

<pre><code>i.e. **keyword pattern**                **frequency**
(destination) hotel                          xxx
flight to (destination)                      yyy
(theme) (destination) hotel                  zzz
</code></pre></li>
</ol>

<p><strong>POTENTIAL SOLUTIONS</strong></p>

<ol>
<li><p><strong>Manual</strong> - get an exhaustive as possible list of locations (most of
the queries will be for tourist destinations) and look for a match
against the keywords. Again, compare the keywords against possible
Line of Business Identifiers and Affinity Identifiers</p></li>
<li><p><strong>Google Cloud Natural Language API</strong> - This can be used to analyse
Entities and Sentiment of text. e.g.
     hotels in london -> entity(hotels), entity(london)
     barcelona flight -> entity(flight), entity(barcelona)
This isn't very powerful and only supports english.</p></li>
<li><p><strong>Machine learning</strong> - seems difficult as I haven't got any descriptors
for the keywords. Would Naive Bayes be applicable or SVM?</p></li>
</ol>

<p>I'd preferably like to run any solution in R too. </p>

<p>Could someone please suggest a direction/potential solution?</p>
","nlp"
"28739","Why are Chunking and IOB tags necessary?","2018-03-07 06:26:40","","2","2112","<nltk><named-entity-recognition><nlp>","<p>I've just come across chunking and I can't get my head around why is it necessary? I know that it is used for 'named entity recognition'. I have few questions:</p>

<ul>
<li>Why and how is Chunking helpful?</li>
<li>Plus aren't POS tags enough?</li>
<li>Why are three tags necessary for the representation? Why not just use 'inside' tag and 'outside' tag?</li>
</ul>
","nlp"
"28714","Is there ""Attention Is All You Need"" implementation in Keras?","2018-03-06 16:06:27","44684","6","8306","<deep-learning><nlp><keras><machine-translation>","<p>Has anyone seen <a href=""https://arxiv.org/abs/1706.03762"" rel=""nofollow noreferrer"">this model's</a> implementation <strong>using Keras</strong>?</p>

<p><strong>inb4</strong>: <a href=""https://github.com/Kyubyong/transformer"" rel=""nofollow noreferrer"">tensorflow</a>, <a href=""https://github.com/jadore801120/attention-is-all-you-need-pytorch"" rel=""nofollow noreferrer"">pytorch</a></p>
","nlp"
"28661","Training data for multi-category classification algorithm","2018-03-05 19:45:35","","2","484","<neural-network><nlp><multiclass-classification><lstm>","<p>I am putting together a multi-category classification algorithm. Since it's NLP, the training data is very simple with one column for labels and another column for text. However, because it's NLP, some training records can fit multiple categories. Should I iterate through &amp; have multiple entries (same data, different labels) or should each piece of data only have 1 category assigned? </p>
","nlp"
"28615","Bag of words and word2vec clarifications","2018-03-04 21:18:02","","0","1334","<nlp><word2vec>","<p>I have documents,  and I calculated the word vectors using word2vec for all the terms in my corpus.</p>

<p>Now I want to compute similarity between documents using the bag-of-words model.  After  creating the Bag-of-words ( consisting of  vector representations). </p>

<p>how do I now compute similarity between a sequence of vectors? </p>

<p>Do I simply just take the mean? and calculate the cosine similarity between the mean vectors? </p>

<p>or is there a better approach for computing similarity between two bags?</p>
","nlp"
"28598","Word2Vec embeddings with TF-IDF","2018-03-04 12:07:33","","17","34038","<machine-learning><nlp><word2vec><language-model><tfidf>","<p>When you train the word2vec model (using for instance, gensim) you supply a list of words/sentences.  But there does not seem to be a way to specify weights for the words calculated for instance using TF-IDF.  </p>

<p>Is the usual practice to multiply the word vector embeddings with the associated TF-IDF weight? Or can word2vec organically take advantage of these somehow?</p>
","nlp"
"28583","Words as features of a neural networks","2018-03-04 02:28:41","28586","1","1192","<machine-learning><neural-network><deep-learning><lstm><nlp>","<p>I'm new in Machine learning and I'm working on a problem related to text. I know that in ML we can use features as numerical values as input to neural network, but I don't know how to use features as words. In some papers I read that we take features to be n words with some property. I really don't understand how is that possible. Please, if it is not a problem, just to tell me some good papers or textbooks or links where it is explained how to do that.</p>
","nlp"
"28538","Word2Vec, softmax function","2018-03-02 20:33:17","28560","1","2762","<machine-learning><nlp><word2vec><language-model>","<p>I was going term by term through the softmax function for the word2vec (SKIP-GRAM) model.  I found  most definition of these functions to be not 'clear'  so I modified the notation to make sure I understand it. </p>

<p><strong>Is the following  formulation correct?</strong></p>

<p>$$P(w_{-t} | w_{t} ; \theta) = softmax(score(w_{-t}, w_t))$$</p>

<p>$$P(w_{-t} | w_{t} ; \theta) = \frac{exp(score(w_{-t}, w_t))}{\sum_{w' \in \theta} exp(score(w', w_{t}))}$$</p>

<p>where:   </p>

<p>$w_{-t} =$ context</p>

<p>$w_{t} =$ target word</p>

<p>$score(A,B)$ a measure of similarity between vectors A and B.</p>

<p>$\theta = $ vector representation for all words in vocabulary </p>

<p>In the simplest case:</p>

<p>$$score(A ,B) = A \cdot B$$</p>

<hr>
","nlp"
"28509","Organization of layers in Keras for a NLP problem","2018-03-02 07:35:00","","4","199","<deep-learning><nlp><keras>","<p>I have been trying out an NLP problem where I have to predict multi-label-sentiments for some text.</p>

<p>I have 8 labels and 170k training examples, and 140k for the test set.
My final dictionary size is around 190k.</p>

<p>I am using Keras for trying out an NN approach, although I am not sure if my architecture is right, Below is the model which gives me a 95 % on the test set, I am testing out accuracies on 0.7 - 0.3 split while training:</p>

<pre><code>model = Sequential
model.add(Embedding(max_indexes + 1, 100, weights=[embeddings], 
                  input_length=100))

model.add(Bidirectional(LSTM(256, return_sequences=True)))

model.add(Convolution1D(256, 5, padding='same'))
model.add(Activation('relu'))
model.add(BatchNormalization())

model.add(Convolution1D(256, 5, padding='same'))
model.add(Activation('relu'))
model.add(BatchNormalization())

model.add(Convolution1D(256, 5, padding='same'))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(MaxPool1D())

model.add(Convolution1D(256, 4, padding='same'))
model.add(Activation('relu'))
model.add(BatchNormalization())

model.add(Convolution1D(256, 3, padding='same'))
model.add(Activation('relu'))
model.add(BatchNormalization())

model.add(Convolution1D(256, 3, padding='same'))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(MaxPool1D())

model.add(Flatten())

model.add(Dense(64, activation='relu',
                kernel_regularizer=keras.regularizers.l2(0.02)))

model.add(Dense(target_classes_len, activation='sigmoid',
                kernel_regularizer=keras.regularizers.l2(0.02)))
model.add(Dropout(0.1))

adam_opt = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)

early_stopping = EarlyStopping(monitor='val_loss', patience=5, mode='min')

save_best = ModelCheckpoint('model_x.hdf', save_best_only=True, 
                               monitor='val_loss', mode='min')
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,
                              patience=5, min_lr=0.0001)

model.compile(adam_opt, 'binary_crossentropy', metrics=['accuracy'])
</code></pre>

<p>Over the internet, and while watching a video of CSE231N and other StackOverflow questions, I found out I should follow:</p>

<pre><code>[(CONV-RELU)*n - POOL? ]*m - (FC-RELU)*K, Softmax
 N~5, M~very large, K&gt;=0 and K&lt;=2
</code></pre>

<p>Although it was for ImageNet classification problem and I have an NLP problem at hand.</p>

<p>Also, I found out that should place DropOut after final FullyConnected and BatchNormalization in a sequence with CONV-RELU.</p>

<p>I justed wanted to know what would a good organization of layers for an NLP problem, or any way of evaluating different architectures?</p>

<p>Thanks</p>
","nlp"
"28464","How can I create a ""trained"" dataset for categorizing news articles?","2018-03-01 15:04:19","","1","477","<python><classification><data-mining><nlp><naive-bayes-classifier>","<p>I am trying to automatically categorize news articles according to their primary topics, i.e. politics, entertainment, sports, business, technology, health, etc.</p>
<p>There are some labeled datasets out there, but ideally I would like to create my own (for potential commercial usage later on). I am using python, but an answer clear enough with relation to any language would be sufficient.</p>
<p>So, what would be the best way to go about this task?</p>
<p>My current thoughts are:</p>
<ul>
<li><p>Determine the most popular keywords for each category, then associate
each keyword/keyword set with each respective category, and then use
an algorithm to apply a &quot;category&quot; label to a large set of scraped
articled based on the predefined keyword sets.</p>
</li>
<li><p>Another option is to scrape article from specific sections of news
sites where the categories are already specified, and apply them to
each individual constituent article.</p>
</li>
</ul>
<p>After I have the trained dataset, I plan to implement Naive Bayes classification method to automatically categorize future articles.</p>
<p>...</p>
<p>As you can see I have some ideas, but because the web is a large and magical place, I assume someone with previous experience doing something like this may be able to reduce my effort expenditure by guiding me toward the most feasible solution.</p>
","nlp"
"28375","NLP grouping word categories","2018-02-27 18:46:36","28381","1","390","<nlp>","<p>Suppose I have a dictionary:</p>

<pre><code>{apple:large apple, apple:red apple, apple:aple, orange:mandarin, orange:orang, orange:blood orange}
</code></pre>

<p>and so on...</p>

<p>And then I want to replace a large document of entries with the keys. However, occasionally a new value will come up, i.e. {apple:green apple}</p>

<p>Is there a method where I can replace all values with the corresponding key, but then also replace 'close' values like the one given if they appear?</p>

<p>Example document:</p>

<pre><code>var1
_____
aple
apple
orange
Apple
Red apple
gren Apple
blood Orange
orang

var1_replaced
______________
apple
apple
orange
apple
apple
apple
orange
orange
</code></pre>
","nlp"
"28269","Sentiment Analysis","2018-02-24 20:00:00","","1","111","<machine-learning><python><nlp>","<p>I am currently doing a project in python. This project is Aspect based sentiment analysis. I don't know how to prepare training file for that </p>
","nlp"
"28237","Type of model for space-characters recovery?","2018-02-23 16:05:18","","1","39","<deep-learning><nlp><keras>","<p>I have the string ""Ihaveadog"" and i need to recover space-characters in the string.I tried to use seq2seq model but failed.In my case i've got data like ""Ihaveadog"" -> ""010000101000"" encoded by one-hot method.What type of DL model is good for the task or the best way is the using classic ML methods?</p>

<p>Thanks for your time.</p>
","nlp"
"28157","Giving higher priority to certain inputs in SKLearn Random Forest","2018-02-22 04:44:07","","0","1744","<machine-learning><python><scikit-learn><nlp><random-forest>","<p>Here is the sample data I have:</p>

<p>Tag 1(Val: X), Tag 2(Val: Y), Tag 3(Val: Z), Label (Val: P)</p>

<p>Tag 1(Val: A), Tag 2(Val: B), Tag 3(Val: C), Label (Val: Q)</p>

<p>Tag 1(Val: D), Tag 2(Val: E), Tag 3(Val: F), Label (Val: R)</p>

<p>Tag 1(Val: G), Tag 2(Val: H), Tag 3(Val: I), Label (Val: S)</p>

<p>I am using Scikit Learn Random Forest Classifier.
I want the Classifier to give higher priority to certain inputs.
For Example: 100x priority to Tag 1: 10x priority to Tag 2: 1x priority to Tag 3.</p>

<p>Does Scikit-Learn have any parameters where I can specify this?</p>

<p>[Since the purpose of the Random Forest is to learn the priority of the inputs, is my question nonsensical?]</p>

<p>Appreciate your insight and support.</p>
","nlp"
"28156","Using HashingVectorizer for text vectorization","2018-02-22 02:08:22","","2","3266","<python><scikit-learn><nlp><random-forest><dataframe>","<p>Here is the sample data I have:</p>

<p>Tag 1(Val: X), Tag 2(Val: Y), Tag 3(Val: Z), Label (Val: P)</p>

<p>Tag 1(Val: A), Tag 2(Val: B), Tag 3(Val: C), Label (Val: Q)</p>

<p>Tag 1(Val: D), Tag 2(Val: E), Tag 3(Val: F), Label (Val: R)</p>

<p>Tag 1(Val: G), Tag 2(Val: H), Tag 3(Val: I), Label (Val: S)</p>

<p>I started by putting the Tags into a dataframe df and the Label into a separate dataframe df_label.
Then used a HashingVectorizer to prepare the text for processing by ML models (I want to hash the strings into a unique numerical value so that the ML Models can train on it)</p>

<pre><code>vectorizer = HashingVectorizer()

X_train = vectorizer.transform(df)

y_train = vectorizer.transform(df_label)

clf = RandomForestClassifier(n_jobs=2, random_state=0)

clf.fit(X_train, y_train)
</code></pre>

<p>When I execute this, I get: <code>ValueError: Unknown label type: 'unknown' on y_train.</code></p>

<p>I am new to both Python and ML and I am not sure whether the problem is with my basic logic or whether it is a trivial implementation issue. Appreciate your insight and support.</p>
","nlp"
"28091","How to compute document similarities in case of source codes?","2018-02-21 09:09:52","","6","192","<machine-learning><nlp><text-mining><similar-documents>","<p>I try to detect the probability of common authorship (person, company) of different kind of source code texts (webpages, program codes). My first idea is to apply the usual NLP tools like any token based document representation (TF-IDF or embeddings) and computing similarity on these but somehow I find this approach a bit clumsy. I want to detect ""handprints"" (characteristic comment and abbreviation style, folder structure, used 3rd party tools, order of elements in the code etc.) that seem out of the scope of this approach.</p>

<p>Moreover, I cannot find place for any proper machine learning here. Clearly, finding weights for the any future quantitative features would be nice but this similarity task is not classification/regression, so how to define the target? Clustering seems to be a better tool but we cannot define as many categories as potential authors.</p>

<p>Could you kindly suggest any more reliable method? Does any literature exist for this topic?</p>
","nlp"
"27941","How to identify sentiment of a given word from a sentence","2018-02-18 02:52:08","","3","2472","<machine-learning><deep-learning><nlp><sentiment-analysis>","<p>Use case is like this:</p>

<p>Suppose I have a sentence(review data): <code>The staffs were very kind and helpful. The room is ok for its price. There did not seem to be a heater in the room. So, a bit for our January trip.</code></p>

<p>Now if I want to know the sentiment of a word for example <code>room</code> then how should I proceed.</p>

<p>I have designed sentiment analysis for a given sentence using <code>bag of words</code> model, but determining the sentiment for the given <code>word</code> from the <code>sentence</code> (review data) is kind of new to me. What approach should I take here ?</p>

<p><em>Providing some approach or any link is enough.</em></p>
","nlp"
"27939","How do we pass data to a RNN?","2018-02-18 01:26:48","","4","224","<deep-learning><rnn><language-model><nlp>","<p>Let's say we have A1, A2, ... , Am different articles in the corpus and each of them has W1, W2, ....., Ww words. We are training a language model on them. Do we:</p>

<p><strong>Scheme 1</strong></p>

<ol>
<li>Take the first batch of data as first S (Number of time steps) (S1, S2, .., Ss)words from each article (for the sake of simplicity let us assume batch size = m)</li>
<li>Set the initial hidden state H0 = $[0,0,..,0]$</li>
<li>Calculate loss and gradient on this batch and update the parameters</li>
<li>Move a word forward and get next S words from each article (hence S2, S3, ... , Ss are the same words as in the previous batch)</li>
<li>Use H1 calculated from previous step and use it as H0 in this iteration</li>
<li>Do this to the end</li>
</ol>

<p>*In this scheme we would have to use zero padding on the last batch (at the end of articles)</p>

<p><strong>Scheme 2</strong>
Same as Scheme 1 but in step 5 we reinitialize H0 to a vector of Zeroes</p>

<p><strong>Scheme 3</strong>
Same as scheme 1 but in step 4 we move s words forward to next non overlapping s words in each article and initialize H0 to Hs from last iteration</p>

<p><strong>Scheme 4, 5, 6</strong>
Same as Scheme 1, 2, 3 but instead of taking s consecutive words we take first sentence from each article and zero pad them to the length S</p>

<p>What is the right way to go through the data in feeding it to a RNN, Please give reasons as well. I think it should be Scheme 1 but it could be really slow</p>
","nlp"
"27872","What approach is to be taken to convert a code snippet to simple English?","2018-02-16 01:44:31","","1","34","<machine-learning><neural-network><nlp>","<p>Suppose <code>a</code> is a variable and I have a DSL (Domain Specific language) or a code snippet that says</p>

<p><code>a.size() &gt; 0</code></p>

<p>My task is to convert it to simple English and print <code>Number of a's is greater than zero</code></p>

<p><strong>More examples:</strong></p>

<p><strong>Input</strong>:  <code>a != null</code> , <strong>Output</strong> <code>a is not null</code></p>

<p><strong>Input</strong>:  <code>a != """"</code> , <strong>Output</strong> <code>a is not empty</code></p>

<p><strong>Input</strong>: <code>a.parent.children.size()&gt;3</code>, <strong>Output</strong>: <code>Parent of 'a' has more than 3 children</code> </p>

<p><strong>Input</strong>: <code>a.children().forAll(e|e.value&lt;4)</code>, <strong>Output</strong>: <code>All the children of 'a' have a value less than 4</code></p>

<p>I want to know what the right approach to this kind of problem would be.</p>

<p>I basically want to translate the code snippet to simple English, similar to how humans can have a fair/high level understanding of what the code snippet is trying to do by just reading it.</p>

<p>I've considered the below approaches:</p>

<ol>
<li><p>One approach that I've considered to to write something like a compiler or a parser.</p></li>
<li><p>Another approach that I'm considering is to use machine translation, similar to how RNNs are used to translate one language to another. </p></li>
</ol>

<p>How would one approach this problem? </p>

<p>I asked the same question on SO <a href=""https://stackoverflow.com/questions/48814273/what-approach-is-to-be-taken-to-convert-a-dsl-to-simple-english"">here</a> but I didn't get any response yet, so decided to ask here</p>
","nlp"
"27840","Training an AI to play Starcraft 2 with superhuman level of performance?","2018-02-15 12:46:47","","2","385","<machine-learning><computer-vision><audio-recognition><nlp><ai>","<p>I'm interested in working on challenging AI problems, and after reading this article (<a href=""https://deepmind.com/blog/deepmind-and-blizzard-open-starcraft-ii-ai-research-environment/"" rel=""nofollow noreferrer"">https://deepmind.com/blog/deepmind-and-blizzard-open-starcraft-ii-ai-research-environment/</a>) by DeepMind and Blizzard, I think that developing a robust AI capable of learning to play Starcraft 2 with superhuman level of performance (without prior knowledge or human hard-coded heuristics) would imply a huge breakthrough in AI research.</p>

<p>Sure I know this is an extremely challenging problem, and by no means I pretend to be the one solving it, but I think it's a challenge worth taking on nonetheless because the complexity of the decision making required is much closer to the real world and so this forces you to come up with much more robust, generalizable AI algorithms that could potentially be applied to other domains.</p>

<p>For instance, an AI that plays Starcraft 2 would have to be able to watch the screen, identify objects, positions, identify units moving and their trajectories, update its current knowledge of the world, make predictions, make decisions, have short term and long term goals, listen to sounds (because the game includes sounds), understand natural language (to read and understand text descriptions appearing in the screen as well), it should probably be endowed also with some sort of attention mechanism to be able to pay attention to certain regions of interest of the screen, etc. So it becomes obvious that at least one would need to know about Computer Vision, Object Recognition, Knowledge Bases, Short Term / Long Term Planning, Audio Recognition, Natural Language Processing, Visual Attention Models, etc. And obviously it would not be enough to just study each area independently, it would also be necessary to come up with ways to integrate everything into a single system.</p>

<p>So, does anybody know good resources with content relevant to this problem? I would appreciate any suggestions of papers, books, blogs, whatever useful resource out there (ideally state-of-the-art) which would be helpful for somebody interested in this problem.</p>

<p>Thanks in advance.</p>
","nlp"
"27730","How to cluster sentences based on company names from a post(s) containing several company names using similarity metric.","2018-02-12 14:26:15","","2","356","<python><nlp><text-mining>","<p>My corpus contains several posts having text for several companies i.e. each post contains information about several companies. </p>

<p>I want to cluster the information based on few company names that I can specify. Clustering should be based on some similarity matrix such as euclidean or cosine similarity.</p>

<p>Which algorithm to use based on company name that I can specify and which similarity method to use?</p>
","nlp"
"27726","When to use cosine simlarity over Euclidean similarity","2018-02-12 13:31:46","","34","40830","<machine-learning><nlp><clustering><similarity>","<p>In NLP, people tend to use cosine similarity to measure document/text distances. I want to hear what do people think of the following two scenarios, which to pick, cosine similarity or Euclidean?</p>
<p>Overview of the task set: The task is to compute context similarities of multi-word expressions. For example, suppose we were given an MWE of <code>put up</code>, context refers to the words on the left side of <code>put up</code> and as well as the words on the right side of it in one text. Mathematically speaking, similarity in this task is about calculating</p>
<pre><code>sim(context_of_using_&quot;put_up&quot;, context_of_using_&quot;in_short&quot;)
</code></pre>
<p>Note that context is the feature that built on top of word embeddings, let's assume each word has an embedding dimension of <code>200</code>:</p>
<p>Two scenarios of representing <code>context_of_an_expression</code>.</p>
<ol>
<li><p>concatenate the left and right context words, producing an embedding vector of dimension <code>200*4=800</code> if picking two words on each side. In other words, a feature vector of [lc1, lc2, rc1, rc2] is build for context, where <code>lc=left_context</code> and <code>rc=right_context</code>.</p>
</li>
<li><p>get the mean of the sum of left and right context words, producing a vector of <code>200</code> dimensions. In other words, a feature vector of [mean(lc1+lc2+rc1+rc2)] is built for context.</p>
</li>
</ol>
<p>[Edited] For both scenarios, I think Euclidean distance is a better fit. Cosine similarity is known for handling scale/length effects because of normalization. But I don't think there's much to be normalized.</p>
","nlp"
"27687","How much text is enough to train a good embedding model?","2018-02-10 22:58:37","","7","1494","<nlp><training><word-embeddings><performance><embeddings>","<p>I need to train a word2vec embedding model on Wikipedia articles using Gensim.</p>

<p>Eventually, I will use the entire Wikipedia for that<br />
but for the moment, I'm doing some experimentation/optimization to improve the model quality and I was wondering how many articles would be enough to train a meaningful/good model? How many examples needed for each unique word in vocabulary?</p>
","nlp"
"27663","Performance Metric for topic extraction when there is no ground truth","2018-02-10 06:00:41","","1","117","<nlp><text-mining><model-evaluations><performance><topic-model>","<p>I am extracting topics from text using a predefined ontology containing 2690 concepts, wordnet(to expand concept terms with their synsets, and other morphological forms of the same word) and lucene to index ontology concepts. However, as it is very difficult to find the ground truth about the topics in the documents, it is becoming very difficult to evaluate the model. As manually tagging the correct labels will take a huge amount of time.</p>

<p>Are there any standard ways by which people evaluate models of topic extraction/ entity extraction, when ground truth about the topics is not clear?</p>
","nlp"
"27654","Any research on segmentation of non-text contents out of (mostly) text-documents?","2018-02-09 13:32:34","","1","24","<machine-learning><nlp><text-mining>","<p>Documents, especially technical ones often contain non-text content in blocks (code snippets, os commands etc). Is there any efficient way to identify the starting and ending lines of those blocks? </p>
","nlp"
"27392","So what's the catch with LSTM?","2018-02-02 15:45:12","","13","9309","<neural-network><nlp><lstm><rnn>","<p>I am expanding my knowledge of the Keras package and I have been tooling with some of the available models. I have an NLP binary classification problem that I'm trying to solve and have been applying different models. </p>

<p>After working with some results and reading more and more about LSTM, it seems like this approach is far superior to anything else I've tried (across multiple datasets). I keep thinking to myself, ""why/when would you <em>not</em> use LSTM?"". The use of the additional gates, inherent to LSTM, makes perfect sense to me after having some models that suffer from vanishing gradients. </p>

<p>So what's the catch with LSTM? Where do they not do so well? I know there is no such thing as a ""one size fits all"" algorithm, so there must be a downside to LSTM.</p>
","nlp"
"27348","Sub topics with Latent Dirichlet Allocation","2018-02-01 16:03:43","","3","1909","<python><nlp><lda><gensim>","<p>I'm training an LDA model with gensim's LdaMulticore. The topics look great, but knowing the domain I know there exists topics within topics but I'm not quite sure the best way to model this.</p>

<p>I've come across <a href=""https://github.com/joewandy/hlda/blob/master/notebooks/bbc_test.ipynb"" rel=""nofollow noreferrer"">this</a> implementation of Hierarchical LDA, but I'm having a hard time implementing it (no community support). And I don't think gensim's hdpModel is what I want, given <a href=""https://datascience.stackexchange.com/questions/128/latent-dirichlet-allocation-vs-hierarchical-dirichlet-process"">this</a> discussion.</p>

<p>I'm currently doing this:</p>

<p>1) train LDA model on all records to get general topics</p>

<p>2) use this LDA model to assign each record a primary topic</p>

<p>3) for each topic, retrieve only the records that were assigned that topic</p>

<p>4) train a new LDA model only on the filtered records (for example, where topic ID == 3) to generate sub topics from filtered set</p>

<p>5) Assign each record a general topic ID and a sub topic ID</p>

<p>Is this a valid way to get topics and sub topics? Should I be weary of this approach?</p>

<p>Thanks for any insight.</p>
","nlp"
"27100","How to find categorical features from a vector representation of text?","2018-01-27 10:42:18","","-1","498","<machine-learning><nlp><feature-extraction><word2vec>","<p><strong>The context of the question:</strong>
I have a pandas dataframe where one column has text values and others have categorical values. I trained a <code>word2vec</code> model with tensorflow with some sample data. And I convert my text column into the vector representation. But I want to feed these data to <a href=""https://github.com/catboost/catboost"" rel=""nofollow noreferrer"">Catboost</a> for regression task. But I can't feed those data to catboost because of catboost only accept the <code>Categorical</code> variable. </p>

<blockquote>
  <p>I found a tutorial on Catboost <a href=""https://github.com/catboost/catboost/blob/master/catboost/tutorials/quora_catboost_w2v.ipynb"" rel=""nofollow noreferrer"">Github page</a> for this. But this
  example is for the classification task. It finds <code>cosine</code> and other
  types of relationship between two vector representation of text. But
  in my case, I have only one text field. So how can calculate <code>cosine</code>
  or other types relationship?</p>
</blockquote>

<p>So my question is how to extract categorical features from the vector representation of text data?</p>
","nlp"
"27058","Finding repeating string patterns in thousands of files","2018-01-26 02:36:43","","1","574","<classification><nlp><text-mining>","<p>I have files like the following: <a href=""https://pastebin.com/5mkXY1aU"" rel=""nofollow noreferrer"">https://pastebin.com/5mkXY1aU</a></p>

<p>These are created by filling predefined forms so there are thousands of them that would match a pattern.</p>

<p>I will try to give more generic example for the sake of simplicity.</p>

<p>Let's say I have thousands of documents created using different patterns like `</p>

<pre><code>- ""sjkghkjfs &lt;data&gt; skjfs &lt;data&gt; kjskdfjsfkjs &lt;data&gt; sahkj"";
- ""tretyer erytewr fskjdf &lt;data&gt; trjk"";
- ""sdhfjsdhj &lt;data&gt; &lt;data&gt; &lt;data&gt; dsjadh"";
- ""&lt;data&gt; djfhsdk"";` and so on.
</code></pre>

<p><code>&lt;data&gt;</code> could be just any string.</p>

<p>I don't know what these patterns are beforehand. I want to find them out.</p>

<p>I can easily come up with some bruteforce solution but it would obviously not work on such amount of data.</p>

<p>It sounds to me like a problem that might have already been solved.</p>

<p><strong><em>I wondered if there is some readily available software tool I could utilize directly to solve this or a library atleast ?</em></strong></p>

<p><strong><em>In case there isn't one how should I proceed to solve it in the most painless way ?</em></strong></p>

<p><em>It is a one time task to find these patterns out, so I am not into the idea of spending days on implementing a solution from the scratch.</em></p>
","nlp"
"27048","How to add more features in addition to a 100D word vector","2018-01-25 17:30:33","","5","2573","<python><nlp><keras><word-embeddings><sentiment-analysis>","<p>I am working on sentiment analysis using tweets text. I am able to build a word vector using Keras <code>text_to_sequence()</code> method with pretrained GloVe embeddings in Python. </p>

<p>The question is how to add extra features beside the created GloVe embeddings and shape it in right way to feed to the embedding layer and the algorithm in general? For example, assuming the embedding's length is 100 and the extra features are 3, will appending the 3 features to 100 vector resulting in 103 vector length is the right approach? If so how would you implement it.</p>

<p>Where:</p>

<p><code>corpusAsList</code> is the corpus as text in a list.</p>

<p><code>maxlen</code> is the maximum number of columns to pad the word sequence with, so that all vectors have the same length.</p>

<p><code>maxVocabulary</code> is the maximum amount of words to be used in the sequence and ignore the rest of the words.</p>

<p>And <code>tokenizer</code> is used to represent a word as a digit in the vocabulary.</p>

<p>For instance, if we called <code>texts_to_sequences()</code> like this:</p>

<pre><code>   # Create tokenized corpuss
    listTokenized = tokenizer.texts_to_sequences(corpusAsList)

    # Sequence dataset
    sequenceMatrixPrediction = sequence.pad_sequences(listTokenized, maxlen=maxlen)
</code></pre>

<p>And then trained a Bidirectional LSTM model like this:</p>

<pre><code>def getLSTMModel(maxlen, maxVocabulary, embedSizeLSTM=100, weights=None):

    inp = Input(shape=(maxlen,))
    x = Embedding(maxVocabulary, embedSizeLSTM,
                 weights=[weights], trainable=False)(inp)
    x = Bidirectional(LSTM(80, return_sequences=True))(x)
    x = GlobalMaxPool1D()(x)
    x = Dropout(0.25)(x)
    x = Dense(70, activation=""relu"")(x)
    x = Dropout(0.25)(x)
    x = Dense(1, activation=""sigmoid"")(x)
    model = Model(inputs=inp, outputs=x)
    model.compile(loss='binary_crossentropy',
                  optimizer='adam',
                  metrics=['accuracy'])


return model
</code></pre>

<p>How could we add three extra features to this model?</p>
","nlp"
"27042","GloVe vector representation homomorphism question","2018-01-25 16:10:12","27043","3","820","<word-embeddings><nlp>","<p>In the paper <a href=""https://nlp.stanford.edu/pubs/glove.pdf"" rel=""nofollow noreferrer""><em>GloVe: Global Vectors for Word Representation</em></a>, there is this part (bottom of third page) I don't understand:</p>

<p><a href=""https://i.sstatic.net/tQCzJ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/tQCzJ.png"" alt=""enter image description here""></a></p>

<p>I understand what groups and homomorphisms are. What I don't understand is what requiring $ F $ to be a homomorphism between $ (\mathbb{R},+) $ and $ (\mathbb{R}_{&gt;0},\times) $ has to do with making $ F $ symmetrical in $ w $ and $ \tilde{w}_k $.</p>

<p>Am I misunderstanding something? We want $ F $ to be unchanged if we either interchange $ w_i $ and $ \tilde{w}_k $ OR interchange $ w_j $ and $ \tilde{w}_k $, right? Is this the only way to achieve the symmetry between  $ w $ and $ \tilde{w}_k $?</p>
","nlp"
"27030","Why do good word embeddings come out of maximizing cosine similarity?","2018-01-25 10:41:25","","4","902","<neural-network><nlp><word2vec><word-embeddings>","<p>I have an understanding of the technical details of word2vec. </p>

<p>What I don't understand is why semantically similar words should have high cosine similarity. </p>

<p>From what I know, goodness of a particular embedding is seen in shallow tasks such as word analogy. I am unable to grasp the relationship between maximizing cosine similarity and good word embeddings</p>
","nlp"
"27009","Understanding of naive bayes: computing the conditional probabilities","2018-01-24 20:01:23","27011","7","269","<machine-learning><naive-bayes-classifier><sentiment-analysis><nlp>","<p>For a task on sentiment analysis, suppose we have some classes represented by $c$ and features $i$. </p>

<p>We can represent the conditional probability of each class as: $$P(c | w_i) = \frac{P(w_i|c) \cdot P(c)}{P(w_i)}$$
where $w_i$ represents each feature and $c$ is the class we have. Then empirically, we can represent $$P(w_i|c) = \frac{n_{ci}}{n_c}$$ 
$$P(w_i) = \frac{n_{i}}{n}$$
Our priors for each classes are then given by: $$P(c) = \frac{n_c}{n}$$ where:</p>

<p>$n$ is the total number of features in all classes.</p>

<p>$n_{ci}$ represents the number of counts of that feature $i$ in class $c$.</p>

<p>$n_c$ is the total number of features for the class, and </p>

<p>$n_i$ is the total number of features for all classes.</p>

<p>Is my understanding of the above correct? So given these $P(c|w_i)$ probabilities for each word, I'm the naive bayes assumption is that the words are independent, so I simply multiply each word in a document for a certain class, i.e. to compute $\prod P(c|w_i), i \in N$ where $N$ is the number of words in the document. Is this correct? </p>

<p>To actually compute the conditional probability numerically, would it suffice to do the following:</p>

<p>$$P(c | w_i) = \frac{P(w_i|c) \cdot P(c)}{P(w_i)} = \frac{n_{ci}}{n_c} \cdot \frac{n_c}{n}\cdot \frac{n}{n_i} = \frac{n_{ci}}{n_i}$$</p>

<p>The last part of the equation looks a bit suspicious to me as it seems way too simple to compute for a rather complex probability. </p>
","nlp"
"26967","What are some options for an offline chatbot on Android?","2018-01-23 17:00:20","","3","6715","<machine-learning><nlp>","<p>I have already created an online chatbot for my Android app using Dialogflow (formerly api.ai). It allows users to add a new item to their inventory. I now need to create an offline version of this chatbot. However, Dialogflow does not allow you to download the agent to work offline.</p>

<p>I have looked into several options, including RiveScript and ALICE (Pandorabots). These seem to be a bit too low-level for my purposes and require quite a bit of customization. I was wondering if there are any options similar to Dialogflow that can be used offline.</p>
","nlp"
"26947","Why do we need to add START <s> + END </s> symbols when using Recurrent Neural Nets for Sequence-to-Sequence Models?","2018-01-23 09:39:17","","15","12476","<deep-learning><rnn><sequence-to-sequence><nlp>","<p>In the Sequence-to-Sequence models, we often see that the START (e.g. <code>&lt;s&gt;</code>) and END (e.g. <code>&lt;/s&gt;</code>) symbols are added to the inputs and outputs before training the model and before inference/decoding unseen data.</p>

<p>E.g. <a href=""http://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"" rel=""noreferrer"">http://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html</a> </p>

<pre><code>SOS_token = 0
EOS_token = 1


class Lang:
    def __init__(self, name):
        self.name = name
        self.word2index = {}
        self.word2count = {}
        self.index2word = {0: ""SOS"", 1: ""EOS""}
        self.n_words = 2  # Count SOS and EOS

    def addSentence(self, sentence):
        for word in sentence.split(' '):
            self.addWord(word)

    def addWord(self, word):
        if word not in self.word2index:
            self.word2index[word] = self.n_words
            self.word2count[word] = 1
            self.index2word[self.n_words] = word
            self.n_words += 1
        else:
            self.word2count[word] += 1
</code></pre>

<ul>
<li><p><strong>Is there a technical definition or academic explanation of why that is necessary?</strong></p></li>
<li><p><strong>Or does that need to add the END symbol only applies to Natural Language Processing task where the sentence generation needs to end?</strong></p></li>
<li><p><strong>But what does the START symbol do? Other than to take the initial state where the trained network will start inferrring.</strong> </p></li>
</ul>
","nlp"
"26943","How to initialize word-embeddings for Out of Vocabulary Word?","2018-01-23 06:34:33","","3","10247","<machine-learning><deep-learning><nlp><word-embeddings>","<p>I am trying to use CoNLL-2003 NER (English) Dataset and I am trying to utilize pretrained embeddings for it. I am using SENNA pretrained embeddings. Now I have around 20k words in my vocabulary and out of this I have embedding available for only 9.5k words. <br></p>

<p>My current approach is to initialize an array of <code>20k X embedding_size</code> with zeros and initialize the 9.5k words whose embeddings is known to me and make all the embeddings learn-able.<br></p>

<p>My question is what is the best way to do this? Any reference to such research will be very helpful ? </p>

<p>I have also asked a similar question later on Stackoverflow, <a href=""https://stackoverflow.com/q/48395570/3316461"">here</a> is the link.</p>
","nlp"
"26937","Reducing text input size into word2vec without affecting performance too badly?","2018-01-22 23:16:58","26939","0","453","<nlp><word2vec><word-embeddings><automatic-summarization>","<p>So I am implementing Word2Vec for the first time, and I have a set of training data that I would like to train a word2vec model on. Predictably, the problem is the dataset is rather large, and I have more limited computational power than I would like. This is a very common problem of course, but are there any ways to minimize the input text without too horribly affecting the performance? </p>

<p>For example, if I had the example sentences: </p>

<blockquote>
  <p>Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you’d expect to be involved in anything strange or mysterious, because they just didn’t hold with such nonsense. Mr. Dursley was the director of a firm called runnings, which made drills. He was a big, beefy man with hardly any neck, although he did have a very large mustache.</p>
</blockquote>

<p>I could do some thing like take out the stop words... but as I googled, that's a bad idea according to <a href=""https://www.quora.com/Is-it-compulsory-to-remove-stop-words-with-word2vec"" rel=""nofollow noreferrer"">one link</a> and a <a href=""https://stackoverflow.com/a/34737150/4573703"">good idea</a> according to another... so which is it? I also looked into <a href=""https://research.googleblog.com/2016/08/text-summarization-with-tensorflow.html"" rel=""nofollow noreferrer"">text summarization</a> but that seems <a href=""https://hackernoon.com/how-to-run-text-summarization-with-tensorflow-d4472587602d"" rel=""nofollow noreferrer"">harder than implementing word2vec</a>. Another idea I naively had was to randomly take out X% of sentences from the text. But that would obviously be a performance hit in the model, and I'm not sure how big. </p>

<p>So, are there any general methods for trying to do this, and is stop word removal a way to do it? </p>
","nlp"
"26936","How to improve the accuracy of Random Forest for Text Categorization","2018-01-22 22:46:54","","0","1302","<machine-learning><classification><nlp><text-mining><random-forest>","<p>I am working on a text Categorization problem, the objective is to classify related companies into their corresponding categories. This is a single category classification problem and not multi-class classification. Below is the details of the method I used.</p>

<p>Data Preparation -</p>

<ul>
<li>Broke the documents in the list of tokens/ words. </li>
<li>Removed numbers, stop words, punctuations.</li>
<li>Performed stemming. </li>
<li>Convert corpus into DTM and,</li>
<li>specify the word length in-between(3, 10)</li>
<li>Transformed the documents into TF-IDF vectors</li>
<li>convert it into a matrix and further into a data frame</li>
</ul>

<p>These will be used a feature list for the
classification algorithm. Used Random forest (""ranger"" by using mlr). </p>

<p>I have 853 categorized documents with 20 categories, and I used 75:25 ration for training and test dataset. I used mlr package in R. The classification accuracy I have managed to get is 45% and I need it to be at least 80%.</p>

<p>I also applied xgboost but got only 38% accuracy.</p>

<p>Data Looked like the below-mentioned image: - 
<a href=""https://i.sstatic.net/UulHh.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/UulHh.png"" alt=""Click to see the image of data""></a></p>

<p>Here is an example code I used so far: - </p>

<pre><code>terms &lt;-DocumentTermMatrix(clean_corp,
                                      control=list(wordLengths=c(3, 9), 
                                                   weighting = weightTfIdf))

m &lt;- as.matrix(terms)
vertical_market &lt;- process$label
termsDf &lt;- as.data.frame(m)
termsDf$label &lt;- as.factor(label)
names(termsDf) = lapply(names(termsDf), as.name)

words &lt;- colnames(termsDf)

# taking words with length more than 4
data &lt;- termsDf[, str_length(words) &lt; 16 &amp; str_length(words) &gt; 3]
colnames(data) &lt;- c(paste0(""w"", 1:(ncol(data)-1)), ""label"")

    task &lt;- makeClassifTask(data = data, target = ""label"")
    lrn &lt;- makeLearner(""classif.ranger"", mtry=100,respect.unordered.factors = FALSE, 
                       predict.type = ""response"")
    n &lt;- nrow(data)
    set.seed(123)
    train.set &lt;- sample(n, size = 2/3*n)
    test.set &lt;- setdiff(1:n, train.set)
    model &lt;- mlr::train(lrn, task, subset = train.set)
    pred &lt;- predict(model, task = task, subset = test.set)
    performance(pred, measures = list(mmce,acc))
</code></pre>

<p>Any help on how I can improve the accuracy would be greatly appreciated. Thanks a lot. Please let me know if you need any more details.</p>
","nlp"
"26854","What are useful evaluation metrics used in machine learning","2018-01-20 10:25:00","26855","6","7666","<machine-learning><neural-network><deep-learning><nlp><text-mining>","<p>I am using CNN in order to predict codes after analyzing text. As an example, I will write ""I am crazy"" .. the model will predict some code "" X321"".
 All this based on CNN. </p>

<p>I want to evaluate my model. I used Fscore (recall and precision). Can you advice me more metrics?</p>
","nlp"
"26826","How to detect if one tweet is agreeing with another","2018-01-19 13:06:15","","4","150","<nlp><sentiment-analysis><twitter>","<p>I want to detect tweet text agreement. Suppose someone posts some subjective opinion in twitter. Other users will post reply either agreeing or opposing the original tweet. I want to estimate the amount of agreement. Is there any algorithm/library in any language to do that or any labeled dataset? </p>
","nlp"
"26789","Should I use regex or machine learning?","2018-01-18 14:22:34","26985","3","10441","<machine-learning><nlp><automatic-summarization>","<p>I am thinking of two use cases:</p>

<ol>
<li>You submit a resume in PDF format to a web site and it extracts your contact information, job titles, etc.</li>
<li>You receive an email from a friend that says, ""let's have lunch next Tuesday"" and your email program detects it and asks if you want to save a new calendar entry for ""lunch on Tuesday"".</li>
</ol>

<p>I can see how this can be done with both ML and with (very sophisticated) regex. What approaches are typically used for these scenarios? I'm assuming that the same approach can be applied to both of the scenarios above?</p>
","nlp"
"26733","Getting unexpected result while using CountVectorizer()","2018-01-17 08:30:33","26741","2","632","<python><nlp><scikit-learn><text-mining>","<p>I am trying to use <code>CountVectorizer()</code> in a loop, But I am getting an unexpected result. On the other hand, if I use it outside the loop then it works fine. I believe there is some small problem with the logic.</p>

<pre><code>from sklearn.feature_extraction.text import CountVectorizer

keys_1 = ['funny', 'amusing', 'humorous', 'hilarious', 'jolly']
keys_2 = ['horror', 'fear', 'shock', 'panic', 'scream']
keys_3 = ['romantic', 'intimate', 'passionate', 'love', 'fond']

text = ('funny amusing fear passionate')

for i in range(3):
    keys = 'keys_' + str(i+1)
    cv = CountVectorizer(vocabulary = keys) 
    data = cv.fit_transform([text]).toarray()
    print(data)

cv1 = CountVectorizer(vocabulary = keys_1) 
data = cv1.fit_transform([text]).toarray()
print(data)

cv2 = CountVectorizer(vocabulary = keys_2) 
data = cv2.fit_transform([text]).toarray()
print(data)

cv3 = CountVectorizer(vocabulary = keys_3) 
data = cv3.fit_transform([text]).toarray()
print(data)
</code></pre>

<p>Output</p>

<blockquote>
  <p>[[0 0 0 0 0 0]]</p>
  
  <p>[[0 0 0 0 0 0]]</p>
  
  <p>[[0 0 0 0 0 0]]</p>
  
  <p>[[1 1 0 0 0]]</p>
  
  <p>[[0 1 0 0 0]]</p>
  
  <p>[[0 0 1 0 0]]</p>
</blockquote>
","nlp"
"26721","What is a lower bound on the vocabulary size for generating word/sentence embedding using word2vec or skip thought vectors?","2018-01-16 22:10:04","","1","1948","<nlp><word2vec><word-embeddings><stanford-nlp>","<p>I am working on a NLP related task. I have about 150 documents, each few pages long (5/6 pages long on average). After removing stopwords and other unnecessary symbols and digits, I have about 104,000 unique words. The task at hand probably require some kind of word embedding (such as word2vec) as simple bag-of-words type approach aren't working properly. However, I am concerned about the size of the data I have. I have looked at pre-trained word embedding (GloVec), however, due to the narrow focus of the domain (manufacturing) of our texts, I am hesitating to use these pre-trained vectors. That leaves me with training our own. However, the size of our data set concerns me. Hence I am just throwing this question out there: What should be the lower bound on the size of the vocabulary that we need in order to train a word embedding model (word2vec) that will be reasonable. </p>

<p>Any response would be greatly appreciated.</p>

<p>Thanks</p>
","nlp"
"26693","How to use different classes of words in CountVectorizer()","2018-01-16 12:45:46","26704","3","3223","<python><nlp><scikit-learn><text-mining><probability>","<p>Suppose I have a piece of writing and I want to assign probabilities to different genres (classes) based on its contents. For example</p>

<blockquote>
  <p>Text #1 : Comedy 10%, Horror 50%, Romance 1%</p>
  
  <p>Text #2 : Comedy 40%, Horror 3%, Romance 30%</p>
</blockquote>

<p>We have given keywords in each class through which we make a comparison. Below is the code that explains this scenario better</p>

<pre><code>from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# Comedy
keywords_1 = ['funny', 'amusing', 'humorous', 'hilarious', 'jolly']

# Horror
keywords_2 = ['horror', 'fear', 'shock', 'panic', 'scream']

# Romance
keywords_3 = ['romantic', 'intimate', 'passionate', 'love', 'fond']

text = ('funny hilarious fear passionate')

cv1 = CountVectorizer(vocabulary = keywords_1) 
data = cv1.fit_transform([text]).toarray()
vec1 = np.array(data)
vec2 = np.array([[1, 1, 1, 1, 1]])
print(cosine_similarity(vec1, vec2))

cv2 = CountVectorizer(vocabulary = keywords_2) 
data = cv2.fit_transform([text]).toarray()
vec1 = np.array(data)
vec2 = np.array([[1, 1, 1, 1, 1]])
print(cosine_similarity(vec1, vec2))

cv3 = CountVectorizer(vocabulary = keywords_3) 
data = cv3.fit_transform([text]).toarray()
vec1 = np.array(data)
vec2 = np.array([[1, 1, 1, 1, 1]])
print(cosine_similarity(vec1, vec2))
</code></pre>

<p>The problem with this approach is that <code>vocabulary</code> in <code>CountVectorizer()</code> doesn't consider different word classes (Nouns, Verbs, Adjectives, Adverbs, plurals, etc.) of a word in a text. For example, let's say we have keywords list as below</p>

<blockquote>
  <p>keywords_1 =  [(...),
  ('amusement', 'amusements', 'amuse', 'amuses', 'amused', 'amusing'),
  (...),
  ('hilarious', 'hilariously')
  (...)]</p>
</blockquote>

<p>and we want to compute similarity as follows</p>

<pre><code>cv1 = CountVectorizer(vocabulary = keywords_1) 
data = cv1.fit_transform([text]).toarray()
vec1 = np.array(data) # [[f1, f2, f3, f4, f5]]) # fi is the count of number of keywords matched in a sublist
vec2 = np.array([[n1, n2, n3, n4, n5]]) # ni is the size of sublist
print(cosine_similarity(vec1, vec2))
</code></pre>

<p>How can we modify the above code to capture this scenario. Any advice is appreciated.</p>
","nlp"
"26675","Text understanding and mapping","2018-01-16 04:14:14","","0","171","<nlp><word2vec><word-embeddings>","<p>I am looking at a problem where I have two documents, both are textual documents. The first one is a long (few pages long) textual document, while the second one contains about 10 short texts, each being a one liner. The problem is to automatically understand the 1st document and ""map"" key ideas from here to one or more short texts in the second document. I am not being able to get a good grip on the problem as to how to proceed. I tried the following approach: </p>

<p>first collected as much similar documents as possible which are relevant; I have obtained 136 such documents, each few pages long. Then build a document level embedding using each sentence as a document (doc2vec). Then for each short sentence in the second document, inferred an embedding based on the document model I built. Then tried to find the most similar sentences for each of these short texts from the 2nd document. However the results are not good. Only about 30% match is what I am getting and the  corresponding sentences are not much related to the target sentence. </p>

<p>I am wondering if this is the right approach or are there any other approach available which I am not aware of. </p>

<p>Your advice would be greatly appreciated. It may be the case that my explanation above is not adequate. Please let me know and I will try to improve it. </p>

<p>Your input would be greatly appreciated.</p>

<p>Thanks</p>
","nlp"
"26625","Vocabulary List From word2vec and GloVe","2018-01-14 22:06:42","56106","3","16005","<nlp><word2vec><word-embeddings>","<p>Is there a way I can access just the vocabulary list of pre-trained vectors for word2vec and GloVe? I do not need the entire n-dimensional embeddings.</p>
","nlp"
"26592","What other approaches are there for abstractive summarization, other then seq2seq?","2018-01-13 12:50:38","","2","46","<nlp><rnn><automatic-summarization>","<p>I'm researching on abstractive text summarization, and has come across many recent papers. They all seem to be focusing on Sequence to Sequence models based on RNNs. Apart from RNNs, what other approaches are there when it comes to abstractive text summarization? 
Does ontology-based summarization revolve around the same seq2seq model?</p>

<p>Most of the material I have come across are research papers on this subject; what better sources are there to get an understanding of the underlining concepts of abstractive summarization?</p>
","nlp"
"26564","Classify phrases as biomedical or non-biomedical","2018-01-12 13:54:42","","1","64","<classification><nlp><text-mining>","<p>Words like <code>myotonic</code> are biomedical, but words like <code>new</code> appear in regular English texts. I can imagine a few ways to classify words into biomedical and non-biomedical. Has this problem been tackled before? Does anybody know any ready-to-use solutions for this problem?</p>
","nlp"
"26540","Data set for Q & A system training","2018-01-11 22:29:34","","1","23","<machine-learning><deep-learning><nlp><dataset>","<p>What is the standard (or just good) open data set to train a model for question answering? Ideally, not only in English.</p>

<p>Thanks!</p>
","nlp"
"26521","How to estimate probabilities of different classes for a Text","2018-01-11 14:20:06","26530","2","344","<nlp><text-mining><probability>","<p>Suppose I have a piece of writing and I want to assign probabilities to different genres (classes) based on its contents. For example</p>

<blockquote>
  <p>Text #1 : Comedy 10%, Drama 50%, Fiction 20%, Romance 1%, Mythology 5%, Adventure 10%</p>
  
  <p>Text #2 : Comedy 40%, Drama 3%, Fiction 2%, Romance 30%, Mythology 5%, Adventure 10%</p>
</blockquote>

<p>We have given keywords possibly ngrams in each class through which we make a comparison</p>

<blockquote>
  <p>Class 1 Comedy : k11, k12, ..., k1m</p>
  
  <p>Class 2 Drama : k21, k22, ..., k2n</p>
  
  <p>Class 3 Fiction : k31, k32, ..., k3o</p>
  
  <p>Class 4 Romance : k41, k42, ..., k4p</p>
  
  <p>Class 5 Mythology : k51, k52, ..., k5q</p>
  
  <p>Class 6 Adventure: k61, k62, ..., k6r</p>
</blockquote>

<p>What can be the best probabilistic model that we can use for this task</p>
","nlp"
"26520","In practice, is relation extraction or relationship extraction the correct term?","2018-01-11 13:59:57","","1","196","<nlp><stanford-nlp>","<p>I ask from the practitioner point-of-view, and I hope the answer does not come down to nit-picking, but I would like to settle the matter once and for all in the work I do.</p>

<p>One of the components of an Information Extraction (IE) pipeline is a relation(ship) extractor which picks out associations between entity mentions.  <a href=""https://nlp.stanford.edu/software/relationExtractor.html"" rel=""nofollow noreferrer"">Stanford CoreNLP</a> talks about this extractor strictly using the term relation(s) extractor.  Not that it is authoritative, but <a href=""https://en.wikipedia.org/wiki/Relationship_extraction"" rel=""nofollow noreferrer"">Wikipedia</a> talks about relationship extraction.  The former seems more prevalent based upon both code and papers I have come across.  So is there an authoritative answer as to which is more proper, or are there legitimate cases to use both when talking about what we are trying to do?</p>
","nlp"
"26507","How does Api.ai Google dialogueflow classifies ""intent"" and extracts data from slots","2018-01-11 06:24:28","","3","180","<nlp><text-mining><named-entity-recognition>","<p>I am trying to build a very naive version of Api.ai, now Google DailogueFlow. 
I wanted to know two things.</p>

<ol>
<li><p>How DF classifies sentences with entities in it that can be user created and/or things like date, place, time, currency or even random text (which they call 'system.any')</p></li>
<li><p>How data is extracted from slots after user map the slots with the sentence. </p></li>
</ol>

<p>What algorithm/libraries and techniques do they use to classify, extract and identify user queries. </p>
","nlp"
"26479","Text mining for text matching","2018-01-10 13:51:59","26510","2","7150","<machine-learning><python><data-mining><nlp><text-mining>","<p>I m new in text analysis and need your advice to help medical students to write properly and correctly. The students describe sicknesses as they  observe them; however, they must use an ""official sickness description"". I have the data collected from the students and the correct data. Let me explain:</p>

<p>I have a csv table1 containing 300K rows / one column : each row is the description of health condition written by students ( they can be redundant). </p>

<p>I have an other table2 containing almost 200K rows and 3 columns
Column 1: the official name of the condition ( the correct one the student should use)
Column 2: a code ( number) 
Column 3: the translation of ""the official name of the condition"" in another language.</p>

<p>The idea is to start from table1 and match each row with N official names (with their codes and translation). Let's say 5 official names ranked by distance computation.</p>

<p>I am confused:
Should I go for a recommendation engine or a search/rank algorithms?</p>

<p>What are the steps you can identify to accomplish this task?</p>

<p>PS: the final output might be an API where student start to write the description of the sickness and obtain a list of choices/recommendations where they can pick one. 
I hope I was clear ! Thanks </p>
","nlp"
"26435","Sentiment analysis using sources other than the IMDB data","2018-01-09 12:52:22","27608","1","146","<machine-learning><nlp><text-mining><sentiment-analysis>","<p>I'm curious as to whether the training data for a sentiment analysis tool needs to be specifically geared toward the domain it's being using in. For example, the IMDB movie review data makes sense if I want to gauge sentiment on movie reviews, but would it still be okay to use if I want to gauge sentiment on what the general public thinks about a company's stock?</p>

<p>If I want to gauge sentiment on a company's stock, should I create my own training set that uses data from financial news/social media reaction to companies?</p>
","nlp"
"26338","How to extract specific parts of text from a string?","2018-01-05 19:51:00","26339","1","153","<nlp><text-mining>","<p>For example: 
Here is a textual input: </p>

<p>""ALL imagery SINCE 1952
20 MULE T
aerial BOOSTER
&amp; Multi-Purpose Neutralizer.MAY BE â€œAHMFLL |-E SWALL """""""""""" -5 NETWT4LBS1 DZ (65 OZ) 1.84 kg</p>

<p>""
Output desired:
4 LBS
65 OZ
1.84 kg
""</p>
","nlp"
"26253","Datasets in NLP research papers","2018-01-03 23:14:08","","2","182","<data-mining><nlp><dataset><text-mining>","<p>I am looking for a dataset containing a large number of NLP research papers and abstracts. Are there any open access datasets like that?</p>

<p>If so, can you please share the details?</p>
","nlp"
"26245","Classify sentences containing typos into groups","2018-01-03 21:04:04","26249","1","380","<python><r><nlp>","<p>My data is a list of sentences, where each sentence contains between 1 and 4 words. These sentences are typed in manually so some of them contain typos and some additional words such as <code>GmbH</code>, <code>GER</code> etc.</p>

<p>However, I <em>do know</em> the set of valid sentences. As an example we assume this valid set is given by <code>{Hello human, Horse, Hello bird}</code> and the data (where some sentences contain typos and extra words) is given by</p>

<pre><code>Hello human
Horse
Hello human GmbH
Hello human GmbH, GER
Horse GmbH
Horse
Hello humn
Hell humn
Hello human
Hello bird
</code></pre>

<p>I would like to give each sentence above an ID 1, 2 or 3 where 1 is for <code>Hello world</code>, 2 is <code>Horse</code> and 3 is <code>Hello bird</code>. But due to the typos and extra words such as <code>GmbH</code>, <code>GER</code> I cannot make a simple comparison between strings. </p>

<p>Is there a numerical technique within NLP or a related field that I can use to achieve this task?</p>
","nlp"