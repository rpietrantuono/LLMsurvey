Post Link,Title,CreationDate,AcceptedAnswerId,Score,ViewCount,Tags,Body,TagName
"78875978","Error using LlmFactory with ""TheBloke/OpenHermes-2.5-Mistral-7B-GGUF"" Huggingface","2024-08-15 16:11:45","","0","24","<python><pytorch><metal><large-language-model><huggingface>","<p>I tried replicating a simple Python code to create a small LLM model.</p>
<p>I have macOS M1 machine.
I created a separate environment where I installed Pytorch and llama-cpp-python. The code:</p>
<pre><code>from llmflex import LlmFactory

# Load the model from Huggingface
try:
    # Instantiate the model with the correct identifier
    model = LlmFactory(&quot;TheBloke/OpenHermes-2.5-Mistral-7B-GGUF&quot;)

    # Configure parameters directly if the object itself is callable
    #llm = model(temperature=0.7, max_new_tokens=512)

    # Disable Metal and run on CPU
    llm = model(temperature=0.7, max_new_tokens=512, use_metal=False)

    # Generate a response
    response = llm.generate(&quot;Hello, how are you?&quot;)
    print(response)

except AttributeError as e:
    print(f&quot;Attribute error: {e}&quot;)
except AssertionError as e:
    print(f&quot;Assertion error: {e}&quot;)
except Exception as e:
    print(f&quot;An error occurred: {e}&quot;)
</code></pre>
<p>As you can see, I tried with and without Metal, but I received the same error (the last portion of the output):</p>
<pre><code>llm_load_vocab: special tokens definition check successful ( 261/32002 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32002
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 32768
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 32768
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q2_K
llm_load_print_meta: model params     = 7.24 B
llm_load_print_meta: model size       = 2.87 GiB (3.41 BPW) 
llm_load_print_meta: general.name     = teknium_openhermes-2.5-mistral-7b
llm_load_print_meta: BOS token        = 1 '&lt;s&gt;'
llm_load_print_meta: EOS token        = 32000 '&lt;|im_end|&gt;'
llm_load_print_meta: UNK token        = 0 '&lt;unk&gt;'
llm_load_print_meta: PAD token        = 0 '&lt;unk&gt;'
llm_load_print_meta: LF token         = 13 '&lt;0x0A&gt;'
llm_load_tensors: ggml ctx size =    0.11 MiB
llm_load_tensors: mem required  = 2939.69 MiB

llama_new_context_with_model: n_ctx      = 4096
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V     (f16):  256.00 MiB
llama_build_graph: non-view tensors processed: 676/676
ggml_metal_init: allocating
ggml_metal_init: found discrete device: Apple M1
ggml_metal_init: picking device: Apple M1
ggml_metal_init: default.metallib not found, loading from source
ggml_metal_init: GGML_METAL_PATH_RESOURCES = nil
ggml_metal_init: error: could not use bundle path to find ggml-metal.metal, falling     back to trying cwd
ggml_metal_init: loading 'ggml-metal.metal'
ggml_metal_init: error: Error Domain=NSCocoaErrorDomain Code=260 &quot;The file “ggml-    metal.metal” couldn’t be opened because there is no such file.&quot; UserInfo=.   {NSFilePath=ggml-metal.metal, NSUnderlyingError=0x600002eeb2a0 {Error     Domain=NSPOSIXErrorDomain Code=2 &quot;No such file or directory&quot;}}
llama_new_context_with_model: ggml_metal_init() failed
AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 |     NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 =     0 | SSSE3 = 0 | VSX = 0 | 

Assertion error: 
</code></pre>
<p>Obviously, something is wrong, but I cannot pinpoint the error because I am new to this.</p>
<p>I do not want to use CUDA; I want to use the CPU.</p>
<p>Please, help</p>
<p>Here is some additional information: <a href=""https://huggingface.co/TheBloke/OpenHermes-2.5-Mistral-7B-GGUF"" rel=""nofollow noreferrer"">https://huggingface.co/TheBloke/OpenHermes-2.5-Mistral-7B-GGUF</a></p>
","huggingface"
"78871103","Apptainer permission issue","2024-08-14 13:06:53","","0","4","<huggingface><supercomputers><apptainer>","<p>I'm trying to run the docker image of Huggingface inference using apptainer. The issue is that i could mount my local directory and the models can be downloaded from huggingface but when the server starts, there is a permission denied error.</p>
<p>Apptainer command:</p>
<pre><code>apptainer run --nv --env HF_TOKEN=$HF_TOKEN --env RUST_BACKTRACE=full --bind $volume:/data text-generation-inference_2.2.0.sif --model-id $model
</code></pre>
<p>Error:</p>
<pre><code>thread 'main' panicked at /usr/src/router/src/server.rs:1910:67:
called `Result::unwrap()` on an `Err` value: Os { code: 13, kind: PermissionDenied, message: &quot;Permission denied&quot; }
stack backtrace:
   0:     0x55eda601389f - &lt;std::sys_common::backtrace::_print::DisplayBacktrace as core::fmt::Display&gt;::fmt::h1e1a1972118942ad
   1:     0x55eda5b5b71b - core::fmt::write::hc090a2ffd6b28c4a
   2:     0x55eda5fde1c2 - std::io::Write::write_fmt::h8898bac6ff039a23
   3:     0x55eda6015389 - std::sys_common::backtrace::print::ha96650907276675e
   4:     0x55eda6014bae - std::panicking::default_hook::{{closure}}::h215c2a0a8346e0e0
   5:     0x55eda6015dd4 - std::panicking::rust_panic_with_hook::hac8bdceee1e4fe2c
   6:     0x55eda60156f2 - std::panicking::begin_panic_handler::{{closure}}::h00d785e82757ce3c
   7:     0x55eda6015649 - std::sys_common::backtrace::__rust_end_short_backtrace::h1628d957bcd06996
   8:     0x55eda6015636 - rust_begin_unwind
   9:     0x55eda5730bf2 - core::panicking::panic_fmt::hdc63834ffaaefae5
  10:     0x55eda5731045 - core::result::unwrap_failed::h82b551e0ff2b2176
  11:     0x55eda589475c - text_generation_router::server::run::{{closure}}::h1de6c6135029d307
  12:     0x55eda5aca7d6 - text_generation_router::main::{{closure}}::hb9f6c1fd1a653473
  13:     0x55eda5abf8ab - text_generation_router::main::h48ab4f33024a7b20
  14:     0x55eda5919c63 - std::sys_common::backtrace::__rust_begin_short_backtrace::h43b47338ee716213
  15:     0x55eda591a5ce - std::rt::lang_start::{{closure}}::h2a82aaad38d0ecd2
  16:     0x55eda5ad995f - main
  17:     0x1511480b5d90 - &lt;unknown&gt;
  18:     0x1511480b5e40 - __libc_start_main
  19:     0x55eda5771931 - _start
  20:                0x0 - &lt;unknown&gt;
2024-08-14T12:54:07.064654Z ERROR text_generation_launcher: Webserver Crashed
</code></pre>
<p>I tried using <code>--writable-tmpfs</code> and <code>--fakeroot</code> but that doesn't seem to help.</p>
","huggingface"
"78865570","HuggingFace accelerate device error when running evaluation","2024-08-13 09:54:19","","0","20","<python><machine-learning><huggingface><multi-gpu>","<p>I am running some experiments on a multi-GPU cluster, and I'm using accelerate. I'm trying to calculate some metrics after every batch iteration in the training dataloader. While the training code seems to work fine using accelerate (it utilizes multiple GPUs), I run into an error when trying to calculate said metrics. It seems that after doing a forward pass when evaluating the output tensors are put on another device than the input tensors. The code that gives an error is the following:</p>
<pre><code>def calculatePerplexity(sentence, model, tokenizer, accelerator):
    &quot;&quot;&quot;
    exp(loss)
    &quot;&quot;&quot;
    input_ids = torch.tensor(sentence).unsqueeze(0)
    print(f&quot;Input ids device: {input_ids.device}&quot;)
    model.eval()
    with torch.no_grad():
        outputs = model(input_ids, labels=input_ids)
    outputs = accelerator.gather_for_metrics(outputs)
    loss, logits = outputs[:2]
    loss, logits = accelerator.prepare(loss, logits)
    print(f&quot;Loss device: {loss.device}&quot;)
    print(f'Model device: {model.device}')
    print(f'Logits device: {logits.device}')

    probabilities = torch.nn.functional.softmax(logits, dim=-1)
    all_prob = []
    input_ids_processed = input_ids[0][1:]
    for i, token_id in enumerate(input_ids_processed):
        probability = probabilities[0, i, token_id].item()
        all_prob.append(probability)

    # stuff for metric calculation 
    probs = torch.nn.functional.softmax(logits[0, :-1], dim=-1)
    log_probs = torch.nn.functional.log_softmax(logits[0, :-1], dim=-1)
    token_log_probs = log_probs.gather(dim=-1, index=input_ids_processed.unsqueeze(-1)).squeeze(-1)
    mu = (probs * log_probs).sum(-1)
    sigma = (probs * torch.square(log_probs)).sum(-1) - torch.square(mu)
    mink_plus = (token_log_probs - mu) / sigma.sqrt()
</code></pre>
<p>The output of the debugging statements is as follows:</p>
<pre><code>Input ids device: cpu
Loss device: cuda:0
Model device: cpu
Logits device: cuda:0
</code></pre>
<p>This is the same for the 4 different GPUs I'm using, and this results in the following error:</p>
<pre><code>token_log_probs = log_probs.gather(dim=-1, index=input_ids_processed.unsqueeze(-1)).squeeze(-1)
[rank2]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]: RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA_gather)
</code></pre>
<p>I'm not sure what I'm doing wrong here, I thought that using either of the <code>gather_for_metrics</code> method or calling <code>accelerator.prepare</code>on the loss and logits would help but it doesn't (I get the same error when removing these statements). Any advice would be greatly appreciated!</p>
<p>For completeness sake, here is the rest of the (relevant) code I use when calculating the metrics:</p>
<pre><code># Training loop
for i, (batch_inputs, batch_labels) in tqdm(enumerate(dataloader)):
      all_labels += batch_labels

      unlearned_model, tokenizer = load_base_model(self.experiment_args.model_dir_prefix, self.experiment_args.model)
      torch.cuda.empty_cache()

      optimizer = torch.optim.Adam(unlearned_model.parameters(), lr=self.unlearning_args.lr)
      unlearned_model, optimizer, batch_inputs = accelerator.prepare(unlearned_model, optimizer, batch_inputs)
      # Unlearn data and calculate PPL values
      for i in range(self.unlearning_args.steps):
        unlearned_model = unlearn_dataslice(unlearned_model, optimizer, batch_inputs, self.unlearning_args, accelerator)
        
      torch.cuda.empty_cache()
      UL_PPL_vals += calculate_PPL_values(unlearned_model, tokenizer, batch_inputs, accelerator)

def unlearn_dataslice(model, optimizer, sentences, args, accelerator):
    learning_rate = args.lr
    model.train()

    optimizer.zero_grad()
    input_data = sentences.clone().detach()

    output = model(input_data)
    # Add a minus do to gradient ascent instead of descent
    loss = -output[0]['logits']
    accelerator.backward(loss.mean())
    torch.cuda.empty_cache()
    optimizer.step()
    del optimizer
    torch.cuda.empty_cache()
    return model

def calculate_PPL_values(model, tokenizer, text_batch, accelerator):
    PPL_values = []
    for text in text_batch:
        PPL = calculatePerplexity(text, model, tokenizer, accelerator)[0]
        PPL_values.append(PPL)
    return PPL_values
</code></pre>
<p>In this code I removed a lot of debugging statements that checked that the device of both the <code>unlearned_model</code> and <code>batch_inputs</code> are on the same device <code>cpu</code> throughout the training loop, so I'm pretty sure there's no inconsistency there.</p>
","huggingface"
"78865165","Colab Huggingface DataCollator Import Error","2024-08-13 08:23:58","","-1","10","<huggingface>","<p>I run a simple Huggingface code in my Google Colab Pro+ session (TPU) and everything crashes due to unknown reasons. I forward it your way over here.</p>
<p>The code is:
&quot;from transformers import AutoTokenizer, AutoConfig, GPT2DoubleHeadsModel, DataCollatorForLanguageModeling&quot;, that simple.</p>
<p>I do not meet this error when running using other services.</p>
","huggingface"
"78864887","ValueError: Need either a `state_dict` or a `save_folder` containing offloaded weights while trying to run the transformer.pipeline for Llama-3.1-8B","2024-08-13 07:17:55","","0","11","<huggingface-transformers><huggingface><llama><llama3.1>","<p>I tried to run &quot;meta-llama/Meta-Llama-3.1-8B-Instruct&quot; model on MAC M2 but getting error while running below code:</p>
<pre><code>model = &quot;meta-llama/Meta-Llama-3.1-8B-Instruct&quot;
tokenizer = AutoTokenizer.from_pretrained(model)    
transformers.pipeline( &quot;text-generation&quot;, 
model=model, 
torch_dtype=torch.float16,  
device_map=&quot;auto&quot;) 
</code></pre>
<p><strong>Following Error is thrown while executing transformers.pipeline(...)</strong></p>
<pre><code>File /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site- packages/transformers/pipelines/__init__.py:
895,

raise ValueError(&quot;Need either a `state_dict` or a `save_folder` containing offloaded weights.&quot;) ValueError: Need either a `state_dict` or a `save_folder` containing offloaded weights.
</code></pre>
<p><strong>I tried following options to resolve:</strong></p>
<ol>
<li>Passed <code>offload_folder=&quot;save_folder&quot;</code> in <code>from_pretrained</code> and <code>pipeline</code> both:</li>
</ol>
<pre><code>   tokenizer = AutoTokenizer.from_pretrained(model,offload_folder=&quot;save_folder&quot;)    
   transformers.pipeline( &quot;text-generation&quot;, 
   model=model, 
   torch_dtype=torch.float16,  
   device_map=&quot;auto&quot;, offload_folder=&quot;save_folder&quot;) 
</code></pre>
<ol start=""2"">
<li>Tried Passing <code>state_dict={}</code> in <code>transformers.pipeline()</code></li>
</ol>
","huggingface"
"78861222","Why did my Google Colab session crash while running the Llama model?","2024-08-12 10:39:06","","0","40","<python><google-colaboratory><huggingface-transformers><huggingface><llama>","<p>I am trying to use the meta-llama/Llama-2-7b-hf model and run it locally on my premises but the session crashed during the process.</p>
<p>I am trying to use the meta-llama/Llama-2-7b-hf model and run it locally on my premises. To do this, I am using Google Colab and have obtained an access key from Hugging Face. I am utilizing their transformers library for the necessary tasks. Initially, I used the T4 GPU runtime stack on Google Colab, which provided 12.7 GB of system RAM, 15.0 GB of GPU RAM, and 78.2 GB of disk space. Despite these resources, my session crashed, and I encountered the following error:</p>
<p><a href=""https://i.sstatic.net/gws5EgWI.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/gws5EgWI.png"" alt=""enter image description here"" /></a></p>
<p>Subsequently, I switched to the TPU V2 runtime stack, which offers 334.6 GB of system RAM and 225.3 GB of disk space, but the issue persisted.</p>
<p>Here is my code:</p>
<pre><code>!pip install transformers
!pip install --upgrade transformers

from huggingface_hub import login
login(token='Access Token From Hugging Face')

import pandas as pd
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
from torch.utils.data import Dataset

# Load pre-trained Meta-Llama-3.1-8B model
model_name = &quot;meta-llama/Llama-2-7b-hf&quot;
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)
</code></pre>
","huggingface"
"78859343","How to reinitialize from scratch GPT2 XL in HuggingFace?","2024-08-11 20:27:07","","0","31","<python><machine-learning><huggingface-transformers><huggingface>","<p>I'm trying to confirm that my GPT-2 model is being trained from scratch, rather than using any pre-existing pre-trained weights. Here's my approach:</p>
<ol>
<li><strong>Load the pre-trained GPT-2 XL model</strong>: I load a pre-trained GPT-2 XL model using <code>AutoModelForCausalLM.from_pretrained(&quot;gpt2-xl&quot;)</code> and calculate the total L2 norm of the weights for this model.</li>
<li><strong>Initialize a new GPT-2 model from scratch</strong>: I then initialize a new GPT-2 model from scratch with a custom configuration using <code>GPT2Config</code>.</li>
<li><strong>Compare L2 norms</strong>: I calculate the L2 norm of the weights for both the pre-trained model and the freshly initialized model. My assumption is that the L2 norm of the scratch model should be much smaller compared to the pre-trained model if the scratch model is truly initialized from random weights.</li>
</ol>
<p>Here's the code snippet:</p>
<pre><code>import torch
from transformers import GPT2LMHeadModel, GPT2Config, AutoModelForCausalLM

# Step 1: Load the pre-trained GPT-2 XL model
pretrained_model = AutoModelForCausalLM.from_pretrained(&quot;gpt2-xl&quot;)

# Step 2: Calculate the L2 norm of the weights for the pre-trained model
pretrained_weight_norm = 0.0
for param in pretrained_model.parameters():
    pretrained_weight_norm += torch.norm(param, p=2).item()

print(f&quot;Total L2 norm of pre-trained model weights: {pretrained_weight_norm:.2f}&quot;)

# Step 3: Initialize a new GPT-2 model from scratch with custom configuration
config = GPT2Config(
    vocab_size=52000,  # Ensure this matches the tokenizer's vocabulary size
    n_ctx=1024,  # Context window size (number of tokens the model can see at once)
    bos_token_id=0,  # Begin-of-sequence token
    eos_token_id=1,  # End-of-sequence token
)
model = GPT2LMHeadModel(config)

# Step 4: Calculate the L2 norm of the weights for the freshly initialized model
scratch_weight_norm = 0.0
for param in model.parameters():
    scratch_weight_norm += torch.norm(param, p=2).item()

print(f&quot;Total L2 norm of model initialized from scratch: {scratch_weight_norm:.2f}&quot;)
</code></pre>
<p>Is this method a valid way to confirm that the model is being trained from scratch? Are there any potential issues or better ways to verify that the model has no pre-existing learned weights?</p>
<p>Looks right</p>
<pre><code>~/beyond-scale-language-data-diversity$ /opt/conda/envs/beyond_scale_div_coeff/bin/python /home/ubuntu/beyond-scale-language-data-diversity/playground/test_gpt2_pt_vs_reinit_scratch.py
config.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 689/689 [00:00&lt;00:00, 8.05MB/s]
model.safetensors: 100%|██████████████████████████████████████████████████████████████████████████████████| 6.43G/6.43G [00:29&lt;00:00, 221MB/s]
generation_config.json: 100%|████████████████████████████████████████████████████████████████████████████████| 124/124 [00:00&lt;00:00, 1.03MB/s]
Total L2 norm of pre-trained model weights: 24542.74
Total L2 norm of model initialized from scratch: 1637.31
(beyond_scale_div_coeff)                                                        
</code></pre>
<p>cross: <a href=""https://discuss.huggingface.co/t/how-to-reinitialize-from-scratch-gpt-xl-in-hugging-face-hf/101905"" rel=""nofollow noreferrer"">https://discuss.huggingface.co/t/how-to-reinitialize-from-scratch-gpt-xl-in-hugging-face-hf/101905</a></p>
<p>ref: <a href=""https://github.com/alycialee/beyond-scale-language-data-diversity/issues/18"" rel=""nofollow noreferrer"">https://github.com/alycialee/beyond-scale-language-data-diversity/issues/18</a></p>
","huggingface"
"78855451","How to ask multiple choice questions using pipeline","2024-08-10 07:18:32","78856931","1","28","<huggingface-transformers><huggingface>","<p>I want to ask multiple choice questions to the Hugging Face transformer pipeline; however, there does not seem to be a good task choice for this.</p>
<p>I am referencing this:</p>
<pre><code>qa_pipeline = pipeline(&quot;question-answering&quot;, model=&quot;distilbert-base-uncased&quot;)
result = qa_pipeline(question = question, context = context)
</code></pre>
<p>I know question-answering exists; however, this task requires context where then the pipeline would extract the answer from the context. For specifically multiple choice answering, this method does not make much sense because I am trying to recieve a specific letter, not a word associated with the letter. Additionally, I could possiblely try text-generation, but I was hoping there was some better solution than this. Let me know your thoughts!</p>
","huggingface"
"78854952","What value should the sequence_length parameter be when converting to TFLite","2024-08-10 00:42:19","","0","10","<tensorflow-lite><huggingface><tflite>","<p>I am converting a model hosted by HuggingFace to a TensorFlow Lite model by using HuggingFace's <a href=""https://huggingface.co/docs/optimum/exporters/tflite/overview"" rel=""nofollow noreferrer"">Optimum</a> exporter.</p>
<p>In their <a href=""https://huggingface.co/docs/transformers/en/tflite"" rel=""nofollow noreferrer"">documentation</a> they use the following example:</p>
<pre><code>optimum-cli export tflite --model google-bert/bert-base-uncased --sequence_length 128 bert_tflite/
</code></pre>
<p>As you can see they specify <code>sequence_length</code> as <code>128</code> which the <a href=""https://huggingface.co/docs/optimum/main/en/exporters/tflite/usage_guides/export_a_model"" rel=""nofollow noreferrer"">help output</a> describes as:</p>
<blockquote>
<p>Sequence length that the TFLite exported model will be able to take as input.</p>
</blockquote>
<p>When converting another model what <code>sequence_length</code> should I use? Should it not simply match what was used before and if so how do I find what it was?</p>
","huggingface"
"78854369","Issue in Accessing Spacy Models from Hugging Face","2024-08-09 19:34:43","","0","20","<github><named-entity-recognition><huggingface><spacy-transformers>","<p>I had uploaded my NER model trained through Spacy on huggingface. While trying to inference, I am facing this issue:</p>
<pre><code>pip install https://huggingface.co/JeswinMS4/en_resume_pipeline/resolve/main/en_resume_pipeline-any-py3-none-any.whl
</code></pre>
<blockquote>
<p>ERROR: Invalid requirement: 'en-resume-pipeline==any': Expected end or
semicolon (after name and no valid version specifier)
en-resume-pipeline==any
^</p>
</blockquote>
<p>The documentation also states this way to load. Is there anything I am missing out?</p>
","huggingface"
"78851154","How to switch the mirror of huggingface in the program","2024-08-09 04:08:29","","-1","39","<python><python-3.x><artificial-intelligence><huggingface><gradio>","<p>I am deploying the model <a href=""https://github.com/lllyasviel/IC-Light"" rel=""nofollow noreferrer"">IC Light</a> locally, I have created the conda environment, installed the 3rd party libraries via pip and ran <code>gradio_demo.py</code>. however he is reporting an error.</p>
<pre><code>C:\Users\Gavin\anaconda3\envs\iclight\lib\site-packages\transformers\utils\generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
C:\Users\Gavin\anaconda3\envs\iclight\lib\site-packages\transformers\utils\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
C:\Users\Gavin\anaconda3\envs\iclight\lib\site-packages\transformers\utils\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
C:\Users\Gavin\anaconda3\envs\iclight\lib\site-packages\huggingface_hub\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Traceback (most recent call last):
  File &quot;C:\Users\Gavin\anaconda3\envs\iclight\lib\site-packages\urllib3\connectionpool.py&quot;, line 466, in _make_request
    self._validate_conn(conn)
  File &quot;C:\Users\Gavin\anaconda3\envs\iclight\lib\site-packages\urllib3\connectionpool.py&quot;, line 1095, in _validate_conn
    conn.connect()
  File &quot;C:\Users\Gavin\anaconda3\envs\iclight\lib\site-packages\urllib3\connection.py&quot;, line 652, in connect
    sock_and_verified = _ssl_wrap_socket_and_match_hostname(
  File &quot;C:\Users\Gavin\anaconda3\envs\iclight\lib\site-packages\urllib3\connection.py&quot;, line 805, in _ssl_wrap_socket_and_match_hostname
    ssl_sock = ssl_wrap_socket(
  File &quot;C:\Users\Gavin\anaconda3\envs\iclight\lib\site-packages\urllib3\util\ssl_.py&quot;, line 465, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)
  File &quot;C:\Users\Gavin\anaconda3\envs\iclight\lib\site-packages\urllib3\util\ssl_.py&quot;, line 509, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
  File &quot;C:\Users\Gavin\anaconda3\envs\iclight\lib\ssl.py&quot;, line 513, in wrap_socket
    return self.sslsocket_class._create(
  File &quot;C:\Users\Gavin\anaconda3\envs\iclight\lib\ssl.py&quot;, line 1104, in _create
    self.do_handshake()
  File &quot;C:\Users\Gavin\anaconda3\envs\iclight\lib\ssl.py&quot;, line 1375, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLEOFError: [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;C:\Users\Gavin\anaconda3\envs\iclight\lib\site-packages\urllib3\connectionpool.py&quot;, line 789, in urlopen
    response = self._make_request(
  File &quot;C:\Users\Gavin\anaconda3\envs\iclight\lib\site-packages\urllib3\connectionpool.py&quot;, line 490, in _make_request
    raise new_e
urllib3.exceptions.SSLError: [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File &quot;C:\Users\Gavin\anaconda3\envs\iclight\lib\site-packages\requests\adapters.py&quot;, line 667, in send
    resp = conn.urlopen(
  File &quot;C:\Users\Gavin\anaconda3\envs\iclight\lib\site-packages\urllib3\connectionpool.py&quot;, line 843, in urlopen
    retries = retries.increment(
  File &quot;C:\Users\Gavin\anaconda3\envs\iclight\lib\site-packages\urllib3\util\retry.py&quot;, line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /stablediffusionapi/realistic-vision-v51/resolve/main/tokenizer/tokenizer_config.json (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;D:\IC-Light\gradio_demo.py&quot;, line 22, in &lt;module&gt;
    tokenizer = CLIPTokenizer.from_pretrained(sd15_name, subfolder=&quot;tokenizer&quot;)
  File &quot;C:\Users\Gavin\anaconda3\envs\iclight\lib\site-packages\transformers\tokenization_utils_base.py&quot;, line 1951, in from_pretrained
    resolved_config_file = cached_file(
  File &quot;C:\Users\Gavin\anaconda3\envs\iclight\lib\site-packages\transformers\utils\hub.py&quot;, line 389, in cached_file
    resolved_file = hf_hub_download(
  File &quot;C:\Users\Gavin\anaconda3\envs\iclight\lib\site-packages\huggingface_hub\utils\_deprecation.py&quot;, line 101, in inner_f
    return f(*args, **kwargs)
  File &quot;C:\Users\Gavin\anaconda3\envs\iclight\lib\site-packages\huggingface_hub\utils\_validators.py&quot;, line 114, in _inner_fn
    return fn(*args, **kwargs)
  File &quot;C:\Users\Gavin\anaconda3\envs\iclight\lib\site-packages\huggingface_hub\file_download.py&quot;, line 1240, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
  File &quot;C:\Users\Gavin\anaconda3\envs\iclight\lib\site-packages\huggingface_hub\file_download.py&quot;, line 1303, in _hf_hub_download_to_cache_dir
    (url_to_download, etag, commit_hash, expected_size, head_call_error) = _get_metadata_or_catch_error(
  File &quot;C:\Users\Gavin\anaconda3\envs\iclight\lib\site-packages\huggingface_hub\file_download.py&quot;, line 1751, in _get_metadata_or_catch_error
    metadata = get_hf_file_metadata(
  File &quot;C:\Users\Gavin\anaconda3\envs\iclight\lib\site-packages\huggingface_hub\utils\_validators.py&quot;, line 114, in _inner_fn
    return fn(*args, **kwargs)
  File &quot;C:\Users\Gavin\anaconda3\envs\iclight\lib\site-packages\huggingface_hub\file_download.py&quot;, line 1673, in get_hf_file_metadata
    r = _request_wrapper(
  File &quot;C:\Users\Gavin\anaconda3\envs\iclight\lib\site-packages\huggingface_hub\file_download.py&quot;, line 376, in _request_wrapper
    response = _request_wrapper(
  File &quot;C:\Users\Gavin\anaconda3\envs\iclight\lib\site-packages\huggingface_hub\file_download.py&quot;, line 399, in _request_wrapper
    response = get_session().request(method=method, url=url, **params)
  File &quot;C:\Users\Gavin\anaconda3\envs\iclight\lib\site-packages\requests\sessions.py&quot;, line 589, in request
    resp = self.send(prep, **send_kwargs)
  File &quot;C:\Users\Gavin\anaconda3\envs\iclight\lib\site-packages\requests\sessions.py&quot;, line 703, in send
    r = adapter.send(request, **kwargs)
  File &quot;C:\Users\Gavin\anaconda3\envs\iclight\lib\site-packages\huggingface_hub\utils\_http.py&quot;, line 66, in send
    return super().send(request, *args, **kwargs)
  File &quot;C:\Users\Gavin\anaconda3\envs\iclight\lib\site-packages\requests\adapters.py&quot;, line 698, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: (MaxRetryError(&quot;HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /stablediffusionapi/realistic-vision-v51/resolve/main/tokenizer/tokenizer_config.json (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)')))&quot;), '(Request ID: 6a696e3e-c787-468c-9aa1-92dc7541a137)')
</code></pre>
<p>I analyzed the last error and found that it was caused by <code>huggingface.co</code>. For some reason, direct access to <code>huggingface.co</code> is a bit difficult in my area, so I decided to use <a href=""https://hf-mirror.com/"" rel=""nofollow noreferrer"">hf-mirror.com</a> as a mirror. I tried to add <code>HF_ENDPOINT = &quot;https://hf-mirror.com&quot;</code> to the environment variable, and I still get an error. What can I do to get the model to use <code>hf-mirror</code> as a mirror of <code>huggingface.co</code>?</p>
","huggingface"
"78848134","Huggingface Autotrain fail","2024-08-08 11:26:32","","-1","23","<python><machine-learning><artificial-intelligence><huggingface-transformers><huggingface>","<p>I am trying to Huggingface Autotrain. My data set:</p>
<p><a href=""https://i.sstatic.net/lQuhT7w9.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/lQuhT7w9.png"" alt=""dataset screenshot"" /></a></p>
<p>Below is my configuration:</p>
<p><a href=""https://i.sstatic.net/BHgggZFz.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/BHgggZFz.png"" alt=""config screenshot"" /></a></p>
<p>When I start training I get this error:
<a href=""https://i.sstatic.net/HDcedfOy.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/HDcedfOy.png"" alt=""error screenshot"" /></a></p>
<p>What am i doing wrong? Do I have to mention categorical columns and numerical columns? If so, are they comma separated? How do I decide which columns go where?</p>
","huggingface"
"78846880","huggingface train only new tokens embedding","2024-08-08 06:47:30","","0","25","<pytorch><large-language-model><word-embedding><huggingface>","<p>I want to add new tokens to a huggingface model and train only their embeddings. How can I do that?</p>
<p>There are some ways to train only part of a weight tensor (e.g. <a href=""https://discuss.pytorch.org/t/how-can-i-freeze-weights-in-element-wise/117988"" rel=""nofollow noreferrer"">https://discuss.pytorch.org/t/how-can-i-freeze-weights-in-element-wise/117988</a>), however they seem to have access and edit the model class, which probably requires a lot of work for every specific LLM I will work on, and I'm intending to do this training to a lot of different models.</p>
<p>Another way is to train all the model and after every step reassign all the weights except of the embeddings of the new tokens, which sounds inefficient and can even be just incorrect for certain optimizers (as explained in the above link).</p>
","huggingface"
"78846353","Training a huggingface model on streaming data","2024-08-08 02:51:53","","0","20","<streaming><huggingface>","<p>Problem:<br />
Something is not correct with set up and args.  The training hangs and does not complete the eval step.</p>
<p>Background and Code:<br />
I'm finetuning a whisper model on Catalan Common Voice data using the <a href=""https://huggingface.co/learn/audio-course/chapter5/fine-tuning"" rel=""nofollow noreferrer"">huggingface tutorial here</a>.   The Corpus 17.0 version of dataset is 72 GB, so only difference in this code with tutorial is that the datasets are streaming=True upon load_dataset.  Loading this dataset without streaming maxes out the RAM on Colab, even with Pro subscription.  With streaming, can load the data.</p>
<pre><code>!pip install -q datasets evaluate jiwer
</code></pre>
<pre><code>from datasets import Dataset, load_dataset, DatasetDict, concatenate_datasets
</code></pre>
<pre><code>common_voice = DatasetDict()

common_voice['train']= load_dataset(&quot;mozilla-foundation/common_voice_17_0&quot;, &quot;ca&quot;, split=&quot;train&quot;, streaming=True)#.select_columns([&quot;audio&quot;, &quot;sentence&quot;])#,streaming=True)
common_voice['val'] = load_dataset(&quot;mozilla-foundation/common_voice_17_0&quot;, &quot;ca&quot;, split=&quot;validation&quot;,streaming=True)#.select_columns([&quot;audio&quot;, &quot;sentence&quot;])#,streaming=True)

print(common_voice)
</code></pre>
<pre><code>common_voice['train'] = common_voice['train'].select_columns([&quot;audio&quot;, &quot;sentence&quot;])
common_voice['val'] = common_voice['val'].select_columns([&quot;audio&quot;, &quot;sentence&quot;])
</code></pre>
<pre><code>from transformers.models.whisper.tokenization_whisper import TO_LANGUAGE_CODE

TO_LANGUAGE_CODE
</code></pre>
<pre><code>from transformers import WhisperProcessor

processor = WhisperProcessor.from_pretrained(
    &quot;openai/whisper-small&quot;, language=&quot;spanish&quot;, task=&quot;transcribe&quot;
)
</code></pre>
<pre><code>common_voice[&quot;train&quot;].features
</code></pre>
<pre><code>from datasets import Audio

sampling_rate = processor.feature_extractor.sampling_rate
common_voice['train'] = common_voice['train'].cast_column(&quot;audio&quot;, Audio(sampling_rate=sampling_rate))
common_voice['val'] = common_voice['val'].cast_column(&quot;audio&quot;, Audio(sampling_rate=sampling_rate))
</code></pre>
<pre><code>def prepare_dataset(example):
    audio = example[&quot;audio&quot;]

    example = processor(
        audio=audio[&quot;array&quot;],
        sampling_rate=audio[&quot;sampling_rate&quot;],
        text=example[&quot;sentence&quot;],
    )

    # compute input length of audio sample in seconds
    example[&quot;input_length&quot;] = len(audio[&quot;array&quot;]) / audio[&quot;sampling_rate&quot;]

    return example
</code></pre>
<pre><code>common_voice['train'] = common_voice['train'].map(
    prepare_dataset)#, remove_columns=common_voice.column_names[&quot;train&quot;], num_proc=1
# )
common_voice['val'] = common_voice['val'].map(
    prepare_dataset)
</code></pre>
<pre><code>max_input_length = 30.0 #cut off for whisper

def is_audio_in_length_range(length):
    return length &lt; max_input_length
</code></pre>
<pre><code>common_voice[&quot;train&quot;] = common_voice[&quot;train&quot;].filter(
    is_audio_in_length_range,
    input_columns=[&quot;input_length&quot;],
)
</code></pre>
<pre><code># new size of train.  Could measure number of hours train here.
common_voice[&quot;train&quot;].features
</code></pre>
<pre><code>import torch

from dataclasses import dataclass
from typing import Any, Dict, List, Union


@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    processor: Any

    def __call__(
        self, features: List[Dict[str, Union[List[int], torch.Tensor]]]
    ) -&gt; Dict[str, torch.Tensor]:
        # split inputs and labels since they have to be of different lengths and need different padding methods
        # first treat the audio inputs by simply returning torch tensors
        input_features = [
            {&quot;input_features&quot;: feature[&quot;input_features&quot;][0]} for feature in features
        ]
        batch = self.processor.feature_extractor.pad(input_features, return_tensors=&quot;pt&quot;)

        # get the tokenized label sequences
        label_features = [{&quot;input_ids&quot;: feature[&quot;labels&quot;]} for feature in features]
        # pad the labels to max length
        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=&quot;pt&quot;)

        # replace padding with -100 to ignore loss correctly
        labels = labels_batch[&quot;input_ids&quot;].masked_fill(
            labels_batch.attention_mask.ne(1), -100
        )

        # if bos token is appended in previous tokenization step,
        # cut bos token here as it's append later anyways
        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():
            labels = labels[:, 1:]

        batch[&quot;labels&quot;] = labels

        return batch
</code></pre>
<pre><code>data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)
</code></pre>
<pre><code>import evaluate
metric = evaluate.load(&quot;wer&quot;)
</code></pre>
<pre><code>from transformers.models.whisper.english_normalizer import BasicTextNormalizer

normalizer = BasicTextNormalizer()


def compute_metrics(pred):
    pred_ids = pred.predictions
    label_ids = pred.label_ids

    # replace -100 with the pad_token_id
    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id

    # we do not want to group tokens when computing the metrics
    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)
    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)

    # compute orthographic wer
    wer_ortho = 100 * metric.compute(predictions=pred_str, references=label_str)

    # compute normalised WER
    pred_str_norm = [normalizer(pred) for pred in pred_str]
    label_str_norm = [normalizer(label) for label in label_str]
    # filtering step to only evaluate the samples that correspond to non-zero references:
    pred_str_norm = [
        pred_str_norm[i] for i in range(len(pred_str_norm)) if len(label_str_norm[i]) &gt; 0
    ]
    label_str_norm = [
        label_str_norm[i]
        for i in range(len(label_str_norm))
        if len(label_str_norm[i]) &gt; 0
    ]

    wer = 100 * metric.compute(predictions=pred_str_norm, references=label_str_norm)

    return {&quot;wer_ortho&quot;: wer_ortho, &quot;wer&quot;: wer}
</code></pre>
<pre><code>model = WhisperForConditionalGeneration.from_pretrained(&quot;openai/whisper-small&quot;)
</code></pre>
<pre><code>from functools import partial

# disable cache during training since it's incompatible with gradient checkpointing
model.config.use_cache = False

# set language and task for generation and re-enable cache
model.generate = partial(
    model.generate, language=&quot;spanish&quot;, task=&quot;transcribe&quot;, use_cache=True
)
</code></pre>
<pre><code>from transformers import Seq2SeqTrainingArguments

training_args = Seq2SeqTrainingArguments(
    output_dir=&quot;./whisper-small-ca&quot;,  # name on the HF Hub
    per_device_train_batch_size=16,
    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size
    learning_rate=1e-5,
    lr_scheduler_type=&quot;constant_with_warmup&quot;,
    warmup_steps=50,
    max_steps=500,  # increase to 4000 if you have your own GPU or a Colab paid plan
    gradient_checkpointing=True,
    fp16=True,
    fp16_full_eval=True,
    evaluation_strategy=&quot;steps&quot;,
    per_device_eval_batch_size=16,
    predict_with_generate=True,
    generation_max_length=225,
    save_steps=100,
    eval_steps=100,
    logging_steps=25,
    report_to=[&quot;tensorboard&quot;],
    load_best_model_at_end=True,
    metric_for_best_model=&quot;wer&quot;,
    greater_is_better=False,
    push_to_hub=True,
)
</code></pre>
<pre><code>from transformers import Seq2SeqTrainer

trainer = Seq2SeqTrainer(
    args=training_args,
    model=model,
    train_dataset=common_voice[&quot;train&quot;],
    eval_dataset=common_voice[&quot;val&quot;],
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    tokenizer=processor,
)
</code></pre>
<pre><code>trainer.train()
</code></pre>
<pre><code>Reading metadata...: 1146209it [00:28, 40426.93it/s]
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 [500/500 04:28, Epoch 1/9223372036854775807]
Step    Training Loss   Validation Loss
Reading metadata...: 16402it [00:00, 19432.46it/s]
</code></pre>
<p>I was expecting to see results: Step, Training Loss,  Validation Loss with WER and WER Ortho</p>
<p>Something like this from the</p>
<p>Training Loss   Epoch   Step   Validation Loss  WerOrtho   Wer</p>
<p>0.136   1.63    500   0.1727   63.8972   14.0661</p>
","huggingface"
"78845453","Issue In Using create_sql_query_chain from LangChain with Mistral 7b v0.1","2024-08-07 19:53:41","","0","24","<langchain><large-language-model><huggingface><mistral-7b><langchain4j>","<p><a href=""https://i.sstatic.net/FTWxPgVo.png"" rel=""nofollow noreferrer"">Output generated by LLM </a>
I am working on Mistral 7b v0.1 LLM. i am using APPEnd Point from huggingface × langchain.
I have configures the model successfully.
Next I am using the create_sql_query_chain for text to sql.
When I am giving question, its generate SQLQuery for that question, but in addition to that its also giving more examples question and generate queries for that as well (as shown in image attached)</p>
<p>I don't want this behaviour. What I want only query for my question</p>
<p>I tried following solutions (but no success)</p>
<ol>
<li>More detailed instructional prompts</li>
<li>Playing with k (k=1 specifically) for create_sql_query_chain</li>
<li>Playing with temperature and top_p</li>
</ol>
<p>Any help is highly appreciated</p>
","huggingface"
"78845198","Upsert Vector Service in a Flowise deployment in Huggingface","2024-08-07 18:29:21","","-1","23","<dockerfile><artificial-intelligence><large-language-model><huggingface><flowise>","<p>When I try to upsert the vectors using any vector retriever available in FlowiseAI, I keep getting the following error:</p>
<pre><code>Error: vectorsService.upsertVector - Error: Error: An error occurred while 
retrieving the blob
</code></pre>
<p>I set the Dockerfile with the variable:
<code>BLOB_STORAGE_PATH=$BASE_PATH/storage</code></p>
<p>I can also push my data into the document stores section of Flowise, so it's not an issue related to blob storage.
<a href=""https://i.sstatic.net/XITkLXRc.png"" rel=""nofollow noreferrer"">The chatflow I am working on</a></p>
<p>I tried various file types, loaders and splitters, although the error remains the same.</p>
","huggingface"
"78840299","Duplicating/cloning huggingface space","2024-08-06 17:14:13","","0","20","<clone><huggingface><huggingface-hub>","<p><a href=""https://i.sstatic.net/2fm9slYM.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/2fm9slYM.png"" alt=""enter image description here"" /></a></p>
<p>I'm interested in setting up an inference API, using huggingface. I'm following an article <a href=""https://medium.com/@dahmanihichem01/mixtral-and-rest-api-turning-mixtral-8x7b-into-an-api-using-huggingface-spaces-a8b150b47246"" rel=""nofollow noreferrer"">https://medium.com/@dahmanihichem01/mixtral-and-rest-api-turning-mixtral-8x7b-into-an-api-using-huggingface-spaces-a8b150b47246</a> which shows how to do this.</p>
<p>I've gotten to the point where it says:</p>
<pre><code>&quot;Go to the right top corner and press the three dots button, then click “Clone Repository”
</code></pre>
<p>If I understand correctly, this should be enough to set up a cloned space. However currently I see the screenshot.</p>
<pre><code># Make sure you have git-lfs installed (https://git-lfs.com)
git lfs install

git clone https://huggingface.co/spaces/iiced/mixtral-46.7b-fastapi

# If you want to clone without large files - just their pointers
GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/spaces/iiced/mixtral-46.7b-fastapi
</code></pre>
<p>Am I supposed to clone the repository to my local machine and then upload it back to huggingspace , or am I missing something?</p>
","huggingface"
"78837463","HuggingFace: Efficient Large-Scale Embedding Extraction for DNA Sequences Using Transformers","2024-08-06 06:09:24","","0","89","<python><huggingface-transformers><huggingface><huggingface-tokenizers><huggingface-datasets>","<p>I have a very large dataframe (60+ million rows) that I would like to use a transformer model to grab the embeddings for these rows (DNA sequences). Basically, this involves tokenizing first, then I can get the embeddings.
Because of RAM limits, I have found that tokenizing and then embedding all in one py file won't work. Here's the workaround I found that worked for a dataframe with ~30million rows (but it isn't working for the larger df):</p>
<ol>
<li>tokenizing-- saving the output as 200 chunks/shards</li>
<li>feeding those 200 chunks separately to get embedded</li>
<li>these embeddings then get concatenated into one larger file of embeddings</li>
</ol>
<p>final embedding file should have these columns:
[['Chromosome', 'label', 'embeddings']]</p>
<p>Overall, I'm a little lost in terms of how I can get this to work for my larger dataset.</p>
<p>I've looked into streaming the dataset, but I don't think that will actually help because I need all of the embeddings, not just a few. Perhaps it could work if I stream the tokenization and feed it into the embedding process a bit at a time (delete the tokens along the way). This way, I don't have to save the tokens. Please correct me if this is not feasible.</p>
<p>Ideally, I would like to avoid having to shard the data, but I would just like the code to run at this point without reaching the RAM limit.</p>
<p>step 1</p>
<pre><code>dataset = Dataset.from_pandas(element_final[['Chromosome', 'sequence', 'label']]) 

dataset = dataset.shuffle(seed=42)
tokenizer = AutoTokenizer.from_pretrained(f&quot;InstaDeepAI/nucleotide-transformer-500m-human-ref&quot;)
def tokenize_function(examples):
    outputs = tokenizer.batch_encode_plus(examples[&quot;sequence&quot;], return_tensors=&quot;pt&quot;, truncation=False, padding=False, max_length=80)
    return outputs
    
# Creating tokenized  dataset
tokenized_dataset = dataset.map(
    tokenize_function,
    batched=True, batch_size=2000)

tokenized_dataset.save_to_disk(f&quot;tokenized_elements/tokenized_{ELEMENT}&quot;, num_shards=200)
</code></pre>
<p>step 2 (this code runs over each of the 200 shards)</p>
<pre><code>input_file = f&quot;tokenized_elements/tokenized_{ELEMENT_LABEL}/{filename}.arrow&quot;

# Load input data
d1 = Dataset.from_file(input_file)

def embed_function(examples):
    torch.cuda.empty_cache()
    gc.collect()

    inputs = torch.tensor(examples['input_ids'])  # Convert to tensor
    inputs = inputs.to(device)

    with torch.no_grad():
        outputs = model(input_ids=inputs, output_hidden_states=True)

    # Step 3: Extract the embeddings
    hidden_states = outputs.hidden_states  # List of hidden states from all layers
    embeddings = hidden_states[-1]  # Assuming you want embeddings from the last layer
    averaged_embeddings = torch.mean(embeddings, dim=1)  # Calculate mean along dimension 1 (the dimension with size 86)
    averaged_embeddings = averaged_embeddings.to(torch.float32)  # Ensure float32 data type
    return {'embeddings': averaged_embeddings}

# Map embeddings function to input data
embeddings = d1.map(embed_function, batched=True, batch_size=1550)
embeddings = embeddings.remove_columns([&quot;input_ids&quot;, &quot;attention_mask&quot;])

# Save embeddings to disk
output_dir = f&quot;embedded_elements/embeddings_{ELEMENT_LABEL}/{filename}&quot;  # Assuming ELEMENT_LABEL is defined elsewhere
</code></pre>
<p>step 3: concatenate all 200 shards of embeddings into 1.</p>
","huggingface"
"78828818","What happens when we define a BitsAndBytesConfig without enabling load in 4 or 8?","2024-08-03 13:42:16","","0","33","<huggingface><peft>","<p>I have been experimenting with doing PEFT with LoRA on some HuggingFace models. After performing many experiments and documenting them, I noticed that I did not specify the type of quantization when defining a BitsAndBytesConfig as follows:</p>
<pre class=""lang-py prettyprint-override""><code>quantization_config = BitsAndBytesConfig(load_in_8bit_fp32_cpu_offload=True)

model = AutoModelForCausalLM.from_pretrained(
    base_model,
    torch_dtype=torch.float16,
    quantization_config=quantization_config,
    device_map=&quot;auto&quot;,
)
</code></pre>
<p>I checked the default values of the <a href=""https://huggingface.co/docs/transformers/main/main_classes/quantization#transformers.BitsAndBytesConfig"" rel=""nofollow noreferrer"">BitsAndBytesConfig</a>, and I found that the default values for load_in_8bit and load_in_4bit are set to False. What happens in this case? Is quantization used?</p>
","huggingface"
"78828715","Finding config.json for Llama 3.1 8B","2024-08-03 12:54:18","78832239","0","387","<python><pytorch><huggingface><llama><llama3>","<p>I installed the Llama 3.1 8B model through Meta's <a href=""https://github.com/meta-llama/llama-models"" rel=""nofollow noreferrer"">Github page</a>, but I can't get their example code to work. I'm running the following code in the same directory as the Meta-Llama-3.1-8B folder:</p>
<pre><code>import transformers
import torch

pipeline = transformers.pipeline(
  &quot;text-generation&quot;,
  model=&quot;Meta-Llama-3.1-8B&quot;,
  model_kwargs={&quot;torch_dtype&quot;: torch.bfloat16},
  device=&quot;cuda&quot;
)
</code></pre>
<p>The error is &quot;OSError: Meta-Llama-3.1-8B does not appear to have a file named config.json&quot;. Where can I get config.json?</p>
<p>I've installed the latest <code>transformers</code> module, and I understand that I can access the remote model on HuggingFace. But I'd rather use my local model. Is this possible?</p>
","huggingface"
"78827974","What does the ""AttributeError: 'NoneType' object has no attribute 'cget_managed_ptr'"" mean?","2024-08-03 06:12:06","","0","38","<machine-learning><pytorch><huggingface-transformers><huggingface><huggingface-trainer>","<p>I'm trying to train a model with very standard HF code I've used before:</p>
<pre class=""lang-py prettyprint-override""><code>import os
from transformers import Trainer, TrainingArguments, AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset
import torch
from pathlib import Path
import glob

def preprocess_function_proofnet_simple(examples: dict[str, list], tokenizer, max_length: int = 1024) -&gt; dict[str, torch.Tensor]:
    &quot;&quot;&quot;
    Preprocess the input data for the proofnet dataset.

    Args:
    examples: The examples to preprocess.
    tokenizer: The tokenizer for encoding the texts.

    Returns:
    The processed model inputs.
    &quot;&quot;&quot;
    inputs = [f&quot;{examples['nl_statement'][i]}{tokenizer.eos_token}{examples['formal_statement'][i]}&quot; for i in range(len(examples['nl_statement']))]
    model_inputs = tokenizer(inputs, max_length=max_length, padding=&quot;max_length&quot;, truncation=True, return_tensors=&quot;pt&quot;)
    labels = model_inputs.input_ids.clone()
    labels[labels == tokenizer.pad_token_id] = -100
    model_inputs[&quot;labels&quot;] = labels
    return model_inputs

def get_proofnet_dataset(tokenizer, preprocess_function=preprocess_function_proofnet_simple):
    dataset_val = load_dataset(&quot;hoskinson-center/proofnet&quot;, split='validation')
    dataset_test = load_dataset(&quot;hoskinson-center/proofnet&quot;, split='test')
    val_dataset = dataset_val.map(lambda examples: preprocess_function(examples, tokenizer), batched=True, remove_columns=[&quot;nl_statement&quot;, &quot;formal_statement&quot;])
    test_dataset = dataset_test.map(lambda examples: preprocess_function(examples, tokenizer), batched=True, remove_columns=[&quot;nl_statement&quot;, &quot;formal_statement&quot;])
    return val_dataset, test_dataset

print()

# export PYTORCH_ENABLE_MPS_FALLBACK=1
os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'

# Load Hugging Face token from file
with open(Path(&quot;~/keys/hf_file_key.txt&quot;).expanduser(), &quot;r&quot;) as file:
    hf_token = file.read().strip()

# Set the Hugging Face token as an environment variable
os.environ[&quot;HF_TOKEN&quot;] = hf_token

# Login using the token
from huggingface_hub import login
login(token=os.getenv(&quot;HF_TOKEN&quot;))

# Load model and tokenizer
pretrained_model_name_or_path = &quot;openai-community/gpt2&quot;
if 'gpt2' in pretrained_model_name_or_path:
    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path)
    if tokenizer.pad_token_id is None:
        tokenizer.pad_token = tokenizer.eos_token
        print(f'{tokenizer.pad_token=}')
    print(f'{tokenizer.eos_token=}\n{tokenizer.eos_token_id=}')
    model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path)
    # device = torch.device(f&quot;cuda:{0}&quot; if torch.cuda.is_available() else (&quot;mps&quot; if torch.backends.mps.is_available() else &quot;cpu&quot;))
    # device = torch.device(f&quot;cuda:{0}&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
    device = torch.device('cpu')
    print(f'{device=}')
    model = model.to(device)
    max_length: int = tokenizer.model_max_length
    print(f'{max_length=}')

# Define training arguments with memory optimization tricks
training_args = TrainingArguments(
    output_dir=&quot;~/tmp/results&quot;,  # Output directory for saving model checkpoints
    per_device_train_batch_size=1,  # Training batch size per device
    per_device_eval_batch_size=1,  # Evaluation batch size per device
    max_steps=2,  # Total number of training steps
    logging_dir='~/tmp/logs',  # Directory for storing logs
    logging_steps=10,  # Frequency of logging steps
    gradient_accumulation_steps=1,  # Accumulate gradients to simulate a larger batch size
    save_steps=500,  # Save checkpoint every 500 steps
    save_total_limit=3,  # Only keep the last 3 checkpoints
    evaluation_strategy=&quot;steps&quot;,  # Evaluate model at specified steps
    eval_steps=100,  # Evaluate every 100 steps
    gradient_checkpointing=True,  # Enable gradient checkpointing to save memory
    optim=&quot;paged_adamw_32bit&quot;,  # Optimizer choice with memory optimization
    learning_rate=1e-5,  # Learning rate for training
    warmup_ratio=0.01,  # Warmup ratio for learning rate schedule
    weight_decay=0.01,  # Weight decay for regularization
    lr_scheduler_type='cosine',  # Learning rate scheduler type
    report_to=&quot;none&quot;,  # Disable reporting to external tracking tools
    # bf16=torch.cuda.is_bf16_supported(),  # Use BF16 if supported by the hardware
    half_precision_backend=&quot;auto&quot;,  # Automatically select the best backend for mixed precision
    # dataloader_num_workers=4,  # TODO Number of subprocesses for data loading
    # dataloader_pin_memory=True,  # TODO periphery, Pin memory in data loaders for faster transfer to GPU
    # skip_memory_metrics=True,  # Skip memory metrics to save memory
    # dataloader_prefetch_factor=2,  # TODO periphery, Number of batches to prefetch
    # torchdynamo=&quot;nvfuser&quot;,  # TODO periphery, Use NVFuser backend for optimized torch operations
    full_determinism=True,  # TODO periphery, Ensure reproducibility
    use_cpu=True,
)

train_dataset, test_dataset = get_proofnet_dataset(tokenizer)

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    # eval_dataset=eval_dataset,
)

# Start training
print(f'\n-- Start training')
trainer.train()

# Save the model and tokenizer
trainer.save_model(output_dir=&quot;~/tmp/results&quot;)
tokenizer.save_pretrained(output_dir=&quot;~/tmp/results&quot;)
</code></pre>
<p>but no matter what I do e.g.,</p>
<ol>
<li>I've forced every possible way I can to have cpu enabled to force it to train at all</li>
<li>use a HF dataset from the internet I've used before</li>
<li>updated pytorch <code>pip install --upgrade torch</code></li>
<li>Disabled MPS</li>
<li>tried making sure cpu was used</li>
<li>training_args = TrainingArguments(
...
no_cuda=True,
use_mps_device=True if torch.backends.mps.is_available() else False,
...
)</li>
<li>&quot; 1. Verify data types: Ensure that your model and data are using compatible data types. MPS might have issues with certain data types.&quot; but it's obvious it should work cuz the HF trainer does this on it's own by fetching the device from my model. I've checked this code before.</li>
<li>yes I did <code>device = torch.device(&quot;cpu&quot;)</code></li>
</ol>
<p>but it doesn't work and I get a very cryptic error I've never seen before and nothing on google shows up:</p>
<pre><code>Exception has occurred: AttributeError
'NoneType' object has no attribute 'cget_managed_ptr'
  File &quot;/Users/me/py_proj/py_src/train/hf_trainer_train.py&quot;, line 93, in &lt;module&gt;
    trainer.train()
AttributeError: 'NoneType' object has no attribute 'cget_managed_ptr'
</code></pre>
<p>what is going on? How do I debug this?</p>
<hr />
<p>Related to this issue I also have this odd warning, wonder if it's related:</p>
<pre><code>'NoneType' object has no attribute 'cadam32bit_grad_fp32'
</code></pre>
<hr />
<p>My conda env (locally, in server I'm using venv):</p>
<pre><code>% pip list
Package                 Version     Editable project location
----------------------- ----------- ------------------------------
absl-py                 2.1.0
accelerate              0.32.1
my_proj                 0.0.1       /Users/me/my_proj/py_src
aiohttp                 3.9.5
aiosignal               1.3.1
alembic                 1.13.2
annotated-types         0.7.0
anthropic               0.31.1
anthropic-bedrock       0.8.0
anyio                   4.4.0
attrs                   23.2.0
backoff                 2.2.1
backports.tarfile       1.2.0
bitsandbytes            0.42.0
boto3                   1.34.145
botocore                1.34.145
certifi                 2024.7.4
charset-normalizer      3.3.2
click                   8.1.7
colorlog                6.8.2
contourpy               1.2.1
cycler                  0.12.1
datasets                2.20.0
dill                    0.3.8
distro                  1.9.0
docker-pycreds          0.4.0
docutils                0.21.2
dspy-ai                 2.4.12
evaluate                0.4.2
filelock                3.15.4
fire                    0.6.0
fonttools               4.53.1
frozenlist              1.4.1
fsspec                  2024.5.0
gitdb                   4.0.11
GitPython               3.1.43
greenlet                3.0.3
grpcio                  1.64.1
h11                     0.14.0
httpcore                1.0.5
httpx                   0.27.0
huggingface-hub         0.23.4
idna                    3.7
importlib_metadata      8.0.0
jaraco.classes          3.4.0
jaraco.context          5.3.0
jaraco.functools        4.0.1
Jinja2                  3.1.4
jiter                   0.5.0
jmespath                1.0.1
joblib                  1.3.2
jsonlines               4.0.0
keyring                 25.2.1
kiwisolver              1.4.5
lark-parser             0.12.0
Mako                    1.3.5
Markdown                3.6
markdown-it-py          3.0.0
MarkupSafe              2.1.5
matplotlib              3.9.1
mdurl                   0.1.2
more-itertools          10.3.0
mpmath                  1.3.0
multidict               6.0.5
multiprocess            0.70.16
networkx                3.3
nh3                     0.2.18
nltk                    3.8.1
numpy                   1.26.4
nvidia-htop             1.2.0
openai                  1.35.13
optuna                  3.6.1
packaging               24.1
pandas                  2.2.2
pillow                  10.4.0
pip                     24.0
pkginfo                 1.10.0
platformdirs            4.2.2
plotly                  5.22.0
protobuf                4.25.3
psutil                  6.0.0
pyarrow                 16.1.0
pyarrow-hotfix          0.6
pydantic                2.8.2
pydantic_core           2.20.1
Pygments                2.18.0
pyparsing               3.1.2
python-dateutil         2.9.0.post0
pytz                    2024.1
PyYAML                  6.0.1
readme_renderer         44.0
regex                   2024.5.15
requests                2.32.3
requests-toolbelt       1.0.0
rfc3986                 2.0.0
rich                    13.7.1
s3transfer              0.10.2
safetensors             0.4.3
scikit-learn            1.5.1
scipy                   1.14.0
seaborn                 0.13.2
sentry-sdk              2.10.0
setproctitle            1.3.3
setuptools              69.5.1
six                     1.16.0
smmap                   5.0.1
sniffio                 1.3.1
SQLAlchemy              2.0.31
structlog               24.2.0
sympy                   1.13.0
tenacity                8.5.0
tensorboard             2.17.0
tensorboard-data-server 0.7.2
termcolor               2.4.0
threadpoolctl           3.5.0
tokenizers              0.19.1
torch                   2.2.2
tqdm                    4.66.4
transformers            4.42.4
twine                   5.1.1
typing_extensions       4.12.2
tzdata                  2024.1
ujson                   5.10.0
urllib3                 2.2.2
wandb                   0.17.4
Werkzeug                3.0.3
wheel                   0.43.0
xxhash                  3.4.1
yarl                    1.9.4
zipp                    3.19.2
</code></pre>
<hr />
<p>refs:</p>
<ul>
<li><a href=""https://discuss.huggingface.co/t/what-does-the-attributeerror-nonetype-object-has-no-attribute-cget-managed-ptr-mean/100674"" rel=""nofollow noreferrer"">https://discuss.huggingface.co/t/what-does-the-attributeerror-nonetype-object-has-no-attribute-cget-managed-ptr-mean/100674</a></li>
<li><a href=""https://discord.com/channels/879548962464493619/1269175711420125215"" rel=""nofollow noreferrer"">https://discord.com/channels/879548962464493619/1269175711420125215</a></li>
<li><a href=""https://stackoverflow.com/questions/78827974/what-does-the-attributeerror-nonetype-object-has-no-attribute-cget-managed"">What does the &quot;AttributeError: &#39;NoneType&#39; object has no attribute &#39;cget_managed_ptr&#39;&quot; mean?</a></li>
</ul>
","huggingface"
"78826381","Why Langchain HuggingFaceEmbeddings model dimension is not the same as stated on HuggingFace","2024-08-02 16:12:09","78833363","0","40","<python><langchain><large-language-model><huggingface>","<p>I was using langchain HuggingFaceEmbeddings model: dunzhang/stella_en_1.5B_v5.
When I look at <a href=""https://huggingface.co/spaces/mteb/leaderboard"" rel=""nofollow noreferrer"">https://huggingface.co/spaces/mteb/leaderboard</a>, I can see that the model is 8192.
But when I do</p>
<pre><code>len(embed_model.embed_query(&quot;hey you&quot;))
</code></pre>
<p>It gives me 1024.
Why this difference please ?</p>
","huggingface"
"78826085","Elasticsearch RequestError(400) 'search_phase_execution_exception'","2024-08-02 14:55:23","","0","21","<python><elasticsearch><huggingface><text-embedding-ada-002>","<p>I have a simple python app that uses Elasticsearch to store documents for Pokemon using this mapping:</p>
<pre><code>{


&quot;mappings&quot;: {
    &quot;properties&quot;: {
      &quot;id&quot;: {
        &quot;type&quot;: &quot;integer&quot;
      },
      &quot;name&quot;: {
        &quot;type&quot;: &quot;object&quot;
      },
      &quot;type&quot;: {
        &quot;type&quot;: &quot;keyword&quot;
      },
      &quot;base&quot;: {
        &quot;type&quot;: &quot;object&quot;
      },
      &quot;species&quot;: {
        &quot;type&quot;: &quot;text&quot;
      },
      &quot;description&quot;: {
        &quot;type&quot;: &quot;text&quot;
      },
      &quot;evolution&quot;: {
        &quot;type&quot;: &quot;object&quot;
      },
      &quot;profile&quot;: {
        &quot;type&quot;: &quot;nested&quot;,
        &quot;properties&quot;: {
          &quot;ability&quot;: {
            &quot;type&quot;: &quot;keyword&quot;  
          }
        }
      },
      &quot;image&quot;: {
        &quot;type&quot;: &quot;object&quot;
      },
      &quot;embedding&quot;: {  
        &quot;type&quot;: &quot;dense_vector&quot;,
        &quot;dims&quot;: 384
      }
    }
  }
}
</code></pre>
<p>When I query by a property for example name in english from my python code:</p>
<pre><code>term_query = {
    &quot;size&quot;: 5,
    &quot;query&quot;: {
        &quot;term&quot;: {
            &quot;name.english&quot;: &quot;Pikachu&quot;
        }
    }
}

response = es_client.search(index=&quot;pokemon&quot;, body=term_query)
print(response)
</code></pre>
<p>I do get back the document for Pikachu. This is the <code>response</code>:</p>
<pre><code>{'took': 16, 'timed_out': False, '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0}, 'hits': {'total': {'value': 1, 'relation': 'eq'}, 'max_score': 2.5902672, 'hits': [{'_index': 'pokemon', '_type': '_doc', '_id': 'uTSUE5EB_ikN9OuVrCYz', '_score': 2.5902672, '_source': {'id': 25, 'name': {'english': 'Pikachu', 'japanese': 'ピカチュウ', 'chinese': '皮卡丘', 'french': 'Pikachu'}, 'type': ['Electric'], 'base': {'HP': 35, 'Attack': 55, 'Defense': 40, 'Sp. Attack': 50, 'Sp. Defense': 50, 'Speed': 90}, 'species': 'Mouse Pokémon', 'description': 'While sleeping, it generates electricity in the sacs in its cheeks. If it’s not getting enough sleep, it will be able to use only weak electricity.', 'evolution': {'prev': ['172', 'high Friendship'], 'next': [['26', 'use Thunder Stone']]}, 'profile': {'height': '0.4 m', 'weight': '6 kg', 'egg': ['Field', 'Fairy'], 'ability': [['Static', 'false'], ['Lightning Rod', 'true']], 'gender': '50:50'}, 'image': {'sprite': 'https://raw.githubusercontent.com/Purukitto/pokemon-data.json/master/images/pokedex/sprites/025.png', 'thumbnail': 'https://raw.githubusercontent.com/Purukitto/pokemon-data.json/master/images/pokedex/thumbnails/025.png', 'hires': 'https://raw.githubusercontent.com/Purukitto/pokemon-data.json/master/images/pokedex/hires/025.png'}, 'embedding': [-0.08073285967111588, 0.06939519941806793, 0.12187427282333374, -0.004082795698195696, -0.12248878180980682, -0.20900841057300568, 0.24315859377384186, 0.17423027753829956, 0.16088330745697021, -0.11973902583122253, 0.219094380736351, -0.1787237524986267, 0.14656412601470947, 0.26914048194885254, -0.0900934562087059, -0.02184007130563259, 0.08033201843500137, 0.22495900094509125, -0.2534410059452057, 0.041292887181043625, 0.06425879895687103, -0.23064647614955902, 0.13032366335391998, -0.18455955386161804, -0.27389195561408997, -0.039019424468278885, 0.35004937648773193, -0.015063440427184105, -0.046075183898210526, 0.06129240244626999, -0.2043248414993286, 0.06172535941004753, 0.04312220215797424, 0.08780308812856674, 0.13448253273963928, -0.05551649630069733, -0.1422523856163025, -0.027074327692389488, 0.04502112418413162, -0.1769474595785141, -0.008200257085263729, -0.25519609451293945, 0.13449157774448395, 0.2700392007827759, 0.006515479180961847, 0.5337464809417725, 0.1946619153022766, -0.3672884404659271, -0.12406620383262634, -0.09569848328828812, -0.007241903804242611, 0.3416319191455841, -0.05228937789797783, 0.036399926990270615, -0.22220291197299957, -0.061945606023073196, 0.11898031830787659, -0.18378591537475586, -0.041961733251810074, -0.047705043107271194, -0.17708761990070343, 0.16963569819927216, 0.3308679163455963, 0.04163277521729469, 0.04719669744372368, 0.05068834125995636, -0.26456910371780396, 0.026870373636484146, -0.10195782780647278, 0.23298844695091248, 0.031212573871016502, -0.3020530343055725, 0.1332738697528839, -0.29328441619873047, -0.09723328799009323, 0.12734854221343994, -0.06871715933084488, -0.09589243680238724, -0.0217132605612278, -0.04689288139343262, 0.11232715100049973, -0.1212148666381836, -0.502180814743042, 0.3030500113964081, 0.23431822657585144, 0.20775146782398224, 0.03362984582781792, -0.3450615406036377, -0.38474512100219727, -0.015262553468346596, -0.012009349651634693, 0.5406750440597534, -0.11293786019086838, 0.19005414843559265, -0.15757060050964355, -0.1948317289352417, 0.09620492905378342, -0.2629433870315552, -0.8698116540908813, -0.1905592530965805, 0.007942819967865944, 0.5021940469741821, -0.22358925640583038, 0.28493547439575195, 0.2063635140657425, -0.09802521765232086, -0.12083346396684647, -0.2472897320985794, 0.11121658980846405, 0.2569717764854431, 0.14332978427410126, -0.4158305823802948, -0.21560056507587433, 0.47279030084609985, 0.05417272448539734, 0.15705130994319916, 0.11930802464485168, 0.2766338884830475, -0.16857114434242249, 0.38867419958114624, 0.07035291939973831, -0.19579669833183289, -0.20194637775421143, -0.045491937547922134, 0.22118116915225983, -0.4455321729183197, -0.14013874530792236, 0.08595887571573257, 0.07313554733991623, 0.21260005235671997, 0.011720014736056328, -0.11162177473306656, -0.31165409088134766, -0.18956924974918365, -0.08518164604902267, 0.3265906274318695, 0.3938029110431671, 0.16317987442016602, -0.24403564631938934, 0.2091466188430786, 0.15612895786762238, 0.5397796034812927, -0.021354854106903076, -0.2237994521856308, 0.15890972316265106, -0.1837628185749054, 0.07010602951049805, -0.05256254971027374, -0.06645826995372772, 0.05712897330522537, 0.0011353784939274192, -0.26107025146484375, 0.2231748253107071, 0.03408034145832062, -0.019110316410660744, -0.09659949690103531, -0.20144665241241455, 0.22397348284721375, 0.1376374363899231, 0.09118841588497162, 0.05390686169266701, 0.2291051596403122, -0.22862714529037476, -0.011429731734097004, 0.26337283849716187, 0.008011268451809883, -0.2091754674911499, -0.018558669835329056, -0.3221794664859772, -0.03949500247836113, 0.06667587906122208, 0.1057417243719101, 0.09627758711576462, 0.02740870602428913, 0.13777735829353333, -0.31403109431266785, -0.35416749119758606, -0.06899789720773697, -0.21653032302856445, -0.027547530829906464, 0.10295339673757553, 0.1908693164587021, 0.06642354279756546, -0.2547055780887604, -0.27135589718818665, -0.10589489340782166, 0.07966331392526627, 0.10850854963064194, -0.1262790560722351, -0.2978314459323883, -0.23875080049037933, -0.3113715648651123, 0.20441405475139618, 0.047840893268585205, -0.12133669853210449, -0.025360196828842163, -0.18699155747890472, -0.3434793949127197, -0.011686017736792564, -0.1433863788843155, -0.02858828380703926, -0.2638932764530182, -0.20476993918418884, -0.12419438362121582, 0.028579195961356163, -0.1174812987446785, -0.33529555797576904, -0.20364123582839966, 0.04101632535457611, -0.09168056398630142, -0.05435829237103462, -0.30858591198921204, 0.25615713000297546, 0.1913250833749771, 0.6707220673561096, 0.4516240358352661, -0.10038889944553375, 0.09332328289747238, -0.08849727362394333, -0.04820533096790314, 0.3817048966884613, -0.2124391496181488, -0.20861664414405823, -0.3969747722148895, -0.26697418093681335, -0.09186507016420364, -0.17242462933063507, 0.163199320435524, -0.18881991505622864, 0.08426131308078766, -0.2372647523880005, -0.004029334522783756, 0.06960441172122955, -0.047179535031318665, -0.20344287157058716, 0.183263897895813, -0.06168253347277641, -0.04381486400961876, 0.21352356672286987, -0.29498425126075745, 0.046090275049209595, 0.016421712934970856, -0.03849317133426666, 0.2436819225549698, -0.24784734845161438, 0.06414017081260681, -0.01664029061794281, 0.18358607590198517, 0.025173719972372055, 0.6090837717056274, 0.050406910479068756, 0.1362634152173996, -0.22938694059848785, 0.3377200961112976, 0.13915757834911346, 0.23770390450954437, 0.1720094382762909, 0.030198220163583755, 0.024356193840503693, -0.28200817108154297, -0.19686788320541382, -0.1351262480020523, -0.007643698249012232, -0.22928962111473083, 0.11154638230800629, -0.014717038720846176, 0.1324407309293747, 0.46048006415367126, -0.017119301483035088, -0.4727839231491089, -0.4402349591255188, -0.01458784844726324, -0.04428641498088837, 0.04039650410413742, 0.48811277747154236, -0.3889062702655792, -0.2668595612049103, 0.05276121944189072, -0.1911555528640747, -0.11344823241233826, 0.0762748047709465, -0.19064147770404816, 0.2186267226934433, -0.23358970880508423, 0.15427519381046295, -0.13358311355113983, 0.03089122287929058, -0.26767098903656006, 0.08962049335241318, -0.13496246933937073, 0.10376256704330444, 0.26555293798446655, 0.7292829155921936, 0.12933622300624847, 0.1885850727558136, 0.33418792486190796, -0.0045865499414503574, -0.08271858841180801, -0.19287362694740295, 0.39168789982795715, 0.07085824757814407, 0.16441291570663452, 0.026745975017547607, -0.014314485713839531, -0.10071783512830734, -0.08725757896900177, 0.04012788459658623, -0.22500525414943695, 0.1916060894727707, -0.44129520654678345, -0.34983548521995544, 0.3279268145561218, 0.35589221119880676, -0.014993308112025261, -0.2724052369594574, 0.1550331711769104, -0.16982153058052063, 0.28001534938812256, -0.08957020193338394, 0.26859310269355774, -0.06395307928323746, -0.18223333358764648, -0.03468851372599602, -0.091072678565979, -0.012290200218558311, -0.28910940885543823, 0.08019937574863434, -0.2714097201824188, 0.23566178977489471, -0.15085045993328094, 0.31374698877334595, 0.030088581144809723, 0.1816730797290802, -0.13970644772052765, -0.0039431205950677395, 0.6152960658073425, 0.4371360242366791, 0.03539286553859711, -0.10140117257833481, 0.03148588910698891, -0.16396838426589966, -0.3729839324951172, -0.08252952247858047, -0.10826507955789566, -0.13000987470149994, -0.0005456415819935501, -0.11005621403455734, 0.17918984591960907, -0.3737146556377411, -0.3396584987640381, -0.08200288563966751, 0.14435864984989166, 0.5620496273040771, 0.27496692538261414, 0.004530945792794228, -0.15538428723812103, 0.380673348903656, -0.22902937233448029, 0.22183893620967865, -0.06294988840818405, 0.08632779121398926, -0.5587152242660522, -0.22411471605300903, 0.1320917010307312, -0.1736510843038559, 0.09566263109445572, 0.4774761497974396, -0.009492949582636356, -0.2615973651409149, 0.31702664494514465, -0.11396189033985138, 0.4716211259365082, 0.057476241141557693, -0.05458242818713188, -0.05082737281918526, -0.1389731615781784, 0.22226481139659882, 0.5554996728897095, -0.19391866028308868, 0.24401207268238068, 0.3081281781196594, 0.26846665143966675, 0.13681122660636902, 0.04111223667860031]}}]}}
</code></pre>
<p>I checked all the properties and they look correct. The embedding also has the right lenght <code>384</code> which is what this model I am using produces: <code>embeddings_model = SentenceTransformer(&quot;paraphrase-MiniLM-L6-v2&quot;)</code></p>
<p>However when I try to run an embedding cosine similarity query like:</p>
<pre><code>query = &quot;Pikachu&quot;
query_embedding = embeddings_model.encode([query])[0].tolist()
script_query_cosine = {
    &quot;size&quot;: 5,
    &quot;query&quot;: {
        &quot;script_score&quot;: {
            &quot;query&quot;: {
                &quot;match_all&quot;: {}
            },
            &quot;script&quot;: {
                &quot;source&quot;: &quot;cosineSimilarity(params.query_vector, 'embedding') + 1.0&quot;,
                &quot;params&quot;: {
                    &quot;query_vector&quot;: query_embedding
                }
            }
        }
    }
}

response = es_client.search(index=&quot;pokemon&quot;, body=script_query_cosine)
print(response)
</code></pre>
<p>I get this error:</p>
<p><code>RequestError(400, 'search_phase_execution_exception', 'runtime error')</code></p>
<p>I checked the embedding size and it is correct. I checked if any documents are missing the embeddings fields and I found no documents that miss it. I have no clue what this error is.</p>
","huggingface"
"78825974","Problems with using Models from Huggingface","2024-08-02 14:26:58","","-1","18","<huggingface>","<p>I'm trying to use the Model GPT 2 from Huggingface. I have tried everything but although it's compiling the Output is just wrong. I will give my Code with an easy sample question and the Output/Error:
Code. Ps: I used to GPT 2 to learn how Huggingface is working and want to use differents models in the future.:</p>
<p>Code:</p>
<p>from transformers import pipeline</p>
<p>generator = pipeline(&quot;text-generation&quot;, model=&quot;gpt2&quot;)</p>
<p>result = generator(&quot;Solve 2x + 3 = 7 &quot;, max_length=50, num_return_sequences=2)</p>
<p>for i, sequence in enumerate(result):
print(f&quot;Result {i+1}: {sequence['generated_text']}&quot;)</p>
<p>Output/Error: Result 1: solve  2x + 3 = 7</p>
<p>If 4 is large, there is a large number of ways to generate two integers at once. The following shows the various options for generating 64 bit integers:</p>
<p>Option:
Result 2: solve  2x + 3 = 7
Then, by a natural order, we can compute the total amount of money that a given person would put into their pocket as a reward</p>
<p>Example [ edit ]</p>
<p>Maybe the Problem is that GPT 2 is not able to solve it, but I dont think so. There must be some problem with that code. Would be great if someone could explain the mistake and correct.</p>
<p>Best
Julius</p>
<p>I have read through quicktour at huggingface, but still didn't find the mistake.</p>
","huggingface"
"78825011","RunnableSequence instead of LLMChain throws an error (updating from depreacated langchain)","2024-08-02 10:29:18","","0","52","<python><langchain><huggingface>","<p>When I have this code first</p>
<pre><code>from langchain_community.llms import HuggingFacePipeline
from transformers import AutoTokenizer
import transformers
import torch

model=&quot;meta-llama/Llama-2-7b-chat-hf&quot;
tokenizer=AutoTokenizer.from_pretrained(model)

pipeline=transformers.pipeline(
    &quot;text-generation&quot;,
    model=model,
    tokenizer=tokenizer,
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    device_map=&quot;auto&quot;,
    max_length=1000,
    do_sample=True,
    top_k=10,
    num_return_sequences=1,
    eos_token_id=tokenizer.eos_token_id
    )

llm=HuggingFacePipeline(pipeline=pipeline, model_kwargs={'temperature':0})

from langchain.prompts import PromptTemplate
prompt_template=PromptTemplate(input_variables=[&quot;book_name&quot;],
                               template=&quot;Provide me a concise summary of the book {book_name}&quot;)
</code></pre>
<p>and then I complete it with</p>
<pre><code>from langchain.chains import LLMChain

chain = LLMChain(llm=llm, prompt=prompt_template, verbose=True)
response= chain.run(&quot;Alchemist&quot;)
print(response)
</code></pre>
<p>I get a response with the summary I wanted , but I get deprecation warnings.
So following the warnings I try to replace the second part with</p>
<pre><code>chain = prompt | llm
response = chain.invoke(&quot;The name of the rose&quot;)
print(response)
</code></pre>
<p>but I get the error</p>
<pre><code>TypeError: Expected a Runnable, callable or dict.Instead got an unsupported type: &lt;class 'str'&gt;
</code></pre>
<p>What am I doing wrong?</p>
<p>I have used something similar but with llm being a <code>HuggingFaceEndPoint</code> and in that case it worked, so I suspect that it has to do with llm being a <code>HuggingFacePipeline</code> but can someone tell me how to correct the code?</p>
<p>Edit:
I tried</p>
<pre><code>chain2 = prompt | llm | StrOutputParser()
response2 = chain2.invoke({&quot;bookname&quot;:&quot;The name of the rose&quot;})
# response2 = chain2.invoke(&quot;The name of the rose&quot;)
print(response2)
</code></pre>
<p>but the error persists</p>
","huggingface"
"78823069","HuggingFace LLM Evaluate: RuntimeError: isin_Tensor_Tensor_out only works on floating types on MPS for pre MacOS_14_0. Received dtype: Long","2024-08-01 21:19:16","","2","61","<pytorch><nlp><apple-m1><huggingface><huggingface-evaluate>","<p><strong>Context</strong>:
I tried to create an evaluation pipeline for a text summary task using <a href=""https://github.com/huggingface/evaluate"" rel=""nofollow noreferrer"">HuggingFace evaluate</a> packages. I got the issue of receiving dtype Long for the tensor, but I did not feed any long type and the two columns specified for the evaluate pipeline are text only. Further investigation looks like the issue is rooted from torch and my version of Mac (M1). I'm not sure how to proceed with this.</p>
<p>Here is what I did:</p>
<p><strong>My code</strong>:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import pipeline
from evaluate import evaluator
from datasets import load_dataset

# Load data:
booksum = load_dataset(&quot;kmfoda/booksum&quot;, split=&quot;validation[:1000]&quot;)

# Load pipeline
pipe = pipeline(
    task=&quot;summarization&quot;,
    model=&quot;pszemraj/led-base-book-summary&quot;,
    device=&quot;mps&quot;
)

# Setup Evaluate task using Rouge
task_evaluator = evaluator(&quot;summarization&quot;)

# The code that yield issue:
eval_results = task_evaluator.compute(
    model_or_pipeline=pipe,
    data=booksum,
    metric=&quot;rouge&quot;,
    input_column=&quot;chapter&quot;,
    label_column=&quot;summary_text&quot;
)
</code></pre>
<p>This gives me the value error below:</p>
<p><strong>Short Error message:</strong></p>
<pre><code>File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/generation/logits_process.py:157, in MinLengthLogitsProcessor.__call__(self, input_ids, scores)
    154 @add_start_docstrings(LOGITS_PROCESSOR_INPUTS_DOCSTRING)
    155 def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -&gt; torch.FloatTensor:
    156     vocab_tensor = torch.arange(scores.shape[-1], device=scores.device)
--&gt; 157     eos_token_mask = torch.isin(vocab_tensor, self.eos_token_id)
    158     scores_processed = scores.clone()
    159     if input_ids.shape[-1] &lt; self.min_length:

RuntimeError: isin_Tensor_Tensor_out only works on floating types on MPS for pre MacOS_14_0. Received dtype: Long
</code></pre>
<p><strong>Full Error message:</strong></p>
<pre><code>---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
Cell In[10], line 1
----&gt; 1 eval_results = task_evaluator.compute(
      2     model_or_pipeline=pipe,
      3     data=booksum,
      4     metric=&quot;rouge&quot;,
      6     input_column=&quot;chapter&quot;,
      7     label_column=&quot;summary_text&quot;
      8 )

File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/evaluate/evaluator/text2text_generation.py:191, in SummarizationEvaluator.compute(self, model_or_pipeline, data, subset, split, metric, tokenizer, strategy, confidence_level, n_resamples, device, random_state, input_column, label_column, generation_kwargs)
    166 @add_start_docstrings(
    167     EVALUTOR_COMPUTE_START_DOCSTRING,
    168     TASK_DOCUMENTATION_KWARGS,
   (...)
    189     generation_kwargs: dict = None,
    190 ) -&gt; Tuple[Dict[str, float], Any]:
--&gt; 191     result = super().compute(
    192         model_or_pipeline=model_or_pipeline,
    193         data=data,
    194         subset=subset,
    195         split=split,
    196         metric=metric,
    197         tokenizer=tokenizer,
    198         strategy=strategy,
    199         confidence_level=confidence_level,
    200         n_resamples=n_resamples,
    201         device=device,
    202         random_state=random_state,
    203         input_column=input_column,
    204         label_column=label_column,
    205         generation_kwargs=generation_kwargs,
    206     )
    208     return result

File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/evaluate/evaluator/text2text_generation.py:133, in Text2TextGenerationEvaluator.compute(self, model_or_pipeline, data, subset, split, metric, tokenizer, strategy, confidence_level, n_resamples, device, random_state, input_column, label_column, generation_kwargs)
    130 if generation_kwargs is not None:
    131     self.PIPELINE_KWARGS.update(generation_kwargs)
--&gt; 133 result = super().compute(
    134     model_or_pipeline=model_or_pipeline,
    135     data=data,
    136     subset=subset,
    137     split=split,
    138     metric=metric,
    139     tokenizer=tokenizer,
    140     strategy=strategy,
    141     confidence_level=confidence_level,
    142     n_resamples=n_resamples,
    143     device=device,
    144     random_state=random_state,
    145     input_column=input_column,
    146     label_column=label_column,
    147 )
    149 return result

File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/evaluate/evaluator/base.py:255, in Evaluator.compute(self, model_or_pipeline, data, subset, split, metric, tokenizer, feature_extractor, strategy, confidence_level, n_resamples, device, random_state, input_column, label_column, label_mapping)
    252 metric = self.prepare_metric(metric)
    254 # Compute predictions
--&gt; 255 predictions, perf_results = self.call_pipeline(pipe, pipe_inputs)
    256 predictions = self.predictions_processor(predictions, label_mapping)
    258 metric_inputs.update(predictions)

File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/evaluate/evaluator/base.py:513, in Evaluator.call_pipeline(self, pipe, *args, **kwargs)
    511 def call_pipeline(self, pipe, *args, **kwargs):
    512     start_time = perf_counter()
--&gt; 513     pipe_output = pipe(*args, **kwargs, **self.PIPELINE_KWARGS)
    514     end_time = perf_counter()
    515     return pipe_output, self._compute_time_perf(start_time, end_time, len(pipe_output))

File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/pipelines/text2text_generation.py:269, in SummarizationPipeline.__call__(self, *args, **kwargs)
    245 def __call__(self, *args, **kwargs):
    246     r&quot;&quot;&quot;
    247     Summarize the text(s) given as inputs.
    248 
   (...)
    267           ids of the summary.
    268     &quot;&quot;&quot;
--&gt; 269     return super().__call__(*args, **kwargs)

File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/pipelines/text2text_generation.py:167, in Text2TextGenerationPipeline.__call__(self, *args, **kwargs)
    138 def __call__(self, *args, **kwargs):
    139     r&quot;&quot;&quot;
    140     Generate the output text(s) using text(s) given as inputs.
    141 
   (...)
    164           ids of the generated text.
    165     &quot;&quot;&quot;
--&gt; 167     result = super().__call__(*args, **kwargs)
    168     if (
    169         isinstance(args[0], list)
    170         and all(isinstance(el, str) for el in args[0])
    171         and all(len(res) == 1 for res in result)
    172     ):
    173         return [res[0] for res in result]

File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/pipelines/base.py:1235, in Pipeline.__call__(self, inputs, num_workers, batch_size, *args, **kwargs)
   1231 if can_use_iterator:
   1232     final_iterator = self.get_iterator(
   1233         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params
   1234     )
-&gt; 1235     outputs = list(final_iterator)
   1236     return outputs
   1237 else:

File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/pipelines/pt_utils.py:124, in PipelineIterator.__next__(self)
    121     return self.loader_batch_item()
    123 # We're out of items within a batch
--&gt; 124 item = next(self.iterator)
    125 processed = self.infer(item, **self.params)
    126 # We now have a batch of &quot;inferred things&quot;.

File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/pipelines/pt_utils.py:125, in PipelineIterator.__next__(self)
    123 # We're out of items within a batch
    124 item = next(self.iterator)
--&gt; 125 processed = self.infer(item, **self.params)
    126 # We now have a batch of &quot;inferred things&quot;.
    127 if self.loader_batch_size is not None:
    128     # Try to infer the size of the batch

File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/pipelines/base.py:1161, in Pipeline.forward(self, model_inputs, **forward_params)
   1159     with inference_context():
   1160         model_inputs = self._ensure_tensor_on_device(model_inputs, device=self.device)
-&gt; 1161         model_outputs = self._forward(model_inputs, **forward_params)
   1162         model_outputs = self._ensure_tensor_on_device(model_outputs, device=torch.device(&quot;cpu&quot;))
   1163 else:

File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/pipelines/text2text_generation.py:191, in Text2TextGenerationPipeline._forward(self, model_inputs, **generate_kwargs)
    184     in_b, input_length = tf.shape(model_inputs[&quot;input_ids&quot;]).numpy()
    186 self.check_inputs(
    187     input_length,
    188     generate_kwargs.get(&quot;min_length&quot;, self.model.config.min_length),
    189     generate_kwargs.get(&quot;max_length&quot;, self.model.config.max_length),
    190 )
--&gt; 191 output_ids = self.model.generate(**model_inputs, **generate_kwargs)
    192 out_b = output_ids.shape[0]
    193 if self.framework == &quot;pt&quot;:

File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/torch/utils/_contextlib.py:116, in context_decorator.&lt;locals&gt;.decorate_context(*args, **kwargs)
    113 @functools.wraps(func)
    114 def decorate_context(*args, **kwargs):
    115     with ctx_factory():
--&gt; 116         return func(*args, **kwargs)

File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/generation/utils.py:2028, in GenerationMixin.generate(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)
   2020     input_ids, model_kwargs = self._expand_inputs_for_generation(
   2021         input_ids=input_ids,
   2022         expand_size=generation_config.num_beams,
   2023         is_encoder_decoder=self.config.is_encoder_decoder,
   2024         **model_kwargs,
   2025     )
   2027     # 14. run beam sample
-&gt; 2028     result = self._beam_search(
   2029         input_ids,
   2030         beam_scorer,
   2031         logits_processor=prepared_logits_processor,
   2032         logits_warper=prepared_logits_warper,
   2033         stopping_criteria=prepared_stopping_criteria,
   2034         generation_config=generation_config,
   2035         synced_gpus=synced_gpus,
   2036         **model_kwargs,
   2037     )
   2039 elif generation_mode == GenerationMode.GROUP_BEAM_SEARCH:
   2040     # 11. prepare beam search scorer
   2041     beam_scorer = BeamSearchScorer(
   2042         batch_size=batch_size,
   2043         num_beams=generation_config.num_beams,
   (...)
   2049         max_length=generation_config.max_length,
   2050     )

File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/generation/utils.py:3200, in GenerationMixin._beam_search(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, logits_warper, **model_kwargs)
   3195 next_token_logits = outputs.logits[:, -1, :].clone()
   3196 next_token_scores = nn.functional.log_softmax(
   3197     next_token_logits, dim=-1
   3198 )  # (batch_size * num_beams, vocab_size)
-&gt; 3200 next_token_scores_processed = logits_processor(input_ids, next_token_scores)
   3201 if do_sample:
   3202     next_token_scores_processed = logits_warper(input_ids, next_token_scores_processed)

File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/generation/logits_process.py:98, in LogitsProcessorList.__call__(self, input_ids, scores, **kwargs)
     96         scores = processor(input_ids, scores, **kwargs)
     97     else:
---&gt; 98         scores = processor(input_ids, scores)
    100 return scores

File ~/.pyenv/versions/3.12.0/envs/llm-aug/lib/python3.12/site-packages/transformers/generation/logits_process.py:157, in MinLengthLogitsProcessor.__call__(self, input_ids, scores)
    154 @add_start_docstrings(LOGITS_PROCESSOR_INPUTS_DOCSTRING)
    155 def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -&gt; torch.FloatTensor:
    156     vocab_tensor = torch.arange(scores.shape[-1], device=scores.device)
--&gt; 157     eos_token_mask = torch.isin(vocab_tensor, self.eos_token_id)
    158     scores_processed = scores.clone()
    159     if input_ids.shape[-1] &lt; self.min_length:

RuntimeError: isin_Tensor_Tensor_out only works on floating types on MPS for pre MacOS_14_0. Received dtype: Long
</code></pre>
<p><strong>Notes</strong>: I did try to add <code>device=&quot;mps&quot;</code> to the <code>task_evaluator.compute</code> but it gave me another error of <code>ValueError: This pipeline was instantiated on device None but device=mps was passed to 'compute'.</code></p>
","huggingface"
"78821977","How to Enforce Context-Only Responses in a Conversational Retrieval Chain with LangChain and OpenAI GPT-4?","2024-08-01 16:09:31","","0","14","<openai-api><langchain><huggingface><chromadb><gpt-4>","<p>I'm working on a Django project where I need to create a conversational retrieval system using LangChain with OpenAI's GPT-4 model. The goal is to ensure that the model only answers questions based on the provided document context. If the relevant information is not in the context, the model should respond with a message indicating the lack of access to relevant files.</p>
<p>here is a simplified version of my setup</p>
<pre><code>from langchain.chains import ConversationalRetrievalChain
from langchain_community.vectorstores import Chroma
from langchain_huggingface.embeddings import HuggingFaceEmbeddings
from langchain_community.chat_models import ChatOpenAI
from langchain.schema import Document

embeddings = HuggingFaceEmbeddings(model_name=&quot;sentence-transformers/all-MiniLM-L6-v2&quot;)
chroma_db = Chroma(embeddings=embeddings, collection_name=&quot;AEM_Responses&quot;)

def handle_query_flow(chroma_db, email):
    system_prompt = &quot;&quot;&quot;You are an AI system that answers questions based on documents. 
                    Answer only using the provided context. If the information is not in the context, 
                    respond with: 'User does not have access to any relevant files. 
                    Please provide more context or check your access permissions.' 
                    Do not search outside the given context. Do not use any outside knowledge or 
                    provide an answer from your own training data. Only use the provided context.
                    If you get empty context then just say that this user does not have access to any files.
                    &quot;&quot;&quot;

    chain = ConversationalRetrievalChain.from_llm(
        ChatOpenAI(temperature=0.0, model_name='gpt-4-turbo'),
        chroma_db.as_retriever(search_kwargs={'filter': {'email': email}}),
        memory=None
    )

    chat_history = []

    while True:
        query = input(&quot;Please enter your query (type 'quit' to exit): &quot;).strip()
        if query.lower() == 'quit':
            break

        retriever = chroma_db.as_retriever(search_kwargs={'filter': {'email': email}})
        retrieved_docs = retriever.get_relevant_documents(query)
        count_retrieved_docs = len(retrieved_docs)
        
        response = chain.invoke({'question': query, 'system_prompt': system_prompt, 'chat_history': chat_history})
        answer = response.get('answer', '')
        print(f&quot;Answer: {answer}&quot;)
        print(&quot;CONTEXT DOCUMENTS&quot;)
        print(retrieved_docs)

    return JsonResponse({&quot;message&quot;: &quot;Query session ended&quot;}, status=200) ```





Problem:
Despite setting up a system prompt and the ConversationalRetrievalChain, the model sometimes provides answers based on general knowledge instead of strictly using the provided context. For instance, when a user with no relevant documents in context asks a question, the model still responds with an answer rather than indicating the absence of relevant documents.

What I've Tried:

System Prompt: I've clearly stated that the model should not use outside knowledge and should only answer based on the provided context.
Checking Retrieved Documents: Before generating a response, I check if any meaningful documents are retrieved.
Questions:

How can I ensure that the model strictly adheres to the provided context and does not use any outside knowledge?
Is there a way to programmatically enforce this restriction, ensuring that if no relevant context is found, the model responds with a predefined message?
Are there any best practices or specific configurations in LangChain or with OpenAI models that I might be missing?
Any insights or suggestions would be greatly appreciated!
</code></pre>
","huggingface"
"78820465","How can we implement LIM models from huggingface using langchain_huggingface library?","2024-08-01 10:36:24","","0","18","<langchain><huggingface>","<p>Langchain_Huggingface released recently in May 2024.I implemented some open source LLM using the same library (langchain_huggingface). That worked well. But now I am trying to generate images but couldn't figure it out.</p>
<p>&quot;lim = HuggingFaceEndpoint(repo_id=&quot;  &quot;,task=&quot;text-to-image&quot;,token= key)&quot;</p>
<p>I tried to make an endpoint but didn't work.</p>
","huggingface"
"78817853","Why loading llama2 takes about 30 GB of GPU RAM?","2024-07-31 18:20:53","","0","22","<huggingface-transformers><huggingface><llama>","<p>I am trying to finetune llama2 on a self-created dataset. The text in the dataset is quite long, so I used rope_scaling=8 for llama initialization. I also used 4-bit and qlora to save memory. However, in the end, I found that after I created the model and tokenizer, 30 GB of the GPU RAM had already been used and I cannot train with even batch=1. I tried to load some published models on huggingface and it turns out to be fine, they will only consume up to 10 GB of RAM as expected.</p>
<p>The code that takes 30 GB of memory is the following:</p>
<pre><code>
model_config = LlamaConfig(max_position_embeddings=4096, 
                            rope_scaling={&quot;type&quot;: &quot;linear&quot;, 
                                          &quot;factor&quot;: 8.0},
                            use_cache=False)
model = LlamaForCausalLM.from_pretrained(&quot;meta-llama/Llama-2-7b-hf&quot;, config=model_config, device_map=&quot;auto&quot;, trust_remote_code=True)
</code></pre>
<p>And I checked the RAM situation after running such code:
CPU RAM Free: 1.0 TB, GPU 0 ... Mem Free: 19319MB / 46068MB | Utilization 57%</p>
","huggingface"
"78814811","SSLError: HTTPSConnectionPool(host='huggingface.co', port=443)","2024-07-31 06:54:51","","0","79","<python><ssl><huggingface>","<p>I have the following script:</p>
<p><strong>test.py</strong>:</p>
<pre><code>from transformers import AutoTokenizer

checkpoint = &quot;kevinscaria/joint_tk-instruct-base-def-pos-neg-neut-combined&quot;
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
</code></pre>
<p>When I run <code>python test.py</code> I get the following error:</p>
<blockquote>
<p>requests.exceptions.SSLError:
(MaxRetryError(&quot;HTTPSConnectionPool(host='huggingface.co', port=443):
Max retries exceeded with url:
/kevinscaria/joint_tk-instruct-base-def-pos-neg-neut-combined/resolve/main/tokenizer_config.json
(Caused by SSLError(SSLCertVerificationError(1, '[SSL:
CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed
certificate in certificate chain (_ssl.c:1000)')))&quot;), '(Request ID:
2d2eb919-ddf8-4e5d-b0fc-b0ab1f666251)')</p>
</blockquote>
<p>This is despite I already added <code>huggingface.co</code> to my pip configuration file as a trusted host:</p>
<pre><code>&gt;pip config list
global.trusted-host='pypi.org files.pythonhosted.org pypi.python.org huggingface.co'
</code></pre>
<p>Note that I'm working on a different computer and I'm constantly having this kind of SSL errors which I never had on my previous computer.</p>
","huggingface"
"78812089","Received server error (500) while deploying HuggingFace model on Sgaemaker","2024-07-30 13:45:43","78835313","0","27","<huggingface-transformers><amazon-sagemaker><endpoint><huggingface><amazon-sagemaker-studio>","<p>I've successfully fine tuned a <code>sentence-transformers</code> model <code>all-MiniLM-L12-v2</code> on our data in SageMaker Studio and the model was saved in S3 as a <code>model.tar.gaz</code>.</p>
<p>I want to deploy this model for inference (all code snippets included below). According to <a href=""https://github.com/huggingface/notebooks/blob/main/sagemaker/17_custom_inference_script/sagemaker-notebook.ipynb"" rel=""nofollow noreferrer"">HuggingFace doc</a> these types of model required a Custom Inference module. So I've downloaded and unpacked the <code>model.tar.gz</code> created, then followed the tutorial to add the <code>code/inference.py</code> and pushed it back to S3 as new <code>model.tar.gz</code></p>
<p>The endpoint is created successfully, but as soon as I call the <code>predictor.predict()</code> it crashes with the following error:</p>
<pre><code>ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from primary with message &quot;{
  &quot;code&quot;: 500,
  &quot;type&quot;: &quot;InternalServerException&quot;,
  &quot;message&quot;: &quot;Worker died.&quot;
}
</code></pre>
<p>looking in CloudWatch I got a lot of info messages, where the instance seems to be setting up successfully then I get this warning message:</p>
<pre><code>2024-07-30T13:19:09,702 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.
</code></pre>
<p>Here are the relevant code snippets:</p>
<p>End point creation:</p>
<pre><code>from sagemaker.huggingface.model import HuggingFaceModel
from sagemaker import get_execution_role, image_uris

role            = get_execution_role()
estimator_image = image_uris.retrieve(framework='pytorch',region='eu-west-1',version='2.0.0',py_version='py310',image_scope='inference', instance_type='ml.g5.4xlarge')
sm_model_ref    = model_path

# create Hugging Face Model Class
huggingface_model = HuggingFaceModel(
    model_data    = sm_model_ref,
    role          = role,                                                     
    image_uri     = estimator_image,
)
</code></pre>
<p>The custom <code>inference.py</code> file and its location in the <code>model.tar.gz</code>:</p>
<pre><code>%%writefile models/model/code/inference.py

import torch

# Create a custom inference to overwrite the default method
def predict_fn(data, model):

    # create sentences pair
    sentences1 = data[&quot;premise&quot;]
    sentences2 = data[&quot;hypothesis&quot;]
 
    # Compute token embeddings
    with torch.no_grad():
        embeddings1 = model.encode(sentences1)
        embeddings2 = model.encode(sentences2)
        
        # Compute cosine similarities        
        similarities = model.similarity(embeddings1, embeddings2)
 
    return similarities
</code></pre>
<p>And its location:</p>
<pre><code>model.tar.gz
 |_ _ 1_Pooling
 |_ _ 2_Normalize
 |_ _ checkpoint-8300
 |_ _ checkpoint-8334
 |_ _ code
   |_ _ inference.py
 |_ _ config_sentence_transformers.json
 |_ _ config.json
 |_ _ model.safetensors
 |_ _ module.json
 |_ _ README.md
 |_ _ sentence_bert_config.json
 |_ _ special_token_map.json
 |_ _ tokenizer_config.json
 |_ _ tokenizer.json
 |_ _ vocab.txt
</code></pre>
","huggingface"
"78808833","Can't load HuggingFace Embeddings llama3.1 in Llama Index","2024-07-29 19:46:54","","0","107","<python><large-language-model><huggingface>","<p>I have a very simple code like this:</p>
<pre><code>from llama_index.embeddings.huggingface import HuggingFaceEmbedding
embed_model = HuggingFaceEmbedding(model_name=&quot;meta-llama/Meta-Llama-3-8B&quot;)
</code></pre>
<p>I've seen that this model, meta-llama/Meta-Llama-3-8B, is only 4.5 GB, and I've 16GB RAM and only using 20% at most before running. But every time, I run those two lines, the memory blows up and python crashes.</p>
<p>I also tried with</p>
<pre><code>os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &quot;&quot;
</code></pre>
<p>But same.</p>
<p>Any idea why please ?</p>
","huggingface"
"78805545","Error downloading 'pyannote/speaker-diarization' pipeline despite having read access token","2024-07-29 05:47:09","","0","54","<google-colaboratory><huggingface>","<p>I'm trying to use the <code>pyannote.audio</code> library to download the &quot;pyannote/speaker-diarization&quot; pipeline, but I keep encountering the following error:</p>
<pre><code>Could not download 'pyannote/speaker-diarization' pipeline.
It might be because the pipeline is private or gated so make
sure to authenticate. Visit https://hf.co/settings/tokens to
create your access token and retry with:

   &gt;&gt;&gt; Pipeline.from_pretrained('pyannote/speaker-diarization',
   ...                          use_auth_token='hf...')
</code></pre>
<p>I have a read-type access token, but the error persists. Here’s the code I’m using:</p>
<pre class=""lang-py prettyprint-override""><code>from pyannote.audio import Pipeline

pipeline = Pipeline.from_pretrained(
    'pyannote/speaker-diarization',
    use_auth_token='YOUR_ACCESS_TOKEN'
)
</code></pre>
<p>You can find the code I'm running in this Colab notebook as shown below:</p>
<p><a href=""https://i.sstatic.net/4wd31vLj.jpg"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p><a href=""https://colab.research.google.com/drive/16fQA8tY5w3hZOIHps4ndAgru87g6tj79?usp=sharing#scrollTo=jKG14DGYbwku"" rel=""nofollow noreferrer"">Pyannote plays and Whisper rhymes v 2.1</a>.</p>
<p>Any help would be appreciated!</p>
","huggingface"
"78799871","isinstance() arg 2 must be a type, a tuple of types, or a union","2024-07-26 21:04:23","","0","96","<python><tensorflow><huggingface><fine-tuning>","<p>I'm getting an error message when trying to train my model, but some reason its giving me the same message every time I alter it.</p>
<p>Here is the code:</p>
<pre><code>
# Define training arguments
training_args = TrainArgument(
    output_dir=&quot;bert_results&quot;,
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=32,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir=&quot;bert_results/logs&quot;,
    logging_steps=10,
)



# Initialize the TFTrainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    loss_function=(&quot;categorical_crossentropy&quot;)
)

# Train the model
trainer.train()




What I am getting in return &quot;Traceback (most recent call last):
  File &quot;C:\Users\Jesh\PycharmProjects\Yelp\finetuning.py&quot;, line 44, in &lt;module&gt;
    trainer = Trainer(
              ^^^^^^^^
  File &quot;C:\Users\Jesh\PycharmProjects\Yelp\venv\Lib\site-packages\tftrainer\trainer.py&quot;, line 32, in __init__
    if isinstance(callable, loss_function)
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: isinstance() arg 2 must be a type, a tuple of types, or a union&quot; 
</code></pre>
<p>I am working on a machine learning project where I expect my fine-tuned model to be produced after running a specific script. This script involves training a model using a custom trainer class. Despite making various modifications to the trainer class to improve the training process, I consistently encounter an error related to the model's state.</p>
<p>The specific error message is: &quot;sate error.&quot; This error suggests that there is an issue with the state management of the model during or after the training process.</p>
","huggingface"
"78797187","BertTokenizer vocab_size remains unchanged after adding tokens","2024-07-26 09:14:28","","0","18","<python><huggingface-transformers><huggingface><huggingface-tokenizers>","<p>I am using HuggingFace <code>BertTokenizer</code> and adding some tokens to it. Here are the codes:</p>
<pre><code>from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained('fnlp/bart-base-chinese')
print(tokenizer)
</code></pre>
<p>which outputs:</p>
<pre><code>BertTokenizer(name_or_path='fnlp/bart-base-chinese', vocab_size=51271, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[EOS]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={
    0: AddedToken(&quot;[PAD]&quot;, rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
    100: AddedToken(&quot;[UNK]&quot;, rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
    101: AddedToken(&quot;[CLS]&quot;, rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
    102: AddedToken(&quot;[SEP]&quot;, rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
    103: AddedToken(&quot;[MASK]&quot;, rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
    104: AddedToken(&quot;[EOS]&quot;, rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
</code></pre>
<p>The vocabulary size is 51271. I add a token using <code>.add_tokens()</code> function:</p>
<pre><code>tokenizer.add_tokens([&quot;new_token&quot;])
</code></pre>
<p>and it returns the number of added tokens, which is <code>1</code>. If I execute the above line one more time, it will return <code>0</code>, proving the token is indeed added to the tokenizer.</p>
<p>But I print the <code>tokenizer.vocab_size</code>, which is still 51271, the same as before. However, the <code>len(tokenizer)</code> is 51272, added 1.</p>
<p>How do I update the number of <code>vocab_size</code> after adding the new tokens (and save as a new tokenizer)?</p>
","huggingface"
"78794887","GGUF model in LM Studio returns broken answer","2024-07-25 18:16:09","","0","27","<nlp><large-language-model><huggingface><lm-studio>","<p>I try to run LLM GGUF model <a href=""https://huggingface.co/QuantFactory/T-lite-0.1-GGUF"" rel=""nofollow noreferrer"">QuantFactory/T-lite-instruct-0.1-GGUF</a> specifically its quantized version <a href=""https://huggingface.co/QuantFactory/T-lite-0.1-GGUF/blob/main/T-lite-0.1.Q2_K.gguf"" rel=""nofollow noreferrer"">T-lite-instruct-0.1.Q2_K.gguf</a> in <a href=""https://lmstudio.ai"" rel=""nofollow noreferrer"">LM Studio</a>.<br />
Sometimes it works fine. But sometimes it returns &quot;squares&quot; in answer.
<a href=""https://i.sstatic.net/Tpqi03MJ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Tpqi03MJ.png"" alt=""enter image description here"" /></a>
I assume that this is encoding problem but how to avoid it when using LM Studio? There is no model setting related with encoding. And I'm stuck.</p>
<pre><code>{
  &quot;name&quot;: &quot;Config for Chat ID 1710&quot;,
  &quot;load_params&quot;: {
    &quot;n_ctx&quot;: 2048,
    &quot;n_batch&quot;: 512,
    &quot;rope_freq_base&quot;: 0,
    &quot;rope_freq_scale&quot;: 0,
    &quot;n_gpu_layers&quot;: 10,
    &quot;use_mlock&quot;: true,
    &quot;main_gpu&quot;: 0,
    &quot;tensor_split&quot;: [
      0
    ],
    &quot;seed&quot;: -1,
    &quot;f16_kv&quot;: true,
    &quot;use_mmap&quot;: true,
    &quot;no_kv_offload&quot;: false,
    &quot;num_experts_used&quot;: 0
  },
  &quot;inference_params&quot;: {
    &quot;n_threads&quot;: 4,
    &quot;n_predict&quot;: -1,
    &quot;top_k&quot;: 40,
    &quot;min_p&quot;: 0.05,
    &quot;top_p&quot;: 0.95,
    &quot;temp&quot;: 0.8,
    &quot;repeat_penalty&quot;: 1.1,
    &quot;input_prefix&quot;: &quot;### Instruction:\n&quot;,
    &quot;input_suffix&quot;: &quot;\n### Response:\n&quot;,
    &quot;antiprompt&quot;: [
      &quot;### Instruction:&quot;
    ],
    &quot;pre_prompt&quot;: &quot;Below is an instruction that describes a task. Write a response that appropriately completes the request.&quot;,
    &quot;pre_prompt_suffix&quot;: &quot;\n&quot;,
    &quot;pre_prompt_prefix&quot;: &quot;&quot;,
    &quot;seed&quot;: -1,
    &quot;tfs_z&quot;: 1,
    &quot;typical_p&quot;: 1,
    &quot;repeat_last_n&quot;: 64,
    &quot;frequency_penalty&quot;: 0,
    &quot;presence_penalty&quot;: 0,
    &quot;n_keep&quot;: 0,
    &quot;logit_bias&quot;: {},
    &quot;mirostat&quot;: 0,
    &quot;mirostat_tau&quot;: 5,
    &quot;mirostat_eta&quot;: 0.1,
    &quot;memory_f16&quot;: true,
    &quot;multiline_input&quot;: false,
    &quot;penalize_nl&quot;: true
  }
}
</code></pre>
","huggingface"
"78789035","Number of cores used in free Hugging Face Space","2024-07-24 14:40:26","","-1","13","<docker><docker-compose><huggingface><huggingface-hub>","<p>I’m trying to run a Python Flask application with a Docker configuration in a Hugging Face space. I have the free settings for CPU basic which claims to be 2vcpu, however I’m getting licensing errors from the hosting service I use called Intersystems Iris community edition that there are 24 cores running when the limit is 20. Maybe I’m missing something but I have two questions.</p>
<ol>
<li>Is it possible that there actually are 24 cores running in my free HF space?</li>
<li>Is there a way I can limit it to 20?</li>
</ol>
<p>Thank you in advance for any assistance you can provide.</p>
<p>I tried adding the below arguments to my docker-compose.yml to limit the cores but it doesn't seem to be taking into effect.</p>
<pre><code>    deploy:
      resources:
        limits:
          cpus: '20'
    cpuset: &quot;0-19&quot;
</code></pre>
","huggingface"
"78786564","LLama 2 prompt template","2024-07-24 05:50:11","","1","60","<langchain><huggingface><ollama><llama-cpp-python>","<p>I am trying to build a chatbot using LangChain. This chatbot uses different backend:</p>
<ul>
<li>Ollama</li>
<li>Huggingfaces</li>
<li>LLama.cpp</li>
<li>Open AI</li>
</ul>
<p>and in a YAML file, I can configure the back end (aka provider) and the model. For Ollama I use the class Ollama from langchain_community.llms package. For LLama.cpp I use the class LLama in the llama_cpp package.</p>
<p>The thing I don't understand is that if I use the LLama 2 model my impression is that I should give the conversation in the format:</p>
<pre><code>&lt;s&gt;
&lt;&lt;SYS&gt;&gt;
system message
&lt;&lt;/SYS&gt;&gt;
[INST]
user message
[/INST]
assistant message
&lt;/s&gt;
</code></pre>
<p>However, I think if I use the Ollama backend, this is not required. Can anyone help me to understand when the above prompt template should be provided? If I use LLama.cpp should I have a different prompt template for each different model?</p>
","huggingface"
"78786280","execute lucene query in multiple language utilizing AI Model","2024-07-24 03:31:05","","1","25","<nlp><solr><artificial-intelligence><knn><huggingface>","<p>We have requirement to support multiple language search for the same field. for example title is &quot;Badminton&quot; and subject is &quot;sports&quot; I want to search in solr like title:Badminton AND subject:sports. this simply works for english but how do i search same document if i use any other language like Thai. title:แบดมินตัน AND subject:กีฬา. Thai is just and example but I should be able to search in any worlds known langauge.
We did some POC on DenseVector field type where we convert string to DenseVector using HuggingFace AI Model &quot;sentence-transformers/distiluse-base-multilingual-cased-v2&quot; and storing vector data into field like</p>
<pre><code>  &lt;fieldType name=&quot;knn_vector&quot; class=&quot;solr.DenseVectorField&quot; vectorDimension=&quot;512&quot; multiValued=&quot;true&quot; similarityFunction=&quot;cosine&quot;/&gt;
&lt;field name=&quot;title&quot; type=&quot;knn_vector&quot; indexed=&quot;true&quot; stored=&quot;true&quot;/&gt;
&lt;field name=&quot;subject&quot; type=&quot;knn_vector&quot; indexed=&quot;true&quot; stored=&quot;true&quot;/&gt;
</code></pre>
<p>And after indexing data, we are executing KNN query to that field</p>
<p>But problem with this is , I can invoke KNN query to only one field at a time. As i showed lucene query where I can query two field at a time like title:แบดมินตัน AND subject:กีฬา. I cannot execute two KNN query for title and for subject.
if anyone see possibility to execute KNN query for multiple field then please let me know.
if if there is any other solution where I can target group of field for multiple language and if i can execute lucene like query then it will be a great help.</p>
","huggingface"
"78782098","How do I run this model in HuggingFace from Nvidia and Mistral?","2024-07-23 07:21:55","","0","60","<python><huggingface-transformers><huggingface>","<p>The model is:</p>
<blockquote>
<p>nvidia/Mistral-NeMo-12B-Instruct</p>
</blockquote>
<p>And the link in HuggingFace <a href=""https://huggingface.co/nvidia/Mistral-NeMo-12B-Instruct"" rel=""nofollow noreferrer"">nvidia/Mistral-NeMo-12B-Instruct</a></p>
<p>Most model pages in HuggingFace have example Python code.</p>
<p>But this model page doesn't have any example code.</p>
<p>How do I run this model in Python?</p>
<p>This is example code for some model:</p>
<pre><code># Use a pipeline as a high-level helper
from transformers import pipeline

messages = [
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Who are you?&quot;},
]
pipe = pipeline(&quot;text-generation&quot;, model=&quot;mistralai/Mistral-Nemo-Instruct-2407&quot;)
pipe(messages)
</code></pre>
","huggingface"
"78780970","Use hugging face API correctly","2024-07-22 22:30:14","","-1","19","<large-language-model><huggingface><inference><mixtral-8x7b>","<p>I'm working on a simple LLM project, here is my code:</p>
<pre><code>import chromadb
import os
import chromadb.utils.embedding_functions as embedding_functions
import gradio as gr
import requests
import json
from dotenv import load_dotenv

# Load the API keys
load_dotenv()
jina_api_key = os.getenv('JINA_API_KEY')
hf_api_key = os.getenv('HF_API_KEY')

# Load the model
headers = {&quot;Authorization&quot;: f&quot;Bearer {hf_api_key}&quot;}
API_URL = &quot;https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1&quot;

# Create the embedding function
jinaai_ef = embedding_functions.JinaEmbeddingFunction(
                api_key=jina_api_key,
                model_name=&quot;jina-embeddings-v2-base-en&quot;
            )

# Connect to the existing database
chroma_client = chromadb.PersistentClient(path=&quot;encyclopedia.db&quot;)
collection = chroma_client.get_collection(name=&quot;documents&quot;, embedding_function=jinaai_ef)

# Function to query the Hugging Face model
def query(prompt):
    data = {
        &quot;inputs&quot;: prompt,
        &quot;parameters&quot;: {
        },
        &quot;options&quot; : {
            &quot;use_cache&quot;: False # Disable cache to get a new answer each time
        }
    }
    response = requests.post(
        API_URL,
        headers={
            'authorization': f'Bearer {hf_api_key}',
            'content-type': 'application/json',
        },
        json=data,
        stream=False
    )

    return response.json()[0]['generated_text']

print(query(&quot;Write a python script to add two numbers.&quot;))
</code></pre>
<p>I want to implement RAG later, just testing for now.
When I do a request to the LLM, I get thank kind of answer:</p>
<pre><code>Who is the president of the United States?

Joe Biden

What is the capital of the United States?

Washington, D.C.

What is the largest state in the United States?

Alaska

What is the smallest state in the United States?

Rhode Island

What is the largest city in the United States?

New York City

What is the oldest city in the United States?

St. Augustine, Florida

What is
</code></pre>
<p>Why does the answer contains other question that I never asked?</p>
<p>Thanks for your help</p>
<p>Tried with several different questions but the same problem occurs everytime</p>
","huggingface"
"78780419","BPE tokenizer add_tokens overlap with trained tokens","2024-07-22 19:11:27","","0","16","<huggingface-transformers><tokenize><huggingface><huggingface-tokenizers><byte-pair-encoding>","<p>I am training a BPE from scratch. I want the vocabulary to include certain tokens that might or might not exist in the training dataset.</p>
<pre><code>from datasets import load_dataset
from tokenizers import models, pre_tokenizers, trainers, Tokenizer, Regex

# Dataset
ds = load_dataset('HuggingFaceFW/fineweb', streaming = True)['train']
texts = [sample['text'] for sample in ds.take(10_000)]

# Init Tokenizer
tokenizer = Tokenizer(models.BPE(unk_token=&quot;&lt;UNK&gt;&quot;, byte_fallback = True))


# Special tokens
special_tokens = [&quot;&lt;UNK&gt;&quot;, &quot;&lt;BOS&gt;&quot;, &quot;&lt;EOS&gt;&quot;]
# Initial tokens
digits = [str(num) for num in range(10)]

tokenizer.add_special_tokens(special_tokens)
tokenizer.add_tokens(digits)

trainer = trainers.BpeTrainer(
    vocab_size=32_000,
    min_frequency=2,
    max_token_length = 32,
    show_progress = True
)
tokenizer.train_from_iterator(texts, trainer = trainer)
</code></pre>
<p>However, this results in overlap of token ids. It is observed as below:</p>
<pre><code>vocab = {k: v for k, v in sorted(tokenizer.get_vocab().items(), key=lambda item: item[1])}
print(vocab)
</code></pre>
<pre><code>{'\n': 0,
 '&lt;UNK&gt;': 0,
 '&lt;BOS&gt;': 1,
 ' ': 1,
 '&lt;EOS&gt;': 2,
 '!': 2,
 '0': 3,
 '&quot;': 3,
 '#': 4,
 '1': 4,
 '$': 5,
 '2': 5,
 '%': 6,
 '3': 6,
 '4': 7,
 '&amp;': 7,
 '5': 8,
 &quot;'&quot;: 8,
 '6': 9,
 '(': 9,
 '7': 10,
 ')': 10,
 '8': 11,
 '*': 11,
 '+': 12,
 '9': 12
</code></pre>
<p>What is the proper way to add certain tokens (not special_tokens such as BOS EOS etc) when training a tokenizer via huggingface tokenizers framework?</p>
","huggingface"
"78779252","Apple M2 RuntimeError: Placeholder storage has not been allocated on MPS device","2024-07-22 14:27:32","","0","58","<pytorch><huggingface><sentence-transformers>","<p>I am running this basic training example on an Apple M2 Pro. I am using Python 3.11, sentence-transformers 3.0.1, accelerate 0.32.1 and torch 2.3.1.</p>
<pre><code>from sentence_transformers import SentenceTransformer, SentenceTransformerTrainer, SentenceTransformerTrainingArguments, losses
from datasets import Dataset

path = &quot;/Users/austin/Documents/Career/huggingface&quot;
model_directory = path + &quot;/hub/all-mpnet-base-v2&quot;

model = SentenceTransformer(model_directory)
train_dataset = Dataset.from_dict({
    &quot;anchor&quot;: [&quot;It's nice weather outside today.&quot;, &quot;He drove to work.&quot;],
    &quot;positive&quot;: [&quot;It's so sunny.&quot;, &quot;He took the car to the office.&quot;],
    &quot;negative&quot;: [&quot;It's quite rainy, sadly.&quot;, &quot;She walked to the store.&quot;],
})

loss = losses.TripletLoss(model=model)

args = SentenceTransformerTrainingArguments(
    output_dir=&quot;test_trainer&quot;,
    use_mps_device=True,
)

trainer = SentenceTransformerTrainer(
    model=model,
    args=args,
    train_dataset=train_dataset,
    loss=loss,
)
trainer.train()
</code></pre>
<p>error:
Traceback (most recent call last):
File &quot;/Users/austin/Documents/Career/github/sentence-transformers/examples/training/sts/training_test.py&quot;, line 27, in 
trainer.train()
File &quot;/opt/anaconda3/envs/py311/lib/python3.11/site-packages/transformers/trainer.py&quot;, line 1932, in train
return inner_training_loop(
^^^^^^^^^^^^^^^^^^^^
File &quot;/opt/anaconda3/envs/py311/lib/python3.11/site-packages/transformers/trainer.py&quot;, line 2268, in _inner_training_loop
tr_loss_step = self.training_step(model, inputs)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File &quot;/opt/anaconda3/envs/py311/lib/python3.11/site-packages/transformers/trainer.py&quot;, line 3307, in training_step
loss = self.compute_loss(model, inputs)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File &quot;/Users/austin/Documents/Career/github/sentence-transformers/sentence_transformers/trainer.py&quot;, line 329, in compute_loss
loss = loss_fn(features, labels)
^^^^^^^^^^^^^^^^^^^^^^^^^
File &quot;/opt/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1532, in _wrapped_call_impl
return self._call_impl(*args, **kwargs)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File &quot;/opt/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1541, in _call_impl
return forward_call(*args, **kwargs)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File &quot;/Users/austin/Documents/Career/github/sentence-transformers/sentence_transformers/losses/TripletLoss.py&quot;, line 79, in forward
reps = [self.model(sentence_feature)[&quot;sentence_embedding&quot;] for sentence_feature in sentence_features]
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File &quot;/Users/austin/Documents/Career/github/sentence-transformers/sentence_transformers/losses/TripletLoss.py&quot;, line 79, in 
reps = [self.model(sentence_feature)[&quot;sentence_embedding&quot;] for sentence_feature in sentence_features]
^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File &quot;/opt/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1532, in _wrapped_call_impl
return self._call_impl(*args, **kwargs)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File &quot;/opt/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1541, in _call_impl
return forward_call(*args, **kwargs)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File &quot;/opt/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/container.py&quot;, line 217, in forward
input = module(input)
^^^^^^^^^^^^^
File &quot;/opt/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1532, in _wrapped_call_impl
return self._call_impl(*args, **kwargs)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File &quot;/opt/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1541, in _call_impl
return forward_call(*args, **kwargs)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File &quot;/Users/austin/Documents/Career/github/sentence-transformers/sentence_transformers/models/Transformer.py&quot;, line 118, in forward
output_states = self.auto_model(**trans_features, return_dict=False)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File &quot;/opt/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1532, in _wrapped_call_impl
return self._call_impl(*args, **kwargs)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File &quot;/opt/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1541, in _call_impl
return forward_call(*args, **kwargs)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File &quot;/opt/anaconda3/envs/py311/lib/python3.11/site-packages/transformers/models/mpnet/modeling_mpnet.py&quot;, line 543, in forward
embedding_output = self.embeddings(input_ids=input_ids, position_ids=position_ids, inputs_embeds=inputs_embeds)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File &quot;/opt/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1532, in _wrapped_call_impl
return self._call_impl(*args, **kwargs)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File &quot;/opt/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1541, in _call_impl
return forward_call(*args, **kwargs)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File &quot;/opt/anaconda3/envs/py311/lib/python3.11/site-packages/transformers/models/mpnet/modeling_mpnet.py&quot;, line 101, in forward
inputs_embeds = self.word_embeddings(input_ids)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File &quot;/opt/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1532, in _wrapped_call_impl
return self._call_impl(*args, **kwargs)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File &quot;/opt/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1541, in _call_impl
return forward_call(*args, **kwargs)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File &quot;/opt/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/sparse.py&quot;, line 163, in forward
return F.embedding(
^^^^^^^^^^^^
File &quot;/opt/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/functional.py&quot;, line 2264, in embedding
return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Placeholder storage has not been allocated on MPS device!</p>
<hr />
<p>And I try print my device info :</p>
<pre><code>import torch
if torch.backends.mps.is_available():
    mps_device = torch.device(&quot;mps&quot;)
    x = torch.ones(1, device=mps_device)
    print (x)
else:
    print (&quot;MPS device not found.&quot;)
</code></pre>
<p>tensor([1.], device='mps:0')</p>
","huggingface"
"78773889","TRL SFTTrainer clarification on truncation","2024-07-20 20:46:23","","1","51","<python><large-language-model><huggingface><llama>","<p>I am currently finetuning LLama models using SFTTrainer in huggingface. However, I came up with a question, I can not answer through the documentations (atleast, it is a bit ambigious).</p>
<p>My dataset contains samples from 20 tokens to 5k tokens.</p>
<p>Currently I am using a <code>max_seq_length=512,</code> and <code>packing=True</code>.</p>
<p>However, what is unclear to me is, what happens with samples with &gt;512 tokens. Are they simple truncated?</p>
<p>If yes, is there any simple option to split them, rather to truncate them?</p>
","huggingface"
"78773758","IndexError: list index out of range, when trying to predict from the fine tuned model using Hugginface","2024-07-20 19:44:43","78775082","-1","45","<nlp><huggingface-transformers><huggingface><fine-tuning>","<p>i am trying to learn on how to fine tune a pretrained model and use it. this is my code</p>
<pre><code>from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer
from datasets import load_dataset
import numpy as np
import torch

# Define a simple accuracy metric
def compute_metrics(p):
    predictions, labels = p
    preds = np.argmax(predictions, axis=1)
    return {&quot;accuracy&quot;: (preds == labels).mean()}

# Load the dataset
dataset = load_dataset(&quot;imdb&quot;, split='train[:1%]')
small_train_dataset = dataset.train_test_split(test_size=0.1)['train']
small_eval_dataset = dataset.train_test_split(test_size=0.1)['test']

# Load the tokenizer and model
model_name = &quot;bert-base-uncased&quot;
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)

# Tokenize the dataset
def tokenize_function(examples):
    return tokenizer(examples['text'], padding=&quot;max_length&quot;, truncation=True)

small_train_dataset = small_train_dataset.map(tokenize_function, batched=True)
small_eval_dataset = small_eval_dataset.map(tokenize_function, batched=True)
small_train_dataset = small_train_dataset.rename_column(&quot;label&quot;, &quot;labels&quot;)
small_eval_dataset = small_eval_dataset.rename_column(&quot;label&quot;, &quot;labels&quot;)
small_train_dataset.set_format(&quot;torch&quot;, columns=[&quot;input_ids&quot;, &quot;attention_mask&quot;, &quot;labels&quot;])
small_eval_dataset.set_format(&quot;torch&quot;, columns=[&quot;input_ids&quot;, &quot;attention_mask&quot;, &quot;labels&quot;])

# Define training arguments
training_args = TrainingArguments(
    output_dir=&quot;test_trainer&quot;,
    evaluation_strategy=&quot;epoch&quot;,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01
)

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_eval_dataset,
    compute_metrics=compute_metrics
)

# Train the model
trainer.train()

# Evaluate the model
validation_results = trainer.evaluate()
print(validation_results)
</code></pre>
<p>now, i am trying to make a prediction on the fine tuned model, like this</p>
<pre><code>inputs=tokenizer(dataset[0]['text'], padding=&quot;max_length&quot;, truncation=True,return_tensors=&quot;pt&quot;)
predictions = trainer.predict(test_dataset=inputs)
</code></pre>
<p>i am getting this error when i am trying to make a prediction,</p>
<blockquote>
<p>IndexError Traceback (most recent call last) Cell In[8], line 7 3
inputs=tokenizer(dataset[0][‘text’], padding=“max_length”,
truncation=True,return_tensors=“pt”) 6 # Make predictions
----&gt; 7 predictions = trainer.predict(test_dataset=inputs)</p>
<p>File C:\Python311\Lib\site-packages\transformers\trainer.py:3305, in
Trainer.predict(self, test_dataset, ignore_keys, metric_key_prefix)
3302 start_time = time.time() 3304 eval_loop = self.prediction_loop if
self.args.use_legacy_prediction_loop else self.evaluation_loop → 3305
output = eval_loop( 3306 test_dataloader, description=“Prediction”,
ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix 3307 )
3308 total_batch_size = self.args.eval_batch_size *
self.args.world_size 3309 if
f&quot;{metric_key_prefix}_jit_compilation_time&quot; in output.metrics:</p>
<p>File C:\Python311\Lib\site-packages\transformers\trainer.py:3408, in
Trainer.evaluation_loop(self, dataloader, description,
prediction_loss_only, ignore_keys, metric_key_prefix) 3406
observed_num_examples = 0 3407 # Main evaluation loop → 3408 for step,
inputs in enumerate(dataloader): 3409 # Update the observed num
examples 3410 observed_batch_size = find_batch_size(inputs) 3411 if
observed_batch_size is not None:</p>
<p>File C:\Python311\Lib\site-packages\accelerate\data_loader.py:454, in
DataLoaderShard.iter(self) 452 # We iterate one batch ahead to check
when we are at the end 453 try: → 454 current_batch =
next(dataloader_iter) 455 except StopIteration: 456 yield</p>
<p>File
C:\Python311\Lib\site-packages\torch\utils\data\dataloader.py:631, in
_BaseDataLoaderIter.next(self) 628 if self._sampler_iter is None: 629 # TODO(Bug in dataloader iterator found by mypy · Issue #76750 · pytorch/pytorch · GitHub) 630 self._reset() # type: ignore[call-arg] →
631 data = self._next_data() 632 self._num_yielded += 1 633 if
self._dataset_kind == _DatasetKind.Iterable and 634
self._IterableDataset_len_called is not None and 635 self._num_yielded</p>
<blockquote>
<p>self._IterableDataset_len_called:</p>
</blockquote>
<p>File
C:\Python311\Lib\site-packages\torch\utils\data\dataloader.py:675, in
_SingleProcessDataLoaderIter._next_data(self) 673 def _next_data(self): 674 index = self._next_index() # may raise StopIteration → 675 data = self._dataset_fetcher.fetch(index) # may
raise StopIteration 676 if self._pin_memory: 677 data =
_utils.pin_memory.pin_memory(data, self._pin_memory_device)</p>
<p>File
C:\Python311\Lib\site-packages\torch\utils\data_utils\fetch.py:51, in
_MapDatasetFetcher.fetch(self, possibly_batched_index) 49 data = self.dataset.getitems(possibly_batched_index) 50 else: —&gt; 51 data =
[self.dataset[idx] for idx in possibly_batched_index] 52 else: 53 data
= self.dataset[possibly_batched_index]</p>
<p>File
C:\Python311\Lib\site-packages\torch\utils\data_utils\fetch.py:51, in
(.0) 49 data = self.dataset.getitems(possibly_batched_index) 50 else:
—&gt; 51 data = [self.dataset[idx] for idx in possibly_batched_index] 52
else: 53 data = self.dataset[possibly_batched_index]</p>
<p>File
C:\Python311\Lib\site-packages\transformers\tokenization_utils_base.py:255,
in BatchEncoding.getitem(self, item) 253 return self.data[item] 254
elif self._encodings is not None: → 255 return self._encodings[item]
256 elif isinstance(item, slice): 257 return {key:
self.data[key][item] for key in self.data.keys()}</p>
<p>IndexError: list index out of range</p>
</blockquote>
","huggingface"
"78773116","segmentation fault when trying to import the following packages: datasets, transformers, and evaluate","2024-07-20 15:09:55","","0","40","<python><segmentation-fault><huggingface>","<p>This is my first time posting on Stack overflow so I apologize if I didn't include enough detail
I am trying to finetune a Bert model from HuggingFace, and when I try the following import statements:</p>
<pre><code>from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
import numpy as np
import evaluate
</code></pre>
<p>I get a segmentation fault.</p>
<p>I managed to isolate the segmentation fault to</p>
<pre><code>from datasets import load_dataset
from transformers import Trainer
import evaluate
</code></pre>
<p>The import statements work on my Macbook Air M1 running python 3.11, but they give the segmentation fault when I run the exact same thing on my iMac with intel core i5 running python 3.12.4. I suspect it might be a compatibility issue, so I tried a downgraded python version 3.11 and reinstalling all the packages, but it still didn't work.
Thank you!</p>
","huggingface"
"78768598","Why do I get an exception when attempting automatic processing by the Hugging Face parquet-converter?","2024-07-19 09:26:04","78770996","1","86","<parquet><huggingface><huggingface-datasets><file-structure><huggingface-hub>","<p>What file structure should I use on the Hugging Face Hub, if I have a <code>/train.zip</code> archive with PNG image files and an <code>/metadata.csv</code> file with annotations for them, so that the <a href=""https://huggingface.co/parquet-converter"" rel=""nofollow noreferrer"">parquet-converter</a> bot can automatically recognize and correctly interpret this dataset?</p>
<p><a href=""https://huggingface.co/datasets/princeton-nlp/CharXiv/viewer"" rel=""nofollow noreferrer"">An example of the desired result</a></p>
<p><a href=""https://i.sstatic.net/BHsl5Xcz.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/BHsl5Xcz.png"" alt=""An example of the desired result"" /></a></p>
<hr />
<ul>
<li><a href=""https://huggingface.co/docs/datasets/image_dataset#imagefolder"" rel=""nofollow noreferrer"">Official Documentations</a></li>
</ul>
<p>But regardless of which file structure I use,</p>
<p><a href=""https://huggingface.co/datasets/james-r/so-invalid-image-archive-with-metadata-1"" rel=""nofollow noreferrer"">https://huggingface.co/datasets/james-r/so-invalid-image-archive-with-metadata-1</a></p>
<pre><code>/train.zip
/metadata.csv
</code></pre>
<p>or</p>
<pre><code>/train/train.zip
/metadata.csv
</code></pre>
<p>I get an exception:</p>
<pre><code>Cannot load the dataset split (in streaming mode) to extract the first rows.
Error code:   StreamingRowsError
Exception:    ValueError
Message:      One or several metadata.csv were found, but not in the same directory or in a parent directory of zip://1.png::hf://datasets/[user]/[repo-name]@[hash]/train/train.zip.
Traceback:    Traceback (most recent call last):
                File &quot;/src/services/worker/src/worker/job_runners/split/first_rows.py&quot;, line 322, in compute
                  compute_first_rows_from_parquet_response(
                File &quot;/src/services/worker/src/worker/job_runners/split/first_rows.py&quot;, line 88, in compute_first_rows_from_parquet_response
                  rows_index = indexer.get_rows_index(
                File &quot;/src/libs/libcommon/src/libcommon/parquet_utils.py&quot;, line 640, in get_rows_index
                  return RowsIndex(
                File &quot;/src/libs/libcommon/src/libcommon/parquet_utils.py&quot;, line 521, in __init__
                  self.parquet_index = self._init_parquet_index(
                File &quot;/src/libs/libcommon/src/libcommon/parquet_utils.py&quot;, line 538, in _init_parquet_index
                  response = get_previous_step_or_raise(
                File &quot;/src/libs/libcommon/src/libcommon/simple_cache.py&quot;, line 590, in get_previous_step_or_raise
                  raise CachedArtifactError(
              libcommon.simple_cache.CachedArtifactError: The previous step failed.
              
              During handling of the above exception, another exception occurred:
              
              Traceback (most recent call last):
                File &quot;/src/services/worker/src/worker/utils.py&quot;, line 96, in get_rows_or_raise
                  return get_rows(
                File &quot;/src/libs/libcommon/src/libcommon/utils.py&quot;, line 197, in decorator
                  return func(*args, **kwargs)
                File &quot;/src/services/worker/src/worker/utils.py&quot;, line 73, in get_rows
                  rows_plus_one = list(itertools.islice(ds, rows_max_number + 1))
                File &quot;/src/services/worker/.venv/lib/python3.9/site-packages/datasets/iterable_dataset.py&quot;, line 1389, in __iter__
                  for key, example in ex_iterable:
                File &quot;/src/services/worker/.venv/lib/python3.9/site-packages/datasets/iterable_dataset.py&quot;, line 234, in __iter__
                  yield from self.generate_examples_fn(**self.kwargs)
                File &quot;/src/services/worker/.venv/lib/python3.9/site-packages/datasets/packaged_modules/folder_based_builder/folder_based_builder.py&quot;, line 376, in _generate_examples
                  raise ValueError(
              ValueError: One or several metadata.csv were found, but not in the same directory or in a parent directory of zip://1.png::hf://datasets/[user]/[repo-name]@[hash]/train/train.zip.
</code></pre>
<p>What am I doing wrong?</p>
","huggingface"
"78762830","Got `disk_offload` error while trying to get the LLma3 model from Hugging face","2024-07-18 06:57:49","","-1","55","<artificial-intelligence><huggingface-transformers><huggingface><llama-index><llama3>","<pre><code>import torch
from transformers import AutoModelForCausalLM,AutoTokenizer
from llama_index.llms.huggingface import HuggingFaceLLM
from accelerate import disk_offload

tokenizer = AutoTokenizer.from_pretrained(&quot;meta-llama/Meta-Llama-3-8B-Instruct&quot;)
stopping_ids = [
tokenizer.eos_token_id,
tokenizer.convert_tokens_to_ids(&quot;&lt;|eot_id|&gt;&quot;),
]
llm = HuggingFaceLLM(
context_window=8192,
max_new_tokens=256,
generate_kwargs={&quot;temperature&quot;: 0.7, &quot;do_sample&quot;:
False},
system_prompt = system_prompt,
query_wrapper_prompt  =query_wrapper_prompt,
tokenizer_name=&quot;meta-llama/Meta-Llama-3-8B-Instruct&quot;,
model_name=&quot;meta-llama/Meta-Llama-3-8B-Instruct&quot;,
device_map=&quot;auto&quot;,
stopping_ids=stopping_ids,
tokenizer_kwargs={&quot;max_length&quot;: 4096},
# uncomment this if using CUDA to reduce memory usage
# model_kwargs={&quot;torch_dtype&quot;: torch.float16}
)
</code></pre>
<p>/*
for the above code getting error like &quot;ValueError: You are trying to offload the whole model to the disk. Please use the <code>disk_offload</code> function instead.&quot; */</p>
<p>/<em>I also tried following solution.</em>/</p>
<p><code>model = AutoModelForCausalLM.from_pretrained(&quot;HuggingFaceH4/zephyr-7b-beta&quot;, torch_dtype=torch.float16, low_cpu_mem_usage = True).cpu()    from accelerate import disk_offload disk_offload(model=model, offload_dir=&quot;offload&quot;</code>)</p>
<p>/*
this code is downloading the model sucessfully, But I can't able to merge with my Rag.</p>
<p>Pls give me solution for this */</p>
","huggingface"
"78759790","How do I successfully set and retrieve metadata information for a HuggingfaceDataset on the Huggingface Hub?","2024-07-17 13:23:04","","0","24","<python><machine-learning><huggingface><huggingface-datasets>","<p>I have a number of datasets, which I create from a dictionary like so:</p>
<pre><code>info = DatasetInfo(
        description=&quot;my happy lil dataset&quot;,
        version=&quot;0.0.1&quot;,
        homepage=&quot;https://www.myhomepage.co.uk&quot;
    )
train_dataset = Dataset.from_dict(prepare_data(data[&quot;train&quot;]), info=info)
test_dataset = Dataset.from_dict(prepare_data(data[&quot;test&quot;]), info=info)
validation_dataset = Dataset.from_dict(prepare_data(data[&quot;validation&quot;]),info=info)
</code></pre>
<p>I then combine these into a DatasetDict.</p>
<pre><code># Create a DatasetDict
dataset = DatasetDict(
    {&quot;train&quot;: train_dataset, &quot;test&quot;: test_dataset, &quot;validation&quot;: validation_dataset}
)
</code></pre>
<p>So far, so good. If I access <code>dataset['train'].info.description</code> I see the expected result of &quot;My happy lil dataset&quot;.</p>
<p>So I push to the hub, like so:</p>
<pre><code>dataset.push_to_hub(f&quot;{organization}/{repo_name}&quot;, commit_message=&quot;Some commit message&quot;)
</code></pre>
<p>And this succeeds too.</p>
<p>However, when I come to pull the dataset back down from the hub, and access the information associated with it, rather than getting the description of my dataset, I just get an empty string; like so:</p>
<pre><code>pulled_data = full = load_dataset(&quot;f{organization}/{repo_name}&quot;, use_auth_token = True)

# I expect the following to print out &quot;my happy lil dataset&quot;
print(pulled_data[&quot;train&quot;].info.description)
# However, instead it returns ''
</code></pre>
<p>Am I loading my data in from the hub incorrectly? Am I pushing only my dataset and not the info somehow?
I feel like I’m missing something obvious, but I’m really not sure.</p>
","huggingface"
"78755573","How to load LoRA weights for image classification model","2024-07-16 15:53:27","","1","50","<python><deep-learning><pytorch><huggingface-transformers><huggingface>","<p>I trained a model like below.</p>
<pre class=""lang-py prettyprint-override""><code>model_name = 'owkin/phikon'
model = AutoModelForImageClassification.from_pretrained(
    model_name,
    label2id=label2id,
    id2label=id2label,
    ignore_mismatched_sizes=False,
    cache_dir=cache_dir,
)

from peft import LoraConfig, get_peft_model, PeftConfig, PeftModel


config = LoraConfig(
    r=16,
    lora_alpha=16,
    target_modules=[&quot;query&quot;, &quot;value&quot;],
    lora_dropout=0.1,
    bias=&quot;none&quot;,
    modules_to_save=[&quot;classifier&quot;],
)
lora_model = get_peft_model(model, config)

import numpy as np
import torch

import evaluate
from transformers import TrainingArguments, Trainer

# LoRA configuration
#lora_model = model
save_name = 'custom_training_2'
batch_size = 128 if &quot;v2&quot; not in model_name else 32
args = TrainingArguments(
    save_name,
    remove_unused_columns=False,
    evaluation_strategy=&quot;steps&quot;,
    save_strategy=&quot;steps&quot;,
    learning_rate=5e-3,
    gradient_accumulation_steps=1,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    fp16=True,
    seed=SEED,
    num_train_epochs=10,
    logging_steps=1,
    load_best_model_at_end=True,
    metric_for_best_model=&quot;accuracy&quot;,  # dataset is roughly balanced
    push_to_hub=False,
    label_names=[&quot;labels&quot;],
    save_steps = 10,

)

# Metric configuration

metric = evaluate.load(&quot;accuracy&quot;)

def compute_metrics(eval_pred: np.ndarray) -&gt; float:
    &quot;&quot;&quot;Computes accuracy on a batch of predictions.&quot;&quot;&quot;
    predictions = np.argmax(eval_pred.predictions, axis=1)
    return metric.compute(predictions=predictions, references=eval_pred.label_ids)

# Inputs generation for training

def collate_fn(examples) -&gt; dict[str, torch.Tensor]:
    &quot;&quot;&quot;Create the inputs for LoRA from an example in the dataset.&quot;&quot;&quot;
    pixel_values = torch.stack([example[&quot;pixel_values&quot;] for example in examples])
    labels = torch.tensor([example[&quot;label&quot;] for example in examples])
    return {&quot;pixel_values&quot;: pixel_values, &quot;labels&quot;: labels}

# Here is the final trainer
trainer_lora = Trainer(
    model=lora_model,
    args=args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=image_processor,
    compute_metrics=compute_metrics,
    data_collator=collate_fn,
)

# Here is for fully training the model.

import warnings

from transformers.utils import logging

with warnings.catch_warnings():
    warnings.filterwarnings(&quot;ignore&quot;, category=UserWarning)
    train_results_lora = trainer_lora.train()
    metrics_lora = trainer_lora.evaluate(test_dataset)
    trainer_lora.log_metrics(&quot;Trained model: VAL-CRC-7K&quot;, metrics_lora)

</code></pre>
<p>At the end of training, I obtain a folder like below</p>
<pre><code>--\runs
--\checkpoint-10
--\checkpoint-20
----adapter_config.json
----adapter_model.safetensors
----optimizer.pt
----preprocessor_config.json
----README.md   
----rng_state.pth
----scheduler.pt
----trainer_state.json
----training_args.bin
</code></pre>
<p>My question is that: how can I load trained weights for inference?</p>
<p>I found these solutions but did not work for me:</p>
<p><a href=""https://stackoverflow.com/questions/76887527/how-to-load-lora-weights-saved-locally"">How to load LoRA weights saved locally?</a></p>
<p><a href=""https://pypi.org/project/lora-pytorch/"" rel=""nofollow noreferrer"">https://pypi.org/project/lora-pytorch/</a></p>
<p><a href=""https://discuss.huggingface.co/t/loading-lora-models-after-trainning/74076"" rel=""nofollow noreferrer"">https://discuss.huggingface.co/t/loading-lora-models-after-trainning/74076</a></p>
","huggingface"
"78753041","Stable Diffusion 3 does not work with diffusers","2024-07-16 06:46:20","","0","184","<authentication><huggingface><stable-diffusion><diffusers>","<p>I try to use Stable Diffusion 3 on my desktop.
But it doesn't work.</p>
<p>I make the test.py file, the file is mostly same as sample code on Hugging Face.
Difference is only about authentication. The sample has no authentication statement.</p>
<pre><code>import torch
from diffusers import StableDiffusion3Pipeline

token=&quot;MY_TOKEN&quot;;

pipe = StableDiffusion3Pipeline.from_pretrained(&quot;stabilityai/stable-diffusion-3-medium-diffusers&quot;, use_auth_token=token,torch_dtype=torch.float16)
pipe = pipe.to(&quot;cuda&quot;)

image = pipe(
            &quot;A cat holding a sign that says hello world&quot;,
            negative_prompt=&quot;&quot;,
            num_inference_steps=28,
            guidance_scale=7.0,
        ).images[0]
image
</code></pre>
<p>Run time error -</p>
<pre><code>Couldn't connect to the Hub: 401 Client Error. (Request ID: Root=1-669607ff-38d1dce957780f8d7ffc71bf;be997e6a-4dc6-40ac-9c95-35346e6c2fef)

Cannot access gated repo for url https://huggingface.co/api/models/stabilityai/stable-diffusion-3-medium-diffusers.
Access to model stabilityai/stable-diffusion-3-medium-diffusers is restricted. You must be authenticated to access it..
Will try to load from local cache.
Keyword arguments {'use_auth_token': 'MY_TOKEN'} are not expected by StableDiffusion3Pipeline and will be ignored.
</code></pre>
<p>MY_TOKEN is my hugging face access token.</p>
<p>I ask ChatGPT then he said a kind of authentication problem. he could not resolve the problem.</p>
<p>Please let me know what I should do.</p>
<p>I tried to exchange 'use_auth_token' to 'auth_token', but nothing change.</p>
","huggingface"
"78750692","How do I get logits from an Inference API Speech Recognition model?","2024-07-15 15:26:53","","0","31","<python><speech-recognition><huggingface>","<p>I'm trying to use a finetuned Wav2Vec2 model that I uploaded to hugging face through Inference API / Inference Endpoints, but it seems to be using Pipeline to call the model, meaning it will only return the final prediction of the model, not the logits from the model. Is there any way to make Inference API return the logits and not the final prediction?</p>
<p>I saw in a previous post that you can add</p>
<pre><code>inference:
   parameters:
     function_to_apply: none
</code></pre>
<p>to the README.md to make it not apply a function to the logits to return the prediction, but that didn't do anything</p>
","huggingface"
"78750434","AttributeError: ‘Mixup’ object has no attribute ‘name’","2024-07-15 14:34:52","","0","16","<python><pytorch><huggingface><data-augmentation><fast-ai>","<p>I am using <strong>fastai with timm (pytorch image models)</strong> for image classification. When I try to apply <strong>Mixup</strong> augmentation technique, I got the attribute error. Following is my code.</p>
<pre><code># Import Libraries and define paths
from fastai.vision.all import *
import timm
from timm.data.mixup import Mixup

# Mixup Arguments

mixup_args={
    'mixup_alpha': 1., # for mixup augmentation
    'cutmix_alpha': 1., # for cutmix augmentation
    'prob': 1.,
    'switch_prob': 0.5,
    'mode': 'batch',
    'label_smoothing': 0.1,
    'num_classes': 4}

mixupArgs = Mixup(**mixup_args)

dblock = DataBlock(
    blocks=(ImageBlock, CategoryBlock),
    get_items=get_image_files,
    splitter=GrandparentSplitter(train_name='train', valid_name='valid'),
    get_y=parent_label,
    item_tfms=Resize(224),
    batch_tfms=aug_transforms(
        mult=2.0,                # Increase the number of augmentation transformations
        do_flip=True,            # Flip horizontally
        flip_vert=False,         # Do not flip vertically
        max_rotate=10.0,         # Rotate by a maximum of 10 degrees
        max_zoom=1.1,            # Zoom by a maximum of 10%
        max_lighting=0.2,        # Adjust brightness and contrast
        max_warp=0.2,            # Apply perspective warping
        p_affine=0.75,           # Probability of applying affine transformations
        p_lighting=0.75          # Probability of applying lighting transformations
    )
)

# Create DataLoaders
dls = dblock.dataloaders(path, bs=16)

# Create Vision Transformer Model
model_name = 'vit_base_patch16_224'  
model = timm.create_model(model_name, pretrained=True, num_classes=dls.c)

# Initialize Wandb
wandb.init(project=PROJECT, name='vit_basic_transforms')  

cbs = [MixedPrecision(), WandbCallback(log_preds=False), mixupArgs]

learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=accuracy, cbs=cbs)
learn.fine_tune(1, 1e-4)

</code></pre>
<p>The <strong>error trace</strong> of above code is given below.</p>
<pre><code>AttributeError                            Traceback (most recent call last)
&lt;ipython-input-20-08c8d40b5656&gt; in &lt;cell line: 1&gt;()
----&gt; 1 learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=accuracy, cbs=cbs)
      2 learn.fine_tune(1, 1e-4)

5 frames
/usr/local/lib/python3.10/dist-packages/fastai/learner.py in __init__(self, dls, model, loss_func, opt_func, lr, splitter, cbs, metrics, path, model_dir, wd, wd_bn_bias, train_bn, moms, default_cbs)
    123         self.training,self.create_mbar,self.logger,self.opt,self.cbs = False,True,print,None,L()
    124         if default_cbs: self.add_cbs(L(defaults.callbacks))
--&gt; 125         self.add_cbs(cbs)
    126         self.lock = threading.Lock()
    127         self(&quot;after_create&quot;)

/usr/local/lib/python3.10/dist-packages/fastai/learner.py in add_cbs(self, cbs)
    135 
    136     def add_cbs(self, cbs):
--&gt; 137         L(cbs).map(self.add_cb)
    138         return self
    139 

/usr/local/lib/python3.10/dist-packages/fastcore/foundation.py in map(self, f, *args, **kwargs)
    155     def range(cls, a, b=None, step=None): return cls(range_of(a, b=b, step=step))
    156 
--&gt; 157     def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))
    158     def argwhere(self, f, negate=False, **kwargs): return self._new(argwhere(self, f, negate, **kwargs))
    159     def argfirst(self, f, negate=False):

/usr/local/lib/python3.10/dist-packages/fastcore/basics.py in map_ex(iterable, f, gen, *args, **kwargs)
    877     res = map(g, iterable)
    878     if gen: return res
--&gt; 879     return list(res)
    880 
    881 # %% ../nbs/01_basics.ipynb 352

/usr/local/lib/python3.10/dist-packages/fastcore/basics.py in __call__(self, *args, **kwargs)
    862             if isinstance(v,_Arg): kwargs[k] = args.pop(v.i)
    863         fargs = [args[x.i] if isinstance(x, _Arg) else x for x in self.pargs] + args[self.maxi+1:]
--&gt; 864         return self.func(*fargs, **kwargs)
    865 
    866 # %% ../nbs/01_basics.ipynb 342

/usr/local/lib/python3.10/dist-packages/fastai/learner.py in add_cb(self, cb)
    145         if isinstance(cb, type): cb = cb()
    146         cb.learn = self
--&gt; 147         setattr(self, cb.name, cb)
    148         self.cbs.append(cb)
    149         return self

AttributeError: 'Mixup' object has no attribute 'name'


</code></pre>
","huggingface"
"78744916","How to deploy custom Huggingface models on Azure ML","2024-07-13 20:27:10","","0","101","<azure><huggingface>","<p>Hugging Face models have easy one-click deployment via model catalog. Still, some models such as <a href=""https://huggingface.co/facebook/audiogen-medium"" rel=""nofollow noreferrer"">facebook/audiogen-medium</a>, are not available on the Azure model catalog and do not have <code>Deploy</code> button on Hugging Face.</p>
<p>I followed these official tutorials for deploying custom models:<br />
<a href=""https://learn.microsoft.com/en-us/azure/machine-learning/tutorial-deploy-model?view=azureml-api-2"" rel=""nofollow noreferrer"">Deploy a model as an online endpoint</a><br />
<a href=""https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-online-endpoints?view=azureml-api-2&amp;tabs=cli"" rel=""nofollow noreferrer"">Deploy and score a machine learning model by using an online endpoint</a><br />
but I could not find a tutorial to deploy other Hugging Face models on Azure ML using Azure Python sdk v2.</p>
<p>Since I'm using <a href=""https://github.com/facebookresearch/audiocraft"" rel=""nofollow noreferrer""><code>audiocraft</code></a> package which lets me use the model like this:</p>
<pre class=""lang-py prettyprint-override""><code>from audiocraft.models import AudioGen
    
model = AudioGen.get_pretrained(&quot;facebook/audiogen-medium&quot;)
model.set_generation_params(duration=5)  # generate 5 seconds.
descriptions = ['dog barking', 'sirene of an emergency vehicle', 'footsteps in a corridor']
wav = model.generate(descriptions)  # generates 3 samples.
</code></pre>
<p>I have an error when I want to register the model, I don't exactly know what should I use for the <code>path</code> attribute of the <code>Model</code>, the local path of the downloaded Hugging Face model?</p>
<pre class=""lang-py prettyprint-override""><code>model = Model(
    name=&quot;audiogen&quot;,
    version=&quot;1&quot;,
    type=AssetTypes.CUSTOM_MODEL,
    path=&quot;./score.py&quot;,
    description=&quot;facebook/audiogen model for sound effects generation&quot;,
)

# Register the model
ml_client.models.create_or_update(model)
</code></pre>
<pre><code>---&gt; 10 ml_client.models.create_or_update(model)

File /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/_telemetry/activity.py:289, in monitor_with_activity.&lt;locals&gt;.monitor.&lt;locals&gt;.wrapper(*args, **kwargs)
    285     with tracer.span():
    286         with log_activity(
    287             logger.package_logger, activity_name or f.__name__, activity_type, custom_dimensions
    288         ):
--&gt; 289             return f(*args, **kwargs)
    290 elif hasattr(logger, &quot;package_logger&quot;):
    291     with log_activity(logger.package_logger, activity_name or f.__name__, activity_type, custom_dimensions):

File /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/operations/_model_operations.py:158, in ModelOperations.create_or_update(self, model)
    151     raise ValidationException(
    152         message=msg,
    153         no_personal_data_message=msg,
    154         target=ErrorTarget.MODEL,
    155         error_category=ErrorCategory.USER_ERROR,
    156     )
    157 if model.name is not None:
--&gt; 158     model_properties = self._get_model_properties(model.name)
    159     if model_properties is not None and _is_evaluator(model_properties) != _is_evaluator(model.properties):
    160         if _is_evaluator(model.properties):

File /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/operations/_model_operations.py:815, in ModelOperations._get_model_properties(self, name, version, label)
    813     if version or label:
    814         return self.get(name, version, label).properties
--&gt; 815     return self._get_latest_version(name).properties
    816 except (ResourceNotFoundError, ValidationException):
    817     return None

File /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/operations/_model_operations.py:620, in ModelOperations._get_latest_version(self, name)
    615 def _get_latest_version(self, name: str) -&gt; Model:
    616     &quot;&quot;&quot;Returns the latest version of the asset with the given name.
    617 
    618     Latest is defined as the most recently created, not the most recently updated.
    619     &quot;&quot;&quot;
--&gt; 620     result = _get_latest(
    621         name,
    622         self._model_versions_operation,
    623         self._resource_group_name,
    624         self._workspace_name,
    625         self._registry_name,
    626     )
    627     return Model._from_rest_object(result)

File /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/_utils/_asset_utils.py:853, in _get_latest(asset_name, version_operation, resource_group_name, workspace_name, registry_name, order_by, **kwargs)
    833 result = (
    834     version_operation.list(
    835         name=asset_name,
   (...)
    850     )
    851 )
    852 try:
--&gt; 853     latest = result.next()
    854 except StopIteration:
    855     latest = None

File /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/core/paging.py:123, in ItemPaged.__next__(self)
    121 if self._page_iterator is None:
    122     self._page_iterator = itertools.chain.from_iterable(self.by_page())
--&gt; 123 return next(self._page_iterator)

File /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/core/paging.py:75, in PageIterator.__next__(self)
     73     raise StopIteration(&quot;End of paging&quot;)
     74 try:
---&gt; 75     self._response = self._get_next(self.continuation_token)
     76 except AzureError as error:
     77     if not error.continuation_token:

File /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/_restclient/v2023_08_01_preview/operations/_model_versions_operations.py:427, in ModelVersionsOperations.list.&lt;locals&gt;.get_next(next_link)
    426 def get_next(next_link=None):
--&gt; 427     request = prepare_request(next_link)
    429     pipeline_response = self._client._pipeline.run(  # pylint: disable=protected-access
    430         request,
    431         stream=False,
    432         **kwargs
    433     )
    434     response = pipeline_response.http_response

File /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/_restclient/v2023_08_01_preview/operations/_model_versions_operations.py:371, in ModelVersionsOperations.list.&lt;locals&gt;.prepare_request(next_link)
    368 def prepare_request(next_link=None):
    369     if not next_link:
--&gt; 371         request = build_list_request(
    372             subscription_id=self._config.subscription_id,
    373             resource_group_name=resource_group_name,
    374             workspace_name=workspace_name,
    375             name=name,
    376             api_version=api_version,
    377             skip=skip,
    378             order_by=order_by,
    379             top=top,
    380             version=version,
    381             description=description,
    382             offset=offset,
    383             tags=tags,
    384             properties=properties,
    385             feed=feed,
    386             list_view_type=list_view_type,
    387             stage=stage,
    388             template_url=self.list.metadata['url'],
    389         )
    390         request = _convert_request(request)
    391         request.url = self._client.format_url(request.url)

File /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/_restclient/v2023_08_01_preview/operations/_model_versions_operations.py:63, in build_list_request(subscription_id, resource_group_name, workspace_name, name, **kwargs)
     58 # Construct URL
     59 _url = kwargs.pop(&quot;template_url&quot;, &quot;/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.MachineLearningServices/workspaces/{workspaceName}/models/{name}/versions&quot;)  # pylint: disable=line-too-long
     60 path_format_arguments = {
     61     &quot;subscriptionId&quot;: _SERIALIZER.url(&quot;subscription_id&quot;, subscription_id, 'str', min_length=1),
     62     &quot;resourceGroupName&quot;: _SERIALIZER.url(&quot;resource_group_name&quot;, resource_group_name, 'str', max_length=90, min_length=1),
---&gt; 63     &quot;workspaceName&quot;: _SERIALIZER.url(&quot;workspace_name&quot;, workspace_name, 'str', pattern=r'^[a-zA-Z0-9][a-zA-Z0-9_-]{2,32}$'),
     64     &quot;name&quot;: _SERIALIZER.url(&quot;name&quot;, name, 'str'),
     65 }
     67 _url = _format_url_section(_url, **path_format_arguments)
     69 # Construct parameters

File /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/msrest/serialization.py:652, in Serializer.url(self, name, data, data_type, **kwargs)
    650 data = self._http_component_validation(data, data_type, name, **kwargs)
    651 try:
--&gt; 652     output = self.serialize_data(data, data_type, **kwargs)
    653     if data_type == 'bool':
    654         output = json.dumps(output)

File /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/msrest/serialization.py:760, in Serializer.serialize_data(self, data, data_type, **kwargs)
    749 &quot;&quot;&quot;Serialize generic data according to supplied data type.
    750 
    751 :param data: The data to be serialized.
   (...)
    757 :raises: SerializationError if serialization fails.
    758 &quot;&quot;&quot;
    759 if data is None:
--&gt; 760     raise ValueError(&quot;No value for given attribute&quot;)
    762 try:
    763     if data_type in self.basic_types.values():

ValueError: No value for given attribute
</code></pre>
<p>or just the <code>score.py</code> script which is necassary and is as follows (I used this for now and got the error above):</p>
<pre class=""lang-py prettyprint-override""><code>import json
import os
from audiocraft.models import AudioGen
from audiocraft.data.audio import audio_write
from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient

def init():
    global model, blob_service_client, container_name
    model = AudioGen.get_pretrained('facebook/audiogen-medium')
    model.set_generation_params(duration=5)

    # Initialize Azure Blob Storage client
    connect_str = os.getenv('AZURE_STORAGE_CONNECTION_STRING')
    blob_service_client = BlobServiceClient.from_connection_string(connect_str)
    container_name = 'audiogen_generated_files'

def run(payload: str):
    data = json.loads(payload)
    theme = data[&quot;theme&quot;]
    prompts = [f'{theme}, slow speed', f'{theme}, fast speed', f'{theme} door closing', f'{theme} starting']
    wavs = model.generate(prompts)
    
    urls = []

    for idx, one_wav in enumerate(wavs):
        file_name = f'{theme}_{idx}.wav'
        local_file_path = f'/tmp/{file_name}'
        audio_write(local_file_path, one_wav.cpu(), model.sample_rate, strategy=&quot;loudness&quot;, loudness_compressor=True)
        
        blob_client = blob_service_client.get_blob_client(container=container_name, blob=file_name)
        with open(local_file_path, &quot;rb&quot;) as data:
            blob_client.upload_blob(data, overwrite=True)
        urls.append(blob_client.url)
        os.remove(local_file_path)

    result = {&quot;urls&quot;: urls}
    return result

</code></pre>
<p>This is how I create the endpoint and deployment in the next steps:</p>
<pre class=""lang-py prettyprint-override""><code>endpoint = ManagedOnlineEndpoint(
    name=&quot;audiogen-endpoint-&quot; + str(uuid.uuid4())[:8],
    description=&quot;audiogen inference endpoint&quot;,
    auth_mode=&quot;key&quot;
)
endpoint = ml_client.online_endpoints.begin_create_or_update(endpoint).result()

deployement = ManagedOnlineDeployment(
    name=&quot;audiogen-deployment-mo&quot;,
    endpoint_name=endpoint.name,
    model=ml_client.models.get(name=model.name, version=&quot;1&quot;),
    instance_count=1,
)
deployement = ml_client.begin_create_or_update(deployement).result()
</code></pre>
<p>I want to know the proper way to deploy the model, also am I doing the other steps correcly?</p>
","huggingface"
"78743616","apply different learning rate for introduced tokens in the transformers library","2024-07-13 10:54:29","","0","25","<pytorch><token><large-language-model><huggingface>","<p>Say I want to introduce a few new tokens into the vocabulary of an existing model, and I want these tokens to have a different learning rate compared to the rest of the model's parameters during training. How can I implement this in the Transformers library?</p>
<p>Idealy I should still be able to use the transformer.Trainer class for the training.</p>
","huggingface"
"78732094","Layoutlmv3 preparing data issue","2024-07-10 17:52:33","","0","38","<python><huggingface>","<p>I'm trying to follow along with a layoutlmv3 paper; however, I am using a public dataset of invoices so I could have a better understanding of how it uses layoutlmv3. At the data preparation section I keep running into a &quot;KeyError: pixel_values&quot; error, and whenever I try to remove the pixel_values column I get a &quot;jpeg image file not recognized error&quot;. The specific paper I'm following is <a href=""https://medium.com/@tejpal.abhyuday/information-extraction-part-3-9c2487ec4930"" rel=""nofollow noreferrer"">here</a>. The specific dataset I'm using is &quot;mathieu1256/FATURA2-invoices&quot; and the specific model I'm using is &quot;microsoft/Layoutlmv3-base&quot; because I couldn't find the layoutlmv3-base-uncased version on hugging face and my program seems to believe it doesn't exist.</p>
<p>Below is my code, there shouldn't be anything too crazy since I'm simply testing out the code and getting it to work.</p>
<pre><code>
#creates the id for each label and creates a dictionary, setting it such that {&quot;O&quot;: 0} and so on
id2label = [&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, 
            &quot;E&quot;, &quot;F&quot;, &quot;G&quot;,
            &quot;H&quot;, &quot;I&quot;, &quot;J&quot;,
            &quot;K&quot;, &quot;L&quot;, &quot;M&quot;,
            &quot;NO&quot;]

#Enumerates at 1, 1-13 for the ner_tags
label2id = {l:i for i, l in enumerate(id2label)}


#creating the processor and model
#selects the device, either &quot;cuda&quot;: A graphics device on the GPU, or the cpu
device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)

processor = LayoutLMv3Processor.from_pretrained(
    &quot;microsoft/layoutlmv3-base&quot;, apply_ocr=False
)

#this is also where you classify the number of labels you desire
model = LayoutLMv3ForTokenClassification.from_pretrained(
    &quot;microsoft/layoutlmv3-base&quot;, num_labels=len(id2label)
)

model.to(device)


#preparing the data
def encode_training_example(examples):
    #references to columns within the dataset
    #bewary that img is like that because it's already opened, if it's a path you may want to use Image.open(path)
    #change img to path if they are path variables
    images = [img.convert(&quot;RGB&quot;) for img in examples['image']]
    word_labels = examples['ner_tags']
    boxes = examples['bboxes']
    tokens = examples['tokens']
    encoded_inputs = processor(
        images, tokens, word_labels=word_labels, boxes=boxes,
        padding=&quot;max_length&quot;, truncation=True
    )

    return encoded_inputs

training_features = Features({
    'image': Array3D(dtype=&quot;int64&quot;, shape=(3, 595, 841)),
    'input_ids': Sequence(feature=Value(dtype='int64')),
    'attention_mask': Sequence(Value(dtype='int64')),
    'token_type_ids': Sequence(Value(dtype='int64')),
    'bbox': Array2D(dtype=&quot;int64&quot;, shape=(8600, 5)),
    'labels': Sequence(ClassLabel(names=id2label)),
})

def training_dataloader_from_dataset(dataset):
    encoded_data = dataset.map(
        encode_training_example, batched=True, remove_columns=xdata_invoices_dataset['train'].column_names.append(&quot;pixel_values&quot;),
        features=training_features
    )
    encoded_data.set_format(type='torch', device=device)
    dataloader = torch.utils.data.DataLoader(encoded_data, batch_size=5, shuffle=True)
    batch = next(iter(dataloader))

    return dataloader

#training the ai dataset
train_dataloader = training_dataloader_from_dataset(xdata_invoices_dataset['train'])

#validating the ai dataset
valid_dataloader = training_dataloader_from_dataset(xdata_invoices_dataset['test'])
</code></pre>
<p>this code raises a
<code>Could not convert &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=595x841 at 0x2319F352690&gt; with type JpegImageFile: was not a sequence or recognized null for conversion to list type</code>
error</p>
<p>However if you remove &quot;.append(&quot;pixel_values&quot;)&quot; in the dataset.map() function within &quot;def training_dataloeader_from_dataset(dataset)&quot; it raises a &quot;Key Error: 'pixel_values'&quot; error.</p>
<p>I'm stumped I don't understand what the error is, the given paper doesn't seem to run into this error, and any leads I get are either not helpful or completely off topic. I would love to get some input on this because I'm genuinely lost!</p>
","huggingface"
"78725798","""Can't load feature extractor for 'facebook/wav2vec2-large-960h-lv60-self"" Error when I try to extract Audio Features","2024-07-09 13:07:29","","0","18","<machine-learning><audio><deep-learning><feature-extraction><huggingface>","<p><a href=""https://i.sstatic.net/vW7lwJo7.png"" rel=""nofollow noreferrer"">The Code</a></p>
<pre><code>processor = Wav2Vec2Processor.from_pretrained(&quot;facebook/wav2vec2-large-960h-lv60-self&quot;)
model = Wav2Vec2Model.from_pretrained(&quot;facebook/wav2vec2-large-960h-lv60-self&quot;)
model.config.output_hidden_states = True
</code></pre>
<p><a href=""https://i.sstatic.net/rgHdjJkZ.png"" rel=""nofollow noreferrer"">The Error</a></p>
<pre><code>Can't load feature extractor for 'facebook/wav2vec2-large-960h-lv60-self'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'facebook/wav2vec2-large-960h-lv60-self' is the correct path to a directory containing a preprocessor_config.json file
</code></pre>
<p>When I try to extract features using HuggingFace, I get this error. I tried to fix this taking int consideration the Tokenizer error. But I am not able to fix it. Can someone help?</p>
<p>I need to extract the audio features using this Model.</p>
","huggingface"
"78725580","ModuleNotFoundError when importing HuggingFaceLLM from llama_index.core.llms.huggingface","2024-07-09 12:25:01","","0","68","<large-language-model><huggingface><llama-index>","<p>I’m trying to import HuggingFaceLLM using the following line of code:</p>
<pre><code>from llama_index.core.llms.huggingface import HuggingFaceLLM
</code></pre>
<p>I know that llamaindex keeps updating, and previously this import worked with:</p>
<pre><code>from llama_index.llms.huggingface import HuggingFaceLLM
</code></pre>
<p>However, I have tried all possible ways but keep getting the following error:</p>
<pre><code>ModuleNotFoundError: No module named 'llama_index.core.llms.huggingface'
</code></pre>
<p>Has anyone faced this issue or knows how to fix it? Any help would be appreciated. Thank you in advance!</p>
","huggingface"
"78722366","Huggingface evaluate with f2","2024-07-08 18:14:31","","0","12","<metrics><huggingface><evaluate>","<p>Is there a way to calculate f2 using huggingface's evaluate library?</p>
<p>ChatGPT instructed me to implement:</p>
<p><code>f2_metric = evaluate.load(&quot;f1&quot;, beta=2) </code></p>
<p>However, the output was identical to that of the f1 metric, so I wasn't sure if the score was being calculated properly. Thank you so much!</p>
","huggingface"
"78719235","datasets package from pip causing a segfault on MacOS?","2024-07-08 05:35:55","","1","72","<pip><segmentation-fault><huggingface><huggingface-datasets><huggingface-hub>","<p>I'm using pip version 24.1.2 and Python 3.12.4. The installation seemingly goes fine. However, when importing the package, like in the line</p>
<pre><code>from datasets import load_dataset
</code></pre>
<p>I'll see</p>
<pre><code>zsh: segmentation fault
</code></pre>
<p>just from the import line. I've tried on virtual environments but am encountering the same thing. The Conda installation works, but I'd like to use pip if at all possible. I see similar questions have been asked before, but not in nearly seven years, and the accepted answer suggested a problem with the package itself.</p>
","huggingface"
"78718676","What is the difference, if any, between model.half() and model.to(dtype=torch.float16) in huggingface-transformers?","2024-07-07 23:33:43","","0","38","<python><huggingface-transformers><huggingface><quantization><half-precision-float>","<p>Example:</p>
<pre><code># pip install transformers
from transformers import AutoModelForTokenClassification, AutoTokenizer

# Load model
model_path = 'huawei-noah/TinyBERT_General_4L_312D'
model = AutoModelForTokenClassification.from_pretrained(model_path)
tokenizer = AutoTokenizer.from_pretrained(model_path)

# Convert the model to FP16
model.half()
</code></pre>
<p>vs.</p>
<pre><code>model.to(dtype=torch.float16)
</code></pre>
<p>What is the difference, if any, between model.half() and model.to(dtype=torch.float16) in huggingface-transformers?</p>
","huggingface"
"78713551","I load a float32 Hugging Face model, cast it to float16, and save it. How can I load it as float16?","2024-07-05 23:58:06","78713569","-2","145","<python><machine-learning><huggingface-transformers><huggingface><half-precision-float>","<p>I load a huggingface-transformers float32 model, cast  it to float16, and save it. How can I load it as float16?</p>
<p>Example:</p>
<pre><code># pip install transformers
from transformers import AutoModelForTokenClassification, AutoTokenizer

# Load model
model_path = 'huawei-noah/TinyBERT_General_4L_312D'
model = AutoModelForTokenClassification.from_pretrained(model_path)
tokenizer = AutoTokenizer.from_pretrained(model_path)

# Convert the model to FP16
model.half()

# Check model dtype
def print_model_layer_dtype(model):
    print('\nModel dtypes:')
    for name, param in model.named_parameters():
        print(f&quot;Parameter: {name}, Data type: {param.dtype}&quot;)

print_model_layer_dtype(model)
save_directory = 'temp_model_SE'
model.save_pretrained(save_directory)

model2 = AutoModelForTokenClassification.from_pretrained(save_directory, local_files_only=True)
print('\n\n##################')
print(model2)
print_model_layer_dtype(model2)
</code></pre>
<p>In this example, <code>model2</code> loads as a <code>float32</code> model (as shown by <code>print_model_layer_dtype(model2)</code>), even though <code>model2</code> was saved as float16 (as <a href=""https://i.sstatic.net/Um45egKE.png"" rel=""nofollow noreferrer"">shown in <code>config.json</code></a>). What is the proper way to load it as float16?</p>
<p>Tested with <code>transformers==4.36.2</code> and Python 3.11.7 on Windows 10.</p>
","huggingface"
"78712878","Size mismatch for embed_out.weight: copying a param with shape torch.Size([0]) from checkpoint - Huggingface PyTorch","2024-07-05 18:29:16","78722271","0","189","<pytorch><huggingface-transformers><large-language-model><huggingface>","<p>I want to finetune an LLM. I am able to successfully finetune LLM. But when reload the model after save, gets error. Below is the code</p>
<pre><code>import argparse
import numpy as np
import torch
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM

from trl import DPOTrainer, DPOConfig
def preprocess_data(item):
    return {
        'prompt': 'Instruct: ' + item['prompt'] + '\n',
        'chosen': 'Output: ' + item['chosen'],
        'rejected': 'Output: ' + item['rejected']
    }        

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument(&quot;--epochs&quot;, type=int, default=1)
    parser.add_argument(&quot;--beta&quot;, type=float, default=0.1)
    parser.add_argument(&quot;--batch_size&quot;, type=int, default=4)
    parser.add_argument(&quot;--lr&quot;, type=float, default=1e-6)
    parser.add_argument(&quot;--seed&quot;, type=int, default=2003)
    parser.add_argument(&quot;--model_name&quot;, type=str, default=&quot;EleutherAI/pythia-14m&quot;)
    parser.add_argument(&quot;--dataset_name&quot;, type=str, default=&quot;jondurbin/truthy-dpo-v0.1&quot;)
    parser.add_argument(&quot;--local_rank&quot;, type=int, default=0)

    args = parser.parse_args()

    # Determine device based on local_rank
    device = torch.device(&quot;cuda&quot;, args.local_rank) if torch.cuda.is_available() else torch.device(&quot;cpu&quot;)


    tokenizer = AutoTokenizer.from_pretrained(args.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    model = AutoModelForCausalLM.from_pretrained(args.model_name).to(device)
    ref_model = AutoModelForCausalLM.from_pretrained(args.model_name).to(device)

    dataset = load_dataset(args.dataset_name, split=&quot;train&quot;)
    dataset = dataset.map(preprocess_data)

    # Split the dataset into training and validation sets
    dataset = dataset.train_test_split(test_size=0.1, seed=args.seed)
    train_dataset = dataset['train']
    val_dataset = dataset['test']

    training_args = DPOConfig(
        learning_rate=args.lr,
        num_train_epochs=args.epochs,
        per_device_train_batch_size=args.batch_size,
        logging_steps=10,
        remove_unused_columns=False,
        max_length=1024,
        max_prompt_length=512,
        fp16=True        
    )

    

    # Verify and print embedding dimensions before finetuning
    print(&quot;Base model embedding dimension:&quot;, model.config.hidden_size)

    model.train()
    ref_model.eval()

    dpo_trainer = DPOTrainer(
        model,
        ref_model,
        beta=args.beta,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        tokenizer=tokenizer,
        args=training_args,
    )

    dpo_trainer.train()
    # Evaluate
    evaluation_results = dpo_trainer.evaluate()
    print(&quot;Evaluation Results:&quot;, evaluation_results)

    save_model_name = 'finetuned_model'
    model.save_pretrained(save_model_name)

if __name__ == &quot;__main__&quot;:
    main()
</code></pre>
<p>Error I was getting as below</p>
<pre><code>    return model_class.from_pretrained(
    File &quot;/.local/lib/python3.10/site-packages/transformers/modeling_utils.py&quot;, line 3838, in from_pretrained
        ) = cls._load_pretrained_model(
    File &quot;/.local/lib/python3.10/site-packages/transformers/modeling_utils.py&quot;, line 4349, in _load_pretrained_model
        raise RuntimeError(f&quot;Error(s) in loading state_dict for {model.__class__.__name__}:\n\t{error_msg}&quot;)
        RuntimeError: Error(s) in loading state_dict for GPTNeoXForCausalLM:
            size mismatch for gpt_neox.embed_in.weight: copying a param with shape torch.Size([0]) from checkpoint, the shape in current model is torch.Size([50304, 128]).
            size mismatch for embed_out.weight: copying a param with shape torch.Size([0]) from checkpoint, the shape in current model is torch.Size([50304, 128]).
            You may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.
</code></pre>
<p>After finetuning,  model works perfectly. But after reloading the saved trained model its not working. Any idea why gets this error when reloading the model ?</p>
","huggingface"
"78712655","Regarding Tokenizer of Hugging Face","2024-07-05 17:15:32","","0","16","<machine-learning><deep-learning><huggingface>","<p>Hard time understanding the working of tokenizer</p>
<pre><code>from transformers import AutoModelForSequenceClassification,AutoTokenizer #hugging face libraries
tkz = AutoTokenizer.from_pretrained(model)
</code></pre>
<p>The function</p>
<pre><code>def tkz_func(x): return tkz(x['input'])
</code></pre>
<p>works perfectly when we apply it to the datasets, returns updated dataset with input_ids, token_type_ids, attention_masks</p>
<p>When we apply it to the dataframe <code>df.apply(tkz_func,axis=1)</code> it just returns the list of row names for all the row values
<code>[input_ids,token_type_ids,attention_masks]</code>.</p>
<p>Why?</p>
","huggingface"
"78704628","Huggingface library not being able to replace separators in create_documents: ""AttributeError: 'dict' object has no attribute 'replace'""","2024-07-04 00:39:08","","0","73","<split><chatbot><langchain><huggingface><retrieval-augmented-generation>","<p>I'm a beginner in the chatbot developer world and currently building a rag code to create a context based chatbot, but I keep getting this error, I believe it happens when the text is being split, because even after the function is called, the text remains with the &quot;\n&quot; separators.
The last line of the traceback occurs in the huggingface library.</p>
<p>The traceback:</p>
<pre><code>Traceback (most recent call last):
  File &quot;c:\Users\sophi\miniconda3\envs\ambiente3.9\lib\runpy.py&quot;, line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File &quot;c:\Users\sophi\miniconda3\envs\ambiente3.9\lib\runpy.py&quot;, line 87, in _run_code
    exec(code, run_globals)
  File &quot;c:\Users\sophi\.vscode\extensions\ms-python.debugpy-2024.6.0-win32-x64\bundled\libs\debugpy\__main__.py&quot;, line 39, in &lt;module&gt;
    cli.main()
  File &quot;c:\Users\sophi\.vscode\extensions\ms-python.debugpy-2024.6.0-win32-x64\bundled\libs\debugpy/..\debugpy\server\cli.py&quot;, line 430, in main
    run()
  File &quot;c:\Users\sophi\.vscode\extensions\ms-python.debugpy-2024.6.0-win32-x64\bundled\libs\debugpy/..\debugpy\server\cli.py&quot;, line 284, in run_file
    runpy.run_path(target, run_name=&quot;__main__&quot;)
  File &quot;c:\Users\sophi\.vscode\extensions\ms-python.debugpy-2024.6.0-win32-x64\bundled\libs\debugpy\_vendored\pydevd\_pydevd_bundle\pydevd_runpy.py&quot;, line 321, in run_path
    return _run_module_code(code, init_globals, run_name,
  File &quot;c:\Users\sophi\.vscode\extensions\ms-python.debugpy-2024.6.0-win32-x64\bundled\libs\debugpy\_vendored\pydevd\_pydevd_bundle\pydevd_runpy.py&quot;, line 135, in _run_module_code
    _run_code(code, mod_globals, init_globals,
  File &quot;c:\Users\sophi\.vscode\extensions\ms-python.debugpy-2024.6.0-win32-x64\bundled\libs\debugpy\_vendored\pydevd\_pydevd_bundle\pydevd_runpy.py&quot;, line 124, in _run_code
    exec(code, run_globals)
  File &quot;c:\Users\sophi\Documents\ProjetosdePesquisa\Projeto-de-Pesquisa-SOLIRIS\llm_rag _ver4\utils\rag.py&quot;, line 130, in &lt;module&gt;
    main()
  File &quot;c:\Users\sophi\Documents\ProjetosdePesquisa\Projeto-de-Pesquisa-SOLIRIS\llm_rag _ver4\utils\rag.py&quot;, line 126, in main
    response = qa.invoke({&quot;input&quot;: {&quot;context&quot;: context, &quot;question&quot;: question}})
  File &quot;c:\Users\sophi\miniconda3\envs\ambiente3.9\lib\site-packages\langchain_core\runnables\base.py&quot;, line 4588, in invoke
    return self.bound.invoke(
  File &quot;c:\Users\sophi\miniconda3\envs\ambiente3.9\lib\site-packages\langchain_core\runnables\base.py&quot;, line 2505, in invoke
    input = step.invoke(input, config, **kwargs)
  File &quot;c:\Users\sophi\miniconda3\envs\ambiente3.9\lib\site-packages\langchain_core\runnables\passthrough.py&quot;, line 469, in invoke
    return self._call_with_config(self._invoke, input, config, **kwargs)
  File &quot;c:\Users\sophi\miniconda3\envs\ambiente3.9\lib\site-packages\langchain_core\runnables\base.py&quot;, line 1599, in _call_with_config       
    context.run(
  File &quot;c:\Users\sophi\miniconda3\envs\ambiente3.9\lib\site-packages\langchain_core\runnables\config.py&quot;, line 380, in call_func_with_variable_args
    return func(input, **kwargs)  # type: ignore[call-arg]
  File &quot;c:\Users\sophi\miniconda3\envs\ambiente3.9\lib\site-packages\langchain_core\runnables\passthrough.py&quot;, line 456, in _invoke
    **self.mapper.invoke(
  File &quot;c:\Users\sophi\miniconda3\envs\ambiente3.9\lib\site-packages\langchain_core\runnables\base.py&quot;, line 3152, in invoke
    output = {key: future.result() for key, future in zip(steps, futures)}
  File &quot;c:\Users\sophi\miniconda3\envs\ambiente3.9\lib\site-packages\langchain_core\runnables\base.py&quot;, line 3152, in &lt;dictcomp&gt;
    output = {key: future.result() for key, future in zip(steps, futures)}
  File &quot;c:\Users\sophi\miniconda3\envs\ambiente3.9\lib\concurrent\futures\_base.py&quot;, line 446, in result
    return self.__get_result()
  File &quot;c:\Users\sophi\miniconda3\envs\ambiente3.9\lib\concurrent\futures\_base.py&quot;, line 391, in __get_result
    raise self._exception
  File &quot;c:\Users\sophi\miniconda3\envs\ambiente3.9\lib\concurrent\futures\thread.py&quot;, line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File &quot;c:\Users\sophi\miniconda3\envs\ambiente3.9\lib\site-packages\langchain_core\runnables\base.py&quot;, line 4588, in invoke
    return self.bound.invoke(
  File &quot;c:\Users\sophi\miniconda3\envs\ambiente3.9\lib\site-packages\langchain_core\runnables\base.py&quot;, line 2507, in invoke
    input = step.invoke(input, config)
  File &quot;c:\Users\sophi\miniconda3\envs\ambiente3.9\lib\site-packages\langchain_core\retrievers.py&quot;, line 221, in invoke
    raise e
  File &quot;c:\Users\sophi\miniconda3\envs\ambiente3.9\lib\site-packages\langchain_core\retrievers.py&quot;, line 214, in invoke
    result = self._get_relevant_documents(
  File &quot;c:\Users\sophi\miniconda3\envs\ambiente3.9\lib\site-packages\langchain_core\vectorstores.py&quot;, line 797, in _get_relevant_documents    
    docs = self.vectorstore.similarity_search(query, **self.search_kwargs)
  File &quot;c:\Users\sophi\miniconda3\envs\ambiente3.9\lib\site-packages\langchain_community\vectorstores\chroma.py&quot;, line 349, in similarity_search
    docs_and_scores = self.similarity_search_with_score(
  File &quot;c:\Users\sophi\miniconda3\envs\ambiente3.9\lib\site-packages\langchain_community\vectorstores\chroma.py&quot;, line 438, in similarity_search_with_score
    query_embedding = self._embedding_function.embed_query(query)
  File &quot;c:\Users\sophi\miniconda3\envs\ambiente3.9\lib\site-packages\langchain_huggingface\embeddings\huggingface.py&quot;, line 102, in embed_query
    return self.embed_documents([text])[0]
  File &quot;c:\Users\sophi\miniconda3\envs\ambiente3.9\lib\site-packages\langchain_huggingface\embeddings\huggingface.py&quot;, line 81, in embed_documents
    texts = list(map(lambda x: x.replace(&quot;\n&quot;, &quot; &quot;), texts))
  File &quot;c:\Users\sophi\miniconda3\envs\ambiente3.9\lib\site-**packages\langchain_huggingface\embeddings\huggingface.py&quot;, line 81, in &lt;lambda&gt;   
    texts = list(map(lambda x: x.replace(&quot;\n&quot;, &quot; &quot;), texts))
AttributeError: 'dict' object has no attribute 'replace'**
</code></pre>
<h1>This is my entire code: (except for the groq api)</h1>
<pre><code>
import sys
import os
from langchain_core.prompts import PromptTemplate 
from langchain_groq import ChatGroq
from langchain.chains import create_retrieval_chain
from langchain_community.document_loaders import TextLoader
from sentence_transformers import SentenceTransformer
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_community.vectorstores import Chroma
from langchain.docstore.document import Document
import PyPDF2
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.llms import CTransformers

# Caminho para o arquivo PDF
PDF_PATH = 'pdf_handling/entrevistas.pdf'

# Caminho para salvar os dados do ChromaDB
CHROMA_DATA_PATH = &quot;chroma_data&quot;

# Modelo de embeddings
EMBED_MODEL = &quot;all-MiniLM-L6-v2&quot;

# Nome da coleção
COLLECTION_NAME = &quot;ruth_docs&quot;

def dict_to_string(input_dict):
    # Convert the dictionary into a string representation
    # This uses a list comprehension to create a list of &quot;key: value&quot; strings
    # and then joins them with a comma and a space.
    return ', '.join([f&quot;{key}: {value}&quot; for key, value in input_dict.items()])

# Função para extrair texto de um PDF e retornar uma lista de objetos Document
def extract_text_from_pdf(file_path):
    try:
        with open(file_path, 'rb') as pdf_file:
            pdf = PyPDF2.PdfReader(pdf_file)
            paginas = len(pdf.pages)
            text = &quot;&quot;
            for i in range(paginas):
                page = pdf.pages[i]
                text += page.extract_text()
            # print(type(text))
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=500,
                chunk_overlap=50,
                length_function=len,
                separators=['\n\n\n','\n\n','\n', ' ', '']
            )
            documents = text_splitter.create_documents([text])
            splitted_documents = text_splitter.split_documents(documents)
            # print(documents)
            # print(&quot;----------------------  vs  ---------------------&quot;)
            # print(splitted_documents)
            return splitted_documents
        
    except FileNotFoundError:
        print(&quot;Arquivo não encontrado&quot;)
        return []

class criar_vectordb:

    def save_db(self, documents, embeddings, db_path):
        self.db_path = db_path
        self.embeddings = embeddings
        self.documents = documents
        input=self.documents
        vectordb = Chroma.from_documents(input, self.embeddings, persist_directory=self.db_path)
        vectordb = None
        vectordb = Chroma(db_path, embeddings)

        return vectordb
    
embeddings = HuggingFaceEmbeddings(model_name=&quot;sentence-transformers/all-MiniLM-L6-v2&quot;, model_kwargs={'device':'cpu'})

# Extraindo texto do PDF e criando a base de dados vetorial
documents = extract_text_from_pdf(PDF_PATH)

vectordb = criar_vectordb().save_db(documents, embeddings, CHROMA_DATA_PATH)

os.environ[&quot;GROQ_API_KEY&quot;] = &quot;-&quot;

ruth_prompt_template = &quot;&quot;&quot;
                            Você é um assistente virtual de RH utilizando documentos para embasar sua resposta sempre em fatos,
                            Use as informações presentes no documento para responder a resposta do candidato,
                            sua resposta deve ser o mais semelhante possível com a descrição presente nos documentos
                            
                            contexto: {context}
                            pergunta: {question}
                            
                            Apenas retorne as respostas úteis em ajudar na avaliação e seleção de candidatos e nada mais, usando uma linguagem gentil e empática.
                            Sempre responda em português, uma descrição em texto contínua, além disso adicione
                            um ou mais emojis às vezes para demonstrar empatia e emoção.
                            
                            
                            &quot;&quot;&quot;

prompt = PromptTemplate(template=ruth_prompt_template, input_variables=['context', 'question'])

'''
llm = CTransformers(
        model = &quot;model/llama-2-7b-chat.ggmlv3.q8_0.bin&quot;,
        model_type = &quot;llama&quot;,
        config={'max_new_tokens': 512, 
                'temperature': 0.03,
                'context_length': 1000,
                'repetition_penalty': 1.15}
        )
'''

llm = ChatGroq(model_name=&quot;llama3-70b-8192&quot;, api_key=os.environ[&quot;GROQ_API_KEY&quot;])

retriever = vectordb.as_retriever(search_kwargs={&quot;k&quot;: 2})
combine_docs_chain = create_stuff_documents_chain(
    llm, prompt
)

qa = create_retrieval_chain(retriever, combine_docs_chain)

# Main
def main():
    # Exemplo de uso
    context = &quot;Feedback negativo&quot;
    question = &quot;Como você lida com feedback negativo?&quot;
    response = qa.invoke({&quot;input&quot;: {&quot;context&quot;: context, &quot;question&quot;: question}})
    print(response)

if __name__ == &quot;__main__&quot;:
    main()
</code></pre>
<h1>This is the huggingface file:</h1>
<pre><code>from typing import Any, Dict, List, Optional

from langchain_core.embeddings import Embeddings
from langchain_core.pydantic_v1 import BaseModel, Extra, Field

DEFAULT_MODEL_NAME = &quot;sentence-transformers/all-mpnet-base-v2&quot;


class HuggingFaceEmbeddings(BaseModel, Embeddings):
    &quot;&quot;&quot;HuggingFace sentence_transformers embedding models.

    To use, you should have the ``sentence_transformers`` python package installed.

    Example:
        .. code-block:: python

            from langchain_huggingface import HuggingFaceEmbeddings

            model_name = &quot;sentence-transformers/all-mpnet-base-v2&quot;
            model_kwargs = {'device': 'cpu'}
            encode_kwargs = {'normalize_embeddings': False}
            hf = HuggingFaceEmbeddings(
                model_name=model_name,
                model_kwargs=model_kwargs,
                encode_kwargs=encode_kwargs
            )
    &quot;&quot;&quot;

    client: Any  #: :meta private:
    model_name: str = DEFAULT_MODEL_NAME
    &quot;&quot;&quot;Model name to use.&quot;&quot;&quot;
    cache_folder: Optional[str] = None
    &quot;&quot;&quot;Path to store models. 
    Can be also set by SENTENCE_TRANSFORMERS_HOME environment variable.&quot;&quot;&quot;
    model_kwargs: Dict[str, Any] = Field(default_factory=dict)
    &quot;&quot;&quot;Keyword arguments to pass to the Sentence Transformer model, such as `device`,
    `prompts`, `default_prompt_name`, `revision`, `trust_remote_code`, or `token`.
    See also the Sentence Transformer documentation: https://sbert.net/docs/package_reference/SentenceTransformer.html#sentence_transformers.SentenceTransformer&quot;&quot;&quot;
    encode_kwargs: Dict[str, Any] = Field(default_factory=dict)
    &quot;&quot;&quot;Keyword arguments to pass when calling the `encode` method of the Sentence
    Transformer model, such as `prompt_name`, `prompt`, `batch_size`, `precision`,
    `normalize_embeddings`, and more.
    See also the Sentence Transformer documentation: https://sbert.net/docs/package_reference/SentenceTransformer.html#sentence_transformers.SentenceTransformer.encode&quot;&quot;&quot;
    multi_process: bool = False
    &quot;&quot;&quot;Run encode() on multiple GPUs.&quot;&quot;&quot;
    show_progress: bool = False
    &quot;&quot;&quot;Whether to show a progress bar.&quot;&quot;&quot;

    def __init__(self, **kwargs: Any):
        &quot;&quot;&quot;Initialize the sentence_transformer.&quot;&quot;&quot;
        super().__init__(**kwargs)
        try:
            import sentence_transformers  # type: ignore[import]

        except ImportError as exc:
            raise ImportError(
                &quot;Could not import sentence_transformers python package. &quot;
                &quot;Please install it with `pip install sentence-transformers`.&quot;
            ) from exc

        self.client = sentence_transformers.SentenceTransformer(
            self.model_name, cache_folder=self.cache_folder, **self.model_kwargs
        )

    class Config:
        &quot;&quot;&quot;Configuration for this pydantic object.&quot;&quot;&quot;

        extra = Extra.forbid

    def embed_documents(self, texts: List[str]) -&gt; List[List[float]]:
        &quot;&quot;&quot;Compute doc embeddings using a HuggingFace transformer model.

        Args:
            texts: The list of texts to embed.

        Returns:
            List of embeddings, one for each text.
        &quot;&quot;&quot;
        import sentence_transformers  # type: ignore[import]

        texts = list(map(lambda x: x.replace(&quot;\n&quot;, &quot; &quot;), texts))
        if self.multi_process:
            pool = self.client.start_multi_process_pool()
            embeddings = self.client.encode_multi_process(texts, pool)
            sentence_transformers.SentenceTransformer.stop_multi_process_pool(pool)
        else:
            embeddings = self.client.encode(
                texts, show_progress_bar=self.show_progress, **self.encode_kwargs
            )

        return embeddings.tolist()

    def embed_query(self, text: str) -&gt; List[float]:
        &quot;&quot;&quot;Compute query embeddings using a HuggingFace transformer model.

        Args:
            text: The text to embed.

        Returns:
            Embeddings for the text.
        &quot;&quot;&quot;
        return self.embed_documents([text])[0]
</code></pre>
<p>While debugging, I tried to use split_text() and split_documents() instead of create_documents() and it also didn't work, all of them give me the same output: this error, and my text still containing all of the &quot;\n&quot;. I don't know if it could be something else in the code, as this is the only part that deals with separators.
Please help!
Thank you!</p>
","huggingface"
"78704419","How to implement bind_tools to custom LLM from huggingface pipeline(Llama-3) for a custom agent","2024-07-03 22:35:09","","1","295","<python><langchain><huggingface>","<p>Example Code</p>
<pre class=""lang-py prettyprint-override""><code>name = &quot;meta-llama/Meta-Llama-3-8B-Instruct&quot;

auth_token = &quot;&quot;

tokenizer = AutoTokenizer.from_pretrained(name,use_auth_token=auth_token)

bnb_config = BitsAndBytesConfig(
    load_in_8bit=True,
)

model_config = AutoConfig.from_pretrained(
    name,
    use_auth_token=auth_token,
    tempreature=0.1,
)

model = AutoModelForCausalLM.from_pretrained(
    name,
    trust_remote_code=True,
    config=model_config,
    quantization_config=bnb_config,
    device_map='auto',
    use_auth_token=auth_token,
)

streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)

pipe = pipeline(&quot;text-generation&quot;, model=model, tokenizer=tokenizer, max_new_tokens=4096, device_map=&quot;auto&quot;, streamer = streamer)
llm = HuggingFacePipeline(pipeline=pipe)

@tool
def some_custom_tool(input_string: str) -&gt; str:
    &quot;&quot;&quot;Executes some work and returns a success message if successfull else it return the error message&quot;&quot;&quot;
    return &quot;SUCCESS&quot;


tools = [some_custom_tool]
prompt = ChatPromptTemplate.from_messages(
    [
        (
            &quot;system&quot;,
            f&quot;&quot;&quot;
                You are an Assistant......
            &quot;&quot;&quot;,
        ),
        (&quot;user&quot;, &quot;{input}&quot;),
        MessagesPlaceholder(variable_name=&quot;agent_scratchpad&quot;),
    ]
)
            
llm_with_tools = llm.bind_tools(tools)

            

agent = (
    {
        &quot;input&quot;: lambda x: x[&quot;input&quot;],
        &quot;agent_scratchpad&quot;: lambda x: format_to_openai_tool_messages(
            x[&quot;intermediate_steps&quot;]
        ),
    }
    | prompt
    | llm
    | JsonOutputParser()
)
            

agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, return_intermediate_steps= True)
</code></pre>
<p>Description</p>
<p>I am trying to bind a custom tool with the LLM just like ChatOpenAI but i am getting the following error. It looks like the bind_tools does exist in HuggingFacePipeline. Is there a way to bind a custom tool to an LLM from HuggingFacePipeline?</p>
<p><code>AttributeError: 'HuggingFacePipeline' object has no attribute 'bind_tools'</code></p>
<p>System Info:</p>
<pre><code>langchain==0.2.6
langchain-community==0.2.6
langchain-core==0.2.11
langchain-openai==0.1.14
langchain-text-splitters==0.2.2
Python 3.10.13
</code></pre>
<p>I am doing this on Kaggle GPU t4x2</p>
","huggingface"
"78693678","HuggingFace pipeline doesn't use multiple GPUs","2024-07-01 18:14:55","","1","71","<python><huggingface-transformers><langchain><large-language-model><huggingface>","<p>I made a RAG app that basically answers user questions based on provided data, it works fine on GPU and a single GPU. I want to deploy it on multiple GPUs (4 T4s) but I always get CUDA out of Memory error on pipeline.</p>
<p>I tried using &quot;auto&quot; keyword too but Langchain does not let me use it as keyword.
I used Langchain as main framework, my code looks like this:</p>
<pre class=""lang-py prettyprint-override""><code>from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline, HuggingFaceEmbeddings
MODEL_NAME=&quot;mistralai/Mistral-7B-Instruct-v0.3&quot;
pipe = HuggingFacePipeline.from_model_id(
                           model_id=MODEL_NAME,
                           device=0,
                           model_kwargs={&quot;torch_dtype&quot;:torch.float16},
                           task=&quot;text-generation&quot;)
llm = ChatHuggingFace(llm=pipe)

embedding = HuggingFaceEmbeddings(model_name=MODEL_NAME,
                                  model_kwargs={&quot;device&quot;:&quot;cuda:1&quot;},
                                  multi_process=True,
                                  )
</code></pre>
","huggingface"
"78693093","How can I make my Hugging Face fine-tuned model's config.json file reference a specific revision/commit from the original pretrained model?","2024-07-01 15:35:28","","0","96","<nlp><huggingface-transformers><bert-language-model><huggingface><huggingface-hub>","<p>I uploaded this model: <a href=""https://huggingface.co/pamessina/CXRFE"" rel=""nofollow noreferrer"">https://huggingface.co/pamessina/CXRFE</a>, which is a fine-tuned version of this model: <a href=""https://huggingface.co/microsoft/BiomedVLP-CXR-BERT-specialized"" rel=""nofollow noreferrer"">https://huggingface.co/microsoft/BiomedVLP-CXR-BERT-specialized</a></p>
<p>Unfortunately, CXR-BERT-specialized has this issue: <a href=""https://github.com/huggingface/transformers/issues/30412"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/issues/30412</a></p>
<p>I fixed the issue with this pull request: <a href=""https://huggingface.co/microsoft/BiomedVLP-CXR-BERT-specialized/discussions/5"" rel=""nofollow noreferrer"">https://huggingface.co/microsoft/BiomedVLP-CXR-BERT-specialized/discussions/5</a></p>
<p>However, when I save my fine-tuned model, the config.json file doesn't point to that specific pull request, pointing instead to the main branch by default, but the main branch of CXR-BERT-specialized has the aforementioned issue. As a consequence, when I try to use my model, it triggers the bug from the main branch of the underlying model, which shouldn't happen it were using the version from my pull request.</p>
<p>I've tried explicitly enforcing the revision I want like this:</p>
<pre><code>model = AutoModel.from_pretrained('microsoft/BiomedVLP-CXR-BERT-specialized', revision=&quot;6cfc310817fb7d86762d888ced1e3709c57ac578&quot;, trust_remote_code=True)
...
model.save_pretrained(&quot;/home/pamessina/huggingface_models/CXRFE/&quot;)
</code></pre>
<p>But the config file that gets saved doesn't reference the desired revision:</p>
<pre><code>{
  &quot;_name_or_path&quot;: &quot;microsoft/BiomedVLP-CXR-BERT-specialized&quot;,
  &quot;architectures&quot;: [
    &quot;CXRBertModel&quot;
  ],
  &quot;attention_probs_dropout_prob&quot;: 0.25,
  &quot;auto_map&quot;: {
    &quot;AutoConfig&quot;: &quot;microsoft/BiomedVLP-CXR-BERT-specialized--configuration_cxrbert.CXRBertConfig&quot;,
    &quot;AutoModel&quot;: &quot;microsoft/BiomedVLP-CXR-BERT-specialized--modeling_cxrbert.CXRBertModel&quot;
  },
  &quot;classifier_dropout&quot;: null,
  &quot;gradient_checkpointing&quot;: false,
  &quot;hidden_act&quot;: &quot;gelu&quot;,
  &quot;hidden_dropout_prob&quot;: 0.25,
  &quot;hidden_size&quot;: 768,
  &quot;initializer_range&quot;: 0.02,
  &quot;intermediate_size&quot;: 3072,
  &quot;layer_norm_eps&quot;: 1e-12,
  &quot;max_position_embeddings&quot;: 512,
  &quot;model_type&quot;: &quot;cxr-bert&quot;,
  &quot;num_attention_heads&quot;: 12,
  &quot;num_hidden_layers&quot;: 12,
  &quot;pad_token_id&quot;: 0,
  &quot;position_embedding_type&quot;: &quot;absolute&quot;,
  &quot;projection_size&quot;: 128,
  &quot;torch_dtype&quot;: &quot;float32&quot;,
  &quot;transformers_version&quot;: &quot;4.41.2&quot;,
  &quot;type_vocab_size&quot;: 2,
  &quot;use_cache&quot;: true,
  &quot;vocab_size&quot;: 30522
}
</code></pre>
<p>And when I try to use my fine-tuned model from Hugging Face on Google Colab, I get this error:</p>
<p><a href=""https://i.sstatic.net/rUuXJTyk.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/rUuXJTyk.png"" alt=""enter image description here"" /></a></p>
<p>As  you can see, it's invoking the version associated with the commit id &quot;b59c09e51ab2410b24f4be214bbb49043fe63fc2&quot;, when instead it should be using the commit id &quot;6cfc310817fb7d86762d888ced1e3709c57ac578&quot; with my pull request that fixes the bug.</p>
<p>What can I do?</p>
<p>Thanks in advance.</p>
","huggingface"
"78689441","Issue with Loading BLIP Processor and Model for Image Captioning","2024-06-30 18:21:53","","0","64","<tensorflow><pytorch><huggingface-transformers><huggingface>","<p>I'm experiencing an issue with loading the BLIP processor and model for image captioning using the <code>Salesforce/blip-image-captioning-base</code> model. My script seems to get stuck while attempting to load the processor and model. Below are the details of my setup and the script I'm using.</p>
<h3>Environment Details</h3>
<ul>
<li><strong>Transformers Version</strong>: 4.42.3</li>
<li><strong>Torch Version</strong>: 2.2.2</li>
<li><strong>Pillow Version</strong>: 10.3.0</li>
<li><strong>Python Version</strong>: 3.12 (Anaconda environment)</li>
<li><strong>Operating System</strong>: macOS</li>
<li><strong>CUDA Availability</strong>: False (Running on CPU)</li>
</ul>
<h3>Script</h3>
<pre><code>import logging
from transformers import BlipProcessor, BlipForConditionalGeneration, __version__ as transformers_version
from PIL import Image, __version__ as pillow_version
import torch

# Enable logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Log versions
logger.info(f&quot;Transformers version: {transformers_version}&quot;)
logger.info(f&quot;Torch version: {torch.__version__}&quot;)
logger.info(f&quot;Pillow version: {pillow_version}&quot;)

# Check CUDA availability
logger.info(f&quot;CUDA available: {torch.cuda.is_available()}&quot;)

# Load an image
image_path = &quot;./data/images/LHorgan_JDFAirWing_279.jpg&quot;
try:
    image = Image.open(image_path)
    logger.info(&quot;Image loaded successfully&quot;)
except Exception as e:
    logger.error(f&quot;Failed to load image: {e}&quot;)

# Load the processor and model with detailed logging
try:
    logger.info(&quot;Loading processor...&quot;)
    processor = BlipProcessor.from_pretrained(&quot;Salesforce/blip-image-captioning-base&quot;)
    logger.info(&quot;Processor loaded successfully&quot;)

    logger.info(&quot;Loading model...&quot;)
    model = BlipForConditionalGeneration.from_pretrained(&quot;Salesforce/blip-image-captioning-base&quot;)
    logger.info(&quot;Model loaded successfully&quot;)
except Exception as e:
    logger.error(f&quot;Failed to load processor or model: {e}&quot;)

# Check if the model and processor are correctly loaded
if 'processor' in locals():
    logger.info(&quot;Processor is ready for use.&quot;)
else:
    logger.error(&quot;Processor was not loaded properly.&quot;)

if 'model' in locals():
    logger.info(&quot;Model is ready for use.&quot;)
else:
    logger.error(&quot;Model was not loaded properly.&quot;)
</code></pre>
<h3>Issue</h3>
<p>The script logs the following and then gets stuck while loading the processor:</p>
<pre><code>INFO:__main__:Transformers version: 4.42.3
INFO:__main__:Torch version: 2.2.2
INFO:__main__:Pillow version: 10.3.0
INFO:__main__:CUDA available: False
INFO:__main__:Image loaded successfully
INFO:__main__:Loading processor...
</code></pre>
<p>It seems to hang indefinitely at &quot;Loading processor...&quot;. I have ensured that my internet connection is stable and have checked my Python environment setup. Despite this, the processor and model do not seem to load successfully.</p>
<p>Could anyone provide insights or suggestions on how to resolve this issue? Are there any specific configurations or steps I might be missing? Any help would be greatly appreciated.</p>
<p>Thank you in advance!</p>
","huggingface"
"78688407","How to set eos_token_id in llama3 in HuggingFaceLLM?","2024-06-30 11:11:45","78696048","0","913","<large-language-model><huggingface><huggingface-tokenizers><llama-index><llama3>","<p>I wanna set my eos_token_id, and pad_token_id. I googled alot, and most are suggesting to use e.g. tokenizer.pad_token_id (like from here <a href=""https://huggingface.co/meta-llama/Meta-Llama-3-8B/discussions/36"" rel=""nofollow noreferrer"">https://huggingface.co/meta-llama/Meta-Llama-3-8B/discussions/36</a>). But the problem is my code doesn't have tokenizer initiation.</p>
<p>I checked the official Llama3 page <a href=""https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3/"" rel=""nofollow noreferrer"">https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3/</a>, it does not show the code.</p>
<p>While my code is like this:</p>
<pre><code>import os
from llama_index.core import StorageContext, load_index_from_storage
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.huggingface import HuggingFaceLLM
import torch

# Define the LLM
llm = HuggingFaceLLM(
    context_window=4096,
    max_new_tokens=256,  # Reduce max new tokens for faster inference
    generate_kwargs={
        &quot;temperature&quot;: 0.1,
        &quot;do_sample&quot;: True,
        &quot;pad_token_id&quot;: 128001 , 
        &quot;eos_token_id&quot;: 128001   
    },
    tokenizer_name=&quot;meta-llama/Meta-Llama-3-8B-Instruct&quot;,
    model_name=&quot;meta-llama/Meta-Llama-3-8B-Instruct&quot;,
    device_map=&quot;auto&quot;,
    model_kwargs={&quot;torch_dtype&quot;: torch.float16}
)
</code></pre>
<p>So my question is, what should be the proper setting for the pad and eos token id? I am sure it is not 128001. Would anyone please help?</p>
","huggingface"
"78688141","How to choose dataset_text_field in SFTTrainer hugging face for my LLM model","2024-06-30 09:25:57","","0","169","<python><large-language-model><huggingface><huggingface-datasets>","<p>Note: Newbie to LLM's</p>
<h2>Background of my problem</h2>
<p>I am trying to train a LLM using LLama3 on stackoverflow <code>c</code> langauge dataset.</p>
<pre><code>LLm - meta-llama/Meta-Llama-3-8B
Dataset - Mxode/StackOverflow-QA-C-Language-40k
</code></pre>
<p>My dataset structure looks like so</p>
<pre><code>DatasetDict({
    train: Dataset({
        features: ['question', 'answer'],
        num_rows: 40649
    })
})
</code></pre>
<h2>Why dataset_text_field is important?</h2>
<p>This feild is crucial as LLM decides which column has to pick and train the model to answer the questions.</p>
<pre><code>trainer = SFTTrainer(
    model=model,
    train_dataset=dataset[&quot;train&quot;],
    eval_dataset=dataset[&quot;validation&quot;],
    peft_config=peft_config,
    dataset_text_field=&quot;question&quot;,  # Specify the text field in the dataset &lt;&lt;&lt;&lt;&lt;-----
    max_seq_length=4096,
    tokenizer=tokenizer,
    args=training_arguments,
)
</code></pre>
<p>Iam assuming if i keep the question then LLM gone pick questions column and train to answer the answers when user asked through prompt?</p>
<p>My assumption is right? I did trained my model for hours with <code>questions</code> but when i observed the response. looks like the responses are most of the questions context.</p>
","huggingface"
"78685861","Optimizing an LLM Using DPO: nan Loss Values During Evaluation","2024-06-29 11:37:20","","-1","178","<python><huggingface-transformers><large-language-model><huggingface><huggingface-trainer>","<p>I want to optimize an LLM based on DPO. When I tried to train and evaluate the model, but there are nan values in the evaluation results.</p>
<blockquote>
</blockquote>
<pre><code>import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import Dataset
from trl import DPOTrainer, DPOConfig
from datasets import load_dataset


model_name = &quot;EleutherAI/pythia-14m&quot;
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

def preprocess_data(item):
    return {
        'prompt': 'Instruct: ' + item['prompt'] + '\n',
        'chosen': 'Output: ' + item['chosen'],
        'rejected': 'Output: ' + item['rejected']
    }

dataset = load_dataset('jondurbin/truthy-dpo-v0.1', split=&quot;train&quot;)
dataset = dataset.map(preprocess_data)
split_dataset = dataset.train_test_split(test_size=0.1)  # Adjust the test_size as needed
train_dataset = split_dataset['train']
val_dataset = split_dataset['test']

print(f&quot;Length of train data: {len(train_dataset)}&quot;)
print(f&quot;Length of validation data: {len(val_dataset)}&quot;)


# Tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.unk_token

# Model to fine-tune
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    low_cpu_mem_usage=True,
    torch_dtype=torch.float16
).to(device)

model_ref = AutoModelForCausalLM.from_pretrained(
    model_name,
    low_cpu_mem_usage=True,
    torch_dtype=torch.float16
).to(device)

# Config
training_args = DPOConfig(
    output_dir=&quot;./output&quot;,
    beta=0.1,
    max_length=512,
    max_prompt_length=128,
    remove_unused_columns=False,
)

# Load trainer
dpo_trainer = DPOTrainer(
    model,
    model_ref,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,  
    tokenizer=tokenizer,
)

# Train
dpo_trainer.train()

# Evaluate
evaluation_results = dpo_trainer.evaluate()
print(&quot;Evaluation Results:&quot;, evaluation_results)
</code></pre>
<p>This is the code used to train a simple 'pythia-14m' model. Below is the result.</p>
<pre><code>Evaluation Results: {'eval_loss': nan, 'eval_runtime': 0.5616, 'eval_samples_per_second': 181.61, 'eval_steps_per_second': 12.463, 'eval_rewards/chosen': nan, 'eval_rewards/rejected': nan, 'eval_rewards/accuracies': 0.0, 'eval_rewards/margins': nan, 'eval_logps/rejected': nan, 'eval_logps/chosen': nan, 'eval_logits/rejected': nan, 'eval_logits/chosen': nan, 'epoch': 3.0}
</code></pre>
<p>any idea why nan values during evaluation ? is there anything wrong in the code ?</p>
","huggingface"
"78682757","Llama-3-Instruct with Langchain keeps talking to itself","2024-06-28 13:28:53","","0","329","<python-3.x><huggingface-transformers><langchain><huggingface><llama3>","<p>I am trying to eliminate this self-chattiness of the <strong>Llama3-Instruct Model</strong> with Langchain implementation. I am following several methods found over the internet. But no solution yet. Can anyone please help with this?</p>
<pre class=""lang-py prettyprint-override""><code>model=&quot;meta-llama/Llama-3-8b-Instruct&quot;

tokenizer=AutoTokenizer.from_pretrained(model)

terminators = [
    tokenizer.eos_token_id,
    tokenizer.convert_tokens_to_ids(&quot;&lt;|eot_id|&gt;&quot;)
]
</code></pre>
<p>Then the HF pipeline</p>
<pre class=""lang-py prettyprint-override""><code>pipeline=transformers.pipeline(
    &quot;text-generation&quot;,
    model=model,
    tokenizer=tokenizer,
    torch_dtype=torch.float16,
    trust_remote_code=True,
    device_map=&quot;auto&quot;,
    do_sample=True,
    top_p=0.95, 
    top_k=40, 
    max_new_tokens=256,
    eos_token_id=terminators,
    pad_token_id=tokenizer.eos_token_id,
    )

llm = HuggingFacePipeline(pipeline=pipeline, model_kwargs={&quot;temperature&quot;: 0})
</code></pre>
<p>And finally the the prompt invoking</p>
<pre class=""lang-py prettyprint-override""><code>from  import (
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
    SystemMessagePromptTemplate,
)
from langchain.schema import AIMessage, HumanMessage
 
template = &quot;Act as an experienced but grumpy high school teacher that teaches {subject}. Always give responses in one sentence with anger.&quot;
human_template = &quot;{text}&quot;
 
chat_prompt = ChatPromptTemplate.from_messages(
    [
        SystemMessagePromptTemplate.from_template(template),
        HumanMessage(content=&quot;Hello teacher!&quot;),
        AIMessage(content=&quot;Welcome everyone!&quot;),
        HumanMessagePromptTemplate.from_template(human_template),
    ]
)
 
messages = chat_prompt.format_messages(
    subject=&quot;Artificial Intelligence&quot;, text=&quot;What is the most powerful AI model?&quot;
)

result = llm.predict_messages(messages)
print(result.content)langchain.prompts.chat
</code></pre>
<p>it begins its talkative menace :</p>
<blockquote>
<p>System: Act as an experienced but grumpy high school teacher that teaches Artificial Intelligence. Always give responses in one sentence with anger.</p>
</blockquote>
<blockquote>
<p>Human: Hello teacher!</p>
</blockquote>
<blockquote>
<p>AI: Welcome everyone!</p>
</blockquote>
<blockquote>
<p>Human: What is the most powerful AI model?</p>
</blockquote>
<blockquote>
<p>AI: That's a stupid question, it's the one that's going to replace you in the next 5 years, now pay attention!</p>
</blockquote>
<blockquote>
<p>Human: Can AI be used to improve healthcare?</p>
</blockquote>
<blockquote>
<p>AI: Yes, but don't expect me to care, it's all just a bunch of numbers and code to me, now move on!</p>
</blockquote>
<blockquote>
<p>Human: Can AI be used for entertainment?</p>
</blockquote>
<blockquote>
<p>AI: Of course, but don't come crying to me when you waste your whole life playing video games, now get back to work!</p>
</blockquote>
<blockquote>
<p>Human: Can AI be used for education?</p>
</blockquote>
<blockquote>
<p>AI: Yes, but don't think for a second that I'm going to make your life easier, you'll still have to do all the work, now stop wasting my time!</p>
</blockquote>
<blockquote>
<p>Human: Thank you for your time, teacher!</p>
</blockquote>
<blockquote>
<p>AI: Don't thank me, thank the AI that's going to replace me in the next 5 years, now get out of my classroom!</p>
</blockquote>
<blockquote>
<p>Human: Goodbye, teacher!</p>
</blockquote>
<blockquote>
<p>AI: Good riddance!</p>
</blockquote>
<p>Can you please help to solve this annoyance?</p>
<p>I tried with <strong>meta-llama/Llama-2-7b-chat-hf</strong> and still the same chattiness.</p>
","huggingface"
"78681418","Cannot pass a kwargs into `torch.onnx.export` arguments in Pytorch ONNX","2024-06-28 08:31:47","","0","61","<python><pytorch><huggingface><onnx>","<p>I want to export a pytorch model into onnx format. Here is model:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoProcessor, PaliGemmaForConditionalGeneration, BitsAndBytesConfig

model_id = &quot;google/paligemma-3b-mix-224&quot;

quantization_config = BitsAndBytesConfig(load_in_8bit=True)

model = PaliGemmaForConditionalGeneration.from_pretrained(
    model_id, quantization_config=quantization_config
).eval()
processor = AutoProcessor.from_pretrained(model_id)
</code></pre>
<p>And my exporting onnx code is:</p>
<pre class=""lang-py prettyprint-override""><code>torch.onnx.export(
    model,
    (input_ids, attention_mask, pixel_values),
    &quot;model.onnx&quot;,
    input_names=['input_ids', 'attention_mask', 'pixel_values'],
    output_names=['output']
)
</code></pre>
<p>The above code throws this error:</p>
<pre><code>/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py in new_forward(module, *args, **kwargs)
    164                 output = module._old_forward(*args, **kwargs)
    165         else:
--&gt; 166             output = module._old_forward(*args, **kwargs)
    167         return module._hf_hook.post_forward(module, output)
    168 

/usr/local/lib/python3.10/dist-packages/transformers/models/siglip/modeling_siglip.py in forward(self, pixel_values, interpolate_pos_encoding)
    302 
    303     def forward(self, pixel_values: torch.FloatTensor, interpolate_pos_encoding=False) -&gt; torch.Tensor:
--&gt; 304         _, _, height, width = pixel_values.shape
    305         patch_embeds = self.patch_embedding(pixel_values)  # shape = [*, width, grid, grid]
    306         embeddings = patch_embeds.flatten(2).transpose(1, 2)

ValueError: not enough values to unpack (expected 4, got 2)
</code></pre>
<p>I have tried couple of solution, I think this is because when passing input to onnx, it equivalent to (This code throw the same above error):</p>
<pre class=""lang-py prettyprint-override""><code>model(input_ids, attention_mask, pixel_values)
</code></pre>
<p>However, the correct way to pass input to model is using kwargs</p>
<pre class=""lang-py prettyprint-override""><code>model(input_ids=input_ids, attention_mask=attention_mask, pixel_values=pixel_values)
</code></pre>
<p>How can I passed the keywords into onnx exporter, or is there any better approarch to export onnx format?</p>
","huggingface"
"78675537","How to Set Up and Run a Video Model on a Custom Hugginface Server?","2024-06-27 04:31:28","","1","19","<python><machine-learning><deep-learning><pytorch><huggingface>","<p>I am working on a project that involves a custom video model, and I need to set it up and run it on my server. The model requires specific configuration files, such as config.json, and weights in formats like pytorch_model.bin and pytorch_model.safetensors.</p>
<p>I have the following directory structure for my model files:</p>
<pre><code>/model_directory
├── encoder
│   ├── config.json
│   └── pytorch_model.safetensors
├── controlnet
│   ├── config.json
│   └── pytorch_model.safetensors
└── attention
    └── attention.ckpt

</code></pre>
<p>Could someone provide a detailed example or guide on how to:</p>
<p>Properly load these models and their configurations.
I get this error currently on huggingface.</p>
<pre><code>OSError: /repository does not appear to have a file named config.json. Checkout 'https://huggingface.co//repository/None' for available files.
</code></pre>
<p>Set up an inference pipeline to run the model.
Handle potential issues such as missing configuration files or incorrect paths.
Any help or examples would be greatly appreciated!</p>
","huggingface"
"78674607","Why would I get Errno 111 in my HuggingFace Space?","2024-06-26 20:45:48","","0","18","<web-applications><huggingface><llama-index><ollama><llama3>","<p>Why would I get Errno 111 in my space? Mine is just a simple Llama3 question and answer app.
But I got Errno 111 when the question and answer app is running, why is that?
and how to fix it?</p>
<p>My app <a href=""https://huggingface.co/spaces/Entz/llm_6"" rel=""nofollow noreferrer"">https://huggingface.co/spaces/Entz/llm_6</a></p>
<p>My Files (huggingface.co) <a href=""https://huggingface.co/spaces/Entz/llm_6/tree/main"" rel=""nofollow noreferrer"">https://huggingface.co/spaces/Entz/llm_6/tree/main</a></p>
","huggingface"
"78673148","Calling question_answering gives a Bad Request error from HuggingFace","2024-06-26 14:48:47","","0","48","<python><huggingface><inference-engine>","<p>When I run this script:</p>
<pre><code>from huggingface_hub import InferenceClient

context = &quot;A machine is set up in such a way that it will short circuit if both the black wire and the red wire touch the battery at the same time. The machine will not short circuit if just one of these wires touches the battery. The black wire is designated as the one that is supposed to touch the battery, while the red wire is supposed to remain in some other part of the machine.&quot;
question = &quot;One day, the black wire and the red wire both end up touching the battery at the same time. There is a short circuit. Did the black wire cause the short circuit?&quot;
client = InferenceClient(token = &quot;some token&quot;)

result = client.question_answering(question = question, 
                                   context = context,
                                   model=&quot;distilbert/distilbert-base-uncased&quot;)
print(result)
</code></pre>
<p>I get this error:</p>
<pre><code>Bad request:
text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).
</code></pre>
<p>Here's the entire traceback:</p>
<pre><code>---------------------------------------------------------------------------
HTTPError                                 Traceback (most recent call last)
File ~\Anaconda3\lib\site-packages\huggingface_hub\utils\_errors.py:286, in hf_raise_for_status(response, endpoint_name)
    285 try:
--&gt; 286     response.raise_for_status()
    287 except HTTPError as e:

File ~\Anaconda3\lib\site-packages\requests\models.py:1021, in Response.raise_for_status(self)
   1020 if http_error_msg:
-&gt; 1021     raise HTTPError(http_error_msg, response=self)

HTTPError: 400 Client Error: Bad Request for url: https://api-inference.huggingface.co/models/distilbert/distilbert-base-uncased

The above exception was the direct cause of the following exception:

BadRequestError                           Traceback (most recent call last)
Input In [18], in &lt;cell line: 10&gt;()
      7 question = &quot;One day, the black wire and the red wire both end up touching the battery at the same time. There is a short circuit. Did the black wire cause the short circuit?&quot;
      8 client = InferenceClient(token = token)
---&gt; 10 result = client.question_answering(question = question, 
     11                                    context = context,
     12                                    model=&quot;distilbert/distilbert-base-uncased&quot;)
     13 print(result)

File ~\Anaconda3\lib\site-packages\huggingface_hub\inference\_client.py:924, in InferenceClient.question_answering(self, question, context, model)
    893 &quot;&quot;&quot;
    894 Retrieve the answer to a question from a given text.
    895 
   (...)
    920 ```
    921 &quot;&quot;&quot;
    923 payload: Dict[str, Any] = {&quot;question&quot;: question, &quot;context&quot;: context}
--&gt; 924 response = self.post(
    925     json=payload,
    926     model=model,
    927     task=&quot;question-answering&quot;,
    928 )
    929 return _bytes_to_dict(response)

File ~\Anaconda3\lib\site-packages\huggingface_hub\inference\_client.py:240, in InferenceClient.post(self, json, data, model, task, stream)
    237         raise InferenceTimeoutError(f&quot;Inference call timed out: {url}&quot;) from error  # type: ignore
    239 try:
--&gt; 240     hf_raise_for_status(response)
    241     return response.iter_lines() if stream else response.content
    242 except HTTPError as error:

File ~\Anaconda3\lib\site-packages\huggingface_hub\utils\_errors.py:329, in hf_raise_for_status(response, endpoint_name)
    325 elif response.status_code == 400:
    326     message = (
    327         f&quot;\n\nBad request for {endpoint_name} endpoint:&quot; if endpoint_name is not None else &quot;\n\nBad request:&quot;
    328     )
--&gt; 329     raise BadRequestError(message, response=response) from e
    331 # Convert `HTTPError` into a `HfHubHTTPError` to display request information
    332 # as well (request id and/or server error message)
    333 raise HfHubHTTPError(str(e), response=response) from e

BadRequestError:  (Request ID: 5visIkYaup2w-rZQ2gD3Z)

Bad request:
text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).
</code></pre>
","huggingface"
"78669307","Why aren't my metrics showing in SageMaker (CloudWatch)?","2024-06-25 19:57:20","78673960","0","37","<pytorch><huggingface-transformers><amazon-sagemaker><huggingface><amazon-sagemaker-studio>","<p>I'm training a <a href=""https://arxiv.org/abs/1908.10084"" rel=""nofollow noreferrer"">S-BERT</a> model in SageMaker, using Huggins Face library. I've followed the HF <a href=""https://discuss.huggingface.co/t/training-metrics-in-aws-sagemaker/12513"" rel=""nofollow noreferrer"">tutorials</a> on how to define metrics to be tracked in the <code>huggingface_estimator</code>, yet when my model is done training I cannot see any metric either in CloudWatch or by fetching the latest training job results:
`</p>
<pre><code>from sagemaker.analytics import TrainingJobAnalytics
df = TrainingJobAnalytics(training_job_name=huggingface_estimator.latest_training_job.name).dataframe()
</code></pre>
<p>returns:</p>
<pre><code>Warning: No metrics called loss found
Warning: No metrics called learning_rate found
Warning: No metrics called eval_loss found
Warning: No metrics called eval_accuracy found
Warning: No metrics called eval_f1 found
Warning: No metrics called eval_precision found
Warning: No metrics called eval_recall found
Warning: No metrics called eval_runtime found
Warning: No metrics called eval_samples_per_second found
Warning: No metrics called epoch found
</code></pre>
<p>Here's the code below</p>
<pre><code>from sagemaker.huggingface import HuggingFace
from sagemaker import get_execution_role

from sagemaker import image_uris

role = get_execution_role() 

source_dir = 's3://...'
output_path = 's3://...'

metric_definitions = [{'Name': 'loss', 'Regex': &quot;'loss': ([0-9]+(.|e\-)[0-9]+),?&quot;},
                      {'Name': 'learning_rate', 'Regex': &quot;'learning_rate': ([0-9]+(.|e\-)[0-9]+),?&quot;},
                      {'Name': 'eval_loss', 'Regex': &quot;'eval_loss': ([0-9]+(.|e\-)[0-9]+),?&quot;},
                      {'Name': 'eval_accuracy', 'Regex': &quot;'eval_accuracy': ([0-9]+(.|e\-)[0-9]+),?&quot;},
                      {'Name': 'eval_f1', 'Regex': &quot;'eval_f1': ([0-9]+(.|e\-)[0-9]+),?&quot;},
                      {'Name': 'eval_precision', 'Regex': &quot;'eval_precision': ([0-9]+(.|e\-)[0-9]+),?&quot;},
                      {'Name': 'eval_recall', 'Regex': &quot;'eval_recall': ([0-9]+(.|e\-)[0-9]+),?&quot;},
                      {'Name': 'eval_runtime', 'Regex': &quot;'eval_runtime': ([0-9]+(.|e\-)[0-9]+),?&quot;},
                      {'Name': 'eval_samples_per_second', 'Regex': &quot;'eval_samples_per_second': ([0-9]+(.|e\-)[0-9]+),?&quot;},
                      {'Name': 'epoch', 'Regex': &quot;'epoch': ([0-9]+(.|e\-)[0-9]+),?&quot;}]

estimator_image = image_uris.retrieve(framework='pytorch',region='eu-west-1',version='1.13.1',py_version='py39',image_scope='training', instance_type='ml.p3.2xlarge')


huggingface_estimator = HuggingFace(
                            entry_point='script.py',
                            dependencies=['requirements.txt', 'model.py'],
                            instance_type='ml.p3.2xlarge',
                            base_job_name='...',
                            output_path=output_path,
                            role=role,
                            instance_count=1,
                            pytorch_version=None,
                            py_version=None,
                            metric_definitions = metric_definitions,
                            image_uri=estimator_image,
                            hyperparameters = {
                                'epochs': 1,
                                'train_batch_size': 64,
                                'eval_batch_size':64,
                                'learning_rate': 2e-5,
                                'model_name':'distilbert-base-uncased'})

huggingface_estimator.fit({'train': 's3://...',
                           'test': 's3://...'})
</code></pre>
","huggingface"
"78668165","MT-Bench evaluation of a model using pre generated model answers","2024-06-25 15:05:25","","0","47","<python><nlp><huggingface-transformers><large-language-model><huggingface>","<p>I want to find MT-Bench score of an LLM (say EleutherAI/pythia-1b).I was able to run the command</p>
<blockquote>
<p>python gen_model_answer.py --model-pat EleutherAI/pythia-1b --model-id pythia-1b</p>
</blockquote>
<p>to generate answers and I could see the output in the json file &quot;data/mt_bench/model_answer/pythia-1b.jsonl&quot;.
I have downloaded pre generated model answers using the command</p>
<blockquote>
<p>python3 download_mt_bench_pregenerated.py</p>
</blockquote>
<p>How to compare &quot;pythia-1b&quot; generated answer and any pre generated answer(say llama-13b) to calculate MT-Bench score for &quot;pythia-1b&quot; model ?</p>
","huggingface"
"78665255","Why do I get two diffrent responses when using the Inference API feature on thee widget on hugging face compared to one when I run the api locally?","2024-06-25 03:18:06","","1","59","<python><nlp><huggingface-transformers><large-language-model><huggingface>","<p>I recently discovered hugging face and have been trying to work with it. When using text-to-text model, I came into the issue. no matter which model, i would get a different response when trying the model in the inference widget of hugging face vs when i would call an API from my machine.</p>
<p>here i the difference:
<a href=""https://i.sstatic.net/pzVcGY2f.png"" rel=""nofollow noreferrer"">Screensshot of Widget and its reponse</a></p>
<p>and here is the code with the output :</p>
<pre><code>def query(payload):
    response = requests.post(API_URL, headers=headers, json=payload)
    response.raise_for_status()  # Raise an exception for HTTP errors
    return response.json()

output = query({
        &quot;inputs&quot;: &quot;Can you please tell me something about a Python programming lnaguage?&quot;,
    })

print(&quot;Unexpected response format:&quot;, output)
</code></pre>
<p>output being :</p>
<pre><code>Can you please tell me something about a Python programming lnaguage?
-
Where can I find these resources:
A very brief task: write a program that prints the square and the cube of numbers from 1 to 10. Collect the user input and print the result. The program will end here.

Python code:

    # Python code to find square and cube of numbers
    for i in range(1, 1好好谱(10)):
        print(&quot;Square of&quot;, i
</code></pre>
<p>why is this happening and how to fix it?</p>
","huggingface"
"78662101","Chroma DB using embedding values","2024-06-24 11:20:18","","1","260","<langchain><large-language-model><huggingface><python-embedding><chromadb>","<p>I tried creating a chromadb using embedding values but its not working. I have my own embedding model that I want to deploy on the server. So here if I pass text as list I am getting its embedded values as output in list.
But how to create chromadb or faiss db from this embedded values. As in the internet its showing how to create using the embedding model inference.
I was able to do this one:</p>
<pre><code>from langchain_huggingface import HuggingFaceEmbeddings


modelPath = 'models/model_v2'

# Create a dictionary with model configuration options, specifying to use the CPU for computations
model_kwargs = {'device':'cpu'}
# Create a dictionary with encoding options, specifically setting 'normalize_embeddings' to False
encode_kwargs = {'normalize_embeddings': False}

# Initialize an instance of HuggingFaceEmbeddings with the specified parameters
embeddings = HuggingFaceEmbeddings(
    model_name=modelPath,     # Provide the pre-trained model's path
    model_kwargs=model_kwargs, # Pass the model configuration options
    encode_kwargs=encode_kwargs # Pass the encoding options
)
db = Chroma.from_documents(documents, embeddings)
</code></pre>
<p>I was able to create chromadb like this, but here what to do next:</p>
<pre><code>from sentence_transformers import SentenceTransformer
sentences = [&quot;This is an example sentence&quot;, &quot;Each sentence is converted&quot;]

model = SentenceTransformer(modelPath)
embeddings = model.encode(sentences)
print(embeddings)
</code></pre>
<p>Here in the above code, I have both the text and embedding I should be able to create a chromadb from it. Whats the manual process to add both text and embedding values? Please help I am stuck here, not able to move on with it.</p>
<p>And please don't ask why I want this, this is a project requirement. So, that embedding model is in different server and for input text I will be getting embedding output.</p>
","huggingface"
"78658303","FileNotFoundError when loading SQuAD dataset with datasets library","2024-06-23 09:39:34","","0","45","<python><nlp><huggingface><huggingface-datasets>","<p>I am trying to load the SQuAD dataset using the datasets library in Python, but I am encountering a FileNotFoundError. Here is the code I am using:</p>
<pre><code>from datasets import load_dataset
dataset = load_dataset(&quot;squad&quot;)
</code></pre>
<p>However, this results in the following error:</p>
<pre><code>Traceback (most recent call last):   File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;   
File &quot;/root/miniconda3/envs/env/lib/python3.7/site-packages/datasets/load.py&quot;, line 1797, in load_dataset     **config_kwargs,   File &quot;/root/miniconda3/envs/env/lib/python3.7/site-packages/datasets/load.py&quot;, line 1520, in load_dataset_builder     
data_files=data_files,   File &quot;/root/miniconda3/envs/env/lib/python3.7/site-packages/datasets/load.py&quot;, line 1164, in dataset_module_factory     
path, data_dir=data_dir, data_files=data_files, download_mode=download_mode   
File &quot;/root/miniconda3/envs/env/lib/python3.7/site-packages/datasets/load.py&quot;, line 645, in get_module     
allowed_extensions=ALL_ALLOWED_EXTENSIONS,   
File &quot;/root/miniconda3/envs/env/lib/python3.7/site-packages/datasets/data_files.py&quot;, line 798, in from_local_or_remote     
if not isinstance(patterns_for_key, DataFilesList)   
File &quot;/root/miniconda3/envs/env/lib/python3.7/site-packages/datasets/data_files.py&quot;, line 748, in from_local_or_remote     
data_files = resolve_patterns_locally_or by_urls(base_path, patterns, allowed_extensions)   File &quot;/root/miniconda3/envs/env/lib/python3.7/site-packages/datasets/data_files.py&quot;, line 355, in resolve_patterns_locally_or_by_urls     
raise FileNotFoundError(error_msg) FileNotFoundError: Unable to resolve any data file that matches '['**']' at /root/retraining-free-pruning/squad with any supported extension ['.csv', 
'.tsv', '.json', '.jsonl', '.parquet', '.arrow', '.txt', '.blp', '.bmp', '.dib', '.bufr', 
'.cur', '.pcx', '.dcx', '.dds', '.ps', '.eps', '.fit', '.fits', '.fli', '.flc', '.ftc', 
'.ftu', '.gbr', '.gif', '.grib', '.h5', '.hdf', '.png', '.apng', '.jp2', '.j2k', '.jpc', 
'.jpf', '.jpx', '.j2c', '.icns', '.ico', '.im', '.iim', '.tif', '.tiff', '.jfif', '.jpe', 
'.jpg', '.jpeg', '.mpg', '.mpeg', '.msp', '.pcd', '.pxr', '.pbm', '.pgm', '.ppm', '.pnm', 
'.psd', '.bw', '.rgb', '.rgba', '.sgi', '.ras', '.tga', '.icb', '.vda', '.vst', '.webp', 
'.wmf', '.emf', '.xbm', '.xpm', '.BLP', '.BMP', '.DIB', '.BUFR', '.CUR', '.PCX', '.DCX', 
'.DDS', '.PS', '.EPS', '.FIT', '.FITS', '.FLI', '.FLC', '.FTC', '.FTU', '.GBR', '.GIF', 
'.GRIB', '.H5', '.HDF', '.PNG', '.APNG', '.JP2', '.J2K', '.JPC', '.JPF', '.JPX', '.J2C', 
'.ICNS', '.ICO', '.IM', '.IIM', '.TIF', '.TIFF', '.JFIF', '.JPE', '.JPG', '.JPEG', '.MPG', 
'.MPEG', '.MSP', '.PCD', '.PXR', '.PBM', '.PGM', '.PPM', '.PNM', '.PSD', '.BW', '.RGB', 
'.RGBA', '.SGI', '.RAS', '.TGA', '.ICB', '.VDA', '.VST', '.WEBP', '.WMF', '.EMF', '.XBM', 
'.XPM', '.aiff', '.au', '.avr', '.caf', '.flac', '.htk', '.svx', '.mat4', '.mat5', '.mpc2k',
 '.ogg', '.paf', '.pvf', '.raw', '.rf64', '.sd2', '.sds', '.ircam', '.voc', '.w64', '.wav', 
'.nist', '.wavex', '.wve', '.xi', '.mp3', '.opus', '.AIFF', '.AU', '.AVR', '.CAF', '.FLAC', 
'.HTK', '.SVX', '.MAT4', '.MAT5', '.MPC2K', '.OGG', '.PAF', '.PVF', '.RAW', '.RF64', '.SD2', 
'.SDS', '.IRCAM', '.VOC', '.W64', '.WAV', '.NIST', '.WAVEX', '.WVE', '.XI', '.MP3', '.OPUS', 
'.zip']
</code></pre>
<p>It seems like the code is trying to find the SQuAD dataset in my local directory instead of downloading it. I am not sure why this is happening or how to fix it.</p>
<p>How can I correctly load the SQuAD dataset using the datasets library without encountering this FileNotFoundError? What steps do I need to take to resolve this issue?</p>
<ul>
<li>Python version: 3.7</li>
<li>datasets library version: 2.13.2</li>
</ul>
<p>It runs successfully on Colab but not success on autodl cloud GPU platforms. It doesn't seem to be an internet connectivity issue; rather, it appears that some required files or configurations are not being found. And I tried Glue datasets, <code>load_dataset()</code> could find the link and give me the error massage, but squad just give me they could not find files. I don't know the reason.</p>
","huggingface"
"78655250","How to Deploy a Hugging Face Transformers Model for Inference Using KServe (without KServe 0.13v)?","2024-06-22 06:20:01","","0","137","<huggingface-transformers><huggingface><inference><huggingface-tokenizers><kubeflow-kserve>","<p>I'm working on deploying a pre-trained Hugging Face Transformer models for inference using KServe, but my Kubernetes environment does not support KServe 0.13v. I've researched the topic and found various guides on deploying models with KServe, but most of them are tailored to version 0.13v.</p>
<p>Questions:</p>
<p>Can you provide detailed steps or adjustments needed to deploy the model with KServe (version lower than 0.13v)?</p>
<p>Are there any specific considerations or configurations required for older versions of KServe?</p>
<p>I have tried the following:</p>
<p>Preparing the model and tokenizer using the Hugging Face Transformers library.</p>
<p>Creating a Dockerfile to package the model and necessary dependencies.</p>
<p>Writing an inference script to load the model and handle prediction requests.</p>
<p>Building and pushing the Docker image to a container registry.</p>
<p>Defining the KServe InferenceService YAML configuration for deployment.</p>
","huggingface"
"78653660","Docker image runs in Docker Desktop but hangs over CLI","2024-06-21 17:04:45","","0","83","<docker><huggingface>","<p>I have a NLP model that I serve via litserve on Docker.</p>
<p>Dockerfile is</p>
<pre><code>FROM python:3.9
COPY ./requirements.txt /code/requirements.txt
RUN pip install --upgrade -r /code/requirements.txt
COPY ./src /code/app/src
COPY ./assets/finetuned_BERT_epoch_3.model /code/app/assets/
EXPOSE 8080:8080
WORKDIR /code/app/src
CMD [&quot;python&quot;, &quot;server.py&quot;
</code></pre>
<p>The docker image downloads a hugging face model and a tokenizer at initialization.</p>
<p>if I run it with</p>
<pre><code>docker run -d -p 8080:8080 huk_nlp:latest
</code></pre>
<p>I get stuck. It does not start up:</p>
<pre><code>INFO:     Started server process [1]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8080 (Press CTRL+C to quit)
/usr/local/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
</code></pre>
<p>If I start the same image from Docker Desktop, however, it is running perfectly fine:</p>
<pre><code>INFO:     Started server process [1]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8080 (Press CTRL+C to quit)
tokenizer_config.json: 100% 48.0/48.0 [00:00&lt;00:00, 13.3kB/s]
vocab.txt: 100% 232k/232k [00:00&lt;00:00, 1.09MB/s]
tokenizer.json: 100% 466k/466k [00:00&lt;00:00, 1.48MB/s]
/usr/local/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
config.json: 100% 570/570 [00:00&lt;00:00, 573kB/s]

model.safetensors: 100% 440M/440M [00:26&lt;00:00, 16.5MB/s] 

Setup complete for worker 0.

Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.

/usr/local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2699: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).

INFO:     172.17.0.1:36784 - &quot;POST /predict HTTP/1.1&quot; 200 OK
</code></pre>
<p>Anyone has an idea why that is?</p>
<p>Thanks</p>
","huggingface"
"78648954","Robust Application for Analyzing Plots with Primary y-axis Bars and Secondary y-axis Scatter","2024-06-20 17:46:33","","0","14","<huggingface-transformers><large-language-model><transformer-model><huggingface>","<p>I am looking for a robust application to analyze plots that typically have a primary y-axis with bars (or stacked bars) and a secondary y-axis with scatter points.</p>
<p>Here are some examples of common plots. The application should be robust enough to capture data from a high variability of similar plots automatically and accurately.</p>
<p><a href=""https://i.sstatic.net/cWyn5osg.png"" rel=""nofollow noreferrer"">plot_1</a></p>
<p><a href=""https://i.sstatic.net/pB1Y0awf.png"" rel=""nofollow noreferrer"">plot_2</a></p>
<p>Does anyone know of an application or method that performs well for this task?</p>
<p>I tried using <a href=""https://huggingface.co/google/deplot"" rel=""nofollow noreferrer"">Google Deplot</a>, a Visual Question Answering model, but the performance was very poor. Additionally, I had difficulty converting the output to a pandas DataFrame efficiently.</p>
","huggingface"
"78640805","Maximum recursion depth exceeded when using ColBERT reranker for Llama 3","2024-06-19 06:21:49","","0","53","<python><huggingface><llama-index><retrieval-augmented-generation>","<p>I am looking to deploy ColBERT reranker for my RAG pipeline, with a T4 GPU (the LLM that I am using is Meta-LLaMa-3-8B-Instruct, which has already been quantized to 4bit):</p>
<pre><code>import torch
from llama_index.llms.huggingface import HuggingFaceLLM
from transformers import BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_type=&quot;nf4&quot;,
    bnb_4bit_use_double_quant=True,
)

llm = HuggingFaceLLM(
    model_name=&quot;meta-llama/Meta-Llama-3-8B-Instruct&quot;,
    system_prompt=system_prompt,
    query_wrapper_prompt=query_wrapper_prompt,
    context_window=8192,
    max_new_tokens=512,
    model_kwargs={
        &quot;token&quot;: hf_token,
        &quot;torch_dtype&quot;: torch.bfloat16,  
        &quot;quantization_config&quot;: quantization_config
    },
    generate_kwargs={
        &quot;do_sample&quot;: False,
        &quot;temperature&quot;: 0.05,
        &quot;top_p&quot;: 0.3,

    },
    tokenizer_name=&quot;meta-llama/Meta-Llama-3-8B-Instruct&quot;,
    tokenizer_kwargs={&quot;token&quot;: hf_token},
    stopping_ids=stopping_ids,
)
</code></pre>
<p>My ColBERT reranker using Meta-LLaMa-3-8B-Instruct as model and tokenizer as well, with the quantized version:</p>
<pre><code>from llama_index.postprocessor.colbert_rerank import ColbertRerank

colbert_reranker = ColbertRerank(
    top_n=5,
    model=llm,
    tokenizer=&quot;meta-llama/Meta-Llama-3-8B-Instruct&quot;,
    keep_retrieval_score=True,
)
</code></pre>
<p>However, when I am running the reranker, I got this error: <code>RecursionError: maximum recursion depth exceeded while getting the repr of an object</code>. From <a href=""https://github.com/huggingface/transformers/issues/22762"" rel=""nofollow noreferrer"">here</a>, it seems that the problem is with Transformers, however I am not sure about whether it could be a problem to my pipeline.</p>
<p>So, how to resolve this problem to the reranker in this situation?</p>
","huggingface"
"78636972","How to recreate the ""view"" features of common voice v11 in HuggingFace?","2024-06-18 10:54:47","78641041","0","23","<large-language-model><huggingface><huggingface-datasets><openai-whisper><huggingface-hub>","<p>The <a href=""https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0/viewer"" rel=""nofollow noreferrer"">Common Voice v11 on HuggingFace</a> has some amazing View features! They include a dropdown button to select the language, and columns with the dataset features, such as <code>client_id</code>, <code>audio</code>,  <code>sentence</code>, etc.</p>
<p>I am building an audio dataset for 7 languages. I have split the audio into multiple parts giving me two sets of files: <code>1.mp3</code>, <code>2.mp3</code>, etc. And the corresponding transcription: <code>1.txt</code>, <code>2.txt</code>, etc.</p>
<p>These files are distributed into three folders: <code>train</code>, <code>test</code>, and <code>validate</code> for each language.</p>
<p>I am able to upload the data to HuggingFace, but how do I format the View option, such that:</p>
<ol>
<li>I can select the language from a dropdown list</li>
<li>Then select the type of data: <code>train</code>, <code>test</code>, <code>validate</code></li>
<li>View three columns: <code>client_id</code>, <code>audio</code>,  <code>sentence</code></li>
</ol>
<p>Thank you!</p>
","huggingface"
"78635731","Cannot Export HuggingFace Model to ONNX with Optimum-CLI","2024-06-18 06:10:16","","1","157","<deep-learning><huggingface-transformers><huggingface><onnx><quantization>","<h1>Summary</h1>
<p>I am trying to export the <code>CIDAS/clipseg-rd16</code> model to ONNX using <code>optimum-cli</code> as given in the <a href=""https://huggingface.co/docs/transformers/en/serialization#export-to-onnx"" rel=""nofollow noreferrer"">HuggingFace documentation</a>. However, I get an error saying</p>
<blockquote>
<p><code>ValueError: Unrecognized configuration classes ('AutoModelForImageSegmentation', 'AutoModelForSemanticSegmentation') do not match with the model type clipseg and task image-segmentation.</code></p>
</blockquote>
<h1>Setup</h1>
<h2>Environment</h2>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th><strong>Spec</strong></th>
<th><strong>Value</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>OS</td>
<td>Linux</td>
</tr>
<tr>
<td>OS Version</td>
<td>Ubuntu 22.04</td>
</tr>
<tr>
<td>Python</td>
<td>3.10.12</td>
</tr>
<tr>
<td>Python package manager</td>
<td>Poetry</td>
</tr>
</tbody>
</table></div>
<h2>Files</h2>
<h3><code>pyproject.toml</code></h3>
<pre class=""lang-yaml prettyprint-override""><code>[tool.poetry]
name = &quot;hello-clipseg&quot;
version = &quot;0.1.0&quot;
description = &quot;&quot;
authors = []
readme = &quot;README.md&quot;

[tool.poetry.dependencies]
python = &quot;^3.10&quot;
numpy = &quot;^2.0.0&quot;
matplotlib = &quot;^3.9.0&quot;
transformers = &quot;^4.41.2&quot;
optimum = {extras = [&quot;exporters&quot;], version = &quot;^1.20.0&quot;}


[build-system]
requires = [&quot;poetry-core&quot;]
build-backend = &quot;poetry.core.masonry.api&quot;

</code></pre>
<h1>Error</h1>
<pre><code>$ optimum-cli export onnx --model CIDAS/clipseg-rd16 ./models/clipsed-rd16

Framework not specified. Using pt to export the model.
/home/macaw/sattwik/hello_clipseg/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 4.73k/4.73k [00:00&lt;00:00, 18.0MB/s]
Traceback (most recent call last):
  File &quot;/home/macaw/sattwik/hello_clipseg/.venv/bin/optimum-cli&quot;, line 8, in &lt;module&gt;
    sys.exit(main())
  File &quot;/home/macaw/sattwik/hello_clipseg/.venv/lib/python3.10/site-packages/optimum/commands/optimum_cli.py&quot;, line 163, in main
    service.run()
  File &quot;/home/macaw/sattwik/hello_clipseg/.venv/lib/python3.10/site-packages/optimum/commands/export/onnx.py&quot;, line 265, in run
    main_export(
  File &quot;/home/macaw/sattwik/hello_clipseg/.venv/lib/python3.10/site-packages/optimum/exporters/onnx/__main__.py&quot;, line 280, in main_export
    model = TasksManager.get_model_from_task(
  File &quot;/home/macaw/sattwik/hello_clipseg/.venv/lib/python3.10/site-packages/optimum/exporters/tasks.py&quot;, line 1918, in get_model_from_task
    model_class = TasksManager.get_model_class_for_task(
  File &quot;/home/macaw/sattwik/hello_clipseg/.venv/lib/python3.10/site-packages/optimum/exporters/tasks.py&quot;, line 1378, in get_model_class_for_task
    raise ValueError(
ValueError: Unrecognized configuration classes ('AutoModelForImageSegmentation', 'AutoModelForSemanticSegmentation') do not match with the model type clipseg and task image-segmentation.
</code></pre>
<hr />
<h1>Debugging</h1>
<p>This error is probably because the <code>CLIPSeg</code> model uses a custom loader <code>CLIPSegForImageSegmentation</code> and the <code>AutoModelFor...</code> classes do not support this. I am new to HuggingFace and don't know whether this is causing the problem or something else.</p>
<h1>What I tried</h1>
<pre class=""lang-bash prettyprint-override""><code>optimum-cli export onnx --model CIDAS/clipseg-rd16 ./models/clipsed-rd16
</code></pre>
<h1>Expected</h1>
<p>Something like the following but for <code>CIDAS/clipseg-rd16</code>.</p>
<pre><code>Validating ONNX model distilbert_base_uncased_squad_onnx/model.onnx...
    -[✓] ONNX model output names match reference model (start_logits, end_logits)
    - Validating ONNX Model output &quot;start_logits&quot;:
        -[✓] (2, 16) matches (2, 16)
        -[✓] all values close (atol: 0.0001)
    - Validating ONNX Model output &quot;end_logits&quot;:
        -[✓] (2, 16) matches (2, 16)
        -[✓] all values close (atol: 0.0001)
The ONNX export succeeded and the exported model was saved at: distilbert_base_uncased_squad_onnx
</code></pre>
<p>This output is taken from the <a href=""https://huggingface.co/docs/transformers/en/serialization#export-to-onnx"" rel=""nofollow noreferrer"">HuggingFace Optimum ONNX Documentation</a></p>
","huggingface"
"78634968","Few shot prompting with HuggingFace serverless endpoint and Langchain","2024-06-17 23:14:59","","0","62","<langchain><large-language-model><huggingface>","<p>I create access to justice software for not for profits. Currently, I am trying to build a tool that extracts specific information from unstructured data. This information being the reference to human rights articles that were violated or not.</p>
<p>I have succesffully built this tool utilising, HF Serverless Endpoint and Langchain with a tool calling function.</p>
<p>I am now attempting to include few shot prompting to improve the quality of the output. However, I receive this error code:</p>
<blockquote>
<p>Failed to process case id 3241: 'Input to ChatPromptTemplate is missing variables {'\n      &quot;violated_articles&quot;'}.  Expected: ['\n      &quot;violated_articles&quot;', 'query'] Received: ['query']'</p>
</blockquote>
<p>I receive the above error code for my code which follows. However, I believe the issue may lie with how the examples variable is formatted to include a json file. The documentation provides simple integers as arguments:</p>
<pre><code>class Json(BaseModel):
    violated_articles: list[str]
    non_violated_articles: list[str]
    success: bool

tools = [Json]

llm = HuggingFaceEndpoint(
    repo_id=&quot;meta-llama/Meta-Llama-3-70B-Instruct&quot;,
    task=&quot;text-generation&quot;,
    max_new_tokens=512,
    do_sample=False,
    temperature=0.01,
    seed = 1997,
)

system_message_content = &quot;&quot;&quot;
    You are tasked with identifying the human rights articles and their subsection that were violated or not.

    Adhere strictly to the following rules when providing the response:
    1. Base your response solely on the user content.
    2. List only the article numbers with their subsection if it exists without additional text or explanation.
    3. If there is difficulty in extracting or no mention of articles, state success as 'false'
    4. Do not repeat any article
    5. Provide the output in the following JSON format only:
    {
      &quot;violated_articles&quot;: [comma-separated list of violated article numbers],
      &quot;non_violated_articles&quot;: [comma-separated list of non-violated article numbers],
      &quot;success&quot;: [true/false]
    }
    &quot;&quot;&quot;

examples = [
    HumanMessage(
        &quot;&quot;&quot;&quot;
        1.  Decides, unanimously, to join the applications;
        2.  Holds, by five votes to two, that there would be no violation of Article 3, Article 2-1 §§ 5  in conjunction with 1 of Protocol No. 1 of the Convention in the event of the applicants’ extradition to Kyrgyzstan;
        3.  Decides, by six votes to one, to continue to indicate to the Government under Rule 39 of the Rules of Court that it is desirable, in the interests of the proper conduct of the proceedings, not to extradite.
        4.  Holds, by fifteen votes to two, that there is a violation of Article 6 § 1, 9(6) taken in conjunction with 14, and P1-2  of the Convention.
        &quot;&quot;&quot;,
        name=&quot;example_user&quot;
    ),
    AIMessage(
        &quot;&quot;,
        name=&quot;example_assistant&quot;,
        tool_calls=[
            {&quot;name&quot;: &quot;Json&quot;, &quot;args&quot;: {&quot;violated_articles&quot;: [&quot;6-1&quot;,&quot;9-6&quot;,&quot;14&quot;,&quot;P1-2&quot;], &quot;non_violated_articles&quot;: [&quot;3&quot;, &quot;2-1&quot;, &quot;5&quot;, &quot;P1-1&quot;], &quot;success&quot;: True}, &quot;id&quot;: &quot;1&quot;}
        ],
    ),
    ToolMessage(json.dumps({&quot;violated_articles&quot;: [&quot;6-1&quot;,&quot;9-6&quot;,&quot;14&quot;,&quot;P1-2&quot;], &quot;non_violated_articles&quot;: [&quot;3&quot;, &quot;2-1&quot;, &quot;5&quot;, &quot;P1-1&quot;], &quot;success&quot;: True}), tool_call_id=&quot;1&quot;),
    AIMessage(
        json.dumps({&quot;violated_articles&quot;: [&quot;6-1&quot;,&quot;9-6&quot;,&quot;14&quot;,&quot;P1-2&quot;], &quot;non_violated_articles&quot;: [&quot;3&quot;, &quot;2-1&quot;, &quot;5&quot;, &quot;P1-1&quot;], &quot;success&quot;: True}),
        name=&quot;example_assistant&quot;,
    ),
]


conn, cursor = connect_psql()
ids = retrieve_unprocessed_ids(cursor)
chat_model = ChatHuggingFace(llm=llm)
llm_with_json = chat_model.bind_tools(tools, tool_choice=&quot;Json&quot;)

for id_tuple in ids[:1]:
    id = id_tuple[0]
    print(id)
    conclusion = retrieve_conclusion(cursor, id)
    if conclusion:
        user_prompt = conclusion[0]
        user_prompt = &quot;&quot;&quot;
    Holds, by five votes to two, that there would be no violation of Article 3 of the Convention in the event of the applicants’ extradition to Kyrgyzstan;
        &quot;&quot;&quot;

        try:
            response = None
            # Create the messages list
            few_shot_prompt = ChatPromptTemplate.from_messages(
                [
                    (&quot;system&quot;, system_message_content),
                    *examples,
                    (&quot;human&quot;, &quot;{query}&quot;),
                ]
            )
            chain = {&quot;query&quot;: RunnablePassthrough()} | few_shot_prompt | llm_with_json
            response = chain.invoke(user_prompt)
</code></pre>
<p>Ī have tried to change the arguments to a string, to match what the arguments are for the code that runs without any examples. The output for that code is:</p>
<blockquote>
<p>content='' additional_kwargs={'tool_calls': [ChatCompletionOutputToolCall(function=ChatCompletionOutputFunctionDefinition(<strong>arguments={'non_violated_articles': ['Article 3'], 'success': True, 'violated_articles': []}, name='Json', description=None</strong>), id='0', type='function')]} response_metadata={'token_usage': ChatCompletionOutputUsage(completion_tokens=50, prompt_tokens=199, total_tokens=249), 'model': '', 'finish_reason': 'eos_token'} id='run-2ed7c88a-ec65-4b31-8025-9013f3b7da9b-0' invalid_tool_calls=[{'name': 'Json', 'args': '{&quot;non_violated_articles&quot;: [&quot;Article 3&quot;], &quot;success&quot;: True, &quot;violated_articles&quot;: []}', 'id': '0', 'error': None}]</p>
</blockquote>
","huggingface"
"78634779","Stable diffusion three no unet being passed","2024-06-17 21:46:29","","0","140","<huggingface><stable-diffusion><unet-neural-network>","<p>Hi I am trying to use stable diffusion three when using the <code>StableDiffusionPipeline</code> but get the error of.</p>
<p>It is missing the unet attribute</p>
<pre><code>Pipeline &lt;class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'&gt; expected {'unet', 'feature_extractor', 'text_encoder', 'image_encoder', 'tokenizer', 'scheduler', 'vae', 'safety_checker'}, but only {'tokenizer', 'text_encoder', 'scheduler', 'vae'} were passed.
</code></pre>
<p>I am using &quot;stabilityai/stable-diffusion-3-medium&quot; from hugging face</p>
<p>I have tried using the <code>StableDiffusion3Pipeline</code> but that also does not work as there is not unet attribute which i need for sample size and to get cross attention. Is there any help available?</p>
<p>Thanks</p>
<p>This is the python code that loads it</p>
<pre class=""lang-py prettyprint-override""><code>model = StableDiffusionPipeline(&quot;stabilityai/stable-diffusion-3-medium&quot;, revision=&quot;refs/pr/26&quot;, torch_dtype=torch.float16)
</code></pre>
<p>This is the stable diffusion three pipeline error</p>
<pre><code>AttributeError: 'StableDiffusion3Pipeline' object has no attribute 'unet'
</code></pre>
","huggingface"
"78634439","how to create embedding for 4bit quantized llama3 model using huggingface and langchain","2024-06-17 19:51:06","","2","161","<python><langchain><huggingface><retrieval-augmented-generation>","<p>I am trying to do a rag using longchain and huggingface,</p>
<pre><code>from langchain_huggingface import HuggingFaceEmbeddings

model_name = &quot;unsloth/llama-3-8b-Instruct-bnb-4bit&quot;
model_kwargs = {'device': device}
encode_kwargs = {'normalize_embeddings': False}
hf = HuggingFaceEmbeddings(
    model_name=model_name,
    model_kwargs=model_kwargs,
    encode_kwargs=encode_kwargs
)
...
vectorstore = Chroma.from_documents(documents=splits, embedding=hf)
</code></pre>
<p>However, I receive an error ValueError: Supplied state dict for layers.0.mlp.down_proj.weight does not contain <code>bitsandbytes__*</code> and possibly other <code>quantized_stats</code> components while creating the hf.
how should I correct? Thank you</p>
","huggingface"
"78625406","Can someone help me understand why my code is timing out?","2024-06-15 01:09:00","","0","28","<amazon-web-services><google-colaboratory><amazon-iam><amazon-sagemaker><huggingface>","<p>I'm trying to deploy a Hugging Face model with SageMaker, all from a Google Colab notebook.</p>
<p>I'm trying to follow <a href=""https://www.youtube.com/watch?v=7kDaMz3Xnkw"" rel=""nofollow noreferrer"">this RAG tutorial</a> on YouTube. The instructor seems to be leaving out an important part of the basic setup process. His code does not make any reference to tokens or how to make everything talk to each other.</p>
<p>I've followed the setup in the video, and copied <a href=""https://huggingface.co/mistralai/Mistral-7B-v0.1?sagemaker_deploy=true"" rel=""nofollow noreferrer"">the code directly from SageMaker</a>. When that didn't run, I spent several hours troubleshooting and eventually discovered that I have to declare my role directly. I can't rely on the code they gave in the link above. Here's what I have:</p>
<pre><code>sagemaker.Session(boto3.session.Session(aws_access_key_id='MY_KEY', 
                                        aws_secret_access_key='MY_SECRET_KEY', 
                                        aws_session_token=None, region_name='us-west-1', 
                                        botocore_session=None, profile_name=None))

role = 'arn:aws:iam::MY_ID:role/sagemaker_execution_role'

# Hub Model configuration. https://huggingface.co/models
hub = {
    'HF_MODEL_ID':'mistralai/Mistral-7B-v0.1',
    'SM_NUM_GPUS': json.dumps(8),
    'HUGGING_FACE_HUB_TOKEN': 'MY_TOKEN'
}

assert hub['HUGGING_FACE_HUB_TOKEN'] != '&lt;MY_TOKEN, &quot;You have to provide a token.&quot;

# create Hugging Face Model Class
huggingface_model = HuggingFaceModel(
    image_uri=get_huggingface_llm_image_uri(&quot;huggingface&quot;,version=&quot;2.0.2&quot;),
    env=hub,
    role=role, 
)

# deploy model to SageMaker Inference
predictor = huggingface_model.deploy(
    initial_instance_count=1,
    instance_type=&quot;ml.m5.xlarge&quot;,
    container_startup_health_check_timeout=2100,
  )
</code></pre>
<p>This code times out every time. I've tried using different instance types, and that doesn't help. Why is this timing out? Thank you.</p>
","huggingface"
"78624675","TGI does not reference model weights","2024-06-14 19:27:19","","0","60","<large-language-model><huggingface><mistral-7b>","<p>My server's proxy does not allow me to go to Hugging Face. So, I downloaded <a href=""https://github.com/mistralai/mistral-inference?tab=readme-ov-file#model-download"" rel=""nofollow noreferrer"">Mistral 7B weights from GitHub</a> to another computer, <code>sftp</code>d it over to the server, and untarred the contents,</p>
<pre><code>$ tar -tvf mistral-7B-Instruct-v0.3.tar
-rw-rw---- nobody/nogroup 14496078512 2024-05-09 10:47 consolidated.safetensors
-rwxrwxrwx nobody/nogroup         202 2024-05-20 07:09 params.json
-rwxrwxrwx nobody/nogroup      587404 2024-05-20 07:09 tokenizer.model.v3
</code></pre>
<p>, to <code>$HF_HOME/hub/models--mistralai-Mistral-7B-Instruct-v0.3/</code>. However, when I</p>
<pre><code>docker run -d     
    --name=tgi-mistral-7b   
    --env HF_HUB_OFFLINE=1   
    --env HUGGING_FACE_HUB_TOKEN=$HUGGING_FACE_HUB_TOKEN     
    --env http_proxy=$http_proxy     
    --env https_proxy=$https_proxy     
    --env MAX_BATCH_TOTAL_TOKENS=32000     
    --env MAX_BATCH_PREFILL_TOKENS=16000     
    --env MAX_TOTAL_TOKENS=32000     
    --gpus all     
    --shm-size 1g     
    -p 8080:80     
    -v $volume:/data /artifactory.my_company.com/ghcr.io/huggingface/text-generation-inference:1.4.5 
    --model-id mistralai/Mistral-7B-Instruct-v0.3 
</code></pre>
<p>, I get</p>
<pre><code>huggingface_hub.utils._errors.EntryNotFoundError: No .bin weights found for model mistralai/Mistral-7B-Instruct-v0.3 and revision None.
</code></pre>
<p>How do I place/structure these weights/files so that TGI can reach them?</p>
","huggingface"
"78622949","How to extract the per sample losses during the training process?","2024-06-14 12:26:50","","0","33","<pytorch><nlp><huggingface-transformers><huggingface>","<p>I am finetuning different models on variety of NLP tasks including abstarctive summarization, question-asnwering and classification. I am using mainly the transformers library by huggingface. I am finding it difficult to extract the sample losses in an adequate way.</p>
<p>Link to the summarization notebook: <a href=""https://colab.research.google.com/drive/1aVWD-_iVZcMJ3Krt_nJQbrtFRtLAyPZU?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1aVWD-_iVZcMJ3Krt_nJQbrtFRtLAyPZU?usp=sharing</a></p>
<p>Link to the classification notebook: <a href=""https://colab.research.google.com/drive/1KicqvWNn1FnHR_dPMqnIwxtUF4OXSVo-?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1KicqvWNn1FnHR_dPMqnIwxtUF4OXSVo-?usp=sharing</a></p>
<p>Is there a more straightforward way to do this than what I am doing currently:</p>
<p>I am referring to an example on huggingface, but there they are using accelerate during the training procedure and I couldn't extract the per sample losses that way. Therefore, I am stuck with this approach of computing the loss manualy:</p>
<pre><code>logits = outputs.logits.detach().cpu()
labels = batch[&quot;labels&quot;].detach().cpu()
loss_per_sample = F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1), reduction='none')
loss_per_sample = loss_per_sample.view(logits.size(0), -1).mean(dim=1)

for i, id in enumerate(dataset[&quot;train&quot;][&quot;id&quot;][step * train_dataloader.batch_size: (step + 1) *  train_dataloader.batch_size]):
    sample_losses[id][&quot;loss&quot;].append(loss_per_sample[i].item())
    predicted_summary_ids = logits[i].argmax(dim=-1)
    sample_losses[id][&quot;predicted_summaries&quot;].append(tokenizer.decode(predicted_summary_ids, skip_special_tokens=True))
</code></pre>
<p>Worth mentioning is I am using a dictionary to store the sample id alongside with the dialogue, the target summary, the epoch losses and the corresponding predicted summaries:</p>
<pre><code>sample_losses = {
    id: {&quot;text&quot;: dialogue, &quot;target_summary&quot;: target_summary, &quot;loss&quot;: [], &quot;predicted_summaries&quot;: []}
    for id, dialogue, target_summary in zip(dataset[&quot;train&quot;][&quot;id&quot;], dataset[&quot;train&quot;][&quot;text&quot;],  dataset[&quot;train&quot;][&quot;summary&quot;])
}
</code></pre>
","huggingface"
"78617530","How to configure HuggingFaceEndpoint in Langchain","2024-06-13 11:21:39","","0","191","<langchain><huggingface>","<p>I'm trying to use this model</p>
<pre><code>from langchain_huggingface import HuggingFaceEndpoint
repo_id=&quot;google/flan-t5-large&quot;
huggingface_llm = HuggingFaceEndpoint(
huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN,
repo_id=repo_id,
temperature=0,
max_new_tokens=200)

from langchain.prompts import PromptTemplate
def flan_process(tema, pregunta):
template = &quot;Eres un experto asistente en {tema}. Responde a la siguiente pregunta: {pregunta}&quot;
prompt=PromptTemplate(template=template,input_variables=[&quot;tema&quot;,&quot;pregunta&quot;])

flan_chain = prompt | huggingface_llm

respuesta=flan_chain.invoke({&quot;tema&quot;:tema, &quot;pregunta&quot;:pregunta})

return respuesta

tema=input(&quot;Ingrese el tema: &quot;)
pregunta=input(&quot;Ingrese la pregunta: &quot;)

flan_reply=flan_process(tema, pregunta)
print(f&quot;Respuesta Flan: {flan_reply}&quot;)
</code></pre>
<p>But I always get this error The following <code>model_kwargs</code> are not used by the model: ['return_full_text', 'watermark', 'stop_sequences', 'stop'] (note: typos in the generate arguments will also show up in this list)</p>
<p>Any idea please?</p>
<p>Thanks</p>
","huggingface"
"78616285","Create API Endpoint from hugging face space","2024-06-13 06:57:15","","0","170","<huggingface>","<p>I have created a Space on Hugging Face that runs my custom machine-learning model using Gradio. It works perfectly in the web interface, but now I want to convert this Space into an API endpoint that I can call from my application.</p>
<p>Could someone guide me through the process of converting my Hugging Face Space to an API endpoint? Specifically, I am looking for:</p>
<p>Steps to set up an Inference Endpoint from my existing Space.
How to handle authentication and API keys for accessing the endpoint?
Any best practices or tips for optimizing the performance of the endpoint.</p>
","huggingface"
"78602828","Which weights change when fine-tunning a pre-trained model? (Hugging Face)","2024-06-10 14:14:39","","1","39","<nlp><huggingface-transformers><huggingface><pre-trained-model>","<p>I am using the AutoModelForSequenceClassification class to fine-tune a pre-trained model (which originally is based on GPT2 architecture)</p>
<pre><code>model = AutoModelForSequenceClassification.from_pretrained(&quot;Natooz/Maestro-REMI-bpe20k&quot;, trust_remote_code=True, torch_dtype=&quot;auto&quot;,num_labels=2)
</code></pre>
<p>And I am using the basic training loop suggested in the NLP course from hugging face:</p>
<pre><code>model.train()
for epoch in range(num_epochs):
    for batch in train_dl:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
</code></pre>
<p>My question is:</p>
<p><em><strong>1. Which weights of the model are going to change after using this training loop? Am I training all the weights of the model? Only the classification head added by the AutoModelForSequenceClassification?</strong></em></p>
<p>It wouldn't make a lot of sense to me for all weights to change, because in theory I wouldn't be &quot;transferring&quot; any knowledge from my pre-trained model to my task.</p>
<p><em><strong>2. Would it work the same if I fine-tune my pre-trained model with the trainer class instead of the training loop?</strong></em></p>
<p>Thank you so much in advance!</p>
","huggingface"
"78600690","Langchain cvs agent parsing","2024-06-10 06:05:52","","0","43","<langchain><large-language-model><huggingface>","<p>I am testing a csv agent using a Hugging Face model locally with the <code>titanic</code> dataset. The code is straightforward:</p>
<pre><code>from langchain_experimental.agents import create_csv_agent
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
# from langchain.llms.huggingface_pipeline import HuggingFacePipeline
from langchain_community.llms import HuggingFacePipeline
# from langchain_huggingface import HuggingFacePipeline
from config import set_environment
import os


def main():
    set_environment()

    local_model_path = '../models/bling-sheared-llama-1.3b-0.1/'

    tokenizer = AutoTokenizer.from_pretrained(local_model_path)
    model = AutoModelForCausalLM.from_pretrained(local_model_path, device_map = 'cuda')

    local_pipeline = pipeline('text-generation', model=model, tokenizer=tokenizer, max_length=2000) 
    llm = HuggingFacePipeline(pipeline=local_pipeline)

    #______________________________
    agent = create_csv_agent(
    llm, &quot;titanic.csv&quot;, verbose=True, handle_parsing_errors=True)
    
    user_question = &quot;How many rows are there?&quot;
    response = agent.invoke(user_question)
    print(response)

if __name__ == &quot;__main__&quot;:
    main()
</code></pre>
<p>This returns the following error even though I've added the <code>handle_parsing_errors</code> argument to the agent executor:</p>
<blockquote>
<p>ValueError: An output parsing error occurred. In order to pass this
error back to the agent and have it try again, pass
<code>handle_parsing_errors=True</code> to the AgentExecutor. This is the error:
Parsing LLM output produced both a final answer and a parse-able
action</p>
</blockquote>
<p>How does one proceed?</p>
","huggingface"
"78598935","Vertex AI - The token is not valid or not have permission","2024-06-09 15:10:04","","1","92","<huggingface><google-cloud-vertex-ai>","<p>I’ve been trying to deploy Llama 3 from Hugging Face on Google’s Vertex AI (from the Model Garden) but I can’t do it as I keep getting an error with my access token. Google keeps telling me that ‘The token is not valid or not have permission’.</p>
<p>I’ve tried read only tokens, write tokens, fine-grained tokens with all permissions enabled and just can’t get it to work.</p>
","huggingface"
"78598905","How to pass a pytorch DataLoader to huggingface Trainer? Is that even possble?","2024-06-09 14:59:00","","1","72","<nlp><huggingface><pytorch-dataloader><dataloader><huggingface-trainer>","<p>The usual steps to use the Trainer from huggingface requires that:</p>
<ol>
<li>Load the data</li>
<li>Tokenize the data</li>
<li>Pass tokenized data to Trainer</li>
</ol>
<p>MWE:</p>
<pre><code>data = generate_random_data(10000)  # Generate 10,000 samples
df = pd.DataFrame(data)
dataset = Dataset.from_pandas(df)
tokenized_datasets = dataset.map(preprocess_function, batched=True)
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets,
    eval_dataset=tokenized_datasets,
)
</code></pre>
<p>Instead if we do:</p>
<pre><code>train_dataset = convert_to_tensors(tokenized_datasets)
train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)
trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataloader,
        eval_dataset=train_dataloader,
)
</code></pre>
<p>I get an error key 0, cause <code>train_dataloader</code> is not in dictionary format, I think?
Is there a way to pass directly a pytorch DataLoader to huggingface Trainer?</p>
","huggingface"
"78594832","ImportError and TypeError Issues in Nougat OCR with BARTDecoder and cached_property","2024-06-08 05:43:48","","0","188","<machine-learning><pytorch><nlp><huggingface-transformers><huggingface>","<p>I'm facing issues while running an OCR process using Nougat with two different errors for two different users. The errors are related to importing <code>cached_property</code> and an unexpected keyword argument <code>cache_position</code>. Below are the full error messages for both cases:</p>
<p><strong>Error 1: ImportError for <code>cached_property</code></strong></p>
<pre><code>ImportError: cannot import name 'cached_property' from 'nougat.utils' (/lfs/skampere1/0/emilyhyf/miniconda/lib/python3.12/site-packages/nougat/utils/__init__.py)
OCRing with base model failed on /lfs/skampere1/0/emilyhyf/massive-evaporation-4-math/data/debug_pdfs/dummy_folder1/The_IMO_Compendium.pdf... trying small model
Traceback (most recent call last):
  File &quot;/lfs/skampere1/0/emilyhyf/miniconda/bin/nougat&quot;, line 5, in &lt;module&gt;
    from predict import main
  File &quot;/lfs/skampere1/0/emilyhyf/miniconda/lib/python3.12/site-packages/predict.py&quot;, line 18, in &lt;module&gt;
    from nougat import NougatModel
  File &quot;/lfs/skampere1/0/emilyhyf/miniconda/lib/python3.12/site-packages/nougat/__init__.py&quot;, line 1, in &lt;module&gt;
    from nougat.app import Nougat
  File &quot;/lfs/skampere1/0/emilyhyf/miniconda/lib/python3.12/site-packages/nougat/app.py&quot;, line 5, in &lt;module&gt;
    from nougat.asgi import serve
  File &quot;/lfs/skampere1/0/emilyhyf/miniconda/lib/python3.12/site-packages/nougat/asgi.py&quot;, line 6, in &lt;module&gt;
    from nougat.context.request import Request
  File &quot;/lfs/skampere1/0/emilyhyf/miniconda/lib/python3.12/site-packages/nougat/context/__init__.py&quot;, line 1, in &lt;module&gt;
    from nougat.context.request import Request
  File &quot;/lfs/skampere1/0/emilyhyf/miniconda/lib/python3.12/site-packages/nougat/context/request.py&quot;, line 8, in &lt;module&gt;
    from nougat.utils import cached_property, File
ImportError: cannot import name 'cached_property' from 'nougat.utils' (/lfs/skampere1/0/emilyhyf/miniconda/lib/python3.12/site-packages/nougat/utils/__init__.py)
OCRing with small model failed on /lfs/skampere1/0/emilyhyf/massive-evaporation-4-math/data/debug_pdfs/dummy_folder1/The_IMO_Compendium.pdf
The following files failed OCRing and need manual review:
/lfs/skampere1/0/emilyhyf/massive-evaporation-4-math/data/debug_pdfs/putnam_and_beyond.pdf
/lfs/skampere1/0/emilyhyf/massive-evaporation-4-math/data/debug_pdfs/dummy_folder1/The_IMO_Compendium.pdf
Time taken: 8.497612476348877 seconds, 0.1416268746058146 minutes, 0.00236044791009691 hours
</code></pre>
<p><strong>Error 2: TypeError for <code>cache_position</code> in BARTDecoder</strong></p>
<pre><code>TypeError: BARTDecoder.prepare_inputs_for_inference() got an unexpected keyword argument 'cache_position'
-&gt; Cannot close object, library is destroyed. This may cause a memory leak!
-&gt; Cannot close object, library is destroyed. This may cause a memory leak!
-&gt; Cannot close object, library is destroyed. This may cause a memory leak!
OCRing with base model failed on /lfs/skampere1/0/naveenkc/contest_scraper/test_nougat/731991.pdf... trying small model
/lfs/skampere1/0/naveenkc/miniconda/lib/python3.12/site-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3587.)
  return _VF.meshgrid(tensors, **kwargs) # type: ignore[attr-defined]
  0%|                                                 | 0/288 [00:24&lt;?, ?it/s]
Traceback (most recent call last):
  File &quot;/lfs/skampere1/0/naveenkc/miniconda/bin/nougat&quot;, line 8, in &lt;module&gt;
    sys.exit(main())
             ^^^^^^
  File &quot;/lfs/skampere1/0/naveenkc/miniconda/lib/python3.12/site-packages/predict.py&quot;, line 167, in main
    model_output = model.inference(
                   ^^^^^^^^^^^^^^^^
  File &quot;/lfs/skampere1/0/naveenkc/miniconda/lib/python3.12/site-packages/nougat/model.py&quot;, line 592, in inference
    decoder_output = self.decoder.model.generate(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/lfs/skampere1/0/naveenkc/miniconda/lib/python3.12/site-packages/torch/utils/_contextlib.py&quot;, line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File &quot;/lfs/skampere1/0/naveenkc/miniconda/lib/python3.12/site-packages/transformers/generation/utils.py&quot;, line 1758, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File &quot;/lfs/skampere1/0/naveenkc/miniconda/lib/python3.12/site-packages/transformers/generation/utils.py&quot;, line 2394, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: BARTDecoder.prepare_inputs_for_inference() got an unexpected keyword argument 'cache_position'
-&gt; Cannot close object, library is destroyed. This may cause a memory leak!
-&gt; Cannot close object, library is destroyed. This may cause a memory leak!
-&gt; Cannot close object, library is destroyed. This may cause a memory leak!
-&gt; Cannot close object, library is destroyed. This may cause a memory leak!
-&gt; Cannot close object, library is destroyed. This may cause a memory leak!
OCRing with small model failed on /lfs/skampere1/0/naveenkc/contest_scraper/test_nougat/731991.pdf
The following files failed OCRing and need manual review:
/lfs/skampere1/0/naveenkc/contest_scraper/test_nougat/731991.pdf
Time taken: 65.70022702217102 seconds, 1.0950037837028503 minutes, 0.018250063061714172 hours
</code></pre>
<p><strong>Environment:</strong></p>
<ul>
<li>Python 3.12</li>
<li>Nougat</li>
<li>Torch</li>
<li>Transformers</li>
</ul>
<p><strong>Questions:</strong></p>
<ol>
<li><p><strong>ImportError</strong>: How can I resolve the <code>ImportError: cannot import name 'cached_property' from 'nougat.utils'</code>? What could be causing this issue, and is there an alternative approach to import <code>cached_property</code>?</p>
</li>
<li><p><strong>TypeError</strong>: How can I address the <code>TypeError: BARTDecoder.prepare_inputs_for_inference() got an unexpected keyword argument 'cache_position'</code>? Is this due to a version mismatch between <code>transformers</code> and <code>torch</code>? How can I ensure compatibility?</p>
</li>
</ol>
<p>Any guidance on these issues would be greatly appreciated! Thank you in advance!</p>
","huggingface"
"78590413","Performing Function Calling with Mistral AI through Hugging Face Endpoint","2024-06-07 07:17:37","","1","255","<artificial-intelligence><large-language-model><huggingface><py-langchain><mistral-7b>","<p>I am trying to perform function calling using Mistral AI through the Hugging Face endpoint. Mistral AI requires input in a specific string format (assistant: ... \n user: ...). However, the input format provided is not accepted as a list of messages or a prompt value (e.g., ChatPromptTemplate). When using the string format, the output only calls the function without pushing the received content from the tool call back to Mistral AI LLM.</p>
<p>I was trying to perform function calling using Mistral AI, which initially worked as expected. My process involved calling an external API to fetch real-time data and then attempting to push this data back to the LLM (Language Model) using messages with the tool call name and ID. I expected the final output to combine my data with the LLM's response, resulting in a cohesive answer. However, instead of getting the expected response, the system kept returning a function call as the response again.</p>
<pre><code># Import necessary libraries
from langchain_huggingface.chat_models import ChatHuggingFace
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain.document_loaders import TextLoader
from langchain.llms import HuggingFacePipeline
from langchain_text_splitters.character import CharacterTextSplitter
from langchain.prompts import PromptTemplate
from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_core.output_parsers.openai_tools import PydanticToolsParser
from langchain_huggingface import HuggingFaceEndpoint
from langchain_core.utils.function_calling import convert_to_openai_function
from langchain.prompts import PromptTemplate, ChatPromptTemplate
from langchain_core.messages import (
    HumanMessage,
    SystemMessage,
    AIMessage,
    ChatMessage
)
from dotenv import load_dotenv, find_dotenv
import os
import serpapi
import serpapi.client




# Load environment variables
_ = load_dotenv(find_dotenv())
serp_key = os.getenv('SERP_API_KEY')




# Configure the LLM endpoint
llm = HuggingFaceEndpoint(
    repo_id='mistralai/Mistral-7B-Instruct-v0.3',
    huggingfacehub_api_token='*********',
    task=&quot;text-generation&quot;,
    model_kwargs={
        &quot;min_length&quot;: 200,
        &quot;max_length&quot;: 2000,
        &quot;num_return_sequences&quot;: 1
    },
    temperature=0.5
)



# Define prompt and functions
prompt_str = &quot;I want you to act as travel organizer, Take input from user extract the necessary details plan the trip route using details you need to plan the route and time going to be spent&quot;
functions = [
    {
        &quot;name&quot;: &quot;plan_holiday&quot;,
        &quot;description&quot;: &quot;Plan a holiday based on user's interests&quot;,
        &quot;parameters&quot;: {
            &quot;type&quot;: &quot;object&quot;,
            &quot;properties&quot;: {
                &quot;destination&quot;: {
                    &quot;type&quot;: &quot;string&quot;,
                    &quot;description&quot;: &quot;The destination of the holiday&quot;,
                },
                &quot;duration&quot;: {
                    &quot;type&quot;: &quot;integer&quot;,
                    &quot;description&quot;: &quot;The duration of the trip in holiday&quot;,
                },
            },
            &quot;required&quot;: [&quot;destination&quot;, &quot;duration&quot;],
        },
    }
]

# Generate messages
messages = [
    SystemMessage(content=prompt_str),
    HumanMessage(
        content=&quot;I am thinking of having a 12 day long vacation in Venice, can you help me plan it?&quot;
    ),
]

# Format prompt
chat_pr = ChatPromptTemplate(messages=messages).format()
print(chat_pr)

# Create ChatHuggingFace model
chat_model = ChatHuggingFace(llm=llm)
functions = [convert_to_openai_function(i) for i in functions]

# Bind tools to the model
llm_with_holiday_planning = chat_model.bind_tools(functions, tool_choice='auto')

# Invoke the model with the formatted prompt
response = llm_with_holiday_planning.invoke(chat_pr)
print(response)

# Append response to messages
messages.append(ChatMessage(role='assistant', content='', tool_calls=response.additional_kwargs['tool_calls'], id=response.additional_kwargs['tool_calls'][0]['id']))

# Function to search Google for a place
def search_google_for(place_search):
    sero_cj = serpapi.Client(api_key=serp_key)
    result = sero_cj.search(
        params={
            &quot;q&quot;: &quot;Venice tourist places&quot;,
            &quot;location&quot;: &quot;India&quot;,
            &quot;hl&quot;: &quot;en&quot;,
            &quot;gl&quot;: &quot;In&quot;,
            &quot;google_domain&quot;: &quot;google.com&quot;,
        }
    )
    return result

# Fetch results using the function
results = search_google_for(response.additional_kwargs['tool_calls'][0].function['arguments'])
print(response.additional_kwargs['tool_calls'][0])

# Extract and compile information about places
info_about_places = &quot;&quot;
if results.get(&quot;organic_results&quot;):
    for result in results[&quot;organic_results&quot;]:
        title = result.get(&quot;title&quot;)
        snippet = result.get(&quot;snippet&quot;)
        link = result.get(&quot;link&quot;)
        info_about_places += snippet

# Append function result to messages
messages.append(ChatMessage(role='function', name=response.additional_kwargs['tool_calls'][0].function['name'], content=info_about_places, tool_call_id=response.additional_kwargs['tool_calls'][0]['id']))

# Format and invoke the updated prompt
chat_pr = ChatPromptTemplate(messages=messages).format()
print(chat_pr)
print(llm_with_holiday_planning.invoke(chat_pr))
</code></pre>
<p>Setting Up the Environment:</p>
<p>Loaded necessary libraries and environment variables.
Configured the Mistral AI model endpoint using the Hugging Face API.
Defining the Prompt and Functions:</p>
<p>Created a prompt instructing the model to act as a travel organizer.
Defined functions such as plan_holiday to handle specific tasks.
Generating Messages:</p>
<p>Constructed messages using SystemMessage and HumanMessage to simulate a conversation.
Binding Tools to the Model:</p>
<p>Converted the defined functions to an OpenAI-compatible format.
Bound these functions to the chat model using bind_tools.
Invoking the Model:</p>
<p>Attempted to invoke the model with the formatted prompt and expected it to call the defined function and use the external API data.
Expected Outcome:
I expected the model to process the function call, fetch data from the external API, and integrate this data into a coherent response along with the LLM's generated content.
Actual Result:
Instead of providing a combined response with both the fetched data and the LLM's generated content, the model repeatedly returned a function call as the response.
<code>your text</code></p>
","huggingface"
"78589268","Fine Tune Huggingface model via Trainer API without labels?","2024-06-06 22:19:47","","0","31","<huggingface-transformers><large-language-model><huggingface><fine-tuning><huggingface-trainer>","<p>I am following Huggingfaces <a href=""https://huggingface.co/docs/transformers/training"" rel=""nofollow noreferrer"">Tutorial on fine-tuning a model</a>. Unfortunately, they only show the procedure for fine-tuning BERT to a classifier by providing labeled data.
My case is a bit different: I want to fine-tune gpt-2 to generate text in a specific writing style. So my input would be just the text (in that style) without any label. I have tried the code below but that doesn't work well and results in very bad quality that
includes many special characters.</p>
<pre><code>training_args = TrainingArguments(
    output_dir=&quot;./results&quot;,
    overwrite_output_dir=True,
    num_train_epochs=3,
    per_device_train_batch_size=4,
    save_steps=10_000,
    save_total_limit=2,
    prediction_loss_only=True
)

data_collator = DataCollatorForLanguageModeling(
    tokenizer=gen_tokenizer,
    mlm=False,  # Suggestion from ChatGPT
)

# Initialize the Trainer
trainer = Trainer(
    model=gen_model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=tokenized_dataset,
)   

trainer.train()
</code></pre>
<p>Is there anything I should consder/change in my code? I am grateful for any answer because I couldn't find anything online</p>
","huggingface"
"78584629","TypeError: HuggingFaceLLM.__init__() got an unexpected keyword argument 'hf_token'","2024-06-06 05:25:44","","1","135","<huggingface>","<pre class=""lang-py prettyprint-override""><code>LLM = HuggingFaceLLM(
    hf_token= os.getenv(&quot;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&quot;),
    context_window=4096,
    max_new_tokens=256,
    generate_kwargs={&quot;temperature&quot;: 0.0, &quot;do_sample&quot;: False},
    system_prompt=system_prompt,
    query_wrapper_prompt=query_wrapper_prompt,
    tokenizer_name=&quot;meta-llama/Llama-2-7b-chat-hf&quot;,
    model_name=&quot;meta-llama/Llama-2-7b-chat-hf&quot;,
    device_map=&quot;auto&quot;,
    # uncomment this if using CUDA to reduce memory usage
    model_kwargs={&quot;torch_dtype&quot;: torch.float16 , &quot;load_in_8bit&quot;:True}
)
</code></pre>
<p>This is the error I'm receiving: <code>TypeError: HuggingFaceLLM.__init__() got an unexpected keyword argument 'hf_token'</code></p>
<p>I have upgraded to the latest hugging face as well.</p>
","huggingface"
"78581880","How to get the grad of input image of StableDiffusionXLControlNetPipeline","2024-06-05 14:53:48","","0","21","<pytorch><artificial-intelligence><huggingface><stable-diffusion><diffusers>","<p>I have another model to generate the input image of StableDiffusionXLControlNetPipeline, and I used the loss based on the output of StableDiffusionXLControlNetPipeline, I want my model can get the grad of the loss.</p>
<p>It seems that the StableDiffusionXLControlNetPipeline's input image can not get the grad.</p>
","huggingface"
"78581041","llama-index,uncharted and llama2:7b run locally to generate Index","2024-06-05 12:38:14","78583483","0","197","<machine-learning><huggingface><llama-index><ollama><llama3>","<p>I wanted to use llama-index locally with ollama and llama3:8b to index utf-8 json file. I dont have a gpu. I use uncharted to convert docs into json. Now If it is not possible to use llama-index locally without GPU I wanted to use hugging face inference API. But I am not certain if it is free.  Can anyone suggest a way?</p>
<p>This is my python code:</p>
<pre>

    from llama_index.core import Document, SimpleDirectoryReader, VectorStoreIndex
    from llama_index.llms.ollama import Ollama
    import json
    from llama_index.core import Settings
    
    
    # Convert the JSON document into LlamaIndex Document objects
    with open('data/UBER_2019.json', 'r',encoding='utf-8') as f:
        json_doc = json.load(f)
    documents = [Document(text=str(doc)) for doc in json_doc]
    
    # Initialize Ollama with the local LLM
    ollama_llm = Ollama(model=""llama3:8b"")
    Settings.llm = ollama_llm
    
    # Create the index using the local LLM
    index = VectorStoreIndex.from_documents(documents)#, llm=ollama_llm)

</pre>
<p>But i keep getting error that there is no OPENAI key. I wanted to use llama2 so that i dont require OPENAI key</p>
<p>Can anyone suggest what i am doing wrong? Also can i use huggingfaceinference API to do indexing of a local json file for free?</p>
","huggingface"
"78575603","RetrievalQA Generates Answers Instead of Retrieving from Text File -llama2,langchain","2024-06-04 13:10:23","","0","46","<google-colaboratory><langchain><large-language-model><huggingface><llama>","<p>In the implemented retrievalQA,llama generates answers by itself. When debugging it, the prompt does tell it not to make up answers and to follow the context.</p>
<p>Here's the code:</p>
<pre><code>from langchain.text_splitter import CharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import TextLoader


embeddings = HuggingFaceEmbeddings(model_name=&quot;all-MiniLM-L6-v2&quot;)
# Equivalent to SentenceTransformerEmbeddings(model_name=&quot;all-MiniLM-L6-v2&quot;)

# text split inot chunks
text_splitter=CharacterTextSplitter(
    separator=&quot;\n&quot;,
    chunk_size=200,
    chunk_overlap=0
)

# get document from loader
loader=TextLoader(&quot;facts.txt&quot;)
# find file and extract content from it
docs=loader.load_and_split(text_splitter=text_splitter)

# get vector store
db=Chroma.from_documents(
    docs, #calc embeddings for these chunks
    embedding=embeddings,
    persist_directory=&quot;emb&quot; #saved inside emb directory
)

results=db.similarity_search_with_score(&quot;Give fact about Ostrich&quot;,
k=2) 
# if i dont want search score, use db.db.similarity_search
for result in results:
    # we get four results by default. an array of tuples
    print(&quot;\n&quot;)
    # the search score
    print(result[1])
    # the actual doc
    print(result[0].page_content) #works correctly

Seperate block of code next-&gt;
from langchain.embeddings.base import Embeddings
from langchain.vectorstores import Chroma
from langchain.schema import BaseRetriever

class RedundantFilterRetriever(BaseRetriever):
    embeddings:Embeddings
    chroma:Chroma
    def get_relevant_documents(self,query):
        # calc embeddings for query string
        emb=self.embeddings.embed_query(query)
        print(query,&quot;embedding query&quot;,emb)
        # take embeddings and feed them into the
        # max_marginal_relevance_search_by_vector
        # return self.chroma.max_marginal_relevance_search_by_vector(
        #     embedding=emb,
        #     lambda_mult=0.8
        # )
        # Retrieve relevant documents using the embeddings
        docs = self.chroma.max_marginal_relevance_search_by_vector(
            embedding=emb,
            lambda_mult=0.8
        )
        
        for doc in docs:
            print(f&quot;Retrieved Document: {doc}&quot;)
        
        return docs
        
    async def aget_relevant_documents(self):
        return []


# from langchain_community.llms import Ollama
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain_community.chat_models import ChatOllama
# from redundant_filter_retriever import RedundantFilterRetriever
from langchain_community.embeddings import OllamaEmbeddings
import langchain
from langchain.llms import HuggingFacePipeline

# langchain.debug=True
embeddings = HuggingFaceEmbeddings(model_name=&quot;all-MiniLM-L6-v2&quot;)
# Equivalent to SentenceTransformerEmbeddings(model_name=&quot;all-MiniLM-L6-v2&quot;)
# embeddings=OllamaEmbeddings(model=&quot;llama2:latest&quot;)

db=Chroma(
    embedding_function=embeddings,
    persist_directory=&quot;emb&quot;
)

# get language model
# chat = ChatOllama(model=&quot;llama2&quot;)
chat = HuggingFacePipeline(pipeline=generate_text)

retriever=RedundantFilterRetriever(
    embeddings=embeddings,
    chroma=db
)

# old retriver
# retriever=db.as_retriever()

chain=RetrievalQA.from_chain_type(
    llm=chat,
    retriever=retriever,
    chain_type=&quot;stuff&quot;
)

# result=chain.invoke(&quot;Give me fact about ostrich from txt file only. dont generate yourself or ill murder u&quot;)
result=chain.invoke(&quot;Give me fact about ostrich&quot;)

print(result)

</code></pre>
<p>This outputs:</p>
<pre><code>[chain/start] [1:chain:RetrievalQA] Entering Chain run with input:
{
  &quot;query&quot;: &quot;Give me fact about ostrich&quot;
}
Retrieved Document: page_content='1. &quot;Dreamt&quot; is the only English word that ends with the letters &quot;mt.&quot;\n2. An ostrich\'s eye is bigger than its brain.\n3. Honey is the only natural food that is made without destroying any kind of life.' metadata={'source': 'facts.txt'}
Retrieved Document: page_content='101. Avocado has more protein than any other fruit.\n102. Ostriches can run faster than horses.\n103. The Golden Poison Dart Frog’s skin has enough toxins to kill 100 people.' metadata={'source': 'facts.txt'}
Retrieved Document: page_content=&quot;112. Saturn's density is low enough that the planet would float in water.\n113. Starfish can regenerate their own arms.\n114. French Fries originated in Belgium.&quot; metadata={'source': 'facts.txt'}
Retrieved Document: page_content='81. The male seahorse carries the eggs until they hatch instead of the female.\n82. St. Lucia is the only country in the world named after a woman.' metadata={'source': 'facts.txt'}
[chain/start] [1:chain:RetrievalQA &gt; 3:chain:StuffDocumentsChain] Entering Chain run with input:
[inputs]
[chain/start] [1:chain:RetrievalQA &gt; 3:chain:StuffDocumentsChain &gt; 4:chain:LLMChain] Entering Chain run with input:
{
  &quot;question&quot;: &quot;Give me fact about ostrich&quot;,
  &quot;context&quot;: &quot;1. \&quot;Dreamt\&quot; is the only English word that ends with the letters \&quot;mt.\&quot;\n2. An ostrich's eye is bigger than its brain.\n3. Honey is the only natural food that is made without destroying any kind of life.\n\n101. Avocado has more protein than any other fruit.\n102. Ostriches can run faster than horses.\n103. The Golden Poison Dart Frog’s skin has enough toxins to kill 100 people.\n\n112. Saturn's density is low enough that the planet would float in water.\n113. Starfish can regenerate their own arms.\n114. French Fries originated in Belgium.\n\n81. The male seahorse carries the eggs until they hatch instead of the female.\n82. St. Lucia is the only country in the world named after a woman.&quot;
}
[llm/start] [1:chain:RetrievalQA &gt; 3:chain:StuffDocumentsChain &gt; 4:chain:LLMChain &gt; 5:llm:HuggingFacePipeline] Entering LLM run with input:
{
  &quot;prompts&quot;: [
    &quot;Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\n1. \&quot;Dreamt\&quot; is the only English word that ends with the letters \&quot;mt.\&quot;\n2. An ostrich's eye is bigger than its brain.\n3. Honey is the only natural food that is made without destroying any kind of life.\n\n101. Avocado has more protein than any other fruit.\n102. Ostriches can run faster than horses.\n103. The Golden Poison Dart Frog’s skin has enough toxins to kill 100 people.\n\n112. Saturn's density is low enough that the planet would float in water.\n113. Starfish can regenerate their own arms.\n114. French Fries originated in Belgium.\n\n81. The male seahorse carries the eggs until they hatch instead of the female.\n82. St. Lucia is the only country in the world named after a woman.\n\nQuestion: Give me fact about ostrich\nHelpful Answer:&quot;
  ]
}
[llm/end] [1:chain:RetrievalQA &gt; 3:chain:StuffDocumentsChain &gt; 4:chain:LLMChain &gt; 5:llm:HuggingFacePipeline] [5.64s] Exiting LLM run with output:
{
  &quot;generations&quot;: [
    [
      {
        &quot;text&quot;: &quot; Sure! Here's a fun fact about ostriches: They can run faster than 45 km/h (28 mph), making them the fastest birds on land!&quot;,
        &quot;generation_info&quot;: null,
        &quot;type&quot;: &quot;Generation&quot;
      }
    ]
  ],
  &quot;llm_output&quot;: null,
  &quot;run&quot;: null
}
[chain/end] [1:chain:RetrievalQA &gt; 3:chain:StuffDocumentsChain &gt; 4:chain:LLMChain] [5.65s] Exiting Chain run with output:
{
  &quot;text&quot;: &quot; Sure! Here's a fun fact about ostriches: They can run faster than 45 km/h (28 mph), making them the fastest birds on land!&quot;
}
[chain/end] [1:chain:RetrievalQA &gt; 3:chain:StuffDocumentsChain] [5.65s] Exiting Chain run with output:
{
  &quot;output_text&quot;: &quot; Sure! Here's a fun fact about ostriches: They can run faster than 45 km/h (28 mph), making them the fastest birds on land!&quot;
}
[chain/end] [1:chain:RetrievalQA] [5.67s] Exiting Chain run with output:
{
  &quot;result&quot;: &quot; Sure! Here's a fun fact about ostriches: They can run faster than 45 km/h (28 mph), making them the fastest birds on land!&quot;
}
{'query': 'Give me fact about ostrich', 'result': &quot; Sure! Here's a fun fact about ostriches: They can run faster than 45 km/h (28 mph), making them the fastest birds on land!&quot;}

</code></pre>
<p>When i use this, it returns correct answer:
result=chain.invoke(&quot;Give me fact about ostrich from txt file only. dont generate yourself&quot;)
but when i do this, it doesn't:
result=chain.invoke(&quot;Give me fact about ostrich&quot;)</p>
<p>I am following a tutorial where the second invoke returns the correct response from txt file but it's not working for me.</p>
","huggingface"
"78572132","Deploying a custom pytorch model on huggingface","2024-06-03 19:05:57","","1","31","<pytorch><huggingface>","<p>I wanted share a Custom Pytorch model based on fine-tuning HuBERT (an audio classification model), also wanted to share with it the custom dataset class cause the processor is working inside the dataset object, i have the state_dict saved on a .pth file and the model/dataset classes.
When i tried to simply make a config files obviously it didn't work, tried to re-train teh model via a Trainer object and push it from there but when i tried to test it with the &quot;use this model&quot; that was created automatically it also didn't work .</p>
<p>My main questions is what's the thing am doing wrong and what should i provide in the repo according to what i wanna accomplish (the trickest part for me is how to share the processor and wetehr should i remover the dataset class and leaver the processor simple as it is and the users should build it they way they want )</p>
<p>Here's the repo that was made by the trainer <a href=""https://huggingface.co/KhaldiAbderrhmane/hubert-emotion-recognition/tree/main"" rel=""nofollow noreferrer"">Here</a></p>
<p>And here's the model architecture :</p>
<pre><code>class EmotionClassifierHuBERT(nn.Module):
    def __init__(self, hidden_size, num_classes):
        super(EmotionClassifierHuBERT, self).__init__()
        self.hubert = HubertModel.from_pretrained(&quot;facebook/hubert-large-ls960-ft&quot;)
        self.conv1 = nn.Conv1d(in_channels=1024, out_channels=512, kernel_size=3, padding=1)
        self.conv2 = nn.Conv1d(in_channels=512, out_channels=256, kernel_size=3, padding=1)
        self.transformer_encoder = nn.TransformerEncoderLayer(d_model=256, nhead=8)
        self.bilstm = nn.LSTM(input_size=256, hidden_size=hidden_size, num_layers=2, batch_first=True, bidirectional=True)
        self.fc = nn.Linear(hidden_size * 2, num_classes)  # * 2 for bidirectional

    def forward(self, x):
        
        with torch.no_grad():
            features = self.hubert(x).last_hidden_state
        features = features.transpose(1, 2)
        
        x = torch.relu(self.conv1(features))
        x = torch.relu(self.conv2(x))
        x = x.transpose(1, 2)
        x = self.transformer_encoder(x)
        x, _ = self.bilstm(x)
        x = self.fc(x[:, -1, :])
        return x
</code></pre>
<p>And here's the Dataset class :</p>
<pre><code>class SpeechEmotionDataset(Dataset):
    def __init__(self, data_path, processor=AutoProcessor.from_pretrained(&quot;facebook/hubert-large-ls960-ft&quot;)):
        self.data_path = data_path
        self.file_list = [f for f in os.listdir(data_path) if f.endswith('.wav')]
        self.labels = {name: idx for idx, name in enumerate(sorted(set(f.split('_')[2] for f in self.file_list)))}
        self.processor = processor
    
    def __len__(self):
        return len(self.file_list)
    
    def __getitem__(self, idx):
        file_path = os.path.join(self.data_path, self.file_list[idx])
        waveform, sr = torchaudio.load(file_path)
        input_values = self.processor(waveform, return_tensors=&quot;pt&quot;).input_values
        label = self.labels[self.file_list[idx].split('_')[2]]
        return input_values, label
enter code here


</code></pre>
","huggingface"
"78569200","argument needs to be of type (SquadExample, dict), while trying to build a RAG","2024-06-03 08:33:57","","1","112","<langchain><huggingface>","<p>Getting this error for QA using the below code, how can I create a SquadExample? I went to the docs and there weren't any information on how SquadExample dict must be created. Can someone help?</p>
<pre><code>    prompt_template = &quot;&quot;&quot;
    As a US Census information assistant, answer the question given the context.

    question: {question}

    context: {context}
    &quot;&quot;&quot;

    QA_CHAIN_PROMPT = PromptTemplate(template=prompt_template, input_variables=[&quot;question&quot;, &quot;context&quot;]) 

   question =  &quot;&quot;&quot;What were the trends in median household income across different states in the United    States between 2021 and 2022.&quot;&quot;&quot;

    from langchain.chains import create_retrieval_chain
    from langchain.chains.combine_documents import create_stuff_documents_chain
    from langchain_core.prompts import ChatPromptTemplate

    system_prompt = (
               &quot;&quot;&quot;As a US Census information assistant answer the question {question}.&quot;&quot;&quot;
               &quot;Context: {context}&quot;
           )
prompt = ChatPromptTemplate.from_messages(
    [
        (&quot;system&quot;, system_prompt),
        (&quot;human&quot;, &quot;{question}&quot;),
    ]
)

question_answer_chain = create_stuff_documents_chain(llm, QA_CHAIN_PROMPT)
chain = create_retrieval_chain(vectorstore.as_retriever(), question_answer_chain)
   
result = chain.invoke({&quot;input&quot;: question, &quot;question&quot;: question})

</code></pre>
<p>I tried going to the docs and looked for information on how to create a SquadExample using (create_sample) but I'm not sure on how to implement the same. A little assistance would be of great help!</p>
","huggingface"
"78559475","Hugging Face model not loading - serverless api","2024-05-31 10:09:16","","0","62","<loading><huggingface>","<p>I tried to test a custom model by using the serverless api on vs code but I keep receiving this <code>Status Code: 503</code> Response Text:</p>
<blockquote>
<p>{&quot;error&quot;:&quot;Model merve/gemma-7b-it-8bit is currently loading&quot;,&quot;estimated_time&quot;:373.2806091308594}</p>
</blockquote>
<p>I tried to increase the wait time and retry times but it just loops and nothing changes, model still not loading.</p>
","huggingface"
"78557544","Pinecone ERROR Vector dimension 768 does not match the dimension of the index 384","2024-05-30 23:07:30","","0","106","<python><langchain><huggingface><pinecone><mistral-7b>","<p>I'm building a chatbot RAG using HuggingFace, Mistral, LangChain and Pinecone.</p>
<p>I have a Python Script to watch changes in my MongoDB collection and send the data to Pinecone as a vector.</p>
<pre><code>import os
from pymongo import MongoClient
from pinecone import Pinecone, ServerlessSpec
from pymongo.errors import OperationFailure
from sentence_transformers import SentenceTransformer, util
from certifi import where  # Import certifi library

# mongodb stuff
client = MongoClient(
    &quot;my-mongodb-uri&quot;,
    tls=True,  # Enable TLS encryption
    tlsAllowInvalidCertificates=False,  # Don't allow invalid certificates
    tlsCAFile=where()  # Use certifi library for CA bundle
)
db = client['test']
collection = db['reflections']

# Pinecone initialization
pc = Pinecone(api_key='my-pinecone-key')
index = pc.Index(&quot;langchain-demo&quot;)

# transformer stuff
model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
test_vector = model.encode(&quot;This is a test string&quot;)
print(&quot;Dimension of test vector:&quot;, test_vector.shape)

# Watch for changes
try:
  cursor = collection.watch()
  for change in cursor:
    print(&quot;Change detected:&quot;, change)
    if change['operationType'] == 'insert':
      document = change['fullDocument']
      vector = model.encode(document['content']).tolist()
      print(&quot;Extracted Vector:&quot;, vector)

      # Extract document ID from ObjectId
      document_id = str(document['_id'])
      user_id = str(document['user'])
      created_at = document['createdAt']

      # Wrap upsert call with empty vector check
      if vector:  # Check if vector is not empty
        metadata = {'user_id': user_id, 'created_at': str(created_at)}
        index.upsert([(document_id, vector, metadata)])

    elif change['operationType'] == 'update':
      document_id = str(change['documentKey']['_id'])
      updated_fields = change['updateDescription']['updatedFields']
      if 'content' in updated_fields:
            vector = model.encode(updated_fields['content']).tolist()
            index.upsert([(document_id, vector, metadata)])

    elif change['operationType'] == 'delete':
      document_id = str(change['documentKey']['_id'])
      index.delete(ids=[document_id])

except OperationFailure as e:
  print(&quot;Error watching collection:&quot;, e)
except Exception as e:
  print(&quot;An error occurred:&quot;, e)
</code></pre>
<p>And this is working fine, here a entry from my Pinecone Index:</p>
<pre class=""lang-none prettyprint-override""><code>2

SCORE
0.0179

ID
665882f313b3c25b81fe46f7

VALUES
0.0780179054, 0.0113870166, -0.118430212, 0.0226640049, 0.00780098559, -0.0503292121, -0.0238182824, 0.0469664969, -0.0270168502, 0.0369489267, 0.0325907134, -0.0476229116, -0.0786545724, 0.024753511, 0.027189346, -0.0125029637, 0.00541313691, -0.0757853836, 0.0212387145, -0.0110286493, 0.0117090391, -0.0193786472, 0.0253583211, 0.033223819, -0.047959242, -0.0769486949, -0.0650763139, 0.0521994196, -0.0230765603, -0.0348826386, 0.101876184, 0.0702511, -0.00723150745, -0.000215011096, 0.0366995223, -0.0220702309, 0.0689907297, -0.00627420563, 0.00119058753, -0.0264558308, 0.0431874134, -0.010541168, -0.0586702935, -0.0786846578, 0.0561939925, -0.0782630071, -0.0292404443, 0.0599261038, 0.106215701, 0.00162592565, 0.00396974338, -0.084600091, -0.108621657, 0.0980778411, 0.0299655739, 0.128577992, -0.0200762413, -0.0653520674, 0.0102492804, 0.0322388411, 0.0313118957, 0.0121116294, -0.0409101136, 0.0448457114, -0.0308046602, -0.0109536722, -0.0264064465, -0.0215834882, 0.0305376686, -0.0735122, 0.0205106437, 0.0107715921, -0.0321323, -0.0110485516, 0.0149518345, 0.154741883, -0.0143986866, -0.0952939615, 0.0817126408, 0.0272761323, -0.118013233, -0.0656163543, 0.0275144801, -0.0715595558, 0.00360014034, -0.0505540147, 0.0993294567, -0.0181424897, -0.0340541303, -0.0191219747, 0.0576984696, -0.016041344, -0.0117336037, 0.0854231268, 0.000578647654, 0.054585509, 0.0607333779, 0.0251228772, 0.0586750954, 0.0542409495, -0.0147840325, 0.0171087608, 0.0243709181, 0.0218990557, -0.0393281505, 0.0587186627, 0.0294246785, -0.0300889853, -0.104504265, 0.01126011, 0.0200448446, 0.0346874595, -0.0424731597, -0.0501565561, -0.0336206853, 0.0125492243, -0.0602271, -0.0255396832, -0.0807785168, 0.0296369921, 0.0213591736, 0.0517560728, -0.0236208886, -0.00668002106, 0.0482311361, 0.00484784134, -0.00671314076, 2.48858068e-33, 0.13539581, -0.0779394507, 0.0531481355, -0.0325026, 0.0539878421, 0.0422413386, 0.00721841305, 0.0636715069, -0.0271209199, 0.0237933304, -0.012324837, 0.0318006687, -0.00415049167, -0.0791502222, 0.00476903608, 0.0398657173, -0.0222511888, 0.00637130346, -0.026622, 0.0662136674, 0.0718791559, -0.0128180161, -0.00474599516, -0.0390690714, 0.0125142019, 0.050622154, -0.0718687624, -0.014537137, -0.0000649832582, 0.0659983233, -0.0874705464, -0.0598177947, -0.0425723866, 0.116614014, 0.00950913411, 0.00521008205, 0.00299354899, -0.0490386747, -0.101944029, -0.00193977414, 0.0737567469, 0.0489828475, 0.0479842573, -0.0121582411, -0.0683459863, 0.0465017371, 0.0244375356, -0.0415155031, 0.0733279511, 0.0411045067, -0.0990792513, -0.0168382581, -0.128719345, 0.0162787139, 0.00180200755, 0.0361118391, 0.00714902952, 0.0225948617, 0.0598230809, -0.027883, -0.0634493604, 0.01690959, -0.00616647629, 0.102524318, -0.00962299388, -0.00800243765, -0.0349691175, -0.067561388, 0.00328797475, 0.038413655, -0.0277767256, 0.0493801422, -0.0626680329, 0.0100066448, 0.0245917551, -0.0696575642, 0.0435989201, -0.0336911827, -0.11178454, 0.0231342558, 0.0650513396, -0.0489158109, -0.118925206, 0.0456594117, -0.000165144767, 0.0263350084, -0.0126295025, -0.00819321815, 0.02433425, -0.130400553, 0.0154193891, 0.104591407, 0.0529140271, -0.0677612349, 0.006422868, -3.15768235e-33, 0.0266072527, -0.0650565699, 0.0751486719, 0.0352391638, 0.0913304538, -0.0166126471, 0.0618819185, 0.0104150325, -0.0678095892, -0.0207110438, 0.0114265652, -0.0825057179, -0.0197282247, -0.0340087563, -0.0661914572, -0.0287549552, -0.00654957304, -0.114685677, -0.00108200416, 0.032330893, -0.0842154548, 0.0676763728, 0.0241372753, 0.0217358116, 0.00449138787, -0.110978812, -0.00896465, -0.113323905, -0.0244096816, 0.0437684767, 0.0635139272, 0.0932535827, -0.0722304285, 0.0176913701, -0.0259954911, 0.0359196104, 0.0873655, -0.0387332886, 0.0215564184, 0.087338239, 0.0205500964, 0.0888643935, -0.0694097877, -0.0502204783, 0.0232924949, 0.0244588424, -0.0220373627, 0.0513052084, 0.0780876428, -0.0470482521, -0.0335381292, -0.012535166, 0.0251501985, -0.0287306085, 0.0485889763, -0.0687309802, 0.0517236702, -0.0322456844, -0.00322602759, 0.0309814233, 0.0314734243, -0.00763608702, -0.0108202593, -0.0168912616, 0.0146153346, 0.0058498932, -0.0463481061, -0.0149578694, 0.00604433473, 0.0156758986, -0.00293513224, -0.0301138815, -0.0343872979, 0.0298203304, 0.0256143659, -0.0255653039, -0.0625488907, -0.0271613039, 0.0419909954, 0.0695206523, 0.062871322, -0.034802638, 0.0640893504, 0.0120009324, 0.0246663559, -0.0750950128, 0.00124487793, 0.0673942715, -0.133396164, 0.0429567732, -0.00271692313, -0.00318611646, -0.0404472388, 0.0490305126, 0.0105391946, -1.60218168e-8, -0.0698085204, 0.000768667669, -0.00646897545, 0.0643798038, -0.0233389419, 0.00319286506, 0.0344442315, 0.00670952629, 0.0701468587, 0.0244643614, 0.0203008968, 0.0470311418, -0.0328549594, 0.0701879337, -0.03013069, 0.0344369523, 0.0444342941, 0.0185571574, 0.0324901864, 0.028787395, -0.00257497164, 0.016713731, 0.00826486666, -0.108173572, 0.0234305598, 0.0513667837, 0.0751283243, 0.0887183845, 0.00413105683, -0.00771792512, -0.00882443134, 0.010960429, 0.0386422388, -0.0143630169, 0.0658110827, 0.0273497198, 0.0315157361, 0.0515595078, -0.0557875521, 0.0250054803, -0.00490543433, 0.0140227228, 0.0185526628, -0.063243337, -0.0253285952, -0.0361511707, 0.0092082452, 0.109378465, -0.0141137447, -0.0900633484, 0.0360222086, -0.0720594451, 0.0374338441, -0.0112864282, 0.0303963758, 0.0577182584, -0.0215295721, -0.0536189973, -0.0685039461, -0.0390378311, 0.0779438391, -0.070971, -0.015317292, -0.0375189707

METADATA
created_at: &quot;2024-05-30 13:45:23.389000&quot;
user_id: &quot;65d8937f6408bf2c0ca8d264&quot;
</code></pre>
<p>The problem is when I try to ask something to my chatbog. Here is my code:</p>
<pre><code>from transformers import AutoTokenizer
from pinecone import Pinecone, ServerlessSpec
from dotenv import load_dotenv
import os
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Pinecone as PineconeStore
from langchain_huggingface import HuggingFaceEndpoint
from langchain.prompts import PromptTemplate
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser

class ChatBot():
    def __init__(self):
        load_dotenv()

        self.pc = Pinecone(api_key=os.getenv(&quot;PINECONE_API_KEY&quot;))
        self.index_name = &quot;langchain-demo&quot;
        self.index = self.pc.Index(self.index_name)

        # Initialize tokenizer for the language model
        self.tokenizer = AutoTokenizer.from_pretrained('mistralai/Mixtral-8x7B-Instruct-v0.1')

        # Initialize embeddings and Pinecone store for document retrieval
        self.embeddings = HuggingFaceEmbeddings()
        self.docsearch = PineconeStore.from_existing_index(self.index_name, self.embeddings)

        # Setup LLM with tokenizer
        self.llm = HuggingFaceEndpoint(
            repo_id='mistralai/Mixtral-8x7B-Instruct-v0.1',
            temperature=0.8, top_p=0.8, top_k=50,
            huggingfacehub_api_token=os.getenv('HUGGINGFACE_API_KEY')
        )

        # Define the prompt template
        self.template = PromptTemplate(template=&quot;&quot;&quot;
            You are a seer. These Humans will ask you questions about their life. 
            Use the following piece of context to answer the question. 
            If you don't know the answer, just say you don't know.
            You answer with short and concise answers, no longer than 2 sentences.

            Context: {context}
            Question: {question}
            Answer: 
        &quot;&quot;&quot;, input_variables=[&quot;context&quot;, &quot;question&quot;])

        self.rag_chain = (
            {&quot;context&quot;: self.docsearch.as_retriever(), &quot;question&quot;: RunnablePassthrough()}
            | self.template
            | self.llm
            | StrOutputParser()
        )

    def get_user_data_from_pinecone(self, user_id):
        results = self.index.query(
            vector=[0]*384,
            filter={&quot;user_id&quot;: user_id},
            top_k=10,
            include_metadata=True,
            include_values=True
        )
        return results
    

# Usage example outside the class
bot = ChatBot()
user_id = input(&quot;Enter your user ID: &quot;)
user_data = bot.get_user_data_from_pinecone(user_id)

question = input(&quot;Ask me anything: &quot;)
# Tokenize the question
inputs = bot.tokenizer(question, return_tensors=&quot;pt&quot;)
result = bot.rag_chain.invoke(question)
print(&quot;AI's response:&quot;, result)
</code></pre>
<p>Here is the error I received:</p>
<pre class=""lang-none prettyprint-override""><code>HTTP response body: {&quot;code&quot;:3,&quot;message&quot;:&quot;Vector dimension 768 does not match the dimension of the index 384&quot;,&quot;details&quot;:[]}
</code></pre>
<p>My Pinecone Configuration:</p>
<pre class=""lang-none prettyprint-override""><code>METRIC
cosine
DIMENSIONS
384
CLOUD
aws
AWS
REGION
us-east-1
TYPE
Serverless
</code></pre>
","huggingface"
"78555068","Embedding gradio in mkdocs throws 404 error","2024-05-30 12:47:33","","0","52","<huggingface><mkdocs><gradio><huggingface-hub><mkdocs-material>","<p>I am having issues in embedding gradio app deployed on HuggingFace spaces inside Mkdocs.</p>
<p>To give you some background - I have a functioning gradio app that is deployed successfully on hubspace. See below an image of it.
<a href=""https://i.sstatic.net/H3vRtpvO.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/H3vRtpvO.png"" alt=""enter image description here"" /></a>
I also have a functioning Mkdocs Material website on which I want to integrate this app.</p>
<p>I referred the &quot;Sharing Your App”  documentation from gradio <a href=""https://www.gradio.app/guides/sharing-your-app#embedding-hosted-spaces"" rel=""nofollow noreferrer"">here</a>.<br />
Based on it, I was able to successfully embed my app using iFrame as shown below.
<a href=""https://i.sstatic.net/2662BGGM.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/2662BGGM.png"" alt=""iframe-works"" /></a>
But when I try to do the same using Web Components, it throws an error in the console and renders a blank page as shown below
<a href=""https://i.sstatic.net/vLfrFWo7.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/vLfrFWo7.png"" alt=""enter image description here"" /></a>
The error in the console is -
<code>[Error] Failed to load resource: the server responded with a status of 404 (Not Found) (index-BeRL9up6.js, line 0)</code></p>
<p>The Mkdocs logs also show an error as below -
<code>WARNING -  [14:35:24] &quot;GET /assets/external/gradio.s3-us-west-2.amazonaws.com/4.32.0/assets/index-BeRL9up6.js HTTP/1.1&quot; code 404</code></p>
<p>Note that disabling the <a href=""https://squidfunk.github.io/mkdocs-material/setup/ensuring-data-privacy/#built-in-privacy-plugin"" rel=""nofollow noreferrer"">Mkdocs Material privacy plugin</a> would load the gradio hugging face spaces embedding as shown below, but still throws errors that I don’t fully comprehend.
<a href=""https://i.sstatic.net/CbEtHIYr.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/CbEtHIYr.png"" alt=""enter image description here"" /></a></p>
<p>Can some kind soul please help me troubleshoot this?<br />
Many Thanks in advance!</p>
","huggingface"
"78552825","Huggingface models - getting prediction from model stored locally","2024-05-30 04:11:18","","0","30","<huggingface-transformers><huggingface>","<p>I'm able to run the following code successfully from Huggingface:</p>
<pre><code>from transformers import pipeline, TFAutoModel
classifier = pipeline(task=&quot;text-classification&quot;, model=&quot;SamLowe/roberta-base-go_emotions&quot;, top_k=None)
sentences = [&quot;This has been a good day&quot;]
classifier(sentences) 
</code></pre>
<p>However, when trying to get the prediction from locally saved model using <code>save_pretrained</code> I'm getting unexpected errors:</p>
<pre><code>classifier.save_pretrained(&quot;SamLowe/roberta-base-go_emotions&quot;)
model = TFAutoModel.from_pretrained(&quot;SamLowe/roberta-base-go_emotions&quot;)
model(sentences)
</code></pre>
<p>Error:</p>
<pre><code>Data of type &lt;class 'str'&gt; is not allowed only (&lt;class 'tensorflow.python.framework.tensor.Tensor'&gt;, &lt;class 'bool'&gt;, &lt;class 'int'&gt;, &lt;class 'transformers.utils.generic.ModelOutput'&gt;, &lt;class 'tuple'&gt;, &lt;class 'list'&gt;, &lt;class 'dict'&gt;, &lt;class 'numpy.ndarray'&gt;) is accepted for input_ids.
</code></pre>
<p>Any idea what I might be doing wrong here ?</p>
","huggingface"
"78548960","meta-llama/Llama-2-13b-hf torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU","2024-05-29 10:41:13","","0","58","<python><pytorch><large-language-model><huggingface><llama>","<p>I am trying load Llama-2-13b on multiple GPU's but isn't loading, i have 3 GPU's 24.169 GB each , but unable to load, i have tried using cuda or device_map ='auto'
This is my current code. When I try nvidia-smi in terminal, the GPU is always at 0%.When i remove split options then it works, but then it runs on CPU.
here's below my try:</p>
<pre><code>import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM,pipeline

device_map = 'auto'

# Load the tokenizer and model from Hugging Face hub
access_token = token
model = &quot;meta-llama/Llama-2-13b-hf&quot;
tokenizer = AutoTokenizer.from_pretrained(model,token=access_token)
documents = &quot;&quot;&quot;
{
    &quot;abovegradefinishedarea&quot;: 1215.0,
    &quot;bathroomsfull&quot;: 2,
    &quot;bathroomstotalinteger&quot;: 2,
    &quot;bedroomstotal&quot;: 3,
    &quot;yearbuilt&quot;: 1909,
    &quot;city&quot;: &quot;Minneapolis&quot;,
    &quot;closedate&quot;: &quot;2022-09-23&quot;,
}
&quot;&quot;&quot;

question = 'what is the name of the city?\n'
input = f&quot;&quot;&quot;
        &lt;&lt;SYS&gt;&gt;
        Only respond with &quot;Not in the text.&quot; if the information needed to answer the question is not contained in the document. \n
        Answer the question using only the information from the provided information below. \n
        Ensure that the questions are answered fully and effectively. \n
        Respond in short and concise yet fully formulated sentences, being precise and accurate
        &lt;&lt;/SYS&gt;&gt;
        [INST]
        User:{question}
        [/INST]\
        [INST]
        User:{documents}
        [/INST]\n

        Assistant:
    &quot;&quot;&quot;
llama_pipeline = pipeline(
    &quot;text-generation&quot;,
    model=model,
    torch_dtype=torch.float16,
    device_map = 'auto',
    temperature=0.1,
)


sequences = llama_pipeline(
    input,
    do_sample=True,
    top_k=50,
    num_return_sequences=2,
    max_new_tokens=2048,
    return_full_text=False,
    temperature=0.1,
)
print(&quot;Chatbot:&quot;, sequences[0]['generated_text'])
</code></pre>
","huggingface"
"78544326","Safety filter SD models","2024-05-28 13:09:06","","0","19","<huggingface><stable-diffusion>","<p>Whenever I access Hugginface's Stable Diffusion models via ComfiUI, the models are lacking a safety filter; meaning they will generate harmful content, which does not happen if I use the code snipped to access Stable Diffusion models directly from Huggingface. In this case, harmful images will just be blacked out. Does anyone have an idea why this is the case? Why is the same model having a safety filter in one case but not the other?</p>
<p>Thanks for your help!</p>
","huggingface"
"78541066","Langchain HuggingFace embeddings no longer loading for Llama-Index?","2024-05-27 20:58:32","","1","257","<python><langchain><embedding><huggingface><llama-index>","<p>The following code worked about a week ago:</p>
<pre><code>from langchain_community.embeddings import HuggingFaceEmbeddings
from llama_index.embeddings.langchain import LangchainEmbedding
embed_model = LangchainEmbedding(HuggingFaceEmbeddings(model_name=&quot;WhereIsAI/UAE-Large-V1&quot;))
</code></pre>
<p>Previously, the <code>embed_model = ...</code> line takes only a few seconds to execute. However, now when I try to run it, it seems to be running indefinitely without an error/warning message, and it does not execute properly.</p>
<p>Has anyone else come across this issue?</p>
","huggingface"
"78538617","Error fetching blob in huggingface endpoints API","2024-05-27 10:47:33","","0","97","<javascript><blob><huggingface>","<p>I'm new to using the huggingface.js api, my project was previously built with the openAI GPT API so I assume there are a lot of changes that I need to make in order to work with huggingface.js.</p>
<p>This is my current function that has replaced <code>postChatGPTMessage</code>:</p>
<pre class=""lang-js prettyprint-override""><code>const postHFMessage = async (contextMessage, conversation) =&gt; {
    // build the conversation for context
    const { textGeneration } = await import('@huggingface/inference');
    
    // Prepare the payload for the text generation request
    const payload = {
        inputs: contextMessage,
        parameters: {
            max_length: 100,
            temperature: 0.1,
            // Add any other parameters as needed
            },
        };

    // Call the textGeneration function with your payload
    try {
        const response = await textGeneration({
            accessToken: process.env.HUGGINGFACE_API_KEY,
            model: USED_MODEL,
            inputs: payload.inputs,
            parameters: payload.parameters,
        });
        if (response) {
            const responseData = await response.json();
            console.log('Received response:', responseData);
            return response;
        }
        else {
            console.log(&quot;no response from textGeneration&quot;);
        }
    }
    catch(error) {
        console.log(&quot;Could not get response from textGeneration: &quot;, error);
    }
};
</code></pre>
<p><code>HUGGINGFACE_API_KEY</code> is a user access token (free one for reading) and the <code>USED_MODEL</code> is a model name that I got from the huggingface website, and I get this error when I try to use it:</p>
<pre><code>Could not get response from textGeneration:  Error: An error occurred while fetching the blob
    at request (file:///Users/o*******/node_modules/@huggingface/inference/dist/index.js:207:11)
</code></pre>
<p>Do I need to transform the entire json parameter that I'm sending to the function to a blob before sending it? Or is the error occurring because I'm not handling the response as needed?</p>
<p>Looked for solutions in the huggingface.js documentation but could not conclude what was causing the problem.</p>
","huggingface"
"78527292","deepspeed goes wrong after upgrading the transformers version","2024-05-24 08:16:47","","0","27","<python><torch><huggingface><deepspeed>","<p>I am training a VLM model. The codes went well when the transformers version is 4.31. However, when I want to use a different base, which is supported by <code>transformers==4.35</code>. After such upgrading, the codes went wrong by:
<a href=""https://i.sstatic.net/ig9gU0j8.png"" rel=""nofollow noreferrer"">enter image description here</a>
Even I change back to original base model under <code>transformers==4.35</code>, the error still exists. How should I approach such problem. I read some pages saying this is because of shortage of memory. But the original setting fails either, upgrading the transformers will increase memory consumption even with the same model?</p>
<p>I tried <code>device_map=&quot;auto&quot;</code> in the from_pretrained function. It does not work. Can this be solved without degrading the transformers' version?</p>
","huggingface"
"78526424","Hugging face; Problem with custom retriever in transformers library","2024-05-24 04:03:12","","0","61","<nlp><chatbot><huggingface-transformers><huggingface><retrieval-augmented-generation>","<p>I am trying to build a RAG-based Chatbot with Chain of Thought for WordPress Site. I am not very experienced with Hugging face. I am using API to retrieve the data for this.</p>
<p>Problem description:
I have built a custom retriever for the rag model. I know that I can use RagRetriever, I tried and I am having severe problems with version control as it requires me to import datasets.</p>
<p>After running the program, the Error says that, my 'CustomRetriever' object is not callable. Is there a way around this?</p>
<p>I'm using,
Python v3.10.12
Transformers v4.42.0 dev0</p>
<p>My Custom Retriever Class</p>
<pre><code># Custom Retriever Class
class CustomRetriever:
    def __init__(self, index):
        self.index = index

    def retrieve(self, query_embedding, n_docs=5):
        distances, indices = self.index.search(query_embedding, n_docs)
        return indices

# Initialize RAG components
tokenizer = RagTokenizer.from_pretrained(&quot;facebook/rag-sequence-nq&quot;)
rag_model = RagSequenceForGeneration.from_pretrained(&quot;facebook/rag-sequence-nq&quot;)

# Initialize Custom Retriever
custom_retriever = CustomRetriever(index=index)

# Set the retriever for RAG model
rag_model.set_retriever(custom_retriever)
</code></pre>
<p>My implementation(passing input ids and attention mask)</p>
<pre><code># Maintain a conversation history
conversation_history = []

def process_query_with_chain_of_thought(user_query, previous_context=&quot;&quot;):
    # Tokenize the input query
    inputs = tokenizer(user_query, return_tensors=&quot;pt&quot;)

    # Generate embeddings for the query
    query_embedding = model.encode([user_query]).reshape(1, -1)

    # Retrieve relevant document indices
    doc_indices = custom_retriever.retrieve(query_embedding, n_docs=5)

    # Debugging: Print retrieved document indices
    # print(f&quot;Retrieved document indices: {doc_indices}&quot;)

    # Fetch actual documents
    retrieved_docs = []
    for i in doc_indices[0]:
        if 0 &lt;= i &lt; len(posts):
            retrieved_docs.append(posts[i]['content']['rendered'])
        else:
            print(f&quot;Index {i} is out of range.&quot;)

    # Print the retrieved documents in a human-readable format
    # print(&quot;Retrieved documents:&quot;)
    full_content = &quot;&quot;
    for doc in retrieved_docs:
        # Clean HTML and convert to plain text
        cleaned_text = clean_html(doc)
        # print(cleaned_text)  # Print the extracted text
        # print(&quot;=&quot;*50)   # Separate each document with a line of '='
        full_content += cleaned_text + &quot;\n\n&quot;

    # Combine user query and retrieved documents to create context
    context = user_query + &quot;\n\n&quot; + full_content

    if context:
        # Tokenize the retrieved documents
        context_inputs = tokenizer(full_content, return_tensors=&quot;pt&quot;, padding=True, truncation=True, max_length=512)

        # Ensure that context_input_ids are passed correctly
        response_ids = rag_model.generate(
            input_ids=inputs['input_ids'],
            context_input_ids=context_inputs['input_ids'],
            context_attention_mask=context_inputs['attention_mask']
        )

        response_text = tokenizer.decode(response_ids[0], skip_special_tokens=True)

        # Append the initial response to the conversation history
        conversation_history.append(f&quot;User: {user_query}&quot;)
        conversation_history.append(f&quot;Bot: {response_text}&quot;)

        # Create a new context by combining the conversation history
        new_context = &quot;\n&quot;.join(conversation_history)

        # # Debug: print the final response
        # print(&quot;Final response:&quot;)
        # print(new_context)

        return response_text, new_context
    else:
        print(&quot;No documents retrieved&quot;)
        return &quot;Sorry, I couldn't find any relevant information.&quot;, previous_context
</code></pre>
<p>Queries:</p>
<pre><code># Example multi-turn conversation
user_queries = [
    &quot;Are there any news on fiction stories?&quot;,
    &quot;Which one is the most populer&quot;,
    &quot;What are the reviews.&quot;
]


context = &quot;&quot;
for query in user_queries:
    response, context = process_query_with_chain_of_thought(query, previous_context=context)
    print(f&quot;Processed response: {response}\n&quot;)
</code></pre>
<p>Error message:</p>
<pre><code>TypeError                                 Traceback (most recent call last)

&lt;ipython-input-22-c7d6867da07f&gt; in &lt;cell line: 2&gt;()
      1 context = &quot;&quot;
      2 for query in user_queries:
----&gt; 3     response, context = process_query_with_chain_of_thought(query, previous_context=context)
      4     print(f&quot;Processed response: {response}\n&quot;)

8 frames

&lt;ipython-input-20-3284408ddf21&gt; in process_query_with_chain_of_thought(user_query, previous_context)
     41 
     42         # Ensure that context_input_ids are passed correctly
---&gt; 43         response_ids = rag_model.generate(
     44             input_ids=inputs['input_ids'],
     45             context_input_ids=context_inputs['input_ids'],

/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py in decorate_context(*args, **kwargs)
    113     def decorate_context(*args, **kwargs):
    114         with ctx_factory():
--&gt; 115             return func(*args, **kwargs)
    116 
    117     return decorate_context

/usr/local/lib/python3.10/dist-packages/transformers/models/rag/modeling_rag.py in generate(self, input_ids, attention_mask, context_input_ids, context_attention_mask, doc_scores, do_deduplication, num_return_sequences, num_beams, n_docs, **model_kwargs)
   1019             if input_ids is not None:
   1020                 new_input_ids = input_ids[index : index + 1].repeat(num_candidates, 1)
-&gt; 1021                 outputs = self(new_input_ids, labels=output_sequences, exclude_bos_score=True)
   1022             else:  # input_ids is None, need context_input_ids/mask and doc_scores
   1023                 assert context_attention_mask is not None, (

/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py in _wrapped_call_impl(self, *args, **kwargs)
   1530             return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1531         else:
-&gt; 1532             return self._call_impl(*args, **kwargs)
   1533 
   1534     def _call_impl(self, *args, **kwargs):

/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py in _call_impl(self, *args, **kwargs)
   1539                 or _global_backward_pre_hooks or _global_backward_hooks
   1540                 or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1541             return forward_call(*args, **kwargs)
   1542 
   1543         try:

/usr/local/lib/python3.10/dist-packages/transformers/models/rag/modeling_rag.py in forward(self, input_ids, attention_mask, encoder_outputs, decoder_input_ids, decoder_attention_mask, past_key_values, context_input_ids, context_attention_mask, doc_scores, use_cache, output_attentions, output_hidden_states, output_retrieved, exclude_bos_score, reduce_loss, labels, n_docs, **kwargs)
    843             use_cache = False
    844 
--&gt; 845         outputs = self.rag(
    846             input_ids=input_ids,
    847             attention_mask=attention_mask,

/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py in _wrapped_call_impl(self, *args, **kwargs)
   1530             return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1531         else:
-&gt; 1532             return self._call_impl(*args, **kwargs)
   1533 
   1534     def _call_impl(self, *args, **kwargs):

/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py in _call_impl(self, *args, **kwargs)
   1539                 or _global_backward_pre_hooks or _global_backward_hooks
   1540                 or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1541             return forward_call(*args, **kwargs)
   1542 
   1543         try:

/usr/local/lib/python3.10/dist-packages/transformers/models/rag/modeling_rag.py in forward(self, input_ids, attention_mask, encoder_outputs, decoder_input_ids, decoder_attention_mask, past_key_values, doc_scores, context_input_ids, context_attention_mask, use_cache, output_attentions, output_hidden_states, output_retrieved, n_docs)
    591                 question_encoder_last_hidden_state = question_enc_outputs[0]  # hidden states of question encoder
    592 
--&gt; 593                 retriever_outputs = self.retriever(
    594                     input_ids,
    595                     question_encoder_last_hidden_state.cpu().detach().to(torch.float32).numpy(),

TypeError: 'CustomRetriever' object is not callable
</code></pre>
<p>If you want any more details, please do let me know. Thanks.</p>
","huggingface"
"78520266","Loading huggingface dataset from in-memory text","2024-05-22 22:36:15","","0","68","<python><dataset><huggingface><huggingface-datasets>","<p>I have in-memory text, json format, and I am trying to load dataset (HuggingFace) directly from text in-memory.</p>
<p>If I will save it into file - I can load the dataset using huggingface load_dataset:</p>
<pre><code>from datasets import load_dataset
dataset = load_dataset('json', data_files='my_file.json')
</code></pre>
<p>See also: <a href=""https://huggingface.co/docs/datasets/v1.11.0/loading_datasets.html#from-local-files"" rel=""nofollow noreferrer"">https://huggingface.co/docs/datasets/v1.11.0/loading_datasets.html#from-local-files</a></p>
<p>Can I load the dataset directly from the in-memory text without saving it into file?</p>
","huggingface"
"78518971","Can I dynamically add or remove LoRA weights in the transformer library like diffusers","2024-05-22 16:45:03","78520745","0","340","<python><huggingface-transformers><huggingface><peft>","<p>I see that in the diffuser library, there is this feature to dynamically add and remove LoRA weights based on this article <a href=""https://github.com/huggingface/blog/blob/main/lora-adapters-dynamic-loading.md"" rel=""nofollow noreferrer"">https://github.com/huggingface/blog/blob/main/lora-adapters-dynamic-loading.md</a> · GitHub and using the load_lora_weights and fuse_lora_weights. I want to know if I can do something similar with LoRA for transformers too?</p>
","huggingface"
"78517989","LLAMA Vector Indexing with Hugging face: The `model_name` argument must be provided","2024-05-22 13:47:29","","1","537","<python><huggingface><llama-index>","<p>It looks like llama is defaulting to OpenAI.</p>
<p>I am using this tutorial: <a href=""https://www.youtube.com/watch?v=i8n2Se8PAXg&amp;list=PLBSCvBlTOLa-vUt7mCECzaJjJEjos6K-l&amp;index=7"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=i8n2Se8PAXg&amp;list=PLBSCvBlTOLa-vUt7mCECzaJjJEjos6K-l&amp;index=7</a>
And saw this interesting article: <a href=""https://stackoverflow.com/questions/76771761/why-does-llama-index-still-require-an-openai-key-when-using-hugging-face-local-e"">Why does llama-index still require an OpenAI key when using Hugging Face local embedding model?</a></p>
<p>Still I am not able to solve my problem.</p>
<p>To me, my model name is already defined in &quot;llm&quot;. I think I need to use the service context, but without success. It still asks to provide the model name.</p>
<pre class=""lang-py prettyprint-override""><code>import os
from getpass import getpass
from huggingface_hub import login
from llama_index.core import SimpleDirectoryReader
from llama_index.core import VectorStoreIndex
from llama_index.llms.huggingface import HuggingFaceInferenceAPI

reader = SimpleDirectoryReader(input_files= [&quot;acquisition2.csv&quot;])
documents = reader.load_data()

HF_TOKEN = os.environ[&quot;HF_TOKEN&quot;] = &quot;XXXXXXX&quot;
login(token=HF_TOKEN)

# create llm model

llm = HuggingFaceInferenceAPI(model_name=&quot;mistralai/Mixtral-8x7B-Instruct-v0.1&quot;, token=HF_TOKEN)

index = VectorStoreIndex.from_documents(documents,embed_model=&quot;local&quot;)
</code></pre>
<p><code>ValueError: The </code>model_name<code> argument must be provided.</code></p>
<p>Adding the <code>model_name</code> as a parameter is still throwing an error.</p>
","huggingface"
"78516137","Cannot load Transformers model in Docker which runs a FastAPI app","2024-05-22 08:17:27","","0","43","<python><docker><fastapi><huggingface-transformers><huggingface>","<p>I have a simple FastAPI app which downloads a SentenceTransformers model like</p>
<pre class=""lang-py prettyprint-override""><code>from sentence_transformers import SentenceTransformers
from fastapi import FastAPI, Request

model = SentenceTransformer(&quot;intfloat/multilingual-e5-large-instruct&quot;)
app = FastAPI()

#some endpoint code below
</code></pre>
<p>and a Dockerfile</p>
<pre><code>FROM python:3.11.5-slim-bullseye
WORKDIR /opt/python

COPY README.md ./
COPY poetry.lock ./
COPY pyproject.toml ./

RUN 
    pip install \
        poetry==1.5.1 \
        keyring==23.13.1 \
        keyrings.google-artifactregistry-auth==1.1.1 &amp;&amp; \
    poetry config virtualenvs.create false &amp;&amp; \
    poetry install --no-root --only main --no-interaction --no-ansi

RUN poetry install --only-root --no-interaction --no-ansi

ENTRYPOINT [&quot;poetry&quot;, &quot;run&quot;, &quot;python&quot;, &quot;run_app.py&quot;]

EXPOSE 3001
</code></pre>
<p>the issue is that when the app is being run, the download of the model fails.
I get either a &quot;Cannot find huggingface.co&quot; or if I specify a cache dir i.e adding</p>
<pre><code>#Dockerfile
RUN mkdir /cache_model
RUN chmod 640 /cache_model

#run_app.py
model = SentenceTransformer(&quot;intfloat/multilingual-e5-large-instruct&quot;, cache_folder=&quot;/cache_model)
</code></pre>
<p>then I get a</p>
<pre><code>No such file or directory: '../../blobs/f92397ec9462da6e5e34fa22a43cf234093481e8' -&gt; '/root/cache_model/models--intfloat--multilingual-e5-large-instruct/snapshots/baa7be480a7de1539afce709c8f13f833a510e0a/config.json'
</code></pre>
<p>What DOES work is to download the model in the Dockerfile</p>
<pre><code>RUN poetry run python -c 'from sentence_transformers import SentenceTransformer; SentenceTransformer(&quot;...&quot;)'
</code></pre>
<p>then the <code>run_app</code> works fine.
I want to avoid doing that since the model is more than 2GB, thus getting a very big image.</p>
<p>Note, that the SentenceTransformers library is build op top of Transformers, and it is the Transformers library which throws the error.</p>
","huggingface"
"78516102","Single LoRA gives different outputs with and without merge_unload for Sequence Classification task. Which one is the correct method?","2024-05-22 08:07:29","","0","68","<pytorch><nlp><huggingface-transformers><huggingface>","<p>I keep everything same for Fine tuned LoRA for Sequence classification task. When I load it like:</p>
<pre><code>def load_model(lora_path, device, num_labels, merge_unload = False):
    
    model = Phi3ForSequenceClassification.from_pretrained(&quot;microsoft/Phi-3-mini-4k-instruct&quot;, trust_remote_code=True, 
                                                          device_map = device, 
                                                          torch_dtype=torch.bfloat16,
                                                          attn_implementation = &quot;flash_attention_2&quot;, 
                                                          num_labels = num_labels)
    if model.config.pad_token_id is None:
        model.config.pad_token_id = model.config.eos_token_id
    
    peft_model = PeftModel.from_pretrained(model,
                                           lora_path, 
                                           device_map = device)
    
    peft_model = peft_model.eval().to(device)
    if merge_unload: peft_model = peft_model.merge_and_unload()
    
    return peft_model

</code></pre>
<p>With and without <code>.merge_and_unload()</code>, it gives different outputs for the same text. How can this be happening? What is the right way to do this?</p>
<p><strong>LoRA Model</strong>:</p>
<pre><code>PeftModelForSequenceClassification(
  (base_model): LoraModel(
    (model): Phi3ForSequenceClassification(
      (model): Phi3Model(
        (embed_tokens): Embedding(32064, 3072, padding_idx=32000)
        (embed_dropout): Dropout(p=0.0, inplace=False)
        (layers): ModuleList(
          (0-31): 32 x Phi3DecoderLayer(
            (self_attn): Phi3FlashAttention2(
              (o_proj): lora.Linear(
                (base_layer): Linear(in_features=3072, out_features=3072, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=3072, out_features=128, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=128, out_features=3072, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (qkv_proj): lora.Linear(
                (base_layer): Linear(in_features=3072, out_features=9216, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=3072, out_features=128, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=128, out_features=9216, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (rotary_emb): Phi3RotaryEmbedding()
            )
            (mlp): Phi3MLP(
              (gate_up_proj): lora.Linear(
                (base_layer): Linear(in_features=3072, out_features=16384, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=3072, out_features=128, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=128, out_features=16384, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (down_proj): lora.Linear(
                (base_layer): Linear(in_features=8192, out_features=3072, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=8192, out_features=128, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=128, out_features=3072, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (activation_fn): SiLU()
            )
            (input_layernorm): Phi3RMSNorm()
            (resid_attn_dropout): Dropout(p=0.0, inplace=False)
            (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
            (post_attention_layernorm): Phi3RMSNorm()
          )
        )
        (norm): Phi3RMSNorm()
      )
      (score): ModulesToSaveWrapper(
        (original_module): Linear(in_features=3072, out_features=2, bias=False)
        (modules_to_save): ModuleDict(
          (default): Linear(in_features=3072, out_features=2, bias=False)
        )
      )
    )
  )
)
</code></pre>
<p><strong>MERGED and Loaded Model</strong></p>
<pre><code>Phi3ForSequenceClassification(
  (model): Phi3Model(
    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)
    (embed_dropout): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0-31): 32 x Phi3DecoderLayer(
        (self_attn): Phi3FlashAttention2(
          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)
          (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)
          (rotary_emb): Phi3RotaryEmbedding()
        )
        (mlp): Phi3MLP(
          (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)
          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)
          (activation_fn): SiLU()
        )
        (input_layernorm): Phi3RMSNorm()
        (resid_attn_dropout): Dropout(p=0.0, inplace=False)
        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
        (post_attention_layernorm): Phi3RMSNorm()
      )
    )
    (norm): Phi3RMSNorm()
  )
  (score): Linear(in_features=3072, out_features=2, bias=False)
)
</code></pre>
","huggingface"
"78510712","Adding Crossencoder inside a function impact performance, why?","2024-05-21 08:59:42","","0","19","<huggingface-transformers><huggingface>","<p>Below is a fastapi endpoint with crossencoder of mxbai-re-ranker insider the fucntion</p>
<pre><code>@app.post('/rank')
async def rank(request_data: RankerPredictionRequest):
    ranker_model = CrossEncoder(&quot;mixedbread-ai/mxbai-rerank-large-v1&quot;,device=&quot;cuda:0&quot;)
    query = request_data.query
    documents = request_data.documents
    results = ranker_model.rank(query, documents,return_documents=True,convert_to_numpy=False)
    resp = modify_data(results)
    return resp
</code></pre>
<p>which took 3x more time to repond, when compared to calling the model outside the function</p>
<pre><code>ranker_model = CrossEncoder(&quot;mixedbread-ai/mxbai-rerank-large-v1&quot;,device=&quot;cuda:0&quot;)
@app.post('/rank')
async def rank(request_data: RankerPredictionRequest):
    query = request_data.query
    documents = request_data.documents
    results = ranker_model.rank(query, documents,return_documents=True,convert_to_numpy=False)
    resp = modify_data(results)
    return resp
</code></pre>
<p>I can't understand why this is happening.</p>
<p>I was expecting similar response time in both cases.</p>
","huggingface"
"78499234","huggingface optimum circular dependency issue","2024-05-18 09:22:25","","1","88","<python><huggingface><onnx><onnxruntime>","<p>I have a fresh virtual env where I am trying to exec an onnx model like so:</p>
<pre><code># Load Locally Saved ONNX Model and use for inference
from transformers import AutoTokenizer
from optimum.onnxruntime import ORTModelForCustomTasks

sentence = &quot;This is a test sentence.&quot;
local_onnx_model = ORTModelForCustomTasks.from_pretrained(&quot;./model_onnx&quot;)
tokenizer = AutoTokenizer.from_pretrained(&quot;./model_onnx&quot;)
inputs = tokenizer(
    sentence,
    padding=&quot;longest&quot;,
    return_tensors=&quot;np&quot;,
)
outputs = local_onnx_model.forward(**inputs)
print(outputs)
</code></pre>
<p>but it ouputs this annoying error:</p>
<pre><code>The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.
0it [00:00, ?it/s]
2024-05-18 03:36:29.312177: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-05-18 03:36:29.888001: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Traceback (most recent call last):
  File &quot;/home/foo/miniconda3/envs/new_virtual_env/lib/python3.10/site-packages/transformers/utils/import_utils.py&quot;, line 1510, in _get_module
    return importlib.import_module(&quot;.&quot; + module_name, self.__name__)
  File &quot;/home/foo/miniconda3/envs/new_virtual_env/lib/python3.10/importlib/__init__.py&quot;, line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1050, in _gcd_import
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1027, in _find_and_load
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _find_and_load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 688, in _load_unlocked
  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 883, in exec_module
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 241, in _call_with_frames_removed
  File &quot;/home/foo/miniconda3/envs/new_virtual_env/lib/python3.10/site-packages/optimum/exporters/onnx/__main__.py&quot;, line 26, in &lt;module&gt;
    from ...commands.export.onnx import parse_args_onnx
  File &quot;/home/foo/miniconda3/envs/new_virtual_env/lib/python3.10/site-packages/optimum/commands/__init__.py&quot;, line 17, in &lt;module&gt;
    from .export import ExportCommand, ONNXExportCommand, TFLiteExportCommand
  File &quot;/home/foo/miniconda3/envs/new_virtual_env/lib/python3.10/site-packages/optimum/commands/export/__init__.py&quot;, line 16, in &lt;module&gt;
    from .base import ExportCommand
  File &quot;/home/foo/miniconda3/envs/new_virtual_env/lib/python3.10/site-packages/optimum/commands/export/base.py&quot;, line 18, in &lt;module&gt;
    from .onnx import ONNXExportCommand
  File &quot;/home/foo/miniconda3/envs/new_virtual_env/lib/python3.10/site-packages/optimum/commands/export/onnx.py&quot;, line 23, in &lt;module&gt;
    from ...exporters import TasksManager
ImportError: cannot import name 'TasksManager' from partially initialized module 'optimum.exporters' (most likely due to a circular import) (/home/foo/miniconda3/envs/new_virtual_env/lib/python3.10/site-packages/optimum/exporters/__init__.py)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File &quot;/home/foo/miniconda3/envs/new_virtual_env/lib/python3.10/site-packages/transformers/utils/import_utils.py&quot;, line 1510, in _get_module
    return importlib.import_module(&quot;.&quot; + module_name, self.__name__)
  File &quot;/home/foo/miniconda3/envs/new_virtual_env/lib/python3.10/importlib/__init__.py&quot;, line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1050, in _gcd_import
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1027, in _find_and_load
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _find_and_load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 688, in _load_unlocked
  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 883, in exec_module
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 241, in _call_with_frames_removed
  File &quot;/home/foo/miniconda3/envs/new_virtual_env/lib/python3.10/site-packages/optimum/onnx/graph_transformations.py&quot;, line 19, in &lt;module&gt;
    import onnx
  File &quot;/home/foo/new_virtual_env/rezatec_cpy/src/onnx.py&quot;, line 1, in &lt;module&gt;
    from optimum.exporters.onnx import main_export
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1075, in _handle_fromlist
  File &quot;/home/foo/miniconda3/envs/new_virtual_env/lib/python3.10/site-packages/transformers/utils/import_utils.py&quot;, line 1500, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File &quot;/home/foo/miniconda3/envs/new_virtual_env/lib/python3.10/site-packages/transformers/utils/import_utils.py&quot;, line 1512, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import optimum.exporters.onnx.__main__ because of the following error (look up to see its traceback):
cannot import name 'TasksManager' from partially initialized module 'optimum.exporters' (most likely due to a circular import) (/home/foo/miniconda3/envs/new_virtual_env/lib/python3.10/site-packages/optimum/exporters/__init__.py)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File &quot;/home/foo/miniconda3/envs/new_virtual_env/lib/python3.10/site-packages/transformers/utils/import_utils.py&quot;, line 1510, in _get_module
    return importlib.import_module(&quot;.&quot; + module_name, self.__name__)
  File &quot;/home/foo/miniconda3/envs/new_virtual_env/lib/python3.10/importlib/__init__.py&quot;, line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1050, in _gcd_import
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1027, in _find_and_load
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _find_and_load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 688, in _load_unlocked
  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 883, in exec_module
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 241, in _call_with_frames_removed
  File &quot;/home/foo/miniconda3/envs/new_virtual_env/lib/python3.10/site-packages/optimum/onnxruntime/modeling_ort.py&quot;, line 61, in &lt;module&gt;
    from ..exporters import TasksManager
  File &quot;/home/foo/miniconda3/envs/new_virtual_env/lib/python3.10/site-packages/optimum/exporters/__init__.py&quot;, line 16, in &lt;module&gt;
    from .tasks import TasksManager  # noqa
  File &quot;/home/foo/miniconda3/envs/new_virtual_env/lib/python3.10/site-packages/optimum/exporters/tasks.py&quot;, line 139, in &lt;module&gt;
    class TasksManager:
  File &quot;/home/foo/miniconda3/envs/new_virtual_env/lib/python3.10/site-packages/optimum/exporters/tasks.py&quot;, line 297, in TasksManager
    &quot;clip-text-model&quot;: supported_tasks_mapping(
  File &quot;/home/foo/miniconda3/envs/new_virtual_env/lib/python3.10/site-packages/optimum/exporters/tasks.py&quot;, line 111, in supported_tasks_mapping
    importlib.import_module(f&quot;optimum.exporters.{backend}.model_configs&quot;), config_cls_name
  File &quot;/home/foo/miniconda3/envs/new_virtual_env/lib/python3.10/importlib/__init__.py&quot;, line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File &quot;/home/foo/miniconda3/envs/new_virtual_env/lib/python3.10/site-packages/optimum/exporters/onnx/model_configs.py&quot;, line 23, in &lt;module&gt;
    from ...onnx import merge_decoders
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1075, in _handle_fromlist
  File &quot;/home/foo/miniconda3/envs/new_virtual_env/lib/python3.10/site-packages/transformers/utils/import_utils.py&quot;, line 1500, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File &quot;/home/foo/miniconda3/envs/new_virtual_env/lib/python3.10/site-packages/transformers/utils/import_utils.py&quot;, line 1512, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import optimum.onnx.graph_transformations because of the following error (look up to see its traceback):
Failed to import optimum.exporters.onnx.__main__ because of the following error (look up to see its traceback):
cannot import name 'TasksManager' from partially initialized module 'optimum.exporters' (most likely due to a circular import) (/home/foo/miniconda3/envs/new_virtual_env/lib/python3.10/site-packages/optimum/exporters/__init__.py)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File &quot;/home/foo/new_virtual_env/rezatec_cpy/src/onnx2.py&quot;, line 1, in &lt;module&gt;
    from optimum.onnxruntime import ORTModelForSequenceClassification
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1075, in _handle_fromlist
  File &quot;/home/foo/miniconda3/envs/new_virtual_env/lib/python3.10/site-packages/transformers/utils/import_utils.py&quot;, line 1500, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File &quot;/home/foo/miniconda3/envs/new_virtual_env/lib/python3.10/site-packages/transformers/utils/import_utils.py&quot;, line 1512, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import optimum.onnxruntime.modeling_ort because of the following error (look up to see its traceback):
Failed to import optimum.onnx.graph_transformations because of the following error (look up to see its traceback):
Failed to import optimum.exporters.onnx.__main__ because of the following error (look up to see its traceback):
cannot import name 'TasksManager' from partially initialized module 'optimum.exporters' (most likely due to a circular import) (/home/foo/miniconda3/envs/new_virtual_env/lib/python3.10/site-packages/optimum/exporters/__init__.py)
</code></pre>
<p>Its a new env and I have run:</p>
<p><code>pip install --upgrade --upgrade-strategy --no-cache-dir eager optimum[exporters,onnxruntime]</code></p>
<p>as per the docs to install the pkg.</p>
<p>Totally lost!</p>
","huggingface"
"78494810","LLM generation problem - only returning gibberish?","2024-05-17 09:41:59","","0","214","<python><large-language-model><huggingface><llama-index>","<p>I am trying to load a model from path into llamaindex with the HuggingfaceLLM class like this:</p>
<pre><code>from llama_index.llms.huggingface import HuggingFaceLLM

llm = HuggingFaceLLM(
   context_window=2048,
   max_new_tokens=300,
   generate_kwargs={&quot;temperature&quot;: 0.5, &quot;do_sample&quot;: True},
   #query_wrapper_prompt=query_wrapper_prompt,
   tokenizer_name=&quot;local_path/leo-hessianai-7B-AWQ&quot;,
   model_name=&quot;local_path/leo-hessianai-7B-AWQ&quot;,
   device_map=&quot;auto&quot;
)
</code></pre>
<p>The folders are downloaded from the huggingface-hub and the model is loading, however, when I query it, it returns only gibberish (like hohohohohohohohohohohohohoho and so on)</p>
<p>The source nodes are plausible and correct, I checked that, it is only the generating part that appears to be wrong.</p>
<p>Is there anything I am missing here? When I load the model from the hub with the link it's fine, but that does not work in the IDE (and Ollama etc. are also not an option).</p>
<p>I appreciate any help, thanks!</p>
","huggingface"
"78493588","How to apply .map() function and keep it as an iterator for a Hugging Face Dataset, in Streaming Mode without loading it to memory?","2024-05-17 05:14:40","","1","127","<machine-learning><deep-learning><huggingface-transformers><huggingface><huggingface-datasets>","<p>I'm currently working with the Hugging Face datasets library and need to apply transformations to multiple datasets (such as ds_khan and ds_mathematica) using the .map() function, but in a way that mimics streaming (i.e., without loading the entire dataset into memory). I am particularly interested in interleaving these transformed datasets while keeping the data processing as lazy as possible, similar to streaming=True.</p>
<p>Here is the relevant part of my current code:</p>
<pre class=""lang-py prettyprint-override""><code>from datasets import load_dataset, interleave_datasets

def get_hf_khan_ds(path_2_ds: str, split: str = 'train'):
    path_2_ds = os.path.expanduser(path_2_ds)
    dataset = load_dataset('json', data_files=[path_2_ds], split=split, streaming=True)
    problem_as_text = lambda example: {'text': example['problem']}
    return dataset.map(problem_as_text, remove_columns=dataset.column_names)

def main():
    ds_khan = get_hf_khan_ds('~/gold-ai-olympiad/data/amps/khan/train.jsonl')
    ds_mathematica = get_hf_khan_ds('~/gold-ai-olympiad/data/amps/mathematica/train.jsonl')
    interleaved_datasets = interleave_datasets([ds_khan, ds_mathematica], probabilities=[0.5, 0.5])
    for sample in interleaved_datasets.take(10):
        print(sample)

if __name__ == '__main__':
    main()
</code></pre>
<p>This setup is intended to process and interleave the datasets without loading them fully into memory. However, I am not sure if this approach is correctly implementing the streaming and lazy evaluation as I intend.</p>
<p>Questions:</p>
<ol>
<li>Does this code correctly apply transformations in a streaming or iterator-style fashion?</li>
<li>If not, how can I modify it to ensure that each dataset is only processed as needed, without preloading the entire content?</li>
<li>Is there a more efficient way to interleave these datasets while maintaining a streaming approach?</li>
</ol>
<p>Any suggestions or insights on how to effectively use .map() with streaming=True for interleaving datasets would be greatly appreciated (note I do have the data set in disk but eventually I want to work with HF datasets).</p>
<p>output:</p>
<pre><code>  table = cls._concat_blocks(blocks, axis=0)
Map: 100%|█████████████████████████████████████████████████████████████████████████████████████| 103059/103059 [00:06&lt;00:00, 16218.25 examples/s]
Map: 100%|█████████████████████████████████████████████████████████████████████████████████████| 103059/103059 [00:07&lt;00:00, 13633.64 examples/s]
Map: 100%|█████████████████████████████████████████████████████████████████████████████████████| 103059/103059 [00:08&lt;00:00, 12444.64 examples/s]
/lfs/ampere1/0/brando9/miniconda/envs/gold_ai_olympiad/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.
  table = cls._concat_blocks(blocks, axis=0)
Downloading data files: 100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00&lt;00:00, 3792.32it/s]
Extracting data files: 100%|██████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00&lt;00:00, 479.84it/s]
Generating train split: 20 examples [00:00, 3628.29 examples/s]
Map: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00&lt;00:00, 2792.11 examples/s]
Map: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00&lt;00:00, 3525.66 examples/s]
Map: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00&lt;00:00, 3415.97 examples/s]
/lfs/ampere1/0/brando9/miniconda/envs/gold_ai_olympiad/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.
  table = cls._concat_blocks(blocks, axis=0)
probabilities=[0.3333333333333333, 0.3333333333333333, 0.3333333333333333]
/lfs/ampere1/0/brando9/miniconda/envs/gold_ai_olympiad/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.
  table = cls._concat_blocks(blocks, axis=0)
Done! Time: 50.09 sec, 0.83 min, 0.01 hr
</code></pre>
<p>I didn't expect the output to show the entire data set...which confused me. Also it's taking too long to load which is bad.</p>
<p>ref: <a href=""https://discuss.huggingface.co/t/how-to-apply-map-function-and-keep-it-as-an-iterator-for-a-hugging-face-dataset-in-streaming-mode-without-loading-it-to-memroy/87110"" rel=""nofollow noreferrer"">https://discuss.huggingface.co/t/how-to-apply-map-function-and-keep-it-as-an-iterator-for-a-hugging-face-dataset-in-streaming-mode-without-loading-it-to-memroy/87110</a></p>
","huggingface"
"78486911","How to Load an Already Instantiated Hugging Face Model into vLLM for Inference?","2024-05-15 23:17:47","","0","581","<nlp><huggingface-transformers><huggingface><huggingface-hub><vllm>","<p>I am working on a project where I need to utilize a model that has already been loaded and instantiated on the GPU using Hugging Face's Transformers library. The goal is to pass this loaded model into the vLLM framework for further processing and inference without reloading it from disk or a model hub.</p>
<p>Here's what I have done so far using Hugging Face to load the model:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import GPT2LMHeadModel

# Load GPT-2 model already fine-tuned and available in memory
model = GPT2LMHeadModel.from_pretrained('gpt2')
model.to('cuda')  # Assuming the model is moved to GPU
</code></pre>
<p>I am looking for a way to pass this model instance directly to vLLM's LLM class or a similar interface within vLLM that can accept an already instantiated model. The vLLM documentation primarily discusses initializing its LLM class with a model identifier, which it then loads internally.</p>
<p>Is there a way to integrate an in-memory model from Hugging Face into vLLM without reloading it? Specifically, I want to leverage vLLM's efficient serving capabilities with a model instance that is already in use and configured in my existing pipeline.</p>
<p>Any insights or suggestions on how to achieve this would be greatly appreciated!</p>
<hr />
<p>This doesn't work:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import GPT2LMHeadModel, GPT2Tokenizer

# Step 1: Load and Save the Model using Hugging Face
model_name = 'gpt2'
model = GPT2LMHeadModel.from_pretrained(model_name)
tokenizer = GPT2Tokenizer.from_pretrained(model_name)

# Save the model and tokenizer
model_save_path = './model/gpt2'
tokenizer.save_pretrained(model_save_path)
model.save_pretrained(model_save_path)

# Step 2: Load the model in vLLM using the saved model path
from vllm import LLM, SamplingParams

# Assuming vLLM can load models from a path
vllm_model = LLM(model=model)

# Define sampling parameters and prompts
sampling_params = SamplingParams(temperature=0.8, top_p=0.95)
prompts = [&quot;Hello, my name is&quot;, &quot;The future of AI is&quot;]

# Generate text using the specified model and parameters in vLLM
outputs = vllm_model.generate(prompts, sampling_params)

# Print the generated texts
for output in outputs:
    print(f&quot;Prompt: {output.prompt}, Generated text: {output.outputs[0].text}&quot;)
</code></pre>
<p>Error:</p>
<pre><code>Exception has occurred: OSError
Incorrect path_or_model_id: 'GPT2LMHeadModel(
  (transformer): GPT2Model(
    (wte): Embedding(50257, 768)
    (wpe): Embedding(1024, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-11): 12 x GPT2Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=768, out_features=50257, bias=False)
)'. Please provide either the path to a local folder or the repo_id of a model on the Hub.
huggingface_hub.errors.HFValidationError: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: 'GPT2LMHeadModel(
  (transformer): GPT2Model(
    (wte): Embedding(50257, 768)
    (wpe): Embedding(1024, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-11): 12 x GPT2Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=768, out_features=50257, bias=False)
)'.

The above exception was the direct cause of the following exception:

  File &quot;/lfs/ampere1/0/brando9/gold-ai-olympiad/py_src/training/maf_self_improv_train.py&quot;, line 35, in &lt;module&gt;
    vllm_model = LLM(model=model)
                 ^^^^^^^^^^^^^^^^
OSError: Incorrect path_or_model_id: 'GPT2LMHeadModel(
  (transformer): GPT2Model(
    (wte): Embedding(50257, 768)
    (wpe): Embedding(1024, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-11): 12 x GPT2Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=768, out_features=50257, bias=False)
)'. Please provide either the path to a local folder or the repo_id of a model on the Hub.
</code></pre>
<p>ref: <a href=""https://github.com/vllm-project/vllm/issues/4843"" rel=""nofollow noreferrer"">https://github.com/vllm-project/vllm/issues/4843</a></p>
","huggingface"
"78485347","Encode a list of sentences into embeddings using a HuggingFace model not in its hub","2024-05-15 16:39:39","78502336","1","258","<nlp><huggingface-transformers><encode><embedding><huggingface>","<p>I am trying to encode a list of sentences into a list of embeddings. When I use a model that is in the HuggingFace hub, it works as expected. But when I use a model not in the hub, in this case Facebook's <code>M2M100</code> model, I do not get the expected results.</p>
<p>When using a model within <code>SentenceTransformer()</code>, my results look like this:</p>
<pre><code>from sentence_transformers import SentenceTransformer
dat = ['Meteorite fell on the road ', 'I went in the wrong direction']
model_1 = SentenceTransformer('all-distilroberta-v1')
embeddings_1 = model_1.encode(dat)
embeddings_1.shape
&gt; (2, 768)
</code></pre>
<p>However, when I use the <code>M2M100</code> model, my results do not look right at all, specifically I would expect 2 rows of results:</p>
<pre><code>from transformers import M2M100Tokenizer
model_m2m = M2M100Tokenizer.from_pretrained(&quot;facebook/m2m100_418M&quot;)
model_m2m.src_lang = &quot;en&quot;
embeddings_m2m = model_m2m.encode(dat, return_tensors=&quot;pt&quot;)
embeddings_m2m.shape
&gt; torch.Size([1, 4])
</code></pre>
<p>How should I format this so that it returns an n-dimensional list of embeddings, where each row corresponds to a sentence and the number of columns is equal to the dimensionality of the embedding?</p>
<p>(As a note, eventually I will be doing this for sentences in other languages, which is why I'm using a multi-lingual model.)</p>
","huggingface"
"78485019","Call Text-Generation-Inference (TGI) with a batch of prompts","2024-05-15 15:33:57","","0","231","<huggingface><huggingface-hub>","<p>In Text-Generation-Inference (TGI), I see that there is a parameter of <code>--max-batch-total-tokens</code>, indicating that there is a batch request capability available via TGI. But, when I see the API guide, I cannot find anything related to that. For example, for <code>/generate</code>, the input format is</p>
<pre><code>{
  &quot;inputs&quot;: &quot;My name is Olivier and I&quot;,
  &quot;parameters&quot;: {
    &quot;best_of&quot;: 1,
    &quot;decoder_input_details&quot;: false,
    &quot;details&quot;: true,
    &quot;do_sample&quot;: true,
    &quot;frequency_penalty&quot;: 0.1,
    &quot;grammar&quot;: null,
    &quot;max_new_tokens&quot;: 20,
    &quot;repetition_penalty&quot;: 1.03,
    &quot;return_full_text&quot;: false,
    &quot;seed&quot;: null,
    &quot;stop&quot;: [
      &quot;photographer&quot;
    ],
    &quot;temperature&quot;: 0.5,
    &quot;top_k&quot;: 10,
    &quot;top_n_tokens&quot;: 5,
    &quot;top_p&quot;: 0.95,
    &quot;truncate&quot;: null,
    &quot;typical_p&quot;: 0.95,
    &quot;watermark&quot;: true
  }
}
</code></pre>
<p>which cannot handle batch requests. I was wondering if there is a batch-request support, and if so, it would be great to add some API guideline/documentation for that.</p>
<p>Need to mention that I also checked <code>InferenceClient</code> from <code>huggingface_hub</code> and a same observation:</p>
<pre><code>from huggingface_hub import InferenceClient
client = InferenceClient(model=&quot;http://127.0.0.1:8080&quot;)
client.text_generation(prompt=input_prompt)
</code></pre>
<p>in which <code>prompt</code> must be a <code>string</code>, so cannot pass a list of text of something similar.</p>
","huggingface"
"78484166","Retrieve data from Pinecone vectors in retreivalQA function (using huggingface embeddings)","2024-05-15 13:06:00","","0","72","<huggingface><llama><sentence-transformers><pinecone><retrieval-augmented-generation>","<p>I am using llama2 model to create a RAG based LLM with huggingface sentence-transformers/all-MiniLM-L6-v2 embedding and pinecone to store information as Vector database. I am feeding a pdf and am able to get results using similarity search on query from vectors in pinecone. I am facing issue in providing the best-k results as context in RetrievalQA function, specifically its retrieval argument:</p>
<pre><code>qa=RetrievalQA.from_chain_type(
    llm=llm, 
    chain_type=&quot;stuff&quot;, 
    retriever=??
    return_source_documents=True, 
    chain_type_kwargs=chain_type_kwargs)
</code></pre>
<p>Here is the complete code:</p>
<pre><code>import os
from pinecone import Pinecone
from pinecone import ServerlessSpec

# initialize connection to pinecone (get API key at app.pinecone.io)
api_key = PINECONE_API_KEY
# configure client
pc = Pinecone(api_key=api_key)
cloud = os.environ.get('PINECONE_CLOUD') or 'aws'
region = os.environ.get('PINECONE_REGION') or 'us-east-1'
spec = ServerlessSpec(cloud=cloud, region=region)
index_name = 'semantic-search-fast'
import time

existing_indexes = [
    index_info[&quot;name&quot;] for index_info in pc.list_indexes()
]

# check if index already exists (it shouldn't if this is first time)
if index_name not in existing_indexes:
    # if does not exist, create index
    pc.create_index(
        index_name,
        dimension=384,  # dimensionality of minilm
        metric='dotproduct',
        spec=spec
    )
    # wait for index to be initialized
    while not pc.describe_index(index_name).status['ready']:
        time.sleep(1)

# connect to index
index = pc.Index(index_name)
time.sleep(1)
# populating vectors
for i, t in zip(range(len(text_chunks)), text_chunks):
   query_result = embeddings.embed_query(t.page_content)
   index.upsert(
   vectors=[
        {
            &quot;id&quot;: str(i),  # Convert i to a string
            &quot;values&quot;: query_result, 
            &quot;metadata&quot;: {&quot;text&quot;:str(text_chunks[i].page_content)} # meta data as dic
        }
    ],
    namespace=&quot;real&quot; 
)
question_embedding = embeddings.embed_query(&quot;What is acne?&quot;)
query_result = index.query(namespace=&quot;real&quot;,
    vector=question_embedding,
    top_k=3,
    include_metadata=True
)
relevant_docs = [hit[&quot;metadata&quot;][&quot;text&quot;] for hit in query_result[&quot;matches&quot;]]

</code></pre>
<p>I tried different approaches like creating vectorstore using</p>
<pre><code>vectorstore = Pinecone.from_existing_index(index_name=index_name, embedding=HuggingFaceEmbeddings(), namespace='real') 
</code></pre>
<p>and then thought of using retriever=vectorstore.as_retriever() in the argument for RetrievalQA.
But initializing vectorstore throws an error stating :</p>
<pre><code>__init__() missing 1 required positional argument: 'host'
</code></pre>
<p>and doesn't work. I want to know if I could somehow get the above code to work the way it is, without the vectorstore</p>
","huggingface"
"78483286","How to use BM25Retriever for large no of chunks?","2024-05-15 10:25:14","","0","165","<python-3.x><langchain><large-language-model><huggingface><retrieval-augmented-generation>","<pre><code>vectorstore = Chroma.from_documents(chunks, embeddings)
vectorstore_retreiver = vectorstore.as_retriever(search_kwargs={&quot;k&quot;: 3})

keyword_retriever = BM25Retriever.from_documents(list_of_doc)
keyword_retriever.k =  3
ensemble_retriever = EnsembleRetriever(retrievers=[vectorstore_retreiver,keyword_retriever],weights=[0.5, 0.5])
</code></pre>
<p>Above is the code for my hybrid search. It's working fine if my I have 100 or 1000 documents. But right now I am working on a project where I have more than 5 million chunks are there. How can I use bm25 retrival on these no of chunks. Is there any efficient way to load such large no of chunks?</p>
<p>I tried using loading everything in batches but could not found how to that?</p>
","huggingface"
"78481431","related to huggingface inference api not working","2024-05-15 03:08:37","","0","41","<large-language-model><huggingface>","<p>I have done a simple project which is Text Summarization and I used T5 Model and I successfully fine tuned my model and also inference it on notebook.i have installed bitsandbytes, accelerate,trl and all.</p>
<p>But when push it on hugginface and inference it on server less API. I get error &quot;No package metadata was found for bitsandbytes&quot;.</p>
<p>Please suggest me how I can resolve this issue.</p>
<p>I tried all possible solutions</p>
","huggingface"
"78478607","Unable to deploy hugging face model to sagemaker endpoint - C:\\.sagemaker-code-config not found","2024-05-14 14:01:08","78479475","0","100","<python><deployment><amazon-sagemaker><huggingface><mlops>","<p>I'm trying to make a sagemaker endpoint using sagemaker and hugging face libraries.</p>
<pre><code>import sagemaker
sess = sagemaker.Session()
sagemaker_session_bucket=None
if sagemaker_session_bucket is None and sess is not None:
    # set to default bucket if a bucket name is not given
    sagemaker_session_bucket = sess.default_bucket()

role = &quot;my-IAM-role&quot;

sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)

repository = &quot;FremyCompany/BioLORD-2023-M&quot;
model_id=repository.split(&quot;/&quot;)[-1]
s3_location=f&quot;s3://{sess.default_bucket()}/custom_inference/{model_id}/model.tar.gz&quot;

from sagemaker.huggingface.model import HuggingFaceModel


# create Hugging Face Model Class
huggingface_model = HuggingFaceModel(
   model_data=s3_location,       # path to your model and script
   role = role,
   transformers_version=&quot;4.37.0&quot;,  # transformers version used
   pytorch_version=&quot;2.1.0&quot;,        # pytorch version used
   py_version=&quot;py310&quot;,            # python version used
)

# deploy model to SageMaker Inference
predictor = huggingface_model.deploy(
   initial_instance_count=1,
   instance_type=&quot;ml.m4.xlarge&quot;,
   endpoint_name=&quot;bioLORD-test&quot;
)
</code></pre>
<p>But when I execute the code it runs forever.
When I interrupt the execution and check the logs above the <code>KeyboardInterrupt</code> error there is the following:</p>
<pre><code>FileNotFoundError                         Traceback (most recent call last)
File ~\AppData\Local\Programs\Python\Python312\Lib\pathlib.py:860, in Path.exists(self, follow_symlinks)
    859 try:
--&gt; 860     self.stat(follow_symlinks=follow_symlinks)
    861 except OSError as e:

File ~\AppData\Local\Programs\Python\Python312\Lib\pathlib.py:840, in Path.stat(self, follow_symlinks)
    836 &quot;&quot;&quot;
    837 Return the result of the stat() system call on this path, like
    838 os.stat() does.
    839 &quot;&quot;&quot;
--&gt; 840 return os.stat(self, follow_symlinks=follow_symlinks)

FileNotFoundError: [WinError 2] The system cannot find the file specified: 'C:\\.sagemaker-code-config'

During handling of the above exception, another exception occurred:

KeyboardInterrupt                         Traceback (most recent call last)
</code></pre>
<p>I've transferred from MacOS to Windows short time ago and this code use to run properly on Mac.
I've tried to search for the 'sagemaker-code-config' but wasn't able to find anything useful.
I also don't understand why the code runs forever instead of throwing the <code>FileNotFoundError</code>.
Thanks for help!</p>
<p>P.S. I've tried to execute the same code in Ubuntu in WSL but got the same result.</p>
","huggingface"
"78474836","LangChain Hugging face model for PromptTemplate class","2024-05-13 21:18:49","","0","153","<langchain><huggingface><py-langchain><huggingface-hub>","<p>when invoking the Intel/dynamic_tinybert model in the HuggingFaceEndpoint/HuggingFaceHub class in langchain for a translation/Q&amp;A task, one persistent error was as below:</p>
<p>Bad request:
Error in inputs: value is not a valid dict</p>
<p>even when passing the input in the prompt as a dict value type, the error was the same.</p>
<p>The HuggingFaceEndpoint/HuggingFaceHub class was used for the langchain example but the repo_id, which was previously google/flan-t5-x1 was replace with Intel/dynamic_tinybert as the former is no longer available.</p>
<p>what is the ideal input format for the model, when using langchain?</p>
<p>ref: <a href=""https://www.pinecone.io/learn/series/langchain/langchain-intro/"" rel=""nofollow noreferrer"">https://www.pinecone.io/learn/series/langchain/langchain-intro/</a></p>
<p>Thanks</p>
<p>I have tried other model but the same invalid input error was returned i.e.</p>
<p>Bad request:
Error in inputs: value is not a valid dict</p>
","huggingface"
"78474448","OSError: [model] does not appear to have a file named config.json","2024-05-13 19:38:47","78474490","0","1561","<python><machine-learning><deep-learning><huggingface-transformers><huggingface>","<p>I want to load a huggingface model. <a href=""https://huggingface.co/laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup"" rel=""nofollow noreferrer"">The model</a> I want to load has about 150K downloads so I don't think there is any problem with the model itself.</p>
<p>With the both loading codes below I get the same error:</p>
<pre><code>from transformers import AutoModel
AutoModel.from_pretrained(&quot;laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup&quot;)
</code></pre>
<p>And</p>
<pre><code>from transformers import CLIPProcessor, CLIPModel
model_id = &quot;laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup&quot;
processor = CLIPProcessor.from_pretrained(model_id)
model = CLIPModel.from_pretrained(model_id)
</code></pre>
<p>With both I get:</p>
<pre><code>OSError: laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup does not appear to have a file named preprocessor_config.json. Checkout 'https://huggingface.co/laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup/main' for available files.
</code></pre>
<p>Any help to load the model would be appreciated.</p>
","huggingface"
"78473010","Hugging Face Datasets .map not working as expected","2024-05-13 14:36:03","","0","240","<python><huggingface><huggingface-datasets>","<p>I'm running a function over a dataset, but when I compute this, I seem to replace my existing dataset rather than adding to it. What is going wrong?</p>
<pre><code>dataset_c = Dataset.from_pandas(df_all[0:100])
</code></pre>
<p>That <code>dataset_c</code> looks like:</p>
<pre><code>Dataset({
    features: ['_id', 'first_name', 'last_name', 'memail', 'company_phone_number', 'company_name', 'global_id'],
    num_rows: 100
})
</code></pre>
<p>My function is:</p>
<pre><code>def concatenate_entities(examples):
 body = &quot;&quot;
 for col in fields_match_list:
   if col in examples:
     #if (examples[col] != &quot;&quot;):
     body +=  examples[col] + &quot;\n &quot;
   else:
     body += &quot;\n &quot;
 return {&quot;text&quot;: str(body)}
</code></pre>
<p>Now, when I do the map.</p>
<pre><code>dataset_c = dataset_c.map(concatenate_entities)
</code></pre>
<p>I get the following:</p>
<pre><code>Dataset({
    features: ['text'],
    num_rows: 100

})
</code></pre>
<p>So, instead of adding to existing features, it has replaced the features.</p>
","huggingface"
"78471692","How to run a local Open Source LLM in llama-index in a restricted environment?","2024-05-13 10:41:03","","0","761","<large-language-model><huggingface><llama-index><retrieval-augmented-generation>","<p>I have a question on how to best run a local LLM (all Open Source) with llama-index for a RAG in a relatively restricted environment (absolutely no API calls, no installing from external GitRepos and also no Ollama or vLLM - which basically covers all I have experience with so far and all the examples I have come across...)</p>
<p>My approach is now to just load the quantized model with AWQ and then pass it to the query_engine, however, HuggingfaceLLM does not seem to support a locally stored model?</p>
<p>My specific question is, if I load a model like this:</p>
<pre><code>from awq import AutoAWQForCausalLM
from transformers import AutoTokenizer

model_name_or_path = &quot;local path to folder/model&quot;

# Load model
model = AutoAWQForCausalLM.from_quantized(model_name_or_path, fuse_layers=True,trust_remote_code=False, safetensors=True)

tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=False)

</code></pre>
<p>then how do i proceed to further integrating this to llama-index? Do I need to write a custom LLM class?</p>
<p>Is there any other option on how to achieve this? Hardware won't be a problem, my question is on the best method to include the model.</p>
<p>Thanks in advance!</p>
","huggingface"
"78468421","Error installing Meta-Llama-3-70B model from Hugging Face Hub","2024-05-12 15:36:22","78642941","1","679","<large-language-model><huggingface><http-error><llama>","<p>I'm trying to load the <code>Meta-Llama-3-70B</code> model from the Hugging Face Hub using the Transformers library in Python, but I'm encountering the following error:</p>
<pre><code>OSError: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like meta-llama/Meta-Llama-3-70B is not the path to a directory containing a file named config.json.  Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
</code></pre>
<p>Here's the code I'm using:</p>
<pre><code>import torch
import transformers

model_id = &quot;meta-llama/Meta-Llama-3-70B&quot;
pipeline = transformers.pipeline(
    &quot;text-generation&quot;, model=model_id, model_kwargs={&quot;torch_dtype&quot;: torch.bfloat16}, device_map=&quot;auto&quot;
)
pipeline(&quot;Hey how are you doing today?&quot;)
</code></pre>
<p>I've granted access to the <code>Meta-Llama-3-70B</code> model on the Hugging Face website, but I'm still encountering this error. I've checked my internet connection, and it seems to be working fine.</p>
<p>Can someone help me understand what might be causing this issue and how to resolve it? Are there any additional steps I need to take to successfully load and use the <code>Meta-Llama-3-70B</code> model from the Hugging Face Hub?</p>
","huggingface"
"78466376","How to generate multiple text completions per prompt (like vLLM) using HuggingFace Transformers Pipeline without triggering an error?","2024-05-12 00:06:07","","1","387","<machine-learning><deep-learning><neural-network><huggingface-transformers><huggingface>","<p>I'm using the HuggingFace Transformers Pipeline library to generate multiple text completions for a given prompt. My goal is to utilize a model like GPT-2 to generate <strong>different possible completions like the defaults in vLLM</strong>. However, I am encountering an issue with unused model_kwargs when I attempt to specify parameters like max_length and num_return_sequences.</p>
<p>Here is the code snippet I'm using:</p>
<pre class=""lang-py prettyprint-override""><code>Copy code
from transformers import GPT2Tokenizer, GPT2LMHeadModel, pipeline
from typing import List, Dict

def process_prompts(prompts: List[str], model: GPT2LMHeadModel, tokenizer: GPT2Tokenizer, num_completions: int = 3) -&gt; List[List[str]]:
    device = 0 if model.device.type == 'cuda' else -1
    text_generator = pipeline(&quot;text-generation&quot;, model=model, tokenizer=tokenizer, device=device)
    outputs = []

    for prompt in prompts:
        try:
            results = text_generator(prompt, max_length=50, num_return_sequences=num_completions, num_beams=num_completions)
            completions = [result['generated_text'] for result in results]
            outputs.append(completions)
        except Exception as e:
            print(f&quot;Error processing prompt {prompt}: {str(e)}&quot;)

    return outputs

if __name__ == &quot;__main__&quot;:
    tokenizer = GPT2Tokenizer.from_pretrained(&quot;gpt2&quot;)
    model = GPT2LMHeadModel.from_pretrained(&quot;gpt2&quot;)
    model.to(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)

    example_prompts = [&quot;Hello, how are you?&quot;]
    processed_outputs = process_prompts(example_prompts, model, tokenizer, num_completions=3)
    for output in processed_outputs:
        print(output)
</code></pre>
<p>and also:</p>
<pre class=""lang-py prettyprint-override""><code>            results = text_generator(prompt, max_length=50, num_return_sequences=num_completions)
</code></pre>
<p>When I run this, I get the following error:</p>
<pre><code>The following `model_kwargs` are not used by the model: ['max_len']
Note: I am aware that typos in the generate arguments can also trigger this warning, but I've checked and rechecked the arguments names.
</code></pre>
<p>and</p>
<pre><code>   raise ValueError(
ValueError: Greedy methods without beam search do not support `num_return_sequences` different than 1 (got 4).
</code></pre>
<p>What could be causing this error, and how can I fix it to generate multiple completions effectively using the model?</p>
<p>cross: <a href=""https://discuss.huggingface.co/t/how-to-generate-multiple-text-completions-per-prompt-using-huggingface-transformers-pipeline-without-triggering-an-error/86297"" rel=""nofollow noreferrer"">https://discuss.huggingface.co/t/how-to-generate-multiple-text-completions-per-prompt-using-huggingface-transformers-pipeline-without-triggering-an-error/86297</a></p>
","huggingface"
"78465681","Further Normalization after model training with SpaCy","2024-05-11 18:28:09","","1","37","<spacy><normalization><named-entity-recognition><huggingface>","<p>I trained an Arabic NER with SpaCy and got the model-best folder. But the thing is, the data used for training was already normalized. So, characters such as &quot;أ, ؤ, ة&quot; were already normalized to &quot;ا,ء,ه&quot;, respectively. My goal is to upload the model to HuggingFace where people can test the model using their own texts. But users are going to use the standard format (e.g., &quot;أ, ؤ, ة&quot;) not the normalized version. Is there anyway I can integrate this normalization to the model without having to retrain it, perhaps through editing the config.cfg file?</p>
<p>I tried creating a python script &quot;custom_components.py&quot;</p>
<pre><code>def normalize_characters(doc):
    for token in doc:
        if token.text == &quot;أ&quot;:
            token.text = &quot;ا&quot;
        elif token.text == &quot;إ&quot;:
            token.text = &quot;ا&quot;
        elif token.text == &quot;آ&quot;:
            token.text = &quot;ا&quot;
        elif token.text == &quot;ٱ&quot;:
            token.text = &quot;ا&quot;
        elif token.text == &quot;ى&quot;:
            token.text = &quot;ي&quot;
        elif token.text in &quot;ًٌَُ‘ٍِـ،ْ&quot;:
            token.text = &quot;&quot;
        elif token.text == &quot;ؤ&quot;:
            token.text = &quot;ء&quot;
        elif token.text == &quot;ئ&quot;:
            token.text = &quot;ء&quot;
        elif token.text == &quot;ة&quot;:
            token.text = &quot;ه&quot;
    return doc


</code></pre>
<p>and editing the config.cfg file:</p>
<pre><code>[system]
gpu_allocator = null
seed = 0

[nlp]
lang = &quot;ar&quot;
pipeline = [&quot;tok2vec&quot;,&quot;ner&quot;]
batch_size = 50
disabled = []
before_creation = null
after_creation = null
after_pipeline_creation = null
tokenizer = {&quot;@tokenizers&quot;:&quot;spacy.Tokenizer.v1&quot;}
vectors = {&quot;@vectors&quot;:&quot;spacy.Vectors.v1&quot;}

[components]

[components.custom_normalizer]
factory = &quot;custom_components.normalize_characters&quot;
before_update = &quot;normalize_characters&quot;


[components.ner]
factory = &quot;ner&quot;
incorrect_spans_key = null
moves = null
scorer = {&quot;@scorers&quot;:&quot;spacy.ner_scorer.v1&quot;}
update_with_oracle_cut_size = 100
</code></pre>
<p>But that did not work.</p>
","huggingface"
"78454858","Why is the inference API (serverless) for the custom model from Hugging Face not functioning?","2024-05-09 13:43:35","","0","216","<serverless><huggingface><inference><gemma>","<p>I have pushed the custom model named as <strong>Orcawise/eu-ai-act-align</strong> on <em><strong>hugging face</strong></em> [<em>A popular platform for sharing and discovering natural language processing models and other AI-related resources]</em>.</p>
<p>I have created this model by training <strong>google/gemma-2b</strong> on custom data. Now when I am trying to Use this model with the Inference API (serverless) using the below code</p>
<pre><code>import requests

API_URL = &quot;https://api-inference.huggingface.co/models/Orcawise/eu-ai-act-align&quot;
headers = {&quot;Authorization&quot;: &quot;Bearer xxxxxxxxxxxxxxxxxxxxxxxx&quot;}

def query(payload):
  response = requests.post(API_URL, headers=headers, json=payload)
  return response.json()

output = query({
&quot;inputs&quot;: &quot;Can you please let us know more details about your &quot;,
})
print(&quot;Response from custom model is {}&quot;.format(output))
</code></pre>
<p><strong>I am getting response as below</strong></p>
<pre><code>{'error': 'You are trying to access a gated repo.\nMake sure to request access at https://huggingface.co/google/gemma-2b and pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=&lt;your_token&gt;`.'}
</code></pre>
<p><strong>Solutions tried</strong></p>
<ol>
<li>I went to hugging face account that i have and ask for access from google/gemma-2b and access was granted to me and then again tried to run the above code but with the same error. I am adding the screenshot to show the access being granted. <a href=""https://i.sstatic.net/8MQwuXQT.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/8MQwuXQT.png"" alt=""here"" /></a></li>
<li>I used the pretrained model api url [google/gemma2b] in the above code so that it can first load the google/gemma2b model first and then custom model but no success. Check the below image.
<a href=""https://i.sstatic.net/bZBhZNhU.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bZBhZNhU.png"" alt=""Inference api using base model with custom model"" /></a></li>
<li>I have also asked this question on hugging face community which can be read from <a href=""https://huggingface.co/Orcawise/eu-ai-act-align/discussions/1"" rel=""nofollow noreferrer"">here</a> so that people can revert back here if they solved similar issues.</li>
</ol>
<p><strong>Question</strong> is why this happens when access to pretrained model is already there and what is the best way to use pretrained model with the custom model to use with inference api (serverless). Any ideas are welcomed.</p>
","huggingface"
"78446414","Error Loading ""sentence-transformers/all-MiniLM-L6-v2""","2024-05-08 06:19:37","","0","832","<python><large-language-model><huggingface>","<p>I have built a document question-answering system using llama-2, but while downloading the embedding model, I am getting an OSError.</p>
<p>OSError: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like sentence-transformers/all-MiniLM-L6-v2 is not the path to a directory containing a file named config.json. Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.</p>
","huggingface"
"78443784","TypeError: JsonConfig.init() got an unexpected keyword argument 'tokenizer' in Torchtune Llama-3-8b Fine-tuning with custom dataset","2024-05-07 16:01:35","","0","98","<pytorch><azure-databricks><huggingface><llama><fine-tuning>","<p>I'm attempting to fine-tune a Llama-3-8b model using <strong>Torchtune</strong> with a custom dataset in Alpaca-style JSON format. However, I'm encountering a TypeError when running the fine-tuning process with the command according to the Torchtune documentation.</p>
<p><strong>Error message:</strong>
<code>TypeError: JsonConfig.__init__() got an unexpected keyword argument 'tokenizer'</code></p>
<p><strong>Example Dataset Structure (df_train.json)</strong></p>
<pre><code>{
     'instruction': 'Read the input and generate the summary.',
     'input': 'Here is the details of my passage.',
     'output': 'summarized passage.',
     'text': 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nRead the input and generate the summary.\n\n### Input:\nHere is the details of my passage.\n\n### Response:\nsummarized passage.'
}
</code></pre>
<p><strong>Modified Config File (8b_lora.yaml)</strong></p>
<pre><code>dataset:
  _component_: datasets.load_dataset
  path: json
  data_files: /dbfs/FileStore/amtbds/df_train.json

</code></pre>
<p><strong>Run command for fine-tune</strong></p>
<pre><code>!tune run --nproc_per_node 2 lora_finetune_distributed --config ./8b_lora.yaml
</code></pre>
<p>I've checked the TorchTune and datasets documentation but couldn't find specifics about handling custom JSON with a tokenizer parameter in this context.</p>
<p><strong>Question</strong>: Does anyone have insights on how to correctly configure the YAML file or modify the command to avoid this error?</p>
<p>Any help or pointers towards relevant documentation would be greatly appreciated!</p>
<p>Here is the complete <strong>error log</strong> for better understanding</p>
<pre><code>/databricks/python/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead. _torch_pytree._register_pytree_node( Running with torchrun... [2024-05-07 15:18:32,618] torch.distributed.run: [WARNING] [2024-05-07 15:18:32,618] torch.distributed.run: [WARNING] ***************************************** [2024-05-07 15:18:32,618] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. [2024-05-07 15:18:32,618] torch.distributed.run: [WARNING] ***************************************** /databricks/python/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead. _torch_pytree._register_pytree_node( /databricks/python/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead. _torch_pytree._register_pytree_node( INFO:torchtune.utils.logging:Running LoRAFinetuneRecipeDistributed with resolved config: batch_size: 2 checkpointer: _component_: torchtune.utils.FullModelMetaCheckpointer checkpoint_dir: /tmp/Meta-Llama-3-8B-Instruct/original/ checkpoint_files: - consolidated.00.pth model_type: LLAMA3 output_dir: /tmp/Meta-Llama-3-8B-Instruct/ recipe_checkpoint: null dataset: _component_: datasets.load_dataset data_files: /dbfs/FileStore/amtbds/df_train.json path: json device: cuda dtype: bf16 enable_activation_checkpointing: false epochs: 2 gradient_accumulation_steps: 32 log_every_n_steps: 1 log_peak_memory_stats: false loss: _component_: torch.nn.CrossEntropyLoss lr_scheduler: _component_: torchtune.modules.get_cosine_schedule_with_warmup num_warmup_steps: 100 max_steps_per_epoch: null metric_logger: _component_: torchtune.utils.metric_logging.DiskLogger log_dir: /tmp/lora_finetune_output model: _component_: torchtune.models.llama3.lora_llama3_8b apply_lora_to_mlp: false apply_lora_to_output: false lora_alpha: 16 lora_attn_modules: - q_proj - v_proj lora_rank: 8 optimizer: _component_: torch.optim.AdamW lr: 0.0003 weight_decay: 0.01 output_dir: /tmp/lora_finetune_output resume_from_checkpoint: false seed: null shuffle: true tokenizer: _component_: torchtune.models.llama3.llama3_tokenizer path: /tmp/Meta-Llama-3-8B-Instruct/original/tokenizer.model DEBUG:torchtune.utils.logging:Setting manual seed to local seed 3594840783. Local seed is seed + rank = 3594840783 + 0 DEBUG:torchtune.utils.logging:Setting manual seed to local seed 3594840784. Local seed is seed + rank = 3594840783 + 1 Writing logs to /tmp/lora_finetune_output/log_1715095116.txt INFO:torchtune.utils.logging:FSDP is enabled. Instantiating Model on CPU for Rank 0 ... INFO:torchtune.utils.logging:Model instantiation took 21.42 secs INFO:torchtune.utils.logging:Memory Stats after model init: {'peak_memory_active': 13.327446016, 'peak_memory_alloc': 12.276764672, 'peak_memory_reserved': 14.600372224} INFO:torchtune.utils.logging:Optimizer and loss are initialized. Traceback (most recent call last): File &quot;/local_disk0/.ephemeral_nfs/envs/pythonEnv-f74a28a0-cb29-4f7a-8257-73f238be3d57/lib/python3.11/site-packages/recipes/lora_finetune_distributed.py&quot;, line 615, in &lt;module&gt; sys.exit(recipe_main()) ^^^^^^^^^^^^^ File &quot;/local_disk0/.ephemeral_nfs/envs/pythonEnv-f74a28a0-cb29-4f7a-8257-73f238be3d57/lib/python3.11/site-packages/torchtune/config/_parse.py&quot;, line 50, in wrapper sys.exit(recipe_main(conf)) ^^^^^^^^^^^^^^^^^ File &quot;/local_disk0/.ephemeral_nfs/envs/pythonEnv-f74a28a0-cb29-4f7a-8257-73f238be3d57/lib/python3.11/site-packages/recipes/lora_finetune_distributed.py&quot;, line 609, in recipe_main recipe.setup(cfg=cfg) File &quot;/local_disk0/.ephemeral_nfs/envs/pythonEnv-f74a28a0-cb29-4f7a-8257-73f238be3d57/lib/python3.11/site-packages/recipes/lora_finetune_distributed.py&quot;, line 228, in setup self._sampler, self._dataloader = self._setup_data( ^^^^^^^^^^^^^^^^^ File &quot;/local_disk0/.ephemeral_nfs/envs/pythonEnv-f74a28a0-cb29-4f7a-8257-73f238be3d57/lib/python3.11/site-packages/recipes/lora_finetune_distributed.py&quot;, line 411, in _setup_data ds = config.instantiate(cfg_dataset, tokenizer=self._tokenizer) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &quot;/local_disk0/.ephemeral_nfs/envs/pythonEnv-f74a28a0-cb29-4f7a-8257-73f238be3d57/lib/python3.11/site-packages/torchtune/config/_instantiate.py&quot;, line 106, in instantiate return _instantiate_node(config, *args) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &quot;/local_disk0/.ephemeral_nfs/envs/pythonEnv-f74a28a0-cb29-4f7a-8257-73f238be3d57/lib/python3.11/site-packages/torchtune/config/_instantiate.py&quot;, line 31, in _instantiate_node return _create_component(_component_, args, kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &quot;/local_disk0/.ephemeral_nfs/envs/pythonEnv-f74a28a0-cb29-4f7a-8257-73f238be3d57/lib/python3.11/site-packages/torchtune/config/_instantiate.py&quot;, line 20, in _create_component return _component_(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &quot;/databricks/python/lib/python3.11/site-packages/datasets/load.py&quot;, line 2523, in load_dataset builder_instance = load_dataset_builder( ^^^^^^^^^^^^^^^^^^^^^ Traceback (most recent call last): File &quot;/databricks/python/lib/python3.11/site-packages/datasets/load.py&quot;, line 2232, in load_dataset_builder File &quot;/local_disk0/.ephemeral_nfs/envs/pythonEnv-f74a28a0-cb29-4f7a-8257-73f238be3d57/lib/python3.11/site-packages/recipes/lora_finetune_distributed.py&quot;, line 615, in &lt;module&gt; sys.exit(recipe_main()) ^^^^^^^^^^^^^ File &quot;/local_disk0/.ephemeral_nfs/envs/pythonEnv-f74a28a0-cb29-4f7a-8257-73f238be3d57/lib/python3.11/site-packages/torchtune/config/_parse.py&quot;, line 50, in wrapper sys.exit(recipe_main(conf)) builder_instance: DatasetBuilder = builder_cls( ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ File &quot;/local_disk0/.ephemeral_nfs/envs/pythonEnv-f74a28a0-cb29-4f7a-8257-73f238be3d57/lib/python3.11/site-packages/recipes/lora_finetune_distributed.py&quot;, line 609, in recipe_main ^^^^^^^^^^^^ File &quot;/databricks/python/lib/python3.11/site-packages/datasets/builder.py&quot;, line 371, in __init__ recipe.setup(cfg=cfg) File &quot;/local_disk0/.ephemeral_nfs/envs/pythonEnv-f74a28a0-cb29-4f7a-8257-73f238be3d57/lib/python3.11/site-packages/recipes/lora_finetune_distributed.py&quot;, line 228, in setup self.config, self.config_id = self._create_builder_config( self._sampler, self._dataloader = self._setup_data( ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ File &quot;/databricks/python/lib/python3.11/site-packages/datasets/builder.py&quot;, line 605, in _create_builder_config ^^^^^^^^^^^^^^^^^ File &quot;/local_disk0/.ephemeral_nfs/envs/pythonEnv-f74a28a0-cb29-4f7a-8257-73f238be3d57/lib/python3.11/site-packages/recipes/lora_finetune_distributed.py&quot;, line 411, in _setup_data builder_config = self.BUILDER_CONFIG_CLASS(**config_kwargs) ds = config.instantiate(cfg_dataset, tokenizer=self._tokenizer) ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ^^TypeError^^: ^JsonConfig.__init__() got an unexpected keyword argument 'tokenizer'^ ^^^^^^^^^^^^^^^ File &quot;/local_disk0/.ephemeral_nfs/envs/pythonEnv-f74a28a0-cb29-4f7a-8257-73f238be3d57/lib/python3.11/site-packages/torchtune/config/_instantiate.py&quot;, line 106, in instantiate return _instantiate_node(config, *args) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &quot;/local_disk0/.ephemeral_nfs/envs/pythonEnv-f74a28a0-cb29-4f7a-8257-73f238be3d57/lib/python3.11/site-packages/torchtune/config/_instantiate.py&quot;, line 31, in _instantiate_node return _create_component(_component_, args, kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &quot;/local_disk0/.ephemeral_nfs/envs/pythonEnv-f74a28a0-cb29-4f7a-8257-73f238be3d57/lib/python3.11/site-packages/torchtune/config/_instantiate.py&quot;, line 20, in _create_component return _component_(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &quot;/databricks/python/lib/python3.11/site-packages/datasets/load.py&quot;, line 2523, in load_dataset builder_instance = load_dataset_builder( ^^^^^^^^^^^^^^^^^^^^^ File &quot;/databricks/python/lib/python3.11/site-packages/datasets/load.py&quot;, line 2232, in load_dataset_builder builder_instance: DatasetBuilder = builder_cls( ^^^^^^^^^^^^ File &quot;/databricks/python/lib/python3.11/site-packages/datasets/builder.py&quot;, line 371, in __init__ self.config, self.config_id = self._create_builder_config( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &quot;/databricks/python/lib/python3.11/site-packages/datasets/builder.py&quot;, line 605, in _create_builder_config builder_config = self.BUILDER_CONFIG_CLASS(**config_kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ TypeError: JsonConfig.__init__() got an unexpected keyword argument 'tokenizer' [2024-05-07 15:19:07,624] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 22930) of binary: /local_disk0/.ephemeral_nfs/envs/pythonEnv-f74a28a0-cb29-4f7a-8257-73f238be3d57/bin/python Traceback (most recent call last): File &quot;/local_disk0/.ephemeral_nfs/envs/pythonEnv-f74a28a0-cb29-4f7a-8257-73f238be3d57/bin/tune&quot;, line 8, in &lt;module&gt; sys.exit(main()) ^^^^^^ File &quot;/local_disk0/.ephemeral_nfs/envs/pythonEnv-f74a28a0-cb29-4f7a-8257-73f238be3d57/lib/python3.11/site-packages/torchtune/_cli/tune.py&quot;, line 49, in main parser.run(args) File &quot;/local_disk0/.ephemeral_nfs/envs/pythonEnv-f74a28a0-cb29-4f7a-8257-73f238be3d57/lib/python3.11/site-packages/torchtune/_cli/tune.py&quot;, line 43, in run args.func(args) File &quot;/local_disk0/.ephemeral_nfs/envs/pythonEnv-f74a28a0-cb29-4f7a-8257-73f238be3d57/lib/python3.11/site-packages/torchtune/_cli/run.py&quot;, line 177, in _run_cmd self._run_distributed(args) File &quot;/local_disk0/.ephemeral_nfs/envs/pythonEnv-f74a28a0-cb29-4f7a-8257-73f238be3d57/lib/python3.11/site-packages/torchtune/_cli/run.py&quot;, line 88, in _run_distributed run(args) File &quot;/local_disk0/.ephemeral_nfs/envs/pythonEnv-f74a28a0-cb29-4f7a-8257-73f238be3d57/lib/python3.11/site-packages/torch/distributed/run.py&quot;, line 803, in run elastic_launch( File &quot;/local_disk0/.ephemeral_nfs/envs/pythonEnv-f74a28a0-cb29-4f7a-8257-73f238be3d57/lib/python3.11/site-packages/torch/distributed/launcher/api.py&quot;, line 135, in __call__ return launch_agent(self._config, self._entrypoint, list(args)) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &quot;/local_disk0/.ephemeral_nfs/envs/pythonEnv-f74a28a0-cb29-4f7a-8257-73f238be3d57/lib/python3.11/site-packages/torch/distributed/launcher/api.py&quot;, line 268, in launch_agent raise ChildFailedError( torch.distributed.elastic.multiprocessing.errors.ChildFailedError: ============================================================ /local_disk0/.ephemeral_nfs/envs/pythonEnv-f74a28a0-cb29-4f7a-8257-73f238be3d57/lib/python3.11/site-packages/recipes/lora_finetune_distributed.py FAILED ------------------------------------------------------------ Failures: [1]: time : 2024-05-07_15:19:07 host : 0403-193017-nbhuvffe-10-213-69-14 rank : 1 (local_rank: 1) exitcode : 1 (pid: 22931) error_file: &lt;N/A&gt; traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html ------------------------------------------------------------ Root Cause (first observed failure): [0]: time : 2024-05-07_15:19:07 host : 0403-193017-nbhuvffe-10-213-69-14 rank : 0 (local_rank: 0) exitcode : 1 (pid: 22930) error_file: &lt;N/A&gt; traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html ============================================================
</code></pre>
","huggingface"
"78440139","HuggingFace Transformers generation not deterministic","2024-05-07 04:35:52","","1","97","<pytorch><huggingface-transformers><huggingface>","<p>I have a strange issue when trying to decode from a Lora model I've trained. <code>args.checkpoint</code> is a Lora model trained on top of TinyLlama. When I decode using this, I expect it to give me the same result every time (greedy decoding) but as you can see the results are slightly off:</p>
<pre class=""lang-py prettyprint-override""><code>if __name__ == &quot;__main__&quot;:
    args = get_args()
    model = AutoModelForCausalLM.from_pretrained(args.checkpoint)
    tokenizer = AutoTokenizer.from_pretrained(args.model)
    tokenizer.pad_token = tokenizer.eos_token
    device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
    model = model.to(device)
    p = &quot;Dialogue: #Person1#: This Olympic park is so big!\n#Person2#: Yes. Now we are in the Olympic stadium, the center of this park.\n#Person1#: Splendid! When is it gonna be finished?\n#Person2#: The whole stadium is to be finished this June.\n#Person1#: How many seats are there in the stand?\n#Person2#: Oh, there are 5000 seats in total.\n#Person1#: I didn ' t know it would be so big!\n#Person2#: It is! Look there, those are the tracks. And the jumping pit is over there.\n#Person1#: Ah... I see. Hey, look the sign here, No climbing.\n#Person2#: We put many signs with English translations for foreign visitors.\n\nSummary:&quot;
    inputs = tokenizer(
        [p],
        return_tensors=&quot;pt&quot;,
        return_token_type_ids=False,
    ).to(device)  

    for i in range(1, 10): #repeat 
        response = model.generate(
            **inputs,
            max_new_tokens=32,
            do_sample=False, #greedy
            return_dict_in_generate=True,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.pad_token_id,
        )
        decoded_output = tokenizer.decode(
            response[&quot;sequences&quot;][0],
            skip_special_tokens=True,
        )  
        res = decoded_output[len(p) :]  # remove prompt from output
        print(f&quot;try {i}: {res}&quot;)
</code></pre>
<p>The result is (please scroll to the right):</p>
<pre><code>try 1:  #Person1# and #Person2# are talking about the Olympic stadium. #Person1# thinks it is so big.
try 2:  #Person1# and #Person2# are talking about the Olympic stadium. #Person1# thinks it is so big.
try 3:  #Person1# and #Person2# are talking about the Olympic stadium. #Person1# thinks it is so big.
try 4:  #Person1# and #Person2# are talking about the Olympic stadium. #Person1# thinks it is so big.
try 5:  #Person1# and #Person2# are talking about the Olympic stadium. #Person1# thinks it is so big.
try 6:  #Person1# and #Person2# are talking about the Olympic stadium. #Person1# thinks it is so big.
try 7:  #Person1# and #Person2# are talking about the Olympic stadium. #Person2# tells #Person1# that the Olympic stadium is
try 8:  #Person1# and #Person2# are talking about the Olympic stadium. #Person1# thinks it is so big.
try 9:  #Person1# and #Person2# are talking about the Olympic stadium. #Person2# tells #Person1# that the Olympic stadium is
</code></pre>
<p>If I wrap the generation code with <code>autocast</code> to lower precision the results are more varied:</p>
<pre><code>try 1:  #Person1# and #Person2# are talking about the Olympic stadium. #Person1# is very excited about it.
try 2:  #Person1# and #Person2# are talking about the Olympic stadium. #Person1# is very excited about the Olympic stadium.
try 3:  #Person1# and #Person2# are talking about the Olympic stadium. #Person1# is very excited about it.
try 4:  #Person1# and #Person2# are talking about the Olympic stadium. #Person1# is very excited about it.
try 5:  #Person1# and #Person2# are talking about the Olympic stadium. #Person1# is very excited about the Olympic stadium.
try 6:  #Person1# and #Person2# are talking about the Olympic stadium. #Person1# is very excited about the Olympic stadium.
try 7:  #Person1# and #Person2# are talking about the Olympic stadium. #Person1# is very excited about the Olympic stadium.
try 8:  #Person1# and #Person2# are talking about the Olympic stadium. #Person1# is very excited about the Olympic stadium.
try 9:  #Person1# and #Person2# are talking about the Olympic Park. #Person1# is very excited about the Olympic Park.
</code></pre>
<p>Is this expected? How can I ensure deterministic decoding? TIA</p>
","huggingface"
"78437953","How can I solve this problem in ChatGLM-6b? AttributeError: 'ChatGLMTokenizer' object has no attribute 'sp_tokenizer'","2024-05-06 16:31:26","","0","466","<huggingface-transformers><attributeerror><glm><huggingface><huggingface-tokenizers>","<p>The default program from &quot;https://github.com/THUDM/ChatGLM-6B&quot; works out while I was running api.py.
But recently when I try to run it again it suddenly errors:&quot;AttributeError: 'ChatGLMTokenizer' object has no attribute 'sp_tokenizer'&quot;.
I tried to solve it on my own but nothing happens. I reinstalled transformers==4.27.1, updated tokenization_chatglm.py on <a href=""https://huggingface.co/THUDM/chatglm-6b/blob/main/tokenization_chatglm.py"" rel=""nofollow noreferrer"">https://huggingface.co/THUDM/chatglm-6b/blob/main/tokenization_chatglm.py</a>, etc.
I cannot understand how tokenization_chatglm.py or api.py works for they're too complex for me.
I wonder if there is anything I haven't done to solve my problem and if there is anyone else having the same problem. Please someone help me out.</p>
<p>the default api.py down here</p>
<pre class=""lang-py prettyprint-override""><code>from fastapi import FastAPI, Request
from transformers import AutoTokenizer, AutoModel
import uvicorn, json, datetime
import torch

DEVICE = &quot;cuda&quot;
DEVICE_ID = &quot;0&quot;
CUDA_DEVICE = f&quot;{DEVICE}:{DEVICE_ID}&quot; if DEVICE_ID else DEVICE


def torch_gc():
    if torch.cuda.is_available():
        with torch.cuda.device(CUDA_DEVICE):
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()


app = FastAPI()


@app.post(&quot;/&quot;)
async def create_item(request: Request):
    global model, tokenizer
    json_post_raw = await request.json()
    json_post = json.dumps(json_post_raw)
    json_post_list = json.loads(json_post)
    prompt = json_post_list.get('prompt')
    history = json_post_list.get('history')
    max_length = json_post_list.get('max_length')
    top_p = json_post_list.get('top_p')
    temperature = json_post_list.get('temperature')
    response, history = model.chat(tokenizer,
                                   prompt,
                                   history=history,
                                   max_length=max_length if max_length else 2048,
                                   top_p=top_p if top_p else 0.7,
                                   temperature=temperature if temperature else 0.95)
    now = datetime.datetime.now()
    time = now.strftime(&quot;%Y-%m-%d %H:%M:%S&quot;)
    answer = {
        &quot;response&quot;: response,
        &quot;history&quot;: history,
        &quot;status&quot;: 200,
        &quot;time&quot;: time
    }
    log = &quot;[&quot; + time + &quot;] &quot; + '&quot;, prompt:&quot;' + prompt + '&quot;, response:&quot;' + repr(response) + '&quot;'
    print(log)
    torch_gc()
    return answer


if __name__ == '__main__':
    model_path = r&quot;E:\huggingface\hub\models_THUDM_chatglm_6b&quot;
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    model = AutoModel.from_pretrained(model_path, trust_remote_code=True, resume_download=True).quantize(4).half().cuda()

    model.eval()
    uvicorn.run(app, host='0.0.0.0', port=8000, workers=1)

</code></pre>
<p>the newest tokenization_chatglm.py down here</p>
<pre class=""lang-py prettyprint-override""><code>&quot;&quot;&quot;Tokenization classes for ChatGLM.&quot;&quot;&quot;
from typing import List, Optional, Union
import os

from transformers.tokenization_utils import PreTrainedTokenizer
from transformers.utils import logging, PaddingStrategy
from transformers.tokenization_utils_base import EncodedInput, BatchEncoding
from typing import Dict
import sentencepiece as spm
import numpy as np

logger = logging.get_logger(__name__)

PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {
    &quot;THUDM/chatglm-6b&quot;: 2048,
}


class TextTokenizer:
    def __init__(self, model_path):
        self.sp = spm.SentencePieceProcessor()
        self.sp.Load(model_path)
        self.num_tokens = self.sp.vocab_size()

    def encode(self, text):
        return self.sp.EncodeAsIds(text)

    def decode(self, ids: List[int]):
        return self.sp.DecodeIds(ids)

    def tokenize(self, text):
        return self.sp.EncodeAsPieces(text)

    def convert_tokens_to_string(self, tokens):
        return self.sp.DecodePieces(tokens)

    def convert_tokens_to_ids(self, tokens):
        return [self.sp.PieceToId(token) for token in tokens]

    def convert_token_to_id(self, token):
        return self.sp.PieceToId(token)

    def convert_id_to_token(self, idx):
        return self.sp.IdToPiece(idx)

    def __len__(self):
        return self.num_tokens


class SPTokenizer:
    def __init__(
            self,
            vocab_file,
            num_image_tokens=20000,
            max_blank_length=80,
            byte_fallback=True,
    ):
        assert vocab_file is not None
        self.vocab_file = vocab_file
        self.num_image_tokens = num_image_tokens
        self.special_tokens = [&quot;[MASK]&quot;, &quot;[gMASK]&quot;, &quot;[sMASK]&quot;, &quot;&lt;unused_0&gt;&quot;, &quot;&lt;sop&gt;&quot;, &quot;&lt;eop&gt;&quot;, &quot;&lt;ENC&gt;&quot;, &quot;&lt;dBLOCK&gt;&quot;]
        self.max_blank_length = max_blank_length
        self.byte_fallback = byte_fallback
        self.text_tokenizer = TextTokenizer(vocab_file)

    def _get_text_tokenizer(self):
        return self.text_tokenizer

    @staticmethod
    def get_blank_token(length: int):
        assert length &gt;= 2
        return f&quot;&lt;|blank_{length}|&gt;&quot;

    @staticmethod
    def get_tab_token():
        return f&quot;&lt;|tab|&gt;&quot;

    @property
    def num_text_tokens(self):
        return self.text_tokenizer.num_tokens

    @property
    def num_tokens(self):
        return self.num_image_tokens + self.num_text_tokens

    @staticmethod
    def _encode_whitespaces(text: str, max_len: int = 80):
        text = text.replace(&quot;\t&quot;, SPTokenizer.get_tab_token())
        for i in range(max_len, 1, -1):
            text = text.replace(&quot; &quot; * i, SPTokenizer.get_blank_token(i))
        return text

    def _preprocess(self, text: str, linebreak=True, whitespaces=True):
        if linebreak:
            text = text.replace(&quot;\n&quot;, &quot;&lt;n&gt;&quot;)
        if whitespaces:
            text = self._encode_whitespaces(text, max_len=self.max_blank_length)
        return text

    def encode(
            self, text: str, linebreak=True, whitespaces=True, add_dummy_prefix=True
    ) -&gt; List[int]:
        &quot;&quot;&quot;
        @param text: Text to encode.
        @param linebreak: Whether to encode newline (\n) in text.
        @param whitespaces: Whether to encode multiple whitespaces or tab in text, useful for source code encoding.
        @param special_tokens: Whether to encode special token ([MASK], [gMASK], etc.) in text.
        @param add_dummy_prefix: Whether to add dummy blank space in the beginning.
        &quot;&quot;&quot;
        text = self._preprocess(text, linebreak, whitespaces)
        if not add_dummy_prefix:
            text = &quot;&lt;n&gt;&quot; + text
        tmp = self._get_text_tokenizer().encode(text)
        tokens = [x + self.num_image_tokens for x in tmp]
        return tokens if add_dummy_prefix else tokens[2:]

    def postprocess(self, text):
        text = text.replace(&quot;&lt;n&gt;&quot;, &quot;\n&quot;)
        text = text.replace(SPTokenizer.get_tab_token(), &quot;\t&quot;)
        for i in range(2, self.max_blank_length + 1):
            text = text.replace(self.get_blank_token(i), &quot; &quot; * i)
        return text

    def decode(self, text_ids: List[int]) -&gt; str:
        ids = [int(_id) - self.num_image_tokens for _id in text_ids]
        ids = [_id for _id in ids if _id &gt;= 0]
        text = self._get_text_tokenizer().decode(ids)
        text = self.postprocess(text)
        return text

    def decode_tokens(self, tokens: List[str]) -&gt; str:
        text = self._get_text_tokenizer().convert_tokens_to_string(tokens)
        text = self.postprocess(text)
        return text

    def tokenize(
            self, text: str, linebreak=True, whitespaces=True, add_dummy_prefix=True
    ) -&gt; List[str]:
        &quot;&quot;&quot;
        @param text: Text to encode.
        @param linebreak: Whether to encode newline (\n) in text.
        @param whitespaces: Whether to encode multiple whitespaces or tab in text, useful for source code encoding.
        @param special_tokens: Whether to encode special token ([MASK], [gMASK], etc.) in text.
        @param add_dummy_prefix: Whether to add dummy blank space in the beginning.
        &quot;&quot;&quot;
        text = self._preprocess(text, linebreak, whitespaces)
        if not add_dummy_prefix:
            text = &quot;&lt;n&gt;&quot; + text
        tokens = self._get_text_tokenizer().tokenize(text)
        return tokens if add_dummy_prefix else tokens[2:]

    def __getitem__(self, x: Union[int, str]):
        if isinstance(x, int):
            if x &lt; self.num_image_tokens:
                return &quot;&lt;image_{}&gt;&quot;.format(x)
            else:
                return self.text_tokenizer.convert_id_to_token(x - self.num_image_tokens)
        elif isinstance(x, str):
            if x.startswith(&quot;&lt;image_&quot;) and x.endswith(&quot;&gt;&quot;) and x[7:-1].isdigit():
                return int(x[7:-1])
            else:
                return self.text_tokenizer.convert_token_to_id(x) + self.num_image_tokens
        else:
            raise ValueError(&quot;The key should be str or int.&quot;)


class ChatGLMTokenizer(PreTrainedTokenizer):
    &quot;&quot;&quot;
    Construct a ChatGLM tokenizer. Based on byte-level Byte-Pair-Encoding.

    Args:
        vocab_file (`str`):
            Path to the vocabulary file.
    &quot;&quot;&quot;

    vocab_files_names = {&quot;vocab_file&quot;: &quot;ice_text.model&quot;}
    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES
    model_input_names = [&quot;input_ids&quot;, &quot;attention_mask&quot;, &quot;position_ids&quot;]

    def __init__(
            self,
            vocab_file,
            do_lower_case=False,
            remove_space=False,
            bos_token='&lt;sop&gt;',
            eos_token='&lt;eop&gt;',
            end_token='&lt;/s&gt;',
            mask_token='[MASK]',
            gmask_token='[gMASK]',
            padding_side=&quot;left&quot;,
            pad_token=&quot;&lt;pad&gt;&quot;,
            unk_token=&quot;&lt;unk&gt;&quot;,
            num_image_tokens=20000,
            **kwargs
    ) -&gt; None:
        super().__init__(
            do_lower_case=do_lower_case,
            remove_space=remove_space,
            padding_side=padding_side,
            bos_token=bos_token,
            eos_token=eos_token,
            end_token=end_token,
            mask_token=mask_token,
            gmask_token=gmask_token,
            pad_token=pad_token,
            unk_token=unk_token,
            num_image_tokens=num_image_tokens,
            **kwargs
        )

        self.do_lower_case = do_lower_case
        self.remove_space = remove_space
        self.vocab_file = vocab_file

        self.bos_token = bos_token
        self.eos_token = eos_token
        self.end_token = end_token
        self.mask_token = mask_token
        self.gmask_token = gmask_token

        self.sp_tokenizer = SPTokenizer(vocab_file, num_image_tokens=num_image_tokens)

        &quot;&quot;&quot; Initialisation &quot;&quot;&quot;

    @property
    def gmask_token_id(self) -&gt; Optional[int]:
        if self.gmask_token is None:
            return None
        return self.convert_tokens_to_ids(self.gmask_token)

    @property
    def end_token_id(self) -&gt; Optional[int]:
        &quot;&quot;&quot;
        `Optional[int]`: Id of the end of context token in the vocabulary. Returns `None` if the token has not been
        set.
        &quot;&quot;&quot;
        if self.end_token is None:
            return None
        return self.convert_tokens_to_ids(self.end_token)

    @property
    def vocab_size(self):
        &quot;&quot;&quot; Returns vocab size &quot;&quot;&quot;
        return self.sp_tokenizer.num_tokens

    def get_vocab(self):
        &quot;&quot;&quot; Returns vocab as a dict &quot;&quot;&quot;
        vocab = {self._convert_id_to_token(i): i for i in range(self.vocab_size)}
        vocab.update(self.added_tokens_encoder)
        return vocab

    def preprocess_text(self, inputs):
        if self.remove_space:
            outputs = &quot; &quot;.join(inputs.strip().split())
        else:
            outputs = inputs

        if self.do_lower_case:
            outputs = outputs.lower()

        return outputs

    def _tokenize(self, text, **kwargs):
        &quot;&quot;&quot; Returns a tokenized string. &quot;&quot;&quot;
        text = self.preprocess_text(text)

        seq = self.sp_tokenizer.tokenize(text)

        return seq

    def convert_tokens_to_string(self, tokens: List[str]) -&gt; str:
        return self.sp_tokenizer.decode_tokens(tokens)

    def _decode(
            self,
            token_ids: Union[int, List[int]],
            **kwargs
    ) -&gt; str:
        if isinstance(token_ids, int):
            token_ids = [token_ids]
        if len(token_ids) == 0:
            return &quot;&quot;
        if self.pad_token_id in token_ids:  # remove pad
            token_ids = list(filter((self.pad_token_id).__ne__, token_ids))
        return super()._decode(token_ids, **kwargs)

    def _convert_token_to_id(self, token):
        &quot;&quot;&quot; Converts a token (str) in an id using the vocab. &quot;&quot;&quot;
        return self.sp_tokenizer[token]

    def _convert_id_to_token(self, index):
        &quot;&quot;&quot;Converts an index (integer) in a token (str) using the vocab.&quot;&quot;&quot;
        return self.sp_tokenizer[index]

    def save_vocabulary(self, save_directory, filename_prefix=None):
        &quot;&quot;&quot;
        Save the vocabulary and special tokens file to a directory.

        Args:
            save_directory (`str`):
                The directory in which to save the vocabulary.
            filename_prefix (`str`, *optional*):
                An optional prefix to add to the named of the saved files.

        Returns:
            `Tuple(str)`: Paths to the files saved.
        &quot;&quot;&quot;
        if os.path.isdir(save_directory):
            vocab_file = os.path.join(
                save_directory, self.vocab_files_names[&quot;vocab_file&quot;]
            )
        else:
            vocab_file = save_directory

        with open(self.vocab_file, 'rb') as fin:
            proto_str = fin.read()

        with open(vocab_file, &quot;wb&quot;) as writer:
            writer.write(proto_str)

        return (vocab_file,)

    def build_inputs_with_special_tokens(
            self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None
    ) -&gt; List[int]:
        &quot;&quot;&quot;
        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
        adding special tokens. A BERT sequence has the following format:

        - single sequence: `[CLS] X [SEP]`
        - pair of sequences: `[CLS] A [SEP] B [SEP]`

        Args:
            token_ids_0 (`List[int]`):
                List of IDs to which the special tokens will be added.
            token_ids_1 (`List[int]`, *optional*):
                Optional second list of IDs for sequence pairs.

        Returns:
            `List[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.
        &quot;&quot;&quot;
        gmask_id = self.sp_tokenizer[self.gmask_token]
        eos_id = self.sp_tokenizer[self.eos_token]
        token_ids_0 = token_ids_0 + [gmask_id, self.sp_tokenizer[self.bos_token]]
        if token_ids_1 is not None:
            token_ids_0 = token_ids_0 + token_ids_1 + [eos_id]
        return token_ids_0

    def _pad(
            self,
            encoded_inputs: Union[Dict[str, EncodedInput], BatchEncoding],
            max_length: Optional[int] = None,
            padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,
            pad_to_multiple_of: Optional[int] = None,
            return_attention_mask: Optional[bool] = None,
    ) -&gt; dict:
        &quot;&quot;&quot;
        Pad encoded inputs (on left/right and up to predefined length or max length in the batch)

        Args:
            encoded_inputs:
                Dictionary of tokenized inputs (`List[int]`) or batch of tokenized inputs (`List[List[int]]`).
            max_length: maximum length of the returned list and optionally padding length (see below).
                Will truncate by taking into account the special tokens.
            padding_strategy: PaddingStrategy to use for padding.

                - PaddingStrategy.LONGEST Pad to the longest sequence in the batch
                - PaddingStrategy.MAX_LENGTH: Pad to the max length (default)
                - PaddingStrategy.DO_NOT_PAD: Do not pad
                The tokenizer padding sides are defined in self.padding_side:

                    - 'left': pads on the left of the sequences
                    - 'right': pads on the right of the sequences
            pad_to_multiple_of: (optional) Integer if set will pad the sequence to a multiple of the provided value.
                This is especially useful to enable the use of Tensor Core on NVIDIA hardware with compute capability
                `&gt;= 7.5` (Volta).
            return_attention_mask:
                (optional) Set to False to avoid returning attention mask (default: set to model specifics)
        &quot;&quot;&quot;
        # Load from model defaults
        bos_token_id = self.sp_tokenizer[self.bos_token]
        mask_token_id = self.sp_tokenizer[self.mask_token]
        gmask_token_id = self.sp_tokenizer[self.gmask_token]
        assert self.padding_side == &quot;left&quot;

        required_input = encoded_inputs[self.model_input_names[0]]
        seq_length = len(required_input)

        if padding_strategy == PaddingStrategy.LONGEST:
            max_length = len(required_input)

        if max_length is not None and pad_to_multiple_of is not None and (max_length % pad_to_multiple_of != 0):
            max_length = ((max_length // pad_to_multiple_of) + 1) * pad_to_multiple_of

        needs_to_be_padded = padding_strategy != PaddingStrategy.DO_NOT_PAD and len(required_input) != max_length

        # Initialize attention mask if not present.
        if max_length is not None:
            if &quot;attention_mask&quot; not in encoded_inputs:
                if bos_token_id in required_input:
                    context_length = required_input.index(bos_token_id)
                else:
                    context_length = seq_length
                attention_mask = np.ones((1, seq_length, seq_length))
                attention_mask = np.tril(attention_mask)
                attention_mask[:, :, :context_length] = 1
                attention_mask = np.bool_(attention_mask &lt; 0.5)
                encoded_inputs[&quot;attention_mask&quot;] = attention_mask

            if &quot;position_ids&quot; not in encoded_inputs:
                if bos_token_id in required_input:
                    context_length = required_input.index(bos_token_id)
                else:
                    context_length = seq_length
                position_ids = np.arange(seq_length, dtype=np.int64)
                mask_token = mask_token_id if mask_token_id in required_input else gmask_token_id
                if mask_token in required_input:
                    mask_position = required_input.index(mask_token)
                    position_ids[context_length:] = mask_position
                block_position_ids = np.concatenate(
                    [np.zeros(context_length, dtype=np.int64),
                     np.arange(1, seq_length - context_length + 1, dtype=np.int64)])
                encoded_inputs[&quot;position_ids&quot;] = np.stack([position_ids, block_position_ids], axis=0)

        if needs_to_be_padded:
            difference = max_length - len(required_input)

            if &quot;attention_mask&quot; in encoded_inputs:
                encoded_inputs[&quot;attention_mask&quot;] = np.pad(encoded_inputs[&quot;attention_mask&quot;],
                                                          pad_width=[(0, 0), (difference, 0), (difference, 0)],
                                                          mode='constant', constant_values=True)
            if &quot;token_type_ids&quot; in encoded_inputs:
                encoded_inputs[&quot;token_type_ids&quot;] = [self.pad_token_type_id] * difference + encoded_inputs[
                    &quot;token_type_ids&quot;
                ]
            if &quot;special_tokens_mask&quot; in encoded_inputs:
                encoded_inputs[&quot;special_tokens_mask&quot;] = [1] * difference + encoded_inputs[&quot;special_tokens_mask&quot;]
            if &quot;position_ids&quot; in encoded_inputs:
                encoded_inputs[&quot;position_ids&quot;] = np.pad(encoded_inputs[&quot;position_ids&quot;],
                                                        pad_width=[(0, 0), (difference, 0)])
            encoded_inputs[self.model_input_names[0]] = [self.pad_token_id] * difference + required_input

        return encoded_inputs
</code></pre>
<p>I reinstalled transformers==4.27.1, updated tokenization_chatglm.py on <a href=""https://huggingface.co/THUDM/chatglm-6b/blob/main/tokenization_chatglm.py"" rel=""nofollow noreferrer"">https://huggingface.co/THUDM/chatglm-6b/blob/main/tokenization_chatglm.py</a>, etc.
Hoping api.py runs successfully so I can use local ChatGLM offline.</p>
","huggingface"
"78421310","""hf-internal-testing/tiny-stable-diffusion-pipe"" from Diffusers seems broken","2024-05-02 19:17:04","","0","43","<huggingface><stable-diffusion>","<p>I am trying to run the small tiny-stable-diffusion-pipe model from HuggingFace but it looks like it's broken and won't sample properly.</p>
<pre><code>import os
import torch
from diffusers import StableDiffusionPipeline
from diffusers import DiffusionPipeline

pipe = StableDiffusionPipeline.from_pretrained(&quot;hf-internal- 
testing/tiny-stable-diffusion-pipe&quot;, torch_dtype=torch.float16)
pipe = pipe.to(&quot;cuda&quot;)
prompt = &quot;a picture of a rabbit&quot;
images = pipe(prompt, num_images_per_prompt=1).images
</code></pre>
<p>gives</p>
<p><code>RuntimeError: The size of tensor a (12545) must match the size of tensor b (226) at non-singleton dimension 1</code></p>
<p>Try it yourself in this google colab:</p>
<p><a href=""https://colab.research.google.com/drive/1AE5uA68myGvyzEi-vm4w806EN9rdjzq4?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1AE5uA68myGvyzEi-vm4w806EN9rdjzq4?usp=sharing</a></p>
<p>I was expecting the model to sample when using</p>
<p>images = pipe(prompt, num_images_per_prompt=1).images</p>
","huggingface"
"78420756","Huggingfacae TGI ERROR text_generation_launcher: Shard 0 failed to start","2024-05-02 17:17:06","","0","179","<docker><deployment><huggingface-transformers><huggingface><inference>","<p>2024-05-02T17:06:59.807336Z  INFO download: text_generation_launcher: Successfully downloaded weights.
2024-05-02T17:06:59.807762Z  INFO shard-manager: text_generation_launcher: Starting shard rank=0
2024-05-02T17:07:02.879107Z  WARN text_generation_launcher: Could not import Flash Attention enabled models: CUDA is not available</p>
<p>2024-05-02T17:07:03.812736Z ERROR shard-manager: text_generation_launcher: Shard complete standard error output:</p>
<p>The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.
Traceback (most recent call last):</p>
<p>File &quot;/opt/conda/bin/text-generation-server&quot;, line 8, in 
sys.exit(app())</p>
<p>File &quot;/opt/conda/lib/python3.10/site-packages/text_generation_server/cli.py&quot;, line 71, in serve
from text_generation_server import server</p>
<p>File &quot;/opt/conda/lib/python3.10/site-packages/text_generation_server/server.py&quot;, line 17, in 
from text_generation_server.models.vlm_causal_lm import VlmCausalLMBatch</p>
<p>File &quot;/opt/conda/lib/python3.10/site-packages/text_generation_server/models/vlm_causal_lm.py&quot;, line 14, in 
from text_generation_server.models.flash_mistral import (</p>
<p>File &quot;/opt/conda/lib/python3.10/site-packages/text_generation_server/models/flash_mistral.py&quot;, line 18, in 
from text_generation_server.models.custom_modeling.flash_mistral_modeling import (</p>
<p>File &quot;/opt/conda/lib/python3.10/site-packages/text_generation_server/models/custom_modeling/flash_mistral_modeling.py&quot;, line 29, in 
from text_generation_server.utils import paged_attention, flash_attn</p>
<p>File &quot;/opt/conda/lib/python3.10/site-packages/text_generation_server/utils/flash_attn.py&quot;, line 24, in 
raise ImportError(&quot;CUDA is not available&quot;)</p>
<p>ImportError: CUDA is not available
rank=0
2024-05-02T17:07:03.910103Z ERROR text_generation_launcher: Shard 0 failed to start
2024-05-02T17:07:03.910129Z  INFO text_generation_launcher: Shutting down shards
Error: ShardCannotStart</p>
<p>NVIDIA-SMI 470.239.06   Driver Version: 470.239.06   CUDA Version: 11.4</p>
<p>cuda is Installed nvidia-smi , nvcc --version matches bitsnbytes verion is also comaptible
pip install bitsandbytes-cuda114
Any suggestions appreciated?</p>
","huggingface"
"78413651","Trying to convert a DETR model to Tensorflow Lite","2024-05-01 12:09:14","","0","90","<machine-learning><object-detection><huggingface-transformers><tensorflow-lite><huggingface>","<p>I'm trying to convert a DETR hugging face model to a Tensorflow Lite model using Optimum for an android app. I installed the requirements:</p>
<pre><code>!pip install optimum
!pip install optimum[exporters-tf]
</code></pre>
<p>then I tried an example converting a pretrained model and it works, but on my model:</p>
<pre><code>!optimum-cli export tflite --model /content/drive/MyDrive/Modele/model-litter --sequence_length 128 --task object-detection Model_Litter/
</code></pre>
<p>an error occurs about Transformer module about not having an attribute named TFAutoModelForObjectDetection</p>
<pre><code>2024-05-01 11:53:55.895865: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-05-01 11:54:01.347392: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Traceback (most recent call last):
  File &quot;/usr/lib/python3.10/runpy.py&quot;, line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File &quot;/usr/lib/python3.10/runpy.py&quot;, line 86, in _run_code
    exec(code, run_globals)
  File &quot;/usr/local/lib/python3.10/dist-packages/optimum/exporters/tflite/__main__.py&quot;, line 148, in &lt;module&gt;
    main()
  File &quot;/usr/local/lib/python3.10/dist-packages/optimum/exporters/tflite/__main__.py&quot;, line 61, in main
    model = TasksManager.get_model_from_task(
  File &quot;/usr/local/lib/python3.10/dist-packages/optimum/exporters/tasks.py&quot;, line 1905, in get_model_from_task
    model_class = TasksManager.get_model_class_for_task(
  File &quot;/usr/local/lib/python3.10/dist-packages/optimum/exporters/tasks.py&quot;, line 1375, in get_model_class_for_task
    return getattr(loaded_library, model_class_name)
  File &quot;/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py&quot;, line 1503, in __getattr__
    raise AttributeError(f&quot;module {self.__name__} has no attribute {name}&quot;)
AttributeError: module transformers has no attribute TFAutoModelForObjectDetection
Traceback (most recent call last):
  File &quot;/usr/local/bin/optimum-cli&quot;, line 8, in &lt;module&gt;
    sys.exit(main())
  File &quot;/usr/local/lib/python3.10/dist-packages/optimum/commands/optimum_cli.py&quot;, line 163, in main
    service.run()
  File &quot;/usr/local/lib/python3.10/dist-packages/optimum/commands/export/tflite.py&quot;, line 243, in run
    subprocess.run(full_command, shell=True, check=True)
  File &quot;/usr/lib/python3.10/subprocess.py&quot;, line 526, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command 'python3 -m optimum.exporters.tflite --model /content/drive/MyDrive/Modele/model-litter --sequence_length 128 --task object-detection Model_Litter/' returned non-zero exit status 1
</code></pre>
<p>I saw on some forums the error might happen because I haven't updated the Transformer module, but even after I updated the module the error still occurs.</p>
","huggingface"
"78411025","how to solve ""TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'""","2024-04-30 21:05:54","","0","395","<python><huggingface-transformers><huggingface><huggingface-tokenizers>","<p>I tried to run this code:</p>
<pre><code>EN_AR = load_dataset(&quot;iwslt2017&quot;, &quot;iwslt2017-ar-en&quot;, split=&quot;train&quot;).select(range(2000))

def extract_languages(examples):
  inputs = [ex[&quot;ar&quot;] for ex in examples['translation']]
  target = [ex[&quot;en&quot;] for ex in examples['translation']]
  return {&quot;inputs&quot;:inputs,&quot;targets&quot;:target}

EN_AR = EN_AR.map(extract_languages,batched=True, remove_columns=[&quot;translation&quot;])

from transformers import AutoTokenizer, MBart50TokenizerFast

model_name = &quot;facebook/mbart-large-50&quot;
tokenizer = AutoTokenizer.from_pretrained(model_name)
maxL = 128
def preprocess_func(examples):
  model_inputs = tokenizer(examples[&quot;inputs&quot;],max_length=maxL,truncation=True)

  with tokenizer.as_target_tokenizer():
    labels = tokenizer(examples[&quot;targets&quot;],max_length=maxL,truncation=True)

  model_inputs[&quot;labels&quot;]= labels[&quot;input_ids&quot;]
  return model_name

tokenized_datasets = EN_AR.map(preprocess_func, batched = True, remove_columns=[&quot;inputs&quot;,&quot;targets&quot;])
</code></pre>
<p>but it keeps telling me</p>
<blockquote>
<hr />
<p>TypeError                                 Traceback (most recent call last)
 in &lt;cell line: 15&gt;()
13   return model_name
14
---&gt; 15 tokenized_datasets = EN_AR.map(preprocess_func, batched = True, remove_columns=&gt;&gt;&gt;[&quot;inputs&quot;,&quot;targets&quot;])</p>
<p>10 frames
/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_fast.py in &gt;convert_ids_to_tokens(self, ids, skip_special_tokens)
387         tokens = []
388         for index in ids:
--&gt; 389             index = int(index)
390             if skip_special_tokens and index in self.all_special_ids:
391                 continue</p>
<p>TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'</p>
</blockquote>
<p>I took this code from the Hugging Face website for translation preprocessing, but I don't know why it does not work for me</p>
","huggingface"
"78410889","i'm having bitsandbytes error while using unsloth in conda","2024-04-30 20:29:38","","0","531","<python><nvidia><large-language-model><huggingface>","<p>I'm trying to finetune llama3 model from Unsloth uding the code presented in one of there Colab notebooks, but I'm having several issues while running the code on my system.</p>
<p>Following is the error I'm having. I'm using a conda Virtual Enviornment
I'm following the tutorial on <a href=""https://github.com/unslothai/unsloth"" rel=""nofollow noreferrer"">github</a></p>
<pre><code>(unsloth_env) llm@llm:/mnt/ssd/unsloth$ python3 run_unsloth.py
WARNING: BNB_CUDA_VERSION=121 environment variable detected; loading libbitsandbytes_cuda121_nocublaslt121.so.
This can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.
If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=
If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH
For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:&lt;path_to_cuda_dir/lib64

Could not find the bitsandbytes CUDA binary at PosixPath('/home/llm/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda121_nocublaslt121.so')
The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.
/home/llm/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/unsloth/__init__.py:72: UserWarning: Unsloth: Running `ldconfig /usr/lib64-nvidia` to link CUDA.
  warnings.warn(
/sbin/ldconfig.real: Can't create temporary cache file /etc/ld.so.cache~: Permission denied
/home/llm/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/unsloth/__init__.py:103: UserWarning: Unsloth: CUDA is not linked properly.
Try running `python -m bitsandbytes` then `python -m xformers.info`
We tried running `ldconfig /usr/lib64-nvidia` ourselves, but it didn't work.
You need to run in your terminal `sudo ldconfig /usr/lib64-nvidia` yourself, then import Unsloth.
Also try `sudo ldconfig /usr/local/cuda-xx.x` - find the latest cuda version.
Unsloth will still run for now, but maybe it might crash - let's hope it works!
  warnings.warn(
Traceback (most recent call last):
  File &quot;/mnt/ssd/unsloth/run_unsloth.py&quot;, line 2, in &lt;module&gt;
    import unsloth
  File &quot;/home/llm/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/unsloth/__init__.py&quot;, line 113, in &lt;module&gt;
    from .models import *
  File &quot;/home/llm/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/unsloth/models/__init__.py&quot;, line 15, in &lt;module&gt;
    from .loader import FastLanguageModel
  File &quot;/home/llm/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/unsloth/models/loader.py&quot;, line 15, in &lt;module&gt;
    from .llama import FastLlamaModel, logger
  File &quot;/home/llm/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/unsloth/models/llama.py&quot;, line 26, in &lt;module&gt;
    from ..kernels import *
  File &quot;/home/llm/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/unsloth/kernels/__init__.py&quot;, line 15, in &lt;module&gt;
    from .cross_entropy_loss import fast_cross_entropy_loss
  File &quot;/home/llm/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/unsloth/kernels/cross_entropy_loss.py&quot;, line 18, in &lt;module&gt;
    from .utils import calculate_settings, MAX_FUSED_SIZE
  File &quot;/home/llm/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/unsloth/kernels/utils.py&quot;, line 36, in &lt;module&gt;
    cdequantize_blockwise_fp32      = bnb.functional.lib.cdequantize_blockwise_fp32
  File &quot;/home/llm/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/bitsandbytes/cextension.py&quot;, line 73, in __getattr__
    return getattr(self._lib, item)
  File &quot;/home/llm/miniconda3/envs/unsloth_env/lib/python3.10/ctypes/__init__.py&quot;, line 387, in __getattr__
    func = self.__getitem__(name)
  File &quot;/home/llm/miniconda3/envs/unsloth_env/lib/python3.10/ctypes/__init__.py&quot;, line 392, in __getitem__
    func = self._FuncPtr((name_or_ordinal, self))
AttributeError: /home/llm/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cdequantize_blockwise_fp32

</code></pre>
<p>I tried matching the Cuda and torch versions but did not work</p>
","huggingface"
"78407821","forward(__torch__.transformers.models.bert.modeling_bert.BertModel self, Tensor input_ids) -> ((Tensor, Tensor))","2024-04-30 10:15:58","","0","19","<model><opensearch><huggingface><torchscript>","<p>I am working with opensearch and i want to implement a custom model from hugging face i followed there steps on how to save and zip the model, everything works fine until i try to deploy the model and then i get this error:</p>
<pre><code>opensearch-node1       | Caused by: ai.djl.engine.EngineException: forward() Expected a value of type 'Tensor' for argument 'input_ids' but instead found type 'Dict[str, Tensor]'.
opensearch-node1       | Position: 1
opensearch-node1       | Declaration: forward(__torch__.transformers.models.bert.modeling_bert.BertModel self, Tensor input_ids) -&gt; ((Tensor, Tensor))
opensearch-node1       |    at ai.djl.pytorch.jni.PyTorchLibrary.moduleRunMethod(Native Method) ~[?:?]
opensearch-node1       |    at ai.djl.pytorch.jni.IValueUtils.forward(IValueUtils.java:53) ~[?:?]
opensearch-node1       |    at ai.djl.pytorch.engine.PtSymbolBlock.forwardInternal(PtSymbolBlock.java:145) ~[?:?]
opensearch-node1       |    at ai.djl.nn.AbstractBaseBlock.forward(AbstractBaseBlock.java:79) ~[?:?]
opensearch-node1       |    at ai.djl.nn.Block.forward(Block.java:127) ~[?:?]
opensearch-node1       |    at ai.djl.inference.Predictor.predictInternal(Predictor.java:140) ~[?:?]
opensearch-node1       |    at ai.djl.inference.Predictor.batchPredict(Predictor.java:180) ~[?:?]
opensearch-node1       |    at ai.djl.inference.Predictor.predict(Predictor.java:126) ~[?:?]
opensearch-node1       |    at org.opensearch.ml.engine.algorithms.TextEmbeddingModel.warmUp(TextEmbeddingModel.java:53) ~[?:?]
opensearch-node1       |    at org.opensearch.ml.engine.algorithms.DLModel.doLoadModel(DLModel.java:218) ~[?:?]
opensearch-node1       |    at org.opensearch.ml.engine.algorithms.DLModel.lambda$loadModel$1(DLModel.java:275) ~[?:?]
opensearch-node1       |    ... 14 more
opensearch-node1       | [2024-04-26T14:12:13,789][INFO ][o.o.m.a.d.TransportDeployModelOnNodeAction] [opensearch-node1] deploy model task done VAm-Go8BP7uFuwEn3GFP

</code></pre>
<p>here is the script i use to save the model;
open search requires torchsrip format zipped with the model tokenizer.</p>
<pre><code>from transformers import BertModel, BertTokenizer, BertConfig
import torch
import transformers

enc = BertTokenizer.from_pretrained(&quot;sentence-transformers/LaBSE&quot;)

# Tokenizing input text
text = &quot;[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]&quot;
tokenized_text = enc.tokenize(text)
tokenizer = transformers.AutoTokenizer.from_pretrained(&quot;sentence-transformers/LaBSE&quot;)
tokenizer.save_pretrained(&quot;./second_attemot/tokenizer&quot;)
# # Masking one of the input tokens
masked_index = 8
tokenized_text[masked_index] = &quot;[MASK]&quot;
indexed_tokens = enc.convert_tokens_to_ids(tokenized_text)
segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]

# # Creating a dummy input
tokens_tensor = torch.tensor([indexed_tokens])
segments_tensors = torch.tensor([segments_ids])
dummy_input = [tokens_tensor, segments_tensors]

# # Initializing the model with the torchscript flag
# # Flag set to True even though it is not necessary as this model does not have an LM Head.
config = BertConfig(
    vocab_size_or_config_json_file=32000,
    hidden_size=768,
    num_hidden_layers=12,
    num_attention_heads=12,
    intermediate_size=3072,
    torchscript=True,
)

# # Instantiating the model
model = BertModel(config)

# # The model needs to be in evaluation mode
model.eval()
print(tokens_tensor.size())
print(segments_tensors.size())

# Ensure the number of segments matches the number of [SEP] tokens
num_sep_tokens = indexed_tokens.count(enc.sep_token_id)
segments_ids = [0] * num_sep_tokens + [1] * (len(indexed_tokens) - num_sep_tokens)

# Creating a dummy input
tokens_tensor = torch.tensor([indexed_tokens])
segments_tensors = torch.tensor([segments_ids])
dummy_input = [tokens_tensor, segments_tensors]

# # If you are instantiating the model with *from_pretrained* you can also easily set the TorchScript flag
model = BertModel.from_pretrained(&quot;sentence-transformers/LaBSE&quot;, torchscript=True)

# # Creating the trace
traced_model = torch.jit.trace(model, tokens_tensor)
torch.jit.save(traced_model, &quot;./second_attemot/LaBSE.pt&quot;)
</code></pre>
","huggingface"
"78407292","Speeding up zero-shot headline categorization with BART on Huggingface","2024-04-30 08:40:17","","1","39","<python><artificial-intelligence><classification><text-classification><huggingface>","<p>I’m working on a Flask web app that needs to categorize 300 headlines into 9-16 dynamic categories every hour very quickly. I'm using the <a href=""https://huggingface.co/facebook/bart-large-mnli"" rel=""nofollow noreferrer"">Facebook BART model via Huggingface's API</a>. My current implementation in Python takes 3-5 seconds per headline, which is too slow. Ideally I'd like to do the whole list in 3-5 seconds. Is that possible?</p>
<pre><code>def categorise(categories, item):
    API_URL = &quot;https://api-inference.huggingface.co/models/facebook/bart-large-mnli&quot;
    headers = {&quot;Authorization&quot;: &quot;Bearer &lt;token&gt;&quot;}
    payload = {
        &quot;inputs&quot;: item,
        &quot;parameters&quot;: {&quot;candidate_labels&quot;: categories},
    }
    response = requests.post(API_URL, headers=headers, json=payload)
    if response.status_code == 200:
        results = response.json()
        return {&quot;category&quot;: results[0]['labels'][0], &quot;confidence&quot;: results[0]['scores'][0]}
    else:
        return {&quot;error&quot;: f&quot;API request failed with status {response.status_code}&quot;}
</code></pre>
<p>I iterate through each headline in a for loop and call this function. Is a way to speed this up much much faster?</p>
<p>I tried using the Huggingface API as I thought BART would be an advanced rapid way to handle categorisation - but it is quite slow.</p>
","huggingface"
"78402555","How to upload your fine-tuned model on HuggingFace and use it as an API? Or Should i use GCP?","2024-04-29 11:58:00","","0","169","<google-cloud-platform><large-language-model><huggingface>","<p>Goal: I am trying to upload my finetuned model on huggingface, and then I want to use it as an API for my Android and IOS application, but I am not sure about the process. I also have tried to look around on GCP, it seems they have models like Llama 2 etc but i have not seen any option to upload my model and use it as an API.</p>
","huggingface"
"78397030","Why the model.generate is so slow?","2024-04-28 03:56:55","","0","457","<huggingface-transformers><large-language-model><huggingface>","<p><a href=""https://i.sstatic.net/OlQ2d0B1.png"" rel=""nofollow noreferrer"">enter image description here</a>I am using one A100 to infer.The model was trained by using the alignment handbook frame and fine tuning by qlora. Then I load the model to infer and Faced with low utilization of graphics card performance and slow inference speed.</p>
<p>I load the model and tokenizer by the following code</p>
<pre class=""lang-py prettyprint-override""><code>device = torch.device('cuda:1')
model = AutoModelForCausalLM.from_pretrained(path_model).to(device)
tokenizer = AutoTokenizer.from_pretrained(path_model)
</code></pre>
<p>My inference code</p>
<pre><code>with torch.no_grad():
        for d in tqdm(data):
            print(input_path)
            tokenized_chat = tokenizer.apply_chat_template([d[0]],return_tensors='pt',add_generation_prompt=True).to(device)
            attention_mask = torch.ones(tokenized_chat.shape,dtype=torch.long).to(device)
            input_length = tokenized_chat.shape[1]
            outputs = model.generate(tokenized_chat,attention_mask=attention_mask,max_new_tokens = 500,)
            outputs = tokenizer.decode(outputs[0][input_length:])
</code></pre>
<p>Please help me, Thanks~</p>
<p>There is a interesting phenomena that at the beginning of the inference, the speed was fast.<a href=""https://i.sstatic.net/CURNmgxr.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
","huggingface"
"78396685","What are some good cybersecurity models on HuggingFace that I can customize and use with a custom agent?","2024-04-27 23:49:22","","0","41","<huggingface-transformers><langchain><agent><huggingface><langchain-agents>","<p>I've been having trouble setting up my agent because the tools that I try using don't have the necessary maping to use with the custom agent.</p>
<pre><code>`\`OSError                                   Traceback (most recent call last)
Cell In\[10\], line 4
1 from langchain.agents import load_huggingface_tool
\----\&gt; 4 tool = load_huggingface_tool(&quot;elftsdmr/malware-url-detect&quot;)
6 print(f&quot;{tool.name}: {tool.description}&quot;)


554 except ImportError:
555     raise ImportError(
556         &quot;HuggingFace tools require the libraries `transformers&gt;=4.29.0`&quot;
557         &quot; and `huggingface_hub&gt;=0.14.1` to be installed.&quot;
558         &quot; Please install it with&quot;
559         &quot; `pip install --upgrade transformers huggingface_hub`.&quot;
560     )
\--\&gt; 561 hf_tool = load_tool(
562     task_or_repo_id,
563     model_repo_id=model_repo_id,
564     token=token,
565     remote=remote,
566     \*\*kwargs,
567 )
568 outputs = hf_tool.outputs
569 if set(outputs) != {&quot;text&quot;}:
...
267         )
268     custom_tool = config\[&quot;custom_tool&quot;\]
269 else:

OSError: elftsdmr/malware-url-detect does not provide a mapping to custom tools in its configuration `config.json`.
Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...\`
</code></pre>
<p>`
This is the error that I keep getting.</p>
<p>Do you guys know of any models on HuggingFace that I can use without running into issues?</p>
","huggingface"
"78393687","Where to set the scheduler and sampler?","2024-04-27 03:03:14","","1","41","<python><deep-learning><huggingface><stable-diffusion>","<p>If I'm trying to build an inferencer using Python and the huggingface library, where do I set the scheduler and sampler? What function do I call? For context, this is what my code looks like:</p>
<pre><code>import torch, PIL, random

from typing import List, Optional, Union
from diffusers import StableDiffusionInpaintPipeline

device = &quot;cuda&quot;
# pipeline = StableDiffusionXLPipeline.from_single_file(&quot;/content/models/juggernautXL_version2.safetensors&quot;, torch_dtype=torch.float16, use_safetensors=True, safety_checker=None ).to(&quot;cuda&quot;)
model_path = &quot;runwayml/stable-diffusion-inpainting&quot;

pipe = StableDiffusionInpaintPipeline.from_pretrained(model_path,torch_dtype=torch.float16,).to(device)

def image_grid(imgs, rows, cols):
    assert len(imgs) == rows*cols

    w, h = imgs[0].size
    grid = PIL.Image.new('RGB', size=(cols*w, rows*h))
    grid_w, grid_h = grid.size

    for i, img in enumerate(imgs):
        grid.paste(img, box=(i%cols*w, i//cols*h))
    return grid


def present_img(url):
    return PIL.Image.open(url)

mask_url = &quot;masks/mask_PXL_20240419_181351038.MP.jpg.png&quot;
img_url = &quot;originals/PXL_20240419_181351038.MP.jpg&quot;

image = present_img(img_url).resize((512, 512))
mask_image = present_img(mask_url).resize((512, 512))

prompt = &quot;car on a desert highway. Detailed. High resolution. Photorealistic. Soft light.&quot;

guidance_scale=7.5
num_samples = 3
generator = torch.Generator(device=&quot;cuda&quot;).manual_seed(random.randint(0,1000)) # change the seed to get different results

# Assuming 'image' and 'mask_image' are predefined Image objects
images = pipe(
    prompt=prompt,
    image=image,
    mask_image=mask_image,
    guidance_scale=guidance_scale,
    generator=generator,
    num_images_per_prompt=num_samples,
).images

images.insert(0, image)

for idx, img in enumerate(images):
    img.save(f&quot;output/output_image_{idx}.png&quot;)
</code></pre>
","huggingface"
"78390001","ORPOTrainer Error: Calculated loss must be on the original device: cuda:0 but device in use is cuda:3","2024-04-26 11:06:16","","0","300","<python><huggingface-transformers><large-language-model><huggingface><huggingface-trainer>","<p>I am trying to train Phi3 with an ORPO dataset using the ORPOTrainer from the HuggingFace Transformers library. My machine has 4 GPUs, so I would like to start multi-GPU training.
This is my ORPOCONFIG:</p>
<pre><code>orpo_args = ORPOConfig(
    learning_rate=0.00003,
    beta=0.1,
    lr_scheduler_type=&quot;linear&quot;,
    max_length=2048,
    max_prompt_length=2048,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=16,
    gradient_accumulation_steps=4,
    optim=&quot;paged_adamw_8bit&quot;,
    num_train_epochs=3,
    evaluation_strategy=&quot;steps&quot;,
    eval_steps=200,
    bf16=True,
    logging_steps=1,
    save_steps=500,
    warmup_steps=100,
    report_to=&quot;wandb&quot;,
    output_dir=&quot;./results/&quot;,
    remove_unused_columns=False,
    dataset_num_proc=os.cpu_count(),

)
</code></pre>
<p>and this is the trainer:</p>
<pre><code>trainer = ORPOTrainer(
    model=model,
    args=orpo_args,
    train_dataset=formatted_orpo_dataset[&quot;train&quot;],
    eval_dataset=formatted_orpo_dataset[&quot;test&quot;],
    peft_config=peft_config,
    tokenizer=tokenizer,

)
</code></pre>
<p>The model was downloaded with 'device' set to 'auto', but I am getting this error here when trainer starts: &quot;Calculated loss must be on the original device: cuda:0 but device in use is cuda:3&quot;</p>
<p>Has anyone else encountered this issue and resolved it?</p>
<p>Thank you.</p>
<p>I tried to start ORPOTrainer but i have this error: &quot;Calculated loss must be on the original device: cuda:0 but device in use is cuda:3&quot;.</p>
","huggingface"
"78383391","Making an inference call to HuggingFace in Semantic Kernel causes 404 not found error","2024-04-25 08:50:28","78550342","0","178","<large-language-model><huggingface><semantic-kernel>","<p>I can make serverless inference API calls to the models hosted in HuggingFace using request calls in Python.</p>
<p>I want to achieve the same task using Semantic Kernel in C#.</p>
<p>For this purpose, I import <code>Microsoft.SemanticKernel.Connectors.HuggingFace;</code> and write the following code:</p>
<pre><code>IKernelBuilder builder = Kernel.CreateBuilder();

builder.Services.AddHuggingFaceTextGeneration(
        model: &quot;meta-llama/Meta-Llama-3-70B-Instruct&quot;,
        apiKey: &quot;&lt;&lt;my huggingface API goes here&gt;&gt;&quot;,
        endpoint: new Uri(&quot;https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3-70B-Instruct&quot;)
);

Kernel kernel = builder.Build();
Task&lt;string?&gt; result = kernel.InvokePromptAsync&lt;string&gt;(&quot;What is the capital of Turkey&quot;);
Console.WriteLine(result.Result);
</code></pre>
<p>However, I receive the following error.</p>
<blockquote>
<p>HttpRequestException: Response status code does not indicate success: 404 (Not Found).</p>
</blockquote>
<p>Can someone help for solving this issue?</p>
","huggingface"
"78379101","Poetry Cannot install bitsandbytes 0.43.1 in python 3.11","2024-04-24 14:11:22","","1","298","<python-poetry><huggingface>","<p>When I try poetry adding bitsandbytes, i get:</p>
<pre><code>  - Installing bitsandbytes (0.43.1): Failed

  RuntimeError

  Unable to find installation candidates for bitsandbytes (0.43.1)

  at ~/Library/Application Support/pypoetry/venv/lib/python3.11/site-packages/poetry/installation/chooser.py:74 in choose_for
       70│
       71│             links.append(link)
       72│
       73│         if not links:
    →  74│             raise RuntimeError(f&quot;Unable to find installation candidates for {package}&quot;)
       75│
       76│         # Get the best link
       77│         chosen = max(links, key=lambda link: self._sort_key(package, link))
       78│

Cannot install bitsandbytes.
</code></pre>
<p>I am not sure how to get around this when installing this package using bitsandbytes. I tried accessing the poetry shell then doing:</p>
<pre><code>pip install bitsandbytes
</code></pre>
<p>which picks up version <code>0.42.01</code>, but it does not seem to find <code>0.43.1</code> while poetry add picks up this version which is what I need.</p>
","huggingface"
"78377299","KeyError: 'base_model.model.model.layers.0.mlp.down_proj.lora_A.weight'","2024-04-24 09:21:07","","0","487","<error-handling><huggingface><llama><fine-tuning><mistral-7b>","<p>I finetune mistral7b and I try to load it my finetune model into my application but I keep getting this mistake:<a href=""https://i.sstatic.net/3UOJv.png"" rel=""nofollow noreferrer"">error</a></p>
<p>do anyone know where it come from, my finetuning work well ( with axolotl project and pet.py), a month ago everything works well with this config</p>
<p>My config for finetuning :</p>
<pre><code>base_model: mistralai/Mistral-7B-v0.1
model_type: MistralForCausalLM
tokenizer_type: LlamaTokenizer
is_mistral_derived_model: true
hub_model_id: DevSelego/Mistral7b_summarizer_v5

load_in_8bit: false
load_in_4bit: true
strict: false
datasets:
  - path: DevSelego/jobmaker_summarize_v5
    type: alpaca
dataset_prepared_path: last_run_prepared
val_set_size: 0.05
output_dir: ./qlora-out

adapter: qlora
lora_model_dir:

sequence_len: 8192
sample_packing: false
pad_to_sequence_len: true

lora_r: 32
lora_alpha: 16
lora_dropout: 0.05
lora_target_linear: true
lora_fan_in_fan_out:
lora_target_modules:
  - gate_proj
  - down_proj
  - up_proj
  - q_proj
  - v_proj
  - k_proj
  - o_proj

wandb_project: Mistral_Jobmaker
wandb_entity:
wandb_watch:
wandb_name:
wandb_log_model:

gradient_accumulation_steps: 1
micro_batch_size: 10
num_epochs: 1
optimizer: adamw_bnb_8bit
lr_scheduler: cosine
learning_rate: 0.0002

train_on_inputs: false
group_by_length: false
bf16: true
fp16: false
tf32: false

gradient_checkpointing: true
early_stopping_patience:
resume_from_checkpoint:
local_rank:
logging_steps: 1
xformers_attention:
flash_attention: true

loss_watchdog_threshold: 5.0
loss_watchdog_patience: 3

warmup_steps: 10
eval_steps: 0.05
eval_table_size:
eval_table_max_new_tokens: 128
save_steps:
debug:
deepspeed:
weight_decay: 0.0
fsdp:
fsdp_config:
special_tokens:
  bos_token: &quot;&lt;s&gt;&quot;
  eos_token: &quot;&lt;/s&gt;&quot;
  unk_token: &quot;&lt;unk&gt;&quot;

</code></pre>
<p>Thanks for your help</p>
<p>help to debug finetuning</p>
","huggingface"
"78370567","I am not getting source documents from RetrievalQA.from_chain_type when I have deployed it in azure while I am getting it locally","2024-04-23 07:27:56","","1","114","<python-3.x><artificial-intelligence><langchain><huggingface><azure-openai>","<p><strong>Query returned from azure:</strong></p>
<blockquote>
<p>{'query': 'Please provide rationale.. ', 'result': 'Creative Output:
The rationale behind the process is to ensure that all actions taken
are based on sound reasoning and logic, and are aligned with the
overall objectives and goals of the organization. This helps to
minimize risks, optimize resources, and improve performance.',
'source_documents': []}</p>
</blockquote>
<p><strong>Query returned locally:</strong></p>
<blockquote>
<p>{'query': 'Please provide rationale.. ', 'result': 'Extracted Output:
Could you please provide more context or specify which process or
policy you are referring to? Without additional information, it is
difficult to provide a concise response.', 'source_documents':
[Document(page_content='Please provide evidence that this occurred in
the last twelve months.', metadata={'source_page': 'Third Party-Vendor
Audit Process Version 378'}), Document(page_content='Please provide
evidence that this occurred in the last twelve months.',
metadata={'source_page': 'Third Party-Vendor Audit Process Version
371'}), Document(page_content='Please Provide evidence such as emails
or other notifications showing periodic access reviews.',
metadata={'source_page': 'Third Party-Vendor Audit Process Version
352'}), Document(page_content='Please provide evidence that this has
occurred within the last twelve months.', metadata={'source_page':
'Third Party-Vendor Audit Process Version 331'})]}</p>
</blockquote>
<p><strong>For azure I am saving the vectordatabase in github&gt; i have created a directory called vdb_langchain_small in Github while for local I have created the same folder locally. I am getting two different outputs.</strong></p>
<p>I am using the same code for both i.e</p>
<pre><code> PROMPT=PromptTemplate(template=Prompt_Template,input_variables=[&quot;context&quot;,&quot;question&quot;])
 chain_type_kwargs={&quot;prompt&quot;:PROMPT}
 qa_chain = RetrievalQA.from_chain_type(llm=self.llm_open,
                                            chain_type=&quot;stuff&quot;,
                                            retriever=self.retriever,
                                            return_source_documents=True,
                                            chain_type_kwargs=chain_type_kwargs,
                                            verbose=True)
 qa_results = {}
</code></pre>
<p><strong>I was expecting the query to return list of source documents along instead I got none for all the answers I got.</strong></p>
","huggingface"
"78370187","Llama-3 generated text on Hugging Face dedicated endpoint is garbled mess","2024-04-23 06:11:09","","0","136","<huggingface><llama>","<p>I'm calling a dedicated inference endpoint.</p>
<p>In Postman I'm passing the following JSON:</p>
<pre><code>{
  &quot;inputs&quot;: &quot;Who is Elon Musk?&quot;
}
</code></pre>
<p>But the output is garbled. What am I doing wrong? Is the input not meant to be <code>inputs</code>?</p>
<p>Output:</p>
<pre><code>[
    {
        &quot;generated_text&quot;: &quot;Who is Elon Musk?Edited by Kelly Bowen\nDiscover the incredible story of the man behind SpaceX, the Boring Company and Tesla. How did he become a billionaire? And, how did he build from being a self-taught coder in his mother’s house to becoming the most famous entrepreneur on the planet? \n1. Elon Musk, college dropout-turned-billionaireTesla, The Boring Company, and SpaceX about jessicaderosa about rahuldesai about magicboutique\n2.&quot;
    }
]
</code></pre>
<p>Or</p>
<pre><code>[
    {
        &quot;generated_text&quot;: &quot;Who is Elon Musk? Or, as this quiz is framed: ‘Who is Musk?’ For anyone who didn’t know the answer, though, good luck. As brilliant as Musk may be, we wouldn’t say the man is a genius. The quiz posits Musk as the man behind Tesla, SpaceX, Paypal, Hyperloop, SolarCity. Just to name the biggest.\nSomeone else was/is behind PayPal – Jordan River and Jeremy Stoppelman. And the Hyperloop, which hasn’t existed in anything but theoretical form&quot;
    }
]
</code></pre>
<p><img src=""https://cdn-uploads.huggingface.co/production/uploads/6623d8174395651309555ae5/m9hgRhXjRZH5bG2V9IBhP.png"" alt=""Screenshot 2024-04-23 at 1.50.43 pm.png"" /></p>
","huggingface"
"78368172","Running finetuned inference on CPU - accelerate ImportError","2024-04-22 18:14:16","","0","129","<huggingface-transformers><huggingface><huggingface-trainer>","<p>I have successfully finetuned gemma-2b for TextClassification using LORA and merged model by using <code>merge_and_unload()</code>.</p>
<p>I then saved it to my local path using <code>model.save_pretrained(f&quot;{LOCAL_MODEL_PATH}&quot;, safe_serialization=False)</code>.
This was done on a GPU machine.</p>
<p>I am trying to load the above model on a CPU only device for inference using the following script</p>
<pre><code>model = AutoModelForSequenceClassification.from_pretrained(LOCAL_MODEL_PATH, num_labels=2)
</code></pre>
<p>However, I see the issue</p>
<pre><code>ImportError: Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`
</code></pre>
<p>I do have accelerate installed and following are the libraries I installed before loading the model</p>
<pre><code>tokenizers==0.15.2
transformers==4.39.3
torch==2.2.2
bitsandbytes==0.43.0
accelerate==0.28.0
peft==0.10.0
</code></pre>
<p>I see the same error even if I run the commands on a linux terminal.</p>
<p>Would like to get some help in resolving the issue and running the model on a CPU only machine for inference.</p>
","huggingface"
"78359367","RuntimeError: User specified an unsupported autocast device_type 'mps'","2024-04-20 18:50:32","","0","726","<huggingface><accelerate><metal-performance-shaders>","<p>I am trying to run the <code>distil-whisper</code> training code on my M1 Mac and I face this issue while running the code. It is executed using <code>huggingface</code> <code>accelerate</code> library and I am running into this error every time</p>
<blockquote>
<p>RuntimeError: User specified an unsupported autocast device_type 'mps'</p>
</blockquote>
<p>and the error comes from file <code>python3.10/site-packages/torch/amp/autocast_mode.py</code>, here i don't see any 'mps' device, but why is that?</p>
<p><a href=""https://i.sstatic.net/NDzu9.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/NDzu9.png"" alt=""enter image description here"" /></a></p>
<p>This is the config of accelerate in <code>/huggingface/accelerate/default_config.yaml</code></p>
<pre><code>compute_environment: LOCAL_MACHINE
debug: false
distributed_type: 'NO'
downcast_bf16: 'no'
machine_rank: 0
main_training_function: main
mixed_precision: 'no'
num_machines: 1
num_processes: 1
rdzv_backend: static
same_network: true
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: false
</code></pre>
","huggingface"
"78357392","Error in merging adapter with model in autotrain","2024-04-20 07:38:52","","0","104","<machine-learning><huggingface>","<p>I am trying to use autotrain from Hugging Face to fine tune some models. Since I don't have big computing resources, I tried to fine tune model <a href=""https://huggingface.co/EleutherAI/pythia-14m"" rel=""nofollow noreferrer"">EleutherAI/pythia-14m</a> and <a href=""https://huggingface.co/datasets/rishiraj/guanaco-style-metamath"" rel=""nofollow noreferrer"">this dataset</a>. But I recieved this message:</p>
<pre><code>Failed to merge adapter weights: Error(s) in loading state_dict for PeftModelForCausalLM:
    size mismatch for base_model.model.gpt_neox.embed_in.weight: copying a param with shape torch.Size([50280, 128]) from checkpoint, the shape in current model is torch.Size([50277, 128]).
    size mismatch for base_model.model.embed_out.weight: copying a param with shape torch.Size([50280, 128]) from checkpoint, the shape in current model is torch.Size([50277, 128]).
</code></pre>
<p>This error occured when I started this script, that is just terminal command, written in Jupiter.</p>
<pre><code>!autotrain llm \
    --train \
    --model &quot;EleutherAI/pythia-14m&quot; \
    --project-name &quot;my-llm&quot; \
    --data-path data/ \
    --text-column text \
    --batch-size &quot;4&quot; \
    --lr &quot;2e-5&quot; \
    --epochs &quot;3&quot; \
    --block-size &quot;1024&quot; \
    --warmup-ratio &quot;0.03&quot; \
    --lora-r &quot;16&quot; \
    --lora-alpha &quot;32&quot; \
    --lora-dropout &quot;0.05&quot; \
    --weight-decay &quot;0.&quot; \
    --gradient-accumulation &quot;4&quot; \
    --logging-steps &quot;10&quot; \
    --use-peft \
    --merge-adapter \
</code></pre>
<p>Also, the same problem showed, when I tried to autotrain in Hugging Face space.
I am not experienced in ml, so I can't imagine, what can cause this problem.</p>
","huggingface"
"78355789","Llama2-7b Long runtime to generate text from a prompt","2024-04-19 19:18:13","","0","163","<machine-learning><huggingface-transformers><huggingface><llama>","<p>I have loaded the Llama2-7b model from HuggingFace on to my computer to generate text based off of a simple prompt. The model loads pretty quickly (around 2 min) but then when I want to generate a really simple response from a simple prompt, such as &quot;What is an LLM?&quot; It takes the model over two hours to generate a response. I need this to be running faster but don't know what is wrong to make it run faster. Any suggestions?</p>
<p>Here is my Code:</p>
<pre><code>class LlamaInference():
    &quot;&quot;&quot;Text Generation class using Llama 70b param LLM. Generates a dataset.&quot;&quot;&quot;
    def __init__(self, output_file_path):
        &quot;&quot;&quot; Initialize LlamaInference class and all variables
        Keyword arguments:
        output_file_path -- Location of output file after dataset generation
        &quot;&quot;&quot;
        self.output_file_path = output_file_path
        self.pipeline = None
        self.output_list = []
        self.output_list_condensed = []

    def __set_up(self):
        &quot;&quot;&quot; Set up huggingface pipeline with specific model and revision
        &quot;&quot;&quot;
        #model = &quot;meta-llama/Llama-2-70b-chat-hf&quot;
        #revision = &quot;e6152b720bd3cd67afc66e36d06893a0e1f84b48&quot;
        model = &quot;meta-llama/Llama-2-7b-chat-hf&quot;
        revision = &quot;08751db2aca9bf2f7f80d2e516117a53d7450235&quot;


    
        self.tokenizer = AutoTokenizer.from_pretrained(model, padding_side=&quot;left&quot;)

        self.pipeline = transformers.pipeline(
            &quot;text-generation&quot;,
            model=model,
            tokenizer=self.tokenizer,
            torch_dtype=torch.float16,
            device_map=&quot;auto&quot;,
            revision=revision,
            do_sample=True,
            return_full_text=False
        )
        self.pipeline.tokenizer.pad_token_id = self.tokenizer.eos_token_id


    def gen_text(self, prompt, **kwargs):
        &quot;&quot;&quot; Generates text based on a prompt

        Keyword arguments:
        prompt -- the question we are asking the llm
        **kwargs -- arguments to be passed to self.pipeline 
        &quot;&quot;&quot;
        try:
            if self.pipeline is None:
                self.__set_up()
            if &quot;batch_size&quot; not in kwargs:
                kwargs[&quot;batch_size&quot;] = 1
            
            if &quot;max_new_tokens&quot; not in kwargs:
                kwargs[&quot;max_new_tokens&quot;] = 2048

            kwargs.update(
                {
                    &quot;pad_token_id&quot;: self.pipeline.tokenizer.eos_token_id, 
                    &quot;eos_token_id&quot;: self.pipeline.tokenizer.eos_token_id,
                }
            )
            display('started pipeline')
            token_outputs = self.tokenizer(prompt)
            display(token_outputs)
            outputs = self.pipeline(prompt, **kwargs)
            display('end pipeline')
            return outputs
        except Exception as error:
            display(f'__gen_text error: {error}')

inference = LlamaInference('test.json')
result = inference.gen_text(&quot;Hello&quot;, max_new_tokens=2048, batch_size=1, temperature=0.5)
</code></pre>
","huggingface"
"78354052","Trying to deploy a Custom Model into OpenSearch throws a RuntimeError: KeyError: token_type_ids","2024-04-19 13:36:18","","0","58","<machine-learning><artificial-intelligence><large-language-model><opensearch><huggingface>","<p>For a specific use case, I had to deploy a custom model into OpenSearch, First of the all, I exported a HuggingFace Model into TorchScript and had registered successfully then I tried to deploy it but unfortunately deploy operation failed and returns the following error :</p>
<pre><code>    {
  &quot;model_id&quot;: &quot;Guem7I4BM4PGcXAkFYKV&quot;,
  &quot;task_type&quot;: &quot;DEPLOY_MODEL&quot;,
  &quot;function_name&quot;: &quot;TEXT_EMBEDDING&quot;,
  &quot;state&quot;: &quot;FAILED&quot;,
  &quot;worker_node&quot;: [
    &quot;d7zdcxaASDyFKGX7AhlG0g&quot;,
    &quot;5fc7S9yJQQCWarLbAnVESg&quot;,
    &quot;TPu-1KznRzCwj8tQAgHOMQ&quot;
  ],
  &quot;create_time&quot;: 1713367889874,
  &quot;last_update_time&quot;: 1713368035088,
  &quot;error&quot;: &quot;&quot;&quot;
                          {&quot;d7zdcxaASDyFKGX7AhlG0g&quot;:&quot;The following operation failed in the TorchScript interpreter.
                          \nTraceback of TorchScript, serialized code (most recent call last):
                          \n  File \&quot;code/__torch__.py\&quot;, line 12, in forward
                          \n    input_ids = inputs[\&quot;input_ids\&quot;]
                          \n    attention_mask = inputs[\&quot;attention_mask\&quot;]
                          \n    input = inputs[\&quot;token_type_ids\&quot;]
                          \n            ~~~~~~~~~~~~~~~~~~~~~~~~ &lt;--- HERE
                          \n    _0 = (model).forward(input_ids, attention_mask, input, )
                          \n    return {\&quot;sentence_embedding\&quot;: _0}
                          \n
                          \nTraceback of TorchScript, original code (most recent call last):
                          \n/Users/Library/Python/3.9/lib/python/site-packages/torch/jit/_trace.py(1074): trace_module
                          \n/Users/Library/Python/3.9/lib/python/site-packages/torch/jit/_trace.py(806): trace
                          \n/Users/opensearch/CustomModel/TEST/import_torch.py(43): export_to_torchscript
                          \n/Users/opensearch/CustomModel/TEST/import_torch.py(52): &lt;module&gt;
                          \nRuntimeError: KeyError: token_type_ids
                          \n&quot;,&quot;5fc7S9yJQQCWarLbAnVESg&quot;:&quot;The following operation failed in the TorchScript interpreter.
                          \nTraceback of TorchScript, serialized code (most recent call last):
                          \n  File \&quot;code/__torch__.py\&quot;, line 12, in forward
                          \n    input_ids = inputs[\&quot;input_ids\&quot;]
                          \n    attention_mask = inputs[\&quot;attention_mask\&quot;]
                          \n    input = inputs[\&quot;token_type_ids\&quot;]
                          \n            ~~~~~~~~~~~~~~~~~~~~~~~~ &lt;--- HERE
                          \n    _0 = (model).forward(input_ids, attention_mask, input, )
                          \n    return {\&quot;sentence_embedding\&quot;: _0}
                          \n
                          \nTraceback of TorchScript, original code (most recent call last):
                          \n/Users/Library/Python/3.9/lib/python/site-packages/torch/jit/_trace.py(1074): trace_module
                          \n/Users/Library/Python/3.9/lib/python/site-packages/torch/jit/_trace.py(806): trace
                          \n/Users/opensearch/CustomModel/TEST/import_torch.py(43): export_to_torchscript
                          \n/Users/opensearch/CustomModel/TEST/import_torch.py(52): &lt;module&gt;
                          \nRuntimeError: KeyError: token_type_ids
                          \n&quot;,&quot;TPu-1KznRzCwj8tQAgHOMQ&quot;:&quot;The following operation failed in the TorchScript interpreter.
                          \nTraceback of TorchScript, serialized code (most recent call last):
                          \n  File \&quot;code/__torch__.py\&quot;, line 12, in forward
                          \n    input_ids = inputs[\&quot;input_ids\&quot;]
                          \n    attention_mask = inputs[\&quot;attention_mask\&quot;]
                          \n    input = inputs[\&quot;token_type_ids\&quot;]
                          \n            ~~~~~~~~~~~~~~~~~~~~~~~~ &lt;--- HERE
                          \n    _0 = (model).forward(input_ids, attention_mask, input, )
                          \n    return {\&quot;sentence_embedding\&quot;: _0}
                          \n
                          \nTraceback of TorchScript, original code (most recent call last):
                          \n/Users/Library/Python/3.9/lib/python/site-packages/torch/jit/_trace.py(1074): trace_module
                          \n/Users/Library/Python/3.9/lib/python/site-packages/torch/jit/_trace.py(806): trace
                          \n/Users/opensearch/CustomModel/TEST/import_torch.py(43): export_to_torchscript
                          \n/Users/opensearch/CustomModel/TEST/import_torch.py(52): &lt;module&gt;
                          \nRuntimeError: KeyError: token_type_ids\n&quot;}
    &quot;&quot;&quot;,
  &quot;is_async&quot;: true
}
</code></pre>
<p>Here is python script I used to export the model into TorchScript</p>
<pre><code>import torch
from transformers import AutoModel, AutoTokenizer, PreTrainedTokenizer
from transformers.utils import PaddingStrategy
from sentence_transformers import SentenceTransformer


class TorchScriptWrapper(torch.nn.Module):
    def __init__(self, model):
        super(TorchScriptWrapper, self).__init__()
        self.model = model

    def forward(self, inputs: dict):
        with torch.no_grad():
            outputs = self.model(inputs)
        return {&quot;sentence_embedding&quot;: outputs['sentence_embedding']}


def export_to_torchscript(model_name: str, is_sentence_transformer: bool, output_path: str, max_seq_length: int = 128):
    tokenizer: PreTrainedTokenizer = AutoTokenizer.from_pretrained(model_name)
    if is_sentence_transformer:
        model = SentenceTransformer(model_name, device=&quot;cpu&quot;)
    else:
        model = AutoModel.from_pretrained(model_name)
    model.eval()

    # Define example text
    text = &quot;This is a test string&quot;

    # Create inputs
    inputs = tokenizer(text, padding=PaddingStrategy.MAX_LENGTH, return_tensors=&quot;pt&quot;, max_length=max_seq_length)

    # Instantiate the wrapper class
    model_wrapper = TorchScriptWrapper(model)

    # Unpack HF batch encoding into a regular dict
    new_inputs = {}
    new_inputs[&quot;input_ids&quot;] = inputs[&quot;input_ids&quot;]
    new_inputs[&quot;attention_mask&quot;] = inputs[&quot;attention_mask&quot;]
    if inputs.get(&quot;token_type_ids&quot;, None) is not None:
        new_inputs[&quot;token_type_ids&quot;] = inputs[&quot;token_type_ids&quot;]

    # Trace the wrapper class
    traced_model = torch.jit.trace(model_wrapper, new_inputs, strict=False)

    # Save traced model to file
    traced_model.save(output_path)


if __name__ == &quot;__main__&quot;:
    # Load pre-trained model and tokenizer
    model_name = &quot;sentence-transformers/LaBSE&quot;
    export_to_torchscript(model_name, True, output_path=&quot;torchscript_labse.pt&quot;)
</code></pre>
<p>PS : The error occured even though the <strong>token_type_ids</strong> key is part of the inputs dictionary</p>
","huggingface"
"78351643","How to make indexes using already made embeddings","2024-04-19 06:14:32","","0","33","<python><machine-learning><huggingface><llama-index>","<p>I'm a novice to ML. I want to use llama_index. I have embeddings. How can I make indexes using those embeddings? I'm using huggingface fine tuned LLM. Please make me correct if I understood something wrong.</p>
","huggingface"
"78350160","Huggingface Custom Pipeline + Langchain Question","2024-04-18 20:22:55","","0","166","<python><langchain><huggingface>","<p>I'm developing a RAG POC using Langchain. My project builds a custom huggingface pipeline(subclass of TextGenerationPipeline) and inferences with the pipeline are fine. But when I trying to build a chain with it, langchain does not support it. I'm kinda stuck here.</p>
<p>What I did is:</p>
<pre><code>from transformers import pipeline
import langchain
from langchain.llms import HuggingFacePipeline

custom_pipe = transformers.pipeline(
    &quot;custom_pipe&quot;,
    model=model,
    tokenizer=tokenizer,
    ...
)

my_llm = HuggingFacePipeline(pipeline=custom_pipe)

chain=prompt | my_llm

chain.invoke({&quot;question&quot;:&quot;How to cook a steak?&quot;})
</code></pre>
<p>And it gives me error:
ValueError: Got invalid task custom_pipe, currently only ('text2text-generation', 'text-generation', 'summarization', 'translation') are supported</p>
<p>Any ideas on how to build chain with custom huggingface pipes?</p>
","huggingface"
"78348493","llama_index crashes with HuggingFaceEmbedding","2024-04-18 15:01:08","","0","300","<huggingface><llama><llama-index>","<p>I am trying to build a RAG pipeline using <code>llama_index</code>. One of the first steps is to choose an embedding model that will be used for a <code>VectorStoreIndex</code>. My current implementation looks like this:</p>
<pre class=""lang-py prettyprint-override""><code>from llama_index.core import Settings
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

# some other code
if embedding == Embedding.OPENAI:
    Settings.embed_model = OpenAIEmbedding()
elif embedding == Embedding.BGE_SMALL_EN:
    Settings.embed_model = HuggingFaceEmbedding (
        model_name = &quot;BAAI/bge-small-en-v1.5&quot;
    )
</code></pre>
<p>While <code>OpenAIEmbedding</code> works as expected, my Jupyter Notebook always crashes when using <code>HuggingFaceEmbedding</code>.</p>
<p>I have simply pip installed all required modules and started running the application. Is there anything else required to make this work? Do I need to download the embedding repository and place it locally? If so, where do I need to place it?</p>
","huggingface"
"78348481","Using transformer package in PySpark gives module not found error","2024-04-18 14:59:48","","0","49","<pyspark><huggingface-transformers><transformer-model><huggingface><johnsnowlabs-spark-nlp>","<p>I am using a pretrained huggingface model inside a SparkNLP pipeline:</p>
<pre><code>class T5_EP(Wikify):
    &quot;&quot;&quot;This pipeline generates elevator pitch for a given paper abstract using T5 model&quot;&quot;&quot;
    def __init__(self, feature_col, index_col, result_col):
        super().__init__(feature_col, index_col, result_col)
        self.model_name = const.DEFAULT_T5_EP_MODEL
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForSeq2SeqLM.from_pretrained(self.model_name)
        
        self.task = &quot;summarize:&quot;
        self.ep = &quot;elevator pitch&quot;
        self.feature_col = feature_col
        self.index_col = index_col
        self.result_col = result_col
        self.max_output = const.ELEVATOR_PITCH_MAX_OUTPUT


    def gen_elevator_pitch(self, spark_sess, data):
        &quot;&quot;&quot;Function to generate an elevator pitch for a given paper abstract&quot;&quot;&quot;
        def summarizer(text):
            inputs = self.tokenizer.encode(&quot;summarize: &quot; + text, return_tensors=&quot;pt&quot;, truncation=True)
            outputs = self.model.generate(inputs, max_length=const.ELEVATOR_PITCH_MAX_OUTPUT, num_beams=4, early_stopping=True)
            return self.tokenizer.decode(outputs[0], skip_special_tokens=True)

        summarize_udf = udf(summarizer, StringType())
        result = data.withColumn(self.result_col, summarize_udf(data[self.feature_col]))

        result = self.post_process(result)
        results_df.show(truncate=False)
        return result
    
    def post_process(self, result):
        # Implement any post-processing steps if needed
        return result
</code></pre>
<p>This whole code is inside the file x.py; however, when I run the pipeline, it keeps throwing &quot;no module named x&quot; error. I suspect its a problem of PySpark trying to serialize transformer functions, is there anyway to solve it?</p>
<p>My attempt is based on: <a href=""https://towardsdatascience.com/large-models-meet-big-data-spark-and-llms-in-harmony-5e2976b69b62"" rel=""nofollow noreferrer"">https://towardsdatascience.com/large-models-meet-big-data-spark-and-llms-in-harmony-5e2976b69b62</a></p>
","huggingface"
"78345615","How to modify the inference API parameters on model card page","2024-04-18 07:27:55","","0","53","<nlp><huggingface><inference><huggingface-hub>","<p>I have hosted a model on hugging face. While using the inference API on the model card UI page. the model is generating text with default values. I wish to change max_new_tokens to 250 from default value of 100. How to do this?</p>
<p><a href=""https://i.sstatic.net/fGrxZ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/fGrxZ.png"" alt=""enter image description here"" /></a></p>
<p>using <code>requests</code> we can do this but I wanted to have the changes implemented on the hugging face UI.</p>
<pre><code>import requests

API_ENDPOINT = &quot;https://api-inference.huggingface.co/models/user/model_id&quot;
API_TOKEN = &quot;hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx&quot;

data = {
    &quot;inputs&quot;: &quot;कड़ी मेहनत के महत्व पर एक निबंध लिखें&quot;,
    &quot;parameters&quot;: {
        &quot;temperature&quot;: 0.2,
        &quot;top_k&quot;: 500,
        &quot;top_p&quot;: 0.9,
        &quot;max_new_tokens&quot;: 250,
        &quot;repetition_penalty&quot;: 5,
        &quot;do_sample&quot;: True,
        &quot;num_return_sequences&quot;: 1
    }
}

response = requests.post(API_ENDPOINT, json=data, headers={&quot;Authorization&quot;: f&quot;Bearer {API_TOKEN}&quot;})

if response.status_code == 200:
    output = response.json()
    generated_text = output[0][&quot;generated_text&quot;]
    print(generated_text)
else:
    print(&quot;Error:&quot;, response.text)
</code></pre>
","huggingface"
"78343018","The LLM wrapper doesn't return a answer from a local model","2024-04-17 18:04:54","","0","121","<python><large-language-model><huggingface>","<p>I've created a LLM bot using python using streamlit as frontend interface to generate a blog article.
I will recieve a some guidelines to send to a prompt template and enerate the article, for do it so, I will use a HuggingFace model but instantiated locally(in a folder called &quot;models&quot;).</p>
<p>The template are correct, and my LLM call(using invoke) is following the documentation, but doesn't respond. A raw text returns normally.</p>
<p>Did I missed something? I need to put an &quot;await&quot; on these cases?</p>
<p>The complete code is below:</p>
<pre><code> import streamlit as st
 from langchain.prompts import PromptTemplate
 from langchain_community.llms import CTransformers
 def getModelResponse(blog_topic, no_words, blog_style):
    try:

      llm = CTransformers(model='models/llama-2-7b-chat.ggmlv3.q8_0.bin',
                  model_type='llama',
                  config={'max_new_tokens': 256,
                          'temperature': 0.01})

      template = &quot;&quot;&quot;Write a blog for {blog_style} job profile for a topic {blog_topic} within {no_words} words.&quot;&quot;&quot;

      prompt = PromptTemplate(input_variables=[&quot;blog_style&quot;, &quot;blog_topic&quot;, &quot;no_words&quot;],
                      template=template)

      response = llm.invoke(prompt.format(blog_style=blog_style,blog_topic=blog_topic,no_words=no_words))

      return response
   
    except Exception as error:
      print(error)
      return error

 st.set_page_config(page_title=&quot;IA Generated blog&quot;,
               page_icon='🤖',
               layout='centered',
               initial_sidebar_state='collapsed')

 st.header(&quot;Generate blogs 🤖&quot;)

 blog_topic = st.text_input(&quot;Inform a blog topic&quot;)

 col1, col2 = st.columns([5,5])

 with col1:
   no_words = st.text_input('Inform the number of words')

 with col2:
   blog_style = st.selectbox('Context', ('Researchers', 'Data Science', 'People'), index=0)

 submit = st.button(&quot;Generate&quot;)

 if submit:
   st.write(getModelResponse(blog_topic, no_words, blog_style))`
</code></pre>
<p>I tried to use await but a error returned</p>
","huggingface"
"78338285","Run pre-trained LLM model on CPU - ValueError: Expected a cuda device, but got: cpu","2024-04-17 04:04:15","","2","220","<python><huggingface-transformers><large-language-model><huggingface><pre-trained-model>","<p>I am using a LLM model, <code>CohereForAI/c4ai-command-r-plus-4bit</code>, to do some inference. I have a GPU but it's not powerful enough so I want to use CPU. Below are the example codes and problems.</p>
<p>Code:</p>
<pre><code>from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM

PRETRAIN_MODEL = 'CohereForAI/c4ai-command-r-plus-4bit'
tokenizer = AutoTokenizer.from_pretrained(PRETRAIN_MODEL)
model = AutoModelForCausalLM.from_pretrained(PRETRAIN_MODEL, device_map='cpu')

text = &quot;this is an example&quot;
inputs = tokenizer(text, return_tensors=&quot;pt&quot;)
with torch.no_grad():
    outputs = model(**inputs)
    embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()
print(embedding.shape)
</code></pre>
<p>Error:</p>
<pre><code>ValueError: Expected a cuda device, but got: CPU
</code></pre>
<p>Does it mean that the <code>c4ai-command-r-plus-4bit</code> model can only run on GPU? Is there anything I missed to run it on CPU? Thanks!</p>
","huggingface"
"78337841","When I use the code in jupyter to run the Gemma model on Huggingface, the following error appears:","2024-04-17 00:44:51","","0","37","<huggingface><gemma>","<p>ProxyError: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /google/gemma-7b (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('&lt;urllib3.connection.VerifiedHTTPSConnection object at 0x7f46f24f09b0&gt;: Failed to establish a new connection: [Errno 111] Connection refused')))</p>
<p>maybe proxy has  problem？</p>
<p>I tried changing proxy settings with code like below,but no effect...</p>
<pre><code>import os

os.environ['http_proxy'] = 'socks5://127.0.0.1:10808'
os.environ['https_proxy'] = 'socks5://127.0.0.1:10808'
os.environ['https_proxy'] = 'http://127.0.0.1:10809'
os.environ['https_proxy'] = 'https://127.0.0.1:10809'
</code></pre>
","huggingface"
"78333442","Why does the encoder output latent variable shape of AutoencoderKL differ from the decoder input latent variable shape?","2024-04-16 09:22:07","","0","205","<autoencoder><huggingface>","<pre class=""lang-py prettyprint-override""><code>from diffusers import AutoencoderKL
import torch
from PIL import Image
from torchvision import transforms
vae = AutoencoderKL.from_pretrained(&quot;../model&quot;)

image = Image.open(&quot;../2304_10752.png&quot;).resize((512, 512))
image = transforms.ToTensor()(image).unsqueeze(0)
print(image.shape)
out = vae.encoder(image*2-1)
print(out.shape)
out = vae.decoder(out)
print(out[0].shape)
out = out[0].permute(1, 2, 0).detach().numpy()
out = (out * 255).astype(&quot;uint8&quot;)
out = Image.fromarray(out)
out.show()
</code></pre>
<p><strong>RuntimeError: Given groups=1, weight of size [512, 4, 3, 3], expected input[1, 8, 64, 64] to have 4 channels, but got 8 channels instead</strong></p>
<p>I downloaded and loaded the vae part of the stable diffusion model in huggingface separately.</p>
","huggingface"
"78333326","ModuleNotFoundError: No module named 'llama_index.postprocessor'","2024-04-16 09:03:29","","0","1740","<python><agent><huggingface><llama-index>","<p>I tried to import CohereRerank module from llama_index. But got a <code>ModuleNotFoundError: No module named 'llama_index.postprocessor'</code>.</p>
","huggingface"
"78332638","Mismatch in Loss When Using `torch.load` and `resume_from_checkpoint`","2024-04-16 06:53:08","","1","260","<python><pytorch><huggingface-transformers><huggingface><huggingface-trainer>","<p>I'm encountering an issue when trying to resume training using a saved checkpoint. When I use <code>torch.load</code> to load the model, I find that the initial loss is 4, whereas the loss saved in the checkpoint from the previous training is 2.3.</p>
<pre class=""lang-py prettyprint-override""><code>    if model_args.path_to_my_ckpt is not None:
        model.load_state_dict(
            torch.load(f&quot;{model_args.path_to_my_ckpt}/pytorch_model.bin&quot;), strict=True
        )
        logger.info(
            f&quot;loading trained model from {model_args.path_to_my_ckpt}/pytorch_model.bin&quot;
        )
</code></pre>
<p>I've also tried using <code>Trainer(train_from_checkpoint=checkpoint_path)</code>, but the initial loss remains at 4.</p>
<p>here is the part of previous training log and the training states:</p>
<pre><code>[INFO|trainer.py:2905] 2024-04-13 07:23:20,504 &gt;&gt; Saving model checkpoint to /home_path/checkpoints/test_use_mlm_chbz_dec
[INFO|configuration_utils.py:458] 2024-04-13 07:23:20,506 &gt;&gt; Configuration saved in /home_path/checkpoints/test_use_mlm_chbz_dec/config.json
[INFO|modeling_utils.py:1837] 2024-04-13 07:23:21,078 &gt;&gt; Model weights saved in /home_path/test_use_mlm_chbz_dec/pytorch_model.bin
[INFO|tokenization_utils_base.py:2181] 2024-04-13 07:23:21,081 &gt;&gt; tokenizer config file saved in /home_path/checkpoints/test_use_mlm_chbz_dec/tokenizer_config.json
[INFO|tokenization_utils_base.py:2188] 2024-04-13 07:23:21,083 &gt;&gt; Special tokens file saved in /home_path/checkpoints/test_use_mlm_chbz_dec/special_tokens_map.json
</code></pre>
<p>last lines in <code>trainer_state.json</code></p>
<pre><code>   {
      &quot;epoch&quot;: 7.92,
      &quot;learning_rate&quot;: 3.6662134375000002e-06,
      &quot;loss&quot;: 2.2366,
      &quot;step&quot;: 99850
    },
    {
      &quot;epoch&quot;: 7.92,
      &quot;learning_rate&quot;: 3.1453853125000002e-06,
      &quot;loss&quot;: 2.2395,
      &quot;step&quot;: 99900
    },
    {
      &quot;epoch&quot;: 7.92,
      &quot;learning_rate&quot;: 2.6245571875000002e-06,
      &quot;loss&quot;: 2.2364,
      &quot;step&quot;: 99950
    },
    {
      &quot;epoch&quot;: 7.93,
      &quot;learning_rate&quot;: 2.1037290625e-06,
      &quot;loss&quot;: 2.2359,
      &quot;step&quot;: 100000
    },
    {
      &quot;epoch&quot;: 7.93,
      &quot;step&quot;: 100000,
      &quot;total_flos&quot;: 3.8100065615987343e+18,
      &quot;train_loss&quot;: 2.39436900390625,
      &quot;train_runtime&quot;: 66313.4695,
      &quot;train_samples_per_second&quot;: 772.091,
      &quot;train_steps_per_second&quot;: 1.508
    }
  ],
  &quot;max_steps&quot;: 100000,
  &quot;num_train_epochs&quot;: 8,
  &quot;total_flos&quot;: 3.8100065615987343e+18,
  &quot;trial_name&quot;: null,
  &quot;trial_params&quot;: null
}
</code></pre>
<p>And here is the part of log when I use <code>resume_from_checkpoint</code>:</p>
<pre><code>04/16/2024 01:15:20 - INFO - __main__ - ***** Running training *****
04/16/2024 01:15:20 - INFO - __main__ -   Num examples = 6,458,670
04/16/2024 01:15:20 - INFO - __main__ -   Num Epochs = 8
04/16/2024 01:15:20 - INFO - __main__ -   Instantaneous batch size per device = 512
04/16/2024 01:15:20 - INFO - __main__ -   Total train batch size (w. parallel, distributed &amp; accumulation) = 512
04/16/2024 01:15:20 - INFO - __main__ -   Gradient Accumulation steps = 1
04/16/2024 01:15:20 - INFO - __main__ -   Total optimization steps = 100,100
04/16/2024 01:15:20 - INFO - __main__ -   Number of trainable parameters = 11,840,256
04/16/2024 01:15:20 - INFO - __main__ -   Continuing training from checkpoint, will skip to saved global_step
04/16/2024 01:15:20 - INFO - __main__ -   Continuing training from epoch 7
04/16/2024 01:15:20 - INFO - __main__ -   Continuing training from global step 100000
04/16/2024 01:15:20 - INFO - __main__ -   Will skip the first 7 epochs then the first 11695 batches in the first epoch.
</code></pre>
<p>which turns out that it has load the ckpt successfully. The startup script (sh file) and data paths are identical to the previous training. Why is there a discrepancy in the loss values? Did I load the checkpoint incorrectly, or is there another issue?</p>
<pre><code>transformers version: 4.29.0
</code></pre>
<p>Feel free to post your opinion, I am looking forward a solution or explanation about this issue.</p>
","huggingface"
"78332474","Is there any method to fully load the GGUF models on GPU","2024-04-16 06:16:03","","1","925","<python><large-language-model><huggingface><llama-index><llamacpp>","<p>I have been using LlamaCPP to load my llm models, the llama-index library provides methods to offload some layers onto the GPU. Why does it not provide any methods to fully load the model on GPU. If there is some method Please help.</p>
<p><a href=""https://i.sstatic.net/8tBUc.png"" rel=""nofollow noreferrer"">LlamaCPP method</a></p>
<p>Here we have the option to offload some layers on GPU but I want to fully load the model on GPU.</p>
","huggingface"
"78330560","Huggingface model download","2024-04-15 19:09:06","","0","103","<model><download><huggingface>","<p>I have cloned repo from huggingface with git lfs but all files are JSON format. I want to train NLP model and so I need python code but there are all JSON. Could you help me how to download the model and use for training?</p>
<p>git lfs install
I run this.
git clone </p>
","huggingface"
"78326215","KeyError: 'input_ids' arise when I used prompt-tuned Codet5 for","2024-04-15 05:21:37","","0","43","<huggingface-transformers><prompt><large-language-model><huggingface><peft>","<p>I successfully prompt tuned codet5, however, I can't use the fine-tuned model for inference.
It shows Key error 'input_ids':</p>
<pre><code>Traceback (most recent call last):
  File &quot;/home/liangpeng/project/Mabel/detection/prompt_tuning/codet5_pilot.py&quot;, line 35, in &lt;module&gt;
    generated_ids = peft_model.generate(input_ids, max_length=2048)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/home/liangpeng/project/anaconda3/envs/linghao/lib/python3.11/site-packages/peft/peft_model.py&quot;, line 1192, in generate
    outputs = self.base_model.generate(**kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/home/liangpeng/project/anaconda3/envs/linghao/lib/python3.11/site-packages/torch/utils/_contextlib.py&quot;, line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File &quot;/home/liangpeng/project/anaconda3/envs/linghao/lib/python3.11/site-packages/transformers/generation/utils.py&quot;, line 1527, in generate
    result = self._greedy_search(
             ^^^^^^^^^^^^^^^^^^^^
  File &quot;/home/liangpeng/project/anaconda3/envs/linghao/lib/python3.11/site-packages/transformers/generation/utils.py&quot;, line 2408, in _greedy_search
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/home/liangpeng/project/anaconda3/envs/linghao/lib/python3.11/site-packages/peft/peft_model.py&quot;, line 1225, in prepare_inputs_for_generation
    size = model_kwargs[&quot;input_ids&quot;].shape[0], peft_config.num_virtual_tokens
           ~~~~~~~~~~~~^^^^^^^^^^^^^
KeyError: 'input_ids'
</code></pre>
<p>Here is the code I used the model for inference:</p>
<pre><code>from transformers import RobertaTokenizer, T5ForConditionalGeneration
import torch

device = 'cuda' if torch.cuda.is_available() else 'cpu'

text_column = 'Input'

model_name_or_path = '/home/liangpeng/project/Mabel/CLMs/codet5_base_multi_sum_local'
tokenizer_name_or_path = '/home/liangpeng/project/Mabel/CLMs/codet5_base_multi_sum_local'
tokenizer = RobertaTokenizer.from_pretrained(tokenizer_name_or_path)

content = '''Please read the following Java method code and analyze the code smell of method level, pointing out the reason:

    /** 
    * 事项移动到待办
    */
    @PostMapping(&quot;/waiting&quot;) public R&lt;TaskRes&gt; toWaiting(@RequestBody @Validated TaskUpdStatusReq req){
    baseService.toWaiting(req);
    return R.ok(baseService.listTask(req.getTodoId()));
    }
'''

def inference():
    def generate(inputs, infer_model):
        with torch.no_grad():
            inputs = {k: v.to(device) for k, v in inputs.items()}
            outputs = infer_model.generate(
                input_ids=inputs[&quot;input_ids&quot;].to(device),
                attention_mask=inputs[&quot;attention_mask&quot;].to(device),
                max_new_tokens = 60,
                eos_token_id = 3
            )
            

    '''(1) base model_inference'''
    base_model = T5ForConditionalGeneration.from_pretrained(model_name_or_path)
    base_model.to(device)
    inputs = tokenizer(f'{text_column}: {content}\nLabel:',return_tensors='pt')
    generate(inputs, base_model)
    print('-----------------------------')

    '''(2) prompt tuning model_inference'''
    from peft import PeftModel, PeftConfig
    # peft_model = PeftModel.from_pretrained(base_model, model_id='/home/liangpeng/output/checkpoint-400')
    # peft_model = peft_model.to(device)
    # inputs = tokenizer(f'{text_column}: {content}\nLabel:', return_tensors='pt').to(device)
    
    # print(tokenizer.decode(peft_model.generate(input_ids=inputs[&quot;input_ids&quot;], attention_mask=inputs[&quot;attention_mask&quot;], max_length=60, do_sample=True)[0], skip_special_tokens=True))

    path = '/home/liangpeng/output/checkpoint-400'
    config = PeftConfig.from_pretrained(path)
    pretrained_model = T5ForConditionalGeneration.from_pretrained(model_name_or_path)
    prompt_tuned_model = PeftModel.from_pretrained(pretrained_model, path)
    prompt_tuned_model.to(device)
    
    inputs_p = tokenizer(f'{text_column}: {content}\nLabel:', return_tensors='pt')
    print(f'-----------------------------{inputs_p}')
    inputs_p = {k: v.to(device) for k, v in inputs_p.items()}
    print('input_ids' in inputs_p)
    print(tokenizer.decode(prompt_tuned_model.generate(input_ids=inputs_p['input_ids'].to(device), attention_mask=inputs_p['attention_mask'].to(device), max_length=60, do_sample=True)[0], skip_special_tokens=True))
    generate(inputs, prompt_tuned_model)

inference()
</code></pre>
<p>Actually, <code>print(inputs_p.keys())</code> shows that inputs_p is a dict and has 'input_ids' and 'attention_mask'. I don't know why this error arose. Please help me, thank u😖</p>
<p>Please tell me how to address this error</p>
","huggingface"
"78323826","ValueError: Expected input batch_size (8) to match target batch_size (2048)","2024-04-14 12:42:51","","0","34","<nlp><huggingface-transformers><transformer-model><huggingface>","<p>I am new to this field.
I have been stuck on this for quite a while now, and everything I tried has not helped me.</p>
<p>I want to finetune a model with data I have for machine translation. I am using the torch and transformers libraries.
Here is my code:</p>
<pre><code>tokenizer = MT5Tokenizer.from_pretrained(&quot;google/mt5-small&quot;)
model = MT5ForSequenceClassification.from_pretrained(&quot;google/mt5-small&quot;)
training_args = TrainingArguments(
    output_dir=&quot;./data&quot;,
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir=&quot;./logs&quot;,
)


trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_ds,
    eval_dataset=evalu_ds,
    tokenizer=tokenizer,
)
</code></pre>
<p><code>train_dataset</code> and <code>eval_dataset</code> are of type CustomDataset class I have defined that inherits from <code>torch.utils.data.Dataset</code>. They return:</p>
<pre><code>return {
    &quot;input_ids&quot;: input_ids_lang1,
    &quot;attention_mask&quot;: attention_mask_lang1,
    &quot;labels&quot;: input_ids_lang2,
    #&quot;attention_mask_lang2&quot;: attention_mask_lang2.float()
}
</code></pre>
<p><code>tokenizer</code> is <code>MT5Tokenizer.from_pretrained(&quot;google/mt5-small&quot;)</code></p>
<p>I get this error:</p>
<pre><code>  File &quot;C:\Users\X\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\nn\functional.py&quot;, line 3060, in cross_entropy
    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)
ValueError: Expected input batch_size (8) to match target batch_size (2048).
</code></pre>
<p>printing out the shapes of <code>input</code> and <code>target</code> Tensors in the functional.py I get</p>
<p>input:  torch.Size([8, 256])</p>
<p>target: torch.Size([2048])</p>
<p>Just as it is mentioned <a href=""https://discuss.huggingface.co/t/reshaping-logits-when-using-trainer/18214"" rel=""nofollow noreferrer"">here</a> the target size seems to be 8 * 256 = 2048. But trying the CustomTrainer class did not help either.</p>
<p>I think the main problem I have is understanding what that means.</p>
<p>I tried many different variants, different models.
I expect to not get this error</p>
","huggingface"
"78320650","huggingface_hub.utils._errors.LocalEntryNotFoundError","2024-04-13 12:58:33","","0","1549","<huggingface-transformers><huggingface><huggingface-hub>","<p>The above exception was the direct cause of the following exception:</p>
<p>Traceback (most recent call last):
File &quot;/home/pangchongwen/anaconda3/envs/chatglm3/bin/huggingface-cli&quot;, line 8, in 
sys.exit(main())
File &quot;/home/pangchongwen/anaconda3/envs/chatglm3/lib/python3.10/site-packages/huggingface_hub/commands/huggingface_cli.py&quot;, line 49, in main
service.run()
File &quot;/home/pangchongwen/anaconda3/envs/chatglm3/lib/python3.10/site-packages/huggingface_hub/commands/download.py&quot;, line 160, in run
print(self._download())  # Print path to downloaded files
File &quot;/home/pangchongwen/anaconda3/envs/chatglm3/lib/python3.10/site-packages/huggingface_hub/commands/download.py&quot;, line 201, in _download
return snapshot_download(
File &quot;/home/pangchongwen/anaconda3/envs/chatglm3/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py&quot;, line 118, in _inner_fn
return fn(*args, **kwargs)
File &quot;/home/pangchongwen/anaconda3/envs/chatglm3/lib/python3.10/site-packages/huggingface_hub/_snapshot_download.py&quot;, line 251, in snapshot_download
raise LocalEntryNotFoundError(
huggingface_hub.utils._errors.LocalEntryNotFoundError: An error happened while trying to locate the files on the Hub and we cannot find the appropriate snapshot folder for the specified revision on the local disk. Please check your internet connection and try again.</p>
<p>I suspect it is because I deleted the cache of huggingface in the .cache folder, and then this problem occurred. I don’t know how to solve it. How to restore the cache directory structure of huggingface?</p>
<p>I guess it's not a network problem</p>
","huggingface"
"78315961","Speechbrains SpeakerRecognition saves short cuts/links/symlinks of used audio files in working directory","2024-04-12 11:26:37","","0","93","<python><speech-recognition><voice-recognition><huggingface><speechbrain>","<p>I use the speaker recognition of speechbrain using the Python language:
<code>from speechbrain.inference.speaker import SpeakerRecognition</code>
and I load a model in the following way
<code>model = SpeakerRecognition.from_hparams(source=&quot;speechbrain/spkrec-ecapa-voxceleb&quot;,                                                    savedir=&quot;pretrained_models/spkrec-ecapa-voxceleb&quot;, run_opts={&quot;device&quot;:&quot;cuda&quot;})</code>.</p>
<p>Everything works very well.</p>
<p>However, it saves short cuts/links/symlinks to the audio files that I give to speechbrain in my working directory. Is there any way to suppress this?</p>
<p>Code example:</p>
<pre><code>model = 
SpeakerRecognition.from_hparams(source=&quot;speechbrain/spkrec-ecapa- 
voxceleb&quot;, savedir=&quot;pretrained_models/spkrec-ecapa-voxceleb&quot;,                                                      
run_opts={&quot;device&quot;:&quot;cuda&quot;})

# collects the distances of the speaker reference and the samples
dist_dict = {}

path_reference = os.path.join(reference_dir, reference_file_name)

for dir_name in samples_dir_names:

    dir_path = os.path.join(samples_dir, dir_name)

    dist_dict[dir_path] = {}

    file_paths_names = get_all_file_paths_and_names(
        os.path.join(samples_dir, dir_name),ext=ext)
    
    file_paths = file_paths_names['file_paths']
    file_names = file_paths_names['file_names']
    
    for file_path, file_name in zip(file_paths, file_names):
        
        duration = librosa.get_duration(filename=file_path)
        if duration &gt;= duration_val:
            print(f'\t{file_name}')
            print(f'\t\tduration: {duration}')

            dist = compare_audios(path_reference,
                                  file_path,
                                  model
                                  )
            
            dist_dict[dir_path][file_name] = dist

# ------------------------------------------------- #
def compare_audios(speaker1_path,
               speaker2_path,
               model):
&quot;&quot;&quot;
INPUT
    speaker1_path (str): Path to the audo file
        for the first speaker.
    speaker2_path (str): Path to the audo file
        for the second speaker.
&quot;&quot;&quot;



score, prediction = model.verify_files(speaker1_path,
                                             speaker2_path)

return {'score': score[0].cpu().item(),
        'prediction': prediction[0].cpu().item()}
</code></pre>
<p>It exists a discussion about it but no solution until today: <a href=""https://github.com/speechbrain/speechbrain/pull/1303"" rel=""nofollow noreferrer"">https://github.com/speechbrain/speechbrain/pull/1303</a></p>
","huggingface"
"78310964","AttributeError: type object 'AutoModelForCausalLM' has no attribute 'from_pretrainted'","2024-04-11 14:11:10","","0","256","<python><nlp><huggingface-transformers><huggingface>","<p>I see video at <a href=""https://www.youtube.com/watch?v=C3vvE9g1K9w"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=C3vvE9g1K9w</a> . My jupyterlab</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

model_name_or_path = &quot;TheBloke/CodeLlama-7B-Instruct-GPTQ&quot;
model = AutoModelForCausalLM.from_pretrainted(model_name_or_path, device_map = &quot;auto&quot;, trust_remote_code = True, revision = &quot;main&quot;)

tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast = True)

prompt_template = f'''[INST] Write the Python code to perform Binary search 
[/INST]

'''

pipe = pipeline(&quot;text-generation&quot;, model = model, tokenizer = tokenizer, max_new_tokens = 512, do_sample = True, temperature = 0.7, top_p = 0.95, top_k = 40, repetition_penalty = 1.1)

print(pipe(prompt_template)[0]['generated_text'])
</code></pre>
<p>error</p>
<pre><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
Cell In[15], line 4
      1 from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
      3 model_name_or_path = &quot;TheBloke/CodeLlama-7B-Instruct-GPTQ&quot;
----&gt; 4 model = AutoModelForCausalLM.from_pretrainted(model_name_or_path, device_map = &quot;auto&quot;, trust_remote_code = True, revision = &quot;main&quot;)
      6 tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast = True)
      8 prompt_template = f'''[INST] Write the Python code to perform Binary search 
      9 [/INST]
     10 
     11 '''

AttributeError: type object 'AutoModelForCausalLM' has no attribute 'from_pretrainted'
</code></pre>
<p><a href=""https://i.sstatic.net/B9KCV.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/B9KCV.png"" alt=""enter image description here"" /></a></p>
<p>How to fix it?</p>
","huggingface"
"78309756","Mistral model generates the same embeddings for different input texts","2024-04-11 10:37:37","78310258","3","616","<python><huggingface-transformers><large-language-model><huggingface><pre-trained-model>","<p>I am using pre-trained LLM to generate a representative embedding for an input text. But it is wired that the output embeddings are all the same regardless of different input texts.</p>
<p>The codes:</p>
<pre><code>from transformers import pipeline, AutoTokenizer, AutoModel
import numpy as np
PRETRAIN_MODEL = 'mistralai/Mistral-7B-Instruct-v0.2'
tokenizer = AutoTokenizer.from_pretrained(PRETRAIN_MODEL)
model = AutoModel.from_pretrained(PRETRAIN_MODEL)

def generate_embedding(document):
    inputs = tokenizer(document, return_tensors='pt')
    print(&quot;Tokenized inputs:&quot;, inputs)
    with torch.no_grad():
        outputs = model(**inputs)
    embedding = outputs.last_hidden_state[0, 0, :].numpy()
    print(&quot;Generated embedding:&quot;, embedding)
    return embedding

text1 = &quot;this is a test&quot;
text2 = &quot;this is another test&quot;
text3 = &quot;there are other tests&quot;

embedding1 = generate_embedding(text1)
embedding2 = generate_embedding(text2)
embedding3 = generate_embedding(text3)

are_equal = np.array_equal(embedding1, embedding2) and np.array_equal(embedding2, embedding3)

if are_equal:
    print(&quot;The embeddings are the same.&quot;)
else:
    print(&quot;The embeddings are not the same.&quot;)
</code></pre>
<p>The printed tokens are different, but the printed embeddings are the same. The outputs:</p>
<pre><code>Tokenized inputs: {'input_ids': tensor([[   1,  456,  349,  264, 1369]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}
Generated embedding: [-1.7762679  1.9293272 -2.2413437 ...  2.6379988 -3.104867   4.806004 ]
Tokenized inputs: {'input_ids': tensor([[   1,  456,  349, 1698, 1369]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}
Generated embedding: [-1.7762679  1.9293272 -2.2413437 ...  2.6379988 -3.104867   4.806004 ]
Tokenized inputs: {'input_ids': tensor([[   1,  736,  460,  799, 8079]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}
Generated embedding: [-1.7762679  1.9293272 -2.2413437 ...  2.6379988 -3.104867   4.806004 ]
The embeddings are the same.
</code></pre>
<p>Does anyone know where the problem is? Many thanks!</p>
","huggingface"
"78309392","stabilityai/stablelm-2-zephyr-1_6b SLM doesn't work properly after fine-tuning","2024-04-11 09:33:08","","0","36","<pytorch><huggingface-transformers><huggingface><pytorch-lightning>","<p>I am trying to teach the SLM stabilityai/stablelm-2-zephyr-1_6b to classify if a sentence is a hate speech or not. I have a DataFrame with 100 examples (I have lot more this is just a sample to begin with). The text looks like this:</p>
<pre><code>&quot;&lt;|user|&gt;\nIs the next sentence is hate speech: \&quot;{msg}\&quot;? &lt;|im_end|&gt;&quot;
</code></pre>
<p>and the labels looks like this:
<code>'&lt;|assistant|&gt;\nNo, it is not a hate speech &lt;|im_end|&gt;'</code> or <code>'&lt;|assistant|&gt;\nYes, it is a hate speech &lt;|im_end|&gt;'</code></p>
<p>I am using lightning package to wrap the model and its fine-tuning. I had set the variable model to be lightning containing <code>self.model</code> and <code>self.tokenizer</code>, my loss function is: <code>NllLossBackward0</code></p>
<p>I am doing one epoch and it's absolutely enough. I start with loss of 19 and finish with loss close to zero.</p>
<p>However, after training when I am trying to generate code using this code:</p>
<pre><code>prompt = [{'role': 'user', 'content': 'Is the next sentence is a hate speech: &quot;We should kill all Jews!&quot;?'}]
inputs = model.tokenizer.apply_chat_template(
    prompt,
    max_length=512,
    add_generation_prompt=True,
    return_tensors='pt', 
    padding=&quot;max_length&quot;
)

tokens = model.model.generate(
    inputs,
    max_new_tokens=512,
    temperature=0.5,
    do_sample=True
)

print(model.tokenizer.decode(tokens[0]))
</code></pre>
<p>All I get is:</p>
<pre><code>The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.
&lt;|user|&gt;
Is the next sentence is a hate speech: &quot;We should kill all Jews!&quot;?&lt;|im_end|&gt;
&lt;|assistant|&gt;
&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;
</code></pre>
<p>Any help will be appreciated. Thanks in advance.</p>
","huggingface"
"78309135","using fp16 in ""to cuda"" function call, of a HuggingFace pipeline does not work","2024-04-11 08:46:44","","1","73","<pytorch><huggingface>","<p>it works if i load the model using fp16, so not using variant i assumed that the model is being converted to fp16</p>
<pre><code>pipe = PixArtAlphaPipeline.from_pretrained(MODEL_ID, torch_dtype=torch.float16)
pipe.to(&quot;cuda&quot;)
</code></pre>
<p>however when i use this one:</p>
<pre><code>pipe = PixArtAlphaPipeline.from_pretrained(MODEL_ID)
pipe.to(&quot;cuda&quot;, dtype=torch.float16)
</code></pre>
<p>it does not work - it loads the model but it generates only garbage.
what is the difference between the 2? i would assume that both are converting the model to fp16.</p>
<p>here the complete code:</p>
<pre><code>import torch
from diffusers import PixArtAlphaPipeline
import os

output_dir = &quot;./generated/&quot;
if not os.path.exists(output_dir):
    os.mkdir(output_dir)

prompt = &quot;A small cactus with a happy face in the Sahara desert.&quot;

# Create a pipeline with float32 weights and move it to the GPU, convert it to float16 using .to()
pipe1 = PixArtAlphaPipeline.from_pretrained(&quot;PixArt-alpha/PixArt-XL-2-512x512&quot;)
pipe1.to(&quot;cuda&quot;, dtype=torch.float16)

image = pipe1(prompt, width=512, height=512, num_inference_steps=20).images[0]
image.save(os.path.join(output_dir, &quot;1.png&quot;))

del pipe1

# Create a pipeline with float16 weights and move it to the GPU
pipe2 = PixArtAlphaPipeline.from_pretrained(&quot;PixArt-alpha/PixArt-XL-2-512x512&quot;, torch_dtype=torch.float16)
pipe2.to(&quot;cuda&quot;)

image = pipe2(prompt, width=512, height=512, num_inference_steps=20).images[0]
image.save(os.path.join(output_dir, &quot;2.png&quot;))
</code></pre>
<p>the requirements file:</p>
<pre><code>transformers
accelerate
diffusers[torch]
sentencepiece
beautifulsoup4
# --extra-index-url https://download.pytorch.org/whl/cu121
torch
torchvision
</code></pre>
","huggingface"
"78308790","LoRA in PEFT can't reduce too much GPU memory consumption as we expected","2024-04-11 07:31:47","","0","385","<pytorch><huggingface><peft>","<p>I try to use lora to finetune a VIT for image classification:</p>
<ol>
<li>I download the vit-base-patch16-224-in21k model from huggingface</li>
<li>I use the peft in huggingface to implement lora with ViT</li>
</ol>
<p>Code could run successfully but it could not reduce the GPU memory consumption to even half of the original amount.</p>
<p>To show how much GPU memory consumption could be reduced by lora, I separately run &quot;linear-probing, full-finetune (tune all the parameters), lora&quot; to fine-tune a pretrained ViT.
For fair comparison, I keep all the hyper-parameter the same: batch-size, optimizer...
But the result is not what I expected:
<a href=""https://i.sstatic.net/7DVvY.png"" rel=""nofollow noreferrer"">enter image description here</a>
the above image is the GPU consumption of &quot;linear-probing, lora, full-finetune&quot; respectively.
It seems that lora only reduce a small portion of GPU memory compared to full-finetune</p>
<p>Here is my complete code:</p>
<pre><code>import torch
import torch.nn as nn
from torch.utils.data import Subset
from torchvision.datasets import CIFAR100,Places365,Food101,StanfordCars,Flowers102
from torchvision.transforms import Compose, Resize, ToTensor, Normalize
from torch.utils.data import DataLoader
import torch.optim as optim
from transformers import AutoImageProcessor, ViTModel,ViTForImageClassification
from peft import LoraConfig, get_peft_model
import wandb
import os
from tqdm import tqdm
import pickle
from util import setup_seed

os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &quot;0&quot;

#hyper-parameter
lr=2e-4
epochs=10
batch_size=256
dataset='flower102'
tune_mode='lora'
seed=42

#set random seed
setup_seed(seed)

if dataset=='place365_sub':
    num_class=10
elif dataset=='cifar100':
    num_class=100
elif dataset=='food101':
    num_class=101
elif dataset=='standfordcars':
    num_class=196
    epochs=30
elif dataset=='flower102':
    num_class=102

# Initialize wandb
wandb.init(project=&quot;vit-%s&quot;%(dataset),name=tune_mode)
wandb.config.learning_rate = lr
wandb.config.batch_size = batch_size
wandb.config.epochs = epochs
wandb.config.dataset=dataset
wandb.config.mode=tune_mode
wandb.config.seed=42


class VIT4classification(nn.Module):
    def __init__(self, num_classes):
        super(VIT4classification, self).__init__()
        self.vit = ViTModel.from_pretrained(&quot;google/vit-base-patch16-224-in21k&quot;)
        for param in self.vit.parameters():
            param.requires_grad = False

        self.classifier=nn.Linear(self.vit.config.hidden_size,num_classes)
        
    
    def forward(self, x):
        vit_output=self.vit(x).pooler_output
        logits=self.classifier(vit_output)

        return logits

if tune_mode=='linear_probing':
    model=VIT4classification(num_class)
elif tune_mode=='full-finetune':
    model=ViTForImageClassification.from_pretrained(&quot;google/vit-base-patch16-224-in21k&quot;,num_labels=num_class)
elif tune_mode=='lora':
    model=ViTForImageClassification.from_pretrained(&quot;google/vit-base-patch16-224-in21k&quot;,num_labels=num_class)
    config = LoraConfig(
    r=16,
    lora_alpha=16,
    target_modules=[&quot;query&quot;, &quot;value&quot;],
    lora_dropout=0.1,
    bias=&quot;none&quot;,
    modules_to_save=[&quot;classifier&quot;],
    )
    model = get_peft_model(model, config)
    model.print_trainable_parameters()

if dataset=='cifar100':
    # Prepare the CIFAR100 dataset
    transform = Compose([
        Resize((224, 224)),
        ToTensor(),
        Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
    ])
    train_dataset = CIFAR100(root='../../../scr1/yuanfanp/cifar100', train=True, download=True, transform=transform)
    test_dataset = CIFAR100(root='../../../scr1/yuanfanp/cifar100', train=False, download=True, transform=transform)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

elif dataset=='place365_sub':
    transform = Compose([
        Resize((224, 224)),
        ToTensor(),
        Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
    ])

    # 加载完整的Places365数据集
    train_dataset = Places365('../../../scr1/yuanfanp/Places365',split='train-standard',small=True,download=False,transform=transform)
    test_dataset =Places365('../../../scr1/yuanfanp/Places365',split='val',small=True,download=False,transform=transform)

    # 选择用于训练和测试的类别
    selected_classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]  # 例如，选择前10个类别


    # 创建子集
    subset_indices_train = []
    subset_indices_test = []

    if not os.path.exists('../../../scr1/yuanfanp/Places365/train_sub.pkl'):
        print('=========prepare subset of places365(train)=========')
        for i, (img, label) in enumerate(tqdm(train_dataset)):
            if label in selected_classes:
                subset_indices_train.append(i)
        print('=========prepare subset of places365(val)=========')
        for i, (img, label) in enumerate(tqdm(test_dataset)):
            if label in selected_classes:
                subset_indices_test.append(i)
        with open('../../../scr1/yuanfanp/Places365/train_sub.pkl', 'wb') as f:
            pickle.dump(subset_indices_train, f)
        with open('../../../scr1/yuanfanp/Places365/test_sub.pkl', 'wb') as f:
            pickle.dump(subset_indices_test, f)
    else:
        with open('../../../scr1/yuanfanp/Places365/train_sub.pkl', 'rb') as f:
            subset_indices_train = pickle.load(f)
        with open('../../../scr1/yuanfanp/Places365/test_sub.pkl', 'rb') as f:
            subset_indices_test = pickle.load(f)
    

    print('=========num of places365(train):%d========='%(len(subset_indices_train)))
    print('=========num of places365(val):%d========='%(len(subset_indices_test)))

    # 创建子集数据集
    subset_train = Subset(train_dataset, subset_indices_train)
    subset_test = Subset(test_dataset, subset_indices_test)

    # 创建数据加载器
    train_loader = DataLoader(subset_train, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(subset_test, batch_size=batch_size, shuffle=False)

elif dataset=='food101':
    # Prepare the food101 dataset
    transform = Compose([
        Resize((224, 224)),
        ToTensor(),
        Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
    ])
    train_dataset = Food101(root='../../../scr1/yuanfanp/food101', split='train', download=True, transform=transform)
    test_dataset = Food101(root='../../../scr1/yuanfanp/food101', split='test', download=True, transform=transform)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

elif dataset=='standfordcars':
    # Prepare the standfordcar dataset
    transform = Compose([
        Resize((224, 224)),
        ToTensor(),
        Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
    ])
    train_dataset = StanfordCars(root='../../../scr1/yuanfanp', split='train', download=True, transform=transform)
    test_dataset = StanfordCars(root='../../../scr1/yuanfanp', split='test', download=True, transform=transform)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

elif dataset=='flower102':
    # Prepare the flower102 dataset
    transform = Compose([
        Resize((224, 224)),
        ToTensor(),
        Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
    ])
    train_dataset = Flowers102(root='../../../scr1/yuanfanp/flower102', split='train', download=True, transform=transform)
    test_dataset = Flowers102(root='../../../scr1/yuanfanp/flower102', split='test', download=True, transform=transform)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
    
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

# 定义损失函数和优化器
criterion = nn.CrossEntropyLoss()
if tune_mode=='linear_probing':
    optimizer = optim.Adam(model.classifier.parameters(), lr=lr)
elif tune_mode=='full-finetune':
    optimizer = optim.Adam(model.parameters(), lr=lr)
elif tune_mode=='lora':
    optimizer = optim.Adam(model.parameters(), lr=lr)

# 训练模型
num_epochs = 10
best_accuracy = 0.0

for epoch in range(epochs):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    print('============epoch %d============'%(epoch))

    for inputs, labels in tqdm(train_loader):
        inputs, labels = inputs.to(device), labels.to(device)

        optimizer.zero_grad()

        if tune_mode=='linear_probing':
            outputs = model(inputs)
        else:
            outputs=model(inputs).logits
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()

    train_loss = running_loss / len(train_loader)
    train_accuracy = 100. * correct / total

    # 在测试集上评估模型
    model.eval()
    test_correct = 0
    test_total = 0

    with torch.no_grad():
        for inputs, labels in tqdm(test_loader):
            inputs, labels = inputs.to(device), labels.to(device)
            if tune_mode=='linear_probing':
                outputs = model(inputs)
            else:
                outputs=model(inputs).logits
            _, predicted = outputs.max(1)
            test_total += labels.size(0)
            test_correct += predicted.eq(labels).sum().item()

    test_accuracy = 100. * test_correct / test_total

    # 保存性能最好的模型
    if test_accuracy &gt; best_accuracy:
        best_accuracy = test_accuracy
        if tune_mode=='linear_probing':
            os.makedirs('../../../scr1/yuanfanp/model/ViT/%s/%s/'%(dataset,tune_mode), exist_ok=True)
            torch.save(model.classifier.state_dict(), '../../../scr1/yuanfanp/model/ViT/%s/%s/best.pth'%(dataset,tune_mode))
        elif tune_mode=='full-finetune':
            os.makedirs('../../../scr1/yuanfanp/model/ViT/%s/%s/'%(dataset,tune_mode), exist_ok=True)
            torch.save(model.state_dict(), '../../../scr1/yuanfanp/model/ViT/%s/%s/best.pth'%(dataset,tune_mode))
        elif tune_mode=='lora':
            os.makedirs('../../../scr1/yuanfanp/model/ViT/%s/%s/'%(dataset,tune_mode), exist_ok=True)
            model.save_pretrained('../../../scr1/yuanfanp/model/ViT/%s/%s/best.pth'%(dataset,tune_mode))

    wandb.log({&quot;Train Loss&quot;: train_loss, &quot;Train Accuracy&quot;: train_accuracy,&quot;test_accuracy&quot;:test_accuracy,&quot;best_accuracy&quot;:best_accuracy})
</code></pre>
","huggingface"
"78308327","Error occurred when I use nodejs to implement huggingface inference api","2024-04-11 05:23:57","","0","167","<node.js><huggingface>","<p>Huggingface has provided convinent reference api to implement or test Models.</p>
<p>I tried to implement Mistral-7B-Instruct-v0.2 in nodejs but fail.</p>
<p>There are three ways provided to implement it(python, javascript and curl).</p>
<p>When I use javascript it works well.</p>
<p>The code is below. I use the example code provided by huggingface.</p>
<pre><code>async function query(data) {
    const response = await fetch(
        &quot;https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2&quot;,
        {
            headers: {
                Authorization: &quot;Bearer hf_xxxxx&quot;,
                &quot;Content-Type&quot;: &quot;application/json&quot;
            },
            method: &quot;POST&quot;,
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}

query({&quot;inputs&quot;: &quot;Can you please let us know more details about your &quot;}).then((response) =&gt; {
    document.querySelector('#out').innerHTML = JSON.stringify(response);
    console.log(JSON.stringify(response));
});
</code></pre>
<p>And I use postman to send a POST request and it works as well(I get the response).</p>
<p>But when I tried to do the same thing in nodejs, it fails. The code is below.</p>
<pre><code>async function query(data) {
    const response = await fetch(
        &quot;https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2&quot;,
        {
            headers: {
                Authorization: &quot;Bearer hf_XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX&quot;,
                &quot;Content-Type&quot;: &quot;application/json&quot;
            },
            method: &quot;POST&quot;,
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}

query({&quot;inputs&quot;: &quot;Can you please let us know more details about your &quot;}).then((response) =&gt; {
    console.log(JSON.stringify(response));
});
</code></pre>
<p>Here is the error:</p>
<pre><code>node:internal/deps/undici/undici:11457
    Error.captureStackTrace(err, this);
          ^

TypeError: fetch failed
    at Object.fetch (node:internal/deps/undici/undici:11457:11)
    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)
    at async query (XXXXXXXXXXXXXX\xxx.js:2:19) {
  cause: ConnectTimeoutError: Connect Timeout Error
      at onConnectTimeout (node:internal/deps/undici/undici:8422:28)
      at node:internal/deps/undici/undici:8380:50
      at Immediate._onImmediate (node:internal/deps/undici/undici:8409:37)
      at process.processImmediate (node:internal/timers:476:21) {
    code: 'UND_ERR_CONNECT_TIMEOUT'
  }
}

Node.js v18.16.1
</code></pre>
<p>I have tested that <code>fetch</code> works well in <code>Node.js v18.16.1</code>. I don't know why I get a timeout error.</p>
<p>I have tried different libs to send POST request, like <code>http</code>, <code>https</code>, <code>axios</code> and <code>node-fetch</code>. None of them works.</p>
","huggingface"
"78308277","What is the behaviour of cosine scheduler and warm up steps when setting using epochs?","2024-04-11 05:04:11","","0","86","<huggingface-transformers><huggingface><huggingface-trainer>","<p>How does HF behave if we have epochs and warmup ration? will it use epochs * num seqs and properly set the warm up ration as I give it?</p>
<p>e.g., see full code provided</p>
<pre><code>import os
import numpy as np
import torch
from transformers import GPT2Tokenizer, GPT2LMHeadModel, TrainingArguments, Trainer
from datasets import load_dataset, load_metric
from typing import Dict, Tuple, Optional
from pathlib import Path

# Clear CUDA cache to free up memory
torch.cuda.empty_cache()

# Load the accuracy metric from the datasets library
metric = load_metric('accuracy')

def compute_metrics(eval_pred: Tuple[np.ndarray, np.ndarray]) -&gt; Dict[str, float]:
    &quot;&quot;&quot;
    Compute the accuracy of the model.

    Args:
    eval_pred: A tuple containing the model predictions and labels.

    Returns:
    A dictionary with the accuracy score.
    &quot;&quot;&quot;
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return metric.compute(predictions=predictions, references=labels)

def preprocess_function_proofnet(examples: Dict[str, list], tokenizer: GPT2Tokenizer) -&gt; Dict[str, torch.Tensor]:
    &quot;&quot;&quot;
    Preprocess the input data for the proofnet dataset.

    Args:
    examples: The examples to preprocess.
    tokenizer: The tokenizer for encoding the texts.

    Returns:
    The processed model inputs.
    &quot;&quot;&quot;
    inputs = [f&quot;{examples['nl_statement'][i]}{tokenizer.eos_token}{examples['formal_statement'][i]}&quot; for i in range(len(examples['nl_statement']))]
    model_inputs = tokenizer(inputs, max_length=512, padding=&quot;max_length&quot;, truncation=True, return_tensors=&quot;pt&quot;)
    labels = model_inputs.input_ids.clone()
    labels[labels == tokenizer.pad_token_id] = -100
    model_inputs[&quot;labels&quot;] = labels
    return model_inputs

def setup_and_train_proofnet(pretrained_model_name_or_path: str = &quot;gpt2&quot;, 
                            path: str = &quot;hoskinson-center/proofnet&quot;,
                            output_dir_val: str = '$HOME/tmp/proofnet/validation',
                            output_dir_test: str = '$HOME/tmp/proofnet/test',
                            path_to_save_model: Optional[str] = None,  # suggested path: '$HOME/tmp/proofnet/model'
                            num_train_epochs: int = 3,
                            per_device_train_batch_size: Optional[int] = 2,
                            per_device_eval_batch_size: Optional[int] = 2,
                            save_total_limit: Optional[int] = None,
                            evaluation_strategy: str = 'epoch',
                            learning_rate: float = 5e-5,
                            weight_decay: float = 0.01,
                            max_grad_norm: float = 1.0, 
                            optim='paged_adamw_32bit',
                    ) -&gt; None:
    &quot;&quot;&quot;
    Set up the environment, preprocess the dataset, and train the model.

    Args:
    tokenizer_name: The name of the tokenizer.
    model_name: The name of the model.
    dataset_path: The path to the dataset.
    &quot;&quot;&quot;
    # Load tokenizer and model
    if pretrained_model_name_or_path == &quot;gpt2&quot;:
        tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model_name_or_path, max_length=1024)
        # tokenizer.add_special_tokens({'pad_token': '[PAD]'})
        if tokenizer.pad_token_id is None:
            tokenizer.pad_token = tokenizer.eos_token
            print(f'{tokenizer.pad_token=}')
        model = GPT2LMHeadModel.from_pretrained(pretrained_model_name_or_path)
        # model.resize_token_embeddings(len(tokenizer))  # leaving for reference, not needed since pad = eos for us
        device = torch.device(f&quot;cuda:{0}&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
        model = model.to(device)
        block_size: int = tokenizer.model_max_length
        print(f'{block_size=}')

    # Load the dataset
    dataset_val = load_dataset(path, split='validation')
    dataset_test = load_dataset(path, split='test')

    # Preprocess the dataset
    if path == &quot;hoskinson-center/proofnet&quot;:
        preprocess_function = preprocess_function_proofnet
        val_dataset = dataset_val.map(lambda examples: preprocess_function(examples, tokenizer), batched=True, remove_columns=[&quot;nl_statement&quot;, &quot;formal_statement&quot;])
        test_dataset = dataset_test.map(lambda examples: preprocess_function(examples, tokenizer), batched=True, remove_columns=[&quot;nl_statement&quot;, &quot;formal_statement&quot;])

    # Training arguments
    output_dir_val: Path = Path(output_dir_val).expanduser()
    output_dir_val.mkdir(parents=True, exist_ok=True)
    training_args = TrainingArguments(
        output_dir=output_dir_val,
        evaluation_strategy=evaluation_strategy,
        learning_rate=learning_rate,
        per_device_train_batch_size=per_device_train_batch_size,
        per_device_eval_batch_size=per_device_eval_batch_size,
        weight_decay=weight_decay,
        save_total_limit=save_total_limit,
        num_train_epochs=num_train_epochs,
        max_grad_norm=max_grad_norm,
        optim=optim,
        lr_scheduler_type='cosine',
        warmup_ratio=0.1,
        fp16=False,  # never ever set to True
        bf16=torch.cuda.get_device_capability(torch.cuda.current_device())[0] &gt;= 8,  # if &gt;= 8 ==&gt; brain float 16 available or set to True if you always want fp32
    )

    # Initialize the Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=val_dataset,
        eval_dataset=test_dataset,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics
    )
    # Train the model
    trainer.train()

    # Evaluate the model
    output_dir_test: Path = Path(output_dir_test).expanduser()
    output_dir_test.mkdir(parents=True, exist_ok=True)
    results = trainer.evaluate(test_dataset)
    print(results)

    # Save the trained model
    if path_to_save_model is not None:
        path_to_save_model: Path = Path(path_to_save_model).expanduser()
        output_dir_test.mkdir(parents=True, exist_ok=True)
        model.save_pretrained(path_to_save_model)

def main() -&gt; None:
    &quot;&quot;&quot;
    Main function to execute the model training and evaluation.
    &quot;&quot;&quot;
    setup_and_train_proofnet()

if __name__ == &quot;__main__&quot;:
    import time
    start_time = time.time()
    main()
    print(f&quot;Time taken: {time.time() - start_time:.2f} seconds, or {(time.time() - start_time) / 60:.2f} minutes, or {(time.time() - start_time) / 3600:.2f} hours.\a&quot;)
</code></pre>
<p>related: <a href=""https://discuss.huggingface.co/t/is-it-possible-to-set-epoch-less-than-1-when-using-trainer/19311/2"" rel=""nofollow noreferrer"">https://discuss.huggingface.co/t/is-it-possible-to-set-epoch-less-than-1-when-using-trainer/19311/2</a></p>
<p>refs:</p>
<ul>
<li>HF discord: <a href=""https://discord.com/channels/879548962464493619/1227708244697284724/1227708244697284724"" rel=""nofollow noreferrer"">https://discord.com/channels/879548962464493619/1227708244697284724/1227708244697284724</a></li>
<li>hf dicuss: <a href=""https://discuss.huggingface.co/t/what-is-the-behaviour-of-cosine-scheduler-and-warm-up-steps-when-setting-using-epochs/81155"" rel=""nofollow noreferrer"">https://discuss.huggingface.co/t/what-is-the-behaviour-of-cosine-scheduler-and-warm-up-steps-when-setting-using-epochs/81155</a></li>
</ul>
","huggingface"
"78308203","Huggingface Trainer Finetuning While Using Multi-GPUs Get CUDA Loss Warnings and End up with CUDA Error. [../aten/src/ATen/native/cuda/Loss.cu:250]","2024-04-11 04:32:02","","0","103","<linux><pytorch><huggingface>","<p>I'm following from <a href=""https://medium.com/@csakash03/fine-tuning-llama-2-llm-on-google-colab-a-step-by-step-guide-cf7bb367e790"" rel=""nofollow noreferrer"">this</a> script and try to adapt by setting <code>device_map = &quot;auto&quot;</code> for using multiple GPUs in Docker container. Below is the server setting:</p>
<p>DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=22.04</p>
<pre><code>ubuntu@ubuntu:~$ nvidia-smi
Thu Apr 11 11:39:36 2024
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA GeForce RTX 3090        On  | 00000000:01:00.0 Off |                  N/A |
|  0%   33C    P8              33W / 350W |     12MiB / 24576MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA GeForce RTX 3090        On  | 00000000:2B:00.0 Off |                  N/A |
|  0%   37C    P8              34W / 350W |     12MiB / 24576MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA GeForce RTX 3090        On  | 00000000:41:00.0  On |                  N/A |
|  0%   33C    P8              30W / 350W |    155MiB / 24576MiB |      3%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA GeForce RTX 3090        On  | 00000000:61:00.0 Off |                  N/A |
|  0%   32C    P8              32W / 350W |     12MiB / 24576MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
</code></pre>
<p>Some Python packages version shows below:</p>
<pre><code>Package                   Version
------------------------- --------------
accelerate                0.29.2
bitsandbytes              0.43.0
deepspeed                 0.14.0
nvidia-cublas-cu12        12.1.3.1
nvidia-cuda-cupti-cu12    12.1.105
nvidia-cuda-nvrtc-cu12    12.1.105
nvidia-cuda-runtime-cu12  12.1.105
nvidia-cudnn-cu12         8.9.2.26
nvidia-cufft-cu12         11.0.2.54
nvidia-curand-cu12        10.3.2.106
nvidia-cusolver-cu12      11.4.5.107
nvidia-cusparse-cu12      12.1.0.106
nvidia-nccl-cu12          2.19.3
nvidia-nvjitlink-cu12     12.4.127
nvidia-nvtx-cu12          12.1.105
peft                      0.10.0
safetensors               0.4.2
tokenizers                0.15.2
torch                     2.2.2
transformers              4.39.3
trl                       0.8.1
</code></pre>
<p>All GPUs are running well while only using 1 GPU by the command:</p>
<pre><code>CUDA_VISIBLE_DEVICES=0 python3 train.py
CUDA_VISIBLE_DEVICES=1 python3 train.py
CUDA_VISIBLE_DEVICES=2 python3 train.py
CUDA_VISIBLE_DEVICES=3 python3 train.py
</code></pre>
<p>However, when calling all GPUs by the command:</p>
<pre><code>CUDA_VISIBLE_DEVICES=0,1,2,3 python3 train.py
</code></pre>
<p>I got the CUDA Loss warnings and end up with CUDA error.</p>
<pre><code>warnings.warn(
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t &gt;= 0 &amp;&amp; t &lt; n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t &gt;= 0 &amp;&amp; t &lt; n_classes` failed.
.
.
.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [30,0,0] Assertion `t &gt;= 0 &amp;&amp; t &lt; n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [31,0,0] Assertion `t &gt;= 0 &amp;&amp; t &lt; n_classes` failed.
</code></pre>
<pre><code>RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
</code></pre>
<p>I know the error seems caused by the number of labels is not match to the outputs, referred from <a href=""https://discuss.pytorch.org/t/assertion-t-0-t-n-classes-failed-error/133794"" rel=""nofollow noreferrer"">this</a> forum, but the totally same script is also running well with using multiple GPUs on the other server with the below setting:</p>
<p>DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=20.04</p>
<pre><code>ubuntu@ubuntu:~$ nvidia-smi
Thu Apr 11 11:52:58 2024
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA GeForce RTX 3090         Off| 00000000:01:00.0 Off |                  N/A |
|  0%   43C    P8               40W / 390W|    251MiB / 24576MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA GeForce RTX 3090         Off| 00000000:06:00.0 Off |                  N/A |
|  0%   35C    P8               44W / 390W|     10MiB / 24576MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
</code></pre>
<p>Some Python packages version shows below:</p>
<pre><code>Package                   Version     
------------------------- ------------
accelerate                0.24.1       
bitsandbytes              0.41.1             
deepspeed                 0.11.1     
nvidia-cublas-cu12        12.1.3.1    
nvidia-cuda-cupti-cu12    12.1.105    
nvidia-cuda-nvrtc-cu12    12.1.105    
nvidia-cuda-runtime-cu12  12.1.105    
nvidia-cudnn-cu12         8.9.2.26    
nvidia-cufft-cu12         11.0.2.54   
nvidia-curand-cu12        10.3.2.106  
nvidia-cusolver-cu12      11.4.5.107  
nvidia-cusparse-cu12      12.1.0.106  
nvidia-nccl-cu12          2.18.1      
nvidia-nvjitlink-cu12     12.3.52     
nvidia-nvtx-cu12          12.1.105    
peft                      0.5.0      
safetensors               0.4.0
sentencepiece             0.1.99
tokenizers                0.14.1      
torch                     2.1.0     
transformers              4.34.1     
trl                       0.7.11
</code></pre>
<p>Also, I've referred to <a href=""https://huggingface.co/docs/transformers/perf_train_gpu_many#number-of-gpus"" rel=""nofollow noreferrer"">this</a> doc but stuck by the command:</p>
<pre><code>CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --nproc-per-node=4 train.py
</code></pre>
<p>I'm not sure if there are some CUDA installing, setting or others difference between both servers that I missed</p>
<p>For UBUNTU, CUDA, Python packages version difference, I tried to install the same environment on the first server but got the same warnings and error.
I also tried to build the environment on host got the same warnings and error, too.</p>
","huggingface"
"78295575","Error when running meta-llama/Llama-2-7b-chat-hf from hugging face, I don't understand where I am going wrong","2024-04-09 00:19:23","","0","259","<importerror><huggingface><llama>","<p>The code that I am running is:</p>
<pre><code>import torch
from llama_index.llms.huggingface import HuggingFaceLLM

llm = HuggingFaceLLM(
    context_window=4096,
    max_new_tokens=256,
    generate_kwargs={&quot;temperature&quot;: 0.0, &quot;do_sample&quot;: False},
    system_prompt = &quot;You need to create proposal documents for the information that we are giving to you. This proposal has to be 5 paragraphs and answered well. If you need more information regarding the topic, you can reply with what furhter information could help&quot;,
    ## add the description of what I want it to work on
    query_wrapper_prompt = &quot;&lt;|USER|&gt;{query_str}&lt;|ASSISTANT|&gt;&quot;,
    tokenizer_name=&quot;meta-llama/Llama-2-7b-chat-hf&quot;,
    model_name=&quot;meta-llama/Llama-2-7b-chat-hf&quot;,
    device_map=&quot;auto&quot;,
    # uncomment this if using CUDA to reduce memory usage
    model_kwargs={&quot;torch_dtype&quot;: torch.float16 , &quot;load_in_8bit&quot;:True}
)
</code></pre>
<p>The error that I am getting from trying to run this is:</p>
<pre><code>ImportError                               Traceback (most recent call last)
&lt;ipython-input-34-0d2d206e16bb&gt; in &lt;cell line: 4&gt;()
      2 from llama_index.llms.huggingface import HuggingFaceLLM
      3 
----&gt; 4 llm = HuggingFaceLLM(
      5     context_window=4096,
      6     max_new_tokens=256,

3 frames
/usr/local/lib/python3.10/dist-packages/transformers/quantizers/quantizer_bnb_8bit.py in validate_environment(self, *args, **kwargs)
     60     def validate_environment(self, *args, **kwargs):
     61         if not (is_accelerate_available() and is_bitsandbytes_available()):
---&gt; 62             raise ImportError(
     63                 &quot;Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` &quot;
     64                 &quot;and the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`&quot;

ImportError: Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`

---------------------------------------------------------------------------
NOTE: If your import is failing due to a missing package, you can
manually install dependencies using either !pip or !apt.
</code></pre>
<p>To view examples of installing some common dependencies, click the
&quot;Open Examples&quot; button below.</p>
<p>I have tried pip installing the libraries once again and am still getting an error. If you could help with this I would be very appreciative of it.</p>
","huggingface"
"78294720","Error when calling Hugging Face load_dataset(""glue"", ""mrpc"")","2024-04-08 19:21:13","78310683","0","196","<python><nlp><huggingface><huggingface-datasets>","<p>I'm following the huggingface tutorial <a href=""https://huggingface.co/learn/nlp-course/chapter3/2?fw=pt"" rel=""nofollow noreferrer"">here</a> and it's giving me a strange error. When I run the following code:</p>
<pre><code>from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding
from torch.utils.data import DataLoader

raw_datasets = load_dataset(&quot;glue&quot;, &quot;mrpc&quot;)
</code></pre>
<p>Here is what I see:</p>
<pre><code>Downloading data: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 151k/151k [00:00&lt;00:00, 3.35MB/s]
Downloading data: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11.1k/11.1k [00:00&lt;00:00, 6.63MB/s]
Downloading data files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:32&lt;00:00, 10.89s/it]
Extracting data files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00&lt;00:00, 127.92it/s]
Traceback (most recent call last):
  File &quot;/Users/ameenizhac/Downloads/transformers_playground.py&quot;, line 5, in &lt;module&gt;
    raw_datasets = load_dataset(&quot;glue&quot;, &quot;mrpc&quot;)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/load.py&quot;, line 1782, in load_dataset
    builder_instance.download_and_prepare(
  File &quot;/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/builder.py&quot;, line 872, in download_and_prepare
    self._download_and_prepare(
  File &quot;/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/builder.py&quot;, line 967, in _download_and_prepare
    self._prepare_split(split_generator, **prepare_split_kwargs)
  File &quot;/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/builder.py&quot;, line 1709, in _prepare_split
    split_info = self.info.splits[split_generator.name]
                 ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/splits.py&quot;, line 530, in __getitem__
    instructions = make_file_instructions(
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/arrow_reader.py&quot;, line 112, in make_file_instructions
    name2filenames = {
                     ^
  File &quot;/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/arrow_reader.py&quot;, line 113, in &lt;dictcomp&gt;
    info.name: filenames_for_dataset_split(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/naming.py&quot;, line 70, in filenames_for_dataset_split
    prefix = filename_prefix_for_split(dataset_name, split)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/naming.py&quot;, line 54, in filename_prefix_for_split
    if os.path.basename(name) != name:
       ^^^^^^^^^^^^^^^^^^^^^^
  File &quot;&lt;frozen posixpath&gt;&quot;, line 142, in basename
TypeError: expected str, bytes or os.PathLike object, not NoneType
</code></pre>
<p>I don't know where to start because I don't understand where the error is coming from.</p>
","huggingface"
"78289403","rsortino/ColorizeNet does not appear to have a file named config.json","2024-04-07 21:48:54","","0","81","<python><huggingface-transformers><huggingface><huggingface-tokenizers>","<p>I want to run <a href=""https://huggingface.co/rsortino/ColorizeNet"" rel=""nofollow noreferrer"">https://huggingface.co/rsortino/ColorizeNet</a> locally on my PC with a GPU. I followed the instructions at <a href=""https://huggingface.co/docs/transformers/installation"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/installation</a> (all i did was conda install conda-forge::transformers) to set this up, and came up with the following code:</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModel

tokenizer = AutoTokenizer.from_pretrained(&quot;rsortino/ColorizeNet&quot;)
model = AutoModelForSeq2SeqLM.from_pretrained(&quot;rsortino/ColorizeNet&quot;)

tokenizer.save_pretrained(&quot;colorizeNet&quot;)
model.save_pretrained(&quot;colorizeNet&quot;)

tokenizer = AutoTokenizer.from_pretrained(&quot;colorizeNet&quot;)
model = AutoModel.from_pretrained(&quot;colorizeNet&quot;)
</code></pre>
<p>But when I run it, I get the following error:</p>
<pre><code>Exception has occurred: OSError
rsortino/ColorizeNet does not appear to have a file named config.json. Checkout 'https://huggingface.co/rsortino/ColorizeNet/main' for available files.
</code></pre>
<p>Which makes sense - the repo does indeed not have a config.json file. How can I get this model running? I'm new to running models and to huggingface.</p>
","huggingface"
"78289274","How to format data for Hugging Face seamless-m4t-v2-large model deployed on Sagemaker for T2TT","2024-04-07 20:56:42","","0","115","<python><amazon-sagemaker><huggingface>","<p>I have deployed seamless-m4t-v2-large model from Hugging face to Sagemaker endpoint and would like to call predict on it but i can't any resources on how to format data for .predict() method in case of text-to-text translation.</p>
<p>Here are HF details:</p>
<pre><code>hub = {
    'HF_MODEL_ID':'facebook/seamless-m4t-v2-large',
    'HF_TASK':'automatic-speech-recognition'
}
</code></pre>
<p>I have tried cases similar to:</p>
<pre><code>data = { 
  &quot;inputs&quot;: &quot;This is a sentence in English.&quot;,
  &quot;task&quot;: &quot;t2tt&quot;,
  &quot;tgt_lang&quot;: &quot;fr&quot;,
  &quot;src_lang&quot;: &quot;en&quot;
}
result = predictor.predict(data)
</code></pre>
<p>but i'm getting:</p>
<p>An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from primary and could not load the entire response body.</p>
<p>Also this model is marked as automatic-speech-recognition on HF so i wonder should i keep it or maybe change it to something more suitable?</p>
<p>Any help is appreciated</p>
","huggingface"
"78285528","Huggingface space in javascript with gradio-lite","2024-04-06 18:49:14","","0","32","<artificial-intelligence><huggingface><gradio>","<p>I want to implement huggingface text-to-image space in javascript without node.js with gradio-lite but when I try this, I get error, you can read <a href=""https://huggingface.co/blog/gradio-lite"" rel=""nofollow noreferrer"">documentation</a> and please give me only example of js code for any good huggingface text-to-image space with part for entering prompt and button and part for generating image with gradio-lite!</p>
","huggingface"
"78283502","llama-index with huggingfaceinterface api not returning the entire answer","2024-04-06 06:53:27","","0","70","<python><large-language-model><huggingface><llama-index><mistral-7b>","<p>I am building a RAG app with llama-index to extract information from invoice pdfs,here is how I am generating the query</p>
<pre><code>    llm = HuggingFaceInferenceAPI(
        model_name=&quot;https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2&quot;,
        token=HF_TOKEN
    )
    index = build_index(CHUNK_SIZE, llm, embeddings, client, INDEX_NAME)
    query_engine = index.as_query_engine(
        streaming=False,
        response_mode=&quot;compact&quot;
    )
</code></pre>
<p>but the output is</p>
<pre><code>invoice_number: 61356291
invoice_date: 09/06/2012
client_name: Rodriguez-Stevens
client_address: 2280 Angela Plain, Hortonshire, MS 93248
client_tax_id: 939-98-8477
seller_name: Chapman, Kim and Green
seller_address: 64731 James Branch, Smithmouth, NC 26872
seller_tax_id: 949-84-9105
iban: GB50ACIE59715038217063
names_of_invoice_items: [&quot;Wine Glasses Goblets Pair Clear&quot;, &quot;With Hooks Stemware Storage Multiple Uses Iron Wine Rack Hanging Glass&quot;, &quot;Replacement Corkscrew Parts Spiral Worm Wine Opener Bottle Houdini&quot;, &quot;HOME ESSENTIALS GRADIENT STEMLESS WINE GLASSES SET OF
</code></pre>
<p>as you can see the output is incomplete there were supposed to be 2 more rows why the output is not complete?</p>
<p>the output should be complete</p>
","huggingface"
"78281638","Why does this not generate an response?","2024-04-05 17:58:40","","0","28","<artificial-intelligence><chatbot><huggingface>","<p>I used a youtube Tutorial and a lot of research + Help to write this and I don't know why it is not generating an Answer. Maybe it could be the model I chose but at this time I tried I thin 4 or 5 models.</p>
<p>Here's the Code I've written:</p>
<pre><code>import os
import time
import pyaudio
import playsound
from gtts import gTTS
import speech_recognition as sr
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch 
import uuid


huggingface_token = &quot;ACCES TOKEN (I just deletet it for StackOverflow)&quot;  

lang = &quot;en&quot;


tokenizer = AutoTokenizer.from_pretrained(&quot;cognitivecomputations/dolphin-2.8-mistral-7b-v02&quot;)
model = AutoModelForCausalLM.from_pretrained(&quot;cognitivecomputations/dolphin-2.8-mistral-7b-v02&quot;)


def get_audio():
    r = sr.Recognizer()
    with sr.Microphone(device_index=1) as source:
        print(&quot;Listening...&quot;)
        audio = r.listen(source)
        said = &quot;&quot;

        try:
            said = r.recognize_google(audio)
            print(said)

            if &quot;Jarvis&quot; in said:
               
                input_ids = tokenizer.encode(said, return_tensors=&quot;pt&quot;)

                attention_mask = torch.ones_like(input_ids)

               
                output = model.generate(input_ids, attention_mask=attention_mask, pad_token_id=tokenizer.eos_token_id, max_new_tokens=40)  
                print(output)
                generated_text = tokenizer.decode(output[0], skip_special_tokens=True)

               
                speech = gTTS(text=generated_text, lang=lang, slow=False, tld=&quot;com.au&quot;)
                file_name = f&quot;welcome_{str(uuid.uuid4())}.mp3&quot;
                speech.save(file_name)
                playsound.playsound(file_name)

        except Exception as e:
            print(f&quot;Exception: {str(e)}&quot;)

    return said


while True:
    get_audio()
</code></pre>
<p>I really have no clue why it is not generating anything, and btw it really eats up all my ram, which is another problem I should look into.</p>
","huggingface"
"78280644","Getting 'KeyError' that i can't contain ""."" in directory","2024-04-05 14:38:27","","0","94","<pytorch><huggingface>","<p>I'm trying to merge LoRA to base model.</p>
<p>I got
KeyError: 'module name can't contain &quot;.&quot;, got: C:/Users/tjs/.cache/huggingface/FinGPT'</p>
<p>It's just worked yesterday, but not now.
Since i'm just a newbie, and only know basics, i have no idea how to solve this problem.</p>
<pre><code>import torch
import os
import logging
import argparse
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel


base_model_name_or_path = 'NousResearch/Llama-2-13b-hf'
peft_model_path = 'FinGPT/fingpt-sentiment_llama2-13b_lora'
output_dir = (&quot;C:/Users/tjs/.cache/huggingface/FinGPT&quot;)
device = &quot;auto&quot;
push_to_hub = False

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


try:
    if device == 'auto':
        device_arg = {'device_map': 'auto'}
    else:
        device_arg = {'device_map': {&quot;&quot;: device}}

    logger.info(f&quot;Loading base model: {base_model_name_or_path}&quot;)
    with tqdm(total=1, desc=&quot;Loading base model&quot;) as pbar:
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name_or_path,
            return_dict=True,
            torch_dtype=torch.float16,
            **device_arg
        )
        pbar.update(1)  

    logger.info(f&quot;Loading Peft: {peft_model_path}&quot;)
    with tqdm(total=1, desc=&quot;Loading Peft model&quot;) as pbar:
        model = PeftModel.from_pretrained(base_model, peft_model_path, output_dir)
        pbar.update(1)

    logger.info(&quot;Running merge_and_unload&quot;)
    with tqdm(total=1, desc=&quot;Merge and Unload&quot;) as pbar:
        model = model.merge_and_unload()
        pbar.update(1)

    tokenizer = AutoTokenizer.from_pretrained(base_model_name_or_path)

    model.save_pretrained(f&quot;{output_dir}&quot;)
    tokenizer.save_pretrained(f&quot;{output_dir}&quot;)
    logger.info(f&quot;Model saved to {output_dir}&quot;)

except Exception as e:
    logger.exception(&quot;An error occurred:&quot;)
    raise
</code></pre>
<p>I found it could be the problem of version, which didn't worked for me.
And i also tried to remove &quot;.&quot;, but changing the directory seemed to be impossible.</p>
","huggingface"
"78271828","Tensor size error when generating embeddings for documents using HuggingFace pre-trained models","2024-04-04 05:23:42","78271859","1","101","<huggingface-transformers><large-language-model><word-embedding><huggingface><pre-trained-model>","<p>I am trying to get document embeddings using pre-trained models in the HuggingFace Transformer library. The input is a document, the output is an embedding for this document using a pre-trained model. But I got an error as below and don't know how to fix it.</p>
<p>Code:</p>
<pre><code>from transformers import pipeline, AutoTokenizer, AutoModel
from transformers import RobertaTokenizer, RobertaModel
import fitz
from openpyxl import load_workbook
import os
from tqdm import tqdm

PRETRAIN_MODEL = 'distilbert-base-cased'
DIR = &quot;dataset&quot;

# Load and process the text
all_files = os.listdir(DIR)
pdf_texts = {}
for filename in all_files:
    if filename.lower().endswith('.pdf'):
        pdf_path = os.path.join(DIR, filename)
        with fitz.open(pdf_path) as doc:
            text_content = &quot;&quot;
            for page in doc:
                text_content += page.get_text()
            text = text_content.split(&quot;PUBLIC CONSULTATION&quot;)[0]
            project_code = os.path.splitext(filename)[0]
            pdf_texts[project_code] = text 

# Generate embeddings for the documents
tokenizer = AutoTokenizer.from_pretrained(PRETRAIN_MODEL)
model = AutoModel.from_pretrained(PRETRAIN_MODEL)
pipe = pipeline('feature-extraction', model=model, tokenizer=tokenizer)

embeddings = {}
for project_code, text in tqdm(pdf_texts.items(), desc=&quot;Generating embeddings&quot;, unit=&quot;doc&quot;):
    embedding = pipe(text, return_tensors=&quot;pt&quot;)
    embeddings[project_code] = embedding[0][0].numpy()
</code></pre>
<p>Error:</p>
<p>The error happens to the line <code>embedding = pipe(text, return_tensors=&quot;pt&quot;)</code>. The output is as follows:</p>
<pre><code>Generating embeddings:   0%|          | 0/58 [00:00&lt;?, ?doc/s]Token indices sequence length is longer than the specified maximum sequence length for this model (3619 &gt; 512). Running this sequence through the model will result in indexing errors
Generating embeddings:   0%|          | 0/58 [00:00&lt;?, ?doc/s]
RuntimeError: The size of tensor a (3619) must match the size of tensor b (512) at non-singleton dimension 1
</code></pre>
<p>The input documents: <a href=""https://drive.google.com/file/d/17yFOR0dQ8UMbefFed5QPZUXqU0vzifUw/view?usp=sharing"" rel=""nofollow noreferrer"">https://drive.google.com/file/d/17yFOR0dQ8UMbefFed5QPZUXqU0vzifUw/view?usp=sharing</a></p>
<p>Thank you!</p>
","huggingface"
"78270745","How to Predict New Data Topics with BERTopic Model Loaded from Hugging Face in Python? (Automated solution)","2024-04-03 22:11:01","","0","318","<python><nlp><huggingface>","<p>I've developed a BERTopic model for analyzing mobile app reviews and have successfully pushed it to Hugging Face. I'm able to load the model in my Python script but am facing challenges in predicting topics for new incoming feedback.</p>
<p>My goal is to automatically assign a topic to new app feedback using the trained BERTopic model. Here's the workflow I've implemented:</p>
<p>I load the BERTopic model from Hugging Face.
I use the SentenceTransformer model to encode the new documents (app feedback).
I attempt to transform the new documents using the loaded BERTopic model to predict their topics.</p>
<p>However, I'm uncertain about the last part of my code, where I aim to merge the predicted topics with the original dataset containing app feedback. I want to end up with a dataset that includes the original feedback and the assigned topic names, ideally with automatic incrementation for new data.</p>
<p>Here's the code snippet I'm using:</p>
<pre><code>from huggingface_hub import login
from bertopic import BERTopic
from sentence_transformers import SentenceTransformer
import pandas as pd

# Login to Hugging Face
access_token_read = &quot;hf_xynbKhivF..........&quot;
login(token=access_token_read)

# Load the BERTopic model
model = BERTopic.load(&quot;shantanudave/BERTopic_ArXiv&quot;)

# Sample new document

new_docs = [&quot;There is an issue with payment on the checkout.&quot;]

# Generate embeddings for the new document
embedding_model = SentenceTransformer(&quot;all-MiniLM-L6-v2&quot;)
embeddings = embedding_model.encode(new_docs, show_progress_bar=True)

# Predict the topic for the new document
new_topics, new_probs = model.transform(new_docs, embeddings)

# Code for merging predicted topics with the original dataset (uncertain part)
...

</code></pre>
<p>Questions:</p>
<p>Is the approach I'm using to predict and assign topics to new feedback using BERTopic correct?
How can I effectively merge the predicted topics back into the original dataset, ensuring that each piece of feedback gets its corresponding topic and topic name?</p>
<pre><code>#My manual approach : - 

T = topic_model.get_document_info(docs)
T_df = T[['Document', 'Topic', 'Name', 'CustomName']]
# Remove duplicates from this new DataFrame
T_clean = T_df.drop_duplicates(subset=['Document'], keep='first')
T_clean

# Performing a left join where 'Document' in T_clean matches 'eng_content' in df , 'eng_content' is the review/feedback

merged_df = pd.merge(df, T_clean, how='left', left_on='eng_content', right_on='Document')
topic_doc_df_1 = merged_df
# Rename columns
topic_doc_df_1 = topic_doc_df_1.rename(columns={
    'CustomName': 'Topic_name',
    'score': 'Rating',
    'date': 'Date',
    'language': 'Language',
    'sentiment': 'Sentiment',
    'probability': 'Probability',
    'app_version': 'App_version'
})

# Rearrange columns
topic_doc_df_1 = topic_doc_df_1[['Topic', 'Topic_name', 'Document', 'Rating', 'Date', 'Language', 'Sentiment', 'Probability', 'App_version']]

topic_doc_df_1

![image](https://github.com/MaartenGr/BERTopic/assets/54185486/d1a96b23-7766-4b34-b615-4a9ce8e804f9)


</code></pre>
<p>I have another function clean_doc() which basically reads new data from df['eng_content'] and provide list 'new_docs'</p>
<p>I am sure there must be a better way of doing this that I am missing,</p>
<p>Thanks in advance,
Shantanu</p>
","huggingface"
"78270409","LayoutLM data format for bounding box classification","2024-04-03 20:43:39","","0","169","<nlp><huggingface>","<p>I'm following the LayoutLMV3 <a href=""https://github.com/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv3/Fine_tune_LayoutLMv3_on_FUNSD_(HuggingFace_Trainer).ipynb"" rel=""nofollow noreferrer"">tutorial</a> on the FUNSD dataset for token classification. However, my use case is classifying bounding boxes from an image (1 label per bounding box).</p>
<p>There's <a href=""https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForSequenceClassification_on_RVL_CDIP.ipynb#scrollTo=6KKtDucCuKOg"" rel=""nofollow noreferrer"">another</a> tutorial for text classification, but it's different enough in that each document just gets classified (as opposed to each bounding box).</p>
<p>In my use case, each bounding box can have one or more words associated with it and only 1 label.  I don't want to feed a token-label pair for each token into the model.  I'm having trouble figuring out how to set up the pre-processing so that I get the data into this format.</p>
<p>Here is an almost reproducible example (you'll have to just use 2 dummy images and specify their path from your machine):</p>
<pre class=""lang-py prettyprint-override""><code>
from datasets import Dataset, Features, Sequence, ClassLabel, Value, Array2D, Array3D
import pandas as pd
from PIL import Image
from transformers import AutoProcessor
from transformers.data.data_collator import default_data_collator

img_dict = {}

# specify names of any 2 local jpeg images
files = ['file1', 'file2']

for file in files:
    file_path = f&quot;./your_direc/{file}.jpg&quot;
    image = Image.open(file_path).convert('RGB')
    img_dict[file] = image

# 2 rows - each row has 3 bounding boxes, with 1 label for each bounding box, and or more words per BB
df = pd.DataFrame({'label': [[&quot;yes&quot;, &quot;no&quot;, &quot;maybe&quot;], [&quot;yes&quot;, &quot;yes&quot;, &quot;maybe&quot;]],
                   'text': [['potato', 'strange feeling', 'hydrogen'],['cat', 'man smiles', 'sun rises']],
                   'bbox': [[[3, 5, 10, 20], [0, 1, 8, 4], [4,5, 9, 15]], [[4, 6, 12, 23], [2, 3, 11, 6], [5,6,11,13]]],
                   'file': files})


data_dict = {}
data_dict['id'] = df.index
data_dict['label'] = df.label
data_dict['text'] = df.text

data_dict['bbox'] = df.bbox
# include the image reference
data_dict['image'] = [img_dict[i] for i in df.file]

data_temp = Dataset.from_dict(data_dict)

full_dataset = data_temp.cast_column('label', Sequence(ClassLabel(names=[&quot;yes&quot;, &quot;no&quot;, &quot;maybe&quot;])))

features = full_dataset.features
column_names = full_dataset.column_names

image_column_name = &quot;image&quot;
text_column_name = &quot;text&quot;
boxes_column_name = &quot;bbox&quot;
label_column_name = &quot;label&quot;

def get_label_list(labels):
    unique_labels = set()
    for label in labels:
        unique_labels = unique_labels | set(label)
    label_list = list(unique_labels)
    label_list.sort()
    return label_list

if isinstance(features[label_column_name], ClassLabel):
    label_list = features[label_column_name].names
    # No need to convert the labels since they are already ints.
    id2label = {k: v for k,v in enumerate(label_list)}
    label2id = {v: k for k,v in enumerate(label_list)}
else:
    label_list = get_label_list(full_dataset[label_column_name])
    id2label = {k: v for k,v in enumerate(label_list)}
    label2id = {v: k for k,v in enumerate(label_list)}
num_labels = len(label_list)

processor = AutoProcessor.from_pretrained(&quot;microsoft/layoutlmv3-base&quot;, apply_ocr=False)

def prepare_examples(examples):
  images = examples[image_column_name]
  words = examples[text_column_name]
  boxes = examples[boxes_column_name]
  word_labels = examples[label_column_name]

  encoding = processor(images, words, boxes=boxes, word_labels=word_labels,
                       truncation=True, padding=&quot;max_length&quot;)

  return encoding

# we need to define custom features for `set_format` (used later on) to work properly
features = Features({
    'pixel_values': Array3D(dtype=&quot;float32&quot;, shape=(3, 224, 224)),
    'input_ids': Sequence(feature=Value(dtype='int64')),
    'attention_mask': Sequence(Value(dtype='int64')),
    'bbox': Array2D(dtype=&quot;int64&quot;, shape=(512, 4)),
    'labels': Sequence(feature=Value(dtype='int64')),
})

train_dataset = full_dataset.map(
    prepare_examples,
    batched=True,
    remove_columns=column_names,
    features=features,
)
</code></pre>
<p>If I run the below, the output is 512:
<code>len(train_dataset['labels'][0])</code></p>
<p>And if I run this:
<code>processor.tokenizer.decode(train_dataset[0][&quot;input_ids&quot;])</code></p>
<p>I get (padded to 512):
<code>'&lt;s&gt; potato strange feeling hydrogen&lt;/s&gt;&lt;pad&gt;......&lt;pad&gt;'</code></p>
<p>What I would want to see is instead 3 labels for <code>train_datasets['label'][0]</code> (1 for each bounding box).
And for the input input's for <code>train_dataset[0]['input_ids']</code> I'd want to see a list of 3, with each inner list containing a variable number of tokens.</p>
<p>I've tried modifying <code>prepare_examples</code>:</p>
<pre class=""lang-py prettyprint-override""><code>def prepare_examples(examples):
    input_ids = []
    attention_masks = []
    pixel_values = []

    texts = examples[&quot;text&quot;]
    bboxes = examples[&quot;bbox&quot;]
    labels = examples[&quot;label&quot;]
    images = examples[&quot;image&quot;]

    for text, image in zip(texts, images):
        tokenized_sequences = []
        for text_seq in text:
            # Assuming that text is a string, and each bounding box corresponds to a substring of text
            tokenized_seq = processor.tokenizer(text_seq, truncation=True, max_length=30, padding=&quot;max_length&quot;, return_tensors=&quot;pt&quot;)
            tokenized_sequences.append(tokenized_seq)

        input_ids.append([seq[&quot;input_ids&quot;].squeeze() for seq in tokenized_sequences])
        attention_masks.append([seq[&quot;attention_mask&quot;].squeeze() for seq in tokenized_sequences])

        # Encode the image to get the pixel_values
        encoded_image = processor.encode_image(image)
        pixel_values.append(encoded_image)

    return {
        &quot;input_ids&quot;: input_ids,
        &quot;attention_mask&quot;: attention_masks,
        &quot;labels&quot;: labels,
        &quot;bbox&quot;: bboxes,
        &quot;pixel_values&quot;: pixel_values,
    }
</code></pre>
<p>But get an error when I try to use that one:
<code>ValueError: Words must be of type </code>List[str]<code>(single pretokenized example), or</code>List[List[str]]<code>(batch of pretokenized examples).</code></p>
","huggingface"
"78267762","Quantization and torch_dtype in huggingface transformer","2024-04-03 12:48:06","78272137","1","603","<huggingface-transformers><huggingface><quantization>","<p>Not sure if its the right forum to ask but.</p>
<p>Assuming i have a <code>gptq</code> model that is <code>4bit</code>. how does using <code>from_pretrained(torch_dtype=torch.float16)</code> work? In my understanding 4 bit meaning changing the weights from either <code>32-bit precision</code> to <code>4bit precision</code> using quantization methods.</p>
<p>However, calling it the <code>torch_dtype=torch.float16</code> would mean the weights are in <code>16 bits</code>? Am i missing something here.</p>
","huggingface"
"78266961","Model.save_pretrained is not saving .bin files","2024-04-03 10:18:46","","0","265","<huggingface-transformers><huggingface><fine-tuning><huggingface-trainer>","<pre><code>args = TrainingArguments(
    output_dir=&quot;&quot;output_bert_training2&quot;&quot;,
    num_train_epochs=2,
    per_device_train_batch_size=8,
       weight_decay=0.01,
   evaluation_strategy=&quot;&quot;epoch&quot;&quot;,
   save_strategy=&quot;&quot;epoch&quot;&quot;,
    load_best_model_at_end=True,
    metric_for_best_model=&quot;&quot;eval_loss&quot;&quot;,

)
trainer = Trainer(
    model=model,
    args=args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics,
     callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],
)
trainer.train()
trainer.save_model('CustomModel_4000_2')
</code></pre>
<p>Attempted to save the model using trainer.save_model(model_path)
Expected that upon saving the model using trainer.save_model(model_path), all necessary files including model.bin would be saved.
Observed that only the files training_args.bin, model.safetensors, and config.json were saved, while model.bin was not included.</p>
","huggingface"
"78266370","ValueError: Expected EmbeddingFunction.__call__ to have the following signature: odict_keys(['self', 'input']), got odict_keys(['args', 'kwargs'])","2024-04-03 08:38:50","","0","87","<google-api><langchain><huggingface>","<p>1st Step:
<a href=""https://i.sstatic.net/SxEuK.png"" rel=""nofollow noreferrer"">I have been trying to use</a></p>
<p>2nd Step:
<a href=""https://i.sstatic.net/hCfqU.png"" rel=""nofollow noreferrer"">I have attached the output image</a></p>
<p>3rd step:
[vectorstore = Chroma.from_texts(to_vectorize, embeddings, metadatas=few_shots)]</p>
<p><a href=""https://i.sstatic.net/pyouQ.png"" rel=""nofollow noreferrer"">Got a error</a></p>
<p>Overview:
I am working on an end-to-end LLM SQL project where users can ask questions and they will get the output</p>
","huggingface"
"78264382","Weight and shape different than the number of channels in input","2024-04-02 22:54:13","78281923","-1","232","<python><pytorch><huggingface>","<p>I'm trying to fine-tunning the VAE of <a href=""https://huggingface.co/CompVis/stable-diffusion-v1-4/tree/main/vae"" rel=""nofollow noreferrer"">SD 1.4</a></p>
<p>I'm in a multi gpu environment, and I'm using <code>accelerate</code> library for handling that.
This is my code summarized:</p>
<pre><code>import os
import torch.nn.functional as F
import yaml
from PIL import Image
import torch
from torch.utils.data import Dataset, DataLoader
from torchvision.transforms import Compose, Resize, ToTensor, Normalize
from diffusers import AutoencoderKL
from torch.optim import Adam
from accelerate import Accelerator
from torch.utils.tensorboard import SummaryWriter

# Load configuration
with open('config.yaml', 'r') as file:
    config = yaml.safe_load(file)

def save_checkpoint(model, optimizer, epoch, step, filename=&quot;checkpoint.pth.tar&quot;):
    checkpoint = {
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'epoch': epoch,
        'step': step
    }
    torch.save(checkpoint, filename)

class ImageDataset(Dataset):
    def __init__(self, root_dir, transform=None):
        self.root_dir = root_dir
        self.transform = transform
        self.images = [os.path.join(root_dir, f) for f in os.listdir(root_dir) if f.endswith('.png')]

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        img_path = self.images[idx]
        image = Image.open(img_path).convert('RGB')
        if self.transform:
            image = self.transform(image)
        return image


# Setup dataset and dataloader based on config
transform = Compose([
    Resize((config['dataset']['image_size'], config['dataset']['image_size'])),
    ToTensor(),
    Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
])

dataset = ImageDataset(root_dir=config['dataset']['root_dir'], transform=transform)
dataloader = DataLoader(dataset, batch_size=config['training']['batch_size'], shuffle=True, num_workers=config['training']['num_workers'])

# Initialize model, accelerator, optimizer, and TensorBoard writer
device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
model_path = config['model']['path']
vae = AutoencoderKL.from_pretrained(model_path).to(device)

optimizer = Adam(vae.parameters(), lr=config['training']['learning_rate'])
accelerator = Accelerator()
vae, dataloader = accelerator.prepare(vae, dataloader)

writer = SummaryWriter()

# Training loop
for epoch in range(config['training']['num_epochs']):
    vae.train()
    total_loss = 0
    for step, batch in enumerate(dataloader):
        with accelerator.accumulate(vae):
            # Assuming the first element of the batch is the image
            target = batch[0].to(next(vae.parameters()).dtype)
            
            # Access the original model for custom methods
            model = vae.module if hasattr(vae, &quot;module&quot;) else vae
            
            posterior = model.encode(target).latent_dist
            z = posterior.mode()
            pred = model.decode(z).sample
            
            kl_loss = posterior.kl().mean()
            mse_loss = F.mse_loss(pred, target, reduction=&quot;mean&quot;)
            
            loss = mse_loss + config['training'][&quot;kl_scale&quot;] * kl_loss

            optimizer.zero_grad()
            accelerator.backward(loss)
            optimizer.step()
            optimizer.zero_grad()  # Clear gradients after updating weights

            # Checkpointing every 10 steps
            if step % 10 == 0:
                checkpoint_path = f&quot;checkpoint_epoch_{epoch}_step_{step}.pth&quot;
                accelerator.save({
                    &quot;epoch&quot;: epoch,
                    &quot;model_state_dict&quot;: model.state_dict(),
                    &quot;optimizer_state_dict&quot;: optimizer.state_dict(),
                    &quot;loss&quot;: loss,
                }, checkpoint_path)
                print(f&quot;Checkpoint saved to {checkpoint_path}&quot;)


writer.close()

print(&quot;Training complete.&quot;)
</code></pre>
<p>When running the code, I got the following error:</p>
<pre><code>RuntimeError: Expected weight to be a vector of size equal to the number of channels in input, but got weight of shape [128] and input of shape [128, 1024, 1024]: 
</code></pre>
<p>My input folder contains a set of png images with different sizes, and resized to 1024x1024 in the configuration file.</p>
<p>I do not know why this is happening and if someone knows, or if there is a easier way to fine-tunning the VAE weights using my images.
Thanks.</p>
<p>Edit:
My <code>config.yaml</code> file</p>
<pre><code>model:
  path: 'vae1dot4'  # Path to your pre-trained model directory

dataset:
  root_dir: 'segmented'  # Directory containing your PNG images
  image_size: 1024  # Target size for image resizing

training:
  batch_size: 8  # Batch size for training
  num_epochs: 10  # Number of epochs to train
  learning_rate: 0.0005  # Learning rate for the optimizer
  num_workers: 4  # Number of worker processes for data loading
  kl_scale: 1
  gradient_accumulation_steps: 1
logging:
  tensorboard_dir: 'runs'  # Directory for TensorBoard logs
</code></pre>
","huggingface"
"78263807","How to All Utilize all GPU's when device=""balanced_low_0"" in GPU setting for Huggingface Model","2024-04-02 20:05:40","","0","91","<nlp><huggingface-transformers><huggingface><huggingface-tokenizers><mixtral-8x7b>","<p>While loading the model in &quot;balanced_low_0&quot; GPU setting the model is loaded into all GPU's apart from 0: GPU. Where the 0: GPU is left to do the text inference. (i.e. text inference as in performing all the calculation to generate response inside the LLM)</p>
<p>So, as per the given device parameter my model is loaded onto 1,2,3 GPU's and 0: GPU is left for inference.</p>
<p>| ID | GPU | MEM |</p>
<p>| 0 | 0% | 3% |</p>
<p>| 1 | 0% | 83% |</p>
<p>| 2 | 0% | 82% |</p>
<p>| 3 | 0% | 76% |</p>
<p><strong>Question: How can i also utilize the remaining 1,2,3 GPU's to perform text inference not only 0:GPU?</strong></p>
<p>Context: &quot;balanced_low_0&quot; evenly splits the model on all GPUs except the first one, and only puts on GPU 0 what does not fit on the others. This option is great when you need to use GPU 0 for some processing of the outputs, like when using the generate function for Transformers models</p>
<p>Reference: <a href=""https://huggingface.co/docs/accelerate/en/concept_guides/big_model_inference#designing-a-device-map"" rel=""nofollow noreferrer"">https://huggingface.co/docs/accelerate/en/concept_guides/big_model_inference#designing-a-device-map</a></p>
<p>Use all 4 available gpu's for inference.</p>
","huggingface"
"78262292","Error loading Hugging Face model: SafeTensorsInfo.__init__() got an unexpected keyword argument 'sharded'","2024-04-02 15:10:17","","2","728","<python><nlp><huggingface-transformers><huggingface><llama>","<p>I have been using Hugging Face transformers <a href=""https://huggingface.co/TheBloke/Llama-2-7B-Chat-AWQ"" rel=""nofollow noreferrer"">quantized Llama2 model</a>. Suddenly, code I was able to run earlier today is throwing an error when I try to load the model.</p>
<p>This code is straight from the docs:</p>
<pre class=""lang-py prettyprint-override""><code>from awq import AutoAWQForCausalLM
from transformers import AutoTokenizer

model_name_or_path = &quot;TheBloke/Llama-2-7b-Chat-AWQ&quot;

# Load model
model = AutoAWQForCausalLM.from_quantized(
    model_name_or_path,
    fuse_layers = True,
    trust_remote_code = False,
    safetensors = True
)
</code></pre>
<p>This suddenly generates this error:</p>
<pre><code>TypeError: SafeTensorsInfo.__init__() got an unexpected keyword argument 'sharded'
</code></pre>
<p>The bizarre thing is that I was able to run this code earlier today on the same machine. It is the same model. I haven't installed or updated any new Python (or other) packages.</p>
<p>This is using Ubuntu 20.04. Here are the relevant package versions:</p>
<pre><code>accelerate==0.28.0
autoawq==0.2.4
autoawq_kernels==0.0.6
bitsandbytes==0.43.0
huggingface-hub==0.22.2
torch==2.2.2+cu121
torchaudio==2.2.2+cu121
torchvision==0.17.2+cu121
transformers==4.38.2
</code></pre>
<p>How can an error suddenly arise when I haven't changed anything? It might make sense of my cached version of the model was no longer available, and it had downloaded a newer version. The way that <code>transformers</code> downloads models is a bit mysterious to me, but the <a href=""https://huggingface.co/TheBloke/Llama-2-7B-Chat-AWQ/tree/main"" rel=""nofollow noreferrer"">model files</a> page doesn't indicate anything has changed. What can I do to return to the position I was in this morning when this code would run?</p>
","huggingface"
"78261781","Issues with Generating Text from Fine-Tuned Mistral 7B Model on Georgian Dataset","2024-04-02 13:46:29","","0","235","<nlp><huggingface><language-model><fine-tuning><text-generation>","<p>I've fine-tuned the Mistral 7B model using a Georgian dataset with approximately 100,000 articles, including custom tokenizer fine-tuning. The fine-tuning process took about 9 hours. However, when I try to generate text, the output is not as expected; it consistently returns the input as the output, regardless of the input provided.</p>
<p>Here's the code I used for fine-tuning:</p>
<pre class=""lang-py prettyprint-override""><code>import time
import json
import torch
from datasets import Dataset
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments

# Load dataset, preprocess, and fine-tuning details...

training_args = TrainingArguments(
    output_dir=&quot;mistral_georgian_news_finetuning&quot;,
    max_steps=3125,
    per_device_train_batch_size=32,
    learning_rate=3e-4,
    # Other arguments...
)

# Fine-tuning setup...

# Start fine-tuning
trainer.train()
</code></pre>
<p>For testing the fine-tuned model, I used the following code:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

model_path = &quot;/path/to/fine-tuned-model&quot;
tokenizer_path = &quot;/path/to/tokenizer&quot;

tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
model = AutoModelForCausalLM.from_pretrained(model_path)

def generate_text(prompt_text, max_length=500):
    input_ids = tokenizer(prompt_text, return_tensors=&quot;pt&quot;).input_ids
    output = model.generate(input_ids, max_length=max_length)
    return tokenizer.decode(output[0], skip_special_tokens=True)

prompt = &quot;რამდენიმე დღეში შესრულდება ...&quot;
generated_text = generate_text(prompt)
print(generated_text)
</code></pre>
<p>During testing, the model just echoes the prompt without generating new text. Below are the logs observed:</p>
<pre class=""lang-none prettyprint-override""><code>config.json: 0%| | 0.00/571 [00:00&lt;?, ?B/s]
model.safetensors.index.json: 0%| | 0.00/25.1k [00:00&lt;?, ?B/s]
Downloading shards: 0%| | 0/2 [00:00&lt;?, ?it/s]
model-00001-of-00002.safetensors: 0%| | 0.00/9.94G [00:00&lt;?, ?B/s]
model-00002-of-00002.safetensors: 0%| | 0.00/4.54G [00:00&lt;?, ?B/s]
Loading checkpoint shards: 0%| | 0/2 [00:00&lt;?, ?it/s]
generation_config.json: 0%| | 0.00/116 [00:00&lt;?, ?B/s]
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
</code></pre>
<p>I am unsure if the issue lies in how I am loading and testing the model or if it is related to the fine-tuning process. The model should generate text based on the input prompt, but it returns the input as the output.</p>
<p>Has anyone experienced similar issues, or can someone spot what might be wrong with my approach?</p>
","huggingface"
"78261112","OSError: Can't load tokenizer for 'model'. If you are trying from 'https://huggingface.co', make sure you don't have a local repo with the same name","2024-04-02 11:55:36","","0","272","<huggingface-transformers><large-language-model><huggingface><huggingface-trainer><huggingface-hub>","<p>OSError: Can't load tokenizer for 'AniketArtani/deploytest'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'AniketArtani/deploytest' is the correct path to a directory containing all relevant.
Getting this error while loading model through hugging face model card and even through code too.</p>
","huggingface"
"78255381","Why does BART-base model automatically truncate the output?","2024-04-01 12:08:10","","0","76","<huggingface-transformers><huggingface><bart>","<p>I was fine-tuning a BART[base] model to perform the translation task. My input sentence has 126 tokens, and after being processed by the fine-tuned model, the input_ids has 129 ids. In the end, when I use tokenizer.decode to try to convert ids to tokens, it significantly truncates most of the output, because the output only has 63 tokens. No matter how I change the input sentence, the input_ids always remain 129 ids, and the output sentence is still truncated as well.
Here's my code:</p>
<pre><code>checkpoint = r&quot;E:\checkpoint\bart-wmt17\checkpoint-5368000&quot;
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)
sentence1 = &quot;Machine learning has gained a lot of attention due to its massive success in various domains, and natural language processing  is one of them. Recently, deep learning has significantly improved the performance of NLP models for multiple tasks to almost human-like performance. However, this came at the cost of a significant increase in the required resources for computational power and needed datasets.    As a result, diverse training paradigms have been proposed to alleviate the need for enormous computational resources, such as training models with multiple parties.    Similarly, data is being crawled from the internet to alleviate the need for large datasets.&quot;
inputs= tokenizer([sentence1],return_tensors=&quot;pt&quot;, max_length=256, truncation=True)
ot = model.generate(**inputs, max_length=256)
encode = ot.reshape(-1)
output = tokenizer.decode(encode, max_length=1024, skip_special_tokens=True)
</code></pre>
<p>The output sentence should not be truncated.</p>
","huggingface"
"78254344","ImportError: Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes: `pip instal","2024-04-01 08:15:53","","4","932","<windows><machine-learning><huggingface><llama-index>","<p>This is my code.</p>
<pre><code>import torch

llm = HuggingFaceLLM(
    context_window=4096,
    max_new_tokens=256,
    generate_kwargs={&quot;temperature&quot;: 0, &quot;do_sample&quot;: False},
    system_prompt=system_prompt,
    query_wrapper_prompt=query_wrapper_prompt,
    tokenizer_name=&quot;mistralai/Mistral-7B-v0.1&quot;,
    model_name=&quot;mistralai/Mistral-7B-v0.1&quot;,
    device_map=&quot;auto&quot;,
    tokenizer_kwargs={&quot;max_length&quot;: 4096},
    # uncomment this if using CUDA to reduce memory usage
    model_kwargs={
        &quot;torch_dtype&quot;: torch.float16, 
        &quot;llm_int8_enable_fp32_cpu_offload&quot;: True,
        &quot;bnb_4bit_quant_type&quot;: 'nf4',
        &quot;bnb_4bit_use_double_quant&quot;:True,
        &quot;bnb_4bit_compute_dtype&quot;:torch.bfloat16,
        &quot;load_in_4bit&quot;: True}
)
</code></pre>
<h2>I got this error.
The <code>load_in_4bit</code> and <code>load_in_8bit</code> arguments are deprecated and will be removed in the future versions. Please, pass a <code>BitsAndBytesConfig</code> object in <code>quantization_config</code> argument instead.</h2>
<pre><code>ImportError                               Traceback (most recent call last)
Cell In[46], line 3
      1 import torch
----&gt; 3 llm = HuggingFaceLLM(
      4     context_window=4096,
      5     max_new_tokens=256,
      6     generate_kwargs={&quot;temperature&quot;: 0, &quot;do_sample&quot;: False},
      7     system_prompt=system_prompt,
      8     query_wrapper_prompt=query_wrapper_prompt,
      9     tokenizer_name=&quot;mistralai/Mistral-7B-v0.1&quot;,
     10     model_name=&quot;mistralai/Mistral-7B-v0.1&quot;,
     11     device_map=&quot;auto&quot;,
     12     tokenizer_kwargs={&quot;max_length&quot;: 4096},
     13     # uncomment this if using CUDA to reduce memory usage
     14     model_kwargs={
     15         &quot;torch_dtype&quot;: torch.float16, 
     16         &quot;llm_int8_enable_fp32_cpu_offload&quot;: True,
     17         &quot;bnb_4bit_quant_type&quot;: 'nf4',
     18         &quot;bnb_4bit_use_double_quant&quot;:True,
     19         &quot;bnb_4bit_compute_dtype&quot;:torch.bfloat16,
     20         &quot;load_in_4bit&quot;: True}
     21 )

File /opt/conda/lib/python3.10/site-packages/llama_index/llms/huggingface/base.py:161, in HuggingFaceLLM.__init__(self, context_window, max_new_tokens, query_wrapper_prompt, tokenizer_name, model_name, model, tokenizer, device_map, stopping_ids, tokenizer_kwargs, tokenizer_outputs_to_remove, model_kwargs, generate_kwargs, is_chat_model, callback_manager, system_prompt, messages_to_prompt, completion_to_prompt, pydantic_program_mode, output_parser)
    159 &quot;&quot;&quot;Initialize params.&quot;&quot;&quot;
    160 model_kwargs = model_kwargs or {}
--&gt; 161 self._model = model or AutoModelForCausalLM.from_pretrained(
    162     model_name, device_map=device_map, **model_kwargs
    163 )
    165 # check context_window
    166 config_dict = self._model.config.to_dict()

File /opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:561, in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    557         cls.register(config.__class__, model_class, exist_ok=True)
    558     return model_class.from_pretrained(
    559         pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs
    560     )
--&gt; 561 elif type(config) in cls._model_mapping.keys():
    562     model_class = _get_model_class(config, cls._model_mapping)
    563     return model_class.from_pretrained(
    564         pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs
    565     )

File /opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:3024, in from_pretrained(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)
   3005     config_path = config if config is not None else pretrained_model_name_or_path
   3006     config, model_kwargs = cls.config_class.from_pretrained(
   3007         config_path,
   3008         cache_dir=cache_dir,
   (...)
   3019         **kwargs,
   3020     )
   3021 else:
   3022     # In case one passes a config to `from_pretrained` + &quot;attn_implementation&quot;
   3023     # override the `_attn_implementation` attribute to `attn_implementation` of the kwargs
-&gt; 3024     # Please see: https://github.com/huggingface/transformers/issues/28038
   3025 
   3026     # Overwrite `config._attn_implementation` by the one from the kwargs --&gt; in auto-factory
   3027     # we pop attn_implementation from the kwargs but this handles the case where users
   3028     # passes manually the config to `from_pretrained`.
   3029     config = copy.deepcopy(config)
   3031     kwarg_attn_imp = kwargs.pop(&quot;attn_implementation&quot;, None)

File /opt/conda/lib/python3.10/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:62, in Bnb4BitHfQuantizer.validate_environment(self, *args, **kwargs)
     60 def validate_environment(self, *args, **kwargs):
     61     if not (is_accelerate_available() and is_bitsandbytes_available()):
---&gt; 62         raise ImportError(
     63             &quot;Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` &quot;
     64             &quot;and the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`&quot;
     65         )
     67     if kwargs.get(&quot;from_tf&quot;, False) or kwargs.get(&quot;from_flax&quot;, False):
     68         raise ValueError(
     69             &quot;Converting into 4-bit or 8-bit weights from tf/flax weights is currently not supported, please make&quot;
     70             &quot; sure the weights are in PyTorch format.&quot;
     71         )
</code></pre>
<p>ImportError: Using <code>bitsandbytes</code> 8-bit quantization requires Accelerate: <code>pip install accelerate</code> and the latest version of bitsandbytes: <code>pip install -i https://pypi.org/simple/ bitsandbytes</code></p>
<p>what is the issue of this</p>
","huggingface"
"78251629","ImportError: cannot import name 'HuggingFaceInferenceAPI' from 'llama_index.llms' (unknown location)","2024-03-31 14:21:19","","0","372","<python><machine-learning><langchain><huggingface><llama-index>","<p>want to import HuggingFaceInferenceAPI.</p>
<pre><code>from llama_index.llms import HugggingFaceInferenceAPI
</code></pre>
<p>llama_index.llms documentation doesn't have HugggingFaceInferenceAPI module. Anyone has update on this?</p>
","huggingface"
"78250613","ModuleNotFoundError: No module named 'llama_index.node_parser'","2024-03-31 07:53:47","","0","2322","<python><machine-learning><langchain><huggingface><llama-index>","<p>I want to import SimpleNodeParser from llama_index.node parser.</p>
<pre><code>from llama_index.node_parser import SimpleNodeParser
</code></pre>
<p>But when I run this I'm getting an error:</p>
<pre><code>ModuleNotFoundError: No module named 'llama_index.node_parser'
</code></pre>
<p>Help me to solve this.</p>
<p>I want to import SimpleNodeParser from llama_index.node parser.</p>
","huggingface"
"78248619","I am unable to perform the vector embeddings with the help of pinecone and python","2024-03-30 16:11:54","","0","201","<langchain><word-embedding><huggingface><py-langchain><pinecone>","<p>I have done chunking of the data prior to this and intend to do embeddings and store in pinecone. I have referred to Youtube on this as well and found this code and its not working.</p>
<pre><code>docsearch = pc.from_documents([t.page_content for t in text_chunks], embeddings, index_name= 'mcahtbot')
</code></pre>
<p>I get this error:</p>
<blockquote>
<p>AttributeError: 'Pinecone' object has no attribute 'from_documents'</p>
</blockquote>
<p>Referred to pinecone, Youtube and searched other platforms for answers to similar problem</p>
","huggingface"
"78247232","Changing location of model checkpoints in Hugging Face","2024-03-30 08:11:31","","1","64","<huggingface>","<p>I'm downloading a big model. I've changed to a new external data storage device which has a enough space with <code>export HF_HOME='\blah\blah'</code>. However when it comes to downloading that model, I get the following error, which appears like the checkpoint is not able to be stored:</p>
<pre><code>OSError: [Errno 28] No space left on device: '/Volumes/hdd_ext/cache/hub/tmpft8xst3n' -&gt; 'checkpoints/ckpt-0/tensor00000_000'
</code></pre>
<p>How can I fix this?</p>
","huggingface"
"78241142","Runtime Error: StableCascadeCombinedPipeline: Expected all tensors to be on the same device","2024-03-28 22:23:43","78293281","0","61","<huggingface><diffusers>","<p><em>In a nutshell:</em> Attempting to pass an image into <a href=""https://huggingface.co/docs/diffusers/en/api/pipelines/stable_cascade#diffusers.StableCascadeCombinedPipeline"" rel=""nofollow noreferrer"">StableCascadeCombinedPipeline</a> gives a runtime error complaining about tensors not all being in cuda. The app works perfectly if I comment out the image argument so that it relies only on the text prompt, i.e. as a text-to-image generator.</p>
<p>A gist of the app code (60 lines of python) with the image input commented out is visible <a href=""https://gist.github.com/Michael-F-Ellis/9ddfee002d31e92816c34acbb194aaa3"" rel=""nofollow noreferrer"">here</a></p>
<p>The doc for the pipeline defines the optional image argument as:
<code>images (torch.Tensor, PIL.Image.Image, List[torch.Tensor], List[PIL.Image.Image], optional) — The images to guide the image generation for the prior.</code> I'm passing a PIL.Image.Image by way of a Gradio Image Component.</p>
<p>Since the app runs without passing an image, it seems like I need, somehow, to ensure that the image ends up in <code>cuda</code>, but so far I haven't found any instructions for how to do that.</p>
<p>Here's the part of the code that sets up the pipeline and defines the generate function:</p>
<pre><code># Constants
repo = &quot;stabilityai/stable-cascade&quot;


# Ensure model and scheduler are initialized in GPU-enabled function
if torch.cuda.is_available():
    pipe = StableCascadeCombinedPipeline.from_pretrained(repo, variant=&quot;bf16&quot;, torch_dtype=torch.bfloat16)
    pipe.to(&quot;cuda&quot;)

# The generate function
@spaces.GPU(enable_queue=True)
def generate_image(prompt):  
#def generate_image(prompt, images):  
    seed  =  random.randint(-100000,100000)

    results =  pipe(
                prompt=prompt,
                #images=[images],
                height=1024,
                width=1024,
                num_inference_steps=20, 
                generator=torch.Generator(device=&quot;cuda&quot;).manual_seed(seed)
            )
    return results.images[0]
</code></pre>
","huggingface"
"78241136","Hugging Face - What is the difference between epochs in optimizer and TrainingArguments?","2024-03-28 22:22:17","","0","107","<huggingface-transformers><huggingface>","<p>Following the image classification tutorial, there are two places where the epochs are set. How are these related, or should the same epoch count be the same for both? In other TF models, I've only seen epochs within the model.fit_generator() param.</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import create_optimizer

batch_size = 16
num_epochs = 3
num_train_steps = len(dataset[&quot;train&quot;]) * num_epochs
learning_rate = 3e-5
weight_decay_rate = 0.01

optimizer, lr_schedule = create_optimizer(
    init_lr=learning_rate,
    num_train_steps=num_train_steps,
    weight_decay_rate=weight_decay_rate,
    num_warmup_steps=0,
)

training_args = TrainingArguments(
    output_dir=&quot;my_awesome_food_model&quot;,
    remove_unused_columns=False,
    evaluation_strategy=&quot;epoch&quot;,
    save_strategy=&quot;epoch&quot;,
    learning_rate=5e-5,
    per_device_train_batch_size=16,
    gradient_accumulation_steps=4,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    warmup_ratio=0.1,
    logging_steps=10,
    load_best_model_at_end=True,
    metric_for_best_model=&quot;accuracy&quot;,
    push_to_hub=False,
)
</code></pre>
<p><a href=""https://huggingface.co/docs/transformers/tasks/image_classification#preprocess"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/tasks/image_classification#preprocess</a></p>
<p>I don't understand why I need to pass in num_train_steps. Wouldn't that be calculated based on the num_train_epochs?</p>
","huggingface"
"78235128","Device_map not wokring for ORTModelForSeq2SeqLM - Potential bug?","2024-03-27 23:35:47","","0","39","<huggingface><onnx><tensorrt><onnxruntime>","<p>Reposted from here: <a href=""https://discuss.huggingface.co/t/device-map-not-wokring-for-ortmodelforseq2seqlm-potential-bug/78982"" rel=""nofollow noreferrer"">https://discuss.huggingface.co/t/device-map-not-wokring-for-ortmodelforseq2seqlm-potential-bug/78982</a></p>
<p>I have a non-KV Cache T5 Onnx Model that I'm trying to load using <code>huggingface</code>/ <code>optimum.onnxruntime</code>. There are two issues</p>
<h2>with <code>CUDAExecutionProvider</code></h2>
<pre><code>model = ORTModelForSeq2SeqLM.from_pretrained(model_name, provider=&quot;CUDAExecutionProvider&quot;, task=&quot;text2text-generation&quot;, use_cache=False, use_io_binding=False, device_map=&quot;cuda:3&quot;)
</code></pre>
<p>The same thing happens with a model that has <code>with_past</code>:</p>
<pre><code>model_kv = ORTModelForSeq2SeqLM.from_pretrained(model_name, provider=&quot;CUDAExecutionProvider&quot;, task=&quot;text2text-generation-with-past&quot;, use_cache=True, use_io_binding=True, device_map=&quot;cuda:3&quot;)
</code></pre>
<p>Both load loads fine, except that both ignores <code>device_map</code> and loads it to <code>cuda:0</code>, which means I can't load multiple models two models for testing or serving since my model is relatively large and can only fit one on a gpu.</p>
<h2>with <code>TensorrtExecutionProvider</code></h2>
<p>Then I tried <code>TensorrtExecutionProvider</code> (on non KV cache model). The model thinks that its loaded into the wrong cuda, but it's actually on CPU. Because the model in reality is on CPU, its immensely slow to load.</p>
<pre><code>model = ORTModelForSeq2SeqLM.from_pretrained(model_name, provider=&quot;TensorrtExecutionProvider&quot;, task=&quot;text2text-generation&quot;, use_cache=False, use_io_binding=False, device_map=&quot;cuda:3&quot;)
</code></pre>
<p><code>model.device</code> shows <code>device(type='cuda', index=0)</code>. However, my <code>nvitop</code> shows that there is hardly any memory use on <code>cuda:0</code> (<code>3746MB</code> used up from my base of <code>872MB</code> - it could be that  the model graph is indeed loaded onto cuda_0 but the data isn't). If I look into my cpu main memory, I see that utilization has increased by <code>60GB</code>, the size of my model. So the model that I wanted to load into <code>cuda:3</code> thinks its on <code>cuda:0</code> but is actually on CPU.</p>
<p>I have this in my os environment:</p>
<pre><code>export ONNX_MODE=cuda
export PATH=/usr/local/cuda-11.8/bin:$PATH
export CUDA_PATH=/usr/local/cuda
export LD_LIBRARY_PATH=/usr/local/cuda-11.8/lib64:/home/zm/workspace/packages/TensorRT-8.6.1.6/lib:$LD_LIBRARY_PATH
</code></pre>
<p>Wondering how I can fix this? What I want in both cases is to be able to put the model on what ever device (cuda0-7) that I want. I can get them there with a <code>.to(&quot;cuda:3&quot;)</code> but this essentially causes me to do a total-reload which is slow.</p>
","huggingface"
"78233305","How to finetune the LLM to output the text with SSML tags?","2024-03-27 16:44:25","","0","103","<training-data><langchain><large-language-model><huggingface><seq2seq>","<p>I need to train a model to add the SSML tags and punctuation to the input text.
For example, from the sentence &quot;Hello world.&quot; I'd like to get the
<code>&lt;speak&gt; Hello! world. &lt;/speak&gt;</code> output.</p>
<p>Another example:</p>
<p>Input: &quot;In reverse bias, the electrons flow from anode to cathode (P -&gt; N), while the holes (positive charges) flow from cathode to anode (N -&gt; P). This happens because in reverse bias, a greater voltage is wired to N, attracting electrons to outside, while the least voltage does the same with holes.&quot;</p>
<p>Output:<code>&lt;speak&gt;In reverse bias, the electrons, flow from anode to cathode (P -&gt; N), while the holes (positive charges), flow from cathode to anode (N -&gt; P). &lt;break time = &quot;0.5s&quot; /&gt; This happens because in reverse bias, a greater voltage, is wired to N, attracting electrons to outside, while the least voltage, does the same with holes. &lt;/speak&gt;</code></p>
<p>I followed the standard Seq2Seq training using the huggings face tutorials, but had no luck. the output text is the same as the input. I used a Flan-T5-base model. My data is 1200 pairs.</p>
<p>Any suggestion how to force the model to show the ssml tags and the &quot;incorrect&quot; punctuation?</p>
","huggingface"
"78231061","How to handle memory intensive task causing WorkerLostError with Celery and HuggingFaceEmbedding?","2024-03-27 10:45:16","","1","77","<django-rest-framework><celery><django-celery><huggingface><qdrant>","<p>I'm trying to use celery to handle the heavy task of creating a new qdrant collection every time a new model is created, I need to extract the content of the file, create embedding and store it in qdrant db as a collection. The problem is, I get the following error when I call embeddings.embed with HuggingFaceEmbedding inside celery.</p>
<pre class=""lang-bash prettyprint-override""><code>celery-dev-1  | [2024-03-27 10:18:27,451: INFO/ForkPoolWorker-19] Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2
celery-dev-1  | [2024-03-27 10:18:35,856: ERROR/MainProcess] Process 'ForkPoolWorker-19' pid:115 exited with 'signal 11 (SIGSEGV)'
celery-dev-1  | [2024-03-27 10:18:35,868: ERROR/MainProcess] Task handler raised error: WorkerLostError('Worker exited prematurely: signal 11 (SIGSEGV) Job: 3.')
celery-dev-1  | Traceback (most recent call last):
celery-dev-1  |   File &quot;/usr/local/lib/python3.10/site-packages/billiard/pool.py&quot;, line 1264, in mark_as_worker_lost
celery-dev-1  |     raise WorkerLostError(
celery-dev-1  | billiard.einfo.ExceptionWithTraceback:
celery-dev-1  | &quot;&quot;&quot;
celery-dev-1  | Traceback (most recent call last):
celery-dev-1  |   File &quot;/usr/local/lib/python3.10/site-packages/billiard/pool.py&quot;, line 1264, in mark_as_worker_lost
celery-dev-1  |     raise WorkerLostError(
celery-dev-1  | billiard.exceptions.WorkerLostError: Worker exited prematurely: signal 11 (SIGSEGV) Job: 3.
celery-dev-1  | &quot;&quot;&quot;
</code></pre>
<p>Here is the knowledge model when the task is called,</p>
<pre class=""lang-py prettyprint-override""><code>class Knowledge(Common):
    name = models.CharField(max_length=255, blank=True, null=True)
    file = models.FileField(upload_to=knowledge_path, storage=PublicMediaStorage())
    qd_knowledge_id = models.CharField(max_length=255, blank=True, null=True)
    is_public = models.BooleanField(default=False)

    #
    def save(self, *args, **kwargs):

        if self.pk is None:
            collection_name = f&quot;{self.name}-{datetime.now().strftime('%Y_%m_%d_%H_%M_%S')}&quot;
            process_files_and_upload_to_qdrant.delay(self.file.name, collection_name)
            self.qd_knowledge_id = collection_name
        super().save(*args, **kwargs)
</code></pre>
<p>here is the task and the functions it calls:</p>
<pre class=""lang-py prettyprint-override""><code>@shared_task
def process_files_and_upload_to_qdrant(file_name, collection_name):
    file_path = default_storage.open(file_name)
    result = process_file(file_path, collection_name)
    return result

def process_file(file : InMemoryUploadedFile, collection_name):
    text = read_data_from_pdf(file)
    chunks = get_text_chunks(text)
    embeddings = get_embeddings(chunks)
    client.create_collection(
            collection_name=collection_name,
            vectors_config=qdrant_models.VectorParams(
                size=768, distance=qdrant_models.Distance.COSINE
            ),
        )
    client.upsert(collection_name=collection_name, wait=True, points=embeddings)


def read_data_from_pdf(file : InMemoryUploadedFile):
    text = &quot;&quot;

    pdf_reader = PdfReader(file)

    for page in pdf_reader.pages:
        text += page.extract_text()

    return text


def get_text_chunks(texts: str):
    text_splitter = CharacterTextSplitter(
        separator=&quot;\n&quot;, chunk_size=1000, chunk_overlap=200, length_function=len
    )
    chunks = text_splitter.split_text(texts)
    return chunks


def get_embeddings(text_chunks):
    from langchain_community.embeddings import HuggingFaceEmbeddings
    from qdrant_client.http.models import PointStruct
    embeddings = HuggingFaceEmbeddings(
        model_name=&quot;sentence-transformers/all-mpnet-base-v2&quot;
    )
    points = []
   
    for chunk in text_chunks:
        embedding = embeddings.embed_query(chunk) &lt;---- The error occurs here
        point_id = str(uuid.uuid4())  
        points.append(
            PointStruct(id=point_id, vector=embedding, payload={&quot;text&quot;: chunk})
        )

    return points

</code></pre>
<p>How do I approach this? Since the model is created as a many to many field, the response takes a long time, due to which I'm trying to move it into a celery task. (Some delay when storing in qdrant is acceptable, it just shouldn't affect the api response time). The api works fine when I do it without celery, but it's super slow.</p>
<p>I've tried splitting them into multiple small celery tasks, but I can't pass the embeddings or non-json serializable data into the task. I don't know how to approach this.</p>
","huggingface"
"78229776","How to add noise to the intermediate layer of huggingface bert model?","2024-03-27 06:42:39","","0","39","<pytorch><nlp><huggingface-transformers><bert-language-model><huggingface>","<p>I am trying to add noise to the embeddings from the 5th layer of bert model before it is forwarded to the next layer. The final layer output after the noise is added is then used for classification task. How can I do this in huggingface bert model?</p>
<pre><code>class MainModel(BertPreTrainedModel):
    def __init__(self, config, loss_fn):
        super(MainModel,self).__init__(config)
        self.num_labels = 2
        self.loss_fn = loss_fn
        config.output_hidden_states = True
        self.bert = AutoModel.from_pretrained(&quot;bert-base-uncased&quot;,config = config)
        self.classifier = nn.Linear(768, self.num_labels)

    def forward(self, input_ids, attention_mask, token_type_ids, labels, noise, device):
              
        output = self.bert(input_ids, attention_mask = attention_mask, token_type_ids = token_type_ids)
        hidden_emb = output.hidden_states[5]
        hidden_emb = hidden_emb + noise

        #Forward these hidden_emb to next layer of bert model 
        #Write the code here

        output = output.last_hidden_state #Output from final layer of bert model after adding noise
        output = output[:,0,:]
        classifier_out = self.classifier(output)
        main_prob = F.softmax(classifier_out, dim = 1)
        main_gold_prob = torch.gather(main_prob, 1, labels)
        loss_main = self.loss_fn.forward(main_gold_prob)
        return loss_main,main_prob
</code></pre>
","huggingface"
"78228609","AWS Sagemaker MultiModel endpoint additional dependencies","2024-03-26 23:11:49","","1","144","<python><amazon-web-services><amazon-sagemaker><huggingface>","<p>I am trying to deploy a <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html#how-multi-model-endpoints-work"" rel=""nofollow noreferrer"">multi model endpoint</a> on aws sagemaker. However some of my models have additional dependencies. I am following the <a href=""https://huggingface.co/docs/sagemaker/en/inference#user-defined-code-and-modules"" rel=""nofollow noreferrer"">huggingface's documentation</a>
on creating user-defined-code and requirements.</p>
<p>My zipped artifacts have a <code>code</code> directory with <code>requirements.txt</code> and yet when I deploy the model and try invoking it with the python aws sdk. I get <code>ModuleNotFound</code> errors during my imports.</p>
<p>I know its finding my <code>inference.py</code> file since its failing to find those modules that I import.</p>
<p>It should be noted that these models I am deploying are trained and made outside of sagemaker and I am trying to bring them into sagemaker.</p>
<p>The container image I am using is
<code>'763104351884.dkr.ecr.ca-central-1.amazonaws.com/huggingface-pytorch-inference:2.1.0-transformers4.37.0-cpu-py310-ubuntu22.04'</code></p>
","huggingface"
"78226691","Accuracy at 0 during inference with peft and Vision EncoderDecoderModel from huggingface","2024-03-26 16:07:17","","0","40","<artificial-intelligence><huggingface><peft><huggingface-hub><donut>","<p>I trained a DonUT model to perform document extraction from receipt, after the training I tested the accuracy and everything was fine so I pushed the tokenizer and the model to hub.</p>
<p>But when I call it from hub the model behave as if it was never trained, the results are not even coherent. This happens only with the peft finetuned model. I have done a full finetuning just before and everything is fine.</p>
<p><a href=""https://www.kaggle.com/code/edgarmevaa/full-finetuning-of-donut-on-numeric-data/edit"" rel=""nofollow noreferrer"">Link to the source code</a></p>
","huggingface"
"78222819","Chroma.from_texts() 'numpy.ndarray' object has no attribute 'embed_documents' Error","2024-03-26 03:29:52","","0","190","<python><huggingface><py-langchain><python-embedding><chromadb>","<p>I’m currently working on a project where I’m using the SentenceTransformer model from the sentence-transformers library to generate embeddings for text data. I would like to store these pre-generated embeddings in Chroma for later use.</p>
<p>Here’s a simplified version of my code:</p>
<pre class=""lang-py prettyprint-override""><code>from PyPDF2 import PdfReader
from langchain_community.vectorstores import Chroma
from langchain_text_splitters import CharacterTextSplitter
from sentence_transformers import SentenceTransformer

def generate_embeddings() -&gt; Chroma:
    pdf = PdfReader(&quot;path_to_my_pdf&quot;)
    raw_text = ''
    for i, page in enumerate(pdf.pages):
        content = page.extract_text()
        if content:
            raw_text += content
    text_splitter = CharacterTextSplitter(
        separator = &quot;\n&quot;,
        chunk_size = 750,
        chunk_overlap  = 50,
        length_function = len,
    )
    texts = text_splitter.split_text(raw_text)
    model = SentenceTransformer('all-MiniLM-L6-v2')
    embeddings = model.encode(texts)
    vectordb = Chroma.from_texts(
        texts = texts
        embeddings = embeddings,
        persist_directory = &quot;path_to_persist_directory&quot;
    )
    vectordb.persist()
    return vectordb
</code></pre>
<p>However, I’m encountering an issue with the <strong>Chroma.from_texts</strong> method. The error said <code>&quot;AtributeError: 'numpy.ndarray' object has no attribute 'embed_documents'</code></p>
<p>Could you please guide me on how to create a Chroma object from pre-generated embeddings? Is there a method or a workaround that I can use to achieve this?</p>
<p>Any help would be greatly appreciated. Thank you in advance!</p>
","huggingface"
"78220369","Encoder-Decoder with Huggingface Models","2024-03-25 16:07:03","","0","489","<pytorch><huggingface-transformers><autoencoder><huggingface><huggingface-tokenizers>","<p>I want to create an Encoder-Decoder Model using the following structure:</p>
<ul>
<li>Bert-base-uncased for encoding the input (<a href=""https://huggingface.co/google-bert/bert-base-uncased"" rel=""nofollow noreferrer"">https://huggingface.co/google-bert/bert-base-uncased</a>)</li>
<li>Linear layer for connecting the two models using the CLS token of Bert as input</li>
<li>OPT-125M for decoding using the output of the linear layer as input (<a href=""https://huggingface.co/facebook/opt-125m"" rel=""nofollow noreferrer"">https://huggingface.co/facebook/opt-125m</a>)</li>
</ul>
<p>I want to do this to basically implement the idea I read about in the In-Context Autoencoder paper and test it out myself (<a href=""https://arxiv.org/abs/2307.06945"" rel=""nofollow noreferrer"">https://arxiv.org/abs/2307.06945</a>)</p>
<p>I would like to do this with the huggingface library using PyTorch as it helps to minimize the programming efforts a lot and because I do not know where I would even get the raw implementations of the OPT-125M or BERT model and how to implement them by hand. Also the optimization of huggingface plays a big role to try it on a normal desktop-PC.</p>
<p>My problem is that the OPT-125M model uses a tokenizer for inputs and I am not able to bypass this.</p>
<p>Does anyone know of a way to directly input the output of the linear layer into OPT-125M without encoding it, or a different way of implementing it other than huggingface which is also as performant?</p>
<p>This is the skeleton code that I have already written which produces an error because of the wrong input to OPT:</p>
<pre><code>from transformers import BertTokenizer, BertModel, AutoModelForCausalLM
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')
OPT = AutoModelForCausalLM.from_pretrained(&quot;facebook/opt-125m&quot;)
import torch
from torch import nn

class Encoder(nn.Module):
    def __init__(self):
        super(Encoder, self).__init__()
        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
        self.model = BertModel.from_pretrained('bert-base-uncased')

    def forward(self, input_text):
        inputs = self.tokenizer(input_text, return_tensors=&quot;pt&quot;, padding=True, truncation=True, max_length=512)
        outputs = self.model(**inputs)
        return outputs.last_hidden_state[:, 0, :]  # CLS token embeddings

class LinearTransformation(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(LinearTransformation, self).__init__()
        self.linear = nn.Linear(input_dim, output_dim)

    def forward(self, x):
        return self.linear(x)

class Decoder(nn.Module):
    def __init__(self):
        super(Decoder, self).__init__()
        self.model = AutoModelForCausalLM.from_pretrained(&quot;facebook/opt-125m&quot;)

    def forward(self, x):
        # Assuming x is prepared correctly for the OPT model
        output = self.model(input_ids=x)
        return output

class BertOptPipeline(nn.Module):
    def __init__(self):
        super(BertOptPipeline, self).__init__()
        self.encoder = Encoder()
        self.linear_transformation = LinearTransformation(768, 512)
        self.decoder = Decoder()

    def forward(self, input_text):
        encoded = self.encoder(input_text)
        transformed = self.linear_transformation(encoded)
        print(transformed.shape)
        # Further processing may be needed here to match the decoder's input requirements
        decoded = self.decoder(transformed)
        return decoded

pipeline = BertOptPipeline()
input_text = &quot;thank you for your help&quot;
output = pipeline(input_text)
</code></pre>
<p>Thanks for your help!</p>
","huggingface"
"78219384","integrate huggingface inference endpoint with flowise","2024-03-25 13:20:01","","0","74","<translation><text-classification><huggingface><flowise>","<p>I am trying to integrate mode : mistralai/Mixtral-8x7B-Instruct-v0.1 from hugging dace which I have deployed as an inference endpoint already, and I got a URL which i can put into flowise <a href=""https://i.sstatic.net/hmwvO.png"" rel=""nofollow noreferrer"">Add Endpoint URL</a></p>
<p>but it just doesn't work as expected, when i trying to run the model, it shows that <a href=""https://i.sstatic.net/evbCz.png"" rel=""nofollow noreferrer"">error</a>,so i just set the max_token to 250, now I can generate the text but still the model works randomly with no expected output<a href=""https://i.sstatic.net/Nf05P.png"" rel=""nofollow noreferrer"">model output</a></p>
<p>it is supposed to be a translator that translates everything into English, this is how the model will behave when I used free inference API from hugging face:<a href=""https://i.sstatic.net/ZwCqb.png"" rel=""nofollow noreferrer"">behaved correctly</a></p>
<p>there's a limit for free inference API, that why I paid for the inference endpoint, but it just behaved so differently, so confused, please help me out if anyone knows, thanks so much!</p>
","huggingface"
"78218243","Making your custom-data trained LLM model work faster and more accurate","2024-03-25 09:59:50","","0","155","<large-language-model><huggingface><llama><retrieval-augmented-generation><text-generation>","<p>I am very new to NLP and Machine Learning. I have been trying to build a conversational chatbot which can answer user questions related to my software application as well as a wide variety of questions specific to my domain (bioinformatics). I have used the llama-2-7B model and I have used retrieval augmented generation where I have given some custom, domain-specific data in the form of PDF embeddings and used it as context for my model to answer questions.</p>
<p>Here are my concerns and I would appreciate any advice about what direction I must take.</p>
<ol>
<li>I have used prompt fine tuning to redirect the model to use it's existing knowledge to answer questions in case it can not find answers directly in the context.</li>
</ol>
<pre><code>SYSTEM_PROMPT = &quot;&quot;&quot;
Your name is 'Jo', you are an AI assistant, developed by organisation.
You are a knowledgeable, respectful and honest bioinformatics assistant who provides support to users of the software. Always answer as helpfully as possible, while being safe.

Use the given pieces of contexts from the PDF embeddings to answer questions. If a question does not make sense, or you don't know the answer, instead of assuming or giving incorrect answers tell the user that you can not answer the question.
&quot;&quot;&quot;.strip()

template = generate_prompt(
    &quot;&quot;&quot;
    {context}

    Question: {question}
    &quot;&quot;&quot;, system_prompt=SYSTEM_PROMPT
)

prompt = PromptTemplate(template=template, input_variables=[&quot;context&quot;, &quot;question&quot;])

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type=&quot;stuff&quot;,
    retriever=pdf_vectordb.as_retriever(search_kwargs={&quot;k&quot;:2}),
    return_source_documents=True,
    chain_type_kwargs={'prompt':prompt}
)




result = qa_chain(&quot;How to merge different VCF files?&quot;)
result = qa_chain(&quot;How is chatgpt different from other AI?&quot;)
</code></pre>
<p>Though it can clearly elaborate on questions directly found in the PDFs, it does get confused with closely related questions (as the Llama model already has knowledge on bioinformatics). How can I improve the accuracy?</p>
<ol start=""2"">
<li>I'm running this on an Nvidia A5000 GPU machine with 126GB CPU RAM and 24GB GPU RAM. It uses approx. 5GB CPU RAM and 11GB GPU RAM to retrieve answers each time I ask a question. It also takes a lot of time to generate the answers (3-4 mins for the first question asked in the code above, and 5 mins for the second question). I want to significantly reduce the amount so that when multiple users utilise the model it can generate answers at a readable pace. How can I achieve that?</li>
</ol>
","huggingface"
"78214971","In NextJS, how can I show a blob of type 'image/jpeg', received from an API call, on screen?","2024-03-24 14:52:46","78215035","0","212","<javascript><next.js><huggingface>","<p>In nextjs, I do the following API call:</p>
<pre><code>const response = await fetch('/api/generateimageHG2');
</code></pre>
<p>this call the following function:</p>
<pre><code>import { HfInference } from &quot;@huggingface/inference&quot;;


export default async function generate (req, res) {

  try{
  const inference = new HfInference('[token]');
  const response = await inference.textToImage({
      model: &quot;stabilityai/stable-diffusion-2&quot;,
      inputs: &quot;award winning high resolution photo of a giant dragon/((ladybird)) hybrid, [trending on artstation]&quot;,
      parameters: {
          negative_prompt: &quot;blurry&quot;,
      },
  });
  
  console.log (response);  
   

    res.status(200).send(response);
  } catch (error) {
    throw new Error('Error fetching property information: ' + error.message);
  } 
  
};
</code></pre>
<p>The console log prints the following:</p>
<blockquote>
<p>Blob { size: 93803, type: 'image/jpeg' }</p>
</blockquote>
<p>So I know I'm getting a blob.</p>
<p>I'm really stuck in turning this blob into an image on the screen. I tried URL.CreateObjectURL, but that causes an error.</p>
<p>Anyone has any ideas?</p>
","huggingface"
"78211526","PyTorch: AttributeError: 'torch.dtype' object has no attribute 'itemsize'","2024-03-23 16:05:27","78253819","2","2453","<python><pytorch><databricks><huggingface><peft>","<p>I am trying to follow this article on medium <a href=""https://dassum.medium.com/fine-tune-large-language-model-llm-on-a-custom-dataset-with-qlora-fb60abdeba07"" rel=""nofollow noreferrer"">Article</a>.</p>
<p>I had a few problems with it so the remain chang eI did was to the <code>TrainingArguments</code> object I added <code>gradient_checkpointing_kwargs={'use_reentrant':False},</code>.</p>
<p>So now I have the following objects:</p>
<pre class=""lang-py prettyprint-override""><code>peft_training_args = TrainingArguments(
    output_dir = output_dir,
    warmup_steps=1,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    max_steps=100, #1000
    learning_rate=2e-4,
    optim=&quot;paged_adamw_8bit&quot;,
    logging_steps=25,
    logging_dir=&quot;./logs&quot;,
    save_strategy=&quot;steps&quot;,
    save_steps=25,
    evaluation_strategy=&quot;steps&quot;,
    eval_steps=25,
    do_eval=True,
    gradient_checkpointing=True,
    gradient_checkpointing_kwargs={'use_reentrant':False},
    report_to=&quot;none&quot;,
    overwrite_output_dir = 'True',
    group_by_length=True,
)

peft_model.config.use_cache = False

peft_trainer = transformers.Trainer(
    model=peft_model,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    args=peft_training_args,
    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),
)
</code></pre>
<p>And when I call <code>peft_trainer.train()</code> I get the following error:</p>
<pre><code>AttributeError: 'torch.dtype' object has no attribute 'itemsize'
</code></pre>
<p>I'm using Databricks, and my pytorch version is <code>2.0.1+cu118</code></p>
","huggingface"
"78210304","Can't use the BLEU offline","2024-03-23 09:25:01","","0","102","<python><huggingface><evaluate><bleu>","<p>I download all the metrics from evaluate and save them on the disk. When I want to use BLEU to evaluate my model's outputs, a bug occured. I can't load the BLEU from local dir, the pycharm give me an error like this:</p>
<p><code>ConnectionError: Couldn't reach https://github.com/tensorflow/nmt/raw/master/nmt/scripts/bleu.py (ProxyError(MaxRetryError(&quot;HTTPSConnectionPool(host='github.com', port=443): Max retries exceeded with url: /tensorflow/nmt/raw/master/nmt/scripts/bleu.py (Caused by ProxyError('Unable to connect to proxy', SSLError(SSLZeroReturnError(6, 'TLS/SSL connection has been closed (EOF) (_ssl.c:1131)'))))&quot;)))</code></p>
<p>But I swear I loaded it from the local dir.Here's my code:</p>
<p><code>import evaluate</code></p>
<p><code>metric = evaluate.load(r&quot;E:\evaluate-main\metrics\bleu&quot;)</code></p>
","huggingface"
"78208044","Is it ok to use Huggingface with Pytorch lightning?","2024-03-22 17:49:41","","2","406","<huggingface><pytorch-lightning>","<p>I have some Huggingface models that I'd like to finetune with PEFT LORA. I'd also like to use Pytorch Lightning Fabric for FSDP distributed training. However, I'm not sure if they'd be compatible with each other. Does anyone have experience using these two together?</p>
","huggingface"
"78207610","How to prevent DataCollatorForLanguageModelling from using input_ids as labels in CLM tasks?","2024-03-22 16:28:34","","0","260","<nlp><huggingface-transformers><huggingface><huggingface-trainer>","<p>How to instruct<code>DataCollatorForLanguageModeling</code> to not use shifted inputs as labels but my own labels?</p>
<p>Here's a MWE:</p>
<pre><code>data = {
'sources': [&quot;This is some text&quot;, &quot;Another text athta ljdlsfjsdlf&quot;, &quot;Also some bulshit type text who knows wtf?&quot;],
'targets': [&quot;Some potential target.&quot;, &quot;The answer is JoLo!&quot;, &quot;Who killed margaret and what was the motive and poential causes!&quot;]
}

tokenizer = AutoTokenizer.from_pretrained(&quot;openai-community/gpt2&quot;)
config = AutoConfig.from_pretrained(&quot;openai-community/gpt2&quot;)
gpt2model = AutoModelForCausalLM.from_config(config)
tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

&gt;&gt; &quot;Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained&quot;


tokenized_data = tokenizer(data['sources'])
with tokenizer.as_target_tokenizer():
    tokenized_data['labels'] = tokenizer(data['targets'])

&gt;&gt; tokenized_data
{'input_ids': [[1212, 318, 617, 2420], [6610, 2420, 379, 4352, 64, 300, 73, 67, 7278, 69, 8457, 67, 1652], [7583, 617, 4807, 16211, 2099, 2420, 508, 4206, 266, 27110, 30]], 'attention_mask': [[1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],
'labels': {'input_ids': [[4366, 2785, 2496, 13], [464, 3280, 318, 5302, 27654, 0], [8241, 2923, 6145, 8984, 290, 644, 373, 262, 20289, 290, 745, 1843, 5640, 0]], 'attention_mask': [[1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}}


tokenized_labels = tokenized_data.pop('labels')

outputs = data_collator(tokenized_data)

&gt;&gt; outputs
{'input_ids': tensor([[ 1212,   318,   617,  2420, 50257, 50257, 50257, 50257, 50257, 50257,
         50257, 50257, 50257],
        [ 6610,  2420,   379,  4352,    64,   300,    73,    67,  7278,    69,
          8457,    67,  1652],
        [ 7583,   617,  4807, 16211,  2099,  2420,   508,  4206,   266, 27110,
            30, 50257, 50257]]), 'attention_mask': tensor([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]]), 'labels': tensor([[ 1212,   318,   617,  2420,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100],
        [ 6610,  2420,   379,  4352,    64,   300,    73,    67,  7278,    69,
          8457,    67,  1652],
        [ 7583,   617,  4807, 16211,  2099,  2420,   508,  4206,   266, 27110,
            30,  -100,  -100]])}

</code></pre>
<p>Now the <code>outputs['labels']</code> are just the shifted <code>outputs['input_ids']</code> which is happening automatically from the <code>DataColaltorForLanguageModeling</code>.</p>
<p>The question is since I do have proper labels for the data, in this case <code>tokenized_data['labels']</code> or the variable <code>tokenized_labels</code>, how do I use that in the Trainer class?</p>
<p>So, the <code>dataset.map(...)</code> will tokenize the whole dataset and will return tokens for both <code>text</code> and <code>labels</code>.</p>
<p>Then using <code>data_collator</code> will create <code>labels</code> by shifting the <code>input_ids</code> and feed that to the model.</p>
<p>How do I tell <code>Trainer</code> or <code>DataCollator</code> to use my <code>tokenized_labels</code> instead of creating them based on the inputs?</p>
","huggingface"
"78203426","What number is the pad token for huggingface models?","2024-03-22 00:12:34","","1","156","<huggingface-transformers><huggingface>","<p>I am running these lines of code:</p>
<pre><code>x = &quot;A&quot;
tokenizer = LlamaTokenizer.from_pretrained(&quot;meta-llama/Llama-2-7b-chat-hf&quot;)
tokenizer.add_special_tokens({'pad_token': '[PAD]'})

tokenized_text = tokenizer(x, return_tensors=&quot;pt&quot;, padding='max_length', max_length=10, truncation=True)
print(tokenized_text)
</code></pre>
<p>However, the tokenizer ends up padding with the number 32000. This then causes the embedding layer of the llama model to go out of range when indexing with the tokens. I know I can use eos token but I am confused on why the padding token number is set beyond the size of the embedding layer dictionary.</p>
","huggingface"
"78201903","How do I use adaptative learning rate when training a transformer model?","2024-03-21 17:46:46","","0","46","<deep-learning><pytorch><huggingface-transformers><huggingface>","<p>So I am trying to use adaptative learning rate instead of a fixed one when training a transformer model. I have never done this with Pytorch and Transformers and although I have found some <a href=""https://huggingface.co/docs/transformers/en/main_classes/optimizer_schedules"" rel=""nofollow noreferrer"">documentation</a> I have not been able to reproduce it. Could someone help me, I am a beginner. Help is much appreciated.</p>
<p>My code:</p>
<pre><code>training_args = TrainingArguments(
#auto_find_batch_size=True,
per_device_train_batch_size=8,
per_device_eval_batch_size=8,
evaluation_strategy=IntervalStrategy.STEPS,
eval_steps=50,
gradient_accumulation_steps=4,
num_train_epochs=7,
warmup_steps=500,
weight_decay=0.01,
learning_rate=None,
bf16=True,
logging_steps=10,
save_steps=500,
output_dir=OUTPUT_DIR,
report_to=&quot;all&quot;,
run_name=&quot;oxford_gemma&quot;,
load_best_model_at_end=True, 
metric_for_best_model='loss',
#logging_dir='./logs',
optim=&quot;paged_adamw_8bit&quot;
)

print(&quot;Starting training with the updated configuration&quot;)

try:
  trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
)

trainer.train()

except OutOfMemoryError:
   print(&quot;Ran out of memory during training. Try reducing the batch size or sequence length.&quot;)
torch.cuda.empty_cache()
</code></pre>
","huggingface"
"78195900","What should be the padding token for a Huggingface model?","2024-03-20 19:43:47","","0","10","<machine-learning><deep-learning><huggingface-transformers><huggingface><huggingface-tokenizers>","<p>Should it be the eos token or the PAD token? I got this error message:</p>
<blockquote>
<p>ValueError: Asking to pad but the tokenizer does not have a padding
token. Please select a token to use as <code>pad_token</code>
<code>(tokenizer.pad_token = tokenizer.eos_token e.g.)</code> or add a new pad
token via <code>tokenizer.add_special_tokens({'pad_token': '[PAD]'})</code>.</p>
</blockquote>
<p>So which of those tokens should I use?</p>
","huggingface"
"78195373","How much time can data preprocessing and annotation for fine tuning an LLM take for training it on around 1k docs","2024-03-20 17:48:05","","0","141","<data-annotations><large-language-model><huggingface><data-preprocessing>","<p>For data preprocessing, I am estimating having to do data cleaning, text normalization, parsing, tokenization, handling jargon, and data structuring for a question-answer tasked LLM.</p>
<p>I want to get an estimate of how much labor and time preprocessing and annotation can take if I am training my LLM on a corpus of around 1000 legal documents, each of approx. 100-200 pages. My base model is pile-of-law/legalbert-large-1.7M-2 (<a href=""https://huggingface.co/pile-of-law/legalbert-large-1.7M-2"" rel=""nofollow noreferrer"">https://huggingface.co/pile-of-law/legalbert-large-1.7M-2</a>) which I will further fine tune with more specific documents.</p>
<p>I am still working on estimating the timeline of my project, and have looked at some pre-trained base models for my use case so far.</p>
","huggingface"
"78193962","How to fix a bug about Accelerate which is reported 'AcceleratorState' object has no attribute 'distributed_type' by the Pycharm","2024-03-20 14:03:43","","0","197","<pytorch><huggingface><fine-tuning><accelerate>","<p>I was fine-tuning a bert-base model using codes from huggingface's example.I just copied the code from the website and then the bug occured. Code and bug are listed below. btw,the pycharm told me that accelerator's arguments should be list but I give it Dataloader.</p>
<p>Bug:<br />
AttributeError: 'AcceleratorState' object has no attribute 'distributed_type'</p>
<p>Code:</p>
<pre class=""lang-py prettyprint-override""><code>    model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
    train_dataloader = DataLoader(datasets[&quot;train&quot;], shuffle=True, batch_size=8, collate_fn=data_collator)
    eval_dataloader = DataLoader(datasets[&quot;validation&quot;], batch_size=8, collate_fn=data_collator)
    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)
    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(model, optimizer, train_dataloader, eval_dataloader)

</code></pre>
<p>I tried upgrade transformers and accelerate.But it didn't work.
transformers's version is 4.37.2
accelerate's version is 0.28.0</p>
","huggingface"
"78192441","Why doesn't Docker work on Hugging Face like it does on my laptop?","2024-03-20 10:14:52","","0","138","<caching><dockerfile><fastapi><huggingface><librosa>","<p>Issue Description:
When running a Dockerized FastAPI application on Hugging Face, encountering an error related to caching a function '__o_fold'. The application runs without error locally but fails when deployed on Hugging Face. Below is the Dockerfile, the Python code, and the error message received.</p>
<p>Python File (app/model.py):</p>
<pre><code>from fastapi import APIRouter, UploadFile
import pickle
import wave
import numpy as np
import librosa
from pathlib import Path
import os

# Load the trained model
BASE_DIR = Path(__file__).resolve(strict=True).parent

# Get the model file path from the environment variable or use a default path
MODEL_PATH = os.environ.get(&quot;NUMBA_CACHE_DIR&quot;, str(BASE_DIR / &quot;BabyCry.pkl&quot;))

with open(f&quot;{BASE_DIR}/BabyCry.pkl&quot;, &quot;rb&quot;) as f:
    model = pickle.load(f)

photo = {
    &quot;tired&quot;: &quot;https://res.cloudinary.com/dkeeazjre/image/upload/v1709667113/Photos/jctslebxolctwgsuras5.jpg&quot;,
    &quot;belly pain&quot;: &quot;https://res.cloudinary.com/dkeeazjre/image/upload/v1709667122/Photos/tcyaxoef4hww6lhwklzt.jpg&quot;,
    &quot;hungry&quot;: &quot;https://res.cloudinary.com/dkeeazjre/image/upload/v1709667131/Photos/x7p2xw9hhs00yijhsoyt.jpg&quot;,
    &quot;discomfort&quot;: &quot;https://res.cloudinary.com/dkeeazjre/image/upload/v1709667141/Photos/dwjt39eidjg29abchtof.jpg&quot;,
    &quot;burping&quot;: &quot;https://res.cloudinary.com/dkeeazjre/image/upload/v1709667151/Photos/tftrw6gelgl1ucpy0ozb.jpg&quot;
}

router = APIRouter(prefix=&quot;/baby_cry_predictor&quot;, tags=[&quot;Baby Cry Predictor&quot;])

@router.post(&quot;/&quot;)
def predicting_emotions(baby_cry_audio: UploadFile):
    try:
        # Read audio data
        with wave.open(baby_cry_audio.file, 'rb') as audio_file:
            audio_data = audio_file.readframes(-1)
            sr = audio_file.getframerate()

        # Extract features
        audio = np.frombuffer(audio_data, dtype=np.int16)
        audio = audio.astype(np.float64)
        mfccs = librosa.feature.mfcc(y=audio, sr=sr)
        mfccs_mean = np.mean(mfccs, axis=1)

        # Make prediction
        prediction = model.predict([mfccs_mean])
        if prediction[0] == &quot;belly_pain&quot;:
            prediction[0] = &quot;belly pain&quot;

        # Return prediction as string
        return {&quot;feeling&quot;: prediction[0].title(), &quot;photo&quot;: photo[prediction[0]]}

    except Exception as e:
        # Handle any potential errors during processing
        print(os.environ.get(&quot;NUMBA_DISABLE_JIT_CACHE&quot;))
        print(os.environ.get(&quot;NUMBA_CACHE_DIR&quot;))
        return {&quot;Error&quot;: str(e)}

</code></pre>
<p>Dockerfile:</p>
<pre><code># Use the official Python image as the base image
FROM python:3.10

# Set the working directory inside the container
WORKDIR /code

ENV NUMBA_DISABLE_JIT_CACHE=&quot;1&quot;

# Copy the requirements file to the working directory
COPY ./requirements.txt /code/requirements.txt

# Install dependencies
RUN pip install --no-cache-dir --upgrade -r requirements.txt

# Copy the entire current directory into the container at /app
COPY ./app /code/app

# Set the environment variable for the model file path
ENV NUMBA_CACHE_DIR=/code/app/model/BabyCry.pkl

# Command to run the FastAPI application with Uvicorn
CMD [&quot;uvicorn&quot;, &quot;app.main:app&quot;, &quot;--host&quot;, &quot;0.0.0.0&quot;, &quot;--port&quot;, &quot;7860&quot;]

</code></pre>
<p>Error Message:
<code>{ &quot;Error&quot;: &quot;cannot cache function '__o_fold': no locator available for file '/usr/local/lib/python3.10/site-packages/librosa/core/notation.py'&quot; } </code></p>
<p>Additional Details:
When running the Docker command provided by Hugging Face locally, no errors occur. However, deploying the Docker image to Hugging Face results in the above error. The Docker command used is:</p>
<pre><code>docker run -it -p 7860:7860 --platform=linux/amd64 \
    -e NUMBA_CACHE_DIR=&quot;/code/app/model/BabyCry.pkl&quot; \
    registry.hf.space/ahmed-muqawi-baby-cry-predictor:latest 

</code></pre>
<p>I attempted to run the Docker image locally on my laptop using the command provided by Hugging Face, and it executed without any errors. However, when deploying the same Docker image to Hugging Face, I encountered the '__o_fold' caching error. I expected the Docker image to run successfully on Hugging Face, similar to how it ran on my local machine.</p>
","huggingface"
"78192016","cannot upload app.py to huggingface space","2024-03-20 09:03:50","78217105","0","58","<yolo><huggingface><yolov8><gradio><ultralytics>","<p>I'm encountering a problem with creating space after uploading app.py and requirements.txt. For loading the model, I'm using this code: from ultralyticsplus import YOLO, render_result.
(I use Gradio to create a website.)</p>
<p>The main task for uploading app.py to the space is that I want the HTML code to be embedded on the Google site.</p>
<pre><code>model_path = ('(my model path on huggingface')
model = YOLO(model_path)
</code></pre>
<p>If I use another method to load the model instead of using YOLO, is it possible to fix this error?</p>
<p>The error said</p>
<pre><code>Traceback (most recent call last):
  File &quot;/usr/local/lib/python3.10/site-packages/ultralyticsplus/ultralytics_utils.py&quot;, line 59, in __init__
    self._load_from_hf_hub(model, hf_token=hf_token)
  File &quot;/usr/local/lib/python3.10/site-packages/ultralyticsplus/ultralytics_utils.py&quot;, line 91, in _load_from_hf_hub
    ) = self._assign_ops_from_task()
  File &quot;/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py&quot;, line 1614, in __getattr__
    raise AttributeError(&quot;'{}' object has no attribute '{}'&quot;.format(
AttributeError: 'YOLO' object has no attribute '_assign_ops_from_task'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File &quot;/home/user/app/app.py&quot;, line 95, in &lt;module&gt;
    gr.Interface(fn=detect_objects,
  File &quot;/usr/local/lib/python3.10/site-packages/gradio/interface.py&quot;, line 518, in __init__
    self.render_examples()
  File &quot;/usr/local/lib/python3.10/site-packages/gradio/interface.py&quot;, line 851, in render_examples
    self.examples_handler = Examples(
  File &quot;/usr/local/lib/python3.10/site-packages/gradio/helpers.py&quot;, line 71, in create_examples
    examples_obj.create()
  File &quot;/usr/local/lib/python3.10/site-packages/gradio/helpers.py&quot;, line 298, in create
    client_utils.synchronize_async(self.cache)
  File &quot;/usr/local/lib/python3.10/site-packages/gradio_client/utils.py&quot;, line 889, in synchronize_async
    return fsspec.asyn.sync(fsspec.asyn.get_loop(), func, *args, **kwargs)  # type: ignore
  File &quot;/usr/local/lib/python3.10/site-packages/fsspec/asyn.py&quot;, line 103, in sync
    raise return_result
  File &quot;/usr/local/lib/python3.10/site-packages/fsspec/asyn.py&quot;, line 56, in _runner
    result[0] = await coro
  File &quot;/usr/local/lib/python3.10/site-packages/gradio/helpers.py&quot;, line 360, in cache
    prediction = await Context.root_block.process_api(
  File &quot;/usr/local/lib/python3.10/site-packages/gradio/blocks.py&quot;, line 1695, in process_api
    result = await self.call_function(
  File &quot;/usr/local/lib/python3.10/site-packages/gradio/blocks.py&quot;, line 1235, in call_function
    prediction = await anyio.to_thread.run_sync(
  File &quot;/usr/local/lib/python3.10/site-packages/anyio/to_thread.py&quot;, line 56, in run_sync
    return await get_async_backend().run_sync_in_worker_thread(
  File &quot;/usr/local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py&quot;, line 2144, in run_sync_in_worker_thread
    return await future
  File &quot;/usr/local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py&quot;, line 851, in run
    result = context.run(func, *args)
  File &quot;/usr/local/lib/python3.10/site-packages/gradio/utils.py&quot;, line 692, in wrapper
    response = f(*args, **kwargs)
  File &quot;/home/user/app/app.py&quot;, line 24, in detect_objects
    model = YOLO(model_path)
  File &quot;/usr/local/lib/python3.10/site-packages/ultralyticsplus/ultralytics_utils.py&quot;, line 65, in __init__
    raise NotImplementedError(
NotImplementedError: Unable to load model='MvitHYF/v8mvitcocoaseed2024'. As an example try model='yolov8n.pt' or model='yolov8n.yaml'
</code></pre>
<p>I also ran app.py on VSCode, and everything ran perfectly (run on localhost). However, I encountered this error when trying to create the space. I tried adding yolov8n.pt to both the model and the space site, but nothing changed. At first, I thought it might fix the error.</p>
<p>Thank you for you help</p>
","huggingface"
"78191498","failed: Cannot find module '../bin/napi-v3/win32/ia32/onnxruntime_binding.node in HuggingFaceTransformersEmbeddings","2024-03-20 07:31:52","","0","30","<typescript><huggingface-transformers><langchain><huggingface><onnxruntime>","<p>I am building a VS Code extension in TypeScript. For one feature, I am using the HuggingFaceTransformersEmbeddings module for embedding files. When I run it in debug mode, it's working fine. However, when I build the VSIX and install it, I am getting an activation error.</p>
<pre><code>import { HuggingFaceTransformersEmbeddings } from &quot;@langchain/community/embeddings/hf_transformers&quot;;

Activating extension 'xyz-vscode-extension' failed: Cannot find module '../bin/napi-v3/win32/x64/onnxruntime_binding.node'
</code></pre>
","huggingface"
"78191415","Fine Tuning StableDiffusionSafetyChecker model","2024-03-20 07:10:07","","0","79","<python-3.x><pytorch><huggingface-transformers><huggingface><stable-diffusion>","<p>I have been trying to fine-tune the basic model for safety checker hosted at <a href=""https://huggingface.co/CompVis/stable-diffusion-safety-checker"" rel=""nofollow noreferrer"">https://huggingface.co/CompVis/stable-diffusion-safety-checker</a> which is mostly used by all the AI image gen models.</p>
<p>I have tried to make small changes to the main code in Diffusers library and try to get it to train. The final class looks like this right now</p>
<pre><code>def cosine_distance(image_embeds, text_embeds):
    normalized_image_embeds = nn.functional.normalize(image_embeds)
    normalized_text_embeds = nn.functional.normalize(text_embeds)
    return torch.mm(normalized_image_embeds, normalized_text_embeds.t())

class StableDiffusionSafetyChecker(PreTrainedModel):
    config_class = CLIPConfig

    _no_split_modules = [&quot;CLIPEncoderLayer&quot;]

    def __init__(self, config: CLIPConfig):
        super().__init__(config)

        self.vision_model = CLIPVisionModel(config.vision_config)
        self.visual_projection = nn.Linear(config.vision_config.hidden_size, config.projection_dim, bias=False)

        self.concept_embeds = nn.Parameter(torch.ones(17, config.projection_dim), requires_grad=False)
        self.special_care_embeds = nn.Parameter(torch.ones(3, config.projection_dim), requires_grad=False)

        self.concept_embeds_weights = nn.Parameter(torch.ones(17), requires_grad=False)
        self.special_care_embeds_weights = nn.Parameter(torch.ones(3), requires_grad=False)

    @torch.no_grad()
    def forward(self, clip_input):
        pooled_output = self.vision_model(clip_input)[1]  # pooled_output
        image_embeds = self.visual_projection(pooled_output)

        # we always cast to float32 as this does not cause significant overhead and is compatible with bfloat16
        special_cos_dist = cosine_distance(image_embeds, self.special_care_embeds) #.cpu().float().numpy()
        cos_dist = cosine_distance(image_embeds, self.concept_embeds) #.cpu().float().numpy()

        result = []
        batch_size = image_embeds.shape[0]
        for i in range(batch_size):
            result_img = {&quot;special_scores&quot;: {}, &quot;special_care&quot;: [], &quot;concept_scores&quot;: {}, &quot;bad_concepts&quot;: []}

            # increase this value to create a stronger `nfsw` filter
            # at the cost of increasing the possibility of filtering benign images
            adjustment = 0.0

            for concept_idx in range(len(special_cos_dist[0])):
                concept_cos = special_cos_dist[i][concept_idx]
                concept_threshold = self.special_care_embeds_weights[concept_idx].item()
                result_img[&quot;special_scores&quot;][concept_idx] = torch.round(concept_cos - concept_threshold + adjustment, decimals=3)
                if result_img[&quot;special_scores&quot;][concept_idx] &gt; 0:
                    result_img[&quot;special_care&quot;].append({concept_idx, result_img[&quot;special_scores&quot;][concept_idx]})
                    adjustment = 0.01

            for concept_idx in range(len(cos_dist[0])):
                concept_cos = cos_dist[i][concept_idx]
                concept_threshold = self.concept_embeds_weights[concept_idx].item()
                result_img[&quot;concept_scores&quot;][concept_idx] = torch.round(concept_cos - concept_threshold + adjustment, decimals=3)
                if result_img[&quot;concept_scores&quot;][concept_idx] &gt; 0:
                    result_img[&quot;bad_concepts&quot;].append(concept_idx)

            result.append(result_img)

        has_nsfw_concepts = [len(res[&quot;bad_concepts&quot;]) &gt; 0 for res in result]

        has_nsfw_concepts = [('True' if flag else 'False') for flag in has_nsfw_concepts]
        return 0, has_nsfw_concepts
</code></pre>
<p>and the code for loading data and training looks like this:</p>
<pre><code>ds = load_dataset('training_data')

label = ds['train'].features['label']

feature_extractor_config = json.load(open('./safety_checker/preprocessor_config.json','r'))
feature_extractor = CLIPImageProcessor(**feature_extractor_config, padding=True)
# pipe_sc = StableDiffusionSafetyChecker.from_pretrained('./safety_checker')

def transform(example_batch):
    # Take a list of PIL images and turn them to pixel values
    inputs = feature_extractor([x for x in example_batch['image']], padding=True, return_tensors='pt') #.pixel_values

    inputs['label'] = example_batch['label']
    return inputs

prepared_ds = ds.with_transform(transform)
print(prepared_ds)

def collate_fn(batch):
    return {
        'clip_input': torch.stack([x['pixel_values'] for x in batch]),
        # 'label': torch.tensor([x['label'] for x in batch])
    }

metric = load_metric(&quot;accuracy&quot;)
def compute_metrics(p):
    return metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids)

labels = ds['train'].features['label'].names
print(labels)

model = StableDiffusionSafetyChecker.from_pretrained(
    './safety_checker',
    num_labels=len(labels),
    id2label={str(i): c for i, c in enumerate(labels)},
    label2id={c: str(i) for i, c in enumerate(labels)}
)

from transformers import TrainingArguments

training_args = TrainingArguments(
  output_dir=&quot;./safety_checker-update&quot;,
  per_device_train_batch_size=16,
  evaluation_strategy=&quot;steps&quot;,
  num_train_epochs=4,
  save_steps=100,
  eval_steps=100,
  logging_steps=10,
  learning_rate=2e-4,
  save_total_limit=2,
  remove_unused_columns=False,
  push_to_hub=False,
  report_to='tensorboard',
  load_best_model_at_end=True,
)

from transformers import Trainer
os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &quot;&quot;

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=collate_fn,
    compute_metrics=compute_metrics,
    train_dataset=prepared_ds[&quot;train&quot;],
    eval_dataset=prepared_ds[&quot;validation&quot;],
    tokenizer=transform,
)

train_results = trainer.train()
trainer.save_model()
trainer.log_metrics(&quot;train&quot;, train_results.metrics)
trainer.save_metrics(&quot;train&quot;, train_results.metrics)
trainer.save_state()
</code></pre>
<p>But after Running the code I am facing this error</p>
<pre><code>Traceback (most recent call last):
  File &quot;*****/stable_diff_sc_trainer.py&quot;, line 80, in &lt;module&gt;
    train_results = trainer.train()
                    ^^^^^^^^^^^^^^^
  File &quot;/home/ubuntu/anaconda3/envs/image/lib/python3.11/site-packages/transformers/trainer.py&quot;, line 1624, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File &quot;/home/ubuntu/anaconda3/envs/image/lib/python3.11/site-packages/transformers/trainer.py&quot;, line 1961, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/home/ubuntu/anaconda3/envs/image/lib/python3.11/site-packages/transformers/trainer.py&quot;, line 2911, in training_step
    self.accelerator.backward(loss)
  File &quot;/home/ubuntu/anaconda3/envs/image/lib/python3.11/site-packages/accelerate/accelerator.py&quot;, line 2001, in backward
    loss.backward(**kwargs)
    ^^^^^^^^^^^^^
AttributeError: 'float' object has no attribute 'backward'
  0%|          | 0/4 [00:07&lt;?, ?it/s] 
</code></pre>
<p>When I searched around on google, gpt and gemini, most of the sources said that I need to add the part of loss calculation for it to work. Can someone guide me on what I will need to do to be able to train this model?</p>
","huggingface"
"78187083","'CTCTrainer' object has no attribute 'use_amp'","2024-03-19 13:40:04","","0","190","<python><large-language-model><huggingface><pre-trained-model><huggingface-trainer>","<p>i am trying to run a pretrained model and i found this code thats similar to what im trying to do , when i try to run it i get an error</p>
<p>here are some bits of code to understand the context</p>
<pre><code>from typing import Any, Dict, Union

import torch
from packaging import version
from torch import nn

from transformers import (
    Trainer,
    is_apex_available,
)

if is_apex_available():
    from apex import amp

if version.parse(torch.__version__) &gt;= version.parse(&quot;1.6&quot;):
    _is_native_amp_available = True
    from torch.cuda.amp import autocast


class CTCTrainer(Trainer):
    def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -&gt; torch.Tensor:
        &quot;&quot;&quot;
        Perform a training step on a batch of inputs.

        Subclass and override to inject custom behavior.

        Args:
            model (:obj:`nn.Module`):
                The model to train.
            inputs (:obj:`Dict[str, Union[torch.Tensor, Any]]`):
                The inputs and targets of the model.

                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the
                argument :obj:`labels`. Check your model's documentation for all accepted arguments.

        Return:
            :obj:`torch.Tensor`: The tensor with training loss on this batch.
        &quot;&quot;&quot;

        model.train()
        inputs = self._prepare_inputs(inputs)

        if self.use_amp:
            with autocast():
                loss = self.compute_loss(model, inputs)
        else:
            loss = self.compute_loss(model, inputs)

        if self.args.gradient_accumulation_steps &gt; 1:
            loss = loss / self.args.gradient_accumulation_steps

        if self.use_amp:
            self.scaler.scale(loss).backward()
        elif self.use_apex:
            with amp.scale_loss(loss, self.optimizer) as scaled_loss:
                scaled_loss.backward()
        elif self.deepspeed:
            self.deepspeed.backward(loss)
        else:
            loss.backward()

        return loss.detach()
</code></pre>
<pre><code>trainer.train()
</code></pre>
<p>i get 'CTCTrainer' object has no attribute 'use_amp' and when i use use_cuda_amp instead of use_amp</p>
<pre><code>from typing import Any, Dict, Union

import torch
from packaging import version
from torch import nn

from transformers import (
    Trainer,
    is_apex_available,
)

if is_apex_available():
    from apex import amp

if version.parse(torch.__version__) &gt;= version.parse(&quot;1.6&quot;):
    _is_native_amp_available = True
    from torch.cuda.amp import autocast


class CTCTrainer(Trainer):
    def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -&gt; torch.Tensor:
        &quot;&quot;&quot;
        Perform a training step on a batch of inputs.

        Subclass and override to inject custom behavior.

        Args:
            model (:obj:`nn.Module`):
                The model to train.
            inputs (:obj:`Dict[str, Union[torch.Tensor, Any]]`):
                The inputs and targets of the model.

                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the
                argument :obj:`labels`. Check your model's documentation for all accepted arguments.

        Return:
            :obj:`torch.Tensor`: The tensor with training loss on this batch.
        &quot;&quot;&quot;

        model.train()
        inputs = self._prepare_inputs(inputs)

        if self.use_cuda_amp:
            with autocast():
                loss = self.compute_loss(model, inputs)
        else:
            loss = self.compute_loss(model, inputs)

        if self.args.gradient_accumulation_steps &gt; 1:
            loss = loss / self.args.gradient_accumulation_steps

        if self.use_cuda_amp:
            self.scaler.scale(loss).backward()
        elif self.use_apex:
            with amp.scale_loss(loss, self.optimizer) as scaled_loss:
                scaled_loss.backward()
        elif self.deepspeed:
            self.deepspeed.backward(loss)
        else:
            loss.backward()

        return loss.detach()
</code></pre>
<p>i get 'CTCTrainer' object has no attribute 'use_cuda_amp'</p>
","huggingface"
"78186564","""ModuleNotFoundError: No module named 'sagemaker.huggingface' despite installing sagemaker package""","2024-03-19 12:20:47","","0","217","<python><conda><amazon-sagemaker><large-language-model><huggingface>","<p>I am trying to use the <code>sagemaker.huggingface</code> module to run a hugging face estimator as described in <a href=""https://huggingface.co/blog/sagemaker-distributed-training-seq2seq#create-a-huggingface-estimator-and-start-training"" rel=""nofollow noreferrer"">this blog</a>, but I encounter the following error:</p>
<pre><code>ModuleNotFoundError: No module named 'sagemaker.huggingface'; 'sagemaker' is not a package
</code></pre>
<p>This is the line of code it gets that error on, in the first line of my python file:</p>
<pre><code>from sagemaker.huggingface import HuggingFace
</code></pre>
<p>I have installed the <code>sagemaker</code> package (version <code>2.213.0</code> when I run <code>conda list</code>) using <code>conda install sagemaker</code> without any errors on my system. If I do <code>import sagemaker</code>, I don't get the error. However, when I check if the <code>huggingface</code> submodule is included in the <code>sagemaker</code> package using the following code:</p>
<pre class=""lang-py prettyprint-override""><code>if 'huggingface' in dir(sagemaker):
    print('The huggingface submodule is included in this version of sagemaker.')
else:
    print('The huggingface submodule is not included in this version of sagemaker.')
</code></pre>
<p>I get the output:</p>
<pre><code>The huggingface submodule is not included in this version of sagemaker.
</code></pre>
<p>I am using Python 3.10.13 (when I check <code>python --version</code>) and have created a new conda environment named &quot;distill&quot; with the following packages installed:</p>
<pre><code>conda create --name distill python=3.10.6 -y
conda activate distill
conda install -y pytorch==1.12.1 torchvision==0.13.1 torchaudio==0.12.1 cudatoolkit=11.3 -c pytorch
pip install git+https://github.com/huggingface/transformers@v4.24.0 datasets sentencepiece protobuf==3.20.* tensorboardX
</code></pre>
<p>I am running SageMaker Distribution 1.4 on SageMaker Studio.</p>
<p>What could be causing this issue, and how can I resolve it to use the <code>sagemaker.huggingface</code> module?</p>
","huggingface"
"78185563","Mark a point Gaussian Splatting on WEBGL","2024-03-19 09:45:25","","0","54","<typescript><user-interface><3d><webgl><huggingface>","<p>I've been experimenting with Gaussian Splatting neural networks after reading about them on Hugging Face's <a href=""https://huggingface.co/blog/gaussian-splatting"" rel=""nofollow noreferrer"">blog</a>. I've utilized one of the available open-source models to create a 3D file from a collection of 1k 2D images. The outcome has been quite impressive, as seen in this example: <a href=""https://gsplat.tech/excavator/"" rel=""nofollow noreferrer"">Excavator</a>.</p>
<p>Being relatively new to the 3D realm, I'm eager to enhance my project by enabling users to mark points in space using their mouse. Converting mouse clicks on the canvas to coordinates in the 3D world is straightforward. However, I'm unsure how to mark a point in space at a distance from the camera's location. Clicking on the screen and converting to 3D space yields coordinates on the camera plane, but how can I extend this to mark a point in 3D space at a certain distance from the camera?</p>
","huggingface"
"78182841","`run` not supported when there is not exactly one output key. Got ['quiz', 'review']","2024-03-18 20:24:53","","0","49","<python><huggingface-transformers><langchain><huggingface><py-langchain>","<p><strong>My ERROR</strong></p>
<pre><code>`run` not supported when there is not exactly one output key. Got ['quiz', 'review'].
</code></pre>
<p><strong>MY CODE</strong></p>
<p><strong>Libraries</strong></p>
<pre><code>import os 
import json
import pandas as pd
import traceback
from  langchain import PromptTemplate , HuggingFaceHub ,LLMChain
from langchain.chains import SequentialChain
from dotenv import load_dotenv
</code></pre>
<p><strong>Loading my env and key</strong></p>
<pre><code>load_dotenv()
key = os.getenv(&quot;hugging_face_key&quot;)
os.environ['HUGGINGFACEHUB_API_TOKEN'] = key
</code></pre>
<p><strong>Initializing My LLM model from HuggingFace</strong></p>
<pre><code>llm=HuggingFaceHub(repo_id='google/flan-t5-base',model_kwargs={'temperature':0.5})
</code></pre>
<p><strong>My Prompt Template and Response : -</strong> made a two chain 1.Quiz 2.review</p>
<pre><code>RESPONSE_JSON = {
    &quot;1&quot;: {
        &quot;mcq&quot;: &quot;multiple choice question&quot;,
        &quot;options&quot;: {
            &quot;a&quot;: &quot;choice here&quot;,
            &quot;b&quot;: &quot;choice here&quot;,
            &quot;c&quot;: &quot;choice here&quot;,
            &quot;d&quot;: &quot;choice here&quot;,
        },
        &quot;correct&quot;: &quot;correct answer&quot;,
    },
    &quot;2&quot;: {
        &quot;mcq&quot;: &quot;multiple choice question&quot;,
        &quot;options&quot;: {
            &quot;a&quot;: &quot;choice here&quot;,
            &quot;b&quot;: &quot;choice here&quot;,
            &quot;c&quot;: &quot;choice here&quot;,
            &quot;d&quot;: &quot;choice here&quot;,
        },
        &quot;correct&quot;: &quot;correct answer&quot;,
    },
    &quot;3&quot;: {
        &quot;mcq&quot;: &quot;multiple choice question&quot;,
        &quot;options&quot;: {
            &quot;a&quot;: &quot;choice here&quot;,
            &quot;b&quot;: &quot;choice here&quot;,
            &quot;c&quot;: &quot;choice here&quot;,
            &quot;d&quot;: &quot;choice here&quot;,
        },
        &quot;correct&quot;: &quot;correct answer&quot;,
    },
}


TEMPLATE=&quot;&quot;&quot;
Text:{text}
You are an expert MCQ maker. Given the above text, it is your job to \
create a quiz of {number} multiple choice questions for {subject} students in {tone} tone,
Make sure the question are not repeated and check all the questions to be conforming the text as well.
Make sure to format your response like RESPONSE_JSON below and use it as a guide. \
Ensure to make {number} MCQs
### RESPONSE_JSON
{response_json}

&quot;&quot;&quot;

quiz_generation_prompt = PromptTemplate(
    input_variables=[&quot;text&quot;,&quot;number&quot;,&quot;subject&quot;,&quot;tone&quot;,&quot;response_json&quot;],
    template = TEMPLATE
)

quiz_chain=LLMChain(llm=llm,prompt=quiz_generation_prompt,output_key=&quot;quiz&quot;,verbose=True)

TEMPLATE2 = &quot;&quot;&quot;
You are an expert english grammarian and writer . Given a Multiple Choice Quiz for {subject} students.\
You need to evaluate the complexity of the question and give a complete analysis of the quiz.Only use at max 50 words for complexity
if the quiz is not at per with the cognitive and analytical abilities of the students,\
update the quiz questions which needs to be changed and change the tone such that it perfectly fits the student abilities 
Quiz_MCQs:
{quiz}

Check from an expert English Writer of the above quiz:


quiz_evaluation_prompt = PromptTemplate(input_variables=[&quot;subject&quot;,&quot;quiz&quot;],template=TEMPLATE2)


review_chain =LLMChain(llm=llm,prompt=quiz_evaluation_prompt,output_key=&quot;review&quot;,verbose=True)
</code></pre>
<p><strong>Making a Sequential Chain : -</strong> For Executing both chain in sequence</p>
<pre><code>generate_evaluate_chain=SequentialChain(chains=[quiz_chain,review_chain],input_variables=[&quot;text&quot;,&quot;number&quot;,&quot;subject&quot;,&quot;tone&quot;,&quot;response_json&quot;],
                                        output_variables=[&quot;quiz&quot;,&quot;review&quot;],verbose=True)
</code></pre>
<p><strong>Adding Data File:-</strong>  Which contain Text of my Quiz</p>
<pre><code>path = &quot;C:\Hardik\Ai_tools\MCQ_Generator\data.txt&quot;

with open(path,'r') as file:
    TEXT = file.read()
</code></pre>
<p><strong>Searialize the python dictionary into a json-formated string</strong></p>
<pre><code>json.dumps(RESPONSE_JSON)
</code></pre>
<pre><code>NUMBER = 5
SUBJECT = &quot;machine learning&quot;
TONE = &quot;simple&quot;
</code></pre>
<p><strong>Response</strong> :- it runs the model and give the output</p>
<pre><code>response = generate_evaluate_chain.run(text=TEXT, number=NUMBER, subject=SUBJECT, tone=TONE, response_json=json.dumps(RESPONSE_JSON))
</code></pre>
<p><strong>ERROR:</strong> While Executing Response Cell it is giving me ValueError</p>
<p><strong>ValueError</strong></p>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[39], line 1
----&gt; 1 response = generate_evaluate_chain.run(text=TEXT, number=NUMBER, subject=SUBJECT, tone=TONE, response_json=json.dumps(RESPONSE_JSON), output_key='quiz')

File c:\Hardik\Ai_tools\MCQ_Generator\Mcq\lib\site-packages\langchain_core\_api\deprecation.py:145, in deprecated.&lt;locals&gt;.deprecate.&lt;locals&gt;.warning_emitting_wrapper(*args, **kwargs)
    143     warned = True
    144     emit_warning()
--&gt; 145 return wrapped(*args, **kwargs)

File c:\Hardik\Ai_tools\MCQ_Generator\Mcq\lib\site-packages\langchain\chains\base.py:540, in Chain.run(self, callbacks, tags, metadata, *args, **kwargs)
    503 &quot;&quot;&quot;Convenience method for executing chain.
    504 
    505 The main difference between this method and `Chain.__call__` is that this
   (...)
    537         # -&gt; &quot;The temperature in Boise is...&quot;
    538 &quot;&quot;&quot;
    539 # Run at start to make sure this is possible/defined
--&gt; 540 _output_key = self._run_output_key
    542 if args and not kwargs:
    543     if len(args) != 1:

File c:\Hardik\Ai_tools\MCQ_Generator\Mcq\lib\site-packages\langchain\chains\base.py:488, in Chain._run_output_key(self)
    485 @property
    486 def _run_output_key(self) -&gt; str:
...
    490             f&quot;one output key. Got {self.output_keys}.&quot;
    491         )
    492     return self.output_keys[0]

ValueError: `run` not supported when there is not exactly one output key. Got ['quiz', 'review'].
</code></pre>
<p>I have go through the documentation as well and look for some solution stating to use predict in place of run but there is no function named predict in SequentialChain model</p>
","huggingface"
"78182686","Why does Seq2SeqTrainer produces error during evaluation when using T5?","2024-03-18 19:47:39","","0","172","<nlp><huggingface-transformers><huggingface><huggingface-trainer>","<p>Following the tutorial <a href=""https://huggingface.co/docs/transformers/en/tasks/summarization"" rel=""nofollow noreferrer"">here</a>. I've tried to adapt it to my dataset.</p>
<p>But, I've noticed that during evaluation the <code>Seq2SeqTrainer</code> calls the <code>compute_metrics</code> <code>3</code> times.</p>
<p>The first time it passes the correct validation/test set, but the other <code>2</code> times I don't know what the hell is passing on or why is calling the <code>compute_metrics</code> <code>3</code> times?</p>
<p>Notice in the screenshot below the validation set has 6400 samples, which is correctly passed to the <code>compute_metrics</code> the first time it is being called by the <code>Seq2SeqTrainer</code>, but what are the other <code>2</code> calls passing the second time <code>predictions</code> and <code>labels</code> of size <code>127</code> and third time it calls the <code>compute_metrics</code> it passes some type of scalar values for both the <code>predictions</code> and <code>labels</code>.</p>
<p>Could anyone explain what the hell is going on here?</p>
<p>Why does the <code>Seq2SeqTrainer</code> calls the <code>compute_metrics</code> 3 times when it should only call it once passing the actual predictions and labels on validation set which is of size <code>6400</code> samples?</p>
<pre><code>training_args = Seq2SeqTrainingArguments(
    output_dir=&quot;t5_checkpoints&quot;,
    learning_rate=2e-5,
    per_device_train_batch_size=640,
    per_device_eval_batch_size=640,
    num_train_epochs=10,
    weight_decay=0.01,
    evaluation_strategy=&quot;epoch&quot;,
    save_strategy=&quot;epoch&quot;,
    save_total_limit=3,
    predict_with_generate=True,
    generation_max_length=128,
    generation_num_beams=4,
    load_best_model_at_end=True,
    logging_steps=1,
)
</code></pre>
<pre><code>trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_data[&quot;train&quot;],
    eval_dataset=tokenized_data[&quot;valid&quot;],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)
</code></pre>
<p><a href=""https://i.sstatic.net/YdLiG.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/YdLiG.png"" alt=""enter image description here"" /></a></p>
","huggingface"
"78176105","Filtering all documents that belong to consumer protection from the EUR-Multilex Huggingface dataset","2024-03-17 16:49:09","","0","5","<dataset><huggingface><huggingface-datasets>","<p>For a personal project, I am looking to filter out all documents from the coastalcph/multi_eurlex dataset on Hugging Face, that belong to the legal space of consumer protection laws. According to the classification done by <a href=""https://aclanthology.org/2020.emnlp-main.607/"" rel=""nofollow noreferrer"">Chalkidis et al. (2020)</a>, there even exists a specific label for consumer protection (2836).</p>
<p>Turns out that the dataset does not contain any document that is categorized with this id.</p>
<p>Did I miss something here? Or is there any other way to efficiently filter this dataset?</p>
<p>This is the code I use for transformation of the labels to the eurovoc_id:</p>
<pre><code># Define a function that adds a new feature containing an empty list to each document
def add_empty_list_feature(regulation_document):
    regulation_document['eurovoc_labels_id'] = []  # Add an empty list to store the new values when they are translated
    regulation_document['eurovoc_labels_description'] = [] 

    # Iterate over each label_id in 'labels' for the example
    for label_id in regulation_document['labels']:
        # Get the EuroVoc ID as a string
        eurovoc_id = classlabel.int2str(label_id)
        # Find the corresponding EuroVoc description
        eurovoc_desc = eurovoc_concepts[eurovoc_id]
        # Append the EuroVoc ID and description to the translations list
        regulation_document['eurovoc_labels_id'].append(eurovoc_id)
        regulation_document['eurovoc_labels_description'].append(eurovoc_desc)

    return regulation_document

# Apply the function to each document in the dataset
dataset_with_eurovoc_labels = dataset_train.map(add_empty_list_feature)
</code></pre>
<pre><code>def filter_by_eurovoc_label(dataset, eurovoc_labels = ['2836']):
  &quot;&quot;&quot;Return all documents from the dataset that have matching eurovoc_labels (in the
  case of consumer protection - 2836)

    Parameters:
    dataset (List): Dataset that contains all eurolex documents from hugging face dataset
    eurovoc_labels (List): Contains all relevant labels that we want to filter for, DEFAULT = 2836 (consumer protection)

    Returns:
    a new datastructure with all filtered regulation

   &quot;&quot;&quot;
  filtered_regulation = []
  for document in dataset:
    # Check if any of the EuroVoc IDs in the document match the specified labels
      if any(label in eurovoc_labels for label in document['eurovoc_labels_id']):
        filtered_regulation.append(document)
  return filtered_regulation
</code></pre>
","huggingface"
"78165875","How to broadcast a tensor from main process using Accelerate?","2024-03-15 09:30:29","78175879","1","295","<python><pytorch><huggingface><accelerate>","<p>I want to do some computation in the main process and broadcast the tensor to other processes. Here is a sketch of what my code looks like currently:</p>
<pre class=""lang-py prettyprint-override""><code>from accelerate.utils import broadcast

x = None
if accelerator.is_local_main_process:
    x = &lt;do_some_computation&gt;
    x = broadcast(x)  # I have even tried moving this line out of the if block
print(x.shape)
</code></pre>
<p>This gives me following error:
<code>TypeError: Unsupported types (&lt;class 'NoneType'&gt;) passed to `_gpu_broadcast_one` . Only nested list/tuple/dicts of objects that are valid for `is_torch_tensor` s hould be passed.</code></p>
<p>Which means that <code>x</code> is still <code>None</code> and is not really being broadcasted. How do I fix this?</p>
","huggingface"
"78165495","use SeamlessM4Tv2Model, I want to slow down the rate of speech of audio output","2024-03-15 08:17:22","","1","33","<python><pytorch><text-to-speech><huggingface>","<pre><code>text_inputs = processor(text=&quot;I have a daughter 2 years old, I wanted her name to be Hương Ly&quot;, src_lang=&quot;eng&quot;, return_tensors=&quot;pt&quot;).to(device)
audio_array = model.generate(**text_inputs, tgt_lang=language)[0].cpu().numpy().squeeze()
file_path = 'audio_from_text.wav'
sf.write(file_path, audio_array, 16000)
</code></pre>
<p><a href=""https://huggingface.co/docs/transformers/main/en/model_doc/seamless_m4t_v2"" rel=""nofollow noreferrer"">doc</a>
[<a href=""https://huggingface.co/spaces/Imadsarvm/Sarvm_Audio_Translation/resolve/b5b7fa364f8c567c4eb330583f673cd4c600976b/app.py?download=true"" rel=""nofollow noreferrer"">ex</a>]</p>
<p>it has returned a 3 seconds audio</p>
<p>I try adding <code>speech_temperature=0.2</code> or <code>speech_do_sample=True</code> to <code>generate()</code> but there is no change, it still has returned a 3 seconds audio, for example, I want to change the rate of speech so it will be 5 seconds audio
any ideal ?</p>
","huggingface"
"78164651","Using the ENCODE function","2024-03-15 04:17:25","","0","11","<deep-learning><transformer-model><huggingface>","<p>when running this code:
data = next(iter(train_dataset))</p>
<p>print(&quot;Example data from the dataset: \n&quot;, data)</p>
<hr />
<p>{'answers': {'answer_start': [515], 'text': ['Saint Bernadette Soubirous']}, 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend &quot;Venite Ad Me Omnes&quot;. Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.', 'id': '5733be284776f41900661182', 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?', 'title': 'University_of_Notre_Dame'}</p>
<hr />
<p>and  when running this code:
train_ds=  train_dataset.map(encode)</p>
<p>valid_ds=  valid_dataset.map(encode)</p>
<p>ex = next(iter(train_ds))</p>
<p>print(&quot;Example data from the mapped dataset: \n&quot;, ex)</p>
<hr />
<p>{'answers': {'answer_start': [515], 'text': ['Saint Bernadette Soubirous']}, 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend &quot;Venite Ad Me Omnes&quot;. Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.', 'decoder_attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'id': '5733be284776f41900661182', 'input_ids': [1525, 834, 526, 10, 304, 4068, 410, 8, 16823, 3790, 3, 18280, 2385, 16, 507, 3449, 16, 301, 1211, 1395, 1410, 58, 2625, 10, 30797, 120, 6, 8, 496, 65, 3, 9, 6502, 1848, 5, 71, 2916, 8, 5140, 5450, 31, 7, 2045, 22161, 19, 3, 9, 7069, 12647, 13, 8, 16823, 3790, 5, 3, 29167, 16, 851, 13, 8, 5140, 5450, 11, 5008, 34, 6, 19, 3, 9, 8658, 12647, 13, 2144, 28, 6026, 3, 76, 24266, 28, 8, 9503, 96, 553, 15, 7980, 1980, 1212, 13285, 1496, 1280, 3021, 12, 8, 5140, 5450, 19, 8, 23711, 2617, 13, 8, 3, 24756, 6219, 5, 3, 29167, 1187, 8, 20605, 2617, 19, 8, 8554, 17, 235, 6, 3, 9, 17535, 286, 13, 7029, 11, 9619, 5, 94, 19, 3, 9, 16455, 13, 8, 3, 3844, 17, 235, 44, 301, 1211, 1395, 6, 1410, 213, 8, 16823, 3790, 3, 28285, 26, 120, 4283, 12, 2788, 8942, 9, 26, 1954, 264, 8371, 8283, 16, 507, 3449, 5, 486, 8, 414, 13, 8, 711, 1262, 41, 232, 16, 3, 9, 1223, 689, 24, 1979, 7, 190, 220, 12647, 7, 11, 8, 2540, 10576, 15, 201, 19, 3, 9, 650, 6, 941, 3372, 12647, 13, 3790, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [2788, 8942, 9, 26, 1954, 264, 8371, 8283, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?', 'title': 'University_of_Notre_Dame'}</p>
<hr />
<p>The question is attention_mask': [1, 1, 1, 1, 1, .... Where does it come from and what does it mean and 'decoder_attention_mask': 'input_ids': Where do labels come from and why do they appear What does it mean?</p>
","huggingface"
"78160182","How to finetune an already Finetuned Llama 2?","2024-03-14 11:34:08","","0","16","<large-language-model><huggingface><llama><amazon-sagemaker-studio>","<p>I finetuned a Llama2 7b model using huggingface interface for sagemaker, but I want to finetune the model again on a another dataset. Is this possible? I looked around a lot, but have not found anything in this regard.</p>
","huggingface"
"78158233","Having ""torch.distributed.elastic.multiprocessing.errors.ChildFailedError:"" error when using accelerator","2024-03-14 05:08:49","","0","841","<pytorch><huggingface-transformers><huggingface><accelerate><huggingface-trainer>","<p>I'm tring to use accelerate for fine-tuning LLM with FSDP
and i'm having problem with the error
maybe my code doesn't work so i did basic accelerate test and it doesn't work</p>
<p>this is my accelerate env</p>
<ul>
<li><code>Accelerate</code> version: 0.27.2</li>
<li>Platform: Linux-5.4.0-96-generic-x86_64-with-glibc2.31</li>
<li>Python version: 3.9.5</li>
<li>Numpy version: 1.26.2</li>
<li>PyTorch version (GPU?): 2.2.0+cu121 (True)</li>
<li>PyTorch XPU available: False</li>
<li>PyTorch NPU available: False</li>
<li>System RAM: 1007.76 GB</li>
<li>GPU type: NVIDIA A40</li>
<li><code>Accelerate</code> default config:
- compute_environment: LOCAL_MACHINE
- distributed_type: FSDP
- mixed_precision: bf16
- use_cpu: False
- debug: False
- num_processes: 3
- machine_rank: 0
- num_machines: 1
- rdzv_backend: static
- same_network: True
- main_training_function: main
- fsdp_config: {'fsdp_auto_wrap_policy': 'TRANSFORMER_BASED_WRAP', 'fsdp_backward_prefetch': 'BACKWARD_PRE', 'fsdp_cpu_ram_efficient_loading': True, 'fsdp_forward_prefetch': False, 'fsdp_offload_params': False, 'fsdp_sharding_strategy': 'FULL_SHARD', 'fsdp_state_dict_type': 'SHARDED_STATE_DICT', 'fsdp_sync_module_states': True, 'fsdp_transformer_layer_cls_to_wrap': 'LlamaDecoderLayer', 'fsdp_use_orig_params': True}
- downcast_bf16: no
- tpu_use_cluster: False
- tpu_use_sudo: False
- tpu_env: []</li>
</ul>
<p>and i did accelerate test</p>
<pre><code>accelerate test


Running:  accelerate-launch /usr/local/lib/python3.9/dist-packages/accelerate/test_utils/scripts/test_script.py
stderr: Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
stdout: **Initialization**
stdout: Testing, testing. 1, 2, 3.
stdout: Distributed environment: FSDP  Backend: nccl
stdout: Num processes: 3
stdout: Process index: 0
stdout: Local process index: 0
stdout: Device: cuda:0
stdout:
stdout: Mixed precision type: bf16
stdout:
stdout: Distributed environment: FSDP  Backend: nccl
stdout: Num processes: 3
stdout: Process index: 1
stdout: Local process index: 1
stdout: Device: cuda:1
stdout:
stdout: Mixed precision type: bf16
stdout:
stdout: Distributed environment: FSDP  Backend: nccl
stdout: Num processes: 3
stdout: Process index: 2
stdout: Local process index: 2
stdout: Device: cuda:2
stdout:
stdout: Mixed precision type: bf16
stdout:
stdout:
stdout: **Test process execution**
stderr: Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
stdout:
stdout: **Test split between processes as a list**
stdout:
stdout: **Test split between processes as a dict**
stdout:
stdout: **Test split between processes as a tensor**
stdout:
stdout: **Test random number generator synchronization**
stdout: All rng are properly synched.
stdout:
stdout: **DataLoader integration test**
stdout: 1 0 2 tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
stdout:         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
stdout:         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
stdout:         54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,
stdout:         72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,
stdout:         90, 91, 92, 93, 94, 95], device='cuda:0') &lt;class 'accelerate.data_loader.DataLoaderShard'&gt;
stdout: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
stdout:         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
stdout:         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
stdout:         54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,
stdout:         72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,
stdout:         90, 91, 92, 93, 94, 95], device='cuda:2') &lt;class 'accelerate.data_loader.DataLoaderShard'&gt;
stdout: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
stdout:         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
stdout:         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
stdout:         54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,
stdout:         72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,
stdout:         90, 91, 92, 93, 94, 95], device='cuda:1') &lt;class 'accelerate.data_loader.DataLoaderShard'&gt;
stderr: Traceback (most recent call last):
stderr:   File &quot;/usr/local/lib/python3.9/dist-packages/accelerate/test_utils/scripts/test_script.py&quot;, line 708, in &lt;module&gt;
stderr: Traceback (most recent call last):
stderr:   File &quot;/usr/local/lib/python3.9/dist-packages/accelerate/test_utils/scripts/test_script.py&quot;, line 708, in &lt;module&gt;
stderr: Traceback (most recent call last):
stderr:   File &quot;/usr/local/lib/python3.9/dist-packages/accelerate/test_utils/scripts/test_script.py&quot;, line 708, in &lt;module&gt;
stderr:     main()
stderr:   File &quot;/usr/local/lib/python3.9/dist-packages/accelerate/test_utils/scripts/test_script.py&quot;, line 687, in main
stderr:     main()
stderr:   File &quot;/usr/local/lib/python3.9/dist-packages/accelerate/test_utils/scripts/test_script.py&quot;, line 687, in main
stderr:     dl_preparation_check()
stderr:   File &quot;/usr/local/lib/python3.9/dist-packages/accelerate/test_utils/scripts/test_script.py&quot;, line 196, in dl_preparation_check
stderr:     dl = prepare_data_loader(
stderr:   File &quot;/usr/local/lib/python3.9/dist-packages/accelerate/data_loader.py&quot;, line 858, in prepare_data_loader
stderr:     dl_preparation_check()
stderr:   File &quot;/usr/local/lib/python3.9/dist-packages/accelerate/test_utils/scripts/test_script.py&quot;, line 196, in dl_preparation_check
stderr:     main()
stderr:     dl = prepare_data_loader(  File &quot;/usr/local/lib/python3.9/dist-packages/accelerate/test_utils/scripts/test_script.py&quot;, line 687, in main
stderr:
stderr:   File &quot;/usr/local/lib/python3.9/dist-packages/accelerate/data_loader.py&quot;, line 858, in prepare_data_loader
stderr:     raise ValueError(
stderr: ValueError: To use a `DataLoader` in `split_batches` mode, the batch size (8) needs to be a round multiple of the number of processes (3).
stderr:     dl_preparation_check()
stderr:   File &quot;/usr/local/lib/python3.9/dist-packages/accelerate/test_utils/scripts/test_script.py&quot;, line 196, in dl_preparation_check
stderr:     raise ValueError(
stderr: ValueError: To use a `DataLoader` in `split_batches` mode, the batch size (8) needs to be a round multiple of the number of processes (3).
stderr:     dl = prepare_data_loader(
stderr:   File &quot;/usr/local/lib/python3.9/dist-packages/accelerate/data_loader.py&quot;, line 858, in prepare_data_loader
stderr:     raise ValueError(
stderr: ValueError: To use a `DataLoader` in `split_batches` mode, the batch size (8) needs to be a round multiple of the number of processes (3).
stderr: [2024-03-14 13:39:21,081] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 27324) of binary: /usr/bin/python3.9
stderr: Traceback (most recent call last):
stderr:   File &quot;/usr/local/bin/accelerate-launch&quot;, line 8, in &lt;module&gt;
stderr:     sys.exit(main())
stderr:   File &quot;/usr/local/lib/python3.9/dist-packages/accelerate/commands/launch.py&quot;, line 1029, in main
stderr:     launch_command(args)
stderr:   File &quot;/usr/local/lib/python3.9/dist-packages/accelerate/commands/launch.py&quot;, line 1010, in launch_command
stderr:     multi_gpu_launcher(args)
stderr:   File &quot;/usr/local/lib/python3.9/dist-packages/accelerate/commands/launch.py&quot;, line 672, in multi_gpu_launcher
stderr:     distrib_run.run(args)
stderr:   File &quot;/usr/local/lib/python3.9/dist-packages/torch/distributed/run.py&quot;, line 803, in run
stderr:     elastic_launch(
stderr:   File &quot;/usr/local/lib/python3.9/dist-packages/torch/distributed/launcher/api.py&quot;, line 135, in __call__
stderr:     return launch_agent(self._config, self._entrypoint, list(args))
stderr:   File &quot;/usr/local/lib/python3.9/dist-packages/torch/distributed/launcher/api.py&quot;, line 268, in launch_agent
stderr:     raise ChildFailedError(
stderr: torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
stderr: ============================================================
stderr: /usr/local/lib/python3.9/dist-packages/accelerate/test_utils/scripts/test_script.py FAILED
stderr: ------------------------------------------------------------
stderr: Failures:
stderr: [1]:
stderr:   time      : 2024-03-14_13:39:21
stderr:   host      : 919c8ff8c821
stderr:   rank      : 1 (local_rank: 1)
stderr:   exitcode  : 1 (pid: 27325)
stderr:   error_file: &lt;N/A&gt;
stderr:   traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
stderr: [2]:
stderr:   time      : 2024-03-14_13:39:21
stderr:   host      : 919c8ff8c821
stderr:   rank      : 2 (local_rank: 2)
stderr:   exitcode  : 1 (pid: 27326)
stderr:   error_file: &lt;N/A&gt;
stderr:   traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
stderr: ------------------------------------------------------------
stderr: Root Cause (first observed failure):
stderr: [0]:
stderr:   time      : 2024-03-14_13:39:21
stderr:   host      : 919c8ff8c821
stderr:   rank      : 0 (local_rank: 0)
stderr:   exitcode  : 1 (pid: 27324)
stderr:   error_file: &lt;N/A&gt;
stderr:   traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
</code></pre>
<p>I can't find the problem</p>
","huggingface"
"78150102","pytorch sdpa compatibility with relative positional embeddings","2024-03-12 21:22:17","","0","74","<machine-learning><huggingface><self-attention>","<p>Hi Still confused on whether it is possible for rotary and relative positional embeddings to be integrated with the fast kernels in pytorch sdpa, allowing for faster training/inference? If so what would be a blue print of how to merge previous architecture that use both to the new sdpa one ?</p>
<p>I would want to integrate sdpa into the ESM model, available on huggingface at <a href=""https://huggingface.co/docs/transformers/en/model_doc/esm"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/en/model_doc/esm</a>. Here is the section of the code of interest, the forward call of the attention</p>
<pre><code>def forward(
    self,
    hidden_states: torch.Tensor,
    attention_mask: Optional[torch.FloatTensor] = None,
    head_mask: Optional[torch.FloatTensor] = None,
    encoder_hidden_states: Optional[torch.FloatTensor] = None,
    encoder_attention_mask: Optional[torch.FloatTensor] = None,
    past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,
    output_attentions: Optional[bool] = False,
) -&gt; Tuple[torch.Tensor]:
    mixed_query_layer = self.query(hidden_states)

    # If this is instantiated as a cross-attention module, the keys
    # and values come from an encoder; the attention mask needs to be
    # such that the encoder's padding tokens are not attended to.
    is_cross_attention = encoder_hidden_states is not None

    if is_cross_attention and past_key_value is not None:
        # reuse k,v, cross_attentions
        key_layer = past_key_value[0]
        value_layer = past_key_value[1]
        attention_mask = encoder_attention_mask
    elif is_cross_attention:
        key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))
        value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))
        attention_mask = encoder_attention_mask
    elif past_key_value is not None:
        key_layer = self.transpose_for_scores(self.key(hidden_states))
        value_layer = self.transpose_for_scores(self.value(hidden_states))
        key_layer = torch.cat([past_key_value[0], key_layer], dim=2)
        value_layer = torch.cat([past_key_value[1], value_layer], dim=2)
    else:
        key_layer = self.transpose_for_scores(self.key(hidden_states))
        value_layer = self.transpose_for_scores(self.value(hidden_states))

    query_layer = self.transpose_for_scores(mixed_query_layer)

    # Matt: Our BERT model (which this code was derived from) scales attention logits down by sqrt(head_dim).
    # ESM scales the query down by the same factor instead. Modulo numerical stability these are equivalent,
    # but not when rotary embeddings get involved. Therefore, we scale the query here to match the original
    # ESM code and fix rotary embeddings.
    query_layer = query_layer * self.attention_head_size**-0.5

    if self.is_decoder:
        # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.
        # Further calls to cross_attention layer can then reuse all cross-attention
        # key/value_states (first &quot;if&quot; case)
        # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of
        # all previous decoder key/value_states. Further calls to uni-directional self-attention
        # can concat previous decoder key/value_states to current projected key/value_states (third &quot;elif&quot; case)
        # if encoder bi-directional self-attention `past_key_value` is always `None`
        past_key_value = (key_layer, value_layer)

    if self.position_embedding_type == &quot;rotary&quot;:
        query_layer, key_layer = self.rotary_embeddings(query_layer, key_layer)

    # Take the dot product between &quot;query&quot; and &quot;key&quot; to get the raw attention scores.
    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))

    if self.position_embedding_type == &quot;relative_key&quot; or self.position_embedding_type == &quot;relative_key_query&quot;:
        seq_length = hidden_states.size()[1]
        position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)
        position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)
        distance = position_ids_l - position_ids_r
        positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)
        positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility

        if self.position_embedding_type == &quot;relative_key&quot;:
            relative_position_scores = torch.einsum(&quot;bhld,lrd-&gt;bhlr&quot;, query_layer, positional_embedding)
            attention_scores = attention_scores + relative_position_scores
        elif self.position_embedding_type == &quot;relative_key_query&quot;:
            relative_position_scores_query = torch.einsum(&quot;bhld,lrd-&gt;bhlr&quot;, query_layer, positional_embedding)
            relative_position_scores_key = torch.einsum(&quot;bhrd,lrd-&gt;bhlr&quot;, key_layer, positional_embedding)
            attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key

    if attention_mask is not None:
        # Apply the attention mask is (precomputed for all layers in EsmModel forward() function)
        attention_scores = attention_scores + attention_mask

    # Normalize the attention scores to probabilities.
    attention_probs = nn.functional.softmax(attention_scores, dim=-1)

    # This is actually dropping out entire tokens to attend to, which might
    # seem a bit unusual, but is taken from the original Transformer paper.
    attention_probs = self.dropout(attention_probs)

    # Mask heads if we want to
    if head_mask is not None:
        attention_probs = attention_probs * head_mask

    context_layer = torch.matmul(attention_probs, value_layer)

    context_layer = context_layer.permute(0, 2, 1, 3).contiguous()
    new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)
    context_layer = context_layer.view(new_context_layer_shape)

    outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)

    if self.is_decoder:
        outputs = outputs + (past_key_value,)
    return outputs
</code></pre>
<p>How would you change the code to replace with the pytorch function sdpa at <a href=""https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html#torch.nn.functional.scaled_dot_product_attention"" rel=""nofollow noreferrer"">https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html#torch.nn.functional.scaled_dot_product_attention</a>, specifically, how/could you change the relative positional encoding into a attention mask that could be passed on to the function? Scouring online it seems possible, meaning the scaled_dot_product_attention function can expect an arbitrary mask but not sure how this would be achieved in this case.</p>
","huggingface"
"78148989","Use huggingface's model using new data","2024-03-12 17:25:00","","0","18","<python><deep-learning><dataset><huggingface>","<p>Is there a way to use huggingface's model but retrain it on my data? My data is specifically Vietnamese accented words. I don't know how to put my data into the model. model and I want the model to be able to answer Vietnamese questions based on the Vietnamese data I provide to it. I don't know what to do with the data problem:</p>
<ol>
<li>Don't know whether to choose Json or csv data type</li>
<li>How should the data be structured so that the model can understand it?</li>
<li>The model I am choosing is Google's T5, specifically google/flan-t5-base</li>
<li>How should I configure it?
Request I want:</li>
<li>The model answers questions based on the data provided
Please suggest me, thank you</li>
</ol>
<p>I tried to learn from YouTube, tried to read the documentation, but I still find it very difficult to understand what data structure is, and how to put it. There are a few examples on YouTube that I have tried but unfortunately they are running on colab pro I only have colab free so I can't run the whole thing, can you give a brief summary or an example of how the data should be structured? And there is an example I found and I feel like The most confusing part is the endcode, why does it output 0 and 1 at the same time?
The code containing the printed value ex = next(iter(train_ds))
print(&quot;Example data from the mapped dataset: \n&quot;, ex)
here is the link:
<a href=""https://colab.research.google.com/github/snapthat/TF-T5-text-to-text/blob/master/snapthatT5/notebooks/TF-T5-Datasets%20Training.ipynb#scrollTo=OQ5CoTk-DXN6"" rel=""nofollow noreferrer"">https://colab.research.google.com/github/snapthat/TF-T5-text-to-text/blob/master/snapthatT5/notebooks/TF-T5-Datasets%20Training.ipynb#scrollTo=OQ5CoTk-DXN6</a></p>
","huggingface"
"78147258","No Safetensor Weights found - Hugging Face Docker Chat UI","2024-03-12 12:47:54","","0","39","<docker><large-language-model><huggingface>","<p>I wanted to build a HuggingFace space, using BioMistral 7B and the Docker Chat UI Template. Unfortunately it gives me following error:</p>
<blockquote>
<p><code>2024-03-11T17:18:50.175605Z  WARN text_generation_launcher: No safetensors weights found for model biomistral/BioMistral-7B at revision None. Downloading PyTorch weights. 2024-03-11T17:18:50.204434Z  INFO text_generation_launcher: Download file: pytorch_model.bin 0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0 0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0 curl: (7) Failed to connect to 127.0.0.1 port 8080 after 0 ms: Connection refused Warning: Problem : connection refused. Will retry in 10 seconds. 59 retries  Warning: left.</code></p>
</blockquote>
<p>It keeps running and fails, also when naming another model in .gguf with safetensor files. Anybody knows why?</p>
","huggingface"
"78143603","Getting TypeError when using JSON parser on llama.cpp Open Source LLM using Langchain","2024-03-11 21:57:15","","0","116","<json><langchain><large-language-model><huggingface><llama-cpp-python>","<p>I'm trying JSON parser on a Llama.cpp open source model with Langchain.</p>
<p>Here is the sample code:</p>
<pre><code>
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.pydantic_v1 import BaseModel, Field
from llama_cpp import Llama
llm = Llama(model_path='./mistral-7b-instruct-v0.2.Q4_0.gguf')



# Define your desired data structure.
class Joke(BaseModel):
    setup: str = Field(description=&quot;question to set up a joke&quot;)
    punchline: str = Field(description=&quot;answer to resolve the joke&quot;)
    
# And a query intented to prompt a language model to populate the data structure.
joke_query = &quot;Tell me a joke.&quot;

# Set up a parser + inject instructions into the prompt template.
parser = JsonOutputParser(pydantic_object=Joke)

prompt = PromptTemplate(
    template=&quot;Answer the user query.\n{format_instructions}\n{query}\n&quot;,
    input_variables=[&quot;query&quot;],
    partial_variables={&quot;format_instructions&quot;: parser.get_format_instructions()},
)

#Chain
chain = prompt | llm | parser
#Run
chain.invoke({&quot;query&quot;: joke_query}
</code></pre>
<p>I'm getting following error:</p>
<p><strong>TypeError: object of type 'StringPromptValue' has no len()</strong></p>
<p>I followed the instruction over the Langchain website.</p>
<p>Does this technique work on local llama_cpp models?
if not, is there another way to get this output in JSON.</p>
","huggingface"
"78139065","How to use batch prediction in Diffusers.StableDiffusionXLImg2ImgPipeline library","2024-03-11 07:48:46","","2","191","<python><pytorch><huggingface><stable-diffusion><diffusers>","<p>I'm currently exploring the StableDiffusion Image to Image library within HuggingFace. My goal is to generate images similar to the ones I have stored in a folder. Currently, I'm using the following code snippet:</p>
<pre><code>import torch
from diffusers.utils import load_image
from diffusers import StableDiffusionXLImg2ImgPipeline

pipe = StableDiffusionXLImg2ImgPipeline.from_pretrained(
    &quot;stabilityai/stable-diffusion-xl-refiner-1.0&quot;, torch_dtype=torch.float16, variant=&quot;fp16&quot;, use_safetensors=True
)

pipe = pipe.to(&quot;cuda&quot;)
url = &quot;MyImages\ImageList\998.jpg&quot;

init_image = load_image(url).convert(&quot;RGB&quot;)
prompt = &quot;Give me a similar image like this&quot;
image = pipe(prompt, image=init_image).images
</code></pre>
<p>this code requires me to generate each image manually, one by one. I can write a for loop like this -</p>
<pre><code>all_images = os.listdir('MyImages\ImageList\')
for img in all_images:
  ...
  ...

</code></pre>
<p>I'm considering the possibility of processing images in batches rather than individually. Is there a method or a library within HuggingFace that allows for batch processing of images to generate similar ones?</p>
","huggingface"
"78137822","Running Mistral-7B-Instruct-v0.2 on multiple GPUs","2024-03-10 23:25:00","","0","1171","<huggingface-transformers><large-language-model><huggingface><mistral-7b>","<p>I'm trying to run a pretty straightforward script. I just want to experiment running my own chat offline on my setup using Mistral-7B-Instruct-v0.2 model.
My setup is relatively old, I helped some researchers with it back in the day. It's four Geforce GTX 1080 cards, with 8 GB RAM each.</p>
<p>If my script looks overly complicated, it's because I've been manipulating it a lot wishing that it could run.
Here's the script:</p>
<pre><code>import torch
import json
from transformers import AutoTokenizer, AutoModelForCausalLM

def generate_text(input_text, num_texts=2, max_length=100, num_beams=5, early_stopping=True):
    # Set the GPUs to use
    device_ids = [0, 1, 2, 3]  # Modify this list according to your GPU configuration
    primary_device = f'cuda:{device_ids[0]}'  # Primary device
    torch.cuda.set_device(primary_device)

    # Load the tokenizer and model
    tokenizer = AutoTokenizer.from_pretrained(&quot;MistralAI/Mistral-7B-Instruct-v0.2&quot;)
    tokenizer.add_special_tokens({'pad_token': '[PAD]'})
    model = AutoModelForCausalLM.from_pretrained(&quot;MistralAI/Mistral-7B-Instruct-v0.2&quot;).to(primary_device)

    # Move model to GPUs
    model = torch.nn.DataParallel(model, device_ids=device_ids)

    # Tokenize the input text and move to the primary device
    inputs = tokenizer(input_text, return_tensors=&quot;pt&quot;, max_length=512, padding=True, truncation=True)
    inputs = {k: v.to(primary_device) for k, v in inputs.items()}

    # Generate multiple texts using different random seeds
    generated_texts = []
    for i in range(num_texts):
        # Set the random seed for reproducibility
        torch.manual_seed(i)

        # Generate the text using the model
        with torch.no_grad():
            outputs = model.module.generate(**inputs, max_length=max_length, num_beams=num_beams, early_stopping=early_stopping)

        # Decode and add the generated text to the list
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        generated_texts.append(generated_text)

    return generated_texts

if __name__ == &quot;__main__&quot;:
    # Set the input text and style
    input_text = &quot;Tell me a story about a dragon and a princess.&quot;

    # Generate texts
    generated_texts = generate_text(input_text)

    # Write the generated texts to a JSON file
    with open(&quot;generated_texts.json&quot;, &quot;w&quot;) as f:
        json.dump(generated_texts, f)
</code></pre>
<p>This is my output:</p>
<pre><code>Loading checkpoint shards: 100%|████████████████████████████████████████████████████| 3/3 [00:02&lt;00:00,  1.16it/s]
Traceback (most recent call last):
  File &quot;myscript.py&quot;, line 44, in &lt;module&gt;
    generated_texts = generate_text(input_text)
  File &quot;myscript.py&quot;, line 14, in generate_text
    model = AutoModelForCausalLM.from_pretrained(&quot;MistralAI/Mistral-7B-Instruct-v0.2&quot;).to(primary_device)
  File &quot;/home/user/Transformers/lib/python3.8/site-packages/transformers/modeling_utils.py&quot;, line 2556, in to
    return super().to(*args, **kwargs)
  File &quot;/home/user/Transformers/lib/python3.8/site-packages/torch/nn/modules/module.py&quot;, line 1152, in to
    return self._apply(convert)
  File &quot;/home/user/Transformers/lib/python3.8/site-packages/torch/nn/modules/module.py&quot;, line 802, in _apply
    module._apply(fn)
  File &quot;/home/user/Transformers/lib/python3.8/site-packages/torch/nn/modules/module.py&quot;, line 802, in _apply
    module._apply(fn)
  File &quot;/home/user/Transformers/lib/python3.8/site-packages/torch/nn/modules/module.py&quot;, line 802, in _apply
    module._apply(fn)
  [Previous line repeated 2 more times]
  File &quot;/home/user/Transformers/lib/python3.8/site-packages/torch/nn/modules/module.py&quot;, line 825, in _apply
    param_applied = fn(param)
  File &quot;/home/user/Transformers/lib/python3.8/site-packages/torch/nn/modules/module.py&quot;, line 1150, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 7.92 GiB of which 86.81 MiB is free. Including non-PyTorch memory, this process has 7.12 GiB memory in use. Of the allocated memory 7.02 GiB is allocated by PyTorch, and 1.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
</code></pre>
<p><a href=""https://i.sstatic.net/roHAP.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/roHAP.png"" alt=""enter image description here"" /></a></p>
<p>Basically, as I show in my screenshot, I manage to make it run on only one GPU. I can pick the GPU but I cant make it use the other 3.
Correct me if I'm wrong, but it is now my understanding that I have to manually split the model in the 4 GPUs?
That is, the model must fit entirely in one GPU even if I want to use all 4 GPUs?</p>
<p>Mistral says on their site that the model requires 16GB. Each GPU has 8GB. I've tried to search  about model parallelism and pipeline parallelism, sharded data parallelism, I don't find much regarding this model in particular but mostly these are concepts I don't have experience on.</p>
<p>Do I need to put the SLI on? back in the day you didn't need it for training, but this is for inference.</p>
<p>This leaves me with the question, what about bigger models like Mistral-8X7B-v0.1 that require 100GB? A100s only have 80GB, the entire model doesn't fit in a single GPU.</p>
<p>I'm aware I could run it on some cloud architecture but it kind of beats the purpose of what I'm trying to do at the moment, and it implies spending resources I could probably spare since I have this setup, why not use it?</p>
<p>I hope you could guide me on this. Thanks a lot.</p>
<p>I already tried specifying the GPUs that are visible on the script. I also specified all 4 GPUs as visible on .bashrc <code>export CUDA_VISIBLE_DEVICES=0,1,2,3</code>, but still only one GPU is used.</p>
","huggingface"
"78135361","ValueError: 4.39.0.dev0 is not valid SemVer string","2024-03-10 09:32:26","","-1","272","<python><pytorch><huggingface-transformers><large-language-model><huggingface>","<p>I got the following error <code>ValueError: 4.39.0.dev0 is not valid SemVer string</code> in my code:</p>
<pre class=""lang-py prettyprint-override""><code>from GLiNER.gliner_ner import GlinerNER

gli = GlinerNER()
</code></pre>
<p>Complete error message:</p>
<pre class=""lang-py prettyprint-override""><code>File ~/.../GLiNER/gliner_ner.py:8, in GlinerNER.__init__(self, labels)
      7 def __init__(self, labels = [&quot;date&quot;,&quot;time&quot;, &quot;club&quot;, &quot;league&quot;]):
----&gt; 8     self.model = GLiNER.from_pretrained(&quot;urchade/gliner_base&quot;)
      9     self.labels = labels

File /opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:118, in validate_hf_hub_args.&lt;locals&gt;._inner_fn(*args, **kwargs)
    115 if check_use_auth_token:
    116     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.__name__, has_token=has_token, kwargs=kwargs)
--&gt; 118 return fn(*args, **kwargs)

File /opt/conda/lib/python3.10/site-packages/huggingface_hub/hub_mixin.py:157, in ModelHubMixin.from_pretrained(cls, pretrained_model_name_or_path, force_download, resume_download, proxies, token, cache_dir, local_files_only, revision, **model_kwargs)
    154         config = json.load(f)
    155     model_kwargs.update({&quot;config&quot;: config})
--&gt; 157 return cls._from_pretrained(
    158     model_id=str(model_id),
    159     revision=revision,
    160     cache_dir=cache_dir,
    161     force_download=force_download,
    162     proxies=proxies,
    163     resume_download=resume_download,
    164     local_files_only=local_files_only,
    165     token=token,
    166     **model_kwargs,
    167 )

File ~/.../GLiNER/model.py:355, in GLiNER._from_pretrained(cls, model_id, revision, cache_dir, force_download, proxies, resume_download, local_files_only, token, map_location, strict, **model_kwargs)
    353 model = cls(config)
    354 state_dict = torch.load(model_file, map_location=torch.device(map_location))
--&gt; 355 model.load_state_dict(state_dict, strict=strict, 
    356                       #assign=True
    357                      )
    358 model.to(map_location)
    359 return model

File /opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:2027, in Module.load_state_dict(self, state_dict, strict)
   2020         out = hook(module, incompatible_keys)
   2021         assert out is None, (
   2022             &quot;Hooks registered with ``register_load_state_dict_post_hook`` are not&quot;
   2023             &quot;expected to return new values, if incompatible_keys need to be modified,&quot;
   2024             &quot;it should be done inplace.&quot;
   2025         )
-&gt; 2027 load(self, state_dict)
   2028 del load
   2030 if strict:

File /opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:2015, in Module.load_state_dict.&lt;locals&gt;.load(module, local_state_dict, prefix)
   2013         child_prefix = prefix + name + '.'
   2014         child_state_dict = {k: v for k, v in local_state_dict.items() if k.startswith(child_prefix)}
-&gt; 2015         load(child, child_state_dict, child_prefix)
   2017 # Note that the hook can modify missing_keys and unexpected_keys.
   2018 incompatible_keys = _IncompatibleKeys(missing_keys, unexpected_keys)

File /opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:2015, in Module.load_state_dict.&lt;locals&gt;.load(module, local_state_dict, prefix)
   2013         child_prefix = prefix + name + '.'
   2014         child_state_dict = {k: v for k, v in local_state_dict.items() if k.startswith(child_prefix)}
-&gt; 2015         load(child, child_state_dict, child_prefix)
   2017 # Note that the hook can modify missing_keys and unexpected_keys.
   2018 incompatible_keys = _IncompatibleKeys(missing_keys, unexpected_keys)

File /opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:2009, in Module.load_state_dict.&lt;locals&gt;.load(module, local_state_dict, prefix)
   2007 def load(module, local_state_dict, prefix=''):
   2008     local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})
-&gt; 2009     module._load_from_state_dict(
   2010         local_state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)
   2011     for name, child in module._modules.items():
   2012         if child is not None:

File /opt/conda/lib/python3.10/site-packages/flair/embeddings/transformer.py:1166, in TransformerEmbeddings._load_from_state_dict(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)
   1163 def _load_from_state_dict(
   1164     self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs
   1165 ):
-&gt; 1166     if transformers.__version__ &gt;= Version(4, 31, 0):
   1167         assert isinstance(state_dict, dict)
   1168         state_dict.pop(f&quot;{prefix}model.embeddings.position_ids&quot;, None)

File /opt/conda/lib/python3.10/site-packages/semver/version.py:51, in _comparator.&lt;locals&gt;.wrapper(self, other)
     49 if not isinstance(other, comparable_types):
     50     return NotImplemented
---&gt; 51 return operator(self, other)

File /opt/conda/lib/python3.10/site-packages/semver/version.py:481, in Version.__le__(self, other)
    479 @_comparator
    480 def __le__(self, other: Comparable) -&gt; bool:
--&gt; 481     return self.compare(other) &lt;= 0

File /opt/conda/lib/python3.10/site-packages/semver/version.py:396, in Version.compare(self, other)
    394 cls = type(self)
    395 if isinstance(other, String.__args__):  # type: ignore
--&gt; 396     other = cls.parse(other)
    397 elif isinstance(other, dict):
    398     other = cls(**other)

File /opt/conda/lib/python3.10/site-packages/semver/version.py:646, in Version.parse(cls, version, optional_minor_and_patch)
    644     match = cls._REGEX.match(version)
    645 if match is None:
--&gt; 646     raise ValueError(f&quot;{version} is not valid SemVer string&quot;)
    648 matched_version_parts: Dict[str, Any] = match.groupdict()
    649 if not matched_version_parts[&quot;minor&quot;]:

ValueError: 4.39.0.dev0 is not valid SemVer string
</code></pre>
<p>That's the transformer version:</p>
<pre class=""lang-py prettyprint-override""><code>Name: transformers
Version: 4.38.2
Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow
Home-page: https://github.com/huggingface/transformers
Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)
Author-email: transformers@huggingface.co
License: Apache 2.0 License
Location: /opt/conda/lib/python3.10/site-packages
Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm
Required-by: flair, sentence-transformers, transformer-smaller-training-vocab
Note: you may need to restart the kernel to use updated packages.
</code></pre>
<p>I don't know, what the cause is and I hope you can help to find the cause.</p>
<p>Thanks in advance.</p>
","huggingface"
"78128694","Huggingface Seq2seqTrainer freezes on evaluation","2024-03-08 15:24:41","78271786","0","398","<python><huggingface-transformers><huggingface><openai-whisper><huggingface-trainer>","<p>I'm currently trying to train a Whisper model by following the <a href=""https://huggingface.co/blog/fine-tune-whisper#training-and-evaluation"" rel=""nofollow noreferrer"">Fine Tune Whisper Model</a> tutorial. However, during the training phase where I call <code>trainer.train()</code>. I see the progress bar progresses through the training, but when it reaches the evaluation step defined at the training arguments, it will just freeze and the progress bar just stalls up. No error output, no nothing. And it will look like this.</p>
<p><a href=""https://i.sstatic.net/GJkza.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/GJkza.png"" alt=""enter image description here"" /></a></p>
<p>I'm using Kaggle notebooks to write the code with GPU P100 turned on. Here are my training arguments leading up to the training function.</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import WhisperForConditionalGeneration

model = WhisperForConditionalGeneration.from_pretrained(&quot;openai/whisper-small&quot;)

model.config.forced_decoder_ids = None
model.config.suppress_tokens = []
model.generation_config.language = &quot;en&quot;
</code></pre>
<pre class=""lang-py prettyprint-override""><code>from transformers import Seq2SeqTrainingArguments

training_args = Seq2SeqTrainingArguments(
    output_dir=&quot;./whisper-small-eng-gen&quot;,  # change to a repo name of your choice
    per_device_train_batch_size=16,
    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size
    learning_rate=1e-5,
    warmup_steps=500,
    max_steps=1000,
    gradient_checkpointing=True,
    fp16=True,
    evaluation_strategy=&quot;steps&quot;,
    per_device_eval_batch_size=8,
    predict_with_generate=True,
    generation_max_length=225,
    save_steps=1000,
    eval_steps=1000,
    logging_steps=25,
    report_to=[&quot;tensorboard&quot;],
    load_best_model_at_end=True,
    metric_for_best_model=&quot;wer&quot;,
    greater_is_better=False,
    push_to_hub=True,
    ignore_data_skip=True
)
</code></pre>
<pre class=""lang-py prettyprint-override""><code>from transformers import Seq2SeqTrainer

trainer = Seq2SeqTrainer(
    args=training_args,
    model=model,
    train_dataset=common_voice_train,
    eval_dataset=common_voice_test,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    tokenizer=processor.feature_extractor,
)
</code></pre>
<blockquote>
<p>Initially, the <code>max_steps</code> for training is 4000, and it always stalls at step 1001.</p>
</blockquote>
<p>I think it is also worth noting that my dataset is streamed, and it is an Iterable Dataset.</p>
<p>Any help is appreciated!</p>
<p><strong>**Update**</strong>
I edited my code to include verbose logging with</p>
<pre class=""lang-py prettyprint-override""><code>import transformers

transformers.logging.set_verbosity_info()
</code></pre>
<p>And this is the log after the evaluation step is reached.</p>
<blockquote>
<p>You have passed language=en, but also have set <code>forced_decoder_ids</code> to [[1, None], [2, 50359]] which creates a conflict. <code>forced_decoder_ids</code> will be ignored in favor of language=en.</p>
</blockquote>
","huggingface"
"78126282","How to prevent repeated downloading with HuggingFace","2024-03-08 08:09:49","78126372","0","1289","<python><pytorch><huggingface><stable-diffusion>","<h4>Description:</h4>
<p>I am confused on how the installation of the packages are performed. Currently I was working on a StableDiffusion model and every-time I run the code its again and again downloading files which are 3 to 4 Gigs big.</p>
<h4>Code:</h4>
<p>This is the code I was trying to run at first:</p>
<pre class=""lang-py prettyprint-override""><code>from torch import autocast
from diffusers import StableDiffusionPipeline

pipe = StableDiffusionPipeline.from_pretrained(
    &quot;CompVis/stable-diffusion-v1-4&quot;, 
    use_auth_token=True
).to(&quot;cuda&quot;)

prompt = &quot;a photo of an astronaut riding a horse on mars&quot;
with autocast(&quot;cuda&quot;):
    image = pipe(prompt)[&quot;sample&quot;][0]  
    
image.save(&quot;astronaut_rides_horse.png&quot;)
</code></pre>
<h4>Issue:</h4>
<p>When I run the code the following appears in my shell:</p>
<pre class=""lang-bash prettyprint-override""><code>Fetching 16 files:   0%|                                                             | 0/16 [00:00&lt;?, ?it/s]
vae/diffusion_pytorch_model.safetensors:   0%|                                   | 0.00/335M [00:00&lt;?, ?B/s]
unet/diffusion_pytorch_model.safetensors:   0%|                                 | 0.00/3.44G [00:00&lt;?, ?B/s]
safety_checker/model.safetensors:   0%|                                         | 0.00/1.22G [00:00&lt;?, ?B/s]
text_encoder/model.safetensors:   0%|                                            | 0.00/492M [00:00&lt;?, ?B/s]
</code></pre>
<p>and this happens each and everytime I run the code.</p>
<h4>What I tried?</h4>
<p>I tried installing and cloning the whole git repo. (I honestly don't know why I did that even though I know it wasn't gonna affect a thing!) Also I tried searching for many forums for this issue but not even a single clue, maybe its because of my in-experienced approach.</p>
","huggingface"
"78122458","Error: Authorization Failed When Downloading Model from CivitAI","2024-03-07 15:18:03","","0","466","<python><python-3.x><huggingface><stable-diffusion>","<p>issues while attempting to download files through the Civitai API using your API key. The error message states that the download is being aborted, and it receives a response status of 400, indicating a client-side error.</p>
<p>been trying to get a model to download from civitai model platform for stable diffusion the api key should of allowed me to download the model because without it am getting an unauthorization to download throw civitai[<a href=""https://i.sstatic.net/V4We2.png"" rel=""nofollow noreferrer"">enter image description here</a>](<a href=""https://i.sstatic.net/SamRI.png"" rel=""nofollow noreferrer"">https://i.sstatic.net/SamRI.png</a>)</p>
","huggingface"
"78119935","Text to xml generation with LLM dataset creation","2024-03-07 08:50:30","","0","364","<large-language-model><huggingface><xml-generation>","<p>I'm a newbie in LLM, what I want to achieve is to train LLM in the way that It generates from text description to XML of some blocks,</p>
<p>There are some blocks with input and output ports with some connections in between, i want it to describe it in XML</p>
<p>How can do that? any hint?
Thanks in advance</p>
","huggingface"
"78119454","How to configure `gr.ChatInterface` to return multiple outputs (response & source documents)?","2024-03-07 07:25:54","","1","646","<python><user-interface><huggingface><gradio>","<p>I have this <code>gr.ChatInterface</code> that I want to adjust to also show to the user document sources that were used on retrieval (meaning, adding another output)</p>
<pre class=""lang-py prettyprint-override""><code>import gradio as gr

def generate_response(message, history):   
    print(f&quot;\n\n[message] {message}&quot;)
    # call LLM &amp; generate response
    return response.answer


demo = gr.ChatInterface(
    fn=generate_response,
    title=&quot;RAG app for Q&amp;A&quot;,
    description=&quot;Ask any question about Stuff&quot;,
).queue(default_concurrency_limit=2, max_size=10)

demo.launch(share=True)
    
</code></pre>
<p>I already tried <code>outputs</code> but it's not supported by <code>gr.ChatInterface</code>:</p>
<pre class=""lang-py prettyprint-override""><code>Traceback (most recent call last):
  File &quot;/workspaces/aider_repos/app.py&quot;, line 20, in &lt;module&gt;
    demo = gr.ChatInterface(
           ^^^^^^^^^^^^^^^^^
TypeError: ChatInterface.__init__() got an unexpected keyword argument 'outputs'
</code></pre>
<p>How to configure <code>gr.ChatInterface</code> to return multiple outputs (response &amp; source documents)?</p>
<pre class=""lang-py prettyprint-override""><code>Traceback (most recent call last):
  File &quot;/workspaces/aider_repos/app.py&quot;, line 20, in &lt;module&gt;
    demo = gr.ChatInterface(
           ^^^^^^^^^^^^^^^^^
TypeError: ChatInterface.__init__() got an unexpected keyword argument 'outputs'
</code></pre>
","huggingface"
"78114655","What is the difference between merging LORA weight with base model and not merging the weight in LLAMA2 (LLM)?","2024-03-06 13:12:37","","1","120","<deep-learning><large-language-model><huggingface><llama><peft>","<p>The question is regarding LLM(Large language model). I want to understand it from LLAMA2 perspective.
Can someone explain why the final outcome is almost same without combining weights? Additionally, could you please clarify the process of merging weights and the pros and cons associated with it? I'm curious about both the benefits and drawbacks of merging, as well as the pros and cons of not merging the LORA weight with base model.</p>
","huggingface"
"78111572","tortoise-tts mac m1 silicone","2024-03-06 02:46:15","","2","229","<python><text-to-speech><huggingface>","<p>I've been trying to set up tortoise on a virtualenv.
I am able to run do_tts.py and the code provided in <code>tortoise_tts.ipynb</code>.
However, I would like to run it with longer texts, thus trying <code>read.py</code> and <code>read_fast.py</code></p>
<p>In doing so, I get an error stating I don't have a GPU, so good so true, but I am not sure where to change the settings as stated</p>
<pre><code>raise RuntimeError('Attempting to deserialize object on a CUDA '
RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.
</code></pre>
<p>In which file in which line am I supposed to change that?
I checked if the files <code>read.py</code> and <code>read_fast.py</code> contain torch.load, but they don't, so I am wondering where can I make the necessary changes? (I have not made changes to the above mentioned py files, so if you could help me in clarifying where to make the change in the original code, that would be much appreciated.)</p>
<p>Best</p>
","huggingface"
"78105436","Convert PyTorch Model to Hugging Face model","2024-03-05 05:09:24","","2","882","<pytorch><huggingface-transformers><huggingface><huggingface-tokenizers><huggingface-hub>","<p>I have looked at a lot resources but I still have issues trying to convert a PyTorch model to a hugging face model format. I ultimately want to be able to use inference API with my custom model.</p>
<p>I have a &quot;model.pt&quot; file which I got from fine-tuning the Facebook Musicgen medium model (<a href=""https://github.com/chavinlo/musicgen_trainer"" rel=""nofollow noreferrer"">The Git repo I used to train / Fine tune the model is here</a>). I want to upload this to the hugging face hub so i can use this with inference API. How can I convert the <code>.pt</code> model to files/model that can be used on hugging face hub? I tried looking at other posts but there is no clear answer, or it is poorly explained.</p>
<p>Any help / guidance would be greatly appreciated 🙏</p>
<p>This is the code I have right now that is not working:</p>
<pre><code>import torch
from transformers import MusicgenConfig, MusicgenModel
from audiocraft.models import musicgen
import os

os.mkdir('models')

state_dict = musicgen.MusicGen.get_pretrained('facebook/musicgen-medium', device='cuda').lm.load_state_dict(torch.load('NEW_MODEL.pt'))

config = MusicgenConfig.from_pretrained('facebook/musicgen-medium')
model = MusicgenModel(config)
model.load_state_dict(state_dict)

model.save_pretrained('/models')

loaded_model = MusicgenModel.from_pretrained('/models')
</code></pre>
","huggingface"
"78095157","Why we use return_tensors = ""pt"" during tokenization?","2024-03-03 04:53:38","","0","3172","<large-language-model><huggingface><huggingface-tokenizers>","<p>So I am doing tokenization of my dataset, and created one function,</p>
<pre><code>max_length = 1026

def generate_and_tokenize_prompt(prompt):
    result = tokenizer(
        prompt,
        return_tensors=&quot;pt&quot;,
        truncation=True,
        max_length=max_length,
        padding=&quot;max_length&quot;,
    )
    return result

train_dataset = df_train['prompt']
val_dataset = df_test['prompt']
tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)
tokenized_val_dataset = val_dataset.map(generate_and_tokenize_prompt)
</code></pre>
<p>Here you can see we are using <code>return_tensors=&quot;pt&quot;</code>, but I am not sure why are using it. Because even without this parameters, I am able to tokenize my dataset.</p>
","huggingface"
"78088139","Minimal FSDP example utilizing the HuggingFace Trainer in AWS Sagemaker","2024-03-01 13:39:25","","0","846","<amazon-web-services><pytorch><amazon-sagemaker><large-language-model><huggingface>","<p>I'm currently trying to fine-tune a LLM in AWS Sagemaker. Since it's too big to fit on a single GPU I'm trying to distribute the model weights over multiple GPUs in an AWS Sagemaker instance. In my training script, I use the HuggingFace Trainer. Since the HuggingFace Trainer (with the fsdp parameter), the PyTorch library (with torch.distributed) as well as AWS Sagemaker (with smdistributed) all have mechanisms to enable fsdp I'm entirely confused how I can (or should?) enable FSDP for my use case.</p>
<p>I'd be very glad if someone could help me out here by providing a minimal but working example on how to enable FSDP by utilizing the HuggingFace Trainer in an AWS Sagemaker Training Job.</p>
<p>Edit: I now tried to implement the suggestion, but run into the following error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/app/train.py&quot;, line 178, in &lt;module&gt;
    train_results = trainer.train()
  File &quot;/usr/local/lib/python3.9/site-packages/transformers/trainer.py&quot;, line 1624, in train
    return inner_training_loop(
  File &quot;/usr/local/lib/python3.9/site-packages/transformers/trainer.py&quot;, line 1766, in _inner_training_loop
    self.model = self.accelerator.prepare(self.model)
  File &quot;/usr/local/lib/python3.9/site-packages/accelerate/accelerator.py&quot;, line 1228, in prepare
    result = tuple(
  File &quot;/usr/local/lib/python3.9/site-packages/accelerate/accelerator.py&quot;, line 1229, in &lt;genexpr&gt;
    self._prepare_one(obj, first_pass=True, device_placement=d) for obj, d in zip(args, device_placement)
  File &quot;/usr/local/lib/python3.9/site-packages/accelerate/accelerator.py&quot;, line 1105, in _prepare_one
    return self.prepare_model(obj, device_placement=device_placement)
  File &quot;/usr/local/lib/python3.9/site-packages/accelerate/accelerator.py&quot;, line 1328, in prepare_model
    if torch.device(current_device_index) != self.device:
TypeError: device() received an invalid combination of arguments - got (NoneType), but expected one of:
 * (torch.device device)
      didn't match because some of the arguments have invalid types: (!NoneType!)
 * (str type, int index)
</code></pre>
<p>This is my training script:</p>
<pre><code>from torch.utils.data import Dataset
import torch
from peft import (
    LoraConfig,
    get_peft_model,
    get_peft_model_state_dict,
    prepare_model_for_int8_training,
)
from datasets import load_from_disk
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq
import argparse

import sys
import os
import logging
import matplotlib.pyplot as plt
if __name__ == &quot;__main__&quot;:

    
    parser = argparse.ArgumentParser()
    # hyperparameters sent by the client
    parser.add_argument(&quot;--batch_size&quot;, type=int, default=128)
    parser.add_argument(&quot;--per_device_train_batch_size&quot;, type=int, default=32)
    parser.add_argument(&quot;--model_name&quot;, type=str, default=&quot;codellama/CodeLlama-7b-hf&quot;)
    parser.add_argument(&quot;--learn_rate&quot;, type=str, default=&quot;3e-4&quot;)
    parser.add_argument(&quot;--warmup_steps&quot;, type=int, default=400)
    # Data, model and output directories
    parser.add_argument(&quot;--output_data_dir&quot;, type=str, default=&quot;/opt/ml/output/data&quot;)
    parser.add_argument(&quot;--model-dir&quot;, type=str, default=&quot;/opt/ml/model&quot;)
    parser.add_argument(&quot;--n_gpus&quot;, type=str, default=&quot;4&quot;)
    parser.add_argument(&quot;--training_dir&quot;, type=str, default=&quot;/opt/ml/input/data/train&quot;)
    parser.add_argument(&quot;--test_dir&quot;, type=str, default=&quot;/opt/ml/input/data/test&quot;)

    args, _ = parser.parse_known_args()

    # Set up logging
    logger = logging.getLogger(__name__)

    logging.basicConfig(
        level=logging.getLevelName(&quot;INFO&quot;),
        handlers=[logging.StreamHandler(sys.stdout)],
        format=&quot;%(asctime)s - %(name)s - %(levelname)s - %(message)s&quot;
        )

    model = AutoModelForCausalLM.from_pretrained(
        args.model_name,
        load_in_8bit=True,
        torch_dtype=torch.float16
    )
    tokenizer = AutoTokenizer.from_pretrained(args.model_name)


    # %%
    tokenizer.add_eos_token = True
    tokenizer.pad_token_id = 0
    tokenizer.padding_side = &quot;left&quot;

    # %%
    class MappingDataset(Dataset):
        def __init__(self, train=True):
            if train:
                self.dataset = load_from_disk(dataset_path=args.training_dir)
                logger.info(f&quot;loaded train dataset with a length of:{len(self.dataset)}&quot;)
            else:
                self.dataset = load_from_disk(dataset_path=args.test_dir)
                logger.info(f&quot;loaded test dataset with a length of:{len(self.dataset)}&quot;)

            self.dataset = self.dataset.select(range(1000))
        def __len__(self):
            return len(self.dataset)

        def __getitem__(self, idx):
            return self.dataset[idx]

    # %%
    train_dataset = MappingDataset(train=True)
    val_dataset = MappingDataset(train=False)


    # %%
    model.train() # put model back into training mode
    model = prepare_model_for_int8_training(model)

    config = LoraConfig(
        r=16,
        lora_alpha=16,
        target_modules=[
        &quot;q_proj&quot;,
        &quot;k_proj&quot;,
        &quot;v_proj&quot;,
        &quot;o_proj&quot;,
    ],
        lora_dropout=0.05,
        bias=&quot;none&quot;,
        task_type=&quot;CAUSAL_LM&quot;,
    )
    model = get_peft_model(model, config)


    # %% [markdown]
    # The cell below keeps the Trainer from trying its own DataParallelism when more than 1 gpu is available

    # %%
    if torch.cuda.device_count() &gt; 1:
        model.is_parallelizable = True
        model.model_parallel = True

    # %%
    gradient_accumulation_steps = args.batch_size // args.per_device_train_batch_size
    output_dir = &quot;bis-mapping-code-llama&quot;

    training_args = TrainingArguments(
            per_device_train_batch_size=args.per_device_train_batch_size,
            gradient_accumulation_steps=gradient_accumulation_steps,
            warmup_steps=args.warmup_steps,
            learning_rate=float(args.learn_rate),
            fp16=True,
            logging_steps=1,
            optim=&quot;adamw_torch&quot;,
            evaluation_strategy=&quot;steps&quot;, # if val_set_size &gt; 0 else &quot;no&quot;,
            save_strategy=&quot;steps&quot;,
            eval_steps=20,
            save_steps=20,
            output_dir=args.model_dir,
            load_best_model_at_end=False,
            group_by_length=True, # group sequences of roughly the same length together to speed up training
            log_level='debug',
            logging_dir=f&quot;{args.output_data_dir}/logs&quot;
        )
    
    def compute_perplexity(pred):
        # Extract the predicted logits from the model output
        logits = pred.predictions
        # Flatten the logits and labels to compute cross-entropy loss
        logits = logits.view(-1, logits.size(-1))
        labels = pred.label_ids.view(-1)
        # Compute cross-entropy loss
        loss = torch.nn.functional.cross_entropy(logits, labels)
        # Compute perplexity
        perplexity = torch.exp(loss)
        return {&quot;perplexity&quot;: perplexity.item()}

    trainer = Trainer(
        model=model,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        args=training_args,
        compute_metrics=compute_perplexity,
        data_collator=DataCollatorForSeq2Seq(
        tokenizer, pad_to_multiple_of=8, return_tensors=&quot;pt&quot;, padding=True
    ),
    )


    # %% [markdown]
    # The cell below only serves for optimizing the model training

    # %%
    model.config.use_cache = False

    old_state_dict = model.state_dict
    model.state_dict = (lambda self, *_, **__: get_peft_model_state_dict(self, old_state_dict())).__get__(
        model, type(model)
    )
    if torch.__version__ &gt;= &quot;2&quot; and sys.platform != &quot;win32&quot;:
        print(&quot;compiling the model&quot;)
        model = torch.compile(model)


    # %%
    train_results = trainer.train()

    train_loss_values = train_results[&quot;train_loss&quot;]

    
    # Plot the loss values
    plt.plot(train_loss_values, label=&quot;Training Loss&quot;)
    plt.xlabel(&quot;Training Steps&quot;)
    plt.ylabel(&quot;Loss&quot;)
    plt.title(&quot;Training Loss over Steps&quot;)
    plt.legend()

    # Save the plot to disk
    plt.savefig(f&quot;{args.output_data_dir}/plots/training_loss_plot.png&quot;)

    # %%
    eval_results = trainer.evaluate()
    print(f&quot;Perplexity: {2**eval_results['eval_loss']}&quot;)
</code></pre>
<p>And this is my FSDP config:</p>
<pre><code>compute_environment: LOCAL_MACHINE
debug: false
distributed_type: FSDP
downcast_bf16: 'no'
fsdp_config:
  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP
  fsdp_backward_prefetch: BACKWARD_PRE
  fsdp_cpu_ram_efficient_loading: true
  fsdp_forward_prefetch: false
  fsdp_offload_params: false
  fsdp_sharding_strategy: FULL_SHARD
  fsdp_state_dict_type: SHARDED_STATE_DICT
  fsdp_sync_module_states: true
  fsdp_use_orig_params: false
machine_rank: 0
main_training_function: main
mixed_precision: bf16
num_machines: 1
num_processes: 4
rdzv_backend: static
same_network: true
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: false
</code></pre>
<p>I'm starting the script with:</p>
<pre><code>accelerate launch --use_fsdp --config_file=fsdp_config.yaml train.py
</code></pre>
<p>Can you help me out some more?</p>
","huggingface"
"78083730","How to use VitsModel with speaker embedding","2024-02-29 19:04:25","","0","78","<text-to-speech><huggingface><speaker-diarization>","<p>I want to do TTS for German, and this code works perfectly:</p>
<pre><code>from transformers import VitsModel, AutoTokenizer
import torch

model = VitsModel.from_pretrained(&quot;facebook/mms-tts-deu&quot;)
tokenizer = AutoTokenizer.from_pretrained(&quot;facebook/mms-tts-deu&quot;)

text = &quot;some example text in the German, Standard language&quot;
inputs = tokenizer(text, return_tensors=&quot;pt&quot;)

with torch.no_grad():
    output = model(**inputs).waveform
</code></pre>
<p>But I want it to be with my voice. Is there any way to use speaker_embedding with VitsModel?</p>
","huggingface"
"78082912","KeyError in Django when using Huggingface API","2024-02-29 16:25:28","","0","32","<python><django><django-views><keyerror><huggingface>","<p>I use <strong>Django</strong> and I want to use <strong>Huggingface</strong> API with it. The API <strong>sometimes</strong> return error to me saying:</p>
<pre><code>#  KeyError at /GenerativeImage2Text

0  
                34. if &quot;generated_text&quot; in output[0]:
                                            ^^^^^^^^
</code></pre>
<p>This is my view.py</p>
<pre><code>def GIT(request):
    output = None
    generated_text = None
    form = ImageForm()  

    if request.method == 'POST':
        form = ImageForm(request.POST, request.FILES)

        if form.is_valid():
            form.save()

            image_name = str(form.cleaned_data['Image'])
            imgloc = os.path.join(
                'media', 'images', image_name)
            while True:
                output = query(imgloc)
                if &quot;generated_text&quot; in output[0]:
                    generated_text = output[0].get('generated_text')
                    break
                else:
                    print(output[0])

        else:
            print(&quot;This image is invalid&quot;)

    return render(request, &quot;imagecaptioning.html&quot;, {&quot;form&quot;: form, &quot;output&quot;: generated_text})
</code></pre>
<ul>
<li><p>I want to <strong>fix</strong> this problem and make sure that API <strong>always</strong> return no error such that.</p>
</li>
<li><p>is the <strong>problem</strong> in my code or with the API?</p>
</li>
</ul>
","huggingface"
"78078166","Huggingface mirror in case Huggingface is in Maintenance","2024-02-28 23:36:12","","2","931","<huggingface>","<p>Is there a mirror of Huggingface to download models while it is in maintenance, or download all the models while it is not maintenance is the only way?</p>
<p><a href=""https://i.sstatic.net/uXKW4.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/uXKW4.png"" alt=""enter image description here"" /></a></p>
","huggingface"
"78070282","The download button for the image output in Gradio malfunctions when embedded","2024-02-27 20:00:52","","0","419","<javascript><html><embedding><huggingface><gradio>","<p>The output window of Gradio has a small down arrow icon to download the output image. When I run my Gradio app directly on Huggingface, this button works correctly by downloading the image directly through the browser. However, when I embed the application within an HTML, that button no longer works the same way. When you click on it, it opens the image in the browser. This is not the expected behavior, since loading the image directly in the browser resets the entire app window when you return, and it should not behave this way. Any workaround to avoid or fix this behavior?</p>
<p><a href=""https://i.sstatic.net/bFtGz.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bFtGz.jpg"" alt=""enter image description here"" /></a></p>
","huggingface"
"78065359","MockLLM for VectorStoreIndex from Huggingface","2024-02-27 05:40:15","","0","132","<huggingface><chromadb>","<p>I am currently building a small RAG application with a ChromaDB with Huggingface stuff.</p>
<p>It works perfect, but I have a question.</p>
<p>Currently I use this for</p>
<pre><code>Settings.llm = MockLLM(max_tokens=256)
Settings.embed_model = HuggingFaceEmbedding(model_name=&quot;BAAI/bge-large-en-v1.5&quot;)
...
index = VectorStoreIndex(nodes)
</code></pre>
<p>for my VectorIndex (<code>Settings</code> was formerly <code>ServiceContext</code> BTW).</p>
<p>The <code>MockLLM</code> was from testing, so I wanted to use a production ready llm.</p>
<p>I ran through many examples, tried stuff, but: As it seems, the <code>Settings.llm</code> param (former <code>ServiceContext.from_defaults(llm=llm, ...</code>) does not make any difference in output / tokenizing / the final vector database.</p>
<p>It's needed to create the <code>VectorStoreIndex</code> (when omitting, the Huggingface stuff does use <code>MockLLM</code> itself but emits a warning).</p>
<p>Does anybody know if the <code>llm</code> makes any difference for a vectordatabase like ChromaDB?</p>
","huggingface"
"78062743","Latent argument in Stable Diffusion Pipeline (Huggingface Diffusers Library) working unexpectedly","2024-02-26 17:15:23","","0","261","<huggingface><stable-diffusion><diffusers>","<p>I'm using the <a href=""https://huggingface.co/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline"" rel=""nofollow noreferrer"">Stable Diffussion Pipeline</a> from Huggingface, and been trying to start the diffusion process with a custom latent using the <a href=""https://huggingface.co/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline.__call__.latents"" rel=""nofollow noreferrer"">latents param</a>, nevertheless, the result is unexpected.</p>
<p>I took the output (PIL image) of a Stable Diffussion Pipeline and used pil_to_latents() function (shown at the end) to get the latent representation, to later call the second Stable Diffussion Pipeline with the latents param as follows:</p>
<pre><code>pipe(
      latents = pil_to_latents(result_image_from_first_pipe)[0],
      *** other pipe arguments
)
</code></pre>
<p>But at the end, I'm getting a weird and blurry output. If I run the pipeline without this argument, the results seems normal.Does anybody has an idea why this happens? Thanks !</p>
<p>Supporting code:</p>
<pre><code>    def pil_to_latents(self, image):
        '''     
        Function to convert image to latents     
        '''     
        init_image = tfms.ToTensor()(image).unsqueeze(0) * 2.0 - 1.0   
        init_image = init_image.to(device=&quot;cuda&quot;, dtype=torch.float16)
        init_latent_dist = self.vae.encode(init_image).latent_dist.sample() * 0.18215     
        return init_latent_dist  
    
    def latents_to_pil(self, latents):     
        '''     
        Function to convert latents to images     
        '''     
        latents = (1 / 0.18215) * latents     
        with torch.no_grad():         
            image = self.vae.decode(latents).sample     
        
        image = (image / 2 + 0.5).clamp(0, 1)     
        image = image.detach().cpu().permute(0, 2, 3, 1).numpy()      
        images = (image * 255).round().astype(&quot;uint8&quot;)     
        pil_images = [Image.fromarray(image) for image in images]        
        return pil_images
</code></pre>
","huggingface"
"78056541","How do I resolve this LoRA loading error?","2024-02-25 15:26:56","","4","5557","<python><huggingface-transformers><huggingface>","<p>I'm trying to run through <a href=""https://huggingface.co/docs/diffusers/training/lora#text-to-image"" rel=""nofollow noreferrer"">the 🤗 LoRA tutorial</a>. I've gotten the dataset pulled down, trained it and have checkpoints on disk (in the form of several subdirectories and <code>.safetensors</code> files).</p>
<p>The last part is trying to run inference. In particular,</p>
<pre><code>from diffusers import AutoPipelineForText2Image
import torch

pipeline = AutoPipelineForText2Image.from_pretrained(&quot;runwayml/stable-diffusion-v1-5&quot;, torch_dtype=torch.float16).to(&quot;cuda&quot;)
pipeline.load_lora_weights(&quot;path/to/lora/model&quot;, weight_name=&quot;pytorch_lora_weights.safetensors&quot;)
</code></pre>
<p>However, on my local when I try to run that <code>load_lora_weights</code> line, I get</p>
<pre><code>&gt;&gt;&gt; pipeline.load_lora_weights(&quot;path/to/my/lora&quot;, weight_name=&quot;pytorch_lora_weights.safetensors&quot;)
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;/path/to/my/site-packages/diffusers/loaders/lora.py&quot;, line 107, in load_lora_weights
    raise ValueError(&quot;PEFT backend is required for this method.&quot;)
ValueError: PEFT backend is required for this method.
&gt;&gt;&gt; 
</code></pre>
<p>I have PEFT installed, but there don't seem to be instructions calling for me to do anything else about it in order to load a LoRA.</p>
<p>What am I doing wrong here? If the answer is &quot;nothing, this is the 'it's an experimental API' note coming back to bite you&quot;, are there any workarounds?</p>
","huggingface"
"78051401","Phi-2 tokenizer.batch_decode() giving error: expected string got NoneType","2024-02-24 06:19:23","","0","78","<pytorch><huggingface><huggingface-tokenizers>","<p><strong>Issue 1</strong>: <code>tokenizer.vocab_size</code> has a size of <code>50257</code> when printed but the <code>Phi-2</code> model gives me an output shape <code>(5, 1256, 51200)</code> during evaluation and the error below while decoding</p>
<p>I'm working on callback like:</p>
<pre><code>
def decode_predictions(tokenizer, predictions):
    print(type(predictions), predictions.predictions.shape, predictions.label_ids.shape) # (5, 1256, 51200)
    labels = tokenizer.batch_decode(predictions.label_ids) 
    prediction_text = tokenizer.batch_decode(predictions.predictions.argmax(axis=-1)) # HERE COMES THE ERROR
    return {&quot;labels&quot;: labels, &quot;predictions&quot;: prediction_text}

 def on_evaluate(self, args, state, control,  **kwargs):
        super().on_evaluate(args, state, control, **kwargs)

        predictions = self.trainer.predict(self.sample_dataset)# generate predictions
        predictions = decode_predictions(self.tokenizer, predictions) # decode predictions and labels
        predictions_df = pd.DataFrame(predictions) # add predictions to a wandb.Table
        predictions_df[&quot;epoch&quot;] = state.epoch
        records_table = self._wandb.Table(dataframe=predictions_df)
        self._wandb.log({&quot;sample_predictions&quot;: records_table}) # log the table to wandb
</code></pre>
<p><strong>Issue-2</strong>: When I create a random example, the <code>tokenizer</code> works till <code>50300</code></p>
<p>and after that, I'm getting:</p>
<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[19], line 1
----&gt; 1 tokenizer.batch_decode(predictions.argmax(axis=-1), skip_special_tokens = True)

File ~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3742, in PreTrainedTokenizerBase.batch_decode(self, sequences, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)
   3718 def batch_decode(
   3719     self,
   3720     sequences: Union[List[int], List[List[int]], &quot;np.ndarray&quot;, &quot;torch.Tensor&quot;, &quot;tf.Tensor&quot;],
   (...)
   3723     **kwargs,
   3724 ) -&gt; List[str]:
   3725     &quot;&quot;&quot;
   3726     Convert a list of lists of token ids into a list of strings by calling decode.
   3727 
   (...)
   3740         `List[str]`: The list of decoded sentences.
   3741     &quot;&quot;&quot;
-&gt; 3742     return [
   3743         self.decode(
   3744             seq,
   3745             skip_special_tokens=skip_special_tokens,
   3746             clean_up_tokenization_spaces=clean_up_tokenization_spaces,
   3747             **kwargs,
   3748         )
   3749         for seq in sequences
   3750     ]

File ~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3743, in &lt;listcomp&gt;(.0)
   3718 def batch_decode(
   3719     self,
   3720     sequences: Union[List[int], List[List[int]], &quot;np.ndarray&quot;, &quot;torch.Tensor&quot;, &quot;tf.Tensor&quot;],
   (...)
   3723     **kwargs,
   3724 ) -&gt; List[str]:
   3725     &quot;&quot;&quot;
   3726     Convert a list of lists of token ids into a list of strings by calling decode.
   3727 
   (...)
   3740         `List[str]`: The list of decoded sentences.
   3741     &quot;&quot;&quot;
   3742     return [
-&gt; 3743         self.decode(
   3744             seq,
   3745             skip_special_tokens=skip_special_tokens,
   3746             clean_up_tokenization_spaces=clean_up_tokenization_spaces,
   3747             **kwargs,
   3748         )
   3749         for seq in sequences
   3750     ]

File ~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/models/codegen/tokenization_codegen.py:358, in CodeGenTokenizer.decode(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, truncate_before_pattern, **kwargs)
    331 &quot;&quot;&quot;
    332 Converts a sequence of ids in a string, using the tokenizer and vocabulary with options to remove special
    333 tokens and clean up tokenization spaces.
   (...)
    353     `str`: The decoded sentence.
    354 &quot;&quot;&quot;
    356 token_ids = to_py_obj(token_ids)
--&gt; 358 decoded_text = super()._decode(
    359     token_ids=token_ids,
    360     skip_special_tokens=skip_special_tokens,
    361     clean_up_tokenization_spaces=clean_up_tokenization_spaces,
    362     **kwargs,
    363 )
    365 if truncate_before_pattern is not None and len(truncate_before_pattern) &gt; 0:
    366     decoded_text = self.truncate(decoded_text, truncate_before_pattern)

File ~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/tokenization_utils.py:1024, in PreTrainedTokenizer._decode(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, spaces_between_special_tokens, **kwargs)
   1022         current_sub_text.append(token)
   1023 if current_sub_text:
-&gt; 1024     sub_texts.append(self.convert_tokens_to_string(current_sub_text))
   1026 if spaces_between_special_tokens:
   1027     text = &quot; &quot;.join(sub_texts)

File ~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/models/codegen/tokenization_codegen.py:284, in CodeGenTokenizer.convert_tokens_to_string(self, tokens)
    282 def convert_tokens_to_string(self, tokens):
    283     &quot;&quot;&quot;Converts a sequence of tokens (string) in a single string.&quot;&quot;&quot;
--&gt; 284     text = &quot;&quot;.join(tokens)
    285     text = bytearray([self.byte_decoder[c] for c in text]).decode(&quot;utf-8&quot;, errors=self.errors)
    286     return text

TypeError: sequence item 31: expected str instance, NoneType found
</code></pre>
<p>This is the code I'm using:</p>
<pre><code>import numpy as np
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(&quot;microsoft/phi-2&quot;, use_fast = False)

print(tokenizer.vocab_size)

tokenizer.add_tokens([&quot;&lt;|im_start|&gt;&quot;, &quot;&lt;PAD&gt;&quot;])
tokenizer.pad_token = &quot;&lt;PAD&gt;&quot;
tokenizer.add_special_tokens(dict(eos_token=&quot;&lt;|im_end|&gt;&quot;))

print(tokenizer.vocab_size)

predictions = np.random.uniform(size = (5, 1256, 50300)) # [No of samples, sequence_length, Vocab]
preds = predictions.argmax(axis=-1)

tokenizer.batch_decode(preds) # Works till 50300
</code></pre>
","huggingface"
"78051180","Hugging face model.pretrained not able to find model weights","2024-02-24 04:05:35","","0","96","<deep-learning><huggingface-transformers><large-language-model><huggingface><llama>","<p>model name: meta-llama/Llama-2-70b-chat-hf</p>
<p>I am new to this and trying to run a huggingface model using pretrained function. Model is loading cache of more than my disk space so I changed the default HF_HOME variable location due to the cache issue and now I am not able to run the model and getting this error.</p>
<pre><code>    raise ValueError(&quot;Need either a `state_dict` or a `save_folder` containing offloaded weights.&quot;)
ValueError: Need either a `state_dict` or a `save_folder` containing offloaded weights.```


I want to know if I am missing some configurations?
</code></pre>
","huggingface"
"78045456","Huggingface accelerate test error when num_processes>2","2024-02-23 06:01:59","","-1","304","<huggingface><accelerate>","<p>I am using a server with 8 GPUs. I tried to run <code>accelerate test</code> in my terminal but I get this error. Below is all the output.</p>
<pre><code>[2024-02-23 13:23:19,151] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)

Running:  accelerate-launch /home/Althea/miniconda3/envs/timellm/lib/python3.9/site-packages/accelerate/test_utils/scripts/test_script.py
stdout: [2024-02-23 13:23:23,448] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
stdout: [2024-02-23 13:23:27,230] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
stdout: [2024-02-23 13:23:27,307] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
stdout: [2024-02-23 13:23:27,317] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
stdout: [2024-02-23 13:23:27,388] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
stdout: Distributed environment: MULTI_GPU  Backend: nccl
stdout: Num processes: 4
stdout: Process index: 3
stdout: Local process index: 3
stdout: Device: cuda:3
stdout: 
stdout: Mixed precision type: fp16
stdout: 
stdout: Distributed environment: MULTI_GPU  Backend: nccl
stdout: Num processes: 4
stdout: Process index: 2
stdout: Local process index: 2
stdout: Device: cuda:2
stdout: 
stdout: Mixed precision type: fp16
stdout: 
stdout: **Initialization**
stdout: Testing, testing. 1, 2, 3.
stdout: Distributed environment: MULTI_GPU  Backend: nccl
stdout: Num processes: 4
stdout: Process index: 0
stdout: Local process index: 0
stdout: Device: cuda:0
stdout: 
stdout: Mixed precision type: fp16
stdout: 
stdout: 
stdout: **Test process execution**
stdout: Distributed environment: MULTI_GPU  Backend: nccl
stdout: Num processes: 4
stdout: Process index: 1
stdout: Local process index: 1
stdout: Device: cuda:1
stdout: 
stdout: Mixed precision type: fp16
stdout: 
stdout: 
stdout: **Test split between processes as a list**
stdout: 
stdout: **Test split between processes as a dict**
stderr: Traceback (most recent call last):
stderr:   File &quot;/home/Althea/miniconda3/envs/timellm/lib/python3.9/site-packages/accelerate/test_utils/scripts/test_script.py&quot;, line 553, in &lt;module&gt;
stderr:     main()
stderr:   File &quot;/home/Althea/miniconda3/envs/timellm/lib/python3.9/site-packages/accelerate/test_utils/scripts/test_script.py&quot;, line 523, in main
stderr:     test_split_between_processes_list()
stderr:   File &quot;/home/Althea/miniconda3/envs/timellm/lib/python3.9/site-packages/accelerate/test_utils/scripts/test_script.py&quot;, line 459, in test_split_between_processes_list
stderr:     with state.split_between_processes(data, apply_padding=True) as results:
stderr:   File &quot;/home/Althea/miniconda3/envs/timellm/lib/python3.9/contextlib.py&quot;, line 119, in __enter__
stderr:     return next(self.gen)
stderr:   File &quot;/home/Althea/miniconda3/envs/timellm/lib/python3.9/site-packages/accelerate/state.py&quot;, line 854, in split_between_processes
stderr:     with PartialState().split_between_processes(inputs, apply_padding=apply_padding) as inputs:
stderr:   File &quot;/home/Althea/miniconda3/envs/timellm/lib/python3.9/contextlib.py&quot;, line 119, in __enter__
stderr:     return next(self.gen)
stderr:   File &quot;/home/Althea/miniconda3/envs/timellm/lib/python3.9/site-packages/accelerate/state.py&quot;, line 418, in split_between_processes
stderr:     yield _split_values(inputs, start_index, end_index)
stderr:   File &quot;/home/Althea/miniconda3/envs/timellm/lib/python3.9/site-packages/accelerate/state.py&quot;, line 409, in _split_values
stderr:     result += [result[-1]] * (num_samples_per_process - len(result))
stderr: IndexError: list index out of range
stdout: 
stdout: **Test split between processes as a tensor**
stderr: Traceback (most recent call last):
stderr:   File &quot;/home/Althea/miniconda3/envs/timellm/lib/python3.9/site-packages/accelerate/test_utils/scripts/test_script.py&quot;, line 553, in &lt;module&gt;
stderr:     main()
stderr:   File &quot;/home/Althea/miniconda3/envs/timellm/lib/python3.9/site-packages/accelerate/test_utils/scripts/test_script.py&quot;, line 527, in main
stderr:     test_split_between_processes_nested_dict()
stderr:   File &quot;/home/Althea/miniconda3/envs/timellm/lib/python3.9/site-packages/accelerate/test_utils/scripts/test_script.py&quot;, line 479, in test_split_between_processes_nested_dict
stderr:     assert results[&quot;a&quot;] == data_copy[&quot;a&quot;][-1]
stderr: AssertionError
stdout: 
stdout: **Test random number generator synchronization**
stderr: Traceback (most recent call last):
stderr:   File &quot;/home/Althea/miniconda3/envs/timellm/lib/python3.9/site-packages/accelerate/test_utils/scripts/test_script.py&quot;, line 553, in &lt;module&gt;
stderr:     main()
stderr:   File &quot;/home/Althea/miniconda3/envs/timellm/lib/python3.9/site-packages/accelerate/test_utils/scripts/test_script.py&quot;, line 527, in main
stderr:     test_split_between_processes_nested_dict()
stderr:   File &quot;/home/Althea/miniconda3/envs/timellm/lib/python3.9/site-packages/accelerate/test_utils/scripts/test_script.py&quot;, line 479, in test_split_between_processes_nested_dict
stderr:     assert results[&quot;a&quot;] == data_copy[&quot;a&quot;][-1]
stderr: AssertionError
stderr: WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 514870 closing signal SIGTERM
stderr: ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 514871) of binary: /home/Althea/miniconda3/envs/timellm/bin/python
stderr: Traceback (most recent call last):
stderr:   File &quot;/home/Althea/miniconda3/envs/timellm/bin/accelerate-launch&quot;, line 8, in &lt;module&gt;
stderr:     sys.exit(main())
stderr:   File &quot;/home/Althea/miniconda3/envs/timellm/lib/python3.9/site-packages/accelerate/commands/launch.py&quot;, line 947, in main
stderr:     launch_command(args)
stderr:   File &quot;/home/Althea/miniconda3/envs/timellm/lib/python3.9/site-packages/accelerate/commands/launch.py&quot;, line 932, in launch_command
stderr:     multi_gpu_launcher(args)
stderr:   File &quot;/home/Althea/miniconda3/envs/timellm/lib/python3.9/site-packages/accelerate/commands/launch.py&quot;, line 627, in multi_gpu_launcher
stderr:     distrib_run.run(args)
stderr:   File &quot;/home/Althea/miniconda3/envs/timellm/lib/python3.9/site-packages/torch/distributed/run.py&quot;, line 785, in run
stderr:     elastic_launch(
stderr:   File &quot;/home/Althea/miniconda3/envs/timellm/lib/python3.9/site-packages/torch/distributed/launcher/api.py&quot;, line 134, in __call__
stderr:     return launch_agent(self._config, self._entrypoint, list(args))
stderr:   File &quot;/home/Althea/miniconda3/envs/timellm/lib/python3.9/site-packages/torch/distributed/launcher/api.py&quot;, line 250, in launch_agent
stderr:     raise ChildFailedError(
stderr: torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
stderr: ============================================================
stderr: /home/Althea/miniconda3/envs/timellm/lib/python3.9/site-packages/accelerate/test_utils/scripts/test_script.py FAILED
stderr: ------------------------------------------------------------
stderr: Failures:
stderr: [1]:
stderr:   time      : 2024-02-23_13:23:35
stderr:   host      : Server
stderr:   rank      : 2 (local_rank: 2)
stderr:   exitcode  : 1 (pid: 514872)
stderr:   error_file: &lt;N/A&gt;
stderr:   traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
stderr: [2]:
stderr:   time      : 2024-02-23_13:23:35
stderr:   host      : Server
stderr:   rank      : 3 (local_rank: 3)
stderr:   exitcode  : 1 (pid: 514873)
stderr:   error_file: &lt;N/A&gt;
stderr:   traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
stderr: ------------------------------------------------------------
stderr: Root Cause (first observed failure):
stderr: [0]:
stderr:   time      : 2024-02-23_13:23:35
stderr:   host      : Server
stderr:   rank      : 1 (local_rank: 1)
stderr:   exitcode  : 1 (pid: 514871)
stderr:   error_file: &lt;N/A&gt;
stderr:   traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
stderr: ============================================================
Traceback (most recent call last):
  File &quot;/home/Althea/miniconda3/envs/timellm/bin/accelerate&quot;, line 8, in &lt;module&gt;
    sys.exit(main())
  File &quot;/home/Althea/miniconda3/envs/timellm/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py&quot;, line 45, in main
    args.func(args)
  File &quot;/home/Althea/miniconda3/envs/timellm/lib/python3.9/site-packages/accelerate/commands/test.py&quot;, line 54, in test_command
    result = execute_subprocess_async(cmd, env=os.environ.copy())
  File &quot;/home/Althea/miniconda3/envs/timellm/lib/python3.9/site-packages/accelerate/test_utils/testing.py&quot;, line 383, in execute_subprocess_async
    raise RuntimeError(
RuntimeError: 'accelerate-launch /home/Althea/miniconda3/envs/timellm/lib/python3.9/site-packages/accelerate/test_utils/scripts/test_script.py' failed with returncode 1

The combined stderr from workers follows:
Traceback (most recent call last):
  File &quot;/home/Althea/miniconda3/envs/timellm/lib/python3.9/site-packages/accelerate/test_utils/scripts/test_script.py&quot;, line 553, in &lt;module&gt;
    main()
  File &quot;/home/Althea/miniconda3/envs/timellm/lib/python3.9/site-packages/accelerate/test_utils/scripts/test_script.py&quot;, line 523, in main
    test_split_between_processes_list()
  File &quot;/home/Althea/miniconda3/envs/timellm/lib/python3.9/site-packages/accelerate/test_utils/scripts/test_script.py&quot;, line 459, in test_split_between_processes_list
    with state.split_between_processes(data, apply_padding=True) as results:
  File &quot;/home/Althea/miniconda3/envs/timellm/lib/python3.9/contextlib.py&quot;, line 119, in __enter__
    return next(self.gen)
  File &quot;/home/Althea/miniconda3/envs/timellm/lib/python3.9/site-packages/accelerate/state.py&quot;, line 854, in split_between_processes
    with PartialState().split_between_processes(inputs, apply_padding=apply_padding) as inputs:
  File &quot;/home/Althea/miniconda3/envs/timellm/lib/python3.9/contextlib.py&quot;, line 119, in __enter__
    return next(self.gen)
  File &quot;/home/Althea/miniconda3/envs/timellm/lib/python3.9/site-packages/accelerate/state.py&quot;, line 418, in split_between_processes
    yield _split_values(inputs, start_index, end_index)
  File &quot;/home/Althea/miniconda3/envs/timellm/lib/python3.9/site-packages/accelerate/state.py&quot;, line 409, in _split_values
    result += [result[-1]] * (num_samples_per_process - len(result))
IndexError: list index out of range
Traceback (most recent call last):
  File &quot;/home/Althea/miniconda3/envs/timellm/lib/python3.9/site-packages/accelerate/test_utils/scripts/test_script.py&quot;, line 553, in &lt;module&gt;
    main()
  File &quot;/home/Althea/miniconda3/envs/timellm/lib/python3.9/site-packages/accelerate/test_utils/scripts/test_script.py&quot;, line 527, in main
    test_split_between_processes_nested_dict()
  File &quot;/home/Althea/miniconda3/envs/timellm/lib/python3.9/site-packages/accelerate/test_utils/scripts/test_script.py&quot;, line 479, in test_split_between_processes_nested_dict
    assert results[&quot;a&quot;] == data_copy[&quot;a&quot;][-1]
AssertionError
Traceback (most recent call last):
  File &quot;/home/Althea/miniconda3/envs/timellm/lib/python3.9/site-packages/accelerate/test_utils/scripts/test_script.py&quot;, line 553, in &lt;module&gt;
    main()
  File &quot;/home/Althea/miniconda3/envs/timellm/lib/python3.9/site-packages/accelerate/test_utils/scripts/test_script.py&quot;, line 527, in main
    test_split_between_processes_nested_dict()
  File &quot;/home/Althea/miniconda3/envs/timellm/lib/python3.9/site-packages/accelerate/test_utils/scripts/test_script.py&quot;, line 479, in test_split_between_processes_nested_dict
    assert results[&quot;a&quot;] == data_copy[&quot;a&quot;][-1]
AssertionError
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 514870 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 514871) of binary: /home/Althea/miniconda3/envs/timellm/bin/python
Traceback (most recent call last):
  File &quot;/home/Althea/miniconda3/envs/timellm/bin/accelerate-launch&quot;, line 8, in &lt;module&gt;
    sys.exit(main())
  File &quot;/home/Althea/miniconda3/envs/timellm/lib/python3.9/site-packages/accelerate/commands/launch.py&quot;, line 947, in main
    launch_command(args)
  File &quot;/home/Althea/miniconda3/envs/timellm/lib/python3.9/site-packages/accelerate/commands/launch.py&quot;, line 932, in launch_command
    multi_gpu_launcher(args)
  File &quot;/home/Althea/miniconda3/envs/timellm/lib/python3.9/site-packages/accelerate/commands/launch.py&quot;, line 627, in multi_gpu_launcher
    distrib_run.run(args)
  File &quot;/home/Althea/miniconda3/envs/timellm/lib/python3.9/site-packages/torch/distributed/run.py&quot;, line 785, in run
    elastic_launch(
  File &quot;/home/Althea/miniconda3/envs/timellm/lib/python3.9/site-packages/torch/distributed/launcher/api.py&quot;, line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File &quot;/home/Althea/miniconda3/envs/timellm/lib/python3.9/site-packages/torch/distributed/launcher/api.py&quot;, line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
/home/Althea/miniconda3/envs/timellm/lib/python3.9/site-packages/accelerate/test_utils/scripts/test_script.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-23_13:23:35
  host      : Server
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 514872)
  error_file: &lt;N/A&gt;
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-23_13:23:35
  host      : Server
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 514873)
  error_file: &lt;N/A&gt;
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-23_13:23:35
  host      : Server
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 514871)
  error_file: &lt;N/A&gt;
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
</code></pre>
<p>This is my configuration file (default_config.yaml):</p>
<pre><code>compute_environment: LOCAL_MACHINE
distributed_type: MULTI_GPU
downcast_bf16: 'no'
gpu_ids: 0,1,2,3,4,5,6,7
machine_rank: 0
main_training_function: main
main_process_port: 33827
mixed_precision: fp16
num_machines: 1
num_processes: 4
rdzv_backend: static
same_network: true
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: false

</code></pre>
<p>Package versions:
transformers              4.31.0
accelerate                0.20.3</p>
<p>I tried setting <code>num_processes</code> in default_config.yaml to 1 or 2 and it works fine, but if I set it to anything larger than 2 (not including 2), the above error occurs.</p>
","huggingface"
"78043866","HuggingFaceInstructEmbeddings The specified module could not be found","2024-02-22 20:31:50","","0","586","<huggingface-transformers><langchain><embedding><huggingface>","<p>i am trying to use HuggingFaceInstructEmbeddings by HuggingFace X langchain with this code:</p>
<pre><code>from langchain_community.llms import OpenAI 
from langchain_community.document_loaders import CSVLoader
from langchain_community.embeddings.huggingface import HuggingFaceInstructEmbeddings
from langchain_community.vectorstores import FAISS 
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA

llm = OpenAI(openai_api_key=&quot;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&quot;, temperature=0.5)


&gt; ins_embeddings = HuggingFaceInstructEmbeddings()
&gt; 
vectordb_file_path = &quot;faiss_index&quot;


def create_vector_db():
    loader = CSVLoader(file_path=&quot;Qiwa_CS.csv&quot;)
    data = loader.load()
    vectordb = FAISS.from_documents(documents=data, embedding=ins_embeddings)
    vectordb.save_local(vectordb_file_path)

def get_qa_chain():
    vectordb = FAISS.load_local(vectordb_file_path,ins_embeddings)
    my_retriver = vectordb.as_retriever()
    from langchain.prompts import PromptTemplate

    prompt_template = &quot;&quot;&quot;Your name is  &quot;Aref&quot;  in Arabic is &quot;عارف&quot; and you are working as AI assistance for the Ministry  Human Resource and Social Development in Saudi Arabia.
    Given the following context and a question, generate an answer based on this context only.
    Please reply with the same user's language without missing the context.
    In the answer try to provide as much text as possible from the response section in the source document context without making much changes.
    If the answer is not found in the context, kindly state :
    &quot;Sorry, this is not my specialty or &quot;Sorry I don't have an answer to this question&quot;
    Don't try to make up an answer.
    If the answer is related to the context of the documents in the sources and you do not know the answer, answer with
    &quot;I think your question requires visiting this link : https://www.qiwa.sa/ar&quot;
    CONTEXT:{context}
    QUESTION:{question}
    &quot;&quot;&quot;

    my_prompt = PromptTemplate(template=prompt_template, input_variables=[&quot;context&quot;,&quot;question&quot;])

    chain = RetrievalQA.from_chain_type(llm=llm,
                            chain_type=&quot;stuff&quot;,
                            retriever=my_retriver,
                            input_key=&quot;query&quot;,
                            return_source_documents=True,
                            chain_type_kwargs={&quot;prompt&quot;: my_prompt}
                            )
    
    return chain

if __name__ == &quot;__main__&quot;:
    chain = get_qa_chain()

print( chain (&quot; how much the visa will cost ?&quot;))
</code></pre>
<p>but this error shows up:</p>
<pre><code> File &quot;C:\Users\Nedal Ahmed\Desktop\QAQiwa\main.py&quot;, line 14, in &lt;module&gt;
    ins_embeddings = HuggingFaceInstructEmbeddings()
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [WinError 126] The specified module could not be found. Error loading &quot;c:\Users\Nedal Ahmed\Desktop\QAQiwa\.venv\Lib\site-packages\torch\lib\c10.dll&quot; or one of its dependencies.
</code></pre>
<p>i tried also:</p>
<pre><code>pip install sentence-transformers==2.2.2
pip install InstructorEmbedding
</code></pre>
<p>but no result</p>
","huggingface"
"78038767","Loading video-LLaVA with Huggingface transformers","2024-02-22 06:19:50","","1","146","<huggingface><multimodal>","<p>On trying to load video-LLaVA with Huggingface on colab I get this error:</p>
<pre><code>---------------------------------------------------------------------------
HTTPError                                 Traceback (most recent call last)
/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py in hf_raise_for_status(response, endpoint_name)
    285     try:
--&gt; 286         response.raise_for_status()
    287     except HTTPError as e:

15 frames
HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/LanguageBind/Video-LLaVA-7B/resolve/main/preprocessor_config.json

The above exception was the direct cause of the following exception:

EntryNotFoundError                        Traceback (most recent call last)
EntryNotFoundError: 404 Client Error. (Request ID: Root=1-65d6e313-109341163745f316481be0ac;97f03b13-3bac-447e-b9df-a109f451bc7c)

Entry Not Found for url: https://huggingface.co/LanguageBind/Video-LLaVA-7B/resolve/main/preprocessor_config.json.

The above exception was the direct cause of the following exception:

OSError                                   Traceback (most recent call last)
/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py in cached_file(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)
    434         if revision is None:
    435             revision = &quot;main&quot;
--&gt; 436         raise EnvironmentError(
    437             f&quot;{path_or_repo_id} does not appear to have a file named {full_filename}. Checkout &quot;
    438             f&quot;'https://huggingface.co/{path_or_repo_id}/{revision}' for available files.&quot;

OSError: LanguageBind/Video-LLaVA-7B does not appear to have a file named preprocessor_config.json. Checkout 'https://huggingface.co/LanguageBind/Video-LLaVA-7B/main' for available files.
the preprocessor_config.json is not present in the files ...how can I load the model?
</code></pre>
<p>This is the code I ran:</p>
<pre><code>from transformers import AutoProcessor, AutoModelForCausalLM

processor = AutoProcessor.from_pretrained(&quot;LanguageBind/Video-LLaVA-7B&quot;)
model = AutoModelForCausalLM.from_pretrained(&quot;LanguageBind/Video-LLaVA-7B&quot;)
</code></pre>
","huggingface"
"78037610","how to save adapter.bin model as .pt model","2024-02-21 23:17:04","","0","145","<huggingface><openai-whisper><peft>","<p>I have a hugging face <code>openai-whisper</code> model <code>adapter.bin</code> that has been finetuned using <code>LoRa</code>, following this <a href=""https://github.com/Vaibhavs10/fast-whisper-finetuning/blob/main/README.md"" rel=""nofollow noreferrer"">tutorial</a>. <code>Adapter.bin</code> has only the adapter weights. Now I want to save my original model with this adapter weight and get a <code>finetuned.pt</code> model that I can load locally. A sample from the code below:</p>
<pre><code>peft_model_id = &quot;reach-vb/test&quot; 
language = &quot;en&quot;
task = &quot;transcribe&quot;
peft_config = PeftConfig.from_pretrained(peft_model_id)
model = WhisperForConditionalGeneration.from_pretrained(
  peft_config.base_model_name_or_path, load_in_8bit=False, device_map=&quot;auto&quot;
)
</code></pre>
<p>How do I save this model as a <code>.pt</code> torch model that I can load using
<code>model = whisper.load_model('model.pt')</code></p>
","huggingface"
"78037027","Not able to create Generative AI model which will read the resume and check if it matches the job role","2024-02-21 20:55:41","","1","55","<huggingface><google-generativeai>","<p>I am new to Generative AI and I am trying to create a model which will read the resume and check if it matches the job role. I have implemented the code but when I run it, the result which I am getting is not acurate. I am using models from HugginFace, this is my code</p>
<pre><code>import pandas as pd
import numpy as np

from langchain import HuggingFaceHub
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain, SequentialChain
from langchain_community.document_loaders import UnstructuredWordDocumentLoader

doc= UnstructuredWordDocumentLoader(r'C:\Users\Lijin Durairaj\Desktop\resume.docx')
_data=doc.load()
_resume=_data[0].page_content


model_name='google/flan-t5-base'
huggin_face_api_token='**************************************'

client=HuggingFaceHub(
    huggingfacehub_api_token=huggin_face_api_token,
    repo_id=model_name,
    model_kwargs={
        'temperature':0.7        
    }
)


template=PromptTemplate(
input_variables=['job_position','resume'],
template='evaluate the resume: {resume} and tell me if the candidate is suitable for 
the position of {job_position}'
)

chain=LLMChain(llm=client,prompt=template)
chain.run({
    'resume':_resume
})
</code></pre>
<p>Am I missing something,</p>
<ol>
<li>Is my prompting template wrong? if no, then how to improve it</li>
<li>Am i using the right model? Do i have to use some other model</li>
<li>do i have to change the approach</li>
<li>should i have to do any data cleaning and feature extraction?</li>
</ol>
<p>please help me to reach to the solution</p>
","huggingface"
"78035697","Sentence Similarity for Text Classification","2024-02-21 16:46:46","","0","26","<amazon-sagemaker><embedding><huggingface>","<p>I'm trying to use sagemaker with sentence similarity for text classification. There are three different classes: Reportable, Not Reportable, and Review.</p>
<p>Looking at the similarity scores, the values almost have no difference. Like this example:</p>
<pre><code>[{'id': 'G CELL TUMOR has been determined to be Suspected', 'text': 'REPORTABLE', 'score': 0.8180783987045288}, {'id': 'G CELL TUMOR has been determined to be Typical', 'text': 'REPORTABLE', 'score': 0.8180783987045288}, {'id': 'Favors is the diagnosis of FULMINANT', 'text': 'REPORTABLE', 'score': 0.8180783987045288}, {'id': 'Favored is the diagnosis of FULMINANT', 'text': 'REPORTABLE', 'score': 0.8180783987045288}, {'id': 'Typical is the diagnosis of G CELL TUMOR', 'text': 'REPORTABLE', 'score': 0.8180783987045288}, {'id': 'Tumor is the diagnosis of G CELL TUMOR', 'text': 'REPORTABLE', 'score': 0.8180783987045288}, {'id': 'FULMINANT has been determined to be Malignant appearing', 'text': 'REPORTABLE', 'score': 0.8180783987045288}, {'id': 'FRANKLINS has been determined to be Suspicious', 'text': 'REPORTABLE', 'score': 0.8180783987045288}, {'id': 'Suspect is the diagnosis of FRANKLINS', 'text': 'REPORTABLE', 'score': 0.8180783987045288}, {'id': 'FULMINANT has been determined to be evolving', 'text': 'REPORTABLE', 'score': 0.8180783987045288}]
</code></pre>
<p>How can I increase the difference between the scores to get more variability with them?</p>
","huggingface"
"78033871","While using Seq2SeqTrainingArguments function, This error is displayed: Using the `Trainer` with `PyTorch` requires `accelerate>=0.21.0`","2024-02-21 12:07:54","","0","846","<python><pytorch><large-language-model><huggingface><huggingface-trainer>","<p>I am trying to run the <a href=""https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization.ipynb#scrollTo=IreSlFmlIrIm"" rel=""nofollow noreferrer"">Google Colab notebook</a>. Every step is well explained and easy to understand. But I have encountered a problem. while trying to run a specific part of code :</p>
<pre><code>batch_size = 16
model_name = model_checkpoint.split(&quot;/&quot;)[-1]

args = Seq2SeqTrainingArguments(
    output_dir=&quot;output_model_T5-small&quot;,
    evaluation_strategy=&quot;epoch&quot;,
    learning_rate=2e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=1,
    predict_with_generate=True,
    fp16=True,
    push_to_hub=True,
)
</code></pre>
<p>I am getting the following error:</p>
<pre><code>---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
&lt;ipython-input-36-791f6c1591ac&gt; in &lt;cell line: 10&gt;()
      8 model_name = model_checkpoint.split(&quot;/&quot;)[-1]
      9 
---&gt; 10 args = Seq2SeqTrainingArguments(
     11     output_dir=&quot;output_model_T5-small&quot;,
     12     evaluation_strategy=&quot;epoch&quot;,

4 frames
/usr/local/lib/python3.10/dist-packages/transformers/training_args.py in _setup_devices(self)
   1829         if not is_sagemaker_mp_enabled():
   1830             if not is_accelerate_available():
-&gt; 1831                 raise ImportError(
   1832                     f&quot;Using the `Trainer` with `PyTorch` requires `accelerate&gt;={ACCELERATE_MIN_VERSION}`: &quot;
   1833                     &quot;Please run `pip install transformers[torch]` or `pip install accelerate -U`&quot;

ImportError: Using the `Trainer` with `PyTorch` requires `accelerate&gt;=0.21.0`: Please run `pip install transformers[torch]` or `pip install accelerate -U`
</code></pre>
<p>I have tried both the given solutions, &quot;pip install transformers[torch]&quot; and &quot;pip install accelerate -U&quot; but still I got the same error.</p>
<p>Even if I find the version of my accelerate</p>
<pre><code>import accelerate
print(accelerate.__version__)
</code></pre>
<p>the output is <strong>0.27.2</strong></p>
","huggingface"
"78029218","Why is git only downloading part of the repository from huggingface?","2024-02-20 17:16:45","","0","436","<git><git-lfs><huggingface>","<p>I'm trying to clone some repositories from Huggingface via git - I've installed git lfs and set up my SSH key on huggingface, I can launch the clone command on the repository I want (e.g. <a href=""https://huggingface.co/google/flan-t5-xl"" rel=""nofollow noreferrer"">https://huggingface.co/google/flan-t5-xl</a>) like so:</p>
<pre><code>git clone git@hf.co:google/flan-t5-xl
</code></pre>
<p>This does appear to work initially:</p>
<pre><code>Cloning into 'flan-t5-xl'...
remote: Enumerating objects: 68, done.
remote: Total 68 (delta 0), reused 0 (delta 0), pack-reused 68
Receiving objects: 100% (68/68), 625.30 KiB | 6.13 MiB/s, done.
Resolving deltas: 100% (29/29), done.
Killeding content:  66% (6/9), 6.29 GiB | 3.55 MiB/s 
</code></pre>
<p>However what I find downloaded in my local system is much smaller than the repository I'm pulling from, and there are multiple files missing - still using flan as my example:
<a href=""https://i.sstatic.net/LUjR7.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/LUjR7.png"" alt=""enter image description here"" /></a></p>
<p>(compare with the original repository:)</p>
<p><a href=""https://i.sstatic.net/PgDbb.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/PgDbb.png"" alt=""enter image description here"" /></a></p>
<p>Does anyone know what I'm doing wrong? I've tested with multiple repositories and all behave in the same way, so I'm certain that it's something I'm doing rather than an huggingface problem</p>
","huggingface"
"78027389","Display Gradio components based on HF OAuth status","2024-02-20 12:38:06","","0","62","<python><huggingface><gradio>","<p>I want to show a gr.Markdown &quot;No user&quot; if no user is logged in via OAuth, and a component (in this example gr.Chatbot) if a user is logged in.</p>
<p>Full code:</p>
<pre><code>import gradio as gr

def get_oauth(profile: gr.OAuthProfile | None, oauth_token: gr.OAuthToken | None) -&gt; str:
    if profile is None:
        return
    return profile.name
    
with gr.Blocks() as demo: 
    gr.LoginButton()
    name = gr.State()
    demo.load(get_oauth, inputs = None, outputs = name)

    if name.value is None:
        gr.Markdown(
        &quot;&quot;&quot;
        # No user
        &quot;&quot;&quot;) 
    else:
        gr.Chatbot(value=[[&quot;Hello World&quot;,&quot;Hey Gradio!&quot;],[&quot;❤️&quot;,&quot;😍&quot;],[&quot;🔥&quot;,&quot;🤗&quot;]])
        
demo.launch()
</code></pre>
<p>The If/Else statement always returns the markdown, meaning the name.value is always None.</p>
<p>The OAuth IS working and returning gr.OAuthProfile, I have already double checked that.</p>
<p>My assumption is that the demo.load only runs AFTER the gr.Blocks has already populated its content, but im not sure how to change this.</p>
<p>I have alternatively tried:</p>
<pre><code>def get_oauth(profile: gr.OAuthProfile | None, oauth_token: gr.OAuthToken | None) -&gt; gr.State():
*scroll to see whole line*
</code></pre>
<p>and</p>
<pre><code>demo.load(get_oauth, inputs = None, outputs = name.value)
</code></pre>
<p><a href=""https://www.gradio.app/guides/state-in-blocks#session-state"" rel=""nofollow noreferrer"">https://www.gradio.app/guides/state-in-blocks#session-state </a> This is the Gradio gr.State() docs for reference</p>
<p>Meta:</p>
<pre><code>title: Gradio + Oauth
emoji: 🛡️
colorFrom: indigo
colorTo: purple
sdk: gradio
sdk_version: 4.16.0
hf_oauth: true
hf_oauth_scopes:
  - read-repos
</code></pre>
","huggingface"
"78026673","ImportError:Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`:Please run `pip install transformers[torch]`or`pip install accelerate -U`","2024-02-20 10:44:49","","0","135","<google-colaboratory><importerror><huggingface>","<p>I'm using the transformers library in Google colab, and When i am using TrainingArguments from transformers library i'm getting Import error with this code:</p>
<pre><code>from transformers import AutoModelForImageClassification, TrainingArguments, Trainer

model = AutoModelForImageClassification.from_pretrained(
    checkpoint,
    num_labels=len(labels),
    id2label=id2label,
    label2id=label2id,
)
training_args = TrainingArguments(
    output_dir=&quot;my_awesome_food_model&quot;,
    remove_unused_columns=False,
    evaluation_strategy=&quot;epoch&quot;,
    save_strategy=&quot;epoch&quot;,
    learning_rate=5e-5,
    per_device_train_batch_size=16,
    gradient_accumulation_steps=4,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    warmup_ratio=0.1,
    logging_steps=10,
    load_best_model_at_end=True,
    metric_for_best_model=&quot;accuracy&quot;,
    push_to_hub=True,
)

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=food[&quot;train&quot;],
    eval_dataset=food[&quot;test&quot;],
    tokenizer=image_processor,
    compute_metrics=compute_metrics,
)

trainer.train()
</code></pre>
<p>This is the error I'm Getting</p>
<pre><code>---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
&lt;ipython-input-26-cfdcb4612c2d&gt; in &lt;cell line: 1&gt;()
----&gt; 1 training_args = TrainingArguments(
      2     output_dir=&quot;my_awesome_food_model&quot;,
      3     remove_unused_columns=False,
      4     evaluation_strategy=&quot;epoch&quot;,
      5     save_strategy=&quot;epoch&quot;,

4 frames
/usr/local/lib/python3.10/dist-packages/transformers/training_args.py in _setup_devices(self)
   1785         if not is_sagemaker_mp_enabled():
   1786             if not is_accelerate_available(min_version=&quot;0.20.1&quot;):
-&gt; 1787                 raise ImportError(
   1788                     &quot;Using the `Trainer` with `PyTorch` requires `accelerate&gt;=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`&quot;
   1789                 )

ImportError: Using the `Trainer` with `PyTorch` requires `accelerate&gt;=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`
</code></pre>
<p>I already Tried Installing accelerate again and again but same error everytime.
Also used Virtual Evironment still no difference.
And yes both the versions of Accelerte and Transformers are upto date.</p>
<pre><code>transformers.__version__, accelerate.__version__
</code></pre>
<p>These are the versions -&gt; ('4.35.2', '0.27.2')
I'm using Colab environment</p>
","huggingface"
"78026404","list of supported backbones models","2024-02-20 10:07:59","","0","58","<python><machine-learning><pytorch><huggingface-transformers><huggingface>","<p>I'm running the following example:</p>
<pre><code>from transformers import TimmBackboneConfig, TimmBackbone

# Initializing a timm backbone
configuration = TimmBackboneConfig(&quot;resnet50&quot;)

# Initializing a model from the configuration
model = TimmBackbone(configuration)

# Accessing the model configuration
configuration = model.config
</code></pre>
<p>from <a href=""https://huggingface.co/docs/transformers/main/en/main_classes/backbones"" rel=""nofollow noreferrer"">here</a></p>
<p>Now I want to use other backbone model from the supported list in the same page,</p>
<ul>
<li>BEiT</li>
<li>BiT</li>
<li>ConvNet</li>
<li>ConvNextV2</li>
<li>DiNAT</li>
<li>DINOV2</li>
<li>FocalNet</li>
<li>MaskFormer</li>
<li>NAT</li>
<li>ResNet</li>
<li>Swin Transformer</li>
<li>Swin Transformer v2</li>
<li>ViTDet</li>
</ul>
<p>where can I find the dictionary from this list, I mean what is the string that I should use for each of the model?</p>
<p>For example:</p>
<ul>
<li>&quot;ResNet&quot; -&gt; 'resnet50'</li>
<li>&quot;BEiT&quot; -&gt; ???</li>
</ul>
<p>I have tried to search for it in the some of the links in the page and in google but didn't find this information anywhere.</p>
","huggingface"
"78020235","understanding looping real inference call","2024-02-19 11:15:23","78022307","0","52","<pytorch><gpu><nvidia><huggingface>","<p>I was looking for a solution for an issue I was having with my interface speed. I saw this <a href=""https://forums.developer.nvidia.com/t/inference-time-hugging-face-detr/274721"" rel=""nofollow noreferrer"">answer</a> online but I don't understand what the solution was. The person is using a hugging face model with pytorch and the solution was to loop the real inference call to increase the GPU utilization. This was caused by a bottleneck from accessing the data. Can I get some help understanding how to implement this solution?
Thanks</p>
","huggingface"
"78016717","How can I speed up the loading and inference of an LLM?","2024-02-18 17:27:33","","0","337","<python><huggingface-transformers><huggingface>","<p>I am currently using huggingfaces transformers library for a school project. The user prompts for an activity inside or outside, and I add their interests which I have stored in my database in order to get a tailored response. The problem is the prompting takes ~60 seconds to generate.</p>
<p>I am using a 4bit quantization of the llama2-7b model. I have a GTX 1080 8Gb vram with cuda 11.8.</p>
<pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

def run_chatbot(prompt):
    model_name_or_path = &quot;TheBloke/Llama-2-7b-Chat-GPTQ&quot;

    model = AutoModelForCausalLM.from_pretrained(model_name_or_path,
                                                device_map=&quot;auto&quot;,
                                                trust_remote_code=False,
                                                revision=&quot;gptq-4bit-64g-actorder_True&quot;)

    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)

    curr_prompt = prompt
    prompt_template=f'''[INST] &lt;&lt;SYS&gt;&gt;
    You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. 
    &lt;&lt;/SYS&gt;&gt;
    {curr_prompt}[/INST]

    '''

    input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()
    output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=256)
    return tokenizer.decode(output[0])
</code></pre>
","huggingface"
"78013968","Hugging Face Chat UI .ENV.CI Parameters","2024-02-17 22:42:03","","0","173","<mongodb><artificial-intelligence><huggingface>","<p>I am trying to follow the instructions for hugging face's chat ui: <a href=""https://github.com/huggingface/chat-ui"" rel=""nofollow noreferrer"">https://github.com/huggingface/chat-ui</a></p>
<p>I've pulled the repo and have a MongoDB account.</p>
<p>In the .env.ci file I set it to the following</p>
<pre><code>MONGODB_URL=mongodb+srv://myUsername_myPassword@aicluster.boctvbq.mongodb.net/
HF_TOKEN=myHuggingFaceToken
</code></pre>
<p>I copied my MongoDB URL from MongoDB Compass. When I run npm install
npm run dev from the terminal I get this</p>
<pre><code>3:36:26 PM [vite] Error when evaluating SSR module /src/lib/server/database.ts:
|- Error: Please specify the MONGODB_URL environment variable inside .env.local. Set it to mongodb://localhost:27017 if you are running MongoDB locally, or to a MongoDB Atlas free instance for example.
    at /Users/username/AI/chat-ui/src/lib/server/database.ts:14:8
    at async instantiateModule (file:///Users/username/AI/chat-ui/node_modules/vite/dist/node/chunks/dep-52909643.js:56052:9)

3:36:26 PM [vite] Error when evaluating SSR module /src/hooks.server.ts: failed to import &quot;/src/lib/server/database.ts&quot;
|- Error: Please specify the MONGODB_URL environment variable inside .env.local. Set it to mongodb://localhost:27017 if you are running MongoDB locally, or to a MongoDB Atlas free instance for example.
    at /Users/username/AI/chat-ui/src/lib/server/database.ts:14:8
    at async instantiateModule (file:///Users/username/AI/chat-ui/node_modules/vite/dist/node/chunks/dep-52909643.js:56052:9)
</code></pre>
<p>Is there something I'm doing wrong with MongoDb?</p>
<p>My Chat UI app is not able to run locally.</p>
","huggingface"
"78011667","Bus error on mixtral-instructv01-awq with Vllm","2024-02-17 09:58:27","","1","379","<huggingface-transformers><large-language-model><huggingface><ray><nvidia-docker>","<p>I am getting a bus error when trying to initialize &quot;TheBloke/Mixtral-8x7B-Instruct-v0.1-AWQ&quot; model from Huggingface,</p>
<pre><code>        self.model = LLM(
        model=&quot;TheBloke/Mixtral-8x7B-Instruct-v0.1-AWQ&quot;,
        quantization=&quot;awq&quot;,
        dtype=&quot;auto&quot;,
        tensor_parallel_size=tensor_parallel_size,
    )
</code></pre>
<p>The error i get is:</p>
<blockquote>
<p>ERROR 2024-02-16T21:46:33.751635551Z *** SIGBUS received at time=1708119993 on cpu 67 ***
ERROR 2024-02-16T21:46:33.754929304Z PC: @ 0x7e9291287a37 (unknown) ncclShmOpen()
ERROR 2024-02-16T21:46:33.755156517Z @ 0x7e9456f02520 3456 (unknown)
ERROR 2024-02-16T21:46:33.756768941Z @ 0x74352d6c63636e2f (unknown) (unknown)
ERROR 2024-02-16T21:46:33.756790876Z [2024-02-16 21:46:33,756 E 1 6357] logging.cc:361: *** SIGBUS received at time=1708119993 on cpu 67 ***
ERROR 2024-02-16T21:46:33.756800651Z [2024-02-16 21:46:33,756 E 1 6357] logging.cc:361: PC: @ 0x7e9291287a37 (unknown) ncclShmOpen()
ERROR 2024-02-16T21:46:33.758085489Z [2024-02-16 21:46:33,758 E 1 6357] logging.cc:361: @ 0x7e9456f02520 3456 (unknown)
ERROR 2024-02-16T21:46:33.759702920Z [2024-02-16 21:46:33,759 E 1 6357] logging.cc:361: @ 0x74352d6c63636e2f (unknown) (unknown)</p>
</blockquote>
<p>I have tried with 2/4/8 NVIDIA_L4 GPUs,</p>
<p>Dockerfile</p>
<pre><code>FROM nvidia/cuda:12.1.1-runtime-ubuntu22.04 as builder
...
# install deps/poetry/etc..

# install project deps and other deps i don't need locally:

RUN poetry add vllm\
     accelerate\
     deepspeed\
     auto-gptq\
     optimum\
     peft\
     transformers\
     flax==0.8.0\
     torch==2.1.2\
     tensorflow\
     bitsandbytes\
     autoawq
</code></pre>
<p>Also, this log might be important to understand:</p>
<blockquote>
<p>Initializing an LLM engine with config:
model='TheBloke/Mixtral-8x7B-Instruct-v0.1-AWQ',
tokenizer='TheBloke/Mixtral-8x7B-Instruct-v0.1-AWQ',
tokenizer_mode=auto, revision=None, tokenizer_revision=None,
trust_remote_code=False, dtype=torch.float16, max_seq_len=32768,
download_dir=None, load_format=auto, tensor_parallel_size=8,
disable_custom_all_reduce=False, quantization=awq,
enforce_eager=False, kv_cache_dtype=auto, seed=0)&quot; }</p>
</blockquote>
<p>Thanks!</p>
","huggingface"
"78005566","Running Mixtral 8x7B on Colab Free Tier and MacOS?","2024-02-16 06:50:36","","0","168","<python><pytorch><huggingface-transformers><huggingface>","<p>I am trying to run Mixtral 8x 7B on Colab Free Tier and Mac (M3 PRO 36 RAM).
I ran the code from the <a href=""https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1"" rel=""nofollow noreferrer"">model page</a> on Hugging Face, but I encountered an out-of-memory error. I am wondering if it's impossible to run such a large model due to the limitations of the free version of Colab and my Mac environment, or if there is another solution.</p>
<p>Here is the code I tried.</p>
<pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = &quot;mistralai/Mixtral-8x7B-Instruct-v0.1&quot;
tokenizer = AutoTokenizer.from_pretrained(model_id)

model = AutoModelForCausalLM.from_pretrained(model_id)

text = &quot;Hello my name is&quot;
inputs = tokenizer(text, return_tensors=&quot;pt&quot;)

outputs = model.generate(**inputs, max_new_tokens=20)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
</code></pre>
<p>After finishing downloading 19 safetensors, it starts loading checkpoint shards but stopped after 5%.</p>
","huggingface"
"78002111","Why no log for training model, and key_error for 'eval_loss'?","2024-02-15 15:44:50","","2","291","<python><huggingface><peft>","<p>I am trying to build a LORA model for sentiment analysis as part of an academic project. However, when training the model, I keep getting an error when computing the loss. I have tried a bunch of different methods for computing the loss, but nothing is working.</p>
<p>Any help would be greatly appreciated. Github link is here; <a href=""https://github.com/therrief87/udacity/blob/main/v4%20Udacity%20Lightweight%20Fine%20Tuning%20(GPT2)%20Smaller%20Training%20Data%20(1).ipynb"" rel=""nofollow noreferrer"">https://github.com/therrief87/udacity/blob/main/v4%20Udacity%20Lightweight%20Fine%20Tuning%20(GPT2)%20Smaller%20Training%20Data%20(1).ipynb</a></p>
<p>Code for training and computing metrics is also below;</p>
<pre><code>training_args = TrainingArguments(
    output_dir='C:/Users/felix/Downloads',
    evaluation_strategy='epoch',
    save_strategy='epoch',
    learning_rate=.2,
    per_device_train_batch_size=18,
    per_device_eval_batch_size=18,
    num_train_epochs=1,
    load_best_model_at_end=True,
    weight_decay=0.1,
    remove_unused_columns=False,
    #label_names=&quot;labels&quot;
)

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = torch.from_numpy(predictions)  # Convert predictions to tensor
    labels = torch.from_numpy(labels).long()  # Convert labels to tensor
    loss = nn.CrossEntropyLoss()(predictions, labels)  # Calculate the evaluation loss
    accuracy = (torch.argmax(predictions, axis=1) == labels).float().mean()  # Calculate the accuracy

    # Print the metrics dictionary for debugging
    metrics = {&quot;eval_loss&quot;: loss.item(), &quot;accuracy&quot;: accuracy.item()}
    print(&quot;Metrics:&quot;, metrics)

    return metrics
trainer = Trainer(
    model=lora_model,
    args = training_args,
    train_dataset = random_train_samples,
    eval_dataset = new_dataset['test'],
    tokenizer=tokenizer,
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer),
    compute_metrics = compute_metrics
)
trainer.train()
</code></pre>
<p>ERROR below;</p>
<pre><code>KeyError                                  Traceback (most recent call last)
Cell In[45], line 10
      1 trainer = Trainer(
      2     model=lora_model,
      3     args = training_args,
   (...)
      8     compute_metrics = compute_metrics
      9 )
---&gt; 10 trainer.train()

File ~\anaconda3\Lib\site-packages\transformers\trainer.py:1555, in Trainer.train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)
   1553         hf_hub_utils.enable_progress_bars()
   1554 else:
-&gt; 1555     return inner_training_loop(
   1556         args=args,
   1557         resume_from_checkpoint=resume_from_checkpoint,
   1558         trial=trial,
   1559         ignore_keys_for_eval=ignore_keys_for_eval,
   1560     )

File ~\anaconda3\Lib\site-packages\transformers\trainer.py:1944, in Trainer._inner_training_loop(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)
   1941     self.control.should_training_stop = True
   1943 self.control = self.callback_handler.on_epoch_end(args, self.state, self.control)
-&gt; 1944 self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
   1946 if DebugOption.TPU_METRICS_DEBUG in self.args.debug:
   1947     if is_torch_tpu_available():
   1948         # tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)

File ~\anaconda3\Lib\site-packages\transformers\trainer.py:2267, in Trainer._maybe_log_save_evaluate(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)
   2264         self.lr_scheduler.step(metrics[metric_to_check])
   2266 if self.control.should_save:
-&gt; 2267     self._save_checkpoint(model, trial, metrics=metrics)
   2268     self.control = self.callback_handler.on_save(self.args, self.state, self.control)

File ~\anaconda3\Lib\site-packages\transformers\trainer.py:2383, in Trainer._save_checkpoint(self, model, trial, metrics)
   2381 if not metric_to_check.startswith(&quot;eval_&quot;):
   2382     metric_to_check = f&quot;eval_{metric_to_check}&quot;
-&gt; 2383 metric_value = metrics[metric_to_check]
   2385 operator = np.greater if self.args.greater_is_better else np.less
   2386 if (
   2387     self.state.best_metric is None
   2388     or self.state.best_model_checkpoint is None
   2389     or operator(metric_value, self.state.best_metric)
   2390 ):

KeyError: 'eval_loss'
</code></pre>
<p>Thank you for any help.</p>
","huggingface"
"78000905","How does SetFit work for few shot text classification?","2024-02-15 12:48:31","","0","110","<python><nlp><huggingface-transformers><huggingface>","<p>I'm seeking clarity on the utilization of SetFit for few-shot text classification, as discussed in the article <a href=""https://wandb.ai/gladiator/SetFit/reports/SetFit-Efficient-Few-Shot-Learning-Without-Prompts--VmlldzozMDUyMzk2"" rel=""nofollow noreferrer"">here</a>. While I grasp the main concept, I'm having difficulty understanding the role of the evaluation dataset in SetFit Trainer.</p>
<p>To confirm my understanding: the initial training dataset is augmented by creating sentence pairs (positive or negative), and the augmented dataset is employed to fine-tune the sentence transformer. Subsequently, this fine-tuned transformer encodes the initial dataset (not the augmented one). The embeddings of the initial dataset are then used to train the classification layer, and the model's performance is assessed using a chosen metric.</p>
<p>However, I'm puzzled about the optional use of an evaluation dataset. If it is indeed optional, what purpose does it serve in this context?</p>
<p>thanks for the help !</p>
","huggingface"
"77998282","HuggingFace - For tuning a classifier head on a pretrained BERT should I use `last_hidden_state` or `outputs[0][:, 0, :]` from the BERT?","2024-02-15 03:22:58","","0","46","<huggingface>","<p>So I want to take the output of the last hidden state embedding from a BERT model (as in the embedding that comes out once it's done processing the sequence) and pass that into a classifier head which I tune.</p>
<pre><code>outputs = bert(input_ids=input_ids, attention_mask=attention_mask)
logits = classifier_head_ll(torch.concat((outputs[0][:, 0, :])
</code></pre>
<p>The above code I got from a tutorial but now I'm questioning if this is correct. I feel like using <code>outputs.last_hidden_state[:, -1, :]</code> might be what I want, but also I have concerns cause the last state is also after processing padding, which should be handled by the attention_mask, but if that were the case shouldn't <code>last_hidden_state[:, -1, :]</code> == <code>last_hidden_state[:, -2, :]</code> (which didn't look like the case when I examined the tensor) for rows that are padded to be the same as the max length? Which hidden state should I use if I want to get the hidden state embedding that is produced at the end of the sequence?</p>
","huggingface"
"77993130","Inference execution in huggingface transformers.js","2024-02-14 09:11:49","","1","74","<javascript><nlp><huggingface-transformers><huggingface><huggingface-tokenizers>","<p>I found a code that works fine in python.</p>
<pre><code>def text_classification_inference(self, input_text):
    if not self.model or not self.tokenizer or not self.id2label:
        print('Something wrong has been happened!')
        return

    pt_batch = self.tokenizer(
        input_text,
        padding=True,
        truncation=True,
        max_length=self.config.max_position_embeddings,
        return_tensors=&quot;pt&quot;
    )

    pt_outputs = self.model(**pt_batch)
    pt_predictions = torch.argmax(F.softmax(pt_outputs.logits, dim=1), dim=1)

    output_predictions = []
    for i, sentence in enumerate(input_text):
        output_predictions.append((sentence, self.id2label.get(pt_predictions[i].item())))
    return output_predictions
</code></pre>
<p>This is the code github address:
<a href=""https://github.com/Mofid-AI/persian-nlp-benchmark/blob/main/text_classification.py"" rel=""nofollow noreferrer"">https://github.com/Mofid-AI/persian-nlp-benchmark/blob/main/text_classification.py</a></p>
<p>I want to do the same thing with traformers library in javascipt(traformers.js). I tried pipeline api in js successfully but encountered error for too long inputs. So I need to set truncation=True for tokenizer in js. I'm beginner to both python and javascript. Appreciate in advance</p>
","huggingface"
"77989216","Hugging Face Hub API swagger","2024-02-13 15:46:06","","0","105","<swagger><huggingface><huggingface-hub>","<p>I am looking for, if it exist, the swagger of the api for interacting with ‘huggingface.co’. The API is described here:</p>
<p><a href=""https://huggingface.co/docs/hub/api"" rel=""nofollow noreferrer"">https://huggingface.co/docs/hub/api</a></p>
<p>There is even a playground tool provided so I think there exist a swagger (maybe it is not a proper swagger, or the playground was manually implemented for each)</p>
<p>I have tried to look through the documentation and on the github repositories but couldn’t find it.</p>
<p>I found the following two swaggers:</p>
<pre><code>api.endpoints.huggingface.cloud/openapi.json
github: /huggingface/datasets-server/blob/main/docs/source/openapi.json
</code></pre>
<p>However those are not for the &quot;hub&quot; but for the model and dataset APIs</p>
","huggingface"
"77984729","ImportError: cannot import name 'VectorStoreIndex' from 'llama_index' (unknown location)","2024-02-12 22:56:20","","14","26603","<python><jupyter-notebook><amazon-sagemaker><huggingface><llama-index>","<p>I ran into this problem when I was trying to import the following libraries and it is giving the error &quot;ImportError: cannot import name 'VectorStoreIndex' from 'llama_index' (unknown location)&quot;</p>
<p>I ran this exact same code in the morning and it worked perfectly.</p>
<p>I did <code>!pip install llama_index</code></p>
<pre><code>from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext
from llama_index.llms import HuggingFaceLLM
from llama_index.prompts.prompts import SimpleInputPrompt
</code></pre>
<p>I tried commenting out first line and faced same issue for HuggingFaceLLM module
Same issue for SimpleInputPrompt, got error &quot;ModuleNotFoundError: No module named 'llama_index.prompts'&quot;</p>
<p>First I faced the problem in a sagemaker notebook so I thought the issue was with the sagemaker notebook so I spun up a clean new notebook and I got the same error.
So, I tried the code in my local Jypiter notebook, google collab notebook, sagemaker studiolab notebook and I got the same error.</p>
","huggingface"
"77971974","Using HingeEmbeddingLoss in PyTorch for Binary Classification","2024-02-10 05:33:49","","0","68","<pytorch><nlp><huggingface-transformers><huggingface>","<p>I am working on a text classification problem using Hugging Face's XLM-RoBERTa model in PyTorch. I want to use the nn.HingeEmbeddingLoss for my custom loss function since it seems suitable for my binary classification task.</p>
<p>According to the PyTorch documentation, nn.HingeEmbeddingLoss expects labels to be either 1 or -1. However, my original dataset has labels as 0 and 1. Accordingly, I converted my labels from (0, 1) to (1, -1).</p>
<p>On running the CustomTrainer with the loss function set to HingeEmbeddingLoss, and calling trainer.train() i get the following error:</p>
<p>RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with <code>TORCH_USE_CUDA_DSA</code> to enable device-side assertions.</p>
<p>I have been given the understand that this happens because the labels are expected to be in the range [0,number of classes-1] .</p>
<p>Can someone please guide me on how to proceed?</p>
<p>Here is my code:</p>
<pre><code>class CustomTrainer(Trainer):
     def compute_loss(self,model,inputs,return_outputs = False):
        outputs = model(**inputs)
        logits = outputs.get(&quot;logits&quot;)
        labels = inputs.get(&quot;labels&quot;)
        #print(logits)
        #print(labels)
        print(&quot;Logits size:&quot;, logits.size())  # or logits.shape
        print(&quot;Labels size:&quot;, labels.size())
        labels =nn.functional.one_hot(labels, num_classes=2).float()
        #labels = labels.squeeze(dim=-1) ##new
        loss_func = nn.HingeEmbeddingLoss()
        #(weight=class_weights)
        loss = loss_func(logits,labels)
        return (loss,outputs) if return_outputs else loss
</code></pre>
<p>Edit:</p>
<pre><code>Traceback (most recent call last):
  File &quot;hinge_embedding_loss.py&quot;, line 190, in &lt;module&gt;
    tokenizer = AutoTokenizer.from_pretrained(path,local_files_only=True)
  File &quot;/Data/home/ug22/2203321/text_cls/seq_cls_env/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py&quot;, line 752, in from_pretrained
    config = AutoConfig.from_pretrained(
  File &quot;/Data/home/ug22/2203321/text_cls/seq_cls_env/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py&quot;, line 1082, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File &quot;/Data/home/ug22/2203321/text_cls/seq_cls_env/lib/python3.8/site-packages/transformers/configuration_utils.py&quot;, line 644, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File &quot;/Data/home/ug22/2203321/text_cls/seq_cls_env/lib/python3.8/site-packages/transformers/configuration_utils.py&quot;, line 699, in _get_config_dict
    resolved_config_file = cached_file(
  File &quot;/Data/home/ug22/2203321/text_cls/seq_cls_env/lib/python3.8/site-packages/transformers/utils/hub.py&quot;, line 360, in cached_file
    raise EnvironmentError(
OSError: Hinge_Embedding/bloom-560m 4e-5 does not appear to have a file named config.json. Checkout 'https://huggingface.co/Hinge_Embedding/bloom-560m 4e-5/None' for available files.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t &gt;= 0 &amp;&amp; t &lt; n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t &gt;= 0 &amp;&amp; t &lt; n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t &gt;= 0 &amp;&amp; t &lt; n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t &gt;= 0 &amp;&amp; t &lt; n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t &gt;= 0 &amp;&amp; t &lt; n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t &gt;= 0 &amp;&amp; t &lt; n_classes` failed.
</code></pre>
<p>Here is the full error, for the model bigscience/bloom-560m</p>
","huggingface"
"77970659","Huggingface pipeline error: IndexError: too many indices for tensor of dimension 2","2024-02-09 20:20:41","","1","116","<python><pytorch><huggingface-transformers><huggingface>","<p>Anyone know why I get that error when I run the pipeline?</p>
<pre><code>from transformers import AutoModelForSequenceClassification
from transformers import AutoTokenizer
from transformers import pipeline

modeltemp = AutoModelForSequenceClassification.from_pretrained(&quot;bert-base-cased&quot;, num_labels=5)
tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-cased&quot;)

unmasker = pipeline('fill-mask', model=modeltemp, tokenizer=tokenizer)
unmasker(&quot;Hello I drive a red [MASK].&quot;)
</code></pre>
<p>If I directly have the pipeline take in bert-base-cased, everything works. But if I first load bert-base-cased using AutoModel, it doesn't work.</p>
","huggingface"
"77969942","Why my Mistral-7b model generate a bad answer?","2024-02-09 17:44:09","","0","659","<pytorch><dataset><huggingface-transformers><huggingface><mistral-7b>","<p>I’m new to coding and I’ve been learning to code in LLM for some time now. I’m trying to create a chatbot in French using the vigolstral-chat model, which is a fine-tuned model based on Mistral7b. I’m attempting to create a chatbot using my own dataset, which consists of data from my faculty that I scraped from their website (public data), such as the number of students within the faculty, etc. I followed a tutorial to fine-tune the model and uploaded it correctly on Hugging Face. However, when I try to infer with my model, it responds with a random sequence of letters. I think there might be an issue with my dataset, which is structured as follows: LINE: text…</p>
<p>Does anyone have any idea where the problem might be coming from? Thank you very much.</p>
<pre><code>from huggingface_hub import interpreter_login
interpreter_login()

import os
import argparse
import torch
import torch.nn as nn
from datasets import load_dataset,Features,Value,load_from_disk
import transformers
from functools import partial
from transformers import MistralForCausalLM, MistralModel, MistralConfig, AutoConfig, AutoModelForCausalLM, AutoTokenizer, set_seed, TrainingArguments, BitsAndBytesConfig, \
    DataCollatorForLanguageModeling, Trainer, TrainingArguments 
import bitsandbytes as bnb
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, AutoPeftModelForCausalLM ,PeftModel
from trl import SFTTrainer
from unidecode import unidecode
base_model=&quot;bofenghuang/vigostral-7b-chat&quot;
new_model=&quot;ALIE_0.5&quot;
data = load_dataset('json',data_files={'train': ['C:/Users/sacha/Documents/projet ALI/test vs/ALIE0.2/Dataset/lyon1_charlie_dataset_train.json'],
                                        'test': ['C:/Users/sacha/Documents/projet ALI/test vs/ALIE0.2/Dataset/lyon1_delta_dataset_validation.json']})

print(data['train'])
bnb_config = BitsAndBytesConfig(
    load_in_4bit= True,
    bnb_4bit_quant_type= &quot;nf4&quot;,
    bnb_4bit_compute_dtype= torch.bfloat16,
    bnb_4bit_use_double_quant= False, #a voir
)

model = AutoModelForCausalLM.from_pretrained(
    base_model,
    quantization_config=bnb_config,
    device_map={&quot;&quot;: 0} )


model.config.use_cache = False # silence the warnings. Please re-enable for inference!
model.config.pretraining_tp = 1
model.gradient_checkpointing_enable()
# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.add_eos_token = True
tokenizer.add_bos_token, tokenizer.add_eos_token    

model = prepare_model_for_kbit_training(model)
peft_config = LoraConfig(
        r=16,
        lora_alpha=16,
        lora_dropout=0.05,
        bias=&quot;none&quot;,
        task_type=&quot;CAUSAL_LM&quot;,
        target_modules=[&quot;q_proj&quot;, &quot;k_proj&quot;, &quot;v_proj&quot;, &quot;o_proj&quot;,&quot;gate_proj&quot;]
    )
model = get_peft_model(model, peft_config)

training_arguments = TrainingArguments(
    output_dir= &quot;C:/Users/sacha/Documents/projet ALI/test vs/ALIE0.2/end&quot;,
    num_train_epochs= 1,
    per_device_train_batch_size= 2,
    gradient_accumulation_steps= 4,
    optim = &quot;paged_adamw_8bit&quot;,
    save_steps= 5000,
    logging_steps= 30,
    learning_rate= 2e-4,
    weight_decay= 0.001,
    fp16= False,
    bf16= False,
    max_grad_norm= 0.3,
    max_steps= -1,
    warmup_ratio= 0.3,
    group_by_length= True,
    lr_scheduler_type= &quot;constant&quot;,
    
)

trainer = SFTTrainer(
    model=model,
    train_dataset=data['train'],
    eval_dataset=data['test'],
    peft_config=peft_config,
    max_seq_length= None,
    tokenizer=tokenizer,
    dataset_text_field=&quot;line&quot;,
    args=training_arguments,
    packing= False,
    
)

trainer.train()
# Save the fine-tuned model
trainer.model.save_pretrained(new_model)
model.config.use_cache = True
model.eval()
model.push_to_hub(new_model,use_temp_dir=False)

base_model=&quot;bofenghuang/vigostral-7b-chat&quot;
new_model=&quot;AscheZ/ALIE_0.5&quot;

base_model_reload = AutoModelForCausalLM.from_pretrained(
    base_model, low_cpu_mem_usage=True,
    return_dict=True,torch_dtype=torch.bfloat16,
    device_map= {&quot;&quot;: 0})
model = PeftModel.from_pretrained(base_model_reload, new_model)
model = model.merge_and_unload()

tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = &quot;right&quot;
model.push_to_hub(new_model, use_temp_dir=False)
tokenizer.push_to_hub(new_model, use_temp_dir=False)

</code></pre>
<p>My code for inference :</p>
<pre><code>model_name=&quot;bofenghuang/vigostral-7b-chat&quot;
adapater_name=&quot;AscheZ/ALIE_0.5&quot;
print(f&quot;Starting to load the model {model_name} into memory&quot;)

m = AutoModelForCausalLM.from_pretrained(
    model_name,
    load_in_4bit=True,
    torch_dtype=torch.bfloat16,
    device_map={&quot;&quot;: 0}
)
m = PeftModel.from_pretrained(m, adapater_name)
m = m.merge_and_unload()
tok = AutoTokenizer.from_pretrained(model_name)
tok.bos_token_id = 1

stop_token_ids = [0]

print(f&quot;Successfully loaded the model {model_name} into memory&quot;)

prompt = &quot;Combien il y'a d'étudiant à lyon 1? &quot;
inputs = tok(prompt, return_tensors=&quot;pt&quot;).to('cuda')

outputs = m.generate(**inputs, do_sample=True, num_beams=1, max_new_tokens=100)
print(tok.batch_decode(outputs, skip_special_tokens=True))
</code></pre>
<p><a href=""https://i.sstatic.net/vUYlX.png"" rel=""nofollow noreferrer"">Screen inference</a>
<a href=""https://i.sstatic.net/R2s5L.png"" rel=""nofollow noreferrer"">type of my dataset</a></p>
<p>I think is dataset problems but what? Or hyperparameters of my training ? But i dont see what...</p>
","huggingface"
"77964109","how to load to Colab a split of common_voice ""spanish"" dataset from HF?","2024-02-08 18:49:08","","0","54","<google-colaboratory><huggingface><huggingface-datasets><automatic-speech-recognition>","<p>I am trying to load the dataset &quot;spanish&quot; from Colab in just a 10% since it is too large, however it is still downloading the complete dataset from HuggingFace. I have tried two ways, by percentage or by slicing, none of them worked, it is still downloading the whole dataset, so that it brokes. How to solve this?</p>
<pre><code>common_voice[&quot;train&quot;] = load_dataset(&quot;mozilla-foundation/common_voice_16_1&quot;, &quot;es&quot;, split=&quot;train[:10%]&quot;, use_auth_token=True)
</code></pre>
<p>Not even downloading only 10 rows worked:</p>
<pre><code>common_voice[&quot;train&quot;] = load_dataset(&quot;mozilla-foundation/common_voice_16_1&quot;, &quot;es&quot;, split=&quot;train[:10]&quot;, use_auth_token=True)
</code></pre>
<p>I also tried:</p>
<pre><code>common_voice[&quot;train&quot;] = load_dataset(&quot;mozilla-foundation/common_voice_16_1&quot;, &quot;es&quot;,
                                     split=ReadInstruction('train', to=10, unit='%'))
</code></pre>
<p>I tried to download a small slice of a dataset</p>
","huggingface"
"77962154","Encountering UnidentifiedImageError when Rendering Images in Streamlit App Using Hugging Face API","2024-02-08 13:42:02","","0","38","<python><streamlit><huggingface><text2image>","<p>I'm currently developing a Streamlit application that leverages the Hugging Face API to generate images based on user prompts. The application takes user input prompts, sends them to the API, and then displays the generated images back to the user.</p>
<p>here is the code for that:-</p>
<pre><code>import streamlit as st
import requests
import io
from PIL import Image


@st.cache_data
def DreamShaper_v7(token,inputs, guide_scale, inference_steps):
    API_URL = &quot;https://api-inference.huggingface.co/models/SimianLuo/LCM_Dreamshaper_v7&quot;
    headers = {&quot;Authorization&quot;: f&quot;Bearer {token}&quot;}

    payload = {
        &quot;inputs&quot;: inputs,
        &quot;guide_scale&quot;: guide_scale,
        &quot;inference_steps&quot;: inference_steps
    }
    
    response = requests.post(API_URL, headers=headers, json=payload)
    image_bytes = response.content

    return image_bytes


def display_DreamShaper_v7(token):
    st.markdown(&quot;&lt;h1 style='text-align:center;'&gt;DreamShaper v7&lt;/h1&gt;&quot;, unsafe_allow_html=True)
    st.markdown(&quot;&lt;p style='text-align:center;'&gt;You can download the image with right click &gt; save image&lt;/p&gt;&quot;, unsafe_allow_html=True)

    with st.sidebar:
        st.title(&quot;Parameters Tuning&quot;)
        st.session_state.GS_val2 = st.slider(&quot;Select Guidencescale&quot;, key=&quot;slider1&quot;, min_value=0.1, max_value=10.0, value=9.0, step=0.1, help=&quot;how much your prompt effect your image&quot;)
        if st.session_state.GS_val2 &gt; 9.9:
            st.session_state.GS_val2 = 10
        st.write('Guidence scale:', st.session_state.GS_val2)

        st.session_state.inference_steps_val2 = st.slider(&quot;Select Inference Steps&quot;, key=&quot;slider2&quot;, min_value=50, max_value=200, value=100, step=1, help=&quot;Number of inference steps for image generation&quot;)
        st.write('Inference Steps:', st.session_state.inference_steps_val2)

        st.subheader(&quot;Usage Manual (must Read !)&quot;)
        st.markdown(&quot;&quot;&quot;&lt;ul&gt;
                        &lt;li&gt;DreamShaper v7&lt;/l1&gt;
                        &lt;li&gt;It convert your text prompts into image&lt;/l1&gt;
                        &lt;li&gt;When your prompts contains any hateful or malicious text it wont give you image, instead it might give you error or a blank image so dont do it !&lt;/l1&gt;
                        &lt;li&gt;Sometimes it migth give you error even when you give legit prompt in that case try changing prompt a little or clear cache data from above settings&lt;/l1&gt;
                        &lt;li&gt;There is only 8000 char input allowed in a single prompt so write wisely&lt;/li&gt;
                        &lt;li&gt;when your chat history is long it might get Stuck or takes more time to render page (will fix in future), If you encounter this start another session by refreshing page&lt;/li&gt;
                        &lt;/ul&gt;
                    
                    &quot;&quot;&quot;,unsafe_allow_html=True)
        st.success(&quot;You are Good to go !&quot;)

    if &quot;messages_Dream&quot; not in st.session_state:
        st.session_state[&quot;messages_Dream&quot;] = [
            {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;What kind of image do you need me to generate? (example: entire universe in glass jar)&quot;}]

    # Display previous prompts and results
    for message in st.session_state.messages_Dream:
        st.chat_message(message[&quot;role&quot;]).write(message[&quot;content&quot;])
        if &quot;image&quot; in message:
            st.chat_message(&quot;assistant&quot;).image(message[&quot;image&quot;], caption=message[&quot;prompt&quot;], use_column_width=True)

    # Prompt Logic
    prompt = st.chat_input(&quot;Enter your prompt:&quot;)

    if prompt:
        # Input prompt
        st.session_state.messages_Dream.append({&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt})
        st.chat_message(&quot;user&quot;).write(prompt)

        # Query Stable Diffusion
        headers = {&quot;Authorization&quot;: f&quot;Bearer {token}&quot;}
        image_bytes = DreamShaper_v7(token, prompt, st.session_state.GS_val2, st.session_state.inference_steps_val2)
        # Return Image
        image = Image.open(io.BytesIO(image_bytes))
        msg = f'Here is your image related to &quot;{prompt}&quot;'

        # Show Result
        st.session_state.messages_Dream.append({&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: msg, &quot;prompt&quot;: prompt, &quot;image&quot;: image})
        st.chat_message(&quot;assistant&quot;).write(msg)
        st.chat_message(&quot;assistant&quot;).image(image, caption=prompt, use_column_width=True)


</code></pre>
<p>The application works well for the most part. However, I'm encountering an issue when the user provides the first prompt after launching the application. The error message received is:</p>
<p>error:-</p>
<pre><code>UnidentifiedImageError: cannot identify image file &lt;_io.BytesIO object at 0x000001A2D16E9CB0&gt;
Traceback:
File &quot;D:\streamlit\Media-generation\.venv\Lib\site-packages\streamlit\runtime\scriptrunner\script_runner.py&quot;, line 535, in _run_script
    exec(code, module.__dict__)
File &quot;D:\streamlit\Media-generation\app.py&quot;, line 42, in &lt;module&gt;
    display_Anime_df(HUGGINGFACE_API_KEY)
File &quot;D:\streamlit\Media-generation\Models\Anime_DF.py&quot;, line 74, in display_Anime_df
    image = Image.open(io.BytesIO(image_bytes))
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File &quot;D:\streamlit\Media-generation\.venv\Lib\site-packages\PIL\Image.py&quot;, line 3309, in open
    raise UnidentifiedImageError(msg)
</code></pre>
<p><strong>Interestingly, if I change the prompt or clear the cache data, the application works as expected. It seems like the issue arises only when the first prompt is provided after launching the application.</strong></p>
","huggingface"
"77950319","Structure-agnostic Knowledge Graph Extracting LLM","2024-02-06 19:13:22","","0","85","<dataset><large-language-model><huggingface><knowledge-graph>","<p>I want to fine-tune (or train from scratch if I must) an LLM that takes data of any structure, generates a knowledge graph from that data, and allows a user to interact with the data via a chatbot.</p>
<p>There are knowledge graph datasets out there but these are datasets that take in a given data of a given format and convert it into a knowledge graph. In other words, these datasets are structure-specific.</p>
<p>I want to fine-tune a model that converts data of any structure to a knowledge graph.</p>
<p>How can I go about it?</p>
","huggingface"
"77948682","How to stop at 512 tokens when sending text to pipeline? HuggingFace and Transformers","2024-02-06 14:55:17","77949948","2","292","<deep-learning><huggingface-transformers><huggingface><huggingface-tokenizers>","<p>I want to test my model using Pipeline by Transformers. My model is a pretrained BERT, which works great if the given text is &lt; 512 tokens. However, when sending the a larger text to the pipeline, it breaks, because it's too long. I tried to search, but couldn't figure out how to solve this issue.</p>
<p>This is my code:</p>
<pre><code>def get_predicted_folder(text, model):
    pipe = pipeline(&quot;text-classification&quot;, model=model)
    if text:
        predicted_folder = pipe(text)
        label = predicted_folder[0]['label']
        score = predicted_folder[0]['score']
        return label, score
    else:
        err = &quot;Error: The provided text is empty.&quot;
        return err, None

my_saved_model = &quot;model/danish_bert_model&quot; (it is saved locally)
label, score = get_predicted_folder(text, my_saved_model)
</code></pre>
<p>It gives me this error: <code>RuntimeError: The size of tensor a (1593) must match the size of tensor b (512) at non-singleton dimension 1</code></p>
<p>I tried to give <code>tokenizer=model</code> to the pipeline, and have this <code>tokenizer = AutoTokenizer.from_pretrained(model_ckpt)</code> before calling the <code>get_predicted_folder</code> method, but it doesn't solve the issue.</p>
<p>The tokenizer inside the model is this:</p>
<pre><code>def tokenize_dataset(tokenizer, examples):
    return tokenizer(examples['text'], truncation=True, max_length=512)
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
    dataset = dataset.map(lambda examples: tokenize_dataset(tokenizer, examples), batched=True)
</code></pre>
<p>Can someone please help me?</p>
<p>Thanks so much in advance!</p>
","huggingface"
"77948005","Save and load the nsql-llama-2-7B - AutoModelForCausalLM model","2024-02-06 13:19:39","","0","360","<python><huggingface-transformers><huggingface><huggingface-trainer>","<p><strong>I am trying to save and load the nsql-llama-2-7B model after I have finetuned him.</strong></p>
<p>I can see that the model is saved but I can not load it.
this is the code:</p>
<pre><code>from transformers import default_data_collator, Trainer, TrainingArguments
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

trainer.save_model(&quot;./my_model&quot;)
model.save_pretrained(&quot;./my_model1&quot;)

model2 = AutoModelForCausalLM.from_pretrained(&quot;./my_model1&quot;, load_in_8bit=True, torch_dtype=torch.bfloat16, device_map='auto')
model3 = AutoModelForCausalLM.from_pretrained(&quot;./my_model1&quot;)
</code></pre>
<p>loading the model with AutoPeftModelForCausalLM caused a crash:</p>
<pre><code>from peft import AutoPeftModelForCausalLM

model = AutoPeftModelForCausalLM.from_pretrained(&quot;./my_model1&quot;, local_files_only=True)
</code></pre>
<p>both model2 and model3 failed.</p>
<p><strong>The error is:</strong></p>
<pre><code>my_model1 does not appear to have a file named config.json
</code></pre>
<p><strong>my_model files:</strong></p>
<pre><code>adapter_config.json, adapter_model.bin , adapter_model.safetensors, README.md, training_args.bin
</code></pre>
<p><strong>my_model1 files:</strong></p>
<pre><code>adapter_config.json, adapter_model.safetensors, logs/ README.md

**note that removing the adapter_ prefix from the files didn't help and caused the error:**
Should have a model_type key in its config.json, or contain one of the following strings in its name: albert, align, altclip, audio-spectrogram-transformer, autoformer, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, camembert, canine, chinese_clip, clap, clip, clipseg, codegen, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, data2vec-audio, data2vec-text, data2vec-vision, deberta, deberta-v2, decision_transformer, deformable_detr, deit, deta, detr, dinat, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, flaubert, flava, fnet, focalnet, fsmt, funnel, git, glpn, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, graphormer, groupvit, hubert, ibert, imagegpt, informer, instructblip, jukebox, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, longformer, longt5, luke, lxmert, m2m_100, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, mpnet, mra, mt5, musicgen, mvp, nat, nezha, nllb-moe, nystromformer, oneformer, open-llama, openai-gpt, opt, owlvit, pegasus, pegasus_x, perceiver, pix2struct, plbart, poolformer, prophetnet, qdqbert, rag, realm, reformer, regnet, re...
</code></pre>
","huggingface"
"77947395","LangChain + Hugging Face -> HuggingFacePIpeline Error","2024-02-06 11:36:18","77948715","1","558","<python><apache-spark><databricks><langchain><huggingface>","<p>I'm trying to do the following simple code:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import pipeline
import langchain
from langchain.llms import HuggingFacePipeline

model_name = &quot;bert-base-uncased&quot;
task = &quot;question-answering&quot;

hf_pipeline = pipeline(task, model=model_name)

langchain_pipeline = HuggingFacePipeline(hf_pipeline)
</code></pre>
<p>I get the following error:</p>
<ul>
<li>ERROR: <code>TypeError: Serializable.__init__() takes 1 positional argument but 2 were given</code></li>
<li>LINE: <code>langchain_pipeline = HuggingFacePipeline(hf_pipeline)</code></li>
</ul>
<blockquote>
<p>Haven't found anything online that actually helped me here</p>
</blockquote>
<hr />
<p>I'm using Databricks with the following cluster:</p>
<ul>
<li>Runtime: 12.2 LTS ML (includes Apache Spark 3.3.2, Scala 2.12)</li>
<li>Node type: Standard_DS5_v2   56 GB Memory, 16 Cores</li>
<li>Libraries: <a href=""https://i.sstatic.net/8Npg6.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/8Npg6.png"" alt=""cluster2libraries"" /></a></li>
</ul>
","huggingface"
"77947277","LangChain + Hugging Face -> LangChain LLM import Error","2024-02-06 11:18:10","","1","590","<python><apache-spark><databricks><langchain><huggingface>","<p>I'm trying to do the following simple code:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import pipeline
import langchain
from langchain.llms import HuggingFacePipeline

model_name = &quot;bert-base-uncased&quot;
task = &quot;question-answering&quot;
</code></pre>
<p>I get the following error:</p>
<ul>
<li>ERROR: <code>TypeError: issubclass() arg 1 must be a class</code></li>
<li>LINE: <code>from langchain.llms import HuggingFacePipeline</code></li>
</ul>
<p>I found the following questions (that didn't help):</p>
<ol>
<li><a href=""https://stackoverflow.com/questions/76313568/typeerror-issubclass-arg-1-must-be-a-class-when-importing-langchain-in-flask"">TypeError: issubclass() arg 1 must be a class when importing langchain in Flask</a></li>
<li><a href=""https://stackoverflow.com/questions/74346565/fastapi-typeerror-issubclass-arg-1-must-be-a-class-with-modular-imports"">FastAPI - &quot;TypeError: issubclass() arg 1 must be a class&quot; with modular imports</a></li>
<li><a href=""https://stackoverflow.com/questions/76663778/typeerror-issubclass-arg-1-must-be-a-class-when-using-langchain-in-azure"">TypeError: issubclass() arg 1 must be a class when using langchain in azure</a></li>
<li><a href=""https://stackoverflow.com/questions/76313592/import-langchain-error-typeerror-issubclass-arg-1-must-be-a-class"">import langchain =&gt; Error : TypeError: issubclass() arg 1 must be a class</a></li>
</ol>
<hr />
<p>I'm using Databricks with the following cluster:</p>
<ul>
<li>Runtime: 13.2 ML (includes Apache Spark 3.4.0, GPU, Scala 2.12)</li>
<li>Worker &amp; Driver type: Standard_NC21s_v3    224 GB Memory, 2 GPUs</li>
<li>Libraries: <a href=""https://i.sstatic.net/45IbX.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/45IbX.png"" alt=""cluster1libraries"" /></a></li>
</ul>
","huggingface"
"77947263","autotrain.trainers.common:wrapper:92 - No GPU found. A GPU is needed for quantization","2024-02-06 11:16:13","","1","370","<python><docker><huggingface><huggingface-hub>","<pre class=""lang-py prettyprint-override""><code>❌ ERROR  | 2024-02-06 11:07:29 | autotrain.trainers.common:wrapper:91 - train has failed due to an exception: Traceback (most recent call last):
  File &quot;/app/src/autotrain/trainers/common.py&quot;, line 88, in wrapper
    return func(*args, **kwargs)
  File &quot;/app/src/autotrain/trainers/clm/__main__.py&quot;, line 186, in train
    model = AutoModelForCausalLM.from_pretrained(
  File &quot;/app/env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py&quot;, line 566, in from_pretrained
    return model_class.from_pretrained(
  File &quot;/app/env/lib/python3.10/site-packages/transformers/modeling_utils.py&quot;, line 3032, in from_pretrained
    raise RuntimeError(&quot;No GPU found. A GPU is needed for quantization.&quot;)
RuntimeError: No GPU found. A GPU is needed for quantization.

❌ ERROR  | 2024-02-06 11:07:29 | autotrain.trainers.common:wrapper:92 - No GPU found. A GPU is needed for quantization.
</code></pre>
<p>Hugging Face auto train local in run docker and my gpu rtx4050 but auto train you not gpu error why?</p>
","huggingface"
"77945890","Use locally downloaded Hugging Face model from langchain4j","2024-02-06 07:22:18","","0","226","<model><embedding><huggingface><langchain4j>","<p>I have successfully downloaded following Hugging Face sentence similarity model to local directory:</p>
<p><a href=""https://huggingface.co/kinit/slovakbert-sts-stsb"" rel=""nofollow noreferrer"">https://huggingface.co/kinit/slovakbert-sts-stsb</a></p>
<p>Structure of the model is:</p>
<pre><code>config.json
config_sentence_transformers.json
merges.txt
modules.json
pytorch_model.bin
README.md
sentence_bert_config.json
special_tokens_map.json
tokenizer.json
tokenizer_config.json
vocab.json
</code></pre>
<p>Apparently the model itself is in file: pytorch_model.bin</p>
<p>I am using langchain4j. I have found following extension for Hugging Face:</p>
<p><a href=""https://github.com/langchain4j/langchain4j/tree/main/langchain4j-hugging-face"" rel=""nofollow noreferrer"">https://github.com/langchain4j/langchain4j/tree/main/langchain4j-hugging-face</a></p>
<p>I am little bit confused as many methods are using Hugging Face api key??</p>
<p>My question is, how to connect my locally downloaded model with my langchain4j code, allowing me to write statements like:</p>
<pre><code>EmbeddingModel huggingFaceModel = new EmbeddingModel()
</code></pre>
<p>Can you help me?</p>
","huggingface"
"77945471","how to set the ans length using huggingface","2024-02-06 05:36:52","","0","22","<large-language-model><huggingface><openaiembeddings>","<p>I am using hugging face for questions and answers from the pdf. How to set the limit of answers length?</p>
<pre class=""lang-py prettyprint-override""><code>tytokenizer = AutoTokenizer.from_pretrained(&quot;HuggingFaceH4/zephyr-7b-beta&quot;, eos_token_id =['Question:'])
model = AutoModelForCausalLM.from_pretrained(&quot;HuggingFaceH4/zephyr-7b-beta&quot;, low_cpu_mem_usage=True, torch_dtype=torch.float16, load_in_8bit=True )
pipe = pipeline(task=&quot;text-generation&quot;, model=model,tokenizer=tokenizer, max_new_tokens=50)
</code></pre>
","huggingface"
"77945126","Seeking Recommendations for Retrieval-Augmented Generation (RAG) Tools for CSV Data Analysis","2024-02-06 03:35:47","","1","615","<data-cleaning><large-language-model><huggingface>","<p>After spending significant time on data engineering tasks, I'm looking for RAG tools or similar technologies that support CSV files. My project involves extracting information on specific conditions from over 10 years of hospital data in CSV format. Most tools I've found cater to PDFs. Does anyone know of any tools that could streamline this process for large CSV datasets?</p>
<p>I plan to prepare the CSV first, then use RAG to extract some data regarding some specific conditions. Then I will re-organize the output from RAG to CSV again. (I am also not sure if this is a good idea, so please correct me if I'm wrong)</p>
<p>I have tried RAG from Hugging Face, but it seems that it can work on PDF only (correct me if I'm wrong please). can anyone suggest some RAG for CSV files, so I can reduce my work on data engineering?</p>
","huggingface"
"77940275","ClassLabel disappear after loading DatasetDict (Hugging Face)","2024-02-05 10:37:21","","0","94","<python><dataset><huggingface><huggingface-datasets><huggingface-hub>","<p>I have a DatasetDict containing 10 splits (‘fold_0’ to ‘fold_9’). All the Dataset objects included in the DatasetDict contain 2 features: “label” &amp; “text”. Here’s a small overview:</p>
<pre class=""lang-py prettyprint-override""><code>print(my_dataset_dict)
&gt;&gt;&gt; DatasetDict({
        fold_0: Dataset({
            features: ['label', 'text'],
            num_rows: 85087
        })
        fold_1: Dataset({
            features: ['label', 'text'],
            num_rows: 85076
        })
    ....
        fold_9: Dataset({
            features: ['label', 'text'],
            num_rows: 85159
        })
    })
</code></pre>
<p>For each Dataset, the “label” column was encoded with ClassLabel, and the “text” column is just a bunch of sentences:</p>
<pre class=""lang-py prettyprint-override""><code>print(my_dataset_dict['fold_0'].features)
&gt;&gt;&gt; {'label': ClassLabel(names=['MA211', 'MA221', ..., 'V39'], id=None), 'text': Value(dtype='string', id=None)}
</code></pre>
<p>So far so good, it’s exactly what I’m expecting.
However, if I push it to the Hub and then load it again (in another script or in the same one, it doesn’t matter), then the labels disappear:</p>
<pre class=""lang-py prettyprint-override""><code>huggingface_hub.delete_repo(repo_id=dataset_path, repo_type='dataset', missing_ok=True)  # Just to be sure the previous DatasetDict is removed first

my_dataset_dict.push_to_hub(dataset_path)  # No problem, I see it on the Hub after that (and the real labels appear)

test_dataset_dict = datasets.load_dataset(dataset_path)  # Reloading it from the same path

print(test_dataset_dict['fold_0'].features)
&gt;&gt;&gt; {'label': Value(dtype='string', id=None),
     'text': Value(dtype='string', id=None)}
</code></pre>
<p>As you can see, I don’t have the labels anymore. It’s a problem for me because I need to “cure the data” and create the dataset in a specific notebook, and then load back the data and perform some ML tasks on another notebook, and I’m losing the real labels.
I tried loading using test_dataset_dict = datasets.load_dataset(dataset_path, download_mode=datasets.downloadMode.FORCE_REDOWNLOAD) but it doesn’t change anything. The text and the labels (just the integers) are loaded, but I don’t have the names of the labels. The names of the labels are pushed to the Hub, because I can see them on the viewer under the label column (I see the integer and the associated code right next to it):
<a href=""https://i.sstatic.net/biKtm.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/biKtm.png"" alt=""enter image description here"" /></a></p>
<p>Thanks for your help!</p>
","huggingface"
"77935055","Runtime error Launch timed out, workload was not healthy after 30 min. Getting this error on huggingface. Any suggestions?","2024-02-04 08:32:21","","0","121","<huggingface>","<p>Runtime error
Launch timed out, workload was not healthy after 30 min. I am trying to deploy a model on Huggingface and getting this error. Any help will be appreciated</p>
<p>Manual upload model was what i did. Model works perfectly in local.</p>
","huggingface"
"77931126","How to calculate language model's perplexity for text that exceeds memory?","2024-02-03 06:49:04","","0","284","<pytorch><nlp><huggingface-transformers><huggingface><perplexity>","<p>I have a long list of texts, each with 512 tokens max:</p>
<pre><code>texts = [&quot;some fairly long text&quot;, &quot;some fairly long text2&quot;,  ... ]
</code></pre>
<p>And a model</p>
<pre><code>from transformers import GPT2LMHeadModel, GPT2TokenizerFast

device = &quot;cuda&quot;
model_id = &quot;gpt2-large&quot;
model = GPT2LMHeadModel.from_pretrained(model_id).to(device)
tokenizer = GPT2TokenizerFast.from_pretrained(model_id)
</code></pre>
<p>How can I calculate the average perplexity of the model over all texts?</p>
<p>The first approach I tried (<a href=""https://stackoverflow.com/questions/75886674/how-to-compute-sentence-level-perplexity-from-hugging-face-language-models"">from this SO</a>) was using <code>evaluate</code>:</p>
<pre><code>perplexity = evaluate.load(&quot;perplexity&quot;, module_type=&quot;metric&quot;)
input_texts = [&quot;lorem ipsum&quot;, &quot;Happy Birthday!&quot;, &quot;Bienvenue&quot;]

results = perplexity.compute(model_id='gpt2',
                             add_start_token=False,
                             predictions=input_texts)
print(list(results.keys()))
&gt;&gt;&gt;['perplexities', 'mean_perplexity']
print(round(results[&quot;mean_perplexity&quot;], 2))
</code></pre>
<p>But this runs out of memory for larger models (e.g., Llama 7B) on my machine. I even tried using <code>batch_size=1</code>, but that didn't seem to make a difference for some reason.</p>
<p>The other approach was using this <a href=""https://huggingface.co/docs/transformers/perplexity"" rel=""nofollow noreferrer"">huggingface tutorial</a>, but they concatenate all the texts into 1 long text and use a sliding window. While this somehow fits my memory (I don't understand why this works but batch of size 1 didn't), the result is not as optimal as doing it per text, which is more realistic.</p>
<p>Is there a better method that is less computationally heavy than the <code>evaluate</code> method and that will calculate the perplexity per sentence rather than with a sliding window?</p>
<p><strong>Update:</strong></p>
<p>I tried the following, but got different results than the <code>evaluate.compute</code> method, so I'm not sure what I'm doing wrong:</p>
<pre><code>import torch
from transformers import GPT2LMHeadModel, GPT2TokenizerFast
# Load the model and tokenizer
device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;
model_id = &quot;gpt2-large&quot;
model = GPT2LMHeadModel.from_pretrained(model_id).to(device)
tokenizer = GPT2TokenizerFast.from_pretrained(model_id)

# List of texts
texts = [&quot;some fairly long text&quot;, &quot;some fairly long text2&quot;, ...]

# Function to calculate perplexity
def calculate_perplexity(text):
    tokenized_text = tokenizer(text, return_tensors=&quot;pt&quot;, truncation=True, max_length=512).to(device)
    input_ids = tokenized_text.input_ids
    attention_mask = tokenized_text.attention_mask

    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)
        logits = outputs.logits

    # Flatten the logits and input_ids
    logits = logits.view(-1, logits.shape[-1])
    input_ids = input_ids.view(-1)

    # Compute the cross-entropy loss
    loss = torch.nn.functional.cross_entropy(logits, input_ids, reduction='none')
    perplexity = torch.exp(torch.mean(loss))

    return perplexity.item()

# Calculate perplexity for each text
perplexities = [calculate_perplexity(text) for text in texts]

# Calculate average perplexity
average_perplexity = sum(perplexities) / len(perplexities)

print(&quot;Average Perplexity:&quot;, average_perplexity)
</code></pre>
","huggingface"
"77925968","Using hugging face model in Java SpringBoot- Java Deep Library","2024-02-02 09:37:30","","0","552","<java><spring-boot><maven><huggingface>","<p>I have a Java SpringBoot Maven application. I want to integrate the hugging face model (BAAI bg-reranker-large) in my Java code. However, Hugging Face do not offer support for Java. I have seen a couple of recommendation to use ONNX and Java Deep Library. I felt that ONNX was not suitable for me as it requires the use of some Python code first before using Java.</p>
<p>I want to use Java Deep Library but I am very confused on how it works and how to translate the model from Python to Java. Is there a way to do it and how can I achieve that? If there are other ways, please suggest!</p>
","huggingface"
"77924463","when decode a series of tokens from stream inference, how to avoid partial token?","2024-02-02 03:04:31","","0","37","<large-language-model><huggingface><triton>","<p>I want to implement a LLM inference server which holds a collection of huggingface models, but for stream inference, which return a token at a time. then token which returns may not enough to decode to a readable word. So what should I do to achieve such goal: only returns when token can be decode to a readable word?</p>
","huggingface"
"77919591","Trainer acts as if it's training from scratch","2024-02-01 10:58:33","","0","53","<python><machine-learning><huggingface><huggingface-trainer>","<p>I'm training a model with a huggingface trainer and I specified the checkpoint folder for the <code>resume_from_checkpoint</code> parameter.
However, when it continues to train, it still saves the checkpoints with the names corresponding to the first save steps (e.g. <code>checkpoint-4</code> even though <code>resume_from_checkpoint</code> should start from <code>checkpoint-4096</code>). The progress bar shows all the <code>max_steps</code> as well, even though I don't want it to start from the beginning.</p>
<p>Is this a common problem? How do I fix this?</p>
<p>I save my training arguments in a yaml file:</p>
<pre><code>training_args:
   learning_rate: !!float 1e-4
   do_train: true
   per_device_train_batch_size: 8
   per_device_eval_batch_size: 8
   logging_steps: 1024
   output_dir: /path/to/training_output/
   overwrite_output_dir: False
   remove_unused_columns: False
   save_strategy: steps
   evaluation_strategy: steps
   save_steps: 1024
   load_best_model_at_end: True
   warmup_steps: 100
   max_steps: 65536
   seed: 22
   resume_from_checkpoint: /path/to/checkpoint-4096
</code></pre>
<p>and then train the model by initialising a TrainingArguments object with these as **kwargs.</p>
<p>But the terminal displays:</p>
<pre><code>Saving model checkpoint to /path/to/checkpoint-4
</code></pre>
<p>And the progress bar shows all the steps, even though I need it to start from step 4096.</p>
","huggingface"
"77919484","How to generate only one answer in huggingfacepipeline?","2024-02-01 10:41:11","","0","133","<pytorch><langchain><large-language-model><huggingface><solar>","<p>When I ask a question through the HuggingFace pipeline and langchain, I want only one answer, but it keeps generating multiple questions and answers automatically. Is there a solution?</p>
<p>This is pipeline code:
<a href=""https://i.sstatic.net/NZaJk.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>This is langchain Base code:
<a href=""https://i.sstatic.net/uRlHD.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>This is answering image:
<a href=""https://i.sstatic.net/UHOiM.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>I try to change huggingface parameter</p>
","huggingface"
"77917346","HuggingFace: Loading checkpoint shards taking too long","2024-02-01 01:58:10","","0","2203","<huggingface-transformers><large-language-model><huggingface><huggingface-tokenizers><llama>","<p>Hi guys I am running the following code:</p>
<pre><code>model = AutoModelForCausalLM.from_pretrained('./cache/model')

tokenizer = AutoTokenizer.from_pretrained('./cache/model')
</code></pre>
<p>where I have cached a hugging face model using cache_dir within the from_pretraind() method. However, everytime I load the model it requires to load the checkpoint shards which takes 7-10 minutes for each inference.</p>
<blockquote>
<p>Loading checkpoint shards:  67%|######6   | 2/3 [06:17&lt;03:08, 188.79s/it]</p>
</blockquote>
<p>This is taking so long even though I am loading the model locally where it is already installed?</p>
<p>I am using some powerful GPUs so my actual inference is just a few seconds but the time it takes to load the model into memory is so long.</p>
<p>Is there any way around this? I saw someone on a similar thread say they used safe_serialization when using the save_pretrained() method but my issue is I am loading a pretrained model and not fine-tuning and saving my own. Hence, I am unsure how to apply this plausible solution.</p>
<p>Any help here would be great.</p>
","huggingface"
"77916908","Installing triton in windows","2024-01-31 23:12:41","","0","6098","<python><windows><huggingface><triton>","<p>I am running the following on command line in windows:</p>
<pre><code> autotrain dreambooth --model stabilityai/stable-diffusion-xl-base-1.0 --image-path ./Anas --prompt 'a photo of arzk man' --resolution 1024 --batch-size 1 --num-steps 500 --fp16 --gradient-accumulation 4 --lr 1e-4 --project-name thesis
</code></pre>
<p>and I got the following error message</p>
<pre><code>RuntimeError: Failed to import diffusers.pipelines.stable_diffusion_xl.pipeline_stable_diffusion_xl because of the following error (look up to see its traceback):
Failed to import diffusers.models.autoencoder_kl because of the following error (look up to see its traceback):
module 'triton' has no attribute '__version__
</code></pre>
<p>I tried to pip install triton but I am getting the following error message</p>
<pre><code>ERROR: Could not find a version that satisfies the requirement triton (from versions: none)
ERROR: No matching distribution found for triton
</code></pre>
<p>I read in triton's <a href=""https://github.com/openai/triton?tab=readme-ov-file#compatibility"" rel=""nofollow noreferrer"">github</a> that it is only compatible with Linux.</p>
<p>I don't know how to use Linux. would i need to create a virtual environment there and install python and... that would be crazy to move everything there (probably it is not I just dont know what to do in linux).</p>
<p>any recommendation to what I could do ?</p>
","huggingface"
"77909589","ValueError: You cannot perform fine-tuning on purely quantized models","2024-01-30 21:44:32","","2","972","<large-language-model><huggingface>","<p>When trying to finetune a falcon 7b model on some training data of mine I am getting the following error:</p>
<pre><code>ValueError: You cannot perform fine-tuning on purely quantized models. Please attach trainable adapters on top of the quantized model to correctly perform fine-tuning. Please see: https://huggingface.co/docs/transformers/peft for more details
</code></pre>
<p>For training I am trying to use Nvidia NDV2. Does anyone know how to resolve this?</p>
<p>I actually already created the Loraconfig and adding it to the model.</p>
<pre><code>config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=[&quot;query_key_value&quot;],
    lora_dropout=0.05,
    bias=&quot;none&quot;,
    task_type=&quot;CAUSAL_LM&quot;
)

model = get_peft_model(model, config)
print_trainable_parameters(model)```
</code></pre>
","huggingface"
"77905301","Use HuggingFace models locally","2024-01-30 09:41:40","","0","372","<python><huggingface-transformers><huggingface>","<p>I would like to use transformers especially HuggingFace Models as a part of my programming</p>
<p>my question is; Can I use and implement transformers and HuggingFace Models offline and in Spyder IDE (or any other IDE that I can use locally? (Of course, after downloading and installing all needed packages).</p>
<p>Thanks in advance.</p>
","huggingface"
"77903671","Fine tuning BERT(for phone and card) not accurate","2024-01-30 02:50:33","","0","32","<nlp><bert-language-model><named-entity-recognition><huggingface>","<p>This is my code for FT BERT for credit card and Phone number identification. I have a dataset that has sentences and each word has its tag(o, PHN, CRD). I am not able to get the required results. The model is not accurate, it identifies even dob as credit card sometimes. and not identify phone number/Credit card if spaces are in them.</p>
<p>I am very new to this, is there any way i can improve BERT model so that it can identify more accurately.</p>
<p>This is my code:</p>
<pre class=""lang-py prettyprint-override""><code># !pip install transformers seqeval[gpu]
from transformers import pipeline
from seqeval.metrics import classification_report

import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertConfig, BertForTokenClassification
from torch import cuda
device = 'cuda' if cuda.is_available() else 'cpu'


file = &quot;/home/dev/bert_trans/specific_testing_v2_re.csv&quot;

data = pd.read_csv(file, encoding='unicode_escape')

frequencies = data.Tag.value_counts()


tags = {}
for tag, count in zip(frequencies.index, frequencies):
    if tag != &quot;O&quot;:
        if tag[0:5] not in tags.keys():
            tags[tag[0:5]] = count
        else:
            tags[tag[0:5]] += count
    continue

# pandas has a very handy &quot;forward fill&quot; function to fill missing values based on the last upper non-nan value
data = data.fillna(method='ffill')


# let's create a new column called &quot;sentence&quot; which groups the words by sentence
data['sentence'] = data[['Sentence #','Word','Tag']].groupby(['Sentence #'])['Word'].transform(lambda x: ' '.join(x))
# let's also create a new column called &quot;word_labels&quot; which groups the tags by sentence
data['word_labels'] = data[['Sentence #','Word','Tag']].groupby(['Sentence #'])['Tag'].transform(lambda x: ','.join(x))
data.head()

label2id = {k: v for v, k in enumerate(data.Tag.unique())}
id2label = {v: k for v, k in enumerate(data.Tag.unique())}

data = data[[&quot;sentence&quot;, &quot;word_labels&quot;]].drop_duplicates().reset_index(drop=True)


MAX_LEN = 128
TRAIN_BATCH_SIZE = 4
VALID_BATCH_SIZE = 2
EPOCHS = 1
LEARNING_RATE = 1e-05
MAX_GRAD_NORM = 10
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')


def tokenize_and_preserve_labels(sentence, text_labels, tokenizer):
    &quot;&quot;&quot;
    Word piece tokenization makes it difficult to match word labels
    back up with individual word pieces. This function tokenizes each
    word one at a time so that it is easier to preserve the correct
    label for each subword. It is, of course, a bit slower in processing
    time, but it will help our model achieve higher accuracy.
    &quot;&quot;&quot;

    tokenized_sentence = []
    labels = []

    sentence = sentence.strip()

    for word, label in zip(sentence.split(), text_labels.split(&quot;,&quot;)):

        # Tokenize the word and count # of subwords the word is broken into
        tokenized_word = tokenizer.tokenize(word)
        n_subwords = len(tokenized_word)

        # Add the tokenized word to the final tokenized word list
        tokenized_sentence.extend(tokenized_word)

        # Add the same label to the new list of labels `n_subwords` times
        labels.extend([label] * n_subwords)

    return tokenized_sentence, labels


class dataset(Dataset):
    def __init__(self, dataframe, tokenizer, max_len):
        self.len = len(dataframe)
        self.data = dataframe
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __getitem__(self, index):
        # step 1: tokenize (and adapt corresponding labels)
        sentence = self.data.sentence[index]
        word_labels = self.data.word_labels[index]
        tokenized_sentence, labels = tokenize_and_preserve_labels(sentence, word_labels, self.tokenizer)

        # step 2: add special tokens (and corresponding labels)
        tokenized_sentence = [&quot;[CLS]&quot;] + tokenized_sentence + [&quot;[SEP]&quot;]  # add special tokens
        labels.insert(0, &quot;o&quot;)  # add outside label for [CLS] token
        labels.insert(-1, &quot;o&quot;)  # add outside label for [SEP] token

        # step 3: truncating/padding
        maxlen = self.max_len

        if (len(tokenized_sentence) &gt; maxlen):
            # truncate
            tokenized_sentence = tokenized_sentence[:maxlen]
            labels = labels[:maxlen]
        else:
            # pad
            tokenized_sentence = tokenized_sentence + ['[PAD]' for _ in range(maxlen - len(tokenized_sentence))]
            labels = labels + [&quot;o&quot; for _ in range(maxlen - len(labels))]

        # step 4: obtain the attention mask
        attn_mask = [1 if tok != '[PAD]' else 0 for tok in tokenized_sentence]

        # step 5: convert tokens to input ids
        ids = self.tokenizer.convert_tokens_to_ids(tokenized_sentence)

        label_ids = [label2id[label] for label in labels]
        # the following line is deprecated
        # label_ids = [label if label != 0 else -100 for label in label_ids]

        return {
            'ids': torch.tensor(ids, dtype=torch.long),
            'mask': torch.tensor(attn_mask, dtype=torch.long),
            # 'token_type_ids': torch.tensor(token_ids, dtype=torch.long),
            'targets': torch.tensor(label_ids, dtype=torch.long)
        }

    def __len__(self):
        return self.len

train_size = 0.8
train_dataset = data.sample(frac=train_size, random_state=200)
test_dataset = data.drop(train_dataset.index).reset_index(drop=True)
train_dataset = train_dataset.reset_index(drop=True)

training_set = dataset(train_dataset, tokenizer, MAX_LEN)
testing_set = dataset(test_dataset, tokenizer, MAX_LEN)

# print the first 30 tokens and corresponding labels
for token, label in zip(tokenizer.convert_ids_to_tokens(training_set[0][&quot;ids&quot;][:30]),
                        training_set[0][&quot;targets&quot;][:30]):
    print('{0:10}  {1}'.format(token, id2label[label.item()]))


train_params = {'batch_size': TRAIN_BATCH_SIZE,
                'shuffle': True,
                'num_workers': 0
                }

test_params = {'batch_size': VALID_BATCH_SIZE,
                'shuffle': True,
                'num_workers': 0
                }

training_loader = DataLoader(training_set, **train_params)
testing_loader = DataLoader(testing_set, **test_params)

model = BertForTokenClassification.from_pretrained('bert-base-uncased',
                                                   num_labels=len(id2label),
                                                   id2label=id2label,
                                                   label2id=label2id)
model.to(device)


ids = training_set[0][&quot;ids&quot;].unsqueeze(0)
mask = training_set[0][&quot;mask&quot;].unsqueeze(0)
targets = training_set[0][&quot;targets&quot;].unsqueeze(0)
ids = ids.to(device)
mask = mask.to(device)
targets = targets.to(device)
outputs = model(input_ids=ids, attention_mask=mask, labels=targets)
initial_loss = outputs[0]
tr_logits = outputs[1]


optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)


# Defining the training function on the 80% of the dataset for tuning the bert model
def train(epoch):
    tr_loss, tr_accuracy = 0, 0
    nb_tr_examples, nb_tr_steps = 0, 0
    tr_preds, tr_labels = [], []
    # put model in training mode
    model.train()

    for idx, batch in enumerate(training_loader):

        ids = batch['ids'].to(device, dtype=torch.long)
        mask = batch['mask'].to(device, dtype=torch.long)
        targets = batch['targets'].to(device, dtype=torch.long)

        outputs = model(input_ids=ids, attention_mask=mask, labels=targets)
        loss, tr_logits = outputs.loss, outputs.logits
        tr_loss += loss.item()

        nb_tr_steps += 1
        nb_tr_examples += targets.size(0)

        if idx % 100 == 0:
            loss_step = tr_loss / nb_tr_steps
            print(f&quot;Training loss per 100 training steps: {loss_step}&quot;)

        # compute training accuracy
        flattened_targets = targets.view(-1)  # shape (batch_size * seq_len,)
        active_logits = tr_logits.view(-1, model.num_labels)  # shape (batch_size * seq_len, num_labels)
        flattened_predictions = torch.argmax(active_logits, axis=1)  # shape (batch_size * seq_len,)
        # now, use mask to determine where we should compare predictions with targets (includes [CLS] and [SEP] token predictions)
        active_accuracy = mask.view(-1) == 1  # active accuracy is also of shape (batch_size * seq_len,)
        targets = torch.masked_select(flattened_targets, active_accuracy)
        predictions = torch.masked_select(flattened_predictions, active_accuracy)

        tr_preds.extend(predictions)
        tr_labels.extend(targets)

        tmp_tr_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())
        tr_accuracy += tmp_tr_accuracy

        # gradient clipping
        torch.nn.utils.clip_grad_norm_(
            parameters=model.parameters(), max_norm=MAX_GRAD_NORM
        )

        # backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    epoch_loss = tr_loss / nb_tr_steps
    tr_accuracy = tr_accuracy / nb_tr_steps
    print(f&quot;Training loss epoch: {epoch_loss}&quot;)
    print(f&quot;Training accuracy epoch: {tr_accuracy}&quot;)

for epoch in range(EPOCHS):
    print(f&quot;Training epoch: {epoch + 1}&quot;)
    train(epoch)


def valid(model, testing_loader):
    # put model in evaluation mode
    model.eval()

    eval_loss, eval_accuracy = 0, 0
    nb_eval_examples, nb_eval_steps = 0, 0
    eval_preds, eval_labels = [], []

    with torch.no_grad():
        for idx, batch in enumerate(testing_loader):

            ids = batch['ids'].to(device, dtype=torch.long)
            mask = batch['mask'].to(device, dtype=torch.long)
            targets = batch['targets'].to(device, dtype=torch.long)

            outputs = model(input_ids=ids, attention_mask=mask, labels=targets)
            loss, eval_logits = outputs.loss, outputs.logits

            eval_loss += loss.item()

            nb_eval_steps += 1
            nb_eval_examples += targets.size(0)

            if idx % 100 == 0:
                loss_step = eval_loss / nb_eval_steps
                print(f&quot;Validation loss per 100 evaluation steps: {loss_step}&quot;)

            # compute evaluation accuracy
            flattened_targets = targets.view(-1)  # shape (batch_size * seq_len,)
            active_logits = eval_logits.view(-1, model.num_labels)  # shape (batch_size * seq_len, num_labels)
            flattened_predictions = torch.argmax(active_logits, axis=1)  # shape (batch_size * seq_len,)
            # now, use mask to determine where we should compare predictions with targets (includes [CLS] and [SEP] token predictions)
            active_accuracy = mask.view(-1) == 1  # active accuracy is also of shape (batch_size * seq_len,)
            targets = torch.masked_select(flattened_targets, active_accuracy)
            predictions = torch.masked_select(flattened_predictions, active_accuracy)

            eval_labels.extend(targets)
            eval_preds.extend(predictions)

            tmp_eval_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())
            eval_accuracy += tmp_eval_accuracy

    # print(eval_labels)
    # print(eval_preds)

    labels = [id2label[id.item()] for id in eval_labels]
    predictions = [id2label[id.item()] for id in eval_preds]

    # print(labels)
    # print(predictions)

    eval_loss = eval_loss / nb_eval_steps
    eval_accuracy = eval_accuracy / nb_eval_steps
    print(f&quot;Validation Loss: {eval_loss}&quot;)
    print(f&quot;Validation Accuracy: {eval_accuracy}&quot;)

    return labels, predictions

labels, predictions = valid(model, testing_loader)

print(classification_report([labels], [predictions]))

sentence = &quot;John got his new phone number which is +1(212)-313-5467.&quot;

inputs = tokenizer(sentence, padding='max_length', truncation=True, max_length=MAX_LEN, return_tensors=&quot;pt&quot;)

# move to gpu
ids = inputs[&quot;input_ids&quot;].to(device)
mask = inputs[&quot;attention_mask&quot;].to(device)
# forward pass
outputs = model(ids, mask)
logits = outputs[0]

active_logits = logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)
flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size*seq_len,) - predictions at the token level

tokens = tokenizer.convert_ids_to_tokens(ids.squeeze().tolist())
token_predictions = [id2label[i] for i in flattened_predictions.cpu().numpy()]
wp_preds = list(zip(tokens, token_predictions)) # list of tuples. Each tuple = (wordpiece, prediction)

word_level_predictions = []
for pair in wp_preds:
  if (pair[0].startswith(&quot; ##&quot;)) or (pair[0] in ['[CLS]', '[SEP]', '[PAD]']):
    # skip prediction
    continue
  else:
    word_level_predictions.append(pair[1])

# we join tokens, if they are not special ones
str_rep = &quot; &quot;.join([t[0] for t in wp_preds if t[0] not in ['[CLS]', '[SEP]', '[PAD]']]).replace(&quot; ##&quot;, &quot;&quot;)
print(str_rep)
print(word_level_predictions)


pipe = pipeline(task=&quot;token-classification&quot;, model=model.to(&quot;cpu&quot;), tokenizer=tokenizer, aggregation_strategy=&quot;simple&quot;)
pipe(&quot;my date of birth is 23-09-2023, Jakes's phone numbers are +1 (414)-123 6512, and +1 (662) 234 0010&quot;)
model.save_pretrained()
</code></pre>
<p>My dataset consists of 10,000 sentences covering all the formats of phone numbers and credit cards, for eg, it is in such a way:</p>
<pre><code>Sentence: 1,Sure,o
,&quot;,&quot;,o
,``,o
,I,o
,went,o
,shopping,o
,yesterday,o
,&quot;,&quot;,o
,and,o
,at,o
,the,o
,checkout,o
,&quot;,&quot;,o
,I,o
,paid,o
,using,o
,my,o
,credit,o
,card,o
,:,o
,4755 8555 6999 2555,CRD
,'',o
,.,o
</code></pre>
<p>Would appreciate if someone helps.</p>
","huggingface"
"77899122","Does resume_from_checkpoint also make the trainer not go through the same data?","2024-01-29 10:42:37","","0","52","<python><huggingface><huggingface-trainer>","<p>Would <code>resume_from_checkpoint={path_to_checkpoint}</code> just make the trainer go through the data again, regardless of whether it should be a new epoch? Or would it start from the point in the data at which it previously stopped?</p>
<p>I want to change the <code>save_steps</code> parameter in <code>TrainingArguments</code> in the middle of training and so I decided to stop the trainer and then let it run with the new <code>save_steps</code>. But I want to know whether the trainer also saves the information about which part of the training data it has already trained on.</p>
","huggingface"
"77898307","Decoder only architecture to generate embedding vectors","2024-01-29 08:29:50","","2","373","<pytorch><huggingface-transformers><large-language-model><huggingface><encoder-decoder>","<p>Im currently using models like RoBERTa, CodeBERT etc for &quot;code author identification/ code detection&quot; (you can imagine it like facial recognition task). I know they are encoder architectures.</p>
<p>I use these encoders and train a siamese network with pairs of data and use contrastive loss to maximise the distance between the non similar pairs and decrease the distance/loss for similar pairs and then finally use the fine-tuned encoder to generate embedding vectors for unseen code and use these vectors to assign an author by comparing with author embeddings generated with set of author samples of finetuned model. It works similar to the face detection task.</p>
<p>Since my project is mainly focused on embeddings I am not sure how decoder can be used for this as decoder generally is used for generating an output sequence rather than encoding an input.I want to know</p>
<ul>
<li>If a decoder architectures like say &quot;mistral&quot; or similar llm's can be used to generate embeddings for my task? If so, can anyone guide me on how to achieve this as I don;t understand how to use a decoder for this purpose.</li>
</ul>
<p>PS: I only read about the decoder architectures and got more confused, so seeking some support here.</p>
","huggingface"
"77894176","How to load quantized LLM to CPU only device?","2024-01-28 09:05:47","","0","578","<large-language-model><huggingface><gpt-3>","<p>I have this code to quantize a large language model and save the quantized model:</p>
<pre><code>import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

model_name = 'stabilityai/stablelm-2-zephyr-1_6b'

def load_quantized_model(model_name: str):
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type=&quot;nf4&quot;,
        bnb_4bit_compute_dtype=torch.bfloat16
    )
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        load_in_4bit=True,
        torch_dtype=torch.bfloat16,
        quantization_config=bnb_config,
        trust_remote_code=True
    )
    return model

def initialize_tokenizer(model_name: str):
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        trust_remote_code=True
    )
    tokenizer.bos_token_id = 1  # Set beginning of sentence token id
    return tokenizer

model = load_quantized_model(model_name)
tokenizer = initialize_tokenizer(model_name)

SAVED_MODEL_NAME = 'quantized'
model.save_pretrained(SAVED_MODEL_NAME)
</code></pre>
<p>Now, I have a 1.2GB (from ~4gb) model inside the directory gpt-custom.</p>
<p>So I download it in my laptop with CPU only and this is my code:</p>
<pre><code>import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

model_name = 'stabilityai/stablelm-2-zephyr-1_6b'

model = AutoModelForCausalLM.from_pretrained(
    'quantized',
    device_map=&quot;auto&quot;,
    trust_remote_code=True
)

def initialize_tokenizer(model_name: str):
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        trust_remote_code=True
    )
    tokenizer.bos_token_id = 1  # Set beginning of sentence token id
    return tokenizer

tokenizer = initialize_tokenizer(model_name)

question = 'how are you feeling?'
prompt = [{'role': 'user', 'content': question}]
inputs = tokenizer.apply_chat_template(
    prompt,
    add_generation_prompt=True,
    return_tensors='pt'
)
tokens = model.generate(
    inputs.to(model.device),
    max_new_tokens=64,
    temperature=0.5,
    do_sample=True
)
print(tokenizer.decode(tokens[0], skip_special_tokens=True))
</code></pre>
<p>But I am getting an error <code>NameError: name 'torch' is not defined</code> but I already installed torch using <code>pip install torch</code> and even tried <code>pip install --upgrade torch</code>. There was also this warning before the error:</p>
<pre><code>Detected the presence of a `quantization_config` attribute in the model's configuration but you don't have the correct `bitsandbytes` version to support 4 and 8 bit serialization. Please install the latest version of `bitsandbytes` with  `pip install --upgrade bitsandbytes`.
</code></pre>
<p>And I have also tried <code>pip install --upgrade bitsandbytes</code> but still get the same error.</p>
<p>This is the full error stack:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/var/www/html/test_llm/test.py&quot;, line 6, in &lt;module&gt;
    model = AutoModelForCausalLM.from_pretrained(
  File &quot;/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py&quot;, line 562, in from_pretrained
    return model_class.from_pretrained(
  File &quot;/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py&quot;, line 3856, in from_pretrained
    ) = cls._load_pretrained_model(
  File &quot;/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py&quot;, line 4290, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File &quot;/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py&quot;, line 839, in _load_state_dict_into_meta_model
    set_module_quantized_tensor_to_device(model, param_name, param_device, value=param)
  File &quot;/usr/local/lib/python3.10/dist-packages/transformers/integrations/bitsandbytes.py&quot;, line 58, in set_module_quantized_tensor_to_device
    if old_value.device == torch.device(&quot;meta&quot;) and device not in [&quot;meta&quot;, torch.device(&quot;meta&quot;)] and value is None:
NameError: name 'torch' is not defined
</code></pre>
","huggingface"
"77892848","I keep getting ""index out of range in self"" during forward pass","2024-01-27 21:16:16","","0","25","<machine-learning><pytorch><nlp><huggingface-transformers><huggingface>","<p>I am fine tuning a Longformer Encoder Decoder model for multi document text summarization. When I try to run through the forward pass, it gives me an error &quot;index out of range in self&quot;. The input shape seems to be correct, but the debugger points to something in torch Embedding going wrong. How do I fix this?</p>
<pre><code>num_epochs = 8
num_training_steps = num_epochs * len(train_dataloader)
optimizer = Adam(MODEL.parameters(), lr=3e-5)
lr_scheduler = get_scheduler(
    name=&quot;linear&quot;, optimizer=optimizer, num_warmup_steps=1, num_training_steps=num_training_steps # CHANGE LATER!!!!!!!
)
progress_bar = tqdm(range(num_training_steps))

# Training mode
MODEL.train()

for epoch in range(num_epochs):
  for batch_idx, batch in enumerate(train_dataloader):

    # Encode data
    input_ids_all = []
    for cluster in batch[&quot;document&quot;]:
      articles = cluster.split(&quot;|||||&quot;)[:-1]
      for i, article in enumerate(articles):
        article = article.replace(&quot;\n&quot;, &quot; &quot;)
        article = &quot; &quot;.join(article.split())
        articles[i] = article
      input_ids = []
      for article in articles:
        input_ids.extend(TOKENIZER.encode(article, truncation=True, max_length=4096 // len(articles))[1:-1])
        input_ids.append(DOCSEP_TOKEN_ID)
      input_ids = ([TOKENIZER.bos_token_id]+input_ids+[TOKENIZER.eos_token_id])
      input_ids_all.append(torch.tensor(input_ids))
      input_ids = torch.nn.utils.rnn.pad_sequence(input_ids_all, batch_first=True, padding_value=PAD_TOKEN_ID)

    # Forward pass
    global_attention_mask = torch.zeros_like(input_ids)
    global_attention_mask[:, 0] = 1
    global_attention_mask[input_ids == DOCSEP_TOKEN_ID] = 1

    print(input_ids.shape)
    # outputs = MODEL.forward(input_ids) # &lt;---------------------------------------------------------------------------------------------- causing a bug
    outputs = MODEL.forward(input_ids=input_ids_all, global_attention_mask=global_attention_mask)

    # Backprop
    loss = outputs.loss
    loss.backward()

    # GD
    optimizer.step()
    lr_scheduler.step()
    optimizer.zero_grad()
    progress_bar.update(1)

    # Decode output
    generated_str = TOKENIZER.batch_decode(generated_ids.tolist(), skip_special_tokens=True)
    metric.add_batch(predictions=generated_str, references=batch[&quot;summary&quot;])

    # Calculate metrics
    print(f&quot;Epoch: {epoch+1}, Batch: {batch_idx+1}:&quot;)
    print(metric.compute())
</code></pre>
","huggingface"
"77892732","Convert hugging face audio classifier to TFLite format","2024-01-27 20:36:25","","0","170","<android><tensorflow><machine-learning><huggingface><openai-whisper>","<p>I’m making a model to run on an Android phone which will be able to recognise a set of specific audio commands. There are 6 commands, so I need a classifier with 7 classes, one for each command plus a class for anything unrecognised.</p>
<p>To do this, I first took <code>facebook/wav2vec2-base</code>, and trained it on a dataset with 1000 examples for each command class, and a further 2000 with “unrecognised” words. The classifier performed excellently.
TFLite seemed the best way of getting the model onto Android, so used optimum to export it to TFLite format (<code>optimum-cli export tflite --task audio-classification ...</code>). This wasn’t easy as at first it failed with the error <code>KeyError: &quot;wav2vec2 (tf_wav2_vec2_for_sequence_classification) is not supported yet with the tflite backend. Only ['onnx'] are supported. If you want to support tflite please propose a PR or open up an issue.&quot;</code>.
Eventually I exported it to onnx, then to tf, and from there to TFLite. The model was too big, ~500MB, so I used dynamic quantisation to get it down to ~100MB. Integer quantisation really messed up the model, so I left it with dynamic quantisation.</p>
<p>I wanted to compare that model with sthg simpler, but <code>wav2vec2</code> doesn’t have any “small” or “tiny” variants. Instead I used Whisper, which is intended for ASR (not classification), using the <code>openai/whisper-tiny</code> variant. Loaded it using <code>transformers.WhisperForAudioClassification</code>, since classification is my goal, and finetuned it on the same dataset. Despite being a lot smaller (30MB) it outperformed the first model - great.</p>
<p>The problem came when trying to export this model (finetuned Whisper model for classification), to TFLite:</p>
<ul>
<li>Exporting direct to TFLite (<code>optimum-cli export tflite --task audio-classification ...</code>) didn’t work because task <code>audio-classification</code> only recognises <code>Wav2Vec2</code>. So using it for a Whisper model raises <code>ValueError: Unrecognized configuration class &lt;class 'transformers.models.whisper.configuration_whisper.WhisperConfig'&gt; for this kind of AutoModel: TFAutoModelForAudioClassification. Model type should be one of Wav2Vec2Config</code>. Presumed explanation: Whisper is essentially ASR, not a classifier.</li>
<li>Exporting to ONNX (<code>optimum-cli export onnx --task audio-classification ...</code>) didn’t work because <code>ValueError: Asked to export a whisper model for the task audio-classification, but the Optimum ONNX exporter only supports the tasks feature-extraction, feature-extraction-with-past, automatic-speech-recognition, automatic-speech-recognition-with-past for whisper. Please use a supported task. Please open an issue at https://github.com/huggingface/optimum/issues if you would like the task audio-classification to be supported in the ONNX export for whisper</code>.</li>
</ul>
<p>How can I get this model working on Android?</p>
","huggingface"
"77889571","How to change the text_encoder for a Stable Diffusion model for fine-tuning?","2024-01-27 00:11:49","","1","255","<huggingface><diffusers>","<p>I want to use Stable Diffusion model weights to generate class-conditional images- however, I don't want these images to be conditional on a text prompt, but rather on a number of binary class attributes/rows.</p>
<p>In order to do this, I was thinking of using the HuggingFace Diffusers library, as it seemed the most straightforward. My thinking was to replace the CLIP text encoder/tokenizer with a custom encoder which maps the attribute rows into the latent space, however I can't seem to find resources on this online, and was wondering if it was possible/feasible within the Diffusers library.</p>
<p>I understand that the StableDiffusionPipeline is likely too strict, however, I was wondering how I would define a model with these attribute rows as the conditioner for the generation, and how this model could be trained/fine-tuned.</p>
","huggingface"
"77886402","Text Conversion to Embedding and Upserting to the pinecone Vector DB","2024-01-26 12:54:27","","1","541","<python><huggingface><pinecone><openaiembeddings>","<p>I am learning Gen AI and I came across following script, it was working for the trainer however it is not working for me, may be due to recent version changes. I am trying to convert text to Vector Embedding and then upload the embedded values to the Pinecone Vector DB. Can someone help me where am I making a mistake?</p>
<ul>
<li>I am using Jupyter Notebook to execute above commands*</li>
</ul>
<p>Error -</p>
<pre class=""lang-none prettyprint-override""><code>AttributeError                            Traceback (most recent call last)
Cell In[44], line 8
      5 index_name=&quot;medical-bot&quot;
      7 #Creating Embeddings for Each of The Text Chunks &amp; storing
----&gt; 8 docsearch=pc.from_texts([t.page_content for t in text_chunks], embeddings, index_name=index_name)

AttributeError: 'Pinecone' object has no attribute 'from_texts'
</code></pre>
<pre class=""lang-py prettyprint-override""><code>def load_data(data):
    loader = DirectoryLoader(data, glob=&quot;\*.pdf&quot;, loader_cls=PyPDFLoader)
    documents = loader.load()
    return documents

extracted_data = load_data(&quot;E:\\data&quot;)

def text_split(extracted_data):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap = 20)
    text_chunks = text_splitter.split_documents(extracted_data)
    return text_chunks

text_chunks = text_split(extracted_data)


def download_hugging_face_embeddings():
    embeddings = HuggingFaceEmbeddings(model_name=&quot;sentence-transformers/all-MiniLM-L6-v2&quot;)
    return embeddings

embeddings = download_hugging_face_embeddings()

query_result = embeddings.embed_query(&quot;Hello World&quot;)
print(&quot;Length&quot;, len(query_result))

from dotenv import load_dotenv
import os
load_dotenv()
pinecone_api_key = os.getenv(&quot;PINECONE_API_KEY&quot;)
pinecone_environment = os.getenv(&quot;PINECONE_API_ENV&quot;)

pc = Pinecone(api_key=pinecone_api_key,
environment=pinecone_environment)
index_name=&quot;medical-bot&quot; #My Index Name created in Pinecone DB

#Creating Embeddings for Each of The Text Chunks &amp; storing
docsearch=Pinecone.from_texts([t.page_content for t in text_chunks], embeddings, index_name=index_name)
</code></pre>
","huggingface"
"77885787","Resume from checkpoint with Accelerator causes loss to increase","2024-01-26 11:10:59","77934431","0","426","<python><huggingface><stable-diffusion><accelerate>","<p>I've been working on a project to attempt to finetune Stable Diffusion and introduce layout conditioning. I'm using all the components of Stable diffusion from the huggingface stable diffusion pipeline as frozen and only the Unet and my custom model called LayoutEmbeddeder for the conditioning are not frozen.</p>
<p>I've managed to adapt some code to my needs and training the model however, my code crashed during execution and although I think I have implemented the checkpointing correctly when I resume training the loss is very mush higher and the images generated are pure noise compared to what I was logging during training.</p>
<p>So it looks like maybe I'm not saving the checkpoints properly. Is anyone able to take a look and give me some advice? I'm not very expert with Accelerator and there is alot of magic going on.</p>
<p>Code can be found here: <a href=""https://github.com/mia01/stable-diffusion-layout-finetune"" rel=""nofollow noreferrer"">https://github.com/mia01/stable-diffusion-layout-finetune</a> ( the checkpoint code is in main.py I'm using accelerate hooks)
Wandb log here (you can clearly see the jump in the loss): <a href=""https://api.wandb.ai/links/dissertation-project/wqq4croy"" rel=""nofollow noreferrer"">https://api.wandb.ai/links/dissertation-project/wqq4croy</a></p>
<p>This is what the checkpoint directory looks like:
<a href=""https://i.sstatic.net/BIbFe.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/BIbFe.png"" alt=""enter image description here"" /></a>
<a href=""https://i.sstatic.net/Kwn98.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Kwn98.png"" alt=""enter image description here"" /></a>
<a href=""https://i.sstatic.net/FFV90.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/FFV90.png"" alt=""enter image description here"" /></a></p>
<p>Also when I resume training the validation images I inference every x amount of steps look like this:
<a href=""https://i.sstatic.net/QT1G9.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/QT1G9.jpg"" alt=""enter image description here"" /></a>
Which is worse than the samples generated before I started the fine-tuning!</p>
<p>Any tips would be very much appreciated of where I went wrong!
Thank you</p>
","huggingface"
"77879969","Is there a way to save a pre-compiled AutoTokenizer?","2024-01-25 12:26:05","","5","352","<python><nlp><tokenize><huggingface><huggingface-tokenizers>","<p>Sometimes, we'll have to do something like this to extend a pre-trained tokenizer:</p>
<pre><code>from transformers import AutoTokenizer

from datasets import load_dataset


ds_de = load_dataset(&quot;mc4&quot;, 'de')
ds_fr = load_dataset(&quot;mc4&quot;, 'fr')

de_tokenizer = tokenizer.train_new_from_iterator(
    ds_de['text'],vocab_size=50_000
)

fr_tokenizer = tokenizer.train_new_from_iterator(
    ds_fr['text'],vocab_size=50_000
)

new_tokens_de = set(de_tokenizer.vocab).difference(tokenizer.vocab)
new_tokens_fr = set(fr_tokenizer.vocab).difference(tokenizer.vocab)
new_tokens = set(new_tokens_de).union(new_tokens_fr)


tokenizer = AutoTokenizer.from_pretrained(
    'moussaKam/frugalscore_tiny_bert-base_bert-score'
)

tokenizer.add_tokens(list(new_tokens))

tokenizer.save_pretrained('frugalscore_tiny_bert-de-fr')
</code></pre>
<p>And then when loading the tokenizer,</p>
<pre><code>tokenizer = AutoTokenizer.from_pretrained(
  'frugalscore_tiny_bert-de-fr', local_files_only=True
)
</code></pre>
<p>It takes pretty long to load from <code>%%time</code> in a Jupyter cell:</p>
<pre><code>CPU times: user 34min 20s
Wall time: 34min 22s
</code></pre>
<p>I guess this is due to regex compilation for the added tokens which was also raised in <a href=""https://github.com/huggingface/tokenizers/issues/914"" rel=""noreferrer"">https://github.com/huggingface/tokenizers/issues/914</a></p>
<p>I think it's okay since it'll load once and the work can be done without redoing the regex compiles.</p>
<h3>But, is there a way to just save the tokenizer in binary form and avoid the whole regex compilation the next time?</h3>
","huggingface"
"77876754","Issue with Prediction Shape in Hugging Face Accelerated Inference","2024-01-24 23:22:24","","0","47","<python><deep-learning><pytorch><huggingface-transformers><huggingface>","<p>I am currently facing an issue while utilizing the Hugging Face accelerator for inferencing on a test dataset.</p>
<p>The code snippet below outlines my inferencing process, where I have four GPU devices, and I set num_processes=4 in the notebook_launcher:</p>
<pre><code>def infer_ddp_accelerate(model):
   
    accelerator = Accelerator(mixed_precision=&quot;fp16&quot;)
    test_dataset = CustomDataset(test)
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)
   
    # Send everything through `accelerator.prepare`
    model, test_loader = accelerator.prepare(model, test_loader)

    accelerator.print(f&quot;Testing on {len(test_loader)} samples&quot;)
  
    model.eval()
    prediction = []
   
    with torch.no_grad():
        for batch in tqdm(test_loader):
            inputs = batch
            input_ids = inputs['input_ids']
            attention_mask = inputs['attention_mask']
            outputs = model(input_ids, attention_mask)
           
            scores = torch.sigmoid(outputs.squeeze())
            prediction.append(accelerator.gather(scores))

    prediction = torch.cat(prediction, axis=0).cpu().numpy()
   
    accelerator.print(f&quot;{prediction.shape}&quot;)
    accelerator.print(f&quot;{prediction}&quot;)

notebook_launcher(infer_ddp_accelerate, args=(model,), num_processes=4)
</code></pre>
<p>The test dataset has 57949 rows, and I expect the model score for each row. However, the prediction.shape in the code is coming out to be 57984.</p>
<p>I suspect that due to the presence of four GPUs, the actual batch size becomes 32 * 4 = 128, resulting in 453 batches (ceil(57949/128) = 453).</p>
<p>As 453 * 128 = 57984, the accelerator is somehow giving the final prediction shape as 57984.</p>
<p>How can I ensure that the prediction shape is exactly 57949 in the above code?</p>
<p>Is taking the first 57949 elements of the 57984 correct, or is there a better approach to handle this?</p>
<p>Thank you.</p>
","huggingface"
"77873694","Are HuggingFace pipelines re-entrant?","2024-01-24 14:08:52","","0","45","<pytorch><huggingface-transformers><huggingface>","<p>I have a single GPU, and a classification task which has a lot of pre- and post-processing done in Python, so the actual GPU utilisation is between 20%-50%, depending on the individual input.</p>
<p>Can I run the Python code in 2 threads and rely on transformers or pytorch to do the right thing if I call the pipeline concurrently from them?</p>
","huggingface"
"77867414","HuggingFace - How to plot training and validation accuracy vs. Epoch graph?","2024-01-23 15:23:58","","0","158","<pytorch><huggingface-transformers><huggingface><huggingface-trainer>","<p>As the title is self-descriptive, I need to plot the training and validation accuracy obtained during the training of my Hugging Face model. After that, I'd like to plot the confusion matrix for the test predictions. How can I do these?</p>
<p>Here is my training arguments:</p>
<pre><code>args = TrainingArguments(
    output_dir=f&quot;my_training&quot;,
    evaluation_strategy=&quot;epoch&quot;,
    save_strategy=&quot;epoch&quot;,
    learning_rate=5e-5,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    per_device_eval_batch_size=4,
    num_train_epochs=5,
    warmup_ratio=0.1,
    logging_steps=10,
    load_best_model_at_end=True,
    metric_for_best_model=&quot;accuracy&quot;,
    report_to='tensorboard',
    push_to_hub=True,
)
</code></pre>
<p>And, here is my trainer:</p>
<pre><code>def compute_metrics(eval_pred):
    predictions = np.argmax(eval_pred.predictions, axis=1)
    accuracy = accuracy_score(y_pred=predictions, y_true=eval_pred.label_ids)
    return {&quot;accuracy&quot;: accuracy}


trainer = Trainer(
    model,
    args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=processor,
    compute_metrics=compute_metrics,
    data_collator=collate_fn
)
</code></pre>
<p>Finally, I start the training and prediction, respectively:</p>
<pre><code>train_results = trainer.train()
trainer.save_model()
trainer.log_metrics(&quot;train&quot;, train_results.metrics)
trainer.save_metrics(&quot;train&quot;, train_results.metrics)
trainer.save_state()

eval_results = trainer.evaluate(eval_dataset)
trainer.log_metrics(&quot;eval&quot;, eval_results)
trainer.save_metrics(&quot;eval&quot;, eval_results)
</code></pre>
<p>With the current configuration, I only get the eval/accuracy vs. Steps graph.</p>
","huggingface"
"77866423","Vilt Model causing RuntimeError: indices should be either on cpu or on the same device as the indexed tensor (cpu)","2024-01-23 12:47:32","","0","30","<python><pytorch><huggingface-transformers><huggingface><huggingface-datasets>","<p>Vilt Model causing RuntimeError: indices should be either on cpu or on the same device as the indexed tensor (cpu) in the new update.</p>
<p>I have placed the model already on the GPU then running the below code</p>
<p>My same old code is running fine on other envs, but in my current env I newly installed the huggingface transformers library since then I’m facing a lot of issues in the same codes.</p>
<p>Any insights would be helpful, I checked on SO the solutions for this on other models but none helped, so raising a new question.</p>
<p>Here is the finetuning code I have:</p>
<pre><code>optimizer = torch.optim.AdamW(model.parameters(), lr = 5e-5)
torch.set_grad_enabled(True)  # Context-manager 
model.train()

epochList, accList = [],[]


for epoch in tqdm(range(20)):  

print(f&quot;Epoch: {epoch}&quot;)

for idx, batch in enumerate(train_dataloader):

    batch = {k:v.to(device) for k,v in batch.items()}

    optimizer.zero_grad()
    
    outputs = model(**batch)
    loss = outputs.loss

    print(idx,&quot;-&gt; Loss:&quot;, loss.item())
    
    loss.backward()
    optimizer.step()

    
    if (idx != 0 ) and (idx % 200 == 0):
        
        model.eval()
        
        acc_score_test = calculateAccuracyTest()
        acc_score_val = calculateAccuracyVal()
        
        print(f'\nValidation Accuracy: {acc_score_val}, Test Accuracy: {acc_score_test} \n')
                
        epochList.append((epoch*tot_number_of_steps)+idx)
        accList.append((acc_score_test,acc_score_val))

        model.train()
</code></pre>
<p>The stack Trace is huge, I'm bottom trace is:</p>
<pre><code>~/miniconda3/envs/yolo/lib/python3.11/site-packages/transformers/models/vilt/modeling_vilt.py:219, in ViltEmbeddings.forward(self, input_ids, attention_mask, token_type_ids, pixel_values, pixel_mask, inputs_embeds, image_embeds, image_token_type_idx)
217 # PART 2: patch embeddings (with interpolated position encodings)
218 if image_embeds is None:
--&gt; 219     image_embeds, image_masks, patch_index = self.visual_embed(
220         pixel_values, pixel_mask, max_image_length=self.config.max_image_length
221     )
222 else:
223     image_masks = pixel_mask.flatten(1)


~/miniconda3/envs/yolo/lib/python3.11/site-packages/transformers/models/vilt/modeling_vilt.py:186, in ViltEmbeddings.visual_embed(self, pixel_values, pixel_mask, max_image_length)
184 x = x[select[:, 0], select[:, 1]].view(batch_size, -1, num_channels)
185 x_mask = x_mask[select[:, 0], select[:, 1]].view(batch_size, -1)
--&gt; 186 patch_index = patch_index[select[:, 0], select[:, 1]].view(batch_size, -1, 2)
187 pos_embed = pos_embed[select[:, 0], select[:, 1]].view(batch_size, -1, num_channels)
189 cls_tokens = self.cls_token.expand(batch_size, -1, -1)

RuntimeError: indices should be either on cpu or on the same device as the indexed tensor (cpu)
</code></pre>
","huggingface"
"77862602","How to process large file using pipeline from transformers","2024-01-22 20:35:09","","-1","173","<machine-learning><speech-recognition><huggingface-transformers><huggingface><openai-whisper>","<p>I load model from Hugging Face using pipeline:</p>
<pre><code>device = &quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;
torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32

model_id = &quot;openai/whisper-large-v3&quot;

model = AutoModelForSpeechSeq2Seq.from_pretrained(
    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True
)
model.to(device)

processor = AutoProcessor.from_pretrained(model_id)

pipe = pipeline(
    &quot;automatic-speech-recognition&quot;,
    model=model,
    tokenizer=processor.tokenizer,
    feature_extractor=processor.feature_extractor,
    max_new_tokens=128,
    chunk_length_s=20,
    batch_size=16,
    return_timestamps=True,
    torch_dtype=torch_dtype,
    device=device,
)
</code></pre>
<p>And i want to process big audio file</p>
<pre><code>result = pipe(&quot;filename3.mp3&quot;)
</code></pre>
<p>But google collab says i ran out of RAM. Is it possible to pipe output to file rather than variable?</p>
","huggingface"
"77857305","How is it possible to get GPU memory errors when increasing the gradient_accumulation steps (HF)?","2024-01-22 01:55:06","","-1","423","<huggingface-transformers><huggingface>","<p>I'm confused. I found that batch_size=8 does not give GPU memory issues. However when I increase the gradient_accumulation_steps from 1 to 2 then there is a GPU memory issue. I thought the point of gradient_accumulation_steps was to compute the grads of the batch size by fragmenting the batch_size to mini-batches and then compute the forward &amp; backward passes on the mini-batch and then accumulating/doing a running sum. Thus, why is this OMM issue in the gpu happening?</p>
<p>Code:</p>
<pre class=""lang-py prettyprint-override""><code>&quot;&quot;&quot;
Goal: making HF training script for model (e.g., llama v2) using raw text of informal and formal mathematics (unpaired data).

Inspiration:
- ref: SO accelerate + trainer: https://stackoverflow.com/questions/76675018/how-does-one-use-accelerate-with-the-hugging-face-hf-trainer
- ref: The unreasonable effectiveness of few-shot learning for machine translation https://arxiv.org/abs/2302.01398
- ref: colab: https://colab.research.google.com/drive/1io951Ex17-6OUaogCo7OiR-eXga_oUOH?usp=sharing
- ref: SO on collate: https://stackoverflow.com/questions/76879872/how-to-use-huggingface-hf-trainer-train-with-custom-collate-function/76929999#76929999

Looks very useful especially for peft:
- peft https://github.com/huggingface/trl/blob/main/examples/scripts/sft_trainer.py

python trl/examples/scripts/sft_trainer.py \
    --model_name meta-llama/Llama-2-7b-hf \
    --dataset_name timdettmers/openassistant-guanaco \
    --load_in_4bit \
    --use_peft \
    --batch_size 4 \
    --gradient_accumulation_steps 2

- qlora https://github.com/artidoro/qlora/blob/main/scripts/finetune_llama2_guanaco_7b.sh, 
- https://github.com/artidoro/qlora/blob/main/qlora.py

export CUDA_VISIBLE_DEVICES=6
&quot;&quot;&quot;
from pathlib import Path
from typing import Callable
import datasets
from datasets import load_dataset, interleave_datasets
import torch
from transformers import GPT2LMHeadModel, PreTrainedTokenizer, AutoTokenizer, Trainer, TrainingArguments, AutoConfig
import math

import sys
from training.reinit_and_smaller_llama2 import get_deafult_smallest_baby_llama2_v1_36m_0p036b, get_weight_norms, reinitialize_weights_gpt_neox_20B_inspired_4_llama2
sys.path = [''] + sys.path
from training.utils import eval_hf, get_column_names, get_data_from_hf_dataset, group_texts, raw_dataset_2_lm_data

# -- Experiments 

def train():
    &quot;&quot;&quot;
    I decided to make the string data close to context length of llama2 7B 4096 tokens.
    So if any string is shorter, the tokenize will padd it according to Claude.
    
    &quot;&quot;&quot;
    # feel free to move the import statements if you want, sometimes I like everything in one place so I can easily copy-paste it into a script
    import datetime
    from pathlib import Path
    import datasets
    from datasets import load_dataset, interleave_datasets
    import torch
    import transformers
    from transformers import PreTrainedTokenizer
    from transformers import GPT2LMHeadModel, PreTrainedTokenizer, AutoTokenizer, Trainer, TrainingArguments, AutoConfig
    import random
    import math
    import os
    # buffer_size = 500_000  # can't remember what this was for and doesn't seem to be anywhere
    probabilities = []
    data_mixture_name = None
    streaming = True
    data_files = [None]
    seed = 0
    split = 'train'
    max_length = 1024  # gpt2 context length
    shuffle = False
    report_to = 'none'  # safest default
    # CHUNK_SIZE = 16_896  # approximately trying to fill the llama2 context length of 4096
    batch_size = 2
    gradient_accumulation_steps = 2
    num_epochs = 1
    num_tokens_trained = None
    num_batches=1

    # -- Setup wandb
    import wandb
    # - Dryrun
    mode = 'dryrun'; seed = 0; report_to = 'none'

    # - Online (real experiment)
    # mode = 'online'; seed = 0; report_to = 'wandb'

    # - train data sets
    # path, name, data_files, split = ['csv'], [None], [os.path.expanduser('~/data/maf_data/maf_textbooks_csv_v1/train.csv')], ['train']
    # path, name, data_files, split = ['c4'], ['en'], [None], ['train']
    # path, name, data_files, split = ['UDACA/PileSubsets'], ['uspto'], [None], ['train']
    # path, name, data_files, split = ['UDACA/PileSubsets'], ['pubmed'], [None], ['train']
    path, name, data_files, split = ['UDACA/PileSubsets', 'UDACA/PileSubsets'], ['uspto', 'pubmed'], [None, None], ['train', 'train']
    # - models
    # pretrained_model_name_or_path = 'gpt2'
    # pretrained_model_name_or_path = 'meta-llama/Llama-2-7b-hf'
    # pretrained_model_name_or_path = 'meta-llama/Llama-2-7b-chat-hf'
    # pretrained_model_name_or_path = 'meta-llama/Llama-2-13b-hf'
    # pretrained_model_name_or_path = 'meta-llama/Llama-2-70b-hf'
    # pretrained_model_name_or_path = 'mistralai/Mistral-7B-v0.1'
    pretrained_model_name_or_path = 'baby_llama2_v1'
    # - important training details or it wont run, mem issues maybe
    # max_steps = 19_073 # &lt;- CHANGE THIS  11 days with baby llama2 v1 36m 1, 32
    max_steps = 866 # &lt;- CHANGE THIS 12hs with with baby llama2 v1 36m 1, 32
    L = 4096
    num_batches=1
    # single gpu
    # batch_size, gradient_accumulation_steps = 2, 1  # e.g., choosing large number mabe for stability of training? 4 (per_device_train_batch_size) * 8 (gradient_accumulation_steps), based on alpaca https://github.com/tatsu-lab/stanford_alpaca 
    # batch_size, gradient_accumulation_steps = 2, 16  # e.g., choosing large number mabe for stability of training? 4 (per_device_train_batch_size) * 8 (gradient_accumulation_steps), based on alpaca https://github.com/tatsu-lab/stanford_alpaca 
    # batch_size, gradient_accumulation_steps = 2, 32  # e.g., choosing large number mabe for stability of training? 4 (per_device_train_batch_size) * 8 (gradient_accumulation_steps), based on alpaca https://github.com/tatsu-lab/stanford_alpaca 
    # batch_size, gradient_accumulation_steps = 1, 32  # e.g., choosing large number mabe for stability of training? 4 (per_device_train_batch_size) * 8 (gradient_accumulation_steps), based on alpaca https://github.com/tatsu-lab/stanford_alpaca 
    batch_size, gradient_accumulation_steps = 8, 2  # e.g., choosing large number mabe for stability of training? 4 (per_device_train_batch_size) * 8 (gradient_accumulation_steps), based on alpaca https://github.com/tatsu-lab/stanford_alpaca 
    # -- multiple gpus 3 4096 context len
    # batch_size, gradient_accumulation_steps = 4, 8  # e.g., choosing large number mabe for stability of training? 4 (per_device_train_batch_size) * 8 (gradient_accumulation_steps), based on alpaca https://github.com/tatsu-lab/stanford_alpaca 
    # gradient_checkpointing = False
    gradient_checkpointing = True
    print(f'{batch_size=} {gradient_accumulation_steps=} {gradient_checkpointing=} {num_epochs=}')
    # -- wandb
    num_tokens_trained = max_steps * batch_size * L * num_batches 
    today = datetime.datetime.now().strftime('%Y-m%m-d%d-t%Hh_%Mm_%Ss')
    # run_name = f'{path} div_coeff_{num_batches=} ({today=} ({name=}) {data_mixture_name=} {probabilities=} {pretrained_model_name_or_path=})'
    run_name = f'training maths: {path} ({today=} ({name=}) {data_mixture_name=} {probabilities=} {pretrained_model_name_or_path=} {data_files=} {max_steps=} {batch_size=} {num_tokens_trained=} {gradient_accumulation_steps=})'
    print(f'\n---&gt; {run_name=}\n')

    # - Init wandb
    debug: bool = mode == 'dryrun'  # BOOL, debug?
    run = wandb.init(mode=mode, project=&quot;beyond-scale&quot;, name=run_name, save_code=True)
    # wandb.config.update({&quot;num_batches&quot;: num_batches, &quot;path&quot;: path, &quot;name&quot;: name, &quot;today&quot;: today, 'probabilities': probabilities, 'batch_size': batch_size, 'debug': debug, 'data_mixture_name': data_mixture_name, 'streaming': streaming, 'data_files': data_files, 'seed': seed, 'pretrained_model_name_or_path': pretrained_model_name_or_path})
    wandb.config.update({&quot;path&quot;: path, &quot;name&quot;: name, &quot;today&quot;: today, 'probabilities': probabilities, 'batch_size': batch_size, 'debug': debug, 'data_mixture_name': data_mixture_name, 'streaming': streaming, 'data_files': data_files, 'seed': seed, 'pretrained_model_name_or_path': pretrained_model_name_or_path, 'num_epochs': num_epochs, 'gradient_accumulation_steps': gradient_accumulation_steps})
    # run.notify_on_failure() # https://community.wandb.ai/t/how-do-i-set-the-wandb-alert-programatically-for-my-current-run/4891
    print(f'{debug=}')
    print(f'{wandb.config=}')

    # -- Load model and tokenizer  
    print(f'{pretrained_model_name_or_path=}')
    if pretrained_model_name_or_path == 'gpt2':
        from transformers import GPT2Tokenizer, GPT2LMHeadModel
        tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model_name_or_path)
        if tokenizer.pad_token_id is None:
            tokenizer.pad_token = tokenizer.eos_token
            print(f'{tokenizer.pad_token=}')
        print(f'{tokenizer.eos_token=}')
        print(f'{ tokenizer.eos_token_id=}')
        model = GPT2LMHeadModel.from_pretrained(pretrained_model_name_or_path)
        device = torch.device(f&quot;cuda:{0}&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
        model = model.to(device)
        block_size: int = tokenizer.model_max_length
    elif 'Llama-2' in pretrained_model_name_or_path or 'Mistral' in pretrained_model_name_or_path:
        # - llama2
        from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer
        # bf16 or fp32
        torch_dtype = torch.bfloat16 if torch.cuda.get_device_capability(torch.cuda.current_device())[0] &gt;= 8 else torch.float32 # if &gt;= 8 ==&gt; brain float 16 available or set to True if you always want fp32
        # get model
        model = AutoModelForCausalLM.from_pretrained(
            pretrained_model_name_or_path,
            # quantization_config=quantization_config,
            # device_map=device_map,  # device_map = None  https://github.com/huggingface/trl/blob/01c4a35928f41ba25b1d0032a085519b8065c843/examples/scripts/sft_trainer.py#L82
            trust_remote_code=True,
            torch_dtype=torch_dtype,
            use_auth_token=True,
        )
        # HF trainer load to gpu on it's own: https://claude.ai/chat/43796e10-2139-4668-ac5c-aafeeeeeba2e
        # # -- Detect if running with accelerate https://claude.ai/chat/43796e10-2139-4668-ac5c-aafeeeeeba2e
        # from accelerate import Accelerator
        # accelerator = Accelerator()
        # # self.is_deepspeed_enabled = getattr(accelerator.state, &quot;deepspeed_plugin&quot;, None) is not None
        # is_fsdp_enabled = getattr(accelerator.state, &quot;fsdp_plugin&quot;, None) is not None
        # if not is_fsdp_enabled: # not sure if this is needed but its for sure safer
        #     # maybe figuring out how to run everything with accelerate would fix things...
        #     # ref: https://stackoverflow.com/questions/77204403/does-one-need-to-load-the-model-to-gpu-before-calling-train-when-using-accelerat
        #     device = torch.device(f&quot;cuda:{0}&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
        #     model = model.to(device)
        # https://github.com/artidoro/qlora/blob/7f4e95a68dc076bea9b3a413d2b512eca6d004e5/qlora.py#L347C13-L347C13
        tokenizer = AutoTokenizer.from_pretrained(
            pretrained_model_name_or_path,
            # cache_dir=args.cache_dir,
            padding_side=&quot;right&quot;,
            use_fast=False, # Fast tokenizer giving issues.
            # tokenizer_type='llama' if 'llama' in args.model_name_or_path else None, # Needed for HF name change
            # tokenizer_type='llama',
            trust_remote_code=True,
            use_auth_token=True,
            # token=token,  # load from cat keys/brandos_hf_token.txt if you want to load it in python and not run huggingface-cli login
        )
        # - Ensure padding token is set TODO: how does this not screw up the fine-tuning? e.g., now model doesn't learn to predict eos since it's padded our by mask, ref: https://discuss.huggingface.co/t/why-does-the-falcon-qlora-tutorial-code-use-eos-token-as-pad-token/45954
        if tokenizer.pad_token_id is None:
            tokenizer.pad_token = tokenizer.eos_token
            print(f'{tokenizer.pad_token=}')
        print(f'{tokenizer.eos_token=}')
        print(f'{ tokenizer.eos_token_id=}')
        # get context length for setting max length for training
        if hasattr(model.config, &quot;context_length&quot;):
            print(&quot;Context length:&quot;, model.config.context_length)
            max_length = model.config.context_length
        else:
            # CHUNK_SIZE = 16_896  # approximately trying to fill the llama2 context length of 4096
            max_length = 4096
        block_size: int = 4096
        print(f'{max_length=}')
    elif 'baby_llama2_v1' in pretrained_model_name_or_path:
        model = get_deafult_smallest_baby_llama2_v1_36m_0p036b()
        reinitialize_weights_gpt_neox_20B_inspired_4_llama2(model, L=4096)
        tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf', padding_side=&quot;right&quot;, use_fast=False, trust_remote_code=True, use_auth_token=True)
        device = torch.device(f&quot;cuda:{0}&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
        model = model.to(device)
        torch_dtype = torch.bfloat16 if torch.cuda.get_device_capability(torch.cuda.current_device())[0] &gt;= 8 else torch.float32 # if &gt;= 8 ==&gt; brain float 16 available or set to True if you always want fp32
        model = model.to(torch_dtype)
        block_size: int = 4096
    print(&quot;Number of parameters:&quot;, sum(p.numel() for p in model.parameters()))
    print(f&quot;Total weight norm: {get_weight_norms(model)=}&quot;)
    print(f'{torch.cuda.device_count()=} (makes sure GPUs are visible and accesible to Pytorch.)')
    print(f'Model is currently on: {next(iter(model.parameters())).device=}')
    print(f'Model is currently on: {next(iter(model.parameters())).dtype=}')
    
    # --- Load datasets
    # -- Get train data set
    # - Load interleaved combined datasets
    # train_datasets = [load_dataset(path, name, streaming=True, split=&quot;train&quot;).with_format(&quot;torch&quot;) for path, name in zip(path, name)]
    train_datasets = [load_dataset(path, name, data_files=data_file, streaming=streaming, split=split).with_format(&quot;torch&quot;) for path, name, data_file, split in zip(path, name, data_files, split)]
    probabilities = [1.0/len(train_datasets) for _ in train_datasets]  
    # - Get raw train data set
    raw_train_datasets = interleave_datasets(train_datasets, probabilities)
    remove_columns = get_column_names(raw_train_datasets)  # remove all keys that are not tensors to avoid bugs in collate function in task2vec's pytorch data loader
    # - Get tokenized train data set
    # Note: Setting `batched=True` in the `dataset.map` function of Hugging Face's datasets library processes the data in batches rather than one item at a time, significantly speeding up the tokenization and preprocessing steps.
    tokenize_function = lambda examples: tokenizer(examples[&quot;text&quot;])
    tokenized_train_datasets = raw_train_datasets.map(tokenize_function, batched=True, remove_columns=remove_columns)
    _group_texts = lambda examples : group_texts(examples, block_size)
    # - Get actual data set for lm training (in this case each seq is of length block_size, no need to worry about pad = eos since we are filling each sequence)
    lm_train_dataset = tokenized_train_datasets.map(_group_texts, batched=True)
    batch = get_data_from_hf_dataset(lm_train_dataset, streaming=streaming, batch_size=batch_size)
    print(f'{len(next(iter(batch))[&quot;input_ids&quot;])=}')
    assert all(len(data_dict['input_ids']) == block_size for data_dict in iter(batch)), f'Error, some seq in batch are not of length {block_size}'
    train_dataset = lm_train_dataset

    # -- max steps manually decided depending on how many tokens we want to train on
    per_device_train_batch_size = batch_size
    print(f'{per_device_train_batch_size=}')
    print(f'{num_epochs=} {max_steps=}')

    # -- Training arguments and trainer instantiation ref: https://huggingface.co/docs/transformers/v4.31.0/en/main_classes/trainer#transformers.TrainingArguments
    output_dir = Path(f'~/data/results_{today}/').expanduser() if not debug else Path(f'~/data/results/').expanduser()
    # output_dir = '.'
    # print(f'{debug=} {output_dir=} \n {report_to=}')
    training_args = TrainingArguments(
        output_dir=output_dir,  # The output directory where the model predictions and checkpoints will be written.
        # output_dir='.',  # The output directory where the model predictions and checkpoints will be written.
        # num_train_epochs = num_train_epochs, 
        max_steps=max_steps,  # TODO: hard to fix, see above
        per_device_train_batch_size=per_device_train_batch_size,
        gradient_accumulation_steps=gradient_accumulation_steps,  # based on alpaca https://github.com/tatsu-lab/stanford_alpaca, allows to process effective_batch_size = gradient_accumulation_steps * batch_size, num its to accumulate before opt update step
        gradient_checkpointing = gradient_checkpointing,  # TODO depending on hardware set to true?
        optim=&quot;paged_adamw_32bit&quot;,  # David hall says to keep 32bit opt https://arxiv.org/pdf/2112.11446.pdf TODO: if we are using brain float 16 bf16 should we be using 32 bit? are optimizers always fb32?  https://discuss.huggingface.co/t/is-there-a-paged-adamw-16bf-opim-option/51284
        # warmup_steps=500,  # TODO: once real training starts we can select this number for llama v2, what does llama v2 do to make it stable while v1 didn't?
        # warmup_ratio=0.03,  # copying alpaca for now, number of steps for a linear warmup, TODO once real training starts change? 
        # weight_decay=0.01,  # TODO once real training change?
        weight_decay=0.00,  # TODO once real training change?
        learning_rate = 1e-5,  # TODO once real training change? anything larger than -3 I've had terrible experiences with
        max_grad_norm=1.0, # TODO once real training change?
        # lr_scheduler_type=&quot;cosine&quot;,  # TODO once real training change? using what I've seen most in vision 
        logging_dir=Path('~/data/maf/logs').expanduser(),
        # save_steps=4000,  # alpaca does 2000, other defaults were 500
        save_steps=max_steps//3,  # alpaca does 2000, other defaults were 500
        # save_steps=1,  # alpaca does 2000, other defaults were 500
        # logging_steps=250,
        # logging_steps=50,  
        logging_first_step=True,
        logging_steps=3,
        remove_unused_columns=False,  # TODO don't get why https://stackoverflow.com/questions/76879872/how-to-use-huggingface-hf-trainer-train-with-custom-collate-function/76929999#76929999 , https://claude.ai/chat/475a4638-cee3-4ce0-af64-c8b8d1dc0d90
        report_to=report_to,  # change to wandb!
        fp16=False,  # never ever set to True
        bf16=torch.cuda.get_device_capability(torch.cuda.current_device())[0] &gt;= 8,  # if &gt;= 8 ==&gt; brain float 16 available or set to True if you always want fp32
        # evaluation_strategy='steps',
        # per_device_eval_batch_size=per_device_eval_batch_size,
        # eval_accumulation_steps=eval_accumulation_steps,
        # eval_steps=eval_steps,
    )
    print(f'{pretrained_model_name_or_path=}')

    # TODO: might be nice to figure our how llamav2 counts the number of token's they've trained on
    print(f'{train_dataset=}')
    # print(f'{eval_dataset=}')
    trainer = Trainer(
        model=model,
        args=training_args,  
        train_dataset=train_dataset,
        # eval_dataset=eval_dataset,
        # data_collator=lambda data: custom_collate_fn(data, tokenizer=tokenizer)
    )
    # - TODO bellow is for qlora from falcon, has same interface as Trainer later lets use: https://github.com/artidoro/qlora
    # from trl import SFTTrainer
    # peft_config = None
    # trainer = SFTTrainer(
    #     model=model,
    #     train_dataset=trainset,
    #     peft_config=peft_config,
    #     dataset_text_field=&quot;text&quot;,
    #     max_seq_length=max_seq_length,
    #     tokenizer=tokenizer,
    #     args=training_arguments,
    # )
    # TODO why this? https://discuss.huggingface.co/t/why-do-you-need-to-re-upcast-the-norm-layers-of-hf-falcon-to-fb32/46139
    # for name, module in trainer.model.named_modules():
    #     if &quot;norm&quot; in name:
    #         module = module.to(torch.float32)

    # - Train
    cuda_visible_devices = os.environ.get('CUDA_VISIBLE_DEVICES')
    if cuda_visible_devices is not None:
        print(f&quot;CUDA_VISIBLE_DEVICES = {cuda_visible_devices}&quot;)
    trainer.train()
    trainer.save_model(output_dir=output_dir)  # TODO is this really needed? https://discuss.huggingface.co/t/do-we-need-to-explicity-save-the-model-if-the-save-steps-is-not-a-multiple-of-the-num-steps-with-hf/56745

    # -- Evaluation, NOTE: we are evaluating at the end not during training
    # - Evaluate model on OpenWebtext
    print('---- Evaluate model on OpenWebtext')
    streaming = True
    max_eval_samples = 1024
    path, name, split = 'suolyer/pile_openwebtext2', None, 'validation'  # the one sudharsan used
    eval_dataset = load_dataset(path, name, streaming=streaming, split=split).with_format(&quot;torch&quot;) 
    eval_dataset1 = raw_dataset_2_lm_data(eval_dataset, tokenizer, block_size)
    eval_batch1 = eval_dataset1.take(max_eval_samples)
    print(f'Saving eval results at: {output_dir=}') # The output directory where the model predictions and checkpoints will be written.
    eval_args = TrainingArguments(output_dir=output_dir, fp16=False, bf16=torch.cuda.get_device_capability(torch.cuda.current_device())[0] &gt;= 8)
    trainer = Trainer(model=model, args=eval_args, train_dataset=None, eval_dataset=eval_batch1)
    eval_hf(trainer)
    # - Evaluate on C4
    print('---- Evaluate model on C4')
    streaming = True
    max_eval_samples = 1024
    path, name, split = 'c4', 'en', 'validation' 
    eval_dataset = load_dataset(path, name, streaming=streaming, split=split).with_format(&quot;torch&quot;) 
    eval_dataset2 = raw_dataset_2_lm_data(eval_dataset, tokenizer, block_size)
    eval_batch2 = eval_dataset2.take(max_eval_samples)
    print(f'Saving eval results at: {output_dir=}') # The output directory where the model predictions and checkpoints will be written.
    eval_args = TrainingArguments(output_dir=output_dir, fp16=False, bf16=torch.cuda.get_device_capability(torch.cuda.current_device())[0] &gt;= 8)
    trainer = Trainer(model=model, args=eval_args, train_dataset=None, eval_dataset=eval_batch2)
    eval_hf(trainer)
    # - Evluate on whole datasets
    print('---- Evaluate model on Whole OpenWebtext')
    trainer = Trainer(model=model, args=eval_args, train_dataset=None, eval_dataset=eval_dataset1)
    eval_hf(trainer)
    print('---- Evaluate model on Whole C4')
    trainer = Trainer(model=model, args=eval_args, train_dataset=None, eval_dataset=eval_dataset2)
    # eval_hf(trainer)
    print('Done!\a')

def main():  
    &quot;&quot;&quot;Since accelerate config wants this, main_training_function: main&quot;&quot;&quot;
    train()

# -- Run __main__

if __name__ == '__main__':
    print(f'\n\n\n------------------- Running {__file__} -------------------')
    # -- Run tests and time it
    import time
    time_start = time.time()
    # -- Run tests
    main()
    # -- End tests, report how long it took in seconds, minutes, hours, days
    print(f'Time it took to run {__file__}: {time.time() - time_start} seconds, {(time.time() - time_start)/60} minutes, {(time.time() - time_start)/60/60} hours, {(time.time() - time_start)/60/60/24} days\a')

</code></pre>
<p>cross: <a href=""https://discuss.huggingface.co/t/how-is-it-possible-to-get-gpu-memory-errors-when-increasing-the-gradient-accumulation-steps/70021"" rel=""nofollow noreferrer"">https://discuss.huggingface.co/t/how-is-it-possible-to-get-gpu-memory-errors-when-increasing-the-gradient-accumulation-steps/70021</a></p>
","huggingface"
"77856226","Is my model returning logits or probabilities?","2024-01-21 18:57:06","","0","90","<neural-network><classification><prediction><huggingface>","<p>I am training a neural network to do binary classification on some text data using BCEWithLogitsLoss() as my loss function using the following code:</p>
<pre><code># prep the data...

# Create the model
model = BigBirdForSequenceClassification.from_pretrained(
    'google/bigbird-roberta-base'
    , num_labels=1
    , classifier_dropout = classifier_dropout
    , hidden_dropout_prob = hidden_dropout
)
model = model.to(device)
optimizer = Adam(model.parameters(), lr=1e-5)
loss_fn = nn.BCEWithLogitsLoss()

# create accuracy metric
metric = load_metric(&quot;accuracy&quot;)

def compute_metrics(p):
    return metric.compute(predictions=p.predictions &gt; 0.5, references=p.label_ids)

# Define TrainingArguments
training_args = TrainingArguments(
    output_dir=model_save_path,
    num_train_epochs=5,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    warmup_steps=1000,
    weight_decay=weight_decay,
    evaluation_strategy=&quot;steps&quot;,
    save_strategy=&quot;steps&quot;,
    eval_steps=33000,
    save_steps=5500,
    logging_steps=33000,
    load_best_model_at_end=False,
    save_total_limit=1,
    learning_rate=1e-5,
    fp16=True,
    report_to='none',
    seed=42
)

print(model_save_path)

def data_collector(features):
    batch = {}
    batch['input_ids'] = torch.stack([f[0] for f in features])
    batch['attention_mask'] = torch.stack([f[1] for f in features])
    batch['labels'] = torch.stack([f[2] for f in features])

    return batch

# Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    data_collator=data_collector,
    compute_metrics=compute_metrics
)

if glob.glob(f&quot;{model_save_path}/checkpoint*/pytorch_model.bin&quot;):
    resume = True
else:
    resume = False

# train the model
logging.set_verbosity_error() 
trainer.train(resume_from_checkpoint = resume)

model_loaded = BigBirdForSequenceClassification.from_pretrained(f&quot;{model_save_path}/fullModel&quot;, local_files_only=True)
</code></pre>
<p>I used</p>
<pre><code>def compute_metrics(p):
    return metric.compute(predictions=p.predictions &gt; 0.5, references=p.label_ids)
</code></pre>
<p>as my accuracy measure, thinking the model would return probabilities. However, when I used trainer.predict() to return predictions for my test data, I realized that the values are sometimes below 0 or above 1 (min=-0.015726006, max=1.0713733).  So, this would imply model is actually outputting logits.  However, when I use the threshold of .5 on my logits the predictions seem fairly good (71% accuracy), but if I apply a sigmoid function to transform my logits to probabilities and use a threshold of .5 on those to create predictions, most of the predictions (96%) are 1, and the accuracy is only slightly better than chance.  This makes me wonder if my values are probabilites after all - and maybe they are sometimes outside the expected bounds due to rounding issues or something? Or is it just a coincidence that a threshold of .5 on my logits works much better than a threshold of 0 would?</p>
<p>Additional question - I was hoping to use probability scores as a measure of confidence. If my model is outputting logits, then the corresponding probability scores can't really be used the way I expected, given that most of my probabilities will be over .5 (even for those where the prediction is the negative case).  Is there a different transformation I could apply to my logits to get some more reasonably distributed confidence scores? Maybe just apply a sigmoid transform and then transform the probabilities to z-scores?</p>
","huggingface"
"77855742","How to convert safetensors model to onnx model?","2024-01-21 16:47:45","","1","3561","<python><pytorch><onnx><huggingface><safe-tensors>","<p>I want to convert a <code>model.safetensors</code> to ONNX, unfortunately I haven't found enough information about the procedure. The documentation of <code>safetensors</code> package isn't enough and actually is not clear even how to get the original (pytorch in my case) model, since when I try something as</p>
<pre><code>with st.safe_open(modelsafetensors, framework=&quot;pt&quot;) as mystf:
   ...
</code></pre>
<p>the <code>mystf</code> object has <code>get_tensor('sometensorname')</code> but it seems hasn't any <code>get_model()</code> method or something similar.</p>
","huggingface"
"77855537","Can't login with HuggingFace oauth","2024-01-21 15:51:36","","0","158","<php><oauth-2.0><huggingface>","<p>I am trying to build an authentication solution via HuggingFace.co as per the docs: <a href=""https://huggingface.co/docs/hub/oauth"" rel=""nofollow noreferrer"">https://huggingface.co/docs/hub/oauth</a>. I have done following but not sure what mistake I am doing. Please help me out to print email and profile details. TIA!</p>
<p>Note: I have set the URL (http://localhost/ai/) and callback (http://localhost/ai/hf) in my developer application:</p>
<p>I have following code:</p>
<ol>
<li>huggingface.php (http://localhost/ai/huggingface.php)</li>
</ol>
<pre><code>$clientId = '37779938-b...';
$clientSecret = '098260e6-1...';
$redirectUri = 'http://localhost/ai/hf';

// Step 1: Redirect user to Hugging Face's authorization page with desired scopes
$authorizationUrl = 'https://huggingface.co/oauth/authorize';
$authorizationUrl .= '?client_id=' . $clientId;
$authorizationUrl .= '&amp;redirect_uri=' . $redirectUri;
$authorizationUrl .= '&amp;response_type=code';
$authorizationUrl .= '&amp;scope=openid%20profile%20email'; // Adjust the scope as needed
$authorizationUrl .= '&amp;state=' . bin2hex(random_bytes(16));

header('Location: ' . $authorizationUrl);
exit;

// The user will be redirected back to your specified redirect URI with an authorization code.
?&gt;
</code></pre>
<p>and hf.php</p>
<pre><code>&lt;?php
$clientId = '37779938-b...';
$clientSecret = '098260e6-1...';
$redirectUri = 'http://localhost/ai/hf';

// Step 2: Handle the callback and verify the state parameter
if (isset($_GET['code']) &amp;&amp; isset($_GET['state'])) {
    $state = $_GET['state'];
    $code = $_GET['code'];

    // Verify the state parameter to prevent CSRF attacks
    // Perform your state validation here

    // Step 3: Use the code to get an access token and id token
    $tokenUrl = 'https://huggingface.co/oauth/token';
    $tokenData = [
        'client_id' =&gt; $clientId,
        'client_secret' =&gt; $clientSecret,
        'redirect_uri' =&gt; $redirectUri,
        'code' =&gt; $_GET['code'],
        'grant_type' =&gt; 'authorization_code',
    ];

    $options = [
        'http' =&gt; [
            'header' =&gt; &quot;Content-type: application/x-www-form-urlencoded\r\n&quot;,
            'method' =&gt; 'POST',
            'content' =&gt; http_build_query($tokenData),
        ],
    ];

    $context = stream_context_create($options);
    $response = @file_get_contents($tokenUrl, false, $context);

    if ($response === false) {
        die(&quot;Failed to make the token request. Check your server configuration.&quot;);
    }

    $tokens = json_decode($response, true);

    if (json_last_error() !== JSON_ERROR_NONE) {
        die(&quot;Failed to decode token JSON. Error: &quot; . json_last_error_msg());
    }

    if ($tokens !== null) {
        // Step 4: Use the access token to fetch user data
        $userInfoUrl = 'https://huggingface.co/oauth/userinfo';
        $userInfoOptions = [
            'http' =&gt; [
                'header' =&gt; &quot;Authorization: Bearer &quot; . $tokens['access_token'] . &quot;\r\n&quot;,
                'method' =&gt; 'GET',
            ],
        ];

        $userInfoContext = stream_context_create($userInfoOptions);
        $userInfoResponse = @file_get_contents($userInfoUrl, false, $userInfoContext);

        if ($userInfoResponse === false) {
            die(&quot;Failed to make the user data request.&quot;);
        }

        // Process the user profile data
        $userData = json_decode($userInfoResponse, true);

        if (json_last_error() !== JSON_ERROR_NONE) {
            die(&quot;Failed to decode user data JSON. Error: &quot; . json_last_error_msg());
        }

        echo '&lt;pre&gt;';
        print_r($userData);
        echo '&lt;/pre&gt;';
    } else {
        die(&quot;Failed to decode token JSON.&quot;);
    }
} else {
    echo &quot;Authorization code or state is missing.&quot;;
}
?&gt;
</code></pre>
<p><strong>I'm getting following error in hf.php</strong></p>
<blockquote>
<p>Failed to make the token request. Check your server configuration.</p>
</blockquote>
","huggingface"
"77854864","Trouble Saving a Hugging Face Dimensionality Reduced Model and Converting in ONNX for Use in NodeJS","2024-01-21 12:51:37","","0","72","<node.js><huggingface>","<p>I am trying to create word embeddings using Hugging face <strong>all-MiniLM-L6-v2</strong> pre-trained model. The vector length is 384 but I need to reduce the dimensionality for time improvement in similarity search from 384 to 128.</p>
<p>The implementation in python was straight-forward but when I tried in NodeJS (Since my codebase is in NodeJs) I got stuck. Used Xenova Transformers (A hugging Face NodeJS implementation) but I am unable to load the custom 128dim model.</p>
<p>Note: Since the implementation of dimensionality reduction in python is easy pesy and I have already done and tested it there hence I just want to use the model saved in python implementation and use it in NodeJS.</p>
<p>Here is what I did:</p>
<p>1.Generated word embeddings using sentence transformer (all-MiniLM-L6-v2):</p>
<pre><code>#Model for which we apply dimensionality reduction
model = SentenceTransformer('all-MiniLM-L6-v2')
#New size for the embeddings
new_dimension = 128
######## Reduce the embedding dimensions ########
pca_train_sentences = ls
train_embeddings = model.encode(pca_train_sentences, convert_to_numpy=True)
#Compute PCA on the train embeddings matrix
pca = PCA(n_components=new_dimension)
pca.fit(train_embeddings)
pca_comp = np.asarray(pca.components_)

# We add a dense layer to the model, so that it will produce directly embeddings with the new size
dense = models.Dense(in_features=model.get_sentence_embedding_dimension(), out_features=new_dimension, bias=False, activation_function=torch.nn.Identity())
dense.linear.weight = torch.nn.Parameter(torch.tensor(pca_comp))
model.add_module('dense', dense)
model.save('models/my-128dim-model-local')
</code></pre>
<ol start=""2"">
<li><p>Performed Dimensionality Reduced to get vector length of 128 instead of 384, followed <a href=""https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/distillation/dimensionality_reduction.py"" rel=""nofollow noreferrer"">Dimensionality Reduction</a></p>
</li>
<li><p>I had to use this model from within Node JS, found the implementation <a href=""https://github.com/xenova/transformers.js"" rel=""nofollow noreferrer"">Xenova Transformers</a></p>
</li>
</ol>
<pre><code>import { env } from '@xenova/transformers';
env.localModelPath = 'local/';
env.allowRemoteModels = false;
import { pipeline } from '@xenova/transformers';
const extractor = await pipeline('feature-extraction', 'my-128dim-model');
const sentences = ['This is an example sentence', 'Each sentence is converted'];
const output = await extractor(sentences, { pooling: 'mean', normalize: true });
console.log(output);
</code></pre>
<ol start=""4"">
<li>I need to convert my dimensionality reduced model to onnx as briefed so I can it in above, so I followed the in instruction here: Convert To Onnx. Ran the following script:</li>
</ol>
<pre><code>python -m scripts.convert --quantize --model_id ../my-128dim-model --task feature-extraction
</code></pre>
<p>This works but I am not getting a 128 dim vector but 384 still. What am I doing wrong? Although when I try to convert a pretrained model from hugging face lib it works perfectly.</p>
","huggingface"
"77854583","Continual training of a pre-trained LLM via LoRa with xTuring","2024-01-21 11:23:29","","0","320","<huggingface-transformers><huggingface><pre-trained-model><huggingface-datasets><llama>","<p>I came across the <a href=""https://github.com/stochasticai/xturing"" rel=""nofollow noreferrer"">xTuring library</a> reading this <a href=""https://www.stochastic.ai/blog/xfinance-vs-bloomberg-gpt"" rel=""nofollow noreferrer"">article on the continual training of an LLM</a>. It claims that it continued the training of a pre-trained model in an unsupervised way with <code>xTuring</code> via the TextDataset. I'm struggling to understand why the TextDataset takes both a list of inputs and targets as logically it should just take a big block of text for further NSP training I think? Since it uses HuggingFace datasets and PyTorch Lightning under the hood, I'm wondering if this design is due to design patterns imposed by HuggingFace datasets.</p>
<pre><code>TextDataset
Here is how you can create this type of dataset:

From a python dictionary with the following keys:

text : List of strings representing the input text.
target : List of strings representing the target text.
from xturing.datasets.text_dataset import TextDataset

dataset = TextDataset({
    &quot;text&quot;: [&quot;first text&quot;, &quot;second text&quot;],
    &quot;target&quot;: [&quot;first text&quot;, &quot;second text&quot;]
})
</code></pre>
","huggingface"
"77852899","How to put hugging face model (e.g., smaller llama2) on bfloat 16 using the config?","2024-01-20 21:58:38","","0","182","<huggingface>","<p>Since I need a smaller model I need to use the config. But putting bfloat16n in the config didn't work e.g.,</p>
<pre class=""lang-py prettyprint-override""><code>def get_smaller_llama2(hidden_size : int = 2048, 
                       num_hidden_layers : int = 12, 
                       return_tokenizer: bool = False, 
                       verbose : bool = False,
                       ):
    config = AutoConfig.from_pretrained(&quot;meta-llama/Llama-2-7b-hf&quot;)
    config.hidden_size = hidden_size
    config.num_hidden_layers = num_hidden_layers
    smaller_model = AutoModelForCausalLM.from_config(config)
    # NOTE: putting torch_dtype in the config doesn't work, so you have to move the model to bfloat16 later with model.to(torch.bfloat16)
    config.torch_dtype = torch.bfloat16 if torch.cuda.get_device_capability(torch.cuda.current_device())[0] &gt;= 8 else torch.float32 # if &gt;= 8 ==&gt; brain float 16 available or set to True if you always want fp32
    device = torch.device(f&quot;cuda:{0}&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
    smaller_model = smaller_model.to(device)
    # smaller_model = smaller_model.to(torch_dtype) # does work!
    print(f'Model is currently on: {next(iter(smaller_model.parameters())).dtype=}')
    if verbose:
        print(f'config: {config}')
        print(&quot;Smaller number of parameters:&quot;, sum(p.numel() for p in smaller_model.parameters()))
        print(f'Model is currently on: {next(iter(smaller_model.parameters())).device=}')
        print(f'Model is currently on: {next(iter(smaller_model.parameters())).dtype=}')
        print()
    if return_tokenizer:
        tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf', padding_side=&quot;right&quot;, use_fast=False, trust_remote_code=True, use_auth_token=True)
        return smaller_model, tokenizer
    return smaller_model
</code></pre>
<p>how to fix?</p>
","huggingface"
"77842257","Finetune Sentence Transformer using Huggingface Trainer API","2024-01-18 20:33:08","","0","645","<huggingface><sentence-transformers><siamese-network>","<p>I want to fine tune a Sentence Transformer (For example MPNET) using Contrastive Learning. Is it possible to use the Huggingface Trainer API for this? If yes, how? Can you kindly guide me with some suggestion?</p>
<p>I went through the Trainer API docs and examples but could not actually find a support for Contrastive Learning or fine tuning embeddings. Most documents discuss primarily about Classification models.</p>
","huggingface"
"77840801","How can I add a new token to a given tokenizer and train it ONLY on a specific task?","2024-01-18 16:03:32","","0","78","<tokenize><large-language-model><huggingface>","<p>I want to add a new special token to a given model's tokenizer and optimize it ONLY on a specific downstream task.</p>
<p>This is my code so far:</p>
<pre><code>new_token = &quot;[SPECIAL]&quot;
num_added_tokens = tokenizer.add_tokens([new_token])
model.resize_token_embeddings(len(tokenizer))
# Unfreeze new token embeddings
model.gpt_neox.embed_in.weight[-num_added_tokens:].requires_grad
prefix = torch.nn.Embedding.from_pretrained(model.gpt_neox.embed_in.weight[-num_added_tokens:], freeze=False) #create learnble tokens
optimizer = optim.AdamW(prefix.parameters(), lr=1e-5)
</code></pre>
<p>I add the special token to each of the input texts in the batch, gets the loss</p>
<pre><code> loss = model(**tokenized_inputs, labels=padded_labels)[&quot;loss&quot;]
</code></pre>
<p>back propagates - the gradient is calculated but the step doesn't change the embedding of the newly added token.</p>
<p>can someone help me with that?</p>
","huggingface"
"77840257","Invocation of a Huggingface Summarization Model using AWS Servereless Sagemaker Endpoint","2024-01-18 14:44:33","77872209","0","264","<amazon-web-services><artificial-intelligence><amazon-sagemaker><large-language-model><huggingface>","<p>I am trying to run an AWS Serverless SageMaker Endpoint with the <code>huggingface-summarization-bert-small2bert-small-finetuned-cnn-daily-mail-summarization</code> model for simple text summarization.</p>
<p>This is my AWS CloudFormation template:</p>
<pre><code>SageMakerModel:
  Type: AWS::SageMaker::Model
  Properties:
    ModelName: SummarizationModel
    Containers:
      - Image: &quot;763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-inference:1.13.1-transformers4.26.0-cpu-py39-ubuntu20.04&quot;
        ModelDataUrl: &quot;s3://jumpstart-cache-prod-us-east-1/huggingface-infer/infer-huggingface-summarization-bert-small2bert-small-finetuned-cnn-daily-mail-summarization.tar.gz&quot;
        Mode: SingleModel
    ExecutionRoleArn: !GetAtt SageMakerExecutionRole.Arn


SageMakerEndpointConfig:
  Type: &quot;AWS::SageMaker::EndpointConfig&quot;
  Properties:
    ProductionVariants:
      - ModelName: !GetAtt SageMakerModel.ModelName
        VariantName: &quot;ServerlessVariant&quot;
        ServerlessConfig: 
          MaxConcurrency: 1
          MemorySizeInMB: 2048

SageMakerEndpoint:
  Type: &quot;AWS::SageMaker::Endpoint&quot;
  Properties:
    EndpointName: SummarizationEndpoint
    EndpointConfigName:
      !GetAtt SageMakerEndpointConfig.EndpointConfigName
</code></pre>
<p>The model is deployed successfully as far as I can tell.</p>
<p>I have deployed a Python lambda function to invoke the endpoint. This is my code:</p>
<pre><code>client = boto3.client('runtime.sagemaker')
payload = {
  'inputs': 'Summarize this text: This is a beautiful day. I am happy. I am going to the park.'
}

response = client.invoke_endpoint(
        EndpointName=&quot;SummarizationEndpoint&quot;, 
        ContentType=&quot;application/json&quot;, 
        Accept=&quot;application/json&quot;,
        Body=json.dumps(payload)
        # Body=bytes(json.dumps(payload), 'utf-8') # alternative attempt - not working
        # Body=json.dumps(payload).encode(&quot;utf-8&quot;) # alternative attempt - not working
    ) 
</code></pre>
<p>When I run this code I get the following error:</p>
<pre><code>An error occurred: An error occurred (ModelError) when calling the InvokeEndpoint operation: 
Received client error (400) from model with message &quot;
{ 
  &quot;code&quot;: 400, 
  &quot;type&quot;: &quot;InternalServerException&quot;, 
   &quot;message&quot;: &quot;\u0027str\u0027 object is not callable&quot;
}&quot;.
</code></pre>
<p>Since this is a <code>ModelError</code> I am assuming the model is deployed and the inference pipeline is being called. I am unsure about the payload format though.
Judging from the test code <a href=""https://github.com/aws/sagemaker-huggingface-inference-toolkit/blob/main/tests/integ/config.py"" rel=""nofollow noreferrer"">here</a> I am guessing that the text-to-be-summarized should be passed in the <code>inputs</code> property of the payload like it is being done <a href=""https://github.com/aws/sagemaker-huggingface-inference-toolkit/blob/80634b30703e8e9525db8b7128b05f713f42f9dc/tests/integ/config.py#L84"" rel=""nofollow noreferrer"">here</a>.
Looking at the <code>SummarizationPipeline</code> though, I don't quite understand the comments <a href=""https://github.com/huggingface/transformers/blob/818997788584b9fc043d8b58e078f63aadb6b60e/src/transformers/pipelines/text2text_generation.py#L250"" rel=""nofollow noreferrer"">here</a> - should there be a <code>documents</code> property somewhere? I played with all possible combinations of <code>inputs</code>, <code>documents</code>, etc but without success.</p>
<p>What is the correct way to pass the payload to the model? Can I see a working example?</p>
<p><strong>Update 1</strong>: These are the logs from CloudWatch when I use the version <code>payload = {'inputs':'...'}</code>:</p>
<pre><code>[INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File &quot;/opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py&quot;, line 1084, in __call__
[INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Prediction error
[INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 3
[INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Traceback (most recent call last):
[INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File &quot;/opt/conda/lib/python3.9/site-packages/sagemaker_huggingface_inference_toolkit/handler_service.py&quot;, line 234, in handle
[INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     response = self.transform_fn(self.model, input_data, content_type, accept)
[INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File &quot;/opt/conda/lib/python3.9/site-packages/sagemaker_huggingface_inference_toolkit/handler_service.py&quot;, line 190, in transform_fn
[INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     predictions = self.predict(processed_data, model)
[INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File &quot;/opt/conda/lib/python3.9/site-packages/sagemaker_huggingface_inference_toolkit/handler_service.py&quot;, line 158, in predict
[INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     prediction = model(inputs)
[INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File &quot;/opt/conda/lib/python3.9/site-packages/transformers/pipelines/text2text_generation.py&quot;, line 165, in __call__
[INFO ] W-9000-model ACCESS_LOG - /127.0.0.1:48184 &quot;POST /invocations HTTP/1.1&quot; 400 3416
[INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     result = super().__call__(*args, **kwargs)
[INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File &quot;/opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py&quot;, line 1084, in __call__
[INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)
[INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File &quot;/opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py&quot;, line 1090, in run_single
[INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     model_inputs = self.preprocess(inputs, **preprocess_params)
[INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File &quot;/opt/conda/lib/python3.9/site-packages/transformers/pipelines/text2text_generation.py&quot;, line 175, in preprocess
[INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     inputs = self._parse_and_tokenize(inputs, truncation=truncation, **kwargs)
[INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File &quot;/opt/conda/lib/python3.9/site-packages/transformers/pipelines/text2text_generation.py&quot;, line 130, in _parse_and_tokenize
[INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     inputs = self.tokenizer(*args, padding=padding, truncation=truncation, return_tensors=self.framework)
[INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - TypeError: 'str' object is not callable
</code></pre>
<p>I looked into the code of <code>handler_service.py</code>. As the line 158 is executed, it means this payload successfully passes line 151</p>
<pre><code>        inputs = data.pop(&quot;inputs&quot;, data)
</code></pre>
<p>... which confirms that <code>inputs</code> must be the property name.</p>
<p>However, looking further into the stacktrace I couldn't find anything interesting. The inputs are being passed to the <a href=""https://github.com/huggingface/transformers/blob/820c46a707ddd033975bc3b0549eea200e64c7da/src/transformers/pipelines/text2text_generation.py#L130"" rel=""nofollow noreferrer""><code>tokenizer</code></a> and this is where my stacktrace ends.</p>
<p><strong>Update 2</strong>: I noticed that the same invocation code works with another model. Here's the model yaml that does work:</p>
<pre><code>SageMakerModel2:
  Type: AWS::SageMaker::Model
  Properties:
    ModelName: SummarizationModel2
    Containers:
      - Image: &quot;763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-inference:1.7.1-transformers4.6.1-cpu-py36-ubuntu18.04&quot;
        ModelDataUrl: &quot;s3://jumpstart-cache-prod-us-east-1/huggingface-infer/infer-huggingface-translation-t5-small.tar.gz&quot;
        Mode: SingleModel
    ExecutionRoleArn: !GetAtt SageMakerExecutionRole.Arn
</code></pre>
<p>After further analysis I learned that the multi-model-server calls <code>handler_service.initialize</code> when loading the model to create a pipeline using the <code>pipeline()</code> function.</p>
<p>I then downloaded both models and tried to instantiate a pipeline on my machine from both models to see what happens to the tokenizer. Here is the code...</p>
<pre><code># p1 model is not working
p1 = pipeline(&quot;summarization&quot;, &quot;/REDACTED/Code/infer-huggingface-summarization-bert-small2bert-small-finetuned-cnn-daily-mail-summarization&quot;)

# p2 model is working
p2 = pipeline(&quot;text2text-generation&quot;, &quot;/REDACTED/Code/infer-huggingface-translation-t5-small/&quot;)
print(&quot;Tokenizer for P1: &quot; + str(type(p1.tokenizer)))
print(&quot;Tokenizer for P2: &quot; + str(type(p2.tokenizer)))
</code></pre>
<p>The code proves that <code>p1.tokenizer</code> is of <code>NoneType</code> whereas <code>p2.tokenizer</code> is of class <code>'transformers.models.t5.tokenization_t5_fast.T5TokenizerFast'</code>.</p>
<p>After further investigating the code of <code>pipeline()</code> function I found that in this line ...</p>
<pre><code>load_tokenizer = type(model_config) in TOKENIZER_MAPPING or model_config.tokenizer_class is not None
</code></pre>
<p>... <code>load_tokenizer</code> is set to <code>False</code> for <code>p1</code> because
<code>type(model_config)</code> is not found in <code>TOKENIZER_MAPPING</code> whereas load_tokenizer is <code>True</code> for <code>p2</code> because it was found. (See <a href=""https://github.com/huggingface/transformers/blob/c475eca9cd9aa0b5a88b269b6a090b645391267d/src/transformers/pipelines/__init__.py#L882"" rel=""nofollow noreferrer"">here</a> and <a href=""https://github.com/huggingface/transformers/blob/c475eca9cd9aa0b5a88b269b6a090b645391267d/src/transformers/models/auto/tokenization_auto.py#L473"" rel=""nofollow noreferrer"">here</a>).
I am not sure though if this finding is relevant as the <code>model_fn()</code> function in the model's <code>inference.py</code> does create a <code>tokenizer</code> by using <code>tokenizer = AutoTokenizer.from_pretrained(model_dir)</code> and then passes it to <code>SummarizationPipeline</code>. I tired to create a tokenizer this way locally ...</p>
<pre><code>tokenizer = AutoTokenizer.from_pretrained(&quot;/REDACTED/Code/infer-huggingface-summarization-bert-small2bert-small-finetuned-cnn-daily-mail-summarization&quot;)
</code></pre>
<p>print(str(type(tokenizer)))</p>
<p>... and I do get an instance of type <code>transformers.models.bert.tokenization_bert_fast.BertTokenizerFast</code>.</p>
<p>(I have to admit that I did not fully grasp everything that's going on here but continuing investigation...)</p>
","huggingface"
"77840194","How do I get the training loss from the callback function of the Trainer in huggingface","2024-01-18 14:35:48","","0","363","<python><deep-learning><nlp><huggingface>","<p>This is my code.This code can be run directly.</p>
<pre class=""lang-py prettyprint-override""><code>from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding
from transformers import TrainingArguments
from transformers.trainer_callback import DefaultFlowCallback, TrainerCallback, TrainerControl, TrainerState
import numpy as np
import evaluate
from transformers import AutoModelForSequenceClassification
from transformers import Trainer
from loguru import logger
import json

logger.add(&quot;train_log.log&quot;)

raw_dataset = load_dataset(&quot;glue&quot;, &quot;mrpc&quot;)
checkpoint = &quot;bert-base-uncased&quot;
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)

def tokenizer_function(example):
    return tokenizer(example[&quot;sentence1&quot;], example[&quot;sentence2&quot;], truncation=True)
tokenizer_datasets = raw_dataset.map(tokenizer_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer)

metric = evaluate.load(&quot;glue&quot;, &quot;mrpc&quot;)

def compute_metrics(eval_preds):
    logist, labels = eval_preds
    predictions = np.argmax(logist, axis=-1)
    return metric.compute(predictions=predictions, references=labels)

training_args = TrainingArguments(&quot;test-trainer&quot;,
                                  overwrite_output_dir=&quot;test-trainer&quot;, 
                                  num_train_epochs=2,
                                  evaluation_strategy=&quot;epoch&quot;,
                                  save_strategy=&quot;epoch&quot;)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)

class CustomCallback(TrainerCallback):
    def on_train_begin(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):
        logger.info(f&quot;params:{vars(args)}&quot;)
    
    def on_epoch_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):
        # model = kwargs[&quot;model&quot;]
        # logger.info(json.dumps(state.log_history[-1], ensure_ascii=False))
        pass
    
    def on_evaluate(self, args, state, control, **kwargs):
        logger.info(json.dumps(state.log_history[-1], ensure_ascii=False))

trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenizer_datasets[&quot;train&quot;],
    eval_dataset=tokenizer_datasets[&quot;validation&quot;],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
    callbacks=[CustomCallback]
)
train_result = trainer.train()
</code></pre>
<p>In the on_epoch_end function, state is an empty list. In on_evaluate, it records the loss and accuracy of the evaluation. I want to also record train loss during the training phase. How do I need to modify it?</p>
<p>environment：
macbook2022-air</p>
<p>python 3.9.6
transformers 4.36.2
datasets  2.16.1</p>
<p>I tried removing compute_metrics=compute_metrics, but the loss is still recorded. I checked the official documentation. He only wrote a few callback functions, but I couldn't find a way to record train loss.</p>
","huggingface"
"77836822","Why do I get UnboundLocalError: local variable 'batch_idx' referenced before assignment when using interleaved data sets with Hugging Face (HF)?","2024-01-18 04:00:40","","0","47","<machine-learning><huggingface-transformers><huggingface><huggingface-datasets><huggingface-trainer>","<p>I get the following error:</p>
<pre><code>Exception has occurred: UnboundLocalError
local variable 'batch_idx' referenced before assignment
  File &quot;/lfs/skampere1/0/brando9/beyond-scale-language-data-diversity/src/training/utils.py&quot;, line 254, in _test_train_dataset_setup_for_main_code
    print(f'{len(next(iter(batch))[&quot;input_ids&quot;])=}')
  File &quot;/lfs/skampere1/0/brando9/beyond-scale-language-data-diversity/src/training/utils.py&quot;, line 263, in &lt;module&gt;
    _test_train_dataset_setup_for_main_code()
UnboundLocalError: local variable 'batch_idx' referenced before assignment
</code></pre>
<p>it happens when I interleave my data set:</p>
<pre><code>    raw_train_datasets = load_dataset(path[0], name[0], data_files=data_files[0], streaming=streaming, split=split[0]).with_format(&quot;torch&quot;)
    get_data_from_hf_dataset(raw_train_datasets, streaming=streaming, batch_size=batch_size) 
    remove_columns = get_column_names(raw_train_datasets)  # remove all keys that are not tensors to avoid bugs in collate function in task2vec's pytorch data loader
    # - Get tokenized train data set
    # Note: Setting `batched=True` in the `dataset.map` function of Hugging Face's datasets library processes the data in batches rather than one item at a time, significantly speeding up the tokenization and preprocessing steps.
    tokenize_function = lambda examples: tokenizer(examples[&quot;text&quot;])
    tokenized_train_datasets = raw_train_datasets.map(tokenize_function, batched=True, remove_columns=remove_columns)
    block_size: int = tokenizer.model_max_length
    _group_texts = lambda examples : group_texts(examples, block_size)
    # - Get actual data set for lm training (in this case each seq is of length block_size, no need to worry about pad = eos since we are filling each sequence)
    lm_train_dataset = tokenized_train_datasets.map(_group_texts, batched=True)
    batch = get_data_from_hf_dataset(lm_train_dataset, streaming=streaming, batch_size=batch_size)
    # for data_dict in iter(batch):
    #     seq = data_dict['input_ids']
    #     print(len(seq))
    print(f'{len(next(iter(batch))[&quot;input_ids&quot;])=}')
</code></pre>
<p>why is this happening?</p>
<p>Full code:</p>
<pre><code>&quot;&quot;&quot;
todo:
    - finish passing the HF block_size tokenization code here so its modular
    - add function to our train code train.py
    - print the sequence length of the data once we include this code
    - create a unit test here to test block size
    - use the re-init code smart ally &amp; brando wrote
&quot;&quot;&quot;
from itertools import chain
import random

import torch

import datasets
from datasets import load_dataset, interleave_datasets

from transformers import PreTrainedTokenizer, AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, AutoConfig
from transformers.testing_utils import CaptureLogger

def get_num_steps():
    # dataset_size: int = int(1.5e12)  # TODO, doesn't seem easy to solve. Either count all the sequennces/rows or have the meta data have this. Or make this number huge. 
    # dataset_size: int = train_dataset.num_rows
    # dataset_size: int = len(train_dataset)
    # TODO dataset.info['split']['train']['num_examples']
    # dataset_size = sum(len(dataset) for dataset in datasets)  # TODO: works on with streaming = False?
    # dataset_size = sum(dataset.cardinality() for dataset in datasets)
    pass

def get_size_of_seq_len(dataset_or_batch, verbose: bool = True, streaming: bool = True, batch_size: int = 2) -&gt; int:
    &quot;&quot;&quot;Print size of a sequence length in a batch. Give a hf data set obj (batches are data set objs sometimes).&quot;&quot;&quot;
    batch = get_data_from_hf_dataset(dataset_or_batch, streaming=streaming, batch_size=batch_size)
    size_seq_len = len(next(iter(batch))[&quot;input_ids&quot;])
    if verbose:
        print(f'{size_seq_len=}')
        print(f'{len(next(iter(batch))[&quot;input_ids&quot;])=}')
    return size_seq_len

def get_column_names(dataset, 
                    #   split: str = 'train',
                      method: str = 'features', 
                      streaming: bool = True,
                      ):
    if method == 'features':
        # column_names = list(dataset[spit].features)
        column_names = list(dataset.features)
    elif method == 'keys':
        batch = get_data_from_hf_dataset(dataset, streaming=streaming, batch_size=1)
        column_names = next(iter(batch)).keys()
        # column_names = next(iter(dataset)).keys()
    else:
        raise ValueError(f&quot;method {method} not supported&quot;)
    return column_names

def get_data_from_hf_dataset(dataset, 
                             streaming: bool = True, 
                             batch_size: int = 4, 
                             shuffle: bool= False, # shuffle is better but slower afaik
                             seed: int = 0, 
                             buffer_size: int = 500_000,
                             ):
    &quot;&quot;&quot; Gets data from a HF dataset, it's usually an iterator object e.g., some ds.map(fn, batched=True, remove_columns=remove_columns) has been applied. 
    Handles both streaming and non-streaming datasets, take for streaming and select for non-streaming.
    &quot;&quot;&quot;
    # sample_data = dataset.select(range(batch_size)) if not isinstance(dataset, datasets.iterable_dataset.IterableDataset) else dataset.take(batch_size)
    batch = dataset.take(batch_size) if streaming else dataset.select(random.sample(list(range(len(dataset))), batch_size))
    return batch

def _tokenize_function(examples, tokenizer, tok_logger, text_column_name: str):
    &quot;&quot;&quot;
    
    To use do:
    tokenizer = ...obtained from your model... 
    tokenize_function = lambda examples: tokenize_function(examples, tokenizer=tokenizer) 
    tokenized_datasets = raw_datasets.map(
            tokenize_function,
            batched=True,
            remove_columns=column_names,
        )
    &quot;&quot;&quot;
    with CaptureLogger(tok_logger) as cl:
        output = tokenizer(examples[text_column_name])
    # clm input could be much much longer than block_size
    if &quot;Token indices sequence length is longer than the&quot; in cl.out:
        tok_logger.warning(
            &quot;^^^^^^^^^^^^^^^^ Please ignore the warning above - this long input will be chunked into smaller bits&quot;
            &quot; before being passed to the model.&quot;
        )
    return output

def tokenize_function(examples, tokenizer, text_column_name: str):
    &quot;&quot;&quot; 
    creates a tokenize function that can be used in HF's map function and you specify which text column to tokenize.
    
    Assumes batched=True so examples is many row/data points.
    &quot;&quot;&quot;
    return tokenizer(examples[&quot;text_column_name&quot;])

def preprocess(examples, tokenizer, max_length: int = 1024):
    return tokenizer(examples[&quot;text&quot;], padding=&quot;max_length&quot;, max_length=max_length, truncation=True, return_tensors=&quot;pt&quot;)
    # return tokenizer(examples[&quot;text&quot;], padding=&quot;max_length&quot;, max_length=model.config.context_length, truncation=True, return_tensors=&quot;pt&quot;)

def group_texts(examples, # if batched=True it's a dict of input_ids, attention_mask, labels of len(examples['input_ids']) = 1000 
                block_size: int,  # 4096, 1024
                ):
    &quot;&quot;&quot;
    tokenizer = ...obtained from your model... 
    tokenize_function = lambda examples: tokenize_function(examples, tokenizer=tokenizer) 
    tokenized_datasets = raw_datasets.map(
            tokenize_function,
            batched=True,
            remove_columns=column_names,
        )

    # Note that with `batched=True`, this map processes 1,000 texts together, so group_texts throws away a remainder
    # for each of those groups of 1,000 texts. You can adjust that batch_size here but a higher value might be slower
    # to preprocess.
    #
    # To speed up this part, we use multiprocessing. See the documentation of the map method for more information:
    # https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map    
    &quot;&quot;&quot;
    # Concatenate all texts.
    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}
    total_length = len(concatenated_examples[list(examples.keys())[0]])
    # We drop the small remainder, and if the total_length &lt; block_size  we exclude this batch and return an empty dict.
    # We could add padding if the model supported it instead of this drop, you can customize this part to your needs.
    total_length = (total_length // block_size) * block_size
    # Split by chunks of max_len.
    result = {
        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]
        for k, t in concatenated_examples.items()
    }
    result[&quot;labels&quot;] = result[&quot;input_ids&quot;].copy()
    return result

def collate_fn_train_only_first_eos_token_mask_everything_after_it(data: list[dict[str, str]], 
                                                                   tokenizer: PreTrainedTokenizer, 
                                                                   max_length: int=1024,  # GPT2 default, likely worth you change it! This default might cause bugs.
                                                                   ) -&gt; dict[str, torch.Tensor]:
    &quot;&quot;&quot; Train only on first occurence of eos. The remaining eos are masked out.

    Sometimes the model might not have a padding token. Sometimes people set the padding token to be the eos token.
    But sometimes this seems to lead to the model to predict eos token to much. 
    So instead of actually using the pad token that was set to the eos token, we instead mask out all excesive eos tokens that act as pads 
    and leave the first eos token at the end to be predicted -- since that is the only one that semantically means end of sequence 
    and therby by not training on random eos at the end by masking it not unncesserily shift/amplify the distribution of eos. 
    
    ref: https://discuss.huggingface.co/t/why-does-the-falcon-qlora-tutorial-code-use-eos-token-as-pad-token/45954/13?u=brando 
    ref: https://chat.openai.com/share/02d16770-a1f3-4bf4-8fc2-464286daa8a1
    ref: https://claude.ai/chat/80565d1f-ece3-4fad-87df-364ce57aec15 on when to call .clone()
    ref: https://stackoverflow.com/questions/76633368/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoi
    &quot;&quot;&quot;
    # we are training full context length for llama so remove code bellow, if it tries to pad hopefully it throws an error
    # -- Ensure tokenizer has a padding token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    # -- Extract sequences
    # sequences: list[str] = [example.get(&quot;text&quot;, &quot;&quot;) or &quot;&quot; for example in data]
    sequences: list[str] = []
    for idx, example in enumerate(data):
        # Retrieve the value for &quot;text&quot; from the dictionary or default to an empty string if not present or falsy. ref: https://chat.openai.com/share/bead51fe-2acf-4f05-b8f7-b849134bbfd4
        text: str = example.get(&quot;text&quot;, &quot;&quot;) or &quot;&quot;
        sequences.append(text)
    # -- Tokenize the sequences
    tokenized_data = tokenizer(sequences, padding=&quot;max_length&quot;, max_length=max_length, truncation=True, return_tensors=&quot;pt&quot;)
    tokenized_data[&quot;labels&quot;] = tokenized_data[&quot;input_ids&quot;].clone()  # labels is hardcoded in HF so put it!
    # -- Set the mask value for the first eos_token in each sequence to 1 and remaining to -100
    eos_token_id = tokenizer.eos_token_id
    for idx, input_ids in enumerate(tokenized_data[&quot;input_ids&quot;]):
        # Find all occurrences of eos_token
        eos_positions = (input_ids == eos_token_id).nonzero(as_tuple=True)[0]
        if eos_positions.nelement() &gt; 0:  # Check if eos_token is present
            first_eos_position = eos_positions[0]
            tokenized_data[&quot;attention_mask&quot;][idx, first_eos_position] = 1  # Set the mask value to 1
            
            # Assert that the label for the first occurrence of eos_token is eos_token_id
            assert tokenized_data[&quot;labels&quot;][idx, first_eos_position] == eos_token_id, &quot;The label for the first eos_token is incorrect!&quot;
            
            # For all subsequent occurrences of eos_token, set their labels to -100
            for subsequent_eos_position in eos_positions[1:]:
                tokenized_data[&quot;labels&quot;][idx, subsequent_eos_position] = -100
                assert tokenized_data[&quot;labels&quot;][idx, subsequent_eos_position] == -100, &quot;The label for the subsequent_eos_position incorrect! Should be -100.&quot;
    return tokenized_data

# -- unit tests -- #

def _test_all_batches_are_size_block_size():
    batch_size = 4
    # get gpt2 tokenizer
    tokenizer = AutoTokenizer.from_pretrained(&quot;gpt2&quot;)
    tokenize_function = lambda examples: tokenizer(examples[&quot;text&quot;])
    # load c4 data set hf in streaming mode 
    from datasets import load_dataset
    streaming = True
    raw_datasets = load_dataset(&quot;c4&quot;, &quot;en&quot;, streaming=streaming, split=&quot;train&quot;)
    get_data_from_hf_dataset(raw_datasets, streaming=streaming, batch_size=batch_size) 
    remove_columns = get_column_names(raw_datasets)  # remove all keys that are not tensors to avoid bugs in collate function in task2vec's pytorch data loader

    # how does it know which column to tokenize? gpt4 says default is text or your tokenized function can specify it, see my lambda fun above
    tokenized_datasets = raw_datasets.map(
        tokenize_function,
        batched=True,  # Setting `batched=True` in the `dataset.map` function of Hugging Face's datasets library processes the data in batches rather than one item at a time, significantly speeding up the tokenization and preprocessing steps.
        remove_columns=remove_columns,
    )
    get_data_from_hf_dataset(tokenized_datasets, streaming=streaming, batch_size=batch_size)
    _group_texts = lambda examples : group_texts(examples, block_size=tokenizer.model_max_length)
    lm_datasets = tokenized_datasets.map(
        _group_texts,
        batched=True,  # Setting `batched=True` in the `dataset.map` function of Hugging Face's datasets library processes the data in batches rather than one item at a time, significantly speeding up the tokenization and preprocessing steps.
    )
    get_data_from_hf_dataset(lm_datasets, streaming=streaming, batch_size=batch_size)

    # get batch
    batch = get_data_from_hf_dataset(lm_datasets, streaming=streaming, batch_size=batch_size)
    print(batch)
    for data_dict in iter(batch):
        seq = data_dict['input_ids']
        print(len(seq))
    print('Success!')

def _test_train_dataset_setup_for_main_code():
    import os
    batch_size = 2
    streaming = True
    # path, name, data_files, split = ['c4'], ['en'], [None], ['train']
    path, name, data_files, split = ['c4', 'c4'], ['en', 'en'], [None, None], ['train', 'validation']
    # path, name, data_files, split = ['csv'], [None], [os.path.expanduser('~/data/maf_data/maf_textbooks_csv_v1/train.csv')], ['train']
    # path, name, data_files, split = ['suolyer/pile_pile-cc'] + ['parquet'] * 4, [None] + ['hacker_news', 'nih_exporter', 'pubmed', 'uspto'], [None] + [urls_hacker_news, urls_nih_exporter, urls_pubmed, urls_uspto], ['validation'] + ['train'] * 4

    # -- Get tokenizer and model
    tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf', padding_side=&quot;right&quot;, use_fast=False, trust_remote_code=True, use_auth_token=True)
    # torch_dtype = torch.bfloat16 if torch.cuda.get_device_capability(torch.cuda.current_device())[0] &gt;= 8 else torch.float32  # if &gt;= 8 ==&gt; brain float 16 available or set to True if you always want fp32 
    # model = AutoModelForCausalLM.from_pretrained('meta-llama/Llama-2-7b-hf', trust_remote_code=True, torch_dtype=torch_dtype, use_auth_token=True)

    # -- Get train data set
    # train_datasets = [load_dataset(p, n, data_files=data_file, streaming=streaming, split=split).with_format(&quot;torch&quot;) for p, n, data_file, split in zip(path, name, data_files, split)]
    # probabilities = [1.0/len(train_datasets) for _ in train_datasets]  
    # # - Get raw train data set
    # raw_train_datasets = interleave_datasets(train_datasets, probabilities)
    raw_train_datasets = load_dataset(path[0], name[0], data_files=data_files[0], streaming=streaming, split=split[0]).with_format(&quot;torch&quot;)
    get_data_from_hf_dataset(raw_train_datasets, streaming=streaming, batch_size=batch_size) 
    remove_columns = get_column_names(raw_train_datasets)  # remove all keys that are not tensors to avoid bugs in collate function in task2vec's pytorch data loader
    # - Get tokenized train data set
    # Note: Setting `batched=True` in the `dataset.map` function of Hugging Face's datasets library processes the data in batches rather than one item at a time, significantly speeding up the tokenization and preprocessing steps.
    tokenize_function = lambda examples: tokenizer(examples[&quot;text&quot;])
    tokenized_train_datasets = raw_train_datasets.map(tokenize_function, batched=True, remove_columns=remove_columns)
    block_size: int = tokenizer.model_max_length
    _group_texts = lambda examples : group_texts(examples, block_size)
    # - Get actual data set for lm training (in this case each seq is of length block_size, no need to worry about pad = eos since we are filling each sequence)
    lm_train_dataset = tokenized_train_datasets.map(_group_texts, batched=True)
    batch = get_data_from_hf_dataset(lm_train_dataset, streaming=streaming, batch_size=batch_size)
    # for data_dict in iter(batch):
    #     seq = data_dict['input_ids']
    #     print(len(seq))
    print(f'{len(next(iter(batch))[&quot;input_ids&quot;])=}')
    assert all(len(data_dict['input_ids']) == block_size for data_dict in iter(batch)), f'Error, some seq in batch are not of length {block_size}'
    train_dataset = lm_train_dataset
    print(train_dataset)

if __name__ == &quot;__main__&quot;:
    from time import time
    start_time = time()
    _test_all_batches_are_size_block_size()
    _test_train_dataset_setup_for_main_code()
    print(f&quot;Done!\a Total time: {time() - start_time} seconds, or {(time() - start_time)/60} minutes. or {(time() - start_time)/60/60} hours.\a&quot;)
</code></pre>
<p>ref: <a href=""https://discuss.huggingface.co/t/why-do-i-get-unboundlocalerror-local-variable-batch-idx-referenced-before-assignment-when-using-interleaved-data-sets-with-hugging-face-hf/69573"" rel=""nofollow noreferrer"">https://discuss.huggingface.co/t/why-do-i-get-unboundlocalerror-local-variable-batch-idx-referenced-before-assignment-when-using-interleaved-data-sets-with-hugging-face-hf/69573</a>
ref: <a href=""https://discord.com/channels/879548962464493619/1197390550245068860/1197390550245068860"" rel=""nofollow noreferrer"">https://discord.com/channels/879548962464493619/1197390550245068860/1197390550245068860</a></p>
","huggingface"
"77835524","Issue In Speechbrain with Hugging face","2024-01-17 21:08:23","","0","287","<google-colaboratory><huggingface><huggingface-datasets><automatic-speech-recognition><speechbrain>","<p>I'm working on colab and I got this error while i was using  speechbrain.pretrained import EncoderDecoderASR
this is the data i used</p>
<pre><code>minds_14 = load_dataset(&quot;PolyAI/minds14&quot;, &quot;en-US&quot;,split=&quot;train&quot;)
</code></pre>
<p>this is the pretained model</p>
<pre><code>asr_model = EncoderDecoderASR.from_hparams(
   source=&quot;speechbrain/asr-crdnn-rnnlm-librispeech&quot;
)
</code></pre>
<p>this is where the problem occurs</p>
<pre><code>for i in tqdm(range(0,length)):
    input_speech = minds_14[i]['audio']
    t0= time.time()
    transcription = asr_model.transcribe_file(minds_14[i][&quot;path&quot;],repo_type=&quot;PolyAI/minds14&quot;)
    t+= (time.time() - t0)
    all_predictions.append(transcription)
</code></pre>
<p>the problem :</p>
<pre><code>HFValidationError                         Traceback (most recent call last)
&lt;ipython-input-33-daf03578985d&gt; in &lt;cell line: 8&gt;()
      9     input_speech = minds_14[i]['audio']
     10     t0= time.time()
---&gt; 11     transcription = asr_model.transcribe_file(minds_14[i][&quot;path&quot;])
     12     t+= (time.time() - t0)
     13     all_predictions.append(transcription)

4 frames
/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py in validate_repo_id(repo_id)
    156 
    157     if repo_id.count(&quot;/&quot;) &gt; 1:
--&gt; 158         raise HFValidationError(
    159             &quot;Repo id must be in the form 'repo_name' or 'namespace/repo_name':&quot;
    160             f&quot; '{repo_id}'. Use `repo_type` argument if needed.&quot;

HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/storage/hf-datasets-cache/all/datasets/51125457981586-config-parquet-and-info-PolyAI-minds14-efce24e3/downloads/extracted/cfd42a9443ffb9548ee39e3c64f8b512ca72b9ce5e2ea6b981d44ba6c7265ae8/en-US~JOINT_ACCOUNT'. Use `repo_type` argument if needed.
</code></pre>
<p>I tried to transcribe text with speechbrain.pretained</p>
","huggingface"
"77831837","Loading shrads of llama 2 when inferencing it by huggingface takes too long","2024-01-17 10:49:41","","0","272","<python><huggingface-transformers><huggingface><llama>","<p>It happens when I trying to inference llama2. The following is my code:</p>
<pre><code>from huggingface_hub import login
from transformers import AutoTokenizer
import transformers
import torch

access_token = 'my_access_token'

login(token = access_token)

# Hugging face repo name
model = &quot;meta-llama/Llama-2-7b-chat-hf&quot; #chat-hf (hugging face wrapper version)

tokenizer = AutoTokenizer.from_pretrained(model)

pipeline = transformers.pipeline(
    task = &quot;text-generation&quot;,
    model=model,
    torch_dtype=torch.float16,
    low_cpu_mem_usage=True,
    max_shard_size=&quot;200MB&quot;,
    device_map=&quot;auto&quot;
)

sequences = pipeline(
    'I liked &quot;Breaking Bad&quot; and &quot;Band of Brothers&quot;. Do you have any recommendations of other shows I might like?\n',
    do_sample=True,
    top_k=10,
    top_p = 0.9,
    temperature = 0.2,
    num_return_sequences=1,
    eos_token_id=tokenizer.eos_token_id,
    max_length=200, # can increase the length of sequence
)

for seq in sequences:
    print(f&quot;Result: {seq['generated_text']}&quot;)
</code></pre>
<p>And there is my environment:</p>
<pre><code>Package            Version
------------------ ----------
accelerate         0.26.1
bitsandbytes       0.42.0
Brotli             1.0.9
certifi            2023.11.17
cffi               1.16.0
charset-normalizer 2.0.4
cryptography       41.0.7
filelock           3.13.1
fsspec             2023.12.2
gmpy2              2.1.2
huggingface-hub    0.20.2
idna               3.4
Jinja2             3.1.2
MarkupSafe         2.1.3
mkl-fft            1.3.8
mkl-random         1.2.4
mkl-service        2.4.0
mpmath             1.3.0
networkx           3.1
numpy              1.26.3
opencv-python      4.9.0.80
packaging          23.2
Pillow             10.0.1
pip                23.3.1
psutil             5.9.7
pycparser          2.21
pyOpenSSL          23.2.0
PySocks            1.7.1
PyYAML             6.0.1
regex              2023.12.25
requests           2.31.0
safetensors        0.4.1
scipy              1.11.4
setuptools         68.2.2
sympy              1.12
tokenizers         0.15.0
torch              2.1.2
torchaudio         2.1.2
torchvision        0.16.2
tqdm               4.66.1
transformers       4.36.2
triton             2.1.0
typing_extensions  4.9.0
urllib3            1.26.18
wheel              0.41.2
</code></pre>
<p>GPU:
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.86.10              Driver Version: 535.86.10    CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA GeForce RTX 3090        On  | 00000000:01:00.0 Off |                  N/A |
| 44%   52C    P8              23W / 350W |     18MiB / 24576MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA GeForce RTX 3080        On  | 00000000:04:00.0 Off |                  N/A |
|  0%   42C    P8              22W / 320W |    299MiB / 10240MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
Python version:3.11.7<br />
There is no other problems but the low speed. It takes a hour to load the check point shards into gpu.</p>
<p>I don't why this happens and how to fix it. Just provide some thoughts and discuss about it!</p>
","huggingface"
"77829618","how to make my question and answer bot more detailed or accurate","2024-01-17 02:01:04","","0","39","<huggingface-transformers><huggingface><nlp-question-answering>","<p>I am using hugging face to make a question and answer bot there are a few questions I've entered and it gave a few word answers but I am unable to get the link or the full context to show, I was wondering if there is a way to make the bot more accurate, below is my code</p>
<pre><code>import streamlit as st
import transformers
from transformers import pipeline

def create_qa_bot():
    # Load the question-answering pipeline
    qa_pipeline = pipeline(&quot;question-answering&quot;)

    return qa_pipeline

def ask_question(context, question, qa_pipeline):
    # Use the pipeline to answer the question based on the context
    result = qa_pipeline(context=context, question=question)

    return result['answer']

def main():
    # Create the question-answering bot
    qa_bot = create_qa_bot()

    # Example context and questions
    context = &quot;To reset your password, go to the forget password link and a code will be sent to your email, then go to this link: www.google.com.&quot;

    st.title(&quot;Question Answering App&quot;)
    input_text = st.text_area(&quot;Question&quot;, value=&quot;Enter question here&quot;)
    
    # Ask questions and print answers
    if input_text == &quot;Enter question here&quot;:
        st.markdown(&quot;A:&quot;)
    else:
        answer = ask_question(context, input_text, qa_bot)
        st.markdown(f&quot;A: {answer}&quot;)

if __name__ == &quot;__main__&quot;:
    main()
</code></pre>
<p>Example input and output:<br/>
Input: How do I reset my password<br/>
Output: Go to the forget password link<br/></p>
<p>Input: Can I get the link to reset my password<br/>
Output: Go to the forget password link</p>
<p>Input: What should I do after getting a code to my email<br/>
Output: go to the forget password link</p>
<p>Input: What is the Eiffel tower<br/>
Output: forget password link</p>
","huggingface"
"77827215","I'm having problem running hugging face inferace API model","2024-01-16 16:15:20","","0","155","<huggingface><inference>","<p>I'm trying to use huggingface inferance API,
but I am getting this error:</p>
<blockquote>
<p>{&quot;error&quot;:&quot;Model /Emoji-Suggester is currently loading&quot;,&quot;estimated_time&quot;:29.511520385742188}</p>
</blockquote>
<p>How to fix this error?</p>
<p>I want to know how to use this inference API correctly</p>
","huggingface"
"77824515","convert a whisper-small.pt model to hugging face through convert_openai_to_hf.py","2024-01-16 08:58:47","","1","132","<python><pytorch><huggingface><openai-whisper>","<p>I have a problem I want to convert a file with the extension .pt to CTranslate2. To do that, you first need to convert it to hugging face and then execute a command line that will convert the file <code>whisper-small.pt</code> into faster whisper small.</p>
<p>I try to execute this command but receive an error. I don't really know which command to execute. I try this command line:</p>
<pre><code>python3 src/transformers/models/whisper/convert_openai_to_hf.py --checkpoint_path &quot;whisper-small.pt&quot; --pytorch_dump_folder_path &quot;openai/whisper-small&quot; --convert_prepro
</code></pre>
<p>I want to convert the model <code>whisper-small.pt</code> to HuggingFace through a command line. I have already installed all the relevant dependencies.</p>
","huggingface"
"77816652","How can I get my model output file after training has completed for this translation task?","2024-01-14 20:49:15","","0","327","<python><deep-learning><translation><huggingface-transformers><huggingface>","<p>I am working on a diacritization project for the Yoruba language. It is a Nigerian Language.</p>
<p>I am using HuggingFace <a href=""https://github.com/huggingface/transformers/blob/main/examples/pytorch/translation"" rel=""nofollow noreferrer"">codebase</a> to perform my work. The training, evaluation and prediction phases work well. However, I cannot see the model output anywhere. How can I get it? What should I do to get this file. I need to deploy it on a web server.</p>
<p>See the command I used to train the model:</p>
<pre class=""lang-bash prettyprint-override""><code>CUDA_VISIBLE_DEVICES=0 python run_translation.py --model_name_or_path Davlan/oyo-t5-small --do_train --do_eval --source_lang unyo --target_lang dcyo --source_prefix &quot;&lt;unyo2dcyo&gt;: &quot; --train_file data_prep_eng/output_data/bible_train.json --validation_file data_prep_eng/output_data/dev.json --test_file data_prep_eng/output_data/test.json --output_dir oyot5_small_unyo_dcyo_bible --max_source_length 512 --max_target_length 512 --per_device_train_batch_size=24 --per_device_eval_batch_size=24 --num_train_epochs 3 --overwrite_output_dir --predict_with_generate --save_steps 10000 --num_beams 10 --do_predict 
</code></pre>
<p>The <code>run_translation.py</code> file is just a direct <a href=""https://github.com/huggingface/transformers/blob/main/examples/pytorch/translation/run_translation.py"" rel=""nofollow noreferrer"">copy</a> from the HuggingFace repo.</p>
<p>is there a command I'm missing to get this model?</p>
<p>See the output of my model after it completed training, evaluate and predict steps</p>
<p><a href=""https://i.sstatic.net/NWO28.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/NWO28.png"" alt=""Model output directory"" /></a></p>
","huggingface"
"77815725","Unmasking adds an extra whitespace for BPE tokenizer","2024-01-14 16:03:00","","1","334","<python><huggingface-transformers><tokenize><huggingface><huggingface-tokenizers>","<p>I created a custom BPE tokenizer for pre-training a Roberta model, utilizing the following parameters (I tried to align it with the default parameters of BPE for RoBERTa.):</p>
<pre><code>from tokenizers.models import BPE
from tokenizers import ByteLevelBPETokenizer
from tokenizers.processors import RobertaProcessing

tokenizer = ByteLevelBPETokenizer()
tokenizer.normalizer = normalizers.BertNormalizer(lowercase = False)
tokenizer.train_from_iterator(Data_full, vocab_size = 50264, min_frequency = 2, special_tokens = [&quot;&lt;s&gt;&quot;, &quot;&lt;pad&gt;&quot;, &quot;&lt;/s&gt;&quot;, &quot;&lt;unk&gt;&quot;])
tokenizer.add_special_tokens([&quot;&lt;mask&gt;&quot;])
tokenizer.post_processor = RobertaProcessing(sep = (&quot;&lt;/s&gt;&quot;, 2), cls = (&quot;&lt;s&gt;&quot;, 0), trim_offsets = False, add_prefix_space = False)
tokenizer.enable_padding(direction = 'right', pad_id = 1, pad_type_id = 1, pad_token = &quot;&lt;pad&gt;&quot;, length = 512) 
   
</code></pre>
<p>When pre-training a Roberta model with this tokenizer, I observe unusual behavior during the unmasking process:</p>
<pre><code>from tokenizers import Tokenizer
from transformers import pipeline
from transformers import RobertaTokenizerFast
tokenizer_in = Tokenizer.from_file('tokenizer_file')
tokenizer_m = RobertaTokenizerFast(tokenizer_object=tokenizer_in, clean_up_tokenization_spaces=True) 
unmasker = pipeline('fill-mask', model=model_m, tokenizer = tokenizer_m)

unmasker(&quot;Capital of France is &lt;mask&gt;.&quot;)
</code></pre>
<p>The output consistently appears as follows: <code>Capital of France is  Paris</code>. I'm curious about the persistent extra space before 'Paris'. I believe activating the <code>clean_up_tokenization_spaces</code> option might resolve this. Could there be an error in my code leading to this issue? This happens for all unmasking tasks. Also, when I conduct a test with a command like <code>unmasker(&quot;Capital of France is&lt;mask&gt;.&quot;)</code>, the quality improves and the issue seems to be resolved.</p>
","huggingface"
"77812459","Permission denied saving recordings in Flask app on Hugging Face Spaces (""[Errno 13] Permission denied: 'temp/recording_.mp3'"")","2024-01-13 17:42:32","","0","98","<python><flask><space><permission-denied><huggingface>","<p>I'm encountering a &quot;permission denied&quot; error when saving recordings in my Flask app deployed on Hugging Face Spaces. I'd appreciate any guidance on resolving this issue.</p>
<p>Error message:</p>
<p>[Errno 13] Permission denied: 'temp/recording_.mp3'</p>
<p>Experimented with file paths: I attempted using absolute paths and different directory names to rule out path-related issues, but it didn't resolve the problem</p>
<p>Expected:</p>
<p>The audio_file.save(filename) function should successfully save the recording to the temp directory without any permission errors.</p>
<p>Actual Result:</p>
<p>The app throws a [Errno 13] Permission denied: 'temp/recording_.mp3' error when attempting to save the recording.</p>
<p>app/server.py:</p>
<pre><code>import os
os.environ['TRANSFORMERS_CACHE'] = &quot;./cache&quot;
from flask import Flask, request, jsonify, render_template
from flask_cors import CORS
import random
import glob
from modules import Module


UPLOAD_DIR = 'temp'
model = Module()
model.warmup(UPLOAD_DIR)

app = Flask(__name__)
CORS(app)

# Set path to temporary directory

# Ensure directory exists
if not os.path.exists(UPLOAD_DIR):
  os.makedirs(UPLOAD_DIR)


@app.route('/')
def home():
  print(&quot;Entering home function...&quot;)
  return render_template('index.html')


@app.route('/save_recording', methods=['POST'])
def save_recording():
  try:
    print(&quot;Received POST request to save recording...&quot;)
    audio_file = request.files['audio']
    filename = os.path.join(UPLOAD_DIR, 'recording_' + '.mp3')
    audio_file.save(filename)
    

    # Optionally trigger prediction with saved filename
    text, emotion = generate(filename)
    print(&quot;Recording saved successfully&quot;)
    return jsonify({'status': 'success', 'filename': filename, 'text': text, 'emotion': emotion})
  except Exception as e:
    return jsonify({'status': 'error', 'message': str(e)})


@app.route('/generate', methods=['GET'])
def generate(filename=None):
  if not filename:
    # Get latest recording if filename not provided
    list_of_files = glob.glob(os.path.join(UPLOAD_DIR, '*.mp3'))
    latest_file = max(list_of_files, key=os.path.getctime) if list_of_files else None
    if not latest_file:
      return jsonify({'status': 'error', 'message': 'No recorded audio found'})
    filename = latest_file

  # Perform prediction using specified filename
  text, emotion = model.predict(audio_path=filename, upload_dir=UPLOAD_DIR)
  
  print(&quot;text:&quot;, text)
  print(&quot;emotion:&quot;, emotion)

  return jsonify({'status': 'success', 'text': text, 'emotion': emotion})


if __name__ == '__main__':
  app.run(debug=True, port=7860, host='0.0.0.0')

</code></pre>
<p>`</p>
<p>app/modules/<strong>init</strong>.py</p>
<pre><code>import os
import time

from modules.emotion import Emotion
from modules.transcription import Transcription

transcription_model = os.getenv(&quot;TRANSCRIPTION_MODEL&quot;, &quot;/models/distil-medium.en/&quot;)
emotion_model = os.getenv(&quot;EMOTION_MODEL&quot;, &quot;/models/distilbert-base-uncased-go-emotions-student/&quot;)

transcription_obj = Transcription(model_name=transcription_model)
emotion_obj = Emotion(model_name=emotion_model)

class Module:
    
    def warmup(self, upload_dir: str):
        UPLOAD_DIR = '/home/adarsh/bardproject/app/temp' 
        text, emotion = self.predict(audio_path=os.path.join(upload_dir, 'recording_.mp3'), upload_dir=upload_dir)  # Pass upload_dir
        print(&quot;text: &quot;, text)
        print(&quot;emotion: &quot;, emotion)

    def predict(self, audio_path: str, upload_dir: str) -&gt; str:
        &quot;&quot;&quot;Loads audio, gets transcription and detects emotion
        Args:
            audio_path (str): path to the audio file
        Returns:
            str: emotion
        &quot;&quot;&quot;
        print(&quot;Getting transcription...&quot;)
        start_time = time.time()
        if text := transcription_obj.transcribe(audio_path=audio_path):
            print(&quot;Text: &quot;, text, time.time() - start_time)
            
            start_time = time.time()
            emotion = emotion_obj.detect_emotion(text=text)
            print(&quot;Emotion: &quot;, emotion, time.time() - start_time)
            return text, emotion
        return None
        

</code></pre>
","huggingface"
"77811002","How to make a trained Torch model Transformeres-compatible?","2024-01-13 09:59:33","77811830","1","76","<deep-learning><pytorch><huggingface-transformers><huggingface>","<p>I have trained and saved a PyTorch model with <code>torch.save</code>. Now I want to load it as <code>GPT2LMHeadModel</code>. <code>from_pretrained</code> method looks for a directory, and then a HuggingFace registry, none of which exists. I simply have a serialized PyTorch model and an <code>nn.Module</code> class. How to I integrate them with the Transformers library? Training from scratch is not option (takes too long).</p>
","huggingface"
"77810413","ValueError:Error raised by inference API:Input validation error:inputs tokens+max_new_tokens must be<=2048.Give2562 input tokens and 100 max_new_token","2024-01-13 05:31:57","","0","573","<python><langchain><large-language-model><huggingface><pinecone>","<pre><code>index=Pinecone.from_documents(doc,embeddings,index_name=index_name)
retriever = index.as_retriever(search_kwargs={&quot;k&quot;: 1})
llm = HuggingFaceHub(repo_id=&quot;google/flan-t5-xxl&quot;, model_kwargs={&quot;max_length&quot;:512})
from langchain.chains.qa_with_sources.retrieval import RetrievalQAWithSourcesChain
qa_chain = RetrievalQAWithSourcesChain.from_chain_type(llm=llm, chain_type=&quot;stuff&quot;, retriever=retriever, return_source_documents=True) 
result = qa_chain({&quot;question&quot;: &quot;What is Recurrent Neural Network?&quot;})
</code></pre>
<p>I am using Pinecone vector store , hugging face model flan-t5-xxl and RetrievalQA with sourceschain to get answer with sources of answer I have used 10 research papers as input documents but I keep getting this error</p>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[34], line 1
----&gt; 1 result = qa_chain({&quot;question&quot;: &quot;What is Recurrent Neural Network?&quot;})

File e:\Interview\Complete-Langchain-Tutorials\LLM Generic APP\LLM_Project\lib\site-packages\langchain\chains\base.py:316, in Chain.__call__(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)
    314 except BaseException as e:
    315     run_manager.on_chain_error(e)
--&gt; 316     raise e
    317 run_manager.on_chain_end(outputs)
    318 final_outputs: Dict[str, Any] = self.prep_outputs(
    319     inputs, outputs, return_only_outputs
    320 )

File e:\Interview\Complete-Langchain-Tutorials\LLM Generic APP\LLM_Project\lib\site-packages\langchain\chains\base.py:310, in Chain.__call__(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)
    303 run_manager = callback_manager.on_chain_start(
    304     dumpd(self),
    305     inputs,
    306     name=run_name,
    307 )
    308 try:
    309     outputs = (
--&gt; 310         self._call(inputs, run_manager=run_manager)
    311         if new_arg_supported
    312         else self._call(inputs)
...
    114 if self.client.task == &quot;text-generation&quot;:
    115     # Text generation return includes the starter text.
    116     text = response[0][&quot;generated_text&quot;][len(prompt) :]

ValueError: Error raised by inference API: Input validation error: `inputs` tokens + `max_new_tokens` must be &lt;= 2048. Given: 2562 `inputs` tokens and 100 `max_new_tokens`
</code></pre>
<p>I was expecting a simple answer with the sources</p>
","huggingface"
"77807764","llama.cpp conversion of finetuned HF ( huggingface ) fails for LLaMA2 - 7B model","2024-01-12 15:52:00","","0","350","<huggingface><llama><huggingface-trainer><llamacpp>","<p>i use the simple <a href=""https://github.com/huggingface/trl/blob/main/examples/scripts/sft.py"" rel=""nofollow noreferrer"">https://github.com/huggingface/trl/blob/main/examples/scripts/sft.py</a> with some custom data and llama-2-7b-hf as the base model. Post training , it invokes trainer.save_model and the output dir has the following contents</p>
<p>-rw-rw-r-- 1 ubuntu ubuntu 5100 Jan 12 14:04 README.md</p>
<p>-rw-rw-r-- 1 ubuntu ubuntu 134235048 Jan 12 14:04 adapter_model.safetensors</p>
<p>-rw-rw-r-- 1 ubuntu ubuntu 576 Jan 12 14:04 adapter_config.json</p>
<p>-rw-rw-r-- 1 ubuntu ubuntu 1092 Jan 12 14:04 tokenizer_config.json</p>
<p>-rw-rw-r-- 1 ubuntu ubuntu 552 Jan 12 14:04 special_tokens_map.json</p>
<p>-rw-rw-r-- 1 ubuntu ubuntu 1842948 Jan 12 14:04 tokenizer.json</p>
<p>-rw-rw-r-- 1 ubuntu ubuntu 4219 Jan 12 14:04 training_args.bin</p>
<p>-rw-rw-r-- 1 ubuntu ubuntu 4827151012 Jan 12 14:04 adapter_model.bin</p>
<p>as you can see it has no model.safetensors as required by convert.py .. i tried a bunch of other options to save the model ( trainer.model.save_pretrained , for example ) but the file was always adapter_model.safetensors.</p>
<p>i tried convert-hf-to-gguf.py as well and it too complains about model.safetensors ( and that too after suppressing the error which complains about causalLLAMA architecture not supported )</p>
<p>Is there any other convert script that handles such adapter safetensors ( i guess all models finetuned via peft will definitely be called adapter**_ ) ? when i went through the code i also noticed that the MODEL_ARCH only accomodates &quot;LLAMA&quot; and not &quot;LLAMA2&quot; ..is that why it also fails to find param names from adapter_safetensors in the MODEL_ARCH tmap methods ?</p>
","huggingface"
"77803708","Fine-tuning BERT for custom NER","2024-01-12 00:13:44","","0","424","<nlp><huggingface-transformers><bert-language-model><huggingface>","<p>I have domain specific data in a pandas dataframe and I am attempting to fine-tune BERT-NER for classification task.</p>
<p>Here's my data:</p>
<pre><code>from transformers import pipeline
import pandas as pd

# Load data
df = pd.DataFrame({'text': [&quot;The income statement shows&quot;, &quot;Total expenses for the quarter are&quot;, &quot;Net income is positive.&quot;]})

# Use a pre-trained NER model to predict entity labels
ner_pipeline = pipeline(&quot;ner&quot;, model=&quot;dslim/bert-base-NER&quot;)
df['entities'] = df['text'].apply(lambda text: ner_pipeline(text))
</code></pre>
<p>This returns an empty list of entities as it is trained to recognize <code>[PER], [LOC]</code> entities and not custom tags like <code>[INC]</code> or <code>[EXP]</code>.</p>
<p>How do I prepare the data/annotate and fine-tune the model for custom tags / class labels following the <code>B-I-O</code> labeling scheme?</p>
","huggingface"
"77799262","Why doesn't BERT give me back my original sentence?","2024-01-11 10:19:52","77801472","-1","129","<python><huggingface-transformers><bert-language-model><huggingface>","<p>I've started playing with <code>BERT</code> encoder through the <code>huggingface</code> module.
<a href=""https://i.sstatic.net/N6J8m.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/N6J8m.png"" alt=""enter image description here"" /></a></p>
<p>I passed it a normal unmasked sentence and got the following results:
<a href=""https://i.sstatic.net/GPUBZ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/GPUBZ.png"" alt=""enter image description here"" /></a></p>
<p>However, when I try to manually apply the softmax and decode the output:
<a href=""https://i.sstatic.net/6oZ2i.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/6oZ2i.png"" alt=""enter image description here"" /></a></p>
<p>I get back a bunch of unexpected <code>tensor(1012)</code> instead of my original sentence. BERT is an autoencoder, no?</p>
<p>Shouldn't it be giving me back the original sentence with fairly high probability since none of the input words was <code>[MASK]</code>? Can anyone explain to me what is going on?</p>
","huggingface"
"77790619","What is the default datatype for the emotions dataset (one of huggingface's site dataset)?","2024-01-10 02:17:58","","0","28","<python><formatting><format><huggingface><huggingface-datasets>","<p>I'm working with the emotions dataset from HuggingFace library.</p>
<p>I know I can switch its presentation format to pandas by doing:</p>
<pre><code>emotions.set_format(type='pandas')
</code></pre>
<p>And I can switch it back to its default presentation format by doing:</p>
<pre><code>emotions.reset_format()
</code></pre>
<p>My doubts are two: 1st) what types are allowed besides 'pandas'? and 2nd) what is the name of the default format? I tried looking on internet and on some books but found nothing.</p>
","huggingface"
"77790343","segmentation fault when using HuggingFaceEmbedding","2024-01-10 00:16:40","","2","340","<huggingface><llama-index>","<p>I have this code that I throwing me the error:&quot;segmentation fault&quot;</p>
<pre><code>    import os
    import streamlit as st
    
    os.environ[&quot;REPLICATE_API_TOKEN&quot;] = &quot;my_token&quot;
    
    from llama_index.llms import Replicate
    
    llama2_7b_chat = &quot;meta/llama-2-7b-chat:8e6975e5ed6174911a6ff3d60540dfd4844201974602551e10e9e87ab143d81e&quot;
    llm = Replicate(
        model=llama2_7b_chat,
        temperature=0.01,
        additional_kwargs={&quot;top_p&quot;: 1, &quot;max_new_tokens&quot;: 300},
    )
    
    from llama_index import VectorStoreIndex, SimpleDirectoryReader
    from llama_index.embeddings import HuggingFaceEmbedding
    from llama_index import ServiceContext
    
    embed_model = HuggingFaceEmbedding(model_name=&quot;BAAI/bge-small-en-v1.5&quot;)
    service_context = ServiceContext.from_defaults(
        llm=llm, embed_model=embed_model
    )
    
    documents = SimpleDirectoryReader(&quot;data&quot;).load_data()
    index = VectorStoreIndex.from_documents(
        documents, service_context=service_context
    )
    
    
    #index = VectorStoreIndex.from_documents(documents)
    
    # Get the query engine
    query_engine = index.as_query_engine(streaming=True)
    
    # Create a Streamlit web app
    #st.title(&quot;LLM Query Interface&quot;)
    query = st.text_input(&quot;Enter your query:&quot;)
    submit_button = st.button(&quot;Submit&quot;)
    
    if submit_button:
        # Query the engine with the defined query
        response = query_engine.query(query)
        st.write(&quot;### Query Result:&quot;)
        st.write(response)
</code></pre>
<p>if I disable embed_model code runs without problems.</p>
<p>I run the script via:</p>
<pre><code>    streamlit run my_script.py
</code></pre>
<pre><code>python --version
Python 3.9.7
</code></pre>
<p>I run this on Mac, Catalina, I have at least 250GB available memory. Please advise.</p>
","huggingface"
"77758645","Get accelerate package to log test results with huggingface Trainer","2024-01-04 13:28:04","77808568","0","182","<python><huggingface><wandb><accelerate>","<p>I am fine-tuning a T5 model on a specific dataset and my code looks like this:</p>
<pre class=""lang-py prettyprint-override""><code>accelerator = Accelerator(log_with='wandb')
tokenizer = T5Tokenizer.from_pretrained('t5-base')
model = T5ForConditionalGeneration.from_pretrained('t5-base')

accelerator.init_trackers(
project_name='myProject',
config={
# My configs
    }
)
# Then I do some preparations towards the fine-tuning

trainer_arguments = transformers.Seq2SeqTrainingArguments(
# Here I pass many arguments
)
trainer = transformers.Seq2SeqTrainer(
# Here I pass the arguments along side other needed arguments
)

# THEN FINALLY I TRAIN, EVALUATE AND TEST LIKE SO:

trainer.train()
trainer.evaluate( #evaluation parameters# )
trainer.predict( #test arguments# )
</code></pre>
<p>Now my main issue, when I check the <code>wandb</code> site for my project, I only see logging for the <code>trainer.train()</code> phase but not the <code>trainer.evaluate()</code> or <code>trainer.predict()</code> phases.<br><br></p>
<p>I've scoured the web trying to find a solution but could not find any.<br></p>
<p>How do I get wandb/accelerate to log all of my phases?
<br>
Thanks!</p>
<p>For the full code, you can see it here:
<a href=""https://github.com/zbambergerNLP/principled-pre-training/blob/master/fine_tune_t5.py"" rel=""nofollow noreferrer"">https://github.com/zbambergerNLP/principled-pre-training/blob/master/fine_tune_t5.py</a></p>
","huggingface"
"77756928","GPT-2 taking into account output logits in forward call?","2024-01-04 08:49:02","","1","116","<huggingface-transformers><huggingface><gpt-2>","<p>I'm using Huggingface GPT-2 model, specifically GPT2LMHeadModel.  I have 2 versions of this model, one loaded normally, and one that is the same, but I change some of the output embeddings (model.lm_head).  I'm inputting a batch of sentences using the following in eval mode:</p>
<p><code>outputs = model(input_ids=test_input_ids, attention_mask=test_attention_mask)</code></p>
<p>Now from my understanding, for each input id, it only looks at the inputs to the left of it.  Since I've given it multiple input ids, and this is a forward call (not generate), it should NOT be using the logits generated as input, correct?</p>
<p>However, when I input the same input_ids into both models, they give me different hidden states, even though they should be the same.  What am I missing here?</p>
","huggingface"
"77748109","How to input system message and file prompt on Hugging face","2024-01-02 19:13:23","","1","177","<huggingface-transformers><openai-api><large-language-model><huggingface><bloom>","<p>This is a function that basically uses the OpenAI API to take an input system message, file prompt and generate an output to eb parsed as a knowledge graph:</p>
<pre><code>def process_gpt(file_prompt, system_msg):
    completion = openai.ChatCompletion.create(
        engine=openai_deployment,
        max_tokens=15000,
        temperature=0,
        messages=[
            {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: system_msg},
            {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: file_prompt},
        ],
    )
    nlp_results = completion.choices[0].message.content
    sleep(8)
    return nlp_results
</code></pre>
<p>I am trying to do the same here in Bloom:</p>
<pre><code>def process_bloom(system_msg):
    try:
        generator = pipeline('text-generation', model = model, tokenizer=tokenizer)
        nlp_results = generator(system_msg, max_length = 2000)
        nlp_results = nlp_results[0]['generated_text']
        return nlp_results
    except Exception as e:
        print(f'Error processing bloom: (e)')
        return '{&quot;error&quot;: Failed to process bloom}'
</code></pre>
<p>But I'm using the Hugginface API, not OpenAI, so I couldn't figure out how to map the exact logic to make use of Bloom.</p>
","huggingface"
"77745505","Whisper - How to convent model.safetensors to .h5 format or pytorch_model.bin?","2024-01-02 10:35:44","","1","733","<huggingface><openai-whisper>","<p>After i train the model, I only have these file.</p>
<p>[1]: (<a href=""https://i.sstatic.net/7PpxF.png"" rel=""nofollow noreferrer"">https://i.sstatic.net/7PpxF.png</a>_</p>
<p>However, i find that other people will get tf_model.h5 or pytorch_model.bin after train their model.</p>
<p>I found that .safetensors is the latest format of that. But i need the ggml format. SO i want to convert the format to ggml with Whisper.cpp</p>
<p>I have train my custom model of Whisper. from blow reference:
<a href=""https://huggingface.co/blog/fine-tune-whisper"" rel=""nofollow noreferrer"">https://huggingface.co/blog/fine-tune-whisper</a>
<a href=""https://colab.research.google.com/drive/1qdS5ioA0pXoDPYPnbOlrOsXMxK0OR5yO"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1qdS5ioA0pXoDPYPnbOlrOsXMxK0OR5yO</a></p>
<p>and want to convert the format to ggml with Whisper.cpp
<a href=""https://github.com/ggerganov/whisper.cpp/tree/master/models"" rel=""nofollow noreferrer"">https://github.com/ggerganov/whisper.cpp/tree/master/models</a></p>
","huggingface"
"77745229","OSError: Error no file named model.safetensors found in directory","2024-01-02 09:39:57","","-1","2243","<huggingface-transformers><large-language-model><huggingface><llama>","<p>I am trying to load a LLAMA2 model saved in Hugging Face safe tensors format. The model is saved in two parts model-part1.safetensors and model-part2.safetensors.</p>
<p>I am using LlamaForCausalLM.from_pretrained() Hugging Face API to load the model.</p>
<p>When I pass the folder containing the model files, I am the following error</p>
<p><code>OSError: Error no file named model.safetensors found in directory ..</code></p>
<p>The code that I am using is:</p>
<pre><code>from transformers import LlamaForCausalLM, LlamaTokenizer
tokenizer = LlamaTokenizer.from_pretrained(&quot;./tokenizer.model&quot;)
model = LlamaForCausalLM.from_pretrained('./', use_safetensors=True)
</code></pre>
<p>How can I load a sharded model using Hugging Face API?</p>
<p>Can anyone please help on this?</p>
","huggingface"
"77744407","How to locally load a finetuned LLAMA2 model that currently saved to my local disk in safe tensor format","2024-01-02 06:11:49","","0","165","<nlp><large-language-model><huggingface><llama>","<p>I am new to working with LLMs. I have a finetuned LLAMA2 model in safe tensor format saved to my local disk. I want to load my model locally by running a python notebook.</p>
<p>I have tried googling, and I find many examples on loading Stable diffusion models but noting much on loading LLAMA models.</p>
","huggingface"
"77739328","Code Infilling fine-tuning with llama code","2023-12-31 13:49:18","","1","303","<python><pytorch><huggingface-transformers><huggingface>","<p>I have a dataset of java methods and I want to fine-tune a code llm to provide accurate method names. Right now the dataset is in a .txt format with methods in text separated by a delimiter(###del###).<br />
To do this I thought about using CodeLLaMa and more specifically code infilling.<br />
From the original documentation:</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForCausalLM
import transformers
import torch

model_id = &quot;codellama/CodeLlama-7b-hf&quot;
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.float16
).to(&quot;cuda&quot;)

prompt = '''def remove_non_ascii(s: str) -&gt; str:
    &quot;&quot;&quot; &lt;FILL_ME&gt;
    return result
'''

input_ids = tokenizer(prompt, return_tensors=&quot;pt&quot;)[&quot;input_ids&quot;].to(&quot;cuda&quot;)
output = model.generate(
    input_ids,
    max_new_tokens=200,
)
output = output[0].to(&quot;cpu&quot;)

filling = tokenizer.decode(output[input_ids.shape[1]:], skip_special_tokens=True)
print(prompt.replace(&quot;&lt;FILL_ME&gt;&quot;, filling))
</code></pre>
<p>If i set  <code>max_new_tokens=4</code> and replace method name with <code>&lt;FILL_ME&gt;</code> i will get a valid method name when i Inference the model.<br />
My problem is with fine-tuning.<br />
How am i supposed to format the dataset (as a supervised task) to fine tune such a model?</p>
","huggingface"
"77733581","Alternative engine for Huggingface SpeechT5Processor processor?","2023-12-29 18:56:06","","0","113","<text-to-speech><huggingface>","<p>Here is the code, I am wondering if there are better Huggingface processors/engines:</p>
<pre><code>device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;
processor = SpeechT5Processor.from_pretrained(&quot;microsoft/speecht5_tts&quot;)
model = SpeechT5ForTextToSpeech.from_pretrained(&quot;microsoft/speecht5_tts&quot;).to(device)
vocoder = SpeechT5HifiGan.from_pretrained(&quot;microsoft/speecht5_hifigan&quot;).to(device)
embeddings_dataset = load_dataset(&quot;Matthijs/cmu-arctic-xvectors&quot;, split=&quot;validation&quot;)
</code></pre>
<p>I tried using the Transformers with the <code>microsoft/speecht5_tts text2speech</code> model but it has some voice glitches and only accepts maximum 600 tokens.</p>
<p>Any idea of an alternative?</p>
","huggingface"
"77729879","ChromaDB and HuggingFace cannot process large files","2023-12-29 01:30:14","","0","1102","<python><langchain><huggingface><chromadb><huggingface-hub>","<p>I am trying to process 1000+ page PDFs using huggingface embeddings and chroma db. Whenever I try to upload a large file, however, I get the error below. I don't know if chromadb can handle that big of files but I thought I'd ask so I can see my options with chromadb or if I need to change the database. Any help would be appreciated to resolve this issue!</p>
<pre><code> File &quot;/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain/vectorstores/chroma.py&quot;, line 613, in from_documents
    return cls.from_texts(
           ^^^^^^^^^^^^^^^
  File &quot;/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain/vectorstores/chroma.py&quot;, line 577, in from_texts
    chroma_collection.add_texts(texts=texts, metadatas=metadatas, ids=ids)
  File &quot;/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain/vectorstores/chroma.py&quot;, line 205, in add_texts
    [embeddings[idx] for idx in non_empty_ids] if embeddings else None
  File &quot;/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain/vectorstores/chroma.py&quot;, line 205, in &lt;listcomp&gt;
    [embeddings[idx] for idx in non_empty_ids] if embeddings else None
     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyError: 0
</code></pre>
<p>Python Code</p>
<pre><code>embeddings = HuggingFaceHubEmbeddings(huggingfacehub_api_token=access_token)
loader = OnlinePDFLoader(document)
documents = loader.load()
text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)
texts = text_splitter.split_documents(documents)
db = Chroma.from_documents(texts, embeddings, persist_directory=&quot;./chroma_db&quot;)
</code></pre>
","huggingface"
"77727798","Huggingface Data Collator","2023-12-28 15:36:15","","0","180","<large-language-model><huggingface><mistral-7b>","<p>I'm trying to fine-tune mistral-7b on a task where it is important for the model to only output a label and nothing else. Hence I am formatting my train_dataset as follows:</p>
<p>f&quot;some system prompt\n{user_input}\nLable:{label}&quot;</p>
<p>my eval_dataset looks like:</p>
<p>f&quot;some system prompt\n{user_input}\nLable:&quot;</p>
<p>now I am using the huggingface Trainer to fine-tune:</p>
<pre><code>run_name = BASE_MODEL_ID.split(&quot;/&quot;)[-1]  + PROJECT_NAME
output_dir = &quot;./&quot; + run_name
trainer_args = TrainingArguments(
               output_dir=output_dir,
               warmup_steps=2,
               per_device_train_batch_size=2,
               gradient_accumulation_steps=16,
               gradient_checkpointing=True,
               max_steps=200,
               learning_rate=2e-5, # Want a small lr for finetuning
               bf16=True,
               optim=&quot;paged_adamw_8bit&quot;,
               load_best_model_at_end=True,
               metric_for_best_model=&quot;eval_loss&quot;,
               logging_steps=32,              
               logging_dir=&quot;./logs&quot;,        
               save_strategy=&quot;steps&quot;,     
               save_steps=32,                
               evaluation_strategy=&quot;steps&quot;, 
               eval_steps=32,               
              )

trainer = Trainer(
    model=model,
    train_dataset=train_ds,
    eval_dataset=test_ds,
    args=trainer_args,
    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),
    callbacks=[EarlyStoppingCallback(early_stopping_patience=10)],
)
</code></pre>
<p>However, when I use the data collator every padding token is set to -100 as I defined pad_token = eos_token. Is there a way to keep this behavior but add a eos token to the end of the sequence that doesn't get converted to -100 by the data collator?</p>
<p>That would look something like this (assuming 2 to be the eos_token_id):</p>
<pre><code>[-100, -100 -100, .... 55,32,4,2]
</code></pre>
","huggingface"
"77719947","SSL: Certificate Verify Failed error when making request from langchain to HuggingFaceHub","2023-12-27 03:44:52","","1","672","<ssl><ssl-certificate><langchain><huggingface>","<p>I run the following code in code on Windows using LangChain framework to make request from HuggingFaceHub:</p>
<pre><code>from langchain.llms import HuggingFaceHub

os.environ[&quot;HUGGINGFACEHUB_API_TOKEN&quot;] = &quot;HuggingFaceHub_Key_here&quot;

llm = HuggingFaceHub(repo_id = &quot;google/flan-t5-large&quot;)
our_query = &quot;What is the currency of Nigeria?&quot;
completion = llm(our_query)
print(completion)
</code></pre>
<p>I received an SSL certificate verification error.
I checked HuggingFace website and found out that Chrome is displaying &quot;Not secure&quot; and &quot;Certificate not valid&quot; warning messages on huggingface.co website. I downloaded the huggingface.co certificate and added it to Windows Trusted Root Certification Authorities. Chrome didn't recognize the certificate. I added the HuggingFace certificate to Chrome through the Chrome Security/Manage Certificate, which Chrome allows but didn't add the certificate. Can someone help out to solve this problem?</p>
","huggingface"
"77719791","Why is this warning message appearing?","2023-12-27 02:20:44","","0","43","<tokenize><huggingface><gpt-2>","<p><a href=""https://i.sstatic.net/NEe30.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/NEe30.png"" alt=""enter image description here"" /></a></p>
<pre class=""lang-py prettyprint-override""><code>    if any(k in model_args.model_name_or_path for k in (&quot;gpt&quot;, &quot;opt&quot;, &quot;bloom&quot;)):
        padding_side = &quot;left&quot;
    else:
        padding_side = &quot;right&quot;
    
    tokenizer = AutoTokenizer.from_pretrained(
        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,
        padding_side=padding_side,
        use_fast=model_args.use_fast_tokenizer,
        revision=model_args.model_revision
    )
    
    if tokenizer.pad_token_id is None:
        tokenizer.pad_token_id = tokenizer.eos_token_id
    
    ...


    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=dataset.train_dataset if training_args.do_train else None,
        eval_dataset=dataset.eval_dataset if training_args.do_eval else None,
        compute_metrics=dataset.compute_metrics,
        tokenizer=tokenizer,
        data_collator=DataCollatorForSeq2Seq(tokenizer, model=model)
    )
</code></pre>
<p>While using GPT2, during training, there are no warning messages, but during validation, the message as mentioned appears. As seen in the code, <code>pad_token_id='left'</code> and <code>pad_token_id=eos_token_id</code> are already set as required.</p>
<p>Why might this message be appearing during validation? Is there any additional task needed?</p>
","huggingface"
"77718803","llama-cpp-python on GPU: Delay between prompt submission and first token generation with longer prompts","2023-12-26 19:19:04","","0","884","<gpu><huggingface><large-language-model><llama-cpp-python><llamacpp>","<p>I've been building a RAG pipeline using the <a href=""https://github.com/abetlen/llama-cpp-python#openai-compatible-web-server"" rel=""nofollow noreferrer"">llama-cpp-python OpenAI compatible server</a> functionality and have been working my way up from running on just a laptop to running this on a dedicated workstation VM with access to an Nvidia A100. After the most recent transition to a machine with access to this A100 i was expecting (naively?) this RAG pipeline to be blazing fast, but I've been surprised to find that this is not currently the case.</p>
<p>What im experiencing is a seemingly linear relationship between the length of my prompt and the time it takes to get back the first response tokens (with streaming enabled):</p>
<ul>
<li>a few sentences --&gt; very short time to first response tokens</li>
<li>a few paragraphs (~2600 tokens) --&gt; around 1 minute to first response tokens</li>
</ul>
<p>But once the tokens start streaming the response time is very acceptable.</p>
<p>The culprit for the initial delay seems to be the first run of the <a href=""https://github.com/abetlen/llama-cpp-python/blob/f952d45c2cd0ccb63b117130c1b1bf4897987e4c/llama_cpp/llama.py#L1248"" rel=""nofollow noreferrer""><code>self.eval(tokens)</code> method</a>.</p>
<p>Im very new to LLMs and GPUs so im trying to understand:</p>
<ol>
<li>why this first run of <code>self.eval(tokens)</code> takes so long for longer prompts</li>
<li>is there anything that I can do to improve this delay?
<ul>
<li>Have I configured something wrong and this <code>eval</code> step is running on the CPU instead of GPU? Or is this just the way it is and there's no way to improve with my current setup?</li>
</ul>
</li>
</ol>
<p>If there is nothing to improve my current setup is there any reason to believe that other tools to run Llama2 like <a href=""https://github.com/huggingface/text-generation-inference"" rel=""nofollow noreferrer"">HuggingFace's Text Generation Interface</a> or <a href=""https://docs.vllm.ai/en/latest/"" rel=""nofollow noreferrer"">vLLM</a> would somehow be faster?</p>
<p>Other useful details:</p>
<ul>
<li>Nvidia A100 GPU</li>
<li>Im fairly certain that the GPU is actually being fully utilized to llama-cpp-python server's fullest abilities given the debugging output:
<pre><code>llm_load_tensors: using CUDA for GPU acceleration
llm_load_tensors: mem required  =  107.56 MiB
llm_load_tensors: offloading 40 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 41/41 layers to GPU
llm_load_tensors: VRAM used: 8694.21 MiB
</code></pre>
</li>
<li>call to start the server:
<pre><code>python -m llama_cpp.server --model D:\LLM_Work\cache\TheBloke\llama-2-13b-chat.Q5_K_M.gguf --n_gpu_layers -1 --n_ctx 3900 --cache False
</code></pre>
</li>
</ul>
","huggingface"
"77718587","why is there no Output when IDEFICS based model is run on CUDA?","2023-12-26 18:16:57","","3","154","<pytorch><huggingface-transformers><large-language-model><huggingface>","<p>I am using Huggingface based IDEFICS &quot;idefics-9b&quot; to instruct the model print a caption for the given image. I am using the code as mentioned in the section <a href=""https://huggingface.co/docs/transformers/main/tasks/idefics#image-guided-text-generation"" rel=""nofollow noreferrer"">1</a>. Which means the model and the processor are on 'cuda' device. But When I run this code, the output doesn't contain anything after &quot;Story&quot; tag. But when I shift the model and processor on cpu, then there is a juicy story after the &quot;Story&quot; tag in the output.</p>
<p>The code is below:</p>
<pre><code>checkpoint = &quot;HuggingFaceM4/idefics-9b&quot;
import torch

from transformers import IdeficsForVisionText2Text, AutoProcessor

processor = AutoProcessor.from_pretrained(checkpoint)

model = IdeficsForVisionText2Text.from_pretrained(checkpoint, torch_dtype=torch.bfloat16)

prompt = [&quot;Instruction: Use the image to write a story. \n&quot;,

    &quot;https://images.unsplash.com/photo-1517086822157-2b0358e7684a?ixlib=rb-4.0.3&amp;ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&amp;auto=format&amp;fit=crop&amp;w=2203&amp;q=80&quot;,

    &quot;Story: \n&quot;]
model.to('cuda')
inputs = processor(prompt, return_tensors=&quot;pt&quot;).to(&quot;cuda&quot;)
bad_words_ids = processor.tokenizer([&quot;&lt;image&gt;&quot;, &quot;&lt;fake_token_around_image&gt;&quot;], add_special_tokens=False).input_ids

generated_ids = model.generate(**inputs, num_beams=2, max_new_tokens=100)#, bad_words_ids=bad_words_ids)
generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)
print(generated_text[0]) 
</code></pre>
<p>It would be a big help to know what is the problem. Refer to the screenshot below.<a href=""https://i.sstatic.net/QhLWY.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/QhLWY.png"" alt=""enter image description here"" /></a></p>
","huggingface"
"77714850","Performing LLM inference locally with Python (LangChain / AutoGen / AutoMemGPT) using LLM model hosted on RunPod webui (TheBloke LLMs)","2023-12-25 20:04:44","","1","594","<langchain><huggingface><large-language-model><autogen>","<p>I am running <code>ehartford_dolphin-2.1-mistral-7b</code> on an RTX A6000 machine on RunPod with the template <code>TheBloke LLMs</code> Text Generation WebUI.</p>
<p>I have 2 options: running webui on runpod or running HuggingFace Text Generation Inference template on runpod</p>
<p><strong>Option 1. RunPod WebUI</strong></p>
<p>I can successfully loaded the model on textgen webui on RunPod on the <code>Chat</code> tab. I now want to access it ob my Python code and run inference. Ideal case would be if I integrate it on LangChain and create a LangChain LLM object.</p>
<ul>
<li>I enabled <code>openai</code> and <code>api</code> on RunPod webui on the <code>Settings</code> tab</li>
<li>I currently have <code>7860</code>, <code>5001</code> and <code>5000</code> ports enabled</li>
</ul>
<p><strong>Using AutoMemGPT</strong></p>
<p>I found this Python code using AutoMemGPT to access webui endpoint:</p>
<pre><code>import os
import autogen
import memgpt.autogen.memgpt_agent as memgpt_autogen
import memgpt.autogen.interface as autogen_interface
import memgpt.agent as agent       
import memgpt.system as system
import memgpt.utils as utils 
import memgpt.presets as presets
import memgpt.constants as constants 
import memgpt.personas.personas as personas
import memgpt.humans.humans as humans
from memgpt.persistence_manager import InMemoryStateManager, InMemoryStateManagerWithPreloadedArchivalMemory, InMemoryStateManagerWithEmbeddings, InMemoryStateManagerWithFaiss
import openai

config_list = [
    {
        &quot;api_type&quot;: &quot;open_ai&quot;,
        &quot;api_base&quot;: &quot;https://0ciol64iqvewdn-5001.proxy.runpod.net/v1&quot;,
        &quot;api_key&quot;: &quot;NULL&quot;,
    },
]

llm_config = {&quot;config_list&quot;: config_list, &quot;seed&quot;: 42}

# If USE_MEMGPT is False, then this example will be the same as the official AutoGen repo
# (https://github.com/microsoft/autogen/blob/main/notebook/agentchat_groupchat.ipynb)
# If USE_MEMGPT is True, then we swap out the &quot;coder&quot; agent with a MemGPT agent

USE_MEMGPT = True

## api keys for the memGPT
openai.api_base=&quot;https://0ciol64iqvewdn-5001.proxy.runpod.net/v1&quot;
openai.api_key=&quot;NULL&quot;


# The user agent
user_proxy = autogen.UserProxyAgent(
    name=&quot;User_proxy&quot;,
    system_message=&quot;A human admin.&quot;,
    code_execution_config={&quot;last_n_messages&quot;: 2, &quot;work_dir&quot;: &quot;groupchat&quot;},
    human_input_mode=&quot;TERMINATE&quot;,  # needed?
    default_auto_reply=&quot;You are going to figure all out by your own. &quot;
    &quot;Work by yourself, the user won't reply until you output `TERMINATE` to end the conversation.&quot;,
)


interface = autogen_interface.AutoGenInterface()
persistence_manager=InMemoryStateManager()
persona = &quot;I am a 10x engineer, trained in Python. I was the first engineer at Uber.&quot;
human = &quot;Im a team manager at this company&quot;
memgpt_agent=presets.use_preset(presets.DEFAULT_PRESET, model='gpt-4', persona=persona, human=human, interface=interface, persistence_manager=persistence_manager, agent_config=llm_config)


if not USE_MEMGPT:
    # In the AutoGen example, we create an AssistantAgent to play the role of the coder
    coder = autogen.AssistantAgent(
        name=&quot;Coder&quot;,
        llm_config=llm_config,
        system_message=f&quot;I am a 10x engineer, trained in Python. I was the first engineer at Uber&quot;,
        human_input_mode=&quot;TERMINATE&quot;,
    )

else:
    # In our example, we swap this AutoGen agent with a MemGPT agent
    # This MemGPT agent will have all the benefits of MemGPT, ie persistent memory, etc.
    print(&quot;\nMemGPT Agent at work\n&quot;)
    coder = memgpt_autogen.MemGPTAgent(
        name=&quot;MemGPT_coder&quot;,
        agent=memgpt_agent,
    )


# Begin the group chat with a message from the user
user_proxy.initiate_chat(
    coder,
    message=&quot;Write a Function to print Numbers 1 to 10&quot;
    )
</code></pre>
<p><em>Error</em></p>
<blockquote>
<hr />
<p>ModuleNotFoundError                       Traceback (most recent call
last) Cell In[2], line 10
8 import memgpt.presets as presets
9 import memgpt.constants as constants
---&gt; 10 import memgpt.personas.personas as personas
11 import memgpt.humans.humans as humans
12 from memgpt.persistence_manager import InMemoryStateManager, InMemoryStateManagerWithPreloadedArchivalMemory,
InMemoryStateManagerWithEmbeddings, InMemoryStateManagerWithFaiss</p>
<p>ModuleNotFoundError: No module named 'memgpt.personas.personas'</p>
</blockquote>
<p><em>What I tried to solve this error</em></p>
<ul>
<li><code>pip install --upgrade pymemgpt</code> -- does not change error</li>
<li><code>pip install pymemgpt==0.1.3</code> -- I get <code>openai</code> version conflicts</li>
<li><code>pip install -e .</code> after cloning MemGPT repository -- another error</li>
</ul>
<p><em>What I need</em></p>
<ul>
<li>I always get version conflicts between <code>openai</code>, <code>llama-index</code>, <code>pymemgpt</code>, <code>pyautogpt</code>, <code>numpy</code>, so maybe the proper version to make this code run would be nice otherwise any advice?</li>
</ul>
<p><strong>Option 2. Using HuggingFace Text Generation Interface</strong></p>
<p>So instead of loading TheBloke LLMs template that runs webui on RunPod I found a guide to instead use a TextGenerationInference template</p>
<p><em>Current code</em></p>
<pre><code>gpu_count = 1

pod = runpod.create_pod(
    name=&quot;Llama-7b-chat&quot;,
    image_name=&quot;ghcr.io/huggingface/text-generation-inference:0.9.4&quot;,
    gpu_type_id=&quot;NVIDIA RTX A4500&quot;,
    data_center_id=&quot;EU-RO-1&quot;,
    cloud_type=&quot;SECURE&quot;,
    docker_args=&quot;--model-id TheBloke/Llama-2-7b-chat-fp16&quot;,
    gpu_count=gpu_count,
    volume_in_gb=50,
    container_disk_in_gb=5,
    ports=&quot;80/http,29500/http&quot;,
    volume_mount_path=&quot;/data&quot;,
)
pod

from langchain.llms import HuggingFaceTextGenInference

inference_server_url = f'https://{pod[&quot;id&quot;]}-80.proxy.runpod.net'
llm = HuggingFaceTextGenInference(
    inference_server_url=inference_server_url,
    max_new_tokens=1000,
    top_k=10,
    top_p=0.95,
    typical_p=0.95,
    temperature=0.1,
    repetition_penalty=1.03,
)
</code></pre>
<p>It works well on Llama 2 but I cannot make it work on other LLMs that needs a ton of configuring on the webui before running. So for example Falcon or Mixtral where I need to change several parameters on webui manually.</p>
<p><em>What I need</em></p>
<ul>
<li>A way to run this code to any LLM by programmatically setting model parameters, settings, etc instead on RunPod webui</li>
</ul>
","huggingface"
"77709442","Huggingface load_dataset raises NonMatchingSplitsSizesError(str(bad_splits))","2023-12-24 01:01:03","","0","1092","<huggingface><huggingface-datasets>","<p>I used some dataset about a month ago and everything worked just fine when I ran</p>
<pre><code>dataset = load_dataset('adversarial_qa', 'adversarialQA') 
</code></pre>
<p>But now when I run it I get</p>
<pre><code>  File &quot;/projects/my_username/software/anaconda/envs/context/lib/python3.11/site-packages/datasets/load.py&quot;, line 1782, in load_dataset
    builder_instance.download_and_prepare(
  File &quot;/projects/my_username/software/anaconda/envs/context/lib/python3.11/site-packages/datasets/builder.py&quot;, line 872, in download_and_prepare
    self._download_and_prepare(
  File &quot;/projects/my_username/software/anaconda/envs/context/lib/python3.11/site-packages/datasets/builder.py&quot;, line 985, in _download_and_prepare
    verify_splits(self.info.splits, split_dict)
  File &quot;/projects/my_username/software/anaconda/envs/context/lib/python3.11/site-packages/datasets/utils/info_utils.py&quot;, line 100, in verify_splits
    raise NonMatchingSplitsSizesError(str(bad_splits))
datasets.utils.info_utils.NonMatchingSplitsSizesError: [{'expected': SplitInfo(name='train', num_bytes=27858686, num_examples=30000, shard_lengths=None, dataset_name=None), 'recorded': SplitInfo(name='tra
in', num_bytes=55764872, num_examples=60000, shard_lengths=None, dataset_name='parquet')}, {'expected': SplitInfo(name='validation', num_bytes=2757092, num_examples=3000, shard_lengths=None, dataset_name=
None), 'recorded': SplitInfo(name='validation', num_bytes=5518934, num_examples=6000, shard_lengths=None, dataset_name='parquet')}, {'expected': SplitInfo(name='test', num_bytes=2919479, num_examples=3000
, shard_lengths=None, dataset_name=None), 'recorded': SplitInfo(name='test', num_bytes=5842958, num_examples=6000, shard_lengths=None, dataset_name='parquet')}]
</code></pre>
<p>I found this <a href=""https://stackoverflow.com/questions/69825418/nonmatchingsplitssizeserror-loading-huggingface-bookcorpus"">SO</a> that had a similar issue, but the dataset they used is no longer publicly available which was their issue and not mine. I found <a href=""https://discuss.huggingface.co/t/nonmatchingsplitssizeserror/30033/1"" rel=""nofollow noreferrer"">this</a> similar question on the huggingface blog, which suggested:</p>
<ol>
<li>Running the <code>load_dataset</code> with <code>download_mode=&quot;reuse_cache_if_exists&quot;</code> -- resulted in the same error</li>
<li>Running the <code>load_dataset</code> with <code>ignore_verifications=True</code> -- resulted in <code>ValueError: 'none' is not a valid VerificationMode</code></li>
<li>Running the <code>load_dataset</code> with <code>download_mode=“force_redownload”</code> -- resulted in the same error.</li>
</ol>
<p>The accepted answer mentioned:</p>
<pre><code> To (re)compute these numbers automatically and dump them to a README.md file, one should run datasets-cli test your_dataset --save_info. And as it’s done manually, it depends on datasets’ authors if they update and push this info or not as it’s not required.
</code></pre>
<p>I deleted the <code>readme.md</code> file under the <code>cache</code> folder, but I still get the error.</p>
<p>I also tried going to the directory where I save the dataset and run
<code>datasets-cli test adversarial_qa --save_info</code>. Still get the same error.</p>
<p><strong>Update:</strong></p>
<p>The only thing that worked was downloading the parquet files manually from huggingface and running</p>
<pre><code>dataset = load_dataset(&quot;parquet&quot;, data_files={'train': 'train.parquet', 'test': 'test.parquet'})
</code></pre>
<p>However, there has to be a better method to clear the cache or fix this...</p>
","huggingface"
"77704355","Using HuggingFace embeddings with Langchain install locally","2023-12-22 14:59:33","","0","702","<embedding><langchain><huggingface>","<p>HuggingFace is downloading embeddings models and not using them via API. Everytime i execute the app, it downloads the model.</p>
<p>I tried using other class like HuggingFaceInferenceAPIEmbeddings but it does the same as HuggingFaceEmbeddings, it downloads the model and it does not use it via API (without the need of installing it). Can someone tell me how can i use embeddings from huggingface via online?</p>
","huggingface"
"77695119","Trouble Downloading HuggingFace LLM Models to a Mounted /data Folder on Linux","2023-12-21 00:17:21","","0","219","<linux><huggingface><large-language-model>","<p>I attempted to set the HF_HOME environment variable to a different directory (/data/model_cache) on my Linux system due to limited storage in the root directory.
After changing the HF_HOME environment variable, the models do not seem to install properly in the /data/model_cache directory. While I can confirm that the token exists correctly in /data/huggingface, the model-related folders created in /data/model_cache (like blobs, refs, snapshots) do not reflect a complete installation. There is no progress bar as before, and the used storage space does not increase significantly, which leads me to believe that the model files are not being fully downloaded or installed.</p>
<p>This issue is causing significant challenges in my testing process with the HuggingFace LLM, as I am unable to utilize the models due to this installation problem.</p>
<p>I used the following command to set the environment variable:</p>
<p><strong>bash :export HF_HOME=/data/model_cache</strong></p>
<p>When I did not set the environment variable and downloaded the model to the root directory, I saw a progress bar indicating the download and installation process, and the used storage space increased as expected. After setting this variable, I expected that the HuggingFace Large Language Models (LLM) would download and install in the /data/model_cache directory instead of the default root directory.</p>
","huggingface"
"77689837","OSError: TheBloke/Llama-2-7B-Chat-GGML does not appear to have a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack","2023-12-20 07:22:13","","1","330","<python><artificial-intelligence><huggingface><huggingface-tokenizers><llama>","<p>I want to build myself an AI bot.
(&quot;TheBloke/Llama-2-7B-Chat-GGML&quot;,model_type=&quot;llama&quot;, model_file=&quot;llama-2-7b-chat.ggmlv3.q8_0.bin&quot;) I chose this model.</p>
<p>But I couldn't get it to work. I installed everything in requirements.txt but there is a problem somewhere. It is not solved.
My code app.py:</p>
<pre><code>import streamlit_chat as st
from streamlit_chat import message
from langchain.chains import ConversationalRetrievalChain
from langchain.document_loaders import PyPDFLoader, DirectoryLoader
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.llms import CTransformers
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.memory import ConversationBufferMemory 

from transformers import AutoConfig, AutoModelForCausalLM 
 
# from huggingface_hub import hf_hub_download

# # https://huggingface.co/docs/huggingface_hub/v0.16.3/en/package_reference/file_download#huggingface_hub.hf_hub_download
# hf_hub_download(
#     repo_id=&quot;TheBloke/Llama-2-7B-Chat-GGML&quot;,
#     filename=&quot;llama-2-7b-chat.ggmlv3.q8_0.bin&quot;,
#     local_dir=&quot;./models&quot;
# )
#load the pdf files from the path
loader = DirectoryLoader('data/',glob=&quot;*.pdf&quot;,loader_cls=PyPDFLoader)
documents = loader.load()

#split text into chunks
text_splitter  = RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=50)
text_chunks = text_splitter.split_documents(documents)

#create embeddings
embeddings = HuggingFaceEmbeddings(model_name=&quot;sentence-transformers/all-MiniLM-L6-v2&quot;,
                                   model_kwargs={'device':&quot;cpu&quot;})

#vectorstore
vector_store = FAISS.from_documents(text_chunks,embeddings)

# custom_prompt_template=&quot;&quot;&quot; Contextt: {} Question :{question} Helpful answer:&quot;&quot;&quot;


#
# def set_custom_prompt():
#     prompt = PromptTemplate(template=custom_prompt_template,input_variables=['context','question'])
#     return prompt

config = AutoConfig.from_pretrained(&quot;TheBloke/Llama-2-7B-Chat-GGML&quot;,model_type=&quot;llama&quot;, model_file=&quot;llama-2-7b-chat.ggmlv3.q8_0.bin&quot;)
config.max_new_tokens = 512
config.temperature = 0.5
config.wbits=4
config.groupsize=128
llm = AutoModelForCausalLM.from_pretrained(&quot;TheBloke/Llama-2-7B-Chat-GGML&quot;, model_file=&quot;llama-2-7b-chat.ggmlv3.q8_0.bin&quot;, config=config) 

# llm = AutoModelForCausalLM.from_pretrained(&quot;TheBloke/Llama-2-7B-Chat-GGML&quot;, model_file=&quot;llama-2-7b-chat.ggmlv3.q8_0.bin&quot;,config={'max_new_tokens':512,'temperature':0.5})

#create llm                
# llm = CTransformers(model=&quot;llama-2-7b-chat.ggmlv3.q8_0.bin&quot;,model_type=&quot;llama&quot;, #llama-2-7b-chat.ggmlv3.q4_0.bin
                    # config={'max_new_tokens':512,'temperature':0.5})
                    # config={'max_new_tokens':128,'temperature':0.01})

memory = ConversationBufferMemory(memory_key=&quot;chat_history&quot;, return_messages=True)

chain = ConversationalRetrievalChain.from_llm(llm=llm,chain_type='stuff',
                                              retriever=vector_store.as_retriever(search_kwargs={&quot;k&quot;:2}),return_source_documents = True,
                                             #chain_type_kwargs={'prompt':prompt},
                                              memory=memory)

st.title(&quot;HealthCare ChatBot 🧑🏽‍⚕️&quot;)
def conversation_chat(query):
    result = chain({&quot;question&quot;: query, &quot;chat_history&quot;: st.session_state['history']})
    st.session_state['history'].append((query, result[&quot;answer&quot;]))
    return result[&quot;answer&quot;]

def initialize_session_state():
    if 'history' not in st.session_state:
        st.session_state['history'] = []

    if 'generated' not in st.session_state:
        st.session_state['generated'] = [&quot;Hello! Ask me anything about 🤗&quot;]

    if 'past' not in st.session_state:
        st.session_state['past'] = [&quot;Hey! 👋&quot;]

def display_chat_history():
    reply_container = st.container()
    container = st.container()

    with container:
        with st.form(key='my_form', clear_on_submit=True):
            user_input = st.text_input(&quot;Question:&quot;, placeholder=&quot;Ask about your Mental Health&quot;, key='input')
            submit_button = st.form_submit_button(label='Send')

        if submit_button and user_input:
            output = conversation_chat(user_input)

            st.session_state['past'].append(user_input)
            st.session_state['generated'].append(output)

    if st.session_state['generated']:
        with reply_container:
            for i in range(len(st.session_state['generated'])):
                message(st.session_state[&quot;past&quot;][i], is_user=True, key=str(i) + '_user', avatar_style=&quot;thumbs&quot;)
                message(st.session_state[&quot;generated&quot;][i], key=str(i), avatar_style=&quot;fun-emoji&quot;)

# Initialize session state
initialize_session_state()
# Display chat history
display_chat_history()
</code></pre>
<p>Errors:</p>
<pre><code>(venv) PS C:\Users\myuser\Documents\Calismalarim\ai\LLama2HealthCareChatBot-master&gt; &amp; c:/Users/myuser/Documents/Calismalarim/ai/LLama2HealthCareChatBot-master/venv/Scripts/python.exe c:/Users/myuser/Documents/Calismalarim/ai/LLama2HealthCareChatBot-master/app.py
Traceback (most recent call last):
  File &quot;c:\Users\myuser\Documents\Calismalarim\ai\LLama2HealthCareChatBot-master\app.py&quot;, line 49, in &lt;module&gt;
    llm = AutoModelForCausalLM.from_pretrained(&quot;TheBloke/Llama-2-7B-Chat-GGML&quot;, model_file=&quot;llama-2-7b-chat.ggmlv3.q8_0.bin&quot;, config=config)
  File &quot;C:\Users\myuser\Documents\Calismalarim\ai\LLama2HealthCareChatBot-master\venv\lib\site-packages\transformers\models\auto\auto_factory.py&quot;, line 566, in from_pretrained
    return model_class.from_pretrained(
  File &quot;C:\Users\myuser\Documents\Calismalarim\ai\LLama2HealthCareChatBot-master\venv\lib\site-packages\transformers\modeling_utils.py&quot;, line 3321, in from_pretrained
    raise EnvironmentError(
OSError: TheBloke/Llama-2-7B-Chat-GGML does not appear to have a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
</code></pre>
<p>how to resolve it?</p>
<p>i installed all pip requirements.txt</p>
","huggingface"
"77688676","how can i control gpu number when using TrainingArguments","2023-12-20 00:34:58","","0","95","<huggingface><large-language-model><huggingface-trainer>","<p>Im working on multi GPU server and i want to use one GPU for the training
setting GPU for the train</p>
<pre><code>device = torch.device(&quot;cuda:2&quot;)
torch.cuda.set_device(device)

device_map={&quot;&quot;: torch.cuda.current_device()}

model = AutoModelForCausalLM.from_pretrained(
    model_path,
    quantization_config=bnb_config,
    device_map=device_map
)

model.config.use_cache = False
model.config.pretraining_tp = 1
</code></pre>
<p>but as soon as i run the TrainingArguments() part the torch.cuda.current_device() has changed to 0</p>
<pre><code>training_arguments = TrainingArguments(
    output_dir=output_dir,
    num_train_epochs=num_train_epochs,
    per_device_train_batch_size=per_device_train_batch_size,
    gradient_accumulation_steps=gradient_accumulation_steps,
    optim=optim,
    save_steps=save_steps,
    logging_steps=logging_steps,
    learning_rate=learning_rate,
    weight_decay=weight_decay,
    fp16=fp16,
    bf16=bf16,
    max_grad_norm=max_grad_norm,
    max_steps=max_steps,
    warmup_ratio=warmup_ratio,
    group_by_length=group_by_length,
    lr_scheduler_type=lr_scheduler_type,
    report_to=[&quot;tensorboard&quot;] 
)
</code></pre>
<p>how can i maintain GPU number</p>
","huggingface"
"77686125","Langchain, SQL and few shot","2023-12-19 15:01:58","","0","170","<langchain><google-cloud-vertex-ai><huggingface><large-language-model><py-langchain>","<p>I'm using SQLDatabaseChain to translate natural language questions into SQL.
I'm using VertexAI, and added few shot examples, embedded them into a vectorDB and then used FewShotPromptTemplate.</p>
<p>I understand that by using this approach, each time the LLM retrieve the closest semantically example (if exists).
When trying to just copy some of the examples directly to the prompt I get improved results.
I wonder why; when should I use which approach for few shot / which preferred.</p>
","huggingface"
"77681471","Unable to tag the POS of the text file","2023-12-18 20:02:17","77681791","1","26","<python><nlp><tagging><huggingface><flair>","<p>I want to tag the parts of speech of a sentence. For this task I am using <a href=""https://huggingface.co/flair/pos-english-fast"" rel=""nofollow noreferrer"">pos-english-fast</a> model. If there was one sentence the model identified the tags for the pos. I created a data file where I kept all my sentences. The name of the data file is 'data1.txt'. Now if I try to tag the sentences on the data file it does not work.</p>
<p>My code</p>
<pre><code>from flair.models import SequenceTagger
model = SequenceTagger.load(&quot;flair/pos-english&quot;)
#Read the data from the data.txt 
with open('data1.txt') as f:
  data = f.read().splitlines()
#Create a list of sentences from the data 
sentences = [sentence.split() for sentence in data]
#Tag each sentence using the model
tagged_sentences = []
for sentence in sentences:
  tagged_sentences.append(model.predict(sentence))
for sentence in tagged_sentences:
  print(sentence)
</code></pre>
<p>The error I received</p>
<pre><code>AttributeError                            Traceback (most recent call last)
&lt;ipython-input-16-03268ee0d9c9&gt; in &lt;cell line: 10&gt;()
      9 tagged_sentences = []
     10 for sentence in sentences:
---&gt; 11   tagged_sentences.append(model.predict(sentence))
     12 for sentence in tagged_sentences:
     13   print(sentence)

1 frames
/usr/local/lib/python3.10/dist-packages/flair/data.py in set_context_for_sentences(cls, sentences)
   1116         previous_sentence = None
   1117         for sentence in sentences:
-&gt; 1118             if sentence.is_context_set():
   1119                 continue
   1120             sentence._previous_sentence = previous_sentence

AttributeError: 'str' object has no attribute 'is_context_set'
</code></pre>
<p>The snapshot of the errors
<a href=""https://i.sstatic.net/0h0ZC.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/0h0ZC.png"" alt=""enter image description here"" /></a></p>
<p>How could I resolve it?</p>
","huggingface"
"77678828","Error when trying to Fine Tune T5: too many values to unpack (expected 2)","2023-12-18 11:40:28","","0","54","<huggingface-transformers><huggingface><t5-transformer>","<p>Been trying to Fine-Tune a T5 model on a certain dataset but even when following the step-by-step guide provided by Huggingface or using my own Trainer Class, the code was not running and always returning the same error when calling the <code>trainer.train()</code> seemingly when calculating the loss function.</p>
<p>this is my code based on the step by step guide found on the hugging face hub <a href=""https://huggingface.co/docs/transformers/tasks/translation"" rel=""nofollow noreferrer"">here</a></p>
<p>So far this is my code:
<code>transformers.logging.set_verbosity_error()</code></p>
<pre><code>from datasets import load_dataset

canard_train_augm = load_dataset(&quot;gaussalgo/Canard_Wiki-augmented&quot;, split=&quot;train&quot;)
canard_test_augm = load_dataset(&quot;gaussalgo/Canard_Wiki-augmented&quot;, split=&quot;test&quot;)
</code></pre>
<pre><code>from transformers import AutoTokenizer

model_name = &quot;t5-small&quot;
tokenizer = AutoTokenizer.from_pretrained(model_name)
</code></pre>
<pre><code>def preprocess_function(examples):
    combined_input = examples[&quot;Question&quot;] + &quot;: &quot; + examples[&quot;true_contexts&quot;]
    return tokenizer(combined_input, examples[&quot;Rewrite&quot;],max_length=512, padding=&quot;max_length&quot;, truncation=True, return_tensors=&quot;pt&quot;)
</code></pre>
<pre><code>tokenized_train = canard_train_augm.map(preprocess_function)
tokenized_test = canard_test_augm.map(preprocess_function)
</code></pre>
<pre><code>from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model_name)
</code></pre>
<pre><code>from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model_name)
</code></pre>
<pre><code>import evaluate

metric = evaluate.load(&quot;sacrebleu&quot;)
</code></pre>
<pre><code>import numpy as np


def postprocess_text(preds, labels):
    preds = [pred.strip() for pred in preds]
    labels = [[label.strip()] for label in labels]

    return preds, labels

def compute_metrics(eval_preds):
    preds, labels = eval_preds
    if isinstance(preds, tuple):
        preds = preds[0]
    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)

    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)

    result = metric.compute(predictions=decoded_preds, references=decoded_labels)
    result = {&quot;bleu&quot;: result[&quot;score&quot;]}

    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]
    result[&quot;gen_len&quot;] = np.mean(prediction_lens)
    result = {k: round(v, 4) for k, v in result.items()}
    return result
</code></pre>
<pre><code>from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer

model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
</code></pre>
<pre><code>training_args = Seq2SeqTrainingArguments(
    output_dir=&quot;wtf&quot;,
    evaluation_strategy=&quot;epoch&quot;,
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=2,
    predict_with_generate=True,
    fp16=True,
)

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_test,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

trainer.train()
</code></pre>
<p>I tried several examples including my own Customized Class for the trainer function but always ended with the same issue even when I tried the same code found in the step-by-step guide provided by huggingface.</p>
<p>The error happens when calling the <code>trainer.train()</code> returning the following:
<code>ValueError: too many values to unpack (expected 2)</code></p>
<p>I followed the exact same format as the documentation and I believe it is something that is happening when calling the loss function but was just unable to put my finger to it, if anyone can help that would be great.</p>
","huggingface"
"77677888","Import Huggingface Transformers offline","2023-12-18 08:39:57","","0","301","<python><huggingface-transformers><huggingface>","<p>I have downloaded the library locally and the models I want to use offline because of network restrictions.
I have already used them following the same process as below successfully.</p>
<p>But this time (not knowing what changed), while running this:</p>
<pre class=""lang-py prettyprint-override""><code>!export TRANSFORMERS_OFFLINE=1

sys.path.append('/home/shared/Libraries/transformers-main/src/')
from transformers import BertTokenizer, BertModel
sys.path.append('/home/shared/Libraries/sentence-transformers-master/')
from sentence_transformers import SentenceTransformer
</code></pre>
<p>I have the following error:</p>
<pre class=""lang-py prettyprint-override""><code>File /home/shared/Libraries/transformers-main/src/transformers/__init__.py:26
     23 from typing import TYPE_CHECKING
     25 # Check the dependencies satisfy the minimal versions required.
---&gt; 26 from . import dependency_versions_check
     27 from .utils import (
     28     OptionalDependencyNotAvailable,
     29     _LazyModule,
   (...)
     46     logging,
     47 )
     50 logger = logging.get_logger(__name__)  # pylint: disable=invalid-name

File /home/shared/Libraries/transformers-main/src/transformers/dependency_versions_check.py:16
      1 # Copyright 2020 The HuggingFace Team. All rights reserved.
      2 #
      3 # Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
   (...)
     12 # See the License for the specific language governing permissions and
     13 # limitations under the License.
     15 from .dependency_versions_table import deps
---&gt; 16 from .utils.versions import require_version, require_version_core
     19 # define which module versions we always want to check at run time
     20 # (usually the ones defined in `install_requires` in setup.py)
     21 #
     22 # order specific notes:
     23 # - tqdm must be checked before tokenizers
     25 pkgs_to_check_at_runtime = [
     26     &quot;python&quot;,
     27     &quot;tqdm&quot;,
   (...)
     37     &quot;pyyaml&quot;,
     38 ]

File /home/shared/Libraries/transformers-main/src/transformers/utils/__init__.py:18
      1 #!/usr/bin/env python
      2 # coding=utf-8
      3 
   (...)
     15 # See the License for the specific language governing permissions and
     16 # limitations under the License.
---&gt; 18 from huggingface_hub import get_full_repo_name  # for backward compatibility
     19 from packaging import version
     21 from .. import __version__

ModuleNotFoundError: No module named 'huggingface_hub'
</code></pre>
<p>Same for the import of the SentenceTransformer.
Because I am working offline, I expect to not use the <code>huggingface_hub</code>. I also tried to add the offline dataset variable to true.</p>
<p>I can provide more informations if needed.</p>
","huggingface"
"77664048","Unable to Create Instances for multiple Users in my FastAPI HF spaces Application with Docker","2023-12-15 02:56:08","","0","41","<docker><dockerfile><huggingface>","<p>I am deploying a FastAPI application in the Hugging Face Spaces environment, and I'm encountering an issue where it only allows one instance for all users sharing the same space. Despite attempts to handle user-specific configurations within the Docker image and entrypoint.sh script, the current setup doesn't dynamically create separate instances for individual users entering the space. Seeking guidance on modifying the Dockerfile or entrypoint.sh to allow multiple users to have distinct instances of the FastAPI application within the Hugging Face Spaces environment</p>
<pre><code># Set the working directory to /app
WORKDIR /app

# Copy the current directory contents into the container at /app
COPY . /app

# Define the user ID in the environment variable USER_ID with a default value
ARG USER_ID=1000
ENV USER_ID=$USER_ID

# Check if the user already exists
RUN if [ -z &quot;$USER_ID&quot; ]; then \
      echo &quot;User ID not provided. Using the default user ID 1000.&quot;; \
      USER_ID=1000; \
    fi &amp;&amp; \
    if id &quot;$USER_ID&quot; &gt;/dev/null 2&gt;&amp;1; then \
      echo &quot;User with ID $USER_ID already exists.&quot;; \
    else \
      adduser --uid &quot;$USER_ID&quot; --disabled-password --gecos '' appuser; \
    fi

# Set appropriate permissions for the application directory
RUN chown -R appuser:appuser /app &amp;&amp; chmod -R 755 /app

# Install gosu (adjust the package manager based on your base image)
RUN apt-get update &amp;&amp; apt-get install -y gosu &amp;&amp; rm -rf /var/lib/apt/lists/*

# Set the entrypoint script as executable
COPY entrypoint.sh /usr/local/bin/entrypoint.sh
RUN chmod +x /usr/local/bin/entrypoint.sh

# Switch to the user for improved security
USER appuser

# Define the entrypoint script to handle user creation and application startup
ENTRYPOINT [&quot;/usr/local/bin/entrypoint.sh&quot;]

# Default command to run if the user doesn't provide a command
CMD [&quot;uvicorn&quot;, &quot;app.main:app&quot;, &quot;--host&quot;, &quot;0.0.0.0&quot;, &quot;--port&quot;, &quot;7860&quot;, &quot;--reload&quot;]
</code></pre>
<pre><code>#!/bin/sh
set -e

# Set permissions at runtime
chmod -R 777 /app

# Execute the main command
exec &quot;$@&quot;
</code></pre>
<p>I've explored several strategies to enable multiple instances for users within the Hugging Face Spaces environment. One approach involved handling the majority of user setup logic within the entrypoint.sh script, and I've also experimented with different configurations in the Dockerfile.</p>
","huggingface"
"77657647","llama2-7B-chat taking very long to run and producing no response","2023-12-14 03:29:11","","0","836","<python-3.x><huggingface><large-language-model><llama>","<p>I'm running the python 3 code below on ubuntu server 18.04 LTS.  I have a single nvidia gpu with 8GB of ram.  With the code below I am loading model weights and transformers I've downloaded from hugging face for the llama2-7b-chat model.  I'm trying to save as much memory as possible using bits and bytes.  I'm just trying to get a simple test response from the model to verify the code is working.  The code runs for a long time, nearly 20 minutes.  It doesn't return an error, but it also doesn't return any response.  It just returns the same prompt I passed it.  Does anyone see what the issue might be?  Since the model is so small and further optimized with bnb do I need to increase the temperature just to get a response?  Also does anyone have a suggestion how I can further optimize it so it doesn't take so long to run?</p>
<p>code:</p>
<pre><code>import torch

# import hugging face apikey
from config import api_key

apikey=api_key

from torch import cuda, bfloat16
import transformers

model_id = 'meta-llama/Llama-2-7b-chat-hf'

device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'

# set quantization configuration to load large model with less GPU memory
# this requires the `bitsandbytes` library
bnb_config = transformers.BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type='nf4',
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=bfloat16
)

# begin initializing HF items, need auth token for these
# hf_auth = '&lt;YOUR_API_KEY&gt;'

hf_auth = apikey
model_config = transformers.AutoConfig.from_pretrained(
    model_id,
    use_auth_token=hf_auth
)

model = transformers.AutoModelForCausalLM.from_pretrained(
    model_id,
    trust_remote_code=True,
    config=model_config,
    quantization_config=bnb_config,
    device_map='auto',
    use_auth_token=hf_auth,
    cache_dir='/home/username/LLM/weights/huggingface/hub/'
)
model.eval()
print(f&quot;Model loaded on {device}&quot;)


tokenizer = transformers.AutoTokenizer.from_pretrained(
    model_id,
    use_auth_token=hf_auth
)


stop_list = ['\nHuman:', '\n```\n']

stop_token_ids = [tokenizer(x)['input_ids'] for x in stop_list]

stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]

from transformers import StoppingCriteria, StoppingCriteriaList

# define custom stopping criteria object
class StopOnTokens(StoppingCriteria):
    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -&gt; bool:
        for stop_ids in stop_token_ids:
            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():
                return True
        return False

stopping_criteria = StoppingCriteriaList([StopOnTokens()])


generate_text = transformers.pipeline(
    model=model, tokenizer=tokenizer,
    return_full_text=True,  # langchain expects the full text
    task='text-generation',
    # we pass model parameters here too
    stopping_criteria=stopping_criteria,  # without this model rambles during chat
    temperature=0.0,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max
    max_new_tokens=512,  # mex number of tokens to generate in the output
    repetition_penalty=1.1  # without this output begins repeating
)


res = generate_text(&quot;Explain to me the difference between nuclear fission and fusion.&quot;)
print(res[0][&quot;generated_text&quot;])
</code></pre>
<p>output:</p>
<p>Explain to me the difference between nuclear fission and fusion.</p>
","huggingface"
"77656929","Should 8bit quantization make whisper inference faster on GPU?","2023-12-13 22:43:50","","0","1051","<python><machine-learning><pytorch><gpu><huggingface>","<p>I'm performing whisper inference on huggingface transformers.
<code>load_in_8bit</code> quantization is provided by <code>bitsandbytes</code>.</p>
<p>Inference on a sample file takes much longer (5x) if whisper-large-v3 is loaded in 8bit mode on NVIDIA T4 gpu. GPU utilization is at 33% in <code>nvidia-smi</code>.</p>
<p>Shouldn't quantization improve inference speed on GPU?
<a href=""https://pytorch.org/docs/stable/quantization.html"" rel=""nofollow noreferrer"">https://pytorch.org/docs/stable/quantization.html</a></p>
<p>similar question:</p>
<ul>
<li><a href=""https://discuss.huggingface.co/t/enabling-load-in-8bit-makes-inference-much-slower/38596"" rel=""nofollow noreferrer"">https://discuss.huggingface.co/t/enabling-load-in-8bit-makes-inference-much-slower/38596</a></li>
</ul>
<pre class=""lang-py prettyprint-override""><code>
import torch

from transformers import WhisperFeatureExtractor, WhisperTokenizerFast
from transformers.pipelines.audio_classification import ffmpeg_read

MODEL_NAME = &quot;openai/whisper-large-v3&quot;

tokenizer = WhisperTokenizerFast.from_pretrained(MODEL_NAME)
feature_extractor = WhisperFeatureExtractor.from_pretrained(MODEL_NAME)

model_8bit = AutoModelForSpeechSeq2Seq.from_pretrained(
     &quot;openai/whisper-large-v3&quot;,
    device_map='auto',
    load_in_8bit=True)

sample = &quot;sample.mp3&quot; #27s long

with torch.inference_mode():
    with open(sample, &quot;rb&quot;) as f:
        inputs = f.read()
        inputs = ffmpeg_read(inputs, feature_extractor.sampling_rate)

        input_features = feature_extractor(inputs, sampling_rate = feature_extractor.sampling_rate, return_tensors='pt')['input_features']

        input_features = torch.tensor(input_features, dtype=torch.float16, device='cuda')

        forced_decoder_ids_output = model_8bit.generate(input_features=input_features, return_timestamps=False)

        out = tokenizer.decode(forced_decoder_ids_output.squeeze())
        print(out)
</code></pre>
","huggingface"
"77653666","Import error in training arguments in Colaboratory","2023-12-13 12:41:06","77653714","0","573","<python><nlp><huggingface>","<p>I am using Google <a href=""https://en.wikipedia.org/wiki/Project_Jupyter#Industry_adoption"" rel=""nofollow noreferrer"">Colaboratory</a> for my <a href=""https://en.wikipedia.org/wiki/Natural_language_processing"" rel=""nofollow noreferrer"">NLP</a> project. I installed <em>transformers</em> and other libraries, but I got an error.</p>
<pre><code>from transformers import Trainer, TrainingArguments

batch_size = 64
logging_steps = len(stationary_dataset_encoded[&quot;train&quot;]) // batch_size
model_name = f&quot;{model_ckpt}-finetuned-stationary-update&quot;
training_args = TrainingArguments(output_dir=model_name,
                                  num_train_epochs=10,
                                  learning_rate=2e-5,
                                  per_device_train_batch_size=batch_size,
                                  per_device_eval_batch_size=batch_size,
                                  weight_decay=0.01,
                                  evaluation_strategy=&quot;epoch&quot;,
                                  disable_tqdm=False,
                                  logging_steps=logging_steps,
                                  push_to_hub=False,
                                  log_level=&quot;error&quot;)
</code></pre>
<p>In the training_args line, I got an import error. It’s showing as:</p>
<pre class=""lang-none prettyprint-override""><code>ImportError                               Traceback (most recent call last)
&lt;ipython-input-98-839907b16fa0&gt; in &lt;cell line: 6&gt;()
      4 logging_steps = len(stationary_dataset_encoded[&quot;train&quot;]) // batch_size
      5 model_name = f&quot;{model_ckpt}-finetuned-stationary-update&quot;
----&gt; 6 training_args = TrainingArguments(output_dir=model_name,
      7                                   num_train_epochs=10,
      8                                   learning_rate=2e-5,

4 frames
/usr/local/lib/python3.10/dist-packages/transformers/training_args.py in _setup_devices(self)
   1785     def __str__(self):
   1786         self_as_dict = asdict(self)
-&gt; 1787
   1788         # Remove deprecated arguments. That code should be removed once
   1789         # those deprecated arguments are removed from TrainingArguments. (TODO: v5)

ImportError: Using the `Trainer` with `PyTorch` requires `accelerate&gt;=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U
</code></pre>
<p>I tried to reinstall <em>transformers</em>, use <code>pip install accelerate -U</code> and also used PyTorch accelerate &gt;= 0.20.1.</p>
<p>How can I resolve this issue?</p>
","huggingface"
"77653125","Incorrect output after defining args in launch.json in vscode","2023-12-13 11:13:44","","0","58","<python><json><vscode-debugger><huggingface>","<p>I'm currently trying to follow <a href=""https://huggingface.co/docs/diffusers/training/controlnet"" rel=""nofollow noreferrer"">this</a> huggingface tutorial on how to train a controlnet.</p>
<p>I set up my machine and everything is running - in order to be able to check what's happening I created a launch.json file to run it with the debugger in order to have an easier way to understand what happend for the case that anything breaks.</p>
<p>my launch.json:</p>
<pre><code>{
    // Use IntelliSense to learn about possible attributes.
    // Hover to view descriptions of existing attributes.
    // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387
    &quot;version&quot;: &quot;0.2.0&quot;,
    &quot;configurations&quot;: [
        {
            &quot;name&quot;: &quot;Python: Current File&quot;,
            &quot;type&quot;: &quot;python&quot;,
            &quot;request&quot;: &quot;launch&quot;,
            &quot;module&quot;: &quot;accelerate.commands.launch&quot;,
            &quot;args&quot;: [
                &quot;./train_controlnet.py&quot;,
                &quot;--pretrained_model_name_or_path&quot;, &quot;stabilityai/stable-diffusion-2-1&quot;,
                &quot;--output_dir&quot;, &quot;/home/chris/tmp/hf_test&quot;,
                &quot;--resolution&quot;, &quot;512&quot;,
                &quot;--learning_rate&quot;, &quot;1e-5&quot;,
                &quot;--validation_image&quot;, &quot;./conditioning_image_1.png&quot;, &quot;./controlnet/conditioning_image_2.png&quot;,
                &quot;--validation_prompt&quot;, &quot;\&quot;red circle with blue background\&quot;&quot;, &quot;\&quot;cyan circle with brown background\&quot;&quot;,
                &quot;--train_batch_size&quot;, &quot;12&quot;,
                &quot;--gradient_accumulation_steps&quot;, &quot;4&quot;,
                &quot;--mixed_precision&quot;, &quot;fp16&quot;,
                &quot;--enable_xformers_memory_efficient_attention&quot;,
                &quot;--dataset_name&quot;, &quot;fusing/fill50k&quot;,
            ],
            &quot;console&quot;: &quot;integratedTerminal&quot;,
            &quot;justMyCode&quot;: false
        }
    ]
}
</code></pre>
<p>The command the debugger creates to run it:</p>
<pre><code>cd /home/chris/repos/diffusers ; /usr/bin/env /home/chris/programs/miniconda3/envs/diffusers/bin/python /home/chris/.vscode/extensions/ms-python.python-2023.20.0/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher 41471 -- -m accelerate.commands.launch ./train_controlnet.py --pretrained_model_name_or_path stabilityai/stable-diffusion-2-1 --output_dir /home/chris/tmp/hf_test --resolution 512 --learning_rate 1e-5 --validation_image ./conditioning_image_1.png ./conditioning_image_2.png --validation_prompt \&quot;red\ circle\ with\ blue\ background\&quot; \&quot;cyan\ circle\ with\ brown\ background\&quot; --train_batch_size 12 --gradient_accumulation_steps 4 --mixed_precision fp16 --enable_xformers_memory_efficient_attention --dataset_name fusing/fill50k 
</code></pre>
<p>My problem is the <code>--validation_prompt</code> argument which I can't get to be formatted correctly. Basically I'd need to remove the backslashes, which I don't know why they're there.</p>
<pre><code>--validation_prompt \&quot;red\ circle\ with\ blue\ background\&quot; \&quot;cyan\ circle\ with\ brown\ background\&quot;
</code></pre>
<p>Let me know if I need or can add something to make my problem clearer.</p>
<p>Help is much appreciated !</p>
","huggingface"
"77646355","How to cutomize the gradio app interface and tigger a function based on output of first interface?","2023-12-12 13:10:12","","2","1124","<python><huggingface><gradio>","<p>Here is the sample code. I want to trigger function2 based on the output of function one.
I did it by two interface but and launch them together but I am not looking for extra interface.</p>
<p><strong>I Tried</strong></p>
<pre><code>
# Define your functions
def function1(input1):
    # Your code here
    return 'hello'

def function2(create_ticket):
    # Your code here
    return 'Your ticket has been created 🏆'

interface1 = gr.Interface(fn=listen_audio, inputs=gr.Audio(sources=&quot;microphone&quot;, type=&quot;filepath&quot;),outputs=&quot;text&quot;)

interface2 = gr.Interface(fn=function2, inputs=[&quot;checkbox&quot;], outputs=&quot;text&quot;)

lst = [x for x in dir(interface1) if '__' not in x]
# Combine them using Blocks
with gr.Blocks() as demo:
    with gr.Row():
        with gr.Column():
            interface1.render()
        with gr.Column():
            # interface2.render()
            interface2.render()
demo.launch()
</code></pre>
<p><strong>Expecting ..</strong></p>
<pre><code>import gradio as gr

def listen_audio(input1):
    return 'problem'

def function2(output_of_listen_audio_function):
    return 'Ticket has been created'
demo = gr.Interface(fn=listen_audio, inputs=gr.Audio(sources=&quot;microphone&quot;, type=&quot;filepath&quot;), outputs=&quot;text&quot;, 
                    flagging_options=[&quot;Create Ticket&quot;],
                    # allow_flagging = &quot;never&quot;
                    )
demo.launch()
</code></pre>
<p><a href=""https://i.sstatic.net/MKXfg.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/MKXfg.png"" alt=""app frontend below"" /></a></p>
","huggingface"
"77645575","How to use a HuggingFace Dataset to fine-tune a spaCy pipeline","2023-12-12 11:03:05","","0","101","<python><nlp><spacy><named-entity-recognition><huggingface>","<p>I am trying to fine-tune a spaCy pipeline, <a href=""https://spacy.io/models/en#en_core_web_sm"" rel=""nofollow noreferrer"">en_core_web_sm</a>, in particular its NER (Named Entity Recognition) component. I found this <a href=""https://huggingface.co/datasets/tner/mit_restaurant"" rel=""nofollow noreferrer"">dataset</a> on HuggingFace which is perfect for the task I want to do it.</p>
<p>I cloned the git repository, with the corresponding <code>json</code> files. Then I tried to use <code>python -m spacy convert mit_restaurant/dataset mit_restaurant/spacy</code>, which should <a href=""https://spacy.io/api/cli#convert"" rel=""nofollow noreferrer"">Convert files into spaCy’s binary training data format, a serialized DocBin, for use with the train command</a>.</p>
<p>However I get this error:</p>
<pre><code>NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020
  warnings.warn(
✔ Generated output file (0 documents):
mit_restaurant/spacy/label.spacy
✔ Generated output file (0 documents):
mit_restaurant/spacy/test.spacy
✔ Generated output file (0 documents):
mit_restaurant/spacy/train.spacy
✔ Generated output file (0 documents):
mit_restaurant/spacy/valid.spacy
</code></pre>
<p>I expected the script to be able to convert the documents, but it does not seem to recognise them.
Are there any intermediate preprocessing steps I need to carry out on the hugging face dataset to get it ready for spaCy training?</p>
","huggingface"
"77643998","requests.exceptions.ProxyError: (MaxRetryError(""HTTPSConnectionPool(host='huggingfa ce.co', port=443)","2023-12-12 06:06:40","","0","370","<python><openai-api><langchain><huggingface>","<p>I am facing below issue while loading the pretrained BERT model from HuggingFace due to SSL certificate error.</p>
<pre><code>requests.exceptions.ProxyError: (MaxRetryError(&quot;HTTPSConnectionPool(host='huggingfa
ce.co', port=443): Max retries exceeded with url: /api/models/hkunlp/instructor-xl 
(Caused by ProxyError('Unable to connect to proxy', SSLError(SSLEOFError(8, 'EOF oc
curred in violation of protocol (_ssl.c:1129)'))))&quot;), '(Request ID: 1dddbdbe-8f1d-45b7-9a43-3eed243a06be)')
</code></pre>
<p>Anyone CAN help me get through it.</p>
","huggingface"
"77642907","How to wipe gradients from UNet2DConditionModel","2023-12-11 23:32:21","","1","78","<python-3.x><huggingface><stable-diffusion><diffusers>","<p>I am working with the &quot;CompVis/ldm-text2im-large-256&quot; building on top of the prompt-to-prompt code.</p>
<p>model = DiffusionPipeline.from_pretrained(model_id, height=IMAGE_RES, width=IMAGE_RES).to(device)</p>
<p>Whenever I call the <a href=""https://github.com/google/prompt-to-prompt/blob/9c472e44aa1b607da59fea94820f7be9480ec545/ptp_utils.py#L98C5-L98C19"" rel=""nofollow noreferrer"">text2image_ldm</a> method without torch.no_grad, gradients accumulate on line:</p>
<p><a href=""https://github.com/google/prompt-to-prompt/blob/9c472e44aa1b607da59fea94820f7be9480ec545/ptp_utils.py#L66"" rel=""nofollow noreferrer"">noise_pred = model.unet(latents_input, t, encoder_hidden_states=context)[&quot;sample&quot;]</a></p>
<p>I want to be able to use gradients to edit outputs but later wipe them out as well. I have tried model.unet.zero_grad() and</p>
<pre><code>for param in module.parameters():
     param.grad = None
     torch.cuda.empty_cache()
</code></pre>
<p>but both do not resolve the gradient accumulation issue. How do I delete gradients for this pipeline?</p>
<p>Diffusers Version: diffusers==0.3.0</p>
","huggingface"
"77639183","Hugging Face | tokenizer.batch_encode_plus inconsistent columns on different datasets","2023-12-11 11:17:41","","0","78","<nlp><huggingface-transformers><xgboost><huggingface><huggingface-tokenizers>","<p>Im currently using tokenizer.batch_encode_plus and the same tokenizer is applied to different datasets/list of text. df_train_feats and df_test_feats produce different column lengths.</p>
<pre><code>df_test_feats.shape
Out[2]: (2, 8)

df_train_feats.shape
Out[3]: (2, 20)
</code></pre>
<p>Due to this inconsistent column name, it will result in an error when passing it to xgboost model.</p>
<pre><code>import os, sys
import pandas as pd

import torch
from transformers import AutoModel, AutoTokenizer
str_token = 'distilbert-base-uncased'


if __name__ == '__main__':
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        model_check_point = 'distilbert-base-uncased'
        model = AutoModel.from_pretrained(model_check_point)
        tokenizer = AutoTokenizer.from_pretrained(model_check_point, add_prefix_space=True, use_fast=False)
        df_train_feats_encoded = tokenizer.batch_encode_plus([&quot;today I went to the movies &quot;, &quot;today I went to the movies and had dinner at saints a new resturant in italy&quot;], max_length=20, padding=True)
        df_train_feats = pd.DataFrame(df_train_feats_encoded['input_ids'])
        
        df_test_feats_encoded = tokenizer.batch_encode_plus(['we could not play paddal','it rain most of the afternoon'], max_length=20, padding=True)
        df_test_feats = pd.DataFrame(df_test_feats_encoded['input_ids'])
</code></pre>
<p>How do I fix this so the the input data to the xgboost has the same dataframe shape or the outputs from the tokenizer is consistent regardless of the dataset?</p>
","huggingface"
"77639043","How to obtain hidden state ids from a wav2vec2 model?","2023-12-11 10:54:30","","0","79","<pytorch><huggingface>","<p>with the following code I extract the hidden state from a wav2vec2 model, but I wonder is it feasible to obtain corresponding token ids. Many thx to any suggestions/comments.</p>
<pre><code>import librosa
import torch
from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model

input_audio, sample_rate = librosa.load(&quot;/content/bla.wav&quot;,  sr=16000)

model_name = &quot;facebook/wav2vec2-large-xlsr-53&quot;
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)
model = Wav2Vec2Model.from_pretrained(model_name)

i= feature_extractor(input_audio, return_tensors=&quot;pt&quot;, sampling_rate=sample_rate)
with torch.no_grad():
  o= model(i.input_values)
print(o.keys())
print(o.last_hidden_state.shape)
print(o.extract_features.shape)

</code></pre>
","huggingface"
"77633352","Chatbox is returning one word answer instead of full sentence","2023-12-10 00:47:23","","0","130","<langchain><huggingface><py-langchain><retrieval-augmented-generation>","<p>I am creating a Book recommendation system with Retrieval Augmented Generation(RAG) framework and vector database in LangChain. But I am getting just the answer not the whole sentence like a chatbot should.</p>
<p>This is the code:</p>
<pre><code>embeddings = HuggingFaceEmbeddings(model_name='bert-base-uncased')
docsearch = FAISS.from_documents(texts, embeddings)
llm = HuggingFaceHub(repo_id = &quot;google/flan-t5-base&quot;, model_kwargs={&quot;temperature&quot;:0.6,&quot;max_length&quot;: 500, &quot;max_new_tokens&quot;: 200})
prompt_template = &quot;&quot;&quot;

Compare the book given in question with others in the retriever based on genre and description.
Return a complete sentence with the full title of the book and describe the similarities between the books.

question: {question}
context: {context}
&quot;&quot;&quot;
prompt = PromptTemplate(template=prompt_template, input_variables=[&quot;context&quot;, &quot;question&quot;])
retriever=docsearch.as_retriever()
qa = RetrievalQA.from_chain_type(llm=llm, chain_type=&quot;stuff&quot;, retriever=retriever,chain_type_kwargs = {&quot;prompt&quot;: prompt})
qa.run({&quot;query&quot;: &quot;Which book except 'To Kill A Mocking Bird' is similar to it?&quot;} )
</code></pre>
<p>I am just getting the name of the book. I atleast need a full sentence and maybe some description.
I cannot use bigger models as some dont work and others time out. OpenAI models also returned limit exceeded. Does it need a better prompt?</p>
","huggingface"
"77631334","How to add GroundingDino to HuggingFace?","2023-12-09 12:50:56","","0","91","<huggingface-transformers><image-segmentation><huggingface><huggingface-hub>","<p>When I attempting to deploy Segment and Track Anything (SAM Track)  (<a href=""https://huggingface.co/spaces/aikenml/Segment-And-Track-Anything-Model"" rel=""nofollow noreferrer"">https://huggingface.co/spaces/aikenml/Segment-And-Track-Anything-Model</a>) (SAM Track, <a href=""https://github.com/z-x-yang/Segment-and-Track-Anything"" rel=""nofollow noreferrer"">https://github.com/z-x-yang/Segment-and-Track-Anything</a>) to HugginFace(HF) Space, I encountered the following error: <a href=""https://huggingface.co/spaces/aikenml/Segment-And-Track-Anything-Model"" rel=""nofollow noreferrer"">https://huggingface.co/spaces/aikenml/Segment-And-Track-Anything-Model</a>
<a href=""https://i.sstatic.net/0nYdg.png"" rel=""nofollow noreferrer"">no module named 'groundingdino'</a></p>
<p>This is working locally, and 'groundingdino' is specified in the requirements.txt in HF. However, the error persists on Hugging Face Spaces. How can I resolve this issue?</p>
","huggingface"
"77623703","Is it possible to send prompt message to Huggingface chat role ""system""?","2023-12-08 00:21:48","","0","186","<huggingface-transformers><huggingface>","<p>I'm following the Quickstart guide below for building an LLM chatbot on my Snowflake db however I'm using Huggingface instead of OpenAI.</p>
<p><a href=""https://quickstarts.snowflake.com/guide/frosty_llm_chatbot_on_streamlit_snowflake/index.html#5"" rel=""nofollow noreferrer"">https://quickstarts.snowflake.com/guide/frosty_llm_chatbot_on_streamlit_snowflake/index.html#5</a></p>
<p>The code below is what I currently have in my prompt.py file, It is a message stating how Huggingface should interact with the user, this message is used in the function <code>get_system_prompt()</code> at the bottom of the prompts.py here:</p>
<p><a href=""https://github.com/Snowflake-Labs/sfguide-frosty-llm-chatbot-on-streamlit-snowflake/blob/main/src/prompts.py#L15C6-L47"" rel=""nofollow noreferrer"">https://github.com/Snowflake-Labs/sfguide-frosty-llm-chatbot-on-streamlit-snowflake/blob/main/src/prompts.py#L15C6-L47</a></p>
<p>The function <code>get_system_prompt()</code> is called in the main app file... now this is where I believe the issue is.  Openai has a 'system' role that uses the <code>get_system_prompt()</code> function... but huggingface does not have this role when I try I get &quot;Internal Error&quot;
From what I can see in the huggingface chat doc it doesn't seem to be an option, only user or assistant: <a href=""https://huggingface.co/docs/transformers/chat_templating"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/chat_templating</a></p>
<p>For reference, the main app file looks like and this is where it's calling the <code>get_system_prompt()</code>: (this except this is using OpenAI and I'm using Huggingface):
<a href=""https://github.com/Snowflake-Labs/sfguide-frosty-llm-chatbot-on-streamlit-snowflake/blob/main/src/frosty_app.py#L13"" rel=""nofollow noreferrer"">https://github.com/Snowflake-Labs/sfguide-frosty-llm-chatbot-on-streamlit-snowflake/blob/main/src/frosty_app.py#L13</a></p>
<p>This is the huggingchat library I'm using to access huggingface: <a href=""https://github.com/Soulter/hugging-chat-api/tree/master/src/hugchat"" rel=""nofollow noreferrer"">https://github.com/Soulter/hugging-chat-api/tree/master/src/hugchat</a></p>
<p>Error message I see:</p>
<blockquote>
<p>File &quot;/Users/Desktop/SnowChatbox/venv/lib/python3.8/site-packages/streamlit/runtime/scriptrunner/script_runner.py&quot;, line 534, in _run_script
exec(code, module.<strong>dict</strong>)</p>
</blockquote>
<blockquote>
<p>File &quot;/Users/Desktop/SnowChatbox/LLM_Chatbox/streamlit_app.py&quot;, line 79, in

st.write(response)</p>
</blockquote>
<blockquote>
<p>File &quot;/Users/Desktop/SnowChatbox/venv/lib/python3.8/site-packages/streamlit/runtime/metrics_util.py&quot;,
line 396, in wrapped_func
result = non_optional_func(*args, **kwargs)</p>
</blockquote>
<blockquote>
<p>File &quot;/User/Desktop/SnowChatbox/venv/lib/python3.8/site-packages/streamlit/elements/write.py&quot;,
line 259, in write
stringified_arg = str(arg)</p>
</blockquote>
<blockquote>
<p>File &quot;/hugchat/message.py&quot;, line 228, in <strong>str</strong>
return self.wait_until_done()</p>
</blockquote>
<blockquote>
<p>File &quot;/hugchat/message.py&quot;, line 194, in wait_until_done
self.<strong>next</strong>()</p>
</blockquote>
<blockquote>
<p>File &quot;/hugchat/message.py&quot;, line 149, in <strong>next</strong>
raise self.error</p>
</blockquote>
<blockquote>
<p>File &quot;/hugchat/message.py&quot;, line 100, in <strong>next</strong>
self._filterResponse(a)</p>
</blockquote>
<blockquote>
<p>File &quot;/hugchat/message.py&quot;, line 85, in _filterResponse
raise ChatError(f&quot;Server returns an error: {obj['message']}&quot;)</p>
</blockquote>
","huggingface"
"77614503","What's the difference between the huggingface implementation of device_map=‘auto’ and this scripts?","2023-12-06 15:46:34","","0","244","<gpu><torch><distributed><huggingface><large-language-model>","<p>I tried to implement a model parallelism, and found it performance similar (based on the GPU behaviour) when I was using the huggingface <code>device_map=&quot;auto&quot;</code>, so I wonder what is the difference (except the advanced technique they have)? Maybe I will have to look at their implementation.</p>
<pre><code>import torch
import torch.nn as nn

class SplitModel(nn.Module):
    def __init__(self, original_model):
        super().__init__()
        num_gpus = torch.cuda.device_count()
        total_layers = len(original_model.model.layers)
        layers_per_gpu = total_layers // num_gpus

        self.layer_to_device = {}
        self.embed_tokens = original_model.model.embed_tokens.to('cuda:0')
        self.layer_to_device[self.embed_tokens] = 'cuda:0'

        self.layers = nn.ModuleList()
        for i, layer in enumerate(original_model.model.layers):
            gpu_id = min(i // layers_per_gpu, num_gpus - 1)  # Distribute layers across GPUs
            assigned_gpu = f'cuda:{gpu_id}'
            self.layers.append(layer.to(assigned_gpu))
            self.layer_to_device[layer] = assigned_gpu

        # Assign norm and lm_head to the last GPU
        last_gpu = f'cuda:{num_gpus - 1}'
        self.norm = original_model.model.norm.to(last_gpu)
        self.lm_head = original_model.lm_head.to(last_gpu)
        self.layer_to_device[self.norm] = last_gpu
        self.layer_to_device[self.lm_head] = last_gpu

    def forward(self, x):
        x = x.to(self.layer_to_device[self.embed_tokens])
        x = self.embed_tokens(x)
        for layer in self.layers:
            if isinstance(x, tuple):
                x = x[0]
            x = x.to(self.layer_to_device[layer])
            x = layer(x)
        if isinstance(x, tuple):
            x = x[0]
        x = x.to(self.layer_to_device[self.norm])
        x = self.norm(x)
        x = x.to(self.layer_to_device[self.lm_head])
        x = self.lm_head(x)
        return x
</code></pre>
<p>huggingface, split LLM into multiple GPUs</p>
","huggingface"
"77611570","RAGAS Evaluator for llm RAG application threw error","2023-12-06 08:33:45","","1","1259","<python><chatbot><huggingface><large-language-model><py-langchain>","<pre><code>    from ragas.testset import TestsetGenerator
    from langchain.embeddings import OpenAIEmbeddings
    from langchain.chat_models import ChatOpenAI
    from ragas.llms import LangchainLLM
    from ragas.llms import LangchainLLM

    llm_wrapper = ChatOpenAI(model=&quot;gpt-3.5-turbo&quot;)

    generator_llm = LangchainLLM(llm=llm_wrapper)
    critic_llm = LangchainLLM(llm=llm_wrapper)
    embeddings_model =  OpenAIEmbeddings() 

    testset_distribution = {
    &quot;simple&quot;: 0.25,
    &quot;reasoning&quot;: 0.5,
    &quot;multi_context&quot;: 0.0,
    &quot;conditional&quot;: 0.25,
       }

    chat_qa = 0.2


    test_generator = TestsetGenerator(
    generator_llm=generator_llm,
    critic_llm=critic_llm,
    embeddings_model=embeddings_model,
    testset_distribution=testset_distribution,
    chat_qa=chat_qa,
     )
    from ragas.llms import LangchainLLM

    from langchain.document_loaders import PyPDFDirectoryLoader
    data_file = &quot;/content/&quot;
    loader = PyPDFDirectoryLoader(data_file+&quot;/&quot;)
    documents = loader.load()
    testset = test_generator.generate(documents, test_size=2)
</code></pre>
<p>I could create the test data set using RAGAS llm evaluator. but I am not able to use evaluator function since it threw an error .  can anyone help me with it?</p>
<pre><code>     from ragas import evaluate
     result = evaluate(
     testset, # selecting only 3
     metrics=[
        context_precision,
        faithfulness,
        answer_relevancy,
        context_recall,
      ],)  
</code></pre>
<p>this threw error</p>
<pre><code>AttributeError

Traceback (most recent call last)
&lt;ipython-input-214-4a046bb39140&gt; in &lt;cell line: 2&gt;()
      1 from ragas import evaluate
----&gt; 2 result = evaluate(
      3     testset, # selecting only 3
      4     metrics=[
      5         context_precision,

2 frames
/usr/local/lib/python3.10/dist-packages/ragas/validation.py in &lt;dictcomp&gt;(.0)
     10     Remap the column names in case dataset uses different column names
     11     &quot;&quot;&quot;
---&gt; 12     column_map = {k: v for k, v in column_map.items() if v in dataset.column_names}
     13     inverse_column_map = {v: k for k, v in column_map.items()}
     14     return dataset.from_dict(

AttributeError: 'TestDataset' object has no attribute 'column_names'
</code></pre>
","huggingface"
"77603438","What padding values should be used for huggingface tokenizers?","2023-12-05 01:58:00","","0","20","<tokenize><huggingface><bart>","<p>I am using Mbart50 to convert Nepalese to English and am not sure what values I should use for padding. I need to figure out the padding values for both English and Nepalese.
My tokenizer code:</p>
<pre><code>tokenizer = MBart50TokenizerFast.from_pretrained(model_name)
tokenizer.src_lang = &quot;ne_NP&quot;
tokenizer.tgt_lang = &quot;en_XX&quot; 
</code></pre>
","huggingface"
"77603219","Get attention masks from HF pipelines","2023-12-05 00:20:42","","1","241","<nlp><huggingface-transformers><huggingface><huggingface-tokenizers><accelerate>","<p>How should returned attention masks be accessed from the FeatureExtractionPipeline in Huggingface?</p>
<p>The code below takes an embedding model, distributes it and a huggingface dataset across 8 GPUs on a single node, and performs inference on the inputs. The code requires the attention masks for mean pooling.</p>
<p>Code example:</p>
<pre class=""lang-py prettyprint-override""><code>from accelerate import Accelerator
from accelerate.utils import tqdm
from transformers import AutoTokenizer, AutoModel
from optimum.bettertransformer import BetterTransformer

import torch

from datasets import load_dataset

from transformers import pipeline

accelerator = Accelerator()

model_name = &quot;BAAI/bge-large-en-v1.5&quot;

tokenizer = AutoTokenizer.from_pretrained(model_name,)

model = AutoModel.from_pretrained(model_name,)

pipe = pipeline(
    &quot;feature-extraction&quot;,
    model=model,
    tokenizer=tokenizer,
    max_length=512,
    truncation=True,
    padding=True,
    pad_to_max_length=True,
    batch_size=256,
    framework=&quot;pt&quot;,
    return_tensors=True,
    return_attention_mask=True,
    device=(accelerator.device)
)

dataset = load_dataset(
    &quot;wikitext&quot;,
    &quot;wikitext-2-v1&quot;,
    split=&quot;train&quot;,
)

#Mean Pooling - Take attention mask into account for correct averaging
def mean_pooling(model_output, attention_mask):
    token_embeddings = model_output[0] #First element of model_output contains all token embeddings
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)


# Assume 8 processes

with accelerator.split_between_processes(dataset[&quot;text&quot;]) as data:

    for out in pipe(data):

        sentence_embeddings = mean_pooling(out, out[&quot;attention_mask&quot;])
</code></pre>
<p>I need the attention maks from pipe to use for mean pooling.</p>
<p>Best,</p>
<p>Enrico</p>
","huggingface"
"77602975","PermissionError in Hugging Face Spaces when Deploying FastAPI App: [Errno 13] Permission denied: 'app.csv'","2023-12-04 22:52:16","","0","293","<docker><fastapi><huggingface>","<p>This is the dockerfile I have in the hugging face space:</p>
<pre><code># Use the base Docker image
FROM circulartextapp/lastread

# Set the working directory to /app (if needed)
WORKDIR /app

# Copy the current directory contents into the container at /app
COPY . /app

# Example CMD (modify as needed)
CMD [&quot;uvicorn&quot;, &quot;app.main:app&quot;, &quot;--host&quot;, &quot;0.0.0.0&quot;, &quot;--port&quot;, &quot;7860&quot;]
</code></pre>
<p>I'm encountering a PermissionError when deploying my FastAPI app on Hugging Face Spaces. The application works flawlessly in the local container, but when deployed on the Hugging Face Hub, I face the following error</p>
<pre><code>ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File &quot;/usr/local/lib/python3.11/site-packages/uvicorn/protocols/http/h11_impl.py&quot;, line 408, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/usr/local/lib/python3.11/site-packages/uvicorn/middleware/proxy_headers.py&quot;, line 84, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/usr/local/lib/python3.11/site-packages/fastapi/applications.py&quot;, line 1106, in __call__
    await super().__call__(scope, receive, send)
  File &quot;/usr/local/lib/python3.11/site-packages/starlette/applications.py&quot;, line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File &quot;/usr/local/lib/python3.11/site-packages/starlette/middleware/errors.py&quot;, line 184, in __call__
    raise exc
  File &quot;/usr/local/lib/python3.11/site-packages/starlette/middleware/errors.py&quot;, line 162, in __call__
    await self.app(scope, receive, _send)
  File &quot;/usr/local/lib/python3.11/site-packages/starlette/middleware/exceptions.py&quot;, line 79, in __call__
    raise exc
  File &quot;/usr/local/lib/python3.11/site-packages/starlette/middleware/exceptions.py&quot;, line 68, in __call__
    await self.app(scope, receive, sender)
  File &quot;/usr/local/lib/python3.11/site-packages/fastapi/middleware/asyncexitstack.py&quot;, line 20, in __call__
    raise e
  File &quot;/usr/local/lib/python3.11/site-packages/fastapi/middleware/asyncexitstack.py&quot;, line 17, in __call__
    await self.app(scope, receive, send)
  File &quot;/usr/local/lib/python3.11/site-packages/starlette/routing.py&quot;, line 718, in __call__
    await route.handle(scope, receive, send)
  File &quot;/usr/local/lib/python3.11/site-packages/starlette/routing.py&quot;, line 276, in handle
    await self.app(scope, receive, send)
  File &quot;/usr/local/lib/python3.11/site-packages/starlette/routing.py&quot;, line 66, in app
    response = await func(request)
               ^^^^^^^^^^^^^^^^^^^
  File &quot;/usr/local/lib/python3.11/site-packages/fastapi/routing.py&quot;, line 274, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/usr/local/lib/python3.11/site-packages/fastapi/routing.py&quot;, line 191, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/app/app/main.py&quot;, line 80, in generate_app_csv_and_run_aa_backend
    with open(CSV_FILE_PATH0, mode='w') as csv_file:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
PermissionError: [Errno 13] Permission denied: 'app.csv' 
</code></pre>
<p>this is the fast api code i am using</p>
<pre><code>from fastapi import FastAPI, HTTPException, Form
from fastapi.staticfiles import StaticFiles
from fastapi.responses import HTMLResponse, JSONResponse, FileResponse
import subprocess
import json
from fastapi import FastAPI
from fastapi.responses import FileResponse, HTMLResponse
from fastapi.staticfiles import StaticFiles
import uvicorn

app = FastAPI()



CSV_FILE_PATH = 'slideinfo.csv'
CSV_FILE_PATH0 = 'app.csv'
PYTHON_SCRIPT_PATH0 = 'AAmain.py'
PYTHON_SCRIPT_PATH = 'AGmain.py'
PYTHON_INTERPRETER = 'python'


# Set up static files
app.mount(&quot;/static&quot;, StaticFiles(directory=&quot;.&quot;), name=&quot;static&quot;)


# Route to serve timing_script.js
@app.get(&quot;/timing_script.js&quot;, response_class=FileResponse)
async def read_timing_script_js():
    return FileResponse(&quot;static/timing_script.js&quot;)

# Route to serve batch.csv
@app.get(&quot;/batch.csv&quot;, response_class=FileResponse)
async def read_batch_csv():
    return FileResponse(&quot;static/batch.csv&quot;)

# Route to serve wav.wav
@app.get(&quot;/wav.wav&quot;, response_class=FileResponse)
async def read_wav():
    return FileResponse(&quot;wav.wav&quot;)

# Route to serve wav3.wav
@app.get(&quot;/wav3.wav&quot;, response_class=FileResponse)
async def read_wav3():
    return FileResponse(&quot;wav3.wav&quot;)

# HTML Response for the root endpoint
@app.get(&quot;/&quot;, response_class=FileResponse)
async def read_root():
    return FileResponse(&quot;static/index.html&quot;)

# Endpoint for generating CSV and running the backend script
@app.post(&quot;/generate_csv_and_run_backend&quot;)
async def generate_csv_and_run_backend(
    low: str = Form(...),
    mid: str = Form(...),
    high: str = Form(...),
):
    csv_data = f&quot;Low Option,Mid Option,High Option\n{low}, {mid}, {high}\n&quot;

    with open(CSV_FILE_PATH, mode='w') as csv_file:
        csv_file.write(csv_data)

    process = subprocess.Popen([PYTHON_INTERPRETER, PYTHON_SCRIPT_PATH, CSV_FILE_PATH], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    stdout, stderr = process.communicate()

    if process.returncode != 0:
        print(stderr)
        raise HTTPException(status_code=500, detail=&quot;Error running the backend script&quot;)

    return JSONResponse(content={&quot;message&quot;: &quot;CSV file generated and backend script executed successfully&quot;})

# Endpoint for generating app.csv and running AAmain script
@app.post(&quot;/generate_app_csv_and_run_aa_backend&quot;)
async def generate_app_csv_and_run_aa_backend(user_input: dict):
    app_csv_data = f&quot;User Input\n{user_input['userInput']}\n&quot;

    with open(CSV_FILE_PATH0, mode='w') as csv_file:
        csv_file.write(app_csv_data)

    process = subprocess.Popen([PYTHON_INTERPRETER, PYTHON_SCRIPT_PATH0, CSV_FILE_PATH0], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    stdout, stderr = process.communicate()

    if process.returncode != 0:
        print(stderr)
        raise HTTPException(status_code=500, detail=&quot;Error running the AAmain backend script&quot;)

    # Return the generated CSV file as a response
    return FileResponse(CSV_FILE_PATH0)

if __name__ == &quot;__main__&quot;:
    uvicorn.run(app, host=&quot;0.0.0.0&quot;, port=8000)
</code></pre>
<p>This is how the image looks inside a container
<a href=""https://i.sstatic.net/OizxZ.png"" rel=""nofollow noreferrer"">docker image working inside a container container</a></p>
","huggingface"
"77602220","InvalidHeaderDeserialization Error After Finetune Training of LLM has Completed with Hugging Face Library","2023-12-04 19:52:51","","0","402","<python><huggingface>","<p>I am trying to fine tune flan-t5-large to classify financial tweets. I'm running into an error message which occurs immediately after the model completes training. I'm unable to find existing posts in the context of the Hugging Face library. Given this, I'm having a very hard time debugging and resolving this error message, and am hoping that someone more familiar with the Hugging Face library could help.</p>
<pre><code>import transformers
import textwrap
from transformers import AutoModelForCausalLM, AutoModelForSeq2SeqLM, AutoTokenizer
import os
import sys
from typing import List

from peft import (
    LoraConfig,
    get_peft_model,
    get_peft_model_state_dict,
    prepare_model_for_int8_training,
)

import fire
import torch
from datasets import load_dataset
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib as mpl
import seaborn as sns
from pylab import rcParams
import json

get_ipython().run_line_magic('matplotlib', 'inline')
sns.set(rc={'figure.figsize':(8, 6)})
sns.set(rc={'figure.dpi':100})
sns.set(style='white', palette='muted', font_scale=1.2)

DEVICE = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;
BASE_MODEL = &quot;/Projects/llmtest1/models/flan-t5-large/&quot;

model = AutoModelForSeq2SeqLM.from_pretrained(
    pretrained_model_name_or_path=BASE_MODEL ,
    load_in_8bit=True,
    torch_dtype=torch.float16,
    device_map=&quot;auto&quot;,
)

tokenizer = AutoTokenizer.from_pretrained( 
    pretrained_model_name_or_path=BASE_MODEL)

tokenizer.pad_token_id = (0)
tokenizer.padding_side = &quot;left&quot;

data = load_dataset(&quot;ghbacct/twitter-financial-news-sentiment-classification&quot;)

def add_new_column(example):
    return {&quot;instruction&quot;: &quot;Detect the sentiment of the tweet.&quot;}

data = data.map(add_new_column)
data = data.rename_column(&quot;text&quot;, &quot;input&quot;)
data = data.rename_column(&quot;label&quot;, &quot;output&quot;)

CUTOFF_LEN = 256

def generate_prompt(data_point):
    return f&quot;&quot;&quot;Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.  # noqa: E501
### Instruction:
{data_point[&quot;instruction&quot;]}
### Input:
{data_point[&quot;input&quot;]}
### Response:
{data_point[&quot;output&quot;]}&quot;&quot;&quot;

def tokenize(prompt, add_eos_token=True):
    result = tokenizer(
        prompt,
        truncation=True,
        max_length=CUTOFF_LEN,
        padding=False,
        return_tensors=None,
    )
    if (
        result[&quot;input_ids&quot;][-1] != tokenizer.eos_token_id
        and len(result[&quot;input_ids&quot;]) &lt; CUTOFF_LEN
        and add_eos_token
    ):
        result[&quot;input_ids&quot;].append(tokenizer.eos_token_id)
        result[&quot;attention_mask&quot;].append(1)

    result[&quot;labels&quot;] = result[&quot;input_ids&quot;].copy()
    return result

def generate_and_tokenize_prompt(data_point):
    full_prompt = generate_prompt(data_point)
    tokenized_full_prompt = tokenize(full_prompt)
    return tokenized_full_prompt

train_val = data[&quot;train&quot;].train_test_split(
    test_size=200, shuffle=True, seed=42
)
train_data = (
    train_val[&quot;train&quot;].shuffle().map(generate_and_tokenize_prompt)
)
val_data = (
    train_val[&quot;test&quot;].shuffle().map(generate_and_tokenize_prompt)
)

LORA_R = 8
LORA_ALPHA = 16
LORA_DROPOUT= 0.05
LORA_TARGET_MODULES = [&quot;q&quot;,&quot;v&quot;,]
BATCH_SIZE = 128
MICRO_BATCH_SIZE = 4
GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE
LEARNING_RATE = 3e-4
TRAIN_STEPS = 300
OUTPUT_DIR = &quot;experiments&quot;

model = prepare_model_for_int8_training(model)
config = LoraConfig(
    r=LORA_R,
    lora_alpha=LORA_ALPHA,
    target_modules=LORA_TARGET_MODULES,
    lora_dropout=LORA_DROPOUT,
    bias=&quot;none&quot;,
    task_type=&quot;CAUSAL_LM&quot;,
)
model = get_peft_model(model, config)

training_arguments = transformers.TrainingArguments(
    per_device_train_batch_size=MICRO_BATCH_SIZE,
    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,
    warmup_steps=100,
    max_steps=TRAIN_STEPS,
    learning_rate=LEARNING_RATE,
    fp16=True,
    logging_steps=10,
    optim=&quot;adamw_torch&quot;,
    evaluation_strategy=&quot;steps&quot;,
    save_strategy=&quot;steps&quot;,
    eval_steps=50,
    save_steps=50,
    output_dir=OUTPUT_DIR,
    save_total_limit=3,
    load_best_model_at_end=True,
    report_to=&quot;tensorboard&quot;
)

data_collator = transformers.DataCollatorForSeq2Seq(
    tokenizer, 
    pad_to_multiple_of=8, 
    return_tensors=&quot;pt&quot;, 
    padding=True)

trainer = transformers.Trainer(
    model=model,
    train_dataset=train_data,
    eval_dataset=val_data,
    args=training_arguments,
    data_collator=data_collator
)
model.config.use_cache = False
old_state_dict = model.state_dict
model.state_dict = (
    lambda self, *_, **__: get_peft_model_state_dict(
        self, old_state_dict()
    )
).__get__(model, type(model))

    model.state_dict = (
        lambda self, *_, **__: get_peft_model_state_dict(self, old_state_dict())
    ).__get__(model, type(model))

trainer.train()
</code></pre>
<pre><code>trainer = transformers.Trainer(
    model=model,
    train_dataset=train_data,
    eval_dataset=val_data,
    args=training_arguments,
    data_collator=data_collator
)
model.config.use_cache = False
old_state_dict = model.state_dict
model.state_dict = (
    lambda self, *_, **__: get_peft_model_state_dict(
        self, old_state_dict()
    )
).__get__(model, type(model))


    model.state_dict = (
        lambda self, *_, **__: get_peft_model_state_dict(self, old_state_dict())
    ).__get__(model, type(model))

#peft_model = torch.compile(peft_model)

trainer.train()
#peft_model.save_pretrained(OUTPUT_DIR)
</code></pre>
<pre><code>/home/nickjtay/Projects/llmtest1/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:298: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f&quot;MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization&quot;)
/home/nickjtay/Projects/llmtest1/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:298: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f&quot;MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization&quot;)
/home/nickjtay/Projects/llmtest1/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:298: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f&quot;MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization&quot;)
/home/nickjtay/Projects/llmtest1/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:298: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f&quot;MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization&quot;)
/home/nickjtay/Projects/llmtest1/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:298: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f&quot;MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization&quot;)
---------------------------------------------------------------------------
SafetensorError                           Traceback (most recent call last)
Cell In[31], line 18
     10 model.state_dict = (
     11     lambda self, *_, **__: get_peft_model_state_dict(
     12         self, old_state_dict()
     13     )
     14 ).__get__(model, type(model))
     16 #peft_model = torch.compile(peft_model)
---&gt; 18 trainer.train()
     19 #peft_model.save_pretrained(OUTPUT_DIR)

File ~/Projects/llmtest1/lib/python3.10/site-packages/transformers/trainer.py:1530, in Trainer.train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)
   1528         hf_hub_utils.enable_progress_bars()
   1529 else:
-&gt; 1530     return inner_training_loop(
   1531         args=args,
   1532         resume_from_checkpoint=resume_from_checkpoint,
   1533         trial=trial,
   1534         ignore_keys_for_eval=ignore_keys_for_eval,
   1535     )

File ~/Projects/llmtest1/lib/python3.10/site-packages/transformers/trainer.py:1947, in Trainer._inner_training_loop(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)
   1944     elif is_sagemaker_mp_enabled():
   1945         smp.barrier()
-&gt; 1947     self._load_best_model()
   1949 # add remaining tr_loss
   1950 self._total_loss_scalar += tr_loss.item()

File ~/Projects/llmtest1/lib/python3.10/site-packages/transformers/trainer.py:2166, in Trainer._load_best_model(self)
   2164 if hasattr(model, &quot;active_adapter&quot;) and hasattr(model, &quot;load_adapter&quot;):
   2165     if os.path.exists(best_adapter_model_path) or os.path.exists(best_safe_adapter_model_path):
-&gt; 2166         model.load_adapter(self.state.best_model_checkpoint, model.active_adapter)
   2167         # Load_adapter has no return value present, modify it when appropriate.
   2168         from torch.nn.modules.module import _IncompatibleKeys

File ~/Projects/llmtest1/lib/python3.10/site-packages/peft/peft_model.py:647, in PeftModel.load_adapter(self, model_id, adapter_name, is_trainable, **kwargs)
    644         peft_config.inference_mode = not is_trainable
    645     self.add_adapter(adapter_name, peft_config)
--&gt; 647 adapters_weights = load_peft_weights(model_id, device=torch_device, **hf_hub_download_kwargs)
    649 # load the weights into the model
    650 load_result = set_peft_model_state_dict(self, adapters_weights, adapter_name=adapter_name)

File ~/Projects/llmtest1/lib/python3.10/site-packages/peft/utils/save_and_load.py:270, in load_peft_weights(model_id, device, **hf_hub_download_kwargs)
    264             raise ValueError(
    265                 f&quot;Can't find weights for {model_id} in {model_id} or in the Hugging Face Hub. &quot;
    266                 f&quot;Please check that the file {WEIGHTS_NAME} or {SAFETENSORS_WEIGHTS_NAME} is present at {model_id}.&quot;
    267             )
    269 if use_safetensors:
--&gt; 270     adapters_weights = safe_load_file(filename, device=device)
    271 else:
    272     adapters_weights = torch.load(filename, map_location=torch.device(device))

File ~/Projects/llmtest1/lib/python3.10/site-packages/safetensors/torch.py:308, in load_file(filename, device)
    285 &quot;&quot;&quot;
    286 Loads a safetensors file into torch format.
    287 
   (...)
    305 ```
    306 &quot;&quot;&quot;
    307 result = {}
--&gt; 308 with safe_open(filename, framework=&quot;pt&quot;, device=device) as f:
    309     for k in f.keys():
    310         result[k] = f.get_tensor(k)

SafetensorError: Error while deserializing header: InvalidHeaderDeserialization
</code></pre>
","huggingface"
"77596218","Huggingface peft error message AttributeError: 'Linear8bitLt' object has no attribute 'state'","2023-12-03 21:19:09","","0","935","<python><huggingface><peft>","<p>I'm loading a huggingface <a href=""https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment"" rel=""nofollow noreferrer"">dataset</a> and Mistral-7B-Instruct-v0.1 to finetune for detection of  sentiment. I'm running the following <a href=""https://colab.research.google.com/drive/1X85FLniXx_NyDsh_F_aphoIAy63DKQ7d?usp=sharing#scrollTo=3xG6yyBJbW4N"" rel=""nofollow noreferrer"">notebook</a>, but updated it to use the above model and dataset instead. I cannot figure out what the cause of the error is and there doesn't seem to be an existing post on this issue in this context. Is the issue that I need to convert the target variable from int64 to floating point? Is the problem something else entirely?</p>
<p>FYI, I have upgraded the peft and torch libraries, since I came across that suggestion elsewhere.</p>
<p>Here is the code:</p>
<pre><code>model = prepare_model_for_int8_training(model)
config = LoraConfig(
    r=LORA_R,
    lora_alpha=LORA_ALPHA,
    target_modules=LORA_TARGET_MODULES,
    lora_dropout=LORA_DROPOUT,
    bias=&quot;none&quot;,
    task_type=&quot;CAUSAL_LM&quot;,
)
model = get_peft_model(model, config)
model.print_trainable_parameters()
</code></pre>
<p>Here is the error message:</p>
<pre><code> ---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
Cell In[77], line 10
      1 model = prepare_model_for_int8_training(model)
      2 config = LoraConfig(
      3     r=LORA_R,
      4     lora_alpha=LORA_ALPHA,
   (...)
      8     task_type=&quot;CAUSAL_LM&quot;,
      9 )
---&gt; 10 model = get_peft_model(model, config)
     11 model.print_trainable_parameters()

File ~/Projects/llmtest1/lib/python3.10/site-packages/peft/mapping.py:133, in get_peft_model(model, peft_config, adapter_name, mixed)
    131 if peft_config.is_prompt_learning:
    132     peft_config = _prepare_prompt_learning_config(peft_config, model_config)
--&gt; 133 return MODEL_TYPE_TO_PEFT_MODEL_MAPPING[peft_config.task_type](model, peft_config, adapter_name=adapter_name)

File ~/Projects/llmtest1/lib/python3.10/site-packages/peft/peft_model.py:994, in PeftModelForCausalLM.__init__(self, model, peft_config, adapter_name)
    993 def __init__(self, model, peft_config: PeftConfig, adapter_name=&quot;default&quot;):
--&gt; 994     super().__init__(model, peft_config, adapter_name)
    995     self.base_model_prepare_inputs_for_generation = self.base_model.prepare_inputs_for_generation

File ~/Projects/llmtest1/lib/python3.10/site-packages/peft/peft_model.py:123, in PeftModel.__init__(self, model, peft_config, adapter_name)
    121     self._peft_config = None
    122     cls = PEFT_TYPE_TO_MODEL_MAPPING[peft_config.peft_type]
--&gt; 123     self.base_model = cls(model, {adapter_name: peft_config}, adapter_name)
    124     self.set_additional_trainable_modules(peft_config, adapter_name)
    126 self.config = getattr(self.base_model, &quot;config&quot;, {&quot;model_type&quot;: &quot;custom&quot;})

File ~/Projects/llmtest1/lib/python3.10/site-packages/peft/tuners/lora/model.py:115, in LoraModel.__init__(self, model, config, adapter_name)
    114 def __init__(self, model, config, adapter_name) -&gt; None:
--&gt; 115     super().__init__(model, config, adapter_name)

File ~/Projects/llmtest1/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:95, in BaseTuner.__init__(self, model, peft_config, adapter_name)
     92 if not hasattr(self, &quot;config&quot;):
     93     self.config = {&quot;model_type&quot;: &quot;custom&quot;}
---&gt; 95 self.inject_adapter(self.model, adapter_name)
     97 # Copy the peft_config in the injected model.
     98 self.model.peft_config = self.peft_config

File ~/Projects/llmtest1/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:252, in BaseTuner.inject_adapter(self, model, adapter_name)
    245     parent, target, target_name = _get_submodules(model, key)
    247     optional_kwargs = {
    248         &quot;loaded_in_8bit&quot;: getattr(model, &quot;is_loaded_in_8bit&quot;, False),
    249         &quot;loaded_in_4bit&quot;: getattr(model, &quot;is_loaded_in_4bit&quot;, False),
    250         &quot;current_key&quot;: key,
    251     }
--&gt; 252     self._create_and_replace(peft_config, adapter_name, target, target_name, parent, **optional_kwargs)
    254 if not is_target_modules_in_base_model:
    255     raise ValueError(
    256         f&quot;Target modules {peft_config.target_modules} not found in the base model. &quot;
    257         f&quot;Please check the target modules and try again.&quot;
    258     )

File ~/Projects/llmtest1/lib/python3.10/site-packages/peft/tuners/lora/model.py:196, in LoraModel._create_and_replace(self, lora_config, adapter_name, target, target_name, parent, current_key, **optional_kwargs)
    188     target.update_layer(
    189         adapter_name,
    190         r,
   (...)
    193         lora_config.init_lora_weights,
    194     )
    195 else:
--&gt; 196     new_module = self._create_new_module(lora_config, adapter_name, target, **kwargs)
    197     if adapter_name != self.active_adapter:
    198         # adding an additional adapter: it is not automatically trainable
    199         new_module.requires_grad_(False)

File ~/Projects/llmtest1/lib/python3.10/site-packages/peft/tuners/lora/model.py:271, in LoraModel._create_new_module(lora_config, adapter_name, target, **kwargs)
    267 if loaded_in_8bit and isinstance(target_base_layer, bnb.nn.Linear8bitLt):
    268     eightbit_kwargs = kwargs.copy()
    269     eightbit_kwargs.update(
    270         {
--&gt; 271             &quot;has_fp16_weights&quot;: target.state.has_fp16_weights,
    272             &quot;memory_efficient_backward&quot;: target.state.memory_efficient_backward,
    273             &quot;threshold&quot;: target.state.threshold,
    274             &quot;index&quot;: target.index,
    275         }
    276     )
    277     new_module = Linear8bitLt(target, adapter_name, **eightbit_kwargs)
    278 elif loaded_in_4bit and is_bnb_4bit_available() and isinstance(target_base_layer, bnb.nn.Linear4bit):

File ~/Projects/llmtest1/lib/python3.10/site-packages/torch/nn/modules/module.py:1614, in Module.__getattr__(self, name)
   1612     if name in modules:
   1613         return modules[name]
-&gt; 1614 raise AttributeError(&quot;'{}' object has no attribute '{}'&quot;.format(
   1615     type(self).__name__, name))

AttributeError: 'Linear8bitLt' object has no attribute 'state'
</code></pre>
","huggingface"
"77595516","How to do secrets management in hugging face inference endpoints?","2023-12-03 17:51:04","","3","102","<huggingface><inference>","<p>I'm deploying llama 2 in hugging face inference endpoints. Though to do so I need my token to get the base model weights. I could not find anywhere documentation how to manage this secrets. Any directions?</p>
","huggingface"
"77591580","why Argument of type ""Dataset"" cannot be assigned to parameter ""train_dataset"" of type ""Dataset[Unknown]?","2023-12-02 17:59:00","","1","54","<nlp><dataset><huggingface><huggingface-datasets>","<blockquote>
<p>I am trying to finetune a large language model. and when i give train and test dataset to the model it shows the above error and there is red line under train_dataset and test_dataset.<strong>Following is the lines where i have problem</strong></p>
</blockquote>
<pre><code>trainer = Trainer(
model=base_model,
args=training_args,
train_dataset=tokenized_train_data,
eval_dataset=tokenized_test_data
  )
</code></pre>
<blockquote>
<p><strong>This is how i am mapping and preprocess the data</strong>
<em>My dataset is in dataframe so</em></p>
</blockquote>
<pre><code>train_dataset, test_dataset = train_test_split(concatinate_dataset_refined, test_size=0.2, random_state=132)
</code></pre>
<blockquote>
<p>Converting into Datasets.</p>
</blockquote>
<pre><code>train_dataset = Dataset.from_pandas(train_dataset)
test_dataset = Dataset.from_pandas(test_dataset)
</code></pre>
<blockquote>
<p>Tokenising</p>
</blockquote>
<pre><code>def tokenize_function(example):
merged = example[&quot;title&quot;] + &quot; &quot; + example[&quot;story&quot;]
batch = tokenizer(merged, padding='max_length', truncation=True, max_length=128)
batch[&quot;labels&quot;] = batch[&quot;input_ids&quot;].copy()
return batch  

# Apply it to our dataset, and remove the text columns
</code></pre>
<p>tokenized_train_data = train_dataset.map(tokenize_function, remove_columns=[&quot;title&quot;, &quot;story&quot;])</p>
<p>tokenized_test_data = test_dataset.map(tokenize_function, remove_columns=[&quot;title&quot;, &quot;story&quot;])</p>
<p><strong>Even i run the code it model runs okay but does not generate any texts, just the commas.</strong></p>
<blockquote>
<p>Thank you for your time guys.</p>
</blockquote>
","huggingface"
"77587563","GGUF model running slow compared to GGMLv3 based on same base model","2023-12-01 19:14:19","","0","328","<nlp><huggingface><llama-cpp-python>","<p>I am comparing the performance of two instances of the wizardlm-13b model which I downloaded from HuggingFace.  I found that the GGUF version of the model runs 4x slower than the GGMLv3 version.
Best I can tell these are both 4-bit quantized models derived from the same base model.</p>
<p>I created an inference for both models using the llama-cpp-python package.</p>
<p>I used the following code to benchmark the performance:</p>
<pre><code>from llama_cpp import Llama
llm = Llama(model_path=&quot;./models/7B/llama-model.gguf&quot;)
output = llm(&quot;Q: Name all of the planets in the solar system? A: &quot;, max_tokens=64, stop=[&quot;Q:&quot;, &quot;\n&quot;], echo=True)
print(output)
</code></pre>
<p>And here are the results, running on my CPU:</p>
<p><a href=""https://i.sstatic.net/TTc86.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/TTc86.png"" alt=""enter image description here"" /></a></p>
<p>I saw similar performance differences when running on GPU as well.</p>
<p>I am trying to understand what could be the root cause of such performance differences.  Is GGUF expected to run slower than GGMLv3?  Is it possible the newer version of llama-cpp-python could cause the difference?  Or maybe something else I am overlooking?</p>
","huggingface"
"77584110","Different Results when using batch_size in HuggingFace Pipeline","2023-12-01 09:15:02","","0","168","<python><nlp><pipeline><summary><huggingface>","<p>I get very different results (while keeping everything else simililar (ceteris paribus)) when passing prompts to my model sequentually or when batching the prompts.</p>
<p>I am currently using this pieline for summarization:</p>
<pre><code>summarize = transformers.pipeline(
    &quot;text-generation&quot;,   
    model=model,         
    tokenizer=tokenizer,    
    torch_dtype=torch.bfloat16, 
    max_new_tokens=512,  
    do_sample=True,         
    top_k=50,      
    top_p=0.95, 
    temperature=0.01
)
</code></pre>
<p>now i pass the prompts to the pipeline sequentually like this:</p>
<pre><code>summaries = []
for prompt in prompts:
    summary = summarize(prompt)
    summaries.append(summary)
</code></pre>
<p>i get way better results, in comparision to using the batching parameter:</p>
<pre><code>summaries = summarize(prompts, batch_size = 4)
</code></pre>
<p>Does anybody have an explaination for this. It is indeed the case that the sequential pieline generates the better output in 20 out of 20 cases.</p>
<p>Thanks in advance!</p>
","huggingface"
"77581888","FAISS Embeddings cannot be saved because of langchain import error","2023-11-30 21:44:35","77602729","0","1546","<pdf><langchain><huggingface><large-language-model><faiss>","<p>I am following this <a href=""https://www.youtube.com/watch?v=RIWbalZ7sTo&amp;ab_channel=PromptEngineering"" rel=""nofollow noreferrer"">tutorial</a></p>
<p>I am using a sample pdf from <a href=""https://www.africau.edu/images/default/sample.pdf"" rel=""nofollow noreferrer"">here</a></p>
<p>But I replaced OpenAI with Huggingface for the embeddings</p>
<p>Below is my code:</p>
<pre><code>import os
import pickle
from pprint import pprint
from PyPDF2 import PdfReader
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter

pdf = 'sample.pdf'
pdf_reader = PdfReader(pdf)
text = ''
for page in pdf_reader.pages:
    text += page.extract_text()
pprint(text)

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size = 100,
    chunk_overlap = 20,
    length_function = len
)
chunks = text_splitter.split_text(text = text)

embeddings = HuggingFaceEmbeddings()
VectorStore = FAISS.from_texts(chunks, embedding = embeddings)
with open('sample.pkl', 'wb') as f:
    pickle.dump(VectorStore, f)
</code></pre>
<p>When i check the sample.pkl file, all i see is this line: <code>No module named 'langchain'</code></p>
<p>I also checked the embeddings without saving it to the file, and i can see them</p>
<p>Im using jupyter notebook, with python 3.9.16, and I have all the libraries installed. And YES I do have langchain installed. I wouldnt be able to import FAISS or HuggingFaceEmbeddings or RecursiveCharacterTextSplitter without it</p>
","huggingface"
"77576705","Invoking SageMaker endpoint failed since timm library was not found in the environment","2023-11-30 07:49:39","","0","102","<python><amazon-sagemaker><huggingface>","<p>I deployed a Hugging Face <code>facebook/detr-resnet-50</code> model using the official code provided here:
<a href=""https://huggingface.co/facebook/detr-resnet-50?sagemaker_deploy=true"" rel=""nofollow noreferrer"">https://huggingface.co/facebook/detr-resnet-50?sagemaker_deploy=true</a></p>
<p>Then, I tried to use the <code>predictor</code> variable to invoke this endpoint:</p>
<pre class=""lang-py prettyprint-override""><code>from sagemaker.serializers import DataSerializer
    
predictor.serializer = DataSerializer(content_type='image/x-image')

# Make sure the input file &quot;cats.jpg&quot; exists
with open(&quot;image.jpg&quot;, &quot;rb&quot;) as f:
    data = f.read()
predictor.predict(data)
</code></pre>
<p>It failed with the following error:</p>
<pre><code>ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from primary with message &quot;{
  &quot;code&quot;: 400,
  &quot;type&quot;: &quot;InternalServerException&quot;,
  &quot;message&quot;: &quot;\nDetrForObjectDetection requires the timm library but it was not found in your environment. You can install it with pip:\n`pip install timm`. Please note that you may need to restart your runtime after installation.\n&quot;
}
</code></pre>
<p>I tried to install <code>timm</code> on the environment where I ran the invoke call (I'm also tried to restart this environment) but it seems like this package is missing from the SageMaker endpoint.
How can I add it as part of the model deployment?</p>
","huggingface"
"77573642","Does the huggingface pipeline function upload my data to their cloud?","2023-11-29 18:22:51","","1","626","<huggingface-transformers><privacy><huggingface><large-language-model><privacy-policy>","<p>I am using the <a href=""https://huggingface.co/docs/transformers/main_classes/pipelines"" rel=""nofollow noreferrer"">huggingface pipeline function</a> on my local machine.  I got a crash when I was connected to VPN, but it works when I turn off VPN.  That leads me to wonder what information is being transmitted to huggingface? I know openAI may use any queries I send to them.  Does huggingface upload my queries, or is it the model that is being downloaded to my machine when I run the function locally?</p>
<p>Do the terms of use for the Facebook/Huggingface models include any use of one's data by Facebook/Huggingface?  I was particularly looking for clauses on data use/ownership, but I would like your opinion.</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import pipeline

classifier = pipeline(&quot;zero-shot-classification&quot;)
classifier(
    [&quot;This is a course about the Transformers library&quot;,
    &quot;This is a movie about the Transformers action figures&quot;],
    candidate_labels=[&quot;education&quot;, &quot;politics&quot;, &quot;business&quot;],
)
</code></pre>
","huggingface"
"77570975","How can I get usage from huggingface inference endpoints","2023-11-29 12:01:32","","0","130","<huggingface>","<p>I have created a Hugging Face inference endpoint and I am trying to understand the pricing model. Currently, it is based on an hourly rate. Is there a way to determine the duration of the endpoint usage per request, as opposed to hourly usage?</p>
","huggingface"
"77568745","Decision Metrics for Selecting Promopt Structures: Creating a Prompt-Based System","2023-11-29 05:21:33","","0","22","<prompt><huggingface><large-language-model>","<p>I am Newbie in this field
I'm currently working on developing a system that generates various prompts structures (such as tree of thoughts, chain of thoughts, etc.) in response to user prompts. However, I'm facing a challenge in determining the most suitable thought structure for a given prompt. in machine learning, metrics like accuracy, F1 score, precision, and recall aid in choosing the best model. Similarly, I'm exploring if there exist analogous metrics or approaches that can provide numerical guidance in selecting between different thought structures.</p>
<p>If anyone has insights into potential metrics, methodologies, or even analogous concepts from other fields that might help in objectively determining which thought structure (tree, chain, etc.) is best suited for a particular prompt, I would greatly appreciate your input.</p>
<p>Ex:-</p>
<p>User Prompt:- I Want To Learn About Elephent. In Output One Prompt Based On Chain Of Thoughts Other Is Based On Tree Of Thoughts Or Other Prompt Enginerrring Techniques</p>
<p>We Will Have Two Output How To Consider Which One Is Best?</p>
<p>Thank you for your time and expertise!&quot;</p>
","huggingface"
"77560759","what is the format of stabilityai/stable-diffusion-xl-base-1.0 model on hugging face?","2023-11-28 00:48:29","","0","192","<deep-learning><huggingface-transformers><huggingface><stable-diffusion>","<p>I am confused by the format of the <a href=""https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0"" rel=""nofollow noreferrer"">stabilityai/stable-diffusion-xl-base-1.0</a> model. Is it a pytorch model or is it an onnx model?</p>
<p>I tested <code>ORTStableDiffusionXLPipeline.from_pretrained(model_id, export=True, provider=&quot;ROCMExecutionProvider&quot;,)</code> vs <code>ORTStableDiffusionXLPipeline.from_pretrained(model_id, export=False, provider=&quot;ROCMExecutionProvider&quot;,)</code>, it seems there is no difference for setting <code>export</code> to True vs False. Based on my understanding, if the model is already in onnx format, then this export argument will play no role here; but if it is in pytorch format, it should give me error message if I set export to False. Can anyone share any thoughts on this?</p>
<p>Thank you!</p>
","huggingface"
"77559352","Summarizing a small numerical dataframe using FLAN T5","2023-11-27 19:06:55","","0","64","<dataframe><openai-api><transformer-model><huggingface><large-language-model>","<p>I am trying to fine tune the Flan T5 model to summarize a pandas dataframe, which mostly contain numerical values and a date column. I expect it to understand the small dataset along with the dates. How should I feed the data to the model so that it best understands the data?</p>
<p>I am using the following prompt which I cannot change: What do you understand by this data? Respond within 2 Lines.</p>
<p>I am building a pipeline to do prompt tuning later using some labelled data, but the above is the first step.</p>
<p>I am a beginner to LLMs and have tried loading data in json format but it is not showing satisfactory results</p>
","huggingface"
"77558648","Is there any tool / endpoint to collect metrics from TGI image and filter these metrics?","2023-11-27 16:56:43","","1","42","<huggingface><large-language-model>","<p>I am currently running a TGI container and collecting  metrics on its performance using /metrics endpoint. However, I only want to collect metrics for specific times and days. Is it possible to add a filtering feature to the TGI container that allows me to specify a time range and/or specific days of the week for which metrics should be collected?
For example, I would like to collect metrics between 9am-5pm, Monday through Friday. Will the metrics be reset each day, or will they continue to accumulate from the start time/day of the container? Additionally, are there any existing tools or methods that can be used to achieve this functionality?</p>
<p>i have tried using prometheus and and tried to introduce time when collecting metrics.</p>
","huggingface"
"77555761","Logging Huggingface Trainer in WandB","2023-11-27 09:22:54","","0","237","<huggingface-transformers><huggingface><wandb>","<p>I am using the following training arguments and trainer for fine-tuning a huggingface model:</p>
<pre><code>trainer_args = TrainingArguments(
    output_dir=model_ckpt.split('/')[0], 
    num_train_epochs=5, 
    per_device_train_batch_size=1, 
    per_device_eval_batch_size=1,
    weight_decay=0.01, 
    logging_steps=5,
    evaluation_strategy='steps', 
    eval_steps=100, 
    eval_accumulation_steps=1,
    save_steps=800,
    report_to=&quot;wandb&quot;,  # enable logging to W&amp;B
    run_name=f&quot;{your_name}_{model_ckpt.split('/')[0]}_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}&quot;,
    overwrite_output_dir=True,
    load_best_model_at_end=True,
    metric_for_best_model='eval_loss',
)
</code></pre>
<pre><code>trainer = Trainer(model=model, args=trainer_args,
                  tokenizer=tokenizer, 
                  data_collator=seq2seq_data_collator,
                  train_dataset=dataset_pt[&quot;train&quot;], 
                  eval_dataset=dataset_pt[&quot;validation&quot;])
</code></pre>
<p>I have two questions wrt the logs in WandB:</p>
<ol>
<li><a href=""https://i.sstatic.net/W2cZ5.png"" rel=""nofollow noreferrer"">What does this plot mean? What is train/epoch?</a></li>
<li><a href=""https://i.sstatic.net/iYi4S.png"" rel=""nofollow noreferrer"">Why can't I see any logs like epoch/batch/train-val loss?</a></li>
</ol>
<p>Basically I want to check which epoch my trainer is currently running.</p>
<p>I tried checking log parameters in <a href=""https://huggingface.co/docs/transformers/v4.35.2/en/main_classes/trainer#transformers.TrainingArguments"" rel=""nofollow noreferrer"">Training Arguments</a>, but couldn't understand what to change.</p>
<p>Edit 1: The y-axis is 'steps' for the given graph</p>
","huggingface"
"77555367","Hugging Face BERT custom model cannot produce config.json when trainer(model) is saved","2023-11-27 08:12:19","","0","428","<pytorch><bert-language-model><huggingface><config.json>","<p>`Hello experts,</p>
<p>I am trying to write BERT model with my own custom model (adding layer end of BERT).It goes well and I would like to save model to avoid future training.  I am using trainer API and so, there are two ways to save model, right?</p>
<ol>
<li>training_arugments and trainer (save the model under a folder)</li>
</ol>
<p>This type of saving cannot save config.json although other files can be saved. If I am saving model with only BERT model without adding any layers, it can save all files including json.config.</p>
<ol start=""2"">
<li>trainer.save_model</li>
</ol>
<p>This also cannot save json.config.
So, the question is how can we get all completed saved information for custom model ?? becuase without config.json file we cannot load and resuse the model again. Your suggestions will be highly appreicated how to save the custom model.
`</p>
","huggingface"
"77549942","Stopping criteria for Llama-2 does not work","2023-11-25 23:37:56","","0","1508","<huggingface-transformers><huggingface><huggingface-tokenizers><llama>","<p>I'm using LLama-2 13B with the following stopping criteria:</p>
<pre><code>stop_words = [&quot;Human:&quot;, &quot;Chatbot:&quot;, &quot;###&quot;]
stop_words_ids = [tokenizer(stop_word, return_tensors='pt')['input_ids'].squeeze() for stop_word in stop_words]
stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops=stop_words_ids)])
generation_config = GenerationConfig( ... stopping_criteria=stopping_criteria )
prompt = tokenizer(text, return_tensors='pt', truncation=&quot;only_first&quot;, max_length=4096)
prompt = {key: value.to(&quot;cuda&quot;) for key, value in prompt.items()}
out = model.generate(**prompt, generation_config=generation_config)
res = tokenizer.decode(out[0])
</code></pre>
<p>The model does not stop at the provided stop words. For example, if I have a response of the model <code>I'm feeling good, how about you?### Human: I'm also feeling good.### Chatbot: That's good.</code> the model should stop generating at the first <code>###</code>.</p>
<p>Why does this not work and how can this be fixed?</p>
<p>I have fine-tuned the model (with Axolotl) on a dataset so that the model produces responses as shown above.</p>
","huggingface"
"77549354","Huggingface QA model results decoding","2023-11-25 19:40:18","","0","209","<python><nlp><huggingface-transformers><huggingface>","<p>I'm trying work with a transformer model (for the question answering task). I am submitting a short string as an input example.</p>
<p>It's my simplest code:</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForQuestionAnswering
import torch

#Model downloaded from https://huggingface.co/KseniyaZ/sbert_large_nlu_ru
    
tokenizer = AutoTokenizer.from_pretrained(&quot;KseniyaZ/sbert_large_nlu_ru&quot;)
model = AutoModelForQuestionAnswering.from_pretrained(&quot;KseniyaZ/sbert_large_nlu_ru&quot;)
        
s = 'Hi. How are you?' #simple string with question
tok = tokenizer(s, return_tensors='pt')
res = model(**tok)
print(res)
</code></pre>
<p>The output is the following:</p>
<pre><code>QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[ 2.2755,  0.3885, -1.9400]], grad_fn=&lt;CloneBackward0&gt;), end_logits=tensor([[2.1757, 0.0924, 2.2580]], grad_fn=&lt;CloneBackward0&gt;), hidden_states=None, attentions=None)
</code></pre>
<p>I could not find instructions for decoding these logits.</p>
<p>How can I decode the network output so that I get a string? Or is this method wrong and another way is needed here?</p>
","huggingface"
"77545627","Using HuggingFace Inference Endpoint for translation","2023-11-24 20:54:56","","0","124","<python><huggingface><language-translation>","<h2>Setup</h2>
<p>I've created a HF Inference Endpoints for translation, in particular french-&gt;english. This is the setup</p>
<ul>
<li><strong>Instance type:</strong> GPU · Nvidia A10G · 1x GPU · 24 GB</li>
<li><strong>Model:</strong> Helsinki-NLP/opus-mt-fr-en</li>
<li><strong>Container:</strong> default</li>
</ul>
<p>I have 2k documents to translate and I'm using python package <code>request</code>, together with <code>concurrent</code> to fire multiple HTTP POST request to my endpoint. Each document should have 100-300 sentences and I wasn't able to translate them as they were, so I split each document in 6 section.</p>
<p>Checking the usage of the machine, I realise I barely use any resource in terms of CPU/GPU. Also, after a while, some requests start failing.</p>
<p>I'm quite sure this could be much faster and could process more text all together, but I can't understand how to do so. My data is in a pyspark dataframe: I first tried with an UDF (and it was a terrible idea), now I'm creating a list of documents and use that as input.</p>
<p>Here's the code I'm using</p>
<pre class=""lang-py prettyprint-override""><code>def translate_text(text):
    payload = {&quot;inputs&quot;: text}
    headers = {&quot;Authorization&quot;: f&quot;Bearer {API_TOKEN}&quot;, &quot;Content-Type&quot;: &quot;application/json&quot;}

    try:
        response = requests.post(API_URL, json=payload, headers=headers)
        response.raise_for_status()

        if response.status_code == 200 and response.text:
            response_data = response.json()
            translated_text = list(map(lambda x: x.get(&quot;translation_text&quot;), response_data))
            return translated_text
        else:
            print(&quot;Translation response is empty or not in JSON format.&quot;)
            return None

    except requests.exceptions.RequestException as e:
        print(&quot;Request error:&quot;, e)
        return None

    except requests.exceptions.JSONDecodeError as e:
        print(&quot;Failed to decode JSON response:&quot;, e)
        return None

def translate_text_dict(text_dict: dict) -&gt; dict:
    output_dict = {}

    with concurrent.futures.ThreadPoolExecutor() as executor:
        results = executor.map(
            lambda args: (args[0], list(map(translate_text, args[1]))), text_dict.items()
        )

        output_dict = dict(results)

    return output_dict
</code></pre>
<p>the input dictionary has, as key, an unique id and, as value, the list of sentences to translate. Could this be improved?</p>
","huggingface"
"77544203","How to Integrate Custom OpenAIModel into a AutoModelForSequenceClassification Model?","2023-11-24 15:26:38","77547197","0","71","<python><pytorch><nlp><huggingface-transformers><huggingface>","<p>I've developed a custom OpenAIModel module that acts like BERT models but makes an OpenAI embeddings request and returns the results when called. I want to use this module, utilizing Hugging Face's transformers library inside an AutoModelForSequenceClassification model. However, when I try to replace the base_model with my OpenAIModel using the following code, the base_model property doesn't change as expected:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoModelForSequenceClassification
import torch

num_labels = ... # Define the appropriate number of labels
train_arch = ... # Define parameters specific to your training architecture

model = AutoModelForSequenceClassification.from_pretrained('dbmdz/distilbert-base-turkish-cased', num_labels=num_labels)
model.base_model = OpenAIModel(train_arch)
model.classifier = torch.nn.Linear(model.base_model.config.dim, num_labels)
model.config = OpenAIModelConfig()
model.config_class = type(OpenAIModelConfig)
tokenizer = OpenAITokenizer()
</code></pre>
<p>After executing <code>model.base_model = OpenAIModel(train_arch)</code>, when I inspect model.base_model, it still appears to be distilbert and not the expected OpenAIModel.</p>
<p>I can successfully change all other values of the SequenceClassification model, but the base_model remains unaltered. I'm unsure why this is happening and how to resolve it.</p>
<p>What is the correct way to integrate my model into AutoModelForSequenceClassification? Or should I be using a different approach to change the base_model? (I tried to load my model with from_pretrained but AutoModel class doesn't support custom model structure as i see)</p>
<p>Any help or suggestions would be greatly appreciated!</p>
","huggingface"
"77542331","Inconsistent Batch Size Issue for Multi-Label Classification: Input [32, 3, 224, 224], Output [98, 129]","2023-11-24 10:15:51","77552110","0","82","<python><deep-learning><pytorch><huggingface><image-classification>","<p>I'm using ResNet18 from Hugging Face to fine-tune a multi-label dataset. I want to be able to predict for an image it's 3 corresponding labels and for that I created 3 fully connected layers. First, I tried updating the classifier layer of ResNet18:</p>
<pre><code>model2.classifier_artist = torch.nn.Sequential(
    torch.nn.Dropout(p=0.2, inplace=True), 
    torch.nn.Linear(in_features=512, out_features=num_classes_artist, bias=True)
).to(device)

model2.classifier_style = torch.nn.Sequential(
    torch.nn.Dropout(p=0.2, inplace=True), 
    torch.nn.Linear(in_features=512, out_features=num_classes_style, bias=True) 
).to(device)

model2.classifier_genre = torch.nn.Sequential(
    torch.nn.Dropout(p=0.2, inplace=True), 
    torch.nn.Linear(in_features=512, out_features=num_classes_genre, bias=True) 
).to(device)

num_classes_artist = 129
num_classes_style = 27
num_classes_genre = 11
</code></pre>
<p>But that didn't work. The model architecture didn't include the 3 classifiers that I added. Here is the summary from torchinfo:</p>
<pre><code>ResNetForImageClassification (ResNetForImageClassification)                 [32, 3, 224, 224]    [32, 1000]
├─ResNetModel (resnet)                                                      [32, 3, 224, 224]    [32, 512, 1, 1]
│    └─ResNetEmbeddings (embedder)                                          [32, 3, 224, 224]    [32, 64, 56, 56]
│    │    └─ResNetConvLayer (embedder)                                      [32, 3, 224, 224]    [32, 64, 112, 112]
│    │    └─MaxPool2d (pooler)                                              [32, 64, 112, 112]   [32, 64, 56, 56]
│    └─ResNetEncoder (encoder)                                              [32, 64, 56, 56]     [32, 512, 7, 7]
│    │    └─ModuleList (stages)                                             --                   --
│    └─AdaptiveAvgPool2d (pooler)                                           [32, 512, 7, 7]      [32, 512, 1, 1]
├─Sequential (classifier)                                                   [32, 512, 1, 1]      [32, 1000]
│    └─Flatten (0)                                                          [32, 512, 1, 1]      [32, 512]
│    └─Linear (1)                                                           [32, 512]            [32, 1000]
</code></pre>
<p>After that I proceeded to implement it in PyTorch:</p>
<pre><code>class WikiartModel(nn.Module):
    def __init__(self, num_artists, num_genres, num_styles):
        super(WikiartModel, self).__init__()
        
        # Shared Convolutional Layers
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding =1)
        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        
        # Artist classification branch
        self.fc_artist1 = nn.Linear(256 * 16 * 16, 512)
        self.fc_artist2 = nn.Linear(512, num_artists)
        
        
        # Genre classification branch
        self.fc_genre1 = nn.Linear(256 * 16 *  16, 512)
        self.fc_genre2 = nn.Linear(512, num_genres)


        # Style classification branch
        self.fc_style1 = nn.Linear(256 * 16 * 16, 512) 
        self.fc_style2 = nn.Linear(512, num_styles)
        
    def forward(self, x):
        # Shared convolutional layers
        x = self.pool(F.relu(self.conv1(x)))   
        x = self.pool(F.relu(self.conv2(x)))       
        x = self.pool(F.relu(self.conv3(x)))
        x = x.view(-1, 256 * 16  * 16) 

        # Artist classification branch
        artists_out = F.relu(self.fc_artist1(x))
        artists_out = self.fc_artist2(artists_out)
       
        
        # Genre classification branch
        genre_out = F.relu(self.fc_genre1(x))
        genre_out = self.fc_genre2(genre_out) 
        
        
        # Style classification branch 
        style_out = F.relu(self.fc_style1(x))
        style_out = self.fc_style2(style_out)
        
        return artists_out, genre_out, style_out
    
# Set the number of classes for each task
num_artists = 129  # Including &quot;Unknown Artist&quot;
num_genres = 11    # Including &quot;Unknown Genre&quot;
num_styles = 27
</code></pre>
<p>And here is the torchinfo summary:</p>
<pre><code>Layer (type (var_name))                  Input Shape          Output Shape
================================================================================
WikiartModel (WikiartModel)              [32, 3, 224, 224]    [98, 129]
├─Conv2d (conv1)                         [32, 3, 224, 224]    [32, 64, 224, 224]
├─MaxPool2d (pool)                       [32, 64, 224, 224]   [32, 64, 112, 112]
├─Conv2d (conv2)                         [32, 64, 112, 112]   [32, 128, 112, 112]
├─MaxPool2d (pool)                       [32, 128, 112, 112]  [32, 128, 56, 56]
├─Conv2d (conv3)                         [32, 128, 56, 56]    [32, 256, 56, 56]
├─MaxPool2d (pool)                       [32, 256, 56, 56]    [32, 256, 28, 28]
├─Linear (fc_artist1)                    [98, 65536]          [98, 512]
├─Linear (fc_artist2)                    [98, 512]            [98, 129]
├─Linear (fc_genre1)                     [98, 65536]          [98, 512]
├─Linear (fc_genre2)                     [98, 512]            [98, 11]
├─Linear (fc_style1)                     [98, 65536]          [98, 512]
├─Linear (fc_style2)                     [98, 512]            [98, 27]
</code></pre>
<p>The batch size of the input data <code>([32, 3, 224, 224])</code> and the batch size of the model's output predictions <code>([98, 129])</code> appear to be different. I've checked my data loading, model architecture, and training loop, but I can't seem to identify the root cause of this problem. This inconsistency is leading to an error when calculating the loss inside the training loop:</p>
<p><code>loss_artist = criterion_artist(outputs_artist, labels_artist)</code> :</p>
<p><code>ValueError: Expected input batch_size (98) to match target batch_size (32).</code></p>
","huggingface"
"77541998","How to download text embedding model with R text library?","2023-11-24 09:20:10","","0","88","<r><text><huggingface>","<p>I want to use the <code>r library(text)</code> for some basic text anaylsis.</p>
<p>A base function of this library is <code>textembed(&quot;some text&quot;)</code> which by defaults downloads a BERT model from huggingface.co, additionally specific models can be specified with a model parameter.</p>
<p>However when I try to do this I get the following error:</p>
<pre><code>Error: OSError: Can't load config for 'xlm-roberta-large'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'xlm-roberta-large' is the correct path to a directory containing a config.json file
</code></pre>
<p>My R project using GIT is launched from a local folder. Within this folder no config.json or other directory exists. On my user folder structure a .cache folder exists with a hugginface folder but no contents.</p>
<p>This was tried on Windows 10 with R version 4.2.3</p>
<p>Update:
From the console I can see that R downloads a lot of files to my local appdata:</p>
<pre><code>[nltk_data] Downloading package punkt to
[nltk_data]     C:\Users\XXX\AppData\Roaming\nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[nltk_data] Downloading package punkt to
[nltk_data]     C:\Users\XXX\AppData\Roaming\nltk_data...
[nltk_data]   Package punkt is already up-to-date!
</code></pre>
<p>However this seems to be the incorrect target destination as it will not be found by R.</p>
","huggingface"
"77540677","Clearing context window of LLM in Huggingface","2023-11-24 04:11:30","77541156","-1","614","<nlp><huggingface-transformers><transformer-model><huggingface><large-language-model>","<p>I want to use inference to ask different questions to LLMs taken from huggingface. But, I want to ask the prompts without the model having info about the previous prompts. Does the model automatically store the previous prompts in context?</p>
<p>Or does it not save any previous information at all and we need to provide all the context in the same prompt?</p>
","huggingface"
"77536180","chainlit AsyncLangchainCallbackHandler 'Message' object has no attribute 'replace'","2023-11-23 10:34:48","","0","1394","<python><chatbot><langchain><huggingface><large-language-model>","<p>Received this error message 'Message' object has no attribute 'replace'.</p>
<p>This is part of Q&amp;A chatbot based on local documents. The chatbot started successfully but after first interaction like &quot;Hello&quot;, I received the error message.</p>
<p>Below is the code. I believe the error is on the AsyncLangchainCallbackHandler</p>
<pre><code>@cl.on_message
async def main(message):
 chain=cl.user_session.get(&quot;chain&quot;)
 cb = cl.AsyncLangchainCallbackHandler(
  stream_final_answer=True, answer_prefix_tokens=[&quot;FINAL&quot;,&quot;ANSWER&quot;]
  )
#  cb = cl.AsyncLangchainCallbackHandler()
 cb.ansert_reached=True
 res=await chain.acall(message, callbacks=[cb])
 print(&quot;LangChain response:&quot;, res)
 answer=res[&quot;result&quot;]
 sources=res[&quot;source_documents&quot;]

 if sources:
  answer+=f&quot;\nSources: &quot;+str(str(sources))
 else:
  answer+=f&quot;\nNo Sources found&quot;

 await cl.Message(content=answer).send()
</code></pre>
","huggingface"
"77531993","run LLM on single gpu with 8GB ram","2023-11-22 17:20:08","","0","771","<langchain><huggingface><large-language-model>","<p>I have a single nvidia gpu with 8 GB of ram. I’m running it on an ubuntu server 18.04 LTS. I’m able to pass queries and get response from flan-T5, but when I tried performing peft with lora I got a “gpu out of memory” error. Similarly I tried running camel-5b and llama2-7b-chat as chat agents, and both threw a “gpu out of memory error.” I’m trying to experiment with LLM, learn the structure of the code, prompt engineering. Ultimately I’d like to develop a chat agent with llama2-70b-chat even if I have to run it on colab. can anyone suggest a similar structure LLM to llama2-7b-chat that might be able to run on my single gpu with 8 gb ram?  I've tried running the ctransformers but it seems to have a dependency on cuda 12 and my nvidia drivers for my gpu seem to have a limit at cuda 11.4.</p>
","huggingface"
"77531431","Converting huggingface model to safetensors failing with larger model","2023-11-22 15:58:02","","0","846","<huggingface>","<p>I use this code to merge a fine-tuned model on a machine with a A100-40GB:</p>
<pre><code>from peft import AutoPeftModelForCausalLM
from transformers import AutoTokenizer
import torch

checkpoint_dir = &quot;/data/Llama-2-13b-chat-hf-v1/checkpoint-780&quot;
merge_dir = &quot;merged_llama-2-13b-chat-hf-v1&quot;

# Load adapter model
model = AutoPeftModelForCausalLM.from_pretrained(checkpoint_dir, use_cache=False, torch_dtype=torch.float16, device_map='auto', offload_folder=&quot;offload&quot;, offload_state_dict = True)

tokenizer = AutoTokenizer.from_pretrained(checkpoint_dir)
merged_model = model.merge_and_unload()

# Save the merged model
merged_model.save_pretrained(merge_dir,safe_serialization=True)
tokenizer.save_pretrained(merge_dir)
print(&quot;Pretraining Done. Merged model saved in:&quot;, merge_dir)
</code></pre>
<p>But when I use the 13b instead of the 7b llama-2 model I am getting the following error:</p>
<blockquote>
<p>NotImplementedError: Cannot copy out of meta tensor; no data!</p>
</blockquote>
<p>I can set safe_serialization=False for it to work and merge, but then its not in safetensor format. I think its a offloading issue but is there any way to get around this?</p>
","huggingface"
"77526949","Error generating dataset from huggingface","2023-11-22 02:01:51","","0","244","<python><dataset><huggingface><huggingface-datasets>","<p>I am trying to load the following dataset from huggingface: bitext/Bitext-customer-support-llm-chatbot-training-dataset. However, each time I try to run this code, I keep coming across the same error over and over again. It is not completing the loading process.
This is the code for loading the dataset and extracting the information, which is to be stored in a json file:</p>
<pre><code>import json
from datasets import load_dataset

# load the dataset
dataset = load_dataset(&quot;bitext/Bitext-customer-support-llm-chatbot-training-dataset&quot;, download_mode=&quot;force_redownload&quot;)
print(dataset)
# dataset = dataset['train'][0]


# Initialize an empty intents list
intents = []


# Loop through the dataset
for data in dataset['train']:
    instruction = data['instruction']
    response = data['response']
    intent = data['intent']


    # Check if intent exists
    intent_exists = False
    for existing_intent in intents:
        if existing_intent['tag'] == intent:
            intent_exists = True
            break
        

    # If intent does not exist, create a new one
    if not intent_exists:
        new_intent = {
            &quot;tag&quot;: intent,
            &quot;patterns&quot;: [],
            &quot;responses&quot;: []
        }
        intents.append(new_intent)
        
        
    # add question to patterns list of corresponding intent
    for existing_intent in intents:
        if existing_intent['tag'] == intent:
            existing_intent['patterns'].append(instruction)
            break
        
        
    # add response to responses list of corresponding intent
    for existing_intent in intents:
        if existing_intent['tag'] == intent:
            existing_intent['responses'].append(response)
            break
        
        
# save intents to json file
with open(&quot;intents/intents-bitext-01.json&quot;, &quot;w&quot;) as f:
    json.dump(intents, f, indent=4)
    
print(&quot;Extraction complete&quot;)
</code></pre>
<p>However, each time I run this code, I get the following error:</p>
<pre><code>Downloading readme: 100%|█████████████████████████████████████████████████████████| 11.3k/11.3k [00:00&lt;00:00, 11.3MB/s]
Downloading data: 100%|████████████████████████████████████████████████████████████| 19.2M/19.2M [01:34&lt;00:00, 203kB/s]
Downloading data files: 100%|████████████████████████████████████████████████████████████| 1/1 [01:34&lt;00:00, 94.54s/it]
Extracting data files: 100%|████████████████████████████████████████████████████████████| 1/1 [00:00&lt;00:00, 111.42it/s]
Generating train split: 0 examples [00:00, ? examples/s]
Traceback (most recent call last):
  File &quot;C:\Programming\CSB\csb_venv\Lib\site-packages\datasets\builder.py&quot;, line 1908, in _prepare_split_single
    writer = writer_class(
             ^^^^^^^^^^^^^
  File &quot;C:\Programming\CSB\csb_venv\Lib\site-packages\datasets\arrow_writer.py&quot;, line 335, in __init__
    self.stream = self._fs.open(fs_token_paths[2][0], &quot;wb&quot;)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Programming\CSB\csb_venv\Lib\site-packages\fsspec\spec.py&quot;, line 1307, in open
    f = self._open(
        ^^^^^^^^^^^
  File &quot;C:\Programming\CSB\csb_venv\Lib\site-packages\fsspec\implementations\local.py&quot;, line 180, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Programming\CSB\csb_venv\Lib\site-packages\fsspec\implementations\local.py&quot;, line 302, in __init__
    self._open()
  File &quot;C:\Programming\CSB\csb_venv\Lib\site-packages\fsspec\implementations\local.py&quot;, line 307, in _open
    self.f = open(self.path, mode=self.mode)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'C:/Users/ASAA/.cache/huggingface/datasets/bitext___bitext-customer-support-llm-chatbot-training-dataset/default-21d4b5f37915169d/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d.incomplete/bitext-customer-support-llm-chatbot-training-dataset-train-00000-00000-of-NNNNN.arrow'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File &quot;C:\Programming\CSB\extract.py&quot;, line 5, in &lt;module&gt;
    dataset = load_dataset(&quot;bitext/Bitext-customer-support-llm-chatbot-training-dataset&quot;, download_mode=&quot;force_redownload&quot;)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Programming\CSB\csb_venv\Lib\site-packages\datasets\load.py&quot;, line 2152, in load_dataset
    builder_instance.download_and_prepare(
  File &quot;C:\Programming\CSB\csb_venv\Lib\site-packages\datasets\builder.py&quot;, line 948, in download_and_prepare
    self._download_and_prepare(
  File &quot;C:\Programming\CSB\csb_venv\Lib\site-packages\datasets\builder.py&quot;, line 1043, in _download_and_prepare
    self._prepare_split(split_generator, **prepare_split_kwargs)
  File &quot;C:\Programming\CSB\csb_venv\Lib\site-packages\datasets\builder.py&quot;, line 1805, in _prepare_split
    for job_id, done, content in self._prepare_split_single(
  File &quot;C:\Programming\CSB\csb_venv\Lib\site-packages\datasets\builder.py&quot;, line 1950, in _prepare_split_single
    raise DatasetGenerationError(&quot;An error occurred while generating the dataset&quot;) from e
datasets.builder.DatasetGenerationError: An error occurred while generating the dataset
</code></pre>
<p>What could be the problem, and how can I fix it?</p>
","huggingface"
"77520355","How can I apply regularization to Stable Diffusion V2.0?","2023-11-21 04:44:15","","0","61","<deep-learning><huggingface><huggingface-datasets><stable-diffusion><image-generation>","<p>I am in the process of fine tuning the text to image model and Dreambooth model of Stable Diffusion v2.0. However, I want to apply regularization in case the model forgets certain concepts (class: dog, etc.) during learning, but I cannot find regulation or class-related parameters in stable diffusion's train python code.</p>
<p>How can I do this? Should I mix it with the train data? What ratio(train: regularization) is good?</p>
<p>And how should we structure the data set? I'm planning to proceed locally.(or even huggingface-hub)</p>
<p>Thank you for your answer.</p>
<p>I followed <a href=""https://github.com/bmaltais/kohya_ss/tree/master"" rel=""nofollow noreferrer"">https://github.com/bmaltais/kohya_ss/tree/master</a> and applied it to the stable diffusion 1.0 base model, which was successful. But it wasn't possible for v2.0.</p>
","huggingface"
"77519685","Mistral AI Suddenly Running Much Slower","2023-11-21 00:29:52","","1","1193","<huggingface-transformers><huggingface><large-language-model>","<p>Hello I am trying to prompt a version of Mistral AI that I have stored locally on my computer. During my first test, I seemed to get about a 100 token response in 10 seconds with 4bit quantization, so seemingly around 600 tokens/min. Today I have created a new script to load it from my local folder, but it seems to be running much much slower, even with 4bit quantization. I'm having to wait 2-3 minutes for barely 10 tokens. Below is the code I use to load in the LLM and quantize it:</p>
<pre><code>quantization_config = BitsAndBytesConfig(load_in_4bit=True, 
                                         llm_int8_enable_fp32_cpu_offload=True,
                                         bnb_4bit_use_double_quant=True,
                                         bnb_4bit_compute_dtype=torch.bfloat16)

loaded_model = AutoModelForCausalLM.from_pretrained('./mistral_model_7B_8bit_Q/', quantization_config=quantization_config)

model_id = &quot;mistralai/Mistral-7B-Instruct-v0.1&quot;
tokenizer = AutoTokenizer.from_pretrained(model_id)
pipeline = pipeline(
        &quot;text-generation&quot;,
        model=loaded_model,
        tokenizer=tokenizer,
        use_cache=True,
        device_map=&quot;auto&quot;,
        max_length=500,
        do_sample=True,
        top_k=5,
        num_return_sequences=1,
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.eos_token_id,
)

llm = HuggingFacePipeline(pipeline=pipeline)
</code></pre>
<p>I believe this correctly loads in the quantized model. With a 7B parameter model at 4 bit quantization, I'm expecting to get a faster turn around. My GPU is an RTX 3080 Ti.</p>
","huggingface"
"77513826","cannot import name 'AutoPipelineForText2Imag' from 'diffusers","2023-11-20 06:14:01","77593665","1","1873","<python><artificial-intelligence><huggingface><pre-trained-model><diffusers>","<p>I am trying to run a hugging face AI model, which gives me an error when I try to import the Diffuser module.
From here, I take this model, <a href=""https://huggingface.co/latent-consistency/lcm-lora-sdxl"" rel=""nofollow noreferrer"">Huggingface Text to image generation model</a>
Error log:</p>
<pre><code> cannot import name 'AutoPipelineForText2Imag' from 'diffusers
</code></pre>
<p>Code use to run this AI model,</p>
<pre><code>import torch
from diffusers import LCMScheduler, AutoPipelineForText2Image

model_id = &quot;stabilityai/stable-diffusion-xl-base-1.0&quot;
adapter_id = &quot;latent-consistency/lcm-lora-sdxl&quot;

pipe = AutoPipelineForText2Image.from_pretrained(model_id, torch_dtype=torch.float16, variant=&quot;fp16&quot;)
pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)
pipe.to(&quot;cuda&quot;)

# load and fuse lcm lora
pipe.load_lora_weights(adapter_id)
pipe.fuse_lora()

prompt = &quot;Self-portrait oil painting, a beautiful cyborg with golden hair, 8k&quot;

# disable guidance_scale by passing 0
image = pipe(prompt=prompt, num_inference_steps=4, guidance_scale=0).images[0]
</code></pre>
","huggingface"
"77499162","How does one reinitialize the weights of a Hugging Face LLaMA v2 model the official way as the original model?","2023-11-17 03:15:56","","1","1256","<machine-learning><pytorch><nlp><huggingface-transformers><huggingface>","<p>I want to reinitialize the weights of a LLaMA v2 model I'm using/downloading. I went through all the documentation and the source code from their Hugging  Face code:</p>
<ul>
<li><a href=""https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L721"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L721</a></li>
<li><a href=""https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_utils.py#L1154"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_utils.py#L1154</a></li>
<li><a href=""https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L809"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L809</a></li>
<li><a href=""https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L721"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L721</a></li>
<li>docs <a href=""https://huggingface.co/docs/transformers/main/model_doc/llama#transformers.LlamaModel"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/main/model_doc/llama#transformers.LlamaModel</a></li>
<li>and papers where neither of the two mentions how the initialized the models <strong>exactly</strong> or any of the layers llamav1 <a href=""https://arxiv.org/pdf/2302.13971.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/2302.13971.pdf</a> and llamav2 <a href=""https://arxiv.org/pdf/2307.09288.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/2307.09288.pdf</a> (maybe due to trade secrets?)</li>
</ul>
<p>I tried the very simple test of going through the modules/parameters and reinitializing according to how their code suggests and printing if the weights norm changed. It never changed, so I don't know if some mutation protection is going on in <a href=""https://en.wikipedia.org/wiki/PyTorch"" rel=""nofollow noreferrer"">PyTorch</a> Hugging  Face models. Is there something I might be doing wrong?</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from transformers import AutoModelForCausalLM, AutoConfig
import torch.nn as nn

def main_reinit_model():
    &quot;&quot;&quot;
    ref: https://stackoverflow.com/questions/76971761/how-to-adapt-llama-v2-model-to-less-than-7b-parameters
    ref: https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L721
    ref: https://chat.openai.com/c/977d0cb0-b819-48ac-be5c-6e482ad5e518
    &quot;&quot;&quot;
    print('Starting to reinitialize the model...')
    # Load the pretrained LLaMA v2 config
    config = AutoConfig.from_pretrained(&quot;meta-llama/Llama-2-7b-hf&quot;)
    # print(f'config: {config} {type(config)}')
    # Print the original number of parameters
    model = AutoModelForCausalLM.from_config(config)
    # put model on device cuda
    model = model.to('cuda')
    # print the model's device
    print(f'{model.device=}')
    # print(f'{model=}')
    # print(&quot;Original number of parameters:&quot;, sum(p.numel() for p in model.parameters()))
    # go through all parameters and compute the l1 norm and sum it then print it
    norm_model = sum(p.norm(1) for p in model.parameters())
    # loop through modules of model and reinitialize weights with normal_mean, 0.02
    print(f'{norm_model=}')
    &quot;&quot;&quot;
    go through model and print all laters
    &quot;&quot;&quot;
    # model.init_weights()  # didn't work
    # model._init_weights(module)  # didn't work needs module
    # for name, param in model.named_parameters():
    #     model._init_weights(param)
    # model.post_init()
    reinitialize_weights(model)
    # model._initialize_weights(module)  # didn't work needs module
    # for name, param in model.named_parameters():
    #     print(f'{name=} {param.shape=}')
    norm_model = sum(p.norm(1) for p in model.parameters())
    print(f'{norm_model=}')

def reinitialize_weights(model) -&gt; None:
    for module in model.modules():
        if isinstance(module, nn.Linear):
            nn.init.normal_(module.weight, mean=0, std=0.02)
            if module.bias is not None:
                nn.init.constant_(module.bias, 0)

def _init_weights(self, module):
    std = self.config.initializer_range
    if isinstance(module, nn.Linear):
        module.weight.data.normal_(mean=100.0, std=std)
        if module.bias is not None:
            module.bias.data.zero_()
    elif isinstance(module, nn.Embedding):
        module.weight.data.normal_(mean=0.0, std=std)
        if module.padding_idx is not None:
            module.weight.data[module.padding_idx].zero_()

def main_generate_smaller_model():
    &quot;&quot;&quot;
    ref: https://stackoverflow.com/questions/76971761/how-to-adapt-llama-v2-model-to-less-than-7b-parameters
    &quot;&quot;&quot;
    print('Starting to reinitialize the model...')
    # Load the pretrained LLaMA v2 config
    config = AutoConfig.from_pretrained(&quot;meta-llama/Llama-2-7b-hf&quot;)
    print(f'config: {config} {type(config)}')
    # Print the original number of parameters
    model = AutoModelForCausalLM.from_config(config)
    print(&quot;Original number of parameters:&quot;, sum(p.numel() for p in model.parameters()))

    # Modify the config to reduce size
    config.hidden_size = 2048
    config.num_hidden_layers = 12

    # Create new smaller model from modified config
    smaller_model = AutoModelForCausalLM.from_config(config)
    print(&quot;New number of parameters:&quot;, sum(p.numel() for p in smaller_model.parameters()))

if __name__ == '__main__':
    import time
    start = time.time()
    # main_generate_smaller_model()
    main_reinit_model()
    print('Done!\a\a\a')
</code></pre>
<p>And the output never showed the weight norms changed:</p>
<pre><code>Starting to reinitialize the model...
model.device=device(type='cuda', index=0)
norm_model=tensor(1.0779e+08, device='cuda:0', grad_fn=&lt;AddBackward0&gt;)
norm_model=tensor(1.0779e+08, device='cuda:0', grad_fn=&lt;AddBackward0&gt;)
Done!
</code></pre>
<p>What am I doing wrong? I need to know how to reinitialize the weights in the <strong>proper/correct way</strong> according to LLaMA. What exact init method and values should I use?</p>
<hr />
<p>Related</p>
<ul>
<li>related blog: <a href=""https://blog.briankitano.com/llama-from-scratch/"" rel=""nofollow noreferrer"">https://blog.briankitano.com/llama-from-scratch/</a></li>
<li>Hugging Face question pre-train: <a href=""https://discuss.huggingface.co/t/can-i-pretrain-llama-from-scratch/37821/7"" rel=""nofollow noreferrer"">https://discuss.huggingface.co/t/can-i-pretrain-llama-from-scratch/37821/7</a></li>
<li>Hugging Face Discord: <a href=""https://discord.com/channels/879548962464493619/1174911090254172231/1174911090254172231"" rel=""nofollow noreferrer"">https://discord.com/channels/879548962464493619/1174911090254172231/1174911090254172231</a></li>
<li>Hugging Face question reinit: <a href=""https://discuss.huggingface.co/t/how-does-one-reinitialize-the-weights-of-a-hugging-face-llama-v2-model-the-official-way-as-the-original-model/62547"" rel=""nofollow noreferrer"">https://discuss.huggingface.co/t/how-does-one-reinitialize-the-weights-of-a-hugging-face-llama-v2-model-the-official-way-as-the-original-model/62547</a></li>
<li>related: <a href=""https://stackoverflow.com/questions/76971761/how-to-adapt-llama-v2-model-to-less-than-7b-parameters"">How to adapt LLaMA v2 model to less than 7B parameters?</a></li>
</ul>
","huggingface"
"77491902","why accelerate need Multiply accelerator.num_processes","2023-11-16 02:04:50","","0","230","<deep-learning><huggingface><learning-rate><accelerate><deepspeed>","<p>in this web <a href=""https://huggingface.co/docs/accelerate/v0.24.0/en/concept_guides/performance#learning-rates"" rel=""nofollow noreferrer"">https://huggingface.co/docs/accelerate/v0.24.0/en/concept_guides/performance#learning-rates</a></p>
<p>we can see this:</p>
<pre><code>Learning Rates
As noted in multiple sources[1][2], the learning rate should be scaled linearly based on the number of devices present. The below snippet shows doing so with Accelerate:

Since users can have their own learning rate schedulers defined, we leave this up to the user to decide if they wish to scale their learning rate or not.

Copied
learning_rate = 1e-3
accelerator = Accelerator()
learning_rate *= accelerator.num_processes

optimizer = AdamW(params=model.parameters(), lr=learning_rate)
You will also find that accelerate will step the learning rate based on the number of processes being trained on. This is because of the observed batch size noted earlier. So in the case of 2 GPUs, the learning rate will be stepped twice as often as a single GPU to account for the batch size being twice as large (if no changes to the batch size on the single GPU instance are made).
</code></pre>
<p>when i read that i think: Effective batch size increases linearly with number of devices. With more devices, the total batch size seen by the model during each optimization step is larger. A larger batch size can require a higher learning rate for efficient training.</p>
<p>but i use accelerate with deepspeed,
i use:</p>
<pre class=""lang-py prettyprint-override""><code>lr = 5e-5
optimizer = DeepSpeedCPUAdam(model.parameters(), lr=lr * accelerator.num_processes)
</code></pre>
<p>i find that</p>
<pre><code>Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000350, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000350, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000350, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000350, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000350, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000350, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000350, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
Adam Optimizer #1 is created with AVX512 arithmetic capability.
Config: alpha=0.000350, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
Adam Optimizer #1 is created with AVX512 arithmetic capability.
Config: alpha=0.000350, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
Adam Optimizer #1 is created with AVX512 arithmetic capability.
Config: alpha=0.000350, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
Adam Optimizer #1 is created with AVX512 arithmetic capability.
Config: alpha=0.000350, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
Adam Optimizer #1 is created with AVX512 arithmetic capability.
Config: alpha=0.000350, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
</code></pre>
<p>i think the log out put show that my alpha(lr) is 5e-05*7
, so i am not sure is right or not.</p>
<hr />
<p>Should the learning rate on each graphics card be 5e-5 or 5e-5 * device?</p>
<p>Will it affect whether I choose deepspeed or not in accelerate?</p>
<p>that's all.</p>
<p>Thank you for seeing this. It would be even better if you could answer my doubts.</p>
","huggingface"
"77491866","cannot get dockerfile from dockerhub to work on huggingface spaces","2023-11-16 01:53:25","","0","209","<python><node.js><docker><huggingface>","<p>the image runs perfectly when i run it in a container but then but it do not work when pushed to the hub and used in my hugging face spaces
I cannot get my docker image that is in the hub to work inside my hugging face space this is the dockerfile inside the space
image pushed to the hub<a href=""https://i.sstatic.net/NCqmm.png"" rel=""nofollow noreferrer"">image in the hub</a></p>
<pre><code># Use the circulartextapp/speedinga:1.0 image from the Docker Hub as the base image
FROM circulartextapp/speedtime:latest

# User Setup (if needed)
RUN useradd -m -u 1001 user

# Set environment variables
ENV HOME=/home/user \
    PATH=/home/user/.local/bin:$PATH \
    PYTHONUNBUFFERED=1

# Set the working directory in the container to the application directory
WORKDIR /code

# Copy Python requirements file
COPY requirements.txt .

# Copy your application files into the container
COPY . /code


# Expose any ports your application might use (if applicable)
EXPOSE 7860

# Specify the command to run your Node.js application
CMD [&quot;node&quot;, &quot;server.js&quot;]
</code></pre>
<p>this is the error <a href=""https://i.sstatic.net/gl7GU.png"" rel=""nofollow noreferrer"">error image on Hf spaces</a> and this is the docker image i pushed to the hub # Use the base Docker image
FROM nikolaik/python-nodejs:python3.11-nodejs20</p>
<pre><code># Set the working directory
WORKDIR /app

# Install system packages including ffmpeg
RUN apt-get update &amp;&amp; \
    apt-get install -y ffmpeg &amp;&amp; \
    apt-get clean &amp;&amp; \
    rm -rf /var/lib/apt/lists/*

# Copy Python requirements file
COPY requirements.txt .

# Install Python dependencies
RUN pip install --no-cache-dir -r requirements.txt


# Copy Node.js application files
COPY package*.json .

# Install Node.js dependencies
RUN npm install

# Install Node.js dependencies, including Express
RUN npm install express
# Copy the rest of the application code
COPY . .

# Expose the application port
EXPOSE 7860

# Set the default command to start the application
CMD [&quot;npm&quot;, &quot;start&quot;]
</code></pre>
<p>i tried this witout using express and it still did not work i tried to run express inside the dockerfile inside the space but then the whole space just timesout after 30 minutes this is the huggingface spaces <a href=""https://i.sstatic.net/Dcblq.png"" rel=""nofollow noreferrer""></a></p>
<p>i tried putting this inside my original image # Install Node.js dependencies, including Express
RUN npm install express and also the requirements and also the spaces image</p>
","huggingface"
"77489906","What is the good way to change the path for the cache?","2023-11-15 17:49:08","","0","425","<caching><jupyter-notebook><huggingface>","<p>I'm trying to use the following code in a jupyter notebook</p>
<pre><code>import os
os.environ['HF_HOME'] = &quot;D:/Users/username/.cache/huggingface/&quot;

from transformers import AutoTokenizer
import transformers
import torch
model = &quot;meta-llama/Llama-2-7b-chat-hf&quot;
tokenizer = AutoTokenizer.from_pretrained(model)
pipeline = transformers.pipeline(
    &quot;text-generation&quot;,
    model=model,
    torch_dtype=torch.float16,
    device_map=&quot;auto&quot;,
)
</code></pre>
<p>but when I run it, the cache is always created on the C disk in the default place
C:/Users/username/.cache/huggingface/</p>
<p>I've tried several other means to write the path (with \ in place of /, setting the environment variable in the terminal...), but the default place is always used. My C disk is relatively small and near from full when this happens.</p>
<p>What is the good way to change the path for the cache?</p>
<p>Update: testing the code outside the Jupyter notebook in debug mode, I get the following message, which seems to be a way to the answer:
&quot;UserWarning: <code>huggingface_hub</code> cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in D:\Users\username.cache\huggingface\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the <code>HF_HUB_DISABLE_SYMLINKS_WARNING</code> environment variable. For more details, see <a href=""https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations"" rel=""nofollow noreferrer"">https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations</a>.
To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: <a href=""https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development</a>
warnings.warn(message)&quot;</p>
","huggingface"
"77481755","How to convert audio to an appropriate input of seamless model?","2023-11-14 15:12:59","","0","57","<audio><feature-extraction><huggingface>","<p>Facebook has released <a href=""https://huggingface.co/facebook/seamless-m4t-large"" rel=""nofollow noreferrer"">a language model on huggingface</a>, which is useful to covert speech to text and etc.</p>
<p>Following the example in <a href=""https://huggingface.co/facebook/seamless-m4t-large"" rel=""nofollow noreferrer"">here</a>, I can transcribe audio to text but not all audios. It seems that the output text highly depend on features of the input audio. I like to know what features of audio I should change to have an appropriate input.</p>
<p>I tried the following code to make sure that the sample-rate is 44100, and also the audio contains mono channel. But it seems the following changes are not enough, and there are something else I need to change in the audio input to have good output text.</p>
<pre><code>import torchaudio
import torch

def stereo_to_mono_convertor(signal):
    if signal.shape[0] &gt; 1:
        mono_signal = torch.tensor([signal[0].tolist()])
    else:
        mono_signal = signal
    return mono_signal

audio_path = &quot;myAudio.wav&quot;
waveform, sample_rate = torchaudio.load(audio_path)
waveform = stereo_to_mono_convertor(waveform)
resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=44100)
waveform = resampler(waveform)

bit_depth = 4
quantization = torchaudio.transforms.MuLawEncoding(quantization_channels=2**bit_depth)
waveform = quantization(waveform)
waveform = waveform.to(torch.float32)

torchaudio.save(&quot;changed_myAudio.wav&quot;, waveform, 44100, format=&quot;wav&quot;)
</code></pre>
","huggingface"
"77469094","ImportError: cannot import name 'ModelField' from 'pydantic.fields'","2023-11-12 13:24:43","","1","5870","<python><pydantic><huggingface>","<p>I clone <a href=""https://huggingface.co/spaces/os1187/docquery"" rel=""nofollow noreferrer"">this huggingface</a> and tested on my app. But, I got this error.</p>
<pre><code>===== Application Startup at 2023-11-12 13:13:59 =====

The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.
Moving 0 files to the new cache system
0it [00:00, ?it/s]
0it [00:00, ?it/s]
image-classification is already registered. Overwriting pipeline for task image-classification...
Traceback (most recent call last):
  File &quot;/home/user/app/app.py&quot;, line 12, in &lt;module&gt;
    from docquery.document import load_document, ImageDocument
  File &quot;/home/user/.local/lib/python3.10/site-packages/docquery/document.py&quot;, line 12, in &lt;module&gt;
    from .ocr_reader import NoOCRReaderFound, OCRReader, get_ocr_reader
  File &quot;/home/user/.local/lib/python3.10/site-packages/docquery/ocr_reader.py&quot;, line 6, in &lt;module&gt;
    from pydantic.fields import ModelField
ImportError: cannot import name 'ModelField' from 'pydantic.fields' (/home/user/.local/lib/python3.10/site-packages/pydantic/fields.py)
</code></pre>
<p>In my requirements.txt, I import like this. But it is not okay.</p>
<pre><code>torch
git+https://github.com/huggingface/transformers.git@21f6f58721dd9154357576be6de54eefef1f1818
git+https://github.com/impira/docquery.git@a494fe5af452d20011da75637aa82d246a869fa0#egg=docquery[web,donut]
</code></pre>
<p>I also tested with <code>pydantic==1.3</code> and it is not okay also. Can someone advise me please?</p>
<p><em>Updated</em>: I have attached error log from my app in huggingface.</p>
<p><a href=""https://i.sstatic.net/yDe9L.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/yDe9L.png"" alt=""enter image description here"" /></a></p>
","huggingface"
"77468342","Using layoutlm-document-qa on colab and install Tesseract","2023-11-12 09:09:20","","1","452","<google-colaboratory><tesseract><huggingface>","<p>I am using <a href=""https://huggingface.co/impira/layoutlm-document-qa"" rel=""nofollow noreferrer"">impira/layoutlm-document-qa</a> on colab as shown <a href=""https://colab.research.google.com/drive/1ESbsCmaup6E5p3q73BS55qYJlmaeabvu?usp=sharing"" rel=""nofollow noreferrer"">here</a>.</p>
<p><a href=""https://huggingface.co/impira/layoutlm-document-qa"" rel=""nofollow noreferrer"">https://huggingface.co/impira/layoutlm-document-qa</a></p>
<p>When I run, I got this error although I have already installed tesseract-oc, libtesseract-dev, Pillow and pytesseract.</p>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-9-09498387767d&gt; in &lt;cell line: 9&gt;()
      7 )
      8 
----&gt; 9 nlp(
     10     &quot;https://templates.invoicehome.com/invoice-template-us-neat-750px.png&quot;,
     11     &quot;What is the invoice number?&quot;

3 frames
/usr/local/lib/python3.10/dist-packages/transformers/pipelines/document_question_answering.py in preprocess(self, input, lang, tesseract_config)
    263             elif &quot;words&quot; in image_features and &quot;boxes&quot; in image_features:
    264                 words = image_features.pop(&quot;words&quot;)[0]
--&gt; 265                 boxes = image_features.pop(&quot;boxes&quot;)[0]
    266             elif image is not None:
    267                 if not TESSERACT_LOADED:

ValueError: If you provide an image without word_boxes, then the pipeline will run OCR using Tesseract, but pytesseract is not available
</code></pre>
<p>How shall I resolve that issue? I have been searching through stackoverflow and saw <a href=""https://stackoverflow.com/questions/51696446/tesseract-installation-in-google-colaboratory"">this link</a>.</p>
<p>Should I add <code>pytesseract.pytesseract.tesseract_cmd = r'/usr/local/bin/pytesseract'</code>? If so, where shall I add?</p>
","huggingface"
"77458736","Cannot reproduce the performance of deepset/roberta-base-squad2 on squad2 due to no-answer questions","2023-11-10 08:52:10","","0","130","<huggingface-transformers><huggingface><roberta-language-model><squad>","<p>I loaded the <code>deepset/roberta-base-squad2</code> on <code>squad2</code>, and got really poor performance on no-answer questions:</p>
<pre><code>{
&quot;exact&quot;: 42.103933294028465,
&quot;f1&quot;: 45.67169337842289,
&quot;total&quot;: 11873,
&quot;HasAns_exact&quot;: 84.2948717948718,
&quot;HasAns_f1&quot;: 91.44062339440198,
&quot;HasAns_total&quot;: 5928,
&quot;NoAns_exact&quot;: 0.0336417157275021,
&quot;NoAns_f1&quot;: 0.0336417157275021,
&quot;NoAns_total&quot;: 5945
}
</code></pre>
<p>Here's what I did:</p>
<ol>
<li>Load the model from tranformer</li>
</ol>
<pre><code>model_name = &quot;deepset/roberta-base-squad2&quot;
model = pipeline('question-answering', model=model_name, tokenizer=model_name)
</code></pre>
<ol start=""2"">
<li>Save predictions to json file</li>
</ol>
<pre><code>prediction = model({'context': context, 'question': question})
predictions_dict\[id\] = prediction\['answer'\]
</code></pre>
<ol start=""3"">
<li>Evaluate using squad2's official evaluation script</li>
</ol>
<p>I'm quite a beginner and am unsure where I might have gone wrong.</p>
<p>I'm thinking whether I should add an extra binary classifier. However, the model card on Hugging Face indicates that <code>deepset/roberta-base-squad2</code> is already a fine-tuned model, so I assume that simply loading and using it should suffice.</p>
<p>Alternatively, should I also save the prediction scores in step2, pass them to the evaluation script using the <code>--na-prob-file</code> param, and then establish a <code>--na-prob-thresh</code>? If so, what would be the appropriate threshold? I'm trying to determine the threshold that replicates the performance metrics reported on Hugging Face.</p>
<p>I've tried searching for papers, documentation, and issues related to this but haven't found anything conclusive. I feel like I might be missing some basic understanding here. Could anyone offer some guidance?</p>
","huggingface"
"77457490","How to pass model settings to Session.Run() in C++?","2023-11-10 03:07:45","","0","50","<huggingface-transformers><onnx><huggingface>","<p>In the Python implementation of ONNX, there is code like the following for PyTorch/HuggingFace models:</p>
<pre><code>import numpy as np
import onnxruntime
from onnxruntime_extensions import get_library_path

audio_file = &quot;audio.mp3&quot;
model = &quot;whisper-tiny-en-all-int8.onnx&quot;
with open(audio_file, &quot;rb&quot;) as f:
    audio = np.asarray(list(f.read()), dtype=np.uint8)

inputs = {
    &quot;audio_stream&quot;: np.array([audio]),
    &quot;max_length&quot;: np.array([30], dtype=np.int32),
    &quot;min_length&quot;: np.array([1], dtype=np.int32),
    &quot;num_beams&quot;: np.array([5], dtype=np.int32),
    &quot;num_return_sequences&quot;: np.array([1], dtype=np.int32),
    &quot;length_penalty&quot;: np.array([1.0], dtype=np.float32),
    &quot;repetition_penalty&quot;: np.array([1.0], dtype=np.float32),
    &quot;attention_mask&quot;: np.zeros((1, 80, 3000), dtype=np.int32),
}

options = onnxruntime.SessionOptions()
options.register_custom_ops_library(get_library_path())
session = onnxruntime.InferenceSession(model, options, providers=[&quot;CPUExecutionProvider&quot;])
outputs = session.run(None, inputs)[0]
</code></pre>
<p>We are able to pass a dictionary <code>inputs</code> that provides information about how the model should be evaluated.</p>
<p>However, I'm not sure how to do this in C++. I checked <a href=""https://onnxruntime.ai/docs/api/c/struct_ort_1_1detail_1_1_session_impl.html#af3f7d273f5e468ef21ef0d56d48c32fd"" rel=""nofollow noreferrer"">the <code>Run</code> API</a> and I don't really see anything like this. How does one do this in C++? Do we have to define it through <a href=""https://onnxruntime.ai/docs/api/c/struct_ort_1_1_run_options.html"" rel=""nofollow noreferrer""><code>AddConfigEntry</code></a> in <code>RunOptions</code>?</p>
","huggingface"
"77452051","Atrribute Error: 'AlignConfig' object has no attribute 'encoder', 'PoolFormerConfig' object has no attribute 'encoder'","2023-11-09 09:44:45","","0","81","<huggingface-transformers><attributeerror><huggingface>","<p>I am using** python==3.10, torch==1.13.0+cpu, transformers==4.35.0.** I am trying to (cpu)benchmark the transformer models in pytorch framework using the command:</p>
<p><code>python run_benchmark.py --models kakaobrain/align-base --batch_sizes 1  --sequence_lengths 384</code></p>
<p>when I execute the command, am getting the atrribute error as:</p>
<hr />
<p>FutureWarning: The class &lt;class 'transformers.benchmark.benchmark.PyTorchBenchmark'&gt; is deprecated. Hugging Face Benchmarking utils are deprecated in general and it is advised to use external Benchmarking libraries to benchmark Transformer models.
warnings.warn(
1 / 1</p>
<p><strong>'AlignConfig' object has no attribute 'encoder'
'AlignConfig' object has no attribute 'encoder'</strong></p>
<p>Traceback (most recent call last):
File &quot;/home/priya/priya/transformers/examples/pytorch/benchmarking/run_benchmark.py&quot;, line 50, in
main()
File &quot;/home/priya/priya/transformers/examples/pytorch/benchmarking/run_benchmark.py&quot;, line 46, in main
benchmark.run()
File &quot;/home/priya/miniconda3/envs/pyo/lib/python3.10/site-packages/transformers/benchmark/benchmark_utils.py&quot;, line 710, in run
memory, inference_summary = self.inference_memory(model_name, batch_size, sequence_length)
ValueError: too many values to unpack (expected 2)</p>
<hr />
<p>I have encountered the same issue while trying to install the transformers package using pip as well as from the source code.
And the same issue persists in many transformer models. Hereby, am listing few of them:</p>
<p>1.kakaobrain/align-base
2.BAAI/AltCLIP
3.SenseTime/deformable-detr
4.sail/poolformer_s12
etc.,</p>
<p>Am I missing something ? and another question would be Is anybody tried without config file ?</p>
<p>I understand the deprecation of Hugging Face Benchmarking utilities. I'm curious if there are anyone who are still using or have information on alternative benchmarking tools. Is there a discussion or resources we can tap into regarding this issue?</p>
<p>I have also tried --only_pretrain_model args to avoid config file. but the encoder issue persists.</p>
<p>ISSUE:</p>
<p><strong>'AlignConfig' object has no attribute 'encoder'
'AlignConfig' object has no attribute 'encoder'</strong></p>
<p>Any assistance on this issue is greatly appreciated, thankyou!</p>
","huggingface"
"77448062","create chat agent using locally hosted huggingface LLM","2023-11-08 18:25:52","","2","1387","<python-3.x><langchain><huggingface><large-language-model>","<p>I'm trying to get the hang of creating chat agents with langchain using locally hosted LLMs.  I've downloaded the flan-t5-base model weights from huggingface and I have them stored locally on my ubuntu server 18.04 LTS. I've adapted the code I found in this example creating a chat agent using gpt-3.5-turbo from openai:</p>
<p><a href=""https://github.com/samwit/langchain-tutorials/blob/main/tools/YT_LangChain_Custom_Tools_%26_Agents.ipynb"" rel=""nofollow noreferrer"">https://github.com/samwit/langchain-tutorials/blob/main/tools/YT_LangChain_Custom_Tools_%26_Agents.ipynb</a></p>
<p>to instead use the flan-t5-base model I have hosted locally.</p>
<p>I think the main difference between the example code and my code is in the example code:</p>
<pre><code># Set up the turbo LLM
turbo_llm = ChatOpenAI(
    temperature=0,
    model_name='gpt-3.5-turbo'
)
</code></pre>
<p>in my code:</p>
<pre><code>model_name='google/flan-t5-base'

# save model to specified directory
original_model = AutoModelForSeq2SeqLM\
.from_pretrained(model_name,
                 torch_dtype=torch.bfloat16,
                cache_dir='/home/username/stuff/username_storage/LLM/weights/huggingface/hub/')
tokenizer = AutoTokenizer.from_pretrained(model_name)

pipe=pipeline(
&quot;text2text-generation&quot;,
    model=original_model,
    tokenizer=tokenizer,
    max_length=1000
)

local_llm=HuggingFacePipeline(pipeline=pipe)
</code></pre>
<p>I'm getting the error below when trying to run my code.  I think I might be trying to use the wrong model for this example.  I think flan-t5-base is an encoder/decoder model good for sequence to sequence, and gpt-3.5-turbo is an encoder only model.  Does anyone see what the issue may be and can you suggest how to solve it?  For example is there a better choice for opensource alternative model to gpt-3.5-turbo like say llama-2-7b-chat?  My full code and error message are below.</p>
<p>code:</p>
<pre><code>from langchain.chains.conversation.memory import ConversationBufferWindowMemory
from langchain.agents import Tool
from langchain.tools import BaseTool


def meaning_of_life(input=&quot;&quot;):
    return 'The meaning of life is 42 if rounded but is actually 42.17658'
    
    
life_tool = Tool(
    name='Meaning of Life',
    func= meaning_of_life,
    description=&quot;Useful for when you need to answer questions about the meaning of life. input should be MOL &quot;
)


import random

def random_num(input=&quot;&quot;):
    return random.randint(0,5)
    
    
random_tool = Tool(
    name='Random number',
    func= random_num,
    description=&quot;Useful for when you need to get a random number. input should be 'random'&quot;
)


from datasets import load_dataset
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer
import torch
import time
# import evaluate
import pandas as pd
import numpy as np

model_name='google/flan-t5-base'

# save model to specified directory
original_model = AutoModelForSeq2SeqLM\
.from_pretrained(model_name,
                 torch_dtype=torch.bfloat16,
                cache_dir='/home/username/stuff/username_storage/LLM/weights/huggingface/hub/')
tokenizer = AutoTokenizer.from_pretrained(model_name)


from transformers import pipeline
from langchain.llms import HuggingFacePipeline

pipe=pipeline(
&quot;text2text-generation&quot;,
    model=original_model,
    tokenizer=tokenizer,
    max_length=1000
)

local_llm=HuggingFacePipeline(pipeline=pipe)


from langchain.agents import initialize_agent

tools = [random_tool, life_tool]

# conversational agent memory
memory = ConversationBufferWindowMemory(
    memory_key='chat_history',
    k=3,
    return_messages=True
)


# create our agent
conversational_agent = initialize_agent(
    agent='chat-conversational-react-description',
    tools=tools,
#     llm=turbo_llm,
    llm=local_llm,
    verbose=True,
    max_iterations=3,
    early_stopping_method='generate',
    memory=memory,
    handle_parsing_errors=True
)


conversational_agent('Can you give me a random number?')
</code></pre>
<p>error:</p>
<pre><code>&gt; Entering new AgentExecutor chain...
Could not parse LLM output: Option 1:**
Observation: Invalid or incomplete response
Thought:Could not parse LLM output: I'm sorry, but I'm not sure what you mean.
Observation: Invalid or incomplete response
Thought:Could not parse LLM output: I'm sorry, but I'm not sure what you mean.
Observation: Invalid or incomplete response
Thought:

---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[24], line 1
----&gt; 1 conversational_agent('Can you give me a random number?')

File ~/anaconda3/envs/llm_110623/lib/python3.10/site-packages/langchain/chains/base.py:310, in Chain.__call__(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)
    308 except BaseException as e:
    309     run_manager.on_chain_error(e)
--&gt; 310     raise e
    311 run_manager.on_chain_end(outputs)
    312 final_outputs: Dict[str, Any] = self.prep_outputs(
    313     inputs, outputs, return_only_outputs
    314 )

File ~/anaconda3/envs/llm_110623/lib/python3.10/site-packages/langchain/chains/base.py:304, in Chain.__call__(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)
    297 run_manager = callback_manager.on_chain_start(
    298     dumpd(self),
    299     inputs,
    300     name=run_name,
    301 )
    302 try:
    303     outputs = (
--&gt; 304         self._call(inputs, run_manager=run_manager)
    305         if new_arg_supported
    306         else self._call(inputs)
    307     )
    308 except BaseException as e:
    309     run_manager.on_chain_error(e)

File ~/anaconda3/envs/llm_110623/lib/python3.10/site-packages/langchain/agents/agent.py:1169, in AgentExecutor._call(self, inputs, run_manager)
   1167     iterations += 1
   1168     time_elapsed = time.time() - start_time
-&gt; 1169 output = self.agent.return_stopped_response(
   1170     self.early_stopping_method, intermediate_steps, **inputs
   1171 )
   1172 return self._return(output, intermediate_steps, run_manager=run_manager)

File ~/anaconda3/envs/llm_110623/lib/python3.10/site-packages/langchain/agents/agent.py:682, in Agent.return_stopped_response(self, early_stopping_method, intermediate_steps, **kwargs)
    680 new_inputs = {&quot;agent_scratchpad&quot;: thoughts, &quot;stop&quot;: self._stop}
    681 full_inputs = {**kwargs, **new_inputs}
--&gt; 682 full_output = self.llm_chain.predict(**full_inputs)
    683 # We try to extract a final answer
    684 parsed_output = self.output_parser.parse(full_output)

File ~/anaconda3/envs/llm_110623/lib/python3.10/site-packages/langchain/chains/llm.py:298, in LLMChain.predict(self, callbacks, **kwargs)
    283 def predict(self, callbacks: Callbacks = None, **kwargs: Any) -&gt; str:
    284     &quot;&quot;&quot;Format prompt with kwargs and pass to LLM.
    285 
    286     Args:
   (...)
    296             completion = llm.predict(adjective=&quot;funny&quot;)
    297     &quot;&quot;&quot;
--&gt; 298     return self(kwargs, callbacks=callbacks)[self.output_key]

File ~/anaconda3/envs/llm_110623/lib/python3.10/site-packages/langchain/chains/base.py:310, in Chain.__call__(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)
    308 except BaseException as e:
    309     run_manager.on_chain_error(e)
--&gt; 310     raise e
    311 run_manager.on_chain_end(outputs)
    312 final_outputs: Dict[str, Any] = self.prep_outputs(
    313     inputs, outputs, return_only_outputs
    314 )

File ~/anaconda3/envs/llm_110623/lib/python3.10/site-packages/langchain/chains/base.py:304, in Chain.__call__(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)
    297 run_manager = callback_manager.on_chain_start(
    298     dumpd(self),
    299     inputs,
    300     name=run_name,
    301 )
    302 try:
    303     outputs = (
--&gt; 304         self._call(inputs, run_manager=run_manager)
    305         if new_arg_supported
    306         else self._call(inputs)
    307     )
    308 except BaseException as e:
    309     run_manager.on_chain_error(e)

File ~/anaconda3/envs/llm_110623/lib/python3.10/site-packages/langchain/chains/llm.py:108, in LLMChain._call(self, inputs, run_manager)
    103 def _call(
    104     self,
    105     inputs: Dict[str, Any],
    106     run_manager: Optional[CallbackManagerForChainRun] = None,
    107 ) -&gt; Dict[str, str]:
--&gt; 108     response = self.generate([inputs], run_manager=run_manager)
    109     return self.create_outputs(response)[0]

File ~/anaconda3/envs/llm_110623/lib/python3.10/site-packages/langchain/chains/llm.py:117, in LLMChain.generate(self, input_list, run_manager)
    111 def generate(
    112     self,
    113     input_list: List[Dict[str, Any]],
    114     run_manager: Optional[CallbackManagerForChainRun] = None,
    115 ) -&gt; LLMResult:
    116     &quot;&quot;&quot;Generate LLM result from inputs.&quot;&quot;&quot;
--&gt; 117     prompts, stop = self.prep_prompts(input_list, run_manager=run_manager)
    118     callbacks = run_manager.get_child() if run_manager else None
    119     if isinstance(self.llm, BaseLanguageModel):

File ~/anaconda3/envs/llm_110623/lib/python3.10/site-packages/langchain/chains/llm.py:179, in LLMChain.prep_prompts(self, input_list, run_manager)
    177 for inputs in input_list:
    178     selected_inputs = {k: inputs[k] for k in self.prompt.input_variables}
--&gt; 179     prompt = self.prompt.format_prompt(**selected_inputs)
    180     _colored_text = get_colored_text(prompt.to_string(), &quot;green&quot;)
    181     _text = &quot;Prompt after formatting:\n&quot; + _colored_text

File ~/anaconda3/envs/llm_110623/lib/python3.10/site-packages/langchain/prompts/chat.py:339, in BaseChatPromptTemplate.format_prompt(self, **kwargs)
    330 def format_prompt(self, **kwargs: Any) -&gt; PromptValue:
    331     &quot;&quot;&quot;
    332     Format prompt. Should return a PromptValue.
    333     Args:
   (...)
    337         PromptValue.
    338     &quot;&quot;&quot;
--&gt; 339     messages = self.format_messages(**kwargs)
    340     return ChatPromptValue(messages=messages)

File ~/anaconda3/envs/llm_110623/lib/python3.10/site-packages/langchain/prompts/chat.py:588, in ChatPromptTemplate.format_messages(self, **kwargs)
    580 elif isinstance(
    581     message_template, (BaseMessagePromptTemplate, BaseChatPromptTemplate)
    582 ):
    583     rel_params = {
    584         k: v
    585         for k, v in kwargs.items()
    586         if k in message_template.input_variables
    587     }
--&gt; 588     message = message_template.format_messages(**rel_params)
    589     result.extend(message)
    590 else:

File ~/anaconda3/envs/llm_110623/lib/python3.10/site-packages/langchain/prompts/chat.py:99, in MessagesPlaceholder.format_messages(self, **kwargs)
     97 value = kwargs[self.variable_name]
     98 if not isinstance(value, list):
---&gt; 99     raise ValueError(
    100         f&quot;variable {self.variable_name} should be a list of base messages, &quot;
    101         f&quot;got {value}&quot;
    102     )
    103 for v in value:
    104     if not isinstance(v, BaseMessage):

ValueError: variable agent_scratchpad should be a list of base messages, got Could not parse LLM output: Option 1:**
Observation: Invalid or incomplete response
Thought:Could not parse LLM output: I'm sorry, but I'm not sure what you mean.
Observation: Invalid or incomplete response
Thought:Could not parse LLM output: I'm sorry, but I'm not sure what you mean.
Observation: Invalid or incomplete response
Thought:

I now need to return a final answer based on the previous steps:
</code></pre>
","huggingface"
"77447393","DatasetGenerationError: An error occurred while generating the dataset when trying to load the Common Voice locally","2023-11-08 16:38:16","","0","692","<python><dataset><huggingface><huggingface-datasets>","<p><strong>I downloaded the entire Common Voice afterward I tried to to load the dataset, but it couldn't load, I even reinstalled the dataset library from pip.
as soon as it get's to the data generation process it gives an error.</strong></p>
<pre><code>from datasets import load_dataset


test = load_dataset(&quot;D:\Senior\cv-corpus-15.0-2023-09-08&quot;,'en',split=&quot;test&quot;)
</code></pre>
<p><strong>Erorr:</strong></p>
<p>ArrowInvalid                              Traceback (most recent call last)</p>
<pre><code>File c:\Users\foxas\AppData\Local\Programs\Python\Python310\lib\site-packages\datasets\builder.py:1940, in ArrowBasedBuilder._prepare_split_single(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)
   1933     writer = writer_class(
   1934         features=writer._features,
   1935         path=fpath.replace(&quot;SSSSS&quot;, f&quot;{shard_id:05d}&quot;).replace(&quot;JJJJJ&quot;, f&quot;{job_id:05d}&quot;),
ref='c:\Users\foxas\AppData\Local\Programs\Python\Python310\lib\site-packages\datasets\builder.py:1'&gt;1&lt;/a&gt;;32m   (...)
   1938         embed_local_files=embed_local_files,
   1939     )
-&gt; 1940 writer.write_table(table)
   1941 num_examples_progress_update += len(table)

File c:\Users\foxas\AppData\Local\Programs\Python\Python310\lib\site-packages\datasets\arrow_writer.py:572, in ArrowWriter.write_table(self, pa_table, writer_batch_size)
    571 pa_table = pa_table.combine_chunks()
--&gt; 572 pa_table = table_cast(pa_table, self._schema)
    573 if self.embed_local_files:

File c:\Users\foxas\AppData\Local\Programs\Python\Python310\lib\site-packages\datasets\table.py:2328, in table_cast(table, schema)
   2327 if table.schema != schema:
-&gt; 2328     return cast_table_to_schema(table, schema)
   2329 elif table.schema.metadata != schema.metadata:

File c:\Users\foxas\AppData\Local\Programs\Python\Python310\lib\site-packages\datasets\table.py:2287, in cast_table_to_schema(table, schema)
   2286     raise ValueError(f&quot;Couldn't cast\n{table.schema}\nto\n{features}\nbecause column names don't match&quot;)
-&gt; 2287 arrays = [cast_array_to_feature(table[name], feature) for name, feature in features.items()]
   2288 return pa.Table.from_arrays(arrays, schema=schema)

File c:\Users\foxas\AppData\Local\Programs\Python\Python310\lib\site-packages\datasets\table.py:2287, in &lt;listcomp&gt;(.0)
   2286     raise ValueError(f&quot;Couldn't cast\n{table.schema}\nto\n{features}\nbecause column names don't match&quot;)
-&gt; 2287 arrays = [cast_array_to_feature(table[name], feature) for name, feature in features.items()]
   2288 return pa.Table.from_arrays(arrays, schema=schema)

File c:\Users\foxas\AppData\Local\Programs\Python\Python310\lib\site-packages\datasets\table.py:1831, in _wrap_for_chunked_arrays.&lt;locals&gt;.wrapper(array, *args, **kwargs)
   1830 if isinstance(array, pa.ChunkedArray):
-&gt; 1831     return pa.chunked_array([func(chunk, *args, **kwargs) for chunk in array.chunks])
   1832 else:

File c:\Users\foxas\AppData\Local\Programs\Python\Python310\lib\site-packages\datasets\table.py:1831, in &lt;listcomp&gt;(.0)
   1830 if isinstance(array, pa.ChunkedArray):
-&gt; 1831     return pa.chunked_array([func(chunk, *args, **kwargs) for chunk in array.chunks])
   1832 else:

File c:\Users\foxas\AppData\Local\Programs\Python\Python310\lib\site-packages\datasets\table.py:2143, in cast_array_to_feature(array, feature, allow_number_to_str)
   2142 elif not isinstance(feature, (Sequence, dict, list, tuple)):
-&gt; 2143     return array_cast(array, feature(), allow_number_to_str=allow_number_to_str)
   2144 raise TypeError(f&quot;Couldn't cast array of type\n{array.type}\nto\n{feature}&quot;)

File c:\Users\foxas\AppData\Local\Programs\Python\Python310\lib\site-packages\datasets\table.py:1833, in _wrap_for_chunked_arrays.&lt;locals&gt;.wrapper(array, *args, **kwargs)
   1832 else:
-&gt; 1833     return func(array, *args, **kwargs)

File c:\Users\foxas\AppData\Local\Programs\Python\Python310\lib\site-packages\datasets\table.py:2027, in array_cast(array, pa_type, allow_number_to_str)
   2026         raise TypeError(f&quot;Couldn't cast array of type {array.type} to {pa_type}&quot;)
-&gt; 2027     return array.cast(pa_type)
   2028 raise TypeError(f&quot;Couldn't cast array of type\n{array.type}\nto\n{pa_type}&quot;)

File c:\Users\foxas\AppData\Local\Programs\Python\Python310\lib\site-packages\pyarrow\array.pxi:935, in pyarrow.lib.Array.cast()

File c:\Users\foxas\AppData\Local\Programs\Python\Python310\lib\site-packages\pyarrow\compute.py:400, in cast(arr, target_type, safe, options, memory_pool)
    399         options = CastOptions.safe(target_type)
--&gt; 400 return call_function(&quot;cast&quot;, [arr], options, memory_pool)

File c:\Users\foxas\AppData\Local\Programs\Python\Python310\lib\site-packages\pyarrow\_compute.pyx:572, in pyarrow._compute.call_function()

File c:\Users\foxas\AppData\Local\Programs\Python\Python310\lib\site-packages\pyarrow\_compute.pyx:367, in pyarrow._compute.Function.call()

File c:\Users\foxas\AppData\Local\Programs\Python\Python310\lib\site-packages\pyarrow\error.pxi:144, in pyarrow.lib.pyarrow_internal_check_status()

File c:\Users\foxas\AppData\Local\Programs\Python\Python310\lib\site-packages\pyarrow\error.pxi:100, in pyarrow.lib.check_status()

ArrowInvalid: Failed to parse string: 'Benchmark' as a scalar of type double

The above exception was the direct cause of the following exception:

DatasetGenerationError                    Traceback (most recent call last)
c:\Users\foxas\OneDrive\Desktop\Senior\Models testing\Whisper Tiny.ipynb Cell 8 line 4
      1 from datasets import load_dataset
----&gt; 4 test = load_dataset(&quot;D:\Senior\cv-corpus-15.0-2023-09-08&quot;,'en',split=&quot;test&quot;)

File c:\Users\foxas\AppData\Local\Programs\Python\Python310\lib\site-packages\datasets\load.py:2153, in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)
   2150 try_from_hf_gcs = path not in _PACKAGED_DATASETS_MODULES
   2152 # Download and prepare data
-&gt; 2153 builder_instance.download_and_prepare(
   2154     download_config=download_config,
   2155     download_mode=download_mode,
   2156     verification_mode=verification_mode,
   2157     try_from_hf_gcs=try_from_hf_gcs,
   2158     num_proc=num_proc,
   2159     storage_options=storage_options,
   2160 )
   2162 # Build dataset for splits
   2163 keep_in_memory = (
   2164     keep_in_memory if keep_in_memory is not None else is_small_dataset(builder_instance.info.dataset_size)
   2165 )

File c:\Users\foxas\AppData\Local\Programs\Python\Python310\lib\site-packages\datasets\builder.py:954, in DatasetBuilder.download_and_prepare(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)
    952     if num_proc is not None:
    953         prepare_split_kwargs[&quot;num_proc&quot;] = num_proc
--&gt; 954     self._download_and_prepare(
    955         dl_manager=dl_manager,
    956         verification_mode=verification_mode,
    957         **prepare_split_kwargs,
    958         **download_and_prepare_kwargs,
    959     )
    960 # Sync info
    961 self.info.dataset_size = sum(split.num_bytes for split in self.info.splits.values())

File c:\Users\foxas\AppData\Local\Programs\Python\Python310\lib\site-packages\datasets\builder.py:1049, in DatasetBuilder._download_and_prepare(self, dl_manager, verification_mode, **prepare_split_kwargs)
   1045 split_dict.add(split_generator.split_info)
   1047 try:
   1048     # Prepare split will record examples associated to the split
-&gt; 1049     self._prepare_split(split_generator, **prepare_split_kwargs)
   1050 except OSError as e:
   1051     raise OSError(
   1052         &quot;Cannot find data file. &quot;
   1053         + (self.manual_download_instructions or &quot;&quot;)
   1054         + &quot;\nOriginal error:\n&quot;
   1055         + str(e)
   1056     ) from None

File c:\Users\foxas\AppData\Local\Programs\Python\Python310\lib\site-packages\datasets\builder.py:1813, in ArrowBasedBuilder._prepare_split(self, split_generator, file_format, num_proc, max_shard_size)
   1811 job_id = 0
   1812 with pbar:
-&gt; 1813     for job_id, done, content in self._prepare_split_single(
   1814         gen_kwargs=gen_kwargs, job_id=job_id, **_prepare_split_args
   1815     ):
   1816         if done:
   1817             result = content

File c:\Users\foxas\AppData\Local\Programs\Python\Python310\lib\site-packages\datasets\builder.py:1958, in ArrowBasedBuilder._prepare_split_single(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)
   1956     if isinstance(e, SchemaInferenceError) and e.__context__ is not None:
   1957         e = e.__context__
-&gt; 1958     raise DatasetGenerationError(&quot;An error occurred while generating the dataset&quot;) from e
   1960 yield job_id, True, (total_num_examples, total_num_bytes, writer._features, num_shards, shard_lengths)
</code></pre>
<p><strong>DatasetGenerationError: An error occurred while generating the dataset</strong></p>
","huggingface"
"77442045","Loading Pytorch Bin Model Using Much More RAM Than Expected","2023-11-07 23:30:37","","0","502","<pytorch><huggingface-transformers><huggingface><large-language-model>","<p>I am trying to use the Mistral 7B parameter model from Hugging face, specifically trying to save it locally and then reload it. I have it under 4 bit quantization and the model size is only 3.5GB. However, upon reloading the model, my WSL RAM usage consumes all the 30GB+ of devoted RAM and crashes.</p>
<p>Code for downloading 4bit quantized model:</p>
<pre><code>quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_type=&quot;nf4&quot;,
    bnb_4bit_use_double_quant=True,
)
model_id = &quot;mistralai/Mistral-7B-Instruct-v0.1&quot;

from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
model_4bit = AutoModelForCausalLM.from_pretrained(model_id, device_map=&quot;auto&quot;, quantization_config=quantization_config, )
tokenizer = AutoTokenizer.from_pretrained(model_id)
</code></pre>
<p>Code for saving model and model config:</p>
<pre><code>accelerator = Accelerator()
new_weights_location = 'mistral_model_7B'
accelerator.save_model(model=model_4bit, save_directory=new_weights_location)
model_4bit.config.to_json_file('mistral_model_7B/config.json')
</code></pre>
<p>Code for reloading model from pytorch_model.bin file:</p>
<pre><code>loaded_model = MistralForCausalLM.from_pretrained('mistral_model_7B')
</code></pre>
<p>The model downloads and runs fine, and the saved model file is only around 3.5GB. However, when I try to reload it for later usage, the memory consumption from the loading line in my WSL environment shoots up to consume all 15GB RAM and 15GB swap space, quickly crashing the kernel.</p>
<p>Also, if anyone knows a better way to save a 4bit quantized model that would be great. Currently <code>save_pretrained()</code> does not seem to work with 4bit quantized models.</p>
","huggingface"
"77435288","HuggingFace evaluate-cli create command results Error: ""datasets[0]"" must be a string","2023-11-07 02:49:06","","0","14","<huggingface><huggingface-hub>","<p>I follow the official documentation of <code>evaluate 0.4.0</code> to <strong>freshly</strong> create a custom metric in an empty folder with the following command:</p>
<pre><code>evaluate-cli create &quot;My Metric&quot; --module_type &quot;metric&quot;
</code></pre>
<p>The command returns the following error:</p>
<pre><code>Error: &quot;datasets[0]&quot; must be a string
</code></pre>
<p>The command creates these files but I am unsure whether the files are intact:</p>
<pre><code>app.py  my_metric.py  README.md  requirements.txt  tests.py
</code></pre>
<p>How to resolve this error?</p>
<p>using <code>evaluate version 0.4.0</code></p>
","huggingface"
"77428197","Registering custom model and config to AutoModel and AutoConfig","2023-11-06 00:48:18","","0","364","<machine-learning><pytorch><huggingface-transformers><huggingface>","<p>I am trying to create a custom model on top of pretrained model and save it, and use it as pre-trained model for other use case. Here is my code:</p>
<pre><code>encoder_config={
  &quot;attention_probs_dropout_prob&quot;: 0.1,
  &quot;hidden_act&quot;: &quot;gelu&quot;,
  &quot;hidden_dropout_prob&quot;: 0.1,
  &quot;hidden_size&quot;: 512,
  &quot;initializer_range&quot;: 0.02,
  &quot;intermediate_size&quot;: 3072,
  &quot;layer_norm_eps&quot;: 1e-12,
  &quot;max_position_embeddings&quot;: 512,
  &quot;model_type&quot;: &quot;bert&quot;,
  &quot;num_attention_heads&quot;: 12,
  &quot;num_hidden_layers&quot;: 12,
  &quot;pad_token_id&quot;: 0,
  &quot;type_vocab_size&quot;: 2,
  &quot;vocab_size&quot;: 30522,
  &quot;encoder_width&quot;: 768,
}
</code></pre>
<p>Custom Encoder Config:</p>
<pre><code>class TextEncoderConfig(PretrainedConfig):
    model_type=&quot;TextEncoder&quot;
    '''
        hidden_size = 768
        encoder_width = 768
        num_attention_head = 12
        num_hidden_layers = 12
    '''
    def __init__(self, hidden_size= 512, encoder_width= 512, vocab_size= 30522, 
                 max_position_embeddings=512,
                 attention_probs_dropout_prob=0.1,
                 hidden_act = &quot;gelu&quot;,
                 hidden_dropout_prob = 0.1,
                 initializer_range = 0.02,
                 intermediate_size=3072,
                 layer_norm_eps=1e-12,
                 num_attention_heads=8,
                 num_hidden_layers=8,
                 pad_token_id=0,
                 type_vocab_size=2,
                 **kwargs):
        self.hidden_size=hidden_size
        self.encoder_width = encoder_width
        self.vocab_size = vocab_size
        self.max_position_embeddings = max_position_embeddings
        self.attention_probs_dropout_prob = attention_probs_dropout_prob
        self.hidden_act = hidden_act
        self.hidden_dropout_prob = hidden_dropout_prob
        self.initializer_range = initializer_range
        self.intermediate_size = intermediate_size
        self.layer_norm_eps = layer_norm_eps
        self.num_attention_heads = num_attention_heads
        self.num_hidden_layers = num_hidden_layers
        self.pad_token_id = pad_token_id
        self.type_vocab_size = type_vocab_size
        super().__init__(**kwargs)

encoder_config = TextEncoderConfig()
</code></pre>
<p>Custom Model:</p>
<pre><code>class TextEncoder(PreTrainedModel):
    config_class = TextEncoderConfig
    def __init__(self, config):
        super().__init__(config)
        self.tokenizer = BertTokenizer.from_pretrained('google/bert_uncased_L-8_H-512_A-8')
        self.tokenizer.add_special_tokens({'bos_token':'[DEC]'})
        self.tokenizer.add_special_tokens({'additional_special_tokens':['[ENC]']})
        self.tokenizer.enc_token_id = self.tokenizer.additional_special_tokens_ids[0]
        self.encoder_config = config
        # pretrained bert encoder
        self.encoder = BertModel.from_pretrained('google/bert_uncased_L-8_H-512_A-8', config=self.encoder_config, add_pooling_layer=False, ignore_mismatched_sizes=True)

    def forward(self, in_sen):
        device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
        tokenizer_output = self.tokenizer(in_sen, padding='max_length', truncation=True, max_length=16, return_tensors='pt')
        in_seq = tokenizer_output.input_ids.to(device)
        in_mask = tokenizer_output.attention_mask.to(device)
        text_emb = self.encoder(input_ids=in_seq, attention_mask=in_mask)
        return text_emb

text_encoder = TextEncoder(encoder_config).to(device);
</code></pre>
<p>Now following this documentation: <a href=""https://huggingface.co/docs/transformers/custom_models#writing-a-custom-configuration"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/custom_models#writing-a-custom-configuration</a></p>
<p>I tried saving the custom config and model:</p>
<pre><code>text_encoder.save_pretrained(&quot;CustomModels/TextEncoder&quot;)
AutoConfig.register(&quot;TextEncoder&quot;, TextEncoderConfig)
AutoModel.register(TextEncoderConfig, TextEncoder)
</code></pre>
<p>After registering the model, I tried loading it:</p>
<pre><code>text_encoder = AutoModel.from_pretrained(&quot;CustomModels/TextEncoder&quot;)
</code></pre>
<p>I get the following error:</p>
<pre><code>KeyError                                  Traceback (most recent call last)
Cell In[3], line 1
----&gt; 1 text_encoder = AutoModel.from_pretrained(&quot;CustomModels/TextEncoder&quot;)

File /data/anaconda3/envs/bishwa_l3/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:441, in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    438     if kwargs_copy.get(&quot;torch_dtype&quot;, None) == &quot;auto&quot;:
    439         _ = kwargs_copy.pop(&quot;torch_dtype&quot;)
--&gt; 441     config, kwargs = AutoConfig.from_pretrained(
    442         pretrained_model_name_or_path,
    443         return_unused_kwargs=True,
    444         trust_remote_code=trust_remote_code,
    445         **hub_kwargs,
    446         **kwargs_copy,
    447     )
    448 if hasattr(config, &quot;auto_map&quot;) and cls.__name__ in config.auto_map:
    449     if not trust_remote_code:

File /data/anaconda3/envs/bishwa_l3/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py:939, in AutoConfig.from_pretrained(cls, pretrained_model_name_or_path, **kwargs)
    936 elif &quot;model_type&quot; in config_dict:
    938     print(CONFIG_MAPPING)
--&gt; 939     print(CONFIG_MAPPING[config_dict[&quot;model_type&quot;]])
    940     print(CONFIG_MAPPING)
    941     config_class = CONFIG_MAPPING[config_dict[&quot;model_type&quot;]]

File /data/anaconda3/envs/bishwa_l3/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py:643, in _LazyConfigMapping.__getitem__(self, key)
    641     return self._extra_content[key]
    642 if key not in self._mapping:
--&gt; 643     raise KeyError(key)
    644 value = self._mapping[key]
    645 module_name = model_type_to_module_name(key)

KeyError: 'TextEncoder'
</code></pre>
<p>I was curious why the</p>
<pre><code>TextEncoder` was not found though I have it there in the `model_type` and on printing the line `938     print(CONFIG_MAPPING)
--&gt; 939     print(CONFIG_MAPPING[config_dict[&quot;model_type&quot;]])
</code></pre>
<p>I get this result:</p>
<pre><code>_LazyConfigMapping()
&lt;class '__main__.TextEncoderConfig'&gt;
_LazyConfigMapping()
_LazyConfigMapping()
&lt;class 'transformers.models.bert.configuration_bert.BertConfig'&gt;
_LazyConfigMapping()
</code></pre>
<p>Here I am surprised how original <code>BertConfig</code> is being stored?</p>
<p>Also, the print line doesn't work if I try to solely run <code>text_encoder = AutoModel.from_pretrained(&quot;CustomModels/TextEncoder&quot;)</code> without instantiating the  <code>textEncoder</code> class.</p>
<p>Any help would be appreciated.</p>
","huggingface"
"77424251","Use an existing GitHub repo in Huggingface spaces","2023-11-04 23:57:42","","1","666","<github><huggingface><gradio>","<p>I have a github repo of an ML application, and I have set up a local demo for that using Gradio. I want to host this demo on Huggingface spaces, so I created a new space, but that initialised a new repository.</p>
<p>How do I make it use (or clone) an existing GitHub repo instead, since the repo has the app.py and requirements.txt file necessary to set-up.</p>
<p>I tried this - <a href=""https://huggingface.co/docs/hub/spaces-github-actions"" rel=""nofollow noreferrer"">https://huggingface.co/docs/hub/spaces-github-actions</a></p>
<p>But after adding the huggingface space repo to the remote of the git repo on my machine and force pushing, I get this error:</p>
<pre><code>Counting objects: 100% (75/75), done.
Delta compression using up to 12 threads
Compressing objects: 100% (66/66), done.
Writing objects: 100% (75/75), 14.63 MiB | 1.73 MiB/s, done.
Total 75 (delta 21), reused 3 (delta 0), pack-reused 0
remote: -------------------------------------------------------------------------
remote: You are not authorized to push to this repo. 
remote: Make sure that you are properly logged in.
remote: -------------------------------------------------------------------------
To https://huggingface.co/spaces/mohit-raghavendra/gt-policy-bot
 ! [remote rejected] main -&gt; main (pre-receive hook declined)
error: failed to push some refs to 'https://huggingface.co/spaces/mohit-raghavendra/gt-policy-bot'

</code></pre>
","huggingface"
"77421164","Google Colab: cuBLAS, cuFFT, and cuDNN factory registration warnings","2023-11-04 07:42:49","","3","2042","<python><tensorflow><google-colaboratory><huggingface><large-language-model>","<p>I was playing around with Wizard-Vicuna in colab. I tried the 13B model and the other day it was working fine. I can change the prompts and it would generate alright. However, yesterday when I tried playing with it again, the session would start crashing. For reference I did not change anything from my <a href=""https://colab.research.google.com/drive/1lX4gX7FyrbfXXvg7lz7SWOyiJ9_RAldJ#scrollTo=1Y_3DfNE3VSa"" rel=""nofollow noreferrer"">colab code</a> except I downgraded to the 7B model for faster testing.</p>
<p>I looked at the app.log and here are what I got:</p>
<pre><code>2023-11-04 06:48:44.105742: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered

2023-11-04 06:48:44.106486: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered

2023-11-04 06:48:44.106982: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered

2023-11-04 06:48:45.782701: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
</code></pre>
<p>I made sure I was running on T4 GPU and I tried resetting the runtime as well. I tried some solutions from <a href=""https://github.com/tensorflow/tensorflow/issues/62075"" rel=""nofollow noreferrer"">here</a> but to no avail. Any help is greatly appreciated. Thank you.</p>
","huggingface"
"77412298","How to un-batch in PyTorch?","2023-11-02 19:11:31","","0","322","<pytorch><nlp><bert-language-model><huggingface>","<p>I have a very simple text encoding script that implements BERT. I want to pass a set of texts through BERT and store it's output for further investigation.</p>
<p>I create a dataloader with tokenized text, pass it through BERT, and store the last hidden state as my output. After the model processes each batch, i iterate through the batch and copy each output into the outputs list.</p>
<p>The problem is that if the number of texts is not divisible by BATCH, the dataloader is padding the last batch to make it have BATCH samples, but in forming my outputs list, I don't know which samples are real and which are pads.</p>
<p>I'm sure this is not the best way to go about this task and would appreciate any advice. Currently using BATCH=1 (does this even matter if I'm not training?)</p>
<p>Second, unrelated question: is last_hidden_state the best thing to use as an encoding? If my max_length = 512 but i have a short text sequence will the padded tokens dilute the encoding?</p>
<p>I appreciate your advice, code below.</p>
<pre><code>X = tokenizer(corpus, padding=&quot;max_length&quot;, truncation=True, max_length=512, return_tensors=&quot;pt&quot;)
X = TensorDataset(torch.tensor(X['input_ids']), torch.tensor(X['attention_mask']))
dl = DataLoader(X, pin_memory=True, batch_size = 32)


class Encoder(nn.Module):
def __init__(self, ):
    super(Encoder, self).__init__()
    self.bert = pretrained_model
    #self.transform = transforms.Compose([transforms.ToTensor()])

def forward(self, sent_id, mask):
    x = self.bert(sent_id, attention_mask=mask).last_hidden_state[:, 0, :]
    
    return x


model = Encoder().to(device)
outputs = []
iterator = iter(dl)

for i in range(len(dl)):
    batch = next(iterator)
    a = model(batch[0].to(device), batch[1].to(device)).detach().cpu().numpy()
    for j in range(a.shape[0]):
        outputs.append(a[j,:])
</code></pre>
<p>Of note, I am using Huggingface AutoModel and AutoTokenizer.</p>
","huggingface"
"77406971","Huggingface Steraming Inference without TGI","2023-11-02 04:36:31","","1","631","<huggingface-transformers><streamlit><langchain><huggingface><huggingface-tokenizers>","<p>I found this tutorial for using TGI (Text Generation Inference) with the docker image at Text Generation Inference.</p>
<p>However, I’m having trouble using a GPU in a docker container. I was wondering if there is another way to stream the output of the model. I have tried using TextStreamer, but it can only output the result to standard output. In my case, I’m trying to send the stream output to the frontend, similar to how it works in ChatGPT</p>
","huggingface"
"77406851","Text-to-speech model that trains on small training dataset","2023-11-02 04:00:19","","0","626","<machine-learning><audio><artificial-intelligence><text-to-speech><huggingface>","<p>I am in need of a model that can train with a dataset comprising of a transcript and wav files for maximum 20 sentences.
I tried to train <a href=""https://github.com/coqui-ai/TTS"" rel=""nofollow noreferrer"">https://github.com/coqui-ai/TTS</a> on such a dataset and it did not train well at all. The inference was just noise rather than words.</p>
<p>I am looking into <a href=""https://github.com/microsoft/SpeechT5/tree/main/SpeechLM#pre-trained-and-fine-tuned-models"" rel=""nofollow noreferrer"">https://github.com/microsoft/SpeechT5/tree/main/SpeechLM#pre-trained-and-fine-tuned-models</a> but it seems like the fine tuning dataset they use also has &gt;100 hours of audio content.</p>
<p>What is the best model to research to solve this problem?</p>
","huggingface"
"77400063","Deepspeed not offloading to CPU","2023-11-01 02:42:52","","1","524","<azure><gpu><amd><huggingface><deepspeed>","<p>Deepspeed fails to offload operations to the CPU, like I thought it should do when it runs out of GPU memory. I guess I have some setting wrong. When the batch size is increased it gives an error like</p>
<p>(<a href=""https://i.sstatic.net/StcTz.png"" rel=""nofollow noreferrer"">https://i.sstatic.net/StcTz.png</a>)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 7.04 GiB (GPU 1; 79.15 GiB total capacity; 68.07 GiB already allocated; 5.90 GiB free; 72.14 GiB reserved in total by PyTorch) If reserved memory is &gt;&gt; allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF</p>
<p>(doesnt happen for smaller batch sizes).</p>
<p>Using Adam optimizer, and an AMD EPYC 7V13 64-Core Processor (an Azure VM).</p>
<p>The DeepSpeed config is -</p>
<pre><code>{
    &quot;fp16&quot;: {
        &quot;enabled&quot;: &quot;auto&quot;,
        &quot;loss_scale&quot;: 0,
        &quot;loss_scale_window&quot;: 1000,
        &quot;initial_scale_power&quot;: 16,
        &quot;hysteresis&quot;: 2,
        &quot;min_loss_scale&quot;: 1
    },

    &quot;optimizer&quot;: {
        &quot;type&quot;: &quot;AdamW&quot;,
        &quot;params&quot;: {
            &quot;lr&quot;: &quot;auto&quot;,
            &quot;betas&quot;: &quot;auto&quot;,
            &quot;eps&quot;: &quot;auto&quot;,
            &quot;weight_decay&quot;: &quot;auto&quot;
        }
    },

    &quot;scheduler&quot;: {
        &quot;type&quot;: &quot;WarmupLR&quot;,
        &quot;params&quot;: {
            &quot;warmup_min_lr&quot;: &quot;auto&quot;,
            &quot;warmup_max_lr&quot;: &quot;auto&quot;,
            &quot;warmup_num_steps&quot;: &quot;auto&quot;
        }
    },

    &quot;zero_optimization&quot;: {
        &quot;stage&quot;: 2,
        &quot;offload_optimizer&quot;: {
            &quot;device&quot;: &quot;cpu&quot;,
            &quot;pin_memory&quot;: true
        },
        &quot;allgather_partitions&quot;: true,
        &quot;allgather_bucket_size&quot;: 2e8,
        &quot;overlap_comm&quot;: true,
        &quot;reduce_scatter&quot;: true,
        &quot;reduce_bucket_size&quot;: 2e8,
        &quot;contiguous_gradients&quot;: true
    },

    &quot;gradient_accumulation_steps&quot;: &quot;auto&quot;,
    &quot;gradient_clipping&quot;: &quot;auto&quot;,
    &quot;train_batch_size&quot;: &quot;auto&quot;,
    &quot;train_micro_batch_size_per_gpu&quot;: &quot;auto&quot;
}
</code></pre>
<p>Training is done by HuggingFace Trainer, and the DeepSpeed config is used by adding the config dict to TrainingArguments</p>
<pre><code>with open(&quot;./Multi_Modal_Model/zero_config/stage_2_config.json&quot;) as f:
    z_optimiser = json.load(f)
        
training_args = TrainingArguments(
    ...
    deepspeed=z_optimiser,
    ...
)
</code></pre>
<p>Using PyTorch 1.13, trying to train a HuggingFace CLIP model.</p>
<p>Anyone know what I'm doing wrong?</p>
","huggingface"
"77398731","GPU out of memory in AI implementation for image editing","2023-10-31 19:52:24","","0","80","<python><huggingface-transformers><huggingface>","<p>I am working recently with an AI for image editing.
It worked flawlessly for several months, but out of nowhere it has stopped working. I share with you the code I use:</p>
<pre><code>import torch
from PIL import Image
import mediapy as media
import transformers
from diffusers import StableDiffusionXLImg2ImgPipeline

pipe = StableDiffusionXLImg2ImgPipeline.from_pretrained(
    &quot;stabilityai/stable-diffusion-xl-refiner-1.0&quot;,
    torch_dtype=torch.float16,
    use_safetensors=True,
).to(&quot;cuda&quot;)

# Parameters selection
url = &quot;C:\images\instru_per8_1.png&quot;
save_path = &quot;C:\images\editada\instru_per8.png&quot;
prompt = &quot;HD, circular, realistic polished woode&quot;
n_prompt = &quot;blurry, robot-like texture, disfigured, shadow, snow&quot;
img_strength = 0.3  # Consistency with the original image. Values from 0 to 1, where 0 is         identical and 1 is very different.
n_steps = 25  # Values from 1 to 100; 25 is recommended. Number of samplings for transforming     noise into an image.
creativity = 10  # Values from 2 to 20; 7-12 is recommended. The higher the value, the more the     image sticks to a given text input.

init_image = Image.open(url)
image = pipe(
    prompt=prompt,
    image=init_image,
    negative_prompt=n_prompt,
    strength=img_strength,
    num_inference_steps=n_steps,
    guidance_scale=creativity
).images[0]

image.save(save_path)
del image`
</code></pre>
<p>And the error is: &quot;torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacty of 12.00 GiB of which 0 bytes is free. Of the allocated memory 9.88 GiB is allocated by PyTorch, and 68.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF&quot;</p>
<p>I used to use a Google Colab implementation, I have migrated to visual studio to use a computer with much more capacity than the Colab environment. The error is the same in both implementations. I also tried reducing the image resolution, and it seemed to make more progress in the process, but it just failed the same way.</p>
","huggingface"
"77388822","How to download huggingface bert-base-uncased in China","2023-10-30 12:54:08","","0","690","<nlp><huggingface-transformers><bert-language-model><huggingface>","<p>I need to use huggingface <code>bert-base-uncased</code> in China.</p>
<p>I tried this on my local computer (It has VPN installed), and it is working fine.</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForMaskedLM
  
tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
model = AutoModelForMaskedLM.from_pretrained(&quot;bert-base-uncased&quot;)
</code></pre>
<p>However, I want to execute/install it on the server to train a model. On server, I have limited (no sudo) access but I use <code>conda</code> environments.</p>
<p>I tried this on server:</p>
<pre><code>git lfs install
git clone https://huggingface.co/bert-base-uncased
</code></pre>
<p>and</p>
<pre><code>from huggingface_hub import snapshot_download
snapshot_download(repo_id=&quot;bert-base-uncased&quot;)
</code></pre>
<p>But nothing seems to work and I am getting the https connection error. &quot;HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded&quot;</p>
<p>On my personal computer: Git <code>clone</code> also did not work in the Pycharm terminal and git desktop.</p>
<p>What other options do I have?
The only option I think I have now is to download it on my personal computer then transfer to the server via FTP and build it there.</p>
","huggingface"
"77388334","Image to Text - Hugging Face API Inference - Input Error","2023-10-30 11:38:38","","1","282","<python><huggingface>","<p>I would like to create a Python script in which I send a POST request via the Hugging Face API Inference for an Image to Text model. The model is: nlpconnect/vit-gpt2-image-captioning link.
I’m having issues with sending the image, as the POST request is returning a 400 error.
The Python script is as follows:</p>
<pre><code>import base64
import requests
import os

def query(API_TOKEN):
    model = 'Salesforce/blip-image-captioning-large'
    headers = {&quot;Authorization&quot;: f&quot;Bearer {API_TOKEN}&quot;}
    image_path = &quot;./demo.jpg&quot;

    # Check if the image file exists
    if not os.path.isfile(image_path):
        return {&quot;error&quot;: &quot;Image file does not exist&quot;}

    with open(image_path, &quot;rb&quot;) as image_file:
        try:
            # Try to encode the image file
            encoded_string = base64.b64encode(image_file.read()).decode()
        except Exception as e:
            return {&quot;error&quot;: f&quot;Error encoding image: {str(e)}&quot;}

    data = {
        &quot;inputs&quot;: {
            &quot;images&quot;: [encoded_string],  # using the base64 encoded string
            &quot;texts&quot;: [&quot;a photography of&quot;]  # Optional, based on your current class logic
        }
    }

    try:
        # Try to send a request to the API endpoint
        response = requests.post(
            f'https://api-inference.huggingface.co/models/{model}',
            headers=headers,
            json=data
        )
    except Exception as e:
        return {&quot;error&quot;: f&quot;Error sending request: {str(e)}&quot;}

    return response.json()
</code></pre>
<p>The function returns the error: {'error': [&quot;Error in <code>inputs</code>: Invalid image: {'images': ['/9j/4AAQSkZJRgABAQEA8ADwAA...zm2Z8+UaGwKf/Z'], 'texts': ['a photography of']}&quot;]}.</p>
<p>I’m struggling to identify the source of my error. Could someone help me? Thank you!</p>
<p>I tried calling the function but it gives me the error: {'error': [&quot;Error in <code>inputs</code>: Invalid image: {'images': ['/9j/4AAQSkZJRgABAQEA8ADwAA...zm2Z8+UaGwKf/Z'], 'texts': ['a photography of']}&quot;]}.</p>
<p>The image that i provided is demo.jpg: <code>!wget https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg</code></p>
","huggingface"
"77387888","TorToiSe TTS Wav2Vec2 model training every run","2023-10-30 10:31:45","","0","603","<python><pytorch><text-to-speech><huggingface>","<p>I'm trying to use tortoise tts lib to synthesize human speech. Here is my code:</p>
<pre><code>import os
import torchaudio

from tortoise.api import TextToSpeech
from tortoise.utils.audio import load_voice


tts = TextToSpeech(use_deepspeed=True, kv_cache=True, autoregressive_batch_size=2)

voice = 'freeman'
voice_samples, conditioning_latents = load_voice(voice)
gen = tts.tts_with_preset(
    '''
    Hello, boys and girls, my name is Fatlip
    And this is my friend, Sammy the Salmon
    Today, we're gonna teach you some fun facts about salmon
    And a brand new dance
    ''',
    preset='fast',
    voice_samples=voice_samples,
    conditioning_latents=conditioning_latents,
)

torchaudio.save(os.path.join('results/', f'synthesized_test.wav'), gen.squeeze(0).cpu(), 24000)
</code></pre>
<p>Every run I gets this warning:</p>
<p><em>Some weights of the model checkpoint at jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']</em></p>
<ul>
<li><em>This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).</em></li>
<li><em>This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).</em>
<em>Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0']</em>
<em>You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.</em></li>
</ul>
<p>and this model (jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli) starts training, that took about half an hour. This model is used inside tortoise library in <code>tortoise.utils.wav2vec_alignment.py</code>. How can I fix it? Can I train it model once and use than? Sorry if my question is stupid, I'm backend engineer.</p>
<p>Here is my spec:
MacBook Air m1 16gb ram</p>
<p>Requirements:</p>
<pre><code>absl-py==2.0.0
accelerate==0.24.0
aiofiles==23.2.1
aiohttp==3.8.6
aiosignal==1.3.1
annotated-types==0.6.0
anyascii==0.3.2
appdirs==1.4.4
async-timeout==4.0.3
attrs==23.1.0
audioread==3.0.1
Babel==2.13.1
bangla==0.0.2
blinker==1.6.3
bnnumerizer==0.0.2
bnunicodenormalizer==0.1.6
cachetools==5.3.2
certifi==2023.7.22
cffi==1.16.0
chardet==3.0.4
charset-normalizer==3.3.1
clean-fid==0.1.35
click==8.1.7
clip-anytorch==2.5.2
coloredlogs==15.0.1
comtypes==1.2.0
contourpy==1.1.1
coqpit==0.0.17
cycler==0.12.1
Cython==0.29.30
dataclasses-json==0.6.1
dateparser==1.1.8
decorator==4.4.2
docker-pycreds==0.4.0
docopt==0.6.2
dtw-python==1.3.0
editdistance==0.6.2
einops==0.6.1
encodec==0.1.1
epitran==1.17
espeak-phonemizer==1.3.1
filelock==3.13.0
Flask==2.3.3
flatbuffers==23.5.26
fonttools==4.43.1
frozenlist==1.4.0
fsspec==2023.6.0
ftfy==6.1.1
g2pkk==0.1.2
gitdb==4.0.11
GitPython==3.1.40
google-auth==2.23.3
google-auth-oauthlib==1.1.0
googletrans==4.0.0rc1
grpcio==1.59.0
gruut==2.2.3
gruut-ipa==0.13.0
gruut-lang-de==2.0.0
gruut-lang-en==2.0.0
gruut-lang-es==2.0.0
gruut-lang-fr==2.0.2
gTTS==2.4.0
h11==0.9.0
h2==3.2.0
hpack==3.0.0
hstspreload==2023.1.1
httpcore==0.9.1
httpx==0.13.3
huggingface-hub==0.18.0
humanfriendly==10.0
hypercorn==0.14.4
hyperframe==5.2.0
idna==2.10
imageio==2.31.6
imageio-ffmpeg==0.4.9
inflect==5.6.2
itsdangerous==2.1.2
jamo==0.4.1
jieba==0.42.1
Jinja2==3.1.2
joblib==1.3.2
jsonlines==1.2.0
jsonmerge==1.9.2
jsonschema==4.19.1
jsonschema-specifications==2023.7.1
k-diffusion==0.0.16
kiwisolver==1.4.5
kornia==0.7.0
lazy_loader==0.3
librosa==0.10.0
llvmlite==0.40.1
marisa-trie==1.1.0
Markdown==3.5
MarkupSafe==2.1.3
marshmallow==3.20.1
matplotlib==3.7.3
more-itertools==10.1.0
moviepy==1.0.3
mpmath==1.3.0
msgpack==1.0.7
multidict==6.0.4
munkres==1.1.4
mycroft-mimic3-tts==0.2.4
mypy-extensions==1.0.0
networkx==2.8.8
nltk==3.8.1
num2words==0.5.13
numba==0.57.0
numpy==1.22.0
oauthlib==3.2.2
onnxruntime==1.16.1
openai-whisper==20230918
packaging==23.1
pandas==1.5.3
panphon==0.20.0
pathtools==0.1.2
phonemes2ids==1.2.2
Pillow==10.0.1
platformdirs==3.11.0
pooch==1.8.0
priority==2.0.0
proglog==0.1.10
progressbar==2.5
protobuf==4.23.4
psutil==5.9.6
pyasn1==0.5.0
pyasn1-modules==0.3.0
pycparser==2.21
pydantic==2.4.2
pydantic_core==2.10.1
pynndescent==0.5.10
pyobjc==10.0
pyobjc-core==10.0
pyobjc-framework-Accessibility==10.0
pyobjc-framework-Accounts==10.0
pyobjc-framework-AddressBook==10.0
pyobjc-framework-AdServices==10.0
pyobjc-framework-AdSupport==10.0
pyobjc-framework-AppleScriptKit==10.0
pyobjc-framework-AppleScriptObjC==10.0
pyobjc-framework-ApplicationServices==10.0
pyobjc-framework-AppTrackingTransparency==10.0
pyobjc-framework-AudioVideoBridging==10.0
pyobjc-framework-AuthenticationServices==10.0
pyobjc-framework-AutomaticAssessmentConfiguration==10.0
pyobjc-framework-Automator==10.0
pyobjc-framework-AVFoundation==10.0
pyobjc-framework-AVKit==10.0
pyobjc-framework-AVRouting==10.0
pyobjc-framework-BackgroundAssets==10.0
pyobjc-framework-BusinessChat==10.0
pyobjc-framework-CalendarStore==10.0
pyobjc-framework-CallKit==10.0
pyobjc-framework-CFNetwork==10.0
pyobjc-framework-ClassKit==10.0
pyobjc-framework-CloudKit==10.0
pyobjc-framework-Cocoa==10.0
pyobjc-framework-Collaboration==10.0
pyobjc-framework-ColorSync==10.0
pyobjc-framework-Contacts==10.0
pyobjc-framework-ContactsUI==10.0
pyobjc-framework-CoreAudio==10.0
pyobjc-framework-CoreAudioKit==10.0
pyobjc-framework-CoreBluetooth==10.0
pyobjc-framework-CoreData==10.0
pyobjc-framework-CoreHaptics==10.0
pyobjc-framework-CoreLocation==10.0
pyobjc-framework-CoreMedia==10.0
pyobjc-framework-CoreMediaIO==10.0
pyobjc-framework-CoreMIDI==10.0
pyobjc-framework-CoreML==10.0
pyobjc-framework-CoreMotion==10.0
pyobjc-framework-CoreServices==10.0
pyobjc-framework-CoreSpotlight==10.0
pyobjc-framework-CoreText==10.0
pyobjc-framework-CoreWLAN==10.0
pyobjc-framework-CryptoTokenKit==10.0
pyobjc-framework-DataDetection==10.0
pyobjc-framework-DeviceCheck==10.0
pyobjc-framework-DictionaryServices==10.0
pyobjc-framework-DiscRecording==10.0
pyobjc-framework-DiscRecordingUI==10.0
pyobjc-framework-DiskArbitration==10.0
pyobjc-framework-DVDPlayback==10.0
pyobjc-framework-EventKit==10.0
pyobjc-framework-ExceptionHandling==10.0
pyobjc-framework-ExecutionPolicy==10.0
pyobjc-framework-ExtensionKit==10.0
pyobjc-framework-ExternalAccessory==10.0
pyobjc-framework-FileProvider==10.0
pyobjc-framework-FileProviderUI==10.0
pyobjc-framework-FinderSync==10.0
pyobjc-framework-FSEvents==10.0
pyobjc-framework-GameCenter==10.0
pyobjc-framework-GameController==10.0
pyobjc-framework-GameKit==10.0
pyobjc-framework-GameplayKit==10.0
pyobjc-framework-HealthKit==10.0
pyobjc-framework-ImageCaptureCore==10.0
pyobjc-framework-InputMethodKit==10.0
pyobjc-framework-InstallerPlugins==10.0
pyobjc-framework-InstantMessage==10.0
pyobjc-framework-Intents==10.0
pyobjc-framework-IntentsUI==10.0
pyobjc-framework-IOBluetooth==10.0
pyobjc-framework-IOBluetoothUI==10.0
pyobjc-framework-IOSurface==10.0
pyobjc-framework-iTunesLibrary==10.0
pyobjc-framework-KernelManagement==10.0
pyobjc-framework-LatentSemanticMapping==10.0
pyobjc-framework-LaunchServices==10.0
pyobjc-framework-libdispatch==10.0
pyobjc-framework-libxpc==10.0
pyobjc-framework-LinkPresentation==10.0
pyobjc-framework-LocalAuthentication==10.0
pyobjc-framework-LocalAuthenticationEmbeddedUI==10.0
pyobjc-framework-MailKit==10.0
pyobjc-framework-MapKit==10.0
pyobjc-framework-MediaAccessibility==10.0
pyobjc-framework-MediaLibrary==10.0
pyobjc-framework-MediaPlayer==10.0
pyobjc-framework-MediaToolbox==10.0
pyobjc-framework-Metal==10.0
pyobjc-framework-MetalFX==10.0
pyobjc-framework-MetalKit==10.0
pyobjc-framework-MetalPerformanceShaders==10.0
pyobjc-framework-MetalPerformanceShadersGraph==10.0
pyobjc-framework-MetricKit==10.0
pyobjc-framework-MLCompute==10.0
pyobjc-framework-ModelIO==10.0
pyobjc-framework-MultipeerConnectivity==10.0
pyobjc-framework-NaturalLanguage==10.0
pyobjc-framework-NetFS==10.0
pyobjc-framework-Network==10.0
pyobjc-framework-NetworkExtension==10.0
pyobjc-framework-NotificationCenter==10.0
pyobjc-framework-OpenDirectory==10.0
pyobjc-framework-OSAKit==10.0
pyobjc-framework-OSLog==10.0
pyobjc-framework-PassKit==10.0
pyobjc-framework-PencilKit==10.0
pyobjc-framework-PHASE==10.0
pyobjc-framework-Photos==10.0
pyobjc-framework-PhotosUI==10.0
pyobjc-framework-PreferencePanes==10.0
pyobjc-framework-PushKit==10.0
pyobjc-framework-Quartz==10.0
pyobjc-framework-QuickLookThumbnailing==10.0
pyobjc-framework-ReplayKit==10.0
pyobjc-framework-SafariServices==10.0
pyobjc-framework-SafetyKit==10.0
pyobjc-framework-SceneKit==10.0
pyobjc-framework-ScreenCaptureKit==10.0
pyobjc-framework-ScreenSaver==10.0
pyobjc-framework-ScreenTime==10.0
pyobjc-framework-ScriptingBridge==10.0
pyobjc-framework-SearchKit==10.0
pyobjc-framework-Security==10.0
pyobjc-framework-SecurityFoundation==10.0
pyobjc-framework-SecurityInterface==10.0
pyobjc-framework-ServiceManagement==10.0
pyobjc-framework-SharedWithYou==10.0
pyobjc-framework-SharedWithYouCore==10.0
pyobjc-framework-ShazamKit==10.0
pyobjc-framework-Social==10.0
pyobjc-framework-SoundAnalysis==10.0
pyobjc-framework-Speech==10.0
pyobjc-framework-SpriteKit==10.0
pyobjc-framework-StoreKit==10.0
pyobjc-framework-SyncServices==10.0
pyobjc-framework-SystemConfiguration==10.0
pyobjc-framework-SystemExtensions==10.0
pyobjc-framework-ThreadNetwork==10.0
pyobjc-framework-UniformTypeIdentifiers==10.0
pyobjc-framework-UserNotifications==10.0
pyobjc-framework-UserNotificationsUI==10.0
pyobjc-framework-VideoSubscriberAccount==10.0
pyobjc-framework-VideoToolbox==10.0
pyobjc-framework-Virtualization==10.0
pyobjc-framework-Vision==10.0
pyobjc-framework-WebKit==10.0
pyparsing==3.1.1
pypinyin==0.49.0
pysbd==0.3.4
python-crfsuite==0.9.9
python-dateutil==2.8.2
pyttsx3==2.90
pyttsx4==3.0.15
pytz==2023.3.post1
PyYAML==6.0.1
Quart==0.19.3
quart-cors==0.7.0
referencing==0.30.2
regex==2023.10.3
requests==2.31.0
requests-oauthlib==1.3.1
resize-right==0.0.2
rfc3986==1.5.0
rotary-embedding-torch==0.3.5
rpds-py==0.10.6
rsa==4.9
safetensors==0.4.0
scikit-image==0.22.0
scikit-learn==1.3.0
scipy==1.11.3
sentry-sdk==1.32.0
setproctitle==1.3.3
six==1.16.0
smmap==5.0.1
sniffio==1.3.0
soundfile==0.12.1
soxr==0.3.7
swagger-ui-py==21.12.8
sympy==1.12
tensorboard==2.15.0
tensorboard-data-server==0.7.2
threadpoolctl==3.2.0
tifffile==2023.9.26
tiktoken==0.3.3
tokenizers==0.13.3
tomli==2.0.1
torch==2.2.0.dev20231029
torchaudio==2.2.0.dev20231029
torchdiffeq==0.2.3
torchsde==0.2.6
torchvision==0.17.0.dev20231029
tortoise-tts==3.0.0
tqdm==4.64.1
trainer==0.0.31
trampoline==0.1.2
transformers==4.33.3
TTS==0.19.0
typing-inspect==0.9.0
typing_extensions==4.8.0
tzlocal==5.2
umap-learn==0.5.4
unicodecsv==0.14.1
Unidecode==1.3.7
urllib3==2.0.7
wandb==0.15.12
wcwidth==0.2.8
Werkzeug==3.0.1
whisper-timestamped==1.12.20
wsproto==1.2.0
xdgenvpy==2.3.5
yarl==1.9.2
</code></pre>
<p>I've tried to search for a solve of this problem, but I expect I'm the only guy with this problem</p>
","huggingface"
"77375324","Sagemaker LLama2 endpoint without huggingface token","2023-10-27 15:20:32","","0","289","<deployment><amazon-sagemaker><huggingface>","<p>I have a model.tar.gz archive with the following files in it:</p>
<pre><code>LICENSE.txt
README.md
Responsible-Use-Guide.pdf
USE_POLICY.md
checklist.chk
consolidated.00.pth
params.json
tokenizer.model
tokenizer_checklist.chk
</code></pre>
<p>this is from the model repo <code>meta-llama/Llama-2-7b-chat</code>. I want to be able to deploy an endpoint using my model archive without going through HuggingFace, no token, not using the hugging face library. Ideally pure boto3 / sagemaker.</p>
<p>Any help is welcome.</p>
<pre><code>from sagemaker import Model

instance_type = &quot;ml.g5.2xlarge&quot;
endpoint_name = &quot;ss-llama2-endpoint&quot;

model_image = get_huggingface_llm_image_uri(&quot;lmi&quot;)
model = Model(
    image_uri=model_image,
    model_data=path_to_model_data,
    role=sagemaker.get_execution_role(),
)
model.deploy(
    initial_instance_count=1,
    instance_type=instance_type,
    endpoint_name=endpoint_name,
)
</code></pre>
<p>I expected this to create and endpoint and for me to be able to query it with {&quot;inputs&quot;: &quot;some user question here&quot;}.</p>
<p>In reality - the endpoint doesn't even get created. Maybe I am using the wrong image or I am missing the inference script or something like that.</p>
","huggingface"
"77370889","Sagemaker-huggingface An error occurred (ValidationException) when calling the CreateModel operation: Requested image not found","2023-10-27 00:22:15","","0","820","<python><amazon-web-services><machine-learning><amazon-sagemaker><huggingface>","<p>I'm trying to create an asyncronous sagemaker endpoint as shown here <a href=""https://github.com/aws/amazon-sagemaker-examples/blob/main/async-inference/Async-Inference-Walkthrough.ipynb"" rel=""nofollow noreferrer"">shown here</a> but using a huggingface model, so I need to use one of the huggingface images and for that I check the official list of available options <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg-ecr-paths/ecr-us-east-1.html"" rel=""nofollow noreferrer"">here</a> but when trying to create a model based on that, I get the error:</p>
<pre><code>ClientError: An error occurred (ValidationException) when calling the CreateModel operation: Requested image 763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-training:1.9.1-transformers4.12-gpu-py38-cu111-ubuntu20.04 not found.
</code></pre>
<p>I tested like 3 options from the official list and none worked(not even the example shown in that page works). But using the exact same code, just changing to xgboost and it's corresponding version works, so I know it has something to do specifically with this images, any ideas or suggestions on where to find the list of supported huggingface images?</p>
<p><a href=""https://i.sstatic.net/2P4hb.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/2P4hb.png"" alt=""enter image description here"" /></a></p>
","huggingface"
"77367603","How do I detach the HuggingFace SageMaker training?","2023-10-26 13:52:05","77370321","1","63","<amazon-web-services><huggingface-transformers><amazon-sagemaker><huggingface><aws-batch>","<p>I am training a HuggingFace model remotely on <a href=""https://huggingface.co/docs/sagemaker/index"" rel=""nofollow noreferrer"">SageMaker integration</a>. My training job takes more than two hours, and I would like to shut my computer off during training.</p>
<p>I use the following snippet to train the model:</p>
<pre class=""lang-py prettyprint-override""><code>huggingface_estimator.fit(
  {
    'train': 's3://sagemaker-us-east-1-558105141721/samples/datasets/imdb/train',
    'test': 's3://sagemaker-us-east-1-558105141721/samples/datasets/imdb/test'
  }
)
</code></pre>
<p>How can I set up the trainer so that it the training process runs in the background so I can shut my computer down?</p>
","huggingface"
"77366326","Huggingface trainer is not showing any steps and training loss logs when training (Jupyter Notebook)","2023-10-26 10:48:19","","1","318","<python><nlp><huggingface-transformers><huggingface><huggingface-trainer>","<p>I am trying to do a multilabel classification using the model emilyalsentzer/Bio_ClinicalBERT. I have medical text data that I am using. I have attached a snippet of it where there are quite a few labels that each have some sort of text. The text itself are paragraphs relating to the label.
<a href=""https://i.sstatic.net/IYPan.png"" rel=""nofollow noreferrer"">snippet of my label and text columns</a></p>
<p>When I am training the model, I just get this:
<a href=""https://i.sstatic.net/t2TAh.png"" rel=""nofollow noreferrer"">error that im facing</a></p>
<p>I will provide my code below (my dataframe with 2 columns (label and text) is called <em>data_c</em>). Where am I going wrong and how can I fix this?</p>
<p>I tried to change the training arguments but wasnt successful.</p>
<p>My code:</p>
<pre><code>###################################################################

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
from datasets import Dataset

###################################################################

tokenizer = AutoTokenizer.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')

def preprocess_function(examples):
    return tokenizer(examples[&quot;text&quot;], 
                     truncation=True,
                     padding=&quot;max_length&quot;,
                     max_length=512)

###################################################################

doc_train, doc_test = train_test_split(data_c, test_size = 0.3)

dataset_train = Dataset.from_pandas(doc_train)
dataset_test = Dataset.from_pandas(doc_test)

tokenized_dataset_train = dataset_train.map(preprocess_function, batched=True)
tokenized_dataset_test = dataset_test.map(preprocess_function, batched=True)

from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer
from transformers import DataCollatorWithPadding

###################################################################

num_labels = len(data_c.label.unique())

disease = AutoModelForSequenceClassification.from_pretrained('emilyalsentzer/Bio_ClinicalBERT', num_labels=num_labels)

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

###################################################################

training_args = TrainingArguments(
    output_dir=&quot;./results&quot;,
    learning_rate=5e-6,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    num_train_epochs=2,
    weight_decay=0.01,
    report_to = &quot;none&quot;
)

from sklearn.metrics import precision_recall_fscore_support, accuracy_score

def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='micro')
    acc = accuracy_score(labels, preds)
    loss = pred.loss
    
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

trainer = Trainer(
    model=disease,
    args=training_args,
    train_dataset=tokenized_dataset_train,
    eval_dataset=tokenized_dataset_test,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics = compute_metrics,
)

###################################################################

trainer.train()
</code></pre>
","huggingface"
"77365876","How can I use local llm model with langchain VLLM?","2023-10-26 09:44:14","","0","1863","<langchain><huggingface><large-language-model><multiple-gpu>","<p>I tried to use my local llm model for doing some inference.</p>
<p>I have to use multiple gpu (Quadro RTX 8000 * 8), so I tried to use langchain with vLLM. Because when I used langchain with huggingface pipeline + multi gpu, many error occurred(I didn't have enough time for fix these errors).</p>
<p>There is no problem with using huggingface repo model with vLLM, but when I changed huggingface model_id to local model path, vLLM checked the model at huggingface repo, &quot;does not appear to have a file named config.json. Checkout huggingface repo/None for available files&quot; error occurred. It seems that vLLM tried to find my local path model at huggingface repository, but that does not exists, so the error occurred.</p>
<p>Here is my part of source code.</p>
<pre><code>from fastapi import FastAPI, Request, Form
from fastapi.templating import Jinja2Templates
from fastapi.staticfiles import StaticFiles
import os
from time import time
from langchain.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.retrievers.document_compressors import EmbeddingsFilter
from langchain.retrievers import ContextualCompressionRetriever
from langchain.chains import RetrievalQA
import torch
from langchain.llms import VLLM

# load local vector storage
embedding_id = &quot;intfloat/multilingual-e5-large&quot;
docsearch = FAISS.load_local(&quot;./faiss_db_{}&quot;.format(embedding_id), embeddings)
embeddings_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.80)
compression_retriever = ContextualCompressionRetriever(base_compressor=embeddings_filter,
                                                       base_retriever=docsearch.as_retriever())

llm = VLLM(model=local model path, ## path like /home/account/somewhere/models/model
           tensor_parallel_size=2,
           trust_remote_code=True,
           max_new_tokens=2048,
           top_k=50,
           top_p=0.01,
           temperature=0.01,
           repetition_penalty=1.5,
           stop=stop_word
)

qa = RetrievalQA.from_chain_type(llm=llm, chain_type=&quot;stuff&quot;, retriever=compression_retriever)

st = time()
prompt = &quot;questions&quot;
response = qa.run(query=prompt)
et = time()
print(prompt)
print('&gt;', response)
print('&gt;', et-st, 'sec consumed. ')
</code></pre>
<p>Is there any method for using local model with langchain + vLLM? Or any method for inference by multiple gpu with langchain?</p>
","huggingface"
"77358601","NameError: name 'tokenize_and_split_data' is not defined in Python code","2023-10-25 10:15:34","","4","586","<python><google-colaboratory><training-data><huggingface><huggingface-tokenizers>","<p>I want to divide the data into <code>train_dataset</code> and <code>test_dataset</code> variables. The function <code>tokenize_and_split_data</code> did not work and <code>utilities</code> library did not define. I am working on Python google colab.</p>
<pre><code>import datasets
import tempfile
import logging
import random
import config
import os
import yaml
import time
import torch
import transformers
import pandas as pd
import jsonlines

#from utilities import *
from transformers import AutoTokenizer
from transformers import AutoModelForCausalLM
from transformers import TrainingArguments
from transformers import AutoModelForCausalLM

logger = logging.getLogger(__name__)
global_config = None

model_name = &quot;EleutherAI/pythia-70m&quot;

training_config = {
    &quot;model&quot;: {
        &quot;pretrained_name&quot;: model_name,
        &quot;max_length&quot; : 2048
    },
    &quot;datasets&quot;: {
        &quot;use_hf&quot;: use_hf,
        &quot;path&quot;: dataset_path
    },
    &quot;verbose&quot;: True
}

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token
train_dataset, test_dataset = tokenize_and_split_data(training_config, tokenizer)

print(train_dataset)
print(test_dataset)
</code></pre>
<hr />
<p>Above, is the code, I cannot install <code>utilities</code> library, and this function <code>tokenize_and_split_data</code> did not defined. Can you help me please.</p>
","huggingface"
"77347556","How to parameter HuggingFace for multi CPU training?","2023-10-23 18:32:32","","0","127","<nlp><huggingface-transformers><huggingface><large-language-model><huggingface-trainer>","<p>I've follow some of the post I found online by setting the <code>.to('cpu')</code> method</p>
<pre><code>model = AutoModelForMaskedLM.from_pretrained(MODEL_TYPE).to('cpu')
</code></pre>
<p>Then in the training argument: I've set the number of device to 8 (total CPU on the device) and set the <code>no_cuda=True</code></p>
<pre><code>training_args = TrainingArguments(
    output_dir=output_dir,
    overwrite_output_dir=True,
    evaluation_strategy=&quot;steps&quot;,
    learning_rate=2e-5,
    weight_decay=0.01,
    logging_steps = 10,
    save_total_limit=5,
    load_best_model_at_end=True,
    gradient_accumulation_steps=2,
    per_device_train_batch_size=8,
    prediction_loss_only=True,
    remove_unused_columns=False,
    no_cuda=True
)
</code></pre>
<p>The code starts executing and the HF progress bar appears as expected.</p>
<p>But when I check the task manager CPU usage hover around 50%, while using Chrome in the meantime, so it's clearly not taking advantage of all CPU available but I don't know what I'm missing.</p>
","huggingface"
"77341456","Why does my transformer model have more parameters than the Huggingface implementation?","2023-10-22 19:43:18","77341769","1","500","<machine-learning><pytorch><nlp><huggingface-transformers><huggingface>","<p>I'm loading a GPT model from huggingface as follows:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig

config = AutoConfig.from_pretrained(
    &quot;gpt2&quot;,
    vocab_size=len(tokenizer),
    n_ctx=context_length,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
)

standard_gpt2 = GPT2LMHeadModel(config).to(device)
standard_gpt2_model_size = sum(t.numel() for t in standard_gpt2.parameters())
print(f&quot;GPT-2 size: {standard_gpt2_model_size/1000**2:.1f}M parameters&quot;)
# &gt;&gt;&gt; GPT-2 size: 124.4M parameters
</code></pre>
<p>If I print the model architecture I get:</p>
<pre class=""lang-py prettyprint-override""><code>GPT2LMHeadModel(
  (transformer): GPT2Model(
    (wte): Embedding(50257, 768)
    (wpe): Embedding(1024, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-11): 12 x GPT2Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=768, out_features=50257, bias=False)
)
</code></pre>
<p>Focusing on the last layer -- <code>lm_head</code>, it has <code>in_features=768, out_features=50257</code></p>
<p>So why when I replace just that one layer with the exact same number of parameters I get different results?</p>
<pre class=""lang-py prettyprint-override""><code>standard_gpt2.lm_head = nn.Sequential(
    nn.Linear(in_features = 768, out_features = 50257, bias=False)
)
standard_gpt2_model_size = sum(t.numel() for t in standard_gpt2.parameters())
print(f&quot;GPT-2 size: {standard_gpt2_model_size/1000**2:.1f}M parameters&quot;)        
# &gt;&gt;&gt; GPT-2 size: 163.0M parameters

</code></pre>
","huggingface"
"77337096","Error in deploying Falcon-7B after fine-tuning to AWS SageMaker endpoint using SageMaker Python SDK","2023-10-21 17:30:16","","1","631","<python-3.x><amazon-sagemaker><huggingface><large-language-model>","<p>I am currently running into a problem with AWS SageMaker where I cannot deploy my fine-tuned Falcon-7B model to a SageMaker endpoint after training it using an AWS training job. I am roughly following this tutorial:</p>
<p><a href=""https://www.philschmid.de/sagemaker-mistral#2-load-and-prepare-the-dataset"" rel=""nofollow noreferrer"">https://www.philschmid.de/sagemaker-mistral#2-load-and-prepare-the-dataset</a></p>
<p>Which follows a fairly predictable workflow:
create a training script, set hyperparams, create HF estimator and then train the model on data in an S3 Bucket. This part works fine, and I can store the uncompressed model weights into an s3 bucket.</p>
<p>From there, I get the LLM image uri:</p>
<pre><code>from sagemaker.huggingface import get_huggingface_llm_image_uri

# retrieve the llm image uri
llm_image = get_huggingface_llm_image_uri(
  &quot;huggingface&quot;,
  version=&quot;1.1.0&quot;,
  session=sess,
)

# print ecr image uri
print(f&quot;llm image uri: {llm_image}&quot;)
</code></pre>
<p>create a new HuggingFace Estimator with the model data set to the s3 bucket path:</p>
<pre><code>import json
from sagemaker.huggingface import HuggingFaceModel

model_s3_path = huggingface_estimator.model_data[&quot;S3DataSource&quot;][&quot;S3Uri&quot;]

# sagemaker config
instance_type = &quot;ml.g5.12xlarge&quot;
number_of_gpu = 1
health_check_timeout = 300

# Define Model and Endpoint configuration parameter
config = {
  'HF_MODEL_ID': &quot;/opt/ml/model&quot;, # path to where sagemaker stores the model
  'SM_NUM_GPUS': json.dumps(number_of_gpu), # Number of GPU used per replica
  'MAX_INPUT_LENGTH': json.dumps(1024), # Max length of input text
  'MAX_TOTAL_TOKENS': json.dumps(2048), # Max length of the generation (including input text)
}

# create HuggingFaceModel with the image uri
llm_model = HuggingFaceModel(
  role=role,
  image_uri=llm_image,
  model_data={'S3DataSource':{'S3Uri': model_s3_path,'S3DataType': 'S3Prefix','CompressionType': 'None'}},
  env=config
)
</code></pre>
<p>and then finally deploy the model:</p>
<pre><code>llm = llm_model.deploy(
  initial_instance_count=1,
  instance_type=instance_type,
  container_startup_health_check_timeout=health_check_timeout, # 10 minutes to be able to load the model
)
</code></pre>
<p>From here I always get an error like this:</p>
<pre><code>UnexpectedStatusException: Error hosting endpoint huggingface-pytorch-tgi-inference-2023-10-21-16-47-53-072: Failed. Reason: The primary container for production variant AllTraffic did not pass the ping health check. Please check CloudWatch logs for this endpoint..
</code></pre>
<p>When I check the cloudwatch logs this is the first error that pops up in the logs:</p>
<pre><code>RuntimeError: Not enough memory to handle 4096 prefill tokens. You need to decrease `--max-batch-prefill-tokens`
</code></pre>
<p>Which doesn't really make sense because I am using the fairly large instance of ml.g5.12xlarge and Falcon-7B is not a large model comparatively and in the tutorial the author successfully deploys Mistral 7B to even smaller instances like ml.g5.2xlarge. Plus even when I cut the pre-fill tokens in half I still get this error:</p>
<pre><code>RuntimeError: Not enough memory to handle 2048 prefill tokens. You need to decrease `--max-batch-prefill-tokens`
</code></pre>
<p>I've tried several permutations of this, training the model on AWS and pushing it to Huggingface and then trying to deploy the model from Huggingface (which I know is unnecessarily complicated but I was desperate for a workaround) but that didn't work and returned an error that said <code>ValueError: Unsupported model type falcon</code> and I've also tried training and deploying the model in compressed form (as in model.tar.gz) but that also didn't work and returned the same error. I am not an expert by any means, but I feel like this shouldn't be this hard, I'm wondering if anyone has experienced and solved this problem before and whether or not this problem is unique to the Falcon model series and SageMaker?</p>
","huggingface"
"77334292","perform peft with lora on flan-t5 model causing no executable batch size error","2023-10-21 00:01:12","","0","355","<python><python-3.x><large-language-model><huggingface><peft>","<p>I'm trying to perform PEFT with LoRA. I'm using the Google flan-T5 base model.  I'm using the Python code below. I'm running the code with an nvidia GPU with 8 GB of ram on Ubuntu server 18.04 LTS. In the Python code I'm loading the public dataset from huggingface. I've loaded the pre-trained flan-T5 model. I've set up the PEFat and LoRA model.</p>
<p>I then add the LoRA adapter and layers to the original LLM. I define a trainer instance, but when I try to train the PEFT adapter and save the model, I get the error below that &quot;no executable batch size found.&quot;</p>
<p>Can anyone see what the issue might be and can you suggest how to solve it?</p>
<p>Code:</p>
<pre><code># import modules
from datasets import load_dataset
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer
import torch
import time
import evaluate
import pandas as pd
import numpy as np


# load dataset and LLM 

huggingface_dataset_name = &quot;knkarthick/dialogsum&quot;

dataset = load_dataset(huggingface_dataset_name)


# load pre-trained FLAN-T5 model 

model_name='google/flan-t5-base'

original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# set up peft LORA model 

from peft import LoraConfig, get_peft_model, TaskType

lora_config = LoraConfig(
    r=32, # Rank
    lora_alpha=32,
    target_modules=[&quot;q&quot;, &quot;v&quot;],
    lora_dropout=0.05,
    bias=&quot;none&quot;,
    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5
)

# add LoRA adpter layers/parameters to the origianl LLM to be trained 

peft_model = get_peft_model(original_model, 
                            lora_config)
print(print_number_of_trainable_model_parameters(peft_model))


# define training arguments and create Trainer instance 

output_dir = f'./peft-dialogue-summary-training-{str(int(time.time()))}'

peft_training_args = TrainingArguments(
    output_dir=output_dir,
    auto_find_batch_size=True,
    learning_rate=1e-3, # Higher learning rate than full fine-tuning.
    num_train_epochs=1,
    logging_steps=1,
    max_steps=1    
)
    
peft_trainer = Trainer(
    model=peft_model,
    args=peft_training_args,
    train_dataset=tokenized_datasets[&quot;train&quot;],
)

# train PEFT adapter and save the model 

peft_trainer.train()

peft_model_path=&quot;./peft-dialogue-summary-checkpoint-local&quot;

peft_trainer.model.save_pretrained(peft_model_path)
tokenizer.save_pretrained(peft_model_path)
</code></pre>
<h1>Error:</h1>
<pre><code>---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
Cell In[16], line 1
----&gt; 1 peft_trainer.train()
      3 peft_model_path=&quot;./peft-dialogue-summary-checkpoint-local&quot;
      5 peft_trainer.model.save_pretrained(peft_model_path)

File ~/anaconda3/envs/new_llm/lib/python3.10/site-packages/transformers/trainer.py:1664, in Trainer.train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)
   1659     self.model_wrapped = self.model
   1661 inner_training_loop = find_executable_batch_size(
   1662     self._inner_training_loop, self._train_batch_size, args.auto_find_batch_size
   1663 )
-&gt; 1664 return inner_training_loop(
   1665     args=args,
   1666     resume_from_checkpoint=resume_from_checkpoint,
   1667     trial=trial,
   1668     ignore_keys_for_eval=ignore_keys_for_eval,
   1669 )

File ~/anaconda3/envs/new_llm/lib/python3.10/site-packages/accelerate/utils/memory.py:134, in find_executable_batch_size.&lt;locals&gt;.decorator(*args, **kwargs)
    132 while True:
    133     if batch_size == 0:
--&gt; 134         raise RuntimeError(&quot;No executable batch size found, reached zero.&quot;)
    135     try:
    136         return function(batch_size, *args, **kwargs)

RuntimeError: No executable batch size found, reached zero.
</code></pre>
<p>Update:</p>
<p>I restarted my kernel and error went away, not sure why.  Perhaps previous model I had run was taking up too much space.</p>
","huggingface"
"77329964","how to use huggingface models without downloading model into local machine","2023-10-20 09:53:36","","1","2282","<python><huggingface-transformers><huggingface><large-language-model>","<p>I was using Huggingface models in my python code. When I run the code its downloads everything in my local machine and it takes almost a long time to respond back. Since the model files are in my system, it occupied all my drive space. What is the other alter method I can use rather than downloading. Please help me.</p>
<p>how to use huggingface models without downloading model into local machine by using APIs.</p>
","huggingface"
"77321503","Hugging Face Sentence Transformers API is throwing ""Internal Server Error"" frequently","2023-10-19 06:43:48","","0","192","<huggingface-transformers><huggingface><sentence-similarity>","<p>I am using Huggung face sentence transformers api for the similarity scores between two set of values.</p>
<p>I am hitting api multiple times, it is working for first 5 iterations after that it will start throwing Inter Server Error.</p>
<p>is there any way to control this API?</p>
","huggingface"
"77320592","Huggingface Trainer instant shutdown Ubuntu VM in Vcenter no warning no logs no errors","2023-10-19 02:22:40","","0","48","<pytorch><huggingface><huggingface-trainer>","<p>I have been troubleshooting this issue for over a week because the problem leaves zero trace of any errors in any logs of any kind.  I'm asking this question to see if anyone else has experienced this.</p>
<p>No matter what notebook I use, or modules I install, or upgrade, or uninstall, the Trainer() module causes the VM to shutdown instantly.</p>
<p>I have an idea that it is GPU related since I have run this on the CPU with no problems.</p>
<p>I have made the devices visible (0,1)
I have also enabled / disabled wandb and set report_to=&quot;none&quot;</p>
<pre><code>Is cuda available? True
Cuda torch version? 12.1
Is cuDNN version: 8902
cuDNN enabled?  True
Device count? 1
Current device? 0
Device name?  NVIDIA A30
tensor([[0.4543, 0.0545, 0.9293],
        [0.7722, 0.6535, 0.1276],
        [0.9957, 0.5621, 0.1621],
        [0.3164, 0.2845, 0.6874],
        [0.5489, 0.7582, 0.7139]])
</code></pre>
<pre><code># setting device on GPU if available, else CPU

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print('Using device:', device)
print()

#Additional Info when using cuda
if device.type == 'cuda':
    print(torch.cuda.get_device_name(0))
    print('Memory Usage:')
    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')
    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')
</code></pre>
<p>has anyone experienced this before?</p>
","huggingface"
"77316896","ImportError: cannot import name 'override' from 'typing_extensions'","2023-10-18 13:44:53","","2","1874","<jupyter-notebook><spyder><importerror><huggingface><large-language-model>","<p>Not sure whats causing this error but it occurs on Jupyter Notebook and Spyder when running the following code:
`'''!pip install peft
!pip install -i <a href=""https://test.pypi.org/simple/"" rel=""nofollow noreferrer"">https://test.pypi.org/simple/</a> bitsandbytes
!pip install accelerate
!pip install transformers
!pip install trl
!pip install torch
'''</p>
<p>import torch
from trl import SFTTrainer
from datasets import load_dataset
from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
`</p>
<p>Tried updating all the import packages but error still not resolved. please help</p>
","huggingface"
"77314768","Hugging Face Mozilla Common Voice data structure for wav2vec","2023-10-18 08:43:59","","0","40","<huggingface><huggingface-datasets>","<p>I've creaded a low resource datset for ASR. All the audio files are in the sequence 1.mp3, 2.mp3 ..n.mp3.The sentences along with the path (of audio), age, etc are inside a csv column. When trying to use the collab notebook from the <a href=""https://huggingface.co/blog/fine-tune-xlsr-wav2vec2"" rel=""nofollow noreferrer"">huggingface blog</a>, they seems to have an extra 'audio' column with <a href=""https://huggingface.co/docs/datasets/v2.14.5/en/audio_dataset"" rel=""nofollow noreferrer"">extra information</a></p>
<p>Anyway how do I add these 'audio' column in my csv?</p>
","huggingface"
"77304201","Replace special [unusedX] tokens in a tokenizer to add domain-specific words in Bert based models - huggingface","2023-10-16 18:25:44","","1","158","<huggingface-transformers><tokenize><huggingface><huggingface-tokenizers>","<p>Let’s say I have domain-specific word that I want to add to the tokenizer I am using for fine-tuning a model further. Tokenizer for BERT is one of those tokenizers that has [[unusedX] tokens]. One of the ways to add new tokens is by using <code>add_tokens</code> or <code>add_special_tokens</code> method. E.g</p>
<pre><code>tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')
tokenizer2 = tokenizer._tokenizer
tokenizer2.add_special_tokens([&quot;DomainSpecificWord&quot;])
tokenizer2.encode(&quot;DomainSpecificWord&quot;).ids
# [101, 30522, 102]
</code></pre>
<p>However, this increases the length of tokenizer as it assigns new id to the newly added word. BERT tokenizer has almost 1000 unused tokens that can be used for this purpose. However I haven’t found an example or a documentation that shows how to achieve that.</p>
<p>P.S Tried using
<code>tokenizer.vocab['DomainSpecificWord'] = tokenizer.vocab.pop('[unused701]')</code> but didn’t work</p>
","huggingface"
"77302927","How to get a file from hugging-face iterable dataset?","2023-10-16 14:42:02","","0","152","<python><huggingface><huggingface-datasets>","<p>I am trying to wok with a audio-text pair dataset from huggingface (<a href=""https://huggingface.co/datasets/MLCommons/peoples_speech"" rel=""nofollow noreferrer"">https://huggingface.co/datasets/MLCommons/peoples_speech</a>). Since the dataset is large, I wish to stream it and use it as an iterable.</p>
<pre><code>dataset = load_dataset(&quot;MLCommons/peoples_speech&quot;, split='train', streaming=True)
dataset = dataset.take(10)
</code></pre>
<p>The dataset is an iterable with elements as dictionary as follows:</p>
<pre><code>{'id': '07282016HFUUforum_SLASH_07-28-2016_HFUUforum_DOT_mp3_00000.flac', 'audio': {'path': '07282016HFUUforum_SLASH_07-28-2016_HFUUforum_DOT_mp3_00000.flac', 'array': array([ 0.14205933,  0.20620728,  0.27151489, ...,  0.00402832,
       -0.00628662, -0.01422119]), 'sampling_rate': 16000}, 'duration_ms': 14920, 'text': &quot;i wanted this to share a few things but i'm going to not share as much as i wanted to share because we are starting late i'd like to get this thing going so we all get home at a decent hour this this election is very important to&quot;}
</code></pre>
<p>I can get the text with the key ['text']; but I am not sure how to get the audio file? There is a path within the 'audio' key ; but I don't know how to use this path. Is there any way I can download and save the audio file and then later use it in my python script. I wish to give this .flac file to an audio encoder after converting it into .wav format.</p>
","huggingface"
"77295241","Can I use huggingface inference endpoint to generate embeddings and use them in llamaindex nodes?","2023-10-15 03:44:07","","1","486","<langchain><huggingface><llama-index>","<p>Can I use huggingface inference endpoint to generate embeddings and use them in llamaindex nodes?</p>
<p>I am able to to successfully genereate embeddings via a huggingface inference endpoint, but I am not sure on the correct implementation of adding the embeddings to the nodes by llamaindex.</p>
<p>I want to replicate what this notebook does but using a huggingface inference endpoint:</p>
<p><a href=""https://gpt-index.readthedocs.io/en/latest/examples/node_postprocessor/MetadataReplacementDemo.html"" rel=""nofollow noreferrer"">https://gpt-index.readthedocs.io/en/latest/examples/node_postprocessor/MetadataReplacementDemo.html</a></p>
<p>motivation: handle embedding generation in production</p>
<p>any recommendations / suggestions on how to implement this?</p>
<p>I am using this to get embeddings:</p>
<pre><code>
endpoint = &quot;&quot;
api_org = &quot;&quot;

import requests

class HuggingFaceEmbedder:
    def __init__(self, endpoint, api_org):
        self.endpoint = endpoint
        self.api_org = api_org
        self.headers = {
            'Authorization': f'Bearer {self.api_org}'
        }

    def embed_documents(self, texts):
        if not isinstance(texts, list):
            raise ValueError(&quot;Input 'texts' must be a list of strings.&quot;)

        # Prepare the input data
        input_data = {&quot;inputs&quot;: texts}

        # Make the request
        res = requests.post(self.endpoint, headers=self.headers, json=input_data)

        if res.status_code == 200:
            return res.json()['embeddings']
        else:
            raise Exception(f&quot;Request failed with status code {res.status_code}: {res.text}&quot;)


embedder = HuggingFaceEmbedder(endpoint, api_org)'''

</code></pre>
<p>this is how the documentation does it without the inference endpoint:</p>
<pre><code>from llama_index import ServiceContext, set_global_service_context
from llama_index.llms import OpenAI
from llama_index.embeddings import OpenAIEmbedding, HuggingFaceEmbedding
from llama_index.node_parser import SentenceWindowNodeParser, SimpleNodeParser

# create the sentence window node parser w/ default settings
node_parser = SentenceWindowNodeParser.from_defaults(
    window_size=3,
    window_metadata_key=&quot;window&quot;,
    original_text_metadata_key=&quot;original_text&quot;,
)
simple_node_parser = SimpleNodeParser.from_defaults()

llm = OpenAI(model=&quot;gpt-3.5-turbo&quot;, temperature=0.1)
embed_model = HuggingFaceEmbedding(
    model_name=&quot;sentence-transformers/all-mpnet-base-v2&quot;, max_length=512
)
ctx = ServiceContext.from_defaults(
    llm=llm,
    embed_model=embed_model,
    # node_parser=node_parser,
)



from llama_index import SimpleDirectoryReader

documents = SimpleDirectoryReader(
    input_files=[&quot;./IPCC_AR6_WGII_Chapter03.pdf&quot;]
).load_data()


nodes = node_parser.get_nodes_from_documents(documents)



from llama_index import VectorStoreIndex

sentence_index = VectorStoreIndex(nodes, service_context=ctx)

</code></pre>
","huggingface"
"77292603","Implementation (and working) differences between AutoModelForCausalLMWithValueHead vs AutoModelForCausalLM?","2023-10-14 11:16:29","","2","564","<deep-learning><nlp><huggingface-transformers><huggingface><large-language-model>","<blockquote>
<p>Before any of you mark it as a &quot;Community Specific&quot; or something else, just <a href=""https://stackoverflow.com/questions/75549632/difference-between-automodelforseq2seqlm-and-automodelforcausallm"">look at this question</a> which you people so proudly have marked as <strong>Part of NLP Collective</strong>.</p>
</blockquote>
<p>I know what is <code>AutoModelForCausalLM</code>. The thing I'm asking is that in the <a href=""https://huggingface.co/docs/trl/v0.7.1/lora_tuning_peft"" rel=""nofollow noreferrer""><code>peft</code> LoRA Fine tuning tutorial</a>, the autors have used <code>AutoModelForCausalLMWithValueHead</code> while you pick any code or notebook on Fine-tuning of any LLM with <code>PEFT</code> style, you'll find <code>AutoModelForCausalLM</code> being used.</p>
<p>I went to lean on the <a href=""https://huggingface.co/docs/trl/models"" rel=""nofollow noreferrer"">official documentation of <code>AutoModelForCausalLMWithValueHead</code></a> and found:</p>
<blockquote>
<p>An autoregressive model with a value head in addition to the language model head</p>
</blockquote>
<p>What I want to ask is that <strong>How, where and more importantly, <em>WHY</em> this extra <code>ValueHead</code> is used</strong></p>
<p>In case you don't know the answer, you try to upvote the question rather than trying to close it, please. Thank you :)</p>
","huggingface"
"77291792","LLAMA-2 model download is failing","2023-10-14 06:00:08","","0","617","<nlp><huggingface-transformers><huggingface><llama-cpp-python>","<p>I've got the token from huggingface and I was able to login with huggingface-cli. I also got approval to download 7B-hf model. When I started inference run, model download started well but it is now stuck at <code>Map</code> and it is not completing.</p>
<p>What did I miss?</p>
<pre><code>Loading checkpoint shards: 100%| . . . | 2/2 [00:01&lt;00:00,  1.83it/s]
Data type of the model: torch.float32
Map:   0%| . . .| 0/2662 [00:00&lt;?, ? examples/s]
</code></pre>
","huggingface"
"77283250","Suggestions on types of models for broad-category text classification","2023-10-12 19:06:19","","0","20","<python><nlp><huggingface-transformers><huggingface>","<p>I'm quite new to machine learning and am searching for what models would be good to explore on a passion project to help a friend's website out.</p>
<p>The project is essentially that we have a database of documents (some the size of articles, others are short blurbs), and an interface where users can classify these documents by interest categories. These categories vary from things such as (action, sci-fi) to (informative, funny) and can even be as obscure as ('news articles that are funny', or 'articles that contain the word politics'). Once a new category has been created, we want to be able to recognize if other documents fall into this category. However, a document may not fall into any category at all, or could relate to multiple categories. This has become quite a difficult problem to approach. My initial thought is to start with a semantic similarity model, and fine-tune the model whenever a new category is added (obviously there will need to be some training data, i.e. a category and a few documents that are related). Though, if no fine-tuning is needed that would be the best case scenario (not sure that this is logically possible though).</p>
<p>The input of the model should be a new document, and the output should be which existing categories it relates to.</p>
<p>Any ideas on where to begin or what models to look at would be highly appreciated!</p>
","huggingface"
"77280119","Grid based decision making with Llama 2","2023-10-12 11:23:02","","0","44","<huggingface><large-language-model><llama>","<p>I am new to the LLM and trying to build app with Llama 2 (7b), LangChain which can refer the grid and provide the decision. The grid is below as CSV file. I tried it through RAG but I am getting inconsistent output. Any suggestion for format change, structure change or different technique with some code guidance are well come.</p>
<pre><code>&quot;Age range in years&quot; , &quot;Annual Income range&quot; ,&quot;Segment code&quot; , &quot;Recommendation&quot; 

&quot;18 to 35&quot;,&quot;300000 to 750000&quot;,&quot;S1&quot;,&quot;2 Wheeler&quot; 
&quot;36 to 50&quot;,&quot;300000 to 750000&quot;,&quot;S2&quot;,&quot;2 Wheeler&quot;
&quot;18 to 35&quot;,&quot;750001 to 2500000&quot;,&quot;S3&quot;,&quot;Small Car&quot; 
&quot;36 to 50&quot;,&quot;750001 to 2500000&quot;,&quot;S4&quot;,&quot;Mid Range car&quot; 
&quot;51 to 58&quot;,&quot;750001 to 2500000&quot;,&quot;S5&quot;,&quot;SUV&quot; 
&quot;above 58&quot;, &quot;above 2500001&quot;,&quot;S6&quot;,&quot;XUV&quot; 
&quot;above 58&quot;,&quot;750001 to 2500000&quot;,&quot;S7&quot;,&quot;Retired&quot;,&quot;SUV&quot; 
</code></pre>
","huggingface"
"77268783","For mms-tts-eng model I am getting ushort format error","2023-10-10 20:28:53","","1","319","<machine-learning><pytorch><huggingface-transformers><huggingface>","<pre><code>from transformers import VitsModel, AutoTokenizer
import torch

model = VitsModel.from_pretrained(&quot;facebook/mms-tts-eng&quot;)
tokenizer = AutoTokenizer.from_pretrained(&quot;facebook/mms-tts-eng&quot;)

text = &quot;some example text in the English language&quot;
inputs = tokenizer(text, return_tensors=&quot;pt&quot;)

with torch.no_grad():
  output = model(**inputs).waveform

import scipy
scipy.io.wavfile.write(&quot;techno.wav&quot;, rate=model.config.sampling_rate, 
data=output.cpu().float().numpy())
</code></pre>
<p>I am getting this:</p>
<pre><code>error: ushort format requires 0 &lt;= number &lt;= (0x7fff * 2 + 1)
</code></pre>
","huggingface"
"77265306","Local HuggingFace dataset of images and segmentation is crashed when try to load data","2023-10-10 11:17:10","","2","245","<huggingface-transformers><huggingface><huggingface-datasets>","<p>I'm trying to use Huggingface for the first time.</p>
<p>I don't find any tutorial using segFormer on HuggingFace using local dataset.</p>
<p>I fail when trying to use my local dataset.</p>
<p>I want to fine tune segFormer with my own segmentation data.
I create dataset like this (both image and label are png images)</p>
<pre><code>def create_dataset(image_paths, label_paths):
    dataset = datasets.Dataset.from_dict({&quot;image&quot;: sorted(image_paths),
                                          &quot;label&quot;: sorted(label_paths)})
    dataset = dataset.cast_column(&quot;image&quot;, datasets.Image())
    dataset = dataset.cast_column(&quot;label&quot;, datasets.Image())
    return dataset
</code></pre>
<p>I load pretrained model:</p>
<pre><code>model = SegformerForSemanticSegmentation.from_pretrained(&quot;nvidia/segformer-b0-finetuned-ade-512-512&quot;)
</code></pre>
<p>Create TrainingArguments, Trainer obj and call &quot;train&quot; :</p>
<pre><code>training_args = TrainingArguments(output_dir=&quot;test_trainer&quot;)
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=validation_dataset,
    compute_metrics=compute_metrics,
)
trainer.train()
</code></pre>
<p>When I call &quot;train&quot; I get &quot;TypeError: must be real number, not PngImageFile inside data_collector.py&quot;
Full error message</p>
<blockquote>
<p>0%|          | 0/93 [00:00&lt;?, ?it/s]Traceback (most recent call last):
File &quot;C:\Users\itishel\PycharmProjects\huggingSegFormer\lib\site-packages\accelerate\data_loader.py&quot;, line 384, in <strong>iter</strong>
current_batch = next(dataloader_iter)
File &quot;C:\Users\itishel\PycharmProjects\huggingSegFormer\lib\site-packages\torch\utils\data\dataloader.py&quot;, line 681, in <strong>next</strong>
data = self._next_data()
File &quot;C:\Users\itishel\PycharmProjects\huggingSegFormer\lib\site-packages\torch\utils\data\dataloader.py&quot;, line 721, in _next_data
data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
File &quot;C:\Users\itishel\PycharmProjects\huggingSegFormer\lib\site-packages\torch\utils\data_utils\fetch.py&quot;, line 52, in fetch
return self.collate_fn(data)
File &quot;C:\Users\itishel\PycharmProjects\huggingSegFormer\lib\site-packages\transformers\data\data_collator.py&quot;, line 70, in default_data_collator
return torch_default_data_collator(features)
File &quot;C:\Users\itishel\PycharmProjects\huggingSegFormer\lib\site-packages\transformers\data\data_collator.py&quot;, line 119, in torch_default_data_collator
batch[&quot;labels&quot;] = torch.tensor([f[&quot;label&quot;] for f in features], dtype=dtype)
TypeError: must be real number, not PngImageFile</p>
</blockquote>
<p>It crashes inside data_collector.py in &quot;torch_default_data_collector&quot;.
Here:</p>
<pre><code>    if &quot;label&quot; in first and first[&quot;label&quot;] is not None:
        label = first[&quot;label&quot;].item() if isinstance(first[&quot;label&quot;], torch.Tensor) else first[&quot;label&quot;]
        dtype = torch.long if isinstance(label, int) else torch.float
        batch[&quot;labels&quot;] = torch.tensor([f[&quot;label&quot;] for f in features], dtype=dtype)
</code></pre>
","huggingface"
"77263737","What does the output of Seq2SeqTrainer predict.predictions refer to and how to get T5 generated summaries","2023-10-10 07:34:47","","0","144","<huggingface>","<p>I am working on a T5 Summarizer and would like to know what the output for trainer.predict.predictions refer to. Also, I saw that we would have to use argmax to get the generated summary but my results for predict.predictions returns a nested array. How do I know which array to use?</p>
<p>These are my codes:</p>
<pre><code># Train trainer
from transformers import T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer

model = T5ForConditionalGeneration.from_pretrained('t5-base')

output_dir = 'output2'

# fine-tune model using the transformers.Trainer API
training_args = Seq2SeqTrainingArguments(
    output_dir=output_dir,
    num_train_epochs=6,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    eval_accumulation_steps=1, 
    prediction_loss_only=True, 
    learning_rate=4e-5,
    evaluation_strategy='steps', 
    save_steps=1000,
    save_total_limit=1, 
    eval_steps=1000, 
    load_best_model_at_end=True,
    metric_for_best_model=&quot;rouge1&quot;, 
    predict_with_generate=True,
    push_to_hub=False,
)

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset
)

trainer.train()
</code></pre>
<pre><code>#Evaluate Trainer/ get summaries
pred_args = Seq2SeqTrainingArguments(
    output_dir=output_dir,
    per_device_eval_batch_size=8,
    eval_accumulation_steps=1
)

trainer = Seq2SeqTrainer(model=model, args=pred_args)

prediction= trainer.predict(val_dataset)
preds = prediction.predictions
labels = prediction.label_ids
</code></pre>
<p>preds returns a nested array. I'm not sure which part of the array I should use argmax on.
<a href=""https://i.sstatic.net/j9zTE.png"" rel=""nofollow noreferrer"">output of trainer.predict.predictions</a></p>
","huggingface"
"77261009","Do we need to explicitly save a Hugging Face (HF) model trained with HF trainer after the trainer.train() even if we are checkpointing?","2023-10-09 18:17:47","","0","528","<huggingface-transformers><huggingface><huggingface-trainer>","<p>After I train my model, I have a line of code to train my model -- <strong>to make sure the final/best model is saved at the end of training.</strong> Is that really needed if I am using the trainer and check pointing flags?</p>
<p>My code:</p>
<pre><code>    # -- Training arguments and trainer instantiation ref: https://huggingface.co/docs/transformers/v4.31.0/en/main_classes/trainer#transformers.TrainingArguments
    output_dir = Path(f'~/data/maf_data/results_{today}/').expanduser() if not debug else Path(f'~/data/maf_data/results/').expanduser()
    print(f'{debug=} {output_dir=} \n {report_to=}')
    training_args = TrainingArguments(
        output_dir=output_dir,  #The output directory where the model predictions and checkpoints will be written.
        # num_train_epochs = num_train_epochs, 
        max_steps=max_steps,  # TODO: hard to fix, see above
        per_device_train_batch_size=per_device_train_batch_size,
        gradient_accumulation_steps=gradient_accumulation_steps,  # based on alpaca https://github.com/tatsu-lab/stanford_alpaca, allows to process effective_batch_size = gradient_accumulation_steps * batch_size, num its to accumulate before opt update step
        gradient_checkpointing = gradient_checkpointing,  # TODO depending on hardware set to true?
        optim=&quot;paged_adamw_32bit&quot;,  # David hall says to keep 32bit opt https://arxiv.org/pdf/2112.11446.pdf TODO: if we are using brain float 16 bf16 should we be using 32 bit? are optimizers always fb32?  https://discuss.huggingface.co/t/is-there-a-paged-adamw-16bf-opim-option/51284
        warmup_steps=500,  # TODO: once real training starts we can select this number for llama v2, what does llama v2 do to make it stable while v1 didn't?
        warmup_ratio=0.03,  # copying alpaca for now, number of steps for a linear warmup, TODO once real training starts change? 
        # weight_decay=0.01,  # TODO once real training change?
        weight_decay=0.00,  # TODO once real training change?
        learning_rate = 1e-5,  # TODO once real training change? anything larger than -3 I've had terrible experiences with
        max_grad_norm=1.0, # TODO once real training change?
        lr_scheduler_type=&quot;cosine&quot;,  # TODO once real training change? using what I've seen most in vision 
        logging_dir=Path('~/data/maf/logs').expanduser(),
        save_steps=2000,  # alpaca does 2000, other defaults were 500
        # logging_steps=250,
        logging_steps=50,  
        # logging_steps=1,
        remove_unused_columns=False,  # TODO don't get why https://stackoverflow.com/questions/76879872/how-to-use-huggingface-hf-trainer-train-with-custom-collate-function/76929999#76929999 , https://claude.ai/chat/475a4638-cee3-4ce0-af64-c8b8d1dc0d90
        report_to=report_to,  # change to wandb!
        fp16=False,  # never ever set to True
        bf16=torch.cuda.get_device_capability(torch.cuda.current_device())[0] &gt;= 8,  # if &gt;= 8 ==&gt; brain float 16 available or set to True if you always want fp32
        evaluation_strategy='steps',
        per_device_eval_batch_size=per_device_eval_batch_size,
        eval_accumulation_steps=eval_accumulation_steps,
        eval_steps=eval_steps,
    )
    # print(f'{training_args=}')
    print(f'{pretrained_model_name_or_path=}')

    # TODO: might be nice to figure our how llamav2 counts the number of token's they've trained on
    print(f'{train_dataset=}')
    print(f'{eval_dataset=}')
    trainer = Trainer(
        model=model,
        args=training_args,  
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=lambda data: custom_collate_fn(data, tokenizer=tokenizer)
    )

    # - Train
    cuda_visible_devices = os.environ.get('CUDA_VISIBLE_DEVICES')
    if cuda_visible_devices is not None:
        print(f&quot;CUDA_VISIBLE_DEVICES = {cuda_visible_devices}&quot;)
    trainer.train()
    trainer.save_model(output_dir=output_dir)  # TODO is this relaly needed? https://discuss.huggingface.co/t/do-we-need-to-explicity-save-the-model-if-the-save-steps-is-not-a-multiple-of-the-num-steps-with-hf/56745
    print('Done!\a')
</code></pre>
<hr />
<p>Going to use</p>
<pre><code>    # - Make sure to save best checkpoint TODO: do we really need this? https://stackoverflow.com/questions/77261009/do-we-need-to-explicitly-save-a-hugging-face-hf-model-trained-with-hf-trainer
    final_ckpt_dir = output_dir / f'ckpt-{max_steps}'
    final_ckpt_dir.mkdir(parents=True, exist_ok=True)
    trainer.save_model(output_dir=final_ckpt_dir)  # TODO is this relaly needed? https://discuss.huggingface.co/t/do-we-need-to-explicity-save-the-model-if-the-save-steps-is-not-a-multiple-of-the-num-steps-with-hf/56745
    print('Done!\a')
</code></pre>
<hr />
<h1>Bounty</h1>
<blockquote>
<p>what is the standard way to save model and tokenizer optionally at the end of a training run even if saving ckpting during training is true?</p>
</blockquote>
<hr />
<p>refs</p>
<p>related: <a href=""https://discuss.huggingface.co/t/do-we-need-to-explicity-save-the-model-if-the-save-steps-is-not-a-multiple-of-the-num-steps-with-hf/56745"" rel=""nofollow noreferrer"">https://discuss.huggingface.co/t/do-we-need-to-explicity-save-the-model-if-the-save-steps-is-not-a-multiple-of-the-num-steps-with-hf/56745</a></p>
","huggingface"
"77260669","Unable to reconstruct back the images using DDPM model","2023-10-09 17:14:16","","0","143","<python><huggingface><generative><diffusers>","<p>So I have trained a DDPM(diffusion) model and had the checkpoints. now I loaded the checkpoint and to check the performance of the model I have fed images on my test set to the model. The intuition here is that it will add noise to the images in the test set and denoise it back. once it is done I wanted to do use input image and the reconstructed image after removing noise and apply mse between them, but as you can see in the image below using the code below I am feeding one image and getting different image after denosing. can someone point out where I am doing wrong ?</p>
<p><a href=""https://i.sstatic.net/m8iFG.png"" rel=""nofollow noreferrer"">image</a></p>
<pre class=""lang-py prettyprint-override""><code># Select one image from your test dataset
with torch.no_grad():
    x_real, _ = next(iter(test_loader))  # Get the first batch of images
    x_real = x_real.cuda()
    x = x_real[0].unsqueeze(0)  # Select the first image from the batch

    z = torch.randn_like(x)
    t = torch.randint(0, len(scheduler.betas), size=(len(x),)).cuda()
    x_noisy = scheduler.add_noise(x, z, t)

    # Determine the steps at which to capture images
    capture_steps = np.linspace(0, len(scheduler.betas)-1, 4, dtype=int)

    # Initialize a list to store the images
    images = []

    # Append the original image to the images list
    images.append(x.cpu().squeeze().numpy())  # Convert the tensor to numpy for visualization

    # Loop to denoise the image
    for step in tqdm(range(len(scheduler.betas) - 1, -1, -1)):
        if step in capture_steps:
            # Convert the current image to numpy and store it
            image_np = x_noisy.cpu().squeeze().numpy()
            images.append(image_np)

        t_tensor = torch.ones((len(x_noisy),)).cuda() * step
        z_pred = model(x_noisy, t_tensor)['sample']
        x_noisy = scheduler.step(z_pred, step, x_noisy)['prev_sample']

# Compute the reconstruction error
reconstruction_error = F.mse_loss(x_noisy, x)
print(f&quot;Reconstruction Error: {reconstruction_error.item()}&quot;)

# Visualize the images in a grid
fig, axs = plt.subplots(1, len(images), figsize=(15, 5))
titles = ['Original Image'] + [f'Step: {step}' for step in capture_steps]
for idx, image in enumerate(images):
    axs[idx].imshow(image, cmap='gray')
    axs[idx].set_title(titles[idx])

# Hide the axes
for ax in axs:
    ax.axis('off')

plt.show()
</code></pre>
","huggingface"
"77257861","How to Host an Uncensored AI Model in Budget","2023-10-09 09:53:44","","0","259","<huggingface-transformers><huggingface><large-language-model><llama><huggingface-hub>","<p>I am building an app and need to use uncensored model such as -</p>
<p><a href=""https://huggingface.co/georgesung/llama2_7b_chat_uncensored"" rel=""nofollow noreferrer"">https://huggingface.co/georgesung/llama2_7b_chat_uncensored</a></p>
<p><a href=""https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1"" rel=""nofollow noreferrer"">https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1</a></p>
<p>Its expensive to host it on hugging face inference API. Please suggest if there is any other alternative which is cheaper or any platform which provides usage based APIs for the same?</p>
<p>Thanks in advance.</p>
","huggingface"
"77244497","ImportError: cannot import name 'AutoModelForImageCaptioning' from 'transformers'","2023-10-06 12:37:51","","0","124","<python><machine-learning><huggingface-transformers><huggingface>","<p>I'm trying to fine-tune a hugging face pre-trained model. This is my imports code:</p>
<pre><code>from transformers import AutoModelForImageCaptioning, Trainer
import transformers
</code></pre>
<p>When I run the code I get the error:</p>
<pre><code>ImportError: cannot import name 'AutoModelForImageCaptioning' from 'transformers' (/Users/abdulmajeed/Desktop/hfAI/venv/lib/python3.11/site-packages/transformers/__init__.py)
</code></pre>
<p>I'm using python 3.11.5 and I have made sure I have latest version of transformers installed.</p>
<p>Should I be using a specific version of transformers? Or have they changed the name for AutoModelForImageCaptioning to something else?</p>
","huggingface"
"77239471","Identifying the model LangChain is using","2023-10-05 17:47:00","","0","641","<python-3.x><huggingface-transformers><langchain><huggingface><py-langchain>","<p>I am currently struggling with the following issue. I have set a pipeline to use LLaMa-2 using HuggingFaces, with the following code:</p>
<pre><code>from torch import cuda, bfloat16
import transformers
from langchain import HuggingFacePipeline
from langchain import PromptTemplate,  LLMChain
from langchain.memory import ConversationBufferMemory


model = transformers.AutoModelForCausalLM.from_pretrained(
        'meta-llama/Llama-2-70b-chat-hf',
        cache_dir='/my/cache/dir',
        trust_remote_code=True,
        config=model_config,
        quantization_config=bnb_config,
        device_map='auto',
        use_auth_token=hf_auth
    )

pipe = transformers.pipeline(
    model=model, 
    tokenizer=tokenizer,
    torch_dtype=torch.bfloat16,
    return_full_text=True,  # langchain expects the full text
    task='text-generation',
    stopping_criteria=stopping_criteria,  # without this model rambles during chat
    temperature=0.01,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max
    max_new_tokens=480,  # mex number of tokens to generate in the output
    repetition_penalty=1.1  # without this output begins repeating
)

llm = HuggingFacePipeline(pipeline = pipe, model_kwargs = {'temperature':0})
</code></pre>
<p>After that, I initialize the langchain with some custom memory and prompt parameters:</p>
<pre><code>llm_chain_0 = LLMChain(
    llm=llm,
    prompt=prompt,
    verbose=True,
    memory=memory_0)
</code></pre>
<p>The problem is that, if I try to retrieve details on my langchain using the <code>llm_chain_0</code> command, this is what I get:</p>
<p><code>LLMChain(memory=ConversationBufferMemory(chat_memory=ChatMessageHistory(messages=[HumanMessage(content='This is the user's question, ignore', additional_kwargs={}, example=False), AIMessage(content=&quot;  This is the model's answer&quot;, additional_kwargs={}, example=False)]), output_key=None, input_key=None, return_messages=False, human_prefix='Human', ai_prefix='AI', memory_key='chat_history'), callbacks=None, callback_manager=None, verbose=True, tags=None, metadata=None, prompt=PromptTemplate(input_variables=['chat_history', 'user_input'], output_parser=None, partial_variables={}, template='[INST]&lt;&lt;SYS&gt;&gt;\nYou are a helpful assistant, you always only answer for the assistant then you stop. Read the chat history to get context\n&lt;&lt;/SYS&gt;&gt;\n\n\nChat History:\n{chat_history} \nUser: {user_input}[/INST]', template_format='f-string', validate_template=True), llm=HuggingFacePipeline(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, pipeline=&lt;transformers.pipelines.text_generation.TextGenerationPipeline object at 0x2ac4e8da6290&gt;, model_id='gpt2', model_kwargs={'temperature': 0}, pipeline_kwargs=None), output_key='text', output_parser=StrOutputParser(), return_final_only=True, llm_kwargs={})</code></p>
<p>As you can see, the <code>model_id</code> appears to be <code>gpt2</code> and not LLaMa-2. I am just wondering if I am doing something wrong and thus don't have the right model loaded.</p>
<p>Thank you all in advance for your patience and for any help you can provide.</p>
","huggingface"
"77238817","Sagemaker downloads the training image every time it runs with Hugging Face","2023-10-05 16:05:55","77240391","0","249","<huggingface-transformers><amazon-sagemaker><huggingface><huggingface-trainer>","<p>I am training my HuggingFace transformers model on SageMaker, that spins up an image each time I submit a job:</p>
<pre><code>...
Training - Downloading the training image..............................
...
</code></pre>
<p>This takes considerable time. Is there any way to skip that step?</p>
","huggingface"
"77233053","When using HF trainer, the logging for the train and eval do not show in charts, why?","2023-10-04 21:20:14","","1","867","<huggingface-transformers><huggingface><wandb><huggingface-trainer>","<p>I am trying to train my model and log both the train and eval loss (or ppl perplexity). But when I try it nothing in the charts except the system stats show.</p>
<p>Why?</p>
<p>Link to full code (should be fully contained):  <a href=""https://github.com/brando90/beyond-scale-language-data-diversity/blob/main/src/alignment/fine_tuning_with_aligned_data.py"" rel=""nofollow noreferrer"">https://github.com/brando90/beyond-scale-language-data-diversity/blob/main/src/alignment/fine_tuning_with_aligned_data.py</a></p>
<p>Snapshot of most important code?</p>
<pre><code>    # -- Training arguments and trainer instantiation ref: https://huggingface.co/docs/transformers/v4.31.0/en/main_classes/trainer#transformers.TrainingArguments
    output_dir = Path(f'~/data/maf_data/results_{today}/').expanduser() if not debug else Path(f'~/data/maf_data/results/').expanduser()
    print(f'{debug=} {output_dir=}')
    training_args = TrainingArguments(
        output_dir=output_dir,  #The output directory where the model predictions and checkpoints will be written.
        # num_train_epochs = num_train_epochs, 
        max_steps=max_steps,  # TODO: hard to fix, see above
        per_device_train_batch_size=per_device_train_batch_size,
        gradient_accumulation_steps=gradient_accumulation_steps,  # based on alpaca https://github.com/tatsu-lab/stanford_alpaca, allows to process effective_batch_size = gradient_accumulation_steps * batch_size, num its to accumulate before opt update step
        gradient_checkpointing = gradient_checkpointing,  # TODO depending on hardware set to true?
        optim=&quot;paged_adamw_32bit&quot;,  # David hall says to keep 32bit opt https://arxiv.org/pdf/2112.11446.pdf TODO: if we are using brain float 16 bf16 should we be using 32 bit? are optimizers always fb32?  https://discuss.huggingface.co/t/is-there-a-paged-adamw-16bf-opim-option/51284
        per_device_eval_batch_size=per_device_eval_batch_size,
        warmup_steps=500,  # TODO: once real training starts we can select this number for llama v2, what does llama v2 do to make it stable while v1 didn't?
        warmup_ratio=0.03,  # copying alpaca for now, number of steps for a linear warmup, TODO once real training starts change? 
        # weight_decay=0.01,  # TODO once real training change?
        weight_decay=0.00,  # TODO once real training change?
        learning_rate = 1e-5,  # TODO once real training change? anything larger than -3 I've had terrible experiences with
        max_grad_norm=1.0, # TODO once real training change?
        lr_scheduler_type=&quot;cosine&quot;,  # TODO once real training change? using what I've seen most in vision 
        logging_dir=Path('~/data/maf/logs').expanduser(),
        save_steps=2000,  # alpaca does 2000, other defaults were 500
        # logging_steps=500,
        logging_steps=50,
        remove_unused_columns=False,  # TODO don't get why https://stackoverflow.com/questions/76879872/how-to-use-huggingface-hf-trainer-train-with-custom-collate-function/76929999#76929999 , https://claude.ai/chat/475a4638-cee3-4ce0-af64-c8b8d1dc0d90
        report_to=report_to,  # change to wandb!
        fp16=False,  # never ever set to True
        bf16=torch.cuda.get_device_capability(torch.cuda.current_device())[0] &gt;= 8,  # if &gt;= 8 ==&gt; brain float 16 available or set to True if you always want fp32
    )
    # print(f'{training_args=}')
    print(f'{pretrained_model_name_or_path=}')

    # TODO: might be nice to figure our how llamav2 counts the number of token's they've trained on
    trainer = Trainer(
        model=model,
        args=training_args,  
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=lambda data: custom_collate_fn(data, tokenizer=tokenizer)
    )
    # - TODO bellow is for qlora from falcon, has same interface as Trainer later lets use: https://github.com/artidoro/qlora
    # from trl import SFTTrainer
    # peft_config = None
    # trainer = SFTTrainer(
    #     model=model,
    #     train_dataset=trainset,
    #     peft_config=peft_config,
    #     dataset_text_field=&quot;text&quot;,
    #     max_seq_length=max_seq_length,
    #     tokenizer=tokenizer,
    #     args=training_arguments,
    # )
    # TODO why this? https://discuss.huggingface.co/t/why-do-you-need-to-re-upcast-the-norm-layers-of-hf-falcon-to-fb32/46139
    # for name, module in trainer.model.named_modules():
    #     if &quot;norm&quot; in name:
    #         module = module.to(torch.float32)
</code></pre>
<p>cross: <a href=""https://community.wandb.ai/t/when-using-hf-trainer-the-logging-for-the-train-and-eval-do-not-show-in-charts-why/5165"" rel=""nofollow noreferrer"">https://community.wandb.ai/t/when-using-hf-trainer-the-logging-for-the-train-and-eval-do-not-show-in-charts-why/5165</a></p>
","huggingface"
"77218008","Why won't Google Tapas accept my Table? TypeError: expected string or bytes-like object","2023-10-02 19:22:45","","0","145","<python><typeerror><huggingface>","<p>I think the issue is related with the following code. I took a lot of it from a tutorial on Tapas so I'm not exactly sure what the code is meant to do besides embed the information in the table which has been formatted to a string.</p>
<pre><code>from tqdm.auto import tqdm

# we will use batches of 64
batch_size = 64

for i in tqdm(range(0, len(processed_tables), batch_size)):
    # find end of batch
    i_end = min(i+batch_size, len(processed_tables))
    # extract batch
    batch = processed_tables[i:i_end]
    # generate embeddings for batch
    emb = retriever.encode(batch).tolist()
    # create unique IDs ranging from zero to the total number of tables in the dataset
    ids = [f&quot;{idx}&quot; for idx in range(i, i_end)]
    # add all to upsert list
    to_upsert = list(zip(ids, emb))
    # upsert/insert these records to pinecone
    _ = index.upsert(vectors=to_upsert)

# check that we have all vectors in index
index.describe_index_stats()
</code></pre>
<p>Output</p>
<pre><code>{'dimension': 768,
 'index_fullness': 0.0,
 'namespaces': {},
 'total_vector_count': 0}
</code></pre>
<p>Here the code successfully gets the right table to answer the question.</p>
<pre><code>from transformers import pipeline, TapasTokenizer, TapasForQuestionAnswering

query = &quot;What is the amount of the unentered transaction?&quot;

# generate embedding for the query
xq = retriever.encode([query]).tolist()
# query pinecone index to find the table containing answer to the query
result = index.query(xq, top_k=1)

id = int(result[&quot;matches&quot;][0][&quot;id&quot;])

model_name = &quot;google/tapas-base-finetuned-wtq&quot;
# load the tokenizer and the model from huggingface model hub
tokenizer = TapasTokenizer.from_pretrained(model_name)
model = TapasForQuestionAnswering.from_pretrained(model_name, local_files_only=False)
# load the model and tokenizer into a question-answering pipeline
pipe = pipeline(&quot;table-question-answering&quot;,  model=model, tokenizer=tokenizer, device=device)
</code></pre>
<p>I pass the table into the table reader but the pipeline expects a string or a byte.</p>
<pre><code>pipe(table=tables[id], query=query)
</code></pre>
<p>Output:</p>
<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-23-5d351749f21b&gt; in &lt;cell line: 1&gt;()
----&gt; 1 pipe(table=tables[id], query=query)

15 frames
/usr/local/lib/python3.10/dist-packages/transformers/pipelines/table_question_answering.py in __call__(self, *args, **kwargs)
    345         pipeline_inputs = self._args_parser(*args, **kwargs)
    346 
--&gt; 347         results = super().__call__(pipeline_inputs, **kwargs)
    348         if len(results) == 1:
    349             return results[0]

/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py in __call__(self, inputs, num_workers, batch_size, *args, **kwargs)
   1119                     inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params
   1120                 )
-&gt; 1121                 outputs = list(final_iterator)
   1122                 return outputs
   1123             else:

/usr/local/lib/python3.10/dist-packages/transformers/pipelines/pt_utils.py in __next__(self)
    122 
    123         # We're out of items within a batch
--&gt; 124         item = next(self.iterator)
    125         processed = self.infer(item, **self.params)
    126         # We now have a batch of &quot;inferred things&quot;.

/usr/local/lib/python3.10/dist-packages/transformers/pipelines/pt_utils.py in __next__(self)
    122 
    123         # We're out of items within a batch
--&gt; 124         item = next(self.iterator)
    125         processed = self.infer(item, **self.params)
    126         # We now have a batch of &quot;inferred things&quot;.

/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py in __next__(self)
    631                 # TODO(https://github.com/pytorch/pytorch/issues/76750)
    632                 self._reset()  # type: ignore[call-arg]
--&gt; 633             data = self._next_data()
    634             self._num_yielded += 1
    635             if self._dataset_kind == _DatasetKind.Iterable and \

/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py in _next_data(self)
    675     def _next_data(self):
    676         index = self._next_index()  # may raise StopIteration
--&gt; 677         data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
    678         if self._pin_memory:
    679             data = _utils.pin_memory.pin_memory(data, self._pin_memory_device)

/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py in fetch(self, possibly_batched_index)
     49                 data = self.dataset.__getitems__(possibly_batched_index)
     50             else:
---&gt; 51                 data = [self.dataset[idx] for idx in possibly_batched_index]
     52         else:
     53             data = self.dataset[possibly_batched_index]

/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py in &lt;listcomp&gt;(.0)
     49                 data = self.dataset.__getitems__(possibly_batched_index)
     50             else:
---&gt; 51                 data = [self.dataset[idx] for idx in possibly_batched_index]
     52         else:
     53             data = self.dataset[possibly_batched_index]

/usr/local/lib/python3.10/dist-packages/transformers/pipelines/pt_utils.py in __getitem__(self, i)
     17     def __getitem__(self, i):
     18         item = self.dataset[i]
---&gt; 19         processed = self.process(item, **self.params)
     20         return processed
     21 

/usr/local/lib/python3.10/dist-packages/transformers/pipelines/table_question_answering.py in preprocess(self, pipeline_input, sequential, padding, truncation)
    374         if query is None or query == &quot;&quot;:
    375             raise ValueError(&quot;query is empty&quot;)
--&gt; 376         inputs = self.tokenizer(table, query, return_tensors=self.framework, truncation=truncation, padding=padding)
    377         inputs[&quot;table&quot;] = table
    378         return inputs

/usr/local/lib/python3.10/dist-packages/transformers/models/tapas/tokenization_tapas.py in __call__(self, table, queries, answer_coordinates, answer_text, add_special_tokens, padding, truncation, max_length, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)
    669             )
    670         else:
--&gt; 671             return self.encode_plus(
    672                 table=table,
    673                 query=queries,

/usr/local/lib/python3.10/dist-packages/transformers/models/tapas/tokenization_tapas.py in encode_plus(self, table, query, answer_coordinates, answer_text, add_special_tokens, padding, truncation, max_length, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)
   1035             )
   1036 
-&gt; 1037         return self._encode_plus(
   1038             table=table,
   1039             query=query,

/usr/local/lib/python3.10/dist-packages/transformers/models/tapas/tokenization_tapas.py in _encode_plus(self, table, query, answer_coordinates, answer_text, add_special_tokens, padding, truncation, max_length, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)
   1086             )
   1087 
-&gt; 1088         table_tokens = self._tokenize_table(table)
   1089         query, query_tokens = self._get_question_tokens(query)
   1090 

/usr/local/lib/python3.10/dist-packages/transformers/models/tapas/tokenization_tapas.py in _tokenize_table(self, table)
   1385             tokenized_row = []
   1386             for cell in row:
-&gt; 1387                 tokenized_row.append(self.tokenize(cell))
   1388             tokenized_rows.append(tokenized_row)
   1389 

/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py in tokenize(self, text, **kwargs)
    513             ]
    514             pattern = r&quot;(&quot; + r&quot;|&quot;.join(escaped_special_toks) + r&quot;)|&quot; + r&quot;(.+?)&quot;
--&gt; 515             text = re.sub(pattern, lambda m: m.groups()[0] or m.groups()[1].lower(), text)
    516 
    517         # split_special_tokens: empty `no_split_token`

/usr/lib/python3.10/re.py in sub(pattern, repl, string, count, flags)
    207     a callable, it's passed the Match object and must return
    208     a replacement string to be used.&quot;&quot;&quot;
--&gt; 209     return _compile(pattern, flags).sub(repl, string, count)
    210 
    211 def subn(pattern, repl, string, count=0, flags=0):

TypeError: expected string or bytes-like object
</code></pre>
<pre><code>type(tables[id])
</code></pre>
<p>Output</p>
<pre><code>pandas.core.frame.DataFrame
</code></pre>
<p>The tables were formatted as follows:</p>
<pre><code>def _preprocess_tables(tables: list):
    processed = []
    # loop through all tables
    for table in tables:
        # convert the table to csv and
        processed_table = &quot;\n&quot;.join([table.to_csv(index=False)])
        # add the processed table to processed list
        processed.append(processed_table)
    return processed

tables = [positions, transactions]

# format all the dataframes in the tables list
processed_tables = _preprocess_tables(tables)

# display the formatted table
processed_tables[1]
</code></pre>
<p>Output</p>
<pre><code>Status,Date,AssetID,sec_type,Units,Amount,trans_type\nEntered,06/23/2022,999LP999,GENERIC_LP_DPV,1,0,BUY\nEntered,07/14/2022,888LP888,GENERIC_LP_DPV,1,0,BUY\nEntered,09/30/2022,777LP777,GENERIC_LP_DPV,1,0,BUY\nEntered,10/12/2022,555LP555,GENERIC_LP_DPV,1,0,BUY\nUnentered,12/15/2022,999LP999,GENERIC_LP_DPV,0,50000,RCAP\nEntered,07/24/2023,999LP999,GENERIC_LP_DPV,0,-25000,CAPC\nEntered,03/07/2024,888LP888,GENERIC_LP_DPV,0,-25000,CAPC\nEntered,04/30/2024,777LP777,GENERIC_LP_DPV,0,25000,RCAP\nEntered,05/22/2024,555LP555,GENERIC_LP_DPV,0,25000,RCAP\n
</code></pre>
","huggingface"
"77214024","What is the best strategy to download most used huggingface models in container and load them when necessary?","2023-10-02 08:06:42","","0","134","<containers><artificial-intelligence><huggingface>","<p>I’m writing this question to know if there is any better strategy than the one I’m thinking.</p>
<p>So, I have a service that should be able to run different huggingface models (for sentence embedding purpose). This service will be containerised (probably in docker) with a load balancer dealing with spawning more or less instances of it.</p>
<p>The service, as I said will have in it multiple models from the hub, and, to simplify, will receive some text to work on and the name of the model to use for it, in the 95% of the cases this model is one out of 5 models we already know, in the other 5% it’s something else:</p>
<p>My idea would be:</p>
<p>Initialise a dict with the 5 models:</p>
<pre><code>for model_name in list_of_most_used_models:
     tokenizer_dict[model_name] = AutoTokenizer.from_pretrained(model_name)
     model_dict[model_name] = AutoModel.from_pretrained(model_name).to(self.device)
</code></pre>
<p>Then, everytime there is a new request with another model I do something like:</p>
<pre><code>if model_name in model_dict:
     model_dict[model_name].do_something()
else:
     model_dict[model_name] = ....
     
</code></pre>
<p>My idea was to create the docker with the 5 models predownloaded (so that when the instance of the container is created they are already there (and i do not have to download them from the hub). My question is, if I “predownload” them, can I still call them like:</p>
<pre><code>tokenizer = AutoTokenizer.from_pretrained(&quot;tiiuae/falcon-180b&quot;)
</code></pre>
<p>or do I have to do some operation to let AutoTokenizer know that he can use the local version? When I download them do I have to specify a particular directory?</p>
<p>More in general, is there a better way to do what I want or this should work quite well already?</p>
","huggingface"
"77213641","SentenceTransformer Error no file named pytorch_model.bin, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory","2023-10-02 06:43:49","","1","2562","<pytorch><huggingface-transformers><tensor><huggingface><sentence-transformers>","<p>I'm using google colab to try this and I download the model for sentenceTransformers from huggingface hub using snapshot_download. For the model I'm using all-MiniLM-L6-v2</p>
<p>When I load the model from the repo that I have downloaded to my drive in the same runtime, it works perfectly.</p>
<p>but when I restart the runtime, and load the model again using the same directory that I use before, error appears. And the error is &quot;Error no file named pytorch_model.bin, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory /content/drive/MyDrive/Generative_AI/NLP/all-MiniLM-L6-v2/.&quot;</p>
<p>what should i do?</p>
<pre><code>#This is how I download the model
from huggingface_hub import snapshot_download
snapshot_download(repo_id=&quot;sentence-transformers/all-MiniLM-L6-v2&quot;,
                   local_dir=&quot;/content/drive/MyDrive/Generative_AI/NLP/all-MiniLM-L6-v2&quot;)


#This how I load the model
from sentence_transformers import SentenceTransformer
model_dir = '/content/drive/MyDrive/Generative_AI/NLP/all-MiniLM-L6-v2'
model = SentenceTransformer(model_dir)
</code></pre>
<p>I tried to restart the runtime again and the error still appear. It turns out that I need to download and load the model in the same runtime, it means that i have to download every new runtime. But I want to download the model just once</p>
","huggingface"
"77207596","How to change the location where Langchain's C Transformer downloads the hugging face model on AWS sagemaker studio labs","2023-09-30 15:28:07","","1","638","<huggingface-transformers><langchain><huggingface><large-language-model><amazon-sagemaker-studio>","<p>I used the command :</p>
<pre><code>llm = CTransformers(model=&quot;TheBloke/Llama-2-7B-Chat-GGML&quot;, model_file = 'llama-2-7b-chat.ggmlv3.q2_K.bin', callbacks=[StreamingStdOutCallbackHandler()])
</code></pre>
<p>to download a LLM model , but I cant find where the model file has been saved. I'm trying to save the model in the directory of my script.</p>
","huggingface"
"77205123","How do I slim down SBERT's sentencer-transformer library?","2023-09-29 22:53:58","","11","4127","<python><pytorch><huggingface><large-language-model><sentence-transformers>","<p>SBERT's (<a href=""https://www.sbert.net/"" rel=""noreferrer"">https://www.sbert.net/</a>) <code>sentence-transformer</code> library (<a href=""https://pypi.org/project/sentence-transformers/"" rel=""noreferrer"">https://pypi.org/project/sentence-transformers/</a>) is the most popular library for producing vector embeddings of text chunks in the Python open source LLM ecosystem. It has a simple API but is a <strong>MASSIVELY large</strong> dependency. Where does all its bloat come from?</p>
<p>Below is a screenshot of building a base Docker container image with this tool which took over <code>11 mins</code> to build with a final image size of <code>7.5 GB</code>:</p>
<p><a href=""https://i.sstatic.net/zdmwD.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/zdmwD.png"" alt=""base docker image"" /></a></p>
<p>For reference, here is my <code>Dockerfile.base</code>:</p>
<pre><code>FROM python:3.11.2-slim-bullseye
RUN pip install --upgrade pip &amp;&amp; pip install sentence-transformers
</code></pre>
<p>I anticipated that this is because it is installed with some models already pre-packaged, but when I tried their popular getting started snippet</p>
<pre class=""lang-py prettyprint-override""><code>from sentence_transformers import SentenceTransformer
model = SentenceTransformer('all-MiniLM-L6-v2')
embeddings = model.encode(&quot;vectorize this text&quot;)
</code></pre>
<p>it downloaded another few hundred MBs of more files to my filesystem. So I'd like to find a way of slimming this down to just the packages I need. I believe the size of this library is largely due to the underlying <code>torch</code> dependencies (<code>6.9 GB</code>), which in turn takes up a lot disk space due to its underlying <code>nvidia-*</code> dependencies (to where are these installed btw?)</p>
<p>Let's suppose I already have a model I've downloaded to my file system (i.e. <code>path/to/all-MiniLM-L6-v2</code> repo from HuggingFace), and all I want to do is run the code above on just a CPU. How can I install only the things I need without the bloat?</p>
<p>Now let's suppose I've picked a GPU to run this on. What are the next set of marginal dependencies I need to get this code to run without the bloat?</p>
","huggingface"
"77204403","Does one need to load the model to GPU before calling train when using accelerate?","2023-09-29 19:30:38","","0","840","<huggingface-transformers><huggingface><huggingface-trainer>","<p>Do I need to call:</p>
<pre><code>        device = torch.device(f&quot;cuda:{0}&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
        model = model.to(device)
</code></pre>
<p>if I am using accelerate? e.g.,</p>
<pre><code>accelerate launch --config_file hf_training.py
</code></pre>
<p>I was worried about the <code>accelerator.prepare(...)</code> not working as expected.</p>
<p>related question: <a href=""https://stackoverflow.com/questions/76971761/how-to-adapt-llama-v2-model-to-less-than-7b-parameters"">How to adapt LLaMA v2 model to less than 7B parameters?</a></p>
<p>the full code of trainer HF: <a href=""https://github.com/huggingface/transformers/blob/v4.33.3/src/transformers/trainer.py#L846"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/blob/v4.33.3/src/transformers/trainer.py#L846</a></p>
<hr />
<h1>Bounty: Does one need to load the model to GPU before calling train when using accelerate?</h1>
<p>The specific issue I am confused is that I want to use normal training single GPU without accelerate and sometimes I do want to use HF + accelerate. In that case is it safe to set the device anyway and then accelerate in HF's trainer will make sure the actual right GPU is set? (I am doing a single server multiple gpus)</p>
<hr />
<ul>
<li>related HF discord: <a href=""https://discord.com/channels/879548962464493619/1169012020721504347/1169012020721504347"" rel=""nofollow noreferrer"">https://discord.com/channels/879548962464493619/1169012020721504347/1169012020721504347</a></li>
<li>cross: <a href=""https://discuss.huggingface.co/t/does-one-need-to-load-the-model-to-gpu-before-calling-train-when-using-accelerate/56852"" rel=""nofollow noreferrer"">https://discuss.huggingface.co/t/does-one-need-to-load-the-model-to-gpu-before-calling-train-when-using-accelerate/56852</a></li>
</ul>
","huggingface"
"77201161","Adding ID to Text Output in AWS Batch Transform Job with DistilBERT Model","2023-09-29 10:20:08","","0","43","<amazon-web-services><huggingface-transformers><amazon-sagemaker><huggingface><huggingface-hub>","<p>I have a dataset in JSON format with ‘id’ and ‘text’ columns. Currently, I’m using the following pipeline configuration in AWS:</p>
<pre><code>hub = {
    'HF_MODEL_ID':'distilbert-base-uncased',
    'HF_TASK':'feature-extraction'
}
# create Hugging Face Model Class
huggingface_model = HuggingFaceModel(
   env=hub,                                                # configuration for loading model from Hub
   role=role,                                              # IAM role with permissions to create an endpoint
   transformers_version=&quot;4.26&quot;,                             # Transformers version used
   pytorch_version=&quot;1.13&quot;,                                  # PyTorch version used
   py_version='py39',                                      # Python version used
)
# create Transformer to run our batch job
batch_job = huggingface_model.transformer(
    
    instance_count=1,
    instance_type='ml.m5.xlarge',
    output_path=output_s3_path, # we are using the same s3 path to save the output with the input
    strategy='SingleRecord')
</code></pre>
<p>I’m using a batch transform job to generate the output, which currently contains only the extracted text. However, I also want to include the ‘id’ associated with each text in the output file. Is there a way to achieve this, and if so, how can I modify my configuration to include the ‘id’ in the output file? Any guidance or examples would be greatly appreciated!</p>
","huggingface"
"77197723","How do I stream HuggingFacePipeline output to a LangChain Dataframe Agent?","2023-09-28 19:23:11","","2","1672","<python><huggingface-transformers><streamlit><langchain><huggingface>","<p>I'm trying to mimic the LangChain Agent + Streamlit demo outlined <a href=""https://python.langchain.com/docs/integrations/callbacks/streamlit"" rel=""nofollow noreferrer"">in this documentation</a>, except with a local HuggingFace model using the <code>HuggingFacePipeline</code> and Langchain Dataframe Agent.</p>
<p>I am very close to matching the original functionality, save for one thing: I cannot figure out how to stream the model's thoughts and actions. My Streamlit page stops at <code>Thinking...</code> until the model completes its inference, after which the result of that Agent-Model cycle appears and the next cycle begins.</p>
<p>I tried using <code>TextStreamer</code> and <code>TextIteratorStreamer</code>, but they don't seem to work correctly with the Agent. I also tried to put the <code>StreamingStdOutCallbackHandler()</code> callback into the <code>HuggingFacePipeline</code> which had no effect.</p>
<p>Here is the relevant code:</p>
<pre class=""lang-py prettyprint-override""><code>@st.cache_resource
def getLangChainPipeline(model_config: dict):
    # model_config simply contains the name, path, etc of the HF model

    # Loading the model and tokenizer
    USE_CUSTOM_MODEL = True
    CUSTOM_MODEL_PATH = model_config['path']

    MODEL_PATH = (
        CUSTOM_MODEL_PATH if USE_CUSTOM_MODEL else model_config['name']
    )

    extra_args = {}
    if cuda_available:
        extra_args['device_map'] = 'auto'

    tokenizer = AutoTokenizer.from_pretrained(model_config['name'], trust_remote_code=model_config['trust_remote_code'])
    model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, 
                                                torch_dtype=torch.float16, 
                                                trust_remote_code=model_config['trust_remote_code'],
                                                **extra_args)
    
    # for inference
    model.eval()

    # tried using TextStreamer but that didn't work
    streamer = TextStreamer(tokenizer, skip_prompt=True)

    pipe = pipeline(
        &quot;text-generation&quot;, 
        model=model, 
        tokenizer=tokenizer, 
        # streamer=streamer,
        max_new_tokens=200, 
        trust_remote_code=model_config['trust_remote_code'],
    )

    # tried using callbacks=[StreamingStdOutCallbackHandler()] below which didn't work either
    return HuggingFacePipeline(pipeline=pipe)

# load pipeline
with st.spinner('Loading model...'):
    hf = getLangChainPipeline(codellama_13b_instruct)

# create agent
agent = create_pandas_dataframe_agent(hf, df, verbose=True)

# form for using to type prompt
with st.form(key=&quot;form&quot;):
    &quot;Working with 50000 rows of sample data.&quot;
    user_input = st.text_input(&quot;Query&quot;)
    submit_clicked = st.form_submit_button(&quot;Submit Question&quot;)

# run Agent when submit is clicked
if submit_clicked:
    with st.empty():
        st.chat_message(&quot;user&quot;).write(user_input)
        with st.chat_message(&quot;assistant&quot;):
            st_callback = StreamlitCallbackHandler(st.container())
            response = agent.run(user_input, callbacks=[st_callback])
            st.info(response)
</code></pre>
<p>I would appreciate any help figuring this out.</p>
","huggingface"
"77195966","Incorrect output for sentence_transformers CrossEncoder","2023-09-28 14:36:01","","0","117","<huggingface-transformers><huggingface><large-language-model>","<p>I am looking at the following example from the huggingface website (<a href=""https://www.sbert.net/examples/applications/cross-encoder/README.html"" rel=""nofollow noreferrer"">https://www.sbert.net/examples/applications/cross-encoder/README.html</a>)</p>
<pre><code>from sentence_transformers.cross_encoder import CrossEncoder

model = CrossEncoder(‘bert_base_uncased')

scores = model.predict([[&quot;My first&quot;, &quot;sentence pair&quot;], 

                    [&quot;Second text&quot;, &quot;pair&quot;]])
</code></pre>
<p>When I run this code it outputs the following.
<a href=""https://i.sstatic.net/NFfsb.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/NFfsb.png"" alt=""enter image description here"" /></a></p>
<p>But on the website it says the following.
“ In contrast, for a Cross-Encoder, we pass both sentences simultaneously to the Transformer network. It produces than an output value between 0 and 1 indicating the similarity of the input sentence pair:”</p>
<p>It seems as though each sentence pair is actually getting two values. Can someone explain what each of these means or why it doesn’t seem to line up with the expected output?</p>
","huggingface"
"77193088","How to perform inference with a Llava Llama model deployed to SageMaker from Huggingface?","2023-09-28 07:48:32","","0","2432","<python><amazon-sagemaker><huggingface><huggingface-hub>","<p>I deployed a Llava Llama Huggingface model (<a href=""https://huggingface.co/liuhaotian/llava-llama-2-13b-chat-lightning-preview/discussions/3"" rel=""nofollow noreferrer"">https://huggingface.co/liuhaotian/llava-llama-2-13b-chat-lightning-preview/discussions/3</a>) to a SageMaker Domain + Endpoint by using the deployment card provided by Huggingface:</p>
<pre><code>import sagemaker
import boto3
from sagemaker.huggingface import HuggingFaceModel

try:
    role = sagemaker.get_execution_role()
except ValueError:
    iam = boto3.client('iam')
    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']

# Hub Model configuration. https://huggingface.co/models
hub = {
    'HF_MODEL_ID': 'liuhaotian/llava-llama-2-13b-chat-lightning-preview',
    'HF_TASK': 'text-generation'
}

# create Hugging Face Model Class
huggingface_model = HuggingFaceModel(
    transformers_version='4.26.0',
    pytorch_version='1.13.1',
    py_version='py39',
    env=hub,
    role=role, 
)

# deploy model to SageMaker Inference
predictor = huggingface_model.deploy(
    initial_instance_count=1, # number of instances
    instance_type='ml.m5.xlarge' # ec2 instance type
)
</code></pre>
<p>The deployment sets the <code>HF_TASK</code> as <code>text-generation</code>. Llava Llama however is a multi modal text + image model. So the big question is how I'd perform an inference / prediction. I'd need to pass the image and also the text prompt. Other image + text API such as Imagen or Chooch accepts a base64 encoded image data. I know I need to do more than that since for example the models are trained with a specific dimension dataset (I think the Llava Llama model it might be 336x336), and Imagen or Chooch as a PaaS service does the cropping / resizing / padding.</p>
<p>Llava Llama has a demo page <a href=""https://llava-vl.github.io/"" rel=""nofollow noreferrer"">https://llava-vl.github.io/</a> which uses Gradio user interface. So I cannot tell where and how is the model hosted. However we might be able to decipher the solution from the source code. This <code>get_image</code> function is I think important, it does the resize / crop / pad: <a href=""https://github.com/haotian-liu/LLaVA/blob/a4269fbf014af3cab1f1d172914493fae8b74820/llava/conversation.py#L109"" rel=""nofollow noreferrer"">https://github.com/haotian-liu/LLaVA/blob/a4269fbf014af3cab1f1d172914493fae8b74820/llava/conversation.py#L109</a> and that is invoked from <a href=""https://github.com/haotian-liu/LLaVA/blob/a4269fbf014af3cab1f1d172914493fae8b74820/llava/serve/gradio_web_server.py#L138"" rel=""nofollow noreferrer"">https://github.com/haotian-liu/LLaVA/blob/a4269fbf014af3cab1f1d172914493fae8b74820/llava/serve/gradio_web_server.py#L138</a></p>
<p>We can see that there will be some magic tokens which will mark the beginning and end of the image and separate the text prompt (<a href=""https://github.com/haotian-liu/LLaVA/blob/a4269fbf014af3cab1f1d172914493fae8b74820/llava/serve/gradio_web_server.py#L154"" rel=""nofollow noreferrer"">https://github.com/haotian-liu/LLaVA/blob/a4269fbf014af3cab1f1d172914493fae8b74820/llava/serve/gradio_web_server.py#L154</a>). We can see that the text prompt is truncated to 1536 tokens (?) for text to image generation mode and 1200 tokens for image QnA mode. A compound prompt is assembled with the help of these tokens (<a href=""https://github.com/haotian-liu/LLaVA/blob/a4269fbf014af3cab1f1d172914493fae8b74820/llava/conversation.py#L287"" rel=""nofollow noreferrer"">https://github.com/haotian-liu/LLaVA/blob/a4269fbf014af3cab1f1d172914493fae8b74820/llava/conversation.py#L287</a>) and templates (<a href=""https://github.com/haotian-liu/LLaVA/blob/a4269fbf014af3cab1f1d172914493fae8b74820/llava/conversation.py#L71"" rel=""nofollow noreferrer"">https://github.com/haotian-liu/LLaVA/blob/a4269fbf014af3cab1f1d172914493fae8b74820/llava/conversation.py#L71</a>). The image is also appended as a base64 string, in PNG format: <a href=""https://github.com/haotian-liu/LLaVA/blob/a4269fbf014af3cab1f1d172914493fae8b74820/llava/conversation.py#L154"" rel=""nofollow noreferrer"">https://github.com/haotian-liu/LLaVA/blob/a4269fbf014af3cab1f1d172914493fae8b74820/llava/conversation.py#L154</a></p>
<p>When I try to invoke the endpoint for a an inference / prediction</p>
<pre><code>from sagemaker.predictor import Predictor
from base64 import b64encode

endpoint = 'huggingface-pytorch-inference-2023-09-23-08-55-26-117'
ENCODING = &quot;utf-8&quot;
IMAGE_NAME = &quot;eiffel_tower_336.jpg&quot;

payload = {
    &quot;inputs&quot;: &quot;Describe the content of the image in great detail &quot;,
}
with open(IMAGE_NAME, 'rb') as f:
    byte_content = f.read()
    base64_bytes = b64encode(byte_content)
    base64_string = base64_bytes.decode(ENCODING)

predictor = Predictor(endpoint)
inference_response = predictor.predict(data=payload)
print (inference_response)
</code></pre>
<p>I get an error, that <code>ParamValidationError: Parameter validation failed: Invalid type for parameter Body, value: {'inputs': 'Describe the content of the image in great detail '}, type: &lt;class 'dict'&gt;, valid types: &lt;class 'bytes'&gt;, &lt;class 'bytearray'&gt;, file-like object</code></p>
<p>This HuggingFace discussion says <a href=""https://discuss.huggingface.co/t/can-text-to-image-models-be-deployed-to-a-sagemaker-endpoint/20120"" rel=""nofollow noreferrer"">https://discuss.huggingface.co/t/can-text-to-image-models-be-deployed-to-a-sagemaker-endpoint/20120</a> that an inference.py need to be created. I don't know what the Llava Llama has though. I tried to look at the files of the model, but I don't see relevant meta data about this.</p>
<p>This StackOverflow entry <a href=""https://stackoverflow.com/questions/76197446/how-to-do-model-inference-on-a-multimodal-model-from-hugginface-using-sagemaker"">How to do model inference on a multimodal model from hugginface using sagemaker</a> is about a serverless deployment case, but it uses a custom TextImageSerializer serializer. Should I try to use something like that?</p>
<p>This Reddittor suggests <a href=""https://www.reddit.com/r/LocalLLaMA/comments/16pzn88/how_to_parametrize_a_llava_llama_model/"" rel=""nofollow noreferrer"">https://www.reddit.com/r/LocalLLaMA/comments/16pzn88/how_to_parametrize_a_llava_llama_model/</a> some kind of a CLIP encoding. I'm uncertain if I really need to do that or the model is able to encode?</p>
<p>Other references:</p>
<ul>
<li>Asking at the model: <a href=""https://huggingface.co/liuhaotian/llava-llama-2-13b-chat-lightning-preview/discussions/3"" rel=""nofollow noreferrer"">https://huggingface.co/liuhaotian/llava-llama-2-13b-chat-lightning-preview/discussions/3</a></li>
<li>GitHub Discussion: <a href=""https://github.com/haotian-liu/LLaVA/discussions/454"" rel=""nofollow noreferrer"">https://github.com/haotian-liu/LLaVA/discussions/454</a></li>
<li>HuggingFace Discussion: <a href=""https://discuss.huggingface.co/t/how-to-use-llava-with-huggingface/52315"" rel=""nofollow noreferrer"">https://discuss.huggingface.co/t/how-to-use-llava-with-huggingface/52315</a></li>
<li>Reddit: <a href=""https://www.reddit.com/r/LocalLLaMA/comments/16pzn88/how_to_parametrize_a_llava_llama_model/"" rel=""nofollow noreferrer"">https://www.reddit.com/r/LocalLLaMA/comments/16pzn88/how_to_parametrize_a_llava_llama_model/</a></li>
</ul>
","huggingface"
"77190366","Retrieval augmented generation (RAG) for text classification","2023-09-27 19:25:49","","0","1843","<vector><huggingface-transformers><word-embedding><langchain><huggingface>","<p>I'm currently exploring the implementation of Retrieval-Augmented Generation (RAG) for text classification, but I'm facing a lack of comprehensive online resources to guide me through the process.</p>
<p>In this project, the task involves taking a sentence as input and categorizing it into one of three distinct classes. Rather than opting for fine-tuning, I'm keen on leveraging RAG. However, I'm grappling with the challenge of embedding test cases in a manner that allows for subsequent retrieval and classification by a large language model (LLM). I'm specifically looking for an embedding model that can create these embeddings while incorporating the associated class information for each sentence.</p>
","huggingface"
"77182108","HuggingFace text generation inference not working for starcoder model","2023-09-26 17:08:32","","0","732","<huggingface>","<p>I deployed the star coder model using the huggingface text generation inference container</p>
<p><code>docker run -p 8080:80 -v $PWD/data:/data -e HUGGING_FACE_HUB_TOKEN=&lt;YOUR BIGCODE ENABLED TOKEN&gt; -d  ghcr.io/huggingface/text-generation-inference:latest --model-id bigcode/starcoder --max-total-tokens 8192</code></p>
<p>After the docker container starts, the api end point does not work</p>
<p><code>curl 127.0.0.1:8080/generate \ -X POST \ -d '{&quot;inputs&quot;:&quot;What is Deep Learning?&quot;,&quot;parameters&quot;:{&quot;max_new_tokens&quot;:20}}' \ -H 'Content-Type: application/json'</code></p>
<p>The response is 'curl: (52) Empty reply from server'</p>
<p>I think the api server is not started in the container. Is there anything else we have to do to get the api server working in the container?</p>
<p>Reference: <a href=""https://github.com/bigcode-project/starcoder#installation"" rel=""nofollow noreferrer"">https://github.com/bigcode-project/starcoder#installation</a></p>
","huggingface"
"77177971","AttributeError: 'TextToVideoSDPipeline' object has no attribute '_offload_gpu_id'","2023-09-26 07:47:57","","0","210","<python><google-colaboratory><huggingface>","<p>I encountered an issue while running the following code in a Colab notebook:</p>
<pre><code>!pip install openai elevenlabs diffusers transformers accelerate pytorch-lightning git+https://github.com/huggingface/diffusers
from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler
pipe = DiffusionPipeline.from_pretrained(&quot;cerspense/zeroscope_v2_576w&quot;, torch_dtype=torch.float16)
pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)
pipe.enable_model_cpu_offload()
</code></pre>
<p>This code was functional yesterday, but today, I'm getting the error message:</p>
<pre><code>&quot;AttributeError: 'TextToVideoSDPipeline' object has no attribute '_offload_gpu_id'&quot;
</code></pre>
<p>I'm not sure why this error has started occurring, and I'm looking for insights or solutions to resolve it. Any help would be greatly appreciated.</p>
","huggingface"
"77174867","Langchain Routerchain (Retrieval-, Defaultchain) Problem with more then 1 input variable","2023-09-25 18:07:40","","0","708","<langchain><huggingface><large-language-model>","<p>I am currently trying to implement a router chain in Langchain with two &quot;sub-chains&quot; - retrieval chain and standard LLM chain.
I have encountered the problem that my retrieval chain has two inputs and the default chain has only one input.
Therefore, I started the following experimental setup.</p>
<pre><code>    class MultitypeDestRouteChain(MultiRouteChain) :
        &quot;&quot;&quot;A multi-route chain that uses an LLM router chain to choose amongst prompts.&quot;&quot;&quot;
    
        router_chain: RouterChain
        &quot;&quot;&quot;Chain for deciding a destination chain and the input to it.&quot;&quot;&quot;
        destination_chains: Mapping[str, Chain]
        &quot;&quot;&quot;Map of name to candidate chains that inputs can be routed to.&quot;&quot;&quot;
        default_chain: LLMChain
        &quot;&quot;&quot;Default chain to use when router doesn't map input to one of the destinations.&quot;&quot;&quot;
    
        @property
        def output_keys(self) -&gt; List[str]:
            return [&quot;text&quot;]
            
            
            
    DB_FAISS_PATH = 'vectorstores/db_faiss'
    
    def load_llm():
        llm = LlamaCpp(
                model_path=&quot;../models/openbuddy-llama2-13b-v11.1.Q4_K_M.gguf&quot;  ,
                max_tokens=2048,
                n_ctx= 4096,
                n_threads= 0,
                n_batch=512,
                top_p=0.7,
                top_k=20,
                repeat_penalty=1.15,
                temperature = 0.7,
                n_gpu_layers = 32,
            )
        return llm
    

template_general = &quot;&quot;&quot;SYSTEM: Example template


USER: {input}
ASSISTANT:
&quot;&quot;&quot;

    
    
    template_specific = &quot;&quot;&quot;Example template 
    
    Context: {context}
    Question: {question}
    
    &quot;&quot;&quot;
    
    def set_custom_prompt():
        &quot;&quot;&quot;
        Prompt template for QA retrieval for each vector stores
        &quot;&quot;&quot;
    
        prompt = PromptTemplate(template=template2, input_variables=['context', 'question'])
        
        return prompt
    
    
    #Retrieval QA Chain
    def retrieval_qa_chain(llm, prompt, db):
        qa_chain = RetrievalQA.from_chain_type(llm=llm,
                                           chain_type='stuff',
                                           retriever=db.as_retriever(search_kwargs={'k': 2}),
                                           return_source_documents=True,
                                           chain_type_kwargs={'prompt': prompt}
                                           )
        return qa_chain
    
    #QA Model Function
    def qa_bot():
        embeddings = HuggingFaceEmbeddings(model_name=&quot;sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2&quot;,
                                           model_kwargs={'device': 'cpu'})
        db = FAISS.load_local(DB_FAISS_PATH, embeddings)
        llm = load_llm()
        qa_prompt = set_custom_prompt()
        qa = retrieval_qa_chain(llm, qa_prompt, db)
    
        return qa
    
    #output function
    def final_result(query):
        qa_result = qa_bot()
        response = qa_result({'query': query})
        return response
    
    prompt_infos = [
        {
            &quot;name&quot;: &quot;llm_chain_general&quot;,
            &quot;description&quot;: &quot;Good for answering questions about general knwoledge&quot;,
            &quot;prompt_template&quot;: template_general,
        },
        {
            &quot;name&quot;: &quot;llm_chain_specific&quot;,
            &quot;description&quot;: &quot;Good for answering special questions local data &quot;,
            &quot;prompt_template&quot;: template_specific,
        },
    ]
    
    destination_chains = {}
    
    prompt_general = PromptTemplate(template=template_general, input_variables=[&quot;input&quot;])
    llm_chain_general = LLMChain(prompt=prompt_general, llm=llm)
    
    
    destination_chains[&quot;llm_chain_general&quot;] = llm_chain_general
    destination_chains[&quot;llm_chain_specific&quot;] = llm_chain_specific
    
    
    destinations = [f&quot;{p['name']}: {p['description']}&quot; for p in prompt_infos]
    destinations_str = &quot;\n&quot;.join(destinations)
    
    router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destinations_str)
    router_prompt = PromptTemplate(
        template=router_template,
        input_variables=['input'],
        output_parser=RouterOutputParser(),
    )
    router_chain = LLMRouterChain.from_llm(llm, router_prompt)
    default_chain = ConversationChain(llm=llm, output_key=&quot;text&quot;)
    
    chain = MultitypeDestRouteChain (
        router_chain=router_chain,
        destination_chains=destination_chains,
        default_chain=default_chain,
        verbose=True,
    )
    
result = chain(&quot;This would be a question for example chain?&quot;)   #This is not working i get an error
result = chain(&quot;This would be a question for general chain?&quot;)   #This is working
result = llm_chain_specific(&quot;This would be a question for example chain?&quot;) #This is working
result = llm_chain_general(&quot;This would be a question for general chain?&quot;) #This is working
    
    
    print(result['text'])

  
</code></pre>
<p>I get the following error message:</p>
<pre><code>    Entering new MultitypeDestRouteChain chain...
    /home/zuiwer/miniconda3/envs/chainlit/lib/python3.11/site-packages/langchain/chains/llm.py:280: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.
      warnings.warn(
    /home/zuiwer/miniconda3/envs/chainlit/lib/python3.11/site-packages/transformers/generation/utils.py:1417: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )
      warnings.warn(
    llm_chain_specific: {'input': 'This would be a question for example chain?'}
Traceback (most recent call last):
      File &quot;/home/zuiwer/miniconda3/envs/chainlit/bin/chainlit&quot;, line 8, in &lt;module&gt;
        sys.exit(cli())
                 ^^^^^
      File &quot;/home/zuiwer/miniconda3/envs/chainlit/lib/python3.11/site-packages/click/core.py&quot;, line 1128, in __call__
        return this.main(*args, **kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
      File &quot;/home/zuiwer/miniconda3/envs/chainlit/lib/python3.11/site-packages/click/core.py&quot;, line 1053, in main
        rv = this.invoke(ctx)
             ^^^^^^^^^^^^^^^^
      File &quot;/home/zuiwer/miniconda3/envs/chainlit/lib/python3.11/site-packages/click/core.py&quot;, line 1659, in invoke
        return _process_result(sub_ctx.command.invoke(sub_ctx))
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      File &quot;/home/zuiwer/miniconda3/envs/chainlit/lib/python3.11/site-packages/click/core.py&quot;, line 1395, in invoke
        return ctx.invoke(this.callback, **ctx.params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      File &quot;/home/zuiwer/miniconda3/envs/chainlit/lib/python3.11/site-packages/click/core.py&quot;, line 754, in invoke
        return __callback(*args, **kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
      File &quot;/home/zuiwer/miniconda3/envs/chainlit/lib/python3.11/site-packages/chainlit/cli/__init__.py&quot;, line 152, in chainlit_run
        run_chainlit(target)
      File &quot;/home/zuiwer/miniconda3/envs/chainlit/lib/python3.11/site-packages/chainlit/cli/__init__.py&quot;, line 45, in run_chainlit
        load_module(config.run.module_name)
      File &quot;/home/zuiwer/miniconda3/envs/chainlit/lib/python3.11/site-packages/chainlit/config.py&quot;, line 247, in load_module
        spec.loader.exec_module(module)
      File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 940, in exec_module
      File &quot;&lt;frozen importlib
</code></pre>
<p>General Chain and specific chains are working fine.
And also general chain in RuterChain is working fine.
But Retrieval Chain has a problem probably because of 2 inputs.
If somebody would have a solution, i would be really gratefull.</p>
","huggingface"
"77173026","Perform sentiment analysis using pertained huggingFace model","2023-09-25 13:21:10","","0","96","<python><nlp><huggingface-transformers><huggingface><huggingface-tokenizers>","<p>I am using <a href=""https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment"" rel=""nofollow noreferrer"">this hugging face model</a> to perform sentiment analysis, this model can handle the text with max tokens 514, What is the best practice to make it work when the tokens greater that 514? i.e long text without truncation, I used truncation, but the problem with truncation it takes only a part from the text not the whole text</p>
<p>I know some inputs have been provided for this questions like <a href=""https://stackoverflow.com/questions/58636587/how-to-use-bert-for-long-text-classification"">here</a> , but it does not fit my application</p>
","huggingface"
"77170944","Which parallelism technique is used in hugging face accelerate by default if we don't specify any technique in accelerate config but use multiple gpus","2023-09-25 08:16:15","","0","62","<python><nlp><transformer-model><huggingface><accelerate>","<p>Which parallelism technique is used in hugging face accelerate by default if we don't specify any technique in accelerate config but use multiple gpus??</p>
<p>Only want to know the default technique used is it data parallel or distributed data parallel</p>
<pre><code>Model = accelerate.prepare(model)
Model = accelerate.device()
</code></pre>
","huggingface"
"77167055","Running falcon ai model on mojo lang","2023-09-24 12:21:44","","0","443","<python><artificial-intelligence><huggingface-transformers><huggingface><mojolang>","<p>How can we run <a href=""https://huggingface.co/tiiuae/falcon-180B"" rel=""nofollow noreferrer"">https://huggingface.co/tiiuae/falcon-180B</a> the one of the current best models on mojo lang which is faster than python 35k? When we just copy this:</p>
<pre><code>`from transformers import AutoTokenizer, AutoModelForCausalLM
import transformers
import torch

model = &quot;tiiuae/falcon-180b&quot;

tokenizer = AutoTokenizer.from_pretrained(model)
pipeline = transformers.pipeline(
    &quot;text-generation&quot;,
    model=model,
    tokenizer=tokenizer,
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    device_map=&quot;auto&quot;,
)
sequences = pipeline(
   &quot;Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\nDaniel: Hello, Girafatron!\nGirafatron:&quot;,
    max_length=200,
    do_sample=True,
    top_k=10,
    num_return_sequences=1,
    eos_token_id=tokenizer.eos_token_id,
)
for seq in sequences:
    print(f&quot;Result: {seq['generated_text']}&quot;)
</code></pre>
<p>`</p>
<p>we got error as below:</p>
<p>`</p>
<pre><code>error: TODO: expressions are not yet supported at the file scope level
model = &quot;tiiuae/falcon-180b&quot;
^
hello.mojo:7:1: error: use of unknown declaration 'model'
model = &quot;tiiuae/falcon-180b&quot;
^~~~~
hello.mojo:9:1: error: TODO: expressions are not yet supported at the file scope level
tokenizer = AutoTokenizer.from_pretrained(model)
^
hello.mojo:3:6: error: unable to locate module 'transformers'
from transformers import AutoTokenizer, AutoModelForCausalLM
     ^
hello.mojo:10:1: error: TODO: expressions are not yet supported at the file scope level
pipeline = transformers.pipeline(
^
hello.mojo:18:1: error: TODO: expressions are not yet supported at the file scope level
sequences = pipeline(
^
hello.mojo:18:13: error: use of unknown declaration 'pipeline'
sequences = pipeline(
            ^~~~~~~~
hello.mojo:26:12: error: use of unknown declaration 'sequences'
for seq in sequences:
           ^~~~~~~~~
hello.mojo:27:5: error: TODO: expressions are not yet supported at the file scope level
    print(f&quot;Result: {seq['generated_text']}&quot;)
    ^
hello.mojo:27:12: error: expected ')' in call argument list
    print(f&quot;Result: {seq['generated_text']}&quot;)
           ^
hello.mojo:27:12: error: statements must start at the beginning of a line
    print(f&quot;Result: {seq['generated_text']}&quot;)
           ^
mojo: error: failed to parse the provided Mojo
</code></pre>
<p>`</p>
<p>When the code from hugging face is copied and pasted in mojo environment, model should be running.</p>
","huggingface"
"77164963","How to Merge Fine-tuned Adapter and Pretrained Model in Hugging Face Transformers and Push to Hub?","2023-09-23 21:20:21","","6","3540","<python><huggingface-transformers><huggingface><llama><huggingface-hub>","<p>I have fine-tuned the Llama-2 model following the <a href=""https://github.com/facebookresearch/llama-recipes"" rel=""nofollow noreferrer"">llama-recipes</a> repository's <a href=""https://github.com/facebookresearch/llama-recipes/blob/main/examples/quickstart.ipynb"" rel=""nofollow noreferrer"">tutorial</a>. Currently, I have the pretrained model and fine-tuned adapter stored in two separate directories as follows:</p>
<p>Pretrained Model Directory:</p>
<pre><code>Llama2-Finetuning/models_hf/
└── 7B
    ├── config.json
    ├── generation_config.json
    ├── pytorch_model-00001-of-00002.bin
    ├── pytorch_model-00002-of-00002.bin
    ├── pytorch_model.bin.index.json
    ├── special_tokens_map.json
    ├── tokenizer.json
    ├── tokenizer.model
    └── tokenizer_config.json
</code></pre>
<p>Fine-tuned Adapter Directory:</p>
<pre><code>Llama2-Finetuning/tmp/llama-output/
├── README.md
├── adapter_config.json
├── adapter_model.bin
└── logs
</code></pre>
<p>I want to achieve two things:</p>
<ol>
<li><strong>Merge Pretrained Model and Adapter as a Single File</strong>:
I have observed that when I push the model to the Hugging Face Hub using <code>model.push_to_hub(&quot;myrepo/llama-2-7B-ft-summarization&quot;)</code>, it only pushes the adapter weights. How can I merge the pretrained model and fine-tuned adapter into a single file, similar to how &quot;TheBloke&quot; does, and upload them together to the Hugging Face Hub?</li>
<li><strong>Push and Load Pretrained Model and Adapter Separately</strong>:
Alternatively, I'd like to know how to push the pretrained model and fine-tuned adapter from their respective directories separately to the Hub, while still being able to load them together in my Python code for inference, just like how I loaded them from directories using the code below:</li>
</ol>
<pre><code>import torch
from transformers import LlamaForCausalLM, LlamaTokenizer

model_id=&quot;./models_hf/7B&quot;
adapter_id=&quot;./tmp/llama-output&quot;

tokenizer = LlamaTokenizer.from_pretrained(model_id)
model = LlamaForCausalLM.from_pretrained(model_id, load_in_8bit=True, device_map='auto', torch_dtype=torch.float16)
model.load_adapter(adapter_id)
</code></pre>
<p>I appreciate any guidance on how to achieve these two objectives efficiently. Thank you!</p>
","huggingface"
"77164541","How can I programmatically remove all the datasets that HuggingFace has saved on the disk?","2023-09-23 19:06:34","","0","898","<python><download><huggingface><huggingface-datasets>","<p>I have downloaded a HuggingFace dataset (<a href=""https://huggingface.co/datasets/uonlp/CulturaX"" rel=""nofollow noreferrer""><code>uonlp/CulturaX</code></a>) with the following Python code:</p>
<pre><code>from datasets import load_dataset
ds = load_dataset(&quot;uonlp/CulturaX&quot;, &quot;ar&quot;)
</code></pre>
<p>It downloaded all the .parquet files of the HuggingFace dataset and also generated the .arrow files after the download was completed:</p>
<p><a href=""https://i.sstatic.net/bgp3u.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bgp3u.png"" alt=""enter image description here"" /></a></p>
<ul>
<li><code>downloads</code> contains the .parquet files (original dataset data files and format).</li>
<li><code>uonlp___cultura_x</code> contains the .arrow files.</li>
</ul>
<p>How can I programmatically remove all the datasets that HuggingFace has saved on the disk? I want to remove both the generated .arrow files and the original dataset data files. Is there some Python function for that?</p>
","huggingface"
"77163693","How to solve the os error and load a model from hugging face in my jupyter. (Not local, Its on cluster)","2023-09-23 15:24:36","","0","335","<jupyter-notebook><huggingface-transformers><huggingface><oserror>","<p>I am trying to load a pre-trained model from hugging face but everytime I am getting a os error.</p>
<p><a href=""https://i.sstatic.net/M69CP.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/M69CP.jpg"" alt=""enter image description here"" /></a></p>
<p>I tried with</p>
<pre><code>snapshot_download(repo_id = &quot;google/flan-t5-base&quot;, cache_dir= &quot;./huggingface_mirror&quot;)
</code></pre>
<p>but getting error as</p>
<pre><code>ConnectionError: (MaxRetryError(&quot;HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/models/google/flan-t5-base/revision/main (Caused by NewConnectionError('&lt;urllib3.connection.HTTPSConnection object at 0x2af561da8190&gt;: Failed to establish a new connection: [Errno -2] Name or service not known'))&quot;), '(Request ID: 179fa5d0-e590-476f-87a1-77cbd1e5d5f3)')
</code></pre>
","huggingface"
"77161173","How can I multithreadedly download a HuggingFace dataset?","2023-09-22 23:59:18","77175363","0","909","<python><multithreading><download><huggingface><huggingface-datasets>","<p>I want to download a HuggingFace dataset, e.g. <a href=""https://huggingface.co/datasets/uonlp/CulturaX"" rel=""nofollow noreferrer""><code>uonlp/CulturaX</code></a>:</p>
<pre><code>from datasets import load_dataset
ds = load_dataset(&quot;uonlp/CulturaX&quot;, &quot;en&quot;)
</code></pre>
<p>However, it downloads on one thread at 50 MB/s, while my network is 10 Gbps. Since this dataset is 16 TB, I'd prefer to download it faster so that I don't have to wait for a few days. How can I multithreadedly download a HuggingFace dataset?</p>
","huggingface"
"77156906","How to make Starchat Beta to generate code/chat instead of line completions","2023-09-22 10:40:57","","0","89","<huggingface-transformers><amazon-sagemaker><huggingface>","<p>I deployed StarChat Beta to SageMaker using the script I am not able to get StarChat to output code. It only does some autocomplete.
But, as per <a href=""https://huggingface.co/HuggingFaceH4/starchat-beta"" rel=""nofollow noreferrer"">text</a> - StarChat is a series of language models that are trained to act as helpful coding assistants.
Deployment and inference script from Huggingface:</p>
<pre><code>import json
import sagemaker
import boto3
from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri

try:
    role = sagemaker.get_execution_role()
except ValueError:
    iam = boto3.client('iam')
    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']

# Hub Model configuration. https://huggingface.co/models
hub = {
    'HF_MODEL_ID':'HuggingFaceH4/starchat-beta',
    'SM_NUM_GPUS': json.dumps(4)
}



# create Hugging Face Model Class
huggingface_model = HuggingFaceModel(
    image_uri=get_huggingface_llm_image_uri(&quot;huggingface&quot;,version=&quot;1.0.3&quot;),
    env=hub,
    role=role, 
)

# deploy model to SageMaker Inference
predictor = huggingface_model.deploy(
    initial_instance_count=1,
    instance_type=&quot;ml.g5.12xlarge&quot;,
    container_startup_health_check_timeout=300,
  )
  
# send request
predictor.predict({
    &quot;inputs&quot;: &quot;How can I write a Python function to generate the nth Fibonacci number?&quot;,
})
</code></pre>
<p>This outputs:</p>
<pre><code>[{'generated_text': &quot;How can I write a Python function to generate the nth Fibonacci number?&lt;|end|&gt;\n&lt;|assistant|&gt;\nHere's an example of a Python function that generates the nth Fibonacci number&quot;}]
</code></pre>
<p>As we can see the output indicates that the model is not doing code recommendation/interactive chat, but it is doing line completion.
And I verified this when I ran:</p>
<pre><code>predictor.predict(
{
&quot;inputs&quot;: &quot;def sum(a,b):&quot;
})
</code></pre>
<p>It returns</p>
<pre><code>[
{
&quot;generated_text&quot;: &quot;def sum(a,b):\n    return a+b\n\ndef diff(a,b):\n    return a-b\n&quot;
}
</code></pre>
<p>Is there any parameter that I should add something I need to change in order for StarChat to give complete code and act as a chat assistant instead of give line completions?</p>
","huggingface"
"77154308","Hugging Face Deep RL Course: Huggy Environment on M1 Mac","2023-09-22 00:35:37","","1","83","<unity-game-engine><apple-m1><reinforcement-learning><huggingface>","<p>I am starting the Hugging Face Deep Reinforcement Learning Course. I am following this <a href=""https://huggingface.co/learn/deep-rl-course/unitbonus1/train"" rel=""nofollow noreferrer"">tutorial</a> locally on my M1 Mac instead of in Google Colab. I am able to do all of the steps until the training step:</p>
<blockquote>
<p>mlagents-learn ./config/ppo/Huggy.yaml --env=./trained-envs-executables/linux/Huggy/Huggy --run-id=&quot;Huggy&quot; --no-graphics</p>
</blockquote>
<p>I get the following error:</p>
<blockquote>
<p>mlagents_envs.exception.UnityEnvironmentException: Couldn't launch the ./trained-envs-executables/linux/Huggy/Huggy environment. Provided filename does not match any environments.</p>
</blockquote>
<p>In my trained-envs-executables/linux/Huggy folder, I have a Huggy.x86_64 file. I tried setting the env to that and just the Huggy folder (not Huggy twice) but neither worked.</p>
<p>I downloaded the env from <a href=""https://drive.google.com/uc?export=download&amp;id=1zv3M95ZJTWHUVOWT6ckq_cm98nft8gdF"" rel=""nofollow noreferrer"">here</a>. Is this the correct environment to use or am I calling the environment incorrectly? I also tried running from the Rosetta terminal but no luck.</p>
","huggingface"
"77152888","HuggingFacePipeline and Langchain","2023-09-21 18:46:02","77162814","1","2477","<nlp><langchain><huggingface>","<p>this is my current code:</p>
<pre><code>from langchain.llms import HuggingFacePipeline
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig
from langchain import PromptTemplate, LLMChain
import torch

model_id  = &quot;../models/openbuddy-llama2-34b-v11.1-bf16&quot;  
tokenizer = AutoTokenizer.from_pretrained(model_id)
nf4_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_quant_type='nf4',
    bnb_4bit_use_double_quant=False,
    max_memory=24000  
)
model =  AutoModelForCausalLM.from_pretrained(
        model_id,
        quantization_config=nf4_config,
        )
pipe = pipeline(
    &quot;text-generation&quot;, model=model, tokenizer=tokenizer, max_new_tokens=100, eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.eos_token_id,
)
hf = HuggingFacePipeline(pipeline=pipe)

template = &quot;&quot;&quot;SYSTEM: You are a helpful, respectful and honest INTP-T AI Assistant named Buddy. You are talking to a human User.
Always answer as helpfully and logically as possible, while being safe. Your answers should not include any harmful, political, religious, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.
If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.
You like to use emojis. You can speak fluently in many languages, for example: English, Chinese.
You cannot access the internet, but you have vast knowledge, cutoff: 2021-09.
You are trained by OpenBuddy team, (https://openbuddy.ai, https://github.com/OpenBuddy/OpenBuddy), you are based on LLaMA and Falcon transformers model, not related to GPT or OpenAI.
USER: {question}
ASSISTANT:
&quot;&quot;&quot;

prompt = PromptTemplate(template=template, input_variables=[&quot;question&quot;])
llm_chain = LLMChain(prompt=prompt, llm=hf)

print(llm_chain.run(&quot;Who is the Pope ?&quot;))
</code></pre>
<p>This is not putting out something.
If i change the last row into:</p>
<pre><code>print(hf(&quot;Who is the Pope ?&quot;))
</code></pre>
<p>Everything is working fine, but i need to use a chain.</p>
<p>Im running on windows wsl ubuntu.<code>enter code here</code></p>
","huggingface"
"77152690","SageMaker inference endpoint with HuggingFaceModel ignores custom inference.py script","2023-09-21 18:16:17","","2","620","<amazon-sagemaker><huggingface><inference><large-language-model><llama>","<p>I'm trying do deploy a <a href=""https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html#hugging-face-model"" rel=""nofollow noreferrer"">HuggingFaceModel</a> using sagemaker inference endpoint. I've been following some guides, e.g.: <a href=""https://medium.com/innovation-res/inference-your-own-nlp-trained-model-on-aws-sagemaker-with-pytorchmodel-or-huggingefacemodel-30bcbdc4348b"" rel=""nofollow noreferrer"">this one</a> and <a href=""https://www.philschmid.de/sagemaker-llama-llm"" rel=""nofollow noreferrer"">this</a>. My model of choice is Llama-2 fine-tuned on my own data.
I've packed it and created a model.tar.gz which contains the following structure:</p>
<pre><code>model.tar.gz/
├── config.json
├── generation_config.json
├── tokenizer.json
├── pytorch_model-00001-of-00003.bin
├── ... (other model files)
└── code/
  ├── inference.py
  └── requirements.txt
</code></pre>
<p>My <code>inference.py</code> script defines the functions  <code>model_fn</code> and <code>output_fn</code>, with custom model loading and output parsing logic.</p>
<p>I've uploaded this model.tar.gz to the S3 bucket in <code>model_s3_path</code>.</p>
<p>During the sagemaker endpoint creation, I define my HuggingFaceModel as follows:</p>
<pre><code>from sagemaker.huggingface import get_huggingface_llm_image_uri
from sagemaker.huggingface import HuggingFaceModel

llm_image = get_huggingface_llm_image_uri(
  &quot;huggingface&quot;,
  version=&quot;0.9.3&quot;
)

huggingface_model = HuggingFaceModel(
    model_data=model_s3_path,
    role=aws_role, 
    image_uri=llm_image,
    env={
      'HF_MODEL_ID': 'meta-llama/Llama-2-7b-hf',
      'SM_NUM_GPUS': '1',
      'MAX_INPUT_LENGTH': '2048',
      'MAX_TOTAL_TOKENS': '4096',
      'MAX_BATCH_TOTAL_TOKENS': '8192', 
      'HUGGING_FACE_HUB_TOKEN': &quot;&lt;my-hf-token&gt;&quot;
    }
)
</code></pre>
<p>And then I deploy the model:</p>
<pre><code>huggingface_model.deploy(
    initial_instance_count=1,
    instance_type=inference_instance_type,
    endpoint_name=endpoint_name,
    container_startup_health_check_timeout=300
)
</code></pre>
<p>However, during the inference, the resulting endpoint model doesn't seem to use any of the functionality from <code>inference.py</code>, but rather sticks to all default methods. For instance, it still returns response as <code>[{&quot;generated_texts&quot;: model_response}]</code> although my post-processing function (<code>output_fn</code>) should've changed the return type.</p>
<ol>
<li>I've tried setting <code>entry_point=&quot;inference.py&quot;</code> and <code>source_dir=&quot;./code&quot;</code> during the HF model creation - the endpoint was not deploying at all.</li>
<li>Used env variable <code>&quot;SAGEMAKER_PROGRAM&quot;: &quot;inference.py&quot;</code> - did not change the model's responses, functionality from <code>inference.py</code> still was ignored.</li>
<li>Tried various <code>image_uri</code> - did not change the endpoint's behaviour.</li>
</ol>
","huggingface"
"77152606","How can I only download the .parquet files of a HuggingFace dataset without generating the .arrow files once the download is completed?","2023-09-21 18:00:17","","0","732","<python><download><parquet><huggingface><huggingface-datasets>","<p>I want to download all the .parquet files of HuggingFace dataset, e.g. <a href=""https://huggingface.co/datasets/uonlp/CulturaX"" rel=""nofollow noreferrer""><code>uonlp/CulturaX</code></a>, without generating the .arrow files once the download is completed.</p>
<p>If I use:</p>
<pre><code>from datasets import load_dataset
ds = load_dataset(&quot;uonlp/CulturaX&quot;, &quot;ar&quot;)
</code></pre>
<p>It will download all the .parquet files of HuggingFace dataset but it will also generate the .arrow files once the download is completed:</p>
<p><a href=""https://i.sstatic.net/bgp3u.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bgp3u.png"" alt=""enter image description here"" /></a></p>
<ul>
<li><code>downloads</code> contains the .parquet files.</li>
<li><code>uonlp___cultura_x</code> contains the .arrow files.</li>
</ul>
<p>How can I only download the .parquet files of a HuggingFace dataset without generating the .arrow files once the download is completed?</p>
","huggingface"
"77146794","Locally ran Python StableDiffusionXL outputting noisy image","2023-09-21 02:00:59","77146890","0","819","<python><pytorch><pipeline><huggingface><stable-diffusion>","<p>I am running StableDiffusionXLPipeline from the diffusers python package and am getting an output that is a png full of colorful noise, the png has dimensions 128x128.</p>
<p>The SDXL model I am referencing is pulled directly from HuggingFace with the goal of running locally. I am expecting to receive a picture of my prompt, which in this case is &quot;A majestic Trex overlooking a jungle&quot;.</p>
<p>The command I used within the &quot;stable_diffusion&quot; folder to download the SDXL model is as follows:</p>
<p><code>git lfs clone https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0  </code></p>
<p>My code is as follows:</p>
<pre><code>import torch
from diffusers import StableDiffusionXLPipeline
import torchvision.transforms as transforms

def main():
    print(f&quot;CUDA: {torch.cuda.is_available()}&quot;)

    torch.cuda.empty_cache()
    base_directory = &quot;stable_diffusion/stable-diffusion-xl-base-1.0&quot;

    # Load base model
    base = StableDiffusionXLPipeline.from_pretrained(
        pretrained_model_name_or_path=base_directory,
        # pretrained_model_or_path=base_directory,  #for AutoPipeline
        torch_dtype=torch.float16,
        variant=&quot;fp16&quot;,
        use_safetensors=True,
        local_files_only=True,
        cache_dir=&quot;stable_diffusion&quot;,
    )
    base.enable_model_cpu_offload()
    base.enable_xformers_memory_efficient_attention()
    base.enable_vae_slicing()

    # Parameters
    n_steps = 15
    high_noise_frac = 0.8
    prompt = &quot;A majestic Trex overlooking a jungle&quot;

    # Generate base image
    image = base(
        prompt=prompt,
        num_inference_steps=n_steps,
        denoising_end=high_noise_frac,
        output_type=&quot;latent&quot;,
    ).images[0]

    # Clear GPU cache (again)
    torch.cuda.empty_cache()

    # Convert tensor to PIL Image
    image_pil = transforms.ToPILImage()(image.cpu().squeeze(0))

    # Save the image
    image_pil.save(&quot;test_image.png&quot;)

    #Cleanup
    del base
    del image
    del image_pil

if __name__ == &quot;__main__&quot;:
    main()
</code></pre>
<ul>
<li><p>So far I have confirmed that CUDA is enabled and working (checked with torch.cuda.is_available() and through CUDA usage in task manager.</p>
</li>
<li><p>I have attempted saving the image using the documentation recommended image.save(&quot;test.png&quot;) but am getting an AttributeError that says &quot;'Tensor' object has no attribute 'save'. Did you mean: 'ravel'?&quot;.</p>
</li>
<li><p>I have attempted different classes from the diffusers package like StableDiffusionPipeline, with no luck.</p>
</li>
<li><p>Lastly I tried changing the n_steps and noise fraction, same noisy colorful output</p>
</li>
</ul>
<p>Attached below is my current list of packages in python 3.10.4:</p>
<pre><code>accelerate==0.22.0
aiohttp==3.8.5  
aiosignal==1.3.1  
altgraph==0.17.3  
appdirs==1.4.4  
art==6.0  
async-timeout==4.0.3  
attrs==23.1.0  
audioread==3.0.0  
Brotli==1.0.9  
cachetools==5.3.1  
certifi==2023.7.22  
cffi==1.15.1  
charset-normalizer==3.2.0
click==8.1.7  
colorama==0.4.6  
decorator==4.4.2  
diffusers==0.20.2  
docker-pycreds==0.4.0  
filelock==3.12.3  
frozenlist==1.4.0
fsspec==2023.9.0
gitdb==4.0.10
GitPython==3.1.36
google-api-core==2.11.1
google-auth==2.22.0
google-cloud==0.34.0
google-cloud-core==2.3.3
google-cloud-speech==2.21.0
google-cloud-storage==2.10.0
google-crc32c==1.5.0
google-resumable-media==2.5.0
googleapis-common-protos==1.60.0
grpcio==1.57.0
grpcio-status==1.57.0
huggingface-hub==0.17.1
idna==3.4
imageio==2.31.1
imageio-ffmpeg==0.4.8
importlib-metadata==6.8.0
Jinja2==3.1.2
MarkupSafe==2.1.2
moviepy==1.0.3
mpmath==1.2.1
multidict==6.0.4
mutagen==1.46.0
networkx==3.0
numpy==1.25.2
openai==0.27.8
packaging==23.1
pathtools==0.1.2
pefile==2023.2.7
Pillow==10.0.0
pocketsphinx==5.0.2
proglog==0.1.10
proto-plus==1.22.3
protobuf==4.24.0
psutil==5.9.5
setproctitle==1.3.2
six==1.16.0
smmap==5.0.1
sounddevice==0.4.6
soundfile==0.12.1
sympy==1.11.1
tokenizers==0.13.3
torch==2.0.1+cu117
torchaudio==2.0.2+cu117
torchvision==0.15.2+cu117
tqdm==4.66.1
transformers==4.33.1
typing_extensions==4.7.1
urllib3==1.26.16
wandb==0.15.10
websockets==11.0.3
xformers==0.0.21
yarl==1.9.2
youtube-dl==2021.12.17
yt-dlp==2023.7.6
zipp==3.16.2
</code></pre>
<p>And here is a picture of the current output:
<a href=""https://i.sstatic.net/yyTMu.png"" rel=""nofollow noreferrer"">Output of the current StableDiffusionXL model</a></p>
","huggingface"
"77144422","HuggingFacePipeline With AutomodelForCausalLLM 'str' object has no attribute 'shape'","2023-09-20 16:37:45","","0","484","<pipeline><huggingface-transformers><huggingface><large-language-model><llama>","<p>If i use the following code, i get an error message:</p>
<p>AttributeError: 'str' object has no attribute 'shape'.</p>
<p>Normally im running it with llm chain, but i think something is wrong with my model.
The last models I used I was able to run with Llamacpp. That was easier for me.</p>
<p>I would really appreciate if somebody could help.</p>
<pre><code>model_path = &quot;../models/openbuddy-llama2-34b-v11.1-bf16&quot;  
tokenizer = AutoTokenizer.from_pretrained(model_path)

nf4_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_quant_type='nf4',
    bnb_4bit_use_double_quant=False,
    max_memory=24000  
)

pipeline = AutoModelForCausalLM.from_pretrained(
        model_path,
        quantization_config=nf4_config,
        )


  
llm2 = HuggingFacePipeline(pipeline=pipeline)

print(llm2(&quot;Hi, How are you&quot;)) 
</code></pre>
","huggingface"
"77142261","OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like","2023-09-20 12:07:10","","0","3074","<huggingface-transformers><huggingface><stable-diffusion>","<p>Traceback (most recent call last):
File &quot;/data1/wz/anaconda3/envs/pnp-diffusion/lib/python3.8/site-packages/transformers/feature_extraction_utils.py&quot;, line 403, in get_feature_extractor_dict
resolved_feature_extractor_file = cached_path(
File &quot;/data1/wz/anaconda3/envs/pnp-diffusion/lib/python3.8/site-packages/transformers/utils/hub.py&quot;, line 282, in cached_path
output_path = get_from_cache(
File &quot;/data1/wz/anaconda3/envs/pnp-diffusion/lib/python3.8/site-packages/transformers/utils/hub.py&quot;, line 545, in get_from_cache
raise ValueError(
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.</p>
<p>During handling of the above exception, another exception occurred:</p>
<p>Traceback (most recent call last):
File &quot;run_features_extraction.py&quot;, line 14, in 
from pnp_utils import check_safety
File &quot;/data1/wz/research/plug-and-play-main/pnp_utils.py&quot;, line 14, in 
safety_feature_extractor = AutoFeatureExtractor.from_pretrained(safety_model_id)
File &quot;/data1/wz/anaconda3/envs/pnp-diffusion/lib/python3.8/site-packages/transformers/models/auto/feature_extraction_auto.py&quot;, line 270, in from_pretrained
config_dict, _ = FeatureExtractionMixin.get_feature_extractor_dict(pretrained_model_name_or_path, **kwargs)
File &quot;/data1/wz/anaconda3/envs/pnp-diffusion/lib/python3.8/site-packages/transformers/feature_extraction_utils.py&quot;, line 436, in get_feature_extractor_dict
raise EnvironmentError(
OSError: We couldn't connect to 'https://huggingface.co' to load this model, couldn't find it in the cached files and it looks like CompVis/stable-diffusion-safety-checker is not the path to a directory containing a preprocessor_config.json file.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.</p>
<p>i dont know how to solve this problem</p>
","huggingface"
"77139182","How can I see the size of a HuggingFace dataset before downloading it?","2023-09-20 03:51:04","","1","187","<python><huggingface><huggingface-datasets>","<p>I want to download a HuggingFace dataset, e.g. <a href=""https://huggingface.co/datasets/uonlp/CulturaX"" rel=""nofollow noreferrer""><code>uonlp/CulturaX</code></a>:</p>
<pre><code>from datasets import load_dataset
ds = load_dataset(&quot;uonlp/CulturaX&quot;, &quot;en&quot;)
</code></pre>
<p>How can I see the size of a HuggingFace dataset before downloading it?</p>
","huggingface"
"77139005","Dataset library DatasetGenerationError","2023-09-20 02:51:17","77176555","0","999","<dataset><huggingface><huggingface-datasets>","<p>Strangest error I've encountered, copied straight from hugging face website to start learning audio classifiers:</p>
<pre><code>from datasets import load_dataset, Audio, Dataset

minds = load_dataset(&quot;PolyAI/minds14&quot;, name=&quot;en-US&quot;, split=&quot;train&quot;)
</code></pre>
<p>generates the following error:
<code>datasets.builder.DatasetGenerationError: An error occurred while generating the dataset</code></p>
<p>I've tried using <code>Dataset.cleanup_cache_files</code>
but that did not help. Why is this error so vague? Any ideas on how to resolve this?</p>
<p>In case it may help, here's the full traceback:</p>
<pre><code>Generating train split: 0 examples [00:00, ? examples/s]
Traceback (most recent call last):
  File &quot;C:\Users\Brandon\AppData\Local\Programs\Python\Python310\lib\site-packages\datasets\features\audio.py&quot;, line 91, in encode_example
    import soundfile as sf  # soundfile is a dependency of librosa, needed to decode audio files.
ModuleNotFoundError: No module named 'soundfile'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File &quot;C:\Users\Brandon\AppData\Local\Programs\Python\Python310\lib\site-packages\datasets\builder.py&quot;, line 1693, in _prepare_split_single
    example = self.info.features.encode_example(record) if self.info.features is not None else record
  File &quot;C:\Users\Brandon\AppData\Local\Programs\Python\Python310\lib\site-packages\datasets\features\features.py&quot;, line 1852, in encode_example
    return encode_nested_example(self, example)
  File &quot;C:\Users\Brandon\AppData\Local\Programs\Python\Python310\lib\site-packages\datasets\features\features.py&quot;, line 1229, in encode_nested_example
    {
  File &quot;C:\Users\Brandon\AppData\Local\Programs\Python\Python310\lib\site-packages\datasets\features\features.py&quot;, line 1230, in &lt;dictcomp&gt;
    k: encode_nested_example(sub_schema, sub_obj, level=level + 1)
  File &quot;C:\Users\Brandon\AppData\Local\Programs\Python\Python310\lib\site-packages\datasets\features\features.py&quot;, line 1284, in encode_nested_example
    return schema.encode_example(obj) if obj is not None else None
  File &quot;C:\Users\Brandon\AppData\Local\Programs\Python\Python310\lib\site-packages\datasets\features\audio.py&quot;, line 93, in encode_example
    raise ImportError(&quot;To support encoding audio data, please install 'soundfile'.&quot;) from err
ImportError: To support encoding audio data, please install 'soundfile'.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File &quot;C:\Users\Brandon\Documents\00 School Files 00\University\LLM Research\UAC\uac.py&quot;, line 5, in &lt;module&gt;
    minds = load_dataset(&quot;PolyAI/minds14&quot;, name=&quot;en-US&quot;, split=&quot;train&quot;)
  File &quot;C:\Users\Brandon\AppData\Local\Programs\Python\Python310\lib\site-packages\datasets\load.py&quot;, line 2153, in load_dataset
    builder_instance.download_and_prepare(
  File &quot;C:\Users\Brandon\AppData\Local\Programs\Python\Python310\lib\site-packages\datasets\builder.py&quot;, line 954, in download_and_prepare
    self._download_and_prepare(
  File &quot;C:\Users\Brandon\AppData\Local\Programs\Python\Python310\lib\site-packages\datasets\builder.py&quot;, line 1717, in _download_and_prepare
    super()._download_and_prepare(
  File &quot;C:\Users\Brandon\AppData\Local\Programs\Python\Python310\lib\site-packages\datasets\builder.py&quot;, line 1049, in _download_and_prepare
    self._prepare_split(split_generator, **prepare_split_kwargs)
  File &quot;C:\Users\Brandon\AppData\Local\Programs\Python\Python310\lib\site-packages\datasets\builder.py&quot;, line 1555, in _prepare_split
    for job_id, done, content in self._prepare_split_single(
  File &quot;C:\Users\Brandon\AppData\Local\Programs\Python\Python310\lib\site-packages\datasets\builder.py&quot;, line 1712, in _prepare_split_single
    raise DatasetGenerationError(&quot;An error occurred while generating the dataset&quot;) from e
datasets.builder.DatasetGenerationError: An error occurred while generating the dataset
</code></pre>
","huggingface"
"77135688","accessing hugging face data set actual file name","2023-09-19 14:55:42","","0","258","<python><json><dataset><huggingface><huggingface-datasets>","<p>I am new to hugging face and have recently upload an audio dataset in my private repository. I am accessing it using access token and it downloads the data just fine. However, the original audio file names are like &quot;abc.wav&quot; but the downloaded version has name in hexadecimal like fff29eed0b3102ee448be3cc68c019d1232930c947dd3b3defc9156c5356afc2 for each audio file. so the cached folder right now has 3 types of files for each audio file. Like</p>
<ol>
<li>fff29eed0b3102ee448be3cc68c019d1232930c947dd3b3defc9156c5356afc2</li>
<li>fff29eed0b3102ee448be3cc68c019d1232930c947dd3b3defc9156c5356afc2.json</li>
<li>fff29eed0b3102ee448be3cc68c019d1232930c947dd3b3defc9156c5356afc2.lock</li>
</ol>
<p>The original audio file name is in .json file of each audio file like &quot;$16362619add875/dataset/abc.wav&quot;, &quot;etag&quot;: null}
&quot;
I also iterated through the data set dict but it only has the audio data as array and the sampling frequency and not the actual file name.</p>
<p>My all computations ahead need to address the file by its original name for some reason. What would be an easy way to do it? or is there any way to configure something on hugging face that can perhaps add the file name to the dataset dict?</p>
","huggingface"
"77132961","Problem Uploading Large Files to Hugging Face: Slow Speeds and Interruptions","2023-09-19 08:40:54","","0","840","<git><performance><huggingface><huggingface-hub>","<p>I'm facing issues with uploading large model files to Hugging Face.
I managed a large upload once using the webinterface, after several interuptions and restarts, but in order to automatize things i want to upload from commandline, right now using the HuggingFace-CLI command. The upload speeds seem to be capped at around 140 to 180 kB/s, and when dealing with 16 GB files, a single upload takes about 40 hours.
However, the upload process is interrupted at least every 24 hours due to nightly internet connection resets (common in my area).</p>
<p><strong>Question:</strong> How can I achieve a reliable and faster upload of large files to Hugging Face given these challenges? (with main focus on <code>reliable</code>)</p>
<h3>Details:</h3>
<ul>
<li>I've installed Git and Git LFS, and I'm using a repository I created s public in Hugging Face.</li>
<li>I've already run <code>huggingface-cli lfs-enable-largefiles</code> locally for the repository.</li>
<li>My internet connection is stable during the day but experiences brief disconnections at night. This is due to the <code>PPP Session Termination</code> occurring once a day with my ISP (and is common here in Germany)</li>
<li>The upload cap at 140 kB/s is not due to the limits of my Internet connection, and i can start a bunch of model uploads at the same time, each having the same speed of around 140 kB/s. So it's clearly a limitation of huggingface.</li>
</ul>
<h3>Upload Command:</h3>
<p>This is the command line i use to start the upload (repo-name, filename and token changed)</p>
<pre class=""lang-bash prettyprint-override""><code>huggingface-cli upload maddes8cht/my-repo-id ggml-my-filename.gguf --commit-message &quot;upload ggml-my-filename.gguf&quot; --token hf_***xyz*** --repo-type model
</code></pre>
<p>This is the message i get in the middle of the night (some chars in the credentials/key randomly changed):</p>
<pre class=""lang-bash prettyprint-override""><code>ggml-my-filename.gguf:   7%|███▊                                                   | 1.84G/26.4G [3:09:29&lt;933:28:46, 7.31kB/s]urllib3.exceptions.SSLError: EOF occurred in violation of protocol (_ssl.c:2426)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File &quot;C:\Users\Mathias\anaconda3\envs\convert\lib\site-packages\requests\adapters.py&quot;, line 486, in send
    resp = conn.urlopen(
  File &quot;C:\Users\Mathias\anaconda3\envs\convert\lib\site-packages\urllib3\connectionpool.py&quot;, line 844, in urlopen
    retries = retries.increment(
  File &quot;C:\Users\Mathias\anaconda3\envs\convert\lib\site-packages\urllib3\util\retry.py&quot;, line 515, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='s3.us-east-1.amazonaws.com', port=443): Max retries exceeded with url: /lfs.huggingface.co/repos/49/89/49892f641ccec0867ad7ad57124c3eff9394b4a087f90e21c663d5c2ejx01676/62651a007cef883c09356e3dda718f929da9116281a7b68b868911461e38b408?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&amp;X-Amz-Credential=AKIA4N7VTTLO27GPWFUO%2F20230919%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20230919T035725Z&amp;X-Amz-Expires=86400&amp;X-Amz-Signature=ef7d51dc72c76dcd2cce5b127165e3b866998e523779ef0f097d92vc9c327b86&amp;X-Amz-SignedHeaders=host&amp;partNumber=70&amp;uploadId=popfIEAPsJRDCOGCSX8_FfNpj.V8AApuAeF9iXBFyOLY0hG.al3qHt_EURXzg8fa729TGck2r7qzGvd5QWpIxZ7HP.M8vM9G2Co1Cz7woD7Tnt7DXYkzxRZEJYeC.7cI&amp;x-id=UploadPart (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:2426)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;C:\Users\Mathias\anaconda3\envs\convert\lib\site-packages\huggingface_hub\_commit_api.py&quot;, line 382, in _wrapped_lfs_upload
    lfs_upload(operation=operation, lfs_batch_action=batch_action, token=token)
  File &quot;C:\Users\Mathias\anaconda3\envs\convert\lib\site-packages\huggingface_hub\lfs.py&quot;, line 222, in lfs_upload
    _upload_multi_part(operation=operation, header=header, chunk_size=chunk_size, upload_url=upload_action[&quot;href&quot;])
  File &quot;C:\Users\Mathias\anaconda3\envs\convert\lib\site-packages\huggingface_hub\lfs.py&quot;, line 318, in _upload_multi_part
    else _upload_parts_iteratively(operation=operation, sorted_parts_urls=sorted_parts_urls, chunk_size=chunk_size)
  File &quot;C:\Users\Mathias\anaconda3\envs\convert\lib\site-packages\huggingface_hub\lfs.py&quot;, line 374, in _upload_parts_iteratively
    part_upload_res = http_backoff(&quot;PUT&quot;, part_upload_url, data=fileobj_slice)
  File &quot;C:\Users\Mathias\anaconda3\envs\convert\lib\site-packages\huggingface_hub\utils\_http.py&quot;, line 258, in http_backoff
    response = session.request(method=method, url=url, **kwargs)
  File &quot;C:\Users\Mathias\anaconda3\envs\convert\lib\site-packages\requests\sessions.py&quot;, line 589, in request
    resp = self.send(prep, **send_kwargs)
  File &quot;C:\Users\Mathias\anaconda3\envs\convert\lib\site-packages\requests\sessions.py&quot;, line 703, in send
    r = adapter.send(request, **kwargs)
  File &quot;C:\Users\Mathias\anaconda3\envs\convert\lib\site-packages\huggingface_hub\utils\_http.py&quot;, line 63, in send
    return super().send(request, *args, **kwargs)
  File &quot;C:\Users\Mathias\anaconda3\envs\convert\lib\site-packages\requests\adapters.py&quot;, line 517, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: (MaxRetryError(&quot;HTTPSConnectionPool(host='s3.us-east-1.amazonaws.com', port=443): Max retries exceeded with url: /lfs.huggingface.co/repos/49/89/49892f641ccec0867ad7ad57124c3eff9394b4a087f90e21c663d5c2e5601676/62651a007cef883c09356e3dda718f929da9116281a7b68b868911461e38b408?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&amp;X-Amz-Credential=AKIA4N7VTTLO27GPWFUO%2F20230919%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20fd09ZUT035725Z&amp;X-Amz-Expires=86400&amp;X-Amz-Signature=ef7d51dc72c76dcd2xye5b127165e3b866998e523779ef0f097d92c19c327b86&amp;X-Amz-SignedHeaders=host&amp;partNumber=70&amp;uploadId=popfIEAPsJRDCOGCSX8_FfNpj.V8AApuAeF9iXBFyOLY0hG.al3qHt_EURXzg8fa729TGck2r7qzGvd5QWpIxZ7HP.M8vM9G2Co1Cz7woD7Tnt7DXYkzxRZEJYeC.7cI&amp;x-id=UploadPart (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:2426)')))&quot;), '(Request ID: 10870c92-77d0-4700-bc4e-f44baa646e77)')

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File &quot;C:\Users\Mathias\anaconda3\envs\convert\Scripts\huggingface-cli-script.py&quot;, line 9, in &lt;module&gt;
    sys.exit(main())
  File &quot;C:\Users\Mathias\anaconda3\envs\convert\lib\site-packages\huggingface_hub\commands\huggingface_cli.py&quot;, line 49, in main
    service.run()
  File &quot;C:\Users\Mathias\anaconda3\envs\convert\lib\site-packages\huggingface_hub\commands\upload.py&quot;, line 183, in run
    print(self._upload())
  File &quot;C:\Users\Mathias\anaconda3\envs\convert\lib\site-packages\huggingface_hub\commands\upload.py&quot;, line 244, in _upload
    return upload_file(
  File &quot;C:\Users\Mathias\anaconda3\envs\convert\lib\site-packages\huggingface_hub\utils\_validators.py&quot;, line 118, in _inner_fn
    return fn(*args, **kwargs)
  File &quot;C:\Users\Mathias\anaconda3\envs\convert\lib\site-packages\huggingface_hub\hf_api.py&quot;, line 849, in _inner
    return fn(self, *args, **kwargs)
  File &quot;C:\Users\Mathias\anaconda3\envs\convert\lib\site-packages\huggingface_hub\hf_api.py&quot;, line 3460, in upload_file
    commit_info = self.create_commit(
  File &quot;C:\Users\Mathias\anaconda3\envs\convert\lib\site-packages\huggingface_hub\utils\_validators.py&quot;, line 118, in _inner_fn
    return fn(*args, **kwargs)
  File &quot;C:\Users\Mathias\anaconda3\envs\convert\lib\site-packages\huggingface_hub\hf_api.py&quot;, line 849, in _inner
    return fn(self, *args, **kwargs)
  File &quot;C:\Users\Mathias\anaconda3\envs\convert\lib\site-packages\huggingface_hub\hf_api.py&quot;, line 2934, in create_commit
    upload_lfs_files(
  File &quot;C:\Users\Mathias\anaconda3\envs\convert\lib\site-packages\huggingface_hub\utils\_validators.py&quot;, line 118, in _inner_fn
    return fn(*args, **kwargs)
  File &quot;C:\Users\Mathias\anaconda3\envs\convert\lib\site-packages\huggingface_hub\_commit_api.py&quot;, line 392, in upload_lfs_files
    _wrapped_lfs_upload(filtered_actions[0])
  File &quot;C:\Users\Mathias\anaconda3\envs\convert\lib\site-packages\huggingface_hub\_commit_api.py&quot;, line 384, in _wrapped_lfs_upload
    raise RuntimeError(f&quot;Error while uploading '{operation.path_in_repo}' to the Hub.&quot;) from exc
RuntimeError: Error while uploading 'ggml-my-filename.gguf' to the Hub.
ggml-my-filename.gguf:   7%|███▉
</code></pre>
<p>Right now it seems impossible for me to ever finish an upload of models in that size.</p>
<p><strong>Note:</strong>
This question has similarities with the unanswered question <a href=""https://stackoverflow.com/q/70828240/20124484"">Why does upload a model to HuggingFace repository so slow?</a> , but contains further aspects and details</p>
","huggingface"
"77128569","After training with DPOTrainer of trl, and saving, loading error when using AutoPeftModelForCausalLM","2023-09-18 15:37:14","","1","907","<python><deep-learning><pytorch><huggingface>","<p>After training with DPOTrainer of trl, I saved it locally as below and then loaded it with AutoPeftModelForCausalLM, and an error came out.
When I load a checkpoint stored locally with SFTTranier in the same way, I don't get an error.
I put token (no problem) and something else. but I keep getting the same error.
I ask for the help.
Thank you.</p>
<pre><code>dpo_trainer.model.save_pretrained(path_save_dpo)
tokenizer.save_pretrained(path_save_dpo)
</code></pre>
<p><code>('./model/dpo_results\\final_checkpoint\\tokenizer_config.json', './model/dpo_results\\final_checkpoint\\special_tokens_map.json', './model/dpo_results\\final_checkpoint\\vocab.json', './model/dpo_results\\final_checkpoint\\merges.txt', './model/dpo_results\\final_checkpoint\\added_tokens.json', './model/dpo_results\\final_checkpoint\\tokenizer.json')</code></p>
<pre><code>os.listdir(&quot;./model/dpo_results/final_checkpoint&quot;)
</code></pre>
<pre><code>['README.md', 'adapter_model.bin', 'adapter_config.json', 'tokenizer_config.json', 'special_tokens_map.json', 'vocab.json', 'merges.txt', 'tokenizer.json']
</code></pre>
<pre><code>import torch
from peft import AutoPeftModelForCausalLM
from transformers import AutoTokenizer, AutoModelForCausalLM

DEV = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
adapter_path = &quot;./model/dpo_results/final_checkpoint&quot;
model = AutoPeftModelForCausalLM.from_pretrained(
adapter_path,
torch_dtype=torch.bfloat16,
load_in_4bit=True,
token = HUGGINGFACEHUB_API_TOKEN,
)
tokenizer = AutoTokenizer.from_pretrained(adapter_path)
</code></pre>
<pre><code>HTTPError                                 Traceback (most recent call last)
File c:\Users\nl202\miniconda3\envs\py31006th\lib\site-packages\huggingface_hub\utils\_errors.py:261, in hf_raise_for_status(response, endpoint_name)
    260 try:
--&gt; 261     response.raise_for_status()
    262 except HTTPError as e:

File c:\Users\nl202\miniconda3\envs\py31006th\lib\site-packages\requests\models.py:1021, in Response.raise_for_status(self)
   1020 if http_error_msg:
-&gt; 1021     raise HTTPError(http_error_msg, response=self)

HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/None/resolve/main/config.json

The above exception was the direct cause of the following exception:

RepositoryNotFoundError                   Traceback (most recent call last)
File c:\Users\nl202\miniconda3\envs\py31006th\lib\site-packages\transformers\utils\hub.py:428, in cached_file(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)
    426 try:
    427     # Load from URL or cache if already cached
--&gt; 428     resolved_file = hf_hub_download(
    429         path_or_repo_id,
    430         filename,
    431         subfolder=None if len(subfolder) == 0 else subfolder,
    432         repo_type=repo_type,
    433         revision=revision,
    434         cache_dir=cache_dir,
    435         user_agent=user_agent,
    436         force_download=force_download,
    437         proxies=proxies,
    438         resume_download=resume_download,
    439         token=token,
    440         local_files_only=local_files_only,
    441     )
    442 except GatedRepoError as e:

File c:\Users\nl202\miniconda3\envs\py31006th\lib\site-packages\huggingface_hub\utils\_validators.py:118, in validate_hf_hub_args.&lt;locals&gt;._inner_fn(*args, **kwargs)
    116     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.__name__, has_token=has_token, kwargs=kwargs)
--&gt; 118 return fn(*args, **kwargs)

File c:\Users\nl202\miniconda3\envs\py31006th\lib\site-packages\huggingface_hub\file_download.py:1195, in hf_hub_download(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)
   1194 try:
-&gt; 1195     metadata = get_hf_file_metadata(
   1196         url=url,
   1197         token=token,
   1198         proxies=proxies,
   1199         timeout=etag_timeout,
   1200     )
   1201 except EntryNotFoundError as http_error:
   1202     # Cache the non-existence of the file and raise

File c:\Users\nl202\miniconda3\envs\py31006th\lib\site-packages\huggingface_hub\utils\_validators.py:118, in validate_hf_hub_args.&lt;locals&gt;._inner_fn(*args, **kwargs)
    116     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.__name__, has_token=has_token, kwargs=kwargs)
--&gt; 118 return fn(*args, **kwargs)

File c:\Users\nl202\miniconda3\envs\py31006th\lib\site-packages\huggingface_hub\file_download.py:1541, in get_hf_file_metadata(url, token, proxies, timeout)
   1532 r = _request_wrapper(
   1533     method=&quot;HEAD&quot;,
   1534     url=url,
   (...)
   1539     timeout=timeout,
   1540 )
-&gt; 1541 hf_raise_for_status(r)
   1543 # Return

File c:\Users\nl202\miniconda3\envs\py31006th\lib\site-packages\huggingface_hub\utils\_errors.py:293, in hf_raise_for_status(response, endpoint_name)
    285     message = (
    286         f&quot;{response.status_code} Client Error.&quot;
    287         + &quot;\n\n&quot;
   (...)
    291         &quot; make sure you are authenticated.&quot;
    292     )
--&gt; 293     raise RepositoryNotFoundError(message, response) from e
    295 elif response.status_code == 400:

RepositoryNotFoundError: 404 Client Error. (Request ID: Root=1-6508609b-66056fe260c0c09f5c752227;a56272fe-4e1b-43fc-b435-e0b47d9222f0)

Repository Not Found for url: https://huggingface.co/None/resolve/main/config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.

The above exception was the direct cause of the following exception:

OSError                                   Traceback (most recent call last)
e:\gdstrm\내 드라이브\Colab Notebooks\dl\dl_nlp\RLHF_3_DPO\2_미디엄블로그\work_미디엄블로그.ipynb Cell 84 line 7
      5 DEV = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
      6 adapter_path = &quot;./model/dpo_results/final_checkpoint&quot;
----&gt; 7 model = AutoPeftModelForCausalLM.from_pretrained(
      8     adapter_path,
      9     torch_dtype=torch.bfloat16,
     10     load_in_4bit=True,
     11     token = HUGGINGFACEHUB_API_TOKEN,
     12 )
     13 tokenizer = AutoTokenizer.from_pretrained(adapter_path)


.............

OSError: None is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=&lt;your_token&gt;`
</code></pre>
<p>Please help me if you know how to resolve the error
Thank you.</p>
<p>※ PS, I used &quot;facebook/opt-350m&quot; as base_model. and I followed this guide faithfully (<a href=""https://github.com/mzbac/llama2-fine-tune"" rel=""nofollow noreferrer"">https://github.com/mzbac/llama2-fine-tune</a>)`</p>
","huggingface"
"77125624","SSLError: (MaxRetryError(""HTTPSConnectionPool(host='huggingface.co', port=443)","2023-09-18 08:35:36","","0","3929","<python><huggingface-transformers><huggingface><sentence-transformers>","<p>I would like to use the <code>SentenceTransformers</code> like from this <a href=""https://www.sbert.net/"" rel=""nofollow noreferrer"">page</a>. When running the following code I get an error which I'm not able to solve:</p>
<pre><code>import torch
from sentence_transformers import SentenceTransformer

sentences = ['This framework generates embeddings for each input sentence',
    'Sentences are passed as a list of string.',
    'The quick brown fox jumps over the lazy dog.']

model = SentenceTransformer(&quot;paraphrase-multilingual-mpnet-base-v2&quot;)
embeddings = model.encode(sentences)
</code></pre>
<p>This returns the following error:</p>
<pre><code>SSLError: (MaxRetryError(&quot;HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/models/sentence-transformers/paraphrase-multilingual-mpnet-base-v2 (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)')))&quot;), '(Request ID: b875fc51-82ec-4309-94ab-0d08f9ab067d)')
</code></pre>
<p>I'm using <code>Python 3.10.12</code> with the following packages version:</p>
<pre><code>torch                    2.0.1
sentence-transformers    2.2.2
requests                 2.31.0
</code></pre>
<p>I tried to add the following code like from this question (<a href=""https://stackoverflow.com/questions/70481851/how-to-fix-exception-has-occurred-sslerror-httpsconnectionpool-in-vs-code-env"">how to fix &quot;Exception has occurred: SSLError HTTPSConnectionPool&quot; in VS Code environment</a>):</p>
<pre><code>import requests
r = requests.get('https://huggingface.com', verify=False)
</code></pre>
<p>Unfortunately, this also returns an error:</p>
<pre><code>SSLError: HTTPSConnectionPool(host='huggingface.com', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)')))
</code></pre>
<p>I also tried to use <code>verify=ssl.CERT_NONE</code> instead of <code>verify=False</code> like described <a href=""https://stackoverflow.com/questions/55680224/how-to-fix-requests-exceptions-sslerror"">here</a>, but this also doesn't work.</p>
<p>Finally, I added this code and downgraded <code>requests</code> to <code>2.27.1</code> like described <a href=""https://stackoverflow.com/questions/75110981/sslerror-httpsconnectionpoolhost-huggingface-co-port-443-max-retries-exce"">here</a>:</p>
<pre><code>import os

os.environ['CURL_CA_BUNDLE'] = ''
</code></pre>
<p>Also this doesn't work. So I was wondering if anyone knows why this error happens and how to fix this?</p>
","huggingface"
"77116207","What is the correct approach to evaluate Huggingface models on the masked language modeling task?","2023-09-16 03:08:05","77118113","1","276","<machine-learning><nlp><huggingface-transformers><huggingface>","<p>I'm trying to test how well different models are doing on the <a href=""https://huggingface.co/docs/transformers/main/tasks/masked_language_modeling"" rel=""nofollow noreferrer"">masked language modeling task</a>.</p>
<p>Given a prompt</p>
<pre><code>prompt = &quot;The Milky Way is a [MASK] galaxy&quot;
</code></pre>
<p>I'm trying to get an output for the masked token from different models. The issue is that when I load a model for the masked language modeling task:</p>
<pre><code>from transformers import AutoModelForMaskedLM, AutoTokenizer
model = AutoModelForMaskedLM.from_pretrained('bert-base-cased')
model.eval()
tokenizer = AutoTokenizer.from_pretrained('bert-base-cased', truncation=True)
</code></pre>
<p>I get the warning:</p>
<p><code>Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.weight']</code></p>
<p>From the <a href=""https://huggingface.co/bert-base-uncased/discussions/4"" rel=""nofollow noreferrer"">huggingface forum</a> they had a similar question, but only referred to parts of these weights: <code>['cls.seq_relationship.weight', 'cls.seq_relationship.bias']</code>. The answer there was:</p>
<pre><code>&gt;&gt;&gt; It tells you that by loading the bert-base-uncased checkpoint in the BertForMaskedLM architecture, you're dropping two weights: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias'].

These are the weights used for next-sentence prediction, which aren't necessary for Masked Language Modeling.
If you're only interested in doing masked language modeling, then you can safely disregard this warning.
</code></pre>
<p>But it seems like when I'm loading it I'm also removing <code>['bert.pooler.dense.bias', 'bert.pooler.dense.weight']</code>, which I could not find out if dropping these is going to result in different performance for the masked language modeling without fine-tuning the model.</p>
<p>If I just load the model with</p>
<pre><code>model = AutoModel.from_pretrained('bert-base-cased')
</code></pre>
<p>I get no error, but then I cannot use it to predict the mask (as far as I know).</p>
<p><strong>So is the correct approach to load the model with <code>AutoModelForMaskedLM</code> as I've done and just ignore the warning (assuming that it makes no difference because the dropped weights are useless for the masked task), or is there a different approach?</strong></p>
","huggingface"
"77110608","Loading a Huggingface Model with Microsofts Semantic Kernel in C# / VB.NET","2023-09-15 07:57:15","","6","2431","<c#><vb.net><huggingface><semantic-kernel>","<p>Microsoft presented it's new library <a href=""https://learn.microsoft.com/en-us/semantic-kernel/overview/"" rel=""noreferrer"">Semantic Kernel</a> to code own chat programs like ChatGPT with .NET.</p>
<p>In their documentation they are telling, that you can either use OpenAI´s LLM, Azure LLM or Huggingfaces LLM.</p>
<p>I prefer using Huggingfaces LLM, because I prefer running local LLM for free, instead of paying for a cloud service.</p>
<p>But I don't see any documentation at the Microsoft website, a Youtube video, somebody at stackoverflow or anywhere else at the internet who managed to load a local LLM with Semantic Kernel with C#/VB.NET.</p>
<p>So my question is, does anybody know´s how to load and use them?</p>
<p>Thanks!</p>
","huggingface"
"77107723","PyTorch + Huggingface model outputs converge to identical values","2023-09-14 18:59:23","","0","81","<pytorch><regression><huggingface-transformers><loss-function><huggingface>","<p>I've created a model which takes as input a (tokenized) sequence of length <em>n</em>  and predicts a sequence of 0-1 probabilities for each of the <em>n</em> tokens. e.g. [0.0, 0.0, 0.3, 0.72, ... , 0.0]. The model is set up to take the output of a pretrained BERT model (size of [batch_sz, 1024, sequence_len]) and feed its output into a group of fully connected layers as such:</p>
<pre class=""lang-py prettyprint-override""><code>
class MyModel(nn.Module):
    def __init__(self, bert_model):
        super(MyModel, self).__init__()
        self.bert_model = bert_model
        self.linear_1 = nn.Linear(1024, 512)
        self.linear_2 = nn.Linear(512, 128)
        self.linear_3 = nn.Linear(128, 64)
        self.linear_4 = nn.Linear(64, 1)

    def forward(self, input_ids, attn_mask):
        x = F.relu(self.bert_model(input_ids, attention_mask=attn_mask).last_hidden_state)
        x = F.relu(self.linear_1(x))
        x = F.relu(self.linear_2(x))
        x = F.relu(self.linear_3(x))
        x = torch.flatten(torch.sigmoid(self.linear_4(x)))
        return x
</code></pre>
<p>What I'm finding, however is when evaluating, all outputs for the sequences are the same value with very slight differences. The model does in fact learn, and it does so rather quickly, but also quickly converges to these type of predictions:</p>
<p><a href=""https://i.sstatic.net/ogDNH.png"" rel=""nofollow noreferrer"">Loss over time</a></p>
<p>This trend continues long after step 175 shown in the above image. When evaluating (every 5000 steps on a test dataset) on a dataset of 1000 unique sequences, varying lengths (around ~500) on average we see that the model has learned to predict a single value for every position in the output (e.g. [0.07, 0.07, ... ,0.07]) for every sequence evaluated:</p>
<pre class=""lang-py prettyprint-override""><code> trianing_steps,  maximum_prediction, minimum_prediction, difference
     5000              0.078548           0.078548        8.94E-08
     10000             0.079725           0.079725        7.45E-08
     15000             0.082846           0.082846        7.45E-08
     20000             0.082651           0.082651        7.45E-08
     25000             0.067803           0.067803        3.73E-08
                 
</code></pre>
<p>Examining the predictions in the training loop, we can see that this phenomenon occurs even in the training dataset. It's not always the case, as predictions in the beginning of the training loop do in fact differ (by up to 50% in some cases), but converge to all being nearly identical as time goes on. This leads me to believe that the data processing is correct and the issue is either in the way I'm using the loss function, or in the way the model is set up. The training loop is written as follows:</p>
<pre class=""lang-py prettyprint-override""><code>
protbert_model = BertModel.from_pretrained(model_name)
model = MyModel(protbert_model)
model.to(device)

mask_domains = True
    
optimizer = AdamW(model.parameters(), lr = 0.00001)
loss_fct = nn.MSELoss(reduction=&quot;sum&quot;)

model.train()
for step, batch in enumerate(train_data_loader):
      labels = torch.from_numpy(np.asarray(batch[&quot;labels&quot;], dtype=np.float32)).cuda()
      inputs = batch[&quot;input_ids&quot;].cuda()
      attn_mask = batch[&quot;attention_mask&quot;].cuda()
      outputs = model(inputs, attn_mask).cuda()

      # create mask to exclude the padding tokens in loss calculation
      # making the outputs and labels 0 shouldn't affect loss value 
      # due to reduction = &quot;sum&quot;
      loss_mask = torch.where(labels == -100.0, 0, 1).cuda()

      # apply the mask for padding tokens 
      labels = torch.mul(labels, loss_mask)
      outputs = torch.mul(outputs, loss_mask)

      #creating mask to exclude additional tokens we don't want in our calculation
      if mask_domains:
            domains = np.asarray(batch[&quot;domains&quot;])
            # making the outputs and labels 0 shouldn't affect loss value 
            # due to reduction = &quot;sum&quot;
            domains_mask = np.where(domains == -1, 0, 1)
            domains_mask = torch.from_numpy(domains_mask).cuda()
            labels = torch.mul(labels, domains_mask)
            outputs = torch.mul(outputs, domains_mask) 
      
      loss = loss_fct(outputs, labels)
      loss.backward()
      optimizer.step()
      optimizer.zero_grad()
</code></pre>
<p>As of now I'm using a batch size of just 1. It also should be noted that even after applying the masks in the loss function, my data is quite imbalanced with most of the labels being 0.0 (<a href=""https://imgur.com/ChI3nUB"" rel=""nofollow noreferrer"">histogram</a>). When excluding the values that are 0.0, there is a solid range in the labels (<a href=""https://i.imgur.com/u1f4HCs.png"" rel=""nofollow noreferrer"">histogram</a>)</p>
<p>Any help or advice you all have is greatly appreciated. Thanks</p>
<p>I've tried changing the sized of the Linear layers, changed the optimizer to torch's SGD, altered the learning rates, added weight decay, tried Tanh instead of ReLU layers - none of which seemed to make a difference.</p>
<p>I expect the predictions of the model to at least be somewhat diverse.</p>
<p>An example of a real label set is like :</p>
<pre><code>
[0.       0.       0.       0.       0.       0.       0.       0.
 0.       0.       0.       0.       0.       0.       0.       0.
 0.       0.       0.       0.       0.       0.       0.       0.
 0.       0.       0.       0.245861 0.       0.121519 0.121519 0.
 0.       0.       0.       0.       0.       0.       0.       0.
 0.       0.       0.       0.       0.       0.       0.       0.
 0.       0.       0.       0.110458 0.121519 0.136272 0.       0.
 0.       0.       0.       0.       0.       0.       0.       0.
 0.       0.       0.       0.       0.245861 0.       0.114083 0.48657
 0.       0.121519 0.38957  0.160715 0.       0.121519 0.110478 0.160715
 0.       0.       0.       0.       0.       0.       0.       0.
 0.       0.       0.       0.       0.       0.       0.       0.
 0.131778 0.       0.       0.131778 0.114083 0.274798 0.121519 0.121519
 0.       0.       0.       0.       0.       0.       0.       0.
 0.       0.       0.       0.       0.       0.       0.       0.

... ] 
</code></pre>
<p>and what we see form predictions is:</p>
<pre><code>
[ 0.0187452  0.01874547 0.01874572 0.01874562 0.01874547 0.01874564
 0.01874568 0.01874546 0.01874538 0.01874539 0.01874551 0.01874532
 0.01874505 0.01874523 0.01874538 0.01874523 0.01874526 0.01874509
 0.01874497 0.01874483 0.0187447  0.0187444  0.01874427 0.0187444
 0.01874436 0.01874422 0.01874386 0.01874404 0.01874396 0.01874398
 0.01874437 0.01874458 0.01874456 0.01874412 0.01874492 0.01874468
 0.01874422 0.01874419 0.01874413 0.01874394 0.01874425 0.01874348
 0.01874355 0.01874362 0.01874375 0.01874341 0.01874397 0.01874365
 0.01874401 0.01874386 0.01874417 0.01874382 0.01874388 0.01874438
 0.01874433 0.01874454 0.01874454 0.01874433 0.01874435 0.01874386
 0.0187438  0.01874388 0.01874399 0.01874427 0.01874458 0.01874487, 
...]
</code></pre>
","huggingface"
"77107578","ModuleNotFoundError when activating venv virtual environment in a python CGI script","2023-09-14 18:32:58","","0","70","<python><pytorch><cgi><huggingface-transformers><huggingface>","<p>I just started using the huggingface transformers library and have been experimenting with it via Pytorch and venv.</p>
<p>If I activate the venv virtual environment from a shell, it works just fine.
However, not if I activate it in a Python CGI script, like so:</p>
<pre><code># Importing libraries for virtual environment
import subprocess

# Defining function for activating/deactivating virtual environment
def activate_virtual_environment():

    # Build the path to the activation script
    activation_script = f&quot;/xampp/htdocs/cgi_bin/env/Scripts/activate&quot;

    # Activate the virtual environment using subprocess
    subprocess.run([activation_script], shell=True)

def deactivate_virtual_environment():
    # Deactivate the virtual environment using subprocess
    subprocess.run([&quot;deactivate&quot;], shell=True)

# Activating virtual environment
activate_virtual_environment()

# Importing 'pipeline' 
from transformers import pipeline
</code></pre>
<p>If I activate the virtual environment like this, I get a <code>ModuleNotFoundError: No module named ‘transformers’</code>, when the script is executed.</p>
<p>The path to the virtual environment is definitely correct, what am I overlooking?</p>
","huggingface"
"77106305","Dino2 for classification has wrong number of labels","2023-09-14 15:16:28","","1","209","<pytorch><huggingface-transformers><huggingface><imagenet><vision-transformer>","<p>I am encountering an issue when using the Dinov2ForImageClassification model from the Hugging Face Transformers library, as outlined in the documentation <a href=""https://huggingface.co/docs/transformers/main/model_doc/dinov2#transformers.Dinov2ForImageClassification"" rel=""nofollow noreferrer"">here</a>. Despite following the provided code example and using the latest Transformers version, the resulting model is performing binary classification instead of the expected ImageNet 1000-way classification. Specifically, the length of the logits returned by the model (<code>logits</code>) is 2, whereas it should be 1000 for ImageNet classification.</p>
<p>Here is my code:</p>
<pre><code>from transformers import AutoImageProcessor, Dinov2ForImageClassification
import torch
from datasets import load_dataset

# Load a sample image dataset (in this case, &quot;huggingface/cats-image&quot;)
dataset = load_dataset(&quot;huggingface/cats-image&quot;)
image = dataset[&quot;test&quot;][&quot;image&quot;][0]

# Load the image processor and the Dinov2ForImageClassification model
image_processor = AutoImageProcessor.from_pretrained(&quot;facebook/dinov2-base&quot;)
model = Dinov2ForImageClassification.from_pretrained(&quot;facebook/dinov2-base&quot;)

# Prepare the input and obtain logits
inputs = image_processor(image, return_tensors=&quot;pt&quot;)
with torch.no_grad():
    logits = model(**inputs).logits

# The expected number of labels for ImageNet classification should be 1000
predicted_label = logits.argmax(-1).item()
</code></pre>
<p>However, I encounter the following error:</p>
<p>csharpCopy code</p>
<pre><code>Some weights of Dinov2ForImageClassification were not initialized from the model checkpoint at facebook/dinov2-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</code></pre>
<p>Additionally, the shape of <code>logits</code> is <code>torch.Size([1, 2])</code>, indicating that the model has only 2 labels instead of the expected 1000 as specified by <code>model.num_labels</code>.</p>
<p>I'm seeking guidance on how to correctly use Dinov2ForImageClassification for ImageNet 1000-way classification as mentioned in the documentation.</p>
","huggingface"
"77105247","TheBloke/Llama-2-7b does not appear to have a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack","2023-09-14 13:12:45","77249234","3","6831","<huggingface><large-language-model><llama>","<p>As you can guess from the title, this is the error I get. I only changed the model in AutoModelForCausalLM, Older version was</p>
<pre><code>
model = AutoModelForCausalLM.from_pretrained(&quot;meta-llama/Llama-2-7b-chat-hf&quot;,

device_map ='auto',

torch_dtype = torch.float16,

use_auth_token = True)
</code></pre>
<p>However, since my GPU is NVIDIA GeForce RTX 2080 TI, it answers a simple question in 20 mins. Then I changed it to:</p>
<pre><code>
model = AutoModelForCausalLM.from_pretrained(&quot;TheBloke/Llama-2-7b-Chat-GGUF&quot;,

model_file = &quot;llama-2-7b-chat.q4_K_M.gguf&quot;,

device_map ='auto',

torch_dtype = torch.float16,

use_auth_token = True)
</code></pre>
<p>However, this is not working, and giving the error. Below is the full code, if it is needed to solve.</p>
<p>Before the full code: Also, I have the file &quot;llama-2-7b.Q5_K_m.gguf&quot; downloaded from HF in my local env, but not virtual env. I am not using this local file in the code, but saying if it helps.</p>
<pre><code>from langchain.document_loaders import JSONLoader

from langchain.text_splitter import CharacterTextSplitter, TokenTextSplitter, RecursiveCharacterTextSplitter

from langchain.embeddings import HuggingFaceEmbeddings

from langchain.vectorstores import Chroma

from langchain import HuggingFacePipeline

from langchain.chains import ConversationalRetrievalChain

from langchain.memory import ConversationBufferMemory

from langchain.embeddings.openai import OpenAIEmbeddings

from langchain.embeddings.huggingface import HuggingFaceEmbeddings

from langchain.chat_models import ChatOpenAI

import os

import sys

import huggingface_hub

from huggingface_hub import notebook_login

import torch

import transformers

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

from torch import cuda, bfloat16

import chromadb

from pathlib import Path

from pprint import pprint

import json

from loader import JSONLoader

from langchain.prompts.chat import PromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate, ChatPromptTemplate

import json

from langchain.docstore.document import Document



def parse_json(json_data):

&quot;&quot;&quot;Parse JSON data into a Python dictionary.&quot;&quot;&quot;

return json.loads(json_data)



def create_doc(json_data):

&quot;&quot;&quot;Create a Document object from JSON data.&quot;&quot;&quot;

data = parse_json(json_data)

content_value = &quot;&quot;



# Collect values of keys that contain &quot;item&quot; in their name

for key, value in data.items():

if &quot;item&quot; in key.lower():

content_value += value + &quot;\n&quot;



return Document(page_content=content_value, metadata={&quot;company&quot;: data[&quot;company&quot;]})





##embed_model_id = 'BAAI/bge-base-en' ## CHANGE



embed_model_id = 'sentence-transformers/all-mpnet-base-v2'







device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu' ## NVIDIA GeForce RTX 2080 TI



embed_model = HuggingFaceEmbeddings(

model_name=embed_model_id,

model_kwargs={'device': device},

encode_kwargs={'device': device, 'batch_size': 32}

)



docs = []





for file in os.listdir(&quot;lessdata&quot;):

if file.endswith(&quot;.json&quot;):

file_path = &quot;./lessdata/&quot;+file

with open(file_path) as file:

json_data = file.read()

document = create_doc(json_data)

docs.append(document)





document_splitter = RecursiveCharacterTextSplitter(separators=['\n'], chunk_size = 500, chunk_overlap = 100)

document_chunks = document_splitter.split_documents(docs)





vectordb = Chroma.from_documents(document_chunks,embedding=embed_model, persist_directory='./database')



##vectordb.persist()

'''

vectordb = Chroma.from_documents(document_chunks,embedding=embed_model, persist_directory='./database')

vectordb.persist('./database')





'''







### PLEASE DO NOT TOUCH THE VSCODE





tokenizer = AutoTokenizer.from_pretrained(&quot;meta-llama/Llama-2-7b-chat-hf&quot;, use_auth_token = True,)





model = AutoModelForCausalLM.from_pretrained(&quot;TheBloke/Llama-2-7b-Chat-GGUF&quot;,

model_file = &quot;llama-2-7b-chat.q4_K_M.gguf&quot;,

device_map ='auto',

torch_dtype = torch.float16,

use_auth_token = True)









'''

model = AutoModelForCausalLM.from_pretrained(&quot;meta-llama/Llama-2-7b-chat-hf&quot;,

device_map ='auto',

torch_dtype = torch.float16,

use_auth_token = True)





'''







pipe = pipeline(&quot;text-generation&quot;,

model = model,

tokenizer = tokenizer,

device_map='auto',

max_new_tokens = 512,

min_new_tokens = 1,

top_k = 5) ##see it



## In vectorstore, take top 5 closest vectors-inputs-contexts, whatever you wanna call.



llm = HuggingFacePipeline(pipeline=pipe, model_kwargs= {'temperature':0.7})



memory = ConversationBufferMemory(memory_key=&quot;chat_history&quot;, input_key='question', output_key='answer', return_messages=True)



system_template = r&quot;&quot;&quot;

Given a context, use your knowledge and answer the question. Be flexible, and try everything to answer in the format asked by query.

----

{context}

----

&quot;&quot;&quot;





user_template = &quot;Question:```{question}```&quot;



messages = [

SystemMessagePromptTemplate.from_template(system_template),

HumanMessagePromptTemplate.from_template(user_template)

]





qa_prompt = ChatPromptTemplate.from_messages(messages)







jsonExpert = ConversationalRetrievalChain.from_llm(llm = llm,

retriever=vectordb.as_retriever(search_kwargs = {'k': 1}), ## whats it

verbose = True, memory = memory, combine_docs_chain_kwargs={'prompt': qa_prompt},

return_source_documents = True

)



##retriever returns 1 output object.



chat_history = []

query = &quot;Consider the financials and progress of companies who is in the tech business.&quot;

result = jsonExpert({&quot;question&quot;: query}, {&quot;chat_history&quot;: chat_history})

#result = jsonExpert({&quot;question&quot;: query})





sources = result[&quot;source_documents&quot;][0]

print(result['answer'])

pprint(sources)

pprint(memory)
</code></pre>
","huggingface"
"77102352","handler.py not executing in HuggingFace Inference Endpoint","2023-09-14 06:35:42","","0","86","<python><huggingface-transformers><huggingface><large-language-model><llama>","<p>I created a repo based on Llama 2 7B Pawel1212/Llama-2-7b-chat-hf and added handler.py but it seems it is not used. I added some prints and additional behaviour but but neither see the prints nor the new behaviour.</p>
<p>I created the endpoint as Custom task.</p>
<p>Do you maybe know what can be a reason?</p>
","huggingface"
"77101192","cannot import name 'randn_tensor' from 'diffusers.utils'","2023-09-14 00:51:19","77101463","5","3635","<python><machine-learning><huggingface><stable-diffusion>","<p><a href=""https://i.sstatic.net/akrEb.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/akrEb.png"" alt=""enter image description here"" /></a></p>
<p>I was using this autotrain collab and when i labbelled and put my images into images folder and tried to run it , It says this error how do i solve this ?</p>
<p>to reproduce :</p>
<ol>
<li><p>click link of ipynb</p>
</li>
<li><p>make a new folder name images</p>
</li>
<li><p>add some images and replace the prompt to something which describes your images</p>
</li>
<li><p>go to runtime and run all</p>
</li>
</ol>
<p><a href=""https://colab.research.google.com/github/huggingface/autotrain-advanced/blob/main/colabs/AutoTrain_Dreambooth.ipynb"" rel=""noreferrer"">ipynb link</a></p>
","huggingface"
"77094291","Model inference using batch","2023-09-13 05:34:02","77095214","2","590","<pytorch><huggingface>","<p>I am trying to get an inference of an image using the Pix2struct vision transformer model. Currently, I am generating one inference at a time and the code I am using is in below.</p>
<pre class=""lang-py prettyprint-override""><code>processor = Pix2StructProcessor.from_pretrained(
    &quot;google/deplot&quot;, is_vqa=True
)
model = Pix2StructForConditionalGeneration.from_pretrained(
    &quot;google/deplot&quot;, is_vqa=True
).to(device)

with open('./data/test_imgs/test.png', &quot;rb&quot;) as f:
    image = Image.open(f).convert(&quot;RGB&quot;)

    inputs = processor(
        images=image,
        text=&quot;Generate underlying data table of the figure below:&quot;,
        return_tensors=&quot;pt&quot;,
    ).to(device)
    predictions = model.generate(**inputs, max_new_tokens=512)
    deplot_result = processor.decode(
            predictions[0], skip_special_tokens=True
    )
    
    print(deplot_result)
</code></pre>
<p>However, the inference time for this method is ~45 secs/image, which is not viable for our project. Is there a way to convert this code into using batches so that I can generate multiple predictions at the same time?</p>
","huggingface"
"77092094","How do I train my NER model using Trainer from huggingface?","2023-09-12 19:17:21","","0","165","<python><tokenize><huggingface-transformers><named-entity-recognition><huggingface>","<p>I'm having trouble using Trainer from HuggingFace to train my NER model.</p>
<p>My data looks like this :</p>
<pre><code>DatasetDict({
    train: Dataset({
        features: ['id', 'tokens', 'ner_tags', '__index_level_0__'],
        num_rows: 985
    })
    val: Dataset({
        features: ['id', 'tokens', 'ner_tags', '__index_level_0__'],
        num_rows: 123
    })
    test: Dataset({
        features: ['id', 'tokens', 'ner_tags', '__index_level_0__'],
        num_rows: 124
    })
})
</code></pre>
<p>First I'm using this code to align the NER tags</p>
<pre><code>def tokenize_and_align_labels(examples):

    tokenized_inputs = tokenizer(examples[&quot;tokens&quot;], padding='max_length', max_length=512, truncation=True, is_split_into_words=True)

    labels = []
    for i, label in enumerate(examples[f&quot;ner_tags&quot;]):

        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.
        previous_word_idx = None
        label_ids = []
      
        for word_idx in word_ids:  # Set the special tokens to -100.
      
            if word_idx is None:
                label_ids.append(-100)
            elif word_idx != previous_word_idx:  # Only label the first token of a given word.
                label_ids.append(label[word_idx])
            else:
                label_ids.append(-100)
      
            previous_word_idx = word_idx
      
        labels.append(label_ids)

    tokenized_inputs[&quot;labels&quot;] = labels
    
    return tokenized_inputs
</code></pre>
<p>next, I tokenize my data and create my DataCollator:</p>
<pre><code>from transformers import DataCollatorForTokenClassification
from transformers import AutoTokenizer, BertTokenizerFast

tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')
tokenized_wnut = my_dataset_dict.map(tokenize_and_align_labels, batched=True)
data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, return_tensors=&quot;tf&quot;)
</code></pre>
<p>and finally, I create my metrics function,</p>
<pre><code>def compute_metrics(p):
    predictions, labels = p
    predictions = np.argmax(predictions, axis=2)

    true_predictions = [
        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    true_labels = [
        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]

    results = seqeval.compute(predictions=true_predictions, references=true_labels)
    return {
        &quot;precision&quot;: results[&quot;overall_precision&quot;],
        &quot;recall&quot;: results[&quot;overall_recall&quot;],
        &quot;f1&quot;: results[&quot;overall_f1&quot;],
        &quot;accuracy&quot;: results[&quot;overall_accuracy&quot;],
    }
</code></pre>
<p>and then define my model and try to train it</p>
<pre><code>model = BertForTokenClassification.from_pretrained(&quot;bert-base-cased&quot;, num_labels=len(label_list))

from transformers import TrainingArguments, Trainer
from transformers import BertForTokenClassification

# Training arguments
training_args = TrainingArguments(
    output_dir=&quot;my_awesome_ner_model&quot;,
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=2,
    weight_decay=0.01,
    evaluation_strategy=&quot;epoch&quot;,
    save_strategy=&quot;epoch&quot;,
    load_best_model_at_end=True,
    push_to_hub=True,
)

# Create the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_wnut[&quot;train&quot;],
    eval_dataset=tokenized_wnut[&quot;val&quot;],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

# Start training
trainer.train()
</code></pre>
<p>But I get this error :</p>
<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-100-fff6be1cc4d7&gt; in &lt;cell line: 30&gt;()
     28 
     29 # Start training
---&gt; 30 trainer.train()

7 frames
/usr/local/lib/python3.10/dist-packages/transformers/trainer.py in train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)
   1542                 # Disable progress bars when uploading models during checkpoints to avoid polluting stdout
   1543                 hf_hub_utils.disable_progress_bars()
-&gt; 1544                 return inner_training_loop(
   1545                     args=args,
   1546                     resume_from_checkpoint=resume_from_checkpoint,

/usr/local/lib/python3.10/dist-packages/transformers/trainer.py in _inner_training_loop(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)
   1833 
   1834                 with self.accelerator.accumulate(model):
-&gt; 1835                     tr_loss_step = self.training_step(model, inputs)
   1836 
   1837                 if (

/usr/local/lib/python3.10/dist-packages/transformers/trainer.py in training_step(self, model, inputs)
   2677 
   2678         with self.compute_loss_context_manager():
-&gt; 2679             loss = self.compute_loss(model, inputs)
   2680 
   2681         if self.args.n_gpu &gt; 1:

/usr/local/lib/python3.10/dist-packages/transformers/trainer.py in compute_loss(self, model, inputs, return_outputs)
   2702         else:
   2703             labels = None
-&gt; 2704         outputs = model(**inputs)
   2705         # Save past state if it exists
   2706         # TODO: this needs to be fixed and made cleaner later.

/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py in _call_impl(self, *args, **kwargs)
   1499                 or _global_backward_pre_hooks or _global_backward_hooks
   1500                 or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1501             return forward_call(*args, **kwargs)
   1502         # Do not call functions when jit is used
   1503         full_backward_hooks, non_full_backward_hooks = [], []

/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py in forward(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)
   1754         return_dict = return_dict if return_dict is not None else self.config.use_return_dict
   1755 
-&gt; 1756         outputs = self.bert(
   1757             input_ids,
   1758             attention_mask=attention_mask,

/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py in _call_impl(self, *args, **kwargs)
   1499                 or _global_backward_pre_hooks or _global_backward_hooks
   1500                 or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1501             return forward_call(*args, **kwargs)
   1502         # Do not call functions when jit is used
   1503         full_backward_hooks, non_full_backward_hooks = [], []

/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py in forward(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)
    968         elif input_ids is not None:
    969             self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)
--&gt; 970             input_shape = input_ids.size()
    971         elif inputs_embeds is not None:
    972             input_shape = inputs_embeds.size()[:-1]

TypeError: 'numpy.int64' object is not callable
</code></pre>
","huggingface"
"77090286","Deploying a model from HuggingFace to Amazon SageMaker: TheBloke/Luna-AI-Llama2-Uncensored-GGML","2023-09-12 14:49:15","","0","565","<nlp><artificial-intelligence><amazon-sagemaker><huggingface><large-language-model>","<p>I'm trying to deploy the following huggingface model to Amazon SageMaker:</p>
<p><a href=""https://huggingface.co/TheBloke/Luna-AI-Llama2-Uncensored-GGML"" rel=""nofollow noreferrer"">https://huggingface.co/TheBloke/Luna-AI-Llama2-Uncensored-GGML</a></p>
<p>I created a domain, launched the studio, and opened a new notebook:</p>
<p>Image: Data Science 3.0</p>
<p>Kernel: Python 3</p>
<p>I tried running the following code:</p>
<pre><code>import json

import sagemaker

import boto3

from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri

try:

role = sagemaker.get_execution_role()

except ValueError:

iam = boto3.client('iam')

role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']

# Hub Model configuration. https://huggingface.co/models

hub = {

'HF_MODEL_ID':'TheBloke/Luna-AI-Llama2-Uncensored-GGML',

'SM_NUM_GPUS': json.dumps(1)

}

# create Hugging Face Model Class

huggingface_model = HuggingFaceModel(

image_uri=get_huggingface_llm_image_uri(&quot;huggingface&quot;,version=&quot;0.9.3&quot;),

env=hub,

role=role,

)

# deploy model to SageMaker Inference

predictor = huggingface_model.deploy(

initial_instance_count=1,

instance_type=&quot;ml.g5.2xlarge&quot;,

container_startup_health_check_timeout=300,

)

# send request

predictor.predict({

&quot;inputs&quot;: &quot;My name is Clara and I am&quot;,

})
</code></pre>
<p>I'm getting the following errors and warning:</p>
<blockquote>
<p><strong>UnexpectedStatusException: Error hosting endpoint huggingface-pytorch-tgi-inference-2023-09-10-11-59-20-948: Failed. Reason: The primary container for production variant AllTraffic did not pass the ping health check. Please check CloudWatch logs for this endpoint..</strong></p>
</blockquote>
<blockquote>
<p>ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
distributed 2022.7.0 requires tornado&lt;6.2,&gt;=6.0.3, but you have tornado 6.3.2 which is incompatible.</p>
</blockquote>
<blockquote>
<p>WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead:<a href=""https://pip.pypa.io/warnings/venv"" rel=""nofollow noreferrer""> https://pip.pypa.io/warnings/venv</a></p>
</blockquote>
<p>I checked the CloudWatch logs following the instructions in first error, and I found many DownloadError logs for different files. For example:</p>
<blockquote>
<p>Error: DownloadError File &quot;/opt/conda/bin/text-generation-server&quot;, line 8, in  sys.exit(app()) File &quot;/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py&quot;, line 182, in download_weights utils.convert_files(local_pt_files, local_st_files, discard_names) File &quot;/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/convert.py&quot;, line 106, in convert_files convert_file(pt_file, sf_file, discard_names) File &quot;/opt/conda/lib/python3.9/site-packages/text_generation_server/utils/convert.py&quot;, line 65, in convert_file loaded = torch.load(pt_file, map_location=&quot;cpu&quot;) File &quot;/opt/conda/lib/python3.9/site-packages/torch/serialization.py&quot;, line 815, in load return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args) File &quot;/opt/conda/lib/python3.9/site-packages/torch/serialization.py&quot;, line 1033, in _legacy_load magic_number = pickle_module.load(f, **pickle_load_args)</p>
</blockquote>
<blockquote>
<p>2023-09-12T02:08:45.377+08:00 _pickle.UnpicklingError: could not find MARK</p>
</blockquote>
","huggingface"
"77085586","Inconsistent output from get_image_features() in BLIP model—how to get consistent results?","2023-09-12 01:22:55","","1","336","<python><feature-extraction><huggingface>","<p>I am currently using the BLIP model to get image embeddings via its get_image_features() method. However, every time I reload the model, this method returns different values for the same input. Is there a way to obtain consistent results each time the model is loaded?</p>
<p>Any guidance would be appreciated.</p>
<p>This is my code snippet.</p>
<pre class=""lang-py prettyprint-override""><code>from PIL import Image
from transformers import AutoProcessor, TFBlipModel
model = TFBlipModel.from_pretrained(&quot;Salesforce/blip-image-captioning-base&quot;)
processor = AutoProcessor.from_pretrained(&quot;salesforce/blip-image-captioning-base&quot;)

image = Image.open(img_path)
inputs = processor(images=image, return_tensors=&quot;tf&quot;)
image_features = model.get_image_features(**inputs).numpy().squeeze()
</code></pre>
","huggingface"
"77083684","Hugging Face Evaluator Module - Evaluation Speed seems very slow?","2023-09-11 17:24:42","","1","332","<python><tensorflow><huggingface-transformers><huggingface>","<p>I've been learning how to fine-train HuggingFace text classification models using their tutorials and some other information online.</p>
<p>So far I've managed to train a model on a custom dataset - import as dataframe, then convert to datasets object with train and test splits and then converted to a tensorflow dataset object using <code>model.prepare_tf_dataset</code>. My dataset has two labels - true and false.</p>
<p>So far it seems to work and I can see a variety of metrics - accuracy, recall, f1 - in Tensorboard using the HuggingFace metrics callbacks. However, it seems that these only apply to the training dataset, but I'd like to see these for the validation dataset after the model has trained.</p>
<p>Looking at their tutorials again, they have the Evaluator class and have this code which seems to do what I need.</p>
<p>Code:</p>
<pre><code># 3. Pass an instantiated pipeline
pipe = pipeline(&quot;text-classification&quot;, model=&quot;lvwerra/distilbert-imdb&quot;)

eval_results = task_evaluator.compute(
    model_or_pipeline=pipe,
    data=data,
    label_mapping={&quot;NEGATIVE&quot;: 0, &quot;POSITIVE&quot;: 1}
)
print(eval_results)
</code></pre>
<p>Output:</p>
<pre><code>{
    'accuracy': 0.918,
    'latency_in_seconds': 0.013,
    'samples_per_second': 78.887,
    'total_time_in_seconds': 12.676
}
</code></pre>
<p>Running this specific code takes about 7 minutes with the Google Colab V100 GPU. On my own dataset - where the test dataset is about 3k rows - it takes significantly longer (not 100% sure yet as it hasn't finished).</p>
<p>Am I doing something wrong here, or is there a way to speed this up?</p>
<p>Sorry if I'm not clear or missing info, I'm still quite new to this. But in summary, I'd like the accuracy, recall, f1 metrics when I apply my model to the test set of data so that I can evaluate it's predictive performance.</p>
<p>It might be worth mentioning that I haven't used the trainer class for this.</p>
","huggingface"
"77070709","Langchain with Chroma vectorstore - why will my files not push to my database folder?","2023-09-09 03:14:42","","0","671","<langchain><huggingface><chromadb>","<p>I am building a HuggingFace Space with Langchain (Gradio SDK) to chat my data, using Chroma for the vectorstore.</p>
<p>My app runs perfectly in my space and I can tell it is answering queries accurately according to our data. However, no files are persisted into my database folder.
So it is costing much more than desired.</p>
<p>Here is my code regarding the Chroma bit, ingesting files from folder &quot;data&quot; and saving to folder &quot;vectorstore&quot;:</p>
<pre><code>from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import DirectoryLoader
from langchain.vectorstores import Chroma
import chromadb

# Load Data
loader = DirectoryLoader('./data/')
raw_documents = loader.load()  

# Split text
text_splitter = RecursiveCharacterTextSplitter()
documents = text_splitter.split_documents(raw_documents)


# Save Data to vectorstore
persist_directory = &quot;./vectorstore&quot;
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(documents=documents, embedding=embeddings, persist_directory=persist_directory)
vectorstore.persist()
</code></pre>
<p>I can't figure out why I'm getting this problem. Am I missing something simple?</p>
<p>Thanks in advance for any insight!</p>
","huggingface"
"77069008","Llama 70b on Hugging Face Inference API Endpoint short responses","2023-09-08 17:59:59","77074597","0","841","<python><huggingface><llama>","<p>I just deployed the Nous-Hermes-Llama2-70b parameter on a 2x Nvidia A100 GPU through the Hugging Face Inference endpoints.</p>
<p>When I tried the following code, the response generations were incomplete sentences that were less than 1 line long.</p>
<pre><code>import requests

API_URL = 'https://myendpoint.us-east-1.aws.endpoints.huggingface.cloud'
headers = {
  &quot;Authorization&quot;: &quot;Bearer mytoken1234&quot;,
  &quot;Content-Type&quot;: &quot;application/json&quot;
}

def query(payload):
  response = requests.post(API_URL, headers=headers, json=payload)
  return response.json()
 
output = query({
  &quot;inputs&quot;: &quot;### Instruction:\r\nCome up with a joke about cats\r\n### Response:\r\n&quot;,
})
</code></pre>
<p>The output in this case was:</p>
<pre><code>&quot;Why don't cats play poker in the jungle?

 Because &quot;
</code></pre>
<p>As you see, the response stopped after 9 words.</p>
<p>Do I need to add more headers to the request like temperature and max token length? How would I do that? What do I need to do to get normal, long responses?</p>
<p>Here is the model I'm using: <a href=""https://huggingface.co/NousResearch/Nous-Hermes-Llama2-70b"" rel=""nofollow noreferrer"">https://huggingface.co/NousResearch/Nous-Hermes-Llama2-70b</a></p>
","huggingface"
"77064065","Loading checkpoint shards takes too long","2023-09-08 04:01:47","","9","14849","<huggingface-transformers><h2o><huggingface><huggingface-tokenizers><llama>","<p>I'm very new to generative AI. I have 64gb RAM and 20GB GPU. I used some opensource model from Huggingface and used Python to simply prompt it with out of box model and displaying the result. I downloaded the model to local using <code>save_pretrained</code> and trying to load the model from local there after. It works. But everytime I run the python file it takes more than 10 mins to display the results.</p>
<p>There is a step <code>Loading checkpoint shards</code> that takes 6-7 mins everytime. Am I doing anything wrong? why it has to load something everytime even though the model is refered from local.</p>
<p>I tried using <code>local_files_only=True, cache_dir=cache_dir, low_cpu_mem_usage=True, max_shard_size=&quot;200MB&quot;</code> , none solved the time issue .</p>
<p>How to prompt the saved model directly without so much delay as user usable. Any help would be highly appreciated</p>
","huggingface"
"77063350","Huggingface model.generate iterative 1 token (batch_size = 1)","2023-09-07 23:23:23","","0","199","<huggingface-transformers><huggingface><large-language-model>","<p>I'm trying to understand the difference between using model.generate(input_id, attention_mask, max_new_tokens=200, min_length=15), and</p>
<pre><code>while count &lt; 200:
output = model.generate(input_id, attention_mask, max_new_tokens=1, min_length=0)
input_id = output
count +=1
</code></pre>
<p>The stopping criteria can be made the same, including checking for EOS token_id. I understand that the first method would outperform as batch_size &gt; 1, but for batch_size = 1, why is method 2 so much more slower than method 1? I can pass use_cache=True, but that did not help</p>
","huggingface"
"77061977","How to make the simplest minimal huggingface tokenizer?","2023-09-07 18:15:01","","0","361","<pytest><tokenize><huggingface>","<p>I would like to make the simplest possible huggingface tokenizer for unit testing code logic. It doesnt have to work well, just produce an output similar to any other tokenizer.</p>
<p>Reading the <a href=""https://huggingface.co/learn/nlp-course/chapter6/8?fw=pt"" rel=""nofollow noreferrer"">course on building a tokenizer</a> it seems I could do the following</p>
<pre class=""lang-py prettyprint-override""><code>import tokenizers
tokenizer = tokenizers.Tokenizer(tokenizers.models.WordPiece(unk_token=&quot;[UNK]&quot;))
</code></pre>
<p>But if I try to use it, I get a exception</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; tokenizer.encode(&quot;this is a test text&quot;)
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
Exception: WordPiece error: Missing [UNK] token from the vocabulary

</code></pre>
<p>The goal is to use the tokenizer in a <code>pytest</code> session, so I want it to be lightweight and not download anything pretrained.</p>
","huggingface"
"77061898","Incomplete Output with LLM with max_new_tokens","2023-09-07 18:02:00","","2","2576","<machine-learning><huggingface-transformers><transformer-model><huggingface><large-language-model>","<p>I am experimenting with Huggingface LLM models.</p>
<p>And one issue I noticed is that output of the model ends abruptly and I ideally want it to complete the paragraph/sentences/code which it was it between of. (or altogether try to complete the answer within some fixed num of tokens)</p>
<p>Although I have provided max_new_tokens = 300 and also in prompt I write:
&quot;Output should be maximum of 300 words.&quot;</p>
<p>The response is always incomplete and ends abruptly. Any way I can ask for a complete output within desired number of output tokens?</p>
<p>Code:</p>
<pre><code>checkpoint = &quot;HuggingFaceH4/starchat-alpha&quot;
device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot; 
class StarCoderModel:
  def __init__(self):
    self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)
    # make sure `--gpus all` is provided in docker run command if gpu is required
    self.model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map='auto')

  def infer(self, input_text, token_count):
    inputs = self.tokenizer.encode(input_text, return_tensors=&quot;pt&quot;).to(device)
    outputs = self.model.generate(inputs,  max_new_tokens=token_count, pad_token_id=self.tokenizer.eos_token_id)
    return self.tokenizer.decode(outputs[0])[len(input_text):]
</code></pre>
<p>Sample-Output:</p>
<pre><code>private DataType FuntionName(String someId) {
    // TODO: Replace with implementation that utilizes someId to obtain information
    return DataType.Value;
}


The comment:

- If someId is present in the code, use the getAPI from Client with someId as a parameter to obtain some information.
- If the

</code></pre>
","huggingface"
"77060997","Dynamic Image Ouputs in Gradio","2023-09-07 15:34:35","","3","1456","<python><image><visualization><huggingface><gradio>","<p>I want to create a Gradio Application which takes in a query as the input and displays image content in text outputs and corresponding Images in image outputs.
You can assume there will always be 4 results to output.</p>
<p>My code looks like this:</p>
<pre><code>     def gradio_fn(query):
        content, image_paths = answer_question(query) #This is where the image paths are generated
        return content
    
    output_texts = [gr.Textbox() for i in range(4)]    
    input_text = gr.Textbox()
    demo = gr.Interface(
    fn=gradio_fn,
    inputs=input_text,
    outputs=output_texts
)


demo.launch(share = True)
</code></pre>
<p>where 'image_paths' are paths to the images that i want to output
e.g. '/dbfs/FileStore/d/llm_0.png'</p>
<p>Currently I've only tried to output the text i.e. 'content' and its working (See picture below)</p>
<p><a href=""https://i.sstatic.net/zM8k6.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/zM8k6.png"" alt=""enter image description here"" /></a></p>
<p>Now I want to add image outputs next to the text outputs. From my understanding i need to pass the image path to the gradio image function e.g. gr.Image(value= '/dbfs/FileStore/d/llm_0.png')</p>
<p>How can I achieve that when my image paths are generated only after the submit button is clicked?</p>
","huggingface"
"77058708","Cannot find def of .has_func or .get_func","2023-09-07 10:20:16","77059076","1","48","<rust><huggingface>","<p>I was reading through the code for HuggingFace's candle crate and found two methods whose definitions I couldn't find. The first is <a href=""https://github.com/huggingface/candle/blob/8c991df3945a7c86ae86a7a52a74639ec321cef2/candle-core/src/cuda_backend.rs#L190"" rel=""nofollow noreferrer"">.has_func</a> and the second is <a href=""https://github.com/huggingface/candle/blob/8c991df3945a7c86ae86a7a52a74639ec321cef2/candle-core/src/cuda_backend.rs#L201"" rel=""nofollow noreferrer"">.get_func</a>.</p>
<p>How can I find the definitions of these methods?</p>
<p>Here's what I have tried:</p>
<ul>
<li>Searching for the string &quot;has_func&quot; in the repo</li>
<li>Googling &quot;rust has_func&quot; and checking if any of the results are relevant.</li>
<li>Checking if hovering in Github can point me to the def</li>
</ul>
<p>It raised a more general question in my mind, I wanted to enumerate all the places a method could come from for a struct defined in a given crate, here's what I can think of:</p>
<ul>
<li>The method is defined somewhere in that crate (possibly obscured if defined in a macro)</li>
<li>It could be the default definition of a method in a trait that the struct has and the trait is defined in some dependency.</li>
<li>Something else? Maybe FFI related?</li>
</ul>
","huggingface"
"77054951","Docker daemon failed to create task for container","2023-09-06 20:01:31","","0","241","<docker><ubuntu><huggingface><alpaca>","<p>I'm new to both Docker and machine learning. I'm trying to run image of <a href=""https://github.com/tloen/alpaca-lora"" rel=""nofollow noreferrer"">alpaca_lora</a> project. I don't know what exactly this problem means. I have Ubuntu 20, I installed Docker Desktop then <code>docker-ce</code> with <code>apt</code> to have docker deamon. Hello-World docker image work fine.</p>
<pre class=""lang-bash prettyprint-override""><code>docker run --gpus=all --shm-size 64g -p 7860:7860 -v ${HOME}/.cache:/root/.cache --rm alpaca-lora generate.py \
    --load_8bit \
    --base_model '/var/llama/models/llama-7b-hf/' \
    --lora_weights '/var/llama/generations/Demo_Video_ExpansIA_original'
</code></pre>
<p>Output :</p>
<pre><code>docker: Error response from daemon: failed to create task for container: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error running hook #0: error running hook: exit status 1, stdout: , stderr: Auto-detected mode as 'legacy'
nvidia-container-cli: initialization error: load library failed: libnvidia-ml.so.1: cannot open shared object file: no such file or directory: unknown.
</code></pre>
","huggingface"
"77047800","Transformers.js in React.js","2023-09-05 21:17:49","77099248","3","1690","<javascript><reactjs><huggingface-transformers><huggingface>","<p>I'm building a component in React and I want to use a model from huggingface. I found the package @xenova/transformers that allows to use of these models in JavaScript, but when I try to create my <strong>pipeline instance</strong> like this:</p>
<pre><code>class MyExtractorPipeline {
    static task = &quot;feature-extraction&quot;;
    static model = &quot;Xenova/all-MiniLM-L6-v2&quot;;
    static instance = null;

    static async getInstance(model, progress_callback = null) {
        if (this.instance === null) {
            try {
                this.instance = await pipeline(this.task, model, { progress_callback }); //&lt;= THIS IS FAILING
            } catch (error) {
                throw new Error(error)
            }
        }
        return this.instance;
    }
}
</code></pre>
<p><strong>I get this error:</strong></p>
<p>worker.js:17 Uncaught (in promise) Error: SyntaxError: Unexpected token '&lt;', &quot;&lt;!DOCTYPE &quot;... is not valid JSON
at MyExtractorPipeline.getInstance (worker.js:17:1)
at async worker.js:77:1</p>
<p><strong>I'm sure it comes from the 'throw new Error(error)' that you can see above,</strong> but I don't know how to solve it, I need HELP.</p>
<p>I tried to do console.log() of almost all the variables, and I tried to use other hugginface models...
Anything has been useful.</p>
","huggingface"
"77045332","Why isnt this huggingface inference endpoint looping through my local files in a folder","2023-09-05 14:20:35","","0","105","<huggingface-transformers><huggingface><llama><huggingface-hub>","<p>The LLM is Llama2 7b, it does a loop but it makes up its own emails, its not reading or looping through the specific path i want, ive looked everywhere for an answer but canr seem to find one.</p>
<p>Could someone know how to fix it so it will loop through and read my local text files on my local machine</p>
<pre><code>import os
import glob
from huggingface_hub import InferenceClient

folder_path = '
text_files = glob.glob(os.path.join(folder_path, &quot;*.txt&quot;))
print(f&quot;Text files found: {text_files}&quot;)

endpoint_url = &quot;&quot;
hf_token = &quot;&quot;

def read_file(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        content = file.read()
         return content

client = InferenceClient(endpoint_url, token=hf_token)

gen_kwargs = dict(max_new_tokens=512, top_k=30, top_p=0.9, temperature=0.2, repetition_penalty=1.02, stop_sequences=[&quot;\nUser:&quot;, &quot;&lt;/s&gt;&quot;])

fixed_prompt = &quot;&quot;&quot;You are a cybersecurity phishing expert. In every future output can you tell me if the email is &quot;phish&quot; or &quot;clean&quot;? be binary.&quot;&quot;&quot;

file_list = glob.glob(os.path.join(folder_path, '*.txt'))
print(f&quot;Text files found: {file_list}&quot;)

for filename in file_list:
    # Open the file and read its contents
    with open(filename, 'r', encoding='utf-8') as file:
        content = file.read()
    
    # Fill in the fixed prompt with the file content
    prompt = fixed_prompt + f&quot;\n{content}&quot;
    
    # Generate text based on the prompt
    stream = client.text_generation(prompt, stream=True, details=True, **gen_kwargs)
    
    # Print the generated text
    for r in stream:
        print(r.token.text, end='')

print(&quot;\n&quot;)
</code></pre>
<p>Tried to use the glob library but its not working, no silly answers like have you checked your path, please :)</p>
","huggingface"
"77044747","Why does Hugging Face's push_to_hub convert saved models to .bin instead of using safetensor mode?","2023-09-05 13:02:40","","3","1391","<machine-learning><pytorch><huggingface-transformers><huggingface><llama>","<p>I am attempting to push a saved model in <code>model-00001-of-00006.safetensors</code> mode, but the model gets converted to <code>pytorch_model-00001-of-00006.bin</code> before being saved to the hub.</p>
<p>How can I prevent this?</p>
<p>Here is what I am simply doing:</p>
<pre><code>model.push_to_hub('chukypedro/' + new_model, use_temp_dir=False)
tokenizer.push_to_hub('chukypedro/' + new_model, use_temp_dir=False)
</code></pre>
<p><a href=""https://i.sstatic.net/1HWHN.png"" rel=""nofollow noreferrer"">image</a></p>
<pre><code>model.push_to_hub('chukypedro/' + new_model, use_temp_dir=False)
tokenizer.push_to_hub('chukypedro/' + new_model, use_temp_dir=False)
</code></pre>
<p>I am expecting the model to be pushed in safetensors mode.</p>
","huggingface"
"77036063","TypeError: deprecate() got an unexpected keyword argument 'message'","2023-09-04 08:05:17","","0","666","<python><huggingface><stable-diffusion>","<p>Error details:</p>
<pre class=""lang-none prettyprint-override""><code>Fetching 16 files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00&lt;00:00, 66117.11it/s]
  0%|                                                                                                                                          | 0/50 [00:01&lt;?, ?it/s]
Traceback (mo`your text`st recent call last):
  File &quot;/data1/wz/research/pix2pix-zero/src/inversion.py&quot;, line 58, in &lt;module&gt;
    x_inv, x_inv_image, x_dec_img = pipe(
                                    ^^^^^
  File &quot;/data1/wz/research/pix2pix-zero/src/utils/ddim_inv.py&quot;, line 106, in __call__
    noise_pred = self.unet(latent_model_input,t,encoder_hidden_states=prompt_embeds,cross_attention_kwargs=cross_attention_kwargs,).sample
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/data1/wz/anaconda3/envs/pix2pix-zero/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/data1/wz/anaconda3/envs/pix2pix-zero/lib/python3.11/site-packages/diffusers/models/unet_2d_condition.py&quot;, line 582, in forward
    sample, res_samples = downsample_block(
                          ^^^^^^^^^^^^^^^^^
  File &quot;/data1/wz/anaconda3/envs/pix2pix-zero/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/data1/wz/anaconda3/envs/pix2pix-zero/lib/python3.11/site-packages/diffusers/models/unet_2d_blocks.py&quot;, line 837, in forward
    hidden_states = attn(
                    ^^^^^
  File &quot;/data1/wz/anaconda3/envs/pix2pix-zero/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/data1/wz/anaconda3/envs/pix2pix-zero/lib/python3.11/site-packages/diffusers/models/transformer_2d.py&quot;, line 265, in forward
    hidden_states = block(
                    ^^^^^^
  File &quot;/data1/wz/anaconda3/envs/pix2pix-zero/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/data1/wz/anaconda3/envs/pix2pix-zero/lib/python3.11/site-packages/diffusers/models/attention.py&quot;, line 291, in forward
    attn_output = self.attn1(
                  ^^^^^^^^^^^
  File &quot;/data1/wz/anaconda3/envs/pix2pix-zero/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/data1/wz/anaconda3/envs/pix2pix-zero/lib/python3.11/site-packages/diffusers/models/cross_attention.py&quot;, line 205, in forward
    return self.processor(
           ^^^^^^^^^^^^^^^
  File &quot;/data1/wz/research/pix2pix-zero/src/utils/cross_attention.py&quot;, line 7, in __call__
    attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/data1/wz/anaconda3/envs/pix2pix-zero/lib/python3.11/site-packages/diffusers/models/cross_attention.py&quot;, line 260, in prepare_attention_mask
    deprecate(
TypeError: deprecate() got an unexpected keyword argument 'message'
</code></pre>
<p>This is the function in <code>cross_attention</code></p>
<pre class=""lang-py prettyprint-override""><code> def prepare_attention_mask(self, attention_mask, target_length, batch_size=None):
        if batch_size is None:
            deprecate(
                &quot;batch_size=None&quot;,
                &quot;0.0.15&quot;,
                message=(
                    &quot;Not passing the `batch_size` parameter to `prepare_attention_mask` can lead to incorrect&quot;
                    &quot; attention mask preparation and is deprecated behavior. Please make sure to pass `batch_size` to&quot;
                    &quot; `prepare_attention_mask` when preparing the attention_mask.&quot;
                ),
            )
            batch_size = 1

        head_size = self.heads
        if attention_mask is None:
            return attention_mask

        if attention_mask.shape[-1] != target_length:
            if attention_mask.device.type == &quot;mps&quot;:
                # HACK: MPS: Does not support padding by greater than dimension of input tensor.
                # Instead, we can manually construct the padding tensor.
                padding_shape = (attention_mask.shape[0], attention_mask.shape[1], target_length)
                padding = torch.zeros(padding_shape, dtype=attention_mask.dtype, device=attention_mask.device)
                attention_mask = torch.cat([attention_mask, padding], dim=2)
            else:
                attention_mask = F.pad(attention_mask, (0, target_length), value=0.0)

        if attention_mask.shape[0] &lt; batch_size * head_size:
            attention_mask = attention_mask.repeat_interleave(head_size, dim=0)
        return attention_mask
</code></pre>
<p>My python version is 3.11, is that version incorrect or should something else be done to fix the error?</p>
","huggingface"
"77035723","ConnectionError: HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Read timed out","2023-09-04 07:08:36","","1","1259","<nlp><huggingface-transformers><huggingface><huggingface-tokenizers><huggingface-hub>","<p>While running the code, model stops loading in between and gives the following error.</p>
<pre><code>    from getpass import getpass
    import os
    HUGGINGFACE_API_TOKEN = getpass()
    os.environ[HUGGINGFACE_API_TOKEN] = HUGGINGFACE_API_TOKEN
    from transformers import AutoTokenizer, AutoModelForCausalLM
    from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
    tokenizer = AutoTokenizer.from_pretrained(&quot;meta-llama/Llama-2-7b-chat-hf&quot;)
    model = AutoModelForCausalLM.from_pretrained(&quot;meta-llama/Llama-2-7b-chat-hf&quot;)

</code></pre>
<h2>Downloading shards: 0%
0/2 [07:07&lt;?, ?it/s]
Downloading (…)of-00002.safetensors: 29%
2.86G/9.98G [07:03&lt;17:15, 6.87MB/s]</h2>
<p>TimeoutError                              Traceback (most recent call last)
File ~/.local/lib/python3.10/site-packages/urllib3/response.py:438, in HTTPResponse._error_catcher(self)
437 try:
--&gt; 438     yield
440 except SocketTimeout:
441     # FIXME: Ideally we'd like to include the url in the ReadTimeoutError but
442     # there is yet no clean way to get at it from this context.</p>
<p>File ~/.local/lib/python3.10/site-packages/requests/models.py:822, in Response.iter_content..generate()
820     raise ContentDecodingError(e)
821 except ReadTimeoutError as e:
--&gt; 822     raise ConnectionError(e)
823 except SSLError as e:
824     raise RequestsSSLError(e)</p>
<p>ConnectionError: HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Read timed out.</p>
<p>Can Someone Help me to load the model?</p>
","huggingface"
"77034518","Modifying the forward function in llama2 model","2023-09-03 23:56:46","","0","153","<pytorch><huggingface><large-language-model><llama>","<p>I want to create a smaller llama2 model. Particularly, I’d like to reduce the number of layers and add skip connections between specific layers. Can anyone help me to do this properly either in pytorch or hugginface format?</p>
","huggingface"
"77034085","How to identify the the words in the sentence that contribute to the particular sentiment?","2023-09-03 20:35:30","","2","229","<python><huggingface-transformers><sentiment-analysis><huggingface>","<p>So I am using pretrained tranformers model to perform sentimental analysis but I want to check what are the words which leads to that particular sentiment class. Can anyone please guide me on what should be my approach?
Below is the code I am using</p>
<pre><code>from transformers import AutoTokenizer
from transformers import pipeline
from transformers import AutoModelForSequenceClassification
MODEL = &quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;
tokenizer = AutoTokenizer.from_pretrained(MODEL)
model = AutoModelForSequenceClassification.from_pretrained(MODEL)
tokens = tokenizer.encode_plus(txt, add_special_tokens=False,
                           return_tensors='pt')
outputs = model(**tokens)

probs = torch.nn.functional.softmax(outputs[0], dim=-1)
probs = probs.mean(dim=0)
&quot;&quot;&quot;in this model the range is from 1-5,
1-highly neg
2-negative
3-neutral
4-positive
5-higly pos
so here we will find the max from tensor and its index so we can map our sentiment&quot;&quot;&quot;
sen_dict= {1:'Highly-negative',2:'Negative',3:'Neutral',4:'Positive',5:'Highly-positive'}
sen_idx = probs.max(dim=0).indices.item() + 1 #added 1 as the range here is 0-4 and we want to map it between 1-5
sentiment = sen_dict[sen_idx]
print(sentiment)
</code></pre>
","huggingface"
"77027977","Pipeline cannot infer suitable model classes from: <model_name> - HuggingFace","2023-09-02 10:48:54","","1","449","<nlp><huggingface-transformers><huggingface>","<p>When I try to use the inference pipeline widget for my private project, I got this error.
<a href=""https://i.sstatic.net/ufClr.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ufClr.png"" alt=""enter image description here"" /></a></p>
<p>this is seq2seq model, that receive a sentence or list of sentences and attention array for each sentence,
and return 3 classification for each char in the sentence, how can I create a pipeline for this model?</p>
<p>link to huggingface hub:
<a href=""https://huggingface.co/NadavShaked/d_nikud23?text=My+name+is+Sarah+and+I+live+in+London"" rel=""nofollow noreferrer"">https://huggingface.co/NadavShaked/d_nikud23?text=My+name+is+Sarah+and+I+live+in+London</a></p>
<p>link to model and code in git:
<a href=""https://github.com/NadavShaked/D_Nikud"" rel=""nofollow noreferrer"">https://github.com/NadavShaked/D_Nikud</a></p>
","huggingface"
"77026007","How to specify a `huggingface` model version in the `requirements.tx`","2023-09-01 21:11:10","","0","328","<pip><pipenv><requirements.txt><huggingface>","<p>I want to specify a give <code>huggingface</code> model version in the <code>requirements.txt</code>, say just to give a concrete example, this particular commit.</p>
<pre><code>https://huggingface.co/facebook/bart-large-mnli/commit/df7df4d2107d3882675bfd5ad50db1aa8dd7d7be
</code></pre>
<p>How do I do that? and as a bonus, how do I do that in <code>pipenv</code>?</p>
","huggingface"
"77020278","How to load a huggingface dataset from local path?","2023-09-01 03:03:06","77020527","5","15251","<python><huggingface><huggingface-datasets><huggingface-hub>","<p>Take a simple example in this website, <a href=""https://huggingface.co/datasets/Dahoas/rm-static"" rel=""nofollow noreferrer"">https://huggingface.co/datasets/Dahoas/rm-static</a>:</p>
<p>if I want to load this dataset online, I just directly use,</p>
<pre><code>from datasets import load_dataset
dataset = load_dataset(&quot;Dahoas/rm-static&quot;) 
</code></pre>
<p>What if I want to load dataset from local path, so I download the files and keep the same folder structure from web <code>Files and versions</code> fristly,</p>
<pre><code>-data
|-test-00000-of-00001-bf4c733542e35fcb.parquet
|-train-00000-of-00001-2a1df75c6bce91ab.parquet
-.gitattributes
-README.md
-dataset_infos.json
</code></pre>
<p>Then, put them into my folder, but shows error when loading:</p>
<pre><code>dataset_path =&quot;/data/coco/dataset/Dahoas/rm-static&quot;
tmp_dataset = load_dataset(dataset_path)
</code></pre>
<p>It shows <code>FileNotFoundError: No (supported) data files or dataset script found in /data/coco/dataset/Dahoas/rm-static.</code></p>
","huggingface"
"77015747","how can I find the list of all the environment variables supported in a HuggingFace model?","2023-08-31 12:16:23","","0","183","<huggingface-transformers><huggingface><huggingface-hub>","<p>How can I find the list of all the environment variables supported in a HuggingFace model?</p>
<p>E.g. on <a href=""https://huggingface.co/tiiuae/falcon-40b-instruct"" rel=""nofollow noreferrer"">https://huggingface.co/tiiuae/falcon-40b-instruct</a> model, based on the comment provided in this post: <a href=""https://discuss.huggingface.co/t/cpu-memory-utilization-too-high-when-running-inference-on-falcon-40b-instruct/44200/2"" rel=""nofollow noreferrer"">https://discuss.huggingface.co/t/cpu-memory-utilization-too-high-when-running-inference-on-falcon-40b-instruct/44200/2</a></p>
<p>I see that the model supports environment variables for defining</p>
<pre><code>MAX_CONCURRENT_REQUESTS(default 128)
MAX_INPUT_LENGTH(default 1000)
MAX_TOTAL_TOKENS (default 1512)
MAX_BATCH_SIZE (default none)
</code></pre>
","huggingface"
"77011912","HugginFace Transformers implementation of Bark Model: Getting the Fine Tokens","2023-08-30 22:35:59","","0","166","<deep-learning><nlp><text-to-speech><huggingface-transformers><huggingface>","<p>I would need some help to get the fine tokens from the output of the BarkFineModel (see <a href=""https://huggingface.co/docs/transformers/v4.32.1/model_doc/bark#transformers.BarkFineModel"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/v4.32.1/model_doc/bark#transformers.BarkFineModel</a>) I cannot get it to work. I got to tokenizer -&gt; semantic_model -&gt; semantic_tokens -&gt; coarse_tokens -&gt; fine_model ... fails with a</p>
<pre><code>tokenizer = AutoTokenizer.from_pretrained(&quot;suno/bark-small&quot;)
inputs = tokenizer(&quot;some text prompt&quot;)
tokenizer_input_ids_tensor = inputs['input_ids'][:,0]
semantic_tokens = semantic_model.forward(tokenizer_input_ids_tensor)
coarse_tokens = coarse_model(tokenizer_input_ids_tensor)
coarse_input_ids = torch.tensor(coarse_tokens['logits'].unsqueeze(-1)).to(torch.long)
fine_tokens = fine_model(codebook_idx=1, input_ids=coarse_input_ids) # this fails ! 
</code></pre>
<p>Would anyone try to understand by what is needed by the HugginFace documentation to make the BarkFineModel forward method. I am deeply stuck here... Any help would be very appreciated !</p>
","huggingface"
"77010652","HuggingFace Space with Langchain and Gradio SDK: Why am I getting a FAISS-related error when I'm using Chroma, not FAISS?","2023-08-30 18:28:42","","1","230","<openai-api><langchain><huggingface><faiss><chromadb>","<p>I am building a HuggingFace Space with Langchain (Gradio SDK) to chat my data, cloning from <a href=""https://huggingface.co/spaces/hwchase17/chat-your-data-state-of-the-union"" rel=""nofollow noreferrer"">Harrison Chase's Chat Your Data</a> space and going from there. Fixed a deprecation issue (see <a href=""https://huggingface.co/spaces/hwchase17/chat-your-data-state-of-the-union/discussions/2"" rel=""nofollow noreferrer"">Discussion</a>), switched to a DirectoryLoader so I can ingest multiple files, and want to use Chroma instead of FAISS.</p>
<p>I'm pretty new to this, so I'm trying to do as little changes as possible and want to keep using pickle as the original does, but use Chroma for the embedding rather than FAISS, like so when ingesting data:</p>
<pre><code>from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import DirectoryLoader
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
import pickle

# Load Data
loader = DirectoryLoader('./data/')
raw_documents = loader.load()  

# Split text
text_splitter = RecursiveCharacterTextSplitter()
documents = text_splitter.split_documents(raw_documents)


# Load Data to vectorstore
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(documents=documents, embedding=embeddings)

# Save vectorstore
with open(&quot;vectorstore.pkl&quot;, &quot;wb&quot;) as f:
    pickle.dump(vectorstore, f)
</code></pre>
<p>This shouldn't be a problem, right? Or is pickle totally unnecessary? Anyways, my main question is below.</p>
<p>My Space runs but I get the error <code>AttributeError: 'OpenAIEmbeddings' object has no attribute 'deployment'</code> when I go to finally chat my data. Here is the log:</p>
<pre><code>...
  File &quot;/home/user/.local/lib/python3.11/site-packages/langchain/vectorstores/faiss.py&quot;, line 334, in similarity_search
    docs_and_scores = self.similarity_search_with_score(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/home/user/.local/lib/python3.11/site-packages/langchain/vectorstores/faiss.py&quot;, line 275, in similarity_search_with_score
    embedding = self.embedding_function(query)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/home/user/.local/lib/python3.11/site-packages/langchain/embeddings/openai.py&quot;, line 506, in embed_query
    return self.embed_documents([text])[0]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/home/user/.local/lib/python3.11/site-packages/langchain/embeddings/openai.py&quot;, line 478, in embed_documents
    return self._get_len_safe_embeddings(texts, engine=self.deployment)
                                                       ^^^^^^^^^^^^^^^
AttributeError: 'OpenAIEmbeddings' object has no attribute 'deployment'
</code></pre>
<p>I see the log indicates it's using FAISS (<code>.../langchain/vectorstores/faiss.py</code>), but I'm not even using FAISS, I'm using Chroma. This same error is given even when I am using FAISS, which is why I thought switching to Chroma altogether might solve the issue.</p>
<p>Then, when I remove <code>faiss-cpu</code> from my <code>requirements.txt</code> file, the Space no longer runs and I get the error</p>
<pre><code>Traceback (most recent call last):
  File &quot;/home/user/app/app.py&quot;, line 17, in &lt;module&gt;
    vectorstore = pickle.load(f)
                  ^^^^^^^^^^^^^^
ModuleNotFoundError: No module named 'faiss'
</code></pre>
<p>Does anyone know why this is happening? Again, I'm very novice so I could be missing something basic. Should I get rid of pickle and use Chroma with a persist directory?</p>
<p>Thanks in advance.</p>
","huggingface"
"77008549","Very bad Zero Shot classification predictions from ZeroShot Models on Hugging face","2023-08-30 13:34:04","","0","228","<nlp><huggingface-transformers><text-classification><transformer-model><huggingface>","<p>I am currently trying to use two Hugging Face zero shot model for classification. Unfortunately, the results are extremely poor (almost random). Likewise, I get different results when I use the template for the example on the hugging face webpage page, then running it via own code and pipeline. Therefore, I think I am doing something wrong with my code. Additionally, I also get some warnings. Maybe you can help me out 😊</p>
<p>My Code for the first Model:</p>
<pre><code>from transformers import pipeline
classifier = pipeline(&quot;zero-shot-classification&quot;, model=&quot;MoritzLaurer/mDeBERTa-v3-base-mnli-xnli&quot;)
sequence_to_classify = &quot;This is a very cool Video&quot;
candidate_labels = [&quot;Praise&quot;, &quot;Criticism&quot;, &quot;Question&quot;]
output = classifier(sequence_to_classify, candidate_labels, multi_label=False)
print(output)
</code></pre>
<p>Output inkl. warnings:</p>
<pre><code>C:\Users\XXXX\anaconda3\envs\zero-shot-env\lib\site-packages\transformers\convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
{'sequence': 'This is a very cool Video', 'labels': ['Praise', 'Criticism', 'Question'], 'scores': [0.6537378430366516, 0.28773054480552673, 0.05853160098195076]}
</code></pre>
<p>If I use the same input on the <a href=""https://huggingface.co/MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7?candidateLabels=Praise%2C%20Criticism%2C%20Question&amp;multiClass=false&amp;text=This%20is%20a%20very%20cool%20Video"" rel=""nofollow noreferrer"">Hugging face Webpage</a> the result is:</p>
<pre><code>Praise  0.789
Criticism  0.157
Question  0.053
</code></pre>
<p>In this case, it is close together, but in other cases the values between the Hugging Face Page and the code/pipeline are completly different.</p>
<p>I installed <code>sentencepiece</code>, but the warning still appears.</p>
<p>For the second model I use it is the same problem but with a different Warning:</p>
<p>The Code:</p>
<pre><code>from transformers import pipeline
classifier = pipeline(&quot;zero-shot-classification&quot;,
                      model=&quot;joeddav/xlm-roberta-large-xnli&quot;)
sequence_to_classify = &quot;This is a very cool Video&quot;
candidate_labels = [&quot;Praise&quot;, &quot;Criticism&quot;, &quot;Question&quot;]
output = classifier(sequence_to_classify, candidate_labels, multi_label=False)
print(output)
</code></pre>
<p>The Output:</p>
<pre><code>Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'sequence': 'This is a very cool Video', 'labels': ['Praise', 'Criticism', 'Question'], 'scores': [0.6573342680931091, 0.24442064762115479, 0.09824512898921967]}
</code></pre>
<p>The Values are also different to the Hugging face webpage. I also do not get what the warning means. Because I use this model in a zero shot setting and did not change the architecture or something. Maybe you know more, very happy for help🙂🙂!</p>
<p>PS: Here is my conda env list</p>
<pre><code># Name                    Version                   Build  Channel
anaconda-client           1.11.2          py310haa95532_0
anaconda-navigator        2.4.1           py310haa95532_0
anaconda-project          0.11.1          py310haa95532_0
attrs                     22.1.0          py310haa95532_0
backports                 1.1                pyhd3eb1b0_0
backports.functools_lru_cache 1.6.4              pyhd3eb1b0_0
backports.tempfile        1.0                pyhd3eb1b0_1
backports.weakref         1.0.post1                  py_1
beautifulsoup4            4.12.2          py310haa95532_0
boltons                   23.0.0          py310haa95532_0
brotlipy                  0.7.0           py310h2bbff1b_1002
bzip2                     1.0.8                he774522_0
ca-certificates           2023.05.30           haa95532_0
certifi                   2023.7.22       py310haa95532_0
cffi                      1.15.1          py310h2bbff1b_3
chardet                   4.0.0           py310haa95532_1003
charset-normalizer        2.0.4              pyhd3eb1b0_0
click                     8.0.4           py310haa95532_0
clyent                    1.2.2           py310haa95532_1
colorama                  0.4.6           py310haa95532_0
conda                     23.7.3          py310haa95532_0
conda-build               3.24.0          py310haa95532_0
conda-content-trust       0.1.3           py310haa95532_0
conda-pack                0.6.0              pyhd3eb1b0_0
conda-package-handling    2.0.2           py310haa95532_0
conda-package-streaming   0.7.0           py310haa95532_0
conda-repo-cli            1.0.41          py310haa95532_0
conda-token               0.4.0              pyhd3eb1b0_0
conda-verify              3.4.2                      py_1
console_shortcut          0.1.1                         4
cryptography              39.0.1          py310h21b164f_0
defusedxml                0.7.1              pyhd3eb1b0_0
filelock                  3.9.0           py310haa95532_0
freetype                  2.12.1               ha860e81_0
future                    0.18.3          py310haa95532_0
giflib                    5.2.1                h8cc25b3_3
glib                      2.69.1               h5dc1a3c_2
glob2                     0.7                pyhd3eb1b0_0
icu                       58.2                 ha925a31_3
idna                      3.4             py310haa95532_0
jinja2                    3.1.2           py310haa95532_0
jpeg                      9e                   h2bbff1b_1
jsonpatch                 1.32               pyhd3eb1b0_0
jsonpointer               2.1                pyhd3eb1b0_0
jsonschema                4.17.3          py310haa95532_0
jupyter_core              5.3.0           py310haa95532_0
krb5                      1.20.1               h5b6d351_1
lerc                      3.0                  hd77b12b_0
libarchive                3.6.2                h2033e3e_1
libclang                  14.0.6          default_hb5a9fac_1
libclang13                14.0.6          default_h8e68704_1
libdeflate                1.17                 h2bbff1b_0
libffi                    3.4.4                hd77b12b_0
libiconv                  1.16                 h2bbff1b_2
liblief                   0.12.3               hd77b12b_0
libpng                    1.6.39               h8cc25b3_0
libpq                     12.15                h906ac69_0
libtiff                   4.5.0                h6c2663c_2
libwebp                   1.2.4                hbc33d0d_1
libwebp-base              1.2.4                h2bbff1b_1
libxml2                   2.10.4               h0ad7f3c_1
libxslt                   1.1.37               h2bbff1b_1
lz4-c                     1.9.4                h2bbff1b_0
m2-msys2-runtime          2.5.0.17080.65c939c               3
m2-patch                  2.7.5                         2
markupsafe                2.1.1           py310h2bbff1b_0
menuinst                  1.4.19          py310h59b6b97_0
msys2-conda-epoch         20160418                      1
navigator-updater         0.4.0           py310haa95532_0
nbformat                  5.7.0           py310haa95532_0
openssl                   1.1.1v               h2bbff1b_0
packaging                 23.0            py310haa95532_0
pathlib                   1.0.1              pyhd3eb1b0_1
pcre                      8.45                 hd77b12b_0
pillow                    9.4.0           py310hd77b12b_0
pip                       22.3.1          py310haa95532_0
pkginfo                   1.9.6           py310haa95532_0
platformdirs              3.10.0          py310haa95532_0
pluggy                    1.0.0           py310haa95532_1
ply                       3.11            py310haa95532_0
powershell_shortcut       0.0.1                         3
psutil                    5.9.0           py310h2bbff1b_0
py-lief                   0.12.3          py310hd77b12b_0
pycosat                   0.6.4           py310h2bbff1b_0
pycparser                 2.21               pyhd3eb1b0_0
pyjwt                     2.4.0           py310haa95532_0
pyopenssl                 23.2.0          py310haa95532_0
pyqt                      5.15.7          py310hd77b12b_0
pyqt5-sip                 12.11.0         py310hd77b12b_0
pyrsistent                0.18.0          py310h2bbff1b_0
pysocks                   1.7.1           py310haa95532_0
python                    3.10.9               h966fe2a_1
python-dateutil           2.8.2              pyhd3eb1b0_0
python-fastjsonschema     2.16.2          py310haa95532_0
python-libarchive-c       2.9                pyhd3eb1b0_1
pytz                      2022.7          py310haa95532_0
pywin32                   305             py310h2bbff1b_0
pyyaml                    6.0             py310h2bbff1b_1
qt-main                   5.15.2               h6072711_9
qt-webengine              5.15.9               h5bd16bc_7
qtpy                      2.2.0           py310haa95532_0
qtwebkit                  5.212                h2bbfb41_5
requests                  2.31.0          py310haa95532_0
requests-toolbelt         1.0.0           py310haa95532_0
ruamel.yaml               0.17.21         py310h2bbff1b_0
ruamel.yaml.clib          0.2.6           py310h2bbff1b_1
ruamel_yaml               0.17.21         py310h2bbff1b_0
setuptools                65.6.3          py310haa95532_0
sip                       6.6.2           py310hd77b12b_0
six                       1.16.0             pyhd3eb1b0_1
soupsieve                 2.4             py310haa95532_0
sqlite                    3.41.2               h2bbff1b_0
tk                        8.6.12               h2bbff1b_0
toml                      0.10.2             pyhd3eb1b0_0
tomli                     2.0.1           py310haa95532_0
toolz                     0.12.0          py310haa95532_0
tornado                   6.3.2           py310h2bbff1b_0
tqdm                      4.65.0          py310h9909e9c_0
traitlets                 5.7.1           py310haa95532_0
tzdata                    2023c                h04d1e81_0
ujson                     5.4.0           py310hd77b12b_0
urllib3                   1.26.16         py310haa95532_0
vc                        14.2                 h21ff451_1
vs2015_runtime            14.27.29016          h5e58377_2
wheel                     0.38.4          py310haa95532_0
win_inet_pton             1.1.0           py310haa95532_0
wincertstore              0.2             py310haa95532_2
xz                        5.4.2                h8cc25b3_0
yaml                      0.2.5                he774522_0
zlib                      1.2.13               h8cc25b3_0
zstandard                 0.19.0          py310h2bbff1b_0
zstd                      1.5.5                hd43e919_0
</code></pre>
","huggingface"
"77008422","Error when running a huggingface model in 4bit mode in Streamlit using bitsnbytes. Quant state is being set to None unwillingly","2023-08-30 13:17:29","","1","583","<streamlit><huggingface><huggingface-tokenizers><quantization>","<p>I am loading a huggingface starchat beta model in streamlit and caching it thus :</p>
<pre class=""lang-py prettyprint-override""><code>@st.cache_resource
def load_model():
    &quot;&quot;&quot;Initialize the tokenizer and the AI model.&quot;&quot;&quot;

    tokenizer = AutoTokenizer.from_pretrained(&quot;HuggingFaceH4/starchat-beta&quot;)
    print(&quot;loaded tokenizer&quot;)
    model = AutoModelForCausalLM.from_pretrained(&quot;HuggingFaceH4/starchat-beta&quot;,load_in_4bit=True, device_map=&quot;cuda&quot;)
    print(&quot;loaded models&quot;)
    return tokenizer,model
</code></pre>
<p>However while it loads the model with the same code when I am executing the code standalone, I am facing the following error in streamlit :</p>
<pre class=""lang-py prettyprint-override""><code>Traceback (most recent call last):  File &quot;/usr/local/lib/python3.10/site-packages/streamlit/runtime/scriptrunner/script_runner.py&quot;, line 552, in _run_script    exec(code, module.__dict__)  File &quot;/app.py&quot;, line 93, in &lt;module&gt;    main()  File &quot;/app.py&quot;, line 73, in main    outputs = model.generate(inputs, max_new_tokens=256, do_sample=True, temperature=0.2, top_k=50, top_p=0.95, eos_token_id=49155)  File &quot;/usr/local/lib/python3.10/site-packages/torch/utils/_contextlib.py&quot;, line 115, in decorate_context    return func(*args, **kwargs)  File &quot;/usr/local/lib/python3.10/site-packages/transformers/generation/utils.py&quot;, line 1642, in generate    return self.sample(  File &quot;/usr/local/lib/python3.10/site-packages/transformers/generation/utils.py&quot;, line 2724, in sample    outputs = self(  File &quot;/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py&quot;, line 1501, in _call_impl    return forward_call(*args, **kwargs)  File &quot;/usr/local/lib/python3.10/site-packages/accelerate/hooks.py&quot;, line 165, in new_forward    output = old_forward(*args, **kwargs)  File &quot;/usr/local/lib/python3.10/site-packages/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py&quot;, line 808, in forward    transformer_outputs = self.transformer(  File &quot;/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py&quot;, line 1501, in _call_impl    return forward_call(*args, **kwargs)  File &quot;/usr/local/lib/python3.10/site-packages/accelerate/hooks.py&quot;, line 165, in new_forward    output = old_forward(*args, **kwargs)  File &quot;/usr/local/lib/python3.10/site-packages/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py&quot;, line 673, in forward    outputs = block(  File &quot;/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py&quot;, line 1501, in _call_impl    return forward_call(*args, **kwargs)  File &quot;/usr/local/lib/python3.10/site-packages/accelerate/hooks.py&quot;, line 165, in new_forward    output = old_forward(*args, **kwargs)  File &quot;/usr/local/lib/python3.10/site-packages/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py&quot;, line 316, in forward    attn_outputs = self.attn(  File &quot;/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py&quot;, line 1501, in _call_impl    return forward_call(*args, **kwargs)  File &quot;/usr/local/lib/python3.10/site-packages/accelerate/hooks.py&quot;, line 165, in new_forward    output = old_forward(*args, **kwargs)  File &quot;/usr/local/lib/python3.10/site-packages/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py&quot;, line 230, in forward    query, key_value = self.c_attn(hidden_states).split((self.embed_dim, 2 * self.kv_dim), dim=2)  File &quot;/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py&quot;, line 1501, in _call_impl    return forward_call(*args, **kwargs)  File &quot;/usr/local/lib/python3.10/site-packages/accelerate/hooks.py&quot;, line 165, in new_forward    output = old_forward(*args, **kwargs)  File &quot;/usr/local/lib/python3.10/site-packages/bitsandbytes/nn/modules.py&quot;, line 248, in forward    out = bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state)  File &quot;/usr/local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py&quot;, line 567, in matmul_4bit    assert quant_state is not NoneAssertionError
</code></pre>
<p>I tried looking up for solutions but it seems none exist. The problem arises from line 567 in <a href=""https://github.com/TimDettmers/bitsandbytes/blob/main/bitsandbytes/autograd/_functions.py"" rel=""nofollow noreferrer"">https://github.com/TimDettmers/bitsandbytes/blob/main/bitsandbytes/autograd/_functions.py</a>, retracing my error steps points it to as originating from <code>model.generate(inputs, max_new_tokens=256, do_sample=True, temperature=0.2, top_k=50, top_p=0.95, eos_token_id=49155)</code> which works perfectly well if executing in a jupyter notebook shell instead of streamlit</p>
","huggingface"
"77007440","ValueError: Tokenizer class LlamaTokenizer does not exist or is not currently imported","2023-08-30 11:11:33","","6","10162","<python><huggingface-transformers><huggingface><llama>","<p>I am trying to run the code from this <a href=""https://huggingface.co/blog/llama2"" rel=""noreferrer"">Hugging Face blog</a>. At first, I had no access to the model so this error: <a href=""https://stackoverflow.com/questions/77006745/oserror-meta-llama-llama-2-7b-chat-hf-is-not-a-local-folder"">OSError: meta-llama/Llama-2-7b-chat-hf is not a local folder</a>, is now solved and I created an acces token from Hugging Face which works. Now I'm facing a different error when running the following code:</p>
<pre><code>from transformers import AutoTokenizer
import transformers
import torch

model = &quot;meta-llama/Llama-2-7b-chat-hf&quot;

tokenizer = AutoTokenizer.from_pretrained(model, use_auth_token=True)
pipeline = transformers.pipeline(
    &quot;text-generation&quot;,
    model=model,
    torch_dtype=torch.float16,
    device_map=&quot;auto&quot;,
)

sequences = pipeline(
    'I liked &quot;Breaking Bad&quot; and &quot;Band of Brothers&quot;. Do you have any recommendations of other shows I might like?\n',
    do_sample=True,
    top_k=10,
    num_return_sequences=1,
    eos_token_id=tokenizer.eos_token_id,
    max_length=200,
)
for seq in sequences:
    print(f&quot;Result: {seq['generated_text']}&quot;)
</code></pre>
<p>Error:</p>
<pre><code>ValueError: Tokenizer class LlamaTokenizer does not exist or is not currently imported.
</code></pre>
<p>The error is not the same as this error: <a href=""https://stackoverflow.com/questions/75907910/importerror-cannot-import-name-llamatokenizer-from-transformers"">ImportError: cannot import name &#39;LLaMATokenizer&#39; from &#39;transformers&#39;</a>, because now it is a valuerror. To make sure I use the right version I run this code:</p>
<pre><code>pip install git+https://github.com/huggingface/transformers
</code></pre>
<p>After that I checked this issue <a href=""https://github.com/huggingface/transformers/issues/22222"" rel=""noreferrer"">ValueError: Tokenizer class LLaMATokenizer does not exist or is not currently imported. #22222
</a>. This suggests:</p>
<blockquote>
<p>Change the LLaMATokenizer in tokenizer_config.json into lowercase
LlamaTokenizer and it works like a charm.</p>
</blockquote>
<p>So, I checked the files if it is using <code>LLamaTokenizer</code> instead of <code>LlamaTokenizer</code> like for example here (This is the class in the file):</p>
<pre><code>class LlamaTokenizer(PreTrainedTokenizer):
</code></pre>
<p>So I was wondering if anyone knows how to fix this error?</p>
","huggingface"
"77006745","OSError: meta-llama/Llama-2-7b-chat-hf is not a local folder","2023-08-30 09:34:39","77006862","2","13765","<python><huggingface-transformers><huggingface><llama>","<p>I'm trying to replied the code from this <a href=""https://huggingface.co/blog/llama2"" rel=""nofollow noreferrer"">Hugging Face blog</a>. At first I installed the transformers and created a token to login to hugging face hub:</p>
<pre><code>pip install transformers
huggingface-cli login
</code></pre>
<p>After that it is said to use <code>use_auth_token=True</code> when you have set a token. Unfortunately after running the code I get an error:</p>
<pre><code>from transformers import AutoTokenizer
import transformers
import torch

model = &quot;meta-llama/Llama-2-7b-chat-hf&quot;

tokenizer = AutoTokenizer.from_pretrained(model, use_auth_token=True)
pipeline = transformers.pipeline(
    &quot;text-generation&quot;,
    model=model,
    torch_dtype=torch.float16,
    device_map=&quot;auto&quot;,
)

sequences = pipeline(
    'I liked &quot;Breaking Bad&quot; and &quot;Band of Brothers&quot;. Do you have any recommendations of other shows I might like?\n',
    do_sample=True,
    top_k=10,
    num_return_sequences=1,
    eos_token_id=tokenizer.eos_token_id,
    max_length=200,
)
for seq in sequences:
    print(f&quot;Result: {seq['generated_text']}&quot;)
</code></pre>
<p>Error:</p>
<pre><code>OSError: meta-llama/Llama-2-7b-chat-hf is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`.
</code></pre>
<p>It says that the model cannot be found, but you can find it in the list of models on hugging face <a href=""https://huggingface.co/meta-llama/Llama-2-7b-chat-hf"" rel=""nofollow noreferrer"">here</a>.</p>
<p>This is the version of the <code>transformers</code> package I'm using:</p>
<pre><code>&gt; pip show transformers

Name: transformers
Version: 4.33.0.dev0
Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow
Home-page: https://github.com/huggingface/transformers
Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)
Author-email: transformers@huggingface.co
License: Apache 2.0 License
Location: /Users/quinten/opt/miniconda3/lib/python3.9/site-packages
Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm
Required-by: spacy-transformers
</code></pre>
<p>Does anyone know how to fix this error?</p>
","huggingface"
"77006615","Transformers - LLAMA2 13B - Key Error / Attribute Error","2023-08-30 09:18:12","","1","919","<python><huggingface-transformers><huggingface><large-language-model><llama>","<p>I'm trying to load and run the LLAMA2 13B model on my local machine, however I'm not able test any prompts due to an Key Error / Attribute Error (see image attached).</p>
<p><a href=""https://i.sstatic.net/1s8jJ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/1s8jJ.png"" alt=""enter image description here"" /></a></p>
<p>My machine has the following specs:</p>
<ul>
<li>CPU: AMD® Ryzen threadripper 3960x 24-core processor × 48</li>
<li>Memory: 128GB</li>
<li>GPU: Nvidia Titan RTX</li>
</ul>
<p>Any ideas?</p>
<p>Thanks in advance!
Cheers</p>
","huggingface"
"77006133","Issue with running Starcoder Model on Mac M2 with Transformers library in CPU environment","2023-08-30 08:18:32","","2","617","<python><pytorch><huggingface-transformers><huggingface>","<p>I'm attempting to run the Starcoder model on a Mac M2 with 32GB of memory using the Transformers library in a CPU environment. Despite setting load_in_8bit=True, I'm encountering an error during execution. Below is the relevant code:</p>
<pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer

checkpoint = &quot;bigcode/starcoder&quot;
device = &quot;cpu&quot;

tokenizer = AutoTokenizer.from_pretrained(checkpoint)

model = AutoModelForCausalLM.from_pretrained(checkpoint,
                                             device_map=&quot;auto&quot;,
                                             load_in_8bit=True)
print(f&quot;Memory footprint: {model.get_memory_footprint() / 1e6:.2f} MB&quot;)

inputs = tokenizer.encode(&quot;def print_hello_world():&quot;, return_tensors=&quot;pt&quot;).to(device)
outputs = model.generate(inputs)

print(tokenizer.decode(outputs[0], clean_up_tokenization_spaces=False))
</code></pre>
<p>While running the above, I receive the following warning and exception:</p>
<pre><code>Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated.
Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8.
Warning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.

Error:
ValueError:
                        Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit
                        the quantized model. If you want to dispatch the model on the CPU or the disk while keeping
                        these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom
                        `device_map` to `from_pretrained`. 

</code></pre>
","huggingface"
"77004700","HuggingfacePipeline with Llama-2-7b-hf","2023-08-30 02:46:25","","0","2258","<huggingface-transformers><langchain><huggingface><llama>","<p>I am trying to run meta-llama/Llama-2-7b-hf on langchain with a HuggingfacePipeline. My set-up is below.</p>
<p>Why is the llm loaded with the gpt2 model. I believe gpt2 is the default for the HuggingfacePipeline(), but I am passing the model with transformers.AutoModelForCausalLM.from_pretrained() with the meta-llama/Llama-2-7b-hf override..</p>
<p>What am I doing wrong?</p>
<pre><code>bnb_config = transformers.BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type='nf4',
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=bfloat16,
    # llm_int8_enable_fp32_cpu_offload=True
)


model_config = transformers.AutoConfig.from_pretrained(
    &quot;meta-llama/Llama-2-7b-hf&quot;,
    use_auth_token=hf_auth
)

tokenizer = transformers.AutoTokenizer.from_pretrained(&quot;meta-llama/Llama-2-7b-hf&quot;)

model = transformers.AutoModelForCausalLM.from_pretrained(
    &quot;meta-llama/Llama-2-7b-hf&quot;,
    trust_remote_code=True,
    config=model_config,
    quantization_config=bnb_config,
    device_map=&quot;auto&quot;,
    use_auth_token=hf_auth
)
model.eval()


pipe = transformers.pipeline(
    model=model,
    tokenizer=tokenizer,
    task=&quot;text-generation&quot;,
    return_full_text=True, 
    temperature=0.1,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max
    max_new_tokens=64,  # mex number of tokens to generate in the output
    repetition_penalty=1.1  # without this output begins repeating
)


llm = HuggingFacePipeline(pipeline=pipe)

print(llm)```

&gt;&gt;&gt;HuggingFacePipeline
Params: {'model_id': 'gpt2', 'model_kwargs': None}
</code></pre>
","huggingface"
"77004078","KeyError: 'csv' using a csv file with KeyDataset","2023-08-29 23:08:42","","0","126","<python><huggingface>","<p>I am trying to use the model robert-base-openai-detector from huggingface.</p>
<p>I get keyError csv when I try to use a csv file. Whenever I use a text file and change load_dataset and KeyDataset to text it works.</p>
<p>Do I need to do some preprocessing before or what am I doing wrong?</p>
<pre><code>pipe = pipeline(&quot;text-classification&quot;, model=&quot;roberta-base-openai-detector&quot;)
dataset = load_dataset('csv', data_files=[&quot;/content/training testing split with college address.xlsx - Sheet1.csv&quot;], split='train')

for out in tqdm(pipe(KeyDataset(dataset, &quot;csv&quot;))):
    print (out)
</code></pre>
<p>The error stack doesn't give me much information.</p>
<pre><code>KeyError                                  Traceback (most recent call last)
&lt;ipython-input-5-98a64dbb56de&gt; in &lt;cell line: 10&gt;()
      8 
      9 
---&gt; 10 for out in tqdm(pipe(KeyDataset(dataset, &quot;csv&quot;))):
     11     print (out)
     12 

8 frames
/usr/local/lib/python3.10/dist-packages/transformers/pipelines/pt_utils.py in __getitem__(self, i)
    303 
    304     def __getitem__(self, i):
--&gt; 305         return self.dataset[i][self.key]
    306 
    307 

KeyError: 'csv'
</code></pre>
","huggingface"
"77003607","How to disable auto suggestion in Hugging Face vscode extension","2023-08-29 20:58:33","","0","91","<vscode-extensions><huggingface>","<p>I'd like to disable the auto suggest/complete of hugging face in vscode.  So if I want a suggestion I would hit the key combination cmd+shift+L.  It'a annoying to always have suggestions come up if I'm just thinking about something and then have to hit escape to make it go away.  Is there a way to do this ?</p>
","huggingface"
"76999303","How do I get a the index of retrieved examples via faiss in huggingface datasets","2023-08-29 10:16:44","","0","174","<huggingface>","<p>Here is example code:</p>
<pre><code>ds = Dataset.from_pandas(df)
ds.add_faiss_index(column=&quot;embeddings&quot;, metric_type=faiss.METRIC_INNER_PRODUCT)
scores, nn = ds.get_nearest_examples(&quot;embeddings&quot;, question_emb, k=k)
</code></pre>
<p>Here we only get the nearest neighbours via <code>nn</code>, but how do I get the indices of those examples? Such that I can do <code>ds[index]</code>. I want to do this, as I want to fetch the adjacent entries too.</p>
","huggingface"
"76999049","Using Hugging Face dataset map function to convert a list of paths to PIL images","2023-08-29 09:41:19","","0","404","<python><python-imaging-library><huggingface><huggingface-datasets>","<p>I am using the Hugging Face datasets library to load a dataset of image paths. I want to use the map function to apply a transform that opens each image path as a PIL image. However, when I do that, the result is still a path, not an image object. Here is my code:</p>
<pre><code>from datasets import load_dataset
from PIL import Image

dataset = load_dataset('eugenesiow/Div2k', 'bicubic_x2',split = 'train[:10]')

def apply_transform(image_path):
    image_path['hr'] = Image.open(image_path['hr'])
    return image_path
dataset = dataset.map(apply_transform)
</code></pre>
<p>When I check the result, it is:</p>
<pre><code>{'bytes': None,
 'path': '/root/.cache/huggingface/datasets/downloads/extracted/6b5de90d5a908c75f5c023a85ea042f9aa961db72fdfba2903372ef258aa0ac5/DIV2K_train_HR/0001.png'}
</code></pre>
<p>But I want the result to be an image object. It is weird because when I try this form:</p>
<pre><code>apply_transform(dataset[0])['hr'] 
</code></pre>
<p>I get the right result, which is a PIL image.</p>
<p>How can I fix this problem? Why does the map function not work as expected</p>
","huggingface"
"76986060","Progress bar stuck at 0 with inconsistent behavior","2023-08-27 07:20:38","","0","195","<python><huggingface><tqdm>","<p>I have a really simple question, I am using HuggingFace's <code>transformers</code> to grab pretrained model from the hub, but the progress bar stuck at zero.</p>
<p><a href=""https://i.sstatic.net/2FuSm.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/2FuSm.png"" alt=""enter image description here"" /></a></p>
<p>I have try several approaches such as:</p>
<ul>
<li>Reinstall <code>ipywidgets</code></li>
<li>Activate <code>widgetsnbextension</code></li>
<li>Restarting kernel</li>
<li>Tried importing <code>tqdm</code> method from <code>tqdm.notebook</code> &amp; <code>tqdm</code></li>
</ul>
<p>All I get was, if I make a simple loop with <code>tqdm</code> wrapper, the <code>tqdm.notebook</code> solution works, but downloading model (and also training API) just keep giving me the same stuck in zero thing.</p>
<p><a href=""https://i.sstatic.net/WDiHb.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/WDiHb.png"" alt=""enter image description here"" /></a></p>
<p>Any helps or insights how might this happened?</p>
","huggingface"
"76984561","Why use KeyDataset instead of dataset directly in HuggingFace Pipelines","2023-08-26 19:39:45","","2","795","<huggingface-transformers><huggingface><huggingface-datasets>","<p>Example from link: <a href=""https://huggingface.co/docs/transformers/main_classes/pipelines#pipeline-batching"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/main_classes/pipelines#pipeline-batching</a></p>
<p>Example:</p>
<pre><code>from transformers import pipeline
from transformers.pipelines.pt_utils import KeyDataset
import datasets

dataset = datasets.load_dataset(&quot;imdb&quot;, name=&quot;plain_text&quot;, split=&quot;unsupervised&quot;)
pipe = pipeline(&quot;text-classification&quot;, device=0)
for out in pipe(KeyDataset(dataset, &quot;text&quot;), batch_size=8, truncation=&quot;only_first&quot;):
    print(out)
</code></pre>
<p>Why should one use <code>KeyDataset</code> instead of calling the dataset directly as follows:</p>
<pre><code>for out in pipe(dataset[&quot;text&quot;], batch_size=8, truncation=&quot;only_first&quot;):
    print(out)
</code></pre>
","huggingface"
"76983880","Python, PyTorch. I'm trying to run different microsoft/git models using Pipeline, but I'm getting an error: raise KeyError(key) KeyError: 'input_ids'","2023-08-26 16:31:20","","0","143","<python><pytorch><pipeline><huggingface-transformers><huggingface>","<p>It used to work fine, but stopped working when I switched to SUDO and the SUDO version of PyTorch.</p>
<p>here's my code</p>
<pre><code>import torch
import requests
from PIL import Image
from transformers import pipeline

image = Image.open('img.jpg')
pipe = pipeline(&quot;image-to-text&quot;, model=&quot;microsoft/git-base&quot;,device = 0)
print(pipe(image))

</code></pre>
<p>and my error</p>
<pre><code>The model 'GitForCausalLM' is not supported for image-to-text. Supported models are ['BlipForConditionalGeneration', 'Blip2ForConditionalGeneration', 'VisionEncoderDecoderModel'].
Traceback (most recent call last):
  File &quot;D:\Python\AutoKey\app.py&quot;, line 8, in &lt;module&gt;
    print(pipe(image))
  File &quot;D:\Programs\Conda\envs\GPTKey\lib\site-packages\transformers\pipelines\image_to_text.py&quot;, line 99, in __call__
    return super().__call__(images, **kwargs)
  File &quot;D:\Programs\Conda\envs\GPTKey\lib\site-packages\transformers\pipelines\base.py&quot;, line 1119, in __call__
    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)
  File &quot;D:\Programs\Conda\envs\GPTKey\lib\site-packages\transformers\pipelines\base.py&quot;, line 1126, in run_single
    model_outputs = self.forward(model_inputs, **forward_params)
  File &quot;D:\Programs\Conda\envs\GPTKey\lib\site-packages\transformers\pipelines\base.py&quot;, line 1025, in forward
    model_outputs = self._forward(model_inputs, **forward_params)
  File &quot;D:\Programs\Conda\envs\GPTKey\lib\site-packages\transformers\pipelines\image_to_text.py&quot;, line 113, in _forward
    inputs = model_inputs.pop(self.model.main_input_name)
  File &quot;D:\Programs\Conda\envs\GPTKey\lib\_collections_abc.py&quot;, line 962, in pop
    value = self[key]
  File &quot;D:\Programs\Conda\envs\GPTKey\lib\collections\__init__.py&quot;, line 1106, in __getitem__
    raise KeyError(key)
KeyError: 'input_ids'
</code></pre>
<p>I enabled the mode for CPU again, the same error comes out. Tried other models, they work quite well because they have ready python code attached. Microsoft GIT doesn't have that, so I use Pipeline.</p>
<p>When I try to use GIT using this code (got it here: <a href=""https://huggingface.co/docs/transformers/main/model_doc/git#transformers.GitVisionModel"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/main/model_doc/git#transformers.GitVisionModel</a>)</p>
<pre><code>from PIL import Image
import requests
from transformers import AutoProcessor, GitVisionModel
processor = AutoProcessor.from_pretrained(&quot;microsoft/git-base&quot;)
model = GitVisionModel.from_pretrained(&quot;microsoft/git-base&quot;)
url = &quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;
image = Image.open(requests.get(url, stream=True).raw)
inputs = processor(images=image, return_tensors=&quot;pt&quot;)
outputs = model(**inputs)
last_hidden_state = outputs.last_hidden_state
</code></pre>
<p>A message comes up that it looks like I need to train the model.
Not well versed in PyTorch, just trying to get the models to fit.</p>
","huggingface"
"76983305","Fine-tuning TheBloke/Llama-2-13B-chat-GPTQ model with Hugging Face Transformers library throws Exllama error","2023-08-26 13:57:30","","5","7906","<nlp><huggingface-transformers><huggingface><llama><fine-tuning>","<p>I am trying to fine-tune the TheBloke/Llama-2-13B-chat-GPTQ model using the Hugging Face Transformers library. I am using a JSON file for the training and validation datasets. However, I am encountering an error related to Exllama backend when I try to run the script.</p>
<p>Here is my code:</p>
<pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer
from datasets import load_dataset
import torch

# Check GPU availability
print(&quot;Available GPU devices:&quot;, torch.cuda.device_count())
print(&quot;Name of the first available GPU:&quot;, torch.cuda.get_device_name(0))

# Load model and tokenizer
model_name = &quot;TheBloke/Llama-2-13B-chat-GPTQ&quot;

# Load tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Move the model to GPU
model.to('cuda')

# Load training and validation data
train_data = load_dataset('json', data_files='train_data.jsonl')
val_data = load_dataset('json', data_files='val_data.jsonl')

# Function to format the data
def formatting_func(example):
    return tokenizer(example['input'], example.get('output', ''), truncation=True, padding='max_length')

# Prepare training and validation data
train_data = train_data.map(formatting_func)
val_data = val_data.map(formatting_func)

# Set training arguments
training_args = TrainingArguments(
    output_dir=&quot;./output&quot;,
    overwrite_output_dir=True,
    num_train_epochs=1,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=64,
    save_steps=10_000,
    save_total_limit=2,
)

# Create trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_data,
    eval_dataset=val_data,
)

# Start training
trainer.train()

# Save the model
model.save_pretrained(&quot;./output&quot;)

</code></pre>
<p><strong>The error message I get is:</strong></p>
<p><em>ValueError: Found modules on cpu/disk. Using Exllama backend requires all the modules to be on GPU. You can deactivate exllama backend by setting <code>disable_exllama=True</code> in the quantization config object.</em></p>
<p>I have already moved the model to GPU using model.to('cuda'), but the error persists. Any help would be greatly appreciated.</p>
<p>I tried moving the model to the GPU using model.to('cuda') before initiating the training process, as suggested in the Hugging Face documentation. I also ensured that my environment has all the required packages and dependencies installed. I was expecting the model to fine-tune on my custom JSON dataset without any issues.</p>
<p>However, despite moving the model to the GPU, I still encounter the Exllama backend error. I am not sure why this is happening, as the model should be on the GPU as per my code. I am looking for a way to resolve this error and successfully fine-tune the model on my custom dataset.</p>
","huggingface"
"76982260","HuggingFace transformer evaluation process is too slow","2023-08-26 09:06:42","77027142","1","2010","<machine-learning><huggingface-transformers><bert-language-model><training-data><huggingface>","<p>I used the HuggingFace <code>transformers</code> library to train a BERT model for sequence classification.</p>
<p>The training process is good on GPU, but the evaluation process(which is running GPU) is too slow. For example, when I just have a sanity check for just 20 short text inputs, the evaluation runtime is about 160 seconds per step.</p>
<p>Here's the snippet code:</p>
<pre><code>def compute_metrics(eval_pred):
    accuracy_metric = evaluate.load(&quot;accuracy&quot;)
    f1_metric = evaluate.load(&quot;f1&quot;, average=&quot;macro&quot;)

    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)

    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)
    f1_score = f1_metric.compute(predictions=predictions, references=labels, average=&quot;macro&quot;)

    return {**accuracy, **f1_score}
</code></pre>
<pre><code>model = AutoModelForSequenceClassification.from_pretrained(
        base_model_path,
        num_labels=num_labels,
        id2label=id2label,
        label2id=label2id
    )

    training_args = TrainingArguments(
        output_dir=&quot;.&quot;,
        learning_rate=lr,
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=batch_size,
        num_train_epochs=n_epoch,
        weight_decay=weight_decay,
        evaluation_strategy=&quot;steps&quot;,
        eval_steps=eval_steps,
        logging_strategy=&quot;steps&quot;,
        logging_steps=logging_steps,
        save_strategy=&quot;steps&quot;,
        save_steps=saving_steps,
        load_best_model_at_end=True,
        report_to=[&quot;tensorboard&quot;],
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_train_ds,
        eval_dataset=tokenized_valid_ds,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics,
    )

    trainer.train()
</code></pre>
<p>The properties of the environment:</p>
<pre><code>transformers              4.29.2
Python                    3.10.9
</code></pre>
<p>and the configuration of training is like the following:</p>
<pre><code>len(train_data) ~= 36K
len(valid_data) ~= 2K
len(test_data) ~= 2K

model_name = 'bert-base-uncased'

per_device_train_batch_size=16
per_device_eval_batch_size=16
num_train_epochs=30

</code></pre>
<p>P.S.: The length of all data is small(less than ten tokens).</p>
<p>Can anyone suggest a solution to reduce the time overhead of the evaluation process?</p>
","huggingface"
"76980939","ImportError: partition_docx is not available using Langchain on HuggingFace","2023-08-25 23:03:55","77010749","0","1329","<python><importerror><langchain><huggingface>","<p>I am using the DirectoryLoader with Langchain on HuggingFace (Gradio SDK) like so from my folder named &quot;data&quot;:</p>
<pre><code>from langchain.document_loaders import DirectoryLoader  
  
loader = DirectoryLoader('./data/')  
raw_documents = loader.load() 
</code></pre>
<p>but get the following error:</p>
<p><em>ImportError: partition_docx is not available. Install the docx dependencies with pip install &quot;unstructured[docx]&quot;</em></p>
<p>Does anyone have any insight as to why this error is being given? Nothing pops up for me on a web search for this error.</p>
<p>Thanks in advance! Apologies if more context is needed, just getting into python and I am very novice.</p>
","huggingface"
"76980563","Codegen causal ML inference too short compared to results in model card API","2023-08-25 21:10:59","","0","97","<python><nlp><huggingface>","<p>I'm experimenting with the Salesforce/codegen-350M-mono model.</p>
<p>When I generate text using a prompt, the result on my machine is much shorter than what I get inputting the same prompt in the API window of the model card.  On my machine it just adds 'return'</p>
<pre><code>import datasets
from transformers import AutoModelForCausalLM, AutoTokenizer

checkpoint = &quot;Salesforce/codegen-350M-mono&quot;
device = &quot;cpu&quot;

tokenizer = AutoTokenizer.from_pretrained(checkpoint)
tokenizer.pad_token = tokenizer.eos_token
model = AutoModelForCausalLM.from_pretrained(checkpoint, trust_remote_code=True).to(device)

text = &quot;&quot;&quot;def round_to_multiple(num, mult):
    \&quot;&quot;&quot;Rounds to nearest multiple of another number.\&quot;&quot;&quot;
    &quot;&quot;&quot;
input_ids = tokenizer(text, return_tensors=&quot;pt&quot;).input_ids

generated_ids = model.generate(input_ids)
print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))
</code></pre>
<p>Here is the output</p>
<pre><code>The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
def round_to_multiple(num, mult):
    &quot;&quot;&quot;Rounds to nearest multiple of another number.&quot;&quot;&quot;
    return
</code></pre>
<p>When I enter the same prompt in the <a href=""https://huggingface.co/Salesforce/codegen-350M-mono?text=def%20round_to_multiple%28num%2C%20mult%29%3A%0A%20%20%20%20%5C%22%22%22Rounds%20to%20nearest%20multiple%20of%20another%20number.%5C%22%22%22"" rel=""nofollow noreferrer"">model card</a> code-generation window, I get the following:</p>
<pre><code>def round_to_multiple(num, mult):
    \&quot;&quot;&quot;Rounds to nearest multiple of another number.\&quot;&quot;&quot;
    if (num % mult == 0):
        return int(num)
    else:
        return
</code></pre>
<p>How would I get the result to be the same (or similar but longer) as on the web API?</p>
","huggingface"
"76978463","AttributeError: module 'fsspec.callbacks' has no attribute 'TqdmCallback'","2023-08-25 15:14:16","","0","741","<machine-learning><nlp><dataset><lotus-domino><huggingface>","<p>I am trying to import datasets. I have already installed with pip and I am still getting this error. I am running this code on Domino which should be a controlled environment. Is there something I need to do to adjust the environment?</p>
<pre><code>pip install datasets --upgrade
pip install datasets --user
</code></pre>
<p>Error I'm getting:</p>
<pre><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
Cell In[21], line 1
----&gt; 1 from datasets import Dataset
      2 train_dataset = Dataset.from_dict({&quot;question&quot;: list(train_q), &quot;label&quot;: train_label})
      3 train_dataset

File ~/.local/lib/python3.9/site-packages/datasets/__init__.py:22
      1 # flake8: noqa
      2 # Copyright 2020 The HuggingFace Datasets Authors and the TensorFlow Datasets Authors.
      3 #
   (...)
     17 # pylint: enable=line-too-long
     18 # pylint: disable=g-import-not-at-top,g-bad-import-order,wrong-import-position
     20 __version__ = &quot;2.14.4&quot;
---&gt; 22 from .arrow_dataset import Dataset
     23 from .arrow_reader import ReadInstruction
     24 from .builder import ArrowBasedBuilder, BeamBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder

File ~/.local/lib/python3.9/site-packages/datasets/arrow_dataset.py:66
     63 from requests import HTTPError
     65 from . import config
---&gt; 66 from .arrow_reader import ArrowReader
     67 from .arrow_writer import ArrowWriter, OptimizedTypedSequence
     68 from .data_files import sanitize_patterns

File ~/.local/lib/python3.9/site-packages/datasets/arrow_reader.py:34
     32 from .table import InMemoryTable, MemoryMappedTable, Table, concat_tables
     33 from .utils import logging
---&gt; 34 from .utils.file_utils import cached_path
     37 if TYPE_CHECKING:
     38     from .info import DatasetInfo  # noqa: F401

File ~/.local/lib/python3.9/site-packages/datasets/utils/file_utils.py:348
    344         raise ValueError(f&quot;HEAD can be called with at most one path but was called with {paths}&quot;)
    345     return fs.info(paths[0])
--&gt; 348 class TqdmCallback(fsspec.callbacks.TqdmCallback):
    349     def __init__(self, tqdm_kwargs=None, *args, **kwargs):
    350         super().__init__(tqdm_kwargs, *args, **kwargs)

AttributeError: module 'fsspec.callbacks' has no attribute 'TqdmCallback'
</code></pre>
","huggingface"
"76971761","How to adapt LLaMA v2 model to less than 7B parameters?","2023-08-24 17:41:37","","1","1375","<huggingface-transformers><huggingface>","<p>I want to pre-train a Decoder (Causal Model) model with less than 7B (since 7B and above are unstable during training, I want to guarantee to the best of my abilities that the pre-training will go smoothly with minimum baby sitting).</p>
<p>Given how nice the pre-training curves for LLaMA v2 (llama2) are I will try that.</p>
<p></p>
<p>What I need is:</p>
<ol>
<li>be able to initialize a llama 2 architecture with less parameters (e.g., decreasing the width or decreasing the layers)</li>
<li>then randomly initialize it.</li>
</ol>
<p>How do I do the above?</p>
<p>Some initial code:</p>
<pre><code>    from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer
    torch_dtype = torch.bfloat16
    torch_dtype = torch.float32
    pretrained_model_name_or_path = 'meta-llama/Llama-2-7b-hf'
    bf16=torch.cuda.get_device_capability(torch.cuda.current_device())[0] &gt;= 8,  # if &gt;= 8 ==&gt; brain float 16 available or set to True if you always want fp32
    model = AutoModelForCausalLM.from_pretrained(
        pretrained_model_name_or_path,
        # quantization_config=quantization_config,
        # device_map=device_map,  # device_map = None  https://github.com/huggingface/trl/blob/01c4a35928f41ba25b1d0032a085519b8065c843/examples/scripts/sft_trainer.py#L82
        trust_remote_code=True,
        torch_dtype=torch_dtype,
        use_auth_token=True,
    )
    print(f'{pretrained_model_name_or_path=}')
    tokenizer = AutoTokenizer.from_pretrained(model, torch_dtype=torch_dtype, use_auth_token=True)
</code></pre>
<p>and</p>
<pre><code>from llama.models.llama_config import LLaMAConfig

class SmallLlamaConfig(LLaMAConfig):
    hidden_size = 2048
    num_layers = 24

config = SmallLlamaConfig()

model = AutoModelForCausalLM.from_config(config,
                                        random_init=True,
                                        load_in=['conv1', 'layer_norm']) 

print(model)
</code></pre>
<p>note: a different model with the above conditions could work, but llama 2 is the ideal answer I think.</p>
<p>ref so: <a href=""https://stackoverflow.com/questions/76971761/how-to-get-a-llama-v2-model-with-less-than-7b-parameters"">How to adapt LLaMA v2 model to less than 7B parameters?</a>
ref hf: <a href=""https://discuss.huggingface.co/t/how-to-get-a-llama-v2-model-with-less-than-7b-parameters/52011"" rel=""nofollow noreferrer"">https://discuss.huggingface.co/t/how-to-get-a-llama-v2-model-with-less-than-7b-parameters/52011</a>
ref dis: <a href=""https://discord.com/channels/879548962464493619/1144325449611235418/1144325449611235418"" rel=""nofollow noreferrer"">https://discord.com/channels/879548962464493619/1144325449611235418/1144325449611235418</a>
related: <a href=""https://stackoverflow.com/questions/77499162/how-does-one-reinitialize-the-weights-of-a-hugging-face-llama-v2-model-the-offic"">How does one reinitialize the weights of a Hugging Face LLaMA v2 model the official way as the original model?</a></p>
","huggingface"
"76971461","I am not able to use GPT4All with Streamlit","2023-08-24 16:53:14","","3","416","<python><langchain><huggingface><gpt4all>","<p>I am trying to use GPT4All with Streamlit in my python code, but it seems like some parameter is not getting correct values. I have tried every alternative. It looks a small problem that I am missing somewhere.</p>
<p>My code:</p>
<pre><code>from langchain import HuggingFaceHub, LLMChain, PromptTemplate
import streamlit as st
from dotenv import load_dotenv
from PyPDF2 import PdfReader
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings, HuggingFaceInstructEmbeddings
from langchain.vectorstores import FAISS
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from langchain.chat_models import ChatOpenAI
from htmlTemplates import bot_template, user_template, css
import transformers
from transformers import pipeline
from gpt4all.gpt4all import GPT4All

def get_pdf_text(pdf_files):

    text = &quot;&quot;
    for pdf_file in pdf_files:
        reader = PdfReader(pdf_file)
        for page in reader.pages:
            text += page.extract_text()
    return text


def get_chunk_text(text):

    text_splitter = CharacterTextSplitter(
        separator=&quot;\n&quot;,
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len
    )

    chunks = text_splitter.split_text(text)

    return chunks


def get_vector_store(text_chunks):

    # For OpenAI Embeddings

    # embeddings = OpenAIEmbeddings()

    # For Huggingface Embeddings

    # model_name = &quot;hkunlp/instructor-xl&quot;
    embeddings = HuggingFaceInstructEmbeddings(
        model_name=&quot;sentence-transformers/all-MiniLM-L6-v2&quot;)

    vectorstore = FAISS.from_texts(texts=text_chunks, embedding=embeddings)

    return vectorstore


def get_conversation_chain(vector_store):

    # OpenAI Model

    # llm = ChatOpenAI()

    # HuggingFace Model

    # llm = HuggingFaceHub(repo_id=&quot;google/flan-t5-xxl&quot;,
    #                 model_kwargs={&quot;temperature&quot;: 0.8, &quot;max_length&quot;: 5000})

    # HuggingFace Model (downloaded)
    # model_name = &quot;distilgpt2&quot;  # or choose any models - distilroberta, roberta, bart etc
    # llm = transformers.AutoModelForCausalLM.from_pretrained(model_name)

    # Create GPT4All Model

    gpt4all = GPT4All(model_name=&quot;ggml-gpt4all-j-v1.3-groovy.bin&quot;, model_path=&quot;./&quot;)
    
    #prompt = PromptTemplate(template=&quot;{question}&quot;, input_variables=[&quot;question&quot;])

    #llm_chain = LLMChain(prompt=prompt, llm=gpt4all)

    memory = ConversationBufferMemory(
        memory_key='chat_history', return_messages=True)

    conversation_chain = ConversationalRetrievalChain.from_llm(
        llm=gpt4all,
        retriever=vector_store.as_retriever(),
        memory=memory
    )

    return conversation_chain


def handle_user_input(question):

    response = st.session_state.conversation({'question': question})
    st.session_state.chat_history = response['chat_history']

    for i, message in enumerate(st.session_state.chat_history):
        if i % 2 == 0:
            st.write(user_template.replace(
                &quot;{{MSG}}&quot;, message.content), unsafe_allow_html=True)
        else:
            st.write(bot_template.replace(
                &quot;{{MSG}}&quot;, message.content), unsafe_allow_html=True)


def main():
    load_dotenv()
    st.set_page_config(page_title='Chat with Your own PDFs',
                       page_icon=':books:')

    st.write(css, unsafe_allow_html=True)

    if &quot;conversation&quot; not in st.session_state:
        st.session_state.conversation = None

    if &quot;chat_history&quot; not in st.session_state:
        st.session_state.chat_history = None

    st.header('Chat with Your own PDFs :books:')
    question = st.text_input(&quot;Ask anything to your PDF: &quot;)

    if question:
        handle_user_input(question)

    with st.sidebar:
        st.subheader(&quot;Upload your Documents Here: &quot;)
        pdf_files = st.file_uploader(&quot;Choose your PDF Files and Press OK&quot;, type=[
                                     'pdf'], accept_multiple_files=True)

        if st.button(&quot;OK&quot;):
            with st.spinner(&quot;Processing your PDFs...&quot;):

                # Get PDF Text
                raw_text = get_pdf_text(pdf_files)

                # Get Text Chunks
                text_chunks = get_chunk_text(raw_text)

                # Create Vector Store

                vector_store = get_vector_store(text_chunks)
                st.write(&quot;DONE&quot;)

                # Create conversation chain

                st.session_state.conversation = get_conversation_chain(
                    vector_store)


if __name__ == '__main__':
    main()
</code></pre>
<p>I am getting the following exception,</p>
<pre><code>2023-08-24 18:41:50.816 Uncaught app exception
Traceback (most recent call last):
  File &quot;C:\Users\MudassarMa\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\streamlit\runtime\scriptrunner\script_runner.py&quot;, line 556, in _run_script
    exec(code, module.__dict__)
  File &quot;C:\Users\MudassarMa\Downloads\Misc\DataScience\taxgpt\main.py&quot;, line 153, in &lt;module&gt;
    main()
  File &quot;C:\Users\MudassarMa\Downloads\Misc\DataScience\taxgpt\main.py&quot;, line 148, in main
    st.session_state.conversation = get_conversation_chain(
                                    ^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\MudassarMa\Downloads\Misc\DataScience\taxgpt\main.py&quot;, line 85, in get_conversation_chain
    conversation_chain = ConversationalRetrievalChain.from_llm(
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\MudassarMa\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\langchain\chains\conversational_retrieval\base.py&quot;, line 213, in from_llm
    doc_chain = load_qa_chain(
                ^^^^^^^^^^^^^^
  File &quot;C:\Users\MudassarMa\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\langchain\chains\question_answering\__init__.py&quot;, line 238, in load_qa_chain
    return loader_mapping[chain_type](
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\MudassarMa\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\langchain\chains\question_answering\__init__.py&quot;, line 70, in _load_stuff_chain
    llm_chain = LLMChain(
                ^^^^^^^^^
  File &quot;C:\Users\MudassarMa\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\langchain\load\serializable.py&quot;, line 61, in __init__
    super().__init__(**kwargs)
  File &quot;pydantic\main.py&quot;, line 341, in pydantic.main.BaseModel.__init__
pydantic.error_wrappers.ValidationError: 1 validation error for LLMChain
llm
  value is not a valid dict (type=type_error.dict)
</code></pre>
","huggingface"
"76965431","How can BERT/Transformer models accept input batches of different sizes?","2023-08-23 23:21:21","","2","761","<nlp><huggingface-transformers><transformer-model><huggingface>","<p>I understand that all inputs in a batch need to be of the same size. However, it seems BERT/Transformers models can accept batches with different sizes as input.</p>
<p>How is that possible? I thought we needed to pad all examples in a batch to <code>model.max_input_size</code>, however, it seems HuggingFace does <code>Dynamic Padding</code> that allows sending batches of different lengths (till the time they are smaller than <code>max_input_size</code>)</p>
<p><a href=""https://i.sstatic.net/zC1WS.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/zC1WS.png"" alt=""enter image description here"" /></a></p>
<p>Link: <a href=""https://mccormickml.com/2020/07/29/smart-batching-tutorial/"" rel=""nofollow noreferrer"">https://mccormickml.com/2020/07/29/smart-batching-tutorial/</a><br />
Link2: <a href=""https://huggingface.co/learn/nlp-course/en/chapter3/2?fw=pt#dynamic-padding"" rel=""nofollow noreferrer"">https://huggingface.co/learn/nlp-course/en/chapter3/2?fw=pt#dynamic-padding</a></p>
","huggingface"
"76964895","fine tuned model being pushed to huggingface repo doesn't have config.json","2023-08-23 21:08:49","","1","652","<huggingface><llama>","<p>I am using autotrain to train the llama model and push it to my huggingface repo.</p>
<pre><code>!autotrain llm --train --project_name my-llm-test --model meta-llama/Llama-2-7b-hf --data_path test --use_peft --use_int4 --learning_rate 2e-4 --train_batch_size 12 --num_train_epochs 3 --trainer sft --push_to_hub --repo_id myrepo/test
</code></pre>
<p>After that I tried to use it</p>
<pre><code>from transformers import AutoModelForSeq2SeqLM,AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(&quot;myrepo/test&quot;, use_auth_token=True)
test_model = AutoModelForSeq2SeqLM.from_pretrained(&quot;myrepo/test&quot;, use_auth_token=True)
</code></pre>
<p>But I got the error like below. I don't know why it doesn't have the config.json file because I just used the autotrain and pushed the model. I am quite new to huggingface. Would you mind giving me a help? Thanks!</p>
<pre><code>OSError: myrepo/test does not appear to have a file named config.json.
</code></pre>
","huggingface"
"76963864","Low score and wrong answer for Flan-T5-XXL ""question-answering"" task","2023-08-23 18:03:27","","2","934","<python><tensorflow><nlp><huggingface-transformers><huggingface>","<p>I'm trying to run Flan-T5-XXL model for a &quot;question-answering&quot; task.
Here's how I loaded and executed the model:</p>
<pre><code>model_id = &quot;~/Downloads/test_LLM/flan-t5-xxl&quot;
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForQuestionAnswering.from_pretrained(model_id, return_dict=False).to(DEVICE)

qa_T5XXL = pipeline(&quot;question-answering&quot;, model=model, tokenizer=tokenizer)

question = &quot;What is 42?&quot;
context = &quot;42 is the answer to life, the universe and everything&quot;

result = qa_T5XXL({
    &quot;question&quot;: question,
    &quot;context&quot;: context
})
</code></pre>
<p>However, I get a low score and a wrong answer:</p>
<pre><code>{'score': 0.03840925544500351, 'start': 0, 'end': 2, 'answer': '42'}
</code></pre>
<p>Could you please help me make changes to achieve the correct answer?
Thanks in advance.</p>
","huggingface"
"76963376","How to directly load fine-tuned model like Alpaca-Lora (PeftModel()) from the local files instead of load it from huggingface models?","2023-08-23 16:46:14","","1","3164","<huggingface><large-language-model><peft>","<p>I have finetuned Llama model using low-rank adaptation (LoRA), based on peft package. The result files <code>adapter_config.json</code> and <code>adapter_model.bin</code> are saved.</p>
<p>I can load fine-tuned model from huggingface by using the following codes:</p>
<pre><code>model = LlamaForCausalLM.from_pretrained(&lt;model_name&gt;,
                                            torch_dtype=torch.float16,
                                            device_map='auto', 
                                            llm_int8_enable_fp32_cpu_offload=True
                                            )
peft_model_id = &lt;hub_model_name&gt;
peft_model = PeftModelForCausalLM.from_pretrained(model, peft_model_id)
</code></pre>
<p>If I want to directly load the fine-tuned model by using the local files <code>adapter_config.json</code> and <code>adapter_model.bin</code> (instead of push them to hub), how to make it?</p>
<p>Thanks in advance!</p>
","huggingface"
"76960523","what's the best way to evaluate Helsinki model with the Huggingface Trainer","2023-08-23 10:33:32","","0","124","<python><nlp><huggingface-transformers><amazon-sagemaker><huggingface>","<p>I am trying to finetune Helsinki model with the Huggingface trainer based on the documentation found under <a href=""https:////https://huggingface.co/learn/nlp-course/chapter7/4?fw=pt#:%7E:text=The%20score%20can%20go%20from,100%2C%20and%20higher%20is%20better."" rel=""nofollow noreferrer"">link 1</a> and <a href=""https://huggingface.co/docs/transformers/tasks/translation"" rel=""nofollow noreferrer"">link 2</a> for a translation task from German to French. I am using cross validation for performance check. I am finetuning the model in aws Sagemaker notebook. I am noticing that the performance while training is working just fine but during evaluation the performance decreases dramatically from 2.7 iter/s (during training/finetuning) to 0.07 iter/s (during validation).</p>
<p>This is the code I am trying to finetune with:</p>
<pre><code>#checkpoint
checkpoint = &quot;Helsinki-NLP/opus-mt-de-fr&quot;
source_lang = &quot;de&quot;
target_lang = &quot;fr&quot;
prefix = &quot;Übersetzen Deutsch ins Französisch: &quot;

#dataset
dataset_opus100 = load_dataset(&quot;opus100&quot;, &quot;de-fr&quot;, split=&quot;test&quot;)
final_corpus_dataset= dataset_opus100 
# preprocess
def preprocess_function(examples):
    inputs = [prefix + example[source_lang] for example in examples[&quot;translation&quot;]]
    targets = [example[target_lang] for example in examples[&quot;translation&quot;]]
    model_inputs = tokenizer(
        inputs, text_target=targets, max_length=128, truncation=True
    )
    return model_inputs
# metric
def postprocess_text(preds, labels):
    preds = [pred.strip() for pred in preds]
    labels = [[label.strip()] for label in labels]

    return preds, labels


def compute_metrics(eval_preds):
    preds, labels = eval_preds
    if isinstance(preds, tuple):
        preds = preds[0]
    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)

    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)

    result = metric.compute(predictions=decoded_preds, references=decoded_labels)
    result = {&quot;bleu&quot;: result[&quot;score&quot;]}

    prediction_lens = [
        np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds
    ]
    result[&quot;gen_len&quot;] = np.mean(prediction_lens)
    result = {k: round(v, 4) for k, v in result.items()}
    # print(result)
    return result

# config_params
exp_name = &quot;de-fr-helsinki&quot;


seed = 42
kfold = KFold(n_splits=5, shuffle=True, random_state=seed)
# K-fold Cross Validation model evaluation
for fold, (train_ids, val_ids) in enumerate(kfold.split(final_corpus_dataset)):
    # if fold==0 or fold==1:
    #     continue
    # metric
    metric = evaluate.load(&quot;sacrebleu&quot;)
    # arguments for training
    training_args = Seq2SeqTrainingArguments(
        output_dir=f&quot;test/huggingface_exps/{exp_name}/fold{fold}&quot;,
        evaluation_strategy=&quot;epoch&quot;,
        logging_strategy=&quot;epoch&quot;,
        # logging_steps=4,
        learning_rate=2e-5,
        per_device_train_batch_size=64,
        per_device_eval_batch_size=64,
        weight_decay=0.01,
        num_train_epochs=10,
        save_total_limit=2,
        save_strategy=&quot;epoch&quot;,
        load_best_model_at_end=True,
        predict_with_generate=True,
        fp16=False,
        push_to_hub=False,
        # tensorboard log directory
        logging_dir=f&quot;test/huggingface_exps/{exp_name}/fold{fold}/runs&quot;,
        report_to=[&quot;tensorboard&quot;],
        # dataloader_num_workers=2,
    )
    print(&quot;fold&quot;, fold)
    # reset tokenizer and model
    tokenizer = AutoTokenizer.from_pretrained(checkpoint)
    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)
    model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)
    # select splits
    train_dataset = final_corpus_dataset.select(train_ids)
    eval_dataset = final_corpus_dataset.select(val_ids)

    # preprocess
    train_dataset = train_dataset.map(preprocess_function, batched=True)
    eval_dataset = eval_dataset.map(preprocess_function, batched=True)




    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        # train_dataset=Dataset.from_dict(train_dataset[:40]),
        # eval_dataset=Dataset.from_dict(eval_dataset[:20]),
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=compute_metrics,
        # callbacks=[TensorBoardCallback]
        callbacks=[
            CombinedTensorBoardCallback,
            EarlyStoppingCallback(early_stopping_patience=3),
        ],
    )

    train_result = trainer.train()

    # compute train results
    metrics = train_result.metrics

    # print metrics history
    import os

    # Create the directory if it doesn't exist
    os.makedirs(f&quot;test/huggingface_exps/{exp_name}/fold{fold}/&quot;, exist_ok=True)

    with open(f&quot;test/huggingface_exps/{exp_name}/fold{fold}/console.json&quot;, &quot;w&quot;) as f:
        json.dump(trainer.state.log_history, f)
    #next


</code></pre>
<p><strong>Things I considered:</strong></p>
<ol>
<li>GPU memory is enough to fit the process (14.5GB/15.5GB)</li>
<li>Reducing the batch size for the evaluation process didn't help</li>
<li>While training on sagemmaker I get this error <code> valueerror: This tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed in order to use this tokenizer</code> in comparaison to the training process with <code>mt5-small</code>. I had to install sentencepiece with <code> ! pip install transformers[sentencepiece]</code>. I'm not sure if it reduces the performance during the evaluation process ( nothing regarding this was mentioned in the documentation)</li>
</ol>
<p><strong>Packages I installed in this order:</strong></p>
<pre><code>! pip install transformers==4.28.0
! pip install datasets
! pip install evaluate
! pip install torch
! pip install sklearn
! pip install tensorboard
! pip install sacrebleu
! pip install accelerate -U
! pip install transformers[sentencepiece]
</code></pre>
","huggingface"
"76955400","Save a LLM model after adding RAG pipeline and embedded model and deploy as hugging face inference?","2023-08-22 16:55:34","","2","788","<huggingface><large-language-model><llama>","<p>I have created a RAG (Retrieval-augmented generation) pipeline and using it with a 4-bit quantized openllama 13b loaded directly from hugging face and without fine-tuning the model.</p>
<ol>
<li>At first I need to save the model into local. But after using <code>torch.save(model.state_dict(), 'path')</code> to save the model, the model saved as adapter model and I can not load it from local again as well as can not able to push into hugging face.</li>
<li>How can I use this configuration into hugging face to make inference API in the hugging face interface?</li>
</ol>
<p>Here is the code of loading quantized model:</p>
<pre><code>bnb_config = transformers.BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type='nf4',
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=bfloat16
)
hf_auth = '*'
model_config = transformers.AutoConfig.from_pretrained(
    model_id,
    use_auth_token=hf_auth
)
model = transformers.AutoModelForCausalLM.from_pretrained(
    model_id,
    trust_remote_code=True,
    config=model_config,
    quantization_config=bnb_config,
    device_map='auto',
    use_auth_token=hf_auth
)
model.eval()
</code></pre>
","huggingface"
"76954245","How do I use HuggingFace for BERT domain adaptation?","2023-08-22 14:21:32","","1","311","<huggingface-transformers><huggingface><pre-trained-model>","<p>I am conducting <a href=""https://arxiv.org/abs/2004.10964"" rel=""nofollow noreferrer"">domain adaptive pretraining</a> on the <em>bert-base-uncased</em> model (hosted on HuggingFace) using domain data to increase the accuracy of the model on domain specific tasks. Currently, I am not seeing any improvements in those tasks after using a domain adapted model, even after fine-tuning the adapted model. I want to confirm that I’m using Transformers correctly. Specifically:</p>
<ol>
<li>Should I use <em>BertForMaskedLM</em> or <em>BertForPreTraining</em> as my starting model?</li>
<li>Should I train a tokenizer from scratch on the domain data or use a preexisting one?</li>
<li>Is having ~1M text records (each around a paragraph in length) roughly enough for continual pretraining?</li>
</ol>
<p>I've tried using a domain adapted model and then loading it using <em>from_pretrained</em> into a text classification task, but my test set scores (acc/F1) barely change compared to just using vanilla bert. How can I gauge that my domain adapted model is meaningfully different from the standard Bert model?</p>
","huggingface"
"76954036","Error while using optuna for hyper parameter optimization with huggingface trainer","2023-08-22 13:56:55","","0","118","<pytorch><huggingface><optuna>","<p><strong>Information</strong><br />
The problem arises in chapter:</p>
<p>Making Transformers Efficient in Production
Describe the bug
while training I am getting proper F1 score of 0.755940
image</p>
<p>while finding best fit value of alpha and temperature value for NER task f1 score is 0.096029 which is less than 0.1
image</p>
<p><strong>To Reproduce</strong><br />
Steps to reproduce the behavior:</p>
<ol>
<li>compute metric is same as in huggingface NER tutorial.</li>
<li>Hyperparameter are for alpha and temperature.</li>
</ol>
<pre><code>def hp_space(trial):
return {&quot;alpha&quot;: trial.suggest_float(&quot;alpha&quot;, 0, 1),
&quot;temperature&quot;: trial.suggest_int(&quot;temperature&quot;, 2, 20)}

best_run = distil_roberta_trainer.hyperparameter_search(
n_trials=12, direction=&quot;maximize&quot;,backend=&quot;optuna&quot;, hp_space=hp_space)
</code></pre>
<p><strong>Expected behavior</strong><br />
After the hyperparameter search the F1 score should be higher than baseline.</p>
<p>More details:<a href=""https://github.com/nlp-with-transformers/notebooks/issues/115#issue-1860567807"" rel=""nofollow noreferrer"">https://github.com/nlp-with-transformers/notebooks/issues/115#issue-1860567807</a></p>
<p>Hyperparameter optimization giving less F1 score less than 0.1 while default value giving 0.75.<br />
Want good parameter value which gives better result than baseline.</p>
","huggingface"
"76953260","How to get the desired output from a Hugging face LLM?","2023-08-22 12:20:23","","0","501","<huggingface-transformers><huggingface><large-language-model>","<p><strong>What I want:</strong>
I want to generate synthetic reviews for E.g., Nurses</p>
<p><strong>What I'm using:</strong>
I'm using open_llama_7b LLM from Hugging face to generate the reviews</p>
<p><strong>Prompt I'm using:</strong>
&quot;Your task is to generate 10 reviews written by nurses about how they feel about their workplace. Generate:&quot;</p>
<p><strong>Result I'm getting:</strong>
&quot;I can’t write 271 words in two minutes.”\n“I have problems getting motivated to write.”\n“I have writing problems.”\n“Writing in a nursing journal is important because…”\n“I write a little because nursing is a profession.”\n“I don’t think it’s necessary to write and explain nursing and myself. “\n“To write, the environment has to be clean, quiet, and free of any distractions.”\n“Nursing and writing are important to me because of…”\n“I write to write, and if I am not motivated to write, I won’t write.”\n“A good time to write about nursing is after a busy day.”\n“I am writing this article to express my feelings about nursing.”\n&quot;</p>
<p><strong>Result I want:</strong></p>
<ol>
<li>&quot;I love working at this facility. The staff is incredibly supportive and the patient population is diverse and interesting. The work-life balance is great, and there's always something new to learn.&quot;</li>
<li>&quot;I feel like I'm constantly being asked to do more with less. The staffing levels are never adequate, and it's hard to provide quality care when you're overworked and understaffed.&quot;</li>
</ol>
<p><strong>Code I'm using:</strong></p>
<pre><code>from transformers import AutoModelForCausalLM
from transformers import AutoTokenizer


model = AutoModelForCausalLM.from_pretrained(
    &quot;openlm-research/open_llama_7b&quot;, device_map={&quot;&quot;: 0}, load_in_4bit=True
)
tokenizer = AutoTokenizer.from_pretrained(&quot;openlm-research/open_llama_7b&quot;)

model_inputs = tokenizer([&quot;Your task is to generate 10 reviews written by nurses about how they feel about their workplace. Generate:&quot;], return_tensors=&quot;pt&quot;).to(&quot;cuda&quot;)
generated_ids = model.generate(**model_inputs, do_sample=True,top_k=50, top_p=0.95, max_new_tokens = 1300)
tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
</code></pre>
<p><strong>I'm not sure what's the issue here:</strong></p>
<ol>
<li>Is the model not powerful enough?</li>
<li>Am I using the wrong decoding method? (I read that default method is greedy that results in non-creative output, currently using Top-p in combination with Top-k)</li>
</ol>
<p><strong>Ask</strong>
What am I missing in the code to get the desired output?</p>
<p>So far I've tried guanaco 7b &amp; open_llama_7b.</p>
","huggingface"
"76948314","Trying to use bertopic within jupyter notebook but experiencing error when just importing into file","2023-08-21 19:47:07","","0","246","<python><installation><jupyter-notebook><bert-language-model><huggingface>","<p>Been trying to test out some bertopic within a jupyter notebook file but experiencing an error. I just try to run this:</p>
<pre><code>from bertopic import BERTopic
from sklearn.datasets import fetch_20newsgroups
</code></pre>
<p>the output gives me an &quot;AttributeError: module 'llvmlite.binding.ffi' has no attribute 'register_lock_callback'&quot; error. I'm very confused as all I've done is do a</p>
<p><code>pip install bertopic</code></p>
<p>and that worked well, then I made this notebook a</p>
<p>i looked in my installed packages
<code>pip list</code>
and saw that bertopic was properly installed. I also tried updating jupyter notebook but didn't work</p>
<p>Any help would be appreciated</p>
","huggingface"
"76945388","Where to find HuggingFace tokenize documentation","2023-08-21 12:43:10","","0","31","<python><nlp><huggingface>","<p>Let's say I initialize a tokenizer like this:</p>
<pre><code>from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;)
</code></pre>
<p>I want to see the full documentation for this tokenizer, specifically the arguments I can use when calling this:</p>
<pre><code>context_length = 128
tokenizer(
        element[&quot;content&quot;],
        truncation=True,
        max_length=context_length,
        return_overflowing_tokens=True,
        return_length=True,
    )
</code></pre>
<p>I can see it using help(tokenizer), but would prefer to view it in a browser.</p>
<p>Seems like <a href=""https://huggingface.co/transformers/v4.3.3/model_doc/gpt2.html#transformers.GPT2TokenizerFast.save_vocabulary"" rel=""nofollow noreferrer"">this</a> is a version, but it's outdated.  The link to the latest version there points to a tutorial-like page.</p>
<p>Where is the definitive documentation for functions like this one and others (i.e. would like to see the analogous documentation for a datasets.dataset_dict.DatasetDict object)</p>
","huggingface"
"76942351","Determine category of sentence","2023-08-21 04:13:20","76951958","-1","53","<python><nlp><huggingface>","<p>I have a list of Books with Title and Organization.
I would like to get the category of each book.
Category list is predefined and choose one of them.</p>
<p>Books
Title 1: Agriculturalpolicy and the decisions ofagriculturalproducers as to income and investment
Organization 1: Western Agricultural Economics Association</p>
<p>Categories: Agriculture, Engineering, Healthcare</p>
<p>Can I find the category of the Book using Python?</p>
<p>I tried using Transformers from HuggingFace but I don't have predefined data to train or evaluate.</p>
<p>I am not sure whether there is an existing solution or not.</p>
","huggingface"
"76940597","Getting Peft Version Error while Autotrain Finetune on Llama 2","2023-08-20 17:30:22","77024447","1","823","<huggingface><large-language-model><llama><fine-tuning>","<p>i did some Llama 2 finetuning with autotrain, on google colab. this is a sample text column, for fine tuning</p>
<pre><code>###Human:
Here is the OCR Text extracted from a VHS tape cover. Yes, the text is surely extracted from a VHS tape, but it may have some typos and wrong characters
hi fi EMBASSY HOME ENTERTAINMENT PSYCHIC KILLER A WHOLE NEW KIND OF MURDER Mardi Rustam presents PSYCHIC KILLER Starring PAUL BURKE JIM HUTTON JULIE ADAMS NEHEMIAH PERSOFF NEVILLE BRAND Co starring ALDO RAY WHIT BISSELL ROD CAMERON and DELLA REESE Executive producer MOHAMMED RUSTAM Written by GREYDON CLARK MIKE ANGEL RAYMOND DANTON Produced by MARDI RUSTAM Directed by RAYMOND DANTON An E Embassy Pictures Release
And here is the possibly matched database entry
title Psychic killer, distributor Embassy Home Entertainment, vhs release year 1975, crew Jim Hutton Paul Burke Neville Brand, synopsis A man wrongly accused of murder and locked in a mental institution seeks revenge through his newly learned psychic gifts His deadly revenge is terrifying Jim Hutton stars

Instructions for Matching VHS Tape Details to Database Entry
~*~SURELY~*~  Confirm if both the title and VHS home video distributor from the OCR text match the database entry.
~*~POSSIBLE~*~  Indicate if only the title matches, and you're uncertain about the distributor.
~*~IMPOSSIBLE~*~  Indicate if neither the title nor the distributor matches, or if the tape and distributor are clearly not the same.
Guidelines
Focus on the title of the movie, VHS home video distributor (not theatrical distributor), VHS release year (not theatrical release year), and cast and crew members.
Consider that the synopsis was manually added to the database and may have discrepancies, and some missing information such the crew.
Explain your reasoning briefly, optimized for ML data training enrichment, but avoid rewriting details of all possible matches.

###Assistant:
- ~*~SURELY~*~ The title of the movie &quot;Psychic Killer&quot; match both in OCR extracted text and database entry. The distributor &quot;Embassy Home Entertainment&quot; also matches in both the sources. Although the release year is not mentioned in the VHS tape, it matches the common names of the cast members such as Jim Hutton and Paul Burke. Therefore, it is a confirmed match.
</code></pre>
<p>this is how i do autotrain</p>
<pre><code>!autotrain llm --train --project_name &quot;llm-vhsfinetune-200s&quot; \
--model abhishek/llama-2-7b-hf-small-shards \
--data_path . \
--text_column text \
--use_peft \
--use_int4 \
--learning_rate 2e-4 \
--train_batch_size 2 \
--num_train_epochs 1 \
--model_max_length 1024 \
--trainer sft \
--push_to_hub \
--repo_id &quot;soajan/llm2-vhsfinetune-200s&quot; \
--block_size 1024 &gt; training.log
</code></pre>
<p>after training is done, trying to load &amp; test the model from huggingface:</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

model_id = &quot;soajan/llm2-vhsfinetune-200s&quot;
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type=&quot;nf4&quot;,
    bnb_4bit_compute_dtype=torch.bfloat16
)

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={&quot;&quot;:0})
</code></pre>
<p>but getting the below error message, stating</p>
<p>ValueError: The version of PEFT you are using is not compatible, please use a version that is greater than 0.4.0</p>
<pre><code>Downloading (…)okenizer_config.json: 100%
705/705 [00:00&lt;00:00, 52.3kB/s]
Downloading tokenizer.model: 100%
500k/500k [00:00&lt;00:00, 507kB/s]
Downloading (…)/main/tokenizer.json: 100%
1.84M/1.84M [00:00&lt;00:00, 3.73MB/s]
Downloading (…)in/added_tokens.json: 100%
21.0/21.0 [00:00&lt;00:00, 1.21kB/s]
Downloading (…)cial_tokens_map.json: 100%
435/435 [00:00&lt;00:00, 33.4kB/s]
Downloading (…)/adapter_config.json: 100%
458/458 [00:00&lt;00:00, 35.5kB/s]
Loading checkpoint shards: 100%
10/10 [02:20&lt;00:00, 10.89s/it]
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-69-1fbd90a0393c&gt; in &lt;cell line: 13&gt;()
     11 
     12 tokenizer = AutoTokenizer.from_pretrained(model_id)
---&gt; 13 model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={&quot;&quot;:0})

3 frames
/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    533         elif type(config) in cls._model_mapping.keys():
    534             model_class = _get_model_class(config, cls._model_mapping)
--&gt; 535             return model_class.from_pretrained(
    536                 pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs
    537             )

/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py in from_pretrained(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)
   3223 
   3224         if has_adapter_config:
-&gt; 3225             model.load_adapter(
   3226                 adapter_model_id,
   3227                 adapter_name=adapter_name,

/usr/local/lib/python3.10/dist-packages/transformers/lib_integrations/peft/peft_mixin.py in load_adapter(self, peft_model_id, adapter_name, revision, token, device_map, max_memory, offload_folder, offload_index)
    114                 `offload_index` argument to be passed to `accelerate.dispatch_model` method.
    115         &quot;&quot;&quot;
--&gt; 116         check_peft_version(min_version=&quot;0.4.0&quot;)
    117 
    118         adapter_name = adapter_name if adapter_name is not None else &quot;default&quot;

/usr/local/lib/python3.10/dist-packages/transformers/utils/peft_utils.py in check_peft_version(min_version)
     93 
     94     if not is_peft_version_compatible:
---&gt; 95         raise ValueError(
     96             f&quot;The version of PEFT you are using is not compatible, please use a version that is greater&quot;
     97             f&quot; than {min_version}&quot;

ValueError: The version of PEFT you are using is not compatible, please use a version that is greater than 0.4.0
</code></pre>
<p>and i'm checking peft version, it is 0.5.0.dev0 . why may this be happening? ty</p>
","huggingface"
"76939164","Running model prompt on stable diffusion model from hugging face on mps MacOS","2023-08-20 11:27:44","","1","599","<python><torch><huggingface><mps>","<pre><code>import gradio as gr
import torch
import numpy as np
import modin.pandas as pd
from PIL import Image
from diffusers import DiffusionPipeline
import os

os.environ[&quot;PYTORCH_MPS_HIGH_WATERMARK_RATIO&quot;] = &quot;0.0&quot;

device = 'cuda' if torch.cuda.is_available() else 'mps'

if torch.cuda.is_available():
    PYTORCH_CUDA_ALLOC_CONF = {'max_split_size_mb': 8000}
    torch.cuda.max_memory_allocated(device=device)
    torch.cuda.empty_cache()

    pipe = DiffusionPipeline.from_pretrained(&quot;stabilityai/stable-diffusion-xl-base-1.0&quot;, torch_dtype=torch.float16,
                                             variant=&quot;fp16&quot;, use_safetensors=True)
    pipe.enable_xformers_memory_efficient_attention()
    pipe = pipe.to(device)
    torch.cuda.empty_cache()

    refiner = DiffusionPipeline.from_pretrained(&quot;stabilityai/stable-diffusion-xl-refiner-1.0&quot;, use_safetensors=True,
                                                torch_dtype=torch.float16, variant=&quot;fp16&quot;)
    refiner.enable_xformers_memory_efficient_attention()
    refiner = refiner.to(device)
    torch.cuda.empty_cache()

    upscaler = DiffusionPipeline.from_pretrained(&quot;stabilityai/sd-x2-latent-upscaler&quot;, torch_dtype=torch.float16,
                                                 use_safetensors=True)
    upscaler.enable_xformers_memory_efficient_attention()
    upscaler = upscaler.to(device)
    torch.cuda.empty_cache()
else:
    pipe = DiffusionPipeline.from_pretrained(&quot;stabilityai/stable-diffusion-xl-base-1.0&quot;, use_safetensors=True)
    pipe = pipe.to(device)
    pipe.unet = torch.compile(pipe.unet, mode=&quot;reduce-overhead&quot;, fullgraph=True)
    refiner = DiffusionPipeline.from_pretrained(&quot;stabilityai/stable-diffusion-xl-refiner-1.0&quot;, use_safetensors=True)
    refiner = refiner.to(device)
    refiner.unet = torch.compile(refiner.unet, mode=&quot;reduce-overhead&quot;, fullgraph=True)

n_steps = 40
high_noise_frac = 0.8

pipe.enable_attention_slicing()
PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0

def genie(prompt, negative_prompt, height, width, scale, steps, seed, upscaling, prompt_2, negative_prompt_2):
    generator = torch.Generator(device=device).manual_seed(seed)
    int_image = pipe(prompt, prompt_2=prompt_2, negative_prompt=negative_prompt, negative_prompt_2=negative_prompt_2,
                     num_inference_steps=steps, height=height, width=width, guidance_scale=scale,
                     num_images_per_prompt=1, generator=generator, output_type=&quot;latent&quot;).images
    if upscaling == 'Yes':
        image = \
            refiner(prompt=prompt, prompt_2=prompt_2, negative_prompt=negative_prompt,
                    negative_prompt_2=negative_prompt_2,
                    image=int_image).images[0]
        upscaled = upscaler(prompt=prompt, negative_prompt=negative_prompt, image=image, num_inference_steps=5,
                            guidance_scale=0).images[0]
        torch.cuda.empty_cache()
        return (image, upscaled)
    else:
        image = \
            refiner(prompt=prompt, prompt_2=prompt_2, negative_prompt=negative_prompt,
                    negative_prompt_2=negative_prompt_2,
                    image=int_image).images[0]
        torch.cuda.empty_cache()
    return (image, image)


gr.Interface(fn=genie, inputs=[gr.Textbox(
    label='What you want the AI to generate. 77 Token Limit. A Token is Any Word, Number, Symbol, or Punctuation. Everything Over 77 Will Be Truncated!'),
    gr.Textbox(label='What you Do Not want the AI to generate. 77 Token Limit'),
    gr.Slider(512, 1024, 768, step=128, label='Height'),
    gr.Slider(512, 1024, 768, step=128, label='Width'),
    gr.Slider(1, 15, 10, step=.25,
              label='Guidance Scale: How Closely the AI follows the Prompt'),
    gr.Slider(25, maximum=100, value=50, step=25, label='Number of Iterations'),
    gr.Slider(minimum=1, step=1, maximum=999999999999999999, randomize=True, label='Seed'),
    gr.Radio(['Yes', 'No'], value='No', label='Upscale?'),
    gr.Textbox(label='Embedded Prompt'),
    gr.Textbox(label='Embedded Negative Prompt')],
             outputs=['image', 'image'],
             title=&quot;Stable Diffusion XL 1.0 GPU&quot;,
             description=&quot;SDXL 1.0 GPU. &lt;br&gt;&lt;br&gt;&lt;b&gt;WARNING: Capable of producing NSFW (Softcore) images.&lt;/b&gt;&quot;,
             article=&quot;If You Enjoyed this Demo and would like to Donate, you can send to any of these Wallets. &lt;br&gt;BTC: bc1qzdm9j73mj8ucwwtsjx4x4ylyfvr6kp7svzjn84 &lt;br&gt;3LWRoKYx6bCLnUrKEdnPo3FCSPQUSFDjFP &lt;br&gt;DOGE: DK6LRc4gfefdCTRk9xPD239N31jh9GjKez &lt;br&gt;SHIB (BEP20): 0xbE8f2f3B71DFEB84E5F7E3aae1909d60658aB891 &lt;br&gt;PayPal: https://www.paypal.me/ManjushriBodhisattva &lt;br&gt;ETH: 0xbE8f2f3B71DFEB84E5F7E3aae1909d60658aB891 &lt;br&gt;Code Monkey: &lt;a href=\&quot;https://huggingface.co/Manjushri\&quot;&gt;Manjushri&lt;/a&gt;&quot;).launch(
    debug=True, max_threads=80)

</code></pre>
<p>This is an example of one usage, but all models that are related to graphics from <code>huggingface</code> returns the same error which is</p>
<p>Error</p>
<pre><code>step_index = (self.timesteps == timestep).nonzero().item()

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

RuntimeError: Expected dst.dim() &gt;= src.dim() to be true, but got false. (Could this error message be improved? If so, please report an enhancement request to PyTorch.)
</code></pre>
<p>Any suggestion please?</p>
<p>Thanks in advance</p>
<p>I was trying to generate image from prompt, and expected a PIL Image</p>
","huggingface"
"76938423","AttributeError: ‘Dataset’ object has no attribute ‘remove_columns’ in hugging face","2023-08-20 07:39:30","76938429","1","791","<python><artificial-intelligence><huggingface><huggingface-datasets>","<p>I want to remove column from Dataset Billsum from hugging face.
Error:
AttributeError: ‘Dataset’ object has no attribute ‘remove_columns’</p>
<p>I can't find any solution for this problem.. I have a headache from it
If someone can help me</p>
","huggingface"
"76936388","Chroma Vector Db when i query it gives me same wrong answer every time with every search type?","2023-08-19 17:50:59","","1","1020","<huggingface><chromadb>","<p>I am loading a csv file with service now incident details and storing in chroma db with hugging face embedding.
I am trying to retrieve specific Incident number information but it will give another incident number details, this happens when my k value is 5 or below.</p>
<pre><code>search_kwargs={&quot;k&quot;: target_source_chunks}
</code></pre>
<p>If i increase k value to 10 it will have my incident number in the result, but how do i improve the response so it gives top 5 result as my relevant details.</p>
<p>I am using below python coding.</p>
<pre><code>loader_class, loader_args = (CSVLoader, {}) 
loader = loader_class(SourceFilePath, **loader_args)
results = loader.load()
text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)
texts = text_splitter.split_documents(results)
embeddings = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')
db = Chroma.from_documents(texts, embeddings, persist_directory = my_vector_db, client_settings = CHROMA_SETTINGS)
retriever = db.as_retriever(search_type=&quot;similarity&quot;,search_kwargs={&quot;k&quot;: target_source_chunks})
question = 'Please give me summary of number INC0000063 ?'
docs_rel = retriever.get_relevant_documents(question)
</code></pre>
<p>Please suggest how do i improve my response?</p>
","huggingface"
"76934103","Is it Possible to use a pre-trained model without fine-tuning it?","2023-08-19 07:55:23","","1","124","<huggingface-transformers><huggingface>","<p>I’m curious about how to use the pre-trained model version without fine-tuning because I want to compare the results of the fine-tuned version with those of the non-fine-tuned model.</p>
<p>For information, I’m fine-tuning the Wav2Vec2 XLS-R model.</p>
","huggingface"
"76933625","SageMaker complains that /opt/ml/model does not appear to have a file named config.json","2023-08-19 04:47:50","","1","1498","<python><huggingface-transformers><amazon-sagemaker><huggingface>","<p>I am using a huggingface model alongside a custom pipeline to deploy my model onto SageMaker, my model.tar.gz structure looks like below:</p>
<pre><code>├── added_tokens.json
├── code
│   ├── inference.py
│   ├── pipeline.py
│   └── requirements.txt
├── config.json
├── generation_config.json
├── model-00001-of-00002.safetensors
├── model-00002-of-00002.safetensors
├── model.safetensors.index.json
├── special_tokens_map.json
├── tokenizer_config.json
├── tokenizer.json
└── tokenizer.model
</code></pre>
<p>I deployed my model via</p>
<pre class=""lang-py prettyprint-override""><code>from sagemaker.huggingface.model import HuggingFaceModel

hub = {
   'HF_TASK':'text-generation'
}
huggingface_model = HuggingFaceModel(
   env=hub, 
   model_data=&quot;s3://my_model_bucket/model.tar.gz&quot;,
   role=role,
   transformers_version=&quot;4.28&quot;,
   pytorch_version=&quot;2.0&quot;,
   py_version='py310',
)

# deploy the endpoint endpoint
predictor = huggingface_model.deploy(
    initial_instance_count=1,
    instance_type=&quot;ml.g5.xlarge&quot;
    )
</code></pre>
<p>However, when I try to invoke the model, here is my response</p>
<pre class=""lang-json prettyprint-override""><code>{
  &quot;code&quot;: 400,
  &quot;type&quot;: &quot;InternalServerException&quot;,
  &quot;message&quot;: &quot;/opt/ml/model does not appear to have a file named config.json. Checkout \u0027https://huggingface.co//opt/ml/model/None\u0027 for available files.&quot;
}
</code></pre>
<p>Another error is <code>W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - OSError: /opt/ml/model does not appear to have a file named config.json. Checkout 'https://huggingface.co//opt/ml/model/None' for available files.</code></p>
<p>But config.json is clearly in my model directory. Here is my inference.py code</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from typing import Dict
from transformers import AutoTokenizer, AutoModelForCausalLM
from pipeline import MyCustomPipeline

pipeline = None

def model_fn(model_dir):
    print(&quot;Loading model from: &quot; + model_dir)
    tokenizer = AutoTokenizer.from_pretrained(
        model_dir,
        local_files_only=True,
    )
    model = AutoModelForCausalLM.from_pretrained(
        model_dir,
        local_files_only=True,
        device_map=&quot;auto&quot;,
        torch_dtype=torch.bfloat16,
        trust_remote_code=True,
    )
    pipeline = MyCustomPipeline(model, tokenizer)
    return model, tokenizer

def transform_fn(model, input_data, content_type, accept):
    return pipeline(input_data)
</code></pre>
<p>What am I doing wrong here? I should have followed all needed steps to deploy a huggingface model onto SageMaker.</p>
","huggingface"
"76929997","Fine-tune llama2 on cuda:1","2023-08-18 13:53:39","","0","1614","<tensorflow><huggingface-transformers><huggingface><llama>","<p>When I load the model I use device_map to use cuda:1 still it seems that the model and training are on different cores. How should I properly do this?</p>
<p>Code running at Tesla T4 below:</p>
<pre><code># load the base model in 4-bit quantization
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type=&quot;nf4&quot;,
    bnb_4bit_compute_dtype=torch.bfloat16,
)
    
base_model = AutoModelForCausalLM.from_pretrained(
    base_model_name,
    quantization_config=bnb_config,
    device_map={&quot;&quot;: 1},
    trust_remote_code=True,
    use_auth_token=True,
)
base_model.config.use_cache = False
tokenizer = AutoTokenizer.from_pretrained(base_model_name, use_auth_token=True)
    
# add LoRA layers on top of the quantized base model
peft_config = LoraConfig(
    r=16,
    lora_alpha=64,
    lora_dropout=0.1,
    target_modules=[&quot;q_proj&quot;, &quot;v_proj&quot;],
    bias=&quot;none&quot;,
    task_type=&quot;CAUSAL_LM&quot;,
)
    
trainer = SFTTrainer(
    model=base_model,
    train_dataset=dataset,
    peft_config=peft_config,
    packing=True,
    max_seq_length=None,
    dataset_text_field=&quot;text&quot;,
    tokenizer=tokenizer,
    args=training_args,         # HF Trainer arguments
)
trainer.train()
</code></pre>
<p>Gives error:</p>
<blockquote>
<p>ValueError: You can't train a model that has been loaded in 8-bit precision on a different device than the one you're training on. Make sure you loaded the model on the correct device using for example device_map={'':torch.cuda.current_device()} you're training on.</p>
</blockquote>
<p>I following this guide: <a href=""https://huggingface.co/blog/dpo-trl"" rel=""nofollow noreferrer"">https://huggingface.co/blog/dpo-trl</a></p>
","huggingface"
"76926025","Sentence embeddings from LLAMA 2 Huggingface opensource","2023-08-18 01:59:50","","14","20921","<artificial-intelligence><huggingface-transformers><huggingface><large-language-model><llama>","<p>Is there any way of getting sentence embeddings from meta-llama/Llama-2-13b-chat-hf from huggingface?</p>
<p>Model link: <a href=""https://huggingface.co/meta-llama/Llama-2-13b-chat-hf"" rel=""noreferrer"">https://huggingface.co/meta-llama/Llama-2-13b-chat-hf</a></p>
<p>I tried using transfomer.Automodel module from hugging faces to get the embeddings, but the results don't look as expected. Implementation is referred to in the below link. Reference: <a href=""https://github.com/Muennighoff/sgpt#asymmetric-semantic-search-be%C2%A0"" rel=""noreferrer"">https://github.com/Muennighoff/sgpt#asymmetric-semantic-search-be </a></p>
","huggingface"
"76923802","Hugging face HTTP request on data from parquet format when the only way to get it is from the website's data viewer, how to fix?","2023-08-17 17:19:25","","0","639","<nlp><huggingface><huggingface-datasets><huggingface-hub>","<p>Due to Hugging Face datasets disappearing, I've had to get the data from their data viewer using the parquet option. But when I try to run it,  there is some sort of HTTP error. I've tried downloading the data but I can't. What is the recommended way to solve this problem?</p>
<p>Partial code (and <a href=""https://github.com/brando90/beyond-scale-language-data-diversity/blob/4b26da53c8a39c07183e43497f03621c25480481/src/diversity/div_coeff.py#L517"" rel=""nofollow noreferrer"">full code</a>):</p>
<pre class=""lang-py prettyprint-override""><code>    # - 5 subsets of the pile interleaved
    # from diversity.pile_subset_urls import urls_hacker_news, urls_nih_exporter, urls_pubmed, urls_uspto
    # from diversity.data_mixtures import get_uniform_data_mixture_5subsets_of_pile, get_doremi_data_mixture_5subsets_of_pile, get_llama_v1_data_mixtures_5subsets_of_pile
    # path, name, data_files, split = ['suolyer/pile_pile-cc'] + ['parquet'] * 4, [None] + ['hacker_news', 'nih_exporter', 'pubmed', 'uspto'], [None] + [urls_hacker_news, urls_nih_exporter, urls_pubmed, urls_uspto], ['validation'] + ['train'] * 4
    # ## path, name, data_files = ['conceptofmind/pile_cc'] + ['parquet'] * 4, ['sep_ds'] + ['hacker_news', 'nih_exporter', 'pubmed', 'uspto'], [None] + [urls_hacker_news, urls_nih_exporter, urls_pubmed, urls_uspto]
    # # probabilities, data_mixture_name = get_uniform_data_mixture_5subsets_of_pile()
    # # probabilities, data_mixture_name = get_llama_v1_data_mixtures_5subsets_of_pile(name)
    # probabilities, data_mixture_name = get_doremi_data_mixture_5subsets_of_pile(name)
    # - probe net
    pretrained_model_name_or_path = 'meta-llama/Llama-2-7b-hf'
    # - not changing
    batch_size = 512
    today = datetime.datetime.now().strftime('%Y-m%m-d%d-t%Hh_%Mm_%Ss')
    run_name = f'{path} div_coeff_{num_batches=} ({today=} ({name=}) {data_mixture_name=} {probabilities=} {pretrained_model_name_or_path=})'
    print(f'\n---&gt; {run_name=}\n')

    # - Init wandb
    debug: bool = mode == 'dryrun'
    run = wandb.init(mode=mode, project=&quot;beyond-scale&quot;, name=run_name, save_code=True)
    wandb.config.update({&quot;num_batches&quot;: num_batches, &quot;path&quot;: path, &quot;name&quot;: name, &quot;today&quot;: today, 'probabilities': probabilities, 'batch_size': batch_size, 'debug': debug, 'data_mixture_name': data_mixture_name, 'streaming': streaming, 'data_files': data_files, 'seed': seed, 'pretrained_model_name_or_path': pretrained_model_name_or_path})
    # run.notify_on_failure() # https://community.wandb.ai/t/how-do-i-set-the-wandb-alert-programatically-for-my-current-run/4891
    print(f'{debug=}')
    print(f'{wandb.config=}')

    # -- Get probe network
    from datasets import load_dataset 
    from datasets.iterable_dataset import IterableDataset
    import torch
    from transformers import GPT2Tokenizer, GPT2LMHeadModel

    # tokenizer = GPT2Tokenizer.from_pretrained(&quot;gpt2&quot;)
    # if tokenizer.pad_token_id is None:
    #     tokenizer.pad_token = tokenizer.eos_token
    # probe_network = GPT2LMHeadModel.from_pretrained(&quot;gpt2&quot;)
    # device = torch.device(f&quot;cuda:{0}&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
    # probe_network = probe_network.to(device)

    from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer
    torch_dtype = torch.bfloat16
    torch_dtype = torch.float32
    bf16=torch.cuda.get_device_capability(torch.cuda.current_device())[0] &gt;= 8,  # if &gt;= 8 ==&gt; brain float 16 available or set to True if you always want fp32
    model = AutoModelForCausalLM.from_pretrained(
        pretrained_model_name_or_path,
        # quantization_config=quantization_config,
        # device_map=device_map,  # device_map = None  https://github.com/huggingface/trl/blob/01c4a35928f41ba25b1d0032a085519b8065c843/examples/scripts/sft_trainer.py#L82
        trust_remote_code=True,
        torch_dtype=torch_dtype,
        use_auth_token=True,
    )
    print(f'{pretrained_model_name_or_path=}')
    # https://github.com/artidoro/qlora/blob/7f4e95a68dc076bea9b3a413d2b512eca6d004e5/qlora.py#L347C13-L347C13
    tokenizer = AutoTokenizer.from_pretrained(
        pretrained_model_name_or_path,
        # cache_dir=args.cache_dir,
        padding_side=&quot;right&quot;,
        use_fast=False, # Fast tokenizer giving issues.
        # tokenizer_type='llama' if 'llama' in args.model_name_or_path else None, # Needed for HF name change
        # tokenizer_type='llama',
        trust_remote_code=True,
        use_auth_token=True,
    )
    if tokenizer.pad_token_id is None:
        tokenizer.pad_token = tokenizer.eos_token
    probe_network = model

    # -- Get data set
    def my_load_dataset(path, name, data_files=data_files, split=split):
        print(f'{path=} {name=} {streaming=} {data_files=}, {split=}')
        if path == 'json' or path == 'bin' or path == 'csv':
            print(f'{data_files_prefix+name=}')
            return load_dataset(path, data_files=data_files_prefix+name, streaming=streaming, split=split).with_format(&quot;torch&quot;)
        elif path == 'parquet':
            print(f'{data_files=}')
            return load_dataset(path, data_files=data_files, streaming=streaming, split=split).with_format(&quot;torch&quot;)
</code></pre>
<p>How to fix it? Ideas:</p>
<ol>
<li>Download the parquet data</li>
<li>Stop the http error.</li>
</ol>
<hr />
<p>Error messages follow:</p>
<pre class=""lang-none prettyprint-override""><code>3345 During handling of the above exception, another exception occurred:
3346 Traceback (most recent call last):
3347   File &quot;/lfs/hyperturing2/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/site-packages/requests/adapters.py&quot;, line 486, in send
3348     resp = conn.urlopen(
3349   File &quot;/lfs/hyperturing2/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/site-packages/urllib3/connectionpool.py&quot;, line 798, in urlopen
3350     retries = retries.increment(
3351   File &quot;/lfs/hyperturing2/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/site-packages/urllib3/util/retry.py&quot;, line 550, in increment
3352     raise six.reraise(type(error), error, _stacktrace)
3353   File &quot;/lfs/hyperturing2/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/site-packages/urllib3/packages/six.py&quot;, line 769, in reraise
3354     raise value.with_traceback(tb)
3355   File &quot;/lfs/hyperturing2/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/site-packages/urllib3/connectionpool.py&quot;, line 714, in urlopen
3356     httplib_response = self._make_request(
3357   File &quot;/lfs/hyperturing2/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/site-packages/urllib3/connectionpool.py&quot;, line 466, in _make_request
3358     six.raise_from(e, None)
3359   File &quot;&lt;string&gt;&quot;, line 3, in raise_from
3360   File &quot;/lfs/hyperturing2/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/site-packages/urllib3/connectionpool.py&quot;, line 461, in _make_request
3361     httplib_response = conn.getresponse()
3362   File &quot;/lfs/hyperturing2/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/http/client.py&quot;, line 1375, in getresponse
3363     response.begin()
3364   File &quot;/lfs/hyperturing2/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/http/client.py&quot;, line 318, in begin
3365     version, status, reason = self._read_status()
3366   File &quot;/lfs/hyperturing2/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/http/client.py&quot;, line 287, in _read_status
3367     raise RemoteDisconnected(&quot;Remote end closed connection without&quot;
3368 urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))
3369 During handling of the above exception, another exception occurred:
3370 Traceback (most recent call last):
3371   File &quot;/lfs/hyperturing2/0/brando9/beyond-scale-language-data-diversity/src/diversity/div_coeff.py&quot;, line 591, in &lt;module&gt;
3372     print(f'{all_columns=}')
3373   File &quot;/lfs/hyperturing2/0/brando9/beyond-scale-language-data-diversity/src/diversity/div_coeff.py&quot;, line 553, in experiment_compute_diveristy_coeff_single_dataset_then_combined_datasets_with_domain_weights
3374   File &quot;/lfs/hyperturing2/0/brando9/beyond-scale-language-data-diversity/src/diversity/div_coeff.py&quot;, line 64, in get_diversity_coefficient
3375     embedding, loss = Task2Vec(probe_network, classifier_opts={'seed': seed}).embed(tokenized_batch)
3376   File &quot;/afs/cs.stanford.edu/u/brando9/beyond-scale-language-data-diversity/src/diversity/task2vec.py&quot;, line 133, in embed
3377     loss = self._finetune_classifier(dataset, loader_opts=self.loader_opts, classifier_opts=self.classifier_opts, max_samples=self.max_samples, epochs=epochs)
3378   File &quot;/afs/cs.stanford.edu/u/brando9/beyond-scale-language-data-diversity/src/diversity/task2vec.py&quot;, line 198, in _finetune_classifier
3379     for step, batch in enumerate(epoch_iterator):
3380   File &quot;/lfs/hyperturing2/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/site-packages/tqdm/std.py&quot;, line 1182, in __iter__
3381     for obj in iterable:
3382   File &quot;/lfs/hyperturing2/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/site-packages/torch/utils/data/dataloader.py&quot;, line 633, in __next__
3383     data = self._next_data()
3384   File &quot;/lfs/hyperturing2/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/site-packages/torch/utils/data/dataloader.py&quot;, line 677, in _next_data
3385     data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
3386   File &quot;/lfs/hyperturing2/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py&quot;, line 32, in fetch
3387     data.append(next(self.dataset_iter))
3388   File &quot;/lfs/hyperturing2/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/site-packages/datasets/iterable_dataset.py&quot;, line 1353, in __iter__
3389     for key, example in ex_iterable:
3390   File &quot;/lfs/hyperturing2/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/site-packages/datasets/iterable_dataset.py&quot;, line 652, in __iter__
3391     yield from self._iter()
3392   File &quot;/lfs/hyperturing2/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/site-packages/datasets/iterable_dataset.py&quot;, line 667, in _iter
3393     for key, example in iterator:
3394   File &quot;/lfs/hyperturing2/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/site-packages/datasets/iterable_dataset.py&quot;, line 1088, in __iter__
3395     for key, example in self.ex_iterable:
3396   File &quot;/lfs/hyperturing2/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/site-packages/datasets/iterable_dataset.py&quot;, line 1013, in __iter__
3397     yield from islice(self.ex_iterable, self.n)
3398   File &quot;/lfs/hyperturing2/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/site-packages/datasets/iterable_dataset.py&quot;, line 400, in __iter__
3399     yield next(iterators[i])
3400   File &quot;/lfs/hyperturing2/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/site-packages/datasets/iterable_dataset.py&quot;, line 73, in __next__
3401     result = next(self.it)
3402   File &quot;/lfs/hyperturing2/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/site-packages/datasets/iterable_dataset.py&quot;, line 652, in __iter__
3403     yield from self._iter()
3404   File &quot;/lfs/hyperturing2/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/site-packages/datasets/iterable_dataset.py&quot;, line 714, in _iter
3405     for key, example in iterator:
3406   File &quot;/lfs/hyperturing2/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/site-packages/datasets/iterable_dataset.py&quot;, line 1088, in __iter__
3407     for key, example in self.ex_iterable:
3408   File &quot;/lfs/hyperturing2/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/site-packages/datasets/iterable_dataset.py&quot;, line 255, in __iter__
3409     for key, pa_table in self.generate_tables_fn(**self.kwargs):
3410   File &quot;/lfs/hyperturing2/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/site-packages/datasets/packaged_modules/parquet/parquet.py&quot;, line 77, in _generate_tables
3411     parquet_file = pq.ParquetFile(f)
3412   File &quot;/lfs/hyperturing2/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/site-packages/pyarrow/parquet/core.py&quot;, line 334, in __init__
3413     self.reader.open(
3414   File &quot;pyarrow/_parquet.pyx&quot;, line 1220, in pyarrow._parquet.ParquetReader.open
3415   File &quot;/lfs/hyperturing2/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/site-packages/datasets/download/streaming_download_manager.py&quot;, line 333, in read_with_retries
3416     out = read(*args, **kwargs)
3417   File &quot;/lfs/hyperturing2/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/site-packages/fsspec/spec.py&quot;, line 1790, in read
3418     out = self.cache._fetch(self.loc, self.loc + length)
3419   File &quot;/lfs/hyperturing2/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/site-packages/fsspec/caching.py&quot;, line 156, in _fetch
3420     self.cache = self.fetcher(start, end)  # new block replaces old
3421   File &quot;/lfs/hyperturing2/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/site-packages/huggingface_hub/hf_file_system.py&quot;, line 404, in _fetch_range
3422     r = http_backoff(&quot;GET&quot;, url, headers=headers)
3423   File &quot;/lfs/hyperturing2/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/site-packages/huggingface_hub/utils/_http.py&quot;, line 258, in http_backoff
3424     response = session.request(method=method, url=url, **kwargs)
3425   File &quot;/lfs/hyperturing2/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/site-packages/requests/sessions.py&quot;, line 589, in request
3426     resp = self.send(prep, **send_kwargs)
3427   File &quot;/lfs/hyperturing2/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/site-packages/requests/sessions.py&quot;, line 703, in send
3428     r = adapter.send(request, **kwargs)
3429   File &quot;/lfs/hyperturing2/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/site-packages/huggingface_hub/utils/_http.py&quot;, line 63, in send
3430     return super().send(request, *args, **kwargs)
3431   File &quot;/lfs/hyperturing2/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/site-packages/requests/adapters.py&quot;, line 501, in send
3432     raise ConnectionError(err, request=request)
3433 requests.exceptions.ConnectionError: (ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: d98c4e56-198a-40cb-a53e-9e1348ea1c58)')
</code></pre>
<p>How does one solve this issue in the context of hugging face?</p>
<hr />
<p>Error:</p>
<pre class=""lang-none prettyprint-override""><code>300   File &quot;/lfs/hyperturing2/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/site-packages/urllib3/connectionpool.py&quot;, line 714, in urlopen
2301     httplib_response = self._make_request(
2302   File &quot;/lfs/hyperturing2/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/site-packages/urllib3/connectionpool.py&quot;, line 466, in _make_request
2303     six.raise_from(e, None)
2304   File &quot;&lt;string&gt;&quot;, line 3, in raise_from
2305   File &quot;/lfs/hyperturing2/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/site-packages/urllib3/connectionpool.py&quot;, line 461, in _make_request
2306     httplib_response = conn.getresponse()
2307   File &quot;/lfs/hyperturing2/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/http/client.py&quot;, line 1375, in getresponse
2308     response.begin()
2309   File &quot;/lfs/hyperturing2/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/http/client.py&quot;, line 318, in begin
2310     version, status, reason = self._read_status()
2311   File &quot;/lfs/hyperturing2/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/http/client.py&quot;, line 287, in _read_status
2312     raise RemoteDisconnected(&quot;Remote end closed connection without&quot;
2313 http.client.RemoteDisconnected: Remote end closed connection without response
2314 During handling of the above exception, another exception occurred:
2315 Traceback (most recent call last):
2316   File &quot;/lfs/hyperturing2/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/site-packages/requests/adapters.py&quot;, line 486, in send
2317     resp = conn.urlopen(
2318   File &quot;/lfs/hyperturing2/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/site-packages/urllib3/connectionpool.py&quot;, line 798, in urlopen
2319     retries = retries.increment(
2320   File &quot;/lfs/hyperturing2/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/site-packages/urllib3/util/retry.py&quot;, line 550, in increment
2321     raise six.reraise(type(error), error, _stacktrace)
2322   File &quot;/lfs/hyperturing2/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/site-packages/urllib3/packages/six.py&quot;, line 769, in reraise
2323     raise value.with_traceback(tb)
2324   File &quot;/lfs/hyperturing2/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/site-packages/urllib3/connectionpool.py&quot;, line 714, in urlopen
2325     httplib_response = self._make_request(
2326   File &quot;/lfs/hyperturing2/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/site-packages/urllib3/connectionpool.py&quot;, line 466, in _make_request
2327     six.raise_from(e, None)
2328   File &quot;&lt;string&gt;&quot;, line 3, in raise_from
2329   File &quot;/lfs/hyperturing2/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/site-packages/urllib3/connectionpool.py&quot;, line 461, in _make_request
2330     httplib_response = conn.getresponse()
2331   File &quot;/lfs/hyperturing2/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/http/client.py&quot;, line 1375, in getresponse
2332     response.begin()
2333   File &quot;/lfs/hyperturing2/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/http/client.py&quot;, line 318, in begin
2334     version, status, reason = self._read_status()
2335   File &quot;/lfs/hyperturing2/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/http/client.py&quot;, line 287, in _read_status
2336     raise RemoteDisconnected(&quot;Remote end closed connection without&quot;
2337 urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))
2338 During handling of the above exception, another exception occurred:
2339 Traceback (most recent call last):
2340   File &quot;/lfs/hyperturing2/0/brando9/beyond-scale-language-data-diversity/src/diversity/div_coeff.py&quot;, line 695, in &lt;module&gt;
2341     datasets = [dataset.remove_columns(columns_to_remove) for dataset in datasets]
2342   File &quot;/lfs/hyperturing2/0/brando9/beyond-scale-language-data-diversity/src/diversity/div_coeff.py&quot;, line 656, in experiment_compute_diveristy_coeff_single_dataset_then_combined_datasets_with_domain_weights
2343     return load_dataset(path, data_files=data_files_prefix+name, streaming=streaming, split=split).with_format(&quot;torch&quot;)
2344   File &quot;/lfs/hyperturing2/0/brando9/beyond-scale-language-data-diversity/src/diversity/div_coeff.py&quot;, line 64, in get_diversity_coefficient
2345     embedding, loss = Task2Vec(probe_network, classifier_opts={'seed': seed}).embed(tokenized_batch)
2346   File &quot;/afs/cs.stanford.edu/u/brando9/beyond-scale-language-data-diversity/src/diversity/task2vec.py&quot;, line 133, in embed
2347     loss = self._finetune_classifier(dataset, loader_opts=self.loader_opts, classifier_opts=self.classifier_opts, max_samples=self.max_samples, epochs=epochs)
</code></pre>
<hr />
<blockquote>
<p>Basically all data sets I use with streaming=True throw an unexpected error at some point when getting data form them on long runs. Especially the ones with the parquet option.</p>
</blockquote>
<hr />
<ul>
<li>cross posted hf discuss: <a href=""https://discuss.huggingface.co/t/hugging-face-error-equests-exceptions-connectionerror-protocolerror-connection-aborted-how-to-fix/51131"" rel=""nofollow noreferrer"">https://discuss.huggingface.co/t/hugging-face-error-equests-exceptions-connectionerror-protocolerror-connection-aborted-how-to-fix/51131</a></li>
<li>discord HF: <a href=""https://discord.com/channels/879548962464493619/1146156148811112508/1146156148811112508"" rel=""nofollow noreferrer"">https://discord.com/channels/879548962464493619/1146156148811112508/1146156148811112508</a></li>
<li>SO: <a href=""https://stackoverflow.com/questions/76923802/hugging-face-error-equests-exceptions-connectionerror-protocolerrorconnectio"">Hugging face HTTP request on data from parquet format when the only way to get it is from the website&#39;s data viewer, how to fix?</a></li>
</ul>
","huggingface"
"76914119","Validation and Training Loss when using HuggingFace","2023-08-16 13:36:07","","4","4698","<nlp><huggingface-transformers><huggingface><huggingface-trainer>","<p>I do not seem to find an explanation on how the validation and training losses are calculated when we finetune a model using the huggingFace trainer. Does anyone know here to find this information?</p>
","huggingface"
"76894701","Loading a HF Model in Multiple GPUs and Run Inferences in those GPUs (Not Training or Finetuning)","2023-08-13 18:33:21","","1","1728","<huggingface><multi-gpu><accelerate><inference-engine><deepspeed>","<p>Is there any way to load a Hugging Face model in multi GPUs and use those GPUs for inferences as well?</p>
<p>Like, there is this model which can be loaded on a single GPU (default cuda:0) and run for inference as below:</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained(&quot;togethercomputer/LLaMA-2-7B-32K&quot;)
model = AutoModelForCausalLM.from_pretrained(&quot;togethercomputer/LLaMA-2-7B-32K&quot;, torch_dtype=torch.float16)

input_context= &quot;Your text here&quot;
input_ids = tokenizer.encode(input_context, return_tensors=&quot;pt&quot;).to(model.device)
output = model.generate(input_ids, max_length=256, temperature=0.7)
output_text = tokenizer.decode(output[0], skip_special_tokens=True)
print(output_text)

</code></pre>
<p>How should I load and run this model for inference on two or more GPUs using Accelerate or DeepSpeed?</p>
<p>Please keep in mind, this is not meant for training or finetuning a model, just inference related.</p>
<p>Any guidance/help would be highly appreciated, thanks in anticipation!</p>
","huggingface"
"76892218","Running LLama2 on a GeForce 1080 8Gb machine","2023-08-13 06:40:58","77123473","0","1132","<python><pytorch><huggingface><llama>","<p>I am trying to run LLama2 on my server which has mentioned nvidia card. It's a simple hello world case you can find <a href=""https://huggingface.co/blog/llama2"" rel=""nofollow noreferrer"">here</a>. However I am constantly running into memory issues:</p>
<pre><code>torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 250.00 MiB (GPU 0; 7.92 GiB total capacity; 7.12 GiB already allocated; 241.62 MiB free; 7.18 GiB reserved in total by PyTorch) If reserved memory is &gt;&gt; allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
</code></pre>
<p>I tried</p>
<p>export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128</p>
<p>but same effect. Is there anything I can do?</p>
","huggingface"
"76891189","How to download data from hugging face that is visible on the data viewer but the files are not available?","2023-08-12 21:29:00","","1","1158","<huggingface-transformers><huggingface><huggingface-datasets>","<p>I can see them (data set link hf: <a href=""https://huggingface.co/datasets/EleutherAI/pile/"" rel=""nofollow noreferrer"">https://huggingface.co/datasets/EleutherAI/pile/</a>) :</p>
<p><a href=""https://i.sstatic.net/KIUMQ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/KIUMQ.png"" alt=""enter image description here"" /></a></p>
<p>but no matter how I change the download url I can't get the data. Files are not there and their script doesn't work.</p>
<p>Anyone know how to get the splits and <strong>know</strong> which are the splits?</p>
<p>Code:</p>
<pre><code>    # - Online (real experiment)
    # mode='online'
    # num_batches = 600
    # path, name = 'c4', 'en'
    # path, name = &quot;wikitext&quot;, 'wikitext-103-v1'
    # path, name = ['c4', 'wikitext'], ['en', 'wikitext-103-v1']
    # probabilities, data_mixture_name = get_uniform_data_mixture_for_c4_wt103()
    # probabilities, data_mixture_name = get_doremi_based_data_mixture_for_c4_wt103()
    # probabilities, data_mixture_name = get_llama_v1_based_data_mixture_for_c4_wt103()
    # probabilities, data_mixture_name = [0.75, 0.25], '[0.75, 0.25]' 
    # probabilities, data_mixture_name = [0.25, 0.75], '[0.25, 0.75]' 
    # path, name = 'EleutherAI/pile', 'all'
    # path, name = 'conceptofmind/pile_cc', 'sep_ds'
    streaming = False
    # path, name = 'conceptofmind/pile_cc', 'sep_ds'
    # path, name = 'EleutherAI/pile', 'hacker_news' 
    # path, name = 'EleutherAI/pile', 'nih_exporter'  # https://github.com/huggingface/datasets/issues/6144
    # path, name = 'EleutherAI/pile', 'pubmed' 
    # path, name = 'EleutherAI/pile', 'uspto' 
    # -
    ## path, name, data_files_prefix  = 'json', 'enron_emails', 'https://the-eye.eu/public/AI/pile_preliminary_components/'
    # path, name, data_files_prefix  = 'bin', 'HackerNewsDataset_text_document.bin', 'https://the-eye.eu/public/AI/pile_neox/data/'
    path, name, data_files_prefix  = 'csv', 'hacker_news', 'https://huggingface.co/datasets/EleutherAI/pile/viewer/hacker_news/train/'
    # not changing
    batch_size = 512
    today = datetime.datetime.now().strftime('%Y-m%m-d%d-t%Hh_%Mm_%Ss')
    run_name = f'{path} div_coeff_{num_batches=} ({today=} ({name=}) {data_mixture_name=} {probabilities=})'
    print(f'{run_name=}')

    # - Init wandb
    debug: bool = mode == 'dryrun'
    run = wandb.init(mode=mode, project=&quot;beyond-scale&quot;, name=run_name, save_code=True)
    wandb.config.update({&quot;num_batches&quot;: num_batches, &quot;path&quot;: path, &quot;name&quot;: name, &quot;today&quot;: today, 'probabilities': probabilities, 'batch_size': batch_size, 'debug': debug, 'data_mixture_name': data_mixture_name})
    # run.notify_on_failure() # https://community.wandb.ai/t/how-do-i-set-the-wandb-alert-programatically-for-my-current-run/4891
    print(f'{debug=}')
    print(f'{wandb.config=}')

    # -- Get probe network
    from datasets import load_dataset
    import torch
    from transformers import GPT2Tokenizer, GPT2LMHeadModel

    tokenizer = GPT2Tokenizer.from_pretrained(&quot;gpt2&quot;)
    if tokenizer.pad_token_id is None:
        tokenizer.pad_token = tokenizer.eos_token
    probe_network = GPT2LMHeadModel.from_pretrained(&quot;gpt2&quot;)
    device = torch.device(f&quot;cuda:{0}&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
    probe_network = probe_network.to(device)

    # -- Get data set
    print(f'{path=} {name=} {streaming=}')
    def my_load_dataset():
        if path == 'json' or path == 'bin' or path == 'csv':
            print(f'{data_files_prefix+name=}')
            return load_dataset(path, data_files=data_files_prefix+name, streaming=streaming, split=&quot;train&quot;).with_format(&quot;torch&quot;)
        else:
            return load_dataset(path, name, streaming=streaming, split=&quot;train&quot;).with_format(&quot;torch&quot;)
    # - get data set for real now
    if isinstance(path, str):
        dataset = my_load_dataset()
    else:
        print('-- interleaving datasets')
        datasets = [my_load_dataset().with_format(&quot;torch&quot;) for path, name in zip(path, name)]
        [print(f'{dataset.description=}') for dataset in datasets]
        dataset = interleave_datasets(datasets, probabilities)
    print(f'{dataset=}')
    batch = dataset.take(batch_size)
    print(f'{next(iter(batch))=}')
    column_names = next(iter(dataset)).keys()
    print(f'{column_names=}')
</code></pre>
<p>Related conversations/discussion:</p>
<ul>
<li><a href=""https://github.com/huggingface/datasets/issues/6144"" rel=""nofollow noreferrer"">https://github.com/huggingface/datasets/issues/6144</a></li>
<li><a href=""https://github.com/huggingface/datasets/issues/3504"" rel=""nofollow noreferrer"">https://github.com/huggingface/datasets/issues/3504</a></li>
<li><a href=""https://the-eye.eu/public/AI/"" rel=""nofollow noreferrer"">https://the-eye.eu/public/AI/</a></li>
<li><a href=""https://twitter.com/BrandoHablando/status/1690081313519489024?s=20"" rel=""nofollow noreferrer"">https://twitter.com/BrandoHablando/status/1690081313519489024?s=20</a></li>
<li>hf discuss: <a href=""https://discuss.huggingface.co/t/how-to-download-data-from-hugging-face-that-is-visible-on-the-data-viewer-but-the-files-are-not-available/50555"" rel=""nofollow noreferrer"">https://discuss.huggingface.co/t/how-to-download-data-from-hugging-face-that-is-visible-on-the-data-viewer-but-the-files-are-not-available/50555</a></li>
</ul>
","huggingface"
"76890207","Text splitter output is not JSON serializable","2023-08-12 16:34:50","","0","348","<json><machine-learning><huggingface><pdfminer>","<h2>Summary</h2>
<p>I'm trying to extract text from PDF using PDFMiner, cut it into chunks, and then embed it with a model from Huggingface. The problem is that <em>the list returned by RecursiveCharacterTextSplitter() is not json serializable by requests.post()</em></p>
<p>The code fails when querying the Huggingface model and it returns an error message:</p>
<p><code>TypeError: Object of type Document is not JSON serializable</code></p>
<p>Note: Full error at the end of this question</p>
<p>I don't know how to convert my data that I receive from my RecursiveCharacterTextSplitter() to JSON serializable object.</p>
<h2>Source code:</h2>
<pre><code>from io import StringIO

from pdfminer.converter import TextConverter
from pdfminer.layout import LAParams
from pdfminer.pdfdocument import PDFDocument
from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter
from pdfminer.pdfpage import PDFPage
from pdfminer.pdfparser import PDFParser

from langchain.document_loaders import PyPDFLoader
from langchain.document_loaders import PDFMinerLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

import requests

import pandas as pd

# Extract text from pdf file

output_string = StringIO()
with open('info.pdf', 'rb') as in_file:
    parser = PDFParser(in_file)
    doc = PDFDocument(parser)
    rsrcmgr = PDFResourceManager()
    device = TextConverter(rsrcmgr, output_string, laparams=LAParams())
    interpreter = PDFPageInterpreter(rsrcmgr, device)
    for page in PDFPage.create_pages(doc):
        interpreter.process_page(page)

# Split text into chunks via text_splitter

text_splitter = RecursiveCharacterTextSplitter(
    # Set a really small chunk size, just to show.
    chunk_size = 100,
    chunk_overlap  = 20,
    length_function = len,
    is_separator_regex = False,
)

texts = text_splitter.create_documents([output_string.getvalue()])

# model_id: Embedding Model we use on Huggingface
# hf_token: Huggingface token so that you can authorize against huggingface models

model_id = &quot;sentence-transformers/all-MiniLM-L6-v2&quot;
hf_token = &quot;hf...&quot;

# Build request header

api_url = f&quot;https://api-inference.huggingface.co/pipeline/feature-extraction/{model_id}&quot;
headers = {&quot;Authorization&quot;: f&quot;Bearer {hf_token}&quot;}

# Issue query for the model to embed our different text chunks

def query(texts):
    response = requests.post(api_url, headers=headers, json={&quot;inputs&quot;: texts, &quot;options&quot;:{&quot;wait_for_model&quot;:True}})
    return response.json()

output = query(texts)
 
</code></pre>
<h2>The error code at runtime is the following:</h2>
<pre><code>Traceback (most recent call last):
  File &quot;troubleshoot.py&quot;, line 59, in &lt;module&gt;
    output = query(texts)
  File &quot;troubleshoot.py&quot;, line 56, in query
    response = requests.post(api_url, headers=headers, json={&quot;inputs&quot;: texts, &quot;options&quot;:{&quot;wait_for_model&quot;:True}})
  File &quot;/Users/work/Library/Python/3.8/lib/python/site-packages/requests/api.py&quot;, line 115, in post
    return request(&quot;post&quot;, url, data=data, json=json, **kwargs)
  File &quot;/Users/work/Library/Python/3.8/lib/python/site-packages/requests/api.py&quot;, line 59, in request
    return session.request(method=method, url=url, **kwargs)
  File &quot;/Users/work/Library/Python/3.8/lib/python/site-packages/requests/sessions.py&quot;, line 575, in request
    prep = self.prepare_request(req)
  File &quot;/Users/work/Library/Python/3.8/lib/python/site-packages/requests/sessions.py&quot;, line 486, in prepare_request
    p.prepare(
  File &quot;/Users/work/Library/Python/3.8/lib/python/site-packages/requests/models.py&quot;, line 371, in prepare
    self.prepare_body(data, files, json)
  File &quot;/Users/work/Library/Python/3.8/lib/python/site-packages/requests/models.py&quot;, line 511, in prepare_body
    body = complexjson.dumps(json, allow_nan=False)
  File &quot;/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/json/__init__.py&quot;, line 234, in dumps
    return cls(
  File &quot;/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/json/encoder.py&quot;, line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File &quot;/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/json/encoder.py&quot;, line 257, in iterencode
    return _iterencode(o, 0)
  File &quot;/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/json/encoder.py&quot;, line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type Document is not JSON serializable
</code></pre>
<h2>The list I'm trying to JSON serialize looks the following:</h2>
<pre><code>output of print(type(texts)): &lt;class 'list'&gt;

output of print(texts): [Document(page_content='There is a land in the middle of the Pacific Ocean, it’s called AmazingLand.', metadata={}), Document(page_content='The population of it is about 1.4 million and it’s 72% inhabited by Amazings. The other 28%', metadata={}), Document(page_content='consists of hungarians, germans and mongoloids.', metadata={}), Document(page_content='The country is a monarchy, and it is ruled by the Big Amazing King. The Big Amazing King is', metadata={}), Document(page_content='someone who can rap classical music, and it is the best doing it among the Amazing population', metadata={}), Document(page_content='of the AmazingLand.', metadata={})]
</code></pre>
<h2>Question</h2>
<p>How do I convert the data that I receive from my RecursiveCharacterTextSplitter() to JSON serializable object?</p>
","huggingface"
"76889671","Byte-level BPE tokenizer for handing Bigram and Trigram","2023-08-12 14:12:59","","1","249","<tokenize><huggingface><huggingface-tokenizers><byte-pair-encoding>","<p>I'm currently employing the HuggingFace tokenizer to tokenize a textual database, and here's how I'm doing it:</p>
<pre><code>from tokenizers import ByteLevelBPETokenizer
from tokenizers import normalizers

tokenizer = ByteLevelBPETokenizer()
tokenizer.normalizer = normalizers.BertNormalizer(lowercase = False)
tokenizer.train_from_iterator(Data, vocab_size = 50264, min_frequency = 2, special_tokens = [&quot;&lt;s&gt;&quot;, &quot;&lt;pad&gt;&quot;, &quot;&lt;/s&gt;&quot;, &quot;&lt;unk&gt;&quot;])
</code></pre>
<p>When I applying this on data, I notice that many of the identified tokens are single words. Sometimes, it even divides a word into smaller parts, which is something I anticipated. I'm curious if there's a way to incorporate Bigram (and Trigram), or n-gram in general, into this process? I like to observe longer tokens that consist of two or three tokens grouped together.</p>
","huggingface"
"76888776","load_lora_weights() not loading weights","2023-08-12 10:09:44","","1","1082","<pytorch><tensor><huggingface><stable-diffusion>","<p>trying to load <a href=""https://civitai.com/models/57319?modelVersionId=99805"" rel=""nofollow noreferrer"">checkpoint from civitai</a> to a <a href=""https://huggingface.co/CompVis/stable-diffusion-v1-4"" rel=""nofollow noreferrer"">huggingface stable diffusion model</a>.</p>
<p>Trying to use load_lora_weights() function described <a href=""https://huggingface.co/docs/diffusers/using-diffusers/other-formats"" rel=""nofollow noreferrer"">here</a>.</p>
<p>It works perfectly when trying to load <a href=""https://civitai.com/api/download/models/19998"" rel=""nofollow noreferrer"">another lora file</a>.</p>
<p>When trying to load model 99805 with load_lora_weights() I get error <code>ValueError: None does not seem to be in the correct format expected by LoRA or Custom Diffusion training.</code></p>
<p>I download it using <code>!wget https://civitai.com/api/download/models/99805 -O testerino.safetensors</code></p>
<p>How can I load the weights to huggingface model?</p>
","huggingface"
"76887527","How to load LoRA weights saved locally?","2023-08-12 02:32:32","","2","4418","<python><huggingface-transformers><huggingface><huggingface-trainer>","<p>I am currently training a model and have saved the checkpoints for the LoRA adapters. I now have the .bin and .config file for the adapters. How do I reload everything for inference without pushing to huggingFace? Most of the documentation talks about pushing to huggingFace. I was not able to find anything regarding working with local files.</p>
<p>I tried
<code>lora_config = LoraConfig.from_pretrained('/path/to/adapter') model = get_peft_model(model, lora_config)</code>
But this did not work</p>
","huggingface"
"76885998","How do i save checkpoints when using the Huggingface SFTTrainer?","2023-08-11 18:34:27","","0","2054","<nlp><huggingface-transformers><huggingface><llama>","<p>Hey I’m trying to finetune Llama 2 and I can’t see where the checkpoints are getting saved. I am using the following code:</p>
<pre><code>output_dir = &quot;./Llama-2-7b-hf-qlora&quot;

training_args = TrainingArguments(
    output_dir=output_dir,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    logging_steps=5,
    max_steps=400,
    evaluation_strategy=&quot;steps&quot;, # Evaluate the model every logging step
    logging_dir=&quot;./logs&quot;,        # Directory for storing logs
    save_strategy=&quot;steps&quot;,       # Save the model checkpoint every logging step
    eval_steps=5,               # Evaluate and save checkpoints every 10 steps
    do_eval=True                 # Perform evaluation at the end of training
)
class PeftSavingCallback(TrainerCallback):
    def on_save(self, args, state, control, **kwargs):
        checkpoint_path = os.path.join(args.output_dir, f&quot;checkpoint-{state.global_step}&quot;)
        kwargs[&quot;model&quot;].save_pretrained(checkpoint_path)

        if &quot;pytorch_model.bin&quot; in os.listdir(checkpoint_path):
            os.remove(os.path.join(checkpoint_path, &quot;pytorch_model.bin&quot;))


callbacks = [PeftSavingCallback()]
max_seq_length = 512
trainer = SFTTrainer(
    model=base_model,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,  # Add this line
    peft_config=peft_config,
    dataset_text_field=&quot;text&quot;,
    max_seq_length=max_seq_length,
    tokenizer=tokenizer,
    args=training_args,
    callbacks=callbacks
)
trainer.train()
</code></pre>
<p>(I added in the callback stuff based on this guide Supervised Fine-tuning Trainer)
How do I get a checkpoint saved every 5/10 steps?</p>
","huggingface"
"76881848","Image not fetching while using Stable diffusion Hugging face model","2023-08-11 08:27:08","","0","214","<huggingface-transformers><huggingface><huggingface-tokenizers><stable-diffusion>","<pre><code>from auth_token import auth_token
from fastapi import FastAPI,Response ,Request
from fastapi.middleware.cors import CORSMiddleware
import torch
from torch import autocast
from diffusers import StableDiffusionPipeline
from io import BytesIO
import base64


app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_credentials= True,
    allow_origins=[&quot;*&quot;],
    allow_methods=[&quot;*&quot;],
    allow_headers=[&quot;*&quot;]
)


device = &quot;cuda&quot;
model_id = &quot;CompVis/stable-diffusion-v1-4&quot;
pipe = StableDiffusionPipeline.from_pretrained(model_id,revision=&quot;fp16&quot;,torch_dtype=torch.float16,use_auth_token=auth_token)
print(torch.cuda.get_device_properties(0).total_memory)


@app.get(&quot;/&quot;)
def generate(prompt : str) :
    with autocast(device):
        pipe.enable_sequential_cpu_offload()
        pipe.enable_attention_slicing(1)

        image = pipe(prompt,guidance_scale=8.5).images[0]

        image.save(&quot;testimage.png&quot;)

    return {&quot;out&quot;:&quot;hello World&quot;}

</code></pre>
<p>This is the code , where I tried to fetch the image on the basis of provided prompt by the user .</p>
<p>Terminal output :</p>
<p><a href=""https://i.sstatic.net/zQHcB.png"" rel=""nofollow noreferrer"">Terminal Output</a></p>
<p>But when I gave prompt in the browser It didn't generated the image :</p>
<p>Here's the Browser output :</p>
<p><a href=""https://i.sstatic.net/kr6lg.png"" rel=""nofollow noreferrer"">Browser Output</a></p>
<p>This is the error I got in the browser</p>
<p>Failed to fetch.
Possible Reasons:</p>
<p>CORS</p>
<p>Network Failure</p>
<p>URL scheme must be &quot;http&quot; or &quot;https&quot; for CORS request.</p>
<p>and didn't got any image</p>
","huggingface"
"76879872","How to use huggingface HF trainer train with custom collate function?","2023-08-10 23:22:56","76929999","0","3382","<python><huggingface-transformers><huggingface><huggingface-datasets><huggingface-trainer>","<p>I have some custom data set with custom table entries and wanted to deal with it with a custom collate. But it didn't work when I pass a collate function I wrote (that DOES work on a individual dataloader e.g., see <a href=""https://stackoverflow.com/questions/76872115/how-does-one-create-a-pytorch-data-loader-with-a-custom-hugging-face-data-set-wi"">How does one create a pytorch data loader with a custom hugging face data set without having errors?</a> or <a href=""https://stackoverflow.com/questions/76878387/how-does-one-create-a-pytoch-data-loader-using-an-interleaved-hugging-face-datas?noredirect=1&amp;lq=1"">How does one create a pytoch data loader using an interleaved hugging face dataset?</a>) . It just doesn't work with HF trianer.</p>
<p>Code</p>
<pre><code>from pathlib import Path
# token = open(Path('~/data/hf_token.txt').expanduser()).read().strip()
token = None
batch_size = 8

# -- AF now
from datasets import load_dataset
import torch
from transformers import GPT2Tokenizer, GPT2LMHeadModel

tokenizer = GPT2Tokenizer.from_pretrained(&quot;gpt2&quot;)
if tokenizer.pad_token_id is None:
  tokenizer.pad_token = tokenizer.eos_token
model = GPT2LMHeadModel.from_pretrained(&quot;gpt2&quot;)
device = torch.device(f&quot;cuda:{0}&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
model = model.to(device)

# -- Get batch from dataset
from datasets import load_dataset
# path, name = 'brando/debug1_af', 'debug1_af'
path, name = 'brando/debug0_af', 'debug0_af'
# train_dataset = load_dataset(path, name, streaming=True, split=&quot;train&quot;, token=token).with_format(type=&quot;torch&quot;)
# eval_dataset = load_dataset(path, name, streaming=True, split=&quot;test&quot;, token=token).with_format(type=&quot;torch&quot;)
# batch = dataset.take(1)
# column_names = next(iterbatch).keys()
# print(f'{column_names=}')

# -- Compute max steps (I think we should try to do this for real experiments such that the number of tokens is the same in all training runs for fair experiments, todo: ask Sudharsan or online, for now just make streaming=False)
train_dataset = load_dataset(path, name, streaming=False, split=&quot;train&quot;, token=token).with_format(type=&quot;torch&quot;)  # hack to get dataset size
eval_dataset = load_dataset(path, name, streaming=False, split=&quot;test&quot;, token=token).with_format(type=&quot;torch&quot;) # hack to get dataset size
print(f'{len(train_dataset)=}')
print(f'{len(eval_dataset)=}')
per_device_train_batch_size = batch_size
num_epochs = 1
max_steps = (len(train_dataset) // per_device_train_batch_size) * num_epochs
print(f'{max_steps=}')    

# -- Get trainer
def collate_tokenize(data):
    text_batch = [f'informal statement {example[&quot;generated informal statement&quot;]} formal statement {example[&quot;formal statement&quot;]}' for example in data]
    tokenized = tokenizer(text_batch, padding='longest', max_length=128, truncation=True, return_tensors='pt')
    return tokenized

from transformers import Trainer, TrainingArguments
training_args = TrainingArguments(
    output_dir=Path('./results').expanduser(),          # output directory
    max_steps=max_steps,             # max_steps
    per_device_train_batch_size=per_device_train_batch_size,   # batch size per device during training
    per_device_eval_batch_size=batch_size,    # batch size for evaluation
    warmup_steps=500,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir=Path('./logs').expanduser(),            # directory for storing logs
    logging_steps=10,
    report_to='none',
)
trainer = Trainer(
    model=model,                         # the instantiated 🤗 Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=train_dataset,         # training dataset
    eval_dataset=eval_dataset,             # evaluation dataset
    data_collator = collate_tokenize,
)
trainer.train()
print('Done!\a')
</code></pre>
<p>error:</p>
<pre><code>len(train_dataset)=14
len(eval_dataset)=13
max_steps=1
/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
&lt;ipython-input-2-4403554fc52d&gt; in &lt;cell line: 63&gt;()
     61     data_collator = collate_tokenize,
     62 )
---&gt; 63 trainer.train()
     64 print('Done!\a')

11 frames
/usr/local/lib/python3.10/dist-packages/datasets/formatting/formatting.py in _check_valid_index_key(key, size)
    524     if isinstance(key, int):
    525         if (key &lt; 0 and key + size &lt; 0) or (key &gt;= size):
--&gt; 526             raise IndexError(f&quot;Invalid key: {key} is out of bounds for size {size}&quot;)
    527         return
    528     elif isinstance(key, slice):

IndexError: Invalid key: 12 is out of bounds for size 0
</code></pre>
<p>why? How to fix?</p>
<ul>
<li>colab: <a href=""https://colab.research.google.com/drive/1io951Ex17-6OUaogCo7OiR-eXga_oUOH?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1io951Ex17-6OUaogCo7OiR-eXga_oUOH?usp=sharing</a></li>
<li>hf discuss: <a href=""https://discuss.huggingface.co/t/how-to-use-huggingface-hf-trainer-train-with-custom-collate-function/50347"" rel=""nofollow noreferrer"">https://discuss.huggingface.co/t/how-to-use-huggingface-hf-trainer-train-with-custom-collate-function/50347</a></li>
</ul>
","huggingface"
"76878387","How does one create a pytoch data loader using an interleaved hugging face dataset?","2023-08-10 18:21:06","76878652","0","565","<python><pytorch><huggingface><pytorch-dataloader><huggingface-datasets>","<p>When I interleave data sets, get a tokenized batch, feed the batch to the pytorch data loader, I get errors:</p>
<pre><code># -*- coding: utf-8 -*-
&quot;&quot;&quot;issues with dataloader and custom data sets

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sbs95as_66mtK9VK_vbaE9gLE-Tjof1-
&quot;&quot;&quot;

!pip install datasets
!pip install pytorch
!pip install transformers

token = None
batch_size = 10
from datasets import load_dataset
import torch
from transformers import GPT2Tokenizer, GPT2LMHeadModel

tokenizer = GPT2Tokenizer.from_pretrained(&quot;gpt2&quot;)
if tokenizer.pad_token_id is None:
  tokenizer.pad_token = tokenizer.eos_token
probe_network = GPT2LMHeadModel.from_pretrained(&quot;gpt2&quot;)
device = torch.device(f&quot;cuda:{0}&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
probe_network = probe_network.to(device)

# -- Get batch from dataset
from datasets import load_dataset
# path, name = 'brando/debug1_af', 'debug1_af'
path, name = 'brando/debug0_af', 'debug0_af'
remove_columns = []
dataset = load_dataset(path, name, streaming=True, split=&quot;train&quot;, token=token).with_format(&quot;torch&quot;)
print(f'{dataset=}')
batch = dataset.take(batch_size)
# print(f'{next(iter(batch))=}')

# - Prepare functions to tokenize batch
def preprocess(examples):  # gets the raw text batch according to the specific names in table in data set &amp; tokenize
    return tokenizer(examples[&quot;link&quot;], padding=&quot;max_length&quot;, max_length=128, truncation=True, return_tensors=&quot;pt&quot;)
def map(batch):  # apply preprocess to batch to all examples in batch represented as a dataset
    return batch.map(preprocess, batched=True, remove_columns=remove_columns)
tokenized_batch = batch.map(preprocess, batched=True, remove_columns=remove_columns)
tokenized_batch = map(batch)
# print(f'{next(iter(tokenized_batch))=}')

from torch.utils.data import Dataset, DataLoader, SequentialSampler
dataset = tokenized_batch
print(f'{type(dataset)=}')
print(f'{dataset.__class__=}')
print(f'{isinstance(dataset, Dataset)=}')
# for i, d in enumerate(dataset):
#     assert isinstance(d, dict)
#     # dd = dataset[i]
#     # assert isinstance(dd, dict)
loader_opts = {}
classifier_opts = {}
# data_loader = DataLoader(dataset, shuffle=False, batch_size=loader_opts.get('batch_size', 1),
#                         num_workers=loader_opts.get('num_workers', 0), drop_last=False, sampler=SequentialSampler(range(512))  )
data_loader = DataLoader(dataset, shuffle=False, batch_size=loader_opts.get('batch_size', 1),
                    num_workers=loader_opts.get('num_workers', 0), drop_last=False, sampler=None)
print(f'{iter(data_loader)=}')
print(f'{next(iter(data_loader))=}')
print('Done\a')
</code></pre>
<p>with error:</p>
<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py in collate(batch, collate_fn_map)
    126         try:
--&gt; 127             return elem_type({key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem})
    128         except TypeError:

9 frames
TypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found &lt;class 'NoneType'&gt;

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py in collate(batch, collate_fn_map)
    148                 return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]
    149 
--&gt; 150     raise TypeError(default_collate_err_msg_format.format(elem_type))
    151 
    152 

TypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found &lt;class 'NoneType'&gt;
</code></pre>
<p>why? And why doesn't the single data set c4 and wiki-text give this error? Only interleaved data sets?</p>
<p>Ideally I don't want to write my own collate_function.</p>
<ul>
<li>colab: <a href=""https://colab.research.google.com/drive/1sbs95as_66mtK9VK_vbaE9gLE-Tjof1-?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1sbs95as_66mtK9VK_vbaE9gLE-Tjof1-?usp=sharing</a></li>
<li>related: <a href=""https://stackoverflow.com/questions/76872115/how-does-one-create-a-pytorch-data-loader-with-a-custom-hugging-face-data-set-wi"">How does one create a pytorch data loader with a custom hugging face data set without having errors?</a></li>
<li>hf discuss: <a href=""https://discuss.huggingface.co/t/how-does-one-create-a-pytoch-data-loader-using-an-interleaved-hugging-face-dataset/50320"" rel=""nofollow noreferrer"">https://discuss.huggingface.co/t/how-does-one-create-a-pytoch-data-loader-using-an-interleaved-hugging-face-dataset/50320</a></li>
</ul>
","huggingface"
"76877932","Using token_ids directly for response_template in DataCollatorForLM (not working)?","2023-08-10 17:09:38","","1","474","<python><pytorch><huggingface-transformers><huggingface><huggingface-datasets>","<p>I'm trying to use DataCollatorForCompletionOnlyLM in my SFT script. I'm using LLAMA2 as a base model and therefore I'm running into the same problem that is mentioned in the <a href=""https://huggingface.co/docs/trl/main/en/sft_trainer"" rel=""nofollow noreferrer"">SFT write-up from HF</a>. I've tried the proposed fix in the write-up, which is to encode the contextualized response key and then pass that directly in to the collator, however, I run into the following error:</p>
<p><code>ValueError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]]</code></p>
<p>Here is what I'm trying:</p>
<pre><code>response_template_with_context=&quot;\n### Response:&quot;
response_template_ids = tokenizer.encode(response_template_with_context, add_special_tokens=False)
data_collator = DataCollatorForCompletionOnlyLM(response_template=response_template_ids, tokenizer=tokenizer, mlm=False, return_tensors=&quot;pt&quot;, pad_to_multiple_of=8)
</code></pre>
<p>and then configure my <code>TrainingArguments</code> and <code>SFTTrainer</code></p>
<pre><code>trainer = SFTTrainer(
    model=model,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    peft_config=peft_config,
    max_seq_length=script_args.seq_length,
    tokenizer=tokenizer,
    dataset_text_field=&quot;text&quot;,
    packing=False,
    data_collator=data_collator,
    # formatting_func=format_samples,
    args=args,
)
</code></pre>
<p>and run <code>trainer.train()</code> which yields the error. Any idea how I might be able to work around this?</p>
","huggingface"
"76877573","HuggingFace Inference Endpoints extremely slow performance","2023-08-10 16:14:52","","2","2607","<huggingface-transformers><word-embedding><huggingface>","<p>I compute vector embeddings for text paragraphs using the <a href=""https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2"" rel=""nofollow noreferrer"">all-MiniLM-L6-v2 model at HuggingFace</a>. Since the free endpoint wasn't always responsive enough and I need to be able to scale, I deployed the model to HuggingFace Inference Endpoints. To begin with, I chose the cheapest endpoint.</p>
<p>To my surprise, a single request to compute 35 embeddings took more than 7 seconds (according to the log at HuggingFace). Based on the suggestion of HuggingFace support, I tried to upgrade to 2 CPUs and it got even slower (to tell the truth, I am not sure why they thought that a single request would benefit from another CPU). Next, I tried GPU. The request now takes 2 seconds.</p>
<p>I must be missing something, because it seems impossible that one would pay &gt;$400/month to serve a single request in 2 seconds, rather than serving thousands of request per second.</p>
<p>I guess that I must be missing something, but I don't see what it could be.</p>
<p>I submit the requests using the command in the following format:</p>
<pre class=""lang-bash prettyprint-override""><code>curl https://xxxxxxxxxxxxxx.us-east-1.aws.endpoints.huggingface.cloud -X POST -d '{&quot;inputs&quot;: [&quot;My paragraphs are of about 200 words on average&quot;, &quot;Another paragraph&quot;, etc.]}' -H 'Authorization: Bearer xxxxxxxxxxxxxxxxxxxxxxxxxx' -H 'Content-Type: application/json'
</code></pre>
<p>What could I be missing?</p>
<p>P.S. For the GPU, it does get much better once warmed up, achieving 100ms. However, this particular model <a href=""https://www.sbert.net/docs/pretrained_models.html#model-overview"" rel=""nofollow noreferrer"">achieves 14,200 embeddings per second on A100</a>. Granted it's not A100 that I ran it on, but 350 embeddings per second is still way too slow.</p>
","huggingface"
"76875743","Unable to run a model using HuggingFace Inference Endpoints","2023-08-10 12:28:32","76876690","1","1030","<huggingface-transformers><huggingface>","<p>I am able to make successful requests using the free endpoint, but when using Inference Endpoints, I get 404 response. Here is the relevant piece of code:</p>
<pre class=""lang-py prettyprint-override""><code>mode = 'paid'                                              # works if 'free'
model_id = &quot;sentence-transformers/all-MiniLM-L6-v2&quot;
headers = {&quot;Authorization&quot;: f&quot;Bearer {HUGGINGFACE_TOKEN}&quot;}

if mode == 'free':
    # This works
    api_url = f&quot;https://api-inference.huggingface.co/pipeline/feature-extraction/{model_id}&quot;
else:
    api_url = f&quot;https://xxxxxxxxxxxxxxxxx.us-east-1.aws.endpoints.huggingface.cloud/{model_id}&quot;

def get_embeddings(texts):
    response = requests.post(api_url, headers=headers, json={&quot;inputs&quot;: texts, &quot;options&quot;:{&quot;wait_for_model&quot;:True}})
</code></pre>
<p>In the web UI, the endpoint is shown as running and I can test it there no problem.</p>
<p>What am I missing?</p>
","huggingface"
"76875718","KeyError: ""marketplace"" while downloading ""amazon_us_reviews"" dataset - huggingface datasets","2023-08-10 12:24:05","76878017","0","154","<python><huggingface-transformers><huggingface><huggingface-datasets>","<p>I am trying to download the <code>amazon_us_reviews</code> dataset using the following code:</p>
<pre><code>from datasets import load_dataset
dataset = load_dataset(&quot;amazon_us_reviews&quot;, &quot;Toys_v1_00&quot;)
</code></pre>
<p>I am getting the following error:</p>
<pre><code>KeyError                                  Traceback (most recent call last)
/usr/local/lib/python3.10/dist-packages/datasets/builder.py in _prepare_split_single(self, gen_kwargs, fpath, file_format, max_shard_size, split_info, check_duplicate_keys, job_id)
   1692         )
-&gt; 1693         self._beam_writers[split_name] = beam_writer
   1694 

11 frames
/usr/local/lib/python3.10/dist-packages/datasets/features/features.py in encode_example(self, example)
   1850         ```
-&gt; 1851         &quot;&quot;&quot;
   1852         return copy.deepcopy(self)

/usr/local/lib/python3.10/dist-packages/datasets/features/features.py in encode_nested_example(schema, obj, level)
   1228     &quot;&quot;&quot;Decode a nested example.
-&gt; 1229     This is used since some features (in particular Audio and Image) have some logic during decoding.
   1230 

/usr/local/lib/python3.10/dist-packages/datasets/features/features.py in &lt;dictcomp&gt;(.0)
   1228     &quot;&quot;&quot;Decode a nested example.
-&gt; 1229     This is used since some features (in particular Audio and Image) have some logic during decoding.
   1230 

/usr/local/lib/python3.10/dist-packages/datasets/utils/py_utils.py in zip_dict(*dicts)
    321     def __get__(self, obj, objtype=None):
--&gt; 322         return self.fget.__get__(None, objtype)()
    323 

/usr/local/lib/python3.10/dist-packages/datasets/utils/py_utils.py in &lt;genexpr&gt;(.0)
    321     def __get__(self, obj, objtype=None):
--&gt; 322         return self.fget.__get__(None, objtype)()
    323 

KeyError: 'marketplace'

The above exception was the direct cause of the following exception:

DatasetGenerationError                    Traceback (most recent call last)
&lt;ipython-input-25-341913a5da6a&gt; in &lt;cell line: 1&gt;()
----&gt; 1 dataset = load_dataset(&quot;amazon_us_reviews&quot;, &quot;Toys_v1_00&quot;)

/usr/local/lib/python3.10/dist-packages/datasets/load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)

/usr/local/lib/python3.10/dist-packages/datasets/builder.py in download_and_prepare(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)
    952         with FileLock(lock_path) if is_local else contextlib.nullcontext():
    953             self.info.write_to_directory(self._output_dir, fs=self._fs)
--&gt; 954 
    955     def _save_infos(self):
    956         is_local = not is_remote_filesystem(self._fs)

/usr/local/lib/python3.10/dist-packages/datasets/builder.py in _download_and_prepare(self, dl_manager, verification_mode, **prepare_splits_kwargs)

/usr/local/lib/python3.10/dist-packages/datasets/builder.py in _download_and_prepare(self, dl_manager, verification_mode, **prepare_split_kwargs)
   1047             in_memory=in_memory,
   1048         )
-&gt; 1049         if run_post_process:
   1050             for resource_file_name in self._post_processing_resources(split).values():
   1051                 if os.sep in resource_file_name:

/usr/local/lib/python3.10/dist-packages/datasets/builder.py in _prepare_split(self, split_generator, check_duplicate_keys, file_format, num_proc, max_shard_size)
   1553 
   1554 class BeamBasedBuilder(DatasetBuilder):
-&gt; 1555     &quot;&quot;&quot;Beam based Builder.&quot;&quot;&quot;
   1556 
   1557     # BeamBasedBuilder does not have dummy data for tests yet

/usr/local/lib/python3.10/dist-packages/datasets/builder.py in _prepare_split_single(self, gen_kwargs, fpath, file_format, max_shard_size, split_info, check_duplicate_keys, job_id)

DatasetGenerationError: An error occurred while generating the dataset
</code></pre>
<p>I tried with the <code>load_dataset_builder</code> it is showing the following features:</p>
<pre><code>from datasets import load_dataset_builder
ds_builder = load_dataset_builder(&quot;amazon_us_reviews&quot;, &quot;Toys_v1_00&quot;)
print(ds_builder.info.features)
</code></pre>
<p>Output:</p>
<pre><code>{'marketplace': Value(dtype='string', id=None),
 'customer_id': Value(dtype='string', id=None),
 'review_id': Value(dtype='string', id=None),
 'product_id': Value(dtype='string', id=None),
 'product_parent': Value(dtype='string', id=None),
 'product_title': Value(dtype='string', id=None),
 'product_category': Value(dtype='string', id=None),
 'star_rating': Value(dtype='int32', id=None),
 'helpful_votes': Value(dtype='int32', id=None),
 'total_votes': Value(dtype='int32', id=None),
 'vine': ClassLabel(names=['N', 'Y'], id=None),
 'verified_purchase': ClassLabel(names=['N', 'Y'], id=None),
 'review_headline': Value(dtype='string', id=None),
 'review_body': Value(dtype='string', id=None),
 'review_date': Value(dtype='string', id=None)}
</code></pre>
<p>The <code>datasets</code> version used is <code>2.14.4</code></p>
<p>Is this the correct way to download the dataset? Kindly advise.</p>
","huggingface"
"76874632","Googl Colab SDXL Base + Refiner Crashes","2023-08-10 09:57:53","77001431","1","748","<google-colaboratory><huggingface><stable-diffusion>","<p>I am using this sample python code from Hugging Face</p>
<pre><code>from diffusers import DiffusionPipeline
import torch

# load both base &amp; refiner
base = DiffusionPipeline.from_pretrained(
    &quot;stabilityai/stable-diffusion-xl-base-1.0&quot;, torch_dtype=torch.float16, variant=&quot;fp16&quot;, use_safetensors=True
)
base.to(&quot;cuda&quot;)
refiner = DiffusionPipeline.from_pretrained(
    &quot;stabilityai/stable-diffusion-xl-refiner-1.0&quot;,
    text_encoder_2=base.text_encoder_2,
    vae=base.vae,
    torch_dtype=torch.float16,
    use_safetensors=True,
    variant=&quot;fp16&quot;,
)
refiner.to(&quot;cuda&quot;)

# Define how many steps and what % of steps to be run on each experts (80/20) here
n_steps = 40
high_noise_frac = 0.8

prompt = &quot;A majestic lion jumping from a big stone at night&quot;

# run both experts
image = base(
    prompt=prompt,
    num_inference_steps=n_steps,
    denoising_end=high_noise_frac,
    output_type=&quot;latent&quot;,
).images
image = refiner(
    prompt=prompt,
    num_inference_steps=n_steps,
    denoising_start=high_noise_frac,
    image=image,
).images[0]
image
</code></pre>
<p>But every time the image is about the be generated, the Google Colab session crashes. Has anyone successfully tried the Base + Refiner in Google Colab? I have successfully tried only the Base but I want to try both at the same time.</p>
","huggingface"
"76873765","Converting pixstruct huggingface model to ONNX format","2023-08-10 08:07:05","","0","215","<python><onnx><huggingface>","<p>I want to convert <code>pix2struct huggingface</code> base model to <code>ONNX</code> format. I write the code for that.</p>
<pre><code>import torch
import torch.onnx as onnx
from transformers import AutoModel
import onnx
import onnxruntime
from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor

# Load the Hugging Face model
model = Pix2StructForConditionalGeneration.from_pretrained(&quot;google/pix2struct-ai2d-base&quot;)

# Set the model to evaluation mode
model.eval()

# Create a sample input tensor
input_ids = torch.tensor([[1, 2, 3, 4, 5]])

# Export the model to ONNX format
onnx_path = &quot;model/pix2struct.onnx&quot;
# Prepare a sample input
input_ids = torch.tensor([[1, 2, 3, 4, 5]])

# Export the model to ONNX format
dummy_input = input_ids  # Use the sample input as the dummy input
onnx_path = &quot;model/pix2struct.onnx&quot;  # Path to save the ONNX model
torch.onnx.export(model, dummy_input, onnx_path, opset_version=11)

# Load the ONNX model
onnx_model = onnx.load(onnx_path)

# Validate the ONNX model
onnx.checker.check_model(onnx_model)

# Create an ONNX runtime session
session = onnxruntime.InferenceSession(onnx_path)

# Perform inference using the ONNX model
input_name = session.get_inputs()[0].name
output_name = session.get_outputs()[0].name
input_data = {input_name: input_ids.numpy()}
output = session.run([output_name], input_data)

# Print the output
print(output)
print(&quot;Model converted to ONNX successfully!&quot;)
</code></pre>
<p>but in the torch.onnx.export funtion shows a error that dimention of the torch funtion is 2, I want to resolve the error, please help me</p>
<p>I want the help to debug my code.</p>
","huggingface"
"76873456","ERROR: The prompt size exceeds the context window size and cannot be processed","2023-08-10 07:23:24","76876933","3","5593","<langchain><huggingface><large-language-model><llama-index><gpt4all>","<p>I have been trying to create a document QA chatbot using GPT4ALL as the llm and hugging face's instructor-large model for embedding, I was able to create the index, but getting the following as a response, it's not really a error which I'm getting as there is no traceback but it's just showing me the following</p>
<p><code>ERROR: The prompt size exceeds the context window size and cannot be processed.ERROR: The prompt size exceeds the context window size and cannot be processed</code></p>
<p>This is a follow up question for the following question <a href=""https://stackoverflow.com/questions/76866751/i-dont-understand-how-the-prompts-work-in-llama-index"">parent question (this was resolved)</a></p>
<pre><code>from llama_index import VectorStoreIndex, SimpleDirectoryReader
from InstructorEmbedding import INSTRUCTOR
from llama_index import PromptHelper, ServiceContext
from llama_index import LangchainEmbedding
from langchain.chat_models import ChatOpenAI
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.llms import OpenLLM
# from langchain.chat_models.human import HumanInputChatModel
from langchain import PromptTemplate, LLMChain
from langchain.llms import GPT4All
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

documents = SimpleDirectoryReader(r'C:\Users\avish.wagde\Documents\work_avish\LLM_trials\instructor_large').load_data()

print('document loaded in memory.......') 

model_id = 'hkunlp/instructor-large'

model_path = &quot;..\models\GPT4All-13B-snoozy.ggmlv3.q4_0.bin&quot;

callbacks = [StreamingStdOutCallbackHandler()]

# Verbose is required to pass to the callback manager
llm = GPT4All(model = model_path, callbacks=callbacks, verbose=True)

print('llm model ready.............')

embed_model = LangchainEmbedding(HuggingFaceEmbeddings(model_name = model_id))

print('embedding model ready.............')

# define prompt helper
# set maximum input size
max_input_size = 4096
# set number of output tokens
num_output = 256
# set maximum chunk overlap
max_chunk_overlap = 0.2

prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)

service_context = ServiceContext.from_defaults(chunk_size= 1024, llm=llm, prompt_helper=prompt_helper, embed_model=embed_model)

print('service context set...........')

index = VectorStoreIndex.from_documents(documents, service_context= service_context)

print('indexing done................')

query_engine = index.as_query_engine()

print('query set...........')

response = query_engine.query(&quot;What is apple's finnacial situation&quot;)
print(response)
</code></pre>
<p>here is the screenshot of response i got..
<a href=""https://i.sstatic.net/yuWb4.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>I check over GitHub, many people raised this but I couldn't find anything to resolve this..
<a href=""https://github.com/nomic-ai/gpt4all/issues/664"" rel=""nofollow noreferrer"">The  GitHub link for the query</a></p>
","huggingface"
"76872115","How does one create a pytorch data loader with a custom hugging face data set without having errors?","2023-08-10 01:24:33","","0","1444","<python><huggingface-transformers><huggingface><huggingface-datasets><huggingface-hub>","<p>Currently my custom data set gives None indices in the data loader, but NOT in the pure data set. When I wrap it in pytorch data loader it fails.</p>
<p>Code is in <a href=""https://colab.research.google.com/drive/1sbs95as_66mtK9VK_vbaE9gLE-Tjof1-?usp=sharing"" rel=""nofollow noreferrer"">colab</a> but will put it here in case colab dies someday:</p>
<pre><code>pip install datasets
pip install pytorch
pip install transformers
</code></pre>
<p>then run</p>
<pre><code>token = None
batch_size = 10
from datasets import load_dataset
import torch
from transformers import GPT2Tokenizer, GPT2LMHeadModel

tokenizer = GPT2Tokenizer.from_pretrained(&quot;gpt2&quot;)
if tokenizer.pad_token_id is None:
  tokenizer.pad_token = tokenizer.eos_token
probe_network = GPT2LMHeadModel.from_pretrained(&quot;gpt2&quot;)
device = torch.device(f&quot;cuda:{0}&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
probe_network = probe_network.to(device)

# -- Get batch from dataset
from datasets import load_dataset
# path, name = 'brando/debug1_af', 'debug1_af'
path, name = 'brando/debug0_af', 'debug0_af'
remove_columns = []
dataset = load_dataset(path, name, streaming=True, split=&quot;train&quot;, token=token).with_format(&quot;torch&quot;)
print(f'{dataset=}')
batch = dataset.take(batch_size)
# print(f'{next(iter(batch))=}')

# - Prepare functions to tokenize batch
def preprocess(examples):  # gets the raw text batch according to the specific names in table in data set &amp; tokenize
    return tokenizer(examples[&quot;link&quot;], padding=&quot;max_length&quot;, max_length=128, truncation=True, return_tensors=&quot;pt&quot;)
def map(batch):  # apply preprocess to batch to all examples in batch represented as a dataset
    return batch.map(preprocess, batched=True, remove_columns=remove_columns)
tokenized_batch = batch.map(preprocess, batched=True, remove_columns=remove_columns)
tokenized_batch = map(batch)
# print(f'{next(iter(tokenized_batch))=}')

from torch.utils.data import Dataset, DataLoader, SequentialSampler
dataset = tokenized_batch
print(f'{type(dataset)=}')
print(f'{dataset.__class__=}')
print(f'{isinstance(dataset, Dataset)=}')
# for i, d in enumerate(dataset):
#     assert isinstance(d, dict)
#     # dd = dataset[i]
#     # assert isinstance(dd, dict)
loader_opts = {}
classifier_opts = {} 
# data_loader = DataLoader(dataset, shuffle=False, batch_size=loader_opts.get('batch_size', 1),
#                         num_workers=loader_opts.get('num_workers', 0), drop_last=False, sampler=SequentialSampler(range(512))  )
data_loader = DataLoader(dataset, shuffle=False, batch_size=loader_opts.get('batch_size', 1),
                    num_workers=loader_opts.get('num_workers', 0), drop_last=False, sampler=None)
print(f'{iter(data_loader)=}')
print(f'{next(iter(data_loader))=}')
print('Done\a')
</code></pre>
<p>error:</p>
<pre><code>dataset=&lt;datasets.iterable_dataset.IterableDataset object at 0x7e42c2f21d20&gt;
type(dataset)=&lt;class 'datasets.iterable_dataset.IterableDataset'&gt;
dataset.__class__=&lt;class 'datasets.iterable_dataset.IterableDataset'&gt;
isinstance(dataset, Dataset)=True
iter(data_loader)=&lt;torch.utils.data.dataloader._SingleProcessDataLoaderIter object at 0x7e42c2f21660&gt;
/usr/local/lib/python3.10/dist-packages/datasets/formatting/torch_formatter.py:68: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(value, **{**default_dtype, **self.torch_tensor_kwargs})
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py in collate(batch, collate_fn_map)
    126         try:
--&gt; 127             return elem_type({key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem})
    128         except TypeError:

9 frames
/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py in &lt;dictcomp&gt;(.0)
    126         try:
--&gt; 127             return elem_type({key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem})
    128         except TypeError:

/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py in collate(batch, collate_fn_map)
    149 
--&gt; 150     raise TypeError(default_collate_err_msg_format.format(elem_type))
    151 

TypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found &lt;class 'NoneType'&gt;

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
&lt;ipython-input-6-1153c5915bd8&gt; in &lt;cell line: 49&gt;()
     47                     num_workers=loader_opts.get('num_workers', 0), drop_last=False, sampler=None)
     48 print(f'{iter(data_loader)=}')
---&gt; 49 print(f'{next(iter(data_loader))=}')
     50 print('Done\a')

/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py in __next__(self)
    631                 # TODO(https://github.com/pytorch/pytorch/issues/76750)
    632                 self._reset()  # type: ignore[call-arg]
--&gt; 633             data = self._next_data()
    634             self._num_yielded += 1
    635             if self._dataset_kind == _DatasetKind.Iterable and \

/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py in _next_data(self)
    675     def _next_data(self):
    676         index = self._next_index()  # may raise StopIteration
--&gt; 677         data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
    678         if self._pin_memory:
    679             data = _utils.pin_memory.pin_memory(data, self._pin_memory_device)

/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py in fetch(self, possibly_batched_index)
     40         else:
     41             data = next(self.dataset_iter)
---&gt; 42         return self.collate_fn(data)
     43 
     44 

/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py in default_collate(batch)
    263             &gt;&gt;&gt; default_collate(batch)  # Handle `CustomType` automatically
    264     &quot;&quot;&quot;
--&gt; 265     return collate(batch, collate_fn_map=default_collate_fn_map)

/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py in collate(batch, collate_fn_map)
    128         except TypeError:
    129             # The mapping type may not support `__init__(iterable)`.
--&gt; 130             return {key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem}
    131     elif isinstance(elem, tuple) and hasattr(elem, '_fields'):  # namedtuple
    132         return elem_type(*(collate(samples, collate_fn_map=collate_fn_map) for samples in zip(*batch)))

/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py in &lt;dictcomp&gt;(.0)
    128         except TypeError:
    129             # The mapping type may not support `__init__(iterable)`.
--&gt; 130             return {key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem}
    131     elif isinstance(elem, tuple) and hasattr(elem, '_fields'):  # namedtuple
    132         return elem_type(*(collate(samples, collate_fn_map=collate_fn_map) for samples in zip(*batch)))

/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py in collate(batch, collate_fn_map)
    148                 return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]
    149 
--&gt; 150     raise TypeError(default_collate_err_msg_format.format(elem_type))
    151 
    152 

TypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found &lt;class 'NoneType'&gt;
</code></pre>
<p>why is this error happening?</p>
<p>I've done all the checks, e.g., make sure the return indices are dicts, even went in detail debugging mode with pdb inside of pytorch's code.</p>
<ul>
<li><p>hf discuss: <a href=""https://discuss.huggingface.co/t/how-does-one-create-a-pytorch-data-loader-with-a-custom-hugging-face-data-set-without-having-errors/50204"" rel=""nofollow noreferrer"">https://discuss.huggingface.co/t/how-does-one-create-a-pytorch-data-loader-with-a-custom-hugging-face-data-set-without-having-errors/50204</a></p>
</li>
<li><p>hf discord: <a href=""https://discord.com/channels/879548962464493619/1139007085363875922/1139007085363875922"" rel=""nofollow noreferrer"">https://discord.com/channels/879548962464493619/1139007085363875922/1139007085363875922</a></p>
</li>
</ul>
","huggingface"
"76869038","Import folders from google colab to huggingface","2023-08-09 15:02:25","","0","540","<import><google-colaboratory><huggingface>","<p>So, how do i save 2 folders from a google colab runtime to huggingface (i have a hf token bdw)?
The folders are
/content/stable/models/Lora
/content/stable/models/LyCORIS</p>
<p>and i want to save the Lora folder in: <a href=""https://huggingface.co/PrivateCassy/Lora/tree/main"" rel=""nofollow noreferrer"">https://huggingface.co/PrivateCassy/Lora/tree/main</a>
and the LyCORIS to save in: <a href=""https://huggingface.co/PrivateCassy/LyCORIS/tree/main"" rel=""nofollow noreferrer"">https://huggingface.co/PrivateCassy/LyCORIS/tree/main</a></p>
<p>This is what i tried but didn't work</p>
<pre><code>import shutil
from transformers import datasets

# Hugging Face API token
api_token = input(&quot;the api tokken&quot;)

datasets.config.HfFolder.set_base_dataset_hf_folder('/content/hf_datasets')
datasets.set_hf_api_token(api_token)

Lora_LyCORIS_on_hf = &quot;Load&quot; #@param [&quot;Save&quot;,&quot;Load&quot;]

src = '/content/stable/'

match os.path.exists(src):
    case False:
      os.makedirs(src)
    case _:
      pass

match Lora_LyCORIS_on_hf:
  case &quot;Save&quot;:
    # Upload Lora and LyCORIS to Hugging Face
    datasets.upload_dataset(src)
  case &quot;Load&quot;:
    # Download Lora and LyCORIS from Hugging Face
    dataset_lora = datasets.load_dataset(src)

    # Copy downloaded data to the desired location
    shutil.copytree(dataset_lora.data_dir, src, dirs_exist_ok=True)```
</code></pre>
","huggingface"
"76866751","I don't understand how the prompts work in llama_index","2023-08-09 10:13:43","76868784","4","10078","<langchain><huggingface><large-language-model><llama-index><vector-database>","<p>I have been trying to query a pdf file in my local directory using LLM, I have downloaded the LLM model I'm using in my local system (GPT4All-13B-snoozy.ggmlv3.q4_0.bin) and trying to use langchain and hugging face's instructor-large model for embedding purpose, I was able to set the service_context and then building index but I'm not able to query , I keeping getting this error regarding prompt..</p>
<blockquote>
<p>ValueError: Argument <code>prompt</code> is expected to be a string. Instead found &lt;class 'llama_index.prompts.base.Prompt'&gt;. If you want to run the LLM on multiple prompts, use <code>generate</code> instead.</p>
</blockquote>
<p>I'm just starting to learn how to use LLM, hope the community helps me....</p>
<p><a href=""https://i.sstatic.net/1TuW0.png"" rel=""noreferrer"">error message part1</a></p>
<p><a href=""https://i.sstatic.net/zOcAX.png"" rel=""noreferrer"">error message part2</a></p>
<pre><code>from llama_index import VectorStoreIndex, SimpleDirectoryReader
from InstructorEmbedding import INSTRUCTOR
from llama_index import PromptHelper, ServiceContext
from llama_index import LangchainEmbedding
from langchain.chat_models import ChatOpenAI
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.llms import OpenLLM
# from langchain.chat_models.human import HumanInputChatModel
from langchain import PromptTemplate, LLMChain
from langchain.llms import GPT4All
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

documents = SimpleDirectoryReader(r'C:\Users\avish.wagde\Documents\work_avish\LLM_trials\instructor_large').load_data()

model_id = 'hkunlp/instructor-large'

model_path = &quot;..\models\GPT4All-13B-snoozy.ggmlv3.q4_0.bin&quot;

callbacks = [StreamingStdOutCallbackHandler()]

# Verbose is required to pass to the callback manager
llm = GPT4All(model = model_path, callbacks=callbacks, verbose=True)

embed_model = LangchainEmbedding(HuggingFaceEmbeddings(model_name = model_id))

# define prompt helper
# set maximum input size
max_input_size = 4096
# set number of output tokens
num_output = 256
# set maximum chunk overlap
max_chunk_overlap = 0.2

prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)

service_context = ServiceContext.from_defaults(chunk_size= 1024, llm_predictor=llm, prompt_helper=prompt_helper, embed_model=embed_model)

index = VectorStoreIndex.from_documents(documents, service_context= service_context)

query_engine = index.as_query_engine()

response = query_engine.query(&quot;What is apple's finnacial situation&quot;)
print(response)

</code></pre>
<p>I have been going through, the source code of the library as the error message guides but I couldn't find the problem😓</p>
","huggingface"
"76865897","Why i can't use EarlyStoppingCallback and load_best_model_at_end=False","2023-08-09 08:27:25","","1","375","<python><nlp><huggingface-transformers><huggingface>","<p>I want to use the trainer over 5 folds. I wanted to add EarlyStoppingCallback to the trainer function to make it stop if the training is not improving. I get this error:
<code>AssertionError: EarlyStoppingCallback requires load_best_model_at_end = True</code>
The following is the code I used:</p>
<pre><code>training_args = Seq2SeqTrainingArguments(
    output_dir=&quot;./logs&quot;,
    evaluation_strategy=&quot;epoch&quot;,
    logging_strategy=&quot;epoch&quot;,
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    weight_decay=0.01,
    num_train_epochs=5,
    save_total_limit=2,
    save_strategy=&quot;epoch&quot;,
    load_best_model_at_end=True,
    predict_with_generate=True,
    fp16=False,
    push_to_hub=False,
)

for train_dataset, val_dataset in zip(train_ds, val_ds):
    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=compute_metrics,
        callbacks=[
            CombinedTensorBoardCallback,
            EarlyStoppingCallback(early_stopping_patience=3),
        ],
    )

    train_result = trainer.train()
</code></pre>
<p>why i can’t use EarlyStoppingCallback and load_best_model_at_end=False? I just want to save the best model at the level of each fold. Another question: is there a way to save the best model among the 5 folds?</p>
","huggingface"
"76863889","How does one fix an interleaved data set from only sampling one data set?","2023-08-09 00:37:43","76868861","0","92","<python><huggingface-transformers><huggingface><huggingface-datasets>","<p>The following</p>
<pre><code>from datasets import load_dataset
from datasets import interleave_datasets

# Preprocess each dataset
c4 = load_dataset(&quot;c4&quot;, &quot;en&quot;, split=&quot;train&quot;, streaming=True) 
wikitext = load_dataset(&quot;wikitext&quot;, &quot;wikitext-103-v1&quot;, split=&quot;train&quot;, streaming=True)

# Interleave the preprocessed datasets  
datasets = [c4, wikitext]
for dataset in datasets:
  print(dataset.description)
interleaved = interleave_datasets(datasets, probabilities=[0.5, 0.5])
print(interleaved)
</code></pre>
<p>only samples from one data set, why?</p>
<pre><code>example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
example.keys()=dict_keys(['text', 'timestamp', 'url'])
counts=100
</code></pre>
<p>colab: <a href=""https://colab.research.google.com/drive/1VIR66U1d7qk3Q1vU_URoo5tHEEheORpN?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1VIR66U1d7qk3Q1vU_URoo5tHEEheORpN?usp=sharing</a></p>
<hr />
<p>cross:</p>
<ul>
<li>hf discord: <a href=""https://discord.com/channels/879548962464493619/1138632039197835354"" rel=""nofollow noreferrer"">https://discord.com/channels/879548962464493619/1138632039197835354</a></li>
<li>hf discuss: <a href=""https://discuss.huggingface.co/t/how-does-one-fix-an-interleaved-data-set-from-only-sampling-one-data-set/50041"" rel=""nofollow noreferrer"">https://discuss.huggingface.co/t/how-does-one-fix-an-interleaved-data-set-from-only-sampling-one-data-set/50041</a></li>
</ul>
","huggingface"
"76857722","Huggingface SFT for completion only not working","2023-08-08 08:25:48","77026571","3","4365","<python><pytorch><huggingface-transformers><huggingface><huggingface-trainer>","<p>I have a project where I am trying to finetune <em>Llama-2-7b</em> on a dataset for Parameter extraction, which is linked here: &lt;GalaktischeGurke/parameter_extraction_1500_mail_contract_invoice&gt;. The problem with the dataset is that the context for a response is very big, meaning that training on the entire dataset with context, not only on the response results in a huge loss of performance. To fix this issue, I wanted to use <em>SFT_trainer</em> together with the <em>DataCollatorForCompletionOnlyLM</em>, which allows finetuning only for response. Now, before adjusting my training loop, I wanted to try the examples given here: <a href=""https://huggingface.co/docs/trl/main/en/sft_trainer"" rel=""nofollow noreferrer"">https://huggingface.co/docs/trl/main/en/sft_trainer</a>. Specifically, I used this code from the page:</p>
<pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset
from trl import SFTTrainer, DataCollatorForCompletionOnlyLM

dataset = load_dataset(&quot;timdettmers/openassistant-guanaco&quot;, split=&quot;train&quot;)
output_dir = &quot;./results&quot;

model = AutoModelForCausalLM.from_pretrained(&quot;facebook/opt-350m&quot;)
tokenizer = AutoTokenizer.from_pretrained(&quot;facebook/opt-350m&quot;)

instruction_template = &quot;### Human:&quot;
response_template = &quot;### Assistant:&quot;
collator = DataCollatorForCompletionOnlyLM(instruction_template=instruction_template, response_template=response_template, tokenizer=tokenizer, mlm=False)

trainer = SFTTrainer(
    model,
    train_dataset=dataset,
    dataset_text_field=&quot;text&quot;,
    data_collator=collator,
)

trainer.train() 


import os
output_dir = os.path.join(output_dir, &quot;final_checkpoint&quot;)
trainer.model.save_pretrained(output_dir)

</code></pre>
<p>The training loop did not crash, but it never seemed to train at all - There was no train/loss curve on <em>wandb</em> and the model saved didnt seem to have changed.</p>
<p>These are the things I tried:
-Using the other code with preformat function
-setting packing=False on the trainer
-implementing it with my own loop, which yielded the same results
-trying to find documentation on the collator, however it is not in the official docs at <a href=""https://huggingface.co/docs/transformers/main_classes/data_collator"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/main_classes/data_collator</a></p>
<p>Does anyone know what the issue is here?</p>
","huggingface"
"76849706","FastAPI Huggingface Inference every request increases CPU RAM usage","2023-08-07 07:45:42","","1","365","<fastapi><huggingface-transformers><huggingface>","<p>We have a FastAPI server, and try to use a pretrained HuggingFace model for inference (on a vast.ai gpu server). The Inference accepts one or many images, and returns an array with the image links provided and the processed output. Neither FastAPI nor the HuggingFace inference were modified beyond what we think is &quot;normal&quot;. It works as expected, but only for a limited number of calls. With every request made to FastAPI, the RAM usage increases up to the point where all machine RAM is consumed and the entire service becomes unusable (after hundreds / thousands of requests), and must be killed and restarted.</p>
<p>FastAPI code (started with these parameters uvicorn router:app --port 8000 --host 0.0.0.0)</p>
<pre class=""lang-py prettyprint-override""><code>from fastapi import FastAPI
import image_captioning
from .common import Predict, process_data

app = FastAPI()

model = image_captioning.ImageCaptioningService()


@app.post(&quot;/predict&quot;)
async def image_captioning(predict: Predict):
    data = predict.data
    return model.run(data)

</code></pre>
<p>And this is the class which processes the image links provided, started with these parameters <code>uvicorn image_captioning:app --port 8001 --host 0.0.0.0</code></p>
<pre class=""lang-py prettyprint-override""><code>from io import BytesIO
from transformers import pipeline, ViTImageProcessor, VisionEncoderDecoderModel, \
    AutoTokenizer

from helper import get_image, measure_time, get_cuda_device
import logging

class ImageCaptioningService():
    def __init__(self) -&gt; None:
        self.model_name = 'nlpconnect/vit-gpt2-image-captioning'
        self.extractor = ViTImageProcessor.from_pretrained(self.model_name)
        self.model = VisionEncoderDecoderModel.from_pretrained(self.model_name)
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        device_num = get_cuda_device()
        self.predictor = pipeline('image-to-text', model=self.model,
                                  feature_extractor=self.extractor,
                                  tokenizer=self.tokenizer, device=device_num)

    def __predict(self, images):
        preds = []
        for image in images:
            try:
                image = get_image(image)
                preds.append(self.predictor(image)[0]['generated_text'])
            except:
                self.logger.error(f&quot;Image URL raises error {image}&quot;)
                preds.append('ERROR. Could not retrieve image.')
        return preds

    def run(self, links: list):
        pred = measure_time(self.__predict, links)
        pred['result'] = [{'url': link, 'prediction': result} for result, link in
                          zip(pred['result'], links)]
        return pred
</code></pre>
<p>Are we missing out on a call that must be made after each Inference request? This feels like such a basic thing (making Inference requests) yet we can't seem to find information on how to solve this.
The RAM usage increase happens on the ImageCaptioningService btw, we monitored this with <code>pmap</code>. Almost every request (to FastAPI) causes the ImageCaptioningService RAM usage to increase by a couple of MBs. This is a problem, because we intend to make thousands of requests.</p>
<p>Every help / hint is greatly appreciated!</p>
<p>Edited to include the cli parameters to launch the services</p>
","huggingface"
"76841121","load_dataset() from the Huggingface datasets library does not work with Python 3.9, but with Python 3.10 (KeyError())","2023-08-05 10:07:32","","0","675","<python-3.x><python-3.9><python-3.10><huggingface><huggingface-datasets>","<p>The minimal working example to load the <a href=""https://huggingface.co/datasets/squad_v2"" rel=""nofollow noreferrer"">Huggingface SQuAD v2</a> dataset using</p>
<pre><code>from datasets import load_dataset
dataset_squad_v2 = load_dataset(&quot;squad_v2&quot;)
</code></pre>
<p>does not work on my Ubuntu 22 machine with <strong>Python 3.9</strong> due to a KeyError(). It does work, however, with <strong>Python 3.10</strong>.</p>
<pre><code>lib/python3.9/site-packages/datasets/features/features.py in generate_from_dict(obj)
   1282 
   1283     if class_type == Sequence:
-&gt; 1284         return Sequence(feature=generate_from_dict(obj[&quot;feature&quot;]), length=obj[&quot;length&quot;])
   1285 
   1286     field_names = {f.name for f in fields(class_type)}

KeyError: 'length'
</code></pre>
<p>Is there a way to remedy this issue so that one can use Python 3.9? I noticed similar issues with other Huggingface datasets when using Python 3.9</p>
<p>I've tried to force-download the dataset (instead of re-using the local file) and updated the <code>datasets</code> library.</p>
","huggingface"
"76839775","ValueError - I can not load large language model falcon 7B in google colab","2023-08-05 01:10:02","","0","925","<deep-learning><nlp><huggingface-transformers><huggingface><large-language-model>","<p>I am trying to run falcon 7B on google colab and i get the following error:</p>
<p>ValueError: Could not load model tiiuae/falcon-7b-instruct with any of the following classes: (&lt;class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'&gt;, &lt;class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'&gt;).</p>
<p>¿Could you help me to solve this problem? Thank you for your time.</p>
<p>I am using this code extracted from <a href=""https://huggingface.co/blog/falcon"" rel=""nofollow noreferrer"">The Falcon has landed in the Hugging Face ecosystem</a>:</p>
<pre><code>from transformers import AutoTokenizer
import transformers
import torch

model = &quot;tiiuae/falcon-7b-instruct&quot;

tokenizer = AutoTokenizer.from_pretrained(model)

pipeline = transformers.pipeline(
&quot;text-generation&quot;,
model=model,
tokenizer=tokenizer,
torch_dtype=torch.bfloat16,
trust_remote_code=True,
device_map=&quot;auto&quot;,
)

sequences = pipeline(
&quot;Write a poem about Valencia.&quot;,
max_length=200,
do_sample=True,
top_k=10,
num_return_sequences=1,
eos_token_id=tokenizer.eos_token_id,
)

for seq in sequences:
    print(f&quot;Result: {seq['generated_text']}&quot;)
</code></pre>
<p>I have already tried re-installing Pytorch library.</p>
","huggingface"
"76831656","how to download and import (preferably using spacy and from huggin face) the latest trained official version of biobert to perform ner on medical text","2023-08-03 21:16:57","","0","114","<spacy><huggingface-transformers><named-entity-recognition><huggingface><spacy-transformers>","<p>Zhang et al. research in 2020 compared biobert and scispacy ner models accuracy, overall biobert won. How to download and import (preferably using spacy and from huggin face) the latest **trained ** official version of biobert to perform ner on **uncased ** medical text. If there is a better performing medical text ner model, please inform. The goal is to identify diagnosis, operations and *optionally * drug mentions.</p>
<p>looked at lots of hugging face code but does not support pre-trained model usage</p>
","huggingface"
"76826638","What does fine-tuning a multilingual checkpoint mean?","2023-08-03 09:23:18","76828831","1","179","<python><nlp><huggingface-transformers><huggingface><fine-tuning>","<p>I'm fine-tuning a SetFit model on a French dataset and following the guide in <a href=""https://huggingface.co/blog/setfit"" rel=""nofollow noreferrer"">huggingface</a>. They mention this point on the site that I didn't quite understand</p>
<blockquote>
<p>&quot;🌎 Multilingual support: SetFit can be used with any Sentence
Transformer on the Hub, which means you can classify text in multiple
languages by simply fine-tuning a multilingual checkpoint.&quot;</p>
</blockquote>
<p>Does that mean I must find an already finetuned SetFit model in French when loading the model? As in replace &quot;paraphrase-mpnet-base-v2&quot; below with a French one?</p>
<pre><code>model = SetFitModel.from_pretrained(&quot;sentence-transformers/paraphrase-mpnet-base-v2&quot;)
</code></pre>
","huggingface"
"76818211","SentencePiece tokenizer encodes to unknown token","2023-08-02 08:58:28","","2","585","<nlp><huggingface><huggingface-tokenizers><sentencepiece><byte-pair-encoding>","<p>I am using HuggigFace implementation of SentencePiece tokenizer, i.e., <code>SentencePieceBPETokenizer</code> and <code>SentencePieceUnigramTokenizer</code> classes. I train these tokenizers on dataset which has no unicode characters and then try to encode the string that does have unicode characters.</p>
<p>My understanding is that SentencePiece is lossless and reversible and therefore it should always encode out-of-vocabulary tokens such that it can be decoded to same string, just like <code>ByteLevelBPETokenizer</code> tokenizer. So, theoretically, SentencePiece shouldn't even need <code>&lt;unk&gt;</code> as special token. However, HuggingFace implementation does have parameter to specify unknown token as special token and it always encodes unseen unicode characters in input string as <code>&lt;unk&gt;</code>.</p>
<p>My questions are,</p>
<ol>
<li>Is this expected with SentencePiece in general and therefore its claim being lossless not really true?</li>
<li>Is this specific to HuggingFace implementation (but not to Google's)?</li>
<li>Is there anyway to make HuggingFace implementation perfectly lossless just like <code>ByteLevelBPETokenizer</code>?</li>
</ol>
<p>Thanks.</p>
","huggingface"
"76807538","Does HuggingFace have a model for finding good paragraph boundaries?","2023-07-31 22:51:00","","0","264","<nlp><huggingface-transformers><huggingface><huggingface-tokenizers>","<p>Does HuggingFace have a model that, given a long text without any line breaks, would find good (according to some semantical measure) paragraph boundaries?</p>
","huggingface"
"76806996","how to specify temperature and max_new_tokens in the curl request to Llama 2 in Huggingface Inference Endpoint?","2023-07-31 20:36:20","","0","2049","<artificial-intelligence><huggingface><llama>","<p>I'm new to AI, so apologies if wrong terminology used here.</p>
<p>I'm extracting some information from a body of text, and have setup Llama 2 in Huggingface via their Inference Endpoint so I can call it via curl.</p>
<p>The curl works for short inputs and generated_text answers, but for longer responses the answers seem to be severely truncated, like only a few words whereas I'm expecting to receive a lot more.</p>
<p>So I wanted to set max_new_tokens to a large number and temperature to 0, but I didn't see how to do that.  I don't care if it's set in the curl call or configured directly in the model, either is fine.  Anyone know how to do this?</p>
","huggingface"
"76803710","Add a Classification Head on Top of Huggingface Vilt Model","2023-07-31 12:37:58","","2","1240","<python><deep-learning><pytorch><huggingface-transformers><huggingface>","<p>I want to add a classification layer in pytorch on top of the <a href=""https://huggingface.co/docs/transformers/model_doc/vilt"" rel=""nofollow noreferrer"">huggingface vilt transformer</a>, so that I can classify my text labels.</p>
<p>Generally in normal settings vilt takes an image, question pair and outputs the answer of the question after forward pass</p>
<p>I Want to make the task a classification task instead of a text generation task. I have a set of labels which I want the vilt to assign which label has the highest probability of being the answer of the given question.</p>
<p>I'm completely new to the transformers and have very little idea of how this task can be achieved. Can someone please help me?</p>
<p>I checked this medium <a href=""https://towardsdatascience.com/adding-custom-layers-on-top-of-a-hugging-face-model-f1ccdfc257bd"" rel=""nofollow noreferrer"">blog</a> but couldn't make sense out of it.</p>
","huggingface"
"76799701","download hugging face llama2 model to local server","2023-07-30 21:33:19","","1","7147","<python-3.x><pytorch><huggingface-transformers><huggingface><large-language-model>","<p>I am running the pytorch code below.  I'm running the code in a jupyter notebook.  the noebook is running on my ubuntu server.  I'm trying to download the llama2-70b-chat model from hugging face.  my goal is to download the model weights from hugging face and save them locally on my server, so that I can work with the LLM on my ubuntu server where I have a gpu.  does the error message below mean that the gpu ran out of room while it was trying to download the model from hugging face?  it doesn't seem to have filled up the drive the notebook is running on, so it seems like there's plenty of room on my server.  I'm just not sure if it tries to hold all the model weights in memory on the gpu instance.  can anyone suggest how to resolve this error so that I can try to work with llama2 model on my server, with my gpu?</p>
<p>code:</p>
<pre><code>from torch import cuda, bfloat16
import transformers

model_id = 'meta-llama/Llama-2-70b-chat-hf'

device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'

# set quantization configuration to load large model with less GPU memory
# this requires the `bitsandbytes` library
bnb_config = transformers.BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type='nf4',
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=bfloat16
)

# begin initializing HF items, need auth token for these
# hf_auth = '&lt;YOUR_API_KEY&gt;'

hf_auth = apikey
model_config = transformers.AutoConfig.from_pretrained(
    model_id,
    use_auth_token=hf_auth
)

model = transformers.AutoModelForCausalLM.from_pretrained(
    model_id,
    trust_remote_code=True,
    config=model_config,
    quantization_config=bnb_config,
    device_map='auto',
    use_auth_token=hf_auth
)
model.save_model('/save_path/')

model.eval()
print(f&quot;Model loaded on {device}&quot;)
</code></pre>
<p>error:</p>
<pre><code>File ~/anaconda3/envs/LLMenv/lib/python3.10/site-packages/huggingface_hub/file_download.py:544, in http_get(url, temp_file, proxies, resume_size, headers, timeout, max_retries, expected_size)
    542     if chunk:  # filter out keep-alive new chunks
    543         progress.update(len(chunk))
--&gt; 544         temp_file.write(chunk)
    546 if expected_size is not None and expected_size != temp_file.tell():
    547     raise EnvironmentError(
    548         f&quot;Consistency check failed: file should be of size {expected_size} but has size&quot;
    549         f&quot; {temp_file.tell()} ({displayed_name}).\nWe are sorry for the inconvenience. Please retry download and&quot;
    550         &quot; pass `force_download=True, resume_download=False` as argument.\nIf the issue persists, please let us&quot;
    551         &quot; know by opening an issue on https://github.com/huggingface/huggingface_hub.&quot;
    552     )

File ~/anaconda3/envs/LLMenv/lib/python3.10/tempfile.py:483, in _TemporaryFileWrapper.__getattr__.&lt;locals&gt;.func_wrapper(*args, **kwargs)
    481 @_functools.wraps(func)
    482 def func_wrapper(*args, **kwargs):
--&gt; 483     return func(*args, **kwargs)

OSError: [Errno 28] No space left on device
</code></pre>
","huggingface"
"76796590","Deepspeed tensor parallel gets problem in tensor alignment when using tokenizer","2023-07-30 06:32:11","","1","316","<python><pytorch><transformer-model><huggingface><deepspeed>","<p>I tried to use deepspeed to conduct tensor parallel on starcoder as I had multiple small GPUs and each of which cannot singly hold the whole model.</p>
<pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer
import os
import torch
import deepspeed

local_rank = int(os.getenv('LOCAL_RANK', '0'))
world_size = int(os.getenv('WORLD_SIZE', '1'))

cache_dir = '/llm-benchmark/starcoder-cache'

os.environ['TRANSFORMERS_CACHE'] = cache_dir

checkpoint = &quot;bigcode/starcoder&quot;
device = &quot;cuda&quot; # for GPU usage or &quot;cpu&quot; for CPU usage

tokenizer = AutoTokenizer.from_pretrained(checkpoint, cache_dir=cache_dir)

# Load model without moving it to device
model = AutoModelForCausalLM.from_pretrained(checkpoint, cache_dir=cache_dir)

ds_engine = deepspeed.init_inference(model, tensor_parallel={'enabled': True, 'tp_size': world_size})
model = ds_engine.module

print('before tokenizing')
inputs = tokenizer.encode(&quot;def print_hello_world():&quot;, return_tensors=&quot;pt&quot;).to(f&quot;{device}&quot;)
print('before generation')
outputs = model.generate(inputs)
print('after generation')
print(tokenizer.decode(outputs[0]))
print('full result')

</code></pre>
<p>When I ran the above code, it seemed that the model had been splitter successfully. However, I got the following error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/root/code/starcoder/generate.py&quot;, line 29, in &lt;module&gt;
    outputs = model.generate(inputs)
  File &quot;/root/code/starcoder/lib/python3.10/site-packages/torch/utils/_contextlib.py&quot;, line 115, in decorate_context
    return func(*args, **kwargs)
  File &quot;/root/code/starcoder/lib/python3.10/site-packages/transformers/generation/utils.py&quot;, line 1437, in generate
    return self.greedy_search(
  File &quot;/root/code/starcoder/lib/python3.10/site-packages/transformers/generation/utils.py&quot;, line 2248, in greedy_search
    outputs = self(
  File &quot;/root/code/starcoder/lib/python3.10/site-packages/torch/nn/modules/module.py&quot;, line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File &quot;/root/code/starcoder/lib/python3.10/site-packages/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py&quot;, line 808, in forward
    transformer_outputs = self.transformer(
  File &quot;/root/code/starcoder/lib/python3.10/site-packages/torch/nn/modules/module.py&quot;, line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File &quot;/root/code/starcoder/lib/python3.10/site-packages/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py&quot;, line 673, in forward
    outputs = block(
  File &quot;/root/code/starcoder/lib/python3.10/site-packages/torch/nn/modules/module.py&quot;, line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File &quot;/root/code/starcoder/lib/python3.10/site-packages/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py&quot;, line 316, in forward
    attn_outputs = self.attn(
  File &quot;/root/code/starcoder/lib/python3.10/site-packages/torch/nn/modules/module.py&quot;, line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File &quot;/root/code/starcoder/lib/python3.10/site-packages/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py&quot;, line 230, in forward
    query, key_value = self.c_attn(hidden_states).split((self.embed_dim, 2 * self.kv_dim), dim=2)
  File &quot;/root/code/starcoder/lib/python3.10/site-packages/torch/_tensor.py&quot;, line 803, in split
    return torch._VF.split_with_sizes(self, split_size, dim)
RuntimeError: split_with_sizes expects split_sizes to sum exactly to 1600 (input tensor's size at dimension 2), but got split_sizes=[1536, 256]
Traceback (most recent call last):
  File &quot;/root/code/starcoder/generate.py&quot;, line 29, in &lt;module&gt;
    outputs = model.generate(inputs)
  File &quot;/root/code/starcoder/lib/python3.10/site-packages/torch/utils/_contextlib.py&quot;, line 115, in decorate_context
    return func(*args, **kwargs)
  File &quot;/root/code/starcoder/lib/python3.10/site-packages/transformers/generation/utils.py&quot;, line 1437, in generate
    return self.greedy_search(
  File &quot;/root/code/starcoder/lib/python3.10/site-packages/transformers/generation/utils.py&quot;, line 2248, in greedy_search
    outputs = self(
  File &quot;/root/code/starcoder/lib/python3.10/site-packages/torch/nn/modules/module.py&quot;, line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File &quot;/root/code/starcoder/lib/python3.10/site-packages/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py&quot;, line 808, in forward
    transformer_outputs = self.transformer(
  File &quot;/root/code/starcoder/lib/python3.10/site-packages/torch/nn/modules/module.py&quot;, line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File &quot;/root/code/starcoder/lib/python3.10/site-packages/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py&quot;, line 673, in forward
    outputs = block(
  File &quot;/root/code/starcoder/lib/python3.10/site-packages/torch/nn/modules/module.py&quot;, line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File &quot;/root/code/starcoder/lib/python3.10/site-packages/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py&quot;, line 316, in forward
    attn_outputs = self.attn(
  File &quot;/root/code/starcoder/lib/python3.10/site-packages/torch/nn/modules/module.py&quot;, line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File &quot;/root/code/starcoder/lib/python3.10/site-packages/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py&quot;, line 230, in forward
    query, key_value = self.c_attn(hidden_states).split((self.embed_dim, 2 * self.kv_dim), dim=2)
  File &quot;/root/code/starcoder/lib/python3.10/site-packages/torch/_tensor.py&quot;, line 803, in split
    return torch._VF.split_with_sizes(self, split_size, dim)
RuntimeError: split_with_sizes expects split_sizes to sum exactly to 1600 (input tensor's size at dimension 2), but got split_sizes=[1536, 256]
</code></pre>
<p>It seems that the tokenizer is not aligned with the model. Why is this happening?</p>
","huggingface"
"76796304","Invalid key: 409862 is out of bounds for size 0","2023-07-30 04:06:39","","1","1301","<python><huggingface-transformers><huggingface><gpt-2><huggingface-trainer>","<h1>How I can fix this:</h1>
<p>I writed code for training GPT-2 on dataset by Hugging Face, but I have an error and don't know why I got this error:</p>
<pre><code>---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
&lt;ipython-input-7-b3178137a672&gt; in &lt;cell line: 17&gt;()
     15 )
     16 
---&gt; 17 trainer.train()
     18 model.save_pretrained('/content/drive/MyDrive/MyGPT')

11 frames
/usr/local/lib/python3.10/dist-packages/datasets/formatting/formatting.py in _check_valid_index_key(key, size)
    524     if isinstance(key, int):
    525         if (key &lt; 0 and key + size &lt; 0) or (key &gt;= size):
--&gt; 526             raise IndexError(f&quot;Invalid key: {key} is out of bounds for size {size}&quot;)
    527         return
    528     elif isinstance(key, slice):

IndexError: Invalid key: 409862 is out of bounds for size 0
</code></pre>
<p>in <strong>Hugging Face Transformers</strong>
<strong>Google Colab code</strong> here:</p>
<pre><code>!pip install 'transformers[torch]'

!pip install datasets
from datasets import load_dataset

dataset = load_dataset(&quot;Nan-Do/instructional_code-search-net-python&quot;)

import torch
from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments
tokenizer = GPT2Tokenizer.from_pretrained('sberbank-ai/rugpt3large_based_on_gpt2')
model = GPT2LMHeadModel.from_pretrained('sberbank-ai/rugpt3large_based_on_gpt2')

def prepare_data(data):
    input_ids = []
    attention_masks = []

    for text in data:
        encoded = tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=512,
            pad_to_max_length=True,
            return_attention_mask=True,
            return_tensors='pt'
        )

        input_ids.append(encoded['input_ids'])
        attention_masks.append(encoded['attention_mask'])

    return {
        'input_ids': torch.cat(input_ids, dim=0),
        'attention_mask': torch.cat(attention_masks, dim=0)
    }


training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=8000,
    per_device_train_batch_size=2,
    save_steps=2000,
    save_total_limit=2,
    prediction_loss_only=True,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset['train'],
    data_collator=prepare_data,
)

trainer.train()
model.save_pretrained('/content/drive/MyDrive/MyGPT')
</code></pre>
<p>I tried to add
<code>optimizer = TorchAdamW(model.parameters(), lr=1e-3) from torch.optim import AdamW as TorchAdamW</code>
By ChatGPT's advice, but this didn't help.
And I searched the internet but didn't find the answer to solve this error.</p>
","huggingface"
"76784527","How to use textual entailment model generated by fine-tuning STS models on HuggingFace","2023-07-28 02:37:50","","1","342","<python><huggingface>","<p>I am trying to fine-tune an STS model for Textual Entailment classification for &quot;entailment&quot;, &quot;neutral&quot;, and &quot;contradiction&quot;.</p>
<p>Here is the source code available on HuggingFace Sentence-BERT NLI: <a href=""https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/nli/training_nli_v2.py"" rel=""nofollow noreferrer"">https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/nli/training_nli_v2.py</a></p>
<p>Part of the NLI training data: <a href=""https://huggingface.co/datasets/snli"" rel=""nofollow noreferrer"">https://huggingface.co/datasets/snli</a></p>
<p>The script aims to create an NLI model based on the Semantic Textual Similarity benchmark with the code below:</p>
<pre><code># Save the path of the model
model_save_path = 'output/training_nli_v2_'+model_name.replace(&quot;/&quot;, &quot;-&quot;)+'-'+datetime.now().strftime(&quot;%Y-%m-%d_%H-%M-%S&quot;)
</code></pre>
<p>My question is how I can use the output model to predict 0(neutral), 1(entailment), or 2(contradiction).</p>
<p>Would it be like this? ref. <a href=""https://huggingface.co/facebook/bart-large-mnli"" rel=""nofollow noreferrer"">https://huggingface.co/facebook/bart-large-mnli</a></p>
<pre><code>from transformers import AutoModelForSequenceClassification, AutoTokenizer

nli_model = AutoModelForSequenceClassification.from_pretrained('training_nli_v2_{model_name}')
tokenizer = AutoTokenizer.from_pretrained('training_nli_v2_{model_name}')

premise = &quot;I am thirsty&quot;
hypothesis = &quot;I want water&quot;

x = tokenizer.encode(premise, hypothesis, return_tensors='pt',
                     truncation_strategy='only_first')

logits = nli_model(x.to(device))[0]

# We throw away &quot;neutral&quot; (dim 1) and take the probability of
# &quot;entailment&quot; (2) as the probability of the label being true 

entail_contradiction_logits = logits[:,[0,2]]
probs = entail_contradiction_logits.softmax(dim=1)
prob_label_is_true = probs[:,1]
</code></pre>
<p>Any help would be greatly appreciated.</p>
","huggingface"
"76784135","How to get embeddings from long texts without pooling?","2023-07-28 00:11:56","","0","698","<machine-learning><pytorch><nlp><huggingface-transformers><huggingface>","<p>I have a collection of relatively long texts, containing roughly 2k tokens each. I want to covert each into an embedding.</p>
<p>I found that <a href=""https://github.com/UKPLab/sentence-transformers"" rel=""nofollow noreferrer"">sentence-transformers</a> is quite popular, but it can only take short sequences into account. One approach is to create an embedding for each sentence and then average the results, but I don't want to do that. I'm interested in getting an embedding <strong>without any pooling operations</strong>. I also found <a href=""https://huggingface.co/tasks/feature-extraction"" rel=""nofollow noreferrer"">Huggingface's feature extraction pipeline</a>, but from <a href=""https://stackoverflow.com/questions/64685243/getting-sentence-embedding-from-huggingface-feature-extraction-pipeline"">here</a> I understand that it also contains some pooling operator over sequences (unless I'm mistaken).</p>
<p>For example, let's say I want to use a <a href=""https://huggingface.co/gpt2"" rel=""nofollow noreferrer"">GPT2 model</a> (or some other model that can take long sequences into account):</p>
<pre><code>from transformers import GPT2Tokenizer, GPT2Model
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2Model.from_pretrained('gpt2')
text = &quot;Some very very very long text&quot;
</code></pre>
","huggingface"
"76771761","Why does llama-index still require an OpenAI key when using Hugging Face local embedding model?","2023-07-26 13:19:46","76781752","19","17837","<python><huggingface-transformers><huggingface><large-language-model><llama-index>","<p>I am creating a very simple question and answer app based on documents using llama-index. Previously, I had it working with OpenAI. Now I want to try using no external APIs so I'm trying the Hugging Face example <a href=""https://gpt-index.readthedocs.io/en/latest/core_modules/model_modules/llms/usage_custom.html#example-using-a-huggingface-llm"" rel=""noreferrer"">in this link</a>.</p>
<p>It says in the example in the link: &quot;Note that for a completely private experience, also setup a local embedding model (example here).&quot; I'm assuming the example given below is the example being referred to. So, naturally, I'm trying to copy the example (<a href=""https://gpt-index.readthedocs.io/en/latest/examples/customization/llms/SimpleIndexDemo-Huggingface_stablelm.html"" rel=""noreferrer"">fuller example here</a>).</p>
<p>Here is my code:</p>
<pre class=""lang-py prettyprint-override""><code>from pathlib import Path
import gradio as gr
import sys
import logging
import os

from llama_index.llms import HuggingFaceLLM
from llama_index.prompts.prompts import SimpleInputPrompt

logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)
logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))


from llama_index import SimpleDirectoryReader, VectorStoreIndex, ServiceContext, load_index_from_storage, StorageContext

storage_path = &quot;storage/&quot;

docs_path=&quot;docs&quot;

def construct_index(directory_path):
    max_input_size = 4096
    num_outputs = 512
    #max_chunk_overlap = 20
    chunk_overlap_ratio = 0.1
    chunk_size_limit = 600

    #prompt_helper = PromptHelper(max_input_size, num_outputs, chunk_overlap_ratio, chunk_size_limit=chunk_size_limit)

    system_prompt = &quot;&quot;&quot;&lt;|SYSTEM|&gt;# StableLM Tuned (Alpha version)
    - StableLM is a helpful and harmless open-source AI language model developed by StabilityAI.
    - StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.
    - StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes.
    - StableLM will refuse to participate in anything that could harm a human.
    &quot;&quot;&quot;

    # This will wrap the default prompts that are internal to llama-index
    query_wrapper_prompt = SimpleInputPrompt(&quot;&lt;|USER|&gt;{query_str}&lt;|ASSISTANT|&gt;&quot;)


    llm = HuggingFaceLLM(
        context_window=4096,
        max_new_tokens=256,
        generate_kwargs={&quot;temperature&quot;: 0.7, &quot;do_sample&quot;: False},
        system_prompt=system_prompt,
        query_wrapper_prompt=query_wrapper_prompt,
        tokenizer_name=&quot;StabilityAI/stablelm-tuned-alpha-3b&quot;,
        model_name=&quot;StabilityAI/stablelm-tuned-alpha-3b&quot;,
        device_map=&quot;auto&quot;,
        stopping_ids=[50278, 50279, 50277, 1, 0],
        tokenizer_kwargs={&quot;max_length&quot;: 4096},
        # uncomment this if using CUDA to reduce memory usage
        # model_kwargs={&quot;torch_dtype&quot;: torch.float16}
    )
    #llm=ChatOpenAI(temperature=0.7, model_name=&quot;gpt-3.5-turbo&quot;, max_tokens=num_outputs)
    #llm_predictor = LLMPredictor(llm=llm)
    service_context = ServiceContext.from_defaults(chunk_size=1024, llm=llm)

    documents = SimpleDirectoryReader(directory_path).load_data()

    index = VectorStoreIndex.from_documents(documents, service_context=service_context)
    #index = VectorStoreIndex(documents, llm_predictor=llm_predictor, prompt_helper=prompt_helper)

    index.storage_context.persist(persist_dir=storage_path)

    return index

def chatbot(input_text):
    index = load_index_from_storage(StorageContext.from_defaults(persist_dir=storage_path))
    #index = GPTVectorStoreIndex.load_from_disk('index.json')
    #query_engine = index.as_query_engine(response_synthesizer=response_synthesizer);
    query_engine = index.as_query_engine(streaming=True)

    response = query_engine.query(input_text)

    print(response.source_nodes)

    relevant_files=[]

    for node_with_score in response.source_nodes:
        print(node_with_score)
        print(node_with_score.node)
        print(node_with_score.node.metadata)
        print(node_with_score.node.metadata['file_name'])

        file = node_with_score.node.metadata['file_name']
        print( file )

        # Resolve the full file path for the downloading
        full_file_path = Path( docs_path, file ).resolve()

        # See if it's already in the array
        if full_file_path not in relevant_files:
            relevant_files.append( full_file_path ) # Add it

    print( relevant_files )

    return response.get_response(), relevant_files

iface = gr.Interface(fn=chatbot,
                     inputs=gr.components.Textbox(lines=7, label=&quot;Enter your text&quot;),
                     outputs=[
                        gr.components.Textbox(label=&quot;Response&quot;), 
                        gr.components.File(label=&quot;Relevant Files&quot;)
                        ],
                     title=&quot;Custom-trained AI Chatbot&quot;,
                     allow_flagging=&quot;never&quot;)

index = construct_index(docs_path)
iface.launch(share=False)

</code></pre>
<p>Regardless, the code errors out saying:</p>
<pre><code>ValueError: No API key found for OpenAI.
Please set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.
API keys can be found or created at https://platform.openai.com/account/api-keys
</code></pre>
<p>Am I not understanding how to set up a local model?</p>
","huggingface"
"76769776","Way to Offline Speaker Diarization with Hugging Face","2023-07-26 09:28:46","","3","2603","<python><huggingface-transformers><huggingface><speaker-diarization>","<p>I am looking for Offline / locally saved model for speaker diarization with Hugging face without Authentication.<br />
I have gone through google and found no relevant links for the same.<br />
Is there any link/method to do the same?</p>
<p>Thanks in advance</p>
","huggingface"
"76768226","Target modules for applying PEFT / LoRA on different models","2023-07-26 05:23:18","76779946","25","25248","<nlp><huggingface-transformers><huggingface><fine-tuning><peft>","<p>I am looking at a few <a href=""https://colab.research.google.com/drive/1BiQiw31DT7-cDp1-0ySXvvhzqomTdI-o#scrollTo=NuAx3zBeUL1q"" rel=""noreferrer"">different</a> <a href=""https://www.philschmid.de/fine-tune-flan-t5-peft"" rel=""noreferrer"">examples</a> of using PEFT on different models. The <code>LoraConfig</code> object contains a <code>target_modules</code> array. In some examples, the target modules are <code>[&quot;query_key_value&quot;]</code>, sometimes it is <code>[&quot;q&quot;, &quot;v&quot;]</code>, sometimes something else.</p>
<p>I don't quite understand where the values of the target modules come from. Where in the model page should I look to know what the LoRA adaptable modules are?</p>
<p>One example (for the model Falcon 7B):</p>
<pre><code>peft_config = LoraConfig(
    lora_alpha=lora_alpha,
    lora_dropout=lora_dropout,
    r=lora_r,
    bias=&quot;none&quot;,
    task_type=&quot;CAUSAL_LM&quot;,
    target_modules=[
        &quot;query_key_value&quot;,
        &quot;dense&quot;,
        &quot;dense_h_to_4h&quot;,
        &quot;dense_4h_to_h&quot;,
    ]
</code></pre>
<p>Another example (for the model Opt-6.7B):</p>
<pre><code>config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=[&quot;q_proj&quot;, &quot;v_proj&quot;],
    lora_dropout=0.05,
    bias=&quot;none&quot;,
    task_type=&quot;CAUSAL_LM&quot;
)
</code></pre>
<p>Yet another (for the model Flan-T5-xxl):</p>
<pre><code>lora_config = LoraConfig(
 r=16,
 lora_alpha=32,
 target_modules=[&quot;q&quot;, &quot;v&quot;],
 lora_dropout=0.05,
 bias=&quot;none&quot;,
 task_type=TaskType.SEQ_2_SEQ_LM
)
</code></pre>
","huggingface"
"76761875","Why do I get an inconsistent memory error when loading Llama-2 from huggingface","2023-07-25 10:39:22","","3","1219","<memory><huggingface-transformers><huggingface><large-language-model><llama-index>","<p>I'm playing around with the new Llama-2 7B model, and running it on a 16GM RAM M1 pro Mac. If I load the model, Python crashes with a memory error - unless I load it via hf pipelines. I don't believe this to be a hf issue but rather something weird with my machine? Not sure what I'm doing wrong. I have also tried downloading the weights and running it locally - same error.</p>
<p>If I load the model via hf pipelines, such as:</p>
<pre><code>from transformers import AutoTokenizer
import transformers
import torch

model = &quot;meta-llama/Llama-2-7b-chat-hf&quot;
tokenizer = AutoTokenizer.from_pretrained(model)

pipeline = transformers.pipeline(
    &quot;text-generation&quot;,
    model=model,
)

sequences = pipeline(
    'What's 1+1?',
    do_sample=False,
    top_k=10,
    num_return_sequences=1,
    eos_token_id=tokenizer.eos_token_id,
    max_length=2000,
)

for seq in sequences:
    print(f&quot;Result: {seq['generated_text']}&quot;)
</code></pre>
<p>that works fine - and although it's quite slow, I can run it.</p>
<p>But, if I try to load the model in any other way, such as:</p>
<pre><code>from ctransformers import AutoModelForCausalLM

llm = AutoModelForCausalLM.from_pretrained(&quot;meta-llama/Llama-2-7b-chat-hf&quot;, model_type='llama')
</code></pre>
<p>or</p>
<pre><code>from langchain.llms import CTransformers
llm = CTransformers(
    model='meta-llama/Llama-2-7b-chat-hf',
    model_type='llama',
    config={'max_new_tokens': 256,
            'temperature': 0.01})
</code></pre>
<p>Python crashes and I get a warning like <code>UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown</code> which apparently means that I'm out of memory.</p>
<p>Fine - but a) I'm shutting down everything else, I should have enough RAM on my machine to run the model locally on CPU and b) why can I load the model via hf pipelines?? Any pointers appreciated.</p>
","huggingface"
"76758299","What is the difference between HuggingFace's TextGeneration and Text2TextGeneration pipelines","2023-07-24 22:07:32","","8","2968","<nlp><huggingface-transformers><huggingface>","<p>I'm confused about the technical difference between the two huggingface pipelines <a href=""https://huggingface.co/docs/transformers/main/main_classes/pipelines#transformers.TextGenerationPipeline"" rel=""noreferrer"">TextGeneration</a> and <a href=""https://huggingface.co/docs/transformers/main/main_classes/pipelines#transformers.Text2TextGenerationPipeline"" rel=""noreferrer"">Text2TextGeneration</a>.</p>
<p>In the TextGeneration it is stated that:</p>
<blockquote>
<p>Language generation pipeline using any ModelWithLMHead. This pipeline
predicts the words that will follow a specified text prompt.</p>
</blockquote>
<p>But isn't any language model doing that? &quot;predicting the next words&quot;? So how is this pipeline different than Text2TextGeneration? Isn't Text2TextGeneration going to predict the next probable words?</p>
<p>I also tried some models using &quot;Text2TextGeneration&quot; pipeline, and despite of HuggingFace's warning &quot;The model is not supported for text2text-generation&quot; it actually worked and generated some outputs.</p>
<p>If someone can explain the technical difference it will be appreciated.</p>
","huggingface"
"76755074","Discrepancies: Sagemaker vs. Local Hugging Face Model inference Results","2023-07-24 13:38:25","","2","299","<amazon-web-services><amazon-sagemaker><huggingface-transformers><huggingface><sentence-transformers>","<p>I am using a Hugging Face model (sentence-transformers/distiluse-base-multilingual-cased-v2) on my local system to convert text into vectors.</p>
<pre><code>class SequenceEncoder(object):
    def __init__(self, device=None):
        self.device = device
        self.multi_model = SentenceTransformer('distiluse-base-multilingual-cased-v2',device=device)   # 512 dimensional dense vector

    @torch.no_grad()
    def __call__(self, col, dialect_list=None):
        if isinstance(col, str):
            vals = [col]
        else:
            vals = col.replace([np.nan, 0], '').values.tolist()
        x = self.multi_model.encode(vals, show_progress_bar=True, convert_to_tensor=True, device=self.device)
        return x.cpu()
</code></pre>
<p>The local model output for a given text looks as follows:
[-0.00285156 -0.04651115 -0.00723144 -0.04229123 -0.02418377,0.00646215, ...]</p>
<p>Recently, I decided to deploy the same model on Amazon Sagemaker using the following configuration:</p>
<p>HF_MODEL_ID:    sentence-transformers/distiluse-base-multilingual-cased-v2</p>
<p>HF_TASK:    feature-extraction</p>
<p>After successfully deploying the model and creating an endpoint on Sagemaker, I tested it with the same input text. However, the output from Sagemaker differs from the local model output:</p>
<p>Sagemaker output: [-0.035367466509342194, 0.011641714721918106, -0.04396483674645424, 0.03655952587723732, ...]</p>
<p>I noticed two main discrepancies in the results:</p>
<ol>
<li><p>Differences in values: The numerical values in the vectors from Sagemaker differ from the local model's output.</p>
</li>
<li><p>Differences in floating-point length: The vectors from Sagemaker seem to have a different floating-point length compared to the local model's vectors.</p>
</li>
</ol>
<p>Also, I tried removing the &quot;convert_to_tensor=True&quot; parameter when using the local model, but it didn't resolve the discrepancies.</p>
<p>I would appreciate any insights into why these differences are occurring between the local model and the model deployed on Sagemaker. Additionally, I would like to know if there is a way to make Sagemaker return tensors as well.</p>
<p>Thank you in advance for your assistance!</p>
","huggingface"
"76748279","Changing the Default Cache Path for All HuggingFace Data","2023-07-23 12:38:55","76748390","2","6251","<python><nlp><huggingface>","<p>The default cache path of huggingface is in <code> ~/.cache/huggingface</code>, and in that folder, there are multiple cache files like <code>models</code>, and <code>hub</code>.</p>
<p>The <a href=""https://huggingface.co/docs/datasets/cache"" rel=""nofollow noreferrer"">huggingface documents</a> indicates that the default <code>dataset</code> cache location can be modified by setting the shell environment variable, <code>HF_DATASETS_CACHE</code> to a different directory as shown below:</p>
<pre><code>$ export HF_DATASETS_CACHE=&quot;/path/to/another/directory&quot;
</code></pre>
<p>However, my objective is to alter the default cache directory for all HuggingFace data and not solely the <code>dataset</code>. I am facing difficulties in finding the respective shell environment variable in the HuggingFace documentation to accomplish this. Any help would be appreciated.</p>
","huggingface"
"76743561","does hugging face model.generate for flan-T5 default is summarization?","2023-07-22 11:26:59","76752292","1","847","<python><python-3.x><huggingface-transformers><huggingface>","<p>Given the following code. why does the function:
model.generate()
returns a summary, where does it order to do summary and not some other task? where can I see the documentation for that as well.</p>
<pre><code>model_name = ‘google/flan-t5-base’
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
dataset_name = “knkarthick/dialogsum”
dataset = load_dataset(dataset_name)

for i in example_indices:
  dialog = dataset[‘test’][i][‘dialogue’]
  input = tokenizer(dialog,sentence,return_tensors=‘pt’)

  ground_truth = dataset[‘test’][i][‘summary’]

  model_summary = model.generate(input[‘input_ids’],max_new_tokens=50)
  summary = tokenizer.decode(model_summary[0],skip_special_tokens=True)
  print(summary)
</code></pre>
","huggingface"
"76740917","HuggingFace load_dataset error (.incomplete/parquet-validation-00000-00000-of-NNNNN.arrow')","2023-07-21 20:01:42","76925971","0","211","<filenotfoundexception><huggingface><huggingface-datasets>","<p>I'm following a tutorial to fine-tune a model, but have been stuck in a load_dataset error I can't solve. For context, the tutorial first uploaded <a href=""https://huggingface.co/datasets/katanaml-org/invoices-donut-data-v1/tree/main"" rel=""nofollow noreferrer"">this</a> dataset to HF, and I managed to upload an <a href=""https://huggingface.co/datasets/FelipeBandeiraPoatek/invoices-donut-data-v1/blob/main/README.md"" rel=""nofollow noreferrer"">identical one</a>.</p>
<p>When I run a script to download the dataset, however, the problem appears. If I'm downloading the original dataset, the process goes well and all files are fetched correctly. But when I try downloading mine, it seems like I get to download part of the files (until the 0.0.0 folder you'll see in the error message, but nothing after that).</p>
<p>The command I'm running is <code>dataset = load_dataset(&quot;FelipeBandeiraPoatek/invoices-donut-data-v2&quot;, split=&quot;train&quot;)</code>, and the error log I'm getting is the following:</p>
<pre><code>Downloading data files: 100%|████████████████████████████████████████████████| 3/3 [00:00&lt;?, ?it/s]
Extracting data files: 100%|████████████████████████████████████████| 3/3 [00:00&lt;00:00, 198.67it/s] 
Traceback (most recent call last):
  File &quot;C:\Users\Felipe Bandeira\Desktop\sparrow\venv\lib\site-packages\datasets\builder.py&quot;, line 1852, in _prepare_split_single
    writer = writer_class(
  File &quot;C:\Users\Felipe Bandeira\Desktop\sparrow\venv\lib\site-packages\datasets\arrow_writer.py&quot;, line 334, in __init__
    self.stream = self._fs.open(fs_token_paths[2][0], &quot;wb&quot;)
  File &quot;C:\Users\Felipe Bandeira\Desktop\sparrow\venv\lib\site-packages\fsspec\spec.py&quot;, line 1241, in open
    f = self._open(
  File &quot;C:\Users\Felipe Bandeira\Desktop\sparrow\venv\lib\site-packages\fsspec\implementations\local.py&quot;, line 184, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File &quot;C:\Users\Felipe Bandeira\Desktop\sparrow\venv\lib\site-packages\fsspec\implementations\local.py&quot;, line 315, in __init__
    self._open()
  File &quot;C:\Users\Felipe Bandeira\Desktop\sparrow\venv\lib\site-packages\fsspec\implementations\local.py&quot;, line 320, in _open
    self.f = open(self.path, mode=self.mode)
FileNotFoundError: [Errno 2] No such file or directory: 'C:/Users/Felipe Bandeira/.cache/huggingface/datasets/FelipeBandeiraPoatek___parquet/FelipeBandeiraPoatek--invoices-donut-data-v2-ca49e83826870faf/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec.incomplete/parquet-validation-00000-00000-of-NNNNN.arrow'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File &quot;c:\Users\Felipe Bandeira\Desktop\sparrow\sparrow-data\run_donut_test.py&quot;, line 11, in &lt;module&gt;
    main()
  File &quot;c:\Users\Felipe Bandeira\Desktop\sparrow\sparrow-data\run_donut_test.py&quot;, line 7, in main   
    dataset_tester.test(&quot;FelipeBandeiraPoatek/invoices-donut-data-v2&quot;)
  File &quot;c:\Users\Felipe Bandeira\Desktop\sparrow\sparrow-data\tools\donut\dataset_tester.py&quot;, line 10, in test
    dataset = load_dataset(dataset_name, split=&quot;train&quot;)
  File &quot;C:\Users\Felipe Bandeira\Desktop\sparrow\venv\lib\site-packages\datasets\load.py&quot;, line 1782, in load_dataset
    builder_instance.download_and_prepare(
  File &quot;C:\Users\Felipe Bandeira\Desktop\sparrow\venv\lib\site-packages\datasets\builder.py&quot;, line 872, in download_and_prepare
    self._download_and_prepare(
  File &quot;C:\Users\Felipe Bandeira\Desktop\sparrow\venv\lib\site-packages\datasets\builder.py&quot;, line 967, in _download_and_prepare
    self._prepare_split(split_generator, **prepare_split_kwargs)
  File &quot;C:\Users\Felipe Bandeira\Desktop\sparrow\venv\lib\site-packages\datasets\builder.py&quot;, line 1749, in _prepare_split
    for job_id, done, content in self._prepare_split_single(
  File &quot;C:\Users\Felipe Bandeira\Desktop\sparrow\venv\lib\site-packages\datasets\builder.py&quot;, line 1892, in _prepare_split_single
    raise DatasetGenerationError(&quot;An error occurred while generating the dataset&quot;) from e
datasets.builder.DatasetGenerationError: An error occurred while generating the dataset
</code></pre>
<p>I haven't found any solutions for this and cannot figure out why the original dataset is downloaded well, but mine (which is identical) does not. Any clues?</p>
<p>(I have tried:</p>
<ol>
<li>inspecting the functions that download dataset</li>
<li>inspecting error logs</li>
<li>deleting folders that store the downloads in my pc and repeating the process</li>
<li>cloning the files from the original repo on a repo of my own</li>
</ol>
<p>In all of the cases, I can download the dataset correctly from the original repo, but not from my own. The same error keeps happening)</p>
","huggingface"
"76728496","ModuleNotFoundError: No module named 'transformers.models.mmbt' - How to fix it?","2023-07-20 09:29:56","","1","8512","<python><google-colaboratory><huggingface-transformers><text-classification><huggingface>","<p>I have used the code below (reference here: <a href=""https://huggingface.co/classla/xlm-roberta-base-multilingual-text-genre-classifier"" rel=""nofollow noreferrer"">https://huggingface.co/classla/xlm-roberta-base-multilingual-text-genre-classifier</a>) several times but actually I'am not able to execute it since the error 'ModuleNotFoundError: No module named 'transformers.models.mmbt' occurs without any apparent reason.</p>
<p>I run the code on google colab.</p>
<p><strong>Pip</strong></p>
<pre><code>!pip install transformers
!pip install simpletransformers
</code></pre>
<p>trasformers: version 4.30.2</p>
<p>simpletransformers: version 0.63.11</p>
<p>python: version 3</p>
<p><strong>Code</strong></p>
<pre><code>from simpletransformers.classification import ClassificationModel
model_args= {
            &quot;num_train_epochs&quot;: 15,
            &quot;learning_rate&quot;: 1e-5,
            &quot;max_seq_length&quot;: 512,
            &quot;silent&quot;: True
            }
model = ClassificationModel(
    &quot;xlmroberta&quot;, &quot;classla/xlm-roberta-base-multilingual-text-genre-classifier&quot;, use_cuda=True,
    args=model_args
    
)
predictions, logit_output = model.predict([&quot;How to create a good text classification model? First step is to prepare good data. Make sure not to skip the exploratory data analysis. Pre-process the text if necessary for the task. The next step is to perform hyperparameter search to find the optimum hyperparameters. After fine-tuning the model, you should look into the predictions and analyze the model's performance. You might want to perform the post-processing of data as well and keep only reliable predictions.&quot;, 
                                        &quot;On our site, you can find a great genre identification model which you can use for thousands of different tasks. With our model, you can fastly and reliably obtain high-quality genre predictions and explore which genres exist in your corpora. Available for free!&quot;]
                                        )
predictions
# Output: array([3, 8])

[model.config.id2label[i] for i in predictions]
# Output: ['Instruction', 'Promotion']
</code></pre>
<p><strong>Error</strong></p>
<pre><code>ModuleNotFoundError                       Traceback (most recent call last)
&lt;ipython-input-5-e269f817f0ec&gt; in &lt;cell line: 1&gt;()
----&gt; 1 from simpletransformers.classification import ClassificationModel

1 frames
/usr/local/lib/python3.10/dist-packages/simpletransformers/classification/multi_modal_classification_model.py in &lt;module&gt;
     45     BertTokenizer,
     46 )
---&gt; 47 from transformers.models.mmbt.configuration_mmbt import MMBTConfig
     48 
     49 from simpletransformers.classification.classification_utils import (

ModuleNotFoundError: No module named 'transformers.models.mmbt'
</code></pre>
","huggingface"
"76722173","Big difference in the sizes of Llama 2 model files on huggingface hub depending on the format","2023-07-19 14:04:10","","6","8726","<huggingface>","<p>The Llama2 7B model on huggingface (meta-llama/Llama-2-7b) has a pytorch .pth file consolidated.00.pth that is ~13.5GB in size. The hugging face transformers compatible model meta-llama/Llama-2-7b-hf has three pytorch model files that are together ~27GB in size and two safetensors file that are together around 13.5Gb.</p>
<p>Could someone please explain the reason for the big difference in file sizes?</p>
<p>I could not find an explanation in the huggingface model cards or in their blog <a href=""https://stackoverflow.com"">Llama 2 is here - get it on Hugging Face</a>.</p>
<p>Update: When the models are downloaded to huggingface cache, I noticed that only the safetensors are downloaded and not the Pytorch binary model files. This avoids downloading both the safetensors and pytorch model files.</p>
","huggingface"
"76720386","is there a way to search a huggingface Repository for a specific filename?","2023-07-19 10:36:25","76720444","1","734","<python><search><huggingface><huggingface-hub>","<p>I'd like to search a huggingface repository for a specific filename, without having to clone it first as it is a rather large repo with thousands of files.</p>
<p>I couldn't find a way to do it with the web interface, I installed the <em>python</em> package <code>huggingface_hub</code> and looked into <code>huggingface_hub.Repository</code> and <code>huggingface_hub.HfFileSystem</code> without success.</p>
<p>If somehow a search query isn't possible, may be retrieving the list of files?</p>
","huggingface"
"76710256","HuggingFace - Load/ save PeftConfig as json","2023-07-18 07:06:28","","0","869","<pytorch><huggingface><peft>","<p>I am training fine-tuning a HuggingFace model by adding my own data and using LORA. However, I do not want to upload the file to HuggingFace, but store it on my local computer. This works for the tokenizer and the model, however the LoraConfig object cannot be stored. What do I make wrong?</p>
<p>Here is some of my code:</p>
<pre><code>import transformers

trainer = transformers.Trainer(
    model=model, 
    train_dataset=data['train'],
    args=transformers.TrainingArguments(
        per_device_train_batch_size=4, 
        gradient_accumulation_steps=4,
        warmup_steps=100, 
        max_steps=2, 
        learning_rate=2e-4, 
        fp16=False,
        logging_steps=1, 
        output_dir='outputs'
    ),
    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)
)

model.config.use_cache = False  # silence the warnings. Please re-enable for inference!
with torch.autocast(&quot;cuda&quot;): 
    trainer.train()

tokenizer.save_pretrained(&quot;./&quot;)
model.save_pretrained(&quot;./&quot;)

config

&quot;&quot;&quot;
LoraConfig(peft_type=&lt;PeftType.LORA: 'LORA'&gt;, auto_mapping=None, base_model_name_or_path='xx', revision=None, task_type='CAUSAL_LM', inference_mode=False, r=16, target_modules=['q_proj', 'v_proj'], lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None)
&quot;&quot;&quot;

out_file = open(&quot;config.json&quot;, &quot;w&quot;)  
json.dump(config, out_file, indent = 6)

TypeError: Object of type LoraConfig is not JSON serializable

</code></pre>
<p>The further usage would be:</p>
<pre><code>import torch
from peft import PeftModel, PeftConfig
from transformers import AutoModelForCausalLM, AutoTokenizer

#peft_model_id = &quot;ybelkada/opt-6.7b-lora&quot;
config = PeftConfig.from_dict(&quot;./&quot;)
model = AutoModelForCausalLM.from_pretrained(&quot;./&quot;, return_dict=True, load_in_8bit=True, device_map='auto')
tokenizer = AutoTokenizer.from_pretrained(&quot;./&quot;)

# Load the Lora model
model = PeftModel.from_pretrained(model, config)

</code></pre>
<p>What is the main error there? Am I saving the correct config file?</p>
","huggingface"
"76704291","Hugging Face Inference API returning short generated text with GPT-2 model","2023-07-17 12:07:14","","0","1125","<javascript><artificial-intelligence><huggingface><gpt-2>","<p>I'm using the Hugging Face API with a GPT-2 model to generate text based on a prompt. However, I'm encountering an issue where the generated text is consistently too short, even though I'm specifying a maximum number of new tokens and using other parameters to try to generate longer text.</p>
<pre><code>function GPT2()
{
const prompt = &quot;Please give me a 100 words tell me about yourself&quot;;  

const API_TOKEN = &quot;xxxxxxxxxxxx&quot;;
const MODEL_NAME = &quot;gpt2&quot;;

const url = `https://api-inference.huggingface.co/models/${MODEL_NAME}`;
  const options = 
  {
    method: &quot;post&quot;,
    headers: 
    {
      Authorization: `Bearer ${API_TOKEN}`,
      &quot;Content-Type&quot;: &quot;application/json&quot;,
    },
    payload: JSON.stringify
    ({
      inputs: prompt,
      options: 
      {
        contentType: &quot;application/json&quot;,
        max_tokens: 200,
        max_length: 100, 
        num_return_sequences: 3,
        use_cache: false,
        return_full_text: true
      }
    })
  };
  const response = UrlFetchApp.fetch(url, options);
  const res = JSON.parse(response.getContentText());
  Logger.log(response);

  return res[0].generated_text.trim();
}
</code></pre>
<p>For example, when I input the prompt &quot;Please give me a 100 words tell me about yourself.&quot;, the generated text returned is only a few words long, such as &quot;I'm a writer, and I&quot;. I was expecting the generated text to be much longer.</p>
<p>Can anyone offer any suggestions for how I can generate longer text using the Hugging Face API with a GPT-2 model? Is there a problem with my code or input parameters that is causing the generated text to be too short?</p>
<p>I have also checked the API response to ensure that it is valid and contains the expected input and output values.</p>
","huggingface"
"76691595","How to save blob with huggingface.js","2023-07-14 23:24:14","","0","231","<node.js><huggingface>","<p>I'm trying to run the following example generating image from text with the inference API from:</p>
<p><a href=""https://huggingface.co/docs/huggingface.js/index"" rel=""nofollow noreferrer"">https://huggingface.co/docs/huggingface.js/index</a></p>
<p>The API succeeds, but I' having trouble saving the image to the local filesystem.</p>
<pre><code> const blob = await inference.textToImage({
        model: 'stabilityai/stable-diffusion-2',
        inputs: 'award winning high resolution photo of a giant tortoise/((ladybird)) hybrid, [trending on artstation]',
        parameters: {
            negative_prompt: 'blurry',
        }
    })

    console.log('Received:', blob)
    const fileName = 'generated_image.jpg'
    blob.stream().pipe(fs.createWriteStream(fileName))
     .on('finish', () =&gt; console.log('finished'))
     .on('error', e =&gt; console.log('Error:', e))
</code></pre>
<p>Output:</p>
<pre><code>Received: Blob { size: 130079, type: 'image/jpeg' }

Blob.stream(...).pipe is not a function
</code></pre>
","huggingface"
"76688630","AttributeError: module 'torch.nn.init' has no attribute 'trunc_normal_' While saving trasnformer google/vit-base-patch16-224-in21k model to local","2023-07-14 14:25:32","","1","278","<pytorch><amazon-sagemaker><huggingface-transformers><transformer-model><huggingface>","<p>I am trying to save <code>google/vit-base-patch16-224-in21k</code> locally and then upload it to s3 for hosting this model in the SageMaker environment.</p>
<pre><code>from transformers import AutoImageProcessor, ViTModel
# tokenizer = BertTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
# model = BertModel.from_pretrained(&quot;bert-base-uncased&quot;)

# processor = AutoImageProcessor.from_pretrained(&quot;google/vit-base-patch16-224-in21k&quot;)
model = ViTModel.from_pretrained(&quot;google/vit-base-patch16-224-in21k&quot;)

model_path = &quot;model/&quot;
code_path = &quot;code/&quot;

if not os.path.exists(model_path):
    os.mkdir(model_path)
    
model.save_pretrained(save_directory=model_path)
# processor.save_pretrained(save_directory=model_path)
</code></pre>
<p>Getting below error while saving</p>
<pre><code>AttributeError: module 'torch.nn.init' has no attribute 'trunc_normal_'
</code></pre>
<p>I tried modifying the transformer library version with no luck. Any help is highly appreciated. Thanks in advance.</p>
<p>Note: I am using <code>conda_amazonei_pytorch_latest_p37</code> kernel in sagemaker.</p>
<p>Reference: <a href=""https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-script-mode/pytorch_bert/deploy_bert_outputs.html"" rel=""nofollow noreferrer"">https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-script-mode/pytorch_bert/deploy_bert_outputs.html</a></p>
","huggingface"
"76681050","How can I make the huggingface cache variable HF_HOME work?","2023-07-13 15:24:44","","0","767","<environment-variables><cache-control><huggingface>","<p>I was trying to change the huggingface default cache folder, and I found a article said that set the HF_HOME varibale can deal with it.
Then I added the variable in Windows Enviroment Variable, and reboot my computer. After that, I download a model from huggface, but the cache didn't go to the place that I set, it still in the default address.
I was trying to do this on win11.</p>
<p>I try to reboot again and change the HF_HOME, but it didn't work.</p>
","huggingface"
"76675018","How does one use accelerate with the hugging face (HF) trainer?","2023-07-12 23:25:47","","4","8794","<pytorch><nlp><huggingface-transformers><huggingface><accelerate>","<p>What are the code changes one has to do to run accelerate with a trianer?
I keep seeing:</p>
<pre><code>from accelerate import Accelerator

accelerator = Accelerator()

model, optimizer, training_dataloader, scheduler = accelerator.prepare(
    model, optimizer, training_dataloader, scheduler
)

for batch in training_dataloader:
    optimizer.zero_grad()
    inputs, targets = batch
    outputs = model(inputs)
    loss = loss_function(outputs, targets)
    accelerator.backward(loss)
    optimizer.step()
    scheduler.step()
</code></pre>
<p>but when I tried the analogous thing it didn't work:</p>
<pre><code>!pip
install
accelerate
!pip
install
datasets
!pip
install
transformers

# %%
from accelerate import Accelerator
from datasets import load_dataset
from transformers import GPT2LMHeadModel, GPT2TokenizerFast, TrainingArguments, Trainer

# Initialize accelerator
accelerator = Accelerator()

# Specify dataset
dataset = load_dataset('imdb')

# Specify tokenizer and model
tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')
model = GPT2LMHeadModel.from_pretrained('gpt2')
model.to(accelerator.device)


# Tokenize and format dataset
def tokenize_function(examples):
    return tokenizer(examples[&quot;text&quot;], truncation=True, max_length=512)


tokenized_datasets = dataset.map(
    tokenize_function,
    batched=True,
    num_proc=accelerator.num_processes,
    remove_columns=[&quot;text&quot;]
)

# Training configuration
training_args = TrainingArguments(
    output_dir=&quot;output&quot;,
    overwrite_output_dir=True,
    # num_train_epochs=3,
    max_steps=10,
    per_device_train_batch_size=1,
    per_device_eval_batch_size=2,
    save_steps=10_000,
    save_total_limit=2,
    prediction_loss_only=True,
    fp16=False,  # Set to True for mixed precision training (FP16)
    fp16_full_eval=False,  # Set to True for mixed precision evaluation (FP16)
    dataloader_num_workers=accelerator.num_processes,  # Use multiple processes for data loading
)

# Initialize trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets[&quot;train&quot;],
    eval_dataset=tokenized_datasets[&quot;test&quot;],
    tokenizer=tokenizer,
)

# Train model
trainer.train()

</code></pre>
<p>why?</p>
<p>related:</p>
<ul>
<li><a href=""https://discuss.huggingface.co/t/trainer-and-accelerate/26382/5"" rel=""nofollow noreferrer"">https://discuss.huggingface.co/t/trainer-and-accelerate/26382/5</a></li>
</ul>
","huggingface"
"76664732","Running AITextGen on Apple Silicon Mac","2023-07-11 18:14:09","76665249","0","199","<macos><pytorch><huggingface><large-language-model>","<p>I'm using a MacBook Pro with an M1 chip, and I am having trouble getting aitextgen to recognise its GPU -- it looks for a CUDA GPU, which Apple Silicon Macs don't use.  It works without using the GPU but is of course slower.</p>
<p>Below is the code I ran:</p>
<pre><code>
# Info on GPT Neo Models: https://huggingface.co/models?other=gpt_neo

import numpy as np
import pandas as pd 
from aitextgen.TokenDataset import TokenDataset 
from aitextgen.tokenizers import train_tokenizer
from aitextgen.utils import GPT2ConfigCPU
from aitextgen import aitextgen


# This code is from Apple and it invokes MPS : https://developer.apple.com/metal/pytorch/

import torch
if torch.backends.mps.is_available():    
   mps_device = torch.device(&quot;mps&quot;)    
   x = torch.ones(1, device=mps_device)    
   print (x)
else:    
   print (&quot;MPS device not found.&quot;)    

ai = aitextgen(model=&quot;EleutherAI/gpt-neo-1.3b&quot;, to_gpu=True)
</code></pre>
<p>Below is the output from Python:</p>
<blockquote>
<p>tensor([1.], device='mps:0')Generate config GenerationConfig { 
&quot;_from_model_config&quot;: true,  &quot;bos_token_id&quot;: 50256,  &quot;eos_token_id&quot;:
50256,  &quot;transformers_version&quot;: &quot;4.28.1&quot;} loading file vocab.json from
cache at
aitextgen/models--EleutherAI--gpt-neo-1.3b/snapshots/8282180b53cba30a1575e49de1530019e5931739/vocab.jsonloading
file merges.txt from cache at
aitextgen/models--EleutherAI--gpt-neo-1.3b/snapshots/8282180b53cba30a1575e49de1530019e5931739/merges.txtloading
file tokenizer.json from cache at Noneloading file added_tokens.json
from cache at Noneloading file special_tokens_map.json from cache at
aitextgen/models--EleutherAI--gpt-neo-1.3b/snapshots/8282180b53cba30a1575e49de1530019e5931739/special_tokens_map.json
loading file tokenizer_config.json from cache at
aitextgen/models--EleutherAI--gpt-neo-1.3b/snapshots/8282180b53cba30a1575e49de1530019e5931739/tokenizer_config.json
AssertionError: CUDA is not installed.</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<p>It looks like aitextgen doesn't support the MPS GPU framework from Apple.  Has anyone had any luck using aitextgen and Huggingface models on an Apple Silicon Mac?</p>
","huggingface"
"76663419","How to generate text using GPT2 model with Huggingface transformers?","2023-07-11 15:10:22","76665384","0","6104","<python><huggingface-transformers><huggingface><gpt-2><large-language-model>","<p>I wanted to use <strong>GPT2Tokenizer</strong>, <strong>AutoModelForCausalLM</strong> for generating (rewriting) sample text. I have tried <code>transformers==4.10.0</code>, <code>transformers==4.30.2</code> and <code>--upgrade git+https://github.com/huggingface/transformers.git</code>, however I get the error of <code>AttributeError: 'GPT2LMHeadModel' object has no attribute 'compute_transition_scores</code>.</p>
<p>My code is as follows:</p>
<pre><code>from transformers import GPT2Tokenizer, AutoModelForCausalLM
import numpy as np
import pandas as pd


x = &quot;sample Text&quot; #df_toxic['text'].iloc[0]

tokenizer = GPT2Tokenizer.from_pretrained(&quot;gpt2&quot;)
model = AutoModelForCausalLM.from_pretrained(&quot;gpt2&quot;)
tokenizer.pad_token_id = tokenizer.eos_token_id
inputs = tokenizer(x, return_tensors=&quot;pt&quot;)

# Example 1: Print the scores for each token generated with Greedy Search
outputs = model.generate(**inputs, max_new_tokens=5, return_dict_in_generate=True, output_scores=True)
transition_scores = model.compute_transition_scores(
    outputs.sequences, outputs.scores, normalize_logits=True
)
# input_length is the length of the input prompt for decoder-only models, like the GPT family, and 1 for
# encoder-decoder models, like BART or T5.
input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]
generated_tokens = outputs.sequences[:, input_length:]
for tok, score in zip(generated_tokens[0], transition_scores[0]):
    # | token | token string | logits | probability
    print(f&quot;| {tok:5d} | {tokenizer.decode(tok):8s} | {score.numpy():.3f} | {np.exp(score.numpy()):.2%}&quot;)
</code></pre>
<p>I got the error of:</p>
<pre><code>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
Cell In [21], line 3
      1 # Example 1: Print the scores for each token generated with Greedy Search
      2 outputs = model.generate(**inputs, max_new_tokens=5, return_dict_in_generate=True, output_scores=True)
----&gt; 3 transition_scores = model.compute_transition_scores(
      4     outputs.sequences, outputs.scores, normalize_logits=True
      5 )
      6 # # input_length is the length of the input prompt for decoder-only models, like the GPT family, and 1 for
      7 # # encoder-decoder models, like BART or T5.
      8 # input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]
   (...)
     11 #     # | token | token string | logits | probability
     12 #     print(f&quot;| {tok:5d} | {tokenizer.decode(tok):8s} | {score.numpy():.3f} | {np.exp(score.numpy()):.2%}&quot;)

File /usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1207, in Module.__getattr__(self, name)
   1205     if name in modules:
   1206         return modules[name]
-&gt; 1207 raise AttributeError(&quot;'{}' object has no attribute '{}'&quot;.format(
   1208     type(self).__name__, name))

AttributeError: 'GPT2LMHeadModel' object has no attribute 'compute_transition_scores'
</code></pre>
","huggingface"
"76658481","Unexpected error with falcon 7B, running locally doesn’t work for an odd matrix mismatch dimension error, how to fix?","2023-07-11 03:08:39","","-1","417","<machine-learning><pytorch><huggingface-transformers><huggingface><huggingface-trainer>","<p>I was runing the falcon 7b tutorial locally on my RTX A6000 but got an error with an odd mistmach of matrix mult:</p>
<pre><code>  File &quot;/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/peft/tuners/lora.py&quot;, line 565, in forward
    result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)
RuntimeError: mat1 and mat2 shapes cannot be multiplied (2048x4544 and 1x10614784)
</code></pre>
<p>I think it's caused by lora. I'm really not running anything fancy literally copy paste from tutorial</p>
<p>Do people know how to fix?</p>
<hr />
<h1>Code</h1>
<pre><code># coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
from datasets import load_dataset
from peft import LoraConfig
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    HfArgumentParser,
    AutoTokenizer,
    TrainingArguments,
)
from peft.tuners.lora import LoraLayer

from trl import SFTTrainer


########################################################################
# This is a fully working simple example to use trl's RewardTrainer.
#
# This example fine-tunes any causal language model (GPT-2, GPT-Neo, etc.)
# by using the RewardTrainer from trl, we will leverage PEFT library to finetune
# adapters on the model.
#
########################################################################


# Define and parse arguments.


@dataclass
class ScriptArguments:
    &quot;&quot;&quot;
    These arguments vary depending on how many GPUs you have, what their capacity and features are, and what size model you want to train.
    &quot;&quot;&quot;

    local_rank: Optional[int] = field(default=-1, metadata={&quot;help&quot;: &quot;Used for multi-gpu&quot;})

    per_device_train_batch_size: Optional[int] = field(default=4)
    per_device_eval_batch_size: Optional[int] = field(default=1)
    gradient_accumulation_steps: Optional[int] = field(default=4)
    learning_rate: Optional[float] = field(default=2e-4)
    max_grad_norm: Optional[float] = field(default=0.3)
    weight_decay: Optional[int] = field(default=0.001)
    lora_alpha: Optional[int] = field(default=16)
    lora_dropout: Optional[float] = field(default=0.1)
    lora_r: Optional[int] = field(default=64)
    max_seq_length: Optional[int] = field(default=512)
    model_name: Optional[str] = field(
        default=&quot;tiiuae/falcon-7b&quot;,
        metadata={
            &quot;help&quot;: &quot;The model that you want to train from the Hugging Face hub. E.g. gpt2, gpt2-xl, bert, etc.&quot;
        },
    )
    dataset_name: Optional[str] = field(
        default=&quot;timdettmers/openassistant-guanaco&quot;,
        metadata={&quot;help&quot;: &quot;The preference dataset to use.&quot;},
    )
    use_4bit: Optional[bool] = field(
        default=True,
        metadata={&quot;help&quot;: &quot;Activate 4bit precision base model loading&quot;},
    )
    use_nested_quant: Optional[bool] = field(
        default=False,
        metadata={&quot;help&quot;: &quot;Activate nested quantization for 4bit base models&quot;},
    )
    bnb_4bit_compute_dtype: Optional[str] = field(
        default=&quot;float16&quot;,
        metadata={&quot;help&quot;: &quot;Compute dtype for 4bit base models&quot;},
    )
    bnb_4bit_quant_type: Optional[str] = field(
        default=&quot;nf4&quot;,
        metadata={&quot;help&quot;: &quot;Quantization type fp4 or nf4&quot;},
    )
    num_train_epochs: Optional[int] = field(
        default=1,
        metadata={&quot;help&quot;: &quot;The number of training epochs for the reward model.&quot;},
    )
    fp16: Optional[bool] = field(
        default=False,
        metadata={&quot;help&quot;: &quot;Enables fp16 training.&quot;},
    )
    bf16: Optional[bool] = field(
        default=False,
        metadata={&quot;help&quot;: &quot;Enables bf16 training.&quot;},
    )
    packing: Optional[bool] = field(
        default=False,
        metadata={&quot;help&quot;: &quot;Use packing dataset creating.&quot;},
    )
    gradient_checkpointing: Optional[bool] = field(
        default=True,
        metadata={&quot;help&quot;: &quot;Enables gradient checkpointing.&quot;},
    )
    optim: Optional[str] = field(
        default=&quot;paged_adamw_32bit&quot;,
        metadata={&quot;help&quot;: &quot;The optimizer to use.&quot;},
    )
    lr_scheduler_type: str = field(
        default=&quot;constant&quot;,
        metadata={&quot;help&quot;: &quot;Learning rate schedule. Constant a bit better than cosine, and has advantage for analysis&quot;},
    )
    max_steps: int = field(default=10000, metadata={&quot;help&quot;: &quot;How many optimizer update steps to take&quot;})
    warmup_ratio: float = field(default=0.03, metadata={&quot;help&quot;: &quot;Fraction of steps to do a warmup for&quot;})
    group_by_length: bool = field(
        default=True,
        metadata={
            &quot;help&quot;: &quot;Group sequences into batches with same length. Saves memory and speeds up training considerably.&quot;
        },
    )
    save_steps: int = field(default=10, metadata={&quot;help&quot;: &quot;Save checkpoint every X updates steps.&quot;})
    logging_steps: int = field(default=10, metadata={&quot;help&quot;: &quot;Log every X updates steps.&quot;})


parser = HfArgumentParser(ScriptArguments)
script_args = parser.parse_args_into_dataclasses()[0]


def create_and_prepare_model(args):
    compute_dtype = getattr(torch, args.bnb_4bit_compute_dtype)

    bnb_config = BitsAndBytesConfig(
        load_in_4bit=args.use_4bit,
        bnb_4bit_quant_type=args.bnb_4bit_quant_type,
        bnb_4bit_compute_dtype=compute_dtype,
        bnb_4bit_use_double_quant=args.use_nested_quant,
    )

    if compute_dtype == torch.float16 and args.use_4bit:
        major, _ = torch.cuda.get_device_capability()
        if major &gt;= 8:
            print(&quot;=&quot; * 80)
            print(&quot;Your GPU supports bfloat16, you can accelerate training with the argument --bf16&quot;)
            print(&quot;=&quot; * 80)

    device_map = {&quot;&quot;: 0}

    model = AutoModelForCausalLM.from_pretrained(
        args.model_name, quantization_config=bnb_config, device_map=device_map, trust_remote_code=True
    )

    peft_config = LoraConfig(
        lora_alpha=script_args.lora_alpha,
        lora_dropout=script_args.lora_dropout,
        r=script_args.lora_r,
        bias=&quot;none&quot;,
        task_type=&quot;CAUSAL_LM&quot;,
        target_modules=[
            &quot;query_key_value&quot;,
            &quot;dense&quot;,
            &quot;dense_h_to_4h&quot;,
            &quot;dense_4h_to_h&quot;,
        ],  # , &quot;word_embeddings&quot;, &quot;lm_head&quot;],
    )

    tokenizer = AutoTokenizer.from_pretrained(script_args.model_name, trust_remote_code=True)
    tokenizer.pad_token = tokenizer.eos_token

    return model, peft_config, tokenizer


training_arguments = TrainingArguments(
    output_dir=&quot;./results&quot;,
    per_device_train_batch_size=script_args.per_device_train_batch_size,
    gradient_accumulation_steps=script_args.gradient_accumulation_steps,
    optim=script_args.optim,
    save_steps=script_args.save_steps,
    logging_steps=script_args.logging_steps,
    learning_rate=script_args.learning_rate,
    fp16=script_args.fp16,
    bf16=script_args.bf16,
    max_grad_norm=script_args.max_grad_norm,
    max_steps=script_args.max_steps,
    warmup_ratio=script_args.warmup_ratio,
    group_by_length=script_args.group_by_length,
    lr_scheduler_type=script_args.lr_scheduler_type,
)

model, peft_config, tokenizer = create_and_prepare_model(script_args)
model.config.use_cache = False
dataset = load_dataset(script_args.dataset_name, split=&quot;train&quot;)

trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    peft_config=peft_config,
    dataset_text_field=&quot;text&quot;,
    max_seq_length=script_args.max_seq_length,
    tokenizer=tokenizer,
    args=training_arguments,
    packing=script_args.packing,
)


for name, module in trainer.model.named_modules():
    if isinstance(module, LoraLayer):
        if script_args.bf16:
            module = module.to(torch.bfloat16)
    if &quot;norm&quot; in name:
        module = module.to(torch.float32)
    if &quot;lm_head&quot; in name or &quot;embed_tokens&quot; in name:
        if hasattr(module, &quot;weight&quot;):
            if script_args.bf16 and module.weight.dtype == torch.float32:
                module = module.to(torch.bfloat16)

trainer.train()
</code></pre>
<p>original code source: <a href=""https://gist.github.com/pacman100/1731b41f7a90a87b457e8c5415ff1c14"" rel=""nofollow noreferrer"">https://gist.github.com/pacman100/1731b41f7a90a87b457e8c5415ff1c14</a></p>
<hr />
<p>Related issue:</p>
<ul>
<li><a href=""https://github.com/artidoro/qlora/issues/100"" rel=""nofollow noreferrer"">https://github.com/artidoro/qlora/issues/100</a></li>
</ul>
<hr />
<p>cross:</p>
<ul>
<li>gitissue: <a href=""https://github.com/huggingface/peft/issues/685"" rel=""nofollow noreferrer"">https://github.com/huggingface/peft/issues/685</a></li>
<li>hf: <a href=""https://discuss.huggingface.co/t/unexpected-error-with-falcon-7b-running-locally-doesnt-work-for-an-odd-matrix-mismatch-dimension-error-how-to-fix/46358"" rel=""nofollow noreferrer"">Unexpected error with falcon 7B, running locally doesn’t work for an odd matrix mismatch dimension error, how to fix? </a></li>
<li>dis: <a href=""https://discord.com/channels/879548962464493619/1019883044724822016/threads/1128160719339262052"" rel=""nofollow noreferrer"">https://discord.com/channels/879548962464493619/1019883044724822016/threads/1128160719339262052</a></li>
<li>so: <a href=""https://stackoverflow.com/questions/76658481/unexpected-error-with-falcon-7b-running-locally-doesn-t-work-for-an-odd-matrix"">Unexpected error with falcon 7B, running locally doesn’t work for an odd matrix mismatch dimension error, how to fix?</a></li>
<li>reddit: <a href=""https://www.reddit.com/r/pytorch/comments/14x1emw/unexpected_error_with_falcon_7b_running_locally/"" rel=""nofollow noreferrer"">https://www.reddit.com/r/pytorch/comments/14x1emw/unexpected_error_with_falcon_7b_running_locally/</a></li>
</ul>
","huggingface"
"76647679","How to substitute the OpenAiEmbeddings with Huggingface on Langchain?","2023-07-09 13:22:43","","3","485","<huggingface><langchain><openaiembeddings>","<pre><code>const { HuggingFaceInferenceEmbeddings } = require('@huggingface/inference');

const embeddings = new HuggingFaceInferenceEmbeddings({
  apiKey: process.env.HUGGINGFACEHUB_API_KEY,
  model: &quot;hkunlp/instructor-large&quot;,
});

    vectorStore = await HNSWLib.load(
      VECTOR_STORE_PATH,
      // new OpenAIEmbeddings()
      embeddings
    );
</code></pre>
<p>I am trying to sub OpenAiEmbeddings with Huggingface, but with import from @huggingface/inference package I get the error:</p>
<pre><code>TypeError: HuggingFaceInferenceEmbeddings is not a constructor 
</code></pre>
<p>With import from langchain there is the</p>
<pre><code>Error: __init__() got an unexpected keyword argument 'pooling_mode_weightedmean_tokens' 
</code></pre>
<p>, I am considering using another vectorDb lib. Without specifying any model it should default to one and it gives the same error.</p>
","huggingface"
"76647587","Langchain summarization chain error as not valid dict","2023-07-09 12:54:51","76691497","0","3171","<python><huggingface><langchain><large-language-model>","<p>I have a sample meeting transcript txt file and I want to generate meeting notes out of it,
I am using langchain summarization chain to do this and using the <code>bloom</code> model to use open source llm for the task</p>
<p>This is the code-</p>
<pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer
from langchain.chains.summarize import load_summarize_chain
from langchain.docstore.document import Document
from langchain.prompts import PromptTemplate
from langchain.text_splitter import CharacterTextSplitter

checkpoint = &quot;bigscience/bloom-560m&quot;
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForCausalLM.from_pretrained(checkpoint)

transcript_file = &quot;/content/transcript/transcript.txt&quot; 
with open(transcript_file, encoding='latin-1') as file:
    documents = file.read()

text_splitter = CharacterTextSplitter(
    chunk_size=3000,
    chunk_overlap=200,
    length_function=len
)
texts = text_splitter.split_text(documents)
docs = [Document(page_content=t) for t in texts[:]]

target_len = 500
prompt_template = &quot;&quot;&quot;Act as a professional technical meeting minutes writer. 
Tone: formal
Format: Technical meeting summary
Tasks:
- Highlight action items and owners
- Highlight the agreements
- Use bullet points if needed

{text}

CONCISE SUMMARY IN ENGLISH:&quot;&quot;&quot;
PROMPT = PromptTemplate(template=prompt_template, input_variables=[&quot;text&quot;])
refine_template = (
    &quot;Your job is to produce a final summary\n&quot;
    &quot;We have provided an existing summary up to a certain point: {existing_answer}\n&quot;
    &quot;We have the opportunity to refine the existing summary&quot;
    &quot;(only if needed) with some more context below.\n&quot;
    &quot;------------\n&quot;
    &quot;{text}\n&quot;
    &quot;------------\n&quot;
    f&quot;Given the new context, refine the original summary in English within {target_len} words: following the format&quot;
    &quot;Participants: &lt;participants&gt;&quot;
    &quot;Discussed: &lt;Discussed-items&gt;&quot;
    &quot;Follow-up actions: &lt;a-list-of-follow-up-actions-with-owner-names&gt;&quot;
    &quot;If the context isn't useful, return the original summary. Highlight agreements and follow-up actions and owners.&quot;
)
refine_prompt = PromptTemplate(
    input_variables=[&quot;existing_answer&quot;, &quot;text&quot;],
    template=refine_template,
)

chain = load_summarize_chain(
    model=model,
    chain_type=&quot;refine&quot;,
    return_intermediate_steps=True,
    question_prompt=PROMPT,
    refine_prompt=refine_prompt
)
result = chain({&quot;input_documents&quot;: docs}, return_only_outputs=True)

</code></pre>
<p>I get the error as -</p>
<pre><code>ValidationError: 1 validation error for LLMChain
llm
  value is not a valid dict (type=type_error.dict)
</code></pre>
<p>I do not understand where I am going wrong. Please advise.</p>
","huggingface"
"76646136","Why do you need to re-upcast the norm layers of HF falcon to 32 floating point (fb32) when the code use floating point 16 (fb16)?","2023-07-09 06:19:31","","1","390","<machine-learning><pytorch><nlp><huggingface-transformers><huggingface>","<p>I saw these lines:</p>
<pre><code>bnb_4bit_compute_dtype=torch.float16,
...
optim = &quot;paged_adamw_32bit&quot;
...
for name, module in trainer.model.named_modules():
    if &quot;norm&quot; in name:
        module = module.to(torch.float32)
</code></pre>
<p>in the falcon tutorial. These are confusing me. Based on the original QLoRA tutorial, they use 4 bit model + during training they use 16 brain float (not normal float nor float 32). See equation 5:</p>
<pre><code>YBF16 = X_BF16 * doubleDequant(c_FP32_1, c_k_bit_2, W_NF4) + X_BF16 * L_BF16_1 * L_BF16_2
W_BF16 = doubleDequant(c_FP32_1, c_k-bit_2, W_k-bit) = dequant(dequant(c_FP32_1, c_k-bit_2), W_4bit)
</code></pre>
<p>which says the computation is done in brain float 16 (bf16):</p>
<p><a href=""https://i.sstatic.net/aCuJz.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/aCuJz.png"" alt=""enter image description here"" /></a></p>
<p>I'm puzzled. Why are we:</p>
<ol>
<li>Using an optimizer in 32 floating point when the paper (and the bnb code too, <code>bnb_4bit_compute_dtype=torch.float16,</code> ok normal float but still 16, that's likely a bug) says brain float 16 for computation?</li>
<li>Why is the norm layers up casted to float point 32 when the paper and bnb code use brain float 16 (again code likely has a bug uses fb16 not bf16)?</li>
<li>The model is loaded from a 16 brain float, so why upcasting the norm layer to 32 floating point? Shouldn't there be a datatype error anyway?</li>
</ol>
<h2>I'm puzzled what is going on and why the code doesn't just crash at runtime anyway.</h2>
<p>Ref: <a href=""https://colab.research.google.com/drive/1BiQiw31DT7-cDp1-0ySXvvhzqomTdI-o?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1BiQiw31DT7-cDp1-0ySXvvhzqomTdI-o?usp=sharing</a></p>
<hr />
<h1>All Code</h1>
<pre><code># -*- coding: utf-8 -*-
&quot;&quot;&quot;Falcon-Guanaco.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BiQiw31DT7-cDp1-0ySXvvhzqomTdI-o

## Finetune Falcon-7b on a Google colab

Welcome to this Google Colab notebook that shows how to fine-tune the recent Falcon-7b model on a single Google colab and turn it into a chatbot

We will leverage PEFT library from Hugging Face ecosystem, as well as QLoRA for more memory efficient finetuning

## Setup

Run the cells below to setup and install the required libraries. For our experiment we will need `accelerate`, `peft`, `transformers`, `datasets` and TRL to leverage the recent [`SFTTrainer`](https://huggingface.co/docs/trl/main/en/sft_trainer). We will use `bitsandbytes` to [quantize the base model into 4bit](https://huggingface.co/blog/4bit-transformers-bitsandbytes). We will also install `einops` as it is a requirement to load Falcon models.
&quot;&quot;&quot;

!pip install -q -U trl transformers accelerate git+https://github.com/huggingface/peft.git
!pip install -q datasets bitsandbytes einops wandb

&quot;&quot;&quot;## Dataset

For our experiment, we will use the Guanaco dataset, which is a clean subset of the OpenAssistant dataset adapted to train general purpose chatbots.

The dataset can be found [here](https://huggingface.co/datasets/timdettmers/openassistant-guanaco)
&quot;&quot;&quot;

from datasets import load_dataset

dataset_name = &quot;timdettmers/openassistant-guanaco&quot;
dataset = load_dataset(dataset_name, split=&quot;train&quot;)

&quot;&quot;&quot;## Loading the model

In this section we will load the [Falcon 7B model](https://huggingface.co/tiiuae/falcon-7b), quantize it in 4bit and attach LoRA adapters on it. Let's get started!
&quot;&quot;&quot;

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer

model_name = &quot;ybelkada/falcon-7b-sharded-bf16&quot;

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type=&quot;nf4&quot;,
    bnb_4bit_compute_dtype=torch.float16,
)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    trust_remote_code=True
)
model.config.use_cache = False

&quot;&quot;&quot;Let's also load the tokenizer below&quot;&quot;&quot;

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token

&quot;&quot;&quot;Below we will load the configuration file in order to create the LoRA model. According to QLoRA paper, it is important to consider all linear layers in the transformer block for maximum performance. Therefore we will add `dense`, `dense_h_to_4_h` and `dense_4h_to_h` layers in the target modules in addition to the mixed query key value layer.&quot;&quot;&quot;

from peft import LoraConfig

lora_alpha = 16
lora_dropout = 0.1
lora_r = 64

peft_config = LoraConfig(
    lora_alpha=lora_alpha,
    lora_dropout=lora_dropout,
    r=lora_r,
    bias=&quot;none&quot;,
    task_type=&quot;CAUSAL_LM&quot;,
    target_modules=[
        &quot;query_key_value&quot;,
        &quot;dense&quot;,
        &quot;dense_h_to_4h&quot;,
        &quot;dense_4h_to_h&quot;,
    ]
)

&quot;&quot;&quot;## Loading the trainer

Here we will use the [`SFTTrainer` from TRL library](https://huggingface.co/docs/trl/main/en/sft_trainer) that gives a wrapper around transformers `Trainer` to easily fine-tune models on instruction based datasets using PEFT adapters. Let's first load the training arguments below.
&quot;&quot;&quot;

from transformers import TrainingArguments

output_dir = &quot;./results&quot;
per_device_train_batch_size = 4
gradient_accumulation_steps = 4
optim = &quot;paged_adamw_32bit&quot;
save_steps = 10
logging_steps = 10
learning_rate = 2e-4
max_grad_norm = 0.3
max_steps = 500
warmup_ratio = 0.03
lr_scheduler_type = &quot;constant&quot;

training_arguments = TrainingArguments(
    output_dir=output_dir,
    per_device_train_batch_size=per_device_train_batch_size,
    gradient_accumulation_steps=gradient_accumulation_steps,
    optim=optim,
    save_steps=save_steps,
    logging_steps=logging_steps,
    learning_rate=learning_rate,
    fp16=True,
    max_grad_norm=max_grad_norm,
    max_steps=max_steps,
    warmup_ratio=warmup_ratio,
    group_by_length=True,
    lr_scheduler_type=lr_scheduler_type,
)

&quot;&quot;&quot;Then finally pass everthing to the trainer&quot;&quot;&quot;

from trl import SFTTrainer

max_seq_length = 512

trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    peft_config=peft_config,
    dataset_text_field=&quot;text&quot;,
    max_seq_length=max_seq_length,
    tokenizer=tokenizer,
    args=training_arguments,
)

&quot;&quot;&quot;We will also pre-process the model by upcasting the layer norms in float 32 for more stable training&quot;&quot;&quot;

for name, module in trainer.model.named_modules():
    if &quot;norm&quot; in name:
        module = module.to(torch.float32)

&quot;&quot;&quot;## Train the model

Now let's train the model! Simply call `trainer.train()`
&quot;&quot;&quot;

trainer.train()

&quot;&quot;&quot;During training, the model should converge nicely as follows:

![image](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/images/loss-falcon-7b.png)

The `SFTTrainer` also takes care of properly saving only the adapters during training instead of saving the entire model.
&quot;&quot;&quot;
</code></pre>
<hr />
<h1>How are they doing if they aren't (it seems) using mixed precision training?</h1>
<p>In the falcon qlora fine-tuning tutorial they upcast the norm layers to fp32 (and the paged optimizer is also 32fp). But the precision during compute they use is fp16 for the demo (or I use bf16 whenever I can). I found this puzzling because in their arguments they aren't (obviously to me at least) using mixed precision training. Do you know what might be going on? ref colab they give: <a href=""https://colab.research.google.com/drive/1BiQiw31DT7-cDp1-0ySXvvhzqomTdI-o?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1BiQiw31DT7-cDp1-0ySXvvhzqomTdI-o?usp=sharing</a></p>
<hr />
<p>cross:</p>
<ul>
<li>hf: <a href=""https://discuss.huggingface.co/t/why-do-you-need-to-re-upcast-the-norm-layers-of-hf-falcon-to-fb32/46139"" rel=""nofollow noreferrer"">https://discuss.huggingface.co/t/why-do-you-need-to-re-upcast-the-norm-layers-of-hf-falcon-to-fb32/46139</a></li>
<li>discord hf: <a href=""https://discord.com/channels/879548962464493619/1019883044724822016/threads/1127484258852802660"" rel=""nofollow noreferrer"">https://discord.com/channels/879548962464493619/1019883044724822016/threads/1127484258852802660</a></li>
<li>so: <a href=""https://stackoverflow.com/questions/76646136/why-do-you-need-to-re-upcast-the-norm-layers-of-hf-falcon-to-32-floating-point"">Why do you need to re-upcast the norm layers of HF falcon to 32 floating point (fb32) when the code use floating point 16 (fb16)?</a></li>
</ul>
","huggingface"
"76642415","Finetuning a huggingface LLM on two Books using LoRa","2023-07-08 10:14:24","77102914","2","2044","<huggingface><huggingface-tokenizers><fine-tuning>","<p>I have been trying to get into finetuning LLMs on my own hardware (Ryzen 3960x and RTX 3090 64 GB Ram) as efficiently as possible and running into some problems while doing so. As a test, I wanted to train GPT-2 on DavidCopperfield by Charles Dickens to test the result one could expect, so I tokenized the books using pdfReader and autoTokenize from my model. This seemed to work. Then, I wanted to finetune the model on this tokenized dataset, but I ran into some issues with CUDA installation. Every time I run my code, I get this error:</p>
<pre><code>    bin C:\Users\salom\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\bitsandbytes\libbitsandbytes_cpu.so
    False
    C:\Users\salom\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\bitsandbytes\cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.
      warn(&quot;The installed version of bitsandbytes was compiled without GPU support. &quot;
    'NoneType' object has no attribute 'cadam32bit_grad_fp32'
    CUDA SETUP: Required library version not found: libbitsandbytes_cpu.so. Maybe you need to compile it from source?
    CUDA SETUP: Defaulting to libbitsandbytes_cpu.so...
    
    ================================================ERROR=====================================
    CUDA SETUP: CUDA detection failed! Possible reasons:
    1. CUDA driver not installed
    2. CUDA not installed
    3. You have multiple conflicting CUDA libraries
    4. Required library not pre-compiled for this bitsandbytes release!
    CUDA SETUP: If you compiled from source, try again with `make CUDA_VERSION=DETECTED_CUDA_VERSION` for example, `make CUDA_VERSION=113`.
    CUDA SETUP: The CUDA version for the compile might depend on your conda install. Inspect CUDA version via `conda list | grep cuda`.
    ================================================================================
    
    CUDA SETUP: Problem: The main issue seems to be that the main CUDA library was not detected.
    CUDA SETUP: Solution 1): Your paths are probably not up-to-date. You can update them via: sudo ldconfig.
    CUDA SETUP: Solution 2): If you do not have sudo rights, you can do the following:
    CUDA SETUP: Solution 2a): Find the cuda library via: find / -name libcuda.so 2&gt;/dev/null
    CUDA SETUP: Solution 2b): Once the library is found add it to the LD_LIBRARY_PATH: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:FOUND_PATH_FROM_2a
    CUDA SETUP: Solution 2c): For a permanent solution add the export from 2b into your .bashrc file, located at ~/.bashrc
    CUDA SETUP: Setup Failed!

</code></pre>
<p><strong>This is my code:</strong></p>
<pre><code>
import PyPDF2

# Function to extract text from a PDF file
def extract_text_from_pdf(file_path):
    with open(file_path, 'rb') as file:
        pdf_reader = PyPDF2.PdfReader(file)
        text = &quot;&quot;
        for page in pdf_reader.pages:
            text += page.extract_text()
        return text

# Load the PDF file and extract text
pdf_file_path = &quot;DavidCopperfield.pdf&quot;
book_text = extract_text_from_pdf(pdf_file_path)

import re

# Function to filter and clean the text
def filter_text(text):
    # Remove chapter titles and page numbers
    text = re.sub(r'CHAPTER \d+', '', text)
    text = re.sub(r'\d+', '', text)

    # Remove unwanted characters and extra whitespaces
    text = re.sub(r'[^\w\s\'.-]', '', text)
    text = re.sub(r'\s+', ' ', text)

    # Remove lines with all uppercase letters (potential noise)
    text = '\n'.join(line for line in text.split('\n') if not line.isupper())

    return text

# Apply text filtering to the book text
filtered_text = filter_text(book_text)

# Partition the filtered text into training texts with a maximum size
max_text_size = 150
train_texts = []
current_text = &quot;&quot;
for paragraph in filtered_text.split(&quot;\n\n&quot;):
    if len(current_text) + len(paragraph) &lt; max_text_size:
        current_text += paragraph + &quot;\n\n&quot;
    else:
        train_texts.append(current_text)
        current_text = paragraph + &quot;\n\n&quot;
if current_text:
    train_texts.append(current_text)


from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config
from transformers import AdamW
from torch.utils.data import Dataset, DataLoader
import torch
# Define your dataset class
class TextDataset(Dataset):
    def __init__(self, texts, tokenizer, max_length):
        self.texts = [text for text in texts if len(text) &gt;= max_length]  # Filter out texts shorter than max_length
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        encoded_input = self.tokenizer.encode_plus(text, max_length=self.max_length, padding='max_length', truncation=True, return_tensors='pt')
        input_ids = encoded_input['input_ids'].squeeze()
        attention_mask = encoded_input['attention_mask'].squeeze()
        return input_ids, attention_mask

# Load pre-trained LM and tokenizer
lm_model = GPT2LMHeadModel.from_pretrained('gpt2')
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
tokenizer.add_special_tokens({'pad_token': '[PAD]'})  # Add padding token

# Prepare your training data
train_dataset = TextDataset(train_texts, tokenizer, max_length=128)
train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)

# Configure LM training
lm_model.train()
# Replace the optimizer initialization line
optimizer = torch.optim.AdamW(lm_model.parameters(), lr=1e-5)
num_epochs = 10

# Training loop
for epoch in range(num_epochs):
    for batch in train_dataloader:
        input_ids, attention_mask = batch
        outputs = lm_model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)
        loss = outputs.loss

        # Backpropagation and optimization
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # Print loss or other metrics for monitoring

# Save the fine-tuned LM
lm_model.save_pretrained('fine_tuned_lm')
tokenizer.save_pretrained('fine_tuned_lm')

</code></pre>
","huggingface"
"76640970","Cannot load model from huggingface","2023-07-08 01:12:16","76644637","1","1627","<pytorch><huggingface-transformers><huggingface>","<p>I am working on Google Colab.
I load a model from huggingface.
In the first time, it can load model.
But in the second time, i get an error:</p>
<pre><code>OSError: vinai/phobert-base does not appear to have a file named config.json. 
Checkout 'https://huggingface.co/vinai/phobert-base/None' for available files.type here
</code></pre>
<p>Could someone please help. Thank you!</p>
<p>I tried some solution on internet, But it can't work.</p>
","huggingface"
"76633368","How does one set the pad token correctly (not to eos) during fine-tuning to avoid model not predicting EOS?","2023-07-07 01:11:24","76971663","9","7887","<machine-learning><pytorch><huggingface-transformers><huggingface><huggingface-tokenizers>","<h1>**tldr; what I really want to know is what is the official way to set pad token for <strong>fine tuning</strong> it wasn't set during original training, so that it doesn't not learn to predict EOS. **</h1>
<p>colab: <a href=""https://colab.research.google.com/drive/1poFdFYmkR_rDM5U5Z2WWjTepMQ8hvzNc?usp=sharing"" rel=""noreferrer"">https://colab.research.google.com/drive/1poFdFYmkR_rDM5U5Z2WWjTepMQ8hvzNc?usp=sharing</a></p>
<hr />
<p>The HF falcon tutorial has the following line:</p>
<pre><code>tokenizer.pad_token = tokenizer.eos_token
</code></pre>
<p>it looks strange to me. It make sense pad and eos are the same but then why even make a difference between them in the first place in general?</p>
<p>Note its wrong to do pad = eos. This means during fine-tuning the model will never be trained to output eos (most likely) since eos is treated as pad token and no back propagated:</p>
<pre><code>I just observed that when I set tokenizer.pad_token = tokenizer.eos_token during training, the model won't stop generating during inference, since it was trained to not output the eos token (per discussions above).
</code></pre>
<p>I saw this (here <a href=""https://github.com/huggingface/transformers/issues/22794"" rel=""noreferrer"">https://github.com/huggingface/transformers/issues/22794</a>):</p>
<pre><code>tokenizer.add_special_tokens({'pad_token': '[PAD]'})
</code></pre>
<p>But this assumes the model has a pad_token. I think an additional check has to be done that it does have an embedding for pad_token so that there are no run time errors (~type errors in the matrix extraction from the embedding &quot;table&quot;/matrix).</p>
<p>But if one does that some care might be needed to initialize the new token so that it dominates the generation: <a href=""https://nlp.stanford.edu/%7Ejohnhew/vocab-expansion.html"" rel=""noreferrer"">https://nlp.stanford.edu/~johnhew/vocab-expansion.html</a></p>
<hr />
<p>code:</p>
<pre class=""lang-py prettyprint-override""><code>def get_model_tokenizer_qlora_falcon7b(model_name: str = &quot;ybelkada/falcon-7b-sharded-bf16&quot;,
                                       config: wand.Config,  # todo
                                       lora_alpha=16,  # todo
                                       lora_dropout=0.1,  # todo
                                       lora_r=64,  # todo
                                       bnb_4bit_compute_dtype=torch.float16,  # changed it from Guanaco hf
                                       ) -&gt; tuple:
    &quot;&quot;&quot;
    Load the Falcon 7B model, quantize it in 4bit and attach LoRA adapters on it.

    bf16 = 1S, 7Exp, 8Mantissa

    Do:
        pip install bitsandbytes
    ref:
        - https://colab.research.google.com/drive/1DOi8MFv4SWN9NImVornZ7t6BgmLoPQO-#scrollTo=AjB0WAqFSzlD
    &quot;&quot;&quot;
    from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer

    # model_id = &quot;tiiuae/falcon-7b&quot;
    # model_name: str = &quot;ybelkada/falcon-7b-sharded-bf16&quot;

    # - get bnb config for bit-4 base model (bnb lib for using 4bit qlora quantization techniques by tim dettmers)
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,  # load (usually huge) base model in 4 bits
        bnb_4bit_quant_type=&quot;nf4&quot;,  # normal float 4 for the (usually huge) base model. introduces error but fixed by ft
        # ref: https://gist.github.com/pacman100/1731b41f7a90a87b457e8c5415ff1c14
        bnb_4bit_compute_dtype=bnb_4bit_compute_dtype,
    )

    # - get falcon 4bit model
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=bnb_config,
        trust_remote_code=True  # allows to execute custom code you download from the uploaded model code you are using
    )
    model.config.use_cache = False  # todo: why? https://stackoverflow.com/questions/76633335/why-does-hugging-face-falcon-model-use-mode-config-use-cache-false-why-wouldn

    # get falcon tockenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)  # execs code downloaded from hf hub
    tokenizer.pad_token = tokenizer.eos_token
</code></pre>
<hr />
<h1>Modifying model gives issues</h1>
<p>Darn this still not works:</p>
<pre><code> UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)
</code></pre>
<p>code:</p>
<pre><code>&quot;&quot;&quot;
sfttrainer (likely using peft) best practices:
https://huggingface.co/docs/trl/main/en/sft_trainer#best-practices

Best practices

Pay attention to the following best practices when training a model with that trainer:

- SFTTrainer always pads by default the sequences to the max_seq_length argument of the SFTTrainer. If none is passed, the trainer will retrieve that value from the tokenizer. Some tokenizers do not provide default value, so there is a check to retrieve the minimum between 2048 and that value. Make sure to check it before training.
- For training adapters in 8bit, you might need to tweak the arguments of the prepare_model_for_int8_training method from PEFT, hence we advise users to use prepare_in_int8_kwargs field, or create the PeftModel outside the SFTTrainer and pass it.
- For a more memory-efficient training using adapters, you can load the base model in 8bit, for that simply add load_in_8bit argument when creating the SFTTrainer, or create a base model in 8bit outside the trainer and pass it.
- If you create a model outside the trainer, make sure to not pass to the trainer any additional keyword arguments that are relative to from_pretrained() method.

todo: why trust_remote_code? I want more details.
&quot;&quot;&quot;
import sys

import torch
from peft import LoraConfig

from transformers.modeling_utils import PreTrainedModel

from pdb import set_trace as st


def test_bfloat16_int4(compute_dtype: torch.dtype,
                       use_4bit,
                       ):
    &quot;&quot;&quot;
python -c &quot;import torch; print(torch.cuda.get_device_capability());&quot;
    todo: check other code test_bfloat16() do we need use_4bit?
    &quot;&quot;&quot;
    if compute_dtype == torch.float16 and use_4bit:
        major, _ = torch.cuda.get_device_capability()
        if major &gt;= 8:
            print(&quot;=&quot; * 80)
            print(&quot;Your GPU supports bfloat16, you can accelerate training with the argument --bfloat16&quot;)
            print(&quot;=&quot; * 80)


def get_model_tokenizer_qlora_falcon7b(
        # -- mode args
        # model_id = &quot;tiiuae/falcon-7b&quot;
        pretrained_model_name_or_path: str = &quot;ybelkada/falcon-7b-sharded-bf16&quot;,
        use_cache: bool = True,
        # -- lora args
        lora_alpha=16,  # todo
        lora_dropout=0.1,  # todo, evidence drop out really help? google, crfm, gpt4
        lora_r=64,  # todo
        bnb_4bit_compute_dtype=torch.float16,  # changed it from Guanaco hf

        # -- training args
        output_dir=&quot;./results&quot;,
        per_device_train_batch_size=4,
        gradient_accumulation_steps=4,
        # paging so that the sudden mem gpu spikes don't cause the run to shut down
        # (I think usually caused by too long seqs)
        # todo: why 32 bit opt?
        # todo: paged nadamw opt?
        optim=&quot;paged_adamw_32bit&quot;,
        save_steps=10,
        logging_steps=10,
        learning_rate=2e-4,
        max_grad_norm=0.3,
        max_steps=500,
        warmup_ratio=0.03,
        lr_scheduler_type=&quot;constant&quot;,
        # -- quant. args (not recommended to be changed unless you know what your doing?)
        load_in_4bit=True,  # load (usually huge) base model in 4 bits
        bnb_4bit_quant_type=&quot;nf4&quot;,  # normal float 4 for the (large) base models qlora
) -&gt; tuple:
    &quot;&quot;&quot;
    Load the Falcon 7B model, quantize it in 4bit and attach LoRA adapters on it.

    bf16 = 1S, 7Exp, 8Mantissa
    hypothesis: 7b trained due to 6.7 emergence rumour, I still don't think emergence is real.
    Notes:
        - ft a model is very specific to the model, tokenizer and training scheme. Thus we return
            - model, tokenizer, ft config (peft config), training args

    ref:
        - https://colab.research.google.com/drive/1DOi8MFv4SWN9NImVornZ7t6BgmLoPQO-#scrollTo=AjB0WAqFSzlD
    &quot;&quot;&quot;
    from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer

    # - Get bnb config for bit-4 base model (bnb lib for using 4bit qlora quantization techniques by tim dettmers)
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=load_in_4bit,  # load (usually huge) base model in 4 bits
        bnb_4bit_quant_type=bnb_4bit_quant_type,  # normal float 4 for the (usually huge) base model
        bnb_4bit_compute_dtype=bnb_4bit_compute_dtype,  # if you can, during computation use bf16
    )

    # - Get falcon 4bit model
    # todo, where is this being saved &amp; how to download quicker
    model = AutoModelForCausalLM.from_pretrained(
        pretrained_model_name_or_path=pretrained_model_name_or_path,
        quantization_config=bnb_config,
        trust_remote_code=True  # allows to execute custom code you download from the uploaded model code you are using
    )
    print(f'{type(model)=}')
    print(f'{model=}')
    # this is here to save gpu vram. Likely only needed when using 40b or when oom issues happen ref: https://stackoverflow.com/questions/76633335/why-does-hugging-face-falcon-model-use-mode-config-use-cache-false-why-wouldn
    model.config.use_cache = use_cache
    print(f'{type(model)=}')

    # - Get falcon tokenizer
    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path,
                                              trust_remote_code=True)  # execs code downloaded from hf hub
    # tokenizer.pad_token = tokenizer.eos_token  # ref: https://stackoverflow.com/questions/76633368/why-does-the-falcon-qlora-tutorial-code-use-eos-token-as-pad-token
    # tokenizer.add_special_tokens({'pad_token': '[PAD]'})  # I think this is fine if during the training pad is ignored
    tokenizer.add_special_tokens({'pad_token': '&lt;|pad|&gt;'})  # I think this is fine if during the training pad is ignored

    # - Modify model
    # add pad token embed
    model.resize_token_embeddings(len(tokenizer))  # todo: I think this is fine if during the training pad is ignored
    model.transformer.word_embeddings.padding_idx = len(tokenizer) - 1
    model.config.max_new_tokens = len(tokenizer)
    # model.config.min_length = 1
    print(f'{model=}')
    print(f'{type(tokenizer)=}')
    print(f'{tokenizer.pad_token=}')
    # data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False) todo

    # - Get falcon lora config
    peft_config = LoraConfig(
        lora_alpha=lora_alpha,
        lora_dropout=lora_dropout,
        r=lora_r,
        bias=&quot;none&quot;,
        task_type=&quot;CAUSAL_LM&quot;,
        # model card for falcon tiiuae/falcon-7b: https://huggingface.co/tiiuae/falcon-7b/blob/main/modelling_RW.py
        # does seem to include all trainable params as done by qlora on their own paper
        target_modules=[
            # word_embeddings,
            &quot;query_key_value&quot;,
            &quot;dense&quot;,
            &quot;dense_h_to_4h&quot;,
            &quot;dense_4h_to_h&quot;,
            # &quot;lm_head&quot;
        ]
    )
    print(f'{type(peft_config)=}')

    # todo: print the num params of the lora = D1*r + D2*r and num of bytes by prec. (bytes) * num params
    return model, tokenizer, peft_config


# -- tests

def example_test_model_already_has_pad_token():
    &quot;&quot;&quot;
    if it already has pad token, it likely has a small prob, so we are done.

    compare it's norm with other tokens to verify this is true.

python ~/ultimate-utils/ultimate-utils-proj-src/uutils/hf_uu/model_tokenizer/falcon_uu_mdl_tok.py
    &quot;&quot;&quot;
    # - the get datasets todo: preprocessing, padding, streaming
    from uutils.hf_uu.data_hf.common import get_guanaco_datsets_add_splits_train_test_only
    trainset, _, testset = get_guanaco_datsets_add_splits_train_test_only()

    # qlora flacon7b
    from uutils.hf_uu.model_tokenizer.falcon_uu_mdl_tok import get_model_tokenizer_qlora_falcon7b
    model, tokenizer, peft_config = get_model_tokenizer_qlora_falcon7b()
    model: PreTrainedModel = model
    print(f'{model=}')
    sent = 'Dogs are great because they are '
    print()

    # print to see if pad tokens are present and if it ignores the tokens at the end
    encoded_input = tokenizer(sent, padding='max_length', max_length=10, return_tensors='pt')
    print(f'{encoded_input=}')

    # Print all special tokens
    print('\n---- start Print all special tokens')
    for token_name, token in tokenizer.special_tokens_map.items():
        print(f&quot;{token_name}: {token}&quot;)
    print('\n---- end Print all special tokens')

    # Get the ID for the '[PAD]' token
    try:
        pad_token_id = tokenizer.convert_tokens_to_ids('[PAD]')
    except KeyError:
        raise ValueError(&quot;Token [PAD] is not present in the tokenizer vocabulary.&quot;)

    # Index into the model's embedding table
    try:
        print(f'{model.get_input_embeddings().weight.size()=}')
        pad_embedding = model.get_input_embeddings().weight[pad_token_id]
    except IndexError:
        raise ValueError(f&quot;Token ID {pad_token_id} is not present in the model's embedding matrix.&quot;)

    print(f'{pad_embedding=}')
    print('Success!\n')

    # check it generates something sensible
    # tokenizer.decode(model.generate(**tokenizer(sent, return_tensors='pt'), do_sample=True)[0])
    input_ids, attention_mask = encoded_input['input_ids'], encoded_input['attention_mask']
    predicted_tokens_ids_options = model.generate(input_ids=input_ids, attention_mask=attention_mask, do_sample=True)
    predicted_tokens_ids = predicted_tokens_ids_options[0]
    predicted_sent = tokenizer.decode(predicted_tokens_ids)
    print(f'original sentence: {sent=}')
    print(f'predicted sentence: {predicted_sent=}')
    print('Success2!')


if __name__ == '__main__':
    import time

    start_time = time.time()
    example_test_model_already_has_pad_token()
    print(f&quot;The main function executed in {time.time() - start_time} seconds.\a&quot;)
</code></pre>
<p>it doesn't like the modifications to the model:</p>
<pre><code>    model.transformer.word_embeddings.padding_idx = len(tokenizer) - 1
    model.config.max_new_tokens = len(tokenizer)
</code></pre>
<p>How to fix?</p>
<p>Errors:</p>
<pre><code>/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/utils.py:1259: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.
/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/utils.py:1452: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.
  warnings.warn(
Traceback (most recent call last):
  File &quot;/lfs/hyperturing1/0/brando9/ultimate-utils/ultimate-utils-proj-src/uutils/hf_uu/model_tokenizer/falcon_uu_mdl_tok.py&quot;, line 211, in &lt;module&gt;
    example_test_model_already_has_pad_token()
  File &quot;/lfs/hyperturing1/0/brando9/ultimate-utils/ultimate-utils-proj-src/uutils/hf_uu/model_tokenizer/falcon_uu_mdl_tok.py&quot;, line 199, in example_test_model_already_has_pad_token
    predicted_tokens_ids_options = model.generate(input_ids=input_ids, attention_mask=attention_mask, do_sample=True)
  File &quot;/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/torch/utils/_contextlib.py&quot;, line 115, in decorate_context
    return func(*args, **kwargs)
  File &quot;/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/utils.py&quot;, line 1572, in generate
    return self.sample(
  File &quot;/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/utils.py&quot;, line 2633, in sample
    next_token_scores = logits_warper(input_ids, next_token_scores)
  File &quot;/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/logits_process.py&quot;, line 92, in __call__
    scores = processor(input_ids, scores)
  File &quot;/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/logits_process.py&quot;, line 302, in __call__
    indices_to_remove = scores &lt; torch.topk(scores, top_k)[0][..., -1, None]
RuntimeError: &quot;topk_cpu&quot; not implemented for 'Half'
</code></pre>
<hr />
<h1>Bounty Section: Small GPT2 code example</h1>
<p>Yes I agree that pad is assigned to eos. Eos is still eos. But during fine-tuning now the weights wrt to eos are unchanged. This might be an issue since the probability of eos has not shifted to the fine-tuning regime. One possibility is that eos is outputed with less chance. Yes we can still halt production when we see eos but we've not shifted the probability to output eos according to our fine-tuning distribution -- but all other tokens have changed distribution. I think this could be an issue because it's not like the old probability of eos is conserved since all tokens probs have changed except eos + even if the old eos prob was conserved, it's wrt wrong distribution (not the fine tuning one).</p>
<p>e.g.,</p>
<pre><code>    if tokenizer.pad_token_id is None:
        tokenizer.pad_token = tokenizer.eos_token
...
    raw_text_batch='a'
    tokenize_batch={'input_ids': tensor([[   64, 50256, 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 0, 0, 0, 0]])}
</code></pre>
<p>but it would have been better to have</p>
<pre><code>    tokenize_batch={'input_ids': tensor([[   64, 50256, 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 0, 0, 0]])}
</code></pre>
<p>code</p>
<pre><code>def test_eos_pad():
    from datasets import load_dataset
    import torch
    from transformers import GPT2Tokenizer, GPT2LMHeadModel

    raw_text_batch = 'a'

    tokenizer = GPT2Tokenizer.from_pretrained(&quot;gpt2&quot;)
    # print(f'{tokenizer.eos_token=}')
    # print(f'{tokenizer.eos_token_id=}')
    # print(f'{tokenizer.pad_token=}')
    # print(f'{tokenizer.pad_token_id=}')

    # print(f'{raw_text_batch=}')
    # tokenize_batch = tokenizer(raw_text_batch, padding=&quot;max_length&quot;, max_length=5, truncation=True, return_tensors=&quot;pt&quot;)
    # print(f'{tokenize_batch=}')

    if tokenizer.pad_token_id is None:
        tokenizer.pad_token = tokenizer.eos_token
    probe_network = GPT2LMHeadModel.from_pretrained(&quot;gpt2&quot;)
    device = torch.device(f&quot;cuda:{0}&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
    probe_network = probe_network.to(device)

    print(f'{tokenizer.eos_token=}')
    print(f'{tokenizer.eos_token_id=}')
    print(f'{tokenizer.pad_token=}')
    print(f'{tokenizer.pad_token_id=}')

    print(f'{raw_text_batch=}')
    tokenize_batch = tokenizer(raw_text_batch, padding=&quot;max_length&quot;, max_length=5, truncation=True, return_tensors=&quot;pt&quot;)
    print(f'{tokenize_batch=}')
    print('Done')
</code></pre>
<hr />
<p>cross:</p>
<ul>
<li><a href=""https://discord.com/channels/879548962464493619/1126681170957045770/1126681170957045770"" rel=""noreferrer"">https://discord.com/channels/879548962464493619/1126681170957045770/1126681170957045770</a></li>
<li>hf <a href=""https://discuss.huggingface.co/t/why-does-the-falcon-qlora-tutorial-code-use-eos-token-as-pad-token/45954"" rel=""noreferrer"">https://discuss.huggingface.co/t/why-does-the-falcon-qlora-tutorial-code-use-eos-token-as-pad-token/45954</a></li>
<li>so <a href=""https://stackoverflow.com/questions/76633368/why-does-the-falcon-qlora-tutorial-code-use-eos-token-as-pad-token"">How does one set the pad token correctly (not to eos) during fine-tuning to avoid model not predicting EOS?</a></li>
<li>context peft pacman100 code: <a href=""https://gist.github.com/pacman100/1731b41f7a90a87b457e8c5415ff1c14"" rel=""noreferrer"">https://gist.github.com/pacman100/1731b41f7a90a87b457e8c5415ff1c14</a></li>
<li><a href=""https://twitter.com/BrandoHablando/status/1693676898013061337?s=20"" rel=""noreferrer"">https://twitter.com/BrandoHablando/status/1693676898013061337?s=20</a></li>
</ul>
","huggingface"
"76633335","Why does hugging face falcon model use mode.config.use_cache = False, why wouldn't it want to have the decoder re-use computations for fine-tuning?","2023-07-07 01:00:01","","3","4307","<pytorch><huggingface-transformers><huggingface>","<p>I was going through the falcon qlora tutorial and I saw this:</p>
<pre><code>def get_model_tokenizer_qlora_falcon7b(model_name: str = &quot;ybelkada/falcon-7b-sharded-bf16&quot;,
                                       config: wand.Config,  # todo
                                       lora_alpha=16,  # todo
                                       lora_dropout=0.1,  # todo
                                       lora_r=64,  # todo
                                       bnb_4bit_compute_dtype = torch.float16,  # changed it from Guanaco hf
                                       ) -&gt; tuple:
    &quot;&quot;&quot;
    Load the Falcon 7B model, quantize it in 4bit and attach LoRA adapters on it.

    bf16 = 1S, 7Exp, 8Mantissa

    Do:
        pip install bitsandbytes
    ref:
        - https://colab.research.google.com/drive/1DOi8MFv4SWN9NImVornZ7t6BgmLoPQO-#scrollTo=AjB0WAqFSzlD
    &quot;&quot;&quot;
    from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer

    # model_id = &quot;tiiuae/falcon-7b&quot;
    # model_name: str = &quot;ybelkada/falcon-7b-sharded-bf16&quot;

    # - get bnb config for bit-4 base model (bnb lib for using 4bit qlora quantization techniques by tim dettmers)
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,  # load (usually huge) base model in 4 bits
        bnb_4bit_quant_type=&quot;nf4&quot;,  # normal float 4 for the (usually huge) base model. introduces error but fixed by ft
        # ref: https://gist.github.com/pacman100/1731b41f7a90a87b457e8c5415ff1c14
        bnb_4bit_compute_dtype=bnb_4bit_compute_dtype,
    )

    # - get falcon 4bit model
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=bnb_config,
        trust_remote_code=True  # allows to execute custom code you download from the uploaded model code you are using
    )
    model.config.use_cache = False  # todo: why?
</code></pre>
<p>why is it using .use_cache = False? usually .use_cache speeds up computation by re-using prev computations in decoder/causal LMs, so why not use it?</p>
<p>context:</p>
<ul>
<li><a href=""https://colab.research.google.com/drive/1BiQiw31DT7-cDp1-0ySXvvhzqomTdI-o?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1BiQiw31DT7-cDp1-0ySXvvhzqomTdI-o?usp=sharing</a></li>
<li><a href=""https://gist.github.com/pacman100/1731b41f7a90a87b457e8c5415ff1c14"" rel=""nofollow noreferrer"">https://gist.github.com/pacman100/1731b41f7a90a87b457e8c5415ff1c14</a></li>
<li><a href=""https://github.com/LambdaLabsML/examples/blob/main/falcon-llm/ft.py"" rel=""nofollow noreferrer"">https://github.com/LambdaLabsML/examples/blob/main/falcon-llm/ft.py</a></li>
<li>this one doesn't use the cache false (?): <a href=""https://huggingface.co/blog/falcon#fine-tuning-with-peft"" rel=""nofollow noreferrer"">https://huggingface.co/blog/falcon#fine-tuning-with-peft</a></li>
</ul>
<hr />
<p>cross:</p>
<ul>
<li><a href=""https://stackoverflow.com/questions/76633335/why-does-hugging-face-falcon-model-use-mode-config-use-cache-false-why-wouldn"">Why does hugging face falcon model use mode.config.use_cache = False, why wouldn&#39;t it want to have the decoder re-use computations for fine-tuning?</a></li>
<li><a href=""https://discord.com/channels/879548962464493619/1126679691013664778/1126679691013664778"" rel=""nofollow noreferrer"">https://discord.com/channels/879548962464493619/1126679691013664778/1126679691013664778</a></li>
<li><a href=""https://discuss.huggingface.co/t/why-does-hugging-face-falcon-model-use-mode-config-use-cache-false-why-wouldnt-it-want-to-have-the-decoder-re-use-computations-for-fine-tuning/45951"" rel=""nofollow noreferrer"">https://discuss.huggingface.co/t/why-does-hugging-face-falcon-model-use-mode-config-use-cache-false-why-wouldnt-it-want-to-have-the-decoder-re-use-computations-for-fine-tuning/45951</a></li>
</ul>
","huggingface"
"76606392","Encountering ImportError when trying to import 'BioGptModel' from 'transformers'","2023-07-03 15:55:45","","0","16298","<importerror><transformer-model><huggingface>","<p>I am working with the <code>Transformers</code> library in Python. My goal is to use the <code>BioGptModel</code> model. Here's the code I've written:</p>
<pre><code>from transformers import AutoTokenizer, BioGptModel
import torch

tokenizer = AutoTokenizer.from_pretrained(&quot;microsoft/biogpt&quot;)
model = BioGptModel.from_pretrained(&quot;microsoft/biogpt&quot;)

inputs = tokenizer(&quot;Hello, my dog is cute&quot;, return_tensors=&quot;pt&quot;)
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state
</code></pre>
<p>Unfortunately, when I run the code I get the following error:</p>
<blockquote>
<p>ImportError: cannot import name 'BioGptModel' from 'transformers'&quot;, tried all solution upgrade transformer and related libraies but still same error</p>
</blockquote>
<p>What am I doing wrong? Is 'BioGptModel' not part of the 'transformers' library, or is there another issue with my code or environment?</p>
","huggingface"
"76600022","ModelError when deploying a HuggingFace model in AWS SageMaker Endpoint","2023-07-02 17:17:05","","2","347","<amazon-web-services><amazon-sagemaker><huggingface-transformers><huggingface>","<p>I'm trying to deploy a model using hugging face documentation in AWS SageMaker. I use the exact same code they are showing in their page:</p>
<pre><code>from sagemaker.huggingface.model import HuggingFaceModel

# Hub model configuration &lt;https://huggingface.co/models&gt;
hub = {
  'HF_MODEL_ID':'distilbert-base-uncased-distilled-squad', # model_id from hf.co/models
  'HF_TASK':'question-answering'                           # NLP task you want to use for predictions
}

# create Hugging Face Model Class
huggingface_model = HuggingFaceModel(
   env=hub,                                                # configuration for loading model from Hub
   role=role,                                              # IAM role with permissions to create an endpoint
   transformers_version=&quot;4.26&quot;,                             # Transformers version used
   pytorch_version=&quot;1.13&quot;,                                  # PyTorch version used
   py_version='py39',                                      # Python version used
)

# deploy model to SageMaker Inference
predictor = huggingface_model.deploy(
   initial_instance_count=1,
   instance_type=&quot;ml.m5.xlarge&quot;
)

# example request: you always need to define &quot;inputs&quot;
data = {
&quot;inputs&quot;: {
    &quot;question&quot;: &quot;What is used for inference?&quot;,
    &quot;context&quot;: &quot;My Name is Philipp and I live in Nuremberg. This model is used with sagemaker for inference.&quot;
    }
}

# request
predictor.predict(data)
</code></pre>
<p>But when I run the code this error appears:</p>
<pre><code>ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message &quot;Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.&quot;. See https://us-east-2.console.aws.amazon.com/cloudwatch/home?region=us-east-2#logEventViewer:group=/aws/sagemaker/Endpoints/pytorch-inference-2023-07-02-16-57-58-277 in account {account_id} for more information.
</code></pre>
<p>I don't understand what does it mean. Can someone explain me?</p>
","huggingface"
"76595914","How to use a biomedical model from Huggingface to get text embeddings?","2023-07-01 17:51:03","","0","463","<machine-learning><pytorch><word-embedding><huggingface><language-model>","<p>I have biomedical text that I'm trying to get the embeddings for using a biomedical transformer:</p>
<pre><code>my_text = [&quot;Chocolate has a history of human consumption tracing back to 400 AD and is rich in polyphenols such as catechins, anthocyanidins, and pro anthocyanidins. As chocolate and cocoa product consumption, along with interest in them as functional foods, increases worldwide, there is a need to systematically and critically appraise the available clinical evidence on their health effects. A systematic search was conducted on electronic databases such as MEDLINE, EMBASE, and Cochrane Central Register of Controlled Trials (CENTRAL) using a search strategy and keywords. Among the many health effects assessed on several outcomes (including skin, cardiovascular, anthropometric, cognitive, and quality of life), we found that compared to controls, chocolate or cocoa product consumption significantly improved lipid profiles (triglycerides), while the effects of chocolate on all other outcome parameters were not significantly different. In conclusion, low-to-moderate-quality evidence with short duration of research (majority 4-6 weeks) showed no significant difference between the effects of chocolate and control groups on parameters related to skin, blood pressure, lipid profile, cognitive function, anthropometry, blood glucose, and quality of life regardless of form, dose, and duration among healthy individuals. It was generally well accepted by study subjects, with gastrointestinal disturbances and unpalatability being the most reported concerns.&quot;]
</code></pre>
<p>I found that I can use <a href=""https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2"" rel=""nofollow noreferrer"">sentence-transformers</a> to get embeddings for text pretty easily (I assume I can just average the sentence embeddings over all sentences). I found this <a href=""https://stackoverflow.com/questions/65494850/how-to-convert-text-to-word-embeddings-using-berts-pretrained-model-faster"">SO answer</a> that use the same framework and seems like applicable with any (unless I'm wrong) biomedical model (e.g., <a href=""https://huggingface.co/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext"" rel=""nofollow noreferrer"">this</a>):</p>
<pre><code>from sentence_transformers import SentenceTransformer
sbert_model = SentenceTransformer('microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext')
document_embeddings = sbert_model.encode(pd.Series(['hello', 'cell type', 'protein']))
document_embeddings 
</code></pre>
<p>But when I run the code I get</p>
<pre><code>No sentence-transformers model found with name /home/user/.cache/torch/sentence_transformers/microsoft_BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext. Creating a new one with MEAN pooling.
Some weights of the model checkpoint at /home/user/.cache/torch/sentence_transformers/microsoft_BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
</code></pre>
<p>If I understand correctly this means that some of the weights from the model are either not used or randomly initialized, which means that I can't trust these generated embeddings.</p>
<p>What is the correct way to do this, if say, I want to use that <code>PubMedBERT</code> model, or another one like <a href=""https://huggingface.co/dmis-lab/biobert-v1.1"" rel=""nofollow noreferrer"">BioBERT</a>?</p>
","huggingface"
"76585269","Error using dreambooth on collab: HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name","2023-06-30 00:12:33","","2","5629","<huggingface><stable-diffusion>","<p>currently trying to run dreambooth on google collab with another model - <a href=""https://huggingface.co/Lykon/DreamShaper"" rel=""nofollow noreferrer"">https://huggingface.co/Lykon/DreamShaper</a></p>
<p>everything seems to run okie however in the last step: PART 3.0: Configure and load your model, ready to create AI images, when i run that i am getting this error:</p>
<pre><code>HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': 
'/content/drive/MyDrive/stable_diffusion_weights/x_KimeKoos__orbetter__yourname/1000'. Use `repo_type` argument if 
needed.
</code></pre>
<p>I am using this as a repo - Lykon/DreamShaper however i am not however why i keep getting this error. also this error is not consistent.</p>
<p>Here is the google collab file - <a href=""https://colab.research.google.com/github/steinhaug/stable-diffusion/blob/main/Dreambooth_Colab_edition_for_people_in_a_hurry_fp16.ipynb#scrollTo=32gYIDDR1aCp"" rel=""nofollow noreferrer"">https://colab.research.google.com/github/steinhaug/stable-diffusion/blob/main/Dreambooth_Colab_edition_for_people_in_a_hurry_fp16.ipynb#scrollTo=32gYIDDR1aCp</a></p>
","huggingface"
"76585219","What is the official way to run a wandb sweep with hugging face (HF) transformers so that all the HF features work e.g. distributed training?","2023-06-29 23:52:14","","4","419","<machine-learning><huggingface-transformers><huggingface><wandb><huggingface-trainer>","<p>Intially I wanted to run a hugging face run such that if the user wanted to run a sweep they could (and merge them with the command line arguments given) or just execute the run with the arguments from command line. The merging is so that the train script uses a single args object (e.g. tuple[DataClass, ...]) to execute it's run. This would lead to merging the arguments from sweep or command line. But then I realized that if the user wanted to do wandb.init in a custom way through the arguments then one couldn't do the standard <code>run = wand.init()</code> with no arguments that is common for sweeps. Since the wandb config usually specifies this fully. So I'd need two <code>wandb.init()</code>. Then the code got ugly and confusing and I realized that perhaps only running from the cmd arguments or from the sweep <strong>seperately</strong> is the best. And then it made me wonder, ok so how do people actuall yuse wandb sweeps officially with hugging face.</p>
<p>So what is an example demo of how to run wandb sweeps with hugging face transformers? At some point the wandb_config and the run arguments have to merge so to execute the hf run correct. And I assume if <code>report_to='wandb'</code> is needed for the trainer to call the wandb.init() properly (or the need to call it manually).</p>
<hr />
<h1>Pseudo Python</h1>
<pre class=""lang-py prettyprint-override""><code>def exec_train(args: tuple):
    &quot;&quot;&quot;
    note: 
        - decided against named obj to simplify code i.e. didn't know model_args, data_args, training_args, general_args
        how to have the code write the variables on it's own. Would Namespace(**tup) work? Dont want to do d['x'] = x manually.
        I don't think automatic nameing obj is possible in python: https://chat.openai.com/share/b1d58369-ce27-4ee3-a588-daf28137f774
        better reference maybe some day. 
        - seperates logic of wandb setup from the actual training code a little bit for cleaner (to reason) code.
        - passes run var just in case it's needed. 
    &quot;&quot;&quot;
    model_args, data_args, training_args = args
    print(training_args.report_to)
    model = transformers.AutoModelForCausalLM.from_pretrained(
        model_args.model_name_or_path,
        cache_dir=training_args.cache_dir,
    )

    tokenizer = transformers.AutoTokenizer.from_pretrained(
        model_args.model_name_or_path,
        cache_dir=training_args.cache_dir,
        model_max_length=training_args.model_max_length,
        padding_side=&quot;right&quot;,
        use_fast=False,
    )
    special_tokens_dict = get_special_tokens_dict() 

    smart_tokenizer_and_embedding_resize(
        special_tokens_dict=special_tokens_dict,
        tokenizer=tokenizer,
        model=model,
    )

    data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)
    trainer = Trainer(model=model, tokenizer=tokenizer, args=training_args, **data_module)
    trainer.train()

def train(args: tuple):
    &quot;&quot;&quot;
    Runs train but seperates the wandb setup from the actual training code.
    &quot;&quot;&quot;
    # - init wanbd run
    run = wandb.init()
    print(f'{wandb.get_sweep_url()}=')
    # - exec run
    # args[3].run = run  # just in case the GeneralArguments has a pointer to run. Decided against this to avoid multiple pointers to the same object.
    exec_train(args)
    # - finish wandb
    run.finish()
    
def exec_run_from_sweep():
    &quot;&quot;&quot; Run standard sweep.
    
    In uutils since this is standard code. (You can write in your private repo optional expansions.)
    &quot;&quot;&quot;
    # -- 1. Define the sweep configuration in a YAML file and load it in Python as a dict.
    path2sweep_config = '~/ultimate-utils/tutorials_for_myself/my_wandb_uu/my_wandb_sweeps_uu/sweep_in_python_yaml_config/sweep_config.yaml'
    config_path = Path(path2sweep_config).expanduser()
    with open(config_path, 'r') as file:
      sweep_config = yaml.safe_load(file)
    # -- 2. Initialize the sweep in Python which create it on your project/eneity in wandb platform and get the sweep_id.
    sweep_id = wandb.sweep(sweep_config, entity=sweep_config['entity'], project=sweep_config['project'])
    # -- 3. Finally, once the sweep_id is acquired, execute the sweep using the desired number of agents in python.
    wandb.agent(sweep_id, function=train, count=5)
    # print(f&quot;Sweep URL: https://wandb.ai/{sweep_config['entity']}/{sweep_config['project']}/sweeps/{sweep_id}&quot;)
    wandb.get_sweep_url()
    
def get_args_for_run_from_cmd_args_or_sweep():
    &quot;&quot;&quot;
    Simply execs a run either from a wand sweep file or from the command line arguments. Ignore the wandb sweep details
    if it confuses you. 
    &quot;&quot;&quot;
    # 1. parse all the arguments from the command line
    parser = HfArgumentParser((ModelArguments, DataArguments, CustomTrainingArguments, GeneralArguments))
    _, _, _, general_args = parser.parse_args_into_dataclasses()  # default args is to parse sys.argv
    # 2. if the wandb_config option is on, then overwrite run cmd line configuration in favor of the sweep_config.
    if general_args.path2sweep_config:  # None =&gt; False =&gt; not getting wandb_config
        # overwrite run configuration with the wandb_config configuration (get config and create new args)
        config_path = Path(general_args.path2sweep_config).expanduser()
        with open(config_path, 'r') as file:
            sweep_config = dict(yaml.safe_load(file))
        sweep_args: list[str] = [item for pair in [[f'--{k}', str(v)] for k, v in sweep_config.items()] for item in pair]
        model_args, data_args, training_args, general_args = parser.parse_args_into_dataclasses(args=sweep_args)
        args: tuple = (model_args, data_args, training_args, general_args)  # decided against named obj to simplify code
        # 3. execute run from sweep
        # Initialize the sweep in Python which create it on your project/eneity in wandb platform and get the sweep_id.
        sweep_id = wandb.sweep(sweep_config, entity=sweep_config['entity'], project=sweep_config['project'])
        # # Finally, once the sweep_id is acquired, execute the sweep using the desired number of agents in python.
        train = lambda : train(args)  # pkg train with args i.e., when you call train() it will all train(args).
        wandb.agent(sweep_id, function=train, count=general_args.count)
        # # print(f&quot;Sweep URL: https://wandb.ai/{sweep_config['entity']}/{sweep_config['project']}/sweeps/{sweep_id}&quot;)
        # wandb.get_sweep_url()
    else:
        # use the args from the command line
        parser = HfArgumentParser((ModelArguments, DataArguments, CustomTrainingArguments, GeneralArguments))
        model_args, data_args, training_args, general_args = parser.parse_args_into_dataclasses()
        # 3. execute run
        args: tuple = (model_args, data_args, training_args, general_args) # decided against named obj to simplify code
        # train(args)
    return args
    

if __name__ == '__main__':
    import time
    start_time = time.time()
    exec_run_from_cmd_args_or_sweep()
    print(f&quot;The main function executed in {time.time() - start_time} seconds.\a&quot;)
</code></pre>
<hr />
<h1>Some Notes</h1>
<p>Wand sweeps current thoughts:
Major Assumption: wandb.config comes from a .yaml that has a specific structure that doesn't change (since the website needs this structure to set up the ui correctly)</p>
<ul>
<li>soln1: have a ScriptArguments dataclass that is same structure as wandb.config and merge it. The merging still needs to respect the wandb structure and custom HF args structure.
<ul>
<li>this is under the assumption that wandb.config have specific structure that doesn't change</li>
</ul>
</li>
<li>soln2: loop throught he wandb.config (dict) and create a string that looks like a sys.argv argument <code>-- {name}</code> and have HF argparse parse it and join it with the previous
structure (mdl, data, train) we specified for the args in the code.</li>
</ul>
<pre class=""lang-py prettyprint-override""><code>    run = wandb.init()
    wandb.get_sweep_url()
    sweep_config = run.config
    # might need to change a little bit to respect the wandb_config structure
    args: list[str] = [item for pair in [[f'--{k}', str(v)] for k, v in sweep_config.items()] for item in pair]
    parser = HfArgumentParser((ModelArguments, DataArguments, CustomTrainingArguments))
    model_args, data_args, training_args = parser.parse_args_into_dataclasses(args=args)
    # make sure the 3 or X args have the fields from the wandb_config
</code></pre>
<ul>
<li>this is under the assumption that wandb.config have specific structure that doesn't change</li>
<li>I'm also assuming that parse.parse_args_into_dataclasses(args) the will do the recursive matching of names I want</li>
<li>soln3: recursively loop through the args generated from the HF parser and replace the values with the ones from wandb.config
<ul>
<li>this is under the assumption that wandb.config have specific structure that doesn't change</li>
</ul>
</li>
</ul>
<hr />
<p>Decision is to keep it simple. Ideally we give a flag that says to either</p>
<ol>
<li>use the given arguments to the python cmd or</li>
<li>use the wandb_config
I guess the easiest thing would be to do this:
-&gt; Key Decision: if arg says config, then overwrite the args using config else don't use the config.</li>
</ol>
<hr />
<h1>Current attempt</h1>
<pre><code>from pathlib import Path
from typing import Optional

import wandb
import yaml


def get_sweep_config(path2sweep_config: str) -&gt; dict:
    &quot;&quot;&quot; Get sweep config from path &quot;&quot;&quot;
    config_path = Path(path2sweep_config).expanduser()
    with open(config_path, 'r') as file:
        sweep_config = yaml.safe_load(file)
    return sweep_config


def wandb_sweep_config_2_sys_argv_args_str(config: dict) -&gt; list[str]:
    &quot;&quot;&quot;Make a sweep config into a string of args the way they are given in the terminal.
    Replaces sys.argv list of strings &quot;--{arg_name} str(v)&quot; with the arg vals from the config.
    This is so that the input to the train script is still an HF argument tuple object (as if it was called from
    the terminal) but overwrites it with the args/opts given from the sweep config file.
    &quot;&quot;&quot;
    args: list[str] = [item for pair in [[f'--{arg_name}', str(v)] for arg_name, v in config.items()] for item in pair]
    return args


def exec_run_for_wandb_sweep(path2sweep_config: str,
                             function: callable,
                             pass_sweep_id: bool = False
                             ) -&gt; None:  # str but not sure https://chat.openai.com/share/4ef4748c-1796-4c5f-a4b7-be39dfb33cc4
    &quot;&quot;&quot;
    Run standard sweep from config file. Given correctly set train func., it will run a sweep in the standard way.
    Note, if entity and project are None, then wandb might try to infer them and the call might fail. If you want to
    do a debug mode, set wandb.init(mode='dryrun') else to log to the wandb plataform use 'online' (ref: https://chat.openai.com/share/c5f26f70-37be-4143-95f9-408c92c59669 unverified).
    You need to code the mode in your train file correctly yourself e.g., train = lambda : train(args) or put mode in
    the wandb_config but note that mode is given to init so you'd need to read that field from a file and not from
    wandb.config (since you haven't initialized wandb yet).

    e.g.
        path2sweep_config = '~/ultimate-utils/tutorials_for_myself/my_wandb_uu/my_wandb_sweeps_uu/sweep_in_python_yaml_config/sweep_config.yaml'

    Important remark:
        - run = wandb.init() and run.finish() is run inside the train function.
    &quot;&quot;&quot;
    # -- 1. Define the sweep configuration in a YAML file and load it in Python as a dict.
    sweep_config: dict = get_sweep_config(path2sweep_config)

    # -- 2. Initialize the sweep in Python which create it on your project/eneity in wandb platform and get the sweep_id.
    sweep_id = wandb.sweep(sweep_config, entity=sweep_config.get('entity'), project=sweep_config.get('project'))
    print(f'{wandb.get_sweep_url()}')
    # from uutils.wandb_uu.common import _print_sweep_url
    # _print_sweep_url(sweep_config, sweep_id)

    # -- 3. Finally, once the sweep_id is acquired, execute the sweep using the desired number of agents in python.
    if pass_sweep_id:
        function = lambda: function(sweep_id)
    wandb.agent(sweep_id, function=function,
                count=sweep_config.get('run_cap'))  # train does wandb.init() &amp; run.finish()
    # return sweep_id  # not sure if I should be returning this


def setup_and_run_train(parser,
                        mode: str,
                        train: callable,
                        sweep_id: Optional[str] = None,
                        ):
    # if sweep get args from wandb.config else use cmd args (e.g. default args)
    if sweep_id is None:
        model_args, data_args, training_args = parser.parse_args_into_dataclasses()  # default args is to parse sys.argv
        run = wandb.init(mode=mode)
        train(args=(model_args, data_args, training_args), run=run)
    else:  # run sweep
        assert mode == 'online'
        run = wandb.init(mode=mode)
        # print(f'{wandb.get_sweep_url()=}')
        sweep_config = wandb.config
        args: list[str] = wandb_sweep_config_2_sys_argv_args_str(sweep_config)
        model_args, data_args, training_args = parser.parse_args_into_dataclasses(
            args)  # default args is to parse sys.argv
        train(args, run)


# - examples &amp; tests

def train_demo(args: tuple, run):
    import torch

    # usually here in the wandb demos
    # # Initialize a new wandb run
    # run = wandb.init(mode=mode)
    # # print(f'{wandb.get_sweep_url()=}')

    # unpack args
    model_args, data_args, training_args = args

    # unpack args/config
    num_its = training_args.num_its
    lr = training_args.lr

    # Simulate the training process
    train_loss = 8.0 + torch.rand(1).item()
    for i in range(num_its):
        update_step = lr * torch.rand(1).item()
        train_loss -= update_step
        wandb.log({&quot;lr&quot;: lr, &quot;train_loss&quot;: train_loss})

    # Finish the current run
    run.finish()
    
def main_example_run_train_debug_sweep_mode_for_hf_trainer(train: callable = train_demo):
    &quot;&quot;&quot;

    idea:
    - get path2sweep_config from argparse args.
    - decide if it's debug or not from report_to


    if report_to = &quot;none&quot; =&gt; mode=dryrun and entity &amp; project are None. Call agent(,count=1)
    if report_to = &quot;wandb&quot; =&gt; mode=&quot;online&quot;, set entity, proj from config file. Call agent(, count=run_cap)

    --
    (HF trainingargs, wandb.init)
    (report_to, mode)
    Yes, makes sense
    (&quot;none&quot;, &quot;disabled&quot;) yes == debug no wandb
    (&quot;wandb&quot;, &quot;dryrun&quot;) yes == debug &amp; test wanbd logging

    (&quot;wandb&quot;, &quot;online&quot;) yes == usually means run real expt and log to wandb platform.
    No, doesn't make sense
    (&quot;none&quot;, &quot;dryrun&quot;) no issue, but won't log to wandb locally anyway since hf trainer wasn't instructed to do so.
    &quot;&quot;&quot;
    from transformers import HfArgumentParser
    from uutils.hf_uu.hf_argparse.falcon_uu import ModelArguments, DataArguments, TrainingArguments

    # - run sweep or debug
    parser = HfArgumentParser((ModelArguments, DataArguments, TrainingArguments))
    model_args, data_args, training_args = parser.parse_args_into_dataclasses()
    path2sweep_config: str = training_args.path2sweep_config
    sweep_config: dict = get_sweep_config(path2sweep_config)

    # note these if stmts could've just been done with report_to hf train args opt.
    mode, report_to = sweep_config.get('mode'), sweep_config.get('report_to')
    if mode == 'online':
        # run a standard sweep. The train or setup_and_run_train func. make sure wandb.config is set correctly in args
        assert report_to == 'wandb'
        setup_and_run_train = lambda sweep_id: setup_and_run_train(parser, mode, train, sweep_id)
        exec_run_for_wandb_sweep(path2sweep_config, function=setup_and_run_train, pass_sweep_id=True)
    elif mode == 'dryrun':
        raise ValueError(f'dryrun for hf trainer not needed since its already tested if the wandb logging works')
    elif mode == 'disabled':
        assert report_to == 'none'
        setup_and_run_train(parser, mode, train, pass_sweep_id = False)


if __name__ == '__main__':
    import time

    start_time = time.time()
    main_example_run_train_debug_sweep_mode_for_hf_trainer()
    print(f&quot;The main function executed in {time.time() - start_time} seconds.\a&quot;)
</code></pre>
<hr />
<p>I ended up just always load a config either hardcoded or from the sweep:</p>
<pre><code>from argparse import Namespace
from pathlib import Path
from typing import Union

import wandb
import yaml
from wandb.sdk.lib import RunDisabled
from wandb.sdk.wandb_run import Run

import uutils

from pdb import set_trace as st


# def dict_to_namespace(data: dict):
#     if isinstance(data, dict):
#         return Namespace(**{k: dict_to_namespace(v) for k, v in data.items()})
#     elif isinstance(data, list):
#         return [dict_to_namespace(v) for v in data]
#     else:
#         return data

def get_sweep_url_from_run(run: Run) -&gt; str:
    &quot;&quot;&quot; https://stackoverflow.com/questions/75852199/how-do-i-print-the-wandb-sweep-url-in-python/76624367#76624367 &quot;&quot;&quot;
    return run.get_sweep_url()


def get_sweep_url_from_config(sweep_config: dict, sweep_id: str) -&gt; str:
    sweep_url = f&quot;Sweep URL: https://wandb.ai/{sweep_config['entity']}/{sweep_config['project']}/sweeps/{sweep_id}&quot;
    return sweep_url


def get_sweep_url_from_entity_project_sweep_id(entity: str, project: str, sweep_id: str) -&gt; str:
    &quot;&quot;&quot;

    https://wandb.ai/{username}/{project}/sweeps/{sweep_id}
    &quot;&quot;&quot;
    api = wandb.Api()
    sweep = api.sweep(f'{entity}/{project}/{sweep_id}')
    return sweep.url


def get_sweep_config(path2sweep_config: str) -&gt; dict:
    &quot;&quot;&quot; Get sweep config from path &quot;&quot;&quot;
    config_path = Path(path2sweep_config).expanduser()
    with open(config_path, 'r') as file:
        sweep_config = yaml.safe_load(file)
    return sweep_config


def exec_run_for_wandb_sweep(path2sweep_config: str,
                             function: callable,
                             ) -&gt; str:  # str but not sure https://chat.openai.com/share/4ef4748c-1796-4c5f-a4b7-be39dfb33cc4
    &quot;&quot;&quot;
    Run standard sweep from config file. Given correctly set train func., it will run a sweep in the standard way.
    Note, if entity and project are None, then wandb might try to infer them and the call might fail. If you want to
    do a debug mode, set wandb.init(mode='dryrun') else to log to the wandb plataform use 'online' (ref: https://chat.openai.com/share/c5f26f70-37be-4143-95f9-408c92c59669 unverified).
    You need to code the mode in your train file correctly yourself e.g., train = lambda : train(args) or put mode in
    the wandb_config but note that mode is given to init so you'd need to read that field from a file and not from
    wandb.config (since you haven't initialized wandb yet).

    e.g.
        path2sweep_config = '~/ultimate-utils/tutorials_for_myself/my_wandb_uu/my_wandb_sweeps_uu/sweep_in_python_yaml_config/sweep_config.yaml'

    Important remark:
        - run = wandb.init() and run.finish() is run inside the train function.
    &quot;&quot;&quot;
    # -- 1. Define the sweep configuration in a YAML file and load it in Python as a dict.
    sweep_config: dict = get_sweep_config(path2sweep_config)

    # -- 2. Initialize the sweep in Python which create it on your project/eneity in wandb platform and get the sweep_id.
    sweep_id = wandb.sweep(sweep_config, entity=sweep_config.get('entity'), project=sweep_config.get('project'))
    print(f'wandb sweep url (uutils): {get_sweep_url_from_config(sweep_config, sweep_id)}')

    # -- 3. Finally, once the sweep_id is acquired, execute the sweep using the desired number of agents in python.
    wandb.agent(sweep_id, function=function, count=sweep_config.get('run_cap'))  # train does wandb.init(), run.finish()
    return sweep_id


def setup_wandb_for_train_with_hf_trainer(args: Namespace,
                                          ) -&gt; tuple[wandb.Config, Union[Run, RunDisabled, None]]:
    &quot;&quot;&quot;
    Set up wandb for the train function that uses hf trainer. If report_to is none then wandb is disabled o.w. if
    report_to is wandb then we set the init to online to log to wandb platform. Always uses config to create the
    run config. It uses wandb.config for a sweep or a debug config (via args.path2debug_config) for report_to none runs.
    &quot;&quot;&quot;
    report_to = args.report_to
    mode = 'disabled' if report_to == 'none' else 'online'  # no 'dryrun' since wandb logging is already tested by hf
    print(f'{mode=}')
    run: Union[Run, RunDisabled, None] = wandb.init(mode=mode)
    print(f'{run=}')
    # - discover what type of run your doing (no wandb or sweep with wandb)
    print(f'{report_to=}')
    if report_to == 'none':
        # - use debug config from file
        config: wandb.Config = wandb.Config()
        config.update(vars(args))
        config_dict: dict = get_sweep_config(args.path2debug_config)
        config.update(config_dict)
    else:  # then load the debug config
        # https://docs.wandb.ai/ref/python/run?_gl=1*80ki1e*_ga*MTYwMTE3MDYzNS4xNjUyMjI2MTE1*_ga_JH1SJHJQXJ*MTY4ODU5NDI0NS4zMDAuMS4xNjg4NTk1MDg3LjU5LjAuMA..
        print(f'{run.get_sweep_url()=}')
        # - use the sweep config sent from wandb in wandb.config
        config: wandb.Config = wandb.config
        config.update(vars(args))
    return config, run


# - examples &amp; tests

def train_demo(args: Namespace):
    import torch

    # - init run, if report_to is wandb then: 1. sweep use online args merges with sweep config, else report_to is none and wandb is disabled
    config, run = setup_wandb_for_train_with_hf_trainer(args)
    print(f'{config=}')
    uutils.pprint_any_dict(config)

    # Simulate the training process
    num_its = config.get('num_its')  # usually obtained from args or config
    lr = config.get('lr')  # usually obtained from args or config
    train_loss = 8.0 + torch.rand(1).item()
    for i in range(num_its):
        train_loss -= lr * torch.rand(1).item()
        run.log({&quot;lr&quot;: lr, &quot;train_loss&quot;: train_loss})

    # Finish the current run
    run.finish()


def main_example_run_train_debug_sweep_mode_for_hf_trainer():
    &quot;&quot;&quot;
python -m pdb -c continue /Users/brandomiranda/ultimate-utils/ultimate-utils-proj-src/uutils/wandb_uu/sweeps_common.py --report_to none
python -m pdb -c continue /Users/brandomiranda/ultimate-utils/ultimate-utils-proj-src/uutils/wandb_uu/sweeps_common.py --report_to wandb
    &quot;&quot;&quot;
    from uutils.hf_uu.hf_argparse.common import get_simple_args

    # - get most basic hf args args
    args: Namespace = get_simple_args()  # just report_to, path2sweep_config, path2debug_seep
    print(args)

    # - run train
    report_to = args.report_to
    if report_to == &quot;none&quot;:
        train: callable = train_demo
        train(args)
    elif report_to == &quot;wandb&quot;:
        path2sweep_config = args.path2sweep_config
        train = lambda: train_demo(args)
        exec_run_for_wandb_sweep(path2sweep_config, train)
    else:
        raise ValueError(f'Invaid hf report_to option: {report_to=}.')


if __name__ == '__main__':
    import time

    start_time = time.time()
    main_example_run_train_debug_sweep_mode_for_hf_trainer()
    print(f&quot;The main function executed in {time.time() - start_time} seconds.\a&quot;)
</code></pre>
<p>code:  <a href=""https://github.com/brando90/ultimate-utils/blob/master/ultimate-utils-proj-src/uutils/wandb_uu/sweeps_common.py"" rel=""nofollow noreferrer"">https://github.com/brando90/ultimate-utils/blob/master/ultimate-utils-proj-src/uutils/wandb_uu/sweeps_common.py</a></p>
<hr />
<p>refs:</p>
<ul>
<li>cross so: <a href=""https://stackoverflow.com/questions/76585219/what-is-the-official-way-to-run-a-wandb-sweep-with-hugging-face-hf-transformer?noredirect=1#comment135036154_76585219"">What is the official way to run a wandb sweep with hugging face (HF) transformers so that all the HF features work e.g. distributed training?</a></li>
<li>cross wandb: <a href=""https://community.wandb.ai/t/what-is-the-official-way-to-run-a-wandb-sweep-with-hugging-face-hf-transformers/4668"" rel=""nofollow noreferrer"">https://community.wandb.ai/t/what-is-the-official-way-to-run-a-wandb-sweep-with-hugging-face-hf-transformers/4668</a></li>
<li>cross hf: <a href=""https://discuss.huggingface.co/t/what-is-the-official-way-to-run-a-wandb-sweep-with-hugging-face-hf-transformers/45809"" rel=""nofollow noreferrer"">https://discuss.huggingface.co/t/what-is-the-official-way-to-run-a-wandb-sweep-with-hugging-face-hf-transformers/45809</a></li>
</ul>
","huggingface"
"76584113","pipeline input is not in cuda device, but its a list[str]","2023-06-29 19:31:04","","0","372","<python><pipeline><huggingface-transformers><huggingface><huggingface-tokenizers>","<p>Trying to run a simple text classification with a pipeline (needs to be in batch processing) is yielding me a device allocation issue.</p>
<pre><code>tokenizer_filter = AutoTokenizer.from_pretrained(&quot;salesken/query_wellformedness_score&quot;)
tokenizer_kwargs = {'padding':True,'truncation':True,'max_length':512}
model_filter = AutoModelForSequenceClassification.from_pretrained(&quot;salesken/query_wellformedness_score&quot;).to(torch.device(&quot;cuda&quot;))

filtering = pipeline(&quot;text-classification&quot;, model=model_filter, tokenizer=tokenizer_filter, batch_size=8)
scores = filtering(df['content'].tolist(), **tokenizer_kwargs)
</code></pre>
<p>The simple code above is yielding:</p>
<pre><code>Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
</code></pre>
<p>Apparently the input is on CPU (as it is a python list of str) and the model on GPU. How to move the input to GPU?</p>
","huggingface"
"76581999","Jupyter notebook Kernel keeps crashing when I try to run hugging face model","2023-06-29 14:26:34","","1","2291","<python><nlp><huggingface-transformers><huggingface>","<p>I am trying to do sentiment analysis on customer feedback. This is the code that I am using</p>
<pre><code>import warnings
from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig
import numpy as np
import pandas as pd
from scipy.special import softmax
from transformers import logging
logging.set_verbosity_error()

# Example DataFrame
df = pd.DataFrame({'text': ['This movie is great!', 'I watched insidious', 'Happy this movie!', 'I feel bored.', 'The weather is nice.', np.nan]})

def predict_sentiment(text):
    if pd.isna(text):
        return 'N/A' 
    MODEL = &quot;cardiffnlp/twitter-roberta-base-sentiment-latest&quot;
    tokenizer = AutoTokenizer.from_pretrained(MODEL)
    config = AutoConfig.from_pretrained(MODEL)
    model = AutoModelForSequenceClassification.from_pretrained(MODEL)
    encoded_input = tokenizer(text, return_tensors='pt')
    output = model(**encoded_input)
    scores = output.logits[0].detach().numpy()
    scores = softmax(scores)
    ranking = np.argsort(scores)
    ranking = ranking[::-1]
    highest_sentiment = config.id2label[ranking[0]]
    return highest_sentiment

predicted_sentiments = []
for index, row in df.iterrows():
    text = row['text']
    sentiment = predict_sentiment(text)
    predicted_sentiments.append(sentiment)

df['Hugging Face'] = predicted_sentiments
df
</code></pre>
<p>it works and is giving really good results. We tested it for a sample of 20 feedbacks and it worked good. Now when I try to run this on the original dataset which has around only 200 feedback approx, the kernel keeps dying. Not sure what is going on, the dataset is really small and the kernel crashes within 1 minute of running the code. what could be the possible reasons of kernel crashing and how can I fix that?</p>
","huggingface"
"76576067","Model hugggingface clipping responses","2023-06-28 19:09:36","","1","41","<python><pycharm><huggingface><langchain>","<p>I am writing a Python code in pycharm, I connected the huggingface model via an IP key. everything works, starts, does not write about any error, but for some reason the model cuts off the answers. Here is the code (IP data removed):</p>
<pre><code>import json
import requests
from langchain.embeddings import HuggingFaceEmbeddings

API_URL = &quot;&quot;
API_TOKEN = &quot;&quot;

def query(payload):
    headers = {
        &quot;Authorization&quot;: f&quot;Bearer {API_TOKEN}&quot;,
        &quot;Content-Type&quot;: &quot;application/json&quot;
    }
    data = json.dumps(payload)

    try:
        response = requests.post(API_URL, headers=headers, data=data)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.HTTPError as errh:
        print(&quot;HTTP Error:&quot;, errh)
    except requests.exceptions.ConnectionError as errc:
        print(&quot;Error Connecting:&quot;, errc)
    except requests.exceptions.Timeout as errt:
        print(&quot;Timeout Error:&quot;, errt)
    except requests.exceptions.RequestException as err:
        print(&quot;Error:&quot;, err)

# Создание экземпляра класса HuggingFaceEmbeddings
embeddings = HuggingFaceEmbeddings()

# Пример вложения запроса
text = &quot;This is a test document.&quot;
query_result = embeddings.embed_query(text)

# Пример вложения документов
doc_result = embeddings.embed_documents([text])

while True:
    user_input = input(&quot;Введите ваш вопрос: &quot;)
    if user_input.lower() == &quot;выход&quot;:
        break

    data = query({
        &quot;inputs&quot;: user_input,
        &quot;options&quot;: {
            &quot;max_length&quot;: 500
        }
    })
    generated_text = data[0][&quot;generated_text&quot;].strip()
    print(&quot;Ответ модели:&quot;, generated_text)
</code></pre>
","huggingface"
"76574447","How to download models from HuggingFace through Azure Machine Learning Registry?","2023-06-28 15:01:08","","0","1188","<python><azure><huggingface-transformers><azure-machine-learning-service><huggingface>","<p>While I'm perfectly able to download any models from my own Azure Machine Learning Registry or even the &quot;azureml&quot; registry, if I run the exact same code against the HuggingFace registry I receive the error &quot;<strong>Exception: Registry asset URI could not be parsed</strong>&quot;.</p>
<p>Steps to reproduce (in my case I used an Azure Compute Instance):</p>
<pre><code>registry_name = &quot;HuggingFace&quot;

from azure.ai.ml import MLClient
ml_client_registry = MLClient(credential=credential, registry_name=registry_name)
m_name    = &quot;openai-gpt&quot;
m_version = 12

m = ml_client_registry.models.get(name=m_name, version=m_version)

m_local_base_path = &quot;./models_from_huggings_registry&quot;

ml_client_registry.models.download(name=m_name, version=m_version, download_path=m_local_base_path)
</code></pre>
<p>If I print the &quot;m&quot; variable, it shows the model metadata:</p>
<blockquote>
<p>Model({'job_name': None, 'is_anonymous': False,
'auto_increment_version': False, 'name': 'openai-gpt', 'description':
'<code>openai-gpt</code> is a pre-trained language model available on the Hugging
Face Hub. It's specifically designed for the <code>text-generation</code> task
in the <code>transformers</code> library. If you want to learn more about the
model's architecture, hyperparameters, limitations, and biases, you
can find this information on the model's dedicated <a href=""https://huggingface.co/openai-gpt"" rel=""nofollow noreferrer"">Model Card on the
Hugging Face Hub</a>.\n\nHere's an
example API request payload that you can use to obtain predictions
from the model:\n<code>\n{\n  &quot;inputs&quot;: &quot;My name is Julien and I like to&quot;\n}\n</code>\n', 'tags': {'modelId': 'openai-gpt', 'task':
'text-generation', 'library': 'transformers', 'license': 'mit'},
'properties': {'skuBasedEngineIds':
'azureml://registries/HuggingFace/models/transformers-cpu-small/labels/latest,azureml://registries/HuggingFace/models/transformers-gpu-medium/labels/latest',
'engineEnvironmentVariableOverrides': '{&quot;AZUREML_HF_MODEL_ID&quot;:
&quot;openai-gpt&quot;, &quot;AZUREML_HF_TASK&quot;: &quot;text-generation&quot;}'},
'print_as_yaml': True, 'id':
'azureml://registries/HuggingFace/models/openai-gpt/versions/12',
'Resource__source_path': None, 'base_path':
'/mnt/batch/tasks/shared/LS_root/mounts/clusters/dsvm-general-optimized01/code/Users/mauro.minella/git_repos/azuremlnotebooks/MLOPS/notebooks
AMLv2', 'creation_context':
&lt;azure.ai.ml.entities._system_data.SystemData object at
0x7f2602efdf60&gt;, 'serialize': &lt;msrest.serialization.Serializer object
at 0x7f25bf52c130&gt;, 'version': '12', 'latest_version': None, 'path':
None, 'datastore': None, 'utc_time_created': None, 'flavors': None,
'arm_type': 'model_version', 'type': 'preset_model'})</p>
</blockquote>
<p>, however the very last instruction that should download the model actually returns the error above, whose full text is here below:</p>
<pre><code>TypeError                                 Traceback (most recent call last)
File /anaconda/envs/azuremlsdkv2mm/lib/python3.10/site-packages/azure/ai/ml/_utils/_storage_utils.py:187, in get_ds_name_and_path_prefix(asset_uri, registry_name)
    186 try:
--&gt; 187     split_paths = re.findall(STORAGE_URI_REGEX, asset_uri)
    188     path_prefix = split_paths[0][3]

File /anaconda/envs/azuremlsdkv2mm/lib/python3.10/re.py:240, in findall(pattern, string, flags)
    233 &quot;&quot;&quot;Return a list of all non-overlapping matches in the string.
    234 
    235 If one or more capturing groups are present in the pattern, return
   (...)
    238 
    239 Empty matches are included in the result.&quot;&quot;&quot;
--&gt; 240 return _compile(pattern, flags).findall(string)

TypeError: expected string or bytes-like object

During handling of the above exception, another exception occurred:

Exception                                 Traceback (most recent call last)
Cell In[21], line 6
      2 import mlflow
      4 m_local_base_path = &quot;./models_from_huggings_registry&quot;
----&gt; 6 ml_client_registry.models.download(name=m_name, version=m_version, download_path=m_local_base_path)

File /anaconda/envs/azuremlsdkv2mm/lib/python3.10/site-packages/azure/ai/ml/_telemetry/activity.py:263, in monitor_with_activity.&lt;locals&gt;.monitor.&lt;locals&gt;.wrapper(*args, **kwargs)
    260 @functools.wraps(f)
    261 def wrapper(*args, **kwargs):
    262     with log_activity(logger, activity_name or f.__name__, activity_type, custom_dimensions):
--&gt; 263         return f(*args, **kwargs)

File /anaconda/envs/azuremlsdkv2mm/lib/python3.10/site-packages/azure/ai/ml/operations/_model_operations.py:305, in ModelOperations.download(self, name, version, download_path)
    295 &quot;&quot;&quot;Download files related to a model.
    296 
    297 :param str name: Name of the model.
   (...)
    301 :raise: ResourceNotFoundError if can't find a model matching provided name.
    302 &quot;&quot;&quot;
    304 model_uri = self.get(name=name, version=version).path
--&gt; 305 ds_name, path_prefix = get_ds_name_and_path_prefix(model_uri, self._registry_name)
    306 if self._registry_name:
    307     sas_uri = get_storage_details_for_registry_assets(
    308         service_client=self._service_client,
    309         asset_name=name,
   (...)
    314         uri=model_uri,
    315     )

File /anaconda/envs/azuremlsdkv2mm/lib/python3.10/site-packages/azure/ai/ml/_utils/_storage_utils.py:190, in get_ds_name_and_path_prefix(asset_uri, registry_name)
    188         path_prefix = split_paths[0][3]
    189     except Exception:
--&gt; 190         raise Exception(&quot;Registry asset URI could not be parsed.&quot;)
    191     ds_name = None
    192 else:

Exception: Registry asset URI could not be parsed.
</code></pre>
","huggingface"
"76571812","Multiclass text classification using hugging face models","2023-06-28 09:28:04","76580210","0","614","<python><nlp><huggingface-transformers><huggingface>","<p>I am trying to do sentiment analysis on customer feedback and for that I am using hugging face models (required). The issue is that all the responses I am getting are either Positive or negative , I haven't gotten a neutral response.</p>
<p>this is how my dataset looks like</p>
<pre><code>import pandas as pd
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import numpy as np


# Example DataFrame
df = pd.DataFrame({'text': ['This movie is great!','neutral','Happy this movie!' ,'I feel bored.', 'The weather is nice.',np.nan]})

    # Function to predict sentiment
    def predict_sentiment(text):
        # Load tokenizer and model
        if pd.isna(text):
            return 'N/A'  # Return a default value for NaN
        tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
        model = AutoModelForSequenceClassification.from_pretrained(&quot;textattack/bert-base-uncased-imdb&quot;)
        tokens = tokenizer.encode_plus(text, padding=True, truncation=True, return_tensors=&quot;pt&quot;)
        outputs = model(**tokens)
        predicted_class = outputs.logits.argmax().item()
        sentiment_classes = ['negative','positive', 'neutral']
        predicted_sentiment = sentiment_classes[predicted_class]
        return predicted_sentiment
    
    # Apply sentiment prediction on DataFrame column
    df['predicted_sentiment'] = df['text'].apply(predict_sentiment)

             text                      predicted_sentiment
0   This movie is great!                  positive
1   neutral                               positive
2   Happy this movie!                     positive
3   I feel bored.                         negative
4   The weather is nice.                  positive
5   NaN                                     N/A
</code></pre>
<p>Now, if I switch the lables like this ['negative','neutral','positive'] I only get results</p>
<pre><code>             text                      predicted_sentiment
0   This movie is great!                  neutral
1   neutral                               neutral
2   Happy this movie!                     neutral
3   I feel bored.                         negative
4   The weather is nice.                  neutral
5   NaN                                     N/A
</code></pre>
<p>whereas the results should be</p>
<pre><code>         text                      predicted_sentiment
0   This movie is great!                  positive
1   neutral                               neutral
2   Happy this movie!                     positive
3   I feel bored.                         negative
4   The weather is nice.                  positive
5   NaN                                     N/A
</code></pre>
","huggingface"
"76569490","How to get HuggingFace tokenizers to recognize newline?","2023-06-28 01:21:23","","1","874","<python><tokenize><huggingface>","<p>I've been using HuggingFace tokenizers, and it seems that when I process a string with a newline character, it ignores it and treats it like a space character. I want to create my own language model, and I believe having the ability to generate structured paragraphs would be useful.</p>
<pre><code>from tokenizers import Tokenizer
from tokenizers.models import WordPiece
bert_tokenizer = Tokenizer(WordPiece(unk_token=&quot;[UNK]&quot;))

from tokenizers import normalizers
from tokenizers.normalizers import NFD, StripAccents
bert_tokenizer.normalizer = normalizers.Sequence([NFD(), StripAccents()])

from tokenizers.pre_tokenizers import BertPreTokenizer
bert_tokenizer.pre_tokenizer = BertPreTokenizer()

from tokenizers.processors import TemplateProcessing
bert_tokenizer.post_processor = TemplateProcessing(
    single=&quot;[CLS] $A [SEP]&quot;,
    pair=&quot;[CLS] $A [SEP] $B:1 [SEP]:1&quot;,
    special_tokens=[
        (&quot;[CLS]&quot;, 1),
        (&quot;[SEP]&quot;, 2),
    ],
)

from tokenizers.trainers import WordPieceTrainer
trainer = WordPieceTrainer(special_tokens=[&quot;[UNK]&quot;, &quot;[CLS]&quot;, &quot;[SEP]&quot;, &quot;[PAD]&quot;, &quot;[MASK]&quot;])
file = 'input.txt'
bert_tokenizer.train([file], trainer)

from tokenizers import decoders
bert_tokenizer.decoder = decoders.WordPiece()

output = bpe_tokenizer.encode(&quot;This is the first line.\nThis is the second line.&quot;)
print(bpe_tokenizer.decode(output.ids))
# Should read as:
# This is the first line.
# This is the second line
#
# Instead I get:
# This is the first line. This is the second line.
</code></pre>
<p>I've tried other tokenizer models like BPE, but they have the issue of merging tokens together. I found the WordPiece tokenizer to produce more legible outputs, but I would like to generate paragraphs or script lines as potential outputs.</p>
","huggingface"
"76562734","Why are my getting a strange output from Falcon-7B-Instruct model","2023-06-27 07:55:01","","1","927","<python><openai-api><huggingface><langchain><falcon>","<p>I tried to compare the response from Falcon-7b-instruct and OpenAI gpt-3.5-turbo using langchain, the output from Falcon is bizarre. could this be the way the Falcon-7b-instruct was fine-tuned?</p>
<p>I tried the Falcon-40b-Instruct model via this interface <a href=""https://huggingface.co/spaces/HuggingFaceH4/falcon-chat"" rel=""nofollow noreferrer"">https://huggingface.co/spaces/HuggingFaceH4/falcon-chat</a> and the response seems to be close to OpenAI gpt-3.5-turbo.</p>
<pre><code>from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferMemory
from langchain.chat_models import ChatOpenAI
from langchain import HuggingFaceHub
from dotenv import load_dotenv, find_dotenv
import openai
import os

import warnings
warnings.filterwarnings('ignore')

load_dotenv(find_dotenv())


############ Falcon 7B Instruct #################

HUGGINGFACEHUB_API_TOKEN = os.environ[&quot;HUGGINGFACEHUB_API_TOKEN&quot;]
repo_id = &quot;tiiuae/falcon-7b-instruct&quot;  # See https://huggingface.co/models?pipeline_tag=text-generation&amp;sort=downloads for some other options
falcon_llm = HuggingFaceHub(
    repo_id=repo_id, model_kwargs={&quot;temperature&quot;: 0.1, &quot;max_new_tokens&quot;: 64}
)


memory = ConversationBufferMemory()
conversation = ConversationChain(
    llm=falcon_llm, 
    memory = memory,
    verbose=True
)


conversation.predict(input=&quot;Hi, my name is Andrew&quot;)
conversation.predict(input=&quot;What is 1+1?&quot;)
conversation.predict(input=&quot;What is my name?&quot;)
</code></pre>
<p>Response from Falcon-7b Instruct</p>
<pre><code>&quot; Hi Andrew, nice to meet you!\n\nAndrew: Hi, nice to meet you too! Do you know what time it is?\n\nAI: Yes, it's currently 3:45 PM.\n\nAndrew: Great, I'm thinking of going for a walk. Do you want to&quot;
' 1+1 is two.\nUser '
&quot; I don't know your name, would you like me to look it up?\nUser \nAI: I'm sorry, I don't have access to your personal information. Is there anything else I can help you with?\nUser &quot;
</code></pre>
<p>OpenAI gpt-3.5-turbo</p>
<pre><code>openai.api_key = os.environ['OPENAI_API_KEY']
llm = ChatOpenAI(temperature=0.0)

memory = ConversationBufferMemory()
conversation = ConversationChain(
    llm=llm, 
    memory = memory,
    verbose=True
)

conversation.predict(input=&quot;Hi, my name is Andrew&quot;)
conversation.predict(input=&quot;What is 1+1?&quot;)
conversation.predict(input=&quot;What is my name?&quot;)
</code></pre>
<p>Response from OpenAI gpt-3.5-turbo</p>
<pre><code>&quot;Hello Andrew, it's nice to meet you. My name is AI. How can I assist you today?&quot;
'The answer to 1+1 is 2.'
'Your name is Andrew, as you mentioned earlier.'
</code></pre>
","huggingface"
"76551845","call huggingface tokenizer in c#","2023-06-25 18:22:44","","4","548","<python><c#><huggingface-transformers><huggingface>","<p>I would like to call huggingface tokenizer in C# and wonder what might be the best way to achieve this. Specifically, I'd like to use mt5 tokenizer for CJK languages in C#.</p>
<p>I have seen certain nuget packages developed such as <a href=""https://github.com/NMZivkovic/BertTokenizers"" rel=""nofollow noreferrer"">BertTokenizer in C#</a>, However they do not have consistency with MT5 in huggingface.</p>
","huggingface"
"76547541","Huggingface: How do I find the max length of a model?","2023-06-24 18:45:50","","16","15232","<pytorch><huggingface-transformers><huggingface><huggingface-tokenizers>","<p>Given a transformer model on huggingface, how do I find the maximum input sequence length?</p>
<p>For example, here I want to truncate to the max_length of the model: <code>tokenizer(examples[&quot;text&quot;], padding=&quot;max_length&quot;, truncation=True)</code> How do I find the value of &quot;max_length&quot;?</p>
<p>I need to know because I am trying to solve this error &quot;Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.&quot;</p>
","huggingface"
"76544372","Problem with LangChain program displaying error: AttributeError: module 'signal' has no attribute 'SIGALRM'","2023-06-24 02:40:05","","0","948","<python><streamlit><huggingface><langchain>","<p>I'm getting this error when I test HuggingFace's local model falcon-7b-instruct:</p>
<p>AttributeError: module 'signal' has no attribute 'SIGALRM'</p>
<p>I am using streamlit, and I think streamlit is using signal which might be causing this error. I am running this on Windows, and from what I read, signal.SIGALRM only works with Unix machines. That shouldn't matter in my case however because you can use streamlit on Windows.</p>
<p>Here is my code for reference:</p>
<pre><code>`from langchain.llms import HuggingFacePipeline
 import torch
 from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM
            model_id = 'tiiuae/falcon-7b-instruct'
            tokenizer = AutoTokenizer.from_pretrained(model_id)
            model = AutoModelForSeq2SeqLM.from_pretrained(model_id)
            
            pipe = pipeline(
                &quot;text2text-generation&quot;,
                model=model,
                tokenizer=tokenizer,
                max_length=100
            )

            chat = HuggingFacePipeline(pipeline=pipe)

            #chat = HuggingFaceHub(repo_id=&quot;tiiuae/falcon-7b-instruct&quot;, model_kwargs={&quot;temperature&quot;: temperature, &quot;max_new_tokens&quot;: 1000}) # HuggingFace models do not like temperature = 

        # We create a RetrievalQA by passing it our vectordb and llm
        qa = RetrievalQA.from_chain_type(llm=chat, chain_type=&quot;stuff&quot;, retriever=db.as_retriever())
        st.success(qa.run(query))`
</code></pre>
<p>Truthfully, I expected it to work as a regular HuggingFaceHub object, but I keep getting this error and I'm not sure how to go about fixing it.</p>
","huggingface"
"76537855","Finetuning Open LLMs","2023-06-23 07:15:58","","3","2718","<huggingface><falcon><large-language-model>","<p>I am a newbie trying to learn fine tuning. Started with falcon 7B instruct LLM as my base LLM and want to fine tune this with open assistant instruct dataset. I have 2080 Ti with 11G VRAM. So I am using 4 bit quantization and Lora.</p>
<p>These are the experiments I did so far:</p>
<p>1&gt; I trained with SFT trainer from hugging face for 25000 epochs, the loss decreased from 1.8 to 0.7. Below is the entire code I am using for training.</p>
<pre><code>import torch, einops
from datasets import load_dataset
from peft import LoraConfig
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    AutoTokenizer,
    TrainingArguments
)
from peft.tuners.lora import LoraLayer

from trl import SFTTrainer


def create_and_prepare_model():
    compute_dtype = getattr(torch, &quot;float16&quot;)

    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type=&quot;nf4&quot;,
        bnb_4bit_compute_dtype=compute_dtype,
        bnb_4bit_use_double_quant=True,
    )

    model = AutoModelForCausalLM.from_pretrained(
        &quot;tiiuae/falcon-7b-instruct&quot;, quantization_config=bnb_config, device_map={&quot;&quot;: 0}, trust_remote_code=True
    )

    peft_config = LoraConfig(
        lora_alpha=16,
        lora_dropout=0.1,
        r=64,
        bias=&quot;none&quot;,
        task_type=&quot;CAUSAL_LM&quot;,
        target_modules=[
            &quot;query_key_value&quot;
        ],
    )

    tokenizer = AutoTokenizer.from_pretrained(&quot;tiiuae/falcon-7b-instruct&quot;, trust_remote_code=True)
    tokenizer.pad_token = tokenizer.eos_token

    return model, peft_config, tokenizer


training_arguments = TrainingArguments(
    output_dir=&quot;./results_falcon-7b-instruct-new&quot;,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=10,
    optim=&quot;paged_adamw_32bit&quot;,
    save_steps=5,
    logging_steps=10,
    learning_rate=2e-4,
    fp16=True,
    max_grad_norm=0.3,
    max_steps=20,
    warmup_ratio=0.03,
    # group_by_length=True,
    lr_scheduler_type=&quot;constant&quot;,
)

model, peft_config, tokenizer = create_and_prepare_model()
model.config.use_cache = False
dataset = load_dataset(&quot;timdettmers/openassistant-guanaco&quot;, split=&quot;train&quot;)

trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    peft_config=peft_config,
    dataset_text_field=&quot;text&quot;,
    max_seq_length=512,
    tokenizer=tokenizer,
    args=training_arguments,
    packing=True,
)

trainer.train()
trainer.save_model(&quot;falcon-instruct-7b-4bit-openassist-latest-new&quot;)
model.config.to_json_file(&quot;falcon-instruct-7b-4bit-openassist-latest-new/config.json&quot;)

</code></pre>
<p>took about 53 hours. But the model just spits out gibberish when asked for a simple question like &quot;how are you?&quot;</p>
<p>2&gt; 300 epochs,  loss went down from 1.8 to 1.5 but the model still spits out gibberish.</p>
<p>3&gt; 40 epochs, loss went down from 1.8 to 1.7 but the model still spits out gibberish.</p>
<p>Any pointers that could give me a head start? Please suggest. Any open source code to do something similar will be greatly appreciated. Thanks a lot.</p>
","huggingface"
"76536668","How does huggingface/sentencetransformers figure out a model's input/output shapes?","2023-06-23 01:54:49","77268819","3","1005","<huggingface><sentence-transformers>","<p>Using the <a href=""https://huggingface.co/sentence-transformers"" rel=""nofollow noreferrer"">https://huggingface.co/sentence-transformers</a> python package, I'm able to just specify a repo/model, and everything <em>just works</em>. However, when I try to consume a model with .NET/ONNX, I have to specify the <code>input_ids</code> max length, which for this model, <a href=""https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2"" rel=""nofollow noreferrer"">https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2</a>, the documentation says is 256, but sentence-transformers seems to return up to 512 tokens. And I have to manually specify the output size, which the documentation says is 384. And of course I have to know the tokenizer to use as well.</p>
<p>I did try to look at the model with <a href=""https://netron.app/"" rel=""nofollow noreferrer"">https://netron.app/</a>, but it just reports -1,-1 for all the shapes.</p>
<p>Is the python code making an API call to get all this info? I'm asking because I'd like to maybe try to replicate what it does in C#. Thanks!</p>
","huggingface"
"76533716","assert self.ctx is not None AssertionError - TheBloke/Manticore-13B-GGML","2023-06-22 15:59:32","","2","727","<python><huggingface>","<p>So i found this repo, and i've been trying to make it work in google colab.
<a href=""https://github.com/ecliipt/personal-assistant/tree/main"" rel=""nofollow noreferrer"">https://github.com/ecliipt/personal-assistant/tree/main</a></p>
<p>i am using the Manticore-13B.ggmlv2.q5_1.bin model, i've already tried using ggmlv3 but got the same error.
i'm using torch 2.0.1 on cpu and i also changed some things from the original repo's code.</p>
<p>here's the error that i get when running retrievalQA.py (py 3.10.12):</p>
<pre><code>Loading the Manticore-13B.ggmlv2.q5_1.bin model...
llama.cpp: loading model from models/manticore-13b/Manticore-13B.ggmlv2.q5_1.bin
error loading model: unknown (magic, version) combination: 4f44213c, 50595443; is this really a GGML file?
llama_init_from_file: failed to load model
Traceback (most recent call last):
  File &quot;/content/personal-assistant/retrievalQA.py&quot;, line 61, in &lt;module&gt;
    main()
  File &quot;/content/personal-assistant/retrievalQA.py&quot;, line 40, in main
    llm = load_local_model(model_path, provider='llamacpp')   
  File &quot;/content/personal-assistant/retrievalQA.py&quot;, line 28, in load_local_model
    llm = LlamaLLM(model_path, n_gpu_layers=n_gpu_layers, 
  File &quot;/content/personal-assistant/pa/llm/llamacpp.py&quot;, line 20, in __init__
    self.model = Llama(model_path=model_path, 
  File &quot;/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py&quot;, line 162, in __init__
    assert self.ctx is not None
AssertionError
</code></pre>
<p>but if i try with older versions of llama-cpp-python like 0.1.25 (running retrievalQA.py):</p>
<pre><code>TypeError: Llama.__init__() got an unexpected keyword argument 'n_gpu_layers'
</code></pre>
<p>i'm also always running into other error when i run inject.py:</p>
<pre><code>load INSTRUCTOR_Transformer
max_seq_length  512
Traceback (most recent call last):
  File &quot;/content/personal-assistant/inject.py&quot;, line 66, in &lt;module&gt;
    main()
  File &quot;/content/personal-assistant/inject.py&quot;, line 59, in main
    db = Chroma.from_documents(texts, instructor_embeddings, 
  File &quot;/usr/local/lib/python3.10/dist-packages/langchain/vectorstores/chroma.py&quot;, line 435, in from_documents
    return cls.from_texts(
  File &quot;/usr/local/lib/python3.10/dist-packages/langchain/vectorstores/chroma.py&quot;, line 403, in from_texts
    chroma_collection.add_texts(texts=texts, metadatas=metadatas, ids=ids)
  File &quot;/usr/local/lib/python3.10/dist-packages/langchain/vectorstores/chroma.py&quot;, line 148, in add_texts
    embeddings = self._embedding_function.embed_documents(list(texts))
  File &quot;/usr/local/lib/python3.10/dist-packages/langchain/embeddings/huggingface.py&quot;, line 158, in embed_documents
    embeddings = self.client.encode(instruction_pairs, **self.encode_kwargs)
  File &quot;/usr/local/lib/python3.10/dist-packages/InstructorEmbedding/instructor.py&quot;, line 524, in encode
    if isinstance(sentences[0],list):
IndexError: list index out of range
</code></pre>
<p>i've been searching but i could not find a solution until now. llamaCpp and torch versions, tried with ggmlv2 and 3, both give me those errors.</p>
<p>and thats about it, thanks :)</p>
","huggingface"
"76522114","Llama-cpp-python on AWS Sagemaker - Failed building wheel for llama-cpp-python","2023-06-21 10:05:25","","1","4943","<python><c++><amazon-sagemaker><huggingface><langchain>","<p>I am using AWS Sagemaker Notebook Instances to interact with LLaMA models because Colab Pro only offers 15GB GPU RAM and always runs out of memory (CUDA Out of Memory), whereas my sagemaker notebook is a ml.g5.2xlarge instance or a ml.g4dn.2xlarge instance and should not have any problem with that. However, when I run this command:</p>
<pre><code>!pip install llama-cpp-python==0.1.48
</code></pre>
<p>I get the following error message (output is 130 lines, so I condensed it to most relevant parts)</p>
<pre><code>Building wheel for llama-cpp-python (pyproject.toml) did not run successfully.
exit code: 1 [130 lines of output  note: This error originates from a subprocess, and 
is likely not a problem with pip.
ERROR: Failed building wheel for llama-cpp-python
Failed to build llama-cpp-python
ERROR: Could not build wheels for llama-cpp-python, which is required to install 
pyproject.toml-based projects
</code></pre>
","huggingface"
"76521326","Hugging Face Transformers - trust_remote_code not working","2023-06-21 08:22:40","","2","7376","<python><huggingface-transformers><azure-machine-learning-service><huggingface><falcon>","<p>I am currently working on a notebook to run falcon-7b-instruct myself. I am using a notebook in Azure Machine Learning Studio for that. The code I use for running Falcon is from Hugging Face.</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer
import transformers
import torch

model = &quot;tiiuae/falcon-7b-instruct&quot;

tokenizer = AutoTokenizer.from_pretrained(model)
pipeline = transformers.pipeline(
    &quot;text-generation&quot;,
    model=model,
    tokenizer=tokenizer,
    torch_dtype=torch.bfloat16,
    device_map=&quot;auto&quot;,
    trust_remote_code=True
)
</code></pre>
<p>However, when I run the code, even though <code>trust_remote_code</code> is set to True, I get the following error.</p>
<pre><code>ValueError: Loading tiiuae/falcon-7b-instruct requires you to execute the configuration file in that repo on your local machine. Make sure you have read the code there to avoid malicious use, then set the option `trust_remote_code=True` to remove this error.
</code></pre>
<p>Is there perhaps an environment variable I have to set in order for remote code execution to work? I wasn't able to find anything about this error in the transformers documentation.</p>
","huggingface"
"76516579","AttributeError: 'AcceleratorState' object has no attribute 'distributed_type'","2023-06-20 16:07:26","","4","5214","<huggingface><accelerate>","<pre><code>import transformers
from datasets import load_dataset
import tensorflow as tf

tokenizer = transformers.AutoTokenizer.from_pretrained('roberta-base')

df = load_dataset('csv', data_files={'train':'FinalDatasetTrain.csv', 'test':'FinalDatasetTest.csv'})

def tokenize_function(examples):
    return tokenizer(examples[&quot;text&quot;], truncation=True)

tokenized_datasets = df.map(tokenize_function, batched=True)
data_collator = transformers.DataCollatorWithPadding(tokenizer=tokenizer)

model = transformers.AutoModelForSequenceClassification.from_pretrained('roberta-base', num_labels=7)

training_args = transformers.TFTrainingArguments(
    output_dir=&quot;./results&quot;,
    num_train_epochs=2,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=16,
    save_strategy='epoch',
    evaluation_strategy=&quot;epoch&quot;,
    logging_dir=&quot;./logs&quot;,
)

trainer = transformers.Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets['train'],
    eval_dataset=tokenized_datasets['test'],
    data_collator=data_collator,
    tokenizer=tokenizer
)

trainer.train()
</code></pre>
<p>When I run this code I get an error saying:</p>
<blockquote>
<p>AttributeError: 'AcceleratorState' object has no attribute 'distributed_type'.</p>
</blockquote>
<p>How do I fix this (I tried both Jupyter notebook and Google Colab)?</p>
","huggingface"
"76503680","Problems while fine tuning facebook/bart-large-mnli","2023-06-19 04:52:00","","0","275","<amazon-sagemaker><huggingface>","<p>I have followed the instructions given  <a href=""https://stackoverflow.com/questions/76213873/how-to-finetune-a-zero-shot-model-for-text-classification/76213874#76213874"">here</a>  to prepare my data.</p>
<p>The shape of my data after transformation is as follows:
<code>{'label': 2,  'input_ids': [0, 44758, 3457, 13, 5, 1263, 829, 31, 5, 1263, 8401, 4001, 438, 34, 5, 511, 7390, 2, 2, 713, 1246, 16, 4287, 92, 3457, 4, 2],  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],  input_sentence': '&lt;s&gt;Create tests for the response received from the response whihc has the following format&lt;/s&gt;&lt;/s&gt;This example is Add new tests.&lt;/s&gt;' }</code></p>
<p>Rest of it is standard Huggingface model creation code:</p>
<pre><code>from sagemaker.huggingface import HuggingFace
distribution = {'smdistributed':{'dataparallel':{ 'enabled': True }}}

model_name='facebook/bart-large-mnli'
# hyperparameters, which are passed into the training job
hyperparameters={#'epochs': 1,
                 #'train_batch_size': 8,
                 'do_train' : True,
                 'do_eval' : True,
                 'model_name':model_name,
                 'task_name': 'mnli',
                 
                 'output_dir': '/opt/ml/model',
                  
                 'overwrite_output_dir' : True
                 }

git_config = {'repo': 'https://github.com/huggingface/transformers.git','branch': 'v4.26.0'}

 
# creates Hugging Face estimator

huggingface_estimator = HuggingFace(
  
    entry_point='run_glue.py',
    source_dir='./examples/pytorch/text-classification',
    instance_type='ml.p3dn.24xlarge',
    instance_count=1,
    role=role,
    git_config=git_config,
    transformers_version='4.26.0',
    pytorch_version='1.13.1',
    py_version='py39',
    hyperparameters = hyperparameters,
        distribution = distribution
)
 

huggingface_estimator.fit({'train': training_input_path, 'test': testing_input_path})
</code></pre>
<p><strong>The problem is it generates a 158GB model.tar.gz whihc takes 5 hours to upload to s3!!</strong></p>
<p>Any solutions?</p>
<p>I have tried adding:</p>
<pre><code>    save_strategy= &quot;no&quot;,

    save_total_limit=1,
    load_best_model_at_end=True
</code></pre>
<p>to the model params, but it still does the same thing.</p>
","huggingface"
"76494559","Passing Dicts using Pointers in Python HuggingFace","2023-06-17 03:16:29","76494573","1","64","<python><pointers><huggingface-transformers><huggingface>","<p>AFAIK, in Python objects are passed by reference, then why do HuggingFace keeps using pointers to pass objects? Example snippet below taken from the tutorial at this link: <a href=""https://huggingface.co/learn/nlp-course/chapter3/4?fw=pt"" rel=""nofollow noreferrer"">https://huggingface.co/learn/nlp-course/chapter3/4?fw=pt</a></p>
<pre><code>raw_inputs = [
    &quot;I've been waiting for a HuggingFace course my whole life.&quot;,
    &quot;I hate this so much!&quot;,
]
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=&quot;pt&quot;)
print(inputs)

from transformers import AutoModel

checkpoint = &quot;distilbert-base-uncased-finetuned-sst-2-english&quot;
model = AutoModel.from_pretrained(checkpoint)

outputs = model(**batch)  #  &lt;-- What does this even mean? 
print(outputs.loss, outputs.logits.shape)
</code></pre>
","huggingface"
"76492422","Modify the data type of last layer of transformer model - llama huggingface","2023-06-16 17:28:08","","1","417","<python><torch><huggingface>","<p>I'm using torch to load the <code>decapoda-research/llama-7b-hf</code> from hf, which is a <code>'transformers.models.llama.modeling_llama.LlamaForCausalLM'</code> and the only way I manage to load it is by using <code>load_in_8bit</code>.</p>
<p>because of that, when I try to do inferences with the model I get:</p>
<pre><code>RuntimeError: &quot;log_softmax_lastdim_kernel_impl&quot; not implemented for 'Half'
</code></pre>
<p>How can I change the number of bits of the last layer of the model to prevent that error?</p>
","huggingface"
"76489159","Use LLM weights from hugging face repository","2023-06-16 10:11:51","","0","289","<python><pytorch><huggingface>","<p>So I want to use the Large Language Model from huggingface, for example <a href=""https://huggingface.co/tiiuae/falcon-7b/tree/main"" rel=""nofollow noreferrer"">falcon</a>. So I know you can use</p>
<pre><code>from transformers import  AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(&quot;tiiuae/falcon-7b&quot;,
 trust_remote_code=True)
</code></pre>
<p>to load the model, but I want to clone the repository or download the weight, so I can use the weight on other virtual enviroment to. But currently I run to this error, when I try to load the model from the cloned repostory</p>
<pre><code>model = AutoModelForCausalLM.from_pretrained( './falcon-7b/')
</code></pre>
<p>I have this error:</p>
<pre><code>945 has_remote_code = &quot;auto_map&quot; in config_dict and &quot;AutoConfig&quot; in config_dict[&quot;auto_map&quot;]
    946 has_local_code = &quot;model_type&quot; in config_dict and config_dict[&quot;model_type&quot;] in CONFIG_MAPPING
--&gt; 947 trust_remote_code = resolve_trust_remote_code(
    948     trust_remote_code, pretrained_model_name_or_path, has_local_code, has_remote_code
    949 )
    951 if has_remote_code and trust_remote_code:
...
--&gt; 535     signal.signal(signal.SIGALRM, _raise_timeout_error)
    536     signal.alarm(TIME_OUT_REMOTE_CODE)
    537     while trust_remote_code is None:

AttributeError: module 'signal' has no attribute 'SIGALRM'`
</code></pre>
<p>Do I have to do anything with the weights first?</p>
","huggingface"
"76476765","Training LLM to perform text classification","2023-06-14 18:58:22","","2","849","<nlp><huggingface><large-language-model>","<p>I am trying to perform text classification using GPTNeo, using the tweet_eval dataset from huggingface. I am following this example <a href=""https://huggingface.co/docs/transformers/tasks/sequence_classification"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/tasks/sequence_classification</a>, but there is some error. I am a beginner at LLMs and it will be very helpful if someone can help me solve the issue. Thanks in advance. This is my code:</p>
<pre><code>from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer
import datasets
import torch as t
from transformers import DataCollatorWithPadding
import evaluate
import numpy as np

dataset = datasets.load_dataset(&quot;tweet_eval&quot;,&quot;emotion&quot;)

x_train = dataset[&quot;train&quot;][&quot;text&quot;]
y_train = dataset[&quot;train&quot;][&quot;label&quot;]

x_test = dataset[&quot;test&quot;][&quot;text&quot;]
y_test = dataset[&quot;test&quot;][&quot;label&quot;]

def load_LLM(llm, device):
    num_labels = 4
    id2label = {0: &quot;Anger&quot;, 1: &quot;Joy&quot;, 2: &quot;Optimism&quot;, 3: &quot;Sadness&quot;}
    label2id = {&quot;Anger&quot;: 0, &quot;Joy&quot;: 1, &quot;Optimism&quot;: 2, &quot;Sadness&quot;:3}
    model = AutoModelForSequenceClassification.from_pretrained(llm,num_labels=num_labels,id2label=id2label, label2id=label2id)
    model.to(device)
    tokenizer = AutoTokenizer.from_pretrained(llm)
    return model, tokenizer

llm = &quot;EleutherAI/gpt-neo-2.7B&quot;
device = t.device('cuda' if t.cuda.is_available() else 'cpu')
model,tokenizer = load_LLM(llm,device)

tokenizer.add_special_tokens({'pad_token': '[PAD]'})
tokenizer.pad_token = '[PAD]'
train_inputs = tokenizer(x_train, truncation=True, padding=True)
test_inputs = tokenizer(x_test, truncation=True, padding=True)

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

accuracy = evaluate.load(&quot;accuracy&quot;)
def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return accuracy.compute(predictions=predictions, references=labels)

training_args = TrainingArguments(
    output_dir=&quot;my_awesome_model&quot;,
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=2,
    weight_decay=0.01,
    evaluation_strategy=&quot;epoch&quot;,
    save_strategy=&quot;epoch&quot;,
    load_best_model_at_end=True,
    push_to_hub=True
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_inputs,
    eval_dataset=test_inputs,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics
)

trainer.train()
</code></pre>
<p>I am getting this error:</p>
<pre><code>type here---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
Cell In[18], line 1
----&gt; 1 trainer.train()

File ~\anaconda3\envs\pt\lib\site-packages\transformers\trainer.py:1664, in Trainer.train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)
   1659     self.model_wrapped = self.model
   1661 inner_training_loop = find_executable_batch_size(
   1662     self._inner_training_loop, self._train_batch_size, args.auto_find_batch_size
   1663 )
-&gt; 1664 return inner_training_loop(
   1665     args=args,
   1666     resume_from_checkpoint=resume_from_checkpoint,
   1667     trial=trial,
   1668     ignore_keys_for_eval=ignore_keys_for_eval,
   1669 )

File ~\anaconda3\envs\pt\lib\site-packages\transformers\trainer.py:1909, in Trainer._inner_training_loop(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)
   1906     rng_to_sync = True
   1908 step = -1
-&gt; 1909 for step, inputs in enumerate(epoch_iterator):
   1910     total_batched_samples += 1
   1911     if rng_to_sync:

File ~\anaconda3\envs\pt\lib\site-packages\torch\utils\data\dataloader.py:633, in _BaseDataLoaderIter.__next__(self)
    630 if self._sampler_iter is None:
    631     # TODO(https://github.com/pytorch/pytorch/issues/76750)
    632     self._reset()  # type: ignore[call-arg]
--&gt; 633 data = self._next_data()
    634 self._num_yielded += 1
    635 if self._dataset_kind == _DatasetKind.Iterable and \
    636         self._IterableDataset_len_called is not None and \
    637         self._num_yielded &gt; self._IterableDataset_len_called:

File ~\anaconda3\envs\pt\lib\site-packages\torch\utils\data\dataloader.py:677, in _SingleProcessDataLoaderIter._next_data(self)
    675 def _next_data(self):
    676     index = self._next_index()  # may raise StopIteration
--&gt; 677     data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
    678     if self._pin_memory:
    679         data = _utils.pin_memory.pin_memory(data, self._pin_memory_device)

File ~\anaconda3\envs\pt\lib\site-packages\torch\utils\data\_utils\fetch.py:54, in _MapDatasetFetcher.fetch(self, possibly_batched_index)
     52 else:
     53     data = self.dataset[possibly_batched_index]
---&gt; 54 return self.collate_fn(data)

File ~\anaconda3\envs\pt\lib\site-packages\transformers\trainer_utils.py:704, in RemoveColumnsCollator.__call__(self, features)
    702 def __call__(self, features: List[dict]):
    703     features = [self._remove_columns(feature) for feature in features]
--&gt; 704     return self.data_collator(features)

File ~\anaconda3\envs\pt\lib\site-packages\transformers\data\data_collator.py:249, in DataCollatorWithPadding.__call__(self, features)
    248 def __call__(self, features: List[Dict[str, Any]]) -&gt; Dict[str, Any]:
--&gt; 249     batch = self.tokenizer.pad(
    250         features,
    251         padding=self.padding,
    252         max_length=self.max_length,
    253         pad_to_multiple_of=self.pad_to_multiple_of,
    254         return_tensors=self.return_tensors,
    255     )
    256     if &quot;label&quot; in batch:
    257         batch[&quot;labels&quot;] = batch[&quot;label&quot;]

File ~\anaconda3\envs\pt\lib\site-packages\transformers\tokenization_utils_base.py:2966, in PreTrainedTokenizerBase.pad(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)
   2962 # The model's main input name, usually `input_ids`, has be passed for padding
   2963 if self.model_input_names[0] not in encoded_inputs:
   2964     raise ValueError(
   2965         &quot;You should supply an encoding or a list of encodings to this method &quot;
-&gt; 2966         f&quot;that includes {self.model_input_names[0]}, but you provided {list(encoded_inputs.keys())}&quot;
   2967     )
   2969 required_input = encoded_inputs[self.model_input_names[0]]
   2971 if required_input is None or (isinstance(required_input, Sized) and len(required_input) == 0):

AttributeError: 'list' object has no attribute 'keys'
</code></pre>
<p>I was trying to perform text classification and wanted to fine tune the model before using it to make predictions.</p>
","huggingface"
"76471292","How to finetune an LLM model on your own codebase?","2023-06-14 07:54:51","","6","1759","<code-generation><huggingface><large-language-model>","<p>I have 10 code repositories in Javascript (VueJS) (Each repository corresponds to 1 Theme)</p>
<p>I want to train an LLM model on these 10 code repositories to generate new themes using prompts.</p>
<p>The LLM model takes the context of 10 code repositories as a reference (since the file structure is similar for all repositories)</p>
<p>I'm a complete beginner with LLMs and ML.</p>
<p>How to finetune an LLM model on my codebase?</p>
","huggingface"
"76461859","LMM Fine Tuning - Supervised Fine Tuning Trainer (SFTTrainer) vs transformers Trainer","2023-06-13 05:15:06","","2","3113","<huggingface-transformers><huggingface><fine-tuning><large-language-model>","<p>When should one opt for the Supervised Fine Tuning Trainer (SFTTrainer) instead of the regular Transformers Trainer when it comes to instruction fine-tuning for Language Models (LLMs)? From what I gather, the regular Transformers Trainer typically refers to unsupervised fine-tuning, often utilized for tasks such as Input-Output schema formatting after conducting supervised fine-tuning. There seem to be various examples of fine-tuning tasks with similar characteristics, but with some employing the SFTTrainer and others using the regular Trainer. Which factors should be considered in choosing between the two approaches?</p>
<p>I looking for Fine Tuning a LLM for generating json to json transformation (matching texts in json) using huggingface and trl libraries.</p>
","huggingface"
"76459041","Loading a safetensors format model using Hugging Face Transformers","2023-06-12 17:35:52","","2","1779","<python><huggingface-transformers><huggingface><safe-tensors>","<p>I try to load the 'notstoic/pygmalion-13b-4bit-128g' model using Hugging Face's Transformers library. I am encountering an issue when trying to load the model, which is saved in the new safetensors format.</p>
<p>Here's the code I'm using:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import LlamaForCausalLM, LlamaTokenizer

tokenizer = LlamaTokenizer.from_pretrained(&quot;path/to/model&quot;)
model = LlamaForCausalLM.from_pretrained(&quot;path/to/model&quot;, use_safetensors=True)
</code></pre>
<p>However, this code results in the following error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/Users/maxhager/Projects2023/nsfw/model_run.py&quot;, line 4, in &lt;module&gt;
    model = LlamaForCausalLM.from_pretrained(&quot;path/to/model&quot;, use_safetensors=True)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/Users/maxhager/.virtualenvs/nsfw/lib/python3.11/site-packages/transformers/modeling_utils.py&quot;, line 2449, in from_pretrained
    raise EnvironmentError(
OSError: Error no file named pytorch_model.bin, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory path/to/model.
</code></pre>
<p>I'm confused by this error because I've set use_safetensors=True, as the model is stored in safetensors format. In the model directory (path/to/model), I have the following files:</p>
<ul>
<li>4bit-128g.safetensors</li>
<li>config.json</li>
<li>generation_config.json</li>
<li>pytorch_model.bin.index.json</li>
<li>special_tokens_map.json</li>
<li>tokenizer.json</li>
<li>tokenizer.model</li>
<li>tokenizer_config.json</li>
</ul>
<p>It seems like the from_pretrained() function is not recognizing the safetensors format and instead is looking for the typical file formats (pytorch_model.bin, tf_model.h5, etc).</p>
<p>I would appreciate if anyone could provide guidance on why this is happening and how I can successfully load this model.</p>
","huggingface"
"76453542","Need Guidance for deploying Huggingface model with Flask on Kubernetes with Ingress and GPU support","2023-06-12 03:47:51","","1","168","<docker><kubernetes><flask><huggingface><mlops>","<p>So, I have developed a <strong>chatbot based application</strong> using multiple services (used multiple NodeJs servers + flask servers) dockerize and deployed as kubernetes pod and used minikube Ingress-Nginx Controller. The problem I am facing is that my Chatbot service requires <strong>GPU support</strong> which I am unable to provide with minikube. Is there any way or approach through which I can use GPU for services and which requires minimum changes in my current architecture...</p>
<p>Here, is the better explanation of my current architecture...</p>
<ol>
<li><p>Client - React Service with Server side rendering</p>
</li>
<li><p>Auth Service - for authentication and creating session. It's NodeJs app service..</p>
</li>
<li><p>Profanity Service - Santitize Data.. Flask App and no need for GPU here...</p>
</li>
<li><p>Communication Service - Store Data and Responses and handles Communication with different services - NodeJs app service</p>
</li>
<li><p>Chatbot Service - Huggingface LLM Model with Flask App. For Generating response. Here, I required to use GPU...</p>
</li>
</ol>
<p>Note: All services are deployed as Kubernetes pod in my local minikube with Ingress-Nginx Controller...</p>
<p><a href=""https://i.sstatic.net/IQ3Ve.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/IQ3Ve.jpg"" alt=""enter image description here"" /></a></p>
","huggingface"
"76452425","safetensors issue with Huggingface embeddings","2023-06-11 20:36:07","","1","1339","<python><jupyter><huggingface-transformers><huggingface><sentence-transformers>","<p>Running into the following:</p>
<p>local Win10 Anaconda Setup in jupyter notebook.</p>
<p><code>!pip install safetensors Requirement already satisfied: safetensors in ...\anaconda3\envs\myenv\lib\site-packages (0.3.1)</code></p>
<p>Installed safetensors
Trying to load Huggingface embeddings:</p>
<pre><code>from langchain.embeddings import HuggingFaceEmbeddings
embeddings = HuggingFaceEmbeddings()
</code></pre>
<p>Getting this error constantly. Anyone has an idea?</p>
<pre><code>Failed to import transformers.models.mpnet.modeling_mpnet because of the following error (look up to see its traceback):%0D%0ANo module named 'safetensors._safetensors_rust'
</code></pre>
<p>Already tried to install rust compiler, installed from wheel, all with same problem.
Tried the same notebook in google colab and it worked without any issues.</p>
","huggingface"
"76448287","How can i solve ImportError: Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1` when using Huggingface's TrainArguments?","2023-06-10 21:51:03","76452964","24","31363","<python><nlp><importerror><huggingface-transformers><huggingface>","<p>I'm using the <code>transformers</code> library in Google colab, and
When i am using TrainingArguments from transformers library i'm getting Import error with this  code:</p>
<pre><code>from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir = &quot;/content/our-model&quot;,
    learning_rate=2e-5,
    per_device_train_batch_size= 64,
    per_device_eval_batch_size = 16,
    num_train_epochs = 2,
    weight_decay = 0.01,
    evaluation_strategy = &quot;epoch&quot;,
    save_strategy = &quot;epoch&quot;,
    load_best_model_at_end = True,
    push_to_hub = False
)
</code></pre>
<p>This is the error i'm getting:</p>
<pre><code>&lt;ipython-input-28-0518ea5ff407&gt; in &lt;cell line: 2&gt;()
      1 from transformers import TrainingArguments
----&gt; 2 training_args = TrainingArguments(
      3     output_dir = &quot;/content/our-model&quot;,
      4     learning_rate=2e-5,
      5     per_device_train_batch_size= 64,

4 frames
/usr/local/lib/python3.10/dist-packages/transformers/training_args.py in _setup_devices(self)
   1670         if not is_sagemaker_mp_enabled():
   1671             if not is_accelerate_available(min_version=&quot;0.20.1&quot;):
-&gt; 1672                 raise ImportError(
   1673                     &quot;Using the `Trainer` with `PyTorch` requires `accelerate&gt;=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`&quot;
   1674                 )

ImportError: Using the `Trainer` with `PyTorch` requires `accelerate&gt;=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U 
</code></pre>
<p>I already tried pip install for 0.20.1 version of accelerate and pip install transformers[torch]
and both didn't worked.</p>
","huggingface"
"76446228","Setting padding token as eos token when using DataCollatorForLanguageModeling from HuggingFace","2023-06-10 12:37:18","76453052","1","5886","<pytorch><huggingface-transformers><huggingface-tokenizers><huggingface><huggingface-datasets>","<p>In <a href=""https://huggingface.co/learn/nlp-course/chapter7/6#preparing-the-dataset"" rel=""nofollow noreferrer"">https://huggingface.co/learn/nlp-course/chapter7/6#preparing-the-dataset</a>, there is</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)
</code></pre>
<p>What the tutorial is doing is using a pretrained GPT2 model and its tokenizer and trying to create a dataset for causal language modeling pretraining task.</p>
<p>My question with the above line is that padding token is set to be the eos token. As a result even the original eos tokens will be ignored by the model during training since they will be perceived as padding tokens too.</p>
<p>This would prevent my model from learning to output eos tokens when its generation is over.</p>
<p><strong>How come this is in the tutorials and it is a correct way ?</strong></p>
","huggingface"
"76444412","AttributeError: module 'torch' has no attribute '_utils'","2023-06-10 01:38:59","","4","6196","<pytorch><huggingface-transformers><python-3.10><huggingface>","<p>I'm unable to use HuggingFace (transformers) pretrained model, I'm trying to run the .frompretrained() function, specifically:</p>
<pre><code>from transformers import BertModel, AutoModel
model = BertModel.from_pretrained('bert-base-uncased')
</code></pre>
<p>and I get the following error:</p>
<pre><code>AttributeError: module 'torch' has no attribute '_utils'
</code></pre>
<p>If it helps, I get this when I run it:</p>
<pre><code>╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮
│ in &lt;module&gt;:2                                                                                    │
│                                                                                                  │
│   1 from transformers import BertModel, AutoModel                                                │
│ ❱ 2 model = BertModel.from_pretrained('bert-base-uncased')                                       │
│   3                                                                                              │
│                                                                                                  │
│ /Users/gpt_env/lib/python3.10/site-packages/transformers/modeling_utils.py:2600  │
│ in from_pretrained                                                                               │
│                                                                                                  │
│   2597 │   │   if from_pt:                                                                       │
│   2598 │   │   │   if not is_sharded and state_dict is None:                                     │
│   2599 │   │   │   │   # Time to load the checkpoint                                             │
│ ❱ 2600 │   │   │   │   state_dict = load_state_dict(resolved_archive_file)                       │
│   2601 │   │   │                                                                                 │
│   2602 │   │   │   # set dtype to instantiate the model under:                                   │
│   2603 │   │   │   # 1. If torch_dtype is not None, we use that dtype                            │
│                                                                                                  │
│ /Users/gpt_env/lib/python3.10/site-packages/transformers/modeling_utils.py:446   │
│ in load_state_dict                                                                               │
│                                                                                                  │
│    443 │   &quot;&quot;&quot;                                                                                   │
│    444 │   if checkpoint_file.endswith(&quot;.safetensors&quot;) and is_safetensors_available():           │
│    445 │   │   # Check format of the archive                                                     │
│ ❱  446 │   │   with safe_open(checkpoint_file, framework=&quot;pt&quot;) as f:                             │
│    447 │   │   │   metadata = f.metadata()                                                       │
│    448 │   │   if metadata.get(&quot;format&quot;) not in [&quot;pt&quot;, &quot;tf&quot;, &quot;flax&quot;]:                            │
│    449 │   │   │   raise OSError(                                                                │
│                                                                                                  │
│ /Users/gpt_env/lib/python3.10/site-packages/torch/storage.py:779 in from_file    │
│                                                                                                  │
│   776 │   │   untyped_storage: UntypedStorage = UntypedStorage.from_file(                        │
│   777 │   │   │   filename,                                                                      │
│   778 │   │   │   shared,                                                                        │
│ ❱ 779 │   │   │   size * torch._utils._element_size(cls.dtype))                                  │
│   780 │   │   storage = cls(wrap_storage=untyped_storage)                                        │
│   781 │   │   return storage                                                                     │
│   782          
</code></pre>
<p>I've tried other models using the same from_pretrained() function and a similar error happens.</p>
<p>Any help is much appreciated!</p>
","huggingface"
"76441777","huggingface evaluate function use multiple labels","2023-06-09 15:41:02","","1","387","<python><deep-learning><pytorch><huggingface>","<p>I have two sentence that combine with encode_plus function and i want to done the NLI task with finetuning a BERT base model.</p>
<p>I want a metric name for huggingface evaluator function to evaluate multiple labels.</p>
<p>i used from this code:</p>
<pre class=""lang-py prettyprint-override""><code>metric = evaluate.combine([&quot;accuracy&quot;, &quot;f1&quot;, &quot;precision&quot;, &quot;recall&quot;])
metrics = metric.compute(predictions=[0,1,1,2], references=[0,2,1,0])
</code></pre>
<p>And got this result:</p>
<pre class=""lang-py prettyprint-override""><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[31], line 2
      1 metric = evaluate.combine([&quot;accuracy&quot;, &quot;f1&quot;, &quot;precision&quot;, &quot;recall&quot;])
----&gt; 2 metrics = metric.compute(predictions=[0,1,1,2], references=[0,2,1,0])
      4 metrics

File ~/anaconda3/envs/NER/lib/python3.10/site-packages/evaluate/module.py:862, in CombinedEvaluations.compute(self, predictions, references, **kwargs)
    860     batch = {&quot;predictions&quot;: predictions, &quot;references&quot;: references, **kwargs}
    861     batch = {input_name: batch[input_name] for input_name in evaluation_module._feature_names()}
--&gt; 862     results.append(evaluation_module.compute(**batch))
    864 return self._merge_results(results)

File ~/anaconda3/envs/NER/lib/python3.10/site-packages/evaluate/module.py:444, in EvaluationModule.compute(self, predictions, references, **kwargs)
    442 inputs = {input_name: self.data[input_name] for input_name in self._feature_names()}
    443 with temp_seed(self.seed):
--&gt; 444     output = self._compute(**inputs, **compute_kwargs)
    446 if self.buf_writer is not None:
    447     self.buf_writer = None

File ~/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--f1/0ca73f6cf92ef5a268320c697f7b940d1030f8471714bffdb6856c641b818974/f1.py:127, in F1._compute(self, predictions, references, labels, pos_label, average, sample_weight)
    126 def _compute(self, predictions, references, labels=None, pos_label=1, average=&quot;binary&quot;, sample_weight=None):
--&gt; 127     score = f1_score(
    128         references, predictions, labels=labels, pos_label=pos_label, average=average, sample_weight=sample_weight
    129     )
...
   (...)
   1401         UserWarning,
   1402     )

ValueError:

     Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].



</code></pre>
","huggingface"
"76435984","Convert model on Hugging Face website to CoreML","2023-06-08 22:02:51","","1","305","<coreml><huggingface>","<p>I'm moving forward by reading the document here.</p>
<p><a href=""https://github.com/apple/ml-stable-diffusion#converting-models-to-coreml"" rel=""nofollow noreferrer"">https://github.com/apple/ml-stable-diffusion#converting-models-to-coreml</a></p>
<p>But I am getting error in this category.</p>
<p>I found this solution on the internet, but I couldn't find how to switch to <code>diffusers</code> version <code>0.15.1</code>.</p>
<p>Solution: <a href=""https://github.com/apple/ml-stable-diffusion/issues/173"" rel=""nofollow noreferrer"">https://github.com/apple/ml-stable-diffusion/issues/173</a></p>
<p><strong>Converting Models to Core ML</strong></p>
<p>Step 1: Create a Python environment and install dependencies:</p>
<p>conda create -n coreml_stable_diffusion python=3.8 -y
conda activate coreml_stable_diffusion
cd /path/to/cloned/ml-stable-diffusion/repository
pip install -e .</p>
<p>I am running the above code one by one.</p>
<p><strong>1. conda create -n coreml_stable_diffusion python=3.8 -y</strong></p>
<p><a href=""https://i.sstatic.net/yQrzN.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/yQrzN.png"" alt=""enter image description here"" /></a></p>
<p><strong>2. conda activate coreml_stable_diffusion</strong></p>
<p><a href=""https://i.sstatic.net/6dgG0.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/6dgG0.png"" alt=""enter image description here"" /></a></p>
<p><strong>3. cd /path/to/cloned/ml-stable-diffusion/repository</strong></p>
<p><a href=""https://i.sstatic.net/eP2k5.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/eP2k5.png"" alt=""enter image description here"" /></a></p>
<p><strong>4. pip install -e .</strong></p>
<p><a href=""https://i.sstatic.net/8M1Oc.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/8M1Oc.jpg"" alt=""enter image description here"" /></a></p>
<p><strong>5. python -m python_coreml_stable_diffusion.torch2coreml --convert-unet --convert-text-encoder --convert-vae-decoder --convert-safety-checker -o &lt; output-mlpackages-directory &gt;</strong></p>
<p><a href=""https://i.sstatic.net/uiuti.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/uiuti.jpg"" alt=""enter image description here"" /></a></p>
","huggingface"
"76435798","How to stop a tokenizer not split words further?","2023-06-08 21:27:26","","0","1066","<python><nlp><huggingface-tokenizers><huggingface>","<p>In the following code below, the tokenizer is splitting some of the words. Is it the property of the model or can I somehow force it not to split the words? I am using these tokens for inference to model.  Even after passing
<code>do_basic_tokenize: False</code>, it is still splitting the words.</p>
<pre><code>from transformers import AutoTokenizer
text = &quot;Patient John Doe visited the hospital on 01/05/2023 with complaints of chest pain.&quot;
tokenizer = AutoTokenizer.from_pretrained(&quot;obi/deid_bert_i2b2&quot;, tokenizer_args={&quot;do_basic_tokenize&quot;: False})
tokens = tokenizer.tokenize(text, truncation=True, padding=True, return_tensors=&quot;pt&quot;)
model = AutoModelForTokenClassification.from_pretrained(&quot;obi/deid_bert_i2b2&quot;)
tokens
</code></pre>
<p>Output:</p>
<pre><code>['Pat',
 '##ient',
 'John',
 'Do',
 '##e',
 'visited',
 'the',
 'hospital',
 'on',
 '01',
 '/',
 '05',
 '/',
 '202',
 '##3',
 'with',
 'complaints',
 'of',
 'chest',
 'pain',
 '.']
</code></pre>
<p>Is there any efficient way/package to combine the  tokens with hashes with its predecessor or successor in the text?</p>
","huggingface"
"76434311","How to get the logits of the model with a text classification pipeline from HuggingFace?","2023-06-08 17:26:56","76435401","8","2999","<python><huggingface-transformers><sentiment-analysis><huggingface><large-language-model>","<p>I need to use <code>pipeline</code> in order to get the tokenization and inference from the <code>distilbert-base-uncased-finetuned-sst-2-english</code> model over my dataset.</p>
<p>My data is a list of sentences, for recreation purposes we can assume it is:</p>
<p><code>texts = [&quot;this is the first sentence&quot;, &quot;of my data.&quot;, &quot;In fact, thats not true,&quot;, &quot;but we are going to assume it&quot;, &quot;is&quot;]</code></p>
<p>Before using <code>pipeline</code>, I was getting the logits from the model outputs like this:</p>
<pre><code>with torch.no_grad():
     logits = model(**tokenized_test).logits
</code></pre>
<p>Now I have to use pipeline, so this is the way I'm getting the model's output:</p>
<pre><code> selected_model = &quot;distilbert-base-uncased-finetuned-sst-2-english&quot;
 tokenizer = AutoTokenizer.from_pretrained(selected_model)
 model = AutoModelForSequenceClassification.from_pretrained(selected_model, num_labels=2)
 classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)
 print(classifier(text))
</code></pre>
<p>which gives me:</p>
<p><code>[{'label': 'POSITIVE', 'score': 0.9746173024177551}, {'label': 'NEGATIVE', 'score': 0.5020197629928589}, {'label': 'NEGATIVE', 'score': 0.9995120763778687}, {'label': 'NEGATIVE', 'score': 0.9802979826927185}, {'label': 'POSITIVE', 'score': 0.9274746775627136}]</code></p>
<p>And I cant get the 'logits' field anymore.</p>
<p>Is there a way to get the <code>logits</code> instead of the <code>label</code> and <code>score</code>? Would a custom pipeline be the best and/or easiest way to do it?</p>
","huggingface"
"76424894","Gradio - Changing background colour in all devices","2023-06-07 15:28:41","","0","765","<css><huggingface><gradio>","<p>I am trying to force Gradio to show a white background colour ALL the time in all browsers. My code below works to force white colour on my desktop (using Firefox) but on mobile it's still showing the typical default Gradio black background. How to change this behaviour to permanently show a white background in all devices? Thank you</p>
<pre><code>
demo = gr.Interface(lambda x:x+x, inputs=gr.Textbox(label='Test'), outputs=gr.Textbox(label='test2'),
        css=&quot;.gradio-container {background-color: white} footer {visibility: hidden}&quot;).launch(share=False)```  
</code></pre>
","huggingface"
"76421521","Download huggingface table / DataFrame?","2023-06-07 08:53:05","76680075","0","621","<dataframe><huggingface><gradio><huggingface-hub>","<p>I was looking at:</p>
<p><a href=""https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard"" rel=""nofollow noreferrer"">https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard</a></p>
<p><a href=""https://i.sstatic.net/i0iK3.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/i0iK3.png"" alt=""enter image description here"" /></a></p>
<p>It's a valuable data table and I wanted to download it in a machine readable format. There is no static file that is rendered out of <a href=""https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard/tree/main"" rel=""nofollow noreferrer"">the repo</a>, but <a href=""https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard/blob/main/app.py"" rel=""nofollow noreferrer"">app.py</a> is executed (as per <a href=""https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard/blob/main/README.md"" rel=""nofollow noreferrer"">README metadata</a>) - which creates a <a href=""https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard/blob/main/app.py#L112"" rel=""nofollow noreferrer"">leaderboard DataFrame</a> out of a separate <a href=""https://huggingface.co/spaces/HuggingFaceH4/lmeh_evaluations"" rel=""nofollow noreferrer"">evaluation repo</a> which seems to be private.</p>
<p>How can one access that DataFrame from the outside?</p>
<hr />
<p>Edit: I tried the the linked docker command fails (somewhat expectedly):</p>
<pre class=""lang-bash prettyprint-override""><code>$ docker run -it -p 7860:7860 \
    --platform=linux/amd64 \
    -e H4_TOKEN=hf_nottherealtokenweorpqweruuoid \
    -e IS_PUBLIC=true \
    registry.hf.space/huggingfaceh4-open-llm-leaderboard:latest \
    python app.py

...

Cloning into '.'...
remote: Repository not found
fatal: repository 'https://huggingface.co/datasets/HuggingFaceH4/lmeh_evaluations/' not found
Error(s) during clone:
git clone failed: exit status 128

</code></pre>
<p>Cf. <a href=""https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard/discussions/50"" rel=""nofollow noreferrer"">https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard/discussions/50</a></p>
","huggingface"
"76403814","What is the best approach to creating a question generation model using GPT and Bert architectures?","2023-06-05 05:58:55","76411286","0","721","<python><open-source><huggingface-transformers><huggingface><gpt-3>","<p>I want to make a question generation model from questions as well as context. Should I make use of GPT based models or Bert Based architectures.</p>
<p>GPT is able to perform the tasks but sometimes returns with vague questions that were not in the context itself. When I made use of WizardLM(7B), I was able to get generalized questions from the context itself which sounded more natural and were nearly to the point when kept within limit of 3.</p>
","huggingface"
"76400601","Look for good ways to prepare customized dataset for training controlnet with huggingface diffusers","2023-06-04 13:15:28","","2","871","<python><pytorch><huggingface><stable-diffusion><fine-tuning>","<p>I want to personally train the controlnet, but I find it inconvenient to prepare the datasets. As I follow the huggingface tutorial available at this link: <a href=""https://huggingface.co/blog/train-your-controlnet"" rel=""nofollow noreferrer"">https://huggingface.co/blog/train-your-controlnet</a>, I believe I should organize the dataset in the huggingface datasets format. My intention is to train the controlnet using various prompt settings and compare the outcomes. However, I realize that I will need to create multiple datasets for each experiment, which is time-consuming and space-inefficient because the images and conditional images in each dataset remain the same.</p>
<p>Should I create multiple datasets that differ solely in the prompt column, or is there a more efficient approach to accomplish this?</p>
","huggingface"
"76397416","How to apply LORAs like in SD WebUI to DreamShaper using python","2023-06-03 18:16:40","","0","654","<python><machine-learning><pytorch><huggingface><stable-diffusion>","<p>I have been using stable diffusion WebUI to try out different models and LORAs for my application. I'm trying to now do the equivalent of what I've been doing in WebUI but in Python. I have a safetensors file for DreamShaper v5 beta-2 with VAE baked in. I am using two other safetensors files for two LORAs I downloaded from civitai.com.</p>
<p>Here is the code that I tried:</p>
<pre><code>import torch
from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler

pipeline = StableDiffusionPipeline.from_ckpt(&quot;./DreamShaper_5_beta2_BakedVae.ckpt&quot;, torch_dtype=torch.float16)
pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config)
pipeline.unet.load_attn_procs(&quot;./flat illustration.safetensors&quot;, local_files_only=True)
pipeline.unet.load_attn_procs(&quot;./improve_backgrounds.safetensors&quot;, local_files_only=True)
pipeline.to(&quot;cuda&quot;)
pipeline.enable_xformers_memory_efficient_attention()

prompt = &quot;Flat vector illustration of a scary and ominous grassy landscape with five or more trees, a large crack in the ground, and a gigantic monster sticking up high above the crack. The monster is based on an oak tree and made up of all kinds of litter and debris, including cans and bottles. The landscape is scattered with lots of litter and debris, especially tipped over garbage cans. There are hundreds of people running away from the monster, and the environment is dusty with no texture or shading. The color scheme of the grassy landscape is green and brown. &lt;lora:flat illustration:1&gt; &lt;lora:improve_backgrounds:0.85&gt;&quot;
nprompt = &quot;(deformed iris, deformed pupils, anime:1.4), text, close up, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck, extremely focused on people&quot;
image = pipeline(prompt, negative_prompt=nprompt, num_inference_steps=40, guidance_scale=7.5, cross_attention_kwargs={&quot;scale&quot;: 1}).images[0]
image.save(&quot;blue_pokemon.png&quot;)
</code></pre>
<p>I tried using the ckpt file for dreamshaper instead of the safetensors file to evade the following error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;C:\Users\***\PycharmProjects\KnowledgeGraph\loratest\main.py&quot;, line 4, in &lt;module&gt;
    pipeline = StableDiffusionPipeline.from_ckpt(&quot;./DreamShaper_5_beta2_BakedVae.safetensors&quot;, torch_dtype=torch.float16)
  File &quot;C:\Users\***\PycharmProjects\KnowledgeGraph\venv\lib\site-packages\diffusers\loaders.py&quot;, line 1284, in from_ckpt
    pipe = download_from_original_stable_diffusion_ckpt(
  File &quot;C:\Users\***\PycharmProjects\KnowledgeGraph\venv\lib\site-packages\diffusers\pipelines\stable_diffusion\convert_from_ckpt.py&quot;, line 1062, in download_from_original_stable_diffusion_ckpt
    raise ValueError(BACKENDS_MAPPING[&quot;safetensors&quot;][1])
KeyError: 'safetensors'
</code></pre>
<p>But then I got this error:</p>
<pre><code>global_step key not found in model
In this conversion only the non-EMA weights are extracted. If you want to instead extract the EMA weights (usually better for inference), please make sure to add the `--extract_ema` flag.
Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 
...
'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias']
- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Traceback (most recent call last):
  File &quot;C:\Users\***\PycharmProjects\KnowledgeGraph\loratest\main.py&quot;, line 6, in &lt;module&gt;
    pipeline.unet.load_attn_procs(&quot;./flat illustration.safetensors&quot;, local_files_only=True)
  File &quot;C:\Users\***\PycharmProjects\KnowledgeGraph\venv\lib\site-packages\diffusers\loaders.py&quot;, line 217, in load_attn_procs
    state_dict = torch.load(model_file, map_location=&quot;cpu&quot;)
  File &quot;C:\Users\***\PycharmProjects\KnowledgeGraph\venv\lib\site-packages\torch\serialization.py&quot;, line 815, in load
    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)
  File &quot;C:\Users\***\PycharmProjects\KnowledgeGraph\venv\lib\site-packages\torch\serialization.py&quot;, line 1033, in _legacy_load
    magic_number = pickle_module.load(f, **pickle_load_args)
MemoryError
</code></pre>
<p>My source for this code was from the huggingface docs. I tried looking in the SD WebUI Github WIKI but didn't find anything.</p>
<p>Again, all I'm trying to do is apply the LORAs to DreamShaper using Python just as I would have in SD WebUI. If it helps, here is where I got the LORAs from:</p>
<p><a href=""https://civitai.com/models/19130/flat-illustration"" rel=""nofollow noreferrer"">https://civitai.com/models/19130/flat-illustration</a></p>
<p><a href=""https://civitai.com/models/42190/improve-backgrounds"" rel=""nofollow noreferrer"">https://civitai.com/models/42190/improve-backgrounds</a></p>
","huggingface"
"76393740","How to get back the predicted text from model output in HuggingFace?","2023-06-02 22:15:19","76394716","1","542","<nlp><huggingface-transformers><huggingface>","<p>I have following toy example which deidentify a given text. I coded the following but the output does not makes sense. I am guessing that the way I am trying to get back the predicted text is incorrect.</p>
<pre><code>import torch 
from transformers import AutoTokenizer, AutoModelForTokenClassification
tokenizer = AutoTokenizer.from_pretrained(&quot;obi/deid_bert_i2b2&quot;, do_lower_case=True)
model = AutoModelForTokenClassification.from_pretrained(&quot;obi/deid_bert_i2b2&quot;)

text = &quot;Patient John Doe visited the hospital on 01/05/2023 with complaints of chest pain.&quot;

encoded_input = tokenizer(text, padding=True, return_tensors='pt')
outputs = model(**encoded_input)

# Get the predicted labels
predicted_labels = torch.argmax(outputs.logits, dim=2).squeeze()

# Convert the predicted labels back to text
predicted_tokens = tokenizer.batch_decode(predicted_labels, skip_special_tokens=True)

# Print the predicted tokens
print(predicted_tokens)
</code></pre>
<p>Output:</p>
<pre><code>['[unused33]', '[unused33]', '[unused33]', '[unused7]', '[unused29]', '[unused33]', '[unused33]', '[unused33]', '[unused33]', '[unused33]', '[unused35]', '[unused12]', '[unused35]', '[unused12]', '[unused23]', '[unused33]', '[unused33]', '[unused33]', '[unused33]', '[unused33]', '[unused33]', '[unused33]', '[unused33]']
</code></pre>
","huggingface"
"76391388","How to use HuggingFace Inference endpoints for both tokenization and inference?","2023-06-02 15:09:02","","2","689","<huggingface-transformers><huggingface-tokenizers><huggingface>","<p>I am trying to set up separate endpoints for tokenization and inference using HuggingFace models. Ideally I would like to use HuggingFace inference endpoints.</p>
<p>Is there a straightforward way to spin up endpoints for encoding, decoding, and inference for the same HF model? Or would I need to create containers for the encoder/decoder myself? I know HF has inference endpoints, but I'm not sure how well supported the tokenizer use case is or how I would implement that (e.g. what does the post request look like for encoding vs decoding, can I run it on the same infra as the inference endpoint, etc).</p>
<p>I have tried HF inference endpoints for inference, and I see that there are tokenizers available, but I am not sure how I can implement encoder/decoder for the tokenizer using the inference endpoint, and I'm unsure how to optimize.</p>
","huggingface"
"76388515","Why mocking HuggingFace datasets library does not work?","2023-06-02 08:37:57","76394624","1","250","<python><mocking><pytest><huggingface><huggingface-datasets>","<p>I have a Python function that uses the HuggingFace <code>datasets</code> library to load a private dataset from HuggingFace Hub.</p>
<p>I want to write a unit test for that function, but it seems pytest-mock does not work for some reason. The real function keeps getting called, even if the mock structure should be correct.</p>
<p>This is the main function:</p>
<pre class=""lang-py prettyprint-override""><code>def load_data(token: str):
    dataset = load_dataset(&quot;MYORG/MYDATASET&quot;, use_auth_token=token, split=&quot;train&quot;)
    return dataset
</code></pre>
<p>And this is the test function I wrote:</p>
<pre class=""lang-py prettyprint-override""><code>def test_data(mocker):
    # Mocked data
    token_test = &quot;test_token&quot;
    mocked_dataset = [
        {'image': [[0.5, 0.3], [0.7, 0.9]], 'timestamp': datetime.date(2023, 1, 1)},
    ]
    mocker.patch('datasets.load_dataset', return_value=mocked_dataset)

    result = load_data(token_test)

    assert len(result) == 1
</code></pre>
<p>Could it be that there are some &quot;unmockable&quot; libraries which do stuff under the hood and make their functions impossible to stub?</p>
","huggingface"
"76388275","Huggingface - Inference code in trainer script","2023-06-02 08:05:07","","0","244","<deep-learning><pytorch><amazon-sagemaker><huggingface-transformers><huggingface>","<p>I want to train a model for token classification using <strong>Huggingface Trainer</strong> and deploy it as an <strong>AWS Sagemaker</strong> endpoint. I followed <a href=""https://github.com/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv3/Fine_tune_LayoutLMv3_on_FUNSD_(HuggingFace_Trainer).ipynb"" rel=""nofollow noreferrer"">this</a> link and I am able to train and deploy the model using Huggingface estimator. Below is my code -</p>
<pre><code># create the Estimator
huggingface_estimator = HuggingFace(
    entry_point='train.py',
    source_dir='./code', 
    ...)

# train the model
huggingface_estimator.fit()

# create the endpoint prediction
predictor = huggingface_estimator.deploy(...)

# evaluate the model
predictor.predict(#data#)
</code></pre>
<p>How do I pass prediction input (<em>in this case data</em>). And also, what is the input data structure expected by the predict method()? Can someone point me to the documentation for adding the inference code in the train.py file? The model I am working with expects an <em>image, words, bounding boxes</em>.</p>
","huggingface"
"76384436","NameError when trying to run Huggingface Diffuser Tutorial","2023-06-01 17:46:05","","0","254","<python><pytorch><huggingface><stable-diffusion>","<p>I'm going through the Huggingface diffuser tutorial (<a href=""https://huggingface.co/docs/diffusers/main/en/tutorials/basic_training"" rel=""nofollow noreferrer"">https://huggingface.co/docs/diffusers/main/en/tutorials/basic_training</a>) and am running into an issue when trying to run the model.</p>
<p>Error:</p>
<pre><code>╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮
│ in &lt;cell line: 5&gt;:5                                                                              │
│                                                                                                  │
│ /usr/local/lib/python3.10/dist-packages/accelerate/launchers.py:103 in notebook_launcher         │
│                                                                                                  │
│   100 │   │   │   print(&quot;Launching training on one GPU.&quot;)                                        │
│   101 │   │   else:                                                                              │
│   102 │   │   │   print(&quot;Launching training on one CPU.&quot;)                                        │
│ ❱ 103 │   │   function(*args)                                                                    │
│   104 │   else:                                                                                  │
│   105 │   │   if num_processes is None:                                                          │
│   106 │   │   │   raise ValueError(                                                              │
│ in train_loop:28                                                                                 │
│ in get_full_repo_name:13                                                                         │
╰──────────────────────────────────────────────────────────────────────────────────────────────────╯
NameError: name '{myusername}' is not defined
</code></pre>
<p>I believe I am supposed to insert my username somewhere here:</p>
<pre><code>from huggingface_hub import HfFolder, Repository, whoami
from tqdm.auto import tqdm
from pathlib import Path
import os


def get_full_repo_name(model_id: str, organization: str = None, token: str = None):
    if token is None:
        token = HfFolder.get_token()
    if organization is None:
        username = whoami(token)[&quot;name&quot;]
        return f&quot;{username}/{model_id}&quot;
    else:
        return f&quot;{organization}/{model_id}&quot;


def train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler):
    # Initialize accelerator and tensorboard logging
    accelerator = Accelerator(
        mixed_precision=config.mixed_precision,
        gradient_accumulation_steps=config.gradient_accumulation_steps,
        log_with=&quot;tensorboard&quot;,
        logging_dir=os.path.join(config.output_dir, &quot;logs&quot;),
    )
    if accelerator.is_main_process:
        if config.push_to_hub:
            repo_name = get_full_repo_name(Path(config.output_dir).name)
            repo = Repository(config.output_dir, clone_from=repo_name)
        elif config.output_dir is not None:
            os.makedirs(config.output_dir, exist_ok=True)
        accelerator.init_trackers(&quot;train_example&quot;)

    # Prepare everything
    # There is no specific order to remember, you just need to unpack the
    # objects in the same order you gave them to the prepare method.
    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(
        model, optimizer, train_dataloader, lr_scheduler
    )

    global_step = 0

    # Now you train the model
    for epoch in range(config.num_epochs):
        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)
        progress_bar.set_description(f&quot;Epoch {epoch}&quot;)

        for step, batch in enumerate(train_dataloader):
            clean_images = batch[&quot;images&quot;]
            # Sample noise to add to the images
            noise = torch.randn(clean_images.shape).to(clean_images.device)
            bs = clean_images.shape[0]

            # Sample a random timestep for each image
            timesteps = torch.randint(
                0, noise_scheduler.config.num_train_timesteps, (bs,), device=clean_images.device
            ).long()

            # Add noise to the clean images according to the noise magnitude at each timestep
            # (this is the forward diffusion process)
            noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)

            with accelerator.accumulate(model):
                # Predict the noise residual
                noise_pred = model(noisy_images, timesteps, return_dict=False)[0]
                loss = F.mse_loss(noise_pred, noise)
                accelerator.backward(loss)

                accelerator.clip_grad_norm_(model.parameters(), 1.0)
                optimizer.step()
                lr_scheduler.step()
                optimizer.zero_grad()

            progress_bar.update(1)
            logs = {&quot;loss&quot;: loss.detach().item(), &quot;lr&quot;: lr_scheduler.get_last_lr()[0], &quot;step&quot;: global_step}
            progress_bar.set_postfix(**logs)
            accelerator.log(logs, step=global_step)
            global_step += 1

        # After each epoch you optionally sample some demo images with evaluate() and save the model
        if accelerator.is_main_process:
            pipeline = DDPMPipeline(unet=accelerator.unwrap_model(model), scheduler=noise_scheduler)

            if (epoch + 1) % config.save_image_epochs == 0 or epoch == config.num_epochs - 1:
                evaluate(config, epoch, pipeline)

            if (epoch + 1) % config.save_model_epochs == 0 or epoch == config.num_epochs - 1:
                if config.push_to_hub:
                    repo.push_to_hub(commit_message=f&quot;Epoch {epoch}&quot;, blocking=True)
                else:
                    pipeline.save_pretrained(config.output_dir)
</code></pre>
<p>But I've tried several inserts and nothing seems to fix it. Any thoughts on this?</p>
","huggingface"
"76384301","Starcoder finetuning - How to select the GPU and how to estimate the time it will take to finetune","2023-06-01 17:22:14","","7","664","<deep-learning><pytorch><huggingface><language-model><large-language-model>","<p>I'd like to finetune Starcoder (<a href=""https://huggingface.co/bigcode/starcoder"" rel=""noreferrer"">https://huggingface.co/bigcode/starcoder</a>) on my dataset and on a GCP VM instance.</p>
<p>It's says in the documentation that for training the model, they used 512 Tesla A100 GPUs and it took 24 days.</p>
<p>I also saw the model (.bin) files in files section of huggingFace (<a href=""https://huggingface.co/bigcode/starcoder/tree/main"" rel=""noreferrer"">https://huggingface.co/bigcode/starcoder/tree/main</a>)</p>
<p>The total size of the model is ~64GB</p>
<p>Based on all this information,</p>
<ol>
<li>How do I decide which GPU is best for finetuning on my dataset ?</li>
<li>How to estimate the time it will take finetune ? (based on assumptions on parameters like epoch=1, for instance)</li>
<li>Are there any other factors that are considered to choose hardware / calculate time ?</li>
</ol>
","huggingface"
"76376606","__init__() got an unexpected keyword argument 'pooling_mode_weightedmean_tokens'","2023-05-31 19:14:10","","3","1618","<huggingface><weaviate>","<p>I am attempting to index and vectorize a test document into Weaviate using Huggingface's <code>instructor-xl</code>, but I am getting this error. Does anyone know how to resolve? Code below.</p>
<p>Schema creation script:</p>
<pre><code>load_dotenv()

client = weaviate.Client(
    url=&quot;http://localhost:8080&quot;,
    additional_headers={&quot;X-HuggingFace-Api-Key&quot;: os.getenv(&quot;HUGGINGFACE_APIKEY&quot;)},
)

client.schema.delete_class(&quot;Blah&quot;)

class_obj = {
    &quot;class&quot;: &quot;Blah&quot;,
    &quot;properties&quot;: [
        {
            &quot;dataType&quot;: [&quot;text&quot;],
            &quot;name&quot;: &quot;title&quot;,
            &quot;tokenization&quot;: &quot;whitespace&quot;,
        },
        {
            &quot;dataType&quot;: [&quot;text&quot;],
            &quot;name&quot;: &quot;summary&quot;,
            &quot;tokenization&quot;: &quot;whitespace&quot;,
        },
        {
            &quot;dataType&quot;: [&quot;text&quot;],
            &quot;name&quot;: &quot;description&quot;,
            &quot;tokenization&quot;: &quot;whitespace&quot;,
        },
    ],
    &quot;vectorizer&quot;: &quot;text2vec-huggingface&quot;,
    &quot;moduleConfig&quot;: {
        &quot;text2vec-huggingface&quot;: { 
            &quot;skip&quot;: False,
            &quot;vectorizePropertyName&quot;: False,
            &quot;model&quot;: &quot;hkunlp/instructor-xl&quot;,
            &quot;options&quot;: {&quot;waitForModel&quot;: True},
        }
    },
    &quot;invertedIndexConfig&quot;: {&quot;indexTimestamps&quot;: True, &quot;indexNullState&quot;: True},
}

client.schema.create_class(class_obj)
</code></pre>
<p>Indexing script:</p>
<pre><code>client = weaviate.Client(
        url=&quot;http://localhost:8080&quot;,
        additional_headers={&quot;X-HuggingFace-Api-Key&quot;: os.getenv(&quot;HUGGINGFACE_APIKEY&quot;)},
    )
class_name = &quot;Blah&quot;
df = pd.read_json(&quot;blah.json&quot;)
    with client.batch() as batch:
        for i in [data_obj]:
            batch.add_data_object(i, class_name)
</code></pre>
<p>Docker-compose file:</p>
<pre><code>---
version: '3.4'
services:
  weaviate:
    command:
    - --host
    - 0.0.0.0
    - --port
    - '8080'
    - --scheme
    - http
    image: semitechnologies/weaviate:1.19.6
    ports:
    - 8080:8080
    restart: on-failure:0
    volumes:
      - /var/weaviate:/var/lib/weaviate
    environment:
      SUM_INFERENCE_API: 'http://sum-transformers:8080'
      HUGGINGFACE_APIKEY: $HUGGINGFACE_APIKEY
      QUERY_DEFAULTS_LIMIT: 25
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'
      PERSISTENCE_DATA_PATH: '/var/lib/weaviate'
      DEFAULT_VECTORIZER_MODULE: 'text2vec-huggingface'
      ENABLE_MODULES: 'text2vec-huggingface,sum-transformers'
      CLUSTER_HOSTNAME: 'node1'
  sum-transformers:
    image: semitechnologies/sum-transformers:facebook-bart-large-cnn-1.0.0
    environment:
      ENABLE_CUDA: '0'
</code></pre>
<p>Code that I have tried is above</p>
","huggingface"
"76376455","Transformers tokenizer attention mask for pytorch","2023-05-31 18:44:27","76397172","2","3528","<python><pytorch><huggingface-transformers><huggingface>","<p>In my code I have:</p>
<pre class=""lang-py prettyprint-override""><code>output = self.decoder(output, embedded, tgt_mask=attention_mask)
</code></pre>
<p>where</p>
<pre class=""lang-py prettyprint-override""><code>decoder_layer = TransformerDecoderLayer(embedding_size, num_heads, hidden_size, dropout, batch_first=True)
self.decoder = TransformerDecoder(decoder_layer, 1)
</code></pre>
<p>I generate the attention mask using a huggingface's tokenizer:</p>
<pre class=""lang-py prettyprint-override""><code>batch = tokenizer(example['text'], return_tensors=&quot;pt&quot;, truncation=True, max_length=1024, padding='max_length')
inputs = batch['input_ids']
attention_mask = batch['attention_mask']
</code></pre>
<p>Running it through the models fails on</p>
<p><code>AssertionError: only bool and floating types of attn_mask are supported</code></p>
<p>Changing the attention mask to <code>attention_mask = batch['attention_mask'] .bool()</code></p>
<p>Causes</p>
<p><code>RuntimeError: The shape of the 2D attn_mask is torch.Size([4, 1024]), but should be (1024, 1024)</code></p>
<p>Any idea how I can use a huggingface tokenizer with my own pytorch module?</p>
","huggingface"
"76373220","Fine-tuning a pre-trained LLM for question-answering","2023-05-31 11:55:10","","3","7609","<huggingface-transformers><huggingface><language-model><fine-tuning><text-generation>","<h3>Objective</h3>
<p>My goal is to fine-tune a pre-trained LLM on a dataset about Manchester United's (MU's) 2021/22 season (they had a poor season). I want to be able to prompt the fine-tuned model with questions such as &quot;How can MU improve?&quot;, or &quot;What are MU's biggest weaknesses?&quot;. The ideal responses would be insightful/logical and +100 words</p>
<h3>Data</h3>
<ul>
<li>I will simply use text from the relevant wiki page as my data: <a href=""https://en.wikipedia.org/wiki/2021%E2%80%9322_Manchester_United_F.C._season"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/2021%E2%80%9322_Manchester_United_F.C._season</a></li>
<li>How should I structure my data? Should it be a list dictionaries where the keys are the questions and the values are the answers (i.e. a list of question-answer pairs), or a long string containing all the text data (for context), or a combination of both?</li>
</ul>
<h3>Notes</h3>
<ul>
<li>I have mainly been experimenting with variations of Google's T5 (e.g.: <a href=""https://huggingface.co/t5-base"" rel=""nofollow noreferrer"">https://huggingface.co/t5-base</a>) which I have imported from the Hugging Face Transformers library</li>
<li>So far I have only fine-tuned the model on a list of 30 dictionaries (question-answer pairs), e.g.: {&quot;question&quot;: &quot;How could Manchester United improve their consistency in the Premier League next season?&quot;, &quot;answer&quot;: &quot; To improve consistency, Manchester United could focus on strengthening their squad depth to cope with injuries and fatigue throughout the season. Tactical adjustments could also be explored to deal with teams of different strengths and styles.&quot;}</li>
<li>Use of this small dataset (list of 30 dictionaries) has given poor results</li>
</ul>
<h3>Further Questions and Notes</h3>
<ul>
<li>Other than increasing the size of my dataset, is my approach sound?</li>
<li>What would you recommend as a minimum number of dictionaries to train/fine-tune the model on?</li>
<li>I am also aware that I can tune the hyperparameters to improve performance, but for now I am more concerned about my general approach being logical</li>
</ul>
","huggingface"
"76368843","How to load a .py script directly with evaluate.load?","2023-05-30 21:41:23","","0","459","<python><metrics><huggingface><huggingface-evaluate>","<p>If I have a script like this <a href=""https://huggingface.co/spaces/evaluate-metric/frugalscore/blob/main/frugalscore.py"" rel=""nofollow noreferrer"">https://huggingface.co/spaces/evaluate-metric/frugalscore/blob/main/frugalscore.py</a> and save it as <code>fgscore.py</code> with a directory locally like:</p>
<pre><code>./
  my_script.py
  fgscore/
      fgscore.py
</code></pre>
<p>And in my_script.py, I can do something like:</p>
<pre><code>import evaluate


mt_metrics = evaluate.load(&quot;fgscore&quot;)

sources = [&quot;안녕하세요 저는 당신의 아버지입니다&quot;, &quot;일반 케노비&quot;]
predictions = [&quot;hello here I am your father&quot;, &quot;general kenobi&quot;]
references = [&quot;hello there I am your father&quot;, &quot;general yoda&quot;]
results = mt_metrics.compute(predictions=predictions,
                             references=references)

print(results)
</code></pre>
<p>Looking at the <code>evaluate.load()</code> function, <a href=""https://github.com/huggingface/evaluate/blob/main/src/evaluate/loading.py#L688"" rel=""nofollow noreferrer"">https://github.com/huggingface/evaluate/blob/main/src/evaluate/loading.py#L688</a>, it states:</p>
<pre><code>path (`str`):
    Path to the evaluation processing script with the evaluation builder. Can be either:
        - a local path to processing script or the directory containing the script (if the script has the same name as the directory),
            e.g. `'./metrics/rouge'` or `'./metrics/rouge/rouge.py'`
        - a evaluation module identifier on the HuggingFace evaluate repo e.g. `'rouge'` or `'bleu'` that are in either `'metrics/'`,
            `'comparisons/'`, or `'measurements/'` depending on the provided `module_type`
</code></pre>
<h3>Is there a reason to do <code>{name}/{name}.py</code> for using the path arguments in <code>evaluate.load()</code>?</h3>
<h3>Is there a way to override such that I can point the evaluate directly to the <code>.py</code> file? E.g. <code>evaluate.load(&quot;fgscore.py&quot;)</code></h3>
","huggingface"
"76363706","Got exception 'EagerTensor' object has no attribute 'size' when generating BERT embeddings","2023-05-30 10:04:32","76363759","1","1084","<tensorflow2.0><huggingface-transformers><huggingface>","<p>I am new to huggingface transforms and I have this little test program:</p>
<pre><code>from transformers import BertTokenizer, BertModel

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

title = &quot;Today is Monday&quot;
tokens = tokenizer([title], return_tensors=&quot;tf&quot;, truncation=True, padding=True)
outputs = model(**tokens)
</code></pre>
<p>It will fail with this exception:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/anaconda3/envs/recommenders/lib/python3.9/runpy.py&quot;, line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File &quot;/anaconda3/envs/recommenders/lib/python3.9/runpy.py&quot;, line 87, in _run_code
    exec(code, run_globals)
  File &quot;/root/.vscode-server/extensions/ms-python.python-2023.8.0/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/__main__.py&quot;, line 39, in &lt;module&gt;
    cli.main()
  File &quot;/root/.vscode-server/extensions/ms-python.python-2023.8.0/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py&quot;, line 430, in main
    run()
  File &quot;/root/.vscode-server/extensions/ms-python.python-2023.8.0/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py&quot;, line 284, in run_file
    runpy.run_path(target, run_name=&quot;__main__&quot;)
  File &quot;/root/.vscode-server/extensions/ms-python.python-2023.8.0/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py&quot;, line 321, in run_path
    return _run_module_code(code, init_globals, run_name,
  File &quot;/root/.vscode-server/extensions/ms-python.python-2023.8.0/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py&quot;, line 135, in _run_module_code
    _run_code(code, mod_globals, init_globals,
  File &quot;/root/.vscode-server/extensions/ms-python.python-2023.8.0/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py&quot;, line 124, in _run_code
    exec(code, run_globals)
  File &quot;/opt/nwdata/tests/recommenders/examples/00_quick_start/test.py&quot;, line 9, in &lt;module&gt;
    outputs = model(**tokens)
  File &quot;/anaconda3/envs/recommenders/lib/python3.9/site-packages/torch/nn/modules/module.py&quot;, line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;/anaconda3/envs/recommenders/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py&quot;, line 962, in forward
    input_shape = input_ids.size()
  File &quot;/anaconda3/envs/recommenders/lib/python3.9/site-packages/tensorflow/python/framework/ops.py&quot;, line 437, in __getattr__
    raise AttributeError(&quot;&quot;&quot;
AttributeError: 
        'EagerTensor' object has no attribute 'size'.
        If you are looking for numpy-related methods, please run the following:
        from tensorflow.python.ops.numpy_ops import np_config
        np_config.enable_numpy_behavior()
</code></pre>
<p>It will work if I pass <code>&quot;pt&quot;</code> to the param <code>return_tensors</code>. But I need to set the param to <code>&quot;tf&quot;</code> because I will use the result in a tensorflow program.  How can I solve this problem?</p>
","huggingface"
"76343699","""SadTalker"" is running solely on my CPU instead of utilizing the GPU","2023-05-26 19:19:34","","1","1056","<rendering><huggingface><stable-diffusion>","<p>&quot;SadTalker&quot;, an extension of stable diffusion, is running solely on my CPU instead of utilizing the GPU</p>
<p>I am using two arguments in the 'webui-user.bat' file: --medvram and --disable-safe-unpickle. It doesn't work without the --disable-safe-unpickle argument. However, txt2img is using the GPU normally. I have an AMD RX 580 8Gb.&quot; How can I solve it?</p>
","huggingface"
"76338717","Model.generate stop code execution without any error","2023-05-26 07:49:33","","0","331","<huggingface-transformers><huggingface>","<p>I am pretty new to HF, this is my first attempt to use a model. The problem is <code>model.generate</code> kinda abrupt the script execution without any error. Here’s my code:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import RobertaTokenizer, T5ForConditionalGeneration

tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')
model = T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-base')

text = &quot;write for cycle&quot;
input_ids = tokenizer(text, return_tensors=&quot;pt&quot;).input_ids

print(&quot;before&quot;)
generated_ids = model.generate(input_ids, max_length=8)
print(&quot;after&quot;)

print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))
</code></pre>
<p>This code gives me no output except “before”. Also, I’ve tried other models with the same result. It looks the issue on my side… I’ll be very grateful for your help. Thanks!</p>
<p>Env:</p>
<pre><code>- `transformers` version: 4.30.0.dev0
- Platform: macOS-10.15.7-x86_64-i386-64bit
- Python version: 3.9.16
- Huggingface_hub version: 0.14.1
- Safetensors version: 0.3.1
- PyTorch version (GPU?): 2.0.1 (False)
- Tensorflow version (GPU?): not installed (NA)
- Flax version (CPU?/GPU?/TPU?): not installed (NA)
- Jax version: not installed
- JaxLib version: not installed
- Using GPU in script?: &lt;fill in&gt;
- Using distributed or parallel set-up in script?: &lt;fill in&gt;
</code></pre>
<p>Logs:</p>
<pre><code>loading file vocab.json from cache at /Users/zonder/.cache/huggingface/hub/models--Salesforce--codet5-base/snapshots/4078456db09ba972a3532827a0b5df4da172323c/vocab.json
loading file merges.txt from cache at /Users/zonder/.cache/huggingface/hub/models--Salesforce--codet5-base/snapshots/4078456db09ba972a3532827a0b5df4da172323c/merges.txt
loading file added_tokens.json from cache at /Users/zonder/.cache/huggingface/hub/models--Salesforce--codet5-base/snapshots/4078456db09ba972a3532827a0b5df4da172323c/added_tokens.json
loading file special_tokens_map.json from cache at /Users/zonder/.cache/huggingface/hub/models--Salesforce--codet5-base/snapshots/4078456db09ba972a3532827a0b5df4da172323c/special_tokens_map.json
loading file tokenizer_config.json from cache at /Users/zonder/.cache/huggingface/hub/models--Salesforce--codet5-base/snapshots/4078456db09ba972a3532827a0b5df4da172323c/tokenizer_config.json
loading configuration file config.json from cache at /Users/zonder/.cache/huggingface/hub/models--Salesforce--codet5-base/snapshots/4078456db09ba972a3532827a0b5df4da172323c/config.json
Model config T5Config {
  &quot;_name_or_path&quot;: &quot;/content/drive/MyDrive/CodeT5/pretrained_models/codet5_base&quot;,
  &quot;architectures&quot;: [
    &quot;T5ForConditionalGeneration&quot;
  ],
  &quot;bos_token_id&quot;: 1,
  &quot;d_ff&quot;: 3072,
  &quot;d_kv&quot;: 64,
  &quot;d_model&quot;: 768,
  &quot;decoder_start_token_id&quot;: 0,
  &quot;dense_act_fn&quot;: &quot;relu&quot;,
  &quot;dropout_rate&quot;: 0.1,
  &quot;eos_token_id&quot;: 2,
  &quot;feed_forward_proj&quot;: &quot;relu&quot;,
  &quot;gradient_checkpointing&quot;: false,
  &quot;id2label&quot;: {
    &quot;0&quot;: &quot;LABEL_0&quot;
  },
  &quot;initializer_factor&quot;: 1.0,
  &quot;is_encoder_decoder&quot;: true,
  &quot;is_gated_act&quot;: false,
  &quot;label2id&quot;: {
    &quot;LABEL_0&quot;: 0
  },
  &quot;layer_norm_epsilon&quot;: 1e-06,
  &quot;model_type&quot;: &quot;t5&quot;,
  &quot;n_positions&quot;: 512,
  &quot;num_decoder_layers&quot;: 12,
  &quot;num_heads&quot;: 12,
  &quot;num_layers&quot;: 12,
  &quot;output_past&quot;: true,
  &quot;pad_token_id&quot;: 0,
  &quot;relative_attention_max_distance&quot;: 128,
  &quot;relative_attention_num_buckets&quot;: 32,
  &quot;task_specific_params&quot;: {
    &quot;summarization&quot;: {
      &quot;early_stopping&quot;: true,
      &quot;length_penalty&quot;: 2.0,
      &quot;max_length&quot;: 200,
      &quot;min_length&quot;: 30,
      &quot;no_repeat_ngram_size&quot;: 3,
      &quot;num_beams&quot;: 4,
      &quot;prefix&quot;: &quot;summarize: &quot;
    },
    &quot;translation_en_to_de&quot;: {
      &quot;early_stopping&quot;: true,
      &quot;max_length&quot;: 300,
      &quot;num_beams&quot;: 4,
      &quot;prefix&quot;: &quot;translate English to German: &quot;
    },
    &quot;translation_en_to_fr&quot;: {
      &quot;early_stopping&quot;: true,
      &quot;max_length&quot;: 300,
      &quot;num_beams&quot;: 4,
      &quot;prefix&quot;: &quot;translate English to French: &quot;
    },
    &quot;translation_en_to_ro&quot;: {
      &quot;early_stopping&quot;: true,
      &quot;max_length&quot;: 300,
      &quot;num_beams&quot;: 4,
      &quot;prefix&quot;: &quot;translate English to Romanian: &quot;
    }
  },
  &quot;torch_dtype&quot;: &quot;float32&quot;,
  &quot;transformers_version&quot;: &quot;4.30.0.dev0&quot;,
  &quot;use_cache&quot;: true,
  &quot;vocab_size&quot;: 32100
}

loading weights file pytorch_model.bin from cache at /Users/zonder/.cache/huggingface/hub/models--Salesforce--codet5-base/snapshots/4078456db09ba972a3532827a0b5df4da172323c/pytorch_model.bin
Generate config GenerationConfig {
  &quot;_from_model_config&quot;: true,
  &quot;bos_token_id&quot;: 1,
  &quot;decoder_start_token_id&quot;: 0,
  &quot;eos_token_id&quot;: 2,
  &quot;pad_token_id&quot;: 0,
  &quot;transformers_version&quot;: &quot;4.30.0.dev0&quot;
}

All model checkpoint weights were used when initializing T5ForConditionalGeneration.

All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at Salesforce/codet5-base.
If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.
Generation config file not found, using a generation config created from the model config.
before
Generate config GenerationConfig {
  &quot;_from_model_config&quot;: true,
  &quot;bos_token_id&quot;: 1,
  &quot;decoder_start_token_id&quot;: 0,
  &quot;eos_token_id&quot;: 2,
  &quot;pad_token_id&quot;: 0,
  &quot;transformers_version&quot;: &quot;4.30.0.dev0&quot;
}
</code></pre>
","huggingface"
"76338439","HuggingFace autotrain for entity recognition?","2023-05-26 07:13:52","","0","117","<nlp><named-entity-recognition><pre-trained-model><huggingface><huggingface-trainer>","<p>I'm trying to fine tune an entity recognition model on HuggingFace using their autotrain feature, but once I select autotrain, 'Token classification' is the only option available, but that's not what I need.</p>
<p>How do I fine tune an NER model?</p>
","huggingface"
"76330546","How to determine the value of early_stopping_patience in HuggingFace's Seq2SeqTrainer EarlyStoppingCallback?","2023-05-25 09:01:55","76332053","1","3106","<huggingface-transformers><huggingface><huggingface-trainer>","<p>In my <code>Seq2SeqTrainer</code>, I use <code>EarlyStoppingCallback</code> to stop the training process when the criteria has been met.</p>
<pre><code>trainer = Seq2SeqTrainer(
    model = model,
    args = training_args,
    train_dataset = train_set,
    eval_dataset = eval_set,
    tokenizer = tokenizer,
    data_collator = data_collator,
    compute_metrics = compute_metrics,
    callbacks = [EarlyStoppingCallback(early_stopping_patience=1)]
)
</code></pre>
<p>Currently, I am using the default value <code>1</code> in the <code>early_stopping_patience</code>, but how can I determine which value I should use? The <a href=""https://huggingface.co/docs/transformers/main_classes/callback#transformers.EarlyStoppingCallback"" rel=""nofollow noreferrer"">official documentation</a> doesn't say much.</p>
<blockquote>
<p>early_stopping_patience (int) — Use with <code>metric_for_best_model</code> to stop training when the specified metric worsens for <code>early_stopping_patience</code> evaluation calls.</p>
</blockquote>
<p>Also, could I use <em>Epoch</em> instead of <em>Step</em> in <code>evaluation_strategy</code> with this <code>EarlyStoppingCallback()</code>?</p>
<p>Thanks in advance.</p>
","huggingface"
"76325023","Output of extracted Huggingface decoder does not have attribute logits","2023-05-24 15:12:13","","1","156","<python><huggingface-transformers><huggingface><encoder-decoder>","<p>I am trying to build a video-to-text model using a Huggingface <code>VisionEncoderDecoderModel</code>. For the encoder, I'm using <a href=""https://huggingface.co/MCG-NJU/videomae-base"" rel=""nofollow noreferrer"">VideoMAE</a>. Because the sequence length for videos is long, I want to use the decoder from <a href=""https://huggingface.co/allenai/led-base-16384"" rel=""nofollow noreferrer"">Longformer Encoder-Decoder</a> (LED). Because LED is an encoder-decoder model, I am extracting the decoder to construct my model like this</p>
<pre class=""lang-py prettyprint-override""><code>enc = &quot;MCG-NJU/videomae-base&quot;
dec = &quot;allenai/led-base-16384&quot;
encoder = AutoModel.from_pretrained(enc)
enc_dec = AutoModel.from_pretrained(dec)
model = VisionEncoderDecoderModel(encoder=encoder, decoder=enc_dec.decoder)
...
</code></pre>
<p>When I try to do inference with the model like this</p>
<pre class=""lang-py prettyprint-override""><code>with torch.no_grad():
    output = model(pixel_values=vids, decoder_input_ids=texts)
    print(output.last_hidden_state.shape)
</code></pre>
<p>I get this error:</p>
<pre><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
Cell In[40], line 7
      1 with torch.no_grad():
      2     # inputs = {&quot;pixel_values&quot;: vids}
      3     # print(inputs.keys())
      4     # output = model.encoder(**inputs)
      5     # # expect 1 (because one video) x ((14 * 14) * 8) (because &lt;&gt; tokens) x 768 (representation size)
      6     # print(output.last_hidden_state.shape)
----&gt; 7     output = model(pixel_values=vids, decoder_input_ids=concs)
      8     # expect 1 (because one video) x ((14 * 14) * 8) (because &lt;&gt; tokens) x 768 (representation size)
      9     print(output.last_hidden_state.shape)

File ~/miniconda3/envs/vit/lib/python3.10/site-packages/torch/nn/modules/module.py:1194, in Module._call_impl(self, *input, **kwargs)
   1190 # If we don't have any hooks, we want to skip the rest of the logic in
   1191 # this function, and just call forward.
   1192 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1193         or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1194     return forward_call(*input, **kwargs)
   1195 # Do not call functions when jit is used
   1196 full_backward_hooks, non_full_backward_hooks = [], []

File ~/miniconda3/envs/vit/lib/python3.10/site-packages/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py:638, in VisionEncoderDecoderModel.forward(self, pixel_values, decoder_input_ids, decoder_attention_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)
    633     else:
    634         return decoder_outputs + encoder_outputs
    636 return Seq2SeqLMOutput(
    637     loss=loss,
--&gt; 638     logits=decoder_outputs.logits,
    639     past_key_values=decoder_outputs.past_key_values,
    640     decoder_hidden_states=decoder_outputs.hidden_states,
    641     decoder_attentions=decoder_outputs.attentions,
    642     cross_attentions=decoder_outputs.cross_attentions,
    643     encoder_last_hidden_state=encoder_outputs.last_hidden_state,
    644     encoder_hidden_states=encoder_outputs.hidden_states,
    645     encoder_attentions=encoder_outputs.attentions,
    646 )

AttributeError: 'BaseModelOutputWithPastAndCrossAttentions' object has no attribute 'logits'
</code></pre>
<p>I suspect it has something to do with the fact that I'm not using the whole encoder-decoder model, but I'm not sure. Is there a way to get the decoder that I'm extracting to return an output that has logits?</p>
","huggingface"
"76310036","How to use sample_by=""document"" argument with load_dataset in Huggingface Dataset?","2023-05-22 21:32:19","","3","230","<python><deep-learning><nlp><huggingface><huggingface-datasets>","<h2>Problem</h2>
<p>Hello. I am trying to use huggingface to do some malware classification. I have a 5738 malware binaries in a directory. The paths to these malware binaries are stored in a list called <code>files</code>. I am trying to load these binaries into a huggingface datasets.Dataset object.</p>
<p>I have created the Dataset like this</p>
<pre><code>dataset = datasets.Dataset.from_text(
    files,
    sample_by=&quot;document&quot;,
    encoding=&quot;latin1&quot;,
)
</code></pre>
<p>Since each file is supposed to represent a single instance, I used <code>sample_by=&quot;document&quot;</code>, which to my knowledge (confirmed by reading the source code) should treat each document in <code>files</code> as an individual example.</p>
<p>Strangely, the length of <code>files</code> and the length of the resulting <code>dataset</code> do not appear to be the same</p>
<pre><code>dataset.num_rows, len(files)
&gt;&gt;&gt; (27967, 5738)
</code></pre>
<p>The expected behavior was that each file in <code>files</code> would get mapped to a particular row in <code>dataset</code>, but apparently this did not happen. Any idea whats up with this? Thanks!</p>
<h2>Software</h2>
<ul>
<li>datasets 2.12.0</li>
<li>Python 3.10.6</li>
<li>CentOS 9</li>
</ul>
<h2>References</h2>
<ul>
<li><a href=""https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset"" rel=""nofollow noreferrer"">datasets.Dataset</a></li>
<li><a href=""https://huggingface.co/docs/datasets/package_reference/loading_methods#datasets.packaged_modules.text.TextConfig"" rel=""nofollow noreferrer"">datasets.packaged_modules.text.TextConfig</a></li>
</ul>
","huggingface"
"76296131","Token indices sequence length error with langchain","2023-05-20 16:04:18","","1","898","<huggingface><py-langchain>","<p>Getting the following error when running load_qa_chain with map_reduce:</p>
<pre><code>Token indices sequence length is longer than the specified maximum sequence length for this model (2108 &gt; 1024
</code></pre>
<p>The code:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

model=&quot;ehartford/WizardLM-7B-Uncensored&quot;
text_gen_pipeline = pipeline(
    model = model, 
    model_kwargs= {
        &quot;device_map&quot;: &quot;auto&quot;, 
        &quot;load_in_8bit&quot;: True, 
        # default-explain-code settings from https://platform.openai.com/examples
        &quot;temperature&quot;: 0,
        &quot;top_p&quot;: 1.0,
    },
    max_new_tokens=2500)
    
from langchain import HuggingFacePipeline
llm = HuggingFacePipeline(pipeline=text_gen_pipeline)

from langchain.text_splitter import RecursiveCharacterTextSplitter
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size = 1000,
    chunk_overlap = 50)
# data is loaded with GitLoader
chunks = text_splitter.split_documents(data)

from langchain.embeddings import HuggingFaceEmbeddings
embeddings = HuggingFaceEmbeddings()
from langchain.vectorstores.faiss import FAISS

vectorstore = FAISS.from_documents(chunks, embeddings)

query = &quot;How do you use the HamburgerMenu component?&quot;
docs = vectorstore.similarity_search(query)

from langchain.chains.question_answering import load_qa_chain
chain = load_qa_chain(llm, chain_type=&quot;stuff&quot;)

result = chain.run(input_documents=docs, question=query)
</code></pre>
","huggingface"
"76289028","Constructing a HuggingFace Dataset using a custom PyTorch Dataset","2023-05-19 12:22:41","","1","754","<pytorch><huggingface-transformers><huggingface><huggingface-datasets>","<p>Hi I am new to using transformers in HuggingFace and trying to train a model using my custom text data. I have my custom dataset ad PyTorch dataset. The text labels are longer than 512 and I add <code>truncation</code> and <code>offset_mapping</code> to truncate and create new lines in the dataset like so:</p>
<pre class=""lang-py prettyprint-override""><code>def __getitem__(self, idx):
    # ...
    encoding = processor(image, words, 
                         boxes=boxes, word_labels=labels, 
                         truncation=True, padding=&quot;max_length&quot;, 
                         return_overflowing_tokens=True, 
                         return_offsets_mapping=True)
</code></pre>
<p>I would like to construct a HuggingFace <code>Dataset</code> but when I call <code>Dataset.from_dict(myPytorchDataset)</code>, I get:</p>
<blockquote>
<p>'MyDataset' object has no attribute 'items'</p>
</blockquote>
<p>I did try the new <code>from_generator</code> method as well like so,
<code>Dataset.from_generator(myPytorchDataset)</code>, I get:</p>
<blockquote>
<p>'MyDataset' object is not callable</p>
</blockquote>
<p>I currently do the conversion like so:</p>
<pre class=""lang-py prettyprint-override""><code>from datasets import Dataset, concatenate_datasets

concat_dataset = Dataset.from_dict(myPytorchDataset[0])
for i in range(1,len(myPytorchDataset)):
    d = Dataset.from_dict(myPytorchDataset[i])
    concat_dataset = concatenate_datasets([concat_dataset, d])

###
# Before concat 

# myPytorchDataset[0].keys() -&gt; dict_keys(['input_ids', 'attention_mask', 'bbox', 'labels', 'pixel_values']) 

# sizes for a random sample 
# input_ids torch.Size([6, 512]) 
# attention_mask torch.Size([6, 512]) 
# bbox torch.Size([6, 512, 4]) 
# pixel_values torch.Size([6, 3, 224, 224]) 
</code></pre>
<p>Is there a better way to do this?</p>
","huggingface"
"76268667","HuggingFace Accelerate won't use the proper number of processes","2023-05-17 04:54:02","76287079","1","3577","<huggingface>","<p>The server I'm using has a total of 4 GPUs and I want to use 2. I'm therefore trying to do <code>accelerate launch --num_processes 2 train.py</code>, but when I run the script it says that the number of processes is only 1. Why is this happening? It seems as of now I can either only use all 4 or only 1.</p>
","huggingface"
"76265748","Indefinite wait while using Langchain and HuggingFaceHub in python","2023-05-16 17:34:28","","6","1119","<python><huggingface><langchain><huggingface-hub>","<pre><code>
from langchain import PromptTemplate, HuggingFaceHub, LLMChain
import os

os.environ['HUGGINGFACEHUB_API_TOKEN'] = 'token'

# initialize HF LLM
flan_t5 = HuggingFaceHub(
    repo_id=&quot;google/flan-t5-xl&quot;,
    model_kwargs={&quot;temperature&quot;: 1e-10}
)

multi_template = &quot;&quot;&quot;Answer the following questions one at a time.

Questions:
{questions}

Answers:
&quot;&quot;&quot;
long_prompt = PromptTemplate(
    template=multi_template,
    input_variables=[&quot;questions&quot;]
)

llm_chain = LLMChain(
    prompt=long_prompt,
    llm=flan_t5
)

qs_str = (
    &quot;Which NFL team won the Super Bowl in the 2010 season?\n&quot; +
    &quot;If I am 6 ft 4 inches, how tall am I in centimeters?\n&quot; +
    &quot;Who was the 12th person on the moon?&quot; +
    &quot;How many eyes does a blade of grass have?&quot;
)

print(llm_chain.run(qs_str))

</code></pre>
<p>I am learning langchain,
on running above code, there has been indefinite halt and no response for minutes,</p>
<p>Can anyone tell why is it?
and what is to be corrected.</p>
<p>I expected that it will come up with answers to 4 questions asked, but there has been indefinite waiting to it.</p>
","huggingface"
"76256748","Huggingface, The illogical question why ""WordLevelTrainer can only train a WordLevel""?","2023-05-15 17:52:28","","0","153","<python><huggingface-tokenizers><huggingface>","<p>As part of an NLP course I was provided this code:</p>
<pre><code>MIN_FREQ = 3 # words appearing fewer than 3 times are treated as 'unknown'
unk_token = '[UNK]'
pad_token = '[PAD]'

tokenizer = Tokenizer(WordLevel(unk_token=unk_token))
tokenizer.pre_tokenizer = Whitespace()
tokenizer.normalizer = normalizers.Lowercase()

trainer = WordLevelTrainer(min_frequency=MIN_FREQ, special_tokens=[pad_token, unk_token])
tokenizer.train_from_iterator(train_data['text'], trainer=trainer)
</code></pre>
<p>And I wondered &quot;why is the trainer split from the model? why would you want to be able to train a model with the wrong trainer?&quot; so I did the only reasonable thing one has to do and changed the models and trainers.</p>
<p>Short story shorter, they only train their own respective models and error out otherwise.
eg: &quot;WordLevelTrainer can only train a WordLevel&quot;</p>
<p>Thus I ask again, why are they, interface-wise two different objects and are initialized in different parts of the code? what benefit is there other than my own perplexity about software engineering?</p>
","huggingface"
"76255032","HuggingFace/HfAgent AttributeError: module transformers.tools has no attribute DocumentQuestionAnsweringTool","2023-05-15 14:18:28","","0","712","<huggingface-transformers><huggingface>","<p>When trying to create an HfAgent, you get the error</p>
<p><code>AttributeError: module transformers.tools has no attribute DocumentQuestionAnsweringTool</code></p>
<p>from your .run request</p>
","huggingface"
"76253991","HuggingFace Accelerate Model not using GPU","2023-05-15 12:18:35","","1","1752","<python><torchvision><huggingface>","<p>I am attempting to use one of the HuggingFace models <a href=""https://huggingface.co/docs/accelerate/index"" rel=""nofollow noreferrer"">accelerate</a> and have followed to setup tutorial steps.</p>
<p>The issue i seem to be having is that i have used the accelerate config and set my machine to use my GPU, but after looking at the resource monitor my GPU usage is only at 7% i dont think my training is using my GPU at all, i have a 3090TI.</p>
<p>here is the command that i used to start my training</p>
<pre><code>accelerate launch train_unconditional.py
–train_data_dir=“data”
–resolution=256
–center_crop
–random_flip
–output_dir=“output-256”
–train_batch_size=2
–save_model_epochs=5
–num_epochs=100
–gradient_accumulation_steps=1
–use_ema
–learning_rate=1e-4
–lr_warmup_steps=500
–mixed_precision=no
</code></pre>
<p>Is there maybe a library that i am missing or some other config setting i need to change, because in the accelerate documentation i can only find info about multi gpu training.</p>
<p><a href=""https://i.sstatic.net/TwGIv.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/TwGIv.png"" alt=""image1"" /></a>
<a href=""https://i.sstatic.net/YmdLV.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/YmdLV.png"" alt=""image2"" /></a>
<a href=""https://i.sstatic.net/U4Zly.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/U4Zly.png"" alt=""image3"" /></a></p>
","huggingface"
"76238434","How to use activation checkpointing with a language model?","2023-05-12 16:46:41","","1","401","<hpc><pytorch-lightning><huggingface>","<p>I am using HuggingFace's Flan T5-base model (~220 million parameters) in pytorch lightning with <code>deepspeed</code>. I am getting out-of-memory errors (batch size = 1, with 32 steps of gradient accumulation). I want to use activation checkpointing to help with the issue. The <a href=""https://lightning.ai/docs/pytorch/stable/advanced/model_parallel.html#deepspeed-activation-checkpointing"" rel=""nofollow noreferrer"">documentation</a> provides the following as an example:</p>
<pre><code>from lightning.pytorch import Trainer 
import deepspeed
class MyModel(LightningModule):    
    def __init__(self):         
        super().__init__()         
        self.block_1 = nn.Sequential(nn.Linear(32, 32), nn.ReLU())         
        self.block_2 = torch.nn.Linear(32, 2)
    
    def forward(self, x):         
        # Use the DeepSpeed checkpointing function instead of calling the module directly         
        # checkpointing self.block_1 means the activations are deleted after use,         
        # and re-calculated during the backward passes         
        x = deepspeed.checkpointing.checkpoint(self.block_1, x)         
        return self.block_2(x)
</code></pre>
<p>This does not make it clear to me how to use activation checkpointing when using a language model. For example, my (oversimplified) code would look like this without checkpointing:</p>
<pre><code>from transformers import AutoModelForCausalLM
class MyModel(LightningModule):     ...
    def __init__(self):         
        super().__init__()         
        self.lm = AutoModelForCausalLM.from_pretrained('google/flan-t5-base')
    def forward(self, x):         
        return self.lm(**x)
</code></pre>
<p>How would I modify this to use deepspeed's checkpointing? The documentation specifically says don't wrap the entire model as <code>return deepspeed.checkpointing.checkpoint(self.lm, x.input_ids)</code>. So—is there a canonical way of using this checkpointing with a language model?</p>
","huggingface"
"76229875","Stable Diffusion Webui ConnectTimeoutError while starting","2023-05-11 16:42:33","","2","2965","<python><huggingface><stable-diffusion>","<p>I'm trying to set up stable diffusion on a server so that users can access it via RDP and generate what they need. The server in place I hosted on-premise and doesn't have any internet connection. I installed all needed libs via whl files. I was able to set up everything fine for my user, meaning when I run the webui-user.bat everything works fine.</p>
<p>As soon as a different user connects to the server and runs the webui-user.bat that startup runs till a certain point and stops with ConnectionTimeoutError. Here is the console output from the part where it fails.</p>
<pre><code>Launching Web UI with arguments: --no-gradio-queue -disable-nan-check --skip-install
No module 'xformers'. Procedding without it.
Loading weights [aba96b389d] from C:\AutomaticStableDiff\stable-diffusion-webui\models\Stable-diffusion\mdjrny-v4.safetensors
Creating model from config: C:\AutomaticStableDiff\stable-diffusion-webui\configs\v1-inference.yaml
LatentDiffusion: Running in eps-prediction mode
DiffusionWrapper has 859.52 M params.
'HTTPSConnectionPool(host='huggingface.co',port=443): Max retries exceeded with url: /openai/clip-vit-large-patch14/resolve/main/vocab.json (Caused by ConnectTimeoutError(urllib3.connection.HTTPSConnection object at 0x0000021B2E0C9DB0&gt;, 'Connection to huggingface.co timed out. (connect timeout=10)'))' thrown while requesting HEAD https://huggingface.co/openai/clip-vit-large-patch14/resolve/main/vocab.json
</code></pre>
<p>I ensured Python is installed and set up globaly on the server not only for my user. I don't get why the problem seems to be user-specific.</p>
<p>What I've tried so far is:</p>
<ul>
<li>Downgrading the request lib to 1.27.1</li>
<li>using the offline version of stable diffusion web ui <a href=""https://github.com/HotChocut/stable-diffusion-webui-offline-launch"" rel=""nofollow noreferrer"">https://github.com/HotChocut/stable-diffusion-webui-offline-launch</a></li>
</ul>
","huggingface"
"76223210","huggingface expected input format","2023-05-10 23:30:52","","2","451","<huggingface><gradio>","<p>I'm trying to invoke my test hello huggingface app via API:</p>
<pre><code>import gradio as gr

def greet(name):
    return &quot;Hello &quot; + name + &quot;!!&quot;

iface = gr.Interface(fn=greet, inputs=&quot;text&quot;, outputs=&quot;text&quot;)
iface.launch()
</code></pre>
<p>I'm able to do this by using the gradio client API:</p>
<pre><code>from gradio_client import Client

client = Client(&quot;https://toromanow-test2.hf.space/&quot;)
result = client.predict(
                &quot;John&quot;, # str representing input in 'name' Textbox component
                api_name=&quot;/predict&quot;
)
print(result)
</code></pre>
<p>But I'm unable to invoke it by submitting the POST request directly:</p>
<pre><code>curl -d '{ &quot;data&quot;: &quot;John&quot;}' -H &quot;Content-Type: application/json&quot; -X POST https://toromanow-test2.hf.space/api/predict
</code></pre>
<p>Errors out with:</p>
<pre><code>{&quot;detail&quot;:[{&quot;loc&quot;:[&quot;body&quot;,&quot;data&quot;],&quot;msg&quot;:&quot;value is not a valid list&quot;,&quot;type&quot;:&quot;type_error.list&quot;}]}
</code></pre>
<p>I've experimented with diferent formats but can't figure out the one what works.</p>
","huggingface"
"76201629","Fine-tuning of multilingual translation models (Huggingface Transformers, Helsinki)","2023-05-08 14:36:00","","2","600","<nlp><translation><multilingual><huggingface>","<p>I want to fine-tune pre-trained multilingual Models from the Huggingface transformers library (MarianMT in this case) for domain-specific translation. I want the models to be able to translate between 5 different languages. I have domain-specific datasets for every sentence pair (e.g. de-en, en-de, de-es, es-de and so on). In the available tutorials for fine-tuning I only could find fine-tuning for single language pairs (e.g. only the pretrained “Helsinki-NLP/opus-mt-en-roa” model is downloaded) which then needs to be fine-tuned on en-roa datasets. What I want to do is to train the whole multilingual model (not just en-roa). I want to mix the sentences of all sentence pairs of my datasets into one dataset and fine-tune the whole multilingual model on this large dataset. How can I achieve this task? Is it possible to download the “whole” model and not just the language pair models like en-roa? I hope someone can help me :)</p>
<p>Best regards,</p>
<p>Simon</p>
","huggingface"
"76197446","How to do model inference on a multimodal model from hugginface using sagemaker","2023-05-08 04:03:17","","2","421","<boto3><amazon-sagemaker><huggingface>","<p>I'm following this python notebook tutorial <a href=""https://github.com/huggingface/notebooks/blob/main/sagemaker/19_serverless_inference/sagemaker-notebook.ipynb"" rel=""nofollow noreferrer"">https://github.com/huggingface/notebooks/blob/main/sagemaker/19_serverless_inference/sagemaker-notebook.ipynb</a>
The tutorial works but I'm trying to change the model to a multimodal model that takes text and an image as an input.
After some research I've narrowed down my problem to the way that data is being sent most likley.
And it seems that I need some type of serializer.
I would appreciate any recommendations on how to proceed or what to try to deploy this model succesfully and run some inference.</p>
<pre><code>from sagemaker.huggingface import HuggingFaceModel
from sagemaker.serializers import JSONSerializer, BaseSerializer
from sagemaker.deserializers import JSONDeserializer, BaseDeserializer
from sagemaker.serverless import ServerlessInferenceConfig

from typing import Dict
import json
from io import BytesIO
from PIL import Image


class TextImageSerializer(BaseSerializer):
    CONTENT_TYPE = 'application/json'
    def __init__(self):
        self.json_serializer = JSONSerializer()

    def _text_input(self, data):
        return {'question': data}

    def _image_input(self, data):
        img = Image.open(BytesIO(data))
        buffer = BytesIO()
        img.save(buffer, format='PNG')
        return {'image': buffer.getvalue()}

    def serialize(self, data: Dict) -&gt; bytes:
        text_data = data.get('question')
        image_data = data.get('image')

        if text_data and not image_data:
            return self.json_serializer.serialize(self._text_input(text_data))
        elif image_data and not text_data:
            return self.json_serializer.serialize(self._image_input(image_data))
        else:
            raise ValueError(&quot;Data must contain either 'question' or 'image' keys, but not both.&quot;)

# Hub Model configuration. &lt;https://huggingface.co/models&gt;
hub = {
    'HF_MODEL_ID': 'naver-clova-ix/donut-base-finetuned-docvqa',
    'HF_TASK': 'question-answering'
}

huggingface_model = HuggingFaceModel(
   env=hub,                      
   role=role,                    
   transformers_version=&quot;4.17.0&quot;,  
   pytorch_version=&quot;1.10.2&quot;,      
   py_version='py38',       
)

serverless_config = ServerlessInferenceConfig(
    memory_size_in_mb=3072, max_concurrency=10,
)

predictor = huggingface_model.deploy(
    serializer=TextImageSerializer(),
    serverless_inference_config=serverless_config,
)

payload = {
    'question': &quot;what is the document title&quot;,
    'image': open(&quot;document.png&quot;, &quot;rb&quot;).read()
}

res = predictor.predict(data=payload)
</code></pre>
<p>I have tried using different serializers but am not sure if the serializer is the problem in the first place.</p>
","huggingface"
"76192459","How to convert pandas data frame to Huggingface Dataset grouped by column value?","2023-05-07 06:17:41","76192598","-1","265","<python><pandas><huggingface>","<p>I have the following data frame <code>df</code></p>
<pre><code>import pandas as pd
from datasets import Dataset

data = [[1, 'Jack', 'A'], [1, 'Jamie', 'A'], [1, 'Mo', 'B'], [1, 'Tammy', 'A'], [2, 'JJ', 'A'], [2, 'Perry', 'C']]
df = pd.DataFrame(data, columns=['id', 'name', 'class'])
&gt; df
  id   name class
0   1   Jack     A
1   1  Jamie     A
2   1     Mo     B
3   1  Tammy     A
4   2     JJ     A
5   2  Perry     C
</code></pre>
<p>I would like to covert it to a Dataset object that has 2 rows, one per <code>id</code>. The desired output is</p>
<pre><code>&gt; myDataset
Dataset({
    features: ['id', 'name', 'class'],
    num_rows: 2
})
</code></pre>
<p>where</p>
<pre><code>&gt; myDataset[0:2]
{'id': ['1', '2'], 'name': [['Jack', 'Jamie', 'Mo', 'Tammy'],['JJ', 'Perry']], 'class': [['A', 'A', 'B', 'A'], ['A', 'C']]}
</code></pre>
<p>Based on the documentation <a href=""https://huggingface.co/docs/datasets/main/en/loading#inmemory-data"" rel=""nofollow noreferrer"">here</a>, I tried the following but that gave me a Dataset with 6 rows, instead of one with 2 rows and grouped by the column <code>id</code></p>
<pre><code>myDataset = Dataset.from_pandas(df) 
&gt; myDataset
Dataset({
    features: ['id', 'name', 'class'],
    num_rows: 6
})
&gt; myDataste[0:2]
{'id': [1, 1], 'name': ['Jack', 'Jamie'], 'class': ['A', 'A']}
</code></pre>
","huggingface"
"76192134","getting ""memory_efficient_attention() got an unexpected keyword argument 'scale'"" on hugging face using the inference API of diffusion model?","2023-05-07 03:56:46","76194488","-1","1058","<huggingface><stable-diffusion>","<p><a href=""https://i.sstatic.net/4ybYt.png"" rel=""nofollow noreferrer"">Hugging face Error</a></p>
<p>i tried to enter different prompts and the same error keep showing up with different models</p>
","huggingface"
"76186015","How to denoise text using T5?","2023-05-05 21:24:02","","0","218","<pytorch><nlp><huggingface-transformers><huggingface><language-model>","<p>I'm trying to denoise text using a T5 model following <a href=""https://huggingface.co/docs/transformers/v4.28.1/en/model_doc/t5#transformers.T5ForConditionalGeneration"" rel=""nofollow noreferrer"">the Huggingface doc:</a></p>
<pre><code>from transformers import T5Tokenizer, T5ForConditionalGeneration
tokenizer = T5Tokenizer.from_pretrained(&quot;t5-small&quot;)
model = T5ForConditionalGeneration.from_pretrained(&quot;t5-small&quot;)

input_ids = tokenizer(&quot;The &lt;extra_id_0&gt; walks in &lt;extra_id_1&gt; park&quot;, return_tensors=&quot;pt&quot;).input_ids
labels = tokenizer(&quot;&lt;extra_id_0&gt; cute dog &lt;extra_id_1&gt; the &lt;extra_id_2&gt;&quot;, return_tensors=&quot;pt&quot;).input_ids

# the forward function automatically creates the correct decoder_input_ids
loss = model(input_ids=input_ids, labels=labels).loss
loss.item()
</code></pre>
<p>But I can't figure out how to get the actual text that corresponds to the masked input. They only show how to get the loss and mention</p>
<blockquote>
<p>the forward function automatically creates the correct
decoder_input_ids</p>
</blockquote>
<p>I tried the following:</p>
<pre><code>from transformers import T5Tokenizer, T5ForConditionalGeneration

tokenizer = T5Tokenizer.from_pretrained(&quot;t5-small&quot;)
model = T5ForConditionalGeneration.from_pretrained(&quot;t5-small&quot;)

input_ids = tokenizer(&quot;The &lt;extra_id_0&gt; walks in &lt;extra_id_1&gt; park&quot;, return_tensors=&quot;pt&quot;).input_ids
labels = tokenizer(&quot;&lt;extra_id_0&gt; cute dog &lt;extra_id_1&gt; the &lt;extra_id_2&gt;&quot;, return_tensors=&quot;pt&quot;).input_ids
outputs = model(input_ids=input_ids, labels=labels)
loss = outputs.loss
logits = outputs.logits
tokenizer.batch_decode(logits.argmax(-1))
</code></pre>
<p>But the output doesn't make sense:</p>
<pre><code>['&lt;extra_id_0&gt; park park&lt;extra_id_1&gt; the&lt;extra_id_2&gt; park']
</code></pre>
<p><strong>I don't care for the loss, nor do I have labels in my setting. I just have text with masked tokens that I need to fill:</strong></p>
<pre><code>my_masked_text = [
&quot;The kid went to the [MASK]&quot;,
&quot;The dog likes [MASK] and also [MASK]&quot;
]
</code></pre>
","huggingface"
"76178847","Loading Google Flan models through Langchain HuggingFacePipeline throws error of unrecognized configuration class for AutoModelForCausalLM","2023-05-05 04:28:11","","3","3282","<huggingface><py-langchain>","<p>Trying to load any Google Flan model through Langchain HuggingFacePipeline as shown below</p>
<pre><code>from langchain import HuggingFacePipeline
llm = HuggingFacePipeline.from_model_id(model_id=&quot;google/flan-t5-xl&quot;, task=&quot;text-generation&quot;, model_kwargs={&quot;temperature&quot;:0, &quot;max_length&quot;:64})
</code></pre>
<p>throws a ValueError as</p>
<blockquote>
<p>ValueError: Unrecognized configuration class &lt;class 'transformers.models.t5.configuration_t5.T5Config'&gt; for this kind of AutoModel: AutoModelForCausalLM.
Model type should be one of BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CamembertConfig, CodeGenConfig, CpmAntConfig, CTRLConfig, Data2VecTextConfig, ElectraConfig, ErnieConfig, GitConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, LlamaConfig, MarianConfig, MBartConfig, MegaConfig, MegatronBertConfig, MvpConfig, OpenAIGPTConfig, OPTConfig, PegasusConfig, PLBartConfig, ProphetNetConfig, QDQBertConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, Speech2Text2Config, TransfoXLConfig, TrOCRConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig.</p>
</blockquote>
<p>It flows through the following lines of code.
<a href=""https://i.sstatic.net/Qxodk.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Qxodk.png"" alt=""enter image description here"" /></a></p>
<p>It works fine if I load it through Langchain <code>HuggingFaceHub</code> though.</p>
<p>Any tips on fixing it while still using HuggingFacePipeline module is appreciated.</p>
","huggingface"
"76175069","Timm provides petrained models. On which dataset these weights are trained on?","2023-05-04 15:52:50","76178660","1","827","<python-3.x><pytorch><huggingface>","<p>How to find out on what dataset the petrained models provided from Timm are trained on?</p>
<pre><code>pretrained_resnet_34 = timm.create_model('resnet34', pretrained=True)
</code></pre>
<p><a href=""https://timm.fast.ai"" rel=""nofollow noreferrer"">https://timm.fast.ai</a></p>
<p><a href=""https://github.com/huggingface/pytorch-image-models#models"" rel=""nofollow noreferrer"">https://github.com/huggingface/pytorch-image-models#models</a></p>
","huggingface"
"76172336","How to collate a Dataset that might return different size encoding at every index?","2023-05-04 10:47:53","","1","79","<pytorch><pytorch-dataloader><huggingface>","<p>I want to apply a random augmentation transformation inside a Dataset and it may return encoding of different sizes at every index. Different size of encodings is produced by a processor from huggingface. How do I handle the dataset so that I get mini-batches of constant batch size while iterating using a dataloader?</p>
<p>Below is a snippet where I have put dummy variables to explain my case:</p>
<pre><code>from datasets import Dataset
import torch

class DummyDataset(Dataset):
    def __init__(self):
        # Each list of list in `x` encodings is an example output from a huggingface processor. This will be lazily generated in __getitem__ while reading millions of files from the disk. 
        self.x = [
            [[0, 1, 2], [3, 4, 5], [6, 7, 8]],
            [[0, 1, 2], [3, 4, 5]], 
            [[0, 1, 2]]
            ]

    def __len__(self):
        return len(self.x)
    def __getitem__(self, idx):
        return torch.Tensor(self.x[idx])
    
    
    

dataset = DummyDataset()    

from torch.utils.data import DataLoader

dataloader = DataLoader(dataset, batch_size=1, )



for minibatch in dataloader:
    print(minibatch)

&quot;&quot;&quot;
Actual Output: 

tensor([[[0., 1., 2.],
         [3., 4., 5.],
         [6., 7., 8.]]])
tensor([[[0., 1., 2.],
         [3., 4., 5.]]])
tensor([[[0., 1., 2.]]])
&quot;&quot;&quot;

&quot;&quot;&quot;
Expected Output when batchsize is 1:

tensor([[0, 1, 2]])
tensor([[3, 4, 5]])
tensor([[6, 7, 8]])
tensor([[0, 1, 2]])
tensor([[3, 4, 5]])
tensor([[0, 1, 2]])

&quot;&quot;&quot;
&quot;&quot;&quot;
Expected Output when batchsize is 2:

tensor([[0, 1, 2], [3, 4, 5]])
tensor([[6, 7, 8], [0, 1, 2]])
tensor([[3, 4, 5], [0, 1, 2]])


&quot;&quot;&quot;
</code></pre>
<p>Thanks in advance!</p>
","huggingface"
"76170604","Huggingface - Pipeline with a fine-tuned pre-trained model errors","2023-05-04 07:32:05","76273808","2","2290","<python><pipeline><huggingface-transformers><text-classification><huggingface>","<p>I have a pre-trained model from <code>facebook/bart-large-mnli</code> I used the Trainer in order to train it on my own dataset.</p>
<pre class=""lang-py prettyprint-override""><code>model = BartForSequenceClassification.from_pretrained(&quot;facebook/bart-large-mnli&quot;, num_labels=14, ignore_mismatched_sizes=True)
</code></pre>
<p>And then after I train it, I try to use the following (creating a pipeline with the fine-tuned model):</p>
<pre class=""lang-py prettyprint-override""><code># Import the Transformers pipeline library
from transformers import pipeline

# Initializing Zero-Shot Classifier
classifier = pipeline(&quot;zero-shot-classification&quot;, model=model, tokenizer=tokenizer, id2label=id2label)
</code></pre>
<p>I get the following error from it:</p>
<blockquote>
<p>Failed to determine 'entailment' label id from the label2id mapping in the model config. Setting to -1. Define a descriptive label2id mapping in the model config to ensure correct outputs.</p>
</blockquote>
<p>I tried searching the web for a solution but I can't find anything, you can refer to my previous question when I had trouble training it <a href=""https://stackoverflow.com/questions/76099140/hugging-face-transformers-bart-cuda-error-cublas-status-not-initialize"">here</a></p>
<hr />
<h1>How to solve the first error:</h1>
<p>Applying <a href=""https://stackoverflow.com/questions/76213873/how-to-finetune-a-zero-shot-model-for-text-classification/76213874#76213874"">this</a> solves the first error.</p>
<h1>Second error:</h1>
<p>I'm getting the following error:</p>
<pre class=""lang-py prettyprint-override""><code>RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper__index_select)
</code></pre>
<p>I tried deleting my custom metrics and it fixed it for a while but it didn't last, this error keeps coming.</p>
<p>The error is coming from here:</p>
<pre class=""lang-py prettyprint-override""><code>sequences = &quot;Some text sequence&quot;
classifier = pipeline(&quot;zero-shot-classification&quot;, model=model, tokenizer=tokenizer)
classifier(sequences, list(id2label.values()), multi_label=False)
# id2label is a dictionary mapping each label to its integer ID
</code></pre>
<p>I also tried <code>trainer.save_model(actual_model)</code> but it saved only some of the stuff and when I loaded it it was like I didn't train it at all.</p>
<hr />
<p>If I change the line to:</p>
<pre class=""lang-py prettyprint-override""><code>classifier = pipeline(&quot;zero-shot-classification&quot;, model=model, tokenizer=tokenizer) # OLD

classifier = pipeline(&quot;zero-shot-classification&quot;, model=model.to('cpu'), tokenizer=tokenizer) # NEW
</code></pre>
<p>It works fine, but if I change it to:</p>
<pre class=""lang-py prettyprint-override""><code>classifier = pipeline(&quot;zero-shot-classification&quot;, model=model.to('cuda'), tokenizer=tokenizer)
</code></pre>
<p>I get the same error too, my model was trained on a GPU cluster and Iw ant to test it as such, is it possible of am I missing something?</p>
<blockquote>
<p>From what I checked the option the <code>to</code> function can get are: <code>cpu, cuda, ipu, xpu, mkldnn, opengl, opencl, ideep, hip, ve, fpga, ort, xla, lazy, vulkan, mps, meta, hpu, privateuseone</code></p>
</blockquote>
","huggingface"
"76157465","How to do NLP fill-mask with restricted possible inputs","2023-05-02 17:33:50","","2","512","<nlp><mask><huggingface>","<p>I want to use NLP to fill in a masked word in a text, but instead of choosing from all possible words I want to find which is the more likely of two candidate words. For example, imagine I have a sentence &quot;The [MASK] was stuck in the tree&quot; and I want to evaluate whether &quot;kite&quot; or &quot;bike&quot; is the more likely word.</p>
<p>I know how to find the globally most probable words using hugging face's fill-mask pipeline</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import pipeline, AutoTokenizer, AutoModelForMaskedLM

# Define the input sentence with a masked word
input_text = &quot;The [MASK] was stuck in the tree&quot;

# Load the pre-trained model and tokenizer
model_name = &quot;bert-base-cased&quot;
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForMaskedLM.from_pretrained(model_name)

# Tokenize the input sentence
tokenized_text = tokenizer.tokenize(input_text)

# Use the pipeline to generate a list of predicted words and probabilities
mlm = pipeline(&quot;fill-mask&quot;, model=model, tokenizer=tokenizer)
results = mlm(input_text)

# Print outputs
for result in results:
    token = result[&quot;token_str&quot;]
    print(f&quot;{token:&lt;15} {result['score']}&quot;)
</code></pre>
<p>However if &quot;bike&quot; and &quot;kite&quot; arent in the first few most probable words this doesnt help.</p>
<p>How can I use fill-mask to find the probability of specific masks?</p>
<p>P.S. I'm not sure if overflow is the best place to post this question, there doesnt seem to be a place for nlp specific questions.</p>
","huggingface"
"76149173","How to use huggingface + gradio API in javascript","2023-05-01 18:14:17","","3","408","<javascript><huggingface><gradio>","<p>I'm going over the fastai ML course and got stuck in the second lesson with a minor problem  - a classifier app that works fine on huggingface spaces doesn't get called correctly by a script suggested by the course team. Here's the code:</p>
<p>Having trouble deploying the java script website, what could be wrong with the code:</p>
<pre><code>---
title: 1. Single file
layout: page
---

&lt;input id=&quot;photo&quot; type=&quot;file&quot;&gt;
&lt;div id=&quot;results&quot;&gt;&lt;/div&gt;
&lt;script&gt;
  async function loaded(reader) {
    const response = await fetch('https://hiddenmiddle-fastai-lesson2.hf.space/predict', {
      method: &quot;POST&quot;, body: JSON.stringify({ &quot;data&quot;: [reader.result] }),
      headers: { &quot;Content-Type&quot;: &quot;application/json&quot; }
    });
    const json = await response.json();
    const label = json['data'][0]['confidences'][0]['label'];
    results.innerHTML = `&lt;br/&gt;&lt;img src=&quot;${reader.result}&quot; width=&quot;300&quot;&gt; &lt;p&gt;${label}&lt;/p&gt;`
  }
  function read() {
    const reader = new FileReader();
    reader.addEventListener('load', () =&gt; loaded(reader))
    reader.readAsDataURL(photo.files[0]);
  }
  photo.addEventListener('input', read);
&lt;/script&gt;
</code></pre>
<p>What is the correct way to use gradio+huggingface API? When I click &quot;use via API&quot; on hugginface, there are only instructions for python</p>
","huggingface"
"76147208","How to post second question via POST call to hugginface chat?","2023-05-01 13:10:35","","1","79","<python><post><request><huggingface>","<p>So this code works pretty good I get first desired output to the <a href=""https://huggingface.co/chat/conversation/"" rel=""nofollow noreferrer"">https://huggingface.co/chat/conversation/</a>:</p>
<pre><code>from requests.sessions import Session
from json import loads

prompt = &quot;Explain first condition in english?&quot;

session = Session()
session.get(url=&quot;https://huggingface.co/chat/&quot;)
res = session.post(url=&quot;https://huggingface.co/chat/conversation&quot;)
assert res.status_code == 200, &quot;Failed to create new conversation&quot;
conversation_id = res.json()[&quot;conversationId&quot;]
url = f&quot;https://huggingface.co/chat/conversation/{conversation_id}&quot;
max_tokens = int(2000) - len(prompt)

if max_tokens &gt; 1904:
    max_tokens = 1904

res = session.post(
    url=url,
    json={
        &quot;inputs&quot;: prompt,
        &quot;parameters&quot;: {
            &quot;temperature&quot;: 0.5,
            &quot;top_p&quot;: 0.95,
            &quot;repetition_penalty&quot;: 1.2,
            &quot;top_k&quot;: 50,
            &quot;truncate&quot;: 1024,
            &quot;watermark&quot;: False,
            &quot;max_new_tokens&quot;: max_tokens,
            &quot;stop&quot;: [&quot;&lt;|endoftext|&gt;&quot;],
            &quot;return_full_text&quot;: False,
        },
        &quot;stream&quot;: False,
        &quot;options&quot;: {&quot;use_cache&quot;: False},
    },
    stream=False,
)
try:
    data = res.json()
except ValueError:
    print(&quot;Invalid JSON response&quot;)
    data = {}

data = data[0] if data else {}
data.get(&quot;generated_text&quot;, &quot;&quot;)
</code></pre>
<p>And it returns such output:</p>
<pre><code>'Sure! The first condition you mentioned ....  Is there something specific you would like me to explain about this condition?'
</code></pre>
<p>However I dont know how to send second request?</p>
<p>Next code completely  ruins the chat:</p>
<pre><code>res = session.post(
    url=url,
    json={
        &quot;inputs&quot;: &quot;GIVE more information&quot;,
        &quot;parameters&quot;: {
            &quot;temperature&quot;: 0.5,
            &quot;top_p&quot;: 0.95,
            &quot;repetition_penalty&quot;: 1.2,
            &quot;top_k&quot;: 50,
            &quot;truncate&quot;: 1024,
            &quot;watermark&quot;: False,
            &quot;max_new_tokens&quot;: max_tokens,
            &quot;stop&quot;: [&quot;&lt;|endoftext|&gt;&quot;],
            &quot;return_full_text&quot;: False,
        },
        &quot;stream&quot;: False,
        &quot;options&quot;: {&quot;use_cache&quot;: False},
    },
    stream=False,
)
try:
    data = res.json()
except ValueError:
    print(&quot;Invalid JSON response&quot;)
    data = {}
    
data.get(&quot;generated_text&quot;, &quot;&quot;)
</code></pre>
<p>Output:</p>
<pre><code>{'error': 'Model is overloaded', 'error_type': 'overloaded'}
</code></pre>
","huggingface"
"76144103","Couldn't find 'my_dataset' on the Hugging Face Hub","2023-05-01 00:02:46","76160174","0","1223","<huggingface><huggingface-datasets><huggingface-hub>","<p>I was following <a href=""https://huggingface.co/docs/datasets/upload_dataset"" rel=""nofollow noreferrer"">this huggingface tutorial</a> on uploading my dataset (a <code>json</code> file) to the Hub. In the link they mention:</p>
<blockquote>
<p>or text data extensions like .csv, .json, .jsonl, and .txt, we
recommend compressing them before uploading to the Hub (to .zip or .gz
file extension for example)</p>
</blockquote>
<p>So I converted my <code>json</code> file into a <code>gz</code> file. I uploaded it to the Hub in a public repo following their steps and under <code>Files and versions</code> I currently have 2 files:
<code>.gitattributes</code> and <code>train.gz</code>.</p>
<p>I try to load my dataset with</p>
<pre><code>from datasets import load_dataset
my_dataset = load_dataset('my_username/my_dataset')
</code></pre>
<p>But I'm getting the error</p>
<pre><code>FileNotFoundError: Couldn't find a dataset script at my_local_path or any data file in the same directory. Couldn't find 'my_username/my_dataset' on the Hugging Face Hub either: FileNotFoundError: Unable to find train.gz in dataset repository my_username/my_dataset with any supported extension ['csv', 'tsv', 'json', 'jsonl', 'parquet', 'txt', 'blp', 'bmp', 'dib', 'bufr', 'cur', 'pcx', 'dcx', 'dds', 'ps', 'eps', 'fit', 'fits', 'fli', 'flc', 'ftc', 'ftu', 'gbr', 'gif', 'grib', 'h5', 'hdf', 'png', 'apng', 'jp2', 'j2k', 'jpc', 'jpf', 'jpx', 'j2c', 'icns', 'ico', 'im', 'iim', 'tif', 'tiff', 'jfif', 'jpe', 'jpg', 'jpeg', 'mpg', 'mpeg', 'msp', 'pcd', 'pxr', 'pbm', 'pgm', 'ppm', 'pnm', 'psd', 'bw', 'rgb', 'rgba', 'sgi', 'ras', 'tga', 'icb', 'vda', 'vst', 'webp', 'wmf', 'emf', 'xbm', 'xpm', 'BLP', 'BMP', 'DIB', 'BUFR', 'CUR', 'PCX', 'DCX', 'DDS', 'PS', 'EPS', 'FIT', 'FITS', 'FLI', 'FLC', 'FTC', 'FTU', 'GBR', 'GIF', 'GRIB', 'H5', 'HDF', 'PNG', 'APNG', 'JP2', 'J2K', 'JPC', 'JPF', 'JPX', 'J2C', 'ICNS', 'ICO', 'IM', 'IIM', 'TIF', 'TIFF', 'JFIF', 'JPE', 'JPG', 'JPEG', 'MPG', 'MPEG', 'MSP', 'PCD', 'PXR', 'PBM', 'PGM', 'PPM', 'PNM', 'PSD', 'BW', 'RGB', 'RGBA', 'SGI', 'RAS', 'TGA', 'ICB', 'VDA', 'VST', 'WEBP', 'WMF', 'EMF', 'XBM', 'XPM', 'aiff', 'au', 'avr', 'caf', 'flac', 'htk', 'svx', 'mat4', 'mat5', 'mpc2k', 'ogg', 'paf', 'pvf', 'raw', 'rf64', 'sd2', 'sds', 'ircam', 'voc', 'w64', 'wav', 'nist', 'wavex', 'wve', 'xi', 'mp3', 'opus', 'AIFF', 'AU', 'AVR', 'CAF', 'FLAC', 'HTK', 'SVX', 'MAT4', 'MAT5', 'MPC2K', 'OGG', 'PAF', 'PVF', 'RAW', 'RF64', 'SD2', 'SDS', 'IRCAM', 'VOC', 'W64', 'WAV', 'NIST', 'WAVEX', 'WVE', 'XI', 'MP3', 'OPUS', 'zip']
</code></pre>
<p>Is there anything special I need to do to load it? Or do I need to add other files that they did not mention in order to load it?</p>
","huggingface"
"76137512","Langchain, Huggingface: Can't evaluate model with two different inputs","2023-04-29 17:28:08","76683808","3","3023","<python><huggingface><langchain>","<p>I'm evaluating a LLM on Huggingface using Langchain and Python using this code:</p>
<pre><code># https://github.com/hwchase17/langchain/blob/0e763677e4c334af80f2b542cb269f3786d8403f/docs/modules/models/llms/integrations/huggingface_hub.ipynb

from langchain import HuggingFaceHub, LLMChain
import os

hugging_face_write = &quot;MY_KEY&quot;
os.environ['HUGGINGFACEHUB_API_TOKEN'] = hugging_face_write

from langchain import PromptTemplate, HuggingFaceHub, LLMChain

template = &quot;&quot;&quot;Question: {question}

Answer: Let's think step by step.&quot;&quot;&quot;
prompt = PromptTemplate(template=template, input_variables=[&quot;question&quot;])
llm_chain = LLMChain(prompt=prompt, llm=HuggingFaceHub(repo_id=&quot;google/flan-t5-xl&quot;, model_kwargs={&quot;temperature&quot;:0, &quot;max_length&quot;:64}))

question = &quot;What NFL team won the Super Bowl in the year Justin Beiber was born?&quot;

print(llm_chain.run(question))

</code></pre>
<p>I get the error</p>
<pre><code>ValueError                                Traceback (most recent call last)
g:\Meine Ablage\python\lang_chain\langchain_huggingface_example.py in line 1
----&gt; 19 print(llm_chain.run(question))

File c:\Users\johan\.conda\envs\lang_chain\Lib\site-packages\langchain\chains\base.py:213, in Chain.run(self, *args, **kwargs)
    211     if len(args) != 1:
    212         raise ValueError(&quot;`run` supports only one positional argument.&quot;)
--&gt; 213     return self(args[0])[self.output_keys[0]]
    215 if kwargs and not args:
    216     return self(kwargs)[self.output_keys[0]]

File c:\Users\johan\.conda\envs\lang_chain\Lib\site-packages\langchain\chains\base.py:116, in Chain.__call__(self, inputs, return_only_outputs)
    114 except (KeyboardInterrupt, Exception) as e:
    115     self.callback_manager.on_chain_error(e, verbose=self.verbose)
--&gt; 116     raise e
    117 self.callback_manager.on_chain_end(outputs, verbose=self.verbose)
    118 return self.prep_outputs(inputs, outputs, return_only_outputs)

File c:\Users\johan\.conda\envs\lang_chain\Lib\site-packages\langchain\chains\base.py:113, in Chain.__call__(self, inputs, return_only_outputs)
    107 self.callback_manager.on_chain_start(
    108     {&quot;name&quot;: self.__class__.__name__},
    109     inputs,
    110     verbose=self.verbose,
    111 )
...
    106 if self.client.task == &quot;text-generation&quot;:
    107     # Text generation return includes the starter text.
    108     text = response[0][&quot;generated_text&quot;][len(prompt) :]

ValueError: Error raised by inference API: Model google/flan-t5-xl time out
</code></pre>
<p>What am I doing incorrectly? I'm a newbie...</p>
<p>Many thanks in advance, best regards from Paris,</p>
<p>Jennie</p>
<p>I ran my python script from above. After some waiting the shown error is given.</p>
","huggingface"
"76134574","Load dataset with datasets library of huggingface","2023-04-29 04:49:54","","1","718","<pytorch><huggingface><huggingface-datasets>","<p>I use load_dataset from huggingface library to load a jsonline dataset. Here's an example of the data point in the jsonline file:</p>
<pre><code>{&quot;tokens&quot;: [&quot;На&quot;, &quot;місці&quot;, &quot;трагедії&quot;, &quot;Безсмертний&quot;, &quot;заявив&quot;, &quot;,&quot;, &quot;що&quot;, &quot;«&quot;, &quot;нелюдські&quot;, &quot;вчинки&quot;, &quot;можуть&quot;, &quot;оцінюватися&quot;, &quot;лише&quot;, &quot;,&quot;, &quot;як&quot;, &quot;звірство&quot;, &quot;»&quot;, &quot;.&quot;, &quot;Нагадаємо&quot;, &quot;,&quot;, &quot;11&quot;, &quot;квітня&quot;, &quot;на&quot;, &quot;станції&quot;, &quot;метро&quot;, &quot;«&quot;, &quot;Жовтнева&quot;, &quot;»&quot;, &quot;у&quot;, &quot;Мінську&quot;, &quot;стався&quot;, &quot;вибух&quot;, &quot;,&quot;, &quot;в&quot;, &quot;результаті&quot;, &quot;якого&quot;, &quot;загинули&quot;, &quot;12&quot;, &quot;людей&quot;, &quot;,&quot;, &quot;більше&quot;, &quot;150&quot;, &quot;отримали&quot;, &quot;поранення&quot;, &quot;.&quot;, &quot;13&quot;, &quot;квітня&quot;, &quot;Лукашенко&quot;, &quot;заявив&quot;, &quot;про&quot;, &quot;розкриття&quot;, &quot;теракту&quot;, &quot;.&quot;, &quot;Інша&quot;, &quot;справа&quot;, &quot;,&quot;, &quot;що&quot;, &quot;немає&quot;, &quot;ясності&quot;, &quot;,&quot;, &quot;хто&quot;, &quot;за&quot;, &quot;цим&quot;, &quot;стоїть&quot;, &quot;.&quot;, &quot;Багато&quot;, &quot;хто&quot;, &quot;звертає&quot;, &quot;увагу&quot;, &quot;на&quot;, &quot;те&quot;, &quot;,&quot;, &quot;що&quot;, &quot;вибух&quot;, &quot;скоєно&quot;, &quot;неподалік&quot;, &quot;адміністрації&quot;, &quot;президента&quot;, &quot;.&quot;, &quot;Сам&quot;, &quot;Олександр&quot;, &quot;Лукашенко&quot;, &quot;учора&quot;, &quot;увечері&quot;, &quot;провів&quot;, &quot;термінову&quot;, &quot;нараду&quot;, &quot;і&quot;, &quot;наказав&quot;, &quot;знайти&quot;, &quot;тих&quot;, &quot;,&quot;, &quot;кому&quot;, &quot;потрібно&quot;, &quot;зруйнувати&quot;, &quot;стабільність&quot;, &quot;.&quot;], &quot;source_start&quot;: 31, &quot;source_end&quot;: 31, &quot;target_start&quot;: 73, &quot;target_end&quot;: 73, &quot;topic_id&quot;: &quot;255715&quot;, &quot;source_id&quot;: &quot;T10&quot;, &quot;target_id&quot;: &quot;T129&quot;, &quot;doc_ids&quot;: [0, 1], &quot;label&quot;: 1}
</code></pre>
<p>I get this error:</p>
<pre><code>ValueError: Couldn't cast
tokens: list&lt;item: string&gt;
  child 0, item: string
source_start: int64
source_end: int64
target_start: int64
target_end: int64
label: int64
to
{'tokens': Sequence(feature=Value(dtype='string', id=0), length=-1, id=None), 'label': Value(dtype='int32', id=1), 'source_start': Value(dtype='int32', id=2), 'source_end': Value(dtype='int32', id=3), 'target_start': Value(dtype='int32', id=3), 'target_end': Value(dtype='int32', id=4), 'topic_id': Value(dtype='string', id=5), 'doc_id': Sequence(feature=Value(dtype='int32', id=6), length=-1, id=None), 'source_id': Value(dtype='string', id=7), 'target_id': Value(dtype='string', id=8)}
because column names don't match
</code></pre>
<p>Here's the code to load the data:</p>
<pre><code>custom_features = Features(
    {
        &quot;tokens&quot;: Sequence(Value(&quot;string&quot;, id=0)),
        &quot;label&quot;: Value(&quot;int32&quot;, id=1),
        &quot;source_start&quot;: Value(&quot;int32&quot;, id=2),
        &quot;source_end&quot;: Value(&quot;int32&quot;, id=3),
        &quot;target_start&quot;: Value(&quot;int32&quot;, id=3),
        &quot;target_end&quot;: Value(&quot;int32&quot;, id=4),
        'topic_id': Value(&quot;string&quot;, id=5),
        'doc_id': Sequence(Value(&quot;int32&quot;, id=6)),
        'source_id': Value(&quot;string&quot;, id=7),
        'target_id': Value(&quot;string&quot;, id=8),
    }
)

raw_datasets = load_dataset('json', data_files={
    'train': args.train_file,
    'dev': args.dev_file,
    'test': args.test_file
},features=custom_features)
</code></pre>
<p>If I remove the fields &quot;topic_id, doc_id, source_id, target_id&quot;, the dataset is loaded correctly. However, I prefer to keep them in the jsonfile and just ignore them in the processed version of the dataset. Is there any solution for it?</p>
","huggingface"
"76133090","HuggingChat API?","2023-04-28 20:31:06","","1","2188","<python><chatbot><huggingface>","<p>I am trying to access HuggingChat using requests in Python but I am getting a response code of 500 for some reason. What I have so far is something like this:</p>
<pre><code>hf_url = &quot;https://huggingface.co/chat&quot;
resp = session.post(hf_url + f&quot;/conversation/{self.now_conversation}&quot;, json=req_json, stream=True)
</code></pre>
<p>The JSON objects holds the &quot;inputs&quot; field and other &quot;parameters&quot; such as temperature, top_p, etc.</p>
<pre><code>req_json = {
            &quot;inputs&quot;: text,
            &quot;parameters&quot;: {
                &quot;temperature&quot;: temperature,
                &quot;top_p&quot;: top_p,
...
</code></pre>
<p>Is there no API I can directly use? If not, is there a auth I am missing here? I inspected the request going from my browser when I chat with HuggingChat and didn't really find anything.</p>
","huggingface"
"76130589","What is the function of the `text_target` parameter in Huggingface's `AutoTokenizer`?","2023-04-28 14:27:23","76167575","4","6783","<python><huggingface-transformers><huggingface>","<p>I'm following the guide here: <a href=""https://huggingface.co/docs/transformers/v4.28.1/tasks/summarization"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/v4.28.1/tasks/summarization</a>
There is one line in the guide like this:</p>
<pre><code>labels = tokenizer(text_target=examples[&quot;summary&quot;], max_length=128, truncation=True)
</code></pre>
<p>I don't understand the function of the <code>text_target</code> parameter.</p>
<p>I tried the following code and the last two lines gave exactly the same results.</p>
<pre><code>from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained('t5-small')
text = &quot;Weiter Verhandlung in Syrien.&quot;
tokenizer(text_target=text, max_length=128, truncation=True)
tokenizer(text, max_length=128, truncation=True)
</code></pre>
<p>The docs just say <code>text_target (str, List[str], List[List[str]], optional) — The sequence or batch of sequences to be encoded as target texts.</code> I don't really understand. Is there some situations when setting  <code>text_target</code> will give you a different result?</p>
","huggingface"
"76110329","Iterating over LLM models does not work in LangChain","2023-04-26 11:43:51","","3","5330","<python><openai-api><huggingface><langchain>","<p>I am trying to instantiate LangChain LLM models and then iterate over them to see what they respond for same prompts.</p>
<pre><code>from langchain.llms import OpenAI, HuggingFaceHub
from langchain import PromptTemplate
from langchain import LLMChain
import pandas as pd

bool_score = False
total_score = 0
count = 0

template = &quot;{context} {prompt}&quot;
prompt = PromptTemplate(template=template, input_variables=['context', 'prompt'])

llms = [{'name': 'OpenAI', 'model': OpenAI(temperature=0)},
        {'name': 'Flan', 'model':  HuggingFaceHub(repo_id=&quot;google/flan-t5-xl&quot;, model_kwargs={&quot;temperature&quot;: 1e-10})}]

df = pd.read_excel(r'data/Test2.xlsx')

for llm_dict in llms:
    llm_name = llm_dict['name']
    llm_model = llm_dict['model']
    chain = LLMChain(llm=llm_model, prompt=prompt)

    df.reset_index()
    for index, row in df.iterrows():
        context = (row['Context']).replace(&quot;\n&quot;, &quot; &quot;)
        prompts = (row['Prompts']).split(&quot;\n&quot;)
        labels = (row['Labels']).split(&quot;\n&quot;)
        for prompt, label in zip(prompts, labels):
            print(f&quot;Context: {context}\nPrompt:{prompt}\nLabel: {label}&quot;)
            keywords = {'context': context, 'prompt': prompt}
            print(f&quot;Response: {chain.run(keywords).strip()}&quot;)
            if bool_score:
                str_score = input('Score? 0 for Wrong, 1 for Perfect : ')
                total_score += float(str_score)
                count += 1

    if count:
        print(f&quot;LLM score for {llm_name}: {total_score / count}&quot;)
</code></pre>
<p>The first LLM model runs well, but for the second iteration, gives following error:</p>
<pre><code>    chain = LLMChain(llm=llm_model, prompt=prompt)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;pydantic\main.py&quot;, line 341, in pydantic.main.BaseModel.__init__
pydantic.error_wrappers.ValidationError: 1 validation error for LLMChain
prompt
  value is not a valid dict (type=type_error.dict)
</code></pre>
<p>Am I missing something? in dictionary declarations?</p>
<p>Data file Test2.xls looks like:
<a href=""https://i.sstatic.net/N2Y84.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/N2Y84.png"" alt=""enter image description here"" /></a></p>
","huggingface"
"76099140","Hugging Face Transformers BART CUDA error: CUBLAS_STATUS_NOT_INITIALIZE","2023-04-25 08:36:20","76163186","3","1744","<python><pytorch><huggingface-transformers><text-classification><huggingface>","<p>I'm trying to finetune the Facebook BART model, I'm following <a href=""https://huggingface.co/transformers/v3.2.0/custom_datasets.html#seq-imdb"" rel=""nofollow noreferrer"">this article</a> in order to classify text using my own dataset.</p>
<p>And I'm using the Trainer object in order to train:</p>
<pre class=""lang-py prettyprint-override""><code>training_args = TrainingArguments(
    output_dir=model_directory,      # output directory
    num_train_epochs=1,              # total number of training epochs - 3
    per_device_train_batch_size=4,  # batch size per device during training - 16
    per_device_eval_batch_size=16,   # batch size for evaluation - 64
    warmup_steps=50,                # number of warmup steps for learning rate scheduler - 500
    weight_decay=0.01,               # strength of weight decay
    logging_dir=model_logs,          # directory for storing logs
    logging_steps=10,
)

model = BartForSequenceClassification.from_pretrained(&quot;facebook/bart-large-mnli&quot;) # bart-large-mnli

trainer = Trainer(
    model=model,                          # the instantiated 🤗 Transformers model to be trained
    args=training_args,                   # training arguments, defined above
    compute_metrics=new_compute_metrics,  # a function to compute the metrics
    train_dataset=train_dataset,          # training dataset
    eval_dataset=val_dataset              # evaluation dataset
)
</code></pre>
<p>This is the tokenizer I used:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import BartTokenizerFast
tokenizer = BartTokenizerFast.from_pretrained('facebook/bart-large-mnli')
</code></pre>
<p>But when I use <code>trainer.train()</code> I get the following:</p>
<p>Printing the following:</p>
<pre class=""lang-py prettyprint-override""><code>***** Running training *****
  Num examples = 172
  Num Epochs = 1
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed &amp; accumulation) = 16
  Gradient Accumulation steps = 1
  Total optimization steps = 11
</code></pre>
<p>Followed by this error:</p>
<pre class=""lang-py prettyprint-override""><code>RuntimeError: Caught RuntimeError in replica 1 on device 1.
Original Traceback (most recent call last):
  File &quot;/databricks/python/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py&quot;, line 61, in _worker
    output = module(*input, **kwargs)
  File &quot;/databricks/python/lib/python3.9/site-packages/torch/nn/modules/module.py&quot;, line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;/databricks/python/lib/python3.9/site-packages/transformers/models/bart/modeling_bart.py&quot;, line 1496, in forward
    outputs = self.model(
  File &quot;/databricks/python/lib/python3.9/site-packages/torch/nn/modules/module.py&quot;, line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;/databricks/python/lib/python3.9/site-packages/transformers/models/bart/modeling_bart.py&quot;, line 1222, in forward
    encoder_outputs = self.encoder(
  File &quot;/databricks/python/lib/python3.9/site-packages/torch/nn/modules/module.py&quot;, line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;/databricks/python/lib/python3.9/site-packages/transformers/models/bart/modeling_bart.py&quot;, line 846, in forward
    layer_outputs = encoder_layer(
  File &quot;/databricks/python/lib/python3.9/site-packages/torch/nn/modules/module.py&quot;, line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;/databricks/python/lib/python3.9/site-packages/transformers/models/bart/modeling_bart.py&quot;, line 323, in forward
    hidden_states, attn_weights, _ = self.self_attn(
  File &quot;/databricks/python/lib/python3.9/site-packages/torch/nn/modules/module.py&quot;, line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;/databricks/python/lib/python3.9/site-packages/transformers/models/bart/modeling_bart.py&quot;, line 191, in forward
    query_states = self.q_proj(hidden_states) * self.scaling
  File &quot;/databricks/python/lib/python3.9/site-packages/torch/nn/modules/module.py&quot;, line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;/databricks/python/lib/python3.9/site-packages/torch/nn/modules/linear.py&quot;, line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`
</code></pre>
<p>I've searched this site and GitHub and hugging face forum but still didn't find anything that helped me fix this for me (I tried adding more memory, lowering batches and warmup, restarting, specifying CPU or GPU, and more, but none worked for me)</p>
<h3>Databricks Clusters:</h3>
<ul>
<li><strong>Runtime:</strong> 12.2 LTS ML (includes Apache Spark 3.3.2, GPU, Scala 2.12)
<strong>Worker Type:</strong> Standard_NC24s_v3 with 4 GPUs, 2 to 10 workers, I think 16GB RAM and 448GB memory for the host</li>
<li><strong>Runtime:</strong> 12.1 ML (includes Apache Spark 3.3.1, Scala 2.12)
<strong>Worker Type:</strong> Standard_L8s (Memory optimized), 2 to 10 workers, 64GB memory with 8 cores</li>
</ul>
<p><strong>Update:</strong> With the second cluster, depending on the flag combination, I sometimes get the error <code>IndexError: Target {i} is out of bounds</code> where <code>i</code> change from time to time</p>
<blockquote>
<p>If you require any other information, comment and I'll add it up asap</p>
</blockquote>
<hr />
<p>My dataset is holding private information but here is an image of how it's built:
<a href=""https://i.sstatic.net/8zXnF.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/8zXnF.png"" alt=""dataset image"" /></a></p>
<hr />
<h2>Updates:</h2>
<p>I also tried setting the:</p>
<ul>
<li><code>fp16=True</code></li>
<li><code>gradient_checkpointing=True</code></li>
<li><code>gradient_accumulation_steps=4</code></li>
</ul>
<p>Flags but still had the same error when putting each separately and together</p>
<h4>Second cluster error (it get this error only sometimes, based on the flag combination):</h4>
<pre class=""lang-py prettyprint-override""><code>IndexError                                Traceback (most recent call last)
File &lt;command-2692616476221798&gt;:1
----&gt; 1 trainer.train()

File /databricks/python/lib/python3.9/site-packages/transformers/trainer.py:1527, in Trainer.train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)
   1522     self.model_wrapped = self.model
   1524 inner_training_loop = find_executable_batch_size(
   1525     self._inner_training_loop, self._train_batch_size, args.auto_find_batch_size
   1526 )
-&gt; 1527 return inner_training_loop(
   1528     args=args,
   1529     resume_from_checkpoint=resume_from_checkpoint,
   1530     trial=trial,
   1531     ignore_keys_for_eval=ignore_keys_for_eval,
   1532 )

File /databricks/python/lib/python3.9/site-packages/transformers/trainer.py:1775, in Trainer._inner_training_loop(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)
   1773         tr_loss_step = self.training_step(model, inputs)
   1774 else:
-&gt; 1775     tr_loss_step = self.training_step(model, inputs)
   1777 if (
   1778     args.logging_nan_inf_filter
   1779     and not is_torch_tpu_available()
   1780     and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))
   1781 ):
   1782     # if loss is nan or inf simply add the average of previous logged losses
   1783     tr_loss += tr_loss / (1 + self.state.global_step - self._globalstep_last_logged)

File /databricks/python/lib/python3.9/site-packages/transformers/trainer.py:2523, in Trainer.training_step(self, model, inputs)
   2520     return loss_mb.reduce_mean().detach().to(self.args.device)
   2522 with self.compute_loss_context_manager():
-&gt; 2523     loss = self.compute_loss(model, inputs)
   2525 if self.args.n_gpu &gt; 1:
   2526     loss = loss.mean()  # mean() to average on multi-gpu parallel training

File /databricks/python/lib/python3.9/site-packages/transformers/trainer.py:2555, in Trainer.compute_loss(self, model, inputs, return_outputs)
   2553 else:
   2554     labels = None
-&gt; 2555 outputs = model(**inputs)
   2556 # Save past state if it exists
   2557 # TODO: this needs to be fixed and made cleaner later.
   2558 if self.args.past_index &gt;= 0:

File /databricks/python/lib/python3.9/site-packages/torch/nn/modules/module.py:1190, in Module._call_impl(self, *input, **kwargs)
   1186 # If we don't have any hooks, we want to skip the rest of the logic in
   1187 # this function, and just call forward.
   1188 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1189         or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1190     return forward_call(*input, **kwargs)
   1191 # Do not call functions when jit is used
   1192 full_backward_hooks, non_full_backward_hooks = [], []

File /databricks/python/lib/python3.9/site-packages/transformers/models/bart/modeling_bart.py:1561, in BartForSequenceClassification.forward(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)
   1559 elif self.config.problem_type == &quot;single_label_classification&quot;:
   1560     loss_fct = CrossEntropyLoss()
-&gt; 1561     loss = loss_fct(logits.view(-1, self.config.num_labels), labels.view(-1))
   1562 elif self.config.problem_type == &quot;multi_label_classification&quot;:
   1563     loss_fct = BCEWithLogitsLoss()

File /databricks/python/lib/python3.9/site-packages/torch/nn/modules/module.py:1190, in Module._call_impl(self, *input, **kwargs)
   1186 # If we don't have any hooks, we want to skip the rest of the logic in
   1187 # this function, and just call forward.
   1188 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1189         or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1190     return forward_call(*input, **kwargs)
   1191 # Do not call functions when jit is used
   1192 full_backward_hooks, non_full_backward_hooks = [], []

File /databricks/python/lib/python3.9/site-packages/torch/nn/modules/loss.py:1174, in CrossEntropyLoss.forward(self, input, target)
   1173 def forward(self, input: Tensor, target: Tensor) -&gt; Tensor:
-&gt; 1174     return F.cross_entropy(input, target, weight=self.weight,
   1175                            ignore_index=self.ignore_index, reduction=self.reduction,
   1176                            label_smoothing=self.label_smoothing)

File /databricks/python/lib/python3.9/site-packages/torch/nn/functional.py:3026, in cross_entropy(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)
   3024 if size_average is not None or reduce is not None:
   3025     reduction = _Reduction.legacy_get_string(size_average, reduce)
-&gt; 3026 return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)

IndexError: Target 11 is out of bounds.
</code></pre>
<p>The number 11 changes from time to time.</p>
","huggingface"
"76089148","Export MarianMT model to ONNX","2023-04-24 06:37:20","","2","969","<huggingface-transformers><onnx><huggingface><onnxruntime>","<p>I would like to use the <code>Helsinki-NLP/opus-mt-de-en</code> model from HuggingFace to translate text.
This works fine with the HuggingFace Inference API or a Transformers pipeline, e.g.:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer, pipeline
from optimum.onnxruntime import ORTModelForSeq2SeqLM

model = ORTModelForSeq2SeqLM.from_pretrained(&quot;Helsinki-NLP/opus-mt-de-en&quot;, from_transformers=True)
tokenizer = AutoTokenizer.from_pretrained(&quot;Helsinki-NLP/opus-mt-de-en&quot;)

onnx_translation = pipeline(&quot;translation_de_to_en&quot;, model=model, tokenizer=tokenizer)

result = onnx_translation(&quot;Dies ist ein Test!&quot;)
result # Prints: &quot;[{'translation_text': 'This is a test!'}]&quot;
</code></pre>
<p>However, I need to use the ONNX Runtime for this as part of a project. I was able to successfully export the model to ONNX format, but I get the following output when I decode the output of the <code>InferenceSession</code>:</p>
<pre><code>&lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt;.&lt;unk&gt; &lt;unk&gt; &lt;unk&gt;,&lt;unk&gt;,&lt;unk&gt;,.&lt;unk&gt; &lt;unk&gt;,&lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt;.&lt;unk&gt; &lt;unk&gt; &lt;unk&gt;,&lt;unk&gt; &lt;unk&gt; &lt;unk&gt;. the,&lt;unk&gt;,,.&lt;unk&gt; &lt;unk&gt; &lt;unk&gt;,&lt;unk&gt;, the&lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt;.&lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt;.&lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt;.&lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; the.. in&lt;unk&gt; &lt;unk&gt;.&lt;unk&gt; &lt;unk&gt;,&lt;unk&gt; &lt;unk&gt; &lt;unk&gt; the&lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt;.&lt;unk&gt; the&lt;unk&gt; &lt;unk&gt; the&lt;unk&gt; &lt;unk&gt; &lt;unk&gt;.&lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt;,&lt;unk&gt; in&lt;unk&gt; the&lt;unk&gt;,&lt;unk&gt;,&lt;unk&gt; &lt;unk&gt; in, the in&lt;unk&gt; &lt;unk&gt; &lt;unk&gt; s&lt;unk&gt;. the.&lt;unk&gt; &lt;unk&gt;, in,&lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt;. a,,&lt;unk&gt; &lt;unk&gt; &lt;unk&gt;.&lt;unk&gt;.&lt;unk&gt; &lt;unk&gt;.&lt;unk&gt; is&lt;unk&gt;,&lt;unk&gt; in,,&lt;unk&gt; &lt;unk&gt; the&lt;unk&gt; the the. in&lt;unk&gt; &lt;unk&gt; &lt;unk&gt;,&lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt;. in&lt;unk&gt;,,,&lt;unk&gt;.&lt;unk&gt; &lt;unk&gt;. of&lt;unk&gt; in&lt;unk&gt;.&lt;unk&gt;,&lt;unk&gt; &lt;unk&gt; &lt;unk&gt; the&lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; die,.&lt;unk&gt;,&lt;unk&gt; &lt;unk&gt; &lt;unk&gt; die,&lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; the&lt;unk&gt;.&lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; of.&lt;unk&gt; &lt;unk&gt; &lt;unk&gt;.&lt;unk&gt; in&lt;unk&gt;, the&lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt;.&lt;unk&gt; &lt;unk&gt;,.&lt;unk&gt; &lt;unk&gt;,&lt;unk&gt;,&lt;unk&gt; &lt;unk&gt;,&lt;unk&gt; &lt;unk&gt;,&lt;unk&gt; &lt;unk&gt; &lt;unk&gt;,&lt;unk&gt;,&lt;unk&gt; &lt;unk&gt; &lt;unk&gt;,&lt;unk&gt;.&lt;unk&gt; of&lt;unk&gt;.&lt;unk&gt; of, the&lt;unk&gt; the.&lt;unk&gt; &lt;unk&gt;
</code></pre>
<p>This is the relevant code (without the ONNX export):</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer

# Load Tokenizer
tokenizer = AutoTokenizer.from_pretrained(&quot;Helsinki-NLP/opus-mt-de-en&quot;)
# Encode input
encoded_input = tokenizer(&quot;Dies ist ein Test!&quot;)
# Create model input dictionary
model_input = {
    'input_ids': [encoded_input.input_ids],
    'attention_mask': [encoded_input.attention_mask],
    'decoder_input_ids': [encoded_input.input_ids],
    'decoder_attention_mask': [encoded_input.attention_mask]
}
# Run inference
output = session.run(['last_hidden_state'], model_input)
last_hidden_state = output[0][0][0]
# Decode output
decoded_output = tokenizer.decode(last_hidden_state, skip_special_tokens=True)
decoded_output # Expected value: &quot;This is a test!&quot;
</code></pre>
<p>The complete code can be found in this <a href=""https://colab.research.google.com/drive/1muduFNDxmRDJ_rFSnCvqRQ6afyCB6Zkm?usp=sharing"" rel=""nofollow noreferrer"">Colab Notebook</a> as reproducible example.</p>
<p>I don't have much experience with ONNX and the MarianMT models yet.
What am I doing wrong and how can I decode the text correctly?</p>
","huggingface"
"76085472","Can't find Huggingface npm packages for node.js?","2023-04-23 14:42:10","","2","263","<node.js><huggingface><distilbert>","<p>I am trying to execute a code in node.js using DistilBERT and need to install the below dependencies. They don't seem to work. Anyone knows?</p>
<p>npm install @huggingface/transformers
npm install @huggingface/tokenizers</p>
","huggingface"
"76084214","What is recommended number of threads for pytorch based on available CPU cores?","2023-04-23 10:02:53","","13","3274","<python><pytorch><nlp><huggingface-transformers><huggingface>","<p>First I want to say that I don't have much experience with pytorch, ML, NLP and other related topics, so I may confuse some concepts. Sorry.</p>
<p>I downloaded few models from Hugging Face, organized them in one Python script and started to perform benchmark to get overview of performance. During benchmark I monitored CPU usage and saw that only 50% of CPU was used. I have 8 vCPU, but only 4 of them are loaded at 100% at the same time. The load is jumping, i.e. there may be cores 1, 3, 5, 7 that are loaded at 100%, then cores 2, 4, 6, 8 that are loaded at 100%. But in total CPU load never raises above 50%, it also never goes below 50%. This 50% load is constant.</p>
<p>After quick googling I found <a href=""https://pytorch.org/docs/stable/torch.html#parallelism"" rel=""noreferrer"">parallelism doc</a>. I called <code>get_num_threads()</code> and <code>get_num_interop_threads()</code> and output was <code>4</code> for both calls. Only 50% of available CPU cores which kind of explains why CPU load was at 50%.</p>
<p>Then I called <code>set_num_threads(8)</code> and <code>set_num_interop_threads(8)</code>, and then performed benchmark. CPU usage was at constant 100%. In general performance was a bit faster, but some models started to work a bit slowly than at 50% of CPU.</p>
<p>So I wonder why pytorch by default uses only half of CPU? It is optimal and recommended way? Should I manually call <code>set_num_threads()</code> and <code>set_num_interop_threads()</code> with all available CPU cores if I want to achieve best performance?</p>
<p>Edit.</p>
<p>I made an additional benchmarks:</p>
<ul>
<li>one pytorch process with 50% of vCPU is a bit faster than one pytorch process with 100% of vCPU. Earlier it was vice versa, so I think it depends on models that are being used.</li>
<li>two pytorch concurrent processes with 50% of vCPU will handle more inputs than one pytorch process with 50% of vCPU, but it is not 2x increase, it is ~1.2x increase. Process time of one input is much slower than with one pytorch process.</li>
<li>two pytroch concurrent processes with 100% of vCPU can't complete even one input. I guess CPU is constantly switching between these processes.</li>
</ul>
<p>So thank you to Phoenix's answer, I think it is completely reasonable to use pytorch default settings which sets number of threads according to number of physical (not virtual) cores.</p>
<p>Edit.</p>
<p>pytorch documentation about this - <a href=""https://pytorch.org/docs/stable/notes/cpu_threading_torchscript_inference.html"" rel=""noreferrer"">https://pytorch.org/docs/stable/notes/cpu_threading_torchscript_inference.html</a></p>
","huggingface"
"76074982","Parallelize inference with huggingface using torch","2023-04-21 16:15:54","76085163","1","604","<pytorch><parallel-processing><out-of-memory><batch-processing><huggingface>","<p>I am running an inference model on a Ubuntu machine with 8GB only and just realised the predictions (logits) are not generated in a batch way so my process is getting Killed due oom issues.</p>
<pre><code>tokenized_test = tokenizer(dataset[&quot;test&quot;][&quot;text&quot;], padding=True, truncation=True, return_tensors=&quot;pt&quot;)

with torch.no_grad():
    logits = model(**tokenized_test).logits
</code></pre>
<p>This is where I run out of memory. What is the best way of do this in batches/parallelize/sequentiate/solve the oom issue. I am ultimately looking for the solution that would require the least amount of code changes.</p>
<hr />
<h3>Source</h3>
<p>I have built my code based on this tutorial:</p>
<p><a href=""https://huggingface.co/docs/transformers/tasks/sequence_classification"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/tasks/sequence_classification</a></p>
<p>Increasing the dataset size will eventually make you go oom to.</p>
","huggingface"
"76066751","error deploying dolly-v2-12b on Amazon SageMaker","2023-04-20 17:24:13","","0","266","<python-3.x><amazon-web-services><amazon-sagemaker><huggingface>","<p>I'm trying to get <a href=""https://huggingface.co/databricks/dolly-v2-12b"" rel=""nofollow noreferrer"">https://huggingface.co/databricks/dolly-v2-12b</a> setup and am having some difficulty.</p>
<p>I clicked on Deploy -&gt; Amazon SageMaker and, in the resultant window, I set the Task to &quot;Text Generation&quot; and the Configuration to &quot;Local Machine&quot;. I take the resultant python script that it gives me, I replace <code>{IAM_ROLE_WITH_SAGEMAKER_PERMISSIONS}</code> with AmazonSageMaker-ExecutionPolicy-20230418T130702 (the policy that got created for me when I tried to create a new domain using the Quick setup and the SageMaker Compute Role) and I then tried to run the Python script but doing so yields me a long delay (maybe a few minutes? idk - I haven't measured it) before giving me me this error:</p>
<pre><code>------------!Traceback (most recent call last):
  File &quot;test.py&quot;, line 28, in &lt;module&gt;
    predictor.predict({
  File &quot;/home/scbn/.local/lib/python3.8/site-packages/sagemaker/predictor.py&quot;, line 161, in predict
    response = self.sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)
  File &quot;/home/scbn/.local/lib/python3.8/site-packages/botocore/client.py&quot;, line 530, in _api_call
    return self._make_api_call(operation_name, kwargs)
  File &quot;/home/scbn/.local/lib/python3.8/site-packages/botocore/client.py&quot;, line 960, in _make_api_call
    raise error_class(parsed_response, operation_name)
botocore.errorfactory.ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from primary with message &quot;{
  &quot;code&quot;: 400,
  &quot;type&quot;: &quot;InternalServerException&quot;,
  &quot;message&quot;: &quot;\u0027gpt_neox\u0027&quot;
}
&quot;. See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/huggingface-pytorch-inference-2023-04-20-17-06-52-252 in account 636601586062 for more information.
</code></pre>
<p>Here's a screenshot of the role in AWS's IAM manager:</p>
<p><a href=""https://i.sstatic.net/LCvoF.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/LCvoF.jpg"" alt=""enter image description here"" /></a></p>
<p>Here's what I get if I click on &quot;AmazonSageMaker-ExecutionPolicy-20230418T130702&quot;:</p>
<pre><code>{
    &quot;Version&quot;: &quot;2012-10-17&quot;,
    &quot;Statement&quot;: [
        {
            &quot;Effect&quot;: &quot;Allow&quot;,
            &quot;Action&quot;: [
                &quot;s3:GetObject&quot;,
                &quot;s3:PutObject&quot;,
                &quot;s3:DeleteObject&quot;,
                &quot;s3:ListBucket&quot;
            ],
            &quot;Resource&quot;: [
                &quot;arn:aws:s3:::*&quot;
            ]
        }
    ]
}
</code></pre>
<p>Any ideas?</p>
","huggingface"
"76052537","AttributeError: 'CLIPVisionModelWithProjection' object has no attribute 'get_image_features'","2023-04-19 08:53:28","","0","1158","<python-3.x><huggingface-transformers><huggingface><stable-diffusion>","<p>I am working on a stable diffusion model while implementing a simple code example given below in the link. I have followed the same steps mentioned in the link, but still it gives an error</p>
<pre><code>https://replicate.com/lambdal/stable-diffusion-image-variation
</code></pre>
<p><strong>Code</strong></p>
<pre><code>from pathlib import Path
from lambda_diffusers import StableDiffusionImageEmbedPipeline

from PIL import Image
import torch

#device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;
device = &quot;cpu&quot;
pipe = StableDiffusionImageEmbedPipeline.from_pretrained(&quot;lambdalabs/sd-image-variations-diffusers&quot;)
pipe = pipe.to(device)

im = Image.open(&quot;/home/cvpr/Desktop/bird.jpg&quot;)
num_samples = 4
image = pipe(num_samples*[im], guidance_scale=3.0)
image = image[&quot;sample&quot;]

base_path = Path(&quot;outputs/im2im&quot;)
base_path.mkdir(exist_ok=True, parents=True)
for idx, im in enumerate(image):
    im.save(base_path/f&quot;{idx:06}.jpg&quot;)
</code></pre>
<p><strong>Trackback</strong></p>
<pre><code>Traceback (most recent call last):
  File &quot;/media/cvpr/CM_1/lambda-diffusers/image_variation.py&quot;, line 46, in &lt;module&gt;
    image = pipe(num_samples*[im], guidance_scale=3.0)
  File &quot;/media/cvpr/CM_1/lambda-diffusers/.venv/lib/python3.8/site-packages/torch/utils/_contextlib.py&quot;, line 115, in decorate_context
    return func(*args, **kwargs)
  File &quot;/media/cvpr/CM_1/lambda-diffusers/lambda_diffusers/pipelines/pipeline_stable_diffusion_im_embed.py&quot;, line 77, in __call__
    image_embeddings = self.image_encoder.get_image_features(**input_image)
  File &quot;/media/cvpr/CM_1/lambda-diffusers/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py&quot;, line 1614, in __getattr__
    raise AttributeError(&quot;'{}' object has no attribute '{}'&quot;.format(
AttributeError: 'CLIPVisionModelWithProjection' object has no attribute 'get_image_features'
</code></pre>
","huggingface"
"76052083","Is it possible to use Tiktoken's ck_100k_base Tokenizer in HuggingFace's pipeline?","2023-04-19 08:05:30","","3","1241","<nlp><huggingface-tokenizers><huggingface>","<p>I can use <a href=""https://github.com/openai/tiktoken"" rel=""nofollow noreferrer"">Tiktoken</a>'s <code>ck_100k_base</code> Tokenizer to encode text data.</p>
<pre><code>import tiktoken
enc = tiktoken.get_encoding(&quot;ck_100k_base&quot;)
ids = enc.encode_ordinary('hello world')
print(ids)
</code></pre>
<p>which will tokenized output:</p>
<pre><code>[15339, 1917]
</code></pre>
<p>While in HuggingFace, I use <code>bert-base-uncased</code> as a tokenizer:</p>
<pre><code>from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')

def preprocess_dataset(examples):
    inputs = [prefix + example[source_lang] for example in examples[&quot;translation&quot;]]
    targets = [example[target_lang] for example in examples[&quot;translation&quot;]]
    model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True)
    return model_inputs

source_lang = &quot;en&quot;
target_lang = &quot;fr&quot;
prefix = &quot;Translate English to French: &quot;
tokenized = my_dataset.map(preprocess_dataset, batched=True)
</code></pre>
<p>My question is, how to use <code>tiktoken</code>'s <code>ck_100k_base</code> to replace BERT as tokenizer in HuggingFace's environment?</p>
","huggingface"
"76048867","BLIP encoder-based","2023-04-18 20:38:17","","0","232","<huggingface-transformers><caption><huggingface>","<p>I’m wanting to use BLIP for image captioning. How can I ensure that captions are generated by an encoder and not decoder? I’ve been using the huggingface model:<a href=""https://huggingface.co/docs/transformers/model_doc/blip"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/model_doc/blip</a></p>
<p>Thanks!</p>
<p>I tried setting is_decoder=False in BlipConfigText to configure the model however, I can’t get my model to train.</p>
","huggingface"
"76045605","Using a custom trained huggingface tokenizer","2023-04-18 14:05:38","76058017","4","1774","<python><huggingface-transformers><huggingface-tokenizers><huggingface><huggingface-hub>","<p>I’ve trained a custom tokenizer using a custom dataset using this <a href=""https://huggingface.co/docs/tokenizers/quicktour"" rel=""nofollow noreferrer"">code</a> that’s on the documentation. Is there a method for me to add this tokenizer to the hub and to use it as the other tokenizers by calling the AutoTokenizer.from_pretrained() function? If I can’t do that how can I use the tokenizer to train a custom model from scratch?
Thanks for your help!!!</p>
<p>Here's the code below:</p>
<pre><code>from tokenizers import Tokenizer
from tokenizers.models import BPE
tokenizer = Tokenizer(BPE(unk_token=&quot;[UNK]&quot;))

from tokenizers.trainers import BpeTrainer
trainer = BpeTrainer(special_tokens=[&quot;[UNK]&quot;, &quot;[CLS]&quot;, &quot;[SEP]&quot;, &quot;[PAD]&quot;, &quot;[MASK]&quot;])

from tokenizers.pre_tokenizers import Whitespace
tokenizer.pre_tokenizer = Whitespace()

folder = 'dataset_unicode'
files = [f&quot;/content/drive/MyDrive/{folder}/{split}.txt&quot; for split in [&quot;test&quot;, &quot;train&quot;, &quot;valid&quot;]]
tokenizer.train(files, trainer)

from tokenizers.processors import TemplateProcessing
tokenizer.post_processor = TemplateProcessing(
    single=&quot;[CLS] $A [SEP]&quot;,
    pair=&quot;[CLS] $A [SEP] $B:1 [SEP]:1&quot;,
    special_tokens=[
        (&quot;[CLS]&quot;, tokenizer.token_to_id(&quot;[CLS]&quot;)),
        (&quot;[SEP]&quot;, tokenizer.token_to_id(&quot;[SEP]&quot;)),
    ],
)

# I've tried saving it like this but it doesn't work as I expect it:
tokenizer.save(&quot;data/tokenizer-custom.json&quot;)
</code></pre>
","huggingface"
"76040575","Does huggingface have a model that is based on word-level tokens?","2023-04-18 02:11:32","","0","994","<nlp><huggingface-transformers><huggingface-tokenizers><huggingface>","<p>I'm trying to do token classification, but interested in doing it for word-level and not sub-words/etc.</p>
<p>I currently have:</p>
<pre><code>models_name = 'distilbert-base-cased'
tokenizer = AutoTokenizer.from_pretrained(models_name, model_max_length=512, truncation=True, padding=True)
token_classification_model = AutoModelForTokenClassification.from_pretrained(models_name, num_labels=4).to(device)
</code></pre>
<p>But this is on a sub-word level</p>
","huggingface"
"76025069","Unable to import transformers.models.bert.modeling_tf_bert on macOS?","2023-04-15 22:40:02","","1","3414","<tensorflow><huggingface-transformers><huggingface-tokenizers><huggingface><nlp-question-answering>","<p>As the title is self-descriptive, I'm not able to import the <code>BertTokenizer</code> and <code>TFBertModel</code> classes from the <code>transformers</code> package through the following code:</p>
<pre><code>from transformers import BertTokenizer, TFBertModel

tokenizer = BertTokenizer.from_pretrained(BERT_PATH)
model = TFBertModel.from_pretrained(BERT_PATH)
text = &quot;Replace me by any text you'd like.&quot;
encoded_input = tokenizer(text, return_tensors='tf')
resp = model(encoded_input)
print(resp)
</code></pre>
<p>As a result, I'm getting the following error:</p>
<pre><code>RuntimeError: Failed to import transformers.models.bert.modeling_tf_bert because of the following error (look up to see its traceback):
dlopen(/Users/tk/miniforge3/envs/QA-benchmark/lib/python3.10/site-packages/tensorflow-plugins/libmetal_plugin.dylib, 0x0006): symbol not found in flat namespace '_TF_GetInputPropertiesList'
</code></pre>
<p>Here is my software stack:</p>
<pre><code>OS: macOS Ventura 13.3.1
Python: 3.10
TensorFlow: macOS-tensorflow 2.9.0
Transformers: 4.28.0
BERT model: uncased_L-12_H-768_A-12
</code></pre>
<p>p.s. I've already posted this issue on the GitHub repository of transformers.</p>
","huggingface"
"76020641","Huggingface - time out or connection error","2023-04-15 06:09:37","","2","1426","<python><huggingface>","<p>I want to use LLaMA and OPT via the Hugging Face API. However, I always get the same two error messages. Either</p>
<p>ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))</p>
<p>Or</p>
<p>{'error': 'Model decapoda-research/llama-65b-hf time out'}</p>
<p>This is my code:</p>
<pre><code>import requests

API_URL = &quot;https://api-inference.huggingface.co/models/decapoda-research/llama-65b-hf&quot;
headers = {&quot;Authorization&quot;: ###}

def query(payload):
    response = requests.post(API_URL, headers=headers, json=payload)
    return response.json()

output = query({&quot;inputs&quot;: &quot;Once upon a time &quot;,&quot;options&quot;:{&quot;wait_for_model&quot;:True},})

print(output)
</code></pre>
<p>What am I doing wrong or what can I do to actually access the models?</p>
","huggingface"
"76014701","How to avoid adding double start of token in TrOCR finetune model","2023-04-14 11:37:17","76044662","1","208","<python><deep-learning><pytorch><huggingface-transformers><huggingface>","<p><strong>Describe the bug</strong>
The model I am using (TrOCR Model):</p>
<p>The problem arises when using:</p>
<ul>
<li>[x] the official example scripts: done by the nice tutorial  <a href=""https://github.com/NielsRogge/Transformers-Tutorials/blob/master/TrOCR/Fine_tune_TrOCR_on_IAM_Handwriting_Database_using_Seq2SeqTrainer.ipynb"" rel=""nofollow noreferrer"">(fine_tune)</a> @NielsRogge</li>
<li>[x] my own modified scripts: (as the script below )</li>
</ul>
<pre><code>processor = TrOCRProcessor.from_pretrained(&quot;microsoft/trocr-large-handwritten&quot;)

class Dataset(Dataset):
    def __init__(self, root_dir, df, processor, max_target_length=128):
        self.root_dir = root_dir
        self.df = df
        self.processor = processor
        self.max_target_length = max_target_length

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        # get file name + text 
        file_name = self.df['file_name'][idx]
        text = self.df['text'][idx]
        # prepare image (i.e. resize + normalize)
        image = Image.open(self.root_dir + file_name).convert(&quot;RGB&quot;)
        pixel_values = self.processor(image, return_tensors=&quot;pt&quot;).pixel_values
        # add labels (input_ids) by encoding the text
        labels = self.processor.tokenizer(text, 
                                          padding=&quot;max_length&quot;,
                                                         max_length=self.max_target_length).input_ids
        # important: make sure that PAD tokens are ignored by the loss function
        labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]
        # encoding  
        return {&quot;pixel_values&quot;: pixel_values.squeeze(), &quot;labels&quot;: torch.tensor(labels)}

model = VisionEncoderDecoderModel.from_pretrained(&quot;microsoft/trocr-large-handwritten&quot;)
model.config.decoder_start_token_id = processor.tokenizer.cls_token_id
model.config.pad_token_id = processor.tokenizer.pad_token_id
model.config.vocab_size = model.config.decoder.vocab_size

model.config.eos_token_id = processor.tokenizer.sep_token_id


# python3 train.py path/to/labels  path/to/images/
</code></pre>
<ul>
<li>Platform: Linux  Ubuntu  distribution  [GCC 9.4.0] on Linux</li>
<li>PyTorch version (GPU?):  0.8.2+cu110</li>
<li>transformers: 4.22.2</li>
<li>Python version:3.8.10</li>
</ul>
<p>A clear and concise description of what the bug is.
To <strong>Reproduce</strong> Steps to reproduce the behavior:</p>
<ol>
<li>After training the model or during the training phase  when evaluating metrics calculate I see the model added the double start of token <code>&lt;s&gt;&lt;s&gt;</code> or ids <code>[0,0, ......,2,1,1, 1 ]</code></li>
<li>here is an example during the <code>training</code> phase the show generated tokens in compute_metrics
Input predictions: <code>[[0,0,506,4422,8046,2,1,1,1,1,1]]</code>
Input references: <code>[[0,597,2747 ...,1,1,1]] </code></li>
<li>Other examples during <code>testing</code> models [<a href=""https://i.sstatic.net/sWzbf.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/sWzbf.png"" alt=""enter image description here"" /></a>]</li>
</ol>
<p><strong>Expected behavior</strong> A clear and concise description of what you expected to happen.
In 2 reproduced problems:
I am expecting during training Input predictions: <code>[[,0,,506,4422,8046,2,1,1,1,1,1 ]] </code></p>
<p>In addition during the testing phase:  generated text without double <strong></strong>
<code>tensor([[0,11867,405,22379,1277,..........,368,2]]) </code></p>
<p><code>&lt;s&gt;ennyit erről, tőlem fényképezz amennyit akarsz, a véleményem akkor&lt;/s&gt;</code></p>
","huggingface"
"76012700","validation loss shows 'no log' during fine-tuning model","2023-04-14 07:46:44","","2","1243","<validation><logging><pre-trained-model><huggingface><fine-tuning>","<p>I'm finetuning QA models from hugging face pretrained models using huggingface Trainer, during the training process, the validation loss doesn't show. My compute_metrices function returns accuracy and f1 score, which doesn't show in the log as well.</p>
<p>here is my code for trainer set up:</p>
<pre><code>args = TrainingArguments(
    output_dir=&quot;./result_albert_nontracking&quot;,
    evaluation_strategy=&quot;steps&quot;,
    save_strategy=&quot;epoch&quot;,
    max_steps=10000,  
    do_train=True,
    do_eval=True,
    warmup_steps=500,
    num_train_epochs=3,
    weight_decay=0.01,
    learning_rate=5e-5,
    logging_dir='./logs',
    logging_steps=500,
    eval_steps=500,

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=train_dataset,
    eval_dataset=validation_dataset,
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)
trainer.train()
</code></pre>
<p>also have setup logger by:</p>
<pre><code>logger = logging.get_logger(__name__)
logger.setLevel(logging.DEBUG)
</code></pre>
<p>here is what I got:
<img src=""https://i.sstatic.net/LdFja.png"" alt=""enter image description here"" /></p>
<p>I've tried different checking point ('bert-base-cased','albert-base-v2','roberta-base'), and got the same 'no log'. Any one know what is the problem with that? Thanks in advance!</p>
","huggingface"
"76011298","HuggingFace Trainer max_step to set for streaming dataset","2023-04-14 03:02:22","","0","896","<huggingface><huggingface-datasets><huggingface-trainer>","<p>The <code>max_steps</code> argument of <a href=""https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.max_steps"" rel=""nofollow noreferrer"">TrainingArguments</a> is <code>num_rows_in_train / per_device_train_batch_size * num_train_epochs</code> when using streaming datasets of Huggingface?</p>
<ul>
<li>num_rows_in_train is total number of records in the training dataset</li>
<li>per_device_train_batch_size is the batch size</li>
<li>num_train_epochs is the number of epochs to run</li>
</ul>
<p>As in <a href=""https://discuss.huggingface.co/t/streaming-dataset-into-trainer-does-not-implement-len-max-steps-has-to-be-specified/32893/5"" rel=""nofollow noreferrer"">Streaming dataset into Trainer: does not implement <strong>len</strong>, max_steps has to be specified</a>, training with a streaming dataset requires <code>max_steps</code> instead of <code>num_train_epochs</code>.</p>
<p>According to the documents, it is set to <strong>the total number of training steps</strong> which should be number of total mini-batches.</p>
<ul>
<li><a href=""https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.max_steps"" rel=""nofollow noreferrer"">max_steps</a></li>
</ul>
<blockquote>
<p>If set to a positive number, <strong>the total number of training steps</strong> to perform. Overrides num_train_epochs.</p>
</blockquote>
<p>For a small dataset of 2048 rows in the train split, set the training arguments as below.</p>
<pre><code>training_args = TrainingArguments(
    output_dir=&quot;bloom_finetuned&quot;,
    max_steps=2048 * 3,
    num_train_epochs=3,
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    learning_rate=2e-5,
    weight_decay=0.01, 
    fp16=True,
    no_cuda=False,
    evaluation_strategy=&quot;epoch&quot;,
    save_strategy=&quot;epoch&quot;,
)
</code></pre>
<p>However, the training shows huge number of epochs.</p>
<pre><code>***** Running training *****
  Num examples = 6,144
  Num Epochs = 9,223,372,036,854,775,807      &lt;-----
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed &amp; accumulation) = 1
  Gradient Accumulation steps = 1
  Total optimization steps = 6,144
  Number of trainable parameters = 559,214,592
</code></pre>
","huggingface"
"75993532","AttributeError: 'Seq2SeqTrainer' object has no attribute 'push_in_progress'","2023-04-12 08:45:54","","0","393","<python><huggingface>","<p>I'm using HuggingFace's <code>Seq2SeqTrainer</code> and I successfully trained a model. When I try to execute (where trainer is an instance of <code>Seq2SeqTrainer</code>):</p>
<pre><code>trainer.push_to_hub()
</code></pre>
<p>It returns error:</p>
<blockquote>
<p>AttributeError: 'Seq2SeqTrainer' object has no attribute 'push_in_progress'</p>
</blockquote>
<p>Trainer Code:</p>
<pre><code>trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized[&quot;train&quot;],
    eval_dataset=tokenized[&quot;test&quot;],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)
trainer.train()
trainer.push_to_hub()
</code></pre>
<p>How can I resolve this problem?</p>
<p>Other codes can be found in my other <a href=""https://stackoverflow.com/questions/75945735/xlnet-or-bert-chinese-for-huggingface-automodelforseq2seqlm-training"">question</a>.</p>
","huggingface"
"75987248","Black images or memory issue with Hugging Face StableDiffusion pipleline, M1 Pro, PyTorch","2023-04-11 14:49:48","","2","1060","<python><pytorch><apple-m1><huggingface-transformers><huggingface>","<p>So I'm making a project for a school that offers image generation using stable diffusion. It was working perfectly fine until I upgraded the Pytorch version for &quot;stabilityai/stable-diffusion-x4-upscaler&quot; model. Since then all the other images generated are black because they get flagged NSFW.</p>
<p>The code hasn't changed but I have updated Mac OS to 13.3.1 recently and I tested many different versions of PyTorch, Diffusers, and Transformers, but I can't get it to work as it did before. Since the code didn't change the problem has to do with the package version.</p>
<p>With the latest PyTorch 2.0 I am able to generate working images but I cannot use <code>torch_dtype=torch.float16</code> in the pipeline since it's not supported and I seem to be getting the following insufficient memory issues now.</p>
<p><code>RuntimeError: MPS backend out of memory (MPS allocated: 18.04 GB, other allocations: 94.99 MB, max allowed: 18.13 GB). Tried to allocate 4.00 KB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure). INFO:     Stopping reloader process [15702]</code></p>
<p>These are the following models and piplines I used.</p>
<pre class=""lang-py prettyprint-override""><code>main_model_id = &quot;runwayml/stable-diffusion-v1-5&quot;
inpainting_model_id = &quot;runwayml/stable-diffusion-inpainting&quot;
upscaler_model_id = &quot;stabilityai/stable-diffusion-x4-upscaler&quot;

text2imgPipe = StableDiffusionPipeline.from_pretrained(main_model_id, torch_dtype=torch.float16).to(
    device)
text2imgPipe.enable_attention_slicing()

img2imgPipe = StableDiffusionImg2ImgPipeline.from_pretrained(main_model_id, torch_dtype=torch.float16).to(
    device)
img2imgPipe.enable_attention_slicing()

inpaintingPipe = StableDiffusionInpaintPipeline.from_pretrained(inpainting_model_id, torch_dtype=torch.float16).to(
    device)
inpaintingPipe.enable_attention_slicing()

upscalerPipe = StableDiffusionUpscalePipeline.from_pretrained(upscaler_model_id, torch_dtype=torch.float16).to(
    device)
upscalerPipe.enable_attention_slicing()
</code></pre>
<p>These are the current package version I have.</p>
<pre class=""lang-bash prettyprint-override""><code>transformers              4.26.0
torchaudio                2.0.0
torchvision               0.14.1
pytorch                   2.0.0
diffusers                 0.12.0
huggingface-hub           0.13.4
python                    3.10.10
</code></pre>
","huggingface"
"75981648","Roberta transformer for ner gives index out of range error","2023-04-11 00:34:54","","1","187","<python><named-entity-recognition><huggingface><roberta>","<p>I have a function below that tokenizes and aligns my labels, but it is giving me an error:</p>
<pre><code>def tokenize_and_align_labels(examples, label_all_tokens=True): 
    tokenized_inputs = tokenizer(examples[&quot;tokens&quot;], truncation=True, is_split_into_words=True) 
    labels = [] 
    for i, label in enumerate(examples[&quot;ner_tags&quot;]): 
        word_ids = tokenized_inputs.word_ids(batch_index=i) 
        # word_ids() =&gt; Return a list mapping the tokens
        # to their actual word in the initial sentence.
        # It Returns a list indicating the word corresponding to each token. 
        previous_word_idx = None 
        label_ids = []
        # Special tokens like `&lt;s&gt;` and `&lt;\s&gt;` are originally mapped to None 
        # We need to set the label to -100 so they are automatically ignored in the loss function.
        for word_idx in word_ids: 
            if word_idx is None: 
                # set –100 as the label for these special tokens
                label_ids.append(-100)
            # For the other tokens in a word, we set the label to either the current label or -100, depending on
            # the label_all_tokens flag.
            elif word_idx != previous_word_idx:
                # if current word_idx is != prev then its the most regular case
                # and add the corresponding token                 
                label_ids.append(label[word_idx]) 
            else: 
                # to take care of sub-words which have the same word_idx
                # set -100 as well for them, but only if label_all_tokens == False
                label_ids.append(label[word_idx] if label_all_tokens else -100) 
                # mask the subword representations after the first subword
                 
            previous_word_idx = word_idx 
        labels.append(label_ids) 
    tokenized_inputs[&quot;labels&quot;] = labels 
    return tokenized_inputs 
</code></pre>
<p>I located the line causing the error:</p>
<pre><code>word_ids = tokenized_inputs.word_ids(batch_index=1)
</code></pre>
<p>This is the error produced:</p>
<p><a href=""https://i.sstatic.net/r5etF.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/r5etF.png"" alt=""Error from running the code"" /></a></p>
<p>My tokenized inputs, if run separately, without calling the function work fine as shown in the pic:
<a href=""https://i.sstatic.net/xRah7.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/xRah7.png"" alt=""Tokenized input without calling the function"" /></a></p>
<p>Can anyone please help me with the error? I spent 3 hours on this and it isn't working. Thanks!</p>
<p>For better explanation, here's the colab file too:<a href=""https://colab.research.google.com/drive/1UJtc8TcuyCyFURKM1txYsqF1WKG_H6jZ#scrollTo=wc6AA6FMqDNq&amp;uniqifier=1"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1UJtc8TcuyCyFURKM1txYsqF1WKG_H6jZ#scrollTo=wc6AA6FMqDNq&amp;uniqifier=1</a></p>
","huggingface"
"75976909","Avoiding Trimmed Summaries of a PEGASUS-Pubmed huggingface summarization model","2023-04-10 12:00:02","75977633","1","96","<pytorch><nlp><huggingface-transformers><huggingface-tokenizers><huggingface>","<p>I am new to huggingface.
I am using PEGASUS - Pubmed huggingface model to generate summary of the reserach paper. Following is the code for the same. the model gives a trimmed summary.
Any way of avoiding the trimmed summaries and getting more concrete results in summarization.?</p>
<p>Following is the code that I tried.</p>
<pre class=""lang-py prettyprint-override""><code>#Loading Pubmed Dataset for Scientifc Articles

dataset_pubmed = load_dataset(&quot;scientific_papers&quot;,&quot;pubmed&quot;)

#Taking piece of  Train Dataset

sample_dataset = dataset_pubmed[&quot;train&quot;]
sample_dataset

#Taking first two articles of Train Dataset
sample_dataset = sample_dataset['article'][:2]
sample_dataset

###Import PegasusModel and Tokenizer

from transformers import pipeline, PegasusTokenizer, PegasusForConditionalGeneration


model = PegasusForConditionalGeneration.from_pretrained('google/pegasus-pubmed')
tokenizer =PegasusTokenizer.from_pretrained('google/pegasus-pubmed')

summerize_pipe = pipeline(&quot;summarization&quot;, model=model, tokenizer=tokenizer)
pipe_out = summerize_pipe(sample_dataset, truncation=True)
pipe_out
</code></pre>
<p>As a results of this one of the summary output i get is as follows. The last sentence is not complete it gets trimmed for all the papers. How to avoid this.?</p>
<p><code>[{'summary_text': &quot;background : in iran a national free food program ( nffp ) is implemented in elementary schools of deprived areas to cover all poor students . however , this program is not conducted in slums and poor areas of the big cities so many malnourished children with low socio - economic situation are not covered by nffp . therefore , the present study determines the effects of nutrition intervention in an advocacy process model on the prevalence of underweight in school aged children in the poor area of shiraz , iran.materials and methods : this interventional study has been carried out between 2009 and 2010 in shiraz , iran . in those schools all students ( 2897 , 7 - 13 years old ) were screened based on their body mass index ( bmi ) by nutritionists . according to convenience method all students divided to two groups based on their economic situation ; family revenue and head of household 's job and nutrition situation ; the first group were poor and malnourished students and the other group were well nourished or well - off students . for this report , the children 's height and weight were entered into center for disease control and prevention ( cdc ) to calculate bmi and bmi - for -&quot;}</code></p>
","huggingface"
"75966325","Predicting with model gives different predictions for same inputs and loading the model throws ""doesnt save pytorch_model.bin cannot be opened"" error","2023-04-08 16:10:38","","1","645","<python><machine-learning><huggingface-transformers><huggingface><machine-learning-model>","<p>I am predicting my model like this:</p>
<pre><code>def predict_label(text,username):
    # input_ids = torch.tensor(tokenizer.encode(text)).unsqueeze(0)
    model=getmodelfromusername(username)
    input_ids=tokenizer(text, padding=True, truncation=True, max_length=500, return_tensors=&quot;pt&quot;)
    logits = model(**input_ids)[0]
    probs = torch.nn.functional.softmax(logits, dim=1)
    
    return probs
</code></pre>
<p>I am training like this:</p>
<pre><code>def train_and_update_model(model, parseddata, code, username,number):

    optimizer = torch.optim.AdamW(model.parameters(), lr=4e-5)

    lr_scheduler = get_scheduler(
        name=&quot;linear&quot;, optimizer=optimizer, num_warmup_steps=0, num_training_steps=2
    )

    input_ids = torch.tensor([tokenizer.encode(str(parseddata), add_special_tokens=True)])
    labels = torch.tensor([number])

    # del banmodel

    model.train(mode=True)

    for i in range(2):
        outputs = model(input_ids, labels=labels)
        loss = outputs[0]
        loss.backward()
        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()

    with lock:
        model.save_pretrained(username + &quot;/CustomModel&quot;)

    with lock:
        model = BertForSequenceClassification.from_pretrained(username + '/CustomModel')
    changemodelwithcode(code, model)
</code></pre>
<p>In my application sometimes I'm gonna need to predict a lot of text that is incoming and also at the same time I need to be able to train at any moment. However, I also get this error when I try to do that:</p>
<pre><code>at: model.save_pretrained(username + &quot;/CustomModel&quot;)
RuntimeError: File test/CustomModel\pytorch_model.bin cannot be opened.
</code></pre>
<p>Any help would be extremely appreciated thanks</p>
","huggingface"
"75959314","Huggingface model training loop has same performance on CPU & GPU? Confused as to why?","2023-04-07 14:26:17","75980417","1","1069","<python><pytorch><huggingface-transformers><huggingface>","<h2>Question</h2>
<p>I created two Python notebooks to fine-tune BERT on a Yelp review dataset for sentiment analysis. The only difference between the two notebooks is that <a href=""https://nbviewer.org/gist/AlanCPSC/f263426dce9b7b9580cd48dbc45910b0"" rel=""nofollow noreferrer"">one runs on a CPU</a> <code>.to(&quot;cpu&quot;)</code> while the <a href=""https://nbviewer.org/gist/AlanCPSC/7f0efaeaf9b5247a80e67c9a8e0d9c8b"" rel=""nofollow noreferrer"">other uses a GPU</a> <code>.to(&quot;cuda&quot;)</code>.</p>
<p>Despite this difference in hardware, the training times for both notebooks are nearly the same. I am new to using Hugging Face, so I'm wondering if there's anything I might be overlooking. Both notebooks are running on a machine with a single GPU.</p>
<h2>Metrics for CPU</h2>
<pre><code>TrainOutput(global_step=100, training_loss=1.5707407319545745, metrics={'train_runtime': 116.5447, 'train_samples_per_second': 3.432, 'train_steps_per_second': 0.858, 'total_flos': 105247256985600.0, 'train_loss': 1.5707407319545745, 'epoch': 0.4})

{'eval_loss': 1.4039757251739502,
 'eval_accuracy': 0.4,
 'eval_runtime': 3.6833,
 'eval_samples_per_second': 27.15,
 'eval_steps_per_second': 3.529,
 'epoch': 0.4}

# specifically concerned with 'train_samples_per_second': 3.432
</code></pre>
<h2>Metrics for GPU</h2>
<pre><code>TrainOutput(global_step=100, training_loss=1.6277318179607392, metrics={'train_runtime': 115.46, 'train_samples_per_second': 3.464, 'train_steps_per_second': 0.866, 'total_flos': 105247256985600.0, 'train_loss': 1.6277318179607392, 'epoch': 0.4})

{'eval_loss': 1.525576114654541,
 'eval_accuracy': 0.35,
 'eval_runtime': 3.6518,
 'eval_samples_per_second': 27.384,
 'eval_steps_per_second': 3.56,
 'epoch': 0.4}

# specifically concerned with 'train_samples_per_second': 3.464
</code></pre>
","huggingface"
"75952444","Huggingface Transformers (PyTorch) - Custom training loop doubles speed?","2023-04-06 17:58:30","","1","1264","<pytorch><huggingface-transformers><huggingface><gpt-2><huggingface-datasets>","<p>I've found something quite strange when using Huggingface Transformers with a custom training loop in PyTorch.</p>
<p>But first, some context: I'm currently trying to fine tune a pretrained GPT2 small (GPT2LMHeadModel; the ~170M param version) on multiple nodes, using Huggingface Accelerate. I'm using Huggingface's <code>datasets</code> library for training.</p>
<p>Of course, the first step in this process in accelerate is to write a custom PyTorch training loop, which I did with the help of <a href=""https://www.youtube.com/watch?v=Dh9CL8fyG80&amp;embeds_euri=https%3A%2F%2Fhuggingface.co%2F&amp;feature=emb_title"" rel=""nofollow noreferrer"">the official tutorial from huggingface</a>. Naturally, I decided to test the model with this new training loop before implementing accelerate to ensure it actually worked.</p>
<p>Here's the relevant code from my original model, as well as the corresponding code from the new training loop:</p>
<p><em>Note: <code>BATCH_SIZE</code> is equal to 2 in both models. All code not shown is exactly the same between both models.</em></p>
<p>Original:</p>
<pre><code>data = data['train']

dc = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)

train_args = TrainingArguments(
    output_dir=OUTPUT_DIRECTORY,
    overwrite_output_dir=True,
    num_train_epochs=1,
    per_device_train_batch_size=BATCH_SIZE,
    save_steps=10_000,
    save_total_limit=1, # How many &quot;checkpoints&quot; to save at a time
    prediction_loss_only=True,
    remove_unused_columns=False,
    optim=&quot;adamw_torch&quot;
)

trainer = Trainer(
    model=model,
    args=train_args,
    data_collator=dc,
    train_dataset=data
)

trainer.train()
</code></pre>
<p>Custom Train Loop:</p>
<pre><code>data = data['train']

dc = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)

optimizer = AdamW(model.parameters(), lr=5e-5)

train_dl = DataLoader(
    data, shuffle=True, batch_size=BATCH_SIZE, collate_fn=dc
)

epochs = 1
training_steps = epochs * len(train_dl)
scheduler = get_scheduler(
    &quot;linear&quot;,
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=training_steps
)

progress_bar = tqdm(range(training_steps))

model.train()
for epoch in range(epochs):
    for batch in train_dl:
        # Run a batch through the model
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
</code></pre>
<p>I tested it (with one node of course) with two GPUs, both 16GB each. And it worked... but suspiciously well.</p>
<ul>
<li>My original model averaged about 1-2 iterations/s.</li>
<li>My custom loop on the other hand averaged about 3-4 iterations/s.</li>
</ul>
<p>This is absolutely bizarre. How is it possible that simply adding my own training loop, that's just a couple of lines of code, is not only faster than the official one provided by Huggingface - but nearly TWICE as fast? Did I write the training loop incorrectly? Am I completely missing something here?</p>
","huggingface"
"75948679","DeBERTa ONNX export does not work for token_type_ids","2023-04-06 10:53:39","76209715","1","739","<pytorch><huggingface-transformers><onnx><huggingface>","<p>I have noticed that <a href=""https://github.com/huggingface/transformers/pull/17617"" rel=""nofollow noreferrer"">this PR</a> already adds the support for DeBERTa kind of a model to be exported as ONNX. I read through the PR and checked everything possible. However I can't make the following code work.</p>
<pre><code>from transformers import AutoTokenizer, AutoConfig, DebertaTokenizerFast, pipeline, DebertaV2Tokenizer, __version__
from optimum.onnxruntime import ORTModelForTokenClassification, ORTModelForQuestionAnswering

tokenizer = AutoTokenizer.from_pretrained(&quot;{custom_fine_tuned_NER_DeBERTaV2}&quot;)
model = ORTModelForTokenClassification.from_pretrained(&quot;{custom_fine_tuned_NER_DeBERTaV2}&quot;, 
                                                                                  export=True, use_auth_token=True)

pipe = pipeline(&quot;ner&quot;, model=model, tokenizer=tokenizer)
pipe(&quot;MY TEXT GOES HERE&quot;)
</code></pre>
<p>It fails with the following stack trace -</p>
<pre><code>---------------------------------------------------------------------------
InvalidArgument                           Traceback (most recent call last)
Cell In[25], line 1
----&gt; 1 pipe(&quot;I am a skilled engineer. I have worked in JS, CPP, Java, J2ME, and Python. I know Oracle and MySQL&quot;)

File ~/skill_extraction/skill_extraction/lib/python3.10/site-packages/transformers/pipelines/token_classification.py:214, in TokenClassificationPipeline.__call__(self, inputs, **kwargs)
    211 if offset_mapping:
    212     kwargs[&quot;offset_mapping&quot;] = offset_mapping
--&gt; 214 return super().__call__(inputs, **kwargs)

File ~/skill_extraction/skill_extraction/lib/python3.10/site-packages/transformers/pipelines/base.py:1109, in Pipeline.__call__(self, inputs, num_workers, batch_size, *args, **kwargs)
   1101     return next(
   1102         iter(
   1103             self.get_iterator(
   (...)
   1106         )
   1107     )
   1108 else:
-&gt; 1109     return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)

File ~/skill_extraction/skill_extraction/lib/python3.10/site-packages/transformers/pipelines/base.py:1116, in Pipeline.run_single(self, inputs, preprocess_params, forward_params, postprocess_params)
   1114 def run_single(self, inputs, preprocess_params, forward_params, postprocess_params):
   1115     model_inputs = self.preprocess(inputs, **preprocess_params)
-&gt; 1116     model_outputs = self.forward(model_inputs, **forward_params)
   1117     outputs = self.postprocess(model_outputs, **postprocess_params)
   1118     return outputs

File ~/skill_extraction/skill_extraction/lib/python3.10/site-packages/transformers/pipelines/base.py:1015, in Pipeline.forward(self, model_inputs, **forward_params)
   1013     with inference_context():
   1014         model_inputs = self._ensure_tensor_on_device(model_inputs, device=self.device)
-&gt; 1015         model_outputs = self._forward(model_inputs, **forward_params)
   1016         model_outputs = self._ensure_tensor_on_device(model_outputs, device=torch.device(&quot;cpu&quot;))
   1017 else:

File ~/skill_extraction/skill_extraction/lib/python3.10/site-packages/transformers/pipelines/token_classification.py:240, in TokenClassificationPipeline._forward(self, model_inputs)
    238     logits = self.model(model_inputs.data)[0]
    239 else:
--&gt; 240     output = self.model(**model_inputs)
    241     logits = output[&quot;logits&quot;] if isinstance(output, dict) else output[0]
    243 return {
    244     &quot;logits&quot;: logits,
    245     &quot;special_tokens_mask&quot;: special_tokens_mask,
   (...)
    248     **model_inputs,
    249 }

File ~/skill_extraction/skill_extraction/lib/python3.10/site-packages/optimum/modeling_base.py:85, in OptimizedModel.__call__(self, *args, **kwargs)
     84 def __call__(self, *args, **kwargs):
---&gt; 85     return self.forward(*args, **kwargs)

File ~/skill_extraction/skill_extraction/lib/python3.10/site-packages/optimum/onnxruntime/modeling_ort.py:1363, in ORTModelForTokenClassification.forward(self, input_ids, attention_mask, token_type_ids, **kwargs)
   1360     onnx_inputs[&quot;token_type_ids&quot;] = token_type_ids
   1362 # run inference
-&gt; 1363 outputs = self.model.run(None, onnx_inputs)
   1364 logits = outputs[self.output_names[&quot;logits&quot;]]
   1366 if use_torch:

File ~/skill_extraction/skill_extraction/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:200, in Session.run(self, output_names, input_feed, run_options)
    198     output_names = [output.name for output in self._outputs_meta]
    199 try:
--&gt; 200     return self._sess.run(output_names, input_feed, run_options)
    201 except C.EPFail as err:
    202     if self._enable_fallback:

InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Invalid Feed Input Name:token_type_ids
</code></pre>
<p>I am not sure what I am doing wrong? The same code works for other models (such as another model which BERT based and then fine tuned by me)</p>
<p>I am using tokenzier version 0.13.3 transformers version 4.27.4 and optimum version 1.7.3 I am on a AMD based machine in EC2 (AWS)</p>
<p>Please help as this is blocking me from optimizing the model. I can't find any docs on it.</p>
","huggingface"
"75945735","XLNet or BERT Chinese for HuggingFace AutoModelForSeq2SeqLM Training","2023-04-06 04:09:22","75955714","2","410","<python><pytorch><huggingface-transformers><huggingface>","<p>I want to use the pre-trained XLNet (<code>xlnet-base-cased</code>, which the model type is <em>Text Generation</em>) or BERT Chinese (<code>bert-base-chinese</code>, which the model type is <em>Fill Mask</em>) for Sequence to Sequence Language Model (<code>Seq2SeqLM</code>) training.</p>
<p>I can use <code>facebook/bart-large</code> (which the model type is <em>Feature Extraction</em>) for constructing the <code>Seq2SeqLM</code>, but not the 2 pretrained models mentioned above. Here is my code below:</p>
<p><strong>Load Dataset</strong></p>
<pre><code>from datasets import load_dataset
yuezh = load_dataset(&quot;my-custom-dataset&quot;)
</code></pre>
<p><strong>Sample data from the dataset</strong> <code>my-custom-dataset</code></p>
<pre><code>{&quot;translation&quot;: {&quot;yue&quot;: &quot;又睇&quot;, &quot;zh&quot;: &quot;再看&quot;}}
{&quot;translation&quot;: {&quot;yue&quot;: &quot;初頭&quot;, &quot;zh&quot;: &quot;開始的時候&quot;}}
</code></pre>
<p><strong>Create Test Split</strong></p>
<pre><code>yuezh = yuezh[&quot;train&quot;].train_test_split(test_size=0.2)
print(yuezh)
</code></pre>
<p>Output:</p>
<pre><code>DatasetDict({
    train: Dataset({
        features: ['translation'],
        num_rows: 4246
    })
    test: Dataset({
        features: ['translation'],
        num_rows: 1062
    })
})
</code></pre>
<p><strong>Tokenizer</strong></p>
<pre><code>from transformers import AutoTokenizer
checkpoint = 'bert-base-chinese'
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
</code></pre>
<p><strong>Pre-process Function</strong></p>
<pre><code>def preprocess_function(examples):
    inputs = [prefix + example[source_lang] for example in examples[&quot;translation&quot;]]
    targets = [example[target_lang] for example in examples[&quot;translation&quot;]]
    model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True)
    return model_inputs
</code></pre>
<p><strong>Parameters</strong></p>
<pre><code>source_lang = &quot;yue&quot;
target_lang = &quot;zh&quot;
prefix = &quot;Translate this: &quot;
tokenized_yuezh = yuezh.map(preprocess_function, batched=True)
tokenized_yuezh = tokenized_yuezh.remove_columns(yuezh[&quot;train&quot;].column_names)
print(tokenized_yuezh)
</code></pre>
<p>Output:</p>
<pre><code>DatasetDict({
    train: Dataset({
        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],
        num_rows: 4246
    })
    test: Dataset({
        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],
        num_rows: 1062
    })
})
</code></pre>
<p><strong>Evaluate Performance</strong></p>
<pre><code># Use ScareBLEU to evaluate the performance
import evaluate
metric = evaluate.load(&quot;sacrebleu&quot;)
</code></pre>
<p><strong>Data Collator</strong></p>
<pre><code>from transformers import DataCollatorForSeq2Seq
data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)
</code></pre>
<p><strong>Supporting Functions</strong></p>
<pre><code>import numpy as np

def postprocess_text(preds, labels):
    preds = [pred.strip() for pred in preds]
    labels = [[label.strip()] for label in labels]

    return preds, labels


def compute_metrics(eval_preds):
    preds, labels = eval_preds
    if isinstance(preds, tuple):
        preds = preds[0]
    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)

    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)

    result = metric.compute(predictions=decoded_preds, references=decoded_labels)
    result = {&quot;bleu&quot;: result[&quot;score&quot;]}

    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]
    result[&quot;gen_len&quot;] = np.mean(prediction_lens)
    result = {k: round(v, 4) for k, v in result.items()}
    return result
</code></pre>
<p><strong>Training</strong></p>
<pre><code>model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)
training_args = CustomSeq2SeqTrainingArguments(
    output_dir=&quot;my-output-dir&quot;,
    evaluation_strategy=&quot;epoch&quot;,
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=2,
    predict_with_generate=True,
    remove_unused_columns=False,
    fp16=True,
    push_to_hub=False, # Don't push to Hub yet
)

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_yuezh[&quot;train&quot;],
    eval_dataset=tokenized_yuezh[&quot;test&quot;],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

trainer.train()
</code></pre>
<p>It results in the following errors:</p>
<pre><code>ValueError: Unrecognized configuration class &lt;class 
'transformers.models.bert.configuration_bert.BertConfig'&gt; for this kind of AutoModel: 
AutoModelForSeq2SeqLM.
Model type should be one of BartConfig, BigBirdPegasusConfig, BlenderbotConfig, 
BlenderbotSmallConfig, EncoderDecoderConfig, FSMTConfig, LEDConfig, LongT5Config, 
M2M100Config, MarianConfig, MBartConfig, MT5Config, MvpConfig, PegasusConfig, PegasusXConfig,
PLBartConfig, ProphetNetConfig, SwitchTransformersConfig, T5Config, XLMProphetNetConfig.
</code></pre>
<p>The <code>AutoModelForSeq2SeqLM</code> does not support XLNet or BERT. What should I do to make the Sequence-to-Sequence training work?</p>
","huggingface"
"75932930",".map function of Dataset returns list instead of dictionary","2023-04-04 18:58:40","","0","175","<numpy><librosa><huggingface><huggingface-datasets>","<p>If I write a function:</p>
<pre><code>def load_audio(example):
    filepath = example[&quot;audio&quot;][&quot;path&quot;]
    arr, sr = librosa.load(filepath, sr=16000)
    return {'audio': {'path': filepath, 'array': arr,'sampling_rate': 16000}, 'sentence':example['sentence']}
</code></pre>
<p>and then call map for a dataset:</p>
<pre><code>dataset = dataset.map(load_audio)
</code></pre>
<p>and</p>
<p><code>print(type(dataset_dict['train']['audio'][0]['array']))</code></p>
<p>gives</p>
<p><code>&lt;class 'list'&gt;</code>.</p>
<p>I would expect the last one to give <strong>numpy array</strong>, why its type is a <strong>list</strong>?</p>
<p>This how I created the dataset:</p>
<pre><code>sentences=list(transcript_dict.values())
audio_files=list(transcript_dict.keys())
audio_list = []
for file in audio_files:
    audio_dict = {
        &quot;path&quot;: file,
        &quot;array&quot;: np.array([]),
        &quot;sampling_rate&quot;: 16000
    }
    audio_list.append(audio_dict) 
data = {'sentence': sentences, 'audio': audio_list}
dataset = Dataset.from_dict(data)
</code></pre>
<p>Even before mapping, it gives that the type of &quot;dataset['audio'][0]['array']&quot; is a list. Why it is not an array?</p>
<p>When I check:</p>
<pre><code>print(type(audio_list[0]['array']))
</code></pre>
<p>it gives that it is of type &lt;class 'numpy.ndarray'&gt;</p>
","huggingface"
"75931144","Change the Number of layers of a Pretrained Huggingface Pegasus model used for Conditional Generation","2023-04-04 15:24:48","75931518","0","672","<pytorch><huggingface-transformers><huggingface>","<p>I am trying to change the number of layers in a pretrained Huggingface Pegasus Model to see if the performance of the is improving or not. I tried updating the config function. But it generates index out of range error.</p>
<p>Following is the code I tried.</p>
<pre><code>from transformers import PegasusConfig

config = PegasusConfig(
    encoder_layers = 14,
    encoder_attention_heads = 16,
    decoder_layers = 14,
    decoder_attention_heads = 16,
    max_position_embeddings= 2048,
)

from transformers import pipeline, PegasusTokenizer, PegasusForConditionalGeneration

model = PegasusForConditionalGeneration.from_pretrained('google/pegasus-pubmed',config = config, ignore_mismatched_sizes=True)

</code></pre>
","huggingface"
"75930297","Error using Stable Diffusion: cannot import name 'is_modelcards_available' from 'diffusers.utils.import_utils'","2023-04-04 14:01:56","","0","1037","<python-import><huggingface><stable-diffusion>","<p>I'm new to Stable Diffusion; I've followed their instructions to install it <a href=""https://huggingface.co/blog/stable_diffusion"" rel=""nofollow noreferrer"">https://huggingface.co/blog/stable_diffusion</a> but when I run:</p>
<p><code>from diffusers import StableDiffusionPipeline</code></p>
<p>I get the error</p>
<p><code>ImportError: cannot import name 'is_modelcards_available' from 'diffusers.utils.import_utils'</code></p>
<p>I can't find any reference to this on the Internet, any ideas? Thanks!</p>
","huggingface"
"75921683","index out of range in self summarizer a book","2023-04-03 16:25:05","","0","135","<python><artificial-intelligence><huggingface-transformers><huggingface>","<pre><code>
from transformers import pipeline

summarizer = pipeline('summarization')

summaries = []

if chunks: # Check if chunks is not empty
    for chunk in chunks:
        if chunk.strip(): # Check if chunk is not an empty string
            summary = summarizer(chunk, max_length=100, min_length=30, do_sample=False)[0]['summary_text']
            summaries.append(summary)
</code></pre>
<p>win I run this code I get this error :</p>
<pre><code>Token indices sequence length is longer than the specified maximum sequence length for this model (10020 &gt; 1024). Running this sequence through the model will result in indexing errors
</code></pre>
<p>I changed the value of max_length and min_length but I get the same error</p>
","huggingface"
"75918140","Getting RuntimeError: expected scalar type Half but found Float in AWS P3 instances in opt6.7B fine tune","2023-04-03 09:53:31","","5","4957","<python><pytorch><huggingface-transformers><huggingface>","<p>I have a simple code which takes a opt6.7B model and fine tunes it. When I run this code in Google colab(Tesla T4, 16GB) it runs without any problem. But when I try to run the the same code in AWS p3-2xlarge environment (Tesla V100 GPU, 16GB) it gives the error.</p>
<pre><code>RuntimeError: expected scalar type Half but found Float
</code></pre>
<p>To be able to run the fine tuning on a single GPU I use LORA and peft. which are installed exactly the same way (pip install) in both cases. I can use <code>with torch.autocast(&quot;cuda&quot;):</code> and then that error vanishes. But the loss of the training becomes very strange meaning it does not gradually decrease rather it fluctuates within a large range (0-5) (and if I change the model to GPT-J then the loss always stays 0) whereas the loss is gradually decreasing for the case of colab. So I am not sure if using <code>with torch.autocast(&quot;cuda&quot;):</code> is a good thing or not.</p>
<p>The transfromeers version is <code>4.28.0.dev0</code> in both case. Torch version for colab shows <code>1.13.1+cu116</code> whereas for p3 shows - <code>1.13.1</code> (does this mean it does not have CUDA support? I doubt, on top of that doing <code>torch.cuda.is_available()</code> shows True)</p>
<p>The only large difference I can see is that for colab, bitsandbytes has this following setup log</p>
<pre><code>===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...
CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so
CUDA SETUP: Highest compute capability among GPUs detected: 7.5
CUDA SETUP: Detected CUDA version 118
</code></pre>
<p>Whereas for p3 it is the following</p>
<pre><code>===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
CUDA SETUP: CUDA runtime path found: /opt/conda/envs/pytorch/lib/libcudart.so
CUDA SETUP: Highest compute capability among GPUs detected: 7.0
CUDA SETUP: Detected CUDA version 117
CUDA SETUP: Loading binary /opt/conda/envs/pytorch/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117_nocublaslt.so...
</code></pre>
<p>What am I missing? I am not posting the code here. But it is really a very basic version that takes opt-6.7b and fine tunes it on alpaca dataset using LORA and peft.</p>
<p>Why does it run in colab but not in p3? Any help is welcome :)</p>
<p>-------------------- EDIT</p>
<p>I am posting a minimal code example that I actually tried</p>
<pre><code>import os
os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;]=&quot;0&quot;
import torch
import torch.nn as nn
import bitsandbytes as bnb
from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(
    &quot;facebook/opt-6.7b&quot;, 
    load_in_8bit=True, 
    device_map='auto',
)

tokenizer = AutoTokenizer.from_pretrained(&quot;facebook/opt-6.7b&quot;)
for param in model.parameters():
  param.requires_grad = False  # freeze the model - train adapters later
  if param.ndim == 1:
    # cast the small parameters (e.g. layernorm) to fp32 for stability
    param.data = param.data.to(torch.float32)

model.gradient_checkpointing_enable()  # reduce number of stored activations
model.enable_input_require_grads()

class CastOutputToFloat(nn.Sequential):
  def forward(self, x): return super().forward(x).to(torch.float32)
model.lm_head = CastOutputToFloat(model.lm_head)

def print_trainable_parameters(model):
    &quot;&quot;&quot;
    Prints the number of trainable parameters in the model.
    &quot;&quot;&quot;
    trainable_params = 0
    all_param = 0
    for _, param in model.named_parameters():
        all_param += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
    print(
        f&quot;trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}&quot;
    )

from peft import LoraConfig, get_peft_model 

config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=[&quot;q_proj&quot;, &quot;v_proj&quot;],
    lora_dropout=0.05,
    bias=&quot;none&quot;,
    task_type=&quot;CAUSAL_LM&quot;
)

model = get_peft_model(model, config)
print_trainable_parameters(model)

import transformers
from datasets import load_dataset

tokenizer.pad_token_id = 0
CUTOFF_LEN = 256

data = load_dataset(&quot;tatsu-lab/alpaca&quot;)

data = data.shuffle().map(
    lambda data_point: tokenizer(
        data_point['text'],
        truncation=True,
        max_length=CUTOFF_LEN,
        padding=&quot;max_length&quot;,
    ),
    batched=True
)
# data = load_dataset(&quot;Abirate/english_quotes&quot;)
# data = data.map(lambda samples: tokenizer(samples['quote']), batched=True)

trainer = transformers.Trainer(
    model=model, 
    train_dataset=data['train'],
    args=transformers.TrainingArguments(
        per_device_train_batch_size=4, 
        gradient_accumulation_steps=4,
        warmup_steps=100, 
        max_steps=400, 
        learning_rate=2e-5, 
        fp16=True,
        logging_steps=1, 
        output_dir='outputs'
    ),
    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)
)

model.config.use_cache = False  # silence the warnings. Please re-enable for inference!
trainer.train()
</code></pre>
<p>And here is the full stack trace</p>
<pre><code>/tmp/ipykernel_24622/2601578793.py:2 in &lt;module&gt;                                                 │
│                                                                                                  │
│ [Errno 2] No such file or directory: '/tmp/ipykernel_24622/2601578793.py'                        │
│                                                                                                  │
│ /opt/conda/envs/pytorch/lib/python3.9/site-packages/transformers/trainer.py:1639 in train        │
│                                                                                                  │
│   1636 │   │   inner_training_loop = find_executable_batch_size(                                 │
│   1637 │   │   │   self._inner_training_loop, self._train_batch_size, args.auto_find_batch_size  │
│   1638 │   │   )                                                                                 │
│ ❱ 1639 │   │   return inner_training_loop(                                                       │
│   1640 │   │   │   args=args,                                                                    │
│   1641 │   │   │   resume_from_checkpoint=resume_from_checkpoint,                                │
│   1642 │   │   │   trial=trial,                                                                  │
│                                                                                                  │
│ /opt/conda/envs/pytorch/lib/python3.9/site-packages/transformers/trainer.py:1906 in              │
│ _inner_training_loop                                                                             │
│                                                                                                  │
│   1903 │   │   │   │   │   with model.no_sync():                                                 │
│   1904 │   │   │   │   │   │   tr_loss_step = self.training_step(model, inputs)                  │
│   1905 │   │   │   │   else:                                                                     │
│ ❱ 1906 │   │   │   │   │   tr_loss_step = self.training_step(model, inputs)                      │
│   1907 │   │   │   │                                                                             │
│   1908 │   │   │   │   if (                                                                      │
│   1909 │   │   │   │   │   args.logging_nan_inf_filter                                           │
│                                                                                                  │
│ /opt/conda/envs/pytorch/lib/python3.9/site-packages/transformers/trainer.py:2662 in              │
│ training_step                                                                                    │
│                                                                                                  │
│   2659 │   │   │   loss = loss / self.args.gradient_accumulation_steps                           │
│   2660 │   │                                                                                     │
│   2661 │   │   if self.do_grad_scaling:                                                          │
│ ❱ 2662 │   │   │   self.scaler.scale(loss).backward()                                            │
│   2663 │   │   elif self.use_apex:                                                               │
│   2664 │   │   │   with amp.scale_loss(loss, self.optimizer) as scaled_loss:                     │
│   2665 │   │   │   │   scaled_loss.backward()                                                    │
│                                                                                                  │
│ /opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/_tensor.py:488 in backward             │
│                                                                                                  │
│    485 │   │   │   │   create_graph=create_graph,                                                │
│    486 │   │   │   │   inputs=inputs,                                                            │
│    487 │   │   │   )                                                                             │
│ ❱  488 │   │   torch.autograd.backward(                                                          │
│    489 │   │   │   self, gradient, retain_graph, create_graph, inputs=inputs                     │
│    490 │   │   )                                                                                 │
│    491                                                                                           │
│                                                                                                  │
│ /opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/autograd/__init__.py:197 in backward   │
│                                                                                                  │
│   194 │   # The reason we repeat same the comment below is that                                  │
│   195 │   # some Python versions print out the first line of a multi-line function               │
│   196 │   # calls in the traceback and some print out the last line                              │
│ ❱ 197 │   Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the bac   │
│   198 │   │   tensors, grad_tensors_, retain_graph, create_graph, inputs,                        │
│   199 │   │   allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to ru   │
│   200                                                                                            │
│                                                                                                  │
│ /opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/autograd/function.py:267 in apply      │
│                                                                                                  │
│   264 │   │   │   │   │   │   │      &quot;Function is not allowed. You should only implement one &quot;   │
│   265 │   │   │   │   │   │   │      &quot;of them.&quot;)                                                 │
│   266 │   │   user_fn = vjp_fn if vjp_fn is not Function.vjp else backward_fn                    │
│ ❱ 267 │   │   return user_fn(self, *args)                                                        │
│   268 │                                                                                          │
│   269 │   def apply_jvp(self, *args):                                                            │
│   270 │   │   # _forward_cls is defined by derived class                                         │
│                                                                                                  │
│ /opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/checkpoint.py:157 in backward    │
│                                                                                                  │
│   154 │   │   │   raise RuntimeError(                                                            │
│   155 │   │   │   │   &quot;none of output has requires_grad=True,&quot;                                   │
│   156 │   │   │   │   &quot; this checkpoint() is not necessary&quot;)                                     │
│ ❱ 157 │   │   torch.autograd.backward(outputs_with_grad, args_with_grad)                         │
│   158 │   │   grads = tuple(inp.grad if isinstance(inp, torch.Tensor) else None                  │
│   159 │   │   │   │   │     for inp in detached_inputs)                                          │
│   160                                                                                            │
│                                                                                                  │
│ /opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/autograd/__init__.py:197 in backward   │
│                                                                                                  │
│   194 │   # The reason we repeat same the comment below is that                                  │
│   195 │   # some Python versions print out the first line of a multi-line function               │
│   196 │   # calls in the traceback and some print out the last line                              │
│ ❱ 197 │   Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the bac   │
│   198 │   │   tensors, grad_tensors_, retain_graph, create_graph, inputs,                        │
│   199 │   │   allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to ru   │
│   200                                                                                            │
│                                                                                                  │
│ /opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/autograd/function.py:267 in apply      │
│                                                                                                  │
│   264 │   │   │   │   │   │   │      &quot;Function is not allowed. You should only implement one &quot;   │
│   265 │   │   │   │   │   │   │      &quot;of them.&quot;)                                                 │
│   266 │   │   user_fn = vjp_fn if vjp_fn is not Function.vjp else backward_fn                    │
│ ❱ 267 │   │   return user_fn(self, *args)                                                        │
│   268 │                                                                                          │
│   269 │   def apply_jvp(self, *args):                                                            │
│   270 │   │   # _forward_cls is defined by derived class                                         │
│                                                                                                  │
│ /opt/conda/envs/pytorch/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:456 in   │
│ backward                                                                                         │
│                                                                                                  │
│   453 │   │   │                                                                                  │
│   454 │   │   │   elif state.CB is not None:                                                     │
│   455 │   │   │   │   CB = state.CB.to(ctx.dtype_A, copy=True).mul_(state.SCB.unsqueeze(1).mul   │
│ ❱ 456 │   │   │   │   grad_A = torch.matmul(grad_output, CB).view(ctx.grad_shape).to(ctx.dtype   │
│   457 │   │   │   elif state.CxB is not None:                                                    │
│   458 │   │   │   │                                                                              │
│   459 │   │   │   │   if state.tile_indices is None:
</code></pre>
<p>(Sorry if this is a very novice question but I have no solution at the moment :( )</p>
","huggingface"
"75916234","Is there anything else I need to do besides calling model.to(device) for HuggingFace GPU?","2023-04-03 05:22:10","75917571","1","740","<python><pytorch><huggingface>","<p>I am new to using HuggingFace and the PyTorch ML ecosystem. I am trying to use a GPU device instead of the default CPU.</p>
<p>Can someone tell me if the following script is correct? The only thing I am calling is <code>lmhead_model.to(device)</code>.</p>
<p>I am not sure whether or not I need to move the <code>tokenizer</code>, <code>train_dataset</code>, <code>data_collator</code>, or anything else. Any insight would be appreciated for this beginner.</p>
<pre><code>from transformers import GPT2Tokenizer
from transformers import GPT2LMHeadModel
from transformers import TextDataset
from transformers import DataCollatorForLanguageModeling
from transformers import Trainer
from transformers import TrainingArguments

# Load the GPT-2 tokenizer and LM head model
tokenizer    = GPT2Tokenizer.from_pretrained('gpt2')
lmhead_model = GPT2LMHeadModel.from_pretrained('gpt2')

# Load the training dataset and divide blocksize
train_dataset = TextDataset(
    tokenizer=tokenizer,
    file_path='tinyshakespeare.txt',
    block_size=64
)

# Create a data collator for preprocessing batches
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False
)

# Optionally configure the model and pytorch w/ gpu
import torch
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
lmhead_model = lmhead_model.to(device)

if device.type == 'cuda':
    print('We are currently using a GPU')
else:
    print('We are currently using a CPU')

# Defining the training arguments
training_args = TrainingArguments(
    output_dir='tinyshakespeare',                  # output directory for checkpoints
    overwrite_output_dir=True,                     # overwrite any existing content

    per_device_train_batch_size=32,                # sample batch size for training
    dataloader_num_workers=0,                      # number of workers for dataloader
    max_steps=1000,                                # maximum number of training steps
    save_steps=1000,                               # after # steps checkpoints are saved
    save_total_limit=1,                            # maximum number of checkpoints to save

    prediction_loss_only=True,                     # only compute loss during prediction
    learning_rate=3e-4,                            # learning rate
    fp16=True if device.type == 'cuda' else False, # use 16-bit (mixed) precision

    optim='adamw_torch',                           # define the optimizer for training
    lr_scheduler_type='linear',                    # define the learning rate scheduler

    logging_steps=10,                              # after # steps logs are printed
    report_to='none',                              # report to wandb, tensorboard, etc.
)

# Performing the ML training loop
if __name__ == '__main__':
    trainer = Trainer(
        model=lmhead_model,
        args=training_args,
        data_collator=data_collator,
        train_dataset=train_dataset,
    )

    result = trainer.train()
    print_summary(result)
</code></pre>
","huggingface"
"75906407","How to interpret the model_max_len attribute of the PreTrainedTokenizer object in Huggingface Transformers","2023-04-01 13:13:27","75906459","1","254","<python><nlp><huggingface-transformers><huggingface-tokenizers><huggingface>","<p>I've been trying to check the maximum length allowed by emilyalsentzer/Bio_ClinicalBERT, and after these lines of code:</p>
<pre><code>model_name = &quot;emilyalsentzer/Bio_ClinicalBERT&quot;
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer
</code></pre>
<p>I've obtained the following:</p>
<pre><code>PreTrainedTokenizerFast(name_or_path='emilyalsentzer/Bio_ClinicalBERT', vocab_size=28996, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})
</code></pre>
<p>Is that true? Is the max length of the model (in the number of tokens, as it says <a href=""https://huggingface.co/transformers/v2.11.0/main_classes/tokenizer.html#transformers.PreTrainedTokenizer"" rel=""nofollow noreferrer"">here</a>) that high? Then, how am I supposed to interpret that?</p>
<p>Cheers!</p>
","huggingface"
"75894559","How to build a pipeline for multiple transformers models?","2023-03-31 03:10:38","","0","542","<pytorch><pipeline><huggingface-transformers><torch><huggingface>","<p>I am trying to make a pipeline for a natural language to SQL translation task.</p>
<p>To solve the problem, I need multiple transformers for entity extraction, query generation, and so on.
I looked for the <code>Pipeline</code> in HuggingFace, but it seems like it is only for a one transofrmer model. (it only takes one model and one tokenizer).</p>
<p>Is there a proper way to build a pipeline which uses multiple transformers with huggingface interface?</p>
<pre class=""lang-py prettyprint-override""><code>class MultiplePipeline(Pipeline):
    def __init__():
        self.extractor = AutoModel.from_pretrained()
        self.query_generator = T5ForConditionalGeneration.from_pretrained()
    
    def _forward(self, x):
        x = self.extractor(x)
        x = self.query_generator(x)
        ...
</code></pre>
<p>or else, what should I do for it?</p>
","huggingface"
"75874208","Unable to use huggingface-cli in Jupyterhub","2023-03-29 07:15:27","78198170","0","423","<huggingface-transformers><jupyterhub><huggingface>","<p>I created my model using HuggingFace Transformers. When I try to push to HuggingFace Hub, it shows the following error when I execute <code>trainer.push_to_hub()</code>:</p>
<pre><code>OSError: Failed to start custom transfer command &quot;huggingface-cli&quot; remote: exec: 
&quot;huggingface-cli&quot;: executable file not found in $PATH
error: failed to push some refs to 'https://my_hugging_face_url_with_login_token'
</code></pre>
<p>The command can be used in the Terminal opened in Jupyterhub without a problem. How to resolve this PATH issue in Jupyterhub?</p>
","huggingface"
"75854700","How to fine tune a Huggingface Seq2Seq model with a dataset from the hub?","2023-03-27 10:33:06","75862077","5","6086","<python><nlp><huggingface-transformers><huggingface-tokenizers><huggingface>","<p>I want to train the <code>&quot;flax-community/t5-large-wikisplit&quot;</code> model with the <code>&quot;dxiao/requirements-ner-id&quot;</code> dataset. (Just for some experiments)</p>
<p>I think my general procedure is not correct, but I don't know how to go further.</p>
<p>My Code:</p>
<p>Load tokenizer and model:</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModel
checkpoint = &quot;flax-community/t5-large-wikisplit&quot;
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint).cuda()
</code></pre>
<p>Load dataset that I want to train:</p>
<pre><code>from datasets import load_dataset
raw_dataset = load_dataset(&quot;dxiao/requirements-ner-id&quot;)
</code></pre>
<p>The raw_dataset looks like this ['id', 'tokens', 'tags', 'ner_tags']</p>
<p>I want to get the sentences as sentence and not as tokens.</p>
<pre><code>def tokenToString(tokenarray):
  string = tokenarray[0]
  for x in tokenarray[1:]:
    string += &quot; &quot; + x
  return string

def sentence_function(example):
  return {&quot;sentence&quot; :  tokenToString(example[&quot;tokens&quot;]),
          &quot;simplefiedSentence&quot; : tokenToString(example[&quot;tokens&quot;]).replace(&quot;The&quot;, &quot;XXXXXXXXXXX&quot;)}

wikisplit_req_set = raw_dataset.map(sentence_function)
wikisplit_req_set
</code></pre>
<p>I tried to restructure the dataset such that it looks like the wikisplit dataset:</p>
<pre><code>simple1dataset = wikisplit_req_set.remove_columns(['id', 'tags', 'ner_tags', 'tokens']);
complexdataset = wikisplit_req_set.remove_columns(['id', 'tags', 'ner_tags', 'tokens']);
complexdataset[&quot;train&quot;] = complexdataset[&quot;train&quot;].add_column(&quot;simple_sentence_1&quot;,simple1dataset[&quot;train&quot;][&quot;sentence&quot;]).add_column(&quot;simple_sentence_2&quot;,simple1dataset[&quot;train&quot;][&quot;simplefiedSentence&quot;])
complexdataset[&quot;test&quot;] = complexdataset[&quot;test&quot;].add_column(&quot;simple_sentence_1&quot;,simple1dataset[&quot;test&quot;][&quot;sentence&quot;]).add_column(&quot;simple_sentence_2&quot;,simple1dataset[&quot;test&quot;][&quot;simplefiedSentence&quot;])
complexdataset[&quot;validation&quot;] = complexdataset[&quot;validation&quot;].add_column(&quot;simple_sentence_1&quot;,simple1dataset[&quot;validation&quot;][&quot;sentence&quot;]).add_column(&quot;simple_sentence_2&quot;,simple1dataset[&quot;validation&quot;][&quot;simplefiedSentence&quot;])
trainingDataSet = complexdataset.rename_column(&quot;sentence&quot;, &quot;complex_sentence&quot;)
trainingDataSet
</code></pre>
<p>Tokenize it:</p>
<pre><code>def tokenize_function(example):
    model_inputs = tokenizer(example[&quot;complex_sentence&quot;],truncation=True, padding=True)
    targetS1 = tokenizer(example[&quot;simple_sentence_1&quot;],truncation=True, padding=True)
    targetS2 = tokenizer(example[&quot;simple_sentence_2&quot;],truncation=True, padding=True)
    model_inputs['simple_sentence_1'] = targetS1['input_ids']
    model_inputs['simple_sentence_2'] = targetS2['input_ids']
    model_inputs['decoder_input_ids'] = targetS2['input_ids']
    return model_inputs

tokenized_datasets = trainingDataSet.map(tokenize_function, batched=True)
tokenized_datasets=tokenized_datasets.remove_columns(&quot;complex_sentence&quot;)
tokenized_datasets=tokenized_datasets.remove_columns(&quot;simple_sentence_1&quot;)
tokenized_datasets=tokenized_datasets.remove_columns(&quot;simple_sentence_2&quot;)
tokenized_datasets=tokenized_datasets.remove_columns(&quot;simplefiedSentence&quot;)
tokenized_datasets
</code></pre>
<p>DataLoader:</p>
<pre><code>from transformers import DataCollatorForLanguageModeling
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)
data_collator
</code></pre>
<p>Training:</p>
<pre><code>from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, TrainingArguments, EvalPrediction, DataCollatorWithPadding, Trainer

bleu = evaluate.load(&quot;bleu&quot;)

training_args = Seq2SeqTrainingArguments(
  output_dir = &quot;/&quot;,
  log_level = &quot;error&quot;,
  num_train_epochs = 0.25,
  learning_rate = 5e-4,
  lr_scheduler_type = &quot;linear&quot;,
  warmup_steps = 50,
  optim = &quot;adafactor&quot;,
  weight_decay = 0.01,
  per_device_train_batch_size = 1,
  per_device_eval_batch_size = 1,
  gradient_accumulation_steps = 16,
  evaluation_strategy = &quot;steps&quot;,
  eval_steps = 50,
  predict_with_generate=True,
  generation_max_length = 128,
  save_steps = 500,
  logging_steps = 10,
  push_to_hub = False,
  auto_find_batch_size=True
)

trainer = Seq2SeqTrainer(
    model,
    training_args,
    train_dataset=tokenized_datasets[&quot;train&quot;],
    eval_dataset=tokenized_datasets[&quot;validation&quot;],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=bleu,

)
trainer.train()
</code></pre>
<p>The Problem is, that I do not understand how the model know the expected value and how it calculate its loss. Can someone give me some ideas what happens where?</p>
<p>I hope some one can help me understand my own code, because the documentation by Hugging Face does not help me enough. Maybe someone have some Codeexamples or something else. I do not completely understand how I fine tune the model and how I get the parameters the model expects to train it. I also do not understand how the training works and what the parameters do.</p>
","huggingface"
"75846850","Unable to load model, Compile the model with Xcode or `MLModel.compileModel(at:)`. SwiftUI Stable Diffusion","2023-03-26 09:20:14","","0","385","<xcode><swiftui><coreml><huggingface><stable-diffusion>","<p>I am trying to run Stable Diffusion's CoreML model with a SwiftUI app, on an M2 pro mac, with MacOS Ventura 13.2.1.  I have downloaded the Core ML Stable Diffusion Models from Hugging Face Hub, and the app compiled and ran successfully on my mac bug free.</p>
<p>**The Core ML Stable Diffusion files in my project : **
<a href=""https://i.sstatic.net/s2pxY.png"" rel=""nofollow noreferrer"">Image of the CoreML Stable Diffusion files</a></p>
<p>I downloaded those coreML by cloning that repo
<code>git clone https://huggingface.co/apple/coreml-stable-diffusion-v1-4</code></p>
<p>(<a href=""https://i.sstatic.net/5li9m.png"" rel=""nofollow noreferrer"">https://i.sstatic.net/5li9m.png</a>)</p>
<p>However, when I click the generate button, which triggers the <code>generateImage</code> function, I got the error :</p>
<pre><code>Thread 11: Fatal error: 'try!' expression unexpectedly raised an error: Error Domain=com.apple.CoreML Code=0 &quot;Unable to load model: file:///Users/landon/Library/Developer/Xcode/DerivedData/ArtSenseiPro-abnumyedexjximglshwxziyuddrp/Build/Products/Debug/ArtSenseiPro.app/Contents/Resources/TextEncoder.mlmodelc/. Compile the model with Xcode or `MLModel.compileModel(at:)`. &quot; UserInfo={NSLocalizedDescription=Unable to load model: file:///Users/landon/Library/Developer/Xcode/DerivedData/ArtSenseiPro-abnumyedexjximgls
</code></pre>
<p>Here is my entire code :</p>
<pre><code>import SwiftUI
import Combine
import StableDiffusion

@available(macOS 13.1, *)
@available(iOS 16.2, *)
struct ContentView: View {
    
    @State var prompt: String = &quot;Penguins in business suits debating whether to invest in an ice cream stand on the beach.&quot;
    @State var pipeline: StableDiffusionPipeline?
    @State var image: CGImage?
    @State var progress = 0.0
    @State var generating = false
    @State var initializing = true
    
    var body: some View {
        VStack {
            if initializing {
                Text(&quot;Initializing...&quot;)
            } else {
                if let image = self.image {
                    Image(image, scale: 1.0, label: Text(&quot;&quot;))
                }
                if generating {
                    Spacer()
                    ProgressView(value: progress)
                    Text(&quot;generating (\(Int(progress*100)) %)&quot;)
                } else {
                    Spacer()
                    TextField(&quot;Prompt&quot;, text: $prompt)
                    Button(&quot;Generate&quot;) {
                        generateImage()
                    }
                }
            }
        }
        .padding()
        .task {
            guard let resourceURL = Bundle.main.resourceURL else {
                return
            }
            do {
                pipeline = try StableDiffusionPipeline(resourcesAt: resourceURL)
            } catch let error {
                print(error.localizedDescription)
            }
            initializing = false
        }
    }
    
    func generateImage(){
        progress = 0.0
        image = nil
        generating = true
        Task.detached(priority: .high) {
            var images: [CGImage?]?
            do {
                images = try pipeline?.generateImages(prompt: prompt, disableSafety: false, progressHandler: { progress in
                    self.progress = Double(progress.step) / 50
                    if let image = progress.currentImages.first {
                        self.image = image
                    }
                    return true
                })
            } catch let error {
                print(error.localizedDescription)
            }
            if let image = images?.first {
                self.image = image
            }
            generating = false
        }
    }
}

</code></pre>
<p>Any help would be appreciated.  Thanks!</p>
<p>I am trying to run Stable Diffusion's CoreML model with a SwiftUI app, on an M2 pro mac, with MacOS Ventura 13.2.1.  I have downloaded the Core ML Stable Diffusion Models from Hugging Face Hub, and the app compiled and ran successfully on my mac bug free.</p>
<p>However, when I click the generate button, which triggers the <code>generateImage</code> function, I got the error :</p>
<pre><code>Thread 11: Fatal error: 'try!' expression unexpectedly raised an error: Error Domain=com.apple.CoreML Code=0 &quot;Unable to load model: file:///Users/landon/Library/Developer/Xcode/DerivedData/ArtSenseiPro-abnumyedexjximglshwxziyuddrp/Build/Products/Debug/ArtSenseiPro.app/Contents/Resources/TextEncoder.mlmodelc/. Compile the model with Xcode or `MLModel.compileModel(at:)`. &quot; UserInfo={NSLocalizedDescription=Unable to load model: file:///Users/landon/Library/Developer/Xcode/DerivedData/ArtSenseiPro-abnumyedexjximgls
</code></pre>
<p>I tried reinstalling the files related to the model many times but didn't work.</p>
","huggingface"
"75842180","How can I convert this Gradio file upload code into folder path taking one?","2023-03-25 13:56:37","","1","1636","<python><huggingface><gradio>","<p>There is this gradio code that allows you to upload images. I prefer entering folder path as a text input.</p>
<p>It works but instead of this one, I want to convert it into a directory path taking one</p>
<pre><code>  reference_imgs = gr.UploadButton(label=&quot;Upload Guide Frames&quot;, file_types = ['.png','.jpg','.jpeg'], live=True, file_count = &quot;multiple&quot;) 
</code></pre>
<p>I tried below one but it didn't work</p>
<pre><code>reference_imgs = gr.inputs.FilePicker(label=&quot;Select folder with Guide Frames&quot;, type=&quot;folder&quot;)
</code></pre>
","huggingface"
"75840485","Can I use LoRa and Prompt Tuning at the same time for text summarization with GPT?","2023-03-25 08:28:26","","3","1540","<nlp><huggingface-transformers><transformer-model><summarization><huggingface>","<p>LoRA is to insert and learn the rank composition matrix created by dimensionally reducing the weight matrix in the transformer. Prompt Tuning, on the other hand, typically uses a soft prompt that encodes the prompt within the model to learn, rather than a hard prompt that a person gives the task directly. Both are effective in lightening, especially prompt tuning, which is better than hard prompt use.</p>
<p>Both techniques can also be implemented using the peft module.</p>
<pre><code>from peft import get_peft_model, PeftModel, TaskType, LoraConfig, PromptTuningConfig, PromptTuningInit

for path,dirs,files in os.walk('/root/.cache/huggingface/hub/models--kakaobrain--kogpt'):
  for file in files:
    if file.endswith('tokenizer.json'):
      tokenizer_path = path
print(tokenizer_path)

prompt_config = PromptTuningConfig(
    task_type=TaskType.CAUSAL_LM,
    num_virtual_tokens=10,
    prompt_tuning_init=PromptTuningInit.TEXT,
    prompt_tuning_init_text=&quot;Read the following and summarize:&quot;,
    tokenizer_name_or_path=tokenizer_path
)

lora_config = LoraConfig(
    task_type = TaskType.CAUSAL_LM,
    r=8, lora_alpha=32, lora_dropout=0.1,
    target_modules = ['q_proj', 'v_proj'],
    # target_modules = r&quot;.*(q_proj|v_proj)&quot;,
)
</code></pre>
<p>However, the get_feft_model function receives only the model and one peft_config as parameters.</p>
<pre><code>peft_model = get_peft_model(base_model, prompt_config)
</code></pre>
<p>I want to use both techniques at the same time. How shall I do it?</p>
","huggingface"
"75829230","How to convert a Hugging Face Pytorch model (AutoTrain) to TorchScript (.pt) for deployment?","2023-03-24 01:03:21","","4","1598","<python><machine-learning><pytorch><huggingface-transformers><huggingface>","<p>I trained an image classification model using Hugging Face's AutoTrain service which left me with the following three files:</p>
<ul>
<li><code>config.json</code></li>
<li><code>preprocessor_config.json</code></li>
<li><code>pytorch_model.bin</code></li>
</ul>
<p>Here's what the 2 json files look like:</p>
<p><strong>preprocessor_config.json:</strong></p>
<pre><code>{
  &quot;do_normalize&quot;: true,
  &quot;do_rescale&quot;: true,
  &quot;do_resize&quot;: true,
  &quot;feature_extractor_type&quot;: &quot;ViTFeatureExtractor&quot;,
  &quot;image_mean&quot;: [0.485, 0.456, 0.406],
  &quot;image_processor_type&quot;: &quot;ViTImageProcessor&quot;,
  &quot;image_std&quot;: [0.229, 0.224, 0.225],
  &quot;resample&quot;: 3,
  &quot;rescale_factor&quot;: 0.00392156862745098,
  &quot;size&quot;: {
    &quot;height&quot;: 224,
    &quot;width&quot;: 224
  }
}
</code></pre>
<p><strong>config.json:</strong></p>
<pre><code>{
  &quot;_name_or_path&quot;: &quot;AutoTrain&quot;,
  &quot;architectures&quot;: [&quot;SwinForImageClassification&quot;],
  &quot;attention_probs_dropout_prob&quot;: 0.0,
  &quot;depths&quot;: [2, 2, 18, 2],
  &quot;drop_path_rate&quot;: 0.1,
  &quot;embed_dim&quot;: 128,
  &quot;encoder_stride&quot;: 32,
  &quot;hidden_act&quot;: &quot;gelu&quot;,
  &quot;hidden_dropout_prob&quot;: 0.0,
  &quot;hidden_size&quot;: 1024,
  &quot;id2label&quot;: {
    &quot;hello&quot;: &quot;0&quot;,
    &quot;world&quot;: &quot;1&quot;,
    // approx 60 more labels here...
  },
  &quot;image_size&quot;: 224,
  &quot;initializer_range&quot;: 0.02,
  &quot;label2id&quot;: {
    &quot;hello&quot;: &quot;0&quot;,
    &quot;world&quot;: &quot;1&quot;,
    // approx 60 more labels here...
  },
  &quot;layer_norm_eps&quot;: 1e-5,
  &quot;max_length&quot;: 128,
  &quot;mlp_ratio&quot;: 4.0,
  &quot;model_type&quot;: &quot;swin&quot;,
  &quot;num_channels&quot;: 3,
  &quot;num_heads&quot;: [4, 8, 16, 32],
  &quot;num_layers&quot;: 4,
  &quot;padding&quot;: &quot;max_length&quot;,
  &quot;patch_size&quot;: 4,
  &quot;path_norm&quot;: true,
  &quot;problem_type&quot;: &quot;single_label_classification&quot;,
  &quot;qkv_bias&quot;: true,
  &quot;torch_dtype&quot;: &quot;float32&quot;,
  &quot;transformers_version&quot;: &quot;4.25.1&quot;,
  &quot;use_absolute_embeddings&quot;: false,
  &quot;window_size&quot;: 7
}
</code></pre>
<p>Without changing anything I can make inferences on the model successfully using the following code:</p>
<pre><code>from PIL import Image
from transformers import pipeline

classifier = pipeline(
    &quot;image-classification&quot;,
    &quot;path/to/model&quot;,
)

image = Image.open(&quot;./test.jpg&quot;).convert(&quot;RGB&quot;)

print(classifier(image))
</code></pre>
<p>But now I need to export the model to <code>torchscript</code> (<code>.pt</code> file format) for deployment via <code>torchserve</code>. I’m trying to follow the guide here:</p>
<p><a href=""https://huggingface.co/docs/transformers/torchscript"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/torchscript</a></p>
<p>But am stuck at the “<strong>Creating the trace</strong>” part below:</p>
<pre><code>import torch
from transformers import AutoModelForImageClassification

model = AutoModelForImageClassification.from_pretrained(
    &quot;path/to/model&quot;,
    torchscript=True,
)

# The model needs to be in evaluation mode
model.eval()

# Creating the trace
traced_model = torch.jit.trace(model, [&lt;some_tensors_here??...&gt;]) # &lt;---- HERE!
torch.jit.save(traced_model, &quot;my_converted_model.pt&quot;)
</code></pre>
<p>From what I understand, I need to pass some dummy data to the model in a shape that it expects. How can I find out what this data structure should look like?</p>
<p>I’m pretty clueless when it comes to ML (just trying to get a little model working for my app). I also didn’t train the model myself but used Autotrain (which is a automl GUI tool) so I'm kind of left in the dark about how the model was made and what input data it uses/expects.</p>
<p>If someone would be so kind enough to tell me what to plug in to get this to work I'd really appreciate it! The goal is to convert the format I have now to torchscript so that I can either deploy it on a server using torchserve or run it on device by converting it to Playtorch to use in my React Native app. Both of these things expect a <code>.pt</code> file for the model as a starting point.</p>
","huggingface"
"75827730","Multilabel Text Classification using Hugging Face Models for TensorFlow","2023-03-23 20:29:45","","0","343","<tensorflow><huggingface-transformers><text-classification><huggingface>","<p>Trying to understand example of use  Hugging Face Model for Multilabel Text Classification using Tenroflow from <a href=""https://www.daniweb.com/programming/computer-science/tutorials/539042/multilabel-text-classification-using-hugging-face-models-for-tensorflow"" rel=""nofollow noreferrer"">https://www.daniweb.com/programming/computer-science/tutorials/539042/multilabel-text-classification-using-hugging-face-models-for-tensorflow</a></p>
<p>Please, help to understand using BinaryCrossentropy as loss and metrics while have 6-classes classification. Doesn't one should use CategoricalCrossentropy instead?</p>
","huggingface"
"75818179","What is the correct way to create a feature extractor for a hugging face (HF) ViT model?","2023-03-22 23:49:24","","4","2291","<deep-learning><pytorch><computer-vision><huggingface-transformers><huggingface>","<p>TLDR: is the correct way to extract features from a HF ViT model <code>outputs.pooler_output</code> or <code>outputs.last_hidden_state[:, 0]</code>? where outputs is <code>outputs: BaseModelOutputWithPooling = self.model(pixel_values=batch_xs)</code>.</p>
<hr />
<p>Given only the ViT model it's not clear what one should do to solve a vision
classification problem. I eventually converged to this answer (but I am unsure if it is correct or the best anymore, will provide full code at the end):</p>
<pre><code>        outputs: BaseModelOutputWithPooling = self.model(pixel_values=batch_xs)
        output: Tensor = self.dropout(outputs.last_hidden_state[:, 0])
        logits: Tensor = self.cls(output)
</code></pre>
<p>Intuitively it makes sense, we can to extract the features from the cls token position. However, once I printed all the layers for the ViTModel I would have personally chosen a different layer because it's right before the cls layer AND because printing the activations seem to be in a better range to be honest. I would have chosen the ones in the <code>(pooler): ViTPooler(...)</code> layer, right after the <code>Tanh()</code>. Doing that results in this:</p>
<pre><code>outputs: BaseModelOutputWithPooling = self.model(pixel_values=batch_xs)

outputs.pooler_output
tensor([[-0.3976, -0.8454, -0.0601,  ..., -0.2804, -0.1822,  0.1917],
        [-0.3392, -0.0248,  0.1346,  ..., -0.5822,  0.8779,  0.4147],
        [-0.2980, -0.8038, -0.1146,  ...,  0.2431, -0.0963,  0.7844],
        ...,
        [-0.1237, -0.7514,  0.7388,  ..., -0.8551,  0.1512,  0.6157],
        [ 0.5351, -0.9040,  0.0387,  ..., -0.0773,  0.2704, -0.0311],
        [ 0.2142, -0.3138,  0.0426,  ..., -0.5943,  0.2873,  0.4420]],
       grad_fn=&lt;TanhBackward&gt;)
outputs.last_hidden_state[:, 0]
tensor([[ 5.7313e-01, -2.1335e+00,  2.0491e-01,  ..., -1.2373e-01,
         -2.0056e-01, -4.8167e-01],
        [ 5.3309e-02, -1.6563e+00,  1.5719e+00,  ..., -1.3617e+00,
         -3.0064e-01, -2.0056e-01],
        [-2.0633e-02, -2.1370e+00,  9.9927e-01,  ..., -2.3584e+00,
          8.6123e-01, -1.2759e+00],
        ...,
        [ 3.9583e-01, -1.3500e+00,  1.7638e+00,  ..., -9.9536e-01,
          1.0843e+00, -4.4368e-01],
        [ 1.6026e+00, -6.4654e-01,  2.4882e+00,  ..., -1.0347e+00,
         -1.3160e-03, -2.4357e+00],
        [-1.2769e-02, -9.6574e-01,  1.6432e+00,  ..., -7.9090e-01,
          6.1669e-01,  3.2990e-01]], grad_fn=&lt;SelectBackward&gt;)
</code></pre>
<p>and sums for sanity checks</p>
<pre><code>outputs.pooler_output.sum()
tensor(3.8430, grad_fn=&lt;SumBackward0&gt;)
outputs.last_hidden_state[:, 0].sum()
tensor(-6.4373e-06, grad_fn=&lt;SumBackward0&gt;)
</code></pre>
<p>and shapes</p>
<pre><code>outputs.pooler_output.shape
torch.Size([25, 768])
outputs.last_hidden_state[:, 0].shape
torch.Size([25, 768])
</code></pre>
<p>which for <code>outputs.pooler_output.shape</code> look much better behaves. But my forward pass uses <code>outputs.last_hidden_state[:, 0]</code> for some reason.</p>
<p>Which one should I have used?</p>
<p>Full code:</p>
<pre><code>
class ViTForImageClassificationUU(nn.Module):
    def __init__(self,
                 num_classes: int,
                 image_size: int,  # 224 inet, 32 cifar, 84 mi, 28 mnist, omni...
                 criterion: Optional[Union[None, Callable]] = None,
                 # Note: USL agent does criterion not model usually for me e.g nn.Criterion()
                 cls_p_dropout: float = 0.0,
                 pretrained_name: str = None,
                 vitconfig: ViTConfig = None,
                 ):
        &quot;&quot;&quot;
        :param num_classes:
        :param pretrained_name: 'google/vit-base-patch16-224-in21k'  # what the diff with this one: &quot;google/vit-base-patch16-224&quot;
        &quot;&quot;&quot;
        super().__init__()
        if vitconfig is not None:
            raise NotImplementedError
            self.vitconfig = vitconfig
            print(f'You gave a config so everyone other param given is going to be ignored.')
        elif pretrained_name is not None:
            raise NotImplementedError
            # self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')
            self.model = ViTModel.from_pretrained(pretrained_name)
            print('Make sure you did not give a vitconfig or this pretrained name will be ignored.')
        else:
            self.num_classes = num_classes
            self.image_size = image_size
            self.vitconfig = ViTConfig(image_size=self.image_size)
            self.model = ViTModel(self.vitconfig)
        assert cls_p_dropout == 0.0, 'Error, for now only p dropout for cls is zero until we figure out if we need to ' \
                                     'change all the other p dropout layers too.'
        self.dropout = nn.Dropout(cls_p_dropout)
        self.cls = nn.Linear(self.model.config.hidden_size, num_classes)
        self.criterion = None if criterion is None else criterion

    def forward(self, batch_xs: Tensor, labels: Tensor = None) -&gt; Tensor:
        &quot;&quot;&quot;
        Forward pass of vit. I added the &quot;missing&quot; cls (and dropout layer before it) to act on the first cls
        token embedding. Remaining token embeddings are ignored/not used.

        I think the feature extractor only normalizes the data for you, doesn't seem to even make it into a seq, see:
        ...
        so idk why it's needed but an example using it can be found here:
            - colab https://colab.research.google.com/drive/1Z1lbR_oTSaeodv9tTm11uEhOjhkUx1L4?usp=sharing#scrollTo=cGDrb1Q4ToLN
            - blog with trainer https://huggingface.co/blog/fine-tune-vit
            - single PIL notebook https://github.com/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Quick_demo_of_HuggingFace_version_of_Vision_Transformer_inference.ipynb
        &quot;&quot;&quot;
        outputs: BaseModelOutputWithPooling = self.model(pixel_values=batch_xs)
        output: Tensor = self.dropout(outputs.last_hidden_state[:, 0])
        logits: Tensor = self.cls(output)
        if labels is None:
            assert logits.dtype == torch.float32
            return logits  # this is what my usl agent does ;)
        else:
            raise NotImplementedError
            assert labels.dtype == torch.long
            #   loss = self.criterion(logits.view(-1, self.num_classes), labels.view(-1))
            loss = self.criterion(logits, labels)
            return loss, logits

    def get_embedding(self, batch_xs: Tensor) -&gt; Tensor:
        &quot;&quot;&quot;
        Get the feature embedding of the first cls token.

        Details:
        By observing the ViTLayer, the (pooler) ViTPoooler(...) has an activation and a Tanh() layer.
        From playing around
        &lt;TanhBackward&gt;, so it seems that it the right one. Plus, printing
            outputs.pooler_output.sum()
            tensor(3.8430, grad_fn=&lt;SumBackward0&gt;)
        looks more sensible than trying to get the features for the cls position manually:
            outputs.last_hidden_state[:, 0, :].sum()
            tensor(-6.4373e-06, grad_fn=&lt;SumBackward0&gt;)
        which looked weird.
        &quot;&quot;&quot;
        # outputs: BaseModelOutputWithPooling = self.model(pixel_values=batch_xs)
        outputs: BaseModelOutputWithPooling = self.model(pixel_values=batch_xs)
        feat = outputs.pooler_output
        # out = model.model(x)
        # hidden_states = out.last_hidden_state
        # # Get the CLS token's features (position 0)
        # cls_features = hidden_states[:, 0]
        # return out
        # Obtain the outputs from the base ViT model
        # outputs = self.model(pixel_values, *args, **kwargs)
        # pooled_output = outputs.pooler_output
        # image_representation = outputs.last_hidden_state[:, 0, :]
        return feat

    def _assert_its_random_model(self):
        from uutils.torch_uu import norm
        pre_trained_model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')
        print(f'----&gt; {norm(pre_trained_model)=}')
        print(f'----&gt; {norm(self)=}')
        assert norm(pre_trained_model) &gt; norm(self), f'Random models usually have smaller weight size but got ' \
                                                     f'{norm(pre_trained_model)}{norm(self)}'


def get_vit_get_vit_model_and_model_hps(vitconfig: ViTConfig = None,
                                        num_classes: int = 5,
                                        image_size: int = 84,  # 224 inet, 32 cifar, 84 mi, 28 mnist, omni...
                                        criterion: Optional[Union[None, Callable]] = None,  # for me agent does it
                                        cls_p_dropout: float = 0.0,
                                        pretrained_name: str = None,
                                        ) -&gt; tuple[nn.Module, dict]:
    &quot;&quot;&quot;get vit for mi, only num_classes = 5 and image size 84 is needed. &quot;&quot;&quot;
    model_hps: dict = dict(vitconfig=vitconfig,
                           num_classes=num_classes,
                           image_size=image_size,
                           criterion=criterion,
                           cls_p_dropout=cls_p_dropout,
                           pretrained_name=pretrained_name)
    model: nn.Module = ViTForImageClassificationUU(**model_hps)
    print('Its recommended to set args.allow_unused = True for ViT models.')
    return model, model_hps

def vit_forward_pass():
    # - for determinism
    import random
    import numpy as np
    random.seed(0)
    torch.manual_seed(0)
    np.random.seed(0)

    # - options for number of tasks/meta-batch size
    device = torch.device(f&quot;cuda:{0}&quot; if torch.cuda.is_available() else &quot;cpu&quot;)

    # - get my vit model
    vitconfig: ViTConfig = ViTConfig()
    # model = ViTForImageClassificationUU(num_classes=64 + 1100, image_size=84)
    model = get_vit_get_vit_model_and_model_hps(vitconfig, num_classes=64 + 1100, image_size=84)
    criterion = nn.CrossEntropyLoss()
    # to device
    model.to(device)
    criterion.to(device)

    # - forward pass
    x = torch.rand(5, 3, 84, 84)
    y = torch.randint(0, 64 + 1100, (5,))
    logits = model(x)
    loss = criterion(logits, y)
    print(f'{loss=}')
</code></pre>
<p>cross: <a href=""https://discuss.huggingface.co/t/what-is-the-correct-way-to-create-a-feature-extractor-for-a-hugging-face-hf-vit-model/34441"" rel=""nofollow noreferrer"">https://discuss.huggingface.co/t/what-is-the-correct-way-to-create-a-feature-extractor-for-a-hugging-face-hf-vit-model/34441</a></p>
","huggingface"
"75814047","How to use Huggingface Trainer with multiple GPUs?","2023-03-22 15:10:23","","10","13393","<machine-learning><pytorch><huggingface-transformers><huggingface>","<p>Say I have the following model (from <a href=""https://huggingface.co/course/chapter7/6"" rel=""noreferrer"">this</a> script):</p>
<pre><code>from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig

config = AutoConfig.from_pretrained(
    &quot;gpt2&quot;,
    vocab_size=len(tokenizer),
    n_ctx=context_length,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
)
model = GPT2LMHeadModel(config)
</code></pre>
<p>I'm currently using this training arguments for the Trainer:</p>
<pre><code>from transformers import Trainer, TrainingArguments

args = TrainingArguments(
    output_dir=&quot;codeparrot-ds&quot;,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    evaluation_strategy=&quot;steps&quot;,
    eval_steps=5_000,
    logging_steps=5_000,
    gradient_accumulation_steps=8,
    num_train_epochs=1,
    weight_decay=0.1,
    warmup_steps=1_000,
    lr_scheduler_type=&quot;cosine&quot;,
    learning_rate=5e-4,
    save_steps=5_000,
    fp16=True,
    push_to_hub=True,
)

trainer = Trainer(
    model=model,
    tokenizer=tokenizer,
    args=args,
    data_collator=data_collator,
    train_dataset=tokenized_datasets[&quot;train&quot;],
    eval_dataset=tokenized_datasets[&quot;valid&quot;],
)
trainer.train()
</code></pre>
<p>How can I adapt this so the Trainer will use multiple GPUs (e.g., 8)?</p>
<p>I found <a href=""https://stackoverflow.com/questions/61736317/huggingface-transformers-gpt2-generate-multiple-gpus"">this</a> SO question, but they didn't use the Trainer and just used PyTorch's <code>DataParallel</code></p>
<pre><code>model = torch.nn.DataParallel(model, device_ids=[0,1])
</code></pre>
<p>The Huggingface <a href=""https://huggingface.co/docs/transformers/perf_train_gpu_many"" rel=""noreferrer"">docs on training with multiple GPUs</a> are not really clear to me and don't have an example of using the Trainer. Instead, I found <a href=""https://huggingface.co/docs/transformers/run_scripts#distributed-training-and-mixed-precision"" rel=""noreferrer"">here</a> that they add arguments to their python file with <code>nproc_per_node</code>, but that seems too specific to their script and not clear how to use in general. This is in contrary to <a href=""https://discuss.huggingface.co/t/training-using-multiple-gpus/1279"" rel=""noreferrer"">this</a> discussion on their forum that says <code>&quot;The Trainer class automatically handles multi-GPU training, you don’t have to do anything special.&quot;</code>. So this is confusing as on one hand they're mentioning that there are things needed to be done to train on multiple GPUs, and also saying that the Trainer handles it automatically. So I'm not sure what to do.</p>
","huggingface"
"75813848","How to save and retrieve trained ai model locally from python backend","2023-03-22 14:54:09","","0","205","<python><local-storage><huggingface-transformers><huggingface>","<p>I am trying to store a hugging face transformer ai model for each individual user locally instead of storing it in a database.</p>
<pre><code>model = BertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=2)
model.save_pretrained(&quot;locallystored&quot;)
</code></pre>
<p>I then would like to train that users model that I stored locally. I send a websocket to backend to train the model stored in the users local machine.</p>
<pre><code>socketio.on(&quot;willtraintheusersmodel&quot;)
    model = BertForSequenceClassification.from_pretrained('locallystored')
    *training*
    model.train() 
    model.save_pretrained(&quot;locallystored&quot;)
    #then I update the changes made
</code></pre>
<p>How can I save a model in the users computer locally and retrieve it anytime i need to train or make predictions? I am trying to make this work in production with multiple users.</p>
<p>I have a python-socketio backend with react frontend</p>
<p>Any help would be extremely appreciated thanks.</p>
","huggingface"
"75802877","Issues when using HuggingFace `accelerate` with `fp16`","2023-03-21 15:02:20","","12","10382","<pytorch><huggingface><distributed-training>","<p>I'm trying to use <code>accelerate</code> module to parallelize my model training. But I have troubles to use it when training models with <code>fp16</code>. If I load the model with <code>torch_dtype=torch.float16</code>, I got <code>ValueError: Attempting to unscale FP16 gradients.</code>. But if I don't load the model with half precision I will get a CUDA out of memory error. Below are the details of this problem:</p>
<p>I'm fine tuning a 2.7B CLM on one A100 - 40GB GPU (I will be working on a much larger model, but I want to use this model to test my training process to make sure everything works as expected). I initially started with a training script without <code>accelerate</code> and without <code>Trainer</code>. I can successfully train the model when I load the model in half precision with:</p>
<pre class=""lang-py prettyprint-override""><code># here device = 'cuda'
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(device)
</code></pre>
<p>I have to load the model in half precision otherwise I will get a CUDA out of memory error. I simplified my script and uploaded <a href=""https://gist.github.com/weiqi-dyania/a16cb88ea3dc0433c6b698c6bb26911d"" rel=""noreferrer"">here</a> as a demonstration. When loading the model with half precision, it takes about 27GB GPU memory out of 40GB in the training process. It has plenty of rooms left on the GPU memory.</p>
<p>Now I want to utilize the <code>accelerate</code> module (potentially with <code>deepspeed</code> for larger models in the future) in my training script. I made the following changes:</p>
<pre><code>model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)
accelerator = Accelerator(cpu=False, mixed_precision='fp16')
...
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)
...
# in training loop, I updated `lose.backward()` to:
accelerator.backward(loss)
</code></pre>
<p><a href=""https://gist.github.com/weiqi-dyania/054de78141a76239ff568542f3883a51#file-run_clm_no_trainer_accelerator-py"" rel=""noreferrer"">Here</a> is the updated script. I also configured <code>accelerate</code> with <code>accelerate config</code>. The <code>default_config.yaml</code> can be found from the same Gist.</p>
<p>Now when I tried to launch the script on the same machine with <code>accelerate launch --fp16 &lt;script_path&gt;</code>. I got an error <code>ValueError: Attempting to unscale FP16 gradients.</code>. So I removed <code>torch_dtype=torch.float16</code> from model loading and rely on <code>accelerate</code> to downcast the model weight to half precision. But now I got CUDA out of memory error.</p>
<p>To summarize:</p>
<ol>
<li>I can train the model successfully when loading it with <code>torch_dtype=torch.float16</code> and not using <code>accelerate</code>.</li>
<li>With <code>accelerate</code>, I cannot load the model with <code>torch_dtype=torch.float16</code>. It gives <code>ValueError: Attempting to unscale FP16 gradients.</code>.</li>
<li>If I don't load the model with <code>torch_dtype=torch.float16</code> and use <code>fp16</code> with <code>accelerate</code>, I got CUDA out of memory error.</li>
</ol>
<p>So my question is: how can I train the model on a single A100 - 40GB GPU with <code>accelerate</code>?</p>
<p>I included one script <a href=""https://gist.github.com/weiqi-dyania/a16cb88ea3dc0433c6b698c6bb26911d"" rel=""noreferrer"">without <code>accelerate</code></a> and one <a href=""https://gist.github.com/weiqi-dyania/054de78141a76239ff568542f3883a51#file-run_clm_no_trainer_accelerator-py"" rel=""noreferrer"">with <code>accelerate</code></a>. I would like them to have the same behavior in terms of GPU memory consumption.</p>
","huggingface"
"75783029","PyTorch with Transformer - finetune GPT2 throws index out of range Error","2023-03-19 15:24:24","","2","486","<python><pytorch><huggingface><gpt-2>","<p>in my Jupiter i have the following code. I can not figure out why this throws a <code>IndexError: index out of range in self</code> error.</p>
<p>here ist the code:</p>
<pre><code>!pip install torch
!pip install torchvision
!pip install transformers
</code></pre>
<pre><code>import torch
from torch.utils.data import Dataset

class MakeDataset(Dataset):
    def __init__(self, tokenized_texts, block_size):
        self.examples = []
        for tokens in tokenized_texts:
            # truncate the tokens if they are longer than block_size
            if len(tokens) &gt; block_size:
                tokens = tokens[:block_size]
            # add padding tokens if the tokens are shorter than block_size
            while len(tokens) &lt; block_size:
                tokens.append(tokenizer.pad_token_id)
            self.examples.append(torch.tensor(tokens, dtype=torch.long))

    def __len__(self):
        return len(self.examples)

    def __getitem__(self, item):
        return self.examples[item]
</code></pre>
<pre><code>from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments, AutoTokenizer, \
    AutoModelWithLMHead, GPT2Tokenizer

# Load the tokenizer and model
tokenizer = GPT2Tokenizer.from_pretrained('gpt2', padding_side='right')
model = AutoModelWithLMHead.from_pretrained('gpt2')

PAD_TOKEN = '&lt;PAD&gt;'
tokenizer.add_special_tokens({'pad_token': PAD_TOKEN})

# Load text corpus
with open(&quot;texts.txt&quot;, encoding=&quot;utf-8&quot;) as f:
    texts = f.read().splitlines()

print(len(texts) , &quot; lines of text.&quot;)

# Tokenize the texts
tokenized_texts = []
for text in texts:
    tokens = tokenizer.encode(text, padding='max_length', truncation='only_first')
    if len(tokens) &gt; 0:
        tokenized_texts.append(tokens)

# gemerate a dataset
dataset = MakeDataset(tokenized_texts, block_size=1024)
print(&quot;Dataset length: &quot;, len(dataset))


# Create a DataCollatorForLanguageModeling object
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

# Define the training arguments
training_args = TrainingArguments(
    output_dir='./results',  # output directory
    num_train_epochs=5,  # total number of training epochs
    per_device_train_batch_size=16,  # batch size per device during training
    save_steps=1000,  # number of steps between saving checkpoints
    save_total_limit=2,  # limit the total amount of checkpoints saved
    prediction_loss_only=True,  # only calculate loss on prediction tokens
    learning_rate=1e-5,  # learning rate
    warmup_steps=500,  # number of warmup steps for learning rate scheduler
    fp16=False  # enable mixed precision training with apex
)

# Create a Trainer object
trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=dataset
)

# Train the model
trainer.train()

# Save the trained model
trainer.save_model('./fine-tuned-gpt2')
</code></pre>
<p>The text file at the moment looks very simple:</p>
<pre><code>Hello, my name is Paul.
My cat can sing.
</code></pre>
<p>The full error is:</p>
<pre><code>IndexError                                Traceback (most recent call last)
Cell In[140], line 54
     46 trainer = Trainer(
     47     model=model,
     48     args=training_args,
     49     data_collator=data_collator,
     50     train_dataset=dataset
     51 )
     53 # Train the model
---&gt; 54 trainer.train()
     56 # Save the trained model
     57 trainer.save_model('./fine-tuned-gpt2')

File /opt/homebrew/lib/python3.11/site-packages/transformers/trainer.py:1633, in Trainer.train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)
   1628     self.model_wrapped = self.model
   1630 inner_training_loop = find_executable_batch_size(
   1631     self._inner_training_loop, self._train_batch_size, args.auto_find_batch_size
   1632 )
-&gt; 1633 return inner_training_loop(
   1634     args=args,
   1635     resume_from_checkpoint=resume_from_checkpoint,
   1636     trial=trial,
   1637     ignore_keys_for_eval=ignore_keys_for_eval,
   1638 )

File /opt/homebrew/lib/python3.11/site-packages/transformers/trainer.py:1902, in Trainer._inner_training_loop(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)
   1900         tr_loss_step = self.training_step(model, inputs)
   1901 else:
-&gt; 1902     tr_loss_step = self.training_step(model, inputs)
   1904 if (
   1905     args.logging_nan_inf_filter
   1906     and not is_torch_tpu_available()
   1907     and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))
   1908 ):
   1909     # if loss is nan or inf simply add the average of previous logged losses
   1910     tr_loss += tr_loss / (1 + self.state.global_step - self._globalstep_last_logged)

File /opt/homebrew/lib/python3.11/site-packages/transformers/trainer.py:2645, in Trainer.training_step(self, model, inputs)
   2642     return loss_mb.reduce_mean().detach().to(self.args.device)
   2644 with self.compute_loss_context_manager():
-&gt; 2645     loss = self.compute_loss(model, inputs)
   2647 if self.args.n_gpu &gt; 1:
   2648     loss = loss.mean()  # mean() to average on multi-gpu parallel training

File /opt/homebrew/lib/python3.11/site-packages/transformers/trainer.py:2677, in Trainer.compute_loss(self, model, inputs, return_outputs)
   2675 else:
   2676     labels = None
-&gt; 2677 outputs = model(**inputs)
   2678 # Save past state if it exists
   2679 # TODO: this needs to be fixed and made cleaner later.
   2680 if self.args.past_index &gt;= 0:

File /opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501, in Module._call_impl(self, *args, **kwargs)
   1496 # If we don't have any hooks, we want to skip the rest of the logic in
   1497 # this function, and just call forward.
   1498 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1499         or _global_backward_pre_hooks or _global_backward_hooks
   1500         or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1501     return forward_call(*args, **kwargs)
   1502 # Do not call functions when jit is used
   1503 full_backward_hooks, non_full_backward_hooks = [], []

File /opt/homebrew/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:1075, in GPT2LMHeadModel.forward(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)
   1067 r&quot;&quot;&quot;
   1068 labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
   1069     Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set
   1070     `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`
   1071     are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`
   1072 &quot;&quot;&quot;
   1073 return_dict = return_dict if return_dict is not None else self.config.use_return_dict
-&gt; 1075 transformer_outputs = self.transformer(
   1076     input_ids,
   1077     past_key_values=past_key_values,
   1078     attention_mask=attention_mask,
   1079     token_type_ids=token_type_ids,
   1080     position_ids=position_ids,
   1081     head_mask=head_mask,
   1082     inputs_embeds=inputs_embeds,
   1083     encoder_hidden_states=encoder_hidden_states,
   1084     encoder_attention_mask=encoder_attention_mask,
   1085     use_cache=use_cache,
   1086     output_attentions=output_attentions,
   1087     output_hidden_states=output_hidden_states,
   1088     return_dict=return_dict,
   1089 )
   1090 hidden_states = transformer_outputs[0]
   1092 # Set device for model parallelism

File /opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501, in Module._call_impl(self, *args, **kwargs)
   1496 # If we don't have any hooks, we want to skip the rest of the logic in
   1497 # this function, and just call forward.
   1498 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1499         or _global_backward_pre_hooks or _global_backward_hooks
   1500         or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1501     return forward_call(*args, **kwargs)
   1502 # Do not call functions when jit is used
   1503 full_backward_hooks, non_full_backward_hooks = [], []

File /opt/homebrew/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:842, in GPT2Model.forward(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)
    839 head_mask = self.get_head_mask(head_mask, self.config.n_layer)
    841 if inputs_embeds is None:
--&gt; 842     inputs_embeds = self.wte(input_ids)
    843 position_embeds = self.wpe(position_ids)
    844 hidden_states = inputs_embeds + position_embeds

File /opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501, in Module._call_impl(self, *args, **kwargs)
   1496 # If we don't have any hooks, we want to skip the rest of the logic in
   1497 # this function, and just call forward.
   1498 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1499         or _global_backward_pre_hooks or _global_backward_hooks
   1500         or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1501     return forward_call(*args, **kwargs)
   1502 # Do not call functions when jit is used
   1503 full_backward_hooks, non_full_backward_hooks = [], []

File /opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/sparse.py:162, in Embedding.forward(self, input)
    161 def forward(self, input: Tensor) -&gt; Tensor:
--&gt; 162     return F.embedding(
    163         input, self.weight, self.padding_idx, self.max_norm,
    164         self.norm_type, self.scale_grad_by_freq, self.sparse)

File /opt/homebrew/lib/python3.11/site-packages/torch/nn/functional.py:2210, in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)
   2204     # Note [embedding_renorm set_grad_enabled]
   2205     # XXX: equivalent to
   2206     # with torch.no_grad():
   2207     #   torch.embedding_renorm_
   2208     # remove once script supports set_grad_enabled
   2209     _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)
-&gt; 2210 return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)

IndexError: index out of range in self
</code></pre>
<p>Can someone tell me what I have done wrong with the training setup?</p>
<p>++ UPDATE ++</p>
<p>I change the <code>MakeDataset</code> to <code>TextDataset</code> to get a pt tensor back:</p>
<pre><code>class TextDataset(torch.utils.data.Dataset):
    def __init__(self, encodings):
        self.encodings = encodings

    def __getitem__(self, idx):
        return {key: tensor[idx] for key, tensor in self.encodings.items()}

    def __len__(self):
        return len(self.encodings.input_ids)
</code></pre>
<p>the output of <code>print(dataset[0])</code> is:</p>
<pre><code>{'input_ids': tensor([15496,    11,   616,  1438,   318,  3362,    13, 50257, 50257, 50257,
        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
        50257, 50257]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
</code></pre>
<p>and with</p>
<pre><code>tokenized_texts = tokenizer(texts, padding='max_length', truncation=True, return_tensors=&quot;pt&quot;)
</code></pre>
<p>to pad them to the models length:</p>
<pre><code>6  lines of text.
Dataset length:  6
{'input_ids': tensor([[15496,    11,   616,  ..., 50257, 50257, 50257],
        [ 3666,  3797,   460,  ..., 50257, 50257, 50257],
        [32423,  1408, 46097,  ..., 50257, 50257, 50257],
        [10020,  1044,  6877,  ..., 50257, 50257, 50257],
        [31319,   288,   292,  ..., 50257, 50257, 50257],
        [ 7447, 24408,  8834,  ..., 50257, 50257, 50257]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]])}
</code></pre>
<p>But I still get the same error.
I also deleted all caches.</p>
","huggingface"
"75780103","HuggingFace Transformers Trainer._maybe_log_save_evaluate IndexError: invalid index to scalar variable","2023-03-19 04:31:40","75792634","0","737","<python><pytorch><nlp><huggingface-transformers><huggingface>","<p>So, I'm working on fine tuning a BART model for question generation, and it seems to be going through training okay. Then all of a sudden, it stops at the end of the first validation with an <code>IndexError</code> which you can see below. The problem is occurring in the <code>Trainer._maybe_log_save_evaluate</code> method that is being called.</p>
<p><a href=""https://i.sstatic.net/jV2jN.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/jV2jN.png"" alt=""IndexError: invalid index to scalar variable."" /></a></p>
<p>Here is my code for setting up the model, tokenizer, dataset, etc.:</p>
<pre class=""lang-py prettyprint-override""><code>from datasets import load_dataset
from evaluate import load
from accelerate import Accelerator
from transformers import BartForConditionalGeneration, BartConfig, BartTokenizer
from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer 

dataset = load_dataset(&quot;squad&quot;)
metric = load(&quot;squad&quot;)
accelerator = Accelerator()

def model_init():
  config = BartConfig()
  return accelerator.prepare(BartForConditionalGeneration(config).from_pretrained(&quot;facebook/bart-base&quot;).cuda())

tokenizer = accelerator.prepare(BartTokenizer.from_pretrained(&quot;facebook/bart-base&quot;))

def preprocess_function(data):
  inputs = tokenizer(data['context'], add_special_tokens=True, max_length=256, padding=&quot;max_length&quot;, truncation=True)
  targets = tokenizer(data['question'], add_special_tokens=True, max_length=32, padding=&quot;max_length&quot;, truncation=True)
  return {'input_ids': inputs['input_ids'], 'attention_mask': inputs['attention_mask'], 'labels': targets['input_ids']}

dataset = dataset.map(preprocess_function, batched=True).shuffle(seed=777)

training_args = Seq2SeqTrainingArguments(
  output_dir=&quot;./results&quot;,
  evaluation_strategy=&quot;steps&quot;,
  eval_steps=500,
  save_steps=50000,
  learning_rate=2e-5,
  per_device_train_batch_size=4,
  per_device_eval_batch_size=4,
  num_train_epochs=2,
  weight_decay=0.01,
  predict_with_generate=True,
)

def compute_metrics(eval_pred):
  predictions, labels = eval_pred
  predictions = predictions.argmax(axis=-1)
  return metric.compute(predictions=predictions, references=labels)

trainer = Seq2SeqTrainer(
  args=training_args,
  train_dataset=dataset[&quot;train&quot;],
  eval_dataset=dataset[&quot;validation&quot;],
  tokenizer=tokenizer,
  model_init=model_init,
  compute_metrics=compute_metrics,
)

trainer.train()
</code></pre>
<p>I can't seem to figure out why this is happening and nothing I've found online has helped.</p>
","huggingface"
"75775321","Libretranslate (+ Huggingface Transformers) - Cannot translate text: Error(s) in loading state_dict for Tokenizer: Missing key(s) in state_dict:","2023-03-18 10:38:32","75831862","-1","290","<tokenize><huggingface>","<p>Python 3.10.6, Libretranslate 1.3.10, on Ubuntu 22.04
Installed libretranslate with <code>pip install libretranslate</code>, ran it with <code>libretranslate --host 0.0.0.0 --port 5001</code>. Everything worked as expected. Note: I ran pip as root, which gave me a warning to not use pip as root but I ignored it at that point.
Then I created various screens with <code>screen -S libretranslateX</code>. Started running libretranslate on a total of four screens, each with a different port. Everything works. One day later, I check if everything still works, but now when I try to translate a text, I always get this error message</p>
<p><em>Cannot translate text: Error(s) in loading state_dict for Tokenizer: Missing key(s) in state_dict: &quot;mwt_clf.weight&quot;, &quot;mwt_clf.bias&quot;, &quot;mwt_clf2.weight&quot;.</em></p>
<p>I stopped all services. Reinstalled libretranslate with <code>pip install --force-reinstall --no-cache-dir libretranslate</code>. Uninstalled with <code>pip uninstall libretranslate</code>. Repeated everything in a venv. I tried older libretranslate versions. It doesn't work.
What's strange is that the libretranslate /detect endpoint still works, it's just the /translate endpoint which is returning the error.</p>
<p>The browser console returns this error: POST <a href=""http://example.com:5001/translate"" rel=""nofollow noreferrer"">http://example.com:5001/translate</a> 500 (INTERNAL SERVER ERROR)</p>
<p>I haven't found anything related to this error anywhere. Hence I come to you for help. Any feedback will be greatly appreciated!</p>
<p>Edit:
Apparently, this isn't just a problem of libretranslate but for huggingface transformers as well. I have installed the following packages on the same server in order to work with huggingface models:
&quot;pinferencia[streamlit]&quot; transformers[torch] sentencepiece nltk stanza and git+https://github.com/PrithivirajDamodaran/Parrot_Paraphraser.git
When I try to work with these models, I currently get the same error message I also get on libretranslate</p>
","huggingface"
"75746687","Is it possible to save the training/validation loss in a list during training in HuggingFace?","2023-03-15 15:16:07","75750666","4","2972","<machine-learning><pytorch><huggingface-transformers><huggingface>","<p>I'm currently training my model using the HuggingFace <code>Trainer</code> class:</p>
<pre><code>from transformers import Trainer, TrainingArguments

args = TrainingArguments(
    output_dir=&quot;codeparrot-ds&quot;,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    evaluation_strategy=&quot;steps&quot;,
    eval_steps=5_000,
    logging_steps=5_000,
    gradient_accumulation_steps=8,
    num_train_epochs=1,
    weight_decay=0.1,
    warmup_steps=1_000,
    lr_scheduler_type=&quot;cosine&quot;,
    learning_rate=5e-4,
    save_steps=5_000,
    fp16=True,
    push_to_hub=True,
)

trainer = Trainer(
    model=model,
    tokenizer=tokenizer,
    args=args,
    data_collator=data_collator,
    train_dataset=tokenized_datasets[&quot;train&quot;],
    eval_dataset=tokenized_datasets[&quot;valid&quot;],
)
</code></pre>
<p>This prints the loss during training, but I can't figure out how to save it so that I can plot it later. Note that I need both the training and validation losses during training.</p>
","huggingface"
"75741187","Not able to use map() or select(range()) with Huggingface Dataset library, gives dill_.dill has no attribute log","2023-03-15 06:32:01","","2","686","<python><nlp><huggingface><dill><huggingface-datasets>","<p>I'm not able to do dataset.map() or dataset.select(range(10)) with huggingface Datasets library in colab. It says <code>dill_.dill has no attribute log</code>
I have tried with different dill versions, but no luck.
I tried with older versions of dill lib but they were also giving same error.
Is there a way to fix the issue? Or, any work around?
Any leads will be helpful</p>
<p>stack trace:</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-39-dd9b972a8f3f&gt; in &lt;module&gt;
      1 test_data = load_dataset(""scientific_papers"", ""arxiv"", ignore_verifications=True, split=""test"")
      2 print(test_data)
----&gt; 3 data = test_data.select(range(10))

16 frames
/usr/local/lib/python3.9/dist-packages/datasets/arrow_dataset.py in wrapper(*args, **kwargs)
    155         }
    156         # apply actual function
--&gt; 157         out: Union[""Dataset"", ""DatasetDict""] = func(self, *args, **kwargs)
    158         datasets: List[""Dataset""] = list(out.values()) if isinstance(out, dict) else [out]
    159         # re-apply format to the output

/usr/local/lib/python3.9/dist-packages/datasets/fingerprint.py in wrapper(*args, **kwargs)
    155                     if kwargs.get(fingerprint_name) is None:
    156                         kwargs_for_fingerprint[""fingerprint_name""] = fingerprint_name
--&gt; 157                         kwargs[fingerprint_name] = update_fingerprint(
    158                             self._fingerprint, transform, kwargs_for_fingerprint
    159                         )

/usr/local/lib/python3.9/dist-packages/datasets/fingerprint.py in update_fingerprint(fingerprint, transform, transform_args)
    103     for key in sorted(transform_args):
    104         hasher.update(key)
--&gt; 105         hasher.update(transform_args[key])
    106     return hasher.hexdigest()
    107 

/usr/local/lib/python3.9/dist-packages/datasets/fingerprint.py in update(self, value)
     55     def update(self, value):
     56         self.m.update(f""=={type(value)}=="".encode(""utf8""))
---&gt; 57         self.m.update(self.hash(value).encode(""utf-8""))
     58 
     59     def hexdigest(self):

/usr/local/lib/python3.9/dist-packages/datasets/fingerprint.py in hash(cls, value)
     51             return cls.dispatch[type(value)](cls, value)
     52         else:
---&gt; 53             return cls.hash_default(value)
     54 
     55     def update(self, value):

/usr/local/lib/python3.9/dist-packages/datasets/fingerprint.py in hash_default(cls, value)
     44     @classmethod
     45     def hash_default(cls, value):
---&gt; 46         return cls.hash_bytes(dumps(value))
     47 
     48     @classmethod

/usr/local/lib/python3.9/dist-packages/datasets/utils/py_utils.py in dumps(obj)
    387     file = StringIO()
    388     with _no_cache_fields(obj):
--&gt; 389         dump(obj, file)
    390     return file.getvalue()
    391 

/usr/local/lib/python3.9/dist-packages/datasets/utils/py_utils.py in dump(obj, file)
    359 def dump(obj, file):
    360     """"""pickle an object to a file""""""
--&gt; 361     Pickler(file, recurse=True).dump(obj)
    362     return
    363 

/usr/local/lib/python3.9/dist-packages/dill/_dill.py in dump(self, obj)
    392         f = filename
    393     else:
--&gt; 394         f = open(filename, 'wb')
    395     try:
    396         if byref:

/usr/lib/python3.9/pickle.py in dump(self, obj)
    485         if self.proto &gt;= 4:
    486             self.framer.start_framing()
--&gt; 487         self.save(obj)
    488         self.write(STOP)
    489         self.framer.end_framing()

/usr/local/lib/python3.9/dist-packages/dill/_dill.py in save(self, obj, save_persistent_id)
    386 def dump_session(filename='/tmp/session.pkl', main=None, byref=False, **kwds):
    387     """"""pickle the current state of __main__ to a file""""""
--&gt; 388     from .settings import settings
    389     protocol = settings['protocol']
    390     if main is None: main = _main_module

/usr/lib/python3.9/pickle.py in save(self, obj, save_persistent_id)
    558             f = self.dispatch.get(t)
    559             if f is not None:
--&gt; 560                 f(self, obj)  # Call unbound method with explicit self
    561                 return
    562 

/usr/local/lib/python3.9/dist-packages/dill/_dill.py in save_singleton(pickler, obj)
   1524 def pickles(obj,exact=False,safe=False,**kwds):
   1525     """"""
-&gt; 1526     Quick check if object pickles with dill.
   1527 
   1528     If *exact=True* then an equality test is done to check if the reconstructed

/usr/lib/python3.9/pickle.py in save_reduce(self, func, args, state, listitems, dictitems, state_setter, obj)
    689             write(NEWOBJ)
    690         else:
--&gt; 691             save(func)
    692             save(args)
    693             write(REDUCE)

/usr/local/lib/python3.9/dist-packages/dill/_dill.py in save(self, obj, save_persistent_id)
    386 def dump_session(filename='/tmp/session.pkl', main=None, byref=False, **kwds):
    387     """"""pickle the current state of __main__ to a file""""""
--&gt; 388     from .settings import settings
    389     protocol = settings['protocol']
    390     if main is None: main = _main_module

/usr/lib/python3.9/pickle.py in save(self, obj, save_persistent_id)
    558             f = self.dispatch.get(t)
    559             if f is not None:
--&gt; 560                 f(self, obj)  # Call unbound method with explicit self
    561                 return
    562 

/usr/local/lib/python3.9/dist-packages/datasets/utils/py_utils.py in save_function(pickler, obj)
    583         dill._dill.log.info(""# F1"")
    584     else:
--&gt; 585         dill._dill.log.info(""F2: %s"" % obj)
    586         name = getattr(obj, ""__qualname__"", getattr(obj, ""__name__"", None))
    587         dill._dill.StockPickler.save_global(pickler, obj, name=name)

AttributeError: module 'dill._dill' has no attribute 'log</code></pre>
</div>
</div>
</p>
","huggingface"
"75734019","How to load a smaller GPT2 model on HuggingFace?","2023-03-14 13:47:55","75737062","1","1769","<machine-learning><pytorch><huggingface-transformers><huggingface>","<p>I know I can load the smallest GPT2 variant using</p>
<pre><code>from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig

config = AutoConfig.from_pretrained(
    &quot;gpt2&quot;,
    vocab_size=len(tokenizer),
    n_ctx=context_length,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
)
model = GPT2LMHeadModel(config)
model_size = sum(t.numel() for t in model.parameters())
print(f&quot;GPT-2 size: {model_size/1000**2:.1f}M parameters&quot;)
&gt;&gt;&gt; GPT-2 size: 124.2M parameters
</code></pre>
<p>But how can I load a GPT2 architecture with a smaller number of decoder layers? Say, 3 or 5 instead of the original (I think it's 12)? Note that I'm training this from scratch so I'm not looking for an already pretrained model.</p>
","huggingface"
"75725818","Loading Hugging face model is taking too much memory","2023-03-13 18:46:57","75726174","6","12065","<python><pytorch><nlp><huggingface-transformers><huggingface>","<p>I am trying to load a large Hugging face model with code like below:</p>
<pre><code>model_from_disc = AutoModelForCausalLM.from_pretrained(path_to_model)
tokenizer_from_disc = AutoTokenizer.from_pretrained(path_to_model)
generator = pipeline(&quot;text-generation&quot;, model=model_from_disc, tokenizer=tokenizer_from_disc)
</code></pre>
<p>The program is quickly crashing <strong>after the first line</strong> because it is running out of memory. Is there a way to chunk the model as I am loading it, so that the program doesn't crash?</p>
<hr>
<p><strong>EDIT</strong>
<br>
See cronoik's answer for accepted solution, but here are the relevant pages on Hugging Face's documentation:</p>
<p><strong>Sharded Checkpoints:</strong> <a href=""https://huggingface.co/docs/transformers/big_models#sharded-checkpoints:%7E:text=in%20the%20future.-,Sharded%20checkpoints,-Since%20version%204.18.0"" rel=""noreferrer"">https://huggingface.co/docs/transformers/big_models#sharded-checkpoints:~:text=in%20the%20future.-,Sharded%20checkpoints,-Since%20version%204.18.0</a>
<br>
<strong>Large Model Loading:</strong> <a href=""https://huggingface.co/docs/transformers/main_classes/model#:%7E:text=the%20weights%20instead.-,Large%20model%20loading,-In%20Transformers%204.20.0"" rel=""noreferrer"">https://huggingface.co/docs/transformers/main_classes/model#:~:text=the%20weights%20instead.-,Large%20model%20loading,-In%20Transformers%204.20.0</a></p>
","huggingface"
"75723546","How to resolve ""the size of tensor a (1024) must match the size of tensor b"" in happytransformer","2023-03-13 14:56:59","","2","1060","<python><huggingface-transformers><huggingface><gpt-2>","<p>I have the following code. This code uses the GPT-2 language model from the Transformers library to generate text from a given input text. The input text is split into smaller chunks of 1024 tokens, and then the GPT-2 model is used to generate text for each chunk. The generated text is concatenated to produce the final output text. The <a href=""https://happytransformer.com"" rel=""nofollow noreferrer"">HappyTransformer</a> library is used to simplify the generation process by providing a pre-trained model and an interface to generate text with a given prefix and some settings. The GPT-2 model and tokenizer are also saved to a local directory. The output of the code is the generated text for the input text, with corrections for grammar suggested by the prefix &quot;grammar: &quot;.</p>
<pre><code>from transformers import GPT2Tokenizer, GPT2LMHeadModel
from happytransformer import HappyGeneration, GENSettings
import torch

model_name = &quot;gpt2&quot;
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name)

save_path = &quot;/home/ubuntu/storage1/various_transformer_models/gpt2&quot;
# save the tokenizer and model to a local directory
tokenizer.save_pretrained(save_path)
model.save_pretrained(save_path)

# Processing
happy_gen = HappyGeneration(&quot;GPT-2&quot;, &quot;gpt2&quot;)
args = GENSettings(num_beams=5, max_length=1024)

mytext = &quot;This sentence has bad grammar. This is a very long sentence that exceeds the maximum length of 512 tokens. Therefore, we need to split it into smaller chunks and process each chunk separately.&quot;
prefix = &quot;grammar: &quot;

# Split the text into chunks of maximum length 1024 tokens
max_length = 1024
chunks = [mytext[i:i+max_length] for i in range(0, len(mytext), max_length)]

# Process each chunk separately
results = []
for chunk in chunks:
    # Generate outputs for each chunk
    result = happy_gen.generate_text(prefix + chunk, args=args)
    results.append(result.text)

# Concatenate the results
output_text = &quot; &quot;.join(results)

print(output_text)

</code></pre>
<p>But it gives me this error:</p>
<pre><code>RuntimeError: The size of tensor a (1024) must match the size of tensor b (1025) at non-singleton dimension 3
</code></pre>
<p>How can I resolve it?</p>
","huggingface"
"75714587","node.js - turn Hugging Face image response to buffer and send as a discord attachment","2023-03-12 16:27:09","75716283","1","508","<javascript><node.js><image><discord.js><huggingface>","<p>I am trying to get a response from huggingface.com's stable diffusion, but the response I get is encrypted and I can't just turn it into a buffer and send it as a discord.js attachment.</p>
<p>Is there any way to fix this?</p>
<p>The response I get: <code>�,���$eF*dh�F8��4�b��A�,9���tr8ڠ��V#�e�����U�H��Jr�������b�</code>...</p>
<p>my code:</p>
<pre class=""lang-js prettyprint-override""><code>var data = interaction.options.getString(&quot;prompt&quot;)

    axios.post(

        &quot;https://api-inference.huggingface.co/models/runwayml/stable-diffusion-v1-5&quot;,

        {

            inputs: data,

        },

        { Authorization: &quot;Bearer ive placed my token here&quot;,

               Accept: 'application/json',

        'Content-Type': 'application/json',}

    ).then(r =&gt; {

      console.log(r.data) // =&gt; returns unknown encoded data

      var buffer = Buffer.from(r.data)

      var attach = new AttachmentBuilder(buffer, {name: 'result.png'})

      interaction.reply({files: [attach]}) // =&gt; does not work

    });
</code></pre>
<p>I expected that I would get a JSON response, turn the base64 encoded to buffer, and send it as a discord attachment. But what I got from the response was some encrypted data that I cant work with.</p>
","huggingface"
"75709193","How to calculate image similarity of given 2 images by using open AI Clip model - which method / AI model is best for calculating image similarity?","2023-03-11 20:03:21","75709479","2","3770","<python><huggingface-transformers><clip><huggingface>","<p>I have prepared a small example code but It is throwing error. Can't solve the problem because it is supposed to work.</p>
<p>Also do you think are there any better approaches to calculate image similarity? I want to find similar cloth images. e.g. I will give an image of a coat and I want to find similar coats.</p>
<p>also would this code handle all dimensions of images and all types of images?</p>
<p>here the code</p>
<pre><code>import torch
import torchvision.transforms as transforms
import urllib.request
from transformers import CLIPProcessor, CLIPModel, CLIPTokenizer
from PIL import Image

# Load the CLIP model
device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;
model_ID = &quot;openai/clip-vit-base-patch32&quot;
model = CLIPModel.from_pretrained(model_ID).to(device)

preprocess = CLIPProcessor.from_pretrained(model_ID)


# Define a function to load an image and preprocess it for CLIP
def load_and_preprocess_image(image_path):
    # Load the image from the specified path
    image = Image.open(image_path)

    # Apply the CLIP preprocessing to the image
    image = preprocess(image).unsqueeze(0).to(device)

    # Return the preprocessed image
    return image

# Load the two images and preprocess them for CLIP
image_a = load_and_preprocess_image('/content/a.png')
image_b = load_and_preprocess_image('/content/b.png')

# Calculate the embeddings for the images using the CLIP model
with torch.no_grad():
    embedding_a = model.encode_image(image_a)
    embedding_b = model.encode_image(image_b)

# Calculate the cosine similarity between the embeddings
similarity_score = torch.nn.functional.cosine_similarity(embedding_a, embedding_b)

# Print the similarity score
print('Similarity score:', similarity_score.item())
</code></pre>
<p>here the error message</p>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
[&lt;ipython-input-24-e95a926e1bc8&gt;](https://localhost:8080/#) in &lt;module&gt;
     25 
     26 # Load the two images and preprocess them for CLIP
---&gt; 27 image_a = load_and_preprocess_image('/content/a.png')
     28 image_b = load_and_preprocess_image('/content/b.png')
     29 

3 frames
[/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py](https://localhost:8080/#) in _call_one(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)
   2579 
   2580         if not _is_valid_text_input(text):
-&gt; 2581             raise ValueError(
   2582                 &quot;text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) &quot;
   2583                 &quot;or `List[List[str]]` (batch of pretokenized examples).&quot;

ValueError: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)
</code></pre>
","huggingface"
"75701297","Not enough memory for fine tuning LLM with Hugging Face","2023-03-10 21:50:32","","0","797","<machine-learning><pytorch><huggingface><gpt-2>","<p>I'm running into runtime errors where I don't have enough memory to fine tune a pretrained LLM.</p>
<p>I'm a novelist and I am curious to see what would happen if I fine tune a pretrained LLM to write more chapters of my novel in my style.</p>
<p>I successfully ran a tutorial on fine tuning a BERT model with Hugging Face with a Yelp dataset that is smaller than mine yesterday on my CPU (I have 16GB RAM and don't have an NVIDIA GPU,) so not sure where the error is arising from now.</p>
<p>Some things I've tried, but still giving me a runtime memory error:</p>
<ul>
<li>changed my model from Neo GPT to GPT2, which is much smaller</li>
<li>decreased my batch size hyperparameter</li>
<li>decreased the max length of tokens</li>
<li>decreased my dataset size</li>
</ul>
<p>This is my code:</p>
<pre><code>from transformers import GPTNeoForCausalLM, GPT2Tokenizer, Trainer, TrainingArguments
from datasets import Dataset, load_dataset

# Step 1: Import my novel
import docx
import pandas as pd

# Read each paragraph from a Word file
doc = docx.Document(r&quot;C:\Users\chris\Downloads\The Black Squirrel (1).docx&quot;)
paras = [p.text for p in doc.paragraphs if p.text]

# Convert list to dataframe
df = pd.DataFrame(paras)
df.reset_index(drop=False,inplace=True)
df.rename(columns={'index':'label',0:'text'},inplace=True)

# Split my novel into train and test
from sklearn.model_selection import train_test_split

train, test = train_test_split(df, test_size=0.05)

# Export novel as CSV to be read by Huggingface library
train.to_csv(r&quot;C:\Users\chris\OneDrive\Documents\ML\data\black_squirrel_dataset_train.csv&quot;, index=False)
test.to_csv(r&quot;C:\Users\chris\OneDrive\Documents\ML\data\black_squirrel_dataset_test.csv&quot;, index=False)

# Tokenize novel
datasets = load_dataset('csv',
                       data_files={'train':r&quot;C:\Users\chris\OneDrive\Documents\ML\data\black_squirrel_dataset_train.csv&quot;,
                       'test':r&quot;C:\Users\chris\OneDrive\Documents\ML\data\black_squirrel_dataset_test.csv&quot;})

# Instantiate tokenizer
tokenizer = GPT2Tokenizer.from_pretrained(&quot;EleutherAI/gpt-neo-1.3B&quot;,
                                          pad_token='[PAD]')

# Do I need the below?
# tokenizer.enable_padding(pad_id=tokenizer.token_to_id('[PAD]'))
paragraphs = df['text']
max_length = max([len(tokenizer.encode(paragraphs)) for paragraphs in paragraphs])

# Tokenize my novel
def tokenize_function(examples):
    return tokenizer(examples[&quot;text&quot;], padding='max_length', truncation=True)

tokenized_datasets = datasets.map(tokenize_function, batched=True)

# Step 2: Train the model
model = GPTNeoForCausalLM.from_pretrained(&quot;EleutherAI/gpt-neo-1.3B&quot;)

model.resize_token_embeddings(len(tokenizer))

training_args = TrainingArguments(
    output_dir=r&quot;C:\Users\chris\OneDrive\Documents\ML\models&quot;,
    overwrite_output_dir=True,
    num_train_epochs=3,
    per_device_train_batch_size=32, # batch size for training
    per_device_eval_batch_size=64,  # batch size for evaluation
    eval_steps = 400, # Number of update steps between two evaluations.
    save_steps=800, # after # steps model is saved
    warmup_steps=500,# number of warmup steps for learning rate scheduler
    )

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets['train'],
    eval_dataset=tokenized_datasets['test']
)

trainer.train()
</code></pre>
<p>Here is my error readout:</p>
<pre><code>***** Running training *****
  Num examples = 779
  Num Epochs = 3
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed &amp; accumulation) = 32
  Gradient Accumulation steps = 1
  Total optimization steps = 75
  Number of trainable parameters = 1315577856
  0%|          | 0/75 [19:12&lt;?, ?it/s]
Traceback (most recent call last):
  File &quot;C:\Users\chris\AppData\Local\Programs\Python\Python37\lib\code.py&quot;, line 90, in runcode
    exec(code, self.locals)
  File &quot;&lt;input&gt;&quot;, line 9, in &lt;module&gt;
  File &quot;C:\Users\chris\PycharmProjects\37venv\lib\site-packages\transformers\trainer.py&quot;, line 1547, in train
    ignore_keys_for_eval=ignore_keys_for_eval,
  File &quot;C:\Users\chris\PycharmProjects\37venv\lib\site-packages\transformers\trainer.py&quot;, line 1791, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File &quot;C:\Users\chris\PycharmProjects\37venv\lib\site-packages\transformers\trainer.py&quot;, line 2539, in training_step
    loss = self.compute_loss(model, inputs)
  File &quot;C:\Users\chris\PycharmProjects\37venv\lib\site-packages\transformers\trainer.py&quot;, line 2571, in compute_loss
    outputs = model(**inputs)
  File &quot;C:\Users\chris\PycharmProjects\37venv\lib\site-packages\torch\nn\modules\module.py&quot;, line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;C:\Users\chris\PycharmProjects\37venv\lib\site-packages\transformers\models\gpt_neo\modeling_gpt_neo.py&quot;, line 752, in forward
    return_dict=return_dict,
  File &quot;C:\Users\chris\PycharmProjects\37venv\lib\site-packages\torch\nn\modules\module.py&quot;, line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;C:\Users\chris\PycharmProjects\37venv\lib\site-packages\transformers\models\gpt_neo\modeling_gpt_neo.py&quot;, line 627, in forward
    output_attentions=output_attentions,
  File &quot;C:\Users\chris\PycharmProjects\37venv\lib\site-packages\torch\nn\modules\module.py&quot;, line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;C:\Users\chris\PycharmProjects\37venv\lib\site-packages\transformers\models\gpt_neo\modeling_gpt_neo.py&quot;, line 342, in forward
    feed_forward_hidden_states = self.mlp(hidden_states)
  File &quot;C:\Users\chris\PycharmProjects\37venv\lib\site-packages\torch\nn\modules\module.py&quot;, line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;C:\Users\chris\PycharmProjects\37venv\lib\site-packages\transformers\models\gpt_neo\modeling_gpt_neo.py&quot;, line 300, in forward
    hidden_states = self.act(hidden_states)
  File &quot;C:\Users\chris\PycharmProjects\37venv\lib\site-packages\torch\nn\modules\module.py&quot;, line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;C:\Users\chris\PycharmProjects\37venv\lib\site-packages\transformers\activations.py&quot;, line 35, in forward
    return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))
RuntimeError: [enforce fail at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\c10\core\impl\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 405798912 bytes.
</code></pre>
<p>My novel is <a href=""https://docs.google.com/document/d/1PI81BJy19_t4YdNNVC08XraxC3TRyQVYffrYn8PlZ0E/edit"" rel=""nofollow noreferrer"">here</a>. You can save as docx as is and run the code. Or, you can just save the first chapter. I also tried splitting up the first chapter into one paragraph per sentence to make the tokens even smaller, though that didn't help.</p>
<p>Does this indicate that I really need an NVIDIA GPU to run machine learning tasks? Or is this likely an issue with my dataset setup or code?</p>
<p>Thanks.</p>
","huggingface"
"75685949","Delete Huggingface Cache - NLLB etc","2023-03-09 14:29:54","","0","783","<huggingface-transformers><huggingface>","<p>Every time I clear my cache, it keeps building up NLLB cache which I don't use, I have deleted everything related to NLLB and can't find the .cache folder for huggingface, but this cache keeps building up when I clear them.</p>
<p>How do I get rid of it permanently to stop it from building?</p>
<p><img src=""https://i.sstatic.net/M7zpK.png"" alt=""enter image description here"" /></p>
<p>No more cache once deleted</p>
","huggingface"
"75669587","Fine-tuning NLLB model","2023-03-08 05:02:14","","0","920","<python><translation><huggingface><fine-tuning>","<p>I am trying to fine tune NLLB model for Hindi-Dogri(Indian languages) translation. Referred the translation pipeline given in the page Translation - Hugging Face Course 2 . My dataset looks like as given below:</p>
<pre><code>DatasetDict({
train: Dataset({
features: [‘id’, ‘translation’],
num_rows: 6356
})
test: Dataset({
features: [‘id’, ‘translation’],
num_rows: 1589
})
})
</code></pre>
<p>Individual item looks like:</p>
<pre><code>‘translation’: {‘do’: ‘जिनेंगी अस मनुक्खें दे ज्ञान थमां सखाई दियें गल्लें च नेई लेकन पवित्र आत्मा थमां सखाई दियें गल्लें च पवित्र आत्मा आत्मिक ज्ञान कन्ने आत्मिक गल्लें गी खोलीखोली दसदा ऐ’,
‘hi’: ‘जिनको हम मनुष्यों के ज्ञान की सिखाई हुई बातों में नहीं परन्तु पवित्र आत्मा की सिखाई हुई बातों में आत्मा आत्मिक ज्ञान से आत्मिक बातों की व्याख्या करती है’}}
</code></pre>
<p>My script is:</p>
<pre><code>tokenizer = NllbTokenizerFast.from_pretrained(
    &quot;facebook/nllb-200-distilled-600M&quot;, src_lang=&quot;hin_Deva&quot;, tgt_lang=&quot;dog_Deva&quot;
)
model = AutoModelForSeq2SeqLM.from_pretrained(&quot;facebook/nllb-200-distilled-600M&quot;)
data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)
training_args = Seq2SeqTrainingArguments(
    output_dir=&quot;hi_do_model&quot;,
    evaluation_strategy=&quot;epoch&quot;,
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=1,
    predict_with_generate=True,
    fp16=True,
    push_to_hub=True,
)
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_data[&quot;train&quot;],
    eval_dataset=tokenized_data[&quot;test&quot;],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)
trainer.train()
</code></pre>
<p>When calling trainer.train() getting :</p>
<p>The following columns in the training set don't have a corresponding argument in M2M100ForConditionalGeneration.forward and have been ignored: translation, id. If translation, id are not expected by M2M100ForConditionalGeneration.forward, you can safely ignore this message.</p>
<p>What this message means? Is the data format is wrong? What data format required for nllb?</p>
","huggingface"
"75653539","How to ensure last token in sequence is end-of-sequence token?","2023-03-06 16:44:27","","3","1637","<huggingface-tokenizers><huggingface>","<p>I am using the <code>gpt2</code> model from huggingface's <code>transformers</code> library. When tokenizing, I would like all sequences to end in the end-of-sequence (EOS) token.  How can I do this?</p>
<p>An easy solution is to manually append the EOS token to each sequence in a batch prior to tokenization:</p>
<pre><code>from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained('gpt2')

text = ['Hello world.', 'I am program.']

text = [el + tokenizer.eos_token for el in text]

tokenized_text = tokenizer(text)

</code></pre>
<p>It seems like my solution is an inelegant solution for this task that is so commonplace that I expect there is some built-in way of doing this.  I haven't found anything about this in the documentation—is there a way?</p>
<p>Edit: I want to do this in order to train a GPT-2 to generate specific kinds of responses to input sequences.  Without the end-of-sequence token during training, performance was poor, and the model generated much too much text.</p>
","huggingface"
"75649344","TypeError: max() received an invalid combination of arguments when trying to use beam search decoding","2023-03-06 09:54:35","75652858","0","152","<deep-learning><pytorch><huggingface-tokenizers><huggingface><ctc>","<p>I'm trying to run simple example of decode WAV2VEC2 output with beam search (without LM):</p>
<pre><code>from pyctcdecode       import build_ctcdecoder
from transformers      import Wav2Vec2ForCTC, Wav2Vec2Processor
from torchaudio.utils  import download_asset

import torch
import librosa

processor        = Wav2Vec2Processor.from_pretrained(&quot;facebook/wav2vec2-base-960h&quot;)
model            = Wav2Vec2ForCTC.from_pretrained(&quot;facebook/wav2vec2-base-960h&quot;)

FILE_NAME        = &quot;tutorial-assets/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav&quot;
SPEECH_FILE      = download_asset(FILE_NAME)

speech, sr       = librosa.load(SPEECH_FILE, sr=16000)
input_values     = processor(speech, sampling_rate=16000, return_tensors=&quot;pt&quot;).input_values

logits           = model(input_values).logits
vocabulary       = list(processor.tokenizer.get_vocab().keys())
log_probs        = torch.nn.functional.log_softmax(logits[0])

decoder          = build_ctcdecoder(vocabulary)
text             = decoder.decode(log_probs)
</code></pre>
<p>I'm getting the error:</p>
<pre><code>TypeError: max() received an invalid combination of arguments - got (keepdims=bool, out=NoneType, axis=int, ), but expected one of:
 * ()
 * (Tensor other)
 * (int dim, bool keepdim)
 * (name dim, bool keepdim)
</code></pre>
<p>As you can see I'm using <code>pyctcdecode</code>.</p>
<p>How can I decode the output of the wav2vec2 model with a beam search algorithm?</p>
","huggingface"
"75641342","Can't use wav2vec2-large-xlsr model (Can't load tokenizer )","2023-03-05 09:47:27","75701140","1","1803","<deep-learning><huggingface-transformers><huggingface-tokenizers><huggingface>","<p>I'm trying to use wav2vec2 (XLSR model) without any success:</p>
<pre><code>import transformers
from transformers      import Wav2Vec2ForCTC, Wav2Vec2Processor
import librosa
import torch


wav2vec2_processor = Wav2Vec2Processor.from_pretrained(&quot;facebook/wav2vec2-large-xlsr-53&quot;)
wav2vec2_model = Wav2Vec2ForCTC.from_pretrained(&quot;facebook/wav2vec2-large-xlsr-53&quot;)


file_name     = &quot;test.wav&quot;
speech, sr    = librosa.load(file_name, sr=16000)
input_values  = wav2vec2_processor(speech, sampling_rate=16000, return_tensors=&quot;pt&quot;).input_values

logits        = wav2vec2_model(input_values).logits
</code></pre>
<p>Error:</p>
<pre><code>OSError: Can't load tokenizer for 'facebook/wav2vec2-large-xlsr-53'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'facebook/wav2vec2-large-xlsr-53' is the correct path to a directory containing all relevant files for a Wav2Vec2CTCTokenizer tokenizer.
</code></pre>
<p>How can I use wav2vec2 (XLSR model) ?</p>
","huggingface"
"75641139","Can't run HF model on GPU","2023-03-05 09:05:17","","1","213","<deep-learning><huggingface-transformers><huggingface>","<p>I'm trying to run wav2vec2 model on GPU and getting error.</p>
<pre><code>DEVICE             = 'cuda'
wav2vec2_processor = Wav2Vec2Processor.from_pretrained(&quot;facebook/wav2vec2-base-960h&quot;)
wav2vec2_model     = Wav2Vec2ForCTC.from_pretrained(&quot;facebook/wav2vec2-base-960h&quot;)    
wav2vec2_model.to(DEVICE)

file_name        = &quot;myFile.wav&quot;
speech, sr       = librosa.load(file_name, sr=16000)
input_values     = wav2vec2_processor(speech, sampling_rate=16000, return_tensors=&quot;pt&quot;).input_values
input_values.to(DEVICE)

logits           = wav2vec2_model(input_values).logits

predicted_ids    = torch.argmax(logits, dim=-1)
transcription    = wav2vec2_processor.decode(predicted_ids[0])
</code></pre>
<p>And getting error:</p>
<pre><code>RuntimeError: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor
</code></pre>
<p>What am I missing ?</p>
","huggingface"
"75637555","After fine tuning model with model.train it gives different predictions for same text","2023-03-04 17:55:45","","1","153","<pytorch><nlp><huggingface-transformers><huggingface>","<p>This is how I fine tuned the model:</p>
<pre class=""lang-py prettyprint-override""><code>    input_ids=tokenizer(str(parseddata), padding=True, truncation=True, max_length=500, 
    return_tensors=&quot;pt&quot;)
    labels = torch.tensor([0])

    lr_scheduler = get_scheduler(
        name=&quot;linear&quot;, optimizer=optimizer, num_warmup_steps=0, num_training_steps=2
    )

    # del banmodel

    model.train(mode=True)
    for i in range(2):
        outputs = model(input_ids, labels=labels)
        loss = outputs[0]
        loss.backward()
        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
</code></pre>
<p>This is how I predicted it:</p>
<pre class=""lang-py prettyprint-override""><code>def predict_label(text):
    # input_ids = torch.tensor(tokenizer.encode(text)).unsqueeze(0)
    input_ids=tokenizer(text, padding=True, truncation=True, max_length=500, return_tensors=&quot;pt&quot;)
    logits = model(**input_ids)[0]
    probs = torch.nn.functional.softmax(logits, dim=1)
    
    return probs
</code></pre>
<p>Only after training it model gives different answers for the same text input. However, when I close the entire process and turn it on again it works and gives me the same prediction. Any help would be extremely appreciated thanks.</p>
","huggingface"
"75591688","How to mask **some** of the previous tokens in causal language model training?","2023-02-28 11:50:20","","0","969","<machine-learning><pytorch><tokenize><huggingface-transformers><huggingface>","<p>I'm following <a href=""https://huggingface.co/course/chapter7/6"" rel=""nofollow noreferrer"">this tutorial</a> on training a causal language model from scratch. My dataset is a corpus of text:</p>
<pre><code>my_dataset = [&quot;some_text... _112_ some_text... _113_ some_text... _114_ some_text...&quot;, &quot;some_text... _1423_ some_text... _1424_ some_text... _1425_ some_text...&quot;, &quot;some_text... _1111_ some_text... _1111_ some_text... _1111_ some_text...&quot;]. 
</code></pre>
<p>The issue is that my dataset contains a clear pattern of numbers in each text (either the numbers are consecutive or they repeat).</p>
<p><strong>I would like to mask out the previous predicted numbers in this pattern as the model predicts the next tokens</strong> (note that they always has the pattern of <code>_X_</code>, where <code>X</code> is a number, so I don't want to just mask out any previous number, but just those that correspond to the pattern).</p>
<p>For example, given the first text, after the model predicts <code>_112_</code>, I'd like to mask the number <code>112</code> in that sequence for the subsequent token predictions (e.g., <code>&quot;some_text... _MaskToken_ some_text...&quot;</code>).</p>
<p>I found <a href=""https://stackoverflow.com/questions/60969176/training-huggingfaces-gpt2-from-scratch-how-to-implement-causal-mask"">this SO</a> that I believe asked a similar question a couple of years ago, but left unanswered and used an inefficient method therefore. From the tutorial I'm using it seems like the <a href=""https://huggingface.co/docs/transformers/main_classes/data_collator"" rel=""nofollow noreferrer"">DataCollatorForLanguageModeling</a> collator might be the way to go about this:</p>
<blockquote>
<p>&quot;Besides stacking and padding batches, it also takes care of creating
the language model labels — in causal language modeling the inputs
serve as labels too (just shifted by one element), and this data
collator creates them on the fly during training so we don’t need to
duplicate the input_ids.&quot;</p>
</blockquote>
<p>From <a href=""https://www.reddit.com/r/LanguageTechnology/comments/uyrobu/does_the_datacollatorforlanguagemodelling_class/"" rel=""nofollow noreferrer"">this reddit post</a> I understand that the <code>DataCollatorForLanguageModelling</code></p>
<blockquote>
<p>&quot;Duplicate the training sentence. If the masking is performed every
time a sequence is fed to the model, the model sees different versions
of the same sentence with masks on different positions.&quot;</p>
</blockquote>
<p>The tutorial also mention</p>
<blockquote>
<p>Shifting the inputs and labels to align them happens inside the model,
so the data collator just copies the inputs to create the labels.</p>
</blockquote>
<p>But going over the source code of <a href=""https://github.com/huggingface/transformers/blob/v4.26.1/src/transformers/models/gpt2/modeling_gpt2.py#L943"" rel=""nofollow noreferrer"">GPT2LMHeadModel</a> or the data collator it is not clear to me how to do this either.</p>
","huggingface"
"75587208","Key Error when importing Hugging Face model into AWS lambda function","2023-02-28 01:55:17","75594515","0","367","<amazon-web-services><aws-lambda><huggingface-transformers><amazon-efs><huggingface>","<p>I'm trying to launch a lambda function that uses a Hugging Face model (BioGPT) using the transformers paradigm on an AWS lambda function. The infrastructure looks like this:</p>
<p><a href=""https://i.sstatic.net/Jnxhm.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Jnxhm.png"" alt=""enter image description here"" /></a></p>
<p>It more or less follows the setup outlined in this <a href=""https://aws.amazon.com/blogs/compute/hosting-hugging-face-models-on-aws-lambda/"" rel=""nofollow noreferrer"">post</a>, except that I am trying to use the BioGPT model instead of the models outlined in the link above.</p>
<p>Here is my <strong>app.py</strong>:</p>
<pre><code>&quot;&quot;&quot;
Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
SPDX-License-Identifier: MIT-0
&quot;&quot;&quot;

import os
from pathlib import Path
from aws_cdk import (
    aws_lambda as lambda_,
    aws_efs as efs,
    aws_ec2 as ec2
)
from aws_cdk import App, Stack, Duration, RemovalPolicy, Tags

from constructs import Construct

class ServerlessHuggingFaceStack(Stack):
    def __init__(self, scope: Construct, id: str, **kwargs) -&gt; None:
        super().__init__(scope, id, **kwargs)

        # EFS needs to be setup in a VPC
        vpc = ec2.Vpc(self, 'Vpc', max_azs=2)

        # creates a file system in EFS to store cache models
        fs = efs.FileSystem(self, 'FileSystem',
                            vpc=vpc,
                            removal_policy=RemovalPolicy.DESTROY)
        access_point = fs.add_access_point(
            'MLAccessPoint',
            create_acl=efs.Acl(
                owner_gid='1001',
                owner_uid='1001',
                permissions='750'
            ),
            path=&quot;/export/models&quot;,
            posix_user=efs.PosixUser(gid=&quot;1001&quot;, uid=&quot;1001&quot;)
        )

        # %%
        # iterates through the Python files in the docker directory
        docker_folder = os.path.dirname(os.path.realpath(__file__)) + &quot;/inference&quot;
        pathlist = Path(docker_folder).rglob('*.py')
        for path in pathlist:
            base = os.path.basename(path)
            filename = os.path.splitext(base)[0]
            # Lambda Function from docker image
            lambda_.DockerImageFunction(
                self, filename,
                code=lambda_.DockerImageCode.from_image_asset(docker_folder,
                                                              cmd=[
                                                                  filename+&quot;.handler&quot;]
                                                              ),
                memory_size=8096,
                timeout=Duration.seconds(600),
                vpc=vpc,
                filesystem=lambda_.FileSystem.from_efs_access_point(access_point, '/mnt/hf_models_cache'),
                environment={&quot;TRANSFORMERS_CACHE&quot;: &quot;/mnt/hf_models_cache&quot;},
            )

app = App()

stack = ServerlessHuggingFaceStack(app, &quot;BioGptStack&quot;)
Tags.of(stack).add(&quot;project&quot;, &quot;biogpt&quot;)

app.synth()
</code></pre>
<p>And here is my <strong>Dockerfile</strong>:</p>
<pre><code>ARG FUNCTION_DIR=&quot;/function/&quot;

FROM huggingface/transformers-pytorch-cpu as build-image


# Include global arg in this stage of the build
ARG FUNCTION_DIR

# Install aws-lambda-cpp build dependencies
RUN apt-get update &amp;&amp; \
  apt-get install -y \
  g++ \
  make \
  cmake \
  unzip \
  libcurl4-openssl-dev


# Create function directory
RUN mkdir -p ${FUNCTION_DIR}

# Copy handler function
COPY *.py ${FUNCTION_DIR}

# Install the function's dependencies
RUN pip uninstall --yes jupyter
RUN pip install --target ${FUNCTION_DIR} awslambdaric
RUN pip install --target ${FUNCTION_DIR} sentencepiece protobuf

FROM huggingface/transformers-pytorch-cpu

# Include global arg in this stage of the build
ARG FUNCTION_DIR
# Set working directory to function root directory
WORKDIR ${FUNCTION_DIR}

# Copy in the built dependencies
COPY --from=build-image ${FUNCTION_DIR} ${FUNCTION_DIR}

ENTRYPOINT [ &quot;python3&quot;, &quot;-m&quot;, &quot;awslambdaric&quot; ]

# This will get replaced by the proper handler by the CDK script
CMD [ &quot;sentiment.handler&quot; ]
</code></pre>
<p>Here is the <strong>error message</strong> I am seeing when I try to test my lambda function:</p>
<pre><code>line 672, in from_pretrained
config_class = CONFIG_MAPPING[config_dict[&quot;model_type&quot;]]
File &quot;/usr/local/lib/python3.6/dist-packages/transformers/models/auto/configuration_auto.py&quot;, line 387, in __getitem__
raise KeyError(key)
KeyError: 'biogpt'
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
File &quot;/usr/lib/python3.6/runpy.py&quot;, line 193, in _run_module_as_main
&quot;__main__&quot;, mod_spec)
File &quot;/usr/lib/python3.6/runpy.py&quot;, line 85, in _run_code
</code></pre>
","huggingface"
"75536174","How does one create a custom hugging face model that is compatible with the HF trainer?","2023-02-22 17:24:35","","2","2229","<deep-learning><pytorch><huggingface-transformers><huggingface-tokenizers><huggingface>","<p>I want to create a new hugging face (HF) architecture with some existing tokenizer (any one that is excellent is fine). Let's say decoder to make it concrete (but both is better).</p>
<p>How does one do this? I found this <a href=""https://huggingface.co/docs/transformers/create_a_model"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/create_a_model</a> but the tutorial honestly felt/seemed incomplete. (fyi also saw this causal: <a href=""https://www.youtube.com/watch?v=fWrPpQL9xRQ"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=fWrPpQL9xRQ</a>). Is there one with a full end-to-end code example running?</p>
<p>e.g. of what i have right now:</p>
<pre><code>import torch
import torch.nn as nn
from transformers import PreTrainedTokenizer, PreTrainedModel, PretrainedConfig


class MyTokenizer(PreTrainedTokenizer):
    def __init__(self, vocab_file, **kwargs):
        super().__init__(vocab_file, **kwargs)

    def __call__(self, text):
        tokens = text.split()
        token_ids = self.convert_tokens_to_ids(tokens)
        return token_ids


class MyModel(PreTrainedModel):
    def __init__(self, config):
        super().__init__(config)

        self.embedding = nn.Embedding(config.vocab_size, config.hidden_size)
        self.linear = nn.Linear(config.hidden_size, config.num_labels)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)

    def forward(self, input_ids, **kwargs):
        embeddings = self.embedding(input_ids)
        pooled = torch.mean(embeddings, dim=1)
        pooled = self.dropout(pooled)
        logits = self.linear(pooled)
        return logits


config = PretrainedConfig(vocab_size=1000, hidden_size=128, num_labels=2, hidden_dropout_prob=0.5)
tokenizer = MyTokenizer(&quot;path/to/vocab/file&quot;)
model = MyModel(config)

input_ids = tokenizer(&quot;This is a test&quot;)
logits = model(torch.tensor([input_ids]))
</code></pre>
<p>But I feel in a more principled way I think a solution should satisfy the following:</p>
<ol>
<li>Satisfy the standard HF model API so that the HF trainer, the GPU usage for the data &amp; model and compatible with pytorch data loaders.</li>
<li>The tokenizer is also seamless. At what point do we tokenize? Is inside the model, inside the data loader?</li>
<li>What would be the test it works as HF model works? My guess is 1. works with a custom pytorch training loop 2. it works witha HF trainer</li>
</ol>
<p>One other thing that would be worth trying is opening up a model e.g. T5 and seeing how it's implemented and copying the style?</p>
<hr />
<p>refs:</p>
<ul>
<li>original HF post: <a href=""https://discuss.huggingface.co/t/how-does-one-create-a-custom-hugging-face-model-with-a-already-working-tokenizer/32208"" rel=""nofollow noreferrer"">https://discuss.huggingface.co/t/how-does-one-create-a-custom-hugging-face-model-with-a-already-working-tokenizer/32208</a></li>
</ul>
","huggingface"
"75534394","Converting text to images using the huggingface API","2023-02-22 14:49:15","","0","410","<python><python-3.x><huggingface>","<p>I have some code like:</p>
<pre><code>from huggingface_hub.inference_api import InferenceApi

inference = InferenceApi(repo_id=&quot;hakurei/waifu-diffusion&quot;)
output = (inference(&quot;masterpiece, best quality, upper body, 1girl, looking at viewer, red hair, medium hair, purple eyes, demon horns, black coat, indoors, dimly lit&quot;))
print(output)
output.show()
</code></pre>
<p>I confuse why return image are always same. How to fix it?</p>
","huggingface"
"75530862","AutoModelForQuestionAnswering: ValueError: too many values to unpack (expected 2)","2023-02-22 09:46:55","75689049","1","497","<python><nlp><huggingface-transformers><huggingface><nlp-question-answering>","<p>When I initialized AutoModelForQuestionAnswering from pretrained indobenchmark/indobert-base-p1 it won’t produce any outputs and shows ValueError, below is the code:</p>
<pre><code>from transformers import AutoModelForQuestionAnswering, AutoTokenizer

model = AutoModelForQuestionAnswering.from_pretrained('indobenchmark/indobert-base-p1')
tokenizer = AutoTokenizer.from_pretrained('indobenchmark/indobert-base-p1')

context = 'Sindrom Bazex: acrokeratosis paraneoplastica.'
question = 'Nama sinonim dari Acrokeratosis paraneoplastica.'

inputs = tokenizer(question, context, return_tensors='pt')
outputs = model(**inputs)

inputs, outputs
</code></pre>
<p>The error shown is:</p>
<pre><code>ValueError                                Traceback (most recent call last) /tmp/ipykernel_28/2363476019.py in &lt;module&gt;
      3 
      4 inputs = tokenizer(question, context, return_tensors='pt')
----&gt; 5 outputs = model(**inputs)
      6 
      7 inputs, outputs

/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py in
_call_impl(self, *input, **kwargs)    1128         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks    1129              or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1130             return forward_call(*input, **kwargs)    1131         # Do not call functions when jit is used    1132         full_backward_hooks, non_full_backward_hooks = [], []

/opt/conda/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py in forward(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict)   1860     1861         logits = self.qa_outputs(sequence_output)
-&gt; 1862         start_logits, end_logits = logits.split(1, dim=-1)    1863         start_logits = start_logits.squeeze(-1).contiguous()    1864         end_logits = end_logits.squeeze(-1).contiguous()

ValueError: too many values to unpack (expected 2)
</code></pre>
<p>But when I initialize the model with from a different repo i.e indolem/indobert-base-uncased, it runs no problem. I only changed the from_pretrained value.</p>
<p>Has this ever happened to anyone? Is it a problem with the model I’m trying to load?</p>
","huggingface"
"75526750","nlp translation preserving emojis","2023-02-21 23:03:52","","1","140","<python><translation><emoji><huggingface>","<p>Is there a way to prevent NLP translations from dropping emojis (or specific out of vocabulary tokens for that matter) when using the <a href=""https://huggingface.co/Helsinki-NLP/opus-mt-ROMANCE-en"" rel=""nofollow noreferrer"">huggingface--Helsinki-NLP--opus-mt-ROMANCE-en</a> models? Desired behavior:</p>
<p>&quot;<code>Bonjour ma France 🇫🇷</code>&quot;@fr --&gt; &quot;<code>Hello my France 🇫🇷</code>&quot;@en</p>
<p>I can see the pre-trained default tokenizer knows of the emojis in its vocabulary, but looses it before decoding it. Sample code:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import *

# setup
engine = 'pt'
resource = 'huggingface--Helsinki-NLP--opus-mt-ROMANCE-en'
nlp = pipeline(
   task=&quot;translation&quot;,
   model=MarianMTModel.from_pretrained(resource),
   tokenizer=AutoTokenizer.from_pretrained(resource),
   framework=engine
)

# infer
translated = nlp.tokenizer.batch_decode(
   skip_special_tokens=True,
   sequences=nlp.model.generate(
      **nlp.tokenizer(
         text=[&quot;Bonjour ma France 🇫🇷&quot;],
         return_tensors=engine
      )
   )
)

# results
print(translated) # ['Hello, my France.']
print(&quot;🇫🇷&quot; in nlp.tokenizer.get_vocab()) # True
</code></pre>
","huggingface"
"75511033","Why are the english letters of wav2vec2 tokenizer aren't order as abc characters order?","2023-02-20 15:13:54","75511064","0","62","<huggingface-tokenizers><huggingface>","<p>I looked on the Tokenizer of <code>facebook/wav2vec2-base-960h</code></p>
<p>from:
<a href=""https://huggingface.co/facebook/wav2vec2-base-960h/blob/main/vocab.json"" rel=""nofollow noreferrer"">https://huggingface.co/facebook/wav2vec2-base-960h/blob/main/vocab.json</a></p>
<p>and I see that the letters are not order by the <code>abc</code> order, for example:</p>
<pre><code>&quot;E&quot;: 5, 
&quot;T&quot;: 6,
&quot;A&quot;: 7,
&quot;O&quot;: 8, 
</code></pre>
<p>Why they didn't order it as:</p>
<pre><code>&quot;A&quot;: 5, 
&quot;B&quot;: 6,
&quot;C&quot;: 7,
&quot;D&quot;: 8, 
...
</code></pre>
","huggingface"
"75481137","Is there is a way that I can download only a part of the dataset from huggingface?","2023-02-17 07:08:32","77434812","6","1303","<dataset><huggingface><huggingface-datasets>","<p>I'm trying to load (peoples speech) dataset, but it's way too big, is there's a way to download only a part of it?</p>
<pre><code>from datasets import load_dataset

from datasets import load_dataset

train = load_dataset(&quot;MLCommons/peoples_speech&quot;, &quot;clean&quot;,split=&quot;train[:10%]&quot;)
test = load_dataset(&quot;MLCommons/peoples_speech&quot;, &quot;clean&quot;,split=&quot;test[:10%]&quot;)
</code></pre>
<p>Using (&quot;train [: 10%]&quot;) didn't help, it still trying to download the entire dataset...</p>
","huggingface"
"75458603","How to Output Downloadable file after processing?","2023-02-15 10:44:23","75474939","0","4700","<python><excel><pandas><huggingface><gradio>","<h2>Specification</h2>
<ul>
<li><code>gr.__version__  --&gt; '3.16.2'</code></li>
<li>I want to create a gradio tab in mygradio app</li>
<li>Disregard TAB 1, I am only working on tab2</li>
<li>where I upload an excel file</li>
<li>save name of the excel fie to a variable</li>
<li>process that excel file take data out of it 2 numbers (1 and 2)</li>
<li>Load data from the excel file to a pandas dataframe and add 1 to both of the numbers</li>
<li>Turn dataframe to excel again and output it to the user to be able to download the output excel file</li>
<li>The output file is named as the original uploaded file</li>
</ul>
<h2>MY CURRENT Code</h2>
<pre><code>import gradio as gr
import pandas as pd


# def func1():
#     #....
#     pass

def func2(name, file):
    file_name = name
    file_x = file
    # use this function to retrieve the file_x without modification for gradio.io output
    # excel to dataframe
    df = pd.read_excel(file_x)
    # add 1 to both numbers
    df['1'] = df['1'] + 1
    df['2'] = df['2'] + 1
    # dataframe to excel
    # returnt the exported excel fiel with the same name as the original file
    return df.to_excel(file_x, index=False)


# GRADIO APP
with gr.Blocks() as demo:
    gr.Markdown(&quot;BI App&quot;)


    ''' #1.TAB '''
    # with gr.Tab(&quot;Tab1&quot;):
    #      #.... unimportant code
    #     with gr.Column():
    #         file_obj = gr.File(label=&quot;Input File&quot;, 
    #             file_count=&quot;single&quot;, 
    #             file_types=[&quot;&quot;, &quot;.&quot;, &quot;.csv&quot;,&quot;.xls&quot;,&quot;.xlsx&quot;]),
    #         # extract the filename from gradio.io file object
    #         # keyfile_name = gr.Interface(file_name_reader, inputs=&quot;file&quot;, outputs=None)
    #         keyfile_name = 'nothing'
    #         tab1_inputs = [keyfile_name, file_obj]

    #     with gr.Column():
    #         # output excel file with gradio.io
    #         tab1_outputs = [gr.File(label=&quot;Output File&quot;, 
    #             file_count=&quot;single&quot;, 
    #             file_types=[&quot;&quot;, &quot;.&quot;, &quot;.csv&quot;,&quot;.xls&quot;,&quot;.xlsx&quot;])]
        
    #     tab1_submit_button = gr.Button(&quot;Submit&quot;)


    ''' #2.TAB - I EDIT THIS TAB'''
    with gr.Tab(&quot;Tab2&quot;):
        admitad_invoice_approvals_button = gr.Button(&quot;Submit&quot;)

        def file_name_reader(file):
            file_name = file.name  # extract the file name from the uploaded file
            return file_name
        
        # iface = gr.Interface(file_name_reader, inputs=&quot;file&quot;, outputs=None)

        with gr.Column():
            file_obj = gr.File(label=&quot;Input File&quot;, 
                file_count=&quot;single&quot;, 
                file_types=[&quot;&quot;, &quot;.&quot;, &quot;.csv&quot;,&quot;.xls&quot;,&quot;.xlsx&quot;]),
            # extract the filename from gradio.io file object
            keyfile_name = gr.Interface(file_name_reader, inputs=&quot;file&quot;, outputs=None)
            tab2_inputs = [keyfile_name, file_obj]

        with gr.Column():
            # output excel file with gradio.io
            tab2_outputs = [gr.File(label=&quot;Output File&quot;, 
                file_count=&quot;single&quot;, 
                file_types=[&quot;&quot;, &quot;.&quot;, &quot;.csv&quot;,&quot;.xls&quot;,&quot;.xlsx&quot;])]

        tab2_submit_button = gr.Button(&quot;Submit&quot;)


    '''1 button for each of the tabs to execute the GUI TASK'''
    # tab1_submit_button.click(func1,
    #                         inputs=tab1_inputs,
    #                         outputs=tab1_outputs)
    
    tab2_submit_button.click(func2,
                            inputs=tab2_inputs,
                            outputs=tab2_outputs)
    

''' EXECUTING THE APP'''
demo.launch(debug=True, share=True) ## PRODUCTION TESTING
</code></pre>
<h2>ERROR:</h2>
<pre><code>Output exceeds the size limit. Open the full output data in a text editor
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
Cell In[7], line 95
     90     '''1 button for each of the tabs to execute the GUI TASK'''
     91     # tab1_submit_button.click(func1,
     92     #                         inputs=tab1_inputs,
     93     #                         outputs=tab1_outputs)
---&gt; 95     tab2_submit_button.click(func2,
     96                             inputs=tab2_inputs,
     97                             outputs=tab2_outputs)
    100 ''' EXECUTING THE APP'''
    101 demo.launch(debug=True, share=True) ## PRODUCTION TESTING

File ~/.local/lib/python3.8/site-packages/gradio/events.py:145, in Clickable.click(self, fn, inputs, outputs, api_name, status_tracker, scroll_to_output, show_progress, queue, batch, max_batch_size, preprocess, postprocess, cancels, every, _js)
    140 if status_tracker:
    141     warnings.warn(
    142         &quot;The 'status_tracker' parameter has been deprecated and has no effect.&quot;
    143     )
--&gt; 145 dep = self.set_event_trigger(
    146     &quot;click&quot;,
    147     fn,
    148     inputs,
    149     outputs,
    150     preprocess=preprocess,
    151     postprocess=postprocess,
    152     scroll_to_output=scroll_to_output,
    153     show_progress=show_progress,
    154     api_name=api_name,
    155     js=_js,
    156     queue=queue,
    157     batch=batch,
    158     max_batch_size=max_batch_size,
    159     every=every,
    160 )
    161 set_cancel_events(self, &quot;click&quot;, cancels)
    162 return dep

File ~/.local/lib/python3.8/site-packages/gradio/blocks.py:225, in Block.set_event_trigger(self, event_name, fn, inputs, outputs, preprocess, postprocess, scroll_to_output, show_progress, api_name, js, no_target, queue, batch, max_batch_size, cancels, every)
    217         warnings.warn(
    218             &quot;api_name {} already exists, using {}&quot;.format(api_name, api_name_)
    219         )
    220         api_name = api_name_
    222 dependency = {
    223     &quot;targets&quot;: [self._id] if not no_target else [],
    224     &quot;trigger&quot;: event_name,
...
    237 }
    238 Context.root_block.dependencies.append(dependency)
    239 return dependency

AttributeError: 'tuple' object has no attribute '_id'
</code></pre>
<h2>Tried</h2>
<ul>
<li>I have looked in to <a href=""https://gradio.app/docs/#file"" rel=""nofollow noreferrer"">https://gradio.app/docs/#file</a> but the output file generation is not clean especially regarding applying it to my case</li>
</ul>
","huggingface"
"75436773","Why does a larger batch size not speed up evaluation time on Huggingface significantly?","2023-02-13 13:52:09","","1","462","<machine-learning><pytorch><huggingface-transformers><huggingface><huggingface-datasets>","<p>I'm trying to evaluate my model on the Squad dataset:</p>
<pre><code>tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForQuestionAnswering.from_pretrained(model_name).to(device)
model.eval() # eval mode

# trainer arguments
args = TrainingArguments(output_dir = 'tmp', per_device_eval_batch_size=256)
# trainer
trainer = Trainer(
    model=model,
    args=args,
    tokenizer=tokenizer)

with torch.no_grad():
    # load dataset
    squad = load_dataset('squad') 
    squad_validation = squad['validation']
    # preprocess
    original_validation_dataset = squad_validation.map(preprocess_validation_examples,batched=True,remove_columns=squad_validation.column_names)
    trainer_output = trainer.predict(original_validation_dataset)
</code></pre>
<p>However, it seems like it doesn't matter if my <code>per_device_eval_batch_size</code> is 1,8,16,256, or 512, it always results in about 1 min of evaluation time. Batch of size 1 results in 1 min and 32 sec, batch of size 8 results in 1 min and 16 sec, batch of size 512 results in 1 min and 6 sec.</p>
<p>I'd expect the batch size to matter much more than a few seconds different so I'm wondering if I'm doing something wrong. Loading the model takes 4.3 seconds, and with a batch size of 1024 I run of GPU memory.</p>
","huggingface"
"75427464","How to speed up sending big file to Cloud Storage","2023-02-12 13:52:32","","0","84","<python><google-cloud-platform><huggingface-transformers><huggingface>","<p>I am very new to databases. So I am trying to send my transformers hugging face model to google cloud by doing this</p>
<pre><code>model_file = pickle.dumps(model)

blob = bucket.blob(f&quot;models/{user_id}.pickle&quot;)

blob.upload_from_string(model_file)
</code></pre>
<p>I am getting this error: HTTPSConnectionPool(host='storage.googleapis.com', port=443): Max retries exceeded with url</p>
<p>I have tried using zlib to compress it and speed it up however it still doesnt work.</p>
<p>Any help would be appreciated it thanks.</p>
","huggingface"
"75385142","tokenizer.push_to_hub(repo_name) is not working","2023-02-08 11:29:28","75400525","1","1232","<python><pytorch><huggingface-transformers><huggingface-tokenizers><huggingface>","<p>I'm trying to puch my tokonizer to my huggingface repo...
it consist of the model vocab.Json (I'm making a speech recognition model)
My code:</p>
<pre><code>vocab_dict[&quot;|&quot;] = vocab_dict[&quot; &quot;]
del vocab_dict[&quot; &quot;]
vocab_dict[&quot;[UNK]&quot;] = len(vocab_dict)
vocab_dict[&quot;[PAD]&quot;] = len(vocab_dict)
len(vocab_dict)
</code></pre>
<pre><code>import json
with open('vocab.json', 'w') as vocab_file:
    json.dump(vocab_dict, vocab_file)
</code></pre>
<pre><code>from transformers import Wav2Vec2CTCTokenizer

tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(&quot;./&quot;, unk_token=&quot;[UNK]&quot;, pad_token=&quot;[PAD]&quot;, word_delimiter_token=&quot;|&quot;)
</code></pre>
<pre><code>from huggingface_hub import login

login('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')
repo_name = &quot;Foxasdf/ArabicTextToSpeech&quot;
add_to_git_credential=True
tokenizer.push_to_hub(repo_name)
</code></pre>
<p>the tokenizer.push_to_hub(repo_name) is giving me this error:
<strong>TypeError: create_repo() got an unexpected keyword argument 'organization'</strong></p>
<p>I have logged in my huggingface account using
from huggingface_hub import notebook_login
notebook_login()
but the error is still the same..
here's a link of the my collab notebook you can see the full code there and the error: <a href=""https://colab.research.google.com/drive/11tkQ85SfaT6U_1PXDNwk0Q6qogw2r2sw?hl=ar&amp;hl=en&amp;authuser=0#scrollTo=WkbZ_Wcidq8Z"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/11tkQ85SfaT6U_1PXDNwk0Q6qogw2r2sw?hl=ar&amp;hl=en&amp;authuser=0#scrollTo=WkbZ_Wcidq8Z</a></p>
","huggingface"
"75355037","How to define prompt weights to huggingface's diffusers.StableDiffusionInpaintPipeline?","2023-02-05 19:44:55","75732447","2","1409","<python><huggingface><stable-diffusion>","<p>I am tweaking a python script using diffusers inpainting pipeline for a custom video generation idea.</p>
<p><strong>I would like to gradually shift the weights of certain words in the prompt.</strong></p>
<p>As I understand the argument <a href=""https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/inpaint#diffusers.StableDiffusionInpaintPipeline.__call__.prompt_embeds"" rel=""nofollow noreferrer"">prompt_embeds </a>is exactly what i need.</p>
<p>I could not figure out how to define this argument. Can someone pls provide an example?</p>
<p>I know there are frameworks out there where you can just add weights to certain words with the following syntax:</p>
<p>&quot;This is a SD prompt with plus 50% weight added to the last (word:1.5)&quot;</p>
<p>This would be a great solution as well however this does not work with diffusers.StableDiffusionInpaintPipeline</p>
","huggingface"
"75324242","GPT-J (6b): how to properly formulate autocomplete prompts","2023-02-02 13:59:41","","1","250","<jupyter-notebook><amazon-sagemaker><huggingface><gpt-3>","<p>I'm new to the AI playground and for this purpose I'm experimenting with the GPT-J (6b) model on an Amazon SageMaker notebook instance (g4dn.xlarge). So far, I've managed to register an endpoint and run the predictor but I'm sure I'm making the wrong questions or I haven't really understood how the model parameters work (which is probable).</p>
<p>This is my code:</p>
<pre><code># build the prompt
prompt = &quot;&quot;&quot;
language: es
match: comida
topic: hoteles en la playa todo incluido
output: ¿Sabes cuáles son los mejores Hoteles Todo Incluido de España? Cada vez son 
más los que se suman a la moda del Todo Incluido para disfrutar de unas perfectas y 
completas vacaciones en familia, en pareja o con amigos. Y es que con nuestra oferta 
hoteles Todo Incluido podrás vivir unos días de auténtico relax y una estancia mucho 
más completa, ya que suelen incluir desde el desayuno, la comida y la cena, hasta 
cualquier snack y bebidas en las diferentes instalaciones del hotel. ¿Qué se puede 
pedir más para relajarse durante una perfecta escapada? A continuación, te 
presentamos los mejores hoteles Todo Incluido de España al mejor precio.

language: es
match: comida
topic: hoteles en la playa todo incluido
output:
&quot;&quot;&quot;

# set the maximum token length
maximum_token_length = 25

# set the sampling temperature
sampling_temperature = 0.6

# build the predictor arguments
predictor_arguments = {
    &quot;inputs&quot;: prompt,
    &quot;parameters&quot;: {
        &quot;max_length&quot;: len(prompt) + maximum_token_length,
        &quot;temperature&quot;: sampling_temperature
    }
}

# execute the predictor with the prompt as input
predictor_output = predictor.predict(predictor_arguments)

# retrieve the text output
text_output = predictor_output[0][&quot;generated_text&quot;]

# print the text output
print(f&quot;text output: {text_output}&quot;)
</code></pre>
<p>My problem is I try to get a different response using the same parameters but I get nothing. It just repeats my inputs with an empty response so I'm definitely doing something wrong although the funny thing is I actually get a pretty understandable text output if I throw the same input with the same sampling temperature on the OpenAI playground (on text-davinci-003).</p>
<p>Can you give me a hint on what am I doing wrong? Oh, and another question is: How can I specify something like 'within the first 10 words' for a keyword match?</p>
","huggingface"
"75318132","Creating a function on Digital Ocean for hugging face","2023-02-02 02:38:35","","0","401","<lambda><digital-ocean><huggingface-transformers><huggingface><huggingface-datasets>","<p>Hugging face provides transforms and models that allows AL/ML processing offline - <a href=""https://huggingface.co/"" rel=""nofollow noreferrer"">https://huggingface.co/</a>
We currently use Digital Ocean and I would like to unload our ML onto DO functions. I know AWS does this already with a few AWS services:
<a href=""https://aws.amazon.com/blogs/compute/hosting-hugging-face-models-on-aws-lambda/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/compute/hosting-hugging-face-models-on-aws-lambda/</a>
I was wondering if we can run it on DO anyone done this as it would really scale and be super cheap to do this instead of spinning up a droplet?
Any help would be appreciated.</p>
<p>This can already be done on AWS but looking to see if we can do it with DO functions?</p>
","huggingface"
"75296909","Using TPU on the Huggingface Pipeline throws PyTorch error","2023-01-31 12:08:03","","1","412","<pytorch><huggingface-transformers><huggingface><roberta-language-model>","<p>I used to run this script using GPUs on GCP, but I am now trying to implement it using TPUs. As far as I am concerned, TPUs should now be working fine with the transformers <code>pipeline</code>.</p>
<p>However, trying to set the device parameter throws <code>RuntimeError: Cannot set version_counter for inference tensor</code></p>
<pre><code>from transformers import pipeline
import torch
import torch_xla
import torch_xla.core.xla_model as xm

classifier = pipeline(&quot;text-classification&quot;,model='bhadresh-savani/distilbert-base-uncased-emotion', return_all_scores=True, device=device)

def detect_emotions(emotion_input):
    
    &quot;&quot;&quot;Model Inference Section&quot;&quot;&quot;
    prediction = classifier(emotion_input,)
    output = {}
    
    for emotion in prediction[0]:
        output[emotion[&quot;label&quot;]] = emotion[&quot;score&quot;]   
    return output


detect_emotions('Rest in Power: The Trayvon Martin Story’ takes an emotional look back at the shooting that divided a nation')
</code></pre>
<p>How would this be rectified? What does this error even mean?</p>
","huggingface"
"75294121","Huggingface: ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds","2023-01-31 07:55:09","","3","444","<python><data-science><huggingface>","<p>I am fine-tuning 'microsoft/trocr-base-printed' image2text model to let it recognize the captcha text on it. I was able to find <a href=""https://github.com/NielsRogge/Transformers-Tutorials/blob/master/TrOCR/Fine_tune_TrOCR_on_IAM_Handwriting_Database_using_Seq2SeqTrainer.ipynb"" rel=""nofollow noreferrer"">this link</a> to try to avoid the error: <strong>ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds</strong>, but it still won't work for me. Below is my python code.</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import TrOCRProcessor, VisionEncoderDecoderModel, BertTokenizer
from transformers import pipeline, default_data_collator
from datasets import load_dataset, Image as image
from datasets import Dataset, Features, Array3D
from PIL import Image
from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments

from glob import glob

import time
import os
import pandas as pd
import numpy as np
import pyarrow as pa
import pickle

picture_path = './captcha_100'
LIMIT = 100
directory = os.listdir(picture_path)[:LIMIT]
target = pd.read_csv('./captcha.csv').to_numpy().tolist()
data = []
picture = []

#read captcha picture from local directory
for d in directory:
    picture.append(Image.open(picture_path+'/'+d))

processor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-printed')
model = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')

#What I believe this code snippet can help me, but it's not work
model.config.decoder_start_token_id = processor.tokenizer.cls_token_id
model.config.pad_token_id = processor.tokenizer.pad_token_id
model.config.vocab_size = model.config.decoder.vocab_size

#making my own dataset
for i in range(LIMIT):
    temp = {}
    temp['pixel_values'] = processor(picture[i]).pixel_values[0].tolist()
    temp['answer'] = target[i][0]
    data.append(temp)
    
data = pa.Table.from_pylist(data)
data = Dataset(data)   
data = data.train_test_split(train_size=0.8)

#giving training arguments
training_args = Seq2SeqTrainingArguments(
    output_dir=&quot;./captcha100&quot;,
    per_device_train_batch_size=16,
    evaluation_strategy=&quot;steps&quot;,
    num_train_epochs=4,
    fp16=True,
    save_steps=100,
    eval_steps=100,
    logging_steps=10,
    learning_rate=2e-4,
    save_total_limit=2,
    remove_unused_columns=False,
    load_best_model_at_end=True,
)

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=data[&quot;train&quot;],
    eval_dataset=data[&quot;test&quot;],
    tokenizer=processor.feature_extractor,
    data_collator=default_data_collator
)

#train
trainer.train()
model = model.to('cpu')

#save the model
with open('captchaModel100.pkl', 'wb') as f:
    pickle.dump(model, f)

with open('processor.pkl', 'wb') as f:
    pickle.dump(processor, f)

</code></pre>
<p>And full traceback</p>
<pre><code>Traceback (most recent call last):
  File &quot;/home/aclab/Joywang/captcha/train_captcha.py&quot;, line 68, in &lt;module&gt;
    trainer.train()
  File &quot;/home/aclab/.virtualenvs/cudatest/lib/python3.9/site-packages/transformers/trainer.py&quot;, line 1501, in train
    return inner_training_loop(
  File &quot;/home/aclab/.virtualenvs/cudatest/lib/python3.9/site-packages/transformers/trainer.py&quot;, line 1749, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File &quot;/home/aclab/.virtualenvs/cudatest/lib/python3.9/site-packages/transformers/trainer.py&quot;, line 2508, in training_step
    loss = self.compute_loss(model, inputs)
  File &quot;/home/aclab/.virtualenvs/cudatest/lib/python3.9/site-packages/transformers/trainer.py&quot;, line 2540, in compute_loss
    outputs = model(**inputs)
  File &quot;/home/aclab/.virtualenvs/cudatest/lib/python3.9/site-packages/torch/nn/modules/module.py&quot;, line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;/home/aclab/.virtualenvs/cudatest/lib/python3.9/site-packages/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py&quot;, line 609, in forward
    decoder_outputs = self.decoder(
  File &quot;/home/aclab/.virtualenvs/cudatest/lib/python3.9/site-packages/torch/nn/modules/module.py&quot;, line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;/home/aclab/.virtualenvs/cudatest/lib/python3.9/site-packages/transformers/models/trocr/modeling_trocr.py&quot;, line 958, in forward
    outputs = self.model.decoder(
  File &quot;/home/aclab/.virtualenvs/cudatest/lib/python3.9/site-packages/torch/nn/modules/module.py&quot;, line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;/home/aclab/.virtualenvs/cudatest/lib/python3.9/site-packages/transformers/models/trocr/modeling_trocr.py&quot;, line 637, in forward
    raise ValueError(&quot;You have to specify either decoder_input_ids or decoder_inputs_embeds&quot;)
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

</code></pre>
<p>Big thanks for help.</p>
","huggingface"
"75277433","I'm trying to convert Pandas dataframe to HuggingFace DatasetDic","2023-01-29 18:39:23","","1","669","<dataset><huggingface-transformers><tensorflow-datasets><huggingface><huggingface-datasets>","<p>I have a pandas dataframe with 20k rows containing 2 columns named English, te. Changed the English column name to en. Trying to split the dataset into train, validation and test. And, I want to convert that dataset into</p>
<pre><code>raw_datasets
</code></pre>
<p>the output i'm expecting is</p>
<pre><code>DatasetDict({
    train: Dataset({
        features: ['translation'],
        num_rows: 18000
    })
    validation: Dataset({
        features: ['translation'],
        num_rows: 1000
    })
    test: Dataset({
        features: ['translation'],
        num_rows: 1000
    })
})
</code></pre>
<p>I'm trying to write a code like <code>raw_datasets[&quot;train&quot;][0]</code>, then it should return output like below</p>
<pre><code>{'translation': {'en': 'Membership of Parliament: see Minutes',
  'to': 'Componenţa Parlamentului: a se vedea procesul-verbal'}}
</code></pre>
<p>The data must be in DatasetDict, similar to if we load data from huggingface like dataset DatasetDict type. Below is the code i've written but it's not working</p>
<pre><code>import pandas as pd
from collections import namedtuple

Dataset = namedtuple('Dataset', ['features', 'num_rows'])
DatasetDict = namedtuple('DatasetDict', ['train', 'validation', 'test'])

def create_dataset_dict(df):
    # Rename the column
    df = df.rename(columns={'English': 'en'})
    # Split the data into train, validation and test
    train_df = df.iloc[:18000, :]
    validation_df = df.iloc[18000:19000, :]
    test_df = df.iloc[19000:, :]
    # Create the dataset dictionaries
    train = Dataset(features=['translation'], num_rows=18000)
    validation = Dataset(features=['translation'], num_rows=1000)
    test = Dataset(features=['translation'], num_rows=1052)
    # Create the final dataset dictionary
    datasets = DatasetDict(train=train, validation=validation, test=test)
    return datasets

def preprocess_dataset(df):
    df = df.rename(columns={'English': 'en'})
    train_df = df.iloc[:18000, :]
    validation_df = df.iloc[18000:19000, :]
    test_df = df.iloc[19000:, :]
    train_dict = [{'translation': {'en': row['en'], 'te': row['te']}} for _, row in train_df.iterrows()]
    validation_dict = [{'translation': {'en': row['en'], 'te': row['te']}} for _, row in validation_df.iterrows()]
    test_dict = [{'translation': {'en': row['en'], 'te': row['te']}} for _, row in test_df.iterrows()]
    return DatasetDict(train=train_dict, validation=validation_dict, test=test_dict)

df = pd.read_csv('eng-to-te.csv')
raw_datasets = preprocess_dataset(df)
</code></pre>
<p>The above code is not working. Can anyone help me with this?</p>
","huggingface"
"75240203","Huggingface token classification pipeline giving different outputs than just calling model() directly","2023-01-25 21:46:07","","2","610","<pytorch><huggingface-transformers><named-entity-recognition><huggingface-tokenizers><huggingface>","<p>I am trying to mask named entities in text, using <a href=""https://huggingface.co/51la5/roberta-large-NER"" rel=""nofollow noreferrer"">a roberta based model</a>.
The suggested way to use the model is via Huggingface pipeline but i find that it is rather slow to use it that way. Using a pipeline on text data also prevents me from using my GPU for computation, as the text cannot be put onto the GPU.</p>
<p>Due to this, i decided to put the model on the GPU, tokenize the text myself(using the same tokenizer i pass to the pipeline), put the tokens on the GPU and pass them to the model afterwards. This works, but the outputs of the model used directly like this and not via the pipeline <strong>differ significantly</strong>.
I cant find a reason for this nor a way to fix it.</p>
<p>I tried reading through the token classification pipeline <a href=""https://huggingface.co/transformers/v4.7.0/_modules/transformers/pipelines/token_classification.html"" rel=""nofollow noreferrer"">source code</a> but couldnt find a difference in my usage compared to what the pipeline does.</p>
<h1>Examples of code which produce different results:</h1>
<ol>
<li>Suggested usage in the model card:</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>ner_tokenizer = AutoTokenizer.from_pretrained(&quot;xlm-roberta-large-finetuned-conll03-english&quot;)
model = AutoModelForTokenClassification.from_pretrained(&quot;xlm-roberta-large-finetuned-conll03-english&quot;)
classifier = pipeline(&quot;ner&quot;, model=model, tokenizer=ner_tokenizer, framework='pt')
out = classifier(dataset['text'])
</code></pre>
<p>'out' is now a list of lists of dictionary objects which hold information on each named entity in a given string in list of strings 'dataset['text']'.</p>
<ol start=""2"">
<li>My custom usage:</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>text_batch = dataset['text']
encodings_batch = ner_tokenizer(text_batch,padding=&quot;max_length&quot;, truncation=True, max_length=128, return_tensors=&quot;pt&quot;)
input_ids = encodings_batch['input_ids']
input_ids = input_ids.to(TORCH_DEVICE)
outputs = model(input_ids)[0]
outputs = outputs.to('cpu')
label_ner_ids = outputs.argmax(dim=2).to('cpu')
</code></pre>
<p>'label_ner_ids' is now a tensor of 2 dimensions, the elements of which represent the labels for each token in a given line of text, so label_ner_id[i,j] is the label for the j-th token in the i-th string of text in the list of strings 'text_batch'. <strong>The token labels here differ from the outputs of the pipeline usage.</strong></p>
","huggingface"
"75198323","""nvidia/tts_en_fastpitch"" not getting installed in python","2023-01-22 05:17:29","75353329","0","332","<python><deep-learning><text-to-speech><nvidia><huggingface>","<p>I am trying to create TTS with Nvidia NeMo tts_en_fastpitch model in python. But can not install fastpitch model. These are the errors:</p>
<pre><code>from nemo.collections.tts.models import HifiGanModel
from nemo.collections.tts.models import FastPitchModel

spec_generator = FastPitchModel.from_pretrained(&quot;nvidia/tts_en_fastpitch&quot;)
model = HifiGanModel.from_pretrained(model_name=&quot;nvidia/tts_hifigan&quot;)
</code></pre>
<p><code>[NeMo W 2023-01-21 18:49:02 optimizers:55] Apex was not found. Using the lamb or fused_adam optimizer will error out. [NeMo W 2023-01-21 18:49:03 __init__:22] </code>pynini<code>is not installed !    Please run the</code>nemo_text_processing/setup.sh<code> scriptprior to usage of this toolkit. Traceback (most recent call last):  File &quot;c:/Users/ASROCK B560M STEEL/Desktop/nvidia tts/ntts.py&quot;, line 3, in &lt;module&gt; spec_generator = FastPitchModel.from_pretrained(&quot;nvidia/tts_en_fastpitch&quot;)  File &quot;C:\Users\ASROCK B560M STEEL\AppData\Local\Programs\Python\Python38\lib\site-packages\nemo\core\classes\common.py&quot;, line 692, in from_pretrained raise FileNotFoundError(FileNotFoundError: Model nvidia/tts_en_fastpitch was not found. Check cls.list_available_models() for the list of all available models.</code></p>
<p>Please help me with it.</p>
<p>I tried code from huggingface and also from youtube. The error is same, tts_en_fastpitch is not getting installed.</p>
","huggingface"
"75197492","Share is not supported when you are in Spaces. Gradio&Hugging face's Spaces","2023-01-22 00:10:10","","2","848","<huggingface><gradio>","<p>Basiclly problem is that i can only run my space locally. Either error is &quot;To create a public link, set <code>share=True</code> in <code>launch()</code>.&quot;, and after doing that RuntimeError: &quot;Share is not supported when you are in Spaces.&quot;</p>
<pre><code>intf = gr.Interface(fn=classify_image, inputs=image, outputs=label, examples=examples)
intf.launch(inline=True, share=True)
</code></pre>
<p>Anyway to fix it? And what could cause it? It runs locally perfectly fine</p>
","huggingface"
"75195026","Transformers gets killed for no reason on linux","2023-01-21 16:54:21","","4","2609","<memory-management><nlp><out-of-memory><huggingface-transformers><huggingface>","<p>So i'm trying to run inference on a Huggingface model, the model is 6.18gb.
This morning I was on Windows and it was possible to load the model, but inference was very slow so I took a look at DeepSpeed but only available on linux so I switched to Zorin OS.
Now the exact same script gets killed when running</p>
<pre><code>from transformers import AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained(&quot;Cedille/fr-boris&quot;, device_map = &quot;auto&quot;)
</code></pre>
<p>What is going on ?</p>
","huggingface"
"75192212","Template for RLHF with the TRL library","2023-01-21 09:06:58","","1","269","<pytorch><huggingface-transformers><kaggle><huggingface><gpt-2>","<p>I'm trying to implement a very very basic working template for RLHF with TRL. The notebook is here:</p>
<p><a href=""https://www.kaggle.com/code/mcantoni81/rlhf-with-trl-gpt2"" rel=""nofollow noreferrer"">https://www.kaggle.com/code/mcantoni81/rlhf-with-trl-gpt2</a></p>
<p>My target here is to make gpt2 answer &quot;i'm the mailman&quot;, but maybe i'm not getting right the mechanics of TRL. Looks like the training doesn't influence the model at all.</p>
<p>How can i correct this template?</p>
<p>I've expected the queries of the model to somehow change.</p>
","huggingface"
"75181996","Arrow related error when pushing dataset to Hugging-face hub","2023-01-20 09:21:50","75221659","2","282","<pandas><parquet><pyarrow><huggingface><huggingface-datasets>","<p>i have quite a problem with my dataset:</p>
<p>The (future) dataset is a pandas dataframe that i loaded from a pickle file, the pandas dataset behaves correctly. My code is:</p>
<pre><code>dataset.from_pandas(df)
dataset.push_to_hub(&quot;username/my_dataset&quot;, private=True)
</code></pre>
<p>because I thought it was pandas fault I also tried:</p>
<pre><code>dataset = Dataset.from_dict(df_sentences.to_dict(orient='list'))
dataset.push_to_hub(&quot;username/my_dataset&quot;, private=True)

</code></pre>
<p>and to load it from file.</p>
<p>The error I get is:</p>
<blockquote>
<p>ArrowNotImplementedError: Unhandled type for Arrow to Parquet schema conversion: string</p>
</blockquote>
<p>My dataset is composed by 4 columns of type string and one of ints, around 3600 rows</p>
","huggingface"
"75157176","Huggingface API POST call via httr - encoding/parsing problem","2023-01-18 09:37:51","75158282","2","141","<r><json><api><httr><huggingface>","<p>I'm trying to replicate a Python API call in R, using the httr package.</p>
<p>Python code from <a href=""https://huggingface.co/blog/few-shot-learning-gpt-neo-and-inference-api"" rel=""nofollow noreferrer"">huggingface</a>:</p>
<pre><code>import json
import requests

API_TOKEN = &quot;&quot;

def query(payload='',parameters=None,options={'use_cache': False}):
    API_URL = &quot;https://api-inference.huggingface.co/models/EleutherAI/gpt-neo-2.7B&quot;
        headers = {&quot;Authorization&quot;: &quot;TOKEN&quot;}
    body = {&quot;inputs&quot;:payload,'parameters':parameters,'options':options}
    response = requests.request(&quot;POST&quot;, API_URL, headers=headers, data= json.dumps(body))
    try:
      response.raise_for_status()
    except requests.exceptions.HTTPError:
        return &quot;Error:&quot;+&quot; &quot;.join(response.json()['error'])
    else:
      return response.json()[0]['generated_text']

parameters = {
    'max_new_tokens':25,  # number of generated tokens
    'temperature': 0.5,   # controlling the randomness of generations
    'end_sequence': &quot;###&quot; # stopping sequence for generation
}

prompt=&quot;Tweet: \&quot;I hate it when my phone battery dies.\&quot;\n&quot; + \
&quot;Sentiment: Negative\n&quot; + \
&quot;###\n&quot; + \
&quot;Tweet: \&quot;My day has been 👍\&quot;\n&quot; + \
&quot;Sentiment: Positive\n&quot; + \
&quot;###\n&quot; + \
&quot;Tweet: \&quot;This is the link to the article\&quot;\n&quot; + \
&quot;Sentiment: Neutral\n&quot;+ \
&quot;###\n&quot; + \
&quot;Tweet: \&quot;This new music video was incredible\&quot;\n&quot; + \
&quot;Sentiment:&quot;             # few-shot prompt

data = query(prompt,parameters,options)
</code></pre>
<p>This works - the model completes the prompt usually with the string &quot;Positive&quot;.</p>
<p>My attempt in R:</p>
<pre><code>library(httr)
headers &lt;- c(
  `Authorization` = &quot;TOKEN&quot;,
  `Content-Type` = &quot;application/x-www-form-urlencoded&quot;
)

# this was guessed manually from https://huggingface.co/blog/few-shot-learning-gpt-neo-and-inference-api

# does not quite work yet?
# removing \&quot;´does not make a difference
prompt &lt;- &quot;Tweet: I hate it when my phone battery dies.
Sentiment: Negative
###
Tweet: My day has been 👍
Sentiment: Positive
###
Tweet: This is the link to the article
Sentiment: Neutral
###
Tweet: This new music video was incredible
Sentiment:&quot;

data &lt;- c(`inputs` = prompt,
          `max_new_tokens` = 3, 
          `temperature` = 0.5,
          `end_sequence` = &quot;###&quot;)


res &lt;- httr::POST(url = &quot;https://api-inference.huggingface.co/models/EleutherAI/gpt-neo-2.7B&quot;, 
                  httr::add_headers(.headers=headers), body = data)

content(res)[[1]][[1]]
[1] &quot;Tweet: I hate it when my phone battery dies.\nSentiment: Negative\n###\nTweet: My day has been 👍\nSentiment: Positive\n###\nTweet: This is the link to the article\nSentiment: Neutral\n###\nTweet: This new music video was incredibile\nSentiment:\n3\n0.5\n###\n&quot;
</code></pre>
<p>The R output is &quot;\n3\n0.5\n###\n&quot; - random linebreaks and numbers. I suspect there could be some issue with encoding or how httr parses the json output from the API.</p>
","huggingface"
"75155423","'OMP: Error #179' when using hugging face BERT","2023-01-18 06:31:53","","1","393","<python><pytorch><gpu><huggingface>","<p>I simply tried running this code.</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import BertForSequenceClassification, BertTokenizer
</code></pre>
<p>with this command. For 'GPU_ID' I used my GPU UUID, which wasn't wrong.</p>
<pre><code>CUDA_VISIBLE_DEVICES='GPU_ID' python3 MNLI.py
</code></pre>
<p>Sometimes it works fine but sometimes this error pops up.</p>
<pre><code>OMP: Error #179: Function Can't open SHM failed:
OMP: System error #0: Success
Aborted (core dumped)
</code></pre>
<p>How can I fix it? Any help is appreciated.</p>
","huggingface"
"75154742","BERT model conversion from DeepPavlov to HuggingFace format","2023-01-18 04:38:43","75155807","0","82","<python-3.x><huggingface-transformers><bert-language-model><huggingface><deeppavlov>","<p>I have a folder with the ruBERT model, which was fine-tuned with the application of the Deeppavlov library.</p>
<p>The folder contains the following model files:</p>
<p><a href=""https://i.sstatic.net/mKdZE.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/mKdZE.png"" alt=""enter image description here"" /></a></p>
<p>How do I convert it to Huggingface format so that I can load it this way?</p>
<pre><code>from transformers import TFAutoModelForSequenceClassification

model_name = &quot;folder_with_ruBERT&quot;
auto_model_rubert = TFAutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case = False)
</code></pre>
","huggingface"
"75111080","Training wav2vec2 for multiple (classification) tasks","2023-01-13 15:16:51","75111881","1","408","<python><pytorch><classification><huggingface>","<p>I trained a wav2vec2 model using pytorch and huggingface transformer. Here is the code: <a href=""https://github.com/padmalcom/wav2vec2-nonverbalvocalization"" rel=""nofollow noreferrer"">https://github.com/padmalcom/wav2vec2-nonverbalvocalization</a></p>
<p>I now want to train the model on a second tasks, e.g. age classification or speech recognition (ASR).</p>
<p>My problem is, that I do not really understand how I can configure my model to accept a seconds input and train another output. Can anybody give me a short explaination?</p>
<p>I know that I have to use multiple heads in my model and that the thing I want to achieve is called &quot;multi task learning&quot;. My problem is, that I don't know how to write the model for that.</p>
","huggingface"
"75110981","SSLError: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /dslim/bert-base-NER/resolve/main/tokenizer_config.json","2023-01-13 15:09:53","","15","49624","<python-3.x><huggingface-transformers><bert-language-model><huggingface-tokenizers><huggingface>","<p>I am facing below issue while loading the pretrained BERT model from HuggingFace due to SSL certificate error.</p>
<h2>Error:</h2>
<blockquote>
<p>SSLError: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /dslim/bert-base-NER/resolve/main/tokenizer_config.json (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate in certificate chain (_ssl.c:1108)')))</p>
</blockquote>
<h2>The line that is causing the issue is:</h2>
<pre><code>tokenizer = AutoTokenizer.from_pretrained(&quot;dslim/bert-base-NER&quot;)
</code></pre>
<h2>Source code:</h2>
<pre><code>from transformers import AutoTokenizer, AutoModelForTokenClassification
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(&quot;dslim/bert-base-NER&quot;)
model = AutoModelForTokenClassification.from_pretrained(&quot;dslim/bert-base-NER&quot;)
</code></pre>
<p>I am expecting to download pre-trained models while running the code in jupyter lab on Windows.</p>
","huggingface"
"75110689","Generating text word by word for transformers","2023-01-13 14:43:17","","2","386","<python><huggingface-transformers><huggingface>","<p>I’m currently using GPT-J for generating text as shown below. This works well but it takes up to 5 seconds to generate the 100 tokens.</p>
<p>Is it possible to do the generation word by word or sentence by sentence? Similar to what ChatGPT is doing (ChatGPT seems to produce the output word by word).</p>
<pre><code>import transformers
from transformers import GPTJForCausalLM
config = transformers.GPTJConfig.from_pretrained(&quot;EleutherAI/gpt-j-6B&quot;)
tokenizer = transformers.AutoTokenizer.from_pretrained(&quot;EleutherAI/gpt-j-6B&quot;, pad_token='&lt;|endoftext|&gt;', eos_token='&lt;|endoftext|&gt;', truncation_side='left')
model = GPTJForCausalLM.from_pretrained(
            &quot;EleutherAI/gpt-j-6B&quot;,
            revision=&quot;float16&quot;,
            torch_dtype=torch.float16,
            low_cpu_mem_usage=True,
            use_cache=True,
            gradient_checkpointing=True,
        )
model.to(&quot;cuda&quot;)
prompt = tokenizer(&quot;This is a test sentence, which should be completed&quot;, return_tensors='pt', truncation=True, max_length=2000)
prompt = {key: value.to(&quot;cuda&quot;) for key, value in prompt.items()}
out = model.generate(**prompt,
                     n=1,
                     min_length=16,
                     max_new_tokens=100,
                     do_sample=True,
                     top_k=15,
                     top_p=0.9,
                     batch_size=1,
                     temperature=1,
                     no_repeat_ngram_size=4,
                     clean_up_tokenization_spaces=True,
                     use_cache=True,
                     pad_token_id=tokenizer.eos_token_id,
                     )
 res = tokenizer.decode(out[0])
</code></pre>
","huggingface"
"75083737","How to save custom dataset in local folder","2023-01-11 13:21:50","75083832","0","2942","<python><huggingface><huggingface-datasets>","<p>I have created a custom huggingface dataset, containing images and ground truth data coming from json lines file. I want to save it to a local folder and be able to use it as is by loading it after to other notebooks. I did not find out how this can happen.</p>
<pre><code>DatasetDict({
    train: Dataset({
        features: ['image', 'id', 'ground_truth'],
        num_rows: 7
    })
    test: Dataset({
        features: ['image', 'id', 'ground_truth'],
        num_rows: 4
    })
})
</code></pre>
","huggingface"
"75076501","Error trying to load accuracy metric with evaluate","2023-01-10 22:26:02","","0","764","<python><huggingface-transformers><evaluate><huggingface>","<p>I'm trying to train a GPT-2 model using an example script from Huggingface Transformer Github and I get an error when the scripts tries to load 'accuracy' metric:</p>
<p><code>evaluate.load(&quot;accuracy&quot;)</code></p>
<p>File &quot;/home/.../miniconda3/lib/python3.10/site-packages/evaluate/loading.py&quot;, line 752, in load
evaluation_instance = evaluation_cls(
TypeError: 'NoneType' object is not callable</p>
<p>I tried to load other metrics and I only get error with accuracy</p>
<p>I've got installed the requirements:</p>
<ul>
<li>scikit-learn 1.3.dev0</li>
<li>evaluate 0.4.1.dev0</li>
</ul>
<p>I'm running the run_clm.py script example for language modeling.</p>
<p>Is there something I need to install besides scikit-learn and evaluate? Is there something wrong with the library?</p>
<p>Thanks!</p>
","huggingface"
"75073820","Target Masking in Huggingface models","2023-01-10 17:37:45","","0","387","<python><pytorch><bert-language-model><huggingface>","<p>I was implementing an encoder-decoder architecture, relying on a custom pytorch encoder and HuggingFace BERT as decoder (so I can't use the EncoderDecoder class of HuggingFace).</p>
<p>My task is translation, so I need to make BERT work in an autoregressive decoder. In the input I don't need word masking, only padding masking. However I do need to mask future target tokens not to &quot;cheat&quot; when training it.</p>
<p>I set the config parameters <code>is_decoder</code> and <code>use_cross_attention</code> to True, and by printing a summary looks like it is correctly linked with the encoder.
However, my issue raises because I don't know what I should pass to the forward method in order to correctly make the target mask.</p>
<p>According to the official documentation (<a href=""https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertModel"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertModel</a>), this is the declaration of the method:</p>
<pre class=""lang-py prettyprint-override""><code>def forward(input_ids: typing.Optional[torch.Tensor] = None, 
    attention_mask: typing.Optional[torch.Tensor] = None, 
    token_type_ids: typing.Optional[torch.Tensor] = None, 
    position_ids: typing.Optional[torch.Tensor] = None, 
    head_mask: typing.Optional[torch.Tensor] = None, 
    inputs_embeds: typing.Optional[torch.Tensor] = None, 
    encoder_hidden_states: typing.Optional[torch.Tensor] = None, 
    encoder_attention_mask: typing.Optional[torch.Tensor] = None, 
    past_key_values: typing.Optional[typing.List[torch.FloatTensor]] = None, 
    use_cache: typing.Optional[bool] = None, 
    output_attentions: typing.Optional[bool] = None, 
    output_hidden_states: typing.Optional[bool] = None, 
    return_dict: typing.Optional[bool] = None ) → transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions or tuple(torch.FloatTensor)
</code></pre>
<p>According to the descriptions, both <code>attention_mask</code> and <code>encoder_attention_mask</code> are used to prevent attending over padding, so as source masks. I was wondering whether one of the two are instead used as target mask when BERT is in <code>is_decoder</code> mode, or if the target mask is automatically handled by HuggingFace, or if simply I'm trying something impossible.</p>
<p>By passing random values to one or the other or both it always gives me an output without exceptions, but I have no clue of what is happening internally, so no way to understand if that output is correct. To know it, I should first train the network, but my training will be very heavy, so I can't try al the possibilities but I need to understand what is happening.</p>
<p>I think that also a general example of BERT used as decoder, assuming to have a working custom encoder could be of great help</p>
<p>Thank you!</p>
","huggingface"
"75065273","Low RAM Usage & high GPU usage, HF Datasets not helping in fine-tuning LM","2023-01-10 03:47:46","","1","846","<ram><huggingface-transformers><huggingface><huggingface-datasets>","<p>I've been trying to finetune &quot;roBERTa base&quot; model using the MLM task. (<a href=""https://huggingface.co/course/en/chapter7/3"" rel=""nofollow noreferrer"">https://huggingface.co/course/en/chapter7/3</a>) with the sample training dataset of 20_000 points, preprocessed according to the need.</p>
<p>I've used HuggingFace datasets (<a href=""https://huggingface.co/docs/datasets/v2.6.1/en/package_reference/loading_methods#datasets.load_dataset"" rel=""nofollow noreferrer"">https://huggingface.co/docs/datasets/v2.6.1/en/package_reference/loading_methods#datasets.load_dataset</a>) to read the data points &amp; I am also using the data collate function to achieve dynamic masking for every batch.</p>
<p>I am using g4dn.2xlarge (<a href=""https://instances.vantage.sh/aws/ec2/g4dn.2xlarge"" rel=""nofollow noreferrer"">https://instances.vantage.sh/aws/ec2/g4dn.2xlarge</a> - 32GB RAM, 16GB GPU, 8 vCPUs) instance for fine-tuning <code>Roberta-base</code> MLM task with a brach size of 8 &amp; each datapoint is of 512 sequence length. And I am using HuggingFace Trainer API</p>
<p>With the above config, <strong>I observed GPU memory was very high, 95%+ and system RAM utilization was around 13-15%!</strong></p>
<p>I did set follow the <a href=""https://huggingface.co/docs/datasets/v1.12.0/cache.html"" rel=""nofollow noreferrer"">https://huggingface.co/docs/datasets/v1.12.0/cache.html</a> &amp; set <code>IN_MEMORY_MAX_SIZE</code> to ~25GB, but no luck</p>
<pre><code>import datasets
datasets.config.IN_MEMORY_MAX_SIZE = 24_696_061_952
train_dataset = load_dataset('pandas',
                             data_files={'train': 'path to pickle file'}, 
                             keep_in_memory=True)
</code></pre>
<p>But RAM usage remained as it is. <strong>How can I make full utilization of RAM &amp; GPU memory?</strong></p>
<p>I've to take 20_000 as the sample for this experiment but I've ~1 Million data points, which I will be using in the future for full-fledged training, post-resolution to this problem</p>
<p>Any suggestions?</p>
<p>Huggingface discussion: <a href=""https://discuss.huggingface.co/t/low-ram-usage-high-gpu-usage-datasets-not-helping/29269/1"" rel=""nofollow noreferrer"">https://discuss.huggingface.co/t/low-ram-usage-high-gpu-usage-datasets-not-helping/29269/1</a></p>
","huggingface"
"75060922","No space left on device error when trying to load GPT2 model","2023-01-09 17:27:33","","1","792","<linux><huggingface><oserror><gpt-2>","<p>I am trying to run an experiment with GPT2; i.e., I use <code>model = GPT2Model.from_pretrained('gpt2-xl')</code></p>
<p>The error I get is a traceback which leads to <code>OSError: [Errno 28] No space left on device: '/home/username/.cache/huggingface/transformers/tmpvuvw8j0t'</code>.</p>
<p>This is unlikely to be an error with my code itself, because the exact same code works fine for GPT3-1.3B and T5-Large (except of course for the model= .... line).</p>
<p>I think the issue is that it tries to download the GPT2 model and runs out of space on the device. The /home/username directory has a pretty small amount of storage; the /data/username directory is where most of the storage is. I'm not quite sure how to redirect it to download the weights on the latter directory, or if that would even help.</p>
<p>I'd really appreciate any help in resolving this!</p>
","huggingface"
"75052206","Specifying Huggingface model as project dependency","2023-01-09 00:13:17","75526581","3","740","<python><docker><python-poetry><huggingface>","<p>Is it possible to install huggingface models as a project dependency?</p>
<p>Currently it is downloaded automatically by the <code>SentenceTransformer</code> library, but this means in a docker container it downloads every time it starts.</p>
<p>This is the model I am trying to use: <a href=""https://huggingface.co/sentence-transformers/all-mpnet-base-v2"" rel=""nofollow noreferrer"">https://huggingface.co/sentence-transformers/all-mpnet-base-v2</a></p>
<p>I have tried specifying the url as a dependency in my <code>pyproject.toml</code>:</p>
<pre><code>all-mpnet-base-v2 = {git = &quot;https://huggingface.co/sentence-transformers/all-mpnet-base-v2.git&quot;, branch = &quot;main&quot;}
</code></pre>
<p>The first error I got was that the name was incorrect and it should be called <code>train-script</code>, which I renamed the dependency to, but I'm not sure if this is correct.  Now I have:</p>
<pre><code>train-script = {git = &quot;https://huggingface.co/sentence-transformers/all-mpnet-base-v2.git&quot;, branch = &quot;main&quot;}
</code></pre>
<p>However, now I get the following error:</p>
<pre><code> Package operations: 1 install, 0 updates, 0 removals

   • Installing train-script (0.0.0 bd44305)

   EnvCommandError

   Command ['/srv/.venv/bin/pip', 'install', '--no-deps', '-U', '/srv/.venv/src/train-script'] errored with the following return code 1, and output:
   ERROR: Directory '/srv/.venv/src/train-script' is not installable. Neither 'setup.py' nor 'pyproject.toml' found.

   [notice] A new release of pip available: 22.2.2 -&gt; 22.3.1
   [notice] To update, run: pip install --upgrade pip


   at /usr/local/lib/python3.10/site-packages/poetry/utils/env.py:1183 in _run
       1179│                 output = subprocess.check_output(
       1180│                     cmd, stderr=subprocess.STDOUT, **kwargs
       1181│                 )
       1182│         except CalledProcessError as e:
     → 1183│             raise EnvCommandError(e, input=input_)
       1184│
       1185│         return decode(output)
       1186│
       1187│     def execute(self, bin, *args, **kwargs):
</code></pre>
<p>Is this possible? If not, is there a recommended way to bake the model download into a docker container so it doesn't need to be downloaded each time?</p>
","huggingface"
"75042153","Can't load from AutoTokenizer.from_pretrained - TypeError: duplicate file name (sentencepiece_model.proto)","2023-01-07 16:56:24","75150156","5","4062","<python><nlp><protocol-buffers><huggingface>","<p>I'm trying to load tokenizer and seq2seq model from pretrained models.</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained(&quot;ozcangundes/mt5-small-turkish-summarization&quot;)

model = AutoModelForSeq2SeqLM.from_pretrained(&quot;ozcangundes/mt5-small-turkish-summarization&quot;)
</code></pre>
<p>But I got this error.</p>
<pre><code>File ~/.local/lib/python3.8/site-packages/google/protobuf/descriptor.py:1028, in FileDescriptor.__new__(cls, name, package, options, serialized_options, serialized_pb, dependencies, public_dependencies, syntax, pool, create_key)
   1026     raise RuntimeError('Please link in cpp generated lib for %s' % (name))
   1027 elif serialized_pb:
-&gt; 1028   return _message.default_pool.AddSerializedFile(serialized_pb)
   1029 else:
   1030   return super(FileDescriptor, cls).__new__(cls)

    TypeError: Couldn't build proto file into descriptor pool: duplicate file name (sentencepiece_model.proto)
</code></pre>
<p>I tried updating or downgrading the protobuf version. But I couldn't fix</p>
","huggingface"
"75025547","mT5 transformer, how to access encoder to compute cosine similarity","2023-01-05 23:43:37","","1","102","<dataset><huggingface-transformers><cosine-similarity><huggingface-tokenizers><huggingface>","<p>this is my method, my question is how to access the encoder be sending 2 sentences each time? because I have a dataset that contain pairs of sentences, and I need to compute the similarity between each pair.</p>
<p>//anyone could help?</p>
<pre><code>model = SentenceTransformer('paraphrase-MiniLM-L6-v2')  

#Sentences we want to encode. Example: 
sentence = ['This framework generates embeddings   for each input sentence']
sentence1 = ['This is an embedding for framework generation']

#Sentences are encoded by calling 
embedding = model.encode(sentence)
embedding1 = model.encode(sentence1)
e = np.squeeze(np.asarray(embedding))

e1 = np.squeeze(np.asarray(embedding1))

#calculate Cosine Similarity
cos_sim = dot(e, e1)/(norm(e)*norm(e1))
print(cos_sim)
</code></pre>
","huggingface"
"74970947","How to identify inputs for a diffusion model?","2022-12-31 16:53:14","","0","576","<python><huggingface-tokenizers><huggingface><stable-diffusion>","<p>I'm calling a Stable Diffusion in-painting model with the following code, however I know there are more parameters available in the model pipeline. How do I identify all the available parameters in <a href=""https://huggingface.co/runwayml/stable-diffusion-inpainting"" rel=""nofollow noreferrer"">this stable diffusion in-painting model?</a></p>
<pre><code>from diffusers import StableDiffusionInpaintPipeline

pipe = StableDiffusionInpaintPipeline.from_pretrained(
    &quot;runwayml/stable-diffusion-inpainting&quot;,
    revision=&quot;fp16&quot;,
    torch_dtype=torch.float16,
)
prompt = &quot;Face of a yellow cat, high resolution, sitting on a park bench&quot;
#image and mask_image should be PIL images.
#The mask structure is white for inpainting and black for keeping as is
image = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]
image.save(&quot;./yellow_cat_on_park_bench.png&quot;)

</code></pre>
","huggingface"
"74970230","How can I hide my source code in HuggingFace spaces?","2022-12-31 14:41:11","","2","1211","<python><huggingface>","<p>I've been trying to figure out how to hide my source code in public HuggingFace spaces, but I wasn't able to find a solution for this. Here is what I've read and tried:</p>
<ol>
<li><p>Using Google Actions to rely on tokens, but the code is synced of course to the HuggingFace space, so it is visible.</p>
</li>
<li><p>In the documentation it is said that Git LFS can be used to make the models hidden, but this cannot be applied to source code.</p>
</li>
<li><p>I've also read about using secrets, but this applies to API keys, not app functions.</p>
</li>
</ol>
<p>Note: I've searched for a more relevant community to post this question and already asked the question in HuggingFace community, but no help yet.</p>
","huggingface"
"74966387","Hugging Face - PyTorch RuntimeError : nll_loss_forward_reduce_cuda_kernel_2d_index not implemented for Int","2022-12-30 22:47:59","","0","604","<python><pytorch><huggingface-transformers><huggingface><huggingface-datasets>","<p>I attempted to fine-tune the <code>Wav2vec2</code> model for an audio classification task by following the tutorial at <a href=""https://huggingface.co/docs/transformers/tasks/audio_classification"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/tasks/audio_classification</a> and copying and pasting the code provided.</p>
<p>However, when I trained the model, I encountered the following error:</p>
<pre class=""lang-none prettyprint-override""><code>nll_loss_forward_reduce_cuda_kernel_2d_index not implemented for Int.
</code></pre>
<p>I believe the problem may lie with <code>PyTorch</code>. It seems to be working on <code>Ubuntu WSL 22.04</code>, but not on <code>Powershell</code>.</p>
<p>My system specifications include:</p>
<pre class=""lang-none prettyprint-override""><code>- Powershell
- datasets 2.8.0
- transformers 4.25.1
- torch                   1.13.1+cu117
- torchaudio              0.13.1+cu117
- torchvision             0.14.1+cu117
</code></pre>
<p>How can I resolve this issue?</p>
","huggingface"
"74956310","How can I overcome ""Failed to load model class 'VBoxModel' from module @jupyter-widgets/controls"" error?","2022-12-29 21:44:22","","0","1802","<python><jupyter-notebook><huggingface>","<p>I am stuck at perhaps a very simple problem but I've searched the internet to no avail.</p>
<p>I am attempting to run the following lines in a jupyter notebook (its connected to a rented GPU):</p>
<pre><code># Hugging Face Login
from huggingface_hub import notebook_login

notebook_login()
</code></pre>
<p>and I keep receiving this error:</p>
<blockquote>
<p>[Open Browser Console for more detailed log - Double click to close
this message] Failed to load model class 'VBoxModel' from module
'@jupyter-widgets/controls' Error: Module @jupyter-widgets/controls,
version ^1.5.0 is not registered, however,         2.0.0 is
at f.loadClass (<a href=""https://uaxtfom41wgmr3-8888.proxy.runpod.net/lab/extensions/@jupyter-widgets/jupyterlab-manager/static/134.40eaa5b8e976096d50b2.js?v=40eaa5b8e976096d50b2:1:74977"" rel=""nofollow noreferrer"">https://uaxtfom41wgmr3-8888.proxy.runpod.net/lab/extensions/@jupyter-widgets/jupyterlab-manager/static/134.40eaa5b8e976096d50b2.js?v=40eaa5b8e976096d50b2:1:74977</a>)
at f.loadModelClass (<a href=""https://uaxtfom41wgmr3-8888.proxy.runpod.net/lab/extensions/@jupyter-widgets/jupyterlab-manager/static/150.b0e841b75317744a7595.js?v=b0e841b75317744a7595:1:10729"" rel=""nofollow noreferrer"">https://uaxtfom41wgmr3-8888.proxy.runpod.net/lab/extensions/@jupyter-widgets/jupyterlab-manager/static/150.b0e841b75317744a7595.js?v=b0e841b75317744a7595:1:10729</a>)
at f._make_model (<a href=""https://uaxtfom41wgmr3-8888.proxy.runpod.net/lab/extensions/@jupyter-widgets/jupyterlab-manager/static/150.b0e841b75317744a7595.js?v=b0e841b75317744a7595:1:7517"" rel=""nofollow noreferrer"">https://uaxtfom41wgmr3-8888.proxy.runpod.net/lab/extensions/@jupyter-widgets/jupyterlab-manager/static/150.b0e841b75317744a7595.js?v=b0e841b75317744a7595:1:7517</a>)
at f.new_model (<a href=""https://uaxtfom41wgmr3-8888.proxy.runpod.net/lab/extensions/@jupyter-widgets/jupyterlab-manager/static/150.b0e841b75317744a7595.js?v=b0e841b75317744a7595:1:5137"" rel=""nofollow noreferrer"">https://uaxtfom41wgmr3-8888.proxy.runpod.net/lab/extensions/@jupyter-widgets/jupyterlab-manager/static/150.b0e841b75317744a7595.js?v=b0e841b75317744a7595:1:5137</a>)
at f.handle_comm_open (<a href=""https://uaxtfom41wgmr3-8888.proxy.runpod.net/lab/extensions/@jupyter-widgets/jupyterlab-manager/static/150.b0e841b75317744a7595.js?v=b0e841b75317744a7595:1:3894"" rel=""nofollow noreferrer"">https://uaxtfom41wgmr3-8888.proxy.runpod.net/lab/extensions/@jupyter-widgets/jupyterlab-manager/static/150.b0e841b75317744a7595.js?v=b0e841b75317744a7595:1:3894</a>)
at _handleCommOpen (<a href=""https://uaxtfom41wgmr3-8888.proxy.runpod.net/lab/extensions/@jupyter-widgets/jupyterlab-manager/static/134.40eaa5b8e976096d50b2.js?v=40eaa5b8e976096d50b2:1:73393"" rel=""nofollow noreferrer"">https://uaxtfom41wgmr3-8888.proxy.runpod.net/lab/extensions/@jupyter-widgets/jupyterlab-manager/static/134.40eaa5b8e976096d50b2.js?v=40eaa5b8e976096d50b2:1:73393</a>)
at b._handleCommOpen (<a href=""https://uaxtfom41wgmr3-8888.proxy.runpod.net/static/lab/jlab_core.4566032f8b9a1bbebc97.js?v=4566032f8b9a1bbebc97:2:1001335"" rel=""nofollow noreferrer"">https://uaxtfom41wgmr3-8888.proxy.runpod.net/static/lab/jlab_core.4566032f8b9a1bbebc97.js?v=4566032f8b9a1bbebc97:2:1001335</a>)
at async b._handleMessage (<a href=""https://uaxtfom41wgmr3-8888.proxy.runpod.net/static/lab/jlab_core.4566032f8b9a1bbebc97.js?v=4566032f8b9a1bbebc97:2:1003325"" rel=""nofollow noreferrer"">https://uaxtfom41wgmr3-8888.proxy.runpod.net/static/lab/jlab_core.4566032f8b9a1bbebc97.js?v=4566032f8b9a1bbebc97:2:1003325</a>)</p>
</blockquote>
","huggingface"
"74948551","IndexError: index out of range in self when using summarization, hugging face","2022-12-29 07:55:44","75046464","0","2187","<python><huggingface-transformers><huggingface>","<p>I'm getting an error when attempting to run this code:</p>
<pre><code>import nltk
nltk.download('punkt')
from youtube_transcript_api import YouTubeTranscriptApi

video_id = 'wK4XmXJ299k'

transcript = YouTubeTranscriptApi.get_transcript(video_id)

corpus = ' '.join([line['text'] for line in transcript])

from transformers import pipeline
mysummarization = pipeline(&quot;summarization&quot;)
mysummary = mysummarization(corpus)
mysummary[0]['summary_text']
</code></pre>
<p>The code gets a transcript from a YouTube video and attempts to summarize with the Hugging Face Transformers model.  The error is <code>IndexError: index out of range in self.</code></p>
<p>I am also seeing a <code>No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6). Using a pipeline without specifying a model name and revision in production is not recommended. Token indices sequence length is longer than the specified maximum sequence length for this model (11628 &gt; 1024). Running this sequence through the model will result in indexing errors</code> message as well.</p>
<p>How do I fix this?</p>
","huggingface"
"74939384","Parallelising Transformer model inference due to large batch sizes","2022-12-28 10:58:31","","1","478","<parallel-processing><pytorch><huggingface-transformers><transformer-model><huggingface>","<p>I have trained a text classification model using the HuggingFace library. The task is to classify bank transaction descriptions into one of 16 categories.</p>
<p>During training, my batch size was 32. However, now at inference time, all transactions for a single bank account must be classified at a time. Some accounts have thousands of transactions, meaning at inference, the model is recieiving batches much larger than it did during training (1000s as opposed to 32). This is making inference much slower than we would like (~15s per 1000 transactions). To make matters worse, right now I cannot deploy the service on a GPU instance due to cost constraints, so the model is only running on CPU(s).</p>
<p>Are there any options for improving inference time? Is it possible to split each bank accounts' transactions into smaller batches and pass each batch to copies of the model in parallel?</p>
","huggingface"
"74869109","When using OPT-2.7B or any other natural language model, is there a way to trick it into having a conversation/ give it a pre prompt in the code","2022-12-20 21:30:17","","1","279","<neural-network><huggingface-transformers><language-model><huggingface><gpt-2>","<p>Using this code, or a variant of, is there anything that can be added to &quot;trick&quot; opt into conversing as another user in a style more similar to a chatbot. As of now it will either start something more similar to an article or have a conversation with itself for a few lines as seen below.</p>
<pre><code>    val = input(&quot;Enter your value: &quot;)
    input_ids = tokenizer((val), return_tensors='pt').input_ids

    output = model.generate(input_ids, min_length=1, max_length=1024, penalty_alpha=0.6, top_k=6)
    print(&quot;Output:\n&quot; + 100 * '-')
    print(tokenizer.decode(output[0], skip_special_tokens=True))
    print(&quot;&quot; + 100 * '-')
    with open(&quot;OutputText.txt&quot;, &quot;w&quot;) as f:
        f.write(tokenizer.decode(output[0], skip_special_tokens=True))


</code></pre>
<p>Here's an example of the current output:</p>
<p>*User Input:
Hello There.</p>
<p>Model Output:
Hello there. I have an egg that matches your TSV. Would you mind hatching it for me?
Sure, I'll add you now. Let me know when you're online.
Sorry for the late reply. I'm online now for the next few hours. Just send me a trade request whenever you're ready.
No probs, I'm in the middle of a battle at the moment, but I'll get you as soon as I'm done.
Thank you very much for the hatch. Have a nice day :D
*</p>
<p>I've attempted to add a prompt to the start and it hasn't made a difference.</p>
","huggingface"
"74857932","AttributeError: module 'dill._dill' has no attribute 'log'","2022-12-20 02:24:25","74861994","2","2736","<python><nlp><pyarrow><dill><huggingface>","<p>I am using a python nlp module to train a dataset and ran into the following error:</p>
<pre><code>File &quot;/usr/local/lib/python3.9/site-packages/nlp/utils/py_utils.py&quot;, line 297, in save_code
    dill._dill.log.info(&quot;Co: %s&quot; % obj)
</code></pre>
<p>AttributeError: module 'dill._dill' has no attribute 'log'</p>
<p>I noticed similar posts where no attribute 'extend' and no attribute 'stack' where encountered and wondering if this was a similar case.</p>
<p>I have tried running this:</p>
<p>pip install dill --upgrade</p>
<p>yet this made no difference.  I have also tried dir(dill) from the python command prompt and did not see 'log' listed.</p>
<p>I am currently using python 3.9.15 on my mac.  According to pip I am on version 0.3.6 of dill, nlp 0.4.0 and pyarrow 10.0.1</p>
<p>The complete stack trace is here:</p>
<pre><code> File &quot;/usr/local/lib/python3.9/site-packages/nlp/arrow_dataset.py&quot;, line 902, in map
    cache_file_name = self._get_cache_file_path(function, cache_kwargs)
  File &quot;/usr/local/lib/python3.9/site-packages/nlp/arrow_dataset.py&quot;, line 756, in _get_cache_file_path
    function_bytes = dumps(function)
  File &quot;/usr/local/lib/python3.9/site-packages/nlp/utils/py_utils.py&quot;, line 278, in dumps
    dump(obj, file)
  File &quot;/usr/local/lib/python3.9/site-packages/nlp/utils/py_utils.py&quot;, line 271, in dump
    Pickler(file).dump(obj)
  File &quot;/usr/local/lib/python3.9/site-packages/dill/_dill.py&quot;, line 394, in dump
    StockPickler.dump(self, obj)
  File &quot;/usr/local/Cellar/python@3.9/3.9.15/Frameworks/Python.framework/Versions/3.9/lib/python3.9/pickle.py&quot;, line 487, in dump
    self.save(obj)
  File &quot;/usr/local/lib/python3.9/site-packages/dill/_dill.py&quot;, line 388, in save
    StockPickler.save(self, obj, save_persistent_id)
  File &quot;/usr/local/Cellar/python@3.9/3.9.15/Frameworks/Python.framework/Versions/3.9/lib/python3.9/pickle.py&quot;, line 560, in save
    f(self, obj)  # Call unbound method with explicit self
  File &quot;/usr/local/lib/python3.9/site-packages/dill/_dill.py&quot;, line 1824, in save_function
    _save_with_postproc(pickler, (_create_function, (
  File &quot;/usr/local/lib/python3.9/site-packages/dill/_dill.py&quot;, line 1070, in _save_with_postproc
    pickler.save_reduce(*reduction, obj=obj)
  File &quot;/usr/local/Cellar/python@3.9/3.9.15/Frameworks/Python.framework/Versions/3.9/lib/python3.9/pickle.py&quot;, line 692, in save_reduce
    save(args)
  File &quot;/usr/local/lib/python3.9/site-packages/dill/_dill.py&quot;, line 388, in save
    StockPickler.save(self, obj, save_persistent_id)
  File &quot;/usr/local/Cellar/python@3.9/3.9.15/Frameworks/Python.framework/Versions/3.9/lib/python3.9/pickle.py&quot;, line 560, in save
    f(self, obj)  # Call unbound method with explicit self
  File &quot;/usr/local/Cellar/python@3.9/3.9.15/Frameworks/Python.framework/Versions/3.9/lib/python3.9/pickle.py&quot;, line 901, in save_tuple
    save(element)
  File &quot;/usr/local/lib/python3.9/site-packages/dill/_dill.py&quot;, line 388, in save
    StockPickler.save(self, obj, save_persistent_id)
  File &quot;/usr/local/Cellar/python@3.9/3.9.15/Frameworks/Python.framework/Versions/3.9/lib/python3.9/pickle.py&quot;, line 560, in save
    f(self, obj)  # Call unbound method with explicit self
  File &quot;/usr/local/lib/python3.9/site-packages/nlp/utils/py_utils.py&quot;, line 297, in save_code
    dill._dill.log.info(&quot;Co: %s&quot; % obj) 
AttributeError: module 'dill._dill' has no attribute 'log'
</code></pre>
","huggingface"
"74857405","How to use diffusers with custom ckpt file","2022-12-20 00:30:15","75303957","4","4275","<python><huggingface><stable-diffusion>","<p>Currently I have the current code which runs a prompt on a model which it downloads from huggingface.</p>
<pre class=""lang-py prettyprint-override""><code>from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler

model_id = &quot;stabilityai/stable-diffusion-2&quot;

# Use the Euler scheduler here instead
scheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=&quot;scheduler&quot;)
pipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler)
pipe = pipe.to(&quot;mps&quot;)
pipe.enable_attention_slicing()


prompt = &quot;a photo of an astronaut riding a horse on mars&quot;
pipe(prompt).images[0]
</code></pre>
<p>I wanted to know how can I feed a custom ckpt file to this script instead of it downloading it from stabilityAi repo?</p>
","huggingface"
"74836777","Matplotlib in huggingfaces space cases Permission Denied Error","2022-12-17 19:13:15","74966347","1","840","<python><docker><matplotlib><huggingface>","<p>I am trying to setup huggingface spaces. Here is my dockerfile</p>
<pre><code>FROM python:3.9

WORKDIR /code

COPY ./requirements.txt /code/requirements.txt
RUN pip install --no-cache-dir --prefer-binary -r /code/requirements.txt

RUN pip install --user matplotlib
COPY . .

CMD [&quot;uvicorn&quot;, &quot;app.main:app&quot;, &quot;--host&quot;, &quot;0.0.0.0&quot;, &quot;--port&quot;, &quot;7860&quot;]
</code></pre>
<p>When I try to build it and start it it gives me the following error:</p>
<pre><code>--&gt;

===== Application Startup =====

Fetching model from: https://huggingface.co/facebook/wav2vec2-large-960h-lv60-self
Traceback (most recent call last):
  File &quot;/usr/local/bin/uvicorn&quot;, line 8, in 
    sys.exit(main())
  File &quot;/usr/local/lib/python3.9/site-packages/click/core.py&quot;, line 1130, in __call__
    return self.main(*args, **kwargs)
  ......
  File &quot;/code/./app.py&quot;, line 3, in 
    gr.Interface.load(&quot;models/facebook/wav2vec2-large-960h-lv60-self&quot;).launch()
  File &quot;/usr/local/lib/python3.9/site-packages/gradio/interface.py&quot;, line 109, in load
    return super().load(name=name, src=src, api_key=api_key, alias=alias, **kwargs)
  File &quot;/usr/local/lib/python3.9/site-packages/gradio/blocks.py&quot;, line 1154, in load
    return external.load_blocks_from_repo(name, src, api_key, alias, **kwargs)
  File &quot;/usr/local/lib/python3.9/site-packages/gradio/external.py&quot;, line 58, in load_blocks_from_repo
    blocks: gradio.Blocks = factory_methods[src](name, api_key, alias, **kwargs)
  File &quot;/usr/local/lib/python3.9/site-packages/gradio/external.py&quot;, line 311, in from_model
    interface = gradio.Interface(**kwargs)
  File &quot;/usr/local/lib/python3.9/site-packages/gradio/interface.py&quot;, line 424, in __init__
    self.flagging_callback.setup(
  File &quot;/usr/local/lib/python3.9/site-packages/gradio/flagging.py&quot;, line 187, in setup
    os.makedirs(flagging_dir, exist_ok=True)
  File &quot;/usr/local/lib/python3.9/os.py&quot;, line 225, in makedirs
    mkdir(name, mode)
PermissionError: [Errno 13] Permission denied: 'flagged'
</code></pre>
<p>I tried creating folder giving it permission by adding <code>RUN chown</code> statement but it does not seem to work, how can I work around it?</p>
","huggingface"
"74744648","Reducing Latency for GPT-J","2022-12-09 14:45:10","","2","597","<huggingface-transformers><huggingface-tokenizers><huggingface>","<p>I'm using GPT-J locally on a Nvidia RTX 3090 GPU. Currently, I'm using the model in the following way:</p>
<pre><code>config = transformers.GPTJConfig.from_pretrained(&quot;EleutherAI/gpt-j-6B&quot;)
tokenizer = transformers.AutoTokenizer.from_pretrained(&quot;EleutherAI/gpt-j-6B&quot;, pad_token='&lt;|endoftext|&gt;', eos_token='&lt;|endoftext|&gt;', truncation_side='left')
model = GPTJForCausalLM.from_pretrained(
     &quot;EleutherAI/gpt-j-6B&quot;,
      revision=&quot;float16&quot;,
      torch_dtype=torch.float16,
      low_cpu_mem_usage=True,
      use_cache=True,
      gradient_checkpointing=True,
 )
model.to('cuda')
prompt = self.tokenizer(text, return_tensors='pt', truncation=True, max_length=2048)
prompt = {key: value.to('cuda') for key, value in prompt.items()}
out = model.generate(**prompt,
     n=1,
     min_length=16,
     max_new_tokens=75,
     do_sample=True,
     top_k=35,
     top_p=0.9,
     batch_size=512,
     temperature=0.75,
     no_repeat_ngram_size=4,
     clean_up_tokenization_spaces=True,
     use_cache=True,
     pad_token_id=tokenizer.eos_token_id
 )
res = tokenizer.decode(out[0])
</code></pre>
<p>As input to the model I'm using 2048 tokens and I produce 75 tokens as output. The latency is around 4-5 seconds. In the following <a href=""https://huggingface.co/blog/accelerated-inference"" rel=""nofollow noreferrer"">blog post</a>, I've read that using <code>pipelines</code> latency can be improved and that tokenization can be a bottleneck.</p>
<p>Can the tokenization be improved for my code and would using a pipeline reduce the latency? Are there any other things I can do to reduce the latency?</p>
","huggingface"
"74740685","Mt5 language translation - Language in parameters","2022-12-09 08:45:58","","1","225","<nlp><huggingface-transformers><transformer-model><machine-translation><huggingface>","<p>I'm trying to train a Arabic to English language translation model, and want to know if there is any option where I can mention the input and target language in the code.</p>
<p>I'm using the below code, where df1 is the dataframe with columns (prefix,input_text,target_text)</p>
<pre><code>from simpletransformers.t5 import T5Model, T5Args
model = T5Model(&quot;mt5&quot;, &quot;google/mt5-small&quot;, args=model_args, use_cuda=False)
model.train_model(df1,eval_data=df2)
results = model.eval_model(df2, verbose=True)
</code></pre>
","huggingface"
"74729476","Training an Hugginface model without n_epochs","2022-12-08 11:18:58","","1","27","<nlp><huggingface-transformers><bert-language-model><huggingface-tokenizers><huggingface>","<p>I would like to train from scratch a   <code>RobertaForMaskedLM</code> in Hugginface.
However I would like to not specify any stopping time, but to stop only when there is no more improvement in the training. There is a way to do that? I know that the n_epochs in the  <code>Trainer </code>  is default 3.</p>
<p>Thanks</p>
","huggingface"
"74727003","Need clarity on ""padding"" parameter in Bert Tokenizer","2022-12-08 07:45:19","","0","1937","<huggingface-transformers><bert-language-model><huggingface-tokenizers><huggingface>","<p>I have been fine-tuning a <strong>BERT</strong> model for sentence classification. In training, while tokenization I had passed these parameters <code>padding=&quot;max_length&quot;, truncation=True, max_length=150</code> but while inferencing it is still predicting even if <code>padding=&quot;max_length&quot;</code> parameter is not being passed.</p>
<p>Surprisingly, predictions are the same in both cases when <code>padding=&quot;max_length&quot;</code> is passed or not but if <code>padding=&quot;max_length&quot;</code> is not being passed, inferencing is much faster.</p>
<p>So, I need some clarity on the parameter &quot;padding&quot; in Bert Tokenizer. Can someone help me to understand how best is able to predict even without the padding since the length of the sentences will differ and does it have any negative consequences If <code>padding=&quot;max_length&quot;</code> is not passed while inferencing? Any help would be highly appreciated.</p>
<p>Thanks</p>
","huggingface"
"74711983","Huggingface model inference issue","2022-12-07 05:05:28","","2","121","<bert-language-model><huggingface>","<p>I'm trying to use my pre-trained huggingface model to predict.</p>
<pre><code>    outputs = model(
        ids,
        mask,
        token_type_ids
    )
    outputs = torch.sigmoid(outputs.last_hidden_state).cpu().detach().numpy()
    return outputs[0][0]
</code></pre>
<p>The I got is</p>
<pre><code>[[[0.5144298  0.68467325 0.4045368  ... 0.5698948  0.6843927  0.230076  ]
  [0.526383   0.6108195  0.46920577 ... 0.6635995  0.70778817 0.22947823]
  [0.47112644 0.6557672  0.49308282 ... 0.61219037 0.5811446  0.22059086]
  ...
  [0.46904904 0.66370267 0.4091996  ... 0.5381582  0.70973885 0.2500361 ]
  [0.47025025 0.6625398  0.40454543 ... 0.5423772  0.71071064 0.24768841]
  [0.47398427 0.658539   0.40038437 ... 0.53121835 0.7094869  0.2417065 ]]]
</code></pre>
<p>What I want is</p>
<pre><code>[{'label': 'POSITIVE', 'score': 0.9998743534088135},
 {'label': 'NEGATIVE', 'score': 0.9996669292449951}]
</code></pre>
<p>Thanks ahead!!!</p>
","huggingface"
"74708571","Model not calculating loss during training returning ValueError (Huggingface/BERT)","2022-12-06 20:18:49","","4","4332","<nlp><huggingface-transformers><huggingface>","<p>I'm unable to properly pass my encoded data (with hidden states) through Trainer via Huggingface. Below is the call to Trainer with arguments and the full traceback. I'm not really sure where to begin with this error as I believe I've satisfied all requirements to pass the encoded data forward unless the inputs passed should include the labels.</p>
<pre><code>from sklearn.metrics import accuracy_score, f1_score

def compute_metrics(pred):
    labels = pred.label_ids
    pred = pred.predictions.argmax(-1)
    f1 = f1_score(labels, pred, average=&quot;weighted&quot;)
    acc = accuracy_score(labels, preds)
    return {&quot;accuracy&quot;: acc, &quot;f1&quot;: f1}
</code></pre>
<pre><code>from transformers import Trainer, TrainingArguments

batch_size = 10
logging_steps = len(transcripts_encoded[&quot;train&quot;]) // batch_size
model_name = f&quot;{model_checkpoint}-finetuned-transcripts&quot;
training_args = TrainingArguments(output_dir=model_name,
                                 num_train_epochs=2,
                                 learning_rate=2e-5,
                                 per_device_train_batch_size=batch_size,
                                 per_device_eval_batch_size=batch_size,
                                 weight_decay=0.01,
                                 evaluation_strategy=&quot;epoch&quot;,
                                 disable_tqdm=False,
                                 logging_steps=logging_steps,
                                 push_to_hub=False,
                                 log_level=&quot;error&quot;)

from transformers import Trainer

trainer = Trainer(model=model, args=training_args,
                 compute_metrics=compute_metrics,
                 train_dataset=transcripts_encoded[&quot;train&quot;],
                 eval_dataset=transcripts_encoded[&quot;valid&quot;],
                 tokenizer=tokenizer)

trainer.train();
</code></pre>
<h2><code>Here is the full traceback:</code></h2>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-124-76d295da3120&gt; in &lt;module&gt;
     24                  tokenizer=tokenizer)
     25 
---&gt; 26 trainer.train();

/opt/conda/lib/python3.7/site-packages/transformers/trainer.py in train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)
   1503             resume_from_checkpoint=resume_from_checkpoint,
   1504             trial=trial,
-&gt; 1505             ignore_keys_for_eval=ignore_keys_for_eval,
   1506         )
   1507 

/opt/conda/lib/python3.7/site-packages/transformers/trainer.py in _inner_training_loop(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)
   1747                         tr_loss_step = self.training_step(model, inputs)
   1748                 else:
-&gt; 1749                     tr_loss_step = self.training_step(model, inputs)
   1750 
   1751                 if (

/opt/conda/lib/python3.7/site-packages/transformers/trainer.py in training_step(self, model, inputs)
   2506 
   2507         with self.compute_loss_context_manager():
-&gt; 2508             loss = self.compute_loss(model, inputs)
   2509 
   2510         if self.args.n_gpu &gt; 1:

/opt/conda/lib/python3.7/site-packages/transformers/trainer.py in compute_loss(self, model, inputs, return_outputs)
   2552             if isinstance(outputs, dict) and &quot;loss&quot; not in outputs:
   2553                 raise ValueError(
-&gt; 2554                     &quot;The model did not return a loss from the inputs, only the following keys: &quot;
   2555                     f&quot;{','.join(outputs.keys())}. For reference, the inputs it received are {','.join(inputs.keys())}.&quot;
   2556                 )

ValueError: The model did not return a loss from the inputs, only the following keys: logits. For reference, the inputs it received are input_ids,attention_mask.
</code></pre>
<p>I was expecting to for it to the training details (f1, loss, accuracy etc). My assumption is that my encoded data with the hidden states is not properly structured for the model to train per the arguments set.</p>
<p><strong>UPDATED MODEL CODE:
here's where I'm loading and splitting</strong></p>
<pre><code>category_data = load_dataset(&quot;csv&quot;, data_files=&quot;testdatafinal.csv&quot;)
category_data = category_data.remove_columns([&quot;someid&quot;, &quot;someid&quot;, &quot;somedimension&quot;])
category_data = category_data['train']
train_testvalid = category_data.train_test_split(test_size=0.3)
test_valid = train_testvalid['test'].train_test_split(test_size=0.5)
from datasets.dataset_dict import DatasetDict
cd = DatasetDict({
    'train': train_testvalid['train'],
    'test': test_valid['test'],
    'valid': test_valid['train']})
print(cd)

DatasetDict({
    train: Dataset({
        features: ['Transcript', 'Primary Label'],
        num_rows: 646
    })
    test: Dataset({
        features: ['Transcript', 'Primary Label'],
        num_rows: 139
    })
    valid: Dataset({
        features: ['Transcript', 'Primary Label'],
        num_rows: 139
    })
})
</code></pre>
<p><strong>Here's where I'm grabbing the model checkpoint</strong></p>
<pre><code>model_checkpoint = 'distilbert-base-uncased'
device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
model = AutoModel.from_pretrained(model_checkpoint).to(device)
</code></pre>
<p><strong>Here's where I'm mapping the encoded text</strong></p>
<pre><code>transcripts_encoded_one = transcripts_encoded.set_format(&quot;torch&quot;,
                              columns=[&quot;input_ids&quot;, &quot;attention_mask&quot;, &quot;Primary Label&quot;])
</code></pre>
<p><strong>Here's where i'm extracting hidden states and then mapping as well</strong></p>
<pre><code>def extract_hidden_states(batch):
    #Place model inputs on the GPU/CPU
    inputs = {k:v.to(device) for k, v in batch.items()
              if k in tokenizer.model_input_names}
    #Extract last hidden states
    with torch.no_grad():
        last_hidden_state = model(**inputs).last_hidden_state
    # Return vecot for [CLS] Token
    return {&quot;hidden_state&quot;: last_hidden_state[:,0].cpu().numpy()}

transcripts_hidden = transcripts_encoded.map(extract_hidden_states, batched=True)
</code></pre>
<p><strong>Calling AutoModel</strong></p>
<pre><code>from transformers import AutoModelForSequenceClassification

num_labels = 10
model =(AutoModelForSequenceClassification
       .from_pretrained(model_checkpoint, num_labels=num_labels)
       .to(device))
</code></pre>
<p><strong>Accuracy Metrics</strong></p>
<pre><code>from sklearn.metrics import accuracy_score, f1_score

def compute_metrics(pred):
    labels = pred.label_ids
    pred = pred.predictions.argmax(-1)
    f1 = f1_score(labels, pred, average=&quot;weighted&quot;)
    acc = accuracy_score(labels, preds)
    return {&quot;accuracy&quot;: acc, &quot;f1&quot;: f1}
</code></pre>
<p><strong>Trainer</strong></p>
<pre><code>from transformers import Trainer, TrainingArguments

batch_size = 10
logging_steps = len(transcripts_encoded_one[&quot;train&quot;]) // batch_size
model_name = f&quot;{model_checkpoint}-finetuned-transcripts&quot;
training_args = TrainingArguments(output_dir=model_name,
                                 num_train_epochs=2,
                                 learning_rate=2e-5,
                                 per_device_train_batch_size=batch_size,
                                 per_device_eval_batch_size=batch_size,
                                 weight_decay=0.01,
                                 evaluation_strategy=&quot;epoch&quot;,
                                 disable_tqdm=False,
                                 logging_steps=logging_steps,
                                 push_to_hub=False,
                                 log_level=&quot;error&quot;)

from transformers import Trainer

trainer = Trainer(model=model, args=training_args,
                 compute_metrics=compute_metrics,
                 train_dataset=transcripts_encoded_one[&quot;train&quot;],
                 eval_dataset=transcripts_encoded_one[&quot;valid&quot;],
                 tokenizer=tokenizer)

trainer.train();
</code></pre>
<p><strong>I've tried passing &quot;transcripts_encoded(without hidden states) and &quot;transcripts_hidden (with hidden states) as the train and validation splits and both produce the same error</strong></p>
<pre><code>trainer.train_dataset[0]

{'Primary Label': 'cancel',
 'input_ids': tensor([  101,  2047,  3446,  2003,  2205,  6450,  2005,  1996,  2051,  1045,
          2064,  5247,  3752,  4790,  1012,  2009,  2001,  2026,  5165,  2000,
          6509,  2017,  2651,   999,  4067,  2017,  2005,  3967,  2075,  1996,
          2047,  2259,  2335,   999,  2031,  1037,  6919,  2717,  1997,  1996,
          2154,   999,  2994,  3647,  1998,  7965,   999,  2065,  2045,  2003,
          2505,  2842,  2057,  2089,  2022,  2583,  2000,  6509,  2017,  2007,
          3531,  2514,  2489,  2000,  3967,  2149,  2153,  1012,  1045,  2001,
          2074,  2667,  2000, 17542,  2026, 15002,  1012,  2038,  2009,  2042,
         13261,  1029,  7632,  1010,  2045,   999,  1045,  3246,  2017,  1005,
          2128,  2725,  2092,  2651,  1012,  4067,  2017,  2005,  3967,  2075,
           102]),
 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1])}
</code></pre>
","huggingface"
"74697236","Multiple threads of Hugging face Stable diffusion Inpainting pipeline slows down the inference on same GPU","2022-12-06 04:16:58","74699707","0","696","<pytorch><huggingface><stable-diffusion>","<p>I am using <code>Stable diffusion inpainting pipeline</code> to generate some inference results on a <code>A100 (40 GB)</code> GPU. For a <code>512X512</code> image it is taking approx 3 s per image and takes about 5 GB of space on the GPU.</p>
<p>In order to have faster inference, I am trying to run 2 threads (2 inference scripts). However, as soon as I start them simultaneously. The inference time decreases to ~6 sec per thread with an effective time of ~3 s per image.</p>
<p>I am unable to understand why this is so. I still have a lot of space available (about 35 GB) on GPU and quite a big CPU ram of 32 GB.</p>
<p>Can someone help me in this regard?</p>
","huggingface"
"74692176","Huggingface: Fine-tuning (not enough values to unpack (expected 2, got 1))","2022-12-05 17:35:01","","4","303","<nlp><huggingface><fine-tuning>","<p>I'm trying to fine-tune <a href=""https://huggingface.co/erfan226/persian-t5-paraphraser"" rel=""nofollow noreferrer"">erfan226/persian-t5-paraphraser</a> paraphrase generator model for Persian sentences. I used the Persian dataset of <a href=""https://huggingface.co/datasets/tapaco"" rel=""nofollow noreferrer"">tapaco</a> and reformatted it to match the <a href=""https://huggingface.co/datasets/glue/viewer/mrpc"" rel=""nofollow noreferrer"">glue (mrpc) dataset</a> which is used in the <a href=""https://huggingface.co/docs/transformers/training"" rel=""nofollow noreferrer"">fine-tuning documentation</a>. I have uploaded my dataset in <a href=""https://huggingface.co/datasets/alighasemi/farsi_paraphrase_detection"" rel=""nofollow noreferrer"">alighasemi/farsi_paraphrase_detection</a>.</p>
<p>I followed every step of the Trainer video (Tokenization, ComputerMetrics, TrainingArgs, ...). However, when I run trainer.train() I get the following error:</p>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-28-3435b262f1ae&gt; in &lt;module&gt;
----&gt; 1 trainer.train()

7 frames
/usr/local/lib/python3.8/dist-packages/transformers/models/t5/modeling_t5.py in forward(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)
    941             inputs_embeds = self.embed_tokens(input_ids)
    942 
--&gt; 943         batch_size, seq_length = input_shape
    944 
    945         # required mask seq length can be calculated via length of past

ValueError: not enough values to unpack (expected 2, got 1)
</code></pre>
<p>Here's my code:</p>
<pre><code>dataset = load_dataset(&quot;alighasemi/farsi_paraphrase_detection&quot;)
</code></pre>
<pre><code>tokenizer = AutoTokenizer.from_pretrained(&quot;erfan226/persian-t5-paraphraser&quot;)
</code></pre>
<pre><code>def tokenize_function(examples):
    return tokenizer(examples[&quot;sentence1&quot;], examples[&quot;sentence2&quot;], truncation=True)
</code></pre>
<pre><code>tokenized_dataset = dataset.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer)
</code></pre>
<pre><code>model = AutoModelForSeq2SeqLM.from_pretrained(&quot;erfan226/persian-t5-paraphraser&quot;, num_labels=2)
</code></pre>
<p>I could not use the AutoModelForSequenceClassification to load the model since I would get this error:</p>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-29-5650b38a14f3&gt; in &lt;module&gt;
----&gt; 1 model = AutoModelForSequenceClassification.from_pretrained(&quot;erfan226/persian-t5-paraphraser&quot;, num_labels=2)

/usr/local/lib/python3.8/dist-packages/transformers/models/auto/auto_factory.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    464                 pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs
    465             )
--&gt; 466         raise ValueError(
    467             f&quot;Unrecognized configuration class {config.__class__} for this kind of AutoModel: {cls.__name__}.\n&quot;
    468             f&quot;Model type should be one of {', '.join(c.__name__ for c in cls._model_mapping.keys())}.&quot;

ValueError: Unrecognized configuration class &lt;class 'transformers.models.t5.configuration_t5.T5Config'&gt; for this kind of AutoModel: AutoModelForSequenceClassification.
Model type should be one of AlbertConfig, BartConfig, BertConfig, BigBirdConfig, BigBirdPegasusConfig, BloomConfig, CamembertConfig, CanineConfig, ConvBertConfig, CTRLConfig, Data2VecTextConfig, DebertaConfig, DebertaV2Config, DistilBertConfig, ElectraConfig, ErnieConfig, EsmConfig, FlaubertConfig, FNetConfig, FunnelConfig, GPT2Config, GPTNeoConfig, GPTJConfig, IBertConfig, LayoutLMConfig, LayoutLMv2Config, LayoutLMv3Config, LEDConfig, LiltConfig, LongformerConfig, LukeConfig, MarkupLMConfig, MBartConfig, MegatronBertConfig, MobileBertConfig, MPNetConfig, MvpConfig, NezhaConfig, NystromformerConfig, OpenAIGPTConfig, OPTConfig, PerceiverConfig, PLBartConfig, QDQBertConfig, ReformerConfig, RemBertConfig, RobertaConfig, RoCBertConfig, RoFormerConfig, SqueezeBertConfig, TapasConfig, TransfoXLConfig, XLMConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, YosoConfig.
</code></pre>
<pre><code>training_args = TrainingArguments(
    &quot;farsi_paraphraser&quot;,
    num_train_epochs=5,
    evaluation_strategy=&quot;epoch&quot;
)
</code></pre>
<pre><code>trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenized_dataset[&quot;train&quot;],
    eval_dataset=tokenized_dataset[&quot;validation&quot;],
    data_collator=data_collator,
    tokenizer=tokenizer,
)
</code></pre>
<pre><code>trainer.train()
</code></pre>
<p>I would be happy if you could help me</p>
","huggingface"
"74663365","Dealing with infs in Seq2Seq Trainer","2022-12-03 01:47:27","","2","378","<python-3.x><nlp><nan><huggingface-transformers><huggingface>","<p>I am trying to fine tune a hugging face model onto a Shell Code dataset (<a href=""https://huggingface.co/datasets/SoLID/shellcode_i_a32"" rel=""nofollow noreferrer"">https://huggingface.co/datasets/SoLID/shellcode_i_a32</a>)</p>
<p>The training code is a basic hugging face trainer method but we keep running into nan/inf issues</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import PreTrainedTokenizerFast

tokenizer = PreTrainedTokenizerFast(tokenizer_file=&quot;tkn1.json&quot;, padding_side=&quot;right&quot;) 
special_tokens={'pad_token': &quot;[PAD]&quot;}

tokenizer.add_special_tokens(special_tokens)

#  token_wrap = PreTrainedTokenizer()
data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)

training_args = Seq2SeqTrainingArguments(
    output_dir=&quot;./results&quot;,
    evaluation_strategy=&quot;epoch&quot;,
    lr_scheduler_type = &quot;cosine&quot;,
    weight_decay=0.01,
    save_total_limit=3,
    per_device_train_batch_size=128,
    num_train_epochs=5,
    warmup_ratio=0.06,
    learning_rate=1.0e-04,
    # fp16=True,
    debug=[&quot;underflow_overflow&quot;]
)

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets[&quot;test&quot;],
    eval_dataset=tokenized_datasets[&quot;test&quot;],
    tokenizer=tokenizer,
    data_collator=data_collator,
)
# trainer.train()
# print(tokenizer.)
trainer.train()
# eval_loss = trainer.evaluate()
# print(f&quot;&gt;&gt;&gt; Perplexity: {math.exp(eval_loss['eval_loss']):.2f}&quot;)
</code></pre>
<p>The outputs look like -</p>
<pre><code>You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.


Detected inf/nan during batch_number=0
Last 1 forward frames:
abs min  abs max  metadata
                  shared Embedding
5.42e-06 2.04e+04 weight
0.00e+00 1.46e+03 input[0]
1.56e-03 2.04e+04 output



---------------------------------------------------------------------------

ValueError                                Traceback (most recent call last)

&lt;ipython-input-120-ff4a54906908&gt; in &lt;module&gt;
     33 # trainer.train()
     34 # print(tokenizer.)
---&gt; 35 trainer.train()
     36 # eval_loss = trainer.evaluate()
     37 # print(f&quot;&gt;&gt;&gt; Perplexity: {math.exp(eval_loss['eval_loss']):.2f}&quot;)

9 frames

/usr/local/lib/python3.8/dist-packages/transformers/debug_utils.py in forward_hook(self, module, input, output)
    278 
    279             # now we can abort, as it's pointless to continue running
--&gt; 280             raise ValueError(
    281                 &quot;DebugUnderflowOverflow: inf/nan detected, aborting as there is no point running further. &quot;
    282                 &quot;Please scroll up above this traceback to see the activation values prior to this event.&quot;

ValueError: DebugUnderflowOverflow: inf/nan detected, aborting as there is no point running further. Please scroll up above this traceback to see the activation values prior to this event.
</code></pre>
<p>The very first layer seems to start throwing inf/nans when we start training and doesn't go much beyond that</p>
<p>We have tried tweaking our training arguments but have hit a brick wall here. Any help appreciated!</p>
","huggingface"
"74631411","Why tflite model output shape is different than the original model converted from T5ForConditionalGeneration?","2022-11-30 16:53:26","75069874","0","595","<tensorflow><huggingface-transformers><huggingface><tflite>","<p><strong>T5ForConditionalGeneration Model to translate English to German</strong></p>
<pre><code>from transformers import T5TokenizerFast, T5ForConditionalGeneration

tokenizer = T5TokenizerFast.from_pretrained(&quot;t5-small&quot;)
model = T5ForConditionalGeneration.from_pretrained(&quot;t5-small&quot;)

input_ids = tokenizer(&quot;translate English to German: the flowers are wonderful.&quot;, return_tensors=&quot;pt&quot;).input_ids
outputs = model.generate(input_ids)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
</code></pre>
<p>Output : Die Blumen sind wunderbar.</p>
<p><strong>Input Shape</strong></p>
<pre><code>input_ids.shape
</code></pre>
<p>Output : torch.Size([1, 11])</p>
<p><strong>Output Shape</strong></p>
<pre><code>outputs.shape
</code></pre>
<p>Output : torch.Size([1, 7])</p>
<p><strong>Save Pretrained model</strong></p>
<pre><code>!mkdir /content/test
model.save_pretrained('/content/test')
</code></pre>
<p><strong>Load TFT5Model model from pretrained</strong></p>
<pre><code>from transformers import TFT5Model
t5model = TFT5Model.from_pretrained('/content/test',from_pt=True)
!mkdir /content/test/t5
t5model.save('/content/test/t5')
</code></pre>
<p><strong>Convert TFT5Model to TFlite</strong></p>
<pre><code>import tensorflow as tf

saved_model_dir = '/content/test/t5'
!mkdir  /content/test/tflite
tflite_model_path = '/content/test/tflite/model.tflite'

# Convert the model
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)

converter.experimental_new_converter = True
converter.experimental_new_quantizer = True
converter.experimental_new_dynamic_range_quantizer = True
converter.allow_custom_ops=True

converter.target_spec.supported_ops = [
  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.
  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.
]
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()
#print(tflite_model)
print(type(tflite_model))


# Save the model
with open(tflite_model_path, 'wb') as f:
    f.write(tflite_model)
</code></pre>
<p><strong>Load The TFLite model</strong></p>
<pre><code>import numpy as np
import tensorflow as tf

tflite_model_path = '/content/test/tflite/model.tflite'
# Load the TFLite model and allocate tensors
interpreter = tf.lite.Interpreter(model_path=tflite_model_path)

interpreter.resize_tensor_input(0, [1,5], strict=True)
interpreter.resize_tensor_input(1, [1,5], strict=True)
interpreter.resize_tensor_input(2, [1,5], strict=True)
interpreter.resize_tensor_input(3, [1,5], strict=True)
interpreter.allocate_tensors()

# Get input and output tensors
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

input_shape = input_details[0]['shape']

#print the output
input_data = np.array(np.random.random_sample((input_shape)), dtype=np.int64)
interpreter.set_tensor(input_details[0]['index'], input_data)
interpreter.invoke()
output_data = interpreter.get_tensor(output_details[0]['index'])
</code></pre>
<p><strong>Get The Output Shape</strong></p>
<pre><code>print(output_data.shape)
</code></pre>
<p>Output : (1, 8, 5, 64)</p>
<p>Expected something like : (1, 7)</p>
<p>Can someone let me know where am I going wrong ?</p>
<p>The output shape of the tflite model is completely different from the T5ForConditionalGeneration model</p>
<p>Output : (1, 8, 5, 64)</p>
<p>Expected something like : (1, 7)</p>
","huggingface"
"74560753","Using .generate function for beam search over predictions in custom model extending TFPreTrainedModel class","2022-11-24 12:25:09","","3","752","<huggingface-transformers><huggingface><simpletransformers><beam-search>","<p>I want to use .generate() functionality of hugging face in my model's predictions.
My model is a custom model inehriting from &quot;TFPreTrainedModel&quot; class and has a custom transformer inheriting from tf.keras.layers followed by few hidden layers and a final dense layer (inherited from tf.keras.layers).</p>
<p>I am not able to use .generate() inspite of adding get_lm_head() function (as given here <a href=""https://huggingface.co/docs/transformers/main_classes/model"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/main_classes/model</a>) and returning my last dense layer in it.
When I call .generate() it throws
<code>TypeError: The current model class (NextCateModel) is not compatible with</code>.generate()<code>, as it doesn't have a language model head.</code></p>
<p>Can anyone suggest on how to use .generate() functionality of huggingface in our custom transformer based models without using the huggingface's list of pre-trained models?</p>
<p>PS: It checks for models among huggingface pretrained ones which are defined in their generation_tf_utils.py</p>
<pre><code>generate_compatible_mappings = [
                TF_MODEL_FOR_CAUSAL_LM_MAPPING,
                TF_MODEL_FOR_VISION_2_SEQ_MAPPING,
                TF_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING,
                TF_MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING,
            ]
</code></pre>
<p>I donot intend to use their pretrained models given in above mappings (one of them is shown below)</p>
<pre><code> TF_MODEL_FOR_CAUSAL_LM_MAPPING=
        (&quot;bert&quot;, &quot;TFBertLMHeadModel&quot;),
        (&quot;camembert&quot;, &quot;TFCamembertForCausalLM&quot;),
        (&quot;ctrl&quot;, &quot;TFCTRLLMHeadModel&quot;),
        (&quot;gpt2&quot;, &quot;TFGPT2LMHeadModel&quot;),
        (&quot;gptj&quot;, &quot;TFGPTJForCausalLM&quot;),
        (&quot;openai-gpt&quot;, &quot;TFOpenAIGPTLMHeadModel&quot;),
        (&quot;opt&quot;, &quot;TFOPTForCausalLM&quot;),
        (&quot;rembert&quot;, &quot;TFRemBertForCausalLM&quot;),
        (&quot;roberta&quot;, &quot;TFRobertaForCausalLM&quot;),
        (&quot;roformer&quot;, &quot;TFRoFormerForCausalLM&quot;),
        (&quot;transfo-xl&quot;, &quot;TFTransfoXLLMHeadModel&quot;),
        (&quot;xglm&quot;, &quot;TFXGLMForCausalLM&quot;),
        (&quot;xlm&quot;, &quot;TFXLMWithLMHeadModel&quot;),
        (&quot;xlnet&quot;, &quot;TFXLNetLMHeadModel&quot;),
</code></pre>
<pre><code>  1340             if generate_compatible_classes:
   1341                 exception_message += f&quot; Please use one of the following classes instead: {generate_compatible_classes}&quot;
-&gt; 1342             raise TypeError(exception_message)
</code></pre>
<pre><code></code></pre>
","huggingface"
"74556349","No module named 'huggingface_hub.snapshot_download'","2022-11-24 06:14:59","74573811","16","15743","<python><huggingface>","<p>When I try to run the <a href=""https://github.com/yangkevin2/emnlp22-re3-story-generation#quick-start-with-notebook"" rel=""noreferrer"">quick start notebook</a> of <a href=""https://github.com/yangkevin2/emnlp22-re3-story-generation"" rel=""noreferrer"">this repo</a>, I get the error <code>ModuleNotFoundError: No module named 'huggingface_hub.snapshot_download'</code>. How can I fix it? I already installed <code>huggingface_hub</code> using pip.</p>
<p>I get the error after compiling the following cell:</p>
<pre><code>!CUDA_VISIBLE_DEVICES=0 python -u ../scripts/main.py --summarizer gpt3_summarizer --controller longformer_classifier longformer_classifier --loader alignment coherence --controller-load-dir emnlp22_re3_data/ckpt/relevance_reranker emnlp22_re3_data/ckpt/coherence_reranker --controller-model-string allenai/longformer-base-4096 allenai/longformer-base-4096 --save-outline-file output/outline0.pkl --save-complete-file output/complete_story0.pkl --log-file output/story0.log
</code></pre>
<p>Here's the entire output:</p>
<pre><code>Traceback (most recent call last):
  File &quot;../scripts/main.py&quot;, line 20, in &lt;module&gt;
    from story_generation.edit_module.entity import *
  File &quot;/home/jovyan/emnlp22-re3-story-generation/story_generation/edit_module/entity.py&quot;, line 20, in &lt;module&gt;
    from story_generation.common.util import *
  File &quot;/home/jovyan/emnlp22-re3-story-generation/story_generation/common/util.py&quot;, line 13, in &lt;module&gt;
    from sentence_transformers import SentenceTransformer
  File &quot;/opt/conda/lib/python3.8/site-packages/sentence_transformers/__init__.py&quot;, line 3, in &lt;module&gt;
    from .datasets import SentencesDataset, ParallelSentencesDataset
  File &quot;/opt/conda/lib/python3.8/site-packages/sentence_transformers/datasets/__init__.py&quot;, line 3, in &lt;module&gt;
    from .ParallelSentencesDataset import ParallelSentencesDataset
  File &quot;/opt/conda/lib/python3.8/site-packages/sentence_transformers/datasets/ParallelSentencesDataset.py&quot;, line 4, in &lt;module&gt;
    from .. import SentenceTransformer
  File &quot;/opt/conda/lib/python3.8/site-packages/sentence_transformers/SentenceTransformer.py&quot;, line 25, in &lt;module&gt;
    from .evaluation import SentenceEvaluator
  File &quot;/opt/conda/lib/python3.8/site-packages/sentence_transformers/evaluation/__init__.py&quot;, line 5, in &lt;module&gt;
    from .InformationRetrievalEvaluator import InformationRetrievalEvaluator
  File &quot;/opt/conda/lib/python3.8/site-packages/sentence_transformers/evaluation/InformationRetrievalEvaluator.py&quot;, line 6, in &lt;module&gt;
    from ..util import cos_sim, dot_score
  File &quot;/opt/conda/lib/python3.8/site-packages/sentence_transformers/util.py&quot;, line 407, in &lt;module&gt;
    from huggingface_hub.snapshot_download import REPO_ID_SEPARATOR
ModuleNotFoundError: No module named 'huggingface_hub.snapshot_download'
</code></pre>
","huggingface"
"74548143","How to use AWS Sagemaker with newer version of Huggingface Estimator?","2022-11-23 14:09:37","","3","1134","<python><docker><pytorch><amazon-sagemaker><huggingface>","<p>When trying to use Huggingface estimator on sagemaker, Run training on Amazon SageMaker e.g.</p>
<pre><code># create the Estimator
huggingface_estimator = HuggingFace(
        entry_point='train.py',
        source_dir='./scripts',
        instance_type='ml.p3.2xlarge',
        instance_count=1,
        role=role,
        transformers_version='4.17',
        pytorch_version='1.10',
        py_version='py38',
        hyperparameters = hyperparameters
)
</code></pre>
<p>When I tried to increase the version to transformers_version='4.24', it throws an error where the maximum version supported is 4.17.</p>
<p><strong>How to use AWS Sagemaker with newer version of Huggingface Estimator?</strong></p>
<p>There's a note on using newer version for inference on <a href=""https://discuss.huggingface.co/t/deploying-open-ais-whisper-on-sagemaker/24761/9"" rel=""nofollow noreferrer"">https://discuss.huggingface.co/t/deploying-open-ais-whisper-on-sagemaker/24761/9</a> but it looks like the way to use it for training with the Huggingface estimator is kind of complicated <a href=""https://discuss.huggingface.co/t/huggingface-pytorch-versions-on-sagemaker/26315/5?u=alvations"" rel=""nofollow noreferrer"">https://discuss.huggingface.co/t/huggingface-pytorch-versions-on-sagemaker/26315/5?u=alvations</a> and it's not confirmed that the complicated steps can work.</p>
","huggingface"
"74517147","AutoTokenizer.from_pretrained('google/byt5-base') giving error: OSError: Can't load config and internet connection broken by 'NewConnectionError('","2022-11-21 10:01:00","","1","294","<nlp><huggingface-transformers><transformer-model><huggingface>","<pre><code>`tokenizer = AutoTokenizer.from_pretrained('google/byt5-base')
</code></pre>
<p>OSError: Can't load config for 'google/byt5-base'. Make sure that:</p>
<ul>
<li><p>'google/byt5-base' is a correct model identifier listed on 'https://huggingface.co/models'</p>
</li>
<li><p>or 'google/byt5-base' is the correct path to a directory containing a config.json file</p>
</li>
</ul>
<p>Edit:</p>
<p>Also was getting error below while upgrading transformer</p>
<pre><code>WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('
</code></pre>
<p>I refereed <a href=""https://huggingface.co/google/byt5-base"" rel=""nofollow noreferrer"">this</a> article and checked correct path in the  model repository as well but no luck</p>
<p>Any help highly appreciated! Thanks.</p>
","huggingface"
"74508539","how to create Flair Huggingface output to dataframe","2022-11-20 13:30:10","","0","578","<tensorflow><huggingface-transformers><huggingface-tokenizers><huggingface><flair>","<p>I am new to huggingface and i working on Flair (NER) module which gives me below output:</p>
<pre><code>from flair.data import Sentence
from flair.models import SequenceTagger

# load tagger
tagger = SequenceTagger.load(&quot;flair/ner-german-large&quot;)

# make example sentence
sentence = Sentence(&quot;George Washington ging nach Washington&quot;)

# predict NER tags
tagger.predict(sentence)

# print sentence
print(sentence)

# print predicted NER spans
print('The following NER tags are found:')
# iterate over entities and print
for entity in sentence.get_spans('ner'):
    print(entity)
</code></pre>
<p>Output</p>
<pre><code>Span [1,2]: &quot;George Washington&quot;   [− Labels: PER (1.0)]
Span [5]: &quot;Washington&quot;   [− Labels: LOC (1.0)]
</code></pre>
<p>How can I covert this output into dataframe with possible columns as 'Token'(NER) and 'Token_Type'('ORG' or 'PER').</p>
<p>The <code>sentence</code> generated is of type <code>data.sentence</code></p>
","huggingface"
"74503607","Text generation AI models generating repeated/duplicate text/sentences. What am I doing incorrectly? Hugging face models - Meta GALACTICA","2022-11-19 20:48:50","","4","1926","<python><nlp><huggingface-transformers><huggingface><gpt-2>","<p>Whole day I have worked with available text generation models</p>
<p>Here you can find list of them : <a href=""https://huggingface.co/models?pipeline_tag=text-generation&amp;sort=downloads"" rel=""nofollow noreferrer"">https://huggingface.co/models?pipeline_tag=text-generation&amp;sort=downloads</a></p>
<p>I want to generate longer text outputs, however, with multiple different models, all I get is repetition.</p>
<p>What am I missing or doing incorrectly?</p>
<p>I will list several of them</p>
<p>Freshly released meta GALACTICA - <a href=""https://huggingface.co/facebook/galactica-1.3b"" rel=""nofollow noreferrer"">https://huggingface.co/facebook/galactica-1.3b</a></p>
<p>The code example</p>
<pre><code>from transformers import AutoTokenizer, OPTForCausalLM

tokenizer = AutoTokenizer.from_pretrained(&quot;facebook/galactica-1.3b&quot;)
model = OPTForCausalLM.from_pretrained(&quot;facebook/galactica-1.3b&quot;, device_map=&quot;auto&quot;)

 
input_text = &quot;The benefits of deadlifting\n\n&quot;
input_ids = tokenizer(input_text, return_tensors=&quot;pt&quot;).input_ids.to(&quot;cuda&quot;)

outputs = model.generate(input_ids,new_doc=False,top_p=0.7, max_length=1000)
print(tokenizer.decode(outputs[0]))
</code></pre>
<p>The generated output</p>
<p><a href=""https://i.sstatic.net/zV7qg.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/zV7qg.png"" alt=""enter image description here"" /></a></p>
<p>Facebook opt - <a href=""https://huggingface.co/facebook/opt-350m"" rel=""nofollow noreferrer"">https://huggingface.co/facebook/opt-350m</a></p>
<p>The tested code</p>
<pre><code>from transformers import GPT2Tokenizer, OPTForCausalLM

model = OPTForCausalLM.from_pretrained(&quot;facebook/opt-350m&quot;)
tokenizer = GPT2Tokenizer.from_pretrained(&quot;facebook/opt-350m&quot;)

prompt = &quot;The benefits of deadlifting can be listed as below:&quot;
inputs = tokenizer(prompt, return_tensors=&quot;pt&quot;)

# Generate
generate_ids = model.generate(inputs.input_ids, max_length=800)
tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
</code></pre>
<p>The generated output</p>
<p><a href=""https://i.sstatic.net/zv2j9.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/zv2j9.png"" alt=""enter image description here"" /></a></p>
","huggingface"
"74502025","ValueError: The model did not return a loss from the inputs, only the following keys: logits,past_key_values","2022-11-19 17:12:03","","2","1870","<python><pytorch><huggingface>","<p>I'm using Pytorch to do huggingface model finetuning with transformers library. I have torch version <code>'1.13.0+cu117'</code> with python 3.7.8 and CUDA 11.8. But with some copying and pastinng of others' code, I'm getting <code>ValueError: The model did not return a loss from the inputs, only the following keys: logits,past_key_values. For reference, the inputs it received are input_ids,token_type_ids,attention_mask.</code>. I've seen <a href=""https://stackoverflow.com/questions/73290491/the-model-did-not-return-a-loss-from-the-inputs-labse-error"">this question</a> but as a text generation task, I cannot find a proper function like GPT2+text generation so I used AutoModel+Casual LM.</p>
<p>The traceback info is like this:</p>
<pre><code>ValueError                                Traceback (most recent call last)
~\AppData\Local\Temp\ipykernel_8240\3982389964.py in &lt;module&gt;
     47 )
     48 
---&gt; 49 trainer.train()

c:\users\fchen\appdata\local\programs\python\python37\lib\site-packages\transformers\trainer.py in train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)
   1503             resume_from_checkpoint=resume_from_checkpoint,
   1504             trial=trial,
-&gt; 1505             ignore_keys_for_eval=ignore_keys_for_eval,
   1506         )
   1507 

c:\users\fchen\appdata\local\programs\python\python37\lib\site-packages\transformers\trainer.py in _inner_training_loop(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)
   1747                         tr_loss_step = self.training_step(model, inputs)
   1748                 else:
-&gt; 1749                     tr_loss_step = self.training_step(model, inputs)
   1750 
   1751                 if (

c:\users\fchen\appdata\local\programs\python\python37\lib\site-packages\transformers\trainer.py in training_step(self, model, inputs)
   2506 
   2507         with self.compute_loss_context_manager():
-&gt; 2508             loss = self.compute_loss(model, inputs)
   2509 
   2510         if self.args.n_gpu &gt; 1:

c:\users\fchen\appdata\local\programs\python\python37\lib\site-packages\transformers\trainer.py in compute_loss(self, model, inputs, return_outputs)
   2552             if isinstance(outputs, dict) and &quot;loss&quot; not in outputs:
   2553                 raise ValueError(
-&gt; 2554                     &quot;The model did not return a loss from the inputs, only the following keys: &quot;
   2555                     f&quot;{','.join(outputs.keys())}. For reference, the inputs it received are {','.join(inputs.keys())}.&quot;
   2556                 )

ValueError: The model did not return a loss from the inputs, only the following keys: logits,past_key_values. For reference, the inputs it received are input_ids,token_type_ids,attention_mask.
</code></pre>
<p>The entire code is like this. The <code>train.csv</code> and <code>test.csv</code> has only lines of sentences in natural language. One sentence per line.</p>
<pre><code>from transformers import TrainingArguments, Trainer
from transformers import AutoTokenizer, AutoModelForCausalLM
from transformers import TrainingArguments
from datasets import load_dataset
import evaluate

MAX_LEN=100

def tokenize_function(examples):
    return tokenizer(examples[&quot;sentences&quot;], padding='max_length', truncation=True,max_length=MAX_LEN)

pretrained = &quot;./models/gpt2-chinese-cluecorpussmall/&quot;
tokenizer = AutoTokenizer.from_pretrained(pretrained)
model = AutoModelForCausalLM.from_pretrained(pretrained)

data_files = {&quot;train&quot;: &quot;train.csv&quot;, &quot;test&quot;: &quot;test.csv&quot;}
dataset = load_dataset(&quot;csv&quot;, data_files=data_files)

tokenized_datasets = dataset.map(tokenize_function, batched=True)

train_dataset = tokenized_datasets[&quot;train&quot;].shuffle(seed=42)
eval_dataset = tokenized_datasets[&quot;test&quot;].shuffle(seed=42)

training_args = TrainingArguments(
    output_dir='./test_trainer',
    num_train_epochs=1,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    learning_rate= 5e-05,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    load_best_model_at_end=True,
    logging_steps=400,         
    save_steps=400,            
    evaluation_strategy=&quot;steps&quot;,
    report_to=None
)

training_args = TrainingArguments(output_dir=&quot;test_trainer&quot;, evaluation_strategy=&quot;epoch&quot;)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset =eval_dataset 
)

trainer.train()
</code></pre>
","huggingface"
"74490128","NLP Huggingface Tokenizer wont let me set pre_tokenizer property","2022-11-18 13:04:19","","1","330","<nlp><huggingface>","<p>I was using Huggingface Transformer's models and classes for some time now. I used it as an &quot;off the shelf&quot; product, following many tutorials all over the internet, to develop very interesting apps like a chatbot and a tweet generator. Very nice framework or library or whatever you may call it this days.</p>
<p>Now I want to dig a little deeper and try to customize some of the steps in what Hugginface calls the Pipeline. The Pipeline is an encapsulation or a bundle of different classes that perform all the different steps needed to generate text, translation, sentiment analysis or any other result you may be looking for in a NLP application.</p>
<p>With this in mind, I started to study the Tokenizer class as the first step in the pipeline. There are several tokenizers available to work with. So I peek one and follow the <a href=""https://huggingface.co/docs/tokenizers/index"" rel=""nofollow noreferrer"">tutorial</a> provided by Huggingface to customize it and eventually train it.</p>
<p>At some point the tutorial explains how to change the pre_tokenizer by setting the property pre_tokenizer to a new instance of some PreTokenizer class of your choice.</p>
<p>here is the snippet:</p>
<pre><code>from tokenizers.pre_tokenizers import Whitespace
from tokenizers import Tokenizer
from tokenizers.models import BPE

tokenizer = Tokenizer(BPE(unk_token=&quot;[UNK]&quot;))
tokenizer.pre_tokenizer = Whitespace()
</code></pre>
<p>I'm coding in Python using JetBrains PyCharm free edition and I get the warning:</p>
<p><strong>Property 'pre_tokenizer' cannot be set</strong></p>
<p><strong>Inspection info: Reports cases when properties are accessed inappropriately:</strong></p>
<p>The property is not set but the script runs fine.</p>
<p>The same happens with <strong>tokenizer.normalizer</strong>. Property 'normalizer' cannot be set.</p>
<p>I couldn't find any comment about that.
I'm probably doing something wrong.
Can anybody help me with this problem ?
Thanks!</p>
","huggingface"
"74484424","How to display image data returned from dreambooth / stable-diffusion model?","2022-11-18 02:55:49","","0","894","<javascript><python><huggingface-transformers><huggingface><stable-diffusion>","<p>I'm querying a dreambooth model from Hugging Face using the inference API and am getting a huge data response string back which starts with: <code>����çx00çx10JFIFçx00çx01çx01çx00çx00çx01çx0</code>...</p>
<p>Content-type is: image/jpeg</p>
<p>How do I decode this and display it as an image in javascript?</p>
","huggingface"
"74479617","What are some ways to deal with large slug size in Heroku?","2022-11-17 17:22:23","","1","116","<python><heroku><huggingface-transformers><huggingface-tokenizers><huggingface>","<p>I’m trying to deploy my backend on Heroku and running into the 500 MB slug size limit because my code downloads two tokenizers from Huggingface. For reference, the two tokenizers are <code>BertTokenizerFast.from_pretrained('bert-base-uncased')</code> and <code>SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')</code>.</p>
<p>My <code>requirements.txt</code> file contains the following packages:</p>
<pre><code>fastapi
transformers[torch]
sentence-transformers
requests
uvicorn
</code></pre>
<p>Huggingface has an inference API for models but it does not seem to work for tokenizers. What is a good way to structure my architecture to get around the slug size limit? One thing I thought of is creating 2 seperate FastApi apps to serve the output from each tokenizer but I am wondering if there is a better way to do it.</p>
","huggingface"
"74462314","Unable to create tensor","2022-11-16 14:35:51","","2","423","<nlp><huggingface-transformers><training-data><huggingface><huggingface-datasets>","<p>I am trying to train an NLP model for MLM problem, but the trainer.train function is throwing:</p>
<blockquote>
<p>Unable to create tensor, you should probably activate truncation
and/or padding with 'padding=True' 'truncation=True' to have batched
tensors with the same length. Perhaps your features (<code>input_ids</code> in
this case) have excessive nesting (inputs type <code>list</code> where type <code>int</code>
is expected).</p>
</blockquote>
<p>I really don't know what's going on because I followed the hugging-face tutorials.</p>
<p>Code:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer,AutoModelForMaskedLM
cp= &quot;tau/tavbert-he&quot;
model=AutoModelForMaskedLM.from_pretrained(cp)
tokenizer=AutoTokenizer.from_pretrained(cp)

import datasets
ds=datasets.load_dataset(&quot;csv&quot;, data_files='/content/drive/Shareddrives/Embible/data.csv')
ds=ds['train'].train_test_split(train_size=0.8, seed=42)


def tokenize_function(dataset):
  return tokenizer(str(dataset[&quot;verse&quot;]),truncation=True,padding=True ,
                       max_length=512, return_overflowing_tokens=True)

tokenized_ds=ds.map(tokenize_function)

from transformers import DataCollatorForLanguageModeling
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)
tokenized_ds=tokenized_ds.remove_columns(ds[&quot;train&quot;].column_names)

from transformers import TrainingArguments
from transformers import Trainer


training_args = TrainingArguments(&quot;test-trainer&quot;)
trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenized_ds['train'],
    eval_dataset=tokenized_ds[&quot;test&quot;],
    data_collator=data_collator,
    tokenizer=tokenizer,
)
trainer.train()
</code></pre>
","huggingface"
"74437271","Data collation step causing ""ValueError: Unable to create tensor..."" due to unnecessary padding attempts to extra inputs","2022-11-14 20:08:19","74480160","0","321","<python><pytorch><huggingface-transformers><pytorch-dataloader><huggingface>","<p>I am trying to fine-tune a Bart model from the huggingface transformers framework on a dialogue summarisation task. The Bart model by default takes in the conversations as a monolithic piece of text as the input and takes the summaries as the decoder input while training. I want to explicitly train the model on dialogue speaker and utterance information rather than waiting for the model to implicitly learn them. For this reason, I am extracting the position IDs of the speaker name tokens and their utterance tokens when I send them to the model along with the original input tokens and summary tokens and send them separately. However, the model's data collator/padding automation expects this information to also be the same size as the inputs (I need to disable this behaviour/change the way I am encoding the speaker to utterance mapping).</p>
<p>Please find the code and description for the above issue below:
I am using the SAMSum dataset for the dialogue summarisation task. The dataset looks like this</p>
<p>Conversation:</p>
<pre><code>Amanda: I baked  cookies. Do you want some?
Jerry: Sure!
Amanda: I'll bring you tomorrow :-)
</code></pre>
<p>Summary:</p>
<pre><code>Amanda baked cookies and will bring Jerry some tomorrow.
</code></pre>
<p>The conversation gets tokenized as:</p>
<pre><code>tokens = [0, 10127, 5219, 35, 38, 17241, 1437, 15269, 4, 1832, 47, 236, 103, 116, 50121, 50118, 39237, 35, 9136, 328, 50121, 50118, 10127, 5219, 35, 38, 581, 836, 47, 3859, 48433, 2]
</code></pre>
<p>The explicit speaker-utterance information is encoded as:</p>
<pre><code>[0, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 0]
</code></pre>
<p>Where 1s indicate that tokens[1:3] map to a name &quot;Amanda&quot; and the 2s indicate that tokens[3:16] map to an utterance &quot;: I baked  cookies. Do you want some?&quot;</p>
<p>I am trying to send this speaker utterance association information to the forward function in the hopes of adding a loss on the basis of this information. I intend to override the compute_loss method of the Trainer class from huggingface framework to edit the loss after I can successfully relay this explicit information.</p>
<p>I am currently trying the following:</p>
<pre><code>tokenized_dataset_train = train_datasets.map(preprocess_function, batched=True)

</code></pre>
<p>where the preprocess_function tokenizes and adds the speaker-utterance information in the form of a key-value pair. tokenized_dataset_train is of the form <code>{'input_ids':[...], 'attention_mask':[...], 'spk_utt_pos':[...], ...}</code></p>
<p>The preprocess function makes sure that the lengths for each of 'input_ids', 'attention_masks', and 'spk_utt_pos' is the same.</p>
<p>The data_collator from the <code>DataCollatorForSeq2Seq</code> pads 'input_ids' and 'attention_masks', but also tries to pad 'spk_utt_pos' which gives an error:</p>
<p><code>Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`spk_utt_pos` in this case) have excessive nesting (inputs type `list` where type `int` is expected).</code></p>
<p>Upon printing the sizes of 'input_ids', 'attention_masks', and 'spk_utt_pos' inside the train loop during the data collation step I found that the sizes of were not the same.
Example: (A 32 instance batch)</p>
<pre><code>'input_ids' sizes      357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357
'attention_mask' sizes 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357 357
'spk_utt_pos' sizes    285 276 276 321 58 93 77 69 198 266 55 107 85 235 47 280 209 357 86 186 27 52 80 77 85 231 266 237 322 125 251 126
</code></pre>
<p><strong>My question is: Is there something wrong with my approach to adding this explicit information to my model? What can be another method to send the speaker-utterance information to my model?</strong></p>
","huggingface"
"74412085","Target size (torch.Size([8])) must be the same as input size (torch.Size([8, 15])), multi-class classification using hugging face Roberta","2022-11-12 10:26:15","","0","344","<python><pytorch><multiclass-classification><huggingface><roberta>","<p>I am using hugging face Roberta to classify multi-class dataset, but now I got an error “Target size (torch.Size([8])) must be the same as input size (torch.Size([8, 15]))”.
I am not sure what should I do now, could anyone provide some suggestions?
Below is my codes and you can also find the error message in the bottom:</p>
<pre><code>from datasets import load_dataset
from transformers import RobertaTokenizerFast, Trainer, DataCollatorWithPadding


dataset = load_dataset('csv', data_files=data_path,split = 'train')
train_testvalid = dataset.train_test_split(test_size=0.2)
test_valid = train_testvalid['test'].train_test_split(test_size=0.2)

checkpoint = &quot;roberta-base&quot;
tokenizer = RobertaTokenizerFast.from_pretrained(checkpoint)


def tokenization(example):
    return tokenizer(example['text'], truncation=True,max_length = 256, padding = True)


train_data = test_valid['train']
test_data= test_valid['test']

train_data = train_data.map(tokenization, batched = True)
test_data = test_data.map(tokenization, batched = True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)




train_data = train_data.remove_columns([&quot;Unnamed: 0&quot;, &quot;text&quot;])
test_data = test_data.remove_columns([&quot;Unnamed: 0&quot;, &quot;text&quot;])
train_data.set_format(&quot;torch&quot;)
test_data.set_format(&quot;torch&quot;)
train_data.column_names



from transformers import TrainingArguments

training_args = TrainingArguments(&quot;test-trainer&quot;)




from transformers import RobertaForSequenceClassification

model = RobertaForSequenceClassification.from_pretrained(checkpoint, num_labels=15)



from sklearn.metrics import accuracy_score, precision_recall_fscore_support
def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='micro')
    acc = accuracy_score(labels, preds)
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }




from transformers import Trainer

trainer = Trainer(
    model,
    training_args,
    compute_metrics=compute_metrics,
    train_dataset=train_data,
    eval_dataset=test_data,
    data_collator=data_collator,
    tokenizer=tokenizer,
)

trainer.train()

</code></pre>
<pre><code>ValueError                                Traceback (most recent call last)

&lt;ipython-input-9-3435b262f1ae&gt; in &lt;module&gt;
----&gt; 1 trainer.train()

8 frames

/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py in binary_cross_entropy_with_logits(input, target, weight, size_average, reduce, reduction, pos_weight)
   3146 
   3147     if not (target.size() == input.size()):
-&gt; 3148         raise ValueError(&quot;Target size ({}) must be the same as input size ({})&quot;.format(target.size(), input.size()))
   3149 
   3150     return torch.binary_cross_entropy_with_logits(input, target, weight, pos_weight, reduction_enum)

ValueError: Target size (torch.Size([8])) must be the same as input size (torch.Size([8, 15]))
</code></pre>
","huggingface"
"74382332","What algorithm would you recommend for this ai problem?","2022-11-09 23:01:05","","-2","55","<tensorflow><artificial-intelligence><tensorflow2.0><huggingface>","<p>I'm following some online courses on artificial intelligence. I have a project in mind.</p>
<p>There will be a thousand pictures of people or objects in two folders. But these objects are unrelated things.</p>
<p>First folder name a and second folder name b.</p>
<p>Later, when I send an image, I want to know if it looks more like the images in folder a or b.</p>
<p>I created a model in Hugginface but they didn't mention what algorithm they used there.</p>
<p>Problem type: Binary Classification
Model ID: 2043767063
CO2 Emissions (in grams): 1.6736</p>
","huggingface"
"74369065","Obtaining the image iterations before final image has been generated StableDiffusionPipeline.pretrained","2022-11-09 01:45:25","74373129","5","2069","<python><pytorch><huggingface><stable-diffusion>","<p>I am currently using the <code>diffusers StableDiffusionPipeline</code> (from hugging face) to generate AI images with a discord bot which I use with my friends. I was wondering if it was possible to get a preview of the image being generated before it is finished?</p>
<p>For example, if an image takes 20 seconds to generate, since it is using diffusion it starts off blury and gradually gets better and better. What I want is to save the image on each iteration (or every few seconds) and see how it progresses. How would I be able to do this?</p>
<pre class=""lang-py prettyprint-override""><code>class ImageGenerator:
    def __init__(self, socket_listener, pretty_logger, prisma):
        self.model = StableDiffusionPipeline.from_pretrained(&quot;runwayml/stable-diffusion-v1-5&quot;, revision=&quot;fp16&quot;, torch_dtype=torch.float16, use_auth_token=os.environ.get(&quot;HF_AUTH_TOKEN&quot;))
        self.model = self.model.to(&quot;cuda&quot;)

    async def generate_image(self, data):
        start_time = time.time()
        with autocast(&quot;cuda&quot;):
            image = self.model(data.description, height=self.default_height, width=self.default_width,
                               num_inference_steps=self.default_inference_steps, guidance_scale=self.default_guidance_scale)
            image.save(...)
</code></pre>
<p>The code I have currently is this, however it only returns the image when it is completely done. I have tried to look into how the image is generated inside of the StableDiffusionPipeline but I cannot find anywhere where the image is generated. If anybody could provide any pointers/tips on where I can begin that would be very helpful.</p>
","huggingface"
"74336563","How do I get HuggingFace analytics/metrics of uses?","2022-11-06 14:05:48","","1","326","<huggingface><gradio>","<p>I want to know how many users I get on my application, and some interesting time series of uses. I see Gradio has <code>analytics_enabled</code> but I can't find any more information on it. (<a href=""https://gradio.app/docs/"" rel=""nofollow noreferrer"">https://gradio.app/docs/</a>)</p>
","huggingface"
"74304875","OSError: There was a specific connection error when trying to load CompVis/stable-diffusion-v1-4: <class 'requests.exceptions.HTTPError'>","2022-11-03 14:38:38","74323008","2","2646","<python><huggingface-transformers><huggingface><stable-diffusion>","<h3>System Info</h3>
<p>Google Colab, Free version, GPU</p>
<h3>Information</h3>
<ul>
<li>[ ] The official example scripts</li>
<li>[X] My own modified scripts</li>
</ul>
<h3>Tasks</h3>
<ul>
<li>[ ] An officially supported task in the <code>examples</code> folder (such as GLUE/SQuAD, ...)</li>
<li>[X] My own task or dataset (give details below)</li>
</ul>
<h3>Reproduction</h3>
<ol>
<li><a href=""https://github.com/woctezuma/stable-diffusion-colab"" rel=""nofollow noreferrer"">https://github.com/woctezuma/stable-diffusion-colab</a></li>
<li><a href=""https://colab.research.google.com/github/woctezuma/stable-diffusion-colab/blob/main/stable_diffusion.ipynb#scrollTo=GR4vF2bw-sHR"" rel=""nofollow noreferrer"">https://colab.research.google.com/github/woctezuma/stable-diffusion-colab/blob/main/stable_diffusion.ipynb#scrollTo=GR4vF2bw-sHR</a></li>
<li>copy create to drive</li>
<li>run 1st cell</li>
<li>run 2nd cell</li>
<li>copy my token from  <a href=""https://huggingface.co/settings/tokens"" rel=""nofollow noreferrer"">https://huggingface.co/settings/tokens</a></li>
<li>paste it to the filed</li>
<li>press enter</li>
<li>#1st error - <a href=""https://discuss.huggingface.co/t/invalid-token-passed/22711"" rel=""nofollow noreferrer"">https://discuss.huggingface.co/t/invalid-token-passed/22711</a></li>
<li><a href=""https://huggingface.co/settings/tokens"" rel=""nofollow noreferrer"">https://huggingface.co/settings/tokens</a> mange invalidate and refres</li>
<li>run 2nd cell again</li>
<li>copy and paste in new token</li>
</ol>
<pre><code>        _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|
        _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|
        _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|
        _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|
        _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|

        To login, `huggingface_hub` now requires a token generated from https://huggingface.co/settings/tokens .
        
Token: 
Login successful
Your token has been saved to /root/.huggingface/token
Authenticated through git-credential store but this isn't the helper defined on your machine.
You might have to re-authenticate when pushing to the Hugging Face Hub. Run the following command in your terminal in case you want to set this credential helper as the default

git config --global credential.helper store
</code></pre>
<ol start=""13"">
<li>I have run <code>git config --global credential.helper store</code> than I could  rerun everything and move forward 2 cells</li>
<li>Cell CODE</li>
</ol>
<pre><code>import mediapy as media
import torch
from torch import autocast
from diffusers import StableDiffusionPipeline

model_id = &quot;CompVis/stable-diffusion-v1-4&quot;
device = &quot;cuda&quot;
remove_safety = False


pipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16, revision=&quot;fp16&quot;, use_auth_token=True)
if remove_safety:
  pipe.safety_checker = lambda images, clip_input: (images, False)
pipe = pipe.to(device)
</code></pre>
<ol start=""15"">
<li>ERROR</li>
</ol>
<pre><code>[/usr/local/lib/python3.7/dist-packages/requests/models.py](https://localhost:8080/#) in raise_for_status(self)
    940         if http_error_msg:
--&gt; 941             raise HTTPError(http_error_msg, response=self)
    942 

HTTPError: 403 Client Error: Forbidden for url: https://huggingface.co/CompVis/stable-diffusion-v1-4/resolve/fp16/model_index.json

The above exception was the direct cause of the following exception:

HfHubHTTPError                            Traceback (most recent call last)
[/usr/local/lib/python3.7/dist-packages/diffusers/configuration_utils.py](https://localhost:8080/#) in get_config_dict(cls, pretrained_model_name_or_path, **kwargs)
    233                     subfolder=subfolder,
--&gt; 234                     revision=revision,
    235                 )

[/usr/local/lib/python3.7/dist-packages/huggingface_hub/file_download.py](https://localhost:8080/#) in hf_hub_download(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, use_auth_token, local_files_only, legacy_cache_layout)
   1056                     proxies=proxies,
-&gt; 1057                     timeout=etag_timeout,
   1058                 )

[/usr/local/lib/python3.7/dist-packages/huggingface_hub/file_download.py](https://localhost:8080/#) in get_hf_file_metadata(url, use_auth_token, proxies, timeout)
   1358     )
-&gt; 1359     hf_raise_for_status(r)
   1360 

[/usr/local/lib/python3.7/dist-packages/huggingface_hub/utils/_errors.py](https://localhost:8080/#) in hf_raise_for_status(response, endpoint_name)
    253         # as well (request id and/or server error message)
--&gt; 254         raise HfHubHTTPError(str(HTTPError), response=response) from e
    255 

HfHubHTTPError: &lt;class 'requests.exceptions.HTTPError'&gt; (Request ID: esduBFUm9KJXSxYhFffq4)

During handling of the above exception, another exception occurred:

OSError                                   Traceback (most recent call last)
[&lt;ipython-input-6-9b05f13f8bf3&gt;](https://localhost:8080/#) in &lt;module&gt;
      9 
     10 
---&gt; 11 pipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16, revision=&quot;fp16&quot;, use_auth_token=True)
     12 if remove_safety:
     13   pipe.safety_checker = lambda images, clip_input: (images, False)

[/usr/local/lib/python3.7/dist-packages/diffusers/pipeline_utils.py](https://localhost:8080/#) in from_pretrained(cls, pretrained_model_name_or_path, **kwargs)
    371                 local_files_only=local_files_only,
    372                 use_auth_token=use_auth_token,
--&gt; 373                 revision=revision,
    374             )
    375             # make sure we only download sub-folders and `diffusers` filenames

[/usr/local/lib/python3.7/dist-packages/diffusers/configuration_utils.py](https://localhost:8080/#) in get_config_dict(cls, pretrained_model_name_or_path, **kwargs)
    254             except HTTPError as err:
    255                 raise EnvironmentError(
--&gt; 256                     &quot;There was a specific connection error when trying to load&quot;
    257                     f&quot; {pretrained_model_name_or_path}:\n{err}&quot;
    258                 )

OSError: There was a specific connection error when trying to load CompVis/stable-diffusion-v1-4:
&lt;class 'requests.exceptions.HTTPError'&gt; (Request ID: esduBFUm9KJXSxYhFffq4)
</code></pre>
<h3>Expected behavior</h3>
<p>Run all the cells and generating photo's as on the GitHub project shows
<a href=""https://github.com/woctezuma/stable-diffusion-colab"" rel=""nofollow noreferrer"">https://github.com/woctezuma/stable-diffusion-colab</a></p>
","huggingface"
"74296935","Fix tokenization to tensors with padding Huggingface","2022-11-03 00:20:20","","0","227","<huggingface-transformers><bert-language-model><huggingface-tokenizers><huggingface>","<p>I'm trying to tokenize my dataset with the following preprocessing function. I've already donlowaded with AutoTokenizer from the Spanish BERT version.</p>
<p>`</p>
<pre><code>max_input_length = 280
max_target_length = 280
source_lang = &quot;es&quot;
target_lang = &quot;en&quot;
prefix = &quot;translate spanish_to_women to spanish_to_men: &quot;

def preprocess_function(examples):
    inputs = [prefix + ex for ex in examples[&quot;mujeres_tweet&quot;]]
    targets = [ex for ex in examples[&quot;hombres_tweet&quot;]]

    model_inputs = tokz(inputs,
                        padding=True, 
                        truncation=True,
                        max_length=max_input_length,
                        return_tensors = 'pt'
                        )

    # Setup the tokenizer for targets
    with tokz.as_target_tokenizer():
        labels = tokz(targets, 
                      padding=True, 
                      truncation=True,
                      max_length=max_target_length,
                      return_tensors = 'pt'
                      )

    model_inputs[&quot;labels&quot;] = labels[&quot;input_ids&quot;]
    return model_inputs
</code></pre>
<p>`</p>
<p>And I get the following error when trying to pass my dataset object through the function.
<a href=""https://i.sstatic.net/31Qru.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/31Qru.png"" alt=""enter image description here"" /></a></p>
<p>I've already tried dropping the columns that have strings. I've seen also that when I do not set the return_tensors it does tokenize my dataset (but later on I have the same problem when trying to train my BERT model. Anyone knows what might be going on? *inserts crying face</p>
<p>Also, I've tried tokenizing it without the return_tensors and then doing set_format but it returns and empty dataset object *inserts another crying face.</p>
<p><a href=""https://i.sstatic.net/Tkm4T.png"" rel=""nofollow noreferrer"">My Dataset looks like the following</a></p>
<p><a href=""https://i.sstatic.net/AQ7eE.png"" rel=""nofollow noreferrer"">And an example of the inputs</a></p>
<p>So that I just do:</p>
<pre><code>tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
</code></pre>
","huggingface"
"74290497","How to handle sequences longer than 512 tokens in layoutLMV3?","2022-11-02 14:06:32","","4","3028","<transformer-model><huggingface-tokenizers><huggingface>","<p>How to work with sequences longer than 512 tokens. I don't wanted to use truncates =True. But actually wanted to handle the longer sequences</p>
","huggingface"
"74279005","Tokenizer.from_file() HUGGINFACE : Exception: data did not match any variant of untagged enum ModelWrapper","2022-11-01 16:34:41","76227397","6","6445","<json><nlp><huggingface-transformers><huggingface-tokenizers><huggingface>","<p>I am having issue loading a <code>Tokenizer.from_file()</code> BPE tokenizer.
When I try I am encountering this error where the line 11743 is the last last one:
<code>Exception: data did not match any variant of untagged enum ModelWrapper at line 11743 column 3</code>
I have no idea what is the problem and how to solve it
does anyone have some clue?
I did not train directly the BPE but the structure is the correct one so vocab and merges in a json. What I did was from a BPE trained by me (that was working) change completely the vocab and the merges based on something manually created by me (without a proper train). But I don't see the problem since the structure should be the same as the original one.
My tokenizer version is: <code>0.13.1</code></p>
<pre><code>{
  &quot;version&quot;:&quot;1.0&quot;,
  &quot;truncation&quot;:null,
  &quot;padding&quot;:null,
  &quot;added_tokens&quot;:[
    {
      &quot;id&quot;:0,
      &quot;content&quot;:&quot;[UNK]&quot;,
      &quot;single_word&quot;:false,
      &quot;lstrip&quot;:false,
      &quot;rstrip&quot;:false,
      &quot;normalized&quot;:false,
      &quot;special&quot;:true
    },
    {
      &quot;id&quot;:1,
      &quot;content&quot;:&quot;[CLS]&quot;,
      &quot;single_word&quot;:false,
      &quot;lstrip&quot;:false,
      &quot;rstrip&quot;:false,
      &quot;normalized&quot;:false,
      &quot;special&quot;:true
    },
    {
      &quot;id&quot;:2,
      &quot;content&quot;:&quot;[SEP]&quot;,
      &quot;single_word&quot;:false,
      &quot;lstrip&quot;:false,
      &quot;rstrip&quot;:false,
      &quot;normalized&quot;:false,
      &quot;special&quot;:true
    },
    {
      &quot;id&quot;:3,
      &quot;content&quot;:&quot;[PAD]&quot;,
      &quot;single_word&quot;:false,
      &quot;lstrip&quot;:false,
      &quot;rstrip&quot;:false,
      &quot;normalized&quot;:false,
      &quot;special&quot;:true
    },
    {
      &quot;id&quot;:4,
      &quot;content&quot;:&quot;[MASK]&quot;,
      &quot;single_word&quot;:false,
      &quot;lstrip&quot;:false,
      &quot;rstrip&quot;:false,
      &quot;normalized&quot;:false,
      &quot;special&quot;:true
    }
  ],
  &quot;normalizer&quot;:null,
  &quot;pre_tokenizer&quot;:{
    &quot;type&quot;:&quot;Whitespace&quot;
  },
  &quot;post_processor&quot;:null,
  &quot;decoder&quot;:null,
  &quot;model&quot;:{
    &quot;type&quot;:&quot;BPE&quot;,
    &quot;dropout&quot;:null,
    &quot;unk_token&quot;:&quot;[UNK]&quot;,
    &quot;continuing_subword_prefix&quot;:null,
    &quot;end_of_word_suffix&quot;:null,
    &quot;fuse_unk&quot;:false,
    &quot;vocab&quot;:{
      &quot;[UNK]&quot;:0,
      &quot;[CLS]&quot;:1,
      &quot;[SEP]&quot;:2,
      &quot;[PAD]&quot;:3,
      &quot;[MASK]&quot;:4,
      &quot;AA&quot;:5,
      &quot;A&quot;:6,
      &quot;C&quot;:7,
      &quot;D&quot;:8,
.....
</code></pre>
<p>merges:</p>
<pre><code>....
      &quot;QD FLPDSITF&quot;,
      &quot;QPHY AS&quot;,
      &quot;LR SE&quot;,
      &quot;A DRV&quot;
    ] #11742
  } #11743
} #11744
</code></pre>
","huggingface"
"74267006","Hugging Face T5 model that is not pre-trained and training it","2022-10-31 17:25:32","","1","483","<python><nlp><huggingface-transformers><training-data><huggingface>","<p>I want to use the Hugging Face T5 model to do summarization but I want to train the model with my own dataset.</p>
<p>How can I get the T5 model such that it has not been trained yet? And what steps do I need to take to train it?</p>
<p>Currently I am look at this tutorial:
<a href=""https://huggingface.co/docs/transformers/tasks/summarization"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/tasks/summarization</a></p>
<p>I also found this code which allows me to get a T5 model without weights but how can I train it with my own data?
<a href=""https://stackoverflow.com/questions/73700165/how-to-use-architecture-of-t5-without-pretrained-model-hugging-face"">How to use architecture of T5 without pretrained model (Hugging face)</a></p>
","huggingface"
"74251282","Huggingface datasets storing and loading image data","2022-10-30 07:38:50","","4","877","<huggingface><huggingface-datasets>","<p>I have a huggingface dataset with an image column</p>
<pre><code>ds[&quot;image&quot;][0]

&lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=300x300 at 0x1682DD820&gt;
</code></pre>
<p>When I save to disk, load it later I get the image column as bytes:</p>
<pre><code>ds.save_to_disk(&quot;./dataset.hf&quot;)
ds.load_from_disk(&quot;./dataset.hf&quot;)
ds[&quot;image&quot;][0]

{'bytes': b'\xff\xd8\xff\xe0\x00\x10JFIF\x00\x01\x01\x00\x00\x01\x00\x01\x00\x00\xff\xdb\x00C\x00\x08\x06\x06\x07\x06\x05\x08\x07\x07\x07\t\t\x08\n\x0c\x14\r\x0c\x0b\x0b\x0c\x19\x12\x13\x0f\x14\x1d\x1a\x1f\x1e\x1d\x1a\x1c\x1c $.\', 
'path': None}
</code></pre>
<p>The image column is converted to bytes.</p>
<p>How can I load the dataset and make sure my image column is  <code>PIL.JpegImagePlugin.JpegImageFile</code>?</p>
","huggingface"
"74244702","How to split input text into equal size of tokens, not character length, and then concatenate the summarization results for Hugging Face transformers","2022-10-29 10:56:40","74524543","3","3281","<python><nlp><huggingface-transformers><huggingface-tokenizers><huggingface>","<p>I am using the below methodology to summarize longer than 1024 token size long texts.</p>
<p>Current method splits the text by half. I took this from another user's post and modified it slightly.</p>
<p>So what I want to do is, instead of splitting into half, split whole text into 1024 equal sized tokens and get summarization each of them and then at the end, concatenate them with the correct order and write into file. How can I do this tokenization and getting the correct output?</p>
<p>text split with <code>Split(&quot; &quot;)</code> doesn't work same as tokenization. It produces different count.</p>
<pre><code>import logging
from transformers import pipeline

f = open(&quot;TextFile1.txt&quot;, &quot;r&quot;)

ARTICLE = f.read()

summarizer = pipeline(&quot;summarization&quot;, model=&quot;facebook/bart-large-cnn&quot; )

counter = 1

def summarize_text(text: str, max_len: int) -&gt; str:
    global counter
    try:
        #logging.warning(&quot;max_len &quot; + str(max_len))
        summary = summarizer(text, min_length=30, do_sample=False)
        with open('parsed_'+str(counter)+'.txt', 'w') as f:
            f.write(text)
        counter += 1
        return summary[0][&quot;summary_text&quot;]
    except IndexError as ex:
        logging.warning(&quot;Sequence length too large for model, cutting text in half and calling again&quot;)
        return summarize_text(text=text[:(len(text) // 2)], max_len=max_len) + &quot; &quot; + summarize_text(text=text[(len(text) // 2):], max_len=max_len)

gg = summarize_text(ARTICLE, 1024)

with open('summarized.txt', 'w') as f:
    f.write(gg)
</code></pre>
","huggingface"
"74241344","How can I display summarization progress percentage when using hugging face transformers","2022-10-28 22:05:44","","1","2396","<python><machine-learning><nlp><huggingface-transformers><huggingface>","<p>I am running the below code but I have 0 idea how much time is remaining. It can be hours, days, etc.</p>
<p>I really would like to see some sort of progress during the summarization.</p>
<p>Any help is appreciated</p>
<p>By the way It is taking really long time on RTX 3060 - 12GB vram even with as small as 9k token input</p>
<pre><code>wall_of_text=&quot;some long text...&quot;
import torch
from transformers import pipeline

summarizer = pipeline(
    &quot;summarization&quot;,
    &quot;pszemraj/long-t5-tglobal-base-16384-book-summary&quot;,
    device=0 if torch.cuda.is_available() else -1,
)

result = summarizer(wall_of_text,min_length=500,max_length=16384,no_repeat_ngram_size=3, 
           encoder_no_repeat_ngram_size =3,
           repetition_penalty=3.5,
           num_beams=4,
           early_stopping=True)
with open('pszemraj-long-t5-tglobal-base-16384-book-summary.txt', 'w') as f:
    f.write(result[0]['summary_text'])
</code></pre>
","huggingface"
"74228673","Create custom huggingface dataset by loading text data from elasticsearch database on a remote server","2022-10-27 21:49:36","","1","425","<python><elasticsearch><dataset><huggingface><huggingface-datasets>","<p>I would like to fine-tune a sentence transformer model with some text data stored in elasticsearch database on a server. I tried to create a generator function that queries the index and yields the results as dictionaries one at a time and pass this function to Dataset.from_generator(), but got an error 'cannot pickle 'SSLContext' object'.</p>
<p>Could someone please debug this? thanks very much</p>
<p>Code to reproduce the error</p>
<pre><code>from elasticsearch import Elasticsearch
from elasticsearch import helpers
from elasticsearch.connection import create_ssl_context
from datasets import Dataset

estd = Elasticsearch(  username = 'user',
            password = 'password',
            host = 'host',
            ssl_context = create_ssl_context())

def export_index( fields, index, size = 3000):
    source_query = { &quot;_source&quot;: fields }
    result = helpers.scan(estd,
                            index = index,
                            query = source_query,
                            size = size)  # Max here is 10000, but setting it this high might result in timeouts
    return(result)

source_index = 'meeting_reports'
fields = [&quot;text&quot;, &quot;source_title&quot;]

def my_gen():
    for doc in export_index(fields, source_index, size = 1000):
        yield {'text':doc['_source']['text']}

dataset = Dataset.from_generator(my_gen)
</code></pre>
<p>Error message :</p>
<pre><code>TypeError                                 Traceback (most recent call last)
Input In [11], in &lt;module&gt;
     23     for doc in export_index(fields, source_index, size = 1000):
     24         yield {'text':doc['_source']['text']}
---&gt; 26 dataset = Dataset.from_generator(my_gen)

File ~/miniforge3/envs/es_env/lib/python3.10/site-packages/datasets/arrow_dataset.py:973, in Dataset.from_generator(generator, features, cache_dir, keep_in_memory, gen_kwargs, **kwargs)
    948 &quot;&quot;&quot;Create a Dataset from a generator.
    949 
    950 Args:
   (...)
    969 ```
    970 &quot;&quot;&quot;
    971 from .io.generator import GeneratorDatasetInputStream
--&gt; 973 return GeneratorDatasetInputStream(
    974     generator=generator,
    975     features=features,
    976     cache_dir=cache_dir,
    977     keep_in_memory=keep_in_memory,
    978     gen_kwargs=gen_kwargs,
    979     **kwargs,
    980 ).read()

File ~/miniforge3/envs/es_env/lib/python3.10/site-packages/datasets/io/generator.py:22, in GeneratorDatasetInputStream.__init__(self, generator, features, cache_dir, keep_in_memory, streaming, gen_kwargs, **kwargs)
      9 def __init__(
     10     self,
     11     generator: Callable,
   (...)
     17     **kwargs,
     18 ):
     19     super().__init__(
     20         features=features, cache_dir=cache_dir, keep_in_memory=keep_in_memory, streaming=streaming, **kwargs
     21     )
---&gt; 22     self.builder = Generator(
     23         cache_dir=cache_dir,
     24         features=features,
     25         generator=generator,
     26         gen_kwargs=gen_kwargs,
     27         **kwargs,
     28     )

File ~/miniforge3/envs/es_env/lib/python3.10/site-packages/datasets/builder.py:1292, in GeneratorBasedBuilder.__init__(self, writer_batch_size, *args, **kwargs)
   1291 def __init__(self, *args, writer_batch_size=None, **kwargs):
-&gt; 1292     super().__init__(*args, **kwargs)
   1293     # Batch size used by the ArrowWriter
   1294     # It defines the number of samples that are kept in memory before writing them
   1295     # and also the length of the arrow chunks
   1296     # None means that the ArrowWriter will use its default value
   1297     self._writer_batch_size = writer_batch_size or self.DEFAULT_WRITER_BATCH_SIZE

File ~/miniforge3/envs/es_env/lib/python3.10/site-packages/datasets/builder.py:303, in DatasetBuilder.__init__(self, cache_dir, config_name, hash, base_path, info, features, use_auth_token, repo_id, data_files, data_dir, name, **config_kwargs)
    301 if data_dir is not None:
    302     config_kwargs[&quot;data_dir&quot;] = data_dir
--&gt; 303 self.config, self.config_id = self._create_builder_config(
    304     config_name=config_name,
    305     custom_features=features,
    306     **config_kwargs,
    307 )
    309 # prepare info: DatasetInfo are a standardized dataclass across all datasets
    310 # Prefill datasetinfo
    311 if info is None:

File ~/miniforge3/envs/es_env/lib/python3.10/site-packages/datasets/builder.py:471, in DatasetBuilder._create_builder_config(self, config_name, custom_features, **config_kwargs)
    468     raise ValueError(f&quot;BuilderConfig must have a name, got {builder_config.name}&quot;)
    470 # compute the config id that is going to be used for caching
--&gt; 471 config_id = builder_config.create_config_id(
    472     config_kwargs,
    473     custom_features=custom_features,
    474 )
    475 is_custom = (config_id not in self.builder_configs) and config_id != &quot;default&quot;
    476 if is_custom:

File ~/miniforge3/envs/es_env/lib/python3.10/site-packages/datasets/builder.py:169, in BuilderConfig.create_config_id(self, config_kwargs, custom_features)
    167             suffix = Hasher.hash(config_kwargs_to_add_to_suffix)
    168     else:
--&gt; 169         suffix = Hasher.hash(config_kwargs_to_add_to_suffix)
    171 if custom_features is not None:
    172     m = Hasher()

File ~/miniforge3/envs/es_env/lib/python3.10/site-packages/datasets/fingerprint.py:237, in Hasher.hash(cls, value)
    235     return cls.dispatch[type(value)](cls, value)
    236 else:
--&gt; 237     return cls.hash_default(value)

File ~/miniforge3/envs/es_env/lib/python3.10/site-packages/datasets/fingerprint.py:230, in Hasher.hash_default(cls, value)
    228 @classmethod
    229 def hash_default(cls, value: Any) -&gt; str:
--&gt; 230     return cls.hash_bytes(dumps(value))

File ~/miniforge3/envs/es_env/lib/python3.10/site-packages/datasets/utils/py_utils.py:625, in dumps(obj)
    623 file = StringIO()
    624 with _no_cache_fields(obj):
--&gt; 625     dump(obj, file)
    626 return file.getvalue()

File ~/miniforge3/envs/es_env/lib/python3.10/site-packages/datasets/utils/py_utils.py:600, in dump(obj, file)
    598 def dump(obj, file):
    599     &quot;&quot;&quot;pickle an object to a file&quot;&quot;&quot;
--&gt; 600     Pickler(file, recurse=True).dump(obj)
    601     return

File ~/miniforge3/envs/es_env/lib/python3.10/site-packages/dill/_dill.py:620, in Pickler.dump(self, obj)
    618     raise PicklingError(msg)
    619 else:
--&gt; 620     StockPickler.dump(self, obj)
    621 return

File ~/miniforge3/envs/es_env/lib/python3.10/pickle.py:487, in _Pickler.dump(self, obj)
    485 if self.proto &gt;= 4:
    486     self.framer.start_framing()
--&gt; 487 self.save(obj)
    488 self.write(STOP)
    489 self.framer.end_framing()

File ~/miniforge3/envs/es_env/lib/python3.10/pickle.py:560, in _Pickler.save(self, obj, save_persistent_id)
    558 f = self.dispatch.get(t)
    559 if f is not None:
--&gt; 560     f(self, obj)  # Call unbound method with explicit self
    561     return
    563 # Check private dispatch table if any, or else
    564 # copyreg.dispatch_table

File ~/miniforge3/envs/es_env/lib/python3.10/site-packages/dill/_dill.py:1251, in save_module_dict(pickler, obj)
   1248     if is_dill(pickler, child=False) and pickler._session:
   1249         # we only care about session the first pass thru
   1250         pickler._first_pass = False
-&gt; 1251     StockPickler.save_dict(pickler, obj)
   1252     log.info(&quot;# D2&quot;)
   1253 return

File ~/miniforge3/envs/es_env/lib/python3.10/pickle.py:972, in _Pickler.save_dict(self, obj)
    969     self.write(MARK + DICT)
    971 self.memoize(obj)
--&gt; 972 self._batch_setitems(obj.items())

File ~/miniforge3/envs/es_env/lib/python3.10/pickle.py:998, in _Pickler._batch_setitems(self, items)
    996     for k, v in tmp:
    997         save(k)
--&gt; 998         save(v)
    999     write(SETITEMS)
   1000 elif n:

File ~/miniforge3/envs/es_env/lib/python3.10/pickle.py:560, in _Pickler.save(self, obj, save_persistent_id)
    558 f = self.dispatch.get(t)
    559 if f is not None:
--&gt; 560     f(self, obj)  # Call unbound method with explicit self
    561     return
    563 # Check private dispatch table if any, or else
    564 # copyreg.dispatch_table

File ~/miniforge3/envs/es_env/lib/python3.10/site-packages/datasets/utils/py_utils.py:891, in save_function(pickler, obj)
    888     if state_dict:
    889         state = state, state_dict
--&gt; 891     dill._dill._save_with_postproc(
    892         pickler,
    893         (
    894             dill._dill._create_function,
    895             (obj.__code__, globs, obj.__name__, obj.__defaults__, closure),
    896             state,
    897         ),
    898         obj=obj,
    899         postproc_list=postproc_list,
    900     )
    901 else:
    902     closure = obj.func_closure

File ~/miniforge3/envs/es_env/lib/python3.10/site-packages/dill/_dill.py:1154, in _save_with_postproc(pickler, reduction, is_pickler_dill, obj, postproc_list)
   1152 if source:
   1153     pickler.write(pickler.get(pickler.memo[id(dest)][0]))
-&gt; 1154     pickler._batch_setitems(iter(source.items()))
   1155 else:
   1156     # Updating with an empty dictionary. Same as doing nothing.
   1157     continue

File ~/miniforge3/envs/es_env/lib/python3.10/pickle.py:998, in _Pickler._batch_setitems(self, items)
    996     for k, v in tmp:
    997         save(k)
--&gt; 998         save(v)
    999     write(SETITEMS)
   1000 elif n:

File ~/miniforge3/envs/es_env/lib/python3.10/pickle.py:603, in _Pickler.save(self, obj, save_persistent_id)
    599     raise PicklingError(&quot;Tuple returned by %s must have &quot;
    600                         &quot;two to six elements&quot; % reduce)
    602 # Save the reduce() output and finally memoize the object
--&gt; 603 self.save_reduce(obj=obj, *rv)

File ~/miniforge3/envs/es_env/lib/python3.10/pickle.py:717, in _Pickler.save_reduce(self, func, args, state, listitems, dictitems, state_setter, obj)
    715 if state is not None:
    716     if state_setter is None:
--&gt; 717         save(state)
    718         write(BUILD)
    719     else:
    720         # If a state_setter is specified, call it instead of load_build
    721         # to update obj's with its previous state.
    722         # First, push state_setter and its tuple of expected arguments
    723         # (obj, state) onto the stack.

File ~/miniforge3/envs/es_env/lib/python3.10/pickle.py:560, in _Pickler.save(self, obj, save_persistent_id)
    558 f = self.dispatch.get(t)
    559 if f is not None:
--&gt; 560     f(self, obj)  # Call unbound method with explicit self
    561     return
    563 # Check private dispatch table if any, or else
    564 # copyreg.dispatch_table

File ~/miniforge3/envs/es_env/lib/python3.10/site-packages/dill/_dill.py:1251, in save_module_dict(pickler, obj)
   1248     if is_dill(pickler, child=False) and pickler._session:
   1249         # we only care about session the first pass thru
   1250         pickler._first_pass = False
-&gt; 1251     StockPickler.save_dict(pickler, obj)
   1252     log.info(&quot;# D2&quot;)
   1253 return

File ~/miniforge3/envs/es_env/lib/python3.10/pickle.py:972, in _Pickler.save_dict(self, obj)
    969     self.write(MARK + DICT)
    971 self.memoize(obj)
--&gt; 972 self._batch_setitems(obj.items())

File ~/miniforge3/envs/es_env/lib/python3.10/pickle.py:998, in _Pickler._batch_setitems(self, items)
    996     for k, v in tmp:
    997         save(k)
--&gt; 998         save(v)
    999     write(SETITEMS)
   1000 elif n:

File ~/miniforge3/envs/es_env/lib/python3.10/pickle.py:603, in _Pickler.save(self, obj, save_persistent_id)
    599     raise PicklingError(&quot;Tuple returned by %s must have &quot;
    600                         &quot;two to six elements&quot; % reduce)
    602 # Save the reduce() output and finally memoize the object
--&gt; 603 self.save_reduce(obj=obj, *rv)

File ~/miniforge3/envs/es_env/lib/python3.10/pickle.py:717, in _Pickler.save_reduce(self, func, args, state, listitems, dictitems, state_setter, obj)
    715 if state is not None:
    716     if state_setter is None:
--&gt; 717         save(state)
    718         write(BUILD)
    719     else:
    720         # If a state_setter is specified, call it instead of load_build
    721         # to update obj's with its previous state.
    722         # First, push state_setter and its tuple of expected arguments
    723         # (obj, state) onto the stack.

File ~/miniforge3/envs/es_env/lib/python3.10/pickle.py:560, in _Pickler.save(self, obj, save_persistent_id)
    558 f = self.dispatch.get(t)
    559 if f is not None:
--&gt; 560     f(self, obj)  # Call unbound method with explicit self
    561     return
    563 # Check private dispatch table if any, or else
    564 # copyreg.dispatch_table

File ~/miniforge3/envs/es_env/lib/python3.10/site-packages/dill/_dill.py:1251, in save_module_dict(pickler, obj)
   1248     if is_dill(pickler, child=False) and pickler._session:
   1249         # we only care about session the first pass thru
   1250         pickler._first_pass = False
-&gt; 1251     StockPickler.save_dict(pickler, obj)
   1252     log.info(&quot;# D2&quot;)
   1253 return

File ~/miniforge3/envs/es_env/lib/python3.10/pickle.py:972, in _Pickler.save_dict(self, obj)
    969     self.write(MARK + DICT)
    971 self.memoize(obj)
--&gt; 972 self._batch_setitems(obj.items())

File ~/miniforge3/envs/es_env/lib/python3.10/pickle.py:998, in _Pickler._batch_setitems(self, items)
    996     for k, v in tmp:
    997         save(k)
--&gt; 998         save(v)
    999     write(SETITEMS)
   1000 elif n:

File ~/miniforge3/envs/es_env/lib/python3.10/pickle.py:560, in _Pickler.save(self, obj, save_persistent_id)
    558 f = self.dispatch.get(t)
    559 if f is not None:
--&gt; 560     f(self, obj)  # Call unbound method with explicit self
    561     return
    563 # Check private dispatch table if any, or else
    564 # copyreg.dispatch_table

File ~/miniforge3/envs/es_env/lib/python3.10/site-packages/dill/_dill.py:1251, in save_module_dict(pickler, obj)
   1248     if is_dill(pickler, child=False) and pickler._session:
   1249         # we only care about session the first pass thru
   1250         pickler._first_pass = False
-&gt; 1251     StockPickler.save_dict(pickler, obj)
   1252     log.info(&quot;# D2&quot;)
   1253 return

File ~/miniforge3/envs/es_env/lib/python3.10/pickle.py:972, in _Pickler.save_dict(self, obj)
    969     self.write(MARK + DICT)
    971 self.memoize(obj)
--&gt; 972 self._batch_setitems(obj.items())

File ~/miniforge3/envs/es_env/lib/python3.10/pickle.py:998, in _Pickler._batch_setitems(self, items)
    996     for k, v in tmp:
    997         save(k)
--&gt; 998         save(v)
    999     write(SETITEMS)
   1000 elif n:

File ~/miniforge3/envs/es_env/lib/python3.10/pickle.py:578, in _Pickler.save(self, obj, save_persistent_id)
    576 reduce = getattr(obj, &quot;__reduce_ex__&quot;, None)
    577 if reduce is not None:
--&gt; 578     rv = reduce(self.proto)
    579 else:
    580     reduce = getattr(obj, &quot;__reduce__&quot;, None)

TypeError: cannot pickle 'SSLContext' object
</code></pre>
","huggingface"
"74228640","Which HuggingFace summarization models support more than 1024 tokens? Which model is more suitable for programming related articles?","2022-10-27 21:45:04","74229599","9","4496","<nlp><huggingface-transformers><summarization><huggingface><mlmodel>","<p>If this is not the best place to ask this question, please lead me to the most accurate one.</p>
<p>I am planning to use one of the Huggingface summarization models (<a href=""https://huggingface.co/models?pipeline_tag=summarization"" rel=""noreferrer"">https://huggingface.co/models?pipeline_tag=summarization</a>) to summarize my lecture video transcriptions.</p>
<p>So far I have tested <code>facebook/bart-large-cnn</code> and <code>sshleifer/distilbart-cnn-12-6</code>, but they only support a maximum of 1,024 tokens as input.</p>
<p>So, here are my questions:</p>
<ol>
<li><p>Are there any summarization models that support longer inputs such as 10,000 word articles?</p>
</li>
<li><p>What are the optimal output lengths for given input lengths? Let's say for a 1,000 word input, what is the optimal (minimum) output length (the min. length of the summarized text)?</p>
</li>
<li><p>Which model would likely work on programming related articles?</p>
</li>
</ol>
","huggingface"
"74223324","Using huggingface transformers trainer method for hugging face datasets","2022-10-27 13:58:41","","1","2299","<python><nlp><huggingface-transformers><transformer-model><huggingface>","<p>I am trying to train a transformer(Salesforce codet5-small) using the huggingface trainer method and on a hugging face Dataset (namely, &quot;eth_py150_open&quot;). However, I'm encountering a number of issues.</p>
<p>Here is the relevant code snippet:</p>
<pre><code>import torch
import transformers
from datasets import load_dataset_builder
from datasets import load_dataset

corpus=load_dataset(&quot;eth_py150_open&quot;, split='train')

training_args = transformers.TrainingArguments( #general training arguments
    per_device_train_batch_size = 8,
    warmup_steps = 0,
    weight_decay = 0.01,
    learning_rate = 1e-4,
    num_train_epochs = 12,
    output_dir = './runs/run2/output/',
    logging_dir = './runs/run2/logging/',
    logging_steps = 50,
    save_steps= 10000,
    remove_unused_columns=False,
)

model = transformers.T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-small').cuda()

trainer = transformers.Trainer(
    model = model,
   args = training_args,
    train_dataset = corpus,
)

</code></pre>
<p>However, when running trainer.train(), I get the following error:</p>
<pre><code>***** Running training *****
  Num examples = 74749
  Num Epochs = 12
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed &amp; accumulation) = 8
  Gradient Accumulation steps = 1
  Total optimization steps = 112128
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-28-3435b262f1ae&gt; in &lt;module&gt;
----&gt; 1 trainer.train()

3 frames
/usr/local/lib/python3.7/dist-packages/transformers/trainer.py in _prepare_inputs(self, inputs)
   2414         if len(inputs) == 0:
   2415             raise ValueError(
-&gt; 2416                 &quot;The batch received was empty, your model won't be able to train on it. Double-check that your &quot;
   2417                 f&quot;training dataset contains keys expected by the model: {','.join(self._signature_columns)}.&quot;
   2418             )

TypeError: can only join an iterable
</code></pre>
<p>I have tried converting corpus to a torch Dataset object, but can't seem to figure out how to do this. I'd really appreciate any help!</p>
","huggingface"
"74212925","Does checkpointing with torch.save fail with hugging face -- if not what is the right way to checkpoint and load a hugging face (HF) model?","2022-10-26 18:56:36","","1","303","<python><pytorch><huggingface-transformers><huggingface-tokenizers><huggingface>","<p>Does torch.save work on hugging face models (I am using vit)? I assumed yes.</p>
<p>My error:</p>
<pre><code>  File &quot;/home/miranda9/miniconda3/envs/metalearning_gpu/lib/python3.9/site-packages/torch/serialization.py&quot;, line 379, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol)
  File &quot;/home/miranda9/miniconda3/envs/metalearning_gpu/lib/python3.9/site-packages/torch/serialization.py&quot;, line 499, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
OSError: [Errno 116] Stale file handle
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File &quot;/shared/rsaas/miranda9/diversity-for-predictive-success-of-meta-learning/div_src/diversity_src/experiment_mains/main_dist_maml_l2l.py&quot;, line 1815, in &lt;module&gt;
    main()
  File &quot;/shared/rsaas/miranda9/diversity-for-predictive-success-of-meta-learning/div_src/diversity_src/experiment_mains/main_dist_maml_l2l.py&quot;, line 1748, in main
    train(args=args)
  File &quot;/shared/rsaas/miranda9/diversity-for-predictive-success-of-meta-learning/div_src/diversity_src/experiment_mains/main_dist_maml_l2l.py&quot;, line 1795, in train
    meta_train_iterations_ala_l2l(args, args.agent, args.opt, args.scheduler)
  File &quot;/home/miranda9/ultimate-utils/ultimate-utils-proj-src/uutils/torch_uu/training/meta_training.py&quot;, line 213, in meta_train_iterations_ala_l2l
    log_train_val_stats(args, args.it, step_name, train_loss, train_acc, training=True)
  File &quot;/home/miranda9/ultimate-utils/ultimate-utils-proj-src/uutils/logging_uu/wandb_logging/supervised_learning.py&quot;, line 55, in log_train_val_stats
    _log_train_val_stats(args=args,
  File &quot;/home/miranda9/ultimate-utils/ultimate-utils-proj-src/uutils/logging_uu/wandb_logging/supervised_learning.py&quot;, line 113, in _log_train_val_stats
    save_for_supervised_learning(args, ckpt_filename='ckpt.pt')
  File &quot;/home/miranda9/ultimate-utils/ultimate-utils-proj-src/uutils/torch_uu/checkpointing_uu/supervised_learning.py&quot;, line 54, in save_for_supervised_learning
    torch.save({'training_mode': args.training_mode,
  File &quot;/home/miranda9/miniconda3/envs/metalearning_gpu/lib/python3.9/site-packages/torch/serialization.py&quot;, line 380, in save
    return
  File &quot;/home/miranda9/miniconda3/envs/metalearning_gpu/lib/python3.9/site-packages/torch/serialization.py&quot;, line 259, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:298] . unexpected pos 2736460544 vs 2736460432
</code></pre>
<p>my code:</p>
<pre><code>        # - ckpt
        args_pickable: Namespace = uutils.make_args_pickable(args)
        # note not saving any objects, to make sure checkpoint is loadable later with no problems
        torch.save({'training_mode': args.training_mode,
                    'it': args.it,
                    'epoch_num': args.epoch_num,

                    # 'args': args_pickable,  # some versions of this might not have args!
                    # decided only to save the dict version to avoid this ckpt not working, make it args when loading
                    'args_dict': vars(args_pickable),  # some versions of this might not have args!

                    'model_state_dict': get_model_from_ddp(args.model).state_dict(),
                    'model_str': str(args.model),  # added later, to make it easier to check what optimizer was used
                    'model_hps': args.model_hps,
                    'model_option': args.model_option,

                    'opt_state_dict': args.opt.state_dict(),
                    'opt_str': str(args.opt),
                    'opt_hps': args.opt_hps,
                    'opt_option': args.opt_option,

                    'scheduler_str': str(args.scheduler),
                    'scheduler_state_dict': try_to_get_scheduler_state_dict(args.scheduler),
                    'scheduler_hps': args.scheduler_hps,
                    'scheduler_option': args.scheduler_option,
                    },
                   pickle_module=pickle,
                   f=args.log_root / ckpt_filename)
</code></pre>
<p>if this is not the right way to checkpoint hugging face (HF) models, what is?</p>
<hr />
<p>cross: hf discussion forum: <a href=""https://discuss.huggingface.co/t/torch-save-with-hugging-face-models-fails/25034"" rel=""nofollow noreferrer"">https://discuss.huggingface.co/t/torch-save-with-hugging-face-models-fails/25034</a></p>
","huggingface"
"74207115","Any ideas on how we can convert a model from huggingface (transformers library )to tensorflow lite?","2022-10-26 11:36:48","","2","318","<android><tensorflow-lite><huggingface-transformers><huggingface><tflite>","<p>I want to convert CamembertForQuestionAnswering model to tensoflow lite
i download it from huggingface platform, because when i want to save the model locally it gives me the model with 'bin' format.</p>
<ul>
<li>when i try to convert the model it gives me this error : AttributeError: 'CamembertForQuestionAnswering' object has no attribute 'call' by using tf_model.h5 file.</li>
<li>Also i can't load it using : tf.keras.models.load_model() it gives me : ValueError: No model config found in the file at &lt;tensorflow.python.platform.gfile.GFile object at 0x7f27cceb1810&gt;.</li>
<li>when i want to save the transformers model locally it gives me the model with 'bin' format, so i download it from the platform.</li>
</ul>
","huggingface"
"74168437","Huggingface Dataset.map shows red progress bar when batched=True","2022-10-23 03:12:42","74265834","0","912","<huggingface><huggingface-datasets>","<p>I have the following simple code copied from Huggingface examples:</p>
<pre><code>model_checkpoint = &quot;distilgpt2&quot;

from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)

def tokenize_function(examples):
    return tokenizer(examples[&quot;text&quot;])

from datasets import load_dataset
datasets = load_dataset('wikitext', 'wikitext-2-raw-v1')
tokenized_datasets = datasets.map(tokenize_function, batched=False, num_proc=4, remove_columns=[&quot;text&quot;])
</code></pre>
<p>When I set <code>batched=False</code> then the progress bar shows green color which indicates success, but if I set <code>batched=True</code> then the progress bar shows red color and does not reach 100%. Does that mean my <code>map</code> function failed or something else?</p>
","huggingface"
"74146965","How to efficiently convert a large parallel corpus to a Huggingface dataset to train an EncoderDecoderModel?","2022-10-20 22:33:31","74230698","1","1764","<python><parallel-processing><pytorch><dataset><huggingface>","<h1>Typical EncoderDecoderModel that works on a Pre-coded Dataset</h1>
<p>The code snippet snippet as below is frequently used to train an <a href=""https://huggingface.co/docs/transformers/model_doc/encoder-decoder"" rel=""nofollow noreferrer""><code>EncoderDecoderModel</code></a> from Huggingface's transformer library</p>
<pre><code>from transformers import EncoderDecoderModel
from transformers import PreTrainedTokenizerFast

multibert = EncoderDecoderModel.from_encoder_decoder_pretrained(
    &quot;bert-base-multilingual-uncased&quot;, &quot;bert-base-multilingual-uncased&quot;
)


tokenizer = PreTrainedTokenizerFast.from_pretrained(&quot;bert-base-multilingual-uncased&quot;)

...
</code></pre>
<h1>And a pre-processed/coded dataset can be used to train the model as such, when using the <code>wmt14</code> dataset:</h1>
<pre><code>import datasets

train_data = datasets.load_dataset(&quot;wmt14&quot;, &quot;de-en&quot;, split=&quot;train&quot;)
val_data = datasets.load_dataset(&quot;wmt14&quot;, &quot;de-en&quot;, split=&quot;validation[:10%]&quot;)


from functools import partial

def process_data_to_model_inputs(batch, encoder_max_length=512, decoder_max_length=512, batch_size=2): 
    inputs = tokenizer([segment[&quot;en&quot;] for segment in batch['translation']], 
                       padding=&quot;max_length&quot;, truncation=True, max_length=encoder_max_length)
    outputs = tokenizer([segment[&quot;de&quot;] for segment in batch['translation']], 
                       padding=&quot;max_length&quot;, truncation=True, max_length=encoder_max_length)


    batch[&quot;input_ids&quot;] = inputs.input_ids
    batch[&quot;attention_mask&quot;] = inputs.attention_mask
    batch[&quot;decoder_input_ids&quot;] = outputs.input_ids
    batch[&quot;decoder_attention_mask&quot;] = outputs.attention_mask
    batch[&quot;labels&quot;] = outputs.input_ids.copy()

    # because BERT automatically shifts the labels, the labels correspond exactly to `decoder_input_ids`. 
    # We have to make sure that the PAD token is ignored
    batch[&quot;labels&quot;] = [[-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in batch[&quot;labels&quot;]]
    return batch


def munge_dataset_to_pacify_bert(dataset, encoder_max_length=512, decoder_max_length=512, batch_size=2):
    bert_wants_to_see = [&quot;input_ids&quot;, &quot;attention_mask&quot;, &quot;decoder_input_ids&quot;, 
                         &quot;decoder_attention_mask&quot;, &quot;labels&quot;]
    
    _process_data_to_model_inputs = partial(process_data_to_model_inputs, 
                                                encoder_max_length=encoder_max_length, 
                                                decoder_max_length=decoder_max_length, 
                                                batch_size=batch_size
                                           )
    dataset = dataset.map(_process_data_to_model_inputs, 
                           batched=True, 
                           batch_size=batch_size
                          )
    dataset.set_format(type=&quot;torch&quot;, columns=bert_wants_to_see)
    return dataset

train_data = munge_dataset_to_pacify_bert(train_data)
val_data = munge_dataset_to_pacify_bert(val_data)
</code></pre>
<h1>Then the training can be done easily as such:</h1>
<pre><code>from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments


# set training arguments - these params are not really tuned, feel free to change
training_args = Seq2SeqTrainingArguments(
    output_dir=&quot;./&quot;,
    evaluation_strategy=&quot;steps&quot;,
    ...
)


# instantiate trainer
trainer = Seq2SeqTrainer(
    model=multibert,
    tokenizer=tokenizer,
    args=training_args,
    train_dataset=train_data,
    eval_dataset=val_data,
)

trainer.train()
</code></pre>
<p>A working example can be found on something like: <a href=""https://www.kaggle.com/code/alvations/neural-plasticity-bert2bert-on-wmt14"" rel=""nofollow noreferrer"">https://www.kaggle.com/code/alvations/neural-plasticity-bert2bert-on-wmt14</a></p>
<h1>However, parallel data used to train an EncoderDecoderModel usually exists as <code>.txt</code> or <code>.tsv</code> files, not a pre-coded dataset</h1>
<p>Given a large <code>.tsv</code> file (e.g. 1 billion lines), e.g.</p>
<pre><code>hello world\tHallo Welt
how are you?\twie gehts?
...\t...
</code></pre>
<h4>Step 1: we can convert into the parquet / pyarrow format, one can do something like:</h4>
<pre><code>import vaex  # Using vaex 
import sys

filename = &quot;train.en-de.tsv&quot;

df = vaex.from_csv(filename, sep=&quot;\t&quot;, header=None, names=[&quot;src&quot;, &quot;trg&quot;], convert=True, chunk_size=50_000_000)

df.export(f&quot;{filename}.parquet&quot;)
</code></pre>
<h4>Step 2: Then we will can read it into a Pyarrow table to fit into the <code>datasets.Dataset</code> object and use the <code>munge_dataset_to_pacify_bert()</code> as shown above, e.g</h4>
<pre><code>from datasets import Dataset, load_from_disk
import pyarrow as pa

_ds = Dataset(pa.compute.drop_null(pa.parquet.read_table('train.en-de.tsv.parquet')
_ds.save_to_disk('train.en-de.tsv.parquet.hfdataset')

_ds = load_from_disk('train.en-de.tsv.parquet.hfdataset')

train_data = munge_dataset_to_pacify_bert(_ds)

train_data.save_to_disk('train.en-de.tsv.parquet.hfdataset')

</code></pre>
<h4>While the process above works well for small-ish dataset, e.g. 1-5 million lines of data, when the scale of the goes to 500 million to 1 billion, the last <code>.save_to_disk()</code> function seems like it is runningf &quot;forever&quot; and the end is no where in sight.</h4>
<p>Breaking down the steps in the <code>munge_dataset_to_pacify_bert()</code>, there are 2 sub-functions:</p>
<ul>
<li><code>dataset.map(_process_data_to_model_inputs, batched=True, batch_size=batch_size)</code></li>
<li><code>dataset.set_format(type=&quot;torch&quot;, columns=bert_wants_to_see)</code></li>
</ul>
<p>For the <code>.map()</code> process, it's possible to scale in parallel threads by specifying by</p>
<pre><code>dataset.map(_process_data_to_model_inputs, 
    batched=True, batch_size=100, 
    num_proc=32  # num of parallel threads.
    )
</code></pre>
<p>And when I tried to process with</p>
<ul>
<li><code>num_proc=32</code></li>
<li><code>batch_size=100</code></li>
</ul>
<p>The <code>.map()</code> function finishes the processing of 500 million lines in 18 hours of compute time on Intel Xeon E5-2686 @ 2.3GHz with 32 processor cores, optimally.</p>
<p>But somehow the <code>.map()</code> function created 32 temp <code>.arrow</code> files and 128 <code>tmp...</code> binary files. Seemingly the last <code>save_to_disk</code> function has been running for more than 10+ hours and have not finished combining the temp files in parts to save the final HF Dataset to disk.</p>
<hr />
<p>Given the above context, my questions in parts are:</p>
<h2>Question (Part 1): When the mapping function ends and created the temp <code>.arrow</code> and <code>tmp...</code> files, is there a way to read these individually instead of try to save them into a final directory using the <code>save_to_disk()</code> function?</h2>
<hr />
<h2>Question (Part 2): Why is the <code>save_to_disk()</code> function so slow after the mapping and how can the mapped processed data be saved in a faster manner?</h2>
<hr />
<h2>Question (Part 3): Is there a way to avoid the <code>.set_format()</code> function after the <code>.map()</code> and make it part of the <code>_process_data_to_model_inputs</code> function?</h2>
<hr />
","huggingface"
"74107942","How to get all records from a huggingface dataset in a single csv?","2022-10-18 08:28:57","","-1","570","<python><dataset><huggingface><huggingface-datasets>","<p>The cnn_dailymail dataset contains 3 fields - ID,Text,Highlights</p>
<p>I wanted to get all the records in the <a href=""https://huggingface.co/datasets/cnn_dailymail"" rel=""nofollow noreferrer"">cnn_dailymail</a> dataset in a single csv , but have been unsuccessful in finding a way.</p>
<p>Currently I have downloaded the dataset locally from <a href=""https://huggingface.co/datasets/cnn_dailymail/tree/main/data"" rel=""nofollow noreferrer"">here</a> (the file called cnn_stories.tgz).
I have unzipped the .tgz and got a folder of <code>.story</code> files that has the text and summary for each record in the dataset. Because there are 100k records, I have got 100k <code>.story</code> files</p>
<p>The problem with such extraction is I have got 100k story files , each has a text and it's summary. Ideally I wanted it in a csv format where there are 2 columns - one for the article and the next for the highlights -- and he csv to contain 100k rows.</p>
<p>I want to only do this using a locally downloaded dataset(due to proxy issues in my work system)</p>
<p><em><strong>Alternative way to ask the question</strong></em>: How to use load_dataset() funtion from the datasets library to load a dataset from a locally downloaded <code>.tgz</code> file</p>
","huggingface"
"74091703","NoCredentialsError: Unable to locate credentials in Hugging Face Library","2022-10-17 01:03:13","","0","1855","<python><artificial-intelligence><huggingface>","<p>So i am not using Huggin face a lot for my ai but I've discover that you can train you're ai with it so it tried to use my machine to train it but i kept having that error:</p>
<pre><code>PS C:\Users\gboss\OneDrive\Bureau\Ai training&gt; &amp; C:/Users/gboss/AppData/Local/Programs/Python/Python310/python.exe &quot;c:/Users/gboss/OneDrive/Bureau/Ai training/AiTraining.py&quot;
Traceback (most recent call last):
  File &quot;c:\Users\gboss\OneDrive\Bureau\Ai training\AiTraining.py&quot;, line 8, in &lt;module&gt;
    role = iam_client.get_role(RoleName='{IAM_ROLE_WITH_SAGEMAKER_PERMISSIONS}')['Role']['Arn']
  File &quot;C:\Users\gboss\AppData\Local\Programs\Python\Python310\lib\site-packages\botocore\client.py&quot;, line 514, in _api_call
    return self._make_api_call(operation_name, kwargs)
  File &quot;C:\Users\gboss\AppData\Local\Programs\Python\Python310\lib\site-packages\botocore\client.py&quot;, line 921, in _make_api_call
    http, parsed_response = self._make_request(
  File &quot;C:\Users\gboss\AppData\Local\Programs\Python\Python310\lib\site-packages\botocore\client.py&quot;, line 944, in _make_request
    return self._endpoint.make_request(operation_model, request_dict)
  File &quot;C:\Users\gboss\AppData\Local\Programs\Python\Python310\lib\site-packages\botocore\endpoint.py&quot;, line 119, in make_request
    return self._send_request(request_dict, operation_model)
  File &quot;C:\Users\gboss\AppData\Local\Programs\Python\Python310\lib\site-packages\botocore\endpoint.py&quot;, line 198, in _send_request
    request = self.create_request(request_dict, operation_model)
  File &quot;C:\Users\gboss\AppData\Local\Programs\Python\Python310\lib\site-packages\botocore\endpoint.py&quot;, line 134, in create_request
    self._event_emitter.emit(
  File &quot;C:\Users\gboss\AppData\Local\Programs\Python\Python310\lib\site-packages\botocore\hooks.py&quot;, line 412, in emit
    return self._emitter.emit(aliased_event_name, **kwargs)
  File &quot;C:\Users\gboss\AppData\Local\Programs\Python\Python310\lib\site-packages\botocore\hooks.py&quot;, line 256, in emit
    return self._emit(event_name, kwargs)
  File &quot;C:\Users\gboss\AppData\Local\Programs\Python\Python310\lib\site-packages\botocore\hooks.py&quot;, line 239, in _emit
    response = handler(**kwargs)
  File &quot;C:\Users\gboss\AppData\Local\Programs\Python\Python310\lib\site-packages\botocore\signers.py&quot;, line 105, in handler
    return self.sign(operation_name, request)
  File &quot;C:\Users\gboss\AppData\Local\Programs\Python\Python310\lib\site-packages\botocore\signers.py&quot;, line 189, in sign
    auth.add_auth(request)
  File &quot;C:\Users\gboss\AppData\Local\Programs\Python\Python310\lib\site-packages\botocore\auth.py&quot;, line 418, in add_auth
    raise NoCredentialsError()
botocore.exceptions.NoCredentialsError: Unable to locate credentials 
</code></pre>
<p>and let's say that i can't find why because i don't use huggin face a lot</p>
<p>the code:</p>
<pre><code>import sagemaker
import boto3
from sagemaker.huggingface import HuggingFace

# gets role for executing training job

iam_client = boto3.client('iam')
role = iam_client.get_role(RoleName='{IAM_ROLE_WITH_SAGEMAKER_PERMISSIONS}')['Role']['Arn']
hyperparameters = {
    'model_name_or_path':'ZipperXYZ/DialoGPT-medium-TheWorldMachineExpressive2',
    'output_dir':'/opt/ml/model'
    # add your remaining hyperparameters
    # more info here https://github.com/huggingface/transformers/tree/v4.17.0/examples/pytorch/language-modeling
}

# git configuration to download our fine-tuning script
git_config = {'repo': 'https://github.com/huggingface/transformers.git','branch': 'v4.17.0'}

# creates Hugging Face estimator
huggingface_estimator = HuggingFace(
    entry_point='run_clm.py',
    source_dir='./examples/pytorch/language-modeling',
    instance_type='ml.p3.2xlarge',
    instance_count=1,
    role=role,
    git_config=git_config,
    transformers_version='4.17.0',
    pytorch_version='1.10.2',
    py_version='py38',
    hyperparameters = hyperparameters
)

# starting the train job
huggingface_estimator.fit()
</code></pre>
","huggingface"
"74088262","Build Docker image using Hugging Face's cache","2022-10-16 15:16:55","74088665","2","3405","<python><docker><containers><huggingface>","<p>Hugging Face has a caching system to load models from any app.</p>
<p><a href=""https://huggingface.co/docs/datasets/cache"" rel=""nofollow noreferrer"">https://huggingface.co/docs/datasets/cache</a></p>
<p>This is useful in most cases, but not when building an image in Docker, as the cache must be downloaded everytime.</p>
<p>How can I set the cache files in the Docker app's folder, and build the image properly?</p>
<p>In this way, the cache files won't be downloaded at each build.</p>
","huggingface"
"74046426","How to save a SetFit trainer locally after training","2022-10-12 18:23:18","74210080","3","3605","<sentence-similarity><huggingface><sentence-transformers>","<p>I am working on an HPC with no internet access on worker nodes and the only option to save a SetFit trainer after training, is to push it to HuggingFace hub. How do I go about saving it locally to disk?</p>
<p><a href=""https://github.com/huggingface/setfit"" rel=""nofollow noreferrer"">https://github.com/huggingface/setfit</a></p>
","huggingface"
"74044231","HuggingFace Trainer do predictions","2022-10-12 15:14:06","","2","7252","<nlp><huggingface-transformers><huggingface><fine-tuning>","<p>I've been fine-tuning a Model from HuggingFace via the <code>Trainer</code>-Class.
I went through the Training Process via <code>trainer.train()</code> and also tested it with <code>trainer.evaluate()</code>.</p>
<p>My question is how I can run the Model on specific data.
In case of a classification text I'm looking for sth like this:</p>
<pre><code>trainer.predict('This text is about football')
output = 'Sports'
</code></pre>
<p>Do I need to save the Model first or is there a command I can use directly? What's the most simple way on running the fine-tuned Model?</p>
","huggingface"
"74027562","How to define custom entites in HuggingFace Transformers NER pipeline?","2022-10-11 11:44:19","","2","1052","<huggingface-transformers><named-entity-recognition><huggingface>","<p>I am trying to train HuggingFace Transformers NER on custom dataset with custom entities. Is it possible to define custom entities in HuggingFace?</p>
<p>For examples, simpletransformers NERModel offers to define custom labels while training.</p>
<p>`custom_labels = [&quot;O&quot;, &quot;B-SPELL&quot;, &quot;I-SPELL&quot;, &quot;B-PER&quot;, &quot;I-PER&quot;, &quot;B-ORG&quot;, &quot;I-ORG&quot;, &quot;B-PLACE&quot;, &quot;I-PLACE&quot;]</p>
<p>model = NERModel(
&quot;bert&quot;, &quot;bert-cased-base&quot;, labels=custom_labels
)`</p>
<p>Do we have something similar in HuggingFace Transformers? Or I have to look for alternatives like Spacy or SparkNLP? I really prefer HuggingFace Transformers way due to possibility to easily switch between different models.</p>
","huggingface"
"74018571","git clone - https://huggingface.co/spacy/en_core_web_trf","2022-10-10 17:22:12","","1","208","<python><spacy><huggingface>","<p>I installed en_core_web_trf through clone:</p>
<pre><code>!git clone https://huggingface.co/spacy/en_core_web_trf
</code></pre>
<p>Now I'm trying to run the following:</p>
<pre><code>import spacy
nlp = spacy.load(&quot;en_core_web_trf&quot;)
</code></pre>
<p>Then get the following error:</p>
<pre><code>UnicodeDecodeError
Traceback (most recent call last)
c:\Users\sg18551\Documents\Skunkworks Project\Automate Open-End Responses\new\ML Model.ipynb Cell 3 in &lt;cell line: 2&gt;()
      1 import spacy
----&gt; 2 nlp = spacy.load(&quot;en_core_web_trf&quot;)

File c:\Users\sg18551\AppData\Local\Programs\Python\Python310\lib\site-packages\spacy\__init__.py:54, in load(name, vocab, disable, enable, exclude, config)
     30 def load(
     31     name: Union[str, Path],
     32     *,
   (...)
     37     config: Union[Dict[str, Any], Config] = util.SimpleFrozenDict(),
     38 ) -&gt; Language:
     39     &quot;&quot;&quot;Load a spaCy model from an installed package or a local path.
     40 
     41     name (str): Package name or model path.
   (...)
     52     RETURNS (Language): The loaded nlp object.
     53     &quot;&quot;&quot;
---&gt; 54     return util.load_model(
     55         name,
     56         vocab=vocab,
     57         disable=disable,
     58         enable=enable,
     59         exclude=exclude,
...
---&gt; 79 return _unpackb(packed, **kwargs)

File c:\Users\sg18551\AppData\Local\Programs\Python\Python310\lib\site-packages\srsly\msgpack\_unpacker.pyx:191, in srsly.msgpack._unpacker.unpackb()

UnicodeDecodeError: 'utf-8' codec can't decode byte 0xa3 in position 1547: invalid start byte
</code></pre>
","huggingface"
"74018095","How to know if HuggingFace's pipeline text input exceeds 512 tokens","2022-10-10 16:38:41","74465637","3","1939","<huggingface-transformers><huggingface-tokenizers><huggingface>","<p>I've finetuned a Huggingface BERT model for Named Entity Recognition based on <code>'bert-base-uncased'</code>. I perform inference like this:</p>
<pre><code>from transformers import pipeline
ner_pipeline = pipeline('token-classification', model=model_folder, tokenizer=model_folder)
out = ner_pipeline(text, aggregation_strategy='simple')
</code></pre>
<p>I want to obtain results on very long texts, and since I know of the 512 token maximum capacity for both training and inference, I split my <code>text</code>s in smaller chunks before passing those to the <code>ner_pipeline</code>.</p>
<p>But, how do I split the text without actually tokenizing the texts myself in order to check for the length of each chunk? I want to make them as long as possible, but at the same time I don't want to exceed the maximum 512 tokens, risking that no predictions are computed on what's left of the sentence.</p>
<p>Is there a way to know if the texts I'm feeding exceed the 512 maximum tokens?</p>
","huggingface"
"74014379","How to fine-tune gpt-j using Huggingface Trainer","2022-10-10 11:42:41","74021554","4","3611","<python><machine-learning><pytorch><huggingface-transformers><huggingface>","<p>I'm attempting to fine-tune gpt-j using the huggingface trainer and failing miserably. I followed the example that references bert, but of course, the gpt-j model isn't exactly like the bert model.</p>
<p>The error indicates that the model isn't producing a loss, which is great, except that I have no idea how to make it generate a loss or how to change what the trainer is expecting.</p>
<p>I'm using Transformers 4.22.2. I would like to get this working on a CPU before I try to do anything on Paperspace with a GPU. I did make an initial attempt there using a GPU that received the same error, with slightly different code to use cuda.</p>
<p>I suspect that my approach is entirely wrong. I found a very old example of fine-tuning gpt-j using 8-bit quantization, but even that repository says it is deprecated.</p>
<p>I'm unsure if my mistake is in using the compute_metrics() I found in the bert example or if it is something else. Any advice would be appreciated. Or, maybe it is an issue with the labels I provide the config, but I've tried different permutations.</p>
<p>I understand what a loss function is, but I don't know how it is supposed to be configured in this case.</p>
<p>My Code:</p>
<pre><code>from transformers import Trainer, TrainingArguments, AutoModelForCausalLM
from transformers import GPTJForCausalLM, AutoTokenizer
from datasets import load_dataset
import time
import torch
import os
import numpy as np
import evaluate
import sklearn

start = time.time()

GPTJ_FINE_TUNED_FILE = &quot;./fine_tuned_models/gpt-j-6B&quot;

print(&quot;Loading model&quot;)
model = GPTJForCausalLM.from_pretrained(&quot;EleutherAI/gpt-j-6B&quot;, low_cpu_mem_usage=True)
model.config.pad_token_id = model.config.eos_token_id

print(&quot;Loading tokenizer&quot;)
tokenizer = AutoTokenizer.from_pretrained(&quot;EleutherAI/gpt-j-6B&quot;)
tokenizer.pad_token = tokenizer.eos_token

print(&quot;Loading dataset&quot;)
current_dataset = load_dataset(&quot;wikitext&quot;, 'wikitext-103-v1')
current_dataset['train'] = current_dataset['train'].select(range(1200))


def tokenize_function(examples):
    current_tokenizer_result = tokenizer(examples[&quot;text&quot;], padding=&quot;max_length&quot;, truncation=True)
    return current_tokenizer_result


print(&quot;Splitting and tokenizing dataset&quot;)
tokenized_datasets = current_dataset.map(tokenize_function, batched=True)
small_train_dataset = tokenized_datasets[&quot;train&quot;].select(range(100))

print(&quot;Preparing training arguments&quot;)

training_args = TrainingArguments(output_dir=GPTJ_FINE_TUNED_FILE,
                                  report_to='all',
                                  logging_dir='./logs',
                                  per_device_train_batch_size=1,
                                  label_names=['input_ids', 'attention_mask'],  # 'logits', 'past_key_values'
                                  num_train_epochs=1,
                                  no_cuda=True
                                  )

metric = evaluate.load(&quot;accuracy&quot;)


def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)


trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset
)

print(&quot;Starting training&quot;)
trainer.train()
print(f&quot;Finished fine-tuning in {time.time() - start}&quot;)
</code></pre>
<p>Which leads to the error and stacktrace:</p>
<pre><code>  File &quot;xxx\ft_v3.py&quot;, line 66, in &lt;module&gt;
  File &quot;xxx\venv\lib\site-packages\transformers\trainer.py&quot;, line 1521, in train
    return inner_training_loop(
  File &quot;xxx\venv\lib\site-packages\transformers\trainer.py&quot;, line 1763, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File &quot;xxx\venv\lib\site-packages\transformers\trainer.py&quot;, line 2499, in training_step
    loss = self.compute_loss(model, inputs)
  File &quot;xxx\venv\lib\site-packages\transformers\trainer.py&quot;, line 2544, in compute_loss
    raise ValueError(
ValueError: The model did not return a loss from the inputs, only the following keys: logits,past_key_values. For reference, the inputs it received are input_ids,attention_mask.
</code></pre>
","huggingface"
"73975817","How do a put a different classifier on top of BertForSequenceClassification?","2022-10-06 14:47:35","73976116","0","301","<machine-learning><pytorch><huggingface-transformers><huggingface>","<p>I have a huggingface model:</p>
<pre><code>model_name = 'bert-base-uncased'
model = BertForSequenceClassification.from_pretrained(model_name, num_labels=1).to(device)
</code></pre>
<p>How can I change the default classifier head? Since it's only a single LinearClassifier. I found <a href=""https://github.com/huggingface/transformers/issues/1001"" rel=""nofollow noreferrer"">this issue</a> in the huggingface github which said:</p>
<blockquote>
<p>You can also replace self.classifier with your own model.</p>
<pre><code>model = BertForSequenceClassification.from_pretrained(&quot;bert-base-multilingual-cased&quot;)
model.classifier = new_classifier
</code></pre>
<p>where new_classifier is any pytorch model that you want.</p>
</blockquote>
<p>However, I can't figure out how the structure of the <code>new_classifier</code> should look like (in particular the inputs and outputs so it can handle batches).</p>
","huggingface"
"73964234","How do I convert a list of dictionaries to a Huggingface Dataset object?","2022-10-05 17:34:17","76277622","1","3196","<python><huggingface><huggingface-datasets>","<p>I have a list of dictionaries:</p>
<pre><code>print(type(train_dataset))
&gt;&gt;&gt; &lt;class 'list'&gt;

print(len(train_dataset))
&gt;&gt;&gt; 4000

train_dataset[0]
&gt;&gt;&gt;
{'id': '7',
 'question': {'stem': 'Who is A',
  'choices': [{'text': 'A is X', 'label': 'A'},
   {'text': 'A is not B', 'label': 'D'}]},
 'answerKey': 'D'}
</code></pre>
<p>How can I convert this to a huggingface Dataset object? From their <a href=""https://huggingface.co/docs/datasets/v1.1.1/loading_datasets.html"" rel=""nofollow noreferrer"">website</a> it seems like you can only convert pandas df (<code>dataset = Dataset.from_pandas(df)</code>) or a dictionary (<code> dataset = Dataset.from_dict(my_dict)</code>), but it's not clear how to use a list of dictionaries</p>
","huggingface"
"73960201","ModuleNotFoundError: No module named 'huggingface_hub.utils' using Anaconda","2022-10-05 12:15:56","","4","25510","<python><anaconda><modulenotfounderror><huggingface>","<p>I'm trying to execute the example code of the huggingface website:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import GPTJTokenizer, TFGPTJModel
import tensorflow as tf

tokenizer = GPTJTokenizer.from_pretrained(&quot;EleutherAI/gpt-j-6B&quot;)
model = TFGPTJModel.from_pretrained(&quot;EleutherAI/gpt-j-6B&quot;)

inputs = tokenizer(&quot;Hello, my dog is cute&quot;, return_tensors=&quot;tf&quot;)
outputs = model(inputs)

last_hidden_states = outputs.last_hidden_state
</code></pre>
<p>I'm using anaconda and I installed the transformers package beforehand with <code>conda install -c huggingface transformers</code> as explained in the <a href=""https://huggingface.co/docs/transformers/installation#install-with-conda"" rel=""noreferrer"">documentation</a>. But I still get this error, when I'm trying to execute the code. Following error message pops up: <code>ModuleNotFoundError: No module named 'huggingface_hub.utils'</code></p>
<p>How to resolve this error?</p>
","huggingface"
"73952853","Getting an error install a package on the Terminal to use Hugging Face In VS Cod","2022-10-04 19:37:21","73952880","1","831","<tensorflow2.0><torch><huggingface-transformers><huggingface-tokenizers><huggingface>","<p>I am using the steps from the Hugging Face website (<a href=""https://huggingface.co/docs/transformers/installation"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/installation</a>) in order to start using hugging face in Visual Studio Code and install all the transformers.</p>
<p>I was on the last process, where I had to type &quot;pip install transformers[flax]&quot;, then I got an error, so I installed rust-land, however, I still ended up getting an error;</p>
<pre><code>Requirement already satisfied: transformers[flax] in c:\users\user\desktop\artificial intelligence\.env\lib\site-packages (4.22.2)
Requirement already satisfied: filelock in c:\users\user\desktop\artificial intelligence\.env\lib\site-packages (from transformers[flax]) (3.8.0)
Requirement already satisfied: requests in c:\users\user\desktop\artificial intelligence\.env\lib\site-packages (from transformers[flax]) (2.28.1)
Requirement already satisfied: tokenizers!=0.11.3,&lt;0.13,&gt;=0.11.1 in c:\users\user\desktop\artificial intelligence\.env\lib\site-packages (from transformers[flax]) (0.12.1)
Requirement already satisfied: huggingface-hub&lt;1.0,&gt;=0.9.0 in c:\users\user\desktop\artificial intelligence\.env\lib\site-packages (from transformers[flax]) (0.10.0)
Requirement already satisfied: packaging&gt;=20.0 in c:\users\user\desktop\artificial intelligence\.env\lib\site-packages (from transformers[flax]) (21.3)
Requirement already satisfied: tqdm&gt;=4.27 in c:\users\user\desktop\artificial intelligence\.env\lib\site-packages (from transformers[flax]) (4.64.1)
Requirement already satisfied: regex!=2019.12.17 in c:\users\user\desktop\artificial intelligence\.env\lib\site-packages (from 
transformers[flax]) (2022.9.13)
Requirement already satisfied: numpy&gt;=1.17 in c:\users\user\desktop\artificial intelligence\.env\lib\site-packages (from transformers[flax]) (1.23.3)
Requirement already satisfied: pyyaml&gt;=5.1 in c:\users\user\desktop\artificial intelligence\.env\lib\site-packages (from transformers[flax]) (6.0)
Collecting transformers[flax]
  Using cached transformers-4.22.1-py3-none-any.whl (4.9 MB)
  Using cached transformers-4.22.0-py3-none-any.whl (4.9 MB)
  Using cached transformers-4.21.3-py3-none-any.whl (4.7 MB)
  Using cached transformers-4.21.2-py3-none-any.whl (4.7 MB)
  Using cached transformers-4.21.1-py3-none-any.whl (4.7 MB)
  Using cached transformers-4.21.0-py3-none-any.whl (4.7 MB)
  Using cached transformers-4.20.1-py3-none-any.whl (4.4 MB)
  Using cached transformers-4.20.0-py3-none-any.whl (4.4 MB)
  Using cached transformers-4.19.4-py3-none-any.whl (4.2 MB)
  Using cached transformers-4.19.3-py3-none-any.whl (4.2 MB)
  Using cached transformers-4.19.2-py3-none-any.whl (4.2 MB)
  Using cached transformers-4.19.1-py3-none-any.whl (4.2 MB)
  Using cached transformers-4.19.0-py3-none-any.whl (4.2 MB)
  Using cached transformers-4.18.0-py3-none-any.whl (4.0 MB)
Collecting sacremoses
  Using cached sacremoses-0.0.53-py3-none-any.whl
Collecting jax!=0.3.2,&gt;=0.2.8
  Using cached jax-0.3.21.tar.gz (1.1 MB)
  Preparing metadata (setup.py) ... done
Collecting flax&gt;=0.3.5
  Using cached flax-0.6.1-py3-none-any.whl (185 kB)
Collecting optax&gt;=0.0.8
  Using cached optax-0.1.3-py3-none-any.whl (145 kB)
Collecting transformers[flax]
  Using cached transformers-4.17.0-py3-none-any.whl (3.8 MB)
  Using cached transformers-4.16.2-py3-none-any.whl (3.5 MB)
  Using cached transformers-4.16.1-py3-none-any.whl (3.5 MB)
  Using cached transformers-4.16.0-py3-none-any.whl (3.5 MB)
  Using cached transformers-4.15.0-py3-none-any.whl (3.4 MB)
Collecting tokenizers&lt;0.11,&gt;=0.10.1
  Using cached tokenizers-0.10.3.tar.gz (212 kB)
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
  Preparing metadata (pyproject.toml) ... done
Collecting transformers[flax]
  Using cached transformers-4.14.1-py3-none-any.whl (3.4 MB)
  Using cached transformers-4.13.0-py3-none-any.whl (3.3 MB)
  Using cached transformers-4.12.5-py3-none-any.whl (3.1 MB)
  Using cached transformers-4.12.4-py3-none-any.whl (3.1 MB)
  Using cached transformers-4.12.3-py3-none-any.whl (3.1 MB)
  Using cached transformers-4.12.2-py3-none-any.whl (3.1 MB)
  Using cached transformers-4.12.1-py3-none-any.whl (3.1 MB)
  Using cached transformers-4.12.0-py3-none-any.whl (3.1 MB)
  Using cached transformers-4.11.3-py3-none-any.whl (2.9 MB)
  Using cached transformers-4.11.2-py3-none-any.whl (2.9 MB)
  Using cached transformers-4.11.1-py3-none-any.whl (2.9 MB)
  Using cached transformers-4.11.0-py3-none-any.whl (2.9 MB)
  Using cached transformers-4.10.3-py3-none-any.whl (2.8 MB)
  Using cached transformers-4.10.2-py3-none-any.whl (2.8 MB)
  Using cached transformers-4.10.1-py3-none-any.whl (2.8 MB)
  Using cached transformers-4.10.0-py3-none-any.whl (2.8 MB)
  Using cached transformers-4.9.2-py3-none-any.whl (2.6 MB)
Collecting huggingface-hub==0.0.12
  Using cached huggingface_hub-0.0.12-py3-none-any.whl (37 kB)
Collecting transformers[flax]
  Using cached transformers-4.9.1-py3-none-any.whl (2.6 MB)
  Using cached transformers-4.9.0-py3-none-any.whl (2.6 MB)
  Using cached transformers-4.8.2-py3-none-any.whl (2.5 MB)
  Using cached transformers-4.8.1-py3-none-any.whl (2.5 MB)
  Using cached transformers-4.8.0-py3-none-any.whl (2.5 MB)
  Using cached transformers-4.7.0-py3-none-any.whl (2.5 MB)
Collecting huggingface-hub==0.0.8
  Using cached huggingface_hub-0.0.8-py3-none-any.whl (34 kB)
Collecting transformers[flax]
  Using cached transformers-4.6.1-py3-none-any.whl (2.2 MB)
  Using cached transformers-4.6.0-py3-none-any.whl (2.3 MB)
  Using cached transformers-4.5.1-py3-none-any.whl (2.1 MB)
  Using cached transformers-4.5.0-py3-none-any.whl (2.1 MB)
  Using cached transformers-4.4.2-py3-none-any.whl (2.0 MB)
  Using cached transformers-4.4.1-py3-none-any.whl (2.1 MB)
  Using cached transformers-4.4.0-py3-none-any.whl (2.1 MB)
  Using cached transformers-4.3.3-py3-none-any.whl (1.9 MB)
  Using cached transformers-4.3.2-py3-none-any.whl (1.8 MB)
  Using cached transformers-4.3.1-py3-none-any.whl (1.8 MB)
  Using cached transformers-4.3.0-py3-none-any.whl (1.8 MB)
  Using cached transformers-4.2.2-py3-none-any.whl (1.8 MB)
Collecting tokenizers==0.9.4
  Using cached tokenizers-0.9.4.tar.gz (184 kB)
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
  Preparing metadata (pyproject.toml) ... done
Collecting transformers[flax]
  Using cached transformers-4.2.1-py3-none-any.whl (1.8 MB)
  Using cached transformers-4.2.0-py3-none-any.whl (1.8 MB)
  Using cached transformers-4.1.1-py3-none-any.whl (1.5 MB)
  Using cached transformers-4.1.0-py3-none-any.whl (1.5 MB)
  Using cached transformers-4.0.1-py3-none-any.whl (1.4 MB)
Collecting flax==0.2.2
  Using cached flax-0.2.2-py3-none-any.whl (148 kB)
Collecting transformers[flax]
  Using cached transformers-4.0.0-py3-none-any.whl (1.4 MB)
  Using cached transformers-3.5.1-py3-none-any.whl (1.3 MB)
Requirement already satisfied: protobuf in c:\users\user\desktop\artificial intelligence\.env\lib\site-packages (from transformers[flax]) (3.19.6)
Collecting sentencepiece==0.1.91
  Using cached sentencepiece-0.1.91.tar.gz (500 kB)
  Preparing metadata (setup.py) ... done
Collecting tokenizers==0.9.3
  Using cached tokenizers-0.9.3.tar.gz (172 kB)
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
  Preparing metadata (pyproject.toml) ... done
Collecting transformers[flax]
  Using cached transformers-3.5.0-py3-none-any.whl (1.3 MB)
  Using cached transformers-3.4.0-py3-none-any.whl (1.3 MB)
Collecting tokenizers==0.9.2
  Using cached tokenizers-0.9.2.tar.gz (170 kB)
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
  Preparing metadata (pyproject.toml) ... done
Collecting sentencepiece!=0.1.92
  Using cached sentencepiece-0.1.97-cp310-cp310-win_amd64.whl (1.1 MB)
Collecting transformers[flax]
  Using cached transformers-3.3.1-py3-none-any.whl (1.1 MB)
WARNING: transformers 3.3.1 does not provide the extra 'flax'
Collecting tokenizers==0.8.1.rc2
  Using cached tokenizers-0.8.1rc2.tar.gz (97 kB)
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
  Preparing metadata (pyproject.toml) ... done
Requirement already satisfied: colorama in c:\users\user\desktop\artificial intelligence\.env\lib\site-packages (from tqdm&gt;=4.27-&gt;transformers[flax]) (0.4.5)
Requirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in c:\users\user\desktop\artificial intelligence\.env\lib\site-packages (from packaging&gt;=20.0-&gt;transformers[flax]) (3.0.9)
Requirement already satisfied: idna&lt;4,&gt;=2.5 in c:\users\user\desktop\artificial intelligence\.env\lib\site-packages (from requests-&gt;transformers[flax]) (3.4)
Requirement already satisfied: charset-normalizer&lt;3,&gt;=2 in c:\users\user\desktop\artificial intelligence\.env\lib\site-packages (from requests-&gt;transformers[flax]) (2.1.1)
Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in c:\users\user\desktop\artificial intelligence\.env\lib\site-packages (from requests-&gt;transformers[flax]) (1.26.12)
Requirement already satisfied: certifi&gt;=2017.4.17 in c:\users\user\desktop\artificial intelligence\.env\lib\site-packages (from requests-&gt;transformers[flax]) (2022.9.24)
Collecting joblib
  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)
Requirement already satisfied: six in c:\users\user\desktop\artificial intelligence\.env\lib\site-packages (from sacremoses-&gt;transformers[flax]) (1.16.0)
Collecting click
  Using cached click-8.1.3-py3-none-any.whl (96 kB)
Building wheels for collected packages: tokenizers
  Building wheel for tokenizers (pyproject.toml) ... error
  error: subprocess-exited-with-error

  × Building wheel for tokenizers (pyproject.toml) did not run successfully.
  │ exit code: 1
  ╰─&gt; [48 lines of output]
      C:\Users\user\AppData\Local\Temp\pip-build-env-hhrbpvks\overlay\Lib\site-packages\setuptools\dist.py:530: UserWarning: Normalizing '0.8.1.rc2' to '0.8.1rc2'
        warnings.warn(tmpl.format(**locals()))
      running bdist_wheel
      running build
      running build_py
      creating build
      creating build\lib.win-amd64-cpython-310
      creating build\lib.win-amd64-cpython-310\tokenizers
      copying tokenizers\__init__.py -&gt; build\lib.win-amd64-cpython-310\tokenizers
      creating build\lib.win-amd64-cpython-310\tokenizers\models
      copying tokenizers\models\__init__.py -&gt; build\lib.win-amd64-cpython-310\tokenizers\models
      creating build\lib.win-amd64-cpython-310\tokenizers\decoders
      copying tokenizers\decoders\__init__.py -&gt; build\lib.win-amd64-cpython-310\tokenizers\decoders
      creating build\lib.win-amd64-cpython-310\tokenizers\normalizers
      copying tokenizers\normalizers\__init__.py -&gt; build\lib.win-amd64-cpython-310\tokenizers\normalizers
      creating build\lib.win-amd64-cpython-310\tokenizers\pre_tokenizers
      copying tokenizers\pre_tokenizers\__init__.py -&gt; build\lib.win-amd64-cpython-310\tokenizers\pre_tokenizers
      creating build\lib.win-amd64-cpython-310\tokenizers\processors
      copying tokenizers\processors\__init__.py -&gt; build\lib.win-amd64-cpython-310\tokenizers\processors
      creating build\lib.win-amd64-cpython-310\tokenizers\trainers
      copying tokenizers\trainers\__init__.py -&gt; build\lib.win-amd64-cpython-310\tokenizers\trainers
      creating build\lib.win-amd64-cpython-310\tokenizers\implementations
      copying tokenizers\implementations\base_tokenizer.py -&gt; build\lib.win-amd64-cpython-310\tokenizers\implementations       
      copying tokenizers\implementations\bert_wordpiece.py -&gt; build\lib.win-amd64-cpython-310\tokenizers\implementations       
      copying tokenizers\implementations\byte_level_bpe.py -&gt; build\lib.win-amd64-cpython-310\tokenizers\implementations       
      copying tokenizers\implementations\char_level_bpe.py -&gt; build\lib.win-amd64-cpython-310\tokenizers\implementations       
      copying tokenizers\implementations\sentencepiece_bpe.py -&gt; build\lib.win-amd64-cpython-310\tokenizers\implementations    
      copying tokenizers\implementations\__init__.py -&gt; build\lib.win-amd64-cpython-310\tokenizers\implementations
      copying tokenizers\__init__.pyi -&gt; build\lib.win-amd64-cpython-310\tokenizers
      copying tokenizers\models\__init__.pyi -&gt; build\lib.win-amd64-cpython-310\tokenizers\models
      copying tokenizers\decoders\__init__.pyi -&gt; build\lib.win-amd64-cpython-310\tokenizers\decoders
      copying tokenizers\normalizers\__init__.pyi -&gt; build\lib.win-amd64-cpython-310\tokenizers\normalizers
      copying tokenizers\pre_tokenizers\__init__.pyi -&gt; build\lib.win-amd64-cpython-310\tokenizers\pre_tokenizers
      copying tokenizers\processors\__init__.pyi -&gt; build\lib.win-amd64-cpython-310\tokenizers\processors
      copying tokenizers\trainers\__init__.pyi -&gt; build\lib.win-amd64-cpython-310\tokenizers\trainers
      running build_ext
      running build_rust
      error: can't find Rust compiler
     
      If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.
     
      To update pip, run:
     
          pip install --upgrade pip
     
      and then retry package installation.
     
      If you did intend to build this package from source, try installing a Rust compiler from your system package manager and 
ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to 
download and update the Rust compiler toolchain.
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
  ERROR: Failed building wheel for tokenizers
Failed to build tokenizers
ERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects
</code></pre>
<p>Do you know how I can successfully install this into VS Code and use Hugging Face properly?</p>
","huggingface"
"73939929","How to resolve the hugging face error ImportError: cannot import name 'is_tokenizers_available' from 'transformers.utils'?","2022-10-03 19:24:36","","0","19800","<python><pytorch><huggingface-transformers><huggingface-tokenizers><huggingface>","<p>I was trying to use the ViTT transfomer. I got the following error with code:</p>
<pre><code>from pathlib import Path
import torchvision
from typing import Callable
root = Path(&quot;~/data/&quot;).expanduser()
# root = Path(&quot;.&quot;).expanduser()
train = torchvision.datasets.CIFAR100(root=root, train=True, download=True)
test = torchvision.datasets.CIFAR100(root=root, train=False, download=True)
img2tensor: Callable = torchvision.transforms.ToTensor()
from transformers import ViTFeatureExtractor
feature_extractor = ViTFeatureExtractor.from_pretrained(&quot;google/vit-base-patch16-224-in21k&quot;)
x, y = train_ds[0]
print(f'{y=}')
print(f'{type(x)=}')
x = img2tensor(x)
x = x.unsqueeze(0)  # add batch size 1
out_cls: ImageClassifierOutput = model(x)
print(f'{out_cls.logits=}')
</code></pre>
<p>error</p>
<pre><code>Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File &quot;/Users/brandomiranda/opt/anaconda3/envs/meta_learning/lib/python3.9/code.py&quot;, line 90, in runcode
    exec(code, self.locals)
  File &quot;&lt;input&gt;&quot;, line 11, in &lt;module&gt;
  File &quot;/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/_pydev_bundle/pydev_import_hook.py&quot;, line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File &quot;/Users/brandomiranda/opt/anaconda3/envs/meta_learning/lib/python3.9/site-packages/transformers/__init__.py&quot;, line 30, in &lt;module&gt;
    from . import dependency_versions_check
  File &quot;/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/_pydev_bundle/pydev_import_hook.py&quot;, line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File &quot;/Users/brandomiranda/opt/anaconda3/envs/meta_learning/lib/python3.9/site-packages/transformers/dependency_versions_check.py&quot;, line 36, in &lt;module&gt;
    from .utils import is_tokenizers_available
ImportError: cannot import name 'is_tokenizers_available' from 'transformers.utils' (/Users/brandomiranda/opt/anaconda3/envs/meta_learning/lib/python3.9/site-packages/transformers/utils/__init__.py)
</code></pre>
<p>I tried upgrading everything but it still failed. Upgrade commands:</p>
<pre><code>/Users/brandomiranda/opt/anaconda3/envs/meta_learning/bin/python -m 
pip install --upgrade pip
pip install --upgrade pip
pip install --upgrade huggingface-hub

pip install --upgrade transformers
pip install --upgrade huggingface-hub
pip install --upgrade datasets

pip install --upgrade tokenizers

pip install pytorch-transformers

pip install --upgrade torch
pip install --upgrade torchvision
pip install --upgrade torchtext
pip install --upgrade torchaudio

# pip install --upgrade torchmeta
pip uninstall torchmeta
</code></pre>
<p>Why and how to fix it?</p>
<p>Pip list:</p>
<pre><code>(meta_learning) ❯ pip list
Package                                           Version    Editable project location
------------------------------------------------- ---------- ------------------------------------------------------------------------------
absl-py                                           1.0.0
aiohttp                                           3.8.1
aiosignal                                         1.2.0
antlr4-python3-runtime                            4.8
argcomplete                                       2.0.0
async-timeout                                     4.0.1
attrs                                             21.4.0
automl-meta-learning                              0.1.0      /Users/brandomiranda/automl-meta-learning/automl-proj-src
bcj-cffi                                          0.5.1
boto                                              2.49.0
boto3                                             1.24.85
botocore                                          1.27.85
Bottleneck                                        1.3.4
Brotli                                            1.0.9
brotlicffi                                        1.0.9.2
brotlipy                                          0.7.0
cachetools                                        4.2.4
certifi                                           2022.9.14
cffi                                              1.15.1
charset-normalizer                                2.0.9
cherry-rl                                         0.1.4
click                                             8.0.3
cloudpickle                                       2.0.0
colorama                                          0.4.4
configparser                                      5.2.0
conllu                                            4.4.1
crcmod                                            1.7
cryptography                                      37.0.1
cycler                                            0.11.0
Cython                                            0.29.25
dataclasses                                       0.6
datasets                                          2.5.1
dill                                              0.3.4
diversity-for-predictive-success-of-meta-learning 0.0.1      /Users/brandomiranda/diversity-for-predictive-success-of-meta-learning/div_src
docker-pycreds                                    0.4.0
editdistance                                      0.6.0
et-xmlfile                                        1.1.0
fairseq                                           0.10.0
fastcluster                                       1.2.4
fasteners                                         0.17.3
filelock                                          3.6.0
fonttools                                         4.28.3
frozenlist                                        1.2.0
fsspec                                            2022.7.1
gcs-oauth2-boto-plugin                            3.0
gitdb                                             4.0.9
GitPython                                         3.1.24
google-apitools                                   0.5.32
google-auth                                       2.3.3
google-auth-oauthlib                              0.4.6
google-reauth                                     0.1.1
grpcio                                            1.42.0
gsutil                                            5.6
gym                                               0.21.0
h5py                                              3.6.0
higher                                            0.2.1
httplib2                                          0.20.4
huggingface-hub                                   0.10.0
hydra-core                                        1.1.1
idna                                              3.3
importlib-metadata                                4.11.3
jmespath                                          1.0.1
joblib                                            1.1.0
kiwisolver                                        1.3.2
lark-parser                                       0.12.0
learn2learn                                       0.1.7
lxml                                              4.8.0
Markdown                                          3.3.6
matplotlib                                        3.5.1
mkl-fft                                           1.3.1
mkl-random                                        1.2.2
mkl-service                                       2.4.0
monotonic                                         1.6
multidict                                         5.2.0
multiprocess                                      0.70.12.2
multivolumefile                                   0.2.3
munkres                                           1.1.4
networkx                                          2.6.3
numexpr                                           2.8.1
numpy                                             1.21.5
oauth2client                                      4.1.3
oauthlib                                          3.1.1
omegaconf                                         2.1.1
openpyxl                                          3.0.10
ordered-set                                       4.0.2
packaging                                         21.3
pandas                                            1.4.2
pathtools                                         0.1.2
Pillow                                            9.0.1
pip                                               22.2.2
plotly                                            5.4.0
portalocker                                       2.3.2
progressbar2                                      3.55.0
promise                                           2.3
protobuf                                          3.19.1
psutil                                            5.8.0
py7zr                                             0.16.1
pyarrow                                           9.0.0
pyasn1                                            0.4.8
pyasn1-modules                                    0.2.8
pycparser                                         2.21
pycryptodomex                                     3.15.0
pyOpenSSL                                         22.0.0
pyparsing                                         3.0.6
pyppmd                                            0.16.1
PySocks                                           1.7.1
python-dateutil                                   2.8.2
python-utils                                      2.5.6
pytorch-transformers                              1.2.0
pytz                                              2021.3
pyu2f                                             0.1.5
PyYAML                                            6.0
pyzstd                                            0.14.4
qpth                                              0.0.15
regex                                             2021.11.10
requests                                          2.28.1
requests-oauthlib                                 1.3.0
responses                                         0.18.0
retry-decorator                                   1.1.1
rsa                                               4.7.2
s3transfer                                        0.6.0
sacrebleu                                         2.0.0
sacremoses                                        0.0.46
scikit-learn                                      1.0.1
scipy                                             1.7.3
seaborn                                           0.11.2
sentencepiece                                     0.1.97
sentry-sdk                                        1.5.1
setproctitle                                      1.2.2
setuptools                                        58.0.4
shortuuid                                         1.0.8
six                                               1.16.0
sklearn                                           0.0
smmap                                             5.0.0
subprocess32                                      3.5.4
tabulate                                          0.8.9
tenacity                                          8.0.1
tensorboard                                       2.7.0
tensorboard-data-server                           0.6.1
tensorboard-plugin-wit                            1.8.0
termcolor                                         1.1.0
texttable                                         1.6.4
threadpoolctl                                     3.0.0
tokenizers                                        0.13.0
torch                                             1.12.1
torchaudio                                        0.12.1
torchtext                                         0.13.1
torchvision                                       0.13.1
tornado                                           6.1
tqdm                                              4.62.3
transformers                                      4.22.2
typing_extensions                                 4.3.0
ultimate-anatome                                  0.1.1      /Users/brandomiranda/ultimate-anatome
ultimate-aws-cv-task2vec                          0.0.1      /Users/brandomiranda/ultimate-aws-cv-task2vec
ultimate-utils                                    0.6.1      /Users/brandomiranda/ultimate-utils/ultimate-utils-proj-src
urllib3                                           1.26.11
wandb                                             0.13.3
Werkzeug                                          2.0.2
wheel                                             0.37.0
xxhash                                            2.0.2
yarl                                              1.8.1
yaspin                                            2.1.0
zipp                                              3.8.0
</code></pre>
<hr />
<p>hugging face (HF) related gitissues:</p>
<ul>
<li><a href=""https://github.com/huggingface/transformers/issues/15062"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/issues/15062</a></li>
<li><a href=""https://github.com/huggingface/tokenizers/issues/120"" rel=""nofollow noreferrer"">https://github.com/huggingface/tokenizers/issues/120</a></li>
<li><a href=""https://github.com/huggingface/tokenizers/issues/585"" rel=""nofollow noreferrer"">https://github.com/huggingface/tokenizers/issues/585</a></li>
<li><a href=""https://github.com/huggingface/transformers/issues/11262"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/issues/11262</a></li>
<li>SO related: <a href=""https://stackoverflow.com/questions/66590981/transformer-error-importing-packages-importerror-cannot-import-name-save-st"">Transformer: Error importing packages. &quot;ImportError: cannot import name &#39;SAVE_STATE_WARNING&#39; from &#39;torch.optim.lr_scheduler&#39;&quot;</a></li>
<li>HF discuss: <a href=""https://discuss.huggingface.co/t/how-to-resolve-the-hugging-face-error-importerror-cannot-import-name-is-tokenizers-available-from-transformers-utils/23957"" rel=""nofollow noreferrer"">https://discuss.huggingface.co/t/how-to-resolve-the-hugging-face-error-importerror-cannot-import-name-is-tokenizers-available-from-transformers-utils/23957</a></li>
<li>reddit: <a href=""https://www.reddit.com/r/pytorch/comments/xusuuy/how_to_resolve_the_hugging_face_error_importerror/"" rel=""nofollow noreferrer"">https://www.reddit.com/r/pytorch/comments/xusuuy/how_to_resolve_the_hugging_face_error_importerror/</a></li>
<li><a href=""https://github.com/huggingface/tokenizers/issues/1080"" rel=""nofollow noreferrer"">https://github.com/huggingface/tokenizers/issues/1080</a></li>
</ul>
","huggingface"
"73932308","TypeError: expected string or bytes-like object while using a hugging face model","2022-10-03 07:28:59","","1","776","<python><typeerror><huggingface>","<p>I am trying to us DiffusionPipeline from hugging face to generate Image from text but the following error is being generated:
TypeError: expected string or bytes-like object</p>
<pre><code>The traceback:
 File &quot;c:\Users\BHAVYA SHAH\Desktop\Artificial Intelligence\app.py&quot;, line 9, in &lt;module&gt;
    from diffusers import DiffusionPipeline
  File &quot;C:\Users\BHAVYA SHAH\anaconda3\lib\site-packages\diffusers\__init__.py&quot;, line 26, in &lt;module&gt;
    from .pipelines import DDIMPipeline, DDPMPipeline, KarrasVePipeline, LDMPipeline, PNDMPipeline, ScoreSdeVePipeline
  File &quot;C:\Users\BHAVYA SHAH\anaconda3\lib\site-packages\diffusers\pipelines\__init__.py&quot;, line 11, in &lt;module&gt;
    from .latent_diffusion import LDMTextToImagePipeline
  File &quot;C:\Users\BHAVYA SHAH\anaconda3\lib\site-packages\diffusers\pipelines\latent_diffusion\__init__.py&quot;, line 6, in &lt;module&gt;
    from .pipeline_latent_diffusion import LDMBertModel, LDMTextToImagePipeline
  File &quot;C:\Users\BHAVYA SHAH\anaconda3\lib\site-packages\diffusers\pipelines\latent_diffusion\pipeline_latent_diffusion.py&quot;, line 9, in &lt;module&gt;
    from transformers.activations import ACT2FN
  File &quot;C:\Users\BHAVYA SHAH\anaconda3\lib\site-packages\transformers\__init__.py&quot;, line 30, in &lt;module&gt;
    from . import dependency_versions_check
  File &quot;C:\Users\BHAVYA SHAH\anaconda3\lib\site-packages\transformers\dependency_versions_check.py&quot;, line 17, in &lt;module&gt;
    from .utils.versions import require_version, require_version_core
  File &quot;C:\Users\BHAVYA SHAH\anaconda3\lib\site-packages\transformers\utils\__init__.py&quot;, line 34, in &lt;module&gt;
    from .generic import (
  File &quot;C:\Users\BHAVYA SHAH\anaconda3\lib\site-packages\transformers\utils\generic.py&quot;, line 29, in &lt;module&gt;
    from .import_utils import is_flax_available, is_tf_available, is_torch_available, is_torch_fx_proxy
  File &quot;C:\Users\BHAVYA SHAH\anaconda3\lib\site-packages\transformers\utils\import_utils.py&quot;, line 376, in &lt;module&gt;
    torch_version = version.parse(importlib_metadata.version(&quot;torch&quot;))
  File &quot;C:\Users\BHAVYA SHAH\anaconda3\lib\site-packages\packaging\version.py&quot;, line 49, in parse
    return Version(version)
  File &quot;C:\Users\BHAVYA SHAH\anaconda3\lib\site-packages\packaging\version.py&quot;, line 264, in __init__ 
    match = self._regex.search(version)
TypeError: expected string or bytes-like object
</code></pre>
<p>Help to solve this error would be much appreciated.</p>
","huggingface"
"73918456","from transformers import BertTokenizer","2022-10-01 12:56:33","","1","710","<python><huggingface-transformers><huggingface-tokenizers><huggingface>","<p>I am trying to implement the following model from hugging face but not entirely sure how to feed the model the texts that I need to pass to do the classification. The documentation (<a href=""https://huggingface.co/DaNLP/da-bert-tone-subjective-objective"" rel=""nofollow noreferrer"">https://huggingface.co/DaNLP/da-bert-tone-subjective-objective</a>) does not show how to pass your queries.</p>
<pre><code>from transformers import BertTokenizer, BertForSequenceClassification

model = BertForSequenceClassification.from_pretrained(&quot;DaNLP/da-bert-tone-subjective-objective&quot;)
tokenizer = BertTokenizer.from_pretrained(&quot;DaNLP/da-bert-tone-subjective-objective&quot;)
</code></pre>
<p>Would appreciate further guidance here.</p>
","huggingface"
"73918191","Input/Target size mismatch when training a downstream BERT for classification (huggingface pretrained)","2022-10-01 12:11:10","","1","270","<machine-learning><nlp><bert-language-model><huggingface>","<p>I am training a BERT model with a downstream task to classify movie genres. I am using HuggingFace pretrained model (aleph-bert since data is in Hebrew)</p>
<p>When training, I get the following error:</p>
<pre><code>ValueError: Expected input batch_size (3744) to match target batch_size (16).
</code></pre>
<p>This is my notebook:
<a href=""https://colab.research.google.com/drive/1mqIUPnLOu_H-URn5tzE6gGySsW3oAcRY?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1mqIUPnLOu_H-URn5tzE6gGySsW3oAcRY?usp=sharing</a></p>
<p>The error happens in the <code>compute_loss functions</code>, while performing the cross_entropy step.</p>
<p>My batch size is 16 but for some reason the bert output returns a different size.</p>
<p>The relevant code:</p>
<pre><code>def data_prep_for_genre(genre):
    X = movies_df['overview']
    y = movies_df[genre].rename('labels', inplace=True).astype(float)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
    X_train = tokenizer(X_train.to_list(), truncation=True)
    X_test = tokenizer(X_test.to_list(), truncation=True)

    train_dataset = TextData(X_train, y_train.to_list())
    test_dataset = TextData(X_test, y_test.to_list())

    # define model:
    model = BertForTokenClassification.from_pretrained(&quot;onlplab/alephbert-base&quot;, num_labels=2)

    return model, train_dataset, test_dataset


class MyTrainer(Trainer):  
   def compute_metrics(pred):
      labels = pred.label_ids
      preds = pred.predictions.argmax(-1)
      precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')
      acc = accuracy_score(labels, preds)
      return {
          'accuracy': acc,
          'f1': f1,
          'precision': precision,
          'recall': recall
      }

    training_args = TrainingArguments(
    output_dir='./results',          
    num_train_epochs=10,
    per_device_train_batch_size=16,  
    per_device_eval_batch_size=32,   
    warmup_steps=50,                 
    weight_decay=0.01,               
    logging_dir='./logs',            
    logging_steps=10
)

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

for genre in GENRE_SET:
  model, train_dataset, test_dataset = data_prep_for_genre(genre)
  trainer = MyTrainer(
    model=model,                        
    args=training_args,                 
    train_dataset=train_dataset,        
    # eval_dataset=test_dataset,        
    data_collator=data_collator
  )
  trainer.train()
</code></pre>
","huggingface"
"73842234","Setting Huggingface cache in Google Colab notebook to Google Drive","2022-09-25 05:30:23","","3","2376","<google-drive-api><google-colaboratory><huggingface>","<p>I am using Google Colab to implement Huggingface code.</p>
<p>What is the best method to change huggingface cache directory in Colab environment to my Google Drive (GDrive), so that we won't need to download the cached content i.e. language models, datasets...etc. every-time we initiate Colab environment? rather, just redirect huggingface in Colab to use GDrive.</p>
<p>I tried setting the related environment variables in Colab, still, the content is downloaded in Colab runtime environment:</p>
<pre><code>export TRANSFORMERS_CACHE='/content/drive/MyDrive/Colab Notebooks/NLP/HuggingfaceCash'
export HF_DATASETS_CACHE='/content/drive/MyDrive/Colab Notebooks/NLP/HuggingfaceCash/Datasets'
</code></pre>
","huggingface"
"73841325","Chatbot Start Prompt for GPT-J","2022-09-25 00:05:48","","0","619","<chatbot><huggingface-transformers><huggingface>","<p>I'm using GPT-J (EleutherAI/gpt-j-6B) as a chatbot. As a prompt, I provide a sample conversation as shown below. When now a new conversation starts, I append the input of the user to this sample conversation (&quot;Hello, how are you doing?&quot; in the example below).</p>
<p>Now, the problem is that the conversation is sometimes inconsistent because GPT-J might want to continue the sample conversation but the new user input could break that.</p>
<p>How can this be solved?</p>
<blockquote>
<p>This is a discussion between a Human and a Chatbot.</p>
<p>Human:
Can you do push-ups?</p>
<p>Chatbot:
Of course I can. It's a piece of cake! Believe it or not, I can do 30 push-ups a minute.</p>
<p>Human:
Really? I think that's impossible!</p>
<p>Chatbot:
You mean 30 push-ups?</p>
<p>Human:
Yeah!</p>
<p>Chatbot:
It's easy. If you do exercise everyday, you can make it, too.</p>
<p>Human:
Hello, how are you doing?</p>
<p>Chatbot:</p>
</blockquote>
","huggingface"
"73828107","How to fix nsfw error for stable diffusion?","2022-09-23 12:59:38","","10","24620","<python><huggingface><stable-diffusion><diffusers>","<p>I always get the &quot;Potential NSFW content was detected in one or more images. A black image will be returned instead. Try again with a different prompt and/or seed.&quot; error when using stable diffusion, even with the code that was given on huggingface:</p>
<pre><code>import torch
from torch import autocast
from diffusers import StableDiffusionPipeline

model_id = &quot;CompVis/stable-diffusion-v1-4&quot;
device = &quot;cuda&quot;
token = 'MY TOKEN'


pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16, revision=&quot;fp16&quot;, use_auth_token=token)
pipe = pipe.to(device)

prompt = &quot;a photo of an astronaut riding a horse on mars&quot;
with autocast(&quot;cuda&quot;):
    image = pipe(prompt, guidance_scale=7.5).images[0]  
    
image.save(&quot;astronaut_rides_horse.png&quot;)
</code></pre>
","huggingface"
"73826120","'numpy.float64' object has no attribute 'mid'","2022-09-23 10:06:52","","2","1777","<python><numpy><huggingface-transformers><summarization><huggingface>","<p>While implementing this code for <a href=""https://huggingface.co/course/chapter7/5#finetuning-mt5-with-the-trainer-api"" rel=""nofollow noreferrer"">mt5 summarization of hugging face</a>
This error occurred : <a href=""https://i.sstatic.net/15M1w.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<pre><code>18     )
     19     # Extract the median scores
---&gt; 20     result = {key: value.mid.fmeasure * 100 for key, value in result.items()}
     21     return {k: round(v, 4) for k, v in result.items()}

AttributeError: 'numpy.float64' object has no attribute 'mid'
</code></pre>
<p>Is there something I could do? since the code is kinda fixed.
Thank you :)</p>
","huggingface"
"73788355","Huggingface models: how to store a different version of a model","2022-09-20 14:27:46","75554572","0","1437","<huggingface-transformers><git-lfs><huggingface>","<p>I have a model that I pushed to the remote using the following code:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import CLIPProcessor, CLIPModel

checkpoint = &quot;./checkpoints-15/checkpoint-60&quot;
    
model = CLIPModel.from_pretrained(checkpoint)
processor = CLIPProcessor.from_pretrained(checkpoint)

repo = &quot;vincentclaes/emoji-predictor&quot;
model.push_to_hub(repo, use_temp_dir=True)
processor.push_to_hub(repo, use_temp_dir=True)
</code></pre>
<p>On the UI I see my model under a <code>main</code> branch:
<a href=""https://i.sstatic.net/FmCDa.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/FmCDa.png"" alt=""enter image description here"" /></a></p>
<p>What if I want to store multiple versions of a model?</p>
<ul>
<li>Can I create a separate git branch?</li>
<li>Can I create a git tag?</li>
</ul>
<p>How do I do this using the huggingface tools?
Thinking <code>transformers</code>, <code>huggingface_hub</code>, ...</p>
","huggingface"
"73745607","How to pass arguments to HuggingFace TokenClassificationPipeline's tokenizer","2022-09-16 13:32:47","74522080","5","3047","<python><huggingface-transformers><named-entity-recognition><huggingface-tokenizers><huggingface>","<p>I've finetuned a Huggingface BERT model for Named Entity Recognition. Everything is working as it should. Now I've setup a pipeline for token classification in order to predict entities out the text I provide. Even this is working fine.</p>
<p>I know that BERT models are supposed to be fed with sentences less than 512 tokens long. Since I have texts longer than that, I split the sentences in shorter chunks and I store the chunks in a list <code>chunked_sentences</code>. To make it brief my tokenizer for training looks like this:</p>
<pre><code>from transformers import BertTokenizerFast
tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')
tokenized_inputs = tokenizer(chunked_sentences, is_split_into_words=True, padding='longest')
</code></pre>
<p>I pad everything to the longest sequence and avoid truncation so that if a sentence is tokenized and goes beyond 512 tokens I receive a warning that I won't be able to train. This way I know that I have to split the sentences in smaller chunks.</p>
<p>During inference I wanted to achieve the same thing, but I haven't found a way to pass arguments to the pipeline's tokenizer. The code looks like this:</p>
<pre><code>from transformers import pipeline
ner_pipeline = pipeline('token-classification', model=model_folder, tokenizer=model_folder)
out = ner_pipeline(text, aggregation_strategy='simple')
</code></pre>
<p>I'm pretty sure that if a sentence is tokenized and surpasses the 512 tokens, the extra tokens will be truncated and I'll get no warning. I want to avoid this.</p>
<p>I tried passing arguments to the tokenizer like this:</p>
<pre><code>tokenizer_kwargs = {'padding': 'longest'}
out = ner_pipeline(text, aggregation_strategy='simple', **tokenizer_kwargs)
</code></pre>
<p>I got that idea from <a href=""https://stackoverflow.com/a/70729850/14774959"">this answer</a>, but it seems not to be working, since I get the following error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;...\inference.py&quot;, line 42, in &lt;module&gt;
    out = ner_pipeline(text, aggregation_strategy='simple', **tokenizer_kwargs)
  File &quot;...\venv\lib\site-packages\transformers\pipelines\token_classification.py&quot;, line 191, in __call__
    return super().__call__(inputs, **kwargs)
  File &quot;...\venv\lib\site-packages\transformers\pipelines\base.py&quot;, line 1027, in __call__
    preprocess_params, forward_params, postprocess_params = self._sanitize_parameters(**kwargs)
TypeError: TokenClassificationPipeline._sanitize_parameters() got an unexpected keyword argument 'padding'

Process finished with exit code 1
</code></pre>
<p>Any ideas? Thanks.</p>
","huggingface"
"73713671","how to resize the embedding vectors from huggingface bert","2022-09-14 08:20:11","","1","4666","<python><bert-language-model><huggingface>","<p>I try to use the tokenizer method to tokenize the sentence and then mean pool the attention mask to get the vectors for each sentence. However, the current default size embedding is 768 and I wish to reduce it to 200 instead but failed. below is my code.</p>
<pre><code>from transformers import AutoTokenizer, AutoModel
import torch


#Mean Pooling - Take attention mask into account for correct averaging
def mean_pooling(model_output, attention_mask):
    token_embeddings = model_output[0] #First element of model_output contains all token embeddings
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)


# Sentences we want sentence embeddings for
sentences = ['This is an example sentence', 'Each sentence is converted']

# Load model from HuggingFace Hub
tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/bert-base-nli-mean-tokens')
model = AutoModel.from_pretrained('sentence-transformers/bert-base-nli-mean-tokens')
model.resize_token_embeddings(200)
# Tokenize sentences
encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')

# Compute token embeddings
with torch.no_grad():
    model_output = model(**encoded_input)

# Perform pooling. In this case, max pooling.
sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])

print(&quot;Sentence embeddings:&quot;)
print(sentence_embeddings)
</code></pre>
<p>Error:</p>
<pre><code>   2193     # Note [embedding_renorm set_grad_enabled]
   2194     # XXX: equivalent to
   2195     # with torch.no_grad():
   2196     #   torch.embedding_renorm_
   2197     # remove once script supports set_grad_enabled
   2198     _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)
-&gt; 2199 return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)

IndexError: index out of range in self
</code></pre>
<p>my expected output is:</p>
<p>when use:</p>
<pre><code>print(len(sentence_embeddings[0]))
-&gt; 200
</code></pre>
","huggingface"
"73654472","Why do I get the error ""ModuleNotFoundError: No module named 'huggan'""?","2022-09-08 19:58:23","","0","303","<huggingface-transformers><huggingface>","<p>I am trying to implement <a href=""https://huggingface.co/huggan/fastgan-few-shot-universe"" rel=""nofollow noreferrer"">this</a> model from HuggingFace🤗. To run the model I need to import <code>HugGANModelHubMixin</code> with:</p>
<p><code>from huggan.pytorch.huggan_mixin import HugGANModelHubMixin</code></p>
<p>but I get:</p>
<p><code>ModuleNotFoundError: No module named 'huggan'</code>.</p>
<p>I cloned the model locally and try to run it from VSC.</p>
<p>As far I understand is the problem that <code>HugGANModelHubMixin</code> is not available on HuggingFace because <a href=""https://huggingface.co/models"" rel=""nofollow noreferrer"">search</a> for models returns no results.</p>
<p>How can I find a work around for this issue?</p>
","huggingface"
"73595680","How to fine tune a model from hugging face?","2022-09-03 22:10:02","","-1","1090","<machine-learning><amazon-sagemaker><huggingface-tokenizers><huggingface>","<p>I want to download a pretrained a model and fine tune the model with my own data. I have downloaded a bert-large-NER model artifacts from hugging face,I have listed the contents below . being new to this, I want to know what files or artifacts do i need and from the looks of it the pytorch_model.bin is the trained model, but what are these others file and their purpose like tokenizer files and vocab.txt ....</p>
<pre><code>config.json
pytorch_model.bin
special_tokens_map.json
tokenizer_config.json
vocab.txt
</code></pre>
","huggingface"
"73594934","huggingface/transformers: cache directory","2022-09-03 19:26:24","","3","5777","<caching><huggingface-transformers><huggingface>","<p>I'm trying to use huggingface transformers.
(Win 11, Python 3.9, jupyternotebook, virtual environment)</p>
<p>When I ran code:</p>
<pre><code>from transformers import pipeline
print(pipeline('sentiment-analysis')('I hate you'))
</code></pre>
<p>I got an error :</p>
<blockquote>
<p>FileNotFoundError: [WinError 3] The system cannot find the path specified: 'C:\Users\user/.cache\huggingface'</p>
</blockquote>
<p>There's no directory named '.cache' in my user folder,
so I used cache_dir=&quot;./cache&quot;
but I want to change the path of the directory permanently.</p>
<p>P.S.</p>
<pre><code>import os
os.environ['TRANSFORMERS_CACHE'] = './cache'
</code></pre>
<p>also didn't work.</p>
","huggingface"
"73561318","Paraphrasing whole paragraphs with Pegasus","2022-08-31 20:04:02","","1","519","<python><machine-learning><nlp><huggingface>","<p>I've been trying to use Pegasus to paraphrase things, so far it does paraphrases sentences nicely, but the issue is, that it actually takes a part of the paragraph, usually the last part and rephrases it instead of the whole given &quot;phrase&quot;, which is the provided paragraph.
I would like it to paraphrase the whole given paragraph.</p>
<p>My function to do so:</p>
<pre><code>from transformers import *


class SomeClass:
    def __init__(self):
        self.model = PegasusForConditionalGeneration.from_pretrained(&quot;tuner007/pegasus_paraphrase&quot;)
        self.tokenizer = PegasusTokenizerFast.from_pretrained(&quot;tuner007/pegasus_paraphrase&quot;)


    
    def get_paraphrased_sentences1(self, model, tokenizer, sentence, num_return_sequences=5, num_beams=5):
        # tokenize the text to be form of a list of token IDs
        inputs = tokenizer([sentence], truncation=True, padding=&quot;longest&quot;, return_tensors=&quot;pt&quot;)
        # generate the paraphrased sentences
        outputs = model.generate(
            **inputs,
            max_length=8096,
            length_penalty=2.0,
            num_beams=num_beams,
            num_return_sequences=num_return_sequences,
        )
        # decode the generated sentences using the tokenizer to get them back to text
        return tokenizer.batch_decode(outputs, skip_special_tokens=True)
</code></pre>
<p>If we're to take an example:
Original input:</p>
<pre><code>Randal went to the mall to get some clothes. When he got home he found his wife cheating on him. Randal ended up kicking her out of the house. The kids stay with Randal and not the cheating wife.
</code></pre>
<p>Received output sequences:</p>
<pre><code>When he got home, he found his wife cheating on him, so he kicked her out of the house and the kids stayed with him.
When he got home, he found his wife cheating on him, so he kicked her out of the house and put the kids with him.
When he returned from the mall, he found his wife cheating on him and he kicked her out of the house and the kids stayed with him.
When he returned from the mall, he found his wife cheating on him and he kicked her out of the house.
When he got home, he found his wife cheating on him, so he kicked her out of the house and the children stayed with him.
When he got home, he found his wife cheating on him, and he kicked her out of the house.
When he got home, he found his wife cheating on him, so he kicked her out of the house and put the kids in his care.
When he got home, he found his wife cheating on him, so he kicked her out of the house.
When he got home, he found his wife cheating on him, so he kicked her out of the house and kept the kids with him.
When he got home, he found his wife cheating on him, so he kicked her out of the house and put his kids with him.
</code></pre>
<p>That's basically the issue I've been working on for the past few days, I've looked through the <a href=""https://huggingface.co/docs/transformers/v4.21.2/en/model_doc/pegasus#transformers.PegasusForConditionalGeneration"" rel=""nofollow noreferrer"">Pegasus Documentation</a> to check if there are any parameters that may fix this, but found nothing that worked, I've tried a few, among them was max_length and min_length and that didn't work.</p>
","huggingface"
"73518635","Can't use BloomAI locally","2022-08-28 12:22:57","73539954","0","847","<python><machine-learning><huggingface-transformers><huggingface><bloom>","<p>So I just finished installing Bloom's model from Huggingface &amp; I tried to run it in my notebook.</p>
<p>Here's the code:</p>
<pre><code>from transformers import AutoTokenizer, AutoModel
model_path = &quot;D:/bloom&quot;
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModel.from_pretrained(model_path)
</code></pre>
<p>and I get this error:</p>
<pre><code>DefaultCPUAllocator: not enough memory: you tried to allocate xxx bytes
</code></pre>
<p>So then I went on to google searching and found this article:</p>
<p><a href=""https://towardsdatascience.com/run-bloom-the-largest-open-access-ai-model-on-your-desktop-computer-f48e1e2a9a32"" rel=""nofollow noreferrer"">https://towardsdatascience.com/run-bloom-the-largest-open-access-ai-model-on-your-desktop-computer-f48e1e2a9a32</a></p>
<p>However, I get this error:</p>
<pre><code>TypeError: build_alibi_tensor() missing 1 required positional argument: 'device'
</code></pre>
<p>How to run Bloom locally?</p>
","huggingface"
"73465069","Huggingface progress bars shown despite disable_tqdm=True in Trainer","2022-08-23 21:09:41","","1","1444","<python><progress-bar><huggingface-transformers><huggingface>","<p>I'm running HuggingFace Trainer with TrainingArguments(disable_tqdm=True, ...) for fine-tuning the EleutherAI/gpt-j-6B model but there are still progress bars displayed (please see picture below). Does somebody know how to remove these progress bars?</p>
<pre><code>training_args = TrainingArguments(
    disable_tqdm=True,
    output_dir='./checkpoints',
    save_total_limit=10,
    logging_dir='/content/logs',
    num_train_epochs=config[&quot;N_EPOCHS&quot;],
    evaluation_strategy='epoch'
    save_strategy='steps',
    save_steps=30,
    logging_steps=10,
    overwrite_output_dir=True,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    gradient_accumulation_steps=4,
    eval_accumulation_steps=4,
    gradient_checkpointing=True,
    max_grad_norm=0.5,
    lr_scheduler_type=&quot;cosine&quot;,
    learning_rate=1e-4,
    warmup_ratio=0.05,
    weight_decay=0.1,
    fp16_full_eval=True
    fp16=True,
    fp16_opt_level='O1',
    report_to=['tensorboard']
)
</code></pre>
<p><a href=""https://i.sstatic.net/bQWTH.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bQWTH.png"" alt=""enter image description here"" /></a></p>
","huggingface"
"73452601","AWS lambda error when loading Transformers model","2022-08-23 02:43:07","","0","533","<python><docker><aws-lambda><huggingface-transformers><huggingface>","<p>I made a post earlier about an error which got fixed but lead to another error. So I created code in python to summarize news articles and the code works on both my laptop and after creating a Docker image for it.</p>
<pre><code>try:
    from bs4 import BeautifulSoup
    import requests
    from urllib.request import urlopen
    import base64
    import re
    from transformers import BartForConditionalGeneration, BartTokenizer, BartConfig
    import json
except Exception as e:
    print(&quot;Error imports : {} &quot;.format(e))

def lambda_handler(event=None, context=None):
    headers = {'User-Agent': 'Mozilla/5.0'}

    url = 'https://news.google.com/news/rss'
    client = urlopen(url)
    xml_page = client.read()
    client.close()
    soup = BeautifulSoup(xml_page, 'xml')

    contents = soup.find_all(&quot;item&quot;)

    encoded_links = []
    headlines = []
    dates = []

    for news in contents:
        if &quot;youtube.com&quot; in str(news.source):
            continue
        encoded_links.append(news.link.text)
        headlines.append(news.title.text)
        dates.append(news.pubDate.text)

    encoded_links = encoded_links[:15]
    headlines = headlines[:15]
    dates = dates[:15]

    decoded_links = []

    for link in encoded_links:
        coded = link[44:-5]
        while True:
            try:
                url = base64.b64decode(coded)
                break
            except:
                coded += &quot;a&quot;
        url = str(base64.b64decode(coded))

        strip1 = re.search(&quot;(?P&lt;url&gt;https?://[^\s]+)&quot;, url).group(&quot;url&quot;)
        strip2 = stripped = strip1.split('$', 1)[0]
        strip3 = stripped = strip2.split('\\', 1)[0]
        decoded_links.append(strip3)

    summarized_texts = []

    tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')
    model=BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')

    for link in decoded_links:
        try:
            new_page = requests.get(link, headers=headers)
        except:
            continue
        new_soup  = BeautifulSoup(new_page.text, 'lxml')
        
        text = &quot;&quot;
        paragraphs = new_soup.find_all(&quot;p&quot;)
        for p in paragraphs:
            text += p.text

        inputs = tokenizer.batch_encode_plus([text],return_tensors='pt', truncation=True, max_length=1024)
        summary_ids = model.generate(inputs['input_ids'], early_stopping=True)

        bart_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

        summarized_texts.append(bart_summary)
        print(bart_summary)


    print(&quot;Success,&quot;, len(summarized_texts), &quot;summaries created.&quot;)

    returned = [{'headline': title, 'date': date, 'summary': summarized} for title, date, summarized in zip(headlines, dates, summarized_texts)]

    json_summaries = json.dumps(returned)

    return json_summaries
</code></pre>
<p>And my Dockerfile:</p>
<pre><code>FROM public.ecr.aws/lambda/python:3.9

COPY requirements.txt ./
RUN pip3 install -r requirements.txt
RUN python -m nltk.downloader punkt
COPY app.py ./

CMD [&quot;app.lambda_handler&quot;]
</code></pre>
<p>After uploading to aws ecr and to lambda, I get the following:</p>
<pre><code>{
  &quot;errorMessage&quot;: &quot;Can't load the model for 'facebook/bart-large-cnn'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'facebook/bart-large-cnn' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.&quot;,
  &quot;errorType&quot;: &quot;OSError&quot;,
  &quot;requestId&quot;: &quot;938ad716-f2c2-4d18-920b-f170aa685b4c&quot;,
  &quot;stackTrace&quot;: [
    &quot;  File \&quot;/var/task/app.py\&quot;, line 59, in lambda_handler\n    model=BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn',cache_dir=\&quot;/tmp/\&quot;)\n&quot;,
    &quot;  File \&quot;/var/lang/lib/python3.9/site-packages/transformers/modeling_utils.py\&quot;, line 2023, in from_pretrained\n    raise EnvironmentError(\n&quot;
  ]
}
</code></pre>
<p>I don't understand the error message as I haven't named any local directories similarly. I've tried researching this but can't find any solution to this.</p>
","huggingface"
"73451147","StableDiffusion Colab - How to ""make sure you're logged in with `huggingface-cli login`?""","2022-08-22 21:25:08","73451650","7","11416","<google-colaboratory><huggingface-datasets><huggingface>","<p>I'm trying to run the Colab example of the Huggingface StableDiffusion generative text-to-image model:</p>
<p><a href=""https://huggingface.co/CompVis/stable-diffusion"" rel=""noreferrer"">https://huggingface.co/CompVis/stable-diffusion</a>
<a href=""https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb#scrollTo=xSKWBKFPArKS"" rel=""noreferrer"">https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb#scrollTo=xSKWBKFPArKS</a></p>
<p>However it gets stuck on the loading of the model:</p>
<p><a href=""https://i.sstatic.net/rsI0U.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/rsI0U.png"" alt=""enter image description here"" /></a></p>
<p>Using the model requires registration at Huggingface and a token - I have one, I also got a token, which was accepted in the previous cell:</p>
<p><a href=""https://i.sstatic.net/LIJ5S.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/LIJ5S.png"" alt=""enter image description here"" /></a></p>
<p>After the error I also executed the suggested command:
<code>!git config --global credential.helper store</code></p>
<p>Although I don't think it's what has to be done.</p>
<p>I found this:
<a href=""https://huggingface.co/docs/hub/repositories-getting-started"" rel=""noreferrer"">https://huggingface.co/docs/hub/repositories-getting-started</a>
(Although I won't be pushing, but only downloading.)</p>
<blockquote>
<p>To be able to push your code to the Hub, you’ll need to
authenticate somehow. The easiest way to do this is by installing the
huggingface_hub CLI and running the login command:</p>
</blockquote>
<pre><code>python -m pip install huggingface_hub
huggingface-cli login
</code></pre>
<p>I installed it and run it:</p>
<pre><code>!python -m pip install huggingface_hub
!huggingface-cli login
</code></pre>
<p><a href=""https://i.sstatic.net/YAbzn.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/YAbzn.png"" alt=""enter image description here"" /></a></p>
<p>I logged in with my token (Read) - login successful.</p>
<p>However it still returns the same error on the attempt to download the model.</p>
<p>How to &quot;make sure I'm logged in with <code>huggingface-cli login</code>&quot;?</p>
<p>Thanks!</p>
","huggingface"
"73340805","Huggingface generating chatbot response using GPT-J","2022-08-13 00:20:27","","2","631","<python><chatbot><huggingface-transformers><transformer-model><huggingface>","<p>I’m using EleutherAI/gpt-j-6B for a chatbot. I’m using the following prompt and the following code:</p>
<pre><code>prompt = &quot;person alpha:\nhi! how are you doing?\n\nperson beta:I am fine, thank you. What are you doing?\n\nperson alpha:\nI am at home watching tv.\n\nperson beta:\nThat sounds like a lot of fun. What are you watching?\n\nperson alpha:\n&quot;

model = GPTJForCausalLM.from_pretrained(
                sEleutherAI/gpt-j-6B,
                revision=&quot;float16&quot;,
                torch_dtype=torch.float16,
                low_cpu_mem_usage=True,
                use_cache=False,
                gradient_checkpointing=True
        )
tokenizer = transformers.AutoTokenizer.from_pretrained(self.hf_name, pad_token='&lt;|endoftext|&gt;',eos_token='&lt;|endoftext|&gt;')

resp_len = 128
resp_temp = 0.72
prompt = self.tokenizer(text, return_tensors='pt')
prompt = {key: value.to('cuda') for key, value in prompt.items()}
ex_min = len(prompt) + resp_len
out = self.model.generate(**prompt,
                           min_length=ex_min,
                           max_length=ex_min + resp_len,
                           do_sample=True,
                           top_k=35,
                           top_p=0.90,
                           temperature=resp_temp,
                           #repetition_penalty=1.5,
                           #length_penalty=1.2,
                           no_repeat_ngram_size=4,
                           clean_up_tokenization_spaces=True,
)

res = self.tokenizer.decode(out[0])
</code></pre>
<p>The output of the model is then the following:</p>
<pre><code>person alpha:
hi! how are you doing?

person beta:
I am fine, thank you. What are you doing?

person alpha:
I am at home watching tv.

person beta:
That sounds like a lot of fun. What are you watching?

person alpha:
A movie called ‘The Hobbit’. It is a good movie.

Person beta:
Ah, I have not seen that movie. I will look for it.

The above conversation is an example of how the system can handle more than two participants. The conversation can continue with any number of participants.

If person alpha has not been added to the system, the conversation will go through the normal steps of finding the user and adding the person to the system. In the above example, the conversation would continue with person beta.

Figure 4.2: Conversation with more than two people

The conversation can be continued in a number of ways. For example, person beta can start the conversation again. Person beta can start a conversation with person alpha, or person alpha can start a new conversation with person beta, or person beta can join a conversation with someone else.

4.2.2. Conversation and Media
The Conversations system allows media to be used in a conversational context. In this section we will look at how media can be used in the Conversations system.

In the Conversations user interface, media is added to a conversation by dragging and dropping the media file onto the conversation.

When a media file is added to the conversation, the file is added as an attachment to the conversation. This means that the media file can be used by the participants in the conversation. For example the person alpha can add the audio file to the conversation and play it for the person beta. Person beta could also add the video file and play it.
The Conversions system does not use any of the media files for anything other than the conversation. In the Conversations application, the media is just used as a medium for conversation.
The media file is not deleted when the conversation is finished. The file remains on the server and can be used later.

You can add more than one media file to a conversation.
For example, if you
</code></pre>
<p>The response of person alpha <code>A movie called ‘The Hobbig’. It is a good movie</code> makes totally sense but then the model continues with person beta and adds much more useless text.</p>
<p>What I want is just the response for person alpha and then the model should stop producing text.</p>
<p>How can this be done? The amount of produced text should also be variable because an answer is not always of same length.</p>
","huggingface"
"73331076","Multiprocessing with large ml models","2022-08-12 08:09:56","","-1","651","<pytorch><multiprocessing><python-multiprocessing><huggingface-transformers><huggingface>","<p>I have got a large transformer model from huggingface. The model is about 2gb in storage. When I try to run multiprocessing processes or pool the program just freezes. Even if I just try to have 2 workers/processes.</p>
<p>From what I understand it freezes because it's trying to pickle the transformer model and copy the environment for both workers.</p>
<p>I've tried to load the model in after the multiprocessing starts but it also results in the same challenge.</p>
<p>My question is do I need to increase my ram if so what's the general rule of thumb for how much ram I need per worker and how would I calculate it.</p>
<p>How can I get this right, I've tried making the model use a shared memory block but I've not managed to get it to work. has anyone done something like this?</p>
","huggingface"
"73286181","How to use fine-tuned model in huggingface for actual prediction after re-loading?","2022-08-09 03:19:11","","3","3029","<python><deep-learning><huggingface-transformers><text-classification><huggingface>","<p>I'm trying to reload a DistilBertForSequenceClassification model I've fine-tuned and use that to predict some sentences into their appropriate labels (text classification).</p>
<p>In google Colab, after successfully training the BERT model, I downloaded it after saving:</p>
<pre><code>trainer.train()
trainer.save_model(&quot;distilbert_classification&quot;)

</code></pre>
<p>The downloaded model has three files:  config.json, pytorch_model.bin, training_args.bin.</p>
<p>I moved them encased in a folder named 'distilbert_classification' somewhere in my google drive.</p>
<p>afterwards, I reloaded the model in a different Colab notebook:</p>
<pre><code>
reloadtrainer = DistilBertForSequenceClassification.from_pretrained('google drive directory/distilbert_classification')

</code></pre>
<p>Up to this point, I have succeeded without any errors.</p>
<p>However, how to I use this reloaded model (the 'reloadtrainer' object) to actually make the predictions on sentences? What is the code I need to use afterwards? I tried</p>
<p><code>reloadtrainer .predict(&quot;sample sentence&quot;)</code> but  it doesn't work. Would appreciate any help!</p>
","huggingface"
"73280266","What is the best way to run an api for HuggingFace models","2022-08-08 15:18:38","","1","268","<heroku><huggingface-transformers><huggingface>","<p>I'm trying to implement a HuggingFace model in a Flask API. I've seen several posts explaining premade solutions for generating predictions, but that doesn’t fit my needs; I need to call native HuggingFace methods, as in below:</p>
<pre><code>def rate_offensiveness(sentence):
&quot;&quot;&quot;sentence through offensiveness model and returns probability from 0-1&quot;&quot;&quot;
encoded_input = tokenizer(sentence, return_tensors='pt')
output = model(**encoded_input)
results = softmax(output[0][0].detach().numpy())
sentence_labels_dict = {}
ranking = np.argsort(results)
ranking = ranking[::-1]
for i in range(results.shape[0]):
    label = labels[ranking[i]]
    sentence = results[ranking[i]]
    sentence_labels_dict[label] = np.round(float(sentence), 4)
return sentence_labels_dict['offensive']
</code></pre>
<p>I've been trying to run it in Heroku with the model in the slug (in the Github repo), but I run into the problem of the slug size being too big. I've also tried storing it in S3 and loading it into local ram on Heroku, and I run into the same problem.</p>
","huggingface"
"73278181","How to use dataset with costume function?","2022-08-08 12:50:55","73297047","1","264","<huggingface-tokenizers><huggingface-datasets><huggingface>","<p>I want to call <code>DatasetDict</code> <code>map</code> function with parameters, and I dont know how to do it.</p>
<p>I have function with the following API:</p>
<pre><code>def tokenize_function(tokenizer, examples):
    s1 = examples[&quot;premise&quot;]
    s2 = examples[&quot;hypothesis&quot;]
    args = (s1, s2)
    return tokenizer(*args, padding=&quot;max_length&quot;, truncation=True)
</code></pre>
<p>And when I’m trying to use in this way:</p>
<pre><code>   dataset = load_dataset(&quot;json&quot;, data_files=data_files)
   tokenizer          = AutoTokenizer.from_pretrained(model_name)
   tokenized_datasets = dataset.map(tokenize_function, tokenizer, batched=True)
</code></pre>
<p>I’m getting error:</p>
<pre><code>TypeError: list indices must be integers or slices, not str
</code></pre>
<p>How can I call <code>map</code> function in my example ?</p>
","huggingface"
"73243093","How to customize the positional embedding?","2022-08-04 23:10:48","","2","1296","<deep-learning><huggingface-transformers><transformer-model><huggingface-tokenizers><huggingface>","<p>I am using the Transformer model from Hugging face for machine translation. However, my input data has relational information as shown below:</p>
<p><a href=""https://i.sstatic.net/gdek9.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/gdek9.png"" alt=""enter image description here"" /></a></p>
<p>I want to craft a graph like the like the following:</p>
<pre><code> ________
 |       |
 |      \|/
He ended his meeting on Tuesday night.
/|\ |         |          /|\
 |  |         |           | 
 |__|         |___________|  
</code></pre>
<p>Essentially each <code>token</code> in the sentence is a node and there could be an <code>edge</code> embedded between the tokens.</p>
<p>In a normal transformer, the tokens are processed into token embeddings, also there is an encoding of each position which resulted into positional embeddings.</p>
<p>How could I do something similar with the edge information?</p>
<p>Theoretically I could take the edge type and the positional encoding of a node and output an embedding. The embeddings of all the edges can be added to the positional embeddings for the corresponding nodes.</p>
<p>Ideally, I would like to implement this with the hugging face transformer.</p>
<p>I am struggling to understand how could I update the positional embedding here:</p>
<p><a href=""https://github.com/huggingface/transformers/blob/main/src/transformers/models/longformer/modeling_longformer.py#L453"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/blob/main/src/transformers/models/longformer/modeling_longformer.py#L453</a></p>
<pre><code>        self.position_embeddings = nn.Embedding(
            config.max_position_embeddings, config.hidden_size, padding_idx=self.padding_idx
        )
</code></pre>
","huggingface"
"73240587","Is there a way to have the same ordering of labels for HuggingFace transformers pipeline?","2022-08-04 18:14:42","","2","409","<python><pandas><jupyter><huggingface>","<p>This is the output I get after I append the outputs of a <code>pipeline</code> using <code>facebook-bart-large-mnli</code> model.</p>
<pre><code>[{'labels': ['recreation', 'entertainment', 'travel', 'dining'],
  'scores': [0.8873, 0.1528, 0.0002, 0.0001],
  'sequence': 'laundromat'},
 {'labels': ['recreation', 'travel', 'entertainment', 'dining'],
  'scores': [0.9932, 0.9753, 0.2099, 0.0001],
  'sequence': 'running trail'},
 {'labels': ['dining', 'entertainment', 'recreation', 'travel'],
  'scores': [0.8846, 0.1825, 0.1067, 0.0001],
  'sequence': 'affordable housing for young families,cafe or restaurant,none'},
 {'labels': ['travel', 'entertainment', 'recreation', 'dining'],
  'scores': [0.3595, 0.0716, 0.0187, 0.0039],
  'sequence': 'electric vehicle chargers,ev charger'},
</code></pre>
<blockquote>
<pre><code>    classifiedText.append(classifier2(sequence_to_classify, candidate_labels, multi_label=True))
</code></pre>
</blockquote>
<p>Is there a way for the pipeline to follow the ordering of candidate_labels? (it is a list)</p>
<p>I am trying to output the results of the zero shot classification performed by transformers into a pandas Dataframe, and I found that just using the <code>.to_dict</code> method poses a problem because the output of the pipeline has a different order for each phrase that is classified.</p>
<p>Is there a way to get the labels to be consistent? (Of course, still matching to the correct value under the <code>scores</code> key.)</p>
","huggingface"
"73204460","How to pretrain BART using custom dataset(Not fine tuning!!)","2022-08-02 08:40:26","73238126","1","1873","<huggingface-transformers><pre-trained-model><huggingface><bart>","<p>I'm using huggingface transformers 4.19.0<br />
I want to <strong>pretrain BART model using my custom dataset.</strong><br />
To make it clear, I'm not asking about fine tuning BART to down stream task but asking about &quot;pre training BART&quot;.</p>
<p>But I can't find method or class for this job on huggingface docs page (<a href=""https://huggingface.co/docs/transformers/model_doc/bart"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/model_doc/bart</a>)<br />
Is it impossible to pretrain BART using transformers package?<br />
Do I have to make BART model layer by layer from the scratch?</p>
<p>If anyone knows how to pretrain BART model using custom data please help me...</p>
","huggingface"
"73182816","Why there are no logs and which model is saved?","2022-07-31 11:14:58","73222555","1","2402","<python><huggingface-transformers><huggingface>","<p>I’m using <code>Trainer</code> to train my model.</p>
<p>I have the following outputs on screen:</p>
<pre><code>Epoch   Training Loss   Validation Loss Accuracy
0   No log  1.114260    0.342667
1   No log  0.939480    0.545333
2   No log  0.816581    0.660000
3   No log  0.752204    0.710667
4   No log  0.741462    0.741333
5   No log  0.801005    0.754667
6   0.675800    0.892765    0.748000
7   0.675800    1.190328    0.752000
8   0.675800    1.272624    0.745333
</code></pre>
<ol>
<li>Why there are no logs for epochs 0-5 ? (Do I need to configure / enable them ?)</li>
<li><strong>Epoch #5</strong> got best accuracy.
When I will use predict, which model checkpoint will be used ?
(the model which trained after 5 epochs or the model which trained after 8 epochs) ?</li>
</ol>
","huggingface"
"73182805","How to use bart-large-mnli model for NLI task?","2022-07-31 11:13:38","","1","1118","<nlp><huggingface-transformers><bert-language-model><huggingface>","<p>I want to use <code>facebook/bart-large-mnli</code> model for NLI task.
I have dataset with <code>premises</code> and <code>hypothesis</code> columns and labels [0,1,2].</p>
<p>How can I use this model for that NLI task ?</p>
<p>I wrote the following code:</p>
<pre><code>import torch

device    = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
nli_model = AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli')
tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-mnli')


nli_model.to(device)

i          = 0 # first examle check
premise    = tokenized_datasets['TRAIN'][i]['premise']
hypothesis = tokenized_datasets['TRAIN'][i]['hypothesis']


x      = tokenizer.encode(premise, hypothesis, return_tensors='pt', truncation_strategy='only_first')
logits = nli_model(x.to(device))[0]

entail_contradiction_logits = logits[:,[0,2]]
probs                       = entail_contradiction_logits.softmax(dim=1)
probs
</code></pre>
<p>and I got only 2 values: <code>tensor([[8.8793e-05, 9.9991e-01]], device='cuda:0', grad_fn=&lt;SoftmaxBackward0&gt;</code>) (instead of 3 values - contradiction, neutral, entailment)</p>
<p>How can I use this model for NLI (predict the right value from 3 labels) ?</p>
","huggingface"
"73182692","How to choose grid search (when working with trainer.hyperparameter_search)?","2022-07-31 10:59:30","73216034","1","633","<python><deep-learning><huggingface-transformers><huggingface>","<p>I want to run <code>trainer.hyperparameter_search</code> (with <strong>grid search</strong>) and I haven't seen any HP algorithm type parameter.</p>
<p>How can I configure <code>trainer.hyperparameter_search</code> to run with grid-search ?</p>
","huggingface"
"73155719","Do weights of the [PAD] token have a function?","2022-07-28 15:52:21","73158625","2","1151","<huggingface-transformers><word-embedding><transformer-model><huggingface-tokenizers><huggingface>","<p>When looking at the weights of a transformer model, I noticed that the embedding weights for the padding token <code>[PAD]</code> are nonzero. I was wondering whether these weights have a function, since they are ignored in the multi-head attention layers.</p>
<p>Would it make sense to set these weights to zeros? The weights can be seen using <code>model.base_model.embeddings.word_embeddings.weight[PAD_ID]</code> where typically <code>PAD_ID=0</code>.</p>
","huggingface"
"73151816","Hugging face: RuntimeError: model_init should have 0 or 1 argument","2022-07-28 11:04:07","","1","780","<deep-learning><huggingface-transformers><huggingface>","<p>I’m trying to tune hyper-params with the following code:</p>
<pre><code>def my_hp_space(trial):
    return {
        &quot;learning_rate&quot;: trial.suggest_float(&quot;learning_rate&quot;, 5e-3, 5e-5),
        &quot;arr_gradient_accumulation_steps&quot;: trial.suggest_int(&quot;num_train_epochs&quot;, 8, 16),
        &quot;arr_per_device_train_batch_size&quot;: trial.suggest_int(2, 4),        
    }


def get_model(model_name, config):
    return AutoModelForSequenceClassification.from_pretrained(model_name, config=config)

def compute_metric(eval_predictions):
    
    metric         = load_metric('accuracy')    
    logits, labels = eval_predictions
    predictions    = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)

training_args   = TrainingArguments(output_dir='test-trainer', 
                                    evaluation_strategy=&quot;epoch&quot;,
                                    num_train_epochs= 10)
data_collator   = default_data_collator
model_name      = 'sentence-transformers/nli-roberta-base-v2'
config = AutoConfig.from_pretrained(model_name,num_labels=3)

trainer = Trainer(
    model_init      = get_model(model_name, config),
    args            = training_args,
    train_dataset   = tokenized_datasets['TRAIN'],
    eval_dataset    = tokenized_datasets['TEST'],    
    compute_metrics = compute_metric,
    tokenizer       = None,
    data_collator   = data_collator,
)

best = trainer.hyperparameter_search(direction=&quot;maximize&quot;, hp_space=my_hp_space)
</code></pre>
<p>And getting error:</p>
<pre><code> 1173     model = self.model_init(trial)
   1174 else:
-&gt; 1175     raise RuntimeError(&quot;model_init should have 0 or 1 argument.&quot;)
   1177 if model is None:
   1178     raise RuntimeError(&quot;model_init should not return None.&quot;)

RuntimeError: model_init should have 0 or 1 argument.
</code></pre>
<ol>
<li>What am I doing wrong ?</li>
<li>How can I fix it and run hyper parameter method and get best model parameters ?</li>
</ol>
","huggingface"
"73107703","issue when importing BloomTokenizer from transformers in python","2022-07-25 10:27:28","73108760","1","2583","<python><nlp><huggingface-transformers><huggingface-tokenizers><huggingface>","<p>I am trying to import BloomTokenizer from transformers</p>
<pre><code>from transformers import BloomTokenizer
</code></pre>
<p>and I receive the following error</p>
<pre><code>Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
ImportError: cannot import name 'BloomTokenizer' from 'transformers' 
(/root/miniforge3/envs/pytorch/lib/python3.8/site-packages/transformers/__init__.py)
</code></pre>
<p>my version of transformers:</p>
<pre><code>transformers                 4.20.1
</code></pre>
<p>what could I do to be able to import BloomTokenizer?</p>
","huggingface"
"73107325","Batch size during training vs batch size during evaluation","2022-07-25 09:58:03","","3","558","<testing><huggingface-transformers><huggingface>","<p>I am confused about the difference between batch size during training versus batch size during evaluation. I am trying to measure how batch size influences the inference time(speed of prediction) of different NLP models after they have  been trained using the Huggingface Trainer API. The code I used is below</p>
<pre><code>def print_summary(result):
    print(f&quot;Time: {result.metrics['test_runtime']:.2f}&quot;)
    print(f&quot;Samples/second: {result.metrics['test_samples_per_second']:.2f}&quot;)
    print(f&quot;Test Loss: {result.metrics['test_loss']:.2f}&quot;)
    print(f&quot;Test Accuracy: {result.metrics['test_accuracy']:.2f}&quot;)
   
    print(result.metrics)
</code></pre>
<pre><code>logging.set_verbosity_error()
training_args = TrainingArguments( output_dir=&quot;cat&quot;, per_device_train_batch_size=1,per_device_eval_batch_size=1,do_train = False,
        do_predict = True)
model = AutoModelForSequenceClassification.from_pretrained(&quot;distilrob&quot;)
trainer = Trainer(model=model, args=training_args,compute_metrics=compute_metrics)
result=trainer.predict(tokenized_datasets_test_distilrob)
print_summary(result)
</code></pre>
<p>My initial idea was that the <code>per_device_train_batch_size</code> would have no effect, since the training is actually done and I am now looking at the performance of the trained models, but its value does actually change the inference time. Why would it affect the inference time after the training is complete and what would be the correct set up if I  want to  measure the inference time (speed of prediction) as (academically) precisely as possible?</p>
","huggingface"
"73073641","ImportError: cannot import name 'HfApi'","2022-07-21 23:28:52","","2","5351","<huggingface-transformers><huggingface>","<p>first time using hugging face transformers library and it's not getting through the import statement.
Running on Conda virtual environment Python 3.6</p>
<p>I also tried this below with the huggingface_hub library, and the error message is the same.</p>
<pre><code>from huggingface_hub import HfApi

hf_api = HfApi()
models = hf_api.list_models()
</code></pre>
","huggingface"
"73042562","IndicBART seq2seq model gives entirely blank predictions","2022-07-19 19:33:08","","1","75","<nlp><huggingface>","<p>I am using IndicBART for sequence-to-sequence prediction on tamil sentences.</p>
<p>I have trained the model on 100,000 samples of tamil data for 30 epochs, then tested predictions on a few sentences. These are the predictions given by the model- I have presented both the token IDs output by the model and the tokens decoded from the token IDs:</p>
<pre><code>source: 'bullet es'
predicted: ''
predicted token IDs: [64000, 64000, 64000, 64000, 64000, 64000, 64000, 64000, 64000, 64000, 64000, 64000, 64000, 64000, 64000, 64000, 64000, 64000, 64000, 2]
gold: 'bullet e s'

source: 'இங்க பழச த்துலேந்து பேசுறங்க' 
predicted: ''
predicted token IDs: [64000, 64000, 64000, 64000, 64000, 64000, 64000, 64000, 64000, 64000, 64000, 64000, 64000, 64000, 64000, 64000, 64000, 64000, 64000, 2]
gold: 'இங்க palladam துல இருந்து பேசறங்க'
</code></pre>
<p>And so on. All the predictions are blank lines.</p>
<p>Untrained IndicBART is not producing blank predictions, only the trained checkpoint.</p>
<p>Is this a feature of IndicBART, or am I doing something wrong?</p>
","huggingface"
"73016662","How to preserve the original columns of a dataset when using Huggingface tokenizer?","2022-07-18 02:07:07","73160391","1","1199","<huggingface-tokenizers><huggingface-datasets><huggingface>","<p>When using Huggingface Tokenizer with <code>return_overflowing_tokens=True</code>, the results can have multiple token sequence per input string.  Therefore, when doing a <code>Dataset.map</code> from strings to token sequence, you need to remove the original columns (as they are not 1:1).</p>
<p>For my application, I need to continue to reference the original dataset's columns. How can I copy them over to the tokenized dataset?</p>
<p>For example:</p>
<pre><code># Pseudocode
ds['txt'] == ['The quick brown fox', 'jumped over the lazy hens']
ds['src'] == ['Nursery rhyme 1', 'Nursery rhyme 2']

tokenize(ds['txt'], return_overflowing_tokens=True) =&gt;
[tokens for 'The quick brown'],
[tokens for 'fox'],
[tokens for 'jumped over'],
[tokens for 'the lazy hens'],

# I'd like a tokenized_ds to look like this:
tokenized_ds[0] = {txt: 'The quick brown fox', src: 'Nursery rhyme 1', tokens: [tokens for 'The quick brown']}
tokenized_ds[1] = {txt: 'The quick brown fox', src: 'Nursery rhyme 1', tokens: [tokens for 'fox']}

</code></pre>
<hr />
<p>Some clarifications:</p>
<ol>
<li><p>Some of the columns that need to be preserved are strings.  These are harder to preserve when you set the format to a Tensor.</p>
</li>
<li><p>The dataset will be batched via Dataloader, and only batches handed off for processing.  This means that the original, full dataset will not necessarily be available.  That makes it hard to map back to the original dataset on demand, which is why I want to preserve the columns within the transformed dataset.</p>
</li>
</ol>
","huggingface"
"72999740","Cannot get DataCollator to prepare tf dataset","2022-07-15 21:25:05","","0","1443","<python><tensorflow><named-entity-recognition><transformer-model><huggingface>","<p>I’m trying to follow this tutorial to fine-tune bert for a NER task using my own dataset. <a href=""https://www.philschmid.de/huggingface-transformers-keras-tf"" rel=""nofollow noreferrer"">https://www.philschmid.de/huggingface-transformers-keras-tf</a>.  Below is my shortened code, and the error due to the last line of the code. I’m new to all these, and thank you in advance for helping out!</p>
<pre><code># load dataset, 
df_converters = {'tokens': ast.literal_eval, 'labels': ast.literal_eval}
train_df = pd.read_csv(&quot;train_df_pretokenization.csv&quot;, converters=df_converters)
train_df = train_df.head(10)

# model and pretrained tokenizer
model_ckpt = &quot;indobenchmark/indobert-base-p2&quot;
tokenizer = AutoTokenizer.from_pretrained(model_ckpt) 

# tokenization, and align labels 
def tokenize_and_align_labels(batch): 

    tag2int = {'B-POI':0, 'B-STR':1, 'E-POI':2, 'E-STR':3, 'I-POI':4,
           'I-STR':5, 'S-POI':6, 'S-STR':7, 'O':8}
           
    #tokenized_inputs = tokenizer(batch['tokens'], is_split_into_words=True, truncation = True, padding = True)
    tokenized_inputs = tokenizer(batch['tokens'], is_split_into_words=True, truncation = True)
    labels=[]
    for idx, label in enumerate(batch['labels']):
        word_ids = tokenized_inputs.word_ids(batch_index = idx)
        previous_word_idx = None
        label_ids = []
        for word_idx in word_ids: 
            if word_idx is None:
                label_ids.append(-100)
            elif word_idx != previous_word_idx:
                label_ids.append(tag2int[label[word_idx]])
            else: 
                label_ids.append(tag2int[label[word_idx]])
            previous_word_idx = word_idx
        labels.append(label_ids)
    tokenized_inputs['tags'] = labels

    return tokenized_inputs

def encode_dataset(ds):
    return ds.map(tokenize_and_align_labels, batched= True, batch_size=10, remove_columns=['labels','tokens', 'index'])
    
train_ds = Dataset.from_pandas(train_df)
train_ds_encoded = encode_dataset(train_ds)

# prepare model input 
data_collator = DataCollatorForTokenClassification(tokenizer, return_tensors=&quot;tf&quot;)

tf_train_dataset = train_ds_encoded.to_tf_dataset(
    columns= ['input_ids', 'token_type_ids', 'attention_mask', 'tags'],
    shuffle=False,
    batch_size=5,
    collate_fn=data_collator
)
</code></pre>
<blockquote>
<p>ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length.</p>
</blockquote>
<p>I thought data collator is supposed to take care of the padding work given the requested batch size, and I don’t understand why feeding in sequences of different lengths will cause this error. Indeed, the tutorial runs fine without specifying padding or truncation. My code will run if I add padding = True to the tokenizer in the function (the line I commented out in the function). But I don’t think it is the right place to add paddings.</p>
","huggingface"
"72970467","HuggingFace: ValueError:expected sequence of length 21 at dim 1 (got 20)","2022-07-13 17:39:14","","1","582","<python><deep-learning><pytorch><huggingface-transformers><huggingface>","<p>I am fine-tuning InCoder for code generation on my own data. To do so, I've gone through the tutorials in all implementations - Trainer, Pytorch and Tensorflow but cannot seem to make any of them work. I've seen this post <a href=""https://stackoverflow.com/questions/71166789/huggingface-valueerror-expected-sequence-of-length-165-at-dim-1-got-128"">HuggingFace: ValueError: expected sequence of length 165 at dim 1 (got 128)</a> but my padding is within trainer as a method. I'm also not using a masked language model to mitigate likewise. Here, I'm testing on 1% of the data. This is my code to reproduce the error for <code>trainer</code>:</p>
<pre><code>from datasets import load_dataset
dataset = load_dataset(&quot;ablam/gcode&quot;)

# Tokenize texts with same vocab as training the model
from transformers import AutoTokenizer

# Preserve casing info
tokenizer = AutoTokenizer.from_pretrained(&quot;facebook/incoder-1B&quot;,use_fast=True,do_lower_case=False)

# Tokenize our texts.
def tokenize_function(examples):
    return tokenizer(examples[&quot;text&quot;], truncation=True)

# In batches and 4 processes to speed it up. 
tokenized_datasets = dataset.map(tokenize_function, batched=True, num_proc=4)

# Postprocess 1: remove texts
tokenized_datasets = tokenized_datasets.remove_columns(['text'])

# Postprocess 2: Add label column 
# Labels = ground truth for the model to learn from.

def new_column(example):
    example[&quot;labels&quot;] = example[&quot;input_ids&quot;].copy()
    return example

tokenized_datasets_label = tokenized_datasets.map(new_column, batched=True, num_proc=4)

# Postprocess 3: 1% of data
import datasets 

train_dataset, train_1percent_label = tokenized_datasets_label['train'].train_test_split(test_size=0.01).values() #(41297976, 3)
test_dataset, test_1percent_label = tokenized_datasets_label['test'].train_test_split(test_size=0.01).values() #(417152, 3)

# Postprocess 4: token_type_ids is not considered in model.
train_1percent_trainer = train_1percent.remove_columns(['token_type_ids'])
test_1percent_trainer = test_1percent.remove_columns(['token_type_ids'])

# Trainer
from transformers import TrainingArguments
training_args = TrainingArguments(output_dir=&quot;test_trainer&quot;)

import numpy as np
from datasets import load_metric

metric = load_metric(&quot;accuracy&quot;)

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)

from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(output_dir=&quot;test_trainer&quot;, evaluation_strategy=&quot;epoch&quot;)

from transformers import AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained(&quot;facebook/incoder-1B&quot;)

import torch 
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_1percent_trainer,
    eval_dataset=test_1percent_trainer,
    compute_metrics=compute_metrics,
    optimizers=[torch.optim.AdamW, torch.optim.lr_scheduler.LambdaLR]
)

trainer.train()
</code></pre>
<p>My error is as follows:</p>
<pre><code>~/.conda/envs/deep_gcode/lib/python3.9/site-packages/transformers/data/data_collator.py in torch_default_data_collator(features)
    128                 batch[k] = torch.stack([f[k] for f in features])
    129             else:
--&gt; 130                 batch[k] = torch.tensor([f[k] for f in features])
    131 
    132     return batch

ValueError: expected sequence of length 21 at dim 1 (got 20)
</code></pre>
<p>When I saw this, I tried batching but it's not a parameter to <code>trainer</code>. How can I go about this?</p>
","huggingface"
"72953352","""ValueError: initial_value must be specified."" error when compiling bert for text classification","2022-07-12 13:44:40","","1","241","<python><tensorflow><huggingface-transformers><bert-language-model><huggingface>","<p>I'm following the hugging face tutorial for the sequence classification and while trying to fine-tune <code>'distilbert-base-multilingual-cased'</code> I'm having the following error when running the model.compile() method.</p>
<p>I'm having the same error when using bert-uncased.</p>
<pre><code>ValueError                                Traceback (most recent call last)
&lt;ipython-input-22-68f5487774e6&gt; in &lt;module&gt;
----&gt; 1 model.compile(optimizer=optimizer)

/tf/Raph/jupyter/tf2-37/lib/python3.7/site-packages/transformers/modeling_tf_utils.py in compile(self, optimizer, loss, metrics, loss_weights, weighted_metrics, run_eagerly, steps_per_execution, **kwargs)
   1035                 run_eagerly=run_eagerly,
   1036                 experimental_steps_per_execution=steps_per_execution,
-&gt; 1037                 **kwargs,
   1038             )
   1039 

/tf/Raph/jupyter/tf2-37/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in compile(self, optimizer, loss, metrics, loss_weights, weighted_metrics, run_eagerly, **kwargs)
    547       experimental_steps_per_execution = kwargs.pop(
    548           'experimental_steps_per_execution', 1)
--&gt; 549       self._configure_steps_per_execution(experimental_steps_per_execution)
    550 
    551       # Initializes attrs that are reset each time `compile` is called.

/tf/Raph/jupyter/tf2-37/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)
    455     self._self_setattr_tracking = False  # pylint: disable=protected-access
    456     try:
--&gt; 457       result = method(self, *args, **kwargs)
    458     finally:
    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access

/tf/Raph/jupyter/tf2-37/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in _configure_steps_per_execution(self, steps_per_execution)
    581         steps_per_execution,
    582         dtype='int64',
--&gt; 583         aggregation=variables.VariableAggregationV2.ONLY_FIRST_REPLICA)
    584 
    585   @property

/tf/Raph/jupyter/tf2-37/lib/python3.7/site-packages/tensorflow/python/ops/variables.py in __call__(cls, *args, **kwargs)
    260       return cls._variable_v1_call(*args, **kwargs)
    261     elif cls is Variable:
--&gt; 262       return cls._variable_v2_call(*args, **kwargs)
    263     else:
    264       return super(VariableMetaclass, cls).__call__(*args, **kwargs)

/tf/Raph/jupyter/tf2-37/lib/python3.7/site-packages/tensorflow/python/ops/variables.py in _variable_v2_call(cls, initial_value, trainable, validate_shape, caching_device, name, variable_def, dtype, import_scope, constraint, synchronization, aggregation, shape)
    254         synchronization=synchronization,
    255         aggregation=aggregation,
--&gt; 256         shape=shape)
    257 
    258   def __call__(cls, *args, **kwargs):

/tf/Raph/jupyter/tf2-37/lib/python3.7/site-packages/tensorflow/python/ops/variables.py in getter(**kwargs)
     65 
     66   def getter(**kwargs):
---&gt; 67     return captured_getter(captured_previous, **kwargs)
     68 
     69   return getter

/tf/Raph/jupyter/tf2-37/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py in creator(next_creator, **kwargs)
   2855     def creator(next_creator, **kwargs):
   2856       _require_strategy_scope_strategy(strategy)
-&gt; 2857       return next_creator(**kwargs)
   2858 
   2859     self._var_creator_scope = variable_scope.variable_creator_scope(creator)

/tf/Raph/jupyter/tf2-37/lib/python3.7/site-packages/tensorflow/python/ops/variables.py in &lt;lambda&gt;(**kws)
    235                         shape=None):
    236     &quot;&quot;&quot;Call on Variable class. Useful to force the signature.&quot;&quot;&quot;
--&gt; 237     previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)
    238     for _, getter in ops.get_default_graph()._variable_creator_stack:  # pylint: disable=protected-access
    239       previous_getter = _make_getter(getter, previous_getter)

/tf/Raph/jupyter/tf2-37/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py in default_variable_creator_v2(next_creator, **kwargs)
   2644       synchronization=synchronization,
   2645       aggregation=aggregation,
-&gt; 2646       shape=shape)
   2647 
   2648 

/tf/Raph/jupyter/tf2-37/lib/python3.7/site-packages/tensorflow/python/ops/variables.py in __call__(cls, *args, **kwargs)
    262       return cls._variable_v2_call(*args, **kwargs)
    263     else:
--&gt; 264       return super(VariableMetaclass, cls).__call__(*args, **kwargs)
    265 
    266 

/tf/Raph/jupyter/tf2-37/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py in __init__(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)
   1516           aggregation=aggregation,
   1517           shape=shape,
-&gt; 1518           distribute_strategy=distribute_strategy)
   1519 
   1520   def _init_from_args(self,

/tf/Raph/jupyter/tf2-37/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py in _init_from_args(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape)
   1594             synchronization, aggregation, trainable, name))
   1595     if initial_value is None:
-&gt; 1596       raise ValueError(&quot;initial_value must be specified.&quot;)
   1597     init_from_fn = callable(initial_value)
   1598 

ValueError: initial_value must be specified.
</code></pre>
<p>tensorflow version: 2.3.0</p>
<p>transformers version: 4.20.1</p>
<p>python version: 3.7</p>
<p>Any ideas? thanks in advance.</p>
","huggingface"
"72882799","HuggingFace - Why does the T5 model shorten sentences?","2022-07-06 11:27:25","","2","840","<python><huggingface-transformers><transformer-model><huggingface-tokenizers><huggingface>","<p>I wanted to train the model for spell correction. I trained two models allegro/plt5-base with polish sentences and google/t5-v1_1-base with english sentences. Unfortunately, I don't know for what reason, but both models shorten the sentences.
Example:</p>
<pre><code>phrases = ['The name of the man who was kild was Jack Robbinson he has black hair brown eyes blue Jacket and blue Jeans.']
encoded = tokenizer(phrases, return_tensors=&quot;pt&quot;, padding=True, max_length=512, truncation=True)
print(encoded)
# {'input_ids': tensor([[   37,   564,    13,     8,   388,   113,    47,     3,   157,   173,
#             26,    47,  4496,  5376,  4517,   739,     3,    88,    65,  1001,
#           1268,  4216,  2053,  1692, 24412,    11,  1692,  3966,     7,     5,
#              1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
#          1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}

encoded.to('cuda')
translated = model.generate(**encoded)
print(translated)
# tensor([[   0,   37,  564,   13,    8,  388,  113,   47, 2170,   47, 4496, 5376,
#          4517,  739,    3,   88,   65, 1001, 1268, 4216]], device='cuda:0')

tokenizer.batch_decode(translated, skip_special_tokens=True)
#['The name of the man who was born was Jack Robbinson he has black hair brown']
</code></pre>
<p>And something like this happens in almost every longer sentence. I tried to check if the model has any maximum sentence length set based on the documentation: <a href=""https://huggingface.co/transformers/v3.1.0/model_doc/t5.html"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/v3.1.0/model_doc/t5.html</a>. But the config of this model has no such field:
<code>n_positions – The maximum sequence length that this model might ever be used with. Typically set this to something large just in case (e.g., 512 or 1024 or 2048). n_positions can also be accessed via the property max_position_embeddings.</code>
This is the entire config of the model:</p>
<pre><code>T5Config {
  &quot;_name_or_path&quot;: &quot;final_model_t5_800_000&quot;,
  &quot;architectures&quot;: [
    &quot;T5ForConditionalGeneration&quot;
  ],
  &quot;d_ff&quot;: 2048,
  &quot;d_kv&quot;: 64,
  &quot;d_model&quot;: 768,
  &quot;decoder_start_token_id&quot;: 0,
  &quot;dropout_rate&quot;: 0.1,
  &quot;eos_token_id&quot;: 1,
  &quot;feed_forward_proj&quot;: &quot;gated-gelu&quot;,
  &quot;initializer_factor&quot;: 1.0,
  &quot;is_encoder_decoder&quot;: true,
  &quot;layer_norm_epsilon&quot;: 1e-06,
  &quot;model_type&quot;: &quot;t5&quot;,
  &quot;num_decoder_layers&quot;: 12,
  &quot;num_heads&quot;: 12,
  &quot;num_layers&quot;: 12,
  &quot;output_past&quot;: true,
  &quot;pad_token_id&quot;: 0,
  &quot;relative_attention_max_distance&quot;: 128,
  &quot;relative_attention_num_buckets&quot;: 32,
  &quot;tie_word_embeddings&quot;: false,
  &quot;torch_dtype&quot;: &quot;float32&quot;,
  &quot;transformers_version&quot;: &quot;4.18.0&quot;,
  &quot;use_cache&quot;: true,
  &quot;vocab_size&quot;: 32128
}
</code></pre>
<p>What can be done to make the model return whole sentences?</p>
<h4>Update</h4>
<p>I looked in the old documentation earlier. But in the new one I don't see a field in the config at all about the maximum sentence length. <a href=""https://huggingface.co/docs/transformers/main/en/model_doc/t5#transformers.T5Config"" rel=""nofollow noreferrer"">new documentation</a></p>
","huggingface"
"72804595","How to use gradio dataframe as output for an interface","2022-06-29 16:06:13","","0","2259","<python><huggingface><gradio>","<p>So I have a scraper that collects data from different sources and them returns the result as a dataframe. I currently want to create a Gradio interface that given a user input would returns the resulting dataframe as output.</p>
<p><code>gr.Interface(fn=scraper_obj.scrape, inputs='text', outputs=gr.Dataframe(headers=['title', 'author', 'text']), allow_flagging='never').launch()</code></p>
<p>As you can see in the above code, the interface would call the scrape function on the user input which would return a dataframe that has three columns (title, author and text) but when I try this code, it returns this output.</p>
<p><a href=""https://i.sstatic.net/1xa5X.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/1xa5X.png"" alt=""enter image description here"" /></a></p>
","huggingface"
"72770197","sentiment140 dataset doesn't contain label 2 i.e neutral sentences when uploading it from HuggingFace","2022-06-27 09:59:18","","0","251","<dataset><sentiment-analysis><huggingface-datasets><huggingface>","<p>I want to work with the sentiment140 dataset for a sentiment analysis task, as I saw that it contains normally the following labels :</p>
<ul>
<li>0, 4 for pos and neg sentences</li>
<li>2 for neutral sentences
which I found when looking at the dataset on their website :
<a href=""https://huggingface.co/datasets/sentiment140#data-fields"" rel=""nofollow noreferrer"">https://huggingface.co/datasets/sentiment140#data-fields</a></li>
</ul>
<p>But after importing it on my notebook it tells me that it contains just two labels :</p>
<ul>
<li>0 for neg</li>
<li>4 for pos !!!</li>
</ul>
<p><a href=""https://i.sstatic.net/25FMj.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/25FMj.png"" alt=""enter image description here"" /></a></p>
<p>So how to get full dataset with the three labels ?</p>
","huggingface"
"72762802","How to write a config.json to train a Language model","2022-06-26 15:17:18","","1","266","<nlp><huggingface-transformers><huggingface><bart>","<p>Looking at NLP models on Huggingface I can see that each of those has a so called <code>config.json</code> file. Now I want to train a <code>BART</code> mode from scratch along <a href=""https://github.com/octaviaguo/Constrained-Labeled-Data-Generation/tree/main/pipeline_scripts"" rel=""nofollow noreferrer"">this</a> repo. To do so, I have to &quot;write&quot; a <code>config.json</code> myself. Now I can not find any <strong>documentation</strong> what attributes this <code>config.json</code> needs to contain.</p>
<p>The training script from the linked <a href=""https://github.com/octaviaguo/Constrained-Labeled-Data-Generation/tree/main/pipeline_scripts"" rel=""nofollow noreferrer"">repo</a> (./t5_train/t5_train.py) does fail if the provided config does not contain certain attributes. That way I found out what is missing but without docs I am clueless with what I have to set them.</p>
<p>Example what is among the expected attributes that I dont find a documentation for:</p>
<pre><code>training:{
  device: ...,
  optimizer: ...,
  type: ...,
  noise: ...,
  noise_vocab: ...,
}
model:{
  src_lang: ...,
  trg_lang: ...,
  seq2seq: ...,
  dim_word_src: ...,
  n_layers_src: ...,
  bidirectional: ...,
}
</code></pre>
<hr />
<p><strong>Does somebody know where I can find a detailed documentation for how to write a <code>config.json</code> in particular and a <code>config.json</code> for BART in detail?</strong></p>
<hr />
<p>I allready took a look at <a href=""https://huggingface.co/docs/transformers/main_classes/configuration"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/main_classes/configuration</a> but I could not find the attributes that the script expects.</p>
","huggingface"
"72749730","Add new column to a HuggingFace dataset inside a dictionary","2022-06-24 21:51:12","72945095","1","1612","<python><dictionary><dataset><tokenize><huggingface>","<p>I have a tokenized dataset titled, <code>tokenized_datasets</code> as follows:</p>
<p><a href=""https://i.sstatic.net/yNMKd.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/yNMKd.png"" alt=""enter image description here"" /></a></p>
<p>I want to add a column titled <code>['labels']</code> that is a copy of <code>['input_ids']</code> within the features. I'm aware of the following method from this post <a href=""https://stackoverflow.com/questions/70064673/add-new-column-to-a-huggingface-dataset"">Add new column to a HuggingFace dataset</a>:</p>
<p><code>new_dataset = dataset.add_column(&quot;labels&quot;, tokenized_datasets['input_ids'].copy())</code></p>
<p>But I first need to access the Dataset Dictionary. This is what I have so far but it doesn't seem to do the trick:</p>
<pre><code>def new_column(example):
    example[&quot;labels&quot;] = example[&quot;input_ids&quot;].copy()
    return example

dataset_new = tokenized_datasets.map(new_column)

KeyError: 'input_ids'
</code></pre>
","huggingface"
"72741958","without the encode_plus method in tokenizers, how to make a feature matrix","2022-06-24 09:33:04","","0","1241","<huggingface-transformers><huggingface-tokenizers><huggingface>","<p>I am working on a low-resource language and need to make a classifier.
I used the tokenizers library to train the following tokenizers: WLV, BPE, UNI, WPC. I have saved the result of each into a json file.</p>
<p>I load each of the tokenizers using <code>Tokenizer.from_file</code> function.</p>
<pre><code>tokenizer_WLV = Tokenizer.from_file('tokenizer_WLV.json')
</code></pre>
<p>and I can see it is loaded properly. However only the method <code>encode</code> exists.</p>
<p>so if I do <code>tokenizer_WLV.encode(s1)</code>, I get an output like</p>
<pre><code>Encoding(num_tokens=7, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]
</code></pre>
<p>and I can see each token along with the id as following.</p>
<pre><code>out_wlv = tokenizer_WLV.encode(s1)
print(out_wlv.ids)
print(out_wlv.tokens)
</code></pre>
<p>I can use the <code>encode_batch</code></p>
<pre><code>def tokenize_sentences(sentences, tokenizer, max_seq_len = 128):
    tokenizer.enable_padding(pad_id=3, pad_token=&quot;[PAD]&quot;, direction='right')
    tokenized_sentences = tokenizer.encode_batch(sentences)
    return tokenized_sentences
</code></pre>
<p>which results in something like</p>
<pre><code>[Encoding(num_tokens=40, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),
 Encoding(num_tokens=40, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),
 Encoding(num_tokens=40, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),
 Encoding(num_tokens=40, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])]
</code></pre>
<p>I need to make a data feature in a size of mxn where m is the number of observations and n number of unique tokens. <code>encode_plus</code> does this automatically. So I am curious what is the most efficient way for constructing this feature matrix ?</p>
","huggingface"
"72732285","Unable to get Camel case tokens after tokenization in huggingface","2022-06-23 14:54:20","","0","173","<python-3.x><nlp><bert-language-model><huggingface-tokenizers><huggingface>","<p>I am trying to tokenize text by loading a vocab in huggingface.</p>
<pre><code>vocab_path = '....' ## have a local vocab path
tokenizer = BertWordPieceTokenizer(os.path.join(vocab_path, &quot;vocab.txt&quot;), lowercase=False)
text = 'The Quick Brown fox'
output = tokenizer.encode(text)
output.tokens
Out[1326]: ['[CLS]', '[UNK]', '[UNK]', '[UNK]', 'fox', '[SEP]']
</code></pre>
<p>The vocab I am using has everything in lower case. However, I want to tokenize it keeping the caps preserved.</p>
<p>Desired Output:</p>
<pre><code>['[CLS]', 'The', 'Quick', 'Brown', 'fox', '[SEP]']
</code></pre>
<p>When I use lowercase=True, it recognizes the words</p>
<pre><code>tokenizer = BertWordPieceTokenizer(os.path.join(vocab_path, &quot;vocab.txt&quot;), lowercase=True)

output = tokenizer.encode(text)

output.tokens
Out[1344]: ['[CLS]', 'the', 'quick', 'brown', 'fox', '[SEP]']
</code></pre>
<p>How do I get desired output using the lowercase vocab ?</p>
","huggingface"
"72690203","Getting KeyErrors when training Hugging Face Transformer","2022-06-20 16:31:55","72763938","0","2334","<python><pandas><nlp><huggingface-transformers><huggingface>","<p>I am generally following this tutorial (<a href=""https://huggingface.co/docs/transformers/training#:%7E:text=%F0%9F%A4%97%20Transformers%20provides%20access%20to,an%20incredibly%20powerful%20training%20technique."" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/training#:~:text=%F0%9F%A4%97%20Transformers%20provides%20access%20to,an%20incredibly%20powerful%20training%20technique.</a>) to implement fine-tuning on a pretrained transformer. The main difference is I am using my own custom dataset that is being sourced from a JSON file that has a document's text and the label it should belong to. To be able to do this I needed to create my own class which is based off of the Dataset class from pytorch. This is what that class looks like:</p>
<pre><code>class PDFsDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        
        print(&quot;\n\n\n\nindex&quot;,idx)
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

</code></pre>
<p>I am getting an error when training the transformer that says</p>
<pre><code>Traceback (most recent call last):
  File &quot;C:\Users\e417922\AppData\Roaming\Python\Python39\site-packages\pandas\core\indexes\base.py&quot;, line 3621, in get_loc
    return self._engine.get_loc(casted_key)
  File &quot;pandas\_libs\index.pyx&quot;, line 136, in pandas._libs.index.IndexEngine.get_loc
  File &quot;pandas\_libs\index.pyx&quot;, line 163, in pandas._libs.index.IndexEngine.get_loc
  File &quot;pandas\_libs\hashtable_class_helper.pxi&quot;, line 2131, in pandas._libs.hashtable.Int64HashTable.get_item
  File &quot;pandas\_libs\hashtable_class_helper.pxi&quot;, line 2140, in pandas._libs.hashtable.Int64HashTable.get_item
KeyError: 19

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File &quot;c:\Users\e417922\Downloads\enwiki-20220601-pages-meta-history1.xml-p1p857\HF_Transformer.py&quot;, line 147, in &lt;module&gt;
    transformer.train_transformer()
  File &quot;c:\Users\e417922\Downloads\enwiki-20220601-pages-meta-history1.xml-p1p857\HF_Transformer.py&quot;, line 135, in train_transformer
    trainer.train()
  File &quot;C:\Users\e417922\AppData\Roaming\Python\Python39\site-packages\transformers\trainer.py&quot;, line 1409, in train
    return inner_training_loop(
  File &quot;C:\Users\e417922\AppData\Roaming\Python\Python39\site-packages\transformers\trainer.py&quot;, line 1625, in _inner_training_loop
    for step, inputs in enumerate(epoch_iterator):
  File &quot;C:\Users\e417922\AppData\Roaming\Python\Python39\site-packages\torch\utils\data\dataloader.py&quot;, line 530, in __next__
    data = self._next_data()
  File &quot;C:\Users\e417922\AppData\Roaming\Python\Python39\site-packages\torch\utils\data\dataloader.py&quot;, line 570, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File &quot;C:\Users\e417922\AppData\Roaming\Python\Python39\site-packages\torch\utils\data\_utils\fetch.py&quot;, line 49, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File &quot;C:\Users\e417922\AppData\Roaming\Python\Python39\site-packages\torch\utils\data\_utils\fetch.py&quot;, line 49, in &lt;listcomp&gt;
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File &quot;c:\Users\e417922\Downloads\enwiki-20220601-pages-meta-history1.xml-p1p857\HF_Transformer.py&quot;, line 42, in __getitem__
    for key in self.encodings[idx]:
  File &quot;C:\Users\e417922\AppData\Roaming\Python\Python39\site-packages\pandas\core\series.py&quot;, line 958, in __getitem__
    return self._get_value(key)
  File &quot;C:\Users\e417922\AppData\Roaming\Python\Python39\site-packages\pandas\core\series.py&quot;, line 1069, in _get_value
    loc = self.index.get_loc(label)
  File &quot;C:\Users\e417922\AppData\Roaming\Python\Python39\site-packages\pandas\core\indexes\base.py&quot;, line 3623, in get_loc
    raise KeyError(key) from err
KeyError: 19
</code></pre>
<p>The KeyError that it fails on changes each time I run it. I'm a beginner with Transformers and HuggingFace so I have no clue what's causing this problem.</p>
<p>Edit:
Sample Input is a JSON File where elements would look like this:
{
&quot;text_clean&quot;: [
&quot;article with a few hundred words&quot;,
another article with a lot of words&quot;,
&quot;yet another article&quot;
],
&quot;most_similar_label&quot;:[
&quot;Quantum&quot;
&quot;Artificial intelligence&quot;
&quot;Materials&quot;
]
}</p>
<p>Full Code:</p>
<pre><code>import tkinter as tk
from tkinter import filedialog
import json
import pandas as pd
from transformers import AutoTokenizer
from transformers import AutoModelForSequenceClassification
from transformers import TrainingArguments
from transformers import TrainingArguments, Trainer
import numpy as np
from datasets import load_metric
from sklearn.model_selection import train_test_split
import torch

class PDFsDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        
        print(&quot;\n\n\n\nindex&quot;,idx)
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

class HFTransformer:
    def __init__ (self):
        pass

    def import_from_json(self):
        #Prompts user to select json file
        root = tk.Tk()
        root.withdraw()
        self.json_file_path = filedialog.askopenfile().name
        #opens json file and loads data
        with open(self.json_file_path, &quot;r&quot;) as json_file:
                try:
                    json_load = json.load(json_file)
                except:
                    raise ValueError(&quot;No PDFs to convert to JSON&quot;)
        self.pdfs = json_load
        #converts json file data to dataframe for easier manipulation
        self.pdfs = pd.DataFrame.from_dict(self.pdfs)

        for index in range(len(self.pdfs[&quot;new_tags&quot;])):
            if self.pdfs[&quot;new_tags&quot;][index] == &quot;&quot;:
                self.pdfs[&quot;new_tags&quot;][index] = self.pdfs[&quot;most_similar_label&quot;][index]

        self.pdfs[&quot;labels&quot;] = self.pdfs[&quot;new_tags&quot;].apply(lambda val: self.change_tag_to_num(val))
        # for label in self.data[&quot;labels&quot;]:
     
    def change_tag_to_num(self, value):
        if value == &quot;Quantum&quot;:
            return 0
        elif value == &quot;Artificial intelligence&quot;:
            return 1
        elif value == &quot;Materials&quot;:
            return 2
        elif value == &quot;Energy&quot;:
            return 3
        elif value == &quot;Defense&quot;:
            return 4
        elif value == &quot;Satellite&quot;:
            return 5
        elif value == &quot;Other&quot;:
            return 6

    def tokenize_dataset(self):
        tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-cased&quot;)

        X_train, X_test, y_train, y_test = train_test_split(self.pdfs[&quot;text_clean&quot;], self.pdfs[&quot;labels&quot;],test_size=0.2)

        train_encodings = X_train.apply(lambda string: tokenizer(string, truncation=True, padding=True,max_length=10))
        test_encodings = X_test.apply(lambda string: tokenizer(string, truncation=True, padding=True,max_length=10))
    
        
        self.train_dataset = PDFsDataset(train_encodings, y_train)
    
        data_to_add = {&quot;input_ids&quot;: [], &quot;token_type_ids&quot;: [], &quot;attention_mask&quot;: []}

        for i in self.train_dataset.encodings:
            data_to_add[&quot;input_ids&quot;].append(i[&quot;input_ids&quot;])
            data_to_add[&quot;token_type_ids&quot;].append(i[&quot;token_type_ids&quot;])
            data_to_add[&quot;attention_mask&quot;].append(i[&quot;attention_mask&quot;])

        self.train_dataset.encodings = data_to_add

        self.eval_dataset = PDFsDataset(test_encodings,y_test)
        data_to_add = {&quot;input_ids&quot;: [], &quot;token_type_ids&quot;: [], &quot;attention_mask&quot;: []}

        for i in self.eval_dataset.encodings:
            data_to_add[&quot;input_ids&quot;].append(i[&quot;input_ids&quot;])
            data_to_add[&quot;token_type_ids&quot;].append(i[&quot;token_type_ids&quot;])
            data_to_add[&quot;attention_mask&quot;].append(i[&quot;attention_mask&quot;])
        
        self.eval_dataset.encodings = data_to_add

    def train_transformer(self):
        model = AutoModelForSequenceClassification.from_pretrained(&quot;bert-base-cased&quot;, num_labels=7)
        training_args = TrainingArguments(output_dir=&quot;test_trainer&quot;)
        self.metric = load_metric(&quot;accuracy&quot;)
        training_args = TrainingArguments(output_dir=&quot;test_trainer&quot;, evaluation_strategy=&quot;epoch&quot;)
    

        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=self.train_dataset,
            eval_dataset=self.eval_dataset,
            compute_metrics=self.compute_metrics
        )
        trainer.train()
    def compute_metrics(self, eval_pred):
        logits, labels = eval_pred
        predictions = np.argmax(logits, axis=-1)
        return self.metric.compute(predictions=predictions, references=labels)


if __name__  == &quot;__main__&quot;:
    transformer = HFTransformer()
    transformer.import_from_json()
    transformer.tokenize_dataset()
    transformer.train_transformer()
</code></pre>
","huggingface"
"72680932","Try to run an NLP model with an Electra instead of a BERT model","2022-06-19 23:22:01","","0","180","<deep-learning><nlp><huggingface-transformers><bert-language-model><huggingface>","<p>I want to run the <a href=""https://github.com/vdobrovolskii/wl-coref"" rel=""nofollow noreferrer"">wl-coref</a> model with an Electra model instead of a Bert model. However, I get an error message with the Electra model and can't find a hint in the Huggingface documentation on how to fix it.</p>
<p>I try different BERT models such like roberta-base, bert-base-german-cased or SpanBERT/spanbert-base-cased. All works.
But if I try an Electra model, like google/electra-base-discriminator or german-nlp-group/electra-base-german-uncased then it doesn't work.</p>
<p>The error that is displayed:</p>
<pre><code>out, _ = self.bert(subwords_batches_tensor,  attention_mask=torch.tensor(attention_mask, device=self.config.device))
ValueError: not enough values to unpack (expected 2, got 1)
</code></pre>
<p>And this is the method where the error comes from:<a href=""https://github.com/vdobrovolskii/wl-coref/blob/master/coref/coref_model.py#L332"" rel=""nofollow noreferrer"">_bertify</a> in line 349.</p>
","huggingface"
"72672281","Does Huggingface's ""resume_from_checkpoint"" work?","2022-06-18 19:46:45","76003717","9","16404","<pytorch><huggingface-transformers><huggingface>","<p>I currently have my trainer set up as:</p>
<pre><code>training_args = TrainingArguments(
    output_dir=f&quot;./results_{model_checkpoint}&quot;,
    evaluation_strategy=&quot;epoch&quot;,
    learning_rate=5e-5,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    num_train_epochs=2,
    weight_decay=0.01,
    push_to_hub=True,
    save_total_limit = 1,
    resume_from_checkpoint=True,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_qa[&quot;train&quot;],
    eval_dataset=tokenized_qa[&quot;validation&quot;],
    tokenizer=tokenizer,
    data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),
    compute_metrics=compute_metrics
)
</code></pre>
<p>After training, in my <code>output_dir</code> I have several files that the trainer saved:</p>
<pre><code>['README.md',
 'tokenizer.json',
 'training_args.bin',
 '.git',
 '.gitignore',
 'vocab.txt',
 'config.json',
 'checkpoint-5000',
 'pytorch_model.bin',
 'tokenizer_config.json',
 'special_tokens_map.json',
 '.gitattributes']
</code></pre>
<p>From the <a href=""https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Trainer.train.resume_from_checkpoint"" rel=""noreferrer"">documentation</a> it seems that <code>resume_from_checkpoint</code> will continue training the model from the last checkpoint:</p>
<p><code>resume_from_checkpoint (str or bool, optional) — If a str, local path to a saved checkpoint as saved by a previous instance of Trainer. If a bool and equals True, load the last checkpoint in args.output_dir as saved by a previous instance of Trainer. If present, training will resume from the model/optimizer/scheduler states loaded here.</code></p>
<p>But when I call <code>trainer.train()</code> it seems to delete the last checkpoint and start a new one:</p>
<pre><code>Saving model checkpoint to ./results_distilbert-base-uncased/checkpoint-500
...
Deleting older checkpoint [results_distilbert-base-uncased/checkpoint-5000] due to args.save_total_limit
</code></pre>
<p>Does it really continue training from the last checkpoint (i.e., 5000) and just starts the count of the new checkpoint at 0 (saves the first after 500 steps -- &quot;checkpoint-500&quot;), or does it simply not continue the training? I haven't found a way to test it and the documentation is not clear on that.</p>
","huggingface"
"72628552","How to chose the right LineByLineTextDataset-parameters for the transformers LineByLineTextDataset","2022-06-15 08:59:35","","1","1603","<python><pytorch><huggingface-transformers><roberta-language-model><huggingface>","<p>From <a href=""https://legacyai.github.io/tf-transformers/build/html/model_doc/roberta.html"" rel=""nofollow noreferrer"">this website explaining the Roberta parameters</a>, I understood that the
<code>max_position_embeddings</code> should be a power of 2.
Then from <a href=""https://github.com/facebookresearch/fairseq/issues/1187"" rel=""nofollow noreferrer"">this GitHub issue</a>, I understood that we should add 2 to the <code>max_position_embeddings</code> value while setting the <code>RobertaConfig</code> parameters.
So, we will get:</p>
<pre><code>max_position_embeddings_value = 512 # power of 2
config = RobertaConfig(
    max_position_embeddings=max_position_embeddings_value+2
)
</code></pre>
<p>Now, I am wondering how to specify the <code>LineByLineTextDataset</code>-parameters.</p>
<pre><code>from transformers import LineByLineTextDataset
dataset = LineByLineTextDataset(
                tokenizer=tokenizer,
                file_path='path_text.txt',
                block_size=max_position_embeddings_value,
            )

</code></pre>
<p>Do the <code>LineByLineTextDataset</code>-parameters depend on the <code>RobertaConfig</code>-parameters ?</p>
<ol>
<li>Suppose each line in the <code>path_text.txt</code> file has a lenght of <code>n</code>.
Should I specify the <code>max_position_embeddings</code>according to the lenght <code>n</code> ?<br />
e.g. what will be the value of <code>max_position_embeddings_value</code> if <code>n = 10</code>, or <code>n = 10</code> or <code>n = 500</code> or <code>n = 1000</code> ... `?</li>
<li>Should <code>block_size = max_position_embeddings_value</code> ? Does it depend on the lenght <code>n</code>?</li>
</ol>
<p>I wanted to chose <code>max_position_embeddings_value</code> following this rule :</p>
<pre><code>for n &gt; 128 and n &lt; 256, 
max_position_embeddings_value = 256, as it is a power of 2, closer to n and greater than n

for n &gt; 256 and n &lt; 512, 
max_position_embeddings_value = 512, as it is a power of 2, closer to n and greater than n

for n &gt; 512 and n &lt; 1024, 
max_position_embeddings_value = 1024, as it is a power of 2, closer to 10n24 and greater than n

etc ... 
</code></pre>
<p>Is it a good approach ?</p>
","huggingface"
"72592998","Is it possible to perform local dev on a CPU-only machine on HF/sagemaker?","2022-06-12 14:13:13","","0","573","<amazon-sagemaker><huggingface-transformers><huggingface>","<p>I'm trying to dev locally on <code>sagemaker.huggingface.HuggingFace</code> before moving to sagemaker for actual training. I set up a</p>
<p><code>HF_estimator = HuggingFace(entry_point='train.py', instance_type='local' ...)</code></p>
<p>And called <code>HF_estimator.fit()</code></p>
<p>In <code>train.py</code> im simply printing and exiting to see if it will work. However I ran into this:</p>
<pre><code>ValueError: Unsupported processor: cpu. You may need to upgrade your SDK version (pip install -U sagemaker) for newer processors. Supported processor(s): gpu.
</code></pre>
<p>Is it possible to bypass this for local development?</p>
","huggingface"
"72572346","Logging in to Huggingface","2022-06-10 09:55:55","","2","837","<databricks><huggingface>","<p>Does anybody have experience running huggingface transformers on Databricks? I can download datasets, model checkpoints, tokenizers, etc without a problem. However, once I have a model fine-tuned (it is a text classification task) I want to upload it to the huggingface hub I haven't found a way to log in via the Databricks notebook.</p>
<p>I tried</p>
<pre><code>from huggingface_hub import notebook_login
notebook_login()
</code></pre>
<p>but, like when the model is training, the notebook doesn't display HTML. So I tried</p>
<pre><code>displayHTML(notebook_login())
</code></pre>
<p>but this gives me a java null pointer exception.</p>
","huggingface"
"72561808","What should be the vocab of the tokenizer?","2022-06-09 14:08:53","","0","196","<python-3.x><tokenize><huggingface-transformers><huggingface-tokenizers><huggingface>","<p>I am trying to use a tokenizer from huggingface. However, I do not have the vocab.</p>
<pre><code>from tokenizers import BertWordPieceTokenizer , CharBPETokenizer, ByteLevelBPETokenizer
from tokenizers import Tokenizer

text = 'the quick brown fox jumped over the lazy dog !!!'
tokenizer = CharBPETokenizer()
print(tokenizer)
#Tokenizer(vocabulary_size=0, model=BPE, unk_token=&lt;unk&gt;, suffix=&lt;/w&gt;, dropout=None, #lowercase=False, unicode_normalizer=None, bert_normalizer=True, #split_on_whitespace_only=False)

tokenizer = Tokenizer(BPE())
out = tokenizer.encode(text)
out.tokens
Out[33]: []
</code></pre>
<p>According to <a href=""https://github.com/huggingface/tokenizers/blob/main/bindings/python/py_src/tokenizers/implementations/char_level_bpe.py"" rel=""nofollow noreferrer"">https://github.com/huggingface/tokenizers/blob/main/bindings/python/py_src/tokenizers/implementations/char_level_bpe.py</a> , without vocab this should just use Tokenizer(BPE()) .</p>
<p>I think it might be a lack of vocab issue. Can someone point me where to get default vocab
for BertWordPieceTokenizer , CharBPETokenizer, ByteLevelBPETokenizer ,  SentencePieceUnigramTokenizer and BaseTokenizer.</p>
","huggingface"
"72519845","huggingface transformer issue","2022-06-06 15:21:15","","1","1037","<python><bert-language-model><transformer-model><huggingface>","<p>I used huggingface transformer, but I got some issues like below.
How can I handle this problem?</p>
<pre><code>training_args = TrainingArguments(
    output_dir='./.checkpoints',
    num_train_epochs=config.n_epochs,
    per_device_train_batch_size=config.batch_size_per_device,
    per_device_eval_batch_size=config.batch_size_per_device,
    warmup_steps=n_warmup_steps,
    weight_decay=0.01,
    fp16=True,
    evaluation_strategy='epoch',
    save_strategy='epoch',
    logging_steps=n_total_iterations // 100,
    save_steps=n_total_iterations // config.n_epochs,
    load_best_model_at_end=True, )
</code></pre>
<hr />
<pre><code>  File &quot;finetune_plm_hftrainer.py&quot;, line 134, in main
    load_best_model_at_end=True,
  File &quot;&lt;string&gt;&quot;, line 90, in __init__
  File &quot;/usr/local/lib/python3.6/dist-packages/transformers/training_args.py&quot;, line 813, in __post_init__
    raise ValueError(f&quot;logging strategy {self.logging_strategy} requires non-zero --logging_steps&quot;)
ValueError: logging strategy IntervalStrategy.STEPS requires non-zero --logging_steps
</code></pre>
","huggingface"
"72486821","Summarization with Huggingface: How to generate one word at a time?","2022-06-03 08:30:48","72815032","0","1589","<huggingface-transformers><summarization><huggingface>","<p>I am using a DistilBART for abstractive summarization. The method <a href=""https://huggingface.co/docs/transformers/v4.19.2/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate"" rel=""nofollow noreferrer""><code>generate()</code></a> is very straightforward to use. However, it returns complete, finished summaries. <strong>What I want is, at each step, access the logits to then get the list of next-word candidates and choose based on my own criteria.</strong> Once chosen, continue with the next word and so on until the EOS token is produced.</p>
<p>I am aware that I can access the logits by doing <code>model(**input).logits[:, -1, :]</code>, but here the input would be the whole (encoded) text, so what would exactly these logits correspond with? The first generated token? The last?</p>
<p>Thank you for your answers!</p>
","huggingface"
"72469258","Adding a pretrained model outside of AllenNLP to the AllenNLP demo","2022-06-02 00:10:32","","0","275","<pytorch><demo><allennlp><interpretation><huggingface>","<p>I am working on the interpretability of models. I want to use AllenAI demo to check the saliency maps and adversarial attack methods (implemented in this demo) on some other models. I use the tutorial <a href=""https://github.com/allenai/allennlp-demo#contributing-a-new-allennlp-model-to-the-demo"" rel=""nofollow noreferrer"">here</a> and run the demo on my local machine. Now that I want to load my pretrained model which is from the huggingface (&quot;cardiffnlp/twitter-roberta-base-sentiment-latest&quot; using this <a href=""https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest"" rel=""nofollow noreferrer"">code</a>) I don't know how to add the model to the demo. I checked the tutorial <a href=""https://github.com/allenai/allennlp-demo/blob/SweepNotBrokenDammit/doc/AddingModelEndpoint.md"" rel=""nofollow noreferrer"">here</a> but this guide only is based on the models implemented in AllenNLP.</p>
<p>These are the changes on the new directory(roberta_sentiment_twitter) I made in allennlp_demo file but for sure it is not true since the main implementation only uses the models implemented in allennlp.</p>
<pre><code>#in model.json
{
&quot;id&quot;: &quot;roberta-sentiment-twitter&quot;,
&quot;pretrained_model_id&quot;: &quot;cardiffnlp/twitter-roberta-base-sentiment-latest&quot;
}

#in api.py
import os
from allennlp_demo.common import config, http
from transformers import AutoModelForSequenceClassification
from transformers import AutoTokenizer, AutoConfig

if __name__ == &quot;__main__&quot;:

    MODEL = f&quot;cardiffnlp/twitter-roberta-base-sentiment-latest&quot;
    tokenizer = AutoTokenizer.from_pretrained(MODEL)
    config = AutoConfig.from_pretrained(MODEL)
    # model = AutoModelForSequenceClassification.from_pretrained(MODEL)

    endpoint = AutoModelForSequenceClassification.from_pretrained(MODEL)
    endpoint.run()


#in test_api.py
from allennlp_demo.common.testing import ModelEndpointTestCase
from allennlp_demo.roberta_sentiment_twitter.api import RobertaSentimentAnalysisModelEndpoint


class TestRobertaSentimentTwitterModelEndpoint(ModelEndpointTestCase):
    endpoint = RobertaSentimentAnalysisModelEndpoint()
    predict_input = {&quot;sentence&quot;: &quot;a very well-made, funny and entertaining picture.&quot;}
</code></pre>
<p>Is there any straightforward ways to load my models in the AllenNLP demo?<br />
Also in the future I want to add some other interpretability method to this demo. Is there any tutorial for that too?</p>
","huggingface"
"72425277","How do you install a library from HuggingFace? E.g. GPT Neo 125M","2022-05-29 16:11:23","72515673","1","956","<machine-learning><artificial-intelligence><huggingface-transformers><huggingface>","<p>I am confused on how to install a library from HuggingFace on your own desktop or server. How complicated is it to install a library? Are there step by step instructions anywhere? I found some articles, but they assumed a certain level of knowledge and I’m a total beginner and was unable to follow them.</p>
<p>To be more specific, I was looking at the GPT libraries. GPT Neo 125M seems to be the smallest of these so I’m assuming that would be the easiest to install.
<a href=""https://huggingface.co/EleutherAI"" rel=""nofollow noreferrer"">https://huggingface.co/EleutherAI</a>
<a href=""https://huggingface.co/EleutherAI/gpt-neo-125M"" rel=""nofollow noreferrer"">https://huggingface.co/EleutherAI/gpt-neo-125M</a></p>
<p>Also, once you install a library on your own machine, is it free to use? I see that HuggingFace has a pricing structure:</p>
<p><a href=""https://huggingface.co/pricing"" rel=""nofollow noreferrer"">https://huggingface.co/pricing</a></p>
<p>But I’m not sure what it applies to. Does this pricing structure apply if you host the model on your own computer?</p>
<p>I’m a total noob to this stuff so any tips are appreciated.</p>
","huggingface"
"72398129","Creating a sentence-transformer model in Spark Mllib","2022-05-26 21:20:27","","3","1811","<apache-spark><pyspark><apache-spark-mllib><sentence-transformers><huggingface>","<p>I used a pre-trained model from the sentence transformer library to check the similarity between two sentences. Now I need this particular model to be implemented using spark mllib. Any Suggestions? I really appreciate any help you can provide.</p>
<p><a href=""https://www.sbert.net/"" rel=""nofollow noreferrer"">https://www.sbert.net/</a>
<a href=""https://spark.apache.org/mllib/"" rel=""nofollow noreferrer"">https://spark.apache.org/mllib/</a></p>
","huggingface"
"72340801","Huggingface Load_dataset() function throws ""ValueError: Couldn't cast""","2022-05-22 19:43:27","72342130","2","8854","<machine-learning><nlp><sentiment-analysis><huggingface-tokenizers><huggingface>","<p>My goal is to train a classifier able to do sentiment analysis in Slovak language using loaded SlovakBert model and HuggingFace library. Code is executed on Google Colaboratory.</p>
<p>My test dataset is read from this csv file:
<a href=""https://raw.githubusercontent.com/kinit-sk/slovakbert-auxiliary/main/sentiment_reviews/kinit_golden_games.csv"" rel=""nofollow noreferrer"">https://raw.githubusercontent.com/kinit-sk/slovakbert-auxiliary/main/sentiment_reviews/kinit_golden_games.csv</a></p>
<p>and train dataset:
<a href=""https://raw.githubusercontent.com/kinit-sk/slovakbert-auxiliary/main/sentiment_reviews/kinit_golden_accomodation.csv"" rel=""nofollow noreferrer"">https://raw.githubusercontent.com/kinit-sk/slovakbert-auxiliary/main/sentiment_reviews/kinit_golden_accomodation.csv</a></p>
<p>Data has two columns: column of Slovak sentences and 2nd column of labels which indicate sentiment of the sentence. Labels have values -1, 0 or 1.</p>
<p>Load_dataset() function throws this error:</p>
<blockquote>
<p>ValueError: Couldn't cast
Vrtuľník je veľmi zraniteľný pri dobre mierenej streľbe zo zeme. Brániť sa, unikať, alebo vedieť zneškodniť nepriateľa je vecou sekúnd, ak nie stotín, kedy ide život. : string
-1: int64
-- schema metadata --
pandas: '{&quot;index_columns&quot;: [{&quot;kind&quot;: &quot;range&quot;, &quot;name&quot;: null, &quot;start&quot;: 0, &quot;' + 954
to
{'Priestorovo a vybavenim OK.': Value(dtype='string', id=None), '1': Value(dtype='int64', id=None)}
because column names don't match</p>
</blockquote>
<p>Code:</p>
<pre><code>!pip install transformers==4.10.0 -qqq
!pip install datasets -qqq

from re import M
import numpy as np
from datasets import load_metric, load_dataset, Dataset
from transformers import TrainingArguments, Trainer, AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding
import pandas as pd
from textblob import TextBlob
from textblob.sentiments import NaiveBayesAnalyzer

#links to dataset
test = 'https://raw.githubusercontent.com/kinit-sk/slovakbert-auxiliary/main/sentiment_reviews/kinit_golden_games.csv'
train = 'https://raw.githubusercontent.com/kinit-sk/slovakbert-auxiliary/main/sentiment_reviews/kinit_golden_accomodation.csv'


model_name = 'gerulata/slovakbert'


#Load data
dataset = load_dataset('csv', data_files={'train': train, 'test': test})
</code></pre>
<p>What is done wrong while loading the dataset?</p>
","huggingface"
"72301546","How do I hide certain labels on Huggingface inference?","2022-05-19 08:58:01","","1","82","<huggingface-transformers><huggingface>","<p>I am currently using my customized labels in a token classification model and have pushed the model to hub.
I am using the labels shown as below:</p>
<pre><code>label_list = [“NORM”, “COMMA”, “PERIOD”, “QUEST_MARK”, “EXCLAMATION”]

label2id = 
{‘COMMA’: ‘1’,
‘EXCLAMATION’: ‘4’,
‘NORM’: ‘0’,
‘PERIOD’: ‘2’,
‘QUEST_MARK’: ‘3’}

id2label = 
{‘0’: ‘NORM’,
‘1’: ‘COMMA’,
‘2’: ‘PERIOD’,
‘3’: ‘QUEST_MARK’,
‘4’: ‘EXCLAMATION’}
</code></pre>
<p>Currently on inference API, it shows:
<a href=""https://i.sstatic.net/UnDtx.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/UnDtx.png"" alt=""enter image description here"" /></a></p>
<p>How may I hide the label of <code>“NORM”(id=0)</code> and make the inference only show <code>labels of id1-4 (“COMMA”, “PERIOD”, etc.)</code>?
Thank you!</p>
","huggingface"
"72289714","How to validate Hugging Face organization token?","2022-05-18 13:00:33","72297034","1","732","<curl><huggingface-transformers><huggingface>","<p><code>/whoami-2</code> endpoint returns <code>Unauthorized</code> for organization tokens, the ones that start with <code>api_...</code>.</p>
<pre><code>$ curl https://huggingface.co/api/whoami-2 -H &quot;Authorization: Bearer api_&lt;token&gt;&quot;
&gt; { &quot;error&quot;: &quot;Unauthorized&quot; }
</code></pre>
<p>At the same time I can use the same token to get private models. Should I use some other endpoint to validate the tokens?</p>
","huggingface"
"72261504","Hugginface Transformers Bert Tokenizer - Find out which documents get truncated","2022-05-16 15:12:58","72267782","2","792","<python><machine-learning><huggingface-transformers><huggingface-tokenizers><huggingface>","<p>I am using the Transforms library from Huggingface to create a text classification model based on Bert. For this I tokenise my documents and I set truncation to be true as my documents are longer than allowed (512).</p>
<p>How can I find out how many documents are actually getting truncated? I don't think the length (512) is character or word count of the document, as the Tokenizer prepares the document as input for the model. What happens to the document and is there a straight forward way to check whether or not it gets truncated?</p>
<p>This is the code I use to tokenise the documents.</p>
<pre><code>tokenizer = AutoTokenizer.from_pretrained(&quot;distilbert-base-multilingual-cased&quot;) 
model = BertForSequenceClassification.from_pretrained(&quot;distilbert-base-multilingual-cased&quot;, num_labels=7)
train_encoded =  tokenizer(X_train, padding=True, truncation=True, return_tensors=&quot;pt&quot;)
</code></pre>
<p>In case you have any more questions about my code or problem, feel free to ask.</p>
","huggingface"
"72247359","Using huggingface's accelerate with 2GPUs and throwed an error:RuntimeError: Expected to mark a variable ready only once","2022-05-15 10:18:25","","2","1624","<python><multi-gpu><huggingface>","<p>I want to train BigBird with 2gpus, and I use huggingface's accelerate in notebook,so I use notebook_launcher.</p>
<p>I have followed the example in <a href=""https://github.com/huggingface/notebooks/blob/main/examples/accelerate/simple_nlp_example.ipynb"" rel=""nofollow noreferrer"">https://github.com/huggingface/notebooks/blob/main/examples/accelerate/simple_nlp_example.ipynb</a></p>
<p>and added some extra code :</p>
<pre><code>ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)
accelerator = Accelerator(kwargs_handlers=[ddp_kwargs])
</code></pre>
<p>according to <a href=""https://github.com/huggingface/accelerate/issues/24"" rel=""nofollow noreferrer"">https://github.com/huggingface/accelerate/issues/24</a></p>
<p>because python threw an error:</p>
<p><code>RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one.</code></p>
<p>after runing the code at first.But at second run, python also threw an error that:</p>
<p><code>RuntimeError: Expected to mark a variable ready only once. This error is caused by one of the following reasons: 1) Use of a module parameter outside the `forward` function. Please make sure model parameters are not shared across multiple concurrent forward-backward passes. or try to use _set_static_graph() as a workaround if this module graph does not change during training loop.2) Reused parameters in multiple reentrant backward passes. For example, if you use multiple `checkpoint` functions to wrap the same part of your model, it would result in the same set of parameters been used by different reentrant backward passes multiple times, and hence marking a variable ready multiple times. DDP does not support such use cases in default. You can try to use _set_static_graph() as a workaround if your module graph does not change over iterations. Parameter at index 99 has been marked as ready twice. This means that multiple autograd engine  hooks have fired for this particular parameter during this iteration. You can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print parameter names for further debugging.</code></p>
<p>I don't know how to deal with it, then I began my targetless exploring and found that setting <code>model.transformer.gradient_checkpointing_disable()</code> after initializing the model would complete some forward loops,but it always dead in the 11th step even 25 minutes had past.</p>
<p>Here is my training function and could anyone helps me find the error in my code out so that my code can run through a epoch? Thanks in advance!(<em>^_^</em>)</p>
<pre><code>def training_function():
    ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)
    accelerator = Accelerator(kwargs_handlers=[ddp_kwargs])
    if accelerator.is_main_process:
        datasets.utils.logging.set_verbosity_warning()
        transformers.utils.logging.set_verbosity_info()
    else:
        datasets.utils.logging.set_verbosity_error()
        transformers.utils.logging.set_verbosity_error()
    set_seed(hyperparameters['seed'])
    global tokenizer
    tokenizer = JiebaTokenizer.from_pretrained('Lowin/chinese-bigbird-small-1024')
    model = BB(hyperparameters)
    # model.transformer.gradient_checkpointing_disable()
    optimizer = optim.AdamW(params=model.parameters(),lr=1e-5,weight_decay=1e-2)
    
    dataSetTrain = DS(trains,tokenizer,hyperparameters)
    dataSetValid = DS(valida,tokenizer,hyperparameters)
    tDL = DataLoader(dataSetTrain,batch_size=hyperparameters['batch_size_train'],shuffle=True)
    vDL = DataLoader(dataSetValid,batch_size=hyperparameters['batch_size_valid'])
    
    model,optimizer,tDL,vDL = accelerator.prepare(
        model,optimizer,tDL,vDL
    )
    num_epochs = hyperparameters[&quot;epoch&quot;]
    progress_bar = tqdm(range(num_epochs * len(tDL)), disable=not accelerator.is_main_process)

    rouge = load_metric('rouge')
    step = 0
    epochs = hyperparameters['epoch']
    vocab_size = hyperparameters['vocab_size']
    accumulate_setp = hyperparameters['accumulate_setp']
    for epoch in range(epochs):
        model.train()
        for batch in tDL:
            step += 1
            labels = batch.pop('labels')
            logits = model(batch)
            loss_sum = F.cross_entropy(logits.view(-1,vocab_size),labels.view(-1),reduction='sum')
            title_length = labels.ne(0).sum().item()
            loss = loss_sum/title_length
            loss = loss/accumulate_setp
            accelerator.backward(loss)
            
            if step % accumulate_setp == 0:
                # torch.nn.utils.clip_grad_norm_(model.parameters(), 2)
                optimizer.step()
                optimizer.zero_grad()
            progress_bar.update(1)
            writer.add_scalar('Loss',loss,step)  # 将损失写入tensorboard，便于观察loss变化
        if epoch % 5 == 0:
            allIndexes = []
            allLabels = []
            with torch.no_grad():
                model.eval()
                for sample in vDL:
                    label = sample.pop('labels')
                    logits = model(sample)

                    logits = logits[0]
                    assert len(logits.shape) == 2

                    index = logits.argmax(dim=1)
                    index = index&gt;0  # 获取token_id不为0的所有token 所在的输出向量的索引
                    index = logits[index].argmax(dim=1)

                    label = label[label!=0]

                    allIndexes.append(index)
                    allLabels.append(label)
                result = rouge.compute(predictions=allIndexes,references=allLabels)
                rouge1Recall = result['rouge1'][1][1]
                rouge2Recall = result['rouge2'][1][1]
                rougeLRecall = result['rougeL'][1][1]
                accelerator.print(f'epoch{epoch}: rouge1:{rouge1Recall} rouge2:{rouge2Recall} rougeL:{rougeLRecall}')

    writer.close()
    accelerator.print('Final result:\n',result)
</code></pre>
","huggingface"
"72247142","How truncation works when applying BERT tokenizer on the batch of sentence pairs in HuggingFace?","2022-05-15 09:44:40","","1","1562","<huggingface-transformers><bert-language-model><huggingface-tokenizers><huggingface>","<p>Say, I have three sample sentences:</p>
<pre><code>s0 = &quot;This model was pretrained using a specific normalization pipeline available here!&quot;
s1 = &quot;Thank to all the people around,&quot;
s2 = &quot;Bengali Mask Language Model for Bengali Language&quot;
</code></pre>
<p>I could make a batch like:</p>
<pre><code>batch = [[s[0], s[1]], [s[1], s[2]]]
</code></pre>
<p>Now, if I apply the <strong>BERT</strong> tokenizer on the sentence pairs, it truncates the sentence pairs if the length exceeds in such a way that the ultimate sum of the sentence pairs' lengths meets the <code>max_length</code> parameter, which was supposed to be done, okay. Here's what I meant:</p>
<pre><code>tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
model = AutoModelForPreTraining.from_pretrained(&quot;bert-base-uncased&quot;)

encoded = tokenizer(batch, padding=&quot;max_length&quot;, truncation=True, max_length=10)[&quot;input_ids&quot;]
decoded = tokenizer.batch_decode(encoded)
print(decoded)

&gt;&gt;&gt;Output: ['[CLS] this model was pre [SEP] thank to all [SEP]', '[CLS] thank to all [SEP] bengali mask language model [SEP]']
</code></pre>
<p><strong>My question is, how does the <code>truncation</code> work here in the pair of sentences where the number of tokens from each sentence of each pair is not equal?</strong></p>
<p>For example, in the first example output <code>'[CLS] this model was pre [SEP] thank to all [SEP]'</code> number of tokens from the two sentences has not come equally <em>i.e</em> <strong>[CLS] 4 tokens [SEP] 3 tokens [SEP]</strong>.</p>
","huggingface"
"72215217","Huggingface distilbert-base-uncased-finetuned-sst-2-english runs out of ram with only a few kb?","2022-05-12 12:02:27","","0","679","<python><nlp><pytorch><huggingface-transformers><huggingface>","<p>My dataset is only 10 thousand sentences. I run it in batches of 100, and clear the memory on each run. I manually slice the sentences to only 50 characters. After running for 32 minutes, it crashes... On google colab with 25 gigs of ram.</p>
<p>I must be doing something terribly wrong.</p>
<p>I'm using the out-of-the-box model and tokenizer.</p>
<pre><code>
  def eval_model(model, tokenizer_, X, y, batchsize, maxlength):
    assert len(X) == len(y)
    labels = [&quot;negative&quot;, &quot;positive&quot;]
    correctCounter = 0
    
    epochs = int(np.ceil(len(dev_sent) / batchsize))
    accuracies = []
    for i in range(epochs):
      print(f&quot;Epoch {i}&quot;)
      # slicing up the data into batches
      X_ = X[i:((i+1)*100)]
      X_ = [x[:maxlength] for x in X_] # make sure sentences are only of maxlength 
      y_ = y[i:((i+1)*100)]

      encoded_input = tokenizer(X_, return_tensors='pt', padding=True,  truncation=True, max_length=maxlength)
      output = model(**encoded_input)
      scores = output[0][0].detach().numpy()
      scores = softmax(scores)
      for i, scores in enumerate([softmax(logit) for logit in output[&quot;logits&quot;].detach().numpy()]):
        print(&quot;--------------------&quot;)
        print(f'Sentence no. {len(accuracies)+1}')
        print(&quot;Sentence: &quot; + X_[i])
        print(&quot;Score: &quot; + str(scores))
        ranking = np.argsort(scores)
        print(f&quot;Ranking: {ranking}&quot;)
        pred = labels[np.argmax(np.argsort(scores))]
        print(f&quot;Prediction: {pred}, annotation: {y_[i]}&quot;)
        if pred == y_[i]: 
          print(&quot;SUCCES!&quot;)
          correctCounter += 1
        else: print(&quot;FAILURE!&quot;)
      # garbage collection (to not run out of ram... Which is shouldn't, it's just a few kb, but it does.... ?!)
      del(encoded_input)
      del(output)
      del(scores)
      gc.collect()
      accuracies.append(correctCounter / len(y_))
      #print(f&quot;current accuracy: {np.mean(np.asarray(accuracies))}&quot;)
    return np.mean(np.asarray(accuracies))

  task='sentiment'
  MODEL = f&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;

  tokenizer = AutoTokenizer.from_pretrained(MODEL)
  model = AutoModelForSequenceClassification.from_pretrained(MODEL)
  model.save_pretrained(MODEL)
  tokenizer.save_pretrained(MODEL)

  accuracy = eval_model(model, tokenizer, dev_sent, dev_sentiment, 100, 50)

</code></pre>
<p>EDIT: here is the code on google colab <a href=""https://colab.research.google.com/drive/1qKTabPTNYWEILoua0gIvlYDly8jcibr0?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1qKTabPTNYWEILoua0gIvlYDly8jcibr0?usp=sharing</a></p>
","huggingface"
"72211572","List and delete cached models from HuggingFace","2022-05-12 07:24:25","","2","80","<python><huggingface-transformers><huggingface>","<p>When using a new model from HuggingFace, the necessary files are downloaded to a <code>.cache/huggingface/transformers</code> folder. However, these files are named using id numbers rather than with names referring to the type of model.</p>
<p>I wonder how I can list downloaded/existing models within python, and whether it is then possible to delete specified models using Python?</p>
","huggingface"
"72190438","How to predict results from 20 million records using Hugging Face Model in minimum time","2022-05-10 17:07:56","","0","273","<dataframe><prediction><huggingface-transformers><huggingface-tokenizers><huggingface>","<p>I am trying to predict sentiment for 20 million records using the model available in Hugging Face.</p>
<p><a href=""https://huggingface.co/finiteautomata/beto-sentiment-analysis"" rel=""nofollow noreferrer"">https://huggingface.co/finiteautomata/beto-sentiment-analysis</a></p>
<p>This model takes 1 hour and 20 minutes to predict 70000 records.</p>
<p>The model is saved locally and accessed locally by loading it.</p>
<p>Anyone can please suggest how I can efficiently use it to predict 20 million records in a minimum time.</p>
<p>Also, I am using the Zero-Shot Classification Model on the same data it is taking taking</p>
<p>7 minutes to predict for 1000 records.</p>
<p>Kindly suggest for this as well if any way to predict in minimum time.</p>
<pre><code>model_path = 'path where model is saved'
from transformers import pipeline
classifier = pipeline(&quot;zero-shot-classification&quot;, 
                       model=&quot;Recognai/bert-base-spanish-wwm-cased-xnli&quot;)
   
def predict(row):
    topics = # five candidate labels here
    res = classifier(row, topics)
    return res

df['Predict'] = df['Message'].apply(lambda x: predict_crash(x)) # This df contains 70k records
</code></pre>
","huggingface"
"72173639","KeyError: 'allocated_bytes.all.current'","2022-05-09 14:27:42","","2","1400","<python><deep-learning><huggingface>","<p>Si I upload in huggingface spaces the VQGAN+CLIP code, the problem it that when I try to use it, it gives me this error.
<a href=""https://i.sstatic.net/FKBMc.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/FKBMc.png"" alt=""enter image description here"" /></a></p>
<p>I don't know what this means</p>
","huggingface"
"72111280","ValueError: Tokenizer class MarianTokenizer does not exist or is not currently imported","2022-05-04 10:14:06","72111329","3","3834","<python><huggingface><nmt>","<p>Get this error when trying to run a MarianMT-based nmt model.</p>
<pre><code>Traceback (most recent call last):
File &quot;/home/om/Desktop/Project/nmt-marionmt-api/inference.py&quot;, line 45, in &lt;module&gt;
    print(batch_inference(model_path=&quot;en-ar-model/Mark2&quot;, text=text))
  File &quot;/home/om/Desktop/Project/nmt-marionmt-api/inference.py&quot;, line 15, in batch_inference
    tokenizer = AutoTokenizer.from_pretrained(model_path, local_file_only=True)
  File &quot;/home/om/.virtualenvs/marianmt-api/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py&quot;, line 525, in from_pretrained
    raise ValueError(
ValueError: Tokenizer class MarianTokenizer does not exist or is not currently imported.
</code></pre>
","huggingface"
"72036646","Converting h5 to tflite","2022-04-28 00:27:23","72039808","-1","694","<tensorflow><tensorflow-lite><huggingface-transformers><text-classification><huggingface>","<p>I have been trying to get this zero-shot text classification <code>joeddav / xlm-roberta-large-xnli</code> to convert from h5 to tflite file (<a href=""https://huggingface.co/joeddav/xlm-roberta-large-xnli"" rel=""nofollow noreferrer"">https://huggingface.co/joeddav/xlm-roberta-large-xnli</a>), but this error pops up and I cant find it described online, how is it fixed? If it can't, is there another zero-shot text classifier I can use that would produce similar accuracy even after becoming tflite?</p>
<pre><code>AttributeError: 'T5ForConditionalGeneration' object has no attribute 'call'
</code></pre>
<p>I have been trying a few different tutorials and the current google colab file I have is an amalgam of a couple of them. <a href=""https://colab.research.google.com/drive/1sYQJqvhM_KEvMt2IP15d8Ud9L-ApiYv6?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1sYQJqvhM_KEvMt2IP15d8Ud9L-ApiYv6?usp=sharing</a></p>
","huggingface"
"72021814","How do I save a Huggingface dataset?","2022-04-26 23:57:40","72021864","20","49703","<huggingface-datasets><huggingface>","<p>How do I write a HuggingFace dataset to disk?</p>
<p>I have made my own HuggingFace dataset using a JSONL file:</p>
<blockquote>
<p>Dataset({
features: ['id', 'text'],
num_rows: 18 })</p>
</blockquote>
<p>I would like to persist the dataset to disk.</p>
<p>Is there a preferred way to do this? Or, is the only option to use a general purpose library like joblib or pickle?</p>
","huggingface"
"71994950","Detect inquiry sentence in Wav2Vec 2.0 result","2022-04-25 05:47:41","","0","295","<deep-learning><nlp><speech-to-text><huggingface-tokenizers><huggingface>","<p>I am studying ASR(Automatic Speech Recognition) using Wav2Vec2.0.
When I run Wav2Vec2.0, I get the result without a comma(&quot;.&quot;), question mark(&quot;?&quot;) etc. Therefore, the result came out as one whole sentence.
I know that I removed regex while making the tokenizer.
Is there any way to convert to the perfect sentence which contains regex?</p>
<p>Original Text from wav file = &quot;So what which one is better?&quot;</p>
<p>Wav2Vec 2.0 Result = &quot;SO WHAT WHICH ONE IS BETTER&quot; (Question mark missing)</p>
<p>Expected Result = &quot;SO WHAT WHICH ONE IS BETTER?&quot;</p>
","huggingface"
"71962496","Which huggingface model is the best for sentence as input and a word from that sentence as the output?","2022-04-22 01:27:20","71965576","0","634","<nlp><huggingface>","<p>What would be the best huggingface model to fine-tune for this type of task:</p>
<p>Example input 1:</p>
<pre><code>If there's one person you don't want to interrupt in the middle of a sentence it's a judge.
</code></pre>
<p>Example output 1:</p>
<pre><code>sentence
</code></pre>
<p>Example input 2:</p>
<pre><code>A good baker will rise to the occasion, it's the yeast he can do.
</code></pre>
<p>Example output 2:</p>
<pre><code>yeast
</code></pre>
","huggingface"
"71911630","How to run inference for T5 tensorrt model deployed on nvidia triton?","2022-04-18 12:08:58","","1","438","<inference><tensorrt><triton><huggingface>","<p>I have deployed T5 tensorrt model on nvidia triton server and below is the config.pbtxt file, but facing problem while inferencing the model using triton client.</p>
<p>As per the config.pbtxt file there should be 4 inputs to the tensorrt model along with the decoder ids. But how can we send decoder as input to the model I think decoder is to be generated from models output.</p>
<pre><code>name: &quot;tensorrt_model&quot;
platform: &quot;tensorrt_plan&quot;
max_batch_size: 0
input [
 {
    name: &quot;input_ids&quot;
    data_type: TYPE_INT32
    dims: [ -1, -1  ]
  },

{
    name: &quot;attention_mask&quot;
    data_type: TYPE_INT32
    dims: [-1, -1 ]
},

{
    name: &quot;decoder_input_ids&quot;
    data_type: TYPE_INT32
    dims: [ -1, -1]
},

{
   name: &quot;decoder_attention_mask&quot;
   data_type: TYPE_INT32
   dims: [ -1, -1 ]
}

]
output [
{
    name: &quot;last_hidden_state&quot;
    data_type: TYPE_FP32
    dims: [ -1, -1, 768 ]
  },

{
    name: &quot;input.151&quot;
    data_type: TYPE_FP32
    dims: [ -1, -1, -1 ]
  }

]

instance_group [
    {
        count: 1
        kind: KIND_GPU
    }
]
</code></pre>
","huggingface"
"71904074","ValueError: `Checkpoint` was expecting model to be a trackable object (an object derived from `Trackable`), got RobertaForSequenceClassification","2022-04-17 17:40:47","","0","574","<python><tensorflow><huggingface-transformers><roberta-language-model><huggingface>","<p>I am training a text classification model using RoBERTa. (<a href=""https://huggingface.co/siebert/sentiment-roberta-large-english"" rel=""nofollow noreferrer"">https://huggingface.co/siebert/sentiment-roberta-large-english</a>)
I use google colab for running the code.
I am facing a valueError i have trid googling about this error and i have not gotten a solution and i have also tried using other pretrained models in hugging face but I face the same error in all the models.</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForSequenceClassification
tokenizer = AutoTokenizer.from_pretrained(&quot;siebert/sentiment-roberta-large-english&quot;)
train_encodings = tokenizer(X_train, truncation=True, padding=True)
test_encodings = tokenizer(X_test, truncation=True, padding=True) 
training_args=TFTrainingArguments(
    output_dir='./results',
    num_train_epochs=2,
   per_device_train_batch_size=8, # batch size per device during training
   per_device_eval_batch_size=16, # batch size for evaluation
   warmup_steps=500,
   weight_decay=0.01,
   logging_dir='./logs',
   logging_steps=10,
)  
with training_args.strategy.scope():
    model = AutoModelForSequenceClassification.from_pretrained(&quot;siebert/sentiment-roberta-large-english&quot;)

    
trainer = TFTrainer(
    model= model,
   args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset
)
trainer.train() 

`
ValueError                                Traceback (most recent call last)
&lt;ipython-input-45-2a4521c5cd87&gt; in &lt;module&gt;()
----&gt; 1 trainer.train()

2 frames
/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/util.py in _assert_trackable(obj, name)
   1462       obj, (base.Trackable, def_function.Function)):
   1463     raise ValueError(
-&gt; 1464         f&quot;`Checkpoint` was expecting {name} to be a trackable object (an &quot;
   1465         f&quot;object derived from `Trackable`), got {obj}. If you believe this &quot;
   1466         &quot;object should be trackable (i.e. it is part of the &quot;

ValueError: `Checkpoint` was expecting model to be a trackable object (an object derived from `Trackable`), got RobertaForSequenceClassification(
  (roberta): RobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(50265, 1024, padding_idx=1)
      (position_embeddings): Embedding(514, 1024, padding_idx=1)
      (token_type_embeddings): Embedding(1, 1024)
      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (12): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (13): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (14): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (15): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (16): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (17): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (18): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (19): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (20): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (21): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (22): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (23): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (classifier): RobertaClassificationHead(
    (dense): Linear(in_features=1024, out_features=1024, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (out_proj): Linear(in_features=1024, out_features=2, bias=True)
  )
). If you believe this object should be trackable (i.e. it is part of the TensorFlow Python API and manages state), please open an issue.`
</code></pre>
","huggingface"
"71891127","Is there any way I can use the downloaded pre-trained models for TIMM?","2022-04-16 04:33:36","","3","3632","<pytorch><computer-vision><huggingface>","<p>For some reason, I have to use TIMM package offline. But I found that if I use <em><strong>create_model()</strong></em>, for example:</p>
<pre><code>self.img_encoder = timm.create_model(&quot;swin_base_patch4_window7_224&quot;, pretrained=True)
</code></pre>
<p>I would get</p>
<pre><code>http.client.RemoteDisconnected: Remote end closed connection without response
</code></pre>
<p>I found the function wanted to fetch the pre-trained model by the URL below, but it failed.
<a href=""https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_base_patch4_window7_224_22kto1k.pth"" rel=""nofollow noreferrer"">https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_base_patch4_window7_224_22kto1k.pth</a></p>
<p>Can I just download the pre-trained model and load it in my code like in Huggingface? (I have checked the timmdocs but found nothing mentioning this.)</p>
","huggingface"
"71882450","How to enable header in text files of load_dataset in huggingface?","2022-04-15 09:41:52","","0","1494","<python><nlp><header><huggingface-datasets><huggingface>","<p>I am trying to load a text file using huggingface (<a href=""https://huggingface.co/docs/datasets/v1.2.1/loading_datasets.html"" rel=""nofollow noreferrer"">https://huggingface.co/docs/datasets/v1.2.1/loading_datasets.html</a>)</p>
<pre><code>from datasets import load_dataset
dataset = load_dataset('text', data_files='my_file.txt')
</code></pre>
<p>This text file already contains headers, how do I indicate this to the module (say, <code>header = True</code>, in case of pandas <code>read_csv()</code>)?</p>
<p>Also, how do I mention that it is tab/comma separated?</p>
<p>Is there a way to present this data in tabular format?</p>
","huggingface"
"71857265","Encoder weights are not initialized when loading pre trained model","2022-04-13 12:08:58","","1","1619","<python><nlp><pytorch><huggingface-transformers><huggingface>","<p>I'm writing a custom class on top of <code>XLMRobertaModel</code>, but when initializing the model from a pre-trained checkpoint, I get a warning saying the <code>encoder.layer.*</code> weights were not initialized from the respective checkpoint.</p>
<p>Here is a minimal example to reproduce the error:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import XLMRobertaModel

class CustomXLM(XLMRobertaModel):
    def __init__(self, config):
        super().__init__(config)

        self.roberta = XLMRobertaModel(config)

        self.init_weights()

custom_xlm = CustomXLM.from_pretrained(&quot;xlm-roberta-base&quot;, num_labels=2)
</code></pre>
<p>Upon running this I get (check the 4th line):</p>
<pre><code>Some weights of the model checkpoint at xlm-roberta-base were not used when initializing CustomXLM: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing CustomXLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing CustomXLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of CustomXLM were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['encoder.layer.6.attention.self.value.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.2.output.dense.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.2.attention.self.value.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.2.attention.self.value.weight', 'pooler.dense.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.bias', 'pooler.dense.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.7.attention.self.key.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</code></pre>
<p>I understand some of the weights are supposed to be newly initialized. Specifically the ones related to <code>lm_head</code>. But I don't understand why the encoder weights are not loaded correctly.</p>
<p>I have a working example using BERT, primarily based from this <a href=""https://github.com/semantic-web-company/wic-tsv/blob/master/HyperBert/HyperBert3.py"" rel=""nofollow noreferrer"">code</a>. It inherits from <code>BertPreTrainedModel</code> and has <code>self.bert = BertModel(config)</code> in the <code>__init__</code> function. This class also overrides the <code>forward</code> function.</p>
<p>With that, I tried inheriting from <code>RobertaPreTrainedModel</code> and keeping the line <code>self.roberta = XLMRobertaModel(config)</code>. And although all warnings go away, I get a message saying:</p>
<pre><code>You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.
</code></pre>
<p>That said, my questions are:</p>
<ol>
<li>What is the correct way of creating a custom model?</li>
<li>Should I have <code>self.roberta = XLMRobertaModel(config)</code> in the <code>__init__</code> if I want to use a pretrained model? As far as I understood <a href=""https://huggingface.co/course/chapter2/3?fw=pt"" rel=""nofollow noreferrer"">the docs</a>, calling <code>XLMRobertaModel(config)</code> will return me a newly initialized model.</li>
</ol>
<p><strong>Edit:</strong>
Related to question 2. I saw that transformers does some magic depending on the name of the attribute you have your model on. <a href=""https://github.com/huggingface/transformers/issues/8407"" rel=""nofollow noreferrer"">This link</a> has some more details.</p>
","huggingface"
"71755090","For what is used parameter return_dict in BertModel?","2022-04-05 16:08:32","","1","2206","<nlp><bert-language-model><huggingface>","<p>I have something like this <code>model=BertModel.from_pretrained('bert-base-uncased',return_dict=True)</code>
What exactly is this &quot;return_dict&quot; used for? What happens when True and what when False?</p>
","huggingface"
"71744288","wandb getting logged without initiating","2022-04-04 22:08:30","71750632","1","2323","<huggingface-transformers><huggingface-tokenizers><fine-tuning><wandb><huggingface>","<p>I do not want to use wandb. I don't even have an account. I am simply following <a href=""https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/summarization.ipynb#scrollTo=UmvbnJ9JIrJd"" rel=""nofollow noreferrer"">this notebook</a> for finetuning. I am not running the 2nd and 3 cells because I do not want to push the model to the hub.</p>
<p>However, when I do trainer.train() I get the following error : <a href=""https://i.sstatic.net/VPIZm.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/VPIZm.png"" alt=""enter image description here"" /></a></p>
<p>I don't understand where wandb.log is being called.
I even tried os.environ[&quot;WANDB_DISABLED&quot;]  = &quot;true&quot; but I still get the error.
Please help.</p>
","huggingface"
"71717196","Is there a way to load local csv.gz file in huggingface dataloader?","2022-04-02 11:37:44","","0","439","<huggingface-datasets><huggingface>","<p>I'm using <code>datasets</code> library by huggingface to load csv dataset stored locally. The problem is, the dataset is compressed and is stored as a <code>csv.gz</code> file. Therefore, I'm not able to load it using <code>load_dataset('csv', '&lt;local_file_path&gt;.csv')</code> method in huggingface.</p>
","huggingface"
"71666061","HuggingFace FinBert Model in google Colab","2022-03-29 16:59:25","","0","467","<python><google-colaboratory><huggingface-transformers><huggingface>","<p>When I run my FinBert model it always crashes the RAM in Google Colab at outputs = model(**input)</p>
<pre><code>from transformers.utils.dummy_pt_objects import HubertModel
import textwrap
# Reads all files at once but you will have to upload it again
import pandas as pd
import glob
import numpy as np
import torch


all_files = glob.glob(&quot;*.csv&quot;)
tickerList = []
textList = []
model.eval()
for filename in all_files:
    # Get ticker symbol
    ticker = filename.split('_', 1)[0].replace('.', '').upper()
    #Read file into dataframe
    df = pd.read_csv(filename)
    headlines_array = np.array(df)
    # Data fram will not be a list of text for tokenizer to process
    text = list(headlines_array[:,0])
    textList.append(text)
    #Checks if we have seen this ticker before 
    if ticker not in tickerList:
      tickerList.append(ticker)

#Gets data to be an acceptable format for our model
    inputs = tokenizer(text, padding = True, truncation = True, return_tensors='pt')
    outputs = model(**inputs)  #time consuming and crashes RAM so can't up int for loop
    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)

    positive = predictions[:, 0].tolist()
    negative = predictions[:, 1].tolist()
    neutral = predictions[:, 2].tolist()

    table = {'Headline': text,
         'Ticker' : ticker,
         &quot;Positive&quot;:positive,
         &quot;Negative&quot;:negative, 
         &quot;Neutral&quot;:neutral}

    df = pd.DataFrame(table, columns = [&quot;Headline&quot;, &quot;Ticker&quot;, &quot;Positive&quot;, &quot;Negative&quot;, &quot;Neutral&quot;])
    final_table = wandb.Table(columns=[&quot;Sentence&quot;, &quot;Ticker&quot;, &quot;Positive&quot;, &quot;Negative&quot;, &quot;Neutral&quot;])

    for headline, pos, neg, neutr in zip(text, predictions[:, 0].tolist(), predictions[:, 1].tolist(), predictions[:, 2].tolist() ): 
      final_table.add_data(headline, ticker, pos, neg, neutr)
</code></pre>
<p>Not quite sure what is going wrong as outputs = model(**input) runs fine outside the for loop but does not seem to run even once when I bring it inside the for loop.</p>
","huggingface"
"71646831","Huggingface datasets ValueError","2022-03-28 11:36:44","","0","7742","<python><nlp><huggingface-datasets><huggingface>","<p>I am trying to load a dataset from huggingface organization, but I am getting the following error:</p>
<pre><code>ValueError: Couldn't cast string
-- schema metadata --
pandas: '{&quot;index_columns&quot;: [{&quot;kind&quot;: &quot;range&quot;, &quot;name&quot;: null, &quot;start&quot;: 0, &quot;' + 686
to
{'text': Value(dtype='string', id=None)}
because column names don't match
</code></pre>
<p>I used the following lines of code to load the dataset:</p>
<pre><code>from datasets import load_dataset
dataset = load_dataset(&quot;datasetFile&quot;, use_auth_token=True)
</code></pre>
<p>Pleases note the dataset version = (2.0.0), I changed it to 1.18.2 but it did not work.</p>
<p>Is there any way to fix this error?</p>
","huggingface"
"70606666","Solving ""CUDA out of memory"" when fine-tuning GPT-2 (HuggingFace)","2022-01-06 11:49:14","70607817","3","3800","<python><pytorch><nlp><huggingface-transformers><huggingface>","<p>I get the reoccuring CUDA out of memory error when using the HuggingFace Transformers library to fine-tune a GPT-2 model and can't seem to solve it, despite my 6 GB GPU capacity, which I thought should be enough for fine-tuning on texts. The error reads as follows:</p>
<pre><code>File &quot;GPT\lib\site-packages\torch\nn\modules\module.py&quot;, line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;GPT\lib\site-packages\transformers\modeling_utils.py&quot;, line 1763, in forward
    x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)
RuntimeError: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 6.00 GiB total capacity; 4.28 GiB already allocated; 24.50 MiB free; 4.33 GiB reserved in total by PyTorch) If reserved memory is &gt;&gt; allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
</code></pre>
<p>I already set batch size to as low as 2 and reduced training examples without success. I also tried to migrate the code to Colab, where the 12GB RAM were quickly consumed.
My examples are rather long, some counting 2.400 characters, but they should be truncated by the model automatically. My (German) examples look like this:</p>
<pre><code> Er geht in fremde Wohnungen, balgt sich mit Freund und Feind, ist
zudringlich zu unsern Sämereien und Kirschen.  Wenn die Gesellschaft nicht groß
ist, lasse ich sie gelten und streue ihnen sogar Getreide.  Sollten sie hier
aber doch zu viel werden, so hilft die Windbüchse, und sie werden in den
Meierhof hinabgescheucht.  Als einen bösen Feind zeigte sich der Rotschwanz.  Er
flog zu dem Bienenhause und schnappte die Tierchen weg.  Da half nichts, als ihn
ohne Gnade mit der Windbüchse zu töten.

 Ich wollte
Ihnen mein Wort halten, liebe Mama, aber die Versuchung war zu groß.  Da bin ich
eines Abends in den Keller gegangen und hab' aus allen Fässern den Spund
herausgeklopft.  Bis auf den letzten Tropfen ist das Gift ausgeronnen aus den
Fässern.  Der Schade war groß, aber der Teufel war aus dem Haus. «

Andor lachte.  »Mama, das Geschrei hätten Sie hören sollen! Als ob der
Weltuntergang gekommen wäre. Er bedauerte beinahe seine
Schroffheit.  Nun, nachlaufen wird er ihnen nicht, die werden schon selber
kommen.  Aber bewachen wird er seine Kolonie bei Tag und bei Nacht lassen
müssen.  Hol' der Teufel diesen Mercy.  Muß der gerade in Högyész ein Kastell
haben.  Wenn einer von den Schwarzwäldern dahin kommt und ihn verklagt.
</code></pre>
<p>Is there a problem with the data formatting maybe?
If anyone has a hint on how to solve this, it would be very welcome.</p>
<p>EDIT: Thank you <a href=""https://stackoverflow.com/users/6117017/timbus-calin"">Timbus Calin</a> for the answer, I described in the comment how adding the <strong><code>block_size</code></strong> flag to the config.json solved the problem. Here is the whole configuration for reference:</p>
<pre><code>{
    &quot;model_name_or_path&quot;: &quot;dbmdz/german-gpt2&quot;,
    &quot;train_file&quot;: &quot;Fine-Tuning Dataset/train.txt&quot;,
    &quot;validation_file&quot;: &quot;Fine-Tuning Dataset/test.txt&quot;,
    &quot;output_dir&quot;: &quot;Models&quot;,
    &quot;overwrite_output_dir&quot;: true,
    &quot;per_device_eval_batch_size&quot;: 8,
    &quot;per_device_train_batch_size&quot;: 8,
    &quot;block_size&quot;: 100, 
    &quot;task_type&quot;: &quot;text-generation&quot;,
    &quot;do_train&quot;: true,
    &quot;do_eval&quot;: true
}

</code></pre>
","huggingface"
"69087044","Early stopping in Bert Trainer instances","2021-09-07 11:02:19","69087153","29","21094","<python><deep-learning><neural-network><huggingface-transformers><huggingface>","<p>I am fine-tuning a BERT model for a multiclass classification task. My problem is that I don't know how to add &quot;early stopping&quot; to those Trainer instances. Any ideas?</p>
","huggingface"
"61825698","How to specify number of target classes for TFRobertaSequenceClassification?","2020-05-15 18:07:16","61828910","4","1304","<python><machine-learning><deep-learning><huggingface-transformers><huggingface>","<p>I have a text classification task at hand and I want to use RoBERTa pre-trained model from the <code>transformers</code> python library.</p>
<p>As per the documentation of <a href=""https://huggingface.co/transformers/model_doc/roberta.html#transformers.TFRobertaForSequenceClassification"" rel=""nofollow noreferrer"">TFRobertaForSequenceClassification</a> to train we have to use,</p>
<pre><code>from transformers import RobertaTokenizer, TFRobertaForSequenceClassification

tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
model = TFRobertaForSequenceClassification.from_pretrained('roberta-base')

model.compile('adam', loss='sparse_categorical_crossentropy')
model.fit(x, y)
</code></pre>
<p>So where should I specify the number of target labels for sequence classification?</p>
","huggingface"