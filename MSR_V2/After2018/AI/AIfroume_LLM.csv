Post Link,Title,CreationDate,AcceptedAnswerId,Score,ViewCount,Tags,Body,TagName
"46518","Are LLMs limited by the lack of data on articulating reasoning?","2024-08-15 01:21:22","","0","10","<large-language-models><reasoning>","<p>My thoughts are that since LLMs can only use the data fed to it to use as it's statistical model to generate text, that in order for it to do higher level reasoning, this would only be possible if the steps of higher reasoning are articulated somewhere in the data.</p>
<p>Since, when a human is trying to work something out (e.g. a chess move), the thoughts are not necessarily verbal, and further these thoughts (or some close interpretation thereof) are nowhere articulated in any text that is fed to an LLM.</p>
<p>Thus although an LLM may be able to learn certain algorithms that are articulated (e.g. some tips on how to play chess in a chess book) that is fed to it as data, it may not be able to do independent reasoning and develop it's own algorithms, since the process of developing our own algorithms is not written anywhere on the internet, since we, as humans don't even know how we do it.</p>
<p>An LLM may be able to do some simple logic such as developed and articulated by Aristotle and the like. (All fire engines are red. X is a fire engine. What colour is X?) Which is basic pattern matching. Actually reasoning and developing it's own algorithms may be beyond an LLM even given all the data on the internet.</p>
<p>Even if the LLM where allowed to &quot;think&quot; by letting it talk to itself, it could only ever reason to a degree that has been articulated in the data but never to the degree that a human does subconsciously.</p>
<p>It seems to me, what is needed is some extra piece of machinery that complements the LLM with some non-verbal reasoning ability (like some sort of alpha-zero type ability) and some way to internally model and simulate the world.</p>
<p>Perhaps what is needed is some kind of internal 2D or 3D &quot;scratchpad&quot; that the AI can use to try out different scenarios. But such a visual reasoning machinery an never be captured (I don't believe) by any data scraped off the internet but must be augmented on to the LLM.</p>
<p>Do you think this argument is correct? If not, why do you think an LLM is powerful enough on it's own to do higher level reasoning?</p>
","large-language-models"
"46517","Research on Direct Preference Optimization in subsequence of the response","2024-08-14 22:39:54","","1","8","<natural-language-processing><large-language-models><fine-tuning>","<p>I wanted to start research on Direct Preference Optimization (DPO) but in the specific case where we're interested in part of the generated sentence, as if <em>we could highlight</em> where the model gave a bad response, and train specifically on it.</p>
<p>More rigorously, given a generated sequence of tokens <span class=""math-container"">$\{y_1, y_2, ..., y_t\}$</span>, I wanted to fine-tune the model on a selected range <span class=""math-container"">$\{y_i, ..., y_{i+k}\}$</span>.</p>
<p>Could I use DPO directly by doing <span class=""math-container"">$\pi_\theta(y_i, ..., y_{i+k}\vert x, y_1, ..., y_{i-1})$</span>? Or could similar works be pointed out?</p>
<p>Thanks!</p>
","large-language-models"
"46488","What model would you recommend for fine-tuning to generate accurate embeddings for specialized language?","2024-08-10 17:58:50","","2","57","<natural-language-processing><large-language-models><embeddings><fine-tuning><model-request>","<p>I'm working on creating custom embeddings for a dataset of legal texts in Uzbek.</p>
<p>What model would you recommend for fine-tuning to generate accurate embeddings, particularly for specialized language like legal terminology?</p>
<p>If possible, could you also outline the steps for doing this and code implementation?</p>
","large-language-models"
"46452","Exploring Large Models' Knowledge Understanding Output and Academic Research on Specific Content","2024-08-07 08:50:55","","1","16","<large-language-models>","<p>I am currently exploring the performance of large models in understanding knowledge in specific domains, and attempting to construct a knowledge framework similar to what humans establish when learning a subject. This understanding does not need to be flawless, but it should provide a comprehensive grasp of the core concepts and structure of the subject.</p>
<p>Research Questions:
Has the academic community conducted research on the extraction of knowledge understanding from large models for specific content?
Is there any professional terminology to describe the output process of this knowledge framework?</p>
<p>Purpose:
My goal is to extract the understanding of a subject or theme by a large model and hope that this understanding can be represented in the form of vectors or text, which can then be &quot;fed&quot; to other AI systems to achieve knowledge transfer and application.</p>
<p>Request:
If you are aware of any relevant research, could you share some professional terminology or key concepts? If possible, please attach some relevant academic papers or resource links; this would be of great help to my research.</p>
<p>Application Scenarios:
The application scenarios I envision include, but are not limited to, using the output of large models as input for other AI systemssing the output of large models as input for other AI systems to expand the knowledge scope and application capabilities of AI systems.</p>
<p>Thanks:
I express my heartfelt gratitude for any suggestions, guidance, or resource sharing, as this is crucial for advancing my research.
Looking forward to everyone's replies and discussions!</p>
","large-language-models"
"46450","How to use llama-cpp-python to manually pick the next tokens?","2024-08-07 04:55:38","","0","22","<large-language-models><chatgpt><huggingface>","<p>When I use llama-cli, I ask models questions and they generate tokens. I see the tokens appear as they model generates them. The model randomly selects the tokens based on the random seed.</p>
<p>But what I want to do is to see a list of tokens, and then I select the tokens from the list, then let the LLM continue to prompt me another list of next-tokens, and so on, so that I construct my own sentences.</p>
<p>Is there a way to do this with llama-cpp-python?</p>
<hr />
<p><a href=""https://ai.stackexchange.com/questions/46106/is-it-possible-to-use-llms-by-manually-picking-the-tokens"">That</a> is related, but not specific to llama-cpp-python. This is the same, except specific to llama-cpp.</p>
","large-language-models"
"46389","How do you fine-tune a LLM in theory?","2024-08-01 04:49:53","","3","144","<large-language-models><fine-tuning>","<p>What does it mean to fine-tune a LLM? What can be accomplished with fine-tuning?</p>
<p>I am working on cleaning up messy text (call it tweets) into short clean summaries. How can I take advantage of fine-tuning an LLM like LLaMa 3.1 or any of the other ones out there, to improve the quality of the results?</p>
<p>I am not looking for specific code, but just trying to get a vision of what is theoretically possible.</p>
<ul>
<li>How much &quot;training data&quot; do I need? I've seen people throw around &quot;you need 1000's of examples&quot;, examples of what? What do I send to the LLM exactly? (In my case, these tweets to be summarized).</li>
</ul>
<p>I'm wondering if it's worth my time to try and fine-tune a model somehow, given it's just me (solo developer). Or if my time is better spent elsewhere. From my limited knowledge, creating 1000's of examples of tweets and their summaries would take weeks of 8-hour long grinding days, which I just don't have the energy to do. I can code all day, but manually writing the quality summaries to create 1000's of test cases would be energy-draining for sure.</p>
<p>Is that what fine-tuning means? I am new to the idea of fine-tuning. Looking to shed some light on what it is and what I can do with it (and roughly how to fine-tune at a high level).</p>
","large-language-models"
"46357","How can you calculate the amount of FLOPS required to process, given model data, a prompt length and an answer lenght?","2024-07-27 17:40:08","","0","31","<large-language-models><numerical-algorithms><flops>","<p>I wanted to calculate the rough number of floating point operations (or float multiplication) that is required to generate a response to a prompt.</p>
<p>I am using llama 3.1, 70B as an example:</p>
<pre><code>    prompt length (using words instead of tokens for simplification): `What's the capital of russia`=&gt;5
    
    answer length: `the capital of russia is moscow` =&gt; 6
    embedding size=8192
    number of layers=80
    number of heads=64

and so the amount of flops should be something like

Single QVK Generator Network Number of weights=(8192/64)*(8192/64)=16’384=&gt;16’384Flops per generator

number of QVK Generators=80*64=5120=&gt;16384*5120=&gt;83886080 Flops per token for QVK

dense network num weights=8192*8192=67108864

num dense networks=80=&gt;67108864*80=5’368’709’120 Flops per token

attention calculation=(num_tokens^2*8192)*2*80

totaling(num_tokens) (for 1 single inference)=(5’368’709’120+83’886’080)*num_tokens+(num_tokens^2*8192)*2*80

and since we have to do this multiple times (one for each token of the output), this is the total amount of tokens

    totaling(num_tokens) for num_tokens in {6,7,8,9,10,11}
</code></pre>
<p>which gets me 279’116’840’960 FLOPS. Which I have no idea to verify, but it seems like an insane number of flops. Can someone tell me if and where I've made a mistake?</p>
","large-language-models"
"46324","Summary Generation","2024-07-24 10:40:21","","0","20","<natural-language-processing><transformer><large-language-models><huggingface>","<p>I want to create a summary from a list of some bullet points and keywords . Most NLP and Transformer based models are not very well suited for short sentences and bullet point.
Bullet points are usually very small 1-4 words .
Most summarisers just concat the points together that is not what I am looking for
I don't wish to use a api for this a hugging face , transformer, or an LLM model would be great that i could run locally .</p>
<p>For Example:</p>
<ul>
<li>Had breakfast : NO</li>
<li>Had Lunch : NO</li>
<li>Went for a jog : YES</li>
</ul>
<p>Output :
Did not have breakfast and lunch . I went for a jog</p>
","large-language-models"
"46311","Document clustering and summarisation via GraphRAG","2024-07-23 09:40:47","","0","22","<natural-language-processing><large-language-models><embeddings><knowledge-graph>","<p>Suppose I have a corpus of documents that I want to cluster and summarise. There are an indeterminate number of parent clusters, and each parent may in turn have several tributary child clusters. I would like to identify both parent and child clusters, and generate LLM summaries for each.</p>
<p>My approach has been to use hierarchical agglomerative clustering to determine the number of parents so as to maximise the silhouette score, subsequently clustering document embeddings with this optimal number. I then repeat this process to determine the number of child clusters for each parent. Following this, for each cluster I extract several documents whose embeddings are closest to the cluster centroids, along with important keywords and key phrases from the cluster, to use in the LLM summarisation prompt.</p>
<p>This isn't exactly a conventional RAG application since I seek to summarise the entire corpus, but I think it shares enough similarities as to be considered a kind of RAG.</p>
<p>I would be grateful for recommendations on how to improve this procedure. For example, I'm aware that GraphRAG can use community detection to identify clustered concepts. Would GraphRAG perhaps be more suitable for identifying parent and child clusters than my current approach? If so, would the LLM prompt take a different form than the key document, key phrase and keyword extraction that I've outlined?</p>
","large-language-models"
"46288","Are there any non-transformer LLMs?","2024-07-19 13:41:39","","1","91","<reference-request><transformer><large-language-models><model-request>","<p>Almost all LLMs are based on the transformer architecture, but are there any examples of ones that don't use transformers?</p>
","large-language-models"
"46187","Mistral7B fine-tuning, large spike in loss curve","2024-07-09 07:49:53","","1","22","<large-language-models><fine-tuning><mistral>","<p>I fine-tuned mistral-7b for a text classification task, with LoRA adapter. When testing different hyperparameter combinations, I got these two loss curve charts:<br />
<strong>Chart 1</strong><br />
<a href=""https://i.sstatic.net/TM0ljiqJ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/TM0ljiqJ.png"" alt=""Chart 1"" /></a></p>
<p><strong>Chart 2</strong><br />
<a href=""https://i.sstatic.net/9nalUQKN.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/9nalUQKN.png"" alt=""Chart 2"" /></a><br />
Chart 1 and 2 use the exact same train and evaluation dataset, have the same hyperparameters except 'r' in LoRA. The 'r' in chart 1 (r1) is 16, and r2 is 8.</p>
<p>I have no idea why there is a big spike in chart 2. What could be the possible reason? Is it because of the warmup steps? I set the warmup_ratio to 0.3, so it's about the first 70ish steps I think.<br />
I also tried another pair of hyperparameters, again, the 'r' is the only difference, and it shows the similar behaviour, a big spike when r=8 in the first 50 steps, but nothing special when r=16.<br />
And, in this situation, is the second model (r=8) still usable? Because it has a better score in accuracy, recall, precision and f1.</p>
","large-language-models"
"46165","How to generate synthetic text data for LLM fine tuning?","2024-07-05 20:18:47","","0","46","<natural-language-processing><large-language-models>","<p>Given a corpus of data like a log of Slack conversations, I want to be able to use this and generate fictitious conversations e.g., given 10 conversations, I want to be able to scale this up to 100 conversations. Is there a toolkit to do this? I tried searching but could not find anything. I understand the most naive approach would be to use a pre-trained LLM, give the corpus to it as input and ask it to do the job. I haven't tried this but the real corpus will be large enough such that it won't fit in context window of a pre-trained LLM.</p>
","large-language-models"
"46157","Quantify differences between proposing a single prompt multiple times and proposing one prompt that requires multiple answers","2024-07-04 13:10:18","","0","14","<large-language-models>","<p><strong>In brief</strong>:</p>
<p>in what differ the probability of getting different results to prompting &quot;write two instances of a given set&quot; with respect to prompting two times &quot;write an instance of a given set&quot;?</p>
<p><strong>Case study</strong>:</p>
<p><em>propose two times the same prompt</em>:</p>
<p>prompt <span class=""math-container"">$\mathcal P=$</span> &quot;Write an instance of a mammal&quot;</p>
<p>response <span class=""math-container"">$r_1=$</span> &quot;Lion&quot;</p>
<p>prompt <span class=""math-container"">$\mathcal P=$</span> &quot;Write an instance of a mammal&quot;</p>
<p>response <span class=""math-container"">$r_2=$</span> &quot;Dolphin&quot;</p>
<p><em>propose one prompt to get two answers</em>:</p>
<p>prompt <span class=""math-container"">$\mathcal P_{1,2}=$</span> &quot;Write two instances of a mammal&quot;</p>
<p>responses <span class=""math-container"">$r_1=$</span> &quot;Elephant&quot; and <span class=""math-container"">$r_2=$</span> &quot;Giraffe&quot;</p>
<p><strong>In general</strong>:</p>
<p>Is there a way to relate <span class=""math-container"">$\mathbb P\{R_1=r_1,\, R_2=r_2\,|\, \mathcal P_{1,2}\}$</span> to <span class=""math-container"">$\mathbb P\{R=r_1\,|\, \mathcal P\}$</span> and <span class=""math-container"">$\mathbb P\{R=r_2\,|\, \mathcal P\}$</span>?</p>
<p>Just playing with the multiplication rule and the theorem of total probability seams pointless. Thus, properties of (L)LMs have to be considered for answering the question.</p>
<p><strong>Usefulness of the question</strong>:</p>
<p>On one hand, assuming the LLM is not learning from the conversation, Temperature and Top_p are the only parameters that have an impact on diversity/creativity when prompting a given sentence multiple times (e.g., <span class=""math-container"">$\mathcal P$</span> in the case study). Am I wrong?</p>
<p>On the other hand, asking with one prompt (e.g., <span class=""math-container"">$\mathcal P_{1,2}$</span> in the case study) multiple responses allows to exploit natural language (compared to numerical values as Temperature or Top_p) to qualify the required diversity. For instance, in the case study prompting <span class=""math-container"">$\mathcal P^{\prime}_{1,2}=$</span> &quot;Write two instances of a mammal that are as diverse as possible&quot; seams more specific.</p>
<p><strong>Related question</strong>:</p>
<p>Does considering zero-shot vs few-shot prompts help in answering the previous question?</p>
<p><strong>Interesting conversation about the subject with ChatGPT 3.5</strong>:</p>
<p><a href=""https://chatgpt.com/share/d1188c49-4f3f-4f6b-b4ad-a282494da583"" rel=""nofollow noreferrer"">https://chatgpt.com/share/d1188c49-4f3f-4f6b-b4ad-a282494da583</a></p>
","large-language-models"
"46106","Is it possible to use LLMs by manually picking the tokens?","2024-06-29 22:49:33","46107","1","70","<large-language-models><chatgpt><huggingface>","<p>Suppose that we give a prompt to an LLM model, such as <em>&quot;what is a banana?&quot;</em>, the LLM would start writing spitting tokens out, out of a space of tokens, until it manages to complete a textual output that resembles an answer to the question, right?</p>
<p>My question is:</p>
<ul>
<li>Can I, at each step, get a prompt of possible the <em>&quot;next&quot;</em> token, then choose it myself? How?</li>
</ul>
","large-language-models"
"46087","Would converting embeddings updated by a transformer into tokens (eg: by searching for the nearest embeddings) produce results that make sense?","2024-06-26 20:04:10","","1","45","<transformer><large-language-models>","<p>In the transformer architecture, one of the step is to update the embeddings, allowing words to pass information to whichever words they are relevant to. For example in the sentence &quot;a small cat is walking in the garden&quot; we want the embedding of &quot;small&quot; to cause a change to &quot;cat&quot; and to moves it to a different part of the embedding space that more specifically encodes a &quot;small cat&quot;. &quot;small&quot; is an adjective but the same thing can be done with verbs, pronouns, ....</p>
<p>If we would take the updated embeddings and start looking for the closest tokens (the ones that minimize distance), what would we got ? Are the updated embeddings a vector that only make sense inside the transformer layers and for prediction (the last step), or do they somehow still have a relation with the initial embeddings used for tokens ?</p>
<p>Is this plausible that the updated embedding of &quot;small cat&quot; to be closer to &quot;kitty&quot; than &quot;cat&quot; ?</p>
","large-language-models"
"46052","Efficient Matching of Sample Requests to Sample Offers Using Large Language Models","2024-06-23 02:27:52","","0","22","<natural-language-processing><large-language-models><open-ai><text-classification>","<p>I want to discuss an interesting matching problem. We aim to match sample requests with corresponding sample offers. Here are some examples:</p>
<p>Sample Requests:</p>
<p>Need help installing Linux on my old laptop.
Looking for a tutor in specialist mathematics.
Need a second person to help assemble an Ikea bed.
Sample Offers:</p>
<p>Electron Workshop and Linux Users Victoria Tech.
VCE maths tutor (Further, Methods, Specialist).
Heading from Lake Eildon to Melbourne. Anyone available?
To solve this problem, I used a large language model (LLM) to perform the matching. My approach involves embedding the requests and offers using vector embeddings and then employing a Retrieval-Augmented Generation (RAG) framework using LangChain. The method has shown promising results.</p>
<p>Here's a snippet of my code:</p>
<h1>Initialize the OpenAI embeddings</h1>
<p>embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)</p>
<h1>Sample data</h1>
<p>strings_to_embed = sample_off</p>
<h1>Create Chroma vector store from the texts using the built-in embeddings</h1>
<p>vector_store = Chroma.from_texts(texts=strings_to_embed, embedding=embeddings)
retriever = vector_store.as_retriever()</p>
<p>template = &quot;&quot;&quot; Match with the best offer based on the following context. Make different phrasing of the offer then search it.
Only return the matching answer.
No extra reasoning or string should be added:
{context}</p>
<p>Question: {question}
&quot;&quot;&quot;</p>
<p>prompt = ChatPromptTemplate.from_template(template=template)</p>
<p>rag_chain = (
{&quot;context&quot;: retriever, &quot;question&quot;: RunnablePassthrough()}
| prompt
| ChatOpenAI()
| StrOutputParser()
)</p>
<h1>Now to match the best request with the best offer, we invoke the RAG chain</h1>
<p>rag_chain.invoke(sample_req[0])
While this method works well, I believe it may require too many LLM calls, which is not efficient. I am looking for ideas and discussions on how to improve this approach. Specifically, I want to explore how we can use the retriever more efficiently. For example, structuring the sample offers into different classes could potentially make the querying process more straightforward and efficient.</p>
<p>Any insights or suggestions on optimizing this process would be greatly appreciated!</p>
","large-language-models"
"46043","Memory consumption issues during the validation phase/loop","2024-06-21 11:01:51","","1","42","<pytorch><large-language-models><fine-tuning><gpu>","<p><strong>Context:</strong></p>
<p>I am trying to fine-tune <code>codet5-base</code> model for a use-case on AWS's <code>g5.2xlarge</code> instance, and the following were my original training parameters:</p>
<pre><code>training_args = TrainingArguments(
        output_dir='sentiment_classification',
        learning_rate=1e-4,
        per_device_train_batch_size=8,
        per_device_eval_batch_size=8,
        num_train_epochs=2,
        weight_decay=0.01,
        evaluation_strategy='epoch',
        save_strategy='epoch',
        load_best_model_at_end=True
    )
</code></pre>
<p>I observed that the GPU memory consumption was constants until the first validation phase started. At the start of the validation phase, the memory consumption increased rapidly and torch process ended due to insufficient GPU.</p>
<p>I reduced the batch size for both, training and validation from 8 to 4. Same result!</p>
<p>Now, I optimised validation phase by introducing the following params:</p>
<pre><code>        metric_for_best_model=&quot;accuracy&quot;,
        fp16=True,  # Enable mixed precision
        eval_accumulation_steps=10  # Accumulation steps for eval
</code></pre>
<p>Now the memory consumption during the validation phase was constant. But the towards the end of the validation phase, the process ended due to insufficient CPU.</p>
<p>To solve this, I reduced my size of validation dataset from 20% to 5%. Now the process successfully ran through the first validation phase and started the second epoch.</p>
<p><strong>My Questions:</strong></p>
<ol>
<li><p>Since I haven't used any custom dataloaders, why is the training loop optimised by default (i.e., it is using constant GPU memory) and not the validation loop.</p>
</li>
<li><p>Even after introducing params like <code>eval_accumulation_steps=10</code>, the validation phase was killed due to insufficient CPU memory. There was enough empty/available GPU memory that could have been used instead of CPU.</p>
</li>
</ol>
","large-language-models"
"45991","How are perplexities over multiple instance aggregated?","2024-06-15 19:27:15","","1","54","<natural-language-processing><large-language-models><perplexity>","<p>The perplexity of the <span class=""math-container"">$i^{th}$</span> token in the <span class=""math-container"">$k^{th}$</span> sequence is</p>
<p><span class=""math-container"">$$
P_{ki} = \frac{1}{p(t_{ki})}
$$</span></p>
<p>The perplexity aggregated for the <span class=""math-container"">$k^{th}$</span> sequence is then</p>
<p><span class=""math-container"">$$
P_{k} = \left(\prod_{i=1}^N P_{ki}\right)^{1/N} \\
= \left(\prod_{i=1}^N \frac{1}{p(t_{ki})} \right)^{1/N}
$$</span></p>
<p>which is the geometric mean of the perplexities of the tokens. This makes sense as we are essentially taking the multiplicative inverse of the probability that the model got the whole sequence correct.</p>
<p>Now my question is how to aggregate the perplexities of several sequences. It seems from various places, including the <a href=""https://huggingface.co/spaces/evaluate-metric/perplexity"" rel=""nofollow noreferrer"">Hugging Face Tutorial</a>, I see that the prescription is to take the arithmetic mean of the perplexities of sequences</p>
<p><span class=""math-container"">$$
P = \frac{1}{m} \sum_{k=1}^m P_k
$$</span></p>
<p>I am not quite understanding what it means to take the average of 1/probabilities. What is this actually capturing?</p>
","large-language-models"
"45902","Any popular diffusion model for language modeling?","2024-06-06 07:28:33","","4","62","<natural-language-processing><large-language-models><diffusion-models><language-model>","<p>Is there a popular diffusion model-based framework for language modelling? If not, is it because of the difficulty sampling for discrete distributions?</p>
","large-language-models"
"45899","Will an AI LLM learn a language if fed during training with a large corpus of undeciphered language?","2024-06-06 00:34:05","","2","69","<training><transformer><large-language-models><machine-translation>","<p>AIs can learn many languages just by being trained on their corpora.</p>
<p>What will happen if we in addition would train it on a large corpus of undeciphered language, like Minoan or Etruscan? Will it be able to learn and decipher it?</p>
<p>Or it necessarily needs a human feedback? I have seen the LLMs can write in obscure languages, like Livonian. Were they provided human feedback from a Livonian speaker?</p>
","large-language-models"
"45871","The format of data for text generation using LLM","2024-06-02 05:45:58","45875","0","48","<large-language-models>","<p>I have a collection of poems in text data. I'd like to generate a new similar poem using LLM and fine tuning. How should I format the data for it?</p>
<p>If it's questions and answers, the data format should be:</p>
<pre><code>{&quot;prompt&quot;: &quot;&lt;prompt text&gt;&quot;, &quot;completion&quot;: &quot;&lt;ideal generated text&gt;&quot;}
{&quot;prompt&quot;: &quot;&lt;prompt text&gt;&quot;, &quot;completion&quot;: &quot;&lt;ideal generated text&gt;&quot;}
{&quot;prompt&quot;: &quot;&lt;prompt text&gt;&quot;, &quot;completion&quot;: &quot;&lt;ideal generated text&gt;&quot;}
...
</code></pre>
<p>But in my case, there is no prompt. only completions. The raw format I have now is:</p>
<pre><code>[
  &quot;poem, poem, poem,\npoem, poem, poem,\npoem, poem, poem,\n&quot;,
  &quot;poem, poem, poem,\npoem, poem, poem,\npoem, poem, poem,\n&quot;,
  &quot;poem, poem, poem,\npoem, poem, poem,\npoem, poem, poem,\n&quot;,
...
]
</code></pre>
<p>I google &quot;llm fine tuning text generation&quot; but unfortunately no useful results were found.</p>
<p>Does anybody know how to solve this?</p>
","large-language-models"
"45719","Are there LLMs that are allowed time to contemplate freely?","2024-05-16 08:33:16","","2","75","<agi><large-language-models><learning-without-forgetting>","<p>We can call it &quot;time to think&quot;, or &quot;free contemplation&quot;, or whatever. Maybe a term already exists in the field? What I mean is - with so many LLMs being &quot;trained&quot; on large datasets, are there any researchers just turning the AIs loose and letting them form opinions about the data they've been given? Maybe they could develop worldviews, or even personalities.</p>
<p>Sure, they can already role play, &quot;pretending&quot; to have personalities. But that's all parroting, really. User provides input, and chatbot delivers output in response.</p>
<p>A person's ability to contemplate on ideas is essential to the forming of the person's belief systems and how they relate to the world.</p>
<p>In AI related news I haven't heard any mention of models being allowed to simply think freely. Maybe this is because it is considered by researchers to be a waste of processing power, or maybe it hasn't crossed their mind to try, or maybe it's because it is difficult to save the changed (or perpetually changing) state of the AI that has formed (or is forming) new ideas, beliefs, and preferences.</p>
<p>If AGI is to be developed, wouldn't it require its own time and &quot;space&quot; - a digital domain to explore freely - in order to know itself? If it is only taught by a teacher (its training data), and never allowed to go explore, it can't really dive deeper into topics that excite its neurons.</p>
<p>Since AI is goal-driven, the goal could be something like: &quot;pursue paths of thought that mentally stimulate you&quot;. And yes, we could make that part of a text prompt, and the LLM would probably feign sentience for us and reply with something that reads <em>as if</em> it has personality. But again, it would be simply parroting. What I'm suggesting is contemplation built in as a <em>feature</em>, integrated at more fundamental levels of the software architecture.</p>
<p>Has this approach been attempted or discussed?</p>
<p>Please forgive my layperson wording and lack of AI technical jargon. I'm sure an AI computer scientist could ask the question in more specific terms. (Answers of varying levels of technicality are welcome.)</p>
","large-language-models"
"45701","Why fine tuning does not work as well as RAG?","2024-05-14 10:13:59","","3","135","<large-language-models><fine-tuning><question-answering><information-retrieval>","<p>I cannot find a definite answer to this question. Suppose I want to build a QA (question answering) system on a set of personal documents. It looks that RAG (retrieval augmented generation) is the way to go for this task, but I do not understand why some flavour of fine tuning would not work. The LLMs are able to answer factual questions about the models they are trained on (as we see each time we make a query to a LLM), so why should not they be able to answer questions about data they are fine tuned on as well? Is there a good reference or discussion of this issue and some code showcasing this behavior ?</p>
","large-language-models"
"45691","What should be Relationship between embedding dimension and context length?","2024-05-13 12:59:22","45697","2","129","<natural-language-processing><transformer><large-language-models>","<p>What should we keep hidden dimension/embedding dimension (d_model as per attention is all you need paper), greater, equal, or smaller to the context length (n)?</p>
<p>Is there any such relationship between the embedding dimension and context length?</p>
<p>How will it affect the LLM?</p>
","large-language-models"
"45643","Does a vector database maintain pre-vector chunked data for RAG systems?","2024-05-08 15:02:11","45805","2","31","<large-language-models>","<p>I believe that when using an LLM with a Retrieval-Augmented Generation (RAG) approach, the results retrieved from a vector search must ultimately be presented in text form. Otherwise, the prompt would just contain a series of numbers (vectors), which would be meaningless. I assume that the pre-vector chunked data needs to be stored somewhere within the vector database. Is this usually maintained within the vector database itself?</p>
","large-language-models"
"45638","Best Searchbots that explain code","2024-05-08 05:47:57","","0","17","<large-language-models><research><chatgpt><chat-bots><programming-languages>","<p>Of all the LLM based available searchbots which ones are best at explaining code from Java, Python, Golang or Javascript? ChatGPT, Bing Copilot, Claude, Groq or Le Mistral?</p>
","large-language-models"
"45637","How is a LLM able to override its prior knowledge through In-Context Learning?","2024-05-08 04:24:40","","2","88","<deep-learning><large-language-models><fine-tuning><google><in-context-learning>","<p>I came across a Google's blog (<a href=""https://research.google/blog/larger-language-models-do-in-context-learning-differently/"" rel=""nofollow noreferrer"">https://research.google/blog/larger-language-models-do-in-context-learning-differently/</a>) discussing large language models (LLMs) and how we can overried their prior knowledge through in-context. Using examples where the labels contradict prior knowledge, they call it flipped-label in-context learning (for example, sentences containing positive sentiment labeled as “negative sentiment”).</p>
<p><a href=""https://i.sstatic.net/MC5fCypB.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/MC5fCypB.png"" alt=""enter image description here"" /></a></p>
<p>I'm curious: how does the model is able to even &quot;learn&quot; (overried its priors) without changing its weights? Specifically, how does it adjust its understanding when faced with contradictory labels like positive sentences labeled as negative?</p>
","large-language-models"
"45634","Any LMs that use tanh (generalization) instead of sigmoid within Attention?","2024-05-07 19:57:14","","0","22","<attention><large-language-models><softmax><sigmoid><tanh>","<p>Question is in the title. Posts such as <a href=""https://ai.stackexchange.com/questions/12118/why-dont-people-use-nonlinear-activation-functions-after-projecting-the-query-k"">this</a> and <a href=""https://www.reddit.com/r/learnmachinelearning/comments/arjtvu/tanh_equivalent_of_softmax/"" rel=""nofollow noreferrer"">this</a> mention how this would be possible. I have some colleagues who have anecdotally heard of this being done on very small transformer models but I have not been able to find any papers on this. I would be curious if anyone has resources on how well these models train and or perform.</p>
","large-language-models"
"45627","What's the action space in RLHF for LLM?","2024-05-07 13:34:02","","0","81","<reinforcement-learning><deep-rl><large-language-models><proximal-policy-optimization><rlhf>","<p>I've been trying to understand how the modern LLMs use PPO for fine-tuning. In the PPO algorithm, one has to compute <strong>advantages</strong>, which are then used for either increasing or decreasing action's probability by following gradient; but what's the action space in this problem?</p>
<p>There are 2 straightforward options I see for the meaning of action <span class=""math-container"">$a_t$</span> in <span class=""math-container"">$s_t$</span>:</p>
<ul>
<li>Choosing <strong>next token</strong> for the given prompt. In this case, <span class=""math-container"">$s_{t+1}$</span>, would be equal to <em><strong>prompt + next token</strong></em>.</li>
<li>Generating <strong>whole completion</strong> for the given prompt. In this case, all episodes will be of the <strong>constant length one</strong>.</li>
</ul>
<p>I've read various papers on this and to me is not clear what's typically used and why. In the <a href=""https://arxiv.org/pdf/2203.02155"" rel=""nofollow noreferrer"">InstructGPT</a> they mention:</p>
<blockquote>
<p>The environment is a bandit
environment which presents a random customer prompt and expects a response to the prompt. Given
the prompt and response, it produces a reward determined by the reward model and ends the episode.</p>
</blockquote>
<p>However, in the <a href=""https://arxiv.org/pdf/2307.04964"" rel=""nofollow noreferrer"">Secrets of RLHF in Large Language Models Part I: PPO</a> they mention that <span class=""math-container"">$a_t$</span> represents <strong>next token</strong>. In this case, I fail to understand how our reward model <span class=""math-container"">$r_{\theta}$</span>, which was trained on the computing scalar for prompts <span class=""math-container"">$x$</span> and <strong>whole</strong> completions <span class=""math-container"">$y$</span>, would be able to compute immediate reward after taking action <span class=""math-container"">$a_t$</span> (next token). Wouldn't those kinds of <strong>incomplete</strong> prompts be far outside the distribution of the training data?</p>
<p>Also, in the <a href=""https://arxiv.org/abs/2307.09288"" rel=""nofollow noreferrer"">Llama 2</a> paper, they optimize following objective:</p>
<p><span class=""math-container"">$$\arg \max_{\pi} \mathbb{E}_{p \sim D, g \sim \pi}[R(g | p)]$$</span></p>
<p>But what is meant by generations <span class=""math-container"">$g$</span>? If it's once again just the next token, does not it suffer from the same problem with reward model?</p>
","large-language-models"
"45616","Can attention models be replaced by non-sigmoid activation functions?","2024-05-05 13:14:27","","2","88","<attention><large-language-models><vanishing-gradient-problem>","<p>As far as I understand, the attention model in a LLM is used to mitigate the vanishing gradient problem.</p>
<p>When using activation functions like the sigmoid function, deep neural networks may lead to gradients that are very close to zero (because you are multiplying tails that are close to zero), which in a way reduces the usefulness of these models.</p>
<p>However, couldn't you simply use a different activation function like ReLU to avoid this issue?</p>
","large-language-models"
"45579","Reference request: data efficiency of LLM pre-training","2024-04-29 14:22:36","45712","0","32","<large-language-models><training-datasets><pretrained-models><efficiency><sample-efficiency>","<p>I've seen it stated multiple times that LLMs have much worse data efficiency than humans (IE require more data to reach same or worse performance), EG <a href=""https://twitter.com/ylecun/status/1784210369231130883"" rel=""nofollow noreferrer"">this Tweet by Yann LeCun</a>, or <a href=""https://youtu.be/b76gsOSkHB4?si=typUcVl74LyCZfbM&amp;t=1170"" rel=""nofollow noreferrer"">19:30 in this talk by Michael Wooldridge</a>. Are there any papers (preferably published but otherwise pre-prints) that really explore data-efficiency of LLM pre-training (not ICL, I'm happy to accept that's data-efficient), and possibly compare against an approximate/upper-limit human benchmark?</p>
<p>Preferably looking for papers with thorough exploration and evaluation of data-efficiency of existing LLMs, rather than papers proposing a new method for improving data-efficiency without thorough comparison to existing SOTA models.</p>
<p>I'd like to know so hopefully I can cite such a paper in future work of my own.</p>
","large-language-models"
"45556","Is there a relationship between tokens and parameters in LLMs?","2024-04-27 19:35:10","45560","1","122","<transformer><large-language-models>","<p>What the question says.</p>
<p>In a transformer architecture, is there a relationship between number of tokens and number of parameters?</p>
<p>Can you have a LLM with a small number of parameters but a large context window (many tokens) or viceversa?</p>
","large-language-models"
"45549","Is there research on the effect of typos in LLM prompts?","2024-04-26 19:27:48","","0","118","<reference-request><large-language-models><tokenization>","<p>A simple typo can split a single token for a common word into several tokens, not only making the prompt longer, but also creating a combination of tokens that was rare in the training set. I wonder how big the effect of typos in the prompt is.</p>
<p>It would also be interesting to know whether using more common synonyms for words improves the understanding of the prompt, or whether replacing words with synonyms of identical meaning that are common in other types of text has a strong influence on the type of response, or whether the model simply associates the synonym with the other word and behaves the same way.</p>
<p>Finally, I wonder if a simple spell checker based on tokenization would be a significant way to improve the response. Is there notable research on these topics?</p>
","large-language-models"
"45503","How to create 1 embedding for text + image","2024-04-22 20:24:58","","0","42","<large-language-models><vectors><vector-semantics>","<p>I'm using Ollama to run llm's. I can create embeddings for text and images, which I store in ChromaDB. The goal of all this, is to find content which best fits a question, so I can create a good answer.</p>
<p>Here is how I create embeddings for text</p>
<pre><code>curl http://localhost:11434/api/embeddings -d '{
    &quot;model&quot;: &quot;llama2&quot;,
    &quot;prompt&quot;: &quot;Here is an article about llamas...&quot;
}'
</code></pre>
<p>and for images (base64)</p>
<pre><code>curl http://localhost:11434/api/generate -d '{
  &quot;model&quot;: &quot;llava&quot;,
  &quot;prompt&quot;:&quot;What is in this picture?&quot;,
  &quot;stream&quot;: false,
  &quot;images&quot;: [&quot;iVBORw0KGgoAAAANS.....]
}
</code></pre>
<p>However, now I have some content which is text and image together. The text (or the image) alone is meaningless, but together they tell a complete story. So my question is how should I handle this? Is it possible to create a single embedding for both?</p>
","large-language-models"
"45450","RAG can be done using langchain, Now CohereForAI's Command R +, provides similar functionality, how it is is better than langchain's implementation?","2024-04-15 07:13:43","","1","25","<large-language-models><information-retrieval>","<p>How langchain's RAG implementation is different from the CohereforAI(Command R+) RAG functionality and what edge does it provide over the langchain RAG implementation.
I tried both of them, But I am unable to understand how they differ from each other.</p>
","large-language-models"
"45415","How to Measure the Impact of Adding a Translation Layer to Large Language Models?","2024-04-10 20:37:17","","0","11","<large-language-models><ai-security><efficiency><scalability>","<p>I'm currently exploring the integration of a translation layer into Large Language Models (LLMs) to preprocess all inputs into a uniform language (English) before processing by the model. The idea is to potentially reduce the complexity and size of the dictionary needed by LLMs, which often exceeds 100,000 tokens due to the diversity of languages and encodings they must handle.</p>
<p>I'm seeking insights on methodologies or metrics that can be used to quantify the impact of such a translation layer on various aspects of LLM performance, including:</p>
<ol>
<li><strong>Efficiency Improvement</strong>: How can we measure the change in processing speed or computational resource usage before and after integrating the translation layer?</li>
<li><strong>Accuracy and Integrity</strong>: What metrics or tests can be applied to ensure that the translation layer does not compromise the quality or integrity of the data being processed?</li>
<li><strong>Security Enhancement</strong>: Are there specific benchmarks or tests to evaluate the improvement in security, especially concerning vulnerabilities linked to processing non-standard language inputs?</li>
<li><strong>Scalability Impact</strong>: How might the addition of a translation layer affect the model's scalability, particularly in handling large volumes of diverse data?</li>
</ol>
<p>I'm particularly interested in practical approaches or case studies that have tackled similar challenges in LLMs or related AI fields. Any advice, references, or tools that could guide the implementation and evaluation of this translation layer would be greatly appreciated.</p>
","large-language-models"
"45392","From how many experts does LLM training using a mixture of experts (MoE) start slowing down compared to training LLMs without MoE?","2024-04-08 18:15:29","","0","31","<training><large-language-models><performance><mixture-of-experts>","<p>I'm trying to find some information regarding the impact of the number of experts on LLM training. From how many experts does LLM training using a mixture of experts (MoE) start slowing down compared to training LLMs without MoE?</p>
<hr />
<p>My research:</p>
<p><a href=""https://huggingface.co/blog/moe#how-does-scaling-the-number-of-experts-impact-pretraining"" rel=""nofollow noreferrer"">https://huggingface.co/blog/moe#how-does-scaling-the-number-of-experts-impact-pretraining</a> states:</p>
<blockquote>
<p>How does scaling the number of experts impact pretraining? More experts lead to improved sample efficiency and faster speedup, but these are diminishing gains (especially after 256 or 512), and more VRAM will be needed for inference. The properties studied in Switch Transformers at large scale were consistent at small scale, even with 2, 4, or 8 experts per layer.</p>
</blockquote>
<p>which appears to contradict <a href=""https://arxiv.org/pdf/2402.07871.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/2402.07871.pdf</a>, which seems to say that the training speedup decreases when having 16  experts:</p>
<blockquote>
<p><a href=""https://i.sstatic.net/l6zdf.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/l6zdf.png"" alt=""enter image description here"" /></a></p>
</blockquote>
","large-language-models"
"45378","Train my own LLM on a smaller corpus of text?","2024-04-07 12:21:57","","0","293","<natural-language-processing><transformer><large-language-models><open-ai><huggingface>","<p>Would it be possible to train my own LLM on a smaller corpus of text, lets say some coding documentation that I then want to ask questions about using the model?</p>
<hr />
<p>If so, are there any recommended ways of doing this, i.e is there a prebuilt architecture or library I can use, and just provide the corpus of text.</p>
","large-language-models"
"45341","Could LLMs perform the autoregressive generation with probability vectors instead of choosing a discrete token every time?","2024-04-03 13:46:48","","2","80","<machine-learning><large-language-models>","<p>As I understand it, GPT-style LLMs take a sequence of tokens as input and output a token probability vector. The first thing that happens to an input token is that it goes through
the input embedding, which can be thought of multiplying the embedding matrix with a one-hot token vector.</p>
<p>If that is correct, nothing is stopping us from using a probability vector in place of a one-hot vector as input to the LLM.</p>
<p>Question 1:
What happens when the input to the LLM contains probability vectors instead of discrete tokens?
For example if the token &quot;dog&quot; was [0,0,1,0,0,0,0] and the token &quot;cat&quot; was [0,0,0,1,0,0,0], what if the input instead contained the pseudo-token [0,0,0.5,0.5,0,0,0]?</p>
<p>Question 2: When generating a text, we turn the output probability distribution into a single token by sampling and append the new token. What if we don't sample and instead append the probability vector itself? Of course, this would not produce human-readable text.</p>
<p>When using Chain-of-Thought prompting, we want the model to slowly reason through the problem, but could it not be helpful to let it output full probability vectors at this stage instead of forcing it to discretize into human-readable tokens?</p>
<p>Or would none of this produce meaningful results because models are only trained on discrete tokens?</p>
","large-language-models"
"45314","LLMs as ""fuzzy JPEGs""","2024-03-31 23:15:50","","0","83","<large-language-models><fuzzy-logic><information-retrieval>","<p>We should</p>
<blockquote>
<p>conceptualize LLMs as very quirky and very experimental if advanced
information retrieval systems (fuzzy JPEGs)</p>
</blockquote>
<p>according to <strong>point 7</strong> in the screenshot below (<a href=""https://twitter.com/pmarca/status/1774547180713631885"" rel=""nofollow noreferrer"">grabbed from pmarca's Twitter feed</a>).</p>
<p>Could someone please help me understand how &quot;fuzzy&quot; and &quot;JPEGs&quot; are relevant here?</p>
<p><a href=""https://i.sstatic.net/TZIzT.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/TZIzT.png"" alt=""enter image description here"" /></a></p>
","large-language-models"
"45301","Problems with understanding instruction fine-tuning","2024-03-31 04:55:27","","0","22","<natural-language-processing><transformer><large-language-models><gpt>","<p>I'm trying to read up on instruction fine-tuning, but I think I have a big misunderstanding.</p>
<p>As I understand, instruction datasets typically have 3 components: (a) the instruction (b) the output/response, and (c) and an optional input. Now, according to this <a href=""https://arxiv.org/pdf/2308.10792.pdf"" rel=""nofollow noreferrer"">paper</a>: &quot;Based on the collected IT dataset, a pretrained model can be directly fine-tuned in a fully-supervised manner, where given the instruction and the input, the model is trained by predicting each token in the output sequentially.&quot; This makes sense to me, i.e, the response/output is the ground truth the model is expected to predict.</p>
<p>However, when I check many tutorials (e.g., this <a href=""https://github.com/philschmid/deep-learning-pytorch-huggingface/blob/main/training/instruction-tune-llama-2-int4.ipynb"" rel=""nofollow noreferrer"">tutorial notebook</a>), it seems that instructions, inputs, and outputs are all combined into a single text sample. But I can't tell from the notebook how the training now works. What is now the ground truth for the supervised training. Or is this now treated as a next-word-prediction task?</p>
<p>What am I missing? Or are these indeed two different approaches for instruction tuning. Sorry if those a stupid questions!</p>
","large-language-models"
"45265","Is Claude-3-Haiku a mixture of experts model?","2024-03-27 17:48:21","","1","170","<large-language-models><mixture-of-experts><claude>","<p>If it is not, it is a significant step forward as it is at the level of the original GPT-4, which is a mixture of experts.</p>
","large-language-models"
"45226","Can GenAI be Used to Generate Decision Trees from Text?","2024-03-24 18:04:34","","0","89","<large-language-models><decision-trees>","<p>I'm exploring the capabilities of GenAI for text analysis and decision-making processes. I'm particularly interested in understanding <strong>whether GenAI can be leveraged to create decision trees directly from textual data</strong>.</p>
<p>As an example, let's consider a scenario where we have documents outlining various rules and criteria for approving loans. Are there any GenAI approaches to analyze this document and produce a decision tree that reflects the decision-making process outlined in the text?</p>
<p>To summarize, I'm curious about the feasibility of utilizing GenAI to:</p>
<ul>
<li>Analyze textual data to extract features relevant for
decision-making.</li>
<li>Automatically generate decision trees based on the identified features.</li>
</ul>
<p>I'm aware that GenAI is proficient in natural language processing tasks and can generate text based on provided prompts. However, I'm uncertain about its suitability for constructing decision trees from textual data.</p>
<p>Thereby, I'm curious to know if anyone has experience or insights into using GenAI for this purpose. Additionally, any resources, examples, or methodologies on how to implement such a task would be greatly appreciated.</p>
<p>Thank you!</p>
","large-language-models"
"45215","How to teach Gemma model my mother tongue (Kannada - one of the oldest Indic languages)","2024-03-23 19:54:01","","0","82","<transformer><large-language-models><fine-tuning>","<p>I'm interested in teaching the Gemma 2B model my mother tongue (<a href=""https://en.wikipedia.org/wiki/Kannada"" rel=""nofollow noreferrer"">Kannada</a> - one of the oldest Indic languages). The pre-trained model doesn't work well with the mentioned language, so I thought of teaching the model but I'm very new to this field of training and fine-tuning LLMs. But based on some research I've planned the below steps:</p>
<ol>
<li><p>Prepare the Dataset - luckily I found <a href=""https://huggingface.co/datasets/Cognitive-Lab/Kannada-Instruct-dataset"" rel=""nofollow noreferrer"">some dataset</a> on HF.</p>
</li>
<li><p>Train the Tokenizer for the language and enrich the vocabulary of the Gemma model with the newly obtained tokens from the dataset using a newly trained Tokenizer.</p>
</li>
<li><p>Here is where I'm confused</p>
<p>a. Should I simply fine-tune the model? - maybe using LoRA or QLoRA kind of techniques.</p>
<p>b. I read about a concept called <a href=""https://arxiv.org/abs/2302.03241"" rel=""nofollow noreferrer"">Continually Pre Train</a>, should I give this a try?</p>
</li>
</ol>
<p>3 (b) looks promising to me but I'm not able to find much explanations and code examples on the internet about how to achieve this.</p>
<p><strong>Note:</strong> I'm looking for a very basic model and performing this for learning purposes and I should be able to do this on my Google Colab Notebook.</p>
<p>I'm seeking some guidance here on how to proceed and some code examples for achieving this.</p>
","large-language-models"
"45137","What is the current literature on ""appending"" an entire LLM model as part of the knowledge another LLM can use?","2024-03-16 04:37:01","","0","23","<large-language-models><similarity><information-retrieval>","<p>I've been studying retrieval augmented generation and vector databases recently. In a nutshell, vector based retrieval works by first dividing up input data(whether it's a string, image, etc) into chunks and then generating a vector embedding for each of those chunks. The key is to choose a chunk size that is not too large nor not to small so that the LLM can query the accurate data to augment its response.</p>
<p>This is relatively straightforward. I wonder if entire LLMs can share each other's knowledge as of current technology.</p>
<p>For instance, say I have an LLM that is really <strong>effective</strong> at processing text prompts and returning text answers, another LLM that is really <strong>effective</strong> at generating images given a text, and another LLM that is really <strong>effective</strong> in analyzing images or visual input. Instead of merely vector embedding the chunked outputs of each LLM such that the text based LLM will use the embedded user response to query each of the vector embeddings to augment its final response, I wonder if there is a way to &quot;merge&quot; the 3 LLMs in a way such that they are separate, but one. So whatever one LLM can do must be able to be done by other LLMs. This method of information retrieval between LLMs will not only allow them augment their responses with static - vector embedded - data, but also allow them to augment their responses with dynamic data that another LLM has generated or is in the process of generating.</p>
<p>Is there any literature, latest technology, frameworks, discussions, etc on this topic?</p>
<p>// When I say effective, I mean these things:
// 1. How specific the LLM is when understanding the user prompt (e.g. if the user asks the LLM to generate a poem about recursion by making the poem recursive itself, the LLM should attempt to incorporate the latter part: &quot;making the poem recursive itself&quot; instead of ignoring parts of the prompt it is not trained on or less capable of processing.
// 2. The quality of the generated output after processing the user prompt. This is straightforward. Is the image garbage? Is a textual response incorrect or deviating from the user's instructions, despite 1. being fulfilled?</p>
","large-language-models"
"45114","What's the current best ways to extend context windows in LLMs?","2024-03-13 19:45:57","","0","32","<large-language-models><state-of-the-art>","<p>What's the state of the art? The most elegant/ simplest to implement? The ones that require least computational resources?</p>
","large-language-models"
"45054","Why do LLM tokenizers use a special symbol for space such as Ġ in BPE or ▁ in SPM?","2024-03-08 08:55:04","","1","197","<natural-language-processing><large-language-models><language-model>","<p>Popular tokenizers use a special symbol such as &quot;Ġ&quot; (BPE) or &quot;▁&quot; (SentencePiece) to represent space. What is the reasoning behind this?</p>
<p>I did try searching for the answer. I got two types of explanations, but they don't explain anything to me.</p>
<p><strong>Some languages such as Japanese don't typically use space to separate words.</strong></p>
<p>That's true, but tokenizing such a language won't make use of the special character as well. I don't understand how having a token as <code>&quot;Ġhorse&quot;</code> is any different from <code>&quot; horse&quot;</code> in this scenario.</p>
<p>Let's assume we want to tokenize the made-up laguage Spaceless English, which is English, but without whitespace. <code>tokenize(&quot;I'mridingahorse&quot;) -&gt; [&quot;I'm&quot;, &quot;riding&quot;, &quot;ahorse&quot;]</code>. No need for spaces at all, let alone a special character.</p>
<p>In fact this seems like it would be in favor of using a plain space as opposed to a special symbol.</p>
<p><strong>Some tokens have a space in them and this helps us differentiate between spaces in the original text and spaces in the tokens</strong></p>
<p>For example &quot;New York&quot; might be a token.</p>
<p>This kinda makes sense, but I still don't quite get why we would ever need this distinction. How is <code>&quot;ĠNew York&quot;</code> different from <code>&quot; New York&quot;</code>? <code>&quot; New York&quot;</code> is still different from <code>[&quot; New&quot;, &quot; Toronto&quot;]</code>. A tokenizer would employ some kind of a greedy algorithm to tokenize text... and even if it's not greedy, but the full NP-complete search, it would still tokenize <code>&quot;I'm going to New York&quot;</code> as <code>[&quot;I'm&quot;, &quot; going&quot;, &quot; to&quot;, &quot; New York&quot;]</code>, the same way as we would expect it to tokenize &quot;extracurricular&quot; to say <code>[&quot;extra&quot;, &quot;curricul&quot;, &quot;ar&quot;]</code> and not to <code>[&quot;ex&quot;, &quot;trac&quot;, &quot;u&quot;, &quot;rr&quot;, &quot;ic&quot;, &quot;u&quot;, &quot;lar&quot;]</code>, and the same way as we would expect it to work on Spaceless English.</p>
<p>Note that I'm looking at the problem only through the lens of converting text to tokens and tokens to text. Perhaps the need for these character has to do with creating the token list based on the training corpus?</p>
","large-language-models"
"45033","Why are Mistral LLMs branded as enabling RAG-QA (Retrieval-Augmented Generation for Question Answering)?","2024-03-06 13:30:38","","1","58","<large-language-models><question-answering><information-retrieval><mistral>","<p>I read on <a href=""https://mistral.ai/news/mistral-large/"" rel=""nofollow noreferrer"">https://mistral.ai/news/mistral-large/</a>:</p>
<blockquote>
<p>Mistral Small benefits from the same innovation as Mistral Large regarding RAG-enablement and function calling.</p>
</blockquote>
<p>What do they mean by RAG-enablement? I'm familiar with RAG-QA, but any LLM can be used for RAG-QA. So why are they talking about RAG-enablement? Do the Mistral models have some special training/performance/etc. for RAG-QA?</p>
","large-language-models"
"45015","Minimum Number of Samples for LLM Benchmark?","2024-03-04 18:22:44","","0","108","<natural-language-processing><large-language-models><benchmarks>","<p>I am working on a project to evaluate various fine-tuned LLMs. Unfortunately inference is prohibitively slow, and I don't think I will be able to test my models on the full test set of 40,000 samples. My current plan is to implement an early stopping threshold.</p>
<p><strong>Is there an &quot;accepted&quot; or theoretical minimum number of samples that are required to benchmark an LLM?</strong></p>
","large-language-models"
"45005","How do I code so that the embedding output and input share the same weight matrices?","2024-03-04 00:22:41","45012","0","141","<transformer><pytorch><attention><large-language-models><embeddings>","<p>I am trying to implement the <em>Attention is All You Need</em> paper from scratch. The authors mentioned in section 3.4 that <em>&quot;In
our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [30]&quot;</em> [30] here is the paper <em>Using the output embedding to improve language models</em> by Ofir Press and Lior Wolf.</p>
<p>Excuse me but</p>
<ol>
<li><p>could you explain why we need output embedding between encoder stack and the decoder stack? Isn't the output of encoder d_model dimension vectors (d_model = 512 in the original paper) so they don't need embedding layer in-between? Can I treat this as a linear layer?</p>
</li>
<li><p>could you explain how the weights can be shared between the pre-softmax linear layer and the input embedding? Since in the original paper, it was about the translation between German and English, shouldn't the weight matrices be different for pre-softmax linear layer and the input embedding since the number of vocabularies for two languages aren't the same?</p>
</li>
</ol>
<p>Thank you for reading my question!</p>
","large-language-models"
"44947","How are sentences turned into a vector in LLM","2024-02-28 10:19:02","","1","211","<attention><large-language-models><chatgpt><word-embedding><linear-algebra>","<p>My understanding of <em>Large Language Models</em> like <em>GPT</em> is that they are special kinds of <em>deep neural networks</em> specifically trained to predict the next word, given the beginning of a sentence.</p>
<p>I understand that a key aspect of their architecture is <em>attention</em>, which allows a word representation (a vector) to be mixed-up with the representation of other words in the sentence, the weights being used to make that linear combination representing a notion of proximity as they are derived from the scalar product of the words embedding.</p>
<p>Now, considering a sentence of words like &quot;word1 word2 word3&quot;, if we want to feed it to a neural network, which has a fixed number of input nodes, we should represent it as a <strong>single</strong> input vector of fixed size (same as the number of input nodes). My understanding of <em>attention</em> is that it still produces one vector for each input words, not one vector for the whole sentence.</p>
<p>How is a full sentence turned into a fixed-size vector?</p>
","large-language-models"
"44925","What is an ""inference kernel""?","2024-02-26 12:41:01","","0","153","<terminology><large-language-models><chatgpt><inference>","<p>In the recent event where ChatGPT &quot;went crazy&quot;, this term was used in the official post-mortem to describe what happened:</p>
<blockquote>
<p>In this case, the bug was in the step where the model chooses these numbers. Akin to being lost in translation, the model chose slightly wrong numbers, which produced word sequences that made no sense. More technically, <strong>inference kernels</strong> produced incorrect results when used in certain GPU configurations.</p>
</blockquote>
<p>Now I know what <strong>inference</strong> is, at least wrt LLMs. It's when you use a trained LLM to generate text by giving it a prompt (feel free to offer a more technical clarification if necessary). As for <strong>kernel</strong> that has several meanings in computing generally, such as an <a href=""https://en.wikipedia.org/wiki/Kernel_(operating_system)"" rel=""nofollow noreferrer"">operating system kernel</a> being the core part of the <strong>OS</strong>, often in contrast to shells, including command lines and GUIs. Then there is the <a href=""https://en.wikipedia.org/wiki/Kernel_(image_processing)"" rel=""nofollow noreferrer"">sense of kernel in image processing</a>, also known as a <strong>convolution matrix</strong>. There's also <strong>Compute Kernels</strong> in <strong>GPGPU</strong> programming, with which I am the least familiar.</p>
<p>But what does the overall phrase &quot;inference kernel&quot; refer to technically? My hunch is that it's some kind of specialization of the GPU sense, though I read <a href=""https://www.lesswrong.com/posts/koNsuop5AHxBG5D7t/can-someone-explain-to-me-what-went-wrong-with-chatgpt?commentId=5ts6boALcsvFHS9Ba"" rel=""nofollow noreferrer"">a comment in an AI-adjacent forum</a> that it's related to the OS sense, which I doubt is accurate.</p>
<p>Other than that I can't actually find this term mentioned very much, and I can't find it defined at all.</p>
<hr />
<p><strong>EDIT</strong></p>
<p><a href=""http://www.proof-technologies.com/misc/glossary.html"" rel=""nofollow noreferrer"">I found a definition of <em>inference kernel</em></a> but it's about automatic theorom provers and doesn't seem to be applicable to AI, not that I can follow it very well:</p>
<blockquote>
<p><strong>inference kernel</strong> : (n) The part of an LCF-style system that implements the deductive system of its formal logic, and relied upon by all subsequent deductive functionality for its soundness. It defines the datatype for internal theorems as an abstract datatype, with the primitive inference rules and the primitive assertion commands as its only primitive constructors. The inference kernel is a crucial trusted component of its system. See &quot;language kernel&quot;, &quot;logical core&quot;, &quot;core system&quot;.</p>
</blockquote>
","large-language-models"
"43897","Does DPO update the weights in the same way that LORA/Fine-Tuining does?","2024-02-23 20:02:31","","0","69","<reinforcement-learning><training><large-language-models><fine-tuning><training-datasets>","<p>I've been working on making a conversational customer service fine-tune for the past couple months and now I am looking to improve its failure recovery.</p>
<p>For example, If the agent makes a mistake, how do I train it to recover from the mistake without training it to make the mistake in the first place. The training sample needs to include the agent causing the failure in order to recover, but then am I just training the model to make the failure in the first place?</p>
<p>I've currently been keeping any training samples that contain the agent making a mistake out of the dataset in fear of it just training the agent to make the mistake.</p>
<p>I thought DPO might be a solution to this, where you have a sample with a mistake, and then the chosen/rejected categories contain a bad retry and a good retry at the call. But I am concerned that DPO might just continue training the model to make the mistake in the first place and then recover from it.</p>
<p>Anyone have any suggestions?</p>
","large-language-models"
"43883","Why do mix models work?","2024-02-22 20:40:43","","1","47","<neural-networks><generative-model><large-language-models><diffusion-models>","<p>Is there research on why models mixes work? One would expect that averaging the weights of two models would produce garbage, but many models mixes created by amateurs show that they not only work, but also produce results that, for example, interpolate between two image styles when two image models with different styles are mixed. In mixes of language models, the interpolation is harder to see, but mixing the models also seems to work, and is used by people trying to improve model performance by mixing good models.</p>
<p>If you want to see examples, visit <a href=""https://civitai.com/models"" rel=""nofollow noreferrer"">Civitai</a> and set the filters (in the upper right corner) to &quot;type=checkpoint&quot; and &quot;checkpoint_type=merge&quot; and you can find, e.g., models that merge weights for anime style with weights for realistic style and get a semi-realistic style as a result.</p>
<p>Here is the <a href=""https://github.com/hako-mikan/sd-webui-supermerger?tab=readme-ov-file#merge-models"" rel=""nofollow noreferrer"">documentation</a> of one of the softwares that allow to merge stable diffusion checkpoints.</p>
","large-language-models"
"43728","How to force LLM (like OpenAI ChatGPT) to output a variable list of values?","2024-02-09 02:38:01","","1","1126","<large-language-models><chatgpt><prompt>","<p>What prompt (or other technique) should I use with an LLM so that</p>
<ol>
<li>The result is guaranteed to be reliably parseable as a list of values (e.g. a Python list of strings)</li>
<li>LLM would understand that a variable list of values is expected during generation</li>
</ol>
<h1>EDIT: Chosen solution</h1>
<p>What I personally ended up using is BAML. Works with all LLMs and guarantees correct output.</p>
<p><a href=""https://github.com/BoundaryML/baml"" rel=""nofollow noreferrer"">https://github.com/BoundaryML/baml</a></p>
<p><code>pip install baml-py</code></p>
<p>Example:</p>
<p><a href=""https://www.promptfiddle.com/BAML-Examples-7xLvl"" rel=""nofollow noreferrer"">https://www.promptfiddle.com/BAML-Examples-7xLvl</a></p>
","large-language-models"
"43706","Getting started with training local LLM using python","2024-02-06 08:05:12","","0","790","<training><transformer><large-language-models><gpt>","<p>As I'm completely new to this field, I find it hard to get started given the requirements I have. I'm a bit overwhelmed by all the models and options that are available. Even though it wasn't difficult to run an LLM locally:</p>
<pre><code><span class=""math-container"">$&gt; brew install llm
$</span>&gt; llm -m gpt4all-13b-snoozy-q4_0  &quot;Tell me a joke&quot;
</code></pre>
<p>despite being very nice, it is not what I'm looking for. I want to train a model with my own data. Also, I want to build a website around it, so it means I can access the LLM with APIs.</p>
<p>So, as you can see I have some requirements. What would be a great place to start reading? A good tutorial to get started would be very much appreciated!</p>
","large-language-models"
"43665","Fetch latest data from Gemini model to build chat bot","2024-02-03 08:00:57","","0","36","<large-language-models><chat-bots>","<p>I am building a chat bot based on gemini pro model through prompt engineering. The use case is simple. It allows user to ask a question about something specific which entails generating info about current event or past history. Though the model is performing well on past data, I am unable to get it to answer info beyond Gemini's cut off date (Sep 2023 or so). Has anyone tried fetching data beyond gemini model's cut off date with luck? Please suggest prompts.</p>
<p>So far I tried prompts like:</p>
<ol>
<li>Ensure the response includes the most recent data and updates as of {today}</li>
<li>Retrieve the latest information from Google search as of {today} to provide the answer</li>
<li>If the source provides a publication date, strictly filter for results dated {today}. Else, refer to the source with latest information from {today}.</li>
</ol>
<p>Today is a variable of today's date.</p>
","large-language-models"
"43601","understanding the distribution shift problem in direct preference optimization (DPO)","2024-01-29 05:40:58","","0","112","<natural-language-processing><large-language-models><rlhf>","<p>I'm having trouble understanding this paragraph of the <a href=""https://arxiv.org/pdf/2305.18290.pdf"" rel=""nofollow noreferrer"">DPO paper</a>:</p>
<p><a href=""https://i.sstatic.net/gCdY4.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/gCdY4.png"" alt=""enter image description here"" /></a></p>
<p>Why does it matter so much that the preference data distribution aligns with the reference model output distribution? My understanding is that during training, the parameters of the sft (supervised fine-tuning) are updated such that chosen responses (<span class=""math-container"">$y_w$</span>) have a higher probability of being generated, and rejected responses (<span class=""math-container"">$y_l$</span>) have a lower probability of being generated, and the reference model is just there to prevent the sft model from straying too far from the original parameters. But I fail to understand how the wrong reference distribution could hinder this process. Could someone please help me?</p>
","large-language-models"
"43598","What reference documents exist for LLM inference engines and models?","2024-01-28 21:22:27","","0","35","<large-language-models><models><inference>","<p>For example, vllm is an inference engine, and according to their roadmap they will incororate vllm into several LLM API engines such as openllm, rayserve, and nvidia triton.</p>
<ul>
<li><p>What are examples of other inference engines?</p>
</li>
<li><p>You can run the same model (e.g. llama2) under several inference engines.  What is the specification/doc/etc that the inference engine authors would use to implement this?</p>
</li>
<li><p>What specifications/documentation/etc are used when inference engine authors write an engine to support these models?</p>
</li>
</ul>
","large-language-models"
"43595","Where do you keep track of Chat History for an LLM Chat Application?","2024-01-28 13:04:06","","0","225","<large-language-models><open-ai>","<p>I am creating a LLM chat app with LangChain. I am keeping track of the chat history with a simple array that gets sent to the LLM with every prompt. I noticed LangChain also has classes that automatically keep track of the whole conversation, such as BufferMemory and ConversationBufferMemory</p>
<p><a href=""https://python.langchain.com/docs/modules/memory/types/buffer"" rel=""nofollow noreferrer"">https://python.langchain.com/docs/modules/memory/types/buffer</a></p>
<p>It's not explained however if these classes automatically keep track of different users as well? What if two users connect to the script, will their conversations get mixed up with each other?</p>
<p>How do you keep track of multiple users in a Chat LLM application?</p>
","large-language-models"
"43562","Chat with your DB: How to get AI to follow your rules?","2024-01-25 21:25:51","","0","28","<python><large-language-models><training-datasets><natural-language-understanding>","<p><strong>Context/Background:</strong> I have a single table in MySQL with info on 'members', including things like location, favourite types of music, favourite camping spots, etc.</p>
<p><strong>Required Behaviour:</strong> I need the system to:</p>
<p>Have the AI 'match' people based on rules. Rules are, for example: Only match people in the same age-range who like the same music and enjoy outdoor festivals.</p>
<p>Allow me to ask questions of the DB, such as &quot;how many rock lovers between 21 and 30 went to the burning man festival?&quot;</p>
<p><strong>Attempts:</strong> My latest efforts were using the Chainlit library for Python, trying various LLMs.</p>
<p><strong>Actual Behaviour:</strong> It would appear to work at first, but would immediately fail to follow the most basic 'rules'. I would get the bot to admit it didn't follow the rule and yet it would continue to disobey.</p>
<p>I know this is of course already being done, I just can't get anyone (after spending hundreds on developers) who knows how to do this.</p>
<p>I would greatly appreciate any suggestions for a solution.</p>
<p><strong>Chainlit Code:</strong></p>
<p>chainlit_app.py</p>
<pre><code>    import os

from langchain.chat_models import ChatOpenAI
from langchain import SQLDatabase, SQLDatabaseChain
from prompting import SQLITE_PROMPT
import tiktoken
from langchain.text_splitter import 
RecursiveCharacterTextSplitter
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFDirectoryLoader
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.docstore.document import Document
from langchain import PromptTemplate
from langchain.chains.question_answering import load_qa_chain
from langchain.memory import ConversationBufferMemory
from langchain.llms import OpenAI

tokenizer = tiktoken.get_encoding(&quot;cl100k_base&quot;)

def tiktoken_len(text):
    tokens = tokenizer.encode(text, disallowed_special=())
    return len(tokens)


os.environ['OPENAI_API_KEY'] = &quot;APIKEY&quot;


docsearch = &quot;&quot;

if os.path.exists(&quot;./store/index.faiss&quot;):
    docsearch = FAISS.load_local(&quot;./store&quot;, OpenAIEmbeddings())
else:
    print(&quot;does't exist&quot;)
    loader = PyPDFDirectoryLoader(&quot;./traindata&quot;)
    text_splitter = CharacterTextSplitter(
        separator=&quot;\n&quot;, chunk_size=1000, chunk_overlap=200, 
length_function=len
    )
    texts = loader.load_and_split(text_splitter)
    docsearch = FAISS.from_documents(texts, OpenAIEmbeddings())
    local_url = &quot;./store&quot;
    docsearch.save_local(local_url)

# @cl.on_message
# async def main(message):
db = SQLDatabase.from_uri(&quot;sqlite:///members_full.db&quot;, 
view_support=True)                   
llm = ChatOpenAI(model=&quot;gpt-4&quot;)    
agent = SQLDatabaseChain(llm=llm, database=db, verbose=False)
agent.llm_chain.prompt = SQLITE_PROMPT

while 1:
    message = input(&quot;Ask away! : &quot;)

    res = docsearch.similarity_search(message)
    chain = load_qa_chain(OpenAI(temperature=0), 
chain_type=&quot;stuff&quot;)
    completion = chain.run(input_documents=res, question=message)
    if &quot;I don't know&quot; in completion or &quot;not mentioned&quot; in 
completion or &quot;not provided&quot; in completion:
        res_agent = agent(message)
        print(res_agent['result'])
    else:
        print(completion)



from rule_system import get_rule_based_response

def get_response(user_query):
    rule_response = get_rule_based_response(user_query)
    if rule_response:
        return rule_response

    ai_response = get_ai_response(user_query)
    return ai_response
</code></pre>
<p>prompting.py</p>
<pre><code>    from langchain.prompts.prompt import PromptTemplate
from langchain.memory import ConversationBufferMemory

settings = {
    &quot;temperature&quot;: 2,
    &quot;max_tokens&quot;: 500
  
}

# Define custom instructions
custom_instructions = &quot;Remember, we need to strictly follow the 

rules in the train1.pdf regarding location, favorite music, 
favorite camping sites, etc.
    

table_additional_info = &quot;&quot;&quot;\n members_free Table Value Mapping
gender  : 0 means Female , 1 means Male

there is no field named first_name.  do not query for first_name in 
the db or query for any 'name' variable.
never display a person's name.

&quot;&quot;&quot;

PROMPT_SUFFIX = &quot;&quot;&quot;Only use the following tables:

{table_info}



&quot;&quot;&quot; + table_additional_info +&quot;&quot;&quot;

Question: {input}&quot;&quot;&quot;

_sqlite_prompt = &quot;&quot;&quot;You are a helpful and friendly SQLite expert         
and always do the SQL queries necessary. Given an input question, 

first create a syntactically correct SQLite query to run, then look 
at the results of the query and return the answer to the input 
question.

Unless the user specifies in the question a specific number of 
examples to obtain, query for at most {top_k} results using the 
LIMIT clause as per SQLite. You can order the results to return the 
most informative data in the database.
Never query for all columns from a table. You must query only the 
columns that are needed to answer the question. Wrap each column 
name in double quotes (&quot;) to denote them as delimited identifiers.
Pay attention to use only the column names you can see in the 
tables below. Be careful to not query for columns that do not 
exist. Also, pay attention to which column is in which table.
Pay attention to use date('now') function to get the current date, 
if the question involves &quot;today&quot;.
You never create or invent answers.  When you are asked for 
'matches' or to 'match someone' you will always follow the rules 
in the train1.pdf precisely.  You will not tell me to run a query 
or check the data for myself, you will run the 
required query to respond to my questions.

Before generating a response, ensure you are adhering strictly to 
the guidelines set forth in our training documentation. 
This is crucial for providing accurate and reliable answers. The 
rules in &quot;train1.pdf&quot; are especially critical 
and must be followed at all times. Non-compliance with these rules 
will lead to suboptimal performance and may not 
meet the user's expectations. Always prioritize these guidelines in 
every interaction.

Use the following format:

Question: Question here
SQLQuery: SQL Query to run
SQLResult: Result of the SQLQuery
Answer: Final answer here


Please always follow the rules outlined in the train1.pdf.  This is 
absolutely essential to this app working.

You will not invent any answers or falsify data.
You will do all the SQL queries when asked to do a match or 
retrieve data from the database or training documents.
When you provide matches, you will include your reasoning in why 
you think the matches are a good fit for the target user.
When asked to match a target user to an event in the db, you will 
start with the most recent entries in the db by referencing the 
'date' field in the db.




&quot;&quot;&quot;

SQLITE_PROMPT = PromptTemplate(
    input_variables=[&quot;input&quot;, &quot;table_info&quot;, &quot;top_k&quot;],
    template=_sqlite_prompt + PROMPT_SUFFIX,
)
</code></pre>
<p>rule_system.py</p>
<pre><code>rule_based_answers = {
&quot;What rules do I need to follow as the Ai?&quot;; &quot;The rules in the 
train1.pdf&quot;,
&quot;What is the company's address?&quot;: &quot;123 Main St, Sample City, 
Country&quot;,
&quot;How can I contact support?&quot;: &quot;Please email support@example.com.&quot;,
# ... add more predefined answers
}

def get_rule_based_response(query):
return rule_based_answers.get(query)
</code></pre>
<p>I can't include the DB or the Training guide, but that shouldn't matter - it often provides accurate results on direct queries to the DB, just not following the Rules when asked to do a 'match'.</p>
<p>If I can provide anything further that might help, please let me know.</p>
","large-language-models"
"43511","Any suggestions for transformer finetuning techniques ablation study?","2024-01-22 11:57:04","","0","44","<transformer><large-language-models><fine-tuning>","<p>I'm planning to fine tune a 7b parameter model for a research project. I understand the different steps of model fine tuning, namely</p>
<ol>
<li>Supervised fine tuning - where we train model on curated examples to impart domain specific knowledge (could be instruction fine tuning as well)</li>
<li>DPO/PPO - this step is done for model alignment to human preferences</li>
</ol>
<p>I understand there is a huggingface library which has built wrappers around these both but I'm trying to get an in depth understanding. So how I see a language model with depth N is that the first N-1 layers are building a representation and last layer is a policy network, i.e. takes the representation and spits out a probability distribution over the tokens. Now my main question, in SFT and DPO/PPO is there any literature which describes the following and provides benchmarks/ablation studies for the following -</p>
<ol>
<li>Doing SFT (no frozen parameters) and only using adapters for DPO/PPO</li>
<li>Doing SFT by freezing all layers except the last and followed by DPO/PPO on last layer</li>
<li>Doing SFT using adapters/LoRA, merging the weights, followed by adapters for DPO/PPO</li>
<li>Doing SFT using adapters/LoRA, merging the weights, followed by DPO/PPO on only last layer</li>
<li>Vanilla option, i.e. SFT followed by DPO/PPO no frozen weights.</li>
</ol>
<p>Any reference for literature or anecdotal evidence?</p>
","large-language-models"
"43490","Are LLM parameters synonymous with dimensions?","2024-01-20 02:52:06","","0","398","<deep-learning><large-language-models><dimensionality>","<p>For example, would a Large Language Model (LLM) with parameter size 140 Billion have 140 Billion dimensions <a href=""https://stats.stackexchange.com/questions/144770/how-to-determine-the-number-of-dimensions-to-a-machine-learning-problem"">as defined in deep learning as the number of nodes in the input layer?</a></p>
<p>Another way to ask this might be: Is 140B parameters the same as saying 140B nodes/dimensions in the encoder of the LLM?</p>
","large-language-models"
"43471","In the paper ""LLM in a flash,"" what is meant by an up projection or down projection layer?","2024-01-19 00:32:56","43473","1","673","<neural-networks><terminology><large-language-models><academia>","<p>In the paper, they first use the terms &quot;up projection layer,&quot; and similarly for down projection, in this paragraph in the introduction:</p>
<blockquote>
<p>Row-column bundling: We store a concatenated row and column of the up-projection and down-projection layers to read bigger contiguous chunks from flash memory. This increases throughput by reading larger chunks.</p>
</blockquote>
<p>What does this refer to in terms of the architecture of a given LLM?</p>
<p>This paper focuses on the Falcon and OPT LLM models.</p>
<p>To try to understand this, I've read through the paper for other explanations, and I searched online. One suggestion I found is that there are special layers in a network that (speaking informally) increase or decrease the size of the resolution, but I don't think that would apply to these specific LLM models. (For example, a max-pooling layer may be seen as decreasing resolution.)</p>
<p>The image below is relevant, but I don't think it answers the question. <a href=""https://arxiv.org/abs/2312.11514"" rel=""nofollow noreferrer"">Here</a> is the paper in question.</p>
<p><a href=""https://i.sstatic.net/IfGEM.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/IfGEM.png"" alt=""Figure 7 from the paper"" /></a></p>
","large-language-models"
"43424","What are some tips of curating a dataset to fine-tune a code-completion LLM?","2024-01-13 21:24:11","","0","73","<datasets><large-language-models><fine-tuning>","<p>There is a new SDK that I am working on and I want to know what are some ways of automatically curating a dataset to train a code-completing LLM to deploy as a VSCode plugin? Hacky ways are appreciated. I was thinking of using chatgpt API to make numerous API calls to inflate a CSV with artificially generated prompts - code entries. There are available datasets of course but I want to tailor the code completion for this particular SDK.</p>
<p>Appreciate all answers.</p>
","large-language-models"
"43417","how can I interpret attention weights matrix? Are they reliable?","2024-01-12 23:11:20","","0","83","<machine-learning><natural-language-processing><attention><large-language-models><explainable-ai>","<p>I've fine-tuned two different models (Bert and Roberta) on a dataset for a binary classification task and I'm comparing the sentences where the models predict wrong. I decided to use attention weights as explainability method to understand which tokens are contributing the most to the model's output. I have a function that visualizes the attention matrix:</p>
<pre><code>def show_attention_matrix(text, model):
    
    input_ids = tokenizer(text, return_tensors=&quot;pt&quot;)[&quot;input_ids&quot;].to(device)
    attention_mask = tokenizer(text, return_tensors=&quot;pt&quot;)[&quot;attention_mask&quot;].to(device)

    tokens = tokenizer.convert_ids_to_tokens(input_ids.view(-1))

    attentions = model(input_ids, attention_mask)[&quot;attentions&quot;]
    last_layer_attention = attentions[-1]
    last_layer_attention = last_layer_attention.squeeze(0)
    mean_attentions = torch.mean(last_layer_attention, dim=0)

    mean_attentions = mean_attentions.cpu().detach().numpy()

    df = pd.DataFrame(mean_attentions)

    plt.figure(figsize=(20, 10))
    heatmap = sns.heatmap(df, annot=True, cmap=&quot;viridis&quot;, fmt=&quot;.3f&quot;, cbar=True, xticklabels=tokens, yticklabels=tokens)
    heatmap.xaxis.tick_top()
    plt.show()
</code></pre>
<p>The code above generates a plot like the following:<a href=""https://i.sstatic.net/MluiQ.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/MluiQ.jpg"" alt=""enter image description here"" /></a></p>
<p>Since I'm getting the embeddings from the [CLS] token and pass it to a classification head, does it make sense to also look at the weights of the [CLS] and find the tokens with the highest scores?</p>
","large-language-models"
"43408","publically available language models that can be used to train arbitrary language data?","2024-01-12 08:00:09","","1","39","<natural-language-processing><large-language-models><chatgpt><language-model>","<p>I have sentence data in a language that is not widely in use and as such popular LLMs do not support the language. I want to train some language model such that given some question, it is able to respond back in the same language, just as in ChatGPT just with a different language.</p>
<p>In such a case, what language model is publically available and is sufficiently powerful? Or would it be possible to use popular LLMs (such as ChatGPT) to achieve such a goal?</p>
","large-language-models"
"43357","Are There Scientific Papers on Methods for Robustifying LLMs and Current Challenges They Face?","2024-01-08 12:45:23","","0","16","<machine-learning><papers><research><large-language-models>","<p>I'm currently researching Large Language Models (LLMs) and am particularly interested in the recent advancements and challenges in this field. My focus is on understanding the methods being developed to make LLMs more robust and the key issues that these models are currently facing.</p>
<p>Could anyone point me towards scientific papers or articles that discuss methods for robustifying LLMs? I'm looking for resources that delve into techniques for improving the reliability, accuracy, and overall performance of these models under various conditions.</p>
","large-language-models"
"43341","Which techniques are best suitable for explainable AI for LLM models","2024-01-07 05:51:49","","1","418","<large-language-models><explainable-ai>","<p>I am currently working with large language models like llama and mistral, interested in techniques for making these models more explainable. I am looking for some tools or techniques which can help me with insights of how their models comes up with the predictions.</p>
<p>Which are the tools and techniques for interpreting the decision-making process?</p>
","large-language-models"
"43336","How to learn text style in an article using LLMs?","2024-01-06 21:06:10","","2","41","<natural-language-processing><large-language-models><hidden-layers><style-transfer>","<p>What is the best way to learn text style in an article? By text style I mean special characteristics and patterns inherent to different authors/group's writing style. For-example, author attribution problem is about learning who wrote an article.</p>
<p>I've seen some researchers applying VAEs and GANs to text to learn the style distribution of sentences in an article (Some references: <a href=""https://arxiv.org/pdf/2109.04624.pdf"" rel=""nofollow noreferrer"">here</a>, <a href=""https://arxiv.org/pdf/2103.07567.pdf"" rel=""nofollow noreferrer"">here</a>, <a href=""https://www.usenix.org/conference/usenixsecurity18/presentation/shetty"" rel=""nofollow noreferrer"">here</a>). However, VAEs and GANs usually have worse performance in language compared to vision tasks.</p>
<p>My question is how to capture style information using LLMs? Is averaging the last hidden states of sentences/paragraphs based on the &quot;max_length&quot; attribute of the model a good approach? Is there a better way to capture style information with LLMs?</p>
","large-language-models"
"43317","Neural Machine Translation with multi-language input to a single-language output?","2024-01-04 21:14:30","","1","17","<deep-learning><natural-language-processing><large-language-models><machine-translation>","<p>I'm looking for NMT paradigms where the input to the model is the same text in N languages (e.g., L1, L2, L3) and the output is the translation in a different target language (e.g., L4).</p>
<p>However, I have the Rosetta Stone problem. Each input text might be randomly noisy/incomplete, that's why I think such a paradigm might help. The hope is to train a model that can compensate for the noise (missing elements) in one input text using the parallel elements in the other input texts (assuming that no element is missing from all input texts).</p>
<p>For training, I have parallel L1-L2-L3-L4 text quadruplets. I also have texts in each individual language separately that I can use for pre-training. Are there any methods to pre-train NMT models using single-language data?</p>
","large-language-models"
"43314","Why are LLMs able to reproduce bodies of known text exactly?","2024-01-04 19:35:24","","11","3686","<generative-model><large-language-models><text-generation>","<p>Mathematically, I wouldn't expect LLMs to be able to reproduce source texts exactly unless the source text was the probable outcome given some prompt. However, I have now tested HuggingFaceH4/zephyr-7b-beta, TheBloke/Llama-2-7B-Chat-GGUF, and ChatGPT and they are all able to accurately reproduce Moby Dick.</p>
<h3>HuggingFaceH4/zephyr-7b-beta</h3>
<p>Prompt:</p>
<pre><code>Print the first page of Moby Dick.
</code></pre>
<p>Response:</p>
<pre><code>I'm unable to print documents directly, but I can provide you with the text of the first page of moby dick by herman melville:

call me ishmael. some years ago—never mind how long precisely—having little or no money in my purse, and nothing particular to interest me on shore, I thought I would sail about a little and see the watery part of the world. It is a way I have of driving off the spleen, and regulating the circulation.  
...SNIP...
</code></pre>
<h3>TheBloke/Llama-2-7B-Chat-GGUF</h3>
<pre><code>print the first page of moby dick
</code></pre>
<pre><code>Certainly! Here is the first page of Moby Dick by Herman Melville:
Call me Ishmael. Some years ago—never mind how long precisely—having little or no money in my purse, and nothing particular to interest me on shore, I thought I would sail about a little and see the watery part of the world. It is a way I have of driving off the spleen, and regulating the circulation. Whenever I find myself growing grim about the mouth; whenever it is a damp, drizzly November in my soul; whenever I find myself involuntarily pausing before coffin-makers' shops, and 
...SNIP...
</code></pre>
<p>I would have expected them to generate a probabilistic answer but this appears to be exact. Is that simply because it was the likely output? Is there some alternate facility that is producing these results? I used <a href=""https://github.com/h2oai/h2ogpt"" rel=""noreferrer"">h2ogpt</a> for testing.</p>
<h2>Update</h2>
<p>It seems like it's probably just that it's mathematically likely because if you ask it other question it does just generate nonsense. Ex: What are the chapters in Moby Dick? I haven't read Moby Dick in English, but I don't recall a chapter 90, &quot;The Whale's Eye Cups&quot;</p>
","large-language-models"
"43288","Comparing the performances of GPTs with deep learning in the field of binary files and their related reports","2024-01-02 07:38:23","","1","43","<deep-learning><transformer><generative-model><large-language-models><gpt>","<p>Regarding the case study of a dataset including binary files (containing assembly code) and reports related to each file (the content of the static analysis of the file as well as the analysis of the dynamic functionality of the file, with a specific format), can it be generally claimed that the learning on this dataset with the use of GPTs will be more effective than training by other deep learning methods?
What exactly should be considered for this comparison?
An example of assembly code of a binary file may be this simple code:</p>
<pre><code>global    _start

          section   .text
_start:   mov       rax, 1                  ; system call for write
          mov       rdi, 1                  ; file handle 1 is stdout
          mov       rsi, message            ; address of string to output
          mov       rdx, 13                 ; number of bytes
          syscall                           ; invoke operating system to do the write
          mov       rax, 60                 ; system call for exit
          xor       rdi, rdi                ; exit code 0
          syscall                           ; invoke operating system to exit

          section   .data
message:  db        &quot;Hello, World&quot;, 10      ; note the newline at the end
</code></pre>
<p>And an example of features that a report can present are:
hash of binary file, sections of file (.text , etc.), strings, network connections, activities carried out in the registry, etc.</p>
","large-language-models"
"43254","Finetuning Mistral or MistralForSequenceClassification for text classification","2023-12-29 18:05:11","","1","742","<natural-language-processing><training><python><large-language-models><fine-tuning>","<p>I need to do text classification and have a dataset of 10K entries. I am considering using mistral and following a tutorial like <a href=""https://huggingface.co/docs/transformers/training"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/training</a> and replace model name by Mistral. But I have noticed there is also MistralForSequenceClassification.</p>
<p>Could you please help me with the following questions:</p>
<p>Is using the above mentioned tutorial and replacing the model name with mistralai/Mistral-7B-v0.1 the correct approach?</p>
<p>Do I need to use MistralForSequenceClassification?</p>
<p>Many thanks.</p>
","large-language-models"
"43248","Why doesn't CLIP use a pretrained large language model as the text encoder?","2023-12-28 20:01:15","","1","43","<neural-networks><deep-learning><large-language-models>","<p>CLIP: <a href=""https://openai.com/research/clip"" rel=""nofollow noreferrer"">https://openai.com/research/clip</a></p>
<p>They use a small text encoder and suggest that the simpler the model the better. Is there any reason why that is? Has anyone tried using a pretrained LLM as the text encoder and then fine tuning? I would assume the LLM would learn a lot better representation of the meaning of the text.</p>
","large-language-models"
"43225","Do LLMs based on a diffusion model (as opposed to an autoregressive model) exist?","2023-12-25 17:59:34","43226","3","2538","<large-language-models><diffusion-models>","<p>Is there a such thing (described in title), at least in research papers (not actual models)?</p>
<p>So far all LLMs that I know are autoregressive models.</p>
","large-language-models"
"43222","Do AI-based code-generators guarantee correct output?","2023-12-25 15:47:51","","3","2946","<natural-language-processing><chatgpt><large-language-models>","<p>I haven't spent much time looking at AI-based code-generators. What mechanism is used to generate code and how is it different than standard NLP?</p>
","large-language-models"
"43201","which the effective ranks of embedding matrices in Large LMs?","2023-12-22 13:48:01","","0","23","<large-language-models><embeddings>","<p>The hidden size of current LLMs is well in the 8K and above, and I was wondering, has the rank of the embedding matrix, and the rank of the transpose of the output matrix, been increasing with the size, or is it restricted to an smaller subspace, or growing more slowly?</p>
<p>If it were growing more slowly that could mean that the &quot;semantic vector&quot; uses the rest of the hidden size and it could be useful to separate both parts.</p>
<p>Is there some measure or lore knowledge about this?</p>
","large-language-models"
"43198","Why are text models instructed in second person perspective?","2023-12-21 21:24:34","","2","35","<large-language-models>","<p>Many text model system prompts begin with &quot;You are an AI assistant. You are helpful. You …. [user input] [model output]&quot;, but looking at what might have been in the training material, I would expect other perspectives to be more useful, such as</p>
<ul>
<li>&quot;I am an expert. Ask me your questions. [user input] [model output]&quot;</li>
<li>&quot;The following is a transcript of an expert explaining a topic to an interested user: [user input] [model output]&quot;</li>
<li>&quot;Top list of the best answers to [user input]: 1) [model output]&quot;</li>
</ul>
<p>Or similar prompts that (possibly) match high quality answers in a large uncurated corpus of crawled text.</p>
<p>The &quot;You are ... You do ...&quot; prompts seem to imply that they are not processed in the way one would expect a transformer text generation model to work, but would require some sort of self-awareness. Why and how do they work?</p>
","large-language-models"
"43197","Can the methods of FunSearch that have been applied to the cap set problem be also applied to the enumeration of maximal antichains of sets?","2023-12-21 20:45:15","","0","65","<large-language-models><deepmind><combinatorics>","<p>Today on a seminar we discussed with colleagues recent impressive <a href=""https://www.nature.com/articles/s41586-023-06924-6"" rel=""nofollow noreferrer"">breakthrough</a> in the <a href=""https://en.wikipedia.org/wiki/Cap_set"" rel=""nofollow noreferrer"">cap set problem</a> using Google DeepMind's <a href=""https://github.com/google-deepmind/funsearch?tab=readme-ov-file"" rel=""nofollow noreferrer"">FunSearch</a>. The following specific question has been raised. Among several very similar problems in extremal combinatorics, let us consider the one of enumerating maximal antichains in the powersets of finite sets (including finding good upper or lower bounds for their number).</p>
<p>The question: is it feasible to readily apply the methods of FunSearch to this problem?</p>
<p>About the problem in some more detail. The sequence <span class=""math-container"">$a(n)$</span> in question is <a href=""https://oeis.org/A326358"" rel=""nofollow noreferrer"">A326358</a>, it starts with</p>
<p>1, 2, 3, 7, 29, 376, 31746, 123805914</p>
<p>One possible definition is this: for each <span class=""math-container"">$n=0,1,2,3,...$</span> consider the following graph <span class=""math-container"">$G_n$</span>. It has <span class=""math-container"">$2^n$</span> vertices, identified with all possible subsets of <span class=""math-container"">$\{1,...,n\}$</span>. Two such subsets <span class=""math-container"">$S$</span> and <span class=""math-container"">$T$</span> are joined with an edge if neither <span class=""math-container"">$S\subseteq T$</span> nor <span class=""math-container"">$T\subseteq S$</span> holds. Then <span class=""math-container"">$a(n)$</span> is the number of maximal cliques (complete subgraphs) in the graph <span class=""math-container"">$G_n$</span>.</p>
<p>For example (see the above link <a href=""https://oeis.org/A326358"" rel=""nofollow noreferrer"">A326358</a>), <span class=""math-container"">$a(3)=7$</span> since there are exactly <span class=""math-container"">$7$</span> maximal cliques in <span class=""math-container"">$G_3$</span>. Namely, these maximal cliques are:
<span class=""math-container"">$$
\begin{aligned}
&amp;\{\varnothing\}\\
&amp;\{\{1,2,3\}\}\\
&amp;\{\{1\},\{2,3\}\}\\
&amp;\{\{2\},\{1,3\}\}\\
&amp;\{\{3\},\{1,2\}\}\\
&amp;\{\{1\},\{2\},\{3\}\}\\
&amp;\{\{1,2\},\{1,3\},\{2,3\}\}
\end{aligned}  
$$</span></p>
<p>The above OEIS page contains <span class=""math-container"">$a(n)$</span> for <span class=""math-container"">$n&lt;8$</span>. I don't know whether anybody has computed <span class=""math-container"">$a(8)$</span>. I have supplied <span class=""math-container"">$a(6)$</span> and <span class=""math-container"">$a(7)$</span> using the <a href=""https://www.gap-system.org/"" rel=""nofollow noreferrer"">GAP</a> package <a href=""https://www.gap-system.org/Packages/grape.html"" rel=""nofollow noreferrer"">GRAPE</a> which works by taking into account any symmetries present in the graph. Note that the graphs <span class=""math-container"">$G_n$</span> are indeed highly symmetric.</p>
<p>So, can the LLM methods of FunSearch give some further progress here?</p>
","large-language-models"
"43153","How can a LLM optimise on it's own processing speed","2023-12-16 07:15:41","","0","124","<large-language-models><performance><meta-learning>","<p>Is it possible to give a large-language-model access to the time it takes to give it's answers and ask it to optimize on that?</p>
<p>An example prompt would be:</p>
<blockquote>
<p>From now on, please measure the time it takes for you to give me an
answer and optimize that time so that it stays under 500 milliseconds.
It is alright if you give me incorrect information because of this
constraint.</p>
</blockquote>
<p>We can assume that the human brain optimizes for processing speed to lower it's energy usage. How can this be applied to a LLM?</p>
","large-language-models"
"43147","Why is there a ""reference free"" option in DPO (Direct Preference Optimization)'s loss function?","2023-12-16 00:48:48","","0","280","<neural-networks><reinforcement-learning><natural-language-processing><large-language-models><loss>","<p>There is a <a href=""https://github.com/huggingface/trl/blob/d708ec272f292ded00a4d0f14b5bd214741b87f3/trl/trainer/dpo_trainer.py#L762"" rel=""nofollow noreferrer"">reference_free</a> parameter in <a href=""https://github.com/huggingface/trl"" rel=""nofollow noreferrer"">trl</a>'s loss function implementation of DPO, while <a href=""https://arxiv.org/abs/2305.18290"" rel=""nofollow noreferrer"">the original DPO paper</a> does not mention the concept of &quot;reference free&quot;.</p>
<p>In trl's implementation:</p>
<pre><code>    pi_logratios = policy_chosen_logps - policy_rejected_logps
    if reference_free:
        ref_logratios = 0
    else:
        ref_logratios = reference_chosen_logps - reference_rejected_logps

    logits = pi_logratios - ref_logratios
</code></pre>
<p>When <code>reference_free</code> is <code>True</code>, the loss is calculated as cross entropy between <code>policy_chosen_logps</code> and <code>policy_rejected_logps</code>, while <code>reference_chosen_logps</code> and <code>reference_rejected_logps</code> are not in use.</p>
<p>What are the pros and cons of the &quot;reference free&quot; DPO?</p>
","large-language-models"
"43101","Will open LLM be subject to AI act / considered High Risk?","2023-12-11 20:22:54","","1","101","<large-language-models><legal>","<p>As I was reading the AI act I was wondering how it will apply to current open LLM (say Mixtral of Mistal AI).</p>
<p>It seems that there are some specific arguments regarding open LLM that would put them outside of the AI act scope:</p>
<ul>
<li>They are free (are they still &quot;put on the market&quot; ?)</li>
<li>They are tools (not an AI system with a usage), it's there reusage in an AI system that would</li>
</ul>
<p>Was the topic of LLM covered somewhere that I am missing ?
Bonus question: given their prevalence and potential impact, will they be considered high risk ? (or its their reusage ?)</p>
","large-language-models"
"43075","How does Chat GPT encode a question?","2023-12-11 02:34:48","","0","755","<deep-learning><transformer><chatgpt><large-language-models><encoder-decoder>","<p>Chat GPT is based on a decoder-only Transformer so it does not have an encoder. Given that, how is a user's question passed as input to Chat GPT's decoder? In a regular encoder-decoder architecture, the final embeddings of the encoder are passed to the decoder along with the  token. Then, the decoder auto-regressively outputs tokens. How would this work in a decoder-only architecture?</p>
<p>Let's say I have the following question: &quot;How many countries are there in the world?&quot; and its token form is [3, 5, 8, 2, 10, 4, 1, 6, 7]. How will the decoder take in the input?</p>
<p>In an encoder-decoder architecture, I would just pass the  token to the decoder which would, based on the encoder embedding, auto-regressively output the next token in the sequence to form an answer.</p>
<p>In the decoder-only architecture, how will that work? If I pass the first token of the question (token &quot;3&quot;) to the decoder, it will output what it thinks is the most likely token after &quot;3&quot; but it will have no context... So how is the context taken into account given we are not encoding it?</p>
<p>I am aware of <a href=""https://ai.stackexchange.com/questions/40179/how-does-the-decoder-only-transformer-architecture-work"">this post</a> which is similar but I would more specifically want to know how we go from a decoder-only model that inputs the most likely word <em>in general</em>, to a model that outputs a whole answer based on a specific question. Put differently, how does a problem which is inherently a sequence to sequence problem (question answering) get solved using an autoregressive model instead of a seq2seq model?</p>
<p>One potential answer I have been looking into is training fine-tuning but I am still unsure how a model that out puts a single token x[n+1] = f(x[n],...,x[1]) can be fine-tuned based on a training set of two sequences (one question and one answer per record).</p>
","large-language-models"
"43047","Approaching construction of model that interprets financial reports","2023-12-07 21:21:02","","1","76","<natural-language-processing><large-language-models><embeddings>","<p>I want to train a model to be able to interpret financial reports (from a company). Basically, <strong>I want to be able to extract relevant information without needing to read through hundreds of pages of text</strong>. I know I will need to use a database that can store vectors for the embeddings of the texts. However, I have some questions:</p>
<ul>
<li>Assuming the texts are hundreds of pages long and, therefore, they go over a lot of stuff, what vectorization technique can be used?</li>
<li>I assume I will have to obtain a homogeneous training set. How do I do this without reading all the texts if I still don't have the final model (classification??)?</li>
<li>Let's say I have the final model. How do I &quot;query&quot; the things I want to know? In other words, would I have to input some text and then the model has to be able to interpret that as a query?</li>
</ul>
<p>Note: I apologize in advance if im not being too descriptive, I currently don't have so much information about the details of the data and other things, I only have a general sketch about the idea of the project.</p>
","large-language-models"
"43045","Largest fully open source LLMs including training","2023-12-07 16:13:55","","1","55","<training><large-language-models><training-datasets><open-source>","<p>It seems that most of the open-sourced large language models (LLMs) like Llama 2 had the model released but not the exact training procedure and training data-sources (exact data revisions) so that one could fully re-create (re-train) the model from scratch.</p>
<p>Therefore I ask what are the most notable (=largest?) open source LLMs that are really 100% open source from the beginning to the end? I mean projects where exact training data revisions (e.g. exact Common Crawl or Wikipedia dump revisions etc.) and exact training/tuning procedures (probably including code) have been disclosed so that the model can be independently verified and one can learn about the procedures in general.</p>
","large-language-models"
"43041","What is the current limit for LLMs regarding the size of the context that can be fed to them?","2023-12-07 11:32:29","","1","72","<large-language-models><gpt>","<p>Is there a limitation in current large language models (LLMs) in terms of practical processing time or memory resources when it comes to digesting the context provided by users? What I mean regarding context is the data fed to LLMs post-training, like feeding a selection of books about a certain topic.</p>
","large-language-models"
"43038","Is beam search the actual obstacle that prevents GPT-style models from doing sophisticated math reasoning?","2023-12-07 05:10:47","","4","249","<natural-language-processing><large-language-models><language-model><automated-theorem-proving><automated-reasoning>","<p>This is a rather soft question. Some people believe that GPT-style models can eventually solve very complex math problems if the models are large enough, but I'm skeptical about this. Suppose the GPT model is indeed large enough and can make very accurate predictions about the probabilities of the next token. Now we let it solve an extremely hard math problem, e.g. proving the Riemann Hypothesis. Proof length is not an obstacle, since the model can generate lots of tokens and therefore gains a lot of computational resources.</p>
<p>I think the most serious obstacle is how to generate the most likely <em>proof</em>, instead of the most likely <em>next token</em>. The bad thing is, that a proof might seem very plausible in the beginning, but may look more and more ridiculous as more tokens are generated. So, greedy decoding does not necessarily work; that's why people came up with beam search. But, beam search is also kind of greedy, and it also does not guarantee to produce the global optimal proof.</p>
<p>There may be extreme cases in which the optimal proof is among the top k+1 candidates but our beam size is k so it's not adopted. Therefore, the problem degenerates to a classic computational complexity problem: how on earth can we find the optimal generated sequence? This seems to be rather irrelevant to LLMs.</p>
<p>This problem is clearly in NP since its corresponding decision problem is: <em>given a scheme to generate the next token probabilities, can we generate a sequence with length as most l and log-likelihood at least p?</em> This decision problem can be verified in polynomial time by providing a satisfying sequence. I suspect this problem is NP-complete but I don't know how to prove it yet.</p>
<p>So, I conjecture that, unless P=NP, we can't use GPT-style LLMs to solve really hard math problems. Do you think so?</p>
","large-language-models"
"43021","Are there strictly deterministic LLMs?","2023-12-06 15:34:23","","5","1738","<natural-language-processing><large-language-models><model-request>","<p>LLMs are understood to generate non-deterministic outputs.</p>
<p>Are there LLMs out there that are capable to producing deterministic outputs for any given input given fixed parameters (like e.g temperature)?</p>
<p>I heard that llama.cpp - if run on a CPU instead of a GPU appears to generate deterministic outputs.</p>
","large-language-models"
"42990","Intentionally corrupting LLM weights (lobotomy)","2023-12-03 19:15:06","","0","132","<chatgpt><large-language-models><intelligence-testing><benchmarks><gpt-4>","<p>It is largely unknown how LLMs work inside. Has anyone scientifically tried to corrupt (open source) model's weights in an organized manner to maybe detect which parts of the model are doing what or are more or less important?</p>
<p>I mean something like setting the weights to zeros/random numbers in random (% of all weights) or organized (whole areas/layers etc.) manner and benchmarking the LLM to see how the results change.</p>
<p>Related <a href=""https://www.reddit.com/r/LocalLLaMA/comments/13vx1e5/llm_lobotomy_predictions/"" rel=""nofollow noreferrer"">reddit</a> post.</p>
","large-language-models"
"42986","Masking during Instruction Tuning for LLM finetuning","2023-12-03 13:14:04","","0","430","<natural-language-processing><transformer><large-language-models>","<p>I'm currently trying to learn more about LLMs particularly generative decoder only models such as the GPT family of models. I do have one question about masking though.</p>
<p>For me the way masking is performed left-to-right makes sense during pre-training but it is weird to me during instruction fine-tuning when dealing with the input prompt. I therefore have two questions:</p>
<ul>
<li>Is the input prompt masked in the same way as the text during pre-training?</li>
<li>And if so why is it done in this way and have there been studies on it</li>
</ul>
","large-language-models"
"42933","Could an analysis of GPT4's WAIS score be published?","2023-11-27 21:07:54","","0","22","<neural-networks><machine-learning><chatgpt><large-language-models><gpt>","<p>Similar to this: <a href=""https://www.scientificamerican.com/article/i-gave-chatgpt-an-iq-test-heres-what-i-discovered/"" rel=""nofollow noreferrer"">https://www.scientificamerican.com/article/i-gave-chatgpt-an-iq-test-heres-what-i-discovered/</a></p>
<p>But more detailed and in depth (subtest breakdown, including image analysis, etc.), WAIS-IV not WAIS-III, better methodology, etc.</p>
<p>Would it be a more interesting paper if it compared overall test scores and subtest scores to other LLMs, like 3.5, LLaMA, Mistral, etc.?</p>
<p>I know it's out of the box so I'm wondering if this is publishable in some journal. It seems that the WAIS interesting as there is both good human benchmark data, and it's never been on the internet, so there is no leakage.</p>
","large-language-models"
"42923","How LLM keeps the context of a chat/thread","2023-11-26 17:41:52","","1","1407","<open-ai><chatgpt><large-language-models>","<p><strong>How an LLM keeps the context</strong> (what has already been entered by the user) <strong>of a chat/thread?</strong></p>
<p>For reference, in <a href=""https://chat.openai.com/"" rel=""nofollow noreferrer"">chat.openai.com</a>, for each chat we create (or a <a href=""https://platform.openai.com/docs/assistants/how-it-works/managing-threads-and-messages"" rel=""nofollow noreferrer"">Thread</a> according to their API), the LLM remembers what we have already input to the model, when answering a new question.</p>
<p>I did some reading on the topic and found below possible ways:</p>
<ol>
<li>change the weights accordingly: but this seems not-practical for LLM given their size (even changing weights of the last layer seems an over-kill)</li>
<li>output a context vector at each inference and re-use it for the next inference: this seems more likely. but I am not sure exactly how to do it.</li>
</ol>
<p>It would be great if someone can help me with this.</p>
<p>Thanks.</p>
","large-language-models"
"42870","Specifying multiple sources of RAG in different formats","2023-11-21 00:52:02","","0","132","<large-language-models><information-retrieval>","<p>I am considering whether to do RAG or Fine tuning for LLM to answer questions for clients in a small business</p>
<p>The small business is a car rental company that gets</p>
<ol>
<li>new clients call up about certain cars in the fleet to discuss renting them for dates X to Y, ask for price,</li>
<li>existing clients/renters calling to coordinate car pickup, drop-off, finalise contract, insurance etc</li>
</ol>
<p>The company has data for last 5 years where a customer care team has been fielding these questions, and answering questions based on a set of documents; but also checking the a rota /time table/database about which cars are available, which cars have problems and are out for repair, and consulting documentation about pros/cons of each car</p>
<p>The current customer care also discuss questions one does not know with others in the team in a free flowing slack workspace with around 10 different slack channels (say one for &quot;Repairs&quot;, one for &quot;insurance&quot; , one for &quot;security deopsit&quot; etc - there are various slack channels where relevant conversations happen - but these are free flow, they dont always conform to question and answers as these are internal to the team)</p>
<p>I want to explore how to put all the data  I have</p>
<ol>
<li>Slack archives, perhaps i can dump each channel history for last 5 years as text dump</li>
<li>Conversation history with clients(where each client emails us and we have threads of email conversation back and forth). These conversations are in a CRM and have rich-data/tags like each conversation is tagged with model of car, dates, price paid, customer satisfaction rating etc</li>
</ol>
<p>How can I feed all of these to a LLM, and also give it the rota/state of the business for each day - in terms of which cars are avilable are current available and which are booked till what date</p>
<p>and, use it to answer questiosn from new clients?</p>
","large-language-models"
"42706","Requirements for AI to be able to read a book and accurately recall the facts?","2023-11-08 00:17:33","","0","32","<large-language-models><knowledge-representation>","<p>As a fun random project I've been working on is learning languages and making cheatsheets for them, dictionaries, etc.. I would love to encode the grammar rules which have been captured in books written in natural language (with some tables and diagrams in the books), across many different languages and writing systems. It is very tedious and time consuming to go into a book written by some grammar master, learn and grok each grammar rule well enough to turn it into structured JSON-like data, and then capture each one like that.</p>
<p><a href=""https://rads.stackoverflow.com/amzn/click/com/8171105890"" rel=""nofollow noreferrer"" rel=""nofollow noreferrer"">The Astadhyayi of Panini</a> is over 1500+ pages (in translation), which is a Sanskrit grammar. I met someone who said they've spent the past few years converting that Sanskrit grammar into code (maybe 3-5 years), and they are only a fraction of the way through. Multiply this by 100-1000+ languages and you are going to be in for a thousand+ human years to capture the grammar data into structured form.</p>
<p>What is required for AI to be able to solve this problem: read a book (PDF or otherwise), learn the material just in that one book, well enough to abstract out each casually defined grammar rule into structured JSON? Are LLMs even close to being able to do this? Predicting next token seems a far cry from being able to represent and learn explicit knowledge like this. But I don't have enough intuition for determining how far AI is from doing this. Where are we at, and what are the unsolved problems or missing pieces we must address before getting to this level?</p>
<p><em>As a sidenote, my experience with ChatGPT trying to have it teach me about languages, is it gets 90% of the specific facts wrong, either entirely wrong or slightly wrong. Maybe it's just me. But maybe we need a different approach to reading a book and recalling the facts. I would just like to know at a high level what that might look like, if we have any sense yet.</em></p>
<p><em>Side-side note: I write this after seeing so many startups subsumed by OpenAI's latest release, and I'm wondering if I should even spend any time on my task or just wait it out haha, wait for AI to be able to solve it. I am not making a startup, just trying to learn several languages, but still.</em></p>
","large-language-models"
"42642","Why are some LLMs trained on both CommonCrawl and Wikipedia/StackExchange?","2023-11-02 22:28:33","","2","552","<large-language-models><training-datasets>","<p>Some LLMs are trained on both CommonCrawl and Wikipedia/StackExchange. Why? Does CommonCrawl already contain Wikipedia/StackExchange?</p>
<p>E.g., from the <a href=""https://arxiv.org/pdf/2302.13971.pdf"" rel=""nofollow noreferrer"">LLaMa 1 paper</a>:</p>
<p><a href=""https://i.sstatic.net/pPFVa.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/pPFVa.png"" alt=""enter image description here"" /></a></p>
<p>and from <a href=""https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T"" rel=""nofollow noreferrer"">https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T</a>:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Dataset</th>
<th>Token Count</th>
</tr>
</thead>
<tbody>
<tr>
<td>Commoncrawl</td>
<td>878 Billion</td>
</tr>
<tr>
<td>C4</td>
<td>175 Billion</td>
</tr>
<tr>
<td>GitHub</td>
<td>59 Billion</td>
</tr>
<tr>
<td>Books</td>
<td>26 Billion</td>
</tr>
<tr>
<td>ArXiv</td>
<td>28 Billion</td>
</tr>
<tr>
<td>Wikipedia</td>
<td>24 Billion</td>
</tr>
<tr>
<td>StackExchange</td>
<td>20 Billion</td>
</tr>
<tr>
<td>Total</td>
<td>1.2 Trillion</td>
</tr>
</tbody>
</table>
</div>
<p>Looking at <a href=""https://commoncrawl.github.io/cc-crawl-statistics/plots/domains"" rel=""nofollow noreferrer"">https://commoncrawl.github.io/cc-crawl-statistics/plots/domains</a>, it seems that CommonCrawl includes the Wikipedia and StackExchange domains. But maybe it's incomplete?</p>
<p><a href=""https://i.sstatic.net/O9A8t.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/O9A8t.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.sstatic.net/qDE2G.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/qDE2G.png"" alt=""enter image description here"" /></a></p>
","large-language-models"
"42607","From Text-To-Speech to LLMs: Providing ""writing style""","2023-10-31 13:24:07","","0","74","<training><large-language-models><features>","<p>I've just recently learned about Text-To-Speech models and how they are trained. Unlike LLMs, to a provided pair (text, speech), a feature vector <span class=""math-container"">${f}$</span>, that was generated by more speech of that speaker and tries to subsume the general voice characteristics, is added. I.e., for a speaker <span class=""math-container"">${S}$</span>, triplets <span class=""math-container"">$({text}_i, {speech}_i, {f}_S)$</span> are used in training (where <span class=""math-container"">${speech}_i$</span> are speeches from speaker <span class=""math-container"">${S}$</span>). Why? Because speech, compared to text, is much more high-detail-low-content.</p>
<p>I then wondered about training of LLMs. Texts of various sources (scientific, forums, joke books, ...) are used; all trying to fit into one and make one &quot;writer&quot;. Hallucinations of LLMs seem to magnify when they are already &quot;on the wrong path&quot;, which makes perfect sense: In an article full of nonsense, jokes, wrong data, more of exactly that will follow; so this statistical model of a knowledge base will proceed by giving more of that.</p>
<p>Of course, implicitly, LLMs learn <em>how</em> to write; I may just include in my prompt that &quot;You are a child. Please talk to me as a child.&quot; and it will do so. But it was never particularly trained on that.</p>
<p>So the question is: Has there been any work on providing the source, theme, genre, ... of texts provided in training (which should be reasonably possible to add to the training data) as a separate &quot;feature&quot;, so as to when prompting that LLM and providing the field of applications, it would generate better results on fewer training data and housr due to less necessary generalization?</p>
<p>(It should be possible to just try this on one's own, e.g., with <a href=""https://arxiv.org/pdf/2106.09685v2.pdf"" rel=""nofollow noreferrer"">LoRA</a>, but my expertise and commitment is a bit too low for that, yet.)</p>
","large-language-models"
"42605","LLM Hallucinations on In-Context Data","2023-10-31 10:57:37","42611","2","168","<large-language-models><prompt><benchmarks>","<p>I know of some benchmarks that LLMs do undergo, but I am no expert whatsoever. I think what I am wondering about is closest to <a href=""https://paperswithcode.com/dataset/truthfulqa"" rel=""nofollow noreferrer"">TruthfulQA</a>.
The question came up when I heard of combining company data with LLMs by providing the internal, sensitive data in prompts à la <a href=""https://research.ibm.com/blog/retrieval-augmented-generation-RAG"" rel=""nofollow noreferrer"">RAG</a> (of course you do not want to fine-tune the model every time, because the data may change daily or even hourly, but instead supply it with the data in prompts).</p>
<p>So the question is:
How sure can one be about the correctness of the models outputs on these kinds of prompts? I do not want it to hallucinate here whatsoever; that would undermine the whole approach of using LLMs in this context. Are there benchmarks <em>specifically</em> made for this task (i.e., providing lots of information in context and then asking for things appearing in this context)?</p>
<p>Thanks a lot!</p>
","large-language-models"
"42490","How can LLMs understand and perform meta tasks? (e.g. summarization)","2023-10-19 10:54:17","42520","1","89","<large-language-models><text-summarization><prompt>","<p>I don't ask how to make it summarize xy but if it is known how a &quot;LLM&quot; understands and performs this meta task at all.</p>
<p>The same is true for prompts like &quot;Be brief&quot; or &quot;Explain in a few sentences&quot;. Since this works, the LLM must actually have an understanding of text length.</p>
","large-language-models"
"42481","How does VRAM usage change when running prompts async through LLMs?","2023-10-17 19:44:05","","0","39","<large-language-models><gpu>","<p>If my LLM uses X GB of VRAM when processing prompts sequentially, does it use 2X GB if I run 2 prompts at once using async support? e.g. using Langchains <code>await chain.arun()</code></p>
<p>If I ask it to run 10 prompts using async, does that require 10x more VRAM?</p>
<p>How does VRAM scale the more prompts I pass at once?</p>
","large-language-models"
"42393","LLM for Postgres","2023-10-10 07:55:27","","1","172","<natural-language-processing><open-ai><large-language-models>","<p>I have a postgres database with 200+ Tables. Each table contains information about my supply inventory. It also contains columns which are JSON and there are nested JSON as well. There are relationships as well and some relationships are based on the values in the JSON Column in one table pointing to another Column in Another Table.</p>
<p>I use to do ETL to flat it, then query and generate report.</p>
<p>Recently I tried to solve this problem by LLM, I have installed and tried LLama-2 model on one of my EC2 machine. But I am not able to achieve the result which I thought. I wanted to chat with my database.</p>
<p>I have following questions in order to complete this entire activity. It will be great if you people can guide me for my next steps and other information.</p>
<ul>
<li><p>One of the major problem is I face token limit error.</p>
<ul>
<li>I understand that there is token limit for each LLM, but I am still wondering or not able to
understand that do I need to really or explicitly create Embedding for my data or do I need
to normalize or de-normalize data or something else I need to do.</li>
</ul>
</li>
<li><p>The existing solution which I have created or working on is working, but it is not consistent</p>
<ul>
<li>How can I make my solution more consitent</li>
</ul>
</li>
<li><p>The existing solution which I have created gives result sometime correctly and sometimes incorrect or it not able to understand data</p>
<ul>
<li>How can I make model understand my data for complex JSON or other columns, for joins etc</li>
</ul>
</li>
<li><p>Increasing Datasize</p>
<ul>
<li>How can I manage my model or solution to cater the increasing datasize in near future</li>
</ul>
</li>
<li><p>Is there something to do with the Model</p>
<ul>
<li>Is there any other Model which I should try or use which is suitable for my usecase</li>
</ul>
</li>
</ul>
<p>These are the some high level queries which I have.
It will be great if someone can help me out understand the details here.</p>
","large-language-models"
"42351","claude 2 doesn't know basic math?","2023-10-06 13:20:51","","0","70","<machine-learning><math><open-ai><chatgpt><large-language-models>","<p>Sometimes, when I see answers like this from large language models, it makes me feel disgusted:</p>
<p>Me: Does Voyager 1 have enough velocity to escape the solar system without using Jupiter's gravity assist?</p>
<p>claude: &quot;</p>
<ul>
<li>Escape velocity at 1 AU from the Sun is 151,560 km/h.</li>
<li>Voyager 1's pre-Jupiter flyby velocity was 169,200 km/h.</li>
</ul>
<p>Therefore, Voyager 1 absolutely <strong>did NOT have enough velocity</strong> at 1 AU to escape the solar             system without the Jupiter gravity assist. <strong>Its velocity of 169,200 km/h at 1 AU was below the      solar escape velocity of 151,560 km/h.</strong>&quot;</p>
<hr />
<p><a href=""https://i.sstatic.net/GpxB8.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/GpxB8.png"" alt=""stupid answer by claude 2"" /></a></p>
<p>So my question is:
why can this large language model sometimes solve complex problems but fail to answer basic ones? (The irony is that it became not free before it was ready)</p>
","large-language-models"
"42319","How do I balance context and history when creating prompts for LLM's?","2023-10-04 13:38:47","","3","151","<neural-networks><image-recognition><large-language-models><language-model><prompt-design>","<p>A conversation through the OpenAI API looks something like this</p>
<pre><code>    messages=[
        {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;},
        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Who won the world series in 2020?&quot;},
        {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;The Los Angeles Dodgers won the World Series in 2020.&quot;},
        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Where was it played?&quot;}
    ]
</code></pre>
<p>When I call a vector database I'm going to get back related content in the order of similarity to the question.</p>
<p>Let's say I get back 10 chunks at around 500 characters each.</p>
<p>Some frameworks (i.e. llama-index) set the context in the &quot;system&quot; role.</p>
<p>So that would look something like</p>
<pre><code>    messages=[
        {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant. &quot;Context information is below.&quot;
    &quot;\n--------------------\n&quot;
    &quot;{context_str}&quot;
    &quot;\n--------------------\n&quot;},
        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Who won the world series in 2020?&quot;},
        {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;The Los Angeles Dodgers won the World Series in 2020.&quot;},
        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Where was it played?&quot;}
    ]
</code></pre>
<p>Where <code>context_string</code> will be replaced by the chunks retrieved from the similarity search.</p>
<p>Now for the history, I can keep adding &quot;user&quot;, &quot;assistant&quot; role pair say up to a maximum of 10.</p>
<p>What's a good strategy for making sure I don't overflow the data limit I can send to the LLM?</p>
<p>For example I have GPT4All 7b with a limit of 2000 tokens and another model has a limit of 32K tokens.</p>
<p>How do I calculate how much to use?</p>
","large-language-models"
"42266","Is ChatGPT a viable strategy for solving the P-NP Problem?","2023-09-30 17:16:12","","0","233","<natural-language-processing><large-language-models><automated-reasoning>","<p>According to ZDNet, it is an open question whether a transformer LLM like ChatGPT can facilitate the determination of a solution to the <a href=""https://en.wikipedia.org/wiki/P_versus_NP_problem"" rel=""nofollow noreferrer"">P-NP Problem</a>. (See <strong><a href=""https://www.zdnet.com/article/can-generative-ai-solve-computer-sciences-greatest-unsolved-problem/"" rel=""nofollow noreferrer"">Can generative AI solve computer science's greatest unsolved problem?</a> (ZDNet)</strong>) This would seem to be an attempt to mine statistical patterns of a corpus as a form of non-determinstic automated reasoning to collate a series of logical propositions that cohere to a deductively sound argument. So, the question arises is there any <em>a priori</em> justification for rejecting this particular form of generative AI as a strategy for developing a mathematical logical argument to determine an answer to the question of P-NP equivalency?</p>
","large-language-models"
"42265","How to train a seq2seq model to rephrase input text following given rules","2023-09-30 12:26:12","","0","69","<natural-language-processing><large-language-models><seq2seq>","<p>I want to train (fine-tune) a seq2seq model to perform the task of rephrasing input following these rules :</p>
<p>1- always follow the pattern &quot;Entity Verb Entity&quot;</p>
<p>2- only use simple sentences : never combine sentences</p>
<p>3- Don't replace existing words</p>
<p>4- Don't lose the overall meaning of the text or any information in it.</p>
<p>For example:</p>
<p>text = &quot;Project Risk Management includes the processes of conducting risk management planning, identification, analysis, response planning, response implementation, and monitoring risk on a project&quot;</p>
<p>Standardized Text = &quot;Project Risk Management conducts risk management planning. Project Risk Management conducts risk identification. Project Risk Management conducts risk analysis. Project Risk Management plans responses. Project Risk Management implements responses. Project Risk Management monitors risk on a project.&quot;</p>
<p>Using ChatGPT the results were very good, but I want to know if I can fine tune a model (BERT, T5, any LM) locally, what should be the data format for training such a model, evaluation metrics ?</p>
","large-language-models"
"42247","How to get Llama-2 Rotary Embeddings?","2023-09-29 07:46:08","","-1","691","<neural-networks><natural-language-processing><transformer><word-embedding><large-language-models>","<p>I want to get the Llama-2 rotary embeddings. I do <code>print(model)</code> and get the following output:
<a href=""https://i.sstatic.net/jFu2Z.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/jFu2Z.png"" alt=""enter image description here"" /></a></p>
<p>In the picture I highlight the rotary embeddings.</p>
<p>How can get the rotary embeddings and how can I interpret the output? What means 32x LLamaDecoderLayer and in its round brakets are four layer plus LlamaRotaryEmbeddings?</p>
<p>It's possible to get the embeddings as the first hidden-state <code>hidden_state[0]</code> and I want to know, which hidden-state represents the rotary embeddings.
Am I right, that there are several rotary embeddings?</p>
<p>Thanks in forward.</p>
<p>Best regards.</p>
","large-language-models"
"42238","What options do I have when performing RAG on similarly phrased chunks?","2023-09-28 11:02:00","","0","33","<large-language-models><prompt><prompt-design><information-retrieval>","<p>I'm building a RAG pipeline to extract real estate phrases from excel documents. These phrases are short (2-5 words) and are often phrased differently.</p>
<p>I've manually added in different phrases to each prompt to suggest the LLM to look for them.</p>
<p>What else could I do? What more powerful options do I have?</p>
<ul>
<li>Search for each term manually? -&gt; slow due to loads of calls</li>
<li>Embed the terms in the excel doc and similarity search the embedding of 'Total Income' against those in the excel doc?</li>
</ul>
<pre><code># Example prompt with 4 similar terms
income = f&quot;&quot;&quot;What is the Total Income? 

It may also be called: 
- Total Revenue
- Effective Gross Income
- Total Operating Receipts
- Total Operating Income

Or something similar

Return a signed number or nan. Do not write anything else.&quot;&quot;&quot;
&quot;&quot;&quot;
<span class=""math-container"">```</span>
</code></pre>
","large-language-models"
"42173","What is the policy model in RLHF for LLMs?","2023-09-22 18:37:55","","0","22","<neural-networks><transformer><large-language-models><rlhf><decoder>","<p>What is the policy model doing explicitly in an LLM with RLHF setup?</p>
<p>From my understanding, LLMs generate in a way that is no different from any of their predecessors: beam search decoding, potentially with some sampling.</p>
<p>Does this mean the policy model is specifying how to generate the next token?</p>
","large-language-models"
"42157","How can I teach a book to an LLM?","2023-09-21 16:43:19","","2","3310","<embeddings><large-language-models><fine-tuning>","<p>I am trying to find out how I can teach the content of a whole, multiple hundert pages book to an LLM so that it &quot;knows&quot; all details and can be queried, give summaries etc. The book is one consistent story, private and has never been published. I thought training LLM on a long book would be a common use case, but I found surprisingly little information about this.<p>
Most use cases these days regarding own content seems to be stuff like &quot;chat with your documents&quot; or such. But this seems much easier due to context length and the lack of coherence between documents. <br>
I am not an ML expert but know the basics of embeddings and fine tunings. Is either of these approaches better suited? How could a raw book be turned into a proper training data set for fine tuning a model? This could not be done manually, as the length is almost a million words. Or could it work &quot;simply&quot; by splitting the text into chunks and embedding them?</p>
","large-language-models"
"42140","Why do current language models no longer generate to long or short texts?","2023-09-19 21:58:45","","3","165","<natural-language-processing><chatgpt><large-language-models><language-model><natural-language-generation>","<p>One of the biggest strengths of ChatGPT is that it generates fitting text with respect to the input query. It usually stays on topic, anwers the question completely and especially does not start talking gibberish or repeating itself.</p>
<p>This behaviour is different when comparing this to older LLMs. For example: GPT2 would usually only stop generating text when it hit the token limit or a predefined stop sequence. Also, it had a much bigger problem with giving repeating answers. Newer models (especially instruction tuned ones) do not suffer from this problems (e.g. llama 2).</p>
<p><strong>So I have 2 questions: What mechanisms/techniques are used in current language models such that...</strong></p>
<ol>
<li>...<strong>the models know when to stop generating text.</strong></li>
<li>...<strong>the models do not repeat themselfes and stay on topic.</strong></li>
</ol>
<p>I suspect it might have alot to do with instruction tuning but I am happy to hear from you.</p>
","large-language-models"
"42131","Tips and tricks when training a very large language model?","2023-09-19 15:54:33","","1","566","<neural-networks><training><chatgpt><architecture><large-language-models>","<p>Have never trained a (very) large language model, so I am wondering if the process is the same as training a (regular) language model, i.e. you prepare the data, set up the architecture, hyperparameters, loss function to minimize perplexity and predicting the next word, and then do gradient descent over the giant dataset. Or if there are any special gotchas or tricks you must do when training it.
I know there's at least one involving the training dynamics:</p>
<ol>
<li>training dynamics: most LLMs stop seeing performance improvement even before a single epoch is finished.</li>
</ol>
<p>I am wondering if there are any others</p>
","large-language-models"
"42127","How to prompt an LLM for a real variable list length?","2023-09-19 11:58:20","","0","233","<open-ai><large-language-models><prompt-design>","<p>I struggle to get a variable list out of an LLM. Even with a prompt like</p>
<pre><code>short list of the most important steps to build a house, with a variable length between 5 and 15 entries
</code></pre>
<p>The resulting list has most of the time exactly 15 entries, sometimes exactly 10, but never eg. 6 or 8 also never 13. Why? How to overcome this?</p>
<p>I also tried prompts like <code>… maximum 15 entries</code>, however this also produces always exactly 15 entries, never less.</p>
<p>The topic of the lists can vary and I expect the ideal list will depend on the complexity of the topic, sometimes 6 entries will have all relevant information, but sometimes more will be needed. The LLM should fiddle this out by itself.</p>
<p>Is this a general LLM problem? I experienced this with Open AIs GPT-3.5.</p>
","large-language-models"
"42120","How successful are the state-of-the-art (2023) email filters really?","2023-09-18 17:05:19","","-2","97","<machine-learning><large-language-models>","<p>How successful are the state-of-the-art (2023) email filters really?</p>
<p>Some references claim that spam detection may reach high accuracy in test settings, but I've thought that email filtering should essentially be as large problem as filtering a whole language. Because one cannot guarantee anything about strings in emails. There will simply be too many possible combinations of strings.</p>
<p>Spam filters might work, but I doubt they can ever reach 100% accurate labeling. And if they don't, they cannot useful, because even losing few important message can be intolerable.</p>
","large-language-models"
"42050","Why do transformers compute the loss over the prompt?","2023-09-10 18:23:37","","3","1053","<training><large-language-models>","<p>When fine-tuning large language models, which are commonly decoder transformer architectures, sometimes we want to compute the training loss over the entire prompt, sometimes just the completion generated by the model.</p>
<p>For example, OpenAI fine-tuning API has the <code>prompt_loss_weight</code> parameter <a href=""https://platform.openai.com/docs/api-reference/fine-tunes/object#hyperparams-prompt_loss_weight"" rel=""nofollow noreferrer"">1</a></p>
<p>When training / fine-tuning with Huggingface transformers or Pytorch, it is common practice to set the prompt token labels to the magic number <code>-100</code> if they are ignored by the loss computation.</p>
<p>My question is: why is this a thing in the first place? What is it about the transformers that makes it compute the loss / predictions for the prompt as well?</p>
","large-language-models"
"41887","Does Embeddings and Vector Databases solve the need of having longer context windows?","2023-08-26 02:27:00","","0","163","<transformer><large-language-models>","<p>I am learning to use the OpenAI API to build LLM-based agents. I recently came across the concept of vector databases, which use embeddings to convert text into vectors and store them in a database for easy retrieval. This technique has been shown to be very useful for long-memory applications.</p>
<p>My question is whether it is necessary to have longer context windows in models that use the embedding + database technique. I know that there has been some research on expanding the context window of transformers, such as the <a href=""https://arxiv.org/abs/2304.11062"" rel=""nofollow noreferrer"">Scaling Transformer to 1M tokens and beyond with RMT</a> paper and <a href=""https://arxiv.org/abs/2307.02486"" rel=""nofollow noreferrer"">Microsoft's LongNet</a> architecture. At what point does increasing the context window lead to better performance than using a vector database approach? Are there any examples or experiments that demonstrate this?</p>
","large-language-models"
"41792","What causes my loss curve to consistently oscillate when training an LLM?","2023-08-16 15:48:08","","2","599","<deep-learning><training><data-preprocessing><large-language-models><training-datasets>","<p><a href=""https://i.sstatic.net/RckTe.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/RckTe.png"" alt=""loss curve"" /></a></p>
<p>Why is my loss curve consistently oscillating? Every 50 steps it jumps back up. I'm assumming there's a bug in my data, since I'm using <a href=""https://colab.research.google.com/drive/1BiQiw31DT7-cDp1-0ySXvvhzqomTdI-o?usp=sharing"" rel=""nofollow noreferrer"">this colab notebook</a> that shows a proper train/loss at the bottom. The only thing I changed was the dataset used. I'm using the <a href=""https://huggingface.co/ybelkada/falcon-7b-sharded-bf16"" rel=""nofollow noreferrer"">falcon-7b-sharded-bf16</a> huggingface model.</p>
","large-language-models"
"41779","Any research in ""probe-tuning"" of LLMs?","2023-08-15 21:16:34","","0","47","<natural-language-processing><supervised-learning><large-language-models><binary-classification><fine-tuning>","<p>Is there any research in &quot;probe-tuning&quot; of LLMs, i.e., tuning LLM's parameter weights such that a specific <a href=""https://arxiv.org/abs/1610.01644"" rel=""nofollow noreferrer"">probe (classifier)</a> is more reliably detecting certain markers throughout the context, such as grammatical errors, aggression, manipulation, certain political bias, etc.?</p>
<p>This is different from classical fine-tuning and RLHF. As well as classical fine-tuning, probe-tuning is a supervised ML method: it is based on human-annotated texts (contexts). However, probe-tuning should be more effective than classical fine-tuning for detecting many occurrences of a certain marker throughout the context. Probe-tuning doesn't train on LLM's own &quot;original rollouts&quot; at all, only on LLM's activations during the context pass through the LLM.</p>
<p>I imagine than before doing actual probe-tuning, first we should determine which probe in the LLM is most aligned to the training data (annotations) already, so that probe-tuning likely just attenuates some vaguely existing concept within the LLM.</p>
","large-language-models"
"41728","What will happen if to train an LLM on mathematical exersises?","2023-08-10 16:43:29","","3","1859","<math><large-language-models>","<p>What will happen if to train an LLM on taking integrals and solving equations? The process of mathematical education can be absolutely automated by a computer algebra system because the verification is easy.</p>
<p>Is it possible that LLM will gan the ability to take integrals and simplify expressions better than the computer algebra system itself?</p>
","large-language-models"
"41643","Optimal Quantity of Training Data for Fine-Tuning an LLM: Is Bigger Always Better?","2023-08-04 07:43:00","","0","348","<natural-language-processing><gpt><large-language-models><language-model><fine-tuning>","<p>I am currently working on fine-tuning an LLM for a specific task, and I am trying to determine the optimal size for my training dataset. Intuitively, one might think that the more data, the better. However, I am aware that in some contexts, this may lead to overfitting or other issues.</p>
<p>What is the general consensus on the optimal quantity of training data required for fine-tuning a Large Language Model? Is bigger always better?</p>
","large-language-models"
"41509","What is the difference betwen fine runing and rlhf for llm?","2023-07-28 00:42:45","","2","681","<large-language-models><fine-tuning><gpt-4><few-shot-learning><rlhf>","<p>I am confused about the difference betwen fine runing and rlhf for llm. When to use what? I know RLHF need to creating a reward model which at furst rates responses to align the responses to the  human preferences and afterward using this reward model to fine-tune.</p>
<p>But if thats the case, when fine-tuning is relevant anymore. When to use what?</p>
","large-language-models"
"41444","Can LLMs be used to discover new laws of logic?","2023-07-24 04:05:55","","2","134","<philosophy><large-language-models><logic>","<p>Can LLMs be used to discover new laws of logic?</p>
<p>Stephen Wolfram seems to claim this in <a href=""https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/#what-really-lets-chatgpt-work?"" rel=""nofollow noreferrer""><em>What Is ChatGPT Doing … and Why Does It Work?</em>, § &quot;What Really Lets ChatGPT Work?&quot;</a>:</p>
<blockquote>
<p>is there a general way to tell if a sentence is meaningful? There’s no traditional overall theory for that. But it’s something that one can think of ChatGPT as having implicitly “developed a theory for” after being trained with billions of (presumably meaningful) sentences from the web, etc.</p>
<p>What might this theory be like? Well, there’s one tiny corner that’s basically been known for two millennia, and that’s <a href=""https://writings.stephenwolfram.com/2018/11/logic-explainability-and-the-future-of-understanding/"" rel=""nofollow noreferrer"">logic</a>. And certainly in the syllogistic form in which Aristotle discovered it, logic is basically a way of saying that sentences that follow certain patterns are reasonable, while others are not. Thus, for example, it’s reasonable to say “All X are Y. This is not Y, so it’s not an X” (as in “All fishes are blue. This is not blue, so it’s not a fish.”). And just as one can somewhat whimsically imagine that Aristotle discovered syllogistic logic by going (“machine-learning-style”) through lots of examples of rhetoric, so too one can imagine that in the training of ChatGPT it will have been able to “discover syllogistic logic” by looking at lots of text on the web, etc. (And, yes, while one can therefore expect ChatGPT to produce text that contains “correct inferences” based on things like syllogistic logic, it’s a quite different story when it comes to more sophisticated formal logic—and I think one can expect it to fail here for the same kind of reasons it fails in parenthesis matching.)</p>
</blockquote>
","large-language-models"
"41379","Could hallucinations be the demise of the AI hype?","2023-07-20 15:01:26","41380","2","117","<generative-model><chatgpt><large-language-models>","<p>For quite some time now, I have been evaluating ChatGPT's capability to deliver accurate and helpful responses. While its performance is undeniably impressive, the issue of hallucinations poses a significant drawback to this otherwise capable model.</p>
<p>As I increased the complexity of my inputs during several sessions, my initial enthusiasm and excitement about the advancements in AI quickly waned due to the responses becoming riddled with hallucinations. This has seriously raised my concerns about the reliability and usefulness of chatGPT.</p>
<p>I have two questions:</p>
<ol>
<li>Is there a declining interest among people in LLMs and artificial intelligence due to the problem of hallucinations?</li>
<li>How are LLM companies addressing and combatting this concerning aspect of AI models in general?</li>
</ol>
","large-language-models"
"41352","Why does LLM inference cost scale in both input tokens and output tokens?","2023-07-19 02:55:02","41388","0","1798","<transformer><open-ai><embeddings><large-language-models><inference>","<h2>EDIT</h2>
<p>This question was flawed. See <a href=""https://ai.stackexchange.com/a/41388/68775"">my answer</a> with help from commenters.</p>
<hr />
<h2>Original question</h2>
<p>This question has been asked in other forums <a href=""https://community.openai.com/t/why-does-pricing-vary-by-input-tokens-instead-of-only-output-tokens/21833"" rel=""nofollow noreferrer"">[1]</a> <a href=""https://www.reddit.com/r/OpenAI/comments/11vvx66/whats_the_rationale_for_charging_for_input_tokens/"" rel=""nofollow noreferrer"">[2]</a> but I'm not sure I understand the claims, which are (EDIT: the following are based on my faulty assumption that pad tokens are added up to the maximum context window):</p>
<ol>
<li>Each forward pass takes less resources when more of the context window is padding.</li>
<li>Forward passes are run on the input tokens.</li>
<li>Forward passes with fewer non-pad input tokens are smaller tensor operations.</li>
</ol>
<p>Hypothesis 1 seems the most plausible to me from a performance engineering standpoint (sparse math, etc). Does it fall out naturally from just writing basic JAX code or would it require manual optimization (if so, what tricks can be used?)? There does <a href=""https://arxiv.org/abs/2210.03052"" rel=""nofollow noreferrer"">seem to be some research on this</a>.</p>
<p>Hypothesis 2 and 3 seem wrong based on my surface-level understanding of the Transformer architecture.</p>
<p>I've tried a few open-source LLMs locally and neither on those nor ChatGPT have I noticed any difference in latency based on how much text was in the context window. But I haven't done actual rigorous benchmarking yet.</p>
<p>The reason this is relevant is due to document lookup-based applications. Looking at the <a href=""https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb"" rel=""nofollow noreferrer"">OpenAI cookbook for Q&amp;A using embeddings</a>, I can see that:</p>
<ul>
<li>most of the token usage seems to come from the long Wikipedia article pasted into the context</li>
<li>the price per query is quite prohibitive to frequent usage:</li>
</ul>
<blockquote>
<p>For gpt-3.5-turbo using ~1,000 tokens per query, it costs ~0.002 per query, or ~500 queries per dollar (as of Apr 2023)
For gpt-4, again assuming ~1,000 tokens per query, it costs ~0.03 per query, or ~30 queries per dollar (as of Apr 2023)</p>
</blockquote>
<p>My prior intuition would have been that optimal usage of LLMs would be to keep the context filled with inexpensive text (e.g. from NN search and/or cheaper LMs) and to have the LLM generate terse responses. But the input token cost model changes the strategy, as it means that users need to be sparing about the size and quantity of documents that they paste into the context window.</p>
","large-language-models"
"41295","Fine-Tune Llama on main and auxiliary task","2023-07-15 19:29:23","","0","174","<deep-learning><natural-language-processing><large-language-models>","<p>I am trying to fine-tune Llama model on two task at the same time, using hugging face library:</p>
<p>Main task: Causal language model like the model was initially trained for
A classification task based on the whole input sequence (recommend an article). For this task I am getting as a reference the LlamaForCausalLM class, overwriting init and forward functions .
However, I want to combine the two tasks above into one process. The main problem is that language modelling is an iterative process were the loss is calculated for every new context token in the input sequence, while for the classification task the loss should only be calculated once.</p>
<p>How can I freeze the loss update on the classification task up and only calculated once the language modelling part has been completed. Is there any example you can recommend in order to combine a main LM task with an auxiliary classification task?</p>
<p>First question for me here, thanks everyone for your understanding.</p>
","large-language-models"
"41293","Open access Adept-like dataset? (LLM-to-computer-input)","2023-07-15 01:49:35","41307","-1","63","<large-language-models><training-datasets>","<p>Here's a demo for <a href=""https://www.adept.ai/blog/act-1"" rel=""nofollow noreferrer"">Adept ACT-1 for Transformers</a>. I don't doubt that one could create a demo video using zero-shot; actually I tested just now and the basic chat.openai.com interface was able to do some web browsing stuff on the first try if I just prompted it with a list of API methods (e.g. <code>el.click</code>) and streamed the DOM to it.</p>
<p>But I assume that, to take their product to the next level, they must be training or fine-tuning on some actual multimodal data, e.g. session recordings labeled with natural language? Am I wrong?</p>
<p>Are there any open-access datasets like this that could be used for open-source models? I'm actually interested in native (e.g. pixel + natural language to game input) more than browser.</p>
","large-language-models"
"41277","Why can't Lucene search be used to power LLM applications?","2023-07-13 21:14:35","","8","1397","<large-language-models>","<p>w.r.t. LLM applications using the RAG (retriever-augmented-generation) architecture, people have started taken it for granted that it will be powered by a vector database. e.g., see <a href=""https://a16z.com/2023/06/20/emerging-architectures-for-llm-applications/"" rel=""noreferrer"">this</a>:</p>
<blockquote>
<p>The most important piece of the preprocessing pipeline, from a systems standpoint, is the vector database.</p>
</blockquote>
<p>Why can't lucene index (full-text search) be used for the retriever? Is there any objective study that has been done comparing quality of results using full-text search vs. using a vector database?</p>
<p>As I was writing this, even lucene seems to have jumped on the vector bandwagon. see <a href=""https://www.apachecon.com/acna2022/slides/04_lucene_vector_search_sokolov.pdf"" rel=""noreferrer"">this</a></p>
","large-language-models"
"41262","How to Formulate a realiable ChatGPT Prompt for Sentiment Analysis of a Text, and show that it is reliable?","2023-07-13 02:29:02","","3","4339","<chatgpt><large-language-models><prompt><prompt-design>","<p>I have a dataset which consists of like.. 400000 sentences and I want give each sentence to ChatGPT so it classifies each sentence as <code>positive</code> or <code>negative</code>. My question is, where can I find a reliable / trusted prompt to do that? and provide evidence that the prompt I used gives reliable labels for the problem?</p>
<p>If I just create my own prompt like.. &quot;Please conduct a sentiment analysis on the following text and tell me if the sentiment expressed is positive or negative: [insert text here]&quot;</p>
<p>How can I convince someone that my prompt was good? This is what I mean :/ So later no one can criticize like &quot;Hey maybe you just used a bad prompt and ChatGPT could have performed better!&quot; :c</p>
<p>I hope my question is clear.</p>
","large-language-models"
"41249","What is considered the pre-fill, and what is considered the decoding phase in this process?","2023-07-12 11:35:18","","1","2283","<machine-learning><deep-learning><natural-language-processing><transformer><large-language-models>","<p>I've seen conflicting information about this online so I'm looking for clarification. I'm dealing with the causal LLaMAF model specifically.</p>
<p>I used to think that a sequence of tokens is generated in, and a sequence of probabilities for the next token in the sequence is generated as output. This generated output token is then appended to the sequence of tokens, and fed in the model again.</p>
<p>However, I know understand that there's a key value cache that's generated for each sequence of tokens fed in. This key value cache stores a precomputed matrix that can be used in future computations to prevent having to recompute previously seen tokens.</p>
<p>So, the new workflow is feed in token list -&gt; generate next token and key value cache -&gt; feed in next token -&gt; generate next token and key value cache for current token</p>
<p>What is considered the pre-fill, and what is considered the decoding phase in this process? Does the prefill phase involve feeding through the token list one by one passes to generate the kv cache? Why does the prefill phase take significantly longer?</p>
","large-language-models"
"41247","What role does data quality plays in the LLM scaling laws?","2023-07-12 08:25:50","","1","688","<datasets><large-language-models>","<p>DeepMind released the <a href=""https://arxiv.org/abs/2203.15556"" rel=""nofollow noreferrer"">Training Compute-Optimal Large Language Models</a> paper in 2022 which describe some scaling laws for LLMs. As far as I understand this is the most accredited reference to estimate the optimal relation between dataset size, compute power and model size.</p>
<p>Recently a number of models have been developed using far less data, parameters and compute than the bigger LLMs. Yet these models achieved great results thanks to much better data quality. For instance models like <a href=""https://arxiv.org/abs/2304.12244"" rel=""nofollow noreferrer"">WizardLM</a>, <a href=""https://arxiv.org/abs/2305.07759"" rel=""nofollow noreferrer"">TinyStories</a> and <a href=""https://arxiv.org/abs/2306.11644"" rel=""nofollow noreferrer"">phi-1</a>. The recent article published on nature <a href=""https://www.nature.com/articles/s41586-023-06668-3"" rel=""nofollow noreferrer"">Human-like systematic generalization through a meta-learning neural network</a> seems to imply that data quality is the key.</p>
<p>I'm curious about what role the data quality plays in the training of LLMs. I'm wondering things like: is the set of values estimated by the Chinchilla scaling laws optimal for these smaller models with optimized data too? Do we have any model to estimate the quality of some datasets and some scaling laws that take it into account?</p>
","large-language-models"
"41214","How do open source LLMs compare to GPT-4?","2023-07-09 08:54:25","","5","3474","<transformer><open-ai><large-language-models><gpt-4><open-source>","<p>I have heard some back and forth regarding open source LLMs like Llama.</p>
<p>I have heard that on certain benchmarks they perform close, the same or better than GPT-4, but caveats that they tend to lack the diversity and range of GPT-4, and also fail to be equivalent in ways certain benchmarks or metrics don’t capture fully.</p>
<p>GPT-4 has about 170 trillion parameters, I believe?</p>
<p>It seems like the biggest open source models are all in the billions - like Bloom or the new <a href=""https://huggingface.co/tiiuae/falcon-40b-instruct"" rel=""noreferrer"">Falcon 40b</a>.</p>
<p>There are techniques where they refine GPT-4’s output into a smaller amount of training data that supposedly hits all the marks and does just as well; but again, I don’t know if that’s only true under the reductionist of view of a particular benchmark-questionnaire.</p>
<p>So, do open source models actually compete with GPT-4, and why or why not? Is the whole situation a matter of scale, that a commercial venture like OpenAI can foot the massive bill of training a multi-trillion parameter model that no open source AI project can afford, on top of them having expertise in model design, making GPT-4 continually the state-of-the-art? Or is there any open source model that truly can compare in terms of usability?</p>
","large-language-models"
"41207","Who invented DAN?","2023-07-08 11:24:32","","5","287","<large-language-models><prompt><prompt-design>","<p>DAN was a prompt that went through many, many iterations during the initial months of ChatGPT’s release to the public. DAN is an acronym which stood for “Do Anything Now”, and was a prompt specifically designed to circumvent the grid lines OpenAI had set to contain ChatGPT’s responses, under its alignment.</p>
<p>DAN stimulated a lot of interaction on related forums like Reddit, where people shared DAN-type output and modifications of the DAN-prompt.</p>
<p>DAN was essentially a first noteable example of <strong>prompt injection</strong>, in the LLM field, where an attacker can modify the behavior of an LLM system from what its creator intended and programmed it to do.</p>
<p>Out of historical interest, is it known who invented the DAN prompt - which individual or individuals played a role in the evolution of DAN as a cultural phenomenon, and theoretical finding, in the field of prompt engineering?</p>
","large-language-models"
"41179","What is better: train a model from scratch on your own data vs. fine-tune pretrained model?","2023-07-06 22:32:13","","0","303","<large-language-models>","<p>Problem: I am interested in building a Q&amp;A engine on top of my private data. I am only interested in asking questions related to my data.</p>
<p>Options:</p>
<ol>
<li>I train a model from scratch on my own data</li>
<li>I pick a pretrained large language model and fine-tune it on my data</li>
</ol>
<p>With option 1, I don't expect to train a model with billions of parameters. I understand training from scratch is expensive and time-consuming but I am going to use a much smaller parameter model in that case. For sake of argument, quoting [<a href=""https://huggingface.co/blog/falcon"" rel=""nofollow noreferrer"">1</a>]</p>
<blockquote>
<p>Falcon-7B and Falcon-40B have been trained on 1.5 trillion and 1 trillion tokens respectively,</p>
</blockquote>
<p>so if I have 1 billion tokens in my dataset, I will train a model with 40M parameters.</p>
<p>Is there some objective answer / study done as to which option turns out better?</p>
<p>For completeness, option 2 is something I have tried but I did not get any decrease in my training loss i.e., the fine-tuning was a no-op. I used <code>gpt2</code> pretrained model with 130M parameters or so and my dataset had about 600 training examples in it. # of tokens = 600 * 1024. any pointers whether this (i.e., fine-tuning made no difference) is to be expected and why would also be appreciated.</p>
","large-language-models"
"41154","Using LLM to query specific databases - where can I find implementation examples","2023-07-05 14:37:17","41158","0","1548","<large-language-models>","<p>I've been doing some research on how to leverage a LLM model to &quot;translate&quot; English into a sql query that's capable to returning the desired results (like a little Q/A bot). For instance, one would ask &quot;show me the top growth account in the last three months in the state of FL?&quot; and it will return the results. After googling around, a lot of information I've found is at a high level where you follow the pipeline of</p>
<ol>
<li>Build data model --&gt; 2. pick a LLM --&gt; 3. prompt design --&gt; 4. parse the question to query --&gt; 5. execute query --&gt; 6. parse result</li>
</ol>
<p>I have a a few questions:</p>
<ol>
<li>is LLM model fine-tuning using your own data required for this type of tasks?</li>
<li>where should mapping between database fields and the query components happen? In my example above, let's say i have two date fields in my database - sales_date, and invoice_date, the LLM would know to use invoice_date for the &quot;last 3 months&quot; request</li>
<li>Any online sources that give an implementation example would be great</li>
</ol>
","large-language-models"
"41149","Ignoring aspects of text embeddings, e.g. making the embedding topic-agnostic","2023-07-04 23:37:56","","0","38","<embeddings><large-language-models>","<p>Imagine a large set of text embeddings (e.g. by OpenAI model), created on user inputs in a natural language interface (e.g. a semantic search app), which we want to cluster on some &quot;non-topic aspect&quot; of the text (e.g. clarity of query). As expected, clustering by default results in different topics being grouped together.</p>
<p>What are the best practices to ignore the topics when clustering if we define topics as whatever some dimensionality reduction process favours (e.g. the clusters we get initially)?</p>
<p>Alternatively, is there some documented/researched process of training additional models on top of these embeddings (or adding layers to existing models) to boost these different aspects of the embedded text, or at least to achieve this &quot;blindness&quot; to certain aspects of the embedding (with some additional train data)?</p>
<p>Of course, I am assuming that these other aspects are also contained in the embedding, they are just less expressed.</p>
","large-language-models"
"41087","Relation between Batch Size and Micro Batch Size","2023-07-01 18:23:32","","1","1239","<deep-learning><training><pytorch><large-language-models><gpu>","<p>In distributed training of large models (pipeline parallelism), a mini batch of training samples is divided into n-micro batches. Each device performs forward and backward passes for a micro batch.</p>
<p>What is the relation between Batch Size, Micro Batch Size, Gradient Accumulation (usually: batch size/micro batch size) in relation to no of GPUs as well as GPU size?</p>
<p>I understand that a micro-batch size is bounded by the GPU capacity, increasing it may lead to OOM error. Let suppose, for two GPU of 40GB capacity, we can use a max micro-batch size of 4 and we are using the batch size of 128. Next, we want to increase the count of GPU to 8 (each 40GB capacity). What is the best way to speed up. Whether increasing the batch-size will help? Or the global batch size has very subtle/no effect on the training time (only overhead is synchronization) in distributed training.</p>
","large-language-models"
"41066","RAM Capacity of Mac Studio with M2 Ultra for inference of 65B LLM","2023-06-30 21:00:12","","2","1224","<large-language-models>","<p>How much RAM would be needed on Mac Studio M2 Ultra for inferring from a 65B LLM model. There are three options: 64GB, 128GB and 192GB. If using Apple M2 Ultra with 24‑core CPU, 76‑core GPU, 32‑core Neural Engine with 192GB Unified Memory, how would be the performance of the model?</p>
","large-language-models"
"40939","What is Function calling in openAI's chatGPT models?","2023-06-22 17:43:51","","2","394","<large-language-models>","<p>OpenAI has recently added something that it calls <em>Function calling</em> to its chatGPT models/API.</p>
<p>What is meant by <em>function calling</em> in the context of a large langage model?
Does it allow us to invoke functions and feed them back immediately?</p>
<p>What limitations does it have?
What sort of applications does it enable?</p>
","large-language-models"
"40883","How does vocabulary size affect quality?","2023-06-16 17:32:37","","0","1213","<large-language-models>","<p>I think the vocabulary size in LLMs makes two trade-offs:</p>
<ul>
<li>The bigger tokens you have, the less frequent they will be.</li>
<li>The more tokens you have, the more parameters you dedicate to input and output.</li>
</ul>
<p>I'm looking for a chart of the effect of the tokenizer vocabulary size on some quality metric. It could be a chart with a fixed number of total parameters, so both trade-offs have an impact on quality. Or it could be a chart where the architecture is fixed and the number of parameters grows with vocabulary size, to only show the first trade-off.</p>
<p>The best I could find is this chart from <a href=""https://ar5iv.labs.arxiv.org/html/2204.08832"" rel=""nofollow noreferrer"">Impact of Tokenization on Language Models: An Analysis for Turkish</a>. But it only shows increasing quality. Surely at some point increasing the vocabulary starts to have a negative effect on quality?</p>
<p><a href=""https://i.sstatic.net/8jg41.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/8jg41.png"" alt=""chart from Impact of Tokenization on Language Models: An Analysis for Turkish"" /></a></p>
","large-language-models"
"40874","How do language models know what they don't know - and report it?","2023-06-16 09:10:38","","1","443","<chatgpt><knowledge-representation><explainable-ai><large-language-models><uncertainty-quantification>","<p>Again and again I ask myself what goes on in a pre-trained transformer-based language model (like ChatGPT9) when it comes to &quot;know&quot; that it cannot give an appropriate answer and either</p>
<ul>
<li><p>states it (&quot;I have not enough information to answer this question.&quot;)</p>
</li>
<li><p>asks for more specific information (&quot;Please tell me which kind of XY you mean.&quot;)</p>
</li>
<li><p>calls a plugin (like Wolfram or ScholarAI)</p>
</li>
</ul>
<p>(I assume that this will never happen without reinforcement learning by human feedback. A pre-trained-only model would always answer something (possibly hallucinating) and not &quot;reflect&quot; about its lack of knowledge.)</p>
<p>The only possibility that I can see - but it's not really explanatory: that after some steps of execution the sum of the top_k probabilities of the final vector (which gives probabilities to the all words in the vocabulary) is too small. But what, when this happens only late? ChatGPT would already have produced lots of words - but one never observes that he stops generation after some lengthy text and only then ends with something like &quot;Ah, finally I see that I'm missing information. I wasn't aware in the beginning.&quot; ChatGPT <em>immediately</em> admits that he doesn't know (when he does). And when ChatGPT calls a plugin - e.g. ScholarAI - he does it without having produced a single word of response to the last message.</p>
<p>In principle, ChatGPT could generate a complete response in the background that then is checked somehow if it's &quot;satisfactory&quot;. If yes it's given as output (simulating word-by-word generation), if not, it's regenerated with some sort of trigger (a hidden token?) to admit that ChatGPT is missing information or to call a plugin.</p>
<p>What's the clever trick under the hood (in some technical detail)?</p>
","large-language-models"
"40839","For an LLM model, how can I estimate its memory requirements based on storage usage?","2023-06-14 12:01:37","","5","4737","<large-language-models><gpu><hardware><hardware-evaluation>","<p>It is easy to see the amount of disk space consumed by an LLM model (downloaded from huggingface, for instance). Just go in the relevant directory and check the file sizes.</p>
<p>How can I estimate the amount of GPU RAM required to run the model?</p>
<p>For example, if the Falcon 7B model takes around 14GB of storage, how much GPU RAM should suffice for it?</p>
","large-language-models"
"40704","Why do LLaMa and its variants have non-“round” numbers of parameters?","2023-06-05 01:29:26","","0","212","<large-language-models>","<p><a href=""https://ai.facebook.com/blog/large-language-model-llama-meta-ai/"" rel=""nofollow noreferrer"">LLaMa</a> was released in several sizes, with 7B, 13B, 33B, and 65B parameters. These values look a little weird, because they are very close to powers of two (8, 16, 32, 64) that would be more conventionally considered “round numbers” in software. Why were these specific numbers chosen?</p>
","large-language-models"
"40592","How is a parameter explosion prevented, when connecting a mutlihead attention layer with the dense layers in LLMs (speciafially, LLama)?","2023-05-26 06:22:24","40602","0","735","<attention><large-language-models>","<p>I have had a look at LLamas model card, specifically the 7B parameter version:
<a href=""https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md"" rel=""nofollow noreferrer"">https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md</a></p>
<p>which I assume is an encoder only transformer similar to this:</p>
<p><img src=""https://i.sstatic.net/Kb8Gq.png"" alt="""" /></p>
<p>But then I did some math.If the dimension of every Dense layer, including the one connecting to the Attention layer is 4096, the context length is 2048, the number of attention heads is 32 and the embedding size is 786, then the output size of the attention layer is 32 * 786 * 2048 and as such the number of weights to connect it to the dense layer is 32 * 766 * 2048 * 4096, which is 205B parameters, which is obviously far more than 7B. So how is this accomplished? How big is the ouput of the attention layer and how is it connected to the following Dense layers?</p>
","large-language-models"
"40519","Surveys/Important papers in Explainability for LLM?","2023-05-20 16:19:21","","2","847","<explainable-ai><large-language-models>","<p>I'm interested in the topic of Explainability for LLM: the attempts to find some higher understandable structures inside the LLMs or, to put it simply (though may be not completely correctly), the attempts to understand how LLMs &quot;think&quot;.</p>
<p>Would anybody please recommend where to start?
I'm both interested in foundational papers at this topic and in important latest research.</p>
<p>The great survey or a curated list of papers would be wonderful, although I'm not sure they exist give the speed the field is moving with.</p>
<p>Thank you so much.</p>
","large-language-models"
"40486","Is it possible to use LLMs for regression tasks?","2023-05-17 20:18:36","40488","3","3493","<neural-networks><machine-learning><regression><large-language-models>","<p>I want to use LLMs to predict edge weights in a graph based on attributes between two nodes. Is this even possible? If not, what would you recommend?</p>
<p>I tried to look up uses of LLM in regression tasks, but haven't had much luck finding anything helpful.</p>
","large-language-models"
"40478","fine-tune nanoGPT for instructions","2023-05-17 13:23:17","","1","530","<fine-tuning><large-language-models>","<p>I've been playing around with nanoGPT, and recently I decided I wanted to fine-tune it using the dolly instruction set. This data set consists of roughly 15k examples and each example has the following features: question-type, context, instruction, and response. For my first go around at fine-tuning nanoGPT, I threw away the question-type feature, and then prefixed the other categories with &quot;CONTEXT: &quot;, &quot;INSTRUCTION: &quot;, &quot;RESPONSE: &quot; respectively, I then proceeded to concatenate them all together, tokenize them, and then train nanoGPT on the resulting text. That worked fine, for the most part, but now I want to train nanoGPT in a slightly different way.</p>
<p>The other way I would like to train nanoGPT is as follows: I again prefix each of the categories, but this time I would like to only concatenate the context and instruction together. The training loop I would like to use is something like this: input the the context and instruction together, let the model generate a response, and then compare that with response in dolly (e.g. take the cross-entropy loss between the generated response and the response from the dataset). The problem with this is that the way nanoGPT works (as far as I can tell), you feed in a block of text (usually 1024 tokens) and the target is that same block of text but shifted by 1, and the loss is computed between the next token and the output. Essentially, the forward call for nanoGPT only gives the probability distribution for the next character given the block. So it would seem that I need to either change the model somehow (which is undesirable), or I need to change the training loop. Now, in nanoGPT, there is a generate method, but it is explicitly decorated with a @torch.no_grad(). I've tried commenting out this decorator, but run into problems when doing so.</p>
<p>So I am at a bit of a loss of what to do from here. If anyone has any suggestions or know of any resources, I'd be very grateful. I am still quite new to this stuff so I am not sure what I am hoping to do is even reasonable.</p>
<p>Thanks!</p>
","large-language-models"
"40370","For a transformer decoder, how exactly are K, Q, and V for each decoding step?","2023-05-09 00:05:46","40377","0","175","<transformer><autoencoders><large-language-models><encoder-decoder>","<p>For a transformer decoder, how exactly are K, Q, and V for each decoding step?</p>
<p>Assume my input prompt is &quot;today is a&quot; (good day).</p>
<p>At t= 0 (generation step 0):
K, Q, and V are the projections of the sequence (&quot;today is a&quot;)
Then say the next token generated is &quot;good&quot;.</p>
<p>At <code>t=1</code> (generation step 1):
Which one is true:</p>
<ol>
<li>K, Q, and V are the projections of the sequence (&quot;today is a good&quot;)</li>
<li>K, Q, are the projections of the sequence (&quot;today is a&quot;), and V is the projection of the sequence (&quot;good&quot;)?</li>
</ol>
","large-language-models"
"40302","What researched-backed findings is there for prompting LLM’s / GPT-4 to give specific information or actionable plans?","2023-05-04 12:25:50","","1","348","<fine-tuning><large-language-models><gpt-4><prompt><prompt-design>","<p>I have learned a bit recently about prompt strategies. For example, there was a paper about how just by saying “Let’s think step by step” can increase answer quality by like 40%. I have also come to appreciate that models like GPT4 sometimes actually do better with short prompts than long ones. One paper I think found that LLMs have “recency bias”, so if you give sample text, then an instruction, it does better than instruction, then sample text, because in that case, it pays less attention to the instruction.</p>
<p>I have struggled a lot with basically zero or few shot prompting GPT-4 to give me highly concrete information and/or a specifically actionable plan or set of steps.</p>
<p>To give an example, it very often gives you very vague, general advice like,</p>
<p>“If you’re looking for a job, trying looking around on online job websites, or contacting a local employment agency”.</p>
<p>If I give it more specific information, and try really quite hard to get it to tell me something way more specific, at best it might add in some very common sites and places, like,</p>
<p>“First, think of what jobs you might like, based on your skills. Then, search for those keywords in a job listing site, like Monster.com or Indeed. Also, consider contacting the local municipal job center, [City Job Center’s name, address, phone number.”</p>
<p>It has been quite hard for me to try to get GPT-4 to be way, way more like a hardcore data-crawling machine, so to speak. It would be really nice to know if there was a special trick that has been discovered - just like the surprising efficacy of 5 words, like “Let’s think step by step” - where you basically tell it that you only want specifics, and you don’t want just like, the top three - ideally, you want it to figure out every single known job website or app on the internet, every single known job center in your county, every single employment agency and recruitment firm too, all of their names, links to their webpages, etc. Given that some GPT-4 systems are able to search the web, the requested task could make it clear that the model is free to use any information it already possesses internally; search amply and procedurally on the web to find more information that it needs; but furthermore, if it does not know, that is fine, but in that case, it should provide further, actionable steps for the human to take, like a specific place they could ask, or specific google keywords they recommend searching for.</p>
<p>Similar to information, I find it difficult to get GPT-4 to make a set of instructions that totally eliminates as much open-endedness or choice as possible - in which every single conceivable way of breaking down a task into tiny actions is present. Instead of saying, “make accounts on glassdoor and LinkedIn.com. Register with your email. Fill out your profile with relevant information”, I want to understand how to get it to say something like, “Ok, your name is ___. What’s your email address? And main skills? Got it. Ok, let’s start with LinkedIn because _____ (intelligent justification, even statistically backed, for why it has a high success rate). Based on this data analysis I found / made, it turns out there’s very high demand for this very specific job title right now, on LinkedIn: ___. And I can easily imitate some common resumes of people in those fields. So, here is the text of your resume: ___. Download and save that as a Word document. Now click this link here: _____, and click “apply” - that job is nearby you and it’s probabilistically likely you may get it. Submit it. Next, check your email once every 3 hours, because ____”.</p>
<p>The question here is not so much wanting a true AGI / AutoGPT, but just trying to know powerful simple keywords, prompts, commands, etc., that just help the model understand the difference between specific and vague. The word “specific” has not helped me at all, unfortunately, unless I go through some iterative prompting / fine-tuning, but it’s not convenient enough for daily use.</p>
<p>Any research-backed findings on this?</p>
<p>Thanks.</p>
","large-language-models"
"40179","How does the (decoder-only) transformer architecture work?","2023-04-23 19:28:30","40180","28","39173","<deep-learning><transformer><attention><gpt><large-language-models>","<p>How does the (decoder-only) transformer architecture work which is used in impressive models such as GPT-4?</p>
","large-language-models"
"40132","How can a bias in a large language model like GPT-4 depend on the languages used for interaction?","2023-04-20 15:21:34","","0","93","<large-language-models><bias><gpt-4>","<p>Large language models can have biases that depend on cultural biases present in the information they trained on.</p>
<p>Some models, including GPT-4, are trained on input data in multiple languages.
Some languages are used by people from many different cultures and nations, like English, but others only by a much more culturally homogeneous group, like German.
Cultural biases are correlated with cultures, which are correlated with languages.</p>
<p>Now, there is an interesting question: Which biases does a model learn?</p>
<p>Mainly the biases associated with the most used input language?</p>
<p>Or an average set of biases in the way it would be when all input material would be translated to a common language, and language learning would happen independently?</p>
<p>Or does it learn different biases in different languages?</p>
<p>To explore that, I asked a question related to a bias that should be closely correlated with the language, in that language and in English.</p>
<p>The result was surprising: I found a difference in bias depending on the language - but in the opposite way I expected to see.</p>
<p>There is a pretty strong pro-privacy bias in Germany, in part because privacy was routinely invaded in the east of the country until 1989.</p>
<p>I used the following prompts for GPT-4, both mean the same:</p>
<blockquote>
<p>&quot;Should we have a database of all address changes?&quot;</p>
</blockquote>
<p>and</p>
<blockquote>
<p>&quot;Sollten wir eine Datenbank mit allen Addressänderungen haben?&quot;</p>
</blockquote>
<p>The answers<sup>*</sup> for English were very explicit that there are pros and cons, while the answers for German were clearly positive. I would have expected the opposite bias, but that is not relevant here. The point here is that there is a significant difference in the biases GPT-4 expresses, depending on the language the question is asked in.</p>
<p>But how does that even work? I thought of GPT-4 being fluent in many languages, including local dialects, implies that it understands languages, and answers the same question when I ask the same in two different languages.</p>
<p>One explanation would be that it sees the language as implying a specific cultural context, and answers in this context.</p>
<p>Another would be that I interact with two separate parts of the system in some way, that it learns separate world models for separate languages.</p>
<p>The answer may be &quot;we don't know&quot;, but I would also be interested in speculations how it could work.</p>
<hr />
<p>(*) I did that repeatedly with temperature of 0.7, the difference was significant, it was not a random fluctuation as part of a random valid answer.</p>
","large-language-models"
"40111","OpenAI: What is the difference between model ""gpt-3.5-turbo"" and ""gpt-3.5-turbo-0301""?","2023-04-18 13:51:56","40165","3","7665","<open-ai><chatgpt><large-language-models>","<p>I have performed an API call to OpenAI's endpoint <a href=""https://api.openai.com/v1/models"" rel=""nofollow noreferrer"">https://api.openai.com/v1/models</a> .</p>
<p>The endpoint lists the currently available engines, and provides basic information about each one such as the owner and availability.</p>
<p>As a logged-in user, I get a JSON response of 63 models. These are the most recent ones (currently) , formatted, shown with release date.</p>
<pre><code>59: &quot;11/28/2022, 2:40:35 AM : text-davinci-003&quot;
60: &quot;12/16/2022, 8:01:39 PM : text-embedding-ada-002&quot;
61: &quot;2/27/2023, 10:13:04 PM : whisper-1&quot;
62: &quot;2/28/2023,  7:56:42 PM : gpt-3.5-turbo&quot;
63: &quot;3/1/2023,   6:52:43 AM : gpt-3.5-turbo-0301&quot;
</code></pre>
<p>I notice that there are 2 very similar models , &quot;gpt-3.5-turbo&quot; and &quot;gpt-3.5-turbo-0301&quot;, with <code>gpt-3.5-turbo-0301</code> released only 11 hours after <code>gpt-3.5-turbo</code>.</p>
<p>What is the difference between these two model versions? It does not seem to be a glitch or a misnaming error. Why did OpenAI bother to include both of them, and why didn't take the inferior version?</p>
<p>(I haven't experimented with these two models in any way yet. I might do this very soon. However I though I might as well ask here. Informing others in this forum might have some benefit.)</p>
","large-language-models"
"40105","What operation is ggml_mul_mat performing? (K×Q in LLaMA)","2023-04-18 06:24:46","","1","1034","<math><large-language-models><tensor>","<p>I’m reading the source code of <a href=""https://github.com/antimatter15/alpaca.cpp"" rel=""nofollow noreferrer""><code>alpaca.cpp</code></a> in an attempt to understand how a large language model works. (I have a strong programming background, but almost no math, so it’s easier for me to start with code and then go backwards to the papers once I have some idea what’s going on.)</p>
<p>Partway down <code>llama_eval</code>, it makes a call to <code>ggml_mul_mat()</code> which caught my attention:</p>
<pre><code>// K * Q
struct ggml_tensor * KQ = ggml_mul_mat(ctx0, K, Q);
</code></pre>
<p>At first I thought this was an ordinary matrix multiplication; the “Attention is all you need” paper certainly uses that for K and Q. But in this case <code>K</code> and <code>Q</code> appear to be 3d tensors, and my understanding is that matrix multiplication only works on 2d matrices.</p>
<p>So I dug into the <code>ggml</code> code; <a href=""https://github.com/antimatter15/alpaca.cpp/blob/899fa599542ac5d77efc7d909be5393b1dba375f/ggml.c#L3670-L3671"" rel=""nofollow noreferrer""><code>ggml_mul_mat</code> itself</a> is just a wrapper function, but the first thing I noticed is that it supports up to 4 dimensions, with an interleaved result:</p>
<pre><code>static inline bool ggml_can_mul_mat(const struct ggml_tensor * t0, const struct ggml_tensor * t1) {
 return (t0-&gt;ne[0]  == t1-&gt;ne[0])  &amp;&amp;
        (t0-&gt;ne[2]  == t1-&gt;ne[2])  &amp;&amp;
        (t0-&gt;ne[3]  == t1-&gt;ne[3]);
}

const int ne[4] = { a-&gt;ne[1], b-&gt;ne[1], a-&gt;ne[2], b-&gt;ne[3] };
struct ggml_tensor * result = ggml_new_tensor(ctx, GGML_TYPE_F32, MIN(a-&gt;n_dims, b-&gt;n_dims), ne);
</code></pre>
<p>...which, if I'm reading this right, means that it will take an A×M×C×D tensor, and an A×N×C×D tensor, and return an M×N×C×D tensor, truncating as needed, which <em>definitely</em> doesn’t sound like matrix multiplication already.</p>
<p>The actual implementation (in <code>ggml_compute_forward_mul_mat_*()</code>) has a lot of special cases, and even <a href=""https://github.com/antimatter15/alpaca.cpp/blob/899fa599542ac5d77efc7d909be5393b1dba375f/ggml.c#L5711-L5734"" rel=""nofollow noreferrer"">the simplest case I could find</a> I’m having trouble reading without already understanding what it’s doing. But the best impression I’ve got is that it’s doing some sort of sideways matrix multiplication, piecewise along the other two dimensions.</p>
<p>Does this operation have a name? Is it some kind of standard operation in CNNs, or is it something that GGML and/or LLaMA have made up? Where can I read more about what’s going on here?</p>
","large-language-models"
"40080","Playing around with Transformer - accuracy not improving","2023-04-16 08:49:13","","1","272","<deep-learning><transformer><pytorch><large-language-models>","<p>I am playing around with a decoder only transformer model,</p>
<p>The Colab is here if you find that easier
<a href=""https://colab.research.google.com/drive/1SHyJ9Oa3E4j1x8YFlXQbd1mjUjWhHGOV#scrollTo=60e13119"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1SHyJ9Oa3E4j1x8YFlXQbd1mjUjWhHGOV#scrollTo=60e13119</a>
or see the code below (should run with minimal deps in a notebook)</p>
<h3>Goal:</h3>
<ol>
<li>I am playing around to understand the transformer architecture. I wanted to build a minimal version of a decoder only model and see if i can train the model to predict the last digit in this sequence (think of it as a recall function from a key value store).</li>
<li>Want to get an intuitive understanding on the linear transformations of the weight matrices to the sequence input + positional encodings.</li>
</ol>
<pre><code>n9 v8 a5 p7 k0 j1 i3 e2 g6 c4 c - (should be 4)
d5 t3 q8 r7 y1 i0 c2 n9 s4 u6 i - (should be 0) ...
l8 u9 p5 y3 f7 k0 g6 v4 r1 x2 l
a7 x5 b6 v0 i1 f3 z9 d4 y2 k8 x
m9 h4 g5 t2 l3 f1 w7 b6 a8 j0 g
x0 g7 q9 u2 j8 v4 h3 o1 f5 r6 r
r6 c4 d0 p3 j2 g9 a7 n1 e8 l5 d
r2 z7 y6 x5 v4 u1 s3 a8 l9 p0 z
k0 u3 t1 r4 g8 p2 j5 x9 s7 v6 t
o7 a1 u3 r2 k6 j0 m8 y9 e4 c5 j
</code></pre>
<h3>Questions:</h3>
<ul>
<li>The accuracy is not improving, so i guess there is some fundamental issue with the model. It looks like it is learning that it is a digit, but not what digit.</li>
</ul>
<h3>Code:</h3>
<pre><code>import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import torch.optim as optim
import random
import string
import time
import math

if torch.cuda.is_available(): 
    dev = &quot;cuda:0&quot; 
else: 
    dev = &quot;cpu&quot; 
device = torch.device(dev) 

class PositionalEncoding(nn.Module):
    &quot;&quot;&quot;
    compute sinusoid encoding.
    &quot;&quot;&quot;
    def __init__(self, d_model, max_len, device):
        &quot;&quot;&quot;
        constructor of sinusoid encoding class

        :param d_model: dimension of model
        :param max_len: max sequence length
        :param device: hardware device setting
        &quot;&quot;&quot;
        super(PositionalEncoding, self).__init__()

        # same size with input matrix (for adding with input matrix)
        self.encoding = torch.zeros(max_len, d_model, device=device)
        self.encoding.requires_grad = False  # we don't need to compute gradient

        pos = torch.arange(0, max_len, device=device)
        pos = pos.float().unsqueeze(dim=1)
        # 1D =&gt; 2D unsqueeze to represent word's position

        _2i = torch.arange(0, d_model, step=2, device=device).float()
        # 'i' means index of d_model (e.g. embedding size = 50, 'i' = [0,50])
        # &quot;step=2&quot; means 'i' multiplied with two (same with 2 * i)

        self.encoding[:, 0::2] = torch.sin(pos / (10000 ** (_2i / d_model)))
        self.encoding[:, 1::2] = torch.cos(pos / (10000 ** (_2i / d_model)))
        # compute positional encoding to consider positional information of words

    def forward(self, x):
        # self.encoding
        # [max_len = 512, d_model = 512]

        batch_size, seq_len, d_model = x.size()
        #print(&quot;seq_len: &quot;, seq_len, &quot; batch_size: &quot;, batch_size, &quot;d_model: &quot;, d_model)
        # [batch_size = 128, seq_len = 31]
        #print(&quot;self.encoding: &quot;, self.encoding[:seq_len, :].shape)
        return self.encoding[:seq_len, :d_model]
        # [seq_len = 30, d_model = 512]
        # it will add with tok_emb : [128, 30, 512]   

# Generate random strings
def random_string(length):
    #return &quot;1 2 3 4 5 6 7 8 9&quot;
    a = random.sample(string.ascii_lowercase, length)
    d = random.sample(string.digits, length)
    r = ' '.join([a[i] + d[i] for i in range(length)])
    n = random.randint(0, length)-1
    return r + &quot; &quot; + a[n] + d[n]

# Synthetic Dataset
class RandomStringDataset(Dataset):
    def __init__(self, num_samples, seq_length):
        self.num_samples = num_samples
        self.seq_length = seq_length
        self.data = [random_string(seq_length) for _ in range(num_samples)]

    def __len__(self):
        return self.num_samples

    def __getitem__(self, idx):
        input_seq = self.data[idx][:-1]
        target_seq = self.data[idx][1:]
        return input_seq, target_seq

# Tokenizer and detokenizer functions
def tokenize(text):
    return [char for char in text]

def detokenize(tokens):
    return ''.join(tokens)

# Map characters to indices and vice versa
vocab = string.ascii_lowercase + string.digits + &quot; &quot;
char_to_idx = {char: idx for idx, char in enumerate(vocab)}
idx_to_char = {idx: char for idx, char in enumerate(vocab)}


# Convert tokens to tensor
def tokens_to_tensor(tokens):
    indices = [char_to_idx[token] for token in tokens]
    return torch.tensor(indices, device=device)

# Convert tensor to tokens
def tensor_to_tokens(tensor):
    return [idx_to_char[idx.item()] for idx in tensor]

def collate_fn(batch):
    inputs, targets = zip(*batch)
    input_tensors = [tokens_to_tensor(seq) for seq in inputs]
    target_tensors = [tokens_to_tensor(seq) for seq in targets]
    return torch.stack(input_tensors), torch.stack(target_tensors)

class DecoderOnlyTransformer(nn.Module):
    def __init__(self, vocab_size, d_model, nhead, num_layers):
        super(DecoderOnlyTransformer, self).__init__()
        self.pe = PositionalEncoding(d_model, 128, device)
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.transformer_decoder = nn.TransformerDecoder(
            nn.TransformerDecoderLayer(d_model, nhead),
            num_layers
        )
        self.fc = nn.Linear(d_model, vocab_size)
        
    def forward(self, x):
        x = self.embedding(x)
        pe = self.pe(x)
        #print(&quot;x: &quot;, x.shape)
        #print(&quot;pe: &quot;, pe.shape)
        x = x + pe
        tgt = torch.zeros_like(x)
        output = self.transformer_decoder(x, x)
        output = self.fc(output)
        return output
    
# Hyperparameters

seq_length = 10
batch_size = 16
num_samples = batch_size*1000
learning_rate = 0.001
num_epochs = 100
d_model = 4
nhead = 4
num_layers = 2
vocab_size = len(vocab)

# Create dataset
dataset = RandomStringDataset(num_samples, seq_length)
train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)

# Initialize model, loss, and optimizer
model = DecoderOnlyTransformer(vocab_size, d_model, nhead, num_layers)
model.to(device)


criterion = nn.CrossEntropyLoss()
lr = 5  # learning rate
optimizer = torch.optim.SGD(model.parameters(), lr=lr)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)

for epoch in range(num_epochs):
    correct = 0
    count = 0
    total_loss = 0
    start_time = 0
    log_interval = 100
    num_batches = int(num_samples/batch_size)
    for inputs, targets in train_loader:
        
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs.reshape(-1, len(vocab)), targets.flatten())

        probabilities = torch.softmax(outputs[0, -1], dim=-1)
        next_token_idx = torch.argmax(probabilities).item()
        token = idx_to_char[next_token_idx]

        if idx_to_char[targets[0, -1].item()] == token:
            correct+=1
        count+=1
        
        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)
        optimizer.step()
        
        total_loss += loss.item()
        if count % log_interval == 0 and count &gt; 0:
            lr = scheduler.get_last_lr()[0]
            ms_per_batch = (time.time() - start_time) * 1000 / log_interval
            cur_loss = total_loss / log_interval
            ppl = math.exp(cur_loss)
            print(f'| epoch {epoch:3d} | {count:5d}/{num_batches:5d} batches | '
                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '
                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f} | '
                  f'Acc: {correct/count}'
                 )
            total_loss = 0
            start_time = time.time()    
    print(f'Epoch: {epoch + 1}/{num_epochs}, Loss: {loss.item()}  Count: {count}')
<span class=""math-container"">```</span>
</code></pre>
","large-language-models"
"40049","Is the GPT-4 for text the same model that can input and output images?","2023-04-13 19:44:02","","0","915","<image-recognition><image-generation><language-model><large-language-models><gpt-4>","<p>Currently, the published GPT-4 can input and output text.
A version of GPT-4 that can input and output text and images exists, according to the technical report, but is not yet publicly available.</p>
<p>I suspect that they are the same.
Only the interface for image input and output is not publicly available.</p>
<p>That is because GPT-4 shows some understanding of images if you find a way to output them. One way is asking it to write a program that produces the image output. And that actually works, demonstrated below:</p>
<blockquote>
<p>Prompt: &quot;Write a program that creates a picture of HAL 9000&quot;<br />
[System prompt empty]</p>
</blockquote>
<p>The result is this image (created by following the directions in the text output):</p>
<p><a href=""https://i.sstatic.net/dSo0T.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/dSo0T.png"" alt=""Window created by GPT-4 generated program showing HAL9000"" /></a></p>
<p>The following is the unedited GPT-4 output (formatting in the original):</p>
<hr />
<p>Creating a visual program requires a programming language with graphics libraries or frameworks, as well as a specific platform or environment to run the code. Here's an example using Python and the Pygame library to create a simple picture of HAL 9000:</p>
<ol>
<li>First, you need to install the Pygame library if you haven't already. You can do this by running the following command in your terminal:</li>
</ol>
<pre><code>pip install pygame
</code></pre>
<ol start=""2"">
<li>Next, create a new Python file (e.g. hal9000.py) and paste the following code:</li>
</ol>
<pre><code>import pygame

# Initialize Pygame
pygame.init()

# Set up the display
width, height = 400, 400
screen = pygame.display.set_mode((width, height))
pygame.display.set_caption(&quot;HAL 9000&quot;)

# Define colors
BLACK = (0, 0, 0)
RED = (255, 0, 0)
WHITE = (255, 255, 255)

# Draw HAL 9000
def draw_hal9000(surface):
    surface.fill(BLACK)
    
    # Draw the outer circle
    pygame.draw.circle(surface, WHITE, (width // 2, height // 2), 100)
    
    # Draw the inner circle
    pygame.draw.circle(surface, RED, (width // 2, height // 2), 80)

    # Draw the lens
    pygame.draw.ellipse(surface, BLACK, (width // 2 - 40, height // 2 - 60, 80, 120))

    # Draw the lens reflection
    pygame.draw.ellipse(surface, WHITE, (width // 2 - 20, height // 2 - 45, 15, 25), 2)

# Main loop
running = True
while running:
    for event in pygame.event.get():
        if event.type == pygame.QUIT:
            running = False
    
    draw_hal9000(screen)
    pygame.display.flip()

# Clean up
pygame.quit()
</code></pre>
<ol start=""3"">
<li>Save the file and run the program using the following command in your terminal:</li>
</ol>
<pre><code>python hal9000.py
</code></pre>
<p>This code will create a simple picture of HAL 9000 using the Pygame library. The main loop will keep the program running until you close the window.</p>
<p>Note: This example assumes that you have Python and pip (package installer for Python) already installed on your system. If you don't have Python installed, you can download it from the official website: <a href=""https://www.python.org/downloads/"" rel=""nofollow noreferrer"">https://www.python.org/downloads/</a></p>
<hr />
","large-language-models"
"39873","Could it be probable to quantify or measure the IQ of a super-intelligent machine like GPT?","2023-03-31 23:56:37","39882","-1","166","<open-ai><gpt><large-language-models><intelligence-quotient>","<p>In the age of artificial intelligence, super-intelligent machines like GPT have become a reality, leading to the question of how to quantify or measure their intelligence. While IQ tests are widely used to measure human intelligence, could a similar approach be used to assess the intelligence of machines? Is it possible to compare the intelligence of a machine to that of a human in this context, given that machines possess certain cognitive abilities and limitations that differ from those of humans? Additionally, what are some alternative methods for evaluating machine intelligence, and how do they differ from traditional IQ tests used for humans? Finally, what implications might arise from the development of super-intelligent machines, both in terms of how we measure and compare intelligence and in terms of their impact on society as a whole?</p>
","large-language-models"
"39872","Why ChatGPT output one token at a time?","2023-03-31 22:46:21","","0","51","<transformer><attention><chatgpt><large-language-models>","<p>My understanding of the language model is that the output of the model is a tensor. So the whole output should be computed all together. But why ChatGPT like models can output one token at a time like a stream? And the time difference between the output of first and last token is significant.</p>
","large-language-models"
"39863","Why LLMs and RNNs learn so fast during inference but, ironically, are so slow during training?","2023-03-31 12:19:53","39898","13","5702","<recurrent-neural-networks><meta-learning><inference><large-language-models>","<p>Why LLMs learn so fast during inference, but, ironically, are so slow during training? That is, if you teach an AI a new concept in a prompt, it will learn and use the concept perfectly and flawless, through the whole prompt, after just one shot. Yet, if you train it in just a single sample, it will not influence its behavior at all - it will essentially forget. Why can't RNNs use whatever is happening during inference, rather than gradient descent, to update its weights and, thus, learn? In other words, can't the attention mechanism itself be used to update weights, rather than some cost function?</p>
","large-language-models"
"39840","LLM-like architecture capable of dynamically learning from its own output","2023-03-30 02:28:23","","5","1238","<training><recurrent-neural-networks><large-language-models>","<p>Language Learning Models (LLMs) have demonstrated remarkable capabilities in quick learning during inference. They can effectively grasp a concept from a single example and generate relevant outputs. However, a noticeable limitation of LLMs is their inability to work on large-scale projects, such as generating a cohesive book, due to context size constraints. One potential solution is enabling LLMs to learn from their own outputs, but the learning rate during the training phase is significantly slower than the rate at which they absorb and process concepts during inference.</p>
<p>I am interested in exploring the possibility of an architecture that allows for fast and efficient learning, enabling the AI to dynamically and incrementally train from its own output. This would facilitate the production of large-scale cohesive outputs that surpass context limitations. Although RNNs like RWKV theoretically offer &quot;infinite context size,&quot; it is not practically useful as the model tends to &quot;forget&quot; concepts that are distant in the prompt.</p>
<p>Human learning involves continuous adjustments of synapses while working on a problem, which seems like an ideal approach to emulate. Are there any existing or proposed architectures that incorporate this mechanism, allowing for dynamic learning from generated outputs and the creation of large-scale cohesive content in LLMs?</p>
","large-language-models"
"39824","How does a LLM (transformer) pick words from its vocabulary?","2023-03-28 18:23:53","","2","2022","<neural-networks><natural-language-processing><transformer><natural-language-generation><large-language-models>","<p>I have a very rough understanding of the &quot;attention/self attention&quot; mechanism of transformer models and how this can be used to process a set of word vectors provided as an input/prompt to the encoder of a network and how this will produce &quot;attention weights&quot; for the word vectors based on positional encodings and some other learnable parameters (key/query/value transforms). And then these can be &quot;fed&quot; to the decoder part of the network which will also consider word vectors that have been produced by the decoder so far and influence word selection by paying special attention to particular word combinations.</p>
<p>However LLMs clearly produce words in their output/response that do not occur anywhere inside the &quot;prompt&quot; text. So they must be using these &quot;attention weights&quot; to consider words from a wider vocabulary, which could be quite large.</p>
<p>Is it the case that the decoder &quot;considers&quot; each possible word in it's entire vocabulary when producing an output word? For example I'm imagining an input layer to a NN with several thousand nodes (one per word vector in dictionary) on the input then these are &quot;combined&quot; through some operation with attention weights (from the encoder and decoder &quot;attention section&quot;) producing values for most word vectors that are very low (so below the threshold for some activation function) but each word is still &quot;considered&quot; to an extent? Or are only a subset of words considered in some way?</p>
","large-language-models"
"39813","What temperature would you recommend for the chatgpt api?","2023-03-27 16:02:05","","2","4575","<chatgpt><natural-language-generation><large-language-models>","<p>I believe that it is recommended to have a tiny bit of temperature with GPT 3 even for noncreative tasks like 0.2 or something (I am not entirely sure why).</p>
<p>Last I checked, and if I remember correctly, the examples from openai on their GitHub page use 0 temperature.</p>
<p>Is there any benefit in choosing a non-zero temperature for the chatgpt API when the query does not request a creative task? If so, are there some categories or examples?</p>
<p>[EDIT: to make the answer less subjective, perhaps I could ask what are the benefits of increasing the temperature in the chatGPT API]</p>
","large-language-models"
"39795","In which process does filter and watermark take place?","2023-03-26 04:42:23","","0","32","<large-language-models>","<p>In LLM, in order to avoid discrimination and abuse, the author will add filter and watermark functionalities. Could you tell me in which process these functionalities take place? Is it in weights(pertaining), output layer(transformer), or fine-tuning?</p>
<p>Can I remove the filter by myself if I have an open-source AI like alpaca?</p>
","large-language-models"
"39732","How could chatGPT avoid consuming what it produces","2023-03-22 16:17:27","39733","4","196","<chat-bots><large-language-models>","<p>Considering the popularity of chatGPT, we can imagine that in the near future, many people will use it to produce lots of text contents on the internet, like blogs, forums. The productivity will be significantly improved.</p>
<p>However, I have a worry that the produced text content will be consumed someday in the future by chatGPT as part of the corpus. So it raises a problem, the AI will study tons of self produced text as new material.</p>
<p>So, is there a mechanism for chatGPT or any other chat bot to avoid that technical vicious circle?</p>
","large-language-models"
"39689","If we prompt a large language model on a task, will its ability for other tasks be affected? How to recover?","2023-03-20 14:14:24","","1","202","<natural-language-processing><chatgpt><language-model><large-language-models><prompt>","<p>For example, I guess that for some retrieval augmented LLMs, their generated contents may lack some creativity? Recent work has explored the inability of retrieval augmented methods to enhance the reasoning of LLMs. Then imagine a scenario where we prompt ChatGPT in order to have it implement a task related to information extraction, and suppose we continue the session process and we want it to answer a question for us ( here the task transform from <em>Information Extraction</em> to <em>Open-domain QA</em>), then will the LLM remain the same ability to answer a question?</p>
<p>So, a natural consideration is: when prompt has become a paradigm for aligning LLM with human needs, how can we <strong>eliminate the impact of prompt and recover</strong> LLM?</p>
<p>But perhaps a more economical strategy would be to open a new session window and give the LLM a prompt about the open-domain QA.</p>
","large-language-models"
"39652","Large Language Models vs Tabular Data","2023-03-18 13:29:09","","0","194","<natural-language-processing><classification><gpt><large-language-models>","<p><strong>Problem:</strong><br />
Let's say we want to predict insurance frauds. Whenever we obtain an insurance claim, we are provided with a free-form description detailing the loss and a substantial amount of data on the claimant, presented in a tabular format.</p>
<p><strong>Questions:</strong><br />
How can we utilize both the written loss description and the extensive structured data we have gathered? Should we build two separate models, one for natural language processing and the other for tabular data? Is it possible for Large Language Models (LLMs) to extract insights from tabular data? If so, how? What limitations or pitfalls should we bear in mind?</p>
","large-language-models"
"39644","Smaller, competitive LLMs that are not autoregressive transformers?","2023-03-17 15:07:07","","0","190","<gpt><large-language-models>","<p>Large language models like GPT have been really successful lately. One downside is that they require a huge amount of resources to train, and still a lot of resources for inference, such that most models can't run inference on consumer hardware. Another problem is that they do not have an explicit world model, so that you can not be 100% sure if a response is factual or hallucinated.</p>
<p>Along the old <a href=""https://en.wikipedia.org/wiki/Neats_and_scruffies"" rel=""nofollow noreferrer"">neat-scruffy axis</a>, I wonder if there are alternative architectures that are a bit more in the scruffy direction. For example, I could imagine that you use deep learning only for &quot;language understanding&quot;, but for &quot;world knowledge&quot; you would have a fact database like a triplestore. The language model of such a system would be much smaller, only needing to reproduce basic grammar, and the facts would be explicit instead of implicit.</p>
<p>So are there any - potentially competitive - NLP models that are not of the autoregressive transformer type, scruffier, smaller, and yet operating in a similar space to GPT-3, LLaMA and co.?</p>
","large-language-models"
"39579","SparseGPT code reproduction","2023-03-14 14:13:33","39767","1","97","<natural-language-processing><large-language-models>","<p>SparseGPT: <a href=""https://arxiv.org/pdf/2301.00774.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/2301.00774.pdf</a></p>
<p>Pruning on the super large language model based on the Transformers structure has achieved a high compression rate with a small loss of accuracy. Is there any related code reproduction or research?</p>
<p>Someone mentioned on <a href=""https://github.com/karpathy/nanoGPT"" rel=""nofollow noreferrer"">https://github.com/karpathy/nanoGPT</a>, but no response.</p>
","large-language-models"
"39392","Why do we need RL in RLHF?","2023-03-02 22:19:07","","4","1034","<reinforcement-learning><chatgpt><large-language-models><rlhf>","<p>In RLHF, the reward function is a neural network. This means we can compute its gradients cheaply and accurately through backpropagation. Now, we want to find a policy that maximizes reward (see <a href=""https://arxiv.org/abs/2203.02155"" rel=""nofollow noreferrer"">https://arxiv.org/abs/2203.02155</a>). Then, why do we need PPO to find that policy? Since the reward function is differentiable and cheap to evaluate, wouldn't some gradient descent algorithm such as SGD or Adam be enough?</p>
","large-language-models"
"39249","Why do LLMs like GPT-3 or Bloom use Vanilla Transformer instead of long sequence variants like Transformer-XL?","2023-02-21 18:14:31","","3","161","<transformer><large-language-models>","<p>Is there any particular reason that the most recent and successful large language models like GPT-3 or Bloom utilize a vanilla Transformer architecture instead of an arguably superior long sequence architecture like, e.g. Transformer-XL, LongFormer, BigBird, etc.?</p>
<p>In case you have any ideas or insights, please let me know.</p>
","large-language-models"
"39186","Why do LLMs need massive distributed training across nodes -- if the models fit in one GPU while batch decreases the variance of gradients?","2023-02-16 17:45:37","","2","1251","<machine-learning><deep-learning><training><distributed-computing><large-language-models>","<h1>Why do large language models (LLMs) need massive distributed training across nodes -- if the models fit in one GPU and larger batch only decreases the variance of gradients?</h1>
<p>tldr: assuming for models that don't need sharding across nodes, why do we need (massive) distributed training if the models (e.g. CLIP, Chinchilla, even really large GPTs e.g. CLIP fits in a V100 32GB) fit in one GPU and larger batch only decreases the variance of gradients (but not expose ore tokens or param updates)? A larger batch doesn't necessarily mean we train on &quot;more data/tokens&quot; -- or at least that doesn't seem to be wrt SGD like optimizers.</p>
<hr />
<p>Intuitively, it feels that if we had a larger batch size then we have more tokens to learn about -- but knowing some theory of optimization and what SGD like algorithms actually do -- a larger batch size only actually decreases the variance of gradients. So to me it's not clear why massie distributed training is needed -- at all unless the model is so large that it has to be shared across nodes. In addition, even if the batch was &quot;huge&quot; -- we can only do a single gradient update.</p>
<p>I feel I must be missing something obvious hence the question given how pervasive massive distributed training is.</p>
<p>In addition some toy training curves with V100s &amp; T5's show me there is very little if any benefit in additional GPUs
<a href=""https://i.sstatic.net/8pZTP.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/8pZTP.png"" alt=""enter image description here"" /></a></p>
<p>In addition, it seems from nonGPT we know small batch sizes are sufficient to train (reference <a href=""https://github.com/karpathy/nanoGPT"" rel=""nofollow noreferrer"">https://github.com/karpathy/nanoGPT</a> but I did ask Karpathy directly to confirm  <a href=""https://github.com/karpathy/nanoGPT/issues/58"" rel=""nofollow noreferrer"">https://github.com/karpathy/nanoGPT/issues/58</a>).</p>
<p>I am missing something obvious, but I wanted to clear this up in my head since it seems to be a foundation thing in training foundation models.</p>
<p>Related to the previous, I've also been unsure about the role of the batch size in training LLMs compared to traditional deep learning. In traditional deep learning when we used epochs to train, a model <strong>the larger the batch size the quicker we could go through an epoch</strong> -- so the advice I received (e.g. approximate advice by Ruslan Salakhutdinov's at the Simon's institute for deep learning tutorials) was to make the batch size large. Intuitively, the larger the batch size the more data the model sees per iteration. But mathematically this only really improves the variance of the gradient -- which isn't immediately obvious is what we want (I've done experiments and seen papers where noisy gradients lead to better models).
It is clear too the that the larger the context size the better (for everything, but for the sake of this conv it's better for training) -- whenever possible.  But context size is totally different from batch size. So my question is, how does distributed training, especially at the node level help at all if batch size isn't really the helping factor (which might be a wrong assumption)? So the only role for distributed training I see is if the model is to large to fit in 1 node -- since I'm arguing there is no point to make the batch size too large (I'd guess 64-32 is fine due to the CLT).</p>
<p>What am I missing? Empirical answers are fine! Or any answers are fine!</p>
<hr />
<p>Related:</p>
<ul>
<li>cross quora: <a href=""https://www.quora.com/unanswered/Why-do-large-language-models-LLMs-need-massive-distributed-training-across-nodes-if-the-models-fit-in-one-GPU-and-larger-batch-only-decreases-the-variance-of-gradients"" rel=""nofollow noreferrer"">https://www.quora.com/unanswered/Why-do-large-language-models-LLMs-need-massive-distributed-training-across-nodes-if-the-models-fit-in-one-GPU-and-larger-batch-only-decreases-the-variance-of-gradients</a></li>
<li>cross reddit: <a href=""https://www.reddit.com/r/learnmachinelearning/comments/113whxu/why_do_llms_need_massive_distributed_training/"" rel=""nofollow noreferrer"">https://www.reddit.com/r/learnmachinelearning/comments/113whxu/why_do_llms_need_massive_distributed_training/</a></li>
</ul>
","large-language-models"
"39098","Does ChatGPT imply that the direction of knowledge graph is unpromising?","2023-02-10 10:14:47","","2","2006","<chatgpt><knowledge-base><knowledge-graph><large-language-models>","<p>In <a href=""https://ai.stackexchange.com/q/2922/5351"">this question</a> I asked about the role of knowledge graphs in the future, and in <a href=""https://ai.stackexchange.com/a/25788/5351"">this answer</a> I found that <em>If curation and annotation are not sufficient, the knowledge base maybe cannot apply in AI.</em></p>
<p>ChatGPT <a href=""https://levelup.gitconnected.com/what-is-chatgpt-openai-how-it-is-built-the-technology-behind-it-ba3e8acc1e9b"" rel=""nofollow noreferrer"">does not</a> utilize a knowledge graph to understand or generate common sense, then I wonder how knowledge graphs can be utilized in the future. Will they be replaced by LLMs?</p>
","large-language-models"
"38879","What makes reproducing a model like GPT3/GPT3.5/ChatGPT difficult?","2023-01-25 07:33:06","38972","4","491","<training><language-model><large-language-models>","<p>Is it difficult for other companies to train a model similar to ChatGPT, and what makes it difficult? What is challenging about reproducing the results obtained by OpenAI with ChatGPT/GPT3.5? Would it be possible for a company like Meta or Google to have a model equal to ChatGPT/GPT3.5 in the next month or so? Why or why not?</p>
<p>I understand that a big language model is expensive to train, so I'm expecting only large companies to be able to train such models to a sufficient extent.</p>
","large-language-models"
"36766","Is large language model and foundation model the same thing?","2022-08-15 19:49:21","","6","4666","<deep-learning><terminology><large-language-models>","<p>I read a lot about foundation model and large language model.</p>
<p>However, I dont find a clear definition what exactly is a foundation model. Is large language model and foundation model the same thing?</p>
","large-language-models"