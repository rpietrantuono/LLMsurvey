Post ID,Title,CreationDate,Score,ViewCount,CommentCount,AnswerCount,FavoriteCount,AcceptedAnswerId,Body
"46516","How to convert a positionally encoded predicted embedding from a decoder to its matching token?","2024-08-14 20:14:41","0","24","0","1","","46521","<p>Is it valid to just subtract the positional encoding from a predicted output if the decoder was also positionally encoded? Or does masking take care of this problem, and the decoder should only learn the embedding (not positionally encoded)</p>
"
"45866","Can transformer attention make predictions based on analogy?","2024-06-01 16:35:20","1","88","0","2","","45941","<p>Suppose I have included 3 examples of an idiosyncratic sentence for training by a transformer:</p>
<ul>
<li>Example 1: <strong>Asdfogiug likes Zsdfoiusdhf and Zsdfoiusdhf likes Asdfogiug too.</strong></li>
<li>Example 2: <strong>Bsodifhas likes Zsdfoiusdhf and Zsdfoiusdhf likes Bsodifhas too.</strong></li>
<li>Example 3: <strong>Clkwjehrq likes Zsdfoiusdhf and Zsdfoiusdhf likes Clkwjehrq too.</strong></li>
</ul>
<hr />
<p>Now we prompt the above-trained transformer with the following:</p>
<p><strong>Dlwkjerhtw likes Zsdfoiusdhf and Zsdfoiusdhf likes ______________</strong></p>
<hr />
<p>Can our transformer complete the prompt correctly?</p>
<p>How would the attention mechanism know to generate <strong>&quot;Dlwkjerhtw&quot;</strong> (a word which it has never seen before anywhere else) as the predicted next word in our prompt?</p>
"
"45768","Why Tokenization Algorithm replace space with underscore ""_""?","2024-05-21 12:23:06","0","47","0","1","","45775","<p>I have recently read an article for Byte-Pair Encoding (BPE), in which they have replaced space with an underscore(_), or we can say, adding underscore after each word/token.</p>
<p>I did not understand the intuition for adding an underscore; we can just do it without adding an underscore.
We can also have any other character that is not frequently used; why underscore?
We can also add an underscore at the start of the word or both end and start, but why add to the end?</p>
<p>Site: <a href=""https://medium.com/@hsinhungw/understanding-byte-pair-encoding-fd196ebfe93f"" rel=""nofollow noreferrer"">https://medium.com/@hsinhungw/understanding-byte-pair-encoding-fd196ebfe93f</a></p>
"
"45579","Reference request: data efficiency of LLM pre-training","2024-04-29 14:22:36","0","32","0","1","","45712","<p>I've seen it stated multiple times that LLMs have much worse data efficiency than humans (IE require more data to reach same or worse performance), EG <a href=""https://twitter.com/ylecun/status/1784210369231130883"" rel=""nofollow noreferrer"">this Tweet by Yann LeCun</a>, or <a href=""https://youtu.be/b76gsOSkHB4?si=typUcVl74LyCZfbM&amp;t=1170"" rel=""nofollow noreferrer"">19:30 in this talk by Michael Wooldridge</a>. Are there any papers (preferably published but otherwise pre-prints) that really explore data-efficiency of LLM pre-training (not ICL, I'm happy to accept that's data-efficient), and possibly compare against an approximate/upper-limit human benchmark?</p>
<p>Preferably looking for papers with thorough exploration and evaluation of data-efficiency of existing LLMs, rather than papers proposing a new method for improving data-efficiency without thorough comparison to existing SOTA models.</p>
<p>I'd like to know so hopefully I can cite such a paper in future work of my own.</p>
"
"43843","What language model can convert normal text to JSON data","2024-02-20 05:32:39","0","229","0","1","","43855","<p>I have tried training T5-small, T5-base and T5-Large on around 15K rows of data where input data was something like but I did not get desired results</p>
<pre><code>Nutrition Facts,
100g per,
Energy 646.95Kcal Carbohydrates 19.31g,
 Protein 21.94g 53.55g Total Fat 6.64g Saturated Fat 14.97g Dietary Fiber,
&lt;1.Omg Cholesterol Sodium 0.29g Sugars 3.39g,
Lightly Salted and to Perfection,
Ingredients: Peanuts, Almonds,,
Cashews, Pistachios, Vegetable Oil, Salt,
aa, ,
74G,
Pistachio, 61129110611336177
WE ARE NUTS ABOUT QUALITY,
Baked,
Nuts Salted,
Mixed
WE ARE NUTS ABOUT,
Community 364, 13 Street Plot No. 36,
Al Area 1, 24149,UAE 4971 4 3355777,
License Number: 224614 VAT No: 100058529700003,
CERTIFIED COMPANY,
ALLERGEN WARNING: in a facility that also processes nuts, sesame and mustard,
Store in a cool dry place away from heat moisture,
Instruction Once store in airtight container and consume before expiry date,
Pro: 14/12/23,
Exp:13/12/24,
Net 40gms
</code></pre>
<p>Output data will be in JSON format of the above details.
What language models can be trained for this purpose and minimum how many Parameters it should have?</p>
"
"43178","How does chatgpt interpret prompts?","2023-12-19 04:35:18","0","118","0","1","","43182","<p>First of all, my English isn't very good, please bear with me.</p>
<p>I've heard that chatgpt is a decoder-only model, which doesn't use any encoder model.
How does it interpret our prompts if it doesn't use any encoder model?</p>
"
"43128","What is accelerated years in describing the amount of the training time?","2023-12-14 09:59:07","1","738","0","1","","43130","<p>As described in this <a href=""https://arxiv.org/abs/2204.05149"" rel=""nofollow noreferrer"">article</a>, it was written that GPT-3 took 405 V100 years to train in 2020.
I'm a bit confused about this definition, does that mean the process was accelerated like using a V100 GPU to train in 405 years?</p>
"
"42490","How can LLMs understand and perform meta tasks? (e.g. summarization)","2023-10-19 10:54:17","1","89","0","1","","42520","<p>I don't ask how to make it summarize xy but if it is known how a &quot;LLM&quot; understands and performs this meta task at all.</p>
<p>The same is true for prompts like &quot;Be brief&quot; or &quot;Explain in a few sentences&quot;. Since this works, the LLM must actually have an understanding of text length.</p>
"
"41352","Why does LLM inference cost scale in both input tokens and output tokens?","2023-07-19 02:55:02","0","1798","7","1","","41388","<h2>EDIT</h2>
<p>This question was flawed. See <a href=""https://ai.stackexchange.com/a/41388/68775"">my answer</a> with help from commenters.</p>
<hr />
<h2>Original question</h2>
<p>This question has been asked in other forums <a href=""https://community.openai.com/t/why-does-pricing-vary-by-input-tokens-instead-of-only-output-tokens/21833"" rel=""nofollow noreferrer"">[1]</a> <a href=""https://www.reddit.com/r/OpenAI/comments/11vvx66/whats_the_rationale_for_charging_for_input_tokens/"" rel=""nofollow noreferrer"">[2]</a> but I'm not sure I understand the claims, which are (EDIT: the following are based on my faulty assumption that pad tokens are added up to the maximum context window):</p>
<ol>
<li>Each forward pass takes less resources when more of the context window is padding.</li>
<li>Forward passes are run on the input tokens.</li>
<li>Forward passes with fewer non-pad input tokens are smaller tensor operations.</li>
</ol>
<p>Hypothesis 1 seems the most plausible to me from a performance engineering standpoint (sparse math, etc). Does it fall out naturally from just writing basic JAX code or would it require manual optimization (if so, what tricks can be used?)? There does <a href=""https://arxiv.org/abs/2210.03052"" rel=""nofollow noreferrer"">seem to be some research on this</a>.</p>
<p>Hypothesis 2 and 3 seem wrong based on my surface-level understanding of the Transformer architecture.</p>
<p>I've tried a few open-source LLMs locally and neither on those nor ChatGPT have I noticed any difference in latency based on how much text was in the context window. But I haven't done actual rigorous benchmarking yet.</p>
<p>The reason this is relevant is due to document lookup-based applications. Looking at the <a href=""https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb"" rel=""nofollow noreferrer"">OpenAI cookbook for Q&amp;A using embeddings</a>, I can see that:</p>
<ul>
<li>most of the token usage seems to come from the long Wikipedia article pasted into the context</li>
<li>the price per query is quite prohibitive to frequent usage:</li>
</ul>
<blockquote>
<p>For gpt-3.5-turbo using ~1,000 tokens per query, it costs ~0.002 per query, or ~500 queries per dollar (as of Apr 2023)
For gpt-4, again assuming ~1,000 tokens per query, it costs ~0.03 per query, or ~30 queries per dollar (as of Apr 2023)</p>
</blockquote>
<p>My prior intuition would have been that optimal usage of LLMs would be to keep the context filled with inexpensive text (e.g. from NN search and/or cheaper LMs) and to have the LLM generate terse responses. But the input token cost model changes the strategy, as it means that users need to be sparing about the size and quantity of documents that they paste into the context window.</p>
"
"39863","Why LLMs and RNNs learn so fast during inference but, ironically, are so slow during training?","2023-03-31 12:19:53","13","5702","1","4","","39898","<p>Why LLMs learn so fast during inference, but, ironically, are so slow during training? That is, if you teach an AI a new concept in a prompt, it will learn and use the concept perfectly and flawless, through the whole prompt, after just one shot. Yet, if you train it in just a single sample, it will not influence its behavior at all - it will essentially forget. Why can't RNNs use whatever is happening during inference, rather than gradient descent, to update its weights and, thus, learn? In other words, can't the attention mechanism itself be used to update weights, rather than some cost function?</p>
"
"39293","Is the ""Chinese room"" an explanation of how ChatGPT works?","2023-02-25 09:16:11","39","16225","8","7","","39297","<p>Sorry if this question makes no sense. I'm a software developer but know very little about AI.</p>
<p>Quite a while ago, I read about the Chinese room, and the person inside who has had a lot of training/instructions how to combine symbols, and, as a result, is very good at combining symbols in a &quot;correct&quot; way, for whatever definition of correct. I said &quot;training/instructions&quot; because, for the purpose of this question, it doesn't really make a difference if the &quot;knowledge&quot; was acquired by parsing many many examples and getting a &quot;feeling&quot; for what's right and what's wrong (AI/learning), or by a very detailed set of instructions (algorithmic).</p>
<p>So, the person responds with perfectly reasonable sentences, without ever understanding Chinese, or the content of its input.</p>
<p>Now, as far as I understand ChatGPT (and I might be completely wrong here), that's exactly what ChatGPT does. It has been trained on a huge corpus of text, and thus has a very good feeling which words go together well and which don't, and, given a sentence, what's the most likely continuation of this sentence. But that doesn't really mean it understands the content of the sentence, it only knows how to chose words based on what it has seen. And because it doesn't really understand any content, it mostly gives answers that are correct, but sometimes it's completely off because it &quot;doesn't really understand Chinese&quot; and doesn't know what it's talking about.</p>
<p>So, my question: is this &quot;juggling of Chinese symbols without understanding their meaning&quot; an adequate explanation of how ChatGPT works, and if not, where's the difference? And if yes, how far is AI from models that can actually understand (for some definition of &quot;understand&quot;) textual content?</p>
"
"38294","How does ChatGPT respond to novel prompts and commands?","2022-12-12 18:56:43","4","4148","6","4","","42500","<p>So I understand how a language model could scan a large data set like the internet and produce text that mimicked the statistical properties of the input data, eg completing a sentence like &quot;eggs are healthy because ...&quot;, or producing text that sounded like the works of a certain author.</p>
<p>However, what I don't get about ChatGPT is that it seems to understand the commands it has been given, even if that command was not part of its training data, and can perform tasks totally separate from extrapolating more data from the given dataset. My (admittedly imperfect) understanding of machine learning doesn't really account for how such a model could follow novel instructions without having some kind of authentic understanding of the intentions of the writer, which ChatGPT seems not to have.</p>
<p>A clear example: if I ask &quot;write me a story about a cat who wants to be a dentist&quot;, I'm pretty sure there are zero examples of that in the training data, so even if it has a lot of training data, how does that help it produce an answer that makes novel combinations of the cat and dentist aspects? Eg:</p>
<blockquote>
<p>Despite his passion and talent, Max faced many challenges on his journey to become a dentist. <strong>For one thing, he was a cat, and most people didn't take him seriously when he told them about his dream. They laughed and told him that only humans could be dentists, and that he should just stick to chasing mice and napping in the sun.</strong></p>
</blockquote>
<blockquote>
<p>But Max refused to give up. He knew that he had what it takes to be a great dentist, and he was determined to prove everyone wrong. <strong>He started by offering his services to his feline friends, who were more than happy to let him work on their teeth. He cleaned and polished their fangs</strong>, and he even pulled a few pesky cavities.</p>
</blockquote>
<p>In the above text, the bot is writing things about a cat dentist that wouldn't be in any training data stories about cats or any training data stories about dentists.</p>
<p>Similarly, how can any amount of training data on computer code generally help a language model <a href=""https://twitter.com/amasad/status/1598042665375105024?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1598042665375105024%7Ctwgr%5Ee5ccdc91f902f35c3b969c90350e988509889451%7Ctwcon%5Es1_c10&amp;ref_url=https%3A%2F%2Fwww.bleepingcomputer.com%2Fnews%2Ftechnology%2Fopenais-new-chatgpt-bot-10-coolest-things-you-can-do-with-it%2F"" rel=""nofollow noreferrer"">debug novel code examples</a>? If the system isn't actually accumulating conceptual understanding like a person would, what is it accumulating from training data that it is able to solve novel prompts? It doesn't seem possible to me that you could look at the linguistic content of many programs and come away with a function that could map queries to correct explanations unless you were actually modeling conceptual understanding.</p>
<p>Does anyone have a way of understanding this at a high level for someone without extensive technical knowledge?</p>
"
"38150","How does ChatGPT retain the context of previous questions?","2022-12-04 11:23:04","55","48890","10","2","","38262","<p>One of the innovations with OpenAI's ChatGPT is how natural it is for users to interact with it.</p>
<p>What is the technical enabler for ChatGPT to maintain the context of previous questions in its answers? For example, ChatGPT understands a prompt of &quot;tell me more&quot; and expands on it's previous answer.</p>
<p>Does it use activations from previous questions? Is there a separate input for the context? How does it work?</p>
"
"37427","What is the difference between Neurosymbolic AI and Transformer AI","2022-10-14 14:21:10","2","589","0","1","","37456","<p>I'm looking at the AI timeline and I came across Neuro-Symbolic AI (being symbolic AI used in combination with deep learning) and Transformer AI (which I understand as neural networks that take context into account).</p>
<p>If I look at <a href=""https://en.wikipedia.org/wiki/Neuro-symbolic_AI#:%7E:text=Examples%20include%20BERT%2C%20RoBERTa%2C%20and,how%20to%20evaluate%20game%20positions."" rel=""nofollow noreferrer"">Wikipedia</a> then I'll find that GTP-3 is a form of Neuro-Symbolic AI, but its name is clearly a giveaway for Transformer AI (Generative Pre-trained Transformer-3).</p>
<p>So I'm left wondering; what exactly is the different between these two?</p>
"
"37172","What is the role of self loop in Hidden Markov Models(HMM)?","2022-09-23 11:53:44","0","146","3","1","","37201","<p>What actually does the self-loop (within the single hidden state ) in the Hidden Markov model helpful for?</p>
<p>I learn that one of the use cases concerning Natural language Understanding is that it helps a model to stay within the current state in case of (time variable: long sound/ short sound) pronunciation of the same word. But I can't understand what role the self-loop plays here.</p>
<p>Any explanations would be much appreciated.</p>
"
"34898","How might AI analyze abusive discussion using natural language grammar?","2022-03-20 06:19:24","2","86","1","1","","34902","<h1>Opening thoughts</h1>
<p>This does not only apply to SE comments, but the idea in general.</p>
<p>This is <em>not</em> a Question for <a href=""https://linguistics.stackexchange.com/"">Linguistics.SE</a>; those Questions might come later, <em>after</em> AI analysis. Example Linguistics Quesions:</p>
<ul>
<li><strong>What grammar categories might AI use to research for an analysis of abusive vs helpful discussion?</strong> <em>(before the AI research, after this OP Question is answered)</em></li>
<li><strong>What grammar patterns can we identify from AI researched that analyzed abusive vs helpful discussion?</strong> <em>(after the AI research)</em></li>
</ul>
<p>This is a Question about how AI might be useful in the real world, thus helping AI programmers decide where to effectively focus energies.</p>
<h1>AI analyzes comments and discussion</h1>
<p>Many web apps and sites (including Facebook, YouTube, and Stack Exchange) analyze posted content using what some people call AI algorithms.</p>
<p>Presuming this is used also for comments on posts on sites such as these...</p>
<p>AI may take many factors into consideration, viz buzz words (type 'COVID' on a post and watch the info-notice pop up), profanities, bigotous phrases, etc.</p>
<p>I'm curious about the results if AI analyzed just the grammar of a history of comments that were deemed &quot;abusive&quot; juxtaposed against a history that was not deemed abusive.</p>
<h1>Why ask?</h1>
<p>Creative-analytical thinkers like Steven Levitt (viz <a href=""https://www.freakonomics.com/"" rel=""nofollow noreferrer"">Freakonomics</a>) and <a href=""https://www.gladwellbooks.com/"" rel=""nofollow noreferrer"">Malcolm Gladwell</a> like to discuss counter-intuitive findings from research. Levitt says that this is &quot;economics&quot; (nothing to do with money). Even the video game <a href=""https://leagueoflegends.com"" rel=""nofollow noreferrer"">League of Legends</a> has stats on how often certain gaming choices (items, champion, etc) win and lose. But, we need the data. I want to know if &quot;grammar&quot; is a good place to dig.</p>
<p>I would be curious if there were any grammar patterns that might indicate abuse, as might be found by AI research from past comments. Pardon the grammar lingo, but for example:</p>
<ul>
<li>Complex subjects</li>
<li>Imperatives</li>
<li>Subjunctives</li>
<li>Passives</li>
<li>Verbal pauses (Bothering to type out &quot;Um...&quot; in &quot;Um... No.&quot;)</li>
<li>Direct Objects vs Indirect Objects vs &quot;<a href=""https://english.stackexchange.com/questions/441683"">Raised Objects</a>&quot;</li>
<li>Prepositions</li>
</ul>
<p>...Say research finds that comments containing &quot;at&quot; were 60% more likely to be flagged as abusive. That would be great content to ask on Linguistics to see if there were other patterns to analyze.</p>
<p>I don't know what should be analyzed, nor do I know what all grammatical categories would go into such an analysis. That would be my next Question for Linguistics.</p>
<h1>Scope of my question</h1>
<p>I'm trying to ask for open-ended answers, not binary (yes/no) answers, while also narrowing scope. So, let me put it this way...</p>
<h2>Can AI be used to analyse abusive vs non-abusive discussions through grammar patterns and categories?</h2>
<p>If so, which models or algorithms can be used to achieve that? References are also appreciated.</p>
"
"32500","What does it mean to apply decomposition at inference-time in a machine translation system?","2021-11-23 05:50:17","0","35","0","1","","32532","<p>I'm reading <a href=""https://aclanthology.org/2020.wat-1.21.pdf"" rel=""nofollow noreferrer"">this paper</a> for sub-character decomposition for logographic languages and the authors mention decomposition at inference-time. They're using Transformer architecture.</p>
<p>More specifically, the authors write:</p>
<blockquote>
<p>We propose a flexible inference-time sub-character decomposition procedure which targets unseen characters, and show that it aids adequacy and reduces misleading overtranslation in unseen character translation.</p>
</blockquote>
<p>What do inference-time and inference-only decomposition mean in this context? My best guess would be that inference-time would be at some point during the decoding process, but I'm not 100% clear on whether that's the case and, if so, when exactly.</p>
<p>I'm going to keep digging and update if I find something helpful. In the meantime, if anyone needs more context just let me know.</p>
"
"32141","Given the immaturity of NLP tools for non-English languages, should I first translate the non-English language to English before text pre-processing?","2021-10-22 02:05:18","0","123","1","1","","32142","<p>For non-English languages (in my case Portuguese), what is the best approach? Should I use the not-so-complete tools in my language, or should I translate the text to English, and after using the tools in English? Lemmatization, for example, is not so good in non-English languages.</p>
"
"28556","How to measure the similarity the pronunciation of two words?","2021-07-07 06:54:19","2","1566","0","1","","28647","<p>I would like to know how I could measure the pronunciation of two words. These two words are quite similar and differ only in one vowel.
I know there is, e.g., the Hamming distance or the Levenshtein distance but they measure the &quot;general&quot; difference between words. I'm also interested in that but mainly I would like to know how they sound differently. I think there must be something like this to test text-to-speech results?</p>
<p>Best would even be an online source where I could just type in those two words.</p>
"
"28495","Are the held-out datasets used for testing, validation or both?","2021-07-02 01:59:10","1","369","0","2","","28508","<p>I came across a new term &quot;held-out corpora&quot; and I confused regarding its usage in the NLP domain</p>
<p>Consider the following three paragraphs from <a href=""https://web.stanford.edu/%7Ejurafsky/slp3/3.pdf"" rel=""nofollow noreferrer"">N-gram Language Models</a></p>
<p>#1: <strong>held-out corpora as a non-train data</strong></p>
<blockquote>
<p>For an intrinsic evaluation of a language model we need a test set. As
with many of the statistical models in our field, the probabilities of
an <span class=""math-container"">$n-$</span>gram model come from the corpus it is trained on, the training
set or training corpus. We can then measure training set the quality
of an n-gram model by its performance on some unseen data called the
test set or test corpus.  <strong>We will also sometimes call test sets and
other datasets that are not in our training sets held out corpora
because we hold them out from the held out training data</strong>.</p>
</blockquote>
<p>This paragraph clearly says that held-out corpora can be used for either testing or validation or others except training.</p>
<p>#2: <strong>development set or devset for hyperparameter tuning</strong></p>
<blockquote>
<p><strong>Sometimes we use a particular test set so often that we implicitly
tune to its characteristics. We then need a fresh test set that is
truly unseen. In such cases, we call the initial test set the
development test set or,devset</strong>. How do we divide our data into
training, development, and test sets? We want our test set to be as
large as possible, since a small test set may be accidentally
unrepresentative, but we also want as much training data as possible.
At the minimum, we would want to pick the smallest test set that gives
us enough statistical power to measure a statistically significant
difference between two potential models. In practice, <strong>we often just
divide our data into 80% training, 10% development, and 10% test.</strong>
Given a large corpus that we want to divide into training and test,
test data can either be taken from some continuous sequence of text
inside the corpus, or we can remove smaller “stripes” of text from
randomly selected parts of our corpus and combine them into a test
set.</p>
</blockquote>
<p>This paragraph clearly says that development set is used for hyperparameter tuning.</p>
<p>#3: <strong>held-out corpora for hyperparameter tuning</strong></p>
<blockquote>
<p>How are these <span class=""math-container"">$\lambda$</span> values set? Both the simple interpolation and
conditional interpolation <span class=""math-container"">$\lambda'$</span>s are learned from a held-out
corpus.  <strong>A held-out corpus is an additional training corpus that we
use to set hyperparameters</strong> like these <span class=""math-container"">$\lambda$</span> values, by choosing
the <span class=""math-container"">$\lambda$</span> values that maximize the likelihood of the held-out
corpus.</p>
</blockquote>
<p>This paragraph is clearly saying that held-out corpus is used for hyper-parameter training.</p>
<p><strong>I am interpreting or understanding the terms as follows</strong>:</p>
<p><strong>Train corpus</strong> is used to train the model for learning parameters.</p>
<p><strong>Test corpus</strong> is used for evaluating the model wrt parameters.</p>
<p><strong>Development set</strong> is used for evaluating the model wrt hyperparameters.</p>
<p><strong>Held-out corpus</strong> includes any corpus outside training corpus. So, it can be used for evaluating either parameters or hyperparameters.</p>
<p>To be concise, informally, data = training data + held-out data = training data + development set + test data</p>
<p>Is my understanding true? I got confusion because of paragraph 3, which says that held-out corpus is used (only) for learning the hyperparameters while paragraph 1 says that held-out corpus includes any corpus outside train corpus. Does held-out corpora include devset or same as devset?</p>
"
"26739","What is the difference between a language model and a word embedding?","2021-03-09 21:43:24","5","7926","0","2","","26745","<p>I am self-studying applications of deep learning on the NLP and machine translation.</p>
<p>I am confused about the concepts of &quot;Language Model&quot;, &quot;Word Embedding&quot;, &quot;BLEU Score&quot;.</p>
<p>It appears to me that a language model is a way to predict the next word given its previous word. Word2vec is the similarity between two tokens. BLEU score is a way to measure the effectiveness of the language model.</p>
<p>Is my understanding correct? If not, can someone please point me to the right articles, paper, or any other online resources?</p>
"
"25676","How can I find words in a string that are related to a given word, then associate a sentiment to that found word?","2021-01-10 12:39:09","2","882","0","1","","25682","<p>I came up with an NLP-related problem where I have a list of words and a string. My goal is to find any word in the list of words that is related to the given string.</p>
<p>Here is an example.</p>
<p>Suppose a word from the list is <strong>healthy</strong>. If the string has any of the following words: healthy, healthier, healthiest, not healthy, more healthy, zero healthy, etc., it will be extracted from the string.</p>
<p>Also, I want to judge whether the extracted word/s is/are bearing positive/negative sentiment.</p>
<p>Let me further explain what I mean by using the previous example.</p>
<p>Our word was <strong>healthy</strong>. So, for instance, if the word found in the string was <strong>healthier</strong>, then we can say it is bearing <strong>positive</strong> sentiment with respect to the word <strong>healthy</strong>. If we find the word <strong>not healthy</strong>, it is <strong>negative</strong> with respect to the word <strong>healthy</strong>.</p>
"
"25315","What is MNLI-(m/mm)?","2020-12-20 21:44:30","1","2772","0","1","","30245","<p>I came across the term <strong>MNLI-(m/mm)</strong> in Table 1 of the paper <a href=""https://arxiv.org/pdf/1810.04805.pdf"" rel=""nofollow noreferrer"">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a>. I know what MNLI stands for, i.e. Multi-Genre Natural Language Inference, but I'm just unsure about the <strong>-(m/mm)</strong> part.</p>
<p>I tried to find some information about this in the paper <a href=""https://arxiv.org/pdf/1804.07461.pdf"" rel=""nofollow noreferrer"">GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding</a>, but this explained only the basic Multi-Genre Language Inference concept. I assume that the <strong>m/mm</strong> part was introduced later, but this doesn't make any sense because the BERT paper appeared earlier.</p>
<p>It would be nice if someone knows this or has a paper that explains this.</p>
"
"25217","In the multi-head attention mechanism of the transformer, why do we need both $W_i^Q$ and ${W_i^K}^T$?","2020-12-16 15:58:47","2","1179","0","2","","25225","<p>In the <a href=""https://arxiv.org/pdf/1706.03762.pdf"" rel=""nofollow noreferrer"">Attention is all you need</a> paper, on the 4th page, we have equation 1, which describes the self-attention mechanism of the transformer architecture</p>
<p><span class=""math-container"">$$
\text { Attention }(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V
$$</span></p>
<p>Everything is fine up to here.</p>
<p>Then they introduce the multi-head attention, which is described by the following equation.</p>
<p><span class=""math-container"">$$
\begin{aligned}
\text { MultiHead }(Q, K, V) &amp;=\text { Concat}\left(\text {head}_{1}, \ldots, \text {head}_{\mathrm{h}}\right) W^{O} \\
\text { where head}_{\mathrm{i}} &amp;=\text {Attention}\left(Q W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V}\right)
\end{aligned}
$$</span></p>
<p>Once the multi-head attention is motivated at the end of page 4, they state that for a single head (the <span class=""math-container"">$i$</span>th head), the query <span class=""math-container"">$Q$</span> and key <span class=""math-container"">$K$</span> inputs are first linearly projected by <span class=""math-container"">$W_i^Q$</span> and <span class=""math-container"">$W_i^K$</span>, then dot product is calculated, let's say <span class=""math-container"">$Q_i^p =  Q W_i^Q$</span> and <span class=""math-container"">$K_i^p = K W_i^K$</span>.</p>
<p>Therefore, the dot product of the projected query and key becomes the following from simple linear algebra.</p>
<p><span class=""math-container"">$$Q_i^p {K_i^p}^\intercal = Q W_i^Q {W_i^K}^T K^T =  Q W_i K^T,$$</span></p>
<p>where</p>
<p><span class=""math-container"">$$W_i = W_i^Q {W_i^K}^T$$</span></p>
<p>Here, <span class=""math-container"">$W$</span> is the outer product of query projection by the key projection matrix. However, it is a matrix with shape <span class=""math-container"">$d_{model} \times d_{model}$</span>. Why did the authors not define only a <span class=""math-container"">$W_i$</span> instead of <span class=""math-container"">$W_i^Q$</span> and <span class=""math-container"">$W_i^K$</span> pair which have <span class=""math-container"">$2 \times d_{model} \times d_{k}$</span> elements? In deep learning applications, I think it would be very inefficient.</p>
<p>Is there something that I am missing, like these 2 matrices <span class=""math-container"">$W_i^Q$</span> and <span class=""math-container"">$W_i^K$</span> should be separate because of this and that?</p>
"
"23723","Is it possible to classify the subject of a conversation?","2020-09-24 08:50:23","1","87","4","2","","23855","<p>I would like to classify the subject of a conversation. I could classify each messages of the conversation, but I will loose some imformation because of related messages.</p>
<p>I also need to do it gradually and not at the end of the conversation.</p>
<p>I searched near <strong>recurrent neural network</strong> and <strong>connectionist classification</strong> but I'm not sure it answer really well my issue.</p>
"
"23449","How could facts be distinguished from opinions?","2020-09-06 13:31:51","1","536","0","1","","23684","<p>As a software engineer, I am searching for an existing solution or, if none exists, willing to create one that will be able to process texts (e.g. news from online media) to extract/paraphrase <strong>dry facts</strong> from them, leaving all opinions, analysis, speculations, humor, etc., behind.</p>
<p>If no such solution exists, what would be a good way to start creating it (considering that I have zero experience in AI/machine learning)?</p>
<p>It would be no problem to manually create a set of examples (pairs of original news + dry facts extracted), but is that basically what it takes? I doubt so.</p>
<p>(This knowledge domain is already huge, so which parts of it need to be learned first and foremost to figure out how to achieve the goal?)</p>
"
"22877","How much computing power does it cost to run GPT-3?","2020-08-05 17:35:44","1","15800","2","2","","22881","<p>I know it cost around $4.3 million dollars to train, but how much computing power does it cost to run the finished program? IBM Watson chatbot AI only costs a few cents per chat message to use, OpeenAI Five seemed to run on a single gaming PC setup. So I'm wondering how much computing power does it need to run the finished ai program.</p>
"
"20277","How would you build an AI to output the primary concept of a paragraph?","2020-04-15 09:52:52","2","98","1","1","","20279","<p>My thinking is you input a paragraph, or sentence, and the program can boil it down to the primary concept(s).</p>
<p>Example:</p>
<p><strong>Input:</strong></p>
<blockquote>
<p>Sure, it would be nice if morality was simply a navigation toward greater states of conscious well-being, and diminishing states of suffering, but aren't there other things to value independent of well-being? Like truth, or beauty?</p>
</blockquote>
<p><strong>Output:</strong></p>
<blockquote>
<p>Questioning moral philosophy.</p>
</blockquote>
<hr />
<p>Is there any group that's doing this already? If not, why not?</p>
"
"16346","What happens when the output length in the brevity penalty is zero?","2019-11-08 02:22:40","2","228","0","1","","16359","<p>The brevity penalty is defined as</p>

<p><span class=""math-container"">$$bp = e^{(1- r/c)},$$</span></p>

<p>where <span class=""math-container"">$r$</span> is the reference length and <span class=""math-container"">$c$</span> is the output length.</p>

<p>But what happens if the output length gets zero? Is there any standard way of coping with that issue?</p>
"
"9982","How to recognise metaphors in texts using NLP/NLU?","2019-01-14 11:59:12","7","2890","1","1","","9986","<p>What are the current NLP/NLU techniques that can extract metaphors from texts?</p>
<p>For example</p>
<blockquote>
<p>His words cut deeper than a knife.</p>
</blockquote>
<p>Or a simpler form like:</p>
<blockquote>
<p>Life is a journey that must be travelled no matter how bad the roads and accommodations.</p>
</blockquote>
"
"7747","How does the ""Lorem Ipsum"" generator work?","2018-08-28 23:10:23","4","1234","0","2","","7752","<p>I've seen many <a href=""https://en.wikipedia.org/wiki/Lorem_ipsum"" rel=""nofollow noreferrer"">Lorem Ipsum</a> generators on the web, but not only, there is also ""bacon ispum"", ""space ispum"", etc. So, how do these generators generate the text? Are they powered by an AI?</p>
"
"5422","What is easier or more efficient to summarize voice or text? [DP/RN]","2018-02-24 21:55:33","2","143","0","2","","5424","<p>If possible consider the relationship between implementation difficulty and accuracy in voice examples or simply chat conversations.</p>

<p>And currently, what are the directions on algorithms like Deep Learning or others to solve this.</p>
"
"4991","Why does Google Translate produce two different translations for the same Chinese text?","2018-01-13 20:13:58","0","2516","0","3","","4995","<p>I don't understand why Google Translate translates the same text in different ways.</p>
<p><a href=""https://en.wikipedia.org/wiki/Enter_the_Dragon"" rel=""nofollow noreferrer"">Here is the Wikipedia page of the 1973 film &quot;Enter the Dragon&quot;</a>. You can see that its traditional Chinese title is: 龍爭虎鬥. <a href=""https://translate.google.com"" rel=""nofollow noreferrer"">Google translates</a> this as &quot;Dragons fight&quot;.</p>
<p>Then, if we go to <a href=""https://zh-yue.wikipedia.org/wiki/%E9%BE%8D%E7%88%AD%E8%99%8E%E9%AC%A5_(%E9%9B%BB%E5%BD%B1)"" rel=""nofollow noreferrer"">Chinese Wikipedia page of this film</a>, and search for 龍爭虎鬥 using Ctrl-F, it will be found on several places:</p>
<p><a href=""https://i.sstatic.net/cezxH.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/cezxH.png"" alt=""enter image description here"" /></a></p>
<p>But if we try to copy the hyperlink of Chinese page into Google translate, it will be the word &quot;tiger&quot; from somewhere:</p>
<p><a href=""https://i.sstatic.net/bsf5G.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bsf5G.png"" alt=""enter image description here"" /></a></p>
<p>Even more, if we try to translate Chinese page into English using build-in Chrome translate, it will be sometimes translated as &quot;Enter the Dragon&quot;, in English manner:</p>
<p><a href=""https://i.sstatic.net/JIRQY.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/JIRQY.png"" alt=""enter image description here"" /></a></p>
<p>Why it gives different translations for the same Chinese text here?</p>
"
"2580","How can a system like Jarvis understand the commands and take actions?","2016-12-30 18:02:31","2","1504","0","1","","2585","<p>I am looking to make an AI like <a href=""https://github.com/sukeesh/Jarvis"" rel=""nofollow noreferrer"">Jarvis</a>. A perfect real-life example of this type of system is the simple AI that Mark Zuckerberg has recently built. <a href=""https://www.facebook.com/notes/mark-zuckerberg/building-jarvis/10154361492931634"" rel=""nofollow noreferrer"">Here</a> is a description of how his AI works. From what I understand, the AI understands keywords, context, synonyms, and then from there decides what to do. <a href=""https://www.youtube.com/watch?v=vvimBPJ3XGQ"" rel=""nofollow noreferrer"">Here</a> is also a video of the AI in action.</p>
<p>I have many questions about how these systems works. Firstly, what necessary steps are required to gather the meaning of a input? Secondly, how does the system, once it extracts all of the necessary information on the input, determine what action it needs to take and what to say back to the user? Lastly, it also states that the system can learn the habits and preferences of the user. How can a system do this?</p>
"
"2281","Is it ethical to create a chatbot to answer questions on Stack Overflow?","2016-11-06 12:45:11","1","756","2","2","","2282","<p>I was wondering if I should do this, because 2 out of 5 questions on Stack Overflow don't ever get answered, or if they do get (an) answer (s), most of the time they're not helpful.</p>

<p>So I was thinking -- why not create a chat bot to answer Stack Overflow's questions &amp; provide necessary information to the general public?</p>

<p>I mean why not? I've always been interested in AI, and all I'd need to do is create a basic logic database and a context system, pack an artificial personality with (partial) human instincts, and bam I'm done.</p>

<p>But then again, would it be ethical?</p>
"
"1859","Is anybody still using Conceptual Dependency Theory?","2016-09-05 13:46:57","12","726","0","2","","6572","<p>Roger Schank did some interesting work on language processing with Conceptual Dependency (CD) in the 1970s. He then moved somewhat out of the field, being in Education these days. There were some useful applications in natural language generation (BABEL), story generation (TAILSPIN) and other areas, often involving planning and episodes rather than individual sentences.</p>
<p>Has anybody else continued to use CD or variants thereof? I am not aware of any other projects that do, apart from Hovy's PAULINE, which uses CD as representation for the story to generate.</p>
"
"198","What research has been done in the domain of ""identifying sarcasm in text""?","2016-08-03 09:01:05","18","368","1","2","","1345","<p>Identifying sarcasm is considered one of the most difficult open-ended problems in the domain of ML and NLP/NLU.</p>
<p>So, was there any considerable research done on that front? If yes, then what is the accuracy like? Please, also, explain the NLP model briefly.</p>
"