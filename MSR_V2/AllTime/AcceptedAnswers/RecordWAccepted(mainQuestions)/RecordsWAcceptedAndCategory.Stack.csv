Post ID,Title,CreationDate,Score,ViewCount,CommentCount,AnswerCount,FavoriteCount,AcceptedAnswerId,Body
"78877667","Top-p sampling not working. CUDA error: device-side assert triggered","2024-08-16 05:06:05","0","18","0","1","","78880295","<p>I was trying to re-implement the model.generate() function of transformers' models from huggingface. I did that so I could implement logit-bias, that normal function does not allow. But before I could reach that, I encountered a lot of problems with my top-p sampling.</p>
<p>Here's the code snippet:</p>
<pre><code>generation_args = {
    &quot;max_new_tokens&quot;: 500,
    &quot;temperature&quot;: 0.4,  # Adjust temperature if needed for more or less randomness
    &quot;do_sample&quot;: True,  # Enable sampling
    &quot;top_p&quot;: 0.5,  # Set the cumulative probability for nucleus sampling
    &quot;top_k&quot;: None,  # Optionally, you can set top_k if you want to use it alongside or instead of top_p
}


def top_p_filtering(logits, top_p):
    &quot;&quot;&quot;Filter the logits using top-p (nucleus) sampling.&quot;&quot;&quot;
    # Sort logits in descending order and get the sorted indices
    sorted_logits, sorted_indices = torch.sort(logits, descending=True)

    # Compute the cumulative probabilities of the sorted logits
    cumulative_probs = torch.cumsum(torch.nn.functional.softmax(sorted_logits, dim=-1), dim=-1)

    # Create a mask for the tokens to keep
    sorted_indices_to_keep = cumulative_probs &lt;= top_p

    # Ensure that at least one token is kept (the first token, which has the highest logit)
    sorted_indices_to_keep[..., 0] = True

    # Filter out the tokens to remove by setting their logits to negative infinity
    logits[sorted_indices[~sorted_indices_to_keep]] = float('-inf')

    return logits


def custom_generate(input_ids, streamer, max_new_tokens, temperature, top_p):
    past_key_values = None
    attention_mask = torch.ones(input_ids.shape, device=input_ids.device)

    for _ in range(max_new_tokens):
        with torch.no_grad():
            outputs = model(
                input_ids=input_ids,
                past_key_values=past_key_values,
                attention_mask=attention_mask,
                use_cache=True
            )

        logits = outputs.logits[:, -1, :]  # Get logits of the last token

        # Apply temperature to logits
        if temperature != 1.0:
            logits = logits / temperature

        # Apply top-p sampling
        if top_p is not None and top_p &lt; 1.0:
            logits = top_p_filtering(logits, top_p)
        print(&quot;1&quot;)
        next_token_probs = torch.nn.functional.softmax(logits, dim=-1)
        print(&quot;2&quot;)
        # Check if next_token_probs contains valid probabilities


        next_token_id = torch.multinomial(next_token_probs,
                                          num_samples=1)  
        print(&quot;3&quot;)
        streamer.put(next_token_id)  # Pass the tensor directly to the streamer

        input_ids = next_token_id  # Set the next input to the last generated token
        attention_mask = torch.cat(
            [attention_mask, torch.ones((attention_mask.shape[0], 1), device=attention_mask.device)], dim=1)

        past_key_values = outputs.past_key_values

        if next_token_id.item() == tokenizer.eos_token_id:  
            break

with torch.no_grad():
    custom_generate(input_ids, streamer, generation_args[&quot;max_new_tokens&quot;], generation_args[&quot;temperature&quot;], generation_args[&quot;top_p&quot;])
</code></pre>
<p>The error that I face:</p>
<pre><code>../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [10,0,0], thread: [63,0,0] Assertion `-sizes[i] &lt;= index &amp;&amp; index &lt; sizes[i] &amp;&amp; &quot;index out of bounds&quot;` failed.
Exception in thread Thread-18 (generate):
Traceback (most recent call last):
  File &quot;/usr/lib/python3.10/threading.py&quot;, line 1016, in _bootstrap_inner
    self.run()
  File &quot;/usr/lib/python3.10/threading.py&quot;, line 953, in run
    self._target(*self._args, **self._kwargs)
  File &quot;/mnt/c/Users/User/Documents/EmpatheticChatBot/Inference-Server.py&quot;, line 130, in generate
    custom_generate(input_ids, streamer, generation_args[&quot;max_new_tokens&quot;], generation_args[&quot;temperature&quot;], generation_args[&quot;top_p&quot;])
  File &quot;/mnt/c/Users/User/Documents/EmpatheticChatBot/Inference-Server.py&quot;, line 108, in custom_generate
    next_token_id = torch.multinomial(next_token_probs,
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
</code></pre>
<p>The entire problem arised only after adding top-p sampling.</p>
<p>I expected my sampling to work, as I have looked through my code maybe 30 times already. ChatGPT says this code is perfect, and that my error is really hard to debug. My hypothesis is that values are getting incorrectly filtered or setting them to &quot;bad&quot; values.</p>
"
"78842047","How to quantize safetensors model and save it to GGUF with less then q8_0 quntization?","2024-08-07 06:10:38","0","54","0","1","","78842471","<p>I'm developing LLM agents using llama.cpp as inference engine. Sometimes I want to use models in safetensors format and there is a python script (<a href=""https://github.com/ggerganov/llama.cpp/blob/master/convert_hf_to_gguf.py"" rel=""nofollow noreferrer"">https://github.com/ggerganov/llama.cpp/blob/master/convert_hf_to_gguf.py</a>) to convert.</p>
<p>Script is awesome, but minimum number size is 8 bit (q8_0). Is there any other script or repo with other quantization formats?</p>
"
"78712265","Problems retrieving the embeddings data form OpenAI API Batch embedding job","2024-07-05 15:35:12","0","117","0","1","","78752532","<p>I have to embed over 300,000 products description for a multi-classification project. I split the descriptions onto chunks of 34,337 descriptions to be under the Batch embeddings limit size.</p>
<p>A sample of my jsonl file for batch processing:</p>
<pre><code>{&quot;custom_id&quot;: &quot;request-0&quot;, &quot;method&quot;: &quot;POST&quot;, &quot;url&quot;: &quot;/v1/embeddings&quot;, &quot;body&quot;: {&quot;model&quot;: &quot;text-embedding-ada-002&quot;, &quot;input&quot;: &quot;Base L\u00edquida Maybelline Superstay 24 Horas Full Coverage Cor 220 Natural Beige 30ml&quot;, &quot;encoding_format&quot;: &quot;float&quot;}}
{&quot;custom_id&quot;: &quot;request-1&quot;, &quot;method&quot;: &quot;POST&quot;, &quot;url&quot;: &quot;/v1/embeddings&quot;, &quot;body&quot;: {&quot;model&quot;: &quot;text-embedding-ada-002&quot;, &quot;input&quot;: &quot;Sand\u00e1lia Havaianas Top Animals Cinza/Gelo 39/40&quot;, &quot;encoding_format&quot;: &quot;float&quot;}}
</code></pre>
<p>My jsonl file has 34,337 lines.</p>
<p>I've susscesfully uploaded the file:</p>
<pre><code>File 'batch_emb_file_1.jsonl' uploaded succesfully:
 FileObject(id='redacted for work compliance', bytes=6663946, created_at=1720128016, filename='batch_emb_file_1.jsonl', object='file', purpose='batch', status='processed', status_details=None)
</code></pre>
<p>and ran the embedding job:</p>
<pre><code>Batch job created successfully:
 Batch(id='redacted for work compliance', completion_window='24h', created_at=1720129886, endpoint='/v1/embeddings', input_file_id='redacted for work compliance', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1720216286, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'Batch job for embedding large quantity of product descriptions', 'initiated_by': 'Marcio', 'project': 'Product Classification', 'date': '2024-07-04 21:51', 'comments': 'This is the 1 batch job of embeddings'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))
</code></pre>
<p>The work was completed:</p>
<pre><code>client.batches.retrieve(batch_job_1.id).status
'completed'
</code></pre>
<p><code>client.batches.retrieve('redacted for work compliance')</code>, returns:</p>
<pre><code>Batch(id='redacted for work compliance', completion_window='24h', created_at=1720129886, endpoint='/v1/embeddings', input_file_id='redacted for work compliance', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1720135956, error_file_id=None, errors=None, expired_at=None, expires_at=1720216286, failed_at=None, finalizing_at=1720133521, in_progress_at=1720129903, metadata={'description': 'Batch job for embedding large quantity of product descriptions', 'initiated_by': 'Marcio', 'project': 'Product Classification', 'date': '2024-07-04 21:51', 'comments': 'This is the 1 batch job of embeddings'}, output_file_id='redacted for work compliance', request_counts=BatchRequestCounts(completed=34337, failed=0, total=34337))
</code></pre>
<p>But when I try to get the content using output_file_id string</p>
<p><code>client.files.content(value of output_file_id)</code>, returns:</p>
<pre><code>&lt;openai._legacy_response.HttpxBinaryResponseContent at 0x79ae81ec7d90&gt;
</code></pre>
<p>I have tried:
<code>client.files.content(value of output_file_id).content</code> but this kills my kernel</p>
<p>What am I doing wrong? Also I believe I am under utilizing Batch embeddings. the 90,000 limits conflicts with Batch Queue Limit of 'text-embedding-ada-002' model which is: 3,000,000</p>
<p>Could someone help?</p>
"
"78689230","English text tokenization in C# not python is possible?","2024-06-30 16:52:01","-1","61","0","1","","78689270","<p>In our software we have to analyze a plain text file. First we should break the text into paragraph, then into sentences, then into tokens. Final steps (as far as I understand) is the <code>stemming</code> and <code>lemmatization</code>.</p>
<p>If we have a text like this: <code>We are singing great songs about heroes</code> I would love to see the tokens as [<code>we</code>,<code>be</code>,<code>sing</code>,<code>great</code>,<code>song</code>,<code>about</code>,<code>hero</code>]. To achieve that - as I understand - we need some method to find the tokens in the original text somehow, but the hard part is to stemming/lemmatize it.</p>
<p>I know there is a python project <strong>NLTK</strong> or <strong>spaCy</strong> which are good at these things, but we need to use C# for this project. I searched for hours, but cannot find any available packages for this. Cannot believe it, so I must ask - are there any libs, or must somehow call these libraries from C# to do this?</p>
"
"78621519","How to parse search engine keywords input","2024-06-14 07:07:50","0","75","2","1","","78623063","<p>I'm implementing a tool that lets users search for terms in texts. I'm currently focused on handling more complex input from the search.</p>
<p>The operators I am looking to support are :</p>
<ul>
<li>| = OR</li>
<li>&amp; = AND</li>
<li>^ = NOT</li>
<li>&quot; &quot; = Quotes to escape everything in a sequence</li>
<li>( ) = Parentheses for giving precedence to the encapsulated</li>
</ul>
<p>This code should go into the Python backend that builds the query to the database engine, so I need a way to parse the query to transform the appropriate parts. Are there modules that would allow me to do that ?</p>
<p>I have tried looking at NLTK's logic package but it seems to do way too much things and it's not clear to me how to reduce it to these functions. I think I need something like NLTK's grammar trees but all I've found are packages that add grammatical tags and thus are tied to a language model.</p>
"
"78570279","Speeding up load time of LLMs","2024-06-03 12:30:11","3","206","0","2","","78584162","<p>I am currently only able to play around with a V100 on GCP. I understand that I can load a LLM in 4bit quantization as shown below. However, (assuming due to the quantization) it is taking up to 10 minutes to load this model.</p>
<p>Is there a way to speed up this loading process?</p>
<ol>
<li>I see that there is GGUF file format which may help in this regard (although I am not sure why/ how).</li>
<li>Would doing torch.compile somehow help me load the model next time in a fast manner. My hypothesis being that when compiled, I can save the resulting model in a binary format that can load faster?</li>
<li>Should I be baking the loaded model into the docker image somehow to speed this up? The downside being due to cuda the docker image is already at 4GB.</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

device = &quot;cuda&quot; # the device to load the model onto
model_id = &quot;mistralai/Mistral-7B-Instruct-v0.2&quot;
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type=&quot;nf4&quot;,
    bnb_4bit_compute_dtype=torch.bfloat16
)

model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=&quot;auto&quot;)
</code></pre>
"
"78538749","Not able to install spacy==2.3.5 version","2024-05-27 11:20:09","1","105","0","1","","78541534","<p>I tried to install spacy==2.3.5 for a resume analyser program. Encountered with a pip subprocess to install build dependencies did not run successfully error.</p>
<p>Using Python 3.12.3</p>
<p>Also it gives a E053 config file error when running the program regarding pyresparser:
&quot;OSError: [E053] Could not read config file from C:\Smart_Resume_Analyser_App-master.venv\Lib\site-packages\pyresparser\config.cfg&quot;</p>
<pre><code>`(.venv) PS C:\Smart_Resume_Analyser_App-master&gt; pip install spacy==2.3.5
</code></pre>
<p><code> </code></p>
<pre><code>    Getting requirements to build wheel did not run successfully.
    exit code: 1

    [267 lines of output]

    Error compiling Cython file:
    ------------------------------------------------------------
    ...
        len_t* widths
        int i
        int nr_layer
        int batch_size

        __init__(len_t* widths, int nr_layer, int batch_size) nogil:
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:140:4: function definition in pxd file must be declared 'cdef inline'

    Error compiling Cython file:
    ------------------------------------------------------------
    ...
            this._nr_feat = &lt;len_t*&gt;calloc(batch_size, sizeof(len_t))
            this._is_valid = &lt;int*&gt;calloc(batch_size * widths[nr_layer-1], sizeof(int))
            this._costs = &lt;weight_t*&gt;calloc(batch_size * widths[nr_layer-1], sizeof(weight_t))
            this.signatures = &lt;uint64_t*&gt;calloc(batch_size, sizeof(uint64_t))

        __dealloc__() nogil:
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:157:4: function definition in pxd file must be declared 'cdef inline'

    Error compiling Cython file:
    ------------------------------------------------------------
    ...
            free(this._nr_feat)
            free(this._is_valid)
            free(this._costs)
            free(this.signatures)

        void reset() nogil:
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:172:4: function definition in pxd file must be declared 'cdef inline'

    Error compiling Cython file:
    ------------------------------------------------------------
    ...
            for i in range(this.i):
                free(this._feats[i])
                this._feats[i] = NULL
            this.i = 0

        int nr_in() nogil:
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:189:4: function definition in pxd file must be declared 'cdef inline'

    Error compiling Cython file:
    ------------------------------------------------------------
    ...
            this.i = 0

        int nr_in() nogil:
            return this.widths[0]

        int nr_out() nogil:
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:192:4: function definition in pxd file must be declared 'cdef inline'

    Error compiling Cython file:
    ------------------------------------------------------------
    ...
            return this.widths[0]

        int nr_out() nogil:
            return this.widths[this.nr_layer - 1]

        int push_back(const FeatureC* feats, int nr_feat,
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:195:4: function definition in pxd file must be declared 'cdef inline'

    Error compiling Cython file:
    ------------------------------------------------------------
    ...
                for i in range(this.nr_out()):
                    this.is_valid(this.i)[i] = 1
            this.i += 1
            return this.i &gt;= this.batch_size

        FeatureC* features(int i) nogil:
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:226:4: function definition in pxd file must be declared 'cdef inline'

    Error compiling Cython file:
    ------------------------------------------------------------
    ...
            return this.i &gt;= this.batch_size

        FeatureC* features(int i) nogil:
            return this._feats[i]

        int nr_feat(int i) nogil:
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:229:4: function definition in pxd file must be declared 'cdef inline'

    Error compiling Cython file:
    ------------------------------------------------------------
    ...
            return this._feats[i]

        int nr_feat(int i) nogil:
            return this._nr_feat[i]

        weight_t* fwd(int i, int j) nogil:
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:232:4: function definition in pxd file must be declared 'cdef inline'

    Error compiling Cython file:
    ------------------------------------------------------------
    ...
            return this._nr_feat[i]

        weight_t* fwd(int i, int j) nogil:
            return this._fwd[i] + (j * this.widths[i])

        weight_t* bwd(int i, int j) nogil:
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:235:4: function definition in pxd file must be declared 'cdef inline'

    Error compiling Cython file:
    ------------------------------------------------------------
    ...
            return this._fwd[i] + (j * this.widths[i])

        weight_t* bwd(int i, int j) nogil:
            return this._bwd[i] + (j * this.widths[i])

        weight_t* scores(int i) nogil:
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:238:4: function definition in pxd file must be declared 'cdef inline'

    Error compiling Cython file:
    ------------------------------------------------------------
    ...
            return this._bwd[i] + (j * this.widths[i])

        weight_t* scores(int i) nogil:
            return this.fwd(this.nr_layer-1, i)

        weight_t* losses(int i) nogil:
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:241:4: function definition in pxd file must be declared 'cdef inline'

    Error compiling Cython file:
    ------------------------------------------------------------
    ...
            return this.fwd(this.nr_layer-1, i)

        weight_t* losses(int i) nogil:
            return this.bwd(this.nr_layer-1, i)

        weight_t* costs(int i) nogil:
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:244:4: function definition in pxd file must be declared 'cdef inline'

    Error compiling Cython file:
    ------------------------------------------------------------
    ...
            return this.bwd(this.nr_layer-1, i)

        weight_t* costs(int i) nogil:
            return this._costs + (i * this.nr_out())

        int* is_valid(int i) nogil:
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:247:4: function definition in pxd file must be declared 'cdef inline'

    Error compiling Cython file:
    ------------------------------------------------------------
    ...
            return this._costs + (i * this.nr_out())

        int* is_valid(int i) nogil:
            return this._is_valid + (i * this.nr_out())

        int guess(int i) nogil:
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:250:4: function definition in pxd file must be declared 'cdef inline'

    Error compiling Cython file:
    ------------------------------------------------------------
    ...
            return this._is_valid + (i * this.nr_out())

        int guess(int i) nogil:
            return VecVec.arg_max_if_true(this.scores(i), this.is_valid(i), this.nr_out())

        int best(int i) nogil:
        ^
    ------------------------------------------------------------

    thinc\structs.pxd:253:4: function definition in pxd file must be declared 'cdef inline'
    warning: thinc\linalg.pxd:14:0: The 'IF' statement is deprecated and will be removed in a future Cython version. Consider using runtime conditions or C macros instead. See https://github.com/cython/cython/issues/4310
    warning: thinc\linalg.pxd:90:8: The 'IF' statement is deprecated and will be removed in a future Cython version. Consider using runtime conditions or C macros instead. See https://github.com/cython/cython/issues/4310
    warning: thinc\linalg.pxd:174:8: The 'IF' statement is deprecated and will be removed in a future Cython version. Consider using runtime conditions or C macros instead. See https://github.com/cython/cython/issues/4310
    Compiling thinc/linalg.pyx because it changed.
    Compiling thinc/structs.pyx because it changed.
    Compiling thinc/typedefs.pyx because it changed.
    Compiling thinc/linear/avgtron.pyx because it changed.
    Compiling thinc/linear/features.pyx because it changed.
    Compiling thinc/linear/serialize.pyx because it changed.
    Compiling thinc/linear/sparse.pyx because it changed.
    Compiling thinc/linear/linear.pyx because it changed.
    Compiling thinc/neural/optimizers.pyx because it changed.
    Compiling thinc/neural/ops.pyx because it changed.
    Compiling thinc/neural/_aligned_alloc.pyx because it changed.
    Compiling thinc/extra/eg.pyx because it changed.
    Compiling thinc/extra/mb.pyx because it changed.
    Compiling thinc/extra/search.pyx because it changed.
    Compiling thinc/extra/cache.pyx because it changed.
    [ 1/15] Cythonizing thinc/extra/cache.pyx
    [ 2/15] Cythonizing thinc/extra/eg.pyx
    Traceback (most recent call last):
      File &quot;C:\Smart_Resume_Analyser_App-master\.venv\Lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py&quot;, line 353, in &lt;module&gt;
        main()
      File &quot;C:\Smart_Resume_Analyser_App-master\.venv\Lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py&quot;, line 335, in main
        json_out['return_val'] = hook(**hook_input['kwargs'])
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      File &quot;C:\Smart_Resume_Analyser_App-master\.venv\Lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py&quot;, line 118, in get_requires_for_build_wheel      
        return hook(config_settings)
               ^^^^^^^^^^^^^^^^^^^^^
      File &quot;C:\Users\vasud\AppData\Local\Temp\pip-build-env-iv7ops9s\overlay\Lib\site-packages\setuptools\build_meta.py&quot;, line 325, in get_requires_for_build_wheel
        return self._get_build_requires(config_settings, requirements=['wheel'])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      File &quot;C:\Users\vasud\AppData\Local\Temp\pip-build-env-iv7ops9s\overlay\Lib\site-packages\setuptools\build_meta.py&quot;, line 295, in _get_build_requires
        self.run_setup()
      File &quot;C:\Users\vasud\AppData\Local\Temp\pip-build-env-iv7ops9s\overlay\Lib\site-packages\setuptools\build_meta.py&quot;, line 311, in run_setup
        exec(code, locals())
      File &quot;&lt;string&gt;&quot;, line 258, in &lt;module&gt;
      File &quot;&lt;string&gt;&quot;, line 195, in setup_package
      File &quot;C:\Users\vasud\AppData\Local\Temp\pip-build-env-iv7ops9s\overlay\Lib\site-packages\Cython\Build\Dependencies.py&quot;, line 1154, in cythonize
        cythonize_one(*args)
      File &quot;C:\Users\vasud\AppData\Local\Temp\pip-build-env-iv7ops9s\overlay\Lib\site-packages\Cython\Build\Dependencies.py&quot;, line 1321, in cythonize_one
        raise CompileError(None, pyx_file)
    Cython.Compiler.Errors.CompileError: thinc/extra/eg.pyx
    [end of output]

    note: This error originates from a subprocess, and is likely not a problem with pip.
  error: subprocess-exited-with-error

  Getting requirements to build wheel did not run successfully.
  exit code: 1

  See above for output.

  note: This error originates from a subprocess, and is likely not a problem with pip.
  [end of output]
</code></pre>
<p>note: This error originates from a subprocess, and is likely not a problem with pip.
error: subprocess-exited-with-error</p>
<p>× pip subprocess to install build dependencies did not run successfully.
│ exit code: 1
╰─&gt; See above for output.</p>
<p>note: This error originates from a subprocess, and is likely not a problem with pip.
`</p>
"
"78530360","Using torchrun with AWS sagemaker estimator on multi-GPU node","2024-05-24 19:25:29","1","105","0","1","","78545500","<p>I would like to run a training job ml.p4d.24xlarge machine on AWS SageMaker. I ran into a similar issue described <a href=""https://github.com/huggingface/transformers/issues/28916"" rel=""nofollow noreferrer"">here</a> with significant slowdowns in training time. I understand now that I should run it with torchrun. My constraints are that I don't want to use the HuggingFace or PyTorch estimators from SageMaker (for customizability and to properly understand the stack).</p>
<p>Currently, the entrypoint to my container is set as such in my Dockerfile:</p>
<p><code>ENTRYPOINT [&quot;python3&quot;, &quot;/opt/program/entrypoint.py&quot;]</code></p>
<p>How should I change it, and can I change it to use torchrun instead? Is it just a matter of setting:</p>
<p><code>ENTRYPOINT [&quot;torchrun --nproc_per_node 8&quot;, &quot;/opt/program/entrypoint.py&quot;]</code></p>
"
"78489915","How to lemmatize text column in pandas dataframes using stanza?","2024-05-16 12:34:50","0","53","0","1","","78491545","<p>I read csv file into pandas dataframe.</p>
<p>my text column is df['story'].</p>
<p>how do I lemmatize  this colummn ?</p>
<p>should I tokenize before?</p>
"
"78461078","How to optimize this function and improve running time?","2024-05-10 15:18:00","0","37","2","1","","78461789","<p>I have function aimed at creating a data-frame with three columns; bigram-phrase, count ( of the bigram-phrase), and PMI score ( for the bigram-phrase). Since I want to run this on a large dataset with over a million phrases, the compute time is incredibly long. I recognize that the nested for-loops and matching conditions are contributing to the computation difficulties. Is there an alternative way to do the same thing and cut down run-time?
Here's my code:</p>
<pre><code>
def pmi_count_phrase_create(pmi_tups,freq_list):

    import pandas as pd

    &quot;&quot;&quot;pmi_tups is result of running pmi_tups = [i for i in finder.score_ngrams(bigram_measures.pmi)]  
       freq_list is a result of running freq_list= finder.ngram_fd.items() 
       
       -&gt; df made up of columns for  pmi list, count list, phrase list&quot;&quot;&quot;
    pmi3_list =[]
    count3_list =[]
    phrase3_list =[]
    for phrase, pmi in pmi_tups: #pmi_tups is list of tuples of form:[((phrase),pmi),..]
        for item in freq_list:  
            quadgram,count = item
            if quadgram == phrase:
                pmi3_list.append(pmi)
                count3_list.append(count)
                phrase3_list.append(phrase)

                # create dataframe
    df = pd.DataFrame({'Phrase':phrase3_list,'PMI':pmi3_list,'Count':count3_list})
    return df 
</code></pre>
<p>Running this code on my pmi_tups and freq_list, it is still running and it's been over 1000 minutes. I'm open to also using a different libarary to evaluate the bi-gram phrases, pmi's and frequencies.</p>
"
"78443980","FastText language_identification in R returns too many arguments - how to match to texts?","2024-05-07 16:46:21","1","70","0","1","","78444037","<p>FastText language_identification returns multiple predictions per original text, and also fails to indicate which belong to which original document.</p>
<p>There are differing numbers of predictions per original document too -- their GitHub forums are closed now, but does anyone know how to match the output to the original texts?</p>
<p>Code:</p>
<pre><code>DF = data.frame(doc_id = seq(1, 5),
speechtext = c(&quot;Hello. Fake text entry 1.&quot;, &quot;Fake text entry 2&quot;, &quot;more text&quot;, &quot;Text in a
different language&quot;, &quot;Hola&quot;))

library(fastText)
# download .ftz pretrained model from https://fasttext.cc/docs/en/language-identification.html
file_ftz = system.file(&quot;language_identification/lid.176.ftz&quot;, package = &quot;fastText&quot;)
lang1 = language_identification(DF$speechtext,
                                pre_trained_language_model_path = file_ftz,
                                verbose = T)
</code></pre>
<p>I was expecting one prediction per original text, or at least a consistent number, or some way of marking which document the predictions align with.</p>
<p>Really I could guess based on the largest number per series of a few elements outputted, but this doesn't seem optimal -- it does seem like a bug.</p>
<p>(I tried adding intern = T as an argument per <a href=""https://stackoverflow.com/questions/65130621/r-fasttext-how-to-load-output-into-a-dataframe-from-command-line"">R - fasttext how to load output into a dataframe from command line</a> -- this is not recognized as an argument).</p>
"
"78423352","SpaCy GPU memory utilization for NER training","2024-05-03 07:52:34","2","62","0","1","","78506143","<p>My training code:</p>
<pre><code>spacy.require_gpu()
nlp = spacy.blank('en')

if 'ner' not in nlp.pipe_names:
    ner = nlp.add_pipe('ner')
else:
    ner = nlp.get_pipe('ner')

docs = load_data(ANNOTATED_DATA_FILENAME_BIN)
train_data, test_data = split_data(docs, DATA_SPLIT)

unique_labels = set(ent.label_ for doc in train_data for ent in doc.ents)
for label in unique_labels:
    ner.add_label(label)

optimizer = nlp.initialize()

for i in range(EPOCHS):
    print(f&quot;Starting Epoch {i+1}...&quot;)
    losses = {}
    batches = minibatch(train_data, size=compounding(4., 4096, 1.001))
    for batch in batches:
        for doc in batch:
            example = Example.from_dict(doc, {'entities': [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]})
            nlp.update([example], drop=0.5, losses=losses, sgd=optimizer)
    print(f&quot;Losses at iteration {i}: {losses}&quot;)
</code></pre>
<p>This code almost completely does not utilize GPU memory. Utilization is about 11-13% during training, which is almost the same as idle.</p>
<p><a href=""https://i.sstatic.net/82YRwe4T.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/82YRwe4T.jpg"" alt=""nvidia-smi"" /></a></p>
<p>I did allocation test with torch, and all 8Gigs are allocated, so server works fine.
The problem is with SpaCy or my code.</p>
<p>Could you please help?</p>
"
"78393709","Performance of textSimilarity() from R's text library","2024-04-27 03:15:58","2","178","3","2","","78407238","<p>I have a large <code>data.frame</code> with about 4 million rows and 2 columns.</p>
<p>The two columns contain long character strings, texts representing recipes.</p>
<p>For each row, I am comparing the similarity of the recipes in column A and column B, using textSimilarity() from the <code>text</code> library in R.</p>
<p>I like the textSimilarity() function, because it uses text embeddings that &quot;comprises values that represent the latent meaning of a word&quot;.</p>
<p>Yet, performance is very slow. Are there ways of speeding this up? Or am I coding this wrong? Can/should I set this up in parallel? Can I use my GPU?</p>
<p>Example data - with way shorter texts:</p>
<pre><code>df &lt;- data.frame(
columnA= c(&quot;tomato sauce is very tasty to use&quot;, &quot;without garlic, this dish is not chinese&quot;, &quot;British food is as tasteless as it can get&quot;), 
columnB= c(&quot;pizza is the source of life&quot;, &quot;a nice xiaolongbao is steamed until it is soft&quot;, &quot;braised pork can be very healthy if prepared well&quot;)
)
</code></pre>
<pre><code>&gt; df
                                     columnA                                           columnB
1          tomato sauce is very tasty to use                       pizza is the source of life
2   without garlic, this dish is not chinese    a nice xiaolongbao is steamed until it is soft
3 British food is as tasteless as it can get braised pork can be very healthy if prepared will
</code></pre>
<p>To get the similarity, I use:</p>
<pre><code>df$sim &lt;- textSimilarity(textEmbed(df$columnA)$texts$texts , textEmbed(df$columnB)$texts$texts)
</code></pre>
<p>In the current set-up, this process takes days rather than hours. How to speed this up? Parallelization? GPU? Or are there alternatives?</p>
"
"78340299","Diffrence between gguf and lora","2024-04-17 10:30:18","0","272","0","1","","78341625","<p>Does the gguf format perform model quantization even though it's already quantized with LORA?</p>
<p>Hello ! im new to Llms ,and l've fine-tuned the CODELLAMA model on kaggle using LORA.I've merged and pushed it to  hugging face.I want to know if the model is already quantized with LORA why we need to requantized with gguf .</p>
"
"78267762","Quantization and torch_dtype in huggingface transformer","2024-04-03 12:48:06","1","603","0","1","","78272137","<p>Not sure if its the right forum to ask but.</p>
<p>Assuming i have a <code>gptq</code> model that is <code>4bit</code>. how does using <code>from_pretrained(torch_dtype=torch.float16)</code> work? In my understanding 4 bit meaning changing the weights from either <code>32-bit precision</code> to <code>4bit precision</code> using quantization methods.</p>
<p>However, calling it the <code>torch_dtype=torch.float16</code> would mean the weights are in <code>16 bits</code>? Am i missing something here.</p>
"
"78215873","Comparison between stemmiation and lemmatization","2024-03-24 19:41:33","1","89","0","1","","78216510","<p>Based on several research , i found following important  compartive analysis :</p>
<p><a href=""https://i.sstatic.net/6C0H7.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/6C0H7.png"" alt=""comparative analysis"" /></a></p>
<p>if we look on texts, most probably lemmatization should return more correct  output right? not only correct, but also shortened version, i have  made  a experiment on this line :</p>
<pre><code>sentence =&quot;having playing  in today gaming ended with greating victorious&quot;
</code></pre>
<p>but when i have run code  for both lemmatizer and stemmization, i got following result :
<code>['have', 'play', 'in', 'today', 'game', 'end', 'with', 'great', 'victori'] ['having', 'playing', 'in', 'today', 'gaming', 'ended', 'with', 'greating', 'victorious']</code></p>
<p>first one is  stemming   and  everything looks like fine except victori(it should be victory right) and second one is lemmatization(all of them  are correct but in original form), so in this case which option is good?short version  and mostly incorrect or long version and correct?</p>
<pre><code>        import nltk
        from nltk.tokenize import word_tokenize,sent_tokenize
        from nltk.corpus import stopwords
        from sklearn.feature_extraction.text import  CountVectorizer
        from nltk.stem import PorterStemmer,WordNetLemmatizer
        mylematizer =WordNetLemmatizer()
        mystemmer =PorterStemmer()
        nltk.download('stopwords')
        sentence =&quot;having playing  in today gaming ended with greating victorious&quot;
        words =word_tokenize(sentence)
        # print(words)
        stemmed =[mystemmer.stem(w)  for w in words]
        lematized=[mylematizer.lemmatize(w) for w in words ]
        print(stemmed)
        print(lematized)
        # mycounter =CountVectorizer()
        # mysentence =&quot;i love ibsu. because ibsu is great university&quot;
        # # print(word_tokenize(mysentence))
        # # print(sent_tokenize(mysentence))
        # individual_words=word_tokenize(mysentence)
        # stops =list(stopwords.words('english'))
        # words =[w  for w in  individual_words if w not in  stops  and  w.isalnum() ]
        # reduced =[mystemmer.stem(w) for w  in words]
        
        # new_sentence =' '.join(words)
        # frequencies =mycounter.fit_transform([new_sentence])
        # print(frequencies.toarray())
        # print(mycounter.vocabulary_)
        # print(mycounter.get_feature_names_out())
        # print(new_sentence)
        # print(words)
        # # print(list(stopwords.words('english')))
</code></pre>
"
"78155250","Langchain/Huggingface Pipeline Error about model_kwargs which I did not include","2024-03-13 16:06:55","0","1027","0","1","","78157893","<p>I am currently trying to use the Helsinki-NLP/opus-mt-en-de and de-en models. I was trying to setup a pipeline and use both as LLMChain but I keep getting the same error:</p>
<pre><code>ValueError: The following `model_kwargs` are not used by the model: ['pipeline_kwargs', 'return_full_text'] (note: typos in the generate arguments will also show up in this list)
</code></pre>
<p>I used the following snippet to initialise both models and ran the snippet after to test the output:</p>
<pre class=""lang-py prettyprint-override""><code>def get_translation_chains():
    _de_en_translation_prompt = PromptTemplate.from_template(
        &quot;&quot;&quot;Translate the following text from German to English:
        {text}
        &quot;&quot;&quot;
    )

    _en_de_translation_prompt = PromptTemplate.from_template(
        &quot;&quot;&quot;Translate the following text from English to German:
        {text}
        &quot;&quot;&quot;
    )

    _en_to_de_tokenizer = AutoTokenizer.from_pretrained(&quot;Helsinki-NLP/opus-mt-en-de&quot;)
    _en_to_de_model = AutoModelForSeq2SeqLM.from_pretrained(&quot;Helsinki-NLP/opus-mt-en-de&quot;)
    _de_to_en_tokenizer = AutoTokenizer.from_pretrained(&quot;Helsinki-NLP/opus-mt-de-en&quot;)
    _de_to_en_model = AutoModelForSeq2SeqLM.from_pretrained(&quot;Helsinki-NLP/opus-mt-de-en&quot;)

    _en_to_de_pipeline = pipeline(
        model=_en_to_de_model,
        tokenizer=_en_to_de_tokenizer,
        task=&quot;translation&quot;,
    )

    _de_to_en_pipeline = pipeline(
        model=_de_to_en_model,
        tokenizer=_de_to_en_tokenizer,
        task=&quot;translation&quot;,
    )

    _de_to_en_llm = HuggingFacePipeline(pipeline=_de_to_en_pipeline)
    _en_to_de_llm = HuggingFacePipeline(pipeline=_en_to_de_pipeline)

    _de_to_en_chain = LLMChain(
        prompt=_de_en_translation_prompt,
        llm=_de_to_en_llm,
    )

    _en_to_de_chain = LLMChain(
        prompt=_en_de_translation_prompt,
        llm=_en_to_de_llm,
    )

    return _en_to_de_chain, _de_to_en_chain


</code></pre>
<pre class=""lang-py prettyprint-override""><code>en_to_de_chain, de_to_en_pipeline = get_translation_chains()

print(en_to_de_chain.invoke({&quot;text&quot;: &quot;Hello, how are you?&quot;}))
</code></pre>
<p>I am fairly new to using LLMs and both the huggingface and langchain libraries and could not find anything to give me a clue on this one.</p>
<p>I tried to use the pipeline with only setting the task I wanted &quot;translation_de_to_en&quot; and the other way around as well as using &quot;translation&quot; only for both default and more detailed pipeline. I also tried to set the kwargs option to None and False but with no success</p>
"
"78150042","Deploying LLM on Sagemaker Endpoint - CUDA out of Memory","2024-03-12 21:05:17","-1","703","6","1","","78350517","<p>I am trying to deploy huggingface LLM (for inference) to Sagemaker Endpoint using custom scripts (Using Pytorch framework with model and inference script zipped as .tar.gz file).  The tar.gz file structure is:</p>
<pre><code>model.tar.gz/
|- pytorch_model.bin
|- ....
|- code/
  |- inference.py
  |- requirements.txt 
</code></pre>
<p>In inference.py, I have defined functions model_fn and predict_fn.</p>
<p>This tar.gz file is uploaded to S3 and the model while deployment is being picked from this S3 location.</p>
<p>I have followed the process defined in <a href=""https://huggingface.co/docs/sagemaker/en/inference"" rel=""nofollow noreferrer"">https://huggingface.co/docs/sagemaker/en/inference</a> --&gt; Sections: <em>Create a model artifact for deployment</em> and <em>User defined code and modules</em></p>
<p>After following all these steps, I am getting an error :</p>
<blockquote>
<p>CUDA out of memory. Tried to allocate 20.00 MiB. GPU 1 has a total
capacty of 22.20 GiB of which 13.12 MiB is free. Process 13234 has
2.25 GiB memory in use. Process 13238 has 3.82 GiB memory in use. Process 13236 has 8.06 GiB memory in use. Process 13239 has 8.06 GiB
memory in use. Of the allocated memory 6.93 GiB is allocated by
PyTorch, and 49.59 MiB is reserved by PyTorch but unallocated. If
reserved but unallocated memory is large try setting max_split_size_mb
to avoid fragmentation.  See documentation for Memory Management and
PYTORCH_CUDA_ALLOC_CONF : 400</p>
</blockquote>
<p>My model is an LLM with 7b parameters and compute is ml.g5.12x (192 GB  and GPU 24 GB x 4). The memory is more than sufficient (as I was getting this error, I tried such a large compute) and the code I have tried is using AutoModelForCausalLM.from_pretrained and Autotokenizer.from_pretrained.  I have tried device maps of &quot;auto&quot;, balanced_low_0, and balanced. The memory on GPU is sufficient to start with (as checked by me from memory summary)</p>
<p>The thing is I was able to get a response for a couple of pings and then I started getting this error. I am clearing the cache in my predict function but still I am getting this error.</p>
<p>How can I resolve my out-of-memory error? I get out of memory error either right at the start or my memory of GPU fills incrementally with each inference.</p>
"
"78060804","How to get the SHAP value per class?","2024-02-26 12:08:54","0","192","0","1","","78068645","<p>I want to get shap value per class. I have checked tutorial and I found below example how to do this. However, the code do not work because of <code>shap_value.shape</code> is (10,None,6). 10 is your the number of samples, 4 is class.</p>
<pre class=""lang-py prettyprint-override""><code>import datasets
import pandas as pd
import transformers
import shap

dataset = datasets.load_dataset(&quot;emotion&quot;, split=&quot;train&quot;)
data = pd.DataFrame({&quot;text&quot;: dataset[&quot;text&quot;], &quot;emotion&quot;: dataset[&quot;label&quot;]})

# load the model and tokenizer
tokenizer = transformers.AutoTokenizer.from_pretrained(
    &quot;nateraw/bert-base-uncased-emotion&quot;, use_fast=True
)
model = transformers.AutoModelForSequenceClassification.from_pretrained(
    &quot;nateraw/bert-base-uncased-emotion&quot;
).cuda()

# build a pipeline object to do predictions
pred = transformers.pipeline(
    &quot;text-classification&quot;,
    model=model,
    tokenizer=tokenizer,
    device=0,
    return_all_scores=True,
)
explainer = shap.Explainer(pred)
shap_values = explainer(data[&quot;text&quot;][:3])
shap.plots.bar(shap_values[:, :, &quot;joy&quot;].mean(0))
</code></pre>
<p>Are there any way to get bar plot for per class?</p>
"
"78020235","understanding looping real inference call","2024-02-19 11:15:23","0","52","0","1","","78022307","<p>I was looking for a solution for an issue I was having with my interface speed. I saw this <a href=""https://forums.developer.nvidia.com/t/inference-time-hugging-face-detr/274721"" rel=""nofollow noreferrer"">answer</a> online but I don't understand what the solution was. The person is using a hugging face model with pytorch and the solution was to loop the real inference call to increase the GPU utilization. This was caused by a bottleneck from accessing the data. Can I get some help understanding how to implement this solution?
Thanks</p>
"
"77824012","Pytorch LayerNorm’s mean and std div are not fixed while inferencing","2024-01-16 07:15:28","0","425","1","1","","78059111","<p>I’m working on recreating the input after torch.LayerNorm. As far as I know, the mean and standard deviation for LayerNorm are fixed during the inference phase. Therefore, I thought I could extract these factors and recreate the original input from the LayerNorm output.</p>
<p>I have successfully extracted the weight and bias, which are not necessarily identical to the mean and standard deviation because LayerNorm has its own weight and bias parameters. My weight and bias parameters are fused from various factors, but they successfully recreate the original input from the LayerNorm output.</p>
<p>However, when I applied these extracted weight and bias parameters to another input tensor and expected LayerNorm to work in the same way as with the previous input, I obtained a completely different output. I assumed that LayerNorm calculated new mean and standard deviation values for the second input, causing the difference. But I’m puzzled as to why LayerNorm computed the mean and standard deviation for the second input; they should have remained fixed during inference.
below is my code</p>
<pre><code>layer = layer().eval()
with torch.inference_mode():
    out = layer(input_data)

w = torch.zeros(len(out[0, :, 0]))
b = torch.zeros(len(out[0, :, 0]))

for i in range(len(out[0, :, 0])):
    w[i] = (input_data[0, i, 0] - input_data[0, i, 10]) / (out[0, i, 0] - out[0, i, 10])
    b[i] = (input_data[0, i, 0] * out[0, i, 10] - input_data[0, i, 10] * out[0, i, 0]) / (out[0, i, 10] - out[0, i, 0])

for i1 in range(len(input_remade[0, :, 0])):
    input_remade[0, i1, :] = out[0, i1, :] * w[i1] + b[i1]
print(torch.sum(input_remade - input_data))


input_data2 = torch.randn(1, 577, 768)
input_remade2 = torch.randn(1, 577, 768)
with torch.inference_mode():
    out2 = layer(input_data2)

for i1 in range(len(input_remade2[0, :, 0])):
    input_remade2[0, i1, :] = out2[0, i1, :] * w[i1] + b[i1]
print(torch.sum(input_remade2 - input_data2))

w1 = torch.zeros(len(out2[0, :, 0]))
b1 = torch.zeros(len(out2[0, :, 0]))

for i in range(len(out2[0, :, 0])):
    w1[i] = (input_data2[0, i, 0] - input_data2[0, i, 10]) / (out2[0, i, 0] - out2[0, i, 10])
    b1[i] = (input_data2[0, i, 0] * out2[0, i, 10] - input_data2[0, i, 10] * out2[0, i, 0]) / (out2[0, i, 10] - out2[0, i, 0])

for i1 in range(len(input_remade2[0, :, 0])):
    input_remade2[0, i1, :] = out2[0, i1, :] * w1[i1] + b1[i1]
print(torch.sum(input_remade2 - input_data2))
</code></pre>
<pre><code>tensor(-0.0061)
tensor(1280.9966)
tensor(0.0014)
</code></pre>
<p>Or is there Any way to extracte fixed mean and standard deviation from LayerNorm layer?</p>
"
"77609784","How to make stanza lemmatizer to return just the lemma instead of a dictionary?","2023-12-05 23:30:30","0","231","1","1","","77609905","<p>I'm implementing stanza's lemmatizer because it works well with spanish texts but the lemmatizer retuns a whole dictionary with ID and other characteristics I don't care about for the time being. I checked the &quot;processors&quot; in the pipeline but I don't seem to find and example where I just get the sence with the lemmatized text instead of the dictionary.</p>
<p>This is what I have:</p>
<pre><code>stanza.download('es', package='ancora', processors='tokenize,mwt,pos,lemma', verbose=False)
stNLP = stanza.Pipeline(processors='tokenize,mwt,pos,lemma', lang='es', use_gpu=True)
stNLP('me hubiera gustado mas “sincronia” con la primaria')
</code></pre>
<p>Output:</p>
<pre><code>[
  [
    {
      &quot;id&quot;: 1,
      &quot;text&quot;: &quot;me&quot;,
      &quot;lemma&quot;: &quot;yo&quot;,
      &quot;upos&quot;: &quot;PRON&quot;,
      &quot;xpos&quot;: &quot;pp1cs000&quot;,
      &quot;feats&quot;: &quot;Case=Dat|Number=Sing|Person=1|PrepCase=Npr|PronType=Prs&quot;,
      &quot;start_char&quot;: 0,
      &quot;end_char&quot;: 2
    },
....
</code></pre>
<p>Of course when I try to lemmatize my document it returns a lot of text I don't need at the moment, how can I just obtain the lemma? I'm aware I could possibly extract the word from the dictionary but it takes a lot of time as it is, what I want to avoid is giving the fuction extra work.</p>
<p>Thank you in advance.</p>
"
"77594086","How to run a NLP+Transformers LLM on low memory GPUs?","2023-12-03 11:32:25","2","821","1","1","","77600312","<p>I am trying to load an AI pre-trained model, from intel on hugging face, I have used Colab its resources exceeded, used Kaggle resources increased, used paperspace, which showing me an error:</p>
<pre><code>The kernel for Text_Generation.ipynb appears to have died. It will restart automatically.
</code></pre>
<p>this is the model load script:</p>
<pre><code>import transformers


model_name = 'Intel/neural-chat-7b-v3-1'
model = transformers.AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)

def generate_response(system_input, user_input):

    # Format the input using the provided template
    prompt = f&quot;### System:\n{system_input}\n### User:\n{user_input}\n### Assistant:\n&quot;

    # Tokenize and encode the prompt
    inputs = tokenizer.encode(prompt, return_tensors=&quot;pt&quot;, add_special_tokens=False)

    # Generate a response
    outputs = model.generate(inputs, max_length=1000, num_return_sequences=1)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # Extract only the assistant's response
    return response.split(&quot;### Assistant:\n&quot;)[-1]


# Example usage
system_input = &quot;You are a math expert assistant. Your mission is to help users understand and solve various math problems. You should provide step-by-step solutions, explain reasonings and give the correct answer.&quot;
user_input = &quot;calculate 100 + 520 + 60&quot;
response = generate_response(system_input, user_input)
print(response)

# expected response
&quot;&quot;&quot;
To calculate the sum of 100, 520, and 60, we will follow these steps:

1. Add the first two numbers: 100 + 520
2. Add the result from step 1 to the third number: (100 + 520) + 60

Step 1: Add 100 and 520
100 + 520 = 620

Step 2: Add the result from step 1 to the third number (60)
(620) + 60 = 680

So, the sum of 100, 520, and 60 is 680.
&quot;&quot;&quot;
</code></pre>
<p>My purpose is to load this pretrained model, I have done some research on my end I have find some solutions but not working with me,</p>
<blockquote>
<p>download packages using cuda instead of pip</p>
</blockquote>
"
"77511368","LLM slow inference even on A100 GPU","2023-11-19 15:31:45","0","977","0","3","","77592933","<p>I am planning to deploy a fine-tuned version of Open-Orca-Platypus-2. It takes around 13.5GB on the GPU. I tried using g4dn.12xlarge in AWS which has 4 GPUs, but the inference still takes around 40 seconds. I also tried it on A100 GPU provided by Colab, but still the same.</p>
<p>What am I doing wrong? Do I still need more computational power or is anything wrong with my code?</p>
<pre><code>
    from transformers import AutoTokenizer, AutoModelForCausalLM
    import torch
    import os

    os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &quot;0,1,2,3&quot;
    device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)

    # Load model
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        device_map=&quot;auto&quot;
    )

    # Set the model to evaluation mode
    model.eval()

    # Load tokenizer
    tokenizer = AutoTokenizer.from_pretrained(&quot;Open-Orca/OpenOrca-Platypus2-13B&quot;, trust_remote_code=True)

    def ask_bot(question):
        with torch.no_grad():
            # Tokenize input question
            input_ids = tokenizer.encode(question, return_tensors=&quot;pt&quot;).cuda()

            # Generate output
            output = model.module.generate(
                input_ids,
                max_length=200,
                num_return_sequences=1,
                do_sample=True,
                top_k=50
            )

        # Decode and extract the response
        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
        response = generated_text.split(&quot;-&gt;:&quot;)[-1]
        return response
</code></pre>
"
"77482126","OpenAI API error: ""You tried to access openai.Model, but this is no longer supported in openai\>=1.0.0""","2023-11-14 16:06:57","0","2387","0","2","","77482219","<p>Using Visual Studio Code and PyCharm, after install openai (pip install openai) a strange error is bugging me - please help.</p>
<p>If for example I write:</p>
<pre><code>import openai

openai.api_key = &quot;sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx&quot;

lista_de_modelos = openai.Model.list()
print(lista_de_modelos)
</code></pre>
<p>it fails and I get an this error:</p>
<pre><code>PS C:\\proyectoVS_Python\&gt; &amp; &quot;C:/Users/kitkatuser/AppData/Local/Programs/Python/Python312/python.exe&quot; &quot;c:/proyectoVS_Python/import os.py&quot;
Traceback (most recent call last):
File &quot;c:\\proyectoVS_Python\\import os.py&quot;, line 5, in \&lt;module\&gt;
lista_de_modelos = openai.Model.list()
^^^^^^^^^^^^^^^^^
File &quot;C:\\Users\\kitkatuser\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai_utils_proxy.py&quot;, line 22, in __getattr__
return getattr(self.__get_proxied__(), attr)
^^^^^^^^^^^^^^^^^^^^^^
File &quot;C:\\Users\\kitkatuser\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai_utils_proxy.py&quot;, line 43, in __get_proxied__  
return self.__load__()
^^^^^^^^^^^^^^^
File &quot;C:\\Users\\kitkatuser\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\lib_old_api.py&quot;, line 33, in __load__
raise APIRemovedInV1(symbol=self.\_symbol)
openai.lib.\_old_api.APIRemovedInV1:

You tried to access openai.Model, but this is no longer supported in openai\&gt;=1.0.0 - see the README at https://github.com/openai/openai-python for the API.

You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface.

Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`

A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742

PS C:\\proyectoVS_Python\&gt;
</code></pre>
<p>What I´m doing wrong? why I can't access openAI, I've try several keys, the same program and process to install works well with other friends. Using Pycharm shows similar. I'm used several programs to try but always similar response! I don't find a solution or similar problems! I'm really confused!Please Help</p>
"
"77433100","How to get perplexity per token rather than average perplexity?","2023-11-06 17:30:20","1","897","0","2","","77433933","<p>I can get the perplexity of a whole sentence from <a href=""https://huggingface.co/docs/transformers/perplexity"" rel=""nofollow noreferrer"">here</a>:</p>
<pre><code>device = &quot;cuda&quot;
from transformers import GPT2LMHeadModel, GPT2TokenizerFast

device = &quot;cuda&quot;
model_id = &quot;gpt2&quot;
model = GPT2LMHeadModel.from_pretrained(model_id).to(device)
tokenizer = GPT2TokenizerFast.from_pretrained(model_id)
sent = 'Happy Birthday!'
input_ids = tokenizer(sent, return_tensors='pt')['input_ids']
target_ids = input_ids.clone()
outputs = model(input_ids.to(device), labels=target_ids)
ppl = torch.exp(outputs.loss)
print(ppl)
&gt;&gt;&gt;tensor(1499.6934, device='cuda:0', grad_fn=&lt;ExpBackward0&gt;)
</code></pre>
<p>But how can I get the perplexity value for each token, instead of of the average perplexity of the entire sequence of tokens? The input sentence in this example, <code>'Happy Birthday!'</code> is composed of 3 tokens. Based on the formula for perplexity:
<a href=""https://i.sstatic.net/pCsqc.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/pCsqc.png"" alt=""enter image description here"" /></a></p>
<p>This should result in 3 values: log probability of the first token, log probability of the second token given the first, and the log probability of the third token given the first 2. Each should be exponentiated to get the perplexity value of each token.</p>
<p>I currently have the following:</p>
<pre><code>import torch
from transformers import GPT2LMHeadModel, GPT2TokenizerFast

device = &quot;cuda&quot;
model_id = &quot;gpt2&quot;
model = GPT2LMHeadModel.from_pretrained(model_id).to(device)
tokenizer = GPT2TokenizerFast.from_pretrained(model_id)

sent = 'Happy Birthday!'
input_ids = tokenizer(sent, return_tensors='pt')['input_ids'].to(device)
target_ids = input_ids.clone()

# Initialize an empty list to store perplexities for each token
perplexities = []

# Calculate perplexity for each token
for i in range(input_ids.shape[1]):
    output = model(input_ids[:, :i+1], labels=target_ids[:, :i+1])
    log_prob = output.loss.item()
    perplexity = torch.exp(torch.tensor(log_prob))
    perplexities.append(perplexity.item())

# Perplexities is now a list containing the perplexity values for each token
for i, token in enumerate([tokenizer.decode(i) for i in input_ids[0]]):
    print(f&quot;Token: {token}, Perplexity: {perplexities[i]}&quot;)
    &gt;&gt;&gt; Token: Happy, Perplexity: nan
Token:  Birthday, Perplexity: 54192.46484375
Token: !, Perplexity: 1499.693359375
</code></pre>
<p>But I'm not sure what I'm doing wrong, as the last token seem to have the same perplexity as the entire sentence.</p>
"
"77427999","Custom spaCy tagger to tag all words that are in a dictionary","2023-11-05 23:07:34","1","140","0","1","","77429721","<p>I'm trying spaCy to extract specific information from a text.
So I need to configure a custom tokenizer to identify them and a custom tagger to label all the words that are in an external dictionary in JSON format.</p>
<p>The tokenizer worked on several attempts, but the labeler has been having problems when processing simple text.
I hope that the label I will add to the words is a custom POS-Tag &quot;UNM&quot; and that I can attribute it to token.pos_ like all other labels &quot;NOUN&quot;, &quot;VERB&quot;, etc.</p>
<pre><code>import requests

#keywords dictionary
dictionary = requests.get(
    &quot;https://github.com/dglopes/NBR15575/raw/main/unidades_medidas.json&quot;).json()

    
#Creating the Custom Tagger
Doc.set_extension('pos_tag', default=None, force=True)

@Language.factory(&quot;keyword_pos_tagger&quot;)
class KeywordPosTagger:
   def __init__(self, name, nlp, keywords, pos_tag):
       self.keywords = keywords
       self.pos_tag = pos_tag
       #Doc.set_extension('pos_tag', default=None, force=True)

   def __call__(self, doc):
       for token in doc:
           if token.text in self.keywords:
               token._.pos_tag = self.pos_tag
       return doc

nlp = spacy.load('pt_core_news_md')


keywords = ('m²', 'm2', '(W/K)', 'ºC')
pos_tag = 'UNM' # substitua por seu rótulo POS

keyword_pos_tagger = KeywordPosTagger(nlp, 'keyword_pos_tagger', keywords, pos_tag)

config = {&quot;nlp&quot;: nlp, &quot;keywords&quot;: keywords, &quot;pos_tag&quot;: pos_tag}

nlp.add_pipe('keyword_pos_tagger', config = config)
</code></pre>
<p>&lt;<strong>main</strong>.KeywordPosTagger at 0x78d568e4cee0&gt;</p>
<p>And when I use the custom tagger:</p>
<pre><code>doc = nlp('A temperatura tem 159ºC ou 20 ºC. Também precisa ter 20m de largura e 14 m² de área, caso contrário terá 1 Kelvin (W/K)')
for token in doc:
   print(token.text, token._.pos_tag)
</code></pre>
<p>it returns this error</p>
<pre><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-5-3c241e1c89fd&gt; in &lt;cell line: 1&gt;()
----&gt; 1 doc = nlp('A temperatura tem 159ºC ou 20 ºC. Também precisa ter 20m de largura e 14 m² de área, caso contrário terá 1 Kelvin (W/K)')
      2 for token in doc:
      3    print(token.text, token._.pos_tag)

4 frames
/usr/local/lib/python3.10/dist-packages/spacy/tokens/underscore.py in __setattr__(self, name, value)
     74     def __setattr__(self, name: str, value: Any):
     75         if name not in self._extensions:
---&gt; 76             raise AttributeError(Errors.E047.format(name=name))
     77         default, method, getter, setter = self._extensions[name]
     78         if setter is not None:

AttributeError: [E047] Can't assign a value to unregistered extension attribute 'pos_tag'. Did you forget to call the `set_extension` method?
</code></pre>
"
"77210041","troubleshooting PyTorch and Hugging Face's Pre-trained deBerta Model on Windows 11 with an RTX 3070 GPU","2023-10-01 10:15:10","0","427","4","1","","77222088","<p>I'm running Windows 11 on my desktop, which has an NVIDIA RTX 3070 GPU. I'm working on an NLP task using Hugging Face's AutoModelForSequenceClassification and I want to utilize my GPU for training. I've successfully installed PyTorch 1.9.0 with CUDA 11.1 and confirmed that CUDA is available on my system.</p>
<p>However, when I try to run my script, I encounter an ImportError suggesting that I need to install the accelerate library. When I attempt to do so, it not only fails but also replaces my existing <code>PyTorch 1.9.0</code> installation with version 2.1.0. I've tried various commands like pip install transformers[torch] and <code>pip install accelerate -U</code>, but they all result in the same issue.</p>
<p>The error message also indicates that accelerate requires at least PyTorch 1.10, but I can't find a compatible CUDA version for my RTX 3070.</p>
<p>Does anyone have a solution for running a proper installation of transformers + torch + accelerate?</p>
"
"77159136","Efficiently using Hugging Face transformers pipelines on GPU with large datasets","2023-09-22 15:57:36","14","18155","3","2","","77452808","<p>I'm relatively new to Python and facing some performance issues while using Hugging Face Transformers for sentiment analysis on a relatively large dataset. I've created a DataFrame with 6000 rows of text data in Spanish, and I'm applying a sentiment analysis pipeline to each row of text. Here's a simplified version of my code:</p>
<pre><code>import pandas as pd
import torch
from tqdm import tqdm
from transformers import pipeline


data = {
    'TD': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
    'text': [
        # ... (your text data here)
    ]
}

df_model = pd.DataFrame(data)

device = 0 if torch.cuda.is_available() else -1
py_sentimiento = pipeline(&quot;sentiment-analysis&quot;, model=&quot;finiteautomata/beto-sentiment-analysis&quot;, tokenizer=&quot;finiteautomata/beto-sentiment-analysis&quot;, device=device, truncation=True)

tqdm.pandas()
df_model['py_sentimiento'] = df_model['text'].progress_apply(py_sentimiento)
df_model['py_sentimiento'] = df_model['py_sentimiento'].apply(lambda x: x[0]['label'])
</code></pre>
<p>However, I've encountered a warning message that suggests I should use a dataset for more efficient processing. The warning message is as follows:</p>
<pre><code>&quot;You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset.&quot;
</code></pre>
<p>I have a two questions:</p>
<p>What does this warning mean, and why should I use a dataset for efficiency?</p>
<p>How can I modify my code to batch my data and use parallel computing to make better use of my GPU resources, what code or function or library should be used with hugging face transformers?</p>
<p>I'm eager to learn and optimize my code.</p>
"
"77135502","Adding a lemma for a new word and the concept of normalization/lemmatization in spaCy","2023-09-19 14:33:56","0","121","0","2","","77139418","<p>Following the examples from documentation regarding tokenization I have the following code:</p>
<pre><code>import spacy
from spacy.symbols import ORTH, NORM

nlp = spacy.load(&quot;en_core_web_sm&quot;)
special_case = [{ORTH: &quot;gim&quot;, NORM: &quot;give&quot;}, {ORTH: &quot;me&quot;}]
nlp.tokenizer.add_special_case(&quot;gimme&quot;, special_case)

doc = nlp(&quot;gimme that. he gave me that. Going to someplace.&quot;)
</code></pre>
<p>Then I check the tokenization</p>
<pre><code>doc[0].norm_  # 'give'  (as expected)
</code></pre>
<p>But the lemmatizer does not return the same output</p>
<pre><code>lemmatizer = nlp.get_pipe(&quot;lemmatizer&quot;)
lemmatizer.lemmatize(doc[0])  # ['gim']  (expected ['give']
</code></pre>
<p>In other hand</p>
<pre><code>lemmatizer.lemmatize(doc[5]) # ['give']
lemmatizer.lemmatize(doc[9]) # [go']
</code></pre>
<p>What I'm doing wrong? How to &quot;fix&quot;?  In spaCy what is the difference between <strong>normalized</strong> tokens and <strong>lemmatized</strong> tokens? How can I &quot;teach&quot; the lemmatization of a single token  (as this <code>gim</code> token in example) ?</p>
"
"77094149","How to use both gpus in kaggle for training in pytorch?","2023-09-13 04:56:24","2","1740","1","2","","77728508","<p>I was training a model in kaggle gpu.
But as I can see only one GPU is working.
<a href=""https://i.sstatic.net/8E0H3.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/8E0H3.png"" alt=""enter image description here"" /></a>
I use the ordinary method for training like</p>
<pre class=""lang-py prettyprint-override""><code>device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
model = model.to(device)
</code></pre>
<p>How can I use both the gpus?</p>
"
"77085879","LIME gives this error ""classifier models without probability scores"" in python","2023-09-12 03:14:13","0","444","3","1","","77093884","<p>it is my first time to use LIME and i have never used any interpretation technique before.</p>
<p>most likeley i am doing something wrong but i cannot figure out what is it.</p>
<p>I tried googling and go through SOF question to find the way to resolve this but did not find anything that can help me.</p>
<p>my dataset <strong>df_reps</strong> looks like this</p>
<pre><code>Toyota Horse Toyota Gear... Mazda Night King
Green Mazda King Toyota ... Blue Mazda Toyota
...
...
Gear Tyre Toyota Geaer ... Horse Blue Park
Laptop Invoice Toyota ...  Horse Mango Kitkat
</code></pre>
<p>and labels to predict, is whether the customer approved of not so the labels are only 0 and 1</p>
<p>Here is my code</p>
<pre><code>def BOW(df):
  CountVec = CountVectorizer() # to use only  bigrams ngram_range=(2,2)
  Count_data = CountVec.fit_transform(df)
  Count_data = Count_data.astype(np.uint8)
  cv_dataframe=pd.DataFrame(Count_data.toarray(), columns=CountVec.get_feature_names_out(), index=df.index)  # &lt;- HERE
  return cv_dataframe.astype(np.uint8)

df = BOW(df_reps)
y = df_Labels    # this is either 0 or 1
X = df
X_train, X_test, y_train, y_test = train_test_split(X, y)

clf = RandomForestClassifier(max_depth=100)
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
</code></pre>
<p>I converted text into tabular format using BOW</p>
<p>therefore, i will using
# Here is the part for LIME</p>
<pre><code>explainer = LimeTabularExplainer(X_train.values, feature_names=X_train.columns, verbose=True, mode='classification')
exp = explainer.explain_instance(X_test.values[1], clf.predict, num_features=10000)
</code></pre>
<p>but i am getting this error</p>
<blockquote>
<p>NotImplementedError: LIME does not currently support classifier models
without probability scores. If this conflicts with your use case,
please let us know: <a href=""https://github.com/datascienceinc/lime/issues/16"" rel=""nofollow noreferrer"">https://github.com/datascienceinc/lime/issues/16</a></p>
</blockquote>
"
"77084206","How can I enhance morphological information for English models in spaCy?","2023-09-11 19:05:34","1","203","0","1","","77102380","<p>I am trying to detect verbs that are in the imperative mood using English models in spaCy but I am seeing morphological features that are inconsistent with the examples found in the <a href=""https://spacy.io/usage/linguistic-features#morphology"" rel=""nofollow noreferrer"">Morphology</a> documentation. This issue is similar to this unanswered <a href=""https://stackoverflow.com/q/59008046/72992"">Extracting English imperative mood from verb tags with spaCy</a> question. Specifically, there seems to very few <a href=""https://universaldependencies.org/u/feat/Mood.html"" rel=""nofollow noreferrer"">mood</a> features identified.</p>
<p>I am not sure if I am missing some configuration or if I need to somehow train the model to better identify morphological features. Before I go down the path of training, I'd like to understand why what I am doing is not matching the documentation.</p>
<p>I have written a small example that demonstrates the discrepancy.</p>
<pre><code>'''
Prerequisites

pip install spacy
python -m spacy download en_core_web_lg
'''
import spacy

nlp = spacy.load(&quot;en_core_web_lg&quot;)

def show_morph_as_markdown_table(doc):
    print(&quot;|Context|Token|Lemma|POS|TAG|MORPH|&quot;)
    print(&quot;|----|----|----|----|----|----|&quot;)
    for token in doc:
        print(f'|{doc}|{token.text}|{token.lemma_}|{token.pos_}|{token.tag_}|{token.morph.to_dict()}|')

def show_morph_for_sentences_as_markdown_table(sentences):
    sentence_docs = list(nlp.pipe(sentences))
    for sentence_doc in sentence_docs:
        show_morph_as_markdown_table(sentence_doc)

example_sentences = [
    &quot;I was reading the paper&quot;,
    &quot;I don’t watch the news, I read the paper&quot;,
    &quot;I read the paper yesterday&quot;
]

show_morph_for_sentences_as_markdown_table(example_sentences)
</code></pre>
<p>I have trimmed the output to include only rows shown in the <a href=""https://spacy.io/usage/linguistic-features#morphology"" rel=""nofollow noreferrer"">Morphology</a> documentation.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Context</th>
<th>Token</th>
<th>Lemma</th>
<th>POS</th>
<th>TAG</th>
<th>MORPH</th>
</tr>
</thead>
<tbody>
<tr>
<td>I was reading the paper</td>
<td>reading</td>
<td>read</td>
<td>VERB</td>
<td>VBG</td>
<td>{'Aspect': 'Prog', 'Tense': 'Pres', 'VerbForm': 'Part'}</td>
</tr>
<tr>
<td>I don’t watch the news, I read the paper</td>
<td>read</td>
<td>read</td>
<td>VERB</td>
<td>VBD</td>
<td>{'Tense': 'Past', 'VerbForm': 'Fin'}</td>
</tr>
<tr>
<td>I read the paper yesterday</td>
<td>read</td>
<td>read</td>
<td>VERB</td>
<td>VBP</td>
<td>{'Tense': 'Pres', 'VerbForm': 'Fin'}</td>
</tr>
</tbody>
</table>
</div>
<p>This is very different from the expected output of:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Context</th>
<th>Token</th>
<th>Lemma</th>
<th>POS</th>
<th>TAG</th>
<th>MORPH</th>
</tr>
</thead>
<tbody>
<tr>
<td>I was reading the paper</td>
<td>reading</td>
<td>read</td>
<td>VERB</td>
<td>VBG</td>
<td>{'VerbForm': 'Ger'}</td>
</tr>
<tr>
<td>I don’t watch the news, I read the paper</td>
<td>read</td>
<td>read</td>
<td>VERB</td>
<td>VBD</td>
<td>{'VerbForm': 'Fin', 'Mood': 'Ind', 'Tense': 'Pres'}</td>
</tr>
<tr>
<td>I read the paper yesterday</td>
<td>read</td>
<td>read</td>
<td>VERB</td>
<td>VBP</td>
<td>{'VerbForm': 'Fin', 'Mood': 'Ind', 'Tense': 'Past'}</td>
</tr>
</tbody>
</table>
</div>
<p>I've tried adding a morphologizer to the pipeline using the DEFAULT_MORPH_MODEL but was met with an initialization error. I don't know enough about the pipeline yet to understand why.</p>
<pre><code>from spacy.pipeline.morphologizer import DEFAULT_MORPH_MODEL

config = {&quot;model&quot;: DEFAULT_MORPH_MODEL}
nlp.add_pipe(&quot;morphologizer&quot;, config=config)

# ValueError: [E109] Component 'morphologizer' could not be run. Did you forget to call `initialize()`?

# trying to fix above error with the following
nlp.initialize()

# [E955] Can't find table(s) lexeme_norm for language 'en' in spacy-lookups-data. Make sure you have the package installed or provide your own lookup tables if no default lookups are available for your language.

</code></pre>
<p>In researching further, it appears that spaCy version 3 <a href=""https://spacy.io/usage/v3#migrating-training-mappings-exceptions"" rel=""nofollow noreferrer"">manages tag_map and morph_rules with AttributeRuler</a>. Could it be possible that the downloadable models aren't including the same information that the documentation is using?</p>
<p>I'm hoping for an easy configuration fix that I am missing or a pointer to the right rabbit hole (I've been down many).</p>
"
"77078119","OpenAI API: How to catch all 5xx errors in Python?","2023-09-10 22:37:57","-1","641","0","2","","77088097","<p>I want to catch all <code>5xx</code> errors (e.g., <code>500</code>) that OpenAI API sends so that I can retry before giving up and reporting an exception.</p>
<p>Right now I'm basically doing the following:</p>
<pre><code>try:
    response = openai.ChatCompletion.create(req)
except InvalidRequestError as e:
    reportError
except ServiceUnavailableError as e:
    retry
except Exception as e:
    response = f&quot;Exception: {e}&quot;
    raise Exception(response)
</code></pre>
<p>Some <code>5xx</code> errors are getting caught as unknown errors (last case) which I want to catch so that I can retry them as I do in the case of the <code>ServiceUnavailableError</code>. But I don't know how to go about catching all the <code>5xx</code> errors for retry. The docs just talk about how to catch the specifically named errors.</p>
"
"77074094","Dataloader/sampler/collator to create batches based on the sample contents (sequence length)","2023-09-09 21:28:17","0","359","0","1","","77075337","<p>I am converting someone else's code into a neater torch-y pipeline, using datasets and dataloaders, collate functions and samplers. While I have done such work before, I am not sure how to tackle the following problem.</p>
<p>The dataset contains sentences as samples. Every samples therefore has a number of words (or <code>tokens</code>), which we can get by naively splitting the sample on white space (<code>sample.split()</code>). Such a dummy dataset can look like this:</p>
<pre class=""lang-py prettyprint-override""><code>from random import randint

from torch.utils.data import Dataset


class DummyDataset(Dataset):
    def __init__(self):
        data = []
        for _ in range(128):
            data.append(&quot;hello &quot; * randint(64, 176))
        self.data = data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx: int):
        return self.data[idx]
</code></pre>
<p>Now I want to be able to load data so that the max. number of <em>tokens</em> in a batch is not more than 250. That implies that the batch size can differ between iterations. One batch may contain two samples that have no more than 250 tokens in total (for instance 127 + 77) and another can have three (66+66+66). Now, the core functionality for this is rather straightforward. Full example below; not optimized by sorting on length or something but that's okay for this example.</p>
<p>The question is, how can I integrate this in the PyTorch eco-system? Batch sizes are so often used to indicate the number of <code>samples</code> (like in the dataloader). So where should I plug this in, or what should I subclass, to make this work like a regular dataloader?</p>
<pre class=""lang-py prettyprint-override""><code>from random import randint

from torch.utils.data import Dataset

class DummyDataset(Dataset):
    def __init__(self):
        data = []
        for _ in range(128):
            data.append(&quot;hello &quot; * randint(64, 176))
        self.data = data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx: int):
        return self.data[idx]


if __name__ == '__main__':
    dataset = DummyDataset()

    def get_batch(max_tokens: int = 250):
        data_idxs = list(range(len(dataset)))

        batch = []
        total_batch_len = 0
        while data_idxs:
            sample = dataset[data_idxs[0]]
            sample_len = len(sample.split())

            if total_batch_len + sample_len &lt;= max_tokens:
                batch.append(sample)
                total_batch_len += sample_len
                data_idxs.pop(0)
            elif batch:
                yield batch
                batch = []
                total_batch_len = 0

        yield batch

    # Sanity check that we indeed get all items from the dataset
    num_samples = 0
    num_batches = 0
    for b in get_batch():
        num_samples += len(b)
        num_batches += 1

    print(f&quot;Created {num_batches} batches&quot;)
    assert num_samples == len(dataset)
</code></pre>
<p>Maybe torchtext's <a href=""https://torchtext.readthedocs.io/en/latest/data.html#iterator"" rel=""nofollow noreferrer"">Iterator</a> and its <code>batch_size_fn</code> can help but I have no experience with it (where should I add it; is it a dataloader itself or should I still wrap a dataloader around it, etc.).</p>
"
"77043285","NLTK sentence_bleu() returns 0 while evaluating Chinese sentences","2023-09-05 09:25:09","0","315","3","2","","77131799","<p>I'm trying to evaluate Chinese sentence BLEU scores with NLTK's <code>sentence_bleu()</code> function. The code is as follows:</p>
<pre><code>import nltk
import jieba

from transformers import AutoTokenizer, BertTokenizer, BartForConditionalGeneration

src = '樓上漏水耍花招不處理可以怎麼做'
ref = '上層漏水耍手段不去處理可以怎麼做'

checkpoint = 'fnlp/bart-base-chinese'
tokenizer = BertTokenizer.from_pretrained(checkpoint)
model = BartForConditionalGeneration.from_pretrained(checkpoint)

hypothesis_translations = []

for sentence in [src]:
    inputs = tokenizer(sentence, return_tensors=&quot;pt&quot;, truncation=True, max_length=100, return_token_type_ids=False)
    outputs = model.generate(**inputs)
    translated_sentence = tokenizer.decode(outputs[0], skip_special_tokens=True)
    hypothesis_translations.append(translated_sentence)

# for Reference tokenization
inputs_ref = tokenizer(ref, return_tensors=&quot;pt&quot;, truncation=True, max_length=100, return_token_type_ids=False)
outputs_ref = model.generate(**inputs_ref)
tokenized_ref = tokenizer.decode(outputs_ref[0], skip_special_tokens=True)

nltk_bleu = nltk.translate.bleu_score.sentence_bleu(tokenized_ref, hypothesis_translations)
print(nltk_bleu)
</code></pre>
<p>The output of printing <code>nltk_bleu</code> is <code>0</code>.</p>
<p>But when I use the <code>corpus_score()</code> of <code>SacreBLEU</code> library, it returns normal and expected results:</p>
<pre><code>import evaluate
from sacrebleu.metrics import BLEU

bleu = BLEU()
bleu_score = bleu.corpus_score(references=tokenized_ref, hypotheses=hypothesis_translations)
print(bleu_score)
</code></pre>
<p>which returns:</p>
<blockquote>
<p>BLEU = 4.79 73.3/3.6/1.9/1.0 (BP = 1.000 ratio = 15.000 hyp_len = 15 ref_len = 1)</p>
</blockquote>
<p>How can I make the NLTK <code>sentence_score</code> return correct results?</p>
<hr />
<p><strong>UPDATE</strong> After adding NLTK's Method 3 into consideration:</p>
<pre><code>from nltk.translate.bleu_score import SmoothingFunction
smooth_fn = SmoothingFunction()
nltk_bleu = nltk.translate.bleu_score.sentence_bleu(tokenized_ref, hypothesis_translations, smoothing_function=smooth_fn.method3)
</code></pre>
<p>the value of <code>nltk_bleu</code> is still <code>0</code>.</p>
"
"76956484","How to fix this runtime error in this Databricks distributed training tutorial workbook","2023-08-22 19:50:40","0","494","0","1","","77175577","<p>I am following along with this <a href=""https://docs.databricks.com/en/_extras/notebooks/source/deep-learning/distributed-fine-tuning-hugging-face.html"" rel=""nofollow noreferrer"">notebook</a> found from this <a href=""https://docs.databricks.com/en/machine-learning/train-model/distributed-training/spark-pytorch-distributor.html"" rel=""nofollow noreferrer"">article</a>. I am attempting to fine tune the model with a single node and multiple GPUs, so I run everything up to the &quot;Run Local Training&quot; section, but from there I skip to &quot;Run distributed training on a single node with multiple GPUs&quot;. When I run the that first block though, I get this error:</p>
<p><code>RuntimeError: TorchDistributor failed during training. View stdout logs for detailed error message.</code></p>
<p>Here is the full output I see from the code block:</p>
<pre><code>We're using 4 GPUs
Started local training with 4 processes
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
*****************************************
2023-08-22 19:31:47.794586: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-22 19:31:47.809864: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-22 19:31:47.824423: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-22 19:31:47.828933: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
/databricks/python/lib/python3.10/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/databricks/python/lib/python3.10/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/databricks/python/lib/python3.10/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/databricks/python/lib/python3.10/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
Traceback (most recent call last):
  File &quot;/tmp/tmpz1ss252g/train.py&quot;, line 8, in &lt;module&gt;
    output = train_fn(*args)
  File &quot;&lt;command-2821949673242075&gt;&quot;, line 46, in train_model
  File &quot;/databricks/python/lib/python3.10/site-packages/transformers/trainer.py&quot;, line 1664, in train
    return inner_training_loop(
  File &quot;/databricks/python/lib/python3.10/site-packages/transformers/trainer.py&quot;, line 1855, in _inner_training_loop
    self.control = self.callback_handler.on_train_begin(args, self.state, self.control)
  File &quot;/databricks/python/lib/python3.10/site-packages/transformers/trainer_callback.py&quot;, line 353, in on_train_begin
    return self.call_event(&quot;on_train_begin&quot;, args, state, control)
  File &quot;/databricks/python/lib/python3.10/site-packages/transformers/trainer_callback.py&quot;, line 397, in call_event
    result = getattr(callback, event)(
  File &quot;/databricks/python/lib/python3.10/site-packages/transformers/integrations.py&quot;, line 1021, in on_train_begin
    self.setup(args, state, model)
  File &quot;/databricks/python/lib/python3.10/site-packages/transformers/integrations.py&quot;, line 990, in setup
    self._ml_flow.start_run(run_name=args.run_name, nested=self._nested_run)
  File &quot;/databricks/python/lib/python3.10/site-packages/mlflow/tracking/fluent.py&quot;, line 363, in start_run
    active_run_obj = client.create_run(
  File &quot;/databricks/python/lib/python3.10/site-packages/mlflow/tracking/client.py&quot;, line 326, in create_run
    return self._tracking_client.create_run(experiment_id, start_time, tags, run_name)
  File &quot;/databricks/python/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py&quot;, line 133, in create_run
    return self.store.create_run(
  File &quot;/databricks/python/lib/python3.10/site-packages/mlflow/store/tracking/rest_store.py&quot;, line 178, in create_run
    response_proto = self._call_endpoint(CreateRun, req_body)
  File &quot;/databricks/python/lib/python3.10/site-packages/mlflow/store/tracking/rest_store.py&quot;, line 59, in _call_endpoint
    return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)
  File &quot;/databricks/python/lib/python3.10/site-packages/mlflow/utils/databricks_utils.py&quot;, line 422, in get_databricks_host_creds
    config = provider.get_config()
  File &quot;/databricks/python/lib/python3.10/site-packages/databricks_cli/configure/provider.py&quot;, line 134, in get_config
    raise InvalidConfigurationError.for_profile(None)
databricks_cli.utils.InvalidConfigurationError: You haven't configured the CLI yet! Please configure by entering `/tmp/tmpz1ss252g/train.py configure`
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2572 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2573 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2574 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2571) of binary: /local_disk0/.ephemeral_nfs/envs/pythonEnv-3b3dff80-496a-4c7d-9684-b04a17a299d3/bin/python
Traceback (most recent call last):
  File &quot;/usr/lib/python3.10/runpy.py&quot;, line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File &quot;/usr/lib/python3.10/runpy.py&quot;, line 86, in _run_code
    exec(code, run_globals)
  File &quot;/databricks/python/lib/python3.10/site-packages/torch/distributed/run.py&quot;, line 766, in &lt;module&gt;
    main()
  File &quot;/databricks/python/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py&quot;, line 346, in wrapper
    return f(*args, **kwargs)
  File &quot;/databricks/python/lib/python3.10/site-packages/torch/distributed/run.py&quot;, line 762, in main
    run(args)
  File &quot;/databricks/python/lib/python3.10/site-packages/torch/distributed/run.py&quot;, line 753, in run
    elastic_launch(
  File &quot;/databricks/python/lib/python3.10/site-packages/torch/distributed/launcher/api.py&quot;, line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File &quot;/databricks/python/lib/python3.10/site-packages/torch/distributed/launcher/api.py&quot;, line 246, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
/tmp/tmpz1ss252g/train.py FAILED
------------------------------------------------------------
Failures:
  &lt;NO_OTHER_FAILURES&gt;
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-08-22_19:31:58
  host      : 0821-144503-em46c4jc-10-52-237-200
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2571)
  error_file: &lt;N/A&gt;
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
</code></pre>
<p>Do I need to enable more traceback to see more of the error? Do I need to 'configure the CLI', whatever that means? Is there something extremely obvious I'm just missing?</p>
<p>I am using a g5.12xlarge with 4 GPUs, and my DataBricks runtime version is '13.2 ML (includes Apache Spark 3.4.0, GPU, Scala 2.12)'. I'm running this from within a DataBricks notebook.</p>
"
"76892275","Some doubts about huggingface's BPE algorithm","2023-08-13 07:01:27","2","595","4","1","","76893606","<p>In most BPE(Byte-Pair Encoding) tutorials, it is mentioned to add <code>&lt;/w&gt;</code> after a word. The function of this mark is to distinguish whether a subword is a prefix of a word or a suffix of a word.</p>
<p>We know that the input of the model is a sequence of subwords(usually represented by ID), and the output of the model is naturally a sequence of subwords. But obviously the readability of this sequence is not strong, we still need to combine these subwords to get a normal sequence. The role of the <code>&lt;/w&gt;</code> mark is to merge subwords into words. Without <code>&lt;/w&gt;</code>, we naturally don't know the boundary of a word.</p>
<p>The <a href=""https://github.com/huggingface/transformers/blob/v4.31.0/src/transformers/models/gpt2/tokenization_gpt2.py#L104"" rel=""nofollow noreferrer"">BPE implementation</a> of huggingface refers to the source code of openAI's gpt-2, I checked their source code carefully and found that there is no mark like <code>&lt;/w&gt;</code>, so how do we get a normal sequence during the decoding process?</p>
"
"76814175","How to avoid lemmatizing already lemmatized sentences of a row in pandas dataframe for speedup","2023-08-01 18:00:52","1","49","0","2","","76814299","<p><strong>Given</strong>:</p>
<p>A simple and small pandas dataframe as follows:</p>
<pre><code>df = pd.DataFrame(
    {
        &quot;user_ip&quot;:       [&quot;u7&quot;, &quot;u3&quot;, &quot;u1&quot;, &quot;u9&quot;, &quot;u4&quot;,&quot;u8&quot;, &quot;u1&quot;, &quot;u2&quot;, &quot;u5&quot;],
        &quot;raw_sentence&quot;:  [&quot;First sentence!&quot;, np.nan, &quot;I go to school everyday!&quot;, &quot;She likes chips!&quot;, &quot;I go to school everyday!&quot;, &quot;This is 1 sample text!&quot;, &quot;She likes chips!&quot;, &quot;This is the thrid sentence.&quot;, &quot;I go to school everyday!&quot;],
    }
  )

    user_ip    raw_sentence
0   u7         First sentence!
1   u3         NaN
2   u1         I go to school everyday! 
3   u9         She likes chips!
4   u4         I go to school everyday!     &lt;&lt;&lt; duplicate &gt;&gt;&gt;
5   u8         This is 1 sample text!
6   u1         She likes chips!             &lt;&lt;&lt; duplicate &gt;&gt;&gt;
7   u2         This is the thrid sentence.
8   u5         I go to school everyday!     &lt;&lt;&lt; duplicate &gt;&gt;&gt;
</code></pre>
<p><strong>Goal</strong>:</p>
<p>I wonder if I could possibly avoid calling <code>map</code> or consider any other strategies for those rows with duplicated (exact similar) sentences in <code>raw_sentence</code> column. My intention is to speedup my implementation for bigger sized pandas dataframe (<code>~100K</code> rows).</p>
<p><strong>[<em>Inefficient</em>] Solution</strong>:</p>
<p>Right now, I take advantage of <code>.map()</code> using <code>lambda</code> which goes through each row and call <code>get_lm()</code> function to retrieves lemmas of raw input sentences as follows:</p>
<pre><code>import nltk
nltk.download('all', quiet=True, raise_on_error=True,)
STOPWORDS = nltk.corpus.stopwords.words('english')
wnl = nltk.stem.WordNetLemmatizer()
tokenizer = nltk.tokenize.RegexpTokenizer(r'\w+')

def get_lm(input_sent:str=&quot;my text!&quot;):
    tks = [ w for w in tokenizer.tokenize(input_sent.lower()) if not w in STOPWORDS and len(w) &gt; 1 and not w.isnumeric() ]
    lms = [ wnl.lemmatize(w, t[0].lower()) if t[0].lower() in ['a', 's', 'r', 'n', 'v'] else wnl.lemmatize(w) for w, t in nltk.pos_tag(tks)] 
    return lms

df[&quot;lemma&quot;] = df[&quot;raw_sentence&quot;].map(lambda raw: get_lm(input_sent=raw), na_action='ignore')

    user_ip     raw_sentence                    lemma
0   u7          First sentence!                 [first, sentence]         &lt;&lt;&lt; 1st occurrence =&gt; lemmatization OK! &gt;&gt;&gt;
1   u3          NaN                             NaN                       &lt;&lt;&lt; ignone None using na_action='ignore' &gt;&gt;&gt;
2   u1          I go to school everyday!        [go, school, everyday]    &lt;&lt;&lt; 1st occurrence =&gt; lemmatization OK! &gt;&gt;&gt;
3   u9          She likes chips!                [like, chip]              &lt;&lt;&lt; 1st occurrence =&gt; lemmatization OK! &gt;&gt;&gt;
4   u4          I go to school everyday!        [go, school, everyday]    &lt;&lt;&lt; already lemmatized, no need to do it again &gt;&gt;&gt;
5   u8          This is 1 sample text!          [sample, text]            &lt;&lt;&lt; 1st occurrence =&gt; lemmatization OK! &gt;&gt;&gt;
6   u1          She likes chips!                [like, chip]              &lt;&lt;&lt; already lemmatized, no need to do it again &gt;&gt;&gt;
7   u2          This is the thrid sentence.     [thrid, sentence]         &lt;&lt;&lt; 1st occurrence =&gt; lemmatization OK! &gt;&gt;&gt;
8   u5          I go to school everyday!        [go, school, everyday]    &lt;&lt;&lt; already lemmatized, no need to do it again &gt;&gt;&gt;
</code></pre>
<p>Is there any more efficient approach to fix this issue?</p>
<p>Cheers,</p>
"
"76802665","F1-Score and Accuracy for Text-Similarity","2023-07-31 10:20:25","-1","907","1","1","","76803336","<p>I am trying to understand how to calculate F1-Score and accuracy between texts while fine-tuning a QA model.</p>
<p>Let's assume we have this:</p>
<p><code>labels = [I am fine, He was born in 1995, The Eiffel tower, dogs]</code></p>
<p><code>preds = [I am fine, born in 1995, Eiffel, dog]</code></p>
<p>In this case, it is clear that the predictions are pretty accurate, but how can I measure the F1-Score here? Dog and dogs are not an exact match, but they are very similar.</p>
"
"76589840","Can't Run Transformer Fine Tuning With M1 Mac CPU","2023-06-30 14:31:19","0","2761","2","2","","76590240","<p>Here's the code I use</p>
<pre><code>from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir='./results',          # output directory
    num_train_epochs=1,              # total number of training epochs
    per_device_train_batch_size=16,  # batch size per device during training
    per_device_eval_batch_size=64,   # batch size for evaluation
    warmup_steps=500,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir='./logs',            # directory for storing logs
    logging_steps=10,
    use_mps_device=False
)

model = DistilBertForSequenceClassification.from_pretrained(&quot;distilbert-base-uncased&quot;)

trainer = Trainer(
    model=model,                         # the instantiated 🤗 Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=train_dataset,         # training dataset
    eval_dataset=val_dataset             # evaluation dataset
)
 
device = torch.device(&quot;cpu&quot;)
model.to(device)
  
trainer.train()
</code></pre>
<p>I get this error: message</p>
<p>RuntimeError: Placeholder storage has not been allocated on MPS device!</p>
<p>If I change the argument to use_mps_device=True, it will train with GPU, even though the model.to statement is pointing to CPU?</p>
<p>I could get the training to run with GPU but not with CPU.</p>
"
"76422222","How to do Tokenizer Batch processing? - HuggingFace","2023-06-07 10:15:05","7","13826","17","2","","76456769","<p>in the <a href=""https://huggingface.co/docs/transformers/main/main_classes/tokenizer"" rel=""noreferrer"">Tokenizer</a> documentation from huggingface, the <strong>call</strong> fuction accepts List[List[str]] and says:</p>
<blockquote>
<p>text (str, List[str], List[List[str]], optional) — The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set is_split_into_words=True (to lift the ambiguity with a batch of sequences).</p>
</blockquote>
<p>things run normally if I run:</p>
<pre><code> test = [&quot;hello this is a test&quot;, &quot;that transforms a list of sentences&quot;, &quot;into a list of list of sentences&quot;, &quot;in order to emulate, in this case, two batches of the same lenght&quot;, &quot;to be tokenized by the hf tokenizer for the defined model&quot;]
 tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')
 tokenized_test = tokenizer(text=test, padding=&quot;max_length&quot;, is_split_into_words=False, truncation=True, return_tensors=&quot;pt&quot;)
</code></pre>
<p>but if I try to emulate batches of sentences:</p>
<pre><code> test = [&quot;hello this is a test&quot;, &quot;that transforms a list of sentences&quot;, &quot;into a list of list of sentences&quot;, &quot;in order to emulate, in this case, two batches of the same lenght&quot;, &quot;to be tokenized by the hf tokenizer for the defined model&quot;]
 test = [test, test]
 tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')
 tokenized_test = tokenizer(text=test, padding=&quot;max_length&quot;, is_split_into_words=False, truncation=True, return_tensors=&quot;pt&quot;)
</code></pre>
<p>I get:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/Users/lucazeve/Coding/WxCC_Sentiment_Analysis/modify_scores.py&quot;, line 53, in &lt;module&gt;
    tokenized_test = tokenizer(text=test, padding=&quot;max_length&quot;, is_split_into_words=False, truncation=True, return_tensors=&quot;pt&quot;)
  File &quot;/Users/lucazeve/Coding/WxCC_Sentiment_Analysis/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py&quot;, line 2548, in __call__
    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
  File &quot;/Users/lucazeve/Coding/WxCC_Sentiment_Analysis/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py&quot;, line 2634, in _call_one
    return self.batch_encode_plus(
  File &quot;/Users/lucazeve/Coding/WxCC_Sentiment_Analysis/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py&quot;, line 2825, in batch_encode_plus
    return self._batch_encode_plus(
  File &quot;/Users/lucazeve/Coding/WxCC_Sentiment_Analysis/venv/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py&quot;, line 428, in _batch_encode_plus
    encodings = self._tokenizer.encode_batch(
TypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]
</code></pre>
<p>Is the documentation wrong? I just need a way to tokenize and predict using batches, it shouldn't be that hard.</p>
<p>Is it something to do with the <code>is_split_into_words</code> arguments?</p>
<hr />
<h2>Contextualizing</h2>
<p>I will feed that into a sentiment score model (the one defined in the code snippets). I am facing OOM problems when predicting it so I need to feed the data in batches to the model.</p>
<p>The documentation (refered above) stated that I can feed List[List[str]] in the tokenizer which is not the case. The question remains the same: How to tokenize batches of sentences?</p>
<p>Note: I don't need the tokenizing process to be in batches (although it would yield batches of tokens/attention_tokens), which would solve my problem: using the model for prediction with batches like this:</p>
<pre><code>with torch.no_grad():
    logits = model(**tokenized_test).logits

</code></pre>
"
"76417261","How to extract only those rows of the DataFrame where the values of two columns of the DataFrame are in English Language?","2023-06-06 17:58:21","2","64","0","1","","76417660","<p>I have a dataframe which has 27 columns including columns <code>FonctionsStagiaire</code> and <code>ExigencesParticulieres</code>. The dataframe has 13774 rows which are either entirely in English or French. The csv file can be found here: <a href=""https://drive.google.com/file/d/14-ge2hWvF4CyBDaFH_8snQKgKPiIcl3q/view?usp=sharing"" rel=""nofollow noreferrer"">GDrive link</a></p>
<p>I am trying to keep <strong>only</strong> those rows of the dataframe where the values or contents of <code>FonctionsStagiaire</code> or <code>ExigencesParticulieres</code> are in English. I want to drop the <strong>entire</strong> rows where these columns contain values in French language.</p>
<p>I am using <code>langdetect</code> but is getting the error <code>langdetect.lang_detect_exception.LangDetectException: No features in text.</code> I checked out all the solutions on SO to resolve this error but nothing is working.</p>
<p>I also want to check for if these 2 columns contain only <code>NaN</code> values, only numeric or whitespace characters or only punctuation marks, etc. which can't be detected whether it is in English or French and might cause error while using <code>langdetect</code>.</p>
<p>When I am trying to see which row is throwing the error based on this <a href=""https://stackoverflow.com/questions/40783383/error-using-langdetect-in-python-no-features-in-text"">SO question</a> it is showing that <code>This row throws error</code> for every row.</p>
<p>I intend to use <code>langdetect</code> but any other solution would also be helpful as long as it works properly.</p>
<p>Any help is much appreciated.</p>
<p>I am trying with this code:</p>
<pre><code>from langdetect import detect
import pandas as pd
import re
import string

def filter_nonenglish(df):
     
    list = ['FonctionsStagiaire', 'ExigencesParticulieres']

    for col in list:       
        #to check if values are only whitespaces or only numeric characters or only punctuation marks
        #something like this -&gt; if (df[col].apply(lambda x: x.isnumeric()) == True) | (df[col].apply(lambda x: x.isspace()) == True) | (all(i in string.punctuation for i in df[col]) == True):
            return False     
        else:
            #trying to use apply() to apply detect() to each &lt;str&gt; values of the columns and not to the entire pd.Series object
            new_df = df[(df[col].apply(detect).eq('en'))]

    return new_df
   
    #df['FonctionsStagiaire'] = df['FonctionsStagiaire'].apply(detect)
    #df['ExigencesParticulieres'] = df['ExigencesParticulieres'].apply(detect)
    
    #df = df[df['FonctionsStagiaire'] == 'en']
    #df = df[df['ExigencesParticulieres'] == 'en']
    
    #new_df = df[(df.FonctionsStagiaire.apply(detect).eq('en')) &amp; (df.ExigencesParticulieres.apply(detect).eq('en'))]
    
df = pd.read_csv('emplois_df_parsed.csv')

df = df[df['FonctionsStagiaire'].notna() &amp; df['ExigencesParticulieres'].notna()]   #to remove empty values

#to check whether all empty values are removed or not in the column 'FonctionsStagiaire'
#df['FonctionsStagiaire'] = df['FonctionsStagiaire'].str.lower()  
#df = df[df['FonctionsStagiaire'].str.islower()]

#to check whether all empty values are removed or not in the column 'ExigencesParticulieres'
#df['ExigencesParticulieres'] = df['ExigencesParticulieres'].str.lower()
#df = df[df['ExigencesParticulieres'].str.islower()]

#to make sure that values of both the columns are of &lt;str&gt; datatype
df['FonctionsStagiaire'] = pd.Series(df['FonctionsStagiaire'], dtype = &quot;string&quot;)
df['ExigencesParticulieres'] = pd.Series(df['ExigencesParticulieres'], dtype = &quot;string&quot;)

#bool(re.match('^(?=.*[a-zA-Z])', df.loc[:, 'FonctionsStagiaire']))
#bool(re.match('^(?=.*[a-zA-Z])', df.loc[:, 'ExigencesParticulieres']))

#   df[df[column].map(lambda x: x.isascii())]

df_new = filter_nonenglish(df)

df_new.to_csv('emplois_df_filtered.csv', index= False)

</code></pre>
"
"76321540","Call openai-node in the backend or call https in the frontend?","2023-05-24 08:32:05","2","928","0","1","","76322987","<p>I have a web application by ReactJS and Nodejs. This application calls OpenAI APIs.</p>
<p>Previously, when a user launches a request in the frontend, we send a request to the endpoint in our backend, call <code>createChatCompletion</code> of <a href=""https://github.com/openai/openai-node"" rel=""nofollow noreferrer"">https://github.com/openai/openai-node</a> in the backend, and returns the result to the frontend. Note that the server of our frontend and the server of our backend are separate and not in the same location; users are everywhere in the world.</p>
<p>We just realized that we can also request directly <code>https://api.openai.com/v1/chat/completions</code> in the frontend as follows:</p>
<pre><code>    const res = await fetch(&quot;https://api.openai.com/v1/chat/completions&quot;, {
        method: 'POST',
        headers: {
            &quot;Content-Type&quot;: &quot;application/json&quot;,
            Authorization: `Bearer ${API_KEY}`
        },
        body: JSON.stringify({
            model: model,
            messages: [{ role: &quot;user&quot;, content: prompt }]
        })
    })
</code></pre>
<p>At the moment, our pain-point is the time from sending a request by a user to seeing the result in the application is too long. From this perspective of speed, does anyone know which approach is better and why?</p>
"
"76186890","Why is perplexity calculation giving different results for the same input?","2023-05-06 02:41:33","1","621","0","1","","76433107","<p>I'm following <a href=""https://huggingface.co/docs/transformers/perplexity"" rel=""nofollow noreferrer"">Huggingface</a> doc on calculating the perplexity of fixed-length models. I'm trying to verify that the formula works for various strings and I'm getting odd behavior. In particular, they mention</p>
<blockquote>
<p>We don’t want the log-likelihood for the tokens we’re just treating as context to be included in our loss, so we can set these targets to -100 so that they are ignored</p>
</blockquote>
<p>So given 2 different contexts but the same remaining tokens, the formula should return the same perplexity. However, it does not:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import T5Tokenizer, T5ForConditionalGeneration
import torch
tokenizer = T5Tokenizer.from_pretrained(&quot;t5-small&quot;)
model = T5ForConditionalGeneration.from_pretrained(&quot;t5-small&quot;)

context_1 = 'here is some context_1 and some more stuff'
context_2 = 'here is some context and some more stuff and more stuff aspodkaspd'
answer_1 = 'this is not the answer'

input_ids_wrong = tokenizer(context_1 + answer_1, return_tensors=&quot;pt&quot;).input_ids
input_ids_correct = tokenizer(context_2 + answer_1, return_tensors=&quot;pt&quot;).input_ids
context_1_tokens_length = len(tokenizer(context_1, return_tensors=&quot;pt&quot;).input_ids[0])
context_2_tokens_length = len(tokenizer(context_2, return_tensors=&quot;pt&quot;).input_ids[0])

target_ids_wrong = input_ids_wrong.clone()
target_ids_correct = input_ids_correct.clone()

target_ids_wrong[:, :context_1_tokens_length] = -100 
target_ids_correct[:, :context_2_tokens_length] = -100 

print('target_ids_wrong', target_ids_wrong)
print('target_ids_correct', target_ids_correct)

with torch.no_grad():
    outputs_wrong = model(input_ids_wrong, labels=target_ids_wrong)
    outputs_correct = model(input_ids_correct, labels=target_ids_correct)
    
    neg_log_likelihood_wrong = outputs_wrong.loss
    neg_log_likelihood_correct = outputs_correct.loss

    ppl_wrong = torch.exp(neg_log_likelihood_wrong)
    ppl_correct = torch.exp(neg_log_likelihood_correct)
    print('ppl_wrong', ppl_wrong)
    print('ppl_correct', ppl_correct)
</code></pre>
<p>Output:</p>
<pre><code>    target_ids_wrong tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,   19,
               59,    8, 1525,    1]])
    target_ids_correct tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
             -100, -100, -100, -100, -100, -100,   19,   59,    8, 1525,    1]])
    ppl_wrong tensor(9.0377)
    ppl_correct tensor(21.1208)
</code></pre>
<p>I tried this with other models as well (e.g., <code>gpt2</code> and <code>sshleifer/tiny-gpt2</code>) and got the same odd behavior. From the <a href=""https://huggingface.co/docs/transformers/model_doc/t5"" rel=""nofollow noreferrer"">T5 doc</a> they wrote</p>
<blockquote>
<p>we must make sure that padding token id’s of the labels are not taken into account by the loss function. In PyTorch and Tensorflow, this can be done by replacing them with -100, which is the ignore_index of the CrossEntropyLoss.
So I don't understand why it takes the pad token into account. They also wrote in the same link
which for T5 is equal to 0 (i.e. the id of the pad token)</p>
</blockquote>
<p>So I tried replacing the <code>-100</code> with <code>0</code> and actually a got different perplexity score (still different than each other, but different than the <code>-100</code>). Which makes me think they don't actually ignore the <code>-100</code> token for some reason.</p>
<p>Am I missing something?</p>
"
"76158903","Installing Spacy for GPU training of Transformer","2023-05-02 21:11:16","2","980","1","1","","76159512","<p>I installed also the CudaToolkit, and cuDNN.</p>
<p>I have the following</p>
<pre><code>GPU:
| NVIDIA-SMI 531.68                 Driver Version: 531.68       CUDA Version: 12.1     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                      TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA GeForce RTX 3070 T...  WDDM | 00000000:01:00.0 Off |                  N/A |
| N/A   43C    P8                9W /  N/A|      0MiB /  8192MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+

+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
</code></pre>
<p>I have been trying to install spacy with the following instructions on a couple of posts on the site and used</p>
<pre class=""lang-py prettyprint-override""><code>pip install spacy[cuda121]
</code></pre>
<p>I get a warning that spacy 3.5.2 does not detect or support CUDA version 12.1.</p>
<p>I manage to get tensorflow to detect the GPU and when I run the following:</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
print('Num GPUs Available: ', len(tf.config.list_physical_devices('GPU')))
</code></pre>
<p>It detects 1 GPU.
I also ran:</p>
<pre class=""lang-py prettyprint-override""><code>pip install cudy
</code></pre>
<p>I am wondering if there is something wrong with setup or if I doing something wrong. Any support would be much appreciated.</p>
"
"76136216","How do I reshape data to calculate ROC and AUC for binary text classification?","2023-04-29 12:27:00","2","126","0","1","","76136550","<p>I'm very new to python and need to calculate the ROC and AUC of two binary classification models using NLP data. I can't seem to get my head around sparse vs dense arrays (I mean, I get that sparse arrays contain a ton of zeros, and dense arrays do not), data shape, and dimensionality.</p>
<p>I think I can produce pretty good preprocessed data, but inputting that into my classifiers in a way they can read has me stymied.</p>
<p>In my code below, you'll note that I have tried more than one train test split. I get</p>
<pre><code>TypeError: A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.
</code></pre>
<p>if I don't convert x and y to dense.</p>
<p>I get</p>
<pre><code>ValueError: y should be a 1d array, got an array of shape (1594, 286579) instead

UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless
</code></pre>
<p>when I do the dense conversion.</p>
<p>And I get</p>
<pre><code>ValueError: Found input variables with inconsistent numbers of samples: [1594, 399]
</code></pre>
<p>when (if I'm remembering correctly) using the commented out train test split.</p>
<p>Here is my messy, redundant code:</p>
<pre><code>import joblib
import re
import string
import numpy as np
import pandas as pd
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import accuracy_score, cohen_kappa_score, f1_score, classification_report
from sklearn.model_selection import StratifiedKFold, train_test_split
from sklearn.naive_bayes import MultinomialNB

categories = ['rec.sport.baseball', 'rec.sport.hockey']

news_group_data = fetch_20newsgroups(subset=&quot;all&quot;, remove=(&quot;headers&quot;, &quot;footers&quot;, &quot;quotes&quot;), categories=categories)

df = pd.DataFrame(dict(text=news_group_data[&quot;data&quot;],target=news_group_data[&quot;target&quot;]))
df[&quot;target&quot;] = df.target.map(lambda x: categories[x])

def process_text(text):
    text = str(text).lower()
    text = re.sub(f&quot;[{re.escape(string.punctuation)}]&quot;, &quot; &quot;, text)
    text = &quot; &quot;.join(text.split())
    return text

df[&quot;clean_text&quot;] = df.text.map(process_text)

#df_train, df_test = train_test_split(df, test_size=0.20, stratify=df.target)

vec = CountVectorizer(ngram_range=(1, 3), stop_words=&quot;english&quot;,)

x = vec.fit_transform(df.clean_text)
y = vec.transform(df.clean_text)


#X = vec.fit_transform(df_train.clean_text)
#Y = vec.transform(df_test.clean_text)

X = x.toarray()
Y = y.toarray()

#y_train = df_train.target
#y_test = df_test.target

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.2,
                                                    random_state=0)

from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB

RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features=5,
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, #min_impurity_split=None,
                       min_samples_leaf=1, min_samples_split=2,
                       min_weight_fraction_leaf=0.0, n_estimators=500,
                       n_jobs=None, oob_score=False, random_state=None,
                       verbose=0, warm_start=False)

nb = GaussianNB()
nb.fit(X_train, Y_train)

r_probs = [0 for _ in range(len(Y_test))]
rf_probs = rf.predict_proba(X_test)
nb_probs = nb.predict_proba(X_test)

rf_probs = rf_probs[:, 1]
nb_probs = nb_probs[:, 1]

from sklearn.metrics import roc_curve, roc_auc_score

r_auc = roc_auc_score(Y_test, r_probs)
rf_auc = roc_auc_score(Y_test, rf_probs)
nb_auc = roc_auc_score(Y_test, nb_probs)

print('Random (chance) Prediction: AUROC = %.3f' % (r_auc))
print('Random Forest: AUROC = %.3f' % (rf_auc))
print('Naive Bayes: AUROC = %.3f' % (nb_auc))

r_fpr, r_tpr, _ = roc_curve(Y_test, r_probs)
rf_fpr, rf_tpr, _ = roc_curve(Y_test, rf_probs)
nb_fpr, nb_tpr, _ = roc_curve(Y_test, nb_probs)

import matplotlib.pyplot as plt

plt.plot(r_fpr, r_tpr, linestyle='--', label='Random prediction (AUROC = %0.3f)' % r_auc)
plt.plot(rf_fpr, rf_tpr, marker='.', label='Random Forest (AUROC = %0.3f)' % rf_auc)
plt.plot(nb_fpr, nb_tpr, marker='.', label='Naive Bayes (AUROC = %0.3f)' % nb_auc)

plt.title('ROC Plot')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.show()
</code></pre>
"
"76091659","Failed lemmatization","2023-04-24 12:03:27","0","50","0","1","","76091954","<p>I'm trying to lemmatize german texts which are in a dataframe.
I use <code>german</code> library to succesfully handle with specific grammatic structure: <a href=""https://github.com/jfilter/german-preprocessing"" rel=""nofollow noreferrer"">https://github.com/jfilter/german-preprocessing</a></p>
<p>My code:</p>
<pre><code>from german import preprocess

df = pd.read_csv('Afd.csv', sep=',')

Lemma = open('MessageAFD_lemma.txt', 'w')
for i in df['message']:
    preprocess (i, remove_stop=True)
    Lemma.write(i)
Lemma.close()

</code></pre>
<p>The process of lemmatization goes successfully, there's no any error in the terminal, but openning the file &quot;MessageAFD_lemma.txt&quot;, I get this : <a href=""https://i.sstatic.net/sX8Dg.png"" rel=""nofollow noreferrer"">(nothing was lemmatized)</a></p>
<p>The expected result is like:</p>
<p>Input:</p>
<pre><code>preprocess(['Johpannes war einer von vielen guten Schülern.', 'Julia trinkt gern Tee.'], remove_stop=True)
</code></pre>
<p>Output:
<code>['johannes gut schüler', 'julia trinken tee']</code></p>
<p>What goes wrong?</p>
"
"76074982","Parallelize inference with huggingface using torch","2023-04-21 16:15:54","1","604","0","1","","76085163","<p>I am running an inference model on a Ubuntu machine with 8GB only and just realised the predictions (logits) are not generated in a batch way so my process is getting Killed due oom issues.</p>
<pre><code>tokenized_test = tokenizer(dataset[&quot;test&quot;][&quot;text&quot;], padding=True, truncation=True, return_tensors=&quot;pt&quot;)

with torch.no_grad():
    logits = model(**tokenized_test).logits
</code></pre>
<p>This is where I run out of memory. What is the best way of do this in batches/parallelize/sequentiate/solve the oom issue. I am ultimately looking for the solution that would require the least amount of code changes.</p>
<hr />
<h3>Source</h3>
<p>I have built my code based on this tutorial:</p>
<p><a href=""https://huggingface.co/docs/transformers/tasks/sequence_classification"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/tasks/sequence_classification</a></p>
<p>Increasing the dataset size will eventually make you go oom to.</p>
"
"76067091","GPU out of memory fine tune flan-ul2","2023-04-20 18:13:02","1","492","2","1","","76320287","<blockquote>
<p>OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB
(GPU 0; 15.78 GiB total capacity; 14.99 GiB already allocated; 3.50
MiB free; 14.99 GiB reserved in total by PyTorch) If reserved memory
is &gt;&gt; allocated memory try setting max_split_size_mb to avoid
fragmentation.  See documentation for Memory Management and
PYTORCH_CUDA_ALLOC_CONF</p>
</blockquote>
<p>I have Standard_NC24s_v3 single node GPU with 448GB memory and 4 GPUs. However the error message says the total capacity is 15.78GiB. Is the fine tune not using 4 GPUs? How to get all the 4 GPUs used in the fine tune of Flan-UL2 using huggingface transformers?</p>
"
"76050174","lemmatization or normalization using a dictionary and list of variations","2023-04-19 01:47:23","1","50","0","1","","76050942","<p>I have a pandas data frame with string column which is a transaction string column. I am trying to some manual lemmatization. I have manually created a dictionary which has the main word as the key and a list of variations of the words as the values. I would like to substitute the words in the list with the main word.</p>
<p>here is the example code of the data I have.</p>
<pre><code>import pandas as pd
list1 = ['0412 UBER TRIP HELP.UBER.COMCA',
'0410 UBER TRIP HELP.UBER.COMCA',
'MOBILE PURCHASE 0410 VALENCIA WHOLE FOODS SAN FRANCISCOCA',
'WHOLEFDS WBG#1 04/13 PURCHASE WHOLEFDS WBG#104 BROOKLYN NY',
'0414 LYFT *CITI BIKE BIK LYFT.COM CA',
'0421 WALGREENS.COM 877-250-5823 IL',
'0421 Rapha Racing PMT LLC XXX-XX72742 OR',
'0422 UBER EATS PAYMENT HELP.UBER.COMCA',
'0912 WHOLEFDS NOE 10379 SAN FRANCISCOCA',
'PURCHASE 1003 CAVIAR*JUNOON WWW.DOORDASH.CA']
df = pd.DataFrame(list1, columns = ['feature'])

map1 = {'payment':['pmts','pmnt','pmt','pmts','pyment','pymnts'],
'account':['acct'],
 'pharmacy':['walgreens','walgreen','riteaid','cvs','pharm'],
 'food_delivery':['uber eats','doordash','seamless','grubhub','caviar'],
 'ride_share':['uber','lyft'],
 'whole_foods':['wholefds','whole foods','whole food']
}
</code></pre>
<p>I know how to do it one word at a time using <code>df['feature'].str.replace('variation','main word')</code>. However, this is laborious and time consuming. Is there a faster way to do this? Thank you.</p>
"
"75928704","Swift API Call Decoding Issue: decoding error","2023-04-04 11:17:33","-1","611","10","1","","75929723","<p>Last week I made a chatGpt application using this tutorial: <a href=""https://www.youtube.com/watch?v=bUDCW2NeO8Y"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=bUDCW2NeO8Y</a> . It worked just fine, but when I started working on it again today it didn't work. This uses the <a href=""https://swiftpackageindex.com/adamrushy/OpenAISwift"" rel=""nofollow noreferrer"">OpenAISwift</a> package.</p>
<p>After 'debugging' it a bit I got this error:</p>
<blockquote>
<p>failure(OpenAISwift.OpenAIError.decodingError(error:
Swift.DecodingError.keyNotFound(CodingKeys(stringValue: &quot;object&quot;,
intValue: nil), Swift.DecodingError.Context(codingPath: [],
debugDescription: &quot;No value associated with key
CodingKeys(stringValue: &quot;object&quot;, intValue: nil) (&quot;object&quot;).&quot;,
underlyingError: nil))))</p>
</blockquote>
<p>The code I use is almost the same as in the video, the only difference is that I trim white spaces and new lines (this isn't the issue I checked)</p>
<p>My code (that has the error) looks like this:</p>
<pre><code>import OpenAISwift
import SwiftUI    
final class ViewModel: ObservableObject{
    init(){}
    
    private var client: OpenAISwift?
    
    func setup(){
        client = OpenAISwift(authToken: &quot;MY_API_KEY&quot;)
    }
    
    func makeCall(text: String,
                  completion: @escaping (String) -&gt; Void){
        client?.sendCompletion(with: text,
                               maxTokens: 500,
                               completionHandler: { result in
            switch result {
            case .success(let model):
                let output = model.choices.first?.text.trimmingCharacters(in: .whitespacesAndNewlines) ?? &quot;&quot;
                completion(output)
            case .failure:
                print(&quot;Failed: \(result)&quot;)
                break
            }
        })
    }
}
</code></pre>
<p>I hope someone could help me with this, because everything I have tried failed. I tried using other methods of calling the API but that also wouldn't work...
So if you know what the issue is and how to fix it please let me know</p>
<p>Edit sendCompletion code:</p>
<pre><code>public func sendCompletion(with prompt: String, model: OpenAIModelType = .gpt3(.davinci), maxTokens: Int = 16, temperature: Double = 1, completionHandler: @escaping (Result&lt;OpenAI&lt;TextResult&gt;, OpenAIError&gt;) -&gt; Void) {
    let endpoint = Endpoint.completions
    let body = Command(prompt: prompt, model: model.modelName, maxTokens: maxTokens, temperature: temperature)
    let request = prepareRequest(endpoint, body: body)

makeRequest(request: request) { result in
    switch result {
    case .success(let success):
        do {
            let res = try JSONDecoder().decode(OpenAI&lt;TextResult&gt;.self, from: success)
            completionHandler(.success(res))
        } catch {
            completionHandler(.failure(.decodingError(error: error)))
        }
    case .failure(let failure):
        completionHandler(.failure(.genericError(error: failure)))
    }
}
</code></pre>
<p>}</p>
<p>Edit after feedback:</p>
<p>I updated the version, and after this it stopped giving me that error, but it now gives me a different one.... this one:</p>
<blockquote>
<p>success(OpenAISwift.OpenAI&lt;OpenAISwift.TextResult&gt;(object: nil, model:
nil, choices: nil, usage: nil, data: nil))</p>
</blockquote>
"
"75851367","How to kill training on specific GPUs?","2023-03-27 00:48:18","0","143","3","2","","75856233","<p>I am training transformers model on differtnt GPUs(3 gpus out of 8) and want to kill training on spesfic gpus only <code>(0,6,7)</code>
I trained <code>top</code> command I can see only <code>PID</code> <a href=""https://i.sstatic.net/7QEjZ.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/7QEjZ.jpg"" alt=""enter image description here"" /></a>. But don't know which GPUs belong to <code>PID</code>
THE kill -9  I do not want to use because don't know which GPU will stop as I want to stop (0,7,6) and keep the others running</p>
<p>I reproduce the problem with a small example :</p>
<pre><code>from accelerate import Accelerator, notebook_launcher
from accelerate.utils import set_seed

def training_loop():
    set_seed(42)
    accelerator = Accelerator(mixed_precision=&quot;fp16&quot;)
    print(&quot;Hello There!&quot;)
    # main()   
notebook_launcher(training_loop(),  num_processes=2) #training_loop(),
</code></pre>
<p>lunching the script with termonal :</p>
<p><code>CUDA_VISIBLE_DEVICES=0,6,7</code></p>
<p><code>python3 AccelerateTrainer.py</code></p>
<p>I expect after running <code>Nvidia-smi</code>  <code>0%</code> for both 0,6, and 7 GPUs</p>
"
"75763642","TF-IDF value is not matching the output of TfidfVectorizer","2023-03-17 03:58:24","1","321","0","1","","75769023","<p>I am learning NLP and was interested in understanding the TF-IDF model using the sklearn library and the class <code>TfidfVectorizer</code>
I have pasted the sample code below.</p>
<pre><code>
corpus = [
    'This is the first document.',
    'This is the second second document.',
    'And the third one.',
    'Is this the first document?',
]

vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(corpus)
pd.DataFrame(X.toarray(), columns = vectorizer.get_feature_names())
</code></pre>
<p>The feature names:
<code>vectorizer.get_feature_names()</code>
<code>['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']</code></p>
<p>And the tf-idf values are:</p>
<pre><code>array([[0.        , 0.43877674, 0.54197657, 0.43877674, 0.        ,
        0.        , 0.35872874, 0.        , 0.43877674],
       [0.        , 0.27230147, 0.        , 0.27230147, 0.        ,
        0.85322574, 0.22262429, 0.        , 0.27230147],
       [0.55280532, 0.        , 0.        , 0.        , 0.55280532,
        0.        , 0.28847675, 0.55280532, 0.        ],
       [0.        , 0.43877674, 0.54197657, 0.43877674, 0.        ,
        0.        , 0.35872874, 0.        , 0.43877674]])
</code></pre>
<p>I was interested in calculating the tf-idf value of the term &quot;document&quot; for the above mentioned corpus, which comes out to be 0.43877674 for the first document.</p>
<p>I tried using the below formula both for base 10 and base e (natural logarithm), since <code>smooth_idf=True</code> by default and as per the documentation written in https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting</p>
<p><em>Using the TfidfTransformer’s default settings, TfidfTransformer(norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False) the term frequency, the number of times a term occurs in a given document, is multiplied with idf component, which is computed as</em></p>
<p><img src=""https://user-images.githubusercontent.com/45195453/225404191-db3cd0dd-efce-4c0b-a80b-856fe71a38df.png"" alt=""image"" /></p>
<p>where <code>n</code> is the total number of documents in the document set, and <code>df(t)</code> is the number of documents in the document set that contain term <code>t</code></p>
<p><a href=""https://i.sstatic.net/XCYt3.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/XCYt3.png"" alt=""enter image description here"" /></a></p>
<p>According to the output from the program written, it should be 0.43877674</p>
"
"75686316","changing the output of text_tokens function in R","2023-03-09 14:58:22","0","58","0","1","","75686412","<p>I have a question redarding text mining with the <code>corpus package</code> and the function <code>text_tokens()</code>. I want to use the function for stemming and deleting stop words. I have a huge amount of data (almost 1.000.000 comments) where I want to use it for. But I've problems with the output, the function <code>text_tokens</code> produces. So here is a basic example of my data and code:</p>
<pre><code>library(tidyverse)
library(corpus)
library(stopwords)

text &lt;- data.frame(comment_id = 1:2,
                   comment_content = c(&quot;Hallo mein Name ist aaron&quot;,&quot;Vielen Lieben Dank für das Video&quot;))


tmp &lt;- text_tokens(text$comment_content, 
                   text_filter(stemmer = &quot;de&quot;,drop = stopwords(&quot;german&quot;)))
</code></pre>
<p>My problem now is, that I want a <code>data.frame</code> as output with the comment_id in the first column and word_token in the column. So the output I would like to have looks as followed:</p>
<pre><code>df &lt;- data.frame(comment_id = c(1,1,1,2,2,2),
                 comment_tokens = c(&quot;hallo&quot;,&quot;nam&quot;,&quot;aaron&quot;,&quot;lieb&quot;,&quot;dank&quot;,&quot;video&quot;))
</code></pre>
<p><a href=""https://i.sstatic.net/MgBkw.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/MgBkw.png"" alt=""output I need"" /></a></p>
<p>I tried different <code>do.calls</code> (cbind/rbind), but they don't give me the result I need. So what is the function I'm looking for, is it <code>map()</code> from the tidyverse?</p>
<p>Thank you in advance.</p>
<p>Cheers,</p>
<p>Aaron</p>
"
"75624727","Why I am getting less BLEU score?","2023-03-03 08:26:32","2","1497","1","2","","75625303","<pre><code>from nltk.translate.bleu_score import sentence_bleu
reference = [['this', 'is', 'ae', 'test']]
candidate = ['this', 'is', 'ad', 'test']
score = sentence_bleu(reference, candidate)
print(score)
</code></pre>
<p>I am using this code to calculate the BLEU score and the score I am getting is <code>1.0547686614863434e-154</code>. I wander why I am getting so small value even only one letter is different in candidate list.</p>
<pre><code>score = sentence_bleu(reference, candidate,weights = [1])
</code></pre>
<p>I tried adding weight = [1] as a parameter and it gave me <code>0.75</code> as output.  I cant understand why I have to add weight to get a reasonable result. Any help would be appreciated.</p>
<p>I thought its maybe because the sentence is not long enough so I added more words:</p>
<pre><code>from nltk.translate.bleu_score import sentence_bleu
reference = [['this', 'is', 'ae', 'test','rest','pep','did']]
candidate = ['this', 'is', 'ad', 'test','rest','pep','did']
score = sentence_bleu(reference, candidate)
print(score)
</code></pre>
<p>Now I am getting <code>0.488923022434901</code> but still I think is too low value.</p>
"
"75595699","HuggingFace's BertTokenizerFast is between 39000 and 258300 times slower than expected","2023-02-28 17:57:54","1","3169","0","1","","75618063","<p>As part of training a BERT model, I am tokenizing a 600MB corpus, <a href=""https://huggingface.co/docs/tokenizers/index"" rel=""nofollow noreferrer"">which should apparently take approx. 12 seconds</a>. I tried this on a computing cluster and on a Google Colab Pro server, and got time estimates ranging from 130 to 861 hours.</p>
<p>Here's the minimal working example (most of the values aren't hard-coded, but I specified the ones I use most of the time here for simplicity):</p>
<pre class=""lang-py prettyprint-override""><code>training_args = TrainingArguments(
    output_dir=args.output_dir,
    overwrite_output_dir=True,
    num_train_epochs=1,
    per_gpu_train_batch_size=512,
    save_steps=10_000,
    save_total_limit=2,
    prediction_loss_only=True,
    learning_rate=2e-5,
    weight_decay=0.15,
    push_to_hub=False,
    gradient_accumulation_steps=4
)

dataset = load_dataset(
    &quot;text&quot;,
    data_files=&quot;mycorpus.txt&quot;)['train'].shuffle(seed=42)

tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
model = BertForMaskedLM.from_pretrained(&quot;bert-base-uncased&quot;)

# Code stolen from https://github.com/huggingface/notebooks/blob/main/examples/language_modeling.ipynb
# except I replaced the tokenization function with a lambda
tokenized_dataset = dataset.map(
    lambda examples: tokenizer(examples[&quot;text&quot;]),
    batched=True,
    num_proc=4,
    remove_columns=[&quot;text&quot;])

lm_dataset = tokenized_dataset.map(
    group_texts,
    batched=True,
    batch_size=512,
    num_proc=4
)
# /steal

data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer, mlm=True, mlm_probability=0.15
)

model_trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=lm_dataset
)
model_trainer.train()
</code></pre>
<p>Having traced the execution path in PDB, the issue arises in the call to <code>model_trainer.train()</code>, which I guess ends up calling the lambda used in the declaration of <code>tokenized_dataset</code>.</p>
<p>I do get the following message:</p>
<pre><code>You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
</code></pre>
<p>However, I do believe my lambda calls the <code>__call__</code> function implicitly, does it not? Can I do something about this?</p>
<p>That said, I'm doubtful that this warning message is relevant, as it seems to imply a relatively minor slowdown. I feel like it would have a more dramatic tone if it were related to the staggering difference that I'm observing.</p>
"
"75590491","Difference between MultiheadAttention and Attention layer in Tensorflow","2023-02-28 10:06:07","4","4162","1","1","","75595063","<p>What is the difference between the following layers in Tensorflow: <code>tf.keras.layers.Attention</code>, <code>tf.keras.layers.MultiHeadAttention</code> and <code>tf.keras.layers.AdditiveAttention</code>?</p>
<p>Also how to implement <code>tf.keras.layers.MultiHeadAttention</code> using fundamental layers like Dense, Add, LayerNormalization, etc? I want to understand the exact operations happening inside <a href=""https://www.tensorflow.org/text/tutorials/nmt_with_attention#the_attention_layer"" rel=""nofollow noreferrer"">this</a> tutorial.</p>
"
"75488355","Using the Earley library to parse with features and unification","2023-02-17 19:05:34","1","119","0","1","","75503036","<p>The <a href=""https://hackage.haskell.org/package/Earley"" rel=""nofollow noreferrer""><code>Earley</code></a> parsing library is great for writing linguistic parsers in Haskell. CFGs can be specified in an intuitive way, and there is excellent support for backtracking and ambiguity. A simple example:</p>
<pre class=""lang-hs prettyprint-override""><code>{-# LANGUAGE OverloadedStrings #-}

import Text.Earley

np = rule (&quot;John&quot; &lt;|&gt; &quot;Mary&quot;)
vp = rule (&quot;runs&quot; &lt;|&gt; &quot;walks&quot;)

sentence = do
  subj &lt;- np
  pred &lt;- vp
  return $ (++) &lt;$&gt; subj &lt;*&gt; pred
</code></pre>
<p><code>sentence</code> can be used to parse <code>[&quot;John&quot;, &quot;runs&quot;]</code> or <code>[&quot;Mary&quot;, &quot;walks&quot;]</code>, among other inputs.</p>
<p>It would be nice to be able to use <code>Earley</code> to write parsers for <strong>F</strong>CFGs, where nonterminals are complexes of a label and a feature bundle, and feature matching can happen via unification (for example, the Earley parser in <a href=""https://www.nltk.org/howto/featgram.html"" rel=""nofollow noreferrer"">NLTK</a> parses FCFGs). However, it is not clear how to do this using <code>Earley</code>, or whether it can even be done. An example of something we might want in something like BNF:</p>
<pre><code>np[sg] ::= &quot;John&quot; | &quot;Mary&quot;

np[?x] ::= det n[?x]
n[pl]  ::= &quot;boys&quot; | &quot;girls&quot;

det    ::= &quot;the&quot;

vp[sg] ::= &quot;runs&quot; | &quot;walks&quot;
vp[pl] ::= &quot;run&quot;  | &quot;walk&quot;

s ::= np[?x] vp[?x]
</code></pre>
<p>Under this FCFG, <code>[&quot;John&quot;, &quot;runs&quot;]</code> is an <code>s</code> (since their number features match, as required by the <code>s</code> rule), and <code>[&quot;the&quot;, &quot;boys&quot;, &quot;walks&quot;]</code> isn't an <code>s</code> (since <code>[&quot;the&quot;, &quot;boys&quot;]</code> parses to <code>np[pl]</code> and <code>[&quot;walks&quot;]</code> parses to <code>vp[sg]</code>).</p>
<p>One can in general rewrite an FCFG into an equivalent CFG, but this can be highly inconvenient, and result in a blowup of the grammar, especially when we have many possible features ranging over many possible values.</p>
"
"75394318","python text parsing to split list into chunks including preceding delimiters","2023-02-09 04:49:23","1","226","0","3","","75394630","<p><strong>What I Have</strong></p>
<p>After OCR'ing some public Q&amp;A deposition pdfs which have a Q&amp;A form, I have raw text like the following:</p>
<pre><code>text = &quot;&quot;&quot;\na\n\nQ So I do first want to bring up exhibit No. 46, which is in the binder 
in front of\nyou.\n\nAnd that is a letter [to] Alston\n&amp; Bird...
\n\nIs that correct?\n\nA This is correct.\n\nQ Okay.&quot;&quot;&quot;
</code></pre>
<p>...which I want to split into the separate questions and answers. Each Question or Answer starts with <code>'\nQ '</code>, <code>'\nA '</code>, <code>'\nQ_'</code> or <code>'\nA_'</code> (e.g. matches regex <code>&quot;\n[QA]_?\s&quot;</code>)</p>
<p><strong>What I've Done So Far</strong></p>
<p>I can get a list of all questions and answers with the following code:</p>
<pre><code>pattern = &quot;\n[QA]_?\s&quot;
q_a_list = re.split(pattern, text)
print(q_a_list)
</code></pre>
<p>which yields <code>q_a_list</code>:</p>
<pre><code>['\na\n', 
'So I do first want to bring up exhibit No. 46, which is in the binder \nin front of\nyou.\n\nAnd that is a letter [to] Alston\n&amp; Bird...\n\n\nIs that correct?\n', 
'This is correct.\n', 
'Okay.']
</code></pre>
<p><strong>What I Want</strong></p>
<p>This is close to what I want, but has the following problems:</p>
<ul>
<li>It's not always clear if a statement is a Question or an Answer, and</li>
<li>Sometimes, such as in this particular example, the first item in the list may be neither a Question nor Answer, but just random text before the first <code>\Q</code> delimiter.</li>
</ul>
<p>I would like a modified version of the my <code>q_a_list</code> above, but which addresses the two bulleted problems by linking each text chunk to the delimiter that preceded it. Something like:</p>
<pre><code>[{'0': '\na\n', 
  '\nQ': 'So I do first want to bring up exhibit No. 46, which is in the binder \nin front of\nyou.\n\nAnd that is a letter [to] Alston\n&amp; Bird...\n\n\nIs that correct?\n',
  '\nA': 'This is correct.\n',
  '\nQ': 'Okay.'}]
</code></pre>
<p>or</p>
<pre><code>[{'\nQ': 'So I do first want to bring up exhibit No. 46, which is in the binder \nin front of\nyou.\n\nAnd that is a letter [to] Alston\n&amp; Bird...\n\n\nIs that correct?\n',
  '\nA': 'This is correct.\n',
  '\nQ': 'Okay.'}]
</code></pre>
<p>or maybe even just a list with delimiters pre-pended:</p>
<pre><code>['\nQ: So I do first want to bring up exhibit No. 46, which is in the binder \nin front of\nyou.\n\nAnd that is a letter [to] Alston\n&amp; Bird...\n\n\nIs that correct?\n',
'\nA: This is correct.\n',
'\nQ: Okay.'
]
</code></pre>
"
"75371762","How can i group words to reduce vocabulary in python td idf vectorizer","2023-02-07 10:03:32","2","108","2","1","","75373688","<p>I want to reduce the size of the sparse matrix of the tf-idf vectorizer outputs since i am using it with cosine similarity and it takes a long time to go through each vector. I have about 44,000 sentences so the vocabulary size is also very large.</p>
<p>I was wondering if there was a way to combine a group of words to mean one word for example teal, navy and turquiose will all mean blue and that will have same tf-idf value.</p>
<p>I am dealing with a dataset of clothing items so things like colour, and similar clothing articles like shirt, t-shirt and sweatshirts are things i want to group.</p>
<p>I know i can use stop words to give certain words a value of 1 but is it possible to group words to have the same value?</p>
<p>Here is my code</p>
<pre><code>import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

dataset_2 = &quot;/dataset_files/styles_2.csv&quot;
df = pd.read_csv(dataset_2)
df = df.drop(['gender', 'masterCategory', 'subCategory', 'articleType', 'baseColour', 'season', 'year', 'usage'], axis = 1)

tfidf = TfidfVectorizer(stop_words='english') 
tfidf_matrix = tfidf.fit_transform(new_df['ProductDisplayName'])
cos_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)

</code></pre>
"
"75335523","Error 400 when using GPT API (in JavaScript)","2023-02-03 12:08:08","1","2336","1","2","","75335600","<p>I keep getting a 400 Error when I try to run my very basic chatbot using the GPT API:
<a href=""https://i.sstatic.net/VzyEy.png"" rel=""nofollow noreferrer"">error</a></p>
<p>Attached is my code; am I doing something wrong with the API key?</p>
<pre><code>const chatHistoryContent = document.querySelector(&quot;#chat-history-content&quot;);
const chatMessageInput = document.querySelector(&quot;#chat-message-input&quot;);
const chatMessageSubmit = document.querySelector(&quot;#chat-message-submit&quot;);



chatMessageSubmit.addEventListener(&quot;click&quot;, async function () {
    const message = chatMessageInput.value;
    chatMessageInput.value = &quot;&quot;;

    // Add the user's message to the chat history
    const userMessageDiv = document.createElement(&quot;div&quot;);
    userMessageDiv.innerHTML = `You: ${message}`;
    chatHistoryContent.appendChild(userMessageDiv);

    // Use the OpenAI GPT-3 API to get a response from the chatbot
    const response = await getResponseFromAPI(message);

    // Add the chatbot's response to the chat history
    const chatbotMessageDiv = document.createElement(&quot;div&quot;);
    chatbotMessageDiv.innerHTML = `Max: ${response}`;
    chatHistoryContent.appendChild(chatbotMessageDiv);
});

async function getResponseFromAPI(message) {

    const apiKey = &quot;sk-myapikey&quot;;
    const endpoint = `https://api.openai.com/v1/engines/davinci/jobs`;

    const response = await fetch(endpoint, {
        method: &quot;POST&quot;,
        headers: {
            &quot;Content-Type&quot;: `application/json`,
            &quot;Authorization&quot;: `Bearer ${apiKey}`,
        },
        body: JSON.stringify({
            model: &quot;text-davinci-003&quot;,
            prompt: &quot;test prompt&quot;, 
            temperature: 0.5,
            max_tokens: 512,
            top_p: 1,
            frequency_penalty: 0,
            presence_penalty: 0,
        })
    });

    const data = await response.json();
    return data.choices[0].text;
}
</code></pre>
<p>Thanks</p>
<p>I have tried consulting many websites to see solutions to this but have had no luck.</p>
"
"75326344","NLP classification with sparse and numerical features crashes","2023-02-02 16:44:14","0","72","5","1","","77515871","<p>I have a dataset of 10 million english shows, which has been cleaned and lemmatized, and their classification into different category types such as comedy, documentary, action, ... etc</p>
<p>I also have a feature called <code>duration</code>, which is the length of the tv show.</p>
<p>Data can be found <a href=""https://drive.google.com/file/d/16kRQfTo_76yfNzQ2_WHBdNg-U5QAQn6l/view?usp=share_link"" rel=""nofollow noreferrer"">here</a></p>
<p>I perform tfidf vectorization on the titles, which returns a sparse matrix and normalization on the duration column.</p>
<p>Then I want to feed the data to a logistic regression classifier.</p>
<p>side question: I want to know if theres a better way to handle combining a sparse matrix and a numerical column</p>
<p>when I try to do it using <code>todense()</code> or <code>toarray()</code>, It works</p>
<p>When i pass it to the logistic regression function, the notebook crashes. But if i dont have the duration col, which means i dont have to apply the toarray() or todense() function, it works perfectly. Is this a memory issue?</p>
<p>This is my code:</p>
<pre><code>import os

import pandas as pd

from sklearn import metrics
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import LogisticRegression

def normalize(df, col = ''):
    mms = MinMaxScaler()
    mms_col = mms.fit_transform(df[[col]])
    return mms_col

def tfidf(X, col = ''):
    tfidf_vectorizer = TfidfVectorizer(max_df = 0.8, max_features = 10000)
    return tfidf_vectorizer.fit_transform(X[col])

def get_training_data(df):
    df = shuffle(pd.read_csv(df).dropna())
    data = df[['name_title', 'Duration']]

    X_duration = normalize(data, col = 'Duration')
    X_sparse = tfidf(data, col = 'name_title')
    X = pd.DataFrame(X_sparse.toarray())

    X['Duration'] = X_duration
    y = df['target']

    return X, y

def logistic_regression(X, y):
    X_train, X_test, y_train, y_test = train_test_split(X, y)
    lr = LogisticRegression(C = 100.0, random_state = 1, solver = 'lbfgs', multi_class = 'ovr')
    lr.fit(X_train, y_train)
    y_predict = lr.predict(X_test)
    print(y_predict)
    print(&quot;Logistic Regression Accuracy %.3f&quot; %metrics.accuracy_score(y_test, y_predict))

data_path = '../data/'
X, y = get_training_data(os.path.join(data_path, 'podcasts_en_processed.csv'))
print(X.shape) # this prints (971426, 10001)
logistic_regression(X, y)
</code></pre>
"
"75214153","Lemmatization taking forever with Spacy","2023-01-23 19:26:53","1","1028","2","1","","75215495","<p>I'm trying to lemmatize chat registers in a dataframe using spacy. My code is:</p>
<pre><code>nlp = spacy.load(&quot;es_core_news_sm&quot;)
df[&quot;text_lemma&quot;] = df[&quot;text&quot;].apply(lambda row: &quot; &quot;.join([w.lemma_ for w in nlp(row)]))
</code></pre>
<p>I have aprox 600.000 rows and the apply takes more than two hours to execute. Is there a faster package/way to lemmatize? (I need a solution that works for spanish)</p>
<p>I have only tried using spacy package</p>
"
"75211491","Increasing efficiency for zero shot classification","2023-01-23 15:18:53","0","194","0","1","","75250052","<p>I am using zero shot classification to label large amounts of data. I have written a simple function to assist me with this and am wondering if there is a better way for this to run. My current logic was to take the highest score and label and append this label into a dataframe.</p>
<pre><code>def labeler(input_df,output_df):
    labels = ['Fruit','Vegetable','Meat','Other']


    for i in tqdm(range(len(input_df))):
        temp = classifier(input_df['description'][i],labels)
        output ={'work_order_num':input_df['order_num'][i],
                 'work_order_desc':input_df['description'][i],
                'label':temp['labels'][0],
                'score':temp['scores'][0]}
        output_df.append(output)
</code></pre>
<p>In terms of speed and resources would it be better to shape this function with lambda?</p>
"
"75173490","How can I check similarity in meaning and not just having same words between two texts with spacy","2023-01-19 14:06:08","2","1258","4","1","","75193330","<p>I'm trying to compare two different texts—one coming from a Curriculum Vitae (CV) and the other from a job announcement.</p>
<p>After cleaning the texts, I'm trying to compare them to detect if a job announcement is more linked to a specific CV.</p>
<p>I am trying to do this using similarity matching in spaCy via the following code:</p>
<pre class=""lang-py prettyprint-override""><code>similarity = pdf_text.similarity(final_text_from_annonce)
</code></pre>
<p>This works well, but I'm getting strange results from two different CVs for the same job announcement. Specifically, I get the same similarity score (~0.6), however, one should clearly be higher than the other.</p>
<p>I checked on spaCy website and I found this very important sentence:</p>
<blockquote>
<p>Vector averaging means that the vector of multiple tokens is insensitive to the order of the words. Two documents expressing the same meaning with dissimilar wording will return a lower similarity score than two documents that happen to contain the same words while expressing different meanings.</p>
</blockquote>
<p>So, what do I need to use or code to make spaCy compare my two texts based on their <em>meaning</em> instead of the occurrence of words?</p>
<p>I am expecting a parameter for the <code>similarity</code> function of spaCy, or another function that will compare my both texts and calculate a similarity score based on the meaning of the texts and not if the same words are used.</p>
"
"75116397","How to define pos_pattern for extracting nouns followed by zero or more sequence of nouns or adjectives for KeyphraseCountVectorizer?","2023-01-14 07:17:36","2","272","0","1","","75121673","<p>I'm trying to extract Arabic keywords from tweets. I'm using keyBERT with KeyphraseCountVectorizer</p>
<p><code>vectorizer = KeyphraseCountVectorizer(pos_pattern='&lt; N.*&gt;*')</code></p>
<p>I'm trying to write more custom pos patterns regExp to select nouns followed by zero or more sequence of nouns or adjectives but not verbs.
can you please help me to write the right regExp?
Thank you</p>
"
"75038237","Not loading all checkpoints when training again","2023-01-07 05:07:41","0","53","0","1","","75045505","<p>I want to be able to start training the relevant model from the continuation of the previous day's training, but each time the training starts from a certain checkpoint, not from the last checkpoint, and this makes the training time of the model longer each time. By changing the value of &quot;continue_from_global_step&quot; parameter to 1, there was no change in the result.
Code snippet related to loading checkpoints:</p>
<h1>Training</h1>
<pre><code>if args.do_train:
    # If output files already exists, assume to continue training from latest checkpoint (unless overwrite_output_dir is set)
    continue_from_global_step = 0 # If set to 0, start training from the beginning
    if os.path.exists(args.output_dir) and os.listdir(args.output_dir) and args.do_train and not args.overwrite_output_dir:
        checkpoints = list(os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + '/*/' + WEIGHTS_NAME, recursive=True)))
        if len(checkpoints) &gt; 0:
            checkpoint = checkpoints[-1]
            logger.info(&quot;Resuming training from the latest checkpoint: %s&quot;, checkpoint)
            continue_from_global_step = int(checkpoint.split('-')[-1])
            model = model_class.from_pretrained(checkpoint)
            model.to(args.device)
    
    train_dataset, features = load_and_cache_examples(args, model, tokenizer, processor, evaluate=False)
    global_step, tr_loss = train(args, train_dataset, features, model, tokenizer, processor, continue_from_global_step)
    logger.info(&quot; global_step = %s, average loss = %s&quot;, global_step, tr_loss)
</code></pre>
"
"75013624","Early stopping based on BLEU in FairSeq","2023-01-05 03:40:24","1","449","0","1","","75015379","<p>My goal is to use BLEU as early stopping metric while training a translation model in FairSeq.</p>
<p>Following the documentation, I am adding the following arguments to my training script:</p>
<pre><code>--eval-bleu --eval-bleu-args --eval-bleu-detok --eval-bleu-remove-bpe
</code></pre>
<p>I am getting the following error:</p>
<pre><code>fairseq-train: error: unrecognized arguments: --eval-bleu --eval-bleu-args --eval-bleu-detok --eval-bleu-remove-bpe
</code></pre>
<p>System information:</p>
<ul>
<li>fairseq version: 0.10.2</li>
<li>torch: 1.10.1+cu113</li>
</ul>
<p>More Details:</p>
<p>When I am trying to finetune M2M100 model, I am getting error as:</p>
<p>KeyError: 'bleu'</p>
<p>when using following:</p>
<pre class=""lang-bash prettyprint-override""><code>CUDA_VISIBLE_DEVICES=0,1,2,3 fairseq-train \
    $path_2_data --ddp-backend=no_c10d \
    --best-checkpoint-metric bleu \
    --maximize-best-checkpoint-metric \
    --max-tokens 2048 --no-epoch-checkpoints \
    --finetune-from-model $pretrained_model \
    --save-dir $checkpoint --task translation_multi_simple_epoch \
    --encoder-normalize-before \
    --langs 'af,am,ar,ast,az,ba,be,bg,bn,br,bs,ca,ceb,cs,cy,da,de,el,en,es,et,fa,ff,fi,fr,fy,ga,gd,gl,gu,ha,he,hi,hr,ht,hu,hy,id,ig,ilo,is,it,ja,jv,ka,kk,km,kn,ko,lb,lg,ln,lo,lt,lv,mg,mk,ml,mn,mr,ms,my,ne,nl,no,ns,oc,or,pa,pl,ps,pt,ro,ru,sd,si,sk,sl,so,sq,sr,ss,su,sv,sw,ta,th,tl,tn,tr,uk,ur,uz,vi,wo,xh,yi,yo,zh,zu' \
    --lang-pairs $lang_pairs \
    --decoder-normalize-before --sampling-method temperature \
    --sampling-temperature 1.5 --encoder-langtok src \
    --decoder-langtok --criterion label_smoothed_cross_entropy \
    --label-smoothing 0.2 --optimizer adam --adam-eps 1e-06
    --adam-betas '(0.9, 0.98)' --lr-scheduler inverse_sqrt \
    --lr 3e-05 --warmup-updates 2500 --max-update 400000 \
    --dropout 0.3 --attention-dropout 0.1 \
    --weight-decay 0.0 --update-freq 2 --save-interval 1 \
    --save-interval-updates 5000 --keep-interval-updates 10 \
    --seed 222 --log-format simple --log-interval 2 --patience 5  \
    --arch transformer_wmt_en_de_big --encoder-layers 24 \
    --decoder-layers 24 --encoder-ffn-embed-dim 8192 \
    --decoder-ffn-embed-dim 8192 --encoder-layerdrop 0.05 \
    --decoder-layerdrop 0.05 --share-decoder-input-output-embed \
    --share-all-embeddings --fixed-dictionary $fix_dict --fp16 \
    --skip-invalid-size-inputs-valid-test
</code></pre>
"
"74953747","Why is the spaCy Scorer returning None for the entity scores but the model is extracting entities?","2022-12-29 16:37:40","0","362","0","2","","74960186","<p>I am really confused why the Scorer.score is returning ents_p, ents_r, and ents_f as None for the below example. I am seeing something every similar with my own custom model and want to understand why it is returning None?</p>
<p><strong>Example Scorer Code - Returning None for ents_p, ents_r, ents_f</strong></p>
<pre><code>import spacy
from spacy.scorer import Scorer
from spacy.tokens import Doc
from spacy.training.example import Example

examples = [
    ('Who is Talha Tayyab?',
     {(7, 19, 'PERSON')}),
    ('I like London and Berlin.',
     {(7, 13, 'LOC'), (18, 24, 'LOC')}),
     ('Agra is famous for Tajmahal, The CEO of Facebook will visit India shortly to meet Murari Mahaseth and to visit Tajmahal.',
     {(0, 4, 'LOC'), (40, 48, 'ORG'), (60, 65, 'GPE'), (82, 97, 'PERSON'), (111, 119, 'GPE')})
]

def my_evaluate(ner_model, examples):
    scorer = Scorer()
    example = []
    for input_, annotations in examples:
        pred = ner_model(input_)
        print(pred,annotations)
        temp = Example.from_dict(pred, dict.fromkeys(annotations))
        example.append(temp)
    scores = scorer.score(example)
    return scores

ner_model = spacy.load('en_core_web_sm') # for spaCy's pretrained use 'en_core_web_sm'
results = my_evaluate(ner_model, examples)
print(results)
</code></pre>
<p><strong>Scorer Results</strong></p>
<pre><code>{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'sents_p': None, 'sents_r': None, 'sents_f': None, 'tag_acc': None, 'pos_acc': None, 'morph_acc': None, 'morph_micro_p': None, 'morph_micro_r': None, 'morph_micro_f': None, 'morph_per_feat': None, 'dep_uas': None, 'dep_las': None, 'dep_las_per_type': None, 'ents_p': None, 'ents_r': None, 'ents_f': None, 'ents_per_type': None, 'cats_score': 0.0, 'cats_score_desc': 'macro F', 'cats_micro_p': 0.0, 'cats_micro_r': 0.0, 'cats_micro_f': 0.0, 'cats_macro_p': 0.0, 'cats_macro_r': 0.0, 'cats_macro_f': 0.0, 'cats_macro_auc': 0.0, 'cats_f_per_type': {}, 'cats_auc_per_type': {}}
</code></pre>
<p>It is clearly picking out entities from the text</p>
<pre><code>doc = ner_model('Agra is famous for Tajmahal, The CEO of Facebook will visit India shortly to meet Murari Mahaseth and to visit Tajmahal.')
for ent in doc.ents:
    print(ent.text, ent.label_)
</code></pre>
<p><strong>Output</strong></p>
<pre><code>Agra PERSON
Tajmahal ORG
Facebook ORG
India GPE
Murari Mahaseth PERSON
Tajmahal ORG
</code></pre>
"
"74943838","Split several sentences in pandas dataframe","2022-12-28 18:37:01","0","154","3","2","","74944042","<p>I have a pandas dataframe with a column that looks like this.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>sentences</th>
</tr>
</thead>
<tbody>
<tr>
<td>['This is text.', 'This is another text.', 'This is also text.', 'Even more text.']</td>
</tr>
<tr>
<td>['This is the same in another row.', 'Another row another text.', 'Text in second row.', 'Last text in second row.']</td>
</tr>
</tbody>
</table>
</div>
<p>In every row there are 10 sentences in ' ' or &quot; &quot; separated by commas. The column type is &quot;str&quot;. I was not able to transform it to a list of strings.</p>
<p>I want to transform the values of this dataframe that they look like this:</p>
<pre><code>[['This', 'is', 'text'], ['This', 'is', 'another', 'text'], ['This', 'is', 'also', 'text'], ['Even', 'more', 'text']]
</code></pre>
<p>I tried something like this:</p>
<pre><code>    new_splits = []
    for num in range(len(refs)):
      komma = refs[num].replace(&quot; &quot;, &quot;\', \'&quot;)#regex=True)
      new_splits.append(komma)
</code></pre>
<p>and this:</p>
<pre><code>    new_splits = []
    for num in range(len(refs)):
      splitted = refs[num].split(&quot;', '&quot;)
      new_splits.append(splitted)
</code></pre>
<p>Disclaimer: I need this for evaluating bleu score and haven't found a way to do this for this kind of dataset. Thanks in advance!</p>
"
"74922924","How to add threshold limit to TF-IDF values in a sparse matrix","2022-12-26 18:14:27","2","553","0","1","","74923600","<p>I am using sklearn.feature_extraction.text, TfidfTransformer to get the TF_IDF values for my corpus.</p>
<p>This is how my code looks like</p>
<pre><code>    X = dataset[:,0]
    Y = dataset[:,1]

    for index, item in enumerate(X):
        reqJson = json.loads(item, object_pairs_hook=OrderedDict)
        X[index] = json.dumps(reqJson, separators=(',', ':'))
    count_vect = CountVectorizer()
    X_train_counts = count_vect.fit_transform(X)


    tfidf_transformer = TfidfTransformer()
    X_train_tfidf = (tfidf_transformer.fit_transform(X_train_counts))

    #(58720, 167216) is the size of my sparse matrix


    for i in range (0,58720):
        for j in range (0,167216):
            print(i,j)
            if X_train_tfidf[i,j]&gt;0.35:
                X_train_tfidf[i,j]=0
</code></pre>
<p>As you can see that I want to filter out tf-idf values which more than 0.35 so that I can reduce my feature set and make my model more time efficient but using a for loop just makes worse. I have looked into the documentation of TfidfTransformer but cannot find a way to make it any better. Any ideas or tips? Thank you.</p>
"
"74861149","how to get only the nouns from a sentence","2022-12-20 09:46:02","0","133","2","1","","74862110","<p>I'm trying to find out which nouns exist in a sentence, i'm using pos_tag from nltk but it's not working very well
here is my code/function</p>
<pre><code>def Noun(sentence):
    lista=[]
    words=(word_tokenize(sentence))
    pos=pos_tag(words)
    for i in range(len(pos)):
        if((pos[i][1].startswith('N'))):
            lista.append(pos[i][0])
        else:
            pass
    return pos,lista


</code></pre>
<p>for example :
tweet=&quot;let's talk to Thomas and check if he will come to the party&quot;
Noun(tweet)
expected :</p>
<pre><code>output: ['Thomas','party']
</code></pre>
<p>what i got:</p>
<pre><code>['let', 'talk', 'Thomas', 'party'])
</code></pre>
"
"74851128","Language detection for short user-generated string","2022-12-19 13:20:52","3","2069","2","2","","74972235","<p>I need to detect the language of text sent in chat, and I am faced with 2 problems:</p>
<ul>
<li>the length of the message</li>
<li>the errors that may be in it and the noise (emoji etc...)</li>
</ul>
<p>For the noise, I clean the message and that works fine, but the length of the message is a problem.</p>
<p>For example, if a user writes &quot;hi&quot;, Fasttext detects the language as Dutch text, but Google Translate detects it as English. And most likely it is a message in English.</p>
<p>I try to train my own Fasttext model, but how can I adjust the model to have better results with short strings? Do I need to train the model with the dictionary of a lot of languages to get a better result?</p>
<p>I use Fasttext because it's the most accurate language detector.</p>
<p>Here is an exemple of the problem with Fasttext:</p>
<pre class=""lang-py prettyprint-override""><code># wget https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin

import fasttext

text = &quot;Hi&quot;

pretrained_lang_model = &quot;lid.176.bin&quot;
model = fasttext.load_model(pretrained_lang_model)

predictions = model.predict(text, k=2)
print(predictions)
# (('__label__de', '__label__en'), array([0.51606238, 0.31865335]))
</code></pre>
"
"74835558","Python: Can I create a dummy based on search conditions in one column with text series?","2022-12-17 16:11:25","0","103","0","2","","74836612","<p>I was wondering how I could create a dummy variable for the following condition: column 'lemmatised' contains at least two words from 'innovation_words'. Innovation_words is a list I defined myself:</p>
<pre><code>innovation_words = ['community', 'local', 'charity', 'event', 'partner',
                'volunteering', 'plastic', 'surplusfood']
</code></pre>
<p>The lemmatised column looks like this (I'm fine changing the type or formatting if needed):</p>
<p><a href=""https://i.sstatic.net/tWdud.png"" rel=""nofollow noreferrer"">data to use for condition</a></p>
<p>So, if any observation includes for example <em>local</em> and <em>plastic</em>, I would like to have a dummy variable: 'innovation' = 1. Hope someone can help me with this. Some code I already tried:</p>
<pre><code>conditions = [df_posts['lemmatised'].isin(innovation_words), 
          df_posts['lemmatised'].isin(innovation_words)]

dummy = [1,0]

df_posts['innovation'] = np.select(conditions, dummy)
</code></pre>
"
"74705964","Why does the nltk lemmatizer not work for every word in Python?","2022-12-06 16:30:13","0","442","0","1","","74706121","<pre><code>import ntlk
lemmatizer = ntlk.WordNetLemmatizer()
print(lemmatizer.lemmatize(&quot;goes&quot;))
print(lemmatizer.lemmatize(&quot;transforming&quot;)) 
</code></pre>
<p>The first example will with &quot;goes&quot; do work. The output is: &quot;go&quot;. The second does not work. I get the output &quot;transforming&quot; but should be &quot;transform&quot;.</p>
"
"74576157","NLP stemming with JavaScript and PHP pages in the browser","2022-11-25 17:23:59","0","615","0","1","","74580029","<p>I'm trying to figure out, how to implement and use <a href=""https://www.tabnine.com/code/javascript/functions/stem"" rel=""nofollow noreferrer"">stemming</a> results with JavaScript and PHP pages in the browser.</p>
<p>By using <code>node index.js</code> in the VS Code terminal I got the output <code>word</code> using <a href=""https://github.com/NaturalNode/natural"" rel=""nofollow noreferrer"">natural</a>:</p>
<pre><code>var natural = require(&quot;natural&quot;);
console.log(natural.PorterStemmer.stem(&quot;words&quot;)); 
</code></pre>
<p>with <a href=""https://github.com/axa-group/nlp.js"" rel=""nofollow noreferrer"">NLP.js</a> librarie:</p>
<pre><code>const { StemmerIt } = require(&quot;@nlpjs/lang-it&quot;);
const stemmer = new StemmerIt();
const input = [&quot;ho&quot;, &quot;visto&quot;, &quot;uno&quot;, &quot;sviluppatore&quot;];
console.log(stemmer.stem(input));
</code></pre>
<p>they are <a href=""https://nodejs.org/en/"" rel=""nofollow noreferrer"">Node.js</a> libraries, and I got an error in the Chrome browser console log using <code>require</code>:</p>
<blockquote>
<p>Uncaught ReferenceError: require is not defined
at index.js:320:15</p>
</blockquote>
<p>I'm not sure if there is some way to use libraries with the browser, like for example <a href=""https://brain.js.org/#/"" rel=""nofollow noreferrer"">Brain.js</a> providing <a href=""https://www.npmjs.com/package/brain.js?activeTab=readme#NPM"" rel=""nofollow noreferrer"">NPM and CND</a>.</p>
<p>I did not try to use <a href=""https://github.com/spencermountain/compromise/"" rel=""nofollow noreferrer"">Compromise.cool</a> library yet, which can be used to run NLP on the browser, looking for a <code>stemming</code> method.</p>
<p>I've tried <a href=""https://tartarus.org/martin/PorterStemmer/js.txt"" rel=""nofollow noreferrer"">Porters Stemming Algorithm Javascript</a> but can't figure out, how to use it as a function. Seems like I'm doing it incorrectly:</p>
<pre><code>var result = stemmer(&quot;words&quot;);
console.log(result);
</code></pre>
<blockquote>
<p>Uncaught TypeError: stemmer is not a function
at index.js:346:14</p>
</blockquote>
<p>Also, I've tried to follow <a href=""https://betterprogramming.pub/natural-language-processing-in-the-browser-8ca5fdf2488b"" rel=""nofollow noreferrer"">Natural Language Processing in the Browser</a> guide, separately installing each dependency listed in <code>package.json</code>:</p>
<pre><code>&quot;@nlpjs/core&quot;: &quot;^4.14.0&quot;,
&quot;@nlpjs/lang-en-min&quot;: &quot;^4.14.0&quot;,
&quot;@nlpjs/nlp&quot;: &quot;^4.15.0&quot;,
&quot;@nlpjs/request-rn&quot;: &quot;^4.14.3&quot;,
&quot;browserify&quot;: &quot;^17.0.0&quot;,
&quot;terser&quot;: &quot;^5.3.8&quot;
</code></pre>
<p>but <code>npm run build</code> throws the error:</p>
<blockquote>
<p>10 error missing script: build</p>
</blockquote>
<p>Any advice, guide, or example would be useful.</p>
"
"74519464","AttributeError: 'tuple' object has no attribute 'rank' when calling model.fit() in NLP task","2022-11-21 13:08:51","1","556","0","1","","78104970","<p>I'm following this tutorial
<a href=""https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-9-neural-networks-with-tfidf-vectors-using-d0b4af6be6d7"" rel=""nofollow noreferrer"">https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-9-neural-networks-with-tfidf-vectors-using-d0b4af6be6d7</a></p>
<p>However, while implementing the ANN based on the TF-IDF features, I'm getting this error
<strong>AttributeError: 'tuple' object has no attribute 'rank'</strong></p>
<p>This is the snippet-</p>
<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
tvec1 = TfidfVectorizer(max_features=100000,ngram_range=(1, 3))
tvec1.fit(x_train)


x_train_tfidf = tvec1.transform(x_train)
x_validation_tfidf = tvec1.transform(x_validation).toarray()

seed = 7
np.random.seed(seed)
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.layers import Flatten
#from tensorflow.keras.layers.embeddings import Embedding
from tensorflow.keras.layers import Embedding
from tensorflow.keras.preprocessing import sequence

def batch_generator(X_data, y_data, batch_size):
    samples_per_epoch = X_data.shape[0]
    number_of_batches = samples_per_epoch/batch_size
    counter=0
    index = np.arange(np.shape(y_data)[0])
    while 1:
        index_batch = index[batch_size*counter:batch_size*(counter+1)]
        X_batch = X_data[index_batch,:].toarray()
        y_batch = y_data[y_data.index[index_batch]]
        counter += 1
        yield X_batch,y_batch
        if (counter &gt; number_of_batches):
            counter=0

model = Sequential()
model.add(Dense(64, activation='relu', input_dim=100000))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

model.fit(batch_generator(x_train_tfidf, y_train, 32), epochs=5, validation_data=(x_validation_tfidf, y_validation),steps_per_epoch=x_train_tfidf.shape[0]/32)
</code></pre>
<p>This is the error-</p>
<pre><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
~\AppData\Local\Temp\ipykernel_13000\1276649087.py in &lt;module&gt;
      1 model.fit(batch_generator(x_train_tfidf, y_train, 32),
      2                     epochs=5, validation_data=(x_validation_tfidf, y_validation),
----&gt; 3                     steps_per_epoch=x_train_tfidf.shape[0]/32)

~\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\keras\engine\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
   1145           use_multiprocessing=use_multiprocessing,
   1146           model=self,
-&gt; 1147           steps_per_execution=self._steps_per_execution)
   1148 
   1149       # Container that configures and calls `tf.keras.Callback`s.

~\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\keras\engine\data_adapter.py in get_data_handler(*args, **kwargs)
   1362   if getattr(kwargs[&quot;model&quot;], &quot;_cluster_coordinator&quot;, None):
   1363     return _ClusterCoordinatorDataHandler(*args, **kwargs)
-&gt; 1364   return DataHandler(*args, **kwargs)
   1365 
   1366 

~\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\keras\engine\data_adapter.py in __init__(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)
   1164         use_multiprocessing=use_multiprocessing,
   1165         distribution_strategy=ds_context.get_strategy(),
-&gt; 1166         model=model)
   1167 
   1168     strategy = ds_context.get_strategy()

~\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\keras\engine\data_adapter.py in __init__(self, x, y, sample_weights, workers, use_multiprocessing, max_queue_size, model, **kwargs)
    826       return tensor_shape.TensorShape([None for _ in shape.as_list()])
    827 
--&gt; 828     output_shapes = nest.map_structure(_get_dynamic_shape, peek)
    829     output_types = nest.map_structure(lambda t: t.dtype, peek)
    830 

~\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\util\nest.py in map_structure(func, *structure, **kwargs)
    865 
    866   return pack_sequence_as(
--&gt; 867       structure[0], [func(*x) for x in entries],
    868       expand_composites=expand_composites)
    869 

~\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\util\nest.py in &lt;listcomp&gt;(.0)
    865 
    866   return pack_sequence_as(
--&gt; 867       structure[0], [func(*x) for x in entries],
    868       expand_composites=expand_composites)
    869 

~\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\keras\engine\data_adapter.py in _get_dynamic_shape(t)
    822       shape = t.shape
    823       # Unknown number of dimensions, `as_list` cannot be called.
--&gt; 824       if shape.rank is None:
    825         return shape
    826       return tensor_shape.TensorShape([None for _ in shape.as_list()])

AttributeError: 'tuple' object has no attribute 'rank'
</code></pre>
"
"74402544","WordNet Hierarchy","2022-11-11 12:30:20","0","250","0","1","","74437039","<p>In the WordNet synsets, there are a bunch of concepts such as hyponyms, hypernys, holonyms and meronyms in the NLTK library.</p>
<p>Can someone explain these terms and maybe provide a few examples for this?</p>
"
"74248881","How to extract phrases from text using specific noun-verb-noun NLTK PoS tag patterns?","2022-10-29 21:14:03","1","497","0","1","","74249680","<p>I have a data frame that has a column containing some text.</p>
<p>I want to extract phrases from the text with the format <code>NN + VB + NN</code> or <code>NN + NN + VB + NN</code> or <code>NN + ... + NN + VB + NN</code> et cetera. Basically, I want to get the simple phrases with 1 to n <code>nouns</code> before the first encountered <code>verb</code>, followed by a <code>noun</code>.</p>
<p>I'm using <code>nltk.pos_tag</code> after tokenizing the texts to get the tag of each word, however I cannot find a way to get what I want.</p>
<p>I also thought about <code>bigrams</code>, <code>trigrams</code>, <code>ngrams</code> etc. but couldn't find a way to apply it.</p>
<p>Any help, please?</p>
"
"74196558","How do I retrieve phrases from a NLTK.tree using custom node labels?","2022-10-25 15:29:07","1","112","0","1","","74201199","<p>Given a NLTK tree produced using the code below, how do I retrieve the leaf values (phrases) that potentially match all of the node labels assigned using the <code>nltk.RegexParser</code> (e.g. those phrases which match the <code>Present_Indefinite</code> or <code>Present_Perfect</code> tense)?</p>
<pre class=""lang-py prettyprint-override""><code>from nltk import word_tokenize, pos_tag
import nltk

text = &quot;#NOVAVAX has produced the #NUVAXOVID vaccine.\
 Will that provide a new rally? We see Biotechnology\
  Stock $NVAX Entering the Buying Area.&quot;
tokenized = word_tokenize(text) # Tokenize text
tagged = pos_tag(tokenized) # Tag tokenized text with PoS tags

my_grammar = r&quot;&quot;&quot;
Future_Perfect_Continuous: {&lt;MD&gt;&lt;VB&gt;&lt;VBN&gt;&lt;VBG&gt;}
Future_Continuous:         {&lt;MD&gt;&lt;VB&gt;&lt;VBG&gt;}
Future_Perfect:            {&lt;MD&gt;&lt;VB&gt;&lt;VBN&gt;}
Past_Perfect_Continuous:   {&lt;VBD&gt;&lt;VBN&gt;&lt;VBG&gt;}
Present_Perfect_Continuous:{&lt;VBP|VBZ&gt;&lt;VBN&gt;&lt;VBG&gt;}
Future_Indefinite:         {&lt;MD&gt;&lt;VB&gt;}
Past_Continuous:           {&lt;VBD&gt;&lt;VBG&gt;}
Past_Perfect:              {&lt;VBD&gt;&lt;VBN&gt;}
Present_Continuous:        {&lt;VBZ|VBP&gt;&lt;VBG&gt;}
Present_Perfect:           {&lt;VBZ|VBP&gt;&lt;VBN&gt;}
Past_Indefinite:           {&lt;VBD&gt;}
Present_Indefinite:        {&lt;VBZ&gt;|&lt;VBP&gt;}&quot;&quot;&quot;


def check_grammar(grammar, tags):
    cp = nltk.RegexpParser(grammar)
    result = cp.parse(tags)
    return result

# Apply regex parser and create parse tree
result = check_grammar(my_grammar, tagged)
print(type(result))
# Output: &lt;class 'nltk.tree.tree.Tree'&gt;
</code></pre>
<p>More specifically, given that the output of <code>print(result)</code> is as shown below, how can I retrieve the phrases labelled as <code>Present_Perfect</code> and <code>Present_Indefinite</code>, or more generally, any other phrases which match the labels in my grammar?</p>
<pre class=""lang-none prettyprint-override""><code>(S
  #/#
  NOVAVAX/NNP
  (Present_Perfect has/VBZ produced/VBN)
  the/DT
  #/#
  NUVAXOVID/NNP
  vaccine/NN
  ./.
  Will/MD
  that/WDT
  provide/VB
  a/DT
  new/JJ
  rally/NN
  ?/.
  We/PRP
  (Present_Indefinite see/VBP)
  Biotechnology/NNP
  Stock/NNP
  $/$
  NVAX/NNP
  Entering/NNP
  the/DT
  Buying/NNP
  Area/NNP
  ./.)
</code></pre>
"
"74181750","A checklist for Spacy optimization?","2022-10-24 13:23:04","8","2920","2","1","","74193846","<p>I have been trying to understand how to systematically make Spacy run as fast as possible for a long time and I would like this post to become a wiki-style public post if possible.</p>
<p>Here is what I currently know, with subsidiary questions on each point:</p>
<p><strong>1. Space will run faster on faster hardware. For example, try a computer with more CPU cores, or more RAM/primary memory.</strong></p>
<p>What I do not know:</p>
<ul>
<li><em>What specific aspects of the execution of Spacy - especially the main one of instantiating the</em> <code>Doc</code> <em>object - depend more on CPU vs. RAM and why?</em></li>
<li><em>Is the instantiation of a</em> <code>Doc</code> <em>object a sequence of arithmetical calculations (the compiled binary of the neural networks), so the more CPU cores, the more calculations can be done at once, therefore faster? Does that mean increasing RAM would not make this process faster?</em></li>
<li><em>Are there any other aspects of CPUs or GPUs to watch out for, other than cores, that would make one chip better than another, for Spacy? Someone mentioned &quot;hyper threading&quot;.</em></li>
<li><em>Is there any standard mathematical estimate of time per pipeline component, such as parser, relative to input string length? Like Parser, seconds = number of characters in input? / number of CPU cores</em></li>
</ul>
<p><strong>2. You can make Spacy run faster by removing <a href=""https://spacy.io/usage/spacy-101#pipelines"" rel=""noreferrer"">components</a> you don't need, for example by</strong> <code>nlp = spacy.load(&quot;en_core_web_sm&quot;, disable=['tagger', 'ner', 'lemmatizer', 'textcat'])</code></p>
<ul>
<li><em>Just loading the Spacy module itself with <code>import spacy</code> is slightly slow. If you haven't even loaded the language model yet, what are the most significant things being loaded here, apart from just adding functions to the namespace? Is it possible to only load a part of the module you need?</em></li>
</ul>
<p><strong>3. You can make Spacy faster by using certain options that simply make it run faster.</strong></p>
<ul>
<li><em>I have read about multiprocessing with</em> <code>nlp.pipe</code>, <code>n_process</code>, <code>batch_size</code> and <code>joblib</code><em>, but that's for multiple documents and I'm only doing a single document right now.</em></li>
</ul>
<p><strong>4. You can make Spacy faster by minimising the number of times it has to perform the same operations.</strong></p>
<ul>
<li><p><em>You can keep Spacy alive on a server and pass processing commands to it when you need to</em></p>
</li>
<li><p><em>You can serialize a</em> <code>Doc</code> <em>to reload it later, and you can further exclude attributes you don't need with</em> <code>doc.to_bytes(exclude=[&quot;tensor&quot;])</code> or <code>doc.to_array([LOWER, POS, ENT_TYPE, IS_ALPHA])</code></p>
</li>
</ul>
<p><strong>5. Anything else?</strong></p>
"
"74161769","Python too slow to find text in string in for loop","2022-10-22 07:22:28","3","171","2","2","","74162147","<p>I want to <em><strong>improve the loop performance</strong></em> where it counts word occurrences in text, but it runs <em><strong>around 5 minutes for 5 records now</strong></em></p>
<p>DataFrame</p>
<pre><code>No                  Text   
1     I love you forever...*500 other words
2     No , i know that you know xxx *100 words
</code></pre>
<p>My word list</p>
<pre><code>wordlist =['i','love','David','Mary',......]
</code></pre>
<p>My code to count word</p>
<pre><code>for i in wordlist :
    df[i] = df['Text'].str.count(i)
</code></pre>
<p>Result :</p>
<pre><code>No   Text                  I    love  other_words
 1    I love you ...       1      1      4
 2    No, i know ...       1      0      5  
</code></pre>
"
"74146965","How to efficiently convert a large parallel corpus to a Huggingface dataset to train an EncoderDecoderModel?","2022-10-20 22:33:31","1","1764","2","2","","74230698","<h1>Typical EncoderDecoderModel that works on a Pre-coded Dataset</h1>
<p>The code snippet snippet as below is frequently used to train an <a href=""https://huggingface.co/docs/transformers/model_doc/encoder-decoder"" rel=""nofollow noreferrer""><code>EncoderDecoderModel</code></a> from Huggingface's transformer library</p>
<pre><code>from transformers import EncoderDecoderModel
from transformers import PreTrainedTokenizerFast

multibert = EncoderDecoderModel.from_encoder_decoder_pretrained(
    &quot;bert-base-multilingual-uncased&quot;, &quot;bert-base-multilingual-uncased&quot;
)


tokenizer = PreTrainedTokenizerFast.from_pretrained(&quot;bert-base-multilingual-uncased&quot;)

...
</code></pre>
<h1>And a pre-processed/coded dataset can be used to train the model as such, when using the <code>wmt14</code> dataset:</h1>
<pre><code>import datasets

train_data = datasets.load_dataset(&quot;wmt14&quot;, &quot;de-en&quot;, split=&quot;train&quot;)
val_data = datasets.load_dataset(&quot;wmt14&quot;, &quot;de-en&quot;, split=&quot;validation[:10%]&quot;)


from functools import partial

def process_data_to_model_inputs(batch, encoder_max_length=512, decoder_max_length=512, batch_size=2): 
    inputs = tokenizer([segment[&quot;en&quot;] for segment in batch['translation']], 
                       padding=&quot;max_length&quot;, truncation=True, max_length=encoder_max_length)
    outputs = tokenizer([segment[&quot;de&quot;] for segment in batch['translation']], 
                       padding=&quot;max_length&quot;, truncation=True, max_length=encoder_max_length)


    batch[&quot;input_ids&quot;] = inputs.input_ids
    batch[&quot;attention_mask&quot;] = inputs.attention_mask
    batch[&quot;decoder_input_ids&quot;] = outputs.input_ids
    batch[&quot;decoder_attention_mask&quot;] = outputs.attention_mask
    batch[&quot;labels&quot;] = outputs.input_ids.copy()

    # because BERT automatically shifts the labels, the labels correspond exactly to `decoder_input_ids`. 
    # We have to make sure that the PAD token is ignored
    batch[&quot;labels&quot;] = [[-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in batch[&quot;labels&quot;]]
    return batch


def munge_dataset_to_pacify_bert(dataset, encoder_max_length=512, decoder_max_length=512, batch_size=2):
    bert_wants_to_see = [&quot;input_ids&quot;, &quot;attention_mask&quot;, &quot;decoder_input_ids&quot;, 
                         &quot;decoder_attention_mask&quot;, &quot;labels&quot;]
    
    _process_data_to_model_inputs = partial(process_data_to_model_inputs, 
                                                encoder_max_length=encoder_max_length, 
                                                decoder_max_length=decoder_max_length, 
                                                batch_size=batch_size
                                           )
    dataset = dataset.map(_process_data_to_model_inputs, 
                           batched=True, 
                           batch_size=batch_size
                          )
    dataset.set_format(type=&quot;torch&quot;, columns=bert_wants_to_see)
    return dataset

train_data = munge_dataset_to_pacify_bert(train_data)
val_data = munge_dataset_to_pacify_bert(val_data)
</code></pre>
<h1>Then the training can be done easily as such:</h1>
<pre><code>from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments


# set training arguments - these params are not really tuned, feel free to change
training_args = Seq2SeqTrainingArguments(
    output_dir=&quot;./&quot;,
    evaluation_strategy=&quot;steps&quot;,
    ...
)


# instantiate trainer
trainer = Seq2SeqTrainer(
    model=multibert,
    tokenizer=tokenizer,
    args=training_args,
    train_dataset=train_data,
    eval_dataset=val_data,
)

trainer.train()
</code></pre>
<p>A working example can be found on something like: <a href=""https://www.kaggle.com/code/alvations/neural-plasticity-bert2bert-on-wmt14"" rel=""nofollow noreferrer"">https://www.kaggle.com/code/alvations/neural-plasticity-bert2bert-on-wmt14</a></p>
<h1>However, parallel data used to train an EncoderDecoderModel usually exists as <code>.txt</code> or <code>.tsv</code> files, not a pre-coded dataset</h1>
<p>Given a large <code>.tsv</code> file (e.g. 1 billion lines), e.g.</p>
<pre><code>hello world\tHallo Welt
how are you?\twie gehts?
...\t...
</code></pre>
<h4>Step 1: we can convert into the parquet / pyarrow format, one can do something like:</h4>
<pre><code>import vaex  # Using vaex 
import sys

filename = &quot;train.en-de.tsv&quot;

df = vaex.from_csv(filename, sep=&quot;\t&quot;, header=None, names=[&quot;src&quot;, &quot;trg&quot;], convert=True, chunk_size=50_000_000)

df.export(f&quot;{filename}.parquet&quot;)
</code></pre>
<h4>Step 2: Then we will can read it into a Pyarrow table to fit into the <code>datasets.Dataset</code> object and use the <code>munge_dataset_to_pacify_bert()</code> as shown above, e.g</h4>
<pre><code>from datasets import Dataset, load_from_disk
import pyarrow as pa

_ds = Dataset(pa.compute.drop_null(pa.parquet.read_table('train.en-de.tsv.parquet')
_ds.save_to_disk('train.en-de.tsv.parquet.hfdataset')

_ds = load_from_disk('train.en-de.tsv.parquet.hfdataset')

train_data = munge_dataset_to_pacify_bert(_ds)

train_data.save_to_disk('train.en-de.tsv.parquet.hfdataset')

</code></pre>
<h4>While the process above works well for small-ish dataset, e.g. 1-5 million lines of data, when the scale of the goes to 500 million to 1 billion, the last <code>.save_to_disk()</code> function seems like it is runningf &quot;forever&quot; and the end is no where in sight.</h4>
<p>Breaking down the steps in the <code>munge_dataset_to_pacify_bert()</code>, there are 2 sub-functions:</p>
<ul>
<li><code>dataset.map(_process_data_to_model_inputs, batched=True, batch_size=batch_size)</code></li>
<li><code>dataset.set_format(type=&quot;torch&quot;, columns=bert_wants_to_see)</code></li>
</ul>
<p>For the <code>.map()</code> process, it's possible to scale in parallel threads by specifying by</p>
<pre><code>dataset.map(_process_data_to_model_inputs, 
    batched=True, batch_size=100, 
    num_proc=32  # num of parallel threads.
    )
</code></pre>
<p>And when I tried to process with</p>
<ul>
<li><code>num_proc=32</code></li>
<li><code>batch_size=100</code></li>
</ul>
<p>The <code>.map()</code> function finishes the processing of 500 million lines in 18 hours of compute time on Intel Xeon E5-2686 @ 2.3GHz with 32 processor cores, optimally.</p>
<p>But somehow the <code>.map()</code> function created 32 temp <code>.arrow</code> files and 128 <code>tmp...</code> binary files. Seemingly the last <code>save_to_disk</code> function has been running for more than 10+ hours and have not finished combining the temp files in parts to save the final HF Dataset to disk.</p>
<hr />
<p>Given the above context, my questions in parts are:</p>
<h2>Question (Part 1): When the mapping function ends and created the temp <code>.arrow</code> and <code>tmp...</code> files, is there a way to read these individually instead of try to save them into a final directory using the <code>save_to_disk()</code> function?</h2>
<hr />
<h2>Question (Part 2): Why is the <code>save_to_disk()</code> function so slow after the mapping and how can the mapped processed data be saved in a faster manner?</h2>
<hr />
<h2>Question (Part 3): Is there a way to avoid the <code>.set_format()</code> function after the <code>.map()</code> and make it part of the <code>_process_data_to_model_inputs</code> function?</h2>
<hr />
"
"74052776","Gender Detection for Nouns in Spanish","2022-10-13 08:36:31","1","218","0","1","","74055927","<p>I am implementing a search engine in Spanish. In order to ensure gender neutrality, I need to get the gender of nouns in Spanish - e.g. &quot;pintora&quot; (painter, female) and &quot;pintor&quot; (painter, male). I am currently using <a href=""https://huggingface.co/flair/ner-spanish-large"" rel=""nofollow noreferrer"">FAIR library</a> - that it is really great for NER in Spanish. However, I cannot find any good implementation/library for gender detection in Spanish nouns. Could you help me?</p>
<p>Thank you in advance for your help</p>
"
"73976285","Reverse from POS tagging to sentence using pandas","2022-10-06 15:20:32","0","49","1","1","","73976396","<p>I have pos_token dataset and I want to transform them to be a sentence again using pandas</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>pos_token</th>
<th>sentence</th>
</tr>
</thead>
<tbody>
<tr>
<td>[(No, DT), (you, PRP), (lying, VBG)]</td>
<td>No you lying</td>
</tr>
</tbody>
</table>
</div>"
"73869397","Retrieve a list of model-specific POS tags using spaCy","2022-09-27 14:34:21","1","683","0","1","","73876748","<p>I am looking for a way to get a list of <strong>all possibly usable POS tags for a specific language</strong> model in spaCy.</p>
<p>In <a href=""https://stackoverflow.com/a/60303305/11583484"">an answer to another question, spaCy's <code>TAG_MAP</code> has been referenced to</a>, but I am not sure how to access this. The documentation of spaCy says that <a href=""https://spacy.io/usage/v3#incompat"" rel=""nofollow noreferrer"">this attribute has been replaced</a>. Since spaCy only uses a specific subset of all POS tags for a specific language, I would like to retrieve a list of all POS tags that are currently used with the initialized language model.</p>
<p>I did currently just set up a model this way:</p>
<pre class=""lang-py prettyprint-override""><code>import spacy

tagger = spacy.load(&quot;de_dep_news_trf&quot;)

# TODO print(pos_tags)
</code></pre>
<p>Now, how do I print a list of all possible pos tags for this model?</p>
"
"73835778","Bleu_score in NLTK library","2022-09-24 08:28:14","0","157","0","1","","73835949","<p>I am new to using the nltk library. I want to find the two most similar strings. In doing so, I used the 'bleu_score' as follows:</p>
<pre><code>import nltk
from nltk.translate import bleu
from nltk.translate.bleu_score import SmoothingFunction
smoothie = SmoothingFunction().method4```


C1 = 'FISSEN Ltds'
C2 = 'FISSEN Ltds Maschinen- und Werkzeugbau'
C3 = 'V.R.P. Baumaschinen Ltds'
print('BLEUscore1:',bleu([C1], C2, smoothing_function=smoothie, auto_reweigh=False))
print('BLEUscore2:',bleu([C2], C3, smoothing_function=smoothie, auto_reweigh=False))
print('BLEUscore3:',bleu([C1], C3, smoothing_function=smoothie, auto_reweigh=False))
</code></pre>
<p>The output is like this:</p>
<pre><code>BLEUscore1: 0.2585784506653774
BLEUscore2: 0.26042143846335913
BLEUscore3: 0.1472821272412462
</code></pre>
<p>I wonder why the results show the best similarity between C2 and C3 while C1 and C2 are the best answers. And what is the best way to assess this similarity between two strings whose answer is C1 and C2?</p>
<p>I appreciate any help you can provide :)</p>
"
"73807176","Spacy train dev and test data","2022-09-21 21:36:09","1","951","0","1","","73851179","<p>Does spaCy use dev-data to tune hyper-parameters? Or dev-data is totally out of the training process, and so equivalent to test data?
Following the standard greatly explained <a href=""https://www.statology.org/validation-set-vs-test-set/"" rel=""nofollow noreferrer"">here</a>, validation data and test data are different. Please is someone can clarify which is the case for spaCy under <a href=""https://www.statology.org/validation-set-vs-test-set/"" rel=""nofollow noreferrer"">the referred standard</a>. Thanks a lot.</p>
"
"73436826","Training model on TPU VM aborts with core dump","2022-08-21 17:39:20","0","308","0","1","","73447772","<p>I am trying to train T5X on the Winograd schema challenge. When I run my training script, I receive the following error.</p>
<pre><code>2022-08-21 17:27:01.141608: F ./tensorflow/core/tpu/tpu_library_init_fns.inc:101] TpuEmbeddingEngine_ConfigureCommunication not available in this library.
Aborted (core dumped) 
</code></pre>
<p>Any idea of what is going on?</p>
"
"73290224","python - TypeError: __init__() got an unexpected keyword argument 'checkpoint_callback'","2022-08-09 10:22:42","2","11524","0","2","","73290470","<p>I'm getting this error message:</p>
<pre><code>TypeError                                 Traceback (most recent call last)
&lt;ipython-input-41-2892cdd4e738&gt; in &lt;module&gt;()
      5   max_epochs=N_EPOCHS,
      6   gpus=1, #GPU
----&gt; 7   progress_bar_refresh_rate=30
      8 )

/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/argparse.py in insert_env_defaults(self, *args, **kwargs)
    343 
    344         # all args were already moved to kwargs
--&gt; 345         return fn(self, **kwargs)
    346 
    347     return cast(_T, insert_env_defaults)

TypeError: __init__() got an unexpected keyword argument 'checkpoint_callback'
</code></pre>
<p>... when I run this chunk:</p>
<pre><code>trainer = pl.Trainer(
  logger=logger, 
  checkpoint_callback=checkpoint_callback,
  callbacks=[early_stopping_callback],
  max_epochs=N_EPOCHS,
  gpus=1, #GPU
  progress_bar_refresh_rate=30
)
</code></pre>
<p>The 'checkpoint_callback' is defined like this:</p>
<pre><code>checkpoint_callback = ModelCheckpoint(
  dirpath=&quot;checkpoints&quot;,
  filename=&quot;best-checkpoint&quot;,
  save_top_k=1,
  verbose=True,
  monitor=&quot;val_loss&quot;,
  mode=&quot;min&quot;
)
</code></pre>
<p>I can't figure out what's causing the error - can anyone help me?</p>
<p>View full source code here: <a href=""https://colab.research.google.com/drive/1hT7PDVb0oGSpLejMGFBMWzRKTPwsSwwS?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1hT7PDVb0oGSpLejMGFBMWzRKTPwsSwwS?usp=sharing</a></p>
"
"73232595","Huggingface Trainer load_best_model f1 score vs. loss and overfitting","2022-08-04 08:20:30","2","4307","1","1","","73234634","<p>I have trained a roberta-large and specified <code>load_best_model_at_end=True</code> and <code>metric_for_best_model=f1</code>. During training, I can see overfitting after the 6th epoch, which is the sweetspot. In Epoch 8, which is the next one to evaluate due to gradient accumulation, we can see that train loss decreases and eval_loss increases. Thus, overfitting starts. The transformers trainer in the end loads the model from epoch 8, checkpoint <code>-14928</code>, as the f1 score is a bit highea. I was wondering, in theory, wouldn't be the model from epoch 6 be better suited, as it did not overfit? Or does one really go for the f1 metric here even though the model did overfit? (the eval loss decreased in epochs &lt;6 constantly).</p>
<p>The test_loss from the second checkpoint, which is then loaded as the &quot;best&quot;, is 0.128. Is it possible to lower that using the first checkpoint which should be the better model anyway?</p>
<pre><code>checkpoint-11196:
{'loss': 0.0638, 'learning_rate': 8.666799323450404e-06, 'epoch': 6.0}

{'eval_loss': 0.09599845856428146, 'eval_accuracy': 0.9749235986101227, 'eval_precision': 0.9648319293367138, 'eval_recall': 0.9858766505097777, 'eval_f1': 0.9752407721241682, 'eval_runtime': 282.2294, 'eval_samples_per_second': 84.637, 'eval_steps_per_second': 2.647, 'epoch': 6.0}

VS.

checkpoint-14928:
{'loss': 0.0312, 'learning_rate': 7.4291115311909265e-06, 'epoch': 8.0}

{'eval_loss': 0.12377820163965225, 'eval_accuracy': 0.976305103194206, 'eval_precision': 0.9719324391455539, 'eval_recall': 0.9810295838208257, 'eval_f1': 0.9764598236566295, 'eval_runtime': 276.7619, 'eval_samples_per_second': 86.309, 'eval_steps_per_second': 2.699, 'epoch': 8.0}
</code></pre>
"
"73228267","Trouble with applying a UDF on a column in Pyspark Dataframe","2022-08-03 22:02:03","0","846","0","1","","73229182","<p>My goal is to clean the Data in a column in a Pyspark DF. I have written a function for cleaning .</p>
<pre><code>def preprocess(text):
    text = text.lower() 
    text=text.strip()  
    text=re.compile('&lt;.*?&gt;').sub('', text) 
    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)  
    text = re.sub('\s+', ' ', text)  
    text = re.sub(r'\[[0-9]*\]',' ',text) 
    text=re.sub(r'[^\w\s]', '', text.lower().strip())
    text = re.sub(r'\d',' ',text) 
    text = re.sub(r'\s+',' ',text) 
    return text

 

#LEMMATIZATION
# Initialize the lemmatizer
wl = WordNetLemmatizer()

stop_words = set(stopwords.words('english'))
def remove_stopwords(text):
    text = [i for i in text.split() if not i in stop_words]
    return text
 
# This is a helper function to map NTLK position tags
def get_wordnet_pos(tag):
    if tag.startswith('J'):
        return wordnet.ADJ
    elif tag.startswith('V'):
        return wordnet.VERB
    elif tag.startswith('N'):
        return wordnet.NOUN
    elif tag.startswith('R'):
        return wordnet.ADV
    else:
        return wordnet.NOUN
# Tokenize the sentence
def lemmatizer(string):
    word_pos_tags = nltk.pos_tag(word_tokenize(string)) # Get position tags
    a=[wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)] # Map the position tag and lemmatize the word/token
    return &quot; &quot;.join(a)

#Final Function
def finalpreprocess(string):
    return lemmatizer(' '.join(remove_stopwords(preprocess(string))))
</code></pre>
<p>The functions seems to work fine when I test it . When I do</p>
<pre><code>text = 'Ram and Bheem are buddies. They (both) like &lt;b&gt;running&lt;/b&gt;. They got better at it over the weekend'

print(finalpreprocess(text))
</code></pre>
<p>I see the exact result I want.</p>
<pre><code>ram bheem buddy like run get well weekend
</code></pre>
<p>How ever when I try to apply this function finalpreprocess() to a column in pyspark dataframe . I am getting errors.
Here is what I did.</p>
<p>udf_txt_clean = udf(lambda x: finalpreprocess(x),StringType())
df.withColumn(&quot;cleaned_text&quot;,lem(col(&quot;reason&quot;))).select(&quot;reason&quot;,&quot;cleaned_text&quot;).show(10,False)</p>
<p>Then I am getting the error :</p>
<pre><code>Traceback (most recent call last):
  File &quot;/databricks/spark/python/pyspark/serializers.py&quot;, line 473, in dumps
    return cloudpickle.dumps(obj, pickle_protocol)
  File &quot;/databricks/spark/python/pyspark/cloudpickle/cloudpickle_fast.py&quot;, line 73, in dumps
    cp.dump(obj)
  File &quot;/databricks/spark/python/pyspark/cloudpickle/cloudpickle_fast.py&quot;, line 563, in dump
    return Pickler.dump(self, obj)
TypeError: cannot pickle '_thread.RLock' object
PicklingError: Could not serialize object: TypeError: cannot pickle '_thread.RLock' object
</code></pre>
<p>So far here is what I did. In my finalpreprocess(), I am using three different functions preprocess(),remove_stopwords(), lemmatizer() . I changed my udf_txt_clean accordingly . Like</p>
<pre><code>udf_txt_clean = udf(lambda x: preprocess(x),StringType())
udf_txt_clean = udf(lambda x: remove_stopwords(x),StringType())
</code></pre>
<p>These two run fine But -</p>
<p>udf_txt_clean = udf(lambda x: lemmatizer (x),StringType())</p>
<p>is the one that is giving me the error. I am not able to understand why this function is giving the error but not the other two. From my limited understating I see that its having trouble trying to pickle this function but I am not able to understand why its trying to pickle this in the first place or if there is a work around for it.</p>
"
"73059189","Why does comparing two images take longer when running the procedure in parallel using python's Pool module?","2022-07-20 23:38:26","1","309","11","1","","73397004","<p>I'm developing a program that involves computing similarity scores for around 480 pairs of images (20 directories with around 24 images in each). I'm utilizing the <code>sentence_transformers</code> Python module for image comparison, and it takes around 0.1 - 0.2 seconds on my Windows 11 machine to compare two images when running in serial, but for some reason, that time gets increased to between 1.5 and 3.0 seconds when running in parallel using a process <code>Pool</code>. So, either a), there's something going on behind the scenes that I'm not yet aware of, or b) I just did it wrong.</p>
<p>Here's a rough structure of the image comparison function:</p>
<pre><code>def compare_images(image_one, image_two, clip_model):
    start = time()
    images = [image_one, image_two]
    # clip_model is set to SentenceTransformer('clip-ViT-B-32') elsewhere in the code
    encoded_images = clip_model.encode(images, batch_size = 2, convert_to_tensor = True, show_progress_bar = False)
    processed_images = util.paraphrase_mining_embeddings(encoded_images)
    stop = time()
    print(&quot;Comparison time: %f&quot; % (stop - start) )
    score, image_id1, image_id2 = processed_images[0]
    return score
</code></pre>
<p>Here's a rough structure of the serial version of the code to compare every image:</p>
<pre><code>def compare_all_images(candidate_image, directory, clip_model):
    for dir_entry in os.scandir(directory):
        dir_image_path = dir_entry.path
        dir_image = Image.open(dir_image_path)
        similiarity_score = compare_images(candidate_image, dir_image, clip_model)

        # ... code to determine whether this is the maximum score the program has seen...
</code></pre>
<p>Here is a rough structure of the parallel version:</p>
<pre><code>def compare_all_images(candidate_image, directory, clip_model):
    pool_results = dict()
    pool = Pool()

    for dir_entry in os.scandir(directory):
        dir_image_path = dir_entry.path
        dir_image = Image.open(dir_image_path)
        pool_results[dir_image_path] = pool.apply_async(compare_images, args = (candidate_image, dir_image, clip_model)

    # Added everything to the pool, close it and wait for everything to finish
    pool.close()
    pool.join()

    # ... remaining code to determine which image has the highest similarity rating
</code></pre>
<p>I'm not sure where I might be erring.</p>
<p>The interesting thing here is that I also developed a smaller program to verify whether I was doing things correctly:</p>
<pre><code>def func():
    sleep(6)

def main():
    pool = Pool()
    for i in range(20):
        pool.apply_async(func)
    pool.close()

    start = time()
    pool.join()
    stop = time()
    print(&quot;Time: %f&quot; % (stop - start) ) # This gave an average of 12 seconds 
                                        # across multiple runs on my Windows 11 
                                        # machine, on which multiprocessing.cpu_count=12
</code></pre>
<p>Is this a problem with trying to make things parallel with sentence transformers, or does the problem lie elsewhere?</p>
<p><strong>UPDATE:</strong> Now I'm especially confused. I'm now only passing <code>str</code> objects to the comparison function and have temporarily slapped a <code>return 0</code> as the very first line in the function to see if I can further isolate the issue. Oddly, even though the parallel function is doing absolutely nothing now, several seconds (usually around 5) still seem to pass between the time that the pool is closed and the time that <code>pool.join()</code> finishes. Any thoughts?</p>
<p><strong>UPDATE 2:</strong> I've done some more playing around, and have found out that <em>an empty pool still has some overhead</em>. This is the code I'm testing out currently:</p>
<pre><code>            # ...
            pool = Pool()

            pool.close()
            start = time()
            DebuggingUtilities.debug(&quot;empty pool closed, doing a join on the empty pool to see if directory traversal is messing things up&quot;)
            pool.join()
            stop = time()

            DebuggingUtilities.debug(&quot;Empty pool join time: %f&quot; % (stop - start) )
</code></pre>
<p>This gives me an &quot;Empty pool join time&quot; of about 5 seconds. Moving this snippet to the very first part of my main function still yields the same. Perhaps <code>Pool</code> works differently on Windows? In WSL (Ubuntu 20.04), the same code runs in about 0.02 seconds. So, what would cause even an empty <code>Pool</code> to hang for such a long time on Windows?</p>
<p><strong>UPDATE 3:</strong> I've made another discovery. The empty pool problem goes away if the only imports I have are <code>from multiprocessing import Pool</code> and <code>from time import time</code>. However, the program uses a boatload of import statements across several source files, which causes the program to hang a bit when it first starts. I suspect that this is propagating down into the <code>Pool</code> for some reason. Unfortunately, I need all of the <code>import</code> statements that are in the source files, so I'm not sure how to get around this (or why the imports would affect an empty Pool).</p>
<p><strong>UPDATE 4:</strong> So, apparently it's the <code>from sentence_transformers import SentenceTransformer</code> line that's causing issues (without that import, the <code>pool.join()</code> call happens relatively quickly. I think the easiest solution now is to simply move the <code>compare_images</code> function into a separate file. I'll update this question again with updates as I implement this.</p>
<p><strong>UPDATE 5:</strong> I've done a little more playing around, and it seems like on Windows, the import statements get executed multiple times whenever a <code>Pool</code> gets created, which I think is just weird. Here's the code I used to verify this:</p>
<pre><code>from multiprocessing import Pool
from datetime import datetime
from time import time
from utils import test

print(&quot;outside function lol&quot;)

def get_time():

    now = datetime.now()

    return &quot;%02d/%02d/%04d - %02d:%02d:%02d&quot; % (now.month, now.day, now.year, now.hour, now.minute, now.second)


def main():
    pool = Pool()

    print(&quot;Starting pool&quot;)

    &quot;&quot;&quot;
    for i in range(4):
        print(&quot;applying %d to pool %s&quot; % (i, get_time() ) )
        pool.apply_async(test, args = (i, ) )
    &quot;&quot;&quot;

    pool.close()
    print(&quot;Pool closed, waiting for all processes to finish&quot;)
    start = time()
    pool.join()

    stop = time()

    print(&quot;pool done: %f&quot; % (stop - start) )

if __name__ == &quot;__main__&quot;:

    main()
</code></pre>
<p>Running through Windows command prompt:</p>
<pre><code>outside function lol
Starting pool
Pool closed, waiting for all processes to finish
outside function lol
outside function lol
outside function lol
outside function lol
outside function lol
outside function lol
outside function lol
outside function lol
outside function lol
outside function lol
outside function lol
outside function lol
pool done: 4.794051
</code></pre>
<p>Running through WSL:</p>
<pre><code>outside function lol
Starting pool
Pool closed, waiting for all processes to finish
pool done: 0.048856
</code></pre>
<p><strong>UPDATE 6:</strong> I think I might have a workaround, which is to create the <code>Pool</code> in a file that doesn't directly or indirectly import anything from <code>sentence_transformers</code>. I then pass the model and anything else I need from <code>sentence_transformers</code> as parameters to a function that handles the <code>Pool</code> and kicks off all of the parallel processes. Since the <code>sentence_transformers</code> import seems to be the only problematic one, I'll wrap that import statement in an <code>if __name__ == &quot;__main__&quot;</code> so it only runs once, which will be fine, as I'm passing the things I need from it as parameters. It's a rather janky solution, and probably not what others would consider as &quot;Pythonic&quot;, but I have a feeling this will work.</p>
<p><strong>UPDATE 7:</strong> The workaround was successful. I've managed to get the pool join time on an empty pool down to something reasonable (0.2 - 0.4 seconds). The downside of this approach is that there is definitely considerable overhead in passing the entire model as a parameter to the parallel function, which I needed to do as a result of creating the <code>Pool</code> in a different place than the model was being imported. I'm quite close, though.</p>
"
"73017872","i get an NameError although i defined my variable","2022-07-18 06:00:08","-2","55","3","1","","73022878","<p>hello my programmer friends... i'm doing my first NLP project that counts and shows 5 documents TFIDF. here's part of the code:</p>
<pre><code>def IDF(corpus , unique_words):
    idf_dict = {}
    N = len(corpus)
    for i in unique_words:
        count = 0
        for sen in corpus:
            if i in sen.split():
                count = count+1
            idf_dict[i] = (math.log((1 + N) / (count+1))) + 1
    return idf_dict

def fit(whole_data):
    unique_words = set()
    if isinstance(whole_data, (list,)):
        for x in whole_data:
            for y in x.split():
                if len(y)&lt;2:
                    continue
                unique_words.add(y)
            unique_words = sorted(list(unique_words))
            vocab = {j:i for i,j in enumerate(unique_words)}
    Idf_values_of_all_unique_words = IDF(whole_data,unique_words)
    return vocab, Idf_values_of_all_unique_words
vocabulary, idf_of_vocabulary = fit(corpus)
</code></pre>
<p>The word <strong>IDF</strong> in line 22 gives me a NameError.
is it about positioning?</p>
"
"73015102","Error in fit_transform while finding tf-idf in Python","2022-07-17 20:22:09","0","174","2","1","","73017360","<pre><code>import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
mylist = [
    'a a b c',
    'a c c c d e f',
    'a c d d d',
    'a d f',
]
df = pd.DataFrame({&quot;texts&quot;: mylist})
tfidf_vectorizer = TfidfVectorizer(ngram_range=[1, 1])
tfidf_separate = tfidf_vectorizer.fit_transform(df[&quot;texts&quot;])
</code></pre>
<p>I am trying to find tf-idf value for “d” in line 3. But, it is showing me empty vocabulary error &quot;ValueError: empty vocabulary; perhaps the documents only contain stop words&quot;.</p>
<p>Any advice on how to resolve the error would be appreciated!</p>
"
"73011617","How to perform stemming and put back the words in the orginal review format?","2022-07-17 12:05:46","2","222","0","1","","73011796","<p>I have a dataset with one column being <code>full_text</code> that contains review text from an online website. I wanted to clean these reviews, by removing stop words and stemming and putting them back to their original format (having all stemmed words forming a sentence, i.e.: one row per review instead of having 1 stemmed word per row.)</p>
<p>I am attempting the following:</p>
<pre><code>sw &lt;- stop_words %&gt;% filter(lexicon == &quot;SMART&quot;)

for (j in 1:nrow(reviews_df)) {

  nostopwords &lt;- reviews_df[j,] %&gt;% unnest_tokens(word, full_text) %&gt;%
                  anti_join(sw, by = &quot;word&quot;)
  stemmed &lt;- wordStem(nostopwords[ , &quot;word&quot;], language = &quot;porter&quot;)
  
reviews_df[j, &quot;stemmed_Description&quot;] &lt;- paste(stemmed, collapse = &quot; &quot;)

}
</code></pre>
<p>However, this new column <code>stemmed_Description</code> does not look how I wanted. It didn't perform stemming and also it is not in &quot;sentence&quot; style but rather as a vector of strings <code>c(&quot;word1&quot;, &quot;word2&quot;, &quot;word3&quot;)</code>.</p>
<p>How can I achieve a result of the style: &quot;stemmedword1 stemmedword2 stemmedword3&quot; ?</p>
<p>Current output:</p>
<pre><code>full_text
1 pseudoindependence no one looking over your shoulder and youre free to use your own judgement to problem solve. they sometimes expect more than what a person can give. dont overwork yourself. the packages aint going no where!
stemmed_Description
1 c(&quot;pseudoindependence&quot;, &quot;shoulder&quot;, &quot;youre&quot;, &quot;free&quot;, &quot;judgement&quot;, &quot;problem&quot;, &quot;solve&quot;, &quot;expect&quot;, &quot;person&quot;, &quot;give&quot;, &quot;dont&quot;, &quot;overwork&quot;, &quot;packages&quot;, &quot;ain't&quot;)
</code></pre>
"
"72933472","ModuleNotFoundError in spacy version 3.3.1 tried previous mentioned solution not working","2022-07-11 03:41:29","0","282","2","1","","72939110","<pre><code>  import spacy

     from spacy.lemmatizer import Lemmatizer

     from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES

     lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)

     lemmatizer('chuckles', 'NOUN')
</code></pre>
<p>the output should be chuckle.
using version 3.1.1</p>
"
"72920750","Error getting prediction explanation using shap_values when using scikit-learn pipeline?","2022-07-09 10:52:04","-1","1368","2","1","","72929720","<p>I am building an NLP model to predict language type (C/C++/C#/Python...) for a given code.
Now I need to provide an explanation for my model prediction. For example the following user_input is written in Java and the model is predicting that, but I need to show the users why it predicts so.</p>
<p>I am using shap_values to achieve this.
For some reason, the following code results in an error (I have added the error at the bottom).
Please advise how can I get shap_values and plots for my model predictions.</p>
<p>Link to data: <a href=""https://sharetext.me/bd68ryvzi0"" rel=""nofollow noreferrer"">https://sharetext.me/bd68ryvzi0</a></p>
<p>Code:</p>
<pre><code>import pandas as pd

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import FunctionTransformer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import Pipeline

# Loading Data:
DATA_PATH = r&quot;sample.csv&quot;

data = pd.read_csv(DATA_PATH, dtype='object')
data = data.convert_dtypes()
data = data.dropna()
data = data.drop_duplicates()

# Train/Test split
X, y = data.content, data.language
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)

# Model params to match:
# 1. Variable and module names, words in a string, keywords: [A-Za-z_]\w*\b
# 2. Operators: [!\#\$%\&amp;\*\+:\-\./&lt;=&gt;\?@\\\^_\|\~]+
# 3. Tabs, spaces and Brackets: [ \t\(\),;\{\}\[\]`&quot;']
# with the following regex:
token_pattern = r&quot;&quot;&quot;(\b[A-Za-z_]\w*\b|[!\#\$%\&amp;\*\+:\-\./&lt;=&gt;\?@\\\^_\|\~]+|[ \t\(\),;\{\}\[\]`&quot;'])&quot;&quot;&quot;


def preprocess(x):
 &quot;&quot;&quot; Clean up single-character variable names or ones constituted of a sequence of the same character &quot;&quot;&quot;
 return pd.Series(x).replace(r'\b([A-Za-z])\1+\b', '', regex=True)\
 .replace(r'\b[A-Za-z]\b', '', regex=True)


# Pipe steps:
# Define a transformer:
transformer = FunctionTransformer(preprocess)
# Perform TF-IDF vectorization with our token pattern:
vectorizer = TfidfVectorizer(token_pattern=token_pattern, max_features=3000)
# Create Random Forest Classifier:
clf = RandomForestClassifier(n_jobs=4)

pipe_RF = Pipeline([
 ('preprocessing', transformer),
 ('vectorizer', vectorizer),
 ('clf', clf)]
)

# Setting best params (after performing GridSearchCV)
best_params = {
 'clf__criterion': 'gini',
 'clf__max_features': 'sqrt',
 'clf__min_samples_split': 3,
 'clf__n_estimators': 300
}

pipe_RF.set_params(**best_params)

# Fitting
pipe_RF.fit(X_train, y_train)

# Evaluation
print(f'Accuracy: {pipe_RF.score(X_test, y_test)}')



user_input = [&quot;&quot;&quot; public class Fibonacci {

public static void main(String[] args) {

int n = 10;

System.out.println(fib(n));

}

public static int fib(int n) {

if (n &lt;= 1) {

return n;

}

return fib(n - 1) + fib(n - 2);

}

} &quot;&quot;&quot;]


import shap

shap.initjs()
explainer = shap.TreeExplainer(pipe_RF.named_steps['clf'])
observation = pipe_RF[:-1].transform(user_input).toarray()
shap_values = explainer.shap_values(observation)
</code></pre>
<p>Load the data and run it to get the following error:</p>
<blockquote>
<p>ExplainerError: Additivity check failed in TreeExplainer! Please
ensure the data matrix you passed to the explainer is the same shape
that the model was trained on. If your data shape is correct then
please report this on GitHub. Consider retrying with the
feature_perturbation='interventional' option. This check failed
because for one of the samples the sum of the SHAP values was
46609069202029743624438153216.000000, while the model output was 0.004444. If this difference is acceptable you can set check_additivity=False to disable this check.</p>
</blockquote>
"
"72837120","Keyword importance over time with TF-IDF","2022-07-02 07:03:47","0","402","0","1","","72845148","<p>I'm doing research on keyword importance in annual reports of cloud providers. I already extracted the text from the PDFs and I'm mostly looking for the importance of the word <code>&quot;cloud&quot;</code> in there.</p>
<p>So I decided to use TF-IDF algorithm to define the importance of the keyword across multiple documents. However, I'm not a data scientist, I'm software engineer. I do not know if my solution makes sense. Here is what I have:</p>
<pre class=""lang-py prettyprint-override""><code>import glob
from pathlib import Path
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np

x=[2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021]

def get_line_and_trend(directory_path):
    # Extracts text from files
    text_files = glob.glob(f&quot;{directory_path}/**/*.txt&quot;, recursive=True)
    text_titles = [Path(text).stem for text in text_files]
    
    # TF-IDF
    tfidf_vectorizer = TfidfVectorizer(input='filename', stop_words=&quot;english&quot;)
    tfidf_vector = tfidf_vectorizer.fit_transform(text_files)
    
    # Search for word &quot;cloud&quot;
    df = pd.DataFrame(
        tfidf_vector.toarray(),
        index=text_titles,
        columns=tfidf_vectorizer.get_feature_names()
    ).sort_index()
    tfidf_slice = df[['cloud']]
    tfidf_slice.round(decimals=2)
    
    # Draw the trend line
    z = np.polyfit(x, tfidf_slice[&quot;cloud&quot;], 1)
    p = np.poly1d(z)
    return tfidf_slice[&quot;cloud&quot;], p(x)
</code></pre>
<p>I'm passing a folder as parameter which contains the annual reports (.txt) in the different folders (2014-2021). With that I can plot something like:</p>
<p><a href=""https://i.sstatic.net/i7AWC.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/i7AWC.png"" alt=""TF-IDF google cloud and alibaba cloud"" /></a></p>
<p>with the following code:</p>
<pre class=""lang-py prettyprint-override""><code>alibaba_cloud, trendline = get_line_and_trend(&quot;./alibaba&quot;)
plt.plot(x, alibaba_cloud, color='b')
plt.plot(x, trendline, &quot;b--&quot;)

google_cloud, trendline = get_line_and_trend(&quot;./google&quot;)
plt.plot(x, google_cloud, color='r')
plt.plot(x, trendline, &quot;r--&quot;)

plt.legend([&quot;Alibaba&quot;, &quot;Trend line&quot;, &quot;Google&quot;, &quot;Trend line&quot;])
plt.title(&quot;TF-IDF for \&quot;cloud\&quot; in annual reports&quot;)
plt.show()
</code></pre>
<p>So my questions are:</p>
<ul>
<li>Does it make sense to use TF-IDF to track the importance of keyword over time? Should I use something else?</li>
<li>Does the chart really represents what I'm trying to do?</li>
</ul>
"
"72804704","Reduce fastText memory usage for big models","2022-06-29 16:14:26","1","1715","9","1","","73458984","<p>I trained a machine learning sentence classification model that uses, among other features, also the vectors obtained from a pretrained fastText model (like <a href=""https://fasttext.cc/docs/en/crawl-vectors.html"" rel=""nofollow noreferrer"">these</a>) which is 7Gb.  I use the pretrained fastText Italian model: I am using this word embedding only to get some semantic features to feed into the effective ML model.</p>
<p>I built a simple API based on fastText that, at prediction time, computes the vectors needed by the effective ML model. Under the hood, this API receives a string as input and calls <code>get_sentence_vector</code>. When the API starts, it loads the fastText model into memory.</p>
<p><strong>How can I reduce the memory footprint of fastText, which is loaded into RAM?</strong></p>
<p>Constraints:</p>
<ul>
<li>My model works fine, training was time-consuming and expensive, so I wouldn't want to retrain it using smaller vectors</li>
<li>I need the fastText ability to handle out-of-vocabulary words, so I can't use just vectors but I need the full model</li>
<li>I should reduce the RAM usage, even at the expense of a reduction in speed.</li>
</ul>
<p>At the moment, I'm starting to experiment with <a href=""https://github.com/avidale/compress-fasttext"" rel=""nofollow noreferrer"">compress-fasttext</a>...</p>
<p><strong>Please share your suggestions and thoughts even if they do not represent full-fledged solutions.</strong></p>
"
"72782449","nltk stopwords - AttributeError: 'function' object has no attribute 'words'","2022-06-28 07:38:41","1","1257","0","1","","72782983","<p>This is my import:</p>
<pre><code>from nltk.corpus import stopwords
</code></pre>
<p>And this is my code:</p>
<pre><code>def stopwords(text):
&quot;&quot;&quot;a function for removing the stopword&quot;&quot;&quot;
sw = stopwords.words('english')
# removing the stop words and lowercasing the selected words
text = [word.lower() for word in text.split() if word.lower() not in sw]
# joining the list of words with space separator
return &quot; &quot;.join(text)
</code></pre>
<p>Applying:</p>
<pre><code>df['col_text'] = df['col_text'].apply(stopwords)
</code></pre>
<p>I got this error:
AttributeError: 'function' object has no attribute 'words'</p>
<p>Someone can help me with this problem please?</p>
"
"72701918","Is `sklearn.Pipeline` with regex really more performant than `spacy` for preprocessing huge volumes of text?","2022-06-21 13:43:44","1","268","0","1","","72709520","<h1>TL;DR</h1>
<p>I need help selecting between <code>spacy</code> and <code>sklearn</code> for processing a huge text corpus. I ran a test to measure the performance of each, but the results were unexpected. Moreover, because I'm new-ish to the frameworks involved, I lack confidence that my test is completely valid. I'd really appreciate some guidance.</p>
<ul>
<li><p><a href=""https://pastebin.com/dBAW1rrD"" rel=""nofollow noreferrer"">Code</a></p>
</li>
<li><p><a href=""https://i.sstatic.net/KiSbg.jpg"" rel=""nofollow noreferrer"">Results</a></p>
</li>
</ul>
<h1>Background</h1>
<p>I'm doing a project that involves preprocessing 35 million Reddit comments. This is a pretty massive amount of text. So I'm searching for the most efficient framework to accomplish this with.</p>
<p>Currently, I am considering using either <code>spacy</code>’s <code>nlp.pipe</code> with several custom components, or a <code>sklearn.Pipeline</code> with a ton of regex-based data transformers. Since (1) <code>spacy</code> is optimized for text and (2) regex in Python is slow, I figured the <code>spacy</code> option is the way to go. But I wanted to test my assumptions before proceeding.</p>
<h1>The test</h1>
<p><a href=""https://pastebin.com/dBAW1rrD"" rel=""nofollow noreferrer"">So I wrote a quick and dirty script to do just that.</a> It seems like a lot of code at first skim, but it's actually not. It's very modular, mostly consisting of simple classes. Skip to <code>if __name__ ...</code> at the end to see the overall logic.</p>
<p>Anyway, this script defines what I <em>think</em> are broadly equivalent pipelines, one <code>spacy</code>-based and one <code>sklearn</code>-based, that simply remove (1) punctuation and (2) inline code <code>like this</code>. These pipelines subclass an additional class which actually carries out the test. So the script loads a ~7.5k-comment sample from <a href=""https://www.reddit.com/r/LanguageTechnology/"" rel=""nofollow noreferrer"">r/LanguageTechnology</a> as a <code>dask.dataframe</code> (for parallelization), applies the same preprocessing 100 times using each pipeline, then averages out the results.</p>
<p>To be clear, my actual pipeline will do several more things than just remove punctuation and inline code. I only chose those particular transformations for testing purposes, to keep my tests simple and to the point.</p>
<h1>Results</h1>
<p>My findings (in seconds) are as follows, illustrated graphically <a href=""https://i.sstatic.net/KiSbg.jpg"" rel=""nofollow noreferrer"">here</a>:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;"">pipeline</th>
<th style=""text-align: right;"">mean</th>
<th style=""text-align: right;"">standard dev</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;""><code>spacy</code></td>
<td style=""text-align: right;"">13.49772</td>
<td style=""text-align: right;"">1.182763</td>
</tr>
<tr>
<td style=""text-align: center;""><code>sklearn</code></td>
<td style=""text-align: right;"">6.853291</td>
<td style=""text-align: right;"">0.127701</td>
</tr>
</tbody>
</table>
</div>
<p>Clearly, <code>spacy</code> was massively slower. This contradicted my expectations, and leaves me unable to draw firm conclusions.</p>
<p>Is <code>sklearn.Pipeline</code> with regex truly the more efficient framework for this? Or was there an issue with my test, or how I structured my pipelines? The latter seems plausible because almost everything the script uses is new-ish to me - <code>dask.dataframe</code>, <code>spacy</code> with custom components, and <code>sklearn.Pipeline</code> with custom transformers. So it may very well be that e.g., I'm just using <code>spacy</code> wrong, or there's something about my script that renders the comparison apples to oranges instead of apples to apples.</p>
<h1>Cry for help</h1>
<p>In light of this uncertainty, I'd sincerely appreciate some input from anyone familiar with these frameworks. I'd also appreciate some eyes on my code, if possible, just to check that I've actually used everything properly.</p>
<p>Any and all input is welcome. Thank you!</p>
"
"72645822","List of dependencies in Spacy","2022-06-16 12:24:28","1","948","0","1","","72653864","<p>I'm a beginner in NLP and i've decided to start with Spacy. It's simple to handle and to comprehend.
Neverthless, i can't acess to the full documentation or parsing.
I mean , i don't know the meaning of &quot;IN&quot; , &quot;RB&quot; for example
And, displacy that is used to display the dependency parsing doesn't show up a real information about the dependencies.
Exemple : <a href=""https://i.sstatic.net/eiXsA.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>I understand the concept of dependency parsing, this example is in French.
What means the dependencies &quot;Fixed&quot; , &quot;cop&quot;, &quot;advmod&quot; and finally where can i get a full documentation about it.
Thank you</p>
"
"72572232","How to preserve column order after applying sklearn.compose.ColumnTransformer on numpy array","2022-06-10 09:45:32","0","874","0","2","","72572491","<p>I want to use <code>Pipeline</code> and <code>ColumnTransformer</code> modules from sklearn library to apply scaling on numpy array. Scaler is applied on some of the columns. And, I want to have the output with same column order of input.</p>
<p>Example:</p>
<pre><code>import numpy as np
from sklearn.compose import ColumnTransformer 
from sklearn.preprocessing import  MinMaxScaler


X = np.array ( [(25, 1, 2, 0),
                (30, 1, 5, 0),
                (25, 10, 2, 1),
                (25, 1, 2, 0),
                (np.nan, 10, 4, 1),
                (40, 1, 2, 1) ] )



column_trans = ColumnTransformer(
    [ ('scaler', MinMaxScaler(), [0,2]) ], 
     remainder='passthrough') 
      
X_scaled = column_trans.fit_transform(X)
</code></pre>
<p>The problem is that <code>ColumnTransformer</code> changes the order of columns. How can I preserve the original order of columns?</p>
<p>I am aware of this <a href=""https://stackoverflow.com/questions/68874492/preserve-column-order-after-applying-sklearn-compose-columntransformer"">post</a>. But, it is for pandas DataFrame. For some reasons, I cannot use DataFrame and I have to use numpy array in my code.</p>
<p>Thanks.</p>
"
"72394840","lemmatizing a verb list in a data frame in Python","2022-05-26 16:09:34","0","162","2","1","","72395079","<p>I want to ask a seemingly simple question to Python wizs (I am a total newbie so have no idea how simple/complex this question is)!</p>
<p>I have a verb list in a dataframe looking as below:</p>
<p><strong>id  verb</strong><br />
15  believe<br />
64  start<br />
90  believe</p>
<p>I want to lemmatize it. The problem is that most lemmatization comes with sentence strings. My data does not provide context to decide its part-of-speech because I only need 'verb' speech lemmas.</p>
<p>Would you have any ideas about how to go about lemmatizing this verb list?
Many thanks in advance for considering my question!</p>
"
"72349165","Grammar parser for parsing parliamentary debates?","2022-05-23 13:12:49","-1","60","1","1","","72370632","<p>I'm looking to parse the plain text from a transcription tool (the goal is to render it into LegalDocML).</p>
<p>My issue is that I do not know where to start and learning a grammar parser is quite a steep learning curve. I'm looking for guidance as to what kind of parser would be appropriate for the problem.</p>
<p>My gut feel is that the below is a candidate for LR grammar tools as there might be some clear delimiters? (all caps for speaker, brackets for speaker role, square brackets to speech time) but also some NLP needs - for grievances  the person the speech is addressed to is often loosely in the first sentence of the speech..</p>
<p>Any advice would be appreciated</p>
<p>as a sample:</p>
<pre><code>Legislative Assembly
Thursday, 19 May 2022
               
THE SPEAKER (Mrs M.H. Roberts) took the chair at 9.00 am, acknowledged country and read prayers.
PAPER TABLED
A paper was tabled and ordered to lie upon the table of the house.
SMALL BUSINESS ASSISTANCE GRANTS
Statement by Minister for Small Business
Statement
MR D.T. PUNCH (Bunbury — Minister for Small Business) [9.01 am]: I would like to bring to the attention of the house some recent changes made by the McGowan government to the small business assistance grants. As I have previously advised the house, in February the state government announced a $67 million level 1 COVID-19 business assistance package, and more recently a $72 million package for businesses impacted by level 2 public health and social measures, taking the total committed to COVID-19 business support to almost $1.7 billion over the past two years. The level 1 package includes $42 million in rent relief assistance and the level 2 package includes a $66.8 million small business hardship grants program.
Last month, a revision and expansion of the small business hardship grants program was announced.
.
.
.
HOME INDEMNITY INSURANCE
Grievance
MR R.S. LOVE (Moore — Deputy Leader of the Opposition) [9.06 am]: I grieve today to the Parliamentary Secretary to the Minister for Commerce on behalf of Western Australian residents who have had their
</code></pre>
"
"72284795","How to access a row in a pandas dataframe with custom index labels?","2022-05-18 07:24:12","0","2005","0","2","","72289074","<p>I've created a pandas dataframe to hold the data like so:</p>
<pre><code>TF_IDF = pd.DataFrame(index = vocab)
TF_IDF['0'] = 0
</code></pre>
<p>where <code>vocab</code> is an array of strings, so the index label of each column is the corresponding string</p>
<p>so when I print it, it looks like this:</p>
<pre><code>           0
life       0
math       0
student    0
experi     0
control    0
...       ..
slave      0
linga      0
31-32      0
democrat   0
unsustain  0
</code></pre>
<p>How would I access a row at a given index by using a string?</p>
<p>for example, if I have the string &quot;math&quot; how would one access the value using the index label &quot;math&quot;?</p>
"
"72249074","Matching patterns in spaCy returns a empty result","2022-05-15 14:16:49","2","211","0","1","","72253835","<p>I was hoping to find some patterns with this simple code. But the result is empty.
I'm forgetting something?</p>
<pre><code>for tk in doc[:30]:
     print (tk.text, ':', tk.pos_)
</code></pre>
<p>Método : NOUN
de : ADP
avaliaçãoSimulação : NOUN
computacional : ADJ
conforme : ADP
procedimentos : NOUN
apresentados : VERB
em : ADP
: SPACE
Ediﬁ : PROPN
cações : NOUN
em : ADP
fase : NOUN
de : ADP
projetoA : NOUN
avaliação : NOUN
deve : VERB
ser : AUX
feita : VERB
para : ADP
um : NUM
dia : NOUN
típico : ADJ
de : ADP
projeto : NOUN
de : ADP
verão : NOUN
e : CCONJ
de : ADP</p>
<pre><code>pattern = [
       {'POS': 'NOUN'},
       {'LOWER': 'ADP'},
       ]
</code></pre>
<pre><code>    #Matcher class object
matcher = Matcher(nlp.vocab)
matcher.add(&quot;matching_1&quot;, patterns = [pattern]) 

result = matcher(doc, as_spans=True) 

print(result)
</code></pre>
<p>[]</p>
<p>So I was expecting the pattern of the POS Tags 'NOUN' + 'ADP' could find the words:
'Método de',
'cações em',
'fase de',
'projeto de'.</p>
"
"72241814","I created a TF-IDF code to analyze an annual report, I want to know the importance of specific keywords","2022-05-14 15:49:36","0","173","0","1","","72242436","<pre><code>import pandas as pd
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
import path
import re



with open(r'C:\Users\maxim\PycharmProjects\THESIS\data\santander2020_1.txt', 'r') as file:
    data = file.read()

dataset = [data]


tfIdfVectorizer=TfidfVectorizer(use_idf=True, stop_words=&quot;english&quot;
                                , lowercase=True,max_features=100,ngram_range=(1,3))
tfIdf = tfIdfVectorizer.fit_transform(dataset)
df = pd.DataFrame(tfIdf[0].T.todense(), index=tfIdfVectorizer.get_feature_names(), columns=[&quot;TF-IDF&quot;])
df = df.sort_values('TF-IDF', ascending=False)




print (df.head(25))
</code></pre>
<p>The above code is what ive created to do a TF-IDF analysis on an annual report, however currently it is giving me the values of the most important words within the report. However, I only need the TFIDF values for the keywords
[&quot;digital&quot;,&quot;hardware&quot;,&quot;innovation&quot;,&quot;software&quot;,&quot;analytics&quot;,&quot;data&quot;,&quot;digitalisation&quot;,&quot;technology&quot;], is there a way I can specify to only look for the tfidf values of these terms?</p>
<p>I'm very new to programming with little experience, I'm doing this for my thesis.</p>
<p>Any help is greatly appreciated.</p>
"
"72193062","Sci-kit TF-IDF - Unsure of Interpretation of TD-IDF Array?","2022-05-10 21:18:44","0","74","0","1","","72193798","<p>I have a subset of a dataframe like:</p>
<pre><code>&lt;OUT&gt;
PageNumber    Top_words_only
56            people sun flower festival 
75            sunflower sun architecture red buses festival
</code></pre>
<p>I want to calculate TF-IDF on the <code>English_tags</code> df column with each row acting as a document. I have tried:</p>
<pre><code>Vectorizer = TfidfVectorizer(lowercase = True, max_df = 0.8, min_df = 5, stop_words = 'english')
Vectors = Vectorizer.fit_transform(df['top_words_only'])
</code></pre>
<p>If I print the array it comes out as:</p>
<pre><code>array([[0.        , 0.        , 0.        , ..., 0.        , 0.35588179,
        0.        ],
       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,
        0.        ],
       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,
        0.        ],
       ...,
       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,
        0.        ],
       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,
        0.        ],
       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,
        0.        ]])
</code></pre>
<p>But I am a little confused by what this means - why are there so many o values? Does implementing <code>TfidfVectorizer()</code> automatically calculate the TF-IDF values for each tag taking into account all documents (i.e. corpus)?</p>
"
"72189892","Python syntax error in list comprehension on string for Lemmatization","2022-05-10 16:24:16","1","53","0","1","","72189974","<p>I'm trying to only perform Lemmatization on words in a string that have more than 4 letters. The desired output from the following code should be 'us american', but I received an invalid syntax error.</p>
<pre><code>import nltk
from nltk.tokenize import TweetTokenizer
lemmatizer = nltk.stem.WordNetLemmatizer()
w_tokenizer = TweetTokenizer()    

wd = w_tokenizer.tokenize(('us americans'))
    [lemmatizer.lemmatize(w) for w in wd if len(w)&gt;4 else wd for wd in w]
</code></pre>
"
"71962152","How to solve 'str' object has no attribute 'lemma_' using Spacy?","2022-04-22 00:20:40","0","2098","3","1","","71976633","<p>I tried to do a lemmatization for my DataFrame using <code>Spacy</code> in python. The code that I used is like this below:</p>
<pre class=""lang-py prettyprint-override""><code># import spaCy's language model
nlp = spacy.load(&quot;en_core_web_sm&quot;)

# function to lemmatize text
def lemmatization(texts):
    output = []
    for i in texts:
        lem = [str(token).lemma_ for token in nlp(i) or str(token) in [&quot;-PRON-&quot;]]
        output.append(' '.join(lem))
    return output

train['clean_tweet'] = lemmatization(train['clean_tweet'])
test['clean_tweet'] = lemmatization(test['clean_tweet'])
</code></pre>
<p>turns out I get an error which said:</p>
<blockquote>
<p>'str' object has no attribute 'lemma_'</p>
</blockquote>
<p>How can I resolve this?</p>
"
"71960583","why the bleu score is zero for this pair even though they are similar","2022-04-21 20:34:29","2","1148","0","1","","71960891","<p>why I'm getting 0 score even though sentences are similar. Please refer the code below</p>
<pre><code>from nltk.translate.bleu_score import sentence_bleu

score = sentence_bleu(['where', 'are', 'economic', 'networks'], ['where', 'are', 'the', 'economic', 'network'])

print(score)
</code></pre>
<p>score value is '0' if u run the above code</p>
"
"71876033","How to build a normalized tf dataframe?","2022-04-14 18:16:31","2","86","2","1","","71878085","<p>I want to apply this into my tf function. <a href=""https://i.sstatic.net/ttWLp.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ttWLp.png"" alt=""enter image description here"" /></a> But unable to build the function.</p>
<hr />
<p>My dataset looks like this
<a href=""https://i.sstatic.net/unVrX.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/unVrX.png"" alt=""enter image description here"" /></a></p>
<p>I have tried to buield the function like this</p>
<pre><code>def term_document_matrix(data, vocab_list = None, doc_index= 'ID', text= 'text'):
      tf_matirx = pd.DataFrame(columns=df[document_index], index= vocab).fillna(0)
    a = int(input(&quot;enter the value&quot;))
    for word in tf_matrix.index:
    
    for doc in data[document_index]:
        
        result = a + (1-a)*[data[data[document_index] == doc][text].values[0].count(word)/X]
        X = ????????
        tf_matrix.loc[word,doc] = result
return tf_matrix
</code></pre>
<p>But unable to build this completely.</p>
<p>Here parameters are described as below</p>
<blockquote>
<pre><code>parameter: 
</code></pre>
</blockquote>
<pre><code>    data: DataFrame. 
    Frequency of word calculated against the data.
    
    vocab_list: list of strings.
    Vocabulary of the documents    
    
    doc_index: str.
    Column name for document index in DataFrame passed.
    
    text: str
    Column name containing text for all documents in DataFrame,
    
returns:
    tf_matrix: DataFrame.
    DataFrame containing term document matrix.
    &quot;&quot;&quot;
</code></pre>
<p><strong>My goal is to get a dataframe like this</strong>
<a href=""https://i.sstatic.net/wXpQZ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/wXpQZ.png"" alt=""enter image description here"" /></a></p>
"
"71866288","Mixing non-overlapping everygrams in order","2022-04-14 03:48:10","0","108","0","1","","71866465","<p>I'm looking for a method to generate sequences from every-grams up to length <code>n</code> that match an input sentence:</p>
<p>Given a sentence:  <code>&quot;Break this into sequences&quot;</code>  and <code>n = 3</code></p>
<p>I want to create the sequences:</p>
<pre><code>(&quot;Break&quot;, &quot;this&quot;, &quot;into&quot;, &quot;sequences&quot;)
(&quot;Break&quot;, &quot;this&quot;, &quot;into sequences&quot;)
(&quot;Break&quot;, &quot;this into&quot;, &quot;sequences&quot;)
(&quot;Break this&quot;, &quot;into&quot;, &quot;sequences&quot;)
(&quot;Break this&quot;, &quot;into sequences&quot;)
(&quot;Break&quot;, &quot;this into sequences&quot;)
(&quot;Break this into&quot;, &quot;sequences&quot;)
</code></pre>
<p><code>nltk</code> has the <a href=""https://www.nltk.org/api/nltk.util.html?highlight=everygram#nltk.util.everygrams"" rel=""nofollow noreferrer""><code>everygram</code></a> package, but I'm not quite sure how I'd use it toward my goal.</p>
<p>I've tried adapting the problem to focus on characters for simplicity, i.e.,</p>
<p>It may be helpful to consider these as character-grams (and, as rici suggested, spacing out characters [with and without spacing shown for clarity]):</p>
<p><code>abcd</code> goes to:</p>
<pre><code>(a, b, c, d)       (a, b, c, d)
(a, b, c  d)       (a, b, cd)
(a, b  c, d)       (a, bc, d)
(a  b, c, d)       (ab, c, d)
(a  b, c  d)       (ab, cd)
(a, b  c  d)       (a, bcd)
(a  b  c, d)       (abc, d)
</code></pre>
<p>For clarity, this should generalize for any length, given a <code>n</code> as the maximum-sized n-gram; so, for <code>abcde</code> with <code>n=3</code> we'd have:</p>
<pre><code>(a, b, c, d, e)     (a, b, c, d, e)
(a, b, c, d  e)     (a, b, c, de)
(a, b, c  d, e)     (a, b, cd, e)
(a, b  c, d  e)     (a, bc, d, e)
(a  b, c, d, e)     (ab, c, d, e)
(a, b  c, d  e)     (a, bc, de)
(a  b, c, d  e)     (ab, c, de)
(a  b, c  d, e)     (ab, cd, e)
(a, b, c  d  e)     (a, b, cde)
(a, b  c  d, e)     (a, bcd, e)
(a  b  c, d, e)     (abc, d, e)
(a  b, c  d  e)     (ab, cde)
(a  b  c, d  e)     (abc, de)
</code></pre>
<p>I'm thinking I may need to generate a grammar, something like:</p>
<pre><code>exp ::= ABC, d | a, BCD
ABC ::= AB, c | A, BC
BCD ::= BC, d | b, CD
AB ::= A, b | a, B
BC ::= B, c | b, C
CD ::= C, d | c, D
A ::= a
B ::= b
C ::= c
D ::= d
</code></pre>
<p>and find all parses of the sentence, but certainly there must be a procedural way to go about this?</p>
"
"71726244","Is possible to get dependency/pos information for entities in Spacy?","2022-04-03 13:11:50","1","665","0","1","","71732367","<p>I am working on extracting entities from scientific text (I am using <strong>scispacy</strong>) and later I will want to extract relations using hand-written rules. I have extracted entities and their character span successfully, and I can also get the pos and dependency tags for tokens and noun chunks. So I am comfortable with the two tasks separately, but I want to bring the two together and I have been stuck for a while.</p>
<p>The idea is that I want to be able to write rules such as: (just an example) if in a sentence/clause there are two entities where the first one is a 'DRUG/CHEMICAL' + is the <em>subject</em>, and the second one is a 'DISEASE' + is an <em>object</em> --&gt; (then) infer 'treatment' relation between the two.</p>
<p>If anyone has any hints on how to approach this task, I would really appreciate it. Thank you!</p>
<p>S.</p>
<p>What I am doing to <strong>extract entities</strong>:</p>
<p><code>doc = nlp(text-with-more-than-one-sent)</code></p>
<p><code>for ent in doc.ents:</code></p>
<pre><code>`... (get information about the ent e.g. its character span)`
</code></pre>
<p><strong>Getting dependency information (for noun chunks and for tokens):</strong></p>
<p><code>for chunk in doc.noun_chunks:</code></p>
<p><code>    print(f&quot;Text: {chunk.text}, Root text: {chunk.root.text}, Root dep: {chunk.root.dep_}, Root head text: {chunk.root.head.text}, POS: {chunk.root.head.pos_}&quot;)</code></p>
<p>_</p>
<p><code>for token in doc:</code></p>
<p><code>    print(f&quot;Text: {token.text}, DEP label: {token.dep_}, Head text: {token.head.text}, Head POS: {token.head.pos_}, Children: {[child for child in token.children]}&quot;)</code></p>
"
"71722637","SpaCy Matcher - Restricting Potential Matches","2022-04-03 02:52:14","0","920","1","1","","71876346","<p>Not too sure exactly how to word the problem, so thank you for indulging the title...</p>
<p>I'm using SpaCy's Matcher function to parse clauses (adverbial/prepositional/etc.) as a part of pre-processing. Some of these clauses are fairly complex and it would be impossible to create strict rules for every instance. Consequently, I have utilized <em><strong>{'OP': '</strong></em><strong>'}</strong>* in my Matcher to account for the tokens that I cannot manually create rules for. <strong>My issue:</strong> is that each clause type cannot permit certain token types. <strong>I would like to</strong> create a rule within my Pattern Matcher that permits all token types, except for particular tokens that I could specify.</p>
<p>Simplified version of my current Matcher for Adjectival Clauses:</p>
<pre><code>pattern = [{'TAG': ',', 'OP': '+'},
           {'DEP': 'det', 'OP': '*'},
           {'DEP': 'det', 'OP': '*'},
           {'DEP': 'amod', 'OP': '+'},
           {'OP': '*'},
           {'TAG': '.', 'OP': '+'}]
</code></pre>
<p><strong>GOAL:</strong> Maintain the core structure of the pattern while being able to exclude &quot;ROOT&quot; dependencies, because the inclusion of &quot;ROOT&quot; Dependency Tokens create false matches.</p>
<p>I have tried to add <strong>{'DEP': 'ROOT', 'OP': '!'}</strong> to create an exception for <em><strong>{'OP': '</strong></em><strong>'}</strong>*. The code resultingly looks like this:</p>
<pre><code>pattern = [{'TAG': ',', 'OP': '+'},
           {'DEP': 'det', 'OP': '*'},
           {'DEP': 'det', 'OP': '*'},
           {'DEP': 'amod', 'OP': '+'},
           {'OP': '*'},
           {'DEP': 'ROOT', 'OP': '!'}
           {'TAG': '.', 'OP': '+'}]
</code></pre>
<p>I expected the matcher to initially parse the unwanted token and accept it in the Matcher, then reject it once it hit the {'DEP': 'ROOT', 'OP': '!'} rule. The goal is to be able to parse the clause from sentence (1) and not parse sentence (2):</p>
<p>(1) &quot;It has started a revolution, this merry band.&quot;
(2) &quot;And yes, this merry band isn’t all happy or all dudes.&quot;</p>
<p>As far as I'm aware, {'OP': '*'} is the only rule that will accept all tokens and {'DEP': 'ROOT', 'OP': '!'} is the only rule to negate tokens. I've tried to mix the order but that hasn't helped either.</p>
<p>If anyone knows of a way to utilize the {'OP': '*'} rule while also being able to restrict specific token types that would be greatly appreciated. Thank you!</p>
"
"71620687","Stemming and lemming words","2022-03-25 17:07:02","-1","825","2","2","","71620861","<p>I have a text document i need to use stemming and Lemmatization on. I have already cleaned the data and tokenised it as well as removing stop words</p>
<p>what i need to do is take the list as an input and return a dict and the dict should have the keys 'original stem and lemmma. and the values being the nth word transformed in that way</p>
<pre><code>  snowball stemmer is defined as Stemmer()
  and WordNetLemmatizer is defined as lemmatizer()
</code></pre>
<p>heres the code ive written but it does give our an error</p>
<pre><code>def find_roots(token_list, n):
n = 2
original = tokens
stem = [ele for sub in original for idx, ele in 
enumerate(sub.split()) if idx == (n - 1)]
stem = stemmer(stem)
lemma = [ele for sub in original for idx, ele in 
enumerate(sub.split()) if idx == (n - 1)]
lemma = lemmatizer()
return 
</code></pre>
<p>Any help would be appreciated</p>
"
"71617889","How to use metadata for document retrieval using Sentence Transformers?","2022-03-25 13:39:32","1","2391","0","1","","71619185","<p>I'm trying to use Sentence Transformers and Haystack for document retrieval, focusing on searching documents on other metadata beside document text.</p>
<p>I'm using a dataset of academic publication titles, and I've appended a fake publication year (which I want to use as a search term). From reading around I've combined the columns and just added a separator between the title and publication year, and included the column titles since I thought maybe this could add context. An example input looks like:</p>
<p><code>title Sparsity-certifying Graph Decompositions [SEP] published year 1980</code></p>
<p>I have a document store and method of retrieving here, based on <a href=""https://github.com/yashprakash13/haystack-search-engine"" rel=""nofollow noreferrer"">this</a>:</p>
<pre><code>document_store_faiss = FAISSDocumentStore(faiss_index_factory_str=&quot;Flat&quot;,
                                          return_embedding=True,
                                          similarity='cosine')

retriever_faiss = EmbeddingRetriever(document_store_faiss,
                                     embedding_model='all-mpnet-base-v2',
                                     model_format='sentence_transformers')

document_store_faiss.write_documents(df.rename(columns={'combined':'content'}).to_dict(orient='records'))
document_store_faiss.update_embeddings(retriever=retriever_faiss)

def get_results(query, retriever, n_docs = 25):
  return [(item.content) for item in retriever.retrieve(q, top_k = n_docs)]

q = 'published year 1999'
print('Results: ')
res = get_results(q, retriever_faiss) 
for r in res:
  print(r) 
</code></pre>
<p>I do a check to see if any inputs actually have a publication year matching the search term, but when I look at my search results I'm getting entries with seemingly random published years. I was hoping that at least the results would all be the same published year, since I hoped to do more complicated queries like <em>&quot;published year before 1980&quot;</em>.</p>
<p>If anyone could either tell me what I'm doing wrong, or whether I have misunderstood this process / expected results it would be much appreciated.</p>
"
"71607906","understanding gpu usage huggingface classification - Total optimization steps","2022-03-24 18:46:36","3","1365","2","1","","71655154","<p>I am training huggingface longformer for a classification problem and got below output.</p>
<ol>
<li><p>I am confused about <code>Total optimization steps</code>. As I have 7000 training data points and 5 epochs and <code>Total train batch size (w. parallel, distributed &amp; accumulation) = 64</code>, shouldn't I get
<code>7000*5/64</code> steps? that comes to <code>546.875</code>? why is it showing <code> Total optimization steps = 545</code></p>
</li>
<li><p>Why in the below output, there are 16 steps of <code>Input ids are automatically padded from 1500 to 1536 to be a multiple of config.attention_window: 512</code> then <code> [ 23/545 14:24 &lt; 5:58:16, 0.02 it/s, Epoch 0.20/5]</code>? what are these steps?</p>
</li>
</ol>
<p>==========================================================</p>
<pre><code>***** Running training *****
  Num examples = 7000
  Num Epochs = 5
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed &amp; accumulation) = 64
  Gradient Accumulation steps = 16
  Total optimization steps = 545
Initializing global attention on CLS token...
Input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
Initializing global attention on CLS token...
Input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
Initializing global attention on CLS token...
Input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
Initializing global attention on CLS token...
Input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
Initializing global attention on CLS token...
Input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
Initializing global attention on CLS token...
Input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
Initializing global attention on CLS token...
Input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
Initializing global attention on CLS token...
Input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
Initializing global attention on CLS token...
Input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
Initializing global attention on CLS token...
Input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
Initializing global attention on CLS token...
Input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
Initializing global attention on CLS token...
Input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
Initializing global attention on CLS token...
Input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
Initializing global attention on CLS token...
Input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
Initializing global attention on CLS token...
Input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
Initializing global attention on CLS token...
Input ids are automatically padded from 1500 to 1536 to be a multiple of `config.attention_window`: 512
 [ 23/545 14:24 &lt; 5:58:16, 0.02 it/s, Epoch 0.20/5]
Epoch   Training Loss   Validation Loss
</code></pre>
<hr />
<p><strong>#update</strong></p>
<p>adding <code>Trainer</code> and <code>TrainingArguments</code></p>
<pre><code>#class weights
class CustomTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.get(&quot;labels&quot;)
        # forward pass
        outputs = model(**inputs)
        logits = outputs.get(&quot;logits&quot;)
        # compute custom loss (suppose one has 3 labels with different weights)
        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 0.5243])).to(device)
        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1)).to(device)
        return (loss, outputs) if return_outputs else loss

 trainer = CustomTrainer(
        model=model,
        args=training_args,
        compute_metrics=compute_metrics,
        train_dataset=train_df_tuning_dataset_tokenized,
        eval_dataset=val_dataset_tokenized
    )



# define the training arguments
training_args = TrainingArguments(
    
    
num_train_epochs = 5,# changed this from 5
per_device_train_batch_size = 4,#4,#8,
gradient_accumulation_steps = 16,
per_device_eval_batch_size= 16,#16
evaluation_strategy = &quot;epoch&quot;,

save_strategy = &quot;epoch&quot;,
learning_rate=2e-5,
load_best_model_at_end=True,
greater_is_better=False,

disable_tqdm = False, 

weight_decay=0.01,
optim=&quot;adamw_torch&quot;,#removing on 18 march from huggingface example notebook
run_name = 'longformer-classification-16March2022'
)
</code></pre>
"
"71585275","Distance of Noun from Verb","2022-03-23 10:04:56","1","169","1","1","","71585593","<p>Is there a way to get the distance of a Noun from the Verb from multiple sentences in a csv file using NLTK and Python?</p>
<p>Example of sentences in a .csv file:</p>
<pre><code>video shows adam stabbing the bystander.
woman quickly ran from the police after the incident.
</code></pre>
<p>Output:</p>
<p>1st sentence: <code>1 (Verb is right after the noun)</code></p>
<p>2nd sentence:
<code>2 (Verb is after another POS tag)</code></p>
"
"71532653","understanding gpu usage huggingface classification","2022-03-18 20:09:03","1","1302","0","1","","71543615","<p>I am building a classifier using huggingface and would like to understand the line <code>Total train batch size (w. parallel, distributed &amp; accumulation) = 64</code> from below</p>
<pre><code> Num examples = 7000
  Num Epochs = 3
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed &amp; accumulation) = 64
  Gradient Accumulation steps = 16
  Total optimization steps = 327
</code></pre>
<p>i have 7000 rows of data, i have defined epochs to be 3 and <code> per_device_train_batch_size = 4</code> and  <code>per_device_eval_batch_size= 16</code>. I also get that <code>Total optimization steps = 327</code> - (7000*3/64)</p>
<p>But I am not clear about <code>Total train batch size (w. parallel, distributed &amp; accumulation) = 64</code>. Does it mean that there are 16 devices as 16*4(<code>Instantaneous batch size per device = 4</code>) comes to 64?</p>
"
"71417857","Position of that Noun and Verb","2022-03-10 01:32:38","1","123","6","1","","71477291","<p>I have a rule-based code that prints out the Noun which is followed by a verb in a sentence</p>
<pre><code>for text_id, text in enumerate(news_df['news_title'].values):
    
    # Remove the comma and full stops
    text = text.replace(',', '').replace('.', '').replace('-','')
    sentence_tags = POSTAG(text.lower())
    
    print(text)
    
    # Sentences parts
    for index, part in enumerate(sentence_tags):
        try:
            
            if 'NN' in part[1] and 'VB' in sentence_tags[index + 1][1]:
            print(&quot;&gt;&quot;, part[0])
            break
            
        elif 'NN' in part[1] and 'NN' in sentence_tags[index + 1][1] and 'VB' in sentence_tags[index + 2][1]:
            print(&quot;&gt;&quot;, part[0],  sentence_tags[index + 1][0])
            break
            
        elif 'NN' in part[1] and 'NN' in sentence_tags[index + 1][1] and 'NN' in sentence_tags[index + 2][1] and 'VB' in sentence_tags[index + 3][1]:
            print(&quot;&gt;&quot;, part[0],  sentence_tags[index + 1][0], sentence_tags[index + 2][0])
            break

        except:
            pass
    print()
</code></pre>
<p>The output of a sentence following this rule:</p>
<p>high school football players charged after video surfaces showing hazing</p>
<pre><code>&gt; school football players
</code></pre>
<p>trump accuser pushes new york to pass the adult survivors act plans to sue</p>
<pre><code>&gt;trump accuser
</code></pre>
<p>Is there a way to also print out the position of that Noun that was printed due to the rule?
for example :</p>
<pre><code>&gt;trump accuser , [0,5,&quot;NN&quot;] , [6,13,&quot;VB&quot;]
</code></pre>
"
"71409353","Finding the position of Noun and Verb in a sentence Python","2022-03-09 12:26:56","6","2015","0","2","","71409646","<p>Is there a way to find the position of the words with pos-tag 'NN' and 'VB' in a sentence in Python?</p>
<p>example of a sentences in a csv file:
&quot;Man walks into a bar.&quot;
&quot;Cop shoots his gun.&quot;
&quot;Kid drives into a ditch&quot;</p>
"
"71398882","CUDA: RuntimeError: CUDA out of memory - BERT sagemaker","2022-03-08 17:06:34","2","8364","0","1","","71399097","<p>I have been trying to train a BertSequenceForClassification Model using AWS Sagemaker. i'm using hugging face estimators. but I keep getting the error: <code>RuntimeError: CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 11.17 GiB total capacity; 10.73 GiB already allocated; 87.88 MiB free; 10.77 GiB reserved in total by PyTorch)</code> the same code runs fine on my laptop.</p>
<ol>
<li>how do I check what is occupying that 10GB of memory? my dataset is pretty small (68kb), so is my batch size (8) and epochs (1). When I run nvidia-smi, i can only see &quot;No processes running&quot; and the GPU memory usage is zero. When I run <code>print(torch.cuda.memory_summary(device=None, abbreviated=False))</code> from within my training script (right before it throws the error) it prints</li>
</ol>
<pre><code>|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|
</code></pre>
<p>but I have no idea what it means or how to interpret it</p>
<ol start=""2"">
<li>when i run <code>!df -h</code> I can see:</li>
</ol>
<pre><code>Filesystem      Size  Used Avail Use% Mounted on
devtmpfs         30G   72K   30G   1% /dev
tmpfs            30G     0   30G   0% /dev/shm
/dev/xvda1      109G   93G   16G  86% /
/dev/xvdf       196G   61M  186G   1% /home/ec2-user/SageMaker
</code></pre>
<p>how is this memory different from the GPU? if theres 200GB in /dev/xvdf is there anyway I can just use that..? in my test script I tried<br />
<code>model = BertForSequenceClassification.from_pretrained(args.model_name,num_labels=args.num_labels).to(&quot;cpu&quot;)</code>
but that just gives the same error</p>
"
"71327407","Extracting important entities from unstructured data","2022-03-02 18:18:30","1","812","0","1","","71367592","<p>I am working on a NLP problem where I am completely stuck at certain point. I am new to these so pardon if the question is dumb.
I have got a completely unstructured text let's say: &quot;<code>a person named x y is travelling to country ab, he spent xyz (alpha/currency/beta/gamma), ate a b c d e f food items and many more.</code>&quot;
now I have to extract</p>
<pre><code>|name of person| country's name | amount spent and the currency | food items he ate | place of              
stay|
</code></pre>
<p><strong>Constraint on this is, the text contains some false information, for example: the food b and c cannot be found in a particular country, and thus it should not be extracted.</strong>
I have a nested dictionary which looks like this:</p>
<pre><code>{country_name: {place 1: {name of hotels:[hotel1, hotel2, hotel3....],
                          eatables: [food1, food2, food3, food4.....],
                          currency_accepted: [c1, c2, c3, c4.......],
                          }
                }
} 
</code></pre>
<p><strong>I want to use this dictionary in the unstructured text so that I can parse the data and extract entities which are relevant in separate columns of dataframe.</strong>
I have seen NER based approaches, but I guess it requires tagging of words, and I have got huge data.</p>
<p><strong>I have tried regex based approach for pattern matching, but that doesn't give all the results, further to that I have tried to match all the entities stored in a list, but this creates the problem of many false entities being extracted and accuracy is quite important here.</strong></p>
<p>I am looking for more improve parsing based approaches, also if there is any way a certain model is trained on this dictionary such that it looks for values of nested dictionary only if a key is found in the unstructured text.</p>
"
"71280204","Group numpy array elements without for-loop","2022-02-26 20:54:45","0","72","0","2","","71280315","<p>After doing some text processing, I've got a list of tokens and a list of sentence indices, one for each token. Now I'd like to reassemble the tokens into sentences. I've used Numpy, but I feel like there's a better/faster/more-numpy-ish way to do this...without a for loop. There could be a lot more than two sentences in the future.</p>
<pre><code>import numpy as np

all_tokens = np.array(['I', 'spent', 'a', 'lot', 'of', 'time', ',', 'money', ',', 'and', 'effort', 'childproofing', 'my', 'house', '.', 'However', ',', 'the', 'kids', 'still', 'get', 'in', '.'])
sent_ids = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1])

new_sents = []
for unique_sent_id in np.unique(sent_ids):
    sent_tokens = all_tokens[sent_ids == unique_sent_id].tolist()
    new_sents.append(' '.join(sent_tokens))
</code></pre>
<p>Result: [&quot;I spent a lot of time , money , and effort childproofing my house .&quot;, &quot;However , the kids still get in .&quot;]</p>
"
"71261467","Should you Stem and lemmatize?","2022-02-25 05:33:46","1","2572","1","1","","71264583","<p>I am currently working with python NLTK to preprocess text data for <a href=""https://www.kaggle.com/uciml/sms-spam-collection-dataset/data"" rel=""nofollow noreferrer"">Kaggle SMS Spam Classification Dataset</a>. I have completed the following steps during preprocessing:</p>
<ol>
<li>Removed any extra spaces</li>
<li>Removed punctuation and special characters</li>
<li>Converted the text to lower case</li>
<li>Replaced abbreviations such as lol,brb etc with their meaning or full form.</li>
<li>Removed stop words</li>
<li>Tokenized the data</li>
</ol>
<p>Now I plan to perform lemmatization and stemming separately on the tokenized data followed by TF-IDF done separately on lemmatized data and stemmed data.</p>
<p><strong>Questions are as follows:</strong></p>
<ul>
<li>Is there a practical use case to perform lemmatization on the tokenized data and then stem that lemmatized data or vice versa</li>
<li>Does the idea of stemming the lemmatized data or vice versa make any sense theoretically, or is it completely incorrect.</li>
</ul>
<p><strong>Context:</strong> I am relatively new to NLP and hence I am trying to understand as much as I can about these concepts. The main idea behind this question is to understand whether lemmatization or stemming together make any sense theoretically/practically or whether these should be done separately.</p>
<p><strong>Questions Referenced:</strong></p>
<ul>
<li><a href=""https://stackoverflow.com/questions/49354665/should-i-perform-both-lemmatization-and-stemming"">Should I perform both lemmatization and stemming?</a>: The answer to this question was inconclusive and not accepted, it never discussed why you should or should not do it in the first place.</li>
<li><a href=""https://stackoverflow.com/questions/1787110/what-is-the-difference-between-lemmatization-vs-stemming/1787121#1787121"">What is the difference between lemmatization vs stemming?</a>: Provides the ideas behind stemming and lemmatization but I was unable to conclude the answers to my questions based on this</li>
<li><a href=""https://stackoverflow.com/questions/17317418/stemmers-vs-lemmatizers?noredirect=1&amp;lq=1"">Stemmers vs Lemmatizers</a>: Explains the pros and cons, as well as the context in which stemming and lemmatization, might help</li>
<li><a href=""https://stackoverflow.com/questions/62651893/nlp-stemming-and-lemmatization-using-regular-expression-tokenization"">NLP Stemming and Lemmatization using Regular expression tokenization</a>: The question discusses the different preprocessing steps and does stemming and lemmatization separately</li>
</ul>
"
"71117302","Spacy lemmatizer suddenly returns other value than three months ago, words are not transformed into the singular form anymore","2022-02-14 19:31:24","2","319","2","1","","71121723","<p>I used spacy a few months ago to lemmatize a large amount of text.
Today I had to rerun the written script and the output of spacy changed, it is mostly that plural forms of words are not transformed to the singular anymore.
I tried to reproduce the problem with a simpler use case and the word queen which breaks down to the following:</p>
<pre><code>import spacy

nlp = spacy.load('en_core_web_lg')

sentence = &quot;queen queenhat queens queen&quot;

test = nlp(sentence)

for word in test:
    print(word.lemma_)
</code></pre>
<p>The output of this is: queen, queenhat, queens, queen</p>
<p>If I remove the last queen (&quot;queen queenhat queens&quot;) the output is: queen, queenhat, queen</p>
<p>In that case, the s gets removed like it did three months ago.</p>
<p>I assumed from this that the s only gets removed if the queen is at the end, since the input &quot;queen queenhat queens queens&quot; also returns: queen queenhat queens queen</p>
<p>But if I at another queens the output becomes: queen queenhat queens queens queens in which case not even the last queen is transformed to the singular form anymore.</p>
<p>I assume this happens because I reinstalled spacy between today and three months ago and got a newer version, I fixed the problem by giving spacy only single words and no full texts, but this really slows the entire script  down from seconds to hours. This also happens for other words, queen is just the example I choose to test spacy with.</p>
<p>Is there someway I can fix this? Thanks in advance</p>
"
"71050697","Transformers: How to use CUDA for inferencing?","2022-02-09 13:44:59","8","19863","2","2","","71052343","<p>I have fine-tuned my models with GPU but inferencing process is very slow, I think this is because inferencing uses CPU by default. Here is my inferencing code:</p>
<pre><code>txt = &quot;This was nice place&quot;
model = transformers.BertForSequenceClassification.from_pretrained(model_path, num_labels=24)
tokenizer = transformers.BertTokenizer.from_pretrained('TurkuNLP/bert-base-finnish-cased-v1')
encoding = tokenizer.encode_plus(txt, add_special_tokens = True, truncation = True, padding = &quot;max_length&quot;, return_attention_mask = True, return_tensors = &quot;pt&quot;)
output = model(**encoding)
output = output.logits.softmax(dim=-1).detach().cpu().flatten().numpy().tolist()
</code></pre>
<p>Here is my second inferencing code, which is using pipeline (for different model):</p>
<pre><code>classifier = transformers.pipeline(&quot;sentiment-analysis&quot;, model=&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;)
result = classifier(txt)
</code></pre>
<p>How can I force transformers library to do faster inferencing on GPU? I have tried adding <code>model.to(torch.device(&quot;cuda&quot;))</code> but that throws error:</p>
<pre><code>Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu
</code></pre>
<p>I suppose the problem is related to the data not being sent to GPU. There is a similar issue here: <a href=""https://stackoverflow.com/questions/68585678/pytorch-summary-fails-with-huggingface-model-ii-expected-all-tensors-to-be-on-t"">pytorch summary fails with huggingface model II: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu</a></p>
<p>How would I send data to GPU with and without pipeline? Any advise is highly appreciated.</p>
"
"70956389","Why my output return in a strip-format and cannot be lemmatized/stemmed in Python?","2022-02-02 13:21:12","0","122","0","1","","70957123","<p>First step is tokenizing the text from dataframe using NLTK. Then, I create a spelling correction using TextBlob. For this, I convert the output from tuple to string. After that, I need to lemmatize/stem (using NLTK). The problem is my output return in a strip-format. Thus, it cannot be lemmatized/stemmed.</p>
<pre><code>#create a dataframe
import pandas as pd
import nltk
df = pd.DataFrame({'text': [&quot;spellling&quot;, &quot;was&quot;, &quot;working cooking listening&quot;,&quot;studying&quot;]})

#tokenization
w_tokenizer = nltk.tokenize.WhitespaceTokenizer()
def tokenize(text):
    return [w for w in w_tokenizer.tokenize(text)]
df[&quot;text2&quot;] = df[&quot;text&quot;].apply(token)

#spelling correction
def spell_eng(text):
  text=TextBlob(str(text)).correct()
  #convert from tuple to str
  text=functools.reduce(operator.add, (text))
  return text
df['text3'] = df['text2'].apply(spell_eng)


#lemmatization/stemming
def stem_eng(text):
   lemmatizer = nltk.stem.WordNetLemmatizer()
   return [lemmatizer.lemmatize(w,'v') for w in text]
df['text4'] = df['text3'].apply(stem_eng)
</code></pre>
<p><strong>Generated output:</strong>
<a href=""https://i.sstatic.net/AUj5M.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/AUj5M.jpg"" alt=""enter image description here"" /></a></p>
<p><strong>Desired output:</strong></p>
<pre><code>text4
--------------
[spell]
[be]
[work,cook,listen]
[study]
</code></pre>
"
"70905201","Finding non-existing words with spaCy?","2022-01-29 12:08:54","2","1074","0","1","","70911934","<p>I am new to spaCy.  I have a (German) text in which I want to find all the words not in the dictionary (using the <code>de_core_news_lg</code> pipeline).  Reading spaCy's documentation, the only thing I found that looked promising was <code>Token.has_vector()</code>.  When I check all the tokens in the Doc object I get by running <code>nlp(TEXT)</code> I find that, indeed, the tokens for which <code>has_vector()</code> returns <code>False</code> seem to be either typos or rare words not likely to be in the dictionary.</p>
<p>So my hypothesis is that returning <code>False</code> from <code>Token.has_vector()</code> is equivalent to not having found the respective word in the dictionary.  Am I correct?  Is there a better way for finding words not in dictionary?</p>
"
"70880940","Combine two regexp grammars in nltk","2022-01-27 15:10:16","1","198","0","1","","70885691","<p>I'm defining a noun phrase using grammar in <code>nltk</code>. The example provided by <code>nltk</code> is:</p>
<pre><code>grammar = &quot;NP: {&lt;DT&gt;?&lt;NNP&gt;*&lt;NN&gt;}&quot;
</code></pre>
<p>Then if I have a sentence like: <code>show me the Paris hospitals</code>, the library can detect the noun phrase:</p>
<pre><code>&gt;&gt;&gt; s
'show me the Paris hospitals'
&gt;&gt;&gt; grammar = &quot;NP: {&lt;DT&gt;?&lt;NNP&gt;*&lt;NNS&gt;}&quot;
&gt;&gt;&gt; nltk.RegexpParser(grammar).parse(nltk.pos_tag(nltk.word_tokenize(s)))
Tree('S', [('show', 'VB'), ('me', 'PRP'), Tree('NP', [('the', 'DT'), ('Paris', 'NNP'), ('hospitals', 'NNS')])])
</code></pre>
<p>Now, the sentence can be written in another way: <code>show me the hospitals of Paris</code>, and hence I need to change the grammar to:</p>
<pre><code>&gt;&gt;&gt; grammar = &quot;NP: {&lt;DT&gt;?&lt;NNS&gt;&lt;IN&gt;&lt;NNP&gt;}&quot;
&gt;&gt;&gt; s = &quot;show me the hospitals in Paris&quot;
&gt;&gt;&gt; nltk.RegexpParser(grammar).parse(nltk.pos_tag(nltk.word_tokenize(s)))
Tree('S', [('show', 'VB'), ('me', 'PRP'), Tree('NP', [('the', 'DT'), ('hospitals', 'NNS'), ('in', 'IN'), ('Paris', 'NNP')])])
</code></pre>
<p>How do I combine the two grammars in a unique one? I couldn't figure out the OR condition for the two grammars.</p>
"
"70837053","Can I update the spacy's Entity Linking knowledge base after training?","2022-01-24 16:21:53","1","357","0","1","","70843867","<p>Let's suppose I have successfully trained an Entity Linking model, and it is working just fine. But, eventually, I'm going to update some aliases of the knowledge base. Just some aliases not the description nor new entities.</p>
<p>I know that spacy has a method to do so which is: <code>kb.add_alias(alias=&quot;Emerson&quot;, entities=qids, probabilities=probs)</code>. But, what if I have to do that after the training process? Should I re-run everything, or updating the KB will do?</p>
"
"70713831","Convert words between part of speech, when wordnet doesn't do it","2022-01-14 16:41:51","2","140","0","1","","70720319","<p>There are a lot of Q&amp;A about part-of-speech conversion, and they pretty much all point to WordNet <code>derivationally_related_forms()</code>  (For example, <a href=""https://stackoverflow.com/questions/14489309/convert-words-between-verb-noun-adjective-forms"">Convert words between verb/noun/adjective forms</a>)</p>
<p>However, I'm finding that the WordNet data on this has important gaps. For example, I can find no relation at all between 'succeed', 'success', 'successful' which seem like they should be V/N/A variants on the same concept. Likewise none of the lemmatizers I've tried seem to see these as related, although I can get snowball stemmer to turn 'failure' into 'failur' which isn't really much help.</p>
<p>So my questions are:</p>
<ol>
<li>Are there any other (programmatic, ideally python) tools out there that do this POS-conversion, which I should check out? (The WordNet hits are masking every attempt I've made to google alternatives.)</li>
<li>Failing that, are there ways to submit additions to WordNet despite the &quot;due to lack of funding&quot; situation they're presently in? (Or, can we set up a crowdfunding campaign?)</li>
<li>Failing that, are there straightforward ways to distribute supplementary corpus to users of nltk that augments the WordNet data where needed?</li>
</ol>
"
"70703655","Unable to create a custom torchtext BucketIterator","2022-01-13 22:04:13","0","425","0","1","","70720797","<p>I'm trying to create a POS tagger with LSTM and I'm facing some difficulties with preparing the data.</p>
<p>I've successfully followed a guide that used the following code to prepare the data itertors:</p>
<pre><code>TEXT = data.Field(lower = True)
UD_TAGS = data.Field(unk_token = None)
PTB_TAGS = data.Field(unk_token = None)
fields = ((&quot;text&quot;, TEXT), (&quot;udtags&quot;, UD_TAGS), (&quot;ptbtags&quot;, PTB_TAGS))
train_data, valid_data, test_data = datasets.UDPOS.splits(fields)
MIN_FREQ = 2

TEXT.build_vocab(train_data, 
                 min_freq = MIN_FREQ,
                 vectors = &quot;glove.6B.100d&quot;,
                 unk_init = torch.Tensor.normal_)
UD_TAGS.build_vocab(train_data)
PTB_TAGS.build_vocab(train_data)

BATCH_SIZE = 128

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(
    (train_data, valid_data, test_data), 
    batch_size = BATCH_SIZE,
    device = device)
</code></pre>
<p>And then when training the model the code is:</p>
<pre><code>for batch in iterator:
        text = batch.text
        tags = batch.udtags
</code></pre>
<p>Now for my problem - I have a dataset of lists: list of sentences (where every sentence is a list of words) and a list of lists of tags corresponsing to the sentences words.</p>
<p>I created a torch DataSet intance from the x_train, y_train (each one is a list of lists).
But, it does not behave like the 'train_data' that comes from datasets.UDPOS.splits(fields). So, when trying to access the data with:</p>
<pre><code>for batch in iterator:
        text = batch.text
        tags = batch.udtags
</code></pre>
<p>I'm getting an error since my iterator does not have the fields inside. I tried accecing the data in a different manner but coudn't find a way around it.
I also noticed that in the above example, the data in the batch is with the embeddings indexes, while the batch in my code is still the words themselves.</p>
<p>All of the examples I found on the internet uses datasets from <code>torchtext.legacy.datasets</code>, so it does not really help me with my problem.</p>
<p>If it helps, here is my code (it's part of a bigger project, so a bit messy):</p>
<pre><code>class ConvertDataset(Dataset):
    &quot;&quot;&quot;
    Create an instances of pytorch Dataset from lists.
    &quot;&quot;&quot;

    def __init__(self, x, y):
        # data loading
        self.x = x
        self.y = y

    def __getitem__(self, index):
        return {'text': self.x[index], 'tags': self.y[index]}

    def __len__(self):
        return len(self.x)


</code></pre>
<pre><code># ## model variables
DROPOUT = 0.25
HIDDEN_DIM = 128

# ## load and prepare train data
train_set = load_annotated_corpus(params_d['data_fn'])
x_train, y_train = _prepare_data(train_set)
TEXT = Field(lower=True)
UD_TAGS = Field(unk_token=None)

# ## build words and tags vocabularies
TEXT.build_vocab(x_train,
                 min_freq=params_d['min_frequency'],
                 vectors='glove.6B.100d',
                 unk_init=torch.Tensor.normal_,
                 max_size=None if params_d['max_vocab_size'] == -1 else 
                              params_d['max_vocab_size'])

UD_TAGS.build_vocab(y_train)

# ## more model variables
INPUT_DIM = len(TEXT.vocab)
PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]

# ## initiate a model
 lstm_model = BiLSTM.LSTM(input_dim=INPUT_DIM,
                             embedding_dim=params_d['embedding_dimension'],
                             hidden_dim=HIDDEN_DIM,
                             output_dim=params_d['output_dimension'],
                             n_layers=params_d['num_of_layers'],
                             dropout=DROPOUT,
                             pad_idx=PAD_IDX)

lstm_model.apply(_init_weights)

pretrained_embeddings = TEXT.vocab.vectors
lstm_model.embedding.weight.data.copy_(pretrained_embeddings)
# set pad tag embedding to 0
lstm_model.embedding.weight.data[PAD_IDX] = torch.zeros(params_d['embedding_dimension'])

BATCH_SIZE = 128
</code></pre>
<p>My data (lists of lists):</p>
<pre><code>x_train, y_train = _prepare_data(train_data)
</code></pre>
<p>Data preparation</p>
<pre><code>train_torch_dataset = ConvertDataset(x_train, y_train)

# ## create data iterators
train_iterator = BucketIterator(
        train_torch_dataset,
        batch_size=BATCH_SIZE,
        device=device,
        # Function to use for sorting examples.
        sort_key=lambda x: len(x['text']),
        # Repeat the iterator for multiple epochs.
        repeat=True,
        # Sort all examples in data using `sort_key`.
        sort=False,
        # Shuffle data on each epoch run.
        shuffle=True,
        # Use `sort_key` to sort examples in each batch.
        sort_within_batch=True
    )
</code></pre>
"
"70609579","Use Quantization on HuggingFace Transformers models","2022-01-06 15:36:37","1","1888","0","1","","70619775","<p>I'm learning <strong>Quantization</strong>, and am experimenting with <strong>Section 1</strong> of this <a href=""https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/notebooks/bert/Bert-GLUE_OnnxRuntime_quantization.ipynb"" rel=""nofollow noreferrer"">notebook</a>.</p>
<p>I want to use this code on my own models.</p>
<p>Hypothetically, I only need to assign to <code>model</code> variable in <strong>Section 1.2</strong></p>
<hr />
<pre class=""lang-py prettyprint-override""><code># load model
model = BertForSequenceClassification.from_pretrained(configs.output_dir)
model.to(configs.device)
</code></pre>
<p>My models are from a different library: <code>from transformers import pipeline</code>. So <code>.to()</code> throws an <code>AttributeError</code>.</p>
<p>My Model:</p>
<pre><code>pip install transformers
</code></pre>
<pre class=""lang-py prettyprint-override""><code>from transformers import pipeline

unmasker = pipeline('fill-mask', model='bert-base-uncased')
model = unmasker(&quot;Hello I'm a [MASK] model.&quot;)
</code></pre>
<p>Output:</p>
<pre><code>Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
</code></pre>
<hr />
<p><strong>How might I run the linked Quantization code on my example model?</strong></p>
<p>Please let me know if there's anything else I should clarify in this post.</p>
"
"70607224","HuggingFace - 'optimum' ModuleNotFoundError","2022-01-06 12:39:23","1","7214","5","1","","70667079","<p>I want to run the 3 code snippets from this <a href=""https://huggingface.co/hardware"" rel=""nofollow noreferrer"">webpage</a>.</p>
<p>I've made all 3 one post, as I am assuming it all stems from the same problem of <code>optimum</code> not having been imported correctly?</p>
<p>Kernel: <code>conda_pytorch_p36</code></p>
<hr />
<p>Installations:</p>
<pre><code>pip install optimum
</code></pre>
<p>OR</p>
<pre><code>! pip install datasets transformers optimum[intel]
</code></pre>
<p>Both provide same Traceback:</p>
<pre><code>Requirement already satisfied: optimum in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (0.1.3)
Requirement already satisfied: transformers&gt;=4.12.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from optimum) (4.15.0)
Requirement already satisfied: coloredlogs in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from optimum) (15.0.1)
Requirement already satisfied: torch&gt;=1.9 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from optimum) (1.10.1)
Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from optimum) (1.8)
Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torch&gt;=1.9-&gt;optimum) (3.10.0.0)
Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torch&gt;=1.9-&gt;optimum) (0.8)
Requirement already satisfied: numpy&gt;=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers&gt;=4.12.0-&gt;optimum) (1.19.5)
Requirement already satisfied: packaging&gt;=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers&gt;=4.12.0-&gt;optimum) (21.3)
Requirement already satisfied: pyyaml&gt;=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers&gt;=4.12.0-&gt;optimum) (5.4.1)
Requirement already satisfied: sacremoses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers&gt;=4.12.0-&gt;optimum) (0.0.46)
Requirement already satisfied: tqdm&gt;=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers&gt;=4.12.0-&gt;optimum) (4.62.3)
Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers&gt;=4.12.0-&gt;optimum) (2021.4.4)
Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers&gt;=4.12.0-&gt;optimum) (2.25.1)
Requirement already satisfied: huggingface-hub&lt;1.0,&gt;=0.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers&gt;=4.12.0-&gt;optimum) (0.2.1)
Requirement already satisfied: tokenizers&lt;0.11,&gt;=0.10.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers&gt;=4.12.0-&gt;optimum) (0.10.3)
Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers&gt;=4.12.0-&gt;optimum) (4.5.0)
Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers&gt;=4.12.0-&gt;optimum) (3.0.12)
Requirement already satisfied: humanfriendly&gt;=9.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from coloredlogs-&gt;optimum) (10.0)
Requirement already satisfied: mpmath&gt;=0.19 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sympy-&gt;optimum) (1.2.1)
Requirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from packaging&gt;=20.0-&gt;transformers&gt;=4.12.0-&gt;optimum) (2.4.7)
Requirement already satisfied: zipp&gt;=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata-&gt;transformers&gt;=4.12.0-&gt;optimum) (3.4.1)
Requirement already satisfied: idna&lt;3,&gt;=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests-&gt;transformers&gt;=4.12.0-&gt;optimum) (2.10)
Requirement already satisfied: certifi&gt;=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests-&gt;transformers&gt;=4.12.0-&gt;optimum) (2021.5.30)
Requirement already satisfied: chardet&lt;5,&gt;=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests-&gt;transformers&gt;=4.12.0-&gt;optimum) (4.0.0)
Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests-&gt;transformers&gt;=4.12.0-&gt;optimum) (1.26.5)
Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses-&gt;transformers&gt;=4.12.0-&gt;optimum) (1.0.1)
Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses-&gt;transformers&gt;=4.12.0-&gt;optimum) (8.0.1)
Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses-&gt;transformers&gt;=4.12.0-&gt;optimum) (1.16.0)
Note: you may need to restart the kernel to use updated packages.
</code></pre>
<hr />
<pre class=""lang-py prettyprint-override""><code>from optimum.intel.lpot.quantization import LpotQuantizerForSequenceClassification

# Create quantizer from config 
quantizer = LpotQuantizerForSequenceClassification.from_config(
    &quot;echarlaix/quantize-dynamic-test&quot;,
    &quot;quantization.yml&quot;,
    model_name_or_path=&quot;textattack/bert-base-uncased-SST-2&quot;,
)

model = quantizer.fit_dynamic()
</code></pre>
<p>Traceback:</p>
<pre><code>---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
&lt;ipython-input-6-9dcf25f181ea&gt; in &lt;module&gt;
----&gt; 1 from optimum.intel.lpot.quantization import LpotQuantizerForSequenceClassification
      2 
      3 # Create quantizer from config
      4 quantizer = LpotQuantizerForSequenceClassification.from_config(
      5     &quot;echarlaix/quantize-dynamic-test&quot;,

ModuleNotFoundError: No module named 'optimum.intel.lpot'
</code></pre>
<pre class=""lang-py prettyprint-override""><code>from optimum.intel.lpot.pruning import LpotPrunerForSequenceClassification

# Create pruner from config 
pruner = LpotPrunerForSequenceClassification.from_config(
    &quot;echarlaix/magnitude-pruning-test&quot;,
    &quot;prune.yml&quot;,
    model_name_or_path=&quot;textattack/bert-base-uncased-SST-2&quot;,
)

model = pruner.fit()
</code></pre>
<p>Traceback:</p>
<pre><code>---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
&lt;ipython-input-7-e9872c164aee&gt; in &lt;module&gt;
----&gt; 1 from optimum.intel.lpot.pruning import LpotPrunerForSequenceClassification
      2 
      3 # Create pruner from config
      4 pruner = LpotPrunerForSequenceClassification.from_config(
      5     &quot;echarlaix/magnitude-pruning-test&quot;,

ModuleNotFoundError: No module named 'optimum.intel.lpot'
</code></pre>
<pre class=""lang-py prettyprint-override""><code>from optimum.graphcore import IPUTrainer
from optimum.graphcore.bert import BertIPUConfig
from transformers import BertForMaskedLM, BertTokenizer
from poptorch.optim import AdamW

# Allocate model and tokenizer as usual
tokenizer = BertTokenizer.from_pretrained(&quot;bert-base-cased&quot;)
model = BertForMaskedLM.from_pretrained(&quot;bert-base-cased&quot;)

# Trainer + poptorch custom configuration optional 
ipu_config = BertIPUConfig()
trainer = IPUTrainer(model, trainings_args, config=ipu_config)
optimizer = AdamW(model.parameters)

# This is hidden from the user, it will be handled by the Trainer
with trainer.compile(some_data_loader) as model_f:
    for steps in range(10):  # !
        outputs = trainer.step(optimizer)    

# Save the model and/or push to hub
model.save_pretrained(&quot;...&quot;)
model.push_to_hub(&quot;...&quot;)
</code></pre>
<p>Traceback:</p>
<pre><code>---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
&lt;ipython-input-8-921e03245390&gt; in &lt;module&gt;
----&gt; 1 from optimum.graphcore import IPUTrainer
      2 from optimum.graphcore.bert import BertIPUConfig
      3 from transformers import BertForMaskedLM, BertTokenizer
      4 from poptorch.optim import AdamW
      5 

ModuleNotFoundError: No module named 'optimum.graphcore'
</code></pre>
<p>Please let me know if there's anything else I can add to post.</p>
"
"70546666","Search for particular parts of speech (e.g. nouns) and print them along with a preceding word","2022-01-01 02:17:37","1","509","0","2","","70562037","<p>I have a text which is made up of a list of basic sentences, such as <em>&quot;she is a doctor&quot;</em>, <em>&quot;he is a good person&quot;</em>, and so forth. I'm trying to write a program which will return only the nouns and the preceding pronoun (e.g. she, he, it). I need them to print as a pair, for example <code>(she, doctor)</code> or <code>(he, person)</code>. I'm using <code>SpaCy</code> as this will allow me to work with similar texts in French and German as well.</p>
<p><a href=""https://stackoverflow.com/questions/58844962/match-the-last-noun-before-a-particular-word"">This</a> is the closest thing I've found elsewhere on this site as to what I need. What I've been trying so far is to produce a list of nouns in the text and then search the text for nouns in the list, and print the noun and the word 3 places before it (since this is the pattern for most of the sentences, and most is good enough for my purposes). This is what I've got for creating the list:</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>def spacy_tag(text):
  text_open = codecs.open(text, encoding='latin1').read()
  parsed_text = nlp_en(text_open)
  tokens = list([(token, token.tag_) for token in parsed_text])
  list1 = []
  for token, token.tag_ in tokens:
    if token.tag_ == 'NN':
      list1.append(token)
  return(list1)</code></pre>
</div>
</div>
</p>
<p>However, when I try to do anything with it, I get an error message. I've tried using enumerate but I couldn't get that to work either. This is the current code I have for searching the text for the words in the list (I haven't gotten around to adding the part which should print the word several places beforehand as I'm still stuck on the searching part):</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>def spacy_search(text, list):
  text_open = codecs.open(text, encoding='latin1').read()
  for word in text_open:
   if word in list:
     print(word)</code></pre>
</div>
</div>
</p>
<p>The error I get is at line 4, <code>&quot;if word in list:&quot;, and it says &quot;TypeError: Argument 'other' has incorrect type (expected spacy.tokens.token.Token, got str)&quot;</code></p>
<p>Is there a more efficient way of printing a <code>PRP</code>, <code>NN pair</code> using <code>SpaCy</code>? And alternatively, how can I amend my code to work so it searches the text for the nouns in the list? (It doesn't need to be a particularly elegant solution, it just needs to produce a result).</p>
"
"70529754","Python: How to speed up lemmatisation if I check the POS for each word?","2021-12-30 09:32:11","0","220","0","1","","70538225","<p>I am new to NLP. I wish to lemmatise. But understand that for WordNetLemmatizer, it depends on the type of words passed in Noun, Verb, etc.</p>
<p>Hence I tried the below code but it is very slow. Basically all my text are saved in a column called &quot;Text&quot; in df. I use the pre_process(text) function by looping each row (Option 1) but it is v slow.</p>
<p>I tried apply (Option 2) , but it just as slow.
Any way to speed up? Thank you!</p>
<pre><code>from nltk import WordNetLemmatizer, pos_tag
import pandas as pd

def pre_process(text):
 
    words_only = words_only.lower().split()    

    lem = WordNetLemmatizer()
    words_only1=[]
    for j in range(0, len(words_only)):
        
        pos_label = (pos_tag(words_only)[j][1][0]).lower()
        word=words_only[j]
        
        if pos_label == 'j': pos_label = 'a'    # 'j' &lt;--&gt; 'a' reassignment
        
        if pos_label in ['r']:  # For adverbs it's a bit different
            try:
                word=wordnet.synset(word+'.r.1').lemmas()[0].pertainyms()[0].name() # Could have errors for words like 'not'
            except:
                word=lem.lemmatize(word)

        elif pos_label in ['a', 's', 'v']: # For adjectives and verbs
            word=lem.lemmatize(word, pos=pos_label)

        else:   # For nouns and everything else as it is the default kwarg
            word=lem.lemmatize(word)
        
        words_only1.append(word)
    
    words_only=words_only1
    return( &quot; &quot;.join(words_only)) 


df=pd.read_excel( 'C:/Users/Desktop/TEST.xlsx', 
                   sheet_name='Text', 
                   engine='openpyxl')

**Option 1**
num_text = df.shape[0]
clean_text= []
for i in range(0, num_text):
    clean_text.append(pre_process(df['Text'].iloc[i]))


**Option 2**
df_bd['Processed Text']=df['Text'].apply(pre_process_bow)
clean_text= df['Processed Text'].tolist()
</code></pre>
"
"70472067","based word for battling and lemmatization","2021-12-24 10:44:18","-1","54","0","1","","70472243","<p>All,</p>
<p>What is the base form of <strong>battling</strong>? Lemmatization results in <strong>battling</strong> where as I think it should be <strong>battle</strong>. Is my understanding of lemmatization wrong?</p>
<pre><code>from nltk import download
download('wordnet')
from nltk.stem.wordnet import WordNetLemmatizer

lemmatizer = WordNetLemmatizer()

def get_lemma(word):
    return lemmatizer.lemmatize(word)

get_lemma('battling')
</code></pre>
<p>The same is for the word <strong>coming</strong></p>
<p><a href=""https://i.sstatic.net/koQPL.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/koQPL.png"" alt=""enter image description here"" /></a></p>
"
"70339341","NLP translation giving me sentence translations instead of word translation","2021-12-13 18:30:01","0","235","0","1","","70347017","<p>I trained a Transformers using a Portuguese-English dataset from <a href=""http://www.manythings.org/anki/"" rel=""nofollow noreferrer"">http://www.manythings.org/anki/</a>. This is a parallelized sentence dataset.</p>
<p>After training, I tried translating the word &quot;doente&quot; which should've translated to &quot;sick&quot; but it instead I got &quot;I feel sick&quot;.</p>
<p>Any ideas of how do I get just the word sick ?</p>
<p>Am I training my model with the wrong dataset ? sentence based instead of word based ?</p>
<p>tks in advance</p>
"
"70328615","Stemming words in a list (Python NLTK)","2021-12-12 23:45:12","0","811","2","2","","70328718","<p>I feel like I'm doing something really stupid here, I am trying to stem words I have in a list but it is not giving me the intended outcome, my code is:</p>
<pre><code>from nltk.stem.snowball import SnowballStemmer
snowball = SnowballStemmer(language=&quot;english&quot;)
my_words = ['works', 'shooting', 'runs']
for w in my_words:
    w=snowball.stem(w)
print(my_words)
</code></pre>
<p>and the ouput I get is</p>
<pre><code>['works', 'shooting', 'runs]
</code></pre>
<p>as opposed to</p>
<pre><code>['work','shoot','run']
</code></pre>
<p>I feel like I'm doing something very silly with my list but could anyone enlighten me what I'm doing wrong?</p>
"
"70189982","How to find strings of a list in a text with typo's","2021-12-01 19:13:48","1","237","0","1","","70198066","<p>I'm trying to check if some String in a list are in a given text. But the given text can have some typos. For example let's take this.</p>
<p>text: The brownw focx and the cat are in th eforest.
and my list is: [brown fox, forest, cat]</p>
<p>What I do actually to do this is that I separate my text in multiple groups, groups of one word and two words like so:
[The, brownw, focx, and, the, cat, are, in, th, eforest, The brownw, brownw focx, focx and, and the, the cat, cat are, are in, in th, th eforest]</p>
<p>Than I iterate over each group of word and check with the Levensthein algorithm how much the two strings match with each other. In case it's more than 90% I consider they are the same.</p>
<p>This approach however is very time consuming and I wonder if I can find an alternative to this.</p>
"
"70130957","How to handle errors from IBM Watson when iterating over rows","2021-11-26 23:32:09","0","31","1","1","","70131406","<p>I am a student working on a project using IBM Watson's NLU to parse through various News articles and return a sentiment score.  I have the articles in a table, and I have a loop set up to go through each cell in the first column, analyze it, normalize it, and append the new data to the table.</p>
<pre><code>masterdf = pd.DataFrame()

for index, row in df.iterrows():
    text2 = row['CONTENT']
    response = natural_language_understanding.analyze(
        text = text2,
        features=Features(sentiment=SentimentOptions(targets=[&quot;Irish&quot;,]))).get_result()
    json_tbl = pd.json_normalize(response['sentiment'], 
                       record_path='targets',
                       meta=[['document','score'], ['document','label']])
    json_tbl = json_tbl.set_index([pd.Index([index])])
    print(json_tbl.head())
    masterdf = masterdf.append(json_tbl)

masterdf = pd.concat([df, masterdf], axis=1)
masterdf.head()
</code></pre>
<p>The issue that I am having is that sometimes the entity that I am targeting isn't in the article I am analyzing, and so IBM throws an error.  This completely breaks my code.  What I would like to do is that whenever IBM returns an error, my code just fills in the row with &quot;N/A&quot; and progresses to the next cell below it.  I am really a beginner so any help would be really really appreciated.</p>
"
"70108900","Applying RAND index with cluster numbers and cluster labels","2021-11-25 09:45:51","0","980","4","1","","70109775","<p>I have a set of reviews and I've clustered them with k-means and got the clusters each review belongs to (Ex: 1,2,3...). I also have the real labels of which clusters these belongs to Ex: location, food etc.) and I need to compare them with Rand index.</p>
<p>As I have cluster numbers and cluster labels how I can I apply Rand index to compare?</p>
<p>Is there any intermediate step that I should follow?</p>
<p>Edit:
I've seen the post <a href=""https://stackoverflow.com/questions/49586742/rand-index-function-clustering-performance-evaluation"">Rand Index function (clustering performance evaluation)</a> but it does not answer my question.</p>
<p>In that question, you have</p>
<pre><code>labels_true = [1, 1, 0, 0, 0, 0]
labels_pred = [0, 0, 0, 1, 0, 1]
</code></pre>
<p>but what I have is something like below,</p>
<pre><code>labels_true = ['food', 'view', 'room', 'food', 'staff', 'staff']
labels_pred = [0, 0, 0, 1, 0, 1]
</code></pre>
<p>Any help is highly appreciated.</p>
"
"70064477","How to handle LemmatizerTrainer 'UTFDataFormatException: encoded string too long'?","2021-11-22 10:41:54","1","401","3","1","","74253692","<p>I am using Opennlp to train a model for lemmatization of german words. Therefore I use the opennlp cli and the training set of <a href=""https://github.com/UniversalDependencies/UD_German-HDT/blob/master/README.md"" rel=""nofollow noreferrer"">UD_German-HDT</a> which can be downloaded <a href=""https://universaldependencies.org/#download"" rel=""nofollow noreferrer"">here</a></p>
<p>The training itself works fine (just need a little bit of ram) but the cli fails to write the model because of an <code>UTFDataFormatException: encoded string too long</code> exception.</p>
<p>The cli command I am using: <code>opennlp LemmatizerTrainerME.conllu -params params.txt -lang de -model de-lemmatizer.bin -data UD_German-HDT/de_hdt-ud-train.conllu -encoding UTF-8</code></p>
<p>Stacktrace:</p>
<pre><code>Writing lemmatizer model ... failed
Error during writing model file 'de-lemmatizer.bin'
encoded string too long: 383769 bytes
java.io.UTFDataFormatException: encoded string too long: 383769 bytes
        at java.base/java.io.DataOutputStream.writeUTF(DataOutputStream.java:364)
        at java.base/java.io.DataOutputStream.writeUTF(DataOutputStream.java:323)
        at opennlp.tools.ml.maxent.io.BinaryGISModelWriter.writeUTF(BinaryGISModelWriter.java:71)
        at opennlp.tools.ml.maxent.io.GISModelWriter.persist(GISModelWriter.java:97)
        at opennlp.tools.ml.model.GenericModelWriter.persist(GenericModelWriter.java:75)
        at opennlp.tools.util.model.ModelUtil.writeModel(ModelUtil.java:71)
        at opennlp.tools.util.model.GenericModelSerializer.serialize(GenericModelSerializer.java:36)
        at opennlp.tools.util.model.GenericModelSerializer.serialize(GenericModelSerializer.java:29)
        at opennlp.tools.util.model.BaseModel.serialize(BaseModel.java:597)
        at opennlp.tools.cmdline.CmdLineUtil.writeModel(CmdLineUtil.java:182)
        at opennlp.tools.cmdline.lemmatizer.LemmatizerTrainerTool.run(LemmatizerTrainerTool.java:77)
        at opennlp.tools.cmdline.CLI.main(CLI.java:256)
</code></pre>
<p>Has somebody encountered this problem and has a solution?</p>
"
"69988135","Creating a Neural Machine Translation basics","2021-11-16 11:16:45","2","180","3","1","","71001973","<p>I'm currently working on a project design where I will create a program/model to translate my native dialect to English, I'm asking is there any books or anything that can you recommend to me in creating my project.</p>
"
"69883861","Incorrect entity being returned by EntityLinker Spacy","2021-11-08 12:58:53","2","919","0","1","","69893105","<p>I have been training an entity linker with Spacy which has 6,000 entities from Wikidata.</p>
<p>The training data contains of 30,000 sentences.</p>
<p>I'm following the notebook provided by Spacy <a href=""https://github.com/explosion/projects/blob/v3/tutorials/nel_emerson/notebooks/notebook_video.ipynb"" rel=""nofollow noreferrer"">https://github.com/explosion/projects/blob/v3/tutorials/nel_emerson/notebooks/notebook_video.ipynb</a></p>
<p>The training goes fine and the accuracy seems pretty good, until I test the model out on a string that's clearly incorrect. Such &quot;barack obama is a French born florist living in Spain with 36 cats and two hamsters&quot;, but the model predicts the person in this string as <a href=""https://www.wikidata.org/wiki/Q76"" rel=""nofollow noreferrer"">https://www.wikidata.org/wiki/Q76</a></p>
<p>I've tried adding additional parameters into the config, such as <code>n_sents</code></p>
<p><code> entity_linker = nlp.add_pipe(&quot;entity_linker&quot;, config={&quot;incl_prior&quot;: False, &quot;n_sents&quot;: 6}, last=True)</code></p>
<p>Is there a way to improve this? it would be better to return NIL instead of a wrong answer. Or is there a confidence score than can be output?</p>
"
"69861444","TypeError: ""hypothesis"" expects pre-tokenized hypothesis (Iterable[str]):","2021-11-06 04:26:19","0","1821","0","2","","69861501","<p>I am trying to calculate the Meteor score for the following:</p>
<pre><code>print (nltk.translate.meteor_score.meteor_score(
    [&quot;this is an apple&quot;, &quot;that is an apple&quot;], &quot;an apple on this tree&quot;))
</code></pre>
<p>However I am getting this error every time and I am not sure how to fix it.</p>
<pre><code>TypeError: &quot;hypothesis&quot; expects pre-tokenized hypothesis (Iterable[str]): an apple on this tree
</code></pre>
<p>I also tried to put &quot;an apple on this tree&quot; in a list</p>
<pre><code>    from nltk.translate.meteor_score import meteor_score
import nltk 
print (nltk.translate.meteor_score.meteor_score(
    [&quot;this is an apple&quot;, &quot;that is an apple&quot;], [&quot;an apple on this tree&quot;]))
</code></pre>
<p>but it gave me this error.</p>
<pre><code>TypeError: &quot;reference&quot; expects pre-tokenized reference (Iterable[str]): this is an apple
</code></pre>
"
"69779095","Is there a way to apply Spacy en_core_web_sm to data in chunks?","2021-10-30 11:14:48","2","386","0","1","","69783986","<p>I've got this huge dataset of 300.000 articles and I wanted to use Spacy's en_core_web_sm to do Tokenization, POS tagging, lemmatization, syntactic dependencies and NER. However my pc keeps running out of RAM. Is there a way in which I can change my code to process the data in chunks?</p>
<p>This is the dataset: <a href=""https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/ULHLCB"" rel=""nofollow noreferrer"">https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/ULHLCB</a></p>
<p>This is what I;m using:</p>
<pre><code>df_2018 = pd.read_csv(&quot;2018_articles.csv&quot;)
import spacy
nlp_spacy_core_web_sm = spacy.load(&quot;en_core_web_sm&quot;)
df_18_salon[&quot;spacy_sm&quot;] = df_18_salon[&quot;content&quot;].apply(lambda x: nlp_spacy_core_web_sm(x))
</code></pre>
<p>After about 30 minutes I get a out of memory error.</p>
"
"69628487","How to get SHAP values for Huggingface Transformer Model Prediction [Zero-Shot Classification]?","2021-10-19 09:39:46","9","4762","0","2","","69683069","<p>Given a Zero-Shot Classification Task via Huggingface as follows:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import pipeline
classifier = pipeline(&quot;zero-shot-classification&quot;, model=&quot;facebook/bart-large-mnli&quot;)

example_text = &quot;This is an example text about snowflakes in the summer&quot;
labels = [&quot;weather&quot;, &quot;sports&quot;, &quot;computer industry&quot;]
        
output = classifier(example_text, labels, multi_label=True)
output 
{'sequence': 'This is an example text about snowflakes in the summer',
'labels': ['weather', 'sports'],
'scores': [0.9780895709991455, 0.021910419687628746]}
</code></pre>
<p>I am trying to extract the SHAP values to generate a text-based explanation for the prediction result like shown here: <a href=""https://colab.research.google.com/github/ml6team/quick-tips/blob/main/nlp/2021_04_22_shap_for_huggingface_transformers/explainable_transformers_using_shap.ipynb#scrollTo=Ocd9majYrupz"" rel=""noreferrer"">SHAP for Transformers</a></p>
<p>I already tried the following based on the above url:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoModelForSequenceClassification, AutoTokenizer, ZeroShotClassificationPipeline

model = AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli')
tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-mnli')

pipe = ZeroShotClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=True)

def score_and_visualize(text):
    prediction = pipe([text])
    print(prediction[0])

    explainer = shap.Explainer(pipe)
    shap_values = explainer([text])

    shap.plots.text(shap_values)

score_and_visualize(example_text)
</code></pre>
<p><strong>Any suggestions? Thanks for your help in advance!</strong></p>
<p>Alternatively to the above pipeline the following also works:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoModelForSequenceClassification, AutoTokenizer, ZeroShotClassificationPipeline

model = AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli')
tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-mnli')

classifier = ZeroShotClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=True)

example_text = &quot;This is an example text about snowflakes in the summer&quot;
labels = [&quot;weather&quot;, &quot;sports&quot;]

output = classifier(example_text, labels)
output 
{'sequence': 'This is an example text about snowflakes in the summer',
'labels': ['weather', 'sports'],
'scores': [0.9780895709991455, 0.021910419687628746]}
</code></pre>
"
"69520218","How to get the percentage of documents that contain a feature(s)","2021-10-11 01:43:03","0","87","1","1","","69521809","<p>I'm using this solution(<a href=""https://stackoverflow.com/questions/58486577/get-what-percent-of-documents-contain-a-feature-quanteda"">get what percent of documents contain a feature - quanteda</a>) to find the number of documents that contain any one of a group of features in my dataset. As long as the document contains any one of the words, I want it to return TRUE.</p>
<p>I got it to work, but it only works some of the time and I can't figure out why. Removing or adding words works sometimes and not at other times. This is the code I used (the compound phrases have already been &quot;tokens_compound&quot; in the dfm)</p>
<pre><code>thetarget &lt;- c(&quot;testing&quot;, &quot;test&quot;, &quot;example words&quot;, &quot;example&quot;)

df &lt;- data.frame(docname = docnames(dfm),
                 Year = docvars(dfm, c(&quot;Year&quot;)),
                 contains_target = rowSums(dfm[, thetarget]) &gt; 0,
                 row.names = NULL)
</code></pre>
<p>And the error I get sometimes</p>
<pre><code>Error in h(simpleError(msg, call)) : 
  error in evaluating the argument 'x' in selecting a method for function 'rowSums': 
Subscript out of bounds
</code></pre>
<p>TIA</p>
<p>edit (script to create table showing a year and number of documents containing any of the target words):</p>
<pre><code> df2 &lt;- df %&gt;%
  mutate_if(is.logical, as.character) %&gt;%
  filter(!str_detect(contains_target, &quot;FALSE&quot;)) %&gt;%
  group_by(Year) %&gt;%
    summarise(n = n())        
</code></pre>
"
"69433514","Test Intel Extension for Pytorch(IPEX) in multiple-choice from huggingface / transformers","2021-10-04 09:10:05","2","492","0","1","","69610933","<p>I am trying out one huggingface sample with SWAG dataset
<a href=""https://github.com/huggingface/transformers/tree/master/examples/pytorch/multiple-choice"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/tree/master/examples/pytorch/multiple-choice</a></p>
<p>I would like to use Intel Extension for Pytorch in my code to increase the performance.</p>
<p>Here I am using the one without training (run_swag_no_trainer)</p>
<p>In the run_swag_no_trainer.py , I made some changes to use ipex .
#Code before changing is given below:</p>
<pre><code>device = accelerator.device
model.to(device)
</code></pre>
<p>#After adding ipex:</p>
<pre><code>import intel_pytorch_extension as ipex
    device = ipex.DEVICE
    model.to(device)
</code></pre>
<p>While running the below command, its taking too much time.</p>
<pre><code>export DATASET_NAME=swag

accelerate launch run_swag_no_trainer.py \
  --model_name_or_path bert-base-cased \
  --dataset_name $DATASET_NAME \
  --max_seq_length 128 \
  --per_device_train_batch_size 32 \
  --learning_rate 2e-5 \
  --num_train_epochs 3 \
  --output_dir /tmp/$DATASET_NAME/
</code></pre>
<p>Is there any other method to test the same on intel ipex?</p>
"
"69426006","Tensorflow ""Transformer model for language understanding"" with another Dataset?","2021-10-03 14:54:48","4","677","0","2","","69532366","<p>I have been reading the official guide here (<a href=""https://www.tensorflow.org/text/tutorials/transformer"" rel=""nofollow noreferrer"">https://www.tensorflow.org/text/tutorials/transformer</a>) to try and recreate the Vanilla Transformer in Tensorflow. I notice the dataset used is quite specific, and at the end of the guide, it says to try with a different dataset.</p>
<p>But that is where I have been stuck for a long time! I am trying to use the WMT14 dataset (as used in the original paper, Vaswani et. al.) here: <a href=""https://www.tensorflow.org/datasets/catalog/wmt14_translate#wmt14_translatede-en"" rel=""nofollow noreferrer"">https://www.tensorflow.org/datasets/catalog/wmt14_translate#wmt14_translatede-en</a> .</p>
<p>I have also tried Multi30k and IWSLT dataset from Spacy, but are there any guides on how I can fit the dataset to what the model requires? Specifically, to tokenize it. The official TF guide uses a pretrained tokenizer, which is specific to the PR-EN dataset given.</p>
<pre><code>model_name = &quot;ted_hrlr_translate_pt_en_converter&quot;
</code></pre>
<p>I am wondering, how I can use the TF (bert) tokenizer to tokenize the Spacy dataset? I have the code for PyTorch, unfortunately I do not know how to adapt it for Tensorflow. Any help would be greatly appreciated!</p>
<pre><code>import spacy

spacy_de = spacy.load('de')
spacy_en = spacy.load('en')

def tokenize_de(text):
    return [tok.text for tok in spacy_de.tokenizer(text)]

def tokenize_en(text):
    return [tok.text for tok in spacy_en.tokenizer(text)]

BOS_WORD = '&lt;s&gt;'
EOS_WORD = '&lt;/s&gt;'
BLANK_WORD = &quot;&lt;blank&gt;&quot;
SRC = data.Field(tokenize=tokenize_de, pad_token=BLANK_WORD)
TGT = data.Field(tokenize=tokenize_en, init_token = BOS_WORD, 
                 eos_token = EOS_WORD, pad_token=BLANK_WORD)

MAX_LEN = 100
train, val, test = datasets.IWSLT.splits(
    exts=('.de', '.en'), fields=(SRC, TGT), 
    filter_pred=lambda x: len(vars(x)['src']) &lt;= MAX_LEN and 
        len(vars(x)['trg']) &lt;= MAX_LEN)
MIN_FREQ = 2
SRC.build_vocab(train.src, min_freq=MIN_FREQ)
TGT.build_vocab(train.trg, min_freq=MIN_FREQ)
</code></pre>
"
"69327798","How to use ExceptT to replace lots of IO (Either a b)","2021-09-25 15:54:15","4","1368","3","2","","69328310","<p>I have a function which connects to a database, then runs a query. Each of these steps results in <code>IO (Either SomeErrorType SomeResultType)</code>.</p>
<p>One of the things I have really liked about using <code>Either</code> and similar monads in learning Haskell has been the ability to use the monad functions like <code>&gt;&gt;=</code> and combinators like <code>mapLeft</code> to streamline a  lot of the handling of expected error states.</p>
<p>My expectation here from reading blog posts, the <code>Control.Monad.Trans</code> documentation, and other answers on SO is that I have to somehow use transformers / lift to move from the <code>IO</code> context to the <code>Either</code> context.</p>
<p><a href=""https://stackoverflow.com/a/52016583/907981"">This answer</a> in particular is really good, but I'm struggling to apply it to my own case.</p>
<p>A simpler example of my code:</p>
<pre><code>simpleVersion :: Integer -&gt; Config -&gt; IO ()
simpleVersion id c = 
  connect c &gt;&gt;= \case 
      (Left e)     -&gt; printErrorAndExit e
      (Right conn) -&gt; (run . query id $ conn)
              &gt;&gt;= \case 
                    (Left e)  -&gt; printErrorAndExit e
                    (Right r) -&gt; print r
                                   &gt;&gt; release conn
</code></pre>
<p>My problem is that (a) I'm not really understanding the mechanics of how <code>ExceptT</code> gets me to a similar place to the <code>mapLeft handleErrors $ eitherErrorOrResult &gt;&gt;= someOtherErrorOrResult &gt;&gt;= print</code> world; (b) I'm not sure how to ensure that the connection is always released in the nicest way (even in my simple example above), although I suppose I would use the <a href=""https://wiki.haskell.org/Bracket_pattern"" rel=""nofollow noreferrer"">bracket pattern</a>.</p>
<p>I'm sure every (relatively) new Haskeller says this but I still really don't understand monad transformers and everything I read (except aforelinked SO answer) is too opaque for me (yet).</p>
<p>How can I transform the code above into something which removes all this nesting and error handling?</p>
"
"69253108","Get a tag list from pos tagging","2021-09-20 10:42:03","-2","227","1","1","","69253311","<p>Currently, I am working on an NLP project, and after applying pos tagging, I have received the below output.</p>
<blockquote>
<pre><code>[[(ද්විපාර්ශවික, NNP), (එකඟතා, NNP), (ජන, JJ), (ජීවිත, NNJ), (සෞඛ්යය, NNC), (මනාව, RB)]]
</code></pre>
</blockquote>
<p>for my work, I need to retrieve tags, like this.</p>
<pre><code>&gt; pos_tag_list = [['NNP', 'NNP', 'JJ', 'NNJ', 'NNC', 'RB']]
</code></pre>
"
"69218494","Pyspark - Display Top 10 words of document","2021-09-17 06:07:28","1","1292","0","1","","69223273","<p>I'm quite new to Pyspark and did a tfidf processing on a dataframe with the following code</p>
<pre><code>from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer
from pyspark.sql.types import *
from pyspark.sql.functions import udf
wordsData = spark.createDataFrame([(0,&quot;Hello&quot;,&quot;World&quot;,&quot;Spark&quot;,&quot;Is&quot;,&quot;Awesome&quot;,&quot;Hello&quot;,&quot;World&quot;]),(1,[&quot;Hello&quot;,&quot;World&quot;,&quot;Spark&quot;,&quot;Is&quot;,&quot;Awesome&quot;,&quot;Hello&quot;,&quot;World&quot;]),(2,[&quot;Hello&quot;,&quot;World&quot;]),(3,[&quot;PYTHON&quot;, &quot;Is&quot;, &quot;Pretty&quot;, &quot;Awesome&quot;])],[&quot;label&quot;,&quot;words&quot;])

#hashingTF way
hashingTF = HashingTF(inputCol=&quot;words&quot;, outputCol=&quot;rawFeatures&quot;)
featurizedData = hashingTF.transform(wordsData)
idf = IDF(inputCol=&quot;rawFeatures&quot;, outputCol=&quot;features&quot;)
idfModel = idf.fit(featurizedData)
rescaledData = idfModel.transform(featurizedData)
rescaledData.show(truncate=False)
print(hashingTF.indexOf(&quot;PYTHON&quot;))
</code></pre>
<p>Now I want to store the Top 10 words with their tfidf value in a separate column. But since I'm not really used to working with vectors I'm a little stuck on how to achieve this. I know I somehow need to apply the indexOf function to every token of a document to find a mapping to its value, but I don't know how to do it. As far as I understood each vector is built up like this: (Size,[Key],[Value])</p>
<p>I was also thinking of using the CounteVectorizer way (and for that using its vocabulary) but I run in the same problem there.</p>
<p>Anyone who can help?
This is the output so far:</p>
<pre><code>+-----+------------------------------------------------+----------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------+
|label|words                                           |rawFeatures                                                     |features                                                                                                                                       |
+-----+------------------------------------------------+----------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------+
|0    |[Hello, World, Spark, Is, Awesome, Hello, World]|(262144,[32755,44691,64441,179674,262052],[2.0,2.0,1.0,1.0,1.0])|(262144,[32755,44691,64441,179674,262052],[0.44628710262841953,0.44628710262841953,0.22314355131420976,0.5108256237659907,0.22314355131420976])|
|1    |[Hello, World, Spark, Is, Awesome, Hello, World]|(262144,[32755,44691,64441,179674,262052],[2.0,2.0,1.0,1.0,1.0])|(262144,[32755,44691,64441,179674,262052],[0.44628710262841953,0.44628710262841953,0.22314355131420976,0.5108256237659907,0.22314355131420976])|
|2    |[Hello, World]                                  |(262144,[32755,44691],[1.0,1.0])                                |(262144,[32755,44691],[0.22314355131420976,0.22314355131420976])                                                                               |
|3    |[PYTHON, Is, Pretty, Awesome]                   |(262144,[61511,64441,191247,262052],[1.0,1.0,1.0,1.0])          |(262144,[61511,64441,191247,262052],[0.9162907318741551,0.22314355131420976,0.9162907318741551,0.22314355131420976])                           |
+-----+------------------------------------------------+----------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------+
</code></pre>
"
"69217515","Run dependency parser on pre-initialized doc object of spacy","2021-09-17 03:42:43","1","245","1","1","","69219128","<p>I am trying to incorporate spacy's dependency parser into a legacy code in java through web API.</p>
<p>All other components tokenizer, tagger, merged_words, NER are done from the legacy NLP code. I am only interested to apply the dependency parser along with the dependency rule matcher of spacy 3.</p>
<p>I have tried the following approach</p>
<ol>
<li>creating a new doc object using <a href=""https://spacy.io/api/doc#init"" rel=""nofollow noreferrer"">https://spacy.io/api/doc#init</a>.</li>
</ol>
<pre class=""lang-py3 prettyprint-override""><code>from spacy.tokens import Doc
sent=[&quot;The heating_temperature was found to be 500 C&quot;]
words=[&quot;The&quot;,&quot;heating_temperature&quot;, &quot;was&quot;, &quot;found&quot;, &quot;to&quot;, &quot;be&quot;, &quot;500&quot;, &quot;C&quot;]
spaces=[True,True,True,True,True,True,True,False]
tags=[&quot;DT&quot;,&quot;NN&quot;,&quot;VBD&quot;,&quot;VBN&quot;,&quot;TO&quot;,&quot;VB&quot;,&quot;CD&quot;,&quot;NN&quot;]
ents=[&quot;O&quot;,&quot;I-PARAMETER&quot;,&quot;O&quot;,&quot;O&quot;,&quot;O&quot;,&quot;O&quot;,&quot;I-VALUE&quot;,&quot;O&quot;]
doc = Doc(nlp.vocab, words=words,spaces=spaces, tags=tags, ents=ents)
</code></pre>
<ol start=""2"">
<li>Create an NLP pipeline with only parser</li>
</ol>
<pre class=""lang-py3 prettyprint-override""><code>#can use nlp.blank too
nlp2 = spacy.load(&quot;en_core_web_sm&quot;, exclude=['attribute_ruler', 'lemmatizer', 'ner', &quot;parser&quot;,&quot;tagger&quot;])
pipeWithParser = nlp2.add_pipe(&quot;parser&quot;, source=spacy.load(&quot;en_core_web_sm&quot;))
processed_dep = pipeWithParser(doc) #refer similar example in https://spacy.io/api/tagger#call
</code></pre>
<p>However, I am getting the following dependency tree</p>
<p><a href=""https://i.sstatic.net/vNRjz.jpg"" rel=""nofollow noreferrer"">dependency tree</a></p>
<p>where every word is an nmod relation to the first word.</p>
<p>What am I missing?
I could use the tagger of spacy too if req. I tried including tagger using above similar method but all tags were labeled 'NN'</p>
"
"69215446","How can I use Ensemble learning of two models with different features as an input?","2021-09-16 21:35:36","1","864","0","1","","69218787","<p>I have a fake news detection problem and it predicts the binary labels &quot;1&quot;&amp;&quot;0&quot; by vectorizing the 'tweet' column, I use three different models for detection but I want to use the ensemble method to increase the accuracy but they use different vectorezer.</p>
<blockquote>
<p>I have 3 KNN models the first and the second one vectorizes the 'tweet' column using TF-IDF.</p>
</blockquote>
<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
    vector = TfidfVectorizer(max_features =5000, ngram_range=(1,3))
    X_train = vector.fit_transform(X_train['tweet']).toarray()
    X_test = vector.fit_transform(X_test['tweet']).toarray()
</code></pre>
<blockquote>
<p>for the third model I used fastText for sentence vectorization</p>
</blockquote>
<pre><code>%%time
sent_vec = []
for index, row in X_train.iterrows():
    sent_vec.append(avg_feature_vector(row['tweet']))
%%time
sent_vec1 = []
for index, row in X_test.iterrows():
    sent_vec1.append(avg_feature_vector(row['tweet']))
</code></pre>
<blockquote>
<p>after scaling and... my third model fits the input like this</p>
</blockquote>
<pre><code>scaler.fit(sent_vec)
scaled_X_train= scaler.transform(sent_vec)
scaled_X_test= scaler.transform(sent_vec1)
.
.
.
knn_model1.fit(scaled_X_train, y_train)
</code></pre>
<blockquote>
<p>now I want to combine the three models like this and I want the ensemble method to give me the majority just like<code>VotingClassifier</code>, but I have no idea how can I deal with the different inputs (<strong>TF-IDF &amp; fastText</strong>) is there another way to do that?</p>
</blockquote>
"
"69189754","How to find whether a sentence contain a noun using spacy?","2021-09-15 08:38:46","4","829","0","1","","69189976","<p>Currently doing a project in NLP. I need to find out whether a sentence have a noun in it. How can I achieve this using spacy?</p>
"
"69125678","Surprising results for German lemmatization in Spacy","2021-09-09 23:22:58","4","755","0","1","","69130377","<p>I wanted to use the lemmatizer for German in Spacy, but I am very surprised by the results:</p>
<pre><code>import spacy

nlp = spacy.load(&quot;de_dep_news_trf&quot;)
[token.lemma_ for token in nlp('ich du er sie mein dein sein ihr unser')]
</code></pre>
<p>gives</p>
<pre><code>['ich', 'du', 'ich', 'ich', 'meinen', 'mein', 'mein', 'mein', 'sich']
</code></pre>
<p>and I am not sure I can use that:</p>
<pre><code>vielen dank für deinen sehr guten tweet
</code></pre>
<p>becomes</p>
<pre><code>viel danken für mein sehr gut tweet
</code></pre>
<p>which clearly changes the meaning of the sentence.</p>
<p>Is that expected? Am I missing a tuning/configuration that would make that lemmatizer less &quot;aggressive&quot;?</p>
"
"69006922","Using RTX3090 with Haystack","2021-09-01 01:46:07","1","361","1","1","","69158746","<p>The current release of Haystack (deepset.ai) supports GPU's but it is not using a new enough CUDA support (&gt;= 11.1) to work with my RTX3090.  Can I just uninstall the current version of torch and reinstall with cu111 support?  Or are other components also tied to an earlier version of the CUDA library?</p>
"
"68971791","Using Gensin Word2Vec to improve search","2021-08-29 09:58:03","0","74","0","1","","68975633","<p>I have a dataset off millions of arrays like follows:</p>
<pre><code>  sentences=[
    [
     'query_foo bar',
     'split_query_foo',
     'split_query_bar',
     'sku_qwre',
     'brand_A B C',
     'split_brand_A',
     'split_brand_B',
     'split_brand_C',
     'color_black',
     'category_C1',
     'product_group_clothing',
     'silhouette_t_shirt_top',
  ],
  [...]
  ]
</code></pre>
<p>where you find a query, a sku that was acquired by the user doing the query and a few attributes of the SKU. My idea was to do a very basic model based on word2vec where I could find similar things together.</p>
<p>In a simple way, if I search for <code>t-shirt</code> on the model I would expect to have t-shirt SKUs near the query.</p>
<p>I try to use gensim (I'm new to this library) with different attributes to build a model:</p>
<pre><code>from gensim.models.callbacks import CallbackAny2Vec

class callback(CallbackAny2Vec):
    '''Callback to print loss after each epoch.'''

    def __init__(self):
        self.epoch = 0
        self.loss_to_be_subed = 0

    def on_epoch_end(self, model):
        loss = model.get_latest_training_loss()
        loss_now = loss - self.loss_to_be_subed
        self.loss_to_be_subed = loss
        print('Loss after epoch {}: {}'.format(self.epoch, loss_now))
        self.epoch += 1

model = Word2Vec(
  sentences=sentences, 
  vector_size=100, 
  window=1000, 
  min_count=2, 
  workers=-1,
  epochs=10,
#   negative=5,
  compute_loss=True,
  callbacks=[callback()]
)
</code></pre>
<p>I got this output:</p>
<pre><code>Loss after epoch 0: 0.0
Loss after epoch 1: 0.0
Loss after epoch 2: 0.0
Loss after epoch 3: 0.0
Loss after epoch 4: 0.0
Loss after epoch 5: 0.0
Loss after epoch 6: 0.0
Loss after epoch 7: 0.0
Loss after epoch 8: 0.0
Loss after epoch 9: 0.0
</code></pre>
<p>All losses of 0!!!
I start to get very suspicious at this point.</p>
<p>Note: Each element of <code>sentences</code> are independent, I hop the library don't try to mix different terms in different arrays.</p>
<p>For trying to test the model, I tried a very frequent query like <code>model.wv.most_similar('query_t-shirt', topn=100)</code> and the results are completely absurd.</p>
<p>Is my idea crazy or am I using incorrectly the library?</p>
"
"68928954","I compare two identical sentences with GLEU NLTK and don't get 1.0. Why?","2021-08-25 19:39:04","1","364","0","1","","68929120","<p>I’m trying to use GLEU score from NLTK for quality evaluation of the machine translation. I wanted to check this code with two identical sentences, I’m comparing two sentences and not corpuses. But as a result I’m getting 0.015151515151515152. What am I doing wrong? Two identical sentences should get score 1.0.</p>
<p>My code:</p>
<pre><code>from nltk.translate.gleu_score import sentence_gleu

hyp1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'which', 'ensures', 'that', 'the', 'military', 'always', 'obeys', 'the', 'commands', 'of', 'the', 'party']

ref1a = ['It', 'is', 'a', 'guide', 'to', 'action', 'which', 'ensures', 'that', 'the', 'military', 'always', 'obeys', 'the', 'commands', 'of', 'the', 'party']

gleu_score = sentence_gleu(ref1a, hyp1)

print(gleu_score)
</code></pre>
<p>My result:</p>
<p>0.015151515151515152</p>
<p>Process finished with exit code 0</p>
<p>Am I mistaken? Please help!</p>
"
"68927809","I compare two identical sentences with RIBES NLTK and get an error. Why?","2021-08-25 17:59:35","0","163","0","1","","68931168","<p>I’m trying to use RIBES score from NLTK for quality evaluation of the machine translation. I wanted to check this code with two identical sentences. But when I’m running my code I get errors.</p>
<p>My code:</p>
<pre><code>from nltk.translate.ribes_score import sentence_ribes

hyp1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'which', 'ensures', 'that', 'the', 'military', 'always', 'obeys', 'the', 'commands', 'of', 'the', 'party']

ref1a = ['It', 'is', 'a', 'guide', 'to', 'action', 'which', 'ensures', 'that', 'the', 'military', 'always', 'obeys', 'the', 'commands', 'of', 'the', 'party']

ribes_score = sentence_ribes(ref1a, hyp1)

print(ribes_score)
 
</code></pre>
<p>Errors:</p>
<pre><code>Traceback (most recent call last):

  File &quot;D:/Users/anastasia.emelyanova/PycharmProjects/Metrics_NLTK/ribes_test.py&quot;, line 4, in &lt;module&gt;

    ribes_score = sentence_ribes(ref1a, hyp1)

  File &quot;D:\Users\anastasia.emelyanova\AppData\Local\Programs\Python\Python38\lib\site-packages\nltk\translate\ribes_score.py&quot;, line 55, in sentence_ribes

    nkt = kendall_tau(worder)

  File &quot;D:\Users\anastasia.emelyanova\AppData\Local\Programs\Python\Python38\lib\site-packages\nltk\translate\ribes_score.py&quot;, line 290, in kendall_tau

    tau = 2 * num_increasing_pairs / num_possible_pairs - 1

ZeroDivisionError: division by zero


Process finished with exit code 1
</code></pre>
<p>Why I’m getting these errors? Am I mistaken? I just took two identical sentences and there shouldn’t be division by zero, because numbers of possible pairs should be more than 1. Two identical sentences should get a score 1.0. I'm coding on Python 3, Windows 7, in PyCharm. Please help!</p>
"
"68901673","count the occurrences of POS tagging pattern","2021-08-24 04:08:45","0","358","0","1","","68901816","<p>So I've applied POS tagging to one of the columns in my dataframe. For each sentence, I want to count the occurrences of this pattern: NNP, MD, VB.</p>
<p>For example, I have the following sentence:
communications between the Principal and the Contractor shall be in the English language</p>
<p>The POS tagging would be:
(communications, NNS), (between,IN), (the, DT), (Principal, NNP), (and, CC), (the, DT), <strong>(Contractor, NNP), (shall, MD), (be,VB)</strong>, (in, DT), (the, DT), (English, JJ), (language, NN).</p>
<p>Notice that in the POS tagging result, the pattern (NNP, MD, VB) exists and occurs 1 time. I'd like to create a new column in the df for this number of occurrences.</p>
<p>Any ideas how I can do this?</p>
<p>Thanks in advance</p>
"
"68872771","Workflow of NLP","2021-08-21 11:54:04","1","134","0","1","","68878734","<p>When should I perform preprocessing and matrix creation of text data in NLP, before or after <code>train_test_split</code>? Below is my sample code where I have done preprocessing and matrix creation (tfidf) before <code>train_test_split</code>. I want to know will there be data leakage?</p>
<pre><code>corpus = []

for i in range(0 ,len(data1)):
    review = re.sub('[^a-zA-Z]', ' ', data1['features'][i])
    review = review.lower()
    review = review.split()
    review = [stemmer.stem(j) for j in review if not j in set(stopwords.words('english'))]
    review = ' '.join(review)
    corpus.append(review)

from sklearn.feature_extraction.text import TfidfVectorizer
cv = TfidfVectorizer(max_features = 6000)
x = cv.fit_transform(corpus).toarray()

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y = le.fit_transform(data1['label'])

from sklearn.model_selection import train_test_split
train_x, test_x, train_y, test_y = train_test_split(x, y, test_size = 0.2, random_state = 69, 
                                                                                stratify = y)

spam_model = MultinomialNB().fit(train_x, train_y)
pred = spam_model.predict(test_x)
c_matrix = confusion_matrix(test_y, pred)
acc_score = accuracy_score(test_y, pred)
</code></pre>
"
"68817989","Bert model output interpretation","2021-08-17 13:11:18","1","1240","0","2","","68819822","<p>I searched a lot for this but havent still got a clear idea so I hope you can help me out:</p>
<p>I am trying to translate german texts to english! I udes this code:</p>
<pre><code>
tokenizer = AutoTokenizer.from_pretrained(&quot;Helsinki-NLP/opus-mt-de-en&quot;)
model = AutoModelForSeq2SeqLM.from_pretrained(&quot;Helsinki-NLP/opus-mt-de-en&quot;)

batch = tokenizer(
    list(data_bert[:100]),
    padding=True,
    truncation=True,
    max_length=250,
    return_tensors=&quot;pt&quot;)[&quot;input_ids&quot;]

results = model(batch)  
</code></pre>
<p>Which returned me a size error! I fixed this problem (thanks to the community: <a href=""https://github.com/huggingface/transformers/issues/5480"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/issues/5480</a>) with switching the last line of code to:</p>
<pre><code>results = model(input_ids = batch,decoder_input_ids=batch)
</code></pre>
<p>Now my output looks like a really long array. What is this output precisely? Are these some sort of word embeddings? And if yes: How shall I go on with converting these embeddings to the texts in the english language? Thanks alot!</p>
"
"68813979","Bert Transformer ""Size Error"" while Machine Traslation","2021-08-17 08:22:37","2","2047","0","2","","68814093","<p>I am getting desperate as I have no clue what is the problem over here. I want to translate a list of sentences from german to english. This is my code:</p>
<pre><code>
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained(&quot;Helsinki-NLP/opus-mt-de-en&quot;)
model = AutoModelForSeq2SeqLM.from_pretrained(&quot;Helsinki-NLP/opus-mt-de-en&quot;)

batch = tokenizer(
    list(data_bert[:100]),
    padding=True,
    truncation=True,
    max_length=250,
    return_tensors=&quot;pt&quot;
)



results = model(batch)
</code></pre>
<p>And I am getting this error:</p>
<pre><code>---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
~/miniconda3/envs/textmallet/lib/python3.9/site-packages/transformers/tokenization_utils_base.py in __getattr__(self, item)
    247         try:
--&gt; 248             return self.data[item]
    249         except KeyError:

KeyError: 'size'

During handling of the above exception, another exception occurred:

AttributeError                            Traceback (most recent call last)
/tmp/ipykernel_26502/2652187977.py in &lt;module&gt;
     14 
     15 
---&gt; 16 results = model(batch)
     17 

~/miniconda3/envs/textmallet/lib/python3.9/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   1049         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1050                 or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1051             return forward_call(*input, **kwargs)
   1052         # Do not call functions when jit is used
   1053         full_backward_hooks, non_full_backward_hooks = [], []

~/miniconda3/envs/textmallet/lib/python3.9/site-packages/transformers/models/marian/modeling_marian.py in forward(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)
   1274                 )
   1275 
-&gt; 1276         outputs = self.model(
   1277             input_ids,
   1278             attention_mask=attention_mask,
</code></pre>
<p>I have no clue what could be the precise issue over here. If someone can help me out I d be really thankful.</p>
"
"68759885","Print input / output / grad / loss at every step/epoch when training Transformers HuggingFace model","2021-08-12 15:02:53","2","2781","0","1","","68819794","<p>I'm working on HuggingFace Transformers and using toy example from here:
<a href=""https://huggingface.co/transformers/custom_datasets.html#fine-tuning-with-trainer"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/custom_datasets.html#fine-tuning-with-trainer</a></p>
<p>What I actually need: ability to print input, output, grad and loss at every step.
It is trivial using Pytorch training loop, but it is not obvious using HuggingFace <code>Trainer</code>.
At the current moment I have next idea: create a <code>CustomCallback</code> like this:</p>
<pre><code>class MyCallback(TrainerCallback):
    &quot;A callback that prints a grad at every step&quot;

    def on_step_begin(self, args, state, control, **kwargs):
        print(&quot;next step&quot;)
        print(kwargs['model'].classifier.out_proj.weight.grad.norm())

args = TrainingArguments(
    output_dir='test_dir',
    overwrite_output_dir=True,
    num_train_epochs=1,
    logging_steps=100,
    report_to=&quot;none&quot;,
    fp16=True,
    disable_tqdm=True,
)


trainer = Trainer(
    model=model,
    args=args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    callbacks=[MyCallback],
)

trainer.train()
</code></pre>
<p>This way I can print grad and weights for any model layer.
But I still can't figure out how to print input/output (for example, I want to check them on <code>nan</code>) and loss?</p>
<p>P.S. I also read something about <code>forward_hook</code> but still can't find good code examples for it.</p>
"
"68739904","Why is my SpaCy v3 scorer returing 0 for precision, recall and f1?","2021-08-11 10:04:48","0","947","0","1","","68740996","<p>I have the following code (migration from SpaCy v2) where I would like to calculate the precision, recall and f1-score for a given model:</p>
<pre><code>nlp = spacy.load(&quot;my_model&quot;)
scorer = Scorer(nlp)
examples = []
for text, annotations in TEST_DATA:
    examples.append(Example.from_dict(nlp.make_doc(text), annotations))
results = scorer.score(examples)
print(
    &quot;Precision {:0.4f}\tRecall {:0.4f}\tF-score {:0.4f}&quot;.format(results['ents_p'], results['ents_r'], results['ents_f'])
)
</code></pre>
<p>The weird thing I try to understand is why it always returns</p>
<pre><code>Precision 0.0000    Recall 0.0000   F-score 0.0000
</code></pre>
<p>My TEST_DATA set is in the same form as the TRAIN_DATA set I was using to train the same model. Here is how it looks like:</p>
<pre><code>[
    (
        'Line 106 – for dilution times, the units should be specified', {'entities': [(51, 60, 'ACTION'), (41, 47, 'MODAL'), (11, 40, 'CONTENT'), (0, 8, 'LOCATION')]}
    ),
    (
        'It should be indicated what test was applied  to verify the normality of distribution.', {'entities': [(13, 22, 'ACTION'), (28, 85, 'CONTENT'), (3, 9, 'MODAL')]}
    )
]
</code></pre>
"
"68738363","Building own classifier based POS tagger using NLTK's SklearnClassifier and ClassifierBasedPOSTagger","2021-08-11 08:17:25","0","243","0","1","","68894824","<p>I'm trying to build my own classifier based POS tagger using <code>SklearnClassifier</code> and <code>ClassifierBasedPOSTagger</code>. The code that I've tried is given below.</p>
<pre><code>from nltk.corpus import treebank
nltk.download('treebank')

data = treebank.tagged_sents()
train_data = data[:3500]
test_data = data[3500:]
</code></pre>
<pre><code>from nltk.classify import SklearnClassifier
from sklearn.naive_bayes import BernoulliNB
from nltk.tag.sequential import ClassifierBasedPOSTagger

bnb = SklearnClassifier(BernoulliNB())
bnb_tagger = ClassifierBasedPOSTagger(train=train_data,
                                      classifier_builder=bnb.train)

# evaluate tagger on test data and sample sentence
print(bnb_tagger.evaluate(test_data))

# see results on our previously defined sentence
print(bnb_tagger.tag(nltk.word_tokenize(sentence)))
</code></pre>
<p>This code is yielding the following error:</p>
<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
C:\Users\ABDULL~1.IMR\AppData\Local\Temp/ipykernel_6580/266992580.py in &lt;module&gt;
      4 
      5 bnb = SklearnClassifier(BernoulliNB())
----&gt; 6 bnb_tagger = ClassifierBasedPOSTagger(train=train_data,
      7                                       classifier_builder=bnb.train)
      8 

~\Miniconda3\envs\nlp_course\lib\site-packages\nltk\tag\sequential.py in __init__(self, feature_detector, train, classifier_builder, classifier, backoff, cutoff_prob, verbose)
    637 
    638         if train:
--&gt; 639             self._train(train, classifier_builder, verbose)
    640 
    641     def choose_tag(self, tokens, index, history):

~\Miniconda3\envs\nlp_course\lib\site-packages\nltk\tag\sequential.py in _train(self, tagged_corpus, classifier_builder, verbose)
    673         if verbose:
    674             print(&quot;Training classifier ({} instances)&quot;.format(len(classifier_corpus)))
--&gt; 675         self._classifier = classifier_builder(classifier_corpus)
    676 
    677     def __repr__(self):

~\Miniconda3\envs\nlp_course\lib\site-packages\nltk\classify\scikitlearn.py in train(self, labeled_featuresets)
    110 
    111         X, y = list(zip(*labeled_featuresets))
--&gt; 112         X = self._vectorizer.fit_transform(X)
    113         y = self._encoder.fit_transform(y)
    114         self._clf.fit(X, y)

~\Miniconda3\envs\nlp_course\lib\site-packages\sklearn\feature_extraction\_dict_vectorizer.py in fit_transform(self, X, y)
    288             Feature vectors; always 2-d.
    289         
--&gt; 290         return self._transform(X, fitting=True)
    291 
    292     def inverse_transform(self, X, dict_type=dict):

~\Miniconda3\envs\nlp_course\lib\site-packages\sklearn\feature_extraction\_dict_vectorizer.py in _transform(self, X, fitting)
    233                     if feature_name in vocab:
    234                         indices.append(vocab[feature_name])
--&gt; 235                         values.append(self.dtype(v))
    236 
    237             indptr.append(len(indices))

TypeError: float() argument must be a string or a number, not 'NoneType'
</code></pre>
<p>How to do it right?</p>
"
"68698065","How can I fix cuda runtime error on google colab?","2021-08-08 05:38:15","1","3773","0","1","","68698418","<p>I'm trying to execute the named entity recognition example using BERT and pytorch following the Hugging Face page: <a href=""https://huggingface.co/transformers/custom_datasets.html#token-classification-with-w-nut-emerging-entities"" rel=""nofollow noreferrer"">Token Classification with W-NUT Emerging Entities</a>.</p>
<p>There was <a href=""http://Huggingface%20BERT%20NER%20Example%20Batch_Size%20error"" rel=""nofollow noreferrer"">a related question</a> on stackoverflow, but the error message is different from my case.</p>
<p><code>cuda runtime error (710) : device-side assert triggered at /pytorch/aten/src/THC/generic/THCTensorMath.cu:29</code></p>
<p><strong>I have trouble with fixing the above cuda runtime error.</strong></p>
<p>How can I execute the sample code on google colab with <strong>the run time type, GPU</strong>?</p>
<h2>Error</h2>
<pre><code>trainer.train()

# Error Message
/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)
    147     Variable._execution_engine.run_backward(
    148         tensors, grad_tensors_, retain_graph, create_graph, inputs,
--&gt; 149         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag
    150 
    151 

RuntimeError: cuda runtime error (710) : device-side assert triggered at /pytorch/aten/src/THC/generic/THCTensorMath.cu:29
</code></pre>
<h2>Code</h2>
<p>I didn't change the original data and code introduced on the tutorial, <a href=""https://huggingface.co/transformers/custom_datasets.html#token-classification-with-w-nut-emerging-entities"" rel=""nofollow noreferrer"">Token Classification with W-NUT Emerging Entities</a>.</p>
<p>Access from the browser to Token Classification with W-NUT Emerging Entities code:
<a href=""https://colab.research.google.com/github/huggingface/notebooks/blob/master/transformers_doc/custom_datasets.ipynb"" rel=""nofollow noreferrer"">custom_datasets.ipynb - Colaboratory</a></p>
<pre><code>from pathlib import Path
import re

def read_wnut(file_path):
    file_path = Path(file_path)

    raw_text = file_path.read_text().strip()
    raw_docs = re.split(r'\n\t?\n', raw_text)
    token_docs = []
    tag_docs = []
    for doc in raw_docs:
        tokens = []
        tags = []
        for line in doc.split('\n'):
            token, tag = line.split('\t')
            tokens.append(token)
            tags.append(tag)
        token_docs.append(tokens)
        tag_docs.append(tags)

    return token_docs, tag_docs

texts, tags = read_wnut('wnut17train.conll')
</code></pre>
<pre><code>from sklearn.model_selection import train_test_split
train_texts, val_texts, train_tags, val_tags = train_test_split(texts, tags, test_size=.2)
unique_tags = set(tag for doc in tags for tag in doc)
tag2id = {tag: id for id, tag in enumerate(unique_tags)}
id2tag = {id: tag for tag, id in tag2id.items()}
</code></pre>
<pre><code>from transformers import DistilBertTokenizerFast
tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased')
train_encodings = tokenizer(train_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)
val_encodings = tokenizer(val_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)
</code></pre>
<pre><code>import numpy as np

def encode_tags(tags, encodings):
    labels = [[tag2id[tag] for tag in doc] for doc in tags]
    encoded_labels = []
    for doc_labels, doc_offset in zip(labels, encodings.offset_mapping):
        # create an empty array of -100
        doc_enc_labels = np.ones(len(doc_offset),dtype=int) * -100
        arr_offset = np.array(doc_offset)

        # set labels whose first offset position is 0 and the second is not 0
        doc_enc_labels[(arr_offset[:,0] == 0) &amp; (arr_offset[:,1] != 0)] = doc_labels
        encoded_labels.append(doc_enc_labels.tolist())

    return encoded_labels

train_labels = encode_tags(train_tags, train_encodings)
val_labels = encode_tags(val_tags, val_encodings)
</code></pre>
<pre><code>import torch
import os

#os.environ['CUDA_LAUNCH_BLOCKING'] = &quot;1&quot;
torch.backends.cudnn.enabled = False
# check if CUDA is available
train_on_gpu = torch.cuda.is_available()
# torch.backends.cudnn.enabled

if not train_on_gpu:
    print('CUDA is not available.  Training on CPU ...')
else:
    print('CUDA is available!  Training on GPU ...')

class WNUTDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

train_encodings.pop(&quot;offset_mapping&quot;) # we don't want to pass this to the model
val_encodings.pop(&quot;offset_mapping&quot;)
train_dataset = WNUTDataset(train_encodings, train_labels)
val_dataset = WNUTDataset(val_encodings, val_labels)
</code></pre>
<pre><code>from transformers import DistilBertForTokenClassification
model = DistilBertForTokenClassification.from_pretrained('distilbert-base-cased', num_labels=len(unique_tags))
</code></pre>
<pre><code>from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments, DistilBertForTokenClassification
from sklearn.metrics import precision_recall_fscore_support
import tensorflow as tf

def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')
    acc = accuracy_score(labels, preds)
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

training_args = TrainingArguments(
    output_dir='./results',          # output directory
    num_train_epochs=3,              # total number of training epochs
    per_device_train_batch_size=16,  # batch size per device during training
    per_device_eval_batch_size=64,   # batch size for evaluation
    warmup_steps=500,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir='./logs',            # directory for storing logs
    logging_steps=10,
)

model = DistilBertForTokenClassification.from_pretrained(&quot;distilbert-base-uncased&quot;)

trainer = Trainer(
    model=model,                         # the instantiated 🤗 Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=train_dataset,         # training dataset
    eval_dataset=val_dataset,             # evaluation dataset
    compute_metrics=compute_metrics
)

trainer.train()
</code></pre>
<h2>What I did</h2>
<p>I checked cuda and GPU related settings.</p>
<pre><code>#os.environ['CUDA_LAUNCH_BLOCKING'] = &quot;1&quot;
torch.backends.cudnn.enabled = False
# check if CUDA is available
train_on_gpu = torch.cuda.is_available()
# torch.backends.cudnn.enabled

if not train_on_gpu:
    print('CUDA is not available.  Training on CPU ...')
else:
    print('CUDA is available!  Training on GPU ...')

#output
CUDA is available!  Training on GPU ...

training_args.device

#output
device(type='cuda', index=0)
</code></pre>
<h2>Responce to an answer</h2>
<p>When I comment out the part,</p>
<pre><code>#os.environ['CUDA_LAUNCH_BLOCKING'] = &quot;1&quot;
#torch.backends.cudnn.enabled = False
</code></pre>
<p>The error message changed to the below when I didn't reset runtime.</p>
<pre><code>/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py in _make_grads(outputs, grads)
     49                 if out.numel() != 1:
     50                     raise RuntimeError(&quot;grad can be implicitly created only for scalar outputs&quot;)
---&gt; 51                 new_grads.append(torch.ones_like(out, memory_format=torch.preserve_format))
     52             else:
     53                 new_grads.append(None)

RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
</code></pre>
<p>If I reset runtime, the message was the same.</p>
<pre><code>RuntimeError: cuda runtime error (710) : device-side assert triggered at /pytorch/aten/src/THC/generic/THCTensorMath.cu:29
</code></pre>
"
"68659164","How to remove unnecessary words from string for better search","2021-08-04 23:59:11","-1","193","0","1","","68708517","<p>I have different strings for searching the related data but due to unnecessary words, retrieved results are not good. For example, &quot;Working of genetic algorithm&quot;, so the words &quot;working of&quot; are not important in here. I can remove &quot;of&quot; by considering it as a stop word. But how about &quot;working&quot;? I can do stemming but it will just remove &quot;ing&quot;, which doesn't solve the problem. Similarly another string &quot;Determination of.....&quot;, I consider that other words in the string are important and &quot;Determination of&quot; are not important, so I want to remove them before proceeding further. Any ideas or hints how I can remove these words, since there are a lot of these types of words and I cannot hardcode them.</p>
"
"68654498","Removing words from lemmatisation dictionary/updating lemma dictionary in textstem","2021-08-04 15:59:30","0","123","0","1","","68654948","<p>I am using the textstem package to lemmatise words in some responses. However there is one word (spotting) which I do not wan't to be included, and reduced to &quot;spot&quot;. I want it to remain as spotting. How might I be able to do this? Do I need to make a custom dictionary? Currently doing:</p>
<pre><code>lemmatize_strings(df, dictionary = lexicon::hash_lemmas)
</code></pre>
"
"68599547","Preprocess words that do not match list of words","2021-07-31 05:56:13","1","91","2","1","","68602740","<p>I have a very specific case I'm trying to match: I have some text and a list of words (which may contain numbers, underscores, or ampersand), and I want to clean the text of numeric characters (for instance) unless it is a word in my list. This list is also long enough that I can't just make a regex that matches every one of the words.</p>
<p>I've tried to use regex to do this (i.e. doing something along the lines of <code>re.sub(r'\d+', '', text)</code>, but trying to come up with a more complex regex to match my case. This obviously isn't quite working, as I don't think regex is meant to handle that kind of case.</p>
<p>I'm trying to experiment with other options like pyparsing, and tried something like the below, but this also gives me an error (probably because I'm not understanding pyparsing correctly):</p>
<pre><code>from pyparsing import *
import re

phrases = [&quot;76&quot;, &quot;tw3nty&quot;, &quot;potato_man&quot;, &quot;d&amp;&quot;]
text = &quot;there was once a potato_man with tw3nty cars and d&amp; 76 different homes&quot;
parser = OneOrMore(oneOf(phrases) ^ Word(alphanums).setParseAction(lambda word: re.sub(r'\d+', '', word)))
parser.parseString(text)
</code></pre>
<p>What's the best way to approach this sort of matching, or are there other better suited libraries that would be worth a try?</p>
"
"68499164","Add known matches to Spacy document with character offsets","2021-07-23 12:25:13","1","243","0","1","","68524306","<p>I would like to run some analysis on documents using different Spacy tools, though I am interested in the Dependency Matcher in particular.</p>
<p>It just so happens that for these documents, I already have the character offsets of some difficult-to-parse entities. A somewhat-contrived example:</p>
<pre><code>from spacy.lang.en import English

nlp = English()
text = &quot;Apple is opening its first big office in San Francisco.&quot;
already_known_entities = [
    {&quot;offsets&quot;:(0,5), &quot;id&quot;: &quot;apple&quot;}, 
    {&quot;offsets&quot;:(41,54), &quot;id&quot;: &quot;san-francisco&quot;}
]

# do something here so that `nlp` knows about those entities 

doc = nlp(text)
</code></pre>
<p>I've thought about doing something like this:</p>
<pre><code>from spacy.lang.en import English

nlp = English()
text = &quot;Apple is opening its first big office in San Francisco.&quot;
already_known_entities = [{&quot;offsets&quot;:(0,5), &quot;id&quot;: &quot;apple&quot;}, {&quot;offsets&quot;:(41,54), &quot;id&quot;: &quot;san-francisco&quot;}]

ruler = nlp.add_pipe(&quot;entity_ruler&quot;)
patterns = []
for e in already_known_entities:
    patterns.append({
        &quot;label&quot;: &quot;GPE&quot;,
        &quot;pattern&quot;: text[e[&quot;offsets&quot;][0]:e[&quot;offsets&quot;][1]]
    })
ruler.add_patterns(patterns)

doc = nlp(text)
</code></pre>
<p>This technically works, and it's not the worst solution in the world, but I was still wondering if offsets can be added to the <code>nlp</code> object directly. As far as I can tell, the <a href=""https://spacy.io/api/matcher"" rel=""nofollow noreferrer"">Matcher docs</a> don't show anything like this. I also understand this might be a bit of a departure from typical Matcher behavior, where a pattern can be applied to all documents in a corpus--whereas here I want to tag entities at certain offsets only for particular documents. Offsets from one document do not apply to other documents.</p>
"
"68394241","Unstructured data, NLP Lemmatize Book Review","2021-07-15 12:54:05","0","104","0","1","","68396056","<p>Here I have m trying to read the content let's say 'book1.txt' and here I have to remove all the special characters and punctuation marks and word tokenise the content using nltk's word tokeniser.
Lemmatize those token using  wordnetLemmatizer
And write those token into csv file one by one.
Here is the code I m using which obviously is not working but just need some suggestion on this please.</p>
<pre><code>    import nltk
from nltk.stem import WordNetLemmatizer
import csv
from nltk.tokenize import word_tokenize

file_out=open('data.csv','w')
with open('book1.txt','r') as myfile:
  for s in myfile:
    words = nltk.word_tokenize(s)
    words=[word.lower() for word in words if word.isalpha()]
    for word in words:
      token=WordNetLemmatizer().lemmatize(words,'v')
      filtered_sentence=[&quot;&quot;]
      for n in words:
        if n not in token:
          filtered_sentence.append(&quot;&quot;+n)
        file_out.writelines(filtered_sentence+[&quot;\n&quot;])
</code></pre>
"
"68306484","Not able to import python package jax in Google TPU","2021-07-08 17:49:37","3","1134","0","2","","68308553","<p>I am working on linux console and typing python takes me into the python console.
When I use the following command in TPU machine</p>
<pre class=""lang-python prettyprint-override""><code>import jax
</code></pre>
<p>then it generates following mss and get out of the python prompt.</p>
<pre class=""lang-bash prettyprint-override""><code>paramjeetsingh80@t1v-n-1c883486-w-0:~$ python3
Python 3.8.5 (default, Jan 27 2021, 15:41:15)
[GCC 9.3.0] on linux
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt; import jax
2021-07-08 17:41:39.660523: F external/org_tensorflow/tensorflow/core/tpu/tpu_executor_init_fns.inc:110] TpuTransferManager_ReadDynamicShapes not available in this library.
Aborted (core dumped)
paramjeetsingh80@t1v-n-1c883486-w-0:~$
</code></pre>
<p>This issue is causing problem in my code so I would like to figure out, what is this issue and how to get rid of this?</p>
"
"68274864","TF-IDF and text chunks","2021-07-06 17:16:30","0","593","0","2","","68275055","<p>I am a begginer in NLP and I am using TF-IDF method to apply then a ML model. If I have a dataset like this</p>
<pre><code>dataset = ['I have three cars', 'and one motorbike']
</code></pre>
<p>which is the correct the way (A or B) to apply TF-IDF and why?</p>
<p>Option 1</p>
<pre><code>Tfidf_vect = TfidfVectorizer(max_features=100000, ngram_range = (1,2))
Tfidf_vect.fit(dataset)
</code></pre>
<p>Option 2</p>
<pre><code>for d in dataset:
  Tfidf_vect2 = TfidfVectorizer(max_features=100000, ngram_range = (1,2))
  Tfidf_vect2.fit(d)
</code></pre>
<p>Moreover, The Option 2, isn't working, and I can't understand why. Please help me.</p>
"
"68185061","Strange results with huggingface transformer[marianmt] translation of larger text","2021-06-29 20:10:15","3","2441","2","1","","68518389","<p>I need to translate large amounts of text from a database. Therefore, I've been dealing with transformers and models for a few days. I'm absolutely no data science expert and unfortunately I don't get any further.</p>
<p>The problem starts with longer text. The 2nd issue is the usual-maximum token size (512) of the sequencers. Just truncating is not really an option. <a href=""https://towardsdatascience.com/how-to-apply-transformers-to-any-length-of-text-a5601410af7f"" rel=""nofollow noreferrer"">Here</a> I did  find a work-around, but it does not work properly and the result is a word salad on longer texts (&gt;300 sequences)</p>
<p>Here an Example <em>(please ignore the warnings, this is another issues - which does not hurt currently that much)</em>;</p>
<p>If i take the Example Sentence 2 (55 seq) or 5 times (163 sequences) - <strong>no issues.</strong></p>
<p>But it get messed up with e.g. 433 sequences (the 3rd green text block in the screenshot).</p>
<p><a href=""https://i.sstatic.net/IzYKf.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/IzYKf.png"" alt=""enter image description here"" /></a></p>
<p>With more than 510 sequences, I tried to split it up in chunks as in the upper described link. But the result here is as well pretty strange.</p>
<p>I am pretty sure - that I have more than just one mistake and underestimated this topic.
But I see no alternative (free/cheap) way for translating big amount of text.</p>
<p>Can you guys help me out? Which (thinking) errors do you see and how would you suggest to solve the issues? Thank you very much.</p>
<p><a href=""https://i.sstatic.net/S8jMW.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/S8jMW.png"" alt=""enter image description here"" /></a></p>
<pre><code>from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch

if torch.cuda.is_available():  
  dev = &quot;cuda&quot;
else:  
  dev = &quot;cpu&quot; 
device = torch.device(dev)
 
mname = 'Helsinki-NLP/opus-mt-de-en'
tokenizer = AutoTokenizer.from_pretrained(mname)
model = AutoModelForSeq2SeqLM.from_pretrained(mname)
model.to(device)

chunksize = 512

text_short = &quot;Nach nur sieben Seiten appellierte man an die Wählerinnen und Wähler, sich richtig zu entscheiden, nämlich für Frieden, Freiheit, Sozialismus. &quot;
text_long = text_short
#this loop is just for debugging/testing and simulating long text
for x in range(30):
    text_long = text_long + text_short

tokens = tokenizer.encode_plus(text_long, return_tensors=&quot;pt&quot;, add_special_tokens=True, padding=False, truncation=False).to(device)
str_len = len(tokens['input_ids'][0])

if str_len &gt; 510:
    # split into chunks of 510 tokens, we also convert to list (default is tuple which is immutable)
    input_id_chunks = list(tokens['input_ids'][0].split(chunksize - 2))
    mask_chunks = list(tokens['attention_mask'][0].split(chunksize - 2))

    cnt = 1
    for tensor in input_id_chunks:
        print('\033[96m' + 'chunk ' + str(cnt) + ': ' + str(len(tensor)) + '\033[93m')
        cnt += 1
    
    # loop through each chunk
    # https://towardsdatascience.com/how-to-apply-transformers-to-any-length-of-text-a5601410af7f
    for i in range(len(input_id_chunks)):
        # add CLS and SEP tokens to input IDs
        input_id_chunks[i] = torch.cat([
            torch.tensor([101]).to(device), input_id_chunks[i], torch.tensor([102]).to(device)
        ])
        # add attention tokens to attention mask
        mask_chunks[i] = torch.cat([
            torch.tensor([1]).to(device), mask_chunks[i], torch.tensor([1]).to(device)
        ])
        # get required padding length
        pad_len = chunksize - input_id_chunks[i].shape[0]
        # check if tensor length satisfies required chunk size
        if pad_len &gt; 0:
            # if padding length is more than 0, we must add padding
            input_id_chunks[i] = torch.cat([
                input_id_chunks[i], torch.Tensor([0] * pad_len).to(device)
            ])
            mask_chunks[i] = torch.cat([
                mask_chunks[i], torch.Tensor([0] * pad_len).to(device)
            ])
   
    input_ids = torch.stack(input_id_chunks)
    attention_mask = torch.stack(mask_chunks)
    input_dict = {'input_ids': input_ids.long(), 'attention_mask': attention_mask.int()}
    
    outputs = model.generate(**input_dict)
    #this doesnt work - following error comes to the console --&gt; &quot;host_softmax&quot; not implemented for 'Long'
    #probs = torch.nn.functional.softmax(outputs[0], dim=-1)
    # probs
    # probs = probs.mean(dim=0)
    # probs
  
else:
    tokens[&quot;input_ids&quot;] = tokens[&quot;input_ids&quot;][:, :512] #truncating normally not necessary
    tokens[&quot;attention_mask&quot;] = tokens[&quot;attention_mask&quot;][:, :512]
    outputs = model.generate(**tokens)

decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
print('\033[94m' + str(str_len))
print('\033[92m' + decoded)
</code></pre>
<p>Remark; following libs are necessary:</p>
<blockquote>
<p>pip3 install torch==1.9.0+cu102 torchvision==0.10.0+cu102 torchaudio===0.9.0 -f <a href=""https://download.pytorch.org/whl/torch_stable.html"" rel=""nofollow noreferrer"">https://download.pytorch.org/whl/torch_stable.html</a></p>
</blockquote>
<blockquote>
<p>pip install transformers</p>
</blockquote>
<blockquote>
<p>pip install sentencepiece</p>
</blockquote>
"
"68140256","How can I lemmatize a tokenized column of a dataframe in python?","2021-06-26 07:41:55","0","748","0","1","","68140312","<p>I try to lemmatize the column &quot;tokenized&quot; in a dataframe. One cell of the column &quot;tokenized&quot; looks as follows <em>&quot;  yeah    simply    zurich    generic    serving    think    media    bland    prepared    curry    kind    paying    well    loves    used    parboiled    oily    place    elaborate    non    tasteful    stay    underspiced    institution    vegetarian    indian    clueless    away    hiltl    anyone    served    support    veg    long    like    normal    strong    worth    insult    not    rice    kitchen    know    wont    food    cuisine    fantastic    fan    time    term    patrons  &quot;.</em></p>
<p><strong>When I run my code it returns something like this: &quot;,,e,n,d,e,d,,,p,a,y,i&quot;</strong> which is not what i want. How can I lemmatize full words?</p>
<p>This is my code:</p>
<pre><code>reviews_english['tokenized_lem'] = reviews_english['tokenized'].apply(
                    lambda lst:[lmtzr.lemmatize(word) for word in lst])
reviews_english
</code></pre>
"
"67997713","ModuleNotFoundError: No java install detected. Please install java to use language-tool-python","2021-06-16 06:50:29","2","5291","2","2","","67998073","<p>I would like to check the number if issues in a given sentence.</p>
<p>my code is</p>
<pre><code>import language_tool_python
tl = language_tool_python.LanguageTool('en-US')

txt = &quot;good mooorning sirr and medam my namee anderen i am from amerecia !&quot;
m = tl.check(txt)
len(m)
</code></pre>
<p>Instead of returning the number i am getting error message as shown below.</p>
<pre><code>ModuleNotFoundError                       Traceback (most recent call last)
&lt;ipython-input-1-1c4c9134d6f4&gt; in &lt;module&gt;
      1 import language_tool_python
----&gt; 2 tool = language_tool_python.LanguageTool('en-US')
      3 
      4 text = &quot;Your the best but their are allso  good !&quot;
      5 matches = tool.check(text)

E:\Anaconda\lib\site-packages\language_tool_python\server.py in __init__(self, language, motherTongue, remote_server, newSpellings, new_spellings_persist)
     43             self._update_remote_server_config(self._url)
     44         elif not self._server_is_alive():
---&gt; 45             self._start_server_on_free_port()
     46         if language is None:
     47             try:

E:\Anaconda\lib\site-packages\language_tool_python\server.py in _start_server_on_free_port(self)
    212             self._url = 'http://{}:{}/v2/'.format(self._HOST, self._port)
    213             try:
--&gt; 214                 self._start_local_server()
    215                 break
    216             except ServerError:

E:\Anaconda\lib\site-packages\language_tool_python\server.py in _start_local_server(self)
    222     def _start_local_server(self):
    223         # Before starting local server, download language tool if needed.
--&gt; 224         download_lt()
    225         err = None
    226         try:

E:\Anaconda\lib\site-packages\language_tool_python\download_lt.py in download_lt(update)
    142     ]
    143 
--&gt; 144     confirm_java_compatibility()
    145     version = LATEST_VERSION
    146     filename = FILENAME.format(version=version)

E:\Anaconda\lib\site-packages\language_tool_python\download_lt.py in confirm_java_compatibility()
     73         # found because of a PATHEXT-related issue
     74         # (https://bugs.python.org/issue2200).
---&gt; 75         raise ModuleNotFoundError('No java install detected. Please install java to use language-tool-python.')
     76 
     77     output = subprocess.check_output([java_path, '-version'],

ModuleNotFoundError: No java install detected. Please install java to use language-tool-python.
</code></pre>
<p>When I run the code I get no java install detected
How to solve this issue?</p>
"
"67925248","How to get a pair of dependency relation between two words in a sentence using spacy?","2021-06-10 16:38:16","1","1450","0","1","","67937061","<p>I am using spacy to get the dependency relation, this works well. But I have a problem of getting a pair of token with a specific dependency relation (except for the <code>conj</code> relation).</p>
<p>When using the <code>.dep_</code>, I can get the dependency attribute of each seprate token.
However, I would like to a pair of token for a specific dependency relation.
For example, in the following code, I can get the shown result.</p>
<pre><code>import spacy
nlp = spacy.load(&quot;en_core_web_md&quot;)
sentence = 'The Marlins were stymied by Austin Gomber and the Rockies in their 4-3 loss'
doc = nlp(sentence)
for token in doc:
    print (token, token.dep_)

</code></pre>
<p>Current output:</p>
<pre><code>The det
Marlins nsubjpass
were auxpass
stymied ROOT
by agent
Austin compound
Gomber pobj
and cc
the det
Rockies conj
in prep
their poss
4 nummod
- punct
3 prep
loss pobj

</code></pre>
<p>But what I <strong>desire</strong> to get is:
(please ignore the output style, I only want to get a pair of token with a specific dependency relation, e.g., here is <code>pobj</code>)</p>
<pre><code>'Gomber' is a 'pobj' of 'by'
'Loss' is a 'pobj' of 'in'
</code></pre>
<p>In another word, I do not only want to get the result of <strong>current output</strong>, I also want to get <strong>paired</strong> token for each word.</p>
<p>For the <code>conj</code> dependency relation, I can get them simply by just using <code>token.conjuncts</code>, but for the rest of other dependency relations, such the <code>pobj</code>, <code>prep</code>, I have not found any method can be used directly in spacy.</p>
<p>Does anyone have a hint on getting this <code>pobj</code> relation? Thanks in advance!</p>
"
"67794357","How to build parameter grid with FeatureUnion?","2021-06-01 18:37:41","0","33","0","1","","67794722","<p>I am trying to run this combined model, of text and numeric features, and I am getting the error <code>ValueError: Invalid parameter tfidf for estimator</code>. Is the problem in the <code>parameters</code> synthax?
Possibly helpful links:
<a href=""https://towardsdatascience.com/how-to-combine-textual-and-numerical-features-for-machine-learning-in-python-dc1526ca94d9"" rel=""nofollow noreferrer"">FeatureUnion usage</a>
<a href=""https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html"" rel=""nofollow noreferrer"">FeatureUnion documentation</a></p>
<pre><code>tknzr = tokenize.word_tokenize
vect = CountVectorizer(tokenizer=tknzr, stop_words={'english'}, max_df=0.9, min_df=2)
scl = StandardScaler(with_mean=False)
tfidf = TfidfTransformer(norm=None)
parameters = {
    'vect__ngram_range': [(1, 1), (1, 2), (1, 3), (2, 2), (2, 3), (3, 3)],
    'tfidf__use_idf': (True, False),
    'clf__alpha': tuple(10 ** (np.arange(-4, 4, dtype='float'))),
    'clf__loss': ('hinge', 'squared_hinge', 'log', 'modified_huber', 'perceptron'),
    'clf__penalty': ('l1', 'l2'),
    'clf__tol': (1e07, 1e-6, 1e-5, 1e-4, 1e-3)
}

combined_clf = Pipeline([
    ('features', FeatureUnion([
        ('numeric_features', Pipeline([
            ('selector', transfomer_numeric)
        ])),
        ('text_features', Pipeline([
            ('selector', transformer_text),
            ('vect', vect),
            ('tfidf', tfidf),
            ('scaler', scl),
        ]))
    ])),
    ('clf', SGDClassifier(random_state=42,
                          max_iter=int(10 ** 6 / len(X_train)), shuffle=True))
])
</code></pre>
"
"67789544","Given a word can we get all possible lemmas for it using Spacy?","2021-06-01 13:13:35","3","2171","0","3","","67796633","<p>The input word is standalone and not part of a sentence but I would like to get all of its possible lemmas as if the input word were in different sentences with all possible POS tags. I would also like to get the lookup version of the word's lemma.</p>
<p>Why am I doing this?</p>
<p>I have extracted lemmas from all the documents and I have also calculated the number of dependency links between lemmas. Both of which I have done using <code>en_core_web_sm</code>. Now, given an input word, I would like to return the lemmas that are linked most frequently to all the possible lemmas of the input word.</p>
<p>So in short, I would like to replicate the behaviour of <code>token._lemma</code> for the input word with all possible POS tags to maintain consistency with the lemma links I have counted.</p>
"
"67734007","Counting in how many documents does a word appear","2021-05-28 06:42:02","-2","402","4","1","","67734401","<p>I'm trying to implement a TFIDF vectorizer without sklearn. I want to count the number of documents(list of strings) in which a  word appears, and so on for all the words in that corpus.
Example:</p>
<pre><code>corpus = [
     'this is the first document',
     'this document is the second document',
     'and this is the third one',
     'is this the first document',
]
</code></pre>
<p><strong>Desired OP:</strong> <code>{this : 4, is : 4}</code> and so on for every word</p>
<p><strong>My code:</strong></p>
<pre><code>def docs(corpus):
    doc_count = dict()
    for line in corpus:
        for word in line.split():
            if word in line:
                doc_count[word] +=1
            else:
                doc_count[word] = 1
        print(counts)

docs(corpus)
</code></pre>
<p><strong>Error I'm facing:</strong></p>
<pre><code>KeyError                                  Traceback (most recent call last)
&lt;ipython-input-70-6bf2b69708bc&gt; in &lt;module&gt;
      9         print(counts)
     10 
---&gt; 11 docs(corpus)

&lt;ipython-input-70-6bf2b69708bc&gt; in docs(corpus)
      4         for word in line.split():
      5             if word in line.split():
----&gt; 6                 doc_count[word] +=1
      7             else:
      8                 doc_count[word] = 1

KeyError: 'this'
</code></pre>
<p>Please let me know where I'm lacking and if I'm not iterating properly. Thank you!</p>
"
"67664837","Problems understanding nDCG format in pytrec_eval?","2021-05-23 22:10:55","3","481","0","1","","75764096","<p>I am using <code>pytrec_eval</code> to calculate nDCG scores. For example, for <code>qrel</code>:</p>
<pre><code>qrel = {
    'q1': {
        'd1': 0,
        'd2': 1,
        'd3': 0,
    }
}
</code></pre>
<p>And <code>run</code>:</p>
<pre><code>run = {
    'q1': {
        'd1': 1.0,
        'd2': 0.0,
        'd3': 1.5,
    }
}
</code></pre>
<p>The nDCG score can be calculated like this:</p>
<pre><code>import pytrec_eval
import json    
evaluator = pytrec_eval.RelevanceEvaluator(
    qrel, {'ndcg'})

print(json.dumps(evaluator.evaluate(run), indent=1))


 &quot;q1&quot;: {
  &quot;ndcg&quot;: 0.5
 }
</code></pre>
<p>My understanding is that nDCG takes into account the order of the retrieved documents indices, however, if you change the document ordering in the <code>run</code> you still get the same nDCG score, for example:</p>
<pre><code>run2 = {
    'q1': {
        'd1': 1.0,
        'd3': 1.5,
        'd2': 0.0,

    }
}

evaluator = pytrec_eval.RelevanceEvaluator(qrel, {'ndcg'})
print(json.dumps(evaluator.evaluate(run2), indent=1))
</code></pre>
<p>Is this the expected behavior of calculating nDCG? What is the usage of the <code>qrel</code>? My undertanding is that the qrel tells you how relevant is the retrieved document, while run, is the resulting ranking of your query and IR system. Then, why if I change the order of <code>run</code> the nDCG score is the same?</p>
"
"67612600","How to fix this code and make my own POS-tagger? (PYTHON)","2021-05-20 00:50:39","1","120","11","1","","67613763","<p>My program need to read a file with sentences and produce an output like that:</p>
<p>input: Ixé Maria.
output: Ixé\PRON Maria\N-PR.</p>
<p>Until now, I wrote this, but the outfile gives me an empty textfile. (please, give me suggestions):</p>
<pre class=""lang-py prettyprint-override""><code>infile = open('corpus_test.txt', 'r', encoding='utf-8').read()
outfile = open('tag_test.txt', 'w', encoding='utf-8')

dicionario = {'mimbira': 'N',
             'anama-itá': 'N-PL',
             'Maria': 'N-PR',
             'sumuara-kunhã': 'N-FEM',
             'sumuara-kunhã-itá': 'N-FEM-PL',
             'sapukaia-apigaua': 'N-MASC',
             'sapukaia-apigaua-itá': 'N-MASC-PL',
             'nhaã': 'DEM',
             'nhaã-itá': 'DEM-PL',
             'ne': 'POS',
             'mukuĩ': 'NUM',
             'muíri': 'QUANT',
             'iepé': 'INDF',
             'pirasua': 'A1',
             'pusé': 'A2',
             'ixé': 'PRON1',
             'se': 'PRON2',
             '. ;': 'PUNCT'
             }

np_words = dicionario.keys()
np_tags = dicionario.values()

for line in infile.splitlines():
   list_of_words = line.split()
   if np_words in list_of_words:
       tag_word = list_of_words.index(np_words)+1
       word_tagged = list_of_words.insert(tag_word, f'\{np_tags}') 
       word_tagged = &quot; &quot;.join(word_tagged)
       print(word_tagged, file=outfile)

outfile.close()
</code></pre>
"
"67543209","Calculating BLEU and Rouge score as fast as possible","2021-05-15 04:15:19","3","4850","1","2","","67621076","<p>I have around 200 candidate sentences and for each candidate, I want to measure the bleu score by comparing each sentence with thousands of reference sentences. These references are the same for all candidates. Here is how I'm doing it right now:</p>
<pre><code>ref_for_all = [reference] *len(sents)
score = corpus_bleu(ref_for_all, [i.split() for i in sents], weights=(0, 1, 0, 0))
</code></pre>
<p>The <code>reference</code> contains the whole corpus I want to compare each sentence with, and <code>sent</code> are my sentences (candidates). Unfortunately, this takes too long and given the experimental nature of my code, I cannot wait that long to get the results. Is there any other way (for example using Regex) that I can get these scores faster? I also have this problem with Rouge, so any suggestion is highly appreciated for that too!</p>
"
"67519425","translator() from Googletrans not translating the texts to English","2021-05-13 12:40:49","0","1137","1","1","","68482504","<p>I am trying to translate the field Short description to English since some of the rows are not in English. But using the code below I am not able to translate. The translate column and the original columns look exactly the same. Please see the image attached for the output.</p>
<pre><code>from googletrans import Translator
translator = Translator()

mask = data['Short description'] !='en'

data['Short description_translated'] = data['Short description']
f = lambda x: translator.translate(x, dest='en').text
data.loc[mask, 'Short description_translated'] = data.loc[mask, 'Short description'].apply(f)
print (data)
</code></pre>
<p><a href=""https://i.sstatic.net/uE0Rd.png"" rel=""nofollow noreferrer"">Output</a></p>
"
"67507820","Join multiple values into same cell R","2021-05-12 16:45:50","1","889","0","2","","67507985","<p>I have a data frame with pos values for each document split down into single tokens. How can I merge the individual pos values into one single cell separated by a comma?
So now I have something like</p>
<pre><code>  doc_id sentence_id token_id    token  pos entity
1  text1           1        1   xxxxxx PRON       
2  text1           1        2     xxxx  AUX       
3  text1           1        3      xxx  AUX       
4  text1           1        4  xxxxxxx VERB       
5  text2           1        5     xxxx  DET       
6  text2           1        6      xxx NOUN  
</code></pre>
<p>How can I make it into</p>
<pre><code>  doc_id                      pos    entity
1  text1  PRON, AUX, AUX, VERB...       
2  text2  AUX, NOUN, PRON, ADJ...       
3  text3  ...
4  text4  ...  
5  text5  ...
6  text6  ...
</code></pre>
<p>Do I need to create a new data frame or is there a Spacy function that can do this directly?
Thank you</p>
"
"67474728","Conjuncts are not identified completely in spaCy?","2021-05-10 17:03:39","0","150","0","1","","67528661","<p>I want to identify all the conjuncts by using .conjuncts in spaCy dependency parsing.</p>
<p>But, I found a problem that: not all conjuncts are identified.</p>
<p>For example, in the following sentence template:</p>
<p><code>A....B....C.... D....</code></p>
<p>If <code>A</code> and <code>D</code> have <code>conj</code> dependency relation; <code>C</code> and <code>D</code> also have a <code>conj</code> relation. But, <code>A</code> has no <code>conj</code> relation with <code>B</code> and <code>C</code>; <code>D</code> has no <code>conj</code> relation with <code>B</code> and <code>C</code>.</p>
<p>In this case, the <code>conj</code> relation between <code>C</code> and <code>D</code> can be shown in the graphical dependency relation by using <code>.displacy</code>, <strong>BUT</strong>, while using the <code>.conjuncts</code> to list all conjunct pairs (chunk and conjunct), the conjunct (tuple) of <code>C</code> is empty <code>()</code>, the conjunct (tuple) of <code>D</code> is empty <code>()</code>.</p>
<p>Code for getting the conjuncts:</p>
<pre><code>prev_end=0
for chunk in doc.noun_chunks:
    span = doc[prev_end: chunk.end]
    conj_ = span.conjuncts
    prev_end = chunk.end 

</code></pre>
<p><strong>--Does anyone know the reason?</strong></p>
<p><strong>--Is it because of the bug in the spaCy library or anything else?</strong></p>
<p>Thanks in advance!</p>
"
"67441897","How to resolve Spacy POS Attribute E1005 Error","2021-05-07 21:32:12","2","1296","2","1","","67464811","<p>I was able to install spaCy and download the standard English model (en_core_web_sm).</p>
<p>But by just loading the standard data model, I received the following error message:</p>
<pre><code>import spacy
​
# Load English tokenizer, tagger, parser and NER
nlp = spacy.load(&quot;en_core_web_sm&quot;)


ValueError: [E1005] Unable to set attribute 'POS' in tokenizer exception for '  '. 
Tokenizer exceptions are only allowed to specify ORTH and NORM.
</code></pre>
<p>I check the <code>Config.CFG</code> but don't see any <code>POS</code> attribute. Any help is greatly appreciated as I searched the Internet for an answer....</p>
<p>PS, using <code>pip freeze</code>, here are some of the libraries</p>
<pre><code>spacy==3.0.6
spacy-legacy==3.0.5
en-core-web-sm==2.2.0
</code></pre>
"
"67426019","Generating new values by combining two lists in Python","2021-05-06 21:12:25","0","52","0","1","","67426116","<p>I have a series of phrases that I tokenized. I then found the synonyms of each word and saved them in a dictionary with the word (i.e. token) as the key and a list of synonyms as the value. My goal is to generate new phrases by replacing each word with its synonyms and create new phrases.</p>
<p>For example, we have a phrase that has 3 tokens. The first token (limited) has 18 synonyms, the second token (social) has 4 synonyms, and the last token (support) has 16 synonyms. So theoretically we would have <code>18 * 4 * 16 = 1,152</code> new phrases by combining all 3 lists together.</p>
<pre><code>phrases = ['limited', 'social', 'support']

dictionary = {
    'limited': ['express', 'limited', 'restrict', 'restrain', 'trammel', 'limit', 'bound', 'confine', 'throttle', 'circumscribe', 'specify', 'set', 'determine', 'define', 'fix', 'circumscribed', 'modified', 'special'],
    'social': ['sociable', 'social', 'mixer', 'societal'],
    'support': ['support', 'reinforcement', 'reenforcement', 'documentation', 'keep', 'livelihood', 'living', 'bread_and_butter', 'sustenance', 'supporting', 'accompaniment', 'musical_accompaniment', 'backup', 'financial_support', 'funding', 'backing']
}

new_phrases = [['express', 'sociable', 'support'], ['express', 'social', 'support'], ['express', 'mixer', 'support'], ['express', 'societal', 'support'], ..., [...]]
</code></pre>
<p>My attempt was to iterate through the items in each list but I'm having a hard time conceptualizing how to combine these 3 lists together to generate something similar to <code>new_phrases</code> as shown in the code chunk above.</p>
<pre><code>for word in phrases:
    print(&quot;\nthe word is:&quot;, word)
    print(&quot;list of synonyms is:&quot;, dictionary[word])
    print(&quot;the list has&quot;, len(dictionary[word]), &quot;elements&quot;)
    for syn in dictionary[word]:
        print(&quot;a synonmy is:&quot;, syn)
</code></pre>
"
"67397321","Lemmatize multiple MB of raw text with Spacy and Inline::Python in Perl. Why is this slow?","2021-05-05 08:00:58","3","440","5","1","","67398182","<p>I work on an NLP and I need to lemmatize tons of tokens from raw input text file from 10MB to 300MB and I decided to use <code>Inline::Python</code> with <code>spacy</code> to do this task. The problem is that it's very slow. After this, I create bags of words to put in a cosine similarity module to classify texts from the past years. Is there a way to process faster, multi-processing, multi-threading, or is it the pipe to Python that is slow? And i have i9, 64GB RAM, RTX 2080TI and SSD connected by nvme.</p>
<p>Here is the piece of code to lemmatize in French some text content and filter stop words:</p>
<pre class=""lang-perl prettyprint-override""><code>use Inline Python =&gt; &lt;&lt;'END_OF_PYTHON';

import spacy
from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop
nlp = spacy.load('fr_core_news_md')
nlp.max_length = 40000000

def lemmatizer(words):
    doc = nlp(words)
    return list(filter(lambda x: x not in list(fr_stop), list(map(lambda token: token.lemma_ , doc))))

END_OF_PYTHON
</code></pre>
<p>Unfortunately, there is no good French lemmatizer in Perl and the lemmatization increases my accuracy to classify text files in good categories by 5%. It's important when you have already 90% good results without it. In this piece of code, I only use the function <code>lemmatizer</code> in Perl after this. I don't reload each time the nlp <code>spacy</code> module in French (I think ?)</p>
<p>I thought about creating one thread per file. I have 15 big text files to lemmatize. One file per category from the recent years. But imo, the I/O is the problem. Do you have some ideas? I can't show more code because there are 1500 lines. I need 1000 seconds to process automatic classification with the smallest category (50/60 files from the current year). The biggest is 10x bigger than the smallest.</p>
"
"67303890","Parsing with nltk of a Recursive Grammar","2021-04-28 16:08:17","1","224","0","1","","67306087","<p>I must be missing something fundamental about recursively defined nonterminals giving issue, but all I want to do is to recognize something like a regular expression, where a series of numbers followed by a series of letters.</p>
<pre><code>from nltk import CFG
import nltk

grammar = CFG.fromstring(&quot;&quot;&quot;
S -&gt; N L
N -&gt; N | '1' | '2' | '3'
L -&gt; L | 'A' | 'B' | 'C'
&quot;&quot;&quot;)

from nltk.parse import BottomUpChartParser
parser = nltk.ChartParser(grammar)

sentence = '1 2 1 3 A C B C'.split()

for t in parser.parse(sentence):
    print(t)
</code></pre>
<p>the above code returns an empty parse and nothing is printed</p>
"
"67257008","OSError: libmkl_intel_lp64.so.1: cannot open shared object file: No such file or directory","2021-04-25 18:40:25","5","13391","5","1","","67479054","<p>I am trying to run a model on TPU as given in <a href=""https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb"" rel=""noreferrer"">colab notebook</a>. The model was working fine, but today I could not run the model.</p>
<p>I used the following code to install pytorch-xla.</p>
<pre><code>VERSION = &quot;nightly&quot;  #@param [&quot;1.5&quot; , &quot;20200325&quot;, &quot;nightly&quot;]
!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py
!python pytorch-xla-env-setup.py --version $VERSION
</code></pre>
<p>I try to install required libraries as below:</p>
<pre><code>!pip install -U nlp
!pip install sentencepiece
!pip install numpy --upgrade
</code></pre>
<p>However, when I try the following</p>
<pre><code>import nlp
</code></pre>
<p>It gives the following error:</p>
<pre><code>OSError: libmkl_intel_lp64.so.1: cannot open shared object file: No such file or directory
</code></pre>
<p>I searched the error and I tried the followings, but still does not work. Any ideas how to fix it? Note: It was working a few days ago, however, today it is not.</p>
<pre><code>!pip install mkl
#!export PATH=&quot;$PATH:/opt/intel/bin&quot;
#!export LD_LIBRARY_PATH=&quot;$PATH:opt/intel/mkl/lib/intel64_lin/&quot;
!export LID_LIBRAEY_PATH=&quot;$LID_LIBRARY_PATH:/opt/intel/mkl/lib/intel64_lin/&quot;
</code></pre>
"
"67194634","Error loading weights from a Hugging Face model","2021-04-21 11:07:07","2","4575","3","1","","67246204","<p>I'm using transformers and I already have loaded a model and It works fine:</p>
<pre><code>from transformers import AutoModelForSequenceClassification
from transformers import AutoTokenizer

task='sentiment'
MODEL = &quot;cardiffnlp/twitter-roberta-base-{task}&quot;
tokenizer = AutoTokenizer.from_pretrained(MODEL)

# PT
model = AutoModelForSequenceClassification.from_pretrained(MODEL) 
model.save_pretrained(MODEL)
</code></pre>
<p>but If I try to load another task like &quot;emotion&quot; or &quot;hate&quot;, I get this error:</p>
<pre><code>from transformers import AutoModelForSequenceClassification
from transformers import AutoTokenizer

task='emotion'
MODEL = &quot;cardiffnlp/twitter-roberta-base-{task}&quot;
tokenizer = AutoTokenizer.from_pretrained(MODEL)

# PT
model = AutoModelForSequenceClassification.from_pretrained(MODEL)  ## Here I get the error
model.save_pretrained(MODEL)
</code></pre>
<p>This error:</p>
<pre><code>OSError: Can't load weights for 'cardiffnlp/twitter-roberta-base-emotion'. Make sure that:

- 'cardiffnlp/twitter-roberta-base-emotion' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'cardiffnlp/twitter-roberta-base-emotion' is the correct path to a directory containing a file named one of pytorch_model.bin, tf_model.h5, model.ckpt.
</code></pre>
<p>I have checked it and these models actually exists are Hugging Face models, as you can see <a href=""https://huggingface.co/models?filter=arxiv:2010.12421"" rel=""nofollow noreferrer"">here</a>, so I dont get why is not working.</p>
<p>Edit: I have noticed that the first time I run it, It works with all the tasks (hate, emotion, sentiment) but If I try to run it again, then I get the error.</p>
"
"67083987","Lemmatize df column","2021-04-14 00:30:13","0","552","0","1","","67084115","<p>I am trying to lemmatize content in a df but the function I wrote isn't working. Prior to trying to lemmatize the data in the column looked like this.</p>
<p><a href=""https://i.sstatic.net/hCfCP.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/hCfCP.png"" alt=""enter image description here"" /></a></p>
<p>Then I ran the following code:</p>
<pre><code>import nltk
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer 

# Init the Wordnet Lemmatizer
lemmatizer = WordNetLemmatizer()

def lemmatize_text(text):
    lemmatizer = WordNetLemmatizer()
    return [lemmatizer.lemmatize(w) for w in text]  

df['content'] = df[&quot;content&quot;].apply(lemmatize_text)
print(df.content)
</code></pre>
<p>Now the content column looks like this:</p>
<p><a href=""https://i.sstatic.net/xiVsl.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/xiVsl.png"" alt=""enter image description here"" /></a></p>
<p>I'm not sure what i did wrong, but I am just trying to lemmatize the data in the content column. Any help would be greatly appreciated.</p>
"
"66822048","Why Sacrebleu returns zero BLEU score for short sentences?","2021-03-26 17:56:53","3","2157","1","1","","66834146","<p>Why <code>scarebleu</code> needs that sentences ends with dot? If I remove dots, the value is zero.</p>
<pre><code>import sacrebleu, nltk
sys = [&quot;This is cat.&quot;] 
refs = [[&quot;This is a cat.&quot;], 
        [&quot;This is a bad cat.&quot;]] 

b3 = sacrebleu.corpus_bleu(sys, refs)
print(&quot;b3&quot;, b3.score)
print(&quot;b3&quot;, round(b3.score,2))
</code></pre>
<p>This returns the following:</p>
<pre><code>b3 35.1862973998119
b3 35.19
</code></pre>
<p>When I remove the ending dots.</p>
<pre><code>sys = [&quot;This is cat&quot;] 
refs = [[&quot;This is a cat&quot;], 
        [&quot;This is a bad cat&quot;]] 


b3 = sacrebleu.corpus_bleu(sys, refs)
print(&quot;b3&quot;, b3.score)
print(&quot;b3&quot;, round(b3.score,2))
</code></pre>
<p>It prints zero using scarebleu which is again weird!:</p>
<pre><code>b3 0.0
b3 0.0
</code></pre>
"
"66624212","NLTK CFG ValueError: Grammar does not cover some of the input words","2021-03-14 11:53:42","1","1898","0","1","","66643853","<p>I am processing text with <code>nltk.ChartParser(grammar)</code> and get the error message as described in the headline.</p>
<p>I do not understand why, since all the words of my sentence are covered in the grammar, as you can see here in my code:</p>
<p><strong>1. Step: preprocessing</strong> <em>(no errors)</em></p>
<pre><code>message = &quot;The burglar robbed the bank&quot;

import nltk
    
def preprocess(text):
    sentences = nltk.sent_tokenize(text)                     # sentence segmentation
    sentences = [nltk.word_tokenize(s) for s in sentences]   # word tokenization
    sentences = [nltk.pos_tag(s) for s in sentences]         # part-of-speech tagger
    return sentences

preprocessed = preprocess(message)

print(preprocessed) # &gt;&gt;&gt;&gt; [[('The', 'DT'), ('burglar', 'NN'), ('robbed', 'VBD'), ('the', 'DT'), ('bank', 'NN')]]
</code></pre>
<p>At this point, I have the sentences preprocessed and can define my grammar. It covers all the word in the example sentence as you can see here:</p>
<p><strong>2. Step: defining grammer</strong> <em>(no errors)</em></p>
<pre><code>grammar = nltk.CFG.fromstring(&quot;&quot;&quot;
S -&gt; NP VP
NP -&gt; DT NN
VP -&gt; VBD NP
DT -&gt; 'the' | 'The'
NN -&gt; 'burglar' | 'bank'
VBD -&gt; 'robbed'
&quot;&quot;&quot;)
</code></pre>
<p>But executing the actual parsing results in an error:</p>
<p><strong>3. Step: parsing</strong></p>
<pre><code>parser = nltk.ChartParser(grammar)

for sentence in preprocessed:
    for tree in parser.parse(sentence):
        print(tree)

# &gt;&gt;&gt;&gt; ValueError: Grammar does not cover some of the input words: &quot;('The', 'DT'), ('burglar', 'NN'), ('robbed', 'VBD'), ('the', 'DT'), ('bank', 'NN')&quot;.
</code></pre>
<p>I don't see why this error occurs. The words are clearly in the grammar.</p>
"
"66606563","Use SHAP values to explain LogisticRegression Classification","2021-03-12 20:09:44","3","3619","0","2","","66647619","<p>I am trying to do some bad case analysis on my product categorization model using SHAP. My data looks something like this:
<a href=""https://i.sstatic.net/DArig.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/DArig.png"" alt=""enter image description here"" /></a></p>
<pre><code>corpus_train, corpus_test, y_train, y_test = train_test_split(data['Name_Description'],
                                                              data['Category_Target'],
                                                              test_size = 0.2,
                                                              random_state=8)

vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 3), min_df=3, analyzer='word')

X_train = vectorizer.fit_transform(corpus_train)
X_test = vectorizer.transform(corpus_test)

model = LogisticRegression(max_iter=200)
model.fit(X_train, y_train)

X_train_sample = shap.sample(X_train, 100)
X_test_sample = shap.sample(X_test, 20)

masker = shap.maskers.Independent(data=X_test_sample)

explainer = shap.LinearExplainer(model, masker=masker)
shap_values = explainer.shap_values(X_test_sample)
X_test_array = X_test_sample.toarray()

shap.summary_plot(shap_values, X_test_array, feature_names=vectorizer.get_feature_names(), class_names=data['Category'].unique())
</code></pre>
<p>Now to save space I didn't include the actual summary plot, but it looks fine. My issue is that I want to be able to analyze a single prediction and get something more along these lines:</p>
<p><a href=""https://i.sstatic.net/Aboei.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Aboei.png"" alt=""enter image description here"" /></a></p>
<p>In other words, I want to know which specific words contribute the most to the prediction. But when I run the code in cell 36 in the image above I get an</p>
<pre><code>AttributeError: 'numpy.ndarray' object has no attribute 'output_names'
</code></pre>
<p>I'm still confused on the indexing of shap_values. How can I solve this?</p>
"
"66530272","Tokenize text - Very slow when doing it","2021-03-08 12:57:12","1","1625","6","2","","66530792","<p><strong>Question</strong></p>
<p>I have a data frame with +90,000 rows and with a column ['text'] that contains the text of some news.</p>
<p>The length of the text has an average of 3.000 words and when I pass the word_tokenize it makes it very slow, Which could be a more efficent method to do it?</p>
<pre class=""lang-py prettyprint-override""><code>from nltk.tokenize import word_tokenize
df['tokenized_text'] = df.iloc[0:10]['texto'].apply(word_tokenize) 
df.head()
</code></pre>
<p>Also word_tokenize hasn't some punctuations and other characters that I don't want, so I created a function to filter them where I'm using spacy.</p>
<pre class=""lang-py prettyprint-override""><code>from spacy.lang.es.stop_words import STOP_WORDS
from nltk.corpus import stopwords
spanish_stopwords = set(stopwords.words('spanish'))
otherCharacters = ['`','�',' ','\xa0']
def tokenize(phrase):
    sentence_tokens = []
    tokenized_phrase = nlp(phrase)
    for token in tokenized_phrase:
        if ~token.is_punct or ~token.is_stop or ~(token.text.lower() in spanish_stopwords) or ~(token.text.lower() in otherCharacters) or ~(token.text.lower() in STOP_WORDS):
            sentence_tokens.append(token.text.lower())
    return sentence_tokens
</code></pre>
<p>Any other better method to do it?</p>
<p>Thanks for reading my maybe noob👨🏽‍💻 question😀, have a nice day🌻.</p>
<p><strong>Appreciations</strong></p>
<ol>
<li>nlp is defined before</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>import spacy
import es_core_news_sm
nlp = es_core_news_sm.load()
</code></pre>
<ol start=""2"">
<li>I'm using spacy to tokenize but also using the nltk stop_words for spanish language.</li>
</ol>
"
"66444961","Genome representation for a NLTK sentence","2021-03-02 18:19:29","0","84","0","1","","66547591","<p>Given a NLTK grammar, how can I represent a sentence using an arrays of integers?</p>
<p>I am using NLTK to generate some sentences from a specific grammar. I would like to generate an array of integers to represent a genome for a generated sentence (phenotype).</p>
<p>With that representation of integers I would evolve the genome in a Genetic Algorithm, performing some mutations to get better sentences.</p>
<p>For example,</p>
<pre class=""lang-py prettyprint-override""><code>from nltk import CFG
from nltk.parse.generate import generate, demo_grammar

g = CFG.fromstring(demo_grammar)
sentence = next(generate(g, n=1))

print(sentence) # ex: ['the', 'man', 'saw', 'the', 'park']

convert_to_genotype(sentence) # returns [253, 69, 221, 97, 190, 254, 67, 137, 95, 72, 54, 232, 11, 136] for example.
</code></pre>
<p>How can I create the <code>convert_to_genotype</code> function?</p>
<p>Thanks</p>
"
"66433496","How do I fix ValueError when doing nlp.add_pipe(LanguageDetector(), name='language_detector', last=True) with spacy 3","2021-03-02 04:48:05","9","14565","1","2","","66433607","<p>Every time I run the following code I found on Kaggle, I get <code>ValueError</code>. This is because of new version v3 of <strong>SpaCy</strong>:</p>
<pre><code>import scispacy
import spacy
import en_core_sci_lg
from spacy_langdetect import LanguageDetector

nlp = en_core_sci_lg.load(disable=[&quot;tagger&quot;, &quot;ner&quot;])
nlp.max_length = 2000000
nlp.add_pipe(LanguageDetector(), name='language_detector', last=True)
</code></pre>
<p>ValueError: [E966] <code>nlp.add_pipe</code> now takes the string name of the registered component factory, not a callable component. Expected string, but got &lt;spacy_langdetect.spacy_langdetect.LanguageDetector object at 0x00000216BB4C8D30&gt; (name: 'language_detector').</p>
<ul>
<li><p>If you created your component with <code>nlp.create_pipe('name')</code>: remove nlp.create_pipe and call <code>nlp.add_pipe('name')</code> instead.</p>
</li>
<li><p>If you passed in a component like <code>TextCategorizer()</code>: call <code>nlp.add_pipe</code> with the string name instead, e.g. <code>nlp.add_pipe('textcat')</code>.</p>
</li>
<li><p>If you're using a custom component: Add the decorator <code>@Language.component</code> (for function components) or <code>@Language.factory</code> (for class components / factories) to your custom component and assign it a name, e.g. <code>@Language.component('your_name')</code>. You can then run <code>nlp.add_pipe('your_name')</code> to add it to the pipeline.</p>
</li>
</ul>
<p>I have installed these versions:</p>
<pre><code>python_version : 3.8.5
spacy.version  : '3.0.3'
scispacy.version  :  '0.4.0'
en_core_sci_lg.version  :  '0.4.0'
</code></pre>
"
"66356878","How to split a string in a pandas dataframe into bigrams that can then exploded into new rows?","2021-02-24 18:36:09","2","431","11","2","","66356985","<p>I have a database of name records that I'm trying to create bigrams for and have the bigrams turned into new rows in the dataframe. The reason I'm doing this is because there are certain records that contain multiple names and also some can have different orders for the same name. My ultimate goal is to look for duplicates and create one ultimate record for each unique individual. I plan to use TF-IDF and cosine similarity on the results of this. Below is an example of what I'm trying to do.</p>
<p>Current:
<a href=""https://i.sstatic.net/KMfna.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/KMfna.png"" alt=""enter image description here"" /></a></p>
<p>Goal:
<a href=""https://i.sstatic.net/IFRZH.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/IFRZH.png"" alt=""enter image description here"" /></a></p>
"
"66350670","Understanding TfidfVectorizer output","2021-02-24 12:10:49","1","1195","0","1","","66352012","<p>I'm testing <code>TfidfVectorizer</code> with simple example, and I can't figure out the results.</p>
<pre><code>corpus = [&quot;I'd like an apple&quot;,
          &quot;An apple a day keeps the doctor away&quot;,
          &quot;Never compare an apple to an orange&quot;,
          &quot;I prefer scikit-learn to Orange&quot;,
          &quot;The scikit-learn docs are Orange and Blue&quot;]
vect = TfidfVectorizer(min_df=1, stop_words=&quot;english&quot;)
tfidf = vect.fit_transform(corpus)

print(vect.get_feature_names())    
print(tfidf.shape)
print(tfidf)
</code></pre>
<p>output:</p>
<pre><code>['apple', 'away', 'blue', 'compare', 'day', 'docs', 'doctor', 'keeps', 'learn', 'like', 'orange', 'prefer', 'scikit']
(5, 13)
  (0, 0)    0.5564505207186616
  (0, 9)    0.830880748357988
  ...
</code></pre>
<p>I'm calculating the <code>tfidf</code> of the first sentence and I'm getting different results:</p>
<ul>
<li>The first document (&quot;<code>I'd like an apple</code>&quot;) contains just 2 words (after removeing stop words (according to the print of <code>vect.get_feature_names()</code> (we stay with: &quot;<code>like</code>&quot;, &quot;<code>apple</code>&quot;)</li>
<li>TF(&quot;apple&quot;, Doucment_1) = 1/2 = 0.5</li>
<li>TF(&quot;like&quot;, Doucment_1) = 1/2 = 0.5</li>
<li>The word <code>apple</code> appears 3 times in the corpus.</li>
<li>The word <code>like</code> appears 1 time in the corpus.</li>
<li>IDF (&quot;apple&quot;) = ln(5/3) = 0.51082</li>
<li>IDF (&quot;like&quot;) = ln(5/1) = 1.60943</li>
</ul>
<p>so:</p>
<ul>
<li><code>tfidf(&quot;apple&quot;)</code> in document1 = 0.5 * 0.51082 = 0.255 != 0.5564</li>
<li><code>tfidf(&quot;like&quot;)</code> in document1 = 0.5 * 1.60943 = 0.804 != 0.8308</li>
</ul>
<p>What am I missing ?</p>
"
"66332810","spacy lemmatization of nouns and noun chunks","2021-02-23 12:06:14","4","1296","0","1","","66839470","<p>I am trying to create a corpus of documents which consists of lemmatized nouns and noun-chunks. I am using this code:</p>
<pre><code>import spacy
nlp = spacy.load('en_core_web_sm')

def lemmatizer(doc, allowed_postags=['NOUN']):                                                     
    doc = [token.lemma_ for token in doc if token.pos_ in allowed_postags]
    doc = u' '.join(doc)
    return nlp.make_doc(doc)


nlp.add_pipe(nlp.create_pipe('merge_noun_chunks'), after='ner')
nlp.add_pipe(lemmatizer, name='lemm', after='merge_noun_chunks')

doc_list = []                                                                                      
for doc in data:                                                                                    
    pr = nlp(doc)
    doc_list.append(pr) 

   
</code></pre>
<p>The sentence <code>'the euro area has advanced a long way as a monetary union'</code> after identifiying noun-chunks <code>['the euro area', 'advanced', 'long', 'way', 'a monetary union']</code> and lemmatization gets to: <code>['euro', 'area', 'way', 'monetary', 'union']</code>.
Is there a way to combine the words of the identified noun-chunks to get an output like this: <code>['the euro area','way', 'a monetary union']</code> or <code>['the_euro_area','way', 'a_monetary_union']</code>?</p>
<p>Thanks for your help!</p>
"
"66326872","TfidfVectorizer shrinks my datafreame from 799 to 3","2021-02-23 04:01:06","0","24","0","1","","66326947","<p>I have dataframe that has text columns</p>
<p>and multilabel values</p>
<p>RepID, RepText,                                     Code
1      This is a test. thanks for purchasing...     Fruit, Meat
2      Purchased Milk, and Bananas, I also p...     Dairy, Fruit, Others</p>
<p>Here is my code</p>
<pre><code>######## df has 1000 records

multilabel_binarizer = MultiLabelBinarizer()
multilabel_binarizer.fit(df['Code'])
y = multilabel_binarizer.transform(df['Code'])
X = df[df.columns.difference([&quot;Code&quot;])]

######## df split into X (RepID, RepText)
######## and y (Code)


xtrain, xval, ytrain, yval = train_test_split(X, y, test_size=0.2, random_state=9)

##### xtrain.shape = (800,3)
##### xval.shape = (200,3)
##### ytrain.shape = (800,1725)
##### yval.shape = (200,1725)


tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=10000)
xtrain_tfidf = tfidf_vectorizer.fit_transform(xtrain)
xval_tfidf = tfidf_vectorizer.transform(xval)

##### But after the code above
##### xtrain_tfidf.shape = (3,3)
##### xval_tfidf.shape = (3,3)
##### ytrain.shape = (800,1725)
##### yval.shape = (200,1725)

##### when means when I do the next line

xval_tfidf.shape

#mdl = LinearRegression()
mdl = LogisticRegression()
#mdl = SVC(gamma='auto', probability=True)
clf = OneVsRestClassifier(mdl)
clf.fit(xtrain_tfidf, ytrain)
</code></pre>
<p>I get this error</p>
<pre><code>ValueError: Found input variables with inconsistent numbers of samples: [3, 799]
</code></pre>
<p>Why? why am I getting only <strong>3 records</strong> instead of <strong>800</strong> after <em>TfidfVectorizer</em> lines?</p>
<p>When I tried to view what is in <em>xtrain_tfidf</em>, I got this</p>
<pre><code>xtrain_tfidf
Out[56]: 
&lt;3x3 sparse matrix of type '&lt;class 'numpy.float64'&gt;'
    with 3 stored elements in Compressed Sparse Row format&gt;
</code></pre>
"
"66267633","Textual Data Augmentation in Tensorflow","2021-02-18 20:24:15","0","441","0","1","","67245903","<p>I'm doing a sentiment analysis on the IMDB dataset in tensorflow and I'm trying to augment the training dataset by using the <a href=""https://github.com/dsfsi/textaugment#Implementation"" rel=""nofollow noreferrer"">textaugment library</a> which they said is 'plug and play' into tensorflow. So it should be rather simple, but I'm new to tf so I'm not sure how to go about doing that.  Here is what I have and what I am trying, based on reading the tutorials on the site.</p>
<p>I tried to do a map to augment the training data but I got an error.  You can scroll down to the last code block to see the error.</p>
<pre><code>pip install -q tensorflow-text
pip install -q tf-models-official
import os
import shutil
import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_text as text
from official.nlp import optimization # to create AdamW Optimizer
import matplotlib.pyplot as plt

tf.get_logger().setLevel('ERROR')
</code></pre>
<p>#Downloading the  IMDB dataset and making the train/validation/test sets</p>
<pre><code>url = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'

dataset = tf.keras.utils.get_file('aclImdb_v1.tar.gz', url,
                                  untar=True, cache_dir='.',
                                  cache_subdir='')

dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')

train_dir = os.path.join(dataset_dir, 'train')

# remove unused folders to make it easier to load the data
remove_dir = os.path.join(train_dir, 'unsup')
shutil.rmtree(remove_dir)


AUTOTUNE = tf.data.AUTOTUNE
batch_size = 32
seed = 42

raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(
    'aclImdb/train',
    batch_size=batch_size,
    validation_split=0.2,
    subset='training',
    seed=seed)

class_names = raw_train_ds.class_names
train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)

val_ds = tf.keras.preprocessing.text_dataset_from_directory(
    'aclImdb/train',
    batch_size=batch_size,
    validation_split=0.2,
    subset='validation',
    seed=seed)

val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)

test_ds = tf.keras.preprocessing.text_dataset_from_directory(
    'aclImdb/test',
    batch_size=batch_size)

test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)


#setting up the textaugment
try:
  import textaugment
except ModuleNotFoundError:
  !pip install textaugment
  import textaugment
from textaugment import EDA
import nltk
nltk.download('stopwords')
</code></pre>
<p>Now this is where I get the error, I tried a map on the train_ds and tried to add a random swap to each of the elements while keeping the class the same:</p>
<pre><code>aug_ds = train_ds.map(
    lambda x, y: (t.random_swap(x), y))
</code></pre>
<p>Error Message:</p>
<pre><code>AttributeError                            Traceback (most recent call last)
&lt;ipython-input-24-b4af68cc0677&gt; in &lt;module&gt;()
      1 aug_ds = train_ds.map(
----&gt; 2     lambda x, y: (t.random_swap(x), y))

10 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py in wrapper(*args, **kwargs)
    668       except Exception as e:  # pylint:disable=broad-except
    669         if hasattr(e, 'ag_error_metadata'):
--&gt; 670           raise e.ag_error_metadata.to_exception(e)
    671         else:
    672           raise

AttributeError: in user code:

    &lt;ipython-input-24-b4af68cc0677&gt;:2 None  *
        lambda x, y: (t.random_swap(x), y))
    /usr/local/lib/python3.6/dist-packages/textaugment/eda.py:187 random_swap  *
        self.validate(sentence=sentence, n=n)
    /usr/local/lib/python3.6/dist-packages/textaugment/eda.py:74 validate  *
        if not isinstance(kwargs['sentence'].strip(), str) or len(kwargs['sentence'].strip()) == 0:

    AttributeError: 'Tensor' object has no attribute 'strip'
</code></pre>
"
"66213829","ValueError: Shape of passed values is, indices imply","2021-02-15 18:56:59","1","6102","6","1","","66214452","<p>Reposting again because i didn't get a response to the first post</p>
<p>I have the following data is below:</p>
<pre><code>desc = pd.DataFrame(description, columns =['new_desc'])

                                             new_desc
257623  the public safety report is compiled from crim...
161135  police say a sea isle city man ordered two pou...
156561  two people are behind bars this morning, after...
41690   pumpkin soup is a beloved breakfast soup in ja...
70092   right now, 15 states are grappling with how be...
...                                                   ...
207258  operation legend results in 59 more arrests, i...
222170                                      see story, 3a
204064  st. louis — missouri secretary of state jason ...
151443  tony lavell jones, 54, of sunset view terrace,...
97367   walgreens, on the other hand, is still going t...

[9863 rows x 1 columns]
</code></pre>
<p>I'm trying to find the dominant topic within the documents, and When I run the following code</p>
<pre><code>best_lda_model = lda_desc
data_vectorized = tfidf
lda_output = best_lda_model.transform(data_vectorized)
topicnames = [&quot;Topic &quot; + str(i) for i in range(best_lda_model.n_components)]
docnames = [&quot;Doc &quot; + str(i) for i in range(len(dataset))]
df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns = topicnames, index = docnames)
dominant_topic = np.argmax(df_document_topic.values, axis = 1)
df_document_topic['dominant_topic'] = dominant_topic
</code></pre>
<p>I've tried tweaking the code, however, no matter what I change, I get the following error tracebook error</p>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
c:\python36\lib\site-packages\pandas\core\internals\managers.py in create_block_manager_from_blocks(blocks, axes)
   1673 
-&gt; 1674         mgr = BlockManager(blocks, axes)
   1675         mgr._consolidate_inplace()

c:\python36\lib\site-packages\pandas\core\internals\managers.py in __init__(self, blocks, axes, do_integrity_check)
    148         if do_integrity_check:
--&gt; 149             self._verify_integrity()
    150 

c:\python36\lib\site-packages\pandas\core\internals\managers.py in _verify_integrity(self)
    328             if block.shape[1:] != mgr_shape[1:]:
--&gt; 329                 raise construction_error(tot_items, block.shape[1:], self.axes)
    330         if len(self.items) != tot_items:

ValueError: Shape of passed values is (9863, 8), indices imply (0, 8)

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
&lt;ipython-input-41-bd470d69b181&gt; in &lt;module&gt;
      4 topicnames = [&quot;Topic &quot; + str(i) for i in range(best_lda_model.n_components)]
      5 docnames = [&quot;Doc &quot; + str(i) for i in range(len(dataset))]
----&gt; 6 df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns = topicnames, index = docnames)
      7 dominant_topic = np.argmax(df_document_topic.values, axis = 1)
      8 df_document_topic['dominant_topic'] = dominant_topic

c:\python36\lib\site-packages\pandas\core\frame.py in __init__(self, data, index, columns, dtype, copy)
    495                 mgr = init_dict({data.name: data}, index, columns, dtype=dtype)
    496             else:
--&gt; 497                 mgr = init_ndarray(data, index, columns, dtype=dtype, copy=copy)
    498 
    499         # For data is list-like, or Iterable (will consume into list)

c:\python36\lib\site-packages\pandas\core\internals\construction.py in init_ndarray(values, index, columns, dtype, copy)
    232         block_values = [values]
    233 
--&gt; 234     return create_block_manager_from_blocks(block_values, [columns, index])
    235 
    236 

c:\python36\lib\site-packages\pandas\core\internals\managers.py in create_block_manager_from_blocks(blocks, axes)
   1679         blocks = [getattr(b, &quot;values&quot;, b) for b in blocks]
   1680         tot_items = sum(b.shape[0] for b in blocks)
-&gt; 1681         raise construction_error(tot_items, blocks[0].shape[1:], axes, e)
   1682 
   1683 

ValueError: Shape of passed values is (9863, 8), indices imply (0, 8)
</code></pre>
<p>The desired results is to produce a list of documents according to a specific topic. Below is example code and desired output.</p>
<pre><code>df_document_topic(df_document_topic['dominant_topic'] == 2).head(10)
</code></pre>
<p>When I run this code, I get the following traceback</p>
<pre><code>TypeError                                 Traceback (most recent call last)
&lt;ipython-input-55-8cf9694464e6&gt; in &lt;module&gt;
----&gt; 1 df_document_topic(df_document_topic['dominant_topic'] == 2).head(10)

TypeError: 'DataFrame' object is not callable
</code></pre>
<p>Below is the desired output</p>
<p><a href=""https://i.sstatic.net/hOh5P.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/hOh5P.png"" alt=""enter image description here"" /></a></p>
<p>Any help would be greatly appreciated.</p>
"
"66204478","input/output/recurrent dropout layers in BiLSTM_Classifier and how they affect the model and prediction","2021-02-15 08:03:44","2","353","0","1","","66624185","<p>I would like to have some understanding/information on the input/output/recurrent dropout layers in BiLSTM_Classifier and how they affect the model and prediction.</p>
<pre><code># Output drop out
model_out_dp = Sequential()
model_out_dp.add(Embedding(vocab_size, embedding_dim, input_length=maxlen,weights=[embedding_matrix],trainable=False))
model_out_dp.add(Bidirectional(LSTM(64)))
model_out_dp.add(Dropout(0.5))
model_out_dp.add(Dense(8, activation='softmax'))

# input drop out
model_input_dp = Sequential()
model_input_dp.add(Embedding(vocab_size, embedding_dim, input_length=maxlen,weights=[embedding_matrix],trainable=False))
model_input_dp.add(Bidirectional(LSTM(64,dropout=0.5)))
model_input_dp.add(Dense(8, activation='softmax'))

# recurrent drop out
model_rec_dp = Sequential()
model_rec_dp.add(Embedding(vocab_size, embedding_dim, input_length=maxlen,weights=[embedding_matrix],trainable=False))
model_rec_dp.add(Bidirectional(LSTM(64,recurrent_dropout=0.5)))
model_rec_dp.add(Dense(8, activation='softmax'))

</code></pre>
"
"66199151","Python: Improving performance of code performing spelling correction on text data","2021-02-14 19:17:27","0","912","0","1","","66210534","<p>I have a text-data in form of comments that I want to preprocess. Apart from cutting away noise like URLs, numbers, ... and performing lemmatization, I also want to perform spelling correction. Specifically, I want to perform spelling correction only on words that do not occur more often than a given number of times to avoid false positives. For that purpose, I use <a href=""https://pyspellchecker.readthedocs.io/en/latest/code.html#"" rel=""nofollow noreferrer"">pyspellchecker</a> for the correction and <a href=""https://www.kite.com/python/docs/nltk.FreqDist"" rel=""nofollow noreferrer"">nltks FreqDist</a> to get word frequencies, however, doing that increases the time needed for preprocessing significantly.</p>
<p>I tried making things as performant as I could, but I am stuck and was wondering if there are still improvements I could make.</p>
<p>Here is my code:
Imports:</p>
<pre><code>from spacy.lang.en import English
from spellchecker import SpellChecker
from nltk.probability import FreqDist
nlp = spacy.load(&quot;en_core_web_sm&quot;)
spell = SpellChecker()
fdist = FreqDist()
</code></pre>
<p>Code:</p>
<pre><code>dict_misspell = {}

pipe = nlp.pipe(list_of_comments, batch_size = 512 ,disable = [&quot;tagger&quot;, &quot;parser&quot;])
    for j, doc in enumerate(pipe):
        tokens = [token.lemma_.lower() for token in doc if not token.is_punct and not token.is_digit\
                                  and not token.like_url and not token.like_email and not token.like_num]
        processed_comments.append(&quot; &quot;.join(tokens))
        fdist += FreqDist(tokens)
        
        #remember which comments contain missspellings to avoid having to look at every comment later
        misspelled = spell.unknown(tokens)
        if (len(misspelled) &gt; 0):
            for misspelled_word in misspelled:
                if misspelled_word in dict_misspell.keys():
                    dict_misspell[misspelled_word].append(j)
                else:
                    dict_misspell[misspelled_word] = [j]
    
    #spell correction is done after the rest because only then is the frequency dict fully build.
    for k, mis in enumerate(dict_misspell.keys()):
        if(fdist[mis] &lt;= 5):  #only fix below certain word frequency to avoid false positives
            missspelling_idxs = dict_misspell[mis]
            correct_spelling = spell.correction(mis)
            for idx in missspelling_idxs:
                processed_comments[idx] = processed_comments[idx].replace(mis, correct_spelling)
</code></pre>
<p>As you can see above, I preprocess each individual comment, add all words of that comment to the frequency dictionary and for each word that the spellchecker considers misspelled I save those words and the index of the comment in which they occur in a misspell dictionary. After doing that the frequency dictionary is fully built and I start correcting possibly misspelled words who's frequency meet a condition in the individual comments.</p>
<p>Does anyone see a way to improve performance here?</p>
"
"66091139","How to find important words using TfIdfVectorizer?","2021-02-07 17:46:20","3","1870","0","1","","66091377","<p>Consider the below example. the important words which represent the documents are 'Bob' and 'Sara'. but with the <code>max_features</code>, the output tends to show frequent words. This will get worse when the corpus is big. How can we only get the important words?</p>
<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd


corpus = [
    'hi, my name is Bob.',
    'hi, my name is Sara.'
]

vectorizer = TfidfVectorizer(max_features=2)
X = vectorizer.fit_transform(corpus).todense()


df = pd.DataFrame(X, columns=vectorizer.get_feature_names())
</code></pre>
<p>The output:</p>
<pre><code>,hi,is
0,0.7071067811865475,0.7071067811865475
1,0.7071067811865475,0.7071067811865475
</code></pre>
"
"66085788","how to make dataset like this in tensorflow2: <PrefetchDataset shapes: ((), ()), types: (tf.string, tf.string)>","2021-02-07 08:10:50","1","1989","0","1","","66086250","<p>I want to make my own dataset when doing translation in NLP. For example,
x = [&quot;It is an apple&quot;]
y = [&quot;It is a pear&quot;].
How show I make a dataset which can fit &quot;&lt;PrefetchDataset shapes: ((), ()), types: (tf.string, tf.string)&gt;&quot;.</p>
"
"66034713","How can I put every sentence of a text in a row of a table next to its translation in a Google Document?","2021-02-03 19:52:24","-2","323","0","1","","66035638","<p>I want to put a text to be translated in a Google Doc so that every sentence is in its own row of a table. In each row of the next column is the corresponding, machine-translated sentence. Finally, there is a third blank column where I will write my own translation for each sentence.</p>
<p><a href=""https://i.sstatic.net/f07Dj.png"" rel=""nofollow noreferrer"">Like so</a></p>
<p>What would some command line code look like for accessing the specific Google Doc, inserting a table, splitting the text on each sentence, and then writing each sentence to a row of the table?</p>
<p>I am aware of Google API, but I have struggled with authentication problems so far. If someone can sketch out a general outline of what my script should look like, hopefully I can fill in the details.</p>
<p>I am trying my best to meet Stack Overflow's post guidelines, so if the question is not a good one, I'm happy to reformulate it. My question is multi-part, so I prefer to ask the broader version first, before breaking it into smaller aspects.</p>
"
"65677406","Why would a scaled SVD run so much slower than an unscaled SVD in a Random Forest model?","2021-01-12 02:54:59","1","549","0","1","","65678335","<p>I'm studying machine learning and NLP in Python by recreating the common &quot;predict spam messages&quot; project. I did all the preliminary steps of cleanup and preprocessing until I got a TF-IDF document-term matrix of 2,000 terms. I then performed SVD to reduce it to 300 terms (or components) and scaled the results so that I could run a quick logistic classifier to get a benchmark for later models.</p>
<p>Later in the project, while building random forests, I realized I had forgotten to comment out the scaler below and was building the forests with the scaled SVD, which is totally unnecessary. However, I did not realize this would slow down the random forests compared to the unscaled SVD, and worse, sensitivity was about 10% lower as well.</p>
<p>Can anyone help me understand <strong>why this is so</strong>?</p>
<p>Here are results of the grid search with the best (highest sensitivity) unscaled SVD:</p>
<pre><code>Elapsed: 1348 s
Best params: {'max_depth': 20, 'max_features': 250, 'min_samples_split': 10, 'n_estimators': 200}
Confusion matrix on validation set:
     pred_neg  pred_pos
neg       844         2
pos         5       124
Evaluation metrics:
accuracy: 0.9928
sensitivity: 0.9612
specificity: 0.9976
</code></pre>
<p>Here are the results of the grid search with the best (highest sensitivity) scaled SVD:</p>
<pre><code>Elapsed: 5297 s
Best params: {'max_depth': 5, 'max_features': 250, 'min_samples_split': 5, 'n_estimators': 200}
Confusion matrix on validation set:
     pred_neg  pred_pos
neg       838         8
pos        18       111
Evaluation metrics:
accuracy: 0.9733
sensitivity: 0.8605
specificity: 0.9905
</code></pre>
<p>Here's the culprit:</p>
<pre><code>from scipy.sparse.linalg import svds
from sklearn.utils.extmath import svd_flip
from sklearn.preprocessing import MaxAbsScaler

def perform_SVD(X, n_components=300):

    # transpose to a term-document matrix
    U, Sigma, VT = svds(X.asfptype().T, 
                        k=n_components)
    # reverse outputs
    Sigma = Sigma[::-1]
    U, VT = svd_flip(U[:, ::-1], VT[::-1])
    
    # transpose to get V
    V = VT.T
    
    # scale for logistic classifier only
    # can't take log of negative numbers
    # ends up predicting ham base rate
    # comment out for random forests!
    scaler = MaxAbsScaler()
    X_scaled = scaler.fit_transform(V) 
    
    return X_scaled
</code></pre>
"
"65645289","NLP | LimeTextExplainer for bigrams","2021-01-09 17:02:46","2","735","0","1","","65689039","<p>in my NLP task I want to understand the 'rule' of my classifier. For that purpose, I build a LimeTExtExplainer.</p>
<pre><code>c= make_pipeline(cv,naive_bayes)
explainer = LimeTextExplainer(class_names=class_names, random_state=42, bow=False)
exp = explainer.explain_instance(X_test[i], c.predict_proba, num_features=20,) 
fig = exp.as_pyplot_figure()
</code></pre>
<p>The above code creats a nice list of 1grams, exactly as I wanted.
:<a href=""https://i.sstatic.net/Nb7HF.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Nb7HF.png"" alt=""enter image description here"" /></a></p>
<p>In a next step I want to do the same, but with bigrams. I changed the feature extractor to only calculate bigrams:</p>
<pre><code>cv = CountVectorizer(strip_accents='ascii', analyzer='word',                                    
                 token_pattern=u'(?ui)\\b\\w*[a-z]+\\w*\\b',                                
                 lowercase=True, stop_words='english',                                      
                 ngram_range=(2,2), max_features=None)
</code></pre>
<p>The problem(s):</p>
<ol>
<li>I use the same code for the Limeexplainer as above. But now, the
graph only shows 1grams as before, but I only calculated bigrams.</li>
<li>As a side question, the horizontal axis of the graphs displays the
absolute probability that the word  accounts to the classification
probability? For instance, the texts class X probabilty is 0.67,
recognit accounts for ~ 0.009 and langugage for ~ 0.007 of the 0.67,
right?</li>
</ol>
<p>Thanks in advance!</p>
"
"65600562","Stemming on tokenized words","2021-01-06 17:30:36","0","215","4","1","","65600943","<p>Having this dataset:</p>
<pre><code>&gt;cleaned['text']
0         [we, have, a, month, open, #postdoc, position,...
1         [the, hardworking, biofuel, producers, in, iow...
2         [the, hardworking, biofuel, producers, in, iow...
3         [in, today, s, time, it, is, imperative, to, r...
4         [special, thanks, to, gaetanos, beach, club, o...
                                ...                        
130736    [demand, gw, sources, fossil, fuels, renewable...
130737         [there, s, just, not, enough, to, go, round]
130738    [the, answer, to, deforestation, lies, in, space]
130739    [d, filament, from, plastic, waste, regrind, o...
130740          [gb, grid, is, generating, gw, out, of, gw]
Name: text, Length: 130741, dtype: object
</code></pre>
<p>Is there a simple way to stem all the words?</p>
"
"65488631","AttributeError: 'WordList' object has no attribute 'split'","2020-12-29 07:14:28","0","203","0","1","","65488746","<p>I am trying to apply Lemmatization after I tokenized my &quot;script&quot; column. But I get an AttributeError. I tried different thins</p>
<p>Here is my &quot;script&quot; column:</p>
<pre><code>df_toklem[&quot;script&quot;][0:5]
---------------------------------------------------------------------------
type(df_toklem[&quot;script&quot;])
</code></pre>
<p>Output:</p>
<pre><code>id
1    [ext, street, day, ups, man, big, pot, belly, ...
2    [credits, still, life, tableaus, lawford, n, h...
3    [fade, ext, convent, day, whispering, nuns, pr...
4    [fade, int, c, hercules, turbo, prop, night, e...
5    [open, theme, jaws, plane, busts, clouds, like...
Name: script, dtype: object
---------------------------------------------------------------------------
pandas.core.series.Series
</code></pre>
<p>And the code where I try to apply Lemmatization:</p>
<pre><code>from textblob import Word
nltk.download(&quot;wordnet&quot;)
df_toklem[&quot;script&quot;].apply(lambda x: &quot; &quot;.join([Word(word).lemmatize() for word in x.split()]))
</code></pre>
<p><strong>ERROR:</strong></p>
<pre><code>[nltk_data] Downloading package wordnet to
[nltk_data]     C:\Users\PC\AppData\Roaming\nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-72-dbc80c619ec5&gt; in &lt;module&gt;
      1 from textblob import Word
      2 nltk.download(&quot;wordnet&quot;)
----&gt; 3 df_toklem[&quot;script&quot;].apply(lambda x: &quot; &quot;.join([Word(word).lemmatize() for word in x.split()]))

~\Anaconda3\lib\site-packages\pandas\core\series.py in apply(self, func, convert_dtype, args, **kwds)
   4198             else:
   4199                 values = self.astype(object)._values
-&gt; 4200                 mapped = lib.map_infer(values, f, convert=convert_dtype)
   4201 
   4202         if len(mapped) and isinstance(mapped[0], Series):

pandas\_libs\lib.pyx in pandas._libs.lib.map_infer()

&lt;ipython-input-72-dbc80c619ec5&gt; in &lt;lambda&gt;(x)
      1 from textblob import Word
      2 nltk.download(&quot;wordnet&quot;)
----&gt; 3 df_toklem[&quot;script&quot;].apply(lambda x: &quot; &quot;.join([Word(word).lemmatize() for word in x.split()]))

AttributeError: 'WordList' object has no attribute 'split'
</code></pre>
<p>I tried different things but unfortunately couldn't find an efficient solution. Thank you for your time.</p>
"
"65484081","Translating using pre-trained hugging face transformers not working","2020-12-28 21:05:27","0","1342","1","1","","65490127","<p>I have a situation where I am trying to using the pre-trained hugging-face models to translate a pandas column of text from Dutch to English. My input is simple:</p>
<pre><code>Dutch_text             
Hallo, het gaat goed
Hallo, ik ben niet in orde
Stackoverflow is nuttig
</code></pre>
<p>I am using the below code to translate the above column and I want to store my result into a new column ENG_Text. So the output will look like this:</p>
<pre><code>ENG_Text             
Hello, I am good
Hi, I'm not okay
Stackoverflow is helpful
</code></pre>
<p>The code that I am using is as follows:</p>
<pre><code>#https://huggingface.co/Helsinki-NLP for other pretrained models 
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
tokenizer = AutoTokenizer.from_pretrained(&quot;Helsinki-NLP/opus-mt-nl-en&quot;)
model = AutoModelForSeq2SeqLM.from_pretrained(&quot;Helsinki-NLP/opus-mt-nl-en&quot;)
input_1 = df['Dutch_text']
input_ids = tokenizer(&quot;translate English to Dutch: &quot;+input_1, return_tensors=&quot;pt&quot;).input_ids # Batch size 1
outputs = model.generate(input_ids)
decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(decoded)
</code></pre>
<p>Any help would be appreciated!</p>
"
"65454578","What's the difference between NLTK's BLEU score and SacreBLEU?","2020-12-26 08:04:59","4","7065","0","1","","65476289","<p>I'm curious if anyone is familiar with the difference between using <a href=""https://www.nltk.org/_modules/nltk/translate/bleu_score.html"" rel=""nofollow noreferrer"">NLTK's BLEU score calculation</a> and the <a href=""https://github.com/mjpost/sacrebleu"" rel=""nofollow noreferrer"">SacreBLEU library</a>.</p>
<p>In particular, I'm using both library's sentence BLEU scores, averaged over the entire dataset. The two give different results:</p>
<pre><code>&gt;&gt;&gt; from nltk.translate import bleu_score
&gt;&gt;&gt; from sacrebleu import sentence_bleu
&gt;&gt;&gt; print(len(predictions))
256
&gt;&gt;&gt; print(len(targets))
256
&gt;&gt;&gt; prediction = &quot;this is the first: the world's the world's the world's the \
... world's the world's the world's the world's the world's the world's the world \
... of the world of the world'&quot;
...
&gt;&gt;&gt; target = &quot;al gore: so the alliance for climate change has launched two campaigns.&quot;
&gt;&gt;&gt; print(bleu_score.sentence_bleu([target], prediction))
0.05422283394039736
&gt;&gt;&gt; print(sentence_bleu(prediction, [target]).score)
0.0
&gt;&gt;&gt; print(sacrebleu.corpus_bleu(predictions, [targets]).score)
0.678758518214081
&gt;&gt;&gt; print(bleu_score.corpus_bleu([targets], [predictions]))
0
</code></pre>
<p>As you can see, there's a lot of confusing inconsistencies going on. There's no way that my BLEU score is 67.8%, but it's also not supposed to be 0% (there are a lot of overlapping n-grams like &quot;the&quot;).</p>
<p>I'd appreciate it if anyone could shed some light on this. Thanks.</p>
"
"65273410","Issue in creating Semgrex patterns with relation names containing "":"" colon","2020-12-13 07:39:34","0","132","0","1","","65361196","<p>I am trying to perform Semgrex in <a href=""https://corenlp.run/"" rel=""nofollow noreferrer"">https://corenlp.run/</a> on the below sentence to extract the transition event. Since the dependency relation <strong>&quot;obl:from&quot;</strong> has a colon in it, I get an error. But instead, if I used nsubj, I get the desired result. Can someone tell me how to work around this?</p>
<p><em>My text: The automobile shall change states from OFF to ON when the driver is in control.</em></p>
<pre><code>{} &lt;&lt;nsubj {}
{} &lt;&lt;obl:from  {}    
</code></pre>
<p><a href=""https://i.sstatic.net/1JyVK.png"" rel=""nofollow noreferrer"">working scenario screenshot</a>
<a href=""https://i.sstatic.net/Y3gxP.png"" rel=""nofollow noreferrer"">Issue scenario screenshot</a></p>
"
"65160400","How to modify word in a for loop in python","2020-12-05 18:16:11","-1","70","1","1","","65160499","<p>Im trying to stem some text in python with SnowballStemmer, but it wont work. Here is the code:</p>
<pre><code>import nltk
from nltk import SnowballStemmer

stem = SnowballStemmer(&quot;spanish&quot;)

def limpiar (texto):
  texto = texto.split()
  stemm = SnowballStemmer('spanish')
  for palabra in texto:
    palabra = stem.stem(palabra.lower())
    
  return texto
</code></pre>
<p>It returns the text in lower capitals, but without stemming</p>
"
"65153422","Create a vocabulary with pos","2020-12-05 03:19:02","0","237","0","1","","65160717","<p>I would like to create a list of semantic entities (nouns, verbs, punct, etc.) using pos tagging.
I am currently running the following code</p>
<pre><code>import spacy
import pandas as pd
    
nlp = spacy.load('en_core_web_sm',disable=['ner','textcat'])

def fun(text):
    doc = nlp(text)
    pos = &quot;&quot;
    for token in doc:
        pos += token.pos_ + &quot; &quot;
    return pos

df['S']= df.Text.apply(fun)
</code></pre>
<p>to create the structure of sentences.
So, for example, if I have the column Text (see below), this code generate the column S which contains all the information about semantic structure:</p>
<pre><code>Text                                                S
0   “I will meet quite a few people, it’s well...   PUNCT NOUN VERB VERB DET DET ADJ NOUN PUNCT PR...
1   Says “Cristiano Ronaldo’s family still owns”... VERB PUNCT PROPN PROPN PART NOUN ADV VERB PUNC...
2   Joe Biden plagiarized Donald Trump in his... PROPN PROPN VERB PROPN PROPN ADP DET PROP...
</code></pre>
<p>I am wondering if I can create a vocabulary of nouns, verbs, det, adj, ... by editing the code above or if I need to consider a different approach.
To take all the entities (nouns, verbs,...) in the dataframe, I would look at selecting only unique values, in order to creat a list for each of them.</p>
<p>Example of output (it can be also in lists rather than in a dataframe)</p>
<pre><code>PUNCT      NOUN        VERB         ....
“           I          will 
,          people      meet
”          family      says
                       owns
                      plagiarized
</code></pre>
"
"65143979","How to improve my multiclass text-classification on German text?","2020-12-04 12:55:48","0","730","0","1","","65145747","<p>I am new in NLP and it is a bit confusing me.
I am trying to do a text classification with SVC on my dataset.
I have an imbalanced dataset of 6 classes.
The text is news for classes of health, sport, culture, economy, science and web.
I am using TF-IDF for vectorization.</p>
<p>the preprocessing steps: <code>lower-case</code> all the texts and to remove the <code>stop-words</code>. since my text is in German I did not use <code>lemmatization</code></p>
<p>my first try:</p>
<pre><code>from sklearn.model_selection import train_test_split

train, test = train_test_split(df, test_size=0.2, random_state=42)
X_train = train['text']
y_train = train['category']
X_test = test['text']
y_test = test['category']

# Linear SVC:
text_clf_lsvc = Pipeline([('tfidf', TfidfVectorizer()), ('clf', LinearSVC()),])
predictions = text_clf_lsvc.predict(X_test)
</code></pre>
<p>my metrci acuuracy score was: 93%</p>
<p>then I decided to reduce the dimensionality: so on my 2nd try I added TruncatedSVD</p>
<pre><code># Linear SVC:
text_clf_lsvc = Pipeline([('tfidf', TfidfVectorizer()),('svd', TruncatedSVD()),
                     ('clf', LinearSVC()),])
predictions = text_clf_lsvc.predict(X_test)
</code></pre>
<p>my metrci acuuracy score dropped to 34%.</p>
<p><em><strong>my questions:</strong></em></p>
<p>1- How can I improve my model if I want to stick to TF-IDF and SVC for classification<br />
2- What can I do other than that if I want to have a good classification</p>
"
"65068353","Python implementation for the CYK Algorithm","2020-11-30 05:05:36","0","4649","0","1","","65074546","<p>EDIT: error is this line  <code>if len(rhs) == 2 and rhs[0] in T[i][k] and rhs[1] in T[k + 1][j]:</code></p>
<p>I was able to implement the cky algorithm based off of the cky parser wiki with a small set of rules, terminals, and non terminals. But I scaled it to have more rules, words, grammar, and now it is giving me
<code>IndexError: list index out of range </code>
Does anyone have any idea of what I'm doing wrong with a bigger grammar set?</p>
<p>Here is the previous smaller scale of grammar that works if that helps.</p>
<pre><code>non_terminals = [&quot;NP&quot;, &quot;Nom&quot;, &quot;Det&quot;, &quot;AP&quot;,  
                  &quot;Adv&quot;, &quot;A&quot;] 
terminals = [&quot;book&quot;, &quot;orange&quot;, &quot;man&quot;,  
             &quot;tall&quot;, &quot;heavy&quot;,  
             &quot;very&quot;, &quot;muscular&quot;] 
  
# Rules of the grammar 
R = { 
     &quot;NP&quot;: [[&quot;Det&quot;, &quot;Nom&quot;]], 
     &quot;Nom&quot;: [[&quot;AP&quot;, &quot;Nom&quot;], [&quot;book&quot;],  
             [&quot;orange&quot;], [&quot;man&quot;]], 
     &quot;AP&quot;: [[&quot;Adv&quot;, &quot;A&quot;], [&quot;heavy&quot;],  
            [&quot;orange&quot;], [&quot;tall&quot;]], 
     &quot;Det&quot;: [[&quot;a&quot;]], 
     &quot;Adv&quot;: [[&quot;very&quot;], [&quot;extremely&quot;]], 
     &quot;A&quot;: [[&quot;heavy&quot;], [&quot;orange&quot;], [&quot;tall&quot;],  
           [&quot;muscular&quot;]] 
    } 
</code></pre>
<p>Here's my function</p>
<p>def cykParse(w):
n = len(w)</p>
<pre><code># Initialize the table 
T = [[set([]) for j in range(n)] for i in range(n)] 

# Filling in the table 
for j in range(0, n): 

    # Iterate over the rules 
    for lhs, rule in R.items(): 
        for rhs in rule: 
              
            # If a terminal is found 
            if len(rhs) == 1 and rhs[0] == w[j]: 
                T[j][j].add(lhs) 

    for i in range(j, -1, -1):    
           
        # Iterate over the range i to j + 1    
        for k in range(i, j + 1):      

            # Iterate over the rules 
            for lhs, rule in R.items(): 
                for rhs in rule: 
                      
                    # If a terminal is found 
                    if len(rhs) == 2 and rhs[0] in T[i][k] and rhs[1] in T[k + 1][j]: 
                        T[i][j].add(lhs) 

# If word can be formed by rules  
# of given grammar 
if len(T[0][n-1]) != 0: 
    print(&quot;True&quot;) 
else: 
    print(&quot;False&quot;) 
</code></pre>
"
"65040696","spacy aggressive lemmatization and removing unexpected words","2020-11-27 16:30:28","0","2565","4","1","","65050601","<p>I am trying to clean some text data.
fisrt i removed the stop words, then i tried to Lemmatize the text. But words such as nouns are removed</p>
<p><strong>Sample Data</strong></p>
<p><a href=""https://drive.google.com/file/d/1p9SKWLSVYeNScOCU_pEu7A08jbP-50oZ/view?usp=sharing"" rel=""nofollow noreferrer"">https://drive.google.com/file/d/1p9SKWLSVYeNScOCU_pEu7A08jbP-50oZ/view?usp=sharing</a>
<strong>udpated Code</strong></p>
<pre><code># Libraries  
import spacy
import pandas as pd
import gensim
from gensim.utils import simple_preprocess
import nltk; nltk.download('stopwords')
from nltk.corpus import stopwords
stop_words = stopwords.words('english')
stop_words.extend(['covid', 'COVID-19', 'coronavirus'])

article= pd.read_csv(&quot;testdata.csv&quot;)
data = article.title.values.tolist()
nlp = spacy.load('en_core_web_sm')

def sent_to_words(sentences):
    for sentence in sentences:
      yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations

data_words = list(sent_to_words(data))

def remove_stopwords(texts):
    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]
data_words_nostops = remove_stopwords(data_words)
print (&quot;*** Text  After removing Stop words:   &quot;)
print(data_words_nostops)
def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV','PRON']):
    &quot;&quot;&quot;https://spacy.io/api/annotation&quot;&quot;&quot;
    texts_out = []
    for sent in texts:
        doc = nlp(&quot; &quot;.join(sent)) 
        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])
    return texts_out
data_lemmatized = lemmatization(data_words_nostops, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV','PRON'])
print (&quot;*** Text  After Lemmatization:   &quot;)

print(data_lemmatized)
</code></pre>
<p>The output after removing Stopwords is :</p>
<blockquote>
<p>[['qaia', 'flags', 'amman', 'melbourne', 'jetstar', 'flights', 'recovery', 'plan'],<br />
['western', 'amman', 'suburb', 'new','nsw', 'ground', 'zero', children],<br />
['flight', 'returned', 'amman','qaia', 'staff', 'contract','driving'], ]]</p>
</blockquote>
<p>The output after Lematization :</p>
<blockquote>
<p>[['flight', 'recovery', 'plan']</p>
</blockquote>
<blockquote>
<p>['suburb', 'ground']</p>
</blockquote>
<blockquote>
<p>['return', 'contract','driving']</p>
</blockquote>
<p>on each reacord I do not understand the following :</p>
<p><strong>-1st reord: why these words are removed: &quot;'qaia', 'flags', 'amman', 'melbourne', 'jetstar'</strong></p>
<p><strong>-2ed recored: essential words are reomved same as the first reord, Also, I was expecting children to convert to child</strong></p>
<p><strong>-3ed, &quot;driving&quot; is not converted to &quot;drive&quot;</strong></p>
<p>I was expecting that words will such as &quot;Amman&quot; will not removed, Also i am expecting the words will be converted from plural to singular. And the verbs will be converted to the infinitive ...</p>
<p>What i am missing here???
Thanx in advance</p>
"
"64994311","How to vectorize dictionary of word tokens (bag of words implementation)","2020-11-24 20:26:41","0","1378","2","2","","64994600","<p>I'm creating my own bag of words algorithm but I'm stuck. So far I've tokenized the words(A list of strings and a user inputted string) and put them in a dictionary. Now I would like to create word vectors where 0 indicates the word is not in the document and 1 means it's present. My idea is to create a zero vector the size of which corresponds to the amount of unique words. <strong>Then make copies of that base vector, update the values of the vector for each document, and store them in an array</strong>. This is the part where I'm stuck.</p>
<pre><code>import more_itertools as mit
import re
from collections import OrderedDict

def get_vector(lexicon, text):
   
    # Creates a dictionary with inital value 0 for all unique words in the vocabulary
    zero_vector = OrderedDict((token, 0) for token in lexicon)
    corpus_tokens = list(mit.collapse(text.split()))

def BoW(corpus: list, search_doc: str):
    
    word_count = {}
    
    # Regex to grab words here because its just a string
    search_doc_tokens = re.split(r'[-\s.,;!?]+', search_doc)
    
    # I have to do all this business here because it's a list of strings
    grab_words = [word.split() for word in corpus]
    corpus_tokens = list(mit.collapse(grab_words))
    
    # Concatenating the two lists
    vocabulary = corpus_tokens + search_doc_tokens
    
    # Filling dictionary
    for token in vocabulary:
        if token not in word_count:
            word_count[token] = 1
        else:
            word_count[token] += 1
                    
    
    # Unique words in vocab. Used determine size of zero vector
    lexicon = sorted(set(vocabulary))
    zero_vector = OrderedDict((token, 0) for token in lexicon)
    
    print(zero_vector)

documents = [&quot;This is a text document&quot;, &quot;This is another text document&quot;, &quot;Get the picture?&quot;]
BoW(documents, &quot;hello there&quot;) 
</code></pre>
"
"64866522","word in words.words() check too slow and inaccurate in Python","2020-11-16 22:16:07","0","727","6","1","","64866904","<p>I am having a dataset, which is consisting of two columns, one is a Myers-Briggs personality type and the other one is containing the last 50 tweets of that person. I have tokenized, removed the URLs and the stop words from the list, and lemmatized the words.</p>
<p>I am then creating a <code>collections.Counter</code> of the most common words and I am checking whether they are valid English words with <code>nltk</code>.</p>
<p>The problem is that checking if the word exists in the corpora vocabulary takes too much time and I also think that a lot of words are missing from this vocabulary. This is my code:</p>
<pre><code>import nltk    
import collections
from nltk.corpus import words

# nltk.download(&quot;words&quot;)

# Creating a frequency Counter of all the words
frequency_counter = collections.Counter(df.posts.explode())
sorted_common_words = sorted(frequency_counter.items(), key = lambda pair: -pair[1])

words_lst = []
for i in range(len(sorted_common_words)):
    if sorted_common_words[i][1] &gt; 1000:
        words_lst.append(sorted_common_words[i][0])

valid_words = []
invalid_words = []

valid_words = [word for word in words_lst if word in words.words()]
invalid_words = [word for word in words_lst if word not in words.words()]
</code></pre>
<p>My problem is that the <code>invalid_words</code> list is containing some valid English words like:</p>
<ol>
<li>f*ck</li>
<li>changed</li>
<li>surprised</li>
<li>girlfriend</li>
<li>avatar</li>
<li>anymore</li>
</ol>
<p><a href=""https://i.sstatic.net/4z6fe.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/4z6fe.png"" alt=""word.words() check"" /></a></p>
<p>And some more of course. Even checking manually if those words exist in the <code>words.words()</code> it returns <code>False</code>. I tried initially to stem my text but this produced some root of the words, which didn't look right, and that's why decided to lemmatize them.</p>
<p>Is there a library in Python which have all the stemmed versions of the English words? I guess this will speed up significantly my script.</p>
<p>My original dataframe is around 9000 lines, and a bit more than 5M tokenized words and around 110.000 unique words after cleaning the dataset. 'words.words()<code>is containing 236736 words, so checking if those 110.000 words are within</code>words.words()` will take too much time. I have checked and checking of 1000 takes approximately a minute. This is mainly due to the limitation of Python to be run on only one core, so I cannot parallelize the operation on all available cores.</p>
"
"64807140","How to get the TF-IDF scores as well for the most important words?","2020-11-12 15:46:53","2","981","0","1","","64807349","<p>I am working on a project with tf-idf, I have a column (df['liststring']) in my dataframe that contains the preprocessed text (without punctuation, stop words, etc.) from my various documents.</p>
<p>I ran the following code, and I got the top 10 words with the highest tf-idf values but I would like to see their scores as well.</p>
<pre><code>    from sklearn.feature_extraction.text import TfidfVectorizer
    tfidf = TfidfVectorizer()
    X_tfidf = tfidf.fit_transform(df['liststring']).toarray()
    vocab = tfidf.vocabulary_
    reverse_vocab = {v:k for k,v in vocab.items()}
    feature_names = tfidf.get_feature_names()
    df_tfidf = pd.DataFrame(X_tfidf, columns = feature_names)
    idx = X_tfidf.argsort(axis=1)
    tfidf_max10 = idx[:,-10:]
    df_tfidf['top10'] = [[reverse_vocab.get(item) for item in row] for row in tfidf_max10 ]
    
df_tfidf['top10']

0      [kind, pose, world, preventive, sufficient, ke...
1      [mode, california, diseases, evidence, zoonoti...
2      [researcher, commentary, allegranzi, say, mora...
3      [carry, mild, man, whatever, suffering, downpl...
4      [region, service, almost, wednesday, detect, f...
                             ...                        
754    [americans, plan, year, black, online, shop, s...
755    [relate, manor, tuesday, death, portobello, ce...
756    [one, october, eight, exist, transmit, cluster...
757    [wolfe, shelter, county, resident, cupertino, ...
758    [firework, year, blasio, day, marching, reimag...
</code></pre>
<p>If we take the first row as an example, instead of [kind, pose, world, preventive, sufficient, ke...], I would like to get the output look like [kind:0.2, pose:0.3, world:0.4, preventive:0.5, sufficient:0.6, ke...]</p>
"
"64731868","SpaCy: How to manually set POS tag for vertical bar ""|""?","2020-11-07 20:02:28","1","781","0","2","","64770148","<p>When text is tagged by SpaCy, the vertical bar &quot;|&quot; is assigned different POS tags depending on the context, such as &quot;ADV&quot; , &quot;DEL&quot;... While I want &quot;|&quot; to be recognized as &quot;PUNC&quot;. Is there a way to force this POS for &quot;|&quot; ?</p>
<p>I tried this command and it didn't work.</p>
<pre><code>nlp.tokenizer.add_special_case('|', [{ORTH: '|', POS: PUNC}])
</code></pre>
"
"64646551","Spacy pos tagging PPER","2020-11-02 13:32:06","2","727","0","2","","64653722","<p>I'm trying to make spacy tag a german word with <code>PPER</code>. See <a href=""https://spacy.io/api/annotation"" rel=""nofollow noreferrer"">https://spacy.io/api/annotation</a>. Here is my code:</p>
<pre><code>import de_core_news_sm

nlp = de_core_news_sm.load()

tokenized = nlp(&quot;Der Mann liebt Kuchen.&quot;)
for token in tokenized:
    print(token, token.pos_, token.ent_type_)
</code></pre>
<p>Which string do I need to enter to get a <code>PPER</code> tag?</p>
"
"64613067","Sklearn tf-idf TfidfVectorizer failed to capture one letter words","2020-10-30 17:22:19","2","751","1","1","","64613204","<p>A particular instance is <code>&quot;Queens Stop 'N' Swap&quot;</code>. After transforming, I only got three features <code>['Queens', 'Stop', 'SWap']</code>. The <code>'N'</code> has been ignored. How can I capture the <code>'N'</code>?. All the parameters are default settings in my code.</p>
<pre><code>### Create the vectorizer method
tfidf_vec = TfidfVectorizer()

### Transform the text into tf-iwine vectors
text_tfidf = tfidf_vec.fit_transform(title_text)
</code></pre>
"
"64605008","Language detection in Python for big data","2020-10-30 08:34:50","1","3322","0","3","","64605106","<p>I am trying to run language detection on a Series object in a pandas dataframe. However, I am dealing with millions of rows of string data, and the standard Python language detection libraries<code>langdetect</code> and <code>langid</code> are too slow, and after hours of running it still hasn't completed.</p>
<p>I set up my code as follows:</p>
<pre><code>#function to detect language
def detect_language (cell):
    if len(cell) &gt; 0:
        lan = langid.classify(cell)
    else:
        lan = &quot;NaN&quot;
    return lan
</code></pre>
<pre><code>#language detection using langid module

df['language'] = df.apply(lambda row: detect_language(row.Series), axis = 1)
</code></pre>
<p>Does anybody have suggestions on how to speed up my code or if there is another library out there?</p>
"
"64496364","How to apply tf-idf to rows of text","2020-10-23 08:16:05","-1","381","1","1","","64497014","<p>I have rows of blurbs (in text format) and I want to use tf-idf to define the weight of each word. Below is the code:</p>
<pre><code>def remove_punctuations(text):
    for punctuation in string.punctuation:
        text = text.replace(punctuation, '')
    return text
df[&quot;punc_blurb&quot;] = df[&quot;blurb&quot;].apply(remove_punctuations)

df = pd.DataFrame(df[&quot;punc_blurb&quot;])

vectoriser = TfidfVectorizer()
df[&quot;blurb_Vect&quot;] = list(vectoriser.fit_transform(df[&quot;punc_blurb&quot;]).toarray())

df_vectoriser = pd.DataFrame(x.toarray(),
columns = vectoriser.get_feature_names())
print(df_vectoriser)
</code></pre>
<p>All I get is a massive list of numbers, which I am not even sure anymore if its the TF or TF-IDF that it is giving me as the frequent words (the, and, etc) all have a score of more than 0.</p>
<p>The goal is to see the weights in the tf-idf column shown below and I am unsure if I am doing this in the most efficient way:</p>
<p><a href=""https://i.sstatic.net/IYiOP.jpg"" rel=""nofollow noreferrer"">Goal Output table</a></p>
"
"64460941","NLTK TypeError: unhashable type: 'list'","2020-10-21 09:49:03","0","471","3","1","","64461359","<p>I am currently working on the lemmantization of a word from a csv file, where afterwards I passed all words in lowercase letters, removed all punctuation and split the column.</p>
<p>I use only two CSV columns: <code>analyze.info()</code>:</p>
<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 4637 entries, 0 to 4636. Data columns (total 2 columns):
#   Column          Non-Null Count  Dtype
0   Comments        4637 non-null   object
1   Classification  4637 non-null   object

import string
import pandas as pd
from nltk.corpus import stopwords
from nltk.stem import 

analyze = pd.read_csv('C:/Users/(..)/Talk London/ALL_dataset.csv', delimiter=';', low_memory=False, encoding='cp1252', usecols=['Comments', 'Classification'])

lower_case = analyze['Comments'].str.lower()

cleaned_text = lower_case.str.translate(str.maketrans('', '', string.punctuation))

tokenized_words = cleaned_text.str.split()

final_words = []
for word in tokenized_words:
    if word not in stopwords.words('english'):
       final_words.append(word)

wnl = WordNetLemmatizer()
lemma_words = []
lem = ' '.join([wnl.lemmatize(word) for word in tokenized_words])
lemma_words.append(lem)
</code></pre>
<p>When I run the code return this error:</p>
<blockquote>
<p>Traceback (most recent call last): <br/>
File &quot;C:/Users/suiso/PycharmProjects/SA_working/SA_Main.py&quot;, line 52, in 
lem = ' '.join([wnl.lemmatize(word) for word in tokenized_words])<br/>
File &quot;C:/Users/suiso/PycharmProjects/SA_working/SA_Main.py&quot;, line 52, in 
lem = ' '.join([wnl.lemmatize(word) for word in tokenized_words]) <br/>
File &quot;C:\Users\suiso\PycharmProjects\SA_working\venv\lib\site-packages\nltk\stem\wordnet.py&quot;, line 38, in lemmatize
lemmas = wordnet._morphy(word, pos)<br/>
File &quot;C:\Users\suiso\PycharmProjects\SA_working\venv\lib\site-packages\nltk\corpus\reader\wordnet.py&quot;, line 1897, in _morphy<br/>
if form in exceptions:<br/>
TypeError: unhashable type: 'list'</p>
</blockquote>
"
"64435099","pandas: Split and convert series of alphanumeric texts to columns and rows","2020-10-19 21:06:39","0","154","4","1","","64435408","<p><strong>Current data frame:</strong> I have a pandas data frame where each employee has a text code(all codes start with T) and an associated frequency right next to the code. All text codes have 8 characters.</p>
<pre><code>+----------+-------------------------------------------------------------+
|  emp_id  |   text                                                      |
+----------+-------------------------------------------------------------+
|   E0001  | [T0431516,-8,T0401531,-12,T0517519,12]                      |
|   E0002  | [T0701540,-1,T0431516,-2]                                   |
|   E0003  | [T0517519,-1,T0421531,-7,T0516319,9,T0500371,-6,T0309711,-3]|
|   E0004  | [T0516319,-3]                                               |
|   E0005  | [T0431516,2]                                                |
+----------+-------------------------------------------------------------+
</code></pre>
<p><strong>Expected data frame:</strong> I am trying to make the text codes present in the data frame as individual columns and if an employee has a frequency for that code then populate frequency else 0.</p>
<pre><code>+----------+----------------------------------------------------------------------------------------+
|  emp_id  | T0431516 | T0401531 | T0517519 | T0701540 | T0421531 |  T0516319 | T0500371 | T0309711 |                                      
+----------+----------------------------------------------------------------------------------------+
|   E0001  | -8       | -12      | 12       | 0        | 0        | 0         | 0        | 0        |
|   E0002  | -2       | 0        | 0        | -1       | 0        | 0         | 0        | 0        |
|   E0003  | 0        | 0        | -1       | 0        | -7       | 9         | -6       | -3       |
|   E0004  | 0        | 0        | 0        | 0        | 0        | -3        | 0        | 0        |
|   E0005  | 2        | 0        | 0        | 0        | 0        | 0         | 0        | 0        |
+----------+----------------------------------------------------------------------------------------+
</code></pre>
<p><strong>Sample data</strong>:</p>
<pre><code>pd.DataFrame({'emp_id' : {0: 'E0001', 1: 'E0002', 2: 'E0003', 3: 'E0004', 4: 'E0005'},
                'text' :  {0: '[T0431516,-8,T0401531,-12,T0517519,12]', 1: '[T0701540,-1,T0431516,-2]', 2: '[T0517519,-1,T0421531,-7,T0516319,9,T0500371,-6,T0309711,-3]', 3: '[T0516319,-3]', 4: '[T0431516,2]'}
                })
</code></pre>
<p>So, far my attempts were unsuccessful. Any pointers/help is much appreciated!</p>
"
"64236859","My checkpoint albert files does not change when training","2020-10-07 03:42:15","0","105","0","1","","64278592","<p>I train Albert model for question answering task. I have 200 thousand question-answer pairs and I use a saved checkpoint file with 2gb. I trained it on my GPU GeForce 2070 RTX with 1000 steps each time to save checkpoint, during training the checkpoint <code>model.ckpt-96000.data-00000-of-00001</code> files just keep the size of <code>135MB</code> and don't increase. Is this a problem?</p>
<p>I can't see why with a much smaller dataset like 1500 question-answer pairs, it also produces 135 MB checkpoint file. It hasn't stopped training yet but is it possible that the model will improve with this training?</p>
"
"64198426","How to remove unwanted chars and leave only actual words with POS tags?","2020-10-04 18:31:59","0","296","0","1","","64198661","<p>Using Python, I have split up the chunks of a text file data into sentences into a list as below (&quot;My list&quot;). I need to figure out how to only pull out the word tokens and their associated POS tags (included in the sentence). My goal is in a bigram type of structure such as this: [('Football', 'NNP'), ('Baltimore', 'NNP'), ('pulled', 'NNP'), ('off', 'IN'), ('a','IN'),('victory','NN'),('.','.')]. I don't want to see the extra words/characters such as 'I-NP' and 'O' and ':'. However, periods ( . ) and commas ( , ) are fine. Would like to keep those if possible in paired list.</p>
<p>My list:</p>
<pre><code> ['Football',
     'NNP',
     'I-NP',
     'O',
     '-',
     ':',
     'O',
     'O',
     'Baltimore',
     'NNP',
     'I-NP',
     'B-ORG',
     'pulled',
     'NNP',
     'I-NP',
     'O',
     'off',
     'IN',
     'I-PP',
     'O',
     'a',
     'IN',
     'I-NP',
     'O',
     'victory',
     'NN',
     'I-NP',
     'O',
     '.',
     '.',
     'O',
     'O']
</code></pre>
<p>I would like to see like this but not sure how to get there:</p>
<pre><code> [('Football', 'NNP'), ('Baltimore', 'NNP'), ('pulled', 'NNP'), ('off', 'IN'), ('a','IN'),('victory','NN'),('.','.')]
</code></pre>
"
"64185831","Am I missing the preprocessing function in spaCy's lemmatization?","2020-10-03 15:24:13","2","602","5","1","","64188206","<p>I'm trying to get lemmas (i.e. token.lemma_) for all tokens in a document using spacy.</p>
<p><strong>CODE:</strong></p>
<pre><code>sentence = 'I'm looking for all of the lemmas. Please help me find them!'
nlp = spacy.load('en', disable=['parser', 'NER])
doc = nlp(sentence)
tokens = [tokens.lemma_ for token in doc]
</code></pre>
<p><strong>EXPECTED RESULT:</strong></p>
<pre><code>['look', 'lemma', 'help', 'find']
</code></pre>
<p><strong>ACTUAL RESULT:</strong></p>
<pre><code>[-PRON-, 'be', 'look', 'all', 'of', 'the', 'lemma', '.', 'please', 'help', '-PRON-', 'find', '-PRON', '!']
</code></pre>
<p>Am I missing some sort of preprocessing function in spacy, or do I have to preprocess separately? I want all punctuation and stopwords to be removed ahead of lemmatization.</p>
"
"64172681","Parsing html URL into pandas table","2020-10-02 13:48:19","2","97","1","1","","64173997","<p>I have the following URL <a href=""http://www.mso.anu.edu.au/%7Eralph/OPTED/v003/wb1913_e.html"" rel=""nofollow noreferrer"">link</a>.<br />
Is there a simple way to create a pandas table in Jupyter notebook directly from the URL?<br />
Where the first column correspond to the word (e.g Eachwhere), the second column correspond the what's inside the parentheses (e.g adv), and the third column correspond to what's following the parentheses (e.g Everywhere)?</p>
<p>From the link:</p>
<pre><code>E () The fifth letter of the English alphabet.
E () E is the third tone of the model diatonic scale. E/ (E flat) is a tone which is intermediate between D and E.
E- () A Latin prefix meaning out, out of, from; also, without. See Ex-.
Each (a. / a. pron.) Every one of the two or more individuals composing a number of objects, considered separately from the rest. It is used either with or without a following noun; as, each of you or each one of you.
Each (a. / a. pron.) Every; -- sometimes used interchangeably with every.
Eachwhere (adv.) Everywhere.
Eadish (n.) See Eddish.
</code></pre>
"
"64083752","Constituent tree in Python (NLTK)","2020-09-27 00:37:50","1","777","0","1","","64097702","<p>I have found this code <a href=""https://www.geeksforgeeks.org/syntax-tree-natural-language-processing/"" rel=""nofollow noreferrer"">here</a>:</p>
<pre><code># Import required libraries 
import nltk 
nltk.download('punkt') 
nltk.download('averaged_perceptron_tagger') 
from nltk import pos_tag, word_tokenize, RegexpParser 
   
# Example text 
sample_text = &quot;The quick brown fox jumps over the lazy dog&quot;
   
# Find all parts of speech in above sentence 
tagged = pos_tag(word_tokenize(sample_text)) 
   
#Extract all parts of speech from any text 
chunker = RegexpParser(&quot;&quot;&quot; 
                       NP: {&lt;DT&gt;?&lt;JJ&gt;*&lt;NN&gt;}    #To extract Noun Phrases 
                       P: {&lt;IN&gt;}               #To extract Prepositions 
                       V: {&lt;V.*&gt;}              #To extract Verbs 
                       PP: {&lt;P&gt; &lt;NP&gt;}          #To extract Prepostional Phrases 
                       VP: {&lt;V&gt; &lt;NP|PP&gt;*}      #To extarct Verb Phrases 
                       &quot;&quot;&quot;) 
  
# Print all parts of speech in above sentence 
output = chunker.parse(tagged) 
print(&quot;After Extracting\n&quot;, output) 
</code></pre>
<p>As I understand, this code defines PP, NP and VP... My doubt is that the syntactic tags are already defined <a href=""https://catalog.ldc.upenn.edu/docs/LDC95T7/cl93.html"" rel=""nofollow noreferrer"">here</a>. Aren't these composed tags defined in NLTK? Is that the point? Furthermore, in the last row of the chunker <code>{&lt;V&gt; &lt;NP|PP&gt;*}</code>, is it using the above-defined <code>NP: {&lt;DT&gt;?&lt;JJ&gt;*&lt;NN&gt;}</code> and <code>PP: {&lt;P&gt; &lt;NP&gt;}</code>?</p>
"
"63999500","how to get tagset from nltk pos_tag?","2020-09-21 20:24:23","0","278","0","1","","63999874","<p>I'm trying to get the full tag from nltk pos_tag, but I can't find a simple way to do it using nltk. For example, using <code>tagsets='universal'</code>.</p>
<pre class=""lang-py prettyprint-override""><code>from nltk.tokenize import word_tokenize

def nltk_pos(text):
    token = word_tokenize(text)
    return (nltk.pos_tag(token)[0])[1]

nltk_pos('home')
output: 'NN'
expected output: 'NOUN'
</code></pre>
"
"63807411","lemmatize an entire column using lambda function","2020-09-09 08:11:25","1","1339","1","1","","63808958","<p>I have this code tested for a  sentence and I want to convert it so that I can lemmatize an entire column where each row consists in words without punctuation like:  deportivas calcetin hombres deportivas shoes</p>
<pre><code>    import wordnet, nltk
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet
import pandas as pd

df = pd.read_excel(r'C:\Test2\test.xlsx')
# Init the Wordnet Lemmatizer
lemmatizer = WordNetLemmatizer()
sentence = 'FINAL_KEYWORDS'
def get_wordnet_pos(word):
    &quot;&quot;&quot;Map POS tag to first character lemmatize() accepts&quot;&quot;&quot;
    tag = nltk.pos_tag([word])[0][1][0].upper()
    tag_dict = {&quot;J&quot;: wordnet.ADJ,
                &quot;N&quot;: wordnet.NOUN,
                &quot;V&quot;: wordnet.VERB,
                &quot;R&quot;: wordnet.ADV}

    return tag_dict.get(tag, wordnet.NOUN)



#Lemmatize a Sentence with the appropriate POS tag
sentence = &quot;The striped bats are hanging on their feet for best&quot;
print([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sentence)])
</code></pre>
<p>Let's suppose Column name is df['keywords'], can you help me use a lambda function in order to lemmatize the entire column like I lemmatize the sentence above?</p>
<p>Many thanks in advance</p>
"
"63802521","Is it possible to get a list of words given the Lemma in Spacy?","2020-09-08 22:49:01","3","597","1","1","","63810040","<p>I am trying to fix grammatical gender in French text and wanted to know if there is a way to get a list of all words from a certain lemma and if it possible to do a lookup in such list?</p>
"
"63790729","Which NLP task is easier to begin with?","2020-09-08 09:10:19","-3","44","0","1","","63792090","<p>Which one among the following NLP topics will be easier to work with?</p>
<ol>
<li>Question answering</li>
<li>Paraphrase detection</li>
<li>Short text conversation</li>
<li>Author identification</li>
</ol>
"
"63778133","How can I implement meteor score when evaluating a model when using the meteor_score module from nltk?","2020-09-07 13:00:58","4","7806","0","1","","63779708","<p>I currently have 2 files, reference.txt and model.txt. These two text files contain original captions and generated captions after training.<br />
Can I simply do the following to obtain the meteor score:</p>
<pre><code>score = nltk.translate.meteor_score.meteor_score(reference, model)
print(np.mean(meteor_score))
</code></pre>
<p>I have also looked to <a href=""https://github.com/tylin/coco-caption"" rel=""nofollow noreferrer"">https://github.com/tylin/coco-caption</a> but I have no clue how to implement this.</p>
"
"63747454","Is there a way to optimize SpaCy training?","2020-09-04 20:08:58","1","1276","0","1","","63760442","<p>I'm currently training a SpaCy model for multi-label text classification. There are 6 labels: anger, anticipation, disgust, fear, joy, sadness, surprise and trust. The dataset is over 200k. However, per epoch is taking 4 hours. I was wondering if there's a way to optimize the training and do it faster, maybe I'm skipping something here that can improve the model.</p>
<hr />
<p><strong>TRAINING_DATA</strong></p>
<pre class=""lang-py prettyprint-override""><code>TRAIN_DATA = list(zip(train_texts, [{&quot;cats&quot;: cats} for cats in final_train_cats]))

[...
  {'cats': {'anger': 1,
    'anticipation': 0,
    'disgust': 0,
    'fear': 0,
    'joy': 0,
    'sadness': 0,
    'surprise': 0,
    'trust': 0}}),
 ('mausoleum',
  {'cats': {'anger': 1,
    'anticipation': 0,
    'disgust': 0,
    'fear': 0,
    'joy': 0,
    'sadness': 0,
    'surprise': 0,
    'trust': 0}}),
 ...]
</code></pre>
<p><strong>TRAINING</strong></p>
<pre class=""lang-py prettyprint-override""><code>nlp = spacy.load(&quot;en_core_web_sm&quot;)
category = nlp.create_pipe(&quot;textcat&quot;, config={&quot;exclusive_classes&quot;: True})
nlp.add_pipe(category)

# add label to text classifier
category.add_label(&quot;trust&quot;)
category.add_label(&quot;fear&quot;)
category.add_label(&quot;disgust&quot;)
category.add_label(&quot;surprise&quot;)
category.add_label(&quot;anticipation&quot;)
category.add_label(&quot;anger&quot;)
category.add_label(&quot;joy&quot;)

optimizer = nlp.begin_training()
losses = {}

for i in range(100):
    random.shuffle(TRAIN_DATA)

    print('...')
    for batch in minibatch(TRAIN_DATA, size=8):
        texts = [nlp(text) for text, entities in batch]
        annotations = [{&quot;cats&quot;: entities} for text, entities in batch]
        nlp.update(texts, annotations, sgd=optimizer, losses=losses)
    print(i, losses)

...
0 {'parser': 0.0, 'tagger': 27.018985521040854, 'textcat': 0.0, 'ner': 0.0}
...
1 {'parser': 0.0, 'tagger': 27.01898552104131, 'textcat': 0.0, 'ner': 0.0}
...
</code></pre>
"
"63653161","How to correctly label confusion matrix?","2020-08-30 01:18:18","0","531","0","1","","63660877","<p>Trying to do some NLP (take verbatims from a corpus and tag them with a specific topic as the labels).</p>
<p>I'm at the part where I've created a confusion matrix, but I do not know how to correctly label the matrix so that the labels are attributed to the correct part of the matrix.</p>
<p>Concretely, if i have the following  confusion matrix, how do I know where the labels should
correctly reside?</p>
<pre><code> whats label 1? [ 1  0  0  0  0  0  0  0  0  0  0  0  0]
 whats label 2? [ 0  5  0  0  0  0  3  0  0  0  0  0  0]
 etc..          [ 0  0  0  0  0  0  0  0  0  0  0  0  0]
                [ 0  1  0  6  0  1  1  0  0  0  0  0  0]
                [ 0  0  0  0  1  0  0  0  0  1  0  0  0]
                [ 0  0  0  1  0  1  0  0  0  0  0  0  0]
                [ 0  1  0  0  0  0 13  0  0  0  0  0  0]
                [ 0  0  0  0  0  0  1  0  0  0  0  0  0]
                [ 0  0  1  0  0  0  1  0  5  0  0  0  0]
                [ 0  0  0  0  0  0  0  0  0  0  0  0  0]
                [ 0  0  0  0  1  0  1  0  0  0  0  0  1]
                [ 0  0  0  0  0  0  1  0  0  0  0  2  0]
                [ 0  0  0  0  0  0  0  1  0  0  0  0  0]

</code></pre>
<p>Here's my code:</p>
<pre><code>#bag of words ---------------------------------------------------------------

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

def cv(data):
    count_vectorizer = CountVectorizer()

    emb = count_vectorizer.fit_transform(data)

    return emb, count_vectorizer

list_corpus = questions_and_labels['clean_text_lemmed'].tolist()
list_labels = questions_and_labels[&quot;category_1_level_1&quot;].tolist()

X_train, X_test, y_train, y_test = train_test_split(list_corpus, list_labels, test_size=0.2, 
                                                                                random_state=40)

X_train_counts, count_vectorizer = cv(X_train)
X_test_counts = count_vectorizer.transform(X_test)

# Confusion Matrix ---------------------------------------------------------------
import numpy as np
import itertools
from sklearn.metrics import confusion_matrix

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.winter):
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title, fontsize=30)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, fontsize=10, rotation=90)
    plt.yticks(tick_marks, classes, fontsize=10)
    
    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.

    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=&quot;center&quot;, 
                 color=&quot;white&quot; if cm[i, j] &lt; thresh else &quot;black&quot;, fontsize=40)
    
    plt.tight_layout()
    plt.ylabel('True label', fontsize=30)
    plt.xlabel('Predicted label', fontsize=30)

    return plt

    cm = confusion_matrix(y_test, y_predicted_counts)
    fig = plt.figure(figsize=(20, 20))
    plot = plot_confusion_matrix(cm, classes = HOW_DO_I_GET_THIS?, normalize=False, title='Confusion matrix')
    plt.show()
    print(cm)


</code></pre>
<p>Additional context, I've been following and mimicking this example: <a href=""https://github.com/hundredblocks/concrete_NLP_tutorial/blob/master/NLP_notebook.ipynb"" rel=""nofollow noreferrer"">https://github.com/hundredblocks/concrete_NLP_tutorial/blob/master/NLP_notebook.ipynb</a></p>
<p>but in their example, they hard-coded the labels... and i'm not sure how they figured out the correct order</p>
"
"63434178","(TF-IDF)How to return the five related article after calculating cosine similarity","2020-08-16 07:16:54","0","509","5","1","","63452414","<p>I get a dataframe <em>sample_df</em>(4 columns: <em>paper_id</em>,<em>title</em>,<em>abstract</em>,<em>body_text</em>). I extracted the abstract column(~1000 words per abstract) and apply the text cleaning process. Here's my question:</p>
<p>After finished calculating the cosine similarity between question and abstract, how can it return the top5 articles score with corresponding information(e.g. <em>paper_id</em>,<em>title</em>,<em>body_text</em>) since my goal is to do tf -idf question answering.</p>
<p>I'm really sorry that my english is poor and I am new to nlp. I would appreciated if someone can help.</p>
<pre><code>from sklearn.metrics.pairwise import linear_kernel
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.metrics.pairwise import cosine_similarity  

txt_cleaned = get_cleaned_text(sample_df,sample_df['abstract'])
question = ['Can covid19 transmit through air']

tfidf_vector = TfidfVectorizer()

tfidf = tfidf_vector.fit_transform(txt_cleaned)

tfidf_question = tfidf_vector.transform(question)
cosine_similarities = linear_kernel(tfidf_question,tfidf).flatten()

related_docs_indices = cosine_similarities.argsort()[:-5:-1]
cosine_similarities[related_docs_indices]

#output([0.18986527, 0.18339485, 0.14951123, 0.13441914]) 
</code></pre>
"
"63430951","Accessing out of range word in spaCy doc : why does it work?","2020-08-15 21:25:47","0","807","0","1","","63431248","<p>I'm learning spaCy and am playing with <a href=""https://spacy.io/usage/rule-based-matching"" rel=""nofollow noreferrer"">Matchers</a>.</p>
<p>I have:</p>
<ul>
<li>a very basic sentence (&quot;white shepherd dog&quot;)</li>
<li>a matcher object, searching for a pattern (&quot;white shepherd&quot;)</li>
<li>a print to show the match, and the word and POS before that match</li>
</ul>
<p>I just wanted to check how to handle the index out of range exception I'm expecting to get because there's nothing before the match. I didn't expect it to work, but it did and is returning 'dog', which is after the match... and now I'm confused.</p>
<p>It looks like spaCy uses a circular list (or deque I think) ?</p>
<p>This needs a language model to run, you can install it with the following command line, if you'd like to reproduce it:</p>
<p><code>python -m spacy download en_core_web_md</code></p>
<p>And this is the code</p>
<pre><code>import spacy
from spacy.matcher import Matcher 

# Loading language model
nlp = spacy.load(&quot;en_core_web_md&quot;)

# Initialising with shared vocab
matcher = Matcher(nlp.vocab)

# Adding statistical predictions
matcher.add(&quot;DOG&quot;, None, [{&quot;LOWER&quot;: &quot;white&quot;}, {&quot;LOWER&quot;: &quot;shepherd&quot;}])  # searching for white shepherd
doc = nlp(&quot;white shepherd dog&quot;)

for match_id, start, end in matcher(doc):
    span = doc[start:end]  
    print(&quot;Matched span: &quot;, span.text)   
    # Get previous token and its POS
    print(&quot;Previous token: &quot;, doc[start - 1].text, doc[start - 1].pos_) # I would expect the error here
</code></pre>
<p>I get the following:</p>
<pre><code>&gt;&gt;&gt; Matched span:  white shepherd
&gt;&gt;&gt; Previous token:  dog PROPN
</code></pre>
<p>Can someone explain what's going on ?</p>
<p>Thanks !</p>
"
"63302527","RuntimeWarning: Failed to decode a serialized output from CoreNLP server. An incomplete or empty object will be returned","2020-08-07 13:08:28","0","46","0","1","","63304852","<p>I am parsing (getting constituency tree) IMDB dataset using CoreNLP server with Shift-Reduce parser. For few sentences it gives a warning which is:</p>
<p><code>RuntimeWarning: Failed to decode a serialized output from CoreNLP server. An incomplete or empty object will be returned. warnings.warn(&quot;Failed to decode a serialized output from CoreNLP server. An incomplete or empty object will be returned.&quot;, \</code></p>
<p>I really don't have any idea why it is giving this error.</p>
"
"63239929","NLP: Compare these two sentences. Is this a misclassification?","2020-08-04 03:40:28","1","291","0","1","","63243126","<p>I am using the dependence parse of spacy. I am puzzled with these two very similar sentences.</p>
<p>Sentence 1:</p>
<pre><code>text='He noted his father was a nice guy.'
</code></pre>
<p>Note that in this sentence &quot;father&quot; is clearly the subject of &quot;father was a nice guy&quot;:</p>
<pre><code>[(0, 'He', '-PRON-', 'PRON', 'PRP', 'nsubj'), (1, 'noted', 'note', 'VERB', 'VBD', 'ROOT'), (2, 'his', '-PRON-', 'DET', 'PRP$', 'poss'), (3, 'father', 'father', 'NOUN', 'NN', 'nsubj'), (4, 'was', 'be', 'VERB', 'VBD', 'ccomp'), (5, 'a', 'a', 'DET', 'DT', 'det'), (6, 'nice', 'nice', 'ADJ', 'JJ', 'amod'), (7, 'guy', 'guy', 'NOUN', 'NN', 'attr'), (8, '.', '.', 'PUNCT', '.', 'punct')]

        noted              
  ________|_____            
 |   |         was         
 |   |     _____|___        
 |   |  father     guy     
 |   |    |      ___|___    
 He  .   his    a      nice

for child in the_verb.children:
    print(child,child.dep_)
    
&gt;&gt; father nsubj
&gt;&gt; guy attr

for ancestor in the_verb.ancestors:
    print(ancestor,ancestor.dep_)
    
&gt;&gt; noted ROOT
</code></pre>
<p>Sentence 2:</p>
<pre><code>text='He noted his father, as \&quot;a man with different attributes\&quot;, was a nice guy.'
</code></pre>
<p>This is a minor variation of the previous sentence. &quot;father&quot; is not the subject anymore.</p>
<pre><code>[(0, 'He', '-PRON-', 'PRON', 'PRP', 'nsubj'), (1, 'noted', 'note', 'VERB', 'VBD', 'ROOT'), (2, 'his', '-PRON-', 'DET', 'PRP$', 'poss'), (3, 'father', 'father', 'NOUN', 'NN', 'dobj'), (4, ',', ',', 'PUNCT', ',', 'punct'), (5, 'as', 'as', 'ADP', 'IN', 'prep'), (6, '&quot;', '&quot;', 'PUNCT', '``', 'punct'), (7, 'a', 'a', 'DET', 'DT', 'det'), (8, 'man', 'man', 'NOUN', 'NN', 'pobj'), (9, 'with', 'with', 'ADP', 'IN', 'prep'), (10, 'different', 'different', 'ADJ', 'JJ', 'amod'), (11, 'attributes', 'attribute', 'NOUN', 'NNS', 'pobj'), (12, '&quot;', '&quot;', 'PUNCT', &quot;''&quot;, 'punct'), (13, ',', ',', 'PUNCT', ',', 'punct'), (14, 'was', 'be', 'VERB', 'VBD', 'conj'), (15, 'a', 'a', 'DET', 'DT', 'det'), (16, 'nice', 'nice', 'ADJ', 'JJ', 'amod'), (17, 'guy', 'guy', 'NOUN', 'NN', 'attr'), (18, '.', '.', 'PUNCT', '.', 'punct')]

                noted                                 
  ________________|____________________________        
 |   |   |   |    |         as                 |      
 |   |   |   |    |         |                  |       
 |   |   |   |    |        man                 |      
 |   |   |   |    |      ___|______            |       
 |   |   |   |    |     |   |     with        was     
 |   |   |   |    |     |   |      |           |       
 |   |   |   |  father  |   a  attributes     guy     
 |   |   |   |    |     |   |      |        ___|___    
 He  ,   ,   .   his    &quot;   &quot;  different   a      nice


the_verb=spacy_doc[14]

for child in the_verb.children:
    print(child,child.dep_)
    
&gt;&gt; guy attr

for ancestor in the_verb.ancestors:
    print(ancestor,ancestor.dep_)
    
&gt;&gt; noted ROOT
</code></pre>
<p>I am trying to understand how spacy classifies the sentences. Is the second case a misclassification error? I mean &quot;father&quot; should still be the subject?</p>
"
"63211463","Error Expected object of device type cuda but got device type cpu for argument #1 'self' in call to _th_index_select","2020-08-02 01:11:19","0","638","0","1","","63211626","<p>I have the following code taken directly from <a href=""https://towardsdatascience.com/simple-abstractive-text-summarization-with-pretrained-t5-text-to-text-transfer-transformer-10f6d602c426"" rel=""nofollow noreferrer"">here</a> with some pretty little modifications:</p>
<pre><code>import pandas as pd
import torch
import json 
from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config
from torch import cuda

df = pd.read_pickle('df_final.pkl')

model = T5ForConditionalGeneration.from_pretrained('t5-base')
tokenizer = T5Tokenizer.from_pretrained('t5-base')
device = 'cuda' if cuda.is_available() else 'cpu'

text = ''.join(df[(df['col1'] == 'type') &amp; (df['col2'] == 2)].col3.to_list())

preprocess_text = text.strip().replace(&quot;\n&quot;,&quot;&quot;)
t5_prepared_Text = &quot;summarize: &quot;+preprocess_text
#print (&quot;original text preprocessed: \n&quot;, preprocess_text)

tokenized_text = tokenizer.encode(t5_prepared_Text, return_tensors=&quot;pt&quot;, max_length = 500000).to(device)


# summmarize 
summary_ids = model.generate(tokenized_text,
                                    num_beams=4,
                                    no_repeat_ngram_size=2,
                                    min_length=30,
                                    max_length=100,
                                    early_stopping=True)

output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

print (&quot;\n\nSummarized text: \n&quot;,output)
</code></pre>
<p>When executing the <code>model_generate()</code> part i get an error like this:</p>
<pre><code>---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-12-e8e9819a85dc&gt; in &lt;module&gt;
     12                                     min_length=30,
     13                                     max_length=100,
---&gt; 14                                     early_stopping=True).to(device)
     15 
     16 output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

~\Anaconda3\lib\site-packages\torch\autograd\grad_mode.py in decorate_no_grad(*args, **kwargs)
     47         def decorate_no_grad(*args, **kwargs):
     48             with self:
---&gt; 49                 return func(*args, **kwargs)
     50         return decorate_no_grad
     51 

~\Anaconda3\lib\site-packages\transformers\generation_utils.py in generate(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, num_return_sequences, attention_mask, decoder_start_token_id, use_cache, **model_specific_kwargs)
    383             encoder = self.get_encoder()
    384 
--&gt; 385             encoder_outputs: tuple = encoder(input_ids, attention_mask=attention_mask)
    386 
    387         # Expand input ids if num_beams &gt; 1 or num_return_sequences &gt; 1

~\Anaconda3\lib\site-packages\torch\nn\modules\module.py in __call__(self, *input, **kwargs)
    539             result = self._slow_forward(*input, **kwargs)
    540         else:
--&gt; 541             result = self.forward(*input, **kwargs)
    542         for hook in self._forward_hooks.values():
    543             hook_result = hook(self, input, result)

~\Anaconda3\lib\site-packages\transformers\modeling_t5.py in forward(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, past_key_value_states, use_cache, output_attentions, output_hidden_states, return_dict)
    701         if inputs_embeds is None:
    702             assert self.embed_tokens is not None, &quot;You have to intialize the model with valid token embeddings&quot;
--&gt; 703             inputs_embeds = self.embed_tokens(input_ids)
    704 
    705         batch_size, seq_length = input_shape

~\Anaconda3\lib\site-packages\torch\nn\modules\module.py in __call__(self, *input, **kwargs)
    539             result = self._slow_forward(*input, **kwargs)
    540         else:
--&gt; 541             result = self.forward(*input, **kwargs)
    542         for hook in self._forward_hooks.values():
    543             hook_result = hook(self, input, result)

~\Anaconda3\lib\site-packages\torch\nn\modules\sparse.py in forward(self, input)
    112         return F.embedding(
    113             input, self.weight, self.padding_idx, self.max_norm,
--&gt; 114             self.norm_type, self.scale_grad_by_freq, self.sparse)
    115 
    116     def extra_repr(self):

~\Anaconda3\lib\site-packages\torch\nn\functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)
   1482         # remove once script supports set_grad_enabled
   1483         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)
-&gt; 1484     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
   1485 
   1486 

RuntimeError: Expected object of device type cuda but got device type cpu for argument #1 'self' in call to _th_index_select
​
</code></pre>
<p>I've searched this error and fouund some other threads like <a href=""https://stackoverflow.com/questions/55278566/runtimeerror-expected-object-of-backend-cuda-but-got-backend-cpu-for-argument"">this</a> one and <a href=""https://stackoverflow.com/questions/58799486/expected-object-of-device-type-cuda-but-got-device-type-cpu-in-pytorch"">this</a> one but they didn't help me much since their case seems to be completely different. In my case there are no custom instances or classes created, so i don't know how to fix this or where the error come from.</p>
<p>Could you please tell me where is the error coming from and how could i fix it?</p>
<p>Thank you very much in advance.</p>
"
"63155722","NLP: Which are the dependency tags associated with a verb?","2020-07-29 14:17:47","0","164","0","1","","63308266","<p>I need to identify all dependency tags associated with a verb. So far, I have identified:</p>
<ol>
<li><p>'ROOT'</p>
</li>
<li><p>'xcomp'</p>
</li>
</ol>
<blockquote>
<pre><code>spacy.explain('xcomp')
Out[72]: 'open clausal complement'
</code></pre>
</blockquote>
<ol start=""3"">
<li>'aux'</li>
</ol>
<blockquote>
<pre><code>spacy.explain('aux')
Out[73]: 'auxiliary'
</code></pre>
</blockquote>
<p>Are there others?</p>
"
"63135603","Is there any way to give an input file to Stanza (stanford corenlp client) rather then one piece of text while calling server?","2020-07-28 13:45:36","1","872","0","1","","63144919","<p>I have a .csv file consists of Imdb sentiment analysis data-set. Each instance is a paragraph. I am using Stanza <a href=""https://stanfordnlp.github.io/stanza/client_usage.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/stanza/client_usage.html</a> for getting parse tree for each instance.</p>
<pre><code>text = &quot;Chris Manning is a nice person. Chris wrote a simple sentence. He also gives oranges to people.&quot;

with CoreNLPClient(
    annotators=['tokenize','ssplit','pos','lemma','ner', 'parse', 'depparse','coref'],
    timeout=30000,
    memory='16G') as client:
ann = client.annotate(text)
</code></pre>
<p>Right now, I have to re-run server for every instance and it is taking a lot of time since I have 50k instances.</p>
<pre><code>1
Starting server with command: java -Xmx16G -cp /home/wahab/treeattention/stanford-corenlp- 
4.0.0/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 1200000 -threads 
5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-a74576b3341f4cac.props 
-preload parse
2
Starting server with command: java -Xmx16G -cp /home/wahab/treeattention/stanford-corenlp- 
4.0.0/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 1200000 -threads 
5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-d09e0e04e2534ae6.props 
-preload parse
</code></pre>
<p>Is there any way to pass a file or do batching?</p>
"
"63080744","nlp: is this dependence tag correct? What does exactly it mean in this situation?","2020-07-24 20:06:49","0","141","0","2","","63106509","<p>I am exploring the amazing python library and I got this:</p>
<pre><code>text='The Titanic managed to sail into the coast  intact, and Conan went to Chicago.'
</code></pre>
<p>token_pos=[token.pos_ for token in spacy_doc]
token_tag=[token.tag_ for token in spacy_doc]
token_dep=[token.dep_ for token in spacy_doc]</p>
<p>token_pos</p>
<pre><code>['DET', 'PROPN', 'VERB', 'PART', 'VERB', 'ADP', 'DET', 'NOUN', 'SPACE', 'ADJ', 'PUNCT', 'CCONJ', 'PROPN', 'VERB', 'ADP', 'PROPN', 'PUNCT']
</code></pre>
<p>token_tag</p>
<pre><code>['DT', 'NNP', 'VBD', 'TO', 'VB', 'IN', 'DT', 'NN', '_SP', 'JJ', ',', 'CC', 'NNP', 'VBD', 'IN', 'NNP', '.']
</code></pre>
<p>token_dep</p>
<pre><code>['det', 'nsubj', 'ROOT', 'aux', 'xcomp', 'prep', 'det', 'pobj', '', 'advcl', 'punct', 'cc', 'nsubj', 'conj', 'prep', 'pobj', 'punct']
</code></pre>
<p>Tree</p>
<pre><code>def to_nltk_tree(node):
    if node.n_lefts + node.n_rights &gt; 0:
        return Tree(node.orth_, [to_nltk_tree(child) for child in node.children])
    else:
        return node.orth_

[to_nltk_tree(sent.root).pretty_print() for sent in spacy_doc.sents]

                    managed                                 
  _____________________|_________________________            
 |   |     |          sail                       |          
 |   |     |      _____|__________               |           
 |   |     |     |     |         into           went        
 |   |     |     |     |          |          ____|______     
 |   |  Titanic  |     |        coast       |    |      to  
 |   |     |     |     |      ____|____     |    |      |    
 ,  and   The    to  intact the           Conan  .   Chicago
</code></pre>
<p><strong>QUESTIONS: I am puzzled about the dependence relation between &quot;managed&quot; and &quot;went&quot;. It is a &quot;conj&quot;. (1) Is this a classification error? If it is a classification error, what would be the correct classification? If it is not, can you explain why is this happening?</strong> Spacy explains this as a &quot;conjunct&quot;: <strong>(2) Is there a way to differentiate this case from the case below?</strong></p>
<pre><code>spacy.explain('conj')
Out[59]: 'conjunct'
</code></pre>
<p>According to <a href=""https://nlp.stanford.edu/software/dependencies_manual.pdf"" rel=""nofollow noreferrer"">stanford dependence manual</a>:</p>
<p>A conjunct is the relation between two elements connected by a coordinating conjunction, such as “and”, “or”, etc:</p>
<p>“Bill is big and honest”</p>
<p>“They either ski or snowboard”</p>
<p>conj(big, honest)</p>
<p>conj(ski, snowboard)</p>
<p>Look at this last sentence now:</p>
<pre><code>text='They either ski or snowboard.'

spacy_doc = nlp(text)

token_pos=[token.pos_ for token in spacy_doc]
token_tag=[token.tag_ for token in spacy_doc]
token_dep=[token.dep_ for token in spacy_doc]

print(token_pos)
['PRON', 'CCONJ', 'VERB', 'CCONJ', 'NOUN', 'PUNCT']

print(token_tag)
['PRP', 'CC', 'VBP', 'CC', 'NN', '.']

print(token_dep)
['ROOT', 'preconj', 'appos', 'cc', 'conj', 'punct']

[to_nltk_tree(sent.root).pretty_print() for sent in spacy_doc.sents]
           They              
  __________|____             
 |              ski          
 |     __________|______      
 .  either       or snowboard
</code></pre>
<p>The relation dependence between &quot;ski&quot; and &quot;snowboard&quot; is also &quot;conj&quot; and in this case it seems to be the correct classification.</p>
"
"62948595","what is use case of Tokenization and Lemmatization in NLP when we have CountVectorizer and Tfidfvectorizer","2020-07-17 06:49:19","-1","529","0","1","","62948817","<p>i am learning the NLP and gone through;tokenization,Lemmatization Parts of speech and other basics.
I came to know CountVectorizer  and Tfidfvectorizer are there from sklearn which having internal ability to apply tokenization, Lemmatization.</p>
<p>so question is :</p>
<p>when i need use the core NLP activities to get the vocabulary instead of using CountVectorizer  and Tfidfvectorizer?</p>
"
"62895997","nlp: What is exactly a grammar dependence tag 'attr'?","2020-07-14 13:09:52","1","1143","0","1","","62909152","<p>I am exploring the spacy nlp python library. I have got this:</p>
<pre><code>text='Daniel is a smart clever professor.'

spacy_doc = nlp(text)

token_pos=[token.pos_ for token in spacy_doc]
token_tag=[token.tag_ for token in spacy_doc]
token_dep=[token.dep_ for token in spacy_doc]

token_pos
Out[105]: ['PROPN', 'VERB', 'DET', 'ADJ', 'ADJ', 'NOUN', 'PUNCT']

token_tag
Out[106]: ['NNP', 'VBZ', 'DT', 'JJ', 'JJ', 'NN', '.']

token_dep
Out[107]: ['nsubj', 'ROOT', 'det', 'amod', 'amod', 'attr', 'punct']

spacy.explain('attr')
Out[108]: 'attribute'

def to_nltk_tree(node):
    if node.n_lefts + node.n_rights &gt; 0:
        return Tree(node.orth_, [to_nltk_tree(child) for child in node.children])
    else:
        return node.orth_


[to_nltk_tree(sent.root).pretty_print() for sent in spacy_doc.sents]
            is                 
   _________|______             
  |     |      professor       
  |     |    ______|_______     
Daniel  .   a    smart   clever
</code></pre>
<p>Spacy explains that &quot;professor&quot; is an attribute ('attr') of &quot;is&quot;.</p>
<ol>
<li><p>What is exactly an attribute?</p>
</li>
<li><p>Where can I find this kind of information?</p>
</li>
<li><p>Are there other examples of 'attr' in different grammatical contexts?</p>
</li>
</ol>
"
"62735030","# string methods TypeError: Column is not iterable in pyspark","2020-07-04 22:22:58","1","2362","0","1","","62745002","<p>Im trying to re-implement sentiment analysis which is written in python to pyspark as im working with bigdata, im new to pyspark syntax, and im getting an error while trying to apply lemmatization function from nltk package</p>
<p>Error: # string methods TypeError: Column is not iterable
Below is the Code and Data</p>
<pre><code>|overall|       reviewsummary|     cleanreviewText|         reviewText1|  filteredreviewText|
+-------+--------------------+--------------------+--------------------+--------------------+
|    5.0|exactly what i ne...|exactly what i ne...|[exactly, what, i...|[exactly, needed,...|
|    2.0|i agree with the ...|i agree with the ...|[i, agree, with, ...|[agree, review, o...|
|    4.0|love these... i a...|love these... i a...|[love, these, i, ...|[love, going, ord...|
|    2.0|too tiny an openi...|too tiny an openi...|[too, tiny, an, o...|[tiny, opening, t...|
|    3.0|    okay three stars|    okay three stars|[okay, three, stars]|[okay, three, stars]|
|    5.0|exactly what i wa...|exactly what i wa...|[exactly, what, i...|[exactly, wanted,...|
|    4.0|these little plas...|these little plas...|[these, little, p...|[little, plastic,...|
|    3.0|mother - in - law...|mother - in - law...|[mother, in, law,...|[mother, law, wan...|
|    3.0|item is of good q...|item is of good q...|[item, is, of, go...|[item, good, qual...|
|    3.0|i had used my las...|i had used my las...|[i, had, used, my...|[used, last, el, ...|
+-------+--------------------+--------------------+--------------------+--------------------+
only showing top 10 rows


In [18]: dfStopwordRemoved.printSchema()
root
 |-- overall: double (nullable = true)
 |-- reviewText: string (nullable = true)
 |-- summary: string (nullable = true)
 |-- reviewsummary: string (nullable = true)
 |-- cleanreviewText: string (nullable = true)
 |-- reviewText1: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- filteredreviewText: array (nullable = true)
 |    |-- element: string (containsNull = true)
</code></pre>
<p>function lemmatize</p>
<pre><code>def get_part_of_speech(word):
probable_part_of_speech = wordnet.synsets(word)

pos_counts = Counter()
pos_counts[&quot;n&quot;] = len(  [ item for item in probable_part_of_speech if item.pos()==&quot;n&quot;]  )
pos_counts[&quot;v&quot;] = len(  [ item for item in probable_part_of_speech if item.pos()==&quot;v&quot;]  )
pos_counts[&quot;a&quot;] = len(  [ item for item in probable_part_of_speech if item.pos()==&quot;a&quot;]  )
pos_counts[&quot;r&quot;] = len(  [ item for item in probable_part_of_speech if item.pos()==&quot;r&quot;]  )

most_likely_part_of_speech = pos_counts.most_common(1)[0][0]
return most_likely_part_of_speech

def Lemmatizing_Words(Words):
Lm = WordNetLemmatizer()
Lemmatized_Words = []
for word in Words:
    Lemmatized_Words.append(Lm.lemmatize(word,get_part_of_speech(word)))
return Lemmatized_Words
</code></pre>
<p>(function calling)</p>
<pre><code>x2=list()
for word in dfStopwordRemoved.select('filteredreviewText'):
x_temp = Lemmatizing_Words(word)
x2.append(x_temp)
</code></pre>
<p>please refer the below link for the error
<a href=""https://i.sstatic.net/9j1Gm.png"" rel=""nofollow noreferrer"">Error</a></p>
"
"62719639","Does anyone know a fast way to retrieve all 9 letter English words? - Lexical Database Searching (wordnet, python)","2020-07-03 16:49:51","0","210","0","1","","62719798","<p>I am trying to sift through a lexical database (ideally wordnet via NLTK in Python) and extract all 9 letter words. Does anyone know how to do this? The documentation did not show any promising avenues.</p>
<p>I can't just try every 9 letter combination and check if it is defined as this will take forever. However, simply iterating through the lexical database and extracting 9 letter words is feasible.</p>
<p>If I could sort the database in advance, I know this could be very fast.</p>
<p>So this all seems possible and crossword solvers and dictionary programs must have a way of doing this. Does anyone know how to approach this in Python?</p>
"
"62712963","Using spacy to lemmatize a column of parsed html text in a Pandas Dataframe","2020-07-03 09:56:25","3","2441","4","1","","62714641","<p>I want to do something quite trivial but struggeled to write the function to do it. For a NLP Multiclass Classification task I have to preprocess a pandas DataFrame. The column of interest is parsed html text (column: &quot;tweet&quot;). I normalize my data (lowercase, remove punctuation, stopwords, ...) and then I want to lemmatize it using spacy and write it back as a column. However, I can't get the function together. I found a couple of examples on SO, but they all use lists and I cannot translate that to a DF. Because I have a DataFrame which is quite large (10GB) I wanted to use a function which is not tooooo slow. Any help or suggestions would be appreciated. Thank you :)</p>
<pre><code># My real text is in german, but since Englisch is more frequent I use &quot;en_core_web_sm&quot; here
import spacy
en_core = spacy.load('en_core_web_sm')

# Create DataFrame
pos_tweets = [('I love this car', 'positive'), ('This view is amazing', 'positive'), ('I feel great this morning', 'positive'), ('I am so excited about the concert', 'positive'), ('He is my best friend', 'positive')]
df = pd.DataFrame(pos_tweets)
df.columns = [&quot;tweet&quot;,&quot;class&quot;]

# Normalization
df['tweet'] = [entry.lower() for entry in df['tweet']]
# Tokenization
df[&quot;tokenized&quot;] = [w.split() for w in df[&quot;tweet&quot;]]

# Lemmatization
# This is where I struggle. I can't get together the English Model en_core, lemma_ and stuff :(
df[&quot;lemmatized&quot;] = df['tokenized'].apply(lambda x: [en_core(y.lemma_) for y in x])
</code></pre>
"
"62608470","Replace all collocations in a text file with a dictionary of collocations in python","2020-06-27 09:53:42","2","202","3","1","","62610586","<p>I'm trying to replace substrings in a text file [corpus.txt] with some other substrings[collocation|ngram] using python. I have the list of possible substrings in a file sub.txt containing the following:</p>
<pre><code>dogs chase
birds eat
chase birds
chase cat
chase birds .
</code></pre>
<p>and a corpus.txt containing some texts as below:</p>
<pre><code>dogs chase cats around
dogs bark
cats meow
dogs chase birds
cats chase birds , birds eat grains
dogs chase the cats
the birds chirp
</code></pre>
<p>with the desired output</p>
<pre><code>&lt;bop&gt; dogs chase &lt;eop&gt; cats around
dogs bark
cats meow
&lt;bop&gt; dogs chase &lt;eop&gt; birds 
cats &lt;bop&gt; chase birds &lt;eop&gt; , &lt;bop&gt; birds eat &lt;eop&gt; grains
&lt;bop&gt; dogs chase &lt;eop&gt; the cats
the birds chirp
</code></pre>
<p>And my python code with multiprocessing (used multiprocessing due to the size of the <code>corpus</code> and <code>sub</code>)</p>
<pre><code>import sys
import string
import time
from multiprocessing import Pool
import re
import itertools
flatten = itertools.chain.from_iterable

#corpus_dir =  sys.argv[1]
#ngram_dir = sys.argv[2]

#f = open(corpus_dir) # Open file on read mode
#corpus = f.read().split(&quot;\n&quot;) # Create a list containing all lines
#f.close() # Close file

#f2 = open(ngram_dir) # Open file on read mode
#sub = f2.read().split(&quot;\n&quot;) # Create a list containing all lines
#f2.close() # Close file

sub = ['dogs chase', 'birds eat', 'chase birds', 'chase cat', 'chase birds .']
corpus = [' dogs chase cats around ', ' dogs bark ', ' cats meow ', ' dogs chase birds ', ' cats chase birds , birds eat grains ', ' dogs chase the cats ', ' the birds chirp ']
print(&quot;The corpus has &quot;, len(corpus))


sbsx = { &quot; &quot;+ng+&quot; &quot; : &quot; &lt;bop&gt; &quot;+ng+&quot; &lt;eop&gt; &quot; for ng  in sub }
def multiple_replace(string, rep_dict):
     pattern = re.compile(&quot;|&quot;.join([re.escape(k) for k in sorted(rep_dict,key=len,reverse=True)]), flags=re.DOTALL)
     print(&quot;replaced = &quot;)
     return pattern.sub(lambda x: rep_dict[x.group(0)], string)

def f(a_list):
    out = [multiple_replace(sent, sbsx) for sent in a_list]
    '''out = []
    for sent in a_list:
      c = multiple_replace(sent, sbsx)
      out.append(c)
      #print(c)
      time.sleep(0.01)
'''
    return out

def f_amp(a_list):
    #chunks = [a_list[i::5] for i in range(5)]
    chunks = [a_list[x:x+5] for x in range(0, len(a_list), 5)]
    print(len(chunks))

    pool = Pool(processes=10)

    result = pool.map_async(f, chunks)

    while not result.ready():
        print(&quot;Running...&quot;)
        time.sleep(0.5)

    return list(flatten(result.get()))


final_anot = f_amp(corpus)
print(final_anot)

</code></pre>
<p>I added already initialized <code>corpus</code> and <code>sub</code> variable (in the snippet above) to show how the code works.  Both <code>corpus.txt</code> and <code>sub.txt</code> contains millions (200M+ and 4M+ respectively) of lines in the actual setting. I needed a code that can do the task efficiently and I have tried <code>Multiprocessing</code> with <code>pool</code> but it is going to take weeks to complete. Are there other efficient and fast ways to go about the task??</p>
"
"62601716","In Spacy NLP, how extract the agent, action, and patient -- as well as cause/effect relations?","2020-06-26 19:35:52","0","1376","0","1","","62662027","<p>I would like to use Space to extract word relation information in the form of &quot;agent, action, and patient.&quot; For example, &quot;Autonomous cars shift insurance liability toward manufacturers&quot; -&gt; (&quot;autonomous cars&quot;, &quot;shift&quot;, &quot;liability&quot;) or (&quot;autonomous cars&quot;, &quot;shift&quot;, &quot;liability towards manufacturers&quot;). In other words, &quot;who did what to whom&quot; and &quot;what applied the action to something else.&quot; I don't know much about my input data, so I can't make many assumptions.</p>
<p>I also want to extract logical relationships. For example, &quot;Whenever/if the sun is in the sky, the bird flies&quot; or cause/effect cases like &quot;Heat makes ice cream melt.&quot;</p>
<p>For dependencies, Space recommends iterating through sentences word by word and finding the root that way, but I'm not sure what clear pattern in traversal to use in order to get the information in a reliable way I can organize. My use case involves structuring these sentences into a form that I can use for queries and logical conclusions. This might be comparable to my own mini Prolog data store.</p>
<p>For cause/effect, I could hard-code some rules, but then I still need to find a way of reliably traversing the dependency tree and extracting information. (I will probably combine this with core resolution using neuralcoref and also word vectors and concept net to resolve ambiguities, but this is a little tangential.)</p>
<p>In short, the question is really about how to extract that information / how best to traverse.</p>
<p>On a tangential note, I am wondering if I really need a constituency tree as well for phrase-level parsing to achieve this. I think that Stanford provides that, but Spacy might not.</p>
"
"62601020","How to add custom rules to spaCy tokenizer to break down HTML in single tokens?","2020-06-26 18:42:33","4","1980","0","1","","66268015","<p>I know there are a lot of resources out there for this problem, but I could not get spaCy to do exactly what I want.</p>
<p>I would like to add rules to my spaCy tokenizer so that HTML tags (such as <code>&lt;br/&gt;</code> etc...) in my text would be a single token.</p>
<p>I am right now using the &quot;merge_noun_chunks&quot; pipe, so I get tokens like this one:<br />
<code>&quot;documentation&lt;br/&gt;The Observatory Safety System&quot;</code> (this is a single token)</p>
<p>I would like to add a rule so that this would get split into 3 tokens:<br />
<code>&quot;documentation&quot;, &quot;&lt;br/&gt;&quot;, &quot;The Observatory Safety System&quot;</code><br />
I've looked up a lot of resources: <a href=""http://www.longest.io/2018/01/27/spacy-custom-tokenization.html"" rel=""nofollow noreferrer"">here</a>, <a href=""https://stackoverflow.com/questions/47549856/tokenizing-an-html-document"">also here</a>. But I couldn't get that to work in my case</p>
<p>I have tried this:</p>
<pre><code>    
    infix_re = re.compile(r'''&lt;[\w+]\/&gt;''')
    prefix_re = compile_prefix_regex(nlp.Defaults.prefixes)
    suffix_re = compile_suffix_regex(nlp.Defaults.suffixes)

    return Tokenizer(nlp.vocab, prefix_search=prefix_re.search,
                                suffix_search=suffix_re.search,
                                infix_finditer=infix_re.finditer,
                                token_match=None)
</code></pre>
<p>I am not sure I understand exactly what changing the infix does. Should I also remove <code>&lt;</code> from prefixes <a href=""https://stackoverflow.com/a/52595182/13020673"">as suggested here</a>?</p>
"
"62591705","Optimizing the runtime for cleaning n-grams","2020-06-26 09:19:37","1","193","5","1","","62593372","<hr />
<p>I am trying to <code>clean</code> my <code>n-grams</code> obtained from the text column. I also have 2 list of <code>stopwords</code> I want to remove but only at specific places(when it occurs as a first word in the n-gram or occurs as a last word in the n-gram) and I am also looking to <code>remove</code> my n-grams which contain only <code>numbers or %</code>. The following code takes over 10 minutes to process around a million n-grams.</p>
<pre><code>def clean_ngram(ng):
    if 'percent' in ng:
        ng = ng.replace('percent', '%')
    if 'point' in ng:
        ng = ng.replace('point', '.')
    if ng.split(' ')[0] not in stopwords['First'].dropna().values \
        and ng.split(' ')[-1] not in stopwords['Last '].dropna().values \
          and (bool(re.match(r&quot;^[0-9.% ]+$&quot;, ng)) == False):
              return ng

df['Word'] = df['Word'].apply(lambda x: clean_ngram(x))
</code></pre>
<p>I have also tried multiprocessing but I had to terminate the process after 30 minutes as it was still running. The following is the code for the same:</p>
<pre><code>p = Pool(processes=2)
df['Word'] = p.map(clean_ngram, df['Word'])
p.close()
p.join()
</code></pre>
<p>Is there any way I can optimize my code for a considerably less run-time? Any help would be appreciated.
Thanks in advance :)</p>
"
"62581363","How to get indices of words in a Spacy dependency parse?","2020-06-25 17:48:27","0","1414","0","1","","62584909","<p>I am trying to use Spacy to extract word relations/dependencies, but am a little unsure about how to use the information it gives me. I understand how to generate the visual dependency tree for debugging.</p>
<p>Specifically, I don’t see a way to map the list of children of a token to a specific token. There is no index—just a list of words.</p>
<p>Looking at the example here: <a href=""https://spacy.io/usage/linguistic-features#dependency-parse"" rel=""nofollow noreferrer"">https://spacy.io/usage/linguistic-features#dependency-parse</a></p>
<p><code>nlp(&quot;Autonomous cars shift insurance liability toward manufacturers&quot;)</code></p>
<p>Also, if the sentence were <code>nlp(&quot;Autonomous cars shift insurance liability toward manufacturers of cars”)</code>, how would I disambiguate between the two instances of cars?</p>
<p>The only thing I can think of is that maybe these tokens are actually reference types that I can map to indices myself. Is that the case?</p>
<p>Basically, I am looking to start with getting the predicates and args to understand “who did what to whom and how/using what”.</p>
"
"62497616","How to parse this array from a ML result?","2020-06-21 11:02:51","1","66","0","2","","62498197","<p>How would one parse this print out? One would imagine it needs to assigned to a variable.</p>
<pre><code>(array([[1., 1., 1., 1.]]), array([[1605, 1606, 1698, 1607]], dtype=int64))
</code></pre>
<p>For example, how would one parse 1605?</p>
<p>And/or, how would one parse 1605, 1606, 1698, 1607 from the second array?</p>
<p>This is a NLP result which is being worked with for the first time and your answer is very much appreciated. Thank you.</p>
<p>Ps. To clarify my question, this output is one of many NN NLP outputs to be parsed in a virtual environment, running in the API of an app. So, given this output, how would one parse only the second array, which are rows to be returned from a df? ie: One can not manually separate the arrays. Gervais' solution works in practice by manually typing in &quot;np.&quot;, however this solution does not seem sustainable in a repetitive production environment, at least not by using string formatting. Any further insight into automating the parsing with out without numpy while the app is deployed would also be much appreciated. Thank you.</p>
"
"62496544","How to know the words associated with a specific class in NLP model?","2020-06-21 09:09:19","2","759","0","1","","62496722","<p>I have trained an NLP model for &quot;Consumer Complaints Classification&quot; using Logistic regression algorithm and TF-IDF vectorizer. I want to know the words that my model associates with a particular class. I am looking for something like this -<br />
<strong>Class 1</strong> =  <strong>[&quot;List of words that help my model identify that an input text belongs to this class&quot;]</strong></p>
"
"62328151","Best way to retrieve top tokens in TF-IDF models","2020-06-11 15:41:46","0","384","0","1","","62328961","<p>How may one go about getting an overview of most important tokens from a SciKit-learn pipeline with the following components:</p>

<pre><code>multinb = Pipeline([('vect', CountVectorizer()),
           ('tfidf', TfidfTransformer()),
           ('clf', MultinomialNB()),
          ])

multinb.fit(X_train, y_train)
</code></pre>

<p>Looking for a simple snippet that visualizes/plots the top-weighted tokens overall X)</p>
"
"62291441","singularize noun phrases with spacy","2020-06-09 20:37:46","0","1373","2","2","","62336209","<p>I am looking for a way to singularize noun chunks with spacy</p>

<pre><code>S='There are multiple sentences that should include several parts and also make clear that studying Natural language Processing is not difficult '
nlp = spacy.load('en_core_web_sm')
doc = nlp(S)

[chunk.text for chunk in doc.noun_chunks]
# = ['an example sentence', 'several parts', 'Natural language Processing']
</code></pre>

<p>You can also get the ""root"" of the noun chunk:</p>

<pre><code>[chunk.root.text for chunk in doc.noun_chunks]
# = ['sentences', 'parts', 'Processing']
</code></pre>

<p>I am looking for a way to singularize those roots of the chunks.</p>

<p>GOAL: Singulirized: ['sentence', 'part', 'Processing']</p>

<p>Is there any obvious way? Is that always depending on the POS of every root word?</p>

<p>Thanks</p>

<p>note:
I found this: <a href=""https://www.geeksforgeeks.org/nlp-singularizing-plural-nouns-and-swapping-infinite-phrases/"" rel=""nofollow noreferrer"">https://www.geeksforgeeks.org/nlp-singularizing-plural-nouns-and-swapping-infinite-phrases/</a>
but that approach looks to me that leads to many many different methods and of course different for every language. ( I am working in EN, FR, DE)</p>
"
"62272958","finding the POS of the root of a noun_chunk with spacy","2020-06-08 23:58:16","0","917","0","1","","62274548","<p>When using spacy you can easily loop across the noun_phrases of a text as follows:</p>

<pre><code>S='This is an example sentence that should include several parts and also make clear that studying Natural language Processing is not difficult'
nlp = spacy.load('en_core_web_sm')
doc = nlp(S)

[chunk.text for chunk in doc.noun_chunks]
# = ['an example sentence', 'several parts', 'Natural language Processing']
</code></pre>

<p>You can also get the ""root"" of the noun chunk:</p>

<pre><code>[chunk.root.text for chunk in doc.noun_chunks]
# = ['sentence', 'parts', 'Processing']
</code></pre>

<p>How can I get the POS of every of those words (even if looks like the root of a noun_phrase is always a noun), and how can I get the lemma, the shape and the word in singular of that particular word.</p>

<p>Is that even possible?</p>

<p>thx.</p>
"
"62268302","Spacy es_core_news_sm model not loading","2020-06-08 17:55:05","1","5832","0","2","","62313010","<p>I'm trying to use Spacy for <code>pos tagging</code> in Spanish, for this I have checked the official documentation and also have read various post in Stackoverflow nonetheless neither has worked to me.</p>

<p>I have Python 3.7 and Spacy 2.2.4 installed and I'm running my code from a jupyter notebook</p>

<p>So as <a href=""https://spacy.io/models"" rel=""nofollow noreferrer"">documentation</a> suggests I tried:</p>

<p>From my terminal:</p>

<pre><code>python -m spacy download en_core_web_sm
</code></pre>

<p>This gave the result:</p>

<pre><code>Download and installation successful
</code></pre>

<p>Then in my jupyter notebook:</p>

<pre><code>import spacy
nlp = spacy.load(""es_core_news_sm"")
</code></pre>

<p>And I got the following error:</p>

<pre><code>ValueError: [E173] As of v2.2, the Lemmatizer is initialized with an instance of Lookups containing the lemmatization tables. See the docs for details: https://spacy.io/api/lemmatizer#init
</code></pre>

<p>Additionally, I tried:</p>

<pre><code>import spacy

nlp = spacy.load(""es_core_news_sm"")
</code></pre>

<p>And this gave me a different error:</p>

<pre><code>OSError: Can't find model 'es_core_news_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory
</code></pre>

<p>Could you please help me to solve this error?</p>
"
"62259100","How to find semantic similarity between a list of words?","2020-06-08 09:36:12","2","1258","2","1","","62259450","<p>Input:</p>

<pre><code>listToStr = 'degeneration agents alpha alternative amd analysis angiogenesis anti anti vegf appears associated based best bevacizumab blindness blood'
</code></pre>

<p>Code I am using:</p>

<pre><code>simi = []
tokens = nlp(listToStr) 
length = len(tokens)

for i in range(length):
    #print(i)
    sim = tokens[i].similarity(tokens[i+1])
    simi.append(sim)
print(simi)
</code></pre>

<p>Error:</p>

<pre><code>[E040] Attempt to access token at 17, max length 17.
</code></pre>

<p>How can I remove this error?</p>

<p>I am using spacy. Here's the link to it:
<a href=""https://www.geeksforgeeks.org/python-word-similarity-using-spacy/#:~:text=Python%20%7C%20Word%20Similarity%20using%20spaCy,simple%20method%20for%20this%20task."" rel=""nofollow noreferrer"">https://www.geeksforgeeks.org/python-word-similarity-using-spacy/#:~:text=Python%20%7C%20Word%20Similarity%20using%20spaCy,simple%20method%20for%20this%20task.</a></p>
"
"62248455","lemmatize() missing 1 required positional argument: 'word'","2020-06-07 16:35:42","0","1333","13","1","","62250362","<p>When I try to pass this on to the lemmatizer like this:</p>

<pre><code>def lemmatization(token_txt):
    text = [wn.lemmatize(word) for word in token_txt]
   # text = [[wn.lemmatize(word) for word in l] for l in token_text]
    return text

data['Tweet_lem'] = data['Tweet_sw'].apply(lambda x:lemmatization(x))
data.head()
</code></pre>

<p>i get following error</p>

<pre><code>TypeError: lemmatize() missing 1 required positional argument: 'word'
</code></pre>

<p>when i let it run like this:</p>

<pre><code>def lemmatization(token_txt):
      # text = [wn.lemmatize(word) for word in token_txt]
        text = [[wn.lemmatize(word) for word in l] for l in token_text]
        return text


data['Tweet_lem'] = data['Tweet_sw'].apply(lambda x:lemmatization(x))
data.head()
</code></pre>

<p>i get this error</p>

<blockquote>
  <p>NameError: name 'token_text' is not defined</p>
</blockquote>

<p>What do i have to do? </p>

<p>I am trying to apply the function on punctuation and stop words removed sentences. the steps stemming and tokenization are not applied. </p>

<p><a href=""https://i.sstatic.net/o16eG.png"" rel=""nofollow noreferrer"">DataSet</a></p>

<p><a href=""https://i.sstatic.net/VcDrT.png"" rel=""nofollow noreferrer"">DataSet new</a></p>

<p><a href=""https://i.sstatic.net/IIEPv.png"" rel=""nofollow noreferrer"">Full example first part</a></p>

<p><a href=""https://i.sstatic.net/sdiRO.png"" rel=""nofollow noreferrer"">Full example second part</a></p>
"
"62230139","GitHub Actions fails when NLTK ""punkt"" is needed","2020-06-06 10:05:53","2","553","3","2","","64862016","<p>I have written some code which needs to use NLTK's punkt. I have included <code>nltk</code> in the <code>requirements.txt</code> and in the <code>setup.py</code>. However, when I run the build of my project using GitHub actions, it fails with this error.</p>

<pre><code>E       LookupError:   
E       **********************************************************************  
E         Resource punkt not found.  
E         Please use the NLTK Downloader to obtain the resource:  
E       
E         &gt;&gt;&gt; import nltk  
E         &gt;&gt;&gt; nltk.download('punkt') 
</code></pre>

<p>What is the standard way to tell GitHub actions that it needs <code>'punkt'</code> without hard coding <code>nltk.download('punkt')</code> somewhere into the code?
Should I add a line in the <code>ci.yml</code> file, and what is the best way to do it?</p>
"
"62153385","How do I train a pseudo-projective parser on spaCy?","2020-06-02 13:40:32","0","350","0","1","","62160956","<p>I am trying to train a parser for custom semantics following the sample code from <a href=""https://raw.githubusercontent.com/explosion/spaCy/master/examples/training/train_intent_parser.py"" rel=""nofollow noreferrer"">https://raw.githubusercontent.com/explosion/spaCy/master/examples/training/train_intent_parser.py</a>
The idea is to get a non-projective parse so when I pass a text like: <code>ROOT AAAA BBBB 12 21</code> 12 becomes a child of AAAA and 21 becomes a child of BBBB. To test this I am training only this case and testing this same case but it doesn't seem to work, what I get as a response is:</p>

<pre><code>[('ROOT', 'ROOT', 'ROOT'), ('AAAA', 'LETTERS', 'ROOT'), ('BBBB', 'LETTERS', 'ROOT'), ('12', 'NUMBERS', 'BBBB'), ('21', 'NUMBERS', 'BBBB')]
</code></pre>

<p>As you can see both numbers are dependent on BBBB when 12 should be dependent on AAAA.</p>

<p>The code I am using to train and test is:</p>

<pre><code>import plac
import random
import spacy
from spacy.util import minibatch, compounding

TRAIN_DATA = list()

samples = 1000
for _ in range(samples):
    sample = (
        'ROOT AAAA BBBB 12 21',
        {
            'heads': [0, 0, 0, 1, 2],
            'deps': ['ROOT', 'LETTERS', 'LETTERS', 'NUMBERS', 'NUMBERS']
        }
    )
    TRAIN_DATA.append(sample)

def test_model(nlp):
    texts = ['ROOT AAAA BBBB 12 21']
    docs = nlp.pipe(texts)
    for doc in docs:
        print(doc.text)
        print([(t.text, t.dep_, t.head.text) for t in doc if t.dep_ != ""-""])

@plac.annotations(
    model=(""Model name. Defaults to blank 'en' model."", ""option"", ""m"", str),
    n_iter=(""Number of training iterations"", ""option"", ""n"", int),
)

#  Just in case I am using the german model since it supports pseudo-projective parsing (https://explosion.ai/blog/german-model#word-order)
def main(model='de_core_news_sm', n_iter=15):
    """"""Load the model, set up the pipeline and train the parser.""""""
    if model is not None:
        nlp = spacy.load(model)  # load existing spaCy model
        print(""Loaded model '%s'"" % model)
    else:
        nlp = spacy.blank(""en"")  # create blank Language class
        print(""Created blank 'en' model"")

    # We'll use the built-in dependency parser class, but we want to create a
    # fresh instance – just in case.
    if ""parser"" in nlp.pipe_names:
        nlp.remove_pipe(""parser"")
    parser = nlp.create_pipe(""parser"")
    nlp.add_pipe(parser, first=True)

    for text, annotations in TRAIN_DATA:
        for dep in annotations.get(""deps"", []):
            parser.add_label(dep)

    pipe_exceptions = [""parser"", ""trf_wordpiecer"", ""trf_tok2vec""]
    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]
    with nlp.disable_pipes(*other_pipes):  # only train parser
        optimizer = nlp.begin_training()
        for itn in range(n_iter):
            random.shuffle(TRAIN_DATA)
            losses = {}
            # batch up the examples using spaCy's minibatch
            batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))
            for batch in batches:
                texts, annotations = zip(*batch)
                nlp.update(texts, annotations, sgd=optimizer, losses=losses)
            print(""Losses"", losses)

    # test the trained model
    test_model(nlp)

if __name__ == ""__main__"":
    plac.call(main)
</code></pre>

<p>So, what am I doing wrong?</p>

<p>Thank you in advance for any help on this!</p>
"
"62043494","Remove words that occur only once and with low IDF in R","2020-05-27 12:48:06","0","1040","0","3","","62044151","<p>I have a dataframe with a column with some text in it. I want to do three data pre-processing steps: </p>

<p>1) remove words that occur only once
2) remove words with low inverse document frequency (IDF) and 3) remove words that occur most frequently</p>

<p>This is an example of the data:</p>

<pre><code>head(stormfront_data$stormfront_self_content)

Output:

[1] ""        , ,    stormfront!  thread       members  post  introduction,     \"".\""     stumbled   white networking site,    reading &amp; decided  register  account,      largest networking site     white brothers,  sisters!    read : : guidelines  posting - stormfront introduction  stormfront - stormfront  main board consists   forums,  -forums   : newslinks &amp; articles - stormfront ideology  philosophy - stormfront activism - stormfront       network   local level: local  regional - stormfront international - stormfront  ,  .  addition   main board   supply  social groups    utilized  networking.  final note:      steps    sustaining member,  core member      site online,   affords  additional online features. sf: shopping cart   stormfront!""
[2] ""bonjour      warm  brother !   forward  speaking     !""                                                                                                                      
[3] "" check   time  time   forums.      frequently    moved  columbia   distinctly  numbered.    groups  gatherings         ""                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  
[4] ""  !  site  pretty nice.    amount  news articles.  main concern   moment  islamification.""                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                
[5] "" , discovered  site   weeks ago.  finally decided  join   found  article  wanted  share  .   proud   race   long time    idea  site    people  shared  views existed.""                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    
[6] ""  white brothers,  names jay      member   years,        bit  info    ?    stormfront meet ups     ? stay strong guys    jay, uk""                                                                                                           

</code></pre>

<p>Any help would be greatly appreciated, as I am not too familiar with R.</p>
"
"62012899","Replacing a set of words in a large text file","2020-05-26 00:41:48","0","176","2","1","","62018797","<p>I have a large txt file(around 20GB) I want to replace all instances of a list of words from this large file. I am struggling to find a way to optimize this code. This is leading to me processing this file for a very long time.</p>

<p>what could I improve ?
<div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>    corpus_input =  open(corpus_in,""rt"")
    corpus_out = open(corpus_out,""wt"")
    for line in corpus_input:
        temp_str=line
        for word in dict_keys:
            if word in line:
                new_word = word+""_lauren_ipsum""
                temp_str = re.sub(fr'\b{word}\b',new_word,temp_str)

            else:
                continue
        
        corpus_out.writelines(temp_str)

     corpus_input.close()
     corpus_out.close()</code></pre>
</div>
</div>
</p>
"
"61982023","Using WordNetLemmatizer.lemmatize() with pos_tags throws KeyError","2020-05-24 05:32:22","2","3390","0","1","","61985267","<p>I just read that lemmatization results are best when assisted with pos_tags. Hence I followed the below code but getting KeyError for calculated POS_tags. Below is the code</p>

<pre><code>   from nltk import pos_tag
   x['Phrase']=x['Phrase'].transform(lambda value:value.lower())
   x['Phrase']=x['Phrase'].transform(lambda value:pos_tag(value))
</code></pre>

<p>Output after 3rd line (after calculating POS Tags)
<a href=""https://i.sstatic.net/VTiwd.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/VTiwd.png"" alt=""enter image description here""></a></p>

<pre><code>   from nltk.stem import WordNetLemmatizer 
   lemmatizer = WordNetLemmatizer()
   x['Phrase_lemma']=x['Phrase'].transform(lambda value: ' '.join([lemmatizer.lemmatize(a[0],pos=a[1]) for a in  value]))
</code></pre>

<p>Error:</p>

<pre><code> KeyError                                  Traceback (most recent call last)
  &lt;ipython-input-8-c2400a79a016&gt; in &lt;module&gt;
  1 from nltk.stem import WordNetLemmatizer
  2 lemmatizer = WordNetLemmatizer()
  ----&gt; 3 x['Phrase_lemma']=x['Phrase'].transform(lambda value: ' '.join([lemmatizer.lemmatize(a[0],pos=a[1]) for a in  value]))

 KeyError: 'DT'
</code></pre>
"
"61952877","Chomsky hierarchy - examples with real languages","2020-05-22 10:24:48","3","1160","0","1","","61959640","<p>I'm trying to understand the four levels of the Chomsky hierarchy by using some real languages as models. He thought that all the natural languages can be generated through a <strong>Context-free Grammar</strong>, but Schieber contradicted this theory proving that languages such as Swiss German can only be generated through <strong>Context-sensitive grammar</strong>. Since Chomsky is from US, I guess that the American language is an example of Context-free grammar. My questions are:</p>

<ol>
<li>Are there languages which can be generated by regular grammars (type 3)?</li>
<li>Since the Recursively enumerable grammars can generate all languages, why not using that? Are they too complicated and less linear?</li>
<li>What the characteristic of Swiss German which make it impossible to be generated through Context-free grammars?</li>
</ol>
"
"61907589","How to calculate TF-IDF (using tft.tfidf function) in Tensorflow Transform","2020-05-20 07:39:34","2","2983","0","1","","61908293","<p>While going through the docs in tensorflow transform I came across function to perform TD-IDF.</p>

<pre><code>tft.tfidf(
    x, vocab_size, smooth=True, name=None
)
</code></pre>

<p>As the docs in not clear in providing example of how to perform TD-IDF I tried using example_string </p>

<pre><code>example_strings=[[""I"", ""like"", ""pie"", ""pie"", ""pie""], [""yum"", ""yum"", ""pie""]]
</code></pre>

<p>and a vocab size of 1000.(Just random number) but the below code giving me an attribute error.</p>

<pre><code>tft.tfidf(example_strings, vocab_size=1000)
</code></pre>

<p><strong>AttributeError: 'list' object has no attribute 'indices'</strong></p>

<p>Please help me to figure this out as I am naive to Tensorflow transform ops.</p>
"
"61827301","Combine additional data to my TFIDF array","2020-05-15 19:54:06","0","2173","0","2","","61828020","<p>I'm trying to create a text classification model using scikit-learn. At first, I was using only the text's tfidf array as a feature. The structure of my dataset can be seen below (the dataset is stored in a pandas dataframe called <code>df</code>):</p>

<pre><code>&gt;&gt;&gt;df.head(2)

       id_1    id_2    id_3    target    text
       11      454     320     197       some text here
       15      440     111     205       text goes here too

&gt;&gt;&gt;df.info()

    Data columns (total 5 columns):
     #   Column    Non-Null Count   Dtype 
    ---  ------    --------------   ----- 
     0   id_1      500 non-null     uint16
     1   id_2      500 non-null     uint16
     2   id_3      500 non-null     uint16
     3   target    500 non-null     uint16
     4   text      500 non-null     object
</code></pre>

<p>So, I split the train/test datasets and proceeded with creating the tfidf vector and transforming the data for training and testing.</p>

<pre><code>X_train, X_test, y_train, y_test = train_test_split(df['text'], df['target'], random_state=0)

vectorizer = TfidfVectorizer(max_features=500, decode_error=""ignore"", ngram_range=(1, 2))
vectorizer.fit(X_train)

X_train_tfidf = vectorizer.transform(X_train)
X_test_tfidf  = vectorizer.transform(X_test)
</code></pre>

<p>So far apparently the code is working ok. However, there was a need to improve the algorithm, including yet another feature. For this improvment, I want to add the <code>id_1</code> column to my features (it can be an important information to our ML model). So, in addition to my tfidf matrix, I would like to add this column (<code>id_1</code>) with my new feature, so that I can pass it as a parameter to train the model.</p>

<p>What I have tried:</p>

<pre><code>X_train, X_test, y_train, y_test = train_test_split(df['text'], df['target'], random_state=0)

vectorizer = TfidfVectorizer(max_features=500, decode_error=""ignore"", ngram_range=(1, 2))
vectorizer.fit(X_train)

X_train_tfidf = vectorizer.transform(X_train)
X_test_tfidf  = vectorizer.transform(X_test)

X_train_all_features = pd.concat([pd.DataFrame(X_train_tfidf.toarray()), df['id_1']], axis = 1)
</code></pre>

<p>So, the shape of my structure is</p>

<pre><code>&gt;&gt;&gt;print(X_train_tfidf.shape)

(37, 500) # as expected (I'm loading 50 lines, so this is about 75%)

&gt;&gt;&gt;print(X_train_all_features.shape)

(50, 501) # n of columns is expected, but not the lines, because the df[id_1] was not splited in train_test_split function
</code></pre>

<p>In a nutshel, I want pass to my ML algorithm something like the image below - my tfidf vector and my <code>id_1</code> features:</p>

<p><a href=""https://i.sstatic.net/szpQR.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/szpQR.png"" alt=""tfidf concat id_1""></a></p>

<p>I feel that I am missing something extremely basic, but even with all the research I have been able to solve my problem satisfactorily. I'm honestly lost in that part of the problem and I don't know how to evolve from here</p>
"
"61787119","FastText 0.9.2 - why is recall 'nan'?","2020-05-14 00:21:25","5","2059","3","1","","61923324","<p>I trained a supervised model in FastText using the Python interface and I'm getting weird results for precision and recall.</p>
<p>First, I trained a model:</p>
<pre class=""lang-py prettyprint-override""><code>model = fasttext.train_supervised(&quot;train.txt&quot;, wordNgrams=3, epoch=100, pretrainedVectors=pretrained_model)
</code></pre>
<p>Then I get results for the test data:</p>
<pre class=""lang-py prettyprint-override""><code>def print_results(N, p, r):
    print(&quot;N\t&quot; + str(N))
    print(&quot;P@{}\t{:.3f}&quot;.format(1, p))
    print(&quot;R@{}\t{:.3f}&quot;.format(1, r))

print_results(*model.test('test.txt'))
</code></pre>
<p>But the results are always odd, because they show precision and recall @1 as identical, even for different datasets, e.g. one output is:</p>
<pre><code>N   46425
P@1 0.917
R@1 0.917
</code></pre>
<p>Then when I look for the precision and recall for each label, I always get recall as 'nan':</p>
<pre class=""lang-py prettyprint-override""><code>print(model.test_label('test.txt'))
</code></pre>
<p>And the output is:</p>
<pre><code>{'__label__1': {'precision': 0.9202150724134941, 'recall': nan, 'f1score': 1.8404301448269882}, '__label__5': {'precision': 0.9134956983264135, 'recall': nan, 'f1score': 1.826991396652827}}
</code></pre>
<p>Does anyone know why this might be happening?</p>
<p>P.S.: To try a reproducible example of this behavior, please refer to <a href=""https://github.com/facebookresearch/fastText/issues/1072"" rel=""nofollow noreferrer"">https://github.com/facebookresearch/fastText/issues/1072</a> and run it with FastText 0.9.2</p>
"
"61770575","sklearn TfidfVectorizer custom ngrams without characters from regex pattern","2020-05-13 09:09:53","0","1159","0","2","","61771587","<p>I would like to perform custom ngram vectorization using <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"" rel=""nofollow noreferrer"">sklearn TfidfVectorizer</a>. The generated ngrams should not contain any character from a given regex pattern. Unfortunately the custom tokenizer function is completely ignored when <code>analyzer='char'</code> (ngram mode). See the following example:</p>

<pre class=""lang-py prettyprint-override""><code>import re
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer

pattern = re.compile(r'[\.-]'). # split on '.' and on '-'

def tokenize(text):
    return pattern.split(text)

corpus = np.array(['abc.xyz', 'zzz-m.j'])

# word vectorization
tfidf_vectorizer = TfidfVectorizer(tokenizer=tokenize, analyzer='word', stop_words='english')
tfidf_vectorizer.fit_transform(corpus)
print(tfidf_vectorizer.vocabulary_)
# Output -&gt; {'abc': 0, 'xyz': 3, 'zzz': 4, 'm': 2, 'j': 1}
# This is ok!

# ngram vectorization
tfidf_vectorizer = TfidfVectorizer(tokenizer=tokenize, analyzer='char', ngram_range=(2, 2))
tfidf_vectorizer.fit_transform(corpus)
print(tfidf_vectorizer.vocabulary_)
# Output -&gt; {'ab': 3, 'bc': 4, 'c.': 5, '.x': 2, 'xy': 7, 'yz': 8, 'zz': 10, 'z-': 9, '-m': 0, 'm.': 6, '.j': 1}
# This is not ok! I don't want ngrams to include the '.' and '-' chars used for tokenization
</code></pre>

<p>What is the best way to do it?</p>
"
"61757240","Filtering SpaCy noun_chunks by pos_tag","2020-05-12 16:45:08","1","1943","0","2","","61770024","<p>As the subj line says, I'm trying to extract elements of noun_chunks based on their individual POS tags. It seems that elements of a noun_chunk do not have access to the global sentence POS tags.</p>

<p>To demonstrate the issue:</p>

<pre class=""lang-py prettyprint-override""><code>
[i.pos_ for i in nlp(""Great coffee at a place with a great view!"").noun_chunks]
&gt;&gt;&gt; 
AttributeError: 'spacy.tokens.span.Span' object has no attribute 'pos_'
</code></pre>

<p>Here is my inefficient solution:</p>

<pre class=""lang-py prettyprint-override""><code>def parse(text):
    doc = nlp(text.lower())
    tags = [(idx,i.text,i.pos_) for idx,i in enumerate(doc)]

    chunks = [i for i in doc.noun_chunks]

    indices = []
    for c in chunks:
        indices.extend(j for j in range(c.start_char,c.end_char))
    non_chunks = [w for w in ''.join([i for idx,i in enumerate(text) if idx not in indices]).split(' ') 
                  if w != '']

    chunk_words = [tup[1] for tup in tags if tup[1] not in non_chunks and tup[2] not in ['DET','VERB','SYM','NUM']] #these are the POS tags which I wanted to filter out from the beginning!

    new_chunks = []
    for c in chunks:
        new_words = [w for w in str(c).split(' ') if w in chunk_words]
        if len(new_words) &gt; 1:
            new_chunk = ' '.join(new_words)
            new_chunks.append(new_chunk)
    return new_chunks

parse(
""""""
I may be biased about Counter Coffee since I live in town, but this is a great place that makes a great cup of coffee. I have been coming here for about 2 years and wish I would have found it sooner. It is located right in the heart of Forest Park and there is a ton of street parking. The coffee here is great....many other words could describe it, but that sums it up perfectly. You can by coffee by the pound, order a hot drink, and they also have food. On the weekend, there are donuts brought in from Do-Rite Donuts which have almost a cult like following. The food is a little on the high end price wise, but totally worth it. I am a self admitted latte snob and they make an amazing latte here. You can add skim, whole, almond or oat milk and they will make it happen. I always order easy foam and they always make it perfectly. My girlfriend loves the Chai Latte with Oat Milk and I will admit it is pretty good. Give them a try.
"""""")

&gt;&gt;&gt;
['counter coffee',
 'great place',
 'great cup',
 'forest park',
 'street parking',
 'many other words',
 'hot drink',
 'almost cult',
 'high end price',
 'latte snob',
 'amazing latte',
 'oat milk',
 'easy foam',
 'chai latte',
 'oat milk']

</code></pre>

<p>Any quicker approaches to the same solution would be welcomed!</p>
"
"61700506","What does 'theta' mean in a language model?","2020-05-09 16:58:19","0","1681","0","1","","61709975","<p>I know that if X denotes a text , p(X) denotes the language model of the text. And most often , we use maximum likelihood estimation to estimate the language model. 
But in many cases , I find a parameter $\theta$ used to represent a language model. I don't understand the meaning of this $\theta$ . 
For Example , for a document d in a collection what purpose does $\theta$ serve in ' p(d|$\theta$) ' ? </p>

<p>Does $\theta$ represent a maximum likelihood estimator or a language model ? </p>

<p>Can someone please explain this difference between a language model and $\theta$ in depth ? </p>

<p>Thanks in advance ! </p>
"
"61570873","When I use TF-IDF in Natural language processing, it said list is not callable.Can you help me with it?","2020-05-03 07:14:19","0","62","0","1","","61570950","<p>I got error like this :</p>

<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-38-b9ac626e6121&gt; in &lt;module&gt;
      5 
      6 # Fitting TF-IDF to both training and test sets (semi-supervised learning)
----&gt; 7 tfv.fit(list(xtrain) + list(xvalid))
      8 xtrain_tfv =  tfv.transform(xtrain)
      9 xvalid_tfv = tfv.transform(xvalid)

TypeError: 'list' object is not callable
</code></pre>

<p>When I run these codes in python:</p>

<pre><code>tfv = TfidfVectorizer(min_df=3,  max_features=None, 
            strip_accents='unicode', analyzer='word',token_pattern=r'\w{1,}',
            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,
            stop_words = 'english')

# Fitting TF-IDF to both training and test sets (semi-supervised learning)
tfv.fit(list(xtrain) + list(xvalid))
xtrain_tfv =  tfv.transform(xtrain) 
xvalid_tfv = tfv.transform(xvalid)
</code></pre>

<p>P.S. I also tried to convert the xtrain to list with <code>xtrain.tolist()</code>, but it doesn't work for me either.</p>
"
"61560956","Invalid syntax on importing nltk in python 2.7","2020-05-02 14:27:43","5","5103","1","2","","61575357","<p>when I executed the below code in python 2.7 CLI</p>
<pre><code>import nltk
</code></pre>
<p>it is showing the following error</p>
<pre><code>SyntaxError:Invalid Syntax

Traceback (most recent call last):
File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
File &quot;/home/nani/.local/lib/python2.7/site-packages/nltk/__init__.py&quot;, line 128, in &lt;module&gt;
from nltk.collocations import *
File &quot;/home/nani/.local/lib/python2.7/site-packages/nltk/collocations.py&quot;, line 35, in &lt;module&gt;
from nltk.probability import FreqDist
File &quot;/home/nani/.local/lib/python2.7/site-packages/nltk/probability.py&quot;, line 333
print(&quot;%*s&quot; % (width, samples[i]), end=&quot; &quot;)                                      ^
SyntaxError: invalid syntax
</code></pre>
<p>How to fix this?</p>
"
"61498301","Spacy CLI Training Unable to Activate GPU","2020-04-29 09:18:28","1","919","0","1","","61498528","<p>I am trying to train a NER Spacy model on the CLI. Following all the steps necessary I finally created a correct input file, however when trying to train on the GPU I get the message that spacy is unable to activate the GPU, other programs actually are able to use my GPU and cuda is set up correctly. Still it doesn't seem to work, I only have 1 GPU in my computer so I selected -g 0 on the CLI. I can't find any further information as why the GPU cannot be activated, searching the internet has led to nothing either.</p>

<pre><code>Training pipeline: ['ner']
⚠ Unable to activate GPU: 0
Using CPU only
</code></pre>

<ul>
<li>NVIDIA-driver-version: 440.64</li>
<li>CUDA-verion: 10.2</li>
<li>GPU GeForce RTX 2060</li>
</ul>

<p><a href=""https://i.sstatic.net/h1tLu.png"" rel=""nofollow noreferrer"">Image showing the nvidia-smi output</a></p>
"
"61492130","How to optimize this nested loop code dealing with pandas dataframes","2020-04-29 00:16:25","1","271","1","1","","61492599","<p>I am new to optimization and need help improving the run time of this code. It accomplishes my task, but it takes forever. Any suggestions on improving it so it runs faster?</p>

<p>Here is the code:</p>

<pre><code>def probabilistic_word_weighting(df, lookup):

    # instantiate new place holder for class weights for each text sequence in the df
    class_probabilities = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
    for index, row in lookup.iterrows():
        if row.word in df.words.split():
            class_proba_ = row.class_proba.strip('][').split(', ')
            class_proba_ = [float(i) for i in class_proba_]
            class_probabilities = [a + b for a, b in zip(class_probabilities, class_proba_)]

    return class_probabilities

</code></pre>

<p>The two input df's look like this:</p>

<p>df</p>

<pre><code>index                                     word
1                               i  havent  been  back 
2                                            but  its 
3                   they  used  to  get  more  closer 
4                                             no  way 
5       when  we  have  some  type  of  a  thing  for
6                and  she  had  gone  to  the  doctor 
7                                                suze 
8        the  only  time  the  parents  can  call  is
9               i  didnt  want  to  go  on  a  cruise 
10                            people  come  aint  got 
</code></pre>

<p>lookup</p>

<pre><code>index    word                               class_proba
6231    been    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.27899487]
8965    havent  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.27899487]
3270    derive  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.27899487]
7817    a       [0.0, 0.0, 7.451379, 6.552, 0.0, 0.0, 0.0, 0.0]
3452    hello   [0.0, 0.0, 0.0, 0.0, 0.000155327, 0.0, 0.0, 0.0]
5112    they    [0.0, 0.0, 0.00032289312, 0.0, 0.0, 0.0, 0.0, 0.0]
1012    time    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.27899487]
7468    some    [0.000193199, 0.0, 0.0, 0.000212947, 0.0, 0.0, 0.0, 0.0]
6428    people  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.27899487
5537    scuba   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.27899487
</code></pre>

<p>What its doing is essentially iterating through each row in lookup, which contains a word and its relative class weights. If the word is found in any text sequence in df.word, then the class_probabilities for lookup.word get added to the class_probabilities variable assigned to each sequence in df.word. Its looping through every row in df for every iteration on lookup rows.</p>

<p>How can this be done faster?</p>
"
"61446106","What is the difference between parsing and Part Of Speech Tagging?","2020-04-26 18:38:23","5","1375","0","1","","61446569","<p>I know that POS tagging labels each and every word in a sentence with its appropriate Part Of Speech , But isn't that what a Parser does too ? i.e, break a sentence into its component parts? 
I've looked this up on the internet but couldn't find any satisfactory explanation . 
Please clear my doubt.
Thanks in advance </p>
"
"61399545","NLP: How do I combine stemming and tagging?","2020-04-24 01:21:22","0","998","0","2","","61399612","<p>I'm trying to write code which passes in text that has been tokenized and had the stop words filtered out, and then stems and tags it. However, I'm not sure in what order I should stem and tag. This is what I have at the moment:</p>

<pre><code>#### Stemming
ps = PorterStemmer()    # PorterStemmer imported from nltk.stem

stemText = []

for word in swFiltText:    # Tagged text w/o stop words
    stemText.append(ps.stem(word))


#### POS Tagging
def tagging():
    tagTot = []
    try:
        for i in stemText:
            words = nltk.word_tokenize(i)    # I need to tokenize again (idk why?)
            tagged = nltk.pos_tag(words)
            tagTot = tagTot + tagged    # Combine tagged words into list

    except Exception as e:
        print(str(e))
    return tagTot

tagText = tagging()
</code></pre>

<p>At first glance, this works just fine. However, because I stemmed first, <code>pos_tag</code> often mislabels words. For example, it marked ""hous"" as an adjective, when the original word was really the noun ""house"". But when I try to stem after tagging, it gives me an error about how <code>pos_tag</code> can't deal with 'tuples' - I'm guessing this has something to do with the way that the stemmer formats the word list as <code>[('come', 'VB'), ('hous', 'JJ')</code>, etc.</p>

<p>Should I be using a different stemmer/tagger? Or is the error in my code?</p>

<p>Thanks in advance!</p>
"
"61349106","Efficient retrieval of documents represented in the form of multi-dimensional vectors","2020-04-21 17:07:40","2","307","0","2","","61360682","<p>I've trained a deep neural network based model for information retrieval. At the end, my model represents the documents in the form of 128 dimensional vectors. Semantic representations of documents similar to word embedding representation for words (word2vec algorithm). When I give a query to my model, it also represents the query in the same 128 dimensional vector space. Now from the entire vector space, I want to retrieve top k documents closest the the query vector represented in the same vector space. </p>

<p>The similarity measure is cosine similarity which is defined as follows : </p>

<pre><code>sim(Q, D) = np.dot(Q.T, D)/(np.linalg.norm(Q) * np.linalg.norm(D))
</code></pre>

<p>where <code>sim(Q, D)</code> represents similarity between query Q and document D. In simple words, it is dot product of unit vectors of query and document. <br>
Now I have roughly 36 million documents, so calculating cosine similarity for all the documents and the sorting them is not a feasible option for efficient retrieval. I want to efficiently search for the most similar k documents for any query vector represented in the same 128 dimensional vector space. </p>
"
"61244581","Spacy - number of lemma","2020-04-16 07:11:24","0","244","1","1","","61245041","<p>I'm using spacy to replace every word in a sentence with a number/code, after I use the vector as a input of a recurrent neural network.</p>

<pre><code>import spacy
 str=""basing based base""
 sp = spacy.load('en_core_web_sm')
 sentence=sp(str)
 for w in sentence:
    print(w.text,w.lemma)
</code></pre>

<p>In the first layer of Neural network with keras, the Embedding layer, I have to know the max number of words in the look up table, someone know this number?
Thank you</p>
"
"61224496","How can I train spaCy entity link model using GPU?","2020-04-15 08:32:32","1","2430","0","1","","61229363","<p>When I train <strong>spaCy</strong> entity linking model follow the document <a href=""https://github.com/explosion/spaCy/tree/master/bin/wiki_entity_linking"" rel=""nofollow noreferrer"">wiki_entity_linking</a>, and I found that model was trained using cpu. It costs very long time to train epoch.
（About 3 days for 2 epochs in the environment: 16x cpu, 64GB mem）</p>

<p>The command is:
<code>python wikidata_train_entity_linker.py -t 50000 -d 10000 -o xxx</code>. So my question is that how could I do to use GPU for the train phase. </p>
"
"61157314","RuntimeError: Unknown device when trying to run AlbertForMaskedLM on colab tpu","2020-04-11 13:09:53","1","461","0","1","","61226181","<p>I am running the following code on colab taken from the example here: <a href=""https://huggingface.co/transformers/model_doc/albert.html#albertformaskedlm"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/model_doc/albert.html#albertformaskedlm</a></p>

<pre><code>import os
import torch
import torch_xla
import torch_xla.core.xla_model as xm

assert os.environ['COLAB_TPU_ADDR']

dev = xm.xla_device()

from transformers import AlbertTokenizer, AlbertForMaskedLM
import torch

tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')
model = AlbertForMaskedLM.from_pretrained('albert-base-v2').to(dev)
input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"", add_special_tokens=True)).unsqueeze(0)  # Batch size 1

data = input_ids.to(dev)

outputs = model(data, masked_lm_labels=data)
loss, prediction_scores = outputs[:2]
</code></pre>

<p>I haven't done anything to the example code except move <code>input_ids</code> and <code>model</code> onto the TPU device using <code>.to(dev)</code>. It seems everything is moved to the TPU no problem as when I input <code>data</code> I get the following output: <code>tensor([[    2, 10975,    15,    51,  1952,    25, 10901,     3]], device='xla:1')</code></p>

<p>However when I run this code I get the following error:</p>

<pre><code>---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-5-f756487db8f7&gt; in &lt;module&gt;()
      1 
----&gt; 2 outputs = model(data, masked_lm_labels=data)
      3 loss, prediction_scores = outputs[:2]

9 frames
/usr/local/lib/python3.6/dist-packages/transformers/modeling_albert.py in forward(self, hidden_states, attention_mask, head_mask)
    277         attention_output = self.attention(hidden_states, attention_mask, head_mask)
    278         ffn_output = self.ffn(attention_output[0])
--&gt; 279         ffn_output = self.activation(ffn_output)
    280         ffn_output = self.ffn_output(ffn_output)
    281         hidden_states = self.full_layer_layer_norm(ffn_output + attention_output[0])

RuntimeError: Unknown device
</code></pre>

<p>Anyone know what's going on?</p>
"
"61118213","spacy lemmatizing inconsistency with lemma_lookup table","2020-04-09 09:41:55","2","1334","0","2","","61120842","<p>There seems to be an inconsistency when iterating over a spacy document and lemmatizing the tokens compared to looking up the lemma of the word in the Vocab lemma_lookup table.</p>

<pre><code>nlp = spacy.load(""en_core_web_lg"")
doc = nlp(""I'm running faster"")
for tok in doc: 
  print(tok.lemma_)
</code></pre>

<p>This prints out ""faster"" as lemma for the token ""faster"" instead of ""fast"". However the token does exist in the lemma_lookup table.</p>

<pre><code>nlp.vocab.lookups.get_table(""lemma_lookup"")[""faster""]
</code></pre>

<p>which outputs ""fast""</p>

<p>Am I doing something wrong? Or is there another reason why these two are different? Maybe my definitions are not correct and I'm comparing apples with oranges?</p>

<p>I'm using the following versions on Ubuntu Linux:
spacy==2.2.4
spacy-lookups-data==0.1.0</p>
"
"61010075","Parsing CoNLL-U missing annotation (misc)","2020-04-03 10:43:06","0","270","0","1","","61010799","<p>I'm trying to parse .ConLL files from this <a href=""https://raw.githubusercontent.com/crscardellino/nll2rdf-active-learner/master/data/taggedcorpus.tar.bz2"" rel=""nofollow noreferrer"">github Repo</a>, an example of my parsing code:</p>

<pre><code>from io import open
from conllu import parse_tree_incr
import glob
import os

for filename in glob.glob('./licenses-conll-format/22-MIT/MIT_permissionCopy.conll'):
    data_file=open(filename, ""r"", encoding=""utf-8"")
    for tokentree in parse_incr(data_file):
        print(tokentree.serialize())
</code></pre>

<p>output :</p>

<pre><code>24  Permission  _   NN  NN  _   27  nsubjpass   _   _
25  is  _   VBZ VBZ _   27  auxpass _   _
26  hereby  _   RB  RB  _   27  advmod  _   _
27  granted _   VBN VBN _   11  rcmod   _   _
28  ,   _   ,   ,   _   27  punct   _   _
29  free    _   JJ  JJ  _   27  advmod  _   _
30  of  _   IN  IN  _   0   erased  _   _
31  charge  _   NN  NN  _   29  prep_of _   _
</code></pre>

<p>this seems to be missing some annotations (I-PERMISSION,B-PERMISSION etc ..) from the original .conll file :</p>

<pre><code>24  Permission  _   NN  NN  _   27  nsubjpass   _   _   B-PERMISSION    COPY
25  is  _   VBZ VBZ _   27  auxpass _   _   I-PERMISSION
26  hereby  _   RB  RB  _   27  advmod  _   _   I-PERMISSION
27  granted _   VBN VBN _   11  rcmod   _   _   I-PERMISSION
28  ,   _   ,   ,   _   27  punct   _   _   O
29  free    _   JJ  JJ  _   27  advmod  _   _   I-PERMISSION
30  of  _   IN  IN  _   0   erased  _   _   I-PERMISSION
31  charge  _   NN  NN  _   29  prep_of _   _   I-PERMISSION
32  ,   _   ,   ,   _   27  punct   _   _   O
</code></pre>

<p>Any thoughts on how to get all the annotations ?<br></p>
"
"60967134","Named Entity Recognition in aspect-opinion extraction using dependency rule matching","2020-04-01 08:59:23","7","1188","0","1","","61035804","<p>Using Spacy, I extract aspect-opinion pairs from a text, based on the grammar rules that I defined. Rules are based on POS tags and dependency tags, which is obtained by <code>token.pos_</code> and <code>token.dep_</code>. Below is an example of one of the grammar rules. If I pass the sentence <code>Japan is cool,</code> it returns <code>[('Japan', 'cool', 0.3182)]</code>, where the value represents the polarity of <code>cool</code>.</p>

<p>However I don't know how I can make it recognise the Named Entities. For example, if I pass <code>Air France is cool</code>, I want to get <code>[('Air France', 'cool', 0.3182)]</code> but what I currently get is <code>[('France', 'cool', 0.3182)]</code>. </p>

<p>I checked Spacy online documentation and I know how to extract NE(<code>doc.ents</code>). But I want to know what the possible workaround is to make my extractor work. Please note that I don't want a forced measure such as concatenating strings <code>AirFrance</code>, <code>Air_France</code> etc.</p>

<p>Thank you!</p>

<pre><code>import spacy

nlp = spacy.load(""en_core_web_lg-2.2.5"")
review_body = ""Air France is cool.""
doc=nlp(review_body)

rule3_pairs = []

for token in doc:

    children = token.children
    A = ""999999""
    M = ""999999""
    add_neg_pfx = False

    for child in children :
        if(child.dep_ == ""nsubj"" and not child.is_stop): # nsubj is nominal subject
            A = child.text

        if(child.dep_ == ""acomp"" and not child.is_stop): # acomp is adjectival complement
            M = child.text

        # example - 'this could have been better' -&gt; (this, not better)
        if(child.dep_ == ""aux"" and child.tag_ == ""MD""): # MD is modal auxiliary
            neg_prefix = ""not""
            add_neg_pfx = True

        if(child.dep_ == ""neg""): # neg is negation
            neg_prefix = child.text
            add_neg_pfx = True

    if (add_neg_pfx and M != ""999999""):
        M = neg_prefix + "" "" + M

    if(A != ""999999"" and M != ""999999""):
        rule3_pairs.append((A, M, sid.polarity_scores(M)['compound']))
</code></pre>

<p>Result</p>

<pre><code>rule3_pairs
&gt;&gt;&gt; [('France', 'cool', 0.3182)]
</code></pre>

<p>Desired output</p>

<pre><code>rule3_pairs
&gt;&gt;&gt; [('Air France', 'cool', 0.3182)]
</code></pre>
"
"60928526","How to apply nltk.pos_tag on pyspark dataframe","2020-03-30 10:27:11","1","1030","0","1","","60928633","<p>I'm trying to apply pos tagging on one of my tokenized column called ""removed"" in pyspark dataframe.</p>

<p>I'm trying with</p>

<pre><code>nltk.pos_tag(df_removed.select(""removed""))
</code></pre>

<p>But all I get is Value Error: <code>ValueError: Cannot apply 'in' operator against a column: please use 'contains' in a string column or 'array_contains' function for an array column.</code></p>

<p>How can I make it? </p>
"
"60922171","Spacy - entity linker - why is the predict score a combination of prob and cosine sim?","2020-03-29 23:44:50","3","420","0","1","","60922553","<p>I was going through the predict method for the entity linker pipe under spacy, and for some reason the score is defined as the following :</p>

<p><code>scores = prior_probs + sims - (prior_probs*sims)</code></p>

<p>Link <a href=""https://github.com/explosion/spaCy/blob/master/spacy/pipeline/pipes.pyx#L1365"" rel=""nofollow noreferrer"">here</a></p>

<p>Anybody has experience with this / knows where this formula comes from?</p>

<p>Thanks!</p>
"
"60871375","How to do lemmatization using NLTK or pywsd","2020-03-26 16:05:45","1","764","0","2","","60872541","<p>I know that my explaination is rather long but I found it necessary. Hopefully someone is patient and a helpful soul :)
I'm doing a sentiment analysis project atm and I'm stuck i the pre-process part. I did the import of the csv file, made it into a dataframe, transformed the variables/columns into the right data types. Then I did the tokenization like this, where i choose the variable I wanted to tokenize (tweet content) in the dataframe (df_tweet1):</p>

<pre><code># Tokenization
tknzr = TweetTokenizer()
tokenized_sents = [tknzr.tokenize(str(i)) for i in df_tweet1['Tweet Content']]
for i in tokenized_sents:
    print(i)
</code></pre>

<p>The output is a list of list with words (tokens).</p>

<p>Then I perform stop word removal:</p>

<pre><code># Stop word removal
from nltk.corpus import stopwords

stop_words = set(stopwords.words(""english""))
#add words that aren't in the NLTK stopwords list
new_stopwords = ['!', ',', ':', '&amp;', '%', '.', '’']
new_stopwords_list = stop_words.union(new_stopwords)

clean_sents = []
for m in tokenized_sents:
    stop_m = [i for i in m if str(i).lower() not in new_stopwords_list]
    clean_sents.append(stop_m)
</code></pre>

<p>The output is the same but without stop words</p>

<p>The next two steps are confusing to me (part-of-speech tagging and lemmatization). I tried two things:</p>

<p>1) Convert the previous output into a list of strings </p>

<pre><code>new_test = [' '.join(x) for x in clean_sents]
</code></pre>

<p>since I thought that would enable me to use this code to do both steps in one:</p>

<pre><code>from pywsd.utils import lemmatize_sentence

text = new_test
lemm_text = lemmatize_sentence(text, keepWordPOS=True)
</code></pre>

<p>I got the this error: 
TypeError: expected string or bytes-like object</p>

<p>2) Perform POS and lemmatizaion seperately. First POS using clean_sents as input:</p>

<pre><code># PART-OF-SPEECH        
def process_content(clean_sents):
    try:
        tagged_list = []  
        for lst in clean_sents[:500]: 
            for item in lst:
                words = nltk.word_tokenize(item)
                tagged = nltk.pos_tag(words)
                tagged_list.append(tagged)
        return tagged_list

    except Exception as e:
        print(str(e))

output_POS_clean_sents = process_content(clean_sents)
</code></pre>

<p>The output is a list of lists with words with a tag attached
Then I want to lemmatize this output, but how? I tried two modules, but both gave me error:</p>

<pre><code>from pywsd.utils import lemmatize_sentence

lemmatized= [[lemmatize_sentence(output_POS_clean_sents) for word in s]
              for s in output_POS_clean_sents]

# AND

from nltk.stem.wordnet import WordNetLemmatizer

lmtzr = WordNetLemmatizer()
lemmatized = [[lmtzr.lemmatize(word) for word in s]
              for s in output_POS_clean_sents]
print(lemmatized)
</code></pre>

<p>The errors were respectively:</p>

<p>TypeError: expected string or bytes-like object</p>

<p>AttributeError: 'tuple' object has no attribute 'endswith'</p>
"
"60867353","Using LIME for BERT transformer visualization results in memory error","2020-03-26 12:32:02","2","4006","0","1","","60989317","<p><strong>Situation</strong>:  I am currently working on visualizing the results of a huggingface transformers machine learning model I have been building using the <a href=""https://github.com/marcotcr/lime"" rel=""nofollow noreferrer"">LIME package</a> following <a href=""https://marcotcr.github.io/lime/tutorials/Lime%20-%20basic%20usage%2C%20two%20class%20case.html"" rel=""nofollow noreferrer"">this tutorial</a>. </p>

<p><strong>Complication</strong>: My code is set up and runs well until I create the LIME <em>explainer</em> object. At this point I get a memory error.</p>

<p><strong>Question</strong>: What am I doing wrong? Why am I running into a memory error?</p>

<p><strong>Code</strong>: Here is my code (you should be able to just copy-paste this into google colab and run the whole thing)</p>

<pre class=""lang-py prettyprint-override""><code>########################## LOAD PACKAGES ######################
# Install new packages in our environment
!pip install lime
!pip install wget
!pip install transformers

# Import general libraries
import sklearn
import sklearn.ensemble
import sklearn.metrics
import numpy as np
import pandas as pd

# Import libraries specific to this notebook
import lime
import wget
import os
from __future__ import print_function
from transformers import FeatureExtractionPipeline, BertModel, BertTokenizer, BertConfig
from lime.lime_text import LimeTextExplainer

# Let the notebook know to plot inline
%matplotlib inline

########################## LOAD DATA ##########################
# Get URL
url = 'https://nyu-mll.github.io/CoLA/cola_public_1.1.zip'

# Download the file (if we haven't already)
if not os.path.exists('./cola_public_1.1.zip'):
    wget.download(url, './cola_public_1.1.zip')

# Unzip the dataset (if we haven't already)
if not os.path.exists('./cola_public/'):
    !unzip cola_public_1.1.zip

# Load the dataset into a pandas dataframe.
df_cola = pd.read_csv(""./cola_public/raw/in_domain_train.tsv"", delimiter='\t', 
                      header=None, names=['sentence_source', 'label', 
                                          'label_notes', 'sentence'])

# Only look at the first 50 observations for debugging
df_cola = df_cola.head(50)

###################### TRAIN TEST SPLIT ######################
# Apply the train test split
x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(
    df_cola.sentence, df_cola.label, test_size=0.2, random_state=42
)

###################### CREATE LIME CLASSIFIER ######################
# Create a function to extract vectors from a single sentence
def vector_extractor(sentence):

    # Create a basic BERT model, config and tokenizer for the pipeline
    configuration = BertConfig()
    configuration.max_len = 64
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',
                                              do_lower_case=True, 
                                              max_length=64,
                                              pad_to_max_length=True)
    model = BertModel.from_pretrained('bert-base-uncased',config=configuration)

    # Create the pipeline
    vector_extractor = FeatureExtractionPipeline(model=model,
                                                 tokenizer=tokenizer,
                                                 device=0)

    # The pipeline gives us all tokens in the final layer - we want the CLS token
    vector = vector_extractor(sentence)
    vector = vector[0][0]

    # Return the vector
    return vector

# Adjust the format of our sentences (from pandas series to python list)
x_train = x_train.values.tolist()
x_test = x_test.values.tolist()

# First we vectorize our train features for the classifier
x_train_vectorized = [vector_extractor(x) for x in x_train]

# Create and fit the random forest classifier
rf = sklearn.ensemble.RandomForestClassifier(n_estimators=100)
rf.fit(x_train_vectorized, y_train)

# Define the lime_classifier function
def lime_classifier(sentences): 

    # Turn all the sentences into vectors
    vectors = [vector_extractor(x) for x in sentences]

    # Get predictions for all 
    predictions = rf.predict_proba(vectors)

    # Return the probabilies as a 2D-array
    return predictions  

########################### APPLY LIME ##########################
# Create the general explainer object
explainer = LimeTextExplainer()

# ""Fit"" the explainer object to a specific observation
exp = explainer.explain_instance(x_test[1], 
                                 lime_classifier, 
                                 num_features=6)
</code></pre>
"
"60842476","ValueError: Error when checking target: expected dense_22 to have shape (100, 50) but got array with shape (1, 50)","2020-03-25 03:10:08","1","50","2","1","","60843545","<p>I'm training a neural network to predict the Document-Frequency from a set of documents.</p>

<p>So, the main idea is to map a matrix with 100 documents and 50 tokens to the respective document-frequency array.</p>

<p>X = (n_samples, 100, 50) -> y = (n_samples, 1, 50)</p>

<p>My code is:</p>

<pre><code>model = Sequential()
model.add(Dense(50, activation='sigmoid', input_shape=(100,50)))
model.add(Dense(50))
model.compile(optimizer='rmsprop', loss='mse')
model.fit(X_train, y_train, epochs=50)
</code></pre>

<p>But i got an error:</p>

<pre><code>ValueError: Error when checking target: expected dense_22 to have shape (100, 50) but got array with shape (1, 50)
</code></pre>
"
"60832547","Where is perplexity calculated in the Huggingface gpt2 language model code?","2020-03-24 13:58:05","6","6863","2","2","","60834242","<p>I see some github comments saying the output of the model() call's loss is in the form of perplexity:
<a href=""https://github.com/huggingface/transformers/issues/473"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/issues/473</a></p>

<p>But when I look at the relevant code...
<a href=""https://huggingface.co/transformers/_modules/transformers/modeling_openai.html#OpenAIGPTLMHeadModel.forward"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/_modules/transformers/modeling_openai.html#OpenAIGPTLMHeadModel.forward</a></p>

<pre><code>    if labels is not None:
        # Shift so that tokens &lt; n predict n
        shift_logits = lm_logits[..., :-1, :].contiguous()
        shift_labels = labels[..., 1:].contiguous()
        # Flatten the tokens
        loss_fct = CrossEntropyLoss()
        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
        outputs = (loss,) + outputs

    return outputs  # (loss), lm_logits, (all hidden states), (all attentions)
</code></pre>

<p>I see cross entropy being calculated, but no transformation into perplexity. Where does the loss finally get transformed? Or is there a transformation already there that I'm not understanding?</p>
"
"60832385","nltk extract nounphrase with RegexpParser","2020-03-24 13:49:52","0","530","0","1","","60841866","<p>I want to extract nounphrases from text and i use python with NLTK.
There is a pattern I found in internet of using RegexpParser as follows:</p>

<pre><code>grammar = r""""""
        NBAR:
            {&lt;NN.*|JJ&gt;*&lt;NN.*&gt;}  # Nouns and Adjectives, terminated with Nouns
        NP:
            {&lt;NBAR&gt;}
            {&lt;NBAR&gt;&lt;IN&gt;&lt;NBAR&gt;}  # Above, connected with in/of/etc...
    """"""
    cp = nltk.RegexpParser(grammar)
</code></pre>

<p>I want to modify the grammar variable to add the case 'Noun of Noun' or 'Noun in Noun' (""cup of coffee"" or ""water in cup"" for example)
My test string is : 'postal code is new method of delivery'
I want to receive list of phrases : ['portal code', 'new method','new method of delivery']</p>
"
"60809394","Can spaCy link only named entities?","2020-03-23 07:27:51","0","1107","0","1","","60809901","<p>Here's excerpt from a (supposedly) funny review of a restaurant:</p>

<blockquote>
  <p>I'd like to personally shake Mr <strong>Tofu</strong>'s hand.  While I cannot <strong>medically</strong> prove it, I am 100% certain that their soondubu contains undefined <strong>healing</strong> properties.  Some how some way, I always feel better after a meal here.  Got a <strong>cold</strong>?  Screw the <strong>Nyquil</strong> and get the spicy <strong>kimchi</strong> soondubu.</p>
</blockquote>

<p>I'd would like to extract important entities and link them to Wikipedia entities. I've trained spaCy on a small sample of Wikipedia/WikiData and run entity linking on the review:</p>

<pre><code>[('Tofu', 'PERSON', 'Q177378'), 
('Nyquil', 'WORK_OF_ART', 'NIL')]
</code></pre>

<p>I'd like other entities to be extracted and linked as well, e.g.:</p>

<pre><code>kimchi -&gt; Kimchi
cold -&gt; Common cold
healing -&gt; medicine 
medically -&gt; medicine
</code></pre>

<p>It looks like spaCy can link only named entities. I've tried to explicitly list other entities as named (which obviously does not scale well):</p>

<pre><code>ruler = EntityRuler(nlp)
patterns = [{""label"": ""ORG"", ""pattern"": ""kimchi""}, {""label"": ""ORG"", ""pattern"": ""cold""}]
ruler.add_patterns(patterns)
nlp.add_pipe(ruler)
</code></pre>

<p>However, spaCy does not seem to link new entities at all:</p>

<pre><code>[ ('Tofu', 'PERSON', 'Q177378'),
  ('cold', 'ORG', ''),
  ('Nyquil', 'WORK_OF_ART', 'NIL'),
  ('kimchi', 'ORG', '')]
</code></pre>

<ol>
<li>How can I make Spacy recognize also other entities?</li>
<li>Should this be done before training entity linking model or can be done with already trained model?</li>
<li>Is spaCy the right tool for my task at all?</li>
</ol>
"
"60751304","Terms for basic NLP/logical parsing example","2020-03-19 05:17:48","1","89","0","1","","60765752","<p>Given the following clause:</p>

<pre><code>Female or not White
</code></pre>

<p>Is the following tree the correct representation of this?</p>

<pre><code>      OR
    /   \
female  NOT white
</code></pre>

<p>That is, would ""not white"" be one unit, or is it considered two?</p>

<p>Additionally, what are the following four elements usually called in parsing:</p>

<pre><code>OR     -- (logical?)
female -- (variable name?)
NOT    -- (inversion? or is this also logical?)
TRUE   -- (for example, whether the value of female is true or not -- variable value?)
</code></pre>
"
"60680245","Wrong probability calculation in context-free grammar (NLTK, Python 3)","2020-03-14 06:12:48","1","249","0","1","","60681122","<p>I have a problem with showing the most likely constituency structure of some sentence using NLTK's probabilistic grammar. </p>

<p>Here is my sentence ""Ich sah den Tiger under der Felse""</p>

<p>Here is my code:</p>

<pre><code>from nltk import PCFG
tiger_grammar = PCFG.fromstring(""""""
S -&gt; NP VP [1.0]
NP -&gt; ART NN [0.25] | PPER [0.5] | NP PP [0.25]
VP -&gt; VVFIN NP [0.75] | VVFIN NP PP [0.25]
PP -&gt; APPR NP [1.0]
APPR -&gt; 'unter' [1.0]
PPER -&gt; 'Ich' [1.0]
VVFIN -&gt; 'sah' [1.0]
NN -&gt; 'Tiger' [0.5] | 'Felse' [0.5]
ART -&gt; 'den' [0.5] | 'der' [0.5]
"""""")
viterbi_parser = nltk.ViterbiParser(tiger_grammar)
trees = viterbi_parser.parse(['Ich', 'sah', 'den', 'Tiger', 'unter', 'der', 'Felse'])
for t in trees:
    print(t)
</code></pre>

<p>Here is what I get:</p>

<pre><code>(S
  (NP (PPER Ich))
  (VP
    (VVFIN sah)
    (NP (ART den) (NN Tiger))
    (PP (APPR unter) (NP (ART der) (NN Felse))))) (p=0.000488281)
</code></pre>

<p>But the desired result is:</p>

<pre><code>(S
  (NP (PPER Ich))
  (VP
    (VVFIN sah)
    (NP
      (NP (ART den) (NN Tiger))
      (PP (APPR unter) (NP (ART der) (NN Felse))))))
</code></pre>

<p>(I didn't add the probability here, but it should be displayed as well)</p>

<p>According to the grammar, the probability to form <code>VP</code> from <code>VVFIN</code> and <code>NP</code> is higher than from <code>VVFIN</code>, <code>NP</code> and <code>PP</code>. But the parser shows the second structure.</p>

<p>What am I doing wrong?</p>

<p>Would be grateful for suggestions!</p>
"
"60638828","SpaCy use Lemmatizer as stand-alone component","2020-03-11 14:53:44","0","451","0","1","","60642923","<p>I want to use SpaCy's lemmatizer as a standalone component (because I have pre-tokenized text, and I don't want to re-concatenate it and run the full pipeline because SpaCy will most likely tokenize differently in some cases). </p>

<p>I found the lemmatizer in the package but I somehow needs to load the dictionaries with the rules to initialize this Lemmatizer.
These files must be somewhere in the model of the English or German model, right? I couldn't find them there.</p>

<pre><code>from spacy.lemmatizer import Lemmatizer
where do the LEMMA_INDEX, etc. files are comming from?
lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)
</code></pre>

<p>I found a similar question here: <a href=""https://stackoverflow.com/questions/55864933/spacy-lemmatizer-issue-consistency"">Spacy lemmatizer issue/consistency</a>
but this one did not entirely answer how to get these dictionary files from the model. The spacy.lang.* parameter seems to no longer exist in newer versions.</p>
"
"60534999","How to solve Spanish lemmatization problems with SpaCy?","2020-03-04 21:30:46","10","12460","11","4","","60553026","<p>When trying <strong>lemmatize in Spanish</strong> a csv with more than 60,000 words, <strong>SpaCy</strong> does not correctly write certain words, I understand that the model is not 100% accurate. However, I have not found any other solution, since <strong>NLTK</strong> does not bring a Spanish core.</p>

<p>A friend tried to ask this question in Spanish Stackoverflow, however, the community is quite small compared with this community, and we got no answers about it.</p>

<p><strong>code:</strong></p>

<pre class=""lang-py prettyprint-override""><code>nlp = spacy.load('es_core_news_sm')

def lemmatizer(text):  
  doc = nlp(text)
  return ' '.join([word.lemma_ for word in doc])

df['column'] = df['column'].apply(lambda x: lemmatizer(x))
</code></pre>

<p>I tried to lemmatize certain words that I found wrong to prove that SpaCy is not doing it correctly:</p>

<pre class=""lang-py prettyprint-override""><code>text = 'personas, ideas, cosas' 
# translation: persons, ideas, things

print(lemmatizer(text))
</code></pre>

<pre><code># Current output:
personar , ideo , coser 
# translation:
personify, ideo, sew

# The expected output should be:
persona, idea, cosa

# translation: 
person, idea, thing
</code></pre>
"
"60533029","What is a good way to speed up test runs utilizing larger spacy models?","2020-03-04 19:03:11","1","187","1","1","","60541041","<p>I have constructed some tests relying on the <code>en_core_web_md</code> model. The model takes ~15 sec to load into memory on my computer making the tests a pain to run.</p>

<p>Is there a smart way to speed it up?</p>
"
"60481221","Spacy: OSError: [E050] Can't find model on Google Colab | Python","2020-03-02 00:51:16","1","4186","0","2","","60481357","<p>I'm trying to 'lemmatize' spanish text using the spanish core model <code>es_core_news_sm</code>. However, I'm getting <em>OSError.</em></p>

<p>The following code is an example of <em>lemmatization</em> using <strong>SpaCy</strong> on <em>Google Colabs</em>:</p>

<pre class=""lang-py prettyprint-override""><code>import spacy
spacy.prefer_gpu()

nlp = spacy.load('es_core_news_sm')
text = 'yo canto, tú cantas, ella canta, nosotros cantamos, cantáis, cantan…'
doc = nlp(text)
lemmas = [tok.lemma_.lower() for tok in doc]
</code></pre>

<p>Also I tried to <strong>import the core</strong>, but didn't work in this way, getting a similar traceback.</p>

<pre class=""lang-py prettyprint-override""><code>import es_core_news_sm
nlp = es_core_news_sm.load()
</code></pre>

<p><strong>Traceback:</strong></p>

<pre><code>---------------------------------------------------------------------------
OSError                                   Traceback (most recent call last)
&lt;ipython-input-93-fd65d69a4f87&gt; in &lt;module&gt;()
      2 spacy.prefer_gpu()
      3 
----&gt; 4 nlp = spacy.load('es_core_web_sm')
      5 text = 'yo canto, tú cantas, ella canta, nosotros cantamos, cantáis, cantan…'
      6 doc = nlp(text)

1 frames
/usr/local/lib/python3.6/dist-packages/spacy/util.py in load_model(name, **overrides)
    137     elif hasattr(name, ""exists""):  # Path or Path-like to model data
    138         return load_model_from_path(name, **overrides)
--&gt; 139     raise IOError(Errors.E050.format(name=name))
    140 
    141 

OSError: [E050] Can't find model 'es_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.
</code></pre>
"
"60387288","Porter and Lancaster stemming clarification","2020-02-25 03:55:59","2","1151","2","1","","60387996","<p>I am doing <code>stemming</code> using <code>Porter</code> and <code>Lancaster</code> and I find these observations:</p>

<pre><code>Input: replied
Porter: repli
Lancaster: reply


Input:  twice
porter:  twice
lancaster:  twic

Input:  came
porter:  came
lancaster:  cam

Input:  In
porter:  In
lancaster:  in
</code></pre>

<p>My question are:</p>

<ul>
<li><code>Lancaster</code> was supposed to be ""aggressive"" <code>stemmer</code> but it worked properly with <code>replied</code>. Why?</li>
<li>The word <code>In</code> remained the same in <code>Porter</code> with uppercase <code>In</code>, Why?</li>
<li>Notice that the <code>Lancaster</code> is removing words ending with <code>e</code>, Why?</li>
</ul>

<p>I am not able to understand these concepts. Could you please help?</p>
"
"60372143","How to replace word in array of text in Python?","2020-02-24 08:42:07","1","1231","2","3","","60372277","<p>I want to stem my text with my own array:</p>

<pre class=""lang-py prettyprint-override""><code>word_list1 = [""cccc"", ""bbbb"", ""aaa""]

def stem_text(text):
     text = text.split()
     array = np.array(text)
     temp = np.where(array == word_list1, word_list1[0], array)
     text = ' '.join(temp)
     return text
</code></pre>

<p>I want to do like this:</p>

<p>for all of the word in <code>word_list1</code>, check the text and if some word matched, replace it with <code>word_list[0]</code></p>
"
"60363904","How to use the universal POS tags with nltk.pos_tag() function?","2020-02-23 15:41:40","1","4109","0","1","","60379671","<p>I have a text and I want to find number of 'ADJs','PRONs', 'VERBs', 'NOUNs' etc.
I know that there is <code>.pos_tag()</code> function but it gives me different results , and I want to have results as 'ADJ','PRON', 'VERB', 'NOUN'.
This is my code:</p>

<pre><code>import nltk
from nltk.corpus import state_union, brown
from nltk.corpus import stopwords
from nltk import ne_chunk

from nltk.tokenize import PunktSentenceTokenizer
from nltk.tokenize import word_tokenize
from nltk.tokenize import RegexpTokenizer
from nltk.stem import WordNetLemmatizer 

from collections import Counter

sentence = ""this is my sample text that I want to analyze with programming language""

# tokenizing text (make list with evey word)
sample_tokenization = word_tokenize(sample)
print(""THIS IS TOKENIZED SAMPLE TEXT, LIST OF WORDS:\n\n"", sample_tokenization)
print()

# tagging words
taged_words = nltk.pos_tag(sample_tokenization.split(' '))
print(taged_words)
print()


# showing the count of every type of word for new text
count_of_word_type = Counter(word_type for word,word_type in taged_words)
count_of_word_type_list = count_of_word_type.most_common() # making a list of tuples counts
print(count_of_word_type_list)


for w_type, num in count_of_word_type_list:
     print(w_type, num)
print() 
</code></pre>

<p>The code above works but I want to find a way to get this type of tags:</p>

<pre><code>Tag Meaning English Examples
ADJ adjective   new, good, high, special, big, local
ADP adposition  on, of, at, with, by, into, under
ADV adverb  really, already, still, early, now
CONJ    conjunction and, or, but, if, while, although
DET determiner, article the, a, some, most, every, no, which
NOUN    noun    year, home, costs, time, Africa
NUM numeral twenty-four, fourth, 1991, 14:24
PRT particle    at, on, out, over per, that, up, with
PRON    pronoun he, their, her, its, my, I, us
VERB    verb    is, say, told, given, playing, would
.   punctuation marks   . , ; !
X   other   ersatz, esprit, dunno, gr8, univeristy
</code></pre>

<p>I saw that there is a chapter here: <a href=""https://www.nltk.org/book/ch05.html"" rel=""nofollow noreferrer"">https://www.nltk.org/book/ch05.html</a></p>

<p>That says:</p>

<pre><code>from nltk.corpus import brown
brown_news_tagged = brown.tagged_words(categories='news', tagset='universal')
</code></pre>

<p>But I do not know how to apply that on my sample sentence.
Thanks for your help.</p>
"
"60306461","How to convert plural nouns to singular using SpaCy?","2020-02-19 17:54:09","5","8350","4","2","","60345957","<p>I am using SpaCy to lemmatize text, but in some special cases I need to keep original text and <strong>just</strong> convert plural nouns to their singular forms. 
Is there a way to tell SpaCy to <strong>only</strong> convert plural nouns to singulars without lemmatizing the whole text (like removing ed, ing...etc) ? Or should I explicitly test each token to check if it is a plural noun to take its lemma?</p>

<p>P.S. Input text is dynamic, so I don't know in advance if the word is a noun or not</p>

<p>Thanks</p>
"
"60293351","how tfidf value is used in k-means clustering","2020-02-19 05:01:15","2","6492","0","1","","60296409","<p>I am using K-means clustering with TF-IDF using sckit-learn library. I understand that K-means uses distance to create clusters and the distance is represented in (x axis value, y axis value) but the tf-idf is a single numerical value. My question is how is this tf-idf value converted into (x,y) value by K-means clustering.</p>
"
"60291151","Why do I get TypeError: unhashable type when using NLTK lemmatizer on sentence?","2020-02-19 00:01:14","0","1051","9","1","","60294069","<p>I'm currently working on lemmantizing a sentence while also applying pos_tags. This is what I have so far</p>

<pre><code>import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import wordnet
from nltk.stem import WordNetLemmatizer
from nltk import pos_tag

lem = WordNetLemmatizer()

def findTag(sentence):
    sentence = word_tokenize(sentence)
    sentence = [i.strip("" "") for i in sentence]
    pos_label = nltk.pos_tag(sentence)[0][1][0].lower()

    if pos_label == ""j"":
        pos_label == ""a""

    if pos_label in [""a"", ""n"", ""v""]:
        print(lem.lemmatize(word, pos = pos_label))
    elif pos_label in ['r']: 
        print(wordnet.synset(sentence+"".r.1"").lemmas()[0].pertainyms()[0].name())
    else:
        print(lem.lemmatize(sentence))


findTag(""I love running angrily"")
</code></pre>

<p>However, when I input a sentence with this I get the error</p>

<pre><code>Traceback (most recent call last):
  File ""spoilerDetect.py"", line 25, in &lt;module&gt;
    findTag(""I love running angrily"")
  File ""spoilerDetect.py"", line 22, in findTag
    print(lem.lemmatize(sentence))
  File ""/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/nltk/stem/wordnet.py"", line 41, in lemmatize
    lemmas = wordnet._morphy(word, pos)
  File ""/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/nltk/corpus/reader/wordnet.py"", line 1905, in _morphy
    if form in exceptions:
TypeError: unhashable type: 'list'
</code></pre>

<p>I understand that lists are unhashable but am unsure of how to fix this. Do I change lists to a tuple or is there something I'm not understanding?</p>
"
"60184117","How to check the root cause of CUDA out of memory issue in the middle of training?","2020-02-12 08:32:48","1","1562","2","2","","60340477","<p>I'm running roberta on huggingface <code>language_modeling.py</code>. After doing 400 steps I suddenly get a CUDA out of memory issue. Don't know how to deal with it. Can you please help? Thanks</p>
"
"60166480","Counter to return null-value if Part of Speech tag not present","2020-02-11 10:12:21","0","150","2","1","","60167755","<p>Currently i am trying to count the instances a certain part of speech occurs in a given online review. While i am able to retrieve the specific tags corresponding to each word, and count these instances, i face difficulties in also capturing the null-values (if the tag is not present = 0). Ideally, i would have a list of all the tags with either the count of actual occurrences in the review, or if it is not present = 0.   I use NLTK's POS tagger. </p>

<p>The following code will get me the specific tags per review, but thus only specifically to the tokens in the review:</p>

<pre><code>for line in lines:
tokens = nltk.word_tokenize(line)
tagged = nltk.pos_tag(tokens)
counts=Counter(tag for word,tag in tagged)
postag.append(counts)
</code></pre>

<p>I tried to make a separate list with some specific tags (goal was to achieve all the verbs and nouns) but it still only returns only those with actual values (1 or more) and not those with 0 (not present in text). I could potentially insert all the available tags in there however it would thus only return actual values. For instance: </p>

<pre><code>for line in lines:
tokens = nltk.word_tokenize(line)
tagged = nltk.pos_tag(tokens)
selective_tagged = ['NN','NNS','NNP','NNPS','VB','VBD','VBN','VBP','VBZ']
selective_tagged_words =[]
for word,tag in tagged:
    if tag in selective_tagged:
        selective_tagged_words.append((word,tag))
counts=Counter(tag for word,tag in selective_tagged_words)
postag.append(counts) 
</code></pre>

<p>So in the above example output would be:</p>

<pre><code>Counter({'NNS': 3, 'VBP': 3, 'VBN': 1, 'NN': 5, 'VBZ': 1, 'VB': 4, 'NNP': 1})
</code></pre>

<p>But i want </p>

<pre><code>Counter({'NNS': 3, 'VBP': 3, 'VBN': 1, 'NN': 5, 'VBZ': 1, 'VB': 4, 'NNP': 1, 'NNPS': 0, 'VBD': 0})
</code></pre>

<p>Thanks for the help!</p>

<p><strong>Edit 2:</strong>
Code that worked in the end (thanks to manoj yadav):</p>

<pre><code>for line in lines:
tokens = nltk.word_tokenize(line)
tagged = nltk.pos_tag(tokens)
selective_tagged = ['NN','NNS','NNP','NNPS','VB','VBD','VBN','VBP','VBZ']
selective_tagged_words =[]
for word,tag in tagged:
    if tag in selective_tagged:
        selective_tagged_words.append((word,tag))
counts=Counter(tag for word,tag in selective_tagged_words)
other_tags = set(selective_tagged)-set(counts)
for i in other_tags:
    counts[i]=0
postag.append(counts)
</code></pre>
"
"60151850","Correct POS tags for numbers substituted with ## in spacy","2020-02-10 13:49:59","6","1154","1","1","","60190878","<p>The gigaword dataset is a huge corpus used to train abstractive summarization models. It contains summaries like these:</p>

<pre><code>spain 's colonial posts #.## billion euro loss
taiwan shares close down #.## percent
</code></pre>

<p>I want to process these summaries with <em>spacy</em> and get the correct pos tag for each token. The issue is that all numbers in the dataset were replaced with # signs which spacy does not classify as numbers (<em>NUM</em>) but as other tags.</p>

<pre><code>&gt;&gt;&gt; import spacy
&gt;&gt;&gt; from spacy.tokens import Doc
&gt;&gt;&gt; nlp = spacy.load(""en_core_web_sm"")
&gt;&gt;&gt; nlp.tokenizer = lambda raw: Doc(nlp.vocab, words=raw.split(' '))
&gt;&gt;&gt; text = ""spain 's colonial posts #.## billion euro loss""
&gt;&gt;&gt; doc = nlp(text)
&gt;&gt;&gt; [(token.text, token.pos_) for token in doc]
[('spain', 'PROPN'), (""'s"", 'PART'), ('colonial', 'ADJ'), ('posts', 'NOUN'), ('#.##', 'PROPN'), ('billion', 'NUM'), ('euro', 'PROPN'), ('loss', 'NOUN')]
</code></pre>

<p>Is there a way to customize the POS tagger so that it classifies all tokens that only consist of #-sign and dots as numbers?</p>

<p>I know you replace the spacy POS tagger with your own or fine-tune it for your domain with additional data but I don't have tagged training data where all numbers are replaced with # and I would like to change the tagger as little as possible. I would prefer having a regular expression or fixed list of tokens that are always recognized as numbers.</p>
"
"59911279","How to calculate tf-idf for a single term after getting the tf-idf matrix?","2020-01-25 16:40:08","1","497","0","1","","59912361","<p>In the past, I have received help with building a tf-idf for the one of my document and got an output which I wanted (please see below).</p>
<pre class=""lang-r prettyprint-override""><code>TagSet &lt;- data.frame(emoticon = c(&quot;🤔&quot;,&quot;🍺&quot;,&quot;💪&quot;,&quot;🥓&quot;,&quot;😃&quot;),
                     stringsAsFactors = FALSE)

TextSet &lt;- data.frame(tweet = c(&quot;🤔Sharp, adversarial⚔️~pro choice💪~ban Pit Bulls☠️~BSL🕊️~aberant psychology😈~common sense🤔~the Piper will lead us to reason🎵~sealskin woman🐺&quot;,
                                &quot;Blocked by Owen, Adonis. Abbott &amp; many #FBPE😃 Love seaside, historic houses &amp; gardens, family &amp; pets. RTs &amp; likes/ Follows may=interest not agreement 🇬🇧&quot;,
                                &quot;🇺🇸🇺🇸🇺🇸🇺🇸 #healthy #vegetarian #beatchronicillness fix infrastructure&quot;,
                                &quot;LIBERTY-IDENTITARIAN. My bio, photo at Site Info. And kindly add my site to your Daily Favorites bar. Thank you, Eric&quot;,
                                &quot;💙🖤I #BackTheBlue for my son!🖤💙 Facts Over Feelings. Border Security saves lives! #ThankYouICE&quot;,
                                &quot;🤔🇺🇸🇺🇸 I play Pedal Steel @CooderGraw &amp; #CharlieShafter🇺🇸🇺🇸 #GoStars #LiberalismIsAMentalDisorder&quot;,
                                &quot;#Englishman  #Londoner  @Chelseafc  🕵️‍♂️ 🥓🚁 🍺 🏴󠁧󠁢󠁥󠁮󠁧󠁿🇬🇧🇨🇿&quot;,
                                &quot;F*** the Anti-White Agenda #Christian #Traditional #TradThot #TradGirl #European #MAGA #AltRight #Folk #Family #WhitePride&quot;,
                                &quot;🌸🐦❄️Do not dwell in tbaconhe past, do not dream of the future, concentrate the mind on the present moment.🌸🐿️❄️&quot;,
                                &quot;Ordinary girl in a messed up World | Christian | Anti-War | Anti-Zionist | Pro-Life | Pro 🇸🇪 | 👋🏼Hello intro on the Minds Link |&quot;),
                      stringsAsFactors = FALSE)


library(dplyr)
library(quanteda)

tweets_dfm &lt;- dfm(TextSet$tweet)  # convert to document-feature matrix

tweets_dfm %&gt;% 
  dfm_select(TagSet$emoticon) %&gt;% # only leave emoticons in the dfm
  dfm_tfidf() %&gt;%                 # weight with tfidf
  convert(&quot;data.frame&quot;)           # turn into data.frame to display more easily

#     document       🤔             🍺           💪          🥓           😃
# 1     text1      1.39794            1            0            0            0
# 2     text2      0.00000            0            1            0            0
# 3     text3      0.00000            0            0            0            0
# 4     text4      0.00000            0            0            0            0
# 5     text5      0.00000            0            0            0            0
# 6     text6      0.69897            0            0            0            0
# 7     text7      0.00000            0            0            1            1
# 8     text8      0.00000            0            0            0            0
# 9     text9      0.00000            0            0            0            0
# 10   text10      0.00000            0            0            0            0

</code></pre>
<p>But I need a little help with calculating tf-idf per singular term. Meaning, how do I accurately get the tf-idf value for each term from the matrix?</p>
<pre class=""lang-r prettyprint-override""><code># terms      tfidf
# 🤔      #its tfidf the correct way   
# 🍺      #its tfidf the correct way 
# 💪      #its tfidf the correct way 
# 🥓      #its tfidf the correct way 
# 😃      #its tfidf the correct way 
</code></pre>
<p>I am sure, it's not like add all of tf-idf for a term from its matrix column and divide by documents where it appeared. And that would be the value for that term.</p>
<p>I have looked at a few sources such as here, <a href=""https://stats.stackexchange.com/questions/422750/how-to-calculate-tf-idf-for-a-single-term"">https://stats.stackexchange.com/questions/422750/how-to-calculate-tf-idf-for-a-single-term</a>, but the author is asking something else entirely from what I read.</p>
<p>I am currently weak in text-mining/analysis terminology.</p>
"
"59649783","Should the BLEU score for subword NMT be calculated on the subwords or should they be joined first?","2020-01-08 16:10:40","0","549","0","1","","59660764","<p>This wasn't too clear in the papers I've read. When a model is trained on a bilingual corpus that was split into subwords e.g. via Byte-Pair Encoding, is it standard to calculate the BLEU score on the subword outputs or on the full words after rejoining the subwords?</p>
"
"59567357","Lemmatize tokenised column in pandas","2020-01-02 17:15:48","1","4891","0","1","","59567858","<p>I'm trying to lemmatize tokenized column comments_tokenized </p>

<p><a href=""https://i.sstatic.net/wYRoD.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/wYRoD.png"" alt=""enter image description here""></a></p>

<p>I do:</p>

<pre><code>import nltk
from nltk.stem import WordNetLemmatizer 

# Init the Wordnet Lemmatizer
lemmatizer = WordNetLemmatizer()

def lemmatize_text(text):
    return [lemmatizer.lemmatize(w) for w in df1[""comments_tokenized""]]

df1['comments_lemmatized'] = df1[""comments_tokenized""].apply(lemmatize_text)

</code></pre>

<p>but have</p>

<pre><code>TypeError: unhashable type: 'list'

</code></pre>

<p>What can I do to lemmatize a column with bag of words?</p>

<p>And also how to avoid the problem with tokenization that divides [don't] to [do,n't]?</p>
"
"59505444","Remove synonyms of TFIDF results in python","2019-12-27 19:50:18","1","389","1","1","","59507255","<p>I am currently working on a project where get the top 10 most relevant words of set of document using tfidf in python. However, there are results where are get the same word and its plurial or adverb or so. To go around this problem, I decided to use stemming, but this leads to a problem where words and their antonyms can have the same root or by reducing a word to its root does not enable to go back and find that specific word in the document if a user was to search for it. Is there a nlp that might be better in this context than nlp? Any hint or link will be useful. I working on something that is very similar to youtube.</p>
"
"59503113","Is it possible to retrieve information about higher-level dependencies whith SpaCy (Python)?","2019-12-27 15:54:54","0","158","1","1","","59534995","<p>I am working with SpaCy and my problem is that I don't know how to retrieve information about second-level dependencies. I am interested in knowing if a quantifier (e.g. 'every', 'all', 'some') stands in subject or object position; however, if I ask for the dependency of every token, the dependency assigned to the quantifier is 'DET' (determiner):</p>

<pre><code>doc = nlp(""Mary loves every man"")
for token in doc:
    print(token.text, token.dep_)

Mary nsubj 
loves ROOT
every DET
man dobj
</code></pre>

<p>I would be interested instead in the second-level dependency, that is, I want to know if 'every' is the determiner of an object (as in this case, in which 'every' is the determiner of 'man' that is parsed as a 'dobj') or of a subject.</p>

<p>If you have any other idea for solving the problem, even if they don't involve SpaCy, every information would be very helpful. I have tried to solve the problem in another way <a href=""https://stackoverflow.com/questions/59500488/problem-with-indexes-in-enumerate-python/59501503#59501503"">here</a> but it was not very successful.</p>

<p>Thank you very much for your help!!!</p>
"
"59373151","Lemmatizer for pt_br","2019-12-17 11:18:52","1","438","0","1","","59374288","<p>I am working on lemmatizer for Brazilian portugese (pt_br). I know spacy provides lemmatizer for pt_pt. Can this library be used for pt_br as well or there will be significant difference between pt_pt and pt_br.</p>
"
"59347811","What are some examples of the part-of-speech tag ""list-item-marker""?","2019-12-15 20:31:09","1","394","0","1","","59362941","<p>What are some example sentences that include a word that would be tagged as LS (list item marker)?</p>

<p>This tag is used in <code>spacy</code> and seems to come from UPENN:</p>

<p>UPENN:
<a href=""https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"" rel=""nofollow noreferrer"">https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html</a></p>

<p>Spacy:
<a href=""https://github.com/explosion/spaCy/blob/master/spacy/glossary.py#L68"" rel=""nofollow noreferrer"">https://github.com/explosion/spaCy/blob/master/spacy/glossary.py#L68</a></p>
"
"59316859","Displaying the description of entity from kb id in spacy entity linking","2019-12-13 05:53:24","0","381","0","1","","59355742","<blockquote>
<p>I have successfully trained a spacy entity linking model(obviously by
limiting the data).</p>
<p>my question is how to display the description
of entity from kb as output?</p>
</blockquote>
<pre><code>
import spacy
nlp = spacy.load(r&quot;D:\el model\nlp&quot;)
doc = nlp(&quot;Amir Khan is a great boxer&quot;)
ents = [(e.text, e.label_, e.kb_id_) for e in doc.ents]
print(ents) 
</code></pre>
"
"59281409","gensim lemmatize error generator raised StopIteration","2019-12-11 08:17:32","0","837","0","1","","59281516","<p>I'm trying to execute simple code to lemmatize string, but there's an error about iteration.
I have found some solutions which are about reinstalling web.py, but this not worked for me.</p>

<p>python code</p>

<pre><code>from gensim.utils import lemmatize
lemmatize(""gone"")
</code></pre>

<p>error is</p>

<pre><code>---------------------------------------------------------------------------
StopIteration                             Traceback (most recent call last)
I:\Anaconda\lib\site-packages\pattern\text\__init__.py in _read(path, encoding, comment)
    608             yield line
--&gt; 609     raise StopIteration
    610 

StopIteration: 

The above exception was the direct cause of the following exception:

RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-4-9daceee1900f&gt; in &lt;module&gt;
      1 from gensim.utils import lemmatize
----&gt; 2 lemmatize(""gone"")

-------------------------------------------------------------------------------------

I:\Anaconda\lib\site-packages\pattern\text\__init__.py in &lt;genexpr&gt;(.0)
    623     def load(self):
    624         # Arnold NNP x
--&gt; 625         dict.update(self, (x.split("" "")[:2] for x in _read(self._path) if len(x.split("" "")) &gt; 1))
    626 
    627 #--- FREQUENCY -------------------------------------------------------------------------------------

RuntimeError: generator raised StopIteration
</code></pre>
"
"59266675","How to exclude certain names and terms from stemming (Python NLTK SnowballStemmer (Porter2))","2019-12-10 11:56:35","2","1139","0","1","","59266806","<p>I am newly getting into NLP, Python, and posting on Stackoverflow at the same time, so please be patient with me if I might seem ignorant :). </p>

<p>I am using SnowballStemmer in Python's NLTK in order to stem words for textual analysis. While lemmatization seems to understem my tokens, the snowball porter2 stemmer, which I read is mostly preferred to the basic porter stemmer, <em>overstems</em> my tokens. I am analyzing tweets including many names and probably also places and other words which should not be stemmed, like: hillary, hannity, president, which are now reduced to hillari, hanniti, and presid (you probably guessed already whose tweets I am analyzing). </p>

<p>Is there an easy way to exclude certain terms from stemming? Conversely, I could also merely lemmatize tokens and include a rule for common suffixes like -ed, -s, …. Another idea might be to merely stem verbs and adjectives as well as nouns ending in s. That might also be close enough… </p>

<p>I am using below code as of now: </p>

<pre><code># LEMMATIZE AND STEM WORDS

from nltk.stem.snowball import EnglishStemmer

lemmatizer = nltk.stem.WordNetLemmatizer()
snowball = EnglishStemmer() 

def lemmatize_text(text):

    return [lemmatizer.lemmatize(w) for w in text]

def snowball_stemmer(text):

    return [snowball.stem(w) for w in text]

# APPLY FUNCTIONS

tweets['text_snowball'] = tweets.text_processed.apply(snowball_stemmer)
tweets['text_lemma'] = tweets.text_processed.apply(lemmatize_text)
</code></pre>

<p>I hope someone can help… Contrary to my past experience with all kinds of issues, I have not been able to find adequate help for my issue online so far. </p>

<p>Thanks!</p>
"
"59237194","Train default Perceptron Tagger","2019-12-08 15:57:12","0","120","1","1","","59237359","<p>I can't figure out how to extend/retrain existing model. I don't wanna train my own tagger from the scratch, i just want to take existing and make some changes, based on my needs. For example</p>

<pre><code>tagger = PerceptronTagger(load=True) 
tagger.train([[('restaurant','NN')]])

tokens = nltk.word_tokenize(""Show me restaurant in Berlin"")
tags = tagger.tag(tokens)

for i in tags:
    print(i)
</code></pre>

<p>This of course, outputs by default:</p>

<blockquote>
  <p>&lt;'Show', 'VB'> &lt;'me', 'PRP'> &lt;'restaurant', 'VB'> &lt;'in', 'IN'>
  &lt;'Berlin', 'NNP'></p>
</blockquote>

<p>What i expect is restaurant - NN.
Or also, there is a common problem with entities in lower case like berlin, south korea etc ( which will be: NN, JJ, NN ). Any ideas or suggestions?</p>
"
"59183624","fasttext pre trained sentences similarity","2019-12-04 19:46:06","3","6677","0","1","","59217061","<p>I want to use fasttext pre-trained models to compute similarity
a sentence between a set of sentences.
can anyone help me?
what is the best approach?</p>

<p>I computed the similarity between sentences by train a tfidf model. write code like this.
is it possible to change it and use fasttext pre-trained models? for example use vectors to train a tfidf model?</p>

<pre><code>def generate_tfidf_model(sentences):
    print(""generating TfIdf model"")
    texts = [[sentence for sentence in doc.split()] for doc in sentences]
    dictionary = gensim.corpora.Dictionary(texts)    
    feature_cnt = len(dictionary.token2id)
    mycorpus = [dictionary.doc2bow(doc, allow_update=True) for doc in texts]
    tfidf_model = gensim.models.TfidfModel(mycorpus)
    index = gensim.similarities.SparseMatrixSimilarity(tfidf_model[mycorpus]
                                                        , num_features = feature_cnt)
    return tfidf_model, index, dictionary

def query_search(query, tfidf_model, index, dictionary):
    query = normal_stemmer_sentence(query)
    query_vector = dictionary.doc2bow(query.split())
    similarity = index[tfidf_model[query_vector]]
    return similarity
</code></pre>
"
"59173175","converting a random string into date in python raises REdefinition of group name 'm'","2019-12-04 09:53:14","4","1258","8","1","","59174318","<p>I have been trying to parse some very old data to structure and store them in a database. I have some random strings that contain dates.</p>

<p><code>YEAR:1999        DATE:09/1999</code></p>

<p><code>DATE:09/1996</code></p>

<p><code>DATE:1993</code></p>

<p><code>YEAR:2006   DATE:15/05/06</code></p>

<p><code>YEAR:2019 DATE:JANUARY 3, 2019</code></p>

<p><code>YEAR:2019 DATE:FEB. 14, 2019</code></p>

<p><code>YEAR:2019 DATE: 30/06/2019, JUNE 24, 2019</code></p>

<p>as you can see, there's a plethora of possibilities.
I have tried with datetime, dateutil, dateparser and timefhuman to quickly get a date from this.
None, of them had a desired output.
Most success I have had is with dateparser.</p>

<pre class=""lang-py prettyprint-override""><code>    recieving_date = str(row[8])
    try:
      print (recieving_date)
      recieving_date = str(recieving_date.replace(""DATE"", ''))
      recieving_date = str(recieving_date.replace(""YEAR"", ''))
      recieving_date = str(recieving_date.replace("":"", ''))
      print(recieving_date)
      recieving_date = dateparser.parse(recieving_date, date_formats=[""%Y%d/%m/%y"", ""%Y"", ""%Y%m/%y"" '%d/%m/%Y'])
      print (recieving_date)
    except Exception as e:
      print(e)
</code></pre>

<p>I get an error <strong>redefinition of group name 'm' as group 5; was group 2 at position 99</strong>
Any suggestions on how to proceed. I thought about reading each character to see if there is a date keyword. That sounds a little to much. There has to be a better way?  </p>
"
"59050554","Error running Spacy entity linking example","2019-11-26 12:03:29","2","1526","0","1","","59580852","<p>I was trying the entity linking example in spacy.</p>

<p>This is the information about spaCy in my system.</p>

<pre><code>============================== Info about spaCy ==============================

spaCy version    2.2.2
Location         C:\Users\manimaran.p\AppData\Local\Continuum\anaconda3\envs\spacy\lib\site-packages\spacy
Platform         Windows-8.1-6.3.9600-SP0
Python version   3.7.3
Models
</code></pre>

<p>Using this <a href=""https://github.com/explosion/spaCy/blob/v2.2.2/examples/training/train_entity_linker.py"" rel=""nofollow noreferrer"">example</a> to train the entity linker and generating the knowledge base for the same with this <a href=""https://github.com/explosion/spaCy/blob/v2.2.2/examples/training/pretrain_kb.py"" rel=""nofollow noreferrer"">example</a>.</p>

<p>I can create a knowledge base with the available <strong>en_core_web_md</strong>, this is the output for the same.</p>

<pre><code># python ""create kb.py"" -m en_core_web_md -o pret_kb
Loaded model 'en_core_web_md'

2 kb entities: ['Q2146908', 'Q7381115']
1 kb aliases: ['Russ Cochran']

Saved KB to pret_kb\kb
Saved vocab to pret_kb\vocab

Loading vocab from pret_kb\vocab
Loading KB from pret_kb\kb
2 kb entities: ['Q2146908', 'Q7381115']
1 kb aliases: ['Russ Cochran']
</code></pre>

<p>When I try to train the entity linker with the knowledge base from above, I get this error.</p>

<pre><code># python ""entity linker.py"" ./pret_kb/kb ./pret_kb/vocab
Created blank 'en' model with vocab from 'pret_kb\vocab'
Loaded Knowledge Base from 'pret_kb\kb'
Traceback (most recent call last):
  File ""entity linker.py"", line 156, in &lt;module&gt;
    plac.call(main)
  File ""C:\Users\manimaran.p\AppData\Local\Continuum\anaconda3\envs\spacy\lib\site-packages\plac_core.py"", line 328, in call
    cmd, result = parser.consume(arglist)
  File ""C:\Users\manimaran.p\AppData\Local\Continuum\anaconda3\envs\spacy\lib\site-packages\plac_core.py"", line 207, in consume
    return cmd, self.func(*(args + varargs + extraopts), **kwargs)
  File ""entity linker.py"", line 113, in main
    sgd=optimizer,
  File ""C:\Users\manimaran.p\AppData\Local\Continuum\anaconda3\envs\spacy\lib\site-packages\spacy\language.py"", line 515, in update
    proc.update(docs, golds, sgd=get_grads, losses=losses, **kwargs)
  File ""pipes.pyx"", line 1219, in spacy.pipeline.pipes.EntityLinker.update
KeyError: (0, 12)
</code></pre>

<p>I did follow the instructions specified <a href=""https://spacy.io/usage/training#entity-linker"" rel=""nofollow noreferrer"">here</a>. I used the <strong>en_core_web_md</strong> to create the knowledge base since I do not have a pre-trained model.</p>

<p>I did not write any custom code just trying to run this example, Can someone point me to the right direction.</p>
"
"58971014","How to get spaCy to use universal dependencies","2019-11-21 09:06:17","2","1848","0","3","","58980227","<p>Spacy's site said they use universal dependencies scheme in their annotations specifications page. But when I parse ""I love you"", '''you''' was made a ""dobj"" of ""love"". There's no ""dobj"" in the universal dependency relations doc. So I have two questions : </p>

<ol>
<li>How to get spacy to use the universal dependency relations?</li>
<li>How to get the doc for the relations spacy uses?</li>
</ol>

<p><a href=""https://i.sstatic.net/g9mt9.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/g9mt9.jpg"" alt=""enter image description here""></a></p>
"
"58956995","how to create a function that tokenizes and stems the words","2019-11-20 14:43:06","1","3283","1","2","","59042791","<p>My code</p>

<pre><code>def tokenize_and_stem(text):

    tokens = [sent for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(text)]

    filtered_tokens = [token for token in tokens if re.search('[a-zA-Z]', token)]

    stems = stemmer.stem(filtered_tokens)

words_stemmed = tokenize_and_stem(""Today (May 19, 2016) is his only daughter's wedding."")
print(words_stemmed)
</code></pre>

<p>and I'm getting this error</p>

<p>AttributeError                            Traceback (most recent call last)
 in 
     13     return stems
     14 
---> 15 words_stemmed = tokenize_and_stem(""Today (May 19, 2016) is his only daughter's wedding."")
     16 print(words_stemmed)</p>

<p> in tokenize_and_stem(text)
      9<br>
     10     # Stem the filtered_tokens
---> 11     stems = stemmer.stem(filtered_tokens)
     12<br>
     13     return stems</p>

<p>/usr/local/lib/python3.6/dist-packages/nltk/stem/snowball.py in stem(self, word)
   1415 
   1416         """"""
-> 1417         word = word.lower()
   1418 
   1419         if word in self.stopwords or len(word) &lt;= 2:</p>

<p>AttributeError: 'list' object has no attribute 'lower'</p>
"
"58941049","NLTK Word Tokenize doesn't return anything","2019-11-19 18:55:47","0","205","1","1","","58941391","<p>I am trying to tokenize a sentence, and I believe that the code is correct but there is no output. What could be the problem? Here is the code.</p>

<pre><code>import nltk
from nltk.tokenize import word_tokenize
text = word_tokenize(""And now for something completely different"")
nltk.pos_tag(text)

text = word_tokenize(""They refuse to permit us to obtain the refuse permit"")
nltk.pos_tag(text)
</code></pre>
"
"58839334","CNN - Confusion Matrix wrong display","2019-11-13 14:27:50","3","530","3","1","","58839889","<p>I have trained a model for handwritten digits multiclass classification using CNN in Keras. I am trying to evaluate the model with the same training images to get an estimate of the accuracy of the algorithm; however, when I evaluate the CNN confusion matrix, it gives a one column only of the form:</p>

<pre><code>[[4132    0    0    0    0    0    0    0    0    0]
 [4684    0    0    0    0    0    0    0    0    0]
 [4177    0    0    0    0    0    0    0    0    0]
 [4351    0    0    0    0    0    0    0    0    0]
 [4072    0    0    0    0    0    0    0    0    0]
 [3795    0    0    0    0    0    0    0    0    0]
 [4137    0    0    0    0    0    0    0    0    0]
 [4401    0    0    0    0    0    0    0    0    0]
 [4063    0    0    0    0    0    0    0    0    0]
 [4188    0    0    0    0    0    0    0    0    0]]
</code></pre>

<p>I guess the algorithm is giving the correct result since those are the total numbers of each digit in the database; however, the confusion matrix should give something like this:</p>

<pre><code>[[4132    0    0    0    0    0    0    0    0    0]
 [   0 4684    0    0    0    0    0    0    0    0]
 [   0    0 4177    0    0    0    0    0    0    0]
 [   0    0    0 4351    0    0    0    0    0    0]
 [   0    0    0    0 4072    0    0    0    0    0]
 [   0    0    0    0    0 3795    0    0    0    0]
 [   0    0    0    0    0    0 4137    0    0    0]
 [   0    0    0    0    0    0    0 4401    0    0]
 [   0    0    0    0    0    0    0    0 4063    0]
 [   0    0    0    0    0    0    0    0    0 4188]]
</code></pre>

<p><a href=""https://github.com/elopezfune/Number_Digit_CNN"" rel=""nofollow noreferrer"">The code is in this link</a> </p>

<p><a href=""https://www.kaggle.com/c/digit-recognizer/data"" rel=""nofollow noreferrer"">The data can be taken from the ""train.csv"" file in this Kaggle project.</a></p>

<p>I would like to ask you guys what am I doing wrong in the code, such that I obtain this weird result.</p>
"
"58822292","Batch-train word2vec in gensim with support of multiple workers","2019-11-12 15:56:58","1","1357","0","2","","58822726","<p><strong>Context</strong></p>

<p>There exists severals questions about how to train <code>Word2Vec</code> using <code>gensim</code> with streamed data. Anyhow, these questions don't deal with the issue that streaming cannot use multiple workers since there is no array to split between threads.</p>

<p>Hence I wanted to create a generator providing such functionality for gensim. My results look like:</p>

<pre class=""lang-py prettyprint-override""><code>from gensim.models import Word2Vec as w2v

#The data is stored in a python-list and unsplitted.
#It's too much data to store it splitted, so I have to do the split while streaming.
data = ['this is document one', 'this is document two', ...]

#Now the generator-class
import threading

class dataGenerator:
    """"""
    Generator for batch-tokenization.
    """"""

    def __init__(self, data: list, batch_size:int = 40):
        """"""Initialize generator and pass data.""""""

        self.data = data
        self.batch_size = batch_size
        self.lock = threading.Lock()


    def __len__(self):
        """"""Get total number of batches.""""""
        return int(np.ceil(len(self.data) / float(self.batch_size)))


    def __iter__(self) -&gt; list([]):
        """"""
        Iterator-wrapper for generator-functionality (since generators cannot be used directly).
        Allows for data-streaming.
        """"""
        for idx in range(len(self)):
            yield self[idx]


    def __getitem__(self, idx):

        #Make multithreading thread-safe
        with self.lock:

            # Returns current batch by slicing data.
            return [arr.split("" "") for arr in self.data[idx * self.batch_size : (idx + 1) * self.batch_size]]


#And now do the training
model = w2v(
             sentences=dataGenerator(data),
             size=300,
             window=5,
             min_count=1,
             workers=4
            )
</code></pre>

<p>This results in the error </p>

<blockquote>
  <p>TypeError: unhashable type: 'list'</p>
</blockquote>

<p>Since <code>dataGenerator(data)</code> would work if I'd just yield a single splitted document, I assume that gensims <code>word2vec</code> wraps the generator within an extra list. In this case the <code>__iter__</code> would look like:</p>

<pre class=""lang-py prettyprint-override""><code>def __iter__(self) -&gt; list:
    """"""
    Iterator-wrapper for generator-functionality (since generators cannot be used directly.
    Allows for data-streaming.
    """"""
    for text in self.data:
        yield text.split("" "")
</code></pre>

<p>Hence, my batch would also be wrapped resulting in something like <code>[[['this', '...'], ['this', '...']], [[...], [...]]]</code> (=> list of list of list) which cannot be processed by gensim.</p>

<p><br>
<br>
<br>
<strong>My question:</strong></p>

<p><em>Can I ""stream""-pass batches in order to use multiple workers?
How can I change my code accordingly?</em></p>
"
"58800527","How to lemmatize a .txt file rather than a sentence with pywsd.utils?","2019-11-11 11:32:29","1","1085","0","1","","58800887","<p>I am quite new with Python that I try to learn for basic text analysis, topic modelling etc.</p>

<p>I wrote the following code for cleaning my text file. I prefer pywsed.utils lemmatize.sentence() function to NLTK's WordNetLemmatizer() because it produces cleaner texts. The following code works fine with sentences:</p>

<pre><code>from nltk.corpus import stopwords
from pywsd.utils import lemmatize_sentence
import string

s = ""Dew drops fall from the leaves. Mary leaves the room. It's completed. Hello. This is trial. We went home. It was easier. We drank tea. These are Demo Texts. Right?""

lemm = lemmatize_sentence(s)
print (lemm)

stopword = stopwords.words('english') + list(string.punctuation)
removingstopwords = [word for word in lemm if word not in stopword]
print (removingstopwords, file=open(""cleaned.txt"",""a""))
</code></pre>

<p>But what I fail to do is lemmatizing a raw text file in a directory. I guess lemmatize.sentence() only requires strings?</p>

<p>I manage to read contents of a file with</p>

<pre><code>with open ('a.txt',""r+"", encoding=""utf-8"") as fin:
    lemm = lemmatize_sentence(fin.read())
print (lemm)

</code></pre>

<p>but this time the code fails to remove some keywords like ""n't"", ""'ll"", ""'s"", or ""‘"" and punctuations which result in an uncleaned text. </p>

<p>1) What do I do wrong? Should I tokenize first? (I also failed to feed lemmatize.sentence() with its results). </p>

<p>2) How do I get the output file content without any formatting (words without single quotes and bracket)?</p>

<p>Any help is greatly appreciated. Thanks in advance.</p>
"
"58779371","ImportError: cannot import name 'LEMMA_INDEX' from 'spacy.lang.en'","2019-11-09 12:39:17","4","4897","0","1","","58787086","<p>I'm trying to make a lemmatizer in spaCy, however when I run the code I have, this error keeps popping up.</p>

<pre><code>Traceback (most recent call last):
  File ""word_pract.py"", line 46, in &lt;module&gt;
    from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES
ImportError: cannot import name 'LEMMA_INDEX' from 'spacy.lang.en' 
</code></pre>

<p>This is the code,</p>

<pre><code>import spacy
from spacy.lemmatizer import Lemmatizer
from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES

nlp = spacy.load(""en_core_web_sm"")

lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)
lemmas = lemmatizer(u'ducks', u'NOUN')
print(lemmas)
</code></pre>

<p>I have spaCy up to date, and am on python 3.7.4 in a conda environment. I also download en_core_web_sm, so I don't know why its wrong.</p>
"
"58698428","Is there a way to correctly tag (PoS Tagging) the words which are forming a phrase together?","2019-11-04 17:30:47","0","219","4","1","","58709998","<p>I tried various means to correctly tag a bunch of words which form a phrase (especially Noun Phrase) but could not succeed.</p>

<p>Example: 'the', 'first', 'early','morning', 'sunbeams'</p>

<p>'early' and 'morning' are wrongly being tagged as 'Noun' where expected outcome should be: ('first', 'adverb'), ('early', 'adverb'), ('morning', 'adjective'), ('sunbeams', 'noun')</p>

<p>Could you please suggest a procedure to tag these words correctly?</p>

<p>Thanks in advance.</p>
"
"58625071","Spacy custom POS model for Hindi","2019-10-30 12:07:59","0","940","0","1","","58627653","<p>I recently worked on training a Part-of-Speech model for Hindi in Spacy. I got the model already trained but when analyzing any text, the <code>.pos_</code> attribute of any token always points to <code>X</code>. The fine-grained tags, <code>.tag_</code> - which were the ones the model was trained with - are correct though. </p>

<p>The mapping between this fine-grained tags and the ""universal"" tags (VERB, NOUN, ADJ, etc) is found in the <code>spacy/lang/hi/tag_map.py</code> file.</p>

<pre><code>Lemma यूरोप, Lemmatized: False, POS: X, TAG: NNP
Lemma के, Lemmatized: False, POS: X, TAG: PSP
Lemma जिन, Lemmatized: False, POS: X, TAG: DEM
Lemma राजनीतिक, Lemmatized: False, POS: X, TAG: JJ
Lemma दलों, Lemmatized: False, POS: X, TAG: NN
Lemma को, Lemmatized: False, POS: X, TAG: PSP
Lemma व्यवस्था, Lemmatized: False, POS: X, TAG: NN
Lemma ,, Lemmatized: False, POS: SYM, TAG: SYM
Lemma राजनेताओं, Lemmatized: False, POS: X, TAG: NN
Lemma और, Lemmatized: False, POS: X, TAG: CC
Lemma मीडिया, Lemmatized: False, POS: X, TAG: NN
Lemma द्वारा, Lemmatized: False, POS: X, TAG: PSP
Lemma अति, Lemmatized: False, POS: X, TAG: INTF
Lemma दक्षिणपंथी, Lemmatized: False, POS: X, TAG: NN
Lemma कहा, Lemmatized: False, POS: X, TAG: VM
Lemma जाता, Lemmatized: False, POS: X, TAG: VAUX
Lemma है, Lemmatized: False, POS: X, TAG: VAUX
Lemma (, Lemmatized: False, POS: SYM, TAG: SYM
Lemma परन्तु, Lemmatized: False, POS: X, TAG: CC
Lemma मेरी, Lemmatized: False, POS: X, TAG: PRP
Lemma ओर, Lemmatized: False, POS: X, TAG: NST
Lemma से, Lemmatized: False, POS: X, TAG: PSP
Lemma सभ्यतावादी, Lemmatized: False, POS: X, TAG: NNP
Lemma कहा, Lemmatized: False, POS: X, TAG: VM
Lemma जाता, Lemmatized: False, POS: X, TAG: VAUX
Lemma है, Lemmatized: False, POS: X, TAG: VAUX
Lemma ), Lemmatized: False, POS: SYM, TAG: SYM
Lemma उनकी, Lemmatized: False, POS: X, TAG: PRP
Lemma आलोचना, Lemmatized: False, POS: X, TAG: NN
Lemma उनकी, Lemmatized: False, POS: X, TAG: PRP
Lemma भूलों, Lemmatized: False, POS: X, TAG: NN
Lemma और, Lemmatized: False, POS: X, TAG: CC
Lemma अतिवादिता, Lemmatized: False, POS: X, TAG: NN
Lemma के, Lemmatized: False, POS: X, TAG: PSP
Lemma कारण, Lemmatized: False, POS: X, TAG: PSP
Lemma की, Lemmatized: False, POS: X, TAG: VM
Lemma जाती, Lemmatized: False, POS: X, TAG: VAUX
Lemma है|, Lemmatized: False, POS: X, TAG: NNPC
</code></pre>

<p>Investigating a little bit I found out that the reason the <code>.pos_</code> has this <code>X</code> value is because in the generated <code>lang_model/tagger/tag_map</code> binary file, all of its keys point to <code>101</code> which is the ""code"" assigned to the Part-of-Speech <code>X</code>, which is <code>Other</code>. </p>

<p>I deduce it is generating the keys pointing to <code>101</code> because there's no information at how it should map each of the provided tags from the dataset to the ""universal"" ones. The thing is, I can provide a <code>tag_map.py</code> in the definition of my <code>Hindi(Language)</code> class, but when passing a text through the pipeline, it will eventually use the tag map defined in the <code>tagger/</code> directory created with by the output of the <code>train</code> command.</p>

<p>Here's a link which will clarify what I'm explaining:  <a href=""https://universaldependencies.org/tagset-conversion/hi-conll-uposf.html"" rel=""nofollow noreferrer"">https://universaldependencies.org/tagset-conversion/hi-conll-uposf.html</a> </p>

<p>The first item of the first column (<code>CC</code>, <code>DEM</code>, <code>INTF</code>, etc) are the ones provided to the model. The universal tags are the ones from the second column.</p>

<p>My question is, where should I define the tag_map to overwrite the one generated by the <code>spacy train</code> command?</p>
"
"58618352","How to pass part-of-speech in WordNetLemmatizer?","2019-10-30 03:29:37","0","471","0","1","","58618741","<p>I am preprocessing text data. However, I am facing issue with lemmatizing. 
Below is the sample text:</p>

<blockquote>
  <p>'An 18-year-old boy was referred to prosecutors Thursday for allegedly
  stealing about ¥15 million ($134,300) worth of cryptocurrency last
  year by hacking a digital currency storage website, police said.',
  'The case is the first in Japan in which criminal charges have been
  pursued against a hacker over cryptocurrency losses, the police
  said.', '\n', 'The boy, from the city of Utsunomiya, Tochigi
  Prefecture, whose name is being withheld because he is a minor,
  allegedly stole the money after hacking Monappy, a website where users
  can keep the virtual currency monacoin, between Aug. 14 and Sept. 1
  last year.', 'He used software called Tor that makes it difficult to
  identify who is accessing the system, but the police identified him by
  analyzing communication records left on the website’s server.', 'The
  police said the boy has admitted to the allegations, quoting him as
  saying, “I felt like I’d found a trick no one knows and did it as if I
  were playing a video game.”', 'He took advantage of a weakness in a
  feature of the website that enables a user to transfer the currency to
  another user, knowing that the system would malfunction if transfers
  were repeated over a short period of time.', 'He repeatedly submitted
  currency transfer requests to himself, overwhelming the system and
  allowing him to register more money in his account.', 'About 7,700
  users were affected and the operator will compensate them.', 'The boy
  later put the stolen monacoins in an account set up by a different
  cryptocurrency operator, received payouts in a different
  cryptocurrency and bought items such as a smartphone, the police
  said.', 'According to the operator of Monappy, the stolen monacoins
  were kept using a system with an always-on internet connection, and
  those kept offline were not stolen.'</p>
</blockquote>

<p>My code is: </p>

<pre><code>import pandas as pd
import nltk
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords

df = pd.read_csv('All Articles.csv')
df['Articles'] = df['Articles'].str.lower()

stemming = PorterStemmer()
stops = set(stopwords.words('english'))
lemma = WordNetLemmatizer()

def identify_tokens(row):
    Articles = row['Articles']
    tokens = nltk.word_tokenize(Articles)
    token_words = [w for w in tokens if w.isalpha()]
    return token_words


df['words'] = df.apply(identify_tokens, axis=1)


def stem_list(row):
    my_list = row['words']
    stemmed_list = [stemming.stem(word) for word in my_list]
    return (stemmed_list)


df['stemmed_words'] = df.apply(stem_list, axis=1)


def lemma_list(row):
    my_list = row['stemmed_words']
    lemma_list = [lemma.lemmatize(word, pos='v') for word in my_list]
    return (lemma_list)


df['lemma_words'] = df.apply(lemma_list, axis=1)


def remove_stops(row):
    my_list = row['lemma_words']
    meaningful_words = [w for w in my_list if not w in stops]
    return (meaningful_words)


df['stem_meaningful'] = df.apply(remove_stops, axis=1)


def rejoin_words(row):
    my_list = row['stem_meaningful']
    joined_words = ("" "".join(my_list))
    return joined_words


df['processed'] = df.apply(rejoin_words, axis=1)
</code></pre>

<p>As it is clear from the code that I am using pandas. However here I have given sample text. </p>

<p>My problem area is :</p>

<pre><code>def lemma_list(row):
    my_list = row['stemmed_words']
    lemma_list = [lemma.lemmatize(word, pos='v') for word in my_list]
    return (lemma_list)

df['lemma_words'] = df.apply(lemma_list, axis=1)
</code></pre>

<p>Though the code is running without any error lemma function is not working expectedly.</p>

<p>Thanks in Advance.</p>
"
"58506951","Why isn't WSD matching WordNet?","2019-10-22 14:50:56","1","246","0","1","","58522190","<p>I'm getting to grips with WSD and WordNet and I'm trying to work out why they are outputting different results. My understanding when using the below code is that the disambiguate command nominates the most likely Synset:</p>

<pre><code>from pywsd import disambiguate
from nltk.corpus import wordnet as wn

mysent = 'I went to have a drink in a bar'

wsd = disambiguate(mysent)
</code></pre>

<p>Which gives me the below output</p>

<pre><code>('I', None)
('went', Synset('travel.v.01'))
('to', None)
('have', None)
('a', None)
('drink', Synset('swallow.n.02'))
('in', None)
('a', None)
('bar', Synset('barroom.n.01'))
</code></pre>

<p>From this, I find it odd that the word 'I' was returned as 'nonetype' given that when looking up the word in WordNet I get one of four possible interpretations. Surely, 'I' should correspond to at least one of them?</p>

<pre><code>wordnet.synsets('I')

Out:
[Synset('iodine.n.01'), Synset('one.n.01'), Synset('i.n.03'), Synset('one.s.01')]
</code></pre>
"
"58475716","Skit-learn with Spacy parallelization error with RandomizedSearchCV","2019-10-20 17:56:23","1","705","7","1","","58516624","<p>I use Sklearn and Spacy to make a NLP machine learning model. But, I have a parallelization error when I train my model with the class <code>RandomizedSearchCV()</code>.</p>

<p>My class <code>TextProcessor</code> allows me to do text processing with the Spacy library.</p>

<pre><code>class TextProcessor(BaseEstimator, TransformerMixin):

    def __init__(self, remove_stop_word=False):
        self.remove_stop_word = remove_stop_word
        self.nlp = spacy.load('en')
        self.punctuations = string.punctuation

    def spacy_text_processing(self, sentence):
        '''
        This function allow to process the text with spacy
        '''
        final_sentence = []
        for word in self.nlp(sentence):
            if self.remove_stop_word:
                if word.is_stop:
                    continue

            if word.text not in self.punctuations:
                final_sentence.append(word.lemma_)

        return final_sentence

    def transform(self, X, y=None):
        X_transformed = []
        for sentence in X:
            X_transformed.append(' '.join(self.spacy_text_processing(sentence)))
        return X_transformed

    def fit(self, X, y=None):
        return self
</code></pre>

<p>After that I use a sklearn pipeline to perform different processing on the text and finally I add a SVR model (the error comes with any type of model). But when I use the parameter <code>n_jobs</code> with a value other than 1 I get a parallelization error.</p>

<pre><code>param_grid = {...} 

svr_model = Pipeline([('text_processing', TextProcessor()),
                    ('vectorizer', CountVectorizer()),
                    ('tfidf', TfidfTransformer()),
                    ('svr', SVR())])

random_search_svr = RandomizedSearchCV(svr_model, param_grid, scoring='neg_mean_absolute_error', n_jobs=-1)
random_search_svr.fit(X_train, y_train)
</code></pre>

<p>This problem is very annoying because training models with classes like <code>GridSearchCV()</code> and <code>RandomizedSearchCV()</code> take a lot of time. Would there be any way to solve the problem or get around it?</p>

<p>The variables X_train and y_train contain the following sample values:</p>

<pre><code>X_train = [""Morrisons book second consecutive quarter of sales growth"", ""Glencore to refinance its short-term debt early, shares rise"", ...] #List of sentences

y_train = [0.43, 0.34, ...] #Sentiment between -1 and 1 associate to the sentence
</code></pre>

<p>The error is : </p>

<pre><code>Exception in thread QueueFeederThread:
Traceback (most recent call last):
  File ""C:\ProgramData\Anaconda3\lib\site-packages\sklearn\externals\joblib\externals\loky\backend\queues.py"", line 150, in _feed
    obj_ = dumps(obj, reducers=reducers)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\sklearn\externals\joblib\externals\loky\backend\reduction.py"", line 243, in dumps
    dump(obj, buf, reducers=reducers, protocol=protocol)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\sklearn\externals\joblib\externals\loky\backend\reduction.py"", line 236, in dump
    _LokyPickler(file, reducers=reducers, protocol=protocol).dump(obj)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py"", line 284, in dump
    return Pickler.dump(self, obj)
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 437, in dump
    self.save(obj)
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 549, in save
    self.save_reduce(obj=obj, *rv)
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 662, in save_reduce
    save(state)
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 856, in save_dict
    self._batch_setitems(obj.items())
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 882, in _batch_setitems
    save(v)
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 549, in save
    self.save_reduce(obj=obj, *rv)
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 662, in save_reduce
    save(state)
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 856, in save_dict
    self._batch_setitems(obj.items())
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 887, in _batch_setitems
    save(v)
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 549, in save
    self.save_reduce(obj=obj, *rv)
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 662, in save_reduce
    save(state)
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 856, in save_dict
    self._batch_setitems(obj.items())
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 882, in _batch_setitems
    save(v)
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 816, in save_list
    self._batch_appends(obj)
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 843, in _batch_appends
    save(tmp[0])
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 771, in save_tuple
    save(element)
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 771, in save_tuple
    save(element)
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 549, in save
    self.save_reduce(obj=obj, *rv)
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 662, in save_reduce
    save(state)
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 856, in save_dict
    self._batch_setitems(obj.items())
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 882, in _batch_setitems
    save(v)
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 816, in save_list
    self._batch_appends(obj)
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 840, in _batch_appends
    save(x)
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 771, in save_tuple
    save(element)
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 549, in save
    self.save_reduce(obj=obj, *rv)
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 662, in save_reduce
    save(state)
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 856, in save_dict
    self._batch_setitems(obj.items())
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 882, in _batch_setitems
    save(v)
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 549, in save
    self.save_reduce(obj=obj, *rv)
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 662, in save_reduce
    save(state)
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 856, in save_dict
    self._batch_setitems(obj.items())
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 882, in _batch_setitems
    save(v)
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 549, in save
    self.save_reduce(obj=obj, *rv)
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 662, in save_reduce
    save(state)
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 786, in save_tuple
    save(element)
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 524, in save
    rv = reduce(self.proto)
  File ""stringsource"", line 2, in preshed.maps.PreshMap.__reduce_cython__
TypeError: self.c_map cannot be converted to a Python object for pickling

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\ProgramData\Anaconda3\lib\threading.py"", line 917, in _bootstrap_inner
    self.run()
  File ""C:\ProgramData\Anaconda3\lib\threading.py"", line 865, in run
    self._target(*self._args, **self._kwargs)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\sklearn\externals\joblib\externals\loky\backend\queues.py"", line 175, in _feed
    onerror(e, obj)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\sklearn\externals\joblib\externals\loky\process_executor.py"", line 310, in _on_queue_feeder_error
    self.thread_wakeup.wakeup()
  File ""C:\ProgramData\Anaconda3\lib\site-packages\sklearn\externals\joblib\externals\loky\process_executor.py"", line 155, in wakeup
    self._writer.send_bytes(b"""")
  File ""C:\ProgramData\Anaconda3\lib\multiprocessing\connection.py"", line 183, in send_bytes
    self._check_closed()
  File ""C:\ProgramData\Anaconda3\lib\multiprocessing\connection.py"", line 136, in _check_closed
    raise OSError(""handle is closed"")
OSError: handle is closed

---------------------------------------------------------------------------
_RemoteTraceback                          Traceback (most recent call last)
_RemoteTraceback: 
""""""
Traceback (most recent call last):
  File ""C:\ProgramData\Anaconda3\lib\site-packages\sklearn\externals\joblib\externals\loky\backend\queues.py"", line 150, in _feed
    obj_ = dumps(obj, reducers=reducers)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\sklearn\externals\joblib\externals\loky\backend\reduction.py"", line 243, in dumps
    dump(obj, buf, reducers=reducers, protocol=protocol)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\sklearn\externals\joblib\externals\loky\backend\reduction.py"", line 236, in dump
    _LokyPickler(file, reducers=reducers, protocol=protocol).dump(obj)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py"", line 284, in dump
    return Pickler.dump(self, obj)
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 437, in dump
    self.save(obj)
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 549, in save
    self.save_reduce(obj=obj, *rv)
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 662, in save_reduce
    save(state)
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 856, in save_dict
    self._batch_setitems(obj.items())
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 882, in _batch_setitems
    save(v)
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 549, in save
    self.save_reduce(obj=obj, *rv)
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 662, in save_reduce
    save(state)
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 856, in save_dict
    self._batch_setitems(obj.items())
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 887, in _batch_setitems
    save(v)
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 549, in save
    self.save_reduce(obj=obj, *rv)
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 662, in save_reduce
    save(state)
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 856, in save_dict
    self._batch_setitems(obj.items())
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 882, in _batch_setitems
    save(v)
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 816, in save_list
    self._batch_appends(obj)
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 843, in _batch_appends
    save(tmp[0])
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 771, in save_tuple
    save(element)
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 771, in save_tuple
    save(element)
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 549, in save
    self.save_reduce(obj=obj, *rv)
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 662, in save_reduce
    save(state)
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 856, in save_dict
    self._batch_setitems(obj.items())
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 882, in _batch_setitems
    save(v)
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 816, in save_list
    self._batch_appends(obj)
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 840, in _batch_appends
    save(x)
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 771, in save_tuple
    save(element)
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 549, in save
    self.save_reduce(obj=obj, *rv)
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 662, in save_reduce
    save(state)
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 856, in save_dict
    self._batch_setitems(obj.items())
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 882, in _batch_setitems
    save(v)
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 549, in save
    self.save_reduce(obj=obj, *rv)
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 662, in save_reduce
    save(state)
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 856, in save_dict
    self._batch_setitems(obj.items())
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 882, in _batch_setitems
    save(v)
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 549, in save
    self.save_reduce(obj=obj, *rv)
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 662, in save_reduce
    save(state)
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 786, in save_tuple
    save(element)
  File ""C:\ProgramData\Anaconda3\lib\pickle.py"", line 524, in save
    rv = reduce(self.proto)
  File ""stringsource"", line 2, in preshed.maps.PreshMap.__reduce_cython__
TypeError: self.c_map cannot be converted to a Python object for pickling
""""""

The above exception was the direct cause of the following exception:

PicklingError                             Traceback (most recent call last)
&lt;ipython-input-12-8979d799633f&gt; in &lt;module&gt;
     15 
     16 random_search_svr = RandomizedSearchCV(svr_grid_model, param_grid_svr,scoring='neg_mean_absolute_error',n_jobs=-1)
---&gt; 17 random_search_svr.fit(X_train, y_train)

C:\ProgramData\Anaconda3\lib\site-packages\sklearn\model_selection\_search.py in fit(self, X, y, groups, **fit_params)
    720                 return results_container[0]
    721 
--&gt; 722             self._run_search(evaluate_candidates)
    723 
    724         results = results_container[0]

C:\ProgramData\Anaconda3\lib\site-packages\sklearn\model_selection\_search.py in _run_search(self, evaluate_candidates)
   1513         evaluate_candidates(ParameterSampler(
   1514             self.param_distributions, self.n_iter,
-&gt; 1515             random_state=self.random_state))

C:\ProgramData\Anaconda3\lib\site-packages\sklearn\model_selection\_search.py in evaluate_candidates(candidate_params)
    709                                for parameters, (train, test)
    710                                in product(candidate_params,
--&gt; 711                                           cv.split(X, y, groups)))
    712 
    713                 all_candidate_params.extend(candidate_params)

C:\ProgramData\Anaconda3\lib\site-packages\sklearn\externals\joblib\parallel.py in __call__(self, iterable)
    928 
    929             with self._backend.retrieval_context():
--&gt; 930                 self.retrieve()
    931             # Make sure that we get a last message telling us we are done
    932             elapsed_time = time.time() - self._start_time

C:\ProgramData\Anaconda3\lib\site-packages\sklearn\externals\joblib\parallel.py in retrieve(self)
    831             try:
    832                 if getattr(self._backend, 'supports_timeout', False):
--&gt; 833                     self._output.extend(job.get(timeout=self.timeout))
    834                 else:
    835                     self._output.extend(job.get())

C:\ProgramData\Anaconda3\lib\site-packages\sklearn\externals\joblib\_parallel_backends.py in wrap_future_result(future, timeout)
    519         AsyncResults.get from multiprocessing.""""""
    520         try:
--&gt; 521             return future.result(timeout=timeout)
    522         except LokyTimeoutError:
    523             raise TimeoutError()

C:\ProgramData\Anaconda3\lib\concurrent\futures\_base.py in result(self, timeout)
    423                 raise CancelledError()
    424             elif self._state == FINISHED:
--&gt; 425                 return self.__get_result()
    426 
    427             self._condition.wait(timeout)

C:\ProgramData\Anaconda3\lib\concurrent\futures\_base.py in __get_result(self)
    382     def __get_result(self):
    383         if self._exception:
--&gt; 384             raise self._exception
    385         else:
    386             return self._result

PicklingError: Could not pickle the task to send it to the workers.
</code></pre>

<p>Version:</p>

<ul>
<li>Python: 3.7.1</li>
<li>Spacy: 2.2.1</li>
<li>Sklearn: 0.20.1</li>
</ul>
"
"58375251","Elixir/Erlang - Split paragraph into sentences based on the language","2019-10-14 10:52:21","0","277","4","1","","58376369","<p>In Java there is a class called BreakItterator which allows me to pass a paragraph of text in any language (the language it is written in is known) and it will split the text into separate sentences. The magic is that it can take as an argument the locale of the langue the text is written in and it will split the text according to that languages rules (if you look into it it is actually a very complex issue even in English - it is certainly not a case of 'split by full-stops/periods').</p>

<p>Does anybody know how I would do this in elixir? I can't find anything in a Google search.</p>

<p>I am almost at the point of deploying a very thin public API that does only this basic task that I can call into from elixir - but this is really not desirable.</p>

<p>Any help would be really appreciated.</p>
"
"58349049","wrong lemmatizing using nltk ""python 3.7.4""","2019-10-11 22:14:19","1","180","0","1","","58349521","<p>I'm using nltk lemmatizer and I get wrong result everytime !! </p>

<pre><code>&gt;&gt;&gt; import nltk
&gt;&gt;&gt; from nltk.stem import WordNetLemmatizer
&gt;&gt;&gt; print(WordNetLemmatizer().lemmatize('loved'))
loved
&gt;&gt;&gt; print(WordNetLemmatizer().lemmatize('creating'))
creating
</code></pre>

<p>the output is 'loved'/ 'creating'.. and it should be 'love' / 'create'</p>
"
"58295677","NLTK Tokeninizing Optimization","2019-10-09 00:51:02","0","224","3","1","","58497915","<p>I have an <code>NLTK</code> parsing function that I am using to parse a ~2GB text file of a TREC dataset. The goal for this dataset is tokenize the entire collection, perform some calculations (such as calculating TF-IDF weights, etc), and then to run some queries against our collection to use cosine similarity and return the best results.</p>

<p>As it stands, my program works but takes well over an hour (typically between 44-61 minutes) to run. The timing is broken down as follows:</p>

<pre><code>TOTAL TIME TO COMPLETE: 4487.930628299713
TIME TO GRAB SORTED COSINE SIMS: 35.24157094955444
TIME TO CREATE TFIDF BY DOC: 57.06743311882019
TIME TO CREATE IDF LOOKUP: 0.5097501277923584
TIME TO CREATE INVERTED INDEX: 2.5217013359069824
TIME TO TOKENIZE: 4392.5711488723755
</code></pre>

<p>So obviously, the tokenization is accounting for ~98% of the time. I am looking for a way to speed that up.</p>

<p>The tokenization code is below:</p>

<pre><code>def remove_nums(arr): 
    pattern = '[0-9]'  
    arr = [re.sub(pattern, '', i) for i in arr]    
    return arr


def get_words(para):   
    stop_words = list(stopwords.words('english'))    
    words = RegexpTokenizer(r'\w+')
    lower = [word.lower() for word in words.tokenize(para)]
    nopunctuation = [nopunc.translate(str.maketrans('', '', string.punctuation)) for nopunc in lower]
    no_integers = remove_nums(nopunctuation)
    dirty_tokens = [data for data in no_integers if data not in stop_words]
    tokens = [data for data in dirty_tokens if data.strip()]

def driver(file):
   myfile = get_input(file)
    p = r'&lt;P ID=\d+&gt;.*?&lt;/P&gt;'       
    paras = RegexpTokenizer(p)   
    document_frequency = collections.Counter()   
    collection_frequency = collections.Counter()   
    all_lists = []    
    currWordCount = 0   
    currList = [] 
    currDocList = []
    all_doc_lists = []
    num_paragraphs = len(paras.tokenize(myfile))  


    print()
    print("" NOW BEGINNING TOKENIZATION "")
    print()
    for para in paras.tokenize(myfile):             
        group_para_id = re.match(""&lt;P ID=(\d+)&gt;"", para)
        para_id = group_para_id.group(1)       
        tokens = get_words(para)
        tokens = list(set(tokens))     
        collection_frequency.update(tokens)      
        document_frequency.update(set(tokens))       
        para = para.translate(str.maketrans('', '', string.punctuation))     
        currPara = para.lower().split()      
        for token in tokens:          
            currWordCount = currPara.count(token)          
            currList = [token, tuple([para_id, currWordCount])]          
            all_lists.append(currList)

            currDocList = [para_id, tuple([token, currWordCount])]
            all_doc_lists.append(currDocList)

    d = {}
    termfreq_by_doc = {}    
    for key, new_value in all_lists:       
        values = d.setdefault(key, [])       
        values.append(new_value)

    for key, new_value in all_doc_lists:
        values = termfreq_by_doc.setdefault(key, [])
        values.append(new_value)
</code></pre>

<p>I am pretty new to optimization, and am looking for some feedback. I did see <a href=""https://stackoverflow.com/questions/41912083/nltk-tokenize-faster-way"">this post</a> which condemns a lot of my list comprehensions as ""evil"", but I can't think of a way around what I am doing.</p>

<p>The code is <em>not</em> well commented, so if for some reason it is not understandable, that is okay. I see other questions on this forum re: speeding up <code>NLTK</code> tokenization without a lot of feedback, so I am hoping for a positive thread about tokenization optimization programming practices.</p>
"
"58294772","Reducing runtime for cosine similarity calculations between 2 lists in Python","2019-10-08 22:24:11","0","375","0","1","","58295282","<p>I'm assembling a twitter hashtag dictionary using Python. The keys are the hashtag itself and the corresponding entry is a large collection of tweets that contain this hashtag appended end-to-end. I've got a separate list of all hashtagless tweets and am adding them to dictionary entries according to cosine similarity. Everything is working but is VERY slow (a few hours for 4000 tweets). The nested for loops are giving me O(N^2) runtime. Does anyone have any ideas on how I could improve my runtime? Any suggestions will be greatly appreciated!</p>

<pre><code>taglessVects = normalize(vectorizer.transform(needTags))
    dictVects = normalize(vectorizer.transform(newDict))

   #newDict contains: newDict[hashtag]: ""tweets that used that hashtag""
   #needTags is a list of all the tweets that didn;t use a hashtag
    for dVect, entry in zip(dictVects, newDict):
        for taglessVect, tweet in zip(taglessVects, needTags):
            if cosine_similarity(taglessVect, dVect) &gt; .9:
                newDict[entry] = newDict[entry] + ' ' + tweet


    return newDict

</code></pre>
"
"58292167","Hebrew Stanford NLP tag set","2019-10-08 18:32:50","0","169","0","1","","58307999","<p>I am trying to find the <strong>exact</strong> list of tag set used in the Hebrew treebank used by Stanford NLP. Finding this tag set seems to be harder than finding a POS tagger :)</p>

<p>Are there any tools for reading the tag set used for training a (Penn?) tree bank?</p>
"
"58123369","How to identify terms from list in unseen documents","2019-09-26 19:15:52","0","79","0","1","","58124810","<p>Given a list of predefined terms that can be formed by one, two or even three words, the problem is to count their ocurrences in a set of documents with a free vocabulary (ie, much many words). </p>

<pre><code>terms= [
[t1],
[t2, t3],
[t4, t5, t6],
[t7],...]
</code></pre>

<p>and the documents where this terms needs to be recognized are in the form of:</p>

<pre><code>docs = [
[w1, w2, t1, w3, w4, t7],        #d1
[w1, w4, t4, t5, t6, wi, ...],   #d2
[wj, t7, ..] ..]                 #d3
</code></pre>

<p>The desired output should be</p>

<pre><code>[2, 1, 1, ...]
</code></pre>

<p>This is, the first doc has two terms of interest, the second has 1 (formed of three words) and so on.</p>

<p>If the terms needed to be accounted for where 1 word length, then I could easily order each document alphabetically, remove repeted terms (set) and then intersect with the terms of size 1 word. Counting repeated words are the searched result.</p>

<p>But with terms of length >=2 things get tricky.</p>

<p>I've been using gensim to form a bag of words and detect the indexes when using a new phrase </p>

<p>e.g.</p>

<pre><code>dict_terms = corpora.Dictionary(phrases)

sentence = unseen_docs[0]
idxs     = dict_terms[sentence]
</code></pre>

<p>And then count the seend idxs considering if the indexes are sequential, that would mean that a single term has been seen and not 2 o 3 of them.</p>

<p>Any suggestions.</p>
"
"58068992","Query related to stemming in NLP","2019-09-23 19:28:45","-2","725","3","3","","58109352","<p>I am working on hands-on task based on stemming under NLP using python.</p>
<p>Below is the task which would required to be executed step wise to fetch the result.</p>
<p>I have completed till step 13 and got stuck at step number 14 and 15 (see below).</p>
<p>Please help me to know how to perform the step number 14 and 15.</p>
<h3>TASK</h3>
<ol>
<li><p>Import the text corpus <strong>brown</strong>.</p>
</li>
<li><p>Extract the list of words associated with text collections belonging to the humor genre. Store the result in the variable <strong>humor_words</strong>.</p>
</li>
<li><p>Convert each word of the list humor_words into lower case, and store the result in <strong>lc_humor_words</strong>.</p>
</li>
<li><p>Find the list of unique words present in <strong>lc_humor_words</strong>. Store the result in <strong>lc_humor_uniq_words</strong>.</p>
</li>
<li><p>Import the corpus <strong>words</strong>.</p>
</li>
<li><p>Extract the list of words associated with the corpus words. Store the result in the variable <strong>wordlist_words</strong>.</p>
</li>
<li><p>Find the list of unique words present in <strong>wordlist_words</strong>. Store the result in <strong>wordlist_uniq_words</strong>.</p>
</li>
<li><p>Create an instance of PorterStemmer named, <strong>porter</strong>.</p>
</li>
<li><p>Create an instance of LancasterStemmer named, <strong>lancaster</strong>.</p>
</li>
<li><p>Stem each word present in <strong>lc_humor_uniq_words</strong> with porter instance, and store the result in the list <strong>p_stemmed</strong></p>
</li>
<li><p>Stem each word present in <strong>lc_humor_uniq_words</strong> with lancaster instance, and store the result in the <strong>listl_stemmed</strong>`</p>
</li>
<li><p>Filter the stemmed words from <strong>p_stemmed</strong> which are also present in <strong>wordlist_uniq_words</strong>. Store the result in <strong>p_stemmed_in_wordlist</strong>.</p>
</li>
<li><p>Filter the stemmed words from <strong>l_stemmed</strong> which are also present in <strong>wordlist_uniq_words</strong>. Store the result in <strong>l_stemmed_in_wordlist</strong>.</p>
</li>
<li><p>Filter the words from <strong>lc_humor_uniq_words</strong> which have the same length as its corresponding stemmed word present in <strong>p_stemmed</strong>, and also contains at least one different character from the corresponding stemmed word. Store the result in the list <strong>p_stemmed_diff</strong>.</p>
</li>
<li><p>Filter the words from <strong>lc_humor_uniq_words</strong> which have the same length as its corresponding stemmed word, present in <strong>l_stemmed</strong>, and also contains at least one different character from the corresponding stemmed word. Store the result in list <strong>l_stemmed_diff</strong>.</p>
</li>
<li><p>Print the number of words present in <strong>p_stemmed_diff</strong>.</p>
</li>
<li><p>Print the number of words present in <strong>l_stemmed_diff</strong>.</p>
</li>
</ol>
<p>-Below is the which I have completed till step 13.</p>
<pre><code>import nltk

import nltk.corpus

from nltk.corpus import brown

humor_words = brown.words(categories = 'humor')

lc_humor_words = [w.lower() for w in humor_words]

lc_humor_uniq_words = set(lc_humor_words)

from nltk.corpus import words

wordlist_words = words.words()

wordlist_uniq_words = set(wordlist_words)

from nltk.stem import PorterStemmer

porter = PorterStemmer()

from nltk.stem import LancasterStemmer

lancaster = LancasterStemmer()

p_stemmed = []

for word in lc_humor_uniq_words:

    p_stemmed.append(porter.stem(word))

l_stemmed = []

for wordd in lc_humor_uniq_words:

    l_stemmed.append(lancaster.stem(wordd))

p_stemmed_in_wordlist = [word1 for word1 in p_stemmed if word1 in wordlist_uniq_words]

l_stemmed_in_wordlist = [word2 for word2 in l_stemmed if word2 in wordlist_uniq_words]
</code></pre>
"
"58056275","Remove first x characters from a few column headers","2019-09-23 05:45:02","3","862","0","1","","58056286","<p>I have created a sparse matrix dataframe which has taken the values in a list and set them as column headers. A number of rows contain headers for example ""000 bank"". I want to remove the ""000 "" so it is just 'bank' for example. </p>

<pre><code>000 bank    000 claim   000 confirmed   000 debit   000 delete  000 frequent    000 hashed  ...  
0   0.000000    0.0 0.0 0.0 0.0 0.0 0.00000 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0  
1   0.052024    0.0 0.0 0.0 0.0 0.0 0.00000 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 kddi
2   0.000000    0.0 0.0 0.0 0.0 0.0 0.00000 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 e
3   0.000000    0.0 0.0 0.0 0.0 0.0 0.00000 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2

Index(['000', '000 000', '000 3rd', '000 bank', '000 claim', '000 confirmed',
 '000 debit', '000 delete', '000 frequent', '000 hashed',
   ...
   'years multiple', 'yet', 'yet confirm', 'yet evidence', 'yet expired',
   'yet many', 'yet published', 'zarefarid', 'zarefarid wrote', 'Keyword'],
  dtype='object', length=3831)
</code></pre>

<p>How can I get rid of the '000 '. Not all column headers have the 000 in them as you can see in the index above. </p>
"
"57994144","extract pos_tag_sents from pandas series","2019-09-18 13:40:27","0","211","2","1","","57994696","<p>following the advice from the thread <a href=""https://stackoverflow.com/questions/41674573/how-to-apply-pos-tag-sents-to-pandas-dataframe-efficiently"">How to apply pos_tag_sents() to pandas dataframe efficiently</a> I run the code to identify different pos for the text in one of my variables. </p>

<p>Now that I managed to create the column of interest - sub['POS'] - how do I extract my relevant information - all the NN - and create a column for each of them?</p>

<pre><code>print(sub['POS'])

5     [(e-mail, JJ), (new, JJ), (delhi, NN), ((, (),...
4     [(bangladesh, JJ), (garment, NN), (unions, NNS...
41    [(listen, VB), (blaze, NN), (wrecks, NNS), (te...
10    [(11:49, CD), (am, VBP), (,, ,), (september, V...
17    [(listen, JJ), (two, CD), (events, NNS), (plan...
</code></pre>

<p>as an output, I would like a new column (here as 'NN'), that contains all the NN for each row. </p>

<pre><code>df = pd.DataFrame([""delhi"", 
                   ""garment"" , 
                   ""blaze"", 
                   NaN], columns=['NN'])
</code></pre>
"
"57987235","What are the means to compute relevance score between question-answer pairs?","2019-09-18 07:25:25","0","452","0","1","","57997729","<p>In information retrieval or question answering system, we use TD-IDF or BM25 to compute the similarity score of question-question pair as the baseline or coarse ranking for deep learning.</p>

<p>In community question answering, we already have the question-answer pairs to collect some statistics info. Without deep learning, could we invent an algorithm like BM25 to compute the relevance score of question-answer pair?</p>

<p>What are some ways to do it?</p>
"
"57977061","Find successively connected nouns or pronouns in string","2019-09-17 14:59:22","0","278","0","1","","57977458","<p>I want to find stand-alone or successively connected nouns in a text. I put together below code, but it is neither efficient nor pythonic. Does anybody have a more pythonic way of finding these nouns with spaCy?</p>

<p>Below code builds a dict with all tokens and then runs through them to find stand-alone or connected <code>PROPN</code> or <code>NOUN</code> until the for-loop runs out of range. It returns a list of the collected items.</p>

<pre><code>def extract_unnamed_ents(doc):
  """"""Takes a string and returns a list of all succesively connected nouns or pronouns"""""" 
  nlp_doc = nlp(doc)
  token_list = []
  for token in nlp_doc:
    token_dict = {}
    token_dict['lemma'] = token.lemma_
    token_dict['pos'] = token.pos_
    token_dict['tag'] = token.tag_
    token_list.append(token_dict)
  ents = []
  k = 0
  for i in range(len(token_list)):
    try:
      if token_list[k]['pos'] == 'PROPN' or token_list[k]['pos'] == 'NOUN':
        ent = token_list[k]['lemma']

        if token_list[k+1]['pos'] == 'PROPN' or token_list[k+1]['pos'] == 'NOUN':
          ent = ent + ' ' + token_list[k+1]['lemma']
          k += 1
          if token_list[k+1]['pos'] == 'PROPN' or token_list[k+1]['pos'] == 'NOUN':
            ent = ent + ' ' + token_list[k+1]['lemma']
            k += 1
            if token_list[k+1]['pos'] == 'PROPN' or token_list[k+1]['pos'] == 'NOUN':
              ent = ent + ' ' + token_list[k+1]['lemma']
              k += 1
              if token_list[k+1]['pos'] == 'PROPN' or token_list[k+1]['pos'] == 'NOUN':
                ent = ent + ' ' + token_list[k+1]['lemma']
                k += 1
        if ent not in ents:
          ents.append(ent)
    except:
      pass
    k += 1
  return ents
</code></pre>

<p>Test:</p>

<pre><code>extract_unnamed_ents('Chancellor Angela Merkel and some of her ministers will discuss at a cabinet '
                     ""retreat next week ways to avert driving bans in major cities after Germany's ""
                     'top administrative court in February allowed local authorities to bar '
                     'heavily polluting diesel cars.')
</code></pre>

<p>Out:</p>

<pre><code>['Chancellor Angela Merkel',
 'minister',
 'cabinet retreat',
 'week way',
 'ban',
 'city',
 'Germany',
 'court',
 'February',
 'authority',
 'diesel car']
</code></pre>
"
"57925219","Why is ""machine_learning"" lemmatized both as ""machine_learning"" and ""machine_learne""?","2019-09-13 14:08:25","0","88","0","1","","57927922","<p>I am running LDA on a number of texts. When I generated some visualizations of the produced topics, I found that the bigram ""machine_learning"" had been lemmatized both as ""machine_learning"" and ""machine_learne"". Here is as minimal a reproducible example as I can provide:</p>

<pre><code>import en_core_web_sm

tokenized = [
    [
        'artificially_intelligent', 'funds', 'generating', 'excess', 'returns',
        'artificial_intelligence', 'deep_learning', 'compelling', 'reasons',
        'join_us', 'artificially_intelligent', 'fund', 'develop', 'ai',
        'machine_learning', 'capabilities', 'real', 'cases', 'big', 'players',
        'industry', 'discover', 'emerging', 'trends', 'latest_developments',
        'ai', 'machine_learning', 'industry', 'players', 'trading',
        'investing', 'live', 'investment', 'models', 'learn', 'develop',
        'compelling', 'business', 'case', 'clients', 'ceos', 'adopt', 'ai',
        'machine_learning', 'investment', 'approaches', 'rare', 'gathering',
        'talents', 'including', 'quants', 'data_scientists', 'researchers',
        'ai', 'machine_learning', 'experts', 'investment_officers', 'explore',
        'solutions', 'challenges', 'potential', 'risks', 'pitfalls',
        'adopting', 'ai', 'machine_learning'
    ],
    [
        'recent_years', 'topics', 'data_science', 'artificial_intelligence',
        'machine_learning', 'big_data', 'become_increasingly', 'popular',
        'growth', 'fueled', 'collection', 'availability', 'data',
        'continually', 'increasing', 'processing', 'power', 'storage', 'open',
        'source', 'movement', 'making', 'tools', 'widely', 'available',
        'result', 'already', 'witnessed', 'profound', 'changes', 'work',
        'rest', 'play', 'trend', 'increase', 'world', 'finance', 'impacted',
        'investment', 'managers', 'particular', 'join_us', 'explore',
        'data_science', 'means', 'finance_professionals'
    ]
]

nlp = en_core_web_sm.load(disable=['parser', 'ner'])

def lemmatization(descrips, allowed_postags=None):
    if allowed_postags is None:
        allowed_postags = ['NOUN', 'ADJ', 'VERB',
                           'ADV']
    lemmatized_descrips = []
    for descrip in descrips:
        doc = nlp("" "".join(descrip))
        lemmatized_descrips.append([
            token.lemma_ for token in doc if token.pos_ in allowed_postags
        ])
    return lemmatized_descrips

lemmatized = lemmatization(tokenized)

print(lemmatized)
</code></pre>

<p>As you will notice, ""machine_learne"" is found nowhere in the input <code>tokenized</code>, but both ""machine_learning"" and ""machine_learne"" are found in the output <code>lemmatized</code>.</p>

<p>What is the cause of this and can I expect it to cause issues with other bigrams/trigrams?</p>
"
"57857240","Ho to do lemmatization on German text?","2019-09-09 15:43:53","7","14893","0","2","","57860929","<p>I have a German text that I want to apply lemmatization to. If lemmatization is not possible, then I can live with stemming too.</p>
<p><strong>Data:</strong> This is my German text:</p>
<pre><code>mails=['Hallo. Ich spielte am frühen Morgen und ging dann zu einem Freund. Auf Wiedersehen', 'Guten Tag Ich mochte Bälle und will etwas kaufen. Tschüss']
</code></pre>
<p><strong>Goal:</strong> After applying lemmatization it should look similar to this:</p>
<pre><code>mails_lemma=['Hallo. Ich spielen am früh Morgen und gehen dann zu einer Freund. Auf Wiedersehen', 'Guten Tag Ich mögen Ball und wollen etwas kaufen Tschüss']
</code></pre>
<p>I tried using spacy</p>
<blockquote>
<p>conda install -c conda-forge spacy</p>
<p>python -m spacy download de_core_news_md</p>
</blockquote>
<pre><code>import spacy
from spacy.lemmatizer import Lemmatizer
lemmatizer = Lemmatizer()
[lemmatizer.lookup(word) for word in mails]
</code></pre>
<p>I see following problems.</p>
<ol>
<li><p>My data is structured in sentences and not single words</p>
</li>
<li><p>In my case spacy lemmatization doesn't seem to work even for single words.</p>
</li>
</ol>
<p>Can you please tell me how this works?</p>
"
"57813864","Separate of nouns and groups of noun tag using nltk from json file","2019-09-05 22:58:40","0","126","0","1","","57821149","<p>I want to find or separate noun and groups of nouns using NLTK from JSON file, this is the JSON file content:</p>

<pre><code>[
  {
    ""id"": 18009,
    ""ingredients"": [
      ""baking powder"",
      ""eggs"",
      ""all-purpose flour"",
      ""raisins"",
      ""milk"",
      ""white sugar""
    ]
  },
  {
    ""id"": 28583,
    ""ingredients"": [
      ""sugar"",
      ""egg yolks"",
      ""corn starch"",
      ""cream of tartar"",
      ""bananas"",
      ""vanilla wafers"",
      ""milk"",
      ""vanilla extract"",
      ""toasted pecans"",
      ""egg whites"",
      ""light rum""
    ]
  },
</code></pre>

<p>I want to find the <code>NN</code>, <code>NNS</code>, <code>NNP</code>, <code>NNPS</code>.</p>
"
"57779549","Converting Spacy generated dependency into CoNLL format cannot handle more than one ROOT?","2019-09-03 22:50:13","2","847","0","1","","57784032","<p>I used the SpaCy library to generate dependencies and save it into a CoNLL format using the code below. </p>

<pre><code>import pandas as pd
import spacy

df1 = pd.read_csv('cleantweets', encoding='latin1')
df1['tweet'] = df1['tweet'].astype(str)
tweet_list = df1['tweet'].values.tolist()
nlp = spacy.load(""en_core_web_sm"")
for i in tweet_list:
    doc = nlp(i)
    for sent in doc.sents:
        print('\n')
        for i, word in enumerate(sent):
            if word.head is word:
                head_idx = 0
            else:
                 head_idx = doc[i].head.i + 1
            print(""%d\t%s\t%d\t%s\t%s\t%s"" % (
                i+1, 
                word.head,
                head_idx,
                word.text,
                word.dep_,
                word.pos_, 
                ))
</code></pre>

<p>This works, but there are some sentences in my dataset that get splits into two by Spacy because they have two ROOTS. This results in having two fields for one sentence in the CoNLL format. </p>

<p>Example: A random sentence from my dataset is : ""teanna trump probably cleaner twitter hoe but""</p>

<p>in CoNLL format it is saved as :</p>

<pre><code>    1   trump   2   teanna      compound
    2   cleaner 4   trump       nsubj
    3   cleaner 4   probably    advmod
    4   cleaner 4   cleaner     ROOT
    5   hoe     6   twitter     amod
    6   cleaner 4   hoe         dobj


    1   but 2   but ROOT
</code></pre>

<p>Is there a way to save it all in one field instead of two even though it has two ROOTS so that 'but' becomes 7th item in field number 1? Which means it would look like this instead</p>

<pre><code>    1   trump   2   teanna      compound
    2   cleaner 4   trump       nsubj
    3   cleaner 4   probably    advmod
    4   cleaner 4   cleaner     ROOT
    5   hoe     6   twitter     amod
    6   cleaner 4   hoe         dobj
    7   but     2   but         ROOT
</code></pre>
"
"57749696","Implementing a TF-IDF Vectorizer from Scratch","2019-09-01 21:41:41","0","2643","0","1","","57749956","<p>I am trying to implement a tf-idf vectorizer from scratch in Python. I computed my TDF values but the values do not match with the TDF values computed using sklearn's TfidfVectorizer().</p>

<p>What am I doing wrong?</p>

<pre><code>corpus = [
 'this is the first document',
 'this document is the second document',
 'and this is the third one',
 'is this the first document',
]

from collections import Counter
from tqdm import tqdm
from scipy.sparse import csr_matrix
import math
import operator
from sklearn.preprocessing import normalize
import numpy

sentence = []
for i in range(len(corpus)):
sentence.append(corpus[i].split())

word_freq = {}   #calculate document frequency of a word
for i in range(len(sentence)):
    tokens = sentence[i]
    for w in tokens:
        try:
            word_freq[w].add(i)  #add the word as key 
        except:
            word_freq[w] = {i}  #if it exists already, do not add.

for i in word_freq:
    word_freq[i] = len(word_freq[i])  #Counting the number of times a word(key)is in the whole corpus thus giving us the frequency of that word.

def idf():
    idfDict = {}
    for word in word_freq:
        idfDict[word] = math.log(len(sentence) / word_freq[word])
    return idfDict
idfDict = idf()
</code></pre>

<p>expected output:
(output obtained using vectorizer.idf_)</p>

<pre><code>[1.91629073 1.22314355 1.51082562 1.         1.91629073 1.91629073 1.22314355 1.91629073 1.        ]
</code></pre>

<p>actual output:
(the values are the idf values of corresponding keys.</p>

<pre><code>{'and': 1.3862943611198906,
'document': 0.28768207245178085,
'first': 0.6931471805599453,
'is': 0.0,
'one': 1.3862943611198906,
'second': 1.3862943611198906,
'the': 0.0,
'third': 1.3862943611198906,
'this': 0.0
 }
</code></pre>
"
"57742004","Apply nltk.pos_tag to entire dataframe","2019-08-31 22:31:28","1","972","0","1","","57742053","<p>I have the following dataframe</p>

<pre><code>   0     1       2      3     4       5        6
0  i  love  eating  spicy  hand  pulled  noodles
1  i  also    like     to  game    alot         
</code></pre>

<p>I'd like to apply a function to create a new dataframe, but instead of the above words, the df will be populated with each words's part of speech tag.</p>

<p>I'm using <code>nltk.pos_tag</code>, and I did this <code>df.apply(nltk.pos_tag)</code>.</p>

<p>My expected output should look like this:</p>

<pre><code>   0    1    2    3    4    5    6
0  NN   NN   VB   JJ   NN   VB   NN
1  NN   DT   NN   NN   VB   DT   
</code></pre>

<p>However, I get <code>IndexError: ('string index out of range', 'occurred at index 6')</code></p>

<p>Also, I understand that nltk.pos_tag will return a tuple output in the format of: <code>('word', 'pos_tag')</code>. So some further manipulation may be required to only get the tag. Any suggestions on how to go about doing this efficiently? </p>

<hr>

<p><strong>Traceback:</strong></p>

<pre><code>Traceback (most recent call last):
  File ""PartsOfSpeech.py"", line 71, in &lt;module&gt;
    FilteredTrees = pos.run_pos(data.lower())
  File ""PartsOfSpeech.py"", line 59, in run_pos
    df = df.apply(pos_tag)
  File ""/anaconda3/envs/customer_sentiment/lib/python3.6/site-packages/pandas/core/frame.py"", line 6487, in apply
    return op.get_result()
  File ""/anaconda3/envs/customer_sentiment/lib/python3.6/site-packages/pandas/core/apply.py"", line 151, in get_result
    return self.apply_standard()
  File ""/anaconda3/envs/customer_sentiment/lib/python3.6/site-packages/pandas/core/apply.py"", line 257, in apply_standard
    self.apply_series_generator()
  File ""/anaconda3/envs/customer_sentiment/lib/python3.6/site-packages/pandas/core/apply.py"", line 286, in apply_series_generator
    results[i] = self.f(v)
  File ""/anaconda3/envs/customer_sentiment/lib/python3.6/site-packages/nltk/tag/__init__.py"", line 162, in pos_tag
    return _pos_tag(tokens, tagset, tagger, lang)
  File ""/anaconda3/envs/customer_sentiment/lib/python3.6/site-packages/nltk/tag/__init__.py"", line 119, in _pos_tag
    tagged_tokens = tagger.tag(tokens)
  File ""/anaconda3/envs/customer_sentiment/lib/python3.6/site-packages/nltk/tag/perceptron.py"", line 157, in tag
    context = self.START + [self.normalize(w) for w in tokens] + self.END
  File ""/anaconda3/envs/customer_sentiment/lib/python3.6/site-packages/nltk/tag/perceptron.py"", line 157, in &lt;listcomp&gt;
    context = self.START + [self.normalize(w) for w in tokens] + self.END
  File ""/anaconda3/envs/customer_sentiment/lib/python3.6/site-packages/nltk/tag/perceptron.py"", line 242, in normalize
    elif word[0].isdigit():
</code></pre>
"
"57713159","Checking the order of a list of tuples","2019-08-29 15:13:19","0","89","2","1","","57714185","<p>I have a list of tuples that are generated from a string using NLTK's PoS tagger. </p>

<p>I'm trying to find the the ""intent"" of a specific string in order to append it to a dataframe, so I need a way to generate a syntax/grammar rule.</p>

<pre><code>string = ""RED WHITE AND BLUE""

string_list = nltk.pos_tag(a.split())

string_list = [('RED', 'JJ'), ('WHITE', 'NNP'), ('AND', 'NNP'), ('BLUE', 'NNP')]
</code></pre>

<p>The strings vary in size, from 2-3 elements all the way to full on paragraphs (40-50+) so I'm wondering if there is a general form or rule that I can create in order to parse a sentence.</p>

<p>So if I want find a pattern in a list an example pseudocode output would be:</p>

<pre><code>string_pattern = ""I want to kill all the bad guys in the Halo Game""

pattern = ('I', 'PRP') + ('want', 'VBP') + ('to', 'TO') + ('kill:', 'JJ') + ('all', 'DT') + ('bad', 'JJ') + ('guys', 'NNS') + ('in', 'IN') + ('Halo', 'NN') + ('Game', 'NN')
</code></pre>

<p>Ideally I would be able to match part of the pattern in a tagged string, so it finds:</p>

<pre><code>('I', 'PRP') + ('want', 'VBP') + ('to', 'TO') + ('kill:', 'JJ')
</code></pre>

<p>but it doesn't need the rest, or vice versa it can find multiple examples of the pattern in the same string, in the event that the string is a paragraph. If anyone knows the best way to do this or knows a better alternative it would be really helpful!</p>
"
"57592503","Italian Stemmer alternative to Snowball","2019-08-21 13:12:24","4","1240","2","1","","57592922","<p>I'm trying to analyze the texts in Italian in R.
As you do in a textual analysis I have eliminated all the punctuation, special characters and Italian stopwords. 
But I have got a problem with Stemming: there is only one Italian stemmer (Snowball), but it is not very precise. </p>

<p>To do the stemming I used the <code>tm</code> library and in particular the <code>stemDocument</code> function and I also tried to use the <code>SnowballC</code> library and both lead to the same result.</p>

<pre><code>  stemDocument(content(myCorpus[[1]]),language = ""italian"")
</code></pre>

<p>The problem is that the resulting stemming is not very precise. Are there other more precise Italian stemmers?
or is there a way to implement the stemming, already present in the TM library, by adding new terms?</p>
"
"57539043","how to modify Wordnet Lemmatizer to lemmitize specific words?","2019-08-17 18:47:56","1","2894","3","2","","57599728","<p>I am applying wordNet lemmatizer into my corpus and I need to define the pos tagger for lemmatizer:</p>

<pre><code>stemmer = PorterStemmer()
def lemmitize(document):
    return stemmer.stem(WordNetLemmatizer().lemmatize(document, pos='v'))

def preprocess(document):
output = []
    for token in gensim.utils.simple_preprocess(document):
        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) &gt; 3:
            print(""lemmitize: "", lemmitize(token))
            output.append(lemmitize(token))
    return output
</code></pre>

<p>Now as you can see I am defining pos for verb (and I know wordNet default pos is a noun), however when I lemmatized my document:</p>

<pre><code>the left door closed at the night  
</code></pre>

<p>I am getting out put as: </p>

<pre><code>output:  ['leav', 'door', 'close', 'night']
</code></pre>

<p>which this is not what i was expecting. In my above sentences, <code>left</code> points to which door (e.g. right or left). If I choose <code>pos ='n'</code> this problem may solve but it will then act as a wornNet default and there will be no effects on words like <code>taken</code>. </p>

<p>I found a similar issue in <a href=""https://stackoverflow.com/questions/22999273/python-nltk-lemmatization-of-the-word-further-with-wordnet"">here</a> and I modified the exception list in <code>nltk_data/corpora/wordnet/verb.exc</code> and I changed <code>left leave</code> to <code>left left</code> but still, I am getting the same results as <code>leav</code>.<br>
Now I am wondering if there is any solution to this problem or in the best case, is there any way that I can add a custom dictionary of some words (only limited to my document) that wordNet does not lemmatize them like:</p>

<pre><code>my_dict_list = [left, ...]
</code></pre>
"
"57360747","POS tagging a single word in spaCy","2019-08-05 14:33:11","1","2303","1","2","","57365644","<p>spaCy POS tagger is usally used on entire sentences. Is there a way to efficiently apply a unigram POS tagging to a single word (or a list of single words)?</p>

<p>Something like this:</p>

<pre><code>words = [""apple"", ""eat"", good""]
tags = get_tags(words) 
print(tags)
&gt; [""NNP"", ""VB"", ""JJ""]
</code></pre>

<p>Thanks.</p>
"
"57346347","How to get accuracy of an expanded query (User input an query which is expanded for better IR)?","2019-08-04 11:56:26","0","93","0","1","","57347698","<p>Using an algorithm , I am taking an input user query and expanding it. Now I need to test accuracy for my algorithm that is I want to get accuracy ( precision and recall ) for my expanded query ?</p>

<p>I have used terrier and taking a Trec Dataset(having collection of documents), 
I took a random query and retrieved relevant documents using terrier, then I used my algorithm to get expanded query for the random query and retrieved relevant documents.</p>

<p>But I dont know how to get precision and recall using this method. </p>

<p>So how do i get accuracy for my expanded query ?</p>

<p>If any other tool can be used ?</p>

<p>Thanks</p>
"
"57293069","Escape parentheses in NLTK parse tree","2019-07-31 14:24:16","0","766","0","2","","57343183","<p>In NLTK we can convert a parentheses tree into an actual Tree object. However, when a token contains parentheses, the parsing is not what you would expect since NLTK parses those parentheses as a new node.</p>

<p>As an example, take the sentence</p>

<blockquote>
  <p>They like(d) it a lot</p>
</blockquote>

<p>This could be parsed as </p>

<pre><code>(S (NP (PRP They)) (VP like(d) (NP (PRP it)) (NP (DT a) (NN lot))) (. .))
</code></pre>

<p>But if you parse this with NLTK into a tree, and output it - it is clear that the <code>(d)</code> is parsed as a new node, which is no surprise.</p>

<pre class=""lang-py prettyprint-override""><code>from nltk import Tree

s = '(S (NP (PRP They)) (VP like(d) (NP (PRP it)) (NP (DT a) (NN lot))) (. .))'

tree = Tree.fromstring(s)
print(tree)
</code></pre>

<p>The result is</p>

<pre><code>(S
  (NP (PRP They))
  (VP like (d ) (NP (PRP it)) (NP (DT a) (NN lot)))
  (. .))
</code></pre>

<p>So <code>(d )</code> is a node inside the VP rather than part of the token <code>like</code>. Is there a way in the tree parser to escape parentheses?</p>
"
"57193665","How can I see TF-IDF values from tfidf_vectorizer?","2019-07-25 02:53:14","0","479","0","1","","57193799","<p>I am using Python</p>

<p>I have this code that analyse Text documents</p>

<pre><code>tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=10000)


# split dataset into training and validation set
xtrain, xval, ytrain, yval = train_test_split(movies_new['clean_plot'], y, test_size=0.2, random_state=9)


# create TF-IDF features
xtrain_tfidf = tfidf_vectorizer.fit_transform(xtrain)
xval_tfidf = tfidf_vectorizer.transform(xval)
</code></pre>

<p>I know that TF-IDF assigns a value to each word.</p>

<p>Is there a way that let me see what are the values of inside <code>xtrain_tfidf</code> ?</p>
"
"57128766","Using nlp.pipe() with pre-segmented and pre-tokenized text with spaCy","2019-07-20 21:34:53","2","2407","0","4","","57134499","<p>I am trying to tag and parse text that has already been split up in sentences and has already been tokenized. As an example:</p>

<pre class=""lang-py prettyprint-override""><code>sents = [['I', 'like', 'cookies', '.'], ['Do', 'you', '?']]
</code></pre>

<p>The fastest approach to process batches of text is <code>.pipe()</code>. However, it is not clear to me how I can use that with pre-tokenized, and pre-segmented text. Performance is key here. I tried the following, but that threw an error</p>

<pre class=""lang-py prettyprint-override""><code>docs = [nlp.tokenizer.tokens_from_list(sentence) for sentence in sents]
nlp.tagger(docs)
nlp.parser(docs)
</code></pre>

<p>Trace:</p>

<pre><code>Traceback (most recent call last):
  File ""C:\Python\Python37\Lib\multiprocessing\pool.py"", line 121, in worker
    result = (True, func(*args, **kwds))
  File ""C:\Python\projects\PreDicT\predicting-wte\build_id_dictionary.py"", line 204, in process_batch
    self.nlp.tagger(docs)
  File ""pipes.pyx"", line 377, in spacy.pipeline.pipes.Tagger.__call__
  File ""pipes.pyx"", line 396, in spacy.pipeline.pipes.Tagger.predict
  File ""C:\Users\bmvroy\.virtualenvs\predicting-wte-YKqW76ba\lib\site-packages\thinc\neural\_classes\model.py"", line 169, in __call__
    return self.predict(x)
  File ""C:\Users\bmvroy\.virtualenvs\predicting-wte-YKqW76ba\lib\site-packages\thinc\neural\_classes\feed_forward.py"", line 40, in predict
    X = layer(X)
  File ""C:\Users\bmvroy\.virtualenvs\predicting-wte-YKqW76ba\lib\site-packages\thinc\neural\_classes\model.py"", line 169, in __call__
    return self.predict(x)
  File ""C:\Users\bmvroy\.virtualenvs\predicting-wte-YKqW76ba\lib\site-packages\thinc\neural\_classes\model.py"", line 133, in predict
    y, _ = self.begin_update(X, drop=None)
  File ""C:\Users\bmvroy\.virtualenvs\predicting-wte-YKqW76ba\lib\site-packages\thinc\neural\_classes\feature_extracter.py"", line 14, in begin_update
    features = [self._get_feats(doc) for doc in docs]
  File ""C:\Users\bmvroy\.virtualenvs\predicting-wte-YKqW76ba\lib\site-packages\thinc\neural\_classes\feature_extracter.py"", line 14, in &lt;listcomp&gt;
    features = [self._get_feats(doc) for doc in docs]
  File ""C:\Users\bmvroy\.virtualenvs\predicting-wte-YKqW76ba\lib\site-packages\thinc\neural\_classes\feature_extracter.py"", line 21, in _get_feats
    arr = doc.doc.to_array(self.attrs)[doc.start : doc.end]
AttributeError: 'list' object has no attribute 'doc'
</code></pre>
"
"57057992","Wordpiece tokenization versus conventional lemmatization?","2019-07-16 13:07:37","7","1626","0","1","","57072351","<p>I'm looking at NLP preprocessing. At some point I want to implement a context-sensitive word embedding, as a way of discerning word sense, and I was thinking about using the output from BERT to do so. I noticed BERT uses WordPiece tokenization (for example, ""playing"" -> ""play"" + ""##ing"").</p>

<p>Right now, I have my text preprocessed using a standard tokenizer that splits on spaces / some punctuation, and then I have a lemmatizer (""playing"" ->""play""). I'm wondering what the benefit of WordPiece tokenization is over a standard tokenization + lemmatization. I know WordPiece helps with out of vocabulary words, but is there anything else? That is, even if I don't end up using BERT, should I consider replacing my tokenizer + lemmatizer with wordpiece tokenization? In what situations would that be useful?</p>
"
"57026011","Does PorterStemmer supports languages other than english?","2019-07-14 09:11:30","0","1371","0","1","","57284540","<p>Snowball stemmer supports many languages other than english, but does porter also?</p>
"
"57009676","Python, Keras - Binary text classifier prediction results in array of values instead of single probability","2019-07-12 15:08:27","0","713","0","3","","57013292","<p>I am building a very simple DNN binary model which I define as:</p>

<pre><code>def __build_model(self, vocabulary_size):
    model = Sequential()
    model.add(Embedding(vocabulary_size, 12, input_length=vocabulary_size))
    model.add(Flatten())
    model.add(Dense(16, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))

    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])
    return model
</code></pre>

<p>with training like:</p>

<pre><code>def __train_model(self, model, model_data, training_data, labels):
    hist = model.fit(training_data, labels, epochs=20, verbose=True, validation_split=0.2)

    model.save('models/' + model_data['Key'] + '.h5')

    return model
</code></pre>

<p>The idea is to feed tfidf vectorized text after training and predict whenever it belongs to class 1 or 0. Sadly when I run predict against it, I get an array of predictions instead of expected 1 probability for the article belonging to class 1. The array values seem very uniform. I assume this comes from some mistake in the model. I try popping prediction like so:</p>

<pre><code>            self._tokenizer.fit_on_texts(asset_article_data.content)

            predicted_post_vector = self._tokenizer.texts_to_matrix(post, mode='tfidf')

            return model.predict(predicted_post_vector) &gt; 0.60 // here return array instead of true/false
</code></pre>

<p>The training data is vectorized text itself. What might be off?</p>
"
"57008528","How to perform NER on true case, then lemmatization on lower case, with spaCy","2019-07-12 13:59:23","0","954","0","1","","57017151","<p>I try to lemmatize a text using spaCy 2.0.12 with the French model <code>fr_core_news_sm</code>. Morevoer, I want to replace people names by an arbitrary sequence of characters, detecting such names using <code>token.ent_type_ == 'PER'</code>. Example outcome would be ""Pierre aime les chiens"" -> ""~PER~ aimer chien"".</p>

<p>The problem is I can't find a way to do both. I only have these two partial options:</p>

<ul>
<li>I can feed the pipeline with the original text: <code>doc = nlp(text)</code>. Then, the NER will recognize most people names but the lemmas of words starting with a capital won't be correct. For example, the lemmas of the simple question ""Pouvons-nous faire ça?"" would be <code>['Pouvons', '-', 'se', 'faire', 'ça', '?']</code>, where ""Pouvons"" is still an inflected form.</li>
<li>I can feed the pipeline with the lower case text: <code>doc = nlp(text.lower())</code>. Then my previous example would correctly display <code>['pouvoir', '-', 'se', 'faire', 'ça', '?']</code>, but most people names wouldn't be recognized as entities by the NER, as I guess a starting capital is a useful indicator for finding entities.</li>
</ul>

<p>My idea would be to perform the standard pipeline (tagger, parser, NER), then lowercase, and then lemmatize only at the end.</p>

<p>However, lemmatization doesn't seem to have its own pipeline component and the documentation doesn't explain how and where it is performed. <a href=""https://stackoverflow.com/a/52560132/11774841"">This</a> answer seem to imply that lemmatization is performed independent of any pipeline component and possibly at different stages of it. </p>

<p>So my question is: how to choose when to perform the lemmatization and which input to give to it?</p>
"
"56978661","Is there a way to reverse stem in python nltk?","2019-07-10 21:07:36","4","2650","0","1","","56979424","<p>I have a list of stems in NLTK/python and want to get the possible words that create that stem.</p>

<p>Is there a way to take a stem and get a list of words that will stem to it in python?</p>
"
"56914017","how to view tf-idf score against each word","2019-07-06 12:16:50","1","3038","0","2","","56914811","<p>I was trying to know the <code>tf-idf</code> scores of each word in my document. However, it only returns values in the matrix but I see a specific type of representation of <code>tf-idf</code> scores against each word.</p>

<p>I have used processed and the code works however I want to change the way it is presented:</p>

<p>code:</p>

<pre><code>from sklearn.feature_extraction.text import CountVectorizer 
from sklearn.feature_extraction.text import TfidfTransformer

bow_transformer = CountVectorizer(analyzer=text_process).fit(df[""comments""].head())
print(len(bow_transformer.vocabulary_))

tfidf_transformer = CountVectorizer(analyzer=text_process).fit(messages['message'])
bow_transformer.vocabulary_transformer().fit(message_bow)

message_tfidf = tfidf_transformer.transform(message_bow)
</code></pre>

<p>I get the results like this <code>(39028,01),(1393,1672)</code>. However, I expect the results to be like</p>

<pre><code>features    tfidf
fruit       0.00344
excellent   0.00289
</code></pre>
"
"56884204","How to normalize TF*IDF or counts in scikit-learn?","2019-07-04 08:56:43","0","3279","4","1","","56914947","<p>I want to check the cosine similarity of two documents having varying length (say one is a one or two liner while other is of 100-200 lines).</p>

<p>I need a way to normalize tfidf or count vectorizer in scikit-learn for this.</p>
"
"56884020","spacy with joblib library generates _pickle.PicklingError: Could not pickle the task to send it to the workers","2019-07-04 08:48:29","26","39381","2","7","","56885830","<p>I have a large list of sentences (~7 millions), and I want to extract the nouns from them.</p>

<p>I used <strong><code>joblib</code></strong> library to parallelize the extracting process, like in the following:</p>

<pre><code>import spacy
from tqdm import tqdm
from joblib import Parallel, delayed
nlp = spacy.load('en_core_web_sm')

class nouns:

    def get_nouns(self, text):
        doc = nlp(u""{}"".format(text))
        return [token.text for token in doc if token.tag_ in ['NN', 'NNP', 'NNS', 'NNPS']]

    def parallelize(self, sentences):
        results = Parallel(n_jobs=1)(delayed(self.get_nouns)(sent) for sent in tqdm(sentences))
        return results

if __name__ == '__main__':
    sentences = ['we went to the school yesterday',
                 'The weather is really cold',
                 'Can we catch the dog?',
                 'How old are you John?',
                 'I like diving and swimming',
                 'Can the world become united?']
    obj = nouns()
    print(obj.parallelize(sentences))
</code></pre>

<p>when <code>n_jobs</code> in parallelize function is more than 1, I get this long error:</p>

<pre><code>100%|██████████| 6/6 [00:00&lt;00:00, 200.00it/s]
joblib.externals.loky.process_executor._RemoteTraceback: 
""""""
Traceback (most recent call last):
  File ""C:\Python35\lib\site-packages\joblib\externals\loky\backend\queues.py"", line 150, in _feed
    obj_ = dumps(obj, reducers=reducers)
  File ""C:\Python35\lib\site-packages\joblib\externals\loky\backend\reduction.py"", line 243, in dumps
    dump(obj, buf, reducers=reducers, protocol=protocol)
  File ""C:\Python35\lib\site-packages\joblib\externals\loky\backend\reduction.py"", line 236, in dump
    _LokyPickler(file, reducers=reducers, protocol=protocol).dump(obj)
  File ""C:\Python35\lib\site-packages\joblib\externals\cloudpickle\cloudpickle.py"", line 267, in dump
    return Pickler.dump(self, obj)
  File ""C:\Python35\lib\pickle.py"", line 408, in dump
    self.save(obj)
  File ""C:\Python35\lib\pickle.py"", line 520, in save
    self.save_reduce(obj=obj, *rv)
  File ""C:\Python35\lib\pickle.py"", line 623, in save_reduce
    save(state)
  File ""C:\Python35\lib\pickle.py"", line 475, in save
    f(self, obj) # Call unbound method with explicit self
  File ""C:\Python35\lib\pickle.py"", line 810, in save_dict
    self._batch_setitems(obj.items())
  File ""C:\Python35\lib\pickle.py"", line 836, in _batch_setitems
    save(v)
  File ""C:\Python35\lib\pickle.py"", line 520, in save
    self.save_reduce(obj=obj, *rv)
  File ""C:\Python35\lib\pickle.py"", line 623, in save_reduce
    save(state)
  File ""C:\Python35\lib\pickle.py"", line 475, in save
    f(self, obj) # Call unbound method with explicit self
  File ""C:\Python35\lib\pickle.py"", line 810, in save_dict
    self._batch_setitems(obj.items())
  File ""C:\Python35\lib\pickle.py"", line 841, in _batch_setitems
    save(v)
  File ""C:\Python35\lib\pickle.py"", line 520, in save
    self.save_reduce(obj=obj, *rv)
  File ""C:\Python35\lib\pickle.py"", line 623, in save_reduce
    save(state)
  File ""C:\Python35\lib\pickle.py"", line 475, in save
    f(self, obj) # Call unbound method with explicit self
  File ""C:\Python35\lib\pickle.py"", line 810, in save_dict
    self._batch_setitems(obj.items())
  File ""C:\Python35\lib\pickle.py"", line 836, in _batch_setitems
    save(v)
  File ""C:\Python35\lib\pickle.py"", line 475, in save
    f(self, obj) # Call unbound method with explicit self
  File ""C:\Python35\lib\pickle.py"", line 770, in save_list
    self._batch_appends(obj)
  File ""C:\Python35\lib\pickle.py"", line 797, in _batch_appends
    save(tmp[0])
  File ""C:\Python35\lib\pickle.py"", line 475, in save
    f(self, obj) # Call unbound method with explicit self
  File ""C:\Python35\lib\pickle.py"", line 725, in save_tuple
    save(element)
  File ""C:\Python35\lib\pickle.py"", line 475, in save
    f(self, obj) # Call unbound method with explicit self
  File ""C:\Python35\lib\site-packages\joblib\externals\cloudpickle\cloudpickle.py"", line 718, in save_instancemethod
    self.save_reduce(types.MethodType, (obj.__func__, obj.__self__), obj=obj)
  File ""C:\Python35\lib\pickle.py"", line 599, in save_reduce
    save(args)
  File ""C:\Python35\lib\pickle.py"", line 475, in save
    f(self, obj) # Call unbound method with explicit self
  File ""C:\Python35\lib\pickle.py"", line 725, in save_tuple
    save(element)
  File ""C:\Python35\lib\pickle.py"", line 475, in save
    f(self, obj) # Call unbound method with explicit self
  File ""C:\Python35\lib\site-packages\joblib\externals\cloudpickle\cloudpickle.py"", line 395, in save_function
    self.save_function_tuple(obj)
  File ""C:\Python35\lib\site-packages\joblib\externals\cloudpickle\cloudpickle.py"", line 594, in save_function_tuple
    save(state)
  File ""C:\Python35\lib\pickle.py"", line 475, in save
    f(self, obj) # Call unbound method with explicit self
  File ""C:\Python35\lib\pickle.py"", line 810, in save_dict
    self._batch_setitems(obj.items())
  File ""C:\Python35\lib\pickle.py"", line 836, in _batch_setitems
    save(v)
  File ""C:\Python35\lib\pickle.py"", line 475, in save
    f(self, obj) # Call unbound method with explicit self
  File ""C:\Python35\lib\pickle.py"", line 810, in save_dict
    self._batch_setitems(obj.items())
  File ""C:\Python35\lib\pickle.py"", line 841, in _batch_setitems
    save(v)
  File ""C:\Python35\lib\pickle.py"", line 520, in save
    self.save_reduce(obj=obj, *rv)
  File ""C:\Python35\lib\pickle.py"", line 623, in save_reduce
    save(state)
  File ""C:\Python35\lib\pickle.py"", line 475, in save
    f(self, obj) # Call unbound method with explicit self
  File ""C:\Python35\lib\pickle.py"", line 810, in save_dict
    self._batch_setitems(obj.items())
  File ""C:\Python35\lib\pickle.py"", line 836, in _batch_setitems
    save(v)
  File ""C:\Python35\lib\pickle.py"", line 520, in save
    self.save_reduce(obj=obj, *rv)
  File ""C:\Python35\lib\pickle.py"", line 599, in save_reduce
    save(args)
  File ""C:\Python35\lib\pickle.py"", line 475, in save
    f(self, obj) # Call unbound method with explicit self
  File ""C:\Python35\lib\pickle.py"", line 740, in save_tuple
    save(element)
  File ""C:\Python35\lib\pickle.py"", line 520, in save
    self.save_reduce(obj=obj, *rv)
  File ""C:\Python35\lib\pickle.py"", line 623, in save_reduce
    save(state)
  File ""C:\Python35\lib\pickle.py"", line 475, in save
    f(self, obj) # Call unbound method with explicit self
  File ""C:\Python35\lib\pickle.py"", line 740, in save_tuple
    save(element)
  File ""C:\Python35\lib\pickle.py"", line 495, in save
    rv = reduce(self.proto)
  File ""stringsource"", line 2, in preshed.maps.PreshMap.__reduce_cython__
TypeError: self.c_map cannot be converted to a Python object for pickling
""""""Exception in thread QueueFeederThread:
Traceback (most recent call last):
  File ""C:\Python35\lib\site-packages\joblib\externals\loky\backend\queues.py"", line 150, in _feed
    obj_ = dumps(obj, reducers=reducers)
  File ""C:\Python35\lib\site-packages\joblib\externals\loky\backend\reduction.py"", line 243, in dumps
    dump(obj, buf, reducers=reducers, protocol=protocol)
  File ""C:\Python35\lib\site-packages\joblib\externals\loky\backend\reduction.py"", line 236, in dump
    _LokyPickler(file, reducers=reducers, protocol=protocol).dump(obj)
  File ""C:\Python35\lib\site-packages\joblib\externals\cloudpickle\cloudpickle.py"", line 267, in dump
    return Pickler.dump(self, obj)
  File ""C:\Python35\lib\pickle.py"", line 408, in dump
    self.save(obj)
  File ""C:\Python35\lib\pickle.py"", line 520, in save
    self.save_reduce(obj=obj, *rv)
  File ""C:\Python35\lib\pickle.py"", line 623, in save_reduce
    save(state)
  File ""C:\Python35\lib\pickle.py"", line 475, in save
    f(self, obj) # Call unbound method with explicit self
  File ""C:\Python35\lib\pickle.py"", line 810, in save_dict
    self._batch_setitems(obj.items())
  File ""C:\Python35\lib\pickle.py"", line 836, in _batch_setitems
    save(v)
  File ""C:\Python35\lib\pickle.py"", line 520, in save
    self.save_reduce(obj=obj, *rv)
  File ""C:\Python35\lib\pickle.py"", line 623, in save_reduce
    save(state)
  File ""C:\Python35\lib\pickle.py"", line 475, in save
    f(self, obj) # Call unbound method with explicit self
  File ""C:\Python35\lib\pickle.py"", line 810, in save_dict
    self._batch_setitems(obj.items())
  File ""C:\Python35\lib\pickle.py"", line 841, in _batch_setitems
    save(v)
  File ""C:\Python35\lib\pickle.py"", line 520, in save
    self.save_reduce(obj=obj, *rv)
  File ""C:\Python35\lib\pickle.py"", line 623, in save_reduce
    save(state)
  File ""C:\Python35\lib\pickle.py"", line 475, in save
    f(self, obj) # Call unbound method with explicit self
  File ""C:\Python35\lib\pickle.py"", line 810, in save_dict
    self._batch_setitems(obj.items())
  File ""C:\Python35\lib\pickle.py"", line 836, in _batch_setitems
    save(v)
  File ""C:\Python35\lib\pickle.py"", line 475, in save
    f(self, obj) # Call unbound method with explicit self
  File ""C:\Python35\lib\pickle.py"", line 770, in save_list
    self._batch_appends(obj)
  File ""C:\Python35\lib\pickle.py"", line 797, in _batch_appends
    save(tmp[0])
  File ""C:\Python35\lib\pickle.py"", line 475, in save
    f(self, obj) # Call unbound method with explicit self
  File ""C:\Python35\lib\pickle.py"", line 725, in save_tuple
    save(element)
  File ""C:\Python35\lib\pickle.py"", line 475, in save
    f(self, obj) # Call unbound method with explicit self
  File ""C:\Python35\lib\site-packages\joblib\externals\cloudpickle\cloudpickle.py"", line 718, in save_instancemethod
    self.save_reduce(types.MethodType, (obj.__func__, obj.__self__), obj=obj)
  File ""C:\Python35\lib\pickle.py"", line 599, in save_reduce
    save(args)
  File ""C:\Python35\lib\pickle.py"", line 475, in save
    f(self, obj) # Call unbound method with explicit self
  File ""C:\Python35\lib\pickle.py"", line 725, in save_tuple
    save(element)
  File ""C:\Python35\lib\pickle.py"", line 475, in save
    f(self, obj) # Call unbound method with explicit self
  File ""C:\Python35\lib\site-packages\joblib\externals\cloudpickle\cloudpickle.py"", line 395, in save_function
    self.save_function_tuple(obj)
  File ""C:\Python35\lib\site-packages\joblib\externals\cloudpickle\cloudpickle.py"", line 594, in save_function_tuple
    save(state)
  File ""C:\Python35\lib\pickle.py"", line 475, in save
    f(self, obj) # Call unbound method with explicit self
  File ""C:\Python35\lib\pickle.py"", line 810, in save_dict
    self._batch_setitems(obj.items())
  File ""C:\Python35\lib\pickle.py"", line 836, in _batch_setitems
    save(v)
  File ""C:\Python35\lib\pickle.py"", line 475, in save
    f(self, obj) # Call unbound method with explicit self
  File ""C:\Python35\lib\pickle.py"", line 810, in save_dict
    self._batch_setitems(obj.items())
  File ""C:\Python35\lib\pickle.py"", line 841, in _batch_setitems
    save(v)
  File ""C:\Python35\lib\pickle.py"", line 520, in save
    self.save_reduce(obj=obj, *rv)
  File ""C:\Python35\lib\pickle.py"", line 623, in save_reduce
    save(state)
  File ""C:\Python35\lib\pickle.py"", line 475, in save
    f(self, obj) # Call unbound method with explicit self
  File ""C:\Python35\lib\pickle.py"", line 810, in save_dict
    self._batch_setitems(obj.items())
  File ""C:\Python35\lib\pickle.py"", line 836, in _batch_setitems
    save(v)
  File ""C:\Python35\lib\pickle.py"", line 520, in save
    self.save_reduce(obj=obj, *rv)
  File ""C:\Python35\lib\pickle.py"", line 599, in save_reduce
    save(args)
  File ""C:\Python35\lib\pickle.py"", line 475, in save
    f(self, obj) # Call unbound method with explicit self
  File ""C:\Python35\lib\pickle.py"", line 740, in save_tuple
    save(element)
  File ""C:\Python35\lib\pickle.py"", line 520, in save
    self.save_reduce(obj=obj, *rv)
  File ""C:\Python35\lib\pickle.py"", line 623, in save_reduce
    save(state)
  File ""C:\Python35\lib\pickle.py"", line 475, in save
    f(self, obj) # Call unbound method with explicit self
  File ""C:\Python35\lib\pickle.py"", line 740, in save_tuple
    save(element)
  File ""C:\Python35\lib\pickle.py"", line 495, in save
    rv = reduce(self.proto)
  File ""stringsource"", line 2, in preshed.maps.PreshMap.__reduce_cython__
TypeError: self.c_map cannot be converted to a Python object for pickling

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Python35\lib\threading.py"", line 914, in _bootstrap_inner
    self.run()
  File ""C:\Python35\lib\threading.py"", line 862, in run
    self._target(*self._args, **self._kwargs)
  File ""C:\Python35\lib\site-packages\joblib\externals\loky\backend\queues.py"", line 175, in _feed
    onerror(e, obj)
  File ""C:\Python35\lib\site-packages\joblib\externals\loky\process_executor.py"", line 310, in _on_queue_feeder_error
    self.thread_wakeup.wakeup()
  File ""C:\Python35\lib\site-packages\joblib\externals\loky\process_executor.py"", line 155, in wakeup
    self._writer.send_bytes(b"""")
  File ""C:\Python35\lib\multiprocessing\connection.py"", line 183, in send_bytes
    self._check_closed()
  File ""C:\Python35\lib\multiprocessing\connection.py"", line 136, in _check_closed
    raise OSError(""handle is closed"")
OSError: handle is closed



The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "".../playground.py"", line 43, in &lt;module&gt;
    print(obj.Paralize(sentences))
  File "".../playground.py"", line 32, in Paralize
    results = Parallel(n_jobs=2)(delayed(self.get_nouns)(sent) for sent in tqdm(sentences))
  File ""C:\Python35\lib\site-packages\joblib\parallel.py"", line 934, in __call__
    self.retrieve()
  File ""C:\Python35\lib\site-packages\joblib\parallel.py"", line 833, in retrieve
    self._output.extend(job.get(timeout=self.timeout))
  File ""C:\Python35\lib\site-packages\joblib\_parallel_backends.py"", line 521, in wrap_future_result
    return future.result(timeout=timeout)
  File ""C:\Python35\lib\concurrent\futures\_base.py"", line 405, in result
    return self.__get_result()
  File ""C:\Python35\lib\concurrent\futures\_base.py"", line 357, in __get_result
    raise self._exception
_pickle.PicklingError: Could not pickle the task to send it to the workers.
</code></pre>

<p>What is the problem in my code?</p>
"
"56870824","Create a matrix from a dict of dicts for calculating similarities between docs","2019-07-03 13:04:56","0","42","0","1","","56872225","<p>Here is my problem:</p>

<p>I have a dataframe like this:</p>

<pre><code>id   tfidf_weights   
1    {word1: 0.01, word2: 0.01, word3: 0.01, ...}
2    {word4: 0.01, word5: 0.01, word6: 0.01, ...}
3    {word7: 0.01, word8: 0.01, word9: 0.01, ...}
4    {word10: 0.01, word11: 0.01, word12: 0.01, ...}
5    {word13: 0.01, word14: 0.01, word15: 0.01, ...}    
.
.
.
</code></pre>

<p>column 'id' represent the ids of the docs and 'tfidf_weights' the tfidf weight for each word of each docs.</p>

<p>from this dataframe, i can obtain a dict with the following structure:</p>

<pre><code>mydict = {1:{word1: 0.01, word2: 0.01, word3: 0.01, ...}, 2:{word4: 0.01, word5: 0.01, word6: 0.01, ...}, 3:{word7: 0.01, word8: 0.01, word9: 0.01, ...}, 4:{word10: 0.01, word11: 0.01, word12: 0.01, ...}, 5:{word13: 0.01, word14: 0.01, word15: 0.01, ...}, ...}
</code></pre>

<p>what i want to do is, from this dictionary, obtain a matrix like this:</p>

<pre><code>      word1     word2     word3     word4   ...
1     0.01      0.01      0.01      0.01     
2     0.01      0.01      0.01      0.01
3     0.01      0.01      0.01      0.01
4     0.01      0.01      0.01      0.01
5     0.01      0.01      0.01      0.01
.
.
.
</code></pre>

<p>Thank you for your help !</p>
"
"56841777","python spacy look for chunks backwards (before a reference)","2019-07-01 20:02:04","0","413","0","1","","56852257","<p>I am using spacy for a NLP project.
when creating a doc with Spacy you can find out the noun chunks in the text (also known as ""noun phrases"") in the following way:</p>

<pre><code>import spacy
nlp = spacy.load(""en_core_web_sm"")
doc = nlp(u""The companies building cars do not want to spend more money in improving diesel engines because the government will not subsidise such engines anymore."")
for chunk in doc.noun_chunks:
    print(chunk.text)
</code></pre>

<p>This will give a list of the noun phrases.</p>

<p>In this case for instance the first noun phrase is ""The companies"".</p>

<p>Suppose you have a text where noun chunks are referenced with a number.</p>

<p>like:</p>

<pre><code>doc=nlp(the Window (23) is closed because the wall (34) of the beautiful building (45) is not covered by the insurance (45))
</code></pre>

<p>assume I have the code to identify the references for instance tagging them:</p>

<pre><code>myprocessedtext=the Window &lt;ref&gt;(23)&lt;/ref&gt; is closed because the wall &lt;ref&gt;(34)&lt;/ref&gt; of the beautiful building &lt;ref&gt;(45)&lt;/ref&gt; is not covered by the insurance &lt;ref&gt;(45)&lt;/ref&gt;
</code></pre>

<p>How could I get the noun chunks (noun phrases) immediately preceding the references?</p>

<p>my idea: passing the 10 words preceding every reference to a spacy doc object, extract the noun chunks and getting the last one. This is highly inefficient since creating the doc objects is very high time consuming.</p>

<p>Any other idea without having to create extra nlp objects?</p>

<p>thanks.</p>
"
"56754251","Correct multithreaded lemmatization using spaCy","2019-06-25 12:36:58","1","1492","1","1","","56759895","<p>I'm trying to multithread the lemmatization of my corpus using spaCy. Following the <a href=""https://spacy.io/usage/processing-pipelines#section-multithreading"" rel=""nofollow noreferrer"">documentation</a>, this is currently my approach:</p>

<pre><code>import spacy
nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner', 'tagger'])

def lemmatize():
    for doc in nlp.pipe(corpus, batch_size=2, n_threads=10):
        yield ' '.join([token.lemma_ for token in doc])

new_corpus = list(lemmatize())
</code></pre>

<p>However, this takes the same amount of time regardless when using 10 or 1 thread (I use it on 100.000 documents), suggesting that it is not multithreaded. </p>

<p>Is my implementation wrong?</p>
"
"56728218","How to mock spacy models / Doc objects for unit tests?","2019-06-23 22:43:20","12","2093","0","1","","56733926","<p>Loading spacy models slows down running my unit tests. Is there a way to mock spacy models or Doc objects to speed up unit tests?</p>

<p><strong>Example of a current slow tests</strong></p>

<pre><code>import spacy
nlp = spacy.load(""en_core_web_sm"")

def test_entities():
    text = u""Google is a company.""
    doc = nlp(text)
    assert doc.ents[0].text == u""Google""
</code></pre>

<p><strong>Based on the docs my approach is</strong></p>

<p>Constructing the Vocab and Doc manually and setting the entities as tuples.</p>

<pre><code>from spacy.vocab import Vocab
from spacy.tokens import Doc

def test()
    alphanum_words = u""Google Facebook are companies"".split("" "")
    labels = [u""ORG""]
    words = alphanum_words + [u"".""]
    spaces = len(words) * [True]
    spaces[-1] = False
    spaces[-2] = False
    vocab = Vocab(strings=(alphanum_words + labels))
    doc = Doc(vocab, words=words, spaces=spaces)

    def get_hash(text):
        return vocab.strings[text]

    entity_tuples = tuple([(get_hash(labels[0]), 0, 1)])
    doc.ents = entity_tuples
    assert doc.ents[0].text == u""Google""

</code></pre>

<p>Is there a cleaner more Pythonic solution for mocking spacy objects for unit tests for entities?</p>
"
"56721753","AttributeError: 'list' object has no attribute 'lower' from Tfidf_vect.fit","2019-06-23 07:03:24","0","1560","6","1","","56726890","<p>I'm trying to apply SVM using tf-idf features .
but I got this error:</p>

<pre><code>Traceback (most recent call last):
  File ""&lt;input&gt;"", line 1, in &lt;module&gt;
  File ""C:\Program Files\JetBrains\PyCharm 2019.1.3\helpers\pydev\_pydev_bundle\pydev_umd.py"", line 197, in runfile
    pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script
  File ""C:\Program Files\JetBrains\PyCharm 2019.1.3\helpers\pydev\_pydev_imps\_pydev_execfile.py"", line 18, in execfile
    exec(compile(contents+""\n"", file, 'exec'), glob, loc)
  File ""C:/Users/lam/.PyCharm2019.1/config/scratches/scratch_1.py"", line 35, in &lt;module&gt;
    Tfidf_vect.fit(data['input'])
  File ""C:\Users\lam\PycharmProjects\untitled\venv\lib\site-packages\sklearn\feature_extraction\text.py"", line 1631, in fit
    X = super().fit_transform(raw_documents)
  File ""C:\Users\lam\PycharmProjects\untitled\venv\lib\site-packages\sklearn\feature_extraction\text.py"", line 1058, in fit_transform
    self.fixed_vocabulary_)
  File ""C:\Users\lam\PycharmProjects\untitled\venv\lib\site-packages\sklearn\feature_extraction\text.py"", line 970, in _count_vocab
    for feature in analyze(doc):
  File ""C:\Users\lam\PycharmProjects\untitled\venv\lib\site-packages\sklearn\feature_extraction\text.py"", line 352, in &lt;lambda&gt;
    tokenize(preprocess(self.decode(doc))), stop_words)
  File ""C:\Users\lam\PycharmProjects\untitled\venv\lib\site-packages\sklearn\feature_extraction\text.py"", line 256, in &lt;lambda&gt;
    return lambda x: strip_accents(x.lower())
AttributeError: 'list' object has no attribute 'lower'
</code></pre>

<p>This is my code:</p>

<pre><code>data['input']= [nltk.word_tokenize(entry) for entry in data['input']]

Train_X, Test_X, Train_Y, Test_Y = sklearn.model_selection.train_test_split(data['input'],data['Class'],test_size=0.2)

Encoder = LabelEncoder()
Train_Y = Encoder.fit_transform(Train_Y)
Test_Y = Encoder.fit_transform(Test_Y)

Tfidf_vect = TfidfVectorizer()
Tfidf_vect.fit(data['input'])


Train_X_Tfidf = Tfidf_vect.transform(Train_X)
Test_X_Tfidf = Tfidf_vect.transform(Test_X)

print(Tfidf_vect.vocabulary_)
</code></pre>

<p>I'm using python 3.6.0, my dataset is in Arabic.</p>

<p>Thank you,,</p>
"
"56713358","How can I reduce time for filtering my article dataset?","2019-06-22 07:03:42","1","93","2","1","","57547113","<p>I'm trying to filter my dataset which contains nearly 50K articles. From each article I want to filter out stop words and punctuation. But the process is taking long time. I've already filtered the dataset and it took 6 hours. Now I've got another dataset to filter which contains 300K articles.</p>

<p>I'm using python in anaconda environment. PC configuration: 7th Gen. Core i5, 8GB RAM and NVIDIA 940MX GPU. To filter my dataset I've wrote a code which takes each article in dataset, tokenize words and then remove stop words, punctuations and numbers. </p>

<pre class=""lang-py prettyprint-override""><code>def sentence_to_wordlist(sentence, filters=""!\""#$%&amp;()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~\t\n?,।!‍.'0123456789০১২৩৪৫৬৭৮৯‘\u200c–“”…‘""):
    translate_dict = dict((c, ' ') for c in filters)
    translate_map = str.maketrans(translate_dict)
    wordlist = sentence.translate(translate_map).split()
    global c,x;
    return list(filter(lambda x: x not in stops, wordlist))

</code></pre>

<p>Now I want to reduce the time for this process. Is there any way to optimize this?</p>
"
"56689311","How to combine TF-IDF scores to be the equivalent of concatenating two strings","2019-06-20 15:27:55","0","943","0","1","","56689469","<p>I have a corpus of 5000 book titles and I am trying to perform some clustering on these. I am using the sklearn TfidfVectorizer library to generate the TF-IDF matrix for each title.</p>

<p>However, I now combine two of the titles (so ""Book A"" and ""Book B"" becomes ""Book A Book B"") and I am wondering if there is a way of getting the TF-IDF matrix for ""Book A Book B"" by combining the matrix for ""Book A"" and the matrix for ""Book B"".</p>

<p>I have tried recalculating the TF-IDF score again but this can take a lot of time and I would prefer if there was a quicker way of doing it since I actually need to do this several thousand times for different combinations of the titles.</p>

<p>The code below shows what I am doing right now.</p>

<pre><code>import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

tf = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), min_df=0, stop_words='english')

titles = list_of_strings

tfidf_matrix = tf.fit_transform(titles)

# This gives a matrix roughly (5000, 20000)

new_title = titles[0] + ' ' + titles[1]

# Get the tfidf_matrix for the new_title
</code></pre>

<p>I would be great if there was something like:</p>

<pre><code>matrix_for_book_a + matrix_for_book_b 
</code></pre>

<p>and this gave the matrix for ""Book A Book B"" but I'm not sure if this is possible.</p>

<p>Thanks for any help or suggestions.</p>
"
"56687161","How to determine which words have high predictive power in Sentiment Analysis?","2019-06-20 13:32:54","1","233","0","1","","56687231","<p>I am working on a classification problem with Tweeter data. User labeled tweets (relevant, not relevant) are used to train a machine learning classifier to predict if an unseen tweet is relevant or not to the user. </p>

<p>I use a simple preprocessing techniques like removal of stopwords, stemming etc and a sklearn Tfidfvectorizer to convert the words into numbers before feeding them into a classifier e.g. SVM, kernel SVM , Naïve Bayes. </p>

<p>I would like to determine which words (features) have the higher predictive power. What is the best way to do so?</p>

<p>I have tried wordcloud but it just shows the words with highest frequency in the sample.</p>

<p>UPDATE:</p>

<p>The following approach along with sklearns feature_selection seem to provide the best answer so far to my problem:</p>

<p><a href=""https://medium.com/@aneesha/visualising-top-features-in-linear-svm-with-scikit-learn-and-matplotlib-3454ab18a14d"" rel=""nofollow noreferrer"">top features</a> Any other suggestions?</p>
"
"56621702","Regex match a substring if that substring is not preceded by a specific string and ignore the whole string?","2019-06-16 18:48:33","0","58","0","1","","56621767","<p>Trying a few things with negative and positive look-behind, can't really get exactly what I want. Went through this SO question as well: <a href=""https://stackoverflow.com/questions/48071699/regex-match-a-substring-if-that-substring-is-not-preceded-by-a-specific-string"">Regex match a substring if that substring is not preceded by a specific string?</a></p>

<p>What I want is just a ""yes this entire string should be considered"" and a ""no, ignore this entire string"", because of these substring matches. The post above will help me match the substring, but if the negative words precede the substring, it's still a match, you can see my tests here: <a href=""https://regex101.com/r/aqn1gO/2"" rel=""nofollow noreferrer"">https://regex101.com/r/aqn1gO/2</a></p>

<p>What I'm trying to do is have a regex match for the substring <code>i need</code>, but ignore cases where it's not a request, but more a question. The examples are:</p>

<ol>
<li>i need you to do this</li>
<li>hey i need this by tomorrow</li>
<li>hey do i need this in the deliverable?</li>
<li>hey should i need to do this?</li>
<li>how are you doing today?</li>
</ol>

<p>Where <code>1.</code> and <code>2.</code> should match, but <code>3.</code>, <code>4.</code>, and <code>5.</code> should not, even though there's an <code>i need</code> in there.</p>
"
"56610465","NLTK sentence_bleu method 7 gives scores above 1","2019-06-15 12:37:28","2","1156","0","1","","57664963","<p>When using the NLTK <code>sentence_bleu</code> function in combination with <code>SmoothingFunction</code> method 7, the max score is <code>1.1167470964180197</code>. This while the BLEU score is defined to be between <code>0</code> and <code>1</code>.</p>

<p>This score shows up for perfect matches with the reference. I'm using method 7 since I do not always have sentences of length 4, some may be lower. Using method 5 gives the same result. Other methods do give 1.0 as a perfect score.</p>

<p>It occurs when I use a single reference and candidate, for example:</p>

<pre><code>from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
cc = SmoothingFunction()
reference = ['overofficious 98461 54363 39016 78223 52180']
candidate = 'overofficious 98461 54363 39016 78223 52180'
sentence_bleu(reference, candidate, smoothing_function=cc.method7)
</code></pre>

<p>This gives the score: <code>1.1167470964180197</code></p>

<p>Am I doing something wrong, is this expected behavior or is there a bug in the implementation of the smoothing function?</p>
"
"56592351","NLTK : How to get a specific contents of an array in a loop with python?","2019-06-14 06:10:02","-1","306","2","2","","56592530","<p>is it possible to do the following code with python:</p>

<pre class=""lang-py prettyprint-override""><code>import nltk
from nltk.corpus.reader import TaggedCorpusReader
reader = TaggedCorpusReader('cookbook', r'.*\.pos')
train_sents=reader.tagged_sents()
tags=[]
count=0
for sent in train_sents:
    for (word,tag) in sent:
        #if tag is DTDEF i want to get the tag after it
        if tag==""DTDEF"":
            tags[count]=tag[acutalIndex+1]
            count+=1


fd = nltk.FreqDist(tags)
fd.tabulate()

</code></pre>

<p>Thank you in advance for your answer and advice.</p>
"
"56556837","Does keras-tokenizer perform the task of lemmatization and stemming?","2019-06-12 07:33:03","6","2346","0","1","","56560157","<p>Does keras tokenizer provide the functions such as stemming and lemmetization? If it does, then how is it done? Need an intuitive understanding. Also, what does <code>text_to_sequence</code> do in that?</p>
"
"56527814","Stanford typed dependencies using coreNLP in python","2019-06-10 13:54:55","6","3753","0","5","","56635121","<p>In <a href=""https://nlp.stanford.edu/software/dependencies_manual.pdf"" rel=""noreferrer"">Stanford Dependency Manual</a> they mention  ""Stanford typed dependencies"" and particularly the type ""neg"" - negation  modifier. It is also available when using Stanford enhanced++ parser using the website. for example, the sentence: </p>

<blockquote>
  <p>""Barack Obama was not born in Hawaii""</p>
</blockquote>

<p><a href=""https://i.sstatic.net/dEo0q.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/dEo0q.png"" alt=""enter image description here""></a> </p>

<p>The parser indeed find neg(born,not) </p>

<p>but when I'm using the <code>stanfordnlp</code> python library, the only dependency parser I can get will parse the sentence as  follow:</p>

<pre><code>('Barack', '5', 'nsubj:pass')

('Obama', '1', 'flat')

('was', '5', 'aux:pass')

('not', '5', 'advmod')

('born', '0', 'root')

('in', '7', 'case')

('Hawaii', '5', 'obl')
</code></pre>

<p>and the code that generates it: </p>

<pre><code>import stanfordnlp
stanfordnlp.download('en')  
nlp = stanfordnlp.Pipeline()
doc = nlp(""Barack Obama was not born in Hawaii"")
a  = doc.sentences[0]
a.print_dependencies()
</code></pre>

<p>Is there a way to get similar results to the enhanced dependency parser or any other Stanford parser that result in typed dependencies that will give me the negation modifier?</p>
"
"56425454","Extracting the Strings from a Chunk","2019-06-03 10:29:10","0","642","1","1","","56427280","<p>I am using NLTK POS-Tagging to extract information from a text, in this example I am looking for an IBAN. For some texts the code returns more than one chunk, but I don't mind that, I will sort the correct one out later with a RegEx. 
Now here is my question: is there a prettier way to get the Strings of the chunk so I can work with them or save them?</p>

<p>Of course you could go the artifical way (i.e. iterate through all lines in ibanChunk, then ibanChunk.replace(..) etc.) but there must be a better way, at least that's what I'm hoping.</p>

<pre class=""lang-py prettyprint-override""><code>tagged_sents = list(corp.tagged_sents())
tagger = ClassifierBasedGermanTagger(train=tagged_sents)
tagged_sents = tagger.tag(filtered_sentence)

ibanChunkGram = r""""""Chunk: {(&lt;VMPP&gt;&lt;CARD&gt;*)|(&lt;FM&gt;&lt;CARD&gt;+)}""""""
chunkParser = nltk.RegexpParser(ibanChunkGram)
ibanChunk = chunkParser.parse(tagged_sents)

print(ibanChunkGram)
</code></pre>

<p>Right now the output of the line looks like this:</p>

<pre class=""lang-py prettyprint-override""><code>(Chunk DE01/FM 2345/CARD 6789/CARD 0000/CARD 0000/CARD 00/CARD)
</code></pre>

<p>and what I want to have is:</p>

<pre class=""lang-py prettyprint-override""><code>DE01 2345 6789 0000 0000 00
</code></pre>

<p>Edit: Here is a minimalExample:</p>

<pre><code>This is a minimal example of POS-tagging. I want to extract an IBAN (DE01 2345 6789 0000 0000 00) and I hope The Machine 01 can find it quick.
</code></pre>

<p>And this is the output of my code:</p>

<pre class=""lang-py prettyprint-override""><code>(S
  This/NE
  is/FM
  a/FM
  minimal/FM
  example/FM
  of/FM
  POS-tagging/FM
  ./$.
  I/FM
  want/FM
  to/FM
  extract/FM
  IBAN/FM
  (/$(
  (Chunk DE01/FM 2345/CARD 6789/CARD 0000/CARD 0000/CARD 00/CARD)
  )/$(
  and/NE
  I/NE
  hope/VAFIN
  The/NE
  Machine/NE
  01/CARD
  can/XY
  find/XY
  it/XY
  quick/XY
  ./$.)
</code></pre>
"
"56415464","What are document and corpus in tf-idf?","2019-06-02 13:24:38","2","1688","0","1","","56415481","<p>tf-idf = term frequency * inverse document frequency</p>

<p><strong><em>term frequency</em></strong> is defined as the count of a term in a document.</p>

<p><strong><em>inverse document frequency</em></strong> is defined as the total number of documents divided by the number of documents containing the word.</p>

<p>The formula above may vary, but that is the big picture.
Now, supposing I have a data set containing a list of 1 million sentences:</p>

<p>1) Is a document an entry in the data set?</p>

<p>2) Is the entire data set the corpus?</p>

<p>The question somehow relates to [1], but the answers did not help me understand the concept for a real data set.</p>

<p>Thank you.</p>

<p>[1] <a href=""https://stackoverflow.com/questions/41749471/what-does-document-mean-in-a-nlp-context"">What does &quot;document&quot; mean in a NLP context?</a></p>
"
"56392852","NLP - How to add more features?","2019-05-31 09:59:23","2","3110","1","4","","56398033","<p>I want to use a sklearn classifier to train a model to classify data entries (yes,no) using a text feature (content), a numerical feature (population) and a categorical feature (location). </p>

<p><a href=""https://i.sstatic.net/EA1g4.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/EA1g4.png"" alt=""enter image description here""></a></p>

<p>The model below is using only the text data to classify each entry. The text is converted with TF-IDF into a sparse matrix before being imported into the classifier.</p>

<p>Is there a way to add/use also the other features? These features are not in sparse matrix format so not sure how to combine them with the text sparse matrix. </p>

<pre class=""lang-py prettyprint-override""><code>
    #import libraries
    import string, re, nltk
    import pandas as pd
    from pandas import Series, DataFrame
    from nltk.corpus import stopwords
    from nltk.stem.porter import PorterStemmer
    from sklearn.feature_extraction.text import CountVectorizer
    from sklearn.feature_extraction.text import TfidfTransformer
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import classification_report
    from sklearn.pipeline import Pipeline

    # read data and remove empty lines
    dataset = pd.read_csv('sample_data.txt',
                           sep='\t',
                           names=['content','location','population','target'])
                           .dropna(how='all')
                           .dropna(subset=['target'])

    df = dataset[1:]

    #reset dataframe index
    df.reset_index(inplace = True)

    #add an extra column which is the length of text
    df['length'] = df['content'].apply(len)

    #create a dataframe that contains only two columns the text and the target class
    df_cont = df.copy()
    df_cont = df_cont.drop(
        ['location','population','length'],axis = 1)

    # function that takes in a string of text, removes all punctuation, stopwords and returns a list of cleaned text

    def text_process(mess):
        # lower case for string
        mess = mess.lower()

        # check characters and removes URLs
       nourl = re.sub(r'http\S+', ' ', mess)

        # check characters and removes punctuation
        nopunc = [char for char in nourl if char not in string.punctuation]

        # join the characters again to form the string and removes numbers
        nopunc = ''.join([i for i in nopunc if not i.isdigit()])

        # remove stopwords
        return [ps.stem(word) for word in nopunc.split() if word not in set(stopwords.words('english'))]

    #split the data in train and test set and train/test the model

    cont_train, cont_test, target_train, target_test = train_test_split(df_cont['content'],df_cont['target'],test_size = 0.2,shuffle = True, random_state = 1)


    pipeline = Pipeline([('bag_of_words',CountVectorizer(analyzer=text_process)),
                         ('tfidf',TfidfTransformer()),
                         ('classifier',MultinomialNB())])

    pipeline.fit(cont_train,target_train)
    predictions = pipeline.predict(cont_test)

    print(classification_report(predictions,target_test))

</code></pre>

<p>The model is expected to return the following: accuracy, precision, recall ,f1-score</p>
"
"56315645","Avoiding <sos> and <eos> being parsed by Spacy","2019-05-26 17:03:02","1","681","0","1","","56316787","<p>I am stuck with a basic thing but I could not figure out how to make it work. My apologies if it is something super basic. It is just that I am very new to Spacy and do not know how to do this. Could not find any resource on the internet as well. </p>

<p>I have a bunch of sentences like so</p>

<pre class=""lang-py prettyprint-override""><code>a = ""&lt;sos&gt; Hello There! &lt;eos&gt;""
</code></pre>

<p>I am using this following lines of code to tokenize it using Spacy</p>

<pre class=""lang-py prettyprint-override""><code>import spacy
nlp = spacy.load('en_core_web_sm')
for token in nlp(a):
    print(token.text)
</code></pre>

<p>What it prints is something like this</p>

<pre><code>&lt;
sos
&gt;
Hello
There
!
&lt;
eos
&gt;
</code></pre>

<p>As you can see, it parsed the <code>&lt;sos&gt;</code> and <code>&lt;eos&gt;</code> metatags. How can I avoid that? The output I would like to see is something like the following</p>

<pre><code>&lt;sos&gt;
Hello
There
!
&lt;eos&gt;
</code></pre>

<p>I could not figure out how to achieve this. Any help will be great. </p>

<p>Thanks in advance</p>
"
"56304480","How to speed up this word-tuple finding algorithm?","2019-05-25 11:38:15","0","94","2","2","","56304583","<p>I am trying to create a simple model to predict the next word in a sentence. I have a big .txt file that contains sentences seperated by '\n'. I also have a vocabulary file which lists every unique word in my .txt file and a unique ID. I used the vocabulary file to convert the words in my corpus to their corresponding IDs. Now I want to make a simple model which reads the IDs from txt file and find the word pairs and how many times this said word pairs were seen in the corpus. I have managed to write to code below:</p>

<pre><code>tuples = [[]] #array for word tuples to be stored in
data = []   #array for tuple frequencies to be stored in

data.append(0) #tuples array starts with an empty element at the beginning for some reason.
            # Adding zero to the beginning of the frequency array levels the indexes of the two arrays

with open(""markovData.txt"") as f:
    contentData = f.readlines()
    contentData = [x.strip() for x in contentData]
    lineIndex = 0
    for line in contentData:
        tmpArray = line.split() #split line to array of words
        tupleIndex = 0
        tmpArrayIndex = 0
        for tmpArrayIndex in range(len(tmpArray) - 1): #do this for every word except the last one since the last word has no word after it.
            if [tmpArray[tmpArrayIndex], tmpArray[tmpArrayIndex + 1]] in tuples: #if the word pair is was seen before
                data[tuples.index([tmpArray[tmpArrayIndex], tmpArray[tmpArrayIndex + 1]])] += 1 #increment the frequency of said pair
            else:
                tuples.append([tmpArray[tmpArrayIndex], tmpArray[tmpArrayIndex + 1]]) #if the word pair is never seen before
                data.append(1)                                                        #add the pair to list and set frequency to 1.

        #print every 1000th line to check the progress
        lineIndex += 1
        if ((lineIndex % 1000) == 0):
            print(lineIndex)

with open(""markovWindowSize1.txt"", 'a', encoding=""utf8"") as markovWindowSize1File:
    #write tuples to txt file
    for tuple in tuples:
        if (len(tuple) &gt; 0): # if tuple is not epmty
            markovWindowSize1File.write(str(element[0]) + "","" + str(element[1]) + "" "")

    markovWindowSize1File.write(""\n"")
    markovWindowSize1File.write(""\n"")
    #blank spaces between two data

    #write frequencies of the tuples to txt file
    for element in data:
        markovWindowSize1File.write(str(element) + "" "")

    markovWindowSize1File.write(""\n"")
    markovWindowSize1File.write(""\n"")
</code></pre>

<p>This code seems to be working well for the first couple thousands of lines. Then things start to get slower because the tuple list keeps getting bigger and I have to search the whole tuple list to check if the next word pair was seen before or not. I managed to get the data of 50k lines in 30 minutes but I have much bigger corpuses with millions of lines. Is there a way to store and search for the word pairs in a more efficient way? Matrices would probably work a lot faster but my unique word count is about 300.000 words. Which means I have to create a 300k*300k matrix with integers as data type. Even after taking advantage of symmetric matrices, it would require <strong>a lot</strong> more memory than what I have.</p>

<p>I tried using memmap from numpy to store the matrix in disk rather than memory but it required about 500 GB free disk space.</p>

<p>Then I studied the sparse matrices and found out that I can just store the non-zero values and their corresponding row and column numbers. Which is what I did in my code.</p>

<p>Right now, this model works but it is very bad at guessing the next word correctly ( about 8% success rate). I need to train with bigger corpuses to get better results. What can I do to make this word pair finding code more efficient?</p>

<p>Thanks.</p>

<hr>

<p>Edit: Thanks to everyone answered, I am now able to process my corpus of ~500k lines in about 15 seconds. I am adding the final version of the code below for people with similiar problems:</p>

<pre><code>import numpy as np
import time

start = time.time()
myDict = {} # empty dict

with open(""markovData.txt"") as f:
    contentData = f.readlines()
    contentData = [x.strip() for x in contentData]
    lineIndex = 0
    for line in contentData:
        tmpArray = line.split() #split line to array of words
        tmpArrayIndex = 0
        for tmpArrayIndex in range(len(tmpArray) - 1): #do this for every word except the last one since the last word has no word after it.
            if (tmpArray[tmpArrayIndex], tmpArray[tmpArrayIndex + 1]) in myDict: #if the word pair is was seen before
                myDict[tmpArray[tmpArrayIndex], tmpArray[tmpArrayIndex + 1]] += 1  #increment the frequency of said pair
        else:
            myDict[tmpArray[tmpArrayIndex], tmpArray[tmpArrayIndex + 1]] = 1 #if the word pair is never seen before
                                                                              #add the pair to list and set frequency to 1.

    #print every 1000th line to check the progress
    lineIndex += 1
    if ((lineIndex % 1000) == 0):
        print(lineIndex)


end = time.time()
print(end - start)

keyText= """"
valueText = """"

for key1,key2 in myDict:
    keyText += (str(key1) + "","" + str(key2) + "" "")
    valueText += (str(myDict[key1,key2]) + "" "")


with open(""markovPairs.txt"", 'a', encoding=""utf8"") as markovPairsFile:
    markovPairsFile.write(keyText)

with open(""markovFrequency.txt"", 'a', encoding=""utf8"") as markovFrequencyFile:
    markovFrequencyFile.write(valueText)
</code></pre>
"
"56267537","StanfordNLP, CoreNLP, spaCy - different dependency graphs","2019-05-23 03:04:44","0","1378","0","1","","56278823","<p>I'm trying to use simple rules/patterns defined over a dependency graph to extract very basic informations from sentences (e.g., triples such as subject->predicate->object). I started using <a href=""https://stanfordnlp.github.io/stanfordnlp/pipeline.html"" rel=""nofollow noreferrer"">StanfordNLP</a> since it was easy to set up and utlizes the GPU for better performance. However, I've noticed that for some sentences, the resulting dependency graph looked not as I would have expected -- I'm no expert though. I therefore tried two other solutions: <a href=""https://spacy.io/usage/linguistic-features"" rel=""nofollow noreferrer"">spaCy</a> and <a href=""https://stanfordnlp.github.io/CoreNLP/"" rel=""nofollow noreferrer"">Stanford CoreNLP</a> (I understand that these are maintained by different groups?)</p>

<p>For the example sentence <em>""Tom made Sam believe that Alice has cancer.""</em> I've printed the dependencies for all three approaches. CoreNLP and spaCy yield the same dependencies, and they are different from the ones of StanfordNLP. Hence, I'm inclined to swich to CoreNLP and spaCy (another advantage would be that they come with NER out of the box).</p>

<p>Does anyone have some more experience or feedback that would help where to go from here? I don't expect that CoreNLP and spaCy will always yield in the same dependency graphs, but in the example sentence, considering <code>Sam</code> as <code>obj</code> as StandfordNLP is doing compared to being <code>nsubj</code> (CoreNLP, spaCy) seems to be a significant difference</p>

<pre><code>Format:
token   dependency_tag   parent_token

StanfordNLP
Tom     nsubj   made
made    ROOT    ROOT
Sam     obj     made
believe ccomp   made
that    mark    has
Alice   nsubj   has
has     ccomp   believe
cancer  obj     has
.       punct   made

CoreNLP
Tom     nsubj   made
made    ROOT    ROOT
Sam     nsubj   believe
believe ccomp   made
that    mark    has
Alice   nsubj   has
has     ccomp   believe
cancer  dobj    has
.       punct   made

spaCy
Tom     nsubj   made
made    ROOT    ROOT
Sam     nsubj   believe
believe ccomp   made
that    mark    has
Alice   nsubj   has
has     ccomp   believe
cancer  dobj    has
.       punct   made
</code></pre>
"
"56221883","How to compute the perplexity in text classification?","2019-05-20 13:22:49","0","1425","0","1","","56224318","<p>I'm doing dialect text classification with scikit learn, naive bayes and countvectorizer. So far I'm only doing 3 dialects text classification. I'm going to add a new dialect(or actually, the formal language for those dialects). The problem is, the new text that I'm going to add, shares a lot of words with the other 3 dialects. So I read the following in a research document:</p>

<blockquote>
  <p>We train an n-gram model for each dialect from the collected data. To
  train the MSA model, we select sentences from Arabic UN corpus and
  news collections. All the dialect and MSA models share the same
  vocabulary, thus perplexity can be compared properly. At
  classification time, given an input sentence, the classifier computes
  the perplexity for each dialect type and choose the one with minimum
  perplexity as the label.</p>
</blockquote>

<p>They mean by MSA(Modern Standard Arabic) which is the formal language for those dialects. How are they  calculating the perplexity? Are they just using naive bayes or there's more to it?</p>
"
"56198930","How to get the base form of an adj or adverb using lemma in spacy","2019-05-18 12:27:52","2","855","0","1","","56217275","<p>For a project, I would like to be able to get the noun form of an adjective or adverb if there is one using NLP.
For example, ""deathly"" would return ""death"" and ""dead"" would return ""death"".
""lively"" would return ""life"".</p>

<p>I've tried using the spacy lemmatizer but it does not manage to get the base radical form.
For example, if I'd do:</p>

<pre><code>import spacy
nlp = spacy.load('en_core_web_sm')
z = nlp(""deathly lively"")
for token in z:
    print(token.lemma_)
</code></pre>

<p>It would return:
<code>&gt;&gt;&gt; deathly lively</code>
instead of:
<code>&gt;&gt;&gt; death life</code></p>

<p>Does anyone have any ideas?
Any answer is appreciated.</p>
"
"56181929","Save SpaCy render file as SVG using DisplaCy","2019-05-17 07:47:45","7","7478","0","2","","56186497","<p>I have the following code:</p>

<pre><code>import spacy
from spacy import displacy
from pathlib import Path

nlp = spacy.load('en_core_web_sm', parse=True, tag=True, entity=True)

sentence_nlp = nlp(""John go home to your family"")
svg = displacy.render(sentence_nlp, style=""dep"", jupyter=True)

output_path = Path(""/images/dependency_plot.svg"")
output_path.open(""w"", encoding=""utf-8"").write(svg)
</code></pre>

<p>I am trying to write an the rendered file to an svg file in the images folder. 
However, I get the error:</p>

<blockquote>
  <p>Traceback (most recent call last):</p>
  
  <p>File """", line 8, in 
      output_path.open(""w"", encoding=""utf-8"").write(svg)</p>
  
  <p>File
  ""C:\Users****\AppData\Local\Continuum\miniconda3\lib\pathlib.py"",
  line 1183, in open
      opener=self._opener)</p>
  
  <p>File
  ""C:\Users****\AppData\Local\Continuum\miniconda3\lib\pathlib.py"",
  line 1037, in _opener
      return self._accessor.open(self, flags, mode)</p>
  
  <p>File
  ""C:\Users****\AppData\Local\Continuum\miniconda3\lib\pathlib.py"",
  line 387, in wrapped
      return strfunc(str(pathobj), *args)
  FileNotFoundError: [Errno 2] No such file or directory:
  '\images\dependency_plot.svg'</p>
</blockquote>

<p>The directory does exist and so I'm not really sure what I'm doing wrong. I have also looked at the spacy usage page <a href=""https://spacy.io/usage/visualizers#jupyter"" rel=""nofollow noreferrer"">https://spacy.io/usage/visualizers#jupyter</a> and couldn't figure out what I'm doing wrong. I'm using spyder (if this information is required).
Please assist.</p>
"
"56024637","python lemmatizer that lemmatize ""political"" and ""politics"" to the same word","2019-05-07 14:15:40","2","562","5","1","","56030163","<p>I've been testing different python lemmatizers for a solution I'm building out. One difficult problem I've faced is that stemmers are producing non english words which won't work for my use case. Although stemmers get ""politics"" and ""political"" to the same stem correctly, I'd like to do this with a lemmatizer, but spacy and nltk are producing different words for ""political"" and ""politics"". Does anyone know of a more powerful lemmatizer? My ideal solution would look like this:  </p>

<pre><code>from nltk.stem import WordNetLemmatizer

lemmatizer = WordNetLemmatizer()

print(""political = "", lemmatizer.lemmatize(""political""))
print(""politics = "", lemmatizer.lemmatize(""politics""))  
</code></pre>

<p>returning:  </p>

<pre><code>political =  political
politics =  politics  
</code></pre>

<p>Where I want to return:</p>

<pre><code>political =  politics
politics =  politics  
</code></pre>
"
"55984480","Python: TaggedCorpusReader how to get from a STTS to a Universal tagset","2019-05-04 16:19:30","0","276","0","1","","55993906","<p>I'm working on a POS Tagger using Python and Keras. The data I've got is using the STTS Tags, but I'm supposed to create a Tagger for the universal tagset. So I need to translate this.</p>

<p>First I thought of making a dictionary and simply search replace the tags, but then I saw the option of setting a tagset using the TaggedCorpusReader. (e.g. 'brown')</p>

<p>But I miss a list of possible tagsets that can be used there. Can I use the STTS Tagset somehow or do I have to make a dictionary myself?</p>

<p><strong>Example Source:</strong>
Code #3 : map corpus tags to the universal tagset
<a href=""https://www.geeksforgeeks.org/nlp-customization-using-tagged-corpus-reader/"" rel=""nofollow noreferrer"">https://www.geeksforgeeks.org/nlp-customization-using-tagged-corpus-reader/</a></p>

<pre><code>corpus = TaggedCorpusReader(filePath, ""standard_pos_tagged.txt"", tagset='STTS') #?? doesn't work sadly
# ....
trainingCorpus.tagged_sents(tagset='universal')[1]
</code></pre>

<hr>

<p><strong>In the end it looked something like this:</strong> (big thanks to <a href=""https://stackoverflow.com/users/699305/alexis"">alexis</a>)</p>

<pre><code>with open(resultFileName, ""w"") as output:
    for sent in stts_corpus.tagged_sents():
        for word, tag in sent:
            try:
                newTag = mapping_dict[tag];
                output.write(word+""/""+newTag+"" "")               
            except:
                print(""except ""  + str(word) + "" - "" + str(tag))
        output.write(""\n"")
</code></pre>
"
"55955027","Filtering list of Arabic sentences based on language test: Why so slow?","2019-05-02 14:50:44","2","186","0","1","","55955202","<p>I'm trying to go through a list of (mostly) Arabic sentences, and remove those that are not Arabic. I've got a hack for telling if a character is Arabic or not: Arabic has no case, so if the character is alpha but isn't upper case or lower case, it's Arabic.</p>

<p>I've got the code below, which works, but the language identification part is very slow, compared to the other filter. It doesn't seem to me like it's doing anything particularly complex, so I don't understand why it's taking so long. (The corpus is size is about 300K sentences before filtering.)</p>

<p>Is there something I can do to make it more efficient?</p>

<p>Thanks!</p>

<pre><code>def test_lang(string):
    """"""Takes a string and determines if it is written in Arabic 
    characters or foreign, by testing whether the first character 
    has a case attribute. This is intended for Arabic texts that  
    may have English or French words added. If it encounters another 
    case-less language (Chinese for instance), it will falsely 
    identify it as Arabic.""""""

    if not string or not string.isalpha():
        return None
    char = string[0]
    if char.isalpha() and not (char.islower() or char.isupper()):
        lang = 'AR'
    else:
        lang = 'FW'
    return lang
</code></pre>

<p>...</p>

<pre><code># remove sentences that are in English or French - THIS IS SLOW (takes a few mins)
for sent in sents:
    if sent and test_lang(sent[0]) != 'AR':
        sents.remove(sent)

# remove clearly MSA sentences -- THIS IS FAST (takes a few seconds)
msa_features = ['ليس','لست','ليست','ليسوا','الذي','الذين','التي','ماذا', 'عن']
p = re.compile('|'.join(msa_features))
for sent in sents:
    if re.search(p, sent):
        sents.remove(sent)
</code></pre>
"
"55864933","Spacy lemmatizer issue/consistency","2019-04-26 09:45:19","0","1762","3","1","","55953565","<p>I'm currently using spaCy for NLP purpose (mainly lemmatization and tokenization). The model used is en-core-web-sm (2.1.0). </p>

<p>The following code is run to retrieve a list of words ""cleansed"" from a query</p>

<pre><code>import spacy
nlp = spacy.load(""en_core_web_sm"")
doc = nlp(query)
list_words = []
for token in doc:
    if token.text != ' ':
        list_words.append(token.lemma_)
</code></pre>

<p>However I face a major issue, when running this code.
For example, when the query is ""processing of tea leaves"".
The result stored in list_words can be either ['processing', 'tea', 'leaf'] or ['processing', 'tea', 'leave']. </p>

<p>It seems that the result is not consistent. I cannot change my input/query (adding another word for context is not possible) and I really need to find the same result every time. I think the loading of the model may be the issue.</p>

<p>Why the result differ ? Can I load the model the ""same"" way everytime ? Did I miss a parameter to obtain the same result for ambiguous query ?</p>

<p>Thanks for your help</p>
"
"55852115","Token extension versus matcher versus phrase matcher vs entity ruler in spaCy","2019-04-25 14:49:19","14","3178","0","1","","57432475","<p>I am trying to figure out the best way (fast) to extract entities, e.g. a month. I have come up with 5 different approaches using <a href=""https://spacy.io/"" rel=""noreferrer""><code>spaCy</code></a>.</p>

<h2>Initial setup</h2>

<p>For each solution I start with an initial setup</p>

<pre><code>import spacy.lang.en    
nlp = spacy.lang.en.English()
text = 'I am trying to extract January as efficient as possible. But what is the best solution?'
</code></pre>

<h3>Solution: using <a href=""https://spacy.io/usage/processing-pipelines#custom-components-attributes"" rel=""noreferrer"">extension attributes</a> (limited to single token matching)</h3>

<pre><code>import spacy.tokens
NORM_EXCEPTIONS = {
    'jan': 'MONTH', 'january': 'MONTH'
}
spacy.tokens.Token.set_extension('norm', getter=lambda t: NORM_EXCEPTIONS.get(t.text.lower(), t.norm_))
def time_this():
    doc = nlp(text)
    assert [t for t in doc if t._.norm == 'MONTH'] == [doc[5]]

%timeit time_this()
</code></pre>

<blockquote>
  <p>76.4 µs ± 169 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)</p>
</blockquote>

<h3>Solution: using phrase matcher via <a href=""https://spacy.io/usage/rule-based-matching#entityruler"" rel=""noreferrer"">entity ruler</a></h3>

<pre><code>import spacy.pipeline
ruler = spacy.pipeline.EntityRuler(nlp)
ruler.phrase_matcher = spacy.matcher.PhraseMatcher(nlp.vocab, attr=""LOWER"")
ruler.add_patterns([{'label': 'MONTH', 'pattern': 'jan'}, {'label': 'MONTH', 'pattern': 'january'}])
nlp.add_pipe(ruler)
def time_this():
    doc = nlp(text)
    assert [t for t in doc.ents] == [doc[5:6]]
%timeit time_this()
</code></pre>

<blockquote>
  <p>131 µs ± 579 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)</p>
</blockquote>

<h3>Solution: using token matcher via entity ruler</h3>

<pre><code>import spacy.pipeline
ruler = spacy.pipeline.EntityRuler(nlp)
ruler.add_patterns([{'label': 'MONTH', 'pattern': [{'lower': {'IN': ['jan', 'january']}}]}])
nlp.add_pipe(ruler)
def time_this():
    doc = nlp(text)
    assert [t for t in doc.ents] == [doc[5:6]]
%timeit time_this()
</code></pre>

<blockquote>
  <p>72.6 µs ± 76.7 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)</p>
</blockquote>

<h3>Solution: using <a href=""https://spacy.io/usage/rule-based-matching#phrasematcher"" rel=""noreferrer"">phrase matcher</a> directly</h3>

<pre><code>import spacy.matcher
phrase_matcher = spacy.matcher.PhraseMatcher(nlp.vocab, attr=""LOWER"")
phrase_matcher.add('MONTH', None, nlp('jan'), nlp('january'))
def time_this():
    doc = nlp(text)
    matches = [m for m in filter(lambda x: x[0] == doc.vocab.strings['MONTH'], phrase_matcher(doc))]
    assert [doc[m[1]:m[2]] for m in matches] == [doc[5:6]]
%timeit time_this()
</code></pre>

<blockquote>
  <p>115 µs ± 537 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)</p>
</blockquote>

<h3>Solution: using <a href=""https://spacy.io/usage/rule-based-matching#matcher"" rel=""noreferrer"">token matcher</a> directly</h3>

<pre><code>import spacy.matcher
matcher = spacy.matcher.Matcher(nlp.vocab)
matcher.add('MONTH', None, [{'lower': {'IN': ['jan', 'january']}}])
def time_this():
    doc = nlp(text)
    matches = [m for m in filter(lambda x: x[0] == doc.vocab.strings['MONTH'], matcher(doc))]
    assert [doc[m[1]:m[2]] for m in matches] == [doc[5:6]]
%timeit time_this()
</code></pre>

<blockquote>
  <p>55.5 µs ± 459 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)</p>
</blockquote>

<h1>Conclusion</h1>

<p>The custom attributes with is limited to single token matching and the token matcher seems to be faster so that seems to be preferable. The EntityRuler seems to be the slowest which isn't surprising since it is changing the <code>Doc.ents</code>. It is however quite convenient that you have your matches in <code>Doc.ents</code> so you might want to consider this method still. </p>

<p>I was quite surprised that the token matcher outperforms the phrase matcher. I thought it would be opposite:</p>

<blockquote>
  <p>If you need to match large terminology lists, you can also use the PhraseMatcher and create Doc objects instead of token patterns, which is much more efficient overall</p>
</blockquote>

<h1>Question</h1>

<p>Am I missing something important here or can I trust this analysis on a larger scale?</p>
"
"55776532","How to insert pos tags in a dataframe arranged in separate columns in python?","2019-04-20 18:28:32","0","1007","0","1","","55776787","<p>I have POS tagged my input text using TextBlob and exported it into a text file. It gives me three information as POS, Parse Chunker and Deep Parsing. The output of this tagging is in this format: Technique:Plain/NNP/B-NP/O and/CC/I-NP/O . I want this to be arranged in a dataframe in separate columns for each.</p>
<p>This is the code I am using.</p>
<pre><code> import pandas as pd
 import csv
 from textblob import TextBlob
 with open('report1to8_1.txt', 'r') as myfile:
    report=myfile.read().replace('\n', '')
 out = TextBlob(report).parse()
 tagS = 'taggedop.txt'
 f = open('taggedop.txt', 'w')
 f.write(str(out))
 df = pd.DataFrame(columns=['Words', 'POS', 'Parse chunker','Deep 
 Parsing'])
 df = pd.read_csv('taggedop.txt', sep=' ',error_bad_lines=False, 
 quoting=csv.QUOTE_NONE)   
</code></pre>
<p>My expected result is to have a dataframe like this:
<a href=""https://i.sstatic.net/EIDRI.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/EIDRI.png"" alt=""enter image description here"" /></a>
However, currently I am getting this:
<a href=""https://i.sstatic.net/aqS6i.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/aqS6i.png"" alt=""enter image description here"" /></a></p>
<p>Please Help!!</p>
"
"55528385","Finding the word that corresponds to experience","2019-04-05 04:43:50","-3","49","4","1","","55605413","<p>Suppose i have a sentence like the following, how can i find what the experience corresponds to?</p>

<p>Ex: Programmer with 5 years of experience wanted.
I want to find what the experience (5 years) corresponds to, in this case programmer.</p>

<p>The code should also be recognize a corresponding verb, ex: 5 years of programming</p>

<p>So how should i go about it? I was thinking of making a pattern that finds the closest noun or verb.</p>
"
"55482342","How to stem a pandas dataframe using nltk ? The output should be a stemmed dataframe","2019-04-02 19:37:59","0","2937","1","1","","55495341","<p>I'm trying to pre-process a dataset. The dataset contains text data. I have created a pandas DataFrame from that dataset. 
my question is, how can I use stemming on the DataFrame and get a stemmed DataFrame as output?</p>
"
"55474457","How can I create a pandas dataframe column for each part-of-speech tag?","2019-04-02 12:09:25","0","916","0","1","","55475013","<p>I have a dataset that consists of tokenized, POS-tagged phrases as one column of a dataframe: </p>

<p><a href=""https://i.sstatic.net/3TvQU.png"" rel=""nofollow noreferrer"">Current Dataframe</a></p>

<p>I want to create a new column in the dataframe, consisting only of the proper nouns in the previous column:</p>

<p><a href=""https://i.sstatic.net/Chhmw.png"" rel=""nofollow noreferrer"">Desired Solution</a></p>

<p>Right now, I'm trying something like this for a single row: </p>

<pre><code>if 'NNP' in df['Description_POS'][96][0:-1]:
df['Proper Noun'] = df['Description_POS'][96]
</code></pre>

<p>But then I don't know how to loop this for each row, and how to obtain the tuple which contains the proper noun. 
I'm very new right now and at a loss for what to use, so any help would be really appreciated! </p>

<p>Edit: I tried the solution recommended, and it seems to work, but there is an issue. </p>

<p>this was my dataframe: 
<a href=""https://i.sstatic.net/9EPZD.png"" rel=""nofollow noreferrer"">Original dataframe</a></p>

<p>After implementing the code recommended</p>

<pre><code>df['Proper Nouns'] = df['POS_Description'].apply(
    lambda row: [i[0] for i in row if i[1] == 'NNP']) 
</code></pre>

<p>it looks like this: 
<a href=""https://i.sstatic.net/0Z5wb.png"" rel=""nofollow noreferrer"">Dataframe after creating a proper nouns column</a></p>
"
"55352301","Tfidfvectorizer - How can I check out processed tokens?","2019-03-26 08:00:19","3","1922","0","3","","55362465","<p>How can I check the strings tokenized inside <code>TfidfVertorizer()</code>?  If I don't pass anything in the arguments, <code>TfidfVertorizer()</code> will tokenize the string with some pre-defined methods. I want to observe how it tokenizes strings so that I can more easily tune my model.</p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
corpus = ['This is the first document.',
          'This document is the second document.',
          'And this is the third one.',
          'Is this the first document?']
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(corpus)
</code></pre>

<p>I want something like this:</p>

<pre><code>&gt;&gt;&gt;vectorizer.get_processed_tokens()
[['this', 'is', 'first', 'document'],
 ['this', 'document', 'is', 'second', 'document'],
 ['this', 'is', 'the', 'third', 'one'],
 ['is', 'this', 'the', 'first', 'document']]
</code></pre>

<p>How can I do this? </p>
"
"55307452","Is there a way to retrieve the whole noun chunk using a root token in spaCy?","2019-03-22 20:37:55","0","1467","0","1","","55313236","<p>I'm very new to using spaCy. I have been reading the documentation for hours and I'm still confused if it's possible to do what I have in my question. Anyway...</p>

<p>As the title says, is there a way to actually get a given noun chunk using a token containing it. For example, given the sentence:</p>

<pre><code>""Autonomous cars shift insurance liability toward manufacturers""
</code></pre>

<p>Would it be possible to get the <code>""autonomous cars""</code> noun chunk when what I only have the <code>""cars""</code> token? Here is an example snippet of the scenario that I'm trying to go for.</p>

<pre><code>startingSentence = ""Autonomous cars and magic wands shift insurance liability toward manufacturers""
doc = nlp(startingSentence)
noun_chunks = doc.noun_chunks

for token in doc:
    if token.dep_ == ""dobj"":
        print(child) # this will print ""liability""

        # Is it possible to do anything from here to actually get the ""insurance liability"" token?
</code></pre>

<p>Any help will be greatly appreciated. Thanks!</p>
"
"55174358","How cosine similarity differs from Okapi BM25?","2019-03-15 01:32:03","4","2687","0","1","","55176765","<p>I'm conducting a research using elasticsearch. I was planning to use cosine similarity but I noted that it is unavailable and instead we have BM25 as default scoring function.</p>

<p>Is there a reason for that? Is cosine similarity improper for querying documents? Why was BM25 chosen as default?
Thanks</p>
"
"55141126","How to lemmatize Norwegian using spaCy?","2019-03-13 11:44:33","2","921","1","1","","55192428","<p>I'm doing the following:</p>

<pre><code>from spacy.lang.nb import Norwegian
nlp = Norwegian()
doc = nlp(u'Jeg heter Marianne Borgen og jeg er ordføreren i Oslo.')
for token in doc:
    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,token.shape_, token.is_alpha, token.is_stop)
</code></pre>

<p>Lemmatization seems to not work at all, as this is the output:</p>

<pre><code>(u'Jeg', u'Jeg', u'', u'', u'', u'Xxx', True, False)
(u'heter', u'heter', u'', u'', u'', u'xxxx', True, False)
(u'Marianne', u'Marianne', u'', u'', u'', u'Xxxxx', True, False)
(u'Borgen', u'Borgen', u'', u'', u'', u'Xxxxx', True, False)
(u'og', u'og', u'', u'', u'', u'xx', True, True)
(u'jeg', u'jeg', u'', u'', u'', u'xxx', True, True)
(u'er', u'er', u'', u'', u'', u'xx', True, True)
(u'ordf\xf8reren', u'ordf\xf8reren', u'', u'', u'', u'xxxx', True, False)
(u'i', u'i', u'', u'', u'', u'x', True, True)
(u'Oslo', u'Oslo', u'', u'', u'', u'Xxxx', True, False)
(u'.', u'.', u'', u'', u'', u'.', False, False)
</code></pre>

<p>However, looking at <a href=""https://github.com/explosion/spaCy/blob/master/spacy/lang/nb/lemmatizer/_verbs_wordforms.py"" rel=""nofollow noreferrer"">https://github.com/explosion/spaCy/blob/master/spacy/lang/nb/lemmatizer/_verbs_wordforms.py</a>, the verb <em>heter</em> should at least be transformed into <em>hete</em>.</p>

<p>So it looks like spaCy has support, but it's not working? What could be the problem?</p>
"
"55094637","How to use StanfordNLP Python package to do dependency parsing?","2019-03-11 02:52:02","3","1896","0","2","","55096235","<p>I am trying to use the new NN-based parser at <a href=""https://github.com/stanfordnlp/stanfordnlp"" rel=""nofollow noreferrer"">here</a> to find all adjective phrases in a sentence (e.g., <code>good</code> and <code>extremely good</code> in <code>The weather is extremely good</code>), however, it's very lack of documentation and I could not get it working. My current code is</p>

<pre><code>import stanfordnlp
nlp = stanfordnlp.Pipeline()
doc = nlp(""The weather is extremely good"")
doc.sentences[0].print_dependencies()
</code></pre>

<p>which gives me</p>

<pre><code>('The', '2', 'det')
('weather', '5', 'nsubj')
('is', '5', 'cop')
('extremely', '5', 'advmod')
('good', '0', 'root')
</code></pre>

<p>But it is not clear how to extract the information I need, as this does not seem to be a tree structure. Does anyone have an idea?</p>
"
"55090520","German verbs lemmatization with Tiger corpus","2019-03-10 17:41:38","2","583","0","1","","55118103","<p>recently I'm traing to build service for lemmatization of german words.</p>

<p>I found very good article <a href=""https://datascience.blog.wzb.eu/2016/07/13/accurate-part-of-speech-tagging-of-german-texts-with-nltk/"" rel=""nofollow noreferrer"">here</a> </p>

<p>After I've done all the steps described in the article my service works quite good, but while testing I noticed that some of verbs cannot be converted to infinitive form.</p>

<p>E.g. kochst -> kochen. 
The root cause is that my POS tagger gives me ADV for 'kochst' while should VVFIN or at least V... since this is a verb.</p>

<p>I also found that original TIGER corpus file doesn't contain 'kochst' form but only 'kocht'.</p>

<p>I am not familiar with conll format, but have added one additional row which is shown below</p>

<pre><code>50475_11    kochst  kochen  _   VVFIN   _   number=sg|person=2|tense=pres|mood=ind  _   0   _   --  _   _   _   _
</code></pre>

<p>and retrained the tagger without any success, see listing below</p>

<pre><code>&gt;&gt;&gt; import nltk
&gt;&gt;&gt; corp = nltk.corpus.ConllCorpusReader('.', 'tiger_release_aug07.corrected.16012013.conll09',
...                                      ['ignore', 'words', 'ignore', 'ignore', 'pos'],
...                                      encoding='utf-8')
&gt;&gt;&gt; 
&gt;&gt;&gt; tagged_sents = corp.tagged_sents()
&gt;&gt;&gt; 
&gt;&gt;&gt; from ClassifierBasedGermanTagger.ClassifierBasedGermanTagger import ClassifierBasedGermanTagger
&gt;&gt;&gt; tagger = ClassifierBasedGermanTagger(train=tagged_sents)
&gt;&gt;&gt; tagger.tag(['kochst'])
[('kochst', u'ADV')]
&gt;&gt;&gt; 
&gt;&gt;&gt; 
&gt;&gt;&gt; tagged_sents[-1]
[(u'kochst', u'VVFIN')]
</code></pre>

<p>So it might be I added 'kochst' record incorrectly or TIGER corpus is not complete (I found that many verbs in second person form are not there) or I simply don't understand something here, how to train POS tagger to return verb for conjugated verbs.</p>

<p>'kochst' just an example,  I guess there a lot other verbs cannot be recognized</p>

<pre><code>&gt;&gt;&gt; tagger.tag(['fahre'])
[('fahre', u'XY')]
&gt;&gt;&gt; tagger.tag(['musst'])
[('musst', u'PPER')]
</code></pre>
"
"55030231","Python how to extract all list elements if they start with the same character set","2019-03-06 18:50:54","0","44","0","1","","55030408","<p>i have to split a list of POS tagged words into sublists according 
to the POS tag used.
My list looks like this:</p>

<pre><code>List=["", -&gt; ','"", "". -&gt; '!'"", "". -&gt; '.'"", "". -&gt; '?'"", ""CC -&gt; 'but'"", ""CD -&gt; 'hundred'"",
      ""CD -&gt; 'one'"", ""DT -&gt; 'the'"", ""EX -&gt; 'There'"",""IN -&gt; 'as'"", ""IN -&gt; 'because'"",
      ""IN -&gt; 'if'"", ""IN -&gt; 'in'"", ""JJ -&gt; 'Sure'"", 'MD -&gt; ""\'ll""', ""MD -&gt; 'ca'"",
      ""MD -&gt; 'can'"", ""MD -&gt; 'will'"", ""MD -&gt; 'would'"", ""NN -&gt; 'Applause'"",
      ""NN -&gt; 'anybody'"", ""NN -&gt; 'doubt'"", ""NNP -&gt; 'Syria'"",
      ""NNS -&gt; 'Generals'"", ""NNS -&gt; 'people'"", ""NNS -&gt; 'states'"",  ""PRP -&gt; 'it'"",
      ""PRP$ -&gt; 'our'"",  ""RB -&gt; 'there'"", ""RBR -&gt; 'more'"", ""RP -&gt; 'out'"", ""TO -&gt; 'to'"",
      ""UH -&gt; 'Oh'"", ""UH -&gt; 'Wow'"", ""VB -&gt; 'stop'"", ""VB -&gt; 'want'"", ""VBD -&gt; 'knew'"",
      ""VBD -&gt; 'was'"", ""VBG -&gt; 'allowing'"", ""VBG -&gt; 'doing'"", ""VBG -&gt; 'going'"",
      ""VBN -&gt; 'called'"", ""VBP -&gt; 'take'"", 'VBZ -&gt; ""\'s""', ""VBZ -&gt; 'is'"", 
      ""WDT -&gt; 'that'"", ""WP -&gt; 'what'""]
</code></pre>

<p>My desired output would be something like</p>

<pre><code>[[""IN -&gt; 'as'"", ""IN -&gt; 'because'"", ""IN -&gt; 'if'"", ""IN -&gt; 'in'""],[""UH -&gt; 'Oh'"", ""UH -&gt; 'Wow'""]]
</code></pre>

<p>or even better</p>

<pre><code>CC = ['but']
CD = ['hundred', 'one']
</code></pre>

<p>I searched quite a lot but the only function i could find that at least partly does the work is this:</p>

<pre><code>from itertools import groupby
print([list(g) for k, g in groupby(List, key=lambda x: x[0])])
</code></pre>

<p>I have played around with the value of x, but norhing seems to work very well.</p>

<p>I also thaught about using something like this:</p>

<pre><code>RB = []
for item in List:
    if item.startswith('RB'):
        g=re.findall('-&gt; (.*)', item)
        RB.append(g)
</code></pre>

<p>That should certainly work, but it would be a pain to do this for the about 40 different POS tags. There must be a simpler way.</p>
"
"55017747","how to save a parse tree in txt file in python","2019-03-06 07:30:02","0","776","0","1","","55018475","<p>I am having a problem whenever I try to save a parse tree which is generated from some set of rules which I have specified. all I want to save that parse tree in a new text file. when I do this it creates a new text file but it is empty and my code gives an error</p>

<blockquote>
  <p>'charmap' codec can't encode characters in position 12-18: character
  maps to </p>
</blockquote>

<p>here is my code</p>

<pre><code>sen = ""Heyy Jack whats up""
sent = word_tokenize(sen)
gram = (""""""
S -&gt; NP VP
NP -&gt; NU | N N
VP -&gt; NP NP
"""""")
grammar1 = nltk.cfg.fromstring(gram)
sr_parser = nltk.RecursiveDescentParser(grammar1)
for tree in sr_parser.parse(sent):    
    print(tree)
    values = tree
    with open(""new.txt"", ""w"") as output: ## creates new file but empty
        output.write(str(values))
</code></pre>
"
"54874069","NLP: Stemming on opcodes data set","2019-02-25 20:22:47","0","147","0","1","","54874271","<p>I have a dataset of 27 files, each containing opcodes. I want to use stemming to map all versions of similar opcodes into the same opcode. For example: push, pusha, pushb, etc would all be mapped to push; 
addf addi to add, multi multf to mult, etc.). How can I do so? I tried using PorterStemmer with NLTK extensions but it is not working on my dataset. I think it works only on normal human lingual words. (Like played, playing --> play) and not on these opcodes like (pusha, pushb --> push).</p>
"
"54847574","Combining TF-IDF with pre-trained Word embeddings","2019-02-24 00:21:31","5","5659","4","1","","55022104","<p>I have a list of website meta-description (128k descriptions; each with avg. 20-30 words), and am trying to build a similarity ranker (as in: show me the 5 most similar sites to this site meta description)</p>
<p><strong>It worked AMAZINGLY well with TF-IDF uni- and bigram</strong>, and I thought that I could additionally improve it by adding pre-trained word embeddings (spacy &quot;en_core_web_lg&quot; to be exact). <strong>Plot twist: it does not work at all</strong>. Literally did not get one good guess, and its suddenly spits out completely random suggestions.</p>
<p>Below is my code. Any thoughts on where I might have gone wrong? Am I overseeing something highly intuitive?</p>

<pre class=""lang-python prettyprint-override""><code>import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
import sys
import pickle
import spacy
import scipy.sparse
from scipy.sparse import csr_matrix
import math
from sklearn.metrics.pairwise import linear_kernel
nlp=spacy.load('en_core_web_lg')


&quot;&quot;&quot; Tokenizing&quot;&quot;&quot;
def _keep_token(t):
    return (t.is_alpha and 
            not (t.is_space or t.is_punct or 
                 t.is_stop or t.like_num))
def _lemmatize_doc(doc):
    return [ t.lemma_ for t in doc if _keep_token(t)]

def _preprocess(doc_list):     
    return [_lemmatize_doc(nlp(doc)) for doc in doc_list]
def dummy_fun(doc):
    return doc

# Importing List of 128.000 Metadescriptions:
Web_data=open(&quot;./data/meta_descriptions&quot;,&quot;r&quot;, encoding=&quot;utf-8&quot;)
All_lines=Web_data.readlines()
# outputs a list of meta-descriptions consisting of lists of preprocessed tokens:
data=_preprocess(All_lines) 

# TF-IDF Vectorizer:    
vectorizer = TfidfVectorizer(min_df=10,tokenizer=dummy_fun,preprocessor=dummy_fun,)
    tfidf = vectorizer.fit_transform(data)    
dictionary = vectorizer.get_feature_names()

# Retrieving Word embedding vectors:
temp_array=[nlp(dictionary[i]).vector for i in range(len(dictionary))]

# I had to build the sparse array in several steps due to RAM constraints
# (with bigrams the vocabulary gets as large as &gt;1m 
dict_emb_sparse=scipy.sparse.csr_matrix(temp_array[0])
for arr in range(1,len(temp_array),100000):
    print(str(arr))        
    dict_emb_sparse=scipy.sparse.vstack([dict_emb_sparse, scipy.sparse.csr_matrix(temp_array[arr:min(arr+100000,len(temp_array))])])

# Multiplying the TF-IDF matrix with the Word embeddings: 
tfidf_emb_sparse=tfidf.dot(dict_emb_sparse)

# Translating the Query into the TF-IDF matrix and multiplying with the same Word Embeddings:
query_doc= vectorizer.transform(_preprocess([&quot;World of Books is one of the largest online sellers of second-hand books in the world Our massive collection of over million cheap used books also comes with free delivery in the UK Whether it s the latest book release fiction or non-fiction we have what you are looking for&quot;]))
query_emb_sparse=query_doc.dot(dict_emb_sparse)

# Calculating Cosine Similarities:
cosine_similarities = linear_kernel(query_emb_sparse, tfidf_emb_sparse).flatten()

related_docs_indices = cosine_similarities.argsort()[:-10:-1]

# Printing the Site descriptions with the highest match:    
for ID in related_docs_indices:
    print(All_lines[ID])
</code></pre>
<p>I stole parts of the code/logic from <a href=""https://github.com/crownpku/text2vec"" rel=""nofollow noreferrer"">this Github</a> Rep
Does anybody see any straightforward errors here?
Many thanks!!</p>
"
"54784287","NLTK WordNetLemmatizer processes ""US"" as ""u""","2019-02-20 10:37:17","3","1187","0","1","","54788211","<p>If you feed the word <code>""US""</code> (United States), after preprocessing (which becomes <code>""us""</code>, i.e in lower case) into the <code>WordNetLemmatizer</code> from package <code>nltk.stem</code>, it is translated to <code>""u""</code>. For example:</p>

<pre><code>from nltk.stem import WordNetLemmatizer
lmtzr = WordNetLemmatizer()
word = ""US"".lower()  #  ""US"" becomes ""us""
lemma = lmtzr.lemmatize(word)
print(lemma)  # prints ""u""
</code></pre>

<p>I have even tried to lemmatize the word using POS tagging, which results in an <code>'NNP'</code> (NN=Noun and P=Proper, i.e proper noun) according to the <code>pos_tag()</code> function from package <code>nltk</code>. But <code>'NNP'</code> is a <code>wordnet.NOUN</code>, which is the default behavior of the lemmatizer when it processes a word. Therefore, <code>lmtzr.lemmatize(word)</code> and <code>lmtz.lemmatize(word, wordnet.NOUN)</code> is the same (where <code>wordnet</code> is imported from package <code>nltk.stem.wordnet</code>).</p>

<p>Any ideas about how to tackle this problem, apart from the clumsy way of explicitly excluding the processing of the word <code>""us""</code> in a text from the lemmatizer using an <code>if</code> statement?</p>
"
"54745482","what is the difference between tfidf vectorizer and tfidf transformer","2019-02-18 10:45:53","7","11982","2","4","","54748136","<p>I know that the formula for <code>tfidf vectorizer</code> is </p>

<pre><code>Count of word/Total count * log(Number of documents / no.of documents where word is present)
</code></pre>

<p>I saw there's tfidf transformer in the scikit learn and I just wanted to difference between them. I could't find anything that's helpful.</p>
"
"54735204","NLTK - generate text from probabilistic context free grammar (PCFG)","2019-02-17 16:27:26","2","1786","0","1","","54737936","<p>I have a context free grammar and use it to create sentences (using NLTK in python).</p>

<pre><code># Create a CFG
from nltk import CFG
from nltk.parse.generate import generate
grammar = CFG.fromstring(""""""
Story -&gt; Introduction MainQuest End
LocationInfo -&gt; 'He found himself in a small village where he grew up.'
Introduction -&gt; 'Long ago there was a boy who decided to become a knight.'

MainQuest -&gt; LocationInfo 'He had to get a sword first to fight monsters' Navigate

Navigate -&gt; '[He could go west]' GoodEnd | '[He could go east]' BadEnd
GoodEnd -&gt; 'And he lived happily ever after.'
BadEnd -&gt; 'Finally he died painfully.'
End -&gt; 'The End'
"""""")

#print(grammar.start())
#print(grammar.productions())
for sentence in generate(grammar, n=2):
    print('\n'.join(sentence))
    print('\n')
</code></pre>

<p>This is easy and works. But now, I'd like to add probabilities to special cases so that my generated story can either have a good or a bad ending, based on a random factor with given probabilities.</p>

<p>I can not find any example to do so and when I feed my PCFG into nltk.parse.generate it treats it like a CFG.</p>

<p>Hope you can help me out!</p>
"
"54660886","tf-idf vectorizer for multi-label classification problem","2019-02-13 00:49:13","2","966","0","1","","54661544","<p>I have a multi-label classification project for a large number of texts. 
I used the tf-Idf vectorizer on the texts (train_v['doc_text']) as follows:</p>

<pre><code>tfidf_transformer = TfidfTransformer()
X_counts = count_vect.fit_transform(train_v['doc_text']) 
X_tfidf = tfidf_transformer.fit_transform(X_counts) 
x_train_tfidf, x_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(X_tfidf_r, label_vs, test_size=0.33, random_state=9000)
sgd = SGDClassifier(loss='hinge', penalty='l2', random_state=42, max_iter=25, tol=None, fit_intercept=True, alpha = 0.000009  )
</code></pre>

<p>now, I need to use the same vectorizer on a set of features (test_v['doc_text'])to predict the labels.
however, when I use the following </p>

<pre><code>X_counts_test = count_vect.fit_transform(test_v['doc_text']) 
X_tfidf_test = tfidf_transformer.fit_transform(X_counts_test) 
predictions_test = clf.predict(X_tfidf_test)
</code></pre>

<p>I get an error message </p>

<pre><code>ValueError: X has 388894 features per sample; expecting 330204
</code></pre>

<p>any idea on how to deal with this?</p>

<p>Thanks. </p>
"
"54636433","nlp multilabel classification tf vs tfidf","2019-02-11 17:56:26","4","349","0","1","","54661633","<p>I am trying to solve an NLP multilabel classification problem. I have a huge amount of documents that should be classified into 29 categories. </p>

<p>My approach to the problem was, after cleaning up the text, stop word removal, tokenizing etc., is to do the following:</p>

<p>To create the features matrix I looked at the frequency distribution of the terms of each document, I then created a table of these terms (where duplicate terms are removed), I then calculated the term frequency for each word in its corresponding text (<code>tf</code>). So, eventually I ended up with around a 1000 terms and their respected frequency in each document. </p>

<p>I then used <code>selectKbest</code> to narrow them down to around 490. and after scaling them I used OneVsRestClassifier(<code>SVC</code>) to do the classification. </p>

<p>I am getting an <code>F1 score</code> around <code>0.58</code> but it is not improving at all and I need to get <code>0.62</code>. </p>

<p>Am I handling the problem correctly? </p>

<p>Do I need to use <code>tfidf vectorizer</code> instead of <code>tf</code>, and how? </p>

<p>I am very new to NLP and I am not sure at all what to do next and how to improve the score. </p>

<p>Any help in this subject is priceless. </p>

<p>Thanks</p>
"
"54628104","Why Parse Tree is not generated in my code for my sentences?","2019-02-11 10:08:24","0","439","0","1","","54636721","<p>I am trying to create a context free grammar for general english sentences using python 3.7 and different nltk libraries. The code runs successfully, without any error for all the sentences.  But the tree isn't generated for all the sentences.
For example:<br>
In following case:  </p>

<pre class=""lang-none prettyprint-override""><code>q1 = ""I shot an elephant in my pajamas""  
q2 = ""Big Data is huge unstructured type of data""  
</code></pre>

<p>Parse tree is generated for q1 and not for q2. And I don't understand why!  </p>

<pre class=""lang-python prettyprint-override""><code>st1=""""""
S -&gt; NP VP
NP -&gt; NNP|Det N|Det N PP
NN -&gt; JJ NN|N
VP -&gt; V JJ|V  NP|V S|V NP PP
PP -&gt; P NP
""""""+NNP+""\n""+Det+""\n""+N+""\n""+JJ+""\n""+V+""\n""+P

grammar1 = nltk.CFG.fromstring(st1)
sent = q1.split()
rd_parser = nltk.RecursiveDescentParser(grammar1)
for tree in rd_parser.parse(sent):
    print(tree)

print(st1)
print(grammar1)
</code></pre>

<p>Following is an output for q1. And same kind of parse tree needs to be generated for q2.</p>

<pre class=""lang-none prettyprint-override""><code>(S
  (NP (Det ) (N I))
  (VP
    (V shot)
    (NP (Det an) (N elephant) (PP (P in) (NP (Det my) (N pajamas))))))
(S
  (NP (Det ) (N I))
  (VP
    (V shot)
    (NP (Det an) (N elephant))
    (PP (P in) (NP (Det my) (N pajamas)))))
</code></pre>

<p>You can see complete code <a href=""https://drive.google.com/file/d/1cfrpMR_tfVOE2IOpZ6Il4-qsizV0pirc/view?usp=sharing"" rel=""nofollow noreferrer"">here</a></p>
"
"54613100","Parse Parts of Speech Tagged Tree Corpus with Python without NLTK","2019-02-10 03:30:30","0","227","4","1","","54618624","<p>I have tree corpus as below</p>

<pre><code>(TOP END_OF_TEXT_UNIT)

(TOP (S (NP (DT The)
            (NNP Fulton)
            (NNP County)
            (NNP Grand)
            (NNP Jury))
        (VP (VBD said)
            (NP (NNP Friday))
            (SBAR (-NONE- 0)
                  (S (NP (DT an)
                         (NN investigation)
                         (PP (IN of)
                             (NP (NP (NNP Atlanta))
                                 (POS 's)
                                 (JJ recent)
                                 (JJ primary)
                                 (NN election))))
                     (VP (VBD produced)
                         (NP (`` ``)
                             (DT no)
                             (NN evidence)
                             ('' '')
                             (SBAR (IN that)
                                   (S (NP (DT any)
                                          (NNS irregularities))
                                      (VP (VBD took)
                                          (NP (NN place)))))))))))
     (. .))
</code></pre>

<p>I need to parse this tree and convert into a sentence form as below </p>

<pre><code>DT The NNP Fulton NNP County NNP Grand NNP Jury VBD said NNP Friday DT
an NN investigation ...
</code></pre>

<p>Is there any algorithm to parse the above content or we need to use regular expressions to do this and I do not want to use NLTK packages to do this. </p>
"
"54429050","Find all potential similar documents out of a list of documents using clustering","2019-01-29 20:24:11","3","358","0","1","","54434465","<p>I'm working with the quora question pairs csv file which I loaded into a pd dataframe and isolated the qid and question so my questions are in this form : </p>

<pre><code>0        What is the step by step guide to invest in sh...
1        What is the step by step guide to invest in sh...
2        What is the story of Kohinoor (Koh-i-Noor) Dia...
3        What would happen if the Indian government sto...
.....
19408    What are the steps to solve this equation: [ma...
19409                           Is IMS noida good for BCA?
19410              How good is IMS Noida for studying BCA?
</code></pre>

<p>My dataset is actually bigger (500k questions) but I will use these questions to showcase my problem.</p>

<p>I want to identify pairs of questions that have a high probability of asking the same thing. I thought about the naive way, which is to turn each sentence into a vector using doc2vec and then for each sentence calculate the cosine similarity with every other sentence. Then, keep the one with the highest similarity and in the end print all those that have a high enough cosine similarity. The problem is this would take ages to finish so I need another approach.</p>

<p>Then I found an answer in another question that suggests to use clustering to solve a similar problem. So following is the code I implemented based on that answer.</p>

<pre><code>""Load and transform the dataframe to a new one with only question ids and questions""
train_df = pd.read_csv(""test.csv"", encoding='utf-8')

questions_df=pd.wide_to_long(train_df,['qid','question'],i=['id'],j='drop')
questions_df=questions_df.drop_duplicates(['qid','question'])[['qid','question']]
questions_df.sort_values(""qid"", inplace=True)
questions_df=questions_df.reset_index(drop=True)

print(questions_df['question'])

# vectorization of the texts
vectorizer = TfidfVectorizer(stop_words=""english"")
X = vectorizer.fit_transform(questions_df['question'].values.astype('U'))
# used words (axis in our multi-dimensional space)
words = vectorizer.get_feature_names()
print(""words"", words)


n_clusters=30
number_of_seeds_to_try=10
max_iter = 300
number_of_process=2 # seads are distributed
model = KMeans(n_clusters=n_clusters, max_iter=max_iter, n_init=number_of_seeds_to_try, n_jobs=number_of_process).fit(X)

labels = model.labels_
# indices of preferable words in each cluster
ordered_words = model.cluster_centers_.argsort()[:, ::-1]

print(""centers:"", model.cluster_centers_)
print(""labels"", labels)
print(""intertia:"", model.inertia_)

texts_per_cluster = numpy.zeros(n_clusters)
for i_cluster in range(n_clusters):
    for label in labels:
        if label==i_cluster:
            texts_per_cluster[i_cluster] +=1

print(""Top words per cluster:"")
for i_cluster in range(n_clusters):
    print(""Cluster:"", i_cluster, ""texts:"", int(texts_per_cluster[i_cluster])),
    for term in ordered_words[i_cluster, :10]:
        print(""\t""+words[term])

print(""\n"")
print(""Prediction"")

text_to_predict = ""Why did Donald Trump win the elections?""
Y = vectorizer.transform([text_to_predict])
predicted_cluster = model.predict(Y)[0]
texts_per_cluster[predicted_cluster]+=1

print(text_to_predict)
print(""Cluster:"", predicted_cluster, ""texts:"", int(texts_per_cluster[predicted_cluster])),
for term in ordered_words[predicted_cluster, :10]:
    print(""\t""+words[term])
</code></pre>

<p>I thought that this way I could find for each sentence the cluster that it most likely belongs in and then calculate the cosine similarity between all other questions of that cluster. This way instead of doing it on all the dataset I will be doing it on far fewer documents. However using the code for an example sentence ""Why did Donald Trump win the elections?"" I have the following results.</p>

<pre><code>Prediction
Why did Donald Trump win the elections?
Cluster: 25 texts: 244
    trump
    donald
    clinton
    hillary
    president
    vote
    win
    election
    did
    think
</code></pre>

<p>I know that my sentence belongs to cluster 25 and I can see the top words for that cluster. However how could I access the sentences that are in this cluster. Is there any way to do it?</p>
"
"54305070","Lime explainer shows prediction probabilities different to the classifier prediction - sentiment analysis","2019-01-22 09:28:32","1","1330","0","1","","54325344","<p>I am using Lime to trace the behavior behind why the model take his decision  to predict if this sentence is (NEG, POS or NEUTRAL) and for the most of cases lime explain correctly but in case like this why i entered NEG sentence, the model predict it as NEUTRAL but Lime visualize it with NEG highest percentage, so why i got logical error like this?</p>

<p><a href=""https://i.sstatic.net/ExsPP.png"" rel=""nofollow noreferrer"">Model prediction vs Lime prediction</a></p>
"
"54072496","Explaining CNN (Keras) outputs with LIME","2019-01-07 10:24:08","6","3415","0","2","","54706978","<p>I am trying to explain the outputs of my convolutional neural network bult in Keras with <a href=""https://github.com/marcotcr/lime"" rel=""noreferrer"">LIME</a>.</p>

<p>My neural network is a multi-class text classifier where every class is independent. Thus, a text can contain class 1 and 2 or only 1 etc. A fifth ""class"" (None) for cases where no classes are in the text.</p>

<p>However, while i managed to explain a binary classification case with Keras and Lime, I just cannot get the multi-class case with independent classes. A first help was found <a href=""https://github.com/marcotcr/lime/issues/267"" rel=""noreferrer"">here</a>: </p>

<p>However, my code does not work, I get internal errors from Lime such as: ""ValueError: Found input variables with inconsistent numbers of samples: [5000, 100000]""</p>

<pre><code>from lime.lime_text import LimeTextExplainer, TextDomainMapper
explainer = LimeTextExplainer(class_names=encoder.classes_)


chosen_text = 2

def flatten_predict(i):
    global model   
    # catch single string inputs and convert them to list
    if i.__class__ != list:
        i = [i]
        print(""## Caught and transformed single string."")
    # list for predictions
    predStorage = []
    # loop through input list and predict
    for textInput in i:
        textInput = preprocess(textInput)
        textInput = make_predictable(textInput)
        pred = model.predict(textInput)
        pred = np.append(pred, 1-pred, axis=1)
        # control output of function

        predStorage.extend(pred)
    return np.asarray(predStorage)


def get_predict_proba_fn_of_class(label):
    """"""assuming wrapped_predict outputs an (n, d) array of prediction probabilities, where d is the number of labels""""""
    def rewrapped_predict(strings): 
        preds = flatten_predict(strings)[:, np.where(flatten_predict(strings)==label)].reshape(-1, 1)
        ret = np.asarray(np.hstack([(1 - preds), preds]))
        return ret

    return rewrapped_predict

str = 'Ein sehr freundlicher Arzt.'
preds = flatten_predict(str)
labels_to_explain = preds# 
print(labels_to_explain)

explanation_for_label = {}
for label in labels_to_explain:
    wrapped = get_predict_proba_fn_of_class(label)
    explanation_for_label[label] = explainer.explain_instance(str, wrapped)
    explanation_for_label[label].show_in_notebook()
</code></pre>

<p>Error Message: </p>

<pre><code>ValueError                                Traceback (most recent call last)
&lt;ipython-input-26-8df61aaa23f4&gt; in &lt;module&gt;()
     53 for label in labels_to_explain:
     54     wrapped = get_predict_proba_fn_of_class(label)
---&gt; 55     explanation_for_label[label] = explainer.explain_instance(str, wrapped)
     56     explanation_for_label[label].show_in_notebook()
     57 

/usr/local/lib/python3.6/dist-packages/lime/lime_text.py in explain_instance(self, text_instance, classifier_fn, labels, top_labels, num_features, num_samples, distance_metric, model_regressor)
    405                 data, yss, distances, label, num_features,
    406                 model_regressor=model_regressor,
--&gt; 407                 feature_selection=self.feature_selection)
    408         return ret_exp
    409 

/usr/local/lib/python3.6/dist-packages/lime/lime_base.py in explain_instance_with_data(self, neighborhood_data, neighborhood_labels, distances, label, num_features, feature_selection, model_regressor)
    155                                                weights,
    156                                                num_features,
--&gt; 157                                                feature_selection)
    158 
    159         if model_regressor is None:

/usr/local/lib/python3.6/dist-packages/lime/lime_base.py in feature_selection(self, data, labels, weights, num_features, method)
    104                 n_method = 'highest_weights'
    105             return self.feature_selection(data, labels, weights,
--&gt; 106                                           num_features, n_method)
    107 
    108     def explain_instance_with_data(self,

/usr/local/lib/python3.6/dist-packages/lime/lime_base.py in feature_selection(self, data, labels, weights, num_features, method)
     78             clf = Ridge(alpha=0, fit_intercept=True,
     79                         random_state=self.random_state)
---&gt; 80             clf.fit(data, labels, sample_weight=weights)
     81             feature_weights = sorted(zip(range(data.shape[0]),
     82                                          clf.coef_ * data[0]),

/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/ridge.py in fit(self, X, y, sample_weight)
    678         self : returns an instance of self.
    679         """"""
--&gt; 680         return super(Ridge, self).fit(X, y, sample_weight=sample_weight)
    681 
    682 

/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/ridge.py in fit(self, X, y, sample_weight)
    489 
    490         X, y = check_X_y(X, y, ['csr', 'csc', 'coo'], dtype=_dtype,
--&gt; 491                          multi_output=True, y_numeric=True)
    492 
    493         if ((sample_weight is not None) and

/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py in check_X_y(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)
    764         y = y.astype(np.float64)
    765 
--&gt; 766     check_consistent_length(X, y)
    767 
    768     return X, y

/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py in check_consistent_length(*arrays)
    233     if len(uniques) &gt; 1:
    234         raise ValueError(""Found input variables with inconsistent numbers of""
--&gt; 235                          "" samples: %r"" % [int(l) for l in lengths])
    236 
    237 

ValueError: Found input variables with inconsistent numbers of samples: [5000, 100000]
</code></pre>

<p>Does anyone know what I am doing wrong? I am pretty sure it has to do with the <strong>input format</strong>.</p>
"
"54028477","calculating tf_idf for fvt table","2019-01-03 19:16:34","0","94","7","1","","54054933","<p>I have a frequency value table like-</p>

<pre><code>    a   b           
1   3   0                   
2   0   3                   
3   4   5                   
</code></pre>

<p>and I want to calculate the tf_idf.</p>

<p>My code-</p>

<pre><code>l=len(data)
for doc in data:

m=data.groupby(doc).apply(lambda column: column.sum()/(column != 0).sum())
for i in range(l):
    tf=print(data.loc[i,doc])
    idf=log(l/m)                  
    weight=tf*idf
    data.loc[i,doc]=weight
</code></pre>

<p>Explanation-
First I am iterating through each column where I am finding the non zero rows in that column in var m and storing the particular value of that row in column as tf and then calculating the tf_idf and replacing the values in table with tf_idf weights.</p>

<p>expected output-</p>

<p>for column g first row we have tf=3 idf=log(5/4) therefore tf_idf=idf*tf</p>

<pre><code>    a   b           
1   0.4 0                   
2   0   0.4                 
3   0.17 .22                    
</code></pre>
"
"53972614","Is it possible to use typeclasses to change `ReaderT (IO a) IO a` into `ReaderT (i a) IO a`?","2018-12-29 19:13:57","2","106","4","1","","53975238","<p>I'm learning Haskell and I've got the following code thanks to help from <a href=""https://stackoverflow.com/questions/53939191/how-to-flatten-io-io/53939660#53939660"">this answer</a>, which is just an <code>echo</code> program. It works great, but I want to make some improvements to it and am running into trouble.</p>

<pre><code>userInput :: MonadIO m =&gt; ReaderT (IO String) m String
userInput = ask &gt;&gt;= liftIO -- this liftIO eliminates your need for join

echo :: MonadIO m =&gt; ReaderT (IO String) m ()
echo = userInput &gt;&gt;= liftIO . putStrLn -- this liftIO is just so you can use putStrLn in ReaderT

main :: IO ()
main = runReaderT echo getLine
</code></pre>

<p>What I'd like to do is change <code>ReaderT (IO String)</code> to <code>ReaderT (i String)</code> and make it more general so I can swap it out for unit testing. The problem is, because we use <code>liftIO</code> inside <code>userInput</code> it sort of <em>ties</em> <code>i</code> together with <code>IO</code>. Is there some way to replace <code>liftIO</code> with something else to make the following code work?</p>

<pre><code>class Monad i =&gt; MonadHttp i where
  hole :: MonadIO m =&gt; i a -&gt; ReaderT (i a) m a

instance MonadHttp IO where
  hole = liftIO

newtype MockServer m a = MockServer
  { server :: ReaderT (String) m a }
  deriving (Applicative, Functor, Monad, MonadTrans)

instance MonadIO m =&gt; MonadHttp (MockServer m) where
  -- MockServer m a -&gt; ReaderT (MockServer m a) m1 a
  hole s = s -- What goes here?

userInput :: (MonadHttp i, MonadIO m) =&gt; ReaderT (i String) m String
userInput = ask &gt;&gt;= hole

echo :: (MonadHttp i, MonadIO m) =&gt; ReaderT (i String) m ()
echo = userInput &gt;&gt;= \input -&gt;
         ((I.liftIO . putStrLn) input)

main = runReaderT echo (return ""hello"" :: MockServer IO String)
</code></pre>
"
"53939191","How to flatten IO (IO ())?","2018-12-27 02:41:11","3","783","3","2","","53939660","<p>I'm just learning Haskell and monad transformers and I've found myself with an IO (IO ()) that I'd like to flatten into just IO (). I'm sure that I'm doing something wrong, but can't pinpoint exactly where I'm getting lost.</p>

<p>Here is a simplified example of what I'm trying to do. This is a convoluted way of implementing <code>echo</code>, but it illustrates the problem.</p>

<pre><code>userInput :: Monad m =&gt; ReaderT (IO String) m (IO String)
userInput = ask

echo :: Monad m =&gt; ReaderT (IO String) m (IO ())
echo = userInput &gt;&gt;= \input -&gt;  -- unwrap ReaderT to get an IO String
         input &gt;&gt;= (\s -&gt;       -- unwrap IO String to get a String
           putStrLn s)          -- print out the String
         &amp; return               -- rewrap into a ReaderT

main :: IO (IO ())              -- How to turn IO (IO ()) to IO ()?
main = runReaderT echo getLine
</code></pre>

<p>In my real application, I have a <a href=""https://www.spock.li/"" rel=""nofollow noreferrer"">Spock</a> app that makes HTTP requests to an upstream server. Spock apps use a monad transformer stack called <code>SpockCtxT</code> and I'd like to insert a <code>ReaderT</code> into the stack to abstract the HTTP request so that I can swap it out for a mock implementation in my tests. </p>

<p>Fundamentally, the idea is a monad transformer stack where one of the transformers gives you an <code>IO</code> whether it be an HTTP request or <code>getLine</code>. Am I thinking about this incorrectly or is there some way to do this?</p>
"
"53926860","using type classes to provide alternative implementations for when using Acid-State","2018-12-26 02:23:05","0","79","3","1","","53934116","<p>I wrote a web application using scotty and acid state, now i would like to use type classes to be able to provide alternative implementations for the capabilities of my application for testing.
I get the general idea of it and am able to apply it so simple examples but since im am using acid state there are a lot of type classes and template haskell involved which i am not entirely comfortable with yet.</p>

<p>so i have these straight-forward classes for the different capabilities</p>

<pre><code>class Logging m where
  log :: T.Text -&gt; m ()

class Server m where
  body :: m B.ByteString
  respond :: T.Text -&gt; m ()
  setHeader :: T.Text -&gt; T.Text -&gt; m ()

class Db m where
  dbQuery :: (MethodState event ~ Database,QueryEvent event) =&gt; event -&gt; m (EventResult event)
  dbUpdate :: (MethodState event ~ Database,UpdateEvent event) =&gt; event -&gt; m (EventResult event)
</code></pre>

<p>and i also provided instances for them for my ""production"" monad. </p>

<p>But when it comes to the database capability i cant get to work what i want.</p>

<p>the class looks like this</p>

<pre><code>class Db m where
  dbQuery :: (MethodState event ~ Database,QueryEvent event) =&gt; event -&gt; m (EventResult event)
  dbUpdate :: (MethodState event ~ Database,UpdateEvent event) =&gt; event -&gt; m (EventResult event)
</code></pre>

<p>and the instance for the production monad works fine since it only passes the event to the update and query functions of acid state, but for a test monad i would like to have something like this:
    instance Db Test where
      dbQuery (GetVersion) = use (testDb . clientVersion)
      dbQuery (GetUser name) = preuse (testDb . users . ix name)
      dbUpdate (PutUser name user) = users %= M.insert name user
      ...
so that I can match on GetVersion,GetUser etc. (which are generated by the template haskell function makeAcidic ... ) and specify how they should be handled in the test environment. </p>

<p>But I get the error:</p>

<pre><code>Could not deduce: event ~ GetVersion
from the context: (MethodState event ~ Database, QueryEvent event)
  bound by the type signature for:
              dbQuery :: (MethodState event ~ Database, QueryEvent event) =&gt;
                        event -&gt; Test (EventResult event)
  at Main.hs:88:3-9
‘event’ is a rigid type variable bound by
  the type signature for:
    dbQuery :: forall event.
                (MethodState event ~ Database, QueryEvent event) =&gt;
                event -&gt; Test (EventResult event)
  at Main.hs:88:3
• In the pattern: GetVersion
In an equation for ‘dbQuery’:
    dbQuery (GetVersion) = use (testDb . clientVersion)
In the instance declaration for ‘Db Test’
• Relevant bindings include
  dbQuery :: event -&gt; Test (EventResult event)
    (bound at Main.hs:88:3)
</code></pre>

<p>i guess thats because GetVersion, GetUser etc. all have a their different own types. So is there a way to do this?</p>

<hr>

<h2>Incorporating suggestions</h2>

<p>I tried the suggestions proposed by Peter Amidon but sadly it still doesnt compile here is my test code</p>

<pre><code>{-# LANGUAGE GADTs #-}               -- For type equality
{-# LANGUAGE TypeOperators #-}       -- For type equality
{-# LANGUAGE TypeFamilies #-}        -- For EventResult
{-# LANGUAGE ScopedTypeVariables #-} -- For writing castWithWitness
{-# LANGUAGE TypeApplications #-}    -- For convenience
{-# LANGUAGE TemplateHaskell #-}
{-# LANGUAGE ViewPatterns #-}
{-# LANGUAGE OverloadedStrings #-}

import Control.Lens
import Data.Acid
import qualified Data.Text.Lazy as T
import Types
import Data.Typeable

main = return ()

getUser :: Username -&gt; Query Database (Maybe User)
getUser name = preview (users . ix name)

getVersion :: Query Database T.Text
getVersion = view clientVersion

$(makeAcidic ''Database ['getUser,'getVersion])

castWithWitness :: forall b a. (Typeable a, Typeable b)
                =&gt; a -&gt; Maybe (b :~: a, b)
castWithWitness x = case eqT @a @b of
                      Nothing -&gt; Nothing
                      Just Refl -&gt; Just (Refl, x)

exampleFunction :: forall a. QueryEvent a =&gt; a -&gt; EventResult a
exampleFunction (castWithWitness @GetVersion -&gt; (Just Refl, Just GetVersion)) = ""1.0""
exampleFunction (castWithWitness @GetUser -&gt; (Just Refl, Just (GetUser n))) = Nothing
</code></pre>

<p>and here the error</p>

<pre><code>Main.hs:124:49: error:
    • Couldn't match expected type ‘Maybe
                                      (GetVersion :~: a, GetVersion)’
                  with actual type ‘(Maybe (t1 :~: t2), t0)’
    • In the pattern: (Just Refl, Just GetVersion)
      In the pattern:
        castWithWitness @GetVersion -&gt; (Just Refl, Just GetVersion)
      In an equation for ‘exampleFunction’:
          exampleFunction
            (castWithWitness @GetVersion -&gt; (Just Refl, Just GetVersion))
            = ""1.0""
    • Relevant bindings include
        exampleFunction :: a -&gt; EventResult a (bound at Main.hs:124:1)

Main.hs:124:61: error:
    • Couldn't match expected type ‘t0’
                  with actual type ‘Maybe GetVersion’
        ‘t0’ is untouchable
          inside the constraints: t2 ~ t1
          bound by a pattern with constructor:
                    Refl :: forall k (a :: k). a :~: a,
                  in an equation for ‘exampleFunction’
          at Main.hs:124:55-58
    • In the pattern: Just GetVersion
      In the pattern: (Just Refl, Just GetVersion)
      In the pattern:
        castWithWitness @GetVersion -&gt; (Just Refl, Just GetVersion)

Main.hs:125:46: error:
    • Couldn't match expected type ‘Maybe (GetUser :~: a, GetUser)’
                  with actual type ‘(Maybe (t4 :~: t5), t3)’
    • In the pattern: (Just Refl, Just (GetUser n))
      In the pattern:
        castWithWitness @GetUser -&gt; (Just Refl, Just (GetUser n))
      In an equation for ‘exampleFunction’:
          exampleFunction
            (castWithWitness @GetUser -&gt; (Just Refl, Just (GetUser n)))
            = Nothing
    • Relevant bindings include
        exampleFunction :: a -&gt; EventResult a (bound at Main.hs:124:1)

Main.hs:125:79: error:
    • Could not deduce: MethodResult a ~ Maybe a0
      from the context: t5 ~ t4
        bound by a pattern with constructor:
                  Refl :: forall k (a :: k). a :~: a,
                in an equation for ‘exampleFunction’
        at Main.hs:125:52-55
      Expected type: EventResult a
        Actual type: Maybe a0
      The type variable ‘a0’ is ambiguous
    • In the expression: Nothing
      In an equation for ‘exampleFunction’:
          exampleFunction
            (castWithWitness @GetUser -&gt; (Just Refl, Just (GetUser n)))
            = Nothing
    • Relevant bindings include
        exampleFunction :: a -&gt; EventResult a (bound at Main.hs:124:1)
</code></pre>
"
"53877017","Why is this TF-IDF sentiment analysis classifier performing so well?","2018-12-20 22:39:38","-1","1032","1","2","","53880176","<p><a href=""https://github.com/DenJev/NLPAirline/blob/master/AirlinesNotebook.ipynb"" rel=""nofollow noreferrer"">Jupter Notebook</a></p>

<p>The last confusion matrix is for the test set. Is this a case of overfitting with logistic regression? Because even when not pre-processing the text much (including emoticons, punctuation) the accuracy is still very good. Good anyone give some help/advice?</p>
"
"53876024","Parsing with Haskell/Megaparsec: StateT for building up local, lexical scope?","2018-12-20 21:04:39","3","549","6","1","","53877209","<p>So I'm trying to do the standard ""write yourself a parser for a scheme-like language"" exercise to figure out MegaParsec and monad transformers. Following the suggestions of many tutorials and blog posts, I'm using <code>ReaderT</code> and <code>local</code> to implement lexical scope.</p>

<p>I run into trouble trying to implement <code>let*</code>. Both <code>let</code> and <code>let*</code> share the same syntax, binding variables for use in a subsequent expression. The difference between the two is that <code>let*</code> lets you use a binding in subsequent ones, whereas <code>let</code> doesn't:</p>

<pre><code>(let ((x 1) (y 2)) (+ x y))       ; 3
(let* ((x 1) (y (+ x x)) (+ x y)) ; 3
(let ((x 1) (y (+ x x)) (+ x y))  ; Error unbound symbol ""x""
</code></pre>

<p>My problem is that when parsing a <code>let*</code> expression, I need to add the bindings to the current scope one-by-one so that each binding is available for use in the subsequent ones. This seems like a good use case for <code>StateT</code>; allowing me to build up the local scope one binding at a time.
Then, having parsed all the new bindings, I can pass these, together with those inherited from the parent scope, to the third argument of the <code>let*</code> expression, via <code>local</code>.</p>

<p>I build my monad transformer stack as follows:</p>

<pre><code>type Parser = Parsec Void String
type Env = Map.Map String Float
type RSParser = ReaderT Env (StateT Env Parser)
</code></pre>

<p>And here's the parser, simplified as much as I could while still making my point. In particular, <code>Float</code> is the only data type and <code>+</code>, <code>*</code>, and <code>let*</code> are the only commands.</p>

<pre><code>data Op = Plus | Times

spaceConsumer :: Parser ()
spaceConsumer = Lexer.space space1
                            (Lexer.skipLineComment "";"")
                            (Lexer.skipBlockComment ""#|"" ""|#"")
lexeme :: Parser a -&gt; RSParser a
lexeme = lift . lift . Lexer.lexeme spaceConsumer

lParen, rParen :: RSParser Char
lParen = lexeme $ char '('
rParen = lexeme $ char ')'

plus, times :: RSParser Op
plus = lexeme $ char '+' $&gt; Plus
times = lexeme $ char '*' $&gt; Times

keyValuePair :: RSParser ()
keyValuePair = between lParen rParen $ do
    state &lt;- get
    name  &lt;- lift . lift $ Lexer.lexeme spaceConsumer (some letterChar)
    x     &lt;- num
    modify (union (fromList [(name, x)]))

keyValuePairs :: RSParser ()
keyValuePairs = between lParen rParen (many keyValuePair) $&gt; ()

num :: RSParser Float
num = lexeme $ Lexer.signed (return ()) Lexer.float

expr, var :: RSParser Float
expr = num &lt;|&gt; var &lt;|&gt; between lParen rParen (arithExpr &lt;|&gt; letStarExpr)
var = do
    env &lt;- ask
    lift . lift $ do
        name &lt;- Lexer.lexeme spaceConsumer (some letterChar)
        case Map.lookup name env of
            Nothing -&gt; mzero
            Just x  -&gt; return x
arithExpr = do
    op   &lt;- (plus &lt;|&gt; times) &lt;?&gt; ""operation""
    args &lt;- many (expr &lt;?&gt; ""argument"")
    return $ case op of
        Plus  -&gt; sum args
        Times -&gt; product args
letStarExpr = lexeme (string ""let*"") *&gt; do
    keyValuePairs
    bindings &lt;- get
    local (Map.union bindings) expr

main :: IO ()
main = do
    parseTest (runStateT (runReaderT expr (fromList [(""x"", 1)])) Map.empty)
              ""(+ (let* ((x 666.0)) x) x)""
        -- (667.0,fromList [(""x"",666.0)]) Ok
    parseTest (runStateT (runReaderT expr (fromList [(""x"", 1)])) Map.empty)
              ""(+ (let* ((x 666.0)) x) (let* ((w 0.0)) x))""
        -- (1332.0,fromList [(""x"",666.0)]) Wrong
</code></pre>

<p>The first test above succeeds, but the second fails. It fails because the mutable state holding <code>x</code>'s binding in the first <code>let*</code> expression is carried over to the second <code>let*</code> expression. <strong>I need a way to make the this mutable state <em>local</em> to the computation in question and this is what I can't figure out how to do.</strong> Is there an analogue of the <code>local</code> command from <code>Reader</code> for <code>State</code>? Am I using the wrong monad transformer stack? Is my approach fundamentally flawed?</p>

<p>The naive (in retrospect) solution that I tried is resetting the mutable state at each <code>let*</code> expression by adding a <code>put Map.empty</code> statement to <code>letStarExpr</code>:</p>

<pre><code>letStarExpr = lexeme (string ""let*"") *&gt; do
    keyValuePairs
    bindings &lt;- get
    put Map.empty
    local (Map.union bindings) expr
</code></pre>

<p>But this is incompatible with nested <code>let*</code> expressions:</p>

<pre><code>parseTest (runStateT (runReaderT expr (fromList [(""x"", 1)])) Map.empty)
    (let* ( (x 666.0) (y (let* ((z 3.0)) z)) ) x)
</code></pre>

<p>gives 1.0 instead of 666.0.</p>

<p>Any ideas?</p>
"
"53841061","Python Pandas: NLTK Part of Speech Tagging for Entire Column in Dataframe","2018-12-18 21:06:15","0","1415","3","1","","53841530","<p>I have the following sample data frame shown below.  It has been tokenized already. </p>

<pre><code>No  category    problem_definition_stopwords
175 2521       ['coffee', 'maker', 'brewing', 'properly', '2', '420', '420', '420']
211 1438       ['galley', 'work', 'table', 'stuck']
912 2698       ['cloth', 'stuck']
572 2521       ['stuck', 'coffee']
</code></pre>

<p>I want to do part of speech tagging on this data frame.  Below is the beginning of my code.  It is erroring out:  </p>

<pre><code>from nltk.corpus import state_union
from nltk.tokenize import PunktSentenceTokenizer 

train_text = state_union.raw(df['problem_definition_stopwords'])
</code></pre>

<p>Error</p>

<pre><code>TypeError: join() argument must be str or bytes, not 'list'
</code></pre>

<p>My desired result is below where 'XXX' is a tokenized word and after it is the part of speech (i.e. NNP): </p>

<p>[('XXX', 'NNP'), ('XXX', 'VBD'), ('XXX', 'POS')]</p>
"
"53762436","Extracting only nouns from list of lists pos_tag sequence?","2018-12-13 12:59:04","4","1358","0","2","","53762797","<p>I am trying to extract only <strong>nouns</strong> using the <code>nltk.pos_tag()</code>, from a <code>list of lists text sequence</code>. I am able to extract all the nouns from the <code>nltk.pos_tag()</code> list, without preserving the list of lists sequence? How to achieve this by preserving the list of lists sequence. Any help is highly appreciated.</p>

<p>Here, list of lists text sequence collection means: collection of tokenized words separated by lists.</p>

<blockquote>
  <p>[[('icosmos', 'JJ'), ('cosmology', 'NN'), ('calculator', 'NN'), ('with', 'IN'), ('graph', 'JJ')], [('generation', 'NN'), ('the', 'DT'), ('expanding', 'VBG'), ('universe', 'JJ')], [('american', 'JJ'), ('institute', 'NN')]]</p>
</blockquote>

<p>The output should look like:</p>

<blockquote>
  <p>[['cosmology', 'calculator'], ['generation'], [institute]]</p>
</blockquote>

<p>What I have tried is as follows:</p>

<pre><code>def function1():
    tokens_sentences = sent_tokenize(tokenized_raw_data.lower())
    unfiltered_tokens = [[word for word in word_tokenize(word)] for word in tokens_sentences]
    word_list = []
    for i in range(len(unfiltered_tokens)):
        word_list.append([]) 
    for i in range(len(unfiltered_tokens)):
        for word in unfiltered_tokens[i]:
            if word[:].isalpha():
               word_list[i].append(word[:])
    tagged_tokens=[]
    for token in word_list:
        tagged_tokens.append(nltk.pos_tag(token))
    noun_tagged = [(word,tag) for word, tag in tagged_tokens 
            if tag.startswith('NN') or tag.startswith('NNPS')]
    print(nouns_tagged)
</code></pre>

<p>If I used the below mention code-shippet in the original code after appending tagged_tokens list, the output is displayed in a single list, which is not required.</p>

<pre><code>only_tagged_nouns = []
for sentence in tagged_tokens:
    for word, pos in sentence:
        if (pos == 'NN' or pos == 'NNPS'):
            only_tagged_nouns.append(word)
</code></pre>
"
"53755893","Facing AttributeError: for 'tag_' using Spacy in Python","2018-12-13 06:10:24","2","1280","0","1","","53760227","<p>I'm using Spacy for ""POS Tagging"" and getting below error. I have a dataframe, which has the column ""description"" in which I need to extract the POS for each word</p>

<p>Dataframe :</p>

<pre><code>No.      Description
1        My net is not working
2        I will be out for dinner
3        Can I order food
4        Wifi issue
</code></pre>

<p>Code :</p>

<pre><code>import pandas as pd
read_data = pd.read_csv('C:\\Users\\abc\\def\\pqr\\Data\\training_data.csv', encoding=""utf-8"")
entity = []
for parsed_doc in read_data['Description']:
    doc = nlp(parsed_doc)
    a = [(X.text, X.tag_) for X in doc.ents]
    entity.append(a)
</code></pre>

<p>The above code is throwing error:</p>

<blockquote>
  <p>Error : AttributeError: 'spacy.tokens.span.Span' object has no
  attribute 'tag_'</p>
</blockquote>

<p><strong>However, the same code is working fine for ""Label"" attribute and also if I use a single sentence</strong></p>

<pre><code>doc = nlp('can you please help me to install wifi')
for i in doc:
    print (i.text, i.tag_)
</code></pre>
"
"53753614","Cosine Similarity between keywords","2018-12-13 01:04:04","1","2049","2","1","","53771730","<p>I'm new to document similarity in python and I'm confused about how to go about working with some data. Basically, I want to get the cosine similarity between dicts containing keywords. </p>

<p>I have dicts like so, which I am getting straight from a database:</p>

<pre><code>{'hat': 0.12, 'cat': 0.33, 'sat': 0.45}
{'rat': 0.22, 'bat':0.98, 'cat': 0.01}
</code></pre>

<p>I query the database and I get back data in this format. These are each lists of keywords and their respective tf-idf scores/weights. </p>

<pre><code>{'keyword': tfidf_score}
</code></pre>

<p>All I want to do is get the cosine similarity between these two dicts, weighted by the tfidf score. Looking online, I was pretty overwhelmed by all the different python libraries/modules when it comes to document similarity. I have no idea if there is some built-in function out there that I can just pass these sorts of json objects to, if I should be writing my own function that uses the weights, or what. </p>

<p>Any help is appreciated!</p>

<p>Thank you!</p>
"
"53746225","Possible error with Stanford POS Tagger and classifying intent and the replies","2018-12-12 15:24:32","0","58","2","1","","53752907","<p>I have a specific usecase, where a person would say something like this: </p>

<ul>
<li><strong><em>""Hey (Trigger Word), note in object history XYZ""</em></strong> or:</li>
<li><strong><em>""Hey (Trigger Word), record in object diagnosis that PQR""</em></strong></li>
<li>(""object"" as used in the example is a placeholder and can be replaced with words like 'Maintenance/Patient', etc.)</li>
</ul>

<p>I would like to recognize the intent and the slots. </p>

<p>Then I use Stanford Parser to parse the sentence, e.g. parsing <strong><em>""Note in object history object was last updated in may twenty eighteen""</em></strong> gives this list-of-tuple:</p>

<pre><code>[('Note', 'VB'),
 ('in', 'IN'),
 ('object', 'NN'),
 ('history', 'NN'),
 ('object', 'NN'),
 ('was', 'VBD'),
 ('last', 'RB'),
 ('updated', 'VBN'),
 ('in', 'IN'),
 ('may', 'MD'),
 ('twenty', 'CD'),
 ('eighteen', 'CD')]
</code></pre>

<ol>
<li><p>Now, my point is how can I use this information to get the necessary output:</p>

<ul>
<li>Where to note <em>(we have a field in DB: Object History)</em> and</li>
<li>What to note <em>(object was last updated in may twenty eighteen)</em>.</li>
</ul></li>
<li><p>Another issue is since the input of the NLP is from an ASR system, the capitalization is missing. And the POS Tagger mis-tags 'note' as 'NN' (instead of 'VB'). Ideally 'note'/'record' should be a verb. How do I solve this probable error?</p></li>
</ol>
"
"53737431","Multi language Lemmatization in Python","2018-12-12 06:42:52","0","2864","0","1","","53739970","<p>I have a dataset in multiple languages. 
can I apply lemmatization according to its language?
I have already separated data according to its language.
Tried using WordNet lemmatization, but it only supports English language</p>

<p>For stemming in multiple languages, I am using snowballStemmer.</p>
"
"53692117","AttributeError: 'list' object has no attribute 'isdigit'. Specifying POS of each and every word in sentences list efficiently?","2018-12-09 12:02:57","1","1225","0","1","","53703473","<p>Suppose I am having lists of list of sentences (in a large corpus) as collections of tokenized words. The sample format is as follows:</p>

<p>The format of tokenized_raw_data is as follows:</p>

<pre><code>[['arxiv', ':', 'astro-ph/9505066', '.'], ['seds', 'page', 'on', '``', 
'globular', 'star', 'clusters', ""''"", 'douglas', 'scott', '``', 'independent', 
'age', 'estimates', ""''"", 'krysstal', '``', 'the', 'scale', 'of', 'the', 
'universe', ""''"", 'space', 'and', 'time', 'scaled', 'for', 'the', 'beginner',
 '.'], ['icosmos', ':', 'cosmology', 'calculator', '(', 'with', 'graph', 
'generation', ')', 'the', 'expanding', 'universe', '(', 'american', 
'institute', 'of', 'physics', ')']]
</code></pre>

<p>I want to apply the <code>pos_tag</code>.</p>

<p>What I have tried up to now is as follows.</p>

<pre><code>import os, nltk, re
from nltk.corpus import stopwords
from unidecode import unidecode
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.tag import pos_tag


def read_data():
    global tokenized_raw_data
    with open(""path//merge_text_results_pu.txt"", 'r', encoding='utf-8', errors = 'replace') as f:
        raw_data = f.read()
        tokenized_raw_data = '\n'.join(nltk.line_tokenize(raw_data))
read_data()

def function1():
    tokens_sentences = sent_tokenize(tokenized_raw_data.lower())
    unfiltered_tokens = [[word for word in word_tokenize(word)] for word in tokens_sentences]
    tagged_tokens = nltk.pos_tag(unfiltered_tokens)
    nouns = [word.encode('utf-8') for word,pos in tagged_tokens
            if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos ==  'NNPS')]
    joined_nouns_text = (' '.join(map(bytes.decode, nouns))).strip()
    noun_tokens = [t for t in wordpunct_tokenize(joined_nouns_text)]
    stop_words = set(stopwords.words(""english""))
function1()
</code></pre>

<p>I am getting the following error.</p>

<pre><code>&gt; AttributeError: 'list' object has no attribute 'isdigit'
</code></pre>

<p>Please help how to overcome this error in time-efficient manner? Where I am going wrong? </p>

<p>Note: I am using Python 3.7 on Windows 10.</p>
"
"53612938","spaCy NLP word.pos returns digits instead of POS tags","2018-12-04 12:23:51","3","219","0","1","","54382451","<p>I am using spaCy library for POS tagging but when I run this code, it returns numbers in the place of the pos tags:</p>
<pre><code>import spacy
from spacy.lang.fr.examples import sentences

nlp = spacy.load('en')
mystring = &quot; I am missing my lovely family a lot.&quot;
exuu = nlp(mystring)
for word in exuu: 
  print(word.text, word.pos)
</code></pre>
<p>Here is how the output looks like:</p>
<pre><code>102
I 94
am 99
missing 99
my 83
dear 83
family 91
a 89
lot 91
. 96
</code></pre>
"
"53582922","NLTK - statistics count extremely slow with big corpus","2018-12-02 17:43:07","0","231","0","1","","53583415","<p>I'd like to see basic statistics about my corpus like word/sentence counters, distributions etc.
I have a <code>tokens_corpus_reader_ready.txt</code> which contains 137.000 lines of tagged example sentences in this format:</p>

<blockquote>
  <p>Zur/APPRART Zeit/NN kostenlos/ADJD aber/KON auch/ADV nur/ADV 11/CARD kW./NN
  Zur/APPRART Zeit/NN anscheinend/ADJD kostenlos/ADJD ./$.<br>
  ...</p>
</blockquote>

<p>I also have a TaggedCorpusReader() which I have a describe() method for:</p>

<pre><code>class CSCorpusReader(TaggedCorpusReader):
  def __init__(self):
    TaggedCorpusReader.__init__(self, raw_corpus_path, 'tokens_corpus_reader_ready.txt')

    def describe(self):
    """"""
    Performs a single pass of the corpus and
    returns a dictionary with a variety of metrics
    concerning the state of the corpus.

    modified method from https://github.com/foxbook/atap/blob/master/snippets/ch03/reader.py
    """"""
    started = time.time()

    # Structures to perform counting.
    counts = nltk.FreqDist()
    tokens = nltk.FreqDist()

    # Perform single pass over paragraphs, tokenize and count
    for sent in self.sents():
        print(time.time())
        counts['sents'] += 1

        for word in self.words():
            counts['words'] += 1
            tokens[word] += 1

    return {
        'sents':  counts['sents'],
        'words':  counts['words'],
        'vocab':  len(tokens),
        'lexdiv': float(counts['words']) / float(len(tokens)),
        'secs':   time.time() - started,
    }
</code></pre>

<p>If I run the describe method like this in IPython:</p>

<pre><code>&gt;&gt; corpus = CSCorpusReader()
&gt;&gt; print(corpus.describe())
</code></pre>

<p>There is about a 7 second delay between each sentence:</p>

<blockquote>
  <p>1543770777.502544<br>
  1543770784.383989<br>
  1543770792.2057862<br>
  1543770798.992075<br>
  1543770805.819034<br>
  1543770812.599932<br>
  ...</p>
</blockquote>

<p>If I run the same thing with just a few sentences in the <code>tokens_corpus_reader_ready.txt</code> the output time is totally reasonable:</p>

<blockquote>
  <p>1543771884.739753<br>
  1543771884.74035<br>
  1543771884.7408729<br>
  1543771884.7413561<br>
  {'sents': 4, 'words': 212, 'vocab': 42, 'lexdiv': 5.0476190476190474, 'secs': 0.002869129180908203}</p>
</blockquote>

<p>Where does this behavior come from and how can I fix it?</p>

<h2>Edit 1</h2>

<p>By not every time accessing the corpus itself but operate on lists, the time went down to about 3 seconds per sentence, which is still very long, though:</p>

<pre><code>    sents = list(self.sents())
    words = list(self.words())

    # Perform single pass over paragraphs, tokenize and count
    for sent in sents:
        print(time.time())
        counts['sents'] += 1

        for word in words:
            counts['words'] += 1
            tokens[word] += 1
</code></pre>
"
"53565401","Examples where Dependency Parser fails","2018-11-30 21:54:07","2","622","0","1","","53809540","<p>Can anyone give me few sentences on when the dependency parser fails and why they failed and what is the fix for it?</p>
"
"53453944","What purpose does the complexity of `Except` serve in Haskell?","2018-11-23 23:42:07","10","2411","3","1","","53456278","<p>I <a href=""https://stackoverflow.com/a/53192859/656912"">understand</a> (I think) that there is a close relationship between <code>Either</code> and <code>Except</code> in Haskell, and that it is easy to convert from one to the other. But I'm a bit confused about best practices for handling errors in Haskell and under what circumstances and scenarios I would choose one over the other. For example, in the <a href=""http://hackage.haskell.org/package/mtl-2.2.2/docs/Control-Monad-Except.html#g:3"" rel=""noreferrer"">example</a> provided in <code>Control.Monad.Except</code>, <code>Either</code> is used in the definition</p>

<pre><code>type LengthMonad = Either LengthError
</code></pre>

<p>so that <code>calculateLength ""abc""</code> is</p>

<pre><code>Right 3
</code></pre>

<p>If instead one were to define</p>

<pre><code>type LengthMonad = Except LengthError
</code></pre>

<p>then <code>calculateLength ""abc""</code> would be</p>

<pre><code>ExceptT (Identity (Right 3))
</code></pre>

<p>I'm confused about what purpose this would serve and when one one want it. Why does everything returned from <code>calculateLength</code> always have <code>Identity</code>; why not just <code>SomeExceptionType (Right 3)</code> or even <code>SomeSuccessType 3</code>?</p>

<p>I'm a Haskell beginner when it comes to concepts like this, so a concrete example of when I'd want the latter over the former would be much appreciated, especially why it's so (apparently to me) complex. For example, what can a caller of a function that uses the <code>Except</code> version of <code>calculateLength</code> do, that they can't (or at least can't as easily) do with the <code>Either</code> version?</p>
"
"53370715","How to transform the data and calculate the TFIDF value?","2018-11-19 08:20:17","0","204","6","1","","53371297","<p>My data format is：
<code>datas = {[1,2,4,6,7],[2,3],[5,6,8,3,5],[2],[93,23,4,5,11,3,5,2],...}</code>
Each element in datas is a sentence ,and each number is a word.I want to get the TFIDF value for each number. How to do it with sklearn or other ways?</p>

<p>My code:</p>

<pre><code>from sklearn.feature_extraction.text import TfidfTransformer  
from sklearn.feature_extraction.text import CountVectorizer  
datas = {[1,2,4,6,7],[2,3],[5,6,8,3,5],[2],[93,23,4,5,11,3,5,2]}
vectorizer=CountVectorizer()

transformer = TfidfTransformer()
tfidf = transformer.fit_transform(vectorizer.fit_transform(datas))  
print(tfidf)
</code></pre>

<p>My code doesn't work.Error:</p>

<pre><code>Traceback (most recent call last):   File
""C:/Users/zhuowei/Desktop/OpenNE-master/OpenNE-
master/src/openne/buildTree.py"", line 103, in &lt;module&gt;
    X = vectorizer.fit_transform(datas)   File
""C:\Users\zhuowei\Anaconda3\lib\site-
packages\sklearn\feature_extraction\text.py"", line 869, in fit_transform
    self.fixed_vocabulary_)   File ""C:\Users\zhuowei\Anaconda3\lib\site-
packages\sklearn\feature_extraction\text.py"", line 792, in _count_vocab
    for feature in analyze(doc):   File 
""C:\Users\zhuowei\Anaconda3\lib\site-
packages\sklearn\feature_extraction\text.py"", line 266, in &lt;lambda&gt;
    tokenize(preprocess(self.decode(doc))), stop_words)   File 
""C:\Users\zhuowei\Anaconda3\lib\site-
packages\sklearn\feature_extraction\text.py"", line 232, in &lt;lambda&gt;
    return lambda x: strip_accents(x.lower()) 
AttributeError: 'int' object has no attribute 'lower'
</code></pre>
"
"53327804","Any efficient way to find surrounding ADJ respect to target phrase in python?","2018-11-15 20:58:01","5","671","2","1","","53402088","<p>I am doing sentiment analysis on given documents, my goal is I want to find out the closest or surrounding adjective words respect to target phrase in my sentences. I do have an idea how to extract surrounding words respect to target phrases, but How do I find out relatively close or closest adjective or <code>NNP</code> or <code>VBN</code> or other POS tag respect to target phrase.</p>

<p>Here is the sketch idea of how I may get surrounding words to respect to my target phrase.</p>

<pre><code>sentence_List= {""Obviously one of the most important features of any computer is the human interface."", ""Good for everyday computing and web browsing."",
""My problem was with DELL Customer Service"", ""I play a lot of casual games online[comma] and the touchpad is very responsive""}

target_phraseList={""human interface"",""everyday computing"",""DELL Customer Service"",""touchpad""}
</code></pre>

<p>Note that my original dataset was given as dataframe where the list of the sentence and respective target phrases were given. Here I just simulated data as follows:</p>

<pre><code>import pandas as pd
df=pd.Series(sentence_List, target_phraseList)
df=pd.DataFrame(df)
</code></pre>

<p>Here I tokenize the sentence as follow:</p>

<pre><code>from nltk.tokenize import word_tokenize
tokenized_sents = [word_tokenize(i) for i in sentence_List]
tokenized=[i for i in tokenized_sents]
</code></pre>

<p>then I try to find out surrounding words respect to my target phrases by using this <a href=""https://stackoverflow.com/questions/17645701/extract-words-surrounding-a-search-word"">loot at here</a>. However, I want to find out relatively closer or closet <code>adjective</code>, or <code>verbs</code> or <code>VBN</code> respect to my target phrase. How can I make this happen? Any idea to get this done? Thanks</p>
"
"53258578","Compute TF-IDF word score with relevant and random corpus","2018-11-12 08:50:07","2","254","0","1","","53347340","<p>Given a corpus of relevant documents (CORPUS) and a corpus of random documents (ran_CORPUS) I want to compute TF-IDF scores for all words in CORPUS, using ran_CORPUS as a base line. In my project, the ran_CORPUS has approximately 10 times as many documents as CORPUS.</p>

<pre><code>CORPUS = ['this is a relevant document',
          'this one is a relevant text too']
ran_CORPUS = ['the sky is blue',
              'my cat has a furry tail']
</code></pre>

<p>My plan is to normalize the documents, make all documents in CORPUS to one document (CORPUS being now a list with one long string element). To CORPUS I append all ran_CORPUS documents. Using <code>sklearn's TfidfTransformer</code> I then would compute the TF-IDF matrix for the corpus (consisting now of CORPUS and ran_CORPUS). And finally select the first row of that CORPUS to get the TF-IDF scores for my initial relevant CORPUS.</p>

<p>Does anybody know whether this approach could work and if there is a simple way to code it?</p>
"
"53246564","How to rate quality of a (scraped) sentence?","2018-11-11 07:05:14","2","528","5","1","","53340042","<p>I am running a scrape and process routine in Python3 - but some of the sentences I get are garbage. I would like to reject these but cant figure out how to do it.</p>
<p>I am using POS tagging and chunking with NLTK but that doesn't seem to help me identify non-valid sentences. The number of NNs, VBs etc. doesn't seem to be any different in a garbage &quot;sentence&quot; than a good one.</p>
<p>I guess I am just looking for a simple method to score the grammar of a sentence and reject ones with too many &quot;errors&quot;. I tried to use grammar_check but AWS Lambda doesn't like running it. I immediately get &quot;connection refused&quot; error as soon as I initialise it. (NLTK also needs to be 'spoofed' in order to run on AWS Lambda but I found how to do that).</p>
<p>EXAMPLES:
GOOD:Manchester united boss jose mourinho has told his players to 'grow up' in order to stop conceding early on in games following their comeback wins over newcastle bournemouth and juventus</p>
<p>GARBAGE: [latest results brought to you by played 42 draws 8 etihad stadium manchester old trafford manchester etihad stadium manchester old trafford manchester etihad stadium manchester no content available city return to training after manchester derby win external link city draw fc basel in the ucl round of 16 external link report: united 1-2 city external link city win thrilling derby to move 11 point</p>
"
"53173109","Extract text features from dataframe","2018-11-06 13:40:32","1","1226","0","1","","53176447","<p>I have dataframe with two text fields and other features like this format : </p>

<pre><code> message            feature_1      feature_2       score        text
 'This is the text'     4             7            10          extra text
 'This is more text'    3             2            8           and this is another text
</code></pre>

<p>Now my goal is to predict the score, when trying to transform this dataframe into a feature matrix to feed it into my machine learning model, here is what I have did :</p>

<pre><code>    # Create vectorizer for function to use
    vectorizer = TfidfVectorizer()
    # combine the numerical features with the TFIDF generated matrix
    X = sp.sparse.hstack( (vectorizer.fit_transform(df.message),
                      df[['feature_1', 'feature_2']].values, vectorizer.fit_transform(df.text)),
                      format='csr')
</code></pre>

<p>Now when printing the shape of my X matrix I got 2x13, but when I check the X_columsn like this : </p>

<pre><code>X_columns = vectorizer.get_feature_names() + df[['feature_1', 'feature_2']].columns.tolist()
</code></pre>

<p>I don't get all the words in the corpus, it bring me just the words existing in <code>df.text</code> and other features attribute without words in <code>df.message</code> .</p>

<pre><code>['and', 'another', 'extra', 'is', 'text', 'this', 'feature_1', 'feature_2']
</code></pre>

<p>How can I make X contain all my dataframe features !!</p>
"
"53155057","Segmenting sentence into subsentences with CoreNLP","2018-11-05 13:07:35","3","437","0","2","","53166845","<p>I am working on the following problem: I would like to split sentences into subsentences using Stanford CoreNLP. The example sentence could be:</p>

<pre><code>""Richard is working with CoreNLP, but does not really understand what he is doing""
</code></pre>

<p>I would now like my sentence to be split into single ""S"" as shown in the tree diagram below:</p>

<p><a href=""https://i.sstatic.net/UyLba.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/UyLba.png"" alt=""enter image description here""></a></p>

<p>I would like the output to be a list with the single ""S"" as follows:</p>

<pre><code>['Richard is working with CoreNLP', ', but', 'does not really understand what', 'he is doing']
</code></pre>

<p>I would be really thankful for any help :)</p>
"
"53144310","How do I apply any regex to my tagged text in python 3","2018-11-04 18:59:35","0","371","6","1","","53144394","<p>I have a text. I tokenize it and remove stopwords. then I tag these words using stanford POS tagger in python. For now, I am using this code for tagging words and writing it in a file.</p>

<pre><code>tag = nltk.pos_tag(filtered_sentence)
print(""tagging the words"")
fh = open(""Stop_Words.txt"", ""w+"")
for i in range(0,len(filtered_sentence)):
    fh.write((tag[i][0])+"" ""+(tag[i][1])+""\n"")
fh.close()
</code></pre>

<p>Now I get a list something like this in my file:</p>

<pre><code>paper NN
parallel NN
programming VBG
practical JJ
Greg NNP
Wilson NNP
intended VBD
scientist NN
interested JJ
... A big List ...
</code></pre>

<p>What I want to do now is to apply some Regex to this to find particular cases. For example, I want something like (JJ*N+) which means adjective followed by any noun. I did N+ because NN,NNP etc all are nouns.</p>

<p>How should I do this. I am clueless.Any help will be appreciated.</p>
"
"53066951","scikit-learn TfidfVectorizer ignoring certain words","2018-10-30 14:47:49","1","1828","1","2","","53067743","<p>I'm trying TfidfVectorizer on a sentence taken from wikipedia page about the History of Portugal. However i noticed that the <code>TfidfVec.fit_transform</code> method is ignoring certain words. Here's the sentence i tried with:</p>

<pre><code>sentence = ""The oldest human fossil is the skull discovered in the Cave of Aroeira in Almonda.""

TfidfVec = TfidfVectorizer()
tfidf = TfidfVec.fit_transform([sentence])

cols = [words[idx] for idx in tfidf.indices]
matrix = tfidf.todense()
pd.DataFrame(matrix,columns = cols,index=[""Tf-Idf""])
</code></pre>

<p>output of the dataframe:</p>

<p><a href=""https://i.sstatic.net/iEKnc.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/iEKnc.png"" alt=""enter image description here""></a></p>

<p>Essentially, it is ignoring the words ""Aroeira"" and ""Almonda"".</p>

<p>But i don't want it to ignore those words so what should i do? I can't find anywhere on the documentation where they talk about this.</p>

<p>Another question is why is the word ""the"" repeated? should the algorithm consider just one ""the"" and compute its tf-idf?</p>
"
"53047808","dependency parsing (bracket format) - spanish - using nltk and stanford-nlp tag","2018-10-29 14:36:50","1","609","1","2","","53074505","<p>I am trying to parse a plain text corpus of Spanish to get a result like SNLI corpus (used for entailment), I´ve ttached an extract of snli corpus below. </p>

<p>The church has cracks in the ceiling.
( ( The church ) ( ( has ( cracks ( in ( the ceiling ) ) ) ) . ) )
(ROOT (S (NP (DT The) (NN church)) (VP (VBZ has) (NP (NP (NNS cracks)) (PP (IN in) (NP (DT the) (NN ceiling))))) (. .)))</p>

<p>I tried the following code but the output was not good at all.</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>from nltk import Tree
from functools import reduce
from nltk.parse.corenlp import CoreNLPParser

def binarize(tree):
    """"""
    Recursively turn a tree into a binary tree.
    """"""
    if isinstance(tree, str):
        return tree
    elif len(tree) == 1:
        return binarize(tree[0])
    else:
        label = tree.label()
        return reduce(lambda x, y: Tree(label, (binarize(x), binarize(y))), tree)
    
parser = CoreNLPParser(url='http://localhost:9002')
#parse, = parser.raw_parse('you could say that they regularly catch a shower, which adds to their exhilaration and joie de vivre')
parse, = parser.raw_parse('si idioma no es elegido entonces elegir español por defecto.')
print(parse)
t = parse
bt = binarize(t)
print(bt)</code></pre>
</div>
</div>
</p>
"
"53046724","string index out of range in POS tagging","2018-10-29 13:38:18","3","1489","0","2","","53046888","<p>I am doing POS tagging using nltk package in python. Now it's showing error string index out of range even though my string not much big.</p>

<pre><code>import nltk

sample_list = ['', 'emma', 'jane', 'austen', '1816', '', 'volume', 'chapter', 'emma', 'woodhouse', ' ','handsome', ' ', 'clever', ' ', 'rich', ' ', 'comfortable', 'home', 'happy', 'disposition', ' ','seemed', 'unite', 'best','blessings', 'existence', '', 'lived','nearly', 'twenty-one', 'years','world', 'little', 'distress', 'vex', '', 'youngest','two']

tagged = nltk.pos_tag(sample_list)
</code></pre>

<p><a href=""https://i.sstatic.net/GZ2vX.png"" rel=""nofollow noreferrer"">screenshot of the error</a></p>
"
"52981868","Rule based named entity recognizer without parts of speech label or any other information","2018-10-25 05:26:46","0","170","0","1","","52999917","<p>I'm working on a project where I am trying to build a named entity recognizer from texts. So basically I want to build and experiment the NER in 3 different ways.</p>

<p>First, I want to build it using only segmented sentences-> tokenized words. To clarify, I want to input only split/tokenized words into the system. Once again, the NER system is rule-based. Hence, it can only use rules to conclude which is a named entity. In the first NER, it will not have any chunk information or part of speech label. Just the tokenized words. Here, the efficiency is not the concern. Rather the concern lies in comparing the 3 different NERs, how they perform. (The one I am asking about is the 1st one).</p>

<p>I thought of it for a while and could not figure out any rules or any idea of coming up with a solution to this problem. One naive approach would be to conclude all words beginning with an uppercase and that does not follow a period to be a named entity. </p>

<p>Am I missing anything? Any heads up or guidelines would help.</p>
"
"52808220","NLP Getting the most common POS tag for a word and using it in the dictionary with Training Data","2018-10-15 00:16:04","0","944","0","1","","52808635","<p>I have a training text file with the following format (pos, word, tag):</p>

<p>1   i   PRP</p>

<p>2   'd  MD</p>

<p>3   like    VB</p>

<p>4   to  TO</p>

<p>5   go  VB</p>

<p>6   .   .    </p>

<p>1   i   PRP</p>

<p>I am trying to build a dictionary so that when I input a new corpus with the following format (pos, word):</p>

<p>1   who</p>

<p>2   knows</p>

<p>3   what</p>

<p>4   will</p>

<p>5   happen</p>

<p>6   .    </p>

<p>I will be able to tag these from the dictionary I've built with the training data.</p>

<p>the method I'm using is a counter in default dictionary to find the most common tag for a word. From my counter, I'm getting print results like this:</p>

<p>i   PRP 7905</p>

<p>'d  MD  1262</p>

<p>like    VB  2706</p>

<p>like    VBP 201</p>

<p>like    UH  95</p>

<p>like    IN  112</p>

<p>to  TO  4822</p>

<p>to  IN  922</p>

<p>So for the word ""like"", the tag with the highest counts is 'VB' at 2706. I want to my dictionary to take the tag with the highest count and attach it to my word so that if I put a test data set with just the (pos, word), it will return that tag. Here's my code so far:</p>

<pre><code>file=open(""/Users/Desktop/training.txt"").read().split('\n')

from collections import Counter, defaultdict
word_tag_counts = defaultdict(Counter)
for row in file:         
    if not row.strip():
        continue          
    pos, word, tag = row.split()
    word_tag_counts[word.lower()][tag] += 1

stats = word_tag_counts
max(stats, key=stats.get)

with open('/Users/Desktop/training.txt','r') as file:
    for line in file.readlines():
        column = line.split('\t') 
with open('/Users/Desktop/output.txt','w') as file: 
    for tag, num in d.items(): 
        file.write(""\t"".join([column[0], column[1], tag])+""\n"")
</code></pre>

<p>I'm getting the error: TypeError: '>' not supported between instances of 'Counter' and 'Counter'</p>

<p>my output goal is in the same format as the original training file (pos pulled from original txt file, word from original txt file, tag from my dictionary):</p>

<p>Not sure what I can, i tried using lambda as well but it's not working. Anything will help. Thanks. </p>
"
"52787562","necessary condition to fix weird lemma's?","2018-10-12 22:06:32","1","60","0","1","","52791289","<p>(<strong>Executed in jupyter notbook</strong>) I'm applying lemmatization on documents that I've tokenised and I can't help but notice that the word ""us"" gets lemmatized to ""u"" every time which wouldn't make sense from a clarity point of view and could possibly lead people to understand it as something else. Am I missing out a condition for my pos function? How could I fix this problem?</p>

<p><strong>Defining the function</strong></p>

<pre><code>from nltk import pos_tag

def penn2wordNet(treebank_tags):
    wordNet_tag = {'NN':'n', 'JJ':'a',
                  'VB':'v', 'RB':'r'}
    try:
        return wordNet_tag[penntag[:2]]
    except:
        return 'n'
paired_tags = []
for doc in wordTokens:
    paired_tags.append(pos_tag(doc))
    print(paired_tags)
</code></pre>

<p><img src=""https://i.sstatic.net/NxSGZ.png"" alt=""snippet output of the code above""></p>

<p><strong>Lemmatizing the tokens</strong></p>

<pre><code>    from nltk.stem import WordNetLemmatizer
wnl = WordNetLemmatizer()

print(wordTokens[1])
lemmatized_wordTokens = []
for index in range(len(paired_tags)):
    lemmatized_wordTokens.append(([wnl.lemmatize(word, pos=penn2wordNet(tag)) for word, tag in paired_tags[index]]))
print(lemmatized_wordTokens[1])
</code></pre>

<p><img src=""https://i.sstatic.net/ig27A.png"" alt=""output after lemmatization showing before and after""></p>
"
"52659260","Remove Part of Speech Tags after chunking","2018-10-05 06:08:15","0","426","0","1","","52732615","<p>How to remove part of speech tags from the results of chunking ?
I am using NLTK to do this. Currently I can only iterate to the chunks using this code:</p>

<pre><code>for i in sent_list:
tagged = nltk.pos_tag(i)

ChunkGram = r""""""Chunk: {&lt;VB.?&gt;+&lt;JJ.?&gt;*&lt;NN.?&gt;}""""""

ChunkParser = nltk.RegexpParser(ChunkGram)
chunked = ChunkParser.parse(tagged)
for subtree in chunked.subtrees(filter=lambda t: t.label() == 'Chunk'):
    print(subtree)
</code></pre>

<p>lets say my results are as such:</p>

<pre><code>(Chunk routing/VBG rework/NN build/NN)
(Chunk build/VBP instruction/NN schedule/NN lot/NN)
(Chunk based/VBN firm/NN plan/NN)
</code></pre>

<p>Expected Results:</p>

<pre><code>'routing','rework','build'
</code></pre>

<p>OR</p>

<pre><code>'routing rework build'
</code></pre>

<p>would it be possible to do so ? or else please advice me on what i can do to extract these phrases.</p>
"
"52625388","Iterate and Lemmatize List","2018-10-03 10:48:10","1","1093","1","1","","52625609","<p>I'm a newbie and struggling with what I'm sure is a simple task.</p>

<p>I have a list of words taken from <code>POS tagging</code>:</p>

<pre><code>words = ['drink', 'drinking']
</code></pre>

<p>And I want to <code>lemmatize</code> them and then process them (using <code>set</code>?) to ultimately refine my list to:</p>

<pre><code>refined_list = ['drink']
</code></pre>

<p>However, I""m stuck on the next step of lemmatization - my method still returns the following:</p>

<pre><code>refinded_list = ['drink', 'drinking']
</code></pre>

<p>I tried to reference <a href=""https://stackoverflow.com/questions/43747659/python-lemmatizing-input-list-return-output-list"">this</a> but can't figure out what to import so 'lmtzr' works or how to get it to work.</p>

<p>Here's my code so far:</p>

<pre><code>import nltk
words = ['drink', 'drinking']
WNlemma = nltk.WordNetLemmatizer()
refined_list = [WNlemma.lemmatize(t) for t in words]
print(refined_list)
</code></pre>

<p>Thank you for helping me.</p>
"
"52557058","spaCy nlp pipeline order of operations","2018-09-28 14:03:38","2","3170","0","2","","74187846","<p>Does anyone have a chronological list of operations performed by</p>

<pre><code>import spacy
nlp = spacy.load('en_core_web_sm')
doc = nlp(text)
</code></pre>

<p>I can see the major components with <code>nlp.pipe_names</code></p>

<pre><code>['tagger', 'parser', 'ner']
</code></pre>

<p>and an alphabetical list of factory operations with <code>nlp.factories</code></p>

<pre><code>{'merge_entities': &lt;function spacy.language.Language.&lt;lambda&gt;&gt;,
 'merge_noun_chunks': &lt;function spacy.language.Language.&lt;lambda&gt;&gt;,
 'ner': &lt;function spacy.language.Language.&lt;lambda&gt;&gt;,
 'parser': &lt;function spacy.language.Language.&lt;lambda&gt;&gt;,
 'sbd': &lt;function spacy.language.Language.&lt;lambda&gt;&gt;,
 'sentencizer': &lt;function spacy.language.Language.&lt;lambda&gt;&gt;,
 'similarity': &lt;function spacy.language.Language.&lt;lambda&gt;&gt;,
 'tagger': &lt;function spacy.language.Language.&lt;lambda&gt;&gt;,
 'tensorizer': &lt;function spacy.language.Language.&lt;lambda&gt;&gt;,
 'textcat': &lt;function spacy.language.Language.&lt;lambda&gt;&gt;,
 'tokenizer': &lt;function spacy.language.Language.&lt;lambda&gt;&gt;}
</code></pre>

<p>but I can't figure out when the <strong>lemmatizer</strong> is invoked. 
Lemmatization has to happen after <strong>tokenization</strong> and <strong>POS tagging</strong>, and it will run with the <strong>parser</strong> and <strong>ner</strong> disabled. The spaCy <a href=""https://spacy.io/usage/spacy-101#pipelines"" rel=""nofollow noreferrer"">pipeline docs</a> don't mention it at all. Thanks!</p>
"
"52549113","Create a code in python to get the most frequent tag and value pair from a list","2018-09-28 05:51:50","1","573","2","2","","52549732","<p>I have a .txt file with 3 columns: word position, word and tag (NN, VB, JJ, etc.).</p>

<p>Example of txt file: </p>

<pre><code>1   i   PRP

2   want    VBP

3   to  TO

4   go  VB
</code></pre>

<p>I want to find the frequency of the word and tag as a pair in the list in order to find the most frequently assigned tag to a word. 
Example of Results:
3 (food, NN), 2 (Brave, ADJ)</p>

<p>My idea is to start by opening the file from the folder, read the file line by line and split, set a counter using dictionary and print with the most common to uncommon in descending order. </p>

<p>My code is extremely rough (I'm almost embarrassed to post it):</p>

<pre><code>file=open(""/Users/Desktop/Folder1/trained.txt"")
wordcount={}
for word in file.read().split():
    from collections import Counter
    c = Counter()
    for d in dicts.values():
        c += Counter(d)

print(c.most_common())

file.close()
</code></pre>

<p>Obviously, i'm getting no results. Anything will help. Thanks.</p>

<p>UPDATE:</p>

<p>so i got this code posted on here which worked, but my results are kinda funky. here's the code (the author removed it so i don't know who to credit):</p>

<pre><code>file=open(""/Users/Desktop/Folder1/trained.txt"").read().split('\n')

d = {}
for i in file:
    if i[1:] in d.keys():
        d[i[1:]] += 1
    else:
        d[i[1:]] = 1

print (sorted(d.items(), key=lambda x: x[1], reverse=True))
</code></pre>

<p>here are my results:</p>

<pre><code>[('', 15866), ('\t.\t.', 9479), ('\ti\tPRP', 7234), ('\tto\tTO', 4329), ('\tlike\tVB', 2533), ('\tabout\tIN', 2518), ('\tthe\tDT', 2389), ('\tfood\tNN', 2092), ('\ta\tDT', 2053), ('\tme\tPRP', 1870), ('\twant\tVBP', 1713), ('\twould\tMD', 1507), ('0\t.\t.', 1427), ('\teat\tVB', 1390), ('\trestaurant\tNN', 1371), ('\tuh\tUH', 1356), ('1\t.\t.', 1265), ('\ton\tIN', 1237), (""\t'd\tMD"", 1221), ('\tyou\tPRP', 1145), ('\thave\tVB', 1127), ('\tis\tVBZ', 1098), ('\ttell\tVB', 1030), ('\tfor\tIN', 987), ('\tdollars\tNNS', 959), ('\tdo\tVBP', 956), ('\tgo\tVB', 931), ('2\t.\t.', 912), ('\trestaurants\tNNS', 899),
</code></pre>

<p>there seem to be a mix of good results with words and other results with space or random numbers, anyone know a way to remove what aren't real words? also, i know \t is supposed to signify a tab, is there a way to remove that as well? you guys really helped a lot </p>
"
"52519433","Methods to extract keywords from large documents that are relevant to a set of predefined guidelines using NLP/ Semantic Similarity","2018-09-26 13:54:41","0","127","0","1","","52537610","<p>I'm in need of suggestions how to extract keywords from a large document. The keywords should be inline what we have defined as the intended search results. </p>

<p>For example, </p>

<p>I need the owner's name, where the office is situated, what the operating industry is when a document about a company is given, and the defined set of words would be, </p>

<blockquote>
  <p>{owner, director, office, industry...}-(1)</p>
</blockquote>

<p>the intended output has to be something like, </p>

<blockquote>
  <p>{Mr.Smith James, ,Main Street, Financial Banking}-(2)</p>
</blockquote>

<p>I was looking for a method related to Semantic Similarity where sentences containing words similar to the given corpus (1), would be extracted, and using POS tagging to extract nouns from those sentences.</p>

<p>It would be a useful if further resources could be provided that support this approach.  </p>
"
"52481933","`nltk` CoreNLPParser: prevent splitting at hyphens in POS tagger","2018-09-24 14:47:10","1","587","1","1","","52495132","<p>I am using the <code>nltk</code> <code>CoreNLPParser</code> with the Stanford NLP server for POS tagging as described in <a href=""https://stackoverflow.com/a/51981566/3881984"">this answer</a>.</p>

<p>This tagger treats words with hyphens as multiple words, for example dates like <code>2007-08</code> are tagged as <code>CP, :, CP</code>. However, my model uses words with hyphen as one token. Is it possible using the <code>CoreNLPParser</code> to prevent splitting at hyphens?</p>
"
"52409500","Internal Search optimization for relevance","2018-09-19 15:34:52","0","31","0","1","","52413130","<p>My team is using Solr and I have a question regarding it.</p>

<p>There are some search terms which doesn't gives relevant results or results which should have been displayed. For example:</p>

<ol>
<li>Searching for Macy's without the apostrophe like ""Macys"" doesnt give back any result for Macy's.</li>
<li>Searching for JPMorgan vs JP Morgan gives different result</li>
<li>Searching for IBM doesn't show results which contains its full name i.e International business machine.</li>
</ol>

<p>How can we improve and optimize such cases so that it gets applied to all, even to the one we didn't catch apart from these 3 above?</p>

<p>Any suggestions?</p>
"
"52393591","NLTK Lemmatizer, Extract meaningful words","2018-09-18 19:44:59","0","3495","1","2","","52396249","<p>Currently, I am going to create a machine learning based code that automatically maps categories.</p>

<p>I am going to do natural language processing before that.</p>

<p>There are several words list.</p>

<pre><code>      sent ='The laughs you two heard were triggered 
             by memories of his own high j-flying 
             moist moisture moisturize moisturizing '.lower().split()
</code></pre>

<p>I made the following code.
I referenced this url. <a href=""https://stackoverflow.com/questions/35870282/nltk-lemmatizer-and-pos-tag"">NLTK: lemmatizer and pos_tag</a></p>

<pre><code>from nltk.tag import pos_tag
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
def lemmatize_all(sentence):
    wnl = WordNetLemmatizer()
    for word, tag in pos_tag(word_tokenize(sentence)):
        if tag.startswith(""NN""):
            yield wnl.lemmatize(word, pos='n')
        elif tag.startswith('VB'):
            yield wnl.lemmatize(word, pos='v')
        elif tag.startswith('JJ'):
            yield wnl.lemmatize(word, pos='a')



words = ' '.join(lemmatize_all(' '.join(sent)))
</code></pre>

<p>The resulting values are shown below.</p>

<pre><code>laugh heard be trigger memory own high j-flying moist moisture moisturize moisturizing
</code></pre>

<p>I am satisfied with the following results.</p>

<pre><code>laughs -&gt; laugh 
were -&gt; be
triggered -&gt; trigger 
memories -&gt; memory 
moist -&gt; moist 
</code></pre>

<p>However, the following values are not satisfied.</p>

<pre><code>heard -&gt; heard 
j-flying -&gt; j-flying 
moisture -&gt; moisture 
moisturize -&gt; moisturize 
moisturizing -&gt; moisturizing 
</code></pre>

<p>Although it was better than the initial values, I would like the following results.</p>

<pre><code>heard -&gt; hear
j-flying -&gt; fly
moisture -&gt; moist
moisturize -&gt; moist
moisturizing -&gt; moist
</code></pre>

<p>If you have any other good way to extract meaningful words, 
please let me know.
Thank you</p>
"
"52315632","python3 nltk, WordNetLemmatizer An error has occurred","2018-09-13 14:12:38","2","5427","2","1","","52318918","<p>I looked at the book and made the code as it was in the book. By the way, I have the following error. What should I do?</p>

<pre><code>from nltk.stem import PorterStemmer, WordNetLemmatizer

sent = 'The laughs you two heard were triggered by memories 
            of his own high j-flying exits for moving beasts'

lemmatizer = WordNetLemmatizer()
words = lemmatizer.lemmatize(sent, pos = 'pos')

File ""D:/machine_learning/nltk_mapper.py"", line 24, in &lt;module&gt;
    word = lemmatizer.lemmatize(words, pos='pos')
  File ""D:\machine_learning\venv\lib\site-packages\nltk\stem\wordnet.py"", line 40, in lemmatize
    lemmas = wordnet._morphy(word, pos)
  File ""D:\machine_learning\venv\lib\site-packages\nltk\corpus\reader\wordnet.py"", line 1818, in _morphy
    exceptions = self._exception_map[pos]
KeyError: 'pos'
</code></pre>

<p>The original result value is to print only meaningful words as follows:</p>

<pre><code>  ['The', 'laugh', 'two', 'hear', 'trigger', 
   'memory', 'high', 'fly', 'exit', 'move', 'beast']
</code></pre>

<p>Thank you</p>

<hr>

<p>I've solved it.
I referenced the following url.
<a href=""https://stackoverflow.com/questions/35870282/nltk-lemmatizer-and-pos-tag"">NLTK: lemmatizer and pos_tag</a></p>

<pre><code>from nltk.tag import pos_tag
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
def lemmatize_all(sentence):
    wnl = WordNetLemmatizer()
    for word, tag in pos_tag(word_tokenize(sentence)):
        if tag.startswith(""NN""):
            yield wnl.lemmatize(word, pos='n')
        elif tag.startswith('VB'):
            yield wnl.lemmatize(word, pos='v')
        elif tag.startswith('JJ'):
            yield wnl.lemmatize(word, pos='a')
        # else:
        #     yield word

print(' '.join(lemmatize_all('The laughs you two heard were triggered by memories of his own high j-flying exits for moving beasts')))
</code></pre>

<p>result --> laugh heard be trigger memory own high j-flying exit move beast</p>

<p>thank you</p>
"
"52140526","Python nltk stemmers never remove prefixes","2018-09-02 19:51:05","3","3045","2","2","","52142282","<p>I'm trying to preprocess words to remove common prefixes like ""un"" and ""re"", however all of nltk's common stemmers seem to completely ignore prefixes:</p>

<pre><code>from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer

PorterStemmer().stem('unhappy')
# u'unhappi'

SnowballStemmer('english').stem('unhappy')
# u'unhappi'

LancasterStemmer().stem('unhappy')
# 'unhappy'

PorterStemmer().stem('reactivate')
# u'reactiv'

SnowballStemmer('english').stem('reactivate')
# u'reactiv'

LancasterStemmer().stem('reactivate')
# 'react'
</code></pre>

<p>Isn't part of the job of a stemmer to remove common prefixes as well as suffixes? Is there another stemmer which does this reliably?</p>
"
"52039155","Stemming of the multilingual text corpus","2018-08-27 12:14:42","1","2639","9","1","","52055648","<p>I have a text corpus with item descriptions in English, Russian and Polish.</p>

<p>This text corpus has 68K observations. Some of these observations are written in English, some in Russian, and some in Polish.</p>

<p>Could you tell me how <strong><em>properly</em></strong> and <strong><em>cost-efficiently</em></strong> implement a word stemming in this case? I can not use an English stemmer on Russian words and vice versa.</p>

<p>Unfortunately, I could not find a good language identifier. E.g. <code>langdetect</code> works too slow and often incorrectly. For example, I try to identify language of english word 'today': </p>

<pre><code>detect(""today"") 
""so"" 
# i.e Somali 
</code></pre>

<p>So far my code implementation looks bad. I just use one stemmer on another:</p>

<pre><code>import nltk
# polish stemmer
from pymorfologik import Morfologik

clean_items = []

# create stemmers

snowball_en = nltk.SnowballStemmer(""english"")
snowball_ru = nltk.SnowballStemmer(""russian"")
stemmer_pl = Morfologik()

# loop over each item; create an index i that goes from 0 to the length
# of the item list 

for i in range(0, num_items):
    # Call our function for each one, and add the result to the list of
    # clean items

    cleaned = items.iloc[i]

    # to word stem
    clean_items.append(snowball_ru.stem(stemmer_pl(snowball_en.stem(cleaned))))
</code></pre>
"
"51943811","Does the lemmatization mechanism reduce the size of the corpus?","2018-08-21 07:37:45","3","376","2","1","","51978364","<p>Dear Community Members,</p>

<p>During the pre-processing of data, after splitting the raw_data into tokens, I have used the popular WordNet Lemmatizer to generate the stems. I am performing experiments on a dataset that has 18953 tokens. </p>

<p>My question is, does the lemmatization process reduce the size of corpus?
I am confused, kindly help in this regard. Any help is appreciated!</p>
"
"51905788","Add a new stemmer to nltk","2018-08-18 06:05:25","0","179","0","2","","51942139","<p>I have this python function that works as expected. Is it possible to save the logic as NLP stemmer?
If yes, what changes needs to be done?</p>

<pre><code>import itertools, re
def dropdup(mytuple):
    newtup=list()
    for i in mytuple:
        i = i[:-3] if i.endswith('bai') else i
        for r in ((""tha"", ""ta""), (""i"", ""e"")):
            i = i.replace(*r)
            i = re.sub(r'(\w)\1+',r'\1', i)
        newtup.append(''.join(i for i, _ in itertools.groupby(i)))
    return tuple(newtup)

dropdup(('savithabai', 'samiiir', 'aaaabaa'))
('saveta', 'samer', 'aba')
</code></pre>

<p>I will like the users to import something like this...</p>

<pre><code>from nltk.stemmer import indianNameStemmer
</code></pre>

<p>There are a few more rules to be added to the logic. I just want to know if this is a valid (pythonic) idea.</p>
"
"51904251","Does the TfidfVectorizer implicitly threshold its fitted output for large datasets?","2018-08-18 00:26:10","1","380","5","1","","51910110","<p>I'm trying to use <code>sklearn</code>'s <code>TfidfVectorizer</code> to output <code>tf-idf</code> scores for a list of inputs, comprised of both unigrams and bigrams. </p>

<p>Here's the essence of what I'm doing:</p>

<pre><code>comprehensive_ngrams = comprehensive_unigrams + comprehensive_bigrams # List of unigrams and bigrams(comprehensive_unigrams and comprehensive_bigrams are lists in their own right)
print(""Length of input list: "", len(comprehensive_ngrams))
vectorizer = TfidfVectorizer(ngram_range = (1,2), lowercase = True)
vectorizer.fit(comprehensive_ngrams)
vocab = vectorizer.vocabulary_
print(""Length of learned vocabulary: "", len(vocab))
term_document_matrix = vec.toarray()
print(""Term document matrix shape is: "", term_document_matrix.shape)
</code></pre>

<p>This snippet outputs the following: </p>

<pre><code>Length of input list: 12333

Length of learned vocabulary: 6196

Term document matrix shape is: (12333, 6196)
</code></pre>

<p>The length of the dictionary mapping input elements to positional indices emitted by the <code>TfidfVectorizer</code> is shorter than the number of unique inputs it's fed. This doesn't seem to be a problem for smaller datasets (on the order of ~50 elements) - the size of the dictionary the <code>TfidfVectorizer</code> produces once it has been fitted equals the size of the input. </p>

<p>What am I missing?</p>
"
"51895244","Optimizing Python algorithm","2018-08-17 12:15:02","0","75","2","2","","51895442","<p>I am running my code off a 10yr old potato computer (i5 with 4GB RAM) and need to do a lot of language processing with NLTK. I cannot afford a new computer yet. I wrote a simple function (as part of a bigger program). Problem is, I do not know which is more efficient, requires less computing power and is quicker for processing overall?</p>

<p>This snippet uses more variables:</p>

<pre><code>import nltk
from nltk.tokenize import PunktSentenceTokenizer    #Unsupervised machine learning tokenizer.
#This is the custom tagger I created. To use it in future projects, simply import it from Learn_NLTK and call it in your project.
def custom_tagger(training_file, target_file):
    tagged = []
    training_text = open(training_file,""r"")
    target_text = open(target_file,""r"")
    custom_sent_tokenizer = PunktSentenceTokenizer(training_text.read())  #You need to train the tokenizer on sample data.
    tokenized = custom_sent_tokenizer.tokenize(target_text.read())    #Use the trained tokenizer to tag your target file.
    for i in tokenized: 
        words = nltk.word_tokenize(i)
        tagging = nltk.pos_tag(words)
        tagged.append(tagging)
    training_text.close()   #ALWAYS close opened files! This is why I have included the extra code to this function!
    target_text.close()     #ALWAYS close opened files! This is why I have included the extra code to this function!
    return tagged
</code></pre>

<p>Or is this more efficient? I actually prefer this:</p>

<pre><code>import nltk
from nltk.tokenize import PunktSentenceTokenizer    #Unsupervised machine learning tokenizer.
#This is the custom tagger I created. To use it in future projects, simply import it from Learn_NLTK and call it in your project.
def custom_tagger(training_file, target_file):
    tagged = []
    training_text = open(training_file,""r"")
    target_text = open(target_file,""r"")
    #Use the trained tokenizer to tag your target file.
    for i in PunktSentenceTokenizer(training_text.read()).tokenize(target_text.read()): tagged.append(nltk.pos_tag(nltk.word_tokenize(i)))        
    training_text.close()   #ALWAYS close opened files! This is why I have included the extra code to this function!
    target_text.close()     #ALWAYS close opened files! This is why I have included the extra code to this function!
    return tagged
</code></pre>

<p>Does anyone have any other suggestions for optimizing code?</p>
"
"51836500","coreNLPDependencyParser output explanation","2018-08-14 08:00:33","1","146","0","1","","51838247","<p>i am running coreNLPDependencyParser for a sentence </p>

<blockquote>
  <p>The quick brown fox jumps over the lazy dog.</p>
</blockquote>

<p>and i am getting output in this way </p>

<pre><code>The     DT      4       det
quick   JJ      4       amod
brown   JJ      4       amod
fox     NN      5       nsubj
jumps   VBZ     0       ROOT
over    IN      9       case
the     DT      9       det
lazy    JJ      9       amod
dog     NN      5       nmod
.       .       5       punct
</code></pre>

<p>i ran the same input in stanfordDependencyParser and the output is same with different representation</p>

<p><strong>My question is</strong>
if you see the third column it is giving some score sort of thing, i assumed it to be depth in the tree but its not correct </p>

<p>it is not mentioned anywhere what exactly the score is .</p>

<p>you can see tree <a href=""http://www.nltk.org/api/nltk.parse.html?highlight=stanford"" rel=""nofollow noreferrer"">here</a></p>

<p>please enlighten me on output representation?</p>
"
"51787997","Python - Using GridSearchCV with NLTK","2018-08-10 13:49:29","0","270","2","1","","51788385","<p>I'm a little unsure as to how I can apply SKLearn's GridSearchCV to a random forest I'm using with NLTK. How to use GridSearchCV normally is discussed <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"" rel=""nofollow noreferrer"">here</a>, however my data is formatted differently to the standard x and y split. Here is my code:</p>

<pre><code>import nltk
import numpy as np
from nltk.classify.scikitlearn import SklearnClassifier
from nltk.corpus.reader import CategorizedPlaintextCorpusReader
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC


reader_train = CategorizedPlaintextCorpusReader('C:/Users/User/Documents/Sentiment/machine_learning/amazon/amazon/', r'.*\.txt', cat_pattern=r'(\w+)/*', encoding='latin1')

documents_train = [ (list(reader_train.words(fileid)), category)
                 for category in reader_train.categories()
                 for fileid in reader_train.fileids(category) ]

all_words = []

for w in reader_train.words():
    all_words.append(w.lower())

all_words = nltk.FreqDist(all_words)

word_features = list(all_words.keys())[:3500]

def find_features(documents):
    words = set(documents)
    features = {}
    for w in word_features:
        features[w] = (w in words)

return features

featuresets_train = [(find_features(rev), category) for (rev, category) in documents_train]

np.random.shuffle(featuresets_train)

training_set = featuresets_train[:1600]
testing_set = featuresets_train[:400]

RandFor = SklearnClassifier(RandomForestClassifier())
RandFor.train(training_set)
print(""RandFor accuracy:"", (nltk.classify.accuracy(RandFor, testing_set)) *100)
</code></pre>

<p>This code, instead of producing a conventional x and y split, produces a list of tuples, where each tuple is in the following format:</p>

<pre><code>({'i': True, 'am': False, 'conflicted': False ... 'about': False}, neg)
</code></pre>

<p>Is there a way to apply GridSearchCV to data in this format?</p>
"
"51786224","Dependency Parsing using Stanford Dependency Parser","2018-08-10 12:09:28","0","349","0","1","","51810575","<p>i am trying to extract main verb in a sentence and i followed this <a href=""https://stackoverflow.com/questions/19751230/how-can-we-extract-the-main-verb-from-a-sentence"">question</a> , i am expecting output in this format </p>

<pre><code>nsubj(swim-4, Parrots-1)
aux(swim-4, do-2)
neg(swim-4, not-3)
root(ROOT-0, swim-4)
</code></pre>

<p>but i am getting output in this way </p>

<pre><code>[&lt;DependencyGraph with 94 nodes&gt;]
</code></pre>

<p>i did following </p>

<pre><code>  dependencyParser = stanford.StanfordDependencyParser(model_path=""edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz"")
  print (list(dependencyParser.raw_parse(noiseLessInput)))
</code></pre>

<p>i think i am doing something wrong, how can i achieve desired ouput</p>
"
"51772605","Sklearn NotFittedError for CountVectorizer in pipeline","2018-08-09 17:12:26","2","1876","0","1","","51772983","<p>I am trying to learn how to work with text data through sklearn and am running into an issue that I cannot solve.</p>

<p>The tutorial I'm following is: <a href=""http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"" rel=""nofollow noreferrer"">http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html</a></p>

<p>The input is a pandas df with two columns. One with text, one with a binary class.</p>

<p>Code:</p>



<pre class=""lang-py prettyprint-override""><code>from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline

traindf, testdf = train_test_split(nlp_df, stratify=nlp_df['class'])

x_train = traindf['text']
x_test = traindf['text']
y_train = traindf['class']
y_test = testdf['class']

# CV
count_vect = CountVectorizer(stop_words='english')
x_train_modified = count_vect.fit_transform(x_train)
x_test_modified = count_vect.transform(x_test)


# TF-IDF
idf = TfidfTransformer()
fit = idf.fit(x_train_modified)
x_train_mod2 = fit.transform(x_train_modified)

# MNB

mnb = MultinomialNB()
x_train_data = mnb.fit(x_train_mod2, y_train)

text_clf = Pipeline([('vect', CountVectorizer()),
             ('tfidf', TfidfTransformer()),
               ('clf', MultinomialNB()),
                ])

predicted = text_clf.predict(x_test_modified)
</code></pre>

<p>When I try to run the last line:</p>

<pre class=""lang-py prettyprint-override""><code>---------------------------------------------------------------------------
NotFittedError                            Traceback (most recent call last)
&lt;ipython-input-64-8815003b4713&gt; in &lt;module&gt;()
----&gt; 1 predicted = text_clf.predict(x_test_modified)

~/anaconda3/lib/python3.6/site-packages/sklearn/utils/metaestimators.py in &lt;lambda&gt;(*args, **kwargs)
    113 
    114         # lambda, but not partial, allows help() to work with update_wrapper
--&gt; 115         out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)
    116         # update the docstring of the returned function
    117         update_wrapper(out, self.fn)

~/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py in predict(self, X)
    304         for name, transform in self.steps[:-1]:
    305             if transform is not None:
--&gt; 306                 Xt = transform.transform(Xt)
    307         return self.steps[-1][-1].predict(Xt)
    308 

~/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py in transform(self, raw_documents)
    918             self._validate_vocabulary()
    919 
--&gt; 920         self._check_vocabulary()
    921 
    922         # use the same matrix-building strategy as fit_transform

~/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py in _check_vocabulary(self)
    301         """"""Check if vocabulary is empty or missing (not fit-ed)""""""
    302         msg = ""%(name)s - Vocabulary wasn't fitted.""
--&gt; 303         check_is_fitted(self, 'vocabulary_', msg=msg),
    304 
    305         if len(self.vocabulary_) == 0:

~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py in check_is_fitted(estimator, attributes, msg, all_or_any)
    766 
    767     if not all_or_any([hasattr(estimator, attr) for attr in attributes]):
--&gt; 768         raise NotFittedError(msg % {'name': type(estimator).__name__})
    769 
    770 

NotFittedError: CountVectorizer - Vocabulary wasn't fitted.
</code></pre>

<p>Any suggestions on how to fix this error? I am properly transforming the CV model on the test data. I even checked if the vocabulary list was empty and it isn't (count_vect.vocabulary_)</p>

<p>Thank you!</p>
"
"51768414","Looping through Lemmas in NLTK Wordnet","2018-08-09 13:31:34","0","743","0","1","","51776190","<p>Have a script for getting italian synonyms from Wordnet like this:</p>

<pre><code>from nltk.corpus import wordnet as wn

it_lemmas = wn.lemmas(""problema"", lang=""ita"")

hypernyms = it_lemmas[0].synset().hypernyms()

print(hypernyms[0].lemmas(lang=""ita""))
</code></pre>

<p>When I do the looping I get message <code>that list indices must be integers or slices, not Lemma</code></p>

<p>How should I do the looping to get not only one value ([0]) but all the values in this dictionary (the amount can be different) and print them all?</p>
"
"51760307","How to get Only Part of speech From POS_TAG in python","2018-08-09 06:36:34","0","330","0","1","","51760403","<p>Hi every body I wan to get Pos_tag only like ""jj"" etc of a word . how to get this from list of  post_tag . 
I am able to print this result :</p>

<pre><code>list1=nltk.pos_tag(words)
print(list1)
&gt;&gt;[('good', 'JJ')]
</code></pre>

<p>Now My question is now to separate word and post tag from the above result list.
I want to store word in myword variable and jj to mypos variable
<strong>Please store good and jj into two different variable and print separate</strong></p>
"
"51664147","Does Google engine penalize pages containing (machine or human) translated content?","2018-08-03 01:28:42","1","65","1","1","","52395728","<p>Google SE has zero-tolerance policy against duplicate and spun content, but I am not sure how it deals with translated text? Any guesses on how it might detect translated content? The first thing occurs to my mind is they use their own Google Translate to back-translate the translated content into the source language, but if that's the case do they have to try back-translating into all languages? Are there any specific similarity metrics for such a task? Thank you!</p>
"
"51658153","Lemmatize a doc with spacy?","2018-08-02 16:16:12","5","10422","0","3","","51658792","<p>I have a spaCy <code>doc</code> that I would like to lemmatize.</p>

<p>For example:</p>

<pre><code>import spacy
nlp = spacy.load('en_core_web_lg')

my_str = 'Python is the greatest language in the world'
doc = nlp(my_str)
</code></pre>

<p>How can I convert every token in the <code>doc</code> to its lemma?</p>
"
"51609143","Create and exploit a tagged corpora with NLTK","2018-07-31 08:45:27","0","350","2","1","","52247460","<p>I'm trying to create a tagged corpora in Malagasy (my mother tongue). I followed the instructions in the document <strong>Python text Processing</strong> and <strong>natural language</strong> processing and the page  <a href=""https://www.nltk.org/book/ch05.html"" rel=""nofollow noreferrer"">https://www.nltk.org/book/ch05.html</a> .
I have managed to create my own Part-of-Speech Tagset based on the Universal Part-of-Speech Tagset and a little tagged corpora.
This is my code :</p>

<pre><code>        import os, os.path
        path = os.path.expanduser('D:/Mes documents/MY_POS_tagger/nltk_data')
        if not os.path.exists(path):
            os.mkdir(path)
        print(""OS path done :%s""%os.path.exists(path))


        import nltk.data
        nltk.data.path.append('D:/Mes documents/MY_POS_tagger/nltk_data')
        print(""NLTK data path done:%s""%(path in nltk.data.path))

        #read a POSfile
        import nltk
        from nltk.corpus.reader import TaggedCorpusReader
        from nltk.tag import UnigramTagger

   #there's only one document malagasy.pos, it's there where my tagged corpora.

    reader = TaggedCorpusReader('D:/Mes documents/MY_POS_tagger/nltk_data/corpora/cookbook', r'.*\.pos')


    train_sents=reader.tagged_sents()
    tagger=UnigramTagger(train_sents)

#dago.txt contain just sentences without tag, i just wanted to test if the tag i assign on the POS file will work 

    text=(nltk.data.load('corpora/cookbook/dago.txt', format='raw'))
    text_tokenized=nltk.word_tokenize(text)
    print tagger.tag(text_tokenized)
</code></pre>

<p>I have this result:</p>

<pre><code>OS path done :True
NLTK data path done:True
[('Matory', u'VB'), ('ny', None), ('alika', u'NN')]
</code></pre>

<p>So i can see that it's work, but i read in the document above that i have to train my tagger. So i ask if someone can suggest me how i can do that, cause i read i need to pickle a trained tagger and to train and combin Ngram taggers but i don't understand what pickle means or do. And i don't know if what i'm doing now is the correct path to create and exploit a tagged corpora with NLTK.
Thank you</p>
"
"51513261","Transforming Text To Vector","2018-07-25 07:33:09","2","2561","1","2","","51520376","<p>I have a dictionary having words and the frequency of each words. </p>

<pre><code>{'cxampphtdocsemployeesphp': 1,
'emptiness': 1, 
'encodingundefinedconversionerror': 1, 
'msbuildexe': 2,
'e5': 1, 
'lnk4049': 1,
'specifierqualifierlist': 2, .... }
</code></pre>

<p>Now I want to create a bag of words model using this dictionary( I don't want to use standard library and function. I want to apply this using the algorithm.)</p>

<ol>
<li>Find N most popular words in the dictionary and numerate them. Now we have a dictionary of the most popular words.</li>
<li>For each title in the dictionary create a zero vector with the dimension equals to N.</li>
<li>For each text in the corpora iterate over words which are in the dictionary and increase by 1 the corresponding coordinate.</li>
</ol>

<p>I have my text which I will use to create the vector using a function.</p>

<p>The function would look like this,</p>

<pre><code>def my_bag_of_words(text, words_to_index, dict_size):
""""""
    text: a string
    dict_size: size of the dictionary

    return a vector which is a bag-of-words representation of 'text'
""""""


 Let say we have N = 4 and the list of the most popular words is 

['hi', 'you', 'me', 'are']

Then we need to numerate them, for example, like this: 

{'hi': 0, 'you': 1, 'me': 2, 'are': 3}

And we have the text, which we want to transform to the vector:
'hi how are you'

For this text we create a corresponding zero vector 
[0, 0, 0, 0]

And iterate over all words, and if the word is in the dictionary, we increase the value of the corresponding position in the vector:
'hi':  [1, 0, 0, 0]
'how': [1, 0, 0, 0] # word 'how' is not in our dictionary
'are': [1, 0, 0, 1]
'you': [1, 1, 0, 1]

The resulting vector will be 
[1, 1, 0, 1]
</code></pre>

<p>Any help in applying this would be really helpful. I am using python for implementation.</p>

<p>Thanks,</p>

<p>Neel</p>
"
"51415694","keras add features after embedding","2018-07-19 06:23:51","0","1258","3","2","","51416062","<p>I want to add part of speech features into my word vector after embedding in Keras. I would like to add them as one hot and concat them after embedding. But the part of speech of a word is dynamic so I can't use another embedding layer for part of speech one hot look up and combine two embedding layers.</p>
"
"51371356","How to improve Precision and Recall on Imbalanced Dataset in Python","2018-07-16 23:38:15","2","4111","1","1","","51371428","<p>I built a supervised model to classify medical text data (my output predicts the positive or negative occurrence of a disease). The data is very imbalanced (130 positive cases compared to 1600 negative cases, which is understandable since the disease is rare). I first cleaned the data (removed unnecessary words, lemmatization, etc..) and applied POS afterwards. I then applied TfidfVectorizer and TfidfTransformer to this cleaned data. For classification, I tried both SVM and Random Forest, but achieved only 56% precision and 58% recall for the positive data even after tuning their parameters with GridSearchCV (I also made class_weight = 'balanced'). Does anyone have advice as to how to improve this low precision and recall? Thank you very much.</p>

<p>Here is my current Pipeline (obviously I only use one of the classifiers when I run it, but I displayed both just to show their parameters).</p>

<pre><code>pipeline = Pipeline([ 

('vectors', TfidfVectorizer(ngram_range = (2,3),norm = 'l1', token_pattern = r""\w+\b\|\w+"" ,min_df = 2, max_features = 1000).fit(data['final'])),

('classifier', RandomForestClassifier(n_estimators = 51, min_samples_split = 8, min_samples_leaf = 2, max_depth = 14, class_weight= 'balanced')),

('classifier', SVC(C = 1000, gamma = 1, class_weight = 'balanced', kernel='linear')),

])
</code></pre>
"
"51337884","How to extract a topic from a sentence?","2018-07-14 11:10:27","1","367","0","3","","51338217","<p>I wish to tabulate all the topics from questions asked in a question paper. This is an example of the format of two questions asked in the paper:</p>

<pre><code>question1 = 'Write short notes on the anatomy of the Circle of Willis including normal variants.'
question2 = 'Write short notes on the anatomy of the axis (C2 vertebra).'
</code></pre>

<p>From the above questions, I expect to get the topics:</p>

<pre><code>topic1 = 'Circle of Willis including normal variants'
topic2 = 'axis (C2 vertebra)'
</code></pre>

<p>For the above, I wrote the following code snippet:</p>

<pre><code>def extract_topic(message):
    message = re.search('Write short notes on the anatomy of the (.+?).', message)
    if message:
        return message.group(1)
</code></pre>

<p>Of course, the above code failed miserably! What am I to do? What's the easiest way to do the above? Would using NLTK make the above easy?</p>
"
"51316438","German stemmer is not removing feminine suffixes ""-in"" and ""-innen""","2018-07-13 01:17:17","3","929","0","1","","51318333","<p>In German, every job has a feminine and a masculine version. The feminine one is derived from the masculine one by adding an ""-in"" suffix. In the plural form, this turns into ""-innen"".</p>

<p>Example:</p>

<pre><code>      | English          | German
------+------------------+-----------------------
masc. | teacher  doctor  | Lehrer      Arzt
fem.  | teacher  doctor  | Lehrerin    Ärztin
masc. | teachers doctors | Lehrer      Ärzte
fem.  | teachers doctors | Lehrerinnen Ärztinnen
</code></pre>

<p>Currently, I'm using NLTK's <code>nltk.stem.snowball.GermanStemmer</code>. 
It returns these stems:</p>

<pre><code>Lehrer      -&gt; lehr      | Arzt      -&gt; arzt
Lehrerin    -&gt; lehrerin  | Ärztin    -&gt; arztin
Lehrer      -&gt; lehr      | Ärzte     -&gt; arzt
Lehrerinnen -&gt; lehrerinn | Ärztinnen -&gt; arztinn
</code></pre>

<p>Is there a way to make this stemmer return the same stems for all four versions, feminine and masculine ones? Alternatively, is there any other stemmer doing that?</p>

<h2>Update</h2>

<p>I ended up adding ""-innen"" and ""-in"" as the first entries in the step 1 suffix-tuple like so:</p>

<pre><code>stemmer = GermanStemmer()
stemmer._GermanStemmer__step1_suffixes = (""innen"", ""in"") + stemmer._GermanStemmer__step1_suffixes
</code></pre>

<p>This way all of the above words are stemmed to <code>lehr</code> and <code>arzt</code> respectively. Also, all other ""job-forms"" that I tried so far are stemmed correctly, meaning masculine and feminine forms have the same stem. Also, if the ""job-form"" is derived from a verb, like <code>Lehrer/in</code>, they have the same stem as the verb.</p>
"
"51259007","Dependency parsing using spacy","2018-07-10 07:01:29","0","1113","0","1","","51259828","<p>I have a code for dependency parsing which gives output in the form of arcs. Is there any other way to display the parse tree for a paragraph? Because for a paragraph, the parse tree is huge. Is there a better way to display the parse tree for a paragraph?</p>
"
"51252914","Finding whether or not, a word is on the dependency path of two entities with spaCy","2018-07-09 19:39:06","2","2063","0","2","","51390766","<p>I'm working on a nlp problem, given a sentence with two entities I need to generate boolean indicating for each word if it stands on the dependency path between those entities.</p>

<p>For example:</p>

<blockquote>
  <p>'A misty &lt; e1 >ridge&lt; /e1 > uprises from the &lt; e2 >surge&lt; /e2 >'</p>
</blockquote>

<p>I want to iterate on each words and tell if it is on the dependency path between e1 and e2</p>

<p><strong>Two important notes:</strong></p>

<p>-If you try to help me (first thanks), don't bother considering the xml markup with &lt; e1 > and &lt; e2 >, I really am interested in how to find if a word is on the dependency path between any two given words with spaCy, I take care of which words by myself</p>

<p>-As I'm not a nlp expert, I'm kind of confused with the meaning of ""on the dependency path"" and I'm sorry if it is not clear enough (these are the words used by my tutor)</p>

<p>Thanks in advance</p>
"
"51239434","Why is NLTK's PoS tagger tagging for each letter in a word instead of tagging for each word?","2018-07-09 06:37:37","3","1620","0","2","","51239567","<p>Say I have this sentence: <code>I am a boy</code>. I want to find out the Part of Speech of each word in the sentence. This is my code:</p>

<pre><code>import nltk
sentence = 'I am a good boy'
for word in sentence:
    print(word)
    print(nltk.pos_tag(word))
</code></pre>

<p>But this produces the following output:</p>

<pre><code>I
[('I', 'PRP')]

[(' ', 'NN')]
a
[('a', 'DT')]
m
[('m', 'NN')]

[(' ', 'NN')]
a
[('a', 'DT')]

[(' ', 'NN')]
g
[('g', 'NN')]
o
[('o', 'NN')]
o
[('o', 'NN')]
d
[('d', 'NN')]

[(' ', 'NN')]
b
[('b', 'NN')]
o
[('o', 'NN')]
y
[('y', 'NN')]
</code></pre>

<p>So, I tried to do this instead:</p>

<pre><code>sentence = 'I am a good boy'
for word in sentence.split(' '):
    print(word)
    print(nltk.pos_tag(word))
</code></pre>

<p>And this produces the following output:</p>

<pre><code>I
[('I', 'PRP')]
am
[('a', 'DT'), ('m', 'NN')]
a
[('a', 'DT')]
good
[('g', 'NN'), ('o', 'MD'), ('o', 'VB'), ('d', 'NN')]
boy
[('b', 'NN'), ('o', 'NN'), ('y', 'NN')]
</code></pre>

<p>Why is it finding the PoS for each letter instead of each word? And how do I fix this?</p>
"
"51189483","Comparing strings in huge lists but cannot use set in Python","2018-07-05 10:52:24","-1","59","5","3","","51189990","<p>I have a text file with 11965 entries that looks like:</p>

<pre><code>AAA
BBB
CCC
DDD

Which I transformed into:
list_1 = ['AAA', 'BBB', 'CCC', ...]
</code></pre>

<p>And I need to compare it with another text file with 2221545 entries that looks like:</p>

<pre><code>AAA,.ADJ UK
AAA,.N UK
AAA,.N ES
B,.ADV UK
BB,.ADV UK
BBB,.N IT

Which I transformed into:
list_2 = ['AAA\tADJ\tUK', 'AAA\tN\tUK', 'AAA\tN\tES', 'B\tADV\UK', 'BB\tADV\tUK', ...]
</code></pre>

<p>So I have to get a dict that looks like this:</p>

<pre><code>result_dict = {'AAA':[[UK, ADJ, N], [ES,N]], 'BBB':[[IT,N]], ...}
</code></pre>

<p>Due to the size of the second list, if we compare the entries one by one the time complexity will be <code>O(11965*2221545)</code>. (Am I getting in right?)</p>

<p>And because I have to get the entire entry, I cannot use set to compare them. Is there any efficient way to get the job done?</p>
"
"51097463","Snowball Stemmer : poor french stemming","2018-06-29 08:31:18","1","3912","2","1","","51107197","<p>I'm dealing with some nlp tasks. My inputs are french text and so, only Snowball Stemmer is usable in my context. But, unfortunately, it keeps giving me poor stems as it wouldn't remove even <code>plural ""s""</code> or <code>silent e</code>. Below is some example:</p>

<pre><code>from nltk.stem import SnowballStemmer
SnowballStemmer(""french"").stem(""pommes, noisettes dorées &amp; moelleuses, la boîte de 350g"")
Output: 'pommes, noisettes dorées &amp; moelleuses, la boîte de 350g'
</code></pre>
"
"51011474","Identify words that appear in less than 1% of the corpus documents","2018-06-24 15:44:47","2","478","4","1","","51011519","<p>I have a corpus of customer reviews and want to identify rare words, which for me are words that appear in less than 1% of the corpus documents.</p>

<p>I already have a working solution, but it is far too slow for my script:</p>

<pre><code># Review data is a nested list of reviews, each represented as a bag of words
doc_clean = [['This', 'is', 'review', '1'], ['This', 'is', 'review', '2'], ..] 

# Save all words of the corpus in a set
all_words = set([w for doc in doc_clean for w in doc])

# Initialize a list for the collection of rare words
rare_words = []

# Loop through all_words to identify rare words
for word in all_words:

    # Count in how many reviews the word appears
    counts = sum([word in set(review) for review in doc_clean])

    # Add word to rare_words if it appears in less than 1% of the reviews
    if counts / len(doc_clean) &lt;= 0.01:
        rare_words.append(word)
</code></pre>

<p>Does anyone know a faster implementation for this? It seems to be very time-consuming to iterate for each individual words through each individual review.</p>

<p>Thanks in advance and best wishes,
Marcus</p>
"
"50972571","NLTK POS tag: how to put the 'Word' and its corresponding 'POS Tag' in a DataFrame","2018-06-21 15:40:30","2","191","0","1","","50972645","<p>I have a list like 
        <code>list = ['about','above','account','address','after']</code>
 which i passed into the <strong>nltk pos tag</strong> function and the output looks like this in form of a list:</p>

<pre><code>[('about', 'IN'),('above', 'JJ'),('account', 'NN'),('address', 'NN'),('after', 'IN')]
</code></pre>

<p>now i need to display these words and pos tags seperately in form of a <strong>DataFrame</strong> so the output looks something like this:</p>

<pre><code>Words   Pos Tags
about     IN
above     JJ
account   NN
address   NN
after     IN
</code></pre>
"
"50941438","NLTK - Replace chunks with specific word","2018-06-20 06:16:33","0","1141","2","1","","50945216","<p>I am working on NLP using nltk. I am using chunking to extract names of people. After chunking I want to replace the chunks with specific strings 'Male' or 'Female'.</p>

<p>My code is :</p>

<pre><code>import nltk

with open('male_names.txt') as f1:
    male = [line.rstrip('\n') for line in f1]
with open('female_names.txt') as f2:
     female = [line.rstrip('\n') for line in f2]

with open(""input.txt"") as f:
    text = f.read()

words = nltk.word_tokenize(text)
tagged = nltk.pos_tag(words)
chunkregex = r""""""Name: {&lt;NNP&gt;+}""""""
chunkParser = nltk.RegexpParser(chunkregex)
chunked = chunkParser.parse(tagged)

for subtree in chunked.subtrees(filter=lambda t: t.label() == 'Name'):
    chunk=[]
    for word, pos in subtree:
        chunk.append(word)
        temp = "" "".join(chunk)
    **if temp in male:
        subtree = ('Male', pos)
    if temp in female:
        subtree = ('Female', pos)**
    print subtree

print chunked
</code></pre>

<p>My input data is :</p>

<blockquote>
  <p>Captain Jack Sparrow arrives in Port Royal in Jamaica to commandeer a ship. Despite rescuing Elizabeth Swann, the daughter of Governor Weatherby Swann, from drowning, he is jailed for piracy.</p>
</blockquote>

<p>The current output is :</p>

<blockquote>
  <p>(S
    <code>(Name Captain/NNP Jack/NNP Sparrow/NNP)</code>
    arrives/VBZ
    in/IN
    (Name Port/NNP Royal/NNP)
    in/IN
    (Name Jamaica/NNP)
    to/TO
    commandeer/VB
    a/DT
    ship/NN
    ./.
    Despite/IN
    rescuing/VBG
    <code>(Name Elizabeth/NNP Swann/NNP)</code>
    ,/,
    the/DT
    daughter/NN
    of/IN
    <code>(Name Governor/NNP Weatherby/NNP Swann/NNP)</code>
    ,/,
    from/IN
    drowning/VBG
    ,/,
    he/PRP
    is/VBZ
    jailed/VBN
    for/IN
    piracy/NN
    ./.)</p>
</blockquote>

<p>I want to replace the chunks with 'Male' or 'Female' which should give the output as :</p>

<blockquote>
  <p>(S
    <code>Male/NNP</code>
    arrives/VBZ
    in/IN
    (Name Port/NNP Royal/NNP)
    in/IN
    (Name Jamaica/NNP)
    to/TO
    commandeer/VB
    a/DT
    ship/NN
    ./.
    Despite/IN
    rescuing/VBG
    <code>Female/NNP</code>
    ,/,
    the/DT
    daughter/NN
    of/IN
    <code>Male/NNP</code>
    ,/,
    from/IN
    drowning/VBG
    ,/,
    he/PRP
    is/VBZ
    jailed/VBN
    for/IN
    piracy/NN
    ./.)</p>
</blockquote>

<p>The bold part in the code is not doing what it's supposed to. The <code>print subtree</code> statement shows the changes but <code>print chunked</code> does not change.</p>

<p>What am I doing wrong or is there any other way ?<br>
I am new to python and nltk. Any help appreciated.</p>

<p><code>male</code> and <code>female</code> contain list of names as :</p>

<blockquote>
  <p>[""Captain Jack Sparrow"", ""Governor Weatherby Swann"", ""Robin""]</p>
  
  <p>[""Elizabeth Swann"", ""Jenny""]</p>
</blockquote>
"
"50918092","Given cluster of documents, compute similarity between corpus and the cluster","2018-06-18 22:06:03","2","812","3","1","","50935353","<p>I am doing a similarity ranking job by computing the distance between each document in the corpus and the cluster. The cluster is also given as list of documents. What I am in trouble with is that I cannot come up with <strong>a proper way of computing the centroid of the cluster</strong> so that I can compute the similarity. I tried to use the average value of tfidf matrix of cluster while it gives poor result.</p>

<p>For example: my cluster is:</p>

<pre><code>['Line a baking pan with a sheet of parchment paper.',
 'Line the cake pan with parchment paper.',
 'Line the bottom with parchment paper.',
 'Line a baking pan with parchment paper.'
]
</code></pre>

<p>And my courpus contains following 3 documents:</p>

<pre><code>['Add vinegar and sugar.',
 'Remove pan from heat and let stand 5 minutes.',
 'Line the pan with parchment paper.'
]
</code></pre>

<p>I want to compute the similarity between every document and the cluster, which might produce result like:</p>

<pre><code>[0.1, 0.1, 0.8]
</code></pre>

<p>Do you have any suggestion? I tried represent both cluster and corpus documents as tfidf matrices, but it seems hard to give desire result by computing similarity between two matrix. And I tried LSI, but it is the corpus I want to rank not the cluster documents which forces me to find the centroid representative of the cluster.</p>
"
"50906210","Confused with the return result of TfidfVectorizer.fit_transform","2018-06-18 09:19:26","13","11969","0","1","","50906599","<p>I wanted to learn more about NLP. I came across this piece of code. But I was confused about the outcome of <code>TfidfVectorizer.fit_transform</code> when the result is printed. I am familiar with what tfidf is but I could not understand what the numbers mean.</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
import os
import io
import string
import requests
import csv
import nltk
from zipfile import ZipFile

sess = tf.Session()

batch_size = 100
max_features = 1000

save_file_name = os.path.join('smsspamcollection', 'SMSSpamCollection.csv')
if os.path.isfile(save_file_name):
    text_data = []
    with open(save_file_name, 'r') as temp_output_file:
        reader = csv.reader(temp_output_file)
        for row in reader:
            text_data.append(row)

else:
    zip_url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'
    r = requests.get(zip_url)
    z = ZipFile(io.BytesIO(r.content))
    file = z.read('SMSSpamCollection')

    # Format data 
    text_data = file.decode()
    text_data = text_data.encode('ascii', errors='ignore')
    text_data = text_data.decode().split('\n')
    text_data = [x.split('\t') for x in text_data if len(x) &gt;= 1]

    # And write to csv 
    with open(save_file_name, 'w') as temp_output_file:
        writer = csv.writer(temp_output_file)
        writer.writerows(text_data)

texts = [x[1] for x in text_data]
target = [x[0] for x in text_data]
target = [1 if x == 'spam' else 0 for x in target]

# Normalize the text
texts = [x.lower() for x in texts]  # lower
texts = [''.join(c for c in x if c not in string.punctuation) for x in texts]  # remove punctuation
texts = [''.join(c for c in x if c not in '0123456789') for x in texts]  # remove numbers
texts = [' '.join(x.split()) for x in texts]  # trim extra whitespace


def tokenizer(text):
    words = nltk.word_tokenize(text)
    return words


tfidf = TfidfVectorizer(tokenizer=tokenizer, stop_words='english', max_features=max_features)
sparse_tfidf_texts = tfidf.fit_transform(texts)
print(sparse_tfidf_texts)
</code></pre>
<p>And the output is:</p>
<blockquote>
<p>(0, 630)  0.37172623140154337   (0, 160)  0.36805562944957004   (0,
38)   0.3613966215413548   (0, 545)   0.2561101665717327   (0,
326)  0.2645280991765623   (0, 967)   0.3277447602873963   (0,
421)  0.3896274380321477   (0, 227)   0.28102915589024796   (0,
323)  0.22032541100275282   (0, 922)  0.2709848154866997   (1,
577)  0.4007895093299793   (1, 425)   0.5970064521899725   (1,
943)  0.6310763941180291   (1, 878)   0.29102173465492637   (2,
282)  0.1771481430848552   (2, 243)   0.5517018054305785   (2,
955)  0.2920174942032025   (2, 138)   0.30143666813167863   (2,
946)  0.2269933441326121   (2, 165)   0.3051095293405041   (2,
268)  0.2820392223588522   (2, 780)   0.24119626642264894   (2,
823)  0.1890454397278538   (2, 674)   0.256251970757827   (2,
874)  0.19343834015314287   : :   (5569, 648) 0.24171652492226922<br />
(5569, 123)   0.23011909339432202   (5569, 957)   0.24817919217662862<br />
(5569, 549)   0.28583789844730134   (5569, 863)   0.3026729783085827<br />
(5569, 844)   0.20228305447951195   (5569, 146)   0.2514415602877767<br />
(5569, 595)   0.2463259875380789   (5569, 511)    0.3091904754885042<br />
(5569, 230)   0.2872728684768659   (5569, 638)    0.34151390143548765<br />
(5569, 83)    0.3464271621701711   (5570, 370)    0.4199910200421362<br />
(5570, 46)    0.48234172093857797   (5570, 317)   0.4171646676697801<br />
(5570, 281)   0.6456993475093024   (5572, 282)    0.25540827228532487<br />
(5572, 385)   0.36945842040023935   (5572, 448)   0.25540827228532487<br />
(5572, 931)   0.3031800542518209   (5572, 192)    0.29866989620926737<br />
(5572, 303)   0.43990016711221736   (5572, 87)    0.45211284173737176<br />
(5572, 332)   0.3924202767503492   (5573, 866)    1.0</p>
</blockquote>
<p>I would be more than happy if someone can explain about the output.</p>
"
"50814296","Syntax error when using list comprehensions in Python 3.6 to loop over and compare dependency triplets for two sentences","2018-06-12 09:55:24","2","312","0","2","","50816168","<p>I have the following two sentences:</p>

<ol>
<li>I want to go home.</li>
<li>I would like to leave.</li>
</ol>

<p>My goal is to quantify similarity between the two sentences using a kernel suggested in 
<a href=""https://www.cmpe.boun.edu.tr/~ozgur/papers/617_Paper.pdf"" rel=""nofollow noreferrer"">this paper</a>. I extract all the dependency triplets for each sentence. These are 3 item tuples containing all the relations between words in the sentence and look like <em>(tail, relationship, head)</em>.</p>

<p>To calculate similarity, I need to loop through every possible combination of triplet across sentences and add a particular number to the similarity score based on how many nodes match and whether the relationship matches.</p>

<p>I attempted using list comprehensions inside a for loop since I figured it would be more efficient than another nested for loop but am getting a syntax error. Here's my code:</p>

<pre><code>sim = 0
theta = 2.5

for d1 in deps1:
    [sim += theta for d2 in deps2 if ((d1[0]==d2[0] or d1[2]==d2[2]) and d1[1]==d2[1])]
    [sim += 1 for d2 in deps2 if ((d1[0]==d2[0] or d1[2]==d2[2]) and d1[1]!=d2[1])]
</code></pre>

<p>For reference, here's what deps1 and deps2 look like when printed:</p>

<pre><code>[('I', 'nsubj', 'want'), ('want', 'ROOT', 'want'), ('to', 'aux', 'go'), ('go', 'xcomp', 'want'), ('home', 'advmod', 'go')]
[('I', 'nsubj', 'like'), ('would', 'aux', 'like'), ('like', 'ROOT', 'like'), ('to', 'aux', 'leave'), ('leave', 'xcomp', 'like')]
</code></pre>

<p>Questions:</p>

<ol>
<li>What's the correct syntax to do this with a list comprehension?</li>
<li>Is there a more efficient way, maybe using numpy(?), to do this computation?</li>
</ol>
"
"50742516","How to get the index of a token in a sentence in spaCy?","2018-06-07 13:27:32","11","15054","0","1","","50751811","<p>Is there an elegant way to get the index of a word/token in its sentence?
I am aware of the attributes for tokens <a href=""https://spacy.io/api/token#attributes"" rel=""noreferrer"">https://spacy.io/api/token#attributes</a>
The <code>i</code> attribute returns the index within the whole parent document. But the parent document can contain multiple sentences.</p>
<p>Example:</p>
<blockquote>
<p>&quot;This is an example. This is another example.&quot;</p>
</blockquote>
<p>What I need is both <code>&quot;This&quot;</code> to be returned as index <code>0</code>, both <code>&quot;is&quot;</code> to be returned as index <code>1</code> etc...</p>
"
"50711654","Converting Dependency tree into sequence of Arc-eager transitions","2018-06-06 03:00:46","0","551","0","1","","50722360","<p>Currently I'm trying to build syntax-aware NMT model.<br>
In this project, I need the sequence of one of three transition actions (SHIFT, REDUCE-L, REDUCE-R)</p>

<p>Similar to what is in the image 
<a href=""https://i.sstatic.net/09Ian.png"" rel=""nofollow noreferrer"">a</a></p>

<p>This chunk represents the transition-based dependency for 2 sentences(1 for 1 chunk split by empty lines) </p>

<p>I'm using <code>Syntaxnet</code> to get the dependency parse tree first, but it doesn't directly provide that transition action sequences. <br>
It's results are as follows,</p>

<p><a href=""https://i.sstatic.net/4t5IZ.png"" rel=""nofollow noreferrer"">b</a></p>

<p>Is it possible to get the action sequences similar to this image? Is it possible to convert what is achieved from this image to the original image's format.</p>
"
"50685343","How to lemmatize a list of sentences","2018-06-04 16:51:22","6","14013","0","2","","50689970","<p>How can I lemmatize a list of sentences in Python?</p>

<pre><code>from nltk.stem.wordnet import WordNetLemmatizer
a = ['i like cars', 'cats are the best']
lmtzr = WordNetLemmatizer()
lemmatized = [lmtzr.lemmatize(word) for word in a]
print(lemmatized)
</code></pre>

<p>This is what I've tried but it gives me the same sentences. Do I need to tokenize the words before to work properly?</p>
"
"50598129","What is the default smartirs for gensim TfidfModel?","2018-05-30 06:54:18","5","1556","0","1","","53999185","<p>Using <code>gensim</code>:</p>

<pre><code>from gensim.models import TfidfModel
from gensim.corpora import Dictionary

sent0 = ""The quick brown fox jumps over the lazy brown dog ."".lower().split()
sent1 = ""Mr brown jumps over the lazy fox ."".lower().split()

dataset = [sent0, sent1]
vocab = Dictionary(dataset)
corpus = [vocab.doc2bow(sent) for sent in dataset] 
model = TfidfModel(corpus)

# To retrieve the same pd.DataFrame format.
documents_tfidf_lol = [{vocab[word_idx]:tfidf_value for word_idx, tfidf_value in sent} for sent in model[corpus]]
documents_tfidf = pd.DataFrame(documents_tfidf_lol)
documents_tfidf.fillna(0, inplace=True)

documents_tfidf
</code></pre>

<p>[out]:</p>

<pre><code>    dog mr  quick
0   0.707107    0.0 0.707107
1   0.000000    1.0 0.000000
</code></pre>

<p>If we do the TF-IDF computation manually, </p>

<pre><code>sent0 = ""The quick brown fox jumps over the lazy brown dog ."".lower().split()
sent1 = ""Mr brown jumps over the lazy fox ."".lower().split()

documents = pd.DataFrame.from_dict(list(map(Counter, [sent0, sent1])))
documents.fillna(0, inplace=True, downcast='infer')
documents = documents.apply(lambda x: x/sum(x))  # Normalize the TF.
documents.head()

# To compute the IDF for all words.
num_sentences, num_words = documents.shape

idf_vector = [] # Lets save an ordered list of IDFS w.r.t. order of the column names.

for word in documents:
  word_idf = math.log(num_sentences/len(documents[word].nonzero()[0]))
  idf_vector.append(word_idf)

# Compute the TF-IDF table.
documents_tfidf = pd.DataFrame(documents.as_matrix() * np.array(idf_vector), 
                               columns=list(documents))
documents_tfidf
</code></pre>

<p>[out]:</p>

<pre><code>    .   brown   dog fox jumps   lazy    mr  over    quick   the
0   0.0 0.0 0.693147    0.0 0.0 0.0 0.000000    0.0 0.693147    0.0
1   0.0 0.0 0.000000    0.0 0.0 0.0 0.693147    0.0 0.000000    0.0
</code></pre>

<p>If we use <code>math.log2</code> instead of <code>math.log</code>:</p>

<pre><code>    .   brown   dog fox jumps   lazy    mr  over    quick   the
0   0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
1   0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
</code></pre>

<p>It looks like <code>gensim</code>:</p>

<ul>
<li>remove the non-salient words from the TF-IDF model, it's evident when we <code>print(model[corpus])</code></li>
<li>maybe the log base seem to be different from the log_2</li>
<li>maybe there's some normalization going on. </li>
</ul>

<p>Looking at <a href=""https://radimrehurek.com/gensim/models/tfidfmodel.html#gensim.models.tfidfmodel.TfidfModel"" rel=""noreferrer"">https://radimrehurek.com/gensim/models/tfidfmodel.html#gensim.models.tfidfmodel.TfidfModel</a> , the <code>smart</code> scheme difference would have output different values but it's not clear in the docs what is the default value.</p>

<p><strong>What is the default smartirs for gensim TfidfModel?</strong></p>

<p><strong>What are the other default parameters that've caused the difference between a natively implemented TF-IDF and gensim's?</strong></p>
"
"50533070","How to quickly check strings for correct English words? - Python","2018-05-25 16:02:38","1","1119","2","1","","50533439","<p>I have a column in a pandas dataframe where each cell contains a rather long string of words. These strings are from an SQL database and contain a mix of words and alphanumeric id phrases which are not English, separated by spaces. These strings can be up to the character max of SQL. This is also not a small dataframe, i have several million rows. </p>

<p>The question is, what is the fastest way to keep only correct English words for each cell?</p>

<p>Below is my initial method which seemingly would take days to complete based on the speed suggested from tqdm (hence the progress_apply). </p>

<pre><code>import pandas as pd
from nltk.corpus import words
from tqdm import tqdm

def check_for_word(sentence):
    s = sentence.split(' ')
    for word in s:
        if word not in words.words():
            s.remove(word)
    return ' '.join(s)

tqdm.pandas(desc=""Checking for Words in keywords"")
df['keywords'] = df['keywords'].progress_apply(check_for_word)  
</code></pre>

<p>Is there a method which would be significantly faster? </p>

<p>Thanks for your help!</p>

<p>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</p>

<p>The answer below was very helpful and took less than a second to run (GREAT IMPROVEMENT!). In the end I had to change from nltk.corpus words to nltk.corpus wordnet, as words was not exhaustive enough of a list for my purposes. The final result ended up being: </p>

<pre><code>from nltk.corpus import wordnet
from tqdm import tqdm

def check_for_word(s):
    return ' '.join(w for w in str(s).split(' ') if len(wordnet.synsets(w)) &gt; 0)

tqdm.pandas(desc=""Checking for Words in Keywords"")
df['keywords'] = df['keywords'].progress_apply(check_for_word)
</code></pre>

<p>which took 43 seconds to run. </p>
"
"50508049","Stemming and Lemmatization with Python NLTK for both language as English and Russia","2018-05-24 11:20:27","1","16054","0","1","","50508249","<h1>""Stemming and Lemmatization with Python NLTK for both language as English and Russia""</h1>

<p>Source: <a href=""http://text-processing.com/demo/stem/"" rel=""nofollow noreferrer"">http://text-processing.com/demo/stem/</a></p>

<p>I want to use the lib for stemming with Python NLTK for both language as English and Russia.</p>

<p>Could you please give me advice which lib needs to use for this task.</p>
"
"50379985","NLP PROLOG Grammar","2018-05-16 21:14:54","1","200","1","2","","50401662","<p>We have the formal language </p>

<pre><code>G 1 = { V , T , S , P }, where
V = { S , E }
T = { x , y , z }
P = { S-&gt;E , E-&gt;xE , E-&gt;yE , E-&gt;z }
</code></pre>

<p>Can we accept the seven sentences { xz , xy , xyz , xyxz , z , xxyz , Xyz } as well-formed formulas?Verify this using Prolog.</p>

<p>here is my code:</p>

<pre><code>s --&gt; e.
e --&gt; [x], e.
e --&gt; [y], e.
e --&gt; [z].
</code></pre>

<p>It can only recognize s([z], R). Why?</p>

<pre><code>?- s([z], R).
R = [].

?- s([xz], R).
false.

?- s([x], R).
false.
</code></pre>
"
"50298074","Extract probabilities and most likely parse tree from cyk","2018-05-11 18:16:59","0","1680","0","1","","50351368","<p>In order to understand cyk algorithm I've worked through example on : <a href=""https://www.youtube.com/watch?v=VTH1k-xiswM&amp;feature=youtu.be"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=VTH1k-xiswM&amp;feature=youtu.be</a> . </p>

<p>The result of which is : </p>

<p><a href=""https://i.sstatic.net/RR78n.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/RR78n.png"" alt=""enter image description here""></a></p>

<p>How do I extract the probabilities associated with each parse and extract the most likely parse tree ?</p>
"
"50260935","GATE: JAPE rule Java RHS feature map","2018-05-09 19:42:55","0","729","0","2","","50271356","<p>I am trying to get existing annotations and their features within a Sentence annotation i.e for each sentence, there may be multiple annotations which have a i.e majorType, string and type features.</p>

<p>I want a new ‘Sentence contains’ annotation with a feature map of the contained annotations and their respective features.</p>

<p>I believe it should be an extension of the below rule from the excellent Gate Jape Grammar Tutorial pdf : </p>

<pre><code>Phase:usingJAVAinRHS  
Input:  Lookup  
Options: control = all  
Rule: javainRHS1  
(  
{Lookup.majorType == Team}  
)  
:team  
--&gt;  
{  
gate.AnnotationSet team = (gate.AnnotationSet)bindings.get(""team"");       
gate.Annotation teamAnn = (gate.Annotation)team.iterator().next();   
gate.FeatureMap features = Factory.newFeatureMap(); 
features.put(""teamOfSport"", teamAnn.getFeatures().get(""minorType""));  
features.put(""rule"",""javainRHS1"");  
outputAS.add(team.firstNode(), team.lastNode(), ""Team"",features); }
</code></pre>

<p>Except in my new rule, I want to annotate the Sentence, then get the contained annotation:</p>

<pre><code>Phase:usingJAVAinRHS  
Input:  Lookup Sentence  
Options: control = all  
Rule: javainRHS1  
(  
{Sentence contains {Lookup.majorType == Team}}  
)  
:team  
--&gt;  
{  
gate.AnnotationSet team = (gate.AnnotationSet)bindings.get(""team"");   
gate.Annotation teamAnn = (gate.Annotation)team.iterator().next();   
gate.FeatureMap features = Factory.newFeatureMap(); 
features.put(""teamOfSport"",   teamAnn.getFeatures().get(""minorType""));  
features.put(""rule"",""javainRHS1"");  
outputAS.add(team.firstNode(), team.lastNode(), ""Team"",features); }  
</code></pre>

<p>How do you get the feature map of the contained annotations? </p>

<p>Many thanks</p>
"
"50229769","Confusion about word hashing in DSSM?","2018-05-08 08:57:40","1","1278","0","1","","50232063","<p>In this paper <a href=""https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/cikm2013_DSSM_fullversion.pdf"" rel=""nofollow noreferrer"">Learning Deep Structured Semantic Models for Web Search using Clickthrough Data</a>, it uses the word hashing technique to convert a one-hot representation of a word to a (sparse) vector of letter trigrams.</p>

<p>From my understanding, for example, a word <code>look</code> is first decomposed in to letter trigrams <code>[#lo, loo, ook, ok#]</code> then is represented as a vector with ones for each of these trigrams and zeros elsewhere. By doing this it can reduce the dimension of a word vector while having very few collisions as said in the paper.</p>

<p>My confusion is, normally if we use bag-of-words representations to represent a document based on the one-hot representation, we just count the occurrences of each word. However I can imagine if we use bag-of-words based on letter trigrams there'll easily be different words sharing common patterns so it seems difficult to recover the information of which words are in the document by such representation.</p>

<p>Did I understand correctly? How was this issue solved? or it doesn't really matter to the query/title experiment in the paper?</p>
"
"50191231","How do I find a synonym of a word or multi-word paraphrase using the gensim toolkit","2018-05-05 15:44:52","2","3434","1","1","","50191902","<p>Having loaded a pre-trained word2vec model with the gensim toolkit, I would like to find a synonym of a word given a context such as intelligent for 'she is a bright person'.</p>
"
"50155188","Lemmatization on CountVectorizer doesn't remove Stopwords","2018-05-03 12:32:17","5","7587","4","1","","50158249","<p>I'm trying to add Lematization to CountVectorizer from Skit-learn,as follows</p>

<pre><code>import nltk
from pattern.es import lemma
from nltk import word_tokenize
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer
from nltk.stem import WordNetLemmatizer

class LemmaTokenizer(object):
    def __call__(self, text):
        return [lemma(t) for t in word_tokenize(text)]

vectorizer = CountVectorizer(stop_words=stopwords.words('spanish'),tokenizer=LemmaTokenizer())

sentence = [""EVOLUCIÓN de los sucesos y la EXPANSIÓN, ellos juegan y yo les dije lo que hago"",""hola, qué tal vas?""]

vectorizer.fit_transform(sentence)
</code></pre>

<p>This is the output:</p>

<pre><code>[u',', u'?', u'car', u'decir', u'der', u'evoluci\xf3n', u'expansi\xf3n', u'hacer', u'holar', u'ir', u'jugar', u'lar', u'ler', u'sucesos', u'tal', u'yar']
</code></pre>

<p><strong>UPDATED</strong></p>

<p>This is the Stopwords that appears and has been lemmatized:</p>

<blockquote>
  <p>u'lar', u'ler', u'der'</p>
</blockquote>

<p>It lemmatice all words and doesn't remove Stopwords. So, any idea?</p>
"
"50039310","Is nltk wordnet lemmatizer language independent?","2018-04-26 09:21:04","4","4475","1","1","","50039385","<p>Is it true that <a href=""http://www.nltk.org/_modules/nltk/stem/wordnet.html"" rel=""nofollow noreferrer"">nltk's wordnet lemmatizer</a> does not depend on the language of the input text ? Would I use the same sequence of commands:</p>

<pre><code>&gt;&gt;&gt; from nltk.stem import WordNetLemmatizer
&gt;&gt;&gt; wnl = WordNetLemmatizer()
&gt;&gt;&gt; print(wnl.lemmatize('dogs'))
dog
&gt;&gt;&gt; print(wnl.lemmatize('churches'))
church
&gt;&gt;&gt; print(wnl.lemmatize('aardwolves'))
aardwolf
&gt;&gt;&gt; print(wnl.lemmatize('abaci'))
abacus
&gt;&gt;&gt; print(wnl.lemmatize('hardrock'))
hardrock
</code></pre>

<p>for both english and french for instance ?</p>
"
"49959230","How to implement a syntax analyzer for a chatbot?","2018-04-21 18:58:14","0","301","0","1","","49966693","<p>I am actually trying to make a simple chatbot for information retrieval purposes in slack using python and I have come up with a Context Free Grammar (CFG) for synatx check . Now that I have a grammar, I want to create a parsing table/ parse tree for this grammar to validate my input string. It would be really helpful if you could let me know some libraries/ links/ mateirals that can help me implement a parser to perform syntax check for my chatbot.</p>

<p>Any help is appreciated. Thanks.</p>
"
"49944599","Using spacy and textacy. Need to find tf-idf score across corpus of original tweets but cant import textacy vectorizer","2018-04-20 15:01:45","3","3455","0","1","","49946748","<p>I'm new to these frameworks as well as NLP. I am following an example which gives me the following code snippet to  calculate the tf-idf score of all the tokens in the tweets. However I keep getting either import errors or Vectorizer undefined. </p>

<p><strong>Code:</strong></p>

<pre><code>import spacy
 from textacy.vsm import Vectorizer
 import textacy.vsm
 vectorizer = Vectorizer(weighting = 'tfidf')
 term_matrix = vectorizer.fit_transform([tok.lemma_ for tok in doc] for doc 
 in spacy_tweets)
</code></pre>

<p><strong>Errors Recieved:</strong></p>

<pre><code>from textacy.vsm import Vectorizer
ImportError: cannot import name 'Vectorizer
//
import textacy
vectorizer = textacy.Vectorizer(weighting='tfidf')
AttributeError: module 'textacy' has no attribute 'Vectorizer'


//
   import textacy
   vectorizer = Vectorizer(weighting='tfidf')
   NameError: name 'Vectorizer' is not defined
</code></pre>

<p><strong>My Enviroment</strong></p>

<pre><code>operating system: windows 10 64bit
python version: Python 3.6.4 :: Anaconda, Inc.
spacy version: 1.9.0-np111py36_vc14_1 installed
spacy models: en_core_web_sm 
textacy version: 0.3.4-py36_0
</code></pre>

<p>What is the correct import statement to access the textacy vectorizer class?</p>
"
"49941772","Tags in Google Ngrams dataset","2018-04-20 12:34:49","1","481","5","1","","67710914","<p><strong>tl;dr</strong> : I can't find a comprehensive list of all tags used in <a href=""http://storage.googleapis.com/books/ngrams/books/datasetsv2.html"" rel=""nofollow noreferrer"">Google Grams Dataset</a> besides <a href=""https://books.google.com/ngrams/info"" rel=""nofollow noreferrer"">that one</a> which only includes PoS tags and <code>_START_</code>, <code>_ROOT_</code> and <code>_END_</code>.  </p>

<p>What do tokens like <code>,_.</code>, <code>._.</code>, <code>_._</code> mean ? Given their frequencies -- see below -- I'd strongly assume they're <strong>tags</strong> (they can't be proper tokens).</p>

<hr>

<p><strong>Context :</strong><br>
I am trying to extract information from Google's n-grams dataset and have troubles understanding some of their tags, and how to take them into account.</p>

<p>Ultimately, I would like to approximate how likely a word will follow another one.<br>
For example, calculating how likely the token <code>protection</code> will follow <code>equal</code> would roughly mean calculating <code>count(""equal protection"") / count(""equal *"")</code> where <code>*</code> is the wildcard : any 1gram in the corpus.</p>

<p>The tricky part is calculating that <code>count(""equal *"")</code>.<br>
Indeed, for example, the bi-gram <code>equal to</code> accounts many times in the Google n-grams dataset : </p>

<ul>
<li>as <code>equal to</code>, </li>
<li>as <code>equal to_PRT</code> (disambiguated PoS version)</li>
<li>as <code>equal _PRT_</code> (aggregated for all PRT i.e. particles that might follow <code>equal</code>).</li>
</ul>

<p>As shows when I compute this on pyspark :</p>

<pre><code>&gt;&gt;&gt; total = ggrams.filter(ggrams.ngram.startswith(""equal "")).groupby(""ngram"") \
             .sum(""match_count"")

&gt;&gt;&gt; total.sort(""sum(match_count)"", ascending=False).show(n=15)

+------------+----------------+  
|       ngram|sum(match_count)|  
+------------+----------------+  
|equal _NOUN_|        20130934|  
| equal _PRT_|        16620727|  
|    equal to|        16598291|  
|equal to_PRT|        16598291|  
|   equal _._|         5119672|  
| equal _ADP_|         3037747|  
|     equal ,|         2276119|  
|   equal ,_.|         2276119|  
|    equal in|         1682835|  
|equal in_ADP|         1682176|  
|     equal .|         1628257|  
|   equal ._.|         1628257|  
|equal _CONJ_|         1363739|  
|    ...     |             ...|  
</code></pre>

<p>So to avoid accounting the same bigram multiple times, my idea was to rather just sum all counts for all patterns like <code>""equal &lt;POS&gt;""</code> where <code>&lt;POS&gt;</code> is in the described PoS set <code>[_PRT_, _NOUN_, ...]</code> (findable <a href=""https://books.google.com/ngrams/info"" rel=""nofollow noreferrer"">here</a>)</p>

<p>Doing this I obtain sum figures that are 1/3rd of the one I'd get from the displayed dataframe above. Which strenghthen my hypothesis above that one count will account three times. But I can't help persuading myself what the best way to do it is, especially notifying these weird tokens <code>,_.</code>, <code>._.</code>, <code>_._</code> which meanings I don't have any clue.</p>
"
"49929066","Clear approach for assigning semantic tags to each sentence (or short documents) in python","2018-04-19 19:44:29","1","343","0","1","","49935903","<p>I am looking for a good approach using python libraries to tackle the following problem:</p>

<p>I have a dataset with a column that has product description. The values in this column can be very messy and would have a lot of other words that are not related to the product. I want to know which rows are about the same product, so I would need to tag each description sentence with its main topics. For example, if I have the following: 
""500 units shoe green sport tennis import oversea plastic"", I would like the tags to be something like: ""shoe"", ""sport"". So I am looking to build an approach for semantic tagging of sentences, not part of speech tagging. Assume I don't have labeled (tagged) data for training. </p>

<p>Any help would be appreciated.</p>
"
"49891820","how we can create our tagged corpus","2018-04-18 05:29:34","0","106","0","1","","49891951","<p>I am trying to create my own tagged corpus on demonetization dataset and the dataset has approximately 6250 tweets. the code is below, although it is giving the results for a small dataset of size 200 entries.</p>

<pre><code>df = pd.read_csv('Demonetization_data29th2.csv',encoding = ""ISO-8859-1"")

text = df['CONTENT']
sentiment = df['sentiment']
a =[]

tagged = [[nltk.word_tokenize(sent)] for sent in df['CONTENT']]
tagged = [nltk.pos_tag(sent) for sent in tagged]
print tagged[0]

print ""---------""

brown_tagged_sents = tagged
print brown_tagged_sents[0]
size = int(len(brown_tagged_sents) * 0.7)
tags = [tag for (word, tag) in brown.tagged_words()]
defaultTag = nltk.FreqDist(tags).max()
print defaultTag
train_sents = brown_tagged_sents[:size]
test_sents = brown_tagged_sents[size:]
tagger = ngramTagger(train_sents, 2, defaultTag)
print tagger.evaluate(test_sents)
</code></pre>

<p>i got an error like :</p>

<pre><code>File ""C:/Users/HP/pos-2.py"", line 41, in &lt;module&gt;
    tagged = [[nltk.word_tokenize(sent)] for sent in df['CONTENT']]

  File ""C:\ProgramData\Anaconda2\lib\site-packages\nltk\tokenize\__init__.py"", line 130, in word_tokenize
    sentences = [text] if preserve_line else sent_tokenize(text, language)

  File ""C:\ProgramData\Anaconda2\lib\site-packages\nltk\tokenize\__init__.py"", line 97, in sent_tokenize
    return tokenizer.tokenize(text)

  File ""C:\ProgramData\Anaconda2\lib\site-packages\nltk\tokenize\punkt.py"", line 1235, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))

  File ""C:\ProgramData\Anaconda2\lib\site-packages\nltk\tokenize\punkt.py"", line 1283, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]

  File ""C:\ProgramData\Anaconda2\lib\site-packages\nltk\tokenize\punkt.py"", line 1274, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]

  File ""C:\ProgramData\Anaconda2\lib\site-packages\nltk\tokenize\punkt.py"", line 1314, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):

  File ""C:\ProgramData\Anaconda2\lib\site-packages\nltk\tokenize\punkt.py"", line 312, in _pair_iter
    prev = next(it)

  File ""C:\ProgramData\Anaconda2\lib\site-packages\nltk\tokenize\punkt.py"", line 1287, in _slices_from_text
    for match in self._lang_vars.period_context_re().finditer(text):

TypeError: expected string or buffer
</code></pre>

<p>Please guide me where I am going wrong</p>
"
"49871737","NLP Sentiment Analysis using TF-IDF Vector Size","2018-04-17 06:58:23","3","1556","1","2","","49876334","<p>I am relatively new to NLP &amp; Sentiment analysis, but I am enrolled in a Machine Learning class and am creating a Sentiment Analysis NLP that will read a financial article and determine whether or not the overall sentiment is good or bad. </p>

<p>Currently, I have a dataset of about 2000 articles. I know that I need to implement the TF-IDF vector method to cast all the instances in the dataset to the same vector space. Also, I know that TF-IDF requires a ""Vocabulary"" and the size of this ""Vocabulary"" is the length of the vector, each vector representing an article. </p>

<p>My question is, how do I determine this vocabulary? One method I have found is to implement pre-processing (get rid of stop words, noisy words, punctuation, etc.) and then use ALL words in EVERY article in the training set. From here you can remove the words that have very few instances (unimportant words) and remove the words that have too many instances (non-distinguishing words). However, in my opinion, the ""Vocabulary"" is still going to be quite large, hence, the vector size is going to be very large.</p>

<p>Overall, this approach seems logical, but processing heavy. I feel that initially creating a ""Vocabulary"" containing all words in every article is going to be HUGE. And then iterating through every article to see how many times the words in the ""Vocabulary"" have occurred is going to require a lot of processing power. If I am using NLTK and scikit-learn, do I have anything to worry about? If so, is there a better way to create the vocabulary?</p>
"
"49780473","Splitting sentences from prose","2018-04-11 16:44:26","1","83","0","1","","49784853","<p>I am trying to split on sentences, and also preserve dialogue markers.  So a sentence like </p>

<blockquote>
  <p>“Dirty, Mr. Jones? Look at my shoes! Not a speck on them.”  This is a non-dialogue sentence!</p>
</blockquote>

<p>Should return the list</p>

<pre><code>[
    ""“Dirty, Mr. Jones?”"",
    ""“Look at my shoes!”"",
    ""“Not a speck on them.”"",
    ""This is a non-dialogue sentence!""
]
</code></pre>

<p>I’m struggling with preserving the end-of-sentence punctuation while preserving the period on <code>Mr.</code>. I am also struggling with inserting the quotation marks, as currently the returned list is <code>['“Dirty, Mr. Jones”', '“Look at my shoes”', '“Not a speck on them”', '“”', 'This is a non-dialogue sentence', '']</code> and I don’t know why I’m getting the two empty elements. How can I fix these problems?</p>

<p>Here is my code (eventually this will parse the whole book but for now I’m testing it on one phrase):</p>

<pre><code>def get_all_sentences(corpus):

  sentences_in_paragraph = []

  dialogue = False
  dialogue_sentences = """"
  other_sentences = """"

  example_paragraph = ""“Dirty, Mr. Jones? Look at my shoes! Not a speck on them.”  This is a non-dialogue sentence!""

  example_paragraph = example_paragraph.replace(""\n"", """") # remove newline

  for character in example_paragraph:
    if character == ""“"":
        dialogue = True
        continue
    if character == ""”"":
        dialogue = False
        continue

    if dialogue:
        dialogue_sentences += character
    else:
        other_sentences += character

  sentences_in_paragraph  = list(map(lambda x: ""“"" + x.strip() + ""”"", re.split(""(?&lt;!Mr|Ms)(?&lt;!Mrs)[.!?]"", dialogue_sentences))) 
  sentences_in_paragraph += list(map(lambda x: x.strip(), re.split(""(?&lt;!Mr|Ms)(?&lt;!Mrs)[.!?]"", other_sentences)))

  print(sentences_in_paragraph)
</code></pre>
"
"49564176","Python (NLTK) - more efficient way to extract noun phrases?","2018-03-29 20:04:05","12","21698","0","4","","49584275","<p>I've got a machine learning task involving a large amount of text data. I want to identify, and extract, noun-phrases in the training text so I can use them for feature construction later on in the pipeline. 
I've extracted the type of noun-phrases I wanted from text but I'm fairly new to NLTK, so I approached this problem in a way where I can break down each step in list comprehensions like you can see below. </p>

<p>But my real question is, am I reinventing the wheel here? Is there a faster way to do this that I'm not seeing?</p>

<pre><code>import nltk
import pandas as pd

myData = pd.read_excel(""\User\train_.xlsx"")
texts = myData['message']

# Defining a grammar &amp; Parser
NP = ""NP: {(&lt;V\w+&gt;|&lt;NN\w?&gt;)+.*&lt;NN\w?&gt;}""
chunkr = nltk.RegexpParser(NP)

tokens = [nltk.word_tokenize(i) for i in texts]

tag_list = [nltk.pos_tag(w) for w in tokens]

phrases = [chunkr.parse(sublist) for sublist in tag_list]

leaves = [[subtree.leaves() for subtree in tree.subtrees(filter = lambda t: t.label == 'NP')] for tree in phrases]
</code></pre>

<p>flatten the list of lists of lists of tuples that we've ended up with, into
just a list of lists of tuples</p>

<pre><code>leaves = [tupls for sublists in leaves for tupls in sublists]
</code></pre>

<p>Join the extracted terms into one bigram</p>

<pre><code>nounphrases = [unigram[0][1]+' '+unigram[1][0] in leaves]
</code></pre>
"
"49342658","Difference between adequacy and fluency in ngram","2018-03-17 22:44:38","0","390","0","1","","50193264","<blockquote>
  <p>""When 1-gram precision is high, the reference tends to satisfy
  adequacy.</p>
  
  <p>When longer n-gram precision is high, the reference tends to account
  for fluency.""</p>
</blockquote>

<p>What does this mean?</p>
"
"49341740","Lemmatizing txt file and replacing only lemmatized words","2018-03-17 20:45:29","3","4664","2","3","","49341812","<p>Having trouble figuring out how to lemmatize words from a txt file.  I've gotten as far as listing the words, but I'm not sure how to lemmatize them after the fact.</p>

<p>Here's what I have:</p>

<pre><code>import nltk, re
nltk.download('wordnet')
from nltk.stem.wordnet import WordNetLemmatizer

def lemfile():
    f = open('1865-Lincoln.txt', 'r')
    text = f.read().lower()
    f.close()
    text = re.sub('[^a-z\ \']+', "" "", text)
    words = list(text.split())
</code></pre>
"
"49240058","PCFG generation in NLTK","2018-03-12 16:27:22","0","1915","0","1","","49325971","<p>I am trying to learn a PCFG from a file containing parse trees for example:</p>
<blockquote>
<p>(S (DECL_MD (NP_PPSS (PRON_PPSS (i i))) (VERB_MD (pt_verb_md need))
(NP_NN (ADJ_AT (a a)) (NOUN_NN (flight flight)) (PREP_IN (pt_prep_in
from))) (AVPNP_NP (NOUN_NP (charlotte charlotte))</p>
</blockquote>
<p>This is my relevant code:</p>
<pre><code>def loadData(path):
    with open(path ,'r') as f:
        data = f.read().split('\n')
    return data

def getTreeData(data):
    return map(lambda s: tree.Tree.fromstring(s), data)

# Main script
print(&quot;loading data..&quot;)
data = loadData('C:\\Users\\Rayyan\\Desktop\\MSc Data\\NLP\\parseTrees.txt')
print(&quot;generating trees..&quot;)
treeData = getTreeData(data)
print(&quot;done!&quot;)
print(&quot;done!&quot;)
</code></pre>
<p>Now after that I've tried SO much stuff on the internet for example:</p>
<pre><code>grammar = induce_pcfg(S, productions)
</code></pre>
<p>but here the productions is always the built in functions, for example:</p>
<pre><code>productions = []
for item in treebank.items[:2]:
  for tree in treebank.parsed_sents(item):
    productions += tree.productions()
</code></pre>
<p>I've tried replacing <code>production</code> here with <code>treeData</code> in my case, but it doesn't work. What am I missing or doing wrong?</p>
"
"49218417","Counting distinct words in a speech using tagset in nltk","2018-03-11 09:12:09","0","1967","1","1","","49218753","<p>I am currently having trouble with this.</p>

<p>I was given a task is to implement a function that return a sorted list of distinct words with a given part of speech. I am required to use NLTK's pos_tag_sents and NLTK's tokeniser to count the specific words.</p>

<p>I had a similar question to this and got it working thanks to some help from other users from Stack Overflow. And trying to use the same method to solve this problem.</p>

<p>Here is what I have have so far in my code:</p>

<pre><code>import nltk
import collections
nltk.download('punkt')
nltk.download('gutenberg')
nltk.download('brown')
nltk.download('averaged_perceptron_tagger')
nltk.download('universal_tagset')

def pos_counts(text, pos_list):
    """"""Return the sorted list of distinct words with a given part of speech
    &gt;&gt;&gt; emma = nltk.corpus.gutenberg.raw('austen-emma.txt')
    &gt;&gt;&gt; pos_counts(emma, ['DET', 'NOUN'])
    [14352, 32029] - expected result
    """"""

    text = nltk.word_tokenize(text)
    tempword = nltk.pos_tag_sents(text, tagset=""universal"")
    counts = nltk.FreqDist(tempword)

    return [counts[x] or 0 for x in pos_list]
</code></pre>

<p>There are a doctest that should give the result of: [14352, 32029]</p>

<p>I ran my code and got this error message:</p>

<pre><code>Error
**********************************************************************
File ""C:/Users/PycharmProjects/a1/a1.py"", line 29, in a1.pos_counts
Failed example:
    pos_counts(emma, ['DET', 'NOUN'])
Exception raised:
    Traceback (most recent call last):
      File ""C:\Program Files\JetBrains\PyCharm Community Edition 2017.3.4\helpers\pycharm\docrunner.py"", line 140, in __run
        compileflags, 1), test.globs)
      File ""&lt;doctest a1.pos_counts[1]&gt;"", line 1, in &lt;module&gt;
        pos_counts(emma, ['DET', 'NOUN'])
      File ""C:/Users/PycharmProjects/a1/a1.py"", line 35, in pos_counts
        counts = nltk.FreqDist(tempword)
      File ""C:\Users\PycharmProjects\a1\venv\lib\site-packages\nltk\probability.py"", line 108, in __init__
        Counter.__init__(self, samples)
      File ""C:\Users\AppData\Local\Programs\Python\Python36-32\lib\collections\__init__.py"", line 535, in __init__
        self.update(*args, **kwds)
      File ""C:\Users\PycharmProjects\a1\venv\lib\site-packages\nltk\probability.py"", line 146, in update
        super(FreqDist, self).update(*args, **kwargs)
      File ""C:\Users\AppData\Local\Programs\Python\Python36-32\lib\collections\__init__.py"", line 622, in update
        _count_elements(self, iterable)
    TypeError: unhashable type: 'list'
</code></pre>

<p>I feel I'm getting close but I don't know what I'm doing wrong.</p>

<p>Any help will be very appreciated. 
Thank you.</p>
"
"49216816","No result after calculating the similarity of two words based on word vectors via Spacy's parser?","2018-03-11 05:01:04","0","151","0","1","","49218581","<p>I have an example in spacy code:</p>

<pre><code>    from numpy import dot
    from numpy.linalg import norm
    from spacy.lang.en import English

    parser = English()
    # you can access known words from the parser's vocabulary
    nasa = parser.vocab[u'NASA']

    # cosine similarity
    cosine = lambda v1, v2: dot(v1, v2) / (norm(v1) * norm(v2))

    # gather all known words, take only the lowercased versions
    allWords = list({w for w in parser.vocab if w.has_vector and 
    w.orth_.islower() and w.lower_ != unicode(""nasa"")})

    # sort by similarity to NASA
    allWords.sort(key=lambda w: cosine(w.vector, nasa.vector))
    allWords.reverse()
    print(""Top 10 most similar words to NASA:"")
    for word in allWords[:10]:
         print(word.orth_)
</code></pre>

<p>The result is like this:</p>

<pre><code>    Top 10 most similar words to NASA:

    Process finished with exit code 0
</code></pre>

<p>So there is no similar words come out.
I have tried to install the parser and glove via cmd: </p>

<pre><code>    python -m spacy.en.download parser
    python -m spacy.en.download glove
</code></pre>

<p>But failed, it turned out to be:</p>

<pre><code>    C:\Python\python.exe: No module named en
</code></pre>

<p>By the way, I use:</p>

<pre><code>    Python 2.7.9
    Spacy  2.0.9
</code></pre>

<p>What's wrong with it? Thank you</p>
"
"49132482","Where can I find a list of english part of speech constraints?","2018-03-06 13:53:40","0","22","0","1","","49244132","<p>I'm looking for a list of English part of speech sequencing rules (e.g. ""a determiner cannot be followed by a verb""). 
Thought it would be easy but I couldn't find an actual list of more than several examples.
Any ideas?</p>

<p>Thanks. </p>
"
"49094311","How to use Stemmer or Lemmatizer to stem specific word","2018-03-04 10:19:57","1","856","1","1","","49094807","<p>I am currently trying to stem a big corpus(aprox. 800k sentences). I've managed to stem only the basic one. The problem now is that I want to stem only a specific word for example this method only applies if the lemma is a substring of the original word. For instance, the suffix for the word apples are apple and 's'. But if not a substring, it will not split it like the word teeth into tooth.</p>

<p>I've also read about lemmatizer WordNet, where we can add a parameter for pos such as verb, noun or adjective. Is there a way that I can apply the method above?</p>

<p>Thanks in advance!</p>
"
"49090878","Implementing syllabification algorithm but is really slow","2018-03-04 00:57:57","0","133","1","2","","49091739","<p>I implemented simple syllabification algorithm following Improved Lansky algorithm but it's really slow when I need to run this algorithm on corpus over 2 million words. Could someone point me in the direction what causes it to be so slow? Algorithm below:</p>

<ol>
<li><p>Everything after the last vowel (vowel group) belongs to the last syllable</p></li>
<li><p>Everything before the first vowel (vowel group) belongs to the first syllable</p></li>
<li><p>If the number of consonants between vowels is even (2n), they are divided into the
halves first half belongs to the left vowel(s) and second to the right vowel(s) (n/n).</p></li>
<li><p>If the number of consonants between vowel(s) is odd(2n +  1), we divide them into
n / n  + 1 parts.</p></li>
<li><p>If there is only one consonant between vowels, it belongs to the left vowel(s).</p>

<pre><code>#include &lt;stdio.h&gt;
#include &lt;string.h&gt;

#define VOWELS ""aeiou""

int get_n_consonant_between(char *word, int length) {
     int count = 0;
     int i = 0;

     while (i++ &lt; length) {
          if (strchr(VOWELS, *word)) break;
         word++;
         count++;
     } 

     return count;
 }

 void syllabification(char *word, int n_vowel_groups) {
     int i = 0, length = strlen(word), consonants;
     int syllables = 0, vowel_group = 0, syl_length = 0;
     char *syllable = word;
     char hola[length];

     memset(hola, 0, length);

     if (n_vowel_groups &lt; 2) {
         printf(""CAN'T BE SPLIT INTO SYLLABLES\n\n"");
         return;
     }

     while (i &lt; length) {
         if (strchr(VOWELS, word[i])) {
             syl_length++;
             i++;
             if (vowel_group) continue;
             vowel_group = 1;
         }
         else {
             if (vowel_group) {
                  consonants = get_n_consonant_between(word + i, length - i);
                  if (consonants == 1) {
                      // printf(""only one consonant\n"");
                      syl_length++;
                      strncpy(hola, syllable, syl_length);
                      i++;
                  }
                  else {
                      int count = consonants / 2;
                      if ((consonants % 2) == 0) { /* number of consonants is 2n, first half belongs to the left vowel */
                     syl_length += count;
            }
            else {
                syl_length += count;
            }
            strncpy(hola, syllable, syl_length);
            i += count;
        }

        syllables++;
        if (syllables == n_vowel_groups) {
            printf(""syllable done %d: %s\n"", syllables, syllable);
            break;
        }
        printf(""syllable %d: %s\n"", syllables, hola);

        syllable = word + i;
        syl_length = 0;
        memset(hola, 0, length);
    }
    else {
        syl_length++;
        i++;
    }
    vowel_group = 0;
     }
 }    
 }

 int count_vowel_groups(char *word) {
      int i, nvowels = 0;
      int vowel_group = 0;

      for (i = 0; i &lt; strlen(word); i++) {
          if (strchr(VOWELS, word[i])) {
              if (vowel_group) continue;
              vowel_group = 1;
          }
          else {
              if (vowel_group) nvowels++;
              vowel_group = 0;
          }    
      }
      // printf(""%d vowel groups\n"", nvowels);
      return nvowels;
}

 void repl() {
      char *line = NULL;
      size_t len = 0;
      int i = 0;
      int count;
      FILE *file = fopen(""../syllables.txt"", ""r"");
      while(i++ &lt; 15) {
          getline(&amp;line, &amp;len, file);
          printf(""\n\n%s\n"", line);
          count = count_vowel_groups(line);
          syllabification(line, count);
      }
 }

 int main(int argc, char *argv[]) {
     // printf(""Syllabification test:\n"");
     repl();
 }
</code></pre>

<p>`</p></li>
</ol>
"
"49058275","Python NLTK: search for occurrence of a word","2018-03-01 20:34:20","-1","1420","5","1","","49072831","<p>I use the brown corpus ""brown.words()"" which gives me a list of 1161192 words.</p>

<p>Now I want to find any occurrence of the word ""have"", so whenever in the corpus there is an ""has"", ""had"", ""haven't"" ect. I want to do something (could be pushing them into an array, could be a counter, could be something else.</p>

<p><strong>Edit</strong>: Note that this question is about <strong>finding a matching word</strong>. If I <strong>search ""have"" I want a way to match it to ""haven't"" or ""had"", thus the .count() would not solve this problem</strong> as it dosen't help matching anything.</p>

<p>Example code I would use in case stemming/lemmatization would work:</p>

<pre><code>def findWordFamily(findWord):
    wordFamily = []

    lmtzr = WordNetLemmatizer()

    findWord = lmtzr.lemmatize(findWord)
    for word in brown.words():
        lemma = lmtzr.lemmatize(word)
        if lemma == findWord:
            wordFamily.append(word)

    return wordFamily
print(findWordFamily(""have""))
# [""have"", ""have"", ""had"", ""having"",""haven't"", ""having""]
</code></pre>

<p>But the problem is that: </p>

<pre><code>for word in brown.words():
    lemma = lmtzr.lemmatize(word)
    # if word is ""having"" lemma also is ""having"" instead of ""have""
</code></pre>
"
"48984071","Scikit - TF-IDF empty vocabulary","2018-02-26 08:17:54","3","3655","1","2","","48984283","<p>I have to calculate the distance/similarity of two or more texts. Some texts are genuinely really small or do not form proper english words etc, ""A1024515"". This means that it should accept every single word in the list. </p>

<p>As a test case, I have used the following list as a corpus of words. </p>

<pre><code>words= ['A', 'A', 'A']

vect = TfidfVectorizer(min_df =0)
dtm = vect.fit_transform(words)
df_tf_idf = pd.DataFrame(dtm.toarray(), columns=vect.get_feature_names())
</code></pre>

<p>However, I get the following error </p>

<pre><code>ValueError: empty vocabulary; perhaps the documents only contain stop words
</code></pre>

<p><strong>How can I ensure that the list is accepted as possible words and ensure stop words are not removed from the corpus?</strong></p>
"
"48897910","Parse NLTK tree output in a list of noun phrase","2018-02-21 03:53:17","1","2511","1","2","","48898103","<p>I have a sentence </p>

<pre><code>text  = '''If you're in construction or need to pass fire inspection, or just want fire resistant materials for peace of mind, this is the one to use. Check out 3rd party sellers as well Skylite'''
</code></pre>

<p>I applied NLTK chunking on it and getting a tree as output. </p>

<pre><code>sentences = nltk.sent_tokenize(d)
sentences = [nltk.word_tokenize(sent) for sent in sentences]
sentences = [nltk.pos_tag(sent) for sent in sentences]

grammar = """"""NP: {&lt;DT&gt;?&lt;JJ&gt;*&lt;NN.*&gt;+}
       RELATION: {&lt;V.*&gt;}
                 {&lt;DT&gt;?&lt;JJ&gt;*&lt;NN.*&gt;+}
       ENTITY: {&lt;NN.*&gt;}""""""

cp = nltk.RegexpParser(grammar)
for i in sentences:
    result = cp.parse(i)
    print(result)
    print(type(result))
    result.draw() 
</code></pre>

<p>Output is as follows:</p>

<pre><code>(S If/IN you/PRP (RELATION 're/VBP) in/IN (NP construction/NN) or/CC (NP need/NN) to/TO (RELATION pass/VB) (NP fire/NN inspection/NN) ,/, or/CC just/RB (RELATION want/VB) (NP fire/NN) (NP resistant/JJ materials/NNS) for/IN (NP peace/NN) of/IN (NP mind/NN) ,/, this/DT (RELATION is/VBZ) (NP the/DT one/NN) to/TO (RELATION use/VB) ./.)
</code></pre>

<p>HOw can I get noun phrase in format of list of a strings:</p>

<pre><code>[construction, need, fire inspection, fire, resistant materials, peace, mind, the one]
</code></pre>

<p>Some suggestions please......? </p>
"
"48896682","Python Package with NLTK as a Dependency","2018-02-21 01:05:33","0","441","1","1","","48896922","<p>I've looked around for a question pertaining to this without any hits, so here we go:</p>

<p>I am working on a toy python package to deploy on PyPi.org. A part of its job involves streamlining the process of parsing text and generating tokenized sentences. Naturally, I have considered using <code>nltk</code> for the job, having personally used tools like <code>punkt</code> from the package.</p>

<p>Here's the problem and my question: Having looked at the size of <code>nltk</code> and the requirements for it to work, with the corpora nearly <strong>10 gigabytes</strong> in size, I've come to the conclusion this is an outlandish burden to put on anyone who wants to use my package given its use-case.</p>

<p>Is there anyway to deploy a ""pre-trained"" instance of <code>punkt</code>? Or can I control the size of the corpora used by <code>nltk</code>?</p>

<p>I am equally open to an alternative package/solution for parsing relatively ""sane"" human text that is somewhat close to performance of <code>nltk</code> but without the same disk memory footprint.</p>

<p>Thanks for any help.</p>

<hr>

<p>solution as indicated below by @matisetorm for me is:</p>

<pre><code>python -m nltk.downloader punkt
</code></pre>
"
"48872564","I use python 3.6 and i want to make sentiment analysis but i have an error in nltk.metrics package?","2018-02-19 18:59:34","1","425","1","1","","48872610","<p>i write the code as here : </p>

<pre><code>    print ('pos precision:', nltk.metrics.precision(refsets['pos'], 
    testsets['pos']))
    print ('pos recall:', nltk.metrics.recall(refsets['pos'], 
    testsets['pos']))
</code></pre>

<p>and the output as here : </p>

<pre><code>   line 35, in evaluate_classifier
   print ('pos precision:', nltk.metrics.precision(refsets['pos'], 
   testsets['pos']))
   AttributeError: module 'nltk.translate.metrics' has no attribute 
  'precision'
</code></pre>

<p>How can i solve this error ? </p>
"
"48872023","Print pos tag with removed adjectives (NLTK)","2018-02-19 18:20:04","0","1299","1","3","","48872512","<pre><code>abc = nltk.pos_tag(info)
      print(s for s in abc if s[1] != 'ADV')
</code></pre>

<p>Returns: generator object pos. locals>. genexpr> at 0x000000000E000D00></p>

<p>If using [] round print I get ""Invalid syntax""</p>
"
"48789043","Is there a quicker snowball stemmer in python 3.6 than NLTK's?","2018-02-14 14:01:02","1","1578","0","1","","48794891","<p>I am currently using NLTK's SnowballStemmer to stem the words in my documents and this was working fine when I had 68 documents. Now I have 4000 documents and this is way too slow. I read another post where someone suggested to use  <code>PyStemmer</code>, but this is not offered on Python 3.6 Are there any other packages that would do the trick? Or maybe there's something I can do in the code to speed up the process.</p>

<p>Code:</p>

<pre><code>eng_stemmer = nltk.stem.SnowballStemmer('english')
...
class StemmedCountVectorizer(CountVectorizer):
    def build_analyzer(self):
        analyzer = super(StemmedCountVectorizer, self).build_analyzer()
        return lambda doc: ([eng_stemmer.stem(w) for w in analyzer(doc)])
</code></pre>
"
"48671265","Optimizing Language Detection code and Lemmatization in Python","2018-02-07 18:53:27","0","819","4","1","","48679657","<p>I have a data of amazon user reviews in JSON format which i am importing to pandas dataframe and using it to train a  model for text classification. I am trying to preprocess the user review text before training a model with that data. I have two questions here:</p>

<p>1) I have written a code to detect it's language using Textblob library in Python which is working fine but consuming a lot of time. Please tell me if there can be a optimal approach.I am using Textblob library in python and the code is:</p>

<pre><code>    from textblob import TextBlob
    def detect_language(text):
        if len(text)&gt;3:
            r=TextBlob(text)
            lang = r.detect_language()
            return lang
    dataset['language']=dataset.reviewText.apply(lambda x: detect_language(x))
</code></pre>

<p>2) I want to lemmatize my words before training the model. But as lemmatization in NLTK will work properly if the we have parts-of-speech tagged with the words, I am trying it as follows but getting some error:</p>

<pre><code>    from nltk import pos_tag
    from nltk.stem import WordNetLemmatizer
    text='my name is shubham'
    text=pos_tag(text.split())
    wl=WordNetLemmatizer()
    for i in text:
        print(wl.lemmatize(i))
</code></pre>

<p>Here i am getting pos tagged as:</p>

<pre><code>    [('my', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), ('shubham', 'JJ')]
</code></pre>

<p>and while doing lemmatization i am getting error as:</p>

<pre><code>    AttributeError: 'tuple' object has no attribute 'endswith'
</code></pre>

<p>Can you please suggest an efficient way to perform lemmatization.
Here is my sample data on which i am performing language detection and lemmatization:</p>

<pre><code>    overall reviewText
        5   Not much to write about here, but it does exac...
        5   The product does exactly as it should and is q...
        5   The primary job of this device is to block the...
        5   Nice windscreen protects my MXL mic and preven...
        5   This pop filter is great. It looks and perform...
</code></pre>
"
"48657359","NLP - Sentence does not follow any of the grammar rule in Syntactic parsing","2018-02-07 06:30:53","0","255","0","1","","48713323","<p>I have grammar extracted from Treebank dataset from nltk library. Now the parser will use these rules to parse the sentence. So for example I have grammar like below:</p>

<pre><code>A-&gt;B C
B-&gt;'b'
C-&gt;'c'
D-&gt;'d'
</code></pre>

<p>Now suppose I have sentence like <code>b c</code> so the parser will make parse tree like below</p>

<pre><code>    A
   / \
  B   C
  |   |
  b   c
</code></pre>

<p>So like this I have all the grammar from training dataset. Now for testing assume a completely new sentence is there like <code>""c d""</code></p>

<p>Parse tree for above sentence will be</p>

<pre><code>  C   D
  |   |
  c   c
</code></pre>

<p>and  the parser will stop as there is no rule for <code>A-&gt;C D</code></p>

<p>So how to parse these kind of sentences because training grammar does not have any grammar like <code>A -&gt; C D</code></p>

<p>So the question is how to parse sentence if it completely new and grammar rule is not present in training data set? I am using probabilistic grammar.</p>
"
"48621246","Is it safe to use space as delimiter to concatenate words where language of the content is unknown","2018-02-05 11:08:35","0","60","0","1","","51744556","<p>I have to perform NLP (language detection in particular) on a input. The input has multiple fields:</p>

<blockquote>
  <p>{   field1: """",   field2: """"   ... }</p>
</blockquote>

<p>I want to merge all the fields and perform NLP. I am not sure however, is it safe to use ""SPACE"" as the delimiter to concatenate the content.</p>
"
"48620621","Specify NLTK feature grammar within Python function in code","2018-02-05 10:36:32","0","611","0","1","","48635902","<p>I have parsed input string by loading grammar specified within a .fcfg file as given in the NLTK book. Is there anyway to specify this grammar within the Python function itself?</p>

<p>Grammar:</p>

<pre><code>% start S
S[SEM=(?np + WHERE + ?vp)] -&gt; NP[SEM=?np] VP[SEM=?vp]
VP[SEM=(?v + ?pp)] -&gt; IV[SEM=?v] PP[SEM=?pp]
VP[SEM=(?v + ?ap)] -&gt; IV[SEM=?v] AP[SEM=?ap]
NP[SEM=(?det + ?n)] -&gt; Det[SEM=?det] N[SEM=?n]
PP[SEM=(?p + ?np)] -&gt; P[SEM=?p] NP[SEM=?np]
AP[SEM=?pp] -&gt; A[SEM=?a] PP[SEM=?pp]
NP[SEM='Country=""greece""'] -&gt; 'Greece'
NP[SEM='Country=""china""'] -&gt; 'China'
Det[SEM='SELECT'] -&gt; 'Which' | 'What'
N[SEM='City FROM city_table'] -&gt; 'cities'
IV[SEM=''] -&gt; 'are'
A[SEM=''] -&gt; 'located'
P[SEM=''] -&gt; 'in'
</code></pre>

<p>I need this because I need to create the grammar dynamically w.r.t to input string before parsing.</p>
"
"48611488","Measure of similarity using meronym/holonym edge on Wordnet","2018-02-04 18:16:48","0","432","3","1","","48618732","<p>StackOverflow!</p>

<p>I searched on stack but I have not found any response about my doubt. My question is follow:</p>

<p>There are any measure of similarity for Wordnet which explores (navigate) holonym / meronym and hypernym / hyponym edges at the same time? I have found only measures which look for common hypernyms vertex on Wordnet ...</p>

<p>My question not contains a snippet of code, it's only about a Wordnet feature.</p>

<p>UPDATE:
I'm searching for a measure which not only use 'is-a' for find two concepts for semantic comparation. I want some measure which, in some cases, for ""bind"" two concepts admits ""skip"" 'is-a' taxonomy until reach most close hyperonym and choose navigate in 'member of'(holonyms/meronyms) taxonomy under some justificative.</p>

<p>Thanks in advance.</p>
"
"48572405","Find the corresponding nouns or verbs of adjectives and adverbs in an English sentence","2018-02-01 21:57:00","1","1463","0","1","","48574480","<p>My goal is to build an algorithm that given an adjective or adverb within a sentence, indicates the corresponding noun or verb. </p>

<p>For example: </p>

<blockquote>
  <p>The boy threw the heavy stone angrily to the window that was very far.</p>
</blockquote>

<p>heavy (adj) -> stone </p>

<p>angrily (adv) -> threw</p>

<p>far (adj) -> window</p>

<p>So far I was able to tag Parts of Speech for each word and identify adjectives, nouns, verbs, and adverbs in a given sentence. </p>

<ol>
<li>In Natural Language Processing, is there a terminology for what I'm trying to do? </li>
<li>Does a well known algorithm or approach exist for my goal? </li>
<li>I wonder if it makes sense to manually train several sentences and build a machine learning model for this? or is it over-engineering the problem? </li>
</ol>
"
"48532723","R - Parsing Python NLTK Trees via Reticulate","2018-01-31 00:39:28","2","348","1","1","","48532948","<p>I am trying to make use of Python's NLTK package from within R using the Reticulate package. For the most part, I have been successful.</p>

<p>Now, I would like to perform named entity recognition (i.e. to determine which tokens represent named entities and what type of named entity they represent.) using NLTK's <code>ne_chunk()</code> function. My problem is that the function returns an object of the class <code>nltk.tree.Tree</code>, which I cannot figure out how to parse in R.</p>

<p>If <code>ne_chunk()</code> is fed up to ten token-tag pairs, it will return a result which can be converted into a character using <code>as.character()</code>, which can be parsed via regular expression functions (this is just a hack and I am not satisfied with it). Over ten pairs, however, and it will return a shorthand representation of the tree, from which no meaningful data can be extracted using R methods.</p>

<p>Here is a minimally-reproducible example:</p>

<pre><code>library(reticulate)
nltk &lt;- import(""nltk"")

sent_tokenize &lt;- function(text, language = ""english"") {
  nltk$tokenize$sent_tokenize(text, language)
}
word_tokenize &lt;- function(text, language = ""english"", preserve_line = FALSE) {
  nltk$tokenize$word_tokenize(text, language, preserve_line)
}
pos_tag &lt;- function(tokens, tagset = NULL, language = ""eng"") {
  nltk$pos_tag(tokens, tagset, language)
}
ne_chunk &lt;- function(tagged_tokens, binary = FALSE) {
  nltk$ne_chunk(tagged_tokens, binary)
}

text &lt;- ""Christopher is having a difficult time parsing NLTK Trees in R.""
tokens &lt;- word_tokenize(text)
tagged_tokens &lt;- pos_tag(tokens)
ne_tagged_tokens &lt;- ne_chunk(tagged_tokens)
</code></pre>

<p>Here is the shorthand that is returned when the text from the previous example is processed:</p>

<pre><code>&gt; ne_tagged_tokens
List (11 items)
</code></pre>

<p>Here are the classes to which <code>ne_tagged_tokens</code> belongs:</p>

<pre><code>&gt; class(ne_tagged_tokens)
[1] ""nltk.tree.Tree""        ""python.builtin.list""   ""python.builtin.object""
</code></pre>

<p>I am not interested in suggestions to use alternative, pre-existing R packages.</p>
"
"48529212","How to remove words of a sentence by using a dictionary as reference","2018-01-30 19:35:38","0","487","0","2","","48529556","<p>I have a dictionary created and saved as a text file. I open it as</p>

<pre><code>with open(pathDoc+'/WordsDictionary.txt', 'r+', encoding=""utf8"") as inf:
wordsDictionary = eval(inf.read())
</code></pre>

<p>saved format is this: <code>{'word1':'tag1', 'word2':'tag2'}</code></p>

<p>when a sentence is given, i want to remove words that belong to a certain tag set. (simply what is done in <code>stop words removal in nltk</code>, but this is for a language that is not supported by nltk toolkit). example is given below.</p>

<pre><code> wordsDictionary = {'word1':'tag1', 'word2':'tag2', 'word3':'tag3'}
    Sentence = ""word1 word2 word3 word2 word1""
# I want to remove words that belong to 'tag2' type
FinalSentence = ""word1 word3 word1""
</code></pre>

<p>How can i generate <code>FinalSentence</code>?</p>

<p>Thanks!</p>
"
"48431173","Is there a way to get only the IDF values of words using scikit or any other python package?","2018-01-24 20:36:27","5","5294","3","1","","48459406","<p>I have a text column in my dataset and using that column I want to have a IDF calculated for all the words that are present. TFID implementations in scikit, like <em><code>tfidf</code> vectorize</em>, are giving me TFIDF values directly as against just word IDFs. Is there a way to get word IDFs give a set of documents?</p>
"
"48340974","how to resolve the error: AttributeError: 'generator' object has no attribute 'endswith'","2018-01-19 12:19:17","1","21034","4","1","","48341103","<p>When I'm trying to run this code to preprocess a text, I get the error below, someone is having a similar problem but the post did not have enough details. </p>

<p>I am putting everything in context here hoping to help reviewer to help us better.</p>

<p>Here is the function;</p>

<pre><code>def preprocessing(text):
    #text=text.decode(""utf8"")
    #tokenize into words
    tokens=[word for sent in nltk.sent_tokenize(text) for word in 
    nltk.word_tokenize(sent)]
    #remove stopwords
    stop=stopwords.words('english')
    tokens=[token for token in tokens if token not in stop]
    #remove words less than three letters
    tokens=[word for word in tokens if len(word)&gt;=3]
    #lower capitalization
    tokens=[word.lower() for word in tokens]
    #lemmatization
    lmtzr=WordNetLemmatizer()
    tokens=[lmtzr.lemmatize(word for word in tokens)]
    preprocessed_text=' '.join(tokens)
    return preprocessed_text
</code></pre>

<h1>calling the function here;</h1>

<pre><code>#open the text data from disk location
sms=open('C:/Users/Ray/Documents/BSU/Machine_learning/Natural_language_Processing_Pyhton_And_NLTK_Chap6/smsspamcollection/SMSSpamCollection')
sms_data=[]
sms_labels=[]
csv_reader=csv.reader(sms,delimiter='\t')
for line in csv_reader:
    #adding the sms_id
    sms_labels.append(line[0])
    #adding the cleaned text by calling the preprocessing method
    sms_data.append(preprocessing(line[1]))
sms.close()
</code></pre>

<p>result;</p>

<pre><code>--------------------------------------------------------------------------- AttributeError                            Traceback (most recent call last) &lt;ipython-input-38-b42d443adaa6&gt; in &lt;module&gt;()
      8     sms_labels.append(line[0])
      9     #adding the cleaned text by calling the preprocessing method
---&gt; 10     sms_data.append(preprocessing(line[1]))
     11 sms.close()

&lt;ipython-input-37-69ef4cd83745&gt; in preprocessing(text)
     12     #lemmatization
     13     lmtzr=WordNetLemmatizer()
---&gt; 14     tokens=[lmtzr.lemmatize(word for word in tokens)]
     15     preprocessed_text=' '.join(tokens)
     16     return preprocessed_text

~\Anaconda3\lib\site-packages\nltk\stem\wordnet.py in lemmatize(self, word, pos)
     38 
     39     def lemmatize(self, word, pos=NOUN):
---&gt; 40         lemmas = wordnet._morphy(word, pos)
     41         return min(lemmas, key=len) if lemmas else word
     42 

~\Anaconda3\lib\site-packages\nltk\corpus\reader\wordnet.py in
_morphy(self, form, pos, check_exceptions)    1798     1799         # 1. Apply rules once to the input to get y1, y2, y3, etc.
-&gt; 1800         forms = apply_rules([form])    1801     1802         # 2. Return all that are in the database (and check the original too)

~\Anaconda3\lib\site-packages\nltk\corpus\reader\wordnet.py in apply_rules(forms)    1777         def apply_rules(forms):    1778     return [form[:-len(old)] + new
-&gt; 1779                     for form in forms    1780                     for old, new in substitutions    1781                     if form.endswith(old)]

~\Anaconda3\lib\site-packages\nltk\corpus\reader\wordnet.py in &lt;listcomp&gt;(.0)    1779                     for form in forms    1780   for old, new in substitutions
-&gt; 1781                     if form.endswith(old)]    1782     1783         def filter_forms(forms):

AttributeError: 'generator' object has no attribute 'endswith'
</code></pre>

<p>I believe the error is coming from the source code for nltk.corpus.reader.wordnet </p>

<p>The whole source code can be seen in the nltk documentation page. It's too long to post here; but below is the raw <a href=""http://www.nltk.org/_modules/nltk/corpus/reader/wordnet.html"" rel=""nofollow noreferrer"">link</a>: </p>

<p>Thanks for your help.</p>
"
"48186483","NLTK Regex Chunker Not Processing multiple Grammar Rules in one command","2018-01-10 11:30:51","2","1415","2","1","","48217191","<p>I am trying to extract phrases from my corpus for this i have defined two rules one is noun followed by multiple nouns and other is adjective followed by noun, here i want that if same phrase is extracted from both rules the program should ignore second one, the problem I am facing is that the phrases are extracted form the first rule only and the second rule is not being applied.
below is the code:</p>

<pre><code>PATTERN = r""""""
      NP: {&lt;NN&gt;&lt;NN&gt;+}
      {&lt;ADJ&gt;&lt;NN&gt;*}

       """"""
    MIN_FREQ = 1
    MIN_CVAL = -13 # lowest cval -13
    def __init__(self):
        corpus_root = os.path.abspath('../multiwords/test')
        self.corpus = nltk.corpus.reader.TaggedCorpusReader(corpus_root,'.*')
        self.word_count_by_document = None
        self.phrase_frequencies = None

def calculate_phrase_frequencies(self):
        """"""
       extract the sentence chunks according to PATTERN and calculate
       the frequency of chunks with pos tags
       """"""

        # pdb.set_trace()
        chunk_freq_dict = defaultdict(int)
        chunker = nltk.RegexpParser(self.PATTERN)

        for sent in self.corpus.tagged_sents():

            sent = [s for s in sent if s[1] is not None]

            for chk in chunker.parse(sent).subtrees():

                if str(chk).startswith('(NP'):                  

                    phrase = chk.__unicode__()[4:-1]

                    if '\n' in phrase:
                        phrase = ' '.join(phrase.split())

                    just_phrase = ' '.join([w.rsplit('/', 1)[0] for w in phrase.split(' ')])
                   # print(just_phrase)
                    chunk_freq_dict[just_phrase] += 1
        self.phrase_frequencies = chunk_freq_dict
        #print(self.phrase_frequencies)
</code></pre>
"
"48169545","Does spacy take as input a list of tokens?","2018-01-09 13:43:53","10","8475","1","2","","48187129","<p>I would like to use spacy's POS tagging, NER, and dependency parsing without using word tokenization. Indeed, my input is a list of tokens representing a sentence, and I would like to respect the user's tokenization.
Is this possible at all, either with spacy or any other NLP package ? </p>

<p>For now, I am using this spacy-based function to put a sentence (a unicode string) in the Conll format:</p>

<pre><code>import spacy
nlp = spacy.load('en')
def toConll(string_doc, nlp):
   doc = nlp(string_doc)
   block = []
   for i, word in enumerate(doc):
          if word.head == word:
                  head_idx = 0
          else:
                  head_idx = word.head.i - doc[0].i + 1
          head_idx = str(head_idx)
          line = [str(i+1), str(word), word.lemma_, word.tag_,
                      word.ent_type_, head_idx, word.dep_]
          block.append(line)
   return block
conll_format = toConll(u""Donald Trump is the new president of the United States of America"")

Output:
[['1', 'Donald', u'donald', u'NNP', u'PERSON', '2', u'compound'],
 ['2', 'Trump', u'trump', u'NNP', u'PERSON', '3', u'nsubj'],
 ['3', 'is', u'be', u'VBZ', u'', '0', u'ROOT'],
 ['4', 'the', u'the', u'DT', u'', '6', u'det'],
 ['5', 'new', u'new', u'JJ', u'', '6', u'amod'],
 ['6', 'president', u'president', u'NN', u'', '3', u'attr'],
 ['7', 'of', u'of', u'IN', u'', '6', u'prep'],
 ['8', 'the', u'the', u'DT', u'GPE', '10', u'det'],
 ['9', 'United', u'united', u'NNP', u'GPE', '10', u'compound'],
 ['10', 'States', u'states', u'NNP', u'GPE', '7', u'pobj'],
 ['11', 'of', u'of', u'IN', u'GPE', '10', u'prep'],
 ['12', 'America', u'america', u'NNP', u'GPE', '11', u'pobj']]
</code></pre>

<p>I would like to do the same while having as input a list of tokens...</p>
"
"48109690","NLTK: Adding two values for a feature in FCFG","2018-01-05 08:20:09","2","130","0","1","","48109959","<p>For NLTK's Feature Grammar, how do I add more than one value for a feature.
E.g. Currently I'm able to successfully use the following,</p>

<p><code>NP[TYPE=name]</code></p>

<p>I want something like,</p>

<p><code>NP[TYPE=[name,organisation,location]]</code></p>

<p>That is, <code>NP</code> should be able to take values whose type are either name, organisation or location.</p>
"
"48054677","Context free grammar with feature structure in Python","2018-01-02 01:42:27","1","1053","1","1","","48055937","<p>Am trying to generate sentences from a defined grammar with python, to avoid agreement problem I used feature structures,</p>

<p>This is the code I have done so far:</p>

<pre><code>&gt;&gt;&gt; from __future__ import print_function
   &gt;&gt;&gt; import nltk
   &gt;&gt;&gt; from nltk.featstruct import FeatStruct
   &gt;&gt;&gt; from nltk import grammar, parse
   &gt;&gt;&gt; from nltk.parse.generate import generate
   &gt;&gt;&gt; from nltk import CFG
   &gt;&gt;&gt; g = """"""
    % start DP
    DP-&gt; D[AGR=[NUM='sg', PERS=3, GND='m']] N[AGR=[NUM='sg', GND='m']]
    D[AGR=[NUM='sg', PERS=3, GND='f']] -&gt; 'une' | 'la'
    D[AGR=[NUM='sg', PERS=3, GND='m']] -&gt; 'un' | 'le'
    D[AGR=[NUM='pl', PERS=3]] -&gt; 'des' | 'les'
    N[AGR=[NUM='sg', GND='m']] -&gt; 'garçon'
    N[AGR=[NUM='pl', GND='m']] -&gt; 'garçons'
    N[AGR=[NUM='sg', GND='f']] -&gt; 'fille'
    N[AGR=[NUM='pl', GND='f']] -&gt; 'filles'
    """"""
        &gt;&gt;&gt; for sentence in generate(grammar, n=30):
            print(''.join(sentence))
</code></pre>

<p>This is the output am getting:</p>

<pre><code>unegarçon
unegarçons
unefille
unefilles
lagarçon
lagarçons
lafille
lafilles
ungarçon
ungarçons
unfille
unfilles
legarçon
legarçons
lefille
lefilles
desgarçon
desgarçons
desfille
desfilles
lesgarçon
lesgarçons
lesfille
lesfilles
</code></pre>

<p>While am supposed to have an output like this:</p>

<pre><code>un garçon
le garçon
</code></pre>

<p>The problems I have are:</p>

<ol>
<li><p>The agreement is not working out, am having sentences that does not respect the agreement</p></li>
<li><p>There is no space between the two words in the sentence.</p></li>
</ol>

<p>What is that I can't see?</p>
"
"48048297","Explicit CPU placement in TensorFlow","2018-01-01 08:46:22","3","922","0","1","","48048786","<p>I found there are a piece of code in official model sample which confused me. </p>

<pre><code>with tf.device(""/cpu:0""):
  embedding = tf.get_variable(
      ""embedding"", [vocab_size, size], dtype=data_type())
  inputs = tf.nn.embedding_lookup(embedding, input_.input_data)
</code></pre>

<p>Why using <code>tf.device(""/cpu:0"")</code> here? Except the case GPU memory leak, is there any other situation which we need to designate CPU operations explicitly?</p>
"
"47872303","Counting matrix pairs using a threshold","2017-12-18 16:12:40","0","379","2","1","","47878305","<p>I have a folder with hundreds of txt files I need to analyse for similarity. Below is an example of a script I use to run similarity analysis. In the end I get an array or a matrix I can plot etc.</p>

<p>I would like to see how many pairs there are with <code>cos_similarity &gt; 0.5</code> (or any other threshold I decide to use), removing <code>cos_similarity == 1</code> when I compare the same files, of course. </p>

<p>Secondly, I need a list of these pairs based on file names. </p>

<p>So the output for the example below would look like:</p>

<p><code>1</code></p>

<p>and</p>

<p><code>[""doc1"", ""doc4""]</code></p>

<p>Will really appreciate your help as I feel a bit lost not knowing which direction to go.</p>

<p>This is an example of my script to get the matrix:</p>

<pre><code>doc1 = ""Amazon's promise of next-day deliveries could be investigated amid customer complaints that it is failing to meet that pledge.""
doc2 = ""The BBC has been inundated with comments from Amazon Prime customers. Most reported problems with deliveries.""
doc3 = ""An Amazon spokesman told the BBC the ASA had confirmed to it there was no investigation at this time.""
doc4 = ""Amazon's promise of next-day deliveries could be investigated amid customer complaints...""
documents = [doc1, doc2, doc3, doc4]

# In my real script I iterate through a folder (path) with txt files like this:
#def read_text(path):
#    documents = []
#    for filename in glob.iglob(path+'*.txt'):
#        _file = open(filename, 'r')
#        text = _file.read()
#        documents.append(text)
#    return documents

import nltk, string, numpy
nltk.download('punkt') # first-time use only
stemmer = nltk.stem.porter.PorterStemmer()
def StemTokens(tokens):
    return [stemmer.stem(token) for token in tokens]
remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)
def StemNormalize(text):
    return StemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))

nltk.download('wordnet') # first-time use only
lemmer = nltk.stem.WordNetLemmatizer()
def LemTokens(tokens):
    return [lemmer.lemmatize(token) for token in tokens]
remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)
def LemNormalize(text):
    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))

from sklearn.feature_extraction.text import CountVectorizer
LemVectorizer = CountVectorizer(tokenizer=LemNormalize, stop_words='english')
LemVectorizer.fit_transform(documents)
tf_matrix = LemVectorizer.transform(documents).toarray()

from sklearn.feature_extraction.text import TfidfTransformer
tfidfTran = TfidfTransformer(norm=""l2"")
tfidfTran.fit(tf_matrix)
tfidf_matrix = tfidfTran.transform(tf_matrix)
cos_similarity_matrix = (tfidf_matrix * tfidf_matrix.T).toarray()

from sklearn.feature_extraction.text import TfidfVectorizer
TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')
def cos_similarity(textlist):
    tfidf = TfidfVec.fit_transform(textlist)
    return (tfidf * tfidf.T).toarray()
cos_similarity(documents)
</code></pre>

<p>Out:</p>

<pre><code>array([[ 1.        ,  0.1459739 ,  0.03613371,  0.76357693],
       [ 0.1459739 ,  1.        ,  0.11459266,  0.19117117],
       [ 0.03613371,  0.11459266,  1.        ,  0.04732164],
       [ 0.76357693,  0.19117117,  0.04732164,  1.        ]])
</code></pre>
"
"47813330","how to retrieve subtrees while parsing in nlp","2017-12-14 12:24:57","1","427","0","1","","47878904","<p>I would like to retrieve the sub tress while parsing the sentence like below:</p>
<pre><code>sentence = &quot;All new medications must undergo testing before they can be 
             prescribed&quot;
parser = stanford.StanfordParser()
tree_parse = parser.raw_parse(sentence)
for i, sub_tree in enumerate(tree_parse[0].subtrees()):
   if sub_tree.label() in [&quot;S&quot;]:
      sub_list = sub_tree
      print(sub_list)
</code></pre>
<p>What I am expecting is to access the subtree labeled &quot;S&quot; individually like below:</p>
<h1>first subtree</h1>
<pre><code>(S
  (NP (DT All) (JJ new) (NNS medications))
  (VP
    (MD must)
    (VP
      (VB undergo)
</code></pre>
<h1>second subtree</h1>
<pre><code>(S
    (VP
      (VBG testing)
      (SBAR
        (IN before)
    
</code></pre>
<h1>3rd subtree</h1>
<pre><code>(S
          (NP (PRP they))
          (VP (MD can) (VP (VB be) (VP (VBN prescribed)))))))))))
</code></pre>
<p>But the actual output is like below:</p>
<pre><code> (NP (DT All) (JJ new) (NNS medications))
  (VP
  (MD must)
  (VP
    (VB undergo)
    (S
      (VP
        (VBG testing)
        (SBAR
          (IN before)
          (S
            (NP (PRP they))
            (VP (MD can) (VP (VB be) (VP (VBN prescribed))))))))))
 How to access the sub tress individually like accessing items in a list?
</code></pre>
"
"47793039","TF-IDF extracting keywords","2017-12-13 12:27:57","0","182","0","1","","47793230","<p>Working on function somewhat like this:</p>



<pre class=""lang-python prettyprint-override""><code>def get_feature_name_by_tfidf(text_to_process):
    with open(master_path + '\\additional_stopwords.txt', 'r') as f:
        additional_stop_words = ast.literal_eval(f.read())
    stop_words = text.ENGLISH_STOP_WORDS.union(set(additional_stop_words))
    tf = TfidfVectorizer(analyzer='word', ngram_range=(1, 4), min_df=0, stop_words=stop_words)
    tfidf_matrix = tf.fit_transform(text_to_process.split(','))
    tagged = nltk.pos_tag(tf.get_feature_names())
    feature_names_with_tags = {k: v for k, v in dict(tagged).items() if v != 'VBP'}
    return list(feature_names_with_tags.keys())
</code></pre>

<p>Which return the list of keywords in the passed text.
Is there any way to get the keywords in the same case as it is provided?
Like passed string </p>

<p>Input:</p>

<pre class=""lang-python prettyprint-override""><code>a = ""TIME is the company where I work""
</code></pre>

<p>Instead of getting keyword list as:</p>

<pre class=""lang-python prettyprint-override""><code>['time', 'company']
</code></pre>

<p>I like to get:</p>

<pre class=""lang-python prettyprint-override""><code>['TIME', 'company']
</code></pre>
"
"47778403","Computing TF-IDF on the whole dataset or only on training data?","2017-12-12 17:34:21","19","14835","3","3","","47781492","<p>In the chapter seven of this book ""TensorFlow Machine Learning Cookbook"" the author in pre-processing data uses <code>fit_transform</code> function of scikit-learn to get the <code>tfidf</code> features of text for training. The author gives all text data to the function before separating it into train and test. Is it a true action or we must separate data first and then perform <code>fit_transform</code> on train and <code>transform</code> on test?</p>
"
"47769818","Why is my NLTK function slow when processing the DataFrame?","2017-12-12 10:01:54","4","5681","12","1","","47788736","<p>I am trying to run through a function with my million lines in a datasets. </p>

<ol>
<li>I read the data from CSV in a dataframe</li>
<li>I use drop list to drop data i don't need</li>
<li>I pass it through a NLTK function in a for loop.</li>
</ol>

<p>code:</p>

<pre><code>def nlkt(val):
    val=repr(val)
    clean_txt = [word for word in val.split() if word.lower() not in stopwords.words('english')]
    nopunc = [char for char in str(clean_txt) if char not in string.punctuation]
    nonum = [char for char in nopunc if not char.isdigit()]
    words_string = ''.join(nonum)
    return words_string
</code></pre>



<p>Now i am calling the above function using a for loop to run through by million records. Even though i am on a heavy weight server with 24 core cpu and 88 GB Ram i see the loop is taking too much time and not using the computational power that is there</p>



<p>I am calling the above function like this</p>

<pre><code>data = pd.read_excel(scrPath + ""UserData_Full.xlsx"", encoding='utf-8')
droplist = ['Submitter', 'Environment']
data.drop(droplist,axis=1,inplace=True)

#Merging the columns company and detailed description

data['Anylize_Text']= data['Company'].astype(str) + ' ' + data['Detailed_Description'].astype(str)

finallist =[]

for eachlist in data['Anylize_Text']:
    z = nlkt(eachlist)
    finallist.append(z)
</code></pre>

<p>The above code works perfectly OK just too slow when we have few million record. It is just a sample record in excel but actual data will be in DB which will run in few hundred millions. Is there any way I can speed up the operation to pass the data through the function faster - use more computational power instead?</p>
"
"47727078","What does a weighted word embedding mean?","2017-12-09 09:16:02","18","14596","0","2","","47728457","<p>In the <a href=""http://www.aclweb.org/anthology/S17-2100"" rel=""noreferrer"">paper</a> that I am trying to implement, it says,</p>

<blockquote>
  <p>In this work, tweets were modeled using three types of text
  representation. The first one is a bag-of-words model weighted by
  tf-idf (term frequency
  - inverse document frequency) (Section
  2.1.1). The second represents a sentence by averaging the word embeddings of all words (in the sentence) and the third represents a
  sentence by averaging the weighted word embeddings of all words, the
  weight of a word is given by tf-idf (Section
  2.1.2).</p>
</blockquote>

<p>I am not sure about the <em>third representation</em> which is mentioned as the weighted word embeddings which is using the weight of a word is given by tf-idf. I am not even sure if they can used together. </p>
"
"47726833","Get the word from stem (stemming)","2017-12-09 08:39:08","2","1327","0","2","","47726883","<p>I am using porter stemmer as follows to get the stem of my words.</p>

<pre><code>from nltk.stem.porter import PorterStemmer
stemmer = PorterStemmer()
def stem_tokens(tokens, stemmer):
    stemmed = []
    for item in tokens:
        stemmed.append(stemmer.stem(item))
    return stemmed
</code></pre>

<p>Now, I want to know the possibility of some word from the stem to make it readable. For example <code>environ</code> to <code>environment</code> or <code>educ</code> to <code>education</code> etc. Is it possible to do?</p>
"
"47725035","Lemmatization with apache lucene","2017-12-09 03:29:19","10","4021","2","2","","62033221","<p>I'm developing a text analysis project using apache lucene. I need to lemmatize some text (transform the words to their canonical forms). I've already written the code that makes stemming. Using it, I am able to convert the following sentence</p>
<blockquote>
<p>The stem is the part of the word that never changes even when morphologically inflected; a lemma is the base form of the word. For example, from &quot;produced&quot;, the lemma is &quot;produce&quot;, but the stem is &quot;produc-&quot;. This is because there are words such as production</p>
</blockquote>
<p>into</p>
<blockquote>
<p>stem part word never chang even when morpholog inflect lemma base form word exampl from produc lemma produc stem produc becaus word product</p>
</blockquote>
<p>However, I need to get the base forms of the words: <em>example</em> instead of <em>exampl</em>, <em>produce</em> instead of <em>produc</em>, and so on.</p>
<p>I am using lucene because it has analyzers for many languages (I need at least English and Russian). I know about <a href=""https://stanfordnlp.github.io/CoreNLP/"" rel=""nofollow noreferrer"">Stanford NLP</a> library, but it has no Russian language support.</p>
<p>So is there any way to do lemmatization for several languages like I do stemming using lucene?</p>
<p>The simplified version of my code responsible for stemming:</p>
<pre><code>//Using apache tika to identify the language
LanguageIdentifier identifier = new LanguageIdentifier(text);
//getting analyzer according to the language (eg, EnglishAnalyzer for 'en')
Analyzer analyzer = getAnalyzer(identifier.getLanguage());
TokenStream stream = analyzer.tokenStream(&quot;field&quot;, text);
stream.reset();
while (stream.incrementToken()) {
    String stem = stream.getAttribute(CharTermAttribute.class).toString();
    // doing something with the stem
    System.out.print(stem+ &quot; &quot;);
}
stream.end();
stream.close();
</code></pre>
<p><strong>UPDATE:</strong> I found the <a href=""https://github.com/AKuznetsov/russianmorphology"" rel=""nofollow noreferrer"">library</a> that does almost what I need (for English and Russian languages) and uses apache lucene (although in its own way), it's definitely worth exploring.</p>
"
"47683077","NLTK and PYTHON GRAMMAR","2017-12-06 20:38:38","-1","57","0","1","","47684578","<p>I already have python 2.7 and <code>nltk</code> installed on my system
please and please, how can I make my python program use the below cfg to be attaching part of speech to my sentences??</p>

<pre><code>N -&gt; 'ọnẹkẹlẹ'|'igbẹlẹ'|'ọma-ọnẹkẹlẹ' 
Pr -&gt; 'Omi'|'uwẹ'|'awa'|'ama'|'oñwu'|'I'
Dart -&gt; 'lẹ'
Adj -&gt; 'kẹkẹ'|'nya'|'kpa' 
Adv -&gt; 'ọgboolo'|'nyọnyọ'|'lile'
Ord -&gt; 'ejodudu'|'ẹkeji'|'ẹkẹta'
Card -&gt; 'ka'|'meji'|'mẹta' 
Quant -&gt; 'wewe'|'gwẹẹ'|'Uchẹkibọ'|'uchẹkibọ'
Dem -&gt; 'dẹi'|'i'|'lẹi'
Poss -&gt; 'mi'|'wa'|'wẹ'|'ñw'|'ma'
</code></pre>
"
"47633449","How to get better lemmas from Spacy","2017-12-04 12:31:06","3","2350","1","2","","47984304","<p>While ""PM"" can mean ""pm(time)"" it can also mean ""Prime Minister"".</p>

<p>I want to capture the latter. I want lemma of ""PM"" to return ""Prime Minister"". How can I do this using <code>spacy</code>?</p>

<p>Example returning unexpected lemma: </p>

<pre><code>&gt;&gt;&gt; import spacy
&gt;&gt;&gt; #nlp = spacy.load('en')
&gt;&gt;&gt; nlp = spacy.load('en_core_web_lg')
&gt;&gt;&gt; doc = nlp(u'PM means prime minister')
&gt;&gt;&gt; for word in doc:
...     print(word.text, word.lemma_)
... 
PM pm
means mean
prime prime
minister minister
</code></pre>

<p>As per doc <a href=""https://spacy.io/api/annotation"" rel=""nofollow noreferrer"">https://spacy.io/api/annotation</a>, spacy uses WordNet for lemmas; </p>

<blockquote>
  <p>A lemma is the uninflected form of a word. The English lemmatization data is taken from WordNet..</p>
</blockquote>

<p>When I tried inputting ""pm"" in <a href=""http://wordnetweb.princeton.edu/perl/webwn?s=pm&amp;sub=Search%20WordNet&amp;o2=&amp;o0=1&amp;o8=1&amp;o1=1&amp;o7=&amp;o5=&amp;o9=&amp;o6=&amp;o3=&amp;o4=&amp;h=0000"" rel=""nofollow noreferrer"">Wordnet</a>, it shows ""Prime Minister"" as one of the lemmas.</p>

<p>What am I missing here?</p>
"
"47624347","Read my own dataset for NLTK Part of Speech tagging using PerceptronTagger","2017-12-03 23:09:17","1","520","0","1","","47627822","<p>I'm new to NLTK and still pretty new to python. I want to use my own dataset to train and test NLTK's Perceptron tagger. The training and testing data has the following format (it's just saved in a txt file): </p>

<pre><code>Pierre  NNP
Vinken  NNP
,       ,
61      CD
years   NNS
old     JJ
,       ,
will    MD
join    VB
the     DT
board   NN
as      IN
a       DT
nonexecutive    JJ
director        NN
Nov.    NNP
29      CD
.       .
</code></pre>

<p>I want to call these functions on the data:</p>

<pre><code>perceptron_tagger = nltk.tag.perceptron.PerceptronTagger(load=False)
perceptron_tagger.train(train_data)
accuracy = perceptron_tagger.evaluate(test_data)
</code></pre>

<p>I've tried a few things but I just can't figure out what format the data is expected to be in. Any help would be appreciated! Thanks</p>
"
"47605377","Semantic search - retrieve sentences from a bunch of text files that closely matches the passed in search phrase","2017-12-02 06:44:34","2","1445","0","1","","47605779","<p>I have a bunch of text files. My application requirement is to search for sentences (or paragraphs) which matches semantically to the search phrase I pass.</p>

<p>e.g.: Let us say there is a sentence ""The quick brown fox jumped over the lazy dog"".</p>

<p>I would like the following search phrases to search through my text files and list the above sentence (sometimes along with the previous and the next sentence so as to show the context)</p>

<ul>
<li>quick fox</li>
<li>fox jump over dog</li>
<li>bron fox (note the spelling mistake here)</li>
</ul>

<p>(This is typically what they say is used in the patent search sites to identify patents based on the search phrase - semantic search)</p>

<p>For implementation - I looked up the internet and this is what I found:</p>

<ol>
<li>Use sentence tokenizer from the nltk python library to break up text files into sentences:</li>
</ol>

<blockquote>
<pre><code>from nltk.tokenize import sent_tokenize 
f = open(""fileName"")
mytext = f.readline()
sent_tokenize(mytext)
</code></pre>
</blockquote>

<ol start=""2"">
<li>Need an equivalent of elastic search's match feature where passing the search phrases as above would actually find the sentence I am looking for.</li>
</ol>

<p><a href=""https://i.sstatic.net/RsQNK.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/RsQNK.jpg"" alt=""enter image description here""></a></p>

<p>Please suggest me a simple way of achieving both 1 and 2 using some library. This application just runs locally on my machine.</p>
"
"47599575","NLTK RegexParser: chunking consecutive overlapping nouns","2017-12-01 18:39:35","0","768","0","2","","51709530","<p>I want to use a RegexParser to chunk all consecutive overlapping nouns from a text, for example, I have the following tagged text:</p>

<pre><code>[('APPLE', 'NN'), ('BANANA', 'NN'), ('GRAPE', 'NN'), ('PEAR', 'NN')]
</code></pre>

<p>I want to extract:</p>

<pre><code>['APPLE BANANA', 'BANANA GRAPE', 'GRAPE PEAR']
</code></pre>

<p>I tried using the following grammar to avoid consuming the matched consecutive noun but it doesn't work:</p>

<pre><code>""CONSEC_NOUNS: {(?=(&lt;NN&gt;{2}))}""
</code></pre>

<p>Is there any possible way to do that?</p>

<p>EDIT: code</p>

<pre><code>import nltk

extract = []
grammar = ""CONSEC_NOUNS: {(?=(&lt;NN&gt;{2}))}""
cp = nltk.RegexpParser(grammar)
result = cp.parse([('APPLE', 'NN'), ('BANANA', 'NN'), ('GRAPE', 'NN'), ('PEAR', 'NN')])

for elem in result:
    if type(elem) == nltk.tree.Tree:
        extract.append(' '.join([pair[0] for pair in elem.leaves()]))

&gt;&gt;&gt; print(extract) //[]

// but I want to get ['APPLE BANANA', 'BANANA GRAPE', 'GRAPE PEAR']
</code></pre>
"
"47566940","Why do I need to lift when using MonadLog with Pipes","2017-11-30 06:11:17","1","130","0","1","","47574977","<p>I'm trying to get <code>logging-error</code> working with <code>pipes</code>. I'm nearly there—in the sense that I have something working—but I don't think it's quite right and I don't know how to fix it. The code:</p>

<pre><code>{-# LANGUAGE FlexibleContexts  #-}
{-# LANGUAGE OverloadedStrings #-}
{-# LANGUAGE NoImplicitPrelude #-}
{-# LANGUAGE PartialTypeSignatures #-}

module Main where

import           Protolude hiding ((&lt;&gt;), empty, for, get)

import           Control.Monad.Log
import           Text.PrettyPrint.Leijen.Text

import           Pipes


testApp :: (MonadIO m, MonadLog (WithSeverity Doc) m) =&gt; m ()
testApp = logInfo $ textStrict ""Logging works. Yah!""


printMessage :: (MonadIO m, MonadLog (WithSeverity Doc) m) =&gt; Consumer Text m ()
printMessage = forever $ await &gt;&gt;= putStrLn


readInputMessage :: (MonadIO m, MonadLog (WithSeverity Doc) m) =&gt; Producer Text m ()
readInputMessage = forever action      
  where
    action = do
      liftIO $ putStr (""&gt; "" :: Text)
      liftIO getLine &gt;&gt;= yield
      lift $ logInfo $ text ""Waits with abated breath""


runMyLogging :: MonadIO m =&gt; LoggingT (WithSeverity Doc) m a -&gt; m a
runMyLogging f = runLoggingT f (print . renderWithSeverity identity)


runPipesApp :: IO ()
runPipesApp = runMyLogging $ runEffect $
        readInputMessage
    &gt;-&gt; printMessage


runTestApp :: IO ()
runTestApp = runMyLogging testApp


main :: IO ()
main = do
  runTestApp
  runPipesApp
</code></pre>

<p>In <code>readInputMessage</code> I need to <code>lift</code> <code>logInfo</code> otherwise it won't compile. However <code>testApp</code> <code>logInfo</code> dosen't need to be lift'ed. Why do I need to lift in one but not the other?</p>

<p>Without <code>lift</code> this is the compilation error:</p>

<pre><code>/home/rgh/dev/haskell/fa-logging/app/Main.hs:29:7: error:
    • Could not deduce (MonadLog
                          (WithSeverity Doc) (Pipes.Proxy X () () Text m))
        arising from a use of ‘logInfo’
      from the context: (MonadIO m, MonadLog (WithSeverity Doc) m)
        bound by the type signature for:
                   readInputMessage :: forall (m :: * -&gt; *).
                                       (MonadIO m, MonadLog (WithSeverity Doc) m) =&gt;
                                       Producer Text m ()
        at app/Main.hs:23:1-84
    • In a stmt of a 'do' block:
        logInfo $ text ""Waits with abated breath""
      In the expression:
        do liftIO $ putStr (""&gt; "" :: Text)
           liftIO getLine &gt;&gt;= yield
           logInfo $ text ""Waits with abated breath""
      In an equation for ‘action’:
          action
            = do liftIO $ putStr (""&gt; "" :: Text)
                 liftIO getLine &gt;&gt;= yield
                 logInfo $ text ""Waits with abated breath""
   |
29 |       logInfo $ text ""Waits with abated breath""
   |       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

--  While building package fa-logging-0.0.0 using:
      /srv/cache/rgh/.stack/setup-exe-cache/x86_64-linux-nopie/Cabal-simple_mPHDZzAJ_2.0.0.2_ghc-8.2.1 --builddir=.stack-work/dist/x86_64-linux-nopie/Cabal-2.0.0.2 build lib:fa-logging exe:fa-logging --ghc-options "" -ddump-hi -ddump-to-file""
    Process exited with code: ExitFailure 1
</code></pre>

<p>I <em>think</em> it's not compiling because the compiler can't work out what type <code>m</code> is but I don't know how to fix it.</p>
"
"47523112","Detect stopword after lemma in Spacy","2017-11-28 02:43:11","5","3772","0","1","","47524257","<p>How to detect if word is a stopword after stemming and lemmatization in <code>spaCy</code>?</p>

<p>Assume sentence</p>

<pre><code>s = ""something good\nsomethings 2 bad""
</code></pre>

<p>In this case <code>something</code> is a stopword. Obviously (to me?) <code>Something</code> and <code>somethings</code> are also stopwords, but it needs to stemmed before. Following script will say that the first is true, but latter isn't.</p>

<pre><code>import spacy
from spacy.tokenizer import Tokenizer
nlp = spacy.load('en')
tokenizer = Tokenizer(nlp.vocab)

s = ""something good\nSomething 2 somethings""
tokens = tokenizer(s)

for token in tokens:
  print(token.lemma_, token.is_stop)
</code></pre>

<p>Returns:</p>

<pre><code>something True
good False
""\n"" False
Something False
2 False
somethings False
</code></pre>

<p>Is there a way to detect that through <code>spaCy</code> API?</p>
"
"47495044","Painfully slow Postgres query using WHERE on many adjacent rows","2017-11-26 10:18:38","2","237","4","3","","47496945","<p>I have the following psql table. It has roughly 2 billion rows in total. </p>

<pre><code> id  word      lemma     pos              textid  source     
 1  Stuffing   stuff      vvg             190568  AN         
 2  her        her        appge           190568  AN         
 3  key        key        nn1             190568  AN         
 4  into       into       ii              190568  AN         
 5  the        the        at              190568  AN         
 6  lock       lock       nn1             190568  AN         
 7  she        she        appge           190568  AN         
 8  pushed     push       vvd             190568  AN         
 9  her        her        appge           190568  AN         
10  way        way        nn1             190568  AN         
11  into       into       ii              190568  AN         
12  the        the        appge           190568  AN         
13  house      house      nn1             190568  AN         
14  .                     .               190568  AN         
15  She        she        appge           190568  AN         
16  had        have       vhd             190568  AN         
17  also       also       rr              190568  AN         
18  cajoled    cajole     vvd             190568  AN         
19  her        her        appge           190568  AN         
20  way        way        nn1             190568  AN         
21  into       into       ii              190568  AN         
22  the        the        at              190568  AN         
23  home       home       nn1             190568  AN         
24  .                     .               190568  AN         
..  ...        ...        ..              ...     ..
</code></pre>

<p>I would like to create the following table, which shows all ""way""-constructions with the words side-by-side and some data from the columns ""source"", ""lemma"" and ""pos"". </p>

<pre><code>source     word   word       word       lemma      pos        word       word     word       word       word       lemma      pos        word       word       
AN         lock   she        pushed     push       vvd        her        way      into       the        house      house      nn1        .          she
AN         had    also       cajoled    cajole     vvd        her        way      into       the        home       home       nn1        .          A          
AN         tried  to         force      force      vvi        her        way      into       the        palace     palace     nn1        ,          officials  
</code></pre>

<p>Here you can see the code I use: </p>

<pre><code>copy(
SELECT   c1.source, c1.word,  c2.word, c3.word,  c4.word, c4.lemma, c4.pos, c5.word, c6.word, c7.word, c8.word, c9.word, c9.lemma, c9.pos, c10.word, c11.word

FROM 

orderedflatcorpus AS c1, orderedflatcorpus AS c2, orderedflatcorpus AS c3, orderedflatcorpus AS c4, orderedflatcorpus AS c5, orderedflatcorpus AS c6, orderedflatcorpus AS c7, orderedflatcorpus AS c8, orderedflatcorpus AS c9, orderedflatcorpus AS c10, orderedflatcorpus AS c11

WHERE

c1.word LIKE '%' AND
c2.word LIKE '%' AND
c3.word LIKE '%' AND
c4.pos LIKE 'v%' AND
c5.pos = 'appge' AND
c6.lemma = 'way' AND
c7.pos LIKE 'i%' AND
c8.word = 'the' AND
c9.pos LIKE 'n%' AND
c10.word LIKE '%' AND
c11.word LIKE '%' 

AND 

c1.id + 1 = c2.id AND c1.id + 2 = c3.id AND c1.id + 3 = c4.id AND c1.id + 4 = c5.id AND c1.id + 5 = c6.id AND c1.id + 6 = c7.id AND c1.id + 7 = c8.id AND c1.id + 8 = c9.id AND c1.id + 9 = c10.id AND c1.id + 10 = c11.id

ORDER BY c1.id
)
TO 
'/home/postgres/Results/OUTPUT.csv'
DELIMITER E'\t'
csv header;
</code></pre>

<p>The query takes almost 9 hours to execute for the two billion rows (the result has about 19,000 rows). </p>

<p>What could I do to improve performance? </p>

<p>The word, pos and lemma columns already have btree indices. </p>

<p>Should I stick to my code and simply use a more powerful server with more cores/a faster CPU and more RAM (mine has only 8 GBs of RAM, a mere 2 cores and 2.8 GHz) ? Or would you recommend a different, more efficient SQL query?</p>

<p>Thanks!</p>
"
"47458616","Deep NLP pipeline with Whoosh","2017-11-23 15:13:13","2","474","0","1","","47464016","<p>I am very new to NLP and IR programs. I am trying to implement a deep NLP pipeline i.e. adding Lemmatizing, Dependency parsing features to the indexing of sentences. Following is my schema and searcher.</p>

<pre><code>my_analyzer = RegexTokenizer()| StopFilter()| LowercaseFilter() | StemFilter() | Lemmatizer()
    pos_analyser = RegexTokenizer() | StopFilter()| LowercaseFilter() | PosTagger()
    schema = Schema(id=ID(stored=True, unique=True), stem_text=TEXT(stored= True, analyzer=my_analyzer), pos_tag= pos_analyser)

for sentence in sent_tokenize_list1:
    writer.add_document(stem_text = sentence, pos_tag = sentence)
for sentence in sent_tokenize_list2:
    writer.add_document(stem_text = sentence, pos_tag = sentence)
writer.commit()
with ix.searcher() as searcher:
    og = qparser.OrGroup.factory(0.9)
    query_text = MultifieldParser([""stem_text"",""pos_tag""], schema = ix.schema, group= og).parse(
        ""who is controlling the threat of locusts?"")
     results = searcher.search(query_text, sortedby= scores, limit = 10 )
</code></pre>

<p>This is the custom analyzer.</p>

<pre><code>class PosTagger(Filter):
    def __eq__(self, other):
        return (other
                and self.__class__ is other.__class__
                and self.__dict__ == other.__dict__)

    def __ne__(self, other):
        return not self == other

    def __init__(self):
         self.cache = {}

    def __call__(self, tokens):
         assert hasattr(tokens, ""__iter__"")
         words = []
         tokens1, tokens2 = itertools.tee(tokens)
         for t in tokens1:
            words.append(t.text)
         tags = pos_tag(words)
         i=0
         for t in tokens2:
             t.text = tags[i][0] + "" ""+ tags[i][1]
             i += 1
             yield t
</code></pre>

<p>I am getting the following error.</p>

<blockquote>
  <p><strong>whoosh.fields.FieldConfigurationError: CompositeAnalyzer(RegexTokenizer(expression=re.compile('\w+(\.?\w+)*'),
  gaps=False), StopFilter(stops=frozenset({'for', 'will', 'tbd', 'with',
  'and', 'the', 'if', 'it', 'by', 'is', 'are', 'this', 'as', 'when',
  'us', 'or', 'from', 'yet', 'you', 'have', 'can', 'be', 'we', 'of',
  'to', 'on', 'a', 'an', 'your', 'at', 'in', 'may', 'not', 'that'}),
  min=2, max=None, renumber=True), LowercaseFilter(),
  PosTagger(cache={})) is not a FieldType object</strong></p>
</blockquote>

<p>Am I doing it a wrong way? Is this the proper way to add NLP pipeline to search engine?</p>
"
"47419335","Apertium translator. Is there a way to get the original phrase","2017-11-21 17:39:39","5","308","0","1","","47422332","<p>Is there a way in apertium translator to get the original phrase for a translation?</p>

<p>I.E. get something like:</p>

<pre><code>phrase: {
  original: { Hola, buenos días},
  translated: {Hello, good morning}
}
</code></pre>

<p>I need that in order to make a mechanism to improve the translations.</p>
"
"47400302","ValueError: setting an array element with a sequence - after making TF_IDF vectorization","2017-11-20 20:16:12","1","747","2","1","","47400532","<p>I'm new to data science and NLP. I want to perform TF_IDF vectorization on some text documents and after use the results to train different machine learning models. But when I try to train SVC model I obtain the ValueError: setting an array element with a sequence. Here is my code. </p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer(min_df=1, stop_words='english')
df['vect_message'] = vectorizer.fit_transform(df['message_encoding'])
X = df['vect_message']
y = df['severity']
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

from sklearn import svm
model = svm.SVC() 
model.fit(X_train, y_train) 
prediction = model.predict(X_test)
</code></pre>

<p>And I got an error on the line <code>model.fit(X_train, y_train)</code></p>

<p>I have already searched other similar questions and I found one where they advise using <code>.toarray()</code> method to transform sparse matrix into np.array. But this didn't help me. </p>
"
"47266183","R: How to create clusters based on row strings","2017-11-13 14:05:34","4","798","3","1","","47268265","<p>I m trying to create clusters from data based on the string value of each row. I m using the R langage. What I m calling a ""cluster"" is a big thematic (= family) that can define each keywords. I imagine something autogenearated based on the keyword, maybe by using lemmatization or ngram.</p>

<p>For example both keywords ""cloud services"" and ""the cloud service"" should be in the ""service"" cluster.</p>

<p>Here is my input vector:</p>

<pre><code>keywords_df &lt;- c(""cloud storage"", ""cloud computing"", ""google cloud storage"", ""the cloud service"", 
        ""free cloud storage"", ""what is cloud computing"", ""best cloud storage"",""cloud computing definition"", 
        ""amazon cloud services"", ""cloud service providers"", ""cloud services"", ""google cloud computing"", ""cloud computing services"", ""benefits of cloud computing"")
</code></pre>

<p>Here is the expected output dataframe:</p>

<pre><code>| Keyword                   |  Thematic |
|---------------------------|:---------:|
|cloud storage              |storage  |
|cloud computing            |computing|
|google cloud storage       |storage  |
|the cloud service          |service  |
|free cloud storage         |storage  |
|what is cloud computing    |computing|
|best cloud storage         |storage  |
|cloud computing definition |computing|
|amazon cloud service       |service |
|cloud service providers        |services |
|cloud service              |service |
|google cloud computing     |computing|
|cloud computing services   |service |
|benefits of cloud computing|computing|
</code></pre>

<p>The goal is to clean up the data in the ""keyword"" column and auto extract a kind of lemm or ngram.</p>

<p>Here is what I have done for now :</p>

<ol>
<li><p>Create the ""Thematic"" column based on keyword column:</p>

<pre><code>keywords_df &lt;- mutate(keywords_df,Thematic=Keyword)
keywords_df$Thematic &lt;- as.character(keywords_df$Thematic)
</code></pre></li>
<li><p>Remove Stopwords:</p>

<pre><code>stopwords_list&lt;-(c(""cloud"")) #Remove the main word
stopwords &lt;- stopwords(kind = ""en"")
stopwords &lt;- append(stopwords,stopwords_list)
x  = keywords_df$Thematic        
x  =  removeWords(x,stopwords)
keywords_df$Thematic &lt;- x  
</code></pre></li>
</ol>
"
"47239639","Difference between fine-grained and coarse-grained score for WSD tasks?","2017-11-11 15:36:45","4","1137","0","1","","47501128","<p>In all Senseval and SemEval Tasks, two scores are reported - fine-grained and coarse-grained. What do they mean in the context of sense disambiguation?</p>
"
"47029595","Using Custom Word2Vec to find semantic similarity between technical questions?","2017-10-31 07:02:04","0","2571","3","1","","47030495","<p>We can get the similarity of two sentences like ""The boy is playing football"" and ""A kid is playing football"" using Google news vectors by applying ""SIF Embeddings"".</p>

<p>I would like to get the similarity for two sentences which are technical like ""what is an abstract class?"" and ""what is a class?"".</p>

<p>I have used Google-news Vectors in getting the similarity but it didn't work well.</p>

<p>I would like to know how training data should be?</p>
"
"47022246","Warning message after importing gensim module in Windows","2017-10-30 18:46:21","1","769","1","1","","47038874","<p>When I tried to import gensim module in Windows, I end up with below error.</p>

<blockquote>
  <p>c:\python27\lib\site-packages\gensim-3.0.1-py2.7-win-amd64.egg\gensim\utils.py:862: UserWarning: detected Windows; aliasing chunkize to chunkize_serial
    <strong>warnings.warn(""detected Windows; aliasing chunkize to chunkize_serial"")</strong></p>
</blockquote>

<p>Is there any possibility to overcome this warning?</p>
"
"46986560","WordListCorpusReader is not iterable","2017-10-28 05:26:03","13","32719","5","1","","47007462","<p>So, I am new to using Python and NLTK. I have a file called <strong>reviews.csv</strong> which consists of comments extracted from amazon. I have tokenized the contents of this csv file and written it to a file called <strong>csvfile.csv</strong>. Here's the code :</p>

<pre><code>from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.stem import PorterStemmer
import csv #CommaSpaceVariable
from nltk.corpus import stopwords
ps = PorterStemmer()
stop_words = set(stopwords.words(""english""))
with open ('reviews.csv') as csvfile:
    readCSV = csv.reader(csvfile,delimiter='.')    
    for lines in readCSV:
        word1 = word_tokenize(str(lines))
        print(word1)
    with open('csvfile.csv','a') as file:
        for word in word1:
            file.write(word)
            file.write('\n')
    with open ('csvfile.csv') as csvfile:
        readCSV1 = csv.reader(csvfile)
    for w in readCSV1:
        if w not in stopwords:
            print(w)
</code></pre>

<p>I am trying to perform stemming on csvfile.csv. But I get this error:</p>

<pre><code>  Traceback (most recent call last):&lt;br&gt;
  File ""/home/aarushi/test.py"", line 25, in &lt;module&gt; &lt;br&gt;
   if w not in stopwords: &lt;br&gt;
  TypeError: argument of type 'WordListCorpusReader' is not iterable
</code></pre>
"
"46924452","What to do when Seq2Seq network repeats words over and over in output?","2017-10-25 05:13:03","12","6503","0","2","","59407187","<p>So, I've been working on a project for a while, we have <em>very</em> little data, I know it would become much better if we were able to put together a much much larger dataset. That aside, my issue at the moment is when I have a sentence input, my outputs look like this right now:</p>

<blockquote>
  <p>contactid contactid contactid contactid</p>
</blockquote>

<p>A single word is focused on and repeated over and over again. What can I do to overcome this hurdle?</p>

<p>Things I've tried:</p>

<ol>
<li>Double checked I was appending start/stop tokens and make sure the tokens were properly placed in the top of their vocab files, I am sharing vocab.</li>
<li>I found something saying it could be due to poor word embeddings. To that end I checked with tensorboard and sure enough PCA showed a very dense cluster of points. Seeing that I grabbed Facebook's public pre trained word vectors and loaded them in as the embedding. Trained again and this time tensorboard PCA showed a much better picture.</li>
<li>Switched my training scheduler from basic to SampledScheduling to occasionally replace a training output with the ground truth.</li>
<li>Switched my decoder to use the beam search decoder I figured this may give more robust responses if the word choices were close together in the intermediary feature space.</li>
</ol>

<p>For certain my perplexity is steadily decreasing.</p>

<p>Here is my dataset preperation code:</p>

<pre><code>class ModelInputs(object):
""""""Factory to construct various input hooks and functions depending on mode """"""

def __init__(
    self, vocab_files, batch_size,
    share_vocab=True, src_eos_id=1, tgt_eos_id=2
):
    self.batch_size = batch_size
    self.vocab_files = vocab_files
    self.share_vocab = share_vocab
    self.src_eos_id = src_eos_id
    self.tgt_eos_id = tgt_eos_id

def get_inputs(self, file_path, num_infer=None, mode=tf.estimator.ModeKeys.TRAIN):
    self.mode = mode
    if self.mode == tf.estimator.ModeKeys.TRAIN:
        return self._training_input_hook(file_path)
    if self.mode == tf.estimator.ModeKeys.EVAL:
        return self._validation_input_hook(file_path)
    if self.mode == tf.estimator.ModeKeys.PREDICT:
        if num_infer is None:
            raise ValueError('If performing inference must supply number of predictions to be made.')
        return self._infer_input_hook(file_path, num_infer)

def _prepare_data(self, dataset, out=False):
    prep_set = dataset.map(lambda string: tf.string_split([string]).values)
    prep_set = prep_set.map(lambda words: (words, tf.size(words)))
    if out == True:
        return prep_set.map(lambda words, size: (self.vocab_tables[1].lookup(words), size))
    return prep_set.map(lambda words, size: (self.vocab_tables[0].lookup(words), size))

def _batch_data(self, dataset, src_eos_id, tgt_eos_id):
    batched_set = dataset.padded_batch(
            self.batch_size,
            padded_shapes=((tf.TensorShape([None]), tf.TensorShape([])), (tf.TensorShape([None]), tf.TensorShape([]))),
            padding_values=((src_eos_id, 0), (tgt_eos_id, 0))
    )
    return batched_set

def _batch_infer_data(self, dataset, src_eos_id):
    batched_set = dataset.padded_batch(
        self.batch_size,
        padded_shapes=(tf.TensorShape([None]), tf.TensorShape([])),
        padding_values=(src_eos_id, 0)
    )
    return batched_set

def _create_vocab_tables(self, vocab_files, share_vocab=False):
    if vocab_files[1] is None and share_vocab == False:
        raise ValueError('If share_vocab is set to false must provide target vocab. (src_vocab_file, \
                target_vocab_file)')

    src_vocab_table = lookup_ops.index_table_from_file(
        vocab_files[0],
        default_value=UNK_ID
    )

    if share_vocab:
        tgt_vocab_table = src_vocab_table
    else:
        tgt_vocab_table = lookup_ops.index_table_from_file(
            vocab_files[1],
            default_value=UNK_ID
        )

    return src_vocab_table, tgt_vocab_table

def _prepare_iterator_hook(self, hook, scope_name, iterator, file_path, name_placeholder):
    if self.mode == tf.estimator.ModeKeys.TRAIN or self.mode == tf.estimator.ModeKeys.EVAL:
        feed_dict = {
                name_placeholder[0]: file_path[0],
                name_placeholder[1]: file_path[1]
        }
    else:
        feed_dict = {name_placeholder: file_path}

    with tf.name_scope(scope_name):
        hook.iterator_initializer_func = \
                lambda sess: sess.run(
                    iterator.initializer,
                    feed_dict=feed_dict,
                )

def _set_up_train_or_eval(self, scope_name, file_path):
    hook = IteratorInitializerHook()
    def input_fn():
        with tf.name_scope(scope_name):
            with tf.name_scope('sentence_markers'):
                src_eos_id = tf.constant(self.src_eos_id, dtype=tf.int64)
                tgt_eos_id = tf.constant(self.tgt_eos_id, dtype=tf.int64)
            self.vocab_tables = self._create_vocab_tables(self.vocab_files, self.share_vocab)
            in_file = tf.placeholder(tf.string, shape=())
            in_dataset = self._prepare_data(tf.contrib.data.TextLineDataset(in_file).repeat(None))
            out_file = tf.placeholder(tf.string, shape=())
            out_dataset = self._prepare_data(tf.contrib.data.TextLineDataset(out_file).repeat(None))
            dataset = tf.contrib.data.Dataset.zip((in_dataset, out_dataset))
            dataset = self._batch_data(dataset, src_eos_id, tgt_eos_id)
            iterator = dataset.make_initializable_iterator()
            next_example, next_label = iterator.get_next()
            self._prepare_iterator_hook(hook, scope_name, iterator, file_path, (in_file, out_file))
            return next_example, next_label

    return (input_fn, hook)

def _training_input_hook(self, file_path):
    input_fn, hook = self._set_up_train_or_eval('train_inputs', file_path)

    return (input_fn, hook)

def _validation_input_hook(self, file_path):
    input_fn, hook = self._set_up_train_or_eval('eval_inputs', file_path)

    return (input_fn, hook)

def _infer_input_hook(self, file_path, num_infer):
    hook = IteratorInitializerHook()

    def input_fn():
        with tf.name_scope('infer_inputs'):
            with tf.name_scope('sentence_markers'):
                src_eos_id = tf.constant(self.src_eos_id, dtype=tf.int64)
            self.vocab_tables = self._create_vocab_tables(self.vocab_files, self.share_vocab)
            infer_file = tf.placeholder(tf.string, shape=())
            dataset = tf.contrib.data.TextLineDataset(infer_file)
            dataset = self._prepare_data(dataset)
            dataset = self._batch_infer_data(dataset, src_eos_id)
            iterator = dataset.make_initializable_iterator()
            next_example, seq_len = iterator.get_next()
            self._prepare_iterator_hook(hook, 'infer_inputs', iterator, file_path, infer_file)
            return ((next_example, seq_len), None)

    return (input_fn, hook)
</code></pre>

<p>And here is my model:</p>

<pre><code>class Seq2Seq():

def __init__(
    self, batch_size, inputs,
    outputs, inp_vocab_size, tgt_vocab_size,
    embed_dim, mode, time_major=False,
    enc_embedding=None, dec_embedding=None, average_across_batch=True,
    average_across_timesteps=True, vocab_path=None, embedding_path='./data_files/wiki.simple.vec'
):
    embed_np = self._get_embedding(embedding_path)
    if not enc_embedding:
        self.enc_embedding = tf.contrib.layers.embed_sequence(
            inputs,
            inp_vocab_size,
            embed_dim,
            trainable=True,
            scope='embed',
            initializer=tf.constant_initializer(value=embed_np, dtype=tf.float32)
        )
    else:
        self.enc_embedding = enc_embedding
    if mode == tf.estimator.ModeKeys.TRAIN or mode == tf.estimator.ModeKeys.EVAL:
        if not dec_embedding:
            embed_outputs = tf.contrib.layers.embed_sequence(
                outputs,
                tgt_vocab_size,
                embed_dim,
                trainable=True,
                scope='embed',
                reuse=True
            )
            with tf.variable_scope('embed', reuse=True):
                dec_embedding = tf.get_variable('embeddings')
            self.embed_outputs = embed_outputs
            self.dec_embedding = dec_embedding

        else:
            self.dec_embedding = dec_embedding
    else:
        with tf.variable_scope('embed', reuse=True):
            self.dec_embedding = tf.get_variable('embeddings')

    if mode == tf.estimator.ModeKeys.PREDICT and vocab_path is None:
        raise ValueError('If mode is predict, must supply vocab_path')
    self.vocab_path = vocab_path
    self.inp_vocab_size = inp_vocab_size
    self.tgt_vocab_size = tgt_vocab_size
    self.average_across_batch = average_across_batch
    self.average_across_timesteps = average_across_timesteps
    self.time_major = time_major
    self.batch_size = batch_size
    self.mode = mode

def _get_embedding(self, embedding_path):
    model = KeyedVectors.load_word2vec_format(embedding_path)
    vocab = model.vocab
    vocab_len = len(vocab)
    return np.array([model.word_vec(k) for k in vocab.keys()])

def _get_lstm(self, num_units):
    return tf.nn.rnn_cell.BasicLSTMCell(num_units)

def encode(self, num_units, num_layers, seq_len, cell_fw=None, cell_bw=None):
    if cell_fw and cell_bw:
        fw_cell = cell_fw
        bw_cell = cell_bw
    else:
        fw_cell = self._get_lstm(num_units)
        bw_cell = self._get_lstm(num_units)
    encoder_outputs, bi_encoder_state = tf.nn.bidirectional_dynamic_rnn(
        fw_cell,
        bw_cell,
        self.enc_embedding,
        sequence_length=seq_len,
        time_major=self.time_major,
        dtype=tf.float32
    )
    c_state = tf.concat([bi_encoder_state[0].c, bi_encoder_state[1].c], axis=1)
    h_state = tf.concat([bi_encoder_state[0].h, bi_encoder_state[1].h], axis=1)
    encoder_state = tf.contrib.rnn.LSTMStateTuple(c=c_state, h=h_state)
    return tf.concat(encoder_outputs, -1), encoder_state

def _train_decoder(self, decoder_cell, out_seq_len, encoder_state, helper):
    if not helper:
        helper = tf.contrib.seq2seq.ScheduledEmbeddingTrainingHelper(
            self.embed_outputs,
            out_seq_len,
            self.dec_embedding,
            0.3,
        )
        # helper = tf.contrib.seq2seq.TrainingHelper(
        #     self.dec_embedding,
        #     out_seq_len,
        # )
    projection_layer = layers_core.Dense(self.tgt_vocab_size, use_bias=False)
    decoder = tf.contrib.seq2seq.BasicDecoder(
        decoder_cell,
        helper,
        encoder_state,
        output_layer=projection_layer
    )
    return decoder

def _predict_decoder(self, cell, encoder_state, beam_width, length_penalty_weight):
    tiled_encoder_state = tf.contrib.seq2seq.tile_batch(
        encoder_state, multiplier=beam_width
    )
    with tf.name_scope('sentence_markers'):
        sos_id = tf.constant(1, dtype=tf.int32)
        eos_id = tf.constant(2, dtype=tf.int32)
    start_tokens = tf.fill([self.batch_size], sos_id)
    end_token = eos_id
    projection_layer = layers_core.Dense(self.tgt_vocab_size, use_bias=False)
    emb = tf.squeeze(self.dec_embedding)
    decoder = tf.contrib.seq2seq.BeamSearchDecoder(
        cell=cell,
        embedding=self.dec_embedding,
        start_tokens=start_tokens,
        end_token=end_token,
        initial_state=tiled_encoder_state,
        beam_width=beam_width,
        output_layer=projection_layer,
        length_penalty_weight=length_penalty_weight
    )
    return decoder

def decode(
    self, num_units, out_seq_len,
    encoder_state, cell=None, helper=None,
    beam_width=None, length_penalty_weight=None
):
    with tf.name_scope('Decode'):
        if cell:
            decoder_cell = cell
        else:
            decoder_cell = tf.nn.rnn_cell.BasicLSTMCell(2*num_units)
        if self.mode != estimator.ModeKeys.PREDICT:
            decoder = self._train_decoder(decoder_cell, out_seq_len, encoder_state, helper)
        else:
            decoder = self._predict_decoder(decoder_cell, encoder_state, beam_width, length_penalty_weight)
        outputs = tf.contrib.seq2seq.dynamic_decode(
            decoder,
            maximum_iterations=20,
            swap_memory=True,
        )
        outputs = outputs[0]
        if self.mode != estimator.ModeKeys.PREDICT:
            return outputs.rnn_output, outputs.sample_id
        else:
            return outputs.beam_search_decoder_output, outputs.predicted_ids

def prepare_predict(self, sample_id):
    rev_table = lookup_ops.index_to_string_table_from_file(
        self.vocab_path, default_value=UNK)
    predictions = rev_table.lookup(tf.to_int64(sample_id))
    return tf.estimator.EstimatorSpec(
        predictions=predictions,
        mode=tf.estimator.ModeKeys.PREDICT
    )

def prepare_train_eval(
    self, t_out,
    out_seq_len, labels, lr,
    train_op=None, loss=None
):
    if not loss:
        weights = tf.sequence_mask(
            out_seq_len,
            dtype=t_out.dtype
        )
        loss = tf.contrib.seq2seq.sequence_loss(
            t_out,
            labels,
            weights,
            average_across_batch=self.average_across_batch,
        )

    if not train_op:
        train_op = tf.contrib.layers.optimize_loss(
            loss,
            tf.train.get_global_step(),
            optimizer='SGD',
            learning_rate=lr,
            summaries=['loss', 'learning_rate']
        )

    return tf.estimator.EstimatorSpec(
        mode=self.mode,
        loss=loss,
        train_op=train_op,
    )
</code></pre>
"
"46792667","Convert a set of tuples into values","2017-10-17 14:31:18","2","261","4","2","","46792893","<p>I'm working on an nlp project in which I need to parse tags. I have multiple tags in the following form: a string that is a set of tuples. Example:</p>

<pre><code>'{(Entertainment (Adult), S), (Performing Arts, S), (Comedy Club, S), ($, S), (Comedy, P), (18+, S), (Plays &amp; Shows, P)}'
</code></pre>

<p>But I want it to look like this:</p>

<pre><code>{('Entertainment (Adult)', 'S'), ('Performing Arts', 'S'), ('Comedy Club', 'S'), ('$', 'S'), ('Comedy', 'P'), ('18+', 'S'), ('Plays &amp; Shows', 'P')}
</code></pre>

<p>I tried using literal_eval per <a href=""https://stackoverflow.com/questions/7935680/converting-a-string-of-tuples-to-a-list-of-tuples-in-python"">this question</a>, but I get an invalid syntax error. I think this is because the tag is a set, which contains tuples, which contain strings that are not cast as strings, so the literal_eval gets confused (just guessing here).</p>

<p>I tried doing some bandaid-y string strips and splits, but I can't get a solution that will work dynamically for different tags.</p>
"
"46779116","NLTK-based stemming and lemmatization","2017-10-16 21:07:19","1","2016","2","3","","46779568","<p>I am trying to preprocess a string using <code>lemmatizer</code> and then remove the punctuation and digits. I am using the code below to do this. I am not getting any error but the text is not preprocessed appropriately. Only the stop words are removed but the lemmatizing does not work and punctuation and digits also remain.</p>

<pre><code>from nltk.stem import WordNetLemmatizer
import string
import nltk
tweets = ""This is a beautiful day16~. I am; working on an exercise45.^^^45 text34.""
lemmatizer = WordNetLemmatizer()
tweets = lemmatizer.lemmatize(tweets)
data=[]
stop_words = set(nltk.corpus.stopwords.words('english'))
words = nltk.word_tokenize(tweets)
words = [i for i in words if i not in stop_words]
data.append(' '.join(words))
corpus = "" "".join(str(x) for x in data)
p = string.punctuation
d = string.digits
table = str.maketrans(p, len(p) * "" "")
corpus.translate(table)
table = str.maketrans(d, len(d) * "" "")
corpus.translate(table)
print(corpus)
</code></pre>

<p>The final output I get is:</p>

<pre><code>This beautiful day16~ . I ; working exercise45.^^^45 text34 .
</code></pre>

<p>And expected output should look like:</p>

<pre><code>This beautiful day I work exercise text
</code></pre>
"
"46745871","Interrupting lengthy pure computation in MonadState","2017-10-14 14:54:52","6","110","0","1","","46746232","<p>I can't grasp the correct way of interrupting lengthy pure computation on SIGINT signal. </p>

<p>In the simple example below, I have <code>slowFib</code> function that simulates lengthy computation. When it is run just in <code>IO</code> monad I can terminate it with C-c (using async to spawn worker). </p>

<p>However, when I put computation inside <code>MonadState, MonadIO</code> stack it no longer work... On the other hand, simple <code>threadDelay</code> in the same stack still can be terminated.</p>

<p>The code is following: </p>

<pre><code>{-# LANGUAGE FlexibleContexts #-}
module Main where

import Data.Monoid

import Control.DeepSeq
import Control.Concurrent
import Control.Concurrent.Async

import Control.Monad.State
-- import Control.Monad.State.Strict

import System.Posix.Signals

slowFib :: Integer -&gt; Integer
slowFib 0 = 0
slowFib 1 = 1
slowFib n = slowFib (n - 2 ) + slowFib (n - 1)

data St = St { x :: Integer } deriving (Show)

stateFib :: (MonadState St m, MonadIO m) =&gt; Integer -&gt; m Integer
stateFib n = do
  let f = slowFib n
  modify $ \st -&gt; st{x=f}
  return f

stateWait :: (MonadState St m, MonadIO m) =&gt; Integer -&gt; m Integer
stateWait n = do
  liftIO $ threadDelay 5000000
  return 41

interruptable n act = do
  putStrLn $ ""STARTING EVALUATION: "" &lt;&gt; n
  e &lt;- async act
  installHandler sigINT (Catch (cancel e)) Nothing
  putStrLn ""WAITING FOR RESULT""
  waitCatch e

main = do
  let s0 = St 0

  r &lt;- interruptable ""slowFib"" $ do
    let f = slowFib 41
    f `deepseq` return ()
    return f

  r &lt;- interruptable ""threadDelay in StateT"" $ runStateT (stateWait 41) s0
  putStrLn $ show r

  r &lt;- interruptable ""slowFib in StateT"" $ runStateT (stateFib 41) s0
  putStrLn $ show r
</code></pre>

<p>I suspected that it has something to do with lazy evaluation. I already figured out that in the first example (with just the <code>IO</code> monad) I have to force the result. Otherwise async computation just returns a thunk.</p>

<p>However all my attempts to do something analogous in MonadState failed. Anyway, it seems to be more complicated, since async thread does not return immediately. It waits until the result is computed. For some reason I just cannot terminate it when the pure computation is ""blocking"".</p>

<p>Any clues?</p>

<p>PS. My use case is too add ability to abort computation in custom Jupyter kernel made using <a href=""https://hackage.haskell.org/package/jupyter-0.9.0"" rel=""noreferrer"">jupyter</a> package. Functions evaluating user input are exactly in <code>MonadState</code> and <code>MonadIO</code>.</p>
"
"46713629","Evaluating POS tagger in NLTK","2017-10-12 15:36:44","3","5559","2","2","","46729923","<p>I want to evaluate different POS tags in NLTK using a text file as an input. </p>

<p>For an example, I will take Unigram tagger. I have found how to evaluate Unigram tag using brown corpus.</p>

<pre><code>from nltk.corpus import brown
import nltk

brown_tagged_sents = brown.tagged_sents(categories='news')
brown_sents = brown.sents(categories='news')
# We train a UnigramTagger by specifying tagged sentence data as a parameter
# when we initialize the tagger.
unigram_tagger = nltk.UnigramTagger(brown_tagged_sents)
print(unigram_tagger.tag(brown_sents[2007]))
print(unigram_tagger.evaluate(brown_tagged_sents))
</code></pre>

<p>It produces an output like below. </p>

<pre><code>[('Various', 'JJ'), ('of', 'IN'), ('the', 'AT'), ('apartments', 'NNS'), ('are', 'BER'), ('of', 'IN'), ('the', 'AT'), ('terrace', 'NN'), ('type', 'NN'), (',', ','), ('being', 'BEG'), ('on', 'IN'), ('the', 'AT'), ('ground', 'NN'), ('floor', 'NN'), ('so', 'QL'), ('that', 'CS'), ('entrance', 'NN'), ('is', 'BEZ'), ('direct', 'JJ'), ('.', '.')]
0.9349006503968017
</code></pre>

<p>In a similar manner, I want to read text from a text file and evaluate the accuracy of different POS taggers. </p>

<p>I figured out how to read a text file and how to apply pos tags for the tokens. </p>

<pre><code>import nltk
from nltk.corpus import brown
from nltk.corpus import state_union

brown_tagged_sents = brown.tagged_sents(categories='news')

sample_text = state_union.raw(
    r""C:\pythonprojects\tagger_nlt\new-testing.txt"")
tokens = nltk.word_tokenize(sample_text)

default_tagger = nltk.UnigramTagger(brown_tagged_sents)

default_tagger.tag(tokens)

print(default_tagger.tag(tokens))
[('Honestly', None), ('last', 'AP'), ('seven', 'CD'), ('lectures', None), ('are', 'BER'), ('good', 'JJ'), ('.', '.'), ('Lectures', None), ('are', 'BER'), ('understandable', 'JJ')
</code></pre>

<p>What I wanted to have is a score like <strong>default_tagger.evaluate()</strong>, so that I can compare different POS taggers in NLTK using the same input file to identify the most suited POS tagger for a given file. </p>

<p>Any help will be appreciated. </p>
"
"46687065","Can PostgreSQL's to_tsvector function return tokens/words and not lexemes?","2017-10-11 11:26:41","4","1833","0","1","","46688653","<p>PostgreSQL's <code>to_tsvector</code> function is extremely useful but in regards to my data set it does a little more than I want it to.</p>

<p>For instance:</p>

<pre><code>select * 
from to_tsvector('english', 'This is my favourite game. I enjoy everything about it.');
</code></pre>

<p>produces: <code>'enjoy':7 'everyth':8 'favourit':4 'game':5</code></p>

<p>I am not fussed about stop-words getting filtered out, that is fine. But some words get completely ruined, like <code>everything</code> and <code>favourite</code>.</p>

<p>Is there a way to modify this behaviour or is there a different function that does this?</p>

<p>PS: Yes, I can write my own query that does this (and I have) but I want a faster method.</p>
"
"46637044","Combining nltk.RegexpParser grammars","2017-10-08 23:26:18","2","873","3","1","","46786449","<p>As my next step towards learning more about NLP, I'm trying to implement a simple heuristic that improves results beyond simple n-grams. </p>

<p>Per the Stanford Collocations PDF linked below they mention that passing ""candidate phrases through a part of-speech filter which only lets through those patterns that are likely to be “phrases"""" will produce better results than simply using the most frequently occuring bi-grams. 
Source: Collocations, page 143 - 144: <a href=""https://nlp.stanford.edu/fsnlp/promo/colloc.pdf"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/fsnlp/promo/colloc.pdf</a> </p>

<p>The table on page 144 has 7 tag patterns. In order, the NLTK POS tag equivalent is:</p>

<p>JJ NN</p>

<p>NN</p>

<p>JJ JJ NN</p>

<p>JJ NN NN</p>

<p>NN JJ NN</p>

<p>NN NN NN</p>

<p>NN IN NN</p>

<p>In the code below, I can get the desired result when I <em>independently</em> apply each grammar below. However when I try to combine the same grammars I don't receive the desired result. </p>

<p>In my code, you can see that I uncomment one sentence, uncomment 1 grammar, run it and check the result. </p>

<p>I should be able to combine all sentences, run it through the combined grammar (just 3 of them in the code below) and get the desired results.</p>

<blockquote>
  <p>My question is, how do I correctly combine grammars?</p>
</blockquote>

<p>I'm assuming that combining grammars is like an 'OR', find this pattern, OR this pattern... </p>

<p>Thanks in advance. </p>

<pre><code>import nltk

# The following sentences are correctly grouped with &lt;JJ&gt;*&lt;NN&gt;+. 
# Should see: 'linear function', 'regression coefficient', 'Gaussian random variable' and 
# 'cumulative distribution function'
SampleSentence = ""In mathematics, the term linear function refers to two distinct, although related, notions""
#SampleSentence = ""The regression coefficient is the slope of the line of the regression equation.""
#SampleSentence = ""In probability theory, Gaussian random variable is a very common continuous probability distribution.""
#SampleSentence = ""In probability theory and statistics, the cumulative distribution function (CDF) of a real-valued random variable X, or just distribution function of X, evaluated at x, is the probability that X will take a value less than or equal to x.""

# The following sentences are correctly grouped with &lt;NN.?&gt;*&lt;V.*&gt;*&lt;NN&gt;
# Should see 'mean squared error' and # 'class probability function'. 
#SampleSentence = ""In statistics, the mean squared error (MSE) of an estimator measures the average of the squares of the errors, that is, the difference between the estimator and what is estimated.""
#SampleSentence = ""The class probability function is interesting""

# The sentence below is correctly grouped with &lt;NN.?&gt;*&lt;IN&gt;*&lt;NN.?&gt;*. 
# should see 'degrees of freedom'.
#SampleSentence = ""In statistics, the degrees of freedom is the number of values in the final calculation of a statistic that are free to vary.""

SampleSentence = SampleSentence.lower()

print(""\nFull sentence: "", SampleSentence, ""\n"")

tokens = nltk.word_tokenize(SampleSentence)
textTokens = nltk.Text(tokens)    

# Determine the POS tags.
POStagList = nltk.pos_tag(textTokens)    

# The following grammars work well *independently*
grammar = ""NP: {&lt;JJ&gt;*&lt;NN&gt;+}""
#grammar = ""NP: {&lt;NN.?&gt;*&lt;V.*&gt;*&lt;NN&gt;}""    
#grammar = ""NP: {&lt;NN.?&gt;*&lt;IN&gt;*&lt;NN.?&gt;*}""


# Merge several grammars above into a single one below. 
# Note that all 3 correct grammars above are included below. 

'''
grammar = """"""
            NP: 
                {&lt;JJ&gt;*&lt;NN&gt;+}
                {&lt;NN.?&gt;*&lt;V.*&gt;*&lt;NN&gt;}
                {&lt;NN.?&gt;*&lt;IN&gt;*&lt;NN.?&gt;*}
        """"""
'''

cp = nltk.RegexpParser(grammar)

result = cp.parse(POStagList)

for subtree in result.subtrees(filter=lambda t: t.label() == 'NP'):
    print(""NP Subtree:"", subtree)    
</code></pre>
"
"46612949","Can we build word2vec model in a distributed way?","2017-10-06 19:45:38","3","993","2","2","","46709591","<p>Currently I have 1.2tb text data to build gensim's word2vec model. It is almost taking 15 to 20 days to complete. </p>

<p>I want to build model for 5tb of text data, then it might take few months to create model. I need to minimise this execution time. Is there any way we can use multiple big systems to create model? </p>

<p>Please suggest any way which can help me in reducing the execution time.</p>

<p>FYI, I have all my data in S3 and I use smart_open module to stream the data.</p>
"
"46580932","Calculate TF-IDF using sklearn for n-grams in python","2017-10-05 08:18:46","10","24346","1","2","","46591832","<p>I have a vocabulary list that include n-grams as follows. </p>

<pre><code>myvocabulary = ['tim tam', 'jam', 'fresh milk', 'chocolates', 'biscuit pudding']
</code></pre>

<p>I want to use these words to calculate TF-IDF values.</p>

<p>I also have a dictionary of corpus as follows (key = recipe number, value = recipe).</p>

<pre><code>corpus = {1: ""making chocolates biscuit pudding easy first get your favourite biscuit chocolates"", 2: ""tim tam drink new recipe that yummy and tasty more thicker than typical milkshake that uses normal chocolates"", 3: ""making chocolates drink different way using fresh milk egg""}
</code></pre>

<p>I am currently using the following code.</p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer(vocabulary = myvocabulary, stop_words = 'english')
tfs = tfidf.fit_transform(corpus.values())
</code></pre>

<p>Now I am printing tokens or n-grams of the recipe 1 in <code>corpus</code> along with the tF-IDF value as follows.</p>

<pre><code>feature_names = tfidf.get_feature_names()
doc = 0
feature_index = tfs[doc,:].nonzero()[1]
tfidf_scores = zip(feature_index, [tfs[doc, x] for x in feature_index])
for w, s in [(feature_names[i], s) for (i, s) in tfidf_scores]:
  print(w, s)
</code></pre>

<p>The results I get is <code>chocolates 1.0</code>. However, my code does not detect n-grams (bigrams) such as <code>biscuit pudding</code> when calculating TF-IDF values. Please let me know where I make the code wrong.</p>

<p>I want to get the TD-IDF matrix for <code>myvocabulary</code> terms by using the recipe documents in the <code>corpus</code>. In other words, the rows of the matrix represents <code>myvocabulary</code> and the columns of the matrix represents the recipe documents of my <code>corpus</code>. Please help me.</p>
"
"46519084","NLTK pos_tag module returns LookupError","2017-10-02 03:24:51","4","3574","0","1","","46520077","<p><a href=""https://i.sstatic.net/Axcln.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Axcln.jpg"" alt=""enter image description here""></a></p>

<p><a href=""https://i.sstatic.net/Ux45K.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Ux45K.jpg"" alt=""enter image description here""></a></p>

<p>The details are on the above.
I run it on Jupiter notebook, and get the error message.</p>
"
"46449047","How to replace tokens (words) with stemmed versions of words from my own table?","2017-09-27 13:23:25","0","553","0","1","","46454392","<p>I got data like this (simplified):</p>

<pre><code>library(quanteda)
</code></pre>

<p>sample data</p>

<pre><code>myText &lt;- c(""ala ma kotka"", ""kasia ma pieska"")  
myDF &lt;- data.frame(myText)
myDF$myText &lt;- as.character(myDF$myText)
</code></pre>

<p>tokenization</p>

<pre><code>tokens &lt;- tokens(myDF$myText, what = ""word"",  
             remove_numbers = TRUE, remove_punct = TRUE,
             remove_symbols = TRUE, remove_hyphens = TRUE)
</code></pre>

<p>stemming with my own data
sample dictionary</p>

<pre><code>Origin &lt;- c(""kot"", ""pies"")
Word &lt;- c(""kotek"",""piesek"")

myDict &lt;- data.frame(Origin, Word)

myDict$Origin &lt;- as.character(myDict$Origin)
myDict$Word &lt;- as.character(myDict$Word)
</code></pre>

<p>what i got</p>

<pre><code>tokens[1]
[1] ""Ala""   ""ma""    ""kotka""
</code></pre>

<p>what i would like to get</p>

<pre><code>tokens[1]
[1] ""Ala""   ""ma""    ""kot""
tokens[2]
[1] ""Kasia""   ""ma""    ""pies""
</code></pre>
"
"46444656","BLEU scores：could I use nltk.translate.bleu_score.sentence_bleu for calculating scores of bleu in chinese","2017-09-27 09:46:23","5","11654","1","1","","46445752","<p>If I have chinese word list: like reference = ['我'， '是', '好' ,'人']， hypothesis = ['我', '是', '善良的'，'人] . Could I use the: nltk.translate.bleu_score.sentence_bleu(references, hypothesis) for chinese translation? it is the same as English? How about Japanese? 
I mean If I have word list(chinese and japanese ) like english. Thanks! </p>
"
"46368720","TFIDIF Model Creation TypeError in Gensim","2017-09-22 15:55:37","1","1260","0","2","","46387065","<p>TypeError: 'TfidfModel' object is not callable</p>

<p><strong>Why can I not compute the TFIDF Matrix for each Doc after initializing?</strong></p>

<p>I started with 999 <em>documents</em>: 999 paragraphs with about 5-15 sentences each.
After spaCy tokenizing everything, I created the <em>dictionary</em> (~16k unique tokens) and <em>corpus</em> (a list of lists of tuples)</p>

<p>Now I'm ready to create the tfidf matrix (and later LDA and w2V matricies) for some ML; however, after initializing the tfidf model with my corpus (for calculation of the 'IDF')
<code>tfidf = models.TfidfModel(corpus)</code> I get the following error message when trying to see the tfidf of each doc <code>tfidf(corpus[5])</code>
<strong>TypeError: 'TfidfModel' object is not callable</strong></p>

<p>I am able to create this model using a differnt corpus where i have four docs each comprised of only a sentence.
There I can confirm that the expected corpus fomat is a list of lists of tuples: 
[doc1[(word1, count),(word2, count),...], doc2[(word3, count),(word4,count),...]...]</p>

<pre><code>from gensim import corpora, models, similarities

texts = [['teenager', 'martha', 'moxley'...], ['ok','like','kris','usual',...]...]
dictionary = corpora.Dictionary(texts)
&gt;&gt;&gt; Dictionary(15937 unique tokens: ['teenager', 'martha', 'moxley']...)

corpus = [dictionary.doc2bow(text) for text in texts]
&gt;&gt;&gt; [[(0, 2),(1, 2),(2, 1)...],[(3, 1),(4, 1)...]...]

tfidf = models.TfidfModel(corpus)
&gt;&gt;&gt; TfidfModel(num_docs=999, num_nnz=86642)

tfidf(corpus[0])
&gt;&gt;&gt; TypeError: 'TfidfModel' object is not callable

corpus[0]
&gt;&gt;&gt; [(0, 2),(1, 2),(2, 1)...]

print(type(corpus),type(corpus[1]),type(corpus[1][3]))
&gt;&gt;&gt; &lt;class 'list'&gt; &lt;class 'list'&gt; &lt;class 'tuple'&gt;
</code></pre>
"
"46214001","How to write nltk grammar to check but not capture some text","2017-09-14 08:21:39","2","114","0","1","","47219540","<p>I have 2 sentences:</p>

<pre><code>procedure, when performed, some other text
procedure, limited, some other text
</code></pre>

<p>I want to select VBN with comma after it:</p>

<pre><code>import nltk

sents = [
    ['procedure', ',', 'when', 'performed', ',', 'some', 'other', 'text'],
    ['procedure', ',', 'limited', ',', 'some', 'other', 'text']
]
tokens = [nltk.pos_tag(x) for x in sents]

grammar = r""""""
  CHUNK: {&lt;VBN&gt;&lt;,&gt;}
""""""
chunker = nltk.RegexpParser(grammar)

for x in tokens:
    tree = chunker.parse(x)
    print tree
</code></pre>

<p>It works:</p>

<pre><code>(S procedure/NN ,/, when/WRB (CHUNK performed/VBN ,/,) some/DT other/JJ text/NN)
(S procedure/NN ,/, (CHUNK limited/VBN ,/,) some/DT other/JJ text/NN)
</code></pre>

<p>But I need to select VBN when it <strong>wrapped</strong> by commas. Some kind of <code>re.compile(r'(?:,)\s*([a-z]+ed),')</code></p>

<p>There is any way to use <code>(?:...)</code> in RegexpParser grammar?</p>
"
"46186238","Generate parser that runs a received parser on the output of another parser and monadically joins the results","2017-09-12 22:31:18","3","273","0","2","","46205866","<p>given the following type and function, meant to parse a field of a CSV field into a string:</p>

<pre><code>type Parser resultType = ParsecT String () Identity resultType
cell :: Parser String 
</code></pre>

<p>I have implemented the following function:</p>

<pre><code>customCell :: String -&gt; Parser res  -&gt; Parser res
customCell typeName subparser = 
  cell
    &gt;&gt;= either (const $ unexpected typeName) 
               return . parse (subparser &lt;* eof) """"
</code></pre>

<p>Though I cannot stop thinking that I am not using the Monad concept as much as desired and that eventually there is a better way to merge the result of the inner with the outer parser, specially on what regards its failure.</p>

<p>Does anybody know how could I do so, or is this code what is meant to be done?</p>

<p>PS - I now realised that my type simplification is probably not appropriate and that maybe what I want is to replace the underlying Identity Monad by the Either Monad.... Unfortunately, I do not feel enough acquainted with monad transformers yet.</p>

<p>PS2 - What the hell is the underlying monad good for anyway? </p>
"
"46084574","What is the difference between mteval-v13a.pl and NLTK BLEU?","2017-09-06 21:26:15","8","1603","1","1","","46089409","<p>There is an implementation of BLEU score in Python NLTK,
 <a href=""https://github.com/nltk/nltk/blob/develop/nltk/translate/bleu_score.py"" rel=""noreferrer""><code>nltk.translate.bleu_score.corpus_bleu</code></a> </p>

<p>But I am not sure if it is the same as the <a href=""https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/mteval-v13a.pl"" rel=""noreferrer"">mtevalv13a.pl script</a>.</p>

<p><strong>What is the difference between them?</strong></p>
"
"46059280","How does a Transition-based Dependency parser decide which operation to do next in its configuration stage?","2017-09-05 16:07:17","2","138","0","2","","59854745","<p>I understand that the model uses previously trained Part of Speech tagging during its configuration stage. But what if most of the words are new, how would the parser decide its operation then? </p>
"
"45981339","Text file parsing with python and with a list in grammar","2017-08-31 12:21:03","1","942","3","1","","45995912","<p>I have to do a parsing: the goal is to create a grammar rules that will be applied in a corpus. I have a question: is it possible to have a list within a grammar?</p>

<p>Example:</p>

<pre><code>1) Open the text to be analyzed
2) Write the grammatical rules (just an example):
   grammar(""""""
   S -&gt; NP VP
   NP -&gt; DET N
   VP -&gt; V N
   DET -&gt; list_det.txt
   N -&gt; list_n.txt
   V -&gt; list.txt"""""")
3) Print the result with the entries that obey this grammar
</code></pre>

<p>It's possible?</p>
"
"45951825","Extracting n-th element from lists of a list","2017-08-30 04:16:28","1","241","0","2","","45951891","<p>I have the following output that I got by using nltk .tokenize(), .pos_tag(), and wordnet .synsets(). The output is a list of lists of potential matches for each token of document and wordnet's own part-of-speech tagging (here we have 4 tokens, hence, 4 lists of matches):</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-css lang-css prettyprint-override""><code>[[Synset('document.n.01'),
  Synset('document.n.02'),
  Synset('document.n.03'),
  Synset('text_file.n.01'),
  Synset('document.v.01'),
  Synset('document.v.02')],
 [Synset('be.v.01'),
  Synset('be.v.02'),
  Synset('be.v.03'),
  Synset('exist.v.01'),
  Synset('be.v.05'),
  Synset('equal.v.01'),
  Synset('constitute.v.01'),
  Synset('be.v.08'),
  Synset('embody.v.02'),
  Synset('be.v.10'),
  Synset('be.v.11'),
  Synset('be.v.12'),
  Synset('cost.v.01')],
 [Synset('angstrom.n.01'),
  Synset('vitamin_a.n.01'),
  Synset('deoxyadenosine_monophosphate.n.01'),
  Synset('adenine.n.01'),
  Synset('ampere.n.02'),
  Synset('a.n.06'),
  Synset('a.n.07')],
 [Synset('trial.n.02'),
  Synset('test.n.02'),
  Synset('examination.n.02'),
  Synset('test.n.04'),
  Synset('test.n.05'),
  Synset('test.n.06'),
  Synset('test.v.01'),
  Synset('screen.v.01'),
  Synset('quiz.v.01'),
  Synset('test.v.04'),
  Synset('test.v.05'),
  Synset('test.v.06'),
  Synset('test.v.07')]]</code></pre>
</div>
</div>
</p>

<p>If I want to write a function (a loop, possibly) to extract only the first match for each token and generate the output as a new list, such as the following (using the example above):</p>

<pre><code>[Synset('document.n.01'), Synset('be.v.01'), Synset('angstrom.n.01'), Synset('trial.n.02')]
</code></pre>

<p>What's the most flexible way to write such a function? So that it can be extended to other tokenized documents (with pos tagging)?</p>

<p>Thank you.</p>
"
"45932370","Basic and enhanced dependencies give different results in Stanford coreNLP","2017-08-29 06:41:34","0","697","0","1","","45971705","<p>I am using dependency parsing of coreNLP for a project of mine. The basic and enhanced dependencies are different result for a particular dependency.
I used the following code to get enhanced dependencies.</p>

<pre><code>val lp = LexicalizedParser.loadModel(""edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz"")
lp.setOptionFlags(""-maxLength"", ""80"")
val rawWords = edu.stanford.nlp.ling.Sentence.toCoreLabelList(tokens_arr:_*)
val parse = lp.apply(rawWords)
val tlp = new PennTreebankLanguagePack()
val gsf:GrammaticalStructureFactory = tlp.grammaticalStructureFactory()
val gs:GrammaticalStructure = gsf.newGrammaticalStructure(parse)
val tdl = gs.typedDependenciesCCprocessed()
</code></pre>

<p>For the following example, </p>

<pre><code>Account name of ramkumar.
</code></pre>

<p>I use simple API to get basic dependencies. The dependency i get between
(account,name) is (compound). But when i use the above code to get enhanced dependency i get the relation between (account,name) as (dobj).</p>

<p>What is the fix to this? Is this a bug or am i doing something wrong? </p>
"
"45919639","Improving the performance of text cleanup on a dataframe","2017-08-28 12:57:23","0","675","15","1","","45920982","<p>I have a df:</p>

<pre><code>id    text
1     This is a good sentence
2     This is a sentence with a number: 2015
3     This is a third sentence
</code></pre>

<p>I have a text cleaning function:</p>

<pre><code>def clean(text):
    lettersOnly = re.sub('[^a-zA-Z]',' ', text)
    tokens = word_tokenize(lettersOnly.lower())
    stops = set(stopwords.words('english'))
    tokens = [w for w in tokens if not w in stops]
    tokensPOS = pos_tag(tokens)
    tokensLemmatized = []
    for w in tokensPOS:
        tokensLemmatized.append(WordNetLemmatizer().lemmatize(w[0], get_wordnet_pos(w[1])))
    clean = "" "".join(tokensLemmatized)
    return clean
</code></pre>

<p><code>get_wordnet_pos()</code> is this:</p>

<pre><code>def get_wordnet_pos(treebank_tag):
    if treebank_tag.startswith('J'):
        return wordnet.ADJ
    elif treebank_tag.startswith('V'):
        return wordnet.VERB
    elif treebank_tag.startswith('N'):
        return wordnet.NOUN
    elif treebank_tag.startswith('R'):
        return wordnet.ADV
    else:
        return wordnet.NOUN
</code></pre>

<p>I am applying <code>extractFeatures()</code> to a pandas column and creating a new column with the results:</p>

<pre><code>df['cleanText'] = df['text'].apply(clean)
</code></pre>

<p>Resulting df:</p>

<pre><code>id    cleanText
1     good sentence
2     sentence number
3     third sentence
</code></pre>

<p>The loop time appears to grow exponentially. For example, using <code>%%timeit</code>, applying it to five rows runs at 17 ms per loop. 300 rows runs at 800 ms per loop. 500 rows runs at 1.26 s per loop.</p>

<p>I altered it by instantiating <code>stops</code> and <code>WordNetLemmatizer()</code> outside of the function since those only need to be called once. </p>

<pre><code>stops = set(stopwords.words('english'))
lem = WordNetLemmatizer()
def clean(text):
    lettersOnly = re.sub('[^a-zA-Z]',' ', text)
    tokens = word_tokenize(lettersOnly.lower())
    tokens = [w for w in tokens if not w in stops]
    tokensPOS = pos_tag(tokens)
    tokensLemmatized = []
    for w in tokensPOS:
        tokensLemmatized.append(lem.lemmatize(w[0], get_wordnet_pos(w[1])))
    clean = "" "".join(tokensLemmatized)
    return clean
</code></pre>

<p>Running <code>%prun -l 10</code> on the <code>apply</code> line resulted in this table:</p>

<pre><code>         672542 function calls (672538 primitive calls) in 2.798 seconds

   Ordered by: internal time
   List reduced from 211 to 10 due to restriction &lt;10&gt;

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
     4097    0.727    0.000    0.942    0.000 perceptron.py:48(predict)
     4500    0.584    0.000    0.584    0.000 {built-in method nt.stat}
     3500    0.243    0.000    0.243    0.000 {built-in method nt._isdir}
    14971    0.157    0.000    0.178    0.000 {method 'sub' of '_sre.SRE_Pattern' objects}
    57358    0.129    0.000    0.155    0.000 perceptron.py:250(add)
     4105    0.117    0.000    0.201    0.000 {built-in method builtins.max}
   184365    0.084    0.000    0.084    0.000 perceptron.py:58(&lt;lambda&gt;)
     4097    0.057    0.000    0.213    0.000 perceptron.py:245(_get_features)
      500    0.038    0.000    1.220    0.002 perceptron.py:143(tag)
     2000    0.034    0.000    0.068    0.000 ntpath.py:471(normpath)
</code></pre>

<p>It looks like the perceptron tagger is, predictably, taking a lot of resources, but I'm not sure how to streamline it. In addition, I'm not sure where <code>nt.stat</code> or <code>nt._isdir</code> is being called. </p>

<p>How should I alter the function or apply method to increase performance? Is this function a candidate for Cython or Numba?</p>
"
"45832040","How can I remove all POS tags except for 'VBD' and 'VBN' from my CSV file?","2017-08-23 06:22:01","0","479","3","1","","45870635","<p>I want to remove words tagged with the specific part-of-speech tags <code>VBD</code> and <code>VBN</code> from my CSV file. But, I'm getting the error <em>""IndexError: list index out of range""</em> after entering the following code:</p>

<pre><code>for word in POS_tag_text_clean:
    if word[1] !='VBD' and word[1] !='VBN':
        words.append(word[0])
</code></pre>

<p>My CSV file has 10 reviews of 10 people and the row name is <code>Comment</code>.</p>

<p>Here is my full code:</p>

<pre><code>df_Comment = pd.read_csv(""myfile.csv"")

def clean(text):
    stop = set(stopwords.words('english'))
    exclude = set(string.punctuation)
    lemma = WordNetLemmatizer()
    tagged = nltk.pos_tag(text)

    text = text.rstrip()
    text = re.sub(r'[^a-zA-Z]', ' ', text)
    stop_free = "" "".join([i for i in text.lower().split() if((i not in stop) and (not i.isdigit()))])
    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)
    normalized = "" "".join(lemma.lemmatize(word) for word in punc_free.split())
    return normalized

text_clean = []
for text in df)Comment['Comment']:
    text_clean.append(clean(text).split())
print(text_clean) 

POS_tag_text_clean = [nltk.pos_tag(t) for t in text_clean]
print(POS_tag_text_clean)


words=[]
for word in POS_tag_text_clean:
    if word[1] !='VBD' and word[1] !='VBN':
       words.append(word[0])
</code></pre>

<p>How can I fix the error?</p>
"
"45823199","Python: is this an inefficient way to compare and sort lists of strings?","2017-08-22 16:53:24","1","632","7","1","","45823963","<p>I have two lists of strings, A and B. For each string in A, I'd like to compare it to every string in B and select the most similar match. The comparison function that I'm using is a custom cosine similarity measure that <a href=""https://stackoverflow.com/questions/8897593/similarity-between-two-text-documents"">I found on this question</a>. Here is how it works:</p>

<pre><code>import nltk, string
from sklearn.feature_extraction.text import TfidfVectorizer
nltk.download('punkt')

stemmer = nltk.stem.porter.PorterStemmer()
remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)

def stem_tokens(tokens):
    return [stemmer.stem(item) for item in tokens]

def normalize(text):
    return stem_tokens(nltk.word_tokenize(text.lower().translate(remove_punctuation_map)))

vectorizer = TfidfVectorizer(tokenizer=normalize, stop_words='english')

def cosine_sim(text1, text2):
    tfidf = vectorizer.fit_transform([text1, text2])
    return ((tfidf * tfidf.T).A)[0,1]
</code></pre>

<p>My issue is that if I have somewhat long lists (500-1000 items), and the execution starts to take five or ten minutes. Here's an example using some dummy text:</p>

<pre><code>import requests    
url = 'https://gist.githubusercontent.com/WalkerHarrison/940c005aa23386a69282f373f6160221/raw/6537d999b9e39d62df3784d2d847d4a6b2602876/sample.txt'
sample = requests.get(url).text
A, B = sample[:int(len(sample)/2)], sample[int(len(sample)/2):]
A, B = list(map(''.join, zip(*[iter(A)]*100))), list(map(''.join, zip(*[iter(B)]*100)))
</code></pre>

<p>Now that I have two lists, each with ~500 strings (of 100 characters each), I compute the similarities and take the top one. This is done by taking a string from A, iterating through B, sorting by cosine_sim score, and then taking the last element, and then repeating for all elements in A:</p>

<pre><code>matches = [(a, list(sorted([[b, cosine_sim(a, b)] 
                            for b in B], key=lambda x: x[1]))[-1])
           for a in A]
</code></pre>

<p>The output is a list of matches where each item contains both strings and also their calculated similarity score. That final line took 7 minutes to run though. I'm wondering if there are inefficiencies in my process that are slowing it down or if there's just a lot to compute (500*500 = 250,000 comparisons, plus sorting for the best 500 times)?</p>
"
"45780602","Why do we calculate cosine similarities using tf-idf weightings?","2017-08-20 09:31:34","0","2439","0","2","","45814875","<p>Suppose we are trying to measure similarity between two very similar documents.</p>

<pre><code>Document A: ""a b c d""
Document B: ""a b c e""
</code></pre>

<p>This corresponds to a term-frequency matrix </p>

<pre><code>  a b c d e
A 1 1 1 1 0
B 1 1 1 0 1
</code></pre>

<p>where the cosine similarity on the raw vectors is the dot product of the two vectors A and B, divided by the product of their magnitudes:</p>

<blockquote>
  <p>3/4 = (1*1 + 1*1 + 1*1 + 1*0 + 1*0) / (sqrt(4) * sqrt(4)).</p>
</blockquote>

<p>But when we apply an <a href=""https://en.wikipedia.org/wiki/SMART_Information_Retrieval_System"" rel=""nofollow noreferrer"">inverse document frequency</a> transformation by multiplying each term in the matrix by (log(N / df_i), where N is the number of documents in the matrix, 2, and df_i is the number of documents in which a term is present, we get a tf-idf matrix of</p>

<pre><code>   a b c d    e
A: 0 0 0 log2 0
B: 0 0 0 0    1og2
</code></pre>

<p>Since ""a"" appears in both documents, it has an inverse-document-frequency value of 0. This is the same for ""b"" and ""c"". Meanwhile, ""d"" is in document A, but not in document B, so it is multiplied by log(2/1). ""e"" is in document B, but not in document A, so it is also multiplied by log(2/1).</p>

<p>The cosine similarity between these two vectors is 0, suggesting the two are totally different documents. Obviously, this is incorrect. For these two documents to be considered similar to each other using tf-idf weightings, we would need a third document C in the matrix which is vastly different from documents A and B. </p>

<p>Thus, I am wondering whether and/or why we would use tf-idf weightings in combination with a cosine similarity metric to compare highly similar documents. None of the tutorials or StackOverflow questions I've read have been able to answer this question.</p>

<p><a href=""http://www.p-value.info/2013/02/when-tfidf-and-cosine-similarity-fail.html"" rel=""nofollow noreferrer"">This post</a> discusses similar failings with tf-idf weights using cosine similarities, but offers no guidance on what to do about them.</p>

<p>EDIT: as it turns out, the guidance I was looking for was in the comments of that blog post. It recommends using the formula</p>

<p>1 + log ( N / ni + 1)</p>

<p>as the inverse document frequency transformation instead. This would keep the weights of terms which are in every document close to their original weights, while inflating the weights of terms which are not present in a lot of documents by a greater degree. Interesting that this formula is not more prominently found in posts about tf-idf.</p>
"
"45763803","Python NLTK parsing error? 'str' object has no attribute 'check_coverage'","2017-08-18 19:28:04","3","2691","4","1","","45763880","<p>I am trying to use NLTK to determine if a sentence is valid.
i loaded the grammar, but whenever I try to get the parser it does not work and I get the error ""AttributeError: 'str' object has no attribute 'check_coverage'""
This is my code:</p>

<pre><code>sentence = ['show', 'me', 'northwest', 'flights', 'to', 'detroit', '.']
grammar = nltk.data.load('grammars/large_grammars/atis.cfg', 'text')
parser =  nltk.parse.BottomUpChartParser(grammar)
chart = parser.chart_parse(sentence)
</code></pre>

<p>Here is the full traceback:
    Traceback (most recent call last):</p>

<pre><code> File ""&lt;ipython-input-448-852d3bb24984&gt;"", line 1, in &lt;module&gt;
 chart = parser.chart_parse(sentence)

 File ""C:\Users\Class2016\Anaconda3\lib\site-packages\nltk\parse\chart.py"", 
 line 1310, in chart_parse
  self._grammar.check_coverage(tokens)

AttributeError: 'str' object has no attribute 'check_coverage'
</code></pre>

<p>I got this part of code from the example listed here under Unit tests for LARGE context-free grammars: <a href=""http://www.nltk.org/howto/parse.html"" rel=""nofollow noreferrer"">http://www.nltk.org/howto/parse.html</a></p>

<p>Any information on why this is happening or how to correct this would be greatly appreciated.</p>

<p>Thanks!</p>
"
"45696028","SnowballStemmer for Russian words list","2017-08-15 15:22:43","8","16925","2","1","","45696131","<p>I do know how to perform SnowballStemmer on a single word (in my case, on russian one). Doing the next things:</p>

<pre><code>from nltk.stem.snowball import SnowballStemmer 

stemmer = SnowballStemmer(""russian"") 
stemmer.stem(""Василий"")
'Васил'
</code></pre>

<p>How can I do the following if I have a list of words like ['Василий', 'Геннадий', 'Виталий']?</p>

<p>My approach using for loop seems to be not working :( </p>

<pre><code>l=[stemmer.stem(word) for word in l]
</code></pre>
"
"45690619","Vectorizer the combination of words in Python","2017-08-15 09:59:52","4","660","0","1","","45690666","<p>I have a dataset with medical text data and I apply tf-idf vectorizer on them and calculate tf idf score for the words just like this:</p>

<pre><code>import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer as tf

vect = tf(min_df=60,stop_words='english')

dtm = vect.fit_transform(df) 
l=vect.get_feature_names() 

x=pd.DataFrame(dtm.toarray(), columns=vect.get_feature_names())
</code></pre>

<p>So basically my question is following-while I'm applying TfidfVectorizer it splits the text in distinct words for example: ""pain"", ""headache"", ""nausea"" and so on. How can I get the words combination in the output of TfidfVectorizer for example: ""severe pain"", ""cluster headache"", ""nausea vomiting"". Thanks</p>
"
"45681070","Stemming words in a Python list","2017-08-14 18:47:35","0","1495","2","1","","45681220","<p>Have a list ""l"" with distinct words like this:</p>

<pre><code>'gone',
'done',
'crawled',
'laughed',
'cried'
</code></pre>

<p>I try to apply stemming on this list just that way:</p>

<pre><code>from stemming.porter2 import stem
l = [[stem(word) for word in sentence.split(' ')] for sentence in l]
</code></pre>

<p>But nothing seems to happen and nothing changes. What am I doing wrong with the stemming procedure?</p>
"
"45679331","Reading .eml files with Python 3.6 using emaildata 0.3.4","2017-08-14 16:52:34","10","16385","2","1","","45822930","<p>I am using python 3.6.1 and I want to read in email files (.eml) for processing. I am using the <a href=""https://pypi.python.org/pypi/emaildata/0.3.4"" rel=""noreferrer"">emaildata 0.3.4</a> package, however whenever I try to import the Text class as in the documentation, I get the module errors:</p>

<pre><code>import email
from email.text import Text
&gt;&gt;&gt; ModuleNotFoundError: No module named 'cStringIO'
</code></pre>

<p>When I tried to correct using <a href=""https://stackoverflow.com/questions/28200366/python-3-4-0-email-package-install-importerror-no-module-named-cstringio"">this update</a>, I get the next error relating to <code>mimetools</code></p>

<pre><code>&gt;&gt;&gt; ModuleNotFoundError: No module named 'mimetools'
</code></pre>

<p>Is it possible to use emaildata 0.3.4 with python 3.6 to parse .eml files? Or are there any other packages I can use to parse .eml files? Thanks</p>
"
"45677519","Python and nGrams","2017-08-14 15:03:46","0","380","4","2","","45677892","<p>Aster user here that is trying to move completely over to python for basic text analytics.
I am trying to replicate the output of ASTER ngram in Python using nltk or some other module. I need to be able to do this for ngrams of 1 thru 4. Output to csv.</p>

<p><strong>DATA:</strong></p>

<pre><code>Unique_ID, Text_Narrative
</code></pre>

<p><strong>OUTPUT NEEDED:</strong></p>

<pre><code>Unique_id, ngram(token), ngram(frequency)
</code></pre>

<p><strong>Example output:</strong></p>

<ul>
<li>023345  ""I""  1 </li>
<li>023345  ""Love""  1 </li>
<li>023345  ""Python""  1 </li>
</ul>
"
"45618562","Use sapply/lapply or foreach to access data attributes R","2017-08-10 16:05:02","1","220","4","2","","45619532","<p>this could be a very basic question but honestly, I tried a few solutions on those similar questions but was unable to drive success on my data. It could be because of my data or I am having a hard day and couldn't figure out anything. :(</p>

<p>I have a vector of sentences </p>

<pre><code>vec = c(""having many items"", ""have an apple"", ""item"")
</code></pre>

<p>Also, I have a data frame to lemmatize the data</p>

<pre><code>lem = data.frame(pattern = c(""(items)|(item)"", ""(has)|(have)|(having)|(had)""), replacement = c(""item"", ""have""))
lem$pattern = as.character(lem$pattern)
lem$replacement = as.character(lem$replacement)
</code></pre>

<p>I want to go through each row in the <code>lem</code> data frame to form a replacement command.</p>

<p>Option 1:</p>

<pre><code>library(stringr) #this is said to be quicker than gsub and my data has 3 mil sentences   
vec &lt;- sapply(lem, function(x) str_replace_all(vec, pattern=x$pattern, replacement = x$replacement))

Error in x$pattern : $ operator is invalid for atomic vectors 
</code></pre>

<p>Option 2:</p>

<pre><code>library(doPar)
vec &lt;- foreach(i = 1:nrow(lem)) %dopar% {
str_replace_all(vec, pattern = lem[i, ""pattern""], replacement = lem[i, ""replacement""])
}
</code></pre>

<p>Option 2 returns a list of 2 vectors: the first one is what I want, the second one is the original, which I don't know why. Also, I tested on my machine, <code>doPar</code> (though using parallel programming) is not as fast as <code>sapply</code>. </p>

<p>Since my data is quite big (<strong>3 mil sentences</strong>), could somebody recommend an effective method to lemmatize the text data?</p>
"
"45590278","How to inverse lemmatization process given a lemma and a token?","2017-08-09 12:08:21","8","3193","3","1","","45591295","<p>Generally, in natural language processing, we want to get the lemma of a token. </p>

<p>For example, we can map 'eaten' to 'eat' using wordnet lemmatization.</p>

<p><strong>Is there any tools in python that can inverse lemma to a certain form?</strong></p>

<p>For example, we map 'go' to 'gone' given target form 'eaten'.</p>

<p>PS: Someone mentions we have to store such mappings.
<a href=""https://stackoverflow.com/questions/30266502/how-to-un-stem-a-word-in-python"">How to un-stem a word in Python?</a></p>
"
"45572313","Identical Clusters after Text Clustering in Python","2017-08-08 15:24:53","1","143","1","1","","45574050","<p>I'm performing text clustering on a set of textdata in Python. Basically, I use tf idf score and then apply the result matrix into the kmeans algorithm just like that:</p>

<pre><code>vect = TfidfVectorizer(min_df=100,stop_words=sw) 

dtm = vect.fit_transform(df) 
l=vect.get_feature_names()

k = 15
model = MiniBatchKMeans(n_clusters=k)
model.fit(dtm)

order_centroids = model.cluster_centers_.argsort()[:, ::-1]
terms = vect.get_feature_names()
for i in range(k):
       print(""Cluster %d:"" % i, end='')
       for ind in order_centroids[i, :100]:
           print(' %s' % l[ind], end='')
       print()
</code></pre>

<p>Then after performing the following, I get 15 identical clusters (with almost fully identical word terms in it). I also tried the normalization using LSA method but it gives almost the same.</p>

<p>What am I doing wrong and how it can be fixed?</p>
"
"45520228","Parse nltk chunk string to form Tree","2017-08-05 09:14:09","0","1116","0","1","","45522017","<p>I have a file containing Strings like </p>

<pre><code>Tree('S', [Tree('NP', [('criminal', 'JJ'), ('lawyer', 'NN')]), Tree('NP', 
[('new', 'JJ'), ('york', 'NN')])])
</code></pre>

<p>Is there a python function that parse the string to produce Tree structure again? I tried the <a href=""http://www.nltk.org/_modules/nltk/tree.html#Tree.fromstring"" rel=""nofollow noreferrer"">Tree.fromstring</a> function but it doesn't parse.</p>

<p>I generate these strings like below</p>

<pre><code>&gt;&gt;&gt; import nltk
&gt;&gt;&gt; from nltk import pos_tag
&gt;&gt;&gt; pattern = """"""NP: {&lt;DT&gt;?&lt;JJ&gt;*&lt;NN&gt;}
... VBD: {&lt;VBD&gt;}
... IN: {&lt;IN&gt;}""""""
&gt;&gt;&gt; NPChunker = nltk.RegexpParser(pattern)
&gt;&gt;&gt; sentence = 'criminal lawyer new york'.split()
&gt;&gt;&gt; pos_tag(sentence)
[('criminal', 'JJ'), ('lawyer', 'NN'), ('new', 'JJ'), ('york', 'NN')]
&gt;&gt;&gt; result = NPChunker.parse(pos_tag(sentence))
&gt;&gt;&gt; result
Tree('S', [Tree('NP', [('criminal', 'JJ'), ('lawyer', 'NN')]), Tree('NP', 
[('new', 'JJ'), ('york', 'NN')])])
</code></pre>

<p>Thanks in advance.</p>
"
"45431399","When using word alignment tools like fast_align, does more sentences mean better accuracy?","2017-08-01 07:22:10","2","1173","1","1","","45562087","<p>I am using fast_align <a href=""https://github.com/clab/fast_align"" rel=""nofollow noreferrer"">https://github.com/clab/fast_align</a> to get word alignments between 1000 German sentences and 1000 English translations of those sentences. So far the quality is not so good. </p>

<p>Would throwing more sentences into the process help fast_align to be more accurate? Say I take some OPUS data with 100k aligned sentence pairs and then add my 1000 sentences in the end of it and feed it to fast_align. Will that help? I can't seem to find any info on whether this would make sense.</p>
"
"45403390","Lemmatizing Italian sentences for frequency counting","2017-07-30 18:41:33","11","12606","0","3","","45555676","<p>I would like to lemmatize some Italian text in order to perform some frequency counting of words and further investigations on the output of this lemmatized content.</p>

<p>I am preferring lemmatizing than stemming because I could extract the word meaning from the context in the sentence (e.g. distinguish between a verb and a noun) and obtain words that exist in the language, rather than roots of those words that don't usually have a meaning.</p>

<p>I found out this library called <code>pattern</code> (<code>pip2 install pattern</code>) that should complement <code>nltk</code> in order to perform lemmatization of the <strong>Italian language</strong>, however I am not sure the approach below is correct because each word is lemmatized by itself, not in the context of a sentence.</p>

<p>Probably I should give <code>pattern</code> the responsibility to tokenize a sentence (so also annotating each word with the metadata regarding verbs/nouns/adjectives etc), then retrieving the lemmatized word, but I am not able to do this and I am not even sure it is possible at the moment?</p>

<p>Also: in Italian some articles are rendered with an apostrophe so for example ""l'appartamento"" (in English ""the flat"") is actually 2 words: ""lo"" and ""appartamento"". Right now I am not able to find a way to split these 2 words with a combination of <code>nltk</code> and <code>pattern</code> so then I am not able to count the frequency of the words in the correct way.</p>

<pre><code>import nltk
import string
import pattern

# dictionary of Italian stop-words
it_stop_words = nltk.corpus.stopwords.words('italian')
# Snowball stemmer with rules for the Italian language
ita_stemmer = nltk.stem.snowball.ItalianStemmer()

# the following function is just to get the lemma
# out of the original input word (but right now
# it may be loosing the context about the sentence
# from where the word is coming from i.e.
# the same word could either be a noun/verb/adjective
# according to the context)
def lemmatize_word(input_word):
    in_word = input_word#.decode('utf-8')
    # print('Something: {}'.format(in_word))
    word_it = pattern.it.parse(
        in_word, 
        tokenize=False,  
        tag=False,  
        chunk=False,  
        lemmata=True 
    )
    # print(""Input: {} Output: {}"".format(in_word, word_it))
    the_lemmatized_word = word_it.split()[0][0][4]
    # print(""Returning: {}"".format(the_lemmatized_word))
    return the_lemmatized_word

it_string = ""Ieri sono andato in due supermercati. Oggi volevo andare all'ippodromo. Stasera mangio la pizza con le verdure.""

# 1st tokenize the sentence(s)
word_tokenized_list = nltk.tokenize.word_tokenize(it_string)
print(""1) NLTK tokenizer, num words: {} for list: {}"".format(len(word_tokenized_list), word_tokenized_list))

# 2nd remove punctuation and everything lower case
word_tokenized_no_punct = [string.lower(x) for x in word_tokenized_list if x not in string.punctuation]
print(""2) Clean punctuation, num words: {} for list: {}"".format(len(word_tokenized_no_punct), word_tokenized_no_punct))

# 3rd remove stop words (for the Italian language)
word_tokenized_no_punct_no_sw = [x for x in word_tokenized_no_punct if x not in it_stop_words]
print(""3) Clean stop-words, num words: {} for list: {}"".format(len(word_tokenized_no_punct_no_sw), word_tokenized_no_punct_no_sw))

# 4.1 lemmatize the words
word_tokenize_list_no_punct_lc_no_stowords_lemmatized = [lemmatize_word(x) for x in word_tokenized_no_punct_no_sw]
print(""4.1) lemmatizer, num words: {} for list: {}"".format(len(word_tokenize_list_no_punct_lc_no_stowords_lemmatized), word_tokenize_list_no_punct_lc_no_stowords_lemmatized))

# 4.2 snowball stemmer for Italian
word_tokenize_list_no_punct_lc_no_stowords_stem = [ita_stemmer.stem(i) for i in word_tokenized_no_punct_no_sw]
print(""4.2) stemmer, num words: {} for list: {}"".format(len(word_tokenize_list_no_punct_lc_no_stowords_stem), word_tokenize_list_no_punct_lc_no_stowords_stem))

# difference between stemmer and lemmatizer
print(
    ""For original word(s) '{}' and '{}' the stemmer: '{}' '{}' (count 1 each), the lemmatizer: '{}' '{}' (count 2)""
    .format(
        word_tokenized_no_punct_no_sw[1],
        word_tokenized_no_punct_no_sw[6],
        word_tokenize_list_no_punct_lc_no_stowords_stem[1],
        word_tokenize_list_no_punct_lc_no_stowords_stem[6],
        word_tokenize_list_no_punct_lc_no_stowords_lemmatized[1],
        word_tokenize_list_no_punct_lc_no_stowords_lemmatized[1]
    )
)
</code></pre>

<p>Gives this output:</p>

<pre><code>1) NLTK tokenizer, num words: 20 for list: ['Ieri', 'sono', 'andato', 'in', 'due', 'supermercati', '.', 'Oggi', 'volevo', 'andare', ""all'ippodromo"", '.', 'Stasera', 'mangio', 'la', 'pizza', 'con', 'le', 'verdure', '.']
2) Clean punctuation, num words: 17 for list: ['ieri', 'sono', 'andato', 'in', 'due', 'supermercati', 'oggi', 'volevo', 'andare', ""all'ippodromo"", 'stasera', 'mangio', 'la', 'pizza', 'con', 'le', 'verdure']
3) Clean stop-words, num words: 12 for list: ['ieri', 'andato', 'due', 'supermercati', 'oggi', 'volevo', 'andare', ""all'ippodromo"", 'stasera', 'mangio', 'pizza', 'verdure']
4.1) lemmatizer, num words: 12 for list: [u'ieri', u'andarsene', u'due', u'supermercato', u'oggi', u'volere', u'andare', u""all'ippodromo"", u'stasera', u'mangiare', u'pizza', u'verdura']
4.2) stemmer, num words: 12 for list: [u'ier', u'andat', u'due', u'supermerc', u'oggi', u'vol', u'andar', u""all'ippodrom"", u'staser', u'mang', u'pizz', u'verdur']
For original word(s) 'andato' and 'andare' the stemmer: 'andat' 'andar' (count 1 each), the lemmatizer: 'andarsene' 'andarsene' (count 2)
</code></pre>

<ul>
<li>How to effectively lemmatize some sentences with <code>pattern</code> using their tokenizer? (assuming lemmas are recognized as nouns/verbs/adjectives etc.)</li>
<li>Is there a python alternative to <code>pattern</code> to use for Italian lemmatization with <code>nltk</code>?  </li>
<li>How to split articles that are bound to the next word using apostrophes?</li>
</ul>
"
"45353866","NLTK inter-annotator agreement using Krippendorff Alpha","2017-07-27 14:33:48","6","5761","0","2","","45409680","<p>I am trying to compute inter-annotator agreement on a toy example using NLTK's <code>nltk.metrics.agreement</code> module. </p>

<p>Specifically I am trying to compute agreement using the <code>alpha</code> metric (<a href=""http://repository.upenn.edu/cgi/viewcontent.cgi?article=1043&amp;context=asc_papers"" rel=""noreferrer"">Krippendorff</a>) using two different distance metrics(<code>binary_distance</code> and <code>interval_distance</code>). </p>

<p>The expected result of toy example 1 below, which has near total agreement (only one pair disagrees), is a value close to <code>1</code>. However, in both cases res is <code>0.0</code>. Why? </p>

<p>I understand that Krippendorff's alpha is designed for intervals rather than binary-like two-category labels. I would however not expect a zero agreement value back from the module. For the background, the toy example is simply a specific subset of a larger dataset containing annotation scores in the range [1,4]. The subset belongs to a particular population within that dataset. </p>

<p>In toy example 2 things start to look better for the interval alpha. Binary alpha should probably raise an exception given that there are now three labels in the data.</p>

<p><strong>Toy Example 1</strong></p>

<pre><code>from nltk.metrics.agreement import AnnotationTask
from nltk.metrics import interval_distance, binary_distance 

 annotation_triples = [('coder_1', '1', 4), 
                       ('coder_2', '1', 4), 
                       ('coder_1', '2', 4),
                       ('coder_2', '2', 4), 
                       ('coder_1', '3', 4), 
                       ('coder_2', '3', 4),
                       ('coder_1', '4', 4), 
                       ('coder_2', '4', 3)]

 t = AnnotationTask(annotation_triples, distance=binary_distance)
 result = t.alpha()

 t = AnnotationTask(annotation_triples, distance=interval_distance)
 result = t.alpha()

result binary: 0.0
result interval: 0.0
</code></pre>

<p><strong>Toy Example 2</strong> (replaced first pair using <code>1</code> instead of <code>4</code>)</p>

<pre><code>annotation_triples = [('coder_1', '1', 1), 
                      ('coder_2', '1', 1),  
                      ('coder_1', '2', 4),
                      ('coder_2', '2', 4), 
                      ('coder_1', '3', 4), 
                      ('coder_2', '3', 4),
                      ('coder_1', '4', 4), 
                      ('coder_2', '4', 3)]

result binary: 0.59
result interval: 0.93
</code></pre>
"
"45232671","Obtain tf-idf weights of words with sklearn","2017-07-21 08:23:22","5","7430","0","1","","45240277","<p>I have a set of texts of wikipedia.<br>
Using <strong>tf-idf</strong>, I can define the weight of each word.
Below is the code:</p>

<pre><code>import pandas as pd                                             
from sklearn.feature_extraction.text import TfidfVectorizer

wiki = pd.read_csv('people_wiki.csv')

tfidf_vectorizer = TfidfVectorizer(max_features= 1000000)
tfidf = tfidf_vectorizer.fit_transform(wiki['text'])
</code></pre>

<p>The goal is to see the weights like shown in the <strong>tf-idf</strong> column:</p>

<p><a href=""https://i.sstatic.net/qKNCj.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/qKNCj.png"" alt=""enter image description here""></a></p>

<p>The file 'people_wiki.csv' is here:</p>

<p><a href=""https://ufile.io/udg1y"" rel=""nofollow noreferrer"">https://ufile.io/udg1y</a></p>
"
"45202126","nltk python 3 how do you return true if a noun is in the user input?","2017-07-19 22:33:52","1","174","0","1","","45202286","<p>I am using nltk and this method shown bellow should work it should print ""noun found""
 if there is a noun in the input. The problem is that it doesn't, could someone show 
    me how to do this Thank you!</p>

<pre><code>NOUN_CODES = {
    'NN',  # Noun, base form
    'NNP',  
    'NNS',  
    'NNPS',

    def Command_Noun_Check(what_person_said_l,what_person_said_l_wt):
                        Command_Noun_Result = nltk.pos_tag(what_person_said_l_wt)
                        print (Command_Noun_Result)
                        for x in Command_Noun_Result:

                            if x in NOUN_CODES:

                                print (""Noun Found"")
                                return True
                            else:
                                return False
</code></pre>
"
"45179185","Stemming full strings on Python","2017-07-19 00:38:26","4","6447","0","3","","45180214","<p>I need to perform stemming on portuguese strings.  To do so, i'm tokening the string using nltk.word_tokenize() function a then stemming each word individually. After that, I rebuild the string. It's working, but not performing well. How can i make it faster? The string length is about 2 million words.</p>

<pre><code>    tokenAux=""""
    tokens = nltk.word_tokenize(portugueseString)
        for token in tokens:
            tokenAux = token
            tokenAux = stemmer.stem(token)    
            textAux = textAux + "" ""+ tokenAux
    print(textAux)
</code></pre>

<p>Sorry for bad english and thanks!</p>
"
"44972641","Tfidf Vectorizer not working","2017-07-07 13:55:47","0","856","1","1","","44974256","<p>I have a corpus(<strong>Hotel Reviews</strong>) and I want to do some NLP process including Tfidf. My problem is when I Applied Tfidf and print 100 features it doesn't appear as a single word but the entire sentence.
Here is my code:</p>

<p><strong>Note: clean_doc is a function return my corpus cleaning from stopwords, stemming, and etc</strong></p>

<pre><code>vectorizer = TfidfVectorizer(analyzer='word',tokenizer=clean_doc, 
max_features=100, lowercase = False, ngram_range=(1,3), min_df = 1)
vz  = vectorizer.fit_transform(list(data['Review']))
feature_names = vectorizer.get_feature_names()
for feature in feature_names:
  print(feature)
</code></pre>

<p>it returns something like this:</p>

<pre><code>love view  good room
food amazing recommended 
bad services location far
-----
</code></pre>

<p>any idea why? Thanks in Advance</p>
"
"44946739","Use Tensorflow LSTM PTB example for scoring sentences","2017-07-06 10:44:31","0","395","0","1","","45153343","<p>I try to use an example LSTM, trained according to <a href=""https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/ptb_word_lm.py"" rel=""nofollow noreferrer"">Tensorflow LSTM example</a>. This example allows to get perplexity on whole test set. But I need to use the trained model to score (get loglikes) of each sentence separately (to score hypotheses of STT decoder output). I modified <a href=""https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/reader.py"" rel=""nofollow noreferrer"">reader</a> a bit and used code:</p>

<pre><code>mtests=list()
with tf.name_scope(""Test""):        
    for test_data_item in test_data:
      test_input.append(PTBInput(config=eval_config, data=test_data_item, name=""TestInput""))   
    with tf.variable_scope(""Model"", reuse=True, initializer=initializer):
      for test_input_item in test_input:
        mtests.append(PTBModel(is_training=False, config=eval_config,
                     input_=test_input_item))
sv = tf.train.Supervisor(logdir=FLAGS.model_dir)

with sv.managed_session() as session:
  checkpoint=tf.train.latest_checkpoint(FLAGS.model_dir)      
  sv.saver.restore(session, checkpoint)
  sys.stderr.write(""model restored\n"") 

  for mtest in mtests:      
    score, test_perplexity = run_epoch_test(session, mtest)
    print(score)
</code></pre>

<p>So, using that code, I get score of each sentence independently. If I pass 5 sentences, it works ok. But if I pass 1k sentences to this code, it works extremely slow and uses a lot of memory, because I create 1k models mtest. So, could you tell me another way to reach my goal? Thank you.</p>
"
"44941604","How to automatically identify hypernyms from a group of words?","2017-07-06 06:47:13","0","604","0","1","","44942068","<p>I have several groups of words, for example 
in group A: apple, pear, banana, fruit, grape, watermelon; 
in group B: cat, animal, dog, pig, monkey, duck; 
in group C: Italy, Australia, country, China, Greece. </p>

<p>How can I automatically identify the hypernyms from these 3 groups (e.g., fruit for group A, animal for group B and country for group C)? Thank you very much.</p>
"
"44873156","How can the perplexity of a language model be between 0 and 1?","2017-07-02 16:56:18","0","617","0","1","","44875134","<p>In Tensorflow, I'm getting outputs like 0.602129 or 0.663941. It appears that values closer to 0 imply a better model, but it seems like perplexity is supposed to be calculated as 2^loss, which implies that loss is negative. This doesn't make any sense.</p>
"
"44807639","Convert Averaged Perceptron Tagger POS to WordNet POS and Avoid Tuple Error","2017-06-28 16:08:42","1","716","0","1","","44808234","<p>I have code for POS tagging with NLTK's averaged perceptron tagger:</p>

<pre><code>from nltk.corpus import wordnet
from nltk.stem import WordNetLemmatizer
from nltk import pos_tag
from nltk.tokenize import word_tokenize

string = 'dogs runs fast'

tokens = word_tokenize(string)
tokensPOS = pos_tag(tokens)
print(tokensPOS)
</code></pre>

<p>Result:</p>

<pre><code>[('dogs', 'NNS'), ('runs', 'VBZ'), ('fast', 'RB')]
</code></pre>

<p>I have attempted code for looping through each tagged token and lemmatizing it with the WordNet lemmatizer:</p>

<pre><code>lemmatizedWords = []
for w in tokensPOS:
       lemmatizedWords.append(WordNetLemmatizer().lemmatize(w))

print(lemmatizedWords)
</code></pre>

<p>Resulting Error:</p>

<pre><code>Traceback (most recent call last):

  File ""&lt;ipython-input-30-462d7c3bdbb7&gt;"", line 15, in &lt;module&gt;
    lemmatizedWords = WordNetLemmatizer().lemmatize(w)

  File ""C:\Users\taca\AppData\Local\Continuum\Anaconda3\lib\site-packages\nltk\stem\wordnet.py"", line 40, in lemmatize
    lemmas = wordnet._morphy(word, pos)

  File ""C:\Users\taca\AppData\Local\Continuum\Anaconda3\lib\site-packages\nltk\corpus\reader\wordnet.py"", line 1712, in _morphy
    forms = apply_rules([form])

  File ""C:\Users\taca\AppData\Local\Continuum\Anaconda3\lib\site-packages\nltk\corpus\reader\wordnet.py"", line 1692, in apply_rules
    for form in forms

  File ""C:\Users\taca\AppData\Local\Continuum\Anaconda3\lib\site-packages\nltk\corpus\reader\wordnet.py"", line 1694, in &lt;listcomp&gt;
    if form.endswith(old)]

AttributeError: 'tuple' object has no attribute 'endswith'
</code></pre>

<p>I think I have two problems here:</p>

<ol>
<li>The POS tags are not converted to tags WordNet can understand (I tried implementing something similar to this answer <a href=""https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python"">wordnet lemmatization and pos tagging in python</a> with no success)</li>
<li>The data structures are not correctly formed to be able to loop through each tuple (I couldn't find much on this error beyond <code>os</code> related code)</li>
</ol>

<p>How do I follow up POS tagging with lemmatization to avoid these errors?</p>
"
"44752571","WordNetlemmatizer error - all alphabets are lemmatized","2017-06-26 02:55:43","1","1032","0","1","","44752871","<p>I am trying to lemmatize my dataset for sentiment analysis - What should I do to get the expected output rather than the current output? Input file is a csv - stored as DataFrame object.</p>

<pre><code>dataset = pd.read_csv('xyz.csv')
</code></pre>

<p>Here is my code</p>

<pre><code>from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
list1_ = []
for file_ in dataset:
    result1 = dataset['Content'].apply(lambda x: [lemmatizer.lemmatize(y) for y in x])
    list1_.append(result1)
dataset = pd.concat(list1_, ignore_index=True)
</code></pre>

<p>Expected </p>

<pre><code>&gt;&gt; lemmatizer.lemmatize('cats')
&gt;&gt; [cat]
</code></pre>

<p>Current output</p>

<pre><code>&gt;&gt; lemmatizer.lemmatize('cats')
&gt;&gt; [c,a,t,s]
</code></pre>
"
"44714142","Finding relations between Pronouns and Nouns in sentences","2017-06-23 06:04:52","5","2658","3","3","","44730440","<p>I am working on an NLP project and I need the following functionality illustrated by an example. Say there is a sentence </p>

<blockquote>
  <p>Tell Sam that he will have to leave without Arthur, as he is sick.</p>
</blockquote>

<p>In this statement, the first <code>he</code> has to be tagged to Sam and the second <code>he</code> to Arthur. I work in Python. Any suggestions on what I can use to get the following functionality?</p>
"
"44602346","How to employ Learning-to-rank models (CNN, LSTM) in short-pair ranking?","2017-06-17 08:17:34","2","877","0","1","","44610184","<p>In common applied learn-to-rank tasks, the inputs are usually semantic and have good syntactic structure, like Question-Answer ranking tasks. In this scenario, CNN or LSTM is a good structure to capture the latent information (local or long dependency) of QA-pairs. </p>

<p>But in reality, sometimes we just have short pair and discrete words. In this occasion, CNN or LSTM is still a fair choice？Or is there some more appropriate method can handle this?</p>
"
"44584671","Stemming in python","2017-06-16 08:40:33","0","1600","7","2","","44588364","<p>I want to stem my text, which I am reading from CSV file. But after the stem-operator the text is not changed. Than I have read somewhere that I need to use POS tags in order to stem but it didn't help.</p>

<p>Can you please tell me what I am doing wrong? So I am reading the csv, removing punctuation, tokenizing, getting POS tags, and trying to stem but nothing is changing. </p>

<pre><code>from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.stem import PorterStemmer
import nltk
from nltk import pos_tag

stemmer = nltk.PorterStemmer()
data = pd.read_csv(open('data.csv'),sep=';')

translator=str.maketrans('','',string.punctuation)

with open('output.csv', 'w', newline='') as csvfile:
   writer = csv.writer(csvfile, delimiter=';',
                            quotechar='^', quoting=csv.QUOTE_MINIMAL)

   for line in data['sent']:
        line = line.translate(translator)
        tokens = word_tokenize(line)
        tokens_pos = nltk.pos_tag(tokens)
        final = [stemmer.stem(tagged_word[0]) for tagged_word in tokens_pos]
        writer.writerow(tokens_pos)
</code></pre>

<p>Examples of data for stemming:</p>

<pre><code>The question was, what are you going to cut?
Well, again, while you were on the board of the Woods Foundation...
We've got some long-term challenges in this economy.
</code></pre>

<p>Thank you in advance for any help!</p>
"
"44522536","German Stemming for Sentiment Analysis in Python NLTK","2017-06-13 13:11:01","12","10393","0","2","","45033388","<p>I've recently begun working on a sentiment analysis project on German texts and I'm planning on using a stemmer to improve the results.</p>

<p>NLTK comes with a German Snowball Stemmer and I've already tried to use it, but I'm unsure about the results. Maybe it should be this way, but as a computer scientist and not a linguist, I have a problem with inflected verb forms stemmed to a different stem.</p>

<p>Take the word ""suchen"" (to search), which is stemmed to ""such"" for 1st person singular but to ""sucht"" for 3rd person singular. </p>

<p>I know there is also lemmatization, but no working German lemmatizer is integrated into NLTK as far as I know. There is GermaNet, but their NLTK integration seems to have been aborted.</p>

<p>Getting to the point: I would like inflected verb forms to be stemmed to the same stem, at the very least for regular verbs within the same tense. If this is not a useful requirement for my goal, please tell me why. If it is, do you know of any additional resources to use which can help me achieve this goal?</p>

<p>Edit: I forgot to mention, any software should be free to use for educational and research purposes.</p>
"
"44489768","Creating a Default Tagger Python NLTK","2017-06-11 23:51:43","0","737","0","1","","44489957","<p>I'm trying to create a Default Tagger on python using NLTK, but I keep on receiving an Error. The corpus composed of words in Estonian and the point is to tag the part of speech of each individual word.</p>

<p>My code:</p>

<pre><code>from nltk.corpus.reader import TaggedCorpusReader
mypath = ""/Users/mmo/Downloads/""

EC = TaggedCorpusReader(mypath,""estonianSmall_copy.txt"",
 encoding=""latin-1"")
sents = EC.tagged_sents()


from nltk import DefaultTagger
from nltk.probability import FreqDist

tags =[ [(word,tag)for word,tag in sent]\
    for sent in EC.tagged_sents()]
tagF = FreqDist(tags)
</code></pre>

<p>the error:</p>

<pre><code>tagF = FreqDist(tags)
Traceback (most recent call last):

   File ""&lt;ipython-input-26-c1ca76857fce&gt;"", line 1, in &lt;module&gt;
    tagF = FreqDist(tags)

  File ""/Users/mmo/anaconda/lib/python2.7/site-packages/nltk/probability.py"", line 106, in __init__
    Counter.__init__(self, samples)

  File ""/Users/mmo/anaconda/lib/python2.7/collections.py"", line 477, in __init__
    self.update(*args, **kwds)

  File ""/Users/mmo/anaconda/lib/python2.7/collections.py"", line 567, 
in update
    self[elem] = self_get(elem, 0) + 1

TypeError: unhashable type: 'list'
</code></pre>
"
"44489357","What method should I use to convert words into features for Machine Learning applications?","2017-06-11 22:32:22","2","507","0","1","","44489427","<p>I am planning on building a gender classifier. I know the two popular models are tf-idf and word2vec. 
While tf-idf focuses on the importance of a word in a document and similarity of documents, word2vec focuses more on the relationship between words and similarity between them. </p>

<p>However none of theme seem to be perfect for building vector features to be used for gender classification. Is there any other alternative vectorization model that might suit this task? </p>
"
"44468300","How to POS_TAG a french sentence?","2017-06-10 00:24:02","10","10852","2","2","","44475198","<p>I'm looking for a way to <code>pos_tag</code> a French sentence like the following code is used for English sentences:</p>

<pre><code>def pos_tagging(sentence):
    var = sentence
    exampleArray = [var]
    for item in exampleArray:
        tokenized = nltk.word_tokenize(item)
        tagged = nltk.pos_tag(tokenized)
        return tagged
</code></pre>
"
"44390198","if form in exceptions: TypeError: unhashable type: 'list' in Python nltk","2017-06-06 12:36:39","1","1379","0","1","","44390328","<p>I got following error.</p>

<pre><code> if form in exceptions: TypeError: unhashable type: 'list'
</code></pre>

<p>Following is my code.</p>

<pre><code>from nltk.tokenize import word_tokenize
from nltk.stem.wordnet import WordNetLemmatizer

sentence = 'missed you'
w_tokenize = (word_tokenize(sentence))

for word in w_tokenize:
  print WordNetLemmatizer().lemmatize(w_tokenize,'v')
</code></pre>

<p>Can anyone tell me how can I fix this error.</p>
"
"44382254","NLTK single-word part-of-speech tagging","2017-06-06 05:52:14","2","3536","0","2","","44383998","<p>Is there a way to use NLTK to get a set of possible parts of speech of a single string of letters, taking into account that different words might have homonyms?</p>

<p>For example: report -> {Noun, Verb}  ,  kind -> {Adjective, Noun}</p>

<p>I have not been able to find a POS-tokenizer that tags part-of-speech for words outside of the context of a full sentence. This seems like a very basic request of NLTK, so I'm confused as to why I've had so much trouble finding it.</p>
"
"44361787","Why NLTK Lemmatizer can't lemmatize some plural words?","2017-06-05 04:42:39","0","2397","2","1","","44362491","<p>I have tried to lemmatize a words from Quran Holy Book, but some words can't be lemmatized.</p>

<p>here's my sentence:</p>

<pre><code>sentence = ""Then bring ten surahs like it that have been invented and call upon for assistance whomever you can besides Allah if you should be truthful""
</code></pre>

<p>that sentence is part of my txt dataset.
as you can see, there's ""surahs"" which is a plural form of ""surah"".
I've tried my codes:</p>

<pre><code>def lemmatize(self, ayat):
    wordnet_lemmatizer = WordNetLemmatizer()
    result = []

    for i in xrange (len(ayat)):
        result.append(wordnet_lemmatizer.lemmatize(sentence[i],'v'))
    return result
</code></pre>

<p>which when I run and print, the result is like this:</p>

<pre><code>['bring', 'ten', 'surahs', 'like', u'invent', 'call', 'upon', 'assistance', 'whomever', 'besides', 'Allah', 'truthful']
</code></pre>

<p>the 'surahs' isn't changed into 'surah'.</p>

<p>anybody can tell why? thanks.</p>
"
"44360774","How to deactivate the default stop words feature for sklearn TfidfVectorizer","2017-06-05 02:08:13","4","1097","0","1","","44360829","<p>I am trying to get the tf-idf values for Japanese words.
The problem I am having is that sklearn TfidfVectorizer removes some Japanese characters, which I want to keep, as stop words. </p>

<p>The following is the example:</p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
tf = TfidfVectorizer(stop_words = None)

words_list = [""歯"",""が"",""痛い""]
tfidf_matrix =  tf.fit_transform(words_list)
feature_names = tf.get_feature_names() 
print (feature_names)
</code></pre>

<p>The output is:<code>['痛い']</code></p>

<p>However, I want to keep all those three characters in the list.
I believe TfidfVectorizer removes characters with length of 1 as stop words.
How could I deactivate the default stop words feature and keep all characters?</p>
"
"44279889","Extract SVO triples from preprocessed text","2017-05-31 08:45:04","0","1975","7","1","","44295728","<p>I need to extract subject-verb-object triples from a Dutch text. The text is analysed by a Dutch NLP tool named <a href=""https://languagemachines.github.io/frog/"" rel=""nofollow noreferrer"">Frog</a> which tokenized, parsed, tagged, lemmatized,...it. Frog produces FoLiA XML, or tab-delimited column-formatted output, one line per token. Because of some problems with the XML file, I chose to work with the column format. This example represents one sentence. <a href=""https://i.sstatic.net/7oKiv.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/7oKiv.jpg"" alt=""enter image description here""></a> Now I need to extract per sentence the SVO triples, therefore I need the last column which are the dependency relations. So I need to get the ROOT element and the su and obj1 elements which belong to the ROOT. Unfortunately the example sentence has no obj1. Let's pretend it has. My idea was to first create a nested list with a list per sentence.</p>

<pre><code>    import csv
    with open('romanfragment_frogged.tsv','r') as f:
         reader = csv.reader(f,delimiter='\t')
         tokens = []
         sentences = []
         list_of_sents = []
         for line in reader:
             tokens.append(line)
             #print(tokens)
             for token in tokens:
                 if token == '1':
                    previous_sentence = list_of_sents
                    sentences.append(previous_sentence)
         list_of_sents = []
         list_of_sents.append(tokens)
         print(list_of_sents)
</code></pre>

<p>When I print 'tokens', I get one list with all the tokens. So that is correct, but I'm still trying to create a nested list with 1 list (of tokens) per sentence.
Can someone help me with this problem? </p>

<p>(P.S. the second problem is that I'm not sure, how to continue once I get a nested list)</p>
"
"44229467","Python - difference between tagged_sents and tagged_words in NLTK corpora","2017-05-28 16:17:12","1","2636","0","1","","44233155","<p>What's the difference between nltk tagged_sents and tagged_words?</p>

<p>They both seems to be list with tuples (word, tag). And if you do type(), they are both</p>

<pre><code>nltk.collections.LazySubsequence
</code></pre>
"
"44226139","How to combine tfidf features with selfmade features","2017-05-28 10:13:03","3","2437","0","1","","44232589","<p>For a simple web page classification system I am trying to combine some selfmade features (frequency of HTML tags, frequency of certain word collocations) with the features obtained after applying tfidf. I am facing the following problem, however, and I don't really know how to proceed from here.</p>

<p>Right now I am trying to put all of these together in one dataframe, mainly by following the code from the following <a href=""https://github.com/JonathanReeve/milton-analysis/blob/v0.1/tfidf-scikit.ipynb"" rel=""nofollow noreferrer"">link</a> :</p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd

vectorizer = TfidfVectorizer(stop_words=""english"")
X_train_counts = vectorizer.fit_transform(train_data['text_no_punkt'])
feature_names = vectorizer.get_feature_names()
dense = X_train_counts.todense()
denselist = dense.tolist()

tfidf_df = pd.DataFrame(denselist, columns=feature_names, index=train_data['text_no_punkt'])
</code></pre>

<p>But this doesn't return the index (from 0 to 2464) I had in my original dataframe with the other features, neither does it seem to produce readable column names and instead of using the different words as titles, it uses numbers.</p>

<p>Furthermore I am not sure if this is the right way to combine features as this will result in an extremely high-dimensional dataframe which will probably not benefit the classifiers.</p>
"
"44198863","Matching PoS tags with specific text with `testacy.extract.pos_regex_matches(...)`","2017-05-26 10:07:40","1","1007","1","1","","44199476","<p>I'm using <code>textacy</code>'s <code>pos_regex_matches</code> method to find certain chunks of text in sentences.</p>

<p>For instance, assuming I have the text: <code>Huey, Dewey, and Louie are triplet cartoon characters.</code>, I'd like to detect that <code>Huey, Dewey, and Louie</code> is an enumeration.</p>

<p>To do so, I use the following code (on <code>testacy 0.3.4</code>, the version available at the time of writing):</p>

<pre><code>import textacy

sentence = 'Huey, Dewey, and Louie are triplet cartoon characters.'
pattern = r'&lt;PROPN&gt;+ (&lt;PUNCT|CCONJ&gt; &lt;PUNCT|CCONJ&gt;? &lt;PROPN&gt;+)*'
doc = textacy.Doc(sentence, lang='en')
lists = textacy.extract.pos_regex_matches(doc, pattern)
for list in lists:
    print(list.text)
</code></pre>

<p>which prints:</p>

<pre><code>Huey, Dewey, and Louie
</code></pre>

<p>However, if I have something like the following:</p>

<pre><code>sentence = 'Donald Duck - Disney'
</code></pre>

<p>then the <code>-</code> (dash) is recognised as <code>&lt;PUNCT&gt;</code> and the whole sentence is recognised as a list -- which it isn't.</p>

<p>Is there a way to specify that only <code>,</code> and <code>;</code> are valid <code>&lt;PUNCT&gt;</code> for lists? </p>

<p>I've looked for some reference about this regex language for matching PoS tags with no luck, can anybody help? Thanks in advance!</p>

<p>PS: I tried to replace <code>&lt;PUNCT|CCONJ&gt;</code> with <code>&lt;[;,]|CCONJ&gt;</code>, <code>&lt;;,|CCONJ&gt;</code>, <code>&lt;[;,]|CCONJ&gt;</code>, <code>&lt;PUNCT[;,]|CCONJ&gt;</code>, <code>&lt;;|,|CCONJ&gt;</code> and <code>&lt;';'|','|CCONJ&gt;</code> as suggested in the comments, but it didn't work...</p>
"
"44175810","opennlp vs corenlp : Market reach - popularity","2017-05-25 08:26:44","3","932","0","1","","44176465","<p>I am just doing the comparative study of open source NLP tools, and got an idea about the features/services of openNLP and coreNLP engines. In the recent past, I see that no contribution made for openNLP forum, where as coreNLP forum is still going active. So I wanted to understand if stanford:coreNLP has become more popular and been widely used in commercial applications? Anyone has an idea about it?</p>
"
"44004104","How to filter specific POS tags from list of lists to separate lists?","2017-05-16 14:20:37","1","859","0","2","","44004216","<p>I have huge data of product descriptions and required to separate the product names and the intent from descriptions for which i found out separating NNP tags after tagging the text with POS tags is somewhat helpful for further cleansing. </p>

<p>I have the following similar data for which i want to only filter NNP tags and want them to be filtered in their respective list, but unable to do so. </p>

<pre><code> data = [[('User', 'NNP'),
  ('is', 'VBZ'),
  ('not', 'RB'),
  ('able', 'JJ'),
  ('to', 'TO'),
  ('order', 'NN'),
  ('products', 'NNS'),
  ('from', 'IN'),
  ('iShopCatalog', 'NN'),
  ('Coala', 'NNP'),
  ('excluding', 'VBG'),
  ('articles', 'NNS'),
  ('from', 'IN'),
  ('VWR', 'NNP')],
 [('Arfter', 'NNP'),
  ('transferring', 'VBG'),
  ('the', 'DT'),
  ('articles', 'NNS'),
  ('from', 'IN'),
  ('COALA', 'NNP'),
  ('to', 'TO'),
  ('SRM', 'VB'),
  ('the', 'DT'),
  ('Category', 'NNP'),
  ('S9901', 'NNP'),
  ('Dummy', 'NNP'),
  ('is', 'VBZ'),
  ('maintained', 'VBN')],
 [('Due', 'JJ'),
  ('to', 'TO'),
  ('this', 'DT'),
  ('the', 'DT'),
  ('user', 'NN'),
  ('is', 'VBZ'),
  ('not', 'RB'),
  ('able', 'JJ'),
  ('to', 'TO'),
  ('order', 'NN'),
  ('the', 'DT'),
  ('product', 'NN')],
 [('All', 'DT'),
  ('other', 'JJ'),
  ('users', 'NNS'),
  ('can', 'MD'),
  ('order', 'NN'),
  ('these', 'DT'),
  ('articles', 'NNS')],
 [('She', 'PRP'),
  ('can', 'MD'),
  ('order', 'NN'),
  ('other', 'JJ'),
  ('products', 'NNS'),
  ('from', 'IN'),
  ('a', 'DT'),
  ('POETcatalog', 'NNP'),
  ('without', 'IN'),
  ('any', 'DT'),
  ('problems', 'NNS')],
 [('Furtheremore', 'IN'),
  ('she', 'PRP'),
  ('is', 'VBZ'),
  ('able', 'JJ'),
  ('to', 'TO'),
  ('order', 'NN'),
  ('products', 'NNS'),
  ('from', 'IN'),
  ('the', 'DT'),
  ('Vendor', 'NNP'),
  ('VWR', 'NNP'),
  ('through', 'IN'),
  ('COALA', 'NNP')],
 [('But', 'CC'),
  ('articles', 'NNS'),
  ('from', 'IN'),
  ('all', 'DT'),
  ('other', 'JJ'),
  ('suppliers', 'NNS'),
  ('are', 'VBP'),
  ('not', 'RB'),
  ('orderable', 'JJ')],
 [('I', 'PRP'),
  ('already', 'RB'),
  ('spoke', 'VBD'),
  ('to', 'TO'),
  ('anic', 'VB'),
  ('who', 'WP'),
  ('maintain', 'VBP'),
  ('the', 'DT'),
  ('catalog', 'NN'),
  ('COALA', 'NNP'),
  ('and', 'CC'),
  ('they', 'PRP'),
  ('said', 'VBD'),
  ('that', 'IN'),
  ('the', 'DT'),
  ('reason', 'NN'),
  ('should', 'MD'),
  ('be', 'VB'),
  ('the', 'DT'),
  ('assignment', 'NN'),
  ('of', 'IN'),
  ('the', 'DT'),
  ('plant', 'NN')],
 [('User', 'NNP'),
  ('is', 'VBZ'),
  ('a', 'DT'),
  ('assinged', 'JJ'),
  ('to', 'TO'),
  ('Universitaet', 'NNP'),
  ('Regensburg', 'NNP'),
  ('in', 'IN'),
  ('Scout', 'NNP'),
  ('but', 'CC'),
  ('in', 'IN'),
  ('P17', 'NNP'),
  ('table', 'NN'),
  ('YESRMCDMUSER01', 'NNP'),
  ('she', 'PRP'),
  ('is', 'VBZ'),
  ('assigned', 'VBN'),
  ('to', 'TO'),
  ('company', 'NN'),
  ('001500', 'CD'),
  ('Merck', 'NNP'),
  ('KGaA', 'NNP')],
 [('Please', 'NNP'),
  ('find', 'VB'),
  ('attached', 'JJ'),
  ('some', 'DT'),
  ('screenshots', 'NNS')]]
</code></pre>

<p>I wrote the following code:</p>

<pre><code>def prodname(a):
    p = []
    for i in a:
        for j in range(len(i)):
            if i[j][1]=='NNP':
                p.append(i[j][0])
    return p
</code></pre>

<p>which is giving the following output:</p>

<pre><code>    ['User',
     'Coala',
     'VWR',
     'Arfter',
     'COALA',
     'Category',
     'S9901',
     'Dummy',
     'POETcatalog',
     'Vendor',
     'VWR',
     'COALA',
     'COALA',
     'User',
     'Universitaet',
     'Regensburg',
     'Scout',
     'P17',
     'YESRMCDMUSER01',
     'Merck',
     'KGaA',
     'Please']
</code></pre>

<p>The output i would like to get is:</p>

<pre><code>[['User',
  'Coala',
  'VWR']
['Arfter',
 'COALA',
 'Category',
 'S9901',
 'Dummy']
[],
[],
['POETcatalog'],
['Vendor',
 'VWR',
 'COALA'],
[],
['COALA'],
['User',
 'Universitaet',
 'Regensburg',
 'Scout',
 'P17',
 'YESRMCDMUSER01',
 'Merck',
'KGaA'],
['Please']]
</code></pre>

<p>Also tried to use <code>[[] for i in range(len(data)]</code> to append to their respective lists, but couldn't do so. </p>
"
"43966848","function that returns how many nouns in a sentence (noun POS tag) in python","2017-05-14 17:25:22","0","1531","4","2","","43969608","<pre><code>var = (""this is my problem , i need help for a function like this one "")
exampleArray = [var]
def process_language():
    for item in exampleArray:
        tokenized = nltk.word_tokenize(item)
        tagged = nltk.pos_tag(tokenized)
        print (tagged)
process_language()
</code></pre>

<p>this function returns :</p>

<pre><code>[('this', 'DT'), ('is', 'VBZ'), ('my', 'PRP$'), ('problem', 'NN'), (',', ','), 
('i', 'VBP'), ('need', 'VBP'), ('help', 'NN'), ('for', 'IN'), ('a', 'DT'), 
('function', 'NN'), ('like', 'IN'), ('this', 'DT'), ('one', 'NN')]
</code></pre>

<p>i'm looking for a similair function that returns 4 which means four nouns thank you</p>
"
"43959815","list index out of range error when tag_sents() method of NLTK SennaTagger is called","2017-05-14 02:26:30","0","535","0","1","","43960593","<p><code>IndexError: list index out of range</code> when <code>tag_sents()</code> method of NLTK SennaTagger(<a href=""http://www.nltk.org/_modules/nltk/tag/senna.html"" rel=""nofollow noreferrer"">http://www.nltk.org/_modules/nltk/tag/senna.html</a>) is called.</p>

<p>A list of sentences is given as the input to <code>tag_sents</code>method.</p>

<p>A senna executable file is needed to run the tagger. Installation guide to SENNA toolkit can be found here. <a href=""http://ronan.collobert.com/senna/"" rel=""nofollow noreferrer"">http://ronan.collobert.com/senna/</a></p>

<p>Code:</p>

<pre><code>from nltk.tag import SennaTagger

SENNA_EXECUTABLE_DIR = '../../tools/senna'

pos_tagger = SennaTagger(SENNA_EXECUTABLE_DIR)

tagged = pos_tagger.tag_sents([""All the banks are closed"", ""Today is Sunday""])
</code></pre>

<p>Output:</p>

<pre><code>Traceback (most recent call last):

  File ""&lt;ipython-input-90-886051c3d91d&gt;"", line 1, in &lt;module&gt;
    tagged = pos_tagger.tag_sents([""All the banks are closed"", ""Today is Sunday""])

  File ""F:\Programs\Anaconda3\lib\site-packages\nltk\tag\senna.py"", line 55, in tag_sents
    tagged_sents = super(SennaTagger, self).tag_sents(sentences)

  File ""F:\Programs\Anaconda3\lib\site-packages\nltk\classify\senna.py"", line 161, in tag_sents
    result[tag] = tags[map_[tag]].strip()

IndexError: list index out of rangeenter code here
</code></pre>
"
"43943372","Best evaluation method for real-time machine translation?","2017-05-12 17:27:33","0","156","1","2","","43973705","<p>I'm aware that there are many different methods like BLEU, NIST, METEOR etc. They all have their pros and cons, and their effectiveness differs from corpus to corpus. I'm interested in real-time translation, so that two people could have a conversation by typing out a couple sentences at a time and having it immediately translated.</p>

<p>What kind of corpus would this count as? Would the text be considered too short for proper evaluation by most conventional methods? Would the fact that the speaker is constantly switching make the context more difficult?</p>
"
"43881605","Manipulate Nested Boolean Query String in Python","2017-05-10 00:01:36","0","396","0","1","","43881704","<p>I have string boolean queries like this</p>

<pre><code>   queryString= """"""And(
                      OR(abc,xyz,wxy),
                      AND(AND(xyz,wxy),xzy),
                      XOR(x1,y1, AND(xy,zz))  
                      )""""""
</code></pre>

<p>At current it is hard for me to modify the above query string, as I want to </p>

<ol>
<li>Add another <code>OR(x3,y3)</code> in the last <code>XOR</code> </li>
<li>Remove entire  <code>OR(abc,xyz,wxy)</code></li>
</ol>

<p>with desired output</p>

<pre><code>   resultQueryString= """"""And(                        
                            AND(AND(xyz,wxy),xzy),
                            XOR(x1,y1, AND(xy,zz),OR(x3,y3))  
                            )""""""
</code></pre>

<p>I think I cannot easily do it unless I come up with a sophisticated regex for each different query.</p>

<p>I am trying to write a python function which would take above string boolean query as input and output a tree data structure.</p>

<p>So that I can traverse the tree and evaluate or change whatever portion of query I want to change.</p>

<p>In above example, if I had it as a tree, I can easily see the root is <code>AND</code> and traverse/modify other branches so on.</p>
"
"43876234","Are there constituency parsers that do not aim for a full parse?","2017-05-09 17:25:42","0","151","0","1","","43876491","<p>I am currently working on a set of report-styled documents, of which I want to extract information. At the moment, I am trying to divide the text body into smaller constituents, for individual classification (what kind of information do we expect in the phrase). Because of the inaccurate grammar in which the reports are written, a standard constituency parser won’t find a common root for the sentences. This obviously cries for dependency parsing. I was however interested whether there would be constituency parsers which do not aim for a full parse of the sentence. Something anlong the line of probabilistic CKY which tries to return most probable sub nodes. I am currently working in the Python nltk framework, but Java solutions would be fine as well.</p>
"
"43841467","How to compute perplexity using KenLM?","2017-05-08 06:52:48","5","7755","0","4","","44105678","<p>Let's say we build a model on this:</p>

<pre><code>$ wget https://gist.githubusercontent.com/alvations/1c1b388456dc3760ffb487ce950712ac/raw/86cdf7de279a2b9bceeb3adb481e42691d12fbba/something.txt
$ lmplz -o 5 &lt; something.txt &gt; something.arpa
</code></pre>

<p>From the perplexity formula (<a href=""https://web.stanford.edu/class/cs124/lec/languagemodeling.pdf"" rel=""noreferrer"">https://web.stanford.edu/class/cs124/lec/languagemodeling.pdf</a>) </p>

<p>Applying the sum of inverse log formula to get the inner variable and then taking the nth root, the perplexity number is unusually small:</p>

<pre><code>&gt;&gt;&gt; import kenlm
&gt;&gt;&gt; m = kenlm.Model('something.arpa')

# Sentence seen in data.
&gt;&gt;&gt; s = 'The development of a forward-looking and comprehensive European migration policy,'
&gt;&gt;&gt; list(m.full_scores(s))
[(-0.8502398729324341, 2, False), (-3.0185394287109375, 3, False), (-0.3004383146762848, 4, False), (-1.0249041318893433, 5, False), (-0.6545327305793762, 5, False), (-0.29304179549217224, 5, False), (-0.4497605562210083, 5, False), (-0.49850910902023315, 5, False), (-0.3856896460056305, 5, False), (-0.3572353720664978, 5, False), (-1.7523181438446045, 1, False)]
&gt;&gt;&gt; n = len(s.split())
&gt;&gt;&gt; sum_inv_logs = -1 * sum(score for score, _, _ in m.full_scores(s))
&gt;&gt;&gt; math.pow(sum_inv_logs, 1.0/n)
1.2536033936438895
</code></pre>

<p>Trying again with a sentence not found in the data:</p>

<pre><code># Sentence not seen in data.
&gt;&gt;&gt; s = 'The European developement of a forward-looking and comphrensive society is doh.'
&gt;&gt;&gt; sum_inv_logs = -1 * sum(score for score, _, _ in m.full_scores(s))
&gt;&gt;&gt; sum_inv_logs
35.59524390101433
&gt;&gt;&gt; n = len(s.split())
&gt;&gt;&gt; math.pow(sum_inv_logs, 1.0/n)
1.383679905428275
</code></pre>

<p>And trying again with totally out of domain data:</p>

<pre><code>&gt;&gt;&gt; s = """"""On the evening of 5 May 2017, just before the French Presidential Election on 7 May, it was reported that nine gigabytes of Macron's campaign emails had been anonymously posted to Pastebin, a document-sharing site. In a statement on the same evening, Macron's political movement, En Marche!, said: ""The En Marche! Movement has been the victim of a massive and co-ordinated hack this evening which has given rise to the diffusion on social media of various internal information""""""
&gt;&gt;&gt; sum_inv_logs = -1 * sum(score for score, _, _ in m.full_scores(s))
&gt;&gt;&gt; sum_inv_logs
282.61719834804535
&gt;&gt;&gt; n = len(list(m.full_scores(s)))
&gt;&gt;&gt; n
79
&gt;&gt;&gt; math.pow(sum_inv_logs, 1.0/n)
1.0740582373271952
</code></pre>

<p>Although, it is expected that the longer sentence has lower perplexity, it's strange that the difference is less than 1.0 and in the range of decimals. </p>

<p><strong>Is the above the right way to compute perplexity with KenLM? If not, does anyone know how to computer perplexity with the KenLM through the Python API?</strong></p>
"
"43795249","How does spacy lemmatizer works?","2017-05-05 01:50:39","15","6157","0","3","","43942707","<p>For lemmatization spacy has a <a href=""https://github.com/explosion/spaCy/tree/master/spacy/en/lemmatizer"" rel=""noreferrer"">lists of words</a>:  adjectives, adverbs, verbs... and also lists for exceptions: adverbs_irreg... for the regular ones there is a set of <a href=""https://github.com/explosion/spaCy/blob/master/spacy/en/lemmatizer/_lemma_rules.py"" rel=""noreferrer"">rules</a></p>

<p>Let's take as example the word ""wider""</p>

<p>As it is an adjective the rule for lemmatization should be take from this list:</p>

<pre><code>ADJECTIVE_RULES = [
    [""er"", """"],
    [""est"", """"],
    [""er"", ""e""],
    [""est"", ""e""]
] 
</code></pre>

<p>As I understand the process will be like this:</p>

<p>1) Get the POS tag of the word to know whether it is a noun, a verb...<br>
2) If the word is in the list of irregular cases is replaced directly if not one of the rules is applied.</p>

<p>Now, how is decided to use ""er"" -> ""e"" instead of ""er""-> """" to get ""wide"" and not ""wid""? </p>

<p><a href=""http://textanalysisonline.com/spacy-word-lemmatize"" rel=""noreferrer"">Here</a> it can be tested.</p>
"
"43768099","How to speed up slow POS tagging?","2017-05-03 19:03:52","2","1641","4","1","","43778861","<p>Before you redirect me to another stackoverflow page since I know there are a few questions about speeding up POS tagging, I've already browsed through and sped up my code with the suggestions here: <a href=""https://stackoverflow.com/questions/11610076/slow-performance-of-pos-tagging-can-i-do-some-kind-of-pre-warming?noredirect=1&amp;lq=1"">Slow performance of POS tagging. Can I do some kind of pre-warming?</a></p>

<p>I'm using Python 3.6. I have lists containing ~100,000 words that have been tokenized using nltk. These are pretty hefty lists so I know that tagging all of these words will inherently take some amount of time. I've loaded the tagger outside, as follows:</p>

<pre><code>def tag_wordList(tokenizedWordList):       
    from nltk.tag.perceptron import PerceptronTagger
    tagger=PerceptronTagger() # load outside

    for words in tokenizedWordList:
         taggedList = tagger.tag(tokenizedWordList) # add POS to words

    return taggedList
</code></pre>

<p>Taking this step has sped things up a significant amount, but to get through 100,000+ words, it's still taking over 1.5 hours (and it's still running). The code works fine on a smaller set of data. I believe I tried converting the list to a set at one point without much improvement, though I'm going to try again for good measure. Anyone have any other tips for improving efficiency?</p>
"
"43656078","Word sense disambiguation with WordNet. How to select the words related to the same meaning?","2017-04-27 11:09:30","1","1904","1","1","","43658840","<p>I am using WordNet and NLTK for the word sense disambiguation. I am interested in all the words, which are related to the sound. I have a list of such words and 'roll' is one of them. Then I check if any of my sentences contains this word (I also check it depending on the POS). And if yes I would like to select only such sentences, which are related to sound. In the example below it would be the second sentence. The idea I have now is just to select such words, whos definition has a word 'sound' in it as 'the sound of a drum (especially a snare drum) beaten rapidly and continuously'. But I suspect that there is a more elegant way. Any ideas would be highly appreciated!</p>

<pre><code>from nltk.wsd import lesk
from nltk.corpus import wordnet as wn

samples = [('The van rolled along the highway.','n'),
('The thunder rolled and the lightning striked.','n')]

word = 'roll'
for sentence, pos_tag in samples:
    word_syn = lesk(word_tokenize(sentence.lower()), word, pos_tag)
    print 'Sentence:', sentence
    print 'Word synset:', word_syn
    print  'Corresponding definition:', word_syn.definition()
</code></pre>

<p>output:</p>

<pre><code>Sentence: The van rolled along the highway.
Word synset: Synset('scroll.n.02')
Corresponding definition: a document that can be rolled up (as for storage)
Sentence: The thunder rolled and the lightning striked.
Word synset: Synset('paradiddle.n.01')
Corresponding definition: the sound of a drum (especially a snare drum) beaten rapidly and continuously
</code></pre>
"
"43608448","How to create meaningful column value pair lists from a string?","2017-04-25 10:42:34","1","108","3","1","","43611313","<p>I am trying to categorize columns and values (column=value) meaningfully from an input string using Python dictionaries.</p>

<pre><code>input_string = ""the status is processing and product subtypes are HL year 30 ARM and applicant name is Ryan""
</code></pre>

<p>I have created dictionaries of key value pairs. In the first scenario, the <em>key</em> is the column name. The <em>value</em> represents the lowest index of key found in <code>input_string</code>. </p>

<p>Here is the dictionary of column names:<br></p>

<pre><code> dict_columns = {'status': 4, 'product subtypes': 29, 'applicant name': 69}
</code></pre>

<p>In the above dictionary, <code>'status'</code> has the lowest index of <code>4</code> in the <code>input_string</code>.  </p>

<p><br>Similarly, here is the dictionary of values:<br></p>

<pre><code>dict_values = {'processing': 14, 'hl': 50, 'year': 53, '30': 58, 'arm': 61, 'ryan': 87}
</code></pre>

<p><strong>The question is:</strong>
<br><strong>How to get the expected ouput as:</strong><br></p>

<pre><code>list_parsed_values = ['processing', 'hl year 30 arm', 'ryan']
</code></pre>

<p><strong>and the (optional) corresponding list of columns as:</strong> <br></p>

<pre><code>list_parsed_columns = ['status', 'product subtypes', 'applicant name']
</code></pre>

<p><strong>How to clearly distinguish the values in a list?</strong></p>
"
"43596745","Python parse text from multiple txt file","2017-04-24 20:00:52","15","2873","4","2","","43714754","<p>Seeking advice on how to mine items from multiple text files to build a dictionary. </p>

<p>This text file: <a href=""https://pastebin.com/Npcp3HCM"" rel=""noreferrer"">https://pastebin.com/Npcp3HCM</a></p>

<p>Was manually transformed into this required data structure: <a href=""https://drive.google.com/file/d/0B2AJ7rliSQubV0J2Z0d0eXF3bW8/view"" rel=""noreferrer"">https://drive.google.com/file/d/0B2AJ7rliSQubV0J2Z0d0eXF3bW8/view</a></p>

<p>There are thousands of such text files and they may have different section headings as shown in these examples:</p>

<ol>
<li><a href=""https://pastebin.com/wWSPGaLX"" rel=""noreferrer"">https://pastebin.com/wWSPGaLX</a> </li>
<li><a href=""https://pastebin.com/9Up4RWHu"" rel=""noreferrer"">https://pastebin.com/9Up4RWHu</a></li>
</ol>

<p>I started off by reading the files</p>

<pre><code>from glob import glob

txtPth = '../tr-txt/*.txt'
txtFiles = glob(txtPth)

with open(txtFiles[0],'r') as tf:
    allLines = [line.rstrip() for line in tf]

sectionHeading = ['Corporate Participants',
                  'Conference Call Participiants',
                  'Presentation',
                  'Questions and Answers']

for lineNum, line in enumerate(allLines):
    if line in sectionHeading:
        print(lineNum,allLines[lineNum])
</code></pre>

<p>My idea was to look for the line numbers where section headings existed and try to extract the content in between those line numbers, then strip out separators like dashes. That didn't work and I got stuck in trying to create a dictionary of this kind so that I can later run various natural language processing algorithms on quarried items. </p>

<pre><code>{file-name1:{
    {date-time:[string]},
    {corporate-name:[string]},
    {corporate-participants:[name1,name2,name3]},
    {call-participants:[name4,name5]},
    {section-headings:{
        {heading1:[
            {name1:[speechOrderNum, text-content]},
            {name2:[speechOrderNum, text-content]},
            {name3:[speechOrderNum, text-content]}],
        {heading2:[
            {name1:[speechOrderNum, text-content]},
            {name2:[speechOrderNum, text-content]},
            {name3:[speechOrderNum, text-content]},
            {name2:[speechOrderNum, text-content]},
            {name1:[speechOrderNum, text-content]},
            {name4:[speechOrderNum, text-content]}],
        {heading3:[text-content]},
        {heading4:[text-content]}
        }
    }
}
</code></pre>

<p>The challenge is that different files may have different headings and number of headings. But there will always be a section called ""Presentation"" and very likely to have ""Question and Answer"" section. These section headings are always separated by a string of equal-to signs. And content of different speaker is always separated by string of dashes. The ""speech order"" for Q&amp;A section is indicated with a number in square brackets. The participants are are always indicated in the beginning of the document with an asterisks before their name and their tile is always on the next line. </p>

<p>Any suggestion on how to parse the text files is appreciated. The ideal help would be to provide guidance on how to produce such a dictionary (or other suitable data structure) for each file that can then be written to a database. </p>

<p>Thanks</p>

<p>--EDIT--</p>

<p>One of the files looks like this: <a href=""https://pastebin.com/MSvmHb2e"" rel=""noreferrer"">https://pastebin.com/MSvmHb2e</a></p>

<p>In which the ""Question &amp; Answer"" section is mislabeled as ""Presentation"" and there is no other ""Question &amp; Answer"" section. </p>

<p>And final sample text: <a href=""https://pastebin.com/jr9WfpV8"" rel=""noreferrer"">https://pastebin.com/jr9WfpV8</a></p>
"
"43596005","Parsing textfile with citations","2017-04-24 19:12:09","2","69","14","1","","43597699","<p>The issue is that I’m trying to segment the text file by sentences using php. I currently using the following function:</p>

<pre><code>$results = preg_split('/(?&lt;=[.?!])\s+/', $stringtest, -1, PREG_SPLIT_NO_EMPTY);
</code></pre>

<p>The issue is that with sentences like these:</p>

<pre><code>In his book The Symposium, Plato wrote “Those who are halves of a man whole pursue males, and being slices, so to speak, of the male, love men throughout their boyhood, and take pleasure in physical contact with men” (qtd. in Isay 11).
</code></pre>

<p>It splits it like this:</p>

<pre><code>[0] In his book The Symposium, Plato wrote “Those who are halves of a man whole pursue males, and being slices, so to speak, of the male, love men throughout their boyhood, and take pleasure in physical contact with men” (qtd. 
[1] in Isay 11).
</code></pre>

<p>Another example is:</p>

<pre><code>Dr. Evelyn Hooker, a heterosexual psychologist...
</code></pre>

<p><strong>The Dr. part would be an issue.</strong></p>

<p>These texts are all from the MASC corpus for NLP.</p>
"
"43558145","SyntaxNet to process a large number of sentences, do GPUs increase performance?","2017-04-22 10:31:33","1","232","0","1","","43565143","<p>I have a large dataset of sentences (i.e., ~5.000.000) in raw text which I want to process using SyntaxNet already trained for English. That is, I just want to process the sentences using a SyntaxNet model, I don't want to train any new model. </p>

<p>Setting up a processing environment with GPUs will have any effect on performance ? </p>

<p>I understand that most of the heavy CPU operations is on estimating the parameters and weights of the network/model, once these are estimated, applying the trained network should be faster than training.</p>

<p>Nevertheless, I've never worked before with Tensorflow and I don't know whether GPUs are used when one applies an already trained model to data.</p>

<p>Also, does anyone knows any easy way to setup SyntaxNet as a daemon or web-service, so that batch processing can be made easily?</p>
"
"43546510","String similarity TF-IDF Bag of words or Word2vec","2017-04-21 15:24:32","1","2266","2","1","","44601800","<p>I am trying to create an application that computes the similarity between 2 strings.
The strings are not long. 3 Sentences long at maximum.
I did some research and I came across some possible solution paths.</p>

<p>First one use bag of words: count words and compare the 2 produced vectors ( cosine similarity)</p>

<p>The second use TF-IDF and compare produced vectors.</p>

<p>The third is use word2vec and compare vectors.</p>

<p>Now for the questions.</p>

<p>Performance wise is word2vec performance better that TF-IDF for short sentences?</p>

<p>What is the best way to train word2vec model? Should I use a large amount of text ( wikipedia dump for example) or train it using just the sentences that are being compared.</p>

<p>How to get sentence similarity from word2vec. should I average the words in each sentence or is there a better solution?</p>
"
"43526587","Cosine similarity for already known pairs of duplicates","2017-04-20 17:53:56","0","1099","0","1","","43719014","<p>I have a list of duplicate document pairs saved in a csv file. Each ID from column 1 is a duplicate to the corresponding ID in column 2.
The file goes something like this: </p>

<pre><code>Document_ID1    Document_ID2
12345           87565
34546           45633
56453           78645
35667           67856
13636           67845
</code></pre>

<p>Each Document ID is associated with text that is saved somewhere else. I pulled this text and saved each column of IDs and associated texts into two lsm databases.<br>
So I have <code>db1</code> which has all the IDs from <code>Document_ID1</code> as <em>keys</em> and their corresponding texts as the <em>values</em> for the respective keys. Therefore, like a dictionary. Similarly, <code>db2</code> for all the IDs from <code>Document_ID2</code>.<br>
So, when I say <code>db1[12345]</code>, I get the text associated with the ID 12345.<br></p>

<p>Now, I want to get the cosine similarity scores between each of these pairs to determine their duplicate-ness. Until now I ran a tfidf model to do the same. I created a tfidf matrix with all the documents in db1 as the corpus, and I measured the cosine similarity of each of the tfidf vectors from db2 against the tfidf matrix. For security reasons, I cannot provide the complete code. Code goes like this: </p>

<pre><code># Generator function to pick one key (document) at a time for comparison against other documents
def generator(db):
    for key in db.keys():
        text = db[key]
        yield text

# Use spaCy to create a function to preprocess text from the generator function
nlp = spacy.load('en')
def spacy(generator_object):
    for doc in generator_object:
        words = &lt;code to make words lower case, remove stop words, spaces and punctuations&gt;
        yield u' '.join(words)

# TF-IDF Vectorizer
tfidf = TfidfVectorizer(min_df = 2)

# Applying tf-idf transformer to each key from db1 individually in the generator function.
tfidf_matrix = tfidf.fit_transform(spacy(generator(db1)))

# Function to calculate cosine similarity values between the tfidf matrix and the tfidf vector of a new key
def similarity(tfidf_vector, tfidf_matrix, keys):    
    sim_vec = &lt;code to get cosine similarity&gt;
    return sim_vec.sort_values(ascending=False)

# Applying tf-idf transformer on db2 keys on a loop and getting cosine similarity scores for each key from db2.
for key in db2.keys():
    # Create a new temporary db for each key from db2 to enter into generator function
    new = &lt;code to create a temporary new lsm database&gt;
    text = db2[key]
    new[key] = text
    new_key = &lt;code to get next key from the temporary new lsm database&gt;
    tfidf_vector = tfidf.transform(spacy_proc(corpus_gen(new)))
    similarity_values = similarity(tfidf_vector, tfidf_matrix, list(db1.keys()))
    for idx, i in similarity_values.iteritems(): 
            print new_key, idx, i
    del new[key]
</code></pre>

<p>But this gives me cosine similarity scores against all keys in db1 for  each key in db2. Example: If there are 5 keys in db1 and 5 keys in db2, I get 25 rows as result with this code. <br>
What I want is to get the cosine similarity scores for just corresponding key from db1 for the key in db2. Which means if there are 5 keys each in db1 and db2, I should have only 5 rows as a result - the cosine similarity score for each pair of duplicates only.<br><br>How should I tweak my code to get that?</p>
"
"43450538","Save different POS (parts of speech) in different file using POSTagger in Java?","2017-04-17 11:26:28","1","82","0","1","","43473999","<p>I am using openNLP for tagging POS (Parts of Speech).</p>

<pre><code>InputStream inputStream = new 
             FileInputStream(""C:/en-pos-maxent.bin""); 
          POSModel model = new POSModel(inputStream);

          POSTaggerME tagger = new POSTaggerME(model);



    String sentence = ""This is not a song for the broken-hearted"" + 
            "" No silent prayer for the faith-departed "" + 
            "" I am not gonna be just a face in the crowd "" + 
            "" You are gonna hear my voice "" +
            "" When I shout it out loud"";

    String simple = ""[.?!-]"";      
      String[] splitString = (sentence.split(simple));     


      SimpleTokenizer simpleTokenizer = SimpleTokenizer.INSTANCE;

      //String tokens[] = simpleTokenizer.tokenize();



      for(int i = 0;i&lt;splitString.length;i++)
      {
          String tokens[] = simpleTokenizer.tokenize(splitString[i]);


          String[] tags = tagger.tag(tokens);

          //POSSample sample = new POSSample(tokens, tags);

          /*for(String token : tokens) {         
              System.out.println(token);  
           }*/



         for(int j= 0;j &lt; tags.length;j++)
          {
              if(tags[j].equals(""DT""))
              {
                  //System.out.println(tokens[j]);

                  File file = new File(""DT.txt"");

                  try {

                      PrintWriter output = new PrintWriter(file);
                      output.println(tokens[j]);
                      output.close();

                } catch (Exception e) {
                    // TODO: handle exception
                }
</code></pre>

<p>When I use println in loop.It show me the required value.But when I am going to save this in a file name DT.txt . It just save one value in text file.
<a href=""https://i.sstatic.net/DfV2F.jpg"" rel=""nofollow noreferrer"">Text file output</a>
<a href=""https://i.sstatic.net/Iy3HK.jpg"" rel=""nofollow noreferrer"">Printed Output in console</a></p>
"
"43444261","Word Mover's Distance in Python","2017-04-17 02:14:36","3","4721","1","1","","43510219","<p>I am trying to calculate the similarity of 2 texts using WMD. I have tried to use the following code in Python 3, using gensim:</p>

<pre><code>word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
word2vec_model.init_sims(replace=True) # normalizes vectors
distance = word2vec_model.wmdistance(""string 1"", ""string 2"")  # Compute WMD as normal.
</code></pre>

<p>However, I don't think this is returning me the right value. How should I do this in python?</p>
"
"43427133","How to pars treebank in (python)?","2017-04-15 14:16:55","2","577","0","1","","43428146","<p>I have several .tree files each file contains more than one tree and I try to pars these file in the easiest way.
when I used </p>

<pre><code> for line in txt.readlines():
</code></pre>

<p>I faced error in parsing because sometimes line contains two trees
the question is how to separate trees in separated lines?
is there an effiecent solution to solve such problem?</p>
"
"43191782","How to train Open NLP without file","2017-04-03 18:25:51","2","239","5","1","","51800137","<p>i have the following code for training Open NLP POS Tagger</p>

<pre><code>Trainer(String trainingData, String modelSavePath, String dictionary){

    try {
        dataIn = new MarkableFileInputStreamFactory(
                new File(trainingData));

        lineStream = new PlainTextByLineStream(dataIn, ""UTF-8"");
        ObjectStream&lt;POSSample&gt; sampleStream = new WordTagSampleStream(lineStream);

        POSTaggerFactory fac=new POSTaggerFactory();
        if(dictionary!=null &amp;&amp; dictionary.length()&gt;0)
        {
            fac.setDictionary(new Dictionary(new FileInputStream(dictionary)));
        }
        model = POSTaggerME.train(""en"", sampleStream, TrainingParameters.defaultParams(), fac);

    } catch (IOException e) {
        // Failed to read or parse training data, training failed
        e.printStackTrace();
    } finally {
        if (lineStream != null) {
            try {
                lineStream.close();
            } catch (IOException e) {
                // Not an issue, training already finished.
                // The exception should be logged and investigated
                // if part of a production system.
                e.printStackTrace();
            }
        }
    }
}
</code></pre>

<p>and this works just fine. Now, is it possible to do the same without involving files? I want to store the training data in a database somewhere. Then i can read it as a stream or chunks and feed it to the trainer. I do not want to create a temp file. Is this possible?</p>
"
"43191522","How to know specific TF-IDF value of a word?","2017-04-03 18:10:22","1","2362","0","1","","43191644","<p>How can I know the value of a specific word using the TfidfVectorizer function?
For example, my code is:</p>

<pre><code>docs = []
docs.append(""this is sentence number one"")
docs.append(""this is sentence number two"")
vectorizer = TfidfVectorizer(norm='l2',min_df=0, use_idf=True, smooth_idf=True, stop_words='english', sublinear_tf=True)
sklearn_representation = vectorizer.fit_transform(docs)
</code></pre>

<p>Now, how can I know the TF-IDF value of ""sentence"" in the sentence 2 (docs[1])?</p>
"
"43171573","can anyone tell me about the model (skipgram/ CBOW ) used by Gensim?","2017-04-02 17:55:28","2","406","1","1","","43171638","<p>word2vec uses either of the model for distributed representation of words. I was checking out the codes of gensim but it is not defined about the model used by gensim .</p>
"
"43150019","Remove Words Less Than 4 Characters from Pandas Series","2017-03-31 21:34:30","2","9189","1","4","","43150816","<p>I am trying to remove all words with less than 4 characters from each scalar value in a Pandas Series. What is the best way to do it? Here is my failed attempt:</p>

<pre><code>df['text'] = df['text'].str.join(word for word in df['text'].str.split() if len(word)&gt;3)
</code></pre>

<p>I receive the following error message:</p>

<blockquote>
  <p>AttributeError: 'generator' object has no attribute 'join'</p>
</blockquote>

<p>I based my attempt off of this post regarding the same in a string: <a href=""https://stackoverflow.com/questions/12628958/remove-small-words-using-python"">Remove small words using Python</a></p>

<p>Side note: If its better to tokenize my words before removing with less than 4 characters please let me know.</p>

<p>EDIT: Each scalar value contains sentences so I want to remove for any words less than a length of 4 within the value.</p>
"
"43149878","Why do nested MaybeT's cause exponential allocation","2017-03-31 21:23:56","19","341","4","1","","43151568","<p>I have a program. </p>

<pre><code>import Control.Monad
import Control.Monad.Identity
import Control.Monad.Trans.Maybe

import System.Environment

tryR :: Monad m =&gt; ([a] -&gt; MaybeT m [a]) -&gt; ([a] -&gt; m [a])
tryR f x = do
  m &lt;- runMaybeT (f x)
  case m of
    Just t -&gt; return t
    Nothing -&gt; return x

check :: MonadPlus m =&gt; Int -&gt; m Int
check x = if x `mod` 2 == 0 then return (x `div` 2) else mzero

foo :: MonadPlus m =&gt; [Int] -&gt; m [Int]
foo [] = return []
foo (x:xs) = liftM2 (:) (check x) (tryR foo xs)


runFoo :: [Int] -&gt; [Int]
runFoo x = runIdentity $ tryR foo x

main :: IO ()
main = do
  [n_str] &lt;- getArgs
  let n = read n_str :: Int
  print $ runFoo [2,4..n]
</code></pre>

<p>The main interesting thing about this program is that it can have many nested layers of MaybeT's. Here, doing so serves absolutely no purpose, but it did in the original program where I encountered this problem.</p>

<p>Care to take a guess of the time complexity of this program?</p>

<p>Okay, you cheated by reading the title of this question. Yes, it's exponential:</p>

<pre><code>[jkoppel@dhcp-18-189-103-38:~/tmp]$ time ./ExpAlloc 50                                                                                                                                        (03-31 17:15)
[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25]
./ExpAlloc 50  8.10s user 0.06s system 99% cpu 8.169 total
[jkoppel@dhcp-18-189-103-38:~/tmp]$ time ./ExpAlloc 52                                                                                                                                        (03-31 17:15)
[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26]
./ExpAlloc 52  16.10s user 0.12s system 99% cpu 16.227 total
[jkoppel@dhcp-18-189-103-38:~/tmp]$ time ./ExpAlloc 54                                                                                                                                        (03-31 17:16)
[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27]
./ExpAlloc 54  32.32s user 0.23s system 99% cpu 32.561 total
</code></pre>

<p>Some further inspection shows the reason is because it allocates an exponential amount of memory, which naturally takes an exponential amount of time:</p>

<pre><code>[jkoppel@dhcp-18-189-103-38:~/tmp]$ time ./ExpAlloc 40 +RTS -s                                                                                                                                (03-31 17:17)
[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]
     939,634,520 bytes allocated in the heap
       5,382,816 bytes copied during GC
          75,808 bytes maximum residency (2 sample(s))
          66,592 bytes maximum slop
               2 MB total memory in use (0 MB lost due to fragmentation)

                                     Tot time (elapsed)  Avg pause  Max pause
  Gen  0      1796 colls,     0 par    0.008s   0.009s     0.0000s    0.0000s
  Gen  1         2 colls,     0 par    0.000s   0.000s     0.0001s    0.0001s

  INIT    time    0.000s  (  0.000s elapsed)
  MUT     time    0.243s  (  0.246s elapsed)
  GC      time    0.008s  (  0.009s elapsed)
  EXIT    time    0.000s  (  0.000s elapsed)
  Total   time    0.252s  (  0.256s elapsed)

  %GC     time       3.2%  (3.6% elapsed)

  Alloc rate    3,869,930,149 bytes per MUT second

  Productivity  96.8% of total user, 95.3% of total elapsed

./ExpAlloc 40 +RTS -s  0.25s user 0.00s system 98% cpu 0.260 total
[jkoppel@dhcp-18-189-103-38:~/tmp]$ time ./ExpAlloc 42 +RTS -s                                                                                                                                (03-31 17:17)
[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21]
   1,879,159,424 bytes allocated in the heap
      10,767,048 bytes copied during GC
          95,504 bytes maximum residency (3 sample(s))
          71,152 bytes maximum slop
               2 MB total memory in use (0 MB lost due to fragmentation)

                                     Tot time (elapsed)  Avg pause  Max pause
  Gen  0      3593 colls,     0 par    0.016s   0.018s     0.0000s    0.0000s
  Gen  1         3 colls,     0 par    0.000s   0.000s     0.0001s    0.0001s

  INIT    time    0.000s  (  0.000s elapsed)
  MUT     time    0.493s  (  0.498s elapsed)
  GC      time    0.016s  (  0.018s elapsed)
  EXIT    time    0.000s  (  0.000s elapsed)
  Total   time    0.510s  (  0.517s elapsed)

  %GC     time       3.1%  (3.5% elapsed)

  Alloc rate    3,810,430,292 bytes per MUT second

  Productivity  96.8% of total user, 95.7% of total elapsed

./ExpAlloc 42 +RTS -s  0.51s user 0.01s system 99% cpu 0.521 total
[jkoppel@dhcp-18-189-103-38:~/tmp]$ time ./ExpAlloc 44 +RTS -s                                                                                                                                (03-31 17:17)
[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22]
   3,758,208,408 bytes allocated in the heap
      21,499,312 bytes copied during GC
         102,056 bytes maximum residency (5 sample(s))
          73,784 bytes maximum slop
               2 MB total memory in use (0 MB lost due to fragmentation)

                                     Tot time (elapsed)  Avg pause  Max pause
  Gen  0      7186 colls,     0 par    0.032s   0.037s     0.0000s    0.0009s
  Gen  1         5 colls,     0 par    0.000s   0.001s     0.0001s    0.0001s

  INIT    time    0.000s  (  0.000s elapsed)
  MUT     time    0.979s  (  0.987s elapsed)
  GC      time    0.033s  (  0.038s elapsed)
  EXIT    time    0.000s  (  0.000s elapsed)
  Total   time    1.013s  (  1.024s elapsed)

  %GC     time       3.2%  (3.7% elapsed)

  Alloc rate    3,840,757,815 bytes per MUT second

  Productivity  96.7% of total user, 95.6% of total elapsed

./ExpAlloc 44 +RTS -s  1.01s user 0.01s system 99% cpu 1.029 total
</code></pre>

<p>I cannot for the life of me figure out why it does this. I'd appreciate any light people could shed on the situation.</p>
"
"43136202","tfidfvectorizer prints results based on all words","2017-03-31 08:38:42","1","3411","0","1","","43137081","<p>Though there are six different words. There are only 5 words printed in the result. How to get result based on all words  (6 columns vector)?</p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
sent=[""This is a sample"", ""This is another example""]
tf = TfidfVectorizer(analyzer='word', ngram_range=(1,1), min_df = 0)
tfidf_matrix =  tf.fit_transform(sent)
print tfidf_matrix.toarray()
</code></pre>

<blockquote>
  <p>[[ 0.          0.          0.50154891  0.70490949  0.50154891]  [
  0.57615236  0.57615236  0.40993715  0.          0.40993715]]</p>
</blockquote>

<p>Also how to print the column details (features (words)) and row (document) ?</p>
"
"43074949","How word2vec retrieves result from binary files?","2017-03-28 16:25:57","0","915","2","1","","43078178","<pre><code>from gensim.models.keyedvectors import KeyedVectors
model = KeyedVectors.load_word2vec_format('google_news.bin', binary=True)
print(model['the']) # this prints the 300D vector for the word 'the'
</code></pre>

<p>the code loads the google_news binary file to model. 
my question is, how the line 3 computes the output from a binary file ( Since Binary files contains 0's and 1's).</p>
"
"43037697","Generating PCFG from Universal tagset","2017-03-27 03:54:59","2","557","5","1","","43038238","<p>I am trying to build a PCFG using the POS tags obtained from the below code: </p>

<pre><code>from nltk.corpus import treebank

corpus = treebank.tagged_sents(tagset='universal')
tags = set()

for sent in corpus:
    for (word, tag) in sent: 
        tags.add(tag)

tags = list(tags)
print tags
</code></pre>

<p>Gives, </p>

<pre><code>['ADV', 'NOUN', 'ADP', 'PRON', 'DET', '.', 'PRT', 'NUM', 'X', 'CONJ', 'ADJ', 'VERB']
</code></pre>

<p>I need to generate a PCFG using the POS tags above. But, when I try to construct a grammar using the rule </p>

<pre><code>nltk.grammar.PCFG.fromstring(""""""T5 -&gt; . NT6 [0.136235]"""""")
</code></pre>

<p>it produces</p>

<pre><code>ValueError: Unable to parse line 1: T5 -&gt; . NT6 [0.136235]
Expected a nonterminal, found: . NT6 [0.136235]
</code></pre>

<p>I assume that the exception indicates that ""."" is not a valid non-terminal in <code>nltk.grammar.PCFG</code>. But I am wondering if there is a neat way to fix this. </p>

<p><strong>Related</strong> </p>

<p><a href=""https://stackoverflow.com/questions/33207799/nltk-cant-interpret-grammar-category-prp-output-by-stanford-parser"">nltk cant interpret grammar category PRP$ output by stanford parser</a> gives a nice fix to add '$' from treebank tagset to the grammar. But then again treebank POS tagset contains single quotes (' ') as a POS tag, which is not a valid symbol. </p>

<p>Is there a neat work around for this problem without having to add each special character to the grammar?</p>
"
"43026129","How can I lemmatize english words (example: 'run' and 'ran') using R to bring them all to the same tense?","2017-03-26 07:18:09","1","932","3","1","","43028769","<p>I want to lemmatize english words such that all of them get converted to the same tense. For example:</p>

<pre><code>c(""ran"",""run"",""running"") 
</code></pre>

<p>should become <code>c(""run"",""run"",""run"")</code>.</p>

<p>I have already explored R packages such as tm, wordnet, RTextTools, and Snowball C; but all of them result in the output <code>c(""ran"",""run"",""run"")</code>. As you can see, they do not convert ""ran"" to ""run"". </p>
"
"43022047","Where in the CoreNLP code are the Penn Treebank part-of-speech symbols themselves actually represented?","2017-03-25 21:16:03","3","253","0","2","","43033034","<p>I'm looking specifically for some data structure, enum, or generative process through which the different parts-of-speech are represented internally. I've spent a long time scanning the Javadoc and the source code for a while and can't find what I'm looking for. I would like to access a collection of the tags directly, if possible, if they're stored in some central location. Please forgive me if the question I'm posing constitutes a naive assumption regarding the way CoreNLP pos-tagging operates, but if what I'm describing does exist in some form, this would be very helpful. Thanks!</p>
"
"42966067","NLTK Chart Parser is not printing","2017-03-23 02:16:55","0","2584","3","1","","42966837","<p>Here is the code:</p>

<pre><code>groucho_grammar = nltk.CFG.fromstring(""""""
S -&gt; V NP PP CONJ V NP PP
PP -&gt; PRP NP 
NP -&gt; Det N | PRP N |DET ADJ CONJ ADJ N P 
Det -&gt; 'a' | 'every' | 'all'
N -&gt; 'work'  | 'Word Document' | 'results' | 'step'
ADJ -&gt; 'intermediate' | 'final'
V -&gt; 'Describe' | 'present' 
P -&gt; 'of' | 'in'
CONJ -&gt; 'and'
PRP -&gt; 'your'
"""""")

sent = ['Describe', 'every', 'step' ,'of', 'your', 'work', 'and' ,\
        'present', 'all', 'intermediate' ,'and' ,'final', 'results', 'in' ,'a', 'Word Document']
parser = nltk.ChartParser(groucho_grammar)
for tree in parser.parse(sent):
    print(tree)
</code></pre>

<p>When I do this, It runs without any errors but it doesn't print any grammar trees. I'm not sure what I am doing wrong here. I followed the guidelines in the nltk book but that hasnt helped. </p>
"
"42933127","Parsing terminal symbols with partial match","2017-03-21 16:35:45","0","106","0","1","","42933250","<p>I have a grammar which is something like:</p>

<pre><code>S -&gt; 'My' 'age' 'is' NUM
NUM -&gt; '18', '20'
</code></pre>

<p>the parser I build with such grammar can parse sentences like <code>My age is 18</code> but if my actual sentences is something like <code>My age is 20&gt;</code> -- where <code>20&gt;</code> meaning more than 20 -- it fails.</p>

<p>So I am asking: is there any way to force the parser to accept partial matches on terminal symbols?</p>

<p>Thanks in advance!</p>

<p>P.S. I am quite a noob with <code>ntlk</code> so maybe I am approaching the problem in the worst way possible. Any hint would be extremely appreciated.</p>
"
"42865623","How to output NLTK pos_tag in the string instead of a list?","2017-03-17 19:24:47","0","3021","6","1","","42868773","<p>I need to run nltk.pos_tag on a large dataset and need to have its output like the one that is offered by Stanford tagger.</p>

<p>For example while running the following code I have;</p>

<pre><code>import nltk
text=nltk.word_tokenize(""We are going out.Just you and me."")
print nltk.pos_tag(text)
</code></pre>

<p>the output is: 
[('We', 'PRP'), ('are', 'VBP'), ('going', 'VBG'), ('out.Just', 'IN'), ('you', 'PRP'), ('and', 'CC'), ('me', 'PRP'), ('.', '.')]</p>

<p>In the case that I need it to be like:</p>

<pre><code> We/PRP are/VBP going/VBG out.Just/NN you/PRP and/CC me/PRP ./.
</code></pre>

<p>I prefer to not using string functions and need a dirrect output because the amount of the text is so high and it adds lots of time complexities to the processing</p>
"
"42835852","Which comes first in order of implementation: POS Tagging or Lemmatisation?","2017-03-16 13:44:56","3","1185","0","2","","42841898","<p>If I wanted to make a NLP Toolkit like NLTK, which features would I implement first after tokenisation and normalisation. POS Tagging or Lemmatisation?</p>
"
"42798439","Latent Semantic Analysis and Stemming","2017-03-14 23:30:00","0","322","3","1","","56260103","<p>Assume a very large corpus of any inflective language. Does the following make sense? By applying LSA on such corpus, words with similar concepts converge together in vector space, thus inflected word forms reffering to the same concept should ideally be identical with their lemma in the space. With such assumption, any lemmatization or stemming of queries or corpus is not necessary. Or am i totally wrong?</p>
"
"42638034","Detecting if text is non-English","2017-03-07 00:23:25","-5","2211","9","2","","42638413","<p>What is the most accurate method for detecting if a text (specifically Instagram comments) are non-English? I am happy to use any high-level language, such as Python, PHP, etc.</p>

<pre><code>$ sudo pip2 install guess_language
&gt;&gt;&gt; from guess_language import guessLanguage
&gt;&gt;&gt; guessLanguage('la vita e bella')
'UNKNOWN'
&gt;&gt;&gt; guessLanguage('today is a good day')
'UNKNOWN'
&gt;&gt;&gt; guessLanguage('ボウリング・フォー・コロンバイン(字幕版)')
'ja'
</code></pre>

<p>and with </p>

<pre><code>$ sudo apt-get install php5.6-mbstring

      if(strlen($comment-&gt;text) == mb_strlen($comment-&gt;text, 'utf-8')) {
         echo '- '.$comment-&gt;text.""\n"";
    }
</code></pre>

<p>I get many things with English character which are not English:
examples:</p>

<pre><code>- Khoda be khanevadehashon sabr bede tahamol konan
- Akhey...
- Eshghi
- K
- :-)
- Ey khodaa
- ...
- @samaneaghazamani1990 vaaaaay khoda chejoori payam dadan?
- :(
- Elahiiiii
- May Allah please with them and grant higher rank of jannah salutes to the  bravehearts @taraneh_alidoosti @fanpagemostafazamani
- Elaaaahiii
- Roohetoon shad.
- :'(
- Roheshon shad!! Yadeshon gerami!!
- .:'(
- :-(
- Oooo
- Awli
</code></pre>

<p>I don't want to use something like Google Translate as I am dealing with lots of data. </p>

<p>Update:</p>

<pre><code>$ sudo pip2 install langdetect
&gt;&gt;&gt; from langdetect import detect
&gt;&gt;&gt; detect(""War doesn't show who's right, just who's left."")
'en'
&gt;&gt;&gt; detect(""today is a good day."")
'so'
&gt;&gt;&gt; detect(""la vita e bella!"")
'it'
&gt;&gt;&gt; detect(""khoobi? khoshi?"")
'so'
&gt;&gt;&gt; detect(""wow"")
'pl'
&gt;&gt;&gt; detect(""what a day"")
'en'
&gt;&gt;&gt; detect(""yay!"")
'so'
</code></pre>

<p>Does 'so' refer to unknown? I was expecting that <code>today is a good day</code> be considered as <code>en</code>!</p>
"
"42570593","Why is one set of tagged not parsing?","2017-03-03 04:20:34","0","186","2","1","","42575018","<p>So I'm supposed to chunk some tagged sentences from the WSJ corpus using my very simple parser. When i pos tag the sentences myself it works...but using their given way to get the tagged sentences do not.</p>

<p>My assignment told me to use sentences 200–220 of the tagged WSJ corpus nltk.corpus.treebank.tagged_sents(). however my parser is giving me an error.</p>

<p>My code which works(Manually tagging the sentences <em>which works</em>):</p>

<pre><code>tbss = concat(treebank.sents()[200:220])
tag1 = nltk.pos_tag(tbss)
print(cp.parse(tag1))
</code></pre>

<p>Using their code which doesn't work:</p>

<pre><code>tag2 = nltk.corpus.treebank.tagged_sents()[200:220]
print(cp.parse(tag2))
&gt;&gt;&gt; ValueError: chunk structures must contain tagged tokens or trees
</code></pre>

<p>Why exactly is the second one giving that error? I did a print of both tag 1 and tag 2 and they look almost identical...so why is one parsing and not the other...am I doing something wrong?</p>
"
"42513179","Training Stanford POS tagger using multiple text files","2017-02-28 15:50:02","0","116","0","1","","42589502","<p>I have a corpus of about 20000 text files and i want to train the tagger using these text files, which is better,to group these text files into one text file(i don't know if it will affect tagging accuracy or not) or to include all these text files in the props file?</p>
"
"42452205","Create list from list with function in pandas dataframe","2017-02-25 05:07:30","0","1623","1","1","","42452319","<p>I would like to create a new pandas column by running a word stemming function over a list of words in another column. I can tokenize a single string by using apply and lambda, but I cannot figure out how to extrapolate this to the case of running it over a list of words.</p>

<pre><code>test = {'Statement' : ['congratulations on the future','call the mechanic','more text'], 'Other' : [2,3,4]}
df = pd.DataFrame(test)
df['tokenized'] = df.apply (lambda row: nltk.word_tokenize(row['Statement']), axis=1)
</code></pre>

<p>I know I could solve it with a nested for loop, but that seems inefficient and results in a SettingWithCopyWarning:</p>

<pre><code>df['stems'] = ''
for x in range(len(df)):
    print(len(df['tokenized'][x]))
    df['stems'][x] = row_stems=[]
    for y in range(len(df['tokenized'][x])):
        print(df['tokenized'][x][y])
        row_stems.append(stemmer.stem(df['tokenized'][x][y]))
</code></pre>

<p>Isn't there a better way to do this?</p>

<p>EDIT:</p>

<p>Here's an example of what the result should look like:</p>

<pre><code>    Other     Statement                       tokenized                             stems 
0   2         congratulations on the future   [congratulations, on, the, future]    [congratul, on, the, futur]
1   3         call the mechanic               [call, the, mechanic]                 [call, the, mechan]
2   4         more text                       [more, text]                          [more, text]
</code></pre>
"
"42441257","Issues Regarding Training Maltparser Model","2017-02-24 14:34:25","-1","290","0","1","","42442303","<p>I am trying to train a Maltparser Model for Bangla. I have annotated a small Corpus in Conllu Format. But it it gives me null pointer error. So i tried it with some treebank collected from UD website. And it works on those dataset. My questions are</p>

<ol>
<li><p>Can i train Maltparser Model without XPOSTAG, i have annotated the UPOSTAG field and XPOSTAG field is just copies of UPOSTAG. Do i need to annotate XPOSTAG? This is the only difference between my treebank and UD treebank</p></li>
<li><p>As it is for evaluation purpose can i automatically convert UPOSTAG to XPOSTAG?</p></li>
</ol>

<p>ref: <a href=""http://universaldependencies.org/format.html"" rel=""nofollow noreferrer"">http://universaldependencies.org/format.html</a></p>

<p>For better understanding i am giving example of both my bank and UD bank</p>

<p>My Example Bank(There are mistakes and some empty fields)(Language is Bangla)</p>

<pre><code>1   Ajake   _   NOUN    NOUN    _   5   iobj    _   _
2   rAtera  _   NOUN    NOUN    _   1   nmod    _   _
3   AbahAoYA    _   NOUN    NOUN    _   5   nsubj   _   _
4   kemana  _   ADV ADV _   5   advmod  _   _
5   hate    _   VERB    VERB    _   0   root    _   _
6   pAre    _   AUX AUX _   5   aux _   SpaceAfter=No
7   ?   _   _   _   _   _   _   _   _

1   Ajake   _   NOUN    NOUN    _   5   iobj    _   _
2   bikAlera    _   NOUN    NOUN    _   1   nmod    _   _
3   paribesha   _   NOUN    NOUN    _   5   nsubj   _   _
4   kemana  _   ADV ADV _   5   advmod  _   _
5   hate    _   VERB    VERB    _   0   root    _   _
6   pAre    _   AUX AUX _   5   aux _   SpaceAfter=No
7   ?   _   _   _   _   _   _   _   _
</code></pre>

<p>UD Bank</p>

<pre><code>1   From    _   ADP IN  _   3   case    _   _
2   the _   DET DT  _   3   det _   _
3   AP  _   PROPN   NNP _   4   nmod    _   _
4   comes   _   VERB    VBZ _   0   root    _   _
5   this    _   DET DT  _   6   det _   _
6   story   _   NOUN    NN  _   4   nsubj   _   _
7   :   _   PUNCT   :   _   4   punct   _   _

1   President   _   PROPN   NNP _   2   compound    _   _
2   Bush    _   PROPN   NNP _   5   nsubj   _   _
3   on  _   ADP IN  _   4   case    _   _
4   Tuesday _   PROPN   NNP _   5   nmod    _   _
5   nominated   _   VERB    VBD _   0   root    _   _
6   two _   NUM CD  _   7   nummod  _   _
7   individuals _   NOUN    NNS _   5   dobj    _   _
8   to  _   PART    TO  _   9   mark    _   _
9   replace _   VERB    VB  _   5   advcl   _   _
10  retiring    _   VERB    VBG _   11  amod    _   _
11  jurists _   NOUN    NNS _   9   dobj    _   _
12  on  _   ADP IN  _   14  case    _   _
13  federal _   ADJ JJ  _   14  amod    _   _
14  courts  _   NOUN    NNS _   11  nmod    _   _
15  in  _   ADP IN  _   18  case    _   _
16  the _   DET DT  _   18  det _   _
17  Washington  _   PROPN   NNP _   18  compound    _   _
18  area    _   NOUN    NN  _   14  nmod    _   _
19  .   _   PUNCT   .   _   5   punct   _   _
</code></pre>
"
"42382446","What is the difference between Viterbi CYK and Probabilistic CYK algorithm, Is there any differences?","2017-02-22 03:58:25","1","1037","0","1","","42384138","<p>I think they are the same concept, 
<a href=""https://courses.engr.illinois.edu/cs498jh/Slides/Lecture10.pdf"" rel=""nofollow noreferrer"">https://courses.engr.illinois.edu/cs498jh/Slides/Lecture10.pdf</a>
Probabilistic CYK algorithm is used the viterbi algorithm to parse, is my concept is correct?</p>
"
"42282254","How to get minimum sentences from sentences corpus whose words covers the maximum sentences in the original corpus?","2017-02-16 18:47:52","0","121","3","1","","42303030","<p>everyone.</p>

<p>I have one ""optimization"" issue and I don't really know which way I should set off. Here's description of my problem:</p>

<p>I have a corpus with plenty of text sentences. Now, I need to obtain the minimum of sentences to record (as audio files) but at the same time to maximize the number of sentences in the original corpus formed from the recorded sentences - more exactly from recorded words.</p>

<p>A very short example of what I need to do:</p>

<p><strong>Corpus:</strong></p>

<ul>
<li>black dog</li>
<li>grey cat</li>
<li>big dog</li>
<li>grey mouse</li>
<li>big mouse</li>
</ul>

<p><strong>Example of minimum sentences to cover the maximum of the original corpus:</strong></p>

<ul>
<li>black dog</li>
<li>big mouse</li>
<li>grey cat</li>
</ul>

<p>From 3 sentences (and their words) above we are able to form the rest of sentences in corpus. Of course, I'm looking for some method computationally optimal because my corpus contains thousands of sentences. Do you know any method that is appropriate for this issue?</p>

<p>Thanks for your answers!</p>

<p>Morphid</p>
"
"42220764","Elasticsearch: getting the tf-idf of every term in a given document","2017-02-14 08:02:37","5","13952","2","2","","42227319","<p>I have a document in my elasticsearch with the following id: <code>AVosj8FEIaetdb3CXpP-</code> I'm trying to access for every words in the fields it's tf-idf I did the following:</p>

<pre><code>GET /cnn/cnn_article/AVosj8FEIaetdb3CXpP-/_termvectors
{
  ""fields"" : [""author_wording""],
  ""term_statistics"" : true,
  ""field_statistics"" : true
}'
</code></pre>

<p>The response I've got is:</p>

<pre><code>{
  ""_index"": ""dailystormer"",
  ""_type"": ""dailystormer_article"",
  ""_id"": ""AVosj8FEIaetdb3CXpP-"",
  ""_version"": 3,
  ""found"": true,
  ""took"": 1,
  ""term_vectors"": {
    ""author_wording"": {
      ""field_statistics"": {
        ""sum_doc_freq"": 3408583,
        ""doc_count"": 16111,
        ""sum_ttf"": 7851321
      },
      ""terms"": {
        ""318"": {
          ""doc_freq"": 4,
          ""ttf"": 4,
          ""term_freq"": 1,
          ""tokens"": [
            {
              ""position"": 121,
              ""start_offset"": 688,
              ""end_offset"": 691
            }
          ]
        },
        ""742"": {
          ""doc_freq"": 1,
          ""ttf"": 1,
          ""term_freq"": 1,
          ""tokens"": [
            {
              ""position"": 122,
              ""start_offset"": 692,
              ""end_offset"": 695
            }
          ]
        },
        ""9971"": {
          ""doc_freq"": 1,
          ""ttf"": 1,
          ""term_freq"": 1,
          ""tokens"": [
            {
              ""position"": 123,
              ""start_offset"": 696,
              ""end_offset"": 700
            }
          ]
        },
        ""a"": {
          ""doc_freq"": 14921,
          ""ttf"": 163268,
          ""term_freq"": 11,
          ""tokens"": [
            {
              ""position"": 1,
              ""start_offset"": 13,
              ""end_offset"": 14
            },
            ...
            ""you’re"": {
          ""doc_freq"": 1112,
          ""ttf"": 1647,
          ""term_freq"": 1,
          ""tokens"": [
            {
              ""position"": 80,
              ""start_offset"": 471,
              ""end_offset"": 477
            }
          ]
        }
      }
    }
  }
}
</code></pre>

<p>It returns me some interesting fields like the term frequency (tf) but not the tf-idf. Should I recompute it myself? Is that a good idea? How can I do so?</p>
"
"42212423","Python tf-idf: fast way to update the tf-idf matrix","2017-02-13 19:54:33","8","5828","0","2","","43880304","<p>I have a dataset of several thousand rows of text, my target is to calculate the tfidf score and then cosine similarity between documents, this is what I did using gensim in Python followed the tutorial:</p>

<pre><code>dictionary = corpora.Dictionary(dat)
corpus = [dictionary.doc2bow(text) for text in dat]

tfidf = models.TfidfModel(corpus)
corpus_tfidf = tfidf[corpus]
index = similarities.MatrixSimilarity(corpus_tfidf)
</code></pre>

<p>Let's say we have the tfidf matrix and similarity built, when we have a new document come in, I want to query for its most similar document in our existing dataset.</p>

<p>Question: is there any way we can update the tf-idf matrix so that I don't have to append the new text doc to the original dataset and recalculate the whole thing again? </p>
"
"42206539","NLTK: How to access the chunked string","2017-02-13 14:32:35","1","532","3","1","","42207736","<p>I am using NLTK chunking and i want to capture the string that was matched for my rule. For example</p>

<p>Here is my input</p>

<p><code>The stocks show 67% rise, last year it was 12% fall</code></p>

<p>i want to capture </p>

<p><code>67% rise</code> and <code>12% fall</code></p>

<p>POS Tagging the above sentence shows</p>

<pre><code>('The', 'DT'), ('stocks', 'NNS'), ('show', 'VBP'), ('67', 'CD'), ('%', 'NN'), ('rise', 'NN'), (',', ','), ('last', 'JJ'), ('year', 'NN'), ('it', 'PRP'), ('was', 'VBD'), ('12', 'CD'), ('%', 'NN'), ('fall', 'NN')
</code></pre>

<p>Now, i came up with a simple rule</p>

<p><code>Stat: {&lt;CD&gt;&lt;NN&gt;(&lt;NN&gt;+|&lt;VBN&gt;|JJ)?}</code></p>

<p>which works well and captures</p>

<pre><code>('67', 'CD'), ('%', 'NN'), ('rise', 'NN')

('12', 'CD'), ('%', 'NN'), ('fall', 'NN')
</code></pre>

<p>now, i want to extract the exact strings that were captured. So, i want </p>

<p><code>67% rise</code> and <code>12% fall</code></p>

<p>i tried </p>

<pre><code>current=[]
for word,tag in subtree.leaves():
    current.append(word)
print ' '.join(current)
</code></pre>

<p>but i get </p>

<p><code>67 % rise</code> and <code>12 % fall</code></p>

<p>notice the space between <code>%</code> and the numbers. This is logically correct but not the desired output. I want the exact string as i want to know the starting and ending indices of the captured strings.</p>

<p>How can i achieve this?</p>
"
"42204887","Which natural languages are supported by Google Cloud Natural Language API?","2017-02-13 13:11:16","0","3688","1","2","","42206289","<p>I can't find this crucial information in the official docs or here.
Which natural languages are supported by the service? Which languages will be supported in the near future?
I'm interested in POS tagging with lemmatization and dependency parsing. I tried analysing Russian, Polish and even Italian sentences and got a 400s (“The language ru is not supported for syntax analysis” etc.).
If advertising a service as multilingual, it would be fair to admit which languages are supported before having to register and enter credit card details.</p>
"
"42194446","nltk semantic parsing with coordination","2017-02-12 23:21:38","1","470","0","1","","42215940","<p>I'm trying to build a semantic parser using python's NLTK library and following Neo-Davidsonian event representation.  I've built up my grammar to include semantic features that parse correctly but I'm struggling with coordinated constituents.  For example, my grammar has:</p>

<pre><code>PropN[SEM=&lt;\P.P(Mary)&gt;] -&gt; 'Mary'
PropN[SEM=&lt;\P.P(John)&gt;] -&gt; 'John'

IV[SEM=&lt;\x.exists e.(drinks(e) &amp; drinker(e, x))&gt;] -&gt; 'drinks'

NP[SEM=?np] -&gt; PropN[SEM=?np]
VP[SEM=?v] -&gt; IV[SEM=?v]
S[SEM=&lt;?subj(?vp)&gt;] -&gt; NP[SEM=?subj] VP[SEM=?vp]
</code></pre>

<p>So if the sentences is ""John drinks"" the result is: </p>

<pre><code>exists e.(drinks(e) &amp; drinker(e,John)).
</code></pre>

<p>But if I add a rule such as: </p>

<pre><code>NP[SEM=&lt;?p | ?q&gt;] -&gt; PropN[SEM=?p] CONJ PropN[SEM=?q]
</code></pre>

<p>as in ""John or Mary"", I end up getting this: </p>

<pre><code>(\P.P(John) | \P.P(Mary))(\x.exists e.(drinks(e) &amp; drinker(e,x)))  
</code></pre>

<p>As in, the NP lambdas are not being passed into the argument for the verb.  I tried looking around but there's very little info on coordination in nltk semantic parsing, and even less on using it with lambdas.  I know it's possible, because a classmate apparently got his to work but I'm not sure what the trick is.</p>
"
"42160954","Part-of-speech without Python","2017-02-10 13:48:25","0","73","1","1","","42207722","<p>I am trying to do tagging of a <em>french text</em>, but <code>TreeTagger</code> needs <code>Python</code> which is impossible to install on my PC at work. For security reasons, it is impossible to install other programs (only <code>R</code>).</p>

<p>Is it possible to use <code>R</code> code for tagging which does not require neither <code>java</code> nor <code>Python</code>?</p>
"
"42089517","English verbs processing ending with 'e'","2017-02-07 12:08:11","3","89","1","1","","42089742","<p>I am implementing few string replacers, with these conversions in mind</p>

<pre><code>'thou sittest' → 'you sit'
'thou walkest' → 'you walk'
'thou liest' → 'you lie'
'thou risest' → 'you rise'
</code></pre>

<p>If I keep it naive it is possible to use regex for this case to find &amp; replace, like <code>thou [a-z]+est</code></p>

<p>But the trouble comes in English verbs that end with <code>e</code> because based on the context I need to trim the <code>est</code> in some &amp; trim just <code>st</code> in the rest</p>

<p>What is the quick-dirty solution to achieve this? </p>
"
"42027252","Smart stemming/lemmatizing in Python for Nationalities","2017-02-03 15:07:25","4","1233","1","1","","42028084","<p>I am working with Python, and I would like to find the roots of some words, that mainly refer to countries. Some examples that demonstrate what I need are:</p>

<ul>
<li>Spanish should give me Spain.</li>
<li>English should give me England.</li>
<li>American should give me America.</li>
<li>Nigerian should give me Nigeria.</li>
<li>Greeks (plural) should give me Greece.</li>
<li>Puerto Ricans (plural) should give me Puerto Rico.</li>
<li>Portuguese should give me Portugal.</li>
</ul>

<p>I have experimented a bit with the Porter, Lancaster and Snowball stemmers of the NLTK module. But Porter and Snowball do not change the tokens at all, while Lancaster is too aggressive. For example, the Lancaster stem of American is ""Am"", which is pretty badly butchered.I have also played some with the WordNet lemmatizer, with no success. </p>

<p>Is there a way to get the above results, even if it only works for countries?</p>
"
"41937898","Dependency Parsing graph for a paragraph","2017-01-30 14:07:30","1","470","0","1","","44651648","<p>I am working on a NLP project. I want to create a dependency parsing graph for an entire paragraph, instead of sentence. Is there an existing method for the same? </p>
"
"41793842","WordNet Python words similarity","2017-01-22 17:14:46","6","11079","0","1","","41794714","<p>I'm trying to find a reliable way to measure the semantic similarity of 2 terms.
The first metric could be the path distance on a hyponym/hypernym graph (eventually a linear combination of 2-3 metrics could be better..).</p>

<pre><code>from nltk.corpus import wordnet as wn
dog = wn.synset('dog.n.01')
cat = wn.synset('cat.n.01')
print(dog.path_similarity(cat))
</code></pre>

<ul>
<li>I still don't get what <code>n.01</code> means and why it's necessary.</li>
<li>there is a way to visually show the computed path between 2 terms?</li>
<li>Which other nltk semantic metric could I use?</li>
</ul>
"
"41749471","What does ""document"" mean in a NLP context?","2017-01-19 18:48:56","3","2969","0","2","","41749579","<p>As I was reading about <a href=""https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Inverse_document_frequency"" rel=""nofollow noreferrer"">tf–idf</a> on Wiki, I was confused by what it means by the word ""document"". Does it mean paragraph?</p>

<p>""The inverse document frequency is a measure of how much information the word provides, that is, whether the term is common or rare across all documents. It is the logarithmically scaled inverse fraction of the documents that contain the word, obtained by dividing the total number of documents by the number of documents containing the term, and then taking the logarithm of that quotient.""</p>
"
"41707679","it-idf with TfidfVectorizer on Japanese text","2017-01-17 21:53:07","2","2425","2","2","","41708013","<p>I am working with a huge collection of documents written in several languages. I want to compute cosine distance between documents from their tf-idf scores. So far I have:</p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer

# The documents are located in the same folder as the script
text_files = [r'doc1', r'doc2', r'doc3'] 
files = [open(f) for f in text_files]
documents = [f.read() for f in files]

vectorizer = TfidfVectorizer(ngram_range=(1,1))
tfidf = vectorizer.fit_transform(documents)
vocabulary = vectorizer.vocabulary_
</code></pre>

<p>When the three documents <code>doc1</code>, <code>doc2</code> and <code>doc3</code> contain English text, the algorithm works like a charm and <code>vocabulary</code> does indeed contains unigrams from the different bodies of text. I tried with Russian too, and it also worked great. However, when I try with some Japanese text, the algorithm does not work as intended anymore.</p>

<p>The problem arises from the fact that Japanese language does not have spaces, so that TfidfVectorizer does not understand what's a word and what isn't. For example I would have something like this in my unigram vocabulary:</p>

<blockquote>
  <p>診多索いほ権込真べふり告車クノ般宮えぼぜゆ注携ゆクく供9時ク転組けが意見だっあ税新ト復生ひり教台話辞ゃに</p>
</blockquote>

<p>Whic is clearly a sentence and not a word. How can I solve this problem?</p>
"
"41674573","How to apply pos_tag_sents() to pandas dataframe efficiently","2017-01-16 10:46:33","15","12676","6","3","","42081746","<p>In situations where you wish to POS tag a column of text stored in a pandas dataframe with 1 sentence per row the majority of implementations on SO use the apply method</p>

<pre><code>dfData['POSTags']= dfData['SourceText'].apply(
                 lamda row: [pos_tag(word_tokenize(row) for item in row])
</code></pre>

<p>The NLTK documentation <a href=""http://www.nltk.org/api/nltk.tag.html#nltk.tag.pos_tag"">recommends using the pos_tag_sents()</a> for efficient tagging of more than one sentence.</p>

<p>Does that apply to this example and if so would the code be as simple as changing <code>pso_tag</code> to <code>pos_tag_sents</code> or does NLTK mean text sources of paragraphs </p>

<p>As mentioned in the comments <code>pos_tag_sents()</code> aims to reduce the loading of the preceptor each time <strong>but the issue is how to do this and still produce a column in a pandas dataframe?</strong></p>

<p><a href=""http://www.sharecsv.com/dl/1ec7e9ad579c3e0a4eb5fa7c16164c6d/20kSampleTextData.csv"">Link to Sample Dataset 20kRows</a> </p>
"
"41575278","Is there a BNF with arguments for non-terminal symbols?","2017-01-10 17:54:53","4","365","1","1","","41587352","

<p>In working with Prolog DCG to parse input it is nice to have an accompaning BNF of the grammar. </p>

<p>For example:</p>

<p>BNF</p>

<pre class=""lang-erlang prettyprint-override""><code>&lt;Sentence&gt; ::= &lt;Noun_phrase&gt; &lt;Verb_phrase&gt;
&lt;Noun_phrase&gt; ::= &lt;Determiner&gt; &lt;Noun&gt;
&lt;Verb_phrase&gt; ::= &lt;Verb&gt; &lt;Phrase&gt;
&lt;Determiner&gt; ::= a
&lt;Determiner&gt; ::= the
&lt;Noun&gt; ::= cat
&lt;Noun&gt; ::= mouse
&lt;Verb&gt; ::= scares
&lt;Verb&gt; ::= hates
</code></pre>

<p>as Prolog DCG</p>

<pre class=""lang-erlang prettyprint-override""><code>sentence --&gt; noun_phrase, verb_phrase.
verb_phrase --&gt; verb, noun_phrase.
noun_phrase --&gt; determiner, noun.
determiner --&gt; [a].
determiner --&gt; [the].
noun --&gt; [cat].
noun --&gt; [mouse].
verb --&gt; [scares].
verb --&gt; [hates].
</code></pre>

<p>However Prolog DCG can also have arguments as<br>
in this example <code>Number</code> for <code>singular</code> or <code>plural</code></p>

<pre class=""lang-erlang prettyprint-override""><code>sentence(Number) --&gt; noun_phrase(Number), verb_phrase(Number).
verb_phrase(Number) --&gt; verb(Number), noun_phrase(Number).
noun_phrase(Number) --&gt; determiner(Number), noun(Number).
determiner(singular) --&gt; [a].
determiner(singular) --&gt; [the].
determiner(plural) --&gt; [the].
noun(singular) --&gt; [cat].
noun(plural) --&gt; [cats].
noun(singular) --&gt; [mouse].
noun(plural) --&gt; [mice].
verb(singular) --&gt; [scares].
verb(plural) --&gt; [scare].
verb(singular) --&gt; [hates].
verb(plural) --&gt; [hate].
</code></pre>

<p>Is there a standard or accepted extension to BNF that includes arguments for non-terminals?</p>

<p>If so I need a link to it.</p>

<p>I suspect that ATN (Augmented Transition Networks) are in the ball park and may be the only standard answer, but I am hoping for something that is linear text as opposed to some form vertex/edge graph.</p>
"
"41522476","Stanford Parser for Python: Output Format","2017-01-07 14:28:39","1","1864","2","1","","41522613","<p>I am currently using the Python interface for the Stanford Parser. </p>

<pre><code>    from nltk.parse.stanford import StanfordParser
    import os

    os.environ['STANFORD_PARSER'] ='/Users/au571533/Downloads/stanford-parser-full-2016-10-31'
    os.environ['STANFORD_MODELS'] = '/Users/au571533/Downloads/stanford-parser-full-2016-10-31'
    parser=StanfordParser(model_path=""edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz"")

    new=list(parser.raw_parse(""The young man who boarded his usual train that Sunday afternoon was twenty-four years old and fat. ""))
    print new
</code></pre>

<p>The output I get looks something like this: </p>

<pre><code>    [Tree('ROOT', [Tree('S', [Tree('NP', [Tree('NP', [Tree('DT', ['The']), Tree('JJ', ['young']), Tree('NN', ['man'])]), Tree('SBAR', [Tree('WHNP', [Tree('WP', ['who'])]), Tree('S', [Tree('VP', [Tree('VBD', ['boarded']), Tree('NP', [Tree('PRP$', ['his']), Tree('JJ', ['usual']), Tree('NN', ['train'])]), Tree('NP', [Tree('DT', ['that']), Tree('NNP', ['Sunday'])])])])])]), Tree('NP', [Tree('NN', ['afternoon'])]), Tree('VP', [Tree('VBD', ['was']), Tree('NP', [Tree('NP', [Tree('JJ', ['twenty-four']), Tree('NNS', ['years'])]), Tree('ADJP', [Tree('JJ', ['old']), Tree('CC', ['and']), Tree('JJ', ['fat'])])])]), Tree('.', ['.'])])])]
</code></pre>

<p>However, I only need the part of speech labels, therefore I'd like to have an output in a format that looks like word/tag. </p>

<p>In java it is possible to specify -outputFormat 'wordsAndTags' and it gives exactly what I want. Any hint on how to implement this in Python? </p>

<p>Help would be GREATLY appreciated. 
Thanks!</p>

<p>PS: Tried to use the Stanford POSTagger but it is by far less accurate on some of the words I'm interested in. </p>
"
"41520580","How can I cluster similar type of sentences based on their context and extract keywords from them","2017-01-07 10:48:43","1","879","0","2","","41575916","<p>I wanted to cluster sentences based on their context and extract common keywords from similar context sentences.</p>

<p>For example
1. I need to go to home 
2. I am eating 
3. He will be going home tomorrow 
4. He is at restaurant </p>

<p>Sentences 1 and 3 will be similar with keyword like go and home and maybe it's synonyms like travel and house .</p>

<p>Pre existing API will be helpful like using IBM Watson somehow</p>
"
"41517595","nltk stemmer: string index out of range","2017-01-07 03:48:43","15","4112","8","2","","41526359","<p>I have a set of pickled text documents which I would like to stem using nltk's <code>PorterStemmer</code>. For reasons specific to my project, I would like to do the stemming inside of a django app view.</p>

<p>However, when stemming the documents inside the django view, I receive an <code>IndexError: string index out of range</code> exception from <code>PorterStemmer().stem()</code> for the string <code>'oed'</code>. As a result, running the following:</p>

<pre><code># xkcd_project/search/views.py
from nltk.stem.porter import PorterStemmer

def get_results(request):
    s = PorterStemmer()
    s.stem('oed')
    return render(request, 'list.html')
</code></pre>

<p>raises the mentioned error:</p>

<pre><code>Traceback (most recent call last):
  File ""//anaconda/envs/xkcd/lib/python2.7/site-packages/django/core/handlers/exception.py"", line 39, in inner
    response = get_response(request)
  File ""//anaconda/envs/xkcd/lib/python2.7/site-packages/django/core/handlers/base.py"", line 187, in _get_response
    response = self.process_exception_by_middleware(e, request)
  File ""//anaconda/envs/xkcd/lib/python2.7/site-packages/django/core/handlers/base.py"", line 185, in _get_response
    response = wrapped_callback(request, *callback_args, **callback_kwargs)
  File ""/Users/jkarimi91/Projects/xkcd_search/xkcd_project/search/views.py"", line 15, in get_results
    s.stem('oed')
  File ""//anaconda/envs/xkcd/lib/python2.7/site-packages/nltk/stem/porter.py"", line 665, in stem
    stem = self._step1b(stem)
  File ""//anaconda/envs/xkcd/lib/python2.7/site-packages/nltk/stem/porter.py"", line 376, in _step1b
    lambda stem: (self._measure(stem) == 1 and
  File ""//anaconda/envs/xkcd/lib/python2.7/site-packages/nltk/stem/porter.py"", line 258, in _apply_rule_list
    if suffix == '*d' and self._ends_double_consonant(word):
  File ""//anaconda/envs/xkcd/lib/python2.7/site-packages/nltk/stem/porter.py"", line 214, in _ends_double_consonant
    word[-1] == word[-2] and
IndexError: string index out of range
</code></pre>

<p>Now what is really odd is running the same stemmer on the same string outside django (be it a seperate python file or an interactive python console) produces no error. In other words:</p>

<pre><code># test.py
from nltk.stem.porter import PorterStemmer
s = PorterStemmer()
print s.stem('oed')
</code></pre>

<p>followed by:</p>

<pre><code>python test.py
# successfully prints 'o'
</code></pre>

<p>what is causing this issue?</p>
"
"41431572","Alternatives to TF-IDF and Cosine Similarity (comparing documents with different formats)","2017-01-02 18:19:40","3","3644","0","2","","41437935","<p>I've been working on a small, personal project which takes a user's job skills and suggests the most ideal career for them based on those skills.  I use a database of job listings to achieve this.  At the moment, the code works as follows:</p>

<p>1) Process the text of each job listing to extract skills that are mentioned in the listing</p>

<p>2) For each career (e.g. ""Data Analyst""), combine the processed text of the job listings for that career into one document</p>

<p>3) Calculate the TF-IDF of each skill within the career documents</p>

<p>After this, I'm not sure which method I should use to rank careers based on a list of a user's skills.  The most popular method that I've seen would be to treat the user's skills as a document as well, then to calculate the TF-IDF for the skill document, and use something like cosine similarity to calculate the similarity between the skill document and each career document.</p>

<p>This doesn't seem like the ideal solution to me, since cosine similarity is best used when comparing two documents of the same format.  For that matter, TF-IDF doesn't seem like the appropriate metric to apply to the user's skill list at all.  For instance, if a user adds additional skills to their list, the TF for each skill will drop.  In reality, I don't care what the frequency of the skills are in the user's skills list -- I just care that they have those skills (and maybe how well they know those skills).</p>

<p>It seems like a better metric would be to do the following:</p>

<p>1) For each skill that the user has, calculate the TF-IDF of that skill in the career documents</p>

<p>2) For each career, sum the TF-IDF results for all of the user's skill </p>

<p>3) Rank career based on the above sum</p>

<p>Am I thinking along the right lines here?  If so, are there any algorithms that work along these lines, but are more sophisticated than a simple sum? Thanks for the help!</p>
"
"41315082","Average POS-TAG Frequency","2016-12-24 16:42:06","0","841","1","1","","41346520","<p>I want to take this tagged text (formatted as such) and find the average frequency of the pos-tag DT in each sentence. ex. DT  appears 1/3 words in sentence1 and 1/3 words in sentence2. Then I want to add these up and divide by the number of sentences in the text (2 in this case). This will give me the average appearance of DT per sentence.</p>

<pre><code> from collections import Counter
 import nltk

 tagged_text = [('A', 'DT'), ('hairy', 'NNS'), ('dog', 'NN')]
 [('The', 'DT'), ('mischevious', 'NNS'), ('elephant', 'NN')]

 for eachSentence in tagged_text:
     Counter(tag for word,tag in tagged)/len(eachsentence.split())

 total = sum(counts.values())

 float(average) = sum(counts.values())/len(tagged_text.sents())
 print(float(average))
</code></pre>

<p>The big problem for me is the eachSentence part which I don't not how to get around (I don't know how to define what it is). I want this code to be able to be applied to hundreds of sentences that have the same format. I know there are a lot of problems with the code so if someone can please correct them I would be very grateful.</p>
"
"40957598","How can I retrieve the antonym synset of a target synset in NLTK's Wordnet?","2016-12-04 10:22:11","2","860","4","1","","40971112","<p>I have successfully retrieved synsets connected to a base synset via other semantic relations, as follows:</p>

<pre><code> wn.synset('good.a.01').also_sees()
 Out[63]: 
 [Synset('best.a.01'),
 Synset('better.a.01'),
 Synset('favorable.a.01'),
 Synset('good.a.03'),
 Synset('obedient.a.01'),
 Synset('respectable.a.01')]

wn.synset('good.a.01').similar_tos()
Out[64]: 
[Synset('bang-up.s.01'),
 Synset('good_enough.s.01'),
 Synset('goodish.s.01'),
 Synset('hot.s.15'),
 Synset('redeeming.s.02'),
 Synset('satisfactory.s.02'),
 Synset('solid.s.01'),
 Synset('superb.s.02'),
 Synset('well-behaved.s.01')]
</code></pre>

<p>However, the antonym relation seems different. I managed to retrieve the lemma connected to my base synset, but was not able to retrieve the actual synset, like so:</p>

<pre><code>wn.synset('good.a.01').lemmas()[0].antonyms()
Out[67]: [Lemma('bad.a.01.bad')]
</code></pre>

<p>How can I get the synset, and not the lemma, that is connected via antonymy to my base synset - wn.synset('good.a.01') ? TIA</p>
"
"40948117","Document similarity- Odd one out","2016-12-03 13:12:04","-1","69","1","1","","40952822","<p>Lets say I have ""n"" number of documents over a specific topic giving certain details. I want to get those documents who are not similar to the majority of the documents. As vague as this might seem, I know how to find cosine similarity between 2 documents. But lets say, I ""know"" I have 10 documents that are similar to each other, I introduce an 11th document and I need a way to judge how similar is this document with those 10 collectively and not just with every individual document. </p>

<p>I am working with scikit learn, so an answer or technique with its reference will help! </p>
"
"40860220","How to programmatically determine the Parts of Speech tag of a word?","2016-11-29 07:38:53","2","1370","1","3","","40863079","<p>Been wondering how to determine the POS tag of a word accurately. I played with POS taggers such as Stanford NLP etc, but they hit and miss as a word like ""respond"" is sometimes tagged as a NN (noun) when it is a verb (VB).</p>

<p>Would querying wordnet, or a dictionary dump be more accurate? Eg the word ""respond"" is a verb, and can also be a noun. Or perhaps infer from ngrams or add in a frequency based sanity check?</p>
"
"40853150","Find a word's class (POS tag) in Greek using python","2016-11-28 20:45:26","2","1292","5","2","","40863527","<p>I'm currently working on a document classification app. I use python along with NLTK and wordnet for English which enables me to find a word's class. My problem is that I currently haven't found a way to do the same thing in other languages such as Greek.</p>

<p>I found the Greek Wordnet in <a href=""http://wordnet.okfn.gr/downloads/"" rel=""nofollow noreferrer"">http://wordnet.okfn.gr/downloads/</a>. 
I saved the folder within NLTK corpora folder and tried to load it with:</p>

<pre><code>from nltk.corpus import wordnet-master
</code></pre>

<p>but I got an SyntaxError</p>

<pre><code>SyntaxError: invalid syntax
</code></pre>

<p>If I change the name to wordnet_master, I get an ImportError</p>

<pre><code>ImportError: cannot import name 'wordnet_master'
</code></pre>

<p>Is there any suggestion on how to import the Greek Wordnet? Thanks in advance</p>
"
"40788494","An Algorithm to Determine How Similar Two Sentences Are","2016-11-24 14:09:18","3","4060","4","3","","40929432","<p>A friend of mine had an idea to make a speed reading program that displays words one by one (much like currently existing speed reading programs). However, the program would filter out words that aren't completely necessary to the meaning (if you want to skim something). </p>

<p>I have starting to implement this program, but I'm not quite sure on what the algorithm to get rid of ""unimportant"" words should be. </p>

<p>My idea is to parse the sentence (I'm currently using Stanford Parser) and somehow assign weights based on how important that word is to the sentence's meaning to each word then start removing words with the with the lowest weights. I will continue to do this, check how ""different"" the original tree and the new tree is. I will continue to remove the word with the lowest weight until the two trees are too different (I will determine some constant via a ""calibration"" process that each user goes through once). Finally, I will go through each word of the shortened sentence and try to replace it with a simpler or shorter synonym for that word (again while still trying to retain value).</p>

<p>As well, there will be special cases for very common words like ""the,"" ""a,"" and ""of.""</p>

<p>For example:</p>

<p>""Billy said to Jane, 'Do you want to go out?'""</p>

<p>Would become:</p>

<p>""Billy told Jane 'want go out?'""</p>

<p>This would retain basically all of the meaning of the sentence but shortened it significantly. </p>

<p><strong>Is this a good idea for an algorithm and if so how would I assign the weights, what tree comparison algorithm should I use, and is inserting the synonyms done in a good place (i.e. should it be done before I try to remove any words)?</strong></p>
"
"40766816","Java: How to use TF-IDF to compute similarity of two documents?","2016-11-23 14:14:34","2","2797","4","1","","40783390","<p>My goals is to find a similarity value between two documents (collections of words). I have already found several answers like <a href=""https://stackoverflow.com/questions/8897593/similarity-between-two-text-documents"">this SO post</a> or <a href=""https://stackoverflow.com/questions/2380394/simple-implementation-of-n-gram-tf-idf-and-cosine-similarity-in-python"">this SO post</a> which provide Python libraries that achieve this, but I have trouble understanding the approach and making it work for my use case. </p>

<p>If I understand correctly, TF-IDF of a document is computed with respect to a given term, right? That's how I interpret it from the <a href=""https://en.wikipedia.org/wiki/Tf%E2%80%93idf"" rel=""nofollow noreferrer"">Wikipedia article</a> on this: ""tf-idf...is a numerical statistic that is intended to reflect how important a word is to a document"". </p>

<p>In my case, I don't have a specific search term which I want to compare to the document, but I have two different documents. I assume I need to first compute vectors for the documents, and then take the cosine between these vectors. But all the answers I found with respect to constructing these vectors always assume a search term, which I don't have in my case.</p>

<p>Can't wrap my head around this, any conceptual help or links to Java libraries that achieve this would be highly appreciated.</p>
"
"40640242","pyparsing.ParseException when using parseString (searchString works)","2016-11-16 19:03:11","5","1565","6","1","","40643027","<p>I'm trying to parse some Traffic Violation sentences using pyparsing, when I use <code>grammar.searchString(sentence)</code> it is ok, but when I use <code>parseString</code> a <code>ParseException</code>   is thrown. Can anybody help me please saying what is wrong with my code?</p>

<pre><code>from pyparsing import Or, Literal, oneOf, OneOrMore, nums, alphas, Regex, Word, \
    SkipTo, LineEnd, originalTextFor, Optional, ZeroOrMore, Keyword, Group
import pyparsing as pp

from nltk.tag import pos_tag

sentences = ['Failure to control vehicle speed on highway to avoid collision','Failure to stop at stop sign', 'Introducing additives into special fuel by unauthorized person and contrary to regulations', 'driver fail to stop at yield sign at nearest pointf approaching traffic view when req. for safety', 'Operating unregistered motor vehicle on highway', 'Exceeding maximum speed: 39 MPH in a posted 30 MPH zone']


for sentence in sentences:
    words = pos_tag(sentence.split())
    #print words
    verbs = [word for word, pos in words if pos in ['VB','VBD','VBG']]
    nouns = [word for word, pos in words if pos == 'NN']
    adjectives = [word for word, pos in words if pos == 'JJ']

    adjectives.append('great')  # initializing  
    verbs.append('get') # initializing 


    object_generator = oneOf('for to')
    location_generator = oneOf('at in into on onto over within')
    speed_generator = oneOf('MPH KM/H')

    noun = oneOf(nouns)
    adjective = oneOf(adjectives)

    location = location_generator + pp.Group(Optional(adjective) + noun)

    action = oneOf(verbs)
    speed = Word(nums) + speed_generator

    grammar =  action | location | speed

    parsed = grammar.parseString(sentence)

    print parsed
</code></pre>

<p>Error traceback</p>

<blockquote>
  <p>Traceback (most recent call last): File ""script3.py"", line 35, in  parsed = grammar.parseString(sentence) File ""/Users/alana/anaconda/lib/python2.7/site-packages/pyparsing‌​.py"", line 1032, in parseString raise exc pyparsing.ParseException: Expected Re:('control|avoid|get') (at char 0), (line:1, col:1)</p>
</blockquote>
"
"40631146","efficient way to calculate distance between combinations of pandas frame columns","2016-11-16 11:36:38","6","1969","6","2","","40633734","<p><strong>Task</strong></p>

<p>I have a pandas dataframe where:</p>

<ul>
<li>the columns are document names</li>
<li>the rows are words in those documents</li>
<li>numbers inside the frame cells are a measure of word relevance (word count if you want to keep it simple)</li>
</ul>

<p>I need to calculate a new matrix of doc1-doc similarity where:</p>

<ul>
<li>rows and columns are document names</li>
<li>the cells inside the frame are a similarity measure, (1 - cosine distance) between the two documents</li>
</ul>

<p>The cosine distance is conveniently provided by <a href=""https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html"" rel=""nofollow noreferrer"">script.spatial.distance.cosine</a>.</p>

<p>I'm currently doing this:</p>

<ol>
<li>use itertools to create a list of all 2-combinations of the document names (dataframe columns names)</li>
<li>loop over these and create a update a dictionary of {doc1: {doc2: similarity}} </li>
<li>after the loop, create a new frame using pandas.DataFrame(dict)</li>
</ol>

<p><strong>Problem</strong></p>

<p>But it takes a very very long time. The following shows current speed on a MacBook Pro 13 with 16GB ram and 2.9GHz i5cpu running latest anaconda python 3.5 ... plotting time taken against combinations of docs.</p>

<p><a href=""https://i.sstatic.net/0EZkE.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/0EZkE.png"" alt=""distance calculation performance""></a></p>

<p>You can see that 100,000 combinations takes 1200 seconds. Extrapolating that to my corpus of <strong>7944</strong> documents, which creates 3<strong>1,549,596</strong> combinations, would take <strong>5 days</strong> to calculate this similarity matrix!</p>

<p><strong>Any ideas?</strong></p>

<ul>
<li>I <a href=""http://makeyourowntextminingtoolkit.blogspot.co.uk/2016/11/more-performance-memory-stuff-to-fix.html"" rel=""nofollow noreferrer"">previously</a> was dynamically creating the dataframe df.ix[doc1,doc2]
= similarity .. which was very very much slower.</li>
<li>I've considered numba @git but it fails with pandas data structures.</li>
<li>I can't find a built in function which will do all the work internally (in C?)</li>
<li>What I have to do tactically is to randomly sample the documents to create a much smaller set to work with ... currently a fraction of 0.02 leads to about 20 minutes of calculation!</li>
</ul>

<p><strong>Here's the code (<a href=""https://github.com/makeyourowntextminingtoolkit/makeyourowntextminingtoolkit/tree/master/text_mining_toolkit"" rel=""nofollow noreferrer"">github</a>)</strong></p>

<pre><code>docs_combinations = itertools.combinations(docs_sample, 2)
for doc1, doc2 in docs_combinations:
    # scipy cosine similarity function includes normalising the vectors but is a distance .. so we need to take it from 1.0
    doc_similarity_dict[doc2].update({doc1: 1.0 - scipy.spatial.distance.cosine(relevance_index[doc1],relevance_index[doc2])})
    pass

#convert dict to pandas dataframe
doc_similarity_matrix = pandas.DataFrame(doc_similarity_dict)
</code></pre>

<p><strong>Simple Example</strong></p>

<p>@MaxU asked for an illustrative example.</p>

<p>Relevance matrix (wordcount here, just to keep it simple):</p>

<pre><code>...     doc1 doc2 doc3
wheel   2.   3.   0.
seat    2.   2.   0.
lights  0.   1.   1.
cake    0.   0.   5.
</code></pre>

<p>calculated similarity matrix based on 2-combinations (doc1, doc2), (doc2, doc3), (doc1, doc3)</p>

<pre><code>...     doc2 doc3
doc1    0.9449  0.
doc2    -       0.052
</code></pre>

<p>Take that top left value 0.889 .. thats the dot product (2*3 + 2*2 + 0 + 0) = 10 but normalised by the lengths of the vectors ... so divide by sqrt(8) and sqrt(14) gives 0.9449. You can see that there is no similarity between doc1 and doc3 .. the dot product is zero.</p>

<p>Scale this from 3 documents with 4 words ... to <strong>7944</strong> documents, which creates 3<strong>1,549,596</strong> combinations ...</p>
"
"40568856","How to provide (or generate) tags for nltk lemmatizers","2016-11-12 23:28:15","2","319","0","1","","40579321","<p>I have a set of documents, and I would like to transform those into such form, that it would allow me to count tfidf for words in those documents (so that each document is being represented by vector of tfidf-numbers).</p>

<p>I thought that it is enough to call WordNetLemmatizer.lemmatize(word), and then PorterStemmer - but all 'have', 'has', 'had', etc are not being transformed to 'have' by the lemmatizer - and it goes for other words as well. Then I have read, that I am supposed to provide a hint for the lemmatizer - tag representing a type of the word - whether it is noun, verb, adjective, etc. </p>

<p>My question is - how do I get these tags? What I am supposed to excecute on those documents to get this? </p>

<p>I am using python3.4, and I am lemmatizing + stemming single word at a time. I tried WordNetLemmatizer, and EnglishStemmer from nltk and also stem() from stemming.porter2.</p>
"
"40542523","NLTK: corpus-level bleu vs sentence-level BLEU score","2016-11-11 06:44:47","21","36097","2","2","","40650581","<p>I have imported nltk in python to calculate BLEU Score on Ubuntu. I understand how sentence-level BLEU score works, but I don't understand how corpus-level BLEU score work.</p>

<p>Below is my code for corpus-level BLEU score:</p>

<pre><code>import nltk

hypothesis = ['This', 'is', 'cat'] 
reference = ['This', 'is', 'a', 'cat']
BLEUscore = nltk.translate.bleu_score.corpus_bleu([reference], [hypothesis], weights = [1])
print(BLEUscore)
</code></pre>

<p>For some reason, the bleu score is 0 for the above code. I was expecting a corpus-level BLEU score of at least 0.5.</p>

<p>Here is my code for sentence-level BLEU score</p>

<pre><code>import nltk

hypothesis = ['This', 'is', 'cat'] 
reference = ['This', 'is', 'a', 'cat']
BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis, weights = [1])
print(BLEUscore)
</code></pre>

<p>Here the sentence-level BLEU score is 0.71 which I expect, taking into account the brevity-penalty and the missing word ""a"". However, I don't understand how corpus-level BLEU score work.</p>

<p>Any help would be appreciated.</p>
"
"40513544","Why are there different Lemmatizers in NLTK library?","2016-11-09 18:21:03","5","893","0","2","","40514554","<pre><code>&gt;&gt; from nltk.stem import WordNetLemmatizer as lm1
&gt;&gt; from nltk import WordNetLemmatizer as lm2
&gt;&gt; from nltk.stem.wordnet import WordNetLemmatizer as lm3
</code></pre>

<p>For me all of the three works the same way, but just to confirm, do they provide anything different?</p>
"
"40480839","NLTK relation extraction returns nothing","2016-11-08 07:03:11","6","3498","11","1","","40498962","<p>I am recently working on using nltk to extract relation from text. so i build a sample text:"" Tom is the cofounder of Microsoft."" and using following program to test and return nothing. I cannot figure out why.</p>

<p>I'm using NLTK version: 3.2.1, python version: 3.5.2.</p>

<p>Here is my code:</p>

<pre><code>import re
import nltk
from nltk.sem.relextract import extract_rels, rtuple
from nltk.tokenize import sent_tokenize, word_tokenize


def test():
    with open('sample.txt', 'r') as f:
        sample = f.read()   # ""Tom is the cofounder of Microsoft""

    sentences = sent_tokenize(sample)
    tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]
    tagged_sentences = [nltk.tag.pos_tag(sentence) for sentence in tokenized_sentences]

    OF = re.compile(r'.*\bof\b.*')

    for i, sent in enumerate(tagged_sentences):
        sent = nltk.chunk.ne_chunk(sent) # ne_chunk method expects one tagged sentence
        rels = extract_rels('PER', 'GPE', sent, corpus='ace', pattern=OF, window=10) 
        for rel in rels:
            print('{0:&lt;5}{1}'.format(i, rtuple(rel)))

if __name__ == '__main__':
    test()
</code></pre>

<hr>

<h2>1.　After some debug, if found that when i changed the input as</h2>

<blockquote>
  <p>""Gates was born in Seattle, Washington on October 28, 1955. ""</p>
</blockquote>

<h2>the nltk.chunk.ne_chunk() output is:</h2>

<blockquote>
  <p>(S
    (PERSON Gates/NNS)
    was/VBD
    born/VBN
    in/IN
    (GPE Seattle/NNP)
    ,/,
    (GPE Washington/NNP)
    on/IN
    October/NNP
    28/CD
    ,/,
    1955/CD
    ./.)</p>
</blockquote>

<h2>The test() returns:</h2>

<blockquote>
  <p>[PER: 'Gates/NNS'] 'was/VBD born/VBN in/IN' [GPE: 'Seattle/NNP']</p>
</blockquote>

<h2>2. After i changed the input as:</h2>

<blockquote>
  <p>""Gates was born in Seattle on October 28, 1955. ""</p>
</blockquote>

<p>The test() retuns nothing.</p>

<h2>3. I digged into <strong>nltk/sem/relextract.py</strong> and find this strange</h2>

<p>output is caused by function:
<strong>semi_rel2reldict(pairs, window=5, trace=False), which returns result only when len(pairs) > 2, and that's why when one sentence with less than three NEs will return None.</strong></p>

<p><strong>Is this a bug or i used NLTK in wrong way?</strong></p>
"
"40460401","Products Price Comparison Tool: Difficulty in matching identical items","2016-11-07 08:03:29","0","1885","0","1","","40460500","<p>I'm working on creating an e-comm products price comparison tool(in python) which is somewhat similar to <a href=""http://camelcamelcamel.com"" rel=""nofollow noreferrer"">camelcamelcamel.com</a>, both for fun and profit. I'm facing the difficult when I want to match the identical items from the list that I gathered from various websites using a search term. I'm using <em>Cosine similarity</em> and thinking of using <em>Levenshtein</em>'s Algorithm for product matching, to match the titles of the various items against each other to find the identical items.</p>

<p>For example, I have the following items and their price values as,</p>

<pre><code>{
    product_0: {
        title: ""Apple MacBook Air MMGF2HN/A 13.3-inch Laptop (Core i5/8GB/128GB/Mac OS X/Integrated Graphics)"",
        price: ""xxxx"",
    },
    product_1: {
        title: ""Apple MacBook Air MMGF2HN/A 13.3-inch Laptop (Core i5/8GB/128GB/Mac OS X/Integrated Graphics) cover"",
        price: ""xyzy""
    },
    product_2: {
        title: ""Apple Macbook Air MMGF2HNA Notebook (Intel Core i5- 8GB RAM- 128GB SSD- 33.78 cm(13.3)- OS X El Capitan) (Silver)""
        price: ""xxyy""
    },
    product_3: {
        title: ""...."",
        price: ""....""
    },

    ...

    product_99: {
        // product title and price
    }

}
</code></pre>

<p>When I used cosine similarity on the above list(data) of items, the values are as follows</p>

<pre><code>cosine(product_0 * product_1) = 0.973328526785
cosine(product_0 * product_2) = 0.50251890763
</code></pre>

<p>But in reality <code>product_0</code> and <code>product_1</code> are two different items but their consine similarity value shows that the items are identical; and <code>product_0</code> and <code>product_2</code> are from same entity but their cosine value shows that they are two different items.</p>

<p>I've been struggling to solve this problem on my own, thought I could ask for some suggestion/advice here in stackoverflow. Am I in the right direction using cosine similarity to match the similarities of items?. If not could you please channel me in the right direction.</p>

<p>My basic idea is to do a price comparison on identical items, i.e., Semantic Analysis of various similar product items.</p>

<p>Thanks for your time.</p>
"
"40452180","Python - Finding word on same line as given input","2016-11-06 17:16:20","0","265","0","2","","40452582","<p>For a method I'm creating, I want to take in a word that is found on the end of a line, and I then want to append the word found to the left of it (at the start of the line up to a space character) to an array.</p>

<p>Here is my code so far:</p>

<pre><code>def ruleElements(factor):
    # Creates list of RHS and LHS rule elements
    results = []

    # If RHS factor is found in grammar, append corresponding LHS.
    for line in grammarFile:
        start = line.find(0)
        end = line.find(' ', start)
        if factor in line:
            results.append(line[start:end])

    return results
</code></pre>

<p>So far the outputted array comes up empty all the time. Not sure where my logic is wrong.</p>

<p>A line in the grammarFile looks like, for example:</p>

<p>VP -> V NP</p>

<p>NP -> N</p>

<p>VP -> V PP</p>

<p>I want to take the part on the right-side of -> as an input and append the left-side to an array to be used in other parts of the program.</p>
"
"40423151","Lucene English tokenizer gives weird words","2016-11-04 12:50:58","0","202","1","1","","40423640","<p>I am using english tokenizer to parse out tokens and I am facing a weird situation where words like really/reply gets converted to realli, repli.</p>

<p>Below is the code snippet that I am using.</p>

<pre><code>object Learning {

  def tokenize(content: String): Seq[String] = {
    val tReader = new StringReader(content)
    val analyzer = new EnglishAnalyzer()
    val tStream = analyzer.tokenStream(""contents"", tReader)
    val term = tStream.addAttribute(classOf[CharTermAttribute])
    tStream.reset()

    val result = mutable.ArrayBuffer.empty[String]
    while(tStream.incrementToken()) {
      result += term.toString
    }
    result
  }

  def main(args: Array[String]): Unit = {
    println(tokenize(""This deal looks really interesting, I will look into it and reply""))
  }

}
</code></pre>

<p>This prints out as - ArrayBuffer(deal, look, realli, interest, i, look, repli). As far as I can see, there are no words such as realli,repli in the english language.</p>

<p>Can anybody point why this is giving output in such a way?</p>
"
"40357411","nltk custom grammar for chunking dates using RegexpParser","2016-11-01 09:38:29","0","594","0","1","","40359768","<p>Using the information extraction from this blog <a href=""https://gist.github.com/alexbowe/879414"" rel=""nofollow"">post</a>, I'm trying to define a grammar that includes the addition of dates as a new chunk with the following grammar;</p>

<pre><code>grammar = r""""""
    NBAR:
        {&lt;NN.*|JJ&gt;*&lt;NN.*&gt;}  # Nouns and Adjectives, terminated with Nouns

    NP:
        {&lt;NBAR&gt;}
        {&lt;NBAR&gt;&lt;IN&gt;&lt;NBAR&gt;}  # Above, connected with in/of/etc...
    DATE -&gt; MONTH SEP DAY SEP YEAR
    SEP -&gt; ""/""
    MONTH -&gt; DIGIT | DIGIT DIGIT
    DAY -&gt; DIGIT | DIGIT DIGIT
    YEAR -&gt; DIGIT DIGIT DIGIT DIGIT
    DIGIT -&gt; '1' | '2' | '3' | '4' | '5' | '6' | '7' | '8' | '9' | '0'
</code></pre>

<p>But this throws an illegal chunk pattern when I call <code>chunker = nltk.RegexpParser(grammar)</code>, Any ideas of how I can include the dates which are always represented as 8 digits <code>DD/MM/YYYY</code> or in the long form where the month is spelled out and the date is followed by the ordinal indicator <code>st,nd,  or th</code> so that the result would be <code>DDthMONTHYYYY</code>.         </p>
"
"40328503","Parsing replace quotes","2016-10-30 11:19:51","1","108","0","1","","40328877","<p>I'm trying to parse a text file to do some statistics about it in python. To do so, I want to replace some punctuations by tokens. One example of such a token would be all the punctuations who terminate a sentence(<code>.!?</code> become <code>&lt;EndS&gt;</code>). I managed to do this using a regex. Now I'm trying to parse quotes. therefore, I think, I need a way to distinguish opening quotes and closing quotes. I'm reading the input file line by line and I have no guarantee that the quotes will be equilibrated.</p>

<p>As example:</p>

<pre><code> ""Death to the traitors!"" cried the exasperated burghers.
 ""Go along with you,"" growled the officer, ""you always cry the same thing over again. It is very tiresome.""
</code></pre>

<p>should become something like:</p>

<pre><code> [Open] Death to the traitors! [Close] cried the exasperated burghers.
 [Open] Go along with you, [Close] growled the officer, [Open] you always cry the same thing over again. It is very tiresome. [Close]
</code></pre>

<p>Is it possible to do this using regexes? Is there an easier/better way to do this?</p>
"
"40167612","How to keep only the noun words in a wordlist? python NLTK","2016-10-21 02:52:56","3","7475","5","2","","40167806","<p>I have a wordlist, which consists many subjects. The subjects were auto extracted from sentences. I would like to keep only the noun from the subjects. As u can see some of the subjects have adj which i want to delete it.</p>

<pre><code>wordlist=['country','all','middle','various drinks','few people','its reputation','German Embassy','many elections']
returnlist=[]
for word in wordlist:
    x=wn.synsets(word)
    for syn in x:
        if syn.pos() == 'n':
            returnlist.append(word)
            break
print returnlist
</code></pre>

<p>the results of above is :</p>

<pre><code>['country','it',  'middle']
</code></pre>

<p>However, I want to get the result should be look like this </p>

<pre><code>   wordlist=['country','it', 'middle','drinks','people','reputation','German Embassy','elections']
</code></pre>

<p>How to do that?</p>
"
"40126849","Quantifiers in lambda calculus","2016-10-19 09:06:08","0","812","0","1","","41301818","<p>I'm learning lambda calculus, however I'm quite confused about the quantifiers in lambda calculus. As far as I know, quantifiers such as ""∃"" are concepts of first order logic (FOL), which are not needed by lambda calculus. Moreover, I didn't find anything about quantifiers in any tutorials I have read.</p>

<p>However, I find <a href=""https://arxiv.org/abs/1309.4408"" rel=""nofollow"">this paper</a> (Lambda Dependency-Based Compositional Semantics
), in the first page of which the author used quantifier in lambda calculus. So, are quantifiers used in lambda calculus? If so, what do they mean? Is it the same as in FOL?</p>
"
"40101873","how to analyze the value of tfidf matrix in sklearn?","2016-10-18 07:16:15","4","704","5","1","","40105858","<p>I am using sklearn's KMeans algorithm for document clustering as guided in 
<a href=""http://brandonrose.org/clustering"" rel=""nofollow noreferrer"">http://brandonrose.org/clustering</a></p>

<p>Here is a calculation of the TFIDF matrix. I have understood the concept behind the TFIDF technique but when I print this matrix the matrix is this:</p>

<pre><code>  (0, 11)   0.238317554822
  (0, 34)   0.355850989305
  (0, 7)    0.355850989305
  (0, 21)   0.238317554822
  (0, 16)   0.355850989305
  (0, 35)   0.355850989305
  (0, 8)    0.355850989305
  (0, 17)   0.355850989305
  (0, 36)   0.355850989305
  (1, 11)   0.238317554822
  (1, 21)   0.238317554822
  (1, 23)   0.355850989305
  (1, 0)    0.355850989305
  (1, 24)   0.355850989305
  (1, 12)   0.355850989305
  (1, 22)   0.355850989305
  (1, 25)   0.355850989305
  (1, 13)   0.355850989305
  (2, 2)    0.27430356415
  (2, 18)   0.339992197465
  (2, 26)   0.339992197465
  (2, 39)   0.339992197465
  (2, 3)    0.339992197465
  (2, 19)   0.339992197465
  (2, 27)   0.339992197465
  (2, 4)    0.339992197465
  (2, 20)   0.339992197465
  (3, 2)    0.27430356415
  (3, 40)   0.339992197465
  (3, 9)    0.339992197465
  (3, 1)    0.339992197465
  (3, 5)    0.339992197465
  (3, 41)   0.339992197465
  (3, 10)   0.339992197465
  (3, 6)    0.339992197465
  (3, 42)   0.339992197465
  (4, 11)   0.202877476983
  (4, 21)   0.202877476983
  (4, 28)   0.302932576437
  (4, 31)   0.302932576437
  (4, 37)   0.302932576437
  (4, 14)   0.302932576437
  (4, 29)   0.302932576437
  (4, 32)   0.302932576437
  (4, 38)   0.302932576437
  (4, 15)   0.302932576437
  (4, 30)   0.302932576437
  (4, 33)   0.302932576437
</code></pre>

<p>What do the values in this matrix representing? can anybody that worked on this help me to understand this?</p>
"
"40094400","Classification training with only positive sentences","2016-10-17 19:42:37","0","303","0","1","","40095843","<p>I'm starting a project to build an automated fact checking classificator nad I have some doubts about the process to follow.</p>

<p>I've a database of ~1000 sentences, each one being a fact check positive. In order to build a supervised machine learning model I'll need to have a big set of tagged sentences with the true/false result depending if it's a fact check candidate sentence or not. That would require a lot of time and effort, so I'd like to first get results (with less accuracy I guess) without doing that.</p>

<p>My idea is to use the already tagged positive sentences and apply a PoS tagger to them. This would give me interesting information to spot some patterns like the most common words(e.g: raised, increase, won) and the post tags (e.g. verbs in past/present tense, time and numerals).</p>

<p>With this results I'm thinking about assigning weights in order to analyze new unclassified sentences. The problem is that the weight assignment would be done by me in an ""heuristical"" way. It'd be best to use the results of the PoS tagger to train some model which assigns probabilities in a more sophisticated way.</p>

<p>Could you give me some pointers if there's a way to accomplish this?</p>

<p>I read about Maximum Entropy Classifiers and statistical parsers but I really don't know if they're the right choice.</p>

<p>Edit (I think it'd be better to give more details):</p>

<p>Parsing the sentences with a PoS tagger will give me some useful information about each one of them, allowing me to filter them and weighting them using some custom metrics.</p>

<p>For example:</p>

<p>There are one million more people in poverty than five years ago -> indicatives of a fact check candidate sentence:  verb in present tense, numerals and dates, (than) comparison.</p>

<p>We will increase the GDP by 3% the following year -> indicatives of a NOT fact check candidate sentence: it's in the future tense (indicative of some sort of prediction)</p>
"
"39928277","Extracting specific information from data","2016-10-08 03:06:32","2","105","0","2","","39930029","<p>How can i convert a data format like: </p>

<pre><code>James Smith was born on November 17, 1948
</code></pre>

<p>into something like </p>

<pre><code>(""James Smith"", DOB, ""November 17, 1948"")
</code></pre>

<p>without having to rely on positional index of strings</p>

<p>I have tried the following </p>

<pre><code>from nltk import word_tokenize, pos_tag

new = ""James Smith was born on November 17, 1948""
sentences = word_tokenize(new)
sentences = pos_tag(sentences)
grammar = ""Chunk: {&lt;NNP*&gt;&lt;NNP*&gt;}""
cp = nltk.RegexpParser(grammar)
result = cp.parse(sentences)
print(result)
</code></pre>

<p>How to proceed further to get the output in desired fromat.</p>
"
"39355994","Snowball Stemming: defining Null Region","2016-09-06 18:54:50","2","85","0","1","","39369045","<p>I'm trying to understand the snowball stemming algorithmus. <a href=""https://stackoverflow.com/questions/31848056/snowball-stemming-defining-regions"">HW90</a> has had a similar question with examples, but not mine. The algorithmus is using two regions R1 and R2 that are definied as follows:</p>

<blockquote>
  <p>R1 is the region after the first non-vowel following a vowel, or is
  the null region at the end of the word if there is no such non-vowel.</p>
  
  <p>R2 is the region after the first non-vowel following a vowel in R1, or
  is the null region at the end of the word if there is no such
  non-vowel.</p>
  
  <p><a href=""http://snowball.tartarus.org/texts/r1r2.html"" rel=""nofollow noreferrer"">http://snowball.tartarus.org/texts/r1r2.html</a></p>
</blockquote>

<p>I don't understand, what ""the null region at the end of the word"" is. Could anybody give me some examples for that, please?</p>
"
"39353348","How to extract nouns from dataframe","2016-09-06 16:04:13","1","4103","4","1","","39355909","<p>I want to extract nouns from dataframe. Only nouns.
I do as below</p>

<pre><code>import pandas as pd
import nltk
from nltk.tag import pos_tag
from nltk import word_tokenize
df = pd.DataFrame({'noun': ['good day', 'good night']})
</code></pre>

<p>I want to get</p>

<pre><code>    noun
0   day
1   night
</code></pre>

<p>My code    </p>

<pre><code>df['noun'] = df.apply(lambda row: nltk.word_tokenize(row['noun']), axis=1) 
noun=[]
for  index, row in df.iterrows():
    noun.append([word for word,pos in pos_tag(row) if pos == 'NN'])
df['noun'] = noun 



 ---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-194-688cfbb21ec5&gt; in &lt;module&gt;()
      1 noun=[]
      2 for  index, row in df.iterrows():
----&gt; 3     noun.append([word for word,pos in pos_tag(row) if pos == 'NN'])
      4 df['noun'] = noun

C:\Users\Edward\Anaconda3\lib\site-packages\nltk\tag\__init__.py in pos_tag(tokens, tagset)
    109     """"""
    110     tagger = PerceptronTagger()
--&gt; 111     return _pos_tag(tokens, tagset, tagger)
    112 
    113 

C:\Users\Edward\Anaconda3\lib\site-packages\nltk\tag\__init__.py in _pos_tag(tokens, tagset, tagger)
     80 
     81 def _pos_tag(tokens, tagset, tagger):
---&gt; 82     tagged_tokens = tagger.tag(tokens)
     83     if tagset:
     84         tagged_tokens = [(token, map_tag('en-ptb', tagset, tag)) for (token, tag) in tagged_tokens]

C:\Users\Edward\Anaconda3\lib\site-packages\nltk\tag\perceptron.py in tag(self, tokens)
    150         output = []
    151 
--&gt; 152         context = self.START + [self.normalize(w) for w in tokens] + self.END
    153         for i, word in enumerate(tokens):
    154             tag = self.tagdict.get(word)

C:\Users\Edward\Anaconda3\lib\site-packages\nltk\tag\perceptron.py in &lt;listcomp&gt;(.0)
    150         output = []
    151 
--&gt; 152         context = self.START + [self.normalize(w) for w in tokens] + self.END
    153         for i, word in enumerate(tokens):
    154             tag = self.tagdict.get(word)

C:\Users\Edward\Anaconda3\lib\site-packages\nltk\tag\perceptron.py in normalize(self, word)
    222         if '-' in word and word[0] != '-':
    223             return '!HYPHEN'
--&gt; 224         elif word.isdigit() and len(word) == 4:
    225             return '!YEAR'
    226         elif word[0].isdigit():

AttributeError: 'list' object has no attribute 'isdigit'
</code></pre>

<p>Please, help, how to improve it?
* Sorry, i have ro write some text so that i can insert all traceback
I guess thr problem is that i cann't convert list to needed format?</p>
"
"39349400","How to remove square brackets in result pos_tag","2016-09-06 12:47:36","0","399","0","1","","39349493","<p>I want to extract nouns from dataframe. I do as below</p>

<pre><code>import pandas as pd
import nltk
from nltk.tag import pos_tag
df = pd.DataFrame({'pos': ['noun', 'Alice', 'good', 'well', 'city']})
noun=[]
for index, row in df.iterrows():
    noun.append([word for word,pos in pos_tag(row) if pos == 'NN'])
df['noun'] = noun  
</code></pre>

<p>and i get df['noun']</p>

<pre><code>0     [noun]
1    [Alice]
2         []
3         []
4     [city]
</code></pre>

<p>I use regex </p>

<pre><code>df['noun'].replace('[^a-zA-Z0-9]', '', regex = True)
</code></pre>

<p>and again</p>

<pre><code>0     [noun]
1    [Alice]
2         []
3         []
4     [city]
Name: noun, dtype: object
</code></pre>

<p>what's wrong?</p>
"
"39340907","Converting output of dependency parsing to tree","2016-09-06 04:56:09","2","3266","0","1","","39342032","<p>I am using <code>Stanford dependency parser</code> and the I get the following output of the sentence</p>

<blockquote>
  <p>I shot an elephant in my sleep</p>
</blockquote>

<pre><code>python dep_parsing.py 
[((u'shot', u'VBD'), u'nsubj', (u'I', u'PRP')), ((u'shot', u'VBD'), u'dobj', (u'elephant', u'NN')), ((u'elephant', u'NN'), u'det', (u'an', u'DT')), ((u'shot', u'VBD'), u'nmod', (u'sleep', u'NN')), ((u'sleep', u'NN'), u'case', (u'in', u'IN')), ((u'sleep', u'NN'), u'nmod:poss', (u'my', u'PRP$'))]
</code></pre>

<p><strong>I want to convert this into a graph with nodes being each token and edges being the relation between them.</strong></p>

<p>I need the graph structure for further processing hence it would help if modification to it are easy and also must be easily representable.</p>

<p>Here is my code till now.</p>

<pre><code>from nltk.parse.stanford import StanfordDependencyParser
stanford_parser_dir = 'stanford-parser/'
eng_model_path = stanford_parser_dir  + ""stanford-parser-models/edu/stanford/nlp/models/lexparser/englishRNN.ser.gz""
my_path_to_models_jar = stanford_parser_dir  + ""stanford-parser-3.5.2-models.jar""
my_path_to_jar = stanford_parser_dir  + ""stanford-parser.jar""

dependency_parser = StanfordDependencyParser(path_to_jar=my_path_to_jar, path_to_models_jar=my_path_to_models_jar)

result = dependency_parser.raw_parse('I shot an elephant in my sleep')
dep = result.next()
a = list(dep.triples())
print a
</code></pre>

<p>How can I make such a graph structure?</p>
"
"39321495","AttributeError: 'list' object has no attribute 'isdigit'","2016-09-04 21:05:24","1","4539","0","2","","39321548","<p>I want to extract POS in pandas. I do as below</p>

<pre><code>import pandas as pd
from nltk.tag import pos_tag
df = pd.DataFrame({'pos': ['noun', 'Alice', 'good', 'well', 'city']})
s = df['pos']
tagged_sent = pos_tag(s.str.split())
</code></pre>

<p>but get a traceback:</p>

<pre><code>Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""../lib/python2.7/site-packages/nltk/tag/__init__.py"", line 111, in pos_tag
    return _pos_tag(tokens, tagset, tagger)
  File ""../lib/python2.7/site-packages/nltk/tag/__init__.py"", line 82, in _pos_tag
    tagged_tokens = tagger.tag(tokens)
  File ""/Users/mjpieters/Development/venvs/stackoverflow-2.7/lib/python2.7/site-packages/nltk/tag/perceptron.py"", line 152, in tag
    context = self.START + [self.normalize(w) for w in tokens] + self.END
  File ""../lib/python2.7/site-packages/nltk/tag/perceptron.py"", line 224, in normalize
    elif word.isdigit() and len(word) == 4:
AttributeError: 'list' object has no attribute 'isdigit'
</code></pre>

<p>What's wrong? </p>
"
"39320015","How to split an NLP parse tree to clauses (independent and subordinate)?","2016-09-04 18:10:51","8","8406","0","2","","39320379","<p>Given an NLP parse tree like </p>

<pre><code>(ROOT (S (NP (PRP You)) (VP (MD could) (VP (VB say) (SBAR (IN that) (S (NP (PRP they)) (ADVP (RB regularly)) (VP (VB catch) (NP (NP (DT a) (NN shower)) (, ,) (SBAR (WHNP (WDT which)) (S (VP (VBZ adds) (PP (TO to) (NP (NP (PRP$ their) (NN exhilaration)) (CC and) (NP (FW joie) (FW de) (FW vivre))))))))))))) (. .)))
</code></pre>

<p>Original sentence is ""You could say that they regularly catch a shower, which adds to their exhilaration and joie de vivre.""</p>

<p>How could the clauses be extracted and reverse engineered?
We would be splitting at S and SBAR (to preserve the type of clause, eg subordinated)</p>

<pre><code> - (S (NP (PRP You)) (VP (MD could) (VP (VB say) 
 - (SBAR (IN that) (S (NP (PRP they)) (ADVP (RB regularly)) (VP (VB catch) (NP (NP (DT a) (NN shower))
 - (, ,) (SBAR (WHNP (WDT which)) (S (VP (VBZ adds) (PP (TO to)
   (NP (NP (PRP$ their) (NN exhilaration)) (CC and) (NP (FW joie) (FW
   de) (FW vivre))))))))))))) (. .)))
</code></pre>

<p>to arrive at</p>

<pre><code> - You could say
 - that they regularly catch a shower 
 - , which adds to their exhilaration and joie de vivre.
</code></pre>

<p>Splitting at S and SBAR seems very easy. The problem seems to be stripping away all the POS tags and chunks from the fragments.</p>
"
"39302880","Getting the root word using the Wordnet Lemmatizer","2016-09-03 03:10:45","7","14280","0","1","","39303494","<p>I need to find a common root word matched for all related words for a keyword extractor.</p>

<p>How to convert words into the same root using the python nltk lemmatizer? </p>

<ul>
<li>Eg: 

<ol>
<li>generalized, generalization -> general </li>
<li>optimal, optimized -> optimize (maybe) </li>
<li>configure, configuration, configured -> configure</li>
</ol></li>
</ul>

<p>The python nltk lemmatizer gives 'generalize', for 'generalized' and 'generalizing' when part of speech(pos) tag parameter is used but not for 'generalization'.</p>

<p>Is there a way to do this?</p>
"
"39210008","NLTK can't find the Stanford POS tagger model file","2016-08-29 15:33:41","0","703","1","1","","39210557","<p>I am trying to use StanfordPOSTagger from the NLTK. I  downloaded Stanford POS full tagger. I have set </p>

<pre><code>CLASSPATH=/home/waheeb/Stanford_Tools/stanford-postagger-full-2015-12-09  /stanford-postagger.jar
STANFORD_MODELS=home/waheeb/Stanford_Tools/stanford-postagger-full-2015-12-09/models
</code></pre>

<p>When I type the following in python:</p>

<pre><code>&gt;&gt;&gt; from nltk.tag import StanfordPOSTagger
&gt;&gt;&gt; st = StanfordPOSTagger('english-bidirectional-distsim.tagger')
</code></pre>

<p>I get the following error:</p>

<pre><code>Traceback (most recent call last):
File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
File ""/home/waheeb/anaconda2/lib/python2.7/site-packages/nltk/tag /stanford.py"", line 136, in __init__
super(StanfordPOSTagger, self).__init__(*args, **kwargs)
File ""/home/waheeb/anaconda2/lib/python2.7/site-packages/nltk/tag/stanford.py"", line 56, in __init__
env_vars=('STANFORD_MODELS',), verbose=verbose)
File ""/home/waheeb/anaconda2/lib/python2.7/site-packages /nltk/internals.py"", line 573, in find_file
file_names, url, verbose))
File ""/home/waheeb/anaconda2/lib/python2.7/site-packages/nltk/internals.py"", line 567, in find_file_iter
raise LookupError('\n\n%s\n%s\n%s' % (div, msg, div))
</code></pre>

<p>LookupError: </p>

<pre><code>=========================================================================
NLTK was unable to find the english-bidirectional-distsim.tagger file!
Use software specific configuration paramaters or set the TANFORD_MODELS  environment variable.
==========================================================================
</code></pre>

<p>Why is that?? </p>
"
"39167671","NLTK BigramTagger does not tag half of the sentence","2016-08-26 13:35:40","3","1609","2","1","","39170704","<p>Can someone please explain the behaviour of NLTK's BigramTagger in these examples?</p>

<p>I instantiated the tagger by </p>

<pre><code>bi= BigramTagger(brown.tagged_sents(categories='news')[:500])
</code></pre>

<p>Now, I want to use this on one specific sentence.</p>

<pre><code>&gt;&gt;&gt; bi.tag(brown_sents[2])
[(u'The', u'AT'), (u'September-October', u'NP'), (u'term', u'NN'), (u'jury', u'NN'), (u'had', u'HVD'), (u'been', u'BEN'), (u'charged', u'VBN'), (u'by', u'IN'), (u'Fulton', u'NP-TL'), (u'Superior', u'JJ-TL'), (u'Court', u'NN-TL'), (u'Judge', u'NN-TL'), (u'Durwood', u'NP'), (u'Pye', u'NP'), (u'to', u'TO'), (u'investigate', u'VB'), (u'reports', u'NNS'), (u'of', u'IN'), (u'possible', u'JJ'), (u'``', u'``'), (u'irregularities', u'NNS'), (u""''"", u""''""), (u'in', u'IN'), (u'the', u'AT'), (u'hard-fought', u'JJ'), (u'primary', u'NN'), (u'which', u'WDT'), (u'was', u'BEDZ'), (u'won', u'VBN'), (u'by', u'IN'), (u'Mayor-nominate', u'NN-TL'), (u'Ivan', u'NP'), (u'Allen', u'NP'), (u'Jr.', u'NP'), (u'.', u'.')]
</code></pre>

<p>Works well, but hey, it's all known data.
Let me change one word and see if it sets something off.</p>

<pre><code>&gt;&gt;&gt; sent=brown_sents[2]
&gt;&gt;&gt; sent[5]
u'been'
&gt;&gt;&gt; sent[5] = u'was'
&gt;&gt;&gt; bi.tag(sent)
[(u'The', u'AT'), (u'September-October', u'NP'), (u'term', u'NN'), (u'jury', u'NN'), (u'had', u'HVD'), (u'was', None), (u'charged', None), (u'by', None), (u'Fulton', None), (u'Superior', None), (u'Court', None), (u'Judge', None), (u'Durwood', None), (u'Pye', None), (u'to', None), (u'investigate', None), (u'reports', None), (u'of', None), (u'possible', None), (u'``', None), (u'irregularities', None), (u""''"", None), (u'in', None), (u'the', None), (u'hard-fought', None), (u'primary', None), (u'which', None), (u'was', None), (u'won', None), (u'by', None), (u'Mayor-nominate', None), (u'Ivan', None), (u'Allen', None), (u'Jr.', None), (u'.', None)]
</code></pre>

<p>Now I expected to see changed tuple, <code>(u'been', u'BEN')</code> to now be (u'been', None). Why is everything after it in the sentence now not tagged? Those words were tagged in connection to another ones, not 'been'.</p>

<p>Any recommendation on use of tagged sentences would be appreciated as well.</p>
"
"39104030","Is there a standard diagnostic for handling unknown words in FCG?","2016-08-23 14:30:55","1","48","0","1","","39104215","<p>I have an FCG grammar for English and I'm parsing some text with out-of-vocabulary words. At this moment, I write my own customized diagnostics and repairs. Is there any standard way of treating unknown words in the latest FCG release?</p>
"
"38987138","How to extract the grammar productions rules given bracketed parses?","2016-08-17 02:20:09","4","3039","8","2","","39049856","<p>I have a sample sentence. ""Open the door."" that I parsed a sentence to get the bracketed parse output as below.</p>

<blockquote>
  <p>(S (VP (VB open) (NP (DT the) (NN door))) (. .))</p>
</blockquote>

<p>I need to extract the CFG grammar rules that produce the parsed output.
I can manually write them out as such:</p>

<pre><code>grammar = CFG.fromstring(""""""   
S -&gt; VP NP   
NP -&gt; Det N   
VP -&gt; V   
Det -&gt;'the '   
N -&gt; 'door'   
V -&gt; 'Open'   
"""""")  
</code></pre>

<p>But it's time consuming, how do I produce the grammar rules given the bracketed parsed automatically?</p>
"
"38903589","Train Gate POS tagger for another language","2016-08-11 18:21:49","0","134","1","1","","38913596","<p>I want to re-train GATE pos-tagger for my mother tongue. Is train available in GATE? How should the training samples be formatted?
Thanks in Advance</p>
"
"38819371","Unable to instantiate StanfordNERTagger on OS X","2016-08-07 23:05:19","0","461","1","1","","38819496","<p>I am trying to instantiate <strong><code>StanfordNERTagger</code></strong>. This is what I am trying:</p>

<pre><code>st = StanfordNERTagger(""/Users/attitude/Desktop/english.all.3class.caseless.distsim.crf.ser.gz"",""/Users/attitude/Desktop/stanford-ner-2015-12-09/stanford-ner.jar"")
</code></pre>

<p>I have set the <code>CLASSPATH</code> variable to <code>/Users/attitude/Desktop/stanford-ner-2015-12-09/stanford-ner.jar</code> (I also tried just the parent folder as value - <code>/Users/attitude/Desktop/stanford-ner-2015-12-09</code>).</p>

<p>However, I am getting this error:</p>

<p><code>LookupError: Could not find stanford-ner.jar jar file at /Users/attitude/Desktop/stanford-ner-2015-12-09/stanford-ner.jar</code>.</p>

<p>I have done everything mentioned in these two answers - <a href=""https://stackoverflow.com/questions/32819573/nltk-why-does-nltk-not-recognize-the-classpath-variable-for-stanford-ner"">this</a> and <a href=""https://stackoverflow.com/questions/32652725/importerror-cannot-import-name-stanfordnertagger-in-nltk"">this</a>. What else do I do now to fix this error?</p>

<p>OS X Yosemite - Python 2.7.</p>
"
"38768052","Recursion Error: Maximum Recursion depth exceeded","2016-08-04 12:59:34","0","3312","3","1","","38768326","<pre><code>from __future__ import print_function
import os, codecs, nltk.stem

english_stemmer = nltk.stem.SnowballStemmer('english')
for root, dirs, files in os.walk(""/Users/Documents/corpus/source-document/test1""):
        for file in files:
            if file.endswith("".txt""):
                posts = codecs.open(os.path.join(root,file),""r"", ""utf-8-sig"")
from sklearn.feature_extraction.text import CountVectorizer
class StemmedCountVectorizer(CountVectorizer):
    def build_analyzer(self):
        analyzer = super(StemmedCountVectorizer, self.build_analyzer())
        return lambda doc: (english_stemmer.stem(w) for w in  analyzer(doc))

vectorizer = StemmedCountVectorizer(min_df = 1, stop_words = 'english')
X_train = vectorizer.fit_transform(posts)
num_samples, num_features = X_train.shape
print(""#samples: %d, #features: %d"" % (num_samples, num_features))     #samples: 5, #features: 25
print(vectorizer.get_feature_names())
</code></pre>

<p>When I run the above code for all the text file contained in the directory it is throwing the following error:
RecursionError: maximum recursion depth exceeded.</p>

<p>I tried to resolve the problem with sys.setrecursionlimit, but all in vain. When i provide large value like 20000 the the kernel crash error occurs.</p>
"
"38687056","NLTK issue in deriving sql query using fcfg","2016-07-31 18:18:10","2","1093","2","1","","38713635","<p>I'm using NLTK to get sql query from english text using feature based cfg. I followed this link <a href=""http://www.nltk.org/book/ch10.html"" rel=""nofollow"">http://www.nltk.org/book/ch10.html</a>. I can run the example stated where fcfg is stored in the sql0.fcfg file.</p>

<p>After that I tried to modify it for my own use where I added a following new set of rules:</p>

<pre><code>% start S
## Added by me
S[SEM=(?whadvp + ?sq)] -&gt; WHADVP[SEM=?whadvp] SQ[SEM=?sq]

WHADVP[SEM=(?wrb + ?jj)] -&gt; WRB[SEM=?wrb] JJ[SEM=?jj]
SQ[SEM=(?vbp + ?np + ?vp)] -&gt; VBP[SEM=?vbp] NP[SEM=?np] VP[SEM=?vp]
NP[SEM=(?np + ?pp)] -&gt; NP[SEM=?np] PP[SEM=?pp]
NP[SEM=(?np)] -&gt; JJS[SEM=?jjs]
VP[SEM=(?vbz + ?advp)] -&gt; VBZ[SEM=?vbz] ADVP[SEM=?advp]
PP[SEM=(?in + ?np)] -&gt; IN[SEM=?in] NP[SEM=?np]
NP[SEM=(?prp + ?nn)] -&gt; PRP$[SEM=?prp] NN[SEM=?nn]
ADVP[SEM=(?rb)] -&gt; RB[SEM=?rb]

WRB[SEM='SELECT average(calldurationinsexonds) FROM Task'] -&gt; 'How'

JJ[SEM=''] -&gt; 'long'
VBP[SEM=''] -&gt; 'do'
JJS[SEM=''] -&gt; 'most'
IN[SEM=''] -&gt; 'of'
PRP$[SEM=''] -&gt; 'our'
NN[SEM=''] -&gt; 'phone'
VBZ[SEM=''] -&gt; 'calls'
JJ[SEM=''] -&gt; 'last'

## Default example
S[SEM=(?np + WHERE + ?vp)] -&gt; NP[SEM=?np] VP[SEM=?vp]

VP[SEM=(?v + ?pp)] -&gt; IV[SEM=?v] PP[SEM=?pp]
VP[SEM=(?v + ?ap)] -&gt; IV[SEM=?v] AP[SEM=?ap]
NP[SEM=(?det + ?n)] -&gt; Det[SEM=?det] N[SEM=?n]
PP[SEM=(?p + ?np)] -&gt; P[SEM=?p] NP[SEM=?np]
AP[SEM=?pp] -&gt; A[SEM=?a] PP[SEM=?pp]

NP[SEM='Country=""greece""'] -&gt; 'Greece'
NP[SEM='Country=""china""'] -&gt; 'China'

Det[SEM='SELECT'] -&gt; 'Which' | 'What'

N[SEM='City FROM city_table'] -&gt; 'cities'

IV[SEM=''] -&gt; 'are'
A[SEM=''] -&gt; 'located'
P[SEM=''] -&gt; 'in'
</code></pre>

<p>After saving the file, when I execute following commands I run into errors</p>

<pre><code>cp = load_parser('grammars/book_grammars/sql0.fcfg')
query = 'How long do most of our phone calls last'
trees = list(cp.parse(query.split()))
</code></pre>

<p>Error:</p>

<blockquote>
  <p>Traceback (most recent call last):   File """", line 1, in
     File ""C:\Python27\lib\site-packages\nltk\parse\chart.py"",
  line 1350, in parse
      chart = self.chart_parse(tokens)   File ""C:\Python27\lib\site-packages\nltk\parse\chart.py"", line 1309, in
  chart_parse
      self._grammar.check_coverage(tokens)   File ""C:\Python27\lib\site-packages\nltk\grammar.py"", line 631, in
  check_coverage
      ""input words: %r."" % missing) ValueError: Grammar does not cover some of the input words: u""'How', 'long', 'do', 'most',  'of', 'our',
  'phone', 'calls', 'last'"".</p>
</blockquote>

<p>I don't know if there is a mistake in my added grammar or some other issue. Any help or suggestion would be great.</p>
"
"38614738","Sentiment Analysis - Cross Validation not valid score","2016-07-27 13:44:32","1","871","0","1","","38625869","<p>I am testing a Sentiment Analysis model using NLTK and SKlearn.</p>

<p>Movie_reviews data has ""pos"" and ""neg"" labels. For training the classifier I am using ""featuresets"". I am using cross validation on training data and accuracy on testing data. However cross validation is always much higher than accuracy. In the example below for logistic regression algorithm CV = 97 (average), Accuracy = 70. I have tested also with other algos and still cross validation very high.</p>

<p>I am pretty sure the code I have for cross validation is not right.</p>

<pre><code>import nltk
import random 
from nltk.corpus import movie_reviews  
from sklearn import cross_validation
from nltk.classify.scikitlearn import SklearnClassifier
from sklearn.linear_model import LogisticRegression, SGDClassifier

documents = [(list(movie_reviews.words(fileid)), category)
             for category in movie_reviews.categories()
             for fileid in movie_reviews.fileids(category)]
random.shuffle(documents)

all_words = []
for w in movie_reviews.words():
    all_words.append(w.lower())

all_words = nltk.FreqDist(all_words)
word_features = list(all_words.keys())[:3000]

def find_features(document):
    words = set(document)
    features = {}
    for w in word_features:
        features[w] = (w in words)
    return features

featuresets = [(find_features(rev), category) for (rev, category) in documents]        
training_set = featuresets[:1500]
testing_set =  featuresets[1500:]

cv = cross_validation.KFold(len(training_set), n_folds=10, shuffle=True, random_state=None)
LogisticRegression_classifier = SklearnClassifier(LogisticRegression())    
for traincv, testcv in cv:
    classifier = LogisticRegression_classifier.train(training_set[traincv[0]:traincv[len(traincv)-1]])
    print ('CV_accuracy:', nltk.classify.util.accuracy(classifier, training_set[testcv[0]:testcv[len(testcv)-1]]))

print(""LogisticRegression_classifier accuracy percent:"", (nltk.classify.accuracy(LogisticRegression_classifier, testing_set))*100)
</code></pre>
"
"38571004","Using Stanford CoreNLP Python Parser for specific output","2016-07-25 14:52:07","0","698","1","1","","38571865","<p>I'm using <a href=""https://github.com/dasmith/stanford-corenlp-python"" rel=""nofollow"">SCP</a> to get the parse CFG tree for English sentences. </p>

<pre><code>from corenlp import *
corenlp = StanfordCoreNLP()
corenlp.parse(""Every cat loves a dog"")
</code></pre>

<p>My expected output is a tree like this: </p>

<pre><code>(S (NP (DET Every) (NN cat)) (VP (VT loves) (NP (DET a) (NN dog))))
</code></pre>

<p>But what i got is: </p>

<pre><code>(ROOT (S (NP (DT Every) (NN cat)) (VP (VBZ loves) (NP (DT a) (NN dog)))))
</code></pre>

<p>How to change the POS tag as expected and remove the ROOT node?</p>

<p>Thanks</p>
"
"38545726","How to use the link grammar parser as a grammar checker","2016-07-23 19:33:21","6","1306","3","1","","40243925","<p>Abiword uses the <a href=""http://www.abisource.com/projects/link-grammar/"">link grammar parser</a> as a simple grammar checker. I'd like to duplicate this feature with Python. </p>

<p>Poorly documented Python bindings exist, but I don't know how to use them to mimic the grammar checker in Abiword. </p>

<p>(I'm not interested in the actual parsing results. I only need to know if a sentence parses OK with the link grammar parser and if not which words can't be linked.)</p>

<p>What would be the best method to achieve this?</p>
"
"38541644","Confusion Matrix - Testing Sentiment Analysis Model","2016-07-23 12:10:10","4","3093","2","1","","38551147","<p>I am testing a Sentiment Analysis model using NLTK. I need to add a Confusion Matrix to the classifier results and if possible also Precision, Recall and F-Measure values. I have only accuracy so far. Movie_reviews data has pos and neg labels. However to train the classifier I am using ""featuresets"" that has a different format from the usual (sentence, label) structure. I am not sure if I can use confusion_matrix from sklearn, after training the classifier by ""featuresets""</p>

<pre><code>import nltk
import random
from nltk.corpus import movie_reviews

documents = [(list(movie_reviews.words(fileid)), category)
             for category in movie_reviews.categories()
             for fileid in movie_reviews.fileids(category)]

random.shuffle(documents)

all_words = []

for w in movie_reviews.words():
    all_words.append(w.lower())

all_words = nltk.FreqDist(all_words)

word_features = list(all_words.keys())[:3000]

def find_features(document):
    words = set(document)
    features = {}
    for w in word_features:
        features[w] = (w in words)

    return features


featuresets = [(find_features(rev), category) for (rev, category) in documents]

training_set = featuresets[:1900]
testing_set =  featuresets[1900:]


classifier = nltk.NaiveBayesClassifier.train(training_set)


print(""Naive Bayes Algo accuracy percent:"", (nltk.classify.accuracy(classifier, testing_set))*100)
</code></pre>
"
"38502341","GATE JAPE rule priorities not respected","2016-07-21 10:54:56","1","350","3","1","","38562767","<p>I have following text:</p>

<pre><code>1 hwb wert: 330 kWh
</code></pre>

<p>In the first step, following mapping is tacking place:</p>

<p><code>330 kWh</code> is mapped as: <code>Lookup.major = ""unit""</code></p>

<p><code>hwb wert</code>is mapped as: <code>Lookup.major = ""keyword""</code>  </p>

<p><strong>The JAPE Rules:</strong></p>

<pre><code>Phase: composedUnits
Input: Token Lookup
Options: control=appelt debug=true

Rule: TableRow
Priority:10
 (
  ({Lookup.majorType == ""keyword""})
  ({Token.kind == punctuation})[0,4]
  ({Lookup.majorType == ""unit""})
 )

Rule: ReversedTableRow
Priority: -2
(
 ({Token.kind == number})
 ({Lookup.majorType == ""keyword""})
)
</code></pre>

<p>I can't understand why the <code>ReversedTableRow</code>-Rule is matched and not the <code>TableRow</code>.</p>
"
"38295454","How to convert from Stanford Universal Dependencies to Phrase Grammar?","2016-07-10 18:57:18","1","783","1","1","","38295964","<p>In my application I am using Stanford CoreNLP for parsing english text into a graph data structure (Universal Dependencies).</p>

<p>After some modifications of the graph I need to generate a natural language output for which I am using SimpleNLG: <a href=""https://github.com/simplenlg/simplenlg"" rel=""nofollow"">https://github.com/simplenlg/simplenlg</a></p>

<p>However SimpleNLG is using Phrase Grammar.</p>

<p>Therefore in order to successfully use SimpleNLG for natural language generation I need to convert from Universal Dependencies into Phrase Grammar.</p>

<p>What is the easiest way of achieving this?</p>

<p>So far I have only come across this article on this topic:
<a href=""http://delivery.acm.org/10.1145/1080000/1072147/p14-xia.pdf?ip=86.52.161.138&amp;id=1072147&amp;acc=OPEN&amp;key=4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E6D218144511F3437&amp;CFID=642131329&amp;CFTOKEN=21335001&amp;__acm__=1468166339_844b802736ce07dab89064efb7f8ede9"" rel=""nofollow"">http://delivery.acm.org/10.1145/1080000/1072147/p14-xia.pdf?ip=86.52.161.138&amp;id=1072147&amp;acc=OPEN&amp;key=4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E6D218144511F3437&amp;CFID=642131329&amp;CFTOKEN=21335001&amp;<strong>acm</strong>=1468166339_844b802736ce07dab89064efb7f8ede9</a></p>

<p>I am hoping that someone might have some more practical code examples to share on this issue?</p>
"
"38226864","How to extract special characters using NLTK RegexpParser Chunk for POS_tagged words in Python","2016-07-06 14:41:38","1","1070","0","1","","38292371","<p>I have some text for example say: <code>80% of $300,000 Each Human Resource/IT Department.</code></p>

<p>I would need to extract <code>$300,000</code> along with the words <code>Each Human Resource/IT Department</code></p>

<p>I have used pos tagging to tag the words after tokenizing. I was able to extract 300,000 but not able to extract $ sign along with it.</p>

<p>What I have so far:</p>

<pre><code>text = '80% of $300,000 Each Human Resource/IT Department'
train_text = text
sample_text = text
custom_sent_tokenizer = PunktSentenseTokenizer(train_text)
tokenized = custom_sent_tokenizer.tokenize(sample_text)

for i in tokenized:
    words = nltk.word_tokenize(i)
    tagged = nltk.pos_tag(words)

chunkGram = r""""""chunk: {&lt;DT&gt;+&lt;NN.*&gt;+&lt;NN.*&gt;?|&lt;NNP&gt;?|&lt;CD&gt;+&lt;NN&gt;?|&lt;NNP&gt;?}""""""


chunkParser = nltk.RegexpParser(chunkGram)
chunked = chunkParser.parse(tagged)
</code></pre>

<p>chunked output when coverted to list  - <code>['80 %', '300,000', 'Each Human Resource/IT Department']</code></p>

<p>What I wanted : <code>['80 %', '**$**300,000', 'Each Human Resource/IT Department']</code></p>

<p>I tried </p>

<p><code>chunkGram = r""""""chunk: {**&lt;/$CD&gt;|**&lt;DT&gt;+&lt;NN.*&gt;+&lt;NN.*&gt;?|&lt;NNP&gt;?|&lt;CD&gt;+&lt;NN&gt;?|</code>?}""""""</p>

<p>It still doesn't work. So, all I need is a <strong>$</strong> along with <strong>CD</strong></p>
"
"38105014","Summarization of simple Q&A","2016-06-29 16:13:21","1","98","0","2","","38237004","<p>Is there a way to generate a one-sentence summarization of Q&amp;A pairs?</p>

<p>For example, provided:</p>

<pre><code>Q: What is the color of the car?
A: Red
</code></pre>

<p>I want to generate a summary as </p>

<pre><code>The color of the car is red
</code></pre>

<p>Or, given</p>

<pre><code>Q: Are you a man?
A: Yes
</code></pre>

<p>to</p>

<pre><code>Yes, I am a man.
</code></pre>

<p>which accounts for both question and answer.</p>

<p>What would be some of the most reasonable ways to do this?</p>
"
"38045290","Text Summarization Evaluation - BLEU vs ROUGE","2016-06-27 03:02:43","59","45036","0","3","","39190391","<p>With the results of two different summary systems (sys1 and sys2) and the same reference summaries, I evaluated them with both BLEU and ROUGE. The problem is: All ROUGE scores of sys1 was higher than sys2 (ROUGE-1, ROUGE-2, ROUGE-3, ROUGE-4, ROUGE-L, ROUGE-SU4, ...) but the BLEU score of sys1 was less than the BLEU score of sys2 (quite much).</p>
<p>So my question is: Both ROUGE and BLEU are based on n-gram to measure the similar between the summaries of systems and the summaries of human. So why there are differences in results of evaluation like that? And what's the main different of ROUGE vs BLEU to explain this issue?</p>
"
"38034266","Using Syntaxnet POS tags in python?","2016-06-26 00:47:16","1","1233","0","3","","38275718","<p>I want to use the parser tags (ex: VBD ROOT, NN nsubj, etc.) of syntaxnet in python to help create a chatterbot. The input is done in the console. </p>

<p>Question: How can I get in a variable the result of only VBP ROOT and nothing else? I was able to print the ASCII version of the parse tree with this call: <code>subprocess.call([""echo 'Bob brought the pizza to Alice.' | syntaxnet/demo.sh""], shell =True)</code> but I am a bit confuse on how to get to a particular variable and store it in a python variable.</p>

<p>p.s: I started to learn python a week ago.</p>
"
"38019823","Faster Lemmatization techniques in Python","2016-06-24 18:21:14","7","7355","4","1","","38020540","<p>I am trying to find out a faster way to lemmatize words in a list (named <i> <strong>text</strong></i>) using the NLTK Word Net Lemmatizer. Apparently this is the most time consuming step in my whole program(used cProfiler to find the same). </p>

<p>Following is the piece of code that I am trying to optimize for speed -</p>

<pre><code>def lemmed(text):
    l = len(text)
    i = 0
    wnl = WordNetLemmatizer()
    while (i&lt;l):
        text[i] = wnl.lemmatize(text[i])
        i = i + 1
    return text
</code></pre>

<p>Using the lemmatizer decreases my performance by 20x. Any help would be appreciated.</p>
"
"37994012","Best parser algorithm for lexical structure transfer?","2016-06-23 14:06:46","0","347","2","2","","38227611","<p>As part of a bigger project I want to implement a machine translator from language <em>A</em> to language <em>B</em>. Since there are not available tools that automatically do machine translation over this set of languages, and the available corpus of language B is quite small, <strong>I am trying to do the following</strong>:</p>

<p><strong>1.</strong> Given a sentence in language <em>A</em>, use a tool to get its set of language <em>A</em> PoS (Part-of-speech) tags.</p>

<p><strong>2.</strong> The tool I am using for PoS tagging (Freeling) does not return a parse tree, so I thought on building my own parse tree from the set of tags.</p>

<p><strong>3.</strong> After the parse tree is completed, traverse it by levels (starting on the root) and reorder its elements according to the grammar rules of language <em>B</em>.</p>

<p><a href=""https://en.wikipedia.org/wiki/Apertium#/media/File:Pipeline_of_Apertium_System.svg"" rel=""nofollow noreferrer"">Graphical explanation</a></p>

<p><a href=""https://i.sstatic.net/IgG3h.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/IgG3h.png"" alt=""enter image description here""></a></p>

<p>After doing some research I found out about Earley parsing (whose ability of parsing any language caught my attention because grammar on language <em>B</em> might change overtime, so I cannot guarantee that it will always meet any specific criterion).
<strong>However, given that my ultimate goal is doing structure transfer I am not sure if using a bottom-up parser and trying to reorder the elements as I match them with the rules would give me a better performance, or if I am on the wrong path and my solution is wrong altogether</strong>.</p>
"
"37982632","Translation API with candidates","2016-06-23 04:55:07","1","80","0","1","","37986772","<p>I am looking for a translation API that outputs all the candidates and not just single ""best"" candidate.</p>

<p>All statistical machine translation systems at the last stage score the list of translation candidates and choice the best candidate. I wonder if there is a system like Google translate or Microsoft translate that returns the list of all possible candidates so that I will be able to score them by myself.</p>

<p>Thanks.</p>
"
"37960667","Training IOB Chunker using nltk.tag.brill_trainer (Transformation-Based Learning)","2016-06-22 06:56:41","2","980","0","1","","40508641","<p>I'm trying to train a specific chunker (let's say a noun chunker for simplicity) by using <a href=""http://www.nltk.org/api/nltk.tag.html?highlight=brilltaggertrainer#nltk.tag.brill_trainer.BrillTaggerTrainer.train"" rel=""nofollow"">NLTK's brill module</a>. I'd like to use three features, ie. word, POS-tag, IOB-tag. </p>

<ul>
<li><p><a href=""http://www.aclweb.org/anthology/W95-0107"" rel=""nofollow"">(Ramshaw and Marcus, 1995:7)</a> have shown 100 templates which are generated from the combination of those three features, for example,</p>

<pre><code>W0, P0, T0     # current word, pos tag, iob tag
W-1, P0, T-1   # prev word, pos tag, prev iob tag
...
</code></pre></li>
</ul>

<p>I want to incorporate them into <a href=""http://www.nltk.org/api/nltk.tbl.html?highlight=nltk.tbl.feature#module-nltk.tbl.feature"" rel=""nofollow""><code>nltk.tbl.feature</code></a>, but there are only two kinds of feature objects, ie. <a href=""http://www.nltk.org/api/nltk.tag.html?highlight=brill.word#nltk.tag.brill.Word"" rel=""nofollow""><code>brill.Word</code></a> and <a href=""http://www.nltk.org/api/nltk.tag.html?highlight=brill.pos#nltk.tag.brill.Pos"" rel=""nofollow""><code>brill.Pos</code></a>. Limited by the design, I could only put word and POS feature together like (word, pos), and thus used ( (word, pos), iob) as features for training. For example,</p>

<pre><code>from nltk.tbl import Template
from nltk.tag import brill, brill_trainer, untag
from nltk.corpus import treebank_chunk
from nltk.chunk.util import tree2conlltags, conlltags2tree

# Codes from (Perkins, 2013)
def train_brill_tagger(initial_tagger, train_sents, **kwargs):
    templates = [
        brill.Template(brill.Word([0])),
        brill.Template(brill.Pos([-1])),
        brill.Template(brill.Word([-1])),
        brill.Template(brill.Word([0]),brill.Pos([-1])),]
    trainer = brill_trainer.BrillTaggerTrainer(initial_tagger, templates, trace=3,)
    return trainer.train(train_sents, **kwargs)

# generating ((word, pos),iob) pairs as feature.
def chunk_trees2train_chunks(chunk_sents):
    tag_sents = [tree2conlltags(sent) for sent in chunk_sents]
    return [[((w,t),c) for (w,t,c) in sent] for sent in tag_sents]

&gt;&gt;&gt; from nltk.tag import DefaultTagger
&gt;&gt;&gt; tagger = DefaultTagger('NN')
&gt;&gt;&gt; train = treebank_chunk.chunked_sents()[:2]
&gt;&gt;&gt; t = chunk_trees2train_chunks(train)
&gt;&gt;&gt; bt = train_brill_tagger(tagger, t)
TBL train (fast) (seqs: 2; tokens: 31; tpls: 4; min score: 2; min acc: None)
Finding initial useful rules...
    Found 79 useful rules.

           B      |
   S   F   r   O  |        Score = Fixed - Broken
   c   i   o   t  |  R     Fixed = num tags changed incorrect -&gt; correct
   o   x   k   h  |  u     Broken = num tags changed correct -&gt; incorrect
   r   e   e   e  |  l     Other = num tags changed incorrect -&gt; incorrect
   e   d   n   r  |  e
------------------+-------------------------------------------------------
  12  12   0  17  | NN-&gt;I-NP if Pos:NN@[-1]
   3   3   0   0  | I-NP-&gt;O if Word:(',', ',')@[0]
   2   2   0   0  | I-NP-&gt;B-NP if Word:('the', 'DT')@[0]
   2   2   0   0  | I-NP-&gt;O if Word:('.', '.')@[0]
</code></pre>

<p>As shown above, (word, pos) are treated one feature as a whole. This is not a perfect capture of three features (word, pos-tag, iob-tag).</p>

<ul>
<li>Any other ways to implement word, pos, iob features seperately into <code>nltk.tbl.feature</code>?</li>
<li>If it is impossible in NLTK, are there other implementations of them in python? I was only able to find C++ and Java implementations on the internet.</li>
</ul>
"
"37782569","How to find most frequent noun following the word 'the'?","2016-06-13 05:46:57","4","2039","0","2","","37794246","<pre><code>from nltk.corpus import brown

tagged = brown.tagged_words(tagset='universal')
</code></pre>

<p>I understand that to find the most frequent word following 'the' is done like so </p>

<pre><code>cfd3 = nltk.ConditionalFreqDist(nltk.bigrams(brown.words())

cfd3['the'].max()
</code></pre>

<p>however, how would one go about finding the most frequent noun following the word 'the' </p>
"
"37738333","Creating a full nltk parse tree from a list of nltk subtrees in python 3.5","2016-06-10 00:44:14","0","818","5","1","","37738709","<p>I have list of subtrees which I derived from a parse history formatted as follows:</p>

<p>The parse history:</p>

<pre><code>parse = [('S', 0), ('NP', 1), ('Det', 0), ('N', 0), ('VP', 1), ('V', 4), ('NP', 2), ('NP', 0), ('PN', 1), ('NP', 1), ('Det', 0), ('N', 3)]
</code></pre>

<p>Each tuple in the list has a key to a grammar dictionary which contains a list of rules. The second item in the tuple is the index of the rule for that given key. </p>

<p>The grammar is: </p>

<pre><code>grammar = {'S': [['NP', 'VP']],
               'NP': [['PN'], ['Det', 'N']],
               'VP': [['V'], ['V', 'NP', 'NP']],
               'PN': [['John'], ['Mary'], ['Bill']],
               'Det': [['the'], ['a']],
               'N': [['man'], ['woman'], ['drill sergeant'], ['dog']],
               'V': [['slept'], ['cried'], ['assaulted'],
                     ['devoured'], ['showed']]}
</code></pre>

<p>The list of subtrees is:</p>

<pre><code>[Tree('S', ['NP', 'VP']), Tree('NP', ['Det', 'N']), Tree('Det', ['the']), Tree('N', ['man']), Tree('VP', ['V', 'NP', NP]), Tree('V', ['showed']), Tree('NP', ['PN']), Tree('PN', ['Mary']), Tree('NP', ['Det', 'N']), Tree('Det', ['the']), Tree('N', ['dog'])]
</code></pre>

<p>I created the list of subtrees using the following code:</p>

<pre><code>for item in parse:
        apple = Tree(item[0], grammar[item[0]][item[1]])
        trees.append(apple)
</code></pre>

<p>The output I got when I printed the trees (which I know isn't the correct method but it at least shows the subtrees) is as follows:</p>

<pre><code>(S NP VP)
(NP Det N)
(Det the)
(N man)
(VP V NP)
(V showed)
(NP NP NP)
(NP PN)
(PN Mary)
(NP Det N)
(Det the)
(N dog)
</code></pre>

<p>Thanks for the help!</p>

<p>::EDIT::</p>

<p>The correct output should look like this:</p>

<pre><code>(S(NP(Det the)(N man))(VP(V showed)(NP(PN Mary))(NP(Det the)(N dog))))
</code></pre>
"
"37701305","Where to find an exhaustive list of stop words?","2016-06-08 11:31:16","3","3430","0","2","","37712453","<p>Where could I find an exhaustive list of stop words? The one I have is quite short and it seems to be inapplicable to scientific texts. 
I am creating lexical chains to extract key topics from scientific papers. The problem is that words like <code>based</code>, <code>regarding</code>, etc. should also be considered as stop words as they do not deliver much sense.</p>
"
"37672070","Parsing a sentence with SharpNL & en-parser-chunking.bin","2016-06-07 06:34:03","0","440","0","1","","37739991","<p>Using <a href=""https://github.com/knuppe/SharpNL/blob/33b55c10891ddc354251b394b721152f294a2798/src/SharpNL.Tests/Parser/TreeInsert/ParserTest.cs"" rel=""nofollow"">SharpNL</a> and <a href=""http://opennlp.sourceforge.net/models-1.5/"" rel=""nofollow"">OpenNLP</a>'s <code>en-parser-chunking.bin</code>, I'm attempting to parse a sentence into a tree. One of SharpNL's tests shows that, given a model, you can parse a sentence as follows:</p>

<pre><code>var model = SharpNL.Parser.TreeInsert.Parser.Train(""en"", parseSamples, headRules, 100, 0);

var parser = ParserFactory.Create(model);

// Tests parsing to make sure the code does not has
// a bug which fails always with a runtime exception
var p = parser.Parse(Parse.ParseParse(""She was just another freighter from the "" +
        ""States and she seemed as commonplace as her name .""));
</code></pre>

<p>So I downloaded the en-parser-chunking.bin file, created a model from it as well as a parser and attempted to parse the same input:</p>

<pre><code>var parserModelStream = new FileStream(@""en-parser-chunking.bin"", FileMode.Open, FileAccess.Read);
var parserModel = new ParserModel(parserModelStream);
var parser = ParserFactory.Create(parserModel);

var p = parser.Parse(Parse.ParseParse(""She was just another freighter from the "" +
        ""States and she seemed as commonplace as her name .""));
</code></pre>

<p>This code runs, but when I analyze <code>p</code> in the debugger, it has a Head of TOP and no children. Is this an issue with what model I'm using? Or how I'm using it?</p>
"
"37640057","Checking grammar using a custom module for all possible combinations of words in string","2016-06-05 08:48:59","0","143","0","1","","37640182","<p>I am creating a software for a school, where students would actually type in sentence and then their grammar would be checked however they would be given random combination of words like</p>

<pre><code>    The brown quick fox fence over jumped the
</code></pre>

<p>from this they would have to figure out the sentence and rewrite the sentence with correct grammar. When their answer is wrong, I want the program to rearrange the sentence for all possible combinations and then check the grammar for every single possible combination.</p>

<p>To get the random arrangement of sentences I use,</p>

<pre><code>    text = raw_input(""You:"")
    #shuffling for all possibilities
    def randm(text):
          text = text.split("" "")
          for i in itertools.permutations(text): 
                    rnd_text = "" "".join(i) 
</code></pre>

<p>And then I have my own module to check grammar with the method,</p>

<pre><code>    engrammar.grammar_cache(rnd_text)
</code></pre>

<p>When rnd_text is passed as the argument for the above method, if it is grammatically correct, the rearranged text will be displayed with correct grammar. So how do I pass <strong>a single output from the ""for loop"" one at a time to the method that I have to check grammar FOR ALL POSSIBLE OUTPUTS?</strong></p>
"
"37622518","python: modify PerceptronTagger in nltk to recognize 'and/or'","2016-06-03 20:04:11","0","44","0","1","","37636274","<p>How can I modify <code>PerceptronTagger</code> in <code>nltk</code> module (or maybe add some temporary functionality to it) so that it recognizes 'and/or' as 'CC' tag?</p>
"
"37611061","spaCy token.tag_ full list","2016-06-03 09:46:20","17","29126","0","6","","37612211","<p>The official documentation of <a href=""https://spacy.io/docs#token-postags"" rel=""noreferrer""><code>token.tag_</code></a> in <code>spaCy</code> is as follows:</p>

<blockquote>
  <p>A fine-grained, more detailed tag that represents the word-class and some basic morphological information for the token. These tags are primarily designed to be good features for subsequent models, particularly the syntactic parser. They are language and treebank dependent. The tagger is trained to predict these fine-grained tags, and then a mapping table is used to reduce them to the coarse-grained  .pos tags.</p>
</blockquote>

<p>But it doesn't list the full available tags and each tag's explanation. Where can I find it?</p>
"
"37525012","Error while installating YamCha Package","2016-05-30 11:51:31","2","271","1","1","","37525550","<p>I try to install <a href=""http://chasen.org/~taku/software/yamcha/"" rel=""nofollow"">YamCha</a> tool for NLP tasks, like NER, POS, and chunking.</p>

<p>While trying to install, I followed the installation steps</p>

<pre><code>% ./configure 
% make
% make check
% su
# make install
</code></pre>

<p>I got the following error messages:-</p>

<blockquote>
  <p>param.cpp: In member function 'bool YamCha::Param::open(int, char**, const YamCha::Option*)':
      param.cpp:102:42: error: 'strlen' was not declared in this scope
              size_t nlen = strlen (opts[i].name);
                                                ^
      param.cpp:103:68: error: 'strncmp' was not declared in this scope
              if (nlen == len &amp;&amp; strncmp (&amp;argv[ind][2], opts[i].name, len) == 0) {
                                                                          ^
      param.cpp: In member function 'bool YamCha::Param::open(const char*, const YamCha::Option*)':
      param.cpp:182:28: error: 'strncpy' was not declared in this scope
           strncpy (str, arg, 1024);
                                  ^
      param.cpp:185:12: warning: deprecated conversion from string constant to 'char*' [-Wwrite-strings]
       make  all-recursive
      make<a href=""http://chasen.org/~taku/software/yamcha/"" rel=""nofollow"">1</a>: Entering directory <code>/home/hamada/Documents/YamCha/yamcha-0.33'
      Making all in src
      make[2]: Entering directory</code>/home/hamada/Documents/YamCha/yamcha-0.33/src'
      /bin/bash ../libtool --mode=compile --tag=CXX g++ -DHAVE_CONFIG_H -I. -I. -I..     -O3 -Wno-deprecated -Wall -c -o param.lo param.cpp
       g++ -DHAVE_CONFIG_H -I. -I. -I.. -O3 -Wno-deprecated -Wall -c param.cpp  -fPIC -DPIC -o .libs/param.o
           ptr[0] = PACKAGE;
                  ^
      param.cpp: In member function 'void YamCha::Param::help(std::ostream&amp;, const YamCha::Option*)':
      param.cpp:205:42: error: 'strlen' was not declared in this scope
             size_t l = 1 + strlen (opts[i].name);
                                                ^
      param.cpp:211:38: error: 'strlen' was not declared in this scope
             size_t l = strlen (opts[i].name);
                                            ^
      make[2]: <strong>* [param.lo] Error 1
      make[2]: Leaving directory <code>/home/hamada/Documents/YamCha/yamcha-0.33/src'
      make[1]: *** [all-recursive] Error 1
      make[1]: Leaving directory</code>/home/hamada/Documents/YamCha/yamcha-0.33'
      make: *</strong> [all] Error 2</p>
</blockquote>
"
"37449729","How to parse Penn Tree Bank and get all the child trees using stanford NLP?","2016-05-26 00:32:52","1","549","0","1","","37505799","<p>Is there a way to parse the PTB tree below to get all the child trees
for example:</p>

<pre><code>Text   :  Today is a nice day.
PTB : (3 (2 Today) (3 (3 (2 is) (3 (2 a) (3 (3 nice) (2 day)))) (2 .)))
</code></pre>

<p>Need All child trees possible</p>

<pre><code>Output  : 
(3 (2 Today) (3 (3 (2 is) (3 (2 a) (3 (3 nice) (2 day)))) (2 .)))
(2 Today)
(3 (3 (2 is) (3 (2 a) (3 (3 nice) (2 day)))) (2 .))
(3 (2 is) (3 (2 a) (3 (3 nice) (2 day))))
(3 (2 is) (3 (2 a) (3 (3 nice) (2 day))))
(2 is)
(3 (2 a) (3 (3 nice) (2 day)))
(2 a)
(3 (3 nice) (2 day))
(3 nice)
(2 day)
(2 .)
</code></pre>
"
"37443138","Python stemming (with pandas dataframe)","2016-05-25 16:41:21","12","34765","1","1","","37444102","<p>I created a dataframe with sentences to be stemmed.
I would like to use a Snowballstemmer to obtain higher accuracy with my classification algorithm. How can I achieve this?</p>
<pre><code>import pandas as pd
from nltk.stem.snowball import SnowballStemmer

# Use English stemmer.
stemmer = SnowballStemmer(&quot;english&quot;)

# Sentences to be stemmed.
data = [&quot;programmers program with programming languages&quot;, &quot;my code is working so there must be a bug in the interpreter&quot;] 
    
# Create the Pandas dataFrame.
df = pd.DataFrame(data, columns = ['unstemmed']) 

# Split the sentences to lists of words.
df['unstemmed'] = df['unstemmed'].str.split()

# Make sure we see the full column.
pd.set_option('display.max_colwidth', -1)

# Print dataframe.
df 

+----+---------------------------------------------------------------+
|    | unstemmed                                                     |
|----+---------------------------------------------------------------|
|  0 | ['programmers', 'program', 'with', 'programming', 'languages']|
|  1 | ['my', 'code', 'is', 'working', 'so', 'there', 'must',        |  
|    |  'be', 'a', 'bug', 'in', 'the', 'interpreter']                |
+----+---------------------------------------------------------------+
</code></pre>
"
"37403563","Python: Newspaper Module - Any way to pool getting articles straight from URLs?","2016-05-24 02:47:00","4","2935","2","4","","38232020","<p>I'm using the Newspaper module for python found <a href=""http://newspaper.readthedocs.io/en/latest/user_guide/advanced.html"" rel=""nofollow"">here</a>.</p>

<p>In the tutorials, it describes how you can pool the building of different newspapers s.t. it generates them at the same time. (see the ""Multi-threading article downloads"" in the link above)</p>

<p>Is there any way to do this for pulling articles straight from a LIST of urls? That is, is there any way I can pump in multiple urls into the following set-up and have it download and parse them concurrently?</p>

<pre><code>from newspaper import Article
url = 'http://www.bbc.co.uk/zhongwen/simp/chinese_news/2012/12/121210_hongkong_politics.shtml'
a = Article(url, language='zh') # Chinese
a.download()
a.parse()
print(a.text[:150])
</code></pre>
"
"37317749","Where can I get CoNLL-X training data?","2016-05-19 08:23:40","3","2127","0","1","","37358498","<p>I'm trying to train the Stanford Neural Network Dependency Parser to check phrase similarity.</p>

<p>The way I tried is:</p>

<pre><code>java edu.stanford.nlp.parser.nndep.DependencyParser -trainFile trainPath -devFile devPath -embedFile wordEmbeddingFile -embeddingSize wordEmbeddingDimensionality -model modelOutputFile.txt.gz
</code></pre>

<p>The error that I got is:</p>

<pre><code>Train File: C:\Users\rohit\Downloads\CoreNLP-master\CoreNLP-master\data\edu\stanford\nlp\parser\trees\en-onetree.txt
Dev File: null
Model File: modelOutputFile.txt.gz
Embedding File: null
Pre-trained Model File: null
################### Train
#Trees: 1
0 tree(s) are illegal (0.00%).
1 tree(s) are legal but have multiple roots (100.00%).
0 tree(s) are legal but not projective (0.00%).
###################
#Word: 3
#POS:3
#Label: 2
###################
#Transitions: 3
#Labels: 1
ROOTLABEL: null
Random generator initialized with seed 1459831358061
Exception in thread ""main"" java.lang.NullPointerException
    at edu.stanford.nlp.parser.nndep.Util.scaling(Util.java:49)
    at edu.stanford.nlp.parser.nndep.DependencyParser.readEmbedFile.  (DependencyParser.java:636)
    at edu.stanford.nlp.parser.nndep.DependencyParser.setupClassifierForTraining(DependencyParser.java:787)
    at edu.stanford.nlp.parser.nndep.DependencyParser.train(DependencyParser.java:676)
    at edu.stanford.nlp.parser.nndep.DependencyParser.main(DependencyParser.java:1247)
</code></pre>

<p>The help embedded within the code says that the training file should be a - ""Path to a training treebank in CoNLL-X format"". </p>

<p>Does anyone know where I can find some CoNLL-X training data to train?
I gave training file but not embedding file and got this error.
My guess is if I give the embedding file it might work.</p>

<p>Please shed some light on which training file &amp; embedding file I should use and where I can find them.</p>
"
"37311984","python - extract dates from text by giving as parameter the date of reference which is not the current date","2016-05-19 00:11:11","1","176","0","1","","37312115","<p>I have some fuzzy text which contain information about dates. For example: 'Concert this Saturday'. I want to extract the date that corresponds to ""this Saturday"" by <strong>giving the date of reference as parameter.</strong> 
For example, suppose this is a subject of an email sent on 2016-04-13 and I want to get that ""this Saturday"" that this email was referring to, was on 2016-04-16. Do you know of any package that is able to do that?</p>

<p>P.S.I have used the dateutil.parser but this doesn't take a reference date as a parameter and it gives me as date the following Saturday from the date I'm running the code.</p>
"
"37133079","Choose the best NLP parsers","2016-05-10 08:29:30","-2","437","0","1","","37133213","<p>I want to analyze a sentence using a context free grammar for NLP tasks.
I want to know which grammar parsers as Stanford Parser, Malt Parser,... would be great?</p>

<p>What are advantages and disadvantages about syntactic parsing and dependency parsing in those parsers?</p>

<p>How can they support library for programming languages as Java, PHP,...?</p>
"
"37130722","How to keep punctuation in Stanford dependency parser","2016-05-10 06:24:50","2","567","0","1","","37151286","<p>I am using Stanford CoreNLP (01.2016 version) and I would like to keep the punctuation in the dependency relations. I have found some ways for doing that when you run it from command line, but I didn't find anything regarding the java code which extracts the dependency relations.</p>

<p>Here is my current code. It works, but no punctuation is included:</p>

<pre><code>Annotation document = new Annotation(text);

        Properties props = new Properties();

        props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, parse"");

        props.setProperty(""ssplit.newlineIsSentenceBreak"", ""always"");

        props.setProperty(""ssplit.eolonly"", ""true"");

        props.setProperty(""pos.model"", modelPath1);

        props.put(""parse.model"", modelPath );

        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

        pipeline.annotate(document);

        LexicalizedParser lp = LexicalizedParser.loadModel(modelPath + lexparserNameEn,

                ""-maxLength"", ""200"", ""-retainTmpSubcategories"");

        TreebankLanguagePack tlp = new PennTreebankLanguagePack();

        GrammaticalStructureFactory gsf = tlp.grammaticalStructureFactory();

        List&lt;CoreMap&gt; sentences = document.get(SentencesAnnotation.class);

        for (CoreMap sentence : sentences) {

            List&lt;CoreLabel&gt; words = sentence.get(CoreAnnotations.TokensAnnotation.class);               

            Tree parse = lp.apply(words);

            GrammaticalStructure gs = gsf.newGrammaticalStructure(parse);
            Collection&lt;TypedDependency&gt; td = gs.typedDependencies();

            parsedText += td.toString() + ""\n"";
</code></pre>

<p>Any kind of dependency relation is OK for me, basic, typed, collapsed, etc.
I just want to include the punctuation marks.</p>

<p>Thanks in advance,</p>
"
"37104242","Definition of simplified tags in NLP?","2016-05-08 20:01:34","1","132","2","1","","37106299","<p>I'm having trouble trying to find a good resource that would tell me what simplified tags in <code>NLP</code> mean. I understand that you can use:</p>

<pre><code>nltk.help.upenn_tagset()
</code></pre>

<p>to help determine what the tags mean if we are using unsimplified version of tagging; however, I can't find a resource for the simplified version.</p>

<p>I really am only working with:</p>

<pre><code>PRON
VERB
NOUN
DET
ADJ
ADV
PRT
ADP
CONJ
NUM
</code></pre>

<p>I can obviously get what some of these mean, but am having trouble with others (i.e. <code>PRT</code>...etc.)</p>

<p>Can anyone help point me to a resource in defining these tags?</p>
"
"37056797","ntlk: how to get inflections of words","2016-05-05 17:33:54","3","2144","1","1","","37065916","<p>I have a list of words, nearly 5000 English words, and for each word I need these inflectional forms:</p>

<p>noun: singular and plural</p>

<p>verb: infinitive, present simple, present simple 3rd person, past simple, present participle (ing form), past participle</p>

<p>adjective: comparative and superlative</p>

<p>adverb</p>

<p>How can I extract these information from a given word (e.g. help) in ntlk via python? (or maybe there is a ready made list)</p>
"
"36966019","How areTF-IDF calculated by the scikit-learn TfidfVectorizer","2016-05-01 11:16:24","21","13735","0","3","","36972265","<p>I run the following code to convert the text matrix to TF-IDF matrix.</p>
<pre><code>text = ['This is a string','This is another string','TFIDF computation calculation','TfIDF is the product of TF and IDF']

from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer(max_df=1.0, min_df=1, stop_words='english',norm = None)
                    
X = vectorizer.fit_transform(text)
X_vocab = vectorizer.get_feature_names_out()
X_mat = X.todense()
X_idf = vectorizer.idf_
</code></pre>
<p>I get the following output</p>
<p>X_vocab =</p>
<pre><code>[u'calculation',
 u'computation',
 u'idf',
 u'product',
 u'string',
 u'tf',
 u'tfidf']
</code></pre>
<p>and X_mat =</p>
<pre><code>  ([[ 0.        ,  0.        ,  0.        ,  0.        ,  1.51082562,
      0.        ,  0.        ],
    [ 0.        ,  0.        ,  0.        ,  0.        ,  1.51082562,
      0.        ,  0.        ],
    [ 1.91629073,  1.91629073,  0.        ,  0.        ,  0.        ,
      0.        ,  1.51082562],
    [ 0.        ,  0.        ,  1.91629073,  1.91629073,  0.        ,
      1.91629073,  1.51082562]])
</code></pre>
<p>Now I dont understand how these scores are computed. My idea is that for the text[0], score for only 'string' is computed and there is a score in the 5th coloumn. But as TF_IDF is the product of term frequency which is 2 and IDF which is log(4/2) is 1.39 and not 1.51 as shown in the matrix. How is the TF-IDF score calculated in scikit-learn.</p>
"
"36901948","Does CKY really require CNF?","2016-04-27 22:19:03","4","1539","0","1","","36902434","<p>I've read a number of places that the CYK/CKY algorithms require the grammar to be in Chomsky Normal Form (CNF), e.g.</p>

<blockquote>
  <p>The standard version of CYK operates only on context-free grammars
  given in Chomsky normal form (CNF) ~<a href=""https://en.wikipedia.org/wiki/CYK_algorithm"" rel=""nofollow"">Wikipedia</a></p>
</blockquote>

<p>However, I've also seen a number of examples of CKY algorithms where the grammar was not in CNF.  A common example that Christopher Manning uses is ""fish people fish tanks"" (ref: <a href=""http://nlp.stanford.edu/courses/lsa354/SLoSP-2007-4.ppt"" rel=""nofollow"">PPT slide #19</a>) which contains unary rules:</p>

<pre><code>S -&gt; NP VP [0.9]
S -&gt; VP [0.1]
VP -&gt; V NP [0.4]
Vp -&gt; V [0.6]
...
</code></pre>

<p>I've also seen other examples demonstrating CKY that use three non-terminals in the RHS of the production (e.g. <code>VP -&gt; Verb NP NP</code> <a href=""http://web.engr.illinois.edu/~juliahmr/cs598/Slides/Lecture3.pdf#page=43"" rel=""nofollow"">reference</a>).  Why the discrepancy?</p>
"
"36768459","filterLogging not working on Database.Persist.Sql's runSqlPool function","2016-04-21 11:38:43","2","290","0","1","","36802718","<p>The Haskell library Database.Persist.Sqlite includes functions that run within a LoggingT context, to control debugging output.  So I expected to be able to limit the debugging output they produce, thus:</p>

<pre><code>runStdoutLoggingT . filterLogger (\_ _ -&gt; False) (runSqlPool (insertBy myData) myPool)
</code></pre>

<p>(condensed and simplified from my actual code) However, it doesn't suppress logging.  The evalation of insertBy produces a line on stdout of the form</p>

<pre><code>[Debug#SQL] SELECT ""id"",""key"",""data_source_row_id"",""loaded"" FROM ""data_row"" WHERE ""key""=? AND ""data_source_row_id""=?; [PersistText blahblahblah]
</code></pre>

<p>So why isn't the output suppressed by the filterLogger call ?</p>

<p>Since the question has received two downvotes, I'll add that the pattern shown above (i.e., runStdoutLoggingT . filterLogger) is used in many GitHub projects and I can't see how my application is any different.  It is somewhat frustrating to be downvoted without explanation or means of recourse.</p>
"
"36739843","Celery raised unexpected LookUp Error","2016-04-20 09:43:17","0","322","0","1","","36854352","<p>I am using celery for my django project. </p>

<pre><code>java_path = ""/your/Java/jdk/home/java.exe
os.environ['JAVAHOME'] = java_path

st = POSTagger('/your/postagger/models/path/english-bidirectional-distsim.tagger','/your/postagger/jar/file/path/stanford-postagger.jar')
</code></pre>

<p><code>tag = st.tag([key])</code></p>

<p>here the key is a list of feature words.</p>

<p>I got following errors, when using celery to execute:</p>

<pre><code>raised unexpected: 
           LookupError('\n\n===========================================================================\nNLTK 
        `was unable to find the java file!\nUse software specific configuration paramaters or set the JAVAHOME environment`



variable.\n===========================================================================',)
        Traceback (most recent call last):
          File ""/Users/Envs/lib/python2.7/site-packages/celery/app/trace.py"", line 240, in trace_task
            R = retval = fun(*args, **kwargs)
          File ""/Users/Envs/lib/python2.7/site-packages/celery/app/trace.py"", line 438, in __protected_call__
            return self.run(*args, **kwargs)
          File ""/Users/Envs/src/evolvelearning/tasks.py"", line 772, in RunProgram
            tag = st.tag([key])
          File ""/Users/Envs/lib/python2.7/site-packages/nltk/tag/stanford.py"", line 59, in tag
            return self.tag_sents([tokens])[0]
        File ""/Users/Envs/lib/python2.7/site-packages/nltk/tag/stanford.py"", line 64, in tag_sents
        config_java(options=self.java_options, verbose=False)
      File ""/Users/Envs/lib/python2.7/site-packages/nltk/internals.py"", line 82, in config_java
        _java_bin = find_binary('java', bin, env_vars=['JAVAHOME', 'JAVA_HOME'], verbose=verbose, binary_names=['java.exe'])


        File ""/Users/Envs/lib/python2.7/site-packages/nltk/internals.py"", line 544, in find_binary
            binary_names, url, verbose))
          File ""/Users/Envs/lib/python2.7/site-packages/nltk/internals.py"", line 538, in find_binary_iter
            url, verbose):
          File ""/Users/Envs/lib/python2.7/site-packages/nltk/internals.py"", line 517, in find_file_iter
            raise LookupError('\n\n%s\n%s\n%s' % (div, msg, div))
        LookupError:

        ===========================================================================
    NLTK was unable to find the java file!
    Use software specific configuration paramaters or set the JAVAHOME environment variable.
    ===========================================================================
</code></pre>

<p>I have set <strong>java_path</strong> and <strong>javahome</strong> and I would like to know why still thess errors occur?</p>
"
"36519254","How to stem all words in an ngram, using quanteda?","2016-04-09 16:03:09","2","1471","2","1","","36532481","<p>I'm working with the Quanteda package in R at the moment, and I'd like to calculate the ngrams of a set of stemmed words to get a quick-and-dirty estimate of what content words tend to be near each other.  If I try:</p>

<pre><code>twitter.files &lt;- textfile(files)
twitter.docs &lt;- corpus(twitter.files)
twitter.semantic &lt;- twitter.docs %&gt;%
  dfm(removeTwitter = TRUE, ignoredFeatures = stopwords(""english""),
      ngrams = 2, skip = 0:3, stem = TRUE) %&gt;%
  trim(minCount = 50, minDoc = 2)
</code></pre>

<p>It only stems the final word in the bigrams.  However, if I try to stem first:</p>

<pre><code>twitter.files &lt;- textfile(files)
twitter.docs &lt;- corpus(twitter.files)
stemmed_no_stops &lt;- twitter.docs %&gt;%
   toLower %&gt;%
   tokenize(removePunct = TRUE, removeTwitter = TRUE) %&gt;%
   removeFeatures(stopwords(""english"")) %&gt;%
   wordstem

 twitter.semantic &lt;- stemmed_no_stops %&gt;%
   skipgrams(n = 2, skip = 0:2) %&gt;%
   dfm %&gt;%
   trim(minCount=25, minDoc = 2)
</code></pre>

<p>Then Quanteda doesn't know how to work with the stemmed list; I'll get the error:</p>

<pre><code>assignment of an object of class “NULL” is not valid for @‘ngrams’ 
in an object of class “dfmSparse”; is(value, ""integer"") is not TRUE
</code></pre>

<p>Is there an intermediate step I can do to use a dfm on the stemmed words, or to tell <code>dfm</code> to stem first and do ngrams second?</p>
"
"36512113","How to split sentences using the nltk.parse.stanford library","2016-04-09 02:41:26","0","3527","0","2","","36513178","<p>I'm trying to use the Stanford Parser from nltk.parse.stanford to do a bunch of NLP tasks. There are certain operations on sentences that I am able to do when I explicitly pass a sentence or a list of sentences as input. <strong>But how do I actually split a large amount of text into sentences</strong>? (Obviously, regex with periods etc. won't work well) </p>

<p>I checked the documentation here and found nothing: <a href=""http://www.nltk.org/api/nltk.parse.html?highlight=stanford#module-nltk.parse.stanford"" rel=""nofollow noreferrer"">http://www.nltk.org/api/nltk.parse.html?highlight=stanford#module-nltk.parse.stanford</a></p>

<p>I found something similar that does the job for java here: <a href=""https://stackoverflow.com/questions/9492707/how-can-i-split-a-text-into-sentences-using-the-stanford-parser"">How can I split a text into sentences using the Stanford parser?</a></p>

<p>I think I need something exactly like this for the python version of the library.</p>
"
"36230641","stemDocment in tm package not working on past tense word","2016-03-26 01:33:30","2","1935","0","1","","36234096","<p>I have a file 'check_text.txt' that contains ""<strong>said say says make made</strong>"". I'd like to perform stemming on it to get ""say say say make make"". I tried to use <code>stemDocument</code> in <code>tm</code> package, as the following, but only get ""said say say make made"". Is there a way to perform stemming on past tense words? Is it necessary to do so in real-world natural language processing? Thanks!</p>

<pre><code>filename = 'check_text.txt'
con &lt;- file(filename, ""rb"")
text_data &lt;- readLines(con,skipNul = TRUE)
close(con)
text_VS &lt;- VectorSource(text_data)
text_corpus &lt;- VCorpus(text_VS)
text_corpus &lt;- tm_map(text_corpus, stemDocument, language = ""english"")
as.data.frame(text_corpus)$text
</code></pre>

<p><strong>EDIT</strong>: I also tried <code>wordStem</code> in <code>SnowballC</code> package</p>

<pre><code>&gt; library(SnowballC)
&gt; wordStem(c(""said"", ""say"", ""says"", ""make"", ""made""))
[1] ""said"" ""sai""  ""sai""  ""make"" ""made""
</code></pre>
"
"36124329","How to prepare feature vectors for text classification when the words in the text is not frequently repeating?","2016-03-21 06:17:21","0","184","0","1","","36125057","<p>I need to perform the text classification on set of emails. But all the words in my text are thinly sparse i.e frequency of each word with respect to all the documents are very less. words are not that much frequently repeating. Since to train the classifiers I think document term matrix with frequency as weightage is not suitable. Can you please suggest me what kind of other methods I need to use . </p>

<p>Thanks</p>
"
"36109717","Stanford NLP POS Tagger has issues with very simple phrases?","2016-03-20 02:48:42","1","888","2","2","","36120762","<p>I found examples of inconsistent behavior in my application using Stanford NLP Parser/POS Tagger and I was able to replicate it online <a href=""http://nlp.stanford.edu:8080/corenlp/process"" rel=""nofollow noreferrer"">http://nlp.stanford.edu:8080/corenlp/process</a> . I am using version 3.60:</p>

<p>Here are the 3 issues I have found so far: </p>

<ul>
<li>Dot with or without inconsistency problem:</li>
</ul>

<p><a href=""https://i.sstatic.net/ELhNf.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ELhNf.png"" alt=""NLP Stanford POS Tagger with and without dot""></a></p>

<ul>
<li><p>Verbs that are found as Nouns
<a href=""https://i.sstatic.net/qHKLm.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/qHKLm.png"" alt=""enter image description here""></a></p></li>
<li><p>Verbs that are tagged as Adjectives
<a href=""https://i.sstatic.net/Qu6MJ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Qu6MJ.png"" alt=""enter image description here""></a></p></li>
</ul>

<p>I know that language is fairly ambiguous but I would like to know if I can trust this library even for those simple phrases. I would like to also know if I am doing something wrong. I tried the problematic cases of each of an example alone or in other words in separate sentences and the problem persists.</p>

<p>This is the expected behavior:</p>

<p><a href=""https://i.sstatic.net/vQczS.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/vQczS.png"" alt=""enter image description here""></a></p>

<p>Any help is appreciated! Thanks</p>
"
"36041738","Finding Tf-Idf Scores of only selected words from set of documents using scikit-learn","2016-03-16 16:38:51","3","1180","0","1","","36041915","<p>I have a set of documents (stored as <code>.txt</code> files). I Also have a python dictionary of some selected words. I want to assign tf-idf scores only to these words, and not all words, from the set of documents. How can this be done using <code>scikit-learn</code> or any other library ?</p>

<p>I have referred to this <a href=""http://blog.christianperone.com/2011/10/machine-learning-text-feature-extraction-tf-idf-part-ii/"" rel=""nofollow"">blog post</a> but it gives scores of full vocabulary.</p>
"
"36031018","NLTK PoS tagging","2016-03-16 09:05:41","0","937","0","1","","36031245","<p>I'm new in Python and need it for PoS tagging. Therefore I tried to use the standard tools. I tried to create a tagger and get a ValueError, that I don't understand.
My code:</p>

<pre><code>import nltk
tagged_sents = nltk.corpus.brown.tagged_sents(categories = 'reviews')
tagger =nltk.ClassifierBasedTagger(tagged_sents)
</code></pre>

<p>I have already checked, that tagged_sents is a list of all sentences. Each sentence self is a list of tuples (word, PoS), like in the documentation:</p>

<blockquote>
  <p>:param train: A tagged corpus consisting of a list of tagged
          sentences, where each sentence is a list of (word, tag) tuples.</p>
</blockquote>

<p>Why do I get the Value Error?</p>

<blockquote>
  <p>ValueError: Must specify either training data or trained model.</p>
</blockquote>
"
"36030428","How can I POS tag German texts?","2016-03-16 08:36:56","2","2723","0","3","","36030688","<p>I've been doing some natural language processing work. </p>

<p>For English POS tagging, it's rather simple because I only need to use built-in nltk functions. I want to process German texts similarly.</p>

<p>Since nltk doesn't have a built-in function for German, I've tried using Stanford POSTagger:</p>

<pre><code>from nltk.tag.stanford import StanfordPOSTagger
import os
import nltk
java_path = ""C:/Program Files/Java/jdk1.8.0_71/bin/java.exe""
os.environ['JAVAHOME'] = java_path
sentence = ""Man könnte Klöckner vorhalten, sich an ihre eigenen Appelle nicht zu halten. Doch niemand in der Union wagte das. Nicht einmal die von ihr attackierten Briefschreiber. Klöckner genießt im Moment Narrenfreiheit.""
tokens = nltk.word_tokenize(sentence, 'german')
german_postagger1 = StanfordPOSTagger(r'E:/python/nlptest/models/german-hgc.tagger', r'E:/python/nlptest/stanford-postagger.jar')
gp1 = german_postagger1.tag(tokens)
</code></pre>

<p>It takes almost 7 seconds to finish processing, which is unbearable for me.</p>

<p>I also tried the module <a href=""http://www.clips.ua.ac.be/pages/pattern-de"" rel=""nofollow"">Pattern</a>, but it doesn't support Python 3 and I'm using Python 3.4.</p>

<p>Is there an alternative and faster way to POS tag German sentences?</p>
"
"35963350","What is the NLTK FCFG grammar standard/specification?","2016-03-12 21:10:12","2","3113","3","3","","35964803","<p><strong>NLTK</strong> (Natural Language Toolkit) lets you parse a FCFG grammar using <code>nltk.FCFG.fromstring([grammar string here]).</code> Where is the FCFG grammar format specification*? I googled it to death, but all I could find was <a href=""http://www.nltk.org/howto/grammar.html"" rel=""nofollow"">this</a>. </p>

<p>*i.e. grammar language specification</p>
"
"35915075","NLTK Generate sentences without two occurences of the same word in Python","2016-03-10 11:24:45","1","846","1","1","","35930323","<p>I am using this NLTK code to generate sentences from demo_grammar (see below), and the problem is that with grammar rules like N N or N N N I end up with sentences like ""creation creation creation"". I am only interested in generating sentences where the same word doesn't occur twice (i.e. creation video software).</p>

<p>How could I do that? </p>

<p>The <code>generate.py</code> from NLTK is as such: <a href=""https://github.com/nltk/nltk/blob/develop/nltk/parse/generate.py"" rel=""nofollow"">https://github.com/nltk/nltk/blob/develop/nltk/parse/generate.py</a></p>

<p>I have tried the demo code from the <code>generate.py</code>:</p>

<pre><code>from nltk.grammar import CFG
from nltk.parse import generate    

demo_grammar = """"""
  S -&gt; NP VP
  NP -&gt; Det N
  PP -&gt; P NP
  VP -&gt; 'slept' | 'saw' NP | 'walked' PP
  Det -&gt; 'the' | 'a'
  N -&gt; 'man' | 'park' | 'dog'
  P -&gt; 'in' | 'with'
""""""

def demo(N=23):

    print('Generating the first %d sentences for demo grammar:' % (N,))
    print(demo_grammar)
    grammar = CFG.fromstring(demo_grammar)
    for n, sent in enumerate(generate(grammar, n=N), 1):
        print('%3d. %s' % (n, ' '.join(sent)))
</code></pre>
"
"35882925","NLTK tagger reading from txt","2016-03-09 04:31:19","0","587","0","1","","35923989","<p>I use NLTK on python. I want to read from txt for using default, unigram and pos tagger. However I did not do it because there is not specific import tag for txt. For example in the class, we are using prepared corpus like brown or etc. My question is how can I do import method for using taggers. Eventually, I want to see evaluate performance for each tagger. </p>
"
"35870282","NLTK: lemmatizer and pos_tag","2016-03-08 14:39:21","3","7756","4","1","","35871479","<p>I build a Plaintext-Corpus and the next step is to lemmatize all my texts. I'm using the <strong>WordNetLemmatizer</strong> and need the <strong>pos_tag</strong> for each token in order to do not get the Problem that e.g. loving -> lemma = loving and love -> lemma = love...</p>

<hr>

<p>The default WordNetLemmatizer-POS-Tag is n (=Noun) i think, but how can i use the pos_tag? I think the expected WordNetLemmatizer-POS-Tag are diffrent to the pos_tag i get. Is there a function or something that can help me?!?!</p>

<blockquote>
  <p>in this line i think the word_pos is wrong and that's the error-reason</p>
  
  <p>lemma = wordnet_lemmatizer.lemmatize(word,word_pos)</p>
</blockquote>

<pre><code>import nltk
from nltk.corpus import PlaintextCorpusReader
from nltk import sent_tokenize, word_tokenize, pos_tag
from nltk.stem import WordNetLemmatizer
wordnet_lemmatizer = WordNetLemmatizer()

corpus_root = 'C:\\Users\\myname\\Desktop\\TestCorpus'
lyrics = PlaintextCorpusReader(corpus_root,'.*')

for fileid in lyrics.fileids():
     tokens = word_tokenize(lyrics.raw(fileid))
     tagged_tokens = pos_tag(tokens)
     for tagged_token in tagged_tokens:
         word = tagged_token[0]
         word_pos = tagged_token[1]
         print(tagged_token[0])
         print(tagged_token[1])
         lemma = wordnet_lemmatizer.lemmatize(word,pos=word_pos)
         print(lemma)
</code></pre>

<hr>

<p><em>Additional Question:</em> Is the pos_tag enough for my lemmatization or need i another tagger? My texts are lyrics...</p>
"
"35836907","NLTK v3.2: Unable to nltk.pos_tag()","2016-03-07 05:40:31","4","3462","2","3","","35902494","<p>Hi text mining champions,</p>

<p>I'm using Anaconda with NLTK v3.2 on Windows 10.(client's environment)</p>

<p>When I try to POS tag, I keep getting a URLLIB2 error:</p>

<pre><code>URLError: &lt;urlopen error unknown url type: c&gt;
</code></pre>

<p>It seems urllib2 is unable to recognize windows paths? How can I work around this?</p>

<p>The command is simple as:</p>

<p><code>nltk.pos_tag(nltk.word_tokenize(""Hello World""))</code></p>

<p>edit:
There is a duplicate question, however I think the answers obtained here by manan and alvas are a better fix.</p>
"
"35741627","Memory Error when train TBL POS Tagger in Python","2016-03-02 08:08:29","0","117","1","1","","38298563","<p>When I try to train a corpus having 40K sentences, there is no problem. But when I train 86K sentences, I get error like this:</p>

<pre><code>ERROR:root:
Traceback (most recent call last):
  File ""CLC_POS_train.py"", line 95, in main
    train(sys.argv[10], encoding, flag_tagger, k, percent, eval_flag)
  File ""CLC_POS_train.py"", line 49, in train
    CLC_POS.process('TBL', train_data, test_data, flag_evaluate[1], flag_dump[1], 'pos_tbl.model' + postfix)
  File ""d:\WORKing\VCL\TEST\CongToan_POS\Source\CLC_POS.py"", line 184, in process
    tagger = CLC_POS.train_tbl(train_data)
  File ""d:\WORKing\VCL\TEST\CongToan_POS\Source\CLC_POS.py"", line 71, in train_tbl
    tbl_tagger = brill_trainer.BrillTaggerTrainer.train(trainer, train_data, max_rules=1000, min_score=3)
  File ""C:\Python34\lib\site-packages\nltk-3.1-py3.4.egg\nltk\tag\brill_trainer.py"", line 274, in train
    self._init_mappings(test_sents, train_sents)
  File ""C:\Python34\lib\site-packages\nltk-3.1-py3.4.egg\nltk\tag\brill_trainer.py"", line 341, in _init_mappings
    self._tag_positions[tag].append((sentnum, wordnum))
MemoryError
INFO:root:
</code></pre>

<p>I already used Python 3.5 in Windows 64-bit but still get this error.
This is the code used for training:</p>

<pre><code>t0 = RegexpTagger(MyRegexp.create_regexp_tagger())
t1 = nltk.UnigramTagger(train_data, backoff=t0)
t2 = nltk.BigramTagger(train_data, backoff=t1)
trainer = brill_trainer.BrillTaggerTrainer(t2, brill.fntbl37())
tbl_tagger = brill_trainer.BrillTaggerTrainer.train(trainer, train_data, max_rules=1000, min_score=3)
</code></pre>
"
"35690892","How to Stem Shakespere/KJV Using nltk.stem.snowball","2016-02-29 02:14:33","3","331","0","1","","35693506","<p>I want to stem early modern English text:</p>

<pre><code>sb.stem(""loveth"")
&gt;&gt;&gt; ""lov""
</code></pre>

<p>Apparently, all I need to do is <a href=""http://snowball.tartarus.org/texts/earlyenglish.html"" rel=""nofollow"">a small tweak</a> to the Snowball Stemmer:</p>

<blockquote>
  <p>And to put the endings into the English stemmer, the list</p>
  
  <blockquote>
    <p>ed   edly   ing   ingly</p>
  </blockquote>
  
  <p>of Step 1b should be extended to</p>
  
  <blockquote>
    <p>ed   edly   ing   ingly   est   eth</p>
  </blockquote>
  
  <p>As far as the Snowball scripts are concerned, the endings  'est' 'eth'  must be added against ending  'ing'. </p>
</blockquote>

<p>Great, so I just have to change the variables. Perhaps add a special rule to deal with ""thee""/""thou""/""you"" and ""shalt""/""shall"". The <a href=""http://www.nltk.org/api/nltk.stem.html"" rel=""nofollow"">NLTK documentation</a> show the variables as:</p>

<blockquote>
  <p><strong>class</strong> nltk.stem.snowball.EnglishStemmer(ignore_stopwords=False)</p>
  
  <p><strong>Bases:</strong> nltk.stem.snowball._StandardStemmer</p>
  
  <p><strong>The English Snowball stemmer.</strong></p>
  
  <p><strong>Variables:</strong>    </p>
  
  <blockquote>
    <p>__vowels – The English vowels.</p>
    
    <p>__double_consonants – The English double consonants.</p>
    
    <p>__li_ending – Letters that may directly appear before a word final ‘li’.</p>
    
    <p>__step0_suffixes – Suffixes to be deleted in step 0 of the algorithm.</p>
    
    <p>__step1a_suffixes – Suffixes to be deleted in step 1a of the algorithm.</p>
    
    <p><strong>__step1b_suffixes – Suffixes to be deleted in step 1b of the algorithm. (Here we go)</strong></p>
    
    <p>__step2_suffixes – Suffixes to be deleted in step 2 of the algorithm.</p>
    
    <p>__step3_suffixes – Suffixes to be deleted in step 3 of the algorithm.</p>
    
    <p>__step4_suffixes – Suffixes to be deleted in step 4 of the algorithm.</p>
    
    <p>__step5_suffixes – Suffixes to be deleted in step 5 of the algorithm.</p>
    
    <p><strong>__special_words – A dictionary containing words which have to be stemmed specially. (I can stick my ""thee""/""thou"" and ""shalt"" issues here)</strong></p>
  </blockquote>
</blockquote>

<p>Now, dumb question. How do I change the variable? Everywhere I've looked for the variables, I keep getting ""object has no attribute""...</p>
"
"35524183","Earley cannot handle epsilon-states already contained in chart","2016-02-20 13:50:55","1","346","4","1","","37929474","<p>I have implemented the <a href=""https://en.wikipedia.org/wiki/Earley_parser"" rel=""nofollow noreferrer"">Earley parser</a> using a queue to process states. The queue is seeded with the top-level rule. For each state in the queue, one of the operations (prediction, scanning, completion) is performed by adding new states to the queue. <strong>Duplicate states are not added.</strong></p>

<p>The problem I am having is best described with the following grammar:</p>

<p><a href=""https://i.sstatic.net/evvmZ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/evvmZ.png"" alt=""A -&gt; B B; B -&gt; epsilon""></a></p>

<p>When parsing <code>A</code>, the following happens:</p>

<p><a href=""https://i.sstatic.net/2Hkzf.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/2Hkzf.png"" alt=""enter image description here""></a></p>

<p>As you can tell, <strong><code>A</code> will not be fully resolved</strong>. This is because the completion with the epsilon state will only happen once as it is not added to the queue.</p>

<p><strong>How can I adapt my algorithm to support these epsilon-states?</strong></p>

<p>Edit: Note that this is not an issue when using terminals as a new chart set will be created to insert the scanned state. As the state does not exist there already, it will be processed.</p>
"
"35482943","Computing symmetric Kullback-Leibler divergence between two documents","2016-02-18 13:35:33","11","2631","1","2","","35617031","<p>I have followed the paper <a href=""http://users.softlab.ntua.gr/facilities/public/AD/Text%20Categorization/Using%20Kullback-Leibler%20Distance%20for%20Text%20Categorization.pdf"" rel=""nofollow noreferrer"">here</a> and the code <a href=""https://gist.github.com/mrorii/961963"" rel=""nofollow noreferrer"">here</a> (it is implemented using the symmetric kld and a back-off model proposed in the paper in the 1st link) for computing KLD between two text data sets. I have changed the for-loop in the end to return the probability distribution of two data sets to test if both sum to 1:</p>

<pre><code>import re, math, collections

def tokenize(_str):
    stopwords = ['and', 'for', 'if', 'the', 'then', 'be', 'is', \
                 'are', 'will', 'in', 'it', 'to', 'that']
    tokens = collections.defaultdict(lambda: 0.)
    for m in re.finditer(r""(\w+)"", _str, re.UNICODE):
        m = m.group(1).lower()
        if len(m) &lt; 2: continue
        if m in stopwords: continue
        tokens[m] += 1

    return tokens
#end of tokenize

def kldiv(_s, _t):
    if (len(_s) == 0):
        return 1e33

    if (len(_t) == 0):
        return 1e33

    ssum = 0. + sum(_s.values())
    slen = len(_s)

    tsum = 0. + sum(_t.values())
    tlen = len(_t)

    vocabdiff = set(_s.keys()).difference(set(_t.keys()))
    lenvocabdiff = len(vocabdiff)

    """""" epsilon """"""
    epsilon = min(min(_s.values())/ssum, min(_t.values())/tsum) * 0.001

    """""" gamma """"""
    gamma = 1 - lenvocabdiff * epsilon

    """""" Check if distribution probabilities sum to 1""""""
    sc = sum([v/ssum for v in _s.itervalues()])
    st = sum([v/tsum for v in _t.itervalues()])

    ps=[] 
    pt = [] 
    for t, v in _s.iteritems(): 
        pts = v / ssum 
        ptt = epsilon 
        if t in _t: 
            ptt = gamma * (_t[t] / tsum) 
        ps.append(pts) 
        pt.append(ptt)
    return ps, pt
</code></pre>

<p>I have tested with</p>

<p><code>d1 = """"""Many research publications want you to use BibTeX, which better
organizes the whole process. Suppose for concreteness your source
file is x.tex. Basically, you create a file x.bib containing the
bibliography, and run bibtex on that file.""""""
d2 = """"""In this case you must supply both a \left and a \right because the
delimiter height are made to match whatever is contained between the
two commands. But, the \left doesn't have to be an actual 'left
delimiter', that is you can use '\left)' if there were some reason
to do it.""""""</code></p>

<p><code>sum(ps)</code> = 1  but <code>sum(pt)</code> is way smaller than 1 when:</p>

<p><img src=""https://i.sstatic.net/25rcv.png"" alt=""This should be the case.""></p>

<p>Is there something that is not correct in the code or else? Thanks!</p>

<p><strong>Update:</strong></p>

<p>In order to make both pt and ps sum to 1, I had to change the code to:</p>

<pre><code>    vocab = Counter(_s)+Counter(_t)
    ps=[] 
    pt = [] 
    for t, v in vocab.iteritems(): 
        if t in _s:
            pts = gamma * (_s[t] / ssum) 
        else: 
            pts = epsilon

        if t in _t: 
            ptt = gamma * (_t[t] / tsum) 
        else:
            ptt = epsilon

        ps.append(pts) 
        pt.append(ptt)

    return ps, pt
</code></pre>
"
"35458896","Python: map NLTK Stanford POS tags to WordNet POS tags","2016-02-17 14:06:12","5","3623","0","2","","35465203","<p>I'm reading a list of sentences and tagging each word with NLTK's Stanford POS tagger.  I get outputs like so:</p>

<pre><code>wordnet_sense = []

for o in output:
    a = st.tag(o)
    wordnet_sense.append(a)
</code></pre>

<p>outputs: <code>[[(u'feel', u'VB'), (u'great', u'JJ')], [(u'good', u'JJ')]]</code></p>

<p>I want to map these words with their POS, so that they are recognised in WordNet.</p>

<p>I've attempted this:</p>

<pre><code>sense = []

for i in wordnet_sense:
    tmp = []

    for tok, pos in i:
        lower_pos = pos[0].lower()

        if lower_pos in ['a', 'n', 'v', 'r', 's']:
            res = wn.synsets(tok, lower_pos)
            if len(res) &gt; 0:
                a = res[0]
        else:
            a = ""[{0}, {1}]"".format(tok, pos)

        tmp.append(a)

    sense.append(tmp)

print sense
</code></pre>

<p>outputs: <code>[Synset('feel.v.01'), '[great, JJ]'], ['[good, JJ]']]</code></p>

<p>So <code>feel</code> is recognised as a verb, but <code>great</code> and <code>good</code> are not recognised as adjectives.  I've also checked if <code>great</code> and <code>good</code> actually belong in Wordnet because I thought they weren't being mapped if they weren't there, but they are.  Can anyone help?</p>
"
"35455089","Creating a Tree from an ANTLR Grammar","2016-02-17 11:13:30","0","1286","0","2","","35476018","<p>I have written an ANTLR Grammar file and now i need to generate an adjacency matrix telling me which rule in grammar is associated with which one.
for ex :-
start : ('show' | 'give' | 'get') 'me' ('all')? phrase 
        | 'I' 'would' 'like' ('all')? phrase
        | phrase;</p>

<p>phrase : constructPhrase (('and')? constructPhrase)*
        | constructPhrase 'and' ('its' | 'their') constructPhrase
        | constructPhrase functionPhrase
        | functionPhrase
        ;</p>

<p>Here we have I would like associated with each other..so i need to read the grammar file and generate an adjacency matrix. </p>
"
"35176283","Is their any algorithm for sentence parsing using bigram probability?","2016-02-03 11:51:11","2","238","0","1","","35255992","<p>I know that the <a href=""https://en.wikipedia.org/wiki/CYK_algorithm"" rel=""nofollow"">CKY algorithm</a> can parse sentences using production rules probabilities. </p>

<p>Is there any sentence parsing algorithm I can use if I only know words bigram probabilities?</p>
"
"35168281","what are the methods to estimate probabilities of production rules?","2016-02-03 04:01:59","1","196","0","1","","35255930","<p>I know that n-gram is useful for finding the probability of words,I want to know, How to estimate probabilities of production rules? How many methods or rules to calculate probabilities of production rules?</p>

<p>I could not find any good blog or something on this topic.Now I am studying on probabilistic context free grammar &amp; CKY parsing algorithm.</p>
"
"35032659","What should be the outcome of stemming a word with apostrophe?","2016-01-27 08:52:48","0","1406","1","1","","35041502","<p>I'm using <code>nltk.stem.porter.PorterStemmer</code> in python to get stems of words.</p>

<p>When I get the stem of ""women"" and ""women's"" I get different results respectively: ""women"" and ""women'"". For my purposes I need to have both words having the same stem. </p>

<p>In my line of thought both words refer to the same idea/concept and are pretty much the same word suffering a transformation so they should have the same stem.</p>

<p>Why am I getting two different results? Is this correct?</p>
"
"35029648","How do I add MonadLogger to my Free monad transformer stack?","2016-01-27 05:39:53","2","580","1","2","","35111764","<p>I already asked this question <a href=""https://github.com/kazu-yamamoto/logger/issues/76"" rel=""nofollow"">here</a>, but I thought I would try SO as well.</p>

<p>How do I add <code>MonadLogger</code> to this Free monad transformer stack?</p>

<pre><code>newtype Craft a = Craft { unCraft :: ReaderT CraftEnv (FreeT CraftDSL IO) a }
  deriving ( Functor, Monad, MonadIO, Applicative
           , MonadReader CraftEnv, MonadFree CraftDSL, MonadThrow)
</code></pre>

<p>I was able to add <code>MonadThrow</code> without issue; I was hoping adding <code>MonadLogger</code> would be just as easy.</p>

<p>I tried adding it and I get this error:</p>

<pre><code>No instance for (MonadLogger (FreeT CraftDSL IO))
  arising from the 'deriving' clause of a data type declaration
</code></pre>

<p>If I define an instance:</p>

<pre><code>instance MonadLogger (FreeT CraftDSL IO) where
  monadLoggerLog a b c d = Trans.lift $ monadLoggerLog a b c d
</code></pre>

<p>I get this error:</p>

<pre><code>Could not deduce (MonadLogger IO)
  arising from a use of ‘monadLoggerLog’
</code></pre>

<p><a href=""https://github.com/joehillen/craft-monadfree/blob/master/app/Main.hs#L34"" rel=""nofollow"">Here</a> is a link to basic example of what I'm working on that compiles.</p>
"
"34989721","Decrypting SENNA Chunk, SRL and Parser Output","2016-01-25 10:03:28","0","1215","0","1","","35000615","<p><a href=""http://ml.nec-labs.com/senna/"" rel=""nofollow noreferrer"">Senna</a> is a NLP tool built using neural nets and it's able to do:</p>

<ul>
<li>POS tagging</li>
<li>NER tagging</li>
<li>Chunk tagging</li>
<li>Semantic Role Label tagging and</li>
<li>Parsing </li>
</ul>

<p>After downloading the pre-compiled package from <a href=""http://ml.nec-labs.com/senna/download.html"" rel=""nofollow noreferrer"">http://ml.nec-labs.com/senna/download.html</a></p>

<p>I ran the <code>--help</code> menu and see what are the options:</p>

<pre><code>alvas@ubi:~/senna$ ./senna-linux64 --help
invalid argument: --help

SENNA Tagger (POS - CHK - NER - SRL)
(c) Ronan Collobert 2009

Usage: ./senna-linux64 [options]

 Takes sentence (one line per sentence) on stdin
 Outputs tags on stdout
 Typical usage: ./senna-linux64 [options] &lt; inputfile.txt &gt; outputfile.txt

Display options:
  -h             Display this help
  -verbose       Display model informations on stderr
  -notokentags   Do not output tokens
  -offsettags    Output start/end offset of each token
  -iobtags       Output IOB tags instead of IOBES
  -brackettags   Output 'bracket' tags instead of IOBES

Data options:
  -path &lt;path&gt;   Path to the SENNA data/ and hash/ directories [default: ./]

Input options:
  -usrtokens     Use user's tokens (space separated) instead of SENNA tokenizer

SRL options:
  -posvbs        Use POS verbs instead of SRL style verbs for SRL task
  -usrvbs &lt;file&gt; Use user's verbs (given in &lt;file&gt;) instead of SENNA verbs for SRL task

Tagging options:
  -pos           Output POS
  -chk           Output CHK
  -ner           Output NER
  -srl           Output SRL
  -psg           Output PSG
</code></pre>

<p>The command-line interface is straight forward and the outputs for POS and NER tags are also easy to interpret.</p>

<p>Given this input:</p>

<pre><code>alvas@ubi:~/senna$ cat test.in
Foo went to eat bar at the Foobar.
</code></pre>

<p>This is out standard Penn Treebank tagset:</p>

<pre><code>alvas@ubi:~/senna$ ./senna-linux64 -pos &lt; test.in
            Foo        NNP
           went        VBD
             to         TO
            eat         VB
            bar         NN
             at         IN
            the         DT
         Foobar        NNP
              .          .
</code></pre>

<p>And this is the <a href=""https://stackoverflow.com/questions/17116446/what-do-the-bilou-tags-mean-in-named-entity-recognition"">BIO tagset</a>:</p>

<pre><code>alvas@ubi:~/senna$ ./senna-linux64 -ner &lt; test.in
            Foo      S-PER
           went          O
             to          O
            eat          O
            bar          O
             at          O
            the          O
         Foobar      S-LOC
              .          O
</code></pre>

<p>And for the chunking it's also some sort of the <a href=""https://stackoverflow.com/questions/32333312/how-to-extract-chunks-from-bio-chunked-sentences-python"">BIOE tagset</a> we're used to:</p>

<pre><code>alvas@ubi:~/senna$ ./senna-linux64 -chk &lt; test.in
            Foo       S-NP
           went       B-VP
             to       I-VP
            eat       E-VP
            bar       S-NP
             at       S-PP
            the       B-NP
         Foobar       E-NP
              .          O
</code></pre>

<p><strong>But what does the <code>S-</code> tags mean?</strong> It seems like it's only attached to tokens that are single token chunks, is that true?</p>

<p>The SRL tags are a little weird, they are multiple-annotations per token:</p>

<pre><code>alvas@ubi:~/senna$ ./senna-linux64 -srl &lt; test.in
            Foo               -       S-A1        S-A0
           went            went        S-V           O
             to               -   B-AM-PNC           O
            eat             eat   I-AM-PNC         S-V
            bar               -   I-AM-PNC        S-A1
             at               -   I-AM-PNC    B-AM-LOC
            the               -   I-AM-PNC    I-AM-LOC
         Foobar               -   E-AM-PNC    E-AM-LOC
              .               -          O           O
</code></pre>

<p>The look like the ""tuple-like"" outputs we get from semantic frames but I don't understand the conventions, e.g. what is <code>-AM-</code>? what is <code>-PNC</code>?</p>

<p><strong>What does the output mean and how should we interpret it?</strong></p>

<p>And for the Parser output:</p>

<pre><code>alvas@ubi:~/senna$ ./senna-linux64 -psg &lt; test.in
            Foo (S1(S(NP*)
           went (VP*
             to (S(VP*
            eat (VP*
            bar (ADVP*)
             at (PP*
            the (NP*
         Foobar *))))))
              . *))
</code></pre>

<p>It looks like the <a href=""https://stackoverflow.com/questions/28704060/how-to-flatten-the-parse-tree-and-store-in-a-string-for-further-string-operation"">bracketed parse output we see in parsing</a> but <strong>what does the <code>*</code> mean?</strong></p>
"
"34968716","Why Stanford parser with nltk is not correctly parsing a sentence?","2016-01-23 20:52:03","6","4638","1","1","","34971823","<p>I am using Stanford parser with nltk in python and got help from <a href=""https://stackoverflow.com/questions/13883277/stanford-parser-and-nltk"">Stanford Parser and NLTK</a>  to set up Stanford nlp libraries.</p>

<pre><code>from nltk.parse.stanford import StanfordParser
from nltk.parse.stanford import StanfordDependencyParser
parser     = StanfordParser(model_path=""edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz"")
dep_parser = StanfordDependencyParser(model_path=""edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz"")
one = (""John sees Bill"")
parsed_Sentence = parser.raw_parse(one)
# GUI
for line in parsed_Sentence:
       print line
       line.draw()

parsed_Sentence = [parse.tree() for parse in dep_parser.raw_parse(one)]
print parsed_Sentence

# GUI
for line in parsed_Sentence:
        print line
        line.draw()
</code></pre>

<p>I am getting wrong parse and dependency trees as shown in the example below, it is treating 'sees' as noun instead of verb.</p>

<p><a href=""https://i.sstatic.net/I3hSv.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/I3hSv.png"" alt=""Example parse tree""></a>
<a href=""https://i.sstatic.net/C6gL8.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/C6gL8.png"" alt=""Example dependency tree""></a></p>

<p>What should I do?
It work perfectly right when I change sentence e.g.(one = 'John see Bill').
The correct ouput for this sentence can be viewed from here <a href=""https://stackoverflow.com/questions/10401076/difference-between-constituency-parser-and-dependency-parser"">correct ouput of parse tree</a></p>

<p>Example of correct output is also shown below:</p>

<p><a href=""https://i.sstatic.net/68vBR.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/68vBR.png"" alt=""correctly parsed""></a></p>

<p><a href=""https://i.sstatic.net/evbZ8.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/evbZ8.png"" alt=""correct dependency parsed tree""></a></p>
"
"34831167","WordNet - What does n and the number represent?","2016-01-16 19:28:22","7","3156","1","1","","34831393","<p>My question is related to <a href=""http://www.nltk.org/howto/wordnet.html"" rel=""noreferrer"">WordNet Interface</a>.</p>

<pre><code>   &gt;&gt;&gt; wn.synsets('cat')
       [Synset('cat.n.01'), Synset('guy.n.01'), Synset('cat.n.03'),
        Synset('kat.n.01'), Synset('cat-o'-nine-tails.n.01'), 
        Synset('caterpillar.n.02'), Synset('big_cat.n.01'), 
        Synset('computerized_tomography.n.01'), Synset('cat.v.01'), 
        Synset('vomit.v.01')]
    &gt;&gt;&gt; 
</code></pre>

<p>I could not find the answer to what is the purpose of <code>n</code> and the following <code>number</code> in <code>cat.n.01</code> or <code>caterpillar.n.02</code>.</p>
"
"34738782","match POS tag and sequence of words","2016-01-12 08:30:01","0","3059","4","5","","34742540","<p>I have the following two strings with their POS tags: </p>

<p><strong>Sent1</strong>: ""<em>something like how writer pro or phraseology works would be really cool.</em>""</p>

<blockquote>
  <p>[('something', 'NN'), ('like', 'IN'), ('how', 'WRB'), ('writer',
  'NN'), ('pro', 'NN'), ('or', 'CC'), ('phraseology', 'NN'), ('works',
  'NNS'), ('would', 'MD'), ('be', 'VB'), ('really', 'RB'), ('cool',
  'JJ'), ('.', '.')]</p>
</blockquote>

<p><strong>Sent2</strong>: ""<em>more options like the syntax editor would be nice</em>"" </p>

<blockquote>
  <p>[('more', 'JJR'), ('options', 'NNS'), ('like', 'IN'), ('the', 'DT'),
  ('syntax', 'NN'), ('editor', 'NN'), ('would', 'MD'), ('be', 'VB'),
  ('nice', 'JJ')]</p>
</blockquote>

<p>I am looking for a way to detect (return True) if there is the sequence: ""would"" + be"" + adjective (regardless of the position of the adjective, as long as its after ""would"" ""be"") in these strings. In the second string the adjective, ""nice"" immediately follows ""would be"" but that is not the case in the first string.</p>

<p>The trivial case (no other word before the adjective; <em>""would be nice"")</em> was solved in an earlier question of mine: <a href=""https://stackoverflow.com/questions/34672986/detecting-pos-tag-pattern-along-with-specified-words"">detecting POS tag pattern along with specified words</a></p>

<p>I am now looking for a more general solution where optional words may occur before the adjective.  I am new to NLTK and Python.</p>
"
"34672986","detecting POS tag pattern along with specified words","2016-01-08 08:59:02","1","2272","6","1","","34677248","<p>I need to identify certain POS tags before/after certain specified words, for example the following tagged sentence: </p>

<pre><code>[('This', 'DT'), ('feature', 'NN'), ('would', 'MD'), ('be', 'VB'), ('nice', 'JJ'), ('to', 'TO'), ('have', 'VB')]
</code></pre>

<p>can be abstracted to the form ""would be"" + Adjective</p>

<p>Similarly: </p>

<pre><code>[('I', 'PRP'), ('am', 'VBP'), ('able', 'JJ'), ('to', 'TO'), ('delete', 'VB'), ('the', 'DT'), ('group', 'NN'), ('functionality', 'NN')]
</code></pre>

<p>is of the form ""am able to"" + Verb </p>

<p>How can I go about checking for these type of a pattern in sentences. I am using NLTK. </p>
"
"34587293","Python NLTK : Extract lexical head item from Stanford dependency parsed result","2016-01-04 08:37:25","2","900","4","1","","34590315","<p>I have a sentence and i want to extract lexical head item, i could do the dependency parsing using Stanford NLP library.</p>

<p><strong>How can i extract main head head in a sentence?</strong> </p>

<p>In the case of the sentence <code>Download and share this tool</code>, the head would be <code>Download</code>.</p>

<p>I've tried the following:</p>

<pre><code> def get_head_word(text):
     standepparse=StanfordDependencyParser(path_to_jar='/home/stanford_resource/stanford-parser-full-2014-06-16/stanford-parser.jar',path_to_models_jar='/home/stanford_resource/stanford-parser-full-2014-06-16/stanford-parser-3.4-models.jar',model_path='/home/stanford_resource/stanford-parser-full-2014-06-16/stanford-parser-3.4-models/edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz')
     parsetree=standepparse.raw_parse(text)
     p_tree=list(parsetree)[0]
     print p_tree.to_dot()

 text = 'Download and share this tool'
 get_head_word(text)


output:

digraph G{
edge [dir=forward]
node [shape=plaintext]

0 [label=""0 (None)""]
0 -&gt; 1 [label=""root""]
1 [label=""1 (Download)""]
1 -&gt; 2 [label=""cc""]
1 -&gt; 3 [label=""conj""]
1 -&gt; 5 [label=""dobj""]
2 [label=""2 (and)""]
3 [label=""3 (share)""]
4 [label=""4 (this)""]
5 [label=""5 (software)""]
5 -&gt; 4 [label=""det""]
}
</code></pre>
"
"34557078","Why nltk.align.bleu_score.bleu gives an error?","2016-01-01 14:40:08","4","2993","5","1","","34559915","<p>I found zero-value when I calculate BLEU score for Chinese sentences.</p>

<p>The candidate sentence is <code>c</code> and two references are <code>r1</code> and <code>r2</code></p>

<pre><code>c=[u'\u9274\u4e8e', u'\u7f8e\u56fd', u'\u96c6', u'\u7ecf\u6d4e', u'\u4e0e', u'\u8d38\u6613', u'\u6700\u5927', u'\u56fd\u4e8e', u'\u4e00\u8eab', u'\uff0c', u'\u4e0a\u8ff0', u'\u56e0\u7d20', u'\u76f4\u63a5', u'\u5f71\u54cd', u'\u7740', u'\u4e16\u754c', u'\u8d38\u6613', u'\u3002']

r1 = [u'\u8fd9\u4e9b', u'\u76f4\u63a5', u'\u5f71\u54cd', u'\u5168\u7403', u'\u8d38\u6613', u'\u548c', u'\u7f8e\u56fd', u'\u662f', u'\u4e16\u754c', u'\u4e0a', u'\u6700\u5927', u'\u7684', u'\u5355\u4e00', u'\u7684', u'\u7ecf\u6d4e', u'\u548c', u'\u8d38\u6613\u5546', u'\u3002']

r2=[u'\u8fd9\u4e9b', u'\u76f4\u63a5', u'\u5f71\u54cd', u'\u5168\u7403', u'\u8d38\u6613', u'\uff0c', u'\u56e0\u4e3a', u'\u7f8e\u56fd', u'\u662f', u'\u4e16\u754c', u'\u4e0a', u'\u6700\u5927', u'\u7684', u'\u5355\u4e00', u'\u7684', u'\u7ecf\u6d4e\u4f53', u'\u548c', u'\u8d38\u6613\u5546', u'\u3002']
</code></pre>

<p>The code is： </p>

<pre><code>weights = [0.1, 0.8, 0.05, 0.05]
print nltk.align.bleu_score.bleu(c, [r1, r2], weights)
</code></pre>

<p>But I got a result <code>0</code>. When I step into the <code>bleu</code> process, I found that </p>

<pre><code>try:
    s = math.fsum(w * math.log(p_n) for w, p_n in zip(weights, p_ns))
except ValueError:
    # some p_ns is 0
    return 0
</code></pre>

<p>The above program goes to <code>except ValueError</code>. However, I don't know why this returns an error. If I try other sentences, I can get a non-zero value. </p>
"
"34477757","How can I get verbs, nouns, adjectives from brown corpus?","2015-12-27 05:07:12","2","3416","4","1","","34478716","<p>I have been trying to get all the nouns, verbs..etc separately from the brown corpus, so I tried to use the code</p>

<pre><code>brown.all_synsets('n')
</code></pre>

<p>but apparently this code works with wordnet only. I am using python 3.4 by the way.</p>

<hr>

<h1>EDITED</h1>

<p>@alvas answer worked. But when I used it with random it gets an error. Have a look.</p>

<pre><code>nn = {word for word, pos in brown.tagged_words() if pos.startswith('NN')}
print(nn)
</code></pre>

<p>the output is </p>

<pre><code>{'such', 'rather', 'Quite', 'Such', 'quite'}
</code></pre>

<p>but when I use </p>

<pre><code>random.choice(nn)
</code></pre>

<p>I get</p>

<pre><code>Traceback (most recent call last):
  File ""/home/aziz/Desktop/2222.py"", line 5, in &lt;module&gt;
    print(random.choice(NN))
  File ""/usr/lib/python3.4/random.py"", line 256, in choice
    return seq[i]
TypeError: 'set' object does not support indexing
</code></pre>
"
"34439208","nltk StanfordNERTagger : How to get proper nouns without capitalization","2015-12-23 15:47:02","8","6913","0","2","","34458164","<p>I am trying to use the StanfordNERTagger and nltk to extract keywords from a piece of text. </p>

<pre><code>docText=""John Donk works for POI. Brian Jones wants to meet with Xyz Corp. for measuring POI's Short Term performance Metrics.""

words = re.split(""\W+"",docText) 

stops = set(stopwords.words(""english""))

    #remove stop words from the list
words = [w for w in words if w not in stops and len(w) &gt; 2]

str = "" "".join(words)
print str
stn = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz') 
stp = StanfordPOSTagger('english-bidirectional-distsim.tagger') 
stanfordPosTagList=[word for word,pos in stp.tag(str.split()) if pos == 'NNP']

print ""Stanford POS Tagged""
print stanfordPosTagList
tagged = stn.tag(stanfordPosTagList)
print tagged
</code></pre>

<p>this gives me </p>

<pre><code>John Donk works POI Brian Jones wants meet Xyz Corp measuring POI Short Term performance Metrics
Stanford POS Tagged
[u'John', u'Donk', u'POI', u'Brian', u'Jones', u'Xyz', u'Corp', u'POI', u'Short', u'Term']
[(u'John', u'PERSON'), (u'Donk', u'PERSON'), (u'POI', u'ORGANIZATION'), (u'Brian', u'ORGANIZATION'), (u'Jones', u'ORGANIZATION'), (u'Xyz', u'ORGANIZATION'), (u'Corp', u'ORGANIZATION'), (u'POI', u'O'), (u'Short', u'O'), (u'Term', u'O')]
</code></pre>

<p>so clearly, things like <code>Short</code> and <code>Term</code> were tagged as <code>NNP</code>. The data that i have contains many such instances where <strong>non <code>NNP</code> words are capitalized</strong>. This might be due to typos or maybe they are headers. I dont have much control over that. </p>

<p>How can i parse or clean up the data so that i can detect a non <code>NNP</code> term even though it may be capitalized? <strong>I dont want terms like <code>Short</code> and <code>Term</code> to be categorized as <code>NNP</code></strong></p>

<p>Also, not sure why <code>John Donk</code> was captured as a person but <code>Brian Jones</code> was not. Could it be due to the other capitalized non <code>NNP</code>s in my data? Could that be having an effect on how the <code>StanfordNERTagger</code> treats everything else?</p>

<p><strong>Update, one possible solution</strong></p>

<p>Here is what i plan to do</p>

<ol>
<li>Take each word and convert to lower case</li>
<li>Tag the lowercase word</li>
<li>If the tag is <code>NNP</code> then we know that the original word must also be an <code>NNP</code></li>
<li>If not, then  the original word was mis-capitalized</li>
</ol>

<p>Here is what i tried to do</p>

<pre><code>str = "" "".join(words)
print str
stp = StanfordPOSTagger('english-bidirectional-distsim.tagger') 
for word in str.split():
    wl = word.lower()
    print wl
    w,pos = stp.tag(wl)
    print pos
    if pos==""NNP"":
        print ""Got NNP""
        print w
</code></pre>

<p>but this gives me error</p>

<pre><code>John Donk works POI Jones wants meet Xyz Corp measuring POI short term performance metrics
john
Traceback (most recent call last):
  File ""X:\crp.py"", line 37, in &lt;module&gt;
    w,pos = stp.tag(wl)
ValueError: too many values to unpack
</code></pre>

<p>i have tried multiple approaches but some error always shows up. <strong>How can i Tag a single word?</strong></p>

<p>I dont want to convert the whole string to lower case and then Tag. If i do that, the <code>StanfordPOSTagger</code> returns an empty string</p>
"
"34375657","Precision and Recall computation for different group sizes","2015-12-19 21:36:54","1","159","0","1","","34376149","<p>I didn't find an answer for this question anywhere, so I hope someone here could help me and also other people with the same problem.</p>

<p>Suppose that I have <strong><em>1000 Positive samples</em></strong> and <strong><em>1500 Negative samples</em></strong>.</p>

<p>Now, suppose that there are <strong><em>950 True Positives</em></strong> (positive samples correctly classified as positive) and <strong><em>100 False Positives</em></strong> (negative samples incorrectly classified as positive).</p>

<p>Should I use these raw numbers to compute the <strong><em>Precision</em></strong>, or should I consider the different group sizes?</p>

<p>In other words, should my precision be:</p>

<p><strong><em>TruePositive / (TruePositive + FalsePositive)</em></strong> = 950 / (950 + 100) = 90.476%</p>

<p><strong>OR</strong> should it be:</p>

<p><strong><em>(TruePositive / 1000) / [(TruePositive / 1000) + (FalsePositive / 1500)]</em></strong> = 0.95 / (0.95 + 0.067) = 93.44%</p>

<p>In the first calculation, I took the raw numbers without any consideration to the amount of samples in each group, while in the second calculation, I used the proportions of each measure to its corresponding group, to remove the bias caused by the groups' different size</p>
"
"34351978","NLTK RegEx Chunker not capturing defined grammar patterns with wildcards","2015-12-18 09:07:55","5","2199","1","1","","34361516","<p>I am trying to chunk a sentence using NLTK's POS tags as regular expressions. 2 rules are defined to identify phrases, based on the tags of words in the sentence.</p>
<p>Mainly, I wanted to capture the chunk of <strong>one or more verbs followed by an optional determiner and then one or more nouns at the end</strong>. This is the first rule in definition. But it is not getting captured as Phrase Chunk.</p>
<pre><code>import nltk

## Defining the POS tagger 
tagger = nltk.data.load(nltk.tag._POS_TAGGER)


## A Single sentence - input text value
textv=&quot;This has allowed the device to start, and I then see glitches which is not nice.&quot;
tagged_text = tagger.tag(textv.split())

## Defining Grammar rules for  Phrases
actphgrammar = r&quot;&quot;&quot;
     Ph: {&lt;VB*&gt;+&lt;DT&gt;?&lt;NN*&gt;+}  # verbal phrase - one or more verbs followed by optional determiner, and one or more nouns at the end
     {&lt;RB*&gt;&lt;VB*|JJ*|NN*\$&gt;} # Adverbial phrase - Adverb followed by adjective / Noun or Verb
     &quot;&quot;&quot;

### Parsing the defined grammar for  phrases
actp = nltk.RegexpParser(actphgrammar)

actphrases = actp.parse(tagged_text)
</code></pre>
<p>The input to the chunker, tagged_text is as below.</p>
<blockquote>
<p>tagged_text
Out[7]:
[('This', 'DT'),
('has', 'VBZ'),
('allowed', 'VBN'),
('the', 'DT'),
('device', 'NN'),
('to', 'TO'),
('start,', 'NNP'),
('and', 'CC'),
('I', 'PRP'),
('then', 'RB'),
('see', 'VB'),
('glitches', 'NNS'),
('which', 'WDT'),
('is', 'VBZ'),
('not', 'RB'),
('nice.', 'NNP')]</p>
</blockquote>
<p>In the final output, only the adverbial phrase ('<strong>then see</strong>'), that is matching the second rule is being captured.
I expected the verbal phrase ('<strong>allowed the device</strong>') to match with the first rule and get captured as well, but its not.</p>
<blockquote>
<p>actphrases Out[8]: Tree('S', [('This', 'DT'), ('has', 'VBZ'),
('allowed', 'VBN'), ('the', 'DT'), ('device', 'NN'), ('to', 'TO'),
('start,', 'NNP'), ('and', 'CC'), ('I', 'PRP'), <em><strong>Tree('Ph', [('then',
'RB'), ('see', 'VB')])</strong></em>, ('glitches', 'NNS'), ('which', 'WDT'), ('is',
'VBZ'), ('not', 'RB'), ('nice.', 'NNP')])</p>
</blockquote>
<p>NLTK version used is 2.0.5 (Python 2.7)
Any help or suggestion would be greatly appreciated.</p>
"
"34320024","Speeding up Levenshtein distance calculation in Ionic app","2015-12-16 19:01:14","0","258","4","2","","35614291","<ul>
<li><p>What I'm doing: I'm developing a mobile dictionary app for a number of languages</p></li>
<li><p>How I'm doing it: Using ionic framework with combination of some angular and some pure js (imported from a working online dictionary site of the same languages)</p></li>
<li><p>The problem: Our search function is an approximate search that uses a Levenstein distance calculator to rank all entries in the dictionary with respect to the query form. When the dictionary has up to 1,500 words, this isn't a problem at all on phones, but when the dictionary has around 10,000 words, there is about a 5-8 second delay before results are shown, despite it being instantaneous on a web browser using ""ionic serve"". When I run firebug, the javascript that takes the longest time to process are the distance calculations, so my working assumption is that this is where I should start, but I'm open to any suggestions at all.</p></li>
</ul>

<p>Here's the distance calculator:</p>

<pre><code>/**
 * editDistance.js
 * 
 * A simple Levenshtein distance calculator, except weighted such
 * that insertions at the beginning and deletions at the end cost less.
 *
 * AUTHOR: Pat Littell
 * LAST UPDATED: 2015-05-16
 */

var distanceCalculator = {

insertionCost : 1.0,
deletionCost : 1.0,
insertionAtBeginningCost : 0.11,
deletionAtEndCost : 0.1,
substitutionCost : 1.0,

getEditDistance : function(a, b) {
  if(a.length === 0) return b.length; 
  if(b.length === 0) return a.length; 

  var matrix = [];
 // var currentInsertionCost, currentDeletionCost, currentSubstitutionCost = 0;

  // increment along the first column of each row
  var i;
  for(i = 0; i &lt;= b.length; i++){
    matrix[i] = [i * this.insertionAtBeginningCost];
  }

  // increment each column in the first row
  var j;
  for(j = 0; j &lt;= a.length; j++){
    matrix[0][j] = j;
  }

  // Fill in the rest of the matrix
  for(i = 1; i &lt;= b.length; i++){
    for(j = 1; j &lt;= a.length; j++){
        currentInsertionCost = matrix[i][j-1] + this.insertionCost;
        currentSubstitutionCost = matrix[i-1][j-1] + (b.charAt(i-1) != a.charAt(j-1) ? this.substitutionCost : 0);
        currentDeletionCost = matrix[i-1][j] + (j==a.length ? this.deletionAtEndCost : this.deletionCost);            
        matrix[i][j] = Math.min(currentSubstitutionCost, Math.min(currentInsertionCost, currentDeletionCost));

    }
  }

  return matrix[b.length][a.length];
},


// Given a query &lt;a&gt; and a series of targets &lt;bs&gt;, return the least distance to any target
getLeastEditDistance : function(a, bs) {
    var that = this;
    return Math.min.apply(null, bs.map(function(b) {
        return that.getEditDistance(a,b);
    }));
}
}
</code></pre>
"
"34249579","How to parse more than one sentence from text file using Stanford dependency parse?","2015-12-13 09:14:11","0","991","0","2","","34249648","<p>I have a text file which have many line, i wanted to parse all sentences, but it seems like i get all sentences but parse only the first sentence, not sure where m i making mistake.</p>

<pre><code>import nltk
from nltk.parse.stanford import StanfordDependencyParser
dependency_parser = StanfordDependencyParser(  model_path=""edu\stanford\lp\models\lexparser\englishPCFG.ser.gz"")
txtfile =open('sample.txt',encoding=""latin-1"")
s=txtfile.read()
print(s)
result = dependency_parser.raw_parse(s)
for i in result:
print(list(i.triples()))
</code></pre>

<p>but it give only the first sentence parse tripples not other sentences, any help ?</p>

<pre><code>'i like this computer'
'The great Buddha, the .....'
'My Ashford experience .... great experience.'


[[(('i', 'VBZ'), 'nsubj', (""'"", 'POS')), (('i', 'VBZ'), 'nmod', ('computer', 'NN')), (('computer', 'NN'), 'case', ('like', 'IN')), (('computer', 'NN'), 'det', ('this', 'DT')), (('computer', 'NN'), 'case', (""'"", 'POS'))]]
</code></pre>
"
"34242030","Using NLTK's universalt tagset with non-English corpora","2015-12-12 16:13:15","1","2913","0","1","","34255998","<p>I'm using NLTK (3.0.4-1) in Python 3.4.3+ and I'd like to proccess some of the tagged corpora using the universal tagset (which I had to install), <a href=""http://www.nltk.org/book/ch05.html"" rel=""nofollow"">as explained in NLTK book, chapter 5</a>.</p>

<p>I can access any of these corpora with their original PoS tagset, e.g.:</p>

<pre><code>from nltk.corpus import brown, cess_esp, floresta

print(brown.tagged_sents()[0])
[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN'), (""Atlanta's"", 'NP$'), ('recent', 'JJ'), ('primary', 'NN'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'AT'), ('evidence', 'NN'), (""''"", ""''""), ('that', 'CS'), ('any', 'DTI'), ('irregularities', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('.', '.')]

print(cess_esp.tagged_sents()[0])
[('El', 'da0ms0'), ('grupo', 'ncms000'), ('estatal', 'aq0cs0'), ('Electricité_de_France', 'np00000'), ('-Fpa-', 'Fpa'), ('EDF', 'np00000'), ('-Fpt-', 'Fpt'), ('anunció', 'vmis3s0'), ('hoy', 'rg'), (',', 'Fc'), ('jueves', 'W'), (',', 'Fc'), ('la', 'da0fs0'), ('compra', 'ncfs000'), ('del', 'spcms'), ('51_por_ciento', 'Zp'), ('de', 'sps00'), ('la', 'da0fs0'), ('empresa', 'ncfs000'), ('mexicana', 'aq0fs0'), ('Electricidad_Águila_de_Altamira', 'np00000'), ('-Fpa-', 'Fpa'), ('EAA', 'np00000'), ('-Fpt-', 'Fpt'), (',', 'Fc'), ('creada', 'aq0fsp'), ('por', 'sps00'), ('el', 'da0ms0'), ('japonés', 'aq0ms0'), ('Mitsubishi_Corporation', 'np00000'), ('para', 'sps00'), ('poner_en_marcha', 'vmn0000'), ('una', 'di0fs0'), ('central', 'ncfs000'), ('de', 'sps00'), ('gas', 'ncms000'), ('de', 'sps00'), ('495', 'Z'), ('megavatios', 'ncmp000'), ('.', 'Fp')]

print(floresta.tagged_sents()[0])
[('Um', '&gt;N+art'), ('revivalismo', 'H+n'), ('refrescante', 'N&lt;+adj')]
</code></pre>

<p>So far so good, but when I use the option <code>tagset='universal'</code> to access the simplified version of the PoS tags, it works only for the Brown corpus. </p>

<pre><code>print(brown.tagged_sents(tagset='universal')[0])
[('The', 'DET'), ('Fulton', 'NOUN'), ('County', 'NOUN'), ('Grand', 'ADJ'), ('Jury', 'NOUN'), ('said', 'VERB'), ('Friday', 'NOUN'), ('an', 'DET'), ('investigation', 'NOUN'), ('of', 'ADP'), (""Atlanta's"", 'NOUN'), ('recent', 'ADJ'), ('primary', 'NOUN'), ('election', 'NOUN'), ('produced', 'VERB'), ('``', '.'), ('no', 'DET'), ('evidence', 'NOUN'), (""''"", '.'), ('that', 'ADP'), ('any', 'DET'), ('irregularities', 'NOUN'), ('took', 'VERB'), ('place', 'NOUN'), ('.', '.')]
</code></pre>

<p>When accessing the corpora in Spanish and Portuguese I get a long chain of errors and a <code>LookupError</code> exception.</p>

<pre><code>print(cess_esp.tagged_sents(tagset='universal')[0])
---------------------------------------------------------------------------
LookupError                               Traceback (most recent call last)
&lt;ipython-input-6-4e2e43e54e2d&gt; in &lt;module&gt;()
----&gt; 1 print(cess_esp.tagged_sents(tagset='universal')[0])

[...]

LookupError: 
**********************************************************************
  Resource 'taggers/universal_tagset/unknown.map' not found.
  Please use the NLTK Downloader to obtain the resource:  &gt;&gt;&gt;
  nltk.download()
  Searched in:
    - '/home/victor/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/local/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/local/lib/nltk_data'
    - ''
**********************************************************************
</code></pre>

<p>Among the mappings located in my <code>taggers/universal_tagset</code> directory, I can find the mappings for Spanish (<code>es-cast3lb.map</code>) and Portuguese (<code>pt-bosque.map</code>), but I don't have any <code>unknown.map</code> file. Any ideas how to solve it? </p>

<p>Thanks in advance :-)</p>
"
"34232190","Scikit Learn TfidfVectorizer : How to get top n terms with highest tf-idf score","2015-12-11 20:39:35","54","69775","1","3","","34236002","<p>I am working on keyword extraction problem. Consider the very general case</p>
<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer(tokenizer=tokenize, stop_words='english')

t = &quot;&quot;&quot;Two Travellers, walking in the noonday sun, sought the shade of a widespreading tree to rest. As they lay looking up among the pleasant leaves, they saw that it was a Plane Tree.

&quot;How useless is the Plane!&quot; said one of them. &quot;It bears no fruit whatever, and only serves to litter the ground with leaves.&quot;

&quot;Ungrateful creatures!&quot; said a voice from the Plane Tree. &quot;You lie here in my cooling shade, and yet you say I am useless! Thus ungratefully, O Jupiter, do men receive their blessings!&quot;

Our best blessings are often the least appreciated.&quot;&quot;&quot;

tfs = tfidf.fit_transform(t.split(&quot; &quot;))
str = 'tree cat travellers fruit jupiter'
response = tfidf.transform([str])
feature_names = tfidf.get_feature_names()

for col in response.nonzero()[1]:
    print(feature_names[col], ' - ', response[0, col])
</code></pre>
<p>and this gives me</p>
<pre><code>  (0, 28)   0.443509712811
  (0, 27)   0.517461475101
  (0, 8)    0.517461475101
  (0, 6)    0.517461475101
tree  -  0.443509712811
travellers  -  0.517461475101
jupiter  -  0.517461475101
fruit  -  0.517461475101
</code></pre>
<p>which is good. For any new document that comes in, is there a way to get the top n terms with the highest tfidf score?</p>
"
"34232047","NLTK: How to create a corpus from csv file","2015-12-11 20:29:51","1","9870","0","1","","34236148","<p>I have a csv file as</p>

<pre><code>col1         col2      col3

some text    someID    some value
some text    someID    some value
</code></pre>

<p>in each row, col1 corresponds to the text of an entire document. I would like to create a corpus from this csv. my aim is to use sklearn's TfidfVectorizer to compute document similarity and keyword extraction. So consider</p>

<pre><code>tfidf = TfidfVectorizer(tokenizer=tokenize, stop_words='english')
tfs = tfidf.fit_transform(&lt;my corpus here&gt;)
</code></pre>

<p>so then i can use</p>

<pre><code>str = 'here is some text from a new document'
response = tfidf.transform([str])
feature_names = tfidf.get_feature_names()
for col in response.nonzero()[1]:
    print feature_names[col], ' - ', response[0, col]
</code></pre>

<p>how do i create a corpus using nltk?
what form/data structure should the corpus be so that it can be supplied to the transform function?</p>
"
"34198237","How to get JJ and NN (adjective and Noun) from the triples generated StanfordDependencyParser with NLTK?","2015-12-10 09:37:09","1","1725","0","1","","34231849","<p>i got triples using the following code, but i want to get nouns and adjective from tripples, i tried alot but failed, new to NLTK and python, any help ?</p>

<pre><code>from nltk.parse.stanford import StanfordDependencyParser
dp_prsr= StanfordDependencyParser('C:\Python34\stanford-parser-full-2015-04-20\stanford-parser.jar','C:\Python34\stanford-parser-full-2015-04-20\stanford-parser-3.5.2-models.jar', model_path='edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz')
word=[]
s='bit is good university'
sentence = dp_prsr.raw_parse(s)
for line in sentence:
    print(list(line.triples()))
</code></pre>

<p>[(('university', 'NN'), 'nsubj', ('bit', 'NN')), (('university', 'NN'), 'cop', ('is', 'VBZ')), (('university', 'NN'), 'amod', ('good', 'JJ'))]</p>

<p>i want to get university and good and BIT and universityi tried the following but couldnt work.</p>

<pre><code>   for line in sentence:
    if (list(line.triples)).__contains__()  == 'JJ':
       word.append(list(line.triples()))
   print(word)
</code></pre>

<p>but i get empty array... please any help.</p>
"
"34145785","Stanford Parser: How to include the punctuations?","2015-12-08 00:11:48","2","236","0","2","","37767636","<p>I have used Stanford Parser to parse some of my <strong>already tokenized and POS tagged</strong> (by Stanford POS tagger with Gate Twitter model). But the resulting <strong>conll 2007</strong> formatted output does not include any punctuations. Why is that?</p>

<p>The command I have used:</p>

<pre><code>java -mx16g -cp ""*"" edu.stanford.nlp.parser.lexparser.LexicalizedParser -sentences newline -tokenized -tagSeparator § -tokenizerFactory edu.stanford.nlp.process.WhitespaceTokenizer -tokenizerMethod newCoreLabelTokenizerFactory -escaper edu.stanford.nlp.process.PTBEscapingProcessor -outputFormat conll2007 edu/stanford/nlp/models/lexparser/englishPCFG.caseless.ser.gz ..test.tagged &gt; ../test.conll
</code></pre>

<p>e.g. </p>

<p>Original tweet:</p>

<pre><code>bbc sp says they don't understand why the tories aren't 8% ahead in the polls given the current economics stats ; bbc bias ? surely not ?
</code></pre>

<p>POS tagged tweet, used as input for Stanford parser:</p>

<pre><code>bbc§NN sp§NN says§VBZ they§PRP don't§VBP understand§VB why§WRB the§DT tories§NNS aren't§VBZ 8%§CD ahead§RB in§IN the§DT polls§NNS given§VBN the§DT current§JJ economics§NNS stats§NNS ;§: bbc§NN bias§NN ?§. surely§RB not§RB ?§.
</code></pre>

<p>Resulting conll 2007 formatted parse:</p>

<pre><code>1   bbc _   NN  NN  _   2   compound    _   _
2   sp  _   NN  NN  _   3   nsubj   _   _
3   says    _   VBZ VBZ _   0   root    _   _
4   they    _   PRP PRP _   5   nsubj   _   _
5   don't   _   VBP VBP _   3   ccomp   _   _
6   understand  _   VB  VB  _   5   xcomp   _   _
7   why _   WRB WRB _   10  advmod  _   _
8   the _   DT  DT  _   9   det _   _
9   tories  _   NNS NNS _   10  nsubj   _   _
10  aren't  _   VBZ VBZ _   6   ccomp   _   _
11  8%  _   CD  CD  _   12  nmod:npmod  _   _
12  ahead   _   RB  RB  _   15  advmod  _   _
13  in  _   IN  IN  _   15  case    _   _
14  the _   DT  DT  _   15  det _   _
15  polls   _   NNS NNS _   10  nmod    _   _
16  given   _   VBN VBN _   15  acl _   _
17  the _   DT  DT  _   19  det _   _
18  current _   JJ  JJ  _   19  amod    _   _
19  economics   _   NNS NNS _   16  dobj    _   _
20  stats   _   NNS NNS _   19  dep _   _
22  bbc _   NN  NN  _   23  compound    _   _
23  bias    _   NN  NN  _   20  dep _   _
25  surely  _   RB  RB  _   26  advmod  _   _
26  not _   RB  RB  _   16  neg _   _
</code></pre>

<p>As you can see, Most of the punctuations are not included in the parse. But why?</p>
"
"34090734","How to use nltk regex pattern to extract a specific phrase chunk?","2015-12-04 14:37:13","11","11554","4","1","","34093919","<p>I have written the following regex to tag certain phrases pattern</p>

<pre><code>pattern = """"""
        P2: {&lt;JJ&gt;+ &lt;RB&gt;? &lt;JJ&gt;* &lt;NN&gt;+ &lt;VB&gt;* &lt;JJ&gt;*}
        P1: {&lt;JJ&gt;? &lt;NN&gt;+ &lt;CC&gt;? &lt;NN&gt;* &lt;VB&gt;? &lt;RB&gt;* &lt;JJ&gt;+}
        P3: {&lt;NP1&gt;&lt;IN&gt;&lt;NP2&gt;}
        P4: {&lt;NP2&gt;&lt;IN&gt;&lt;NP1&gt;}

    """"""
</code></pre>

<p>This pattern would correctly tag a phrase such as:</p>

<pre><code>a = 'The pizza was good but pasta was bad'
</code></pre>

<p>and give the desired output with 2 phrases: </p>

<ol>
<li>pizza was good</li>
<li>pasta was bad</li>
</ol>

<p>However, if my sentence is something like:</p>

<pre><code>a = 'The pizza was awesome and brilliant'
</code></pre>

<p>matches only the phrase:</p>

<pre><code>'pizza was awesome' 
</code></pre>

<p>instead of the desired:</p>

<pre><code>'pizza was awesome and brilliant'
</code></pre>

<p><strong>How do I incorporate the regex pattern for my second example as well?</strong></p>
"
"33988038","Detect language changes in file using Python","2015-11-29 20:57:16","3","436","1","1","","41202547","<p>I need to detect language changes in a file, and tag each word accordingly. I've come up with a hacky way, that works for 2 languages (english and greek).</p>

<p>The script is this:</p>

<pre><code>#!/usr/bin/env python
# -*- coding: utf-8 -*-
import sys

#open file
filename = sys.argv[1]
f = open(filename,'r')
content = f.read()
f.close()


#initialize new content
newSentence=''
#for every line, if the first letter of the token isn't ascii, it's nonsense, tag it.
for line in content.split('\n'):
    newSentence+='\n'
    for token in line.split():
        try:
            result = token[0].decode('ascii','ignore')
            newSentence += ' /en'+token
        except:
            newSentence += ' /gr'+token


print newSentence

f=open(filename+'_new.txt','w')
f.write(newSentence)
f.close()
</code></pre>

<p>The main idea is that if the first letter of each word isn't ascii decodeable it mustn't be english,so it's the only other option.</p>

<p>Now i realize this is awfully hacky and I'd like to know how would I go about doing it in a more pythonic way? Even in a way that works for multiple languages in a document.</p>

<p>PS. I know how to detect language in a document in general, however I was wondering if there was faster way to detecting just the changes without invoking tools such as nltk etc.</p>
"
"33815401","NLTK: How do I traverse a noun phrase to return list of strings?","2015-11-19 22:18:10","8","3057","1","2","","33816257","<p>In NLTK, how do I traverse a parsed sentence to return a list of noun phrase strings?</p>

<p>I have two goals:<br>
(1) Create the list of Noun Phrases instead of printing them using the 'traverse()' method. I presently use StringIO to record the output of the existing traverse() method. That is not an acceptable solution.<br>
(2) De-parse the Noun Phrase string so: '(NP Michael/NNP Jackson/NNP)' becomes 'Michael Jackson'. Is there a method in NLTK to de-parse? </p>

<p>The NLTK documentation recommends using traverse() to view the Noun Phrase, but how do I capture the 't' in this recursive method so I generate a list of string Noun Phrases?</p>

<pre><code>from nltk.tag import pos_tag

def traverse(t):
  try:
      t.label()
  except AttributeError:
      return
  else:
      if t.label() == 'NP': print(t)  # or do something else
      else:
          for child in t: 
              traverse(child)

def nounPhrase(tagged_sent):
    # Tag sentence for part of speech
    tagged_sent = pos_tag(sentence.split())  # List of tuples with [(Word, PartOfSpeech)]
    # Define several tag patterns
    grammar = r""""""
      NP: {&lt;DT|PP\$&gt;?&lt;JJ&gt;*&lt;NN&gt;}   # chunk determiner/possessive, adjectives and noun
      {&lt;NNP&gt;+}                # chunk sequences of proper nouns
      {&lt;NN&gt;+}                 # chunk consecutive nouns
      """"""
    cp = nltk.RegexpParser(grammar)  # Define Parser
    SentenceTree = cp.parse(tagged_sent)
    NounPhrases = traverse(SentenceTree)   # collect Noun Phrase
    return(NounPhrases)

sentence = ""Michael Jackson likes to eat at McDonalds""
tagged_sent = pos_tag(sentence.split())  
NP = nounPhrase(tagged_sent)  
print(NP)  
</code></pre>

<p>This presently prints:<br>
(NP Michael/NNP Jackson/NNP)<br>
(NP McDonalds/NNP)<br>
and stores 'None' to NP  </p>
"
"33784448","UMBC Semantic Similarity Implementation","2015-11-18 15:47:03","3","620","0","1","","33804870","<p>I am using the semantic similarity web API, provided by UMBC. In my java program, I send an HTTP request 
<a href=""http://swoogle.umbc.edu/SimService/GetSimilarityoperation=api&amp;phrase1=XXXX&amp;phrase2=XXXX"" rel=""nofollow"">http://swoogle.umbc.edu/SimService/GetSimilarityoperation=api&amp;phrase1=XXXX&amp;phrase2=XXXX</a> 
and I parse the output to get the result. </p>

<p>The problem I am having is that I am processing a large scale data. It takes so long and I have to do it many times. I was wondering whether there is a faster way to query a Web API in java? or, is there an implementable version of this tool? and how easy is it for someone not an expert in NLP to implement it?</p>
"
"33705923","Steps to generate parse tree from CYK Algorithm (Natural Language Processing)","2015-11-14 06:48:52","3","5997","0","1","","33745331","<p>I am currently working on a project involving NLP. I have implemented a CKY identifier as given in Jurafsky and Martin (algorithm on page 450). The table so produced actually stores the nonterminals in the table (instead of the usual boolean values). However, the only issue I am getting is to retrieve the parse tree.</p>

<p>Here is an illustration of what my CKY identifier does: </p>

<p>This is my grammar</p>

<pre><code>          S -&gt; NP VP 
          S -&gt; VP 
          NP -&gt; MODAL PRON | DET NP | NOUN VF | NOUN | DET NOUN | DET FILENAME
          MODAL -&gt; 'MD'
          PRON -&gt; 'PPSS' | 'PPO'
          VP -&gt; VERB NP
          VP -&gt; VERB VP
          VP -&gt; ADVERB VP
          VP -&gt; VF
          VERB -&gt; 'VB' | 'VBN'
          NOUN -&gt; 'NN' | 'NP'
          VF -&gt; VERB FILENAME
          FILENAME -&gt; 'NN' | 'NP'
          ADVERB -&gt; 'RB'
          DET -&gt; 'AT'
</code></pre>

<p>And this is the algorithm:</p>

<pre><code>for j  from i to LENGTH(words) do
    table[j-1,j] = A where A -&gt; POS(word[j])
    for i from j-2 downto 0
        for k from i+1 to j-1
            table[i,j] = Union(table[i,j], A such that A-&gt;BC)
            where B is in table[i,k] and C is in table[k,j]
</code></pre>

<p>And this is what my parsing table looks like after being filled:</p>

<p><a href=""https://i.sstatic.net/3zIwQ.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/3zIwQ.jpg"" alt=""CKY Table filled as per algorithm mentioned""></a></p>

<p>Now that I know that since S resides in [0,5], the string has been parsed, and that for k = 1 (as per the algorithm given in Martin and Jurafsky), we have S -> table[0][2] table[2][5]
    i.e. S -> NP VP</p>

<p>The only issue I am getting is that I have been able to retrieve the rules used, but then they are in a jumbled format, i.e. not on the basis of their appearance in parse tree. Can someone suggest an algorithm to retrieve the correct parse tree?</p>

<p>Thankyou.</p>
"
"33705555","How can I remove POS tags before slashes in nltk?","2015-11-14 05:49:43","4","2455","12","4","","33766792","<p>This is part of my project where I need to represent the output after phrase detection like this - (a,x,b) where a, x, b are phrases. I constructed the code and got the output like this:</p>

<pre><code>(CLAUSE (NP Jack/NNP) (VP loved/VBD) (NP Peter/NNP))
(CLAUSE (NP Jack/NNP) (VP stayed/VBD) (NP in/IN London/NNP))
(CLAUSE (NP Tom/NNP) (VP is/VBZ) (NP in/IN Kolkata/NNP))
</code></pre>

<p>I want to make it just like the previous representation which means I have to remove 'CLAUSE', 'NP', 'VP', 'VBD', 'NNP' etc tags.</p>

<p>How to do that? </p>

<h2>What I tried</h2>

<p>First wrote this in a text file, tokenize and used <code>list.remove('word')</code>. But that is not at all helpful.
I am clarifying a bit more.</p>

<h2>My Input</h2>

<p><code>(CLAUSE (NP Jack/NNP) (VP loved/VBD) (NP Peter/NNP))
(CLAUSE (NP Jack/NNP) (VP stayed/VBD) (NP in/IN London/NNP))</code></p>

<h2>Output will be</h2>

<p>[Jack,loved,Peter], [Jack,stayed,in London] 
The output is just according to the braces and without the tags.</p>
"
"33676526","POS-Tagger is incredibly slow","2015-11-12 16:32:19","8","4294","3","4","","33677051","<p>I am using <code>nltk</code> to generate n-grams from sentences by first removing given stop words. However, <code>nltk.pos_tag()</code> is extremely slow taking up to 0.6 sec on my CPU (Intel i7).</p>

<p>The output:</p>

<pre><code>['The first time I went, and was completely taken by the live jazz band and atmosphere, I ordered the Lobster Cobb Salad.']
0.620481014252
[""It's simply the best meal in NYC.""]
0.640982151031
['You cannot go wrong at the Red Eye Grill.']
0.644664049149
</code></pre>

<p>The code:</p>

<pre><code>for sentence in source:

    nltk_ngrams = None

    if stop_words is not None:   
        start = time.time()
        sentence_pos = nltk.pos_tag(word_tokenize(sentence))
        print time.time() - start

        filtered_words = [word for (word, pos) in sentence_pos if pos not in stop_words]
    else:
        filtered_words = ngrams(sentence.split(), n)
</code></pre>

<p>Is this really that slow or am I doing something wrong here?</p>
"
"33611766","NLTK WordNet Lemmatizer - How to remove the unknown words?","2015-11-09 14:50:50","2","2570","0","1","","33611944","<p>I'm trying to use the NLTK WordNet Lemmatizer on tweets. </p>

<p>I would like to remove all words that are not found in WordNet (twitter handles and such), but there is no feedback from WordNetLemmatizer.lemmatize(). It simply returns the word unchanged if it can't find it.</p>

<p><strong>Is there a way to check if a word is found in WordNet or not?</strong></p>

<p>Alternatively is there a better way to remove anything but ""proper english words"" from a string?</p>
"
"33594721","Why NLTK lemmatization has wrong output even if verb.exc has added right value?","2015-11-08 13:55:51","7","2293","0","1","","33600258","<p>When I open verb.exc, I can see</p>

<pre><code>saw see
</code></pre>

<p>While I use lemmatization in code</p>

<pre><code>&gt;&gt;&gt;print lmtzr.lemmatize('saw', 'v')
saw
</code></pre>

<p>How can this happen? Do I misunderstand in revising wordNet?</p>
"
"33326810","scikit weighted f1 score calculation and usage","2015-10-25 06:17:33","6","11407","0","1","","33330487","<p>I have a question regarding <code>weighted</code> average in sklearn.metrics.f1_score</p>

<pre><code>sklearn.metrics.f1_score(y_true, y_pred, labels=None, pos_label=1, average='weighted', sample_weight=None)

Calculate metrics for each label, and find their average, weighted by support (the number of true instances for each label). This alters ‘macro’ to account for label imbalance; it can result in an F-score that is not between precision and recall.
</code></pre>

<p>First, if there is any reference that justifies the usage of weighted-F1, I am just curios in which cases I should use weighted-F1.</p>

<p>Second, I heard that weighted-F1 is deprecated, is it true?</p>

<p>Third, how actually weighted-F1 is being calculated, for example</p>

<pre><code>{
    ""0"": {
        ""TP"": 2,
        ""FP"": 1,
        ""FN"": 0,
        ""F1"": 0.8
    },
    ""1"": {
        ""TP"": 0,
        ""FP"": 2,
        ""FN"": 2,
        ""F1"": -1
    },
    ""2"": {
        ""TP"": 1,
        ""FP"": 1,
        ""FN"": 2,
        ""F1"": 0.4
    }
}
</code></pre>

<p>How to calculate weighted-F1 of the above example. I though it should be something like (0.8*2/3 + 0.4*1/3)/3, however I was wrong.</p>
"
"33326704","scikit-learn calculate F1 in multilabel classification","2015-10-25 05:59:10","4","4853","0","1","","33330620","<p>I am trying to calculate macro-F1 with scikit in <a href=""https://en.wikipedia.org/wiki/Multi-label_classification"" rel=""nofollow"">multi-label classification</a></p>

<pre><code>from sklearn.metrics import f1_score

y_true = [[1,2,3]]
y_pred = [[1,2,3]]

print f1_score(y_true, y_pred, average='macro')
</code></pre>

<p>However it fails with error message</p>

<pre><code>ValueError: multiclass-multioutput is not supported
</code></pre>

<p>How I can calculate macro-F1 with multi-label classification?</p>
"
"33242858","Applied NLP: how to score a document against a lexicon of multi-word terms?","2015-10-20 17:17:56","1","805","1","1","","33271991","<p>This is probably a fairly basic NLP question but I have the following task at hand: I have a collection of text documents that I need to score against an (English) lexicon of terms that could be 1-, 2-, 3- etc <code>N</code>-word long. <code>N</code> is bounded by some ""reasonable"" number but the distribution of various terms in the dictionary for various values of <code>n = 1, ..., N</code> might be fairly uniform. This lexicon can, for example, contain a list of devices of certain type and I want to see if a given document is likely about any of these devices. So I would want to score a document high(er) if it has one or more occurrences of any of the lexicon entries. </p>

<p>What is a standard NLP technique to do the scoring while accounting for various forms of the words that may appear in the lexicon? What sort of preprocessing would be required for both the input documents and the lexicon to be able to perform the scoring? What sort of open-source tools exist for both the preprocessing and the scoring?</p>
"
"33236269","NLTK lemmatization wrong result","2015-10-20 12:08:27","1","816","2","1","","33236991","<p>I've use NLTK and got wrong result like this:</p>

<pre><code>&gt;&gt;&gt; print lmtzr.lemmatize('coding', 'v')
cod
</code></pre>

<p>I consider the answer is ""code"" instead of a fish.
Is there anyway to solve this or other python Lib can do better job?</p>
"
"33232303","Morphological realisation for the Spanish language","2015-10-20 09:00:14","0","188","2","1","","33291533","<p>Does anyone know a Morphological realisation Tool (preferably a Java one).
I am working on a project and I need to realise the correct verb ""to be"" providing if it is for male/female - singular/plural - first person/third person and regarding such inputs generate the correct verb ""to be"".
SimpleNLG is the ideal software that contains a Morphological realisation but it is only for English and French.For example : if the features are male first person singular the result will be ""I"", if the features are plural third person males the result will be ""they"".</p>
"
"33176328","Idf score for an unknown word?","2015-10-16 17:33:57","1","1051","0","1","","33720145","<p>My task is to extract keywords from a text. What i did is following:</p>

<p>I'm using the tf-idf ""algorithm"". For the idf part i'm crawling wikipedia articles and extract the noun phrases (opennlp) and store them in a database.</p>

<p>So when i analyze a text i just have to calculate the tf part and get the idf part from the database.</p>

<p>The results so far are very appealing. My only problem is -> since the texts i have to analyze differ from the wikipedia corpus, some words have a high tf but no idf value (it was not found in the wiki corpus). But sometimes these words are still very important (an example for this could be a new company which is not listed on wikipedia yet).</p>

<p>What should i take as an idf value if it wasn't found in the db(corpus)? (average idf is probably not so a good idea)</p>
"
"33115343","Python scikit learn's TfidfVectorizer - max of 1.0?","2015-10-14 01:54:10","3","918","0","1","","33115514","<p>I couldn't find the answer to this online, but are the results of tfidfVectorizer.fit_transform an array with <strong>max value of 1.0</strong>? </p>

<p>Because, with 
<code>idf(term_i)= 
log (#number of docs/ number of docs containing term_i )</code>, shouldn't idf, and subsequently tfidf, be > 1.0 in many cases?</p>

<p>i.e. Documents containing the word 'absinthe'. Say our term freq (tf) is 1, but idf is (1000 total documents/ 1 document containing 'absinthe') = 1000, 1*1000 = 1000, no?</p>

<p>But in my cases of using scikit-learn's TfidfVectorizer, the max value I get seems to be 1. Is it normalized?</p>
"
"33072971","Python NLTK: parse sentence using conjoint structure, getting into infinite recursion","2015-10-12 03:47:25","1","546","0","1","","33086422","<p>I am asked to create <strong><code>two</code></strong> different parse tree for the following sentence:</p>

<pre><code>foo while bar and baz
</code></pre>

<p>Based on these two constructions:</p>

<pre><code>S-&gt; S while S
S-&gt; S and S
</code></pre>

<p>The two different trees I have are the following:</p>

<p>Tree A)</p>

<pre><code>     S
   / | \
  P  U  S
  |    /|\
  W   P U P  
      |   |
      W   W
</code></pre>

<p>Here is the code for A:</p>

<pre><code>import nltk

groucho_grammar = nltk.CFG.fromstring (""""""
S -&gt; P U S | P U P
P -&gt; W
U -&gt; 'while' | 'and'
W -&gt; 'foo'|'bar'|'baz'
"""""")

print(groucho_grammar)

sentence = ""foo while bar and baz""

rd_parser = nltk.RecursiveDescentParser(groucho_grammar)
for tree in rd_parser.parse(sentence.split()):
    print(tree)
</code></pre>

<p>And the result for A:</p>

<pre><code>(S (P (W foo)) (U while) (S (P (W bar)) (U and) (P (W baz))))
</code></pre>

<p>Tree B)</p>

<pre><code>       S
     / | \
    S  U  P
  / | \    \
 P  U  P    W
 |     |
 W     W
</code></pre>

<p>Now for part B, I just modified the grammar to the following:</p>

<pre><code>groucho_grammar = nltk.CFG.fromstring (""""""
S -&gt; S U P | P U P
P -&gt; W
U -&gt; 'while' | 'and'
W -&gt; 'foo'|'bar'|'baz'
"""""")
</code></pre>

<p>But I am getting infinite recursion error:</p>

<pre><code>    if isinstance(index, (int, slice)):
RuntimeError: maximum recursion depth exceeded in __instancecheck__
</code></pre>

<p>Any help would be appreciated.</p>

<p>Thanks.</p>
"
"33015326","Maltparser giving error in NLTK","2015-10-08 12:07:28","1","614","2","1","","33019535","<p>My COde is</p>

<pre><code>from nltk.parse import malt
mp = malt.MaltParser(working_dir=""/other/apps/maltparser-1.8.1"",mco=""engmalt.poly-1.7.mco"",additional_java_args=['-Xmx1024m'])
print mp.raw_parse(""Hello World"")
</code></pre>

<p>And the error is</p>

<pre><code>    Traceback (most recent call last):
  File ""malt.py"", line 13, in &lt;module&gt;
    print mp.raw_parse(""Hello World"")
  File ""/usr/local/lib/python2.7/dist-packages/nltk/parse/malt.py"", line 139, in raw_parse
    return self.parse(words, verbose)
  File ""/usr/local/lib/python2.7/dist-packages/nltk/parse/malt.py"", line 126, in parse
    return self.parse_sents([sentence], verbose)[0]
  File ""/usr/local/lib/python2.7/dist-packages/nltk/parse/malt.py"", line 114, in parse_sents
    return self.tagged_parse_sents(tagged_sentences, verbose)
  File ""/usr/local/lib/python2.7/dist-packages/nltk/parse/malt.py"", line 194, in tagged_parse_sents
    ""code %d"" % (' '.join(cmd), ret))
Exception: MaltParser parsing (java -Xmx1024m -jar /usr/local/bin/malt.jar -w /other/apps/maltparser-1.8.1 -c engmalt.poly-1.7.mco -i /other/apps/maltparser-1.8.1/malt_input.conlljOba1P -o /other/apps/maltparser-1.8.1/malt_output.conllPLcoTu -m parse) failed with exit code 1
</code></pre>
"
"32981188","Penn tree Bank tagset for NLTK","2015-10-06 23:03:31","0","1802","0","1","","32981825","<p>I am using NLTK as a part of my project, and have implemented Viterbi algorithm for the purpose of tagging (although I am aware of the fact that a tagger is very much available).
I have used the following snipped in my code</p>

<pre><code>tagdict = load('help/tagsets/brown_tagset.pickle')
taglist = tagdict.keys()
tag_sequence_corpus = brown.tagged_sents(tagset='brown')
</code></pre>

<p>The first two lines have been used to extract the keys out of the brown tag-set, where the keys are the list of tags available in the Brown tag-set.
The argument tag-set='brown' in third line is used to tag the brown corpus according to the tag-set offered by the Brown corpus.</p>

<p>Is there any means by which I can set the tag-set argument to the Penn bank tag-set? The motivation for pursuing so is the fact that the Penn Bank tree has some 36-45 tags, which makes it feasible to implement the Viterbi algorithm (complexity of the algorithms being O(n*|S|^3) ) where n is the length of the sentence ans |S| refers to the magnitude of the tag-set, while the brown corpus has some ~226 tags in it (which increases the computation time). And the universal tag-set is prone to word sense ambiguity.</p>

<p>If PTB tagger is not available, may anyone suggest an alternative to it (apart from Brown/universal)?</p>

<p>Thankyou.</p>
"
"32973119","Python - NLP - convert iter(iter(tree)) to list(tree)","2015-10-06 14:54:00","5","572","2","2","","32973431","<p>I have a parser function which returns <code>iter(iter(tree))</code>. </p>

<pre><code>parsedSentence = parser.raw_parse_sents([sentence],False)  
</code></pre>

<p>How can I convert the parsedSentence type to list(tree)  and access 1st element of that list. </p>

<p><em>I've already tried <code>list(parser.raw_parse_sents([sentence],False))</code> but it's not converting the result to list.</em> </p>

<p><strong>Edited:</strong>  </p>

<pre><code>s1 = parsedSentence[0]
t1 = Tree.convert(s1)
positions = t1.treepositions()
</code></pre>

<p>Here it throws an error:</p>

<pre><code>'listiterator' object has no attribute 'treepositions'
</code></pre>

<p>Thank You.</p>
"
"32957895","WordNetLemmatizer not returning the right lemma unless POS is explicit - Python NLTK","2015-10-05 21:06:44","11","17161","2","1","","32959872","<p>I'm lemmatizing the Ted Dataset Transcript. There's something strange I notice:
Not all words are being lemmatized. To say,</p>

<pre><code>selected -&gt; select
</code></pre>

<p>Which is right. </p>

<p>However, <code>involved !-&gt; involve</code> and <code>horsing !-&gt; horse</code> unless I explicitly input the 'v' (Verb) attribute.</p>

<p>On the python terminal, I get the right output but not in my <a href=""http://pastebin.com/midgUJ02"" rel=""noreferrer"">code</a>:</p>

<pre><code>&gt;&gt;&gt; from nltk.stem import WordNetLemmatizer
&gt;&gt;&gt; from nltk.corpus import wordnet
&gt;&gt;&gt; lem = WordNetLemmatizer()
&gt;&gt;&gt; lem.lemmatize('involved','v')
u'involve'
&gt;&gt;&gt; lem.lemmatize('horsing','v')
u'horse'
</code></pre>

<p>The relevant section of the code is this:</p>

<pre><code>for l in LDA_Row[0].split('+'):
    w=str(l.split('*')[1])
    word=lmtzr.lemmatize(w)
    wordv=lmtzr.lemmatize(w,'v')
    print wordv, word
    # if word is not wordv:
    #   print word, wordv
</code></pre>

<p>The whole code is <a href=""http://pastebin.com/midgUJ02"" rel=""noreferrer"">here</a>. </p>

<p>What is the problem?</p>
"
"32939186","Logical Semantics, Information Extraction and Summarization","2015-10-04 22:50:00","1","68","0","1","","32956680","<p>I want to know a general idea about these questions, in the field of data analysis and NLP. 
What are the steps included? If I want to retrieve the meaningful information from any domain-specific text and understand the general idea of any text.</p>

<p>Another question, the larger the size of the analyzed text the better is the result? </p>

<p>Excuse my ignorance. I want to understand more and it would help me a lot if you suggested some tutorials or readings. </p>
"
"32751688","NLP techniques for document classification?","2015-09-24 00:49:38","1","2113","2","1","","32758921","<p>I was wondering if there are any NLP techniques for document classification.  I was wondering if statistics of n-grams from part-of-speech tagging could be useful?  I can't seem to find too much in the literature on the topic..</p>

<p>Has anyone found any nlp technique that enhanced their document classification efforts?  If you know of any surveys on this topic that would be awesome.</p>

<p>Note.  I saw <a href=""https://stackoverflow.com/questions/5499448/part-of-speech-pos-tag-feature-selection-for-text-classification"">this question</a>, but my corpus is way too large for the only solution there to be practical.</p>
"
"32748859","Accurately splitting sentences","2015-09-23 20:27:09","5","2730","3","5","","32749119","<p>My program takes a text file and splits each sentence into a list using <code>split('.')</code> meaning that it will split when it registers a full stop however it can be inaccurate.</p>

<h1>For Example</h1>

<pre><code>str='i love carpets. In fact i own 2.4 km of the stuff.'
</code></pre>

<h2>Output</h2>

<p><code>listOfSentences = ['i love carpets', 'in fact i own 2', '4 km of the stuff']</code></p>

<h2>Desired Output</h2>

<pre><code> listOfSentences = ['i love carpets', 'in fact i own 2.4 km of the stuff']
</code></pre>

<p>My question is: <strong>How do I split the end of sentences and not at every full stop.</strong></p>
"
"32740104","Template language for natural language text mutations","2015-09-23 12:50:22","1","79","0","2","","42246116","<p>Stuck upon a rather trivial task that seems to lead to a wider problem.</p>

<p>Need to be able to generate light variations of a same short text. Some word forms depend on the speaker's gender, some can be replaced with synonyms.</p>

<p>Pseudo code:</p>

<pre><code>I {random:decided|made up my mind} to {random:try|test|give a try to}
this {new|fresh} {cool|awesome} {service|web service|online tool}.
</code></pre>

<p>I'm looking for an ""industry standard"" templating language to describe such texts and possible variations. Thinking further, I might want <em>global</em> variables (like the gender one), <em>cross-links</em> for dependencies picked earlier in the sentence.</p>

<p>This looks close to regular expressions syntax. Ideally it would be more readable/writable by non-programmers.</p>

<p>Perhaps the problem is well-known, with a solid state solution like some programming language specifically for the task?</p>
"
"32414333","Handling (, ,) and (. .) and other punctuation when processing natural language parse trees with Lisp","2015-09-05 14:43:14","2","666","0","2","","32414670","<p>My question has to do with post-processing of part-of-speech tagged and parsed natural language sentences.  Specifically, I am writing a component of a Lisp post-processor that takes as input a sentence parse tree (such as, for example, one produced by the Stanford Parser), extracts from that parse tree the phrase structure rules invoked to generate the parse, and then produces a table of rules and rule counts.  An example of input and output would be the following:</p>

<p>(1)  Sentence:</p>

<pre><code>John said that he knows who Mary likes
</code></pre>

<p>(2)  Parser output:</p>

<pre><code>(ROOT
  (S
    (NP (NNP John))
    (VP (VBD said)
      (SBAR (IN that)
        (S
          (NP (PRP he))
          (VP (VBZ knows)
            (SBAR
              (WHNP (WP who))
              (S
                (NP (NNP Mary))
                (VP (VBZ likes))))))))))
</code></pre>

<p>(3)  My Lisp program post-processor output for this parse tree:</p>

<pre><code>(S --&gt; NP VP)             3
(NP --&gt; NNP)              2
(VP --&gt; VBZ)              1
(WHNP --&gt; WP)             1
(SBAR --&gt; WHNP S)         1
(VP --&gt; VBZ SBAR)         1
(NP --&gt; PRP)              1
(SBAR --&gt; IN S)           1
(VP --&gt; VBD SBAR)         1
(ROOT --&gt; S)              1
</code></pre>

<p>Note the lack of punctuation in sentence (1).  That's intentional.  I am having trouble parsing the punctuation in Lisp -- precisely because some punctuation (commas, for example) are reserved for special purposes.  But parsing sentences without punctuation changes the distribution of the parse rules as well as the symbols contained in those rules, as illustrated by the following:</p>

<p>(4)  Input sentence:</p>

<pre><code>I said no and then I did it anyway
</code></pre>

<p>(5)  Parser output:</p>

<pre><code>(ROOT
  (S
    (NP (PRP I))
    (VP (VBD said)
      (ADVP (RB no)
        (CC and)
        (RB then))
      (SBAR
        (S
          (NP (PRP I))
          (VP (VBD did)
            (NP (PRP it))
            (ADVP (RB anyway))))))))
</code></pre>

<p>(6)  Input sentence (with punctuation):</p>

<pre><code>I said no, and then I did it anyway.
</code></pre>

<p>(7)  Parser output:</p>

<pre><code> (ROOT
   (S
     (S
       (NP (PRP I))
       (VP (VBD said)
         (INTJ (UH no))))
     (, ,)
     (CC and)
     (S
       (ADVP (RB then))
       (NP (PRP I))
       (VP (VBD did)
         (NP (PRP it))
         (ADVP (RB anyway))))
     (. .)))
</code></pre>

<p>Note how including punctuation completely rearranges the parse tree and also involves different POS tags (and thus, implies that different grammar rules were invoked to produce it)  So including punctuation is important, at least for my application.</p>

<p>What I need is to discover a way to include punctuation in rules, so that I can produce rules like the following, which would appear, for example, in the table like (3), as follows:</p>

<p>(8)  Desired rule:</p>

<pre><code>S --&gt; S , CC S .
</code></pre>

<p>Rules like (8) are in fact desired for the specific application I am writing.</p>

<p>But I am finding that doing this in Lisp is difficult:  In (7), for example, we observe the appearance of (, ,) and (. .) , both of which are problematic to handle in Lisp.</p>

<p>I have included my relevant Lisp code below.  Please note that I'm a neophyte Lisp hacker and so my code isn't particularly pretty or efficient.  If someone could suggest how I might modify my below code such that I can parse (7) to produce a table like (3) that includes a rule like (8), I would be most appreciative.</p>

<p>Here is my Lisp code relevant to this task:</p>

<pre><code>(defun WRITE-RULES-AND-COUNTS-SORTED (sent)
  (multiple-value-bind (rules-list counts-list)
      (COUNT-RULES-OCCURRENCES sent)
    (setf comblist (sort (pairlis rules-list counts-list) #'&gt; :key #'cdr))
    (format t ""~%"")
    (do ((i 0 (incf i)))
        ((= i (length comblist)) NIL)
      (format t ""~A~26T~A~%"" (car (nth i comblist)) (cdr (nth i comblist))))
    (format t ""~%"")))


 (defun COUNT-RULES-OCCURRENCES (sent)
   (let* ((original-rules-list (EXTRACT-GRAMMAR sent))
          (de-duplicated-list (remove-duplicates original-rules-list :test #'equalp))
          (count-list nil))
     (dolist (i de-duplicated-list)
       (push (reduce #'+ (mapcar #'(lambda (x) (if (equalp x i) 1 0)) original-rules-list) ) count-list))
     (setf count-list (nreverse count-list))
    (values de-duplicated-list count-list)))


 (defun EXTRACT-GRAMMAR (sent &amp;optional (rules-stack nil))
   (cond ((null sent) 
          NIL)
         ((and (= (length sent) 1)
               (listp (first sent))
               (= (length (first sent)) 2)
               (symbolp (first (first sent)))
               (symbolp (second (first sent))))
          NIL)
         ((and (symbolp (first sent)) 
               (symbolp (second sent)) 
               (= 2 (length sent)))
          NIL)
         ((symbolp (first sent))
          (push (EXTRACT-GRAMMAR-RULE sent) rules-stack)
          (append rules-stack (EXTRACT-GRAMMAR (rest sent)   )))
         ((listp (first sent))
          (cond ((not (and (listp (first sent)) 
                           (= (length (first sent)) 2) 
                           (symbolp (first (first sent))) 
                           (symbolp (second (first sent)))))
                 (push (EXTRACT-GRAMMAR-RULE (first sent)) rules-stack)
                 (append rules-stack (EXTRACT-GRAMMAR (rest (first sent))) (EXTRACT-GRAMMAR (rest sent) )))
               (t (append rules-stack (EXTRACT-GRAMMAR (rest sent)  )))))))


(defun EXTRACT-GRAMMAR-RULE (sentence-or-phrase)
  (append (list (first sentence-or-phrase))
          '(--&gt;)
          (mapcar #'first (rest sentence-or-phrase))))
</code></pre>

<p>The code is invoked as follows (using (1) as input, producing (3) as output):</p>

<pre><code>(WRITE-RULES-AND-COUNTS-SORTED  '(ROOT
  (S
    (NP (NNP John))
    (VP (VBD said)
      (SBAR (IN that)
        (S
          (NP (PRP he))
          (VP (VBZ knows)
            (SBAR
              (WHNP (WP who))
              (S
                (NP (NNP Mary))
                (VP (VBZ likes)))))))))))
</code></pre>
"
"32399299","How do I extract patterns from lists of POS tagged words? NLTK","2015-09-04 13:26:54","4","4235","1","2","","32399618","<p>I have a text file that contains multiple lists; each list contains tuples of word/pos-tag pairs, like so:</p>

<pre><code>    [('reviewtext', 'IN'), ('this', 'DT'), ('movie', 'NN'), ('was', 'VBD'), ('great', 'JJ'), ('and', 'CC'), ('fun', 'NN'), ('i', 'PRP'), ('really', 'RB'), ('enjoyed', 'VBD'), ('this', 'DT'), ('awesome', 'NN'), ('movie', 'NN')]
    [('reviewtext', 'IN'), ('it', 'PRP'), ('was', 'VBD'), ('fun', 'VBN'), ('but', 'CC'), ('long', 'RB')]
    [('reviewtext', 'IN'), ('i', 'PRP'), ('loved', 'VBD'), ('the', 'DT'), ('new', 'JJ'), ('movie', 'NN'), ('my', 'PRP$'), ('brother', 'NN'), ('got', 'VBD'), ('sad', 'JJ'), ('and', 'CC'), ('unhappy', 'JJ'), ('at', 'IN'), ('the', 'DT'), ('end', 'NN')]
</code></pre>

<p>I need to extract all adjective-conjunction-adjective pairs, or all JJ-CC-JJ pairs (the words only, and not the pos tags). For this example, the final output should be a list containing all the patterns:</p>

<pre><code>    ['great and fun', 'sad and unhappy']
</code></pre>

<p>I used the following code to tag the text:</p>

<pre><code>    with open(""C:\\Users\\M\\Desktop\\sample dataset.txt"") as fileobject:
        for line in fileobject:
            line = line.lower() #lowercase
            line = re.sub(r'[^\w\s]','',line) #remove punctuation
            line = nltk.word_tokenize(line) #tokenize
            line = nltk.pos_tag(line) #POS tag

            fo = open(""C:\\Users\\M\\Desktop\\movies1_complete.txt"", ""a"")
            fo.write(str(line))
            fo.write(""\n"")
            fo.close()
</code></pre>

<p>But how do I extract the words in the above mentioned patters? I checked <a href=""https://stackoverflow.com/questions/29073796/retrieving-tags-from-result-of-pos-tagging"">here</a> and <a href=""https://stackoverflow.com/questions/27045684/extracting-sentences-from-pos-tagged-corpus-with-certain-word-tag-combos"">here</a>, but they do not explain how to extract specific pos patterns. Thanks in advance.</p>
"
"32390030","What are the methods except Bag Of Words (TF-IDF) for converting textual features into numerical features?","2015-09-04 04:22:16","0","828","0","3","","32396587","<p>I have been working on Natural Language Processing these days. My aim is to classify different words in a multi-lingual sentence written in Roman Script based on some criteria. Thus, I need a classifier for it. Unquestionably, there are many. But since my features aren't numerical but textual, and most of the classifiers like Support Vector Machine (SVM) input numerical features, I looked for some methodology to convert my textual features into numerical one. Though the concept of Bag Of Words with the use of Term Frequency and Inverse Document Frequency (TF-IDF) is a generic approach for this purpose, one of my textual feature, namely local context, is of fixed length and i want to know if it is possible to convert it into numerical feature without using TF-IDF. Local context feature refers to considering previous two and next two words (which comprise the context of a particular word). Therefore, I am looking for any other methodology which could prove to be better in this case. I found similar query at Cross Validated <a href=""https://stats.stackexchange.com/questions/144327/convert-categorical-data-into-numerical-data"">here</a>, but that is for document clustering and i want to classify individual words into different classes. I also found one unanswered similar <a href=""https://www.quora.com/What-are-different-techniques-for-converting-qualitative-features-into-numerical-features-for-machine-learning-algorithms"" rel=""nofollow noreferrer"">question</a> on quora.</p>

<p>To serve my purpose, I want either the textual feature to be converted into numerical one or a classifier that can take textual features as input. Is there any one who could help me...</p>
"
"32333312","How to extract chunks from BIO chunked sentences? - python","2015-09-01 13:45:07","8","1514","0","3","","32397158","<p>Give an input sentence, that has <a href=""https://stackoverflow.com/questions/30664677/extract-list-of-persons-and-organizations-using-stanford-ner-tagger-in-nltk"">BIO chunk tags</a>:</p>

<blockquote>
  <p>[('What', 'B-NP'), ('is', 'B-VP'), ('the', 'B-NP'), ('airspeed',
  'I-NP'), ('of', 'B-PP'), ('an', 'B-NP'), ('unladen', 'I-NP'),
  ('swallow', 'I-NP'), ('?', 'O')]</p>
</blockquote>

<p>I would need to extract the relevant phrases out, e.g. if I want to extract <code>'NP'</code>, I would need to extract the fragments of tuples that contains <code>B-NP</code> and <code>I-NP</code>.</p>

<p>[out]:</p>

<pre><code>[('What', '0'), ('the airspeed', '2-3'), ('an unladen swallow', '5-6-7')]
</code></pre>

<p>(Note: the numbers in the extract tuples represent the token index.)</p>

<p>I have tried extracting it using the following code:</p>

<pre><code>def extract_chunks(tagged_sent, chunk_type):
    current_chunk = []
    current_chunk_position = []
    for idx, word_pos in enumerate(tagged_sent):
        word, pos = word_pos
        if '-'+chunk_type in pos: # Append the word to the current_chunk.
            current_chunk.append((word))
            current_chunk_position.append((idx))
        else:
            if current_chunk: # Flush the full chunk when out of an NP.
                _chunk_str = ' '.join(current_chunk) 
                _chunk_pos_str = '-'.join(map(str, current_chunk_position))
                yield _chunk_str, _chunk_pos_str 
                current_chunk = []
                current_chunk_position = []
    if current_chunk: # Flush the last chunk.
        yield ' '.join(current_chunk), '-'.join(current_chunk_position)


tagged_sent = [('What', 'B-NP'), ('is', 'B-VP'), ('the', 'B-NP'), ('airspeed', 'I-NP'), ('of', 'B-PP'), ('an', 'B-NP'), ('unladen', 'I-NP'), ('swallow', 'I-NP'), ('?', 'O')]
print (list(extract_chunks(tagged_sent, chunk_type='NP')))
</code></pre>

<p>But when I have adjacent chunk of the same type:</p>

<pre><code>tagged_sent = [('The', 'B-NP'), ('Mitsubishi', 'I-NP'),  ('Electric', 'I-NP'), ('Company', 'I-NP'), ('Managing', 'B-NP'), ('Director', 'I-NP'), ('ate', 'B-VP'), ('ramen', 'B-NP')]

print (list(extract_chunks(tagged_sent, chunk_type='NP')))
</code></pre>

<p>It outputs this:</p>

<pre><code>[('The Mitsubishi Electric Company Managing Director', '0-1-2-3-4-5'), ('ramen', '7')]
</code></pre>

<p>Instead of the desired:</p>

<pre><code>[('The Mitsubishi Electric Company', '0-1-2-3'), ('Managing Director', '4-5'), ('ramen', '7')]
</code></pre>

<p><strong>How can this be resolved from the above code?</strong></p>

<p><strong>Other than how it's done from the code above, is there a better solution to extract the desired chunks of a specific <code>chunk_type</code>?</strong></p>
"
"32264398","Replace words into special string except nouns and adjectives in Python","2015-08-28 06:10:36","0","962","0","2","","32264508","<p>I want to replace words (e.g., verbs, adverbs...) into some special string (e.g., ""NIL"") except adjectives and nouns. </p>

<p>That is to say, for a text:</p>

<blockquote>
  <p>anarchism originated as a term of abuse first used against early working class radicals</p>
</blockquote>

<p>I first do POS tagging (universal format), resulting in a tagged format:</p>

<blockquote>
  <p>anarchism/NOUN originated/VERB as/ADP a/DET term/NOUN of/ADP abuse/NOUN first/ADV used/VERB against/ADP early/ADJ working/NOUN class/NOUN radicals/NOUN</p>
</blockquote>

<p>and I want to obtain the text like this:</p>

<blockquote>
  <p>anarchism/NOUN NIL NIL NIL term/NOUN NIL abuse/NOUN NIL NIL NIL NIL working/NOUN class/NOUN radicals/NOUN</p>
</blockquote>

<p>which preserve the nouns and adjectives while replace the other words with special string (like ""NIL"").</p>

<p>Is there some efficient way to do this in Python, my corpus size could be 10G+.</p>

<p>Thanks a lot!</p>
"
"32253125","How to get word count from TF*IDF value in sklearn","2015-08-27 15:09:23","4","6126","4","1","","32263235","<p>I want to get the count of a word in a given sentence using only tf*idf matrix of a set of sentences. I use TfidfVectorizer from sklearn.feature_extraction.text.</p>

<p>Example : </p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer

sentences = (""The sun is shiny i like the sun"",""I have been exposed to sun"")
vect = TfidfVectorizer(stop_words=""english"",lowercase=False)
tfidf_matrix = vect.fit_transform(sentences).toarray()
</code></pre>

<p>I want to be able to calculate the number of times the term ""sun"" occurs in the first sentence (which is 2) using only tfidf_matrix[0] and probably vect.idf_ .
I know there are infinite ways to get term frequency and words count but I have a special case where I only have a tf<em>idf matrix.
I already tried to divide the tf</em>idf value of the word ""sun"" in the first sentence by its idf value to get tf. Then I multiplied tf by the total number of words in the sentence to get the words count. Unfortunately, I get wrong values.</p>
"
"32224227","NLTK identifies verb as Noun in Imperatives","2015-08-26 10:32:09","2","3866","1","2","","32225775","<p>I am using NLTK POS tagger as below</p>

<pre><code>sent1='get me now'
sent2='run fast'
tags=pos_tag(word_tokenize(sent2))
print tags
[('run', 'NN'), ('fast', 'VBD')]
</code></pre>

<p>I find similar posts <a href=""https://stackoverflow.com/questions/9406093/nltk-thinks-that-imperatives-are-nouns"">NLTK Thinks that Imperatives are Nouns</a> which suggest to add the word to a dictionary as a verb.
Problem is I have too many such unknown words.
But one clue I have, they always appear at the start of a phrase.</p>

<p>Eg: 'Download now', 'Book it now', 'Sign up'</p>

<p>How can i correctly assist the NLTK to produce correct result </p>
"
"32106090","NLTK Brill Tagger Splitting Words","2015-08-19 21:39:05","2","916","0","1","","32106813","<p>I am using python version 3.4.1 and NLTK version 3 and I am trying to use their  Brill Tagger. </p>

<p>Here is the training code for the brill tagger:</p>

<pre><code>import nltk
from nltk.tag.brill import *
import nltk.tag.brill_trainer as bt
from nltk.corpus import brown

Template._cleartemplates()
templates = fntbl37()
tagged_sentences = brown.tagged_sents(categories = 'news')
tagged_sentences = tagged_sentences[:]
tagger = nltk.tag.BigramTagger(tagged_sentences)
tagger = bt.BrillTaggerTrainer(tagger, templates, trace=3)
tagger = tagger.train(tagged_sentences, max_rules=250)
print(tagger.evaluate(brown.tagged_sents(categories='fiction')[:]))
print(tagger.tag(""Hi I am Harry Potter.""))
</code></pre>

<p>The output to the last command however is:</p>

<pre><code>[('H', 'NN'), ('i', 'NN'), (' ', 'NN'), ('I', 'NN'), (' ', 'NN'), ('a', 'AT'), ('m', 'NN'), (' ', 'NN'), ('H', 'NN'), ('a', 'AT'), ('r', 'NN'), ('r', 'NN'), ('y', 'NN'), (' ', 'NN'), ('P', 'NN'), ('o', 'NN'), ('t', 'NN'), ('t', 'NN'), ('e', 'NN'), ('r', 'NN'), ('.', '.')]
</code></pre>

<p>How do I stop it from splitting the words into letters and tagging the letters instead of the word?</p>
"
"31848056","Snowball Stemming: defining Regions","2015-08-06 06:13:36","3","116","0","1","","31849259","<p>I'm trying to understand the snoball stemming algorithmus. The algorithmus is using two regions R1 and R2 that are definied as follows:</p>
<blockquote>
<p>R1 is the region after the first non-vowel following a vowel, or is
the null region at the end of the word if there is no such non-vowel.</p>
<p>R2 is the region after the first non-vowel following a vowel in R1, or
is the null region at the end of the word if there is no such
non-vowel.</p>
<p><a href=""http://snowball.tartarus.org/texts/r1r2.html"" rel=""nofollow noreferrer"">http://snowball.tartarus.org/texts/r1r2.html</a></p>
</blockquote>
<p>Examples are</p>
<pre><code>    b   e   a   u   t   i   f   u   l
                      |&lt;-------------&gt;|    R1
                              |&lt;-----&gt;|    R2

   b   e   a   u   t   y
                     |&lt;-&gt;|    R1
                       -&gt;|&lt;-  R2

   a   n   i   m   a   d   v   e   r   s   i   o   n
        |&lt;-----------------------------------------&gt;|    R1
                |&lt;---------------------------------&gt;|    R2

   s   p   r   i   n   k   l   e   d
                     |&lt;-------------&gt;|    R1
                                   -&gt;|&lt;-  R2

    e   u   c   h   a   r   i   s   t
            |&lt;---------------------&gt;|    R1
                        |&lt;---------&gt;|    R2
</code></pre>
<p>My question is, why is &quot;kled&quot; in springkled and &quot;harist&quot; in eucharist defined as R1? I thought the correct result would be &quot;inkled&quot; and &quot;arist&quot;?</p>
"
"31844602","Algorithms for Natural Language Understanding","2015-08-05 23:46:08","2","1943","0","2","","31867346","<p>I wanted to know what algorithms I could use for NLU?</p>
<p>For example, let's say I want to start a program, and I have these sentences</p>
<blockquote>
<p>&quot;Let us start&quot;</p>
<p>&quot;Let him start&quot;</p>
</blockquote>
<p>Obviously, the first sentence should start the program, but not the second one (since it doesn't make sense).</p>
<p>Right now, I have am using Stanford's NLP API and have implemented the TokenRegexAnnotator class:</p>
<pre><code>CoreMapExpressionExtractor&lt;MatchedExpression&gt; extractor = CoreMapExpressionExtractor.createExtractorFromFile(env, &quot;tr.txt&quot;);
</code></pre>
<p>So my code &quot;knows&quot; what &quot;Start&quot; should do, that is, &quot;Start&quot; should trigger/start the program. But &quot;Start&quot; could be used with anything, like &quot;Start the car.&quot; In this case, I wouldn't want to &quot;Start&quot; the program because the sentence is about starting a car, not the program. To solve this, I used Stanford's CollapsedDependenciesAnnotation class:</p>
<pre><code>SemanticGraph dependencies = s.get(CollapsedDependenciesAnnotation.class);
Iterable&lt;SemanticGraphEdge&gt; edge_set = dependencies.edgeIterable();
</code></pre>
<p>I used the <code>nsubj</code> dependency to see if the subject was a <code>PRP</code> (pronoun) since I want the program to start only when the subject is a <code>PRP</code>. So when I inputed the sentence &quot;let us start&quot; in my program, the program started. However, when I inputed the sentence &quot;Start the car,&quot; the program didn't start. All is working well...</p>
<p>BUT the program will also start when I input the sentence &quot;Let him start&quot; (as mentioned above). (It starts because &quot;him&quot; is also a pronoun). I do not want the program to start when I input this sentence (because &quot;Let him start&quot; has nothing to do with the starting the program). So how will the program know this? What can I do to solve this problem? Are there algorithms that will let the computer differentiate between &quot;let us start&quot; and &quot;let him start&quot;?</p>
<p>Any ideas on how to solve this problem?</p>
<p>Thank you!</p>
<p>(I hope I am being clear)</p>
"
"31843524","Word Sense Disambiguation for Arabic text with NLTK","2015-08-05 21:56:08","2","1181","0","1","","31843581","<p>NLTK allows me to disambiguate text with <code>nltk.wsd.lesk</code>, e.g. </p>

<pre><code>&gt;&gt;&gt; from nltk.corpus import wordnet as wn
&gt;&gt;&gt; from nltk.wsd import lesk
&gt;&gt;&gt; sent = ""I went to the bank to deposit money""
&gt;&gt;&gt; ambiguous = ""deposit""
&gt;&gt;&gt; lesk(sent, ambiguous, pos='v')
Synset('deposit.v.02')
</code></pre>

<p><a href=""https://github.com/alvations/pywsd"" rel=""nofollow""><code>PyWSD</code></a> does the same but it's only for English text.</p>

<hr>

<p>NLTK supports arabic wordnet from the <a href=""http://compling.hss.ntu.edu.sg/omw/"" rel=""nofollow"">Open Multilingual WordNet</a>, e.g.</p>

<pre><code>&gt;&gt;&gt; wn.synsets('deposit', pos='v')[1].lemma_names(lang='arb')
[u'\u0623\u064e\u0648\u0652\u062f\u064e\u0639\u064e']
&gt;&gt;&gt; print wn.synsets('deposit', pos='v')[1].lemma_names(lang='arb')[0]
أَوْدَعَ
</code></pre>

<p>Also, the synsets are indexed for Arabic:</p>

<pre><code>&gt;&gt;&gt; wn.synsets(u'أَوْدَعَ', lang='arb')
[Synset('entrust.v.02'), Synset('deposit.v.02'), Synset('commit.v.03'), Synset('entrust.v.01'), Synset('consign.v.02')]
</code></pre>

<p>But how could i disambiguate Arabic texts and extract concepts from a query using nltk? </p>

<p>I was wondering if it is possible to use Lesk algorithm to deal with Arabic texts through nltk?</p>
"
"31823347","CoreNLP API for N-grams with position","2015-08-05 04:16:33","1","587","0","2","","31867703","<p>Does CoreNLP have an API for getting ngrams with position etc.?</p>

<p>For example, I have a string ""I have the best car "". 
if I am using mingrams=1 and maxgrams=2.
I should get the following like below.I know stringutil with ngram function but how to get position.</p>

<pre><code>(I,0)
(I have,0)
(have,1)
(have the,1)
(the,2)
(the best,2) etc etc
</code></pre>

<p>based on the string I am passing.</p>

<p>Any help is really appreciated.</p>

<p>Thanks</p>
"
"31723623","How to retrieve all variants of a lexeme in Java?","2015-07-30 12:19:49","1","364","0","1","","31728189","<p>I am searching for a way to retrieve all variants of the lexeme of a specific word.</p>

<p>Example: running -> (run, runs, ran, running…)</p>

<p>I tried out Stanford NLP according to <a href=""https://stackoverflow.com/questions/1578062/lemmatization-java"">this post</a>. However, the lemma-annotator only retrieves the lemma (running -> run), not the complete set of variants. Is there a way to do this with Stanford NLP or another Java Lib/Framework?</p>

<p>Clarification: I do not search for a stemmer. Also, I would like to avoid programming a new algorithm from scratch to crawl WordNet or similar dictionaries.</p>
"
"31689621","How to Traverse an NLTK Tree object?","2015-07-29 01:06:03","6","11642","1","1","","31727984","<p>Given a bracketed parse, I could convert it into a Tree object in NLTK as such:</p>

<pre><code>&gt;&gt;&gt; from nltk.tree import Tree
&gt;&gt;&gt; s = '(ROOT (S (NP (NNP Europe)) (VP (VBZ is) (PP (IN in) (NP (DT the) (JJ same) (NNS trends)))) (. .)))'
&gt;&gt;&gt; Tree.fromstring(s)
Tree('ROOT', [Tree('S', [Tree('NP', [Tree('NNP', ['Europe'])]), Tree('VP', [Tree('VBZ', ['is']), Tree('PP', [Tree('IN', ['in']), Tree('NP', [Tree('DT', ['the']), Tree('JJ', ['same']), Tree('NNS', ['trends'])])])]), Tree('.', ['.'])])])
</code></pre>

<p>But when I try to traverse it, I can only access the top most Tree:</p>

<pre><code>&gt;&gt;&gt; for i in Tree.fromstring(s):
...     print i
... 
(S
  (NP (NNP Europe))
  (VP (VBZ is) (PP (IN in) (NP (DT the) (JJ same) (NNS trends))))
  (. .))
&gt;&gt;&gt; for i in Tree.fromstring(s):
...     print i, i.label()
... 
(S
  (NP (NNP Europe))
  (VP (VBZ is) (PP (IN in) (NP (DT the) (JJ same) (NNS trends))))
  (. .)) S
&gt;&gt;&gt; 
</code></pre>

<p>I could go one level deep as follows:</p>

<pre><code>&gt;&gt;&gt; for i in Tree.fromstring(s):
...     print i.subtrees()
... 
&lt;generator object subtrees at 0x7f1eb1571410&gt;
&gt;&gt;&gt; for i in Tree.fromstring(s):
...     for j in i.subtrees():
...             print j
... 
(S
  (NP (NNP Europe))
  (VP (VBZ is) (PP (IN in) (NP (DT the) (JJ same) (NNS trends))))
  (. .))
(NP (NNP Europe))
(NNP Europe)
(VP (VBZ is) (PP (IN in) (NP (DT the) (JJ same) (NNS trends))))
(VBZ is)
(PP (IN in) (NP (DT the) (JJ same) (NNS trends)))
(IN in)
(NP (DT the) (JJ same) (NNS trends))
(DT the)
(JJ same)
(NNS trends)
(. .)
</code></pre>

<p>But is there a way to traverse all subtrees depth wise?</p>

<p><strong>How should one traverse a tree in NLTK?</strong></p>

<p><strong>How to traverse all subtrees in NLTK?</strong></p>
"
"31684393","How to Interpret NLTK Brill Tagger Rules","2015-07-28 18:30:02","3","512","0","1","","31696079","<p>For the generated Brill Tagger Rule:</p>

<pre><code>Rule('016', 'CS', 'QL', [(Word([1, 2, 3]),'as')])
</code></pre>

<p>I know:
<code>'CS'</code> is subordinating conjunction
<code>'QL'</code> is qualifier</p>

<p>I guess:
<code>[(Word([1, 2, 3]),'as')]</code> means the condition of the rule. It stands for the word <code>'as'</code> appear as the first, second or third position before the target word. Target word is word that is going to be tagged by POS tag.</p>

<p>I do not know:
What is the meaning for <code>'016'</code>?
How to interpret the rule as a whole?</p>
"
"31400985","bad tokenization in stanford postagger","2015-07-14 07:58:57","2","506","2","1","","31421056","<p>I'm trying to use the Stanford POS tagger to tag some French text. To do that, I use the following command:</p>

<blockquote>
  <p>cat file.txt | java -mx10000m -cp 'stanford-postagger.jar:'
  edu.stanford.nlp.tagger.maxent.MaxentTagger -model
  models/french.tagger -sentenceDelimiter newline > output.txt</p>
</blockquote>

<p>(There is one sentence per line.)</p>

<p>But I noticed that the tags were pretty bad, and that the real issue actually comes from the French tokenization itself. I think that the tokenization is done by an English tokenizer.</p>

<p>So I tried to only tokenize the text in French by doing this:</p>

<blockquote>
  <p>cat file.txt | java -mx10000m -cp 'stanford-postagger.jar:'
  edu.stanford.nlp.international.french.process.FrenchTokenizer
  -sentenceDelimiter newline > tokenized.txt</p>
</blockquote>

<p>And there the French tokens are good.</p>

<p>How can I tell the tagger to use the French model for tagging, but also the French tokenizer at the same time?</p>
"
"31349851","Provoke the NLTK part-of-speech tagger to report a plural proper noun","2015-07-10 20:34:49","1","327","0","1","","31393224","<p>Let's try out Python's renouned part-of-speech tagger in the <code>nltk</code> package.</p>

<pre><code>import nltk
# You might also need to run nltk.download('maxent_treebank_pos_tagger') 
#  even after installing nltk

string = 'Buddy Billy went to the moon and came Back with several Vikings.'
nltk.pos_tag(nltk.word_tokenize(string))
</code></pre>

<p>This gives me</p>

<blockquote>
  <p>[('Buddy', 'NNP'), ('Billy', 'NNP'), ('went', 'VBD'), ('to', 'TO'),
  ('the', 'DT'), ('moon', 'NN'), ('and', 'CC'), ('came', 'VBD'),
  ('Back', 'NNP'), ('with', 'IN'), ('several', 'JJ'), ('Vikings',
  'NNS'), ('.', '.')]</p>
</blockquote>

<p>You can interpret the codes <a href=""http://cs.nyu.edu/grishman/jet/guide/PennPOS.html"" rel=""nofollow"">here</a>.  I'm slightly disappointed that 'Back' got categorized as a proper noun (NNP), although the confusion is understandable.  I'm more upset that 'Vikings' got called a simple plural noun (NNS) instead of a plural proper noun (NNPS).  Can anyone come up with a single example of a brief input that leads to at least one NNPS tag?</p>
"
"31327126","Accessing terms statistics in Lucene 4","2015-07-09 20:02:07","2","745","0","1","","31332326","<p>I have a Lucene index, and I need to access some statistics such as term collection frequency. <code>BasicStats</code> class has this information, however, I could not understand whether this class is accessible. </p>

<p>Is it possible to access <code>BasicStats</code> class in Lucene 4?</p>
"
"31223082","How do I extract the offset of a WordNet synset give a synset in Python NLTK?","2015-07-04 16:56:39","2","2818","1","1","","31233978","<p>A sense offset in WordNet is an 8 digit number followed by a POS tag. For example, the offset for the synset 'dog.n.01' is '02084071-n'. I have tried the following code:</p>

<pre><code>    from nltk.corpus import wordnet as wn

    ss = wn.synset('dog.n.01')
    offset = str(ss.offset)
    print (offset)
</code></pre>

<p>However, I get this output:</p>

<pre><code>    &lt;bound method Synset.offset of Synset('dog.n.01')&gt;
</code></pre>

<p>How do I get the actual offset in this format: '02084071-n'?</p>
"
"31220234","Select only 'NN' and 'VB' words from NTLK pos_tag","2015-07-04 11:44:15","0","4874","0","3","","31221130","<p>I need to print only 'NN' and 'VB' words from an entered sentence.</p>

<pre><code>import nltk
import re
import time

var = raw_input(""Please enter something: "")


exampleArray = [var]


def processLanguage():
    try:
        for item in exampleArray:
            tokenized = nltk.word_tokenize(item)
            tagged = nltk.pos_tag(tokenized)
            print tagged

            time.sleep(555)


    except Exception, e:
        print str(e)

processLanguage()
</code></pre>
"
"31057029","Are there any Lucene stemmers that handle Shakespearean English?","2015-06-25 17:34:03","3","99","4","1","","31075605","<p>I'm trying to index some old documents for searching -- 16th, 17th, 18th century.</p>

<p>Modern stemmers don't seem to handle the antiquated word endings: worketh, liveth, walketh.</p>

<p>Are there stemmers that specialize in the English from the time of Shakespeare and the King James Bible? I'm currently using <code>solr.PorterStemFilterFactory</code>.</p>
"
"31023099","How can I easily draw a parse tree from Stanford parsing data in python?","2015-06-24 09:42:20","5","3621","2","3","","31023937","<p>So I have this Stanford-style parsing of an english sentence:</p>

<pre><code>""There is a tree behind a car""
Parse: [S [NP There_EX NP] [VP is_VBZ [NP [NP a_DT tree_NN NP] [PP behind_IN [NP a_DT car_NN NP] PP] NP] VP] S]
</code></pre>

<p>I want to use some of the tree drawing methods in python to draw a parsing tree from the data.</p>

<p>Is there an easy way to use that parsing representation to draw a tree with python or should I change the representation somehow?</p>
"
"30995232","How to use OpenNLP to get POS tags in R?","2015-06-23 06:19:58","7","13316","1","3","","30997328","<p>Here is the R Code:</p>

<pre><code>library(NLP) 
library(openNLP)
tagPOS &lt;-  function(x, ...) {
s &lt;- as.String(x)
word_token_annotator &lt;- Maxent_Word_Token_Annotator()
a2 &lt;- Annotation(1L, ""sentence"", 1L, nchar(s))
a2 &lt;- annotate(s, word_token_annotator, a2)
a3 &lt;- annotate(s, Maxent_POS_Tag_Annotator(), a2)
a3w &lt;- a3[a3$type == ""word""]
POStags &lt;- unlist(lapply(a3w$features, `[[`, ""POS""))
POStagged &lt;- paste(sprintf(""%s/%s"", s[a3w], POStags), collapse = "" "")
list(POStagged = POStagged, POStags = POStags)}
str &lt;- ""this is a the first sentence.""
tagged_str &lt;-  tagPOS(str)
</code></pre>

<p>Output is :</p>

<blockquote>
  <p>tagged_str
      $POStagged
      [1]""this/DT is/VBZ a/DT the/DT first/JJ sentence/NN ./.""</p>
</blockquote>

<p>Now I want to extract only NN word i.e sentence from the above sentence and want to store it into a variable .Can anyone help me out with this .</p>
"
"30821188","Python NLTK pos_tag not returning the correct part-of-speech tag","2015-06-13 16:52:28","36","21117","4","3","","30823202","<p>Having this:</p>

<pre><code>text = word_tokenize(""The quick brown fox jumps over the lazy dog"")
</code></pre>

<p>And running:</p>

<pre><code>nltk.pos_tag(text)
</code></pre>

<p>I get:</p>

<pre><code>[('The', 'DT'), ('quick', 'NN'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'NNS'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'NN'), ('dog', 'NN')]
</code></pre>

<p>This is incorrect. The tags for <code>quick brown lazy</code> in the sentence should be:</p>

<pre><code>('quick', 'JJ'), ('brown', 'JJ') , ('lazy', 'JJ')
</code></pre>

<p>Testing this through their <a href=""http://nlp.stanford.edu:8080/corenlp/process"">online tool</a> gives the same result; <code>quick</code>, <code>brown</code> and <code>fox</code> should be adjectives not nouns.</p>
"
"30816692","How to extract derivation rules from a bracketed parse tree?","2015-06-13 08:33:15","4","782","5","3","","30888733","<p>I have a lot of parse trees like this:</p>
<pre><code>( S ( NP-SBJ ( PRP I  )  )  ( INODE@S ( VP ( VBP have  )  ( NP ( DT a  )  ( INODE@NP ( NN savings  )  ( NN account  )  )  )  )  ( . .  )  )  )
</code></pre>
<p>for a sentence like this: &quot;I have a savings account .&quot;</p>
<p>I need to extract all derivation rules from these trees.
The derivation rules like:</p>
<pre><code>S -&gt; NP-SBJ INODE@S
NP-SBJ -&gt; PRP 
PRP -&gt; I
INODE@S -&gt; VP NP
and so on.
</code></pre>
<p>Is there any prepared code (preferably in java) or pseudo code for this purpose?</p>
<p><strong>Edit:</strong></p>
<p>I think this problem is very general and common in many areas. The simplified problem is to find each parent and it's children from a parenthesis tree.</p>
"
"30746460","How to interpret scikit's learn confusion matrix and classification report?","2015-06-10 03:12:02","36","49940","0","3","","30748053","<p>I have a sentiment analysis task, for this Im using this <a href=""http://pastebin.com/ikbKQcsc"" rel=""noreferrer"">corpus</a> the opinions have 5 classes (<code>very neg</code>, <code>neg</code>, <code>neu</code>, <code>pos</code>, <code>very pos</code>), from 1 to 5. So I do the classification as follows:</p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
tfidf_vect= TfidfVectorizer(use_idf=True, smooth_idf=True,
                            sublinear_tf=False, ngram_range=(2,2))
from sklearn.cross_validation import train_test_split, cross_val_score

import pandas as pd

df = pd.read_csv('/corpus.csv',
                     header=0, sep=',', names=['id', 'content', 'label'])

X = tfidf_vect.fit_transform(df['content'].values)
y = df['label'].values


from sklearn import cross_validation
X_train, X_test, y_train, y_test = cross_validation.train_test_split(X,
                                                    y, test_size=0.33)


from sklearn.svm import SVC
svm_1 = SVC(kernel='linear')
svm_1.fit(X, y)
svm_1_prediction = svm_1.predict(X_test)
</code></pre>

<p>Then with the metrics I obtained the following confusion matrix and classification report, as follows:</p>

<pre><code>print '\nClasification report:\n', classification_report(y_test, svm_1_prediction)
print '\nConfussion matrix:\n',confusion_matrix(y_test, svm_1_prediction)
</code></pre>

<p>Then, this is the result:</p>

<pre><code>Clasification report:
             precision    recall  f1-score   support

          1       1.00      0.76      0.86        71
          2       1.00      0.84      0.91        43
          3       1.00      0.74      0.85        89
          4       0.98      0.95      0.96       288
          5       0.87      1.00      0.93       367

avg / total       0.94      0.93      0.93       858


Confussion matrix:
[[ 54   0   0   0  17]
 [  0  36   0   1   6]
 [  0   0  66   5  18]
 [  0   0   0 273  15]
 [  0   0   0   0 367]]
</code></pre>

<p>How can I interpret the above confusion matrix and classification report. I tried reading the <a href=""http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html"" rel=""noreferrer"">documentation</a> and this <a href=""https://stats.stackexchange.com/questions/95209/how-can-i-interpret-sklearn-confusion-matrix"">question</a>. But still can interpretate what happened here particularly with this data?. Wny this matrix is somehow ""diagonal""?. By the other hand what means the recall, precision, f1score and support for this data?. What can I say about this data?. Thanks in advance guys</p>
"
"30722624","Stanford Parser - Factored model and PCFG","2015-06-09 03:43:20","1","574","0","1","","30739570","<p>What is the difference between the factored and PCFG models of stanford parser? (In terms of theoretical working and mathematical perspective)</p>
"
"30676448","extracting n grams from huge text","2015-06-05 21:59:07","-2","10891","11","5","","30710603","<p>For example we have following text:</p>

<blockquote>
  <p>""Spark is a framework for writing fast, distributed programs. Spark
  solves similar problems as Hadoop MapReduce does but with a fast
  in-memory approach and a clean functional style API. ...""</p>
</blockquote>

<p>I need all possible section of this text respectively, for one word by one word, then two by two, three by three to five to five.
like this:</p>

<blockquote>
  <p>ones : ['Spark', 'is', 'a', 'framework', 'for', 'writing, 'fast',
  'distributed', 'programs', ...]</p>
  
  <p>twos : ['Spark is', 'is a', 'a framework', 'framework for', 'for writing'
  ...] </p>
  
  <p>threes : ['Spark is a', 'is a framework', 'a framework for', 
  'framework for writing', 'for writing fast', ...]</p>
  
  <p>. . .</p>
  
  <p>fives : ['Spark is a framework for', 'is a framework for writing',
  'a framework for writing fast','framework for writing fast distributed', ...]</p>
</blockquote>

<p>Please note that the text to be processed is huge text( about 100GB).
I need the best solution for this process. May be it should be processed multi thread in parallel.</p>

<p>I don't need whole list at once, it can be streaming.</p>
"
"30460713","Parsing multiple sentences with MaltParser using NLTK","2015-05-26 13:58:33","11","1381","0","1","","30600977","<p>There have been many MaltParser and/or NLTK related questions:</p>

<ul>
<li><a href=""https://stackoverflow.com/questions/20091698/malt-parser-throwing-class-not-found-exception"">Malt Parser throwing class not found exception</a></li>
<li><a href=""https://stackoverflow.com/questions/14009330/how-to-use-malt-parser-in-python-nltk"">How to use malt parser in python nltk</a></li>
<li><a href=""https://stackoverflow.com/questions/29513187/maltparser-not-working-in-python-nltk"">MaltParser Not Working in Python NLTK</a></li>
<li><a href=""https://stackoverflow.com/questions/9524553/nltk-maltparser-wont-parse"">NLTK MaltParser won&#39;t parse</a></li>
<li><a href=""https://stackoverflow.com/questions/21815891/dependency-parser-using-nltk-and-maltparser"">Dependency parser using NLTK and MaltParser</a></li>
<li><a href=""https://stackoverflow.com/questions/23463519/dependency-parsing-using-maltparser-and-nltk"">Dependency Parsing using MaltParser and NLTK</a></li>
<li><a href=""https://stackoverflow.com/questions/3944269/parsing-with-maltparser-engmalt"">Parsing with MaltParser engmalt</a></li>
<li><a href=""https://stackoverflow.com/questions/17392790/parse-raw-text-with-maltparser-in-java"">Parse raw text with MaltParser in Java</a></li>
</ul>

<p>Now, there's a more stabilized version of MaltParser API in NLTK: <a href=""https://github.com/nltk/nltk/pull/944"" rel=""nofollow noreferrer"">https://github.com/nltk/nltk/pull/944</a> but there are issues when it comes to parsing multiple sentences at the same time. </p>

<p>Parsing one sentence at a time seems fine:</p>

<pre><code>_path_to_maltparser = '/home/alvas/maltparser-1.8/dist/maltparser-1.8/'
_path_to_model= '/home/alvas/engmalt.linear-1.7.mco'     
&gt;&gt;&gt; mp = MaltParser(path_to_maltparser=_path_to_maltparser, model=_path_to_model)
&gt;&gt;&gt; sent = 'I shot an elephant in my pajamas'.split()
&gt;&gt;&gt; sent2 = 'Time flies like banana'.split()
&gt;&gt;&gt; print(mp.parse_one(sent).tree())
(pajamas (shot I) an elephant in my)
</code></pre>

<p>But parsing a list of sentences doesn't return a DependencyGraph object:</p>

<pre><code>_path_to_maltparser = '/home/alvas/maltparser-1.8/dist/maltparser-1.8/'
_path_to_model= '/home/alvas/engmalt.linear-1.7.mco'     
&gt;&gt;&gt; mp = MaltParser(path_to_maltparser=_path_to_maltparser, model=_path_to_model)
&gt;&gt;&gt; sent = 'I shot an elephant in my pajamas'.split()
&gt;&gt;&gt; sent2 = 'Time flies like banana'.split()
&gt;&gt;&gt; print(mp.parse_one(sent).tree())
(pajamas (shot I) an elephant in my)
&gt;&gt;&gt; print(next(mp.parse_sents([sent,sent2])))
&lt;listiterator object at 0x7f0a2e4d3d90&gt; 
&gt;&gt;&gt; print(next(next(mp.parse_sents([sent,sent2]))))
[{u'address': 0,
  u'ctag': u'TOP',
  u'deps': [2],
  u'feats': None,
  u'lemma': None,
  u'rel': u'TOP',
  u'tag': u'TOP',
  u'word': None},
 {u'address': 1,
  u'ctag': u'NN',
  u'deps': [],
  u'feats': u'_',
  u'head': 2,
  u'lemma': u'_',
  u'rel': u'nn',
  u'tag': u'NN',
  u'word': u'I'},
 {u'address': 2,
  u'ctag': u'NN',
  u'deps': [1, 11],
  u'feats': u'_',
  u'head': 0,
  u'lemma': u'_',
  u'rel': u'null',
  u'tag': u'NN',
  u'word': u'shot'},
 {u'address': 3,
  u'ctag': u'AT',
  u'deps': [],
  u'feats': u'_',
  u'head': 11,
  u'lemma': u'_',
  u'rel': u'nn',
  u'tag': u'AT',
  u'word': u'an'},
 {u'address': 4,
  u'ctag': u'NN',
  u'deps': [],
  u'feats': u'_',
  u'head': 11,
  u'lemma': u'_',
  u'rel': u'nn',
  u'tag': u'NN',
  u'word': u'elephant'},
 {u'address': 5,
  u'ctag': u'NN',
  u'deps': [],
  u'feats': u'_',
  u'head': 11,
  u'lemma': u'_',
  u'rel': u'nn',
  u'tag': u'NN',
  u'word': u'in'},
 {u'address': 6,
  u'ctag': u'NN',
  u'deps': [],
  u'feats': u'_',
  u'head': 11,
  u'lemma': u'_',
  u'rel': u'nn',
  u'tag': u'NN',
  u'word': u'my'},
 {u'address': 7,
  u'ctag': u'NNS',
  u'deps': [],
  u'feats': u'_',
  u'head': 11,
  u'lemma': u'_',
  u'rel': u'nn',
  u'tag': u'NNS',
  u'word': u'pajamas'},
 {u'address': 8,
  u'ctag': u'NN',
  u'deps': [],
  u'feats': u'_',
  u'head': 11,
  u'lemma': u'_',
  u'rel': u'nn',
  u'tag': u'NN',
  u'word': u'Time'},
 {u'address': 9,
  u'ctag': u'NNS',
  u'deps': [],
  u'feats': u'_',
  u'head': 11,
  u'lemma': u'_',
  u'rel': u'nn',
  u'tag': u'NNS',
  u'word': u'flies'},
 {u'address': 10,
  u'ctag': u'NN',
  u'deps': [],
  u'feats': u'_',
  u'head': 11,
  u'lemma': u'_',
  u'rel': u'nn',
  u'tag': u'NN',
  u'word': u'like'},
 {u'address': 11,
  u'ctag': u'NN',
  u'deps': [3, 4, 5, 6, 7, 8, 9, 10],
  u'feats': u'_',
  u'head': 2,
  u'lemma': u'_',
  u'rel': u'dep',
  u'tag': u'NN',
  u'word': u'banana'}]
</code></pre>

<p><strong>Why is that using <code>parse_sents()</code> don't return an iterable of <code>parse_one</code>?</strong></p>

<p>I could however, just get lazy and do:</p>

<pre><code>_path_to_maltparser = '/home/alvas/maltparser-1.8/dist/maltparser-1.8/'
_path_to_model= '/home/alvas/engmalt.linear-1.7.mco'     
&gt;&gt;&gt; mp = MaltParser(path_to_maltparser=_path_to_maltparser, model=_path_to_model)
&gt;&gt;&gt; sent1 = 'I shot an elephant in my pajamas'.split()
&gt;&gt;&gt; sent2 = 'Time flies like banana'.split()
&gt;&gt;&gt; sentences = [sent1, sent2]
&gt;&gt;&gt; for sent in sentences:
&gt;&gt;&gt; ...    print(mp.parse_one(sent).tree())
</code></pre>

<p>But this is not the solution I'm looking for. <strong>My question is how to answer why doesn't the <code>parse_sent()</code> return an iterable of <code>parse_one()</code>. and how could it be fixed in the NLTK code?</strong></p>

<hr>

<p>After @NikitaAstrakhantsev answered, I've tried it outputs a parse tree now but it seems to be confused and puts both sentences into one before parsing it.</p>

<pre><code># Initialize a MaltParser object with a pre-trained model.
mp = MaltParser(path_to_maltparser=path_to_maltparser, model=path_to_model) 
sent = 'I shot an elephant in my pajamas'.split()
sent2 = 'Time flies like banana'.split()
# Parse a single sentence.
print(mp.parse_one(sent).tree())
print(next(next(mp.parse_sents([sent,sent2]))).tree())
</code></pre>

<p>[out]:</p>

<pre><code>(pajamas (shot I) an elephant in my)
(shot I (banana an elephant in my pajamas Time flies like))
</code></pre>

<p>From the code it seems to be doing something weird: <a href=""https://github.com/nltk/nltk/blob/develop/nltk/parse/api.py#L45"" rel=""nofollow noreferrer"">https://github.com/nltk/nltk/blob/develop/nltk/parse/api.py#L45</a></p>

<p><strong>Why is it that the parser abstract class in NLTK is swooshing two sentences into one before parsing? Am I calling the <code>parse_sents()</code> incorrectly? If so, what is the correct way to call <code>parse_sents()</code>?</strong> </p>
"
"30458511","stemming words in python","2015-05-26 12:23:56","0","5269","3","2","","30458575","<p>I'm using this code to stem words, here is how it works, first there's a list of suffixes, the program checks if the word has the ending same as the one in the list if positive it removes the suffix, however, when I run the code I get this result:  </p>

<pre><code>suffixes = ['ing']
def stem(word):
for suff in suffixes:
    return word[:-len(suff)]

stem ('having')
print (stem)
</code></pre>
"
"30413885","NLP- Sentiment Processing for Junk Data takes time","2015-05-23 14:31:29","1","257","0","1","","30414283","<p>I am trying to find the Sentiment for the input text. This test is a junk sentence and when I tried to find the Sentiment the Annotation to parse the sentence is taking around 30 seconds. For normal text it takes less than a second. If i need to process around millions of data it will add up the time to process. Any solution to this.</p>

<pre><code>String text = ""Nm n n 4 n n bkj nun4hmnun Onn njnb hm5bn nm55m nbbh n mnrrnut but n rym4n nbn 4nn65 m nun m n nn nun 4nm 5 gm n my b bb b b rtmrt55tmmm5tttn b b bb g bn nn n h r ret n nun bn d. B bbbbbbbbbbr bung NHnhn nn nk, v v v n gain t g 4gnyhimmigration ndn nb NVnb bin uny 7 nbbbbbnn vv bbvb ninn njnj n4 nm n km n n n cb j bun. Nhfnt bn nn. N hm nn nun m bum my b mmmnbjk nn n by nn nun nun n nun nn bn n nhn n nn n n m NH nb4mnm mkn 4 n n n n hm r b rnfngg4d in b nut mmmkmmm5 bbjn n n ij BBM 8u8i by nun n.nn hm n. n4n By 4n4n bunny RN bny hm j mi. Nymmn FBT not mn n n nm g by n n nnm? Rnyb vCard n5 Yu nn n n n n nt .nm mn nt n nb n n n n by y5nnnhyyh h b b nt njj n m f4n re"";
Properties props = new Properties();
            props.setProperty(""annotators"",""tokenize, ssplit, pos,parse,sentiment"");
            StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
            Annotation annotation = pipeline.process(text);
</code></pre>

<p>For the NLP sentiment calulation I have to parse the data and i think thats the reason its taking time.</p>

<p>thanks</p>
"
"30323409","Python NLTK Brill Tagger does not have SymmetricProximateTokensTemplate, ProximateTokensTemplate, ProximateTagsRule, ProximateWordsRule","2015-05-19 10:44:38","1","2124","0","2","","30526740","<p>When i try importing,</p>

<blockquote>
  <p>from nltk.tag.brill import SymmetricProximateTokensTemplate, ProximateTokensTemplate
  from nltk.tag.brill import ProximateTagsRule, ProximateWordsRule</p>
</blockquote>

<p>Python Throws Import Error,</p>

<blockquote>
  <p>ImportError: cannot import name 'SymmetricProximateTokensTemplate'</p>
</blockquote>

<p>What's the problem?</p>

<p>but this works</p>

<blockquote>
  <p>from nltk.tag import brill</p>
</blockquote>
"
"30283217","UnicodeDecodeError unexpected end of data while stemming over dataset","2015-05-17 03:57:57","2","3331","0","2","","30284782","<p>I am new to python and I am trying to work on a small chunk of Yelp! dataset which was in JSON but I converted to CSV, using <strong>pandas</strong> libraries and <strong>NLTK</strong>. </p>

<p>While doing preprocessing of data, I first try to remove all the punctuations and also the most common stop words. After doing that, I want to apply the Porter Stemming algorithm which is readily available in <strong>nltk.stem</strong>. </p>

<p>Here is my code:</p>

<pre><code>""""""A method for removing the noise in the data and the most common stop.words (NLTK).""""""
def stopWords(review):

    stopset = set(stopwords.words(""english""))
    review = review.lower()
    review = review.replace(""."","""")
    review = review.replace(""-"","" "")
    review = review.replace("")"","""")
    review = review.replace(""("","""")
    review = review.replace(""i'm"","" "")
    review = review.replace(""!"","""")
    review = re.sub(""[$!@#*;:&lt;+&gt;~-]"", '', review)
    row = review.split()

    tokens = ' '.join([word for word in row if word not in stopset])
    return tokens
</code></pre>

<p>and i use the tokens here to input in an stemming method i wrote:</p>

<pre><code>""""""A method for stemming the words to their roots using Porter Algorithm (NLTK)""""""
def stemWords(impWords):
    stemmer = stem.PorterStemmer()
    tok = stopWords(impWords)
    ========================================================================
    stemmed = "" "".join([stemmer.stem(str(word)) for word in tok.split("" "")])
    ========================================================================
    return stemmed
</code></pre>

<p>But i am getting an error <code>UnicodeDecodeError: 'utf8' codec can't decode byte 0xc2 in position 0: unexpected end of data</code>. The line that is inside the '==' is giving me the error.</p>

<p>I have tried cleaning the data and removing all special characters !@#$^&amp;* and others to make this work. But the stop words are working fine. The stemming does not work. Can somebody tell me where i am doing it wrong?</p>

<p>If my data is not clean, or the unicode string is breaking somewhere, any way i can clean it or fix itso that it won't give me this error? I want to do stemming, any suggestions would be helpful.</p>
"
"30275162","determination of human language from text:: system structure","2015-05-16 11:36:56","0","87","0","2","","30275443","<p>I'm using <a href=""http://www.winedt.org/Dict/"" rel=""nofollow"">these word lists</a>.</p>

<p>Right now I'm only thinking about German, Russian, English, and French. </p>

<p>I guess what I'm going to do is put them all as part of a hashmap, one for each language with the word as the key, and a boolean as the value. </p>

<p>When I get an input text I'll search over all the lists and whichever has the most hits will be returned as the answer. </p>

<p>Maybe I'll try to use multi-threading and search each of the dictionaries simultaneously using a different thread. </p>

<p>Is that a good solution to this problem?</p>
"
"30266239","Mecab output - list of name types","2015-05-15 18:17:34","3","424","1","1","","30270898","<p>A sample output from meecab:</p>

<pre><code>に   ニ   ニ   に   助詞-格助詞      
</code></pre>

<p>We have <code>助詞</code>(particle) as the type and <code>格助詞</code> (case-marking particle) as the PoS. Where can I find a list of all possible types and PoS's that mecab uses? I want to be able to map the Japanese to a translated set, without the need to translate it on the fly.</p>
"
"30250726","Python : How to optimize calculations?","2015-05-15 02:47:26","0","412","0","2","","30253046","<p>I'm making some text-mining from a corpus of words, and I'm having a textfile output with 3000 lines  like this :</p>

<blockquote>
  <p>dns 11 11 [2, 355, 706, 1063, 3139, 3219, 3471, 3472, 3473, 4384,
  4444]</p>
  
  <p>xhtml 8 11 [1651, 2208, 2815, 3487, 3517, 4480, 4481, 4504]</p>
  
  <p>javascript 18 18 [49, 50, 175, 176, 355, 706, 1063, 1502, 1651, 2208,
  2280, 2815, 3297, 4068, 4236, 4480, 4481, 4504]</p>
</blockquote>

<p>There is the word, the number of lines where it've appeared, the number of total appearances, and the n° of these lines.</p>

<p>I'm trying to calculate The Chi-squared Value, and that textfile is the input for my code below :</p>

<pre><code>measure = nltk.collocations.BigramAssocMeasures()

dicto = {} 
for i in lines :
    tokens = nltk.wordpunct_tokenize(i)
    m = tokens[0]       #m is the word
    list_i = tokens[4:]
    list_i.pop()
    for x in list_i :
        if x ==',':
            ind = list_i.index(x)
            list_i.pop(ind)
    dicto[m]=list_i #for each word i create a dictionnary with the n° of lines

#for each word I calculate the Chi-squared with every other word 
#and my problem is starting right here i think
#The ""for"" loop and the z = .....


for word1 in dicto :
    x=dicto[word1]
    vector = []

    for word2 in dicto :    
        y=dicto[word2]
        z=[val for val in x if val in y]

        #Contingency Matrix
        m11 = cpt-(len(x)+len(y)-len(z))
        m12 = len(x)-len(z)
        m21 = len(y)-len(z)
        m22 = len(z)

        n_ii =m11
        n_ix =m11+m21
        n_xi =m11+m12
        n_xx =m11+m12+m21+m22 

        Chi_squared = measure.chi_sq(n_ii, (n_ix, n_xi), n_xx)

        #I compare with the minimum value to check independancy between words
        if Chi_squared &gt;3.841 :
            vector.append([word1, word2 , round(Chi_square,3))

    #The correlations calculated
    #I sort my vector in a descending way
    final=sorted(vector, key=lambda vector: vector[2],reverse = True)

    print word1
    #I take the 4 best scores
    for i in final[:4]:
        print i,
</code></pre>

<p>My problem is that the calcul is taking to much time (I'm talking about Hours !!) Is there anything that I can change ? anything that I do to improve my code ? Any other Python structures ? any ideas ?</p>
"
"30249815","Python : How to optimize comparison between two large sets?","2015-05-15 00:49:23","2","290","1","1","","30249900","<p>I salute you ! I'm new here, and I've got a little problem trying to optimize this part of code.</p>

<p>I'm reading from two files :</p>

<p>Corpus.txt -----> Contains my text (of 1.000.000 words)</p>

<p>Stop_words.txt -----> Contains my stop_list (of 4000 words)</p>

<p>I must compare each word from my corpus with every word in the stop_list, because I want to have a text without stop words, so I've :
1.000.000*4000 comparisons to do with the code below : </p>

<pre><code>fich= open(""Corpus.txt"", ""r"")
text = fich.readlines()

fich1= open(""stop_words.txt"", ""r"")
stop = fich1.read()

tokens_stop = nltk.wordpunct_tokenize(stop)
tokens_stop=sorted(set(tokens_stop))

for line in text :
    tokens_rm = nltk.wordpunct_tokenize(line)
    z = [val for val in tokens_rm if val not in tokens_stop]
    for i in z:
        print i
</code></pre>

<p>My question is : Is there anything to do it differently ? Any structure to optimize it ? </p>
"
"30219780","Stanford NLP - Using Parsed or Tagged text to generate Full XML","2015-05-13 15:52:03","1","659","0","1","","30248238","<p>I'm trying to extract data from the PennTreeBank, Wall Street Journal corpus. Most of it already has the parse trees, but some of the data is only tagged.
i.e. wsj_DDXX.mrg and wsj_DDXX.pos files.</p>

<p>I would like to use the already parsed trees and tagged data in these files so as not to use the parser and taggers within CoreNLP, but I still want the output file format that CoreNLP gives; namely, the XML file that contains the dependencies, entity coreference, and the parse tree and tagged data.</p>

<p>I've read many of the java docs but I cannot figure out how to get it the way I described.</p>

<p>For POS, I tried using the <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp-3.5.2/edu/stanford/nlp/parser/lexparser/LexicalizedParser.html"" rel=""nofollow"">LexicalizedParser</a> and it allows me to use the tags, but I can only generate an XML file with the some of the information I want; there is no option for coreference or generating the parse trees. To get it to correctly generate the sub-optimal XML files here, I had to write a script to get rid of all of the brackets within the files. This is the command I use:</p>

<blockquote>
  <p>java -cp ""*"" edu.stanford.nlp.parser.lexparser.LexicalizedParser -outputFormat typedDependenciesCollapsed,wordsAndTags -outputFilesExtension xml -outputFormatOptions xml -writeOutputFiles -outputFilesDirectory my\dir -tokenized -tagSeparator / -tokenizerFactory edu.stanford.nlp.process.WhitespaceTokenizer -tokenizerMethod newCoreLabelTokenizerFactory edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz my\wsj\files\dir</p>
</blockquote>

<p>I also can't generate the data I would like to have for the WSJ data that already has the trees. I tried using what is said <a href=""http://nlp.stanford.edu/software/parser-faq.shtml#s"" rel=""nofollow"">here</a> and I looked at the corresponding Javadocs. I used the command similar to what is described. But I had to write a python program to retrieve the stdout data resulting from analyzing each file and wrote it into a new file. This resulting data is only a text file with the dependencies and is not in the desired XML notation.</p>

<p>To summarize, I would like to use the POS and tree data from these PTB files in order to generate a CoreNLP parse corresponding to what would occur if I used CoreNLP on a regular text file. The pseudo command would be like this:</p>

<blockquote>
  <p>java -cp ""*"" edu.stanford.nlp.pipeline.CoreNLP -useTreeFile wsj_DDXX.mrg</p>
</blockquote>

<p>and</p>

<blockquote>
  <p>java -cp ""*"" edu.stanford.nlp.pipeline.CoreNLP -usePOSFile wsj_DDXX.pos</p>
</blockquote>

<p>Edit: fixed a link.</p>
"
"30210494","StanfordNLP lemmatization cannot handle -ing words","2015-05-13 09:16:59","2","1678","0","2","","30211095","<p>I've been experimenting with Stanford NLP toolkit and its lemmatization capabilities. I am surprised how it lemmatize some words. For example:</p>

<pre><code>depressing -&gt; depressing
depressed -&gt; depressed
depresses -&gt; depress
</code></pre>

<p>It is not able to transform <code>depressing</code> and <code>depressed</code> into the same lemma. Simmilar happens with <code>confusing</code> and <code>confused</code>, <code>hopelessly</code> and <code>hopeless</code>. I am getting the feeling that the only thing it is able to do is remove the <code>s</code> if the word is in such form (e.g. <code>feels -&gt; feel</code>). Is such behaviour normal for Lematizatiors in English? I would expect that they would be able to transform such variations of common words into a same lemma.</p>

<p>If this is normal, should I rather use stemmers? And, is there a way to use stemmers like Porter (Snowball, etc.) in StanfordNLP? There is no mention of stemmers in their documentation; however, there are some <code>CoreAnnotations.StemAnnotation</code> in the API. If not possible with StanfordNLP which stemmers do you recommend for use in Java?</p>
"
"30004939","Semantics - creating grammar in NLTK","2015-05-02 16:51:08","1","1002","1","1","","30126677","<p>I'm trying to expand NLTK's <a href=""https://github.com/nltk/nltk_teach/blob/master/examples/grammars/book_grammars/simple-sem.fcfg"" rel=""nofollow noreferrer"">simple-sem.fcfg</a> so that it supports coordination of phrases. I want it to successfully parse a sentence like: Irene walks and Angus barks. Since this is represented as walk(Irene) &amp; bark(Angus), I think the best way to achieve this is by adding a rule S -> S and S. What I can't grasp is how to do this with the semantic features... I've done this so far:</p>

<p>Added this rule:</p>

<pre><code>S[SEM = &lt;?subj(?vp)&gt;] -&gt; S[SEM = &lt;?subj(?vp)&gt;] &lt;&amp;&gt; S[SEM = &lt;?subj(?vp)&gt;]
</code></pre>

<p>This doesn't work, so is there anyone who has some advice/links etc?</p>
"
"29905761","CoreNLP API for N-grams?","2015-04-27 20:54:20","0","2441","0","2","","29924010","<p>Does CoreNLP have an API for getting unigrams, bigrams, trigrams, etc.?</p>

<p>For example, I have a string <code>""I have the best car ""</code>.  I would love to get:</p>

<pre><code>I
I have
the
the best
car
</code></pre>

<p>based on the string I am passing.</p>
"
"29848095","How do I group companies having different names but are essentially the same semantically?","2015-04-24 12:55:15","4","816","0","1","","29854869","<p>I am doing competitor analysis using Open Government Data from UK public sector. But there are some anomalies in my results. When I am grouping the contracts by the company names, there are a lot of issues like companies are misspelt or they vary in their names.e.g HP, Hewlett-Packard, Hewlett-Packard Limited , ibm, ibm UK, ibm UK limited etc. The thing is I already ran my code and fixed the results manually. Now I have changed some parts of the code and need to run it again. But I can't go back doing the same thing again as it's costly. At the moment I am thinking about writing a general rule that will sort these companies alphabetically, and merge them when they match on the first few words. But it's not a full-proof approach as HP and Hewlett-Packard will be different. Has anyone done any similar kind of work before or can reference me to their work please. I would be grateful. Thanks.</p>
"
"29807175","How to NER and POS tag a pre-tokenized text with Stanford CoreNLP?","2015-04-22 19:33:27","5","1690","0","2","","29807502","<p>I'm using the Stanford's CoreNLP Named Entity Recognizer (NER) and Part-of-Speech (POS) tagger in my application. The problem is that my code tokenizes the text beforehand and then I need to NER and POS tag each token. However I was only able to find out how to do that using the command line options but not programmatically.</p>

<p>Can someone please tell me how programmatically can I NER and POS tag pretokenized text using Stanford's CoreNLP?</p>

<p>Edit: </p>

<p>I'm actually using the individual NER and POS instructions. So my code was written as instructed in the tutorials given in the Stanford's NER and POS packages. But I have CoreNLP in my classpath. So I have the CoreNLP in my classpath but using the tutorials in the NER and POS packages. </p>

<p>Edit:</p>

<p>I just found that there are instructions as how one can set the properties for CoreNLP here <a href=""http://nlp.stanford.edu/software/corenlp.shtml"" rel=""noreferrer"">http://nlp.stanford.edu/software/corenlp.shtml</a> but I wish if there was a quick way to do what I want with Stanford NER and POS taggers so I don't have to recode everything!</p>
"
"29755910","Train model using Named entity","2015-04-20 18:43:00","2","2453","2","2","","29776406","<p>I am looking on standford corenlp using the Named Entity REcognizer.I have different kinds of input text and i need to tag it into my own Entity.So i  started training my own model and it doesnt seems to be working.</p>

<p>For eg: my input text string is ""Book of 49 Magazine Articles on Toyota Land Cruiser 1956-1987 Gold Portfolio <a href=""http://t.co/EqxmY1VmLg"">http://t.co/EqxmY1VmLg</a> <a href=""http://t.co/F0Vefuoj9Q"">http://t.co/F0Vefuoj9Q</a>""</p>

<p>I go through the examples to train my own models and and look for only some words that I am interested in.</p>

<p>My jane-austen-emma-ch1.tsv looks like this</p>

<pre><code>Toyota  PERS
Land Cruiser    PERS
</code></pre>

<p>From the above input text i am only interested in those two words. The one is 
Toyota and the other word is Land Cruiser.</p>

<p>The austin.prop look like this</p>

<pre><code>trainFile = jane-austen-emma-ch1.tsv
serializeTo = ner-model.ser.gz
map = word=0,answer=1
useClassFeature=true
useWord=true
useNGrams=true
noMidNGrams=true
useDisjunctive=true
maxNGramLeng=6
usePrev=true
useNext=true
useSequences=true
usePrevSequences=true
maxLeft=1
useTypeSeqs=true
useTypeSeqs2=true
useTypeySequences=true
wordShape=chris2useLC
</code></pre>

<p>Run the following command to generate the ner-model.ser.gz file </p>

<p>java -cp stanford-corenlp-3.4.1.jar edu.stanford.nlp.ie.crf.CRFClassifier -prop austen.prop</p>

<pre><code>public static void main(String[] args) {
        String serializedClassifier = ""edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz"";
        String serializedClassifier2 = ""C:/standford-ner/ner-model.ser.gz"";
        try {
            NERClassifierCombiner classifier = new NERClassifierCombiner(false, false, 
                    serializedClassifier2,serializedClassifier);
            String ss = ""Book of 49 Magazine Articles on Toyota Land Cruiser 1956-1987 Gold Portfolio http://t.co/EqxmY1VmLg http://t.co/F0Vefuoj9Q"";
            System.out.println(""---"");
            List&lt;List&lt;CoreLabel&gt;&gt; out = classifier.classify(ss);
            for (List&lt;CoreLabel&gt; sentence : out) {
              for (CoreLabel word : sentence) {
                System.out.print(word.word() + '/' + word.get(AnswerAnnotation.class) + ' ');
              }
              System.out.println();
            }

        } catch (ClassCastException e) {
            // TODO Auto-generated catch block
            e.printStackTrace();
        }  catch (Exception e) {
            // TODO Auto-generated catch block
            e.printStackTrace();
        }

    }
</code></pre>

<p>Here is the output I am getting</p>

<pre><code>Book/PERS of/PERS 49/O Magazine/PERS Articles/PERS on/O Toyota/PERS Land/PERS Cruiser/PERS 1956-1987/PERS Gold/O Portfolio/PERS http://t.co/EqxmY1VmLg/PERS http://t.co/F0Vefuoj9Q/PERS
</code></pre>

<p>which i think its wrong.I am looking for Toyota/PERS and Land Cruiser/PERS(Which is a multi valued fied.</p>

<p>Thanks for the Help.Any help is really appreciated.</p>
"
"29733476","I am having problems doing Word Sense Disambiguation in Python using Lesk algorithm","2015-04-19 17:42:26","0","1911","0","1","","29737079","<p>I am new to Python and NLTK so please bear with me. I wish to find the sense of a word in the context of a sentence. I am using the Lesk WSD algorithm but it is giving different outputs every time I run it. I know that Lesk has some level of inaccuracy. But, I think a POS tag will increase accuracy. </p>

<p>The Lesk algorithm takes a POS tag as an argument, but it takes 'n','s','v' as an input and not 'NN','VBP' or other POS tags which are outputted by the pos_tag() function. I would like to know how to tag words in the form of 'n','s','v', or if there is a method in which I can convert the 'NN','VBP' and other tags into 'n','s','v', so I can give them as an input to the lesk(context_sentence,word,pos_tag) function. </p>

<p>I am calculating the sentiment score of every word using SentiWordNet afterwards. </p>

<pre><code>    from nltk.wsd import lesk
    from nltk import word_tokenize
    import nltk, re, pprint
    from nltk.corpus import sentiwordnet as swn

    def word_sense():

        sent = word_tokenize(""He should be happy."")
        word = ""be""
        pos = ""v""
        score = lesk(sent,word,pos)
        print(score)
        print (str(score),type(score))
        set1 = re.findall(""'([^']*)'"",str(score))[0]
        print (set1)
        bank = swn.senti_synset(str(set1))
        print (bank)

    word_sense()
</code></pre>
"
"29721510","NLTK: how can I list all pairs of adjacent subtrees (rooted in specific nonterminal) of a parse tree","2015-04-18 18:58:42","4","184","0","1","","29896275","<p>How can I efficiently list all pairs of subtrees (rooted in specific nonterminal) of a parse tree? For example, I have the following tree:</p>

<blockquote>
  <p>(S (S (S (S (X (PRO pro))) (X (V v))) (X (ADJ adj))) (X (N n))) </p>
</blockquote>

<p>You can see the image on <a href=""https://i.sstatic.net/I9Tzk.png"" rel=""nofollow"">this link</a>.</p>

<p>I want to list all adjacent instances of the symbol X expanding to other symbols, i. e.:</p>

<ol>
<li><code>(X (PRO pro)) and (X (V v))</code></li>
<li><code>(X (V v)) and (X (ADJ adj))</code></li>
<li><code>(X (ADJ adj)) and (X (N n))</code></li>
</ol>
"
"29676112","Python: Goslate translation request returns ""503: Service Unavailable""","2015-04-16 13:24:50","5","3890","0","1","","29680029","<p>A few months ago, I used Python's <code>goslate</code> package to translate a bunch of French text to English. When I tried to do so this morning, though, the service returned an error:</p>

<pre><code>import goslate
gs = goslate.Goslate()
print gs.translate('hello world', 'de')

Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""c:\Python27\lib\site-packages\goslate.py"", line 389, in translate
    return _unwrapper_single_element(self._translate_single_text(text, target_language, source_language))
  File ""c:\Python27\lib\site-packages\goslate.py"", line 317, in _translate_single_text
    results = list(self._execute(make_task(i) for i in split_text(text)))
  File ""c:\Python27\lib\site-packages\goslate.py"", line 200, in _execute
    yield each()
  File ""c:\Python27\lib\site-packages\goslate.py"", line 315, in &lt;lambda&gt;
    return lambda: self._basic_translate(text, target_language, source_lauguage)[0]
  File ""c:\Python27\lib\site-packages\goslate.py"", line 241, in _basic_translate
    response_content = self._open_url(url)
  File ""c:\Python27\lib\site-packages\goslate.py"", line 178, in _open_url
    response = self._opener.open(request, timeout=self._TIMEOUT)
  File ""c:\Python27\lib\urllib2.py"", line 437, in open
    response = meth(req, response)
  File ""c:\Python27\lib\urllib2.py"", line 550, in http_response
    'http', request, response, code, msg, hdrs)
  File ""c:\Python27\lib\urllib2.py"", line 469, in error
    result = self._call_chain(*args)
  File ""c:\Python27\lib\urllib2.py"", line 409, in _call_chain
    result = func(*args)
  File ""c:\Python27\lib\urllib2.py"", line 656, in http_error_302
    return self.parent.open(new, timeout=req.timeout)
  File ""c:\Python27\lib\urllib2.py"", line 437, in open
    response = meth(req, response)
  File ""c:\Python27\lib\urllib2.py"", line 550, in http_response
    'http', request, response, code, msg, hdrs)
  File ""c:\Python27\lib\urllib2.py"", line 475, in error
    return self._call_chain(*args)
  File ""c:\Python27\lib\urllib2.py"", line 409, in _call_chain
    result = func(*args)
  File ""c:\Python27\lib\urllib2.py"", line 558, in http_error_default
    raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)
urllib2.HTTPError: HTTP Error 503: Service Unavailable
</code></pre>

<p>Does anyone know what happened to <code>goslate</code>? If it's gone for good, are there decent alternatives to the <code>goslate</code> package for translating French to English via an API call?</p>
"
"29674389","The inconsistency between the Parser in CoreNLP and the standalone Stanford Parser","2015-04-16 12:13:02","1","147","1","1","","29683827","<p>Just found an inconsistency issue that the parsing results of the Parser in CoreNLP and the standalone Stanford Parser are different. </p>

<p>For example, given a sentence ""Microsoft released Windows 10."". </p>

<p>The Parser in CoreNLP (<a href=""http://nlp.stanford.edu:8080/corenlp/process"" rel=""nofollow noreferrer"">http://nlp.stanford.edu:8080/corenlp/process</a>) will give the following result:
<img src=""https://i.sstatic.net/P2O5V.png"" alt=""enter image description here"">
​
However, the standalone Stanford Parser (<a href=""http://nlp.stanford.edu:8080/parser/index.jsp"" rel=""nofollow noreferrer"">http://nlp.stanford.edu:8080/parser/index.jsp</a>) will give the following result:
<img src=""https://i.sstatic.net/KIpP6.png"" alt=""enter image description here""></p>

<p>​I also tried to run the codes on my machines. Both the parsers used the same model trained on the same date (englishPCFG.ser.gz, 2015-01-29). But the results given by the two parsers are still different. I tried several other sentences, and it looks that the standalone parser gives better results. </p>

<p>Anyone has idea on this?</p>
"
"29575034","Is there a Python wrapper for Stanford Neural Net based dependency parser?","2015-04-11 06:52:08","1","209","0","1","","29593497","<p>I know about the Python wrappers for Stanford CoreNLP package but this package does not seem to contain neural net based dependency parser model. Rather it is present in Stanford-parser-full-****-<strong>-</strong> package for which I can't find any Python wrapper. <strong>My Question: Is there a Python wrapper that would parse using Stanford Neural Net based dependency parser?</strong> Any suggestions or directions would be helpful. Thanks!</p>
"
"29571927","Find semantically similar word for natural language processing","2015-04-10 22:58:41","1","1499","0","1","","29572279","<p>I am working on a natural language processing project in Java. I have a requirement where I want to identify words that belong to similar semantic groups. 
e.g. : if the words such as <code>study</code> , <code>university</code>, <code>graduate</code> , <code>attend</code> are found I want them to be categorized as being related to education.
If words such as <code>golfer</code>, <code>batsman</code>, <code>athlete</code> are found, it should categorize all under a parent like sportsperson.
Is there a way I can achieve this task without using and training approach. Is there some toll like WordNet that can be used directly? Any pointer would be greatly appreciated! 
Thanx cheers!! :-) </p>
"
"29570207","Does NLTK have TF-IDF implemented?","2015-04-10 20:34:16","9","28840","3","2","","29570477","<p>There are TF-IDF implementations in <code>scikit-learn</code> and <code>gensim</code>. </p>

<p>There are simple implementations <a href=""https://stackoverflow.com/questions/2380394/simple-implementation-of-n-gram-tf-idf-and-cosine-similarity-in-python/22577329#22577329"">Simple implementation of N-Gram, tf-idf and Cosine similarity in Python</a></p>

<p>To avoid reinventing the wheel, </p>

<ul>
<li><strong>Is there really no TF-IDF in NLTK?</strong> </li>
<li><strong>Are there sub-packages that we can manipulate to implement TF-IDF in NLTK? If there are how?</strong></li>
</ul>

<p>In this blogpost, it says NLTK doesn't have it. <strong>Is that true?</strong> <a href=""http://www.bogotobogo.com/python/NLTK/tf_idf_with_scikit-learn_NLTK.php"" rel=""noreferrer"">http://www.bogotobogo.com/python/NLTK/tf_idf_with_scikit-learn_NLTK.php</a></p>
"
"29562798","Assign a short text to one of two categories according to previous assignments (votes)","2015-04-10 13:31:59","0","129","0","2","","29570406","<p>There is a stream of short texts. Each one has the size of a tweet, or let us just assume they are all tweets.</p>

<p>The user can vote on any tweet. So, each tweet has one of the following three states:</p>

<p>relevant (positive vote)</p>

<p>default (neutral i.e. no vote)</p>

<p>irrelevant (negative vote)</p>

<p>Whenever a new set of tweets come, they will be displayed in a specific order. This order is determined by the votes of the user on all previous tweets. The aim is to assign a score to each new tweet. This score is calculated based on the word similarity or match between the text of this tweet and all the previous tweets voted by the user. In other words, the tweet with the highest score is going to be the one which contains the maximum number of words voted previously positive and the minimum of words voted previously as negative. Also, the new tweets having a high score will trigger a notification to the user as they are considered very relevant.</p>

<p>One last thing, a minimum of semantic consideration (natural language processing) would be great. </p>

<p>I have read about <strong>Term Frequency–Inverse Document Frequency</strong> and come up with this very simple and basic solution:</p>

<p>Reminder: a high weight in tf–idf is reached by a high word frequency and a low total frequency of the word in the whole collection.</p>

<p>If the user votes positive on a Tweet, all the words of this tweet will receive a positive point (same thing for the negative case). This means that we will have a large set of words where each word has the total number of positive points and negative points.    </p>

<p>If (Tweet score > 0) then this tweet will trigger a notification.</p>

<p>Tweet score = sum of all individual words’ scores of this tweet</p>

<p>word score = word frequency * inverse total frequency</p>

<p>word frequency in all previous votes = ( total <strong>positive</strong> votes for this word - total <strong>negative</strong> votes for this word) / <strong>total</strong> votes for this word</p>

<p>Inverse total frequency = log ( total votes of <strong>all</strong> words / total votes for <strong>this</strong> word)</p>

<p>Is this method enough? I am open to any better methods and any ready API or algorithm.</p>
"
"29495556","What tools can I use to find Part Of Speech Patterns","2015-04-07 15:40:58","2","968","1","2","","29510341","<p>I am looking for tools to find Part Of Speech patterns on a corpus of documents. I am using the Stanford NLP tools for POS tagging my documents. Now I would like to query these tagged documents and find some specific POS patterns such as for example</p>

<p><strong>NP</strong> is <strong>JJ</strong>  (ex:  the movie is nice)</p>

<p>or <strong>JJ</strong> <strong>NP</strong>  (ex : excellent foie gras) </p>

<p>Is there a tool that can do this for me in a simple and efficient manner or do I need to write my own ? </p>
"
"29439545","Python: How can I fetch passages containing given keywords from a text file","2015-04-03 20:54:53","1","2154","4","1","","29451660","<p>I am trying to implement a factoid based question answering system. So far I have retrieved candidate text documents which may contain answers.
But now I am stuck where I have to extract right passages from the documents based on the keywords provided.</p>

<p>I have briefly studied approaches like LCC and InsightSoft, but cannot figure out how to move forward.</p>

<p>Consider that I have a document containing many paragraphs(passages).I want to rank these paragraphs based on certain keywords.</p>

<p><strong>Example:</strong></p>

<p>Keywords- <em>leopard</em>, <em>lion</em></p>

<p>Para 1: ""..no sentence about <em>leopard</em> or <em>leopard</em>..""""</p>

<p>Para 2:""..few sentences about <em>lion</em>..""</p>

<p>Para 3:""..sentences about both <em>lion</em> and <em>leopard</em>..""</p>

<p><strong>Goal</strong>: To rank(or fetch) Para 2 and Para 3 </p>

<p>How can I implement(program) the same? 
And is there a way to rank these passages based on the tag provided to the keywords by a POS tagger?  </p>

<p>Any code or implementation of existing algorithms will be appreciated. Please be elaborate while explaining as I'm not an expert in Python.</p>
"
"29397708","Tagging a single word with the nltk pos tagger tags each letter instead of the word","2015-04-01 18:04:18","7","12632","1","4","","29397935","<p>I'm try to tag a single word with the nltk pos tagger:</p>

<pre><code>word = ""going""
pos = nltk.pos_tag(word)
print pos
</code></pre>

<p>But the output is this:</p>

<pre><code>[('g', 'NN'), ('o', 'VBD'), ('i', 'PRP'), ('n', 'VBP'), ('g', 'JJ')]
</code></pre>

<p>It's tagging each letter rather than just the one word.</p>

<p>What can I do to make it tag the word?</p>
"
"29332851","What does NN VBD IN DT NNS RB means in NLTK?","2015-03-29 18:08:45","34","32417","0","4","","29333217","<p>when I chunk text, I get lots of codes in the output like
<code>NN, VBD, IN, DT, NNS, RB</code>.
Is there a list documented somewhere which tells me the meaning of these? 
I have tried googling <code>nltk chunk code</code> <code>nltk chunk grammar</code> <code>nltk chunk tokens</code>.</p>

<p>But I am not able to find any documentation which explains what these codes mean.</p>
"
"29302518","NLTK CFG Grammar with multiple words","2015-03-27 14:02:15","1","1112","0","1","","29327002","<p><strong>NLTK 3.0:</strong></p>

<p>With a CFG configuration like below (Nonterminal team has 4 values with 1 value that has 2 words (sri lankan).</p>

<p>When I generate the list of possible generations, I can see the two worded coming up in the result. But when I try to parse an input sentence with that two worded grammar, it does not parse.</p>

<pre><code>import nltk
from nltk.parse import generate
from nltk.grammar import Nonterminal


cfg = nltk.CFG.fromstring(""""""
root -&gt; who_player has the most runs
who_player -&gt; who
who_player -&gt; which player
who_player -&gt; which team player
who -&gt; 'who'
which -&gt; 'which'
player -&gt; 'player'
team -&gt; 'indian' | 'australian' | 'england' | 'sri lankan'
has -&gt; 'has'
the -&gt; 'the'
this -&gt; 'this'
most -&gt; 'most'
runs -&gt; 'runs'
"""""")

print(list((n,sent) for n, sent in enumerate(generate.generate(cfg, n=100, start=Nonterminal('root')), 1)))

# Above generate generates ['which', 'sri lankan', 'player', 'has', 'the', 'most', 'runs']
# But the same sentence is not parsable by ChartParser.

result1 = nltk.ChartParser(cfg).parse('which england player has the most runs'.split())
print(list(result1))
result2 = nltk.ChartParser(cfg).parse('which sri lankan player has the most runs'.split()) # Does not work.
print(list(result2))
</code></pre>

<p>How to make multi worded configuration work with ChartParser.</p>
"
"29230623","How can I generate a bracketed tree string in NLTK from a list of nodes and their children","2015-03-24 10:50:11","1","761","2","1","","29275814","<p>The NLTK documentation has plenty of examples of constructing a tree by parsing a bracketed string. Is there an easy way to construct the tree from a list of parents and their children? I want to be able to generate the bracketed string as the output.</p>

<pre><code>parent : children 
14_fine : 12_the,13_nasd
8_allegations : 7_the
10_consented : 11_to
4_admitted : 2_morgan,3_neither,15_.,6_denied,5_nor
11_to : 14_fine
6_denied : 8_allegations,9_but,10_consented
2_morgan : 1_j.p.
</code></pre>
"
"29213886","Malt parser, iterate over parsed tree in java","2015-03-23 15:17:52","0","156","0","1","","29333165","<p>I am trying to extract the tree dependencies from a Malt ConcurrentMaltParserModel. I iterate over the edges like: </p>

<pre><code>SortedSet&lt;ConcurrentDependencyEdge&gt; edges = graph.getEdges();
for (ConcurrentDependencyEdge e : edges) {
      //here I would need to extract the dependency type
}
</code></pre>

<p>I thought I could extract the dependency type in a similar way as StanfordParser, but unfortunately I cant figure out how to do that. </p>
"
"29182611","Does MaltParser actually provide an option for returning probabilities of Parse trees?","2015-03-21 12:16:58","1","109","0","1","","40002540","<p>While looking at the source code of Malt Parser which actually has class LibLinear.java(jar file) and calls the java version of the liblinear toolkit; I don't find any option/way to return probability despite the information that, in principle training the model using liblinear(by default in malt parser) with Logistic regression(-s 0) should produce probability score of parsed trees.</p>

<p>The main concern is: Do the integration of Liblinear and Malt Parser working smoothly without affecting each other expected operations?</p>

<p>Working separately with Liblinear does give me probability output for the datasets.</p>

<pre><code>liblinear-train -s 0 train_scale 
</code></pre>

<p>//training data using logistic regression model</p>

<pre><code>liblinear-predict -b 1 test_scale train_scale.model test_scale_output 
</code></pre>

<p>//labels and classes and probability outputs.  Here -b 1 does extract out probabilities of each datasets.</p>

<p>Reference: <a href=""https://stackoverflow.com/questions/28791352/how-to-get-probability-score-of-parsed-sentences-using-malt-parser"">https://stackoverflow.com/questions/28791352/how-to-get-probability-score-of-parsed-sentences-using-malt-parser</a></p>
"
"29151329","Arabic lemmatization and Stanford NLP","2015-03-19 17:33:54","5","5167","2","2","","29219800","<p>I try to make lemmatization, ie identifying the lemma and possibly the Arabic root of a verb, for example:
يتصل ==> lemma (infinitive of the verb) ==> اتصل ==> root (triliteral root / Jidr thoulathi)
==> و ص ل</p>

<p>Do you think Stanford NLP can do that?</p>

<p>Best Regards,</p>
"
"29142230","Tagger for single words in NLTK","2015-03-19 10:33:17","1","1214","0","1","","29146108","<p><strong>Is there a tagger that would return a single tag for a word in whatever context it maybe?</strong></p>

<p>My requirement is that I need to extract words from unstructured text where the sentences would not have a structured grammar.</p>

<p>POS taggers are meant to work with sentences and would return a tag for a word depending on the context of the word in that sentence. So, I would either have to use another tagger that would give me the same tag for a particular word each time or use all the possible tags for a word while chunking. </p>

<p>Any other solutions would be greatly appreciated. Also, how can you view all the tags that can be assigned for a particular word? </p>
"
"29131332","Add conjunction to grammar rule - NLTK parse into syntax tree in python","2015-03-18 19:44:45","0","1570","0","1","","29132972","<p>Suppose  we  have following incomplete  grammar rule:</p>

<pre><code>grammar2 = nltk.parse_cfg(""""""
S -&gt; NP VP
NP -&gt; Det N
VP -&gt; V NP
PN -&gt; 'David' 
Det -&gt; 'the'
N -&gt; 'man' 
V -&gt; 'saw' | 'helped' 
Pro -&gt; 'him'
Conj -&gt; 'and'
"""""")
</code></pre>

<p>I want to create syntax tree for following sentence:</p>

<pre><code>sent = ['the', 'man', 'saw', 'David', 'and', 'helped', 'him']
parser = nltk.ChartParser(grammar2)
trees = parser.nbest_parse(sent)
for tree in trees:
    print tree
</code></pre>

<p>But I don't know how can I add conjunction <code>and</code> in the grammar?</p>
"
"29057037","NLTK fcfg sem value is awkward","2015-03-15 03:50:11","0","251","0","2","","29091900","<p>My FCFG that I used for this sentence was</p>

<pre><code>S[SEM=&lt;?vp(?np)&gt;] -&gt; NP[NUM=?n, SEM=?np] VP[NUM=?n,SEM=?vp] 
VP[NUM=?n,SEM=&lt;?v(?obj)&gt;] -&gt; TV[NUM=?n,SEM=?v] DET NP[SEM=?obj
NP[NUM=?n, SEM=?np] -&gt; N[NUM=?n, SEM=?np] 
N[NUM=sg, SEM=&lt;\P.P(I)&gt;] -&gt; 'I'
TV[NUM=sg,SEM=&lt;\x y.(run(y,x))&gt;] -&gt; 'run'
DET -&gt; ""a""
N[NUM=sg, SEM=&lt;\P.P(race)&gt;] -&gt; 'race'
</code></pre>

<p>I want to parse out the sentence ""I run a race"" and when I used that sentence</p>

<pre><code>sent = 'I run a race'
parser = load_parser('grammar.fcfg')

for tree in parser.parse(sent.split()):
    print (tree)
</code></pre>

<p>It returns a really awkward looking phrase for the parsed sentence</p>

<pre><code>S[SEM=&lt;run(\P.P(I3),\P.P(race))&gt;]
</code></pre>

<p>However I wanted the code to return </p>

<pre><code>S[SEM=&lt;run(I,race)&gt;]
</code></pre>

<p>How do I get rid of the <code>\P.P</code> which shouldn't belong there?</p>
"
"29007478","How to define a CAS in database as external resource for an annotator in uimaFIT?","2015-03-12 10:37:56","0","137","2","1","","29026564","<p>I am trying to structure my a data processing pipeline using uimaFit as follows:</p>

<p><code>[annotatorA]</code> => <code>[Consumer to dump annotatorA's annotations from CAS into DB]</code> </p>

<p><code>[annotatorB (should take on annotatorA's annotations from DB as input)]</code>=><code>[Consumer for annotatorB]</code></p>

<p>The driver code:</p>

<pre><code>   /* Step 0: Create a reader */
    CollectionReader readerInstance= CollectionReaderFactory.createCollectionReader(
            FilePathReader.class, typeSystem,
            FilePathReader.PARAM_INPUT_FILE,""/path/to/file/to/be/processed"");

   /*Step1: Define Annotoator A*/
    AnalysisEngineDescription annotatorAInstance=
           AnalysisEngineFactory.createPrimitiveDescription(
                    annotatorADbConsumer.class, typeSystem, 
                    annotatorADbConsumer.PARAM_DB_URL,""localhost"",
                    annotatorADbConsumer.PARAM_DB_NAME,""xyz"",
                    annotatorADbConsumer.PARAM_DB_USER_NAME,""name"",
                    annotatorADbConsumer.PARAM_DB_USER_PWD,""pw"");
    builder.add(annotatorAInstance);

    /* Step2: Define binding for annotatorB to take 
         what-annotator-a put in DB above as input */

    /*Step 3: Define annotator B */
    AnalysisEngineDescription annotatorBInstance =
            AnalysisEngineFactory.createPrimitiveDescription(
                    GateDateTimeLengthAnnotator.class,typeSystem)
    builder.add(annotatorBInstance);

    /*Step 4: Run the pipeline*/
    SimplePipeline.runPipeline(readerInstance, builder.createAggregate());
</code></pre>

<p>Questions I have are:</p>

<ol>
<li>Is the above approach correct?</li>
<li>How do we define the depencdency of annotatorA's output in annotatorB in step 2?</li>
</ol>

<p>Is the approach suggested at <a href=""https://code.google.com/p/uimafit/wiki/ExternalResources#Resource_injection"" rel=""nofollow"">https://code.google.com/p/uimafit/wiki/ExternalResources#Resource_injection</a>
, the right direction to achieve it ?</p>
"
"28938999","How to Break a sentence into a few words","2015-03-09 09:49:20","-2","261","1","2","","28951122","<p>I want to ask how to break a sentence into a few words, what this is using of NLP (Natural Language Processing) in python called NLTK or PARSER ? on python i confused with the method, what method should i take in my case.</p>
"
"28925194","Python 2.7: Lesk algorithm returns None","2015-03-08 10:00:01","0","314","0","1","","28985629","<p>I am creating a program that will disamiguate ambiguos words and I was using nltk. Now, when I came to the stage to use lesk algorithm I am having some trouble.</p>

<p>For example, if I try:</p>

<pre><code>c = lesk('There sign bothered consider inverse logic namely mental illness   substance abuse might degree consequence rather cause homelessness ','consider')
</code></pre>

<p>c will be None, which means that algorithm will return none.</p>

<p>I tried to give in place of sentence a list of word: i.e:</p>

<pre><code>sent = word_tokenize('There sign bothered consider inverse logic namely mental illness substance abuse might degree consequence rather cause homelessness ')
c = lesk(sent, 'consider')
</code></pre>

<p>or even list of lemmatas instead of full words, but it still returns None.</p>

<p>Does anyone know if this is a feature of lesk (when it cannot disambiguate the word to return None), or am I doing something wrong? Also if it is a feature, then can it be removed (to return me a word instead of None)?</p>

<p>Thanks!</p>
"
"28883234","Stanford NLP: Chinese Part of Speech labels?","2015-03-05 16:49:02","0","765","0","1","","28884016","<p>I am trying to find a table explaining each label in the Chinese part-of-speech tagger for the 2015.1.30 version.  I couldn't find anything on this topic.  The closest thing I could find was in the ""Morphological features help POS tagging of unknown words across language varieties"" article, but it doesn't explain what VC represent.  I would love to get an updated list.</p>
"
"28881999","TF - IDF vs only IDF","2015-03-05 15:48:32","0","2277","4","1","","28889482","<p>Is there any case when IDF is better than TF-IDF? As far I understood TF is important to give a weight to a word within a document to match that document with a predefined query. If I'd like just to sort the importance of words in a collection of documents without any specific IR purpose, why should I use the TF term?</p>
"
"28859234","Not able to tag hindi sentence properly","2015-03-04 16:08:59","2","1688","0","1","","29629542","<p>I have recently started a project on Hindi data processing. I have tried executing certain below code but have not got the expected output. </p>

<pre><code>    e = u""पूर्ण प्रतिबंध हटाओ : इराक""
    tokens=nltk.word_tokenize(e)
    from nltk import pos_tag
    print tokens
    tag = nltk.pos_tag(tokens)
    print tag
</code></pre>

<p>The output I have obtained is shown below:</p>

<pre><code>[u'\u092a\u0942\u0930\u094d\u0923', u'\u092a\u094d\u0930\u0924\u093f\u092c\u0902\u0927', u'\u0939\u091f\u093e\u0913', u':', u'\u0907\u0930\u093e\u0915']
[(u'\u092a\u0942\u0930\u094d\u0923', 'NN'), (u'\u092a\u094d\u0930\u0924\u093f\u092c\u0902\u0927', '``'), (u'\u0939\u091f\u093e\u0913', ':'), (u':', ':'), (u'\u0907\u0930\u093e\u0915', ':')]
</code></pre>

<p>My query is  tagging the first word of my input as noun and rest are tagged incorrectly. The same query gives a correct output for English data.</p>

<p>What is it that I'm doing wrong? Is there any specific function that I have to use for tagging d Hindi data.</p>

<p>Thank you for your help.</p>
"
"28791328","NLTK POS tagset help not working","2015-03-01 07:51:32","1","1498","0","2","","28791364","<p>I downloaded nltk tagset help is not working.</p>

<p>Whenever I try to access tagset meanings by:-</p>

<pre><code>nltk.help.upenn_tagset('NN')
</code></pre>

<p>I get result as :-</p>

<pre><code>Traceback (most recent call last):
File ""&lt;pyshell#30&gt;"", line 1, in &lt;module&gt;
nltk.help.upenn_tagset('NN')
File ""C:\Python34\lib\site-packages\nltk\help.py"", line 25, in upenn_tagset
_format_tagset(""upenn_tagset"", tagpattern)
File ""C:\Python34\lib\site-packages\nltk\help.py"", line 39, in _format_tagset
tagdict = load(""help/tagsets/"" + tagset + "".pickle"")
File ""C:\Python34\lib\site-packages\nltk\data.py"", line 774, in load
opened_resource = _open(resource_url)
File ""C:\Python34\lib\site-packages\nltk\data.py"", line 888, in _open
return find(path_, path + ['']).open()
File ""C:\Python34\lib\site-packages\nltk\data.py"", line 618, in find
raise LookupError(resource_not_found)
LookupError: 
**********************************************************************
Resource 'help/tagsets/upenn_tagset.pickle' not found.  Please
use the NLTK Downloader to obtain the resource:  &gt;&gt;&gt;
nltk.download()
Searched in:
- 'C:\\Users\\aarushi/nltk_data'
- 'C:\\nltk_data'
- 'D:\\nltk_data'
- 'E:\\nltk_data'
- 'C:\\Python34\\nltk_data'
- 'C:\\Python34\\lib\\nltk_data'
- 'C:\\Users\\aarushi\\AppData\\Roaming\\nltk_data'
</code></pre>

<p>But I already downloaded tagset from models tab by nltk.download()</p>

<p>So what am I doing wrong here?</p>
"
"28782336","Annotating a treebank with lexical information (Head Words) in JAVA","2015-02-28 13:38:50","0","179","1","2","","28785134","<p>I have a treebank with syntactic parse tree for each sentence as given below:</p>

<p><strong>(S (NP (DT The) (NN government)) (VP (VBZ charges) (SBAR (IN that) (S (PP (IN between) (NP (NNP July) (CD 1971)) (CC and) (NP (NNP July) (CD 1992))) (, ,) (NP (NNP Rostenkowski)) (VP (VBD placed) (NP (CD 14) (NNS people)) (PP (IN on) (NP (NP (PRP$ his) (JJ congressional) (NN payroll)) (SBAR (WHNP (WP who)) (S (VP (VBD performed) (NP (NP (JJ personal) (NNS services)) (PP (IN for) (NP (NP (PRP him)) (CC and) (NP (PRP$ his) (NN family))))))))))))))</strong></p>

<p>I want to annotate the parse tree with lexical information like <strong>headwords</strong> for each node in the parse tree.</p>

<p>Can I do that using StanfordCoreNLP? Please guide me in the right direction. I would prefer a solution that can be implemented in JAVA as I am familiar with JAVA.</p>

<p>Thanks a lot!</p>
"
"28774623","NLTK: Parsing sentences using a simple grammar and part of speech tags","2015-02-27 21:29:01","1","1766","0","1","","28780913","<p>For a sentence like ""This is a simple sentence"" which has been part of speech tagged to:</p>

<pre><code>[('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('simple', 'JJ'), ('sentence', 'NN')]
</code></pre>

<p>And using the following grammar:</p>

<pre><code>    my_grammar = nltk.CFG.fromstring(""""""
... S -&gt; DP VP
... DP -&gt; Det NP
... NP -&gt; Adj N
... VP -&gt; V DP
... Det -&gt; 'DT'
... N -&gt; 'NN'
... V -&gt; 'VBZ'
... Adj -&gt; 'JJ'
... """""")
</code></pre>

<p>How do I output a tree structure. I am attempting to use the part of speech tags in place of the word that has been tagged, but still display the word itself in the tree to get something similar to:</p>

<pre><code>    (S
     (DP This)
     (VP
      (V is)
      (DP (Det a)
       (NP
        (Adj simple) (N sentence))))
</code></pre>

<p>EDIT: I've tried the answer <a href=""https://stackoverflow.com/questions/4858467/combining-a-tokenizer-into-a-grammar-and-parser-with-nltk?rq=1"">here</a>, but when I get to using the command:</p>

<pre><code>for tree in parser.parse(pos_tags):
...     print(tree)
</code></pre>

<p>Nothing is returned</p>
"
"28674417","How to read constituency based parse tree","2015-02-23 13:00:45","6","4467","0","3","","28674667","<p>I have a corpus of sentences that were preprocessed by Stanford's <a href=""http://nlp.stanford.edu/software/corenlp.shtml"" rel=""noreferrer"">CoreNLP</a> systems. One of the things it provides is the sentence's Parse Tree (Constituency-based). While I can understand a parse tree when it's drawn (like a tree), I'm not sure how to read it in this format:</p>

<p>E.g.:</p>

<pre><code>          (ROOT
          (FRAG
          (NP (NN sent28))
          (: :)
          (S
          (NP (NNP Rome))
          (VP (VBZ is)
          (PP (IN in)
          (NP
          (NP (NNP Lazio) (NN province))
          (CC and)
          (NP
          (NP (NNP Naples))
          (PP (IN in)
          (NP (NNP Campania))))))))
          (. .)))
</code></pre>

<p>The original sentence is:</p>

<pre><code>sent28: Rome is in Lazio province and Naples in Campania .
</code></pre>

<p>How am I supposed to read this tree, or alternatively, is there a code (in python) that does it properly?
Thanks.</p>
"
"28522106","FCFG error in NLTK, Python. Grammar Issue","2015-02-15 01:18:09","1","825","2","1","","28525687","<p>A line in a feature-based context free grammar I am writing in Python using NLTK gives me the following error. </p>

<pre><code>Error parsing feature structure
ADJ[SEM=&lt;\x.x(\y.(some(y))&gt;] -&gt; 'some'
         ^ Expected logic expression
</code></pre>

<p>I thought the expression after <code>SEM=</code> was a logic expression. </p>
"
"28469094","Why did PortStemmer in NLTK converts my ""string"" into u""string""","2015-02-12 04:04:54","0","1765","2","1","","28487605","<pre><code>import nltk
import string
from nltk.corpus import stopwords


from collections import Counter

def get_tokens():
    with open('comet_interest.xml','r') as bookmark:
        text=bookmark.read()
        lowers=text.lower()

        no_punctuation=lowers.translate(None,string.punctuation)
        tokens=nltk.word_tokenize(no_punctuation)
        return tokens
#remove stopwords
tokens=get_tokens()
filtered = [w for w in tokens if not w in stopwords.words('english')]
count = Counter(filtered)
print count.most_common(10)

#stemming
from nltk.stem.porter import *

def stem_tokens(tokens, stemmer):
    stemmed = []
    for item in tokens:
        stemmed.append(stemmer.stem(item))
    return stemmed

stemmer = PorterStemmer()
stemmed = stem_tokens(filtered, stemmer)
count = Counter(stemmed)
print count.most_common(10)
</code></pre>

<p>The results show like these:</p>

<p>[('analysis', 13), ('spatial', 11), ('feb', 8), ('cdata', 8), ('description', 7), ('item', 6), ('many', 6), ('pm', 6), ('link', 6), ('research', 5)]</p>

<p>[(u'analysi', 13), (u'spatial', 11), (u'use', 11), (u'feb', 8), (u'cdata', 8), (u'scienc', 7), (u'descript', 7), (u'item', 6), (u'includ', 6), (u'mani', 6)]</p>

<p><strong>what's the problem with second one of stemming, why every words has an ""u"" head?</strong></p>
"
"28439522","Is it possible to get a natural word after it has been stemmed?","2015-02-10 18:52:49","3","427","0","2","","28447478","<p>I have a word <strong><em>play</em></strong> which after stemming has become <strong><em>plai</em></strong>. Now I want to get <strong><em>play</em></strong> again. Is it possible? I have used Porter's Stemmer.</p>
"
"28360402","Unknown symbol in nltk pos tagging for Arabic","2015-02-06 07:10:22","0","1476","2","1","","28361181","<p>I have used nltk to tokenize some arabic text </p>

<p>However, i ended up with some results like </p>

<p>(u'an arabic character/word', '``')
or 
(u'an arabic character/word', ':')</p>

<p>However, they do not provide the `` or : in the documentation. </p>

<p>hence i would like to find out what is this </p>

<pre><code>from nltk.toeknize.punkt import PunktWordTokenizer 

z = ""أنا تسلق شجرة""
tkn = PunkWordTokenizer
sen = tkn.tokenize(z)
tokens = nltk.pos_tag(sent)

print tokens
</code></pre>
"
"28341007","Cutting down on Stanford parser's time-to-parse by pruning the sentence","2015-02-05 09:59:40","2","141","1","1","","28377687","<p>We are already aware that the parsing time of Stanford Parser increases as the length of a sentence increases. I am interested in finding creative ways in which we prune the sentence such that the parsing time decreases without compromising on accuracy. For e.g. we can replace known noun phrases with one word nouns. Similarly can there be some other smart ways of guessing a subtree before hand, let's say, using the POS Tag information? We have a huge corpus of unstructured text at our disposal. So we wish to learn some common patterns that can ultimately reduce the parsing time. Also some references to publicly available literature in this regards will also be highly appreciated.</p>

<p>P.S. We already are aware of how to multi-thread using Stanford Parser, so we are not looking for answers from that point of view.</p>
"
"28328372","Why isn't the token_pattern parameter in Tfidfvectorizer working with scikit learn?","2015-02-04 18:09:45","1","7467","1","2","","28334552","<p>I have this text:</p>

<pre><code>data = ['Hi, this is XYZ and XYZABC is $$running']
</code></pre>

<p>I am using the following tfidfvectorizer:</p>

<pre><code>vectorizer = TfidfVectorizer(
            stop_words='english',
            use_idf=False, 
            norm=None,
            min_df=1,
            tokenizer = tokenize,
            ngram_range=(1, 1),
            token_pattern=u'\w{4,}')
</code></pre>

<p>I am fitting the data as follows:</p>

<pre><code>tdm =vectorizer.fit_transform(data)
</code></pre>

<p>Now, when I print </p>

<pre><code>vectorizer.get_feature_names()
</code></pre>

<p>I get this:</p>

<pre><code>[u'hi', u'run', u'thi', u'xyz', u'xyzabc']
</code></pre>

<p>My question is why am I getting 'hi' and 'xyz' even thought I mentioned that I want it to capture only words that have at least 4 characters? - token_pattern=u'\w{4,}'</p>
"
"28214148","How to perform Lemmatization in R?","2015-01-29 11:55:16","31","41220","6","6","","28318683","<p>This question is a possible duplicate of <strong><a href=""https://stackoverflow.com/questions/22993796/lemmatizer-in-r-or-python-am-are-is-be"">Lemmatizer in R or python (am, are, is -> be?)</a></strong>, but I'm adding it again since the previous one was closed saying it was too broad and the only answer it has is not efficient (as it accesses an external website for this, which is too slow as I have very large corpus to find the lemmas for). So a part of this question will be similar to the above mentioned question.</p>

<p>According to Wikipedia, lemmatization is defined as:</p>

<blockquote>
  <p>Lemmatisation (or lemmatization) in linguistics, is the process of grouping together the different inflected forms of a word so they can be analysed as a single item.</p>
</blockquote>

<p>A simple Google search for lemmatization in R will <em>only</em> point to the package <code>wordnet</code> of R. When I tried this package expecting that a character vector <code>c(""run"", ""ran"", ""running"")</code> input to the lemmatization function would result in <code>c(""run"", ""run"", ""run"")</code>, I saw that this package only provides functionality similar to <code>grepl</code> function through various filter names and a dictionary.</p>

<p>An example code from <code>wordnet</code> package, which gives maximum of 5 words starting with ""car"", as the filter name explains itself:</p>

<pre><code>filter &lt;- getTermFilter(""StartsWithFilter"", ""car"", TRUE)
terms &lt;- getIndexTerms(""NOUN"", 5, filter)
sapply(terms, getLemma)
</code></pre>

<p>The above is <strong>NOT</strong> the lemmatization that I'm looking for. What I'm looking for is, using <code>R</code> I want to find true roots of the words: (For e.g. from <code>c(""run"", ""ran"", ""running"")</code> to <code>c(""run"", ""run"", ""run"")</code>).</p>
"
"28131821","NLTK: Converting from tree values to Lambda function notation","2015-01-25 00:17:46","2","497","0","1","","28132353","<p>I am trying to convert a sentence into a lambda function notation such as the one seen <a href=""http://www.nltk.org/book/ch10.html"" rel=""nofollow"">http://www.nltk.org/book/ch10.html</a> in section 4.2. Using code I can make the sentence be converted into the semantic values and parse the sentence using context free grammar but what are ways to convert a sentence into lambda notations.</p>

<pre><code>import nltk
grammar = nltk.CFG.fromstring(""""""
  S -&gt; NP VP
  VP -&gt; V NP 
  V -&gt; ""is"" 
  Adj -&gt; ""Chewing""
  Coj -&gt; ""and""
  Nom -&gt; Adj Nom | N
  NP -&gt;  Nom Coj NP | N
  Det -&gt; ""a"" | ""an"" | ""the"" | ""my""
  N -&gt; ""gum"" | ""candy"" | ""thrilling""
  P -&gt; ""in"" | ""on"" | ""by"" | ""with""
  """""")
sent = ""Chewing gum and candy is thrilling"".split()
rd_parser = nltk.RecursiveDescentParser(grammar, )
for tree in rd_parser.parse(sent):
    print (tree)
</code></pre>
"
"28072775","How do I list out all English terms in a sentence that indicate an animal?","2015-01-21 17:01:07","3","219","1","1","","28083013","<p>For example, in the sentence ""<em>The two horses had just lain down when a brood of ducklings, which had lost their mother, filed into the barn, cheeping feebly and wandering from side to side to find some place where they would not be trodden on.</em>"", there are two animals: horse and duck. </p>

<p>I was looking for vocabulary lists for animal names but was unable to get anything that's complete enough. The <a href=""http://wordnet.princeton.edu/"" rel=""nofollow"">WordNet</a> database looks promising but may be overkill and not broad enough either.</p>
"
"27975767","What are `lexpr` and `ApplicationExpression` nltk?","2015-01-16 01:01:57","-4","472","4","2","","27997729","<p>What exactly does lexpr mean and what do the folloring r'/F x.x mean? Also what is Application Expression?</p>

<pre><code>from nltk.sem.logic import *
lexpr = Expression.fromstring

zero = lexpr(r'\F x.x')
one = lexpr(r'\F x.F(x)')
two = lexpr(r'\F x.F(F(x))')
three = lexpr(r'\F x.F(F(F(x)))')
four = lexpr(r'\F x.F(F(F(F(x))))')
succ = lexpr(r'\N F x.F(N(F,x))')
plus = lexpr(r'\M N F x.M(F,N(F,x))')
mult = lexpr(r'\M N F.M(N(F))')
pred = lexpr(r'\N F x.(N(\G H.H(G(F)))(\u.x)(\u.u))')
v1 = ApplicationExpression(succ, zero).simplify()
</code></pre>
"
"27869416","NLTK: can I add terminal to grammar that is already generated","2015-01-09 21:18:46","5","3746","0","1","","27873105","<p>I have generated grammar from atis grammar, now I wanted to add some rules of my own especially terminals from sentence could I do that?</p>

<pre><code>import nltk
grammar = nltk.data.load('grammars/large_grammars/atis.cfg')
</code></pre>

<p>to <code>grammar</code> I want to add more terminals.</p>
"
"27801219","How to unit test a Symfony2 form when it uses a transformer linked to a database","2015-01-06 14:55:44","2","848","0","1","","27801491","<p>TLDR: I am new to unit tests and I have few questions:</p>

<ol>
<li>Are my transformer tests well written?</li>
<li>Is there a way to decoupled my transformer tests from the database?</li>
<li>How to test my form with the transformer using the database?</li>
<li>Should I decouple my form from my transformer?</li>
</ol>

<hr>

<p>I don't know if my classes are too coupled, if my design is flawed or if my understanding of the unit tests is bad.</p>

<p>Here is some background.<br>
I have a form object with different widgets. One of them is used within a model transformer.<br>
This model transformer uses a connection to the database to retrieve the proper object.</p>

<p>Here is my code:</p>

<pre><code>class BookToStringTransformer implements DataTransformerInterface {

    private $om;

    public function __construct(ObjectManager $om) {
        $this-&gt;om = $om;
    }

    public function transform($book) {
        if (!$book instanceof Book) {
            return """";
        }

        return $book-&gt;getName();
    }

    public function reverseTransform($string) {
        if (!is_string($string) || !$string) {
            return null;
        }

        $book = $this-&gt;om
                -&gt;getRepository('MyBundle:Book')
                -&gt;findOneBy(array('name' =&gt; $string))
        ;

        if (null === $book) {
            throw new TransformationFailedException(sprintf(
                    'The book ""%s"" does not exist!', $string
            ));
        }

        return $book;
    }

}


class ItemType extends AbstractType {

    private $om;

    public function __construct(ObjectManager $om) {
        $this-&gt;om = $om;
    }

    public function buildForm(FormBuilderInterface $builder, array $options) {
        $bookTransformer = new BookToStringTransformer($this-&gt;om);
        $builder-&gt;add($builder-&gt;create('book', 'text', array(
                    'required' =&gt; false,
                ))-&gt;addModelTransformer($bookTransformer));
    }

    public function setDefaultOptions(OptionsResolverInterface $resolver) {
        $resolver-&gt;setDefaults(array(
            'data_class' =&gt; 'MyBundle\Entity\Item',
        ));

    }

    public function getName() {
        return 'mybundle_item';
    }

}
</code></pre>

<p>I wrote unit tests for the transformer using the KernelTestCase</p>

<pre><code>class BookToStringTransformerTest extends KernelTestCase {

    private $name = 'existing name';
    private $em;

    public function setUp() {
        static::$kernel = static::createKernel();
        static::$kernel-&gt;boot();
        $this-&gt;em = static::$kernel-&gt;getContainer()
                -&gt;get('doctrine')
                -&gt;getManager();
    }

    public function testReverseTransform_whenNameExists_returnsBookObject() {
        $transformer = new BookToStringTransformer($this-&gt;em);
        $book = $transformer-&gt;reverseTransform($this-&gt;name);
        $this-&gt;assertInstanceOf('MyBundle\Entity\Book', $book, 'Should return a Book object');
        $this-&gt;assertEquals($this-&gt;name, $book-&gt;getName(), 'Should return a Book object with the selected name');
    }

    /**
     * @expectedException Symfony\Component\Form\Exception\TransformationFailedException
     */
    public function testReverseTransform_whenNameDoesNotExist_throwsException() {
        $transformer = new BookToStringTransformer($this-&gt;em);
        $transformer-&gt;reverseTransform('unknown name');
    }

    /**
     * @param mixed $invalid_parameter
     * @dataProvider provideInvalidParameter
     */
    public function testReverseTransform_whenParameterIsInvalid_returnsNull($invalid_parameter) {
        $om = $this-&gt;getMockBuilder('Doctrine\Common\Persistence\ObjectManager')-&gt;getMock();
        $transformer = new BookToStringTransformer($om);
        $this-&gt;assertNull($transformer-&gt;reverseTransform($invalid_parameter), 'Should return a NULL value');
    }

    /**
     * @return array
     */
    public function provideInvalidParameter() {
        return [
            [null],
            [false],
            [true],
            [''],
            [[]],
            [new \stdClass()],
        ];
    }

    public function testTransform_whenParameterIsBookObject_returnsName() {
        $book = $this-&gt;em-&gt;getRepository('MyBundle:Book')
                -&gt;findOneBy(array('name' =&gt; $this-&gt;name));

        $om = $this-&gt;getMockBuilder('Doctrine\Common\Persistence\ObjectManager')-&gt;getMock();
        $transformer = new BookToStringTransformer($om);
        $this-&gt;assertEquals($this-&gt;name, $transformer-&gt;transform($book), 'Should return a string containing the name');
    }

    /**
     * @param mixed $not_book
     * @dataProvider provideInvalidBookObject
     */
    public function testTransform_whenParameterIsNotBookObject_returnsEmptyString($not_book) {
        $om = $this-&gt;getMockBuilder('Doctrine\Common\Persistence\ObjectManager')-&gt;getMock();
        $transformer = new BookToStringTransformer($om);
        $this-&gt;assertEquals("""", $transformer-&gt;transform($not_book), 'Should return an empty string to be chained');
    }

    /**
     * @return array
     */
    public function provideInvalidBookObject() {
        return [
            [null],
            [123],
            ['123'],
            [[]],
            [true],
            [new \stdClass()],
        ];
    }

}
</code></pre>

<p>As I am new to unit tests, I don't even know if it is the proper way to test that transformer.<br>
I start writing tests for the form object. I am using the TypeTestCase but there is no simple way to get the connection to the database and I can't use the KernelTestCase.</p>

<pre><code>class ItemTypeTest extends TypeTestCase {

    /**
     * @expectedException \PHPUnit_Framework_Error
     */
    public function test_whenCreatedWithNoParameters_raiseException() {
        new ItemType();
    }

    /**
     * @expectedException \PHPUnit_Framework_Error
     */
    public function test_whenCreatedWithBadParameters_raiseException() {
        new ItemType(123);
    }

    public function test_whenCreatedWithGoodParameters_createsFormObject() {
        $om = $this-&gt;getMockBuilder('Doctrine\Common\Persistence\ObjectManager')-&gt;getMock();
        $type = new ItemType($om);
        $form = $this-&gt;factory-&gt;create($type);
        $this-&gt;assertInstanceOf('Symfony\Component\Form\Form', $form);
    }

    public function test_whenSubmittedWithGoodData() {
        $formData = array(
            'name' =&gt; 'existing name',
        );

        $om = $this-&gt;getMockBuilder('Doctrine\Common\Persistence\ObjectManager')-&gt;getMock();
        $type = new ItemType($om);
        $form = $this-&gt;factory-&gt;create($type);

        $form-&gt;submit($formData);
    }

}
</code></pre>

<p>The last test fails because the transformer does get access to the database since I am passing a mock to the form. So should I get a real object (meaning classes are too coupled) or should I find an other way.</p>

<p>Thank you</p>
"
"27773802","Replace words with word-substitutions from another file","2015-01-05 05:17:57","1","301","3","6","","27774208","<p>The words from my text file (mytext.txt) needs to be replaced by some other word provided in another text file (replace.txt)</p>

<pre><code>cat mytext.txt
this is here. and it should be there. 
me is this will become you is that.

cat replace.txt
this that
here there
me you
</code></pre>

<p>The following code does not work as expected. </p>

<pre><code>with open('mytext.txt', 'r') as myf:
    with open('replace.txt' , 'r') as myr:
        for line in myf.readlines():
            for l2 in myr.readlines():
                original, replace = l2.split()
                print line.replace(original, replace)
</code></pre>

<p>Expected output:</p>

<pre><code>that is there. and it should be there. 
you is that will become you is that.
</code></pre>
"
"27766869","lists of tuples by a tuple element?","2015-01-04 15:08:04","1","49","0","1","","27768214","<p>I have a problem on matching the part of speech pos pattern.
we had a rules of preposition phrase pattern such as NN + IN + NN, VBG + IN + NN or ADJ + IN + NN. </p>

<p>The idea is extract the pattern from any given sentence and do matching with the define rules above, if matched then return True.</p>

<p>example extracted from sentence:
 sent_pos = [('increasing', 'VBG'), ('of', 'IN'), ('mutation', 'NN')]
match with either 
 rules1 = [('', 'VBG'), ('', 'IN'), ('', 'NN')]
 or 
 rule2 = [('', 'NN'), ('', 'IN'), ('', 'NN')]
 or 
 [('', 'ADJ'), ('', 'IN'), ('', 'NN')]</p>

<p>result return True.</p>

<p>It is possible in python code?</p>

<p>Thanks appreciated for your reply.</p>
"
"27728001","POS accuracy of known and unknown words","2015-01-01 03:00:18","0","211","0","1","","33906606","<p>How do I calculate the accuracy of known and unknown words in part of speech tagging? For example for known words, is it dividing the correctly tagged known words by all the known words ? Any other ways ?</p>
"
"27676164","how to extract elements from tree.productions()","2014-12-28 11:57:52","2","2036","8","1","","27768211","<p>(1)My goal: To extract left-hand side and right-hand side of a production.</p>

<p>(2)My approach:
I am employing stanford parser and nltk tools to extract parsetree of a sentence. My code is below:</p>

<pre><code>corenlp_dir = ""/home/corenlp-python/stanford-corenlp-full-2013-11-12/""
parser = corenlp.StanfordCoreNLP(corenlp_path=corenlp_dir)

result_json = json.loads(parser.parse(""I have a tree.""))
for sentence in result_json[""sentences""]:
    t = Tree.fromstring(sentence[""parsetree""])
    print t.productions()   # [ROOT -&gt; S, S -&gt; NP VP ., NP -&gt; PRP, PRP -&gt; 'I', VP -&gt; VBP NP, VBP -&gt; 'have', NP -&gt; DT NN, DT -&gt; 'a', NN -&gt; 'tree', . -&gt; '.']

    print t.productions()[1]  # S -&gt; NP VP .
    print type(productions()[1])  # &lt;class 'nltk.grammar.Production'&gt;

    for (i,child) in enumerate(t): 
        print (i,child)  # (0, Tree('S', [Tree('NP', [Tree('PRP', ['I'])]), Tree('VP', [Tree('VBP', ['have']), Tree('NP', [Tree('DT', ['a']), Tree('NN', ['tree'])])]), Tree('.', ['.'])])) I can only get one tree.
</code></pre>

<p>(3)My question is how I can continue to extract elements from both sides of each <strong>production</strong> , such as <strong>'S'</strong> and <strong>'NP VP .'</strong>. Is there any method can be used to solve this problem?</p>

<p>Could anyone help me and maybe point out some directions?</p>
"
"27659179","Porter Stemming of fried","2014-12-26 15:57:14","2","958","0","2","","27659535","<p>Why does the porter stemming algorithm online at</p>

<p><a href=""http://text-processing.com/demo/stem/"" rel=""nofollow"">http://text-processing.com/demo/stem/</a></p>

<p>stem <code>fried</code> to <code>fri</code> and not <code>fry</code>?</p>

<p>I can't recall any words ending with <code>ied</code> past tense in English that have a nominative form ending with <code>i</code>.</p>

<p>Is this a bug?</p>
"
"27649735","Convert PCFG to CNF?","2014-12-25 18:31:59","1","1342","2","1","","27652972","<p>I have this rule in a grammar :</p>

<pre><code>VP-&gt;Verb NP NP   [0.05]
VP-&gt;Verb NP PP   [0.1]
</code></pre>

<p>I want to convert this PCFG(Probabilistic Context Free grammar ) to CNF(Chomsky Normal Form)</p>

<p>To do that I know we can split the rule into two non-terminals </p>

<pre><code>VP-&gt;@V_N NP
VP-&gt;@V_N PP
@V_N-&gt;Verb NP
</code></pre>

<p>Which probability to set for each rule?</p>

<p>Thanks</p>
"
"27522015","How to solve this weird python encoding issue?","2014-12-17 09:18:47","2","3139","1","3","","27522658","<p>I'm doing some NLP task on a corpus of strings from the web - and as you expect, there are encoding issues. Here're a few examples:</p>

<pre><code>they don’t serve sushi : the apostrophe in don't is not standard ' but \xe2\x80\x99
Delicious food – Wow   : the hyphen before wow is \xe2\x80\x93
</code></pre>

<p>So now, I'm gonna read such lines, pass them to NLTK for parsing, use the parse information to train a CRF model through mallet. </p>

<p>Let's begin with the solution I've been seeing everywhere on stack-overflow. Here're a few experiments:-</p>

<pre><code>st = ""they don’t serve sushi""

st.encode('utf-8')
Out[2]: 'they don\xc3\xa2\xe2\x82\xac\xe2\x84\xa2t serve sushi'

st.decode('utf-8')
Out[3]: u'they don\u2019t serve sushi'
</code></pre>

<p>So these are just trial-and-error attempts to see if something might work. </p>

<p>I finally used the encoded sentence and passed it to the next part - pos tagging using nltk. <code>posTags = nltk.pos_tag(tokens)</code> and it throws an ugly exception known to everyone :-</p>

<pre><code> File ""C:\Users\user\workspacePy\_projectname_\CRF\FeatureGen.py"", line 95, in getSentenceFeatures
    posTags = nltk.pos_tag(tokens)
  File ""C:\Users\user\Anaconda\lib\site-packages\nltk\tag\__init__.py"", line 101, in pos_tag
    return tagger.tag(tokens)
  File ""C:\Users\user\Anaconda\lib\site-packages\nltk\tag\sequential.py"", line 61, in tag
    tags.append(self.tag_one(tokens, i, tags))
  File ""C:\Users\user\Anaconda\lib\site-packages\nltk\tag\sequential.py"", line 81, in tag_one
    tag = tagger.choose_tag(tokens, index, history)
  File ""C:\Users\user\Anaconda\lib\site-packages\nltk\tag\sequential.py"", line 634, in choose_tag
    featureset = self.feature_detector(tokens, index, history)
  File ""C:\Users\user\Anaconda\lib\site-packages\nltk\tag\sequential.py"", line 736, in feature_detector
    'prevtag+word': '%s+%s' % (prevtag, word.lower()),
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 0: ordinal not in range(128)
</code></pre>

<p>And when I tried decoding, it says <code>UnicodeEncodeError: 'ascii' codec can't encode character u'\u2013' in position 42: ordinal not in range(128)</code> in the line where I'm decoding the string.</p>

<p>So my current solution is to remove all the non-ascii characters. But it totally changes the word which causes a serious loss of data for unigram-bigram (word combination) based model.</p>

<p>What should be the right approach?</p>
"
"27475658","Difference between Semantic Web and NLP?","2014-12-15 00:13:10","5","3208","1","5","","27486368","<p>What exactly is the difference between Semantic Web and Natural Language Processing? </p>

<p>Is Semantic Web a part of Natural Language Processing?</p>
"
"27467967","how to divide a series of words into ""N"" chunks?","2014-12-14 09:28:34","0","114","2","2","","27468778","<p>first of all forgive me for any ambiguity . i find my problem a bit hard to explain in English . 
basically what i want to do is , to divide a huge set of words to ""N"" parts . </p>

<p>for example read all the words in a file , then divide them between lets say N=10 parts .
to be more precise , i'm working on a data mining project . there are thousands of documents which i need to sort the words of . </p>

<p>say n = 2 . i know i can put a-m and n-z in a file . i need an algorithm which can do this for n > 100 .</p>

<p>PS : my program FIRST has to create the N files ( or chunks ) then read all the words and depending on how they begin , assign them to one of the chunks .</p>

<p>EXAMPLE :
input : 
N = 2
words = [....]</p>

<p>output :
[words starting with a-m] , [words starting with n-z] </p>

<p>in other words i want to divide my words Lexicographically</p>
"
"27311579","Techniques for avoiding stackoverflow in dynamic time warping","2014-12-05 08:20:04","1","229","2","1","","27326275","<p>I wrote a dynamic time warping word recognition system and found out that the largest audio file that I can process with out maximum recursion depth reached is 1 second sample. What are the common techniques to overcome this issue?</p>

<p>Here is the code I am referencing.</p>

<p><a href=""https://gist.github.com/anonymous/3f06cc0eec198bdf0c03"" rel=""nofollow"">https://gist.github.com/anonymous/3f06cc0eec198bdf0c03</a></p>
"
"27227111","Confused about priority between stemmer and pos tagger","2014-12-01 11:11:33","6","1908","0","2","","27229930","<p>So I was analyzing a text corpus and I used stemmer for all the tokenized words.
But I also have to find all the nouns in the corpus so I again did a <code>nltk.pos_tag(stemmed_sentence)</code>
But my question is am I doing it right? </p>

<pre><code>A.] tokenize-&gt;stem-&gt;pos_tagging
</code></pre>

<p>OR</p>

<pre><code>B.] tokenize-&gt;stem       #stemming and pos_tagging done seperately
    tokeinze-&gt;pos_tagging
</code></pre>

<p>Ive followed method A, but Im confused as to its the right way to achieve pos_tagging.</p>
"
"27089678","Natural language query preprocessing","2014-11-23 13:53:12","0","458","2","1","","27090896","<p>I am trying to implement a natural language query preprocessing module which would, given a query formulated in natural language, extract the keywords from that query and submit it to an Information Retrieval (IR) system.</p>

<p>At first, I thought about using some training set to compute tf-idf values of terms and use these values for estimating the importance of single words. But on second thought, this does not make any sense in this scenario - I only have a training collection but I dont have access to index the IR data. Would it be reasonable to only use the idf value for such estimation? Or maybe another weighting approach?</p>

<p>Could you suggest how to tackle this problem? Usually, the articles about NLP processing that I read address training and test data sets. But what if I only have the query and training data?</p>
"
"26988686","Online documentation explaining tags output by Stanford NLP parser?","2014-11-18 07:19:07","0","275","0","2","","26990838","<p>is there any online documentation explaining tags output by Stanford NLP parser?</p>

<p>I'm quite new to NLP and and to me it seems that the tags like NN, VBZ, .. and relationships like poss, nsubj ... seem to follow a kind of standard since I've seen this output on other parsers.</p>

<p>thanks a lot!</p>
"
"26962725","How to convert text file to CoNLL format for malt parser?","2014-11-16 22:20:23","5","7566","2","1","","26966351","<p>I'm trying to use malt parser with the pre made english model. However, I do not know how to convert a text corpus of English sentences into the CoNLL format that is necessary for Malt Parser to operate on. I could not find any documentation on the site. How should I go about this?</p>

<p>Update. I am referring to this post <a href=""https://stackoverflow.com/questions/17450652/create-conll-file-as-output-of-stanford-parser"">Create .conll file as output of Stanford Parser</a> to create a .conll. However, this is using Stanford Parser.</p>
"
"26921862","Chomsky-normal-form grammar extraction from a parse tree","2014-11-14 02:19:01","5","3624","0","1","","26922032","<p>I am trying to extract the Chomsky Normal Form (CNF) - grammar productions of a sentence from its parse tree:</p>

<pre><code>(ROOT
  (S
    (NP (DT the) (NNS kids))
    (VP (VBD opened)
      (NP (DT the) (NN box))
      (PP (IN on)
        (NP (DT the) (NN floor)))))) 
</code></pre>

<p>I put the whole tree into a string named S and then:</p>

<pre><code>tree = Tree.fromstring(S)
tree.chomsky_normal_form()
for p in tree.productions():
    print p
</code></pre>

<p>The output is </p>

<pre><code>(1) NN -&gt; 'box'
(2) PP -&gt; IN NP
(3) DT -&gt; 'the'
(4) ROOT -&gt; S
(5) NP -&gt; DT NN
(6) VBD -&gt; 'opened'
(7) VP|&lt;NP-PP&gt; -&gt; NP PP
(8) VP -&gt; VBD VP|&lt;NP-PP&gt;
(9) NP -&gt; DT NNS
(10) NN -&gt; 'floor'
(11) IN -&gt; 'on'
(12) NNS -&gt; 'kids'
(13) S -&gt; NP VP
</code></pre>

<p>But some of the productions (number 7 &amp; 8) don't seem to be CNF! What is the problem?</p>
"
"26921111","How to deal with some ambiguous context free grammar productions in Python","2014-11-14 00:38:50","0","285","0","1","","26921132","<p>I am trying to use CNF grammar by feeding nltk.cfg with a bunch of grammar productions like: </p>

<pre><code> NN -&gt; 'rubble' | 'slope' | 'Jake'
 VP -&gt; V NP | VP PP 
</code></pre>

<p>But it has problem (gives the error: Expected an arrow) with the productions which have pipes on the left-hand side of the production. Example:</p>

<pre><code>VP | &lt;VBP-SBAR&gt; -&gt; VBP SBAR
</code></pre>

<p>Does nltk have any grammar-method which doesn't have problem with pipes on the left-hand side?</p>

<p>If not, How can I change all those productions to usable productions like the first group? Example:</p>

<pre><code>VP  -&gt; VBP SBAR    
&lt;VBP-SBAR&gt; -&gt; VBP SBAR
</code></pre>
"
"26868077","How to solve StandfordOpenNLP error","2014-11-11 15:14:01","1","54","0","1","","26868180","<p>I have downloaded StanfordOpenNLP and try to use its lemmatiser. But it gives me an error as below,</p>

<blockquote>
  <p>Unsupported major.minor version 52.0</p>
</blockquote>

<p>Can anyone please tell me how to solve this.</p>
"
"26780092","Stanford type dependency, can not extract ""prepositional modfier""","2014-11-06 12:58:05","0","330","0","1","","26786851","<p>I am trying to extract the prepositional modifier, like it is stated in the <strong>Dependency Manual</strong>:</p>

<p>I try to parse the sentence : 
<em>""I saw a cat with a telescope""</em> , using the code: </p>

<pre><code>List&lt;CoreMap&gt; sentences = stanfordDocument.get(SentencesAnnotation.class);
for (CoreMap sentence : sentences) {
   Tree tree = sentence.get(TreeAnnotation.class);
   TreebankLanguagePack languagePack = new PennTreebankLanguagePack();
   GrammaticalStructureFactory grammaticalStructureFactory = languagePack.grammaticalStructureFactory();
   GrammaticalStructure structure = grammaticalStructureFactory.newGrammaticalStructure(tree);
   Collection&lt;TypedDependency&gt; typedDependencies = structure.typedDependenciesCollapsed();
   for (TypedDependency td : typedDependencies) {
      System.out.println(td.reln());
   }
}     
</code></pre>

<p>As stated in the Manual I was expecting to get : <em>prep(saw, with)</em>.</p>

<p>In the Collection of the TypedDependeny I get only
 ""<em>nsubj; root; det; dobj; det; prep_with</em>"" as relation type, and not the <em>""prep/prepc""</em>  as stated in the <a href=""http://robotics.usc.edu/~gkoch/DependencyManual.pdf"" rel=""nofollow"">http://robotics.usc.edu/~gkoch/DependencyManual.pdf</a> (page 8). </p>

<p>I have also tried to extract <em>pcomp : Prepositional compelement</em> (page 7 of the manual) and it doesnt find it.</p>

<p>Did somebody encountered the same problem? Am I doing anything wrong?</p>
"
"26729556","Removing non ASCII from corpus","2014-11-04 07:14:02","3","1547","0","1","","26729837","<p>I'm using NLTK for my project. However, if a non-ascii word like '•' exist. NLTK cannot tokenize it.
I'm using <code>nltk.word_tokenize</code> as the tokenizer.
How do I remove such words from entire corpus or make the tokenizer aware of such words?</p>
"
"26689133","Stanford POS tagger with GATE twitter model is slow","2014-11-01 12:05:33","0","769","1","1","","27175149","<p>I am using the <a href=""http://nlp.stanford.edu/software/tagger.shtml"" rel=""nofollow"">Stanford POS tagger</a> with the <a href=""https://gate.ac.uk/wiki/twitter-postagger.html"" rel=""nofollow"">GATE Twitter model</a> and the tagger takes around 3 seconds to initialize, is this normal or am I loading it incorrectly?</p>

<p><strong>Small sample code:</strong></p>

<pre><code>package tweet.nlp.test;

import edu.stanford.nlp.tagger.maxent.MaxentTagger;

public class TweetNLPTest {

    public static void main(String[] args) {
        String text = ""My sister won't tell me where she hid my food. She's fueling my anorexia. #bestsisteraward #not 😭💀"";

        MaxentTagger tagger = new MaxentTagger(""models/gate-EN-twitter.model"");

        String taggedText = tagger.tagString(text);
    }
}
</code></pre>

<p><strong>Output:</strong></p>

<blockquote>
  <p>Reading POS tagger model from models/gate-EN-twitter.model ...
  warning: no language set, no open-class tags specified, and no
  closed-class tags specified; assuming ALL tags are open class tags
  done [3.1 sec].</p>
  
  <p>My_PRP$ sister_NN won't_MD tell_VB me_PRP where_WRB
  she_PRP hid_VBD my_PRP$ food._NN She's_VBZ fueling_VBG my_PRP$
  anorexia._NN #bestsisteraward_HT #not_HT 😭💀_HT  BUILD SUCCESSFUL
  (total time: 3 seconds)</p>
</blockquote>
"
"26648986","Maltparser doesn't do anything","2014-10-30 09:13:44","2","121","0","1","","26651130","<p>I'm working with maltparser, nltk for process texts. Well i have a integration between maltparser and nltk that works fine. But since every time i execute the program nltk call java VE this take a lot of time... So i think make a webservice who takes conll .txt and return conll parsed by java app. </p>

<p>Well the problem come when i test examples from maltparser sources. I pick one from just initialize model and parser a array of tokens. I just change de model to the regular english one (engmalt.linear-1.7.mco). So execute and return the sentences just like input.</p>

<p>The code is this</p>

<pre><code>public static void main(String[] args) {
    // Loading the Swedish model swemalt-mini
    ConcurrentMaltParserModel model = null;
    try {
        URL swemaltMiniModelURL = new File(""inputs/engmalt.linear-1.7.mco"").toURI().toURL();
        System.out.println(swemaltMiniModelURL.getFile());
        model = ConcurrentMaltParserService.initializeParserModel(swemaltMiniModelURL);
    } catch (Exception e) {
        e.printStackTrace();
    }

    // Creates an array of tokens, which contains the Swedish sentence 'Samtidigt får du högsta sparränta plus en skattefri sparpremie.'
    // in the CoNLL data format.
    String[] tokens = new String[5];
    tokens[0] = ""1\tThis\t_\tDT\tDT\t_\t0\ta\t_\t_"";
    System.out.println(tokens[0]);
    tokens[1] = ""2\tis\t_\tVBZ\tVBZ\t_\t0\ta\t_\t_"";
    System.out.println(tokens[1]);
    tokens[2] = ""3\ta\t_\tZ\tZ\t_\t0\ta\t_\t_"";
    System.out.println(tokens[2]);
    tokens[3] = ""4\ttest\t_\tNN\tNN\t_\t0\ta\t_\t_"";
    System.out.println(tokens[3]);
    tokens[4] = ""5\t.\t_\tFp\tFp\t_\t0\ta\t_\t_"";
    System.out.println(tokens[4]);
    try {
        String[] outputTokens = model.parseTokens(tokens);
        ConcurrentUtils.printTokens(outputTokens);
    } catch (Exception e) {
        e.printStackTrace();
    }
}
</code></pre>

<p>and the output is:</p>

<pre><code>/home/tomas/workspace/PruebaMalt/inputs/engmalt.linear-1.7.mco
1   This    _   DT  DT  _   0   a   _   _
2   is  _   VBZ VBZ _   0   a   _   _
3   a   _   Z   Z   _   0   a   _   _
4   test    _   NN  NN  _   0   a   _   _
5   .   _   Fp  Fp  _   0   a   _   _
1   This    _   DT  DT  _   0   a   _   _
2   is  _   VBZ VBZ _   0   a   _   _
3   a   _   Z   Z   _   0   a   _   _
4   test    _   NN  NN  _   0   a   _   _
5   .   _   Fp  Fp  _   0   a   _   _
</code></pre>

<p>I try with others models and languages and the same... Any suggestions? ty!</p>
"
"26582284","encoding error in pos tagging with nltk 3.0 on python 3.4","2014-10-27 06:41:13","2","849","1","2","","28458759","<p>I am using <code>NLTK 3.0</code> with Python 3.4 and cannot do POS tagging because of the following error:
I have read all <a href=""https://stackoverflow.com/questions/25590089/nltk-3-pos-tag-throws-unicodedecodeerror"">similar posts related to similar problems</a>, but could not find a way to solve the problem. most of the posts mention that upgrading to <code>NLTK 3.0</code> will solve the problem but I already have <code>NLTK 3.0</code>. According to these posts a change in the nltk's <code>data.py</code> solves the problem but <code>NLTK</code> people discourage doing that.
Here is my code:</p>

<pre><code>from nltk.tag import pos_tag
from nltk.tokenize import word_tokenize
pos_tag(word_tokenize(""John's big idea isn't all that bad.""))
</code></pre>

<p>and here is the error:</p>

<blockquote>
  <p>UnicodeDecodeError: 'ascii' codec can't decode byte 0xcb in position 0: ordinal not in range(128)</p>
</blockquote>

<p>Is there any way to do it without manipulating <code>data.py</code>?
Any idea would be appreciated.</p>
"
"26450647","how to construct training vectors of word n-gram using TF-IDF","2014-10-19 12:42:53","2","603","0","1","","26465228","<p>My task is to do the text classification with svm, using word n-gram as features. 
Before using TF-IDF, my code is:</p>

<pre><code>word_dic = ngram.wordNgrams(text, n)
freq_term_vector = [word_dic[gram] if gram in word_dic else 0 for gram in global_vector]
X.append(freq_term_vector)
</code></pre>

<p>And It works well. However, when I tried TF-IDF, the code is below:</p>

<pre><code>freq_term_vector = [word_dic[gram] if gram in word_dic else 0 for gram in global_vector]
tfidf = TfidfTransformer(norm=""l2"")
tfidf.fit(freq_term_vector)
X.append(tfidf.transform(freq_term_vector).toarray())
</code></pre>

<p>The training part can be done, but when the program ran to the predict part, it said</p>

<pre><code> clf.predict(X_test)
  File ""/usr/lib/python2.7/dist-packages/sklearn/linear_model/base.py"", line 223, in predict
    scores = self.decision_function(X)
  File ""/usr/lib/python2.7/dist-packages/sklearn/linear_model/base.py"", line 207, in decision_function
    dense_output=True) + self.intercept_
  File ""/usr/lib/python2.7/dist-packages/sklearn/utils/extmath.py"", line 83, in safe_sparse_dot
    return np.dot(a, b)
ValueError: shapes (1100,1,38) and (1,11) not aligned: 38 (dim 2) != 1 (dim 0)
</code></pre>

<p>The training method and predict method are the same. How can I solve this align problem? Could anyone help me check my code above or give me some idea?</p>
"
"26365791","Using Python and NLP to get the most frequent POS tag from a list","2014-10-14 16:33:45","3","4152","0","3","","26365936","<p>I'm trying to get the most frequent POS tags (top five) from a list.</p>

<pre><code>pos_list = nltk.pos_tag(list)
#pos_list = [('caught', 'NN'), ('black', 'NN'), ('a', 'DT'), ('striped', 'JJ'), ('eel', 'NN')]
tag_fd = nltk.FreqDist(tag for (word, tag) in pos_list)
</code></pre>

<p>I've also tried looping through <code>pos_list</code> to count the tags that way but there seems like there has to be a way to do this using <code>NLTK</code>.  I've also tried to create a string out of the list and trying the same approach but that isn't working either.</p>

<pre><code>str_of_list = "" "".join(list)
tag_fd = nltk.FreqDist(tag for (word, tag) in str_of_list)
</code></pre>

<p>Thanks any help is appreciated!</p>
"
"26352041","NLTK Entity Extraction Difference from NLTK 2.0.4 to NLTK 3.0","2014-10-14 03:13:25","1","867","0","1","","26367962","<p>I'm running into an issue trying to run an entity extraction function.  I believe it's a versioning difference.  The following working example runs in 2.0.4 but does not run in 3.0. I did change one function call: batch_ne_chunk to: nltk.ne_chunk_sents to prevent an error being thrown in 3.0.</p>

<pre><code>def package_get_entities(self,text):
    #text = text[0:300]
    entity_names = []
    chunked = self.get_chunked_sentences(text)
    for tree in chunked:
        entity_names.extend(self.extract_entity_names(tree))
    entity_names = list(set(entity_names))
    return entity_names

def get_chunked_sentences(self,text):
    sentences = nltk.sent_tokenize(text)
    tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]
    tagged_sentences = [nltk.pos_tag(sentence) for sentence in tokenized_sentences]
    chunked_sentences = nltk.ne_chunk_sents(tagged_sentences, binary=True)
    return chunked_sentences

def extract_entity_names(self,t):
    entity_names = []
    if hasattr(t, 'node') and t.node:
        if t.node == 'NE':
            entity_names.append(' '.join([child[0] for child in t]))
        else:
            for child in t:
                entity_names.extend(self.extract_entity_names(child))
    return entity_names
</code></pre>

<p>Running the func:</p>

<pre><code>str = 'this is some text about a man named Abraham Lincoln'
entArray = package_get_entities(str)
</code></pre>

<p>In 2.0.4 outputs [Abraham Lincoln]
In 3.0 outputs []</p>
"
"25973351","sentence boundry detection in noisy or ASR data","2014-09-22 11:46:20","1","295","0","1","","25980612","<p>There are many tools and paper available which perform this task using basic sentence seperators.</p>

<p>Such tools are</p>

<ol>
<li><p><a href=""http://nlp.stanford.edu/software/tokenizer.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/tokenizer.shtml</a>   </p></li>
<li><p>OpenNLP </p></li>
<li><p>NLTK</p></li>
</ol>

<p>and there might be other. They mainly focus on</p>

<pre><code>(a) If it's a period, it ends a sentence.
(b) If the preceding token is on my hand-compiled list of abbreviations, then it doesn't end a sentence.
(c) If the next token is capitalized, then it ends a sentence. 
</code></pre>

<p>There are few paper which suggest techniques for SBD in ASR text</p>

<p><a href=""http://pdf.aminer.org/000/041/703/experiments_on_sentence_boundary_detection.pdf"" rel=""nofollow"">http://pdf.aminer.org/000/041/703/experiments_on_sentence_boundary_detection.pdf</a></p>

<p><a href=""http://www.icsd.aegean.gr/lecturers/kavallieratou/publications_files/icpr_2000.pdf"" rel=""nofollow"">http://www.icsd.aegean.gr/lecturers/kavallieratou/publications_files/icpr_2000.pdf</a></p>

<p><a href=""http://www.icsd.aegean.gr/lecturers/kavallieratou/publications_files/icpr_2000.pdf"" rel=""nofollow"">http://www.icsd.aegean.gr/lecturers/kavallieratou/publications_files/icpr_2000.pdf</a></p>

<p><strong>Is there any tools which can perform sentence detection on ambiguous sentences like</strong></p>

<ol>
<li><p>John is actor and his father Mr Smith was top city doctor in NW (2 sentences)</p></li>
<li><p>Where is statue of liberty, what is it's height and what is the history behind?  (3 sentences)</p></li>
</ol>
"
"25965417","How to create a context free grammar based on both ""lexicon"" and ""rules"" in NLTK","2014-09-22 01:00:57","0","1283","0","1","","25969784","<p>I have two text files for a CFG grammar: one is the ""rules"" (e.g. S->NP VP) and another one contains just the ""lexical symbols"" (e.g. ""these"": Det). Does any one know how I can give this two files as my grammar to NLTK? The second file is also known as ""lexicon"", because it just contains the category of real words. In summary, I just need to provide a lexicon for a specific grammar. Otherwise, I have to write the lexicon as several new rules in my rules' file. Due to the large volume of lexicon, It is not possible to convert the second file to rules and merge it with the first file. So I am completely stuck here... Any help/idea would be appreciated.</p>
"
"25893690","Practical Earley Parsing (Aycock & Horspool 2002): How to add back pointers?","2014-09-17 14:43:51","3","683","0","1","","25899118","<p>I've already coded an Earley parser with back pointers but it doesn't handle nullable grammars very well. I've also implemented Aycock &amp; Horspool 2002's solution which is to make PREDICT skip over the nonterminal token if it is nullable. Unfortunately this does not tell you exactly which particular path the token needs to take to get to epsilon. </p>

<p>My idea (pretty stupid) is: </p>

<blockquote>
  <p>For each nullable nonterminal, create a list of paths for that
  nonterminal to get to epsilon. </p>
  
  <p>Every time you skip over a nullable nonterminal, add a back pointer
  called NULL.</p>
  
  <p>When you're expanding the tree, every time you encounter NULL, you
  create a list of trees, one for each path in the list for that
  nullable nonterminal. </p>
  
  <p>Finally, you go through the list of trees and get rid of duplicates.</p>
</blockquote>

<p>I think this would significantly increase the time complexity of my algorithm but I can't think of a more efficient method to generate all the possible parse trees. </p>

<p>Can anyone suggest a more efficient method of implementing Aycock &amp; Horspool 2002 to create parse trees?</p>
"
"25833693","Python faster alternative to dictionary?","2014-09-14 13:21:59","7","16678","2","1","","25833720","<p>I'm making a simple sentiment mining system using a <code>Naive Bayes classifier</code>. </p>

<p>For training my classifier, I have a text file where each line contains a list of tokens (generated from a tweet), and the associated sentiment (0 for -ve, 4 for positive).</p>

<p>For example:</p>

<pre><code>0 @ switchfoot http : //twitpic.com/2y1zl - Awww , that 's a bummer . You shoulda got David Carr of Third Day to do it . ; D
0 spring break in plain city ... it 's snowing
0 @ alydesigns i was out most of the day so did n't get much done
0 some1 hacked my account on aim now i have to make a new one
0 really do n't feel like getting up today ... but got to study to for tomorrows practical exam ...
</code></pre>

<p>Now, what I'm trying to do is for each token, count how many times it occurs in a positive tweet, and how many times it occurs in a negative tweet. I then plan to use these counts for calculating probabilities. I'm using the built-in dictionary for storing these counts. The keys are the tokens and the values are integer arrays of size 2.</p>

<p>The problem is that this code starts off pretty fast, but keeps getting slower and when it has processed around 200 thousand tweets, it gets really slow - around 1 tweet per second. Since my training set has 1.6 million tweets, this is too slow.
The code I have is this:</p>

<pre><code>def compute_counts(infile):
    f = open(infile)
    counts = {}
    i = 0
    for line in f:
        i = i + 1
        print(i)
        words = line.split(' ')
        for word in words[1:]:
            word = word.replace('\n', '').replace('\r', '')
            if words[0] == '0':
                if word in counts.keys():
                    counts[word][0] += 1
                else:
                    counts[word] = [1, 0]
            else:
                if word in counts.keys():
                    counts[word][1] += 1
                else:
                    counts[word] = [0, 1]
    return counts
</code></pre>

<p>What can I do to make this process faster? A better data structure? </p>

<p>Edit: Not a duplicate, the question is not about something faster than dict in the general case, but in this specific use case.</p>
"
"25817177","Optimize NLTK Code To Make Predictions From Text","2014-09-12 21:32:44","1","1411","3","1","","25841723","<p>I am trying to build a model to predict if the salary of a job description is above or below the 75th percentile (above 1, below 0) My data has about 250,000 rows and its very hard to tokenize all the text from the job descriptions. My code seems to work fine, but it takes insane amounts of time to do it above 100 rows. I need to find a way to make it more efficient so that I can include more rows to my prediction. </p>

<pre><code>import random
import nltk
import pandas
import csv
import numpy as np

io = pandas.read_csv('Train_rev1.csv',sep=',',usecols=(2,10), nrows=501)
#converted = df.apply(lambda io : int(io[0]))
data = [np.array(x) for x in io.values]

random.shuffle(data)
size = int(len(data) * 0.6)
test_set, train_set = data[size:], data[:size]
train_set = np.array(train_set)
test_set = np.array(test_set)
x = train_set[:,1]
Sal75=np.percentile(x,75)
y = test_set[:,1]
Test75=np.percentile(y,75)

for i in range(len(train_set[:,1])):
    if train_set[i,1]&gt;=Sal75:
        train_set[i,1]=1
    else:
        train_set[i,1]=0

for i in range(len(test_set[:,1])):
    if test_set[i,1]&gt;=Test75:
        test_set[i,1]=1
    else:
        test_set[i,1]=0

train_setT = [tuple(x) for x in train_set]
test_setT = [tuple(x) for x in test_set]



from nltk.tokenize import word_tokenize
all_words = set(word.lower() for passage in train_setT for word in word_tokenize(passage[0]))
t = [({word: (word in word_tokenize(x[0])) for word in all_words}, x[1]) for x in train_setT]

classifier = nltk.NaiveBayesClassifier.train(t)

all_words2 = set(word.lower() for passage in test_setT for word in word_tokenize(passage[0]))
tt = [({word: (word in word_tokenize(x[0])) for word in all_words}, x[1]) for x in test_setT]


print nltk.classify.accuracy(classifier, tt)
classifier.show_most_informative_features(20)
testres = []
predres = []
for i in range(len(tt)):
    testres.append(tt[i][1])
for i in range(len(tt)):
    z = classifier.classify(tt[i][0])
    predres.append(z)
from nltk.metrics import ConfusionMatrix
cm = nltk.ConfusionMatrix(testres, predres)
print(cm)
</code></pre>

<p>The csv file was extracted from Kaggle.<a href=""http://www.kaggle.com/c/job-salary-prediction/data"" rel=""nofollow"">Use Train_rev1</a> </p>
"
"25789104","Finding relationships among words in text","2014-09-11 13:45:39","1","2456","5","2","","25791984","<p>In text, sometimes words tend to point to the same object.
For example: <code>John is an actor, his father Abraham was Doctor</code></p>

<p>So here <code>his</code> points to <code>John</code>, and if we have the question <code>Who is John's father?</code> or <code>What is John's father's occupation?</code>, we should be able to answer this but I don't know how to achieve this.</p>

<p>Using lexical analysis, parse; using sentence parsing we can get <code>VP, NP, N</code> etc from the sentence. This can help for it - <a href=""https://pypi.python.org/pypi/pylinkgrammar"" rel=""nofollow"">https://pypi.python.org/pypi/pylinkgrammar</a></p>

<p>Latent semantic analysis and <a href=""http://en.wikipedia.org/wiki/Probabilistic_latent_semantic_analysis"" rel=""nofollow"">Probabilistic latent semantic analysis (PLSA)</a> provides relation and can be used to analyze two-mode and co-occurrence data. But its not clear how it can be used.</p>

<p>More of kinda semantic and syntactic analysis.
Any suggestion or reference for this would much appreciated. </p>
"
"25741209","Diminutive words stemming / lemmatization","2014-09-09 09:33:04","6","621","2","1","","27294819","<p>Currently I use 'lucene' and 'elasticsearch', and have next problem.
I need get stemmed form or lemma for <a href=""http://en.wikipedia.org/wiki/Diminutive"">diminutive</a> word. For instance :</p>

<ul>
<li><em>doggy -> dog</em> </li>
<li><em>kitty -> cat</em></li>
</ul>

<p>etc.</p>

<p>But I get next results :</p>

<ul>
<li><em>doggy -> doggi</em> </li>
<li><em>kitty -> kitti</em></li>
</ul>

<p>Is there any way (not important ready to use library, any algorithm, approach etc.) to get root / original word form for <a href=""http://en.wikipedia.org/wiki/Diminutive"">diminutive</a> word forms?</p>

<p>Target language : Russian.
For example :</p>

<ul>
<li><em>собачка -> собака</em></li>
<li><em>кошечка -> кошка</em></li>
</ul>

<p>Thanks in advance!</p>
"
"25623171","Jape grammar to identify product release","2014-09-02 12:13:24","2","334","0","1","","25624092","<p>How can i use AND operation on jape grammar?. I just want to check whether a sentence contain 'organisation','jobtitle','person' all together in any order. How it possible? There is '|'(OR) operation allowed but i didnt see any documentation about AND operation. </p>
"
"25614279","Hunspell affix condition regex format. Any way to match the start?","2014-09-02 00:27:20","3","1002","0","1","","27910192","<p>Good day.</p>

<p>I'm trying to use Hunspell as a stemmer in my application. I don't quite like porter and snowball stemming because of their ""chopped"" words results like ""abus"", ""exampl"". Lemmatizing seems like a good alternative, but I don't know any good CoreNLP alternatives, and I'm certainly not ready to port my project's source code to Java or use bridges yet. Ideally I would like to see initial, like-in-the-dictionary form of the given word.</p>

<p>As I've noticed most of the dictionaries has separate words in .dic file for: bid and bidding, set and setting, get and getting, etc. I'm not that experienced in Hunspell, but isn't there any clever way to handle double d or t for 3-letter word? Is there a way to make it think that ""setting"" is actually is derivated from ""set""?</p>

<p>My current particular problem with Hunspell is I can't get a good comprehensive documentation for creating/editing an affix file. That's what documentations says here: <a href=""http://manpages.ubuntu.com/manpages/dapper/man4/hunspell.4.html"" rel=""nofollow"">http://manpages.ubuntu.com/manpages/dapper/man4/hunspell.4.html</a></p>

<pre><code>(4) condition.

Zero stripping or affix are indicated by zero. Zero condition is
indicated   by   dot. Condition is a simplified, regular
expression-like pattern, which must be met before the affix  can
be  applied. (Dot  signs  an arbitrary character. Characters in
braces sign an arbitrary character from  the  character  subset.
Dash  hasn’t  got  special  meaning, but circumflex (^) next the
first brace sets the complementer character set.)
</code></pre>

<p>Default one is this:</p>

<pre><code>SFX G Y 2
SFX G   e     ing        e
SFX G   0     ing        [^e] 
</code></pre>

<p>I've tried this one:</p>

<pre><code>SFX G Y 4
SFX G   e     ing        e
SFX G   0     ing        [^e] 
SFX G   0     ting       [bcdfghjklmnpqrstvwxz][aeiou]t 
SFX G   0     ding       [bcdfghjklmnpqrstvwxz][aeiou]d 
</code></pre>

<p>but it clearly will also match asSET. Is there any way to get around it somehow? I've tried ^ symbol at the start of regexp, but it seems like it's not working. What can I do to make it work?</p>

<p>Thanks in advance.</p>
"
"25575668","NLP library for POS tagging","2014-08-29 20:22:09","-4","174","0","2","","25577479","<p>I am looking for a reputable Java, Open Source (preferably) library/package that takes text as an input and identifies and marks Parts of Speech in it. </p>

<p>Components like:</p>

<pre><code>Verbs + Tense + Passive/Active {Simple Present, Past Progressive, Past Passive, Present Perfect ... } 
Prepositions of movement {from, to...}
Prepositions of time and place {in, at, on...}
Adverbs of manner {fast, here, outside ... }
Comparatives {more, less ... }
Superlatives {most, least ... }
Adverbs of quantity {many, all... }
Conditionals 
Relative pronouns
Relative adverbs
Modal Verbs
</code></pre>

<p>This list is something I found online, but I am sure there is a better, standard tagging to do it.</p>
"
"25330079","where can I get training data of part-of-speech tagger?","2014-08-15 16:12:29","4","6648","1","2","","25340657","<p>I want to implement a part-of-speech tagger,but I don't know where I can get a lot of training data?
Thanks!</p>
"
"25293346","Lemmatization makes corpus huge","2014-08-13 18:17:52","0","543","3","1","","25350663","<p>using ipython 2.7 and a corpus with non-Ascii chars.</p>

<p>The cleansing process seems to be fine, but once I use either Wordnet or Porter to lemmatize the corpus, the size of the file increases exponentially.
Please see code below</p>

<pre><code> from nltk.corpus import stopwords

 tokenized_docs_no_stopwords = []
 for doc in tokenized_docs_no_punctuation:
         new_term_vector = []
         for word in doc:
         if not word in stopwords.words('english'):
         new_term_vector.append(word)
tokenized_docs_no_stopwords.append(new_term_vector)
</code></pre>

<p>and the routine</p>

<pre><code> from nltk.stem.porter import PorterStemmer

 from nltk.stem.wordnet import WordNetLemmatizer

 porter = PorterStemmer()

 wordnet = WordNetLemmatizer()

  preprocessed_docs = []
 for doc in tokenized_docs_no_stopwords:
       final_doc = []
       for word in doc:
       final_doc.append(porter.stem(word))
       #final_doc.append(snowball.stem(word))
       #final_doc.append(wordnet.lemmatize(word))
   preprocessed_docs.append(final_doc)
</code></pre>

<p>Seems to make the corpus 10 times bigger.
Is the objective of removing stops words and lemmaising not supposed to reduce the corpus size?</p>

<p>I have tried adjusting the indentation, but I have a feeling there might be a more efficient loop than the 'append' routine, but I am more concerned about the exponential memory increase.</p>

<p>i am working off the example here</p>

<p><a href=""http://stanford.edu/~rjweiss/public_html/IRiSS2013/text2"" rel=""nofollow"">http://stanford.edu/~rjweiss/public_html/IRiSS2013/text2</a>
Any help or direction would be most appreciated</p>
"
"25269369","Combining Pickle files to make one big NLTK classifier","2014-08-12 16:23:05","0","747","5","1","","25275238","<p>I know how to simply make and load one pickle file, what I want to do is:
Have one NLTK naive bayes classifier, that is made from multiple pickle files. Is this possible? I want to know how to make a pickle file out of the classifier, and then append more things learned from other training data so that I have one bigger pickle file. </p>

<p>The main reason is that while it takes my laptop about 2 minutes to train a classifier on about 3500 articles, it takes waaay too long for it to make one big pickle file out of that classifier. I think it's because it's taking up more than 90% of my RAM at that point and I was hoping I could sort of divide and conquer so it's not taking all my RAM at once. Or is there a better way to train off a large number of documents? Will I have to implement my own classifier or pickle-like file to do this? Or is there a way to accomplish this?</p>

<p>I've tried use del() on some variables that I stopped using before pickling but that hardly freed up the amount of RAM those variables took up.</p>

<p>I'm using python 2.7 with Anaconoda on windows 8 64 bit.</p>

<p>Thanks!</p>
"
"25145552","TFIDF for Large Dataset","2014-08-05 18:09:09","49","35334","6","4","","25168689","<p>I have a corpus which has around 8 million news articles, I need to get the TFIDF representation of them as a sparse matrix. I have been able to do that using scikit-learn for relatively lower number of samples, but I believe it can't be used for such a huge dataset as it loads the input matrix into memory first and that's an expensive process.</p>

<p>Does anyone know, what would be the best way to extract out the TFIDF vectors for large datasets?</p>
"
"25108053","What is the accuracy of nltk pos_tagger?","2014-08-03 19:01:47","2","2984","1","1","","25109406","<p>I'm writing a dissertation, and using nltk.pos_tagger in my work. I can't find any information about what the accuracy of this algorithm. Does anybody know where can I find such information?</p>
"
"25106997","What does k fold validation mean in the context of POS tagging?","2014-08-03 17:03:58","1","581","0","1","","25109586","<p>I know that for k-cross validation, I'm supposed to divide the corpus into k equal parts. Of these k parts, I'm to use k-1 parts for training and the remaining 1 part for testing. This process is to be repeated k times, such that <em>each</em> part is used once for testing.</p>

<p>But I don't understand <em>what exactly does training mean</em> and <em>what exactly does testing mean</em> . </p>

<p>What I think is (please correct me if I'm wrong):<br>
<strong><em>1.</em></strong> <strong>Training sets (k-1 out of k):</strong> These sets are to be used build to the Tag transition probabilities and Emission probabilities tables.  And then, apply some algorithm for tagging using these probability tables (Eg. Viterbi Algorithm)<br>
<strong><em>2.</em></strong> <strong>Test set (1 set):</strong> Use the remaining 1 set to validate the implementation done in step 1. That is, this set from the corpus will have untagged words and I should use the step 1 implementation <em>on</em> this set.</p>

<p>Is my understanding correct? Please explain if not.</p>

<p>Thanks.</p>
"
"24990527","How to get Coarse-grained Part of Speech Tags?","2014-07-28 07:38:04","4","1645","0","1","","26654565","<p>I have a data set which is annotated by Collins parser. Right now, I am keeping the POS of each word in the data set as a feature. The problem is that I don't need fine-grained POS. So, I have combined some of the tags. For example, I assume all VBD,VBP,VBZ,VBG under the category of ""Verb"". And for nouns, I assume NNP and NNS as ""Noun"" category.</p>

<p>So, here is the list of POS tags that I have after doing all combinations:</p>

<blockquote>
  <p>VB, NN, TO, JJ, IN, EX, RB, WP, PRP, MD, UH, WRB, WDT, RP, CD, POS, DT, PRP$, WP$, CC, RBR</p>
</blockquote>

<p>Now, my question is where can I find a list of coarse-grained POS tags? Is there any standard coarse-grained POS tag list?</p>

<p>In my system, If I don't combine other POS tags, I can get better results. I am wondering if I am allowed to keep my current list? Or should I combine them as well?</p>

<p>Thanks in advance,</p>
"
"24975573","How to parse custom tags using nltk.Regexp.parser()","2014-07-26 21:10:01","3","6045","3","2","","25005857","<p>My question is similar to this unanswered question: <a href=""https://stackoverflow.com/questions/23744659/using-custom-pos-tags-for-nltk-chunking"">Using custom POS tags for NLTK chunking?</a>, but the error I am getting is different. I am trying to parse a sentence to which I have added my own domain specific tags.</p>

<p>For example:</p>

<pre><code>(u'greatest', 'P'), (u'internet', 'NN'), (u'ever', 'A'), 
(u',', ','), (u'and', 'CC'), (u'its', 'PRP$'), (u'being', 'VBG'), 
(u'slow', 'N'), (u'as', 'IN'), (u'hell', 'NN')`
</code></pre>

<p>where <code>(u'slow', 'N')</code> is a custom tag <code>'N'</code>.</p>

<p>I am trying to parse this using the following:</p>

<pre><code>grammar=r""""""
Chunk:`{&lt;A&gt;?*&lt;P&gt;+}`
""""""
parser=nltk.RegexpParser(grammar)
</code></pre>

<p>But I am getting the following error:</p>

<pre><code>ValueError: Illegal chunk pattern: `{&lt;A&gt;?*&lt;P&gt;+}`
</code></pre>

<p>Does <code>nltk.RegexpParser</code> process custom tags? Is there any other nltk or python based parser which can do that?</p>
"
"24741750","Defining a MonadEither type class","2014-07-14 16:57:24","4","202","2","1","","24741909","<p>I'm going back through <a href=""http://www.cs.virginia.edu/~wh5a/personal/Transformers.pdf"" rel=""nofollow"">Monad Transformers : Step by Step</a> as a refresher, and like many tutorials out there, it uses <code>Control.Monad.Error</code>. GHC now gives a warning that this module is deprecated, so I switched over to <code>Control.Monad.Trans.Either</code> from the <code>either</code> library: <a href=""http://hackage.haskell.org/package/either-3.4/docs/Control-Monad-Trans-Either.html"" rel=""nofollow"">http://hackage.haskell.org/package/either-3.4/docs/Control-Monad-Trans-Either.html</a></p>

<p>Everything is handled smoothly with <code>eval2</code> in the paper, since <code>EitherT</code> is the outermost monad. However, after that everything falls apart -- <code>ReaderT</code> is in no way an <code>Either</code> value, and everything henceforth uses <code>ErrorT</code>, which I'd like to change to <code>EitherT</code>.</p>

<p>My idea, then, was to define a <code>MonadEither</code> type class that boxed <code>left</code> and <code>right</code> in order to handle errors, but this hasn't been fruitful. I don't really understand how the type classes in <code>mtl</code> work, and this instance in particular has to be parameterized over multiple values, which is confusing. I came up with the following, which compiles after including some syntactic extensions:</p>

<pre><code>class (Monad m) =&gt; MonadEither l r m | m -&gt; r where
  right :: r -&gt; m r
  left  :: l -&gt; m r
</code></pre>

<p>But I can't figure out a <code>MonadEither</code> instance of <code>EitherT</code>:</p>

<pre><code>instance Monad m =&gt; MonadEither l r (E.EitherT l m) where
  right = E.right
  left  = E.left
</code></pre>

<p>Edit: I changed the instance declaration to match <code>E.EitherT</code> properly, and get the following error message:</p>

<pre><code>Illegal instance declaration for ‘MonadEither l r (E.EitherT l m)’
  The coverage condition fails in class ‘MonadEither’
    for functional dependency: ‘m -&gt; r’
  Reason: lhs type ‘E.EitherT l m’ does not determine rhs type ‘r’
In the instance declaration for ‘MonadEither l r (E.EitherT l m)’ 
</code></pre>

<p>Again, I'm not really sure what I'm doing. I don't really understand functional dependencies, so I'm just looking for some guidance as to what an appropriate <code>MonadEither</code> type class might look like, if possible to define.</p>
"
"24647400","What is the best stemming method in Python?","2014-07-09 07:12:45","46","89753","3","7","","24648116","<p>I tried all the nltk methods for stemming but it gives me weird results with some words. </p>

<p>Examples</p>

<p>It often cut end of words when it shouldn't do it :</p>

<ul>
<li>poodle => poodl</li>
<li>article articl</li>
</ul>

<p>or doesn't stem very good : </p>

<ul>
<li>easily and easy are not stemmed in the same word</li>
<li>leaves, grows, fairly are not stemmed</li>
</ul>

<p>Do you know other stemming libs in python, or a good dictionary?</p>

<p>Thank you</p>
"
"24534699","parsing a sentence using Stanford parser with nltk in python","2014-07-02 15:08:38","0","1177","0","1","","24542218","<p>I am working with a project in python where i need to parse sentences for comparisons and finding similarity between sentences.</p>

<p>I have seen how to parse sentences using stanford parser in java, for the same to do in python,I have downloaded nltk for python.I am new to both python and natural language processing. </p>

<p>I would like to know what are the methods and libraries available with nltk. Any help with possible resources is appreciated. </p>

<p>Thanks in advance. </p>
"
"24456056","python.NLTK (WindowDiff and PK) vs python.Segeval (WindowDiff and PK)","2014-06-27 15:56:10","3","877","5","1","","32754997","<p><strong>Python NLTK implementation of Beeferman's PK and WindowDIFF are getting complete different results from python segeval implementation of both.</strong> </p>

<p>Using the same parameters.</p>

<pre><code>hyp: 0100100000
ref: 0101000000
k=2
PK's SegEval:0.2222222
PK's NLTK:0.111111111

hyp: 111111
ref: 100100
k=2
PK's SegEval:0.4
PK's NLTK:0.64
</code></pre>

<p>This could lead different research results for who use it.<br>
Why I am getting different results with PK in these 2 Implementations? PK has to have just one result.</p>
"
"24382900","GATE Jape rule annotation consisting of two words","2014-06-24 09:21:51","0","405","0","1","","24383610","<p>I am trying to write a JAPE rule that will catch the annotation ""Job Title"". However, because it consists of two words I couldn't find a way to write it after the Input: section. </p>

<p>Thit is what I am trying to do:</p>

<p>Phase: ...
Input: Job Title
...</p>

<p>However, if I write it like that it will read it as look for annotations Job and Title, and not Job Title.</p>
"
"24363013","startProbability in hidden markov models","2014-06-23 09:53:50","0","117","0","1","","24369801","<p>How can I calculate the startProbabilitise in hidden markov model for POS tagging? is that means: (# repeats of one tag)/(# repeats of all tags)? or (number of sentences beginning with one tag)/(all sentences)? if not, what?</p>
"
"24337031","Word sense disambiguation for pair of words","2014-06-20 23:24:07","2","1695","0","1","","24337089","<p>Say that I have a word A and a word B, where I use B as a hint which implies the meaning of A. For instance, A = bass, B = music, given this word pair, as human beings we can immediately know what does the word A mean. </p>

<p>I know that there are lots of algorithms that work for sentences. I'm wondering if there has been algorithms developed for doing WSD only for a pair of words. </p>
"
"24298677","document similarity with documents using synonyms","2014-06-19 04:15:27","0","197","1","2","","24305819","<p>I have a bunch of documents where some of the documents are a copy of other documents with their text jumbled up and some of the words replaced by their synonyms. Mentioned below is one such example of a sentence:</p>

<blockquote>
  <p><strong><em>Article 1 (original)</em></strong> : I caught up with John Snow in town making purchases at Kingslanding Hardware store to repair a broken tractor.  Snow has farmed soybeans his entire life, as did his father and their fathers. I asked him about his life on the farm.</p>
  
  <p><strong><em>Article 2 (duplicate)</em></strong> : I obtained John Snow which in city in purchases make rise of the hardware at Kingslanding to repair a broken motor tractor.  Snow have soya broad beans complete life have been treated, such as its father and their fathers. I asked him concerning its life on the agriculture company.</p>
  
  <p><strong><em>Article 3 (duplicate)</em></strong> : I took for above with John Snow in the city that made purchases in the warehouse of the hardware of Kingslanding to repair an broken tractor.  Snow has cultivated the soybeans its whole life, like its father and his parents. I asked to him about its life in the farm.</p>
  
  <p><strong><em>Article 4 (duplicate)</em></strong> : I caught up with myself compared to John Snow downtown making of the purchases to the kingslanding store of material to repair a broken tractor.  Snow cultivated soya its life whole, just as his/her father and their fathers. I questioned it about his life with the farm.</p>
</blockquote>

<p>I want to do a document similarity which ends up tagging all these documents in the same group. Any suggestions along with examples or tutorials will be greatly appreciated. </p>
"
"24105887","Scikit-learn TfidfTranformer yielding wrong results?","2014-06-08 11:56:00","2","915","0","1","","24106239","<p>I'm getting ""weird"" results using scikit-learn's Tfidf transformer. Normally, I would expect a word, that occurs in all documents in a corpus to have an idf equal to 0 (using no sort of smoothing or normalization), as the formular I would use would be the logarithm of the number of document in the corpus divided by the number of documents containing the term. Apparently (as illustrated below) scikit-learn's implementation adds one to each idf value compared to my manual implementation. Does anybody know why? Again, notice that I have set smoothing and normalization equal to None/False.</p>

<pre><code>In [101]: from sklearn.feature_extraction.text import TfidfTransformer

In [102]: counts
Out[102]: 
array([[3, 0, 1],
       [2, 0, 0],
       [3, 0, 0],
       [4, 0, 0],
       [3, 2, 0],
       [3, 0, 2]])

In [103]: transformer = TfidfTransformer(norm=None, smooth_idf=False)

In [104]: transformer
Out[104]: 
TfidfTransformer(norm=None, smooth_idf=False, sublinear_tf=False,
         use_idf=True)

In [105]: tfidf = transformer.fit_transform(counts)

In [106]: tfidf.toarray()
Out[106]: 
array([[ 3.        ,  0.        ,  2.09861229],
       [ 2.        ,  0.        ,  0.        ],
       [ 3.        ,  0.        ,  0.        ],
       [ 4.        ,  0.        ,  0.        ],
       [ 3.        ,  5.58351894,  0.        ],
       [ 3.        ,  0.        ,  4.19722458]])

In [107]: transformer.idf_
Out[107]: array([ 1.        ,  2.79175947,  2.09861229])

In [108]: idf1 = np.log(6/6)

In [109]: idf1
Out[109]: 0.0

In [110]: idf2 = np.log(6/1)

In [111]: idf2
Out[111]: 1.791759469228055

In [112]: idf3 = np.log(6/2)

In [113]: idf3
Out[113]: 1.0986122886681098
</code></pre>

<p>I have been unable to find any source that justifies adding one to the idf values. I'm using scikit-learn version '0.14.1'.</p>

<p>Btw another solution than scikit-learn is not really useful to me, as I need to build a scikit-learn pipeline for gridsearch.</p>
"
"23877375","word2vec lemmatization of corpus before training","2014-05-26 20:35:36","32","16843","3","2","","23885178","<p>Word2vec seems to be mostly trained on raw corpus data. However, lemmatization is a standard preprocessing for many semantic similarity tasks. I was wondering if anybody had experience in lemmatizing the corpus before training word2vec and if this is a useful preprocessing step to do.</p>
"
"23813611","Parsing messy texts with Stanford Parser","2014-05-22 17:32:00","0","949","1","2","","23836216","<p>I am running Stanford Parser on a large chunk of texts. The parser terminates when it hits a sentence it cannot parse, and gives the following runtime error. Is there a way to make Stanford Parser ignore the error, and move on to parsing the next sentence?</p>

<p>One way is to break down the text into a myriad of one-sentence documents, and parse each document and record the output. However, this involves loading the Stanford Parser many many times (each time a document is parsed, the Stanford Parser has to be reloaded). Loading the parser takes a lot of time, but parsing takes much shorter time. It would be great to find a way to avoid having to reload the parser on every sentence.</p>

<p>Another solution might be to reload the parser once it hits an error, and picking up the texts where it stopped and continue parsing from there. Does anyone know of a good way to implements this solution?</p>

<p>Last but not least, does there exist any Java wrapper that ignores errors and keeps a Java program running until the program terminates naturally?</p>

<p>Thanks!</p>

<pre><code>Exception in thread ""main"" java.lang.RuntimeException: CANNOT EVEN CREATE ARRAYS OF ORIGINAL SIZE!!
at edu.stanford.nlp.parser.lexparser.ExhaustivePCFGParser.considerCreatingArrays(ExhaustivePCFGParser.java:2190)
at edu.stanford.nlp.parser.lexparser.ExhaustivePCFGParser.parse(ExhaustivePCFGParser.java:347)
at edu.stanford.nlp.parser.lexparser.LexicalizedParserQuery.parseInternal(LexicalizedParserQuery.java:258)
at edu.stanford.nlp.parser.lexparser.LexicalizedParserQuery.parse(LexicalizedParserQuery.java:536)
at edu.stanford.nlp.parser.lexparser.LexicalizedParserQuery.parseAndReport(LexicalizedParserQuery.java:585)
at edu.stanford.nlp.parser.lexparser.ParseFiles.parseFiles(ParseFiles.java:213)
at edu.stanford.nlp.parser.lexparser.ParseFiles.parseFiles(ParseFiles.java:73)
at edu.stanford.nlp.parser.lexparser.LexicalizedParser.main(LexicalizedParser.java:1535)
</code></pre>
"
"23657885","How to get NMecab to output romaji?","2014-05-14 14:42:03","2","772","0","1","","23735101","<p>I'm using <a href=""http://sourceforge.jp/projects/nmecab/"" rel=""nofollow noreferrer"">a .NET port of Mecab (called NMecab)</a> to try to parse Japanese Hiragana, Katakana, and Kanji to romaji.</p>

<p>Here's my code:</p>

<pre><code>using NMeCab;    
MeCabTagger _tagger;

public string Parse(string input)
{
    _tagger = MeCabTagger.Create();
    _tagger.OutPutFormatType = ""lattice"";
    _tagger.LatticeLevel = MeCabLatticeLevel.Two;


    var output = _tagger.Parse(input);

    return output;
}
</code></pre>

<p>When I call <code>Parse(input)</code> using the following Japanese text: ""ども""</p>

<p>I get the output: ""ども助詞,接続助詞,<em>,</em>,<em>,</em>,ども,ドモ,ドモ EOS""</p>

<p>I'm looking for the romaji of ""ども"", which would be ""domo.""</p>

<p>I've tried to use Mecab directly as <a href=""https://stackoverflow.com/questions/6365931/trying-to-get-libmecab-dll-mecab-to-work-with-c-sharp"">discussed in this SO answer</a>, but get the same output.</p>
"
"23506732","Compare two phrases using WordNet?","2014-05-07 00:27:59","0","1471","0","1","","23527690","<p>I am trying to compare the semantic of two phrases.
In Python I am using nltk and difflib.
First I am removing the stop words from the phrases, then I am using WordNetLemmatizer and PorterStemmer to normalise the words then I am comparing the rest with the SequenceMatcher of difflib.
I still think that there is a much better way than using difflib.
Any suggestion or propostion?
Is there any library that use Wordnet in the comparision between phrases?
Is the steps I am making are correct?</p>
"
"23319311","GoLang PoS Tagger script taking longer than it should with no output in terminal","2014-04-27 04:13:58","0","723","0","2","","23320561","<p>This script is compling without errors in play.golang.org: <a href=""http://play.golang.org/p/Hlr-IAc_1f"" rel=""nofollow"">http://play.golang.org/p/Hlr-IAc_1f</a></p>

<p>But when I run in on my machine, much longer than I expect happens with nothing happening in the terminal.</p>

<p>What I am trying to build is a PartOfSpeech Tagger.</p>

<p>I think the longest part is loading lexicon.txt into a map and then comparing each word with every word there to see if it has already been tagged in the lexicon. The lexicon only contains verbs. But doesn't every word need to be checked to see if it is a verb.</p>

<p>The larger problem is that I don't know how to determine if a word is a verb with an easy heuristic like adverbs, adjectives, etc.</p>
"
"23318769","Why cowardly becomes cowardli after stemming?","2014-04-27 02:38:59","1","2220","3","1","","23328299","<p>I noticed that after applying Porter stemming (from NLTK library) I get strange stems such as <em>""cowardli""</em> or <em>""contrari""</em>. For me they don't look like stems at all. </p>

<p>Is it okay? Could it be that I made a mistake smwhere?</p>

<p>Here is my code:</p>

<pre><code>string = string.lower()
tokenized = nltk.tokenize.regexp_tokenize(string,""[a-z]+"")
filtered = [w for w in tokenized if w not in nltk.corpus.stopwords.words(""english"")]


stemmer = nltk.stem.porter.PorterStemmer()
stemmed = []
for w in filtered:
    stemmed.append(stemmer.stem(w))
</code></pre>

<p>And here is the text I used for processing <a href=""http://pastebin.com/XUMNCYAU"" rel=""nofollow"">http://pastebin.com/XUMNCYAU</a> (beginning of ""crime and punishment"" book by Dostoevsky).</p>
"
"23230140","Why is this not LL(1) grammar?","2014-04-22 21:02:36","1","992","3","1","","23230720","<p>S -> 1S2 | S0 | epsilon </p>

<p>I thought it would be LL(1) because it is possible to determine. </p>

<p>For example if the next input symbol was 0 i'd know it was S -> S0 </p>

<p>Does the epsilon mean that it cannot be LL(1)?</p>
"
"23117979","The distance between the meaning of two sentences","2014-04-16 19:02:04","4","5123","2","3","","53465438","<p>I am looking for a way to measure the semantic distance between two sentences. Suppose we have the following sentences:</p>

<pre><code>(S1) The beautiful cherry blossoms in Japan. 
(S2) The beautiful Japan.
</code></pre>

<p>S2 is created from S1 by eliminating the words ""cherry"", ""blossoms"" and ""in"". I want to define a function that gives a high distance between S1 and S2. The reason for this is that they do have significantly different meaning, since beautiful modifies cherry blossoms and not Japan.</p>
"
"23086961","StanfordCoreNLP does not work in my way","2014-04-15 14:39:54","0","1045","0","1","","23099837","<p>I use below code. However, the outcome is not what I expected. The outcome is <code>[machine, Learning]</code>
But I want to get <code>[machine, learn]</code>. How can I do this? Also, when my input is <code>""biggest bigger""</code>, I wanna get the result like <code>[big, big]</code>, but the outcome is just <code>[biggest bigger]</code></p>

<p>(PS: I just add these four jars in my eclipse:<code>joda-time.jar, stanford-corenlp-3.3.1-models.jar, stanford-corenlp-3.3.1.jar, xom.jar</code> Do I need add some more?)</p>

<pre><code>import java.util.LinkedList;
import java.util.List;
import java.util.Properties;

import edu.stanford.nlp.ling.CoreAnnotations.LemmaAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.SentencesAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.TokensAnnotation;
import edu.stanford.nlp.ling.CoreLabel;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.util.CoreMap;

public class StanfordLemmatizer {

    protected StanfordCoreNLP pipeline;

    public StanfordLemmatizer() {
        // Create StanfordCoreNLP object properties, with POS tagging
        // (required for lemmatization), and lemmatization
        Properties props;
        props = new Properties();
        props.put(""annotators"", ""tokenize, ssplit, pos, lemma"");


        this.pipeline = new StanfordCoreNLP(props);
    }

    public List&lt;String&gt; lemmatize(String documentText)
    {
        List&lt;String&gt; lemmas = new LinkedList&lt;String&gt;();
        // Create an empty Annotation just with the given text
        Annotation document = new Annotation(documentText);
        // run all Annotators on this text
        this.pipeline.annotate(document);
        // Iterate over all of the sentences found
        List&lt;CoreMap&gt; sentences = document.get(SentencesAnnotation.class);
        for(CoreMap sentence: sentences) {
            // Iterate over all tokens in a sentence
            for (CoreLabel token: sentence.get(TokensAnnotation.class)) {
                // Retrieve and add the lemma for each word into the
                // list of lemmas
                lemmas.add(token.get(LemmaAnnotation.class));
            }
        }
        return lemmas;
    }


    // Test
    public static void main(String[] args) {
        System.out.println(""Starting Stanford Lemmatizer"");
        String text = ""Machine Learning\n"";
        StanfordLemmatizer slem = new StanfordLemmatizer();
        System.out.println(slem.lemmatize(text));
    }

}
</code></pre>
"
"22999273","Python NLTK Lemmatization of the word 'further' with wordnet","2014-04-10 21:23:14","3","6090","1","1","","23005526","<p>I'm working on a lemmatizer using python, NLTK and the WordNetLemmatizer.
Here is a random text that output what I was expecting</p>

<pre><code>from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet
lem = WordNetLemmatizer()
lem.lemmatize('worse', pos=wordnet.ADJ) // here, we are specifying that 'worse' is an adjective
</code></pre>

<p>Output: <code>'bad'</code></p>

<pre><code>lem.lemmatize('worse', pos=wordnet.ADV) // here, we are specifying that 'worse' is an adverb
</code></pre>

<p>Output: <code>'worse'</code></p>

<p>Well, everything here is fine. The behaviour is the same with other adjectives like <code>'better'</code> (for an irregular form) or <code>'older'</code> (note that the same test with <code>'elder'</code> will never output <code>'old'</code>, but I guess that wordnet is not an exhaustive list of all the existing english word)</p>

<p>My question comes when trying with the word <code>'furter'</code>:</p>

<pre><code>lem.lemmatize('further', pos=wordnet.ADJ) // as an adjective
</code></pre>

<p>Output: <code>'further'</code></p>

<pre><code>lem.lemmatize('further', pos=wordnet.ADV) // as an adverb
</code></pre>

<p>Output: <code>'far'</code></p>

<p>This is the exact opposite behaviour of the one for the <code>'worse'</code> word!</p>

<p>Can anybody explain me why ? Is it a bug coming from the wordnet synsets data or does it come from my misunderstanding of the english grammar ?</p>

<p>Please excuse me if the question is already answered, I've search on google and SO, but when specifying the keyword ""further"", I can find anything related but mess because of the popularity of this word...</p>

<p>Thank you in advance,
Romain G.</p>
"
"22993796","Lemmatizer in R or python (am, are, is -> be?)","2014-04-10 16:31:40","2","3970","1","1","","22994954","<p>I'm not a [computational] linguistic, so please excuse my supper dummy-ness in this topic.</p>

<p>According to Wikipedia, lemmatisation is defined as:</p>

<blockquote>
  <p>Lemmatisation (or lemmatization) in linguistics, is the process of grouping together the different inflected forms of a word so they can be analysed as a single item.</p>
</blockquote>

<p>Now my question is, is the lemmatised version of any member of the set {am, is, are} supposed to be ""be""? If not, why not?</p>

<p>Second question: How do I get that in R or python? I've tried methods like <a href=""https://stackoverflow.com/questions/14952215/wordnet-lemmatizer-for-r"">this</a> link, but non of them gives ""be"" given ""are"". I guess at least for the purpose of classifying text documents, this makes sense to be true.</p>

<p>I also couldn't do that with any of the given demos <a href=""http://text-processing.com/demo/"" rel=""nofollow noreferrer"">here</a>.</p>

<p>What am I doing/assuming wrong?</p>
"
"22930328","Error using Stanford POS Tagger in NLTK Python","2014-04-08 07:27:05","11","9281","7","1","","23181701","<p>I am trying to use Stanford POS Tagger in NLTK but I am not able to run the example code given here <a href=""http://www.nltk.org/api/nltk.tag.html#module-nltk.tag.stanford"" rel=""noreferrer"">http://www.nltk.org/api/nltk.tag.html#module-nltk.tag.stanford</a></p>

<pre><code>import nltk
from nltk.tag.stanford import POSTagger
st = POSTagger(r'english-bidirectional-distim.tagger',r'D:/stanford-postagger/stanford-postagger.jar')
st.tag('What is the airspeed of an unladen swallow?'.split())
</code></pre>

<p>I have already added environment variables as</p>

<pre><code>CLASSPATH = D:/stanford-postagger/stanford-postagger.jar
STANFORD_MODELS =  D:/stanford-postagger/models/
</code></pre>

<p>Here is the error I keep getting</p>

<p>Traceback (most recent call last):</p>

<pre><code>File ""D:\pos_stanford.py"", line 4, in &lt;module&gt;
    st = POSTagger(r'english-bidirectional-distim.tagger',
         r'D:/stanford-postagger/stanford-postagger.jar')  
... LookupError: NLTK was unable to find the english-bidirectional-distim.tagger file! Use software specific configuration paramaters or set the STANFORD_MODELS environment variable.
</code></pre>

<p>Some forums suggest that </p>

<pre><code>File ""C:\Python27\lib\site-packages\nltk\tag\stanford.py"", line 45, in __init__
env_vars=('STANFORD_MODELS'), verbose=verbose)
</code></pre>

<p>should be changed so that there is a comma in </p>

<pre><code>env_vars=('STANFORD_MODELS',), verbose=verbose)
</code></pre>

<p>but it doesn't solve the problem either. 
Please Help me in solving this issue.</p>

<p>Other Information:
I am using
Windows 7 64 bit
Python 2.7 32 bit
NLTK 2.0</p>
"
"22905919","Split texts into sentences fast (Java)","2014-04-07 07:21:10","2","3144","5","5","","22999045","<p>I have a set of article descriptions where I have to split the texts into sentences. The first implementation uses the opennlp tool sentdetect which works very well, but is too slow for my purpose. 
Is there anything similar to this which performs faster and has an outcome of a similar or slightly worse quality?</p>

<p>Note: I'm working with (a huge amount of) short redactional german texts. </p>
"
"22854710","Is it posible to use Office SpellCheck API for POS tagging?","2014-04-04 05:59:51","1","94","0","1","","22863065","<p>I am new in this filed and have no idea if NLP and POS tagging can found in Office SpellCheck API. </p>

<p>Since Office SpellCheck is some kind of language processing, is there possibility to use it as POS tagger.</p>
"
"22849919","How optimize word counting in Python?","2014-04-03 22:06:30","0","451","0","2","","22850033","<p>I'm taking my first steps writing code to do linguistic analysis of texts. I use Python and the NLTK library. The problem is that the actual counting of words takes up close to 100 % of my CPU (iCore5, 8GB RAM, macbook air 2014) and ran for 14 hours before I shut the process down. How can I speed the looping and counting up? </p>

<p>I have created a corpus in NLTK out of three Swedish UTF-8 formatted, tab-separated files Swe_Newspapers.txt, Swe_Blogs.txt, Swe_Twitter.txt. It works fine:</p>

<pre><code>import nltk
my_corpus = nltk.corpus.CategorizedPlaintextCorpusReader(""."", r""Swe_.*"", cat_pattern=r""Swe_(\w+)\.txt"")
</code></pre>

<p>Then I've loaded a text-file with one word per line into NLTK. That also works fine.</p>

<pre><code>my_wordlist = nltk.corpus.WordListCorpusReader(""/Users/mos/Documents/"", ""wordlist.txt"")
</code></pre>

<p>The text-file I want to analyse (Swe_Blogs.txt) has this structure, and works fine to parse:</p>

<pre><code>Wordpress.com   2010/12/08  3   1,4,11  osv osv osv …
bloggagratis.se 2010/02/02  3   0   Jag är utled på plogade vägar, matte är lika utled hon.
wordpress.com   2010/03/10  3   0   1 kruka Sallad, riven
</code></pre>

<p><strong>EDIT:</strong> The suggestion to produce the counter as below, does not work, but can be fixed:</p>

<pre><code>counter = collections.Counter(word for word in my_corpus.words(categories=[""Blogs""]) if word in my_wordlist)
</code></pre>

<p>This produces the error:</p>

<pre><code>IOError                                   Traceback (most recent call last)
&lt;ipython-input-41-1868952ba9b1&gt; in &lt;module&gt;()
----&gt; 1 counter = collections.Counter(word for word in my_corpus.words(""Blogs"") if word    in my_wordlist)
       /usr/local/Cellar/python/2.7.6/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/nltk/corpus/reader/plaintext.pyc in words(self, fileids, categories)
182     def words(self, fileids=None, categories=None):
183         return PlaintextCorpusReader.words(
--&gt; 184             self, self._resolve(fileids, categories))
185     def sents(self, fileids=None, categories=None):
186         return PlaintextCorpusReader.sents(

                /usr/local/Cellar/python/2.7.6/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site- packages/nltk/corpus/reader/plaintext.pyc in words(self, fileids, sourced)
 89                                            encoding=enc)
 90                            for (path, enc, fileid)
 ---&gt; 91                            in self.abspaths(fileids, True, True)])
 92 
 93 
 /usr/local/Cellar/python/2.7.6/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/nltk/corpus/reader/api.pyc in abspaths(self, fileids, include_encoding, include_fileid)
165             fileids = [fileids]
166 
--&gt; 167         paths = [self._root.join(f) for f in fileids]
168 
169         if include_encoding and include_fileid:  

/usr/local/Cellar/python/2.7.6/Frameworks/Python.framework/Versions/2.7/      lib/python2.7/site-packages/nltk/data.pyc in join(self, fileid)
174     def join(self, fileid):
175         path = os.path.join(self._path, *fileid.split('/'))
--&gt; 176         return FileSystemPathPointer(path)
177 
178     def __repr__(self):

/usr/local/Cellar/python/2.7.6/Frameworks/Python.framework/Versions/2.7/  lib/python2.7/site-packages/nltk/data.pyc in __init__(self, path)
152         path = os.path.abspath(path)
153         if not os.path.exists(path):
--&gt; 154             raise IOError('No such file or directory: %r' % path)
155         self._path = path

IOError: No such file or directory: '/Users/mos/Documents/Blogs'
</code></pre>

<p>A fix is to assign my_corpus(categories=[""Blogs""] to a variable:</p>

<pre><code>blogs_text = my_corpus.words(categories=[""Blogs""])
</code></pre>

<p>It's when I try to count all occurrences of each word (about 20K words) in the wordlist within the blogs in the corpus (115,7 MB) that my computer get's a little tired. How can I speed up the following code? It seems to work, no error messages, but it takes >14h to execute. </p>

<pre><code>import collections
counter = collections.Counter()

for word in my_corpus.words(categories=""Blogs""):
    for token in my_wordlist.words():
        if token == word:
            counter[token]+=1
        else:
            continue
</code></pre>

<p>Any help to improve my coding skills is much appreciated!</p>
"
"22544012","NLTK: Getting rid of parentheses and pos- tagger","2014-03-20 20:10:24","1","496","0","4","","22544071","<p>I have this code.</p>

<pre><code>from nltk import pos_tag, ne_chunk
import nltk.chunk
from nltk.corpus import names
qry = ""who is Ronald Avon""
tokens = nltk.tokenize.word_tokenize(qry)
pos = nltk.pos_tag(tokens)
sentt = nltk.ne_chunk(pos, binary = False)
person = []
for subtree in sentt.subtrees(filter=lambda t: t.node == 'PERSON'):
    for leave in subtree.leaves():
        person.append(leave)
print ""person="", person
</code></pre>

<p>It gets  names in a sentence. This is the result I get.</p>

<pre><code>person= [('Ronald', 'NNP'), ('Avon', 'NNP')]
</code></pre>

<p>How do i get the result like this:</p>

<pre><code>Ronald
Avon
</code></pre>

<p>without the 'NNP' and the parentheses. Thanks.</p>
"
"22507623","Existing API for NLP in C++?","2014-03-19 13:36:03","16","15740","3","6","","22671887","<p>Is/are there existing C++ NLP API(s) out there? The closest thing I have found is <code>CLucene</code>, a port of <code>Lucene</code>. However, it seems a bit obsolete and the documentation is far from complete.</p>

<p>Ideally, this/these API(s) would permit tokenization, stemming and PoS tagging.</p>
"
"22333392","Stemming some plurals with wordnet lemmatizer doesn't work","2014-03-11 18:23:55","3","11600","2","2","","22358166","<p>Hi i've a problem with nltk (2.0.4):
I'm trying to stemming the word 'men' or 'teeth' but it doesn't seem to work.
Here's my code:</p>

<pre><code>############################################################################
import nltk
from nltk.corpus import wordnet as wn
from nltk.stem.wordnet import WordNetLemmatizer

lmtzr=WordNetLemmatizer()
words_raw = ""men teeth""
words = nltk.word_tokenize(words_raw)
for word in words:
        print 'WordNet Lemmatizer NOUN: ' + lmtzr.lemmatize(word, wn.NOUN)
#############################################################################
</code></pre>

<p>This should print 'man' and 'tooth' but instead it prints 'men' and 'teeth'.</p>

<p>any solutions? </p>
"
"22213418","How can I give some POS information before Stanford NLP POS tagger execute?","2014-03-06 02:05:37","0","113","0","1","","22220624","<p>If I already know some word's POS information.</p>

<p>eg:I know <strong>st316(my id)</strong> is a Proper nouns <strong>(NR)</strong>.In the sentence""I am st316.""
   How can I make tagger use the Information that st316 is a NR,then decide the POS information of other words(I am).</p>

<p>Just like,</p>

<p>Input:<strong>I am st316/NR .</strong></p>

<p>Output: <strong>I/PN am/VC st316/NR ./PU</strong></p>

<p>Help me.Really thanks!</p>
"
"22156698","How to get a node level with Stanford dependency parser","2014-03-03 20:26:59","0","230","0","1","","22158711","<p>I would like to know if there is any method that allows to give the node level of the parse given by Stanford dependency parser. I haven't found the method that gives the node level. Thanks for the help.</p>
"
"22144324","Lemmatization of non-English words?","2014-03-03 10:31:09","8","12976","3","2","","22146747","<p>I would like to apply lemmatization to reduce the inflectional forms of words. I know that for English language WordNet provides such a functionality, but I am also interested in applying lemmatization for Dutch, French, Spanish and Italian words. Is there any trustworthy and confirmed way to go about this? Thank you!</p>
"
"21909165","ValueError: Could not find stanford-postagger.jar file for hazm library- python NLP","2014-02-20 13:24:50","3","1038","1","2","","21911062","<p>I want to run a code that need to stanford postagger.jar. but i have this error:</p>

<pre><code>  File ""/usr/lib/python2.7/site-packages/nltk/internals.py"", line 562, in find_jar
    (name, path_to_jar))
ValueError: Could not find stanford-postagger.jar jar file at resources/stanford-postagger.jar
</code></pre>

<p>How i can fix this error?</p>

<p><strong>EDIT:</strong>
i use from <a href=""https://github.com/sobhe/hazm"" rel=""nofollow"">hazm</a> module:</p>

<pre><code>from hazm import POSTagger
tagger = POSTagger()
tagger.tag(word_tokenize('ما بسیار کتاب می‌خوانیم'))
</code></pre>

<p>and full result:</p>

<pre><code>Traceback (most recent call last):
  File ""pyt.py"", line 8, in &lt;module&gt;
    tagger = POSTagger()
  File ""/home/vahid/dev/hazm/hazm/POSTagger.py"", line 14, in __init__
    super(stanford.POSTagger, self).__init__(*args, **kwargs)
  File ""/usr/lib/python2.7/site-packages/nltk/tag/stanford.py"", line 42, in __init__
    verbose=verbose)
  File ""/usr/lib/python2.7/site-packages/nltk/internals.py"", line 562, in find_jar
    (name, path_to_jar))
ValueError: Could not find stanford-postagger.jar jar file at resources/stanford-postagger.jar
</code></pre>
"
"21883108","Fast/Optimize N-gram implementations in python","2014-02-19 14:16:01","16","8019","5","3","","21988533","<p>Which ngram implementation is fastest in python?</p>

<p>I've tried to profile nltk's vs scott's zip (<a href=""http://locallyoptimal.com/blog/2013/01/20/elegant-n-gram-generation-in-python/"">http://locallyoptimal.com/blog/2013/01/20/elegant-n-gram-generation-in-python/</a>):</p>

<pre><code>from nltk.util import ngrams as nltkngram
import this, time

def zipngram(text,n=2):
  return zip(*[text.split()[i:] for i in range(n)])

text = this.s

start = time.time()
nltkngram(text.split(), n=2)
print time.time() - start

start = time.time()
zipngram(text, n=2)
print time.time() - start
</code></pre>

<p><strong>[out]</strong></p>

<pre><code>0.000213146209717
6.50882720947e-05
</code></pre>

<p>Is there any faster implementation for generating ngrams in python?</p>
"
"21882460","Filter Specific Part of Speech NLTK","2014-02-19 13:49:22","3","4702","0","1","","21882522","<p>this must be simple but I'm missing it somehow.  I have the code:</p>

<pre><code>import nltk

f=open('...\\t.txt','rU')
raw=f.read()
tokens = nltk.word_tokenize(raw)
print nltk.pos_tag(tokens)
</code></pre>

<p>which returns for instance:</p>

<p>""[('processes', 'NNS'), ('a', 'DT'), ('sequence', 'NN'), ('of', 'IN'), ('words', 'NNS')]</p>

<p>I was wondering how I could just collected solely all 'NN' for example or all 'DT' AND 'IN' instead of every member of the string.</p>

<p>thanks in advance</p>
"
"21871374","Is there any Part-Of-Speech tagger in C#?","2014-02-19 04:53:39","8","4354","0","1","","21874616","<p>My data pre-processing for data clustering needs <strong>part of speech (POS)</strong> tagging. I am wondering if there's some library in C# ready for this.</p>
"
"21818128","Detecting Similarity in Strings","2014-02-16 23:16:22","-1","146","1","1","","21818229","<p>If I search for something on Google News, I can click on the ""Explore in depth"" button and get the same news article from multiple sources. What kind of algorithm is used to compare articles of text and then determine that it is regarding the same thing? I have seen the Question here: </p>

<p><a href=""https://stackoverflow.com/questions/62328/is-there-an-algorithm-that-tells-the-semantic-similarity-of-two-phrases"">Is there an algorithm that tells the semantic similarity of two phrases</a> </p>

<p>However, using methods mentioned there, I feel that if there were articles that were similar in nature but regarding different stories, they would be grouped together using the methods mentioned there. Is there a standard way of detecting Strings that are about the same thing and grouping them, while keeping Strings that are just similar separate? Eg. If I search ""United States Border"" I might get stories about problems at the USA's border, but what would prevent these from all getting grouped together? All I can think of is the date of publication, but what if many stories were published very close to each other?</p>
"
"21815475","Is TF-IDF necessary when using SVM?","2014-02-16 18:23:25","2","2566","0","1","","21816269","<p>I'm using Support Vector Machines to classify phrases. Before using the SVM, I understand I should do some kind of normalization on the phrase-vectors. One popular method is TF-IDF.</p>

<p>The terms with the highest TF-IDF score are often the terms that best characterize the topic of the document.</p>

<p>But isn't that exactly what SVM does anyway? Giving the highest weight to the terms that best characterize the document?</p>

<p>Thanks in advance :-)</p>
"
"21718632","Detecting first/second/third person pronouns","2014-02-12 04:25:43","2","4881","0","1","","21725416","<p>I'm looking for a way to detect whether a personal pronoun is first person (I), second person (you) or third person (they). The code is looking to see if someone is talking about themselves, but has some other applications too.</p>

<p>A python library would be fantastic, but not necessary. nltk.pos_tag will tell me what are personal pronouns, but I can't seem to get more information than that.</p>

<p>Does something like this exist?</p>
"
"21548667","Scala - parse phrase - parsing combinator or NLP?","2014-02-04 09:36:56","1","125","1","1","","21559513","<p>I would like to answer questions such similar to these examples:</p>

<ul>
<li>""23 jul cinema in Paris"" --> returns the list of cinema shows in Paris for this date.</li>
<li>""23/07 cultural activities New Jersey"" --> returns the list</li>
<li>""next week concert of classical music Spain"" --> returns the list</li>
</ul>

<p>How do I go about this? Should I use ScalaNLP, a parsing Combinator or something else?</p>

<p>My second question is how are date formats parsed?</p>
"
"21387489","running weka over a large arff dataset file","2014-01-27 17:26:49","0","555","0","1","","21388258","<p>I am having an <code>arff</code> file that contains <code>700 entries</code>, each of <code>42000+ features</code> for a NLP related project. Right now the format is in dense format, but the entries can be reduced substantially, if sparse representation is used.
I am running on a <code>core 2 duo machine with 2 GB RAM</code>, and I am getting <code>memory out of range eception</code>, in spite of increasing the limit till 1536 MB. </p>

<p>Will it be of any advantage if I convert the arff file to a sparse representation or shall I need to run my code on a much more powerful machine?</p>
"
"21371416","extracting technical keywords from a text document","2014-01-27 01:01:07","3","2015","9","1","","21371453","<p>Re-written: </p>

<p>I have a corpus of computer science related documents. I want to extract domain specific keywords. for example JAVA, C#, HTML, OOP, UML, Unity, etc. I was looking for a source similar to Oxford dictionary for computing, however their API is not up and running yet. I have also tried Webopedia for computer science terms but that one is not as inclusive and updated ( e.g. it doesn’t include some words in my documents such as F#)  or in case of Wikipedia all terms are not listed all together. Is there a more inclusive source or appropriate approach to extract those keywords?  I am using Python with NLTK . For example, tf-idf wasn’t helpful because some domain specific words are common almost in all documents so those words don’t get a high rating. I think it would be helpful if I could use the POS-tagging but I’m not sure which option would be the best for my application. Take the string below as an example:</p>

<p>“Expert level capabilities in JavaScript, JSON, and AJAX, and a deep knowledge of JavaScript frameworks such as JQuery
“
Here I want to extract these words : [‘JavaScript’, ‘JSON’, ‘AJAX’, ‘Frameworks’, ‘JQuery’] but when I search for nouns using POS-tagging of NLTK, I get ‘level’, ‘capability’, ‘knowledge’ … as well.
Thanks for your help.</p>
"
"21357881","Custom NER and POS tagging","2014-01-26 00:03:02","4","2464","1","2","","21361619","<p>I was checking out Stanford CoreNLP in order to understand NER and POS tagging. But what if I want to create custom tags for entities like<code>&lt;title&gt;Nights&lt;/title&gt;, &lt;genre&gt;Jazz&lt;/genre&gt;, &lt;year&gt;1992&lt;/year&gt;</code> How can I do it? is CoreNLP useful in this case?</p>
"
"21294694","Text Classification - using stemmer degrades results?","2014-01-22 21:47:38","3","818","0","1","","21295143","<p>There's <a href=""http://www.cs.indiana.edu/~mkorayem/paper/survey_Arabic.pdf"" rel=""nofollow"">this</a> article about sentiment analysis of Arabic. </p>

<p>In the beginning of page 5 it says that:</p>

<blockquote>
  <p>""Experiments also show that stemming words before feature extraction and classification nearly always degrades the results"".</p>
</blockquote>

<p>Later on in the same page, they state that:</p>

<blockquote>
  <p>""...and an Arabic light stemmer is used for stemming the words""</p>
</blockquote>

<p>Um I thought that a stemmer/lemmatizer was <em>always</em> used before text classifications, why does he say that it degrades the results?</p>

<p>Thanks :)</p>
"
"21208568","How to check Natural Language Sentence Structure validity using parser in java?","2014-01-18 19:03:46","2","1512","1","1","","21222127","<p>I am working on a project in which there is a part where I will have to input a sentence to check whether it is a valid sentence or not. </p>

<p>For example, if I give the input as ""I am working at home"", then the output will give me ""Valid Sentence"" where if I give the input as ""I working home am at"", it will give me ""Invalid Sentence"". </p>

<p>I searched some natural language parsing methods like NLP, Stanford Parser, but it would be  helpful if someone please guide me through some java examples about the related problems.</p>

<p>I will be grateful in advance for this help. Thank you.</p>
"
"21170349","Import my own texts to use NLTK part-of-speech-tagger","2014-01-16 18:50:35","0","1290","0","1","","21170792","<p>I'm a beginner at this but I'd like to create a folder where I have many texts (lets say novels saved as .txt). I'd then like to ask the user to select one of these novels and then automatically have the part-of-speech-tagger analize the entire text. Is this possible? I've been trying with: </p>

<pre><code>text = nltk.word_tokenize(""And now for something completely different"")
nltk.pos_tag(text)
</code></pre>

<p>How do I make it analyse the text the user has selected instead of this sentence? 
And how do I import these texts?</p>
"
"20985604","Using the Stanford Dependency Parser on a previously tagged sentence","2014-01-08 01:54:42","2","2233","0","1","","21009773","<p>I'm currently using the Twitter POS tagger <a href=""http://www.ark.cs.cmu.edu/TweetNLP/"" rel=""nofollow"">available here</a> to tag out tweets into the Penn-Tree Bank tags.  </p>

<p>Here is that code:</p>

<pre><code>import java.util.List;

import cmu.arktweetnlp.Tagger;
import cmu.arktweetnlp.Tagger.TaggedToken;

/* Tags the tweet text */
List&lt;TaggedToken&gt; tagTweet(String text) throws IOException {

    // Loads Penn Treebank POS tags
    tagger.loadModel(""res/model.ritter_ptb_alldata_fixed.txt"");

    // Tags the tweet text
    taggedTokens = tagger.tokenizeAndTag(text);

    return taggedTokens;
}
</code></pre>

<p>Now I need to identify where the direct objects are in these tags.  After some searching, I've discovered that the Stanford Parser can do this, by way of the Stanford Typed Dependencies, (<a href=""http://nlp.stanford.edu:8080/parser/"" rel=""nofollow"">online example</a>).  By using the dobj() call, I should be able to get what I need.</p>

<p>However, I have not found any good documentation about how to feed already-tagged sentences into this tool.  From what I understand, before using the Dependency Parser I need to create a tree from the sentence's tokens/tags. How is this done? I have not been able to find any example code.</p>

<p>The Twitter POS Tagger contains an instance of the Stanford NLP Tools, so I'm not far off, however I am not familiar enough with the Stanford tools to feed my POS-tagged text into it in order to get the dependency parser to work properly.  <a href=""http://nlp.stanford.edu/software/parser-faq.shtml#f"" rel=""nofollow"">The FAQ does mention this functionality</a>, but without any example code to go off of, I'm a bit stuck.</p>
"
"20726214","Inserting ErrorT at the base of transformer stack","2013-12-22 04:16:59","4","236","3","2","","20726559","<p>What is the best way to run a code with type <code>t (ErrorT String IO) a</code> from within a <code>t IO a</code> monad? Consider the code below:</p>

<pre><code>module Sample where

import System.IO
import Control.Monad.Reader
import Control.Monad.Error

type Env = String

inner :: ReaderT Env (ErrorT String IO) ()
inner = do
    s &lt;- ask
    fail s

outer :: ReaderT Env IO ()
outer = do
    env &lt;- ask
    res &lt;- lift $ runErrorT $ runReaderT inner env
    case res of
        Left err -&gt; liftIO $ hPutStrLn stderr err
        Right _ -&gt; return ()
    outer
</code></pre>

<p>This works, but I've been looking for a more graceful way of inserting ErrorT at the base of my stack. Especially that I'm using several different monad transformer stacks in my project and writing the above for each of them is quite tedious.</p>

<p>I was looking for something like:</p>

<pre><code>outer :: ReaderT Env IO ()
outer = do
    res &lt;- (hoist runErrorT) inner
    ...
</code></pre>

<p>But I cannot use <code>hoist</code> due to type mismatch.</p>

<hr>

<p>Edit:</p>

<p>I use <code>StateT</code> in some of my stacks and that's the reason for trying to put <code>ErrorT</code> at the base and not on the top.</p>

<p>The <code>outer</code> is supposed to be an infinite loop.</p>
"
"20332762","Pos tagging german texts using NLTK","2013-12-02 16:15:49","5","4583","0","3","","20455545","<p>I want to use NLTK to POS tag german texts. I found some references on the web, but most of the are outdated. Some reference for example a ""EUROPARL"" thesaurus, but it looks like only ""EUROPARL_raw"" is still available. And that one is not POS tagged. I found also some references to usage of the TIGER corpus, but the latest version seems to be I format I cannot parse with NLTK out of the box.</p>

<p>I'm aware of some non-NTLT alternatives, but I would prefer to use NLTK. Could somebody provide a simple example with POS tagging based on a german corpus?</p>
"
"20075754","Parse HTML style text annotations to a list of dictionaries","2013-11-19 15:39:25","3","331","3","1","","20084516","<p>Currently I have the following problem:</p>

<p>Given a string </p>

<pre><code>""&lt;a&gt;annotated &lt;b&gt;piece&lt;/b&gt;&lt;/a&gt; of &lt;c&gt;text&lt;/c&gt;"" 
</code></pre>

<p>construct a list such that the result is </p>

<pre><code>[{""annotated"": [""a""]}, {""piece"": [""a"", ""b""]}, {""of"": []}, {""text"": [""c""]}].
</code></pre>

<p>My previous attempt looked something like</p>

<pre><code>open_tag = '&lt;[a-z0-9_]+&gt;'
close_tag = '&lt;\/[a-z0-9_]+&gt;'
tag_def = ""("" + open_tag + ""|"" + close_tag + "")""

def tokenize(str):
    """"""
    Takes a string and converts it to a list of words or tokens
    For example ""&lt;a&gt;foo&lt;/a&gt;, of"" -&gt; ['&lt;a&gt;', 'foo', '&lt;/a&gt;', ',' 'of']
    """"""
    tokens_by_tag = re.split(tag_def, str)
    def tokenize(token):
        if not re.match(tag_def, token):
            return word_tokenize(token)
        else:
            return [token]
    return list(chain.from_iterable([tokenize(token) for token in tokens_by_tag]))

def annotations(tokens):
    """"""
    Process tokens into a list with {word : [tokens]} items
    """"""
    mapping = []
    curr = []
    for token in tokens:
        if re.match(open_tag, token):
            curr.append(re.match('&lt;([a-z0-9_]+)&gt;',token).group(1))
        elif re.match(close_tag, token):
            tag = re.match('&lt;\/([a-z0-9_]+)&gt;',token).group(1)
            try:
                curr.remove(tag)
            except ValueError:
                pass
        else:
            mapping.append({token: list(curr)})
    return mapping
</code></pre>

<p>Unfortunately this has a flaw since <code>(n=54)</code> resolves to <code>{""n=54"" : []}</code> but <code>(n=&lt;n&gt;52&lt;/n&gt;)</code> to <code>[{""n="": []}, {52: [""n""]}]</code> thus the length of the two lists differ, making it impossible to merge two different ones later on. </p>

<p>Is there a good strategy for doing parsing HTML/SGML style annotations in a way that two differently annotated (but otherwise identical) strings yield a list of equal size?</p>

<p>Note that I'm well aware that regexps are not suitable for this type of parsing, but also not the problem in this case.</p>

<p><strong>EDIT</strong> Fixed a mistake in the example</p>
"
"19994396","Best way to identify and extract dates from text Python?","2013-11-15 05:50:22","61","83016","4","8","","35069076","<p>As part of a larger personal project I'm working on, I'm attempting to separate out inline dates from a variety of text sources.</p>

<p>For example, I have a large list of strings (that usually take the form of English sentences or statements) that take a variety of forms:</p>

<blockquote>
  <p>Central design committee session Tuesday 10/22 6:30 pm</p>
  
  <p>Th 9/19 LAB: Serial encoding (Section 2.2)</p>
  
  <p>There will be another one on December 15th for those who are unable to make it today.</p>
  
  <p>Workbook 3 (Minimum Wage): due Wednesday 9/18 11:59pm</p>
  
  <p>He will be flying in Sept. 15th.</p>
</blockquote>

<p>While these dates are in-line with natural text, none of them are in specifically natural language forms themselves (e.g., there's no ""The meeting will be two weeks from tomorrow""—it's all explicit).  </p>

<p>As someone who doesn't have too much experience with this kind of processing, what would be the best place to begin? I've looked into things like the <code>dateutil.parser</code> module and <a href=""https://github.com/bear/parsedatetime"">parsedatetime</a>, but those seem to be for <em>after</em> you've isolated the date.</p>

<p>Because of this, is there any good way to extract the date and the extraneous text </p>

<pre><code>input:  Th 9/19 LAB: Serial encoding (Section 2.2)
output: ['Th 9/19', 'LAB: Serial encoding (Section 2.2)']
</code></pre>

<p>or something similar? It seems like this sort of processing is done by applications like Gmail and Apple Mail, but is it possible to implement in Python?</p>
"
"19957656","Running TreeTagger","2013-11-13 15:25:20","0","1857","0","2","","19960755","<p>I'm attempting to run TreeTagger using the French parameter file but I am getting a permission denied error with the french-utf8.par file</p>

<pre><code>    [bash]:echo cmd/tree-tagger-french-utf8 | lib/french-utf8.par
    [bash]:lib/french-utf8.par: Permission denied
</code></pre>

<p>This is quite similar to this question here (<a href=""https://stackoverflow.com/questions/15503388/treetagger-installation-successful-but-cannot-open-par-file"">TreeTagger installation successful but cannot open .par file</a>), but I'm able to run the tagger like this:</p>

<pre><code>    [bash]: echo 'Bonjour' | cmd/tree-tagger-french-utf8
reading parameters ...
tagging ...
 Bonjour    NOM bonjour
 finished.
</code></pre>

<p>I've tried changing to echo bin/tree-tagger, but I get the same error. Any ideas on what I am doing wrong?</p>
"
"19689557","How to format XML file with CSS?","2013-10-30 17:16:15","0","90","0","1","","19689616","<p>I have an XML files with the Brown corpus in it. I want to create a simple CSS to make it more readable. For example I wish to highlight all the nouns in the sentence. This is a sample sentence from the XML file. &lt; w > marks the words in it.</p>

<pre><code>&lt;s n=""1""&gt;
    &lt;w type=""AT""&gt;The&lt;/w&gt;
    &lt;w type=""NP"" subtype=""TL""&gt;Fulton&lt;/w&gt;
    &lt;w type=""NN"" subtype=""TL""&gt;County&lt;/w&gt;
    &lt;w type=""JJ"" subtype=""TL""&gt;Grand&lt;/w&gt;
    &lt;w type=""NN"" subtype=""TL""&gt;Jury&lt;/w&gt;
    &lt;w type=""VBD""&gt;said&lt;/w&gt;
    &lt;w type=""NR""&gt;Friday&lt;/w&gt;
    &lt;w type=""AT""&gt;an&lt;/w&gt;
    &lt;w type=""NN""&gt;investigation&lt;/w&gt;
    &lt;w type=""IN""&gt;of&lt;/w&gt;
    &lt;w type=""NPg""&gt;Atlanta's&lt;/w&gt;
    &lt;w type=""JJ""&gt;recent&lt;/w&gt;
    &lt;w type=""NN""&gt;primary&lt;/w&gt;
    &lt;w type=""NN""&gt;election&lt;/w&gt;
    &lt;w type=""VBD""&gt;produced&lt;/w&gt;
    &lt;c type=""pct""&gt;``&lt;/c&gt;
    &lt;w type=""AT""&gt;no&lt;/w&gt;
    &lt;w type=""NN""&gt;evidence&lt;/w&gt;
    &lt;c type=""pct""&gt;''&lt;/c&gt;
    &lt;w type=""CS""&gt;that&lt;/w&gt;
    &lt;w type=""DTI""&gt;any&lt;/w&gt;
    &lt;w type=""NNS""&gt;irregularities&lt;/w&gt;
    &lt;w type=""VBD""&gt;took&lt;/w&gt;
    &lt;w type=""NN""&gt;place&lt;/w&gt;
    &lt;c type=""pct""&gt;.&lt;/c&gt;
&lt;/s&gt;
</code></pre>

<p>I wish to highlight all words that have the type=""NN"" and type=""NNS""
How do I do that?</p>
"
"19495967","Getting additional information (Active/Passive, Tenses ...) from a Tagger","2013-10-21 13:31:57","6","4176","0","1","","19512810","<p>I'm using the Stanford Tagger for determining the Parts of Speech. However, I want to get more information out of the text. Is there a possibility to get further information like the tense of the sentence or if it is in active/passive?</p>

<p>So far, I'm using the very basic PoS-Tagging approach:</p>

<pre><code>List&lt;List&lt;TaggedWord&gt;&gt; taggedUnits = new ArrayList&lt;List&lt;TaggedWord&gt;&gt;();

String input = ""This sentence is going to be future. The door was opened."";
for (List&lt;HasWord&gt; sentence : MaxentTagger.tokenizeText(new StringReader(input)))
{
     taggedUnits.add(tagger.tagSentence(sentence));
}
</code></pre>
"
"19326278","NLTK Turning a subtree into a list in python / RSS feed chunking","2013-10-11 19:59:27","1","2133","0","2","","19402157","<p>Using the code below I am chunking an already tagged and tokenized rss feed. The ""print subtree.leaves()"" is out-puting:</p>

<p>[('Prime', 'NNP'), ('Minister', 'NNP'), ('Stephen', 'NNP'), ('Harper', 'NNP')]
[('U.S.', 'NNP'), ('President', 'NNP'), ('Barack', 'NNP'), ('Obama', 'NNP')]
[('what\', 'NNP')]
[('Keystone', 'NNP'), ('XL', 'NNP')]
[('CBC', 'NNP'), ('News', 'NNP')]</p>

<p>This looks like a python list but I do not know how to access it directly or iterate over it. I think it is a subtree output.</p>

<p>I want to be able to turn this subtree into a list that I can manipulate. Is there an easy way to do this? This is the first time I have encountered trees in python and I am lost. I want to end up with this list:</p>

<p>docs = [""Prime Minister Stephen Harper"", ""U.S. President Barack Obama"", ""what\"", ""Keystone XL"", ""CBC News""]</p>

<p>Is there a simple way to make this happen?</p>

<p>Thanks, as always for the help!</p>

<pre><code>grammar = r"""""" Proper: {&lt;NNP&gt;+} """"""

cp = nltk.RegexpParser(grammar)
result = cp.parse(posDocuments)
nounPhraseDocs.append(result) 

for subtree in result.subtrees(filter=lambda t: t.node == 'Proper'):
# print the noun phrase as a list of part-of-speech tagged words

    print subtree.leaves()
print"" ""
</code></pre>
"
"19270759","Do tf-idf weights affect the cosine similarity?","2013-10-09 11:26:40","3","2163","0","3","","19272635","<p>I'm clustering text documents. I'm using tf-idf and cosine similarity. However there's something I don't really understand even tho I'm using these measures. Do the tf-idf weights affect the similarity calculations between two documents?</p>

<p>Suppose I have these two documents:</p>

<p>1- High trees.</p>

<p>2- High trees High trees High trees High trees.</p>

<p>Then the similarity between the two documents will be 1, although the tf-idf vectors of the two documents are different. Where the second should normally have higher weights for the terms compared to the first document. </p>

<p>Suppose the weights for the two vectors are (just suppose):</p>

<p>v1(1.0, 1.0)</p>

<p>v2(5.0, 8.0)</p>

<p>calculating the cosine similarity gives 1.0.</p>

<p>Here is a sketch of two random vectors that share the same terms but with different weights. </p>

<p>There's an obvious angel between the vectors, so the weights should play a role!</p>

<p><img src=""https://i.sstatic.net/SVPzt.png"" alt=""enter image description here""></p>

<p>This triggers the question, where do the tf/idf weights play a role in the similarity calculations? Because what I understood so far is that the similarity here only cares about the presence and absence of the terms.</p>
"
"19262597","Why no programming in English? What is the difference between natural languages and programming languages?","2013-10-09 03:30:47","-1","3378","4","3","","19262710","<p>What is the key difference between natural languages (such as English and French) and programming languages like C++ and Perl?</p>

<p>I am familiar with the ambiguity problem, but can't it be solved using an interactive compiler or using a subset of the natural language using a strict grammar but all the time still retaining the essence of the language?</p>

<p>Another issue is context. But lawyers have ways to solve this issue. (This question is not about reducing the programming complexity, it's simply about concise reasons and roadblock in using natural languages for instructing computer.)</p>

<p>Is there any other significant problem besides these two? Or do these two have greater consequences than I mentioned above? Is the interactive solution and lawyers language technically not feasible for programming?</p>
"
"18984722","How to parse the special character in Context Free Grammar?","2013-09-24 14:42:38","3","877","0","2","","19000710","<p>I have a context free grammar (CFG) which involves punctuation. e.g.
nltk.parse_cfg(""""""PP-CLR -> IN `` NP-TTL"""""")</p>

<p>The `` is a valid Penn Treebank POS tag. But nltk cannot recognize it. In fact, nltk.parse_cfg cannot recognize any character other than alphanumeric and dash. While Penn Treebank POS tag has several punctuation, such as $ # : . (</p>

<p>Then, should I keep the punctuation in my dataset? Or is there anyway to parse these characters?</p>

<p>Thanks</p>
"
"18948712","Output results in conll format (POS-tagging, stanford pos tagger)","2013-09-22 21:00:41","3","2116","1","1","","18949018","<p>I am trying to use Stanford POS-tagger, I want to ask if it is possible to parse (actually only pos tag would be enough) an english text and output the results in conll format. Is there such an option?</p>

<p>I am using the full 3.2.0 version of the Stanford pos tagger</p>

<p>Thanks a lot</p>
"
"18871249","Extracting Name from a plain text in java","2013-09-18 11:39:39","0","1035","6","1","","18871643","<p>is there any way to parse name (firstname and lastName s ) from a plain text.Names can be from any country. For now i am building a database of possible names i can get. Is there any other good methods? </p>
"
"18840537","How to generate pertinent text?","2013-09-17 02:50:55","-1","137","2","1","","18855602","<p>What I want to do is, get a text training set (natural language) and increase this set with automatically created text that tries to mimic the text content. I'm using a bag-of-words assumption, sequence doesn't matter, syntax doesn't matter, I just want to create text that contains words that is pertinent with the general topic of the base.</p>

<p>Right now I'm using <strong>Latent Dirichlet Allocation</strong> to classify my documents in topics distributions, average the topic distribution of my set, and generate documents from these topic distribution.</p>

<p>I want to know two things:</p>

<blockquote>
  <p>1- Is there a better way to do that?</p>
  
  <p>2- Can I train LDA with texts that are not of the domain of my set,
  without tainting my topics: Eg. The set that I want to increase has
  texts about politics. Can I train my model with any kind of text
  (cars, fashion, musics) and classificates my base of politics text get its topics distributions and generates similar text from this distribution.</p>
</blockquote>

<p>I'm using python 2.7 and gensim.</p>
"
"18705778","What is the use of Brown Corpus in measuring Semantic Similarity based on WordNet","2013-09-09 19:45:57","6","3648","0","2","","18706698","<p>I came across several methods for measuring semantic similarity that use the structure and hierarchy of WordNet, e.g. Jiang and Conrath measure (JNC), Resnik measure(RES), Lin measure (LIN) etc.</p>

<p>The way they are measured using NLTK is:</p>

<pre><code>sim2=wn.jcn_similarity(entry1,entry2,brown_ic)
sim3=entry1.res_similarity(entry2, brown_ic)
sim4=entry1.lin_similarity(entry2,brown_ic)
</code></pre>

<p>If WordNet is the basis of calculating semantic similarity, what is the use of Brown Corpus here?</p>
"
"18496925","How to parse product titles (unstructured) into structured data?","2013-08-28 19:45:13","11","4540","3","5","","18527584","<p>I am looking to parse unstructured product titles like “Canon D1000 4MP Camera 2X Zoom LCD” into structured data like <code>{brand: canon, model number: d1000, lens: 4MP zoom: 2X, display type: LCD}</code>.</p>

<p>So far I have:</p>

<ol>
<li>Removed stopwords and cleaned up (remove characters like <code>-</code> <code>;</code> <code>:</code> <code>/</code>)</li>
<li>Tokenizing long strings into words.</li>
</ol>

<p>Any techniques/library/methods/algorithms would be much appreciated!</p>

<p>EDIT: There is no heuristic for the product titles. A seller can input <strong>anything</strong> as a title. For eg: 'Canon D1000' can just be the title. Also, this exercise is not only for camera datasets, the title can be of any product.  </p>
"
"18473958","What is Zone Hashing in Natural Language Processing?","2013-08-27 19:33:51","3","310","1","1","","18474612","<p>Has anyone in the NLP field heard of the term <em>Zone Hashing</em>? From what I hear, zone hashing is the process of iterating through a document and extracting sentences. An accumulation of sentences is then hashed, and the process continues for the next <em>n</em> sentences...</p>

<p>I haven't found any references to this on Google, so I'm wondering if it goes by a different name. It should be related to measuring text similarity/nearness. </p>

<p>Perhaps it refers to locality sensitive hashing? </p>
"
"18470873","0th synset in NLTK wordnet interface","2013-08-27 16:25:39","1","455","0","1","","18496893","<p>From the semcor corpus (<a href=""http://www.cse.unt.edu/~rada/downloads.html"" rel=""nofollow"">http://www.cse.unt.edu/~rada/downloads.html</a>), there are senses wasn't mapped to the later versions of wordnet. And magically, the mapping can be found in the NLTK WordNet API as such:</p>

<pre><code>&gt;&gt;&gt; from nltk.corpus import wordnet as wn
# Emunerate the possible senses for the lemma 'delayed'
&gt;&gt;&gt; wn.synsets('delayed')
[Synset('delay.v.01'), Synset('delay.v.02'), Synset('stay.v.06'), Synset('check.v.07'), Synset('delayed.s.01')]
&gt;&gt;&gt; wn.synset('delay.v.01')
Synset('delay.v.01')
# Magically, there is a 0th sense of the word!!!
&gt;&gt;&gt; wn.synset('delayed.a.0')
Synset('delayed.s.01')
</code></pre>

<p>I've checked the code and the API (<a href=""http://nltk.googlecode.com/svn/trunk/doc/api/nltk.corpus.reader.wordnet.Synset-class.html"" rel=""nofollow"">http://nltk.googlecode.com/svn/trunk/doc/api/nltk.corpus.reader.wordnet.Synset-class.html</a>, <a href=""http://nltk.org/_modules/nltk/corpus/reader/wordnet.html"" rel=""nofollow"">http://nltk.org/_modules/nltk/corpus/reader/wordnet.html</a>) but i can't find how they did the magically mapping that didn't shouldn't exist (e.g. for <code>delayed.a.0</code> -> <code>delayed.s.01</code>).</p>

<p><strong>Does anyone know which part of the NLTK Wordnet API code does the magical mapping?</strong></p>
"
"18416561","POS tagging in Scala","2013-08-24 08:29:28","11","2891","0","3","","19659698","<p>I tried to POS tag a sentence in Scala using Stanford parser like below</p>

<pre><code>val lp:LexicalizedParser = LexicalizedParser.loadModel(""edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz"");
lp.setOptionFlags(""-maxLength"", ""50"", ""-retainTmpSubcategories"")
val s = ""I love to play""
val parse :Tree =  lp.apply(s)
val taggedWords = parse.taggedYield()
println(taggedWords)
</code></pre>

<p>I got an error <strong>type mismatch; found : java.lang.String required: java.util.List[_ &lt;: edu.stanford.nlp.ling.HasWord]</strong> in the line <strong>val parse :Tree =  lp.apply(s)</strong></p>

<p>I don't know whether this is the right way of doing it or not. Are there any other easy ways of POS tagging a sentence in Scala?</p>
"
"18366071","Looking for a database or text file of english words with their different forms","2013-08-21 19:31:06","7","2366","0","1","","18386728","<p>I am working on a project and I need to get the root of a given word (stemming). As you know, the stemming algorithms that don't use a dictionary are not accurate. Also I tried the WordNet but it is not good for my project. I found phpmorphy project but it doesn't include API in Java. </p>

<p>At this time I am looking for a database or a text file of english words with their different forms. for example:</p>

<p>run running ran ...
include including included ...
...</p>

<p>Thank you for your help or advise. </p>
"
"18332234","What parsing strategy is used on Stanford Parser?","2013-08-20 10:01:47","0","161","1","1","","18337677","<p>I'm writing a paper where I analyse different available tools for natural language parsing. I found out that two main strategies for parsing are top-down and bottom down.</p>

<p><strong>I wonder which strategy is used in Stanford Parser?</strong> </p>

<p>I know that they used probabilistic approach there, but is not based on any of bottom-up or top-down?</p>
"
"18140415","How to extract entity using stanford parser?","2013-08-09 05:15:55","0","324","0","1","","18140504","<p>I am using OpenNLP but it is not giving me exact name,location,organization entity. How to extract entity using stanford parser?</p>
"
"18138238","Looking for language translation function for PHP","2013-08-09 00:02:57","-1","2062","3","1","","18138696","<p>I would like to add language translation to my site.  Is there any freeware (or at least inexpensive packages for a one-time fee) that will work more-or-less like this:</p>

<pre><code>$french_text = translate ($german_text, 'german', 'french');
</code></pre>

<p>I don't expect it to support every language in the world, but to be useful it should at least support most European languages.  Also, being an English site, it's acceptable to me if it needs to use English as an intermediate language like this:</p>

<pre><code>$english_text = translate ($german_text, 'german', english');
$french_text = translate ($english_text, 'english', 'french');
</code></pre>

<p>Does anything like this exist for PHP?</p>
"
"17695611","NLTK Context Free Grammar Genaration","2013-07-17 09:06:53","10","17094","3","5","","17753906","<p>I'm working on a non-English parser with Unicode characters. For that, I decided to use NLTK.</p>

<p>But it requires a predefined context-free grammar as below: </p>

<pre><code>  S -&gt; NP VP
  VP -&gt; V NP | V NP PP
  PP -&gt; P NP
  V -&gt; ""saw"" | ""ate"" | ""walked""
  NP -&gt; ""John"" | ""Mary"" | ""Bob"" | Det N | Det N PP
  Det -&gt; ""a"" | ""an"" | ""the"" | ""my""
  N -&gt; ""man"" | ""dog"" | ""cat"" | ""telescope"" | ""park""
  P -&gt; ""in"" | ""on"" | ""by"" | ""with"" 
</code></pre>

<p>In my app, I am supposed to minimize hard coding with the use of a rule-based grammar. 
For example, I can assume any word ending with <strong>-ed</strong> or <strong>-ing</strong> as a verb. So it should work for any given context.</p>

<p>How can I feed such grammar rules to NLTK? Or generate them dynamically using Finite State Machine?</p>
"
"17684186","NLTK words lemmatizing","2013-07-16 18:21:47","4","2361","1","1","","17687095","<p>I am trying to do lemmatization on words with <code>NLTK</code>.  </p>

<p>What I can find now is that I can use the <code>stem</code> package to get some results like transform ""cars"" to ""car"" and ""women"" to ""woman"", however I cannot do lemmatization on some words with affixes like ""acknowledgement"".  </p>

<p>When using <code>WordNetLemmatizer()</code> on ""acknowledgement"", it returns ""acknowledgement"" and using <code>.PorterStemmer()</code>, it returns ""acknowledg"" rather than ""acknowledge"".  </p>

<p>Can anyone tell me how to eliminate the affixes of words?<br>
Say, when input is ""acknowledgement"", the output to be ""acknowledge""</p>
"
"17447045","Java library for keywords extraction from input text","2013-07-03 11:43:06","34","25451","0","3","","17453157","<p>I'm looking for a Java library to extract keywords from a block of text.</p>

<p>The process should be as follows:</p>

<p>stop word cleaning -> stemming -> searching for keywords based on English linguistics statistical information - meaning if a word appears more times in the text than in the English language in terms of probability than it's a keyword candidate.</p>

<p>Is there a library that performs this task?</p>
"
"17408543","How to correctly set Hunpos tagger in NLTK for POS tagging in english?","2013-07-01 15:47:26","0","2497","0","2","","17425786","<p>I'm trying to use the <a href=""http://nltk.org/api/nltk.tag.html#module-nltk.tag.hunpos"" rel=""nofollow"">Hunpos tagger</a> for POS tagging with NLTK instead of the traditional <code>pos_tag()</code>, but I'm having some trouble with loading the binary <code>english.model</code> or <code>en_wsj.model</code>.</p>

<p>In fact, I'm in linux mint and I put them in <code>/usr/local/bin</code>, set the <code>HUNPOS</code> environment variable to this path, and even tried to pass this path to the parameter <code>path_to_bin</code> used in the <code>__init__</code> of <code>nltk/tag/hunpos.py</code> file, but when it recognizes the file, it throws this error:</p>

<pre><code>&gt;&gt;&gt; ht = HunposTagger('en_wsj.model','/usr/local/bin/en_wsj.model')
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/usr/local/lib/python2.7/dist-packages/nltk-2.0.4-py2.7.egg/nltk/tag/hunpos.py"", line 89, in __init__
    shell=False, stdin=PIPE, stdout=PIPE, stderr=PIPE)
  File ""/usr/lib/python2.7/subprocess.py"", line 679, in __init__
    errread, errwrite)
  File ""/usr/lib/python2.7/subprocess.py"", line 1249, in _execute_child
    raise child_exception
OSError: [Errno 8] Exec format error
</code></pre>

<p>Does anyone got an idea about what is happening?</p>
"
"17325554","Difference between IOB Accuracy and Precision","2013-06-26 16:27:49","4","3236","0","1","","20332548","<p>I'm doing some works on NLTK with named entity recognition and chunkers. I retrained a classifier using <code>nltk/chunk/named_entity.py</code> for that and I got the following mesures:</p>

<pre><code>ChunkParse score:
    IOB Accuracy:  96.5%
    Precision:     78.0%
    Recall:        91.9%
    F-Measure:     84.4%
</code></pre>

<p>But I don't understand what is the exact difference between IOB Accuracy and Precision in this case. Actually, I found on the docs (<a href=""http://nltk.googlecode.com/svn/trunk/doc/book/ch07.html"" rel=""nofollow"">here</a>) the following for an specific example:</p>

<blockquote>
  <p>The IOB tag accuracy indicates that more than a third of the words are
  tagged with O, i.e. not in an NP chunk. However, since our tagger did
  not find any chunks, its precision, recall, and f-measure are all
  zero.</p>
</blockquote>

<p>So, if IOB accuracy is just the number of O labels, how come we don't have chunks and IOB accuracy is not 100% at the same time, in that example?</p>

<p>Thank you in advance</p>
"
"17317418","Stemmers vs Lemmatizers","2013-06-26 10:19:01","81","25720","3","4","","17320458","<p>Natural Language Processing (NLP), especially for English, has evolved into the stage where stemming would become an archaic technology if ""perfect"" lemmatizers exist. It's because stemmers change the surface form of a word/token into some meaningless stems. </p>

<p>Then again the definition of the ""perfect"" lemmatizer is questionable because different NLP task would have required different level of lemmatization. E.g. <a href=""https://stackoverflow.com/questions/14489309/convert-words-between-verb-noun-adjective-forms"">Convert words between verb/noun/adjective forms</a>. </p>

<p><strong>Stemmers</strong> </p>

<pre><code>[in]: having
[out]: hav
</code></pre>

<p><strong>Lemmatizers</strong></p>

<pre><code>[in]: having
[out]: have
</code></pre>

<ul>
<li><p>So the question is, are English stemmers any useful at all today? Since we have a plethora of lemmatization tools for English</p></li>
<li><p>If not, then how should we move on to build robust lemmatizers that
can take on <code>nounify</code>, <code>verbify</code>, <code>adjectify</code> and <code>adverbify</code>
preprocesses?</p></li>
<li><p>How could the lemmatization task be easily scaled to other languages
that have similar morphological structures as English?</p></li>
</ul>
"
"17314506","Why do I need a tokenizer for each language?","2013-06-26 07:54:15","14","6307","2","3","","17316106","<p>When processing text, why would one need a tokenizer specialized for the language? </p>

<p>Wouldn't tokenizing by whitespace be enough? What are the cases where it is not good idea to use simply a white space tokenization?</p>
"
"17259970","tagging pos in nltk using backoff ngrams","2013-06-23 10:47:34","2","5279","0","2","","17267945","<p>I tried make part of speech (or POS) tagger in nltk but I can't get it to work for more than one ngram tagger for a time using backoff. I read that you use more taggers for higher scores but it won't work for me. I want it to first use more words then use less down to one. I tried it like this,</p>

<pre><code>import nltk
from nltk.corpus import brown

#sentence =  brown.sents(categories = ""news"")
trains = brown.tagged_sents(categories = ""news"")


from nltk import NgramTagger

fortest = [""hi"", ""how"",""are"", ""you""]

tagger = (nltk.NgramTagger (n, trains, backoff=n-1) for n in range (3))
print tagger.tag(fortest)
</code></pre>

<p>But it gives me error AttributeError: 'generator' object has no attribute 'tag'</p>

<p>so i make it without a list:</p>

<pre><code>for n in range(3):
    tagger = nltk.NgramTagger(n, trains, backoff=n-1)
</code></pre>

<p>But then I get:</p>

<pre><code>File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/nltk/tag/sequential.py"", line 271, in __init__
  ContextTagger.__init__(self, model, backoff)
File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/nltk/tag/sequential.py"", line 121, in __init__
  SequentialBackoffTagger.__init__(self, backoff)
File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/nltk/tag/sequential.py"", line 46, in __init__
  self._taggers = [self] + backoff._taggers AttributeError: 'int' object has no attribute '_taggers'
</code></pre>

<p>Please help would be appreciated for I am new to doing python.</p>
"
"17251156","Finding out adjectives describing a noun using Stanford NLP","2013-06-22 13:14:04","3","5134","1","1","","17259249","<p>I need to write a code that takes a few lines of comments about a product as input and rate the product based on adjectives that describe the product in the reviews. I have just used POS tagger to tag the parts of speech of every comment. Now, I have to pick out the adjectives that describe the nouns, And if a noun appears to be related to the product, I need to consider the corresponding adjective. This is the code I've used for POS tagging.. It just works fine.    </p>

<pre><code>import java.io.*;
import edu.stanford.nlp.tagger.maxent.MaxentTagger;
public class Tagg {
public static void main(String[] args) throws IOException,
ClassNotFoundException {

String tagged;

// Initialize the tagger
MaxentTagger tagger = new MaxentTagger(""edu/stanford/nlp/models/pos-tagger/wsj-        left3words/wsj-0-18-left3words-distsim.tagger"");
FileInputStream fstream = new FileInputStream(""src/input.txt"");
BufferedReader br = new BufferedReader(new InputStreamReader(fstream));
FileWriter q = new FileWriter(""src/output.txt"",true);
BufferedWriter out =new BufferedWriter(q);
String sample;
//we will now pick up sentences line by line from the file input.txt and store it in the string sample
while((sample = br.readLine())!=null)
{
//tag the string
tagged = tagger.tagString(sample);
System.out.print(tagged+""\n"");
//write it to the file output.txt
out.write(tagged);
out.newLine();
}
out.close();
}
}
</code></pre>

<p>I need a way to proceed. .</p>
"
"17247874","What is the difference between Information Extraction and Text Mining?","2013-06-22 06:10:04","11","8489","2","2","","17247934","<p>It may be looking easy. But I am confused. </p>

<p>What is the difference between Text Mining and Information Extraction ? </p>
"
"17186824","Java Google Engine Library","2013-06-19 08:48:49","0","118","1","1","","17187057","<p>i want a java library, to search a text on Google and returns some pages based on text and semantic similarity. is there any API doing this job?
i wrote a crawler myself and search to specific depth from a root page, but i dont know how to input a text as a search query for Google.  </p>
"
"17176362","Integrating MaltParser into java code, without using a separate process","2013-06-18 18:36:47","1","1296","0","1","","17205066","<p>There are several resources already available for training and executing the grammatical dependency parser, MaltParser; most notably is the project's homepage: <a href=""http://www.maltparser.org/userguide.html#startusing"" rel=""nofollow"">http://www.maltparser.org/userguide.html#startusing</a>). And looking at the NLTK code that uses MaltParser, I see how I could write equivalent Java code to start up a separate child process to run MaltParser: <a href=""http://nltk.org/_modules/nltk/parse/malt.html"" rel=""nofollow"">http://nltk.org/_modules/nltk/parse/malt.html</a>. However, what I am asking, or rather looking for, is code that clearly and cleanly shows how to integrate MaltParser as a library into a Java program.</p>

<p>To be specific, I want to write Java code to do the following:</p>

<ol>
<li><p>Train a parsing model.</p></li>
<li><p>Load a trained model and parse sentences in an online fashion (i.e. stream sentences and use a MaltParser object to parse each one).</p></li>
</ol>

<p>To whomever has the knowledge, patience, and willingness: please to help me answer 1 and 2!</p>
"
"17160097","lucene API textSimilarity","2013-06-18 03:29:55","0","610","0","2","","17165343","<p>i am writing a project on plagiarism detection with Java, 
in this case for the first step i need to do the following tasks :</p>

<pre><code>inputing  file (txt, .pdf, .doc)
convert the file content to text
removing stop words
tokenizng into n-gram
processing the text-similarity algorithms on the texts
reporting plagiarism detection signs
</code></pre>

<p>i did these steps by coding myself, but now i feel a lot of performance lacks in it, so i started using available API es for my work, such as word vector tool(<a href=""http://sourceforge.net/projects/wvtool/"" rel=""nofollow"">http://sourceforge.net/projects/wvtool/</a>) , wordnet and <code>Lucene</code>. the vvtool failed because of poor Doc available.
now my problem is how to do these with Lucene, should i input the file as a string and add it as a Field in a Document object or it has especial class for text similarity examin?
please help me on <code>Lucene</code> Library.
thanks in advance. </p>

<p>Ps- do you have any sample code source worked on Lucene i can start with? </p>
"
"17093322","Python parse words from URL string","2013-06-13 17:24:42","3","1834","3","3","","17131507","<p>I have a large data set of urls and I need a way to parse words from the urls eg:</p>

<pre><code>realestatesales.com -&gt; {""real"",""estate"",""sales""}
</code></pre>

<p>I would prefer to do it in python.  This seems like it should be possible with some kind of english language dictionary.  There might be some ambiguous cases, but I feel like there should be a solution out there somewhere.</p>
"
"17076635","How to extract lines numbers that match a regular expression in a text file","2013-06-12 22:44:42","4","11606","0","2","","17076741","<p>I'm doing a project on statistical machine translation in which I need to extract line numbers from a POS-tagged text file that match a regular expression (any non-separated phrasal verb with the particle 'out'), and write the line numbers to a file (in python).</p>

<p>I have this regular expression: '\w*_VB.?\sout_RP' and my POS-tagged text file: 'Corpus.txt'.
I would like to get an output file with the line numbers that match the above-mentioned regular expression, and the output file should just have one line number per line (no empty lines), e.g.:</p>

<p>2</p>

<p>5</p>

<p>44</p>

<p>So far all I have in my script is the following:</p>

<pre><code>OutputLineNumbers = open('OutputLineNumbers', 'w')
with open('Corpus.txt', 'r') as textfile:
    phrase='\w*_VB.?\sout_RP'
    for phrase in textfile: 

OutputLineNumbers.close()
</code></pre>

<p>Any idea how to solve this problem?</p>

<p>In advance, thanks for your help!</p>
"
"17015658","looking for a java library with a simple to calculate tf–idf, term frequency–inverse document frequency","2013-06-10 00:57:27","-3","1305","1","1","","17016149","<p>I need to calculate tf-idf for a set of documents and am looking for a java library that does this.</p>

<p>NOTE: I am aware of Mahout but I really want is a library with a simple interface and one that does not require infrastructure setup.</p>
"
"17013370","How to change the word order of phrasal verbs in a POS-tagged corpus file","2013-06-09 19:34:22","0","1069","0","1","","17013989","<p>I have a POS-tagged parallel corpus text file in which I would like to do word reordering, so that the ""separable phrasal verb particle"" will appear next to the ""verb"" of the phrasal verb ('make up a plan' instead of 'make a plan up') . This used for preprocessing in a statistical machine translation system. Here are some example lines from the POS-tagged text file:</p>

<ol>
<li>you_PRP mean_VBP we_PRP should_MD kick_VB them_PRP out_RP ._. </li>
<li>don_VB &apos;t_NNP take_VB it_PRP off_RP until_IN I_PRP say_VBP so_RB ._.</li>
<li>please_VB help_VB the_DT man_NN out_RP ._.</li>
<li>shut_VBZ it_PRP down_RP !_.</li>
</ol>

<p>I would like to move all the particles (in the examples: out_RP, off_RP, out_RP, down_RP) right next to the closest preceding verb (i.e. the verb that in combination with the particle makes up the phrasal verb). Here's what the lines should looks like after having changed the word order:  </p>

<ol>
<li>you_PRP mean_VBP we_PRP should_MD kick_VB out_RP them_PRP ._. </li>
<li>don_VB &apos;t_NNP take_VB off_RP it_PRP until_IN I_PRP say_VBP so_RB ._.</li>
<li>please_VB help_VB out_RP the_DT man_NN ._.</li>
<li>shut_VBZ down_RP it_PRP !_.</li>
</ol>

<p>So far I've tried using python and regular expressions to sort the problem by using re.findall:</p>

<pre><code>import re 

file=open('first100k.txt').read()
matchline3='\w*_VB.?\s\w*_DT\s\w*_NN\s\w*_RP'
wordorder1=re.findall(matchline3,file)
print wordorder1
</code></pre>

<p>This will find all the phrasal verbs in word order 1(see below), but that's as far as I've got since I can't figure out how to move the particle next to the verb. Any ideas how to solve this problem properly (not necessarily by using python and regex)? I would like to be able to search for all phrasal verbs and move the particles in the following word orders:</p>

<p>(The used tags are taken from the Penn Treebank tagset (<a href=""http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"" rel=""nofollow"">http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html</a> )(the x denotes an optional character in order to include all verb forms, and * denotes a wildcard word))</p>

<ol>
<li>*_VBx+*_DT+*_NN+*_RP</li>
<li>*_VBx+*_DT+*_NNS+*_RP</li>
<li>*_<em>VBx+*</em>_DT+*_.JJ+*_NN+*_RP</li>
<li><p>*_<em>VBx+*</em>_DT+*_.JJ+*_NNS+*_RP</p></li>
<li><p>*_VBx+*_PRP$+*_NN+*_RP</p></li>
<li>*_VBx+*_PRP$+*_NNS+*_RP</li>
<li>*_<em>VBx+*</em>_PRP$+*_.JJ+*_NN+*_RP</li>
<li><p>*_<em>VBx+*</em>_PRP$+*_.JJ+*_NNS+*_RP</p></li>
<li><p>*_VBx+*_NNP+*_RP</p></li>
<li><p>*_VBx+*_JJ+*_NNP+*_RP</p></li>
<li><p>*_VBx+*_NNPS+*_RP</p></li>
<li><p>*_VBx+*_PRP+*_RP </p></li>
</ol>

<p>In advance, thanks for your help!</p>
"
"16835372","Python Stemming words in a File","2013-05-30 11:50:02","2","6544","4","1","","16836828","<p>I want to do stemming in a file. When I use it in terminal it works fine, but when I apply it in a text file, it does not work. 
Terminal code:</p>

<pre><code>print PorterStemmer().stem_word('complications')
</code></pre>

<p>Function code:</p>

<pre><code>def stemming_text_1():
    with open('test.txt', 'r') as f:
        text = f.read()
        print text
        singles = []    

        stemmer = PorterStemmer() #problem from HERE
        for plural in text:
            singles.append(stemmer.stem(plural))
        print singles
</code></pre>

<p>Input test.txt</p>

<pre><code>126211 crashes bookmarks runs error logged debug core bookmarks
126262 manual change crashes bookmarks propagated ion view bookmarks
</code></pre>

<p>Desired/expected output</p>

<pre><code>126211 crash bookmark runs error logged debug core bookmark
126262 manual change crash bookmark propagated ion view bookmark
</code></pre>

<p>Any suggestion will be greatly appreciated, thanks :)</p>
"
"16791716","Obtain multiple taggings with Stanford POS Tagger","2013-05-28 12:09:54","8","971","1","3","","60792771","<p>I'm performing POS tagging with the <a href=""http://nlp.stanford.edu/software/tagger.shtml"">Stanford POS Tagger</a>. The tagger only returns one possible tagging for the input sentence. For instance, when provided with the input sentence ""The clown weeps."", the POS tagger produces the (erroneous) ""The_DT clown_NN weeps_NNS ._."".</p>

<p>However, my application will try to parse the result, and may reject a POS tagging because there is no way to parse it. Hence, in this example, it would reject ""The_DT clown_NN weeps_NNS ._."" but would accept ""The_DT clown_NN weeps_VBZ ._."" which I assume is a lower-confidence tagging for the parser.</p>

<p>I would therefore like the POS tagger to provide multiple hypotheses for the tagging of each word, annotated by some kind of confidence value. In this way, my application could choose the POS tagging with highest confidence that achieves a valid parsing for its purposes.</p>

<p>I have found no way to ask the Stanford POS Tagger to produce multiple (n-best) tagging hypotheses for each word (or even for the whole sentence). Is there a way to do this? (Alternatively, I am also OK with using another POS tagger with comparable performance that would have support for this.)</p>
"
"16734074","How to get whole sentence that contain particular word?","2013-05-24 11:36:38","-1","1405","5","3","","16735015","<p>I want to get whole sentence and paragraph by finding word. 
For eg: If I search ""released"" in text ""Hundreds of political prisoners have been released, and censorship rules have been relaxed. The EU and US have lifted the majority of sanctions against Burma as a result. "" 
it should to return ""Hundreds of political prisoners have been released, and censorship rules have been relaxed."" and whole paragraph as well. </p>
"
"16717161","Where to find formal grammar for Ruby on Rails, Cobal, and VSL?","2013-05-23 14:50:46","-1","377","0","1","","16721618","<p>Does anyone know where I can get the formal grammar for these languages especially Ruby on Rails?
I'm interested in looking at how the language is processed.</p>

<p>Thanks</p>
"
"16556598","Splitting HTML Content Into Sentences, But Keeping Subtags Intact","2013-05-15 03:59:58","2","2205","6","1","","16556737","<p>I'm using the code below to separate all text within a paragraph tag into sentences. It is working okay with a few exceptions. However, tags within paragraphs are chewed up and spit out. Example:</p>

<pre><code>&lt;p&gt;This is a sample of a &lt;a href=""#""&gt;link&lt;/a&gt; getting chewed up.&lt;/p&gt;
</code></pre>

<p>So, <strong>how can I ignore tags such that I could just parse sentences and place span tags around them and keep , , etc...tags in place?</strong> Or is it smarter to somehow walk the DOM and do it that way?</p>

<pre><code>// Split text on page into clickable sentences
$('p').each(function() {
    var sentences = $(this)
        .text()
        .replace(/(((?![.!?]['""]?\s).)*[.!?]['""]?)(\s|$)/g, 
                 '&lt;span class=""sentence""&gt;$1&lt;/span&gt;$3');
    $(this).html(sentences);
});
</code></pre>

<p>I am using this in a Chrome extension content script; which means that the javascript is injected into any page that it comes in contact with and parses up the <code>&lt;p&gt;</code> tags on the fly. Therefore, it needs to be javascript.</p>
"
"16523067","How to use Stanford parser","2013-05-13 13:17:55","8","11498","4","2","","18140605","<p>I downloaded the Stanford parser 2.0.5 and use Demo2.java source code that is in the package, but After I compile and run the program it has many errors. 
A part of my program is:</p>

<pre><code>public class testStanfordParser {
/** Usage: ParserDemo2 [[grammar] textFile] */
  public static void main(String[] args) throws IOException {
    String grammar = args.length &gt; 0 ? args[0] : ""edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz"";
    String[] options = { ""-maxLength"", ""80"", ""-retainTmpSubcategories"" };
    LexicalizedParser lp = LexicalizedParser.loadModel(grammar, options);
    TreebankLanguagePack tlp = new PennTreebankLanguagePack();
    GrammaticalStructureFactory gsf = tlp.grammaticalStructureFactory();
 ...
</code></pre>

<p>the errors are:</p>

<pre><code>Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz java.io.IOException: Unable to resolve edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz"" as either class path, filename or URL
at edu.stanford.nlp.io.IOUtils.getInputStreamFromURLOrClasspathOrFileSystem(IOUtils.java:408)
at edu.stanford.nlp.io.IOUtils.readStreamFromString(IOUtils.java:356)
at edu.stanford.nlp.parser.lexparser.LexicalizedParser.getParserFromSerializedFile(LexicalizedParser.java:594)
at edu.stanford.nlp.parser.lexparser.LexicalizedParser.getParserFromFile(LexicalizedParser.java:389)
at edu.stanford.nlp.parser.lexparser.LexicalizedParser.loadModel(LexicalizedParser.java:157)
at edu.stanford.nlp.parser.lexparser.LexicalizedParser.loadModel(LexicalizedParser.java:143)
at testStanfordParser.main(testStanfordParser.java:19).                                             Loading parser from text file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz Exception in thread ""main"" java.lang.NoSuchMethodError: edu.stanford.nlp.io.IOUtils.readerFromString(Ljava/lang/String;)Ljava/io/BufferedReader;
at edu.stanford.nlp.parser.lexparser.LexicalizedParser.getParserFromTextFile(LexicalizedParser.java:528)
at edu.stanford.nlp.parser.lexparser.LexicalizedParser.getParserFromFile(LexicalizedParser.java:391)
at edu.stanford.nlp.parser.lexparser.LexicalizedParser.loadModel(LexicalizedParser.java:157)
at edu.stanford.nlp.parser.lexparser.LexicalizedParser.loadModel(LexicalizedParser.java:143)
at testStanfordParser.main(testStanfordParser.java:19)
</code></pre>

<p>please help me to solve it.
Thanks</p>
"
"16381218","How do I get the definition for a sense in NLTK's senseval module?","2013-05-05 03:50:54","2","1166","0","1","","16391584","<p>In the NLTK <code>senseval</code> module, senses are of the form <code>HARD1</code>, <code>HARD2</code>, etc. (see source <a href=""http://nltk.org/_modules/nltk/corpus/reader/senseval.html"" rel=""nofollow"">here</a>). However, there doesn't seem to be a way to get the actual definition. I'm trying to implement the Lesk algorithm, and I'm now attempting to check whether the sense predicted by the Lesk algorithm is correct (using a definition from WordNet).</p>

<p>The problem I'm running into is how to unify the WordNet definition with the <code>senseval</code> answer (<code>HARD1</code>, <code>HARD2</code>). Does anybody know how to translate the SENSEVAL sense into a definition, or look it up somewhere?</p>
"
"16325390","Match alphanumeric string in nltk grammar","2013-05-01 19:52:52","5","1267","2","1","","24316054","<p>I'm trying to use NTLK grammar and parse algorithms as they seem pretty simple to use. Though, I can't find a way to match an alphanumeric string properly, something like:</p>

<pre><code>import nltk
grammar = nltk.parse_cfg (""""""
# Is this possible?
TEXT -&gt; \w*  
"""""")

parser = nltk.RecursiveDescentParser(grammar)

print parser.parse(""foo"")
</code></pre>

<p>Is there an easy way to achieve this?</p>
"
"16251708","Python Openopt Integration Prob (IP) interalg TypeError 'module' object is not callable","2013-04-27 11:43:49","0","448","0","1","","18540406","<p>I am trying to run the program found here:
<a href=""http://trac.openopt.org/openopt/browser/PythonPackages/FuncDesigner/FuncDesigner/examples/integrate3.py?rev=1214"" rel=""nofollow"">FuncDesigner integration example 3</a>
However i get the error/output:</p>

<pre><code>------------------------- OpenOpt 0.34 -------------------------
solver: interalg_0.21   problem: unnamed    type: IP
 iter   objFunVal   
    0  0.000e+00 
OpenOpt Warning: solver interalg_0.21 require p.fTol value (required objective function     tolerance); 10^-7 will be used
Traceback (most recent call last):
  File ""/home/morten/Desktop/integrate3.py"", line 27, in &lt;module&gt;
r = p.solve('interalg', maxIter = 50000, maxActiveNodes = 150, maxNodes = 500000,     iprint = 100)
  File ""/usr/lib/pymodules/python2.7/openopt/kernel/baseProblem.py"", line 235, in solve
    return runProbSolver(self, *args, **kwargs)
File ""/usr/lib/pymodules/python2.7/openopt/kernel/runProbSolver.py"", line 237, in runProbSolver
solver(p)
File ""/usr/lib/pymodules/python2.7/openopt/solvers/UkrOpt/interalg_oo.py"", line 226, in __solver__
ip = func10(y, e, vv)
File ""/usr/lib/pymodules/python2.7/openopt/solvers/UkrOpt/interalgMisc.py"", line 42, in func10
domain = ooPoint(domain, skipArrayCast = True)
TypeError: 'module' object is not callable
</code></pre>

<p>I have searched and found i wrongfully call a module instead of a class if i am not mistaken, but i can't find my error.</p>

<p>(In the end what i would like to do is minimize over an integration using FuncDesigner, but right now i am stuck at the integration part.)</p>

<p>Any help would be appreciated. </p>
"
"16181419","Is it possible to speed up Wordnet Lemmatizer?","2013-04-24 00:30:58","19","7912","0","2","","17117425","<p>I'm using the Wordnet Lemmatizer via NLTK on the Brown Corpus (to determine if the nouns in it are used more in their singular form or their plural form).<br>
i.e.  <code>from nltk.stem.wordnet import WordNetLemmatizer</code><br>
<code>l = WordnetLemmatizer()</code>  </p>

<p>I've noticed that even the simplest queries such as the one below takes quite a long time (at least a second or two).<br>
<code>l(""cats"")</code></p>

<p>Presumably this is because a web connection must be made to Wordnet for each query?..<br>
I'm wondering if there is a way to still use the Wordnet Lemmatizer but have it perform much faster?  For instance, would it help at all for me to download Wordnet on to my machine?
Or any other suggestions? </p>

<p>I'm trying to figure out if the Wordnet Lemmatizer can be made faster rather than trying a different lemmatizer, because I've found it works the best among others like Porter and Lancaster.</p>
"
"16074238","Stanford POS Tagger not tagging Chinese text","2013-04-18 04:00:16","5","931","1","1","","16093199","<p>I'm using Stanford POS Tagger (for the first time) and while it tags English correctly, it does not seem to recognize (Simplified) Chinese even when changing the model parameter. Have I overlooked something?</p>
<p>I've downloaded and unpacked the latest full version from here:
<a href=""http://nlp.stanford.edu/software/tagger.shtml"" rel=""nofollow noreferrer"">http://nlp.stanford.edu/software/tagger.shtml</a></p>
<p>Then I've inputed sample text into the &quot;sample-input.txt&quot;.</p>
<blockquote>
<p>这是一个测试的句子。这是另一个句子。</p>
</blockquote>
<p>Then I simply run</p>
<blockquote>
<p>./stanford-postagger.sh models/chinese-distsim.tagger sample-input.txt</p>
</blockquote>
<p>The expected output is to tag each of the words with a part of speech, but instead it recognizes the entire string of text as one word:</p>
<blockquote>
<p>Loading default properties from tagger models/chinese-distsim.tagger</p>
<p>Reading POS tagger model from models/chinese-distsim.tagger ... done [3.5 sec].</p>
<p>這是一個測試的句子。這是另一個句子。#NR</p>
<p>Tagged 1 words at 30.30 words per second.</p>
</blockquote>
<p>I appreciate any help.</p>
"
"16062511","Should I remove Stop words with POS tagging?","2013-04-17 14:17:14","1","2174","0","1","","16064369","<p>I'm new to this NLP stuff but all the examples of POS tagging and Sentence Chunking I have seen don't seem to have removed stops words. So question I have if I'm doing POS tagging and Chunking is does this remove the need to remove stopwords (and also Stem)?</p>
"
"16026881","Stanford Dependency Parser - how to get spans?","2013-04-16 00:36:40","4","1028","0","2","","16208956","<p>I'm doing dependency parsing with the Stanford library in Java.
Is there any way to get back the indices within my original string of a dependency?
I have tried to call the getSpans() method, but it returns null for every token:</p>

<pre><code>LexicalizedParser lp = LexicalizedParser.loadModel(
        ""edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz"",
        ""-maxLength"", ""80"", ""-retainTmpSubcategories"");
TreebankLanguagePack tlp = new PennTreebankLanguagePack();
GrammaticalStructureFactory gsf = tlp.grammaticalStructureFactory();
Tree parse = lp.apply(text);
GrammaticalStructure gs = gsf.newGrammaticalStructure(parse);
Collection&lt;TypedDependency&gt; tdl = gs.typedDependenciesCollapsedTree();
for(TypedDependency td:tdl)
{
      td.gov().getSpan()  // it's null!
      td.dep().getSpan()  // it's null!
}
</code></pre>

<p>Any idea?</p>
"
"15931765","Parser tags for OpenNLP","2013-04-10 16:46:18","2","1012","1","1","","15938300","<p>Is there any documentation about the meaning of parser tags in OpenNLP? I know that the POS tag types follows the TreeBank convention, but unfortunately I haven't found any information about the parser tags, such as ""SBAR"", etc.</p>

<p>Does this documentation exist somewhere or I have to figure it out myself?</p>
"
"15916143","Checking English Grammar with NLTK","2013-04-10 02:58:02","4","4611","1","1","","15931170","<p>I'm starting to use the <a href=""http://nltk.org/"" rel=""nofollow noreferrer"">NLTK library</a>, and I want to check whether a sentence in English is correct or not.</p>

<p>Example:</p>

<p>""He see Bob"" - not correct</p>

<p>""He sees Bob"" - correct</p>

<p>I read <a href=""http://www.ling.helsinki.fi/kit/2008s/clt231/nltk-0.9.5/doc/en/ch08.html"" rel=""nofollow noreferrer"">this</a>, but it's quite hard for me. 
I need an easier example.</p>
"
"15867808","How to determine a word is English or any other language","2013-04-07 21:05:50","2","2267","3","2","","15867989","<p>I am developing a small library automation software and I need to determine a word is in <a href=""http://en.wikipedia.org/wiki/English_language"" rel=""nofollow"">English</a> or <a href=""http://en.wikipedia.org/wiki/Turkish_language"" rel=""nofollow"">Turkish</a>. An example scenario is like this:</p>

<ul>
<li>User enters a book title.</li>
<li>Determine it's Turkish or English.</li>
<li>Set the languge combobox to the respective language to help user fill the form. </li>
</ul>

<p>A friend of mine suggested me ""connect to Google Translate and use it"" which seems reasonable but an algorithm without connecting an external service or database will be more appropriate for me. (I also search the Turkish/English specific characters like ç,ş,İ/w,x to decide) Therefore I am searching an algorithm to do this job maybe based on letter frequencies or something like it. Anything available in literature? Thanks, in advance. (I use php, mysql if it's important)</p>
"
"15827947","What other inputs are there to Word Sense Disambiguation task?","2013-04-05 07:13:36","0","174","0","1","","15902853","<p>In <code>Natural Language Processing</code> (NLP), the <code>Word Sense Disambiguation</code> (WSD) task computationally determines the meaning(s) or sense(s) or concept(s) of a polysemous word given a sentence that the word appears in. For example:</p>

<ul>
<li><em>""Some was stupid enough to rob the central</em> <strong>bank*</strong>.""*</li>
<li><em>""The river</em> <strong>bank</strong> <em>is full of stones""</em></li>
</ul>

<p><strong>Do anyone know on WSD performed in paragraph or document level?</strong></p>

<p>Other than disambiguate senses/meaning from context words in one sentence, <strong>what other input could be introduce to perform <code>WSD</code> task?</strong>  (I've seen WSD with images before, <a href=""http://acl.ldc.upenn.edu/W/W03/W03-0601.pdf"" rel=""nofollow"">http://acl.ldc.upenn.edu/W/W03/W03-0601.pdf</a>)</p>
"
"15768680","NLTK Regular Expressions and CFGs","2013-04-02 15:57:53","3","1509","0","1","","15770184","<p>Is there any practical difference in power between a 'regular expression' as exampled by NLTK's docs and a CFG from the same?  There definitely should be, since there are context-free languages which are not regular, but I can't find a concrete example where the CFG approach outshines a regular expression.</p>

<p><a href=""http://nltk.org/book/ch07.html"" rel=""nofollow"">http://nltk.org/book/ch07.html</a></p>
"
"15727144","Increase performance of Stanford-tagger based program","2013-03-31 06:53:41","8","693","0","1","","15800786","<p>I just implemented a program that uses the Stanford POS tagger in Java.</p>

<p>I used an input file of a few KB in size, consisting of a few hundred words. I even set the heap size to 600 MB.</p>

<p>But it is still slow and sometimes runs out of heap memory. How can I increase its execution speed and memory performance? I would like to be able to use a few MB as input.</p>

<pre><code>  public static void postag(String args) throws ClassNotFoundException

  {

     try

     {

     File filein=new File(""c://input.txt"");

     String content = FileUtils.readFileToString(filein);

     MaxentTagger tagger = new MaxentTagger(""postagging/wsj-0-18-bidirectional-distsim.tagger"");

     String tagged = tagger.tagString(content);

        try 
        {
            File file = new File(""c://output.txt"");
            if (!file.exists()) 
            {
                file.createNewFile();
            } 

            FileWriter fw = new FileWriter(file.getAbsoluteFile());
            BufferedWriter bw = new BufferedWriter(fw);
            bw.write(""\n""+tagged);
            bw.close();

            }
              catch (IOException e) 
              {
                    e.printStackTrace();
               }

     } catch (IOException e1)
     {
         e1.printStackTrace();
     }

 }
</code></pre>
"
"15687440","How to do part-of-speech tagging of texts, containing mathematical expressions?","2013-03-28 16:40:34","2","467","6","2","","21759006","<p>The goal is a syntactic parsing of scientific texts. And first I need to make part-of-speech tagging of sentences of such texts. Texts are from arxiv.org. So they are originally in LaTeX. When extracting text from LaTeX documents, math expressions can be converted into MathML (or maybe some other format, but I prefer MathML cause this work is being done to create a specific web-app, and MathML is a convenient tool for this).</p>

<p>The only idea I have is to substitute mathematical expressions with some phrases of natural language and then use some implemented algorithm for pos-tagging. So the question is how to implement this substitutions or, in general, how to implement pos-tagging of texts with mathematics in them?</p>
"
"15625509","How to determine semantic hierarchies / relations in using NLTK?","2013-03-25 21:53:09","0","1692","1","1","","15651748","<p>I want to use NLTK and wordnet to understand the semantic relation between two words. Like if I enter ""employee"" and ""waiter"", it returns something showing that employee is more general than waiter. Or for ""employee"" and ""worker"", it returns equal. Does anyone know how to do that?</p>
"
"15594626","Determine POS tagging in English based on database files","2013-03-24 02:49:35","2","1672","1","1","","15624344","<p>I'm a little bit confused how to determine part-of-speech tagging in English. In this case, I assume that one word in English has one type, for example word ""book"" is recognized as NOUN, not as VERB. I want to recognize English sentences based on tenses. For example, ""I sent the book"" is recognized as past tense.</p>

<p>Description:</p>

<p>I have a number of database (*.txt) files: NounList.txt, verbList.txt, adjectiveList.txt, adverbList.txt, conjunctionList.txt, prepositionList.txt, articleList.txt. And if input words are available in the database, I assume that type of those words can be concluded. But, how to begin lookup in the databases? For example, ""I sent the book"": how to begin a search in the databases for every word, ""I"" as Noun, ""sent"" as verb, ""the"" as article, ""book"" as noun? Any better approach than searching every word in every database? I doubt that every databases has unique element.</p>

<p>I enclose my perspective here.</p>

<pre><code>private List&lt;string&gt; ParseInput(String allInput)
{
    List&lt;string&gt; listSentence = new List&lt;string&gt;();

    char[] delimiter = "".?!;"".ToCharArray();
    var sentences = allInput.Split(delimiter, StringSplitOptions.RemoveEmptyEntries).Select(s =&gt; s.Trim());

    foreach (var s in sentences)
        listSentence.Add(s);

        return listSentence;
}

private void tenseReviewMenu_Click(object sender, EventArgs e)
    {
        string allInput = rtbInput.Text;

        List&lt;string&gt; listWord = new List&lt;string&gt;();
        List&lt;string&gt; listSentence = new List&lt;string&gt;();

        HashSet&lt;string&gt; nounList = new HashSet&lt;string&gt;(getDBList(""nounList.txt""));
        HashSet&lt;string&gt; verbList = new HashSet&lt;string&gt;(getDBList(""verbList.txt""));
        HashSet&lt;string&gt; adjectiveList = new HashSet&lt;string&gt;(getDBList(""adjectiveList.txt""));
        HashSet&lt;string&gt; adverbList = new HashSet&lt;string&gt;(getDBList(""adverbList.txt""));

        char[] separator = new char[] { ' ', '\t', '\n', ',' etc... };         

        listSentence = ParseInput(allInput);

        foreach (string sentence in listSentence)
        {
            foreach (string word in sentence.Split(separator))
                if (word.Trim() != """")
                    listWord.Add(word);               
        }

        string testPOS = """";

        foreach (string word in listWord)
        {
            if (nounList.Contains(word.ToLowerInvariant()))
                testPOS += ""noun "";
            else if (verbList.Contains(word.ToLowerInvariant()))
                testPOS += ""verb "";
            else if (adjectiveList.Contains(word.ToLowerInvariant()))
                testPOS += ""adj "";
            else if (adverbList.Contains(word.ToLowerInvariant()))
                testPOS += ""adv "";

        }
        tbTest.Text = testPOS;
    }
</code></pre>

<p>POS tagging is my secondary explanation in my assignment. So I use a simple approach to determine POS tagging that is based on database. But, if there's a simpler approach: easy to use, easy to understand, easy to get pseudocode, easy to design... to determine POS tagging, please let me know.</p>
"
"15586721","wordnet lemmatization and pos tagging in python","2013-03-23 12:23:54","78","104520","1","8","","15590384","<p>I wanted to use wordnet lemmatizer in python and I have learnt that the default pos tag is NOUN and that it does not output the correct lemma for a verb, unless the pos tag is explicitly specified as VERB.</p>

<p>My question is what is the best shot inorder to perform the above lemmatization accurately?</p>

<p>I did the pos tagging using <code>nltk.pos_tag</code> and I am lost in integrating the tree bank pos tags to wordnet compatible pos tags. Please help</p>

<pre><code>from nltk.stem.wordnet import WordNetLemmatizer
lmtzr = WordNetLemmatizer()
tagged = nltk.pos_tag(tokens)
</code></pre>

<p>I get the output tags in NN,JJ,VB,RB. How do I change these to wordnet compatible tags?</p>

<p>Also do I have to train <code>nltk.pos_tag()</code> with a tagged corpus or can I use it directly on my data to evaluate?</p>
"
"15503388","TreeTagger installation successful but cannot open .par file","2013-03-19 15:17:27","5","8192","0","3","","15508322","<p>Do anyone know how to resolve this file reading error in <code>TreeTagger</code> that is a common Natural Language Processing tool used to <code>POS</code> tag, lemmatize and chunk sentences?</p>

<pre><code>alvas@ikoma:~/treetagger$ echo 'Hello world!' | cmd/tree-tagger-english 
        reading parameters ...

ERROR: Can't open for reading: /home/alvas/treetagger/lib/english.par
aborted.
</code></pre>

<p>I didn't encounter any possible installation problems as hinted on <a href=""http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/installation-hints.txt"" rel=""nofollow"">http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/installation-hints.txt</a>. 
I've followed the instructions on the webpage and it's installed properly (<a href=""http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/#Linux"" rel=""nofollow"">http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/#Linux</a>):</p>

<pre><code>alvas@ikoma:~$ mkdir treetagger
alvas@ikoma:~$ cd treetagger
alvas@ikoma:~/treetagger$ wget ftp://ftp.ims.uni-stuttgart.de/pub/corpora/tree-tagger-linux-3.2.tar.gz
alvas@ikoma:~/treetagger$ wget ftp://ftp.ims.uni-stuttgart.de/pub/corpora/tagger-scripts.tar.gz
alvas@ikoma:~/treetagger$ wget ftp://ftp.ims.uni-stuttgart.de/pub/corpora/install-tagger.sh
alvas@ikoma:~/treetagger$ wget ftp://ftp.ims.uni-stuttgart.de/pub/corpora/dutch-par-linux-3.2-utf8.bin.gz
alvas@ikoma:~/treetagger$ wget ftp://ftp.ims.uni-stuttgart.de/pub/corpora/german-par-linux-3.2-utf8.bin.gz
alvas@ikoma:~/treetagger$ wget ftp://ftp.ims.uni-stuttgart.de/pub/corpora/italian-par-linux-3.2-utf8.bin.gz
alvas@ikoma:~/treetagger$ wget ftp://ftp.ims.uni-stuttgart.de/pub/corpora/spanish-par-linux-3.2-utf8.bin.gz
alvas@ikoma:~/treetagger$ wget ftp://ftp.ims.uni-stuttgart.de/pub/corpora/french-par-linux-3.2-utf8.bin.gz

alvas@ikoma:~/treetagger$ sh install-tagger.sh 

Linux version of TreeTagger installed.
Tagging scripts installed.
German parameter file (Linux, UTF8) installed.
German chunker parameter file (Linux) installed.
French parameter file (Linux, UTF8) installed.
French chunker parameter file (Linux, UTF8) installed.
Italian parameter file (Linux, UTF8) installed.
Spanish parameter file (Linux, UTF8) installed.
Dutch parameter file (Linux, UTF8) installed.
Path variables modified in tagging scripts.

You might want to add /home/alvas/treetagger/cmd and /home/alvas/treetagger/bin to the PATH variable so that you do not need to specify the full path to run the tagging scripts.
</code></pre>

<p><strong>But when i try to test the software i get these errors:</strong></p>

<pre><code>alvas@ikoma:~/treetagger$ echo 'Hello world!' | cmd/tree-tagger-english 
    reading parameters ...

ERROR: Can't open for reading: /home/alvas/treetagger/lib/english.par
aborted.
alvas@ikoma:~/treetagger$ echo 'Das ist ein Test.' | cmd/tagger-chunker-german

ERROR: Can't open for reading: /home/alvas/treetagger/lib/german-chunker.par
aborted.

ERROR: Can't open for reading: /home/alvas/treetagger/lib/german.par
aborted.
    reading parameters ...

ERROR: Can't open for reading: /home/alvas/treetagger/lib/german.par
aborted.
</code></pre>
"
"15431139","java program to get parse score of a sentence using stanford parser","2013-03-15 11:23:11","4","2177","0","1","","15449643","<p>I am able to get the output of Tags and words for the sentence like ""My name is Rahul."" as </p>

<blockquote>
  <p>My/PRP$, name/NN, is/VBZ, Rahul/NNP, ./.]</p>
</blockquote>

<p>with the program:</p>

<pre><code>LexicalizedParser lp = LexicalizedParser.loadModel(
    ""edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz""
);
lp.setOptionFlags(new String[]{""-maxLength"", ""80"", ""-retainTmpSubcategories""});

String sent = ""My name is Rahul"";
Tree parse = (Tree) lp.apply(sent);

List taggedWords = parse.taggedYield();
System.out.println(taggedWords);
</code></pre>

<p>But, I also need to get the parse score of the sentence. Is there any kind of modification that I can do to my program to get the parse score? </p>

<p>Thanks.</p>
"
"15320894","Python, ImportError: MaxentClassifier","2013-03-10 09:46:38","2","1827","9","2","","15321784","<p>I am readying the book Natural Language Processing with Python. </p>

<p><a href=""http://rafale.org/~mattoufoutu/ebooks/Doc_diverse/Cours%20Prog/Python/Books/Natural%20Language%20Processing%20with%20Python%20%282009%29.pdf"" rel=""nofollow"">http://rafale.org/~mattoufoutu/ebooks/Doc_diverse/Cours%20Prog/Python/Books/Natural%20Language%20Processing%20with%20Python%20%282009%29.pdf</a></p>

<p>Paragraph 7.5   Named Entity Recognition.</p>

<p>I follow the example and write:</p>

<pre><code>sent = nltk.corpus.treebank.tagged_sents()[22]
print nltk.ne_chunk(sent, binary=True)
</code></pre>

<p>I receive the following error:</p>

<pre><code>Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/usr/local/lib/python2.7/dist-packages/nltk/chunk/__init__.py"", line 176, in ne_chunk
    chunker = load(chunker_pickle)
  File ""/usr/local/lib/python2.7/dist-packages/nltk/data.py"", line 605, in load
    resource_val = pickle.load(_open(resource_url))
  File ""/usr/local/lib/python2.7/dist-packages/nltk/chunk/named_entity.py"", line 16, in &lt;module&gt;
    from nltk.classify import MaxentClassifier
ImportError: cannot import name MaxentClassifier
</code></pre>

<p>Any idea why it doesn't work?</p>

<p>Thanks!</p>

<p>EDIT: content of the file: /usr/local/lib/python2.7/dist-packages/nltk/classify/<strong>init</strong>.pyc</p>

<pre><code>from nltk.classify.api import ClassifierI, MultiClassifierI
from nltk.classify.mallet import config_mallet, call_mallet
from nltk.classify.megam import config_megam, call_megam
from nltk.classify.weka import WekaClassifier, config_weka
from nltk.classify.naivebayes import NaiveBayesClassifier
from nltk.classify.positivenaivebayes import PositiveNaiveBayesClassifier
from nltk.classify.decisiontree import DecisionTreeClassifier
from nltk.classify.rte_classify import rte_classifier, rte_features, RTEFeatureExtractor
from nltk.classify.util import accuracy, apply_features, log_likelihood

# Conditional imports

try:
    from scikitlearn import SklearnClassifier
except ImportError:
    pass

try:
    import numpy
    from nltk.classify.maxent import (MaxentClassifier, BinaryMaxentFeatureEncoding,
                                      TypedMaxentFeatureEncoding,
                                      ConditionalExponentialClassifier)
    import svmlight
    from nltk.classify.svm import SvmClassifier
except ImportError:
    pass
</code></pre>
"
"15169472","How to stop memory memory leak from Flask and NLTK","2013-03-02 01:05:54","0","2515","1","1","","15169584","<p>I'm building a web application using NLTK and Flask. It's just a simple RESTful application I deployed it on heroku everything went well. However, when the server started getting more request I reached the memory limit from heroku which is 1.5GB. So, I'm guessing it's because I'm loading <code>nltk.RegexpParser</code> every time the request comes. </p>

<p>This is the code which is really simple. </p>

<pre>
<code>

@app.route('/get_keywords', methods=['POST'])
def get_keywords():
    data_json = json.loads(request.data)
    text = urllib.unquote(data_json[""sentence""])
    keywords = KeywordExtraction().extract(text)

    return ','.join(keywords)
</code>
</pre>

<p>And this is the keyword extraction bit. </p>

<pre>
<code>
import re
import nltk

nltk.data.path.append('./nltk_data/')

from nltk.corpus import stopwords

class KeywordExtraction:
    def extract(self, text):

        sentences = nltk.sent_tokenize(text)
        sentences = [nltk.word_tokenize(sent) for sent in sentences]
        sentences = [nltk.pos_tag(sent) for sent in sentences]

        grammar = ""NP: {}""
        cp = nltk.RegexpParser(grammar)
        tree = cp.parse(sentences[0])

        keywords = [subtree.leaves()[0][0] for subtree in tree.subtrees(filter=lambda t: t.node == 'NP')]
        keywords_without_stopwords = [w for w in keywords if not w in stopwords.words('english')]

        return list(set(keywords_without_stopwords + tags))
</code>
</pre>

<p>I'm not sure if it's the problem with my code or Flask or NLTK. I'm pretty new in Python. Any suggestions would be really appreciated. </p>

<p>I tested this by blitz.io and after just 250 requests the server blew up and started throwing R15.</p>
"
"15036048","Why did the tf-idf model in `gensim` throws away the terms and counts after i transform the corpus?","2013-02-23 01:40:55","2","2126","0","1","","15403585","<p>Why did the tf-idf model in <code>gensim</code> throws away the terms and counts after i transform the corpus?</p>

<p>My code:</p>

<pre><code>from gensim import corpora, models, similarities

# Let's say you have a corpus made up of 2 documents.
doc0 = [(0, 1), (1, 1)]
doc1 = [(0,1)]
doc2 = [(0, 1), (1, 1)]
doc3 = [(0, 3), (1, 1)]

corpus = [doc0,doc1,doc2,doc3]

# Train a tfidf model using the corpus
tfidf = models.TfidfModel(corpus)

# Now if you print the corpus, it still remains as the flat frequency counts.
for d in corpus:
  print d
print 

# To convert the corpus into tfidf, re-initialize the corpus 
# according to the model to get the normalized frequencies.
corpus = tfidf[corpus]

for d in corpus:
  print d
</code></pre>

<p>Outputs:</p>

<pre><code>[(0, 1.0), (1, 1.0)]
[(0, 1.0)]
[(0, 1.0), (1, 1.0)]
[(0, 3.0), (1, 1.0)]

[(1, 1.0)]
[]
[(1, 1.0)]
[(1, 1.0)]
</code></pre>
"
"14933345","List of stopwords for NLP","2013-02-18 09:46:11","3","4811","0","1","","14935235","<p>Is there a list of stop words that people usually use to remove punctuations and close class words (such as <code>he, she, it</code>) when performing NLP or IR/IE related task?</p>

<p>I have been trying out topic modeling using gibbs sampling for word sense disambiguation and it keeps giving punctuations and close class words high probabilities just because they appear frequently in the corpus. <a href=""https://github.com/christianscheible/BNB/blob/master/nb_gibbs.py"" rel=""nofollow"">https://github.com/christianscheible/BNB/blob/master/nb_gibbs.py</a> </p>
"
"14822609","Creating a feature function for POS tagging","2013-02-11 23:05:42","2","4017","0","1","","14831206","<p>I am trying to use a Perceptron to perform supervised classification and thereby perform POS tagging of a sentence. I am assuming for now that the tags of each word is independent of the other. (i.e I am just using just the word as a feature). I am fairly new to Machine Learning algorithms, and so I am unable to figure out how to represent the feature function for each word.</p>

<p>I have a training set of 100 sentences, where each word is given a particular tag (say N, V, J(adjective) and so on). 
For instance, </p>

<blockquote>
  <p>Jack(N) and(&amp;) Jill(N) went(V) to(PRP) Peru(N)</p>
</blockquote>

<p>where the tags are in braces. Lets say I have a total of 10 possible tags.
Now my question is how does the feature vector for the word Jack look like? </p>

<p>I am very much interested in implementing it as a vector, since my code will match the notation better. Once I figure out how the feature function looks like, I will be able to implement the Perceptron algorithm!</p>

<p>Also, say I want to add features like (a) Is first letter capitalized? (b) Is word hyphenated etc., How do I incorporate that into my feature vector?</p>

<p>Intuitively I can see that the vector needs to have only binary values, but I am unable to proceed beyond that.</p>

<p>Kindly try to explain with concrete examples if possible!</p>
"
"14802442","How to use a regex backoff tagger in python NLTK to override NN's?","2013-02-10 20:52:31","0","2330","1","1","","15154234","<p>I've been using a custom trained nltk pos_tagger and sometimes I get obvious verbs (ending with ING or ED) come in as NN's.  How do I get the tagger to process all NN's through an additional regexpTagger just to find the additional verbs?</p>

<p>I've included some sample code for the secondary regex tagger.</p>

<pre><code>from nltk.tag.sequential import RegexpTagger

rgt = RegexpTagger(
    (r'.*ing$', 'VBG'),                # gerunds
    (r'.*ed$', 'VBD'),                 # past tense verbs
])
</code></pre>

<p>Thanks</p>
"
"14760902","Are there any off-the-shelf solutions for lexical analysis in Haskell that allow for a run-time dynamic lexicon?","2013-02-07 21:01:45","7","295","7","1","","14769695","<p>I'm working on a small Haskell project that needs to be able to lex a very small subset of strictly formed English in to tokens for semantic parsing.  It's a very naïve natural language interface to a system with many different end effectors than can be issued commands.  I'm currently using Alex for this, but Alex relies on its lexicon to be statically compiled.  The nature of the system is such that the number and even type of end effectors in the world can increase as well as decrease after compilation, and so I need to be able to add or remove viable tokens from the lexicon at runtime.</p>

<p>I've tried looking around for dynamic lexing solutions, and the closest I could get was <a href=""http://www.nondot.org/sabre/Projects/HaskellLexer/"">this</a> Dynamic Lexer Engine that doesn't look to have been updated since 2000.</p>

<p>I've been considering some techniques like using a less-high level approach (Attoparsec, perhaps), or even wiring up a recompilation hook for Alex  and separating the lexer from the rest of the application.</p>

<p>Are there any well-known solutions for this sort of lexical analysis?  I intend on working through <a href=""http://nlpwp.org/book/index.xhtml"">Natural Language Processing for the Working Programmer</a> eventually so I can take a less simplified approach, but currently a basically lexer is what I need.</p>
"
"14540630","Comparison of binary vs tfidf Ngram features in sentiment analysis / classification tasks?","2013-01-26 19:19:52","4","1613","3","1","","14548480","<p>Simple question again: Is it better to use Ngrams (unigram/ bigrams etc) as simple binary features  or rather use their Tfidf scores in ML models such as Support Vectory Machines for performing NLP tasks such as sentiment analysis or text categorization/classification?</p>
"
"14539609","In the NLTK, how to interface to Boxer?","2013-01-26 17:35:27","1","931","0","1","","14544057","<p>I want to be able to use Boxer as a semantic extractor inside NLTK.</p>

<p>I am testing with the following code:</p>

<pre><code>#!/bin/env python
import nltk
x = nltk.sem.boxer.Boxer()
x.interpret(""The capital of Spain is Madrid ."")
</code></pre>

<p>The failure is the following:</p>

<pre><code>Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
    File ""/usr/lib/python2.7/site-packages/nltk/sem/boxer.py"", line 83, in interpret
        d, = self.batch_interpret_multisentence([[input]], discourse_ids, question, verbose)
      File ""/usr/lib/python2.7/site-packages/nltk/sem/boxer.py"", line 140, in batch_interpret_multisentence
          drs_dict = self._parse_to_drs_dict(boxer_out, use_disc_id)
            File ""/usr/lib/python2.7/site-packages/nltk/sem/boxer.py"", line 241, in _parse_to_drs_dict
                line = lines[i]
                IndexError: list index out of range
</code></pre>

<p>From the nltk code, I found at <a href=""http://nltk.org/_modules/nltk/sem/boxer.html#Boxer"" rel=""nofollow"">http://nltk.org/_modules/nltk/sem/boxer.html#Boxer</a> that in the _parse_to_drs_dict(self, boxer_out, use_disc_id) function, it does a <code>i += 4</code> that I haven't been able to understand.</p>

<p>Am I feeding something bad to the Boxer?</p>

<p>Did anyone manage to make it work?</p>

<p>Manually debugging step-by-step, the NLTK actually gets the output from candc and boxer.</p>
"
"14529782","How do I use Regexp Tagger in nltk?","2013-01-25 20:24:12","3","4204","0","1","","14533498","<p>If I try this code :</p>

<pre><code>import nltk
pattern = [(r'(March)$','MAR')]
tagger=nltk.RegexpTagger(pattern)
print tagger.tag('He was born in March 1991')
</code></pre>

<p>I get an output likr this:</p>

<blockquote>
  <p>[('H', None), ('e', None), (' ', None), ('w', None), ('a', None), ('s', None), (' ', None), >('b', None), ('o', None), ('r', None), ('n', None), (' ', None), ('i', None), ('n', None), (' ', None), ('M', None), ('a', None), ('r', None), ('c', None), ('h', None), (' ', None), ('1', None), ('9', None), ('9', None), ('1', None)]</p>
</blockquote>

<p>In fact I would like this tagger to recognise 'March' word with 'MAR' tag.</p>
"
"14529633","Python script to insert space between different character types: Why is this *so* slow?","2013-01-25 20:13:08","0","3631","5","6","","14529735","<p>I'm working with some text that has a mix of languages, which I've already done some processing on and is in the form a list of single characters (called ""letters""). I can tell which language each character is by simply testing if it has case or not (with a small function called ""test_lang"").  I then want to insert a space between characters of different types, so I don't have any words that are a mix of character types. At the same time, I want to insert a space between words and punctuation (which I defined in a list called ""punc"").  I wrote a script that does this in a very straight-forward way that made sense to me (below), but apparently is the <strong>wrong</strong> way to do it, because it is incredibly slow.</p>

<p>Can anyone tell me what the better way to do this is?</p>

<pre><code># Add a space between Arabic/foreign mixes, and between words and punc
cleaned = """"
i = 0
while i &lt;= len(letters)-2: #range excludes last letter to avoid Out of Range error for i+1
    cleaned += letters[i]
    # words that have case are Latin; otherwise Arabic
    if test_lang(letters[i]) != test_lang(letters[i+1]):
        cleaned += "" ""
    if letters[i] in punc or letters[i+1] in punc:
        cleaned += "" ""
    i += 1
cleaned += letters[len(letters)-1] # add in last letter
</code></pre>
"
"14510028","Extract product name from english text","2013-01-24 20:22:20","0","1967","4","1","","14510125","<p>I want extract the names of products being sold from English text. </p>

<p>For example:</p>

<blockquote>
  <p>""I'm selling my xbox brand new""</p>
  
  <p>""Selling rarely used 27 inch TV""</p>
</blockquote>

<p>Should give me <code>""xbox""</code> and <code>""27 inch TV""</code></p>

<p>The only thing I can think of at the moment is to hardcode in a giant list of important nouns and important adjectives: <code>['tv', 'fridge', 'xbox', 'laptop', etc]</code></p>

<p>Is there a better approach?</p>
"
"14506969","NLTK POS tagger not working","2013-01-24 17:15:32","6","8270","0","2","","14508096","<p>If I try this :</p>

<pre><code>import nltk
text = nltk.word_tokenize(""And now for something completely different"")
nltk.pos_tag(text)
</code></pre>

<p>Output:</p>

<pre><code>Traceback (most recent call last):
File ""C:/Python27/pos.py"", line 3, in &lt;module&gt;
nltk.pos_tag(text)
File ""C:\Python27\lib\site-packages\nltk-2.0.4-py2.7.egg\nltk\tag\__init__.py"" ipos_tag
tagger = load(_POS_TAGGER)
File ""C:\Python27\lib\site-packages\nltk-2.0.4-py2.7.egg\nltk\data.py"", line 605,in 
resource_val = pickle.load(_open(resource_url))
ImportError: No module named numpy.core.multiarray
</code></pre>
"
"14473017","how to train the stanford LexicalizedParser to recognize new words as nouns?","2013-01-23 05:24:26","1","1197","0","1","","14490891","<p>I am trying to figure out how to train the stanford LexicalizedParser<br>
( edu.stanford.nlp.parser.lexparser.LexicalizedParser )  to incorporate new nouns into its lexicon.</p>

<p>At first my goal was to take take an existing model and tweak it slightly, rather than creating a brand new model 
from a vast set of training examples.</p>

<p>the answer to this question suggests that is not possible > 
    <a href=""https://stackoverflow.com/questions/5570765/how-can-i-add-more-tagged-words-to-the-stanford-pos-taggers-trained-models"">How can I add more tagged words to the Stanford POS-Tagger&#39;s trained models?</a></p>

<p>Hopefully someone out there can put me on the right track as to how to do this.</p>

<p>As a concrete example of what i want to do, say i have the word 'researchgate' which i want to be treated as a noun when i parse 
sentences.  Currently, 'researchgate' is getting treated as different parts of speech, depending on its 
position.. but i want it identified as an 'NN' (noun).</p>

<p>Examples... </p>

<p>instead of this:</p>

<pre><code>      (NP
        (NP (JJ recent) (NN activity))
        (PP (IN in)
          (NP (PRP$ your) (JJ researchgate) (NNS topics)))))
</code></pre>

<p>i want this: </p>

<pre><code>      (NP
        (NP (JJ recent) (NN activity))
        (PP (IN in)
          (NP (PRP$ your) (NN researchgate) (NNS topics)))))
</code></pre>

<p>and instead of this:</p>

<pre><code>    (ROOT
      (FRAG
        (NP (NN subscription))
        (S
          (VP (TO to)
            (VP (VB researchgate))))))
</code></pre>

<p>i want this: </p>

<pre><code>    (ROOT
      (NP
        (NP (NN subscription))
        (PP (TO to)
          (NP (NN researchgate)))))
</code></pre>

<p>I am currently using this model: models/edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz</p>

<p>I tried doing this > </p>

<pre><code>    java -cp  stanford-parser.jar        
            edu.stanford.nlp.parser.lexparser.LexicalizedParser   -train  /tmp/train.txt
</code></pre>

<p>with the contensts of /tmp/train.txt as follows > </p>

<pre><code>              (NP
                (NP (JJ recent) (NN activity))
                (PP (IN in)
                  (NP (PRP$ your) (JJ researchgate) (NNS topics)))))
</code></pre>

<p>I got a bunch of promising output, but then got this error > </p>

<pre><code>    Error. Can't parse test sentence: [This, is, just, a, test, .]
</code></pre>

<p>So clearly i need to supply more examples than just the one i have in /tmp/train.txt.</p>

<p>Looking at the documentation there seems to be one promising method on 
LexicalizedParser  that I am considering trying... > </p>

<pre><code>    public static LexicalizedParser getParserFromTreebank(Treebank trainTreebank,
                                                          Treebank secondaryTrainTreebank,
                                                          double weight,
                                                          GrammarCompactor compactor,
                                                          Options op,
                                                          Treebank tuneTreebank,
                                                          List&lt;List&lt;TaggedWord&gt;&gt; extraTaggedWords)
</code></pre>

<p>i am hesitant to jump in and try this because it seems tricky to get the Options right.
The doco says:<br>
        options to the parser which MUST be the SAME at both training and testing (parsing) time in 
        order for the parser to work properly</p>

<p>so i might need guidance on how to extract the options used for 
edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz   perhaps it is </p>

<pre><code>        edu.stanford.nlp.parser.lexparser.EnglishTreebankParserParams  ?
</code></pre>

<p>Also, maybe i want to add researchgate in as one of my extraTaggedWords  ?</p>

<p>I have the feeling i am on the right track but was hoping to get some advice before descending 
into a rat hole.</p>

<p>Thanks in advance !</p>

<pre><code>chris
</code></pre>
"
"14428756","How does EitherT work?","2013-01-20 20:10:06","8","2737","0","2","","14429212","<p>I spend half of my day trying to figure out how to use EitherT as a way to deal with errors in my code.</p>

<p>I have defined a transformer stack like this.</p>

<pre><code>-- Stuff Monad

data StuffConfig = StuffConfig {
  appId     :: T.Text,
  appSecret :: T.Text
}

data StuffState = StuffState {
  stateToken :: Maybe Token,
  stateTime  :: POSIXTime
}

newtype Stuff a = Stuff {
  runStuff :: (ReaderT StuffConfig (StateT StuffState (EitherT T.Text IO))) a
} deriving (Monad, Functor, Applicative, 
            MonadIO, 
            MonadReader StuffConfig,
            MonadState StuffState
            )



askStuff :: StuffConfig -&gt; Stuff a -&gt; IO (Either T.Text a)
askStuff config a = do
  t &lt;- getPOSIXTime 
  runEitherT (evalStateT (runReaderT (runStuff a) config) (StuffState Nothing t))
</code></pre>

<p>This works quite well as long as i only use the <code>ReaderT</code> and <code>StateT</code> functions. I am under the impression that now i should be able to write something like this:</p>

<pre><code>faultyFunction :: String -&gt; Stuff String
faultyFunction s = do
  when s == ""left"" $ left ""breaking out""
  ""right""
</code></pre>

<p>More important is capturing <code>Either</code> return values which should be possible with <code>hoistEither</code> from the <code>errors</code> package:</p>

<pre><code>faultyLookup :: Map -&gt; String -&gt; Stuff String
faultyLookup m k = do
  hoistEither $ lookup k m
</code></pre>

<p>I read the <em>real world haskell</em> chapter on monad transformers and fiddled around with <code>lift</code>. But I can't get anything to typecheck. </p>
"