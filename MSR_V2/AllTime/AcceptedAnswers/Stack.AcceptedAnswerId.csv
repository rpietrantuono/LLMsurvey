Post ID,CreationDate,CommentCount,AcceptedAnswerId,ParentId,Body,Tags
"78880295","2024-08-16 17:51:12","2","","78877667","<p>The problem is the indexing you're doing at this line:</p>
<p><code>logits[sorted_indices[~sorted_indices_to_keep]] = float('-inf')</code></p>
<p>For reasons I'll explain, this is causing an index out of bounds error. Out of bounds indexing is a common cause of <code>CUDA error: device-side assert triggered</code> errors.</p>
<p>Consider the following:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn as nn

torch.manual_seed(42)

top_p = 0.2

logits = torch.randn(8, 128) # random logits

# sort logits 
sorted_logits, sorted_indices = torch.sort(logits, descending=True)

# calculate cumulative probs
cumulative_probs = torch.cumsum(torch.nn.functional.softmax(sorted_logits, dim=-1), dim=-1)

# apply top p threshold to cumulative probs
sorted_indices_to_keep = cumulative_probs &lt;= top_p

# ensure at least one index is kept
sorted_indices_to_keep[..., 0] = True

# this is the problem: logits[sorted_indices[~sorted_indices_to_keep]] = float('-inf')
print(logits.shape, sorted_indices[~sorted_indices_to_keep].shape)
&gt; torch.Size([8, 128]) torch.Size([989])
</code></pre>
<p>When you index <code>sorted_indices[~sorted_indices_to_keep]</code>, both inputs are of shape <code>(8, 128)</code>, but the output is of shape <code>(989,)</code> (or similar depending on the random seed for the dummy logits).</p>
<p>This happens because the <code>sorted_indices_to_keep</code> has an irregular number of <code>True</code> values in each row. This means the indexing operation can't resolve the output into a clean 2D tensor where every row is the same size. Pytorch handles this situation by returning an unrolled vector of every <code>True</code> value from the indexing tensor.</p>
<p>This means when you try to compute <code>logits[sorted_indices[~sorted_indices_to_keep]]</code>, you are using a long 1D tensor to index into a small 2D tensor. If you run this on CPU, you get an error like <code>IndexError: index 20 is out of bounds for dimension 0 with size 8</code>. When you run on GPU, you get the Cuda assert error.</p>
<p>To fix this, use the <code>scatter</code> operation. Use something like this:</p>
<pre class=""lang-py prettyprint-override""><code>def top_p_filtering(logits, top_p, shift_indices=True, debug=False):
    &quot;&quot;&quot;Filter the logits using top-p (nucleus) sampling.&quot;&quot;&quot;
    # Sort logits in descending order and get the sorted indices
    sorted_logits, sorted_indices = torch.sort(logits, descending=True)

    # Compute the cumulative probabilities of the sorted logits
    cumulative_probs = torch.cumsum(torch.nn.functional.softmax(sorted_logits, dim=-1), dim=-1)

    # Create a mask for the tokens to keep
    sorted_indices_to_keep = cumulative_probs &lt;= top_p
    
    # Optional: shift indices to the right. This results in keeping the first 
    # token above the top_p threshold. Skip this line to ensure that all 
    # token probs are strictly below the top_p threshold
    if shift_indices:
        sorted_indices_to_keep[..., 1:] = sorted_indices_to_keep[..., :-1].clone()

    # Ensure that at least one token is kept (the first token, which has the highest logit)
    sorted_indices_to_keep[..., 0] = True
    
    # Use scatter to create top_p mask
    mask = sorted_indices_to_keep.scatter(dim=1, index=sorted_indices, src=sorted_indices_to_keep)
    
    # Optional debug check to make sure top_p is being honored
    # Note we need to compute probs before masking because applying softmax 
    # after masking will result in a distribution that sums to 1
    if debug:
        probs = torch.nn.functional.softmax(logits, dim=-1)
        probs[~mask] = 0
        print(probs.sum(-1))
    
    # Use mask to set logit vals to -inf
    logits[~mask] = float('-inf')

    return logits
</code></pre>
",""
"78842471","2024-08-07 07:54:43","0","","78842047","<p>First step is convert huggingface model to gguf (16b float or 32b float is recommended) using <code>convert_hf_to_gguf.py</code> from llama.cpp repository.</p>
<p>Second step is use compiled c++ code from <code>/examples/quantize/</code> subdirectory of llama.cpp (<a href=""https://github.com/ggerganov/llama.cpp/tree/master/examples/quantize"" rel=""nofollow noreferrer"">https://github.com/ggerganov/llama.cpp/tree/master/examples/quantize</a>)</p>
<p>Process is pretty straightforward and well-documented.</p>
",""
"78752532","2024-07-16 02:57:54","0","","78712265","<p>Retrieving the embedding data from batch file is a bit trick, this Tutorial breaks it down set by set <a href=""https://medium.com/@mikehpg/tutorial-batch-embedding-with-openai-api-95da95c9778a"" rel=""nofollow noreferrer"">link</a></p>
<p>after getting the output_file_id, you need to:</p>
<pre><code>output_file =client.files.content(output_files_id).text

embedding_results = []
for line in output_file.split('\n')[:-1]:
            data =json.loads(line)
            custom_id = data.get('custom_id')
            embedding = data['response']['body']['data'][0]['embedding']
            embedding_results.append([custom_id, embedding])


embedding_results = pd.DataFrame(embedding_results, columns=['custom_id', 'embedding'])
</code></pre>
<p>In my case, this retrieves the embedding data from the batch job file</p>
",""
"78689270","2024-06-30 17:07:25","0","","78689230","<p>Trying to do NLP outside of Python is a huge pain in my experience, but there are some libraries for it, e.g. <a href=""https://github.com/curiosity-ai/catalyst"" rel=""nofollow noreferrer"">https://github.com/curiosity-ai/catalyst</a>, which seems to support lemmatization.</p>
<p>Since stemming is usually just an implementation of some basic rule-based algorithm, you can also either adapt some code from another programming language or copy an existing direct implementation like this one: <a href=""https://github.com/nemec/porter2-stemmer"" rel=""nofollow noreferrer"">https://github.com/nemec/porter2-stemmer</a></p>
",""
"78623063","2024-06-14 12:49:23","1","","78621519","<p>I would use <a href=""https://ply.readthedocs.io/en/latest/ply.html#parsing-basics"" rel=""nofollow noreferrer"">PLY</a> (Python Lex-Yacc) or <a href=""https://sly.readthedocs.io/en/latest/"" rel=""nofollow noreferrer"">SLY</a> to parse it.</p>
<p>(<code>PLY</code> and <code>SLY</code> have the same author but <code>PLY</code> uses functions and <code>SLY</code> uses classes)</p>
<p>I took example <code>calc.py</code> from <code>SLY</code> and created code which converts query like</p>
<pre><code>^(abc &amp; def) | xyz
</code></pre>
<p>to nested list</p>
<pre><code>['OR', ['NOT', ['AND', 'abc', 'def']], 'xyz']
</code></pre>
<p>which should be easy to use to generate SQL query</p>
<p>Other example(s):<br />
<code>(A&amp;B)|(^C&amp;D)</code> ==&gt; <code>['OR', ['AND', 'A', 'B'], ['AND', ['NOT', 'C'], 'D']]</code></p>
<hr />
<p>I changed <code>TEXT</code> so it can catch <code>A B C</code> as one string without using <code>&quot; &quot;</code>.<br />
And if you use <code>&quot;A B C&quot;</code> then it returns it as one string without <code>&quot; &quot;</code>.<br />
But it allows to use <code>&quot;</code> inside text (if there is no <code>&quot;</code> at the beginning and at the end).</p>
<p><code>A&quot;B&quot;C &amp; &quot;F G H&quot; &amp; I J K</code> ==&gt; <code>['AND', ['AND', 'A&quot;B&quot;C', 'F G H'], 'I J K']</code>
`</p>
<p>But it doesn't allow to use chars <code>&amp;|^!~()</code> inside text. It would need some changes in <code>TEXT</code> and in functions.</p>
<p><code>&quot;A&amp;B&quot;</code> ==&gt; <code>['AND', 'A', 'B']</code></p>
<hr />
<p>I added</p>
<ul>
<li><code>!</code> as <code>NOT</code> because Python uses <code>!=</code> as <code>not equal</code> so I was writing <code>!</code> automatically in query</li>
<li><code>~</code> as <code>NOT</code> because I often use it in <code>pandas</code> for negations so sometimes I was writing <code>~</code> automatically in query</li>
</ul>
<hr />
<pre><code>from sly import Lexer, Parser
import readline  # it allows to use keys `arrow up` `arrow down` to see previous queries in current session

class QueryLexer(Lexer):
    tokens = { TEXT }
    ignore = ' \t'
    literals = { '&amp;', '|', '^', '!', '~', '(', ')'}

    # Tokens
    #TEXT = r'[a-zA-Z_][a-zA-Z0-9_]*'
    TEXT = r'[^&amp;|^!~()]{1,}'  # at least one char which is not in `literals`

    @_(r'\n+')
    def newline(self, t):
        self.lineno += t.value.count('\n')

    def error(self, t):
        print(&quot;Illegal character '%s'&quot; % t.value[0])
        self.index += 1

class QueryParser(Parser):
    tokens = QueryLexer.tokens

    precedence = (
        ('left', '&amp;', '|', '!', '~'),
        ('right', '^'),
    )

    @_('expr')
    def statement(self, p):
        return p.expr

    @_('expr &quot;&amp;&quot; expr')
    def expr(self, p):
        return ['AND', p.expr0, p.expr1]

    @_('expr &quot;|&quot; expr')
    def expr(self, p):
        return ['OR', p.expr0, p.expr1]

    @_('&quot;^&quot; expr', '&quot;!&quot; expr', '&quot;~&quot; expr')
    def expr(self, p):
        return ['NOT', p.expr]

    @_('&quot;(&quot; expr &quot;)&quot;')
    def expr(self, p):
        return p.expr

    @_('TEXT')
    def expr(self, p):
        return p.TEXT.strip(' ').strip('&quot;')

if __name__ == '__main__':
    lexer = QueryLexer()
    parser = QueryParser()
    while True:
        try:
            text = input('query &gt; ')
        except EOFError:
            break
        except KeyboardInterrupt:
            break
        if text:
            result = parser.parse(lexer.tokenize(text))
            if isinstance(result, str):
               print(f'text: &gt;{result}&lt;')  # I uses `&gt; &lt;` to see if there are spaces
            else:
               print(f'list: {result}')
               #import json
               #print(json.dumps(result, indent=1))
</code></pre>
<hr />
<p>Here version with more modifications</p>
<p>Original version converts <code>A &amp; B &amp; C &amp; D</code> to <code>['AND', ['AND', ['AND', 'A', 'B'], 'C'], 'D']</code>, and new version can use function <code>flatten()</code> to create <code>['AND', 'A', 'B', 'C', 'D']</code></p>
<p>If you send <code>.flat</code> or <code>.notflat</code> as query then it switch variable <code>FLATTEN</code> which decide if it should use <code>flatten()</code>.</p>
<p>I also added function which tries to convert it to SQL query with  <code>[VAR] LIKE &quot;%text%&quot;</code><br />
and for list <code>['AND', 'A', 'B', 'C', 'D']</code> it gives string</p>
<pre><code>([VAR] LIKE &quot;%A%&quot; AND [VAR] LIKE &quot;%B%&quot; AND [VAR] LIKE &quot;%C%&quot; AND [VAR] LIKE &quot;%D%&quot;) 
</code></pre>
<p>So you have to only replace <code>[VAR]</code> with your variable.</p>
<p>But I don't know if I should add <code>%</code> automatically or user should ask <code>A% &amp; %B</code> because it allows to check if text starts with <code>A</code> and ends with <code>B</code></p>
<p>Eventually user could use <code>A* &amp; *B &amp; C</code> and code should convert it to <code>A% AND %B AND %C%</code> (add <code>%</code> automatically if there is no <code>*</code> at both sides.</p>
<p>I added also function which saves history in file and read it at next start.</p>
<p>I added also json to display data as</p>
<pre class=""lang-none prettyprint-override""><code>[
 &quot;AND&quot;,
 &quot;A&quot;,
 [
  &quot;OR&quot;,
  &quot;B&quot;,
  &quot;C&quot;,
  &quot;D&quot;
 ],
 [
  &quot;NOT&quot;,
  &quot;X&quot;
 ]
]
</code></pre>
<hr />
<p>I added function <code>.history</code> (and shortcut <code>.h</code>) to see history.</p>
<p>I also added function to select field like <code>day:*h*day &amp; title:Hello</code><br />
which gives <code>(day LIKE &quot;%h%day&quot; AND title LIKE &quot;%Hello%&quot;)</code></p>
<hr />
<pre><code>from sly import Lexer, Parser
import readline
import atexit
import os
import json

#COLOR = '\x1b[1;31m' # red
COLOR = '\x1b[1;32m' # green
RESET = '\x1b[m'

BASE = os.path.dirname(os.path.abspath(__file__))
HISTORY_PATH = os.path.join(BASE, &quot;.history&quot;)

MAKE_FLAT = True
DEFAULT_VAR = '[VAR]'

class QueryLexer(Lexer):
    tokens = { TEXT }
    ignore = ' \t'
    literals = { '&amp;', '|', '^', '!', '~', '(', ')'}

    # Tokens
    #WORD   = r'[^&amp;|^!~()&quot;]{1,}'      # at least one char which is not in `literals`
    #PHRASE = r'&quot;[^&quot;]*&quot;'          
    TEXT = r'&quot;[^&quot;]*&quot;|[^&amp;|^!~()]{1,}'  # at least one char which is not in `literals`

    @_(r'\n+')
    def newline(self, t):
        self.lineno += t.value.count('\n')

    def error(self, t):
        print(&quot;Illegal character '%s'&quot; % t.value[0])
        self.index += 1

class QueryParser(Parser):
    tokens = QueryLexer.tokens

    precedence = (
        ('left', '&amp;', '|', '!', '~'),
        ('right', '^'),
    )

    @_('expr')
    def statement(self, p):
        return p.expr

    @_('expr &quot;&amp;&quot; expr')
    def expr(self, p):
        result = ['AND', p.expr0, p.expr1]
        if MAKE_FLAT:
            result = flatten(result)
        return result

    @_('expr &quot;|&quot; expr')
    def expr(self, p):
        result = ['OR', p.expr0, p.expr1]
        if MAKE_FLAT:
            result = flatten(result)
        return result

    @_('&quot;^&quot; expr', '&quot;!&quot; expr', '&quot;~&quot; expr')
    def expr(self, p):
        return ['NOT', p.expr]

    @_('&quot;(&quot; expr &quot;)&quot;')
    def expr(self, p):
        return p.expr

    @_('TEXT')
    def expr(self, p):
        return p.TEXT.strip(' ')


def flatten(data):
    key = data[0]
        
    values = []
    
    for item in data[1:]:
        if isinstance(item, list) and item[0] == key:
            values.extend(item[1:])
        else:
            values.append(item)
        
    return [key, *values]

def generate(data):
    if isinstance(data, str):
        text = data
        var  = DEFAULT_VAR

        if not text.startswith('&quot;') and (':' in text):
            var, text = text.split(':', 1)
            print(var, text)
            
        text = text.strip('&quot;')
        
        if '*' in text:
           text = text.replace('*', '%')
        
        if '%' not in text:
           text = f'%{text}%'
           
        return f'{var} LIKE &quot;{text}&quot;'

    key = data[0]
    values = data[1:]
    
    if key in ['NOT']:
        text = data[1]
        var  = DEFAULT_VAR
        
        if not text.startswith('&quot;') and (':' in text):
            var, text = text.split(':', 1)
            print(var, text)

        text = text.strip('&quot;')
        
        if '*' in text:
            text = text.replace('*', '%')
           
        if '%' not in text:           
            text = f'%{text}%'
           
        return f'{var} NOT LIKE &quot;{text}&quot;'
    
    if key in ['AND', 'OR']:
        key = f' {key} '  # add spaces
        
        # convert to `var LIKE value`
        values = [generate(item) for item in values]
        # join using AND, OR
        text = key.join(values)
    else:
        text = str(data)
        
    return f'({text})'  # add ( )
    
if __name__ == '__main__':

    # read history at the beginning    
    try:
        readline.read_history_file(HISTORY_PATH)
        readline.set_history_length(1000)
    except FileNotFoundError:
        pass
    # write history at the end
    atexit.register(readline.write_history_file, HISTORY_PATH)
        
    lexer = QueryLexer()
    parser = QueryParser()

    number = 0    
    while True:
        try:
            number += 1
            text = input(f'[{number}] {COLOR}query &gt;{RESET} ')
        except EOFError:
            break
        except KeyboardInterrupt:
            break
        if text:
            result = parser.parse(lexer.tokenize(text))
            if isinstance(result, str):
                print(f'text: &gt;{result}&lt;')  # I uses `&gt; &lt;` to see if there are spaces
                cmd = result.split(' ')
                if cmd[0] in ('.history', '.h') :
                    for index in range(1, readline.get_current_history_length()+1):
                        print(f'[{index}]', readline.get_history_item(index))
                elif cmd[0] in ('.flat', '.f'):
                    MAKE_FLAT = True
                    print('MAKE_FLAT:', MAKE_FLAT)
                elif cmd[0] == ('.notflat', '.nf'):
                    MAKE_FLAT = False
                    print('MAKE_FLAT:', MAKE_FLAT)
                elif cmd[0] in ('.var', '.v'):
                    if len(cmd) &gt; 1:
                        DEFAULT_VAR = cmd[1]
                    print('DEFAULT_VAR:', DEFAULT_VAR)
                else:
                    print(f'data: {result}')
                    print(json.dumps(result, indent=1))
                    print(generate(result))
            else:
                print(f'data: {result}')
                print(json.dumps(result, indent=1))
                print(generate(result))
</code></pre>
<hr />
<p><strong>.notflat</strong></p>
<p><a href=""https://i.sstatic.net/nuuJC41P.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/nuuJC41P.png"" alt=""enter image description here"" /></a></p>
<p><strong>.flat</strong></p>
<p><a href=""https://i.sstatic.net/OlqQBrJ1.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/OlqQBrJ1.png"" alt=""enter image description here"" /></a></p>
",""
"78584162","2024-06-06 01:51:52","3","","78570279","<p>You havent provided much details about you docker setup. But yes every time you run this docker you will have to download files, until you build your own image which will copy model files into docker, then you can use <code>cache_dir</code> parameter in <a href=""https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoModelForCausalLM.from_pretrained"" rel=""nofollow noreferrer"">from_pretrained</a> to point to location of your model.</p>
<p>I am able to load llama3 8b into Tesla M40 in few seconds.</p>
",""
"78545500","2024-05-28 16:54:26","1","","78530360","<p><a href=""https://github.com/aws/sagemaker-training-toolkit"" rel=""nofollow noreferrer"">SageMaker Training Toolkit</a> has the implementation to call torchrun command within the SageMaker's python sdk classes.</p>
<p>You can refer to the <a href=""https://github.com/aws/sagemaker-training-toolkit/blob/b7c660b294f882601a736d890db2445dd9d3a638/src/sagemaker_training/torch_distributed.py#L82"" rel=""nofollow noreferrer"">&quot;TorchDistributedRunner._create_command()&quot;</a> to see how it constructs the torchrun command and its arguments.</p>
<p>Please also refer to PyTorch document how to use the torchrun command.
<a href=""https://pytorch.org/docs/stable/elastic/run.html"" rel=""nofollow noreferrer"">https://pytorch.org/docs/stable/elastic/run.html</a></p>
",""
"78541534","2024-05-28 00:47:10","2","","78538749","<p>try:</p>
<pre><code>pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz
</code></pre>
<p>or</p>
<pre><code>pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.0/en_core_web_sm-2.3.0.tar.gz
</code></pre>
",""
"78506143","2024-05-20 10:45:22","0","","78423352","<p>Quoting William James Mattingly, Ph.D, who graciously helped:</p>
<blockquote>
<p>This may due to spaCy's change in training for 3.0. Training is done
for projects and via the command line. This is how we used to train
models for 2.0 and while it works, I believe there are certain issues
that arise. This may be one of those issues. The newer approach passes
an argument in the CLI when you train the model.
<a href=""https://spacy.io/usage/training"" rel=""nofollow noreferrer"">https://spacy.io/usage/training</a></p>
<p>In the docs you can specify in the config how to train and on which
device. Training spaCy's Statistical Models - spaCy spacy.io spaCy is
a free open-source library featuring state-of-the-art speed and
accuracy and a powerful Python API.</p>
<p><code>[system] gpu_allocator = &quot;pytorch&quot;</code></p>
<p>this is the important bit üëè üëç üòä</p>
<p>Then when you run train in the CLI, you'd do something like this:</p>
<p><code>python -m spacy train config.cfg --gpu-id 0</code></p>
</blockquote>
<p>Thank you!</p>
",""
"78491545","2024-05-16 17:14:59","0","","78489915","<p>No, you don't necessarily have to tokenize before lemmatizing. You can try the following code:</p>
<pre><code>import stanza
import pandas as pd

nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma')

def lemmatize_text(text):
    doc = nlp(text)
    lemmas = [word.lemma for sent in doc.sentences for word in sent.words]
    return ' '.join(lemmas)

df['lemmatized_story'] = df['story'].apply(lemmatize_text)
</code></pre>
",""
"78461789","2024-05-10 17:42:18","1","","78461078","<p>Ended up changing my function to convert freq_list to a dictionary and list comprehensions instead of for loops and this code instantly returned a data-frame:</p>
<pre><code>def quicker_func(pmi_tups, freq_list):
    import pandas as pd
    freq_dict = dict(freq_list)  # Create a dictionary for faster lookups 

    pmi_list = [pmi for phrase, pmi in pmi_tups if phrase in freq_dict]
    count_list = [freq_dict[phrase] for phrase, pmi in pmi_tups if phrase in freq_dict]
    phrase_list = [phrase for phrase, pmi in pmi_tups if phrase in freq_dict]

    df = pd.DataFrame({'Phrase': phrase_list, 'PMI': pmi_list, 'Count': count_list})
    return df
</code></pre>
",""
"78444037","2024-05-07 16:56:33","0","","78443980","<p>The first argument to <a href=""https://cran.r-project.org/web/packages/fastText/fastText.pdf"" rel=""nofollow noreferrer""><code>fastText::language_identification()</code></a> is defined as:</p>
<blockquote>
<p>either a valid character string to a valid path <em>where each line represents a different text extract or a vector of text extracts</em> (emphasis mine)</p>
</blockquote>
<p>You have line breaks in your input data:</p>
<pre class=""lang-r prettyprint-override""><code>DF$speechtext[4]
[1] &quot;Text in a\ndifferent language&quot;
</code></pre>
<p>As one prediction is generated per line, you'll get two predictions from this element. You have two options:</p>
<ol>
<li>Remove new lines in your input data. This makes sense in this case.</li>
<li>Keep new lines and map document IDs to each line. This makes sense if new lines might actually be in different languages.</li>
</ol>
<h3>Remove new lines</h3>
<p>If you replace new lines with spaces you will get the same number of predictions returned as input rows.</p>
<p>In the regex below, I have used the <a href=""https://www.pcre.org/"" rel=""nofollow noreferrer"">PCRE</a> <code>\v</code> which matches newlines and any character considered vertical whitespace. This now produces five rows, one relating to each input row.</p>
<pre class=""lang-r prettyprint-override""><code>language_identification(gsub(&quot;\\v&quot;, &quot; &quot;, DF$speechtext, perl = TRUE), file_ftz)
#    iso_lang_1   prob_1
#        &lt;char&gt;    &lt;num&gt;
# 1:         en 0.220767
# 2:         en 0.388695
# 3:         en 0.613707
# 4:         en 0.757671
# 5:         es 0.721487
</code></pre>
<p><code>\v</code> includes several vertical space characters (such as form feed and line separator), so should cover all possible types of new line. For full details see the table <a href=""https://perldoc.perl.org/perlrecharclass#Whitespace"" rel=""nofollow noreferrer"">here</a>.</p>
<h3>Keep new lines and map document ID to each line</h3>
<p>Alternatively, if different lines of each input document might be in different languages, you may not want to remove new lines. In this case, you can predict each line separately and then map the document IDs to each line:</p>
<pre class=""lang-r prettyprint-override""><code># As before
lang1 &lt;- language_identification(DF$speechtext, file_ftz)

# Add document IDs
lang1$doc_id &lt;- rep(
    DF$doc_id,
    lengths(strsplit(DF$speechtext, &quot;\\v&quot;, perl = TRUE))
)

lang1
#    iso_lang_1   prob_1 doc_id
#        &lt;char&gt;    &lt;num&gt;  &lt;int&gt;
# 1:         en 0.220767      1
# 2:         en 0.388695      2
# 3:         en 0.613707      3
# 4:         en 0.932691      4
# 5:         en 0.571937      4
# 6:         es 0.721487      5
</code></pre>
",""
"78407238","2024-04-30 08:30:55","1","","78393709","<h3>Contextual embedding models are computationally expensive</h3>
<p>The default model used by the R <code>text</code> package is <a href=""https://huggingface.co/blog/bert-101"" rel=""nofollow noreferrer""><code>bert-base-uncased</code></a>, which has 110m parameters, including a 12 layer feed-forward neural network.</p>
<p>To compare similarity of sentences, each sentence is split into tokens (which are words or parts of words) and each token is represented as a 768-dimensional vector. Then the token vectors from each sentence are aggregated to create a single, 768-dimensional vector to represent that sentence in vector space. The idea is that with decent token (or word) embeddings and a sensible way of aggregating them this representation captures meaning, so similar sentences appear closer together in vector space. You can then calculate the distance between sentence vectors as a measure of their semantic similarity.</p>
<p>The expensive step is creating the token vectors. Unlike older models such as Word2Vec, with BERT the vector representation of each token depends on the context. A <a href=""https://ubiai.tools/from-words-to-vectors-a-dive-into-spacy-transformers-for-embeddings/"" rel=""nofollow noreferrer"">canonical example</a> of this is that the representation of the first word in <em>Apple Inc. was founded by Steve Jobs</em> will not be the same as the first word in <em>Apple is my favourite fruit</em>. This is a strength of this family of models. These sentences should not appear close together in vector space. But calculating the representation of each token based on the others around it, and working out which tokens affect the meaning of other tokens, is computationally expensive.</p>
<h3>Use a more lightweight context-dependent model</h3>
<p>A good way to speed up things a lot without sacrificing too much accuracy is by using a more lightweight model, e.g. <a href=""https://huggingface.co/docs/transformers/en/model_doc/distilbert"" rel=""nofollow noreferrer""><code>distilbert</code></a> which has 66m parameters and 6 rather than 12 feed-forward neural network layers.</p>
<p>This is still a transformer model. You still get context-specific embeddings with an attention mechanism to appropriately weight each token based on the surrounding ones. With distilled models trained on larger models like BERT, the results tend to be fairly close to the model from which they're derived, and they're much faster:</p>
<blockquote>
<p>DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than google-bert/bert-base-uncased, runs 60% faster while preserving over 95% of BERT‚Äôs performances as measured on the GLUE language understanding benchmark.</p>
</blockquote>
<h3>Benchmarking the default model</h3>
<p>We need a little more text to measure the performance. I'll use the <a href=""https://paperswithcode.com/dataset/biosses"" rel=""nofollow noreferrer"">Biomedical Semantic Similarity Estimation System</a> (BIOSSES) dataset of 100 sentence pairs.</p>
<pre class=""lang-r prettyprint-override""><code># Read in the data
biosses  &lt;- jsonlite::read_json(&quot;https://datasets-server.huggingface.co/rows?dataset=biosses&amp;config=default&amp;split=train&amp;offset=0&amp;length=100&quot;)
df2  &lt;- data.frame(
    sentence1 = sapply(biosses$rows, \(row) row$row$sentence1),
    sentence2 = sapply(biosses$rows, \(row) row$row$sentence2)
)

# Define function to calculate similarity
get_similarity &lt;- function(model, dat = df2, col1 = &quot;sentence1&quot;, col2 = &quot;sentence2&quot;) {
    embeds_a &lt;- text::textEmbed(dat[[col1]], model = model)
    embeds_b &lt;- text::textEmbed(dat[[col2]], model = model)

    text::textSimilarity(
        embeds_a$texts$texts,
        embeds_b$texts$texts
    )
}
</code></pre>
<p>No need to microbenchmark here as it takes so long (about 16 minutes):</p>
<pre class=""lang-r prettyprint-override""><code>system.time(
    base_result &lt;- get_similarity(model = &quot;bert-base-uncased&quot;)
) # 956.136 seconds
</code></pre>
<h3>Speed of smaller BERT models</h3>
<p>Let's try three other BERT derivatives:</p>
<ol>
<li><a href=""https://huggingface.co/distilbert/distilbert-base-uncased"" rel=""nofollow noreferrer"">DistilBERT</a>.</li>
<li><a href=""https://huggingface.co/muhtasham/bert-tiny-finetuned-finer-139-full-intel-cpu"" rel=""nofollow noreferrer"">Bert tiny intel CPU optimized</a>.</li>
<li><a href=""https://huggingface.co/distilbert/distilbert-base-uncased-distilled-squad"" rel=""nofollow noreferrer"">DistilBERT distilled</a>.</li>
</ol>
<pre class=""lang-r prettyprint-override""><code>models &lt;- c(
    &quot;distilbert&quot; = &quot;distilbert-base-uncased&quot;,
    &quot;tiny_distilbert&quot; = &quot;muhtasham/bert-tiny-finetuned-finer-139-full-intel-cpu&quot;,
    &quot;distilled_distilbert&quot; = &quot;distilbert/distilbert-base-uncased-distilled-squad&quot;
)

l &lt;- lapply(models, \(model)
    list(
        time = system.time(x &lt;- get_similarity(model))[&quot;elapsed&quot;],
        result = x,
        diff_from_base = abs(base_result - x)
    )
)
</code></pre>
<p>To compare the results:</p>
<pre class=""lang-r prettyprint-override""><code>sapply(l, \(x) x$time)
# distilbert.elapsed      tiny_distilbert.elapsed distilled_distilbert.elapsed 
#             622.071                       61.956                      569.000 
</code></pre>
<p>So the tiny model is by far the fastest at just over a minute to run, with the other two much slower at around 9-10 mins. All are substantially faster than the base model.</p>
<h3>Accuracy of smaller BERT models</h3>
<p>Of course, speed is not everything. Let's compare the distribution of similarity scores to the <code>bert-base-uncased</code> scores:</p>
<pre class=""lang-r prettyprint-override""><code>res  &lt;- cbind(sentence = seq(nrow(df2)), data.frame(l)) |&gt;
    tidyr::pivot_longer(
        cols = !sentence,
        names_pattern = &quot;(.+)\\.(.+)&quot;,
        names_to = c(&quot;model&quot;, &quot;var&quot;)
    ) |&gt;
    tidyr::pivot_wider(
        names_from = var
    )

library(ggplot2)
ggplot(res) +
    geom_density(aes(x = diff_from_base, color = model, fill = model), alpha = 0.2) +
    theme(legend.position = &quot;bottom&quot;) +
    labs(x = &quot;Absolute difference&quot;, title = &quot;Distribution of differences from bert-base-uncased similarity score&quot;)
</code></pre>
<p><a href=""https://i.sstatic.net/A2GIOgq8.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/A2GIOgq8.png"" alt=""enter image description here"" /></a></p>
<p>Interestingly, the fastest model also appears to have the closest results to the base model, at least on this task. You may want to do more robust checks with a subset of your actual data to find the optimal trade-off between speed and similarity to BERT (or whatever your gold standard of similarity is).</p>
<h3>Other approaches to speeding up this task</h3>
<h3>Parallel processing</h3>
<p>You suggested using parallel processing. This should not speed things up, as under the hood, <code>text::textEmbed()</code> wraps the Python Hugging Face <code>transformers</code> library, which in turn calls the Python <code>torch</code> library, which already multi-threaded by default. When I run this code, CPU utilisation spikes for all cores. You might want to check the same happens on your machine but I suspect it will.</p>
<h4>GPU</h4>
<p>You suggested using a GPU. If you have one you should use it. This may speed things up significantly, though how much depends on the model. It looks like the way to do this is to call <code>text::textEmbed(..., device = &quot;gpu&quot;)</code>. I would not feel completely confident this will work out of the box though. Getting <code>torch</code> to recognise a GPU generally requires <a href=""https://pytorch.org/get-started/locally/"" rel=""nofollow noreferrer"">specifying</a> the Cuda version during installation. This cannot be done for you from R when you run <code>text::textrpp_initialize()</code>, as it will not know which hardware or drivers you have installed. You might want to enter the Python <code>conda</code> virtual environment created by the R <code>text</code> package and check whether <code>torch.cuda.is_available() == True</code>. If not, see <a href=""https://stackoverflow.com/questions/60987997/why-torch-cuda-is-available-returns-false-even-after-installing-pytorch-with"">this question</a> for how to find out whether your GPU is compatible with Cuda and if so how to find the compatible <code>torch</code> version.</p>
<h3>A note on using R</h3>
<p>There is a cost to using R here. I replicated these results in Python and it was about twice as fast again. Each sentence is represented as a 768 dimensional vector. There's some work going on to take the Python list of <code>Torch.tensor</code>s representing each sentence, coerce to a <code>numpy</code> array, then pass this to R where ultimately it's stored as a row of a data frame with 768 columns. I suspect that as the size of the data increases, the time spent on translating the vectors from Python to R reduces as a proportion of the total time, but it would certainly be somewhat faster still to do all this in Python.</p>
<p>Nevertheless, if <a href=""https://huggingface.co/muhtasham/bert-tiny-finetuned-finer-139-full-intel-cpu"" rel=""nofollow noreferrer""><code>muhtasham/bert-tiny-finetuned-finer-139-full-intel-cpu</code></a> will do the job and there are benefits to keeping everything in R, hopefully a sixteen-fold speed up is enough to be able to do this.</p>
",""
"78350517","2024-04-18 22:10:18","0","","78150042","<p>I had tried a lot of different things as mentioned on various locations on web but none worked for me. The error was due to inefficient GPU memory allocation strategy for LLM (device_map=&quot;auto&quot; was not working well for me) and some variables that were getting stored on GPU. I am mentioning variables because out-of-memory was appearing within first four inferences (which meant that empty memory available on GPU was very less to start with and the reason why I mentioned that the strategy of GPU memory allocation was not working for me).</p>
<p><em>Before I elaborate on my answer, I will list the various things I found at various locations and what finally worked for me (and I believe, most of the users who had faced this issue will most likely benefit from what I tried rather than the things mentioned below, even during training)</em></p>
<ul>
<li><p>Update PYTORCH_CUDA_ALLOC_CONF max_split_size_mb. But this may be the least helpful option.
os.environ[&quot;PYTORCH_CUDA_ALLOC_CONF&quot;] = &quot;max_split_size_mb:enter-size-here&quot;</p>
</li>
<li><p>A user at a different forum had mentioned that one needs to install following packages: transformers==4.28.1, sentencepiece==0.1.97, accelerate==0.18.0, bitsandbytes==0.37.2 and torch 1.13.1 . But I believe out of memory issues mostly need to be handled by memory management. Package issues might be temporary with new releases but are resolved with certaininity.</p>
</li>
<li><p>Related to Training ONLY - While training for vision models, the images might not fit in GPU solely and so you should adjust it‚Äôs size as well as release them from GPU memory.</p>
</li>
<li><p>Related to Training ONLY - Reduce training batch sizes to as small as 1</p>
</li>
<li><p>Garbage collection gc.collect()</p>
</li>
<li><p>Empty cache torch.cuda.empty_cache()</p>
</li>
<li><p>Increase system RAM/larger compute instance</p>
</li>
</ul>
<p><strong>What really helped me</strong> was distributing the LLM across GPU‚Äôs by defining max_memory of GPU that can be used for storing model. This meant that my GPU was not fully booked by the LLM. It is a three step process:</p>
<ul>
<li><p>Load model with no weights/empty model on GPU.  While inference set
no_grad to avoid any calculations weight updates even though no
weights will be updated. Also, Set device map to fix the max memory
loaded model weights can take.</p>
</li>
<li><p>Load model weights on CPU</p>
</li>
<li><p>The weights for each layer are loaded to GPU, execution/calculation
is done and then, weights are removed from GPU.</p>
</li>
</ul>
<pre>
    with torch.no_grad():
    
            with init_empty_weights():
                old_prediction_model = AutoModelForCausalLM.from_pretrained(
                    model_dir,
                    torch_dtype=torch.bfloat16,
                    quantization_config=quantization_config
                    )
            model = load_checkpoint_and_dispatch(
            old_prediction_model, offload_folder=""/offload_folder_name_or_location"",
                checkpoint=model_dir, device_map=infer_auto_device_map(old_prediction_model, max_memory={0: ""10GiB""}),
                dtype=torch.bfloat16
        )
</pre>
<p><strong>NOTE</strong>: Along with all this, another big cause of out of memory errors is leaving your variables on GPU i.e. since, execution is happening on GPU and during that course you try to create a list of model inferences or evaluations, the GPU memory will continue to fill up as your new inferences are made. To avoid it, with each inference take your variables off the GPU and to cpu memory</p>
",""
"78341625","2024-04-17 14:10:09","3","","78340299","<p>Model quantization and LoRA are different concepts.</p>
<p>As you know, LoRA is a kind of Parameter-Efficient Fine-Tuning technique which reduces <strong>the number of trainable parameters</strong>.</p>
<p>Whereas, model quantization reduces the size of a model by converting model weights from higher-<strong>precision</strong> representation(like FP32) to lower-<strong>precision</strong> representations(like bfloat16 or INT8).</p>
",""
"78272137","2024-04-04 06:40:50","0","","78267762","<p>GPTQ is a Post-Training Quantization method. This means a GPTQ model was created in full precision and then compressed. Not all values will be in 4 bits unless every weight and activation layer has been quantized.</p>
<p>The <a href=""https://huggingface.co/blog/gptq-integration"" rel=""nofollow noreferrer"">GPTQ method</a> does not do this:</p>
<blockquote>
<p>Specifically, GPTQ adopts a mixed int4/fp16 quantization scheme where weights are quantized as int4 while activations remain in float16.</p>
</blockquote>
<p>As these values need to be multiplied together, this means that,</p>
<blockquote>
<p>during inference, weights are dequantized on the fly and the actual compute is performed in float16.</p>
</blockquote>
<p>In a Hugging Face quantization blog post from Aug 2023, they talk about the possibility of quantizing activations as well in the <a href=""https://huggingface.co/blog/gptq-integration#room-for-improvement"" rel=""nofollow noreferrer"">Room for Improvement</a> section. However, at that time there were no open source implementations.</p>
<p>Since then, they have released <a href=""https://github.com/huggingface/quanto"" rel=""nofollow noreferrer"">Quanto</a>. This does support quantizing activations. It looks promising but it is not yet quicker than other quantization methods. It is in beta and the docs say to expect breaking changes in the API and serialization. There are some accuracy and perplexity <a href=""https://github.com/huggingface/quanto/tree/main/bench/generation"" rel=""nofollow noreferrer"">benchmarks</a> which look pretty good with most models. Surprisingly, at the moment it is <a href=""https://github.com/huggingface/quanto/blob/b9ee78335a6f0f90363da5909b5b749a1beaa4ce/README.md?plain=1#L87"" rel=""nofollow noreferrer"">slower</a> than 16-bit models due to lack of optimized kernels, but that seems to be something they're working on.</p>
<p>So this does not just apply to GPTQ. You will find yourself using float16 with any of the popular quantization methods at the moment. For example, <a href=""https://huggingface.co/docs/transformers/main/en/quantization?bnb=4-bit#awq"" rel=""nofollow noreferrer"">Activation-aware Weight Quantization (AWQ)</a> also preserves in full precision a small percentage of the weights that are important for performance. <a href=""https://huggingface.co/blog/overview-quantization-transformers"" rel=""nofollow noreferrer"">This</a> is a useful blog post comparing GPTQ with other quantization methods.</p>
",""
"78216510","2024-03-24 23:57:27","0","","78215873","<p>Here is an example of what parts of speech the lemmatizer is using for the words in your string:</p>
<pre><code>import nltk
nltk.download('wordnet')
from nltk.corpus import wordnet
from nltk.stem.wordnet import WordNetLemmatizer
from nltk import word_tokenize, pos_tag
from collections import defaultdict

tag_map = defaultdict(lambda : wordnet.NOUN)
tag_map['J'] = wordnet.ADJ
tag_map['V'] = wordnet.VERB
tag_map['R'] = wordnet.ADV

sentence = &quot;having playing in today gaming ended with greating victorious&quot;
tokens = word_tokenize(sentence)
wnl = WordNetLemmatizer()
for token, tag in pos_tag(tokens):
    print('found tag', tag[0])
    lemma = wnl.lemmatize(token, tag_map[tag[0]])
    print(token, &quot;lemmatized to&quot;, lemma)
</code></pre>
<p>The output:</p>
<pre><code>found tag V
having lemmatized to have
found tag N
playing lemmatized to playing
found tag I
in lemmatized to in
found tag N
today lemmatized to today
found tag N
gaming lemmatized to gaming
found tag V
ended lemmatized to end
found tag I
with lemmatized to with
found tag V
greating lemmatized to greating
found tag J
victorious lemmatized to victorious
</code></pre>
<p>Lemmatization distills words to their foundational form. It is similar to stemming but brings context to the words, thus linking words with similar meanings to one word. The fancy linguistic word is ‚Äúmorphology‚Äù. So how do the words relate to each other in a given language? If you look at the output above, the ing verbs are being parsed as nouns. ing verbs, while verbs, can also be utilized as nouns: I love swimming. The verb is love and the noun is swimming. And that is how the tags are being interpreted above. And to be honest, your above sentence is not a sentence at all. I would not say one is correct over the other, but consider lemmatization as more powerful when parts of speech are utilized correctly in a sentence that has either an independent clause or dependent along with independent clauses.</p>
",""
"78157893","2024-03-14 03:06:32","1","","78155250","<p>Your code doesn't have any errors.</p>
<p>The reason for the error is that, as of as of version <code>0.0.28</code> of <code>langchain-community</code>, only the tasks</p>
<ul>
<li><code>text2text-generation</code></li>
<li><code>text-generation</code></li>
<li><code>summarization</code></li>
</ul>
<p>are supported with <code>HuggingFacePipeline</code>.</p>
<p>Your task is <code>translation</code>, which is not supported, as of yet.</p>
<p>As to why the error occurs, Langchain passes the argument <code>return_full_text</code> (see <a href=""https://github.com/langchain-ai/langchain/blob/master/libs/community/langchain_community/llms/huggingface_pipeline.py"" rel=""nofollow noreferrer"">this</a>, line 264) to the underlying HuggingFace model. However, <code>MarianMTModel</code> (the model you're using) doesn't take this as a parameter.</p>
<p>You're better off using the base HuggingFace model directly. This is the easiest solution.</p>
<pre class=""lang-py prettyprint-override""><code>translation = _en_to_de_pipeline(&quot;Hello, how are you?&quot;)
print(translation)
</code></pre>
<p>Output</p>
<pre><code>[{'translation_text': &quot;Hallo, wie geht's?&quot;}]
</code></pre>
<p>It returns  without error.</p>
",""
"78104970","2024-03-05 02:03:14","0","","74519464","<p>This happens when function expects tensor, but other type of data is passed instead.
For instance, numpy array, like the one below:</p>
<pre><code>y.shape

</code></pre>
<p>out: (2000,)</p>
<pre><code>y.shape.rank
</code></pre>
<p>out: AttributeError: 'tuple' object has no attribute 'rank'</p>
<p>To correct this, data should be converted to tensor.</p>
<pre><code>tf.constant(y).shape.rank
</code></pre>
<p>out: 1</p>
",""
"78068645","2024-02-27 15:21:03","0","","78060804","<p>After installing shap 41.0, you have to do : !pip3 install mxnet-mkl==1.6.0 numpy==1.23.1 . After that if you encounter : dtype: np.bool you can change: np.bool_ in source code. Of course you can see some warning however graph will be produced !</p>
<p>Edit: if you want to use current version only do this : !pip3 install mxnet-mkl==1.6.0 numpy==1.23.1</p>
",""
"78059111","2024-02-26 07:08:55","0","","77824012","<p>From the pytorch documentation: <a href=""https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html"" rel=""nofollow noreferrer"">https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html</a></p>
<p>This layer uses statistics computed from input data in both training and evaluation modes.</p>
<p><a href=""https://i.sstatic.net/9TgLc.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/9TgLc.png"" alt=""layernorm formu"" /></a></p>
<p>The E[x] and Var[x] are calculated on every input tensor.
Only the Œ≥ and Œ≤ are fixed at training time.</p>
<p>Thus what you are observing is the correct and expected behavior.</p>
",""
"78022307","2024-02-19 16:54:50","1","","78020235","<p>You're misinterpreting the post. It doesn't say that looping inference increases GPU utilization - it posits looping inference as a test to see if the bottleneck is loading data. It's a test, not a solution.</p>
<p>Looping inference loads data once, then runs inference multiple times, allowing you to see the GPU performance without i/o overhead. If GPU performance improves when looping inference, it indicates an i/o bottleneck.</p>
",""
"77728508","2023-12-28 18:19:04","0","","77094149","<p>Using multiple GPUs is specific to machine learning libraries. I stumbled upon the same problem while doing image segmentation in Pytorch. The solution is to use the module <a href=""https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html"" rel=""noreferrer"">torch.nn.DataParallel()</a> with the model. The given code can be changed as follows:</p>
<pre><code>device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
model = torch.nn.DataParallel(model, device_ids = [0,1]).to(device)
</code></pre>
<p>here, the <code>device_ids</code> is the index of GPUs. Suppose if you have 4 GPUs then it would be <code>device_ids = [0,1,2,3]</code> or whatever the index it maybe.</p>
<p>And the result of
<a href=""https://i.sstatic.net/pc8GW.png"" rel=""noreferrer"">using both GPUs</a> is here!.</p>
<p>PS: This is my first post in the prestigious stack overflow, please do share your comments and views.</p>
",""
"77609905","2023-12-06 00:09:19","0","","77609784","<p>I'm not entirely sure yet, but from what I've seen, it appears that Stanza's pipeline generates a nested structure in which each sentence is a list of tokens, and each token is akin to a dictionary containing various attributes such as ID, text, lemma, and so on.</p>
<p>It is easy to extract the lemmas by navigating this nested structure. Here's how I've done it.</p>
<pre><code>stanza.download('es', package='ancora', processors='tokenize,mwt,pos,lemma', verbose=False)
stNLP = stanza.Pipeline(processors='tokenize,mwt,pos,lemma', lang='es', use_gpu=True)
doc = stNLP('me hubiera gustado mas ‚Äúsincronia‚Äù con la primaria')
lemmas = [word.lemma for t in doc.iter_tokens() for word in t.words]
</code></pre>
<p>Note: As of the time of writing, the version of Stanza being used is stanza==1.7.0</p>
",""
"77600312","2023-12-04 14:42:42","0","","77594086","<p>I would recommend looking into <a href=""https://huggingface.co/docs/transformers/main_classes/quantization"" rel=""nofollow noreferrer"">model quantization</a> as this is one of the approaches which specifically addresses this type of problem, of loading a large model for inference.</p>
<p><a href=""https://huggingface.co/TheBloke"" rel=""nofollow noreferrer"">TheBloke</a> has provided a quantized version of this model which is available here: <a href=""https://huggingface.co/TheBloke/neural-chat-7B-v3-1-AWQ"" rel=""nofollow noreferrer"">neural-chat-7B-v3-1-AWQ</a>. To use this, you'll need to use AutoAWQ, and as per Hugging Face <a href=""https://colab.research.google.com/drive/1HzZH89yAXJaZgwJDhQj9LqSBux932BvY#scrollTo=mRwsuEtsZYrs"" rel=""nofollow noreferrer"">in this notebook</a>, for Colab you need to install an earlier version given Colab's CUDA version.</p>
<p>You should also make sure your model is using GPU, not CPU, by adding <code>.cuda()</code> to the input tensor after it is generated:</p>
<pre class=""lang-py prettyprint-override""><code>!pip install -q transformers accelerate
!pip install -q -U https://github.com/casper-hansen/AutoAWQ/releases/download/v0.1.6/autoawq-0.1.6+cu118-cp310-cp310-linux_x86_64.whl

import torch
from awq import AutoAWQForCausalLM
from transformers import AutoTokenizer

model_name = 'TheBloke/neural-chat-7B-v3-1-AWQ'
### Use AutoAWQ and from quantized instead of transformers here
model = AutoAWQForCausalLM.from_quantized(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

def generate_response(system_input, user_input):

    # Format the input using the provided template
    prompt = f&quot;### System:\n{system_input}\n### User:\n{user_input}\n### Assistant:\n&quot;

    ### ADD .cuda()
    inputs = tokenizer.encode(prompt, return_tensors=&quot;pt&quot;, add_special_tokens=False).cuda()

    # Generate a response
    outputs = model.generate(inputs, max_length=1000, num_return_sequences=1)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # Extract only the assistant's response
    return response.split(&quot;### Assistant:\n&quot;)[-1]

</code></pre>
",""
"77592933","2023-12-03 03:16:22","0","","77511368","<p>It does take a long time to generate an output even on powerful GPUs. My use-case was a chatbot, so I figured it would be ideal to stream the output token by token as generated by the model. This reduced the perceived time although the actual output would remain the same.</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForCausalLM, TextIteratorStreamer

    with torch.no_grad():
    # Tokenize input question
    input_ids = tokenizer.encode(question, return_tensors=&quot;pt&quot;, truncation=True).cuda()
    streamer = TextIteratorStreamer(
        tokenizer=tokenizer, timeout=60.0, skip_prompt=True, skip_special_tokens=True
    )
    def generate_and_signal_complete():
        output = model.generate(
            input_ids,
            max_length=1500,
            num_return_sequences=1,
            do_sample=True,
            top_k=50,
            streamer=streamer
        )
t1 = Thread(target=generate_and_signal_complete)
t1.start()
# Decode and extract the response
for new_text in streamer:
    yield new_text
</code></pre>
",""
"77515871","2023-11-20 12:20:16","0","","75326344","<p>It seems like you're encountering a memory issue when combining a large sparse matrix from TF-IDF vectorization with a dense 'duration' feature. Converting a sparse matrix to a dense one with toarray() or todense() dramatically increases memory usage, which is likely causing the crash.</p>
<p>Instead of converting the entire sparse matrix, try combining the sparse TF-IDF features with the dense 'duration' feature while keeping most of the data in sparse format. Use scipy.sparse.hstack for this:</p>
<pre><code>from scipy.sparse import hstack

# Combine the sparse and dense features
X = hstack([X_sparse, X_duration])
</code></pre>
<p>This method maintains the efficiency of sparse data storage. If you're still facing memory issues, consider reducing the number of features in your TF-IDF vectorization [ tfidf_vectorizer = TfidfVectorizer(max_df = 0.8, max_features = 10000) , I think 10000 is a bit too much ] or using incremental learning methods like SGDClassifier with a logistic regression loss. These approaches should help manage the large dataset more effectively.</p>
",""
"77482219","2023-11-14 16:19:55","1","","77482126","<h3>Problem</h3>
<p><strong>The method name you're trying to use doesn't work with the OpenAI Python SDK version <code>1.0.0</code> or newer.</strong></p>
<p>The old SDK (i.e., version <code>0.28</code>) works with the following method name:</p>
<pre><code>client.Model.list
</code></pre>
<p>The new SDK (i.e., version <code>1.0.0</code> or newer) works with the following method name:</p>
<pre><code>client.models.list
</code></pre>
<p><em>Note: Be careful because the API is case-sensitive (i.e., <code>client.Models.list</code> will not work with the new SDK version).</em></p>
<h3>Solution</h3>
<p>Try this:</p>
<pre><code>import os
from openai import OpenAI
client = OpenAI()
OpenAI.api_key = os.getenv('OPENAI_API_KEY')

client.models.list()
</code></pre>
",""
"77452808","2023-11-09 11:31:16","2","","77159136","<p>I think you can ignore this message. I found it being reported on different websites this year, but if I get it correctly, this Github issue on the Huggingface transformers (<a href=""https://github.com/huggingface/transformers/issues/22387"" rel=""noreferrer"">https://github.com/huggingface/transformers/issues/22387</a>) shows that the warning can be safely ignored. In addition, batching or using <code>datasets</code> might not remove the warning or automatically use the resources in the best way. You can do <code>call_count = 0</code> in here (<a href=""https://github.com/huggingface/transformers/blob/main/src/transformers/pipelines/base.py#L1100"" rel=""noreferrer"">https://github.com/huggingface/transformers/blob/main/src/transformers/pipelines/base.py#L1100</a>) to ignore the warning, as explained by <a href=""https://stackoverflow.com/users/22868794/martin-weyssow"">Martin Weyssow</a> above.</p>
<p><strong>How can I modify my code to batch my data and use parallel computing to make better use of my GPU resources:</strong></p>
<p>You can add batching like this:</p>
<pre><code>py_sentimiento = pipeline(&quot;sentiment-analysis&quot;, model=&quot;finiteautomata/beto-sentiment-analysis&quot;, tokenizer=&quot;finiteautomata/beto-sentiment-analysis&quot;, batch_size=8, device=device, truncation=True)
</code></pre>
<p>and most importantly, you can experiment with the batch size that will result to the highest GPU usage possible on your device and particular task.</p>
<p>Huggingface provides here some rules to help users figure out how to batch: <a href=""https://huggingface.co/docs/transformers/main_classes/pipelines#pipeline-batching"" rel=""noreferrer"">https://huggingface.co/docs/transformers/main_classes/pipelines#pipeline-batching</a>. Making the best resource/GPU usage possible might take some experimentation and it depends on the use case you work on every time.</p>
<p><strong>What does this warning mean, and why should I use a dataset for efficiency?</strong></p>
<p>This means the GPU utilization is not optimal, because the data is not grouped together and it is thus not processed efficiently. Using a dataset from the Huggingface library <code>datasets</code> will utilize your resources more efficiently.
However, it is not so easy to tell what exactly is going on, especially considering that we don‚Äôt know exactly how the data looks like, what the device is and how the model deals with the data internally. The warning might go away by using the <code>datasets</code> library, but that does not necessarily mean that the resources are optimally used.</p>
<p><strong>What code or function or library should be used with hugging face transformers?</strong></p>
<p>Here is a code example with <code>pipelines</code> and the <code>datasets</code> library: <a href=""https://huggingface.co/docs/transformers/v4.27.1/pipeline_tutorial#using-pipelines-on-a-dataset"" rel=""noreferrer"">https://huggingface.co/docs/transformers/v4.27.1/pipeline_tutorial#using-pipelines-on-a-dataset</a>. It mentions that using iterables will fill your GPU as fast as possible and batching might also help with computational time improvements.</p>
<p>In your case it seems you are doing a relatively small POC (doing inference for under 10,000 documents with a medium size model), so I don‚Äôt think you need to use pipelines. I assume the sentiment analysis model is a classifier and you want to keep using <code>Pandas</code> as shown in the post, so here is how you can combine both. This is usually fast enough for my experiments and prints no warnings about the resources.</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch as t
import pandas as pd
        
model = AutoModelForSequenceClassification.from_pretrained(&quot;finiteautomata/beto-sentiment-analysis&quot;)
tokenizer = AutoTokenizer.from_pretrained(&quot;finiteautomata/beto-sentiment-analysis&quot;)
            
def classify_dataframe_row(
    example: pd.Series,
):
    output = model(**tokenizer(example[&quot;text&quot;], return_tensors=&quot;pt&quot;))
    prediction = t.argmax(output[0]).detach().numpy()
    return prediction

dataset = pd.read_csv(&quot;file&quot;)
dataset = dataset.assign(
    prediction=dataset.progress_apply(classify_dataframe_row, axis=1)
)
</code></pre>
<p>As soon as your inference starts, either with this snippet or with the <code>datasets</code> library code, you can run <code>nvidia-smi</code> in a terminal and check what the GPU usage is and play around with the parameters to optimize it. Beware that running the code on your local machine with a GPU vs running it on a larger machine, e.g., a Linux server with perhaps a more powerful GPU might lead to different performance and might need different tuning. If you wish to run the code for larger document collections, you can split the data in order to avoid GPU memory errors locally, or in order to speed up the inference with concurrent runs in a server.</p>
",""
"77433933","2023-11-06 19:59:29","9","","77433100","<p>this is happening because in the second code snippet, you loop over the input sequence by adding a new token at each iteration:</p>
<pre><code>i=0: input_ids[:, :i+1] := tensor([[25082]], device='cuda:0')
i=1: input_ids[:, :i+1] := tensor([[25082, 33511]], device='cuda:0')
i=2: input_ids[:, :i+1] := tensor([[25082, 33511,     0]], device='cuda:0')
</code></pre>
<p>Then, the computation of the perplexity in the last iteration of the loop is essentially identical to doing this:</p>
<pre><code>outputs = model(input_ids.to(device), labels=target_ids)
ppl = torch.exp(outputs.loss)
</code></pre>
<p>Here's how you can compute the perplexity and per-token perplexity (see <a href=""https://github.com/huggingface/transformers/blob/v4.35.0/src/transformers/models/gpt2/modeling_gpt2.py#L1103"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/blob/v4.35.0/src/transformers/models/gpt2/modeling_gpt2.py#L1103</a>):</p>
<pre><code>import torch.nn.functional as F
[...]
sent = 'Happy Birthday!'
input_ids = tokenizer(sent, return_tensors='pt')['input_ids'].to(device)
labels = input_ids.clone()

output = model(input_ids, labels=labels)
logits = output.logits
shift_logits = logits[..., :-1, :].contiguous()
shift_labels = labels[..., 1:].contiguous()
loss = F.cross_entropy(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1), reduction='none')
per_token_perplexity = torch.exp(loss)
average_perplexity = torch.exp(torch.mean(loss))
print(per_token_perplexity)
print(average_perplexity)
</code></pre>
<p>The output:</p>
<pre><code>tensor([5.4192e+04, 4.1502e+01], device='cuda:0', grad_fn=&lt;ExpBackward0&gt;)
tensor(1499.6934, device='cuda:0', grad_fn=&lt;ExpBackward0&gt;)
</code></pre>
",""
"77429721","2023-11-06 08:28:09","5","","77427999","<p>You need to provide the config settings in the <code>add_pipe</code> method through a config dict. In your code, the <code>keyword_pos_tagger</code> variable is a stranded component that's not actually added to the <code>nlp</code> pipeline. It shares the same vocab and you could use it for unit testing, but otherwise you can't add it to a pipeline when it's created like this.</p>
<pre class=""lang-py prettyprint-override""><code>nlp.add_pipe(&quot;keyword_pos_tagger&quot;, config={&quot;keywords&quot;: keywords, &quot;pos_tag&quot;: pos_tag})
</code></pre>
<hr />
<p>Edited to expand answer:</p>
<pre class=""lang-py prettyprint-override""><code># tested with spacy==3.7.2
import spacy
from spacy.language import Language
from spacy.tokens import Token

# Creating the Custom Tagger
Token.set_extension(&quot;pos_tag&quot;, default=None, force=True)


@Language.factory(&quot;keyword_pos_tagger&quot;)
class KeywordPosTagger:
    def __init__(self, name, nlp, keywords, pos_tag):
        self.keywords = keywords
        self.pos_tag = pos_tag

    def __call__(self, doc):
        for token in doc:
            if token.text in self.keywords:
                token._.pos_tag = self.pos_tag
        return doc


nlp = spacy.load(&quot;pt_core_news_md&quot;)

keywords = (&quot;m¬≤&quot;, &quot;m2&quot;, &quot;(W/K)&quot;, &quot;¬∫C&quot;)
pos_tag = &quot;UNM&quot;  # substitua por seu r√≥tulo POS

config = {&quot;keywords&quot;: keywords, &quot;pos_tag&quot;: pos_tag}

nlp.add_pipe(&quot;keyword_pos_tagger&quot;, config=config)

doc = nlp(
    &quot;A temperatura tem 159¬∫C ou 20 ¬∫C. Tamb√©m precisa ter 20m de largura e 14 m¬≤ de √°rea, caso contr√°rio ter√° 1 Kelvin (W/K)&quot;
)
assert doc[16]._.pos_tag == &quot;UNM&quot;
</code></pre>
",""
"77222088","2023-10-03 12:01:28","1","","77210041","<p>I have installed PyTorch on multiple combinations (OS+Hardware).</p>
<p>I have installed PyTorch successfully using those commands (in a virtual environment):</p>
<ul>
<li><p><code>%pip install --upgrade transformers</code></p>
</li>
<li><p><code>%pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117</code></p>
</li>
<li><p><code>%pip install accelerate (will take latest at the time of writing @0.23.0)</code></p>
</li>
<li><p><code>%pip install evaluate datasets</code></p>
</li>
</ul>
<p>These helped me kick-start any of the projects which require HuggingFace. I hope it helps you.</p>
",""
"77175577","2023-09-25 20:36:26","0","","76956484","<p>Turns out, I did need to configure the CLI. I never expected I would have to do that from within a databricks notebook, but Spark is a harsh mistress. To do so, click on the icon that sort of looks like a terminal on the bottom right. When you mouse over it, it will say, &quot;Open bottom panel&quot;. Then type in <code>databricks configure</code> and from there follow the steps. DO NOT CLOSE THE PANEL. It is ephemeral, so only close it after fine-tuning is completed.</p>
<p>Running it from there led me to another error though; I had commented out the line <code>report_to=['tensorboard'], # REMOVE MLFLOW INTEGRATION FOR NOW</code></p>
<p>That was a mistake:</p>
<p><code>mlflow.exceptions.RestException: RESOURCE_DOES_NOT_EXIST: No experiment was found. If using the Python fluent API, you can set an active experiment under which to create runs by calling mlflow.set_experiment(&quot;experiment_name&quot;) at the start of your program.</code></p>
<p>Only after uncommenting out the line, and configuring the CLI, did the code work.</p>
",""
"77139418","2023-09-20 05:07:09","2","","77135502","<p>In your code you've customized the tokenizer to handle the special case &quot;gimme&quot; and normalize it to &quot;give.</p>
<p>Here's how you can achieve consistent lemmatization results with your custom normalization</p>
<pre class=""lang-py prettyprint-override""><code>import spacy
from spacy.language import Language
from spacy.symbols import ORTH, NORM
        
nlp = spacy.load(&quot;en_core_web_sm&quot;)
special_case = [{ORTH: &quot;gim&quot;, NORM: &quot;give&quot;}, {ORTH: &quot;me&quot;}]
nlp.tokenizer.add_special_case(&quot;gimme&quot;, special_case)
        
# Define a custom lemmatization function
@Language.component(name=&quot;custom_lemmatizer&quot;)
def custom_lemmatizer_function(doc):
    for token in doc:
        if token.norm_ == &quot;give&quot;:
            token.lemma_ = &quot;give&quot;
    # Add more custom rules for other words if needed
    return doc
        
# Add the custom lemmatizer to the pipeline
nlp.add_pipe(&quot;custom_lemmatizer&quot;, name=&quot;custom_lemmatizer&quot;, after=&quot;lemmatizer&quot;)
        
doc = nlp(&quot;gimme that. he gave me that. Going to someplace.&quot;)
print(doc[0].lemma_)  # 'give' (as expected)
print(doc[5].lemma_)  # 'give' (as expected)
print(doc[9].lemma_)  # 'go' (as expected)
</code></pre>
",""
"77131799","2023-09-19 04:50:39","0","","77043285","<p>The function <code>sentence_bleu</code> expects a list of list of tokens as reference, and a list of tokens as hypothesis. Your supplied input just does not correlate with the expectations.</p>
<p>Once you fix it, you will get:</p>
<pre><code>smooth_fn = SmoothingFunction()
nltk_bleu = nltk.translate.bleu_score.sentence_bleu([tokenized_ref.split(' ')], hypothesis_trans
lations[0].split(' '), smoothing_function=smooth_fn.method3)
print(nltk_bleu)

&gt;&gt;&gt;
0.43560338053780967
</code></pre>
<p>Also, you should take into account that by default it calculates BLEU-4 (for 4-grams) and also consider difference from the smoothing functions.</p>
",""
"77102380","2023-09-14 06:40:20","0","","77084206","<p>This table is the docs is just meant to be a generic example of the kinds of annotation you might see, and the exact annotation from each individual model may be different, also for each individual release/version of a model.</p>
<p>You're not going to have much luck detecting imperatives using the <code>en_core_web_*</code> models because the training data doesn't distinguish imperatives from other forms. The rules that handle the tagset conversion are largely based on this table (note that there's no <code>Mood=Imp</code> for any PTB tag):</p>
<p><a href=""https://universaldependencies.org/tagset-conversion/en-penn-uposf.html"" rel=""nofollow noreferrer"">https://universaldependencies.org/tagset-conversion/en-penn-uposf.html</a></p>
<p>However, it does look like some of the UD English corpora do include <code>Mood=Imp</code> or use fine-grained tags that distinguish imperatives. To start out, you could test out a pretrained UD English EWT model from a tool like Stanza or Trankit to see if that works well enough for your task. It can be a difficult distinction to make, so I don't know how good the overall performance might be, though.</p>
<p>If you'd like to keep working with spacy, you could use <code>spacy-stanza</code> with the default &quot;en&quot; models, which are trained on UD English EWT.</p>
",""
"77093884","2023-09-13 03:32:16","0","","77085879","<p>The <code>LimeTabularExplainer</code> requires probabilities, not predictions. So instead of passing <code>clf.predict</code> you need to either pass <code>clf.predict_proba</code> or a wrapper function that returns probabilities from features. For example based on <a href=""https://marcotcr.github.io/lime/tutorials/Tutorial%20-%20continuous%20and%20categorical%20features.html"" rel=""nofollow noreferrer"">this</a> tutorial:</p>
<pre><code>predict_fn = lambda x: rf.predict_proba(encoder.transform(x))
exp = explainer.explain_instance(X_test, predict_fn)
</code></pre>
",""
"77088097","2023-09-12 10:04:49","0","","77078119","<p>All <code>5xx</code> errors belong to the <code>ServiceUnavailableError</code>. Take a look at the official <a href=""https://platform.openai.com/docs/guides/error-codes/python-library-error-types"" rel=""nofollow noreferrer"">OpenAI documentation</a>:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>TYPE</th>
<th>OVERVIEW</th>
</tr>
</thead>
<tbody>
<tr>
<td>APIError</td>
<td><strong>Cause:</strong> Issue on our side.<br> <strong>Solution:</strong> Retry your request after a brief wait and contact us if the issue persists.</td>
</tr>
<tr>
<td>Timeout</td>
<td><strong>Cause:</strong> Request timed out.<br> <strong>Solution:</strong> Retry your request after a brief wait and contact us if the issue persists.</td>
</tr>
<tr>
<td>RateLimitError</td>
<td><strong>Cause:</strong> You have hit your assigned rate limit.<br> <strong>Solution:</strong> Pace your requests. Read more in our <a href=""https://platform.openai.com/docs/guides/rate-limits"" rel=""nofollow noreferrer"">Rate limit guide</a>.</td>
</tr>
<tr>
<td>APIConnectionError</td>
<td><strong>Cause:</strong> Issue connecting to our services.<br> <strong>Solution:</strong> Check your network settings, proxy configuration, SSL certificates, or firewall rules.</td>
</tr>
<tr>
<td>InvalidRequestError</td>
<td><strong>Cause:</strong> Your request was malformed or missing some required parameters, such as a token or an input.<br> <strong>Solution:</strong> The error message should advise you on the specific error made. Check the <a href=""https://platform.openai.com/docs/api-reference/"" rel=""nofollow noreferrer"">documentation</a> for the specific API method you are calling and make sure you are sending valid and complete parameters. You may also need to check the encoding, format, or size of your request data.</td>
</tr>
<tr>
<td>AuthenticationError</td>
<td><strong>Cause:</strong> Your API key or token was invalid, expired, or revoked.<br> <strong>Solution:</strong> Check your API key or token and make sure it is correct and active. You may need to generate a new one from your account dashboard.</td>
</tr>
<tr>
<td>ServiceUnavailableError</td>
<td><strong>Cause:</strong> Issue on our servers.<br> <strong>Solution:</strong> Retry your request after a brief wait and contact us if the issue persists. Check the <a href=""https://status.openai.com/"" rel=""nofollow noreferrer"">status page</a>.</td>
</tr>
</tbody>
</table>
</div>
<p>Handle the <code>ServiceUnavailableError</code> as follows:</p>
<pre><code>try:
  # Make your OpenAI API request here
  response = openai.Completion.create(prompt=&quot;Hello world&quot;,
                                      model=&quot;text-davinci-003&quot;)

except openai.error.ServiceUnavailableError as e:
  # Handle 5xx errors here
  print(f&quot;OpenAI API request error: {e}&quot;)
  pass
</code></pre>
",""
"77075337","2023-09-10 08:32:26","0","","77074094","<p>After reading some source code, it seems that you can just use any iterator in a Dataloader's <code>batch_sampler</code>. So the following works as expected.</p>
<pre class=""lang-py prettyprint-override""><code>from random import randint

from torch.utils.data import Dataset
from torch.utils.data.dataloader import DataLoader


class DummyDataset(Dataset):
    def __init__(self):
        data = []
        for _ in range(128):
            data.append(&quot;hello &quot; * randint(64, 176))
        self.data = data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx: int):
        return self.data[idx]


class TokenBatchSampler:
    def __init__(self, max_tokens: int = 250):
        self.max_tokens = max_tokens
        self.batches = []
        self._prepare_dataset()

    def __len__(self) -&gt; int:
        return len(self.batches)

    def __iter__(self):
        return iter(self.batches)

    def _prepare_dataset(self):
        data_idxs = list(range(len(dataset)))

        batches = []
        batch_idxs = []
        total_batch_len = 0
        while data_idxs:
            sample_idx = data_idxs[0]
            sample = dataset[sample_idx]
            sample_len = len(sample.split())

            if total_batch_len + sample_len &lt;= self.max_tokens:
                batch_idxs.append(sample_idx)
                total_batch_len += sample_len
                data_idxs.pop(0)
            elif batch_idxs:
                batches.append(batch_idxs)
                batch_idxs = []
                total_batch_len = 0

        batches.append(batch_idxs)

        self.batches = batches


if __name__ == &quot;__main__&quot;:
    dataset = DummyDataset()

    sampler = TokenBatchSampler()
    dataloader = DataLoader(dataset, batch_sampler=sampler)
    # Sanity check that we indeed get all items from the dataset
    for epoch in range(3):
        num_samples = 0
        num_batches = 0
        for b in dataloader:
            num_samples += len(b)
            num_batches += 1

        print(f&quot;Created {num_batches} batches in epoch {epoch}&quot;)
        assert num_samples == len(dataset)

    print(f&quot;DataLoader length {len(dataloader)}&quot;)

</code></pre>
",""
"76893606","2023-08-13 13:34:51","4","","76892275","<p>The end of word marker <code>&lt;/w&gt;</code> is part of the tokens during the creation of a vocabulary, not a token per se.</p>
<p>Once the BPE vocabulary creation is finished, you normally invert the mark: you mark tokens that lack of the end-of-word marker. In the <a href=""https://github.com/rsennrich/subword-nmt"" rel=""nofollow noreferrer"">original implementation</a>, the lack of end-of-word marker was expressed as <code>@@</code>.  That's why to restore the original implementation you simply had to remove the occurrences &quot;@@ &quot;, so that the tokens that belonged to the same words were attached together.</p>
<p>In the HuggingFace implementation, they mimick OpenAI's implementation and use a slightly different approach, representing the space as part of the tokens themselves. For this, they use the <code>\u0120</code> marker, which you can see in the <a href=""https://huggingface.co/gpt2/raw/main/vocab.json"" rel=""nofollow noreferrer"">GPT-2 vocabulary</a> at the beginning of many tokens. You can see details about this in <a href=""https://github.com/openai/gpt-2/issues/80"" rel=""nofollow noreferrer"">this github issue</a>. <a href=""https://discuss.huggingface.co/t/bpe-tokenizers-and-spaces-before-words/475"" rel=""nofollow noreferrer"">This huggingface disccussion</a> shares some context on this.</p>
<p>That's why you won't see any end-of-word marker in BPE vocabularies.</p>
",""
"76814299","2023-08-01 18:24:01","4","","76814175","<p>Don't reinvent the wheel, use <a href=""https://docs.python.org/3/library/functools.html#functools.cache"" rel=""nofollow noreferrer""><code>functools.cache</code></a>:</p>
<pre><code>from functools import cache

@cache
def get_lm(input_sent:str=&quot;my text!&quot;):
    tks = [ w for w in tokenizer.tokenize(input_sent.lower()) if not w in STOPWORDS and len(w) &gt; 1 and not w.isnumeric() ]
    lms = [ wnl.lemmatize(w, t[0].lower()) if t[0].lower() in ['a', 's', 'r', 'n', 'v'] else wnl.lemmatize(w) for w, t in nltk.pos_tag(tks)] 
    return lms

df[&quot;lemma&quot;] = df[&quot;raw_sentence&quot;].map(lambda raw: get_lm(input_sent=raw), na_action='ignore')
</code></pre>
<p>Output:</p>
<pre><code>  user_ip                 raw_sentence                   lemma
0      u7              First sentence!       [first, sentence]
1      u3                          NaN                     NaN
2      u1     I go to school everyday!  [go, school, everyday]
3      u9             She likes chips!            [like, chip]
4      u4     I go to school everyday!  [go, school, everyday]
5      u8       This is 1 sample text!          [sample, text]
6      u1             She likes chips!            [like, chip]
7      u2  This is the thrid sentence.       [thrid, sentence]
8      u5     I go to school everyday!  [go, school, everyday]
</code></pre>
",""
"76803336","2023-07-31 11:51:44","1","","76802665","<p>One popular metric for text similarity is the Levenshtein distance or edit distance, which measures the minimum number of single-character edits (insertions, deletions, or substitutions) required to transform one string into another.</p>
<p>Try implementing below code. Adjust <code>threshold</code> as per your requirement.</p>
<pre><code>import Levenshtein

def text_similarity_evaluation(labels, preds, threshold=0.8):
    tp, fp, fn = 0, 0, 0

    for label, pred in zip(labels, preds):
        similarity_score = 1 - Levenshtein.distance(label, pred) / max(len(label), len(pred))
        if similarity_score &gt;= threshold:
            tp += 1
        else:
            fp += 1

    fn = len(labels) - tp

    precision = tp / (tp + fp)
    recall = tp / (tp + fn)
    f1_score = 2 * (precision * recall) / (precision + recall)

    return precision, recall, f1_score

# Example usage
labels = [&quot;I am fine&quot;, &quot;He was born in 1995&quot;, &quot;The Eiffel tower&quot;, &quot;dogs&quot;]
preds = [&quot;I am fine&quot;, &quot;born in 1995&quot;, &quot;Eiffel&quot;, &quot;dog&quot;]

precision, recall, f1_score = text_similarity_evaluation(labels, preds, threshold=0.8)
print(&quot;Precision:&quot;, precision)
print(&quot;Recall:&quot;, recall)
print(&quot;F1-Score:&quot;, f1_score)
</code></pre>
",""
"76590240","2023-06-30 15:17:57","4","","76589840","<p>The <code>Trainer</code> automatically moves the model to the appropriate device if the <a href=""https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/trainer#transformers.Trainer"" rel=""nofollow noreferrer""><code>place_model_on_device</code> attribute</a> is <code>True</code>.</p>
<p>My suggestion would be to remove any moving code &amp; do it through the <code>transformers</code> library. You can specify <a href=""https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/trainer#transformers.TrainingArguments.no_cuda"" rel=""nofollow noreferrer""><code>no_cuda</code> in <code>TrainingArguments</code></a> as <code>False</code> so that the training objects aren't moved to GPU.</p>
",""
"76456769","2023-06-12 12:40:59","0","","76422222","<h1>Use pipelines, but there is a catch.</h1>
<p>Because you are passing all the processing steps, you need to pass the args for each one of them - when needed. For the tokenizer, we define:</p>
<pre><code> tokenizer = AutoTokenizer.from_pretrained(selected_model)
 tokenizer_kwargs = {'padding':True,'truncation':True,'max_length':512}
 
</code></pre>
<p>The model is straight forward:</p>
<pre><code>model = AutoModelForSequenceClassification.from_pretrained(&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;)
</code></pre>
<p>Then finally:</p>
<pre><code>classifier = pipeline(&quot;text-classification&quot;, model=model, batch_size=32, tokenizer=tokenizer)
</code></pre>
<hr />
<h2>Specific to my application:</h2>
<p>Since I need the <code>logits</code> and not the predicted classes, I will have to modify the pipeline class. <a href=""https://huggingface.co/docs/transformers/add_new_pipeline"" rel=""nofollow noreferrer"">Documentation</a> says that in order to create a custom pipeline class, I need to define four mandatory methods: <code>implement preprocess</code>, <code>_forward</code>, <code>postprocess</code>, and <code>_sanitize_parameters</code>... OR I can overwrite <code>postprocess</code> method from the <code>TextClassificationPipeline</code>:</p>
<pre><code>class MyPipeline(TextClassificationPipeline):
     def postprocess(self, model_outputs):
         return model_outputs[&quot;logits&quot;][0]
</code></pre>
<p>and modify the call:</p>
<pre><code>classifier = pipeline(&quot;text-classification&quot;, model=model, batch_size=32, tokenizer=tokenizer, pipeline_class=MyPipeline)

logits = classifier(text, **tokenizer_kwargs)
</code></pre>
",""
"76433107","2023-06-08 14:50:56","0","","76186890","<p>I think you might not be thinking of &quot;ignore the context&quot; in the same whay that they are. When they want to context to be ignored, they effectively mean they want to compute the log probs for the answer conditioned on the context; e.g., they want something like P(answer|context_1) and P(answer|context_2) instead of P(context_1 + answer) or P(context_2 + answer). If you want to ignore the context entirely, that would be P(answer) - in which case just don't pass the context into the model.</p>
<p>Basically, the probability of the answer SHOULD change when given different contexts  - but you want only the conditional probability of the answer given the context, not the joint of the answer and context. You want &quot;How likely is this answer given this context?&quot;, not &quot;How likely am I to see this context and answer in general&quot;.</p>
<p>Lastly, tokens with value of -100 are ignored by the cross entropy loss - that's why they're used, and why you get a different value if you set them to 0.</p>
",""
"76417660","2023-06-06 19:01:00","2","","76417261","<p>I'm not sure but it seems like <a href=""https://github.com/Mimino666/langdetect"" rel=""nofollow noreferrer""><code>langdetect</code></a> can't handle <em>urls</em> :</p>
<pre><code>tmp = df.loc[[24], [&quot;FonctionsStagiaire&quot;, &quot;ExigencesParticulieres&quot;]].T
‚Äã
                                                                       24
FonctionsStagiaire      https://www2.csrdn.qc.ca//files/jobs/P-22-875-...
ExigencesParticulieres  https://www2.csrdn.qc.ca//files/jobs/P-22-875-...
</code></pre>
<p>Using <code>tmp[24].apply(detect)</code> (<em>row 25</em>) throws a <code>LangDetectException: No features in text.</code></p>
<p>An alternative would be to use <a href=""https://github.com/saffsd/langid.py"" rel=""nofollow noreferrer""><code>langid</code></a> :</p>
<pre><code>#pip install langid
from langid import classify

use_cols = [&quot;FonctionsStagiaire&quot;, &quot;ExigencesParticulieres&quot;]

# checking english content
is_en = [
    classify(r)[0] == &quot;en&quot;
    for r in df[use_cols].fillna(&quot;&quot;).agg(&quot; &quot;.join, axis=1)
]

# not a null content
not_na = df[use_cols].notna().all(axis=1)

# not a random content (optional!)
not_arc = df[use_cols].apply(lambda x: x.str.fullmatch(&quot;\w+\s?\d?&quot;)).any(axis=1)

out = df.loc[is_en &amp; not_na &amp; ~not_arc]
</code></pre>
<p>Output :</p>
<pre><code>print(out.loc[:, use_cols])

                            FonctionsStagiaire                   ExigencesParticulieres
11     As a Level I Technical Customer Serv...  Qualifications           Your contri...
106    What you'll create and do    Today's...  What you'll bring to this role:    A...
140    The Training Department plays a cruc...  REQUIREMENTS:            \t     Coll...
...                                        ...                                      ...
13608  CCHS Facility: Cleveland Clinic Cana...  MINIMUM QUALIFICATIONS:  ¬∑ Registere...
13697                               Calculate!                          Love numbers...
13698                               Calculate!                          Love numbers...

[311 rows x 2 columns]
</code></pre>
",""
"76322987","2023-05-24 11:20:04","0","","76321540","<p>One thing to keep in mind is that everything in your front end is essentially public. In this case, if you do the call directly from the browser, it's trivial for users to capture your api key.</p>
<p>Removing the call to your server likely won't make a significant difference anyway; AI is rather slow. A better solution may be to use the streaming API (and also stream from your backend to your frontend) so the users can see the response as it's generated.</p>
",""
"76320287","2023-05-24 05:23:00","0","","76067091","<p>I solve the issue by using the following package versions.</p>
<pre><code>!pip install transformers==4.28.1
!pip install sentencepiece==0.1.97
!pip install accelerate==0.18.0
!pip install bitsandbytes==0.37.2
!pip install torch==1.13.1
</code></pre>
",""
"76159512","2023-05-02 23:30:23","0","","76158903","<p>After reading the <a href=""https://spacy.io/usage#installation"" rel=""nofollow noreferrer""><code>installation</code></a> documentation and seeing the <code>warning</code> that you got, it seems that <code>spaCy</code> <strong>does not</strong> currently support <code>CUDA 12.1</code>. The installation guide is misleading because you can select the version of <code>CUDA</code> and there is a version for <code>CUDA 10.2, 11.0+</code>. Which suggests that anything above 11.0 is supported. However, that is not the case.</p>
<p>Here is what I mean by the installation guide being misleading:
<a href=""https://i.sstatic.net/NUqoR.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/NUqoR.png"" alt=""enter image description here"" /></a></p>
<p>Your best bet would be to downgrade your version of <code>CUDA</code> to <code>CUDA 11.8</code>. If you're looking for the <code>CUDA 11.8</code> links on the nvidia website:</p>
<ul>
<li><a href=""https://docs.nvidia.com/cuda/archive/11.8.0/"" rel=""nofollow noreferrer"">https://docs.nvidia.com/cuda/archive/11.8.0/</a></li>
<li><a href=""https://developer.nvidia.com/cuda-11-8-0-download-archive"" rel=""nofollow noreferrer"">https://developer.nvidia.com/cuda-11-8-0-download-archive</a></li>
</ul>
<p>Here is a GitHub discussion that talks about which version of <code>CUDA</code> should be used with both <code>spaCy</code> and <code>pytorch</code> after the release of <code>CUDA 12.1</code>:</p>
<ul>
<li><a href=""https://github.com/explosion/spaCy/discussions/12353"" rel=""nofollow noreferrer"">https://github.com/explosion/spaCy/discussions/12353</a></li>
</ul>
",""
"76136550","2023-04-29 13:46:17","2","","76136216","<p>The problem is that you are not using the correct target. You are basically encoding two times the text with the <code>CountVectorizer</code>, in these lines:</p>
<pre><code>x = vec.fit_transform(df.clean_text)
y = vec.transform(df.clean_text)
</code></pre>
<p>Instead you should encode the binary class in df.target as target for the model (your <code>Y</code>)</p>
<pre><code>def labeling(v):
    if v == categories[0]:
        return 0
    else:
        return 1

df[&quot;target_encod&quot;] = df.target.map(labeling)
print(df['target_encod'])
</code></pre>
<p>after that you can use the correct y for your machine learning problem</p>
<pre><code>X = x.toarray()
Y = df[&quot;target_encod&quot;].values
</code></pre>
<p>My result after the changes:</p>
<p><a href=""https://i.sstatic.net/hGtV5.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/hGtV5.png"" alt=""AUROC"" /></a></p>
<p>For the next question, you forgot to assign a variable to the randomForest instance</p>
<pre><code>RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                   criterion='gini', max_depth=None, max_features=5,
                   max_leaf_nodes=None, max_samples=None,
                   min_impurity_decrease=0.0, #min_impurity_split=None,
                   min_samples_leaf=1, min_samples_split=2,
                   min_weight_fraction_leaf=0.0, n_estimators=500,
                   n_jobs=None, oob_score=False, random_state=None,
                   verbose=0, warm_start=False)
</code></pre>
<p>instead of</p>
<pre><code>rf = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                   criterion='gini', max_depth=None, max_features=5,
                   max_leaf_nodes=None, max_samples=None,
                   min_impurity_decrease=0.0, #min_impurity_split=None,
                   min_samples_leaf=1, min_samples_split=2,
                   min_weight_fraction_leaf=0.0, n_estimators=500,
                   n_jobs=None, oob_score=False, random_state=None,
                   verbose=0, warm_start=False)
</code></pre>
",""
"76091954","2023-04-24 12:36:27","3","","76091659","<p>The <a href=""https://github.com/jfilter/german-preprocessing/blob/master/german/preprocessing.py#L30"" rel=""nofollow noreferrer""><code>preprocess</code></a> function returns a copy of the texts, instead of modifying the input. So you need to write <em>the result of <code>preprocess</code></em> to the file, not the original <code>i</code> messages.</p>
<p>Furthermore, <code>preprocess</code> accepts a <em>list</em> of texts to process, so you must wrap your message in <code>[message]</code>, and extract the single result from the returned list with <code>result, = ...</code></p>
<pre><code>from german import preprocess

df = pd.read_csv('Afd.csv', sep=',')

Lemma = open('MessageAFD_lemma.txt', 'w')
for message in df['message']:
    result, = preprocess([message], remove_stop=True)
    Lemma.write(result)
Lemma.close()

# Or, to process all messages in one go:
with open('MessageAFD_lemma.txt', 'w') as f:
    for result in preprocess(df['message'], remove_stop=True):
        f.write(result)
</code></pre>
",""
"76085163","2023-04-23 13:34:09","0","","76074982","<p>try converting test set to <code>TensorDataset</code> and then use <code>DataLoader</code>. somthing like this:</p>
<pre><code>from torch.utils.data import DataLoader, TensorDataset

batch_size = 32

test_dataset = TensorDataset(*[tokenized_test[key] for key in tokenized_test])
test_dataloader = DataLoader(test_dataset, batch_size=batch_size)

with torch.no_grad():
    logits_list = []
    for batch in test_dataloader:
        batch_logits = model(*batch).logits
        logits_list.append(batch_logits)

logits = torch.cat(logits_list)
</code></pre>
",""
"76050942","2023-04-19 05:11:25","1","","76050174","<p>Reverse your map:</p>
<pre><code>reverse_map1 = {rf'(?i)\b{v}\b': k for k, l in map1.items() for v in l}
df['feature'] = df['feature'].replace(reverse_map1, regex=True)
</code></pre>
<p>Output:</p>
<pre><code>&gt;&gt;&gt; df
                                                            feature
0                        0412 ride_share TRIP HELP.ride_share.COMCA
1                        0410 ride_share TRIP HELP.ride_share.COMCA
2         MOBILE PURCHASE 0410 VALENCIA whole_foods SAN FRANCISCOCA
3  whole_foods WBG#1 04/13 PURCHASE whole_foods WBG#104 BROOKLYN NY
4                  0414 ride_share *CITI BIKE BIK ride_share.COM CA
5                                 0421 pharmacy.COM 877-250-5823 IL
6                      0421 Rapha Racing payment LLC XXX-XX72742 OR
7                  0422 food_delivery PAYMENT HELP.ride_share.COMCA
8                        0912 whole_foods NOE 10379 SAN FRANCISCOCA
9           PURCHASE 1003 food_delivery*JUNOON WWW.food_delivery.CA
</code></pre>
<p>Details:</p>
<pre><code>&gt;&gt;&gt; reverse_map1
{'(?i)\\bpmts\\b': 'payment',
 '(?i)\\bpmnt\\b': 'payment',
 '(?i)\\bpmt\\b': 'payment',
 '(?i)\\bpyment\\b': 'payment',
 '(?i)\\bpymnts\\b': 'payment',
 '(?i)\\bacct\\b': 'account',
 '(?i)\\bwalgreens\\b': 'pharmacy',
 '(?i)\\bwalgreen\\b': 'pharmacy',
 '(?i)\\briteaid\\b': 'pharmacy',
 '(?i)\\bcvs\\b': 'pharmacy',
 '(?i)\\bpharm\\b': 'pharmacy',
 '(?i)\\buber eats\\b': 'food_delivery',
 '(?i)\\bdoordash\\b': 'food_delivery',
 '(?i)\\bseamless\\b': 'food_delivery',
 '(?i)\\bgrubhub\\b': 'food_delivery',
 '(?i)\\bcaviar\\b': 'food_delivery',
 '(?i)\\buber\\b': 'ride_share',
 '(?i)\\blyft\\b': 'ride_share',
 '(?i)\\bwholefds\\b': 'whole_foods',
 '(?i)\\bwhole foods\\b': 'whole_foods',
 '(?i)\\bwhole food\\b': 'whole_foods'}
</code></pre>
<ul>
<li><code>(?i)</code>: case insensitive</li>
<li><code>\b...\b</code>: word boundary</li>
</ul>
<p><strong>Update</strong></p>
<p>If you don't care about the lower/upper case, you can use:</p>
<pre><code>reverse_map1 = {rf'\b{v}\b': k for k, l in map1.items() for v in l}
df['feature'] = df['feature'].str.lower().replace(reverse_map1, regex=True)
</code></pre>
",""
"75929723","2023-04-04 13:03:49","13","","75928704","<p>Try this example code, works well for me, <strong>if</strong> your <code>YOUR-APIKEY</code> is valid. Let me know if this does not work for you.</p>
<p><strong>Note</strong>, use the latest <code>main</code> branch of <code>OpenAISwift</code>, not other versions or tags.</p>
<pre><code>import Foundation
import SwiftUI
import OpenAISwift


@MainActor
class OpenAIModel: ObservableObject {
    
    @Published var answers = [String]()
    
    let client: OpenAISwift
    
    init() {
        client = OpenAISwift(authToken: &quot;YOUR-APIKEY&quot;)
    }
    
    func ask(text: String) async {
        do {
            let result = try await client.sendCompletion(
                with: text,
                model: .gpt3(.davinci),
                maxTokens: 500,
                temperature: 1
            )
            let output = result.choices?.first?.text ?? &quot;no answer&quot;
            answers.append(output)
        } catch {
            print(error)
        }
    }
    
    func makeCall(text: String, completion: @escaping (String) -&gt; Void) {
        client.sendCompletion(with: text, maxTokens: 500) { result in
            switch result {
            case .success(let model):
                let output = model.choices?.first?.text.trimmingCharacters(in: .whitespacesAndNewlines) ?? &quot;&quot;
                completion(output)
            case .failure:
                print(&quot;---&gt; Failed: \(result)&quot;)
                completion(&quot;Failed&quot;)
                break
            }
        }
    }
    
}

struct ContentView: View {
    @StateObject var openAI = OpenAIModel()
    let text = &quot;explain Schr√∂dinger's wave equation&quot;
    
    var body: some View {
        VStack {
            Text(&quot;fetching...&quot;)
            ForEach(openAI.answers, id: \.self) { answer in
                Text(answer)
            }
        }
        .onAppear {
            openAI.makeCall(text: text){ answer in
                DispatchQueue.main.async {
                    openAI.answers.append(answer)
                }
            }
        }
//        .task{
//            await openAI.ask(text: text)
//        }
    }
}
</code></pre>
<p>Note also, there is an issue reported at: <a href=""https://github.com/adamrushy/OpenAISwift/issues/28"" rel=""nofollow noreferrer"">https://github.com/adamrushy/OpenAISwift/issues/28</a> regarding a bug in decoding, it may be relevant to your case.</p>
",""
"75856233","2023-03-27 13:16:21","0","","75851367","<p>I found this <code>Linux</code> command that can list all the past or current starting processes:</p>
<p><code>ps -eo pid,lstart,cmd -u user_name | grep -i python3</code>
And then kill specific GPUs by following the command after I know which script running on specific GPUs
<code>kill -9 &lt;process_id&gt;</code></p>
",""
"75769023","2023-03-17 14:52:02","1","","75763642","<p>Your calculation is correct, you are just missing the normalization. With default parameters each document is normalized, so that the euclidian length of each document vector equals 1. You can disable the normalization with  the parameter <code>norm=None</code></p>
<pre><code>corpus = [
    'This is the first document.',
    'This is the second second document.',
    'And the third one.',
    'Is this the first document?',
]

vectorizer = TfidfVectorizer(norm=None)
X = vectorizer.fit_transform(corpus)
</code></pre>
<p>results in:</p>
<pre><code>array([[0.        , 1.22314355, 1.51082562, 1.22314355, 0.        ,
        0.        , 1.        , 0.        , 1.22314355],
       [0.        , 1.22314355, 0.        , 1.22314355, 0.        ,
        3.83258146, 1.        , 0.        , 1.22314355],
       [1.91629073, 0.        , 0.        , 0.        , 1.91629073,
        0.        , 1.        , 1.91629073, 0.        ],
       [0.        , 1.22314355, 1.51082562, 1.22314355, 0.        ,
        0.        , 1.        , 0.        , 1.22314355]])
</code></pre>
<p>Exactly the tfidf value you calculated for the token 'document' in the first document.</p>
",""
"75764096","2023-03-17 05:40:25","0","","67664837","<p>In order to compute the nDCG, you need to know what is the relevance of each document in a <strong>ranked</strong> list of results for this query. This information is contained in <code>qrels</code>. Ranking means that you first need to <strong>sort</strong> retrieved documents in descending order of their score. So you basically sort documents, and then go rank by rank, starting from the lowest rank (i.e. the top-scored document), which is 1. For each rank i you get the document's ground-truth relevance rel_i from <code>qrels</code>, and then you divide this relevance by log_2(i+1) to get a term for this rank i. You sum all these terms across all ranks, and you get the Discounted Cumulative Gain (DCG) for this query.</p>
<p>Therefore, <code>pytrec_eval</code> internally needs to create a sorted list from the dictionary mapping from doc ID to score, in order to get the ranks. This is why the order of the document-score pairs in the dictionary you pass as an input doesn't matter. Now, as an additional detail: to get the nDCG (i.e. normalized DCG), you divide the DCG by the Ideal DCG, which is the maximum DCG achievable by any model; to get the IDCG, you sort the <em>ground-truth</em> (as opposed to retrieved) documents in descending order of their relevance score and compute the DCG. Again, to get the ground-truth relevance scores you need <code>qrels</code>.</p>
",""
"75686412","2023-03-09 15:04:54","0","","75686316","<p>Here's an option  using <code>imap_dfr</code> from <code>purrr</code>:</p>
<pre class=""lang-r prettyprint-override""><code>library(corpus)
library(dplyr)
library(purrr)

text &lt;- data.frame(comment_id = 1:2,
                   comment_content = c(&quot;Hallo mein Name ist aaron&quot;,&quot;Vielen Lieben Dank f√ºr das Video&quot;))


tmp &lt;- text_tokens(text$comment_content, 
                   text_filter(stemmer = &quot;de&quot;,drop = corpus::stopwords_de)) %&gt;% 
  purrr::imap_dfr(function(x, y) {
  tibble(
    comment_id = y,
    comment_tokens = x
  )
})

tmp
#&gt; # A tibble: 6 √ó 2
#&gt;   comment_id comment_tokens
#&gt;        &lt;int&gt; &lt;chr&gt;         
#&gt; 1          1 hallo         
#&gt; 2          1 nam           
#&gt; 3          1 aaron         
#&gt; 4          2 lieb          
#&gt; 5          2 dank          
#&gt; 6          2 video
</code></pre>
<p>Or if you prefer using an anonymous function:</p>
<pre><code>tmp &lt;- text_tokens(text$comment_content, text_filter(stemmer = &quot;de&quot;,drop = corpus::stopwords_de)) %&gt;% 
  purrr::imap_dfr(~ tibble(comment_id = .y, comment_tokens = .x))
</code></pre>
",""
"75625303","2023-03-03 09:28:32","0","","75624727","<p>By default, <code>sentence_bleu</code> is configured with 4 weights: 0.25 for unigram, 0.25 for bigram, 0.25 for trigram, 0.25 for quadrigram. The length of <code>weights</code> give the order of ngram, so the BLEU score is computed for 4 levels of ngrams.</p>
<p>When you use <code>weights=[1]</code>, you only analyze unigram:</p>
<pre><code>reference = [['this', 'is', 'ae', 'test','rest','pep','did']]
candidate = ['this', 'is', 'ad', 'test','rest','pep','did']

&gt;&gt;&gt; sentence_bleu(reference, candidate)  # default weights, order of ngrams=4
0.488923022434901
</code></pre>
<p>But you can also consider unigrams are more important than bigrams which are more important than tri and quadigrams:</p>
<pre><code>&gt;&gt;&gt; sentence_bleu(reference, candidate, weights=[0.5, 0.3, 0.1, 0.1])
0.6511772622175621
</code></pre>
<p>You can also use <a href=""https://www.nltk.org/api/nltk.translate.bleu_score.html"" rel=""nofollow noreferrer""><code>SmoothingFunction</code></a> methods and read the <a href=""https://www.nltk.org/_modules/nltk/translate/bleu_score.html"" rel=""nofollow noreferrer"">docstring</a> from source code to better understanding.</p>
",""
"75618063","2023-03-02 16:12:48","2","","75595699","<p>Turns out, the log message about <code>BertTokenizerFast</code> had nothing to do with the progress bar that appeared right after, which I thought was the tokenization progress bar but was in fact the training progress bar. The actual problem was that the model was training on CPU instead of GPU. I thought I had ruled this out because I had verified that <code>torch.cuda.isAvailable() == True</code> and <a href=""https://discuss.huggingface.co/t/sending-a-dataset-or-datasetdict-to-a-gpu/17208/2"" rel=""nofollow noreferrer"">HuggingFace Trainers are supposed to use CUDA if available</a>. However, the installed version of PyTorch was incorrect for my version of CUDA and despite CUDA being &quot;available&quot;, PyTorch refused to use the GPU, making HuggingFace default back to CPU training. All of this was silent and caused no warnings or error messages.</p>
",""
"75595063","2023-02-28 16:54:01","1","","75590491","<p><a href=""https://paperswithcode.com/"" rel=""nofollow noreferrer"">https://paperswithcode.com/</a> is a good resource to understand nuance of different deep learning terminologies and implementation</p>
<p>The general definition of attention mechanism in the transformer model:</p>
<blockquote>
<p><strong>Attention Mechanisms</strong> are a component used in neural networks to model long-range interaction, for example across a text in NLP. The key idea is to <strong>build shortcuts between a context vector and the input, to allow a model to attend to different parts</strong>. - paperswithcode</p>
</blockquote>
<p><a href=""https://i.sstatic.net/KGpwfm.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/KGpwfm.png"" alt=""enter image description here"" /></a></p>
<p><em>In my own words</em>, the &quot;shortcuts&quot; attention is created by doing sequential matrix multiplications of the  &quot;<strong>query</strong>&quot; (inputs) to &quot;<strong>value</strong>&quot; (the target that you want to map the inputs to), and between there, there is a &quot;<strong>key</strong>&quot; that acts like a signal that the query theoretically should make use of to project the query to the value. And the commonnoutput of the attention mechanism is a vector/matrix/tensor representation of that encodes this shortcut.</p>
<p>There are many variants on these &quot;shortcuts&quot; (aka attention mechanisms) that researchers have tried to find the optimal connection from query + key -&gt; value. See list on <a href=""https://paperswithcode.com/methods/category/attention-mechanisms-1"" rel=""nofollow noreferrer"">https://paperswithcode.com/methods/category/attention-mechanisms-1</a></p>
<h2>Attention vs MultiHeadAttention</h2>
<p><a href=""https://i.sstatic.net/IxWVVm.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/IxWVVm.png"" alt=""enter image description here"" /></a></p>
<p><em>In my own words</em>, the main differentiator between general Attention and MultiHeadAttention is the redundancy put into &quot;MultiHead&quot; inputs. If single head (general) attention maps one Q + K to V, <strong>think of multi-head as creating multiple Qs that corresponds to multiple Ks and you want to create the shortcut to multiple corresponding Vs</strong>.</p>
<p>In code, assuming that the initialization for <code>Attention</code>, <code>MultiHeadAttention</code> are the same, the <code>output_tensor</code> values for the following should be the same:</p>
<pre><code>import tensorflow as tf
from tensorflow.keras.layers import Attention, MultiHeadAttention


layer = MultiHeadAttention(num_heads=1, key_dim=2)
target = tf.keras.Input(shape=[8, 16])
source = tf.keras.Input(shape=[4, 16])
output_tensor, weights = layer(target, source,
                               return_attention_scores=True)




layer_vanilla = Attention()
target_vanilla = tf.keras.Input(shape=[8, 16])
source_vanilla = tf.keras.Input(shape=[4, 16])
output_tensor_vanilla, weights_vanilla = layer_vanilla([target_vanilla, source_vanilla],
                               return_attention_scores=True)

print(output_tensor)
print(output_tensor_vanilla)
</code></pre>
<p>[out]:</p>
<pre><code>KerasTensor(type_spec=TensorSpec(shape=(None, 8, 16), dtype=tf.float32, name=None), name='multi_head_attention_6/attention_output/add:0', description=&quot;created by layer 'multi_head_attention_6'&quot;)

KerasTensor(type_spec=TensorSpec(shape=(None, 8, 16), dtype=tf.float32, name=None), name='attention_3/MatMul_1:0', description=&quot;created by layer 'attention_3'&quot;)

</code></pre>
<p><a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention</a></p>
<h2>Attention vs AdditiveAttention</h2>
<p>Additive Attention is an interesting one; it is the OG attention mechanism:</p>
<blockquote>
<p>Additive Attention, also known as Bahdanau Attention, uses a one-hidden layer feed-forward network to calculate the attention alignment score</p>
</blockquote>
<p>Details: <a href=""https://paperswithcode.com/method/additive-attention"" rel=""nofollow noreferrer"">https://paperswithcode.com/method/additive-attention</a></p>
<p>Before &quot;IMOW&quot;, lets look at the code:</p>
<pre><code>from tensorflow.keras.layers import AdditiveAttention

layer_bdn = AdditiveAttention()
target_bdn = tf.keras.Input(shape=[8, 16])
source_bdn = tf.keras.Input(shape=[4, 16])
output_tensor_bdn, weights_bdn = layer_bdn([target_bdn, source_bdn],
                               return_attention_scores=True)

print(output_tensor_bdn)

</code></pre>
<p>[out]:</p>
<pre><code>&lt;KerasTensor: shape=(None, 8, 16) dtype=float32 (created by layer 'additive_attention')&gt;
</code></pre>
<p>Comparing the implementations:</p>
<ul>
<li><a href=""https://github.com/keras-team/keras/blob/v2.11.0/keras/layers/attention/attention.py#L30-L204"" rel=""nofollow noreferrer"">https://github.com/keras-team/keras/blob/v2.11.0/keras/layers/attention/attention.py#L30-L204</a></li>
<li><a href=""https://github.com/keras-team/keras/blob/v2.11.0/keras/layers/attention/additive_attention.py#L30-L178"" rel=""nofollow noreferrer"">https://github.com/keras-team/keras/blob/v2.11.0/keras/layers/attention/additive_attention.py#L30-L178</a></li>
</ul>
<p><a href=""https://www.diffchecker.com/5i9Viqm9/"" rel=""nofollow noreferrer"">https://www.diffchecker.com/5i9Viqm9/</a></p>
<p><a href=""https://i.sstatic.net/SqqQV.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/SqqQV.png"" alt=""enter image description here"" /></a></p>
<p>The general <code>Attention</code> has:</p>
<pre><code>scores = self.concat_score_weight * tf.reduce_sum(
                    tf.tanh(self.scale * (q_reshaped + k_reshaped)), axis=-1
                )
</code></pre>
<p>where the initializer can be set to <code>initializer=&quot;ones&quot;</code> if <code>if self.score_mode == &quot;concat&quot;</code>:</p>
<pre><code>        if self.score_mode == &quot;concat&quot;:
            self.concat_score_weight = self.add_weight(
                name=&quot;concat_score_weight&quot;,
                shape=(),
                initializer=&quot;ones&quot;,
                dtype=self.dtype,
                trainable=True,
            )
</code></pre>
<p>but the <code>AdditiveAttention</code> uses the glorot initializer if the  <code>self.use_scale</code> is set to <code>True</code>:</p>
<pre><code>        if self.use_scale:
            self.scale = self.add_weight(
                name=&quot;scale&quot;,
                shape=[dim],
                initializer=&quot;glorot_uniform&quot;,
                dtype=self.dtype,
                trainable=True,
            )
</code></pre>
<p>There are further nuances in the implementation though.</p>
<p><em>In my own words,</em> additive attention is the earlier definition of the general attention mechanism. They achieve the same purpose of single headed attention. And if the initializations and scaling are set equally, additive attention == general attention.</p>
<h2>Q: Then what should I be using when choosing the attention layer?</h2>
<p>A: Depends on what is the ultimate goal, if the goal is replicate the original Bahdanau paper, then additive attention would be the closest. If not, then the vanilla attention is most probably what you want.</p>
<h2>Q: What about multi-head?</h2>
<p>A: In most cases, you will <strong>always use multi-head attention</strong> since</p>
<ul>
<li>Additive attention is a type of vanilla attention with specified initialization and operations</li>
<li>Attention is a type of multi-head attention where no. of heads is set to 1</li>
</ul>
",""
"75503036","2023-02-19 19:51:24","4","","75488355","<p>You're not actually doing any particularly interesting unification here, so perhaps it's enough to toss a very simple nondeterminism applicative of your own into the mix. The standard one is <code>[]</code>, but for this case, even <code>Maybe</code> looks like enough. Like this:</p>
<pre><code>{-# Language OverloadedStrings #-}
{-# Language TypeApplications #-}

import Control.Applicative
import Control.Monad
import Data.Foldable
import Text.Earley

data Feature = SG | PL deriving (Eq, Ord, Read, Show)

(=:=) :: (Feature, a) -&gt; (Feature, b) -&gt; Maybe (a, b)
(fa, a) =:= (fb, b) = (a, b) &lt;$ guard (fa == fb)

data NP = Name String | Determined String String deriving (Eq, Ord, Read, Show)

np :: Grammar r (Prod r e String (Feature, NP))
np = rule . asum $
    [ fmap (\name -&gt; (SG, Name name)) (&quot;John&quot; &lt;|&gt; &quot;Mary&quot;)
    , liftA2 (\det n -&gt; (PL, Determined det n)) &quot;the&quot; (&quot;boys&quot; &lt;|&gt; &quot;girls&quot;)
    ]

vp :: Grammar r (Prod r e String (Feature, String))
vp = rule . asum $
    [ (,) SG &lt;$&gt; (&quot;runs&quot; &lt;|&gt; &quot;walks&quot;)
    , (,) PL &lt;$&gt; (&quot;run&quot; &lt;|&gt; &quot;walk&quot;)
    ]

s :: Grammar r (Prod r e String (Maybe (NP, String)))
s = liftA2 (liftA2 (=:=)) np vp

test :: [String] -&gt; IO ()
test = print . allParses @() (parser s)
</code></pre>
<p>Try it out in ghci:</p>
<pre><code>&gt; sequence_ [test (words n ++ [v]) | n &lt;- [&quot;John&quot;, &quot;the boys&quot;], v &lt;- [&quot;walks&quot;, &quot;walk&quot;]]
([(Just (Name &quot;John&quot;,&quot;walks&quot;),2)],Report {position = 2, expected = [], unconsumed = []})
([(Nothing,2)],Report {position = 2, expected = [], unconsumed = []})
([(Nothing,3)],Report {position = 3, expected = [], unconsumed = []})
([(Just (Determined &quot;the&quot; &quot;boys&quot;,&quot;walk&quot;),3)],Report {position = 3, expected = [], unconsumed = []})
</code></pre>
<p>So, the result needs a bit of interpretation -- a successful parse of <code>Nothing</code> really counts as a failed parse -- but perhaps that's not so bad? Not sure. Certainly it's unfortunate that you don't get to reuse <code>Earley</code>'s error-reporting and nondeterminism machinery. Probably to get either thing, you'd have to fork <code>Earley</code>.</p>
<p>If you need to do real unification you could look into returning a <a href=""https://hackage.haskell.org/package/unification-fd"" rel=""nofollow noreferrer""><code>IntBindingT t Identity</code></a> instead of a <code>Maybe</code>, but at least until your features are themselves recursive this is probably enough and much, much simpler.</p>
",""
"75394630","2023-02-09 05:39:47","0","","75394318","<p>This is probably not the most elegant answer, but it seems to work. I won't accept this for the next few days in case someone posts a better answer:</p>
<pre><code># this gets me the location (index start &amp; end) of each occurrence of my regex pattern 
delims = list(re.finditer(pattern, text))

# now let's iterate through each pair of delimiter and next-delimiter locations
q_a_list = []

for delim, next_delim in zip(delims[:-1], delims[1:]):

    # pull &quot;Q&quot; or &quot;A&quot; out of the current delimiter
    prefix = text[delim.span()[0]:delim.span()[1]].strip()

    # The actual question or answer text spans from the end of this 
    # delimiter to the start of the next delimiter
    text_chunk = text[delim.span()[1]:next_delim.span()[0]]

    q_a_list.append(f&quot;{prefix}: {text_chunk}&quot;)

# q_a_list is missing the final prefix and text_chunk, because
# they have no next_delim, so the zip() above doesn't get to it
final_delim = delims[-1]

final_prefix = text[final_delim.span()[0]: final_delim.span()[1]].strip()
final_text_chunk = text[final_delim.span()[1]:]

q_a_list.append(f&quot;{final_prefix}: {final_text_chunk}&quot;)
</code></pre>
<p>now the result:</p>
<pre><code>&gt;&gt;&gt; print(q_a_list)
['Q: So I do first want to bring up exhibit No. 46, which is in the binder \nin front of\nyou.\n\nAnd that is a letter [to] Alston\n&amp; Bird...\n\n\nIs that correct?\n', 
'A: This is correct.\n', 
'Q: Okay.']
</code></pre>
",""
"75373688","2023-02-07 12:56:36","1","","75371762","<p>Unfortunately we can't use the <code>vocabulary</code> optional argument to <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"" rel=""nofollow noreferrer"">TfidfVectorizer</a> to signal synonyms; I tried and got error <em>ValueError: Vocabulary contains repeated indices.&quot;</em></p>
<p>Instead, you could run the tfidf vectorizer algorithm once, then manually merge columns that correspond to synonyms.</p>
<pre class=""lang-py prettyprint-override""><code>from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

## DATA
corpus = ['The grey cat eats the navy mouse.',
          'The ashen cat drives the red car.',
          'There is a mouse on the brown banquette of the crimson car.',
          'The teal car drove over the poor cat and tarnished its beautiful silver fur with scarlet blood.',
          'I bought a turquoise sapphire shaped like a cat and  mounted on a rose gold ring.',
          'Mice and cats alike are drowning in the deep blue sea.']
synonym_groups = [['grey', 'gray', 'ashen', 'silver'],
                  ['red', 'crimson', 'rose', 'scarlet'],
                  ['blue', 'navy', 'sapphire', 'teal', 'turquoise']]

## VECTORIZING FIRST TIME TO GET vectorizer0.vocabulary_
vectorizer = TfidfVectorizer(stop_words='english')
X = vectorizer.fit_transform(corpus)

## MERGING SYNONYM COLUMNS
vocab = vectorizer.vocabulary_
synonym_representants = { group[0] for group in synonym_groups }
redundant_synonyms = { word: group[0] for group in synonym_groups for word in group[1:] }
syns_dict = {group[0]: group for group in synonym_groups}
# syns_dict = {next(word for word in group if word in vocab): group for group in synonym_groups} ## SHOULD BE MORE ROBUST

nonredundant_columns = sorted( v for k, v in vocab.items() if k not in redundant_synonyms )

for rep in synonym_representants:
    X[:,vocab[rep]] = X[:, [vocab[syn] for syn in syns_dict[rep] if syn in vocab]].sum(axis=1)

Y = X[:, nonredundant_columns]
new_vocab = [w for w in sorted(vocab, key=vocab.get) if w not in redundant_synonyms]

## COSINE SIMILARITY
cos_sim = cosine_similarity(Y, Y)

## RESULTS
print(' ', ''.join('{:11.11}'.format(word) for word in new_vocab))
print(Y.toarray())
print()
print('Cosine similarity')
print(cos_sim)
</code></pre>
<p>Output:</p>
<pre><code>  alike      banquette  beautiful  blood      blue       bought     brown      car        cat        cats       deep       drives     drove      drowning   eats       fur        gold       grey       like       mice       mounted    mouse      poor       red        ring       sea        shaped     tarnished 
[[0.         0.         0.         0.         0.49848319 0.         0.         0.         0.29572971 0.         0.         0.         0.         0.         0.49848319 0.         0.         0.49848319 0.         0.         0.         0.40876335 0.         0.         0.         0.         0.         0.        ]
 [0.         0.         0.         0.         0.         0.         0.         0.35369727 0.30309169 0.         0.         0.51089257 0.         0.         0.         0.         0.         0.51089257 0.         0.         0.         0.         0.         0.51089257 0.         0.         0.         0.        ]
 [0.         0.490779   0.         0.         0.         0.         0.490779   0.3397724  0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.4024458  0.         0.490779   0.         0.         0.         0.        ]
 [0.         0.         0.31893014 0.31893014 0.31893014 0.         0.         0.2207993  0.18920822 0.         0.         0.         0.31893014 0.         0.         0.31893014 0.         0.31893014 0.         0.         0.         0.         0.31893014 0.31893014 0.         0.         0.         0.31893014]
 [0.         0.         0.         0.         0.65400152 0.32700076 0.         0.         0.19399619 0.         0.         0.         0.         0.         0.         0.         0.32700076 0.         0.32700076 0.         0.32700076 0.         0.         0.32700076 0.32700076 0.         0.32700076 0.        ]
 [0.37796447 0.         0.         0.         0.37796447 0.         0.         0.         0.         0.37796447 0.37796447 0.         0.         0.37796447 0.         0.         0.         0.         0.         0.37796447 0.         0.         0.         0.         0.         0.37796447 0.         0.        ]]

Cosine similarity
[[1.         0.34430458 0.16450509 0.37391712 0.3479721  0.18840894]
 [0.34430458 1.         0.37091192 0.46132163 0.20500145 0.        ]
 [0.16450509 0.37091192 1.         0.23154573 0.14566346 0.        ]
 [0.37391712 0.46132163 0.23154573 1.         0.3172916  0.12054426]
 [0.3479721  0.20500145 0.14566346 0.3172916  1.         0.2243601 ]
 [0.18840894 0.         0.         0.12054426 0.2243601  1.        ]]
</code></pre>
",""
"75335600","2023-02-03 12:14:02","0","","75335523","<p>400 (Bad Request) error code typically means that client request's data is incorrect. So yes, must be something with your auth headers/body of request. Quite often response contains a reason, please try to print the text of response (before trying to get json output), e.g.</p>
<pre><code>console.log(response.text());
</code></pre>
<p>or just check Network Tab in Dev Console</p>
",""
"75250052","2023-01-26 18:05:39","0","","75211491","<p>Your problem boils down to iteration over the pandas dataframe <code>input_df</code>. Doing that with a for loop is not the most efficient way (see: <a href=""https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas"">How to iterate over rows in a DataFrame in Pandas</a>).</p>
<p>I suggest doing something like this:</p>
<pre><code>output_df['work_order_num', 'work_order_desc'] = input_df['order_num', 'description']  # these columns can be copied as whole.

def classification(df_desc):
    temp = classifier(df_desc, labels)
    return temp['labels'][0], temp['scores'][0]
    
output_df['label'], output_df['score'] = zip(*input_df.apply(classification))
</code></pre>
<p><code>classification</code> function returns tuples of values that need to be unpacked so I used the <code>zip</code> trick from <a href=""https://stackoverflow.com/questions/29550414/how-can-i-split-a-column-of-tuples-in-a-pandas-dataframe"">this question</a>.</p>
<p>Also, building a dataframe by concatenation is a very slow process too. So with the solution above you omit two potentially prohibitively slow operations: slow for-loop and appending rows to a dataframe.</p>
",""
"75215495","2023-01-23 22:00:52","3","","75214153","<p>The slow-down in processing speed is coming from the multiple calls to the spaCy pipeline via <code>nlp()</code>. The faster way to process large texts is to instead process them as a stream using the <code>nlp.pipe()</code> command. When I tested this on 5000 rows of dummy text, it offered a ~3.874x improvement in speed (~9.759sec vs ~2.519sec) over the original method. There are ways to improve this further if required, see <a href=""https://stackoverflow.com/a/74193846/20087266"">this checklist</a> for spaCy optimisation I made.</p>
<h2>Solution</h2>
<pre class=""lang-py prettyprint-override""><code># Assume dataframe (df) already contains column &quot;text&quot; with text

# Load spaCy pipeline
nlp = spacy.load(&quot;es_core_news_sm&quot;)

# Process large text as a stream via `nlp.pipe()` and iterate over the results, extracting lemmas
lemma_text_list = []
for doc in nlp.pipe(df[&quot;text&quot;]):
    lemma_text_list.append(&quot; &quot;.join(token.lemma_ for token in doc))
df[&quot;text_lemma&quot;] = lemma_text_list
</code></pre>
<h2>Full code for testing timings</h2>
<pre class=""lang-py prettyprint-override""><code>import spacy
import pandas as pd
import time

# Random Spanish sentences
rand_es_sentences = [
    &quot;Tus drafts influir√°n en la puntuaci√≥n de las cartas seg√∫n tu n√∫mero de puntos DCI.&quot;,
    &quot;Informaci√≥n facilitada por la Divisi√≥n de Conferencias de la OMI en los cuestionarios enviados por la DCI.&quot;,
    &quot;Oleg me ha dicho que ten√≠as que decirme algo.&quot;,
    &quot;Era como t√∫, muy buena con los ordenadores.&quot;,
    &quot;Mas David tom√≥ la fortaleza de Sion, que es la ciudad de David.&quot;]

# Duplicate sentences specified number of times
es_text = [sent for i in range(1000) for sent in rand_es_sentences]
# Create data-frame
df = pd.DataFrame({&quot;text&quot;: es_text})
# Load spaCy pipeline
nlp = spacy.load(&quot;es_core_news_sm&quot;)


# Original method (very slow due to multiple calls to `nlp()`)
t0 = time.time()
df[&quot;text_lemma_1&quot;] = df[&quot;text&quot;].apply(lambda row: &quot; &quot;.join([w.lemma_ for w in nlp(row)]))
t1 = time.time()
print(&quot;Total time: {}&quot;.format(t1-t0))  # ~9.759 seconds on 5000 rows


# Faster method processing rows as stream via `nlp.pipe()`
t0 = time.time()
lemma_text_list = []
for doc in nlp.pipe(df[&quot;text&quot;]):
    lemma_text_list.append(&quot; &quot;.join(token.lemma_ for token in doc))
df[&quot;text_lemma_2&quot;] = lemma_text_list
t1 = time.time()
print(&quot;Total time: {}&quot;.format(t1-t0))  # ~2.519 seconds on 5000 rows
</code></pre>
",""
"75193330","2023-01-21 12:32:43","2","","75173490","<p>The <a href=""https://spacy.io/usage/linguistic-features#vectors-similarity"" rel=""nofollow noreferrer"">spaCy library by default</a> will use the average of the <a href=""https://en.wikipedia.org/wiki/Word_embedding"" rel=""nofollow noreferrer"">word embeddings</a> of words in a sentence to determine semantic similarity. This can be thought of as a naive <a href=""https://en.wikipedia.org/wiki/Sentence_embedding"" rel=""nofollow noreferrer"">sentence embedding</a> approach. Such an approach could work, but if you were to use it is recommended that you first filter non-meaningful words (e.g. common words) to prevent them from undesirably influencing the final sentence embeddings.</p>
<p>The alternative (and more reliable) solution is to use a different pipeline within spaCy that has been designed to use sentence embeddings created specifically with a dedicated sentence encoder (e.g. the <a href=""https://tfhub.dev/google/collections/universal-sentence-encoder/"" rel=""nofollow noreferrer"">Universal Sentence Encoder</a> (USE) <a href=""https://arxiv.org/pdf/1803.11175.pdf"" rel=""nofollow noreferrer"">[1]</a> by Cer et al.). Martino Mensio created a package called <a href=""https://github.com/MartinoMensio/spacy-universal-sentence-encoder"" rel=""nofollow noreferrer"">spacy-universal-sentence-encoder</a> that makes use of this model. Install it via the following command in your command prompt:</p>
<pre class=""lang-none prettyprint-override""><code>pip install spacy-universal-sentence-encoder
</code></pre>
<p>Then you can compute the semantic similarity between sentences as follows:</p>
<pre class=""lang-py prettyprint-override""><code>import spacy_universal_sentence_encoder

# Load one of the models: ['en_use_md', 'en_use_lg', 'xx_use_md', 'xx_use_lg']
nlp = spacy_universal_sentence_encoder.load_model('en_use_lg')

# Create two documents
doc_1 = nlp('Hi there, how are you?')
doc_2 = nlp('Hello there, how are you doing today?')

# Use the similarity method to compare the full documents (i.e. sentences)
print(doc_1.similarity(doc_2))  # Output: 0.9356049733134972
# Or make the comparison using a predefined span of the second document 
print(doc_1.similarity(doc_2[0:7])) # Output: 0.9739387861159459
</code></pre>
<p>As a side note, when you run the <code>nlp = spacy_universal_sentence_encoder.load_model('en_use_lg')</code> command for the first time, you may have to do so with administrator rights to allow TensorFlow to create the <code>models</code> folder in <code>C:\Program Files\Python310\Lib\site-packages\spacy_universal_sentence_encoder</code> and download the appropriate model. If you don't, it is possible that there will be a <code>PermissionDeniedError</code> and the code will not run.</p>
<h2>References</h2>
<p><a href=""https://arxiv.org/pdf/1803.11175.pdf"" rel=""nofollow noreferrer"">[1]</a> Cer, D., Yang, Y., Kong, S.Y., Hua, N., Limtiaco, N., John, R.S., Constant, N., Guajardo-Cespedes, M., Yuan, S., Tar, C. and Sung, Y.H., 2018. Universal sentence encoder. arXiv preprint arXiv:1803.11175.</p>
",""
"75121673","2023-01-14 22:26:09","0","","75116397","<p>I interpret your requirement to match &quot;nouns followed by zero or more sequence of nouns or adjectives&quot; as matching at least one or more sequential nouns (i.e. <code>&lt;N.*&gt;+</code>), followed by zero or more adjectives (i.e. <code>&lt;J.*&gt;*</code>). So putting these together you get the full RegExp as follows:</p>
<pre class=""lang-py prettyprint-override""><code>vectorizer = KeyphraseCountVectorizer(pos_pattern=&quot;&lt;N.*&gt;+&lt;J.*&gt;*&quot;)
</code></pre>
<p>As a side point, you note that you are attempting to extract <em>Arabic</em> keywords. From my understanding the <a href=""https://pypi.org/project/keyphrase-vectorizers/"" rel=""nofollow noreferrer"">keyphrase_vectorizers</a> package relies on the text being annotated with <code>spaCy</code> PoS tags, and so to change languages from the default (English) you have to load a corresponding <a href=""https://spacy.io/models"" rel=""nofollow noreferrer"">pipeline/model</a> in the desired language and set the stop words to those of the new language. For example, if using the Keyphrase Vectorizer for German:</p>
<pre class=""lang-py prettyprint-override""><code>vectorizer = KeyphraseCountVectorizer(spacy_pipeline='de_core_news_sm', stop_words='german')
</code></pre>
<p>However, at present <code>spaCy</code> does not have a pipeline trained for Arabic text, which means that using <code>KeyphraseCountVectorizer</code> in a straightforward manner with Arabic text is not possible without workarounds (something you may have already solved but I just thought I'd mention it).</p>
",""
"75045505","2023-01-08 04:30:19","0","","75038237","<p>The problem was solved by manually giving the absolute address of the last checkpoint. The last checkpoint was recognized as wrong.</p>
",""
"75015379","2023-01-05 08:01:50","1","","75013624","<p>The task that you are using <a href=""https://github.com/facebookresearch/fairseq/blob/main/fairseq/tasks/translation_multi_simple_epoch.py"" rel=""nofollow noreferrer""><code>translation_multi_simple_epoch</code></a> does not have these arguments; they are specific for <code>translation</code> task.</p>
<p>Note that some of the arguments that you are using require values.</p>
<ul>
<li><p><code>--eval-bleu-args</code> expects a path to a configuration JSON for SacreBLEU.  If you want to you the default 4-gram BLEU, you should skip this.</p>
</li>
<li><p><code>--eval-bleu-detok</code> expects a specification of how you want to detokenize the model output. The default value is <code>space</code> which does not do anything.</p>
</li>
</ul>
<p>For more details, see the <a href=""https://fairseq.readthedocs.io/en/v0.10.2/tasks.html#translation"" rel=""nofollow noreferrer"">documentation of the translation task</a> in FairSeq.</p>
",""
"74972235","2022-12-31 21:56:55","1","","74851128","<p>I have found a way to have better results. If you sum all probabilities of all languages on different detectors like fastText and lingua, and add a dictionary-based detection for short texts, you can have very good results (for my task, I also made a fastText model trained on my data).</p>
",""
"74960186","2022-12-30 09:51:19","1","","74953747","<p>This line is the issue, the annotations are not added to the reference docs because they're not in the right format:</p>
<pre class=""lang-py prettyprint-override""><code>Example.from_dict(pred, dict.fromkeys(annotations))
</code></pre>
<p>The expected format is:</p>
<pre class=""lang-py prettyprint-override""><code>Example.from_dict(pred, {&quot;entities&quot;: [(start, end, label), (start, end, label), ...]})
</code></pre>
<p>You can also use the built-in <code>Language.evaluate</code> if you create examples where <code>Example.predicted</code> is unannotated, which also creates the scorer based on your pipeline so you don't end up a lot of irrelevant <code>None</code> scores:</p>
<pre class=""lang-py prettyprint-override""><code>Example.from_dict(nlp.make_doc(text), {&quot;entities&quot;: [(start, end, label), (start, end, label), ...]})

Once you have these kinds of examples, run:

```python
scores = ner_model.evaluate(examples)
</code></pre>
",""
"74944042","2022-12-28 19:01:07","1","","74943838","<p>You can use <a href=""https://numpy.org/doc/stable/reference/generated/numpy.char.split.html"" rel=""nofollow noreferrer""><code>np.char.split</code></a> in one line:</p>
<pre><code>df['separated'] = np.char.split(df['sentences'].tolist()).tolist()
</code></pre>
<p>@Kata if you think the <code>sentences</code> column type is str meaning the element in each row is a string instead of a list, for e.g. <code>&quot;['This is text.', 'This is another text.', 'This is also text.', 'Even more text.']&quot;</code> then you need to try to convert them into lists first. One way is to use <a href=""https://docs.python.org/3/library/ast.html#ast.literal_eval"" rel=""nofollow noreferrer""><code>ast.literal_eval</code></a>.</p>
<pre><code>from ast import literal_eval
df['sentences'] = df['sentences'].apply(literal_eval)
df['separated'] = np.char.split(df['sentences'].tolist()).tolist()
</code></pre>
<p>NOTE on data: This is not a recommended way of storing data. If possible fix the source from which data is coming. It needs to be strings in each cell not lists preferably, or at least just lists, and not a string representing list.</p>
",""
"74923600","2022-12-26 19:57:17","0","","74922924","<p>It sounds like this question is trying to ignore frequent words.</p>
<p>The <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"" rel=""nofollow noreferrer""><code>TfidfVectorizer</code></a> (<strong>not</strong> <code>TfidfTransformer</code>) implementation includes a <code>max_df</code> parameter for:</p>
<blockquote>
<p>When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words).</p>
</blockquote>
<p>In the following example, <code>word1</code> and <code>word3</code> occur in &gt;50% of documents, so setting <code>max_df=0.5</code> means the resulting array only includes <code>word2</code>:</p>
<pre class=""lang-py prettyprint-override""><code>from sklearn.feature_extraction.text import TfidfVectorizer

raw_data = [
    &quot;word1 word2 word3&quot;,
    &quot;word1 word1 word1&quot;,
    &quot;word2 word2 word3&quot;,
    &quot;word1 word1 word3&quot;,
]

vect = TfidfVectorizer(max_df=0.5)
X = vect.fit_transform(raw_data)

print(vect.get_feature_names_out())
print(X.todense())
</code></pre>
<pre><code>['word2']
[[1.]
 [0.]
 [1.]
 [0.]]
</code></pre>
",""
"74862110","2022-12-20 11:04:15","1","","74861149","<p>There is no problem in your code. The algorithm &quot;pos_tag&quot; using is the reason for the wrong output.
It shows those four word as noun:</p>
<pre><code>[('let', 'NN'), (&quot;'s&quot;, 'POS'), ('talk', 'NN'), ('to', 'TO'), ('Thomas', 'NNP'), ('and', 'CC'), ('check', 'VB'), ('if', 'IN'), ('he', 'PRP'), ('will', 'MD'), ('come', 'VB'), ('to', 'TO'), ('the', 'DT'), ('party', 'NN'), ('.', '.')]
</code></pre>
<p>You can try unigram tagging, n-gram tagging etc.
Follow this link for detailed info: <a href=""https://www.nltk.org/book/ch05.html"" rel=""nofollow noreferrer"">https://www.nltk.org/book/ch05.html</a></p>
",""
"74836612","2022-12-17 18:49:07","1","","74835558","<p>Use from this code</p>
<pre class=""lang-py prettyprint-override""><code>
df['new']=df.lemmatised.map(lambda w: len([i for i in innovation_words if i in w])&gt;1)

</code></pre>
<p>just rename the variables</p>
",""
"74706121","2022-12-06 16:41:45","2","","74705964","<p>You need to pass the tag 'v' to have the lemmatizer interpret the word as a verb. If you don't it will assume it is a noun.</p>
<pre><code>&gt;&gt;&gt; lemmatizer.lemmatize(&quot;transforming&quot;)
'transforming'
&gt;&gt;&gt; lemmatizer.lemmatize(&quot;transforming&quot;, &quot;v&quot;)
'transform'
</code></pre>
<p>There are some helpful answers for you <a href=""https://stackoverflow.com/questions/25534214/nltk-wordnet-lemmatizer-shouldnt-it-lemmatize-all-inflections-of-a-word"">here</a>.</p>
",""
"74580029","2022-11-26 05:48:22","0","","74576157","<p>For browser side NLP &amp; stemming look at the example of <a href=""https://observablehq.com/@winkjs/how-to-do-stemming-and-lemmatization?collection=@winkjs/winknlp-recipes"" rel=""nofollow noreferrer"">stemming and lemmatization</a>. This Observable notebook and the associated collection contain details of making pure browser side NLP apps.</p>
",""
"74437039","2022-11-14 19:45:10","0","","74402544","<p>Wordnet is well-documented (there is even a book). This is perhaps the docs page that most directly answers your questions:
<a href=""https://wordnet.princeton.edu/documentation/wngloss7wn"" rel=""nofollow noreferrer"">https://wordnet.princeton.edu/documentation/wngloss7wn</a></p>
<p>Also, the terms are from linguistics, not invented by the WordNet team. So dictionaries will be useful. Or you can even get meta about it, and <a href=""http://wordnetweb.princeton.edu/perl/webwn?s=hypernym&amp;sub=Search%20WordNet&amp;o2=&amp;o0=1&amp;o8=1&amp;o1=1&amp;o7=&amp;o5=&amp;o9=&amp;o6=&amp;o3=&amp;o4=&amp;h="" rel=""nofollow noreferrer"">look them up in Wordnet</a> :-)</p>
",""
"74253692","2022-10-30 14:01:37","0","","70064477","<p>Recently, I've written a patch to cure <a href=""https://issues.apache.org/jira/browse/OPENNLP-1366"" rel=""nofollow noreferrer"">OpenNLP-1366</a>. The related PR <a href=""https://github.com/apache/opennlp/pull/427"" rel=""nofollow noreferrer"">https://github.com/apache/opennlp/pull/427</a> documents the problem and solution in detail.</p>
<p>In this context, the upcoming OpenNLP version <strong>2.0.1</strong> will bring the cure for the problem reported in the OP. Updating to the aforementioned version will resolve the crashing during writing trained model files.</p>
<p><em>Note:</em> <br/>
I verified that the patch works with <code>UD_German-HDT</code>, <code>UD_German-GSD</code>, and other treebanks for the German language.</p>
",""
"74249680","2022-10-29 23:59:24","0","","74248881","<p>Here is a solution which utilises <code>nltk.RegexParser</code> with a custom grammar rule to match occurrences of any numbers of nouns, followed by a verb, followed by a noun, specifically:</p>
<pre class=""lang-none prettyprint-override""><code>{&lt;N.*&gt;+&lt;V.*&gt;&lt;N.*&gt;} 

which is equivalent to,

{&lt;NN|NNS|NNP|NNPS&gt;+&lt;VB|VBP|VBZ|VBG|VBD|VBN&gt;&lt;NN|NNS|NNP|NNPS&gt;}
</code></pre>
<h2>Example</h2>
<p>Parsing &quot;<em>Prodikos Socrates recommended Plato, and Plato recommended Aristotle</em>&quot; produces the following labelled parse tree:</p>
<p><a href=""https://i.sstatic.net/FVefn.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/FVefn.png"" alt=""nouns-verb-noun-example"" /></a></p>
<p><strong>Output:</strong></p>
<pre><code>['Prodikos', 'Socrates', 'recommended', 'Plato']
['Plato', 'recommended', 'Aristotle']
</code></pre>
<p><strong>Note:</strong> The above rule does not handle symbols and punctuation interrupting the first sequence nouns (e.g. &quot;<em>Prodikos, Socrates recommended Plato</em>&quot; will only match &quot;<em>Socrates recommended Plato</em>&quot;). There is likely a way to handle this case using some <code>regexp</code> pattern and the NLTK PoS tags but it is not immediately obvious to me.</p>
<h2>Solution</h2>
<pre class=""lang-py prettyprint-override""><code>from nltk import word_tokenize, pos_tag, RegexpParser

# Text for testing
text = &quot;Prodikos Socrates recommended Plato, and Plato recommended Aristotle&quot;

tokenized = word_tokenize(text)  # Tokenize text
tagged = pos_tag(tokenized)  # Tag tokenized text with PoS tags
print(tagged)
# Output: [('Prodikos', 'NNP'), ('Socrates', 'NNP'), ('recommended', 'VBD'), ('Plato', 'NNP'), (',', ','),
# ('and', 'CC'), ('Plato', 'NNP'), ('recommended', 'VBD'), ('Aristotle', 'NNP')]

# Create custom grammar rule to label occurrences of any number of nouns, followed by a verb, followed by a noun
my_grammar = r&quot;&quot;&quot;
NOUNS_VERB_NOUN: {&lt;N.*&gt;+&lt;V.*&gt;&lt;N.*&gt;}&quot;&quot;&quot;


# Function to create parse tree using custom grammar rules and PoS tagged text
def get_parse_tree(grammar, pos_tagged_text):
    cp = RegexpParser(grammar)
    parse_tree = cp.parse(pos_tagged_text)
    parse_tree.draw()  # Visualise parse tree
    return parse_tree


# Function to get labels from custom grammar:
# takes line separated NLTK regexp grammar rules
def get_labels_from_grammar(grammar):
    labels = []
    for line in grammar.splitlines()[1:]:
        labels.append(line.split(&quot;:&quot;)[0])
    return labels


# Function takes parse tree &amp; list of NLTK custom grammar labels as input
# Returns phrases which match
def get_phrases_using_custom_labels(parse_tree, custom_labels_to_get):
    matching_phrases = []
    for node in parse_tree.subtrees(filter=lambda x: any(x.label() == custom_l for custom_l in custom_labels_to_get)):
        # Get phrases only, drop PoS tags
        matching_phrases.append([leaf[0] for leaf in node.leaves()])
    return matching_phrases


text_parse_tree = get_parse_tree(my_grammar, tagged)
my_labels = get_labels_from_grammar(my_grammar)
phrases = get_phrases_using_custom_labels(text_parse_tree, my_labels)

for phrase in phrases:
    print(phrase)
# Output:
# ['Prodikos', 'Socrates', 'recommended', 'Plato']
# ['Plato', 'recommended', 'Aristotle']
</code></pre>
",""
"74230698","2022-10-28 04:35:24","0","","74146965","<h1>TL;DR</h1>
<p>(<a href=""https://discuss.huggingface.co/t/how-to-efficiently-convert-a-large-parallel-corpus-to-a-huggingface-dataset-to-train-an-encoderdecodermodel/24788?u=alvations"" rel=""nofollow noreferrer"">Answer's credits goes to @lhoestq</a>)</p>
<p>If you have a TSV file that looks like this:</p>
<pre><code>hello world\tHallo Welt
how are you?\twie gehts?
...\t...
</code></pre>
<p>load the dataset as such:</p>
<pre><code># tatoeba-sentpairs.tsv is a pretty large file.
ds = load_dataset(&quot;csv&quot;, data_files=&quot;../input/tatoeba/tatoeba-sentpairs.tsv&quot;, 
                  streaming=True, delimiter=&quot;\t&quot;, split=&quot;train&quot;)

</code></pre>
<hr />
<h1>In Long</h1>
<h4>Reason not to use parquet, run map functions and save the outputs:</h4>
<ul>
<li>Loading a large dataset into parquet is already quite a feat, in the thread, see <strong>Step 1</strong> in question, so lets avoid that</li>
<li>Mapping the data into the BERT format, i.e. <code>munge_dataset_to_pacify_bert</code> is also quite expensive operation. If that is done for 1B lines and even if it's thread-parallelized, it will take hours to days to complete</li>
<li>The resulting tensors that are saved with <code>dataset.set_format(type=&quot;torch&quot;)</code> is massive, a ~50GB of tsv with 1B lines will easily become TBs of binaries.</li>
</ul>
<h4>Instead, use stream-style processing,</h4>
<p>Huggingface <code>datasets</code> supports it with <code>stream=True</code> when defining the dataset:</p>
<pre><code>ds = load_dataset(&quot;csv&quot;, data_files=&quot;../input/tatoeba/tatoeba-sentpairs.tsv&quot;, 
                  streaming=True, delimiter=&quot;\t&quot;, split=&quot;train&quot;)
</code></pre>
",""
"74201199","2022-10-25 23:35:46","4","","74196558","<p>I've created a <code>get_phrases_using_tense_label()</code> function which takes:</p>
<ul>
<li>the parse tree returned from your <code>check_grammar()</code> function (I've renamed it to <code>get_parse_tree()</code> as this is more meaningful in terms of what the function is doing), and</li>
<li>a list of tense labels based on your grammar.</li>
</ul>
<p>The tense labels are retrieved using the <code>get_labels_from_grammar()</code> function I created, which iterates over the lines in your grammar and splits the string at the &quot;:&quot; retrieving the tense label.</p>
<p>The function then returns the list of phrases (along with their tags) for those nodes in the NLTK tree which match any of your <code>tense_labels</code> (e.g. &quot;Present_Indefinite&quot; and Present_Perfect&quot; in the solution below). I've used a smaller text as input as an example.</p>
<p><a href=""https://i.sstatic.net/SyFFC.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/SyFFC.png"" alt=""Parse tree with multiple tense labels"" /></a></p>
<h2>Solution</h2>
<pre class=""lang-py prettyprint-override""><code>from nltk import word_tokenize, pos_tag
import nltk

text = &quot;#NOVAVAX produces #NUVAXOVID vaccine.\
 Will that provide a new rally? We see Biotechnology\
  Stock $NVAX Entering the Buying Area.&quot;

# Smaller text for testing
textSmall = &quot;We see a surge in sales. It has been a great year.&quot;

tokenized = word_tokenize(textSmall)  # Tokenize text
tagged = pos_tag(tokenized)  # Tag tokenized text with PoS tags

my_grammar = r&quot;&quot;&quot;
Future_Perfect_Continuous: {&lt;MD&gt;&lt;VB&gt;&lt;VBN&gt;&lt;VBG&gt;}
Future_Continuous:         {&lt;MD&gt;&lt;VB&gt;&lt;VBG&gt;}
Future_Perfect:            {&lt;MD&gt;&lt;VB&gt;&lt;VBN&gt;}
Past_Perfect_Continuous:   {&lt;VBD&gt;&lt;VBN&gt;&lt;VBG&gt;}
Present_Perfect_Continuous:{&lt;VBP|VBZ&gt;&lt;VBN&gt;&lt;VBG&gt;}
Future_Indefinite:         {&lt;MD&gt;&lt;VB&gt;}
Past_Continuous:           {&lt;VBD&gt;&lt;VBG&gt;}
Past_Perfect:              {&lt;VBD&gt;&lt;VBN&gt;}
Present_Continuous:        {&lt;VBZ|VBP&gt;&lt;VBG&gt;}
Present_Perfect:           {&lt;VBZ|VBP&gt;&lt;VBN&gt;}
Past_Indefinite:           {&lt;VBD&gt;}
Present_Indefinite:        {&lt;VBZ&gt;|&lt;VBP&gt;}&quot;&quot;&quot;


def get_parse_tree(grammar, pos_tagged_text):
    cp = nltk.RegexpParser(grammar)
    parse_tree = cp.parse(pos_tagged_text)
    # parse_tree.draw()  # Visualise parse tree
    return parse_tree


# Function to get labels from grammar:
# takes line separated NLTK regexp grammar rules
def get_labels_from_grammar(grammar):
    labels = []
    for line in grammar.splitlines()[1:]:
        labels.append(line.split(&quot;:&quot;)[0])
    return labels


# Function takes parse tree &amp; list of NLTK custom grammar labels as input
# Returns phrases which match
def get_phrases_using_tense_labels(parse_tree, tense_labels_to_get):
    matching_phrases = []
    for node in parse_tree.subtrees(filter=lambda x: any(x.label() == tense_lab for tense_lab in tense_labels_to_get)):
        matching_phrases.append(node.leaves()[0])
    return matching_phrases


# Function takes parse tree &amp; list of NLTK custom grammar labels as input
# Returns the tense labels present in the parse tree
def get_tense_labels_in_tree(parse_tree, tense_labels_to_get):
    matching_labels = []
    for node in parse_tree.subtrees(filter=lambda x: any(x.label() == tense_lab for tense_lab in tense_labels_to_get)):
        matching_labels.append(node.label())
    return matching_labels


text_parse_tree = get_parse_tree(my_grammar, tagged)
# print(text_parse_tree)  # View parse tree output
tense_labels = get_labels_from_grammar(my_grammar)
phrases = get_phrases_using_tense_labels(text_parse_tree, tense_labels)
labels = get_tense_labels_in_tree(text_parse_tree, tense_labels)

print(phrases)
# Output: [('see', 'VBP'), ('has', 'VBZ')]
print([phrase[0] for phrase in phrases])
# Output: ['see', 'has']
print(labels)
# ['Present_Perfect', 'Present_Indefinite']
</code></pre>
",""
"74193846","2022-10-25 12:08:51","0","","74181750","<h1>Checklist</h1>
<p>The following checklist is focused on runtime performance optimization and not training (i.e. when one utilises existing <code>config.cfg</code> files loaded with the convenience wrapper <code>spacy.load()</code>, instead of training their own models and creating a new <code>config.cfg</code> file), however, most of the points still apply. This list is not comprehensive: the spaCy library is extensive and there are many ways to build pipelines and carry out tasks. Thus, including all cases here is impractical, regardless, this list intends to be a handy reference and starting point.</p>
<h2>Summary</h2>
<ol>
<li>If more powerful hardware is available, use it.</li>
<li>Use (optimally) small models/pipelines.</li>
<li>Use your GPU if possible.</li>
<li>Process large texts as a stream and buffer them in batches.</li>
<li>Use multiprocessing (if appropriate).</li>
<li>Use only necessary pipeline components.</li>
<li>Save and load progress to avoid re-computation.</li>
</ol>
<h2>1. <strong>If more powerful hardware is available, use it.</strong></h2>
<p><strong>CPU</strong>. Most of spaCy's work at runtime is going to be using CPU instructions to allocate memory, assign values to memory and perform computations, which, in terms of speed, will be CPU bound not RAM, hence, <em>performance is predominantly dependent on the CPU</em>. So, opting for a better CPU as opposed to more RAM is the smarter choice in most situations. As a general rule, newer CPUs with higher frequencies, more cores/threads, more cache etc. will realise faster spaCy processing times. However, simply comparing these numbers between different CPU architectures is not useful. Instead look at benchmarks like <a href=""https://cpu.userbenchmark.com/"" rel=""noreferrer"">cpu.userbenchmark.com</a> (e.g. <a href=""https://cpu.userbenchmark.com/Compare/Intel-Core-i5-12600K-vs-AMD-Ryzen-9-5900X/4120vs4087"" rel=""noreferrer"">i5-12600k vs. Ryzen 9 5900X</a>) and compare the single-core and multi-core performance of prospective CPUs to find those that will likely offer better performance. See Footnote (1) on hyperthreading &amp; core/thread counts.</p>
<p><strong>RAM</strong>. The practical consideration for RAM is the size: larger texts require more memory capacity, speed and latency is less important. If you have limited RAM capacity, disable <code>NER</code> and <code>parser</code> when creating your <code>Doc</code> for large input text (e.g. <code>doc = nlp(&quot;My really long text&quot;, disable = ['ner', 'parser'])</code>). If you require these parts of the pipeline, you'll only be able to process approximately <code>100,000 * available_RAM_in_GB</code> characters at a time, if you don't, you'll be able to process more than this. Note that the default spaCy input text limit is 1,000,000 characters, however this can be changed by setting <code>nlp.max_length = your_desired_length</code>.</p>
<p><strong>GPU</strong>. If you opt to use a GPU, processing times can be improved for certain aspects of the pipeline which make use of GPU-based computations. See the section below on <em>making use of your GPU</em>. The same general rule as with CPUs applies here too: generally, newer GPUs with higher frequencies, more memory, larger memory bus widths, bigger bandwidth etc. will realise faster spaCy processing times.</p>
<p><strong>Overclocking</strong>. If you're experienced with overclocking and have the correct hardware to be able to do it (adequate power supply, cooling, motherboard chipset), then another effective way to gain extra performance without changing hardware is to overclock your CPU/GPU.</p>
<h2>2. Use (optimally) small models/pipelines.</h2>
<p>When computation resources are limited, and/or accuracy is less of a concern (e.g. when experimenting or testing ideas), load spaCy pipelines that are efficiency focused (i.e. those with smaller models). For example:</p>
<pre class=""lang-py prettyprint-override""><code># Load a &quot;smaller&quot; pipeline for faster processing
nlp = spacy.load(&quot;en_core_web_sm&quot;)
# Load a &quot;larger&quot; pipeline for more accuracy
nlp = spacy.load(&quot;en_core_web_trf&quot;)
</code></pre>
<p>As a <a href=""https://spacy.io/usage/facts-figures#benchmarks-speed"" rel=""noreferrer"">concrete example</a> of the differences, on the same system, the smaller <code>en_core_web_lg</code> pipeline is able to process 10,014 words per second, whereas the <code>en_core_web_trf</code> pipeline only processes 684. Remember that there is often a trade-off between speed and accuracy.</p>
<h2>3. Use your GPU if possible.</h2>
<p>Due to the nature of neural network-based models, their computations can be efficiently solved using a GPU, leading to boosts in processing times. <a href=""https://spacy.io/usage/facts-figures#benchmarks-speed"" rel=""noreferrer"">For instance</a>, the <code>en_core_web_lg</code> pipeline can process 10,014 vs. 14,954 words per second when using a CPU vs. a GPU.</p>
<p>spaCy can be installed for a CUDA compatible GPU (i.e. Nvidia GPUs) by calling <code>pip install -U spacy[cuda]</code> in the command prompt. Once a GPU-enabled spaCy installation is present, one can call <code>spacy.prefer_gpu()</code> or <code>spacy.require_gpu()</code> somewhere in your program before any pipelines have been loaded. Note that <code>require_gpu()</code> will raise an error if no GPU is available. For example:</p>
<pre class=""lang-py prettyprint-override""><code>spacy.prefer_gpu() # Or use spacy.require_gpu()
nlp = spacy.load(&quot;en_core_web_sm&quot;)
</code></pre>
<h2>4. Process large texts as a stream and buffer them in batches.</h2>
<p>When processing large volumes of text, the statistical models are usually more efficient if you let them work on batches of texts (default is 1000), and process the texts as a stream using <code>nlp.pipe()</code>. For example:</p>
<pre class=""lang-py prettyprint-override""><code>texts = [&quot;One document.&quot;, &quot;...&quot;, &quot;Lots of documents&quot;]
nlp = spacy.load(&quot;en_core_web_sm&quot;)
docs = list(nlp.pipe(texts, batch_size=1000))
</code></pre>
<h2>5. Use multiprocessing (if appropriate).</h2>
<p>To make use of multiple CPU cores, spaCy includes built-in support for multiprocessing with <code>nlp.pipe()</code> using the <code>n_process</code> option. For example,</p>
<pre class=""lang-py prettyprint-override""><code>texts = [&quot;One document.&quot;, &quot;...&quot;, &quot;Lots of documents&quot;]
nlp = spacy.load(&quot;en_core_web_sm&quot;)
docs = list(nlp.pipe(texts, n_process=4))
</code></pre>
<p><em>Note</em> that each process requires its own memory. This means that every time a new process is <code>spawned</code> (the default start method), model data has to be copied into memory for every individual process (hence, the larger the model, the more overhead to spawn a process). Therefore, it is recommended that if you are just doing small tasks, that you increase the batch size and use fewer processes. For example,</p>
<pre class=""lang-py prettyprint-override""><code>texts = [&quot;One document.&quot;, &quot;...&quot;, &quot;Lots of documents&quot;]
nlp = spacy.load(&quot;en_core_web_sm&quot;)
docs = list(nlp.pipe(texts, n_process=2, batch_size=2000)) # default batch_size = 1000
</code></pre>
<p>Finally, multiprocessing is generally not recommended on GPUs because RAM is limited.</p>
<h2>6. Use only necessary pipeline components.</h2>
<p>Generating predictions from models in the pipeline that you don't require unnecessarily degrades performance. One can prevent this by either <em>disabling</em> or <em>excluding</em> specific components, either when loading a pipeline (i.e. with <code>spacy.load()</code>) or during processing (i.e. with <code>nlp.pipe()</code>).</p>
<p>If you have limited memory, <code>exclude</code> the components you don't need, for example:</p>
<pre class=""lang-py prettyprint-override""><code># Load the pipeline without the entity recognizer
nlp = spacy.load(&quot;en_core_web_sm&quot;, exclude=[&quot;ner&quot;])
</code></pre>
<p>If you might need a particular component later in your program, but still want to improve processing speed for tasks that don't require those components in the interim, use <code>disable</code>, for example:</p>
<pre class=""lang-py prettyprint-override""><code># Load the tagger but don't enable it
nlp = spacy.load(&quot;en_core_web_sm&quot;, disable=[&quot;tagger&quot;])
# ... perform some tasks with the pipeline that don't require the tagger
# Eventually enable the tagger
nlp.enable_pipe(&quot;tagger&quot;)
</code></pre>
<p><em>Note</em> that the <code>lemmatizer</code> depends on <code>tagger</code>+<code>attribute_ruler</code> or <code>morphologizer</code> for a number of languages. If you disable any of these components, you‚Äôll see lemmatizer warnings unless the lemmatizer is also disabled.</p>
<h2>7. Save and load progress to avoid re-computation.</h2>
<p>If one has been modifying the pipeline or vocabulary, made updates to model components, processed documents etc., there is merit in saving one's progress to reload at a later date. This requires one to translate the contents/structure of an object into a format that can be saved -- a process known as <code>serialization</code>.</p>
<h3>Serializing the pipeline</h3>
<pre class=""lang-py prettyprint-override""><code>nlp = spacy.load(&quot;en_core_web_sm&quot;)
# ... some changes to pipeline
# Save serialized pipeline
nlp.to_disk(&quot;./en_my_pipeline&quot;)
# Load serialized pipeline
nlp.from_disk(&quot;./en_my_pipeline&quot;)
</code></pre>
<h3>Serializing multiple <code>Doc</code> objects</h3>
<p>The <code>DocBin</code> class provides an easy method for serializing/deserializing multiple <code>Doc</code> objects, which is also more efficient than calling <code>Doc.to_bytes()</code> on every <code>Doc</code> object. For example:</p>
<pre class=""lang-py prettyprint-override""><code>from spacy.tokens import DocBin
texts = [&quot;One document.&quot;, &quot;...&quot;, &quot;Lots of documents&quot;]
nlp = spacy.load(&quot;en_core_web_sm&quot;)
docs = list(nlp.pipe(texts))
doc_bin = DocBin(docs=docs)
# Save the serialized DocBin to a file
doc_bin.to_disk(&quot;./data.spacy&quot;)
# Load a serialized DocBin from a file
doc_bin = DocBin().from_disk(&quot;./data.spacy&quot;)
</code></pre>
<h3>Footnotes</h3>
<p>(1) &quot;Hyper-threading&quot; is a term trademarked by Intel used to refer to their proprietary Simultaneous Multi-Threading (SMT) implementation that improves parallelisation of computations (i.e. doing multiple tasks at once). AMD has SMT as well, it just doesn't have a fancy name. In short, processors with 2-way SMT (SMT-2) allow an Operating System (OS) to treat each physical core on the processor as two cores (referred to as &quot;virtual cores&quot;). Processors with SMT will perform better on tasks that can make use of these multiple &quot;cores&quot;, sometimes referred to as &quot;threads&quot; (e.g. the Ryzen 5600X is an 6 core/12 thread processor (i.e. 6 physical cores, but with SMT-2, it has 12 &quot;virtual cores&quot; or &quot;threads&quot;)). Note that Intel has recently released a CPU architecture with e-cores, which are cores that don't have hyper-threading, despite other cores on the processor (namely, p-cores) having it, hence you will see some chips like the i5-12600k that have 10 cores with hyper-threading, but it has 16 threads not 20. This is because only the 6 p-cores have hyper-threading, while the 4 e-cores do not, hence 16 threads total.</p>
",""
"74187846","2022-10-24 23:45:24","0","","52557058","<p>For a more recent answer, the pipeline design as of spaCy v3.4 is explained on the spaCy site <a href=""https://spacy.io/models#design"" rel=""nofollow noreferrer"">here</a>. I've reproduced some important parts below should the link become invalid.</p>
<blockquote>
<p>The spaCy v3 trained pipelines are designed to be efficient and
configurable. For example, multiple components can share a common
‚Äútoken-to-vector‚Äù model and it‚Äôs easy to swap out or disable the
lemmatizer. The pipelines are designed to be efficient in terms of
speed and size and work well when the pipeline is run in full.</p>
<p>When modifying a trained pipeline, it‚Äôs important to understand how
the components depend on each other. Unlike spaCy v2, where the
tagger, parser and ner components were all independent, some v3
components depend on earlier components in the pipeline. As a result,
disabling or reordering components can affect the annotation quality
or lead to warnings and errors.</p>
</blockquote>
<h2>CNN/CPU pipeline design</h2>
<p><a href=""https://i.sstatic.net/AnK4z.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/AnK4z.png"" alt=""spaCy v3 pipeline"" /></a></p>
<blockquote>
<p>In the <code>sm/md/lg</code> models:</p>
<ul>
<li>The <code>tagger</code>, <code>morphologizer</code> and <code>parser</code> components listen to the <code>tok2vec</code> component. If the lemmatizer is trainable (v3.3+),
<code>lemmatizer</code> also listens to <code>tok2vec</code>.</li>
<li>The <code>attribute_ruler</code> maps <code>token.tag</code> to <code>token.pos</code> if there is no <code>morphologizer</code>. The <code>attribute_ruler</code> additionally makes sure
whitespace is tagged consistently and copies <code>token.pos</code> to
<code>token.tag</code> if there is no tagger. For English, the attribute ruler
can improve its mapping from <code>token.tag</code> to <code>token.pos</code> if dependency
parses from a <code>parser</code> are present, but the parser is not required.</li>
<li>The <code>lemmatizer</code> component for many languages requires <code>token.pos</code> annotation from either <code>tagger</code>+<code>attribute_ruler</code> or <code>morphologizer</code>.
The <code>ner</code> component is independent with its own internal <code>tok2vec</code>
layer.</li>
</ul>
</blockquote>
<h2>Transformer pipeline design</h2>
<blockquote>
<p>In the transformer (<code>trf</code>) models, the <code>tagger</code>, <code>parser</code> and <code>ner</code>
(if present) all listen to the <code>transformer</code> component. The
<code>attribute_ruler</code> and <code>lemmatizer</code> have the same configuration as in
the CNN models.</p>
</blockquote>
",""
"74162147","2022-10-22 08:42:08","8","","74161769","<p>You can do this by making a <a href=""https://docs.python.org/3/library/collections.html#collections.Counter"" rel=""nofollow noreferrer""><code>Counter</code></a> from the words in each <code>Text</code> value, then converting that into columns (using <code>pd.Series</code>), summing the columns that don't exist in <code>wordlist</code> into <code>other_words</code> and then dropping those columns:</p>
<pre class=""lang-py prettyprint-override""><code>import re
import pandas as pd
from collections import Counter

wordlist = list(map(str.lower, wordlist))
counters = df['Text'].apply(lambda t:Counter(re.findall(r'\b[a-z]+\b', t.lower())))
df = pd.concat([df, counters.apply(pd.Series).fillna(0).astype(int)], axis=1)
other_words = list(set(df.columns) - set(wordlist) - { 'No', 'Text' })
df['other_words'] = df[other_words].sum(axis=1) 
df = df.drop(other_words, axis=1)
</code></pre>
<p>Output (for the sample data in your question):</p>
<pre><code>   No                                 Text  i  love  other_words
0   1    I love you forever... other words  1     1            4
1   2  No , i know that you know xxx words  1     0            7
</code></pre>
<p>Note:</p>
<ul>
<li>I've converted all the words to lower-case so you're not counting <code>I</code> and <code>i</code> separately.</li>
<li>I've used <code>re.findall</code> rather than the more obvious <code>split()</code> so that <code>forever...</code> gets counted as the word <code>forever</code> rather than <code>forever...</code></li>
</ul>
<p>If you only want to count the words in <code>wordlist</code> (and don't want an <code>other_words</code> count), you can simplify this to:</p>
<pre><code>wordlist = list(map(str.lower, wordlist))
counters = df['Text'].apply(lambda t:Counter(w for w in re.findall(r'\b[a-z]+\b', t.lower()) if w in wordlist))
df = pd.concat([df, counters.apply(pd.Series).fillna(0).astype(int)], axis=1)
</code></pre>
<p>Output:</p>
<pre><code>   No                                 Text  i  love
0   1    I love you forever... other words  1     1
1   2  No , i know that you know xxx words  1     0
</code></pre>
<p>Another way of also generating the <code>other_words</code> value is to generate 2 sets of counters, one of all the words, and one only of the words in <code>wordlist</code>. These can then be subtracted from each other to find the count of words in the text which are not in the wordlist:</p>
<pre class=""lang-py prettyprint-override""><code>wordlist = list(map(str.lower, wordlist))
counters = df['Text'].apply(lambda t:Counter(w for w in re.findall(r'\b[a-z]+\b', t.lower()) if w in wordlist))
df = pd.concat([df, counters.apply(pd.Series).fillna(0).astype(int)], axis=1)
c2 = df['Text'].apply(lambda t:Counter(re.findall(r'\b[a-z]+\b', t.lower())))
df['other_words'] = (c2 - counters).apply(lambda d:sum(d.values()))
</code></pre>
<p>Output of this is the same as for the first code sample. Note that in Python 3.10 and later, you should be able to use the new <code>total</code> function:</p>
<pre class=""lang-py prettyprint-override""><code>(c2 - counters).apply(Counter.total)
</code></pre>
",""
"74055927","2022-10-13 12:38:18","0","","74052776","<p>After using multiple search engines, including academic ones to perhaps try and find research papers covering topics pertaining to <em>Spanish word gender detection</em> and other related terms, there seems to be no one that has tackled the problem and implemented a solution in a modern library.</p>
<p>Regardless, you can still tackle the problem by running a Spanish Part of Speech (PoS) tagger (for example, <a href=""https://huggingface.co/mrm8488/RuPERTa-base-finetuned-pos"" rel=""nofollow noreferrer"">RuPERTa-base (Spanish RoBERTa) + POS</a>) to detect nouns/pronouns, combine those labels with your NER output where required, and then write your own rules for determining the gender of particular nouns/pronouns based on Spanish grammar rules (such as those detailed in <a href=""https://doi.org/10.4324/9781315648446"" rel=""nofollow noreferrer"">A New Reference Grammar of Modern Spanish</a>, specifically <a href=""https://www.taylorfrancis.com/chapters/mono/10.4324/9781315648446-1/gender-nouns-john-butt-carmen-benjamin-antonia-moreira-rodr%C3%ADguez?context=ubx&amp;refId=9e494cba-d5f8-40f3-96b6-c2951de6352a"" rel=""nofollow noreferrer"">Chapter 1</a>  <em>Gender of nouns</em>).</p>
<p>Hopefully that helps give you some direction if you don't end up finding a ready-made implementation.</p>
",""
"73976396","2022-10-06 15:27:35","1","","73976285","<p>if pos_token is a list values then try this;</p>
<pre><code>df = pd.DataFrame({&quot;pos_token&quot;:[[(&quot;No&quot;, &quot;DT&quot;), (&quot;you&quot;, &quot;PRP&quot;), (&quot;lying&quot;, &quot;VBG&quot;)]]})

df[&quot;sentence&quot;] = df[&quot;pos_token&quot;].apply(lambda x: &quot; &quot;.join([i[0] for i in x]))

#  output
                              pos_token      sentence
0  [(No, DT), (you, PRP), (lying, VBG)]  No you lying
</code></pre>
",""
"73876748","2022-09-28 05:55:25","4","","73869397","<p>For pretrained pipelines, you can check the labels on the <a href=""https://spacy.io/models/de"" rel=""nofollow noreferrer"">model page</a>, under the &quot;Label Scheme&quot; entry.</p>
<p>If your pipeline has a tagger, like the German one does, you can do this:</p>
<pre><code>tagger = nlp.get_pipe(&quot;tagger&quot;)
print(tagger.labels)
</code></pre>
",""
"73851179","2022-09-26 08:04:23","3","","73807176","<p>The spacy core library does not do any hyperparameter tuning. For <code>spacy train</code>, the dev data is used for the evaluation displayed during training, to select the best model, and for early stopping (the early stopping setting is called <code>patience</code>).</p>
",""
"73835949","2022-09-24 08:59:39","0","","73835778","<p>You can try with SequenceMatcher;</p>
<pre><code>from difflib import SequenceMatcher

C1 = 'FISSEN Ltds'
C2 = 'FISSEN Ltds Maschinen- und Werkzeugbau'
C3 = 'V.R.P. Baumaschinen Ltds'

print(SequenceMatcher(None, C1, C2).ratio())
print(SequenceMatcher(None, C2, C3).ratio())
print(SequenceMatcher(None, C1, C3).ratio())

# Output -&gt;
# 0.4489795918367347
# 0.3548387096774194
# 0.2857142857142857
</code></pre>
<p>Hope this Helps...</p>
",""
"73458984","2022-08-23 12:44:47","2","","72804704","<p>There is no easy solution for my specific problem: if you are using a fastText embedding as a feature extractor, and then you want to use a compressed version of this embedding, you have to retrain the final classifier, since produced vectors are somewhat different.</p>
<p>Anyway, I want to give a general answer for</p>
<h1>fastText models reduction</h1>
<p><a href=""https://i.sstatic.net/iy8RO.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/iy8RO.jpg"" alt=""enter image description here"" /></a></p>
<h2>Unsupervised models (=embeddings)</h2>
<p>You are using pretrained embeddings provided by Facebook or you trained your embeddings in an unsupervised fashion. Format .bin. Now you want to reduce model size/memory consumption.</p>
<p><strong>Straight-forward solutions:</strong></p>
<ul>
<li><p><a href=""https://github.com/avidale/compress-fasttext"" rel=""nofollow noreferrer"">compress-fasttext library</a>: compress fastText word embedding models by orders of magnitude, without significantly affecting their quality; there are also available several pretrained compressed models (other interesting compressed models <a href=""https://zenodo.org/record/4905385#.YwTKHVVBxhE"" rel=""nofollow noreferrer"">here</a>).</p>
</li>
<li><p><a href=""https://fasttext.cc/docs/en/crawl-vectors.html#adapt-the-dimension"" rel=""nofollow noreferrer"">fastText native <code>reduce_model</code></a>: in this case, you are reducing vector dimension (eg from 300 to 100), so you are explictly losing expressiveness; under the hood, this method employs PCA.</p>
</li>
</ul>
<p><strong>If you have training data</strong> and can perform retraining, you can use <a href=""https://github.com/explosion/floret"" rel=""nofollow noreferrer"">floret</a>, a fastText fork by explosion (the company of Spacy), that uses a more compact representation for vectors.</p>
<p><strong>If you are not interested in fastText ability to represent out-of-vocabulary words</strong> (words not seen during training), you can use .vec file (containing only vectors and not model weights) and select only a portion of the most common vectors (eg the first 200k words/vectors). If you need a way to convert .bin to .vec, read this <a href=""https://stackoverflow.com/questions/58337469/how-to-save-fasttext-model-in-vec-format/58342618#58342618"">answer</a>.
Note: gensim package fully supports fastText embedding (unsupervised mode), so these operations can be done through this library (more details in this <a href=""https://stackoverflow.com/questions/59282572/memory-efficiently-loading-of-pretrained-word-embeddings-from-fasttext-library-w"">answer</a>)</p>
<h2>Supervised models</h2>
<p>You used fastText to train a classifier, producing a .bin model. Now you want to reduce classifier size/memory consumption.</p>
<ul>
<li>The best solution is <a href=""https://fasttext.cc/blog/2017/10/02/blog-post.html#model-compression"" rel=""nofollow noreferrer"">fastText native <code>quantize</code></a>: the model is retrained applying weights quantization and feature selection. With the <code>retrain</code> parameter, you can decide whether to fine-tune the embeddings or not.</li>
<li>You can still use <a href=""https://fasttext.cc/docs/en/crawl-vectors.html#adapt-the-dimension"" rel=""nofollow noreferrer"">fastText <code>reduce_model</code></a>, but it leads to less expressive models and the size of the model is not heavily reduced.</li>
</ul>
",""
"73447772","2022-08-22 15:44:32","1","","73436826","<p>We faced this issue yesterday as well, what fixed it for us was setting the TPU software version as <code>tpu-vm-base</code> when provisioning the TPU node.</p>
",""
"73397004","2022-08-18 03:05:20","0","","73059189","<p>I've done a little more digging, and think I've finally discovered the root of the problem, and it has everything to do with what's described <a href=""https://www.pythonforthelab.com/blog/differences-between-multiprocessing-windows-and-linux/"" rel=""nofollow noreferrer"">here</a>.</p>
<p>To summarize, on Linux systems, processes are forked from the main process, meaning that the current process state is copied (which is why the <code>import</code> statements don't run multiple times). On Windows (and macOS), processes are spawned, meaning that interpreter starts at the beginning of the &quot;main&quot; file, thus running all <code>import</code> statements again. So, the behavior I'm seeing is not a bug, but I will need to rethink my program design to account for this.</p>
",""
"73290470","2022-08-09 10:42:22","5","","73290224","<p>As I am looking into pytorch_lightning github, I do not see <code>checkpoint_callback</code> variable in <strong>init</strong> (<a href=""https://github.com/Lightning-AI/lightning/blob/master/src/pytorch_lightning/trainer/trainer.py"" rel=""nofollow noreferrer"">https://github.com/Lightning-AI/lightning/blob/master/src/pytorch_lightning/trainer/trainer.py</a>)</p>
<p>Are you sure thats how it's called? What do you want to achieve by passing this <code>checkpoint_callback</code>?</p>
<p>//edit:
I think you just have to append <code>checkpoint_callback</code> to <code>callbacks</code> list</p>
",""
"73234634","2022-08-04 10:51:54","3","","73232595","<p>You could just comment the <code>metric_for_best_model='f1'</code> part out and see for yourself, loss is the default setting. Or, utilize <code>from_pretrained('path/to/checkpoint')</code> to compare two checkpoints back to back. F-score is threshold sensitive, so it's entirely possible for a lower loss checkpoint to be better in the end (assuming you do optimize the threshold).</p>
",""
"73229182","2022-08-04 00:37:13","4","","73228267","<p>It would help if the example were more reproducible next time. It took a bit to re-create this. No worries, though,I have a solution here.</p>
<p>First, <code>cloudpickle</code> is the mechanism of Spark to move a function from drivers  to workers. So functions are pickled and then sent to the workers for execution. So something you are using can't be pickled. In order to quickly test, you can just use:</p>
<pre><code>import cloudpickle
cloudpickle.dumps(x)
</code></pre>
<p>where x is something that you are testing if it's cloudpickle-able. In this case, I tried a couple of times and found <code>wordnet</code> not to be serializable. You can test with:</p>
<pre><code>cloudpickle.dumps(wordnet)
</code></pre>
<p>and it will reproduce the issue. You can get around this by importing the stuff that can't be pickled inside your function. Here is an end-to-end example for you.</p>
<pre class=""lang-py prettyprint-override""><code>import re
import pandas as pd
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import string
from nltk.tokenize import word_tokenize
from nltk.corpus import wordnet
from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from pyspark.sql.functions import udf
from pyspark.sql.types import ArrayType,IntegerType,StringType

def preprocess(text):
    text = text.lower() 
    text=text.strip()  
    text=re.compile('&lt;.*?&gt;').sub('', text) 
    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)  
    text = re.sub('\s+', ' ', text)  
    text = re.sub(r'\[[0-9]*\]',' ',text) 
    text=re.sub(r'[^\w\s]', '', text.lower().strip())
    text = re.sub(r'\d',' ',text) 
    text = re.sub(r'\s+',' ',text) 
    return text


#LEMMATIZATION
# Initialize the lemmatizer
wl = WordNetLemmatizer()

stop_words = set(stopwords.words('english'))
def remove_stopwords(text):
    text = [i for i in text.split() if not i in stop_words]
    return text
 
def lemmatizer(string):
    from nltk.corpus import wordnet
    def get_wordnet_pos(tag):
        if tag.startswith('J'):
            return wordnet.ADJ
        elif tag.startswith('V'):
            return wordnet.VERB
        elif tag.startswith('N'):
            return wordnet.NOUN
        elif tag.startswith('R'):
            return wordnet.ADV
        else:
            return wordnet.NOUN
    word_pos_tags = nltk.pos_tag(word_tokenize(string)) # Get position tags
    a=[wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)] # Map the position tag and lemmatize the word/token
    return &quot; &quot;.join(a)

#Final Function
def finalpreprocess(string):
    return lemmatizer(' '.join(remove_stopwords(preprocess(string))))

spark = SparkSession.builder.getOrCreate()
text = 'Ram and Bheem are buddies. They (both) like &lt;b&gt;running&lt;/b&gt;. They got better at it over the weekend'
test = pd.DataFrame({&quot;test&quot;: [text]})
sdf = spark.createDataFrame(test)
udf_txt_clean = udf(lambda x: finalpreprocess(x),StringType())
sdf.withColumn(&quot;cleaned_text&quot;,udf_txt_clean(col(&quot;test&quot;))).select(&quot;test&quot;,&quot;cleaned_text&quot;).show(10,False)
</code></pre>
",""
"73022878","2022-07-18 13:12:47","1","","73017872","<pre><code>def fit(whole_data):
    def IDF(whole_data, unique_words):
        idf_dict = {}
        N = len(whole_data)
        for i in unique_words:
            count = 0
            for sen in whole_data:
                if i in sen.split():
                    count = count+1
                idf_dict[i] = (math.log((1 + N) / (count+1))) + 1
        return idf_dict

    unique_words = set()

    if isinstance(whole_data, (list,)):
        for x in whole_data:
            for y in x.split():
                if len(y) &lt; 2:
                    continue
                unique_words.add(y)
        unique_words = sorted(list(unique_words))
        vocab = {j: i for i, j in enumerate(unique_words)}

        Idf_values_of_all_unique_words = IDF(whole_data, unique_words)
    return vocab, Idf_values_of_all_unique_words

vocabulary, idf_of_vocabulary = fit(corpus)
</code></pre>
<p>just like that!</p>
",""
"73017360","2022-07-18 04:33:33","0","","73015102","<p>You can do it like this:</p>
<ul>
<li>define <code>analyzer='char'</code> so that TfidfVectorizer works with the letters;</li>
<li>find the index of <code>d</code> in the vocabulary and use it</li>
</ul>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
mylist = [
    'a a b c',
    'a c c c d e f',
    'a c d d d',
    'a d f',
]
df = pd.DataFrame({&quot;texts&quot;: mylist})
tfidf_vectorizer = TfidfVectorizer(ngram_range=[1, 1], analyzer='char')
tfidf_separate = tfidf_vectorizer.fit_transform(df[&quot;texts&quot;])
ind = tfidf_vectorizer.vocabulary_['d']
tfidf_separate.todense()[2, ind]
&gt;&gt;&gt; 0.6490674853546846
</code></pre>
",""
"73011796","2022-07-17 12:31:53","0","","73011617","<p>An easier way of doing this is just using the available functions in tidytext and dplyr. No need for a loop.</p>
<pre><code>library(tidytext)
library(dplyr)

sw &lt;- stop_words %&gt;% filter(lexicon == &quot;SMART&quot;)

reviews_df %&gt;% 
  unnest_tokens(word, full_text, drop = FALSE) %&gt;% 
  anti_join(sw) %&gt;% # remove stopwords
  mutate(word = SnowballC::wordStem(word)) %&gt;%  # stemming
  group_by(id) %&gt;% 
  summarise(stemmed_description = paste0(word, collapse = &quot; &quot;))

Joining, by = &quot;word&quot;
# A tibble: 2 √ó 2
     id stemmed_description                                                                                  
  &lt;int&gt; &lt;chr&gt;                                                                                                
1     1 pseudoindepend shoulder your free judgement problem solv expect person give dont overwork packag aint
2     2 pseudoindepend shoulder your free judgement problem solv expect person give dont overwork packag aint
</code></pre>
<p>data:</p>
<pre><code>reviews_df &lt;- data.frame(id = 1:2,
                         full_text = c(&quot;pseudoindependence no one looking over your shoulder and youre free to use your own judgement to problem solve. they sometimes expect more than what a person can give. dont overwork yourself. the packages aint going no where!&quot;,
                                       &quot;pseudoindependence no one looking over your shoulder and youre free to use your own judgement to problem solve. they sometimes expect more than what a person can give. dont overwork yourself. the packages aint going no where!&quot;))
</code></pre>
",""
"72939110","2022-07-11 13:08:45","2","","72933472","<p>It looks like they've changed the way the lemmatizer is instantiated but the following should work...</p>
<pre><code>import spacy
nlp = spacy.load('en_core_web_sm', disable=['ner', 'tagger', 'parser', 'lemmatizer'])
lemmatizer = nlp.get_pipe('lemmatizer')
t = nlp('chuckles')[0]  
t.pos_ = 'NOUN'
lemma = lemmatizer.lemmatize(t)[0]
print(lemma)
# &gt;&gt; chuckle

</code></pre>
<p>It's unfortunate that you have to call the lemmatizer with a <code>Token</code> but looking at the code, I don't see a way to call it with <code>(word, pos)</code>. I think you're stuck with calling the empty pipeline with a single word to get a <code>Token</code> then manually setting the <code>pos_</code> before calling <code>lemmatize(t)</code>.</p>
<p>Note that the POS tagger will not work correctly on a single word. It only works in sentences and will probably always assign NOUN for pos if you only have one word. This is why I've disabled the pipeline and set <code>t.pos_</code> manually.</p>
<p>BTW.. if you only need to lemmatize, you might look at <a href=""https://github.com/bjascob/LemmInflect"" rel=""nofollow noreferrer"">lemminflect</a> which is simpler for single word and also more accurate.</p>
",""
"72929720","2022-07-10 15:35:34","0","","72920750","<p>I have figured out how to fix it, posting to help others :)</p>
<pre><code>import pandas as pd

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import FunctionTransformer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import Pipeline

import re
from lime.lime_text import LimeTextExplainer

from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = &quot;all&quot;

# Loading GitHub Repos data containing code and comments from 2.8 million GitHub repositories:
DATA_PATH = r&quot;/Users/stevesolun/Steves_Files/Data/github_repos_data.csv&quot;

data = pd.read_csv(DATA_PATH, dtype='object')
data = data.convert_dtypes()
data = data.dropna()
data = data.drop_duplicates()

# Train/Test split
X, y = data.content, data.language
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)

# Model params to match:
# 1. Variable and module names, words in a string, keywords: [A-Za-z_]\w*\b
# 2. Operators: [!\#\$%\&amp;\*\+:\-\./&lt;=&gt;\?@\\\^_\|\~]+
# 3. Tabs, spaces and Brackets: [ \t\(\),;\{\}\[\]`&quot;']
# with the following regex:
token_pattern = r&quot;&quot;&quot;(\b[A-Za-z_]\w*\b|[!\#\$%\&amp;\*\+:\-\./&lt;=&gt;\?@\\\^_\|\~]+|[ \t\(\),;\{\}\[\]`&quot;'])&quot;&quot;&quot;


def preprocess(x):
    &quot;&quot;&quot; Clean up single-character variable names or ones constituted of a sequence of the same character &quot;&quot;&quot;
    return pd.Series(x).replace(r'\b([A-Za-z])\1+\b', '', regex=True)\
        .replace(r'\b[A-Za-z]\b', '', regex=True)


# Pipe steps:
# Define a transformer:
transformer = FunctionTransformer(preprocess)
# Perform TF-IDF vectorization with our token pattern:
vectorizer = TfidfVectorizer(token_pattern=token_pattern, max_features=1500)
# Create Random Forest Classifier:
clf = RandomForestClassifier(n_jobs=-1)

pipe_RF = Pipeline([
     ('preprocessing', transformer),
     ('vectorizer', vectorizer)]
    )

# Setting best params (after performing GridSearchCV)
best_params = {
    'criterion': 'gini',
    'max_features': 'sqrt',
    'min_samples_split': 3,
    'n_estimators': 300
}

clf.set_params(**best_params)
# Here I am preprocessing the data:
X_train = pipe_RF.fit_transform(X_train).toarray()
X_test = pipe_RF.transform(X_test).toarray()

# Fitting the model outside the pipe - feel free to show if possible to do it inside the pipe + fit_transform the train and test sets.
clf.fit(X_train, y_train)

# Evaluation
print(f'Accuracy: {clf.score(X_test, y_test)}')


user_input = &quot;&quot;&quot; def fib(n):
                    a,b = 0,1
                    while a &lt; n:
                        print(a, end=' ')
                    a,b = b, a+b
                    print()
                    fib(1000)

   &quot;&quot;&quot;
clf.predict(pipe_RF.transform(user_input))[0]

prediction = clf.predict(pipe_RF.transform(user_input))[0]
predicted_class_idx = list(clf.classes_).index(prediction)

import shap

shap.initjs()
explainer = shap.TreeExplainer(clf, X_train)
observation = pipe_RF.transform(user_input).toarray()
shap_values = explainer.shap_values(observation)


shap.force_plot(explainer.expected_value[predicted_class_idx], shap_values[predicted_class_idx], feature_names=vectorizer.get_feature_names_out())
</code></pre>
<p><a href=""https://i.sstatic.net/BoVyW.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/BoVyW.png"" alt=""enter image description here"" /></a></p>
",""
"72845148","2022-07-03 08:53:26","5","","72837120","<p>If I understand what you want, I think &quot;not really&quot;.</p>
<p>The purpose of the IDF factor is to provide a normalized weight for each term. If the term occurs in all your documents, you are ranking it as less important than some other terms which don't.</p>
<p>In so many words, the chart is not wrong in essence; it shows how the frequency of the term has increased over the years. But the Y axis is basically meaningless in isolation; you are dividing by a constant which just obscures the actual number which you want to explore, that is, the absolute frequency of your term of interest.</p>
<p>If you were to compare two different terms, IDF would make sense: It normalizes the weight of really common words (like &quot;the&quot; and &quot;and&quot;) so their <em>relative</em> use in a specific document can be compared against the relative frequency of less common words (like &quot;fraught&quot; and &quot;outwardly&quot;) in the same document on a normalized scale.</p>
<p>It sounds to me like the number you care about is simply the term frequency, though a normalization which could make sense is to divide by the length of the document (so if &quot;cloud&quot; occurs twice in a document with 20,000 words, it's not more significant than if it occurs only once in a smaller document which only contains 10,000 words).</p>
",""
"72782983","2022-06-28 08:16:48","0","","72782449","<p>Hello the problem is that you've named your function like the nltk.corpus module. You should find an other name for your function and it'll work I think.</p>
",""
"72709520","2022-06-22 03:58:39","2","","72701918","<p>Your regexes are faster here because they're only doing the work you need. spaCy is also doing tokenization, which for your preprocessing described here is not necessary, so it's not surprising it's slower.</p>
<p>Since it's likely you'll want tokens for whatever downstream processing you have, your current comparison may not be useful.</p>
",""
"72653864","2022-06-17 02:53:53","1","","72645822","<p>You can find what labels in spaCy mean by using <a href=""https://spacy.io/api/top-level/#spacy.explain"" rel=""nofollow noreferrer""><code>spacy.explain</code></a>.</p>
<pre><code>spacy.explain(&quot;NORP&quot;)
# Nationalities or religious or political groups

doc = nlp(&quot;Hello world&quot;)
for word in doc:
   print(word.text, word.tag_, spacy.explain(word.tag_))
# Hello UH interjection
# world NN noun, singular or mass
</code></pre>
<p>The dependency relations all come from <a href=""https://universaldependencies.org/"" rel=""nofollow noreferrer"">Universal Dependencies</a>.</p>
",""
"72572491","2022-06-10 10:06:09","1","","72572232","<p>Here is a solution by adding a transformer which will apply the inverse column permutation after the column transform:</p>
<pre class=""lang-py prettyprint-override""><code>from sklearn.base import BaseEstimator, TransformerMixin
import re


class ReorderColumnTransformer(BaseEstimator, TransformerMixin):
    index_pattern = re.compile(r'\d+$')
    
    def __init__(self, column_transformer):
        self.column_transformer = column_transformer
        
    def fit(self, X, y=None):
        return self

    def transform(self, X, y=None):
        order_after_column_transform = [int( self.index_pattern.search(col).group()) for col in self.column_transformer.get_feature_names_out()]
        order_inverse = np.zeros(len(order_after_column_transform), dtype=int)
        order_inverse[order_after_column_transform] = np.arange(len(order_after_column_transform))
        return X[:, order_inverse]
</code></pre>
<p>It relies on parsing</p>
<pre class=""lang-py prettyprint-override""><code>column_trans.get_feature_names_out()
# = array(['scaler__x1', 'scaler__x3', 'remainder__x0', 'remainder__x2'],
#      dtype=object)
</code></pre>
<p>to read the initial column order from the suffix number. Then computing and applying the inverse permutation.</p>
<p>To be used as:</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
from sklearn.compose import ColumnTransformer 
from sklearn.preprocessing import  MinMaxScaler
from sklearn.pipeline import make_pipeline

X = np.array ( [(25, 1, 2, 0),
                (30, 1, 5, 0),
                (25, 10, 2, 1),
                (25, 1, 2, 0),
                (np.nan, 10, 4, 1),
                (40, 1, 2, 1) ] )



column_trans = ColumnTransformer(
    [ ('scaler', MinMaxScaler(), [0,2]) ], 
     remainder='passthrough') 

pipeline = make_pipeline( column_trans, ReorderColumnTransformer(column_transformer=column_trans))
X_scaled = pipeline.fit_transform(X)
#X_scaled has same column order as X
</code></pre>
<hr />
<p>Alternative solution not relying on string parsing but reading the column slices of the column transformer:</p>
<pre class=""lang-py prettyprint-override""><code>from sklearn.base import BaseEstimator, TransformerMixin


class ReorderColumnTransformer(BaseEstimator, TransformerMixin):
    
    def __init__(self, column_transformer):
        self.column_transformer = column_transformer
        
    def fit(self, X, y=None):
        return self

    def transform(self, X, y=None):
        slices = self.column_transformer.output_indices_.values()
        n_cols = self.column_transformer.n_features_in_
        order_after_column_transform = [value for slice_ in slices for value in range(n_cols)[slice_]]
        
        order_inverse = np.zeros(n_cols, dtype=int)
        order_inverse[order_after_column_transform] = np.arange(n_cols)
        return X[:, order_inverse]
</code></pre>
",""
"72395079","2022-05-26 16:29:42","1","","72394840","<p>If you are asking how to apply a function over a pandas DataFrame column, you can do</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
from nltk.stem import WordNetLemmatizer


data = pd.DataFrame({
    &quot;id&quot;: [1, 2, 3, 4],
    &quot;verb&quot;: [&quot;believe&quot;, &quot;start&quot;, &quot;believed&quot;, &quot;starting&quot;],
})
# https://www.nltk.org/_modules/nltk/stem/wordnet.html
wnl = WordNetLemmatizer()
data.verb = data.verb.map(lambda word: wnl.lemmatize(word, pos=&quot;v&quot;))

print(data)
</code></pre>
<p>Output</p>
<pre><code>   id     verb
0   1  believe
1   2    start
2   3  believe
3   4    start
</code></pre>
",""
"72370632","2022-05-24 23:54:50","0","","72349165","<p>This problem is indeed in an awkward wasteland between context-free parsing, which is far too precise to handle unstructured discourse, and natural language parsing, which (as I understand the current state of the art) is not designed to take advantage of subtle printed clues.</p>
<p>My recommendation, for what it's worth, is that you use a collection of ad hoc regular expressions to attempt to capture the printed style and the boilerplate phrases. (&quot;A paper was tabled and ordered to lie upon the table of the house.&quot;) That's what I did when I tried to do something like this a couple of decades ago with the Canadian equivalent (in the days in which Perl was state of the art), and it mostly worked, although a certain amount of manual intervention was required. (My style is to use sanity checks to try to detect cases which are mishandled and log them to allow future improvements.) How much work all that is will depend on how precise you need the results to be.</p>
<p>It's quite possible that you could build a machine learning model which did a reasonable job, if you have access to enough computational resources. But you'll still need to do a lot of verification and recalibration, unless you can tolerate errors.</p>
",""
"72289074","2022-05-18 12:21:12","0","","72284795","<p>You should use <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html"" rel=""nofollow noreferrer""><code>.loc[]</code></a> rather than chained indexing (as in your answer) as it's more efficient, and chained indexing can sometimes raise a <code>SettingWithCopy</code> warning (read more <a href=""https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy"" rel=""nofollow noreferrer"">here</a>).</p>
<p>To use .loc, you would call it like below:</p>
<pre><code>df.loc[row_index(es), col_names]
</code></pre>
<p>Which would be the below in your example:</p>
<pre><code>TF_IDF.loc['life', '0']
</code></pre>
<p>returns:</p>
<pre><code>0
</code></pre>
",""
"72253835","2022-05-16 03:15:16","3","","72249074","<p>The following rule will match a token that equals &quot;ADP&quot; when made lowercase. This will not match anything because &quot;ADP&quot; is not lowercase.</p>
<pre><code>{'LOWER': 'ADP'},
</code></pre>
<p>I am not sure what this is supposed to match, maybe you want to match a lowercase word with POS = ADP? In that case you would want a rule like this:</p>
<pre><code>{&quot;POS&quot;: &quot;ADP&quot;, &quot;REGEX&quot;: &quot;^[a-z]+$&quot;}
</code></pre>
<hr />
<p>To restate what I said above: <code>{'LOWER': 'ADP'}</code> <strong>does not</strong> match a lowercase word with the ADP part of speech. You seem to be confused about what &quot;LOWER&quot; means or how rules work.</p>
<p>Let me give an example. <code>{&quot;LOWER&quot;: &quot;dog&quot;}</code> will match words like &quot;Dog&quot;, &quot;DOG&quot;, or &quot;dog&quot;. It will not match words with the part of speech &quot;dog&quot; (which do not exist). <code>&quot;LOWER&quot;: value</code> means, &quot;match words which look like <code>value</code> when they are made lowercase&quot;.</p>
<p>If you want to match lower case words that have the ADP part of speech, you should use the rule I wrote above with the <code>REGEX</code> bit.</p>
",""
"72242436","2022-05-14 17:12:56","4","","72241814","<p>You have defined <code>tfIdf</code> as <code>tfIdf = tfIdfVectorizer.fit_transform(dataset)</code>.</p>
<p>So <code>tfIdf.toarray()</code> would be a 2-D array, where each row refers to a document and each element in the row refers to the TF-IDF score of the corresponding word. To know what word each element is representing, you could use the <code>.get_feature_names()</code> function which would print a list of words. Then you can use this information to create a mapping (dict) from words to scores, like this:</p>
<p><code>wordScores = dict(zip(tfIdfVectorizer.get_feature_names(), tfIdf.toarray()[0]))</code></p>
<p>Now suppose your document contains the word &quot;digital&quot; and you want to know its TF-IDF score, you could simply print the value of <code>wordScores[&quot;digital&quot;]</code>.</p>
",""
"72193798","2022-05-10 23:06:14","0","","72193062","<p>Calling <code>fit_transform</code> calculates a vector for each supplied document. Each vector will be the same size. The size of the vector is the number of unique words across the supplied documents. The number of zero values in the vector will be the vector size - number of unique values in the document.</p>
<p>Using your top_words as a simple example. You show 2 documents:</p>
<pre><code>'people sun flower festival'
'sunflower sun architecture red buses festival'
</code></pre>
<p>These have a total of 8 unique words (<code>Vectorizer.get_feature_names_out()</code> will give you these):</p>
<pre><code>'architecture', 'buses', 'festival', 'flower', 'people', 'red', 'sun', 'sunflower'
</code></pre>
<p>Calling <code>fit_transform</code> with those 2 documents will give 2 vectors (1 for each doc), each with length 8 (number of unique words across the documents).</p>
<p>The first document, <code>'people sun flower festival'</code> has 4 words, so, you get 4 values in the vector, and 4 zeros. Similarly <code>'sunflower sun architecture red buses festival'</code> gives 6 values and 2 zeros.</p>
<p>The more documents you pass in with different words, the longer the vector gets, and the more likely the zeros are.</p>
<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer

top_words = ['people sun flower festival', 'sunflower sun architecture red buses festival']

Vectorizer = TfidfVectorizer()
Vectors = Vectorizer.fit_transform(top_words)

print(f'Feature names: {Vectorizer.get_feature_names_out().tolist()}')
tfidf = Vectors.toarray()
print('')
print(f'top_words[0] = {top_words[0]}')
print(f'tfidf[0] = {tfidf[0].tolist()}')
print('')
print(f'top_words[1] = {top_words[1]}')
print(f'tfidf[1] = {tfidf[1].tolist()}')
</code></pre>
<p>The above code will print:</p>
<pre><code>Feature names: ['architecture', 'buses', 'festival', 'flower', 'people', 'red', 'sun', 'sunflower']

top_words[0] = people sun flower festival
tfidf[0] = [0.0, 0.0, 0.40993714596036396, 0.5761523551647353, 0.5761523551647353, 0.0, 0.40993714596036396, 0.0]

top_words[1] = sunflower sun architecture red buses festival
tfidf[1] = [0.4466561618018052, 0.4466561618018052, 0.31779953783628945, 0.0, 0.0, 0.4466561618018052, 0.31779953783628945, 0.4466561618018052]
</code></pre>
",""
"72189974","2022-05-10 16:30:51","0","","72189892","<p>You could try with this list comprehension:</p>
<pre><code>[lemmatizer.lemmatize(w) if len(w)&gt;4 else w for w in wd]
</code></pre>
<p>Then, if you want a single string considering your input sample, you can use the Python <code>join</code> operation on strings:</p>
<pre><code>' '.join([lemmatizer.lemmatize(w) if len(w)&gt;4 else w for w in wd])
</code></pre>
",""
"71976633","2022-04-23 03:40:48","1","","71962152","<pre><code>string_ = &quot;I am will be playing football tommorrow&quot; # dummy string 
obj = nlp(string_)
lemmatize_token = [x.lemma_ for x in obj]

print(lemmatize_token)
['I', 'be', 'will', 'be', 'play', 'football', 'tommorrow']
</code></pre>
",""
"71960891","2022-04-21 21:08:06","0","","71960583","<p>This has to do with how you hand the references to the method.</p>
<p>Check out the <a href=""https://www.nltk.org/_modules/nltk/translate/bleu_score.html"" rel=""nofollow noreferrer"">documentation</a> or more specifically <a href=""https://github.com/nltk/nltk/blob/develop/nltk/translate/bleu_score.py#L28"" rel=""nofollow noreferrer"">this sample of how to use it</a>.</p>
<p>References have to be a list (a list of all reference translations to compare the hypothesis with). So in your case a list containing the one tokenized reference translation.</p>
<p>Use it like this and it will work:</p>
<pre><code>score = sentence_bleu([['where', 'are', 'economic', 'networks']], ['where', 'are', 'the', 'economic', 'network'])
&gt; 9.283142785759642e-155
</code></pre>
<p>Although I have to admit I have struggled with the exact same problem before and this is not very intuitive on NLTKs part. The documentation even states that references has to be of type <code>list(list(str))</code> but then doesn't check if this is the case and instead returns a misleading result when used incorrectly.</p>
",""
"71878085","2022-04-14 22:06:36","0","","71876033","<p>You can determine tf dataframe by using CountVectorizer. Then divide each value by max value of it's column and repeat this process for every column in your dataframe</p>
<pre><code> df_1st = df.apply(lambda col: col / col.max())
</code></pre>
<p>and then just multiply and add a scaler for each element in your dataframe.</p>
<pre><code>df_2nd = df_1st.apply(lambda col: lambda + col*(1-lambda))
tf_matrix = df_2nd
</code></pre>
",""
"71876346","2022-04-14 18:49:20","0","","71722637","<p>Okay friends, I found the answer. Here are two solutions:</p>
<p>(1) If you know the beginning and end rules for your span, as well as its token length, you can use the <code>'NOT_IN'</code> function within the Matcher to accept all possible tokens except ones you choose to prohibit. The below Matcher defines the beginning, end, and middle tokens. The beginning and end should be clear. The middle token can be anything other than the DEP, TAG, POS, etc. you define. In this case, we want to match any single token except a 'nsubj' and 'PUNCT'.</p>
<pre><code>pattern01 = [{'POS': 'SCONJ', 'OP': '+'},
             {'DEP':{'NOT_IN': ['nsubj']}, 'POS':{'NOT_IN': ['PUNCT']}},
             {'POS': 'VERB', 'OP': '+'}]
</code></pre>
<p>This pattern (1) matches an 'SCONJ' in the first token, (2) matches any token that is not an 'nsubj' or 'PUCNT' in the second token, and (3) matches any 'VERB' in the third token. But what if there are an invariable number of tokens between your desired beginning and end token?</p>
<p>(2) In order to accept indefinitely long matches, while specifying the beginning and end tokens, we combine the <code>{'OP': '*'}</code> and <code>'NOT_IN'</code> functions. We modify the above code as follows:</p>
<pre><code>pattern01 = [{'POS': 'SCONJ', 'OP': '+'},
             {'OP': '*', 'DEP':{'NOT_IN': ['nsubj']}, 'POS':{'NOT_IN': ['PUNCT']}},
             {'POS': 'VERB', 'OP': '+'}]
</code></pre>
<p>This pattern (1) matches an 'SCONJ' in the first token, (2) matches an indefinite string of tokens so long as they are not 'nsubj' or 'PUNCT', and (3) matches any 'VERB' in the third token.</p>
<p>The <code>'OP': '*'</code> tells the Matcher to accept any token. The <code>'NOT_IN'</code> specifies a list of tokens that should be exempt from the aforementioned rule. If one of these specified tokens does exist within the pattern, the matcher will not match the span.</p>
<p>Best of luck everyone!</p>
",""
"71866465","2022-04-14 04:19:21","4","","71866288","<p>Maybe it would be helpful to space your example out a bit:</p>
<pre><code>(a , b , c , d)
(a , b , c   d)
(a , b   c , d)
(a   b , c , d)
(a   b , c   d)
(a , b   c   d)
(a   b   c , d)
(a   b   c   d)  # added for completeness
</code></pre>
<p>Looking at that, it's evident that what differentiates the rows is the presence or absence of commas, a typical binary choice. There are three places a comma could go, so there are eight possibilities, corresponding to the eight binary numbers of three digits.</p>
<p>The easiest way to list these possibilities is to count from <code>0 0 0</code> to <code>1 1 1</code>.</p>
<hr />
<p>For your modified question, in which there is a maximum length of a part, one simple recursive solution in Python is:</p>
<pre class=""lang-py prettyprint-override""><code>def kgram(k, v):
    'Generate all partitions of v with parts no larger than k'
    def helper(sfx, m):
        if m == 0: yield sfx
        else:
            for i in range(1, min(k, m)+1):
                yield from helper([v[m-i:m]]+sfx, m-i)

    yield from helper([], len(v))
</code></pre>
<p>Here's a quick test:</p>
<pre><code>&gt;&gt;&gt; for p in gram(3, 'one two three four five'.split()): print(p)
... 
[['one'], ['two'], ['three'], ['four'], ['five']]
[['one', 'two'], ['three'], ['four'], ['five']]
[['one'], ['two', 'three'], ['four'], ['five']]
[['one', 'two', 'three'], ['four'], ['five']]
[['one'], ['two'], ['three', 'four'], ['five']]
[['one', 'two'], ['three', 'four'], ['five']]
[['one'], ['two', 'three', 'four'], ['five']]
[['one'], ['two'], ['three'], ['four', 'five']]
[['one', 'two'], ['three'], ['four', 'five']]
[['one'], ['two', 'three'], ['four', 'five']]
[['one', 'two', 'three'], ['four', 'five']]
[['one'], ['two'], ['three', 'four', 'five']]
[['one', 'two'], ['three', 'four', 'five']]
</code></pre>
",""
"71732367","2022-04-04 05:21:08","2","","71726244","<p>You can use the <a href=""https://spacy.io/api/pipeline-functions#merge_entities"" rel=""nofollow noreferrer""><code>merge_entities</code></a> mini-component to convert entities to single tokens, which would simplify what you're trying to do. There's also a component to merge noun chunks similarly.</p>
",""
"71655154","2022-03-29 00:30:12","1","","71607906","<h3>1. Why 545 optimization steps?</h3>
<p>Looking at the implementation of the <code>transformers</code> package, we see that the <code>Trainer</code> uses a variable called <code>max_steps</code> when printing the <code>Total optimization steps</code> message in the <code>train</code> method:</p>
<pre class=""lang-py prettyprint-override""><code>logger.info(&quot;***** Running training *****&quot;)
logger.info(f&quot;  Num examples = {num_examples}&quot;)
logger.info(f&quot;  Num Epochs = {num_train_epochs}&quot;)
logger.info(f&quot;  Instantaneous batch size per device = {args.per_device_train_batch_size}&quot;)
logger.info(f&quot;  Total train batch size (w. parallel, distributed &amp; accumulation) = {total_train_batch_size}&quot;)
logger.info(f&quot;  Gradient Accumulation steps = {args.gradient_accumulation_steps}&quot;)
logger.info(f&quot;  Total optimization steps = {max_steps}&quot;)
</code></pre>
<p><a href=""https://github.com/huggingface/transformers/blob/4975002df50c472cbb6f8ac3580e475f570606ab/src/transformers/trainer.py#L1294"" rel=""noreferrer"">Permalink to the above snippet in the transformers repo</a></p>
<p>The <code>Trainer</code> has the following bit of code earlier in the <code>train</code> method:</p>
<pre class=""lang-py prettyprint-override""><code>class Trainer:
    [...]
    def train(self) -&gt; None:
        [Some irrelevant code ommited here...]

        total_train_batch_size = args.train_batch_size * args.gradient_accumulation_steps * args.world_size
        if train_dataset_is_sized:
            num_update_steps_per_epoch = len(train_dataloader) // args.gradient_accumulation_steps
            num_update_steps_per_epoch = max(num_update_steps_per_epoch, 1)
            if args.max_steps &gt; 0:
                max_steps = args.max_steps
                num_train_epochs = args.max_steps // num_update_steps_per_epoch + int(
                    args.max_steps % num_update_steps_per_epoch &gt; 0
                )
                # May be slightly incorrect if the last batch in the training datalaoder has a smaller size but it's
                # the best we can do.
                num_train_samples = args.max_steps * total_train_batch_size
            else:
                max_steps = math.ceil(args.num_train_epochs * num_update_steps_per_epoch)
                num_train_epochs = math.ceil(args.num_train_epochs)
                num_train_samples = len(self.train_dataset) * args.num_train_epochs
</code></pre>
<p><a href=""https://github.com/huggingface/transformers/blob/4975002df50c472cbb6f8ac3580e475f570606ab/src/transformers/trainer.py#L1294"" rel=""noreferrer"">Permalink to the above snippet in the transformers repo</a></p>
<p><code>total_train_batch_size = args.train_batch_size * args.gradient_accumulation_steps * args.world_size</code> in your example will be equal to <code>total_train_batch_size = 4 * 16 * 1 = 64</code>, as expected.</p>
<p>Then we have <code>num_update_steps_per_epoch = len(train_dataloader) // args.gradient_accumulation_steps</code> which will give us <code>num_update_steps_per_epoch = len(train_dataloader) // 16</code>.</p>
<p>Now the length of a <code>DataLoader</code> is equal to the number of batches in that <code>DataLoader</code>. Since you have 7000 samples and we have a <code>per_device_train_batch_size</code> of 4, this will give us <code>7000 / 4 = 1750</code> batches. Going back to <code>num_update_steps_per_epoch</code> We now have <code>num_update_steps_per_epoch = 1750 // 16 = 109</code> (Python integer division takes the floor)</p>
<p>You don't have a number of max steps specified so then we get to <code>max_steps = math.ceil(args.num_train_epochs * num_update_steps_per_epoch)</code> which gives us <code>max_steps = math.ceil(5 * 109) = 545</code>.</p>
<h3>2. Why does the padding operation get logged 16 times?</h3>
<p>In a transformers architecture, you technically don't have to pad <strong>all</strong> your samples to be the same length. What actually matters is that samples within a batch are the same length, that length can differ from batch to batch.</p>
<p>This means that this message will appear for every batch that goes through a forward pass. As to why the message appeared 16 times even though 23 batches have actually gone through a forward pass I can think of two possible reasons:</p>
<ol>
<li>The logging of the padding operation and the logging of the progress bar are happening on two different threads and the former is lagging behind a bit</li>
<li>(Extremely unlikely) you had batches that did not need to be padded because all samples had the same length and that length was a multiple of 512 already.</li>
</ol>
",""
"71620861","2022-03-25 17:22:04","0","","71620687","<p>I really don't understand what you are trying to do in the list comprehensions, so I'll just write how I would do it:</p>
<pre><code>from nltk import WordNetLemmatizer, SnowballStemmer

lemmatizer = WordNetLemmatizer()
stemmer = SnowballStemmer(&quot;english&quot;)


def find_roots(token_list, n):
    token = token_list[n]
    stem = stemmer.stem(token)
    lemma = lemmatizer.lemmatize(token)
    return {&quot;original&quot;: token, &quot;stem&quot;: stem, &quot;lemma&quot;: lemma}


roots_dict = find_roots([&quot;said&quot;, &quot;talked&quot;, &quot;walked&quot;], n=2)
print(roots_dict)
&gt; {'original': 'walked', 'stem': 'walk', 'lemma': 'walked'}
</code></pre>
",""
"71619185","2022-03-25 15:10:08","2","","71617889","<p>It sounds like you need metadata filtering rather than placing the year within the query itself. The <code>FaissDocumentStore</code> doesn't support filtering, I'd recommend switching to the <code>PineconeDocumentStore</code> which Haystack introduced in the v1.3 release a few days ago. It supports the strongest filter functionality in the current set of document stores.</p>
<p>You will need to make sure you have the latest version of Haystack installed, and it needs an additional <code>pinecone-client</code> library too:</p>
<pre><code>pip install -U farm-haystack pinecone-client
</code></pre>
<p>There's a <a href=""https://www.pinecone.io/docs/integrations/haystack/"" rel=""nofollow noreferrer"">guide here</a> that may help, it will go something like:</p>
<pre class=""lang-py prettyprint-override""><code>document_store = PineconeDocumentStore(
    api_key=&quot;&lt;API_KEY&gt;&quot;, # from https://app.pinecone.io
    environment=&quot;us-west1-gcp&quot;
)
retriever = EmbeddingRetriever(
    document_store,
    embedding_model='all-mpnet-base-v2',
    model_format='sentence_transformers'
)
</code></pre>
<p>Before you write the documents you need to convert the data to include your text in <code>content</code> (as you have done above, but no need to pre-append the year), and then include the year as a field in a <code>meta</code> dictionary. So you would create a list of dictionaries that look like:</p>
<pre class=""lang-py prettyprint-override""><code>dicts = [
    {'content': 'your text here', 'meta': {'year': 1999}},
    {'content': 'another record text', 'meta': {'year': 1971}},
    ...
]
</code></pre>
<p>I don't know the exact format of your <code>df</code> but assuming it is something like:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>text</th>
<th>year</th>
</tr>
</thead>
<tbody>
<tr>
<td>&quot;your text here&quot;</td>
<td>1999</td>
</tr>
<tr>
<td>&quot;another record here&quot;</td>
<td>1971</td>
</tr>
</tbody>
</table>
</div>
<p>We could write the following to reformat it:</p>
<pre class=""lang-py prettyprint-override""><code>df = df.rename(columns={'text': 'content'})  # you did this already

# create a new 'meta' column that contains {'year': &lt;year&gt;} data
df['meta'] = df['year'].apply(lambda x: {'year': x})

# we don't need the year column anymore so we drop it
df = df.drop(['year'], axis=1)

# now convert into the list of dictionaries format as you did before
dicts = df.to_dict(orient='records')
</code></pre>
<p>This data replaces the df dictionaries you write, so we would continue as so:</p>
<pre class=""lang-py prettyprint-override""><code>document_store.write_documents(dicts)
document_store.update_embeddings(retriever=retriever)
</code></pre>
<p>Now you can query with filters, for example to search for docs with the publish year of 1999 we use the condition <code>&quot;$eq&quot;</code> (equals):</p>
<pre class=""lang-py prettyprint-override""><code>docs = retriever.retrieve(
    &quot;some query here&quot;,
    top_k=25,
    filters={
        {&quot;year&quot;: {&quot;$eq&quot;: 1999}}
    }
)
</code></pre>
<p>For published before 1980 we can use <code>&quot;$lt&quot;</code> (less than):</p>
<pre class=""lang-py prettyprint-override""><code>docs = retriever.retrieve(
    &quot;some query here&quot;,
    top_k=25,
    filters={
        {&quot;year&quot;: {&quot;$lt&quot;: 1980}}
    }
)
</code></pre>
",""
"71585593","2022-03-23 10:28:47","5","","71585275","<h2>Distance between first verb and previous noun</h2>
<p>Inspired by the very similar question <a href=""https://stackoverflow.com/questions/50151820/extract-nouns-and-verbs-using-nltk"">Extract nouns and verbs using nltk?</a>.</p>
<pre class=""lang-py prettyprint-override""><code>import nltk

def dist_noun_verb(text):
    text = nltk.word_tokenize(text)
    pos_tagged = nltk.pos_tag(text)
    last_noun_pos = None
    for pos, (word, function) in enumerate(pos_tagged):
        if function.startswith('NN'):
            last_noun_pos = pos
        elif function.startswith('VB'):
            assert(last_noun_pos is not None)
            return pos - last_noun_pos

for sentence in ['Video show Adam stabbing the bystander.', 'Woman quickly ran from the police after the incident.']:
    print(sentence)
    d = dist_noun_verb(sentence)
    print('Distance noun-verb: ', d)
</code></pre>
<p>Output:</p>
<pre><code>Video show Adam stabbing the bystander.
Distance noun-verb:  1
Woman quickly ran from the police after the incident.
Distance noun-verb:  2
</code></pre>
<p>Note that <code>function.startswith('VB')</code> detects the first verb in the sentence. If you want to make a distinction between the principal verb or some other kind of verb you need to examine the different kinds of verbs classified by <code>nltk.pos_tagged</code>: 'VBP', 'VBD', etc.</p>
<p>Also, the <code>assert(last_noun_pos is not None)</code> line in my code means the code will crash if the first verb comes before any noun. You might want to handle that differently.</p>
<p>Interestingly, if I add an <code>'s'</code> to <code>'show'</code> and make the sentence <code>'Video shows Adam stabbing the bystander.'</code>, then nltk parses <code>'shows'</code> as a noun rather than a verb.</p>
<h2>Going further: distance between &quot;main&quot; verb and previous noun</h2>
<p>Consider the sentence:</p>
<pre><code>'The umbrella that I used to protect myself from the rain was red.'
</code></pre>
<p>This sentence contains three verbs: <code>'used', 'protect', 'was'</code>. Using <code>nltk.word_tokenize.pos_tag</code> as I did above would correctly identify those three verbs:</p>
<pre class=""lang-py prettyprint-override""><code>text = 'The umbrella that I used to protect myself from the rain was red.'
tokens = nltk.word_tokenize(text)
pos_tagged = nltk.pos_tag(tokens)
print(pos_tagged)
# [('The', 'DT'), ('umbrella', 'NN'), ('that', 'IN'), ('I', 'PRP'), ('used', 'VBD'), ('to', 'TO'), ('protect', 'VB'), ('myself', 'PRP'), ('from', 'IN'), ('the', 'DT'), ('rain', 'NN'), ('was', 'VBD'), ('red', 'JJ'), ('.', '.')]
print([(w,f) for w,f in pos_tagged if f.startswith('VB')])
# [('used', 'VBD'), ('protect', 'VB'), ('was', 'VBD')]
</code></pre>
<p>However, the main verb of the sentence is <code>'was'</code>; the other two verbs are part of the nominal group that forms the subject of the sentence, <code>'The umbrella that I used to protect myself from the rain'</code>.</p>
<p>Thus we might like to write a function <code>dist_subject_verb</code> that returns the distance between the subject and the main verb <code>'was'</code>, rather than between the first verb <code>'used'</code> and the previous noun.</p>
<p>One way to identify the main verb is to parse the sentence into a tree, and ignore verbs that are located in subtrees, only considering the verb that is a direct child of the root.</p>
<p>The sentence should be parsed as something like:</p>
<pre class=""lang-py prettyprint-override""><code>((The umbrella) (that (I used) to (protect (myself) (from (the rain))))) (was) (red)
</code></pre>
<p>And now we can easily ignore <code>'used'</code> and <code>'protect'</code>, which are deep into subtrees, and only consider main verb <code>'was'</code>.</p>
<p>Parsing the sentence into a tree is a much more complex operation that just tokenizing it.</p>
<p>Here is a similar question that deals with parsing a sentence into a tree:</p>
<ul>
<li><a href=""https://stackoverflow.com/a/71492485/3080723"">How to get parse tree using python nltk?</a></li>
</ul>
",""
"71543615","2022-03-20 02:33:35","0","","71532653","<p>Well the variable used for printing that summary is this one: <a href=""https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py#L1211"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py#L1211</a>.</p>
<p>The total train batch size is defined as <code>train_batch_size * gradient_accumulation_steps * world_size</code>, so in your case <code>4 * 16 * 1 = 64</code>. <code>world_size</code> is always 1 except when you are using a TPU/training in parallel, see <a href=""https://github.com/huggingface/transformers/blob/master/src/transformers/training_args.py#L1127"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/blob/master/src/transformers/training_args.py#L1127</a>.</p>
",""
"71477291","2022-03-15 05:03:01","0","","71417857","<p>I changed the script and separated the <code>state machine</code> segment. The most serious problem with this program IMO is it's just returning the first pattern (you can fix it quickly).</p>
<pre><code>import pandas as pd
import nltk
POSTAG = nltk.pos_tag
df = pd.DataFrame({'text':['high school football players charged after video surfaces showing hazing', 'trump accuser pushes new york to pass the adult survivors act plans to sue']})
for text_id, text in enumerate(df['text'].values):
    
    # Remove the comma and full stops
    text = text.replace(',', '').replace('.', '').replace('-','')
    tokens = nltk.word_tokenize(text.lower())
    sentence_tags = POSTAG(tokens)
    words = [item[0] for item in sentence_tags]
    start_end = []
    temp = 0
    for word in words:
      start_end.append([temp, temp+len(word)])
      temp+= (len(word)+1) 
    tags = [item[1] for item in sentence_tags]
    words_to_print = []
    tags_to_print = []
    start_end_to_print = []
    # the state machine 
    verb = False
    first_noun = False
    second_noun = False
    third_noun = False
    for w, t, se in zip(words, tags, start_end):
      if t.startswith('NN'):
        words_to_print.append(w)
        tags_to_print.append(t)
        start_end_to_print.append(se)
        first_noun = True

      elif t.startswith('NN') and first_noun:
        words_to_print.append(w)
        tags_to_print.append(t)
        start_end_to_print.append(se)
        second_noun = True

      elif t.startswith('NN') and second_noun:
        words_to_print.append(w)
        tags_to_print.append(t)
        start_end_to_print.append(se)
        third_noun = True

      elif t.startswith('VB') and (first_noun or second_noun or third_noun):
        break 
      
      elif (first_noun or second_noun or third_noun):
        words_to_print = []
        tags_to_print = []
        start_end_to_print = []
        verb = False
        first_noun, second_noun, third_noun = False, False, False
    
    print('&gt; ', ' '.join(words_to_print), ' '.join([str(item[0])+' '+str(item[1]) for item in zip(start_end_to_print, tags_to_print)]))   
      
</code></pre>
<p>output:</p>
<pre><code>&gt;  school football players [5, 11] NN [12, 20] NN [21, 28] NNS
&gt;  trump accuser [0, 5] NN [6, 13] NN
</code></pre>
",""
"71409646","2022-03-09 12:49:45","3","","71409353","<p>You can find positions for certein PoS tags on a text using some of the existing NLP frameworks such us <a href=""https://spacy.io/"" rel=""noreferrer"">Spacy</a> or <a href=""https://www.nltk.org/"" rel=""noreferrer"">NLTK</a>. Once you process the text you can iterate for each token and check if the pos tag is what you are looking for, then get the start/end position of that token in your text.</p>
<p><strong>Spacy</strong></p>
<p>Using spacy, the code to implement what you want would be something like this:</p>
<pre class=""lang-py prettyprint-override""><code>import spacy

nlp = spacy.load(&quot;en_core_web_lg&quot;)
doc = nlp(&quot;Man walks into a bar.&quot;)  # Your text here

words = []
for token in doc:
    if token.pos_ == &quot;NOUN&quot; or token.pos_ == &quot;VERB&quot;:
        start = token.idx  # Start position of token
        end = token.idx + len(token)  # End position = start + len(token)
        words.append((token.text, start, end, token.pos_))

print(words)
</code></pre>
<p>In short, I build a new document from the string, iterate over all the tokens and keep only those whose post tag is VERB or NOUN. Finally I add the token info to a list for further processing. I strongly recommend that you read the following <a href=""https://spacy.io/usage/spacy-101"" rel=""noreferrer"">spacy tutorial</a> for more information.</p>
<p><strong>NLTK</strong></p>
<p>Using NLTK I think is pretty straightforward too, using <a href=""https://www.nltk.org/api/nltk.tokenize.html"" rel=""noreferrer"">NLTK tokenizer</a> and <a href=""https://www.nltk.org/book/ch05.html"" rel=""noreferrer"">pos tagger</a>. The rest is almost analogous to how we do it using spacy.</p>
<p>What I'm not sure about is the most correct way to get the start-end positions of each token. Note that for this I am using a tokenization helper created by <code>WhitespaceTokenizer().tokenize()</code> method, which returns a list of tuples with the start and end positions of each token. Maybe there is a simpler and NLTK-like way of doing it.</p>
<pre class=""lang-py prettyprint-override""><code>import nltk
from nltk.tokenize import WhitespaceTokenizer

text = &quot;Man walks into a bar.&quot;  # Your text here
tokens_positions = list(WhitespaceTokenizer().span_tokenize(text))  # Tokenize to spans to get start/end positions: [(0, 3), (4, 9), ... ]
tokens = WhitespaceTokenizer().tokenize(text)  # Tokenize on a string lists: [&quot;man&quot;, &quot;walks&quot;, &quot;into&quot;, ... ]

tokens = nltk.pos_tag(tokens) # Run Part-of-Speech tager

# Iterate on each token
words = []
for i in range(len(tokens)):
    text, tag = tokens[i]  # Get tag
    start, end = tokens_positions[i]  # Get token start/end
    if tag == &quot;NN&quot; or tag == &quot;VBZ&quot;:
        words.append((start, end, tag))

print(words)

</code></pre>
<p>I hope this works for you!</p>
",""
"71399097","2022-03-08 17:22:08","2","","71398882","<p>A <code>CUDA out of memory</code> error indicates that your GPU RAM (Random access memory) is full. This is different from the storage on your device (which is the info you get following the <code>df -h</code> command).</p>
<p>This memory is occupied by the model that you load into GPU memory, which is independent of  your dataset size. The GPU memory required by the model is at least twice the actual size of the model, but most likely closer to 4 times (initial weights, checkpoint, gradients, optimizer states, etc).</p>
<p>Things you can try:</p>
<ul>
<li>Provision an instance with more GPU memory</li>
<li>Decrease batch size</li>
<li>Use a different (smaller) model</li>
</ul>
",""
"71367592","2022-03-06 04:02:32","2","","71327407","<p>Before you go to machine learning, you could try using <a href=""https://pypi.org/project/fuzzywuzzy/"" rel=""nofollow noreferrer"">fuzzywuzzy</a>. I had a similar problem at work and was able to achieve high accuracy by adjusting the ratio attribute. So, for each extracted entity, you would have to run it through fuzzywuzzy and your dictionary.</p>
<p>For the issue of</p>
<blockquote>
<p>but this creates the problem of many false entities being extracted</p>
</blockquote>
<p>I would implement a filter: if the extracted &amp; matched entity is not in the list, leave the extracted entity out, otherwise, continue with the logic.</p>
",""
"71280315","2022-02-26 21:14:09","2","","71280204","<p>Assuming <code>sent_ids</code> is ordered, you can find out the position where <code>sent_id</code> has changed  and then split tokens based on that:</p>
<pre><code>list(map(&quot; &quot;.join, np.split(all_tokens, np.flatnonzero(np.diff(sent_ids) != 0)+1)))
# ['I spent a lot of time , money , and effort childproofing my house .', 'However , the kids still get in .']
</code></pre>
",""
"71264583","2022-02-25 10:39:35","0","","71261467","<ol>
<li><p>Is there a practical use case to perform lemmatization on the tokenized data and then stem that lemmatized data or vice versa</p>
</li>
<li><p>Does the idea of stemming the lemmatized data or vice versa make any sense theoretically, or is it completely incorrect.</p>
</li>
</ol>
<p>Regarding (1): Lemmatisation and stemming do essentially the same thing: they convert an inflected word form to a canonical form, on the assumption that features expressed through morphology (such as word endings) are not important for the use case. If you are not interested in tense, number, voice, etc, then lemmatising/stemming will reduce the number of distinct word forms you have to deal with (as different variations get folded into one canonical form). So without knowing what you want to do exactly, and whether morphological information is relevant to that task, it's hard to answer.</p>
<p>Lemmatisation is a linguistically motivated procedure. Its output is a valid word in the target language, but with endings etc removed. It is not without information loss, but there are not that many problematic cases. Is <em>does</em> a third person singular auxiliary verb, or the plural of a female deer? Is <em>building</em> a noun, referring to a structure, or a continuous form of the verb <em>to build</em>? What about <em>housing</em>? A casing for an object (such as an engine) or the process of finding shelter for someone?</p>
<p>Stemming is a less resource intense procedure, but as a trade-off it works with approximations only. You will have less precise results, which might not matter too much in an application such as information retrieval, but if you are at all interested in meaning, then it is probably too coarse a tool. Its output also will not be a word, but a 'stem', basically a character string roughly related to those you get when stemming similar words.</p>
<p>Re (2): no, it doesn't make any sense. Both procedures attempt the same task (normalising inflected words) in different ways, and once you have lemmatised, stemming is pointless. And if you stem first, you generally do not end up with valid words, so lemmatisation would not work anyway.</p>
",""
"71121723","2022-02-15 06:01:13","0","","71117302","<p>It sounds like the version of spaCy definitely changed, maybe from v2 to v3.</p>
<p>First, if spaCy is slow, see the <a href=""https://github.com/explosion/spaCy/discussions/8402"" rel=""nofollow noreferrer"">speed FAQ</a>.</p>
<p>Next, note that spaCy's lemmatizer is clever, so it relies on the part of speech of a word, since that can affect the lemma. This is why changing the contents of your string changes your lemmas - spaCy thinks that's a weird sentence and tries to predict the part of speech of each word and probably doesn't do so well, since it's not actually a sentence. spaCy is designed to take natural language, like full sentences, as input, not arbitrary lists of words.</p>
<p>If you just need to lemmatize standalone words you're better off using the lemmatizer in spaCy as a standalone component or even using the underlying data files directly. There's not a guide for that but if you look at <a href=""https://github.com/explosion/spacy-lookups-data"" rel=""nofollow noreferrer"">spacy-lookups-data</a> you can access it easily enough.</p>
",""
"71052343","2022-02-09 15:28:00","2","","71050697","<p>You should transfer your input to CUDA as well <em>before</em> performing the inference:</p>
<pre><code>device = torch.device('cuda')

# transfer model
model.to(device)

# define input and transfer to device
encoding = tokenizer.encode_plus(txt, 
     add_special_tokens=True, 
     truncation=True, 
     padding=&quot;max_length&quot;, 
     return_attention_mask=True, 
     return_tensors=&quot;pt&quot;)

encoding = encoding.to(device)

# inference
output = model(**encoding)
</code></pre>
<p>Be aware <a href=""https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.to"" rel=""noreferrer""><code>nn.Module.to</code></a> is in-place, while <a href=""https://pytorch.org/docs/stable/generated/torch.Tensor.to.html"" rel=""noreferrer""><code>torch.Tensor.to</code></a> is not (it does a copy!).</p>
",""
"71001973","2022-02-05 20:43:32","0","","69988135","<p>On the NLP side of things there's this course: <a href=""https://www.youtube.com/watch?v=dIUTsFT2MeQ"" rel=""nofollow noreferrer"">Natural Language Processing with spaCy &amp; Python - Course for Beginners</a> and this older course: <a href=""https://www.youtube.com/watch?v=X2vAabgKiuM"" rel=""nofollow noreferrer"">Natural Language Processing (NLP) Tutorial with Python &amp; NLTK</a> on <a href=""https://www.freecodecamp.org/"" rel=""nofollow noreferrer"">Free Code Camp</a>, which is generally a good place to start. Their courses provide in depth explanations of concepts and provide good examples.</p>
<p>On the translation side of things, the <a href=""https://www.deepl.com/en/translator"" rel=""nofollow noreferrer"">DeepL</a> translator is easy to use in multiple languages and offers a free api. It also offers and incredibly easy to use <a href=""https://github.com/DeepLcom/deepl-python"" rel=""nofollow noreferrer"">python library</a> if that's the language you intend to use (which you should because python is the best out there for NLP).</p>
<p>I hope this helps, but <a href=""https://stackoverflow.com/users/3607203/dennlinger"">dennlinger</a> is right - you shouldn't typically ask broad recommendation questions on StackOverflow!</p>
",""
"70957123","2022-02-02 14:10:14","2","","70956389","<p>I got where the problem is, the dataframes are storing these arrays as a string. So, the lemmatization is not working. Also note that, it is from the spell_eng part.</p>
<p>I have written a solution, which is a slight modification for your code.</p>
<pre><code>import pandas as pd
import nltk
from textblob import TextBlob
import functools
import operator

df = pd.DataFrame({'text': [&quot;spellling&quot;, &quot;was&quot;, &quot;working cooking listening&quot;,&quot;studying&quot;]})

#tokenization
w_tokenizer = nltk.tokenize.WhitespaceTokenizer()
def tokenize(text):
    return [w for w in w_tokenizer.tokenize(text)]
df[&quot;text2&quot;] = df[&quot;text&quot;].apply(tokenize)


# spelling correction
def spell_eng(text):
    text = [TextBlob(str(w)).correct() for w in text] #CHANGE
    #convert from tuple to str
    text = [functools.reduce(operator.add, (w)) for w in text] #CHANGE
    return text

df['text3'] = df['text2'].apply(spell_eng)


# lemmatization/stemming
def stem_eng(text):
    lemmatizer = nltk.stem.WordNetLemmatizer()
    return [lemmatizer.lemmatize(w,'v') for w in text] 
df['text4'] = df['text3'].apply(stem_eng)
df['text4']
</code></pre>
<p>Hope these things help.</p>
",""
"70911934","2022-01-30 04:50:59","0","","70905201","<p>spaCy does not include functionality for checking if a word is in the dictionary or not.</p>
<p>If you've loaded a pipeline with vectors, you can use <code>has_vector</code> to check if a word vector is present for a given token. This is kind of similar to checking if a word is in the dictionary, but it depends on the vectors - for most languages the vectors just include any word that appeared at least a certain number of times in a training corpus, so common typos or other strange things will be present, while some words may be randomly missing.</p>
<p>If you want to detect &quot;real&quot; words in some way it's best to source your own list.</p>
",""
"70885691","2022-01-27 21:02:18","3","","70880940","<p>You can just define two NP rules in one grammar:</p>
<pre class=""lang-py prettyprint-override""><code>grammar = &quot;&quot;&quot;
NP: {&lt;DT&gt;?&lt;NNP&gt;*&lt;NNS&gt;}
NP: {&lt;DT&gt;?&lt;NNS&gt;&lt;IN&gt;&lt;NNP&gt;}
&quot;&quot;&quot;
</code></pre>
<p>or using <code>|</code> as the wanted <code>OR</code> condition:</p>
<pre class=""lang-py prettyprint-override""><code>grammar = &quot;NP: {&lt;DT&gt;?&lt;NNP&gt;*&lt;NNS&gt;|&lt;DT&gt;?&lt;NNS&gt;&lt;IN&gt;&lt;NNP&gt;}&quot;
</code></pre>
<p>Full example:</p>
<pre class=""lang-py prettyprint-override""><code>import nltk

sentence_1 = 'show me the Paris hospitals'
sentence_2 = &quot;show me the hospitals in Paris&quot;

grammar_1 = &quot;&quot;&quot;
NP: {&lt;DT&gt;?&lt;NNP&gt;*&lt;NNS&gt;}
NP: {&lt;DT&gt;?&lt;NNS&gt;&lt;IN&gt;&lt;NNP&gt;}
&quot;&quot;&quot;
parser_1 = nltk.RegexpParser(grammar_1)

grammar_2 = &quot;NP: {&lt;DT&gt;?&lt;NNP&gt;*&lt;NNS&gt;|&lt;DT&gt;?&lt;NNS&gt;&lt;IN&gt;&lt;NNP&gt;}&quot;
parser_2 = nltk.RegexpParser(grammar_2)

for s in sentence_1, sentence_2:
    tokens = nltk.word_tokenize(s)
    pos_tags = nltk.pos_tag(tokens)
    print(parser_1.parse(pos_tags))
    print(parser_2.parse(pos_tags))

# outputs the same for both parsers:
# (S show/VB me/PRP (NP the/DT Paris/NNP hospitals/NNS))
# (S show/VB me/PRP (NP the/DT Paris/NNP hospitals/NNS))
# (S show/VB me/PRP (NP the/DT hospitals/NNS) in/IN Paris/NNP)
# (S show/VB me/PRP (NP the/DT hospitals/NNS) in/IN Paris/NNP)
</code></pre>
<p>(<a href=""https://www.nltk.org/api/nltk.chunk.regexp.html?highlight=regexpparser#nltk.chunk.regexp.RegexpParser"" rel=""nofollow noreferrer"">link to the documentation</a>)</p>
",""
"70843867","2022-01-25 05:36:06","1","","70837053","<p>Best thing is to try it and see.</p>
<p>If you're just adding new aliases, it really depends on how much they overlap with existing aliases. If there's no overlap it won't make any difference, but if there is overlap that could have resulted in different evaluations in training, which could modify the model. Whether those differences are significant or not is hard to say.</p>
",""
"70720797","2022-01-15 10:58:12","0","","70703655","<p>Took me a while but I found a solution.
To create a torchtext dataset with input data as lists, use SequenceTaggingDataset (from <code>torchtext.legacy.datasets.SequenceTaggingDataset</code>) but you need to do a simple change to the original <a href=""https://pytorch.org/text/_modules/torchtext/datasets/sequence_tagging.html"" rel=""nofollow noreferrer"">source code</a> in the <code>__init__</code> function, like this:</p>
<pre><code>    def __init__(self, columns, fields, encoding=&quot;utf-8&quot;, separator=&quot;\t&quot;, **kwargs):
        examples = []
        # for 2 fields data sets (text, tags)
        for words, labels in zip(columns[0], columns[-1]):
            examples.append(data.Example.fromlist([words, labels], fields))
        super(SequenceTaggingDataset, self).__init__(examples, fields,
                                                     **kwargs)
</code></pre>
<p>Then, assuming you have a data with two field (in my example, text and pos-tags) you can define the dataset like that:</p>
<pre><code>from torchtext.legacy import data

TEXT = data.Field()
UD_TAGS = data.LabelField()
# define torchtext fields
fields = ((&quot;text&quot;, TEXT), (&quot;udtags&quot;, UD_TAGS))
# push the data into a torchtext type of dataset (** modified SequenceTaggingDataset **)
train_torchtext_dataset = SequenceTaggingDataset([x_train, y_train], fields=fields)
</code></pre>
<p>Note that x_train, y_train are nested lists.</p>
",""
"70720319","2022-01-15 09:38:55","1","","70713831","<p>(Asking for software/data recommendations is off-topic for StackOverflow; but I have tried to give a more general &quot;approach&quot; answer.)</p>
<ol>
<li>Another approach to finding related words would be one of the machine learning approaches. If you are dealing with words in isolation, look at word embeddings such as GloVe or Word2Vec. Spacy and gensim have libraries for working with them, though I'm also getting some search hits for tutorials of working with them in nltk.</li>
</ol>
<p>2/3. One of the (in my opinion) core reasons for the success of Princeton WordNet was the liberal license they used. That means you can branch the project, add your extra data, and redistribute.</p>
<p>You might also find something useful at <a href=""http://globalwordnet.org/resources/global-wordnet-grid/"" rel=""nofollow noreferrer"">http://globalwordnet.org/resources/global-wordnet-grid/</a>    Obviously most of them are not for English, but there are a few multilingual ones in there, that might be worth evaluating?</p>
<p>Another approach would be to create a wrapper function. It first searches a lookup list of fixes and additions you think should be in there. If not found then it searches WordNet as normal. This allows you to add <code>'succeed', 'success', 'successful'</code>, and then other sets of words as end users point out something missing.</p>
",""
"70667079","2022-01-11 12:49:23","0","","70607224","<p>Pointed out by a Contributor of HuggingFace, on this <a href=""https://github.com/huggingface/transformers/issues/15087"" rel=""nofollow noreferrer"">Git Issue</a>,</p>
<blockquote>
<p>The library previously named LPOT has been renamed to Intel Neural Compressor (INC), which resulted in a change in the name of our subpackage from <code>lpot</code> to <code>neural_compressor</code>.
The correct way to import would now be from <code>optimum.intel.neural_compressor.quantization import IncQuantizerForSequenceClassification</code>
Concerning the <code>graphcore</code> subpackage, you need to install it first with <code>pip install optimum[graphcore]</code>
Furthermore you'll need to have access to an IPU in order to use it.</p>
</blockquote>
<hr />
<p><strong>Solution</strong></p>
<pre><code>! pip install datasets transformers optimum[graphcore]
</code></pre>
<p>Instead of:</p>
<pre class=""lang-py prettyprint-override""><code>from optimum.intel.lpot.quantization import LpotQuantizerForSequenceClassification
from optimum.intel.lpot.pruning import LpotPrunerForSequenceClassification
</code></pre>
<pre class=""lang-py prettyprint-override""><code>from optimum.intel.neural_compressor.quantization import IncQuantizerForSequenceClassification
from optimum.intel.neural_compressor.pruning import IncPrunerForSequenceClassification
</code></pre>
",""
"70619775","2022-01-07 10:22:02","0","","70609579","<p>The <code>pipeline</code> approach won't work for Quantisation as we need the models to be returned. You can however, use <code>pipeline</code> for testing the original <a href=""https://huggingface.co/models?pipeline_tag=fill-mask&amp;language=en&amp;library=pytorch&amp;dataset=dataset:bookcorpus&amp;sort=downloads"" rel=""nofollow noreferrer"">models</a> for timing etc.</p>
<hr />
<p><strong>Quantisation Code:</strong></p>
<p><code>token_logits</code> contains the tensors of the quantised model.</p>
<p>You could place a <code>for-loop</code> around this code, and replace <code>model_name</code> with <code>string</code> from a <code>list</code>.</p>
<pre class=""lang-py prettyprint-override""><code>model_name = bert-base-uncased
tokenizer = AutoTokenizer.from_pretrained(model_name )
model = AutoModelForMaskedLM.from_pretrained(model_name)
    
sequence = &quot;Distilled models are smaller than the models they mimic. Using them instead of the large &quot; \
f&quot;versions would help {tokenizer.mask_token} our carbon footprint.&quot;

inputs = tokenizer(sequence, return_tensors=&quot;pt&quot;)
mask_token_index = torch.where(inputs[&quot;input_ids&quot;] == tokenizer.mask_token_id)[1]
    
token_logits = model(**inputs).logits

# &lt;- can stop here
</code></pre>
<p><a href=""https://huggingface.co/models?sort=downloads"" rel=""nofollow noreferrer"">Source</a></p>
",""
"70562037","2022-01-03 05:37:40","5","","70546666","<p>Here is a clean way to implement your intended approach.</p>
<pre><code># put your nouns of interest here
NOUN_LIST = [&quot;doctor&quot;, ...]

def find_stuff(text):
    doc = nlp(text)
    if len(doc) &lt; 4: return None # too short
    
    for tok in doc[3:]:
        if tok.pos_ == &quot;NOUN&quot; and tok.text in NOUN_LIST and doc[tok.i-3].pos_ == &quot;PRON&quot;:
            return (doc[tok.i-3].text, tok.text)
</code></pre>
<p>As the other answer mentioned, your approach here is wrong though. You want the subject and object (or predicate nominative) of the sentence. You should use the <a href=""https://spacy.io/usage/rule-based-matching#dependencymatcher"" rel=""nofollow noreferrer"">DependencyMatcher</a> for that. Here's an example:</p>
<pre><code>from spacy.matcher import DependencyMatcher
import spacy

nlp = spacy.load(&quot;en_core_web_sm&quot;)
doc = nlp(&quot;she is a good person&quot;)

pattern = [
  # anchor token: verb, usually &quot;is&quot;
  {
    &quot;RIGHT_ID&quot;: &quot;verb&quot;,
    &quot;RIGHT_ATTRS&quot;: {&quot;POS&quot;: &quot;AUX&quot;}
  },
  # verb -&gt; pronoun
  {
    &quot;LEFT_ID&quot;: &quot;verb&quot;,
    &quot;REL_OP&quot;: &quot;&gt;&quot;,
    &quot;RIGHT_ID&quot;: &quot;pronoun&quot;,
    &quot;RIGHT_ATTRS&quot;: {&quot;DEP&quot;: &quot;nsubj&quot;, &quot;POS&quot;: &quot;PRON&quot;}
  },
  # predicate nominatives have &quot;attr&quot; relation
  {
    &quot;LEFT_ID&quot;: &quot;verb&quot;,
    &quot;REL_OP&quot;: &quot;&gt;&quot;,
    &quot;RIGHT_ID&quot;: &quot;target&quot;,
    &quot;RIGHT_ATTRS&quot;: {&quot;DEP&quot;: &quot;attr&quot;, &quot;POS&quot;: &quot;NOUN&quot;}
  }
]

matcher = DependencyMatcher(nlp.vocab)
matcher.add(&quot;PREDNOM&quot;, [pattern])
matches = matcher(doc)

for match_id, (verb, pron, target) in matches:
    print(doc[pron], doc[verb], doc[target])
</code></pre>
<p>You can check dependency relations using <a href=""https://explosion.ai/demos/displacy"" rel=""nofollow noreferrer"">displacy</a>. You can learn more about what they are in the <a href=""https://web.stanford.edu/%7Ejurafsky/slp3/"" rel=""nofollow noreferrer"">Jurafsky and Martin book</a>.</p>
",""
"70538225","2021-12-31 01:30:13","0","","70529754","<p>From a quick review of your method, I suggest you to call <code>pos_tag</code> outside of the <code>for</code> loop. Otherwise, you call this method for every word, which could be slow. This could already speed up the process a bit, depending on the complexity of <code>pos_tag</code>.</p>
<p>Note: I suggest you using <code>tqdm</code>. This gives you a nice progress bar and lets you estimate how long it takes.</p>
<pre><code>from tqdm import tqdm

def pre_process(text):
    words_only = words_only.lower().split()    

    lem = WordNetLemmatizer()
    words_only1=[]
    pos_tags = pos_tag(words_only)
    for word, word_pos_tag in tqdm(zip(words_only, pos_tags), total=len(words_only)):
        pos_label = word_pos_tag[1][0].lower()
        if pos_label == 'j': 
            pos_label = 'a'    # 'j' &lt;--&gt; 'a' reassignment
        
        if pos_label in ['r']:  # For adverbs it's a bit different
            try:
                word=wordnet.synset(word+'.r.1').lemmas()[0].pertainyms()[0].name() # Could have errors for words like 'not'
            except:
                word=lem.lemmatize(word)

        elif pos_label in ['a', 's', 'v']: # For adjectives and verbs
            word=lem.lemmatize(word, pos=pos_label)

        else:   # For nouns and everything else as it is the default kwarg
            word=lem.lemmatize(word)
        
        words_only1.append(word)
    
    return(&quot; &quot;.join(words_only1)) 
</code></pre>
",""
"70472243","2021-12-24 11:08:06","0","","70472067","<p>The default lemmatization <code>pos</code> (Part Of Speech) is noun for the <code>lemmatize</code> method. And it produces the output <code>battling</code>.</p>
<p>If you change the <code>pos</code> to a verb, as is the case here, you get the proper result.</p>
<pre><code>lemmatizer.lemmatize(&quot;battling&quot;, wordnet.VERB)
</code></pre>
<p>will give the base <code>battle</code></p>
",""
"70347017","2021-12-14 10:03:21","2","","70339341","<p>Machine translation generally works on sentences, as the context in which a word is used changes its meaning. There is no point in word-for-word translation.</p>
<p>So what will have happened is that your word <em>doente</em> usually occurs in sentences whose English translation is <em>I feel sick</em>; that is the minimal context. As these are all just characters to the machine, there is no 'understanding' that only the <code>sick</code> part corresponds to <em>doente</em> from the point of view of a human being.</p>
<p>If you want to translate words, use a bilingual dictionary; I doubt that there are word-based models for this, as decades of research in machine translation have shown that you need larger chunks of language for translating than just words.</p>
",""
"70328718","2021-12-13 00:06:14","0","","70328615","<p>You can use list comprehension :</p>
<pre><code>[snowball.stem(word) for word in my_words]
</code></pre>
",""
"70198066","2021-12-02 10:49:57","0","","70189982","<p>Instead of using the full Levenshtein distance (which is slow to compute), you could do a couple of sanity checks beforehand, to try and exclude candidates which are obviously wrong:</p>
<ol>
<li>word length: <em>the</em> will never match <em>brown fox</em>, as it is far too short. Count the word length, and exclude all candidates that are more than a few letters shorter or longer.</li>
<li>letters: just check what letters are in the word. for example, <em>the</em> does not contain a single letter from <em>fox</em>, so you can rule it out straightaway. With short words it might not make a big difference in performance, but for longer words it will do. Additional optimisation: look for rare characters (x,q,w) first, or simply ignore common ones (e,t,s) which are more likely to be present anyway.</li>
</ol>
<p>Heuristics such as these will of course not give you the right answer, but they can help to filter out those that are definitely not going to match. Then you only need to perform the more expensive full check on a much smaller number of candidate words.</p>
",""
"70131406","2021-11-27 01:17:33","0","","70130957","<p>I would recommend creating a separate function to encapsulate all that sentiment analysis logic. In the end, you would call it like this:</p>
<pre class=""lang-py prettyprint-override""><code>df['SENTIMENT_SCORE'] = df['CONTENT'].apply(safe_complex_function)
</code></pre>
<p><code>safe_complex_funtion</code> would be your brand new safe function. Give it the name you want. It would be probably something like this:</p>
<pre class=""lang-py prettyprint-override""><code>def sentiment_scores(content):
    try:
        response = natural_language_understanding.analyze(
            text=content,
            features=Features(
                sentiment=SentimentOptions(targets=[&quot;Irish&quot;,])
            )
        ).get_result()
        json_tbl = pd.json_normalize(
            response['sentiment'], 
            record_path='targets',
            meta=[['document','score'], ['document','label']]
        )
        return json_tbl.set_index([pd.Index([index])])
    except &lt;The specific Exception you want to deal&gt;: # please don't put Exception. It is too general
        return None
</code></pre>
<p>Here is an example code:</p>
<h3>Creating a test Dataframe</h3>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

data = [
    (1, 'I am happy'),
    (2, 'I am sad'),
    (3, 'I am neutral'),
    (4, 'Exception generator')
]

df = pd.DataFrame(data,columns=['USER_ID','CONTENT'])

</code></pre>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: right;""></th>
<th style=""text-align: right;"">USER_ID</th>
<th style=""text-align: left;"">CONTENT</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: right;"">0</td>
<td style=""text-align: right;"">1</td>
<td style=""text-align: left;"">I am happy</td>
</tr>
<tr>
<td style=""text-align: right;"">1</td>
<td style=""text-align: right;"">2</td>
<td style=""text-align: left;"">I am sad</td>
</tr>
<tr>
<td style=""text-align: right;"">2</td>
<td style=""text-align: right;"">3</td>
<td style=""text-align: left;"">I am neutral</td>
</tr>
<tr>
<td style=""text-align: right;"">3</td>
<td style=""text-align: right;"">4</td>
<td style=""text-align: left;"">Exception generator</td>
</tr>
</tbody>
</table>
</div><h3>Creating a mocking sentiment analysis function</h3>
<p>This function is solely for mocking.</p>
<pre class=""lang-py prettyprint-override""><code>def fake_sentiment_analysis(content):
    sentiment_scores = {
        'sad': -1,
        'happy': 1,
        'neutral': 0
    }
    for sentiment, score in sentiment_scores.items():
        if sentiment in content:
            return score
    ## rasises KeyError error only for demonstration purposes
    return sentiment_scores['BROKEN']

def complex_function(element):
    sentiment_score = fake_sentiment_analysis(element)
    return sentiment_score
</code></pre>
<h3>Applying that non-safe function on DataFrame</h3>
<p>You would got <code>KeyError</code> calling that function</p>
<pre class=""lang-py prettyprint-override""><code>df['CONTENT'].apply(complex_function)
</code></pre>
<pre><code>---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
&lt;ipython-input-22-d418b58879b8&gt; in &lt;module&gt;()
----&gt; 1 df['CONTENT'].apply(complex_function)

2 frames
pandas/_libs/lib.pyx in pandas._libs.lib.map_infer()

&lt;ipython-input-12-538de52b436a&gt; in fake_sentiment_analysis(content)
      9             return score
     10     ## rasises KeyError error only for demonstration purposes
---&gt; 11     return sentiment_scores['BROKEN']

KeyError: 'BROKEN'
</code></pre>
<h3>Adding Exception handler</h3>
<p>You can make it safer adding exception handling</p>
<pre class=""lang-py prettyprint-override""><code>def safe_complex_function(element):
    try:
        sentiment_score = fake_sentiment_analysis(element)
    except KeyError:
        sentiment_score = None
    return sentiment_score
</code></pre>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: right;""></th>
<th style=""text-align: right;"">USER_ID</th>
<th style=""text-align: left;"">CONTENT</th>
<th style=""text-align: right;"">SENTIMENT_SCORE</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: right;"">0</td>
<td style=""text-align: right;"">1</td>
<td style=""text-align: left;"">I am happy</td>
<td style=""text-align: right;"">1</td>
</tr>
<tr>
<td style=""text-align: right;"">1</td>
<td style=""text-align: right;"">2</td>
<td style=""text-align: left;"">I am sad</td>
<td style=""text-align: right;"">-1</td>
</tr>
<tr>
<td style=""text-align: right;"">2</td>
<td style=""text-align: right;"">3</td>
<td style=""text-align: left;"">I am neutral</td>
<td style=""text-align: right;"">0</td>
</tr>
<tr>
<td style=""text-align: right;"">3</td>
<td style=""text-align: right;"">4</td>
<td style=""text-align: left;"">Exception generator</td>
<td style=""text-align: right;"">nan</td>
</tr>
</tbody>
</table>
</div>",""
"70109775","2021-11-25 10:48:41","2","","70108900","<p>Just use the <code>sklearn.metrics.rand_score</code> function:</p>
<pre><code>from sklearn.metrics import rand_score

rand_score(labels_true, labels_pred)
</code></pre>
<p>It doesn't matter if true labels and predicted labels have values in different domains. Please have a look at the examples:</p>
<pre><code>&gt;&gt;&gt; rand_score(['a', 'b', 'c'], [5, 6, 7])
1.0
&gt;&gt;&gt; rand_score([0, 1, 2], [5, 6, 7])
1.0
&gt;&gt;&gt; rand_score(['a', 'a', 'b'], [0, 1, 2])
0.6666666666666666
&gt;&gt;&gt; rand_score(['a', 'a', 'b'], [7, 7, 2])
1.0
</code></pre>
",""
"69893105","2021-11-09 05:10:09","0","","69883861","<p>The way the Entity Linker works is that, given all potential candidates for an entity, it picks the most likely one.</p>
<p>The issue you are running into is that your florist is not known to the model, so he is not a candidate. Because the only Barack Obama the model knows about is the former US President, the model can say with certainty that &quot;Barack Obama&quot; must refer to the president.</p>
<p>The model has no mechanism to tell if a reference refers to an entity not in the knowledge base. It will also never abstain, and if there are candidates it will pick one. &quot;NIL&quot; is not an abstention, it's for when a reference has no entries in the knowledge base, so there's nothing to pick from.</p>
<p>This may be clearer if you look at the <a href=""https://github.com/explosion/projects/tree/v3/tutorials/nel_emerson"" rel=""nofollow noreferrer"">example project</a>, which uses &quot;Emerson&quot; as an example. The model doesn't decide if &quot;Emerson&quot; is a person it knows or not - it assumes that it must be one of the people it knows, and it has to pick which one is most likely.</p>
",""
"69861501","2021-11-06 04:38:34","3","","69861444","<p>Looking at the library code, it looks like hypothesis should be an iterable. <a href=""https://www.nltk.org/_modules/nltk/translate/meteor_score.html"" rel=""nofollow noreferrer"">https://www.nltk.org/_modules/nltk/translate/meteor_score.html</a>. The error is coming from:</p>
<pre><code>if isinstance(hypothesis, str):
        raise TypeError(
            f'&quot;hypothesis&quot; expects pre-tokenized hypothesis (Iterable[str]): {hypothesis}'
        )
</code></pre>
<p>Try putting &quot;an apple on this tree&quot; in a list.</p>
",""
"69783986","2021-10-31 04:42:47","0","","69779095","<p>The problem is you aren't going to be able to keep all the Docs (spaCy output) in memory at the same time, so you can't just put the output in a column of a dataframe. Also note this is not a spaCy issue, this is a programming issue.</p>
<p>You need to write a for loop and put your processing in it:</p>
<pre><code>for text in texts:
    doc = nlp(text)
    ... do something with the doc ...
</code></pre>
<p>If you do this then the doc will be cleaned up on the next iteration of the for loop, so it won't take up memory.</p>
<p>You may also want to look at the <a href=""https://github.com/explosion/spaCy/discussions/8402"" rel=""nofollow noreferrer"">spaCy speed FAQ</a>.</p>
",""
"69683069","2021-10-22 21:05:38","7","","69628487","<p>The <a href=""https://huggingface.co/transformers/master/main_classes/pipelines.html#transformers.ZeroShotClassificationPipeline"" rel=""noreferrer"">ZeroShotClassificationPipeline</a> is currently not supported by <a href=""https://shap.readthedocs.io/en/latest/index.html"" rel=""noreferrer"">shap</a>, but you can use a workaround. The workaround is required because:</p>
<ol>
<li>The shap Explainer forwards only one parameter to the model (a pipeline in this case), but the ZeroShotClassificationPipeline requires two parameters, namely text, and labels.</li>
<li>The shap Explainer will access the config of your model and use its <code>label2id</code> and <code>id2label</code> properties. They do not match the labels returned from the ZeroShotClassificationPipeline and will result in an error.</li>
</ol>
<p>Below is a suggestion for one possible workaround. I recommend opening an issue at <a href=""https://github.com/slundberg/shap"" rel=""noreferrer"">shap</a> and requesting official support for huggingface's ZeroShotClassificationPipeline.</p>
<pre class=""lang-py prettyprint-override""><code>import shap
from transformers import AutoModelForSequenceClassification, AutoTokenizer, ZeroShotClassificationPipeline
from typing import Union, List

weights = &quot;valhalla/distilbart-mnli-12-3&quot;

model = AutoModelForSequenceClassification.from_pretrained(weights)
tokenizer = AutoTokenizer.from_pretrained(weights)

# Create your own pipeline that only requires the text parameter 
# for the __call__ method and provides a method to set the labels
class MyZeroShotClassificationPipeline(ZeroShotClassificationPipeline):
    # Overwrite the __call__ method
    def __call__(self, *args):
      o = super().__call__(args[0], self.workaround_labels)[0]

      return [[{&quot;label&quot;:x[0], &quot;score&quot;: x[1]}  for x in zip(o[&quot;labels&quot;], o[&quot;scores&quot;])]]

    def set_labels_workaround(self, labels: Union[str,List[str]]):
      self.workaround_labels = labels

example_text = &quot;This is an example text about snowflakes in the summer&quot;
labels = [&quot;weather&quot;,&quot;sports&quot;]

# In the following, we address issue 2.
model.config.label2id.update({v:k for k,v in enumerate(labels)})
model.config.id2label.update({k:v for k,v in enumerate(labels)})

pipe = MyZeroShotClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=True)
pipe.set_labels_workaround(labels)

def score_and_visualize(text):
    prediction = pipe([text])
    print(prediction[0])

    explainer = shap.Explainer(pipe)
    shap_values = explainer([text])

    shap.plots.text(shap_values)


score_and_visualize(example_text)
</code></pre>
<p>Output:
<a href=""https://i.sstatic.net/RQXp4.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/RQXp4.png"" alt=""shap output"" /></a></p>
",""
"69610933","2021-10-18 04:53:40","0","","69433514","<p>First you have to understand, which factors actually increases the running time. Following are these factors:</p>
<ol>
<li>The large input size.</li>
<li>The data structure; shifted mean, and unnormalized.</li>
<li>The large network depth, and/or width.</li>
<li>Large number of epochs.</li>
<li>The batch size not compatible with physical available memory.</li>
<li>Very small or high learning rate.</li>
</ol>
<p>For fast running, make sure to work on the above factors, like:</p>
<ol>
<li>Reduce the input size to the appropriate dimensions that assures no loss in important features.</li>
<li>Always preprocess the input to make it zero mean, and normalized it by dividing it by std. deviation or difference in max, min values.</li>
<li>Keep the network depth and width that is not to high or low. Or always use the standard architecture that are theoretically proven.</li>
<li>Always make sure of the epochs. If you are not able to make any further improvements in your error or accuracy beyond a defined threshold, then there is no need to take more epochs.</li>
<li>The batch size should be decided based on the available memory, and number of CPUs/GPUs. If the batch cannot be loaded fully in memory, then this will lead to slow processing due to lots of paging between memory and the filesystem.</li>
<li>Appropriate learning rate should be determine by trying multiple, and using that which gives the best reduction in error w.r.t. number of epochs.</li>
</ol>
",""
"69532366","2021-10-11 20:51:25","1","","69426006","<p>You can build your own tokenizer following this tutorial <a href=""https://www.tensorflow.org/text/guide/subwords_tokenizer"" rel=""nofollow noreferrer"">https://www.tensorflow.org/text/guide/subwords_tokenizer</a></p>
<p>It is the exact same way they build the ted_hrlr_translate_pt_en_converter tokenizer in the transformers example, you just need to adjust it to your language.</p>
<p>I rewrote it for your case but didn't test it:</p>
<pre class=""lang-py prettyprint-override""><code>import collections
import logging
import os
import pathlib
import re
import string
import sys
import time
import numpy as np
#import matplotlib.pyplot as plt

import tensorflow_datasets as tfds
import tensorflow_text as text
import tensorflow as tf
from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab



examples, metadata = tfds.load('wmt14_translate/de-en', with_info=True,
                               as_supervised=True)
train_examples, val_examples = examples['train'], examples['validation']

for de_examples, en_examples in train_examples.batch(3).take(1):
  for pt in de_examples.numpy():
    print(pt.decode('utf-8'))

  print()

  for en in en_examples.numpy():
    print(en.decode('utf-8'))

train_en = train_examples.map(lambda de, en: en)
train_de = train_examples.map(lambda de, en: de)

bert_tokenizer_params=dict(lower_case=True)
reserved_tokens=[&quot;[PAD]&quot;, &quot;[UNK]&quot;, &quot;[START]&quot;, &quot;[END]&quot;]

bert_vocab_args = dict(
    # The target vocabulary size
    vocab_size = 8000,
    # Reserved tokens that must be included in the vocabulary
    reserved_tokens=reserved_tokens,
    # Arguments for `text.BertTokenizer`
    bert_tokenizer_params=bert_tokenizer_params,
    # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`
    learn_params={},
)

de_vocab = bert_vocab.bert_vocab_from_dataset(
    train_de.batch(1000).prefetch(2),
    **bert_vocab_args
)

print(de_vocab[:10])
print(de_vocab[100:110])
print(de_vocab[1000:1010])
print(de_vocab[-10:])

def write_vocab_file(filepath, vocab):
  with open(filepath, 'w') as f:
    for token in vocab:
      print(token, file=f)

write_vocab_file('de_vocab.txt', de_vocab)

en_vocab = bert_vocab.bert_vocab_from_dataset(
    train_en.batch(1000).prefetch(2),
    **bert_vocab_args
)

print(en_vocab[:10])
print(en_vocab[100:110])
print(en_vocab[1000:1010])
print(en_vocab[-10:])

write_vocab_file('en_vocab.txt', en_vocab)

de_tokenizer = text.BertTokenizer('de_vocab.txt', **bert_tokenizer_params)
en_tokenizer = text.BertTokenizer('en_vocab.txt', **bert_tokenizer_params)

# Tokenize the examples -&gt; (batch, word, word-piece)
token_batch = en_tokenizer.tokenize(en_examples)
# Merge the word and word-piece axes -&gt; (batch, tokens)
token_batch = token_batch.merge_dims(-2,-1)

for ex in token_batch.to_list():
  print(ex)

# Lookup each token id in the vocabulary.
txt_tokens = tf.gather(en_vocab, token_batch)
# Join with spaces.
tf.strings.reduce_join(txt_tokens, separator=' ', axis=-1)

words = en_tokenizer.detokenize(token_batch)
tf.strings.reduce_join(words, separator=' ', axis=-1)

START = tf.argmax(tf.constant(reserved_tokens) == &quot;[START]&quot;)
END = tf.argmax(tf.constant(reserved_tokens) == &quot;[END]&quot;)

def add_start_end(ragged):
  count = ragged.bounding_shape()[0]
  starts = tf.fill([count,1], START)
  ends = tf.fill([count,1], END)
  return tf.concat([starts, ragged, ends], axis=1)

words = en_tokenizer.detokenize(add_start_end(token_batch))
tf.strings.reduce_join(words, separator=' ', axis=-1)

def cleanup_text(reserved_tokens, token_txt):
  # Drop the reserved tokens, except for &quot;[UNK]&quot;.
  bad_tokens = [re.escape(tok) for tok in reserved_tokens if tok != &quot;[UNK]&quot;]
  bad_token_re = &quot;|&quot;.join(bad_tokens)

  bad_cells = tf.strings.regex_full_match(token_txt, bad_token_re)
  result = tf.ragged.boolean_mask(token_txt, ~bad_cells)

  # Join them into strings.
  result = tf.strings.reduce_join(result, separator=' ', axis=-1)

  return result

token_batch = en_tokenizer.tokenize(en_examples).merge_dims(-2,-1)
words = en_tokenizer.detokenize(token_batch)

cleanup_text(reserved_tokens, words).numpy()

class CustomTokenizer(tf.Module):
  def __init__(self, reserved_tokens, vocab_path):
    self.tokenizer = text.BertTokenizer(vocab_path, lower_case=True)
    self._reserved_tokens = reserved_tokens
    self._vocab_path = tf.saved_model.Asset(vocab_path)

    vocab = pathlib.Path(vocab_path).read_text().splitlines()
    self.vocab = tf.Variable(vocab)

    ## Create the signatures for export:

    # Include a tokenize signature for a batch of strings.
    self.tokenize.get_concrete_function(
        tf.TensorSpec(shape=[None], dtype=tf.string))

    # Include `detokenize` and `lookup` signatures for:
    #   * `Tensors` with shapes [tokens] and [batch, tokens]
    #   * `RaggedTensors` with shape [batch, tokens]
    self.detokenize.get_concrete_function(
        tf.TensorSpec(shape=[None, None], dtype=tf.int64))
    self.detokenize.get_concrete_function(
          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))

    self.lookup.get_concrete_function(
        tf.TensorSpec(shape=[None, None], dtype=tf.int64))
    self.lookup.get_concrete_function(
          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))

    # These `get_*` methods take no arguments
    self.get_vocab_size.get_concrete_function()
    self.get_vocab_path.get_concrete_function()
    self.get_reserved_tokens.get_concrete_function()

  @tf.function
  def tokenize(self, strings):
    enc = self.tokenizer.tokenize(strings)
    # Merge the `word` and `word-piece` axes.
    enc = enc.merge_dims(-2,-1)
    enc = add_start_end(enc)
    return enc

  @tf.function
  def detokenize(self, tokenized):
    words = self.tokenizer.detokenize(tokenized)
    return cleanup_text(self._reserved_tokens, words)

  @tf.function
  def lookup(self, token_ids):
    return tf.gather(self.vocab, token_ids)

  @tf.function
  def get_vocab_size(self):
    return tf.shape(self.vocab)[0]

  @tf.function
  def get_vocab_path(self):
    return self._vocab_path

  @tf.function
  def get_reserved_tokens(self):
    return tf.constant(self._reserved_tokens)

tokenizers = tf.Module()
tokenizers.pt = CustomTokenizer(reserved_tokens, 'de_vocab.txt')
tokenizers.en = CustomTokenizer(reserved_tokens, 'en_vocab.txt')

model_name = 'ted_hrlr_translate_de_en_converter'
tf.saved_model.save(tokenizers, model_name)
</code></pre>
",""
"69521809","2021-10-11 06:20:26","3","","69520218","<p>You are getting the error because in some dfm objects you create, not all of the features in <code>thetarget</code> are in the object <code>dfm</code> you have created.</p>
<p>Here's a way to avoid that, using <code>docfreq()</code>:</p>
<pre class=""lang-r prettyprint-override""><code>library(&quot;quanteda&quot;)
## Package version: 3.1.0
## Unicode version: 13.0
## ICU version: 69.1
## Parallel computing: 12 of 12 threads used.
## See https://quanteda.io for tutorials and examples.

thetarget &lt;- c(&quot;nuclear&quot;, &quot;congress&quot;, &quot;economy&quot;, &quot;_not_a_feature_&quot;)

dfmat &lt;- tokens(data_corpus_inaugural) %&gt;%
  tokens_select(thetarget) %&gt;%
  dfm()

docfreq(dfmat) / ndoc(dfmat)
##    economy   congress    nuclear 
## 0.52542373 0.49152542 0.08474576
</code></pre>
<p>To get the data.frame in the question:</p>
<pre class=""lang-r prettyprint-override""><code>df &lt;- data.frame(
  docname = docnames(dfmat),
  Year = docvars(dfmat, c(&quot;Year&quot;)),
  contains_target = as.logical(rowSums(dfmat)),
  row.names = NULL
)

head(df)
##           docname Year contains_target
## 1 1789-Washington 1789            TRUE
## 2 1793-Washington 1793           FALSE
## 3      1797-Adams 1797            TRUE
## 4  1801-Jefferson 1801            TRUE
## 5  1805-Jefferson 1805           FALSE
## 6    1809-Madison 1809            TRUE
</code></pre>
",""
"69328310","2021-09-25 17:03:53","7","","69327798","<p>I think it's very enlightening to look at the source for the <code>Monad</code> instance of <code>ExceptT</code>:</p>
<pre><code>newtype ExceptT e m a = ExceptT (m (Either e a))

instance (Monad m) =&gt; Monad (ExceptT e m) where
    return a = ExceptT $ return (Right a)
    m &gt;&gt;= k = ExceptT $ do
        a &lt;- runExceptT m
        case a of
            Left e -&gt; return (Left e)
            Right x -&gt; runExceptT (k x)
</code></pre>
<p>If you ignore the <code>newtype</code> wrapping and unwrapping, it becomes even simpler:</p>
<pre><code>m &gt;&gt;= k = do
    a &lt;- m
    case a of
        Left e -&gt; return (Left e)
        Right x -&gt; k x
</code></pre>
<p>Or, as you seem to prefer not using <code>do</code>:</p>
<pre><code>m &gt;&gt;= k = m &gt;&gt;= \a -&gt; case a of
    Left e -&gt; return (Left e)
    Right x -&gt; k x
</code></pre>
<p>Does that chunk of code look familiar to you? The only difference between that and your code is that you write <code>printErrorAndExit</code> instead of <code>return . Left</code>! So, let's move that <code>printErrorAndExit</code> out to the top-level, and simply be happy <em>remembering</em> the error for now and not printing it.</p>
<pre><code>simpleVersion :: Integer -&gt; Config -&gt; IO (Either Err ())
simpleVersion id c = connect c &gt;&gt;= \case (Left e)     -&gt; return (Left e)
                                         (Right conn) -&gt; (run . query id $ conn)
                                                          &gt;&gt;= \case (Left e)  -&gt; return (Left e)
                                                                    (Right r) -&gt; Right &lt;$&gt; (print r
                                                          &gt;&gt; release conn)
</code></pre>
<p>Besides the change I called out, you also have to stick a <code>Right &lt;$&gt;</code> at the end to convert from an <code>IO ()</code> action to an <code>IO (Either Err ())</code> action. (More on this momentarily.)</p>
<p>Okay, let's try substituting our <code>ExceptT</code> bind from above for the <code>IO</code> bind. I'll add a <code>'</code> to distinguish the <code>ExceptT</code> versions from the <code>IO</code> versions (e.g. <code>&gt;&gt;=' :: IO (Either Err a) -&gt; (a -&gt; IO (Either Err b)) -&gt; IO (Either Err b)</code>).</p>
<pre><code>simpleVersion id c = connect c &gt;&gt;=' \conn -&gt; (run . query id $ conn)
                                             &gt;&gt;=' \r -&gt; Right &lt;$&gt; (print r
                                             &gt;&gt; {- IO &gt;&gt;! -} release conn)
</code></pre>
<p>That's already an improvement, and some whitespace changes make it even better. I'll also include a <code>do</code> version.</p>
<pre><code>simpleVersion id c =
    connect c &gt;&gt;=' \conn -&gt;
    (run . query id $ conn) &gt;&gt;=' \r -&gt;
    Right &lt;$&gt; (print r &gt;&gt; release conn)

simpleVersion id c = do
    conn &lt;- connect c
    r &lt;- run . query id $ conn
    Right &lt;$&gt; (print r &gt;&gt; release conn)
</code></pre>
<p>To me, that looks pretty clean! Of course, in <code>main</code>, you'll still want to <code>printErrorAndExit</code>, as in:</p>
<pre><code>main = do
    v &lt;- runExceptT (simpleVersion 0 defaultConfig)
    either printErrorAndExit pure v
</code></pre>
<p>Now, about that <code>Right &lt;$&gt; (...)</code>... I said I wanted to convert from <code>IO a</code> to <code>IO (Either Err a)</code>. Well, this kind of thing is why the <code>MonadTrans</code> class exists; let's look at its implementation for <code>ExceptT</code>:</p>
<pre><code>instance MonadTrans (ExceptT e) where
    lift = ExceptT . liftM Right
</code></pre>
<p>Well, <code>liftM</code> and <code>(&lt;$&gt;)</code> are the same function with different names. So if we ignore the <code>newtype</code> wrapping and unwrapping, we get</p>
<pre><code>lift m = Right &lt;$&gt; m
</code></pre>
<p>! So:</p>
<pre><code>simpleVersion id c = do
    conn &lt;- connect c
    r &lt;- run . query id $ conn
    lift (print r &gt;&gt; release conn)
</code></pre>
<p>You could also choose to use <code>liftIO</code> if you like. The difference is that <code>lift</code> always lifts a monadic action up through exactly one transformer, but works for any pair of wrapped type and transformer type; while <code>liftIO</code> lifts an <code>IO</code> action up through as many transformers as necessary for your monad transformer stack, but only works for <code>IO</code> actions.</p>
<p>Of course, so far we've elided all the <code>newtype</code> wrapping and unwrapping. For <code>simpleVersion</code> to be as beautiful as it is in our last example here, you'd need to change <code>connect</code> and <code>run</code> to include those wrappers as appropriate.</p>
",""
"69253311","2021-09-20 11:00:43","1","","69253108","<p>I think this could work.</p>
<pre><code>a = [[('‡∂Ø‡∑ä‡∑Ä‡∑í‡∂¥‡∑è‡∂ª‡∑ä‡∑Å‡∑Ä‡∑í‡∂ö', 'NNP'), ('‡∂ë‡∂ö‡∂ü‡∂≠‡∑è', 'NNP'), ('‡∂¢‡∂±', 'JJ'), ('‡∂¢‡∑ì‡∑Ä‡∑í‡∂≠', 'NNJ'), ('‡∑É‡∑û‡∂õ‡∑ä‡∂∫‡∂∫', 'NNC'), ('‡∂∏‡∂±‡∑è‡∑Ä', 'RB')]]

def foo (data):
    result = []
    if type(data) == tuple:
        return data[1]

    if type(data) == list:
        for inner in data:
            result.append(foo(inner))
    
    return result

result = foo (a)
</code></pre>
",""
"69223273","2021-09-17 12:15:19","2","","69218494","<p>In your output data, <code>rawFeatures</code> and <code>features</code> are sparse vectors and it has 3 parts,<code>size</code>,<code>indices</code>,<code>value</code>.</p>
<p>for eg,<code>(262144,[32755,44691,64441,179674,262052],[0.44628710262841953,0.44628710262841953,0.22314355131420976,0.5108256237659907,0.22314355131420976])</code></p>
<p>here
<code>size = 262144 ,indices = [32755,44691,64441,179674,262052] , values = [0.44628710262841953,0.44628710262841953,0.22314355131420976,0.5108256237659907,0.22314355131420976]</code></p>
<p>the indices are index values mapped to hash values of respective word.
from <a href=""https://spark.apache.org/docs/latest/mllib-feature-extraction.html#tf-idf"" rel=""nofollow noreferrer"">https://spark.apache.org/docs/latest/mllib-feature-extraction.html#tf-idf</a><br />
<code>Our implementation of term frequency utilizes the hashing trick. A raw feature is mapped into an index (term) by applying a hash function. Then term frequencies are calculated based on the mapped indices.</code></p>
<p>Now to read output vector and map to word, we can use hash values of each word using same fitted model and map with <code>features</code> vector indices and get corresponding values.</p>
<p>1.first we get hashed index values for each word.</p>
<pre><code>ndf = wordsData.select('label',f.explode('words').name('expwords')).withColumn('words',f.array('expwords'))
hashudf = f.udf(lambda vector : vector.indices.tolist()[0],StringType())
wordtf = hashingTF.transform(ndf).withColumn('wordhash',hashudf(f.col('rawFeatures')))
wordtf.show()
+-----+--------+---------+--------------------+--------+
|label|expwords|    words|         rawFeatures|wordhash|
+-----+--------+---------+--------------------+--------+
|    0|   Spark|  [Spark]|(262144,[179674],...|  179674|
|    0| Awesome|[Awesome]|(262144,[262052],...|  262052|
|    0|   World|  [World]|(262144,[32755],[...|   32755|
|    0|   Hello|  [Hello]|(262144,[44691],[...|   44691|
|    0|      Is|     [Is]|(262144,[64441],[...|   64441|
|    2|   Hello|  [Hello]|(262144,[44691],[...|   44691|
|    2|   World|  [World]|(262144,[32755],[...|   32755|
|    1|      Is|     [Is]|(262144,[64441],[...|   64441|
|    1|   World|  [World]|(262144,[32755],[...|   32755|
|    1| Awesome|[Awesome]|(262144,[262052],...|  262052|
|    1|   Spark|  [Spark]|(262144,[179674],...|  179674|
|    1|   Hello|  [Hello]|(262144,[44691],[...|   44691|
|    3| Awesome|[Awesome]|(262144,[262052],...|  262052|
|    3|  PYTHON| [PYTHON]|(262144,[191247],...|  191247|
|    3|  Pretty| [Pretty]|(262144,[61511],[...|   61511|
|    3|      Is|     [Is]|(262144,[64441],[...|   64441|
+-----+--------+---------+--------------------+--------+
</code></pre>
<p>2.flatten output <code>features</code> column to get <code>indices</code> &amp; <code>value</code>.</p>
<pre><code>udf1 = f.udf(lambda vec : dict(zip(vec.indices.tolist(),vec.values.tolist())),MapType(StringType(),StringType()))
valuedf = rescaledData.select('label',f.explode(udf1(f.col('features'))).name('wordhash','value'))
valuedf.show()
+-----+--------+-------------------+
|label|wordhash|              value|
+-----+--------+-------------------+
|    0|  179674| 0.5108256237659907|
|    0|   64441|0.22314355131420976|
|    0|   44691|0.44628710262841953|
|    0|   32755|0.44628710262841953|
|    0|  262052|0.22314355131420976|
|    1|  179674| 0.5108256237659907|
|    1|   64441|0.22314355131420976|
|    1|   44691|0.44628710262841953|
|    1|   32755|0.44628710262841953|
|    1|  262052|0.22314355131420976|
|    2|   44691|0.22314355131420976|
|    2|   32755|0.22314355131420976|
|    3|   64441|0.22314355131420976|
|    3|  191247| 0.9162907318741551|
|    3|  262052|0.22314355131420976|
|    3|   61511| 0.9162907318741551|
+-----+--------+-------------------+
</code></pre>
<p>3.get top n words for each document(label) filtering based on its rank and join both DFs and collect &amp; sort to get the words along with its value.</p>
<pre><code>w = Window.partitionBy('label').orderBy(f.desc('value'))
valuedf = valuedf.withColumn('rank',f.rank().over(w)).where(f.col('rank')&lt;=3) # used 3 for testing.
valuedf.join(wordtf,['label','wordhash']).groupby('label').agg(f.sort_array(f.collect_list(f.struct(f.col('value'),f.col('expwords'))),asc=False).name('topn')).show(truncate=False)
+-----+-----------------------------------------------------------------------------------------------------------------------+
|label|topn                                                                                                                   |
+-----+-----------------------------------------------------------------------------------------------------------------------+
|0    |[{0.5108256237659907, Spark}, {0.44628710262841953, World}, {0.44628710262841953, Hello}]                              |
|1    |[{0.5108256237659907, Spark}, {0.44628710262841953, World}, {0.44628710262841953, Hello}]                              |
|3    |[{0.9162907318741551, Pretty}, {0.9162907318741551, PYTHON}, {0.22314355131420976, Is}, {0.22314355131420976, Awesome}]|
|2    |[{0.22314355131420976, World}, {0.22314355131420976, Hello}]                                                           |
+-----+-----------------------------------------------------------------------------------------------------------------------+
</code></pre>
",""
"69219128","2021-09-17 07:05:42","3","","69217515","<p>The <code>parser</code> component in <code>en_core_web_sm</code> depends on the <code>tok2vec</code> component, so you need to run <code>tok2vec</code> on the <code>doc</code> before running <code>parser</code> for the parser to have the right input.</p>
<pre class=""lang-py prettyprint-override""><code>doc = nlp2.get_pipe(&quot;tok2vec&quot;)(doc)
doc = nlp2.get_pipe(&quot;parser&quot;)(doc)
</code></pre>
",""
"69218787","2021-09-17 06:36:33","6","","69215446","<p>You can create a custom <code>MyVotingClassifier</code> which takes a fitted model instead of a model instance yet to be trained. In <code>VotingClassifier</code>, sklearn takes just the unfitted classifiers as input and train them and then apply voting on the predicted result. You can create something like this. The below function might not be the exact function but you can make quite similar function like below for your purpose.</p>
<pre><code>from collections import Counter
clf1 = knn_model_1.fit(X1, y)
clf2 = knn_model_2.fit(X2, y)
clf3 = knn_model_3.fit(X3, y)

class MyVotingClassifier:
    def __init__(self, **models):
        self.models = models
    
    def predict(dict_X):
        '''
        dict_X = {'knn_model_1': X1, 'knn_model_2': X2, 'knn_model_3': X3}
        '''
        preds = []
        for model_name in dict_X:
            model = self.models[model_name]
            preds.append(model.predict(dict_X[model_name]))
        preds = list(zip(*preds))
        final_pred = list(map(lambda x: Counter(x).most_common(1)[0][0]))
        return final_pred
ensemble_model = MyVotingClassifier(knn_model_1=clf1, knn_model_2=clf2, knn_model_3=clf3)
ensemble_model.predict({'knn_model_1': X1, 'knn_model_2': X2, 'knn_model_3': X3}) # Input the pre-processed `X`s 
</code></pre>
",""
"69189976","2021-09-15 08:54:34","1","","69189754","<p><strong>Solution 1:</strong></p>
<pre><code>import spacy
nlp = spacy.load('en_core_web_sm')
doc = nlp(u'hello india how are you?')
print(len([np.text for np in doc.noun_chunks])&gt;0)
</code></pre>
<p><strong>Solution 2:</strong></p>
<pre><code>import spacy
nlp = spacy.load('en_core_web_sm')
doc = nlp(u'hello india how are you?')
print(len([token.pos_ for token in doc if token.pos_==&quot;NOUN&quot;])&gt;0)
</code></pre>
",""
"69158746","2021-09-13 07:20:29","2","","69006922","<p>I have torch 1.9.0 installed with CUDA 11.2 installed and it works fine with the master branches of FARM and haystack. Let me walk you through the steps how to get there.</p>
<p>The problem is that the latest release of haystack 0.9.0 has FARM 0.8.0 as dependency, which fixes the torch dependency to &lt;1.9.
However, if you install haystack from its master branch you could remove the fixed FARM 0.8.0 dependency and install FARM from its master branch as well.
On the current master branch of FARM, the torch dependency is relaxed to &lt;1.10.</p>
<p>I have in mind the following steps to first install FARM and then haystack from their master branches:</p>
<pre><code>  git clone https://github.com/deepset-ai/FARM.git
  cd FARM
  pip install -r requirements.txt
  pip install --editable .
  cd ..
  git clone https://github.com/deepset-ai/haystack.git
  cd haystack

  #now edit requirements.txt and remove line with farm==0.8.0

  pip install --editable .
</code></pre>
<p>Note that all of this won‚Äôt be necessary when the 1.0.0 version of haystack is released, which will be within the next 3 weeks in late September 2021.
Maybe there will also be a minor 0.9.1 even before that, which should solve your problem so that you can simply use <code>pip install farm-haystack</code>.</p>
",""
"69130377","2021-09-10 09:47:32","0","","69125678","<p>The current (v3.1) default German lemmatizer is just not very good. It's a very simple lookup lemmatizer with some questionable entries in its lookup table, but given license constraints for the German pretrained pipelines, there haven't been other good alternatives. (We do have some internal work-in-progress on a statistical lemmatizer, but I'm not sure when it will make into a release.)</p>
<p>The best suggestion here if lemmas are important for your task is to use a different lemmatizer. Depending on your task / size / speed / license requirements, you could consider using a German model from <a href=""https://github.com/explosion/spacy-stanza"" rel=""nofollow noreferrer""><code>spacy-stanza</code></a> or a third-party library like <a href=""https://spacy.io/universe/project/spacy-iwnlp"" rel=""nofollow noreferrer""><code>spacy-iwnlp</code></a> (currently only for spacy v2, but it's probably not hard to update for v3).</p>
",""
"68975633","2021-08-29 18:01:04","0","","68971791","<p><code>workers=-1</code> is not a valid parameter value. If there's an example suggesting that negative-count somewhere, it's a bad example. If you got the impression that would work from something in Gensim's official docs, please report that documentation as a bug to be fixed.</p>
<p>More generally: enabling logging at the <code>INFO</code> level will show a lot more detail about what's happening, and something like &quot;misguided parameter that prevents any training from happening&quot; may become more obvious when using such logging.</p>
<p>Separately:</p>
<ul>
<li><p>The <code>Word2Vec</code> loss-tracking of Gensim has a lot of open issues (including the failure to tally by epoch, which your <code>Callback</code> tries to correct). I'd suggest not futzing with loss-display unless/until you've already achieved some success without it.</p>
</li>
<li><p>Such a low <code>min_count=2</code> is usually a bad idea with the word2vec algorithm, at least in normal natural-language settings. Words with so few occurrences lack the variety of contrasting usage examples to achieve a generalizable word-vector, or, individually, to influence the model much compared to the far-more-numerous other words. But such rare words are, altogether, quite numerous - essentially serving as 'noise' worsening other words. Discarding more such rare words often improves the remaining words, and overall model, noticeably. So, if you have enough raw training data to make word2vec worth applying, it should be more common to <em>increase</em> this cutoff higher than the default <code>min_count=5</code> than reduce it.</p>
</li>
<li><p>For recommendation-like systems being fed by pseudotexts that aren't exactly natural-language-like, it may be especially worthwhile to experiment with the <code>ns_exponent</code> parameter. As per <a href=""https://arxiv.org/abs/1804.04212"" rel=""nofollow noreferrer"">the research paper</a> linked in <a href=""https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec"" rel=""nofollow noreferrer"">the class docs</a>, the original <code>ns_exponent=0.75</code> value, which was an unchangeable constant in early word2vec implementations, might not be ideal for other applications like recommender systems.</p>
</li>
</ul>
",""
"68931168","2021-08-26 00:25:11","0","","68927809","<p>You're getting to a division by zero on this line:</p>
<pre><code>tau = 2 * num_increasing_pairs / num_possible_pairs - 1
</code></pre>
<p>This is because <code>num_possible_pairs</code> is 0 when <code>len(worder)</code> is 1. All of this is because you're calling <code>sentence_ribes</code> with two lists, when the first parameter should be a list of lists (a list of sentences where each sentence is a list of words).</p>
<p>Try calling it like this instead:</p>
<pre><code>ribes_score = sentence_ribes([ref1a], hyp1)
</code></pre>
",""
"68929120","2021-08-25 19:54:21","0","","68928954","<p>The first parameter to <code>sentence_gleu</code> should be a list of lists (a list of reference sentences, where each sentence is a list of words).</p>
<p>Try calling it like this:</p>
<pre><code>gleu_score = sentence_gleu([ref1a], hyp1)
</code></pre>
",""
"68901816","2021-08-24 04:31:09","3","","68901673","<p>A simple counter function would perform what you desired!</p>
<p>Input:</p>
<pre><code>df = pd.DataFrame({'POS':['(communications, NNS), (between,IN), (the, DT), (Principal, NNP), (and, CC), (the, DT), (Contractor, NNP), (shall, MD), (be,VB), (in, DT), (the, DT), (English, JJ), (language, NN)', '(Contractor, NNP), (shall, MD), (be,VB), (communications, NNS), (between,IN), (the, DT), (Principal, NNP), (and, CC), (the, DT), (Contractor, NNP), (shall, MD), (be,VB), (in, DT), (the, DT), (English, JJ), (language, NN)', '(and, CC), (the, DT)']})
</code></pre>
<p>Function:</p>
<pre><code>def counter(pos):
    words, tags = [], []
    for item in pos.split('), ('):
        temp = item.strip(' )(')
        word, tag = temp.split(',')[0], temp.split(',')[-1].strip()
        words.append(word); tags.append(tag)
    length = len(tags)
    if length&lt;3:
        return 0
    count = 0
    for idx in range(length):
        if tags[idx:idx+3]==['NNP', 'MD', 'VB']:
            count+=1
    return count
</code></pre>
<p>Output:</p>
<pre><code>df['occ'] = df['POS'].apply(counter)
df

    POS     occ
0   (communications, NNS), (between,IN), (the, DT)...   1
1   (Contractor, NNP), (shall, MD), (be,VB), (comm...   2
2   (and, CC), (the, DT)    0
</code></pre>
",""
"68894824","2021-08-23 14:55:07","0","","68738363","<p>According to the comment from this <a href=""https://github.com/nltk/nltk/issues/2781"" rel=""nofollow noreferrer"">issue</a>, this is a consequence of a bug in scikit-learn. Scikit-learn's <code>_transform</code> method of <code>DictVectorizer</code> in <code>sklearn/feature_extraction/_dict_vectorizer.py</code> fails when the input argument <code>X</code> contains mappings to <code>None</code>. According to <a href=""https://github.com/tomaarsen"" rel=""nofollow noreferrer"">Tom Aarsen</a>, we can now use the following example to make the work done:</p>
<pre><code>import nltk
from nltk.corpus import treebank

from nltk.classify import SklearnClassifier
from sklearn.naive_bayes import BernoulliNB
from nltk.tag.sequential import ClassifierBasedPOSTagger

nltk.download('treebank')

data = treebank.tagged_sents()
train_data = data[:3]
test_data = data[3:]

class CustomClassifierBasedPOSTagger(ClassifierBasedPOSTagger):

    def feature_detector(self, tokens, index, history):
        return {
            key: str(value) # Ensure that the feature value is a string. Converts None to 'None'
            for key, value in super().feature_detector(tokens, index, history).items()
        }

bnb = SklearnClassifier(BernoulliNB())
bnb_tagger = CustomClassifierBasedPOSTagger(train=train_data,
                                            classifier_builder=bnb.train,
                                            verbose=True)

sentence = &quot;This is a sample sentence which I just made for fun.&quot;
# evaluate tagger on test data and sample sentence
print(bnb_tagger.evaluate(test_data))

# see results on our previously defined sentence
print(bnb_tagger.tag(nltk.word_tokenize(sentence)))
</code></pre>
<p>The output will be like:</p>
<pre><code>[nltk_data] Downloading package treebank to C:\Users\Tom/nltk_data...
[nltk_data]   Package treebank is already up-to-date!
Constructing training corpus for classifier.
Training classifier (58 instances)
0.09338289371682999
[('This', 'NNP'), ('is', 'NNP'), ('a', 'NNP'), ('sample', 'NNP'), ('sentence', 'NNP'), ('which', 'NNP'), ('I', 'NNP'), ('just', 'NNP'), ('made', 'NNP'), ('for', 'NNP'), ('fun', 'NNP'), ('.', 'NNP')]
</code></pre>
",""
"68878734","2021-08-22 05:25:53","4","","68872771","<p>As mentioned in the <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"" rel=""nofollow noreferrer"">official documentation</a> <code>TfidfVectorizer</code> class with <code>max_features</code> argument keeps only k-best features.</p>
<blockquote>
<p>max_featuresint, default=None</p>
<pre><code>If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.
</code></pre>
</blockquote>
<p>If you present the class with the test set it would help to select this feature more efficiently and this is the data leakage (This scenario is based on your question but in most of the cases, it can be seen!) .
The safest way in machine learning is to ignore the test set until prediction/evaluation, think of it just like doesn't exist!</p>
<p><strong>[UPDATED]</strong>
You can see an example from kaggle which uses vectorizer on pre-split datasets <a href=""https://www.kaggle.com/getting-started/75095"" rel=""nofollow noreferrer"">here</a>!
More on this concept mentioned <a href=""https://stats.stackexchange.com/questions/267012/difference-between-preprocessing-train-and-test-set-before-and-after-splitting"">here</a> and <a href=""https://datascience.stackexchange.com/questions/46850/difference-between-train-test-split-before-preprocessing-and-after-preprocessin"">here</a>!</p>
",""
"68819822","2021-08-17 15:09:15","1","","68817989","<p>Adding to Timbus's answer,</p>
<blockquote>
<p>What is this output precisely? Are these some sort of word embeddings?</p>
</blockquote>
<p><code>results</code> is of type <code>&lt;class 'transformers.modeling_outputs.Seq2SeqLMOutput'&gt;</code> and you can do</p>
<pre><code>results.__dict__.keys()
</code></pre>
<p>to check that <code>results</code> contains the following:</p>
<pre><code>dict_keys(['loss', 'logits', 'past_key_values', 'decoder_hidden_states', 'decoder_attentions', 'cross_attentions', 'encoder_last_hidden_state', 'encoder_hidden_states', 'encoder_attentions'])
</code></pre>
<p>You can read more about this class in the <a href=""https://huggingface.co/transformers/v3.3.1/main_classes/output.html#seq2seqlmoutput"" rel=""nofollow noreferrer"">huggingface documentation</a>.</p>
<blockquote>
<p>How shall I go on with converting these embeddings to the texts in the
english language?</p>
</blockquote>
<p>To interpret the text in English, you can use <code>model.generate</code> which is easily decodable in the following way:</p>
<pre><code>predictions = model.generate(batch)
english_text = tokenizer.batch_decode(predictions)
</code></pre>
",""
"68819794","2021-08-17 15:07:06","0","","68759885","<p>While using hooks and custom callbacks is the right way to solve the problem I find better solution - use built-in utility for finding <code>nan/Inf</code> in losses / weights / inputs / outputs:
<a href=""https://huggingface.co/transformers/internal/trainer_utils.html#transformers.debug_utils.DebugUnderflowOverflow"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/internal/trainer_utils.html#transformers.debug_utils.DebugUnderflowOverflow</a>
since the 4.6.0 transformers has such option.</p>
<p>You can use it manually in <code>forward</code> function or just use additional option for <code>TrainingArguments</code> like this:</p>
<pre><code>args = TrainingArguments(
    output_dir='test_dir',
    overwrite_output_dir=True,
    num_train_epochs=1,
    logging_steps=100,
    report_to=&quot;none&quot;,
    fp16=True,
    disable_tqdm=True,
    debug=&quot;debug underflow_overflow&quot;
)
</code></pre>
",""
"68814093","2021-08-17 08:29:50","3","","68813979","<p>In the problem described here (credits to
LysandreJik): <a href=""https://github.com/huggingface/transformers/issues/5480"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/issues/5480</a>, the problem appears to be the data type of a <code>dict</code> instead of <code>tensor</code>.</p>
<p>It might be the case that you need to change the tokenizer output from:</p>
<pre><code>batch = tokenizer(
    list(data_bert[:100]),
    padding=True,
    truncation=True,
    max_length=250,
    return_tensors=&quot;pt&quot;
)
</code></pre>
<p>TO:</p>
<pre><code>batch = tokenizer(
    list(data_bert[:100]),
    padding=True,
    truncation=True,
    max_length=250,
    return_tensors=&quot;pt&quot;)[&quot;input_ids&quot;]
</code></pre>
",""
"68740996","2021-08-11 11:20:52","3","","68739904","<p>The scorer does not run the pipeline on the predicted docs, so you're evaluating blank docs against your test cases.</p>
<p>The recommended way is to use <code>nlp.evaluate</code> instead:</p>
<pre class=""lang-py prettyprint-override""><code>scores = nlp.evaluate(examples)
</code></pre>
<p>If you want the call the scorer directly for some reason, the other alternative is to run the pipeline on the predicted docs (<code>nlp</code> instead of <code>nlp.make_doc</code>), so:</p>
<pre class=""lang-py prettyprint-override""><code>example = Example.from_dict(nlp(text), annots)
</code></pre>
",""
"68708517","2021-08-09 07:47:44","0","","68659164","<p>Well, instead of removing such terms, I would suggest to focus on ngrams. Using the ngrams you can make different combination of search strings, and it could help you find the related information efficiently. Now it depends upon you to what number of combinations you want to make i.e. bigrams or trigrams. To do this, you can use python <code>nltk</code> library.</p>
",""
"68698418","2021-08-08 06:52:26","2","","68698065","<p>Maybe the problem comes from this line:</p>
<p><code>torch.backends.cudnn.enabled = False</code></p>
<p>You might comment or remove it and try again.</p>
",""
"68654948","2021-08-04 16:30:06","0","","68654498","<p>You can create your own dictionary where you remove the token <code>spotting</code></p>
<pre><code># hash_lemmas is a datatable, so you can use column name token instead hash_lemmas$token
my_lex &lt;- lexicon::hash_lemmas[!token == &quot;spotting&quot;, ]

df_lemmatized &lt;- lemmatize_strings(df, dictionary = my_lex)
</code></pre>
<p>Or if you want to do it without creating your own lexicon:</p>
<pre><code>df_lemmatized &lt;- lemmatize_strings(df, dictionary = lexicon::hash_lemmas[!token == &quot;spotting&quot;, ])
</code></pre>
",""
"68602740","2021-07-31 13:53:10","2","","68599547","<p>You are very close to getting this pyparsing cleaner-upper working.</p>
<p>Parse actions generally get their matched tokens as a list-like structure, a pyparsing-defined class called ParseResults.</p>
<p>You can see what actually gets sent to your parse action by wrapping it in the pyparsing decorator <code>traceParseAction</code>:</p>
<pre><code>parser = OneOrMore(oneOf(phrases) ^ Word(alphanums).setParseAction(traceParseAction(lambda word: re.sub(r'\d+', '', word))))
</code></pre>
<p>Actually a little easier to read if you make your parse action a regular def'ed method instead of a lambda:</p>
<pre><code>@traceParseAction
def unnumber(word):
    return re.sub(r'\d+', '', word)
parser = OneOrMore(oneOf(phrases) ^ Word(alphanums).setParseAction(unnumber))
</code></pre>
<p><code>traceParseAction</code> will report what is passed to the parse action and what is returned.</p>
<pre><code>&gt;&gt;entering unnumber(line: 'there was once a potato_man with tw3nty cars and d&amp; 76 different homes', 0, ParseResults(['there'], {}))
&lt;&lt;leaving unnumber (exception: expected string or bytes-like object)
</code></pre>
<p>You can see that the value passed in is in a list structure, so you should replace <code>word</code> in your call to <code>re.sub</code> with <code>word[0]</code> (I also modified your input string to add some numbers to the unguarded words, to see the parse action in action):</p>
<pre><code>text = &quot;there was 1once a potato_man with tw3nty cars and d&amp; 76 different99 homes&quot;

def unnumber(word):
    return re.sub(r'\d+', '', word[0])
</code></pre>
<p>and I get:</p>
<pre><code>['there', 'was', 'once', 'a', 'potato_man', 'with', 'tw3nty', 'cars', 'and', 'd&amp;', '76', 'different', 'homes']
</code></pre>
<p>Also, you use the '^' operator for your parser. You may get a little better performance if you use the '|' operator instead, since '^' (which creates an Or instance) will evaluate all paths and choose the longest - necessary in cases where there is some ambiguity in what the alternatives might match. '|' creates a MatchFirst instance, which stops once it finds a match and does not look further for any alternatives. Since your first alternative is a list of the guard words, then '|' is actually more appropriate - if one gets matched, don't look any further.</p>
",""
"68524306","2021-07-26 03:51:16","2","","68499164","<p>You are looking for <a href=""https://github.com/explosion/spaCy/discussions/8803"" rel=""nofollow noreferrer"">Doc.char_span</a>.</p>
<pre><code>doc = &quot;Blah blah blah&quot;
span = doc.char_span(0, 4, label=&quot;BLAH&quot;)
doc.ents = [span]
</code></pre>
<p>Note that <code>doc.ents</code> is a tuple, so you can't append to it, but you can convert it to a list and set the ents, for example.</p>
",""
"68518389","2021-07-25 12:05:55","4","","68185061","<p>To translate long texts with transformers you can split your text by paragraphs, paragraphs split by sentence and after that feed sentences to your model in batches. In any case it is better to translate with MarianMT in a sentence-by-sentence way, because it can lose some parts if you feed a long text as a one piece to it.</p>
<pre><code>from transformers import MarianMTModel, MarianTokenizer
from nltk.tokenize import sent_tokenize
from nltk.tokenize import LineTokenizer
import math
import torch

if torch.cuda.is_available():  
  dev = &quot;cuda&quot;
else:  
  dev = &quot;cpu&quot; 
device = torch.device(dev)
 
mname = 'Helsinki-NLP/opus-mt-de-en'
tokenizer = MarianTokenizer.from_pretrained(mname)
model = MarianMTModel.from_pretrained(mname)
model.to(device)

lt = LineTokenizer()
batch_size = 8

text_short = &quot;Nach nur sieben Seiten appellierte man an die W√§hlerinnen und W√§hler, sich richtig zu entscheiden, n√§mlich f√ºr Frieden, Freiheit, Sozialismus. &quot;
text_long = text_short * 30

paragraphs = lt.tokenize(text_long)   
translated_paragraphs = []

for paragraph in paragraphs:
    sentences = sent_tokenize(paragraph)
    batches = math.ceil(len(sentences) / batch_size)     
    translated = []
    for i in range(batches):
        sent_batch = sentences[i*batch_size:(i+1)*batch_size]
        model_inputs = tokenizer(sent_batch, return_tensors=&quot;pt&quot;, padding=True, truncation=True, max_length=500).to(device)
        with torch.no_grad():
            translated_batch = model.generate(**model_inputs)
        translated += translated_batch
    translated = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]
    translated_paragraphs += [&quot; &quot;.join(translated)]

translated_text = &quot;\n&quot;.join(translated_paragraphs)
</code></pre>
",""
"68482504","2021-07-22 09:33:33","0","","67519425","<p>The <a href=""https://pypi.org/project/googletrans/"" rel=""nofollow noreferrer"">googletrans API</a> that you are using is the same service as translate.google.com.
Googletrans does not use the <a href=""https://cloud.google.com/translate/docs"" rel=""nofollow noreferrer"">official Google Translation API</a>. It uses the Google Translate Ajax API.
You can use the Python client library or REST API provided in Google‚Äôs official Translation API.
You can refer to the code mentioned below.</p>
<p>Using Python client library:</p>
<pre><code>from os import environ

from google.cloud import translate

project_id = environ.get(&quot;PROJECT_ID&quot;, &quot;&quot;)
assert project_id
parent = f&quot;projects/{project_id}&quot;
client = translate.TranslationServiceClient()

sample_text = &quot;Bonjour&quot;
target_language_code = &quot;en&quot;

response = client.translate_text(
    contents=[sample_text],
    target_language_code=target_language_code,
    parent=parent,
)

for translation in response.translations:
    print(translation.translated_text)
</code></pre>
<p>Using REST API:</p>
<p>Create a request.json file and use the below curl command mentioned.</p>
<p>request.json</p>
<pre><code>{
  &quot;q&quot;: [&quot;Bonjour&quot;, &quot;Merci&quot;],
  &quot;target&quot;: &quot;en&quot;
}
</code></pre>
<p>Curl Command</p>
<pre><code>curl -X POST \
-H &quot;Authorization: Bearer &quot;$(gcloud auth application-default print- 
access-token) \
-H &quot;Content-Type: application/json; charset=utf-8&quot; \
-d @request.json \
https://translation.googleapis.com/language/translate/v2
</code></pre>
",""
"68396056","2021-07-15 14:49:30","0","","68394241","<p>There's some issues here, most notably with the last 2 for loops.</p>
<p>The way you are doing it made it write it as follows:</p>
<pre class=""lang-none prettyprint-override""><code>word1
word1word2
word1word2word3
word1word2word3word4
........etc
</code></pre>
<p>I'm guessing that is not the expected output. I'm assuming the expected output is:</p>
<pre class=""lang-none prettyprint-override""><code>word1
word2
word3
word4
........etc (without creating duplicates)
</code></pre>
<p>I applied the code below to a 3 paragraph <a href=""http://www.catipsum.com/"" rel=""nofollow noreferrer"">Cat Ipsum</a> file. Note that I changed some variable names due to my own naming conventions.</p>
<pre class=""lang-py prettyprint-override""><code>import nltk
nltk.download('punkt')
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from pprint import pprint


# read the text into a single string.
with open(&quot;book1.txt&quot;) as infile:
    text = ' '.join(infile.readlines())
words = word_tokenize(text)
words = [word.lower() for word in words if word.isalpha()]


# create the lemmatized word list
results = []
for word in words:
    # you were using words instead of word below
    token = WordNetLemmatizer().lemmatize(word, &quot;v&quot;)
    # check if token not already in results. 
    if token not in results:
        results.append(token)


# sort results, just because :)
results.sort()

# print and save the results
pprint(results)
print(len(results))
with open(&quot;nltk_data.csv&quot;, &quot;w&quot;) as outfile:
    outfile.writelines(results)
</code></pre>
",""
"68308553","2021-07-08 20:57:41","1","","68306484","<p>It may be that your system does not have the correct version of libtpu. Try installing the version listed <a href=""https://github.com/google/jax/blob/38884b02f72950a8d187f81420a270e46afe8889/setup.py#L27"" rel=""nofollow noreferrer"">here</a>.</p>
<p>You should be able to do this automatically with</p>
<pre><code>$ pip install -U pip  # older pip may not support extra requirements
$ pip install -U jax  # newer jax required for [tpu] extras declaration
$ pip install jax[tpu] -f https://storage.googleapis.com/jax-releases/jax_releases.html
</code></pre>
",""
"68275055","2021-07-06 17:30:54","6","","68274864","<p><strong>TL;DR;</strong> The correct way is Option 1(A).</p>
<hr />
<p>The correct way to apply TFIDF Vectorizer is with an text corpus that is:</p>
<blockquote>
<p>An iterable which yields either str, unicode or file objects.</p>
</blockquote>
<p>As per <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer.fit"" rel=""nofollow noreferrer"">the docs</a>, you have to pass, your case an array of texts.</p>
<p>And example from Scikit-learn docs:</p>
<pre><code>&gt;&gt;&gt; from sklearn.feature_extraction.text import TfidfVectorizer
&gt;&gt;&gt; corpus = [
...     'This is the first document.',
...     'This document is the second document.',
...     'And this is the third one.',
...     'Is this the first document?',
... ]
&gt;&gt;&gt; vectorizer = TfidfVectorizer()
&gt;&gt;&gt; X = vectorizer.fit_transform(corpus)
&gt;&gt;&gt; print(vectorizer.get_feature_names())
['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']
&gt;&gt;&gt; print(X.shape)
(4, 9)
</code></pre>
",""
"68140312","2021-06-26 07:48:51","1","","68140256","<p>The problem is that your &quot;tokenized&quot; column doesn't look ready to apply the lemmatization step, as it contains a string, not a list of tokens. In other words, instead of having</p>
<pre><code>&quot; yeah simply zurich generic serving ...&quot;
</code></pre>
<p>you should have in your dataframe <code>tokenized</code> cell a list of tokens (generated with a tokenizer from your initial sentence), as in</p>
<pre><code>[&quot;yeah&quot;, &quot;simply&quot;, &quot;zurich&quot;, &quot;generic&quot;, &quot;serving&quot;, ...]
</code></pre>
<p>If you don't have a proper list of tokens in your dataframe cell, python will iterate in your <code>apply</code>/<code>lambda</code> list comprehension character by character, which is clearly not what you want.</p>
",""
"67998073","2021-06-16 07:18:31","0","","67997713","<p>I think this is not an issue with the Code itself when I run the code you provided</p>
<pre><code>import language_tool_python
tl = language_tool_python.LanguageTool('en-US')

txt = &quot;good mooorning sirr and medam my namee anderen i am from amerecia !&quot;
m = tl.check(txt)
len(m)
</code></pre>
<p>I get as result a number in this case</p>
<pre><code> OUT: 8
</code></pre>
<p>In the Documentation of the <a href=""https://pypi.org/project/language-tool-python/"" rel=""nofollow noreferrer"">language-tool-python</a> is written:</p>
<blockquote>
<p>By default, language_tool_python will download a LanguageTool server .jar and run that in the background to detect grammar errors locally. However, LanguageTool also offers a Public HTTP Proofreading API that is supported as well. Follow the link for rate-limiting details. (Running locally won't have the same restrictions.)</p>
</blockquote>
<p>So You will need Java (JRE and SKD). Also it's Written in the Requirements of the library:</p>
<blockquote>
<p>Prerequisites
Python 3.5+
LanguageTool (Java 8.0 or higher)
The installation process should take care of downloading LanguageTool (it may take a few minutes). Otherwise, you can manually download LanguageTool-stable.zip and unzip it into where the language_tool_python package resides.</p>
</blockquote>
<p>Source:</p>
<ul>
<li><a href=""https://pypi.org/project/language-tool-python/"" rel=""nofollow noreferrer"">https://pypi.org/project/language-tool-python/</a></li>
<li><a href=""https://stackoverflow.com/questions/45377463/python-2-7-javaerror-when-using-grammar-check-1-3-1-library"">Python 2.7 - JavaError when using grammar-check 1.3.1 library</a></li>
</ul>
<p>I Hope I could help.</p>
",""
"67937061","2021-06-11 12:28:42","2","","67925248","<p>You can use the head index. E.g.,</p>
<pre><code>tok_l = doc.to_json()['tokens']
for t in tok_l:
  head = tok_l[t['head']]
  print(f&quot;'{sentence[t['start']:t['end']]}' is {t['dep']} of '{sentence[head['start']:head['end']]}'&quot;)
</code></pre>
<p>Result:</p>
<pre><code>'The' is det of 'Marlins'
'Marlins' is nsubjpass of 'stymied'
'were' is auxpass of 'stymied'
'stymied' is ROOT of 'stymied'
'by' is agent of 'stymied'
'Austin' is compound of 'Gomber'
'Gomber' is pobj of 'by'
'and' is cc of 'Gomber'
'the' is det of 'Rockies'
'Rockies' is conj of 'Gomber'
'in' is prep of 'stymied'
'their' is poss of 'loss'
'4' is nummod of 'loss'
'-' is punct of '3'
'3' is prep of '4'
'loss' is pobj of 'in'
</code></pre>
",""
"67796633","2021-06-01 22:11:49","1","","67789544","<p>I found it difficult to get lemmas and inflections directly out of spaCy without first constructing an example sentence to give it context.  This wasn't ideal, so I looked further and found <a href=""https://github.com/bjascob/LemmInflect"" rel=""nofollow noreferrer"">LemmaInflect</a> did this very well.</p>
<pre><code>&gt; from lemminflect import getAllLemmas, getInflection, getAllInflections, getAllInflectionsOOV

&gt; getAllLemmas('watches')
{'NOUN': ('watch',), 'VERB': ('watch',)}

&gt; getAllInflections('watch')
{'NN': ('watch',), 'NNS': ('watches', 'watch'), 'VB': ('watch',), 'VBD': ('watched',), 'VBG': ('watching',), 'VBZ': ('watches',),  'VBP': ('watch',)}
</code></pre>
",""
"67794722","2021-06-01 19:09:45","0","","67794357","<p>As stated <a href=""https://scikit-learn.org/stable/modules/compose.html#nested-parameters"" rel=""nofollow noreferrer"">here</a>, nested parameters must be accessed by the <code>__</code> (double underscore) syntax. Depending on the depth of the parameter you want to access, this applies recursively. The parameter <code>use_idf</code> is under:</p>
<p><code>features</code> &gt; <code>text_features</code> &gt; <code>tfidf</code> &gt; <code>use_idf</code></p>
<p>So the resulting parameter in your grid needs to be:</p>
<pre class=""lang-py prettyprint-override""><code>'features__text_features__tfidf__use_idf': [True, False]
</code></pre>
<p>Similarly, the syntax for <code>ngram_range</code> should be:</p>
<pre class=""lang-py prettyprint-override""><code>'features__text_features__vect__ngram_range': [(1, 1), (1, 2), (1, 3), (2, 2), (2, 3), (3, 3)]
</code></pre>
",""
"67734401","2021-05-28 07:13:37","0","","67734007","<pre><code>corpus = [
     'this is the first document',
     'this document is the second document',
     'and this is the third one',
     'is this the first document',
]

def docs(corpus):
    doc_count = dict()
    for line in corpus:
        for word in line.split():
            #you did mistake here
            if word in doc_count:
                doc_count[word] +=1
            else:
                doc_count[word] = 1
    return doc_count    

ans=docs(corpus)
print(ans)
</code></pre>
",""
"67710914","2021-05-26 18:39:01","1","","49941772","<p>The list of POS tags given in the <a href=""https://books.google.com/ngrams/info"" rel=""nofollow noreferrer"">documentation</a> does not mention two of the tags, but the <a href=""https://storage.googleapis.com/pub-tools-public-publication-data/pdf/38277.pdf"" rel=""nofollow noreferrer"">2012 paper <em>Syntactic Annotations for the Google Books Ngram Corpus</em></a> does:</p>
<ul>
<li><code>‚Äò.‚Äô</code> (punctuation marks)</li>
<li><code>X</code> (a catch-all for other categories such as abbreviations or foreign words)</li>
</ul>
<p>So the token <code>,_.</code> is a comma appended with its POS tag, just like the token <code>run_VERB</code>. Similarly, <code>._.</code> is a full stop appended with its POS tag. Finally, <code>_._</code> means punctuation, any punctuation just like <code>_VERB_</code> is any verb.</p>
",""
"67621076","2021-05-20 13:15:05","0","","67543209","<p>After searching and experimenting with different packages and measuring the time each one needed to calculate the scores, I found the <a href=""https://www.nltk.org/_modules/nltk/translate/bleu_score.html"" rel=""nofollow noreferrer"">nltk corpus bleu</a> and <a href=""https://pypi.org/project/rouge-metric/"" rel=""nofollow noreferrer"">PyRouge</a> the most efficient ones. Just keep in mind that in each record, I had multiple hypotheses and that's why I calculate the means once for each record and
This is how I did it for BLEU:</p>
<pre><code>reference = [[i.split() for i in ref]]

def find_my_bleu(text, w):

   candidates_ = [text.split()]
   return corpus_bleu(reference, candidates_, weights=w, 
                                    smoothing_function=cc.method4)

def get_final_bleu(output_df):

   print('Started calculating the bleu scores...')
   output_df.loc[:, 'bleu_1'] = output_df.loc[:, 'final_predicted_verses'].apply(lambda x:[find_my_bleu(t, (1, 0, 0, 0)) for t in x])
   output_df.loc[:, 'bleu_2'] = output_df.loc[:, 'final_predicted_verses'].apply(lambda x:[find_my_bleu(t, (0, 1, 0, 0)) for t in x])
   output_df.loc[:, 'bleu_3'] = output_df.loc[:, 'final_predicted_verses'].apply(lambda x:[find_my_bleu(t, (0, 0, 1, 0)) for t in x])


   print('Now the average score...')
   output_df.loc[:, 'bleu_3_mean'] = output_df.loc[:, 'bleu_3'].apply(lambda x:np.mean(x))
   output_df.loc[:, 'bleu_2_mean'] = output_df.loc[:, 'bleu_2'].apply(lambda x:np.mean(x))
   output_df.loc[:, 'bleu_1_mean'] = output_df.loc[:, 'bleu_1'].apply(lambda x:np.mean(x))

   print('mean bleu_3 score: ', np.mean(output_df.loc[:, 'bleu_3_mean']))
   print('mean bleu_2 score: ', np.mean(output_df.loc[:, 'bleu_2_mean']))
   print('mean bleu_1 score: ', np.mean(output_df.loc[:, 'bleu_1_mean']))
</code></pre>
<p>For ROUGE:</p>
<p>rouge = PyRouge(rouge_n=(1, 2), rouge_l=True, rouge_w=False, rouge_s=False, rouge_su=False)</p>
<pre><code>def find_my_rouge(text):
    hypotheses = [[text.split()]]
    score = rouge.evaluate_tokenized(hypotheses, [[reference_rouge]])
    return score
</code></pre>
<p>Then for taking the mean of all:</p>
<pre><code>def get_short_rouge(list_dicts):

    &quot;&quot;&quot; get the mean of all generated text for each record&quot;&quot;&quot;
    l_r = 0
    l_p = 0
    l_f = 0

    one_r = 0
    one_p  = 0
    one_f  = 0

    two_r  = 0
    two_p  = 0
    two_f  = 0
    
    for d in list_dicts:
        
        
        one_r += d['rouge-1']['r']
        one_p += d['rouge-1']['p']
        one_f += d['rouge-1']['f']


        two_r += d['rouge-2']['r']
        two_p += d['rouge-2']['p']
        two_f += d['rouge-2']['f']
        
        l_r += d['rouge-l']['r']
        l_p += d['rouge-l']['p']
        l_f += d['rouge-l']['f']

    length = len(list_dicts)

    return {'rouge-1': {'r': one_r/length , 'p': one_p/length , 'f': one_f/length},
            'rouge-2': {'r': two_r/length, 'p': two_p/length, 'f': two_f/length},
            'rouge-l': {'r': l_r/length, 'p': l_p/length , 'f': l_f/length}
            }

def get_overal_rouge_mean(output_df):
    print('Started getting the overall rouge of each record...')
    output_df.loc[:, 'rouge_mean'] = output_df.loc[:, 'rouge'].apply(lambda x: get_short_rouge(x))
    print('Started getting the overall rouge of all record...')
    l_r = 0
    l_p = 0
    l_f = 0

    one_r = 0
    one_p  = 0
    one_f  = 0

    two_r  = 0
    two_p  = 0
    two_f  = 0

    for i in range(len(output_df)):
        d = output_df.loc[i, 'rouge_mean']
        
        one_r += d['rouge-1']['r']
        one_p += d['rouge-1']['p']
        one_f += d['rouge-1']['f']


        two_r += d['rouge-2']['r']
        two_p += d['rouge-2']['p']
        two_f += d['rouge-2']['f']
        
        l_r += d['rouge-l']['r']
        l_p += d['rouge-l']['p']
        l_f += d['rouge-l']['f']

    length = len(output_df)
    print('overall rouge scores: ')
    print({'rouge-1': {'r': one_r/length , 'p': one_p/length , 'f': one_f/length},
                'rouge-2': {'r': two_r/length, 'p': two_p/length, 'f': two_f/length},
                'rouge-l': {'r': l_r/length, 'p': l_p/length , 'f': l_f/length}
                })
    return output_df
</code></pre>
<p>I hope it helps anyone who's had this problem.</p>
",""
"67613763","2021-05-20 03:56:43","1","","67612600","<p>Starting simply with NLP makes it easier to understand and also to appreciate the more advanced systems.</p>
<p>This gives what you're looking for:</p>
<pre><code># Use 'with' so that the file is automatically closed when the 'with' ends.
with open('corpus_test.txt', 'r', encoding='utf-8') as f:
    # splitlines is not a method, readlines is.
    # infile will contain a list, where each item is a line.
    # e.g. infile[0] = line 1.
    infile = f.readlines()

dicionario = {
    'Maria': 'N-PR',
    'ix√©': 'PRON1',
}

# Make a list to hold the new lines
outlines = []

for line in infile:
    list_of_words = line.split()
    
    new_line = ''
    # 'if np_words in list_of_words' is asking too much of Python.
    for word in list_of_words:
        # todo: Dictionaries are case-sensitive, so ix√© is different to Ix√©.
        if word in dicionario:
            new_line += word + '\\' + dicionario[word] + ' '
        else:
            new_line += word + ' '

    # Append the completed new line to the list and add a carriage return.
    outlines.append(new_line.strip() + '\n')

with open('tag_test.txt', 'w', encoding='utf-8') as f:
    f.writelines(outlines)
</code></pre>
",""
"67528661","2021-05-14 03:16:13","2","","67474728","<p>This was answered in detail on <a href=""https://github.com/explosion/projects/tree/v3/tutorials/nel_emerson"" rel=""nofollow noreferrer"">the forum</a>, but the issue here is that you aren't using the noun chunks, you're using divisions of the sentence that include noun chunks.</p>
<p>When you call <code>.conjuncts</code> on a span, you get the conjuncts of the span root. In a noun chunk the head noun is the root, but with your spans sometimes verbs are included, so the conjuncts could be conjuncts of that verb, not the noun chunk's head.</p>
",""
"67507985","2021-05-12 16:59:21","0","","67507820","<p>You can collapse it like so:</p>
<pre><code>aggregate(pos ~ doc_id, doc_df, paste, collapse = &quot;, &quot;)
</code></pre>
<p>You can store this in a separate dataframe and merge in any other columns you want to include from the original, or if you just need these two then you can use this directly.</p>
",""
"67479054","2021-05-11 00:20:57","0","","67257008","<pre><code>import os 
os.environ['LD_LIBRARY_PATH']='/usr/local/lib'

!echo $LD_LIBRARY_PATH
!sudo ln -s /usr/local/lib/libmkl_intel_lp64.so /usr/local/lib/libmkl_intel_lp64.so.1
!sudo ln -s /usr/local/lib/libmkl_intel_thread.so /usr/local/lib/libmkl_intel_thread.so.1
!sudo ln -s /usr/local/lib/libmkl_core.so /usr/local/lib/libmkl_core.so.1

!ldconfig
!ldd /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch.so
</code></pre>
<p>worked for me. We will also try to fix the problem internally.</p>
",""
"67464811","2021-05-10 04:32:44","3","","67441897","<p>You have a model for spaCy v2 (the model version starts with 2), but you are using spaCy v3. The models are not compatible with different major versions. You need to uninstall the model and then download the new model:</p>
<pre><code>pip uninstall en-core-web-sm
pip -m spacy download en_core_web_sm
</code></pre>
",""
"67426116","2021-05-06 21:21:28","0","","67426019","<pre><code>import itertools

new_phrases = list(itertools.product(*dictionary.values()))
</code></pre>
<pre><code>&gt;&gt;&gt; new_phrases
[('express', 'sociable', 'support'),
 ('express', 'sociable', 'reinforcement'),
 ('express', 'sociable', 'reenforcement'),
 ('express', 'sociable', 'documentation'),
 ('express', 'sociable', 'keep'),
 ('express', 'sociable', 'livelihood'),
 ('express', 'sociable', 'living'),
 ('express', 'sociable', 'bread_and_butter'),
 ('express', 'sociable', 'sustenance'),
 ('express', 'sociable', 'supporting'),
 ...
 ('special', 'societal', 'living'),
 ('special', 'societal', 'bread_and_butter'),
 ('special', 'societal', 'sustenance'),
 ('special', 'societal', 'supporting'),
 ('special', 'societal', 'accompaniment'),
 ('special', 'societal', 'musical_accompaniment'),
 ('special', 'societal', 'backup'),
 ('special', 'societal', 'financial_support'),
 ('special', 'societal', 'funding'),
 ('special', 'societal', 'backing')]
</code></pre>
",""
"67398182","2021-05-05 09:01:58","5","","67397321","<p>There is a number of speed improvements that you could try:</p>
<ol>
<li>Using <code>yield</code> (actually <a href=""https://stackoverflow.com/questions/35518986/whats-the-difference-between-yield-from-and-yield-in-python-3-3-2""><code>yield from</code></a>) instead of constructing the list in memory before returning it. Also, I don't think you need to create a list from the results from <code>map</code>:</li>
</ol>
<pre><code>def lemmatizer(words):
    doc = nlp(words)
    yield from filter(lambda x: x not in list(fr_stop), map(lambda token: token.lemma_, doc))
</code></pre>
<ol start=""2"">
<li>Using a set instead of a list for containment checking:</li>
</ol>
<pre><code>fr_stop = set(fr_stop)
def lemmatizer(words):
    doc = nlp(words)
    yield from filter(lambda x: x not in fr_stop, map(lambda token: token.lemma_ , doc))
</code></pre>
<p>These should help reducing both processing time and memory pressure.</p>
",""
"67306087","2021-04-28 18:39:18","1","","67303890","<p>The grammar would need to be something like:</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""false"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>from nltk import CFG, ChartParser

grammar = CFG.fromstring(""""""
S -&gt; N L
N -&gt; N N | '1' | '2' | '3'
L -&gt; L L | 'A' | 'B' | 'C'
"""""")

parser = ChartParser(grammar)
sentence = '1 2 1 3 A C B C'.split()
for t in parser.parse(sentence):
    print(t)
    break</code></pre>
<pre class=""snippet-code-html lang-html prettyprint-override""><code>&lt;script src=""https://cdn.jsdelivr.net/gh/pysnippet/pysnippet@latest/snippet.min.js""&gt;&lt;/script&gt;</code></pre>
</div>
</div>
</p>
<p>Using <code>N -&gt; N N</code> as an example: the first <code>N</code> could be &quot;eaten up&quot; and transformed into a <code>1</code> when parsing the sentence, leaving the next <code>N</code> to go on and produce another <code>N -&gt; N N</code>.</p>
<p>But this will result in a lot of possible parses, for something more efficient you probably want something like this:</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""false"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>from nltk import CFG, ChartParser

grammar = CFG.fromstring(""""""
S -&gt; N L
N -&gt; '1' N | '2' N | '3' N | '1' | '2' | '3'
L -&gt; 'A' L | 'B' L | 'C' L | 'A' | 'B' | 'C'
"""""")

parser = ChartParser(grammar)
sentence = '1 2 1 3 A C B C'.split()
for t in parser.parse(sentence):
    print(t)
    break</code></pre>
<pre class=""snippet-code-html lang-html prettyprint-override""><code>&lt;script src=""https://cdn.jsdelivr.net/gh/pysnippet/pysnippet@latest/snippet.min.js""&gt;&lt;/script&gt;</code></pre>
</div>
</div>
</p>
<hr />
<p><strong>Regular Version</strong>. The language from the question: &quot;<em>one or more numbers followed by one or more letters</em>&quot; or <code>(1,2,3)+(A,B,C)+</code> is a regular language, so we can represent it with a <a href=""https://en.wikipedia.org/wiki/Regular_grammar"" rel=""nofollow noreferrer"">regular grammar</a>:</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""false"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>from nltk import CFG, ChartParser

grammar = CFG.fromstring(""""""
S -&gt; N
N -&gt; '1' N | '2' N | '3' N | '1' | '2' | '3' | L
L -&gt; 'A' L | 'B' L | 'C' L | 'A' | 'B' | 'C'
"""""")

parser = ChartParser(grammar)
sentence = '1 2 1 3 A C B C'.split()
for t in parser.parse(sentence):
    print(t)
    break</code></pre>
<pre class=""snippet-code-html lang-html prettyprint-override""><code>&lt;script src=""https://cdn.jsdelivr.net/gh/pysnippet/pysnippet@latest/snippet.min.js""&gt;&lt;/script&gt;</code></pre>
</div>
</div>
</p>
<p>Try all three out and see what the parses look like on different inputs!</p>
",""
"67246204","2021-04-24 18:34:34","3","","67194634","<pre><code># PT
model = AutoModelForSequenceClassification.from_pretrained(MODEL) 
model.save_pretrained(MODEL)
tokenizer.save_pretrained(MODEL)
</code></pre>
<p>Add the third line to this section, rename or remove the model folder you had already, and try again. This time it should indeed work more than once. I hope this gets you rolling!</p>
",""
"67245903","2021-04-24 18:00:50","1","","66267633","<p>I am also trying to do the same. The error occurs because the textaugment function <code>t.random_swap()</code> is supposed to work on <em>Python string objects</em>.</p>
<p>In your code, the function is taking in a <em>Tensor with dtype=string</em>. As of now, tensor objects do not have the same methods as Python strings. Hence, the error code.</p>
<p>Nb. <a href=""https://www.tensorflow.org/tutorials/tensorflow_text/intro"" rel=""nofollow noreferrer"">tensorflow_text</a> has some additional APIs to work with such tensors of string types. Albeit, it is limited at the moment to tokenization, checking upper or lower case etc. A long winded workaround is to use the <code>py_function</code> <a href=""https://www.tensorflow.org/api_docs/python/tf/py_function"" rel=""nofollow noreferrer"">wrapper</a> but <a href=""https://www.tensorflow.org/guide/data#applying_arbitrary_python_logic"" rel=""nofollow noreferrer"">this reduces performance</a>. Cheers and hope this helps. I opted not to use textaugment in the end in my use case.</p>
<p>Nbb. tf.strings <a href=""https://www.tensorflow.org/api_docs/python/tf/strings"" rel=""nofollow noreferrer"">APIs</a> have a bit more functionalities, such as regex replace etc but it is not complicated enough for your use case of augmentation. Would be helpful to see what others come up with, or if there are future updates to either TF or textaugment.</p>
",""
"67084115","2021-04-14 00:50:32","0","","67083987","<p>You are lemmatizing each char instead of word. Your function should look like this instead:</p>
<pre><code>def lemmatize_text(text):
    lemmatizer = WordNetLemmatizer()
    return ' '.join([lemmatizer.lemmatize(w) for w in text.split(' ')])

</code></pre>
",""
"66839470","2021-03-28 08:02:14","0","","66332810","<p>I don't think your problem is about lemmatization.
This method works for your example.</p>
<pre><code># merge noun phrase and entities
def merge_noun_phrase(doc):
    spans = list(doc.ents) + list(doc.noun_chunks)
    spans = spacy.util.filter_spans(spans)
    
    with doc.retokenize() as retokenizer:
        for span in spans:
            retokenizer.merge(span)
    return doc

sentence = &quot;the euro area has advanced a long way as a monetary union&quot;
doc = nlp(sentence)
doc2 = merge_noun_phrase(doc)
for token in doc2:
    print(token)
    #['the euro area', 'way', 'a monetary union']
</code></pre>
<p>I have to note that I'm using spacy2.3.5, maybe <code>spacy.util.filter_spans</code> is deprecated in the newest version. This answer would help you. :)</p>
<p><a href=""https://stackoverflow.com/questions/62259356/module-spacy-util-has-no-attribute-filter-spans-in-jupyter-notebook"">Module 'spacy.util' has no attribute 'filter_spans'</a></p>
<p>And, if you still try to lemmatize noun chunks, you can do it as following:</p>
<pre><code>doc = nlp(&quot;the euro area has advanced a long way as a monetary union&quot;)
for chunk in doc.noun_chunks:
    print(chunk.lemma_)
    #['the euro area', 'a monetary union']
</code></pre>
<p>According to the answer in <a href=""https://github.com/explosion/spaCy/issues/4028"" rel=""nofollow noreferrer"">What is the lemma for 'two pets'</a>, &quot;looking at the lemma on the span level is probably not very useful and it makes more sense to work on the token level.&quot;</p>
",""
"66834146","2021-03-27 17:22:26","0","","66822048","<p>BLEU is defined as a geometrical average of (modified) n-gram precisions for unigrams up to 4-grams (times brevity penalty). Thus if there is no matching 4-gram (no 4-tuple of words) in the whole test set, BLEU is 0 by definition. having a dot at the end which will get tokenized, makes it so that that there are now matches for 4-grams because smoothing is applied.</p>
<p>BLEU was designed for scoring test sets with hundreds of sentences where such case is very unlikely. For scoring single sentences, you can use a sentence-level version of BLEU which uses some kind of smoothing, but the results are still not ideal. You can also use a character-based metric, e.g. chrF (<code>sacrebleu -m chrf</code>).</p>
<p>You can also pass <code>use_effective_order=True</code> to corpus_bleu so that only the matched n-gram orders are counted instead of 4 n-grams. However, in that case, the metric is not exactly what people would refer to BLEU.</p>
",""
"66647619","2021-03-16 00:42:33","0","","66606563","<p>I was unable to find a solution with SHAP, but I found a solution using LIME. The following code displays a very similar output where its easy to see how the model made its prediction and how much certain words contributed.</p>
<pre><code>c = make_pipeline(vectorizer, classifier)

# saving a list of strings version of the X_test object
ls_X_test= list(corpus_test)

# saving the class names in a dictionary to increase interpretability
class_names = list(data.Category.unique())

# Create the LIME explainer
# add the class names for interpretability
LIME_explainer = LimeTextExplainer(class_names=class_names)

# explain the chosen prediction 
# use the probability results of the logistic regression
# can also add num_features parameter to reduce the number of features explained
LIME_exp = LIME_explainer.explain_instance(ls_X_test[idx], c.predict_proba)
LIME_exp.show_in_notebook(text=True, predict_proba=True)
</code></pre>
",""
"66643853","2021-03-15 18:46:25","0","","66624212","<p>It looks like converting the tokens to nltk.pos_tag is not quite right.
Comment that line out and the script works:</p>
<pre><code>import nltk

message = &quot;The burglar robbed the bank&quot;

#----------------------------------------------------------
# Preprocessing
#----------------------------------------------------------
def preprocess(text):
    sentences = nltk.sent_tokenize(text)                     # sentence segmentation
    sentences = [nltk.word_tokenize(s) for s in sentences]   # word tokenization
    # THIS LINE SEEMS TO BE THE ISSUE
    # sentences = [nltk.pos_tag(s) for s in sentences]         # part-of-speech tagger
    return sentences

preprocessed = preprocess(message)

#----------------------------------------------------------
# Define grammer
#----------------------------------------------------------
grammar = nltk.CFG.fromstring(&quot;&quot;&quot;
S -&gt; NP VP
NP -&gt; DT NN
VP -&gt; VBD NP
DT -&gt; 'the' | 'The'
NN -&gt; 'burglar' | 'bank'
VBD -&gt; 'robbed'
&quot;&quot;&quot;)

#----------------------------------------------------------
# Parsing
#----------------------------------------------------------
parser = nltk.ChartParser(grammar)

for sentence in preprocessed:
    for tree in parser.parse(sentence):
        print(tree)
</code></pre>
<p>Output:</p>
<pre><code>(S
  (NP (DT The) (NN burglar))
  (VP (VBD robbed) (NP (DT the) (NN bank))))
</code></pre>
",""
"66624185","2021-03-14 11:50:33","0","","66204478","<p>First we split 'S's and 'A's into groups per the rule -- we assign a unique `group' to each S followed by any number (including none) of As. We also number elements in each group in a sequence</p>
<pre><code>df['group'] = (df['First']=='S').cumsum()
df['el'] = df.groupby('group').cumcount()
</code></pre>
<p>Looks like this:</p>
<pre><code>    First    Second                                               group    el
--  -------  -------------------------------------------------  -------  ----
 0  S        Keeping the Secret of Genetic Testing                    1     0
 1  S        What is genetic risk ?                                   2     0
 2  S        Genetic risk refers more to your chance of inh...        3     0
 3  A        3 4|||Rloc-||||||REQUIRED|||-NONE-|||0                   3     1
 4  S        People get certain disease because of genetic ...        4     0
 5  A        1 2|||Wci|||develop|||REQUIRED|||-NONE-|||0              4     1
 6  A        3 4|||Nn|||diseases|||REQUIRED|||-NONE-|||0              4     2
 7  S        How much a genetic change tells us about your ...        5     0
 8  S        If your genetic results indicate that you have...        6     0
 9  A        8 8|||ArtOrDet|||the|||REQUIRED|||-NONE-|||0             6     1
</code></pre>
<p>Now we set the multi-index to 'group' and 'el' and then <code>unstack</code> 'el' into headers</p>
<pre><code>df.set_index(['group','el'])['Second'].unstack(level=1)
</code></pre>
<p>so it looks like</p>
<pre><code>  group  0                                                  1                                             2
-------  -------------------------------------------------  --------------------------------------------  -------------------------------------------
      1  Keeping the Secret of Genetic Testing              nan                                           nan
      2  What is genetic risk ?                             nan                                           nan
      3  Genetic risk refers more to your chance of inh...  3 4|||Rloc-||||||REQUIRED|||-NONE-|||0        nan
      4  People get certain disease because of genetic ...  1 2|||Wci|||develop|||REQUIRED|||-NONE-|||0   3 4|||Nn|||diseases|||REQUIRED|||-NONE-|||0
      5  How much a genetic change tells us about your ...  nan                                           nan
      6  If your genetic results indicate that you have...  8 8|||ArtOrDet|||the|||REQUIRED|||-NONE-|||0  nan
</code></pre>
<p>This looks like pretty much what you want, except the names of the columns that you can change with <code>.rename(columns = {...})</code> if you need to, and <code>.fillna(0)</code> if you want to replace NaNs with 0s</p>
",""
"66547591","2021-03-09 13:13:28","0","","66444961","<p>After some research, I created an implementation that creates a phenotype for a given genome. That was what I was looking for to evolve my individuals created using the rules from a grammar.</p>
<pre class=""lang-py prettyprint-override""><code>import nltk
from nltk import CFG

GRAMMAR = CFG.fromstring(&quot;&quot;&quot;
string -&gt; letter | letter string
letter -&gt; vowel | consonant | char
char   -&gt; ' '|'!'|'?'|','|'.'

vowel       -&gt; lower_vowel | upper_vowel
lower_vowel -&gt; 'a'|'e'|'o'|'i'|'u'
upper_vowel -&gt; 'A'|'E'|'I'|'O'|'U'

consonant       -&gt; lower_consonant | upper_consonant
lower_consonant -&gt; 'b'|'c'|'d'|'f'|'g'|'h'|'j'|'k'|'l'|'m'|'n'|'p'|'q'|'r'|'s'|'t'|'v'|'w'|'x'|'y'|'z'
upper_consonant -&gt; 'B'|'C'|'D'|'F'|'G'|'H'|'J'|'K'|'L'|'M'|'N'|'P'|'Q'|'R'|'S'|'T'|'V'|'W'|'X'|'Y'|'Z'
&quot;&quot;&quot;)

def genome_to_grammar(array):
  sb = []
  stack = [GRAMMAR.start()]
  index = 0
  wraps = 0

  while stack:
    symbol = stack.pop()
    if isinstance(symbol, str):
      sb.append(symbol)
    else:
      rules = [i for i in GRAMMAR.productions() if i.lhs().symbol() == symbol.symbol()]
      rule_index = 0
      if len(rules) &gt; 1:
        rule_index = array[index] % len(rules)
        index += 1
        if index &gt;= len(array):
          index = 0
          wraps += 1
          if wraps &gt; 10:
            return None
      rule = rules[rule_index]
      for production in reversed(rule.rhs()):
        stack.append(production)

  return ''.join(sb)

genome = [253, 69, 221, 97, 190, 254, 67, 137, 95, 72, 54, 232, 11, 136]
print(genome_to_grammar(genome))

</code></pre>
",""
"66530792","2021-03-08 13:31:30","1","","66530272","<p>In order to make <code>spacy</code> faster when you only wish to tokenize.<br />
you can change:</p>
<pre><code>nlp = es_core_news_sm.load()
</code></pre>
<p>To:</p>
<pre><code>nlp = spacy.load(&quot;es_core_news_sm&quot;, disable=[&quot;tagger&quot;, &quot;ner&quot;, &quot;parser&quot;])
</code></pre>
<p>A small explanation:<br />
Spacy gives a full language model which not merely tokenize your sentence but also do parsing, and pos and ner tagging. when actually most of the calculation time is being done for the other tasks (parse tree, pos, ner) and not the tokenization which is actually much 'lighter' task, computation wise.<br />
But, as you can see spacy allow you to use only what you actually need and by that save you some time.</p>
<p>Another thing, you can make your function more efferent by lowering token only once and add the stop word to spacy (even if you didn't want to do so, the fact that <code>otherCharacters </code> is a list and not a set is not very efficient ).</p>
<p>I would also add this:</p>
<pre><code>for w in stopwords.words('spanish'):
    nlp.vocab[w].is_stop = True
for w in otherCharacters:
    nlp.vocab[w].is_stop = True
for w in STOP_WORDS:
    nlp.vocab[w].is_stop = True
</code></pre>
<p>and than:</p>
<pre><code>for token in tokenized_phrase:
    if not token.is_punct and  not token.is_stop:
        sentence_tokens.append(token.text.lower())
</code></pre>
",""
"66433607","2021-03-02 05:04:34","3","","66433496","<p>The way <code>add_pipe</code> works changed in v3; components have to be registered, and can then be added to a pipeline just using their name. In this case you have to wrap the LanguageDetector like so:</p>
<pre><code>import scispacy
import spacy
import en_core_sci_lg
from spacy_langdetect import LanguageDetector

from spacy.language import Language

def create_lang_detector(nlp, name):
    return LanguageDetector()

Language.factory(&quot;language_detector&quot;, func=create_lang_detector)

nlp = en_core_sci_lg.load(disable=[&quot;tagger&quot;, &quot;ner&quot;])
nlp.max_length = 2000000
nlp.add_pipe('language_detector', last=True)
</code></pre>
<p>You can read more about how this works in <a href=""https://spacy.io/api/language#factory"" rel=""noreferrer"">the spaCy docs</a>.</p>
",""
"66356985","2021-02-24 18:43:08","2","","66356878","<pre><code>bigrams = [[id, ' '.join(b)] for id, l in zip(df['ID'].tolist(), df['Name'].tolist()) for b in zip(l.split(&quot; &quot;)[:-1], l.split(&quot; &quot;)[1:])]
bigrams_df = pd.DataFrame(bigrams, columns = ['ID','Name'])
</code></pre>
",""
"66352012","2021-02-24 13:35:42","0","","66350670","<p>There are several issues with your calculations.</p>
<p><strong>First</strong>, there are multiple conventions on how to calculate TF (see the <a href=""https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Term_frequency_2"" rel=""nofollow noreferrer"">Wikipedia entry</a>); scikit-learn does <em>not</em> normalize it with the document length. From the <a href=""https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting"" rel=""nofollow noreferrer"">user guide</a>:</p>
<blockquote>
<p>[...] the term frequency, the number of times a term occurs in a given document [...]</p>
</blockquote>
<p>So, here, <code>TF(&quot;apple&quot;, Document_1) = 1</code>, and not 0.5</p>
<p><strong>Second</strong>, regarding the IDF definition - from the <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn-feature-extraction-text-tfidftransformer"" rel=""nofollow noreferrer"">docs</a>:</p>
<blockquote>
<p>If <code>smooth_idf=True</code> (the default), the constant ‚Äú1‚Äù is added to the numerator and denominator of the idf as if an extra document was seen containing every term in the collection exactly once, which prevents zero divisions: idf(t) = log [ (1 + n) / (1 + df(t)) ] + 1.</p>
</blockquote>
<p>So, here we will have</p>
<pre><code>IDF (&quot;apple&quot;) = ln(5+1/3+1) + 1 = 1.4054651081081644
</code></pre>
<p>hence</p>
<pre><code>TF-IDF(&quot;apple&quot;) = 1 * 1.4054651081081644 =  1.4054651081081644
</code></pre>
<p><strong>Third</strong>, with the default setting <code>norm='l2'</code>, there is an extra normalization taking place; from the docs again:</p>
<blockquote>
<p>Normalization is ‚Äúc‚Äù (cosine) when <code>norm='l2'</code>, ‚Äún‚Äù (none) when <code>norm=None</code>.</p>
</blockquote>
<p>Explicitly removing this extra normalization from your example, i.e.</p>
<pre><code>vect = TfidfVectorizer(min_df=1, stop_words=&quot;english&quot;, norm=None)
</code></pre>
<p>gives for <code>'apple'</code></p>
<pre><code>(0, 0)  1.4054651081081644
</code></pre>
<p>i.e. as already calculated manually</p>
<p>For the details of how exactly the normalization affects the calculations when <code>norm='l2'</code> (the default setting), see the <a href=""https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting"" rel=""nofollow noreferrer"">Tf‚Äìidf term weighting</a> section of the user guide; by their own admission:</p>
<blockquote>
<p>the tf-idfs computed in scikit-learn‚Äôs <code>TfidfTransformer</code> and <code>TfidfVectorizer</code> differ slightly from the standard textbook notation</p>
</blockquote>
",""
"66326947","2021-02-23 04:12:23","0","","66326872","<p>I found the reason</p>
<p>I forgot to choose only text column in splitting records</p>
<pre><code>xtrain, xval, ytrain, yval = train_test_split(X[&quot;RepText&quot;], y, test_size=0.2, random_state=9)
</code></pre>
",""
"66268015","2021-02-18 20:52:43","0","","62601020","<p>One way to achieve this seems to involve making the tokenizer both</p>
<ol>
<li>break up tokens containing a tag without whitespace, and</li>
<li>&quot;lump&quot; tag-like sequences as single tokens.</li>
</ol>
<p>To split up tokens like the one in your example, you can modify the tokenizer infixes (in <a href=""https://github.com/explosion/spaCy/issues/3673"" rel=""nofollow noreferrer"">the manner described here</a>):</p>
<pre><code>infixes = nlp.Defaults.infixes + [r'([&gt;&lt;])']
nlp.tokenizer.infix_finditer = spacy.util.compile_infix_regex(infixes).finditer
</code></pre>
<p>To ensure tags are regarded as single tokens, you can use &quot;special cases&quot; (see <a href=""https://spacy.io/usage/spacy-101#annotations-token"" rel=""nofollow noreferrer"">the tokenizer overview</a> or the <a href=""https://spacy.io/api/tokenizer#add_special_case"" rel=""nofollow noreferrer"">method docs</a>). You would add special cases for opened, closed and empty tags, e.g.:</p>
<pre><code># open and close
for tagName in &quot;html body i br p&quot;.split():
    nlp.tokenizer.add_special_case(f&quot;&lt;{tagName}&gt;&quot;, [{ORTH: f&quot;&lt;{tagName}&gt;&quot;}])    
    nlp.tokenizer.add_special_case(f&quot;&lt;/{tagName}&gt;&quot;, [{ORTH: f&quot;&lt;/{tagName}&gt;&quot;}])    

# empty
for tagName in &quot;br p&quot;.split():
    nlp.tokenizer.add_special_case(f&quot;&lt;{tagName}/&gt;&quot;, [{ORTH: f&quot;&lt;{tagName}/&gt;&quot;}])    
</code></pre>
<p>Taken together:</p>
<pre><code>import spacy
from spacy.symbols import ORTH

nlp = spacy.load(&quot;en_core_web_trf&quot;)
infixes = nlp.Defaults.infixes + [r'([&gt;&lt;])']
nlp.tokenizer.infix_finditer = spacy.util.compile_infix_regex(infixes).finditer

for tagName in &quot;html body i br p&quot;.split():
    nlp.tokenizer.add_special_case(f&quot;&lt;{tagName}&gt;&quot;, [{ORTH: f&quot;&lt;{tagName}&gt;&quot;}])    
    nlp.tokenizer.add_special_case(f&quot;&lt;/{tagName}&gt;&quot;, [{ORTH: f&quot;&lt;/{tagName}&gt;&quot;}])    

for tagName in &quot;br p&quot;.split():
    nlp.tokenizer.add_special_case(f&quot;&lt;{tagName}/&gt;&quot;, [{ORTH: f&quot;&lt;{tagName}/&gt;&quot;}])    
</code></pre>
<p>This seems to yield the expected result. E.g., applying ...</p>
<pre><code>text = &quot;&quot;&quot;&lt;body&gt;documentation&lt;br/&gt;The Observatory &lt;p&gt; Safety &lt;/p&gt; System&lt;/body&gt;&quot;&quot;&quot;
print(&quot;Tokenized:&quot;)
for t in nlp(text):
    print(t)
</code></pre>
<p>... will print the tag in its entirety and on its own:</p>
<pre><code># ... snip
documentation
&lt;br/&gt;
The
# ... snip
</code></pre>
<p>I found <a href=""https://spacy.io/api/tokenizer#explain"" rel=""nofollow noreferrer"">the tokenizer's explain method</a> quite helpful in this context. It gives you a breakdown of what was tokenized why.</p>
",""
"66214452","2021-02-15 19:45:22","2","","66213829","<p>The index you're passing as <code>docnames</code> is empty which is obtained from <code>dataset</code> as follows:</p>
<pre><code>docnames = [&quot;Doc &quot; + str(i) for i in range(len(dataset))]
</code></pre>
<p>So this means that the <code>dataset</code> is empty too. For a workaround, you can create <code>Doc</code> indices based on the size of <code>lda_output</code> as follows:</p>
<pre><code>docnames = [&quot;Doc &quot; + str(i) for i in range(len(lda_output))]
</code></pre>
<p>Let me know if this works.</p>
",""
"66210534","2021-02-15 15:11:24","1","","66199151","<p>Spell-checking is a rather heavy processing.</p>
<p>You can try to filter out some tokens in dict_misspell, in order to call <code>correction</code> on less words. You can analyse the unknown words of a subset of your comments and create some rules to filter some kind of tokens.</p>
<p>Exemple : words with less than 2 characters; ones having numbers inside; emojis; named entities; ...).</p>
",""
"66091377","2021-02-07 18:07:58","7","","66091139","<p>If you increase the max_features:</p>
<pre><code>vectorizer = TfidfVectorizer(max_features=10)
X = vectorizer.fit_transform(corpus).todense()
df = pd.DataFrame(X, columns=vectorizer.get_feature_names())
print(df)
   bob       hi       is       my     name      sara 
0  0.574962  0.40909  0.40909  0.40909  0.40909  0.000000 
1  0.000000  0.40909  0.40909  0.40909  0.40909  0.574962
</code></pre>
<p>You can see that sara and bob are really important, since tfidf is higher for those and smaller and equal for the other, what makes sense since are repeated in both sentences.</p>
<p>Notice that as in <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"" rel=""nofollow noreferrer"">here</a>.  As in <code>max_features</code>:
&quot;If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.&quot; So it maybe remove the more useful words like in previous case.</p>
<p>Perhaps you may be interested more in the option <code>max_df</code> or <code>min_df</code>:</p>
<pre><code>vectorizer = TfidfVectorizer(max_df=0.5)
X = vectorizer.fit_transform(corpus).todense()
df = pd.DataFrame(X, columns=vectorizer.get_feature_names())
print(df)
   bob  sara
0  1.0   0.0
1  0.0   1.0
</code></pre>
<p>Perhaps is best to try different approach until you get a sense of what is going on.</p>
<p>From another point of view it could be good to remove some of the stop words too.</p>
",""
"66086250","2021-02-07 09:14:19","0","","66085788","<p>All you need to do is to create a <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset"" rel=""nofollow noreferrer""><code>tf.data.Dataset</code></a> with these two tensors as argument to the <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices"" rel=""nofollow noreferrer""><code>from_tensor_slices</code></a> static method.</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf

x = [&quot;It is an apple&quot;]
y = [&quot;It is a pear&quot;]

xy = tf.data.Dataset.from_tensor_slices((x, y))
print(xy)
&gt;&gt;&gt; &lt;TensorSliceDataset shapes: ((), ()), types: (tf.string, tf.string)&gt;
</code></pre>
<p>This corresponds to the Dataset signature that you are looking for. You can create a prefetch dataset with <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#prefetch"" rel=""nofollow noreferrer""><code>prefetch</code></a> method:</p>
<pre class=""lang-py prettyprint-override""><code>dataset = xy.prefetch(1)
print(dataset)
&gt;&gt;&gt; &lt;PrefetchDataset shapes: ((), ()), types: (tf.string, tf.string)&gt;
</code></pre>
",""
"66035638","2021-02-03 21:05:17","5","","66034713","<p><strong>Try this:</strong></p>
<pre><code>function translate() {
  var doc = DocumentApp.openById('documentID');
  var body = doc.getBody();
  var str = &quot;An elephant is the biggest living animal on land. It is quite huge in size. It is usually black or grey in colour. Elephants have four legs, a long trunk and two white tusks near their trunk. Apart from this, they have two big ears and a short tail. Elephants are vegetarian. They eat all kinds of plants especially bananas. They are quite social, intelligent and useful animals. They are used to carry logs of wood from one place to another. They are good swimmers.&quot;;
  //Split paragraph into sentences.
  var result = str.match( /[^\.!\?]+[\.!\?]+/g ).map(str =&gt; str.trim());;
  var translated = [];
  result.forEach(res =&gt; {
    translated.push(LanguageApp.translate(res, 'en', 'es'));
  })

  var table = body.appendTable();
  for(var i = 0; i &lt;= result.length; i++){
    var tr = table.appendTableRow();
    if(i == 0){
      tr.appendTableCell(&quot;Origin&quot;);
      tr.appendTableCell(&quot;Translated&quot;);
      tr.appendTableCell(&quot;&quot;);
    }else{
      tr.appendTableCell(result[i-1]);
      tr.appendTableCell(translated[i-1]);
      tr.appendTableCell(&quot;&quot;);
    }
  }
}
</code></pre>
<p><strong>Output:</strong></p>
<p><a href=""https://i.sstatic.net/QqUIM.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/QqUIM.png"" alt=""output"" /></a></p>
<h2>References:</h2>
<ul>
<li><a href=""https://developers.google.com/apps-script/reference/language/language-app#translatetext,-sourcelanguage,-targetlanguage"" rel=""nofollow noreferrer"">translate(text, sourceLanguage, targetLanguage)</a></li>
<li><a href=""https://developers.google.com/apps-script/reference/document/table#appendtablerow"" rel=""nofollow noreferrer"">appendTableRow()</a></li>
<li><a href=""https://developers.google.com/apps-script/reference/document/table-row#appendtablecell"" rel=""nofollow noreferrer"">appendTableCell()</a></li>
</ul>
",""
"65689039","2021-01-12 17:35:12","0","","65645289","<p>At least I got an answer to the second question:</p>
<p>Those are probabilities, but not in the way I thought.</p>
<p>For instance, predicted probability for Class X is 0.808. If now the word 'recognit' would be removed from the underlying corpus, the total predicted probability for the predicted class would shrink by 0.008.--&gt; probability class x equals 0.800 then.</p>
<p>For detailed information about LIME I highly reccomend:
<em>‚ÄúWhy Should I Trust You?‚Äù Explaining the Predictions of Any Classifier, Riberio et.al (2016)</em></p>
",""
"65678335","2021-01-12 05:13:26","4","","65677406","<p>This is not really surprising. Imagine that you are interested in classification and you have a dataset that looks like this -</p>
<p><a href=""https://i.sstatic.net/aVZgw.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/aVZgw.png"" alt=""enter image description here"" /></a></p>
<p>If you are trying to fit a decision tree to this, it's very easy for it to find the decision boundary and hence you classification accuracy will be quite good.</p>
<p>Now imagine if you are trying to scale it first. The new dataset will look like this -
<a href=""https://i.sstatic.net/i98AD.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/i98AD.png"" alt=""enter image description here"" /></a></p>
<p>As you can see, there is a lot more overlap between the data so it's more difficult for the model to find a decision boundary.</p>
<p>When you are scaling the data, you are bringing the two axes closer to each other. This might have the effect of making them less distinguishable.</p>
<p>At this point you might be wondering, if this is the case, why do we bother doing this re-scaling at all. After all, this effect will be seen regardless of what model you use. While that is true, and doing can have an effect of making the data less distinguable, in models like Neural Net, if you don't do this scaling operation, there are a lot of other downsides that will pop up. Like the weights of one feature being artificially inflated, or gradients not flowing properly and so on. In that case, the advantages of scaling might overweigh the disadvantages and you can still end up with a good model.</p>
<p>As to your question on why there would be a difference in speed, the same effect, the random forest will probably have to search for a longer time to get a good fit in the latter case with the same parameters. It's not really surprising.</p>
<p>Here is the code used to produce the plots -</p>
<pre><code>import numpy as np
import matplotlib.pyplot as plt

from sklearn.preprocessing import MaxAbsScaler

std = 7
X = np.random.multivariate_normal([2, 2], [[std, 0], [0, std]], size=100)

Y = np.random.multivariate_normal([10, 10], [[std, 0], [0, std]], size=100)

plt.scatter(X[:, 0], X[:, 1])
plt.scatter(Y[:, 0], Y[:, 1])
plt.show()

scaler = MaxAbsScaler()
X_scaled = scaler.fit_transform(X)
Y_scaled = scaler.fit_transform(Y)

plt.scatter(X_scaled[:, 0], X_scaled[:, 1])
plt.scatter(Y_scaled[:, 0], Y_scaled[:, 1])
plt.show()  
</code></pre>
",""
"65600943","2021-01-06 17:56:44","4","","65600562","<p>You may find better answers but I personally find the <code>LemmInflect</code> library to be the best for lemmatization and inflections.</p>
<pre><code>#!pip install lemminflect
from lemminflect import getLemma, getInflection, getAllLemmas

word = 'testing'
lemma = list(lemminflect.getAllLemmas(word, upos='NOUN').values())[0]
inflect = lemminflect.getInflection(lemma[0], tag='VBD')

print(word, lemma, inflect)
</code></pre>
<pre><code>testing ('test',) ('tested',)
</code></pre>
<p>I would avoid stemming because it's not really useful if you want to work with language models or just text classification with any context. Stemming and Lemmatization both generate the root form of the inflected words. <strong>The difference is that stem might not be an actual word whereas, lemma is an actual language word</strong>.</p>
<p>Inflections are the opposite of a lemma.</p>
<hr />
<pre><code>sentence = ['I', 'am', 'testing', 'my', 'new', 'library']

def l(sentence):
    lemmatized_sent = []
    for i in sentence:
        try: lemmatized_sent.append(list(getAllLemmas(i, upos='NOUN').values())[0][0])
        except: lemmatized_sent.append(i)
    return lemmatized_sent

l(sentence)
</code></pre>
<pre><code>['I', 'be', 'test', 'my', 'new', 'library']
</code></pre>
<pre><code>#To apply to dataframe use this
df['sentences'].apply(l)
</code></pre>
<p>Do read the <a href=""https://github.com/bjascob/LemmInflect"" rel=""nofollow noreferrer"">documentation</a> for LemmInflect. You can do so much more with it.</p>
",""
"65490127","2020-12-29 09:37:06","3","","65484081","<p>This is not how an MT model is supposed to be used. It is not a GPT-like experiment to test if the model can understand instruction.  It is a translation model that only can translate, there is no need to add the instruction <code>&quot;translate English to Dutch&quot;</code>. (Don't you want to translate the other way round?)</p>
<p>Also, the translation models are trained to translate sentence by sentence. If you concatenate all sentences from the column, it will be treated as a single sentence. You need to either:</p>
<ol>
<li><p>Iterate over the column and translate each sentence independently.</p>
</li>
<li><p>Split the column into batches, so you can parallelize the translation. Note that in that case, you need to pad the sentences in the batches to have the same length. The easiest way to do it is by using the <code>batch_encode_plus</code> method of the tokenizer.</p>
</li>
</ol>
",""
"65488746","2020-12-29 07:28:41","4","","65488631","<p>What you are trying to do won't work because you are applying a string function (split) to a Word List.
I would try to use <code>nltk</code>, instead, and create a new pandas column with my tokenized data:</p>
<pre><code>import nltk
df_toklem['tokenized'] = df_toklem.apply(lambda row: nltk.word_tokenize(row['script']))
</code></pre>
",""
"65476289","2020-12-28 10:36:19","2","","65454578","<p>NLTK and SacreBLEU use different tokenization rules, mostly in how they handle punctuation. NLTK uses its own tokenization, whereas SacreBLEU replicates the original Perl implementation from 2002. The tokenization rules are probably more elaborate in NLTK, but they make the number incomparable with the original implementation.</p>
<p>The corpus BLEU that you got from SacreBLEU is not 67.8%, but 0.67% ‚Äì the numbers from SacreBLEU are already multiplied by 100, unlike NLTK. So, I would not say there is a huge difference between the scores.</p>
<p>The sentence-level BLEU can use different <a href=""https://www.aclweb.org/anthology/W14-3346.pdf"" rel=""noreferrer"">smoothing techniques</a> that should ensure that score would get reasonable values even if 3-gram of 4-gram precision would be zero. However, note that BLEU as a sentence-level metric is very unreliable.</p>
",""
"65361196","2020-12-18 17:16:44","0","","65273410","<p>Found the answer! Just wrap the expression within <code>/ /</code> and it works!
For eg.</p>
<pre><code>{} &lt;&lt;  /obl:from/  {} 
</code></pre>
",""
"65160717","2020-12-05 18:49:43","0","","65153422","<p>You can try:</p>
<pre><code>import spacy
import pandas as pd
nlp = spacy.load('en_core_web_sm',disable=['ner','textcat'])

texts = ['&quot;I will meet quite a few people, it\'s well', 
         'Says &quot;Cristiano Ronaldo\'s family still owns&quot;',
         'Joe Biden plagiarized Donald Trump in his...']

df = pd.DataFrame({&quot;Text&quot;:texts})

d = dict()
def func(text):
    doc = nlp(text)
    for tok in doc:
        if tok.pos_ not in d:
            d[tok.pos_] = [tok.text]
        else:
            d[tok.pos_].append(tok.text)
            
df.Text.apply(func)

pprint(d)
</code></pre>
<hr />
<pre><code>{'ADJ': ['few'],
 'ADP': ['in'],
 'ADV': ['well', 'still'],
 'AUX': [&quot;'s&quot;],
 'DET': ['quite', 'a', 'his'],
 'NOUN': ['people', 'family'],
 'PART': [&quot;'s&quot;],
 'PRON': ['I', 'it'],
 'PROPN': ['Cristiano', 'Ronaldo', 'Joe', 'Biden', 'Donald', 'Trump'],
 'PUNCT': ['&quot;', ',', '&quot;', '&quot;', '...'],
 'VERB': ['will', 'meet', 'Says', 'owns', 'plagiarized']}
</code></pre>
<p>Note, you don't need pandas dependence at all:</p>
<pre><code>docs = nlp.pipe(texts)
d = dict()
for doc in docs:
    for tok in doc:
        if tok.pos_ not in d:
            d[tok.pos_] = [tok.text]
        else:
            d[tok.pos_].append(tok.text)
pprint(d)
</code></pre>
<hr />
<pre><code>{'ADJ': ['few'],
 'ADP': ['in'],
 'ADV': ['well', 'still'],
 'AUX': [&quot;'s&quot;],
 'DET': ['quite', 'a', 'his'],
 'NOUN': ['people', 'family'],
 'PART': [&quot;'s&quot;],
 'PRON': ['I', 'it'],
 'PROPN': ['Cristiano', 'Ronaldo', 'Joe', 'Biden', 'Donald', 'Trump'],
 'PUNCT': ['&quot;', ',', '&quot;', '&quot;', '...'],
 'VERB': ['will', 'meet', 'Says', 'owns', 'plagiarized']}
</code></pre>
<p>These will collect all the tokens under their <code>POS</code>.</p>
<p>If you only need list of unique tokens:</p>
<pre><code>texts = ['&quot;I will will meet quite a few people, it\'s well', 
         'Says &quot;Cristiano Ronaldo\'s family still owns&quot;',
         'Joe Biden plagiarized Donald Trump in his...']

docs = nlp.pipe(texts)
d = dict()
for doc in docs:
    for tok in doc:
        if tok.pos_ not in d:
            d[tok.pos_] = [tok.text]
        elif tok.text not in d[tok.pos_]:
            d[tok.pos_].append(tok.text)
pprint(d)
</code></pre>
<hr />
<pre><code>{'ADJ': ['few'],
 'ADP': ['in'],
 'ADV': ['well', 'still'],
 'AUX': [&quot;'s&quot;],
 'DET': ['quite', 'a', 'his'],
 'NOUN': ['people', 'family'],
 'PART': [&quot;'s&quot;],
 'PRON': ['I', 'it'],
 'PROPN': ['Cristiano', 'Ronaldo', 'Joe', 'Biden', 'Donald', 'Trump'],
 'PUNCT': ['&quot;', ',', '...'],
 'VERB': ['will', 'meet', 'Says', 'owns', 'plagiarized']}
</code></pre>
",""
"65160499","2020-12-05 18:28:08","0","","65160400","<p>The will work :</p>
<pre><code>import nltk
from nltk.stem.snowball import SnowballStemmer

def limpiar(texto):
    words=texto.split()
    stem_words=[]
    for w in words: 
        x = snow_stemmer.stem(w) 
        stem_words.append(x) 

    #print stemming results 
    for e1,e2 in zip(words,stem_words): 
        print(e1+' ----&gt; '+e2.lower())

snow_stemmer = SnowballStemmer(language='spanish')
texto=&quot;...&quot;
limpiar(texto)
</code></pre>
",""
"65145747","2020-12-04 14:54:33","1","","65143979","<p>The best way to improve accuracy, given that you want to stick with this configuration is through <em>hyperparameter</em> tuning, or by introducing additional components, such as <em>feature selection</em>.</p>
<p><strong>Hyperparameter tuning</strong></p>
<p>Most machine learning algorithms and parts of a machine learning pipeline have several parameters you can change. For example, the <code>TfidfVectorizer</code> has different ngram ranges, different analysis levels, different tokenizers, and many more parameters to vary. Most of these will affect your performance. So, what you can do is systematically vary these parameters (and those of your <code>SVC</code>), while monitoring you accuracy on a <em>development set</em> (i.e., <strong>not the test data!</strong>). Instead of fixed development set, cross-validation is typically used in these kinds of settings.</p>
<p>The best way to do this in <code>sklearn</code> is through a <code>RandomizedSearchCV</code> (see <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html"" rel=""nofollow noreferrer"">here</a> for details). This class automatically cross-validates and searches through the possible options you pre-specify by randomly sampling from the option set for a fixed number of iterations. By applying this technique on your training data, you will automatically find models that perform better for your given training data and your options. Ideally, these models would also perform better on your test data. Fair warning: cross-validated search techniques can take a while to run.</p>
<p><strong>Feature Selection</strong></p>
<p>In addition to grid search, another way to improve performance is through <em>feature selection</em>. Feature selection typically consists of a statistical test that determines which features explain variance in the typical task you are trying to solve. The feature selection methods in <code>sklearn</code> are detailed <a href=""https://scikit-learn.org/stable/modules/feature_selection.html#feature-selection"" rel=""nofollow noreferrer"">here</a>.</p>
<p>By far the most important bit here is that the performance of anything you add to your model should be verified on an independent development set, or in cross-validation. Leave your test data alone.</p>
",""
"65074546","2020-11-30 13:31:39","6","","65068353","<p>I guess (because you didn't show the actual error indicating where the error occurs) that it's in this line:</p>
<pre><code>if len(rhs) == 2 and rhs[0] in T[i][k] and rhs[1] in T[k + 1][j]:
</code></pre>
<p>and that <code>k</code> is <code>n-1</code>. If the first two conditions are true then the third one will execute and blow up.</p>
<p>I suspect that there is an off-by-one error in the iteration limit for <code>k</code>. Some code comments would have been useful, or at least a reference to the pseudocode you based your implementation on.</p>
",""
"65050601","2020-11-28 14:22:03","3","","65040696","<p>I'm guessing that most of your issues are because you're not feeding spaCy full sentences and it's not assigning the correct part-of-speech tags to your words.  This can cause the lemmatizer to return the wrong results.  However, since you've only provided snippets of code and none of the original text, it's difficult to answer this question.  Next time consider boiling down your question to a few lines of code that someone else can run on their machine EXACTLY AS WRITTEN, and providing a sample input that fails.  See <a href=""https://stackoverflow.com/help/minimal-reproducible-example"">Minimal Reproducible Example</a></p>
<p>Here's an example that works and is close to what you're doing.</p>
<pre><code>import spacy
from nltk.corpus import stopwords
stop_words = stopwords.words('english')
allow_postags = set(['NOUN', 'VERB', 'ADJ', 'ADV', 'PROPN'])
nlp = spacy.load('en')
text = 'The children in Amman and Melbourne are too young to be driving.'
words = []
for token in nlp(text):
    if token.text not in stop_words and token.pos_ in allow_postags:
        words.append(token.lemma_)
print(' '.join(words))
</code></pre>
<p>This returns <code>child Amman Melbourne young drive</code></p>
",""
"64994600","2020-11-24 20:47:43","3","","64994311","<p>I think that you should construct lexicon dictionary only from corpus list.</p>
<p>I think you can write something like this:</p>
<pre><code>import more_itertools as mit
import re
from collections import OrderedDict

def get_vector(lexicon, text):
    zero_vector = OrderedDict((token, 0) for token in lexicon)
    corpus_tokens = list(mit.collapse(text.split()))
    for token in corpus_tokens:
        if token in zero_vector:
            zero_vector[token] = 1
    return zero_vector
    

def BoW(corpus: list, search_doc: str):
    
    word_count = {}
    
    # Regex to grab words here because its just a string
    search_doc_tokens = re.split(r'[-\s.,;!?]+', search_doc)
    
    # I have to do all this business here because it's a list of strings
    grab_words = [word.split() for word in corpus]
    corpus_tokens = list(mit.collapse(grab_words))
    
    # Concatenating the two lists  (why???)
    vocabulary = corpus_tokens #  + search_doc_tokens
    
    # Filling dictionary
    for token in vocabulary:
        if token not in word_count:
            word_count[token] = 1
        else:
            word_count[token] += 1
                    
    
    # Unique words in vocab. Used determine size of zero vector
    lexicon = sorted(set(vocabulary))
    
    for text in corpus:
        text_vector = get_vector(lexicon, text)
        print(text_vector)
        
    text_vector = get_vector(lexicon, search_doc)
    print(text_vector)
</code></pre>
<p>But it would be much better to have the vector not as ordered dict but as numpy array.</p>
<p>To transform ordered dict you can use something like this:</p>
<pre><code>import numpy as np
tv_vec = np.array(list(test_vector.values()))
</code></pre>
<p>So the question is: why do you need this BoW? How do you want to construct final matrix with vectorized texts? Do you want to include all corpus texts and search_doc together in the matrix?</p>
<p>EDIT:</p>
<p>I think you can do something like this:</p>
<pre><code>    corpus_mat = np.zeros((len(lexicon), len(corpus)))
    for ind, text in enumerate(corpus):
        text_vector = get_vector(lexicon, text)
        corpus_mat[:, ind] = np.array(list(text_vector.values()))
        
    text_vector = get_vector(lexicon, search_doc)
    text_vector = np.array(list(text_vector.values()))
    return corpus_mat, text_vector
</code></pre>
<p>And then use corpus_mat and text_vector to compute similarity with dot product:</p>
<pre><code>cm, tv = BoW(documents, &quot;hello there&quot;) 
print(cm.T @ tv)
</code></pre>
<p>The output is going to be 3 zeros, as the search_doc text has no common words with corpus texts.</p>
",""
"64866904","2020-11-16 22:54:34","1","","64866522","<p>I would suggest this solution:</p>
<pre><code># your code as it was before
words_lst = []
for i in range(len(sorted_common_words)):
    if sorted_common_words[i][1] &gt; 1000:
        words_lst.append(sorted_common_words[i][0])

import numpy as np

words_arr = np.array(words_lst,dtype=str)
words_dictionary = np.array(words.words(),dtype=str)

mask_valid_words = np.in1d(words_arr, words_dictionary)

valid_words = words_arr[mask_valid_words]
invalid_words = words_arr[~mask_valid_words]
</code></pre>
",""
"64862016","2020-11-16 16:41:47","0","","62230139","<p>In the <code>ci.yml</code> file,  adding the <code>nltk.downloader</code> commandline after importing dependencies defined in <code>requirements.txt</code> worked for me.</p>
<pre><code>if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
python -m nltk.downloader punkt stopwords
</code></pre>
",""
"64807349","2020-11-12 15:59:53","1","","64807140","<pre><code>df_tfidf['top10'] = [[(reverse_vocab.get(item), X_tfidf[i, item])  for item in row] 
                     for i, row in enumerate(tfidf_max10) ]
</code></pre>
<p>Test case:</p>
<pre><code>df = pd.DataFrame(
    {'liststring': ['this is a cat', 'that is a dog', &quot;a apple on the tree&quot;]}
)
tfidf = TfidfVectorizer()
X_tfidf = tfidf.fit_transform(df['liststring']).toarray()
vocab = tfidf.vocabulary_
reverse_vocab = {v:k for k,v in vocab.items()}
feature_names = tfidf.get_feature_names()
df_tfidf = pd.DataFrame(X_tfidf, columns = feature_names)
idx = X_tfidf.argsort(axis=1)
tfidf_max2 = idx[:,-2:]
print ([[(reverse_vocab.get(item), X_tfidf[i, item])  for item in row] 
                     for i, row in enumerate(tfidf_max2) ])
</code></pre>
<p>Output:</p>
<pre><code>[[('cat', 0.6227660078332259), ('this', 0.6227660078332259)],
 [('dog', 0.6227660078332259), ('that', 0.6227660078332259)], 
 [('the', 0.5), ('tree', 0.5)]]
</code></pre>
",""
"64770148","2020-11-10 13:47:18","0","","64731868","<p>I would add a simple pipe into the pipeline, right after the <code>tagger</code> :</p>
<pre><code>def pos_postprocessor_pipe(doc) :
    for token in doc :
        if token.text == '|':
            token.pos_ = 'PUNCT'
    return doc

 nlp = spacy.load(&quot;en_core_web_sm&quot;)
 nlp.add_pipe(pos_postprocessor_pipe, name=&quot;pos_postprocessor&quot;, after='tagger')
</code></pre>
",""
"64653722","2020-11-02 21:39:39","0","","64646551","<p>Are you looking for <code>PPER</code> <code>tag_</code> rather? Try <code>ich</code> or <code>sie</code>:</p>
<pre class=""lang-py prettyprint-override""><code>import spacy
nlp = spacy.load(&quot;de_core_news_md&quot;)
doc = nlp(&quot;Ich liebe mich. Sie liebt Dich.&quot;)

for tok in doc:
    print(f&quot;{tok.text:&lt;10} {tok.tag_:&lt;10} {tok.pos_:&lt;10}&quot;)
Ich        PPER       PRON      
liebe      VVFIN      VERB      
mich       PPER       PRON      
.          $.         PUNCT     
Sie        PPER       PRON      
liebt      VVFIN      VERB      
Dich       PPER       PRON      
.          $.         PUNCT  
</code></pre>
",""
"64613204","2020-10-30 17:31:47","2","","64613067","<p>You're not getting <code>'n'</code> as a token because it's not considered a token by default tokenizer:</p>
<pre class=""lang-py prettyprint-override""><code>from sklearn.feature_extraction.text import TfidfVectorizer

texts = [&quot;Queens Stop 'N' Swap&quot;,]
tfidf = TfidfVectorizer(token_pattern='(?u)\\b\\w\\w+\\b',)
tfidf.fit(texts)
tfidf.vocabulary_
{'queens': 0, 'stop': 1, 'swap': 2}
</code></pre>
<p>To capture 1 letter tokens, with capitalzation preserved, change it like:</p>
<pre class=""lang-py prettyprint-override""><code>tfidf = TfidfVectorizer(token_pattern='(?u)\\b\\w+\\b',lowercase=False)
tfidf.fit(texts)
tfidf.vocabulary_
{'Queens': 1, 'stop': 2, 'N': 0, 'swap': 3}
</code></pre>
",""
"64605106","2020-10-30 08:42:47","2","","64605008","<p>You could use <a href=""https://pypi.org/project/swifter/"" rel=""nofollow noreferrer"">swifter</a> to make your <code>df.apply()</code> more efficient. In addition to that, you might want to try <a href=""https://pypi.org/project/whatthelang/"" rel=""nofollow noreferrer"">whatthelang</a> library which should be more efficient than <code>langdetect</code>.</p>
",""
"64497014","2020-10-23 08:59:55","9","","64496364","<p>You don't need punctuation remover if you use <code>TfidfVectorizer</code>. It will take care of punctuation automatically, by virtue of default <code>token_pattern</code> param:</p>
<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer

df = pd.DataFrame({&quot;blurb&quot;:[&quot;this is a sentence&quot;, &quot;this is, well, another one&quot;]})
vectorizer = TfidfVectorizer(token_pattern='(?u)\\b\\w\\w+\\b')
df[&quot;tf_idf&quot;] = list(vectorizer.fit_transform(df[&quot;blurb&quot;].values.astype(&quot;U&quot;)).toarray())
vocab = sorted(vectorizer.vocabulary_.keys())
df[&quot;tf_idf_dic&quot;] = df[&quot;tf_idf&quot;].apply(lambda x: {k:v for k,v in dict(zip(vocab,x)).items() if v!=0})
</code></pre>
",""
"64461359","2020-10-21 10:14:02","0","","64460941","<p><code>tokenized_words</code> is a column of lists. The reason it's not a column of strings is because you used the <code>split</code> method. So you need to use a double for loop like so</p>
<pre class=""lang-py prettyprint-override""><code>lem = ' '.join([wnl.lemmatize(word) for word_list in tokenized_words for word in word_list])
</code></pre>
",""
"64435408","2020-10-19 21:38:28","6","","64435099","<p>You can <code>explode</code> the dataframe and then create a <code>pivot_table</code>:</p>
<pre><code>df = pd.DataFrame({'emp_id' : ['E0001', 'E0002', 'E0003', 'E0004', 'E0005'],
                  'text' : [['T0431516',-8,'T0401531',-12,'T0517519',12],
                 ['T0701540',-1,'T0431516',-2],['T0517519',-1,'T0421531',-7,'T0516319',9,'T0500371',-6,'T0309711',-3],
                 ['T0516319',-3], ['T0431516',2]]})
df = df.explode('text')
df['freq'] = df['text'].shift(-1)
df = df[df['text'].str[0] == 'T']
df['freq'] = df['freq'].astype(int)
df = pd.pivot_table(df, index='emp_id', columns='text', values='freq',aggfunc = 'sum').fillna(0).astype(int)
df
Out[1]: 
text    T0309711  T0401531  T0421531  T0431516  T0500371  T0516319  T0517519  \
emp_id                                                                         
E0001          0       -12         0        -8         0         0        12   
E0002          0         0         0        -2         0         0         0   
E0003         -3         0        -7         0        -6         9        -1   
E0004          0         0         0         0         0        -3         0   
E0005          0         0         0         2         0         0         0   

text    T0701540  
emp_id            
E0001          0  
E0002         -1  
E0003          0  
E0004          0  
E0005          0  
</code></pre>
",""
"64278592","2020-10-09 10:52:34","2","","64236859","<p>While training your model you can store the weights in a collection of files formatted as <code>checkpoints</code> that contain only the weights trained in a binary format.</p>
<p>In particular, the checkpoints contain:</p>
<ul>
<li>one or more blocks that contain the weights of our model</li>
<li>an index file indicating which weights are stored in a particular block</li>
</ul>
<p>So the fact that the size of the checkpoint file is always the same depends on the fact that the model used is always the same. So the number of model parameters is always the same so the size of the weights you are going to save is always the same. While the suffix <code>data-00000-of-00001</code> indicates that you are training the model on a single machine.</p>
<p>The size of the dataset, in my opinion, has nothing to do with it.</p>
",""
"64198661","2020-10-04 18:55:10","0","","64198426","<p>This problem is pretty basic if you can describe which lines to use for the keys and values that you want to retain.  Looking at the data here, it seems that you want to exclude item in the input list that:</p>
<ol>
<li>aren't just letters and '.'</li>
<li>are just the letter 'O'</li>
</ol>
<p>After excluding the items you don't want to use, the keys and values for the dictionary items are just pairs...[K, V, K, V...].  If it turns out that this doesn't work for all of your data, then you need to figure out what the right selection criteria is to delete all but the lines that make up the pairs you want to create dictionaries from.</p>
<p>Here's the code that uses the above criteria to give you what you want:</p>
<pre><code>data = ['Football',
 'NNP',
 'I-NP',
 'O',
 '-',
 ':',
 'O',
 'O',
 'Baltimore',
 'NNP',
 'I-NP',
 'B-ORG',
 'pulled',
 'NNP',
 'I-NP',
 'O',
 'off',
 'IN',
 'I-PP',
 'O',
 'a',
 'IN',
 'I-NP',
 'O',
 'victory',
 'NN',
 'I-NP',
 'O',
 '.',
 '.',
 'O',
 'O']

data = [x for x in data if re.match(r&quot;^[a-zA-Z.]+$&quot;, x) and x != 'O']

result = []
for i in range(0, len(data), 2):
     result.append({data[i]: data[i+1]})

print(result)
</code></pre>
<p>Result:</p>
<pre><code>[{'Football': 'NNP'}, {'Baltimore': 'NNP'}, {'pulled': 'NNP'}, {'off': 'IN'}, {'a': 'IN'}, {'victory': 'NN'}, {'.': '.'}]
</code></pre>
",""
"64188206","2020-10-03 19:30:50","0","","64185831","<p>You can use</p>
<pre><code>&gt;&gt;&gt; [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]
['look', 'lemma', 'help', 'find']
</code></pre>
<p>The following parts have been added:</p>
<ul>
<li><code>if not token.is_stop</code> - if the token is a stopword</li>
<li><code>and</code> - and</li>
<li><code>not token.is_punct</code>  - if the token is punctuation, omit them.</li>
</ul>
",""
"64173997","2020-10-02 15:11:04","0","","64172681","<p>As mentioned in the comments, you can use a library like beautifulsoup or lxml to get the job done. There are several way to approach it. Here's one, using beautifulsoup, for example:</p>
<pre><code>import pandas as pd
from bs4 import BeautifulSoup as bs
import requests

url = &quot;http://www.mso.anu.edu.au/%7Eralph/OPTED/v003/wb1913_e.html&quot;
req=requests.get(url)

soup = bs(req.text,'lxml')
columns = ['word','part','meaning']
entries = []
for p in soup.select('p'):
    entry = []
    prt = p.select_one('i').text if len(p.select_one('i').text)&gt;0 else &quot;na&quot;
    entry.extend([p.select_one('b').text, prt, p.text.split(') ')[-1]])
    entries.append(entry)
pd.DataFrame(entries, columns=columns)
</code></pre>
<p>Output:</p>
<pre><code>   word part    meaning
0   E   na  The fifth letter of the English alphabet.
1   E   na  is a tone which is intermediate between D and E.
2   E-  na  A Latin prefix meaning out, out of, from; also...
3   Each    a. / a. pron.   Every one of the two or more individuals compo...
</code></pre>
<p>etc.</p>
",""
"64097702","2020-09-28 07:28:39","0","","64083752","<p>In the example you found the idea is to use the conventional names for syntactic constituent elements of sentences to create a <em>chunker</em> - a parser that breaks down sentences to a desired level of rather coarse-grained pieces. This simple(istic?) approach is used in favour of a full syntactic parse - which would require breaking the utterances down to word-level and labelling each word with appropriate function in the sentence.</p>
<p>The grammar defined in the parameter of <code>RegexParser</code> is to be chosen arbitrarily depending on the need (and structure of the utterances it is to apply to). These rules can be recurrent - they correspond to the ones of <a href=""https://en.wikipedia.org/wiki/Backus%E2%80%93Naur_form"" rel=""nofollow noreferrer"">BNF</a> formal grammar. Your observation is then valid - the last rule for <code>VP</code> refers to the previously defined rules.</p>
",""
"63999874","2020-09-21 20:56:15","1","","63999500","<p>I had the same problem when doing NLP analysis for a paper I wrote. I had to use a mapping function like this:</p>
<pre><code>import nltk
from nltk.tokenize import word_tokenize

def get_full_tag_pos(pos_tag):
    tag_dict = {&quot;J&quot;: &quot;ADJ&quot;,
                &quot;N&quot;: &quot;NOUN&quot;,
                &quot;V&quot;: &quot;VERB&quot;,
                &quot;R&quot;: &quot;ADV&quot;}
    # assuming pos_tag comes in as capital letters i.e. 'JJR' or 'NN'
    return tag_dict.get(pos_tag[0], 'NOUN')

# example
words = word_tokenize(text)
words_pos = nltk.pos_tag(words)
full_tag_words_pos = [word_pos[0] + &quot;/&quot; + get_full_tag_pos(word_pos[1]) for word_pos in words_pos]
</code></pre>
",""
"63810040","2020-09-09 10:49:39","0","","63802521","<p>Try:</p>
<pre><code>import spacy
lemma_lookup = spacy.lang.en.LOOKUP

reverse_lemma_lookup = {}

for word,lemma in lemma_lookup.items():
    if not reverse_lemma_lookup.get(lemma):
        reverse_lemma_lookup[lemma] = [word]
    elif word not in reverse_lemma_lookup[lemma]:
        reverse_lemma_lookup[lemma].append(word)

reverse_lemma_lookup[&quot;be&quot;]
[&quot;'m&quot;, 'am', 'are', 'arst', 'been', 'being', 'is', 'm', 'was', 'wass', 'were']
</code></pre>
<p>Change <code>spacy.lang.en.LOOKUP</code> to <code>spacy.lang.fr.LOOKUP</code> I guess for French</p>
",""
"63808958","2020-09-09 09:41:20","0","","63807411","<p>Here you go:</p>
<ol>
<li>Use <code>apply</code> to apply on the column's sentences</li>
<li>Use lambda expression that gets a <code>sentence</code> as input and applies the function you wrote, in a similar to how you used in the print statement</li>
</ol>
<p>As lemmatized keywords:</p>
<pre><code># Lemmatize a Sentence with the appropriate POS tag
df['keywords'] =  df['keywords'].apply(lambda sentence: [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sentence)])
</code></pre>
<p>As a lemmatized sentence (<a href=""https://docs.python.org/3/library/stdtypes.html#str.join"" rel=""nofollow noreferrer""><code>join</code></a> keywords using ' '):</p>
<pre><code># Lemmatize a Sentence with the appropriate POS tag
df['keywords'] =  df['keywords'].apply(lambda sentence: ' '.join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sentence)]))
</code></pre>
",""
"63792090","2020-09-08 10:34:11","0","","63790729","<p>The final one, <em>Author identification</em>. You don't need to have any understanding of the language you are dealing with, which the first three presuppose.</p>
<p>There is already a lot of literature on the topic; generally you identify features in texts, and map these onto a set of authors' known features. This can easily be done with cluster analysis or Machine Learning. So, it's not actually as NLP-heavy as the others.</p>
",""
"63779708","2020-09-07 14:36:44","0","","63778133","<p>Lets start by defining terms</p>
<p>Reference: The actual text/ground truth. If there are multiple people generating the ground truth for same datapoint you will have multiple references and all of them are assumed to be correct</p>
<p>hypothesis: The candidate/predicted.</p>
<p>Lets say the 2 people look at an image and they caption</p>
<ul>
<li>this is an apple</li>
<li>that is an apple</li>
</ul>
<p>Now your model looked at the image and predicted</p>
<ul>
<li>an apple on this tree</li>
</ul>
<p>You can calculate the meteor_score of how good the prediction was using</p>
<pre><code>print (nltk.translate.meteor_score.meteor_score(
    [&quot;this is an apple&quot;, &quot;that is an apple&quot;], &quot;an apple on this tree&quot;))
print (nltk.translate.meteor_score.meteor_score(
    [&quot;this is an apple&quot;, &quot;that is an apple&quot;], &quot;a red color fruit&quot;))
</code></pre>
<p>Output:</p>
<pre><code>0.6233062330623306
0.0
</code></pre>
<p>In your case you have to read <code>reference.txt</code> into a list and similarly model predicitons into another. Now you have to get the <code>meteor_score</code> for each line in the first list with the each line in the second list and finally take a mean.</p>
",""
"63760442","2020-09-06 03:27:54","2","","63747454","<p>&quot;200k-record dataset is taking 4 hours per-epoch&quot; doesn't tell us much:</p>
<ol>
<li>Make sure you're not blowing out memory (are you?) How much RAM is it taking?</li>
<li><strong>You are presumably running single-thread, due to the GIL</strong>. See e.g. <a href=""https://medium.com/@vishnups/speeding-up-spacy-f766e3dd033c"" rel=""nofollow noreferrer"">this</a> on how to turn off the GIL to run training multicore. How many cores do you have?</li>
</ol>
<ul>
<li><strong>putting <code>texts = [nlp(text) ...]</code> inside your inner-loop <code>for batch in minibatch(TRAIN_DATA, size=8):</code> looks like trouble, because your code will always hold the GIL, even though you only need it for C-library string calls on processing the input text i.e. the <code>parser</code> stage, not for training.</strong></li>
<li>Refactor your code so you first run <code>nlp()</code> pipeline on all your input, then save some intermediate representation (array or whatever). Keep that code separate to your training loop, so training can be multithreaded.</li>
</ul>
<ol start=""3"">
<li>I can't comment on your choice of <code>minibatch()</code> parameters, but 8 seems very small, and those parameters seem to matter for performance, so try tweaking them (/grid-search a few values).</li>
<li>Finally, once you first check all of the above, find the fastest unicore/multicore box you can, and with enough RAM.</li>
</ol>
",""
"63660877","2020-08-30 18:35:42","0","","63653161","<p>Found the answer.</p>
<p>sklearn.metrics's confusion_matrix has an agruement called <strong>labels</strong>.</p>
<p>If you put all your distinct labels inside of a list for example, the order of the labels in the list will propagate and dictate the order of the confusion matrix.</p>
<p>my example:</p>
<pre><code>class_names = df.labels.unique() # Here im making the unique labels object the order here will dictate the order in confusion matrix

cm = confusion_matrix(y_test, y_predicted_counts, labels=class_names) #here i tell the cm to use my class_names object for ordering

fig = plt.figure(figsize=(20, 20))
plot = plot_confusion_matrix(cm, classes = class_names, normalize=False, title='Confusion matrix')
plt.show()
</code></pre>
",""
"63452414","2020-08-17 13:59:22","1","","63434178","<p>First: if you want 5 articles then instead of <code>[:-5:-1]</code> you have to use <code>[:-6:-1]</code> because for negative values it works little different.</p>
<p>Or use <code>[::-1][:5]</code> - <code>[::-1]</code> will reverse all values and then you can use normal <code>[:5]</code></p>
<hr />
<p>When you have <code>related_docs_indices</code> then you can use <code>.iloc[]</code> to get elements from <code>DataFrame</code></p>
<pre><code> sample_df.iloc[ related_docs_indices ]
</code></pre>
<p>If you will have elements with the same similarity then it will gives them in reversed order.</p>
<hr />
<p><strong>BTW:</strong></p>
<p>You can also add <code>similarities</code> to <code>DataFrame</code></p>
<pre><code>sample_df['similarity'] = cosine_similarities
</code></pre>
<p>and then sort (reversed) and get 5 items.</p>
<pre><code>sample_df.sort_values('similarity', ascending=False)[:5]
</code></pre>
<p>If you will have elements with the same similarity then it will gives them in original order.</p>
<hr />
<p>Minimal working code with some data - so everyone can copy and test it.</p>
<p>Because I have only 5 elements in <code>DataFrame</code> so I search 2 elements.</p>
<pre><code>from sklearn.metrics.pairwise import linear_kernel
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.metrics.pairwise import cosine_similarity  

import pandas as pd

sample_df = pd.DataFrame({
    'paper_id': [1, 2, 3, 4, 5],
    'title': ['Covid19', 'Flu', 'Cancer', 'Covid19 Again', 'New Air Conditioners'],
    'abstract': ['covid19', 'flu', 'cancer', 'covid19', 'air conditioner'],
    'body_text': ['Hello covid19', 'Hello flu', 'Hello cancer', 'Hello covid19 again', 'Buy new air conditioner'],
})

def get_cleaned_text(df, row):
    return row

txt_cleaned = get_cleaned_text(sample_df, sample_df['abstract'])
question = ['Can covid19 transmit through air']

tfidf_vector = TfidfVectorizer()

tfidf = tfidf_vector.fit_transform(txt_cleaned)

tfidf_question = tfidf_vector.transform(question)
cosine_similarities = linear_kernel(tfidf_question,tfidf).flatten()

sample_df['similarity'] = cosine_similarities

number = 2
#related_docs_indices = cosine_similarities.argsort()[:-(number+1):-1]
related_docs_indices = cosine_similarities.argsort()[::-1][:number]

print('index:', related_docs_indices)

print('similarity:', cosine_similarities[related_docs_indices])

print('\n--- related_docs_indices ---\n')

print(sample_df.iloc[related_docs_indices])

print('\n--- sort_values ---\n')

print( sample_df.sort_values('similarity', ascending=False)[:number] )
</code></pre>
<p>Result:</p>
<pre><code>index: [3 0]
similarity: [0.62791376 0.62791376]

--- related_docs_indices ---

   paper_id          title abstract            body_text  similarity
3         4  Covid19 Again  covid19  Hello covid19 again    0.627914
0         1        Covid19  covid19        Hello covid19    0.627914

--- sort_values ---

   paper_id          title abstract            body_text  similarity
0         1        Covid19  covid19        Hello covid19    0.627914
3         4  Covid19 Again  covid19  Hello covid19 again    0.627914
</code></pre>
",""
"63431248","2020-08-15 22:09:00","0","","63430951","<p>You are looking for a token at index 0-1 which evaluated to -1, which is the last token.</p>
<p>I recommend using the <code>Token.nbor</code> method to look for the first token before the span, and if no previous token exists make it None or an empty string.</p>
<pre><code>import spacy
from spacy.matcher import Matcher 

# Loading language model
nlp = spacy.load(&quot;en_core_web_md&quot;)

# Initialising with shared vocab
matcher = Matcher(nlp.vocab)

# Adding statistical predictions
matcher.add(&quot;DOG&quot;, None, [{&quot;LOWER&quot;: &quot;white&quot;}, {&quot;LOWER&quot;: &quot;shepherd&quot;}])  # searching for white shepherd
doc = nlp(&quot;white shepherd dog&quot;)

for match_id, start, end in matcher(doc):
    span = doc[start:end]
    print(&quot;Matched span: &quot;, span.text)
    try:
        nbor_tok = span[0].nbor(-1)
        print(&quot;Previous token:&quot;, nbor_tok, nbor_tok.pos_)
    except IndexError:
        nbor_tok = ''
        print(&quot;Previous token: None None&quot;)
</code></pre>
",""
"63308266","2020-08-07 19:41:59","0","","63155722","<p>I used the NLTK webtext corpus to get a count of dependency tags where part-of-speech is a verb.</p>
<pre><code>from collections import Counter
from nltk.corpus import webtext
import spacy
nlp = spacy.load('en_core_web_lg')
nlp.max_length = 10**50

doc = nlp(webtext.raw())
print(Counter([tok.dep_ for tok in doc if tok.pos_=='VERB']))
</code></pre>
<p>Output:</p>
<pre><code>Counter({'ROOT': 18067, 'aux': 4649, 'advcl': 4159, 'xcomp': 3102, 'ccomp': 3094, 'conj': 2568, 'acl': 1395, 'relcl': 1311, 'amod': 1073, 'pcomp': 1059, 'parataxis': 594, 'compound': 519, 'csubj': 458, 'nsubj': 248, 'dobj': 237, 'dep': 187, 'pobj': 174, 'intj': 157, 'auxpass': 148, 'nmod': 131, 'appos': 119, 'acomp': 119, 'prep': 63, 'attr': 46, 'npadvmod': 40, 'nsubjpass': 24, 'advmod': 21, 'oprd': 17, 'punct': 14, 'poss': 8, 'csubjpass': 6, 'nummod': 4, 'cc': 3, 'preconj': 2, 'mark': 1, 'meta': 1})
</code></pre>
",""
"63304852","2020-08-07 15:29:57","0","","63302527","<p>Update stanford-corenlp-4.0.0 to 4.1.0 (which is latest at the time of posting) and it worked.</p>
",""
"63243126","2020-08-04 08:40:01","0","","63239929","<p>I wonder if you are thinking of a parse tree instead of a dependency tree...</p>
<p>I've always been confused by dependency trees, to be honest. They are good at identifying <em>relative</em> connections between structures but I don't think they are that good at determining <em>absolute</em> semantic structures, for example. Phrase structure rules are quite good at determining the <em>absolute</em> parts-of-speech of specifically nouns, verbs, and their constituents; although still imperfectly. While a dependency parser can be used to detect noun chunks, and prepositional phrases, and infer verb phrases, I don't think that's its main function. That <strong>is</strong> the main function of a parse tree though.</p>
<p>To return to your question:</p>
<p>The way you're talking about &quot;father&quot; being the subject sounds like you're trying to understand the deep syntactic structure (absolute) but using a relative model (dependency parser).</p>
<p>In essence, I believe having the phrase', as &quot;a man with different attributes&quot;, ' is adding layers to the dependency tree. These layers separate the actual subject &quot;his father&quot; from the verb phrase &quot;was a good man&quot;. I'd imagine it's adding a layer for the commas, another layer for the quotes, another layer for the as-clause. Until eventually, the relative relationship that the dependency parser is supposed to be determining gets &quot;too far&quot;.</p>
<p>The syntactic analysis can only be as good as the models that generate them. In fact, You'll see that SpaCy has 2 POS indicators that both attempt to perform a syntactic analysis. One is generated by the dependency parser (available under token.dep_) and the other is generated by a statistical model (available under token.pos_). You'll also see that these POS indicators do not always match due to the imprecise nature of the models that predict them.</p>
<p>Out of interest, I believe <a href=""https://www.nltk.org/api/nltk.parse.html"" rel=""nofollow noreferrer"">NLTK</a> has a more traditional phrase-structure-rules-based parse tree available; although even these have limitations. If you want <em>deep</em>, hard-core syntactic analyses of real-life sentences, you may want to try something like <a href=""https://en.wikipedia.org/wiki/Head-driven_phrase_structure_grammar"" rel=""nofollow noreferrer"">Head-driven phrase structure grammar (HPSG)</a> but you'll see that things start to get just a <em>little</em> bit technical. :)</p>
",""
"63211626","2020-08-02 01:50:33","0","","63211463","<p>Try explicitly moving your model to the GPU.</p>
<pre><code>device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = T5ForConditionalGeneration.from_pretrained('t5-base').to(device)
</code></pre>
",""
"63144919","2020-07-29 01:12:30","8","","63135603","<p>You should only start the server once. It'd be easiest to load the file in Python, extract each paragraph, and submit the paragraphs. You should pass each paragraph from your IMDB to the <code>annotate()</code> method. The server will handle sentence splitting.</p>
",""
"63106509","2020-07-26 22:43:56","0","","63080744","<p>I think the answer lies within your question itself. &quot;managed&quot; and &quot;went&quot; are two elements connected by a coordinating conjunction, that's what we see in spacy's output as well:</p>
<pre><code>text = 'The Titanic managed to sail into the coast  intact, and Conan went to Chicago.'

spacy_doc = nlp(text)
[(token.text, token.dep_) for token in spacy_doc]
</code></pre>
<p>Output:</p>
<pre><code>[('The', 'det'),
 ('Titanic', 'nsubj'),
 ('managed', 'ROOT'),
 ('to', 'aux'),
 ('sail', 'xcomp'),
 ('into', 'prep'),
 ('the', 'det'),
 ('coast', 'pobj'),
 (' ', ''),
 ('intact', 'advmod'),
 (',', 'punct'),
 ('and', 'cc'),
 ('Conan', 'nsubj'),
 ('went', 'conj'),
 ('to', 'prep'),
 ('Chicago', 'pobj'),
 ('.', 'punct')]
</code></pre>
",""
"62948817","2020-07-17 07:04:32","2","","62948595","<p>Tokenization and Lematization are the basic building blocks in NLP. Using Tokenization you break the string into tokens/words. Tokenization depends on the language of the text, how the text is formed etc. For example tokenizing a chinese text is different from that of english and is different from a tweet. So there exist different kinds of Tokenizers.</p>
<p>CountVectorizer and Tfidfvectorizer are used to vectorize a block of text which rely on the words with in the text.  So they need a mechanism to tokenize the words and they support the mechanism to send in our tokenizers (via a callable methods passed as argument). If we dont pass in any tokenizer it uses naive way of splitting over spaces.</p>
<p>See the docs of <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"" rel=""nofollow noreferrer"">CountVectorizer</a></p>
<blockquote>
<p><strong>tokenizer: callable, default=None</strong></p>
<p>Override the string tokenization
step while preserving the preprocessing and n-grams generation steps.
Only applies if analyzer == 'word'.</p>
</blockquote>
<p>So they allow us to pass in our own tokenizers. Same applies for Leamatization.</p>
",""
"62909152","2020-07-15 06:55:10","0","","62895997","<p><em>Attribute</em> is a label of dependency relation that seems to be obsolete now. See page 23 of this manual: <a href=""https://nlp.stanford.edu/software/dependencies_manual.pdf"" rel=""noreferrer"">https://nlp.stanford.edu/software/dependencies_manual.pdf</a></p>
<p><a href=""https://i.sstatic.net/yeGZa.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/yeGZa.png"" alt=""Stanford manual's take on ATTR"" /></a></p>
<p>This dependency relation seems to be used to link the copula verb with its NP argument in attributive constructs, for example Daniel is -&gt; professor, she is -&gt; smart.
Consult page 27 of <a href=""http://www.mathcs.emory.edu/%7Echoi/doc/cu-2012-choi.pdf"" rel=""noreferrer"">http://www.mathcs.emory.edu/~choi/doc/cu-2012-choi.pdf</a></p>
<p><a href=""https://i.sstatic.net/zNHWL.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/zNHWL.png"" alt=""CLEAR-style manual's explanation of ATTR"" /></a></p>
<p>Interestingly, the current annotation guidelines for Universal Dependencies have attributive constructs annotated quite differently: there is a <em>cop</em> label involved and, surprisingly, the attributive NP/AdjP is linked directly to its ‚Äúsubject‚Äù.</p>
<p><a href=""https://universaldependencies.org/v2/copula.html"" rel=""noreferrer"">https://universaldependencies.org/v2/copula.html</a></p>
<p><a href=""https://i.sstatic.net/6CzDK.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/6CzDK.png"" alt=""UD 2.0's definition of copula"" /></a></p>
<p>So, these labels are likely to be changed in the future I believe.</p>
",""
"62745002","2020-07-05 18:48:07","0","","62735030","<p>The error message is accurate : you can iterate through a dataframe column like a standard python iterator.
For applying a standard function such as sum,mean etc., we need to use the withColumn() or select() function. In your case you have your own custom function. So you need to register your function as a udf and use it along with withColumn() or select()</p>
<p>Below is the example of a udf from spark documents - <a href=""https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html?highlight=udf#pyspark.sql.functions.udf"" rel=""nofollow noreferrer"">https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html?highlight=udf#pyspark.sql.functions.udf</a></p>
<pre><code>&gt;&gt;&gt; from pyspark.sql.types import IntegerType
&gt;&gt;&gt; slen = udf(lambda s: len(s), IntegerType())
&gt;&gt;&gt; :udf
... def to_upper(s):
...     if s is not None:
...         return s.upper()
...
&gt;&gt;&gt; :udf(returnType=IntegerType())
... def add_one(x):
...     if x is not None:
...         return x + 1
...
&gt;&gt;&gt; df = spark.createDataFrame([(1, &quot;John Doe&quot;, 21)], (&quot;id&quot;, &quot;name&quot;, &quot;age&quot;))
&gt;&gt;&gt; df.select(slen(&quot;name&quot;).alias(&quot;slen(name)&quot;), to_upper(&quot;name&quot;), add_one(&quot;age&quot;)).show()
+----------+--------------+------------+
|slen(name)|to_upper(name)|add_one(age)|
+----------+--------------+------------+
|         8|      JOHN DOE|          22|
+----------+--------------+------------+
</code></pre>
",""
"62719798","2020-07-03 17:02:05","1","","62719639","<p>As you are using a database, I suppose you are using SQL to look in.
If so, this request returns you every word with 9 letters in alphabetical order:</p>
<pre><code>SELECT word
FROM dictionary
WHERE LENGTH(word) = 9
ORDER BY word ASC;
</code></pre>
<p>Assuming <code>dictionary</code> is the name of the table and <code>word</code> is the name of the column.</p>
",""
"62714641","2020-07-03 11:39:01","0","","62712963","<p>You need to run it on the text, not tokens.</p>
<pre><code>df[&quot;lemmatized&quot;] = df['tweet'].apply(lambda x: &quot; &quot;.join([y.lemma_ for y in en_core(x)]))
</code></pre>
<p>Here, <code>x</code> will be a sentence/text in the <code>tweet</code> column, <code>en_core(x)</code> will create a document out of it, and <code>y</code> will represent each token, with <code>y.lemma_</code> yielding the word lemma. <code>&quot; &quot;.join(...)</code> will concat all the lemms found into a single space-separated string.</p>
",""
"62662027","2020-06-30 16:30:00","0","","62601716","<p>To the first part of your question, it's pretty easy to use <code>token.dep_</code> to identify <code>nsubj</code>, <code>ROOT</code>, and <code>dobj</code> tags.</p>
<pre class=""lang-py prettyprint-override""><code>doc = nlp(&quot;She eats carrots&quot;)

for t in doc:
  if t.dep_ == &quot;nsubj&quot;:
    print(f&quot;The agent is {t.text}&quot;)
  elif t.dep_ == &quot;dobj&quot;:
    print(f&quot;The patient is {t.text}&quot;)
</code></pre>
<p>In passive constructions, the patient's dep is <code>nsubjpass</code>, but there may or may not be an agent - that's the point of passive voice.</p>
<p>To get the words at the same level of the dependency parse, <code>token.lefts</code>, <code>token.children</code> and <code>token.rights</code> are your friends. However, this won't catch things like &quot;He is nuts!&quot;, since <code>nuts</code> isn't a direct object, but an attribute. If you also want to catch that, you'll want to look for <code>attr</code> tags.</p>
<p>For the cause and effect stuff, before you decide on rules vs model, and what library... just gather some data. Get 500 sentences, and annotate them with the cause and effect. Then look at your data. See if you can pull it out with rules. There's a middle ground: you can identify candidate sentences with rules (high recall, low precision), then use a model to actually extract the relationships. But you can't do it from first principles. Doing data science requires being familiar with your data.</p>
",""
"62610586","2020-06-27 13:23:20","2","","62608470","<p>You are recompiling your pattern for every sentence. This takes a fair amout of time.
Instead you can compile your pattern globally once:</p>
<pre><code>sbsx = { &quot; &quot;+ng+&quot; &quot; : &quot; &lt;bop&gt; &quot;+ng+&quot; &lt;eop&gt; &quot; for ng  in sub }
pattern = re.compile(&quot;|&quot;.join([re.escape(k) for k in sorted(sbsx,key=len,reverse=True)]), flags=re.DOTALL)

def multiple_replace(string):
     print(&quot;replaced = &quot;)
     return pattern.sub(lambda x: sbsx[x.group(0)], string)
</code></pre>
<p>I tested this with using your sample sentence 1 Million times and I went from 52 seconds to only 13 seconds.</p>
<p>I hope I did not miss anything and this will help speed up your code.</p>
",""
"62593372","2020-06-26 11:02:36","0","","62591705","<p>In following code, I created random stopwords (containing also some nans) and random words. Then Using functions that I created, I checked how long it take to run when stopwords is pandas Series converted to numpy as you do it in your code. Also I checked how long it takes when stopwords is converted to simple python set of similar length.</p>
<p>As you can see from output it was almost <strong>2000 times faster, when using set</strong>.</p>
<p>Therefore I recommend you not to use <code>not in stopwords['First'].dropna().values</code>, but convert <code>stopwords['First'].dropna().values</code> to set (using <code>stopwords_first = set(stopwords['First'].dropna().values)</code>), then you just do <code>not in stopwords_first</code>. Note that the conversion to set must be done outside the function (so that you don't convert stopwords each time you check another ngram)</p>
<p>This is just an example, and your speed will be highly depending on your data, but I believe it can help.</p>
<pre><code>#first generate random 450 stopwords + 50 nans
&gt;&gt;&gt; stopwords = np.array(['word_num'+str(i) for i in range(450)]+[np.nan for _ in range(50)])

#shuffle the stopwords and print some of them
&gt;&gt;&gt; stopwords = pd.Series(stopwords).sample(frac=1)
&gt;&gt;&gt; stopwords
304    word_num304
84      word_num84
215    word_num215
438    word_num438
276    word_num276
          ...     
217    word_num217
280    word_num280
69      word_num69
365    word_num365
404    word_num404
Length: 500, dtype: object

#generate random words to be checked if they are in stopwords
&gt;&gt;&gt; ngrams = ['word_num{}'.format(int(np.random.rand()*1000)) for _ in range(20000)]
&gt;&gt;&gt; ngrams = np.array(ngrams)
&gt;&gt;&gt; ngrams
array(['word_num642', 'word_num729', 'word_num901', ..., 'word_num940',
       'word_num616', 'word_num58'], dtype='&lt;U11')

#define function that checks words presence in stopwords pd.Series (same way as you did)
#this function returns also time it took to run
&gt;&gt;&gt; def check_ngrams_Series(ngrams,stopwords):
...     func = lambda ng: ng in stopwords.dropna().values
...     time_begin = time()
...     result = list(map(func,ngrams))
...     time_end = time()
...     return np.array(result), time_end-time_begin

#define function that checks words presence in stopwords converted to set
#this function returns also time it took to run
&gt;&gt;&gt; def check_ngrams_set(ngrams,stopwords):
...     func = lambda ng: ng in stopwords
...     time_begin = time()
...     result = list(map(func,ngrams))
...     time_end = time()
...     return np.array(result), time_end-time_begin

#try to run both functions
&gt;&gt;&gt; series_out = check_ngrams_Series(ngrams,stopwords)
&gt;&gt;&gt; sets_out = check_ngrams_set(ngrams,set(stopwords))

#checks their first output (words presence in stopwords) is same
&gt;&gt;&gt; np.all(sets_out[0] == series_out[0])
True

#show how long it took to function that uses set to run
&gt;&gt;&gt; sets_out[1]
0.008014917373657227

#show how long it took to function that uses pd.Series to run
&gt;&gt;&gt; series_out[1]
15.30849814414978
</code></pre>
",""
"62584909","2020-06-25 21:54:24","7","","62581363","<p>Yeah, when you print a token it looks like a string. It‚Äôs not. It‚Äôs an object with tons of metadata, including <code>token.i</code> which is the index you are looking for.</p>
<p>If you‚Äôre just getting started with spaCy, the best use of your time is <a href=""https://course.spacy.io/en/"" rel=""nofollow noreferrer"">the course</a>, it‚Äôs quick and practical.</p>
",""
"62498197","2020-06-21 11:57:51","4","","62497616","<p>The two most likely represent very different things, so I would personally separate them by performing a multiple assignment:</p>
<pre><code>import numpy as np

x = (np.array([[1., 1., 1., 1.]]), 
     np.array([[1605, 1606, 1698, 1607]], dtype=np.int64))

a, b = x
</code></pre>
<p>Then you can access the first element of the second part with normal indexing:</p>
<pre><code>b[0][0]
</code></pre>
<pre><code>Out[9]: 1605
</code></pre>
",""
"62496722","2020-06-21 09:30:12","0","","62496544","<p>I suppose what you need is more something like the most important words (or better <em>tokens</em>) associated with one class. Because usually all tokens will be &quot;associated&quot; with all classes one way or the other. So I will answer your question with the following approach:</p>
<p>Let's assume your tokens (or words) generated by the <code>TfidfVectorizer</code> are stored in <code>X_train</code> with labels in <code>y_train</code>, and you trained a model like:</p>
<pre><code>from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer()
X_train = vectorizer.fit_transform(corpus)

clf = LogisticRegression()
clf.fit(X_train, y_train)
</code></pre>
<p>The <code>coef_</code> attribute of <code>LogisticRegression</code> is of shape <em>(n_classes, n_features)</em> for multiclass problems and contains the coefficients calculated for each token and each class. This means, by indexing it according to the classes one can access the coefficients used for this particular class, e.g. <code>coef_[0]</code> for class <code>0</code>, <code>coef_[1]</code> for class <code>1</code>, and so forth.</p>
<p>Just reassociate the token names with the coefficients and sort them according to their value. Then you will get the most important tokens for each class. An example to get the most important tokens for class <code>0</code>:</p>
<pre><code>import pandas as pd

important_tokens = pd.DataFrame(
    data=clf.coef_[0],
    index=vectorizer.get_feature_names(),
    columns=['coefficient']
).sort_values(ascending=False)
</code></pre>
<p>The tokens in <code>important_tokens</code> are now sorted according to their importance for class <code>0</code> and can be easily extracted via the index values. For example, to get the <em>n</em> most important features as a list: <code>important_tokens.head(n).index.values</code>.</p>
<p>If you want the most important tokens for other classes, just replace the index of the <code>coef_</code> attribute as needed.</p>
",""
"62336209","2020-06-12 01:43:17","0","","62291441","<p>To get the basic form of each word, you can use "".lemma_"" property of chunk or token property</p>

<p>I use Spacy version 2.x</p>

<pre><code>import spacy
nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])
doc = nlp('did displaying words')
print ("" "".join([token.lemma_ for token in doc]))
</code></pre>

<p>and the output :</p>

<pre><code>do display word
</code></pre>

<p>Hope it helps :)</p>
",""
"62328961","2020-06-11 16:25:01","3","","62328151","<p>How about extracting the <code>coef_</code> of <code>MultinomialNB</code>:</p>

<pre><code>import pandas as pd


multinb = Pipeline([('vect', CountVectorizer()),
           ('tfidf', TfidfTransformer()),
           ('clf', MultinomialNB()),
          ])

multinb.fit(X_train, y_train)

token_imp = pd.DataFrame(
    data=multinb['clf'].coef_[0],
    index=multinb['vect'].get_feature_names(),
    columns=['coefficient']
).sort_values(by='coefficient', ascending=False)

print(token_imp)
</code></pre>

<p>This will give you something like feature importances in descending order. Since <code>token_imp</code> is a dataframe, you can also just view the <em>n</em> most important features by using <code>token_imp.head(n)</code> and visualize them with <code>token_imp.plot.bar()</code></p>
",""
"62313010","2020-06-10 20:57:45","0","","62268302","<p>You downloaded English model. In order to use Spanish model, you have to download it <code>python -m spacy download es_core_news_sm</code></p>
",""
"62274548","2020-06-09 03:30:52","1","","62272958","<p>Each <code>chunk.root</code> is a <a href=""https://spacy.io/api/token"" rel=""nofollow noreferrer"">Token</a> where you can get different attributes including <code>lemma_</code> and <code>pos_</code> (or <code>tag_</code> if you prefer the PennTreekbak POS tags).</p>

<pre><code>import spacy
S='This is an example sentence that should include several parts and also make ' \
  'clear that studying Natural language Processing is not difficult'
nlp = spacy.load('en_core_web_sm')
doc = nlp(S)
for chunk in doc.noun_chunks:
    print('%-12s %-6s  %s' % (chunk.root.text, chunk.root.pos_, chunk.root.lemma_))

sentence     NOUN    sentence
parts        NOUN    part
Processing   NOUN    processing
</code></pre>

<p>BTW... In this sentence ""processing"" is a noun so the lemma of it is ""processing"", not ""process"" which is the lemma of the verb ""processing"".</p>
",""
"62259450","2020-06-08 09:54:57","2","","62259100","<p>Inside the <code>for</code> loop, an index that is out of range for the list of tokens is created as a consequence of the <code>tokens[i + 1]</code> operation. You could do something like this instead:</p>

<pre><code>import spacy

nlp = spacy.load(""en_core_web_sm"")

listToStr = 'degeneration agents alpha alternative amd analysis angiogenesis anti anti vegf appears associated based best bevacizumab blindness blood'

simi = []
tokens = nlp(listToStr) 

for idx, tok in enumerate(tokens):
    sim = []
    for nextok in tokens[idx:]:
        sim.append(tok.similarity(nextok))
    simi.append(sim)
</code></pre>

<p>This test the similarity of each word with the next words in the sentence, so the result is a list of lists.</p>
",""
"62250362","2020-06-07 19:10:24","1","","62248455","<p>After few comments I finally understood the problem.
It is the way you import the WordNetLemmatizer, import it as follows:
<code>
import nltk
wn = nltk.WordNetLemmatizer()
</code>
then you can use it as you have already done, i.e.:
<code>wn.lemmatize(""hello"")</code></p>
",""
"62160956","2020-06-02 20:34:08","3","","62153385","<p>The problem is that the simple training example script isn't projectivitizing the training instances when initializing and training the model. The parsing algorithm itself can only handle projective parses, but if the parser component finds projectivized labels in its output, they're deprojectivitzed in a postprocessing step. You don't need to modify any parser settings (so starting with a German model makes no difference), just provide projectivized input in the right format.</p>

<p>The initial projectivization is handled automatically by the train CLI, which uses <code>GoldCorpus.train_docs()</code> to prepare the training examples for <code>nlp.update()</code> and sets <code>make_projective=True</code> when creating the <code>GoldParse</code>s.  In general, I'd recommend switching to the train CLI (which also requires switching to the internal JSON training format, which is admittedly a minor hassle), because the train CLI sets a lot of better defaults.</p>

<p>However, a toy example also works fine as long as you create projectivized training examples (with <code>GoldParse(make_projective=True</code>), add all the projectivized dependency labels to the parser, and train with <code>Doc</code> and the projectivized <code>GoldParse</code> input instead of the text/annotation input:</p>

<pre><code># tested with spaCy v2.2.4
import spacy
from spacy.util import minibatch, compounding
from spacy.gold import GoldParse

TRAIN_DATA = [
    (
        'ROOT AAAA BBBB 12 21',
        {
            'heads': [0, 0, 0, 1, 2],
            'deps': ['ROOT', 'LETTERS', 'LETTERS', 'NUMBERS', 'NUMBERS']
        }
    )
]

samples = 200

def test_model(nlp):
    texts = [""ROOT AAAA BBBB 12 21""]
    for doc in nlp.pipe(texts):
        print(doc.text)
        print([(t.text, t.dep_, t.head.text) for t in doc if t.dep_ != ""-""])
        spacy.displacy.serve(doc)

@plac.annotations(
    n_iter=(""Number of training iterations"", ""option"", ""n"", int),
)

def main(n_iter=10):
    """"""Load the model, set up the pipeline and train the parser.""""""
    nlp = spacy.blank(""xx"")
    parser = nlp.create_pipe(""parser"")
    nlp.add_pipe(parser)

    docs_golds = []
    for text, annotation in TRAIN_DATA:
        doc = nlp.make_doc(text)
        gold = GoldParse(doc, **annotation, make_projective=True)
        # add the projectivized labels
        for dep in gold.labels:
            parser.add_label(dep)
        docs_golds.append((doc, gold))
    # duplicate the training instances
    docs_golds = docs_golds * samples

    pipe_exceptions = [""parser"", ""trf_wordpiecer"", ""trf_tok2vec""]
    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]
    with nlp.disable_pipes(*other_pipes):  # only train parser
        optimizer = nlp.begin_training(min_action_freq=1)
        for itn in range(n_iter):
            random.shuffle(docs_golds)
            losses = {}
            # batch up the examples using spaCy's minibatch
            batches = minibatch(docs_golds, size=compounding(4.0, 32.0, 1.001))
            for batch in batches:
                docs, golds = zip(*batch)
                nlp.update(docs, golds, sgd=optimizer, losses=losses)
            print(""Losses"", losses)

    # test the trained model
    test_model(nlp)

if __name__ == ""__main__"":
    plac.call(main)
</code></pre>

<p><a href=""https://i.sstatic.net/Sn0RX.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Sn0RX.png"" alt=""nonprojective parse""></a></p>
",""
"62044151","2020-05-27 13:20:59","0","","62043494","<p>Here is an approach with <code>tidytext</code></p>

<pre><code>library(tidytext)
library(dplyr)
word_count &lt;- tibble(document = seq(1,nrow(data)), text = data) %&gt;%
  unnest_tokens(word, text) %&gt;%
  count(document, word, sort = TRUE)

total_count &lt;- tibble(document = seq(1,nrow(data)), text = data) %&gt;%
  unnest_tokens(word, text) %&gt;%
  group_by(word) %&gt;% 
  summarize(total = n()) 

words &lt;- left_join(word_count,total_count)

words %&gt;%
  bind_tf_idf(word, document, n)
# A tibble: 111 x 7
   document word             n total     tf   idf tf_idf
      &lt;int&gt; &lt;chr&gt;        &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;
 1        1 stormfront      10    11 0.139  1.10  0.153 
 2        1 networking       3     3 0.0417 1.79  0.0747
 3        1 site             3     6 0.0417 0.693 0.0289
 4        1 board            2     2 0.0278 1.79  0.0498
 5        1 forums           2     3 0.0278 1.10  0.0305
 6        1 introduction     2     2 0.0278 1.79  0.0498
 7        1 local            2     2 0.0278 1.79  0.0498
 8        1 main             2     3 0.0278 1.10  0.0305
 9        1 member           2     3 0.0278 1.10  0.0305
10        1 online           2     2 0.0278 1.79  0.0498
# ‚Ä¶ with 101 more rows
</code></pre>

<p>From here it is trivial to filter with <code>dplyr::filter</code>, but since you don't define any specific criteria other than ""only once"", I'll leave that to you. </p>

<p><strong>Data</strong></p>

<pre><code>data &lt;- structure(c(""        , ,    stormfront!  thread       members  post  introduction,     \"".\""     stumbled   white networking site,    reading &amp; decided  register  account,      largest networking site     white brothers,  sisters!    read : : guidelines  posting - stormfront introduction  stormfront - stormfront  main board consists   forums,  -forums   : newslinks &amp; articles - stormfront ideology  philosophy - stormfront activism - stormfront       network   local level: local  regional - stormfront international - stormfront  ,  .  addition   main board   supply  social groups    utilized  networking.  final note:      steps    sustaining member,  core member      site online,   affords  additional online features. sf: shopping cart   stormfront!"", 
""bonjour      warm  brother !   forward  speaking     !"", "" check   time  time   forums.      frequently    moved  columbia   distinctly  numbered.    groups  gatherings         "", 
""  !  site  pretty nice.    amount  news articles.  main concern   moment  islamification."", 
"" , discovered  site   weeks ago.  finally decided  join   found  article  wanted  share  .   proud   race   long time    idea  site    people  shared  views existed."", 
""  white brothers,  names jay      member   years,        bit  info    ?    stormfront meet ups     ? stay strong guys    jay, uk""
), .Dim = c(6L, 1L))
</code></pre>
",""
"62033221","2020-05-26 23:46:48","0","","47725035","<p>In case someone still needs it, I decided to return to this question and illustrate how to use the <a href=""https://github.com/AKuznetsov/russianmorphology"" rel=""nofollow noreferrer"">russianmorphology</a> library I found earlier to do lemmatization for English and Russian languages.</p>
<p>First of all, you will need these <a href=""https://mvnrepository.com/artifact/org.apache.lucene.morphology/"" rel=""nofollow noreferrer"">dependencies</a> (besides the <code>lucene-core</code>):</p>
<pre><code>&lt;!-- if you need Russain --&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.apache.lucene.morphology&lt;/groupId&gt;
    &lt;artifactId&gt;russian&lt;/artifactId&gt;
    &lt;version&gt;1.1&lt;/version&gt;
&lt;/dependency&gt;

&lt;!-- if you need English--&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.apache.lucene.morphology&lt;/groupId&gt;
    &lt;artifactId&gt;english&lt;/artifactId&gt;
    &lt;version&gt;1.1&lt;/version&gt;
&lt;/dependency&gt;

&lt;dependency&gt;
    &lt;groupId&gt;org.apache.lucene.morphology&lt;/groupId&gt;
    &lt;artifactId&gt;morph&lt;/artifactId&gt;
    &lt;version&gt;1.1&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>
<p>Then, make sure you import the right analyzer:</p>
<pre><code>import org.apache.lucene.morphology.english.EnglishAnalyzer;
import org.apache.lucene.morphology.russian.RussianAnalyzer;
</code></pre>
<p>These analyzers, unlike standard lucene analyzers, use <code>MorphologyFilter</code> which converts each word into a set of its normal forms.</p>
<p>So if you use the following code</p>
<pre><code>String text = &quot;The stem is the part of the word that never changes even when morphologically inflected; a lemma is the base form of the word. For example, from \&quot;produced\&quot;, the lemma is \&quot;produce\&quot;, but the stem is \&quot;produc-\&quot;. This is because there are words such as production&quot;;
Analyzer analyzer = new EnglishAnalyzer();
TokenStream stream = analyzer.tokenStream(&quot;field&quot;, text);
stream.reset();
while (stream.incrementToken()) {
    String lemma = stream.getAttribute(CharTermAttribute.class).toString();
    System.out.print(lemma + &quot; &quot;);
}
stream.end();
stream.close();
</code></pre>
<p>it will print</p>
<blockquote>
<p>the stem be the part of the word that never change even when
morphologically inflected inflect a lemma be the base form of the word
for example from produced produce the lemma be produce but the stem be
produc this be because there are be word such as production</p>
</blockquote>
<p>And for the Russian text</p>
<pre><code>String text = &quot;–ü—Ä–æ–¥–æ–ª–∂–∞—é —Ü–∏–∫–ª –ø–æ—Å—Ç–æ–≤ –æ–± –∞—Å—Ç—Ä–æ–ª–æ–≥–∏–∏ –∏ –Ω–∞—É–∫–µ. –ê—Å—Ç—Ä–æ–ª–æ–≥–∏—è –Ω–µ –∏–º–µ–µ—Ç –Ω–∞—É—á–Ω–æ–≥–æ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏—è, –Ω–æ —è–≤–ª—è–µ—Ç—Å—è —á–∞—Å—Ç—å—é –∏—Å—Ç–æ—Ä–∏–∏ –Ω–∞—É–∫–∏, —á–∞—Å—Ç—å—é –∫—É–ª—å—Ç—É—Ä—ã –∏ –æ–±—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Å–æ–∑–Ω–∞–Ω–∏—è. –ü–æ—ç—Ç–æ–º—É –∞—Å—Ç—Ä–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –≤–∑–≥–ª—è–¥ –Ω–∞ –Ω–∞—É–∫—É –≤–µ—Å—å–º–∞ –∏–Ω—Ç–µ—Ä–µ—Å–µ–Ω.&quot;;
</code></pre>
<p>the <code>RussianAnalyzer</code> will print the following:</p>
<blockquote>
<p>–ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å —Ü–∏–∫–ª –ø–æ—Å—Ç –æ–± –∞—Å—Ç—Ä–æ–ª–æ–≥–∏—è –∏ –Ω–∞—É–∫–∞ –∞—Å—Ç—Ä–æ–ª–æ–≥–∏—è –Ω–µ –∏–º–µ—Ç—å –Ω–∞—É—á–Ω—ã–π
–æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ –Ω–æ —è–≤–ª—è—Ç—å—Å—è —á–∞—Å—Ç—å —á–∞—Å—Ç—å—é –∏—Å—Ç–æ—Ä–∏—è –Ω–∞—É–∫–∞ —á–∞—Å—Ç—å —á–∞—Å—Ç—å—é
–∫—É–ª—å—Ç—É—Ä–∞ –∏ –æ–±—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —Å–æ–∑–Ω–∞–Ω–∏–µ –ø–æ—ç—Ç–æ–º—É –∞—Å—Ç—Ä–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –≤–∑–≥–ª—è–¥ –Ω–∞
–Ω–∞—É–∫–∞ –≤–µ—Å—å–º–∞ –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π</p>
</blockquote>
<p>Yo may notice that some words have more that one base form, e.g. <code>inflected</code> is converted to <code>[inflected, inflect]</code>. If you don't like this behaviour, you would have to change the implementation of the <code>org.apache.lucene.morphology.analyzer.MorhpologyFilter</code> (if you are interested in how exactly to do it, let me know and I'll elaborate on this).</p>
<p>Hope it helps, good luck!</p>
",""
"62018797","2020-05-26 09:25:04","5","","62012899","<p>The most important thing for optimisation is to understand, what exactly is performing poorly.
Then you can see what can be optimized.</p>

<p>If for example reading and writing takes 99% of the time it's not really worth to optimize the processing of your data.
Even if you could speed up the processing by 10 you would just gain 0.9% if reading writing were consuming 99%</p>

<p>I suggest to measure and compare some versions and to post differences in performance.
This might lead potential further suggestions to optimise.</p>

<p><strong>In all below examples I replaced <code>writelines</code> with <code>write</code></strong> as writelines is probably decomposing your line character by character prior to writing.</p>

<p>In any case. You want to use <strong><code>write</code></strong>
You should already gain a speedup of about 5.</p>

<p>1.) Just reading and writing</p>

<pre><code>with open(corpus_in,""rt"") as corpus_input, open(corpus_out,""wt"")
 as corpus_out:
   for line in corpus_input:
       corpus_out.write(line)
</code></pre>

<p>2.) Just reading and writing
 with a bigger buffer</p>

<pre><code>import io

BUF_SIZE = 50 * io.DEFAULT_BUFFER_SIZE # try other buffer sizes if you see an impact
with open(corpus_in,""rt"", BUF_SIZE) as corpus_input, open(corpus_out,""wt"", BUF_SIZE)
 as corpus_out:
   for line in corpus_input:
corpus_out.write(line)
</code></pre>

<p>For me this increases performance by a few percent</p>

<p>3.) move search regexp and replacement generation out of loop.</p>

<pre><code>   rules = []
   for word in dict_keys:
       rules.append((re.compile(fr'\b{word}\b'), word + ""_lorem_ipsum""))

   for line in corpus_input:
       for regexp, new_word in rules: 
           line = regexp.sub(new_word, line)
       corpus_out.write(line)
</code></pre>

<p>On my machine with my frequency of lines containing words this solution is in fact slower then the one having the line <code>if word in line</code></p>

<p>So perhaps try:
3.a) move search regexp and replacement generation out of loop.</p>

<pre><code>   rules = []
   for word in dict_keys:
       rules.append((word, re.compile(fr'\b{word}\b'), word + ""_lorem_ipsum""))

   for line in corpus_input:
       for word, regexp, new_word in rules: 
           if word in line:
               line = regexp.sub(new_word, line)
       corpus_out.write(line)
</code></pre>

<p>3.b) If all replacement strings are longer than the initial strings, then this would be a little faster.</p>

<pre><code>   rules = []
   for word in dict_keys:
       rules.append((word, re.compile(fr'\b{word}\b'), word + ""_lorem_ipsum""))

   for line in corpus_input:
       temp_line = line
       for word, regexp, new_word in rules: 
           if word in line:
               temp_line = regexp.sub(new_word, temp_line)
       corpus_out.write(temp_line)
</code></pre>

<p>4.) if you really replace always with <code>word + ""_lorem_ipsum""</code> combine the regular expression into one.</p>

<pre><code>   regexp = re.compile(fr'\b({""|"".join(dict_keys)})\b')

   for line in corpus_input:
       line = regexp.sub(""\1_lorem_ipsum"", line)
       corpus_out.write(line)
</code></pre>

<p>4.a) depending on the word distribution this might be faster:</p>

<pre><code>   regexp = re.compile(fr'\b({""|"".join(dict_keys)})\b')

   for line in corpus_input:
       if any(word in line for word in dict_keys):
           line = regexp.sub(""\1_lorem_ipsum"", line)
       corpus_out.write(line)
</code></pre>

<p>Whether this is more efficient or not depends probably on the number of words to search and replace and the frequency of thise words.
I don't have that date.</p>

<p>For 5 words and my distribution slower than 3.a</p>

<p>5) if the words to replace are different you might still try to combine the regexps and use a function to replace</p>

<pre><code>   replace_table = {
      ""word1"": ""word1_laram_apsam"",
      ""word2"": ""word2_lerem_epsem"",
      ""word3"": ""word3_lorom_opsom"",

   }

   def repl(match):
      return replace_table[match.group(1)]

   regexp = re.compile(fr'\b({""|"".join(dict_keys)})\b')

   for line in corpus_input:
       line = regexp.sub(repl, line)
       corpus_out.write(line)
</code></pre>

<p>Slower than 5, whether better than 3.a depends on number of words and wird distribution / frequency.</p>
",""
"61985267","2020-05-24 11:28:24","5","","61982023","<p>You get a <code>KeyError</code> because <code>wordnet</code> is not using the same <code>pos</code> labels. The accepted <code>pos</code> labels for <code>wordnet</code> based on <a href=""https://github.com/nltk/nltk/blob/6404712d0a64c3d6e3700032c23a59803615460c/nltk/corpus/reader/wordnet.py#L1076"" rel=""nofollow noreferrer"">source code</a> are these: <code>adj</code>, <code>adv</code>, <code>adv</code> and <code>verb</code>.</p>

<p><strong>EDIT</strong> based on @bivouac0 's comment:</p>

<p>So to bypass this issue you have to make a mapper. Mapping function is heavily based on this <a href=""https://stackoverflow.com/a/15590384/6779252"">answer</a>. Non-supported POS will not be lemmatized.</p>

<pre class=""lang-py prettyprint-override""><code>import nltk
import pandas as pd
from nltk.corpus import wordnet
from nltk.stem import WordNetLemmatizer 

lemmatizer = WordNetLemmatizer()

def get_wordnet_pos(treebank_tag):
    if treebank_tag.startswith('J'):
        return wordnet.ADJ
    elif treebank_tag.startswith('V'):
        return wordnet.VERB
    elif treebank_tag.startswith('N'):
        return wordnet.NOUN
    elif treebank_tag.startswith('R'):
        return wordnet.ADV
    else:
        return None

x = pd.DataFrame(data=[['this is a sample of text.'], ['one more text.']], 
                 columns=['Phrase'])

x['Phrase'] = x['Phrase'].apply(lambda v: nltk.pos_tag(nltk.word_tokenize(v)))


x['Phrase_lemma'] = x['Phrase'].transform(lambda value: ' '.join([lemmatizer.lemmatize(a[0],pos=get_wordnet_pos(a[1])) if get_wordnet_pos(a[1]) else a[0] for a in  value]))
</code></pre>
",""
"61959640","2020-05-22 16:31:04","0","","61952877","<p>I don't think this is an appropriate question for StackOverflow, which is a site for programming questions. But I'll try to address it as best I can.</p>
<p>I don't believe Chomsky was ever under the impression that natural languages could be described with a Type 2 grammar. It is not impossible for noun-verb agreement (singular/plural) to be represented in a Type 2 grammar, because the number of cases is finite, but the grammar is awkward. But there are more complicated features of natural language, generally involving specific rules about how word order can be rearranged, which cannot be captured in a simple grammar. It was Chomsky's hope that a second level of analysis -- &quot;transformational grammars&quot; -- could useful capture these rearrangement rules without making the grammar computationally intractable. That would require finding some systematization which fit between Type 1 and Type 2, because Type 1 grammars are not computationally tractable.</p>
<p>Since we do, in fact, correctly parse our own languages, it stands to reason that there be some computational algorithm. But that line of reasoning might not actually be correct, because there is a limit to the complexity of a sentence which we can parse. Any finite language is regular (Type 3); only languages which have an unlimited number of potential sentences require more sophisticated grammars. So a large collection of finite patterns could suffice to understand natural language. These patterns might be a lot more sophisticated than regular expressions, but as long as each pattern only applies to a sentence of limited length, the pattern could be expressed mathematically as a regular expression. (The most obvious one is to just list all possible sentences as alternatives, which is a regular expression if the number of possible sentences is finite. But in many cases, that might be simplified into something more useful.)</p>
<p>As I understand it, modern attempts to deal with natural language using so-called &quot;deep learning&quot; are essentially based on pattern recognition through neural networks, although I haven't studied the field deeply and I'm sure that there are many complications I'm skipping over in that simple description.</p>
<p>Noam Chomsky is an American, but &quot;American&quot; is not a language (y si fuera, podr√≠a ser castellano, hablado por la mayor√≠a de los residentes de las Americas). As far as I know, his first language is English, but he is not <a href=""https://chomskyreference.org/post/123114893680/thanks-so-much-for-this-blog-its-a-real-gem-how"" rel=""nofollow noreferrer"">by any means unilingual</a>, although I don't know how much Swiss German he speaks. Certainly, there have been criticisms over the years that his theories have an Indo-European bias. Certainly, I don't claim competence in Swiss German, despite having lived several years in Switzerland, but I did read Shieber's paper and some of the follow-ups and discussed them with colleagues who were native Swiss German speakers. (Opinions were divided.)</p>
<p>The basic issue has to do with morphological agreement in lists. As I mentioned earlier, many languages (all Indo-European languages, as far as I know) insist that the form of the verb agrees with the form of the subject, so that a singular subject requires a singular verb and a plural subject requires a plural verb. [Note 1]</p>
<p>In many languages, agreement is also required between adjectives and nouns, and this is not just agreement in number but also agreement in grammatical gender (if applicable). Also, many languages require agreement between the specific verb and the article or adjective of the object of the verb. [Note 2]</p>
<p>Simple agreement can be handled by a context-free (Type 2) grammar, but there is a huge restriction. To put it simply, a context-free grammar can only deal with parenthetic constructions. This can work even if there is more than one type of parenthesis, so a context-free grammar can insist that an <code>[</code> be matched with a <code>]</code> and not a <code>)</code>. But the grammar must have this &quot;inside-out&quot; form: the matching symbols must be in the reverse order to the symbols being matched.</p>
<p>One consequence of this is that there is a context-free grammar for <em>palindromes</em> -- sentences which read the same in both directions, which effectively means that they consist of a phrase followed by its reverse. But there is no context-free grammar for <em>duplications</em>: a language consisting of repeated phrases. In the palindrome, the matching words are in the reverse order to the matched words; in the duplicate, they are in the same order. Hence the difference.</p>
<p>Agreement in natural languages mostly follows this pattern, and some of the exceptions can be dealt with by positing simple rules for reordering finite numbers of phrases -- Chomsky's transformational grammar. But Swiss German features at least one case where agreement is not parenthetic, but rather in the same order. [Note 3] This involves the feature of German in which many sentences are in the order Subject-Object-Verb, which can be extended to Subject Object Object Object... Verb Verb Verb... when the verbs have indirect objects. Shieber showed some examples in which object-verb agreement is ordered, even when there are intervening phrases.</p>
<p>In the general case, such &quot;cross-serial agreement&quot; cannot be expressed in a context-free grammar. But there is a huge underlying assumption: that the length of the agreeing series be effectively unlimited. If, on the other hand, there are a finite number of patterns actually in common use, the &quot;deep learning&quot; model referred to above would certainly be able to handle it.</p>
<p>(I want to say that I'm not endorsing deep learning here. In fact, the way &quot;artificial intelligence&quot; is &quot;trained&quot; involves the uses of trainers whose cultural biases may well not be sufficiently understood. This could easily lead to the same unfortunate consequences alluded to in my first footnode.)</p>
<h3>Notes</h3>
<ol>
<li><p>This is not the case in many native American languages, as Whorf pointed out. In those languages, using a singular verb with a plural noun implies that the action was taken collectively, while using a plural verb would imply that the action was taken separately. Roughly transcribed to English, &quot;The dogs run&quot; would be about a bunch of dogs independently running in different directions, whereas &quot;The dogs runs&quot; would be about a single pack of dogs all running together. Some European &quot;teachers&quot; who imposed their own linguistic prejudices on native languages failed to correctly understand this distinction, and concluded that the native Americans must be too primitive to even speak their own language &quot;correctly&quot;; to &quot;correct&quot; this &quot;deficiency&quot;, they attempted to eliminate the distinction from the language, in some cases with success.</p>
</li>
<li><p>These rules, not present in English, are one of the reasons some English speakers are tortured by learning German. I speak from personal experience.</p>
</li>
<li><p>Ordered agreement, as opposed to parenthetic agreement, is known as <a href=""https://en.wikipedia.org/wiki/Cross-serial_dependencies"" rel=""nofollow noreferrer"">cross-serial dependency</a>.</p>
</li>
</ol>
",""
"61923324","2020-05-20 21:34:50","1","","61787119","<p>It looks like FastText 0.9.2 has a bug in the computation of recall, and that should be fixed with <a href=""https://github.com/facebookresearch/fastText/commit/b64e359d5485dda4b4b5074494155d18e25c8d13"" rel=""noreferrer"">this commit</a>.</p>

<p>Installing a ""bleeding edge"" version of FastText e.g. with</p>

<pre><code>pip install git+https://github.com/facebookresearch/fastText.git@b64e359d5485dda4b4b5074494155d18e25c8d13 --quiet
</code></pre>

<p>and rerunning your code should allow to get rid of the <code>nan</code> values in the recall computation.</p>
",""
"61908293","2020-05-20 08:17:46","0","","61907589","<p>if you would like to compute the tfidf with TFT (<a href=""https://github.com/tensorflow/transform/blob/7f8bc85d5f3a055b1f711b75168bbf970c678839/examples/sentiment_example.py#L187-L192"" rel=""nofollow noreferrer"">here an example</a>) you can do</p>

<pre><code>example_strings = [""I like pie pie pie"", ""yum yum pie""]
VOCAB_SIZE = 100

tf.compat.v1.disable_eager_execution()

tokens = tf.compat.v1.string_split(example_strings)
indices = tft.compute_and_apply_vocabulary(tokens, top_k=VOCAB_SIZE)
bow_indices, weight = tft.tfidf(indices, VOCAB_SIZE + 1)
</code></pre>

<p>otherwise, you can also use TF <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer"" rel=""nofollow noreferrer"">Tokenizer</a>:</p>

<pre><code>tk = tf.keras.preprocessing.text.Tokenizer(num_words=VOCAB_SIZE)
tk.fit_on_texts(example_strings)

tk.sequences_to_matrix(tk.texts_to_sequences(example_strings), mode='tfidf')
</code></pre>
",""
"61828020","2020-05-15 20:43:27","2","","61827301","<p>Your df has 50 rows and X_train_tfidf 37, pd.concat() returns dataframes with 50 rows, with remaining 13 filled with NaN. </p>

<p>You added all values of your feature to training tf-idf, which is not what you want.</p>

<p>Not to mess up train/val split when adding new column, I'd recommend to do the split on the index of original dataframe</p>

<pre><code>idx_train, idx_test = train_test_split(df.index, random_state=0)
X_train, y_train = df.loc[idx_train, 'text'],  df.loc[idx_train, 'target']
# same for test
</code></pre>

<p>Then you can add your ""id1"" feature:</p>

<pre><code>X_train_all_features = pd.concat([pd.DataFrame(X_train_tfidf.toarray()), df.loc[idx_train, 'id_1']], axis = 1)
</code></pre>

<p>UPDATE
I don't see the reason to convert sparse matrix to pandas dataframe. It will be very slow with big enough dataset. Instead, add you feature to the matrix, so you can use it later in downstream algorithm.</p>

<pre><code>from scipy.sparse import hstack 
X_train_tfidf = hstack([X_train_tfidf, df.loc[idx_train, 'id1'].values.reshape(-1, 1)])
</code></pre>

<p>Check dimensions</p>

<pre><code>X_train_tfidf.shape # should be (37, 501)
</code></pre>
",""
"61771587","2020-05-13 09:58:18","0","","61770575","<p>I've written the following solution using <code>nltk</code>:</p>

<pre class=""lang-py prettyprint-override""><code>import re
from nltk.util import ngrams
from sklearn.feature_extraction.text import TfidfVectorizer

pattern = re.compile(r'[\.-]'). # split on '.' and on '-'

corpus = np.array(['abc.xyz', 'zzz-m.j'])


def analyzer(text):
    text = text.lower()
    tokens = pattern.split(text)    
    return [''.join(ngram) for token in tokens for ngram in ngrams(token, 2)]

tfidf_vectorizer = TfidfVectorizer(analyzer=analyzer)
tfidf_vectorizer.fit_transform(corpus)
print(tfidf_vectorizer.vocabulary_)

# Output -&gt; {'ab': 0, 'bc': 1, 'xy': 2, 'yz': 3, 'zz': 4}

</code></pre>

<p>Not sure if this is the best way to go though.</p>
",""
"61770024","2020-05-13 08:42:13","3","","61757240","<p>This doesn't work:</p>

<pre><code>[i.pos_ for i in nlp(""Great coffee at a place with a great view!"").noun_chunks]
</code></pre>

<p>because <code>noun_chunks</code> returns <code>Span</code> objects, not <code>Token</code> objects. </p>

<p>You can get to the POS tags within each noun chunk by iterating over the tokens:</p>

<pre><code>nlp = spacy.load(""en_core_web_md"")
for i in nlp(""Great coffee at a place with a great view!"").noun_chunks:
    print(i, [t.pos_ for t in i])
</code></pre>

<p>which will give you</p>

<pre><code>Great coffee ['ADJ', 'NOUN'] 
a place ['DET', 'NOUN'] 
a great view ['DET', 'ADJ', 'NOUN']
</code></pre>
",""
"61709975","2020-05-10 09:47:11","0","","61700506","<p><code>\theta</code> is a conventional/standard machine learning notation indicating (strictly speaking) a set of parameter (values), often more commonly known as the parameter vector. </p>

<p>The notation <code>P(Y|X;\theta)</code> is to read as the y-values (e.g. MNIST digit labels) are predicted from the x-values (e.g. input images of MNIST digits) with the help of a trained model that is trained on annotated (X,Y) pairs. This model <strong>is parameterized by</strong> <code>\theta</code>. Obviously, if the training algorithm changes, so will the parameter vector <code>\theta</code>.</p>

<p>The structure of these parameter vectors are usually interpreted from the model they are associated with, e.g. for multi-layered neural networks they indicate real-valued vectors initially randomly assigned and then updated by gradient descent at each iteration.</p>

<p>For word generation based language models, they refer to the probability of a word <code>v</code> following a word <code>u</code>, meaning that each element is an entry in a hash-table of the form <code>(u, v) --&gt; count(u.v)/count(u)</code>.
These probabilities are learned from a <strong>training</strong> collection, <code>C</code> of documents, as a result of which they essentially become a <strong>function of the training set</strong>. For a different collection, these probability values will be different.</p>

<p>Hence, the usual convention is to write <code>P(w_n|P_w_{n-1};\theta)</code>, which basically indicates that these word succession probabilities are <strong>parameterized</strong> by <code>\theta</code>.</p>

<p>A similar argument applies for document-level language models in information retrieval, where the weights essentially indicate probabilities of sampling terms from documents.</p>
",""
"61575357","2020-05-03 13:37:45","1","","61560956","<p>nltk dropped support to Python2, Try to use older versions of nltk in which it supports python 2 and I found out that nltk 3.0 version supports python 2 [edited - Thanks to <a href=""https://stackoverflow.com/users/2357112/user2357112-supports-monica"">user2357112 supports Monica
</a>]</p>
<p>So, Try to download and install previous versions of nltk with the command</p>
<pre><code>pip install nltk==3.0
</code></pre>
<p>You can change the version number which is 3.0 in the above mentioned case and can install suitable version whichever you feels working.</p>
<p>It worked for me.If anyone facing same problem can try above mentioned method.</p>
",""
"61570950","2020-05-03 07:23:22","1","","61570873","<p>From the code you provided nothing seems wrong. However, I hypothesize that somewhere before that block of code, you assigned an object to the variable name <code>list</code> (most likely something along the lines of <code>list = [...]</code>) which is usually the cause of this error. </p>

<p>Try to find that line of code if it exists and rename that variable. Generally it is not a good idea to rename built-in types for this reason. For more info read <a href=""https://stackoverflow.com/questions/31087111/typeerror-list-object-is-not-callable-in-python/31087151"">this</a></p>
",""
"61498528","2020-04-29 09:28:26","3","","61498301","<p>Check that you have the correct version of cupy (from your CUDA version above: <code>cupy-cuda102</code>) installed.</p>
",""
"61492599","2020-04-29 01:13:40","2","","61492130","<p>IIUC, you are using <code>df.apply</code> with your function, but you can do it like this. the idea is not to redo the operation on the rows of <code>lookup</code> each time you find a corresponding word but doing it once and reshape <code>df</code> to be able to perform vectorized manipulation</p>

<p>1: reshape the column words of <code>df</code> with <code>str.split</code>, <code>stack</code> and <code>to_frame</code> to get a new line for each word:</p>

<pre><code>s_df = df['words'].str.split(expand=True).stack().to_frame(name='split_word')
print (s_df.head(8))
    split_word
0 0          i
  1     havent
  2       been
  3       back
1 0        but
  1        its
2 0       they
  1       used
</code></pre>

<p>2: Reshape <code>lookup</code> by <code>set_index</code> the word column, <code>str.strip</code>, <code>str.split</code> and <code>astype</code> to get a dataframe with word as index and each value of class_proba in a column</p>

<pre><code>split_lookup = lookup.set_index('word')['class_proba'].str.strip('][')\
                     .str.split(', ', expand=True).astype(float)
print (split_lookup.head())
          0    1         2      3         4    5    6         7
word                                                           
been    0.0  0.0  0.000000  0.000  0.000000  0.0  0.0  5.278995
havent  0.0  0.0  0.000000  0.000  0.000000  0.0  0.0  5.278995
derive  0.0  0.0  0.000000  0.000  0.000000  0.0  0.0  5.278995
a       0.0  0.0  7.451379  6.552  0.000000  0.0  0.0  0.000000
hello   0.0  0.0  0.000000  0.000  0.000155  0.0  0.0  0.000000
</code></pre>

<p>3: <code>Merge</code> both, <code>drop</code> the unnecessary column and <code>groupby</code> the level=0 being the original index of <code>df</code> and <code>sum</code></p>

<pre><code>df_proba = s_df.merge(split_lookup, how='left',
                      left_on='split_word', right_index=True)\
               .drop('split_word', axis=1)\
               .groupby(level=0).sum()
print (df_proba.head())
          0    1         2         3    4    5    6         7
0  0.000000  0.0  0.000000  0.000000  0.0  0.0  0.0  10.55799
1  0.000000  0.0  0.000000  0.000000  0.0  0.0  0.0   0.00000
2  0.000000  0.0  0.000323  0.000000  0.0  0.0  0.0   0.00000
3  0.000000  0.0  0.000000  0.000000  0.0  0.0  0.0   0.00000
4  0.000193  0.0  7.451379  6.552213  0.0  0.0  0.0   0.00000
</code></pre>

<p>4: finally, convert to a list and reassign to the original df with <code>to_numpy</code> and <code>tolist</code>:</p>

<pre><code>df['class_proba'] = df_proba.to_numpy().tolist()
print (df.head())
                                           words  \
0                          i  havent  been  back   
1                                       but  its   
2              they  used  to  get  more  closer   
3                                        no  way   
4  when  we  have  some  type  of  a  thing  for   

                                         class_proba  
0   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 10.55798974]  
1           [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  
2  [0.0, 0.0, 0.00032289312, 0.0, 0.0, 0.0, 0.0, ...  
3           [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  
4  [0.000193199, 0.0, 7.451379, 6.552212946999999...  
</code></pre>
",""
"61446569","2020-04-26 19:09:52","1","","61446106","<p>They are two distinct procedures:</p>

<ul>
<li><p>POS Tagging: each <em>token</em> gets assigned a label which reflects its word class.</p></li>
<li><p>Parsing: each <em>sentence</em> gets assigned a structure (often a tree) which reflects how its components are related to each other.</p></li>
</ul>

<p>POS Tagging takes a tokenised sequence of words, and returns a list of annotated tokens, where each token has a word class label. This is often disambiguated by looking at the context surrounding the token.</p>

<p>There is also <a href=""https://en.wikipedia.org/wiki/Shallow_parsing"" rel=""nofollow noreferrer""><em>chunking</em></a>, which groups tokens into related groups (such as noun phrases). Chunks are non-overlapping sequences.</p>

<p>Parsing commonly results in a <a href=""https://en.wikipedia.org/wiki/Parse_tree"" rel=""nofollow noreferrer"">parse tree</a> for a sentence; often there can be many possible trees in case of ambiguous sentences.</p>

<p>POS tagging is usually a preparatory step in parsing, as a parser typically operates on word classes (though there are some parsing algorithms that work with tokens directly, or a mixture of tags and tokens).</p>
",""
"61399612","2020-04-24 01:30:39","4","","61399545","<p>I would suggest using lemmatization over stemming, stemming just chops off letters from the end until the root/stem word is reached. Lemmatization also looks at the surrounding text to determine the given words's part of speech. </p>
",""
"61360682","2020-04-22 08:34:30","0","","61349106","<p>Use an approximate nearest neighbor (ANN) search library, such as <a href=""https://github.com/nmslib/nmslib"" rel=""nofollow noreferrer"">nmslib</a>. These libraries would allow you to <em>index</em> dense vectors and retrieve a list of such indexed vectors given a query. Some example ipython notebooks can be found <a href=""https://github.com/nmslib/nmslib/blob/master/python_bindings/notebooks/search_vector_dense_optim.ipynb"" rel=""nofollow noreferrer"">here</a>. </p>
",""
"61245041","2020-04-16 07:41:57","0","","61244581","<p>The lemma indices are in fact hashes, so there is not a continuous row of indices from 0 to the number of vocabulary entries. Even <code>sp.vocab.strings[""randomnonwordstring#""]</code> gives you an integer.</p>

<p>For entry ""base"", the ID is <code>4715552063986449646</code> in <code>sp.vocab</code> (note it is a shared vocab both for forms and lemmas). You would never fit such a number of embeddings in a memory.</p>

<p>The correct solution is creating a dictionary transforming words into indices based on what you have in your training data.</p>
",""
"61229363","2020-04-15 12:58:49","0","","61224496","<p>You will need to refactor the code to use spacy.require_gpu() before initialising your NLP models - for more information refer to the docs: <a href=""https://spacy.io/api/top-level#spacy.require_gpu"" rel=""nofollow noreferrer"">https://spacy.io/api/top-level#spacy.require_gpu</a></p>

<p>Before doing this I would make sure your task is running on all cores. If you are not running on all cores you could use joblib for multiprocessing minibatch partitions of your job:</p>

<pre><code>    partitions = minibatch(texts, size=batch_size)
    executor = Parallel(n_jobs=n_jobs, backend=""multiprocessing"", prefer=""processes"")
    do = delayed(partial(transform_texts, nlp))
    tasks = (do(i, batch, output_dir) for i, batch in enumerate(partitions))
    executor(tasks)
</code></pre>

<p>For more information here's a joblib multiprocessing NER training example from the docs: <a href=""https://spacy.io/usage/examples#multi-processing"" rel=""nofollow noreferrer"">https://spacy.io/usage/examples#multi-processing</a></p>
",""
"61226181","2020-04-15 10:09:04","0","","61157314","<p>Solution is here: <a href=""https://github.com/pytorch/xla/issues/1909"" rel=""nofollow noreferrer"">https://github.com/pytorch/xla/issues/1909</a> </p>

<p>Before calling <code>model.to(dev)</code>, you need to call <code>xm.send_cpu_data_to_device(model, xm.xla_device())</code>:</p>

<pre><code>model = AlbertForMaskedLM.from_pretrained('albert-base-v2')
model = xm.send_cpu_data_to_device(model, dev)
model = model.to(dev)
</code></pre>

<p>There are also some issues with getting the gelu activation function ALBERT uses to work on the TPU, so you need to use the following branch of transformers when working on TPU: <a href=""https://github.com/huggingface/transformers/tree/fix-jit-tpu"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/tree/fix-jit-tpu</a></p>

<p>See the following colab notebook (by <a href=""https://github.com/jysohn23"" rel=""nofollow noreferrer"">https://github.com/jysohn23</a>) for full solution: <a href=""https://colab.research.google.com/gist/jysohn23/68d620cda395eab66289115169f43900/getting-started-with-pytorch-on-cloud-tpus.ipynb"" rel=""nofollow noreferrer"">https://colab.research.google.com/gist/jysohn23/68d620cda395eab66289115169f43900/getting-started-with-pytorch-on-cloud-tpus.ipynb</a></p>
",""
"61120842","2020-04-09 12:10:14","1","","61118213","<p>With a model like <code>en_core_web_lg</code> that includes a tagger and rules for a rule-based lemmatizer, it provides the rule-based lemmas rather than the lookup lemmas when POS tags are available to use with the rules. The lookup lemmas aren't great overall and are only used as a backup if the model/pipeline doesn't have enough information to provide the rule-based lemmas.</p>

<p>With <code>faster</code>, the POS tag is <code>ADV</code>, which is left as-is by the rules. If it had been tagged as <code>ADJ</code>, the lemma would be <code>fast</code> with the current rules.</p>

<p>The lemmatizer tries to provide the best lemmas it can without requiring the user to manage any settings, but it's also not very configurable right now (v2.2). If you want to run the tagger but have lookup lemmas, you'll have to replace the lemmas after running the tagger.</p>
",""
"61035804","2020-04-04 22:52:01","2","","60967134","<p>It's very easy to integrate entities in your extractor. For every pair of children, you should check whether the ""A"" child is the head of some named entity, and if it is true, you use the whole entity as your object. </p>

<p>Here I provide the whole code</p>

<pre><code>!python -m spacy download en_core_web_lg
import nltk
nltk.download('vader_lexicon')

import spacy
nlp = spacy.load(""en_core_web_lg"")

from nltk.sentiment.vader import SentimentIntensityAnalyzer
sid = SentimentIntensityAnalyzer()


def find_sentiment(doc):
    # find roots of all entities in the text
    ner_heads = {ent.root.idx: ent for ent in doc.ents}
    rule3_pairs = []
    for token in doc:
        children = token.children
        A = ""999999""
        M = ""999999""
        add_neg_pfx = False
        for child in children:
            if(child.dep_ == ""nsubj"" and not child.is_stop): # nsubj is nominal subject
                if child.idx in ner_heads:
                    A = ner_heads[child.idx].text
                else:
                    A = child.text
            if(child.dep_ == ""acomp"" and not child.is_stop): # acomp is adjectival complement
                M = child.text
            # example - 'this could have been better' -&gt; (this, not better)
            if(child.dep_ == ""aux"" and child.tag_ == ""MD""): # MD is modal auxiliary
                neg_prefix = ""not""
                add_neg_pfx = True
            if(child.dep_ == ""neg""): # neg is negation
                neg_prefix = child.text
                add_neg_pfx = True
        if (add_neg_pfx and M != ""999999""):
            M = neg_prefix + "" "" + M
        if(A != ""999999"" and M != ""999999""):
            rule3_pairs.append((A, M, sid.polarity_scores(M)['compound']))
    return rule3_pairs

print(find_sentiment(nlp(""Air France is cool."")))
print(find_sentiment(nlp(""I think Gabriel Garc√≠a M√°rquez is not boring."")))
print(find_sentiment(nlp(""They say Central African Republic is really great. "")))
</code></pre>

<p>The output of this code will be what you need:</p>

<pre><code>[('Air France', 'cool', 0.3182)]
[('Gabriel Garc√≠a M√°rquez', 'not boring', 0.2411)]
[('Central African Republic', 'great', 0.6249)]
</code></pre>

<p>Enjoy!</p>
",""
"61010799","2020-04-03 11:24:56","2","","61010075","<p>You can specify the tuple of fields yourself:</p>

<pre><code>fields = ('id', 'form', 'lemma', 'upostag', 'xpostag', 'feats', 'head', 'deprel', 'deps', 'misc', 'rest')
for tokentree in parse_incr(data_file, fields=fields):
    print(tokentree.serialize())
</code></pre>

<p>output:</p>

<pre><code>24  Permission  _   NN  NN  _   27  nsubjpass   _   _   B-PERMISSION
25  is  _   VBZ VBZ _   27  auxpass _   _   I-PERMISSION
26  hereby  _   RB  RB  _   27  advmod  _   _   I-PERMISSION
27  granted _   VBN VBN _   11  rcmod   _   _   I-PERMISSION
</code></pre>
",""
"60989317","2020-04-02 10:21:25","3","","60867353","<p>Ended up solving this by re-implementing along the lines of this GitHub post: 
<a href=""https://github.com/marcotcr/lime/issues/409"" rel=""nofollow noreferrer"">https://github.com/marcotcr/lime/issues/409</a></p>

<p>My code is now very different from the above - probably makes sense if you look to the GitHub post for guidance if you're running into similar issues.  </p>
",""
"60928633","2020-03-30 10:33:16","17","","60928526","<p>It seems the answer is in the error message: the input of <code>pos_tag</code> should be a string and you provide a column input. You should apply <code>pos_tag</code> on each row of you column, using the function <code>withColumn</code></p>

<p>For example you start by writing:</p>

<pre class=""lang-py prettyprint-override""><code>my_new_df = df_removed.withColumn(""removed"", nltk.pos_tag(df_removed.removed))
</code></pre>

<p>You can do also : </p>

<pre><code>my_new_df = df_removed.select(""removed"").rdd.map(lambda x: nltk.pos_tag(x)).toDF()
</code></pre>

<p><a href=""https://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html"" rel=""nofollow noreferrer"">Here</a> you have the documentation.</p>
",""
"60922553","2020-03-30 00:38:51","7","","60922171","<p>It is taken from <a href=""https://cogcomp.seas.upenn.edu/papers/GuptaSiRo17.pdf"" rel=""nofollow noreferrer"">Entity Linking via Joint Encoding of Types, Descriptions, and Context</a> section 4 equation 2. </p>

<p>I don't feel confident enough though in explaining the formula in detail, on overall the purpose is to combine probability scores for entitiy candidates derived from external knowledge based resources (KB in the paper), which are the prior probabilities, and scores estimated with a sentence encoder, used to encode the mention to link along with its context, sims in the formula because they compute cosine similarity between the encoded mention vector and all entity candidates (which is why this formula is used only if ""incl_context"" is true).  </p>
",""
"60872541","2020-03-26 17:09:22","1","","60871375","<p>If you're using a dataframe I suggest you to store the pre processing steps results in a new column. In this way you can always check the output, and you can always create a list of lists to use as an input for a model in a line of code afterwords. Another advantage of this approach is that you can easily visualise the line of preprocessing and add other steps wherever you need without getting confused.</p>

<p>Regarding your code, It can be optimised (for example you could perform stop words removal and tokenisation at the same time) and I see a bit of confusion about the steps you performed. For example you performe multiple times lemmatisation, using also different libraries, and there is no point in doing that. In my opinion nltk works just fine, personally I use other libraries to preprocess tweets only to deal with emojis, urls and hashtags, all stuff specifically related to tweets. </p>

<pre><code># I won't write all the imports, you get them from your code
# define new column to store the processed tweets
df_tweet1['Tweet Content Clean'] = pd.Series(index=df_tweet1.index)

tknzr = TweetTokenizer()
lmtzr = WordNetLemmatizer()

stop_words = set(stopwords.words(""english""))
new_stopwords = ['!', ',', ':', '&amp;', '%', '.', '‚Äô']
new_stopwords_list = stop_words.union(new_stopwords)

# iterate through each tweet
for ind, row in df_tweet1.iterrows():

    # get initial tweet: ['This is the initial tweet']
    tweet = row['Tweet Content']

    # tokenisation, stopwords removal and lemmatisation all at once
    # out: ['initial', 'tweet']
    tweet = [lmtzr.lemmatize(i) for i in tknzr.tokenize(tweet) if i.lower() not in new_stopwords_list]

    # pos tag, no need to lemmatise again after.
    # out: [('initial', 'JJ'), ('tweet', 'NN')]
    tweet = nltk.pos_tag(tweet)

    # save processed tweet into the new column
    df_tweet1.loc[ind, 'Tweet Content Clean'] = tweet
</code></pre>

<p>So on overall all you need are 4 lines, one for getting the tweet string, two to preprocess the text, another one to store the tweet. You can add extra processing step paying attention to the output of each step (for example tokenisation return a list of strings, pos tagging return a list of tuples, reason why you are getting troubles). </p>

<p>If you want then you can create a list of lists containing all tweet in the dataframe:</p>

<pre><code># out: [[('initial', 'JJ'), ('tweet', 'NN')], [second tweet], [third tweet]]
all_tweets = [tweet for tweet in df_tweet1['Tweet Content Clean']]
</code></pre>
",""
"60843545","2020-03-25 05:45:19","0","","60842476","<p>That is because your input is two-dimensional so that your model summary is as follows.</p>

<pre><code>_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_1 (Dense)              (None, 100, 50)           2550      
_________________________________________________________________
dense_2 (Dense)              (None, 100, 50)           2550      
=================================================================
Total params: 5,100
Trainable params: 5,100
Non-trainable params: 0
_________________________________________________________________
</code></pre>

<p>Try to reshape the tensor shape after <code>dense_1</code> or <code>dense_2</code> by taking sum/average over the second axis to get the same shape as your <code>Y</code>.</p>

<p><strong>Here is my example for your reference</strong></p>

<p>I wrote a customized sum function to sum up over the second axis to make the tensor of (None, 100, 50) be (None, 1, 50). Then, add another Dense layer.</p>

<pre class=""lang-py prettyprint-override""><code>import keras.backend as K
from keras.layers import Dense, Lambda
from keras.models import Sequential

def mysum(x): 
    return  K.sum(x, axis=1, keepdims=True) 

def mysum_output_shape(input_shape): 
    shape = list(input_shape) 
    print(shape)
    shape[1] = 1 
    return tuple(shape) 

# randomly generate data
import numpy as np
X_train = np.random.normal(0, 1, (50,100,50))
y_train = np.ones((50, 1, 50))

model = Sequential()
model.add(Dense(50, activation='sigmoid', input_shape=(100,50)))
model.add(Lambda(mysum, output_shape=mysum_output_shape)) 
model.add(Dense(50))
model.compile(optimizer='rmsprop', loss='mse')
model.fit(X_train, y_train, epochs=50)

</code></pre>

<p>It is important to know what your input is and how tensors are transformed through every layer. Hope this helps!</p>
",""
"60841866","2020-03-25 01:35:43","0","","60832385","<p>My Answer is:</p>

<pre><code>def ExtractNP(text):
nounphrases = []
words = nltk.word_tokenize(text)
tagged = nltk.pos_tag(words)
grammar = r""""""
     NP:
        {&lt;JJ*&gt;&lt;NN+&gt;&lt;IN&gt;&lt;NN&gt;}
        {&lt;NN.*|JJ&gt;*&lt;NN.*&gt;}
    """"""
chunkParser = nltk.RegexpParser(grammar)
tree = chunkParser.parse(tagged)
for subtree in tree.subtrees(filter=lambda t: t.label() == 'NP'):
    myPhrase = ''
    for item in subtree.leaves():
        myPhrase += ' ' + item[0]
    nounphrases.append(myPhrase.strip())
    # print(myPhrase)
nounphrases = list(filter(lambda x: len(x.split()) &gt; 1, nounphrases))
return nounphrases
</code></pre>

<p>Actually, this is not new but I found that the grammar regressions is chunked orderly as they declared. It is mean that the input sentence ('postal code is new approach of delivery') will be cut the content which match to </p>

<pre><code>{&lt;JJ*&gt;&lt;NN+&gt;&lt;IN&gt;&lt;NN&gt;}
</code></pre>

<p>('new approach of delivery'), and then the rest of it ('postal code is') will be compare and used in next matching with </p>

<pre><code>{&lt;NN.*|JJ&gt;*&lt;NN.*&gt;}
</code></pre>

<p>to return 'postal code'. So that, we cannot get the 'new approach' in the returned result.</p>
",""
"60834242","2020-03-24 15:33:41","2","","60832547","<p>Ah ok, I found the answer. The code is actually returning cross entropy. In the github comment where they say it is perplexity...they are saying that because the OP does</p>

<pre><code>return math.exp(loss)
</code></pre>

<p>which transforms entropy to perplexity :)</p>
",""
"60809901","2020-03-23 08:15:53","2","","60809394","<p>In theory it's possible. First, you'll need to make sure you have a component that tags these kind of entities. You could <a href=""https://spacy.io/usage/training#ner"" rel=""nofollow noreferrer"">train an NER model</a> for this, but be aware that its performance might not be as good on things like &quot;cold&quot; than it would be for actual named entities like &quot;London&quot;.</p>
<p>To create the Knowledge Base and the Entity Linker from Wikipedia/Wikidata, the <a href=""https://github.com/explosion/projects/tree/master/nel-wikipedia"" rel=""nofollow noreferrer"">example scripts</a> are not limited to named entities - they attempt to parse anything that appears in an intra-wiki link. If the word &quot;cold&quot; gets linked to the page &quot;Common cold&quot;, it should be able to learn it. The exact entities that are stored in the KB and that are used for training the EL model, depend on which entities are found by your entity recognizer component. So if you adjust that according to your use-case, the entity linking component will follow automatically.</p>
",""
"60792771","2020-03-21 20:12:40","1","","16791716","<p><a href=""https://opennlp.apache.org/docs/1.8.0/manual/opennlp.html#tools.postagger.tagging.cmdline"" rel=""nofollow noreferrer"">OpenNLP</a> allows getting n best for POS tagging: </p>

<blockquote>
  <p>Some applications need to retrieve the n-best pos tag sequences and
  not only the best sequence. The topKSequences method is capable of
  returning the top sequences. It can be called in a similar way as tag.</p>

<pre><code>Sequence topSequences[] = tagger.topKSequences(sent);
</code></pre>
  
  <p>Each Sequence object contains one sequence. The sequence can be retrieved via Sequence.getOutcomes() which returns a tags array and
  Sequence.getProbs() returns the probability array for this sequence.</p>
</blockquote>

<p>Also, there is also a way to make spaCy do something like this:</p>

<pre><code>Doc.set_extension('tag_scores', default=None)
Token.set_extension('tag_scores', getter=lambda token: token.doc._.tag_scores[token.i])

class ProbabilityTagger(Tagger):
    def predict(self, docs):
        tokvecs = self.model.tok2vec(docs)
        scores = self.model.softmax(tokvecs)
        guesses = []
        for i, doc_scores in enumerate(scores):
            docs[i]._.tag_scores = doc_scores
            doc_guesses = doc_scores.argmax(axis=1)

            if not isinstance(doc_guesses, numpy.ndarray):
                doc_guesses = doc_guesses.get()
            guesses.append(doc_guesses)
        return guesses, tokvecs


Language.factories['tagger'] = lambda nlp, **cfg: ProbabilityTagger(nlp.vocab, **cfg)
</code></pre>

<p>Then each token will have tag_scores with the probabilities for each part of speech from spaCy's <a href=""https://github.com/explosion/spaCy/blob/master/spacy/lang/en/tag_map.py"" rel=""nofollow noreferrer"">tag map</a>.</p>

<p>Source: <a href=""https://github.com/explosion/spaCy/issues/2087"" rel=""nofollow noreferrer"">https://github.com/explosion/spaCy/issues/2087</a></p>
",""
"60765752","2020-03-19 22:07:23","3","","60751304","<p>Try this code:</p>
<pre><code>import spacy
nlp = spacy.load(&quot;en_core_web_sm&quot;)
doc = nlp(&quot;Female or not White&quot;)
spacy.displacy.render(doc, style='dep')
</code></pre>
<p>Output:
<a href=""https://i.sstatic.net/HXdLn.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/HXdLn.png"" alt=""enter image description here"" /></a></p>
<p>So in your case, <code>Not</code> will be considered as inversion</p>
<p>Or you can refer here for sentence parsing- <a href=""https://stackoverflow.com/questions/42322902/how-to-get-parse-tree-using-python-nltk/42323459"">how to get parse tree using python nltk?</a></p>
",""
"60681122","2020-03-14 08:50:21","1","","60680245","<p>Simply because your desired result has lower probability then the result you got. We can compute the probability of your desired result:</p>

<pre><code>S -&gt; NP VP       1.0

NP -&gt; PPER       0.5
PPER -&gt; Ich      1.0

VP -&gt; VVFIN NP   0.75
VVFIN -&gt; sah     1.0
NP -&gt; NP PP      0.25

NP -&gt; ART NN     0.25
ART -&gt; den       0.5
NN -&gt; Tiger      0.5

PP -&gt; APPR NP    1.0
APPR -&gt; unter    1.0

NP -&gt; ART NN     0.25
ART -&gt; der       0.5
NN -&gt; Felse      0.5
</code></pre>

<p>Multiplied together gets probability <code>0.0003662109375</code>, which is definitely less than the result you got <code>0.000488281</code>.</p>
",""
"60642923","2020-03-11 19:05:13","0","","60638828","<p>Here's an extracted bit of code I had, that used the SpaCy lemmatizer by itself.  I'm not somewhere I can run it so it might have a small bug or two if I made an editing mistake.</p>

<p>Note that in general, you need to know the <code>upos</code> for the word in order to lemmatize correctly.  This code will return all the possible lemmas but I would advise modifying it to pass in the correct <code>upos</code> for your word.</p>

<pre><code>class SpacyLemmatizer(object):
    def __init__(self, smodel):
        import spacy
        self.lemmatizer = spacy.load(smodel).vocab.morphology.lemmatizer

    # get the lemmas for every upos
    def getLemmas(self, entry):
        possible_lemmas = set()
        for upos in ('NOUN', 'VERB', 'ADJ', 'ADV'):
            lemmas = self.lemmatizer(entry, upos, morphology=None)
            lemma = lemmas[0]    # See morphology.pyx::lemmatize
            possible_lemmas.add( lemma )
        return possible_lemmas
</code></pre>
",""
"60553026","2020-03-05 19:54:03","14","","60534999","<p>Unlike the English lemmatizer, spaCy's Spanish lemmatizer does not use PoS information at all. It relies on a lookup list of inflected verbs and lemmas (e.g., ideo idear, ideas idear, idea idear, ideamos idear, etc.). It will just output the first match in the list, regardless of its PoS.</p>

<p>I actually developed spaCy's new rule-based lemmatizer for Spanish, which takes PoS and morphological information (such as tense, gender, number) into account. These fine-grained rules make it a lot more accurate than the current lookup lemmatizer. It will be released soon!</p>

<p>Meanwhile, you can maybe use Stanford CoreNLP or FreeLing.</p>
",""
"60541041","2020-03-05 08:29:58","3","","60533029","<p>The v2.2.[0-5] <code>md</code> models have a minor bug that make them particularly slow to load (see <a href=""https://github.com/explosion/spaCy/pull/4990"" rel=""nofollow noreferrer"">https://github.com/explosion/spaCy/pull/4990</a>).</p>

<p>You can reformat one file in the model package to improve the load time.
In the <code>vocab</code> directory for the model package (e.g., <code>lib/python3.7/site-packages/en_core_web_md/en_core_web_md-2.2.5/vocab</code>):</p>

<pre><code>import srsly
orig_data = srsly.read_msgpack(""key2row"")
new_data = {}
for key, value in orig_data.items():
    new_data[int(key)] = int(value)
srsly.write_msgpack(""key2row"", new_data)
</code></pre>

<p>In my tests, this nearly halves the loading time (18s to 10s). The remaining time is mostly loading strings and lexemes for the model, which is harder to optimize further at this point. So this improves things a bit but the overall load time is still relatively burdensome for short tests.</p>
",""
"60481357","2020-03-02 01:12:32","1","","60481221","<p>You first need to download the data:</p>

<pre><code>!spacy download es_core_news_sm
</code></pre>

<p>Then <strong>Restart the runtime</strong>, after which your code will run correctly:</p>

<pre><code>import spacy
spacy.prefer_gpu()

nlp = spacy.load('es_core_news_sm')
text = 'yo canto, t√∫ cantas, ella canta, nosotros cantamos, cant√°is, cantan‚Ä¶'
doc = nlp(text)
lemmas = [tok.lemma_.lower() for tok in doc]
print(len(lemmas))
</code></pre>

<pre><code>16
</code></pre>
",""
"60387996","2020-02-25 05:25:50","0","","60387288","<h1>Q: Lancaster was supposed to be ""aggressive"" stemmer but it worked properly with <code>replied</code>. Why?</h1>

<p>It's because Lancaster stemmer implementation is improved in <a href=""https://github.com/nltk/nltk/pull/1654"" rel=""nofollow noreferrer"">https://github.com/nltk/nltk/pull/1654</a></p>

<p>If we take a look at <a href=""https://github.com/nltk/nltk/blob/develop/nltk/stem/lancaster.py#L62"" rel=""nofollow noreferrer"">https://github.com/nltk/nltk/blob/develop/nltk/stem/lancaster.py#L62</a>, there's a suffix rule, to change <code>-ied &gt; -y</code> </p>

<pre><code>default_rule_tuple = (
    ""ai*2."",   # -ia &gt; -   if intact
    ""a*1."",    # -a &gt; -    if intact
    ""bb1."",    # -bb &gt; -b
    ""city3s."", # -ytic &gt; -ys
    ""ci2&gt;"",    # -ic &gt; -
    ""cn1t&gt;"",   # -nc &gt; -nt
    ""dd1."",    # -dd &gt; -d
    ""dei3y&gt;"",  # -ied &gt; -y
    ...)
</code></pre>

<p>The feature allows users to input new rules and if no additional rules are added, then it'll use the <code>self.default_rule_tuple</code> in <code>parseRules</code> where the <code>rule_tuple</code> will be applied <a href=""https://github.com/nltk/nltk/blob/develop/nltk/stem/lancaster.py#L196"" rel=""nofollow noreferrer"">https://github.com/nltk/nltk/blob/develop/nltk/stem/lancaster.py#L196</a></p>

<pre class=""lang-py prettyprint-override""><code>def parseRules(self, rule_tuple=None):
    """"""Validate the set of rules used in this stemmer.
    If this function is called as an individual method, without using stem
    method, rule_tuple argument will be compiled into self.rule_dictionary.
    If this function is called within stem, self._rule_tuple will be used.
    """"""
    # If there is no argument for the function, use class' own rule tuple.
    rule_tuple = rule_tuple if rule_tuple else self._rule_tuple
    valid_rule = re.compile(""^[a-z]+\*?\d[a-z]*[&gt;\.]?$"")
    # Empty any old rules from the rule set before adding new ones
    self.rule_dictionary = {}

    for rule in rule_tuple:
        if not valid_rule.match(rule):
            raise ValueError(""The rule {0} is invalid"".format(rule))
        first_letter = rule[0:1]
        if first_letter in self.rule_dictionary:
            self.rule_dictionary[first_letter].append(rule)
        else:
            self.rule_dictionary[first_letter] = [rule]
</code></pre>

<p>The <code>default_rule_tuple</code> actually comes from the whoosh implementation of the <a href=""https://bitbucket.org/mchaput/whoosh/src/e344fb64067e45d47ec62dc65a75a50be51264a7/src/whoosh/lang/paicehusk.py?at=default&amp;fileviewer=file-view-default"" rel=""nofollow noreferrer"">paice-husk stemmer</a> which aka as the Lancaster stemmer <a href=""https://github.com/nltk/nltk/pull/1661"" rel=""nofollow noreferrer"">https://github.com/nltk/nltk/pull/1661</a> =)</p>

<h1>Q: The word In remained the same in Porter with uppercase In, Why?</h1>

<p>This is super interesting! And most probably a bug. </p>

<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; from nltk.stem import PorterStemmer
&gt;&gt;&gt; porter = PorterStemmer()
&gt;&gt;&gt; porter.stem('In')
'In'
</code></pre>

<p>If we look at the code, the first thing that <code>PorterStemmer.stem()</code> does it to lowercase, <a href=""https://github.com/nltk/nltk/blob/develop/nltk/stem/porter.py#L651"" rel=""nofollow noreferrer"">https://github.com/nltk/nltk/blob/develop/nltk/stem/porter.py#L651</a></p>

<pre class=""lang-py prettyprint-override""><code>def stem(self, word):
    stem = word.lower()

    if self.mode == self.NLTK_EXTENSIONS and word in self.pool:
        return self.pool[word]

    if self.mode != self.ORIGINAL_ALGORITHM and len(word) &lt;= 2:
        # With this line, strings of length 1 or 2 don't go through
        # the stemming process, although no mention is made of this
        # in the published algorithm.
        return word

    stem = self._step1a(stem)
    stem = self._step1b(stem)
    stem = self._step1c(stem)
    stem = self._step2(stem)
    stem = self._step3(stem)
    stem = self._step4(stem)
    stem = self._step5a(stem)
    stem = self._step5b(stem)

    return stem
</code></pre>

<p>But if we look at the code, everything else returns the <code>stem</code>, which is lowercased but <strong>there are two if clauses that returns some form of the original <code>word</code> that hasn't been lowercased!!!</strong> </p>

<pre class=""lang-py prettyprint-override""><code>if self.mode == self.NLTK_EXTENSIONS and word in self.pool:
    return self.pool[word]

if self.mode != self.ORIGINAL_ALGORITHM and len(word) &lt;= 2:
    # With this line, strings of length 1 or 2 don't go through
    # the stemming process, although no mention is made of this
    # in the published algorithm.
    return word
</code></pre>

<p>The first if clause checks if the word is inside the <code>self.pool</code> which contains the irregular words and their stems. </p>

<p>The second checks if the <code>len(word)</code> &lt;= 2, then return it's original form, which in the case of ""In"" the 2nd if clause returns True, thus the  original non-lowercased form returned. </p>

<h1>Q: Notice that the Lancaster is removing words ending with <code>e</code> in ""came"", Why?</h1>

<p>Not surprisingly also coming from the <code>default_rule_tuple</code> 
 <a href=""https://github.com/nltk/nltk/blob/develop/nltk/stem/lancaster.py#L67"" rel=""nofollow noreferrer"">https://github.com/nltk/nltk/blob/develop/nltk/stem/lancaster.py#L67</a>, there's a rule that changes <code>-e &gt; -</code> =)</p>

<h2>Q: How do I disable the <code>-e &gt; -</code> rule from <code>default_rule_tuple</code>?</h2>

<p>(Un-)fortunately, the <code>LancasterStemmer._rule_tuple</code> object is an immutable tuple, so we can't simply remove one item from it, but we can override it =)</p>

<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; from nltk.stem import LancasterStemmer
&gt;&gt;&gt; lancaster = LancasterStemmer()
&gt;&gt;&gt; lancaster.stem('came')
'cam'

# Create a new stemmer object to refresh the cache.
&gt;&gt;&gt; lancaster = LancasterStemmer()
&gt;&gt;&gt; temp_rule_list = list(lancaster._rule_tuple)
# Find the 'e1&gt;' rule.
&gt;&gt;&gt; lancaster._rule_tuple.index('e1&gt;') 
12

# Create a temporary rule list from the tuple.
&gt;&gt;&gt; temp_rule_list = list(lancaster._rule_tuple)
# Remove the rule.
&gt;&gt;&gt; temp_rule_list.pop(12)
'e1&gt;'
# Override the `._rule_tuple` variable.
&gt;&gt;&gt; lancaster._rule_tuple = tuple(temp_rule_list)

# Et voila!
&gt;&gt;&gt; lancaster.stem('came')
'came'
</code></pre>
",""
"60379671","2020-02-24 15:59:07","2","","60363904","<p>From <a href=""https://github.com/nltk/nltk/blob/develop/nltk/tag/__init__.py#L135"" rel=""noreferrer"">https://github.com/nltk/nltk/blob/develop/nltk/tag/<strong>init</strong>.py#L135</a> </p>

<pre><code>&gt;&gt;&gt; from nltk.tag import pos_tag
&gt;&gt;&gt; from nltk.tokenize import word_tokenize

# Default Penntreebank tagset.
&gt;&gt;&gt; pos_tag(word_tokenize(""John's big idea isn't all that bad.""))
[('John', 'NNP'), (""'s"", 'POS'), ('big', 'JJ'), ('idea', 'NN'), ('is', 'VBZ'),
(""n't"", 'RB'), ('all', 'PDT'), ('that', 'DT'), ('bad', 'JJ'), ('.', '.')]

# Universal POS tags.
&gt;&gt;&gt; pos_tag(word_tokenize(""John's big idea isn't all that bad.""), tagset='universal')
[('John', 'NOUN'), (""'s"", 'PRT'), ('big', 'ADJ'), ('idea', 'NOUN'), ('is', 'VERB'),
(""n't"", 'ADV'), ('all', 'DET'), ('that', 'DET'), ('bad', 'ADJ'), ('.', '.')]
</code></pre>
",""
"60372277","2020-02-24 08:50:26","1","","60372143","<p>You can use list comprehension</p>

<pre><code>word_list1 = [""cccc"", ""bbbb"", ""aaa""]

def stem_text(text):
    text = text.split()
    temp = [word_list1[0] if i in word_list1 else i for i in text]
    text = ' '.join(temp)
    return text

stem_text(""hello bbbb now aaa den kkk"")
</code></pre>

<p>Output :</p>

<pre><code>'hello cccc now cccc den kkk'
</code></pre>
",""
"60345957","2020-02-21 20:40:31","0","","60306461","<p>Thanks to <a href=""https://stackoverflow.com/users/4620420/bivouac0"">bivouac0</a>'s comment. I checked tag_ field of each token and retrieved lemma of tokens being tagged as 'NNS' or 'NNPS'</p>

<pre><code>processed_text = nlp(original_text)
lemma_tags = {""NNS"", ""NNPS""}
for token in processed_text:
   lemma = token.text
   if token.tag_ in lemma_tags:
      lemma = token.lemma_
   ...
   # rest of code
   ...
   ...
</code></pre>
",""
"60340477","2020-02-21 14:18:40","0","","60184117","<p>My problem was that I didn't check the size of my GPU memory with comparison to the sizes of samples. I had a lot of pretty small samples and after many iterations a large one. My bad.
Thank you and remember to check these things if it happens to you to.</p>
",""
"60296409","2020-02-19 09:00:41","2","","60293351","<p>TF-IDF isn't a single value (i.e. scalar). For every document, it returns a vector where each value in the vector corresponds to each word in the vocabulary. </p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
from scipy.sparse.csr import csr_matrix

sent1 = ""the quick brown fox jumps over the lazy brown dog""
sent2 = ""mr brown jumps over the lazy fox""

corpus = [sent1, sent2]
vectorizer = TfidfVectorizer(input=corpus)

X = vectorizer.fit_transform(corpus)
print(X.todense())
</code></pre>

<p>[out]:</p>

<pre><code>matrix([[0.50077266, 0.35190925, 0.25038633, 0.25038633, 0.25038633,
         0.        , 0.25038633, 0.35190925, 0.50077266],
        [0.35409974, 0.        , 0.35409974, 0.35409974, 0.35409974,
         0.49767483, 0.35409974, 0.        , 0.35409974]])
</code></pre>

<p>It returns a 2-D matrix where the rows represents the sentences and the columns represent the vocabulary. </p>

<pre><code>&gt;&gt;&gt; vectorizer.vocabulary_
{'the': 8,
 'quick': 7,
 'brown': 0,
 'fox': 2,
 'jumps': 3,
 'over': 6,
 'lazy': 4,
 'dog': 1,
 'mr': 5}
</code></pre>

<p>So when K-means tries to find the distance/similarity between two documents, it's performing the similarity between two rows in the matrix. E.g. assuming the similarity is just the dot product between two rows:</p>

<pre><code>import numpy as np
vector1 = X.todense()[0]
vector2 = X.todense()[1]
float(np.dot(vector1, vector2.T))
</code></pre>

<p>[out]:</p>

<pre><code>0.7092938737640962
</code></pre>

<p>Chris Potts has a nice tutorial on how vector space models like TF-IDF one is created <a href=""http://web.stanford.edu/class/linguist236/materials/ling236-handout-05-09-vsm.pdf"" rel=""noreferrer"">http://web.stanford.edu/class/linguist236/materials/ling236-handout-05-09-vsm.pdf</a> </p>
",""
"60294069","2020-02-19 06:10:23","1","","60291151","<p>Lets walk through the code and see how to get your desired output.</p>

<p>First the imports, you have</p>

<pre><code>import nltk
from nltk import pos_tag
</code></pre>

<p>and then you were using </p>

<pre><code>pos_label = nltk.pos_tag(...)
</code></pre>

<p>Since you're already using <code>from nltk import pos_tag</code>, the <code>pos_tag</code> is already in the global namespace, just do:</p>

<pre><code>pos_label = pos_tag(...)
</code></pre>

<p>Idiomatically, the imports should be cleaned up a little to look like this:</p>

<pre><code>from nltk import word_tokenize, pos_tag
from nltk.corpus import wordnet as wn
from nltk.stem import WordNetLemmatizer

wnl = WordNetLemmatizer()
</code></pre>

<p>Next actually keeping the list of tokenized words and then the list of pos tags and then the list of lemmas separately sounds logical but since the function finally only returns the function, you should be able to chain up the <code>pos_tag(word_tokenize(...))</code> function and iterate through it so that you can retrieve the POS tag and tokens, i.e. </p>

<pre><code>sentence = ""I love running angrily""
for word, pos in pos_tag(word_tokenize(sentence)):
    print(word, '|', pos)
</code></pre>

<p>[out]:</p>

<pre><code>I | PRP
love | VBP
running | VBG
angrily | RB
</code></pre>

<p>Now, we know that there's a mismatch between the outputs of <code>pos_tag</code> and the POS that the <code>WordNetLemmatizer</code> is expecting. From <a href=""https://github.com/alvations/pywsd/blob/master/pywsd/utils.py#L124"" rel=""nofollow noreferrer"">https://github.com/alvations/pywsd/blob/master/pywsd/utils.py#L124</a>, there is a function call <code>penn2morphy</code> that looks like this:</p>

<pre><code>def penn2morphy(penntag, returnNone=False, default_to_noun=False) -&gt; str:
    """"""
    Converts tags from Penn format (input: single string) to Morphy.
    """"""
    morphy_tag = {'NN':'n', 'JJ':'a', 'VB':'v', 'RB':'r'}
    try:
        return morphy_tag[penntag[:2]]
    except:
        if returnNone:
            return None
        elif default_to_noun:
            return 'n'
        else:
            return ''
</code></pre>

<p>An example:</p>

<pre><code>&gt;&gt;&gt; penn2morphy('JJ')
'a'
&gt;&gt;&gt; penn2morphy('PRP')
''
</code></pre>

<p>And if we use these converted tags as inputs to the <code>WordNetLemmatizer</code> and reusing your if-else conditions:</p>

<pre><code>sentence = ""I love running angrily""
for token, pos in pos_tag(word_tokenize(sentence)):
    morphy_pos = penn2morphy(pos)
    if morphy_pos in [""a"", ""n"", ""v""]:
        print(wnl.lemmatize(token, pos=morphy_pos))
    elif morphy_pos in ['r']: 
        print(wn.synset(token+"".r.1"").lemmas()[0].pertainyms()[0].name())
    else:
        print(wnl.lemmatize(token))
</code></pre>

<p>[out]:</p>

<pre><code>I
love
run
angry
</code></pre>

<p><strong>Hey, what did you do there? Your code works but mine doesn't!</strong></p>

<p>Okay, now that we know how to get the desired output. Lets recap. </p>

<ul>
<li>First, we clean up imports</li>
<li>Then, we clean up the preprocessing (without keeping intermediate variables)</li>
<li>Then, we ""functionalized"" the conversion of POS tags from Penn -> Morphy</li>
<li>Lastly, we applied the same if/else conditions and run the lemmatizer.</li>
</ul>

<hr>

<p><strong>But how is it that my code doesn't work?!</strong></p>

<p>Okay, lets work through your code to see why you're getting the error. </p>

<p>First lets check every output that you get within the <code>findTag</code> function, printing the type of the output and the output</p>

<pre><code>sentence = ""I love running angrily""
sentence = word_tokenize(sentence)
print(type(sentence))
print(sentence)
</code></pre>

<p>[out]:</p>

<pre><code>&lt;class 'list'&gt;
['I', 'love', 'running', 'angrily']
</code></pre>

<p>At <code>sentence = word_tokenize(sentence)</code>, you have already overwritten your original variable to the function, usually that's a sign of error later on =)</p>

<p>Now lets look at the next line:</p>

<pre><code>sentence = ""I love running angrily""
sentence = word_tokenize(sentence)
sentence = [i.strip("" "") for i in sentence]

print(type(sentence))
print(sentence)
</code></pre>

<p>[out]:</p>

<pre><code>&lt;class 'list'&gt;
['I', 'love', 'running', 'angrily']
</code></pre>

<p>Now we see that the <code>sentence = [i.strip("" "") for i in sentence]</code> is actually meaningless given the example sentence. </p>

<p><strong>Q: But is it true that all tokens output by <code>word_tokenize</code> would have no trailing/heading spaces which <code>i.strip(' ')</code> is trying to do?</strong></p>

<p>A: Yes, it seems like so. Then NLTK first performs the regex operations on strings, then call the <a href=""https://docs.python.org/3.8/library/stdtypes.html#str.split"" rel=""nofollow noreferrer""><code>str.split()</code></a> function which would have removed heading/trailing spaces between tokens, see <a href=""https://github.com/nltk/nltk/blob/develop/nltk/tokenize/destructive.py#L141"" rel=""nofollow noreferrer"">https://github.com/nltk/nltk/blob/develop/nltk/tokenize/destructive.py#L141</a></p>

<p>Lets continue:</p>

<pre><code>sentence = ""I love running angrily""
sentence = word_tokenize(sentence)
sentence = [i.strip("" "") for i in sentence]
pos_label = nltk.pos_tag(sentence)[0][1][0].lower()

print(type(pos_label))
print(pos_label)
</code></pre>

<p>[out]:</p>

<pre><code>&lt;class 'str'&gt;
p
</code></pre>

<p><strong>Q: Wait a minute, where is the <code>pos_label</code> only a single string?</strong></p>

<p><strong>Q: And what is POS tag <code>p</code>?</strong></p>

<p>A: Lets look closer what's happening in <code>nltk.pos_tag(sentence)[0][1][0].lower()</code></p>

<p>Usually, when you have to do such <code>[0][1][0]</code> nested index retrieval, its error prone. We need to ask what's <code>[0][1][0]</code>?</p>

<p>We know that sentence now after <code>sentence = word_tokenize(sentence)</code> has became a list of strings. And <code>pos_tag(sentence)</code> would return a list of tuples of strings where the first item in the tuple is the token and the second the POS tag, i.e. </p>

<pre><code>sentence = ""I love running angrily""
sentence = word_tokenize(sentence)
sentence = [i.strip("" "") for i in sentence]
thing = pos_tag(sentence)

print(type(thing))
print(thing)
</code></pre>

<p>[out]:</p>

<pre><code>&lt;class 'list'&gt;
[('I', 'PRP'), ('love', 'VBP'), ('running', 'VBG'), ('angrily', 'RB')]
</code></pre>

<p>Now if we know <code>thing = pos_tag(word_tokenize(""I love running angrily""))</code>, outputs the above, lets work with that to see what <code>[0][1][0]</code> is accessing. </p>

<pre><code>&gt;&gt;&gt; thing = [('I', 'PRP'), ('love', 'VBP'), ('running', 'VBG'), ('angrily', 'RB')]
&gt;&gt;&gt; thing[0][1]
('I', 'PRP')
</code></pre>

<p>So <code>thing[0]</code> outputs the tuple of <code>(token, pos)</code> for the first token. </p>

<pre><code>&gt;&gt;&gt; thing = [('I', 'PRP'), ('love', 'VBP'), ('running', 'VBG'), ('angrily', 'RB')]
&gt;&gt;&gt; thing[0][1]
'PRP'
</code></pre>

<p>And <code>thing[0][1]</code> outputs the POS for the first token. </p>

<pre><code>&gt;&gt;&gt; thing = [('I', 'PRP'), ('love', 'VBP'), ('running', 'VBG'), ('angrily', 'RB')]
&gt;&gt;&gt; thing[0][1][0]
'P'
</code></pre>

<p>Achso, the <code>[0][1][0]</code> looks for the first character of the POS of the first token.</p>

<p>So the question is that the desired behavior? If so, why are you only looking at the POS of the first word?</p>

<hr>

<p><strong>Regardless of what I'm looking at. Your explanation still that doesn't tell me why the <code>TypeError: unhashable type: 'list'</code> occurs. Stop distracting me and tell me how to resolve the <code>TypeError</code>!!</strong> </p>

<p>Okay, we move on, now that we know <code>thing = pos_tag(word_tokenize(""I love running angrily""))</code> and <code>thing[0][1][0].lower() = 'p'</code></p>

<p>Given your if-else conditions, </p>

<pre><code>if pos_label in [""a"", ""n"", ""v""]:
    print(lem.lemmatize(word, pos = pos_label))
elif pos_label in ['r']: 
    print(wordnet.synset(sentence+"".r.1"").lemmas()[0].pertainyms()[0].name())
else:
    print(lem.lemmatize(sentence))
</code></pre>

<p>we find that <code>'p'</code> value would have gone to the else, i.e. <code>print(lem.lemmatize(sentence))</code> but wait a minute remember what has <code>sentence</code> became after you've modified it with:</p>

<pre><code>&gt;&gt;&gt; sentence = word_tokenize(""I love running angrily"")
&gt;&gt;&gt; sentence = [i.strip("" "") for i in sentence]
&gt;&gt;&gt; sentence 
['I', 'love', 'running', 'angrily']
</code></pre>

<p>So what happens if we just ignore all the rest of the code and focus on this:</p>

<pre><code>from nltk.stem import WordNetLemmatizer

lem = WordNetLemmatizer()
sentence = ['I', 'love', 'running', 'angrily']

lem.lemmatize(sentence)
</code></pre>

<p>[out]:</p>

<pre><code>-------------------------------------------------------------------------
TypeError                               Traceback (most recent call last)
&lt;ipython-input-34-497ae98ecaa3&gt; in &lt;module&gt;
      4 sentence = ['I', 'love', 'running', 'angrily']
      5 
----&gt; 6 lem.lemmatize(sentence)

~/Library/Python/3.6/lib/python/site-packages/nltk/stem/wordnet.py in lemmatize(self, word, pos)
     39 
     40     def lemmatize(self, word, pos=NOUN):
---&gt; 41         lemmas = wordnet._morphy(word, pos)
     42         return min(lemmas, key=len) if lemmas else word
     43 

~/Library/Python/3.6/lib/python/site-packages/nltk/corpus/reader/wordnet.py in _morphy(self, form, pos, check_exceptions)
   1903         # 0. Check the exception lists
   1904         if check_exceptions:
-&gt; 1905             if form in exceptions:
   1906                 return filter_forms([form] + exceptions[form])
   1907 

TypeError: unhashable type: 'list'
</code></pre>

<p><strong>Ah ha!! That's where the error occurs!!!</strong> </p>

<p>It's because <code>WordNetLemmatizer</code> is expecting a single string input and you're putting a list of strings. Example usage:</p>

<pre><code>from nltk.stem import WordNetLemmatizer

wnl = WordNetLemmatizer()
token = 'words'
wnl.lemmatize(token, pos='n')
</code></pre>

<p><strong>Q: Why didn't you just get to the point?!</strong></p>

<p>A: Then you would miss out on how to debug your code and make it better =)</p>
",""
"60190878","2020-02-12 14:44:22","0","","60151850","<p>What about replacing <code>#</code> with a digit?</p>

<p>In a first version of this answer I chose the digit <code>9</code>, because it reminds me of the COBOL numeric field formats I used some 30 years ago... But then I had a look at the dataset, and realized that for proper NLP processing one should get at least a couple of things straight:</p>

<ul>
<li>ordinal numerals (1st, 2nd, ...)</li>
<li>dates</li>
</ul>

<p>Ordinal numerals need special handling for any choice of digit, but the digit <code>1</code> produces reasonable dates, except for the year (of course, 1111 may or may not be interpreted as a valid year, but let's play it safe). <code>11/11/2020</code> is clearly better than <code>99/99/9999</code>...</p>

<p>Here is the code:</p>

<pre class=""lang-python prettyprint-override""><code>import re

ic = re.IGNORECASE
subs = [
    (re.compile(r'\b1(nd)\b', flags=ic), r'2\1'),  # 1nd -&gt; 2nd
    (re.compile(r'\b1(rd)\b', flags=ic), r'3\1'),  # 1rd -&gt; 3rd
    (re.compile(r'\b1(th)\b', flags=ic), r'4\1'),  # 1th -&gt; 4th
    (re.compile(r'11(st)\b', flags=ic), r'21\1'),  # ...11st -&gt; ...21st
    (re.compile(r'11(nd)\b', flags=ic), r'22\1'),  # ...11nd -&gt; ...22nd
    (re.compile(r'11(rd)\b', flags=ic), r'23\1'),  # ...11rd -&gt; ...23rd
    (re.compile(r'\b1111\b'), '2020')              # 1111 -&gt; 2020
]

text = '''spain 's colonial posts #.## billion euro loss
#nd, #rd, #th, ##st, ##nd, ##RD, ##TH, ###st, ###nd, ###rd, ###th.
ID=#nd#### year=#### OK'''

text = text.replace('#', '1')
for pattern, repl in subs:
    text = re.sub(pattern, repl, text)

print(text)
# spain 's colonial posts 1.11 billion euro loss
# 2nd, 3rd, 4th, 21st, 22nd, 23RD, 11TH, 121st, 122nd, 123rd, 111th.
# ID=1nd1111 year=2020 OK
</code></pre>

<p>If the preprocessing of the corpus converts any digit into a <code>#</code> anyway, you lose no information with this transformation. Some ‚Äútrue‚Äù <code>#</code> would become a <code>1</code>, but this would probably be a minor problem compared to numbers not being recognized as such. Furthermore, in a visual inspection of about 500000 lines of the dataset I haven't been able to find any candidate for a ‚Äútrue‚Äù <code>#</code>.</p>

<p>N.B.: The <code>\b</code> in the above regular expressions stands for ‚Äúword boundary‚Äù, i.e., the boundary between a <code>\w</code> (word) and a <code>\W</code> (non-word) character, where a word character is any alphanumeric character (further info <a href=""https://www.regular-expressions.info/wordboundaries.html"" rel=""nofollow noreferrer"">here</a>). The <code>\1</code> in the replacement stands for the first group, i.e., the first pair of parentheses (further info <a href=""https://www.regular-expressions.info/replacebackref.html"" rel=""nofollow noreferrer"">here</a>). Using <code>\1</code> the case of all text is preserved, which would not be possible with replacement strings like <code>2nd</code>. I later found that your dataset is normalized to all lower case, but I decided to keep it generic.</p>

<p>If you need to get the text with <code>#</code>s back from the parts of speech, it's simply</p>

<pre class=""lang-python prettyprint-override""><code>token.text.replace('0','#').replace('1','#').replace('2','#').replace('3','#').replace('4','#')
</code></pre>
",""
"60167755","2020-02-11 11:24:36","2","","60166480","<pre><code>for line in lines:
    tokens = nltk.word_tokenize(line)
    tagged = nltk.pos_tag(tokens)
    selective_tagged = ['NN','NNS','NNP','NNPS','VB','VBD','VBN','VBP','VBZ']
    selective_tagged_words = []
    for word, tag in tagged:
        if tag in selective_tagged:
            selective_tagged_words.append((word, tag))
    count = Counter(tag for word, tag in selective_tagged_words)

    other_tags = set(selective_tagged)-set(count)
    for i in other_tags:
        count[i]=0
    postag.append(count)
print(postag)
</code></pre>

<p>try if this works</p>
",""
"59912361","2020-01-25 18:43:11","1","","59911279","<p>In short, you cannot compute a tf-idf value for each feature, isolated from its document context, because each tf-idf value for a feature is specific to a document.</p>

<p>More specifically:  </p>

<ul>
<li>(inverse) document frequency is one value per feature, so indexed by $j$</li>
<li>term frequency is one value per term per document, so indexed by $ij$</li>
<li>tf-idf is therefore indexed by $i,j$</li>
</ul>

<p>You can see this in your example:</p>

<pre><code>&gt; tweets_dfm %&gt;% 
+   dfm_tfidf() %&gt;%
+   dfm_select(TagSet$emoticon) %&gt;% # only leave emoticons in the dfm
+   as.matrix()
        features
docs     \U0001f914 \U0001f4aa \U0001f603 \U0001f953 \U0001f37a
  text1     1.39794          1          0          0          0
  text2     0.00000          0          1          0          0
  text3     0.00000          0          0          0          0
  text4     0.00000          0          0          0          0
  text5     0.00000          0          0          0          0
  text6     0.69897          0          0          0          0
  text7     0.00000          0          0          1          1
  text8     0.00000          0          0          0          0
  text9     0.00000          0          0          0          0
  text10    0.00000          0          0          0          0
</code></pre>

<p>Two more things:</p>

<ol>
<li><p>Averaging by features is not really something that makes sense given the inverse document frequency's characteristic of already being a type of average, or at least the inverse proportion of documents in which a term occurs.  Furthermore, this is usually logged so would require some transformation before you could average it.</p></li>
<li><p>Above, I computed the tf-idf <em>before</em> removing the other features, since this will redefine term frequency if you use relative (""normalized"") term frequencies.  <code>dfm_tfidf()</code> uses term counts by default, so the results here are unaffected.</p></li>
</ol>
",""
"59854745","2020-01-22 07:34:19","0","","46059280","<p>I'd like to flesh @Quantum's answer out into a detailed one as follows: </p>

<p>Before 2014 many parsers were depending on a manually designed set of feature templates, and such methods have two drawbacks: 1) they required a lot of expertise and are usually incomplete; 2) most of the runtime is consumed by the feature extraction part of the configuration stage. After Chen and Mannning published their paper, <a href=""https://nlp.stanford.edu/pubs/emnlp2014-depparser.pdf"" rel=""nofollow noreferrer"">A Fast and Accurate Dependency Parser using Neural Networks</a>, almost all parsers are relying on neural networks. </p>

<p>Let's see how Chen and Manning did the job. </p>

<p><a href=""https://i.sstatic.net/q5Pba.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/q5Pba.png"" alt=""enter image description here""></a></p>

<p>As illustrated in the above diagram, the output of the neural network is a distribution after a softmax function, then it is a simple classification problem depending on some given information. The given information contains mainly three parts: the top 3 words on the stack and buffer, and the two leftmost/rightmost children of the top two words on the stack, and the leftmost and rightmost grandchildren; the POS tags of the above; and the arc labels of all children/grandchildren. </p>

<p>The inputs are embedded into a matrix and transformed by two matrices(and as shown in the picture a cube function) to become the logits and then the distribution of three elements atop of the network. </p>

<p>HTH :)</p>

<p>References: 1) <a href=""https://nlp.stanford.edu/pubs/emnlp2014-depparser.pdf"" rel=""nofollow noreferrer"">A Fast and Accurate Dependency Parser using Neural Networks</a>, 2) <a href=""https://youtu.be/7rp2c7JVymE"" rel=""nofollow noreferrer"">CMU Neural Nets for NLP 2017 (12): Transition-based Dependency Parsing</a></p>
",""
"59660764","2020-01-09 09:24:22","0","","59649783","<p>BLEU score is <strong>always computed on complete tokens</strong>, otherwise, the BLEU scores would not be comparable across models with different word segmentation. Even small differences in tokenization can make a big difference in the final score. This is well-explained in <a href=""https://www.aclweb.org/anthology/W18-6319.pdf"" rel=""nofollow noreferrer"">a recent paper</a> that introduces <a href=""https://github.com/mjpost/sacreBLEU"" rel=""nofollow noreferrer"">SacreBLEU</a> which is now used as a standard tool used for reporting BLEU scores in academic papers.</p>

<p>When computing BLEU on BPE subwords instead of words, the score would become artificially high. Even if the translation quality is quite low, the models usually don't have problems with getting single words correct. Normally, it would be included only in the unigram precision, but with words split into multiple subwords, it would also increase bigram, trigram and perhaps also 4-gram precision.</p>
",""
"59580852","2020-01-03 15:04:02","0","","59050554","<p>This was asked and answered in <a href=""https://github.com/explosion/spaCy/issues/4723"" rel=""nofollow noreferrer"">the following issue</a> on spaCy's GitHub. </p>

<p>It looks like the script no longer worked after a refactor of the entity linking pipeline as it now expects either a statistical or rule-based NER component in the pipeline. </p>

<p><a href=""https://github.com/explosion/spaCy/tree/master/examples/training/train_entity_linker.py"" rel=""nofollow noreferrer"">The new script</a> adds such an <code>EntityRuler</code> to the pipeline as an example. I.e., </p>

<pre class=""lang-py prettyprint-override""><code># Add a custom component to recognize ""Russ Cochran"" as an entity for the example training data.
# Note that in a realistic application, an actual NER algorithm should be used instead.
ruler = EntityRuler(nlp)
patterns = [{""label"": ""PERSON"", ""pattern"": [{""LOWER"": ""russ""}, {""LOWER"": ""cochran""}]}]
ruler.add_patterns(patterns)
nlp.add_pipe(ruler)
</code></pre>

<p>However, this can be replaced with your own statistical NER model.</p>
",""
"59567858","2020-01-02 17:56:20","1","","59567357","<p>You were close on your function! since you are using <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.apply.html"" rel=""nofollow noreferrer""><code>apply</code></a> on the series, you don't need to specifically call out the column in the function. you also are not using the input <code>text</code> at all in your function. So change</p>

<pre><code>def lemmatize_text(text):
    return [lemmatizer.lemmatize(w) for w in df1[""comments_tokenized""]]
</code></pre>

<p>to </p>

<pre><code>def lemmatize_text(text):
    lemmatizer = WordNetLemmatizer()
    return [lemmatizer.lemmatize(w) for w in text]  ##Notice the use of text.
</code></pre>

<p>An example:</p>

<pre><code>df = pd.DataFrame({'A':[[""cats"",""cacti"",""geese"",""rocks""]]})
                             A
0  [cats, cacti, geese, rocks]

def lemmatize_text(text):
    lemmatizer = WordNetLemmatizer()
    return [lemmatizer.lemmatize(w) for w in text]

df['A'].apply(lemmatize_text)

0    [cat, cactus, goose, rock]
</code></pre>
",""
"59534995","2019-12-30 18:05:07","0","","59503113","<p>To analyze the dependency tree, you need to look at both the dependency relation (<code>token.dep_</code>) and the head (token <code>token.head</code> with token index <code>token.head.i</code>).</p>

<pre class=""lang-py prettyprint-override""><code>doc = nlp(""Mary loves every man"")
for token in doc:
    print(token.text, token.dep_, token.head.text, token.head.i)
</code></pre>

<p>Output:</p>

<pre class=""lang-none prettyprint-override""><code>Mary nsubj loves 1
loves ROOT loves 1
every det man 3
man dobj loves 1
</code></pre>

<p>Spacy has a built-in dependency tree visualizer, too:</p>

<pre class=""lang-py prettyprint-override""><code>spacy.displacy.serve(doc)
</code></pre>

<p><a href=""https://i.sstatic.net/bb1bS.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bb1bS.png"" alt=""dependency tree for &quot;Mary loves every man&quot;""></a></p>
",""
"59507255","2019-12-28 00:34:41","0","","59505444","<p>First you'd want to make a choice between <strong>Stems</strong> and <strong>Lemmas</strong> (neither are <strong>Roots</strong>, mind you). Google the difference for more on that. </p>

<p>You mention antonyms, but most are determined by prefix (e.g. important vs (un)important). So the Stemmer should leave most antonyms unchanged. </p>

<p>As for synonyms, let's assume you're thinking only about words with the exact same Stem, because if you want to relate synonyms with completely unrelated roots, you'd be thinking about semantics and something like <strong>wordnet</strong> but that would likely complicate your problem beyond reasonable...</p>

<p>From your question, you already have a Stemmer working in Python...The simplest solution would be using two dictionaries: One dictionary mapping stems/lemmas to the set/list of inflected/derived complete words (and/or their frequency). And a second dictionary mapping those complete words to their various positions in the documents you are indexing.</p>

<p>That way you can stem the user input word, check for it in the top-k <code>tf-idf</code>/stem dictionary, and afterwards map the complete word with the second dictionary to its occurrences in the document set.</p>

<p>(It's hard to elaborate further given your question.)</p>
",""
"59407187","2019-12-19 09:55:53","1","","46924452","<p>This type of repetition is called a <strong>""text degeneration""</strong>.</p>

<p>There is a great paper from 2019 which analyse this phenomenon: <strong><a href=""https://arxiv.org/abs/1904.09751"" rel=""noreferrer"">The Curious Case of Neural Text Degeneration</a></strong> by <strong>Ari Holtzman</strong> et al. from the Allen Institute for Artificial Intelligence.</p>

<p>The repetition may come from the type of text search (text sampling) on the decoder site. Many people implement this just by the most probable next world proposed by the model (argmax on the softmax on the last layer) or by so called beam search. In fact the beam search is the industry standard for today.</p>

<p>This is the example of Beam search from the article:</p>

<p><strong>Continuation (BeamSearch, b=10):</strong></p>

<p><em>""The unicorns were able to communicate with each other, they said unicorns. a statement that the unicorns. Professor of the Department of Los Angeles, the most important place the world to be recognition <strong>of the world to be a of the world to be a of the world to be a of the world to be a of the world to be a of the world to be a of the world to be a of the world to be a of the‚Ä¶</em></strong></p>

<p>As you can see there is a great amount of repetition.</p>

<p>According to the paper this curious case may be explained by the fact that each repeated sequence of words have higher probability than the sequence without the next repetition:
<a href=""https://i.sstatic.net/I0iUG.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/I0iUG.png"" alt=""enter image description here""></a></p>

<p>The article propose some workarounds with words sampling by the decoder. It definitely requires more study, but this is the best explanation we have today.</p>

<p>The other is that your model need still more training. In many cases I faced a similar behaviour when I had big training set and model still couldn't generalise well over whole diversity of the data. To test this hypothesis - try to train on smaller dataset and see if it generalise (produce meaningful results). </p>

<p>But even if your model generalise well enough, that doesn't mean you won't ever face the repetition pattern. Unless you change the sampling patter of the decoder, it is a common scenario.</p>
",""
"59374288","2019-12-17 12:30:08","0","","59373151","<p>The current <code>pt</code> version in spaCy is actually supposed to cover both. See for instance this PR <a href=""https://github.com/explosion/spaCy/pull/2790"" rel=""nofollow noreferrer"">https://github.com/explosion/spaCy/pull/2790</a> which specifically caters for the currency symbol from Brazil, and this PR <a href=""https://github.com/explosion/spaCy/pull/2307"" rel=""nofollow noreferrer"">https://github.com/explosion/spaCy/pull/2307</a> where both versions of both spelling variants of numbers are specifically taken into account.</p>
",""
"59362941","2019-12-16 19:13:46","0","","59347811","<p>Lists of items with periods is the way I've seen LS items. Here's an example:</p>

<pre class=""lang-py prettyprint-override""><code>import spacy
nlp=spacy.load('en_core_web_sm')
doc= nlp('''The system shall:
1) Reprogram software.
2) Reprogram data.
3) Read/write to memory.
4) Lock/unlock flash memory.
5) Clear memory.
6) Range check.''')
print('\n....')

print(""Token Attributes: \n"", ""token.text, token.pos_, token.tag_, token.dep_, token.orth_"")
for token in doc:
    # Print the text and the predicted part-of-speech tag
    print(""{:&lt;12}{:&lt;12}{:&lt;12}{:&lt;12}{:&lt;12}"".format(token.text, token.pos_, token.tag_, token.dep_, token.orth_))
</code></pre>

<p>Outputs:</p>

<pre><code>....
Token Attributes: 
 token.text, token.pos_, token.tag_, token.dep_, token.orth_
The         DET         DT          det         The         
system      NOUN        NN          nsubj       system      
shall       VERB        MD          ROOT        shall       
:           PUNCT       :           punct       :           

           SPACE       _SP                     

1           PUNCT       LS          dep         1           
)           PUNCT       -RRB-       punct       )           
Reprogram   PROPN       NNP         compound    Reprogram   
software    NOUN        NN          dobj        software    
.           PUNCT       .           punct       .           

           SPACE       _SP                     

2           PUNCT       LS          dobj        2           
)           PUNCT       -RRB-       punct       )           
Reprogram   PROPN       NNP         compound    Reprogram   
data        NOUN        NNS         dobj        data        
.           PUNCT       .           punct       .           

           SPACE       _SP                     

3           PUNCT       LS          ROOT        3           
)           PUNCT       -RRB-       punct       )           
Read        VERB        VB          dep         Read        
/           SYM         SYM         punct       /           
write       VERB        VBP         ROOT        write       
to          ADP         IN          prep        to          
memory      NOUN        NN          pobj        memory      
.           PUNCT       .           punct       .           

           SPACE       _SP                     

4           PUNCT       LS          ROOT        4           
)           PUNCT       -RRB-       punct       )           
Lock        PROPN       NNP         npadvmod    Lock        
/           SYM         SYM         punct       /           
unlock      ADJ         JJ          compound    unlock      
flash       NOUN        NN          compound    flash       
memory      NOUN        NN          ROOT        memory      
.           PUNCT       .           punct       .           

           SPACE       _SP                     

5           PUNCT       LS          nummod      5           
)           PUNCT       -RRB-       punct       )           
Clear       ADJ         JJ          amod        Clear       
memory      NOUN        NN          ROOT        memory      
.           PUNCT       .           punct       .           

           SPACE       _SP                     

6           PUNCT       LS          nummod      6           
)           PUNCT       -RRB-       punct       )           
Range       NOUN        NN          compound    Range       
check       NOUN        NN          ROOT        check       
.           PUNCT       .           punct       .           
....

</code></pre>
",""
"59355742","2019-12-16 11:28:42","0","","59316859","<p>As said by Sofie Van Landeghem(Spacy Entity Linking Representative).
The descriptions are currently not stored in the KB itself because of performance reasons. However, from the intermediary results during processing, you should have a file entity_descriptions.csv that maps the WikiData ID to its description in a simple tabular format.</p>
",""
"59281516","2019-12-11 08:25:50","0","","59281409","<p>The error message is misleading ‚Äì it occurs when there's nothing to properly lemmatize.</p>

<p>By default, <code>lemmatize()</code> only accepts word tags <code>NN|VB|JJ|RB</code>. Pass in a regexp that matches any string to change this:</p>

<pre><code>&gt;&gt;&gt; import re
&gt;&gt;&gt; lemmatize(""gone"", allowed_tags=re.compile('.*'))
[b'go/VB']
</code></pre>
",""
"59266806","2019-12-10 12:04:08","2","","59266675","<p>Do you know NER? It means named entity recognition. You can preprocess your text and locate all named entities, which you then exclude from stemming. After stemming, you can merge the data again. </p>
",""
"59237359","2019-12-08 16:18:26","0","","59237194","<p>I advice tou to train the tagger only on lowercase words, so you won't encouter problems like the one you said about the word 'berlin' and 'Berlin'.</p>

<p>I don't know how <code>PerceptronTagger</code> work in detail, but i suggest you this. In order to retrain a model when you have new data you have to <strong>repeat the training process allover again</strong> on the ""updated"" data.</p>

<p>Otherwise, if the tagger has some kind of internal dictionary, maby a <code>dict</code> that associates a word with the relative tag, you can try to update it with your new data, in this case ""&lt;'resturant'>&lt;'NN'></p>
",""
"59217061","2019-12-06 16:38:35","1","","59183624","<p>I think that computing TfIdf could not be necessary, if you can use word embeddings.</p>

<p>A simple but effective method consists in:</p>

<ol>
<li><p>Compute two vectors which represent your two strings, using pretrained word embeddings for your language (eg FastText - get_sentence_vector <a href=""https://fasttext.cc/docs/en/python-module.html#model-object"" rel=""noreferrer"">https://fasttext.cc/docs/en/python-module.html#model-object</a>) </p></li>
<li><p>Compute cosine similarity between two vectors (1: equal strings; 0: really
different strings; read <a href=""https://masongallo.github.io/machine/learning,/python/2016/07/29/cosine-similarity.html"" rel=""noreferrer"">https://masongallo.github.io/machine/learning,/python/2016/07/29/cosine-similarity.html</a>).</p></li>
</ol>
",""
"59174318","2019-12-04 10:53:50","9","","59173175","<p>This is a very verbose parser for the format(s) you provided. Output is given as a list of [year, month, day], where each entry is only present if found in the date.</p>

<pre><code>import datetime
dates = ['YEAR:1999        DATE:09/1999',
         'DATE:09/1996',
         'DATE:1993 ',
         'YEAR:2006   DATE:15/05/06 ',
         'YEAR:2019 DATE:JANUARY 3, 2019',
         'YEAR:2019 DATE:FEB. 14, 2019 ',
         'YEAR:2019 DATE: 30/06/2019']
output = []
for date in dates:
    year = None
    # getting the year from the 'YEAR:' key.
    if 'YEAR' in date:
        year = int(date.split(' ',1)[0].replace('YEAR:','').strip())
        date = date.split(' ',1)[1].strip()
    #Some string cleaning
    date = date.replace('DATE:','')
    date = date.replace('/',' ').strip().replace(',',' ')
    date = date.split()
    if year is None:
        year = int(date[-1])
    date = date[0:-1]
    if len(date)==0:
        output.append([year])
        continue
    elif len(date)==1:
        month = int(date[0])
        output.append([year, month])
        continue
    else:
        try:
            day = int(date[0])
            month = int(date[1])
            output.append([year, month, day])
        except ValueError:
            day = int(date[1])
            #Getting month number from name
            month = datetime.datetime.strptime(date[0][0:3], '%b').month
            output.append([year, month, day])

print(output)
</code></pre>

<p><strong>Update</strong>:</p>

<p>It is possible to get somewhere with dateparser. For your input the code looks like:</p>

<pre><code>import dateparser
dates = ['YEAR:1999        DATE:09/1999',
         'DATE:09/1996',
         'DATE:1993 ',
         'YEAR:2006   DATE:15/05/06 ',
         'YEAR:2019 DATE:JANUARY 3, 2019',
         'YEAR:2019 DATE:FEB. 14, 2019 ',
         'YEAR:2019 DATE: 30/06/2019']

for date in dates:
    if 'YEAR' in date:
        date = date.split(' ',1)[1].strip()
    date = date.replace('DATE:','').strip()
    parsed_date = dateparser.parse(date, 
                                   date_formats=['%m/%Y', '%Y', '%d/%m/%Y', ],
                                   languages = ['en'])
    print(parsed_date)
</code></pre>

<p>but as you see, there will be a month, day, and a time added, which is not given in the input. </p>
",""
"59042791","2019-11-26 02:20:07","9","","58956995","<p>YOUR CODE</p>

<pre><code>def tokenize_and_stem(text):

tokens = [sent for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(text)]

filtered_tokens = [token for token in tokens if re.search('[a-zA-Z]', token)]

stems = stemmer.stem(filtered_tokens)

words_stemmed = tokenize_and_stem(""Today (May 19, 2016) is his only daughter's 
wedding."")
print(words_stemmed)
</code></pre>

<p>The error says """"""word = word.lower()... if word in self.stopwords or len(word) &lt;= 2: list object has no attribute 'lower'""""""</p>

<p>The error is not only because of .lower() but because of the length
If you try to run it with out changing the <strong>filtered_tokens</strong> on the 5th line, 
without changing means using yours.
you will get no error but the output will be like this:</p>

<p><strong>[""today (may 19, 2016) is his only daughter's wedding."", ""today (may 19, 2016) is his only daughter's wedding."", ""today (may 19, 2016) is his only daughter's wedding."", ""today (may 19, 2016) is his only daughter's wedding."", ""today (may 19, 2016) is his only daughter's wedding."", ""today (may 19, 2016) is his only daughter's wedding."", ""today (may 19, 2016) is his only daughter's wedding."", ""today (may 19, 2016) is his only daughter's wedding."", ""today (may 19, 2016) is his only daughter's wedding."", ""today (may 19, 2016) is his only daughter's wedding."", ""today (may 19, 2016) is his only daughter's wedding."", ""today (may 19, 2016) is his only daughter's wedding."", ""today (may 19, 2016) is his only daughter's wedding."", ""today (may 19, 2016) is his only daughter's wedding.""]</strong></p>

<p>Here is your fixed code.</p>

<pre><code>def tokenize_and_stem(text):

    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]

    filtered_tokens = [token for token in tokens if re.search('[a-zA-Z]', token)]

    stems = [stemmer.stem(t) for t in filtered_tokens if len(t) &gt; 0]

    return stems

words_stemmed = tokenize_and_stem(""Today (May 19, 2016) is his only daughter's wedding."")
print(words_stemmed)
</code></pre>

<p>So, i have only <strong>changed line 3 and line 7</strong> </p>
",""
"58980227","2019-11-21 17:09:06","2","","58971014","<p>Spacy's provided models don't use UD dependencies for English or German. From the docs, where you can find tables for the dependency labels (<a href=""https://spacy.io/api/annotation#dependency-parsing"" rel=""nofollow noreferrer"">https://spacy.io/api/annotation#dependency-parsing</a>):</p>

<blockquote>
  <p>The individual labels are language-specific and depend on the training corpus.</p>
</blockquote>

<p>For most other models / languages, UD dependencies are used.</p>
",""
"58941391","2019-11-19 19:17:42","0","","58941049","<p>It seems the following packages are missing.</p>

<ol>
<li>punkt</li>
<li>averaged_perceptron_tagger</li>
</ol>

<p>Note: You need to download them for the first time.</p>

<p>Try this..</p>

<pre><code>import nltk

nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

from nltk.tokenize import word_tokenize
text = word_tokenize(""And now for something completely different"")
print(nltk.pos_tag(text))

text = word_tokenize(""They refuse to permit us to obtain the refuse permit"")
print(nltk.pos_tag(text))

print(""----End of execution----"")
</code></pre>

<p><a href=""https://repl.it/repls/MisguidedNoisyDecagons"" rel=""nofollow noreferrer"">Try this on IDE</a></p>
",""
"58839889","2019-11-13 15:00:33","4","","58839334","<p>I checked your code and I have a solution to your problem. The calculation of the Confusion Matrix works as well as possible. The problem is that your network is not learning at all and it classifies all data to 0. You can verify this by setting the verbose argument to 1 in the fit function and then you can observe an accuracy of about 10%, which is equivalent to random guessing.</p>

<pre><code>model.fit(X_train, Y_train, epochs=100, batch_size=32, validation_data=(X_train, Y_train), verbose=1)
</code></pre>

<p>It's because you don't normalize your data. All you have to do is to divide your dataset by 255 so that the number values are in the range [0; 1] and then everything is working properly and your network is learning.</p>

<pre><code>X_train = X.reshape((-1, 28, 28, 1))
X_train = X_train / 255.0
Y_train = to_categorical(Y)
</code></pre>

<p>The same thing you should do with your test set.</p>
",""
"58822726","2019-11-12 16:22:22","3","","58822292","<p>It seems I was too impatient. I ran the streaming-function written above which processes only one document instead of a batch:</p>

<pre class=""lang-py prettyprint-override""><code>def __iter__(self) -&gt; list:
    """"""
    Iterator-wrapper for generator-functionality (since generators cannot be used directly.
    Allows for data-streaming.
    """"""
    for text in self.data:
        yield text.split("" "")
</code></pre>

<p>After starting the <code>w2v</code>-function it took about ten minutes until all cores were working correctly. </p>

<p>It seems that building the vocabulary does not support multiple cores and, hence, only one was used for this task. Presumably, it took so long because auf the corpus-size. After gensim built the vocab, all cores were used for the training.</p>

<p>So if you are running in this issue as well, maybe some patience will already help :)</p>
",""
"58800887","2019-11-11 11:55:30","4","","58800527","<p>Simply apply lemmatize to each line, one-by-one, and then append that to a string with a new line. So essentially, it's doing the same thing. Except doing each line, appending it to a temp string and seperating each by a new line, then at the end we print out temp string. You can use the temp string at the end as final output.</p>

<pre><code>my_temp_string = """"
with open ('a.txt',""r+"", encoding=""utf-8"") as fin:
    for line in fin:
        lemm = lemmatize_sentence(line)
        my_temp_string += f'{lemm} \n'
print (my_temp_string)
</code></pre>
",""
"58787086","2019-11-10 08:59:04","0","","58779371","<p>This is due to a change from v2.1 to v2.2 to move the large lookup tables out of the main library. The lemmatizer data is now stored in the separate package <code>spacy-lookups-data</code> and the <code>Lemmatizer</code> is initialized with a <code>Lookups</code> object instead of the individual variables. See the second section here about initializing lemmatizers: <a href=""https://spacy.io/usage/v2-2#migrating"" rel=""noreferrer"">https://spacy.io/usage/v2-2#migrating</a></p>

<p>If you install the package <code>spacy-lookups-data</code>, you can access the default English lemmatizer like this:</p>

<pre><code>from spacy.lang.en import English
lemmatizer = English.Defaults.create_lemmatizer()
</code></pre>

<p>It automatically loads the data from <code>spacy-lookups-data</code> if it's available. If it's not available, the lemmas will be the same as the tokens from the text.</p>

<p>If you use an English model like <code>en_core_web_sm</code>, the lookup tables are included with the model, so you don't need the additional package <code>spacy-lookups-data</code>:</p>

<pre><code>import spacy
nlp = spacy.load('en_core_web_sm')
lemmatizer = nlp.Defaults.create_lemmatizer()
</code></pre>
",""
"58709998","2019-11-05 11:03:57","3","","58698428","<p>POS taggers normally use Hidden Markov Models. If your data is not tagged correctly with these methods, then either your tagger (selfmade?) is not suited for your input data or your training data is not adequate (too small, false annotations etc.). Various means I assume to be taggers from NLTK, spaCy, or tools from Stanford (<a href=""https://nlp.stanford.edu/software/"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/software/</a>). These software packages will do the job in the quality of current research, so if it is still error-prone, you won't be able to fix it.
If you have a large cluster at hand, build your own tagger using n-grams with n > 3, if you like, but I doubt this will be any better than the modules named above.</p>
",""
"58627653","2019-10-30 14:30:23","2","","58625071","<p>You need to add your <code>tag_map.py</code> to <code>spacy/lang/hi/</code> and tell the default model (which is what gets loaded with <code>spacy train hi</code>) to load it. It sounds like you already have a <code>tag_map.py</code>, but if not, you can see examples for any of the languages that have provided spacy models, like:</p>

<p><a href=""https://github.com/explosion/spaCy/blob/master/spacy/lang/en/tag_map.py"" rel=""nofollow noreferrer"">https://github.com/explosion/spaCy/blob/master/spacy/lang/en/tag_map.py</a></p>

<p>Import the tag map and add it to the <code>HindiDefaults</code> in <code>spacy/lang/hi/__init__.py</code> to load the tag map:</p>

<pre><code>from .tag_map import TAG_MAP

class HindiDefaults(Language.Defaults):
    tag_map = TAG_MAP
</code></pre>

<p>I think you could also modify the tag map in <code>nlp.vocab.morphology.tag_map</code> on-the-fly after initializing the blank model before you starting training, but I don't think there's any easy way to do it with command-line options to <code>spacy train</code>, so that would require a custom training script.</p>

<p>You can use <code>spacy debug-data hi train.json dev.json</code> to make sure the settings worked, since it will show warnings for any tags in your training data that aren't in the tag map.</p>
",""
"58618741","2019-10-30 04:24:13","4","","58618352","<p>In your code above you are trying to lemmatize words that have been stemmed.  When the lemmatizer runs into a word that it doesn't recognize, it'll simply return that word.  For instance stemming <code>offline</code> produces <code>offlin</code> and when you run that through the lemmatizer it just gives back the same word, <code>offlin</code>.  </p>

<p>Your code should be modified to lemmatize <code>words</code>, like this...</p>

<pre><code>def lemma_list(row):
    my_list = row['words']  # Note: line that is changed
    lemma_list = [lemma.lemmatize(word, pos='v') for word in my_list]
    return (lemma_list)
df['lemma_words'] = df.apply(lemma_list, axis=1)
print('Words: ',  df.ix[0,'words'])
print('Stems: ',  df.ix[0,'stemmed_words'])
print('Lemmas: ', df.ix[0,'lemma_words'])
</code></pre>

<p>This produces...</p>

<pre><code>Words:  ['and', 'those', 'kept', 'offline', 'were', 'not', 'stolen']
Stems:  ['and', 'those', 'kept', 'offlin',  'were', 'not', 'stolen']
Lemmas: ['and', 'those', 'keep', 'offline', 'be',   'not', 'steal']
</code></pre>

<p>Which is is correct.</p>
",""
"58522190","2019-10-23 11:48:11","0","","58506951","<p>In your sentence above, 'I' is a pronoun. The <a href=""http://%20Why%20is%20WordNet%20missing:%20of,%20an,%20the,%20and,%20about,%20above,%20because,%20etc."" rel=""nofollow noreferrer"">wordnet FAQ</a> states that:</p>

<blockquote>
  <p>Q: Why is WordNet missing: of, an, the, and, about, above, because, etc.   </p>
  
  <p>A: WordNet only contains ""open-class words"": nouns, verbs, adjectives, and adverbs. Thus, excluded words include determiners, prepositions, pronouns, conjunctions, and particles.</p>
</blockquote>
",""
"58516624","2019-10-23 06:22:28","0","","58475716","<p>Apparently, the issue lies with the parallel backend of sklearn, which uses <code>'loky'</code> by default. Changing the backend to <code>'multiprocessing'</code> solves this problem. As mentioned here <a href=""https://github.com/explosion/spaCy/issues/3193"" rel=""nofollow noreferrer"">here</a>.</p>

<p>More info on sklearn parallel backend can be found <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.utils.parallel_backend.html"" rel=""nofollow noreferrer"">here</a>.</p>

<p>First, import this: </p>

<pre><code>from sklearn.externals.joblib import parallel_backend
</code></pre>

<p>When running the fit, do this to overwrite the parallel backend:</p>

<pre><code>with parallel_backend('multiprocessing'):
  random_search.fit(X_train, y_train)
</code></pre>
",""
"58497915","2019-10-22 06:01:43","0","","58295677","<p>By:
<a href=""https://codereview.stackexchange.com/users/25834/reinderien"">https://codereview.stackexchange.com/users/25834/reinderien</a></p>

<p>On: <a href=""https://codereview.stackexchange.com/questions/230393/tokenizing-sgml-text-for-nltk-analysis"">https://codereview.stackexchange.com/questions/230393/tokenizing-sgml-text-for-nltk-analysis</a></p>

<h2>Regex compilation</h2>

<p>If performance is a concern, this:</p>

<pre><code>arr = [re.sub(pattern, '', i) for i in arr]
</code></pre>

<p>is a problem. You're re-compiling your regex on every function call - and every loop iteration! Instead, move the regex to a <code>re.compile()</code>d symbol outside of the function.</p>

<p>The same applies to <code>re.match(""&lt;P ID=(\d+)&gt;"", para)</code>. In other words, you should be issuing something like</p>

<pre><code>group_para_re = re.compile(r""&lt;P ID=(\d+)&gt;"")
</code></pre>

<p>outside of the loop, and then</p>

<pre><code>group_para_id = group_para_re.match(para)
</code></pre>

<p>inside the loop.</p>

<h2>Premature generator materialization</h2>

<p>That same line has another problem - you're forcing the return value to be a list. Looking at your <code>no_integers</code> usage, you just iterate over it again, so there's no value to holding onto the entire result in memory. Instead, keep it as a generator - replace your brackets with parentheses.</p>

<p>The same thing applies to <code>nopunctuation</code>.</p>

<h2>Set membership</h2>

<p><code>stop_words</code> should not be a <code>list</code> - it should be a <code>set</code>. Read about its performance <a href=""https://wiki.python.org/moin/TimeComplexity"" rel=""nofollow noreferrer"">here</a>. Lookup is average O(1), instead of O(n) for a list.</p>

<h2>Variable names</h2>

<p><code>nopunctuation</code> should be <code>no_punctuation</code>.</p>
",""
"58376369","2019-10-14 12:02:24","3","","58375251","<p><a href=""https://github.com/erlang-unicode/i18n"" rel=""nofollow noreferrer"">i18n</a> library should be usable for this. Just going from the examples provided, since I have no experience using it, something like the following should work (<code>:en</code> is the locale code):</p>

<pre><code>str = :i18n_string.from(""some string"")
iter = :i18n_iterator.open(:en, :sentence)
sentences = :i18n_string.split(iter, str)
</code></pre>

<p>There's also <a href=""https://github.com/elixir-cldr/cldr"" rel=""nofollow noreferrer"">Cldr</a>, which implements a lot of locale-dependent Unicode algorithms directly in Elixir, but it doesn't seem to include iteration in particular at the moment (you may want to raise an issue there).</p>
",""
"58349521","2019-10-11 23:27:57","2","","58349049","<p>I think Wordnet's lemmatizer defaults the part-of-speech to Noun, so you need to tell it you're lemmatizing a Verb.</p>

<pre><code>print(WordNetLemmatizer().lemmatize('loved', pos='v'))
love
print(WordNetLemmatizer().lemmatize('creating', pos='v'))
create
</code></pre>

<p>Any lemmatizer you use will need to know the part-of-speech so it knows what rules to apply.  While the two words you have are always verbs, lots of words can be both.  For instance, the word ""painting"" can be a noun or a verb.  The lemma of the verb ""painting"" (ie.. I am painting) is ""paint"". 
 If you use ""painting"" as a noun (ie.. A painting), ""painting"" is the lemma since it's the singular form of the noun.</p>

<p>In general, NLTK/Wordnet is not terribly accurate, especially for words not in its word list.  I was unhappy with the performance of the available lemmatizers so I created me own.  See <a href=""https://github.com/bjascob/LemmInflect"" rel=""nofollow noreferrer"">Lemminflect</a>.  The main Readme page also has a comparison of a few common lemmatizers if you don't want to use that one.</p>
",""
"58307999","2019-10-09 15:56:16","1","","58292167","<p>For the stanfordnlp python package, for all languages, the POS tag set used is the <a href=""https://universaldependencies.org/u/pos/all.html"" rel=""nofollow noreferrer"">Universal Dependencies (UD) v2 tag set</a>. Some UD corpora also include an original POS tag set, which is often more fine-grained. But while the Hebrew Treebank was originally built with its own POS tag set, and was then coverted to UD, it seems like the supplied version in the UD repository comes only with the UD tag set. Individual languages may use only a subset of the UD POS tag set. You can find details of that on the <a href=""https://universaldependencies.org/treebanks/he_htb/index.html"" rel=""nofollow noreferrer"">Treebank hub page for the Hebrew TreeBank</a>. You'll see there that 15 of the 17 UD POS tags are used.</p>
",""
"58295282","2019-10-08 23:45:36","2","","58294772","<p>You have created a brute force nearest neighbor algorithm using cosine distance as the metric. The <a href=""http://scikit-learn.org/stable/modules/neighbors.html"" rel=""nofollow noreferrer"">sklearn docs on this topic</a> are good.</p>

<p>Sklearn implements more optimized versions which should be faster. You can use them, but need to change your dictionaries. You'll want some way to map from a vector to the corresponding tweet. </p>

<pre><code>from sklearn.neighbors import NearestNeighbors
neigh = NearestNeighbors(1, metric='cosine')
neigh.fit(dictVects)  

nearest = neigh.kneighbors(taglessVects, return_distance=False)
for x, y in zip(taglessVects, nearest):
    z = y[x][0]
    # z is the nearest tweet vector to the tagless vector x
</code></pre>
",""
"58124810","2019-09-26 21:08:24","0","","58123369","<p>In Scikit-learn ( a very popular Python package for Machine Learning) has a Module that does exactly what you're asking:</p>

<p>Here's how to do it:</p>

<p>First install sklearn</p>

<pre><code>pip install scikit-learn
</code></pre>

<p>Now the code:</p>

<pre><code>from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer(ngram_range=(1, 3))

#Given your corpus is an iterable of strings, or a List of strings, for simplicity:
corpus = [...]

X = vectorizer.fit_transform(corpus)

print(X)
</code></pre>

<p>The output is a matrix of size m x n. E.g:</p>

<pre><code>[[0 1 1 1 0 0 1 0 1]
 [0 2 0 1 0 1 1 0 1]
 [1 0 0 1 1 0 1 1 1]
 [0 1 1 1 0 0 1 0 1]]
</code></pre>

<p><strong>Columns</strong> represent words, <strong>lines</strong> represent documents. So for each line, you have the resulting bag of words.</p>

<p>But how to retrieve which words appear where? You can get each ""column"" name, by using:</p>

<pre><code>print(vectorizer.get_feature_names())
</code></pre>

<p>You'll get a list of words (the words are organized alphabetically).</p>

<p>Now, suppose you want to know the number of times that each word appears in your corpus (not on a single document).</p>

<p>The matrix you receive as output is a ""numpy"" (another package) array. This can be easy flattened (sum all lines) by doing:</p>

<pre><code>import numpy as np #np is like a convention for numpy, if you don't know this already.

sum_of_all_words = np.sum(X, axis=0)
</code></pre>

<p>That'll give you something like:</p>

<pre><code>[[1 4 2 4 1 1 4 1 4]]
</code></pre>

<p>The column order is the same for the words.</p>

<p>Finally, you can filter the terms from your dictionary by doing:</p>

<pre><code>dict_terms = corpora.Dictionary(phrases)
counts = {}
words = vectorizer.get_feature_names()
for idx, word in enumerate(words):
   if word in dict_terms:
      counts[word] = sum_of_all_words[0, idx]


</code></pre>

<p>Hope this helps!</p>

<p>Read more about CountVectorizer here: <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer"" rel=""nofollow noreferrer"">https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer</a></p>

<p>(Also, give a look on TFIDFVectorizer, if you're using bag of words, tf-idf is a huge upgrade in most cases)</p>

<p>I also recommend you to take a look at this page for feature extraction with sklearn: <a href=""https://scikit-learn.org/stable/modules/feature_extraction.html"" rel=""nofollow noreferrer"">https://scikit-learn.org/stable/modules/feature_extraction.html</a></p>
",""
"58109352","2019-09-26 03:56:15","1","","58068992","<h1>Use below code for step 14-17</h1>
<pre><code>p_stemmed_diff=[]
for w1,w2 in zip(lc_humor_uniq_words,p_stemmed):
    if len(w1) == len(w2) and w1 != w2:
        p_stemmed_diff.append(w1)
l_stemmed_diff=[]
for w1,w2 in zip(lc_humor_uniq_words,l_stemmed):
    if len(w1) == len(w2) and w1 != w2:
        l_stemmed_diff.append(w1)
print(len(p_stemmed_diff))
print(len(l_stemmed_diff))
</code></pre>
",""
"58056286","2019-09-23 05:46:11","1","","58056275","<p>Use <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.replace.html"" rel=""nofollow noreferrer""><code>Series.str.replace</code></a> with <code>^</code> for start of string:</p>

<pre><code>df.columns = df.columns.str.replace('^000 ','')
</code></pre>

<p><strong>Sample</strong>:</p>

<pre><code>df = pd.DataFrame(columns=['000', '000 000', '000 3rd', '000 bank', 
                           '000 claim', '000 confirmed'])
print (df)
Empty DataFrame
Columns: [000, 000 000, 000 3rd, 000 bank, 000 claim, 000 confirmed]
Index: []

df.columns = df.columns.str.replace('^000 ','')
print (df)
Empty DataFrame
Columns: [000, 000, 3rd, bank, claim, confirmed]
Index: []
</code></pre>
",""
"57997729","2019-09-18 17:07:46","0","","57987235","<blockquote>
  <p>Without deep learning, could we invent an algorithm like BM25 to
  compute the relevance score of question-answer pair?</p>
</blockquote>

<p>Yes, there are many ways to do it. To make your question a little more directed, let's answer ""Which are the possible ways to compute the relevance of question-answer pair without using question answering?""</p>

<p>Some examples and explanations:</p>

<ul>
<li><p>TF-IDF [that you mentioned] is actually a feature extraction technique. With it, you retrieve which words from the context are present/important for each document - with this, you can compare two similarly worded (that's what BM25 does).</p></li>
<li><p>Another technique is to use <a href=""https://pt.wikipedia.org/wiki/PageRank"" rel=""nofollow noreferrer"">PageRank</a>, which is the algorithm used by Google. You can actually attempt to replicate it, since it is not too complex.</p></li>
<li><p>One other way is to use graphs to do it. I did it in my Masters research and you can read my dissertation <a href=""https://github.com/Sirsirious/dissertation/blob/master/Dissertacao_Tiago_Faceroli_Duque_Graph_Based_Approach_for_QA.pdf"" rel=""nofollow noreferrer"">here</a>.</p></li>
</ul>

<p>Aside from that, I'd advise you to check on this papers for other examples of Question-Answering (you can get to question-answer matching easily if you understand the concepts): <a href=""https://www.sciencedirect.com/science/article/pii/S0020025511003860"" rel=""nofollow noreferrer"">https://www.sciencedirect.com/science/article/pii/S0020025511003860</a> and <a href=""https://www.sciencedirect.com/science/article/pii/S1319157815000890?via%3Dihub"" rel=""nofollow noreferrer"">https://www.sciencedirect.com/science/article/pii/S1319157815000890?via%3Dihub</a>.</p>

<p>Also, keep checking <a href=""https://aclweb.org/aclwiki/Question_Answering_(State_of_the_art)"" rel=""nofollow noreferrer"">ACL State of the Art Question Answering Techniques</a> for the most updated results and techniques.</p>
",""
"57994696","2019-09-18 14:07:01","1","","57994144","<p>So I am assuming you have one column in the dataframe where each row is a list of tuples. Please correct me if I am wrong. From that column you want to create new columns for each 'Tag'. Do you think following is what will achieve what you want to do?</p>

<pre><code>import pandas as pd
import numpy as np

df = pd.DataFrame({""line"":[[('e-mail', 'JJ'), ('new', 'JJ'), ('delhi', 'NN')]]})

def extract_pos(line,pos):
    return [word[0] for word in line if word[1] == pos]

df['NN'] = [extract_pos(line,'NN') for line in df['line']]
df['JJ'] = [extract_pos(line,'JJ') for line in df['line']]
</code></pre>

<p>This way you can add many column as you want and the result might look as some thing like following.</p>

<p><a href=""https://i.sstatic.net/ES7kF.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ES7kF.png"" alt=""enter image description here""></a></p>

<p>Hope this helps,
Cheers</p>
",""
"57977458","2019-09-17 15:23:14","1","","57977061","<p><code>spacy</code> has a way of doing this but I'm not sure it is giving you exactly what you are after</p>

<pre><code>import spacy

text = """"""Chancellor Angela Merkel and some of her ministers will discuss
at a cabinet retreat next week ways to avert driving bans in
major cities after Germany's top administrative court
in February allowed local authorities to bar heavily
polluting diesel cars.
"""""".replace('\n', ' ')

nlp = spacy.load(""en_core_web_sm"")
doc = nlp(text)
print([i.text for i in doc.noun_chunks])
</code></pre>

<p>gives</p>

<pre><code>['Chancellor Angela Merkel', 'her ministers', 'a cabinet retreat', 'ways', 'driving bans', 'major cities', ""Germany's top administrative court"", 'February', 'local authorities', 'heavily polluting diesel cars']
</code></pre>

<p>Here, however the <code>i.lemma_</code> line doesn't really give you what you want (I think this might be fixed by <a href=""https://github.com/explosion/spaCy/pull/4219"" rel=""nofollow noreferrer"">this recent PR</a>).</p>

<p>Since it isn't quite what you are after you could use <a href=""https://docs.python.org/3/library/itertools.html#itertools.groupby"" rel=""nofollow noreferrer""><code>itertools.groupby</code></a> like so</p>

<pre><code>import itertools

out = []
for i, j in itertools.groupby(doc, key=lambda i: i.pos_):
    if i not in (""PROPN"", ""NOUN""):
        continue
    out.append(' '.join(k.lemma_ for k in j))
print(out)
</code></pre>

<p>gives</p>

<pre><code>['Chancellor Angela Merkel', 'minister', 'cabinet retreat', 'week way', 'ban', 'city', 'Germany', 'court', 'February', 'authority', 'diesel car']
</code></pre>

<p>This should give you exactly the same output as your function (the output is slightly different here but I believe this is due to different <code>spacy</code> versions).</p>

<p>If you are feeling really adventurous you could use a list comprehension</p>

<pre><code>out = [' '.join(k.lemma_ for k in j) 
       for i, j in itertools.groupby(doc, key=lambda i: i.pos_) 
       if i in (""PROPN"", ""NOUN"")]
</code></pre>

<p><em>Note I see slightly different results with different <code>spacy</code> versions. The output above is from <code>spacy-2.1.8</code></em></p>
",""
"57927922","2019-09-13 17:22:21","1","","57925219","<p>I think you misunderstood the process of POS Tagging and Lemmatization.</p>

<p><strong>POS Tagging</strong> is based on several other informations than the word alone (I don't know which is your mother language, but that is common to many languages), but also on the surrounding words (for example, one common learned rule is that in many statements verb is usually preceded by a noun, which represents the verb's agent).</p>

<p>When you pass all these 'tokens' to your lemmatizer, spacy's lemmatizer will try to ""guess"" which is the Part of Speech of your solitary word. </p>

<p>In many cases it'll go for a default noun and, if it is not in a lookup table for common and irregular nouns, it'll attempt to use generic rules (such as stripping plural 's').</p>

<p>In other cases it'll go for a <strong>default verb</strong> based on some patterns (the ""-ing"" in the end), which is probably your case. Since no verb ""machine_learning"" exists in any dictionary (there's no instance in its model), it'll go for a ""else"" route and apply generic rules.</p>

<p>Therefore, machine_learning is probably being lemmatized by a generic <strong>'""ing"" to ""e""'</strong> rule (such as in the case of making -> make, baking -> bake), common to many regular verbs.</p>

<p>Look at this test example:</p>

<pre><code>for descrip in tokenized:
        doc = nlp("" "".join(descrip))
        print([
            (token.pos_, token.text) for token in doc
        ])

</code></pre>

<p>Output:</p>

<blockquote>
  <p>[('NOUN', 'artificially_intelligent'), ('NOUN', 'funds'), ('VERB',
  'generating'), ('ADJ', 'excess'), ('NOUN', 'returns'), ('NOUN',
  'artificial_intelligence'), ('NOUN', 'deep_learning'), ('ADJ',
  'compelling'), ('NOUN', 'reasons'), ('PROPN', 'join_us'), ('NOUN',
  'artificially_intelligent'), ('NOUN', 'fund'), ('NOUN', 'develop'),
  ('VERB', 'ai'), ('VERB', 'machine_learning'), ('NOUN',
  'capabilities'), ('ADJ', 'real'), ('NOUN', 'cases'), ('ADJ', 'big'),
  ('NOUN', 'players'), ('NOUN', 'industry'), ('VERB', 'discover'),
  ('VERB', 'emerging'), ('NOUN', 'trends'), ('NOUN',
  'latest_developments'), ('VERB', 'ai'), ('VERB', 'machine_learning'),
  ('NOUN', 'industry'), ('NOUN', 'players'), ('NOUN', 'trading'),
  ('VERB', 'investing'), ('ADJ', 'live'), ('NOUN', 'investment'),
  ('NOUN', 'models'), ('VERB', 'learn'), ('VERB', 'develop'), ('ADJ',
  'compelling'), ('NOUN', 'business'), ('NOUN', 'case'), ('NOUN',
  'clients'), ('NOUN', 'ceos'), ('VERB', 'adopt'), ('VERB', 'ai'),
  ('ADJ', 'machine_learning'), ('NOUN', 'investment'), ('NOUN',
  'approaches'), ('ADJ', 'rare'), ('VERB', 'gathering'), ('NOUN',
  'talents'), ('VERB', 'including'), ('NOUN', 'quants'), ('NOUN',
  'data_scientists'), ('NOUN', 'researchers'), ('VERB', 'ai'), ('ADJ',
  'machine_learning'), ('NOUN', 'experts'), ('NOUN',
  'investment_officers'), ('VERB', 'explore'), ('NOUN', 'solutions'),
  ('VERB', 'challenges'), ('ADJ', 'potential'), ('NOUN', 'risks'),
  ('NOUN', 'pitfalls'), ('VERB', 'adopting'), ('VERB', 'ai'), ('NOUN',
  'machine_learning')]</p>
</blockquote>

<p>You are getting both machine_learning as verb and noun based on context. But see that just concatenating the words gets you messy because they are not ordered in Natural language as expected.</p>

<p>Not even a human can understand and correctly POS Tag this text:</p>

<blockquote>
  <p>artificially_intelligent funds generating excess returns
  artificial_intelligence deep_learning compelling reasons join_us
  artificially_intelligent fund develop ai machine_learning capabilities
  real cases big players industry discover emerging trends
  latest_developments ai machine_learning industry players trading
  investing live investment models learn develop compelling business
  case clients ceos adopt ai machine_learning investment approaches rare
  gathering talents including quants data_scientists researchers ai
  machine_learning experts investment_officers explore solutions
  challenges potential risks pitfalls adopting ai machine_learning</p>
</blockquote>
",""
"57860929","2019-09-09 20:40:05","1","","57857240","<p>Just wrap it into a loop and get the lemma of each token:</p>
<pre class=""lang-py prettyprint-override""><code>import spacy
nlp = spacy.load('de_core_news_md')

mails=['Hallo. Ich spielte am fr√ºhen Morgen und ging dann zu einem Freund. Auf Wiedersehen', 'Guten Tag Ich mochte B√§lle und will etwas kaufen. Tsch√ºss']

mails_lemma = []

for mail in mails:
     doc = nlp(mail)
     result = ' '.join([x.lemma_ for x in doc]) 
     mails_lemma.append(result)
</code></pre>
<p>Output:</p>
<pre class=""lang-py prettyprint-override""><code>['hallo . ich spielen am fr√ºh Morgen und gehen dann zu einer Freund . Auf Wiedersehen ',
 'Guten tagen ich m√∂gen Ball und wollen etwas kaufen . Tsch√ºss']
</code></pre>
",""
"57821149","2019-09-06 11:28:11","6","","57813864","<pre><code>import nltk
from nltk import word_tokenize
for a in data:
    for b in a[""ingredients""]:
        text = word_tokenize(b)
        res = nltk.pos_tag(text)
        res = [t for t in res if t[1] in [""NN"", ""NNS"", ""NNP"", ""NNPS""]]
        print(res)

#output:
#[('powder', 'NN')]
#[('eggs', 'NNS')]
#[('flour', 'NN')]
#[('raisins', 'NNS')]
#[('milk', 'NN')]
#[('sugar', 'NN')]
#[('sugar', 'NN')]
#[('egg', 'NN'), ('yolks', 'NNS')]
#[('corn', 'NN'), ('starch', 'NN')]
# ...

</code></pre>
",""
"57784032","2019-09-04 08:04:43","0","","57779549","<p>I'd recommend using (or adapting) the textacy CoNLL exporter to get the right format, see: <a href=""https://stackoverflow.com/q/57465128/461847"">How to generate .conllu from a Doc object?</a></p>

<p>Spacy's parser is doing sentence segmentation and you're iterating over <code>doc.sents</code>, so you'll see each sentence it exported separately. If you want to provide your own sentence segmentation, you can do that with a custom component, e.g.:</p>

<pre><code>def set_custom_boundaries(doc):
    for token in doc[:-1]:
        if token.text == ""..."":
            doc[token.i+1].is_sent_start = True
    return doc

nlp.add_pipe(set_custom_boundaries, before=""parser"")
</code></pre>

<p>Details (especially about how to handle <code>None</code> vs. <code>False</code> vs. <code>True</code>): <a href=""https://spacy.io/usage/linguistic-features#sbd-custom"" rel=""nofollow noreferrer"">https://spacy.io/usage/linguistic-features#sbd-custom</a></p>

<p>Spacy's default models aren't trained on twitter-like text, the parser probably won't perform well with respect to sentence boundaries here.</p>

<p>(Please ask unrelated questions as separate questions, and also take a look at spacy's docs: <a href=""https://spacy.io/usage/linguistic-features#special-cases"" rel=""nofollow noreferrer"">https://spacy.io/usage/linguistic-features#special-cases</a>)</p>
",""
"57749956","2019-09-01 22:38:56","3","","57749696","<p>There are a few default parameters that might affect what sklearn is calculating, but the particular one here that seems to matter is:</p>

<p><code>smooth_idf : boolean (default=True)</code>
Smooth idf weights by adding one to document frequencies, as if an extra document was seen containing every term in the collection exactly once. Prevents zero divisions.</p>

<p>If you subtract one from each element and raise e to that power, you get values that are very close to 5 / n, for low values of n:</p>

<pre><code>1.91629073 =&gt; 5/2
1.22314355 =&gt; 5/4
1.51082562 =&gt; 5/3
1 =&gt; 5/5
</code></pre>

<p>At any rate, there is not a single tf-idf implementation; the metric you define is simply a heuristic that tries to observe certain properties (like ""a higher idf should correlate with rarity in the corpus"") so I wouldn't worry too much about achieving an identical implementation.</p>

<p>sklearn appears to have used: 
<code>log((document_length + 1) / (frequency of word + 1)) + 1</code>
which is rather like if there was a document that had every single word in the corpus.</p>

<p>Edit: this last paragraph is corroborated by the docstring for <a href=""https://github.com/scikit-learn/scikit-learn/blob/9d62e2bb412653f24e883893737cf903906c795e/sklearn/feature_extraction/text.py#L1258"" rel=""nofollow noreferrer""><code>TfIdfNormalizer</code></a>.</p>
",""
"57742053","2019-08-31 22:43:52","0","","57742004","<p>You can use applymap.</p>

<pre><code>df.fillna('').applymap(lambda x: nltk.pos_tag([x])[0][1] if x!='' else '')

    0   1   2   3   4   5   6
0   NN  NN  VBG NN  NN  VBD NNS
1   NN  RB  IN  TO  NN  NN  
</code></pre>

<p>Note: If your dataframe is large, it'll be more efficient to tag the entire sentences and then convert the tags into a dataframe. The current approach will be slow with big dataset. </p>
",""
"57714185","2019-08-29 16:17:25","5","","57713159","<p>The simplest method I can think of is using brute force (sure, you could adapt it or even use some machine learning to help find classes for easier matching).</p>

<p>A simple bruteforce method follows:</p>

<p><strong>Tag the String</strong></p>

<pre><code>string_list = nltk.pos_tag(a.split())
</code></pre>

<p><strong>Create a list of expected tags</strong></p>

<pre><code>pos_tags = [""NN"", ""VBP"", ""NN""]
</code></pre>

<p><strong>The following function will be able to check wheter this pattern appears:</strong></p>

<pre><code>def find_match(string_list, pos_tags)

    num_matched = 0
    match_start_pos = 0
    matched = False
    #Enumerating gives you an index to compare to enable you to find where matching starts
    for idx, tuple in enumerate(string_list):
        if tuple[1] == pos_tags[num_matched]:
            num_matched += 1
            if num_matched == 0:
                match_start_pos = idx
        else: 
            num_matched = 0
        if num_matched == len(pos_tags):
            matched = True
            break
    return (matched, match_start_pos)
</code></pre>

<p><strong>More Realistically:</strong></p>

<p>Now, more practically, Suppose you belong to a Civilian protection agency and want to be aware of any tweet made by school students mentioning killing. You somehow filter the tweets and want to check if someone wants to kill anyone else. </p>

<p>With just a little modification, you can achieve at something similar (the following ideas are somehow powered by what is called <a href=""https://en.wikipedia.org/wiki/Frame_semantics_(linguistics)"" rel=""nofollow noreferrer"">Frame Semantics</a>):</p>

<pre><code>killing_intent_dict = {""PRP"":set(""I"", ""YOU"", ""He"", ""She""), ""V"": set(""kill""), ""NNP"":set(""All"", ""him"", ""her"")}
if find_match_pattern(string_list, killing_intent_dict):
#    someone wants to kill! Call 911

def find_match_pattern(string_list, pattern_dict) 
    num_matched = 0
    match_start_pos = 0
    matched = False
    #Enumerating gives you an index to compare to enable you to find where matching starts
    for idx, tuple in enumerate(string_list):
        if tuple[1] == pattern_dict.keys()[num_matched]:
            if tuple[0] in pattern_dict[tuple[1]]:
                num_matched += 1
                if num_matched == 0:
                    match_start_pos = idx
            else:
                num_matched = 0
        else: 
            num_matched = 0
        if num_matched == len(pattern_dict):
            matched = True
            break
    return (matched, match_start_pos)
</code></pre>

<p>Keep in mind that this is all experimental and requires a lot of hand coding. You can add to it NER tags so you can abstract names.</p>

<p><strong>Appending another possibility, similar to the one I used in my master's research:</strong></p>

<p>Instead of using a linear bruteforce mechanism, you could create a graph containing the actions, agents and intents, connecting them all. You then use some sort of graph spreading algorithm while your program reads the input. You can read more in my research, but keep in mind that this topic that you are asking (Natural Language Understanding) is deep and under development: <a href=""https://drive.google.com/open?id=12gWLx2saFe5mZI96roUG_p1YfzrqVNbx"" rel=""nofollow noreferrer"">https://drive.google.com/open?id=12gWLx2saFe5mZI96roUG_p1YfzrqVNbx</a></p>
",""
"57664963","2019-08-26 21:08:56","0","","56610465","<p>It looks like this implementation is at least consistent with Chen and Cherry, 2014. They suggested to average <code>n-1, n, n+1</code> -gram counts. The also defined <code>m0_prime</code> as <code>m1 + 1</code> (so in our case it will be 2 and that breaks our computations).</p>

<p>I'm using <code>method5</code> (it's used by <code>method7</code>) from <a href=""https://www.nltk.org/_modules/nltk/translate/bleu_score.html"" rel=""nofollow noreferrer"">here</a>.</p>

<pre><code>cc = SmoothingFunction()
references = ['overofficious 98461 54363 39016 78223 52180'.split()]
candidate = 'overofficious 98461 54363 39016 78223 52180'.split()
p_n = [Fraction(1, 1)] * 4
p_n5 = cc.method5(p_n, references, candidate, len(candidate))
</code></pre>

<p>Output:</p>

<pre><code>[Fraction(4, 3), Fraction(10, 9), Fraction(28, 27), Fraction(82, 81)]
</code></pre>

<p>We may compute <code>4/3</code> like this: <code>(2 + 1 + 1) / 3</code>; <code>10/9 = (4/3 + 1 + 1) / 3</code> and so on. </p>
",""
"57599728","2019-08-21 22:08:21","3","","57539043","<p>You can add a custom dictionary for certain words, like <code>pos_dict = {'breakfasted':'v', 'left':'a', 'taken':'v'}</code></p>

<p>By passing this customized <code>pos_dict</code> along with <code>token</code> into the <code>lemmitize</code> function, you can use the lemmatizer for each token with a POS tag that you specify.</p>

<p><code>lemmatize(token, pos_dict.get(token, 'n'))</code> will pass 'n' for its second argument as a default value, unless the token is in the <code>pos_dict</code> keys. You can change this default value to whatever you want.</p>

<pre><code>def lemmitize(document, pos_dict):
    return stemmer.stem(WordNetLemmatizer().lemmatize(document, pos_dict.get(document, 'n')))

def preprocess(document, pos_dict):
    output = []
    for token in gensim.utils.simple_preprocess(document):
        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) &gt; 3:
            print(""lemmitize: "", lemmitize(token, pos_dict))
            output.append(lemmitize(token, pos_dict))
    return output
</code></pre>
",""
"57592922","2019-08-21 13:34:42","2","","57592503","<p>Another alternative you can check out is the package from this person, he has it for many different languages. Here is the link for <a href=""https://github.com/michmech/lemmatization-lists/blob/master/lemmatization-it.txt"" rel=""nofollow noreferrer"">Italian</a>.</p>

<p>Whether it will help your case or not is another debate but it can also be implemented via the <a href=""https://cran.r-project.org/web/packages/corpus/"" rel=""nofollow noreferrer"">corpus</a> package. A sample example (for English use case, tweak it for Italian) is also given in their documentation if you move down to the Dictionary Stemmer <a href=""https://cran.r-project.org/web/packages/corpus/vignettes/stemmer.html"" rel=""nofollow noreferrer"">section</a>
<hr>
Alternatively, similar to the above way, you can also consider the stemmers or lemmatizers (if you havent considered lemmatizers, they are worth considering) from Python libraries such as <a href=""https://github.com/nltk/nltk"" rel=""nofollow noreferrer"">NLTK</a> or <a href=""https://github.com/explosion/spaCy/blob/master/spacy/lang/it/lemmas.json"" rel=""nofollow noreferrer"">Spacy</a> and check if you are getting better resutls. After all, they are just files containing mappings of root word vs child words. Download them, fine tune the file to your requirement, and use the mappings as per your convenience by passing it via a custom made function.</p>
",""
"57547113","2019-08-18 18:04:48","3","","56713358","<p>I've been trying to optimize your process: </p>

<pre><code>from nltk.corpus import stopwords

cachedStopWords = set(stopwords.words(""english""))

filters = ""!\""#$%&amp;()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~\t\n?,‡•§!‚Äç.'0123456789‡ß¶‡ßß‡ß®‡ß©‡ß™‡ß´‡ß¨‡ß≠‡ßÆ‡ßØ‚Äò\u200c‚Äì‚Äú‚Äù‚Ä¶‚Äò""
trnaslate_table = str.maketrans('', '', filters)
def sentence_to_wordlist(sentence, filters=filters):
    wordlist = sentence.translate(trnaslate_table).split()
    return [w for w in wordlist if w not in cachedStopWords] 

from multiprocessing.pool import Pool

p = Pool(10)
results  = p.map(sentence_to_wordlist, data)
</code></pre>

<ul>
<li><p>data is a list with your articles</p></li>
<li><p>I've been using the stop words from nltk but you can use your own stopwords, please make sure your stopwords is a set not a list (<a href=""https://wiki.python.org/moin/TimeComplexity"" rel=""nofollow noreferrer"">because to find if a element is in a set is O(1) time complexity and in a list is O(n)</a>) </p></li>
</ul>

<p>I've been testing with a list of 100k articles, each article having around 2k characters, took me less than 9 seconds.</p>
",""
"57432475","2019-08-09 14:46:50","0","","55852115","<p>I think ultimately, it all comes down to finding the optimal tradeoff between speed, maintainability of the code and the way this piece of logic fits into the larger picture of your application. Finding a few strings in a text is unlikely to be the end goal of what you're trying to do ‚Äì otherwise, you probably wouldn't be using spaCy and would stick to regular expressions. How your application needs to ""consume"" the result of the matching and what the matches mean in the larger context should motivate the approach you choose. </p>

<p>As you mention in the conclusion, if your matches are ""named entities"" by definition, adding them to the <code>doc.ents</code> makes a lot of sense and will even give you an easy way to combine your logic with statistical predictions. Even if it adds slightly more overhead, it'll likely still outperform any scaffolding you'd otherwise have to write around it yourself.</p>

<blockquote>
  <p>For each solution I start with an initial setup</p>
</blockquote>

<p>If you're running the experiments in the same session, e.g. in a notebook, you may want to include the creation of the <code>Doc</code> object in your initial setup. Otherwise, the caching of the vocabulary entries could theoretically mean that the very first call of <code>nlp(text)</code> is slower than the subsequent calls. It's likely insignificant, though.</p>

<blockquote>
  <p>I was quite surprised that the token matcher outperforms the phrase matcher. I thought it would be opposite</p>
</blockquote>

<p>One potential explanation is that you're profiling the approaches on a very small scale and on single-token patterns where the phrase matcher engine doesn't really have an advantage over the regular token matcher. Another factor could be that matching on a different attribute (e.g. <code>LOWER</code> instead of <code>TEXT</code>/<code>ORTH</code>) requires <a href=""https://github.com/explosion/spaCy/blob/963ea5e8d0290e2198b6b36fa79b420f3152f639/spacy/matcher/phrasematcher.pyx#L153-L158"" rel=""noreferrer"">creating a new <code>Doc</code> during matching</a> that reflects the values of the matched attribute. This should be inexpensive, but it's still one extra object that gets created. So a test <code>Doc</code> <code>""extract January""</code> will actually become <code>""extract january""</code> (when matching on <code>LOWER</code>) or even <code>""VERB PROPN""</code> when matching on <code>POS</code>. That's the trick that makes matching on other attributes work.</p>

<p>Some background on how the <code>PhraseMatcher</code> works and why its mechanism is typically faster: When you add <code>Doc</code> objects to the <code>PhraseMatcher</code>, it sets flags on the tokens included in the patterns, indicating that they're matching a given pattern. It then calls into the regular <code>Matcher</code> and adds token-based patterns using the previously set flags. When you're matching, spaCy will only have to check the flags and not retrieve any token attributes ‚Äì that's what should make the matching itself significantly faster at scale.</p>

<p>This actually brings up another approach you could be profiling for comparison: Using <a href=""https://spacy.io/api/vocab#add_flag"" rel=""noreferrer""><code>Vocab.add_flag</code></a> to set a boolean flag on the respective lexeme (entry in the vocab, so not the context-sensitive token). Vocab entries are cached, so you should only have to compute the flag once for a lexeme like <code>""january""</code>. However, this approach only really makes sense for single tokens, so it's relatively limiting.</p>

<blockquote>
  <p>Am I missing something important here or can I trust this analysis on a larger scale?</p>
</blockquote>

<p>If you want to get any meanigful insights, you should be benchmarking on at least a medium-sized scale. You don't want to be looping over the same small example 10000 times and instead, benchmark on a dataset that you'll only be processing <em>once</em> per test. For instance, a few hundred documents similar to the data you're actually working with. There are caching effects (both within spaCy, but also your CPU), differences in memory allocation and so on that can all have an impact.</p>

<p>Finally, using spaCy's <a href=""https://spacy.io/api/cython"" rel=""noreferrer"">Cython API</a> directly will always be the fastest. So if speed is your number one concern and all you want to optimise for, Cython would be the way to go.</p>
",""
"57365644","2019-08-05 20:23:06","6","","57360747","<p>English unigrams are often hard to tag well, so think about why you want to do this and what you expect the output to be. (Why is the POS of <code>apple</code> in your example <code>NNP</code>? What's the POS of <code>can</code>?)</p>

<p>spacy isn't really intended for this kind of task, but if you want to use spacy, one efficient way to do it is:</p>

<pre><code>import spacy
nlp = spacy.load('en')

# disable everything except the tagger
other_pipes = [pipe for pipe in nlp.pipe_names if pipe != ""tagger""]
nlp.disable_pipes(*other_pipes)

# use nlp.pipe() instead of nlp() to process multiple texts more efficiently
for doc in nlp.pipe(words):
    if len(doc) &gt; 0:
        print(doc[0].text, doc[0].tag_)
</code></pre>

<p>See the documentation for <code>nlp.pipe()</code>: <a href=""https://spacy.io/api/language#pipe"" rel=""noreferrer"">https://spacy.io/api/language#pipe</a></p>
",""
"57347698","2019-08-04 15:06:04","1","","57346347","<p>The TREC datasets (along with a set of queries and a document collection) have associated <a href=""https://trec.nist.gov/data/reljudge_eng.html"" rel=""nofollow noreferrer"">relevance judgments</a> for each query.</p>

<p>To measure the ""accuracy"" (effectiveness) of your query expansion method, you need to test it on the down-stream retrieval performance, i.e. you should consider your query expansion to be effective if it improves the retrieval (in terms of standard measures such as mean average precision or P@5).</p>

<p>Use the <a href=""https://trec.nist.gov/trec_eval/"" rel=""nofollow noreferrer"">trec_eval</a> tool to evaluate the effectiveness of retrieval both before (baseline) and after your query expansion method.</p>
",""
"57343183","2019-08-04 01:33:30","0","","57293069","<p>Initially I thought this was not possible ... but halfway through writing my answer I found a solution. However the solution is quite messy so I have left my original answer with a slightly better solution.</p>
<p><code>nltk</code> allows you to provide custom regexes so you can write a regex to match escaped parentheses. The regex <code>([^\s\(\)\\]+(\\(?=\()\([^\s\(\)\\]+\\(?=\))\))*[\\]*)+</code> will match parentheses escaped by backslashes (<code>\</code>). This however, will include the escaping backslashes in each leaf so you must write a leaf function to remove these. The following code will properly parse it:</p>
<pre class=""lang-py prettyprint-override""><code>from nltk import Tree

s = '(S (NP (PRP They)) (VP like\(d\) (NP (PRP it)) (NP (DT a) (NN lot))) (. .))'

tree = Tree.fromstring(s, leaf_pattern=r&quot;([^\s\(\)\\]+(\\(?=\()\([^\s\(\)\\]+\\(?=\))\))*[\\]*)+&quot;, read_leaf=lambda x: x.replace(&quot;\\(&quot;, &quot;(&quot;).replace(&quot;\\)&quot;, &quot;)&quot;))
print(tree)
</code></pre>
<p>And it outputs:</p>
<pre><code>(S
  (NP (PRP They))
  (VP like(d) (NP (PRP it)) (NP (DT a) (NN lot)))
  (. .))
</code></pre>
<hr />
<h1>Original answer</h1>
<p>Perhaps you could ask <code>nltk</code> to match another bracket:</p>
<pre class=""lang-py prettyprint-override""><code>from nltk import Tree

s = '[S [NP [PRP They]] [VP like(d) [NP [PRP it]] [NP [DT a] [NN lot]]] [. .]]'

tree = Tree.fromstring(s, brackets='[]')
print(tree)
</code></pre>
<p>Which prints out:</p>
<pre><code>(S
  (NP (PRP They))
  (VP like(d) (NP (PRP it)) (NP (DT a) (NN lot)))
  (. .))
</code></pre>
<hr />
<p>You can get different brackets by using the <code>pformat</code> method (which is called internally when you call print):</p>
<pre class=""lang-py prettyprint-override""><code>print(tree.pformat(parens='[]'))
</code></pre>
<p>Which prints out:</p>
<pre><code>[S
  [NP [PRP They]]
  [VP like(d) [NP [PRP it]] [NP [DT a] [NN lot]]]
  [. .]]
</code></pre>
",""
"57284540","2019-07-31 06:27:46","0","","57026011","<p>PorterStemmer was for English. However, the author, Porter himself, extended his work Snowball for English as well as other languages. This is one reason why many use SnowBall for other languages.</p>

<p>Here is a quote from Wikipedia:</p>

<blockquote>
  <p><em>Many implementations of the Porter stemming algorithm were written and freely distributed; however, many of these implementations
  contained subtle flaws. As a result, these stemmers did not match
  their potential. To eliminate this source of error, Martin Porter
  released an official free software (mostly BSD-licensed)
  implementation of the algorithm around the year 2000. He extended this
  work over the next few years by building Snowball, a framework for
  writing stemming algorithms, and implemented an improved English
  stemmer together with stemmers for several other languages.</em></p>
</blockquote>
",""
"57193799","2019-07-25 03:15:34","2","","57193665","<p>Here is an example</p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd

vect = TfidfVectorizer()
tfidf_matrix = vect.fit_transform(documents)
df = pd.DataFrame(tfidf_matrix.toarray(), columns = vect.get_feature_names())
print(df)
</code></pre>
",""
"57134499","2019-07-21 15:18:07","0","","57128766","<p>Just replace the default tokenizer in the pipeline with <code>nlp.tokenizer.tokens_from_list</code> instead of calling it separately:</p>

<pre><code>import spacy
nlp = spacy.load('en')
nlp.tokenizer = nlp.tokenizer.tokens_from_list

for doc in nlp.pipe([['I', 'like', 'cookies', '.'], ['Do', 'you', '?']]):
    for token in doc:
        print(token, token.pos_)
</code></pre>

<p>Output:</p>

<pre><code>I PRON
like VERB
cookies NOUN
. PUNCT
Do VERB
you PRON
? PUNCT
</code></pre>
",""
"57072351","2019-07-17 09:10:01","3","","57057992","<p>The word-piece tokenization helps in multiple ways, and should be better than lemmatizer. Due to multiple reasons:</p>

<ol>
<li>If you have the words 'playful', 'playing', 'played', to be lemmatized to 'play', it can lose some information such as <code>playing</code> is present-tense and <code>played</code> is past-tense, which doesn't happen in word-piece tokenization.</li>
<li>Word piece tokens cover all the word, even the words that do not occur in the dictionary. It splits the words and there will be word-piece tokens, that way, you shall have embeddings for the split word-pieces, unlike removing the words or replacing with 'unknown' token.</li>
</ol>

<p>Usage of word-piece tokenization instead of tokenizer+lemmatizer is merely a design choice, word-piece tokenization should perform well. But you may have to take into count because word-piece tokenization increases the number of tokens, which is not the case in lemmatization.</p>
",""
"57017151","2019-07-13 07:42:25","2","","57008528","<p>If you can, use the most recent version of spacy instead. The French lemmatizer has been improved a lot in 2.1.</p>

<p>If you have to use 2.0, consider using an alternate lemmatizer like this one: <a href=""https://spacy.io/universe/project/spacy-lefff"" rel=""nofollow noreferrer"">https://spacy.io/universe/project/spacy-lefff</a></p>
",""
"57013292","2019-07-12 19:50:43","1","","57009676","<p>The mistake you are probably making is that the <code>post</code> is a string, whereas it should be a list of strings. That's why, as you mentioned, the <code>model.predict()</code> produces a lot of values: because tokenizer has iterated over the characters of <code>post</code> and produced a Tf-idf vector for each of them! Just put it in a list and the problem would be resolved:</p>

<pre><code>... = self._tokenizer.texts_to_matrix([post], ...)
</code></pre>
",""
"56979424","2019-07-10 22:28:06","0","","56978661","<p>To the best of my knowledge the answer is No, and depending on the stemmer it might be difficult to come up with an exhaustive search for reverting the effect of the stemming rules and the results would be mostly invalid words by any standard. E.g for Porter stemmer:</p>

<pre><code>from nltk.stem.porter import *
stemmer = PorterStemmer()
stemmer.stem('grabfuled')
# results in ""grab"" 
</code></pre>

<p>So, a reverse function would generate ""grabfuled"" as one of the valid words as ""-ed"" and ""-ful"" suffixes are removed consecutively in the stemming process. 
However, given a valid lexicon, you can do the following which is independent of the stemming method:</p>

<pre><code>from nltk.stem.porter import *
from collections import defaultdict

vocab = set(['grab', 'grabbing', 'grabbed', 'run', 'running', 'eat'])

# Here porter stemmer, but can be any other stemmer too
stemmer = PorterStemmer()

d = defaultdict(set)
for v in vocab:
    d[stemmer.stem(v)].add(v)  

print(d)
# defaultdict(&lt;class 'set'&gt;, {'grab': {'grab', 'grabbing', 'grabbed'}, 'eat': {'eat'}, 'run': {'run', 'running'}})
</code></pre>

<p>Now we have a dictionary that maps stems to the valid words that can generate them. For any stem we can do the following:</p>

<pre><code>print(d['grab'])
# {'grab', 'grabbed', 'grabbing'}
</code></pre>

<p>For building the vocabulary you can tokenize a corpus or use <a href=""https://stackoverflow.com/questions/28339622/is-there-a-corpora-of-english-words-in-nltk"">nltk's built-in dictionary of English words</a>.</p>
",""
"56914947","2019-07-06 14:21:20","0","","56884204","<p>TfidfVectorizer has an attribute <code>norm</code> (see <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"" rel=""nofollow noreferrer"">the docs</a>) that deals with this issue. Try, for example, something like this:</p>

<pre class=""lang-py prettyprint-override""><code>vectorizer = TfidfVectorizer(analyzer='word', stop_words='english', norm='l2')
</code></pre>

<p>This will normalise the vectors to account for differences in document lengths.</p>
",""
"56914811","2019-07-06 14:01:39","0","","56914017","<p>You can achieve the above result by using following code:</p>

<pre><code>def extract_topn_from_vector(feature_names, sorted_items, topn=5):
    """"""
      get the feature names and tf-idf score of top n items in the doc,                 
      in descending order of scores. 
    """"""

    # use only top n items from vector.
    sorted_items = sorted_items[:topn]

    results= {} 
    # word index and corresponding tf-idf score
    for idx, score in sorted_items:
        results[feature_names[idx]] = round(score, 3)

    # return a sorted list of tuples with feature name and tf-idf score as its element(in descending order of tf-idf scores).
    return sorted(results.items(), key=lambda kv: kv[1], reverse=True)

feature_names = count_vect.get_feature_names()
coo_matrix = message_tfidf.tocoo()
tuples = zip(coo_matrix.col, coo_matrix.data)
sorted_items = sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)

# extract only the top n elements.
# Here, n is 10.
word_tfidf = extract_topn_from_vector(feature_names, sorted_items, 10)

print(""{}  {}"".format(""features"", ""tfidf""))  
for k in word_tfidf:
    print(""{} - {}"".format(k[0], k[1])) 
</code></pre>

<p>Check out the full code below to get a better idea of above code snippet.
The below code is self-explanatory.</p>

<p><strong>Full Code:</strong></p>

<pre><code>from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from nltk.corpus import stopwords
import string
import nltk
import pandas as pd

data = pd.read_csv('yourfile.csv')

stops = set(stopwords.words(""english""))
wl = nltk.WordNetLemmatizer()

def clean_text(text):
    """"""
      - Remove Punctuations
      - Tokenization
      - Remove Stopwords
      - stemming/lemmatizing
    """"""
    text_nopunct = """".join([char for char in text if char not in string.punctuation])
    tokens = re.split(""\W+"", text)
    text = [word for word in tokens if word not in stops]
    text = [wl.lemmatize(word) for word in text]
    return text

def extract_topn_from_vector(feature_names, sorted_items, topn=5):
    """"""
      get the feature names and tf-idf score of top n items in the doc,                 
      in descending order of scores. 
    """"""

    # use only top n items from vector.
    sorted_items = sorted_items[:topn]

    results= {} 
    # word index and corresponding tf-idf score
    for idx, score in sorted_items:
        results[feature_names[idx]] = round(score, 3)

    # return a sorted list of tuples with feature name and tf-idf score as its element(in descending order of tf-idf scores).
    return sorted(results.items(), key=lambda kv: kv[1], reverse=True)

count_vect = CountVectorizer(analyzer=clean_text, tokenizer = None, preprocessor = None, stop_words = None, max_features = 5000)                                        
freq_term_matrix = count_vect.fit_transform(data['text_body'])

tfidf = TfidfTransformer(norm=""l2"")
tfidf.fit(freq_term_matrix)  

feature_names = count_vect.get_feature_names()

# sample document
doc = 'watched horrid thing TV. Needless say one movies watch see much worse get.'

tf_idf_vector = tfidf.transform(count_vect.transform([doc]))

coo_matrix = tf_idf_vector.tocoo()
tuples = zip(coo_matrix.col, coo_matrix.data)
sorted_items = sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)

# extract only the top n elements.
# Here, n is 10.
word_tfidf = extract_topn_from_vector(feature_names,sorted_items,10)

print(""{}  {}"".format(""features"", ""tfidf""))  
for k in word_tfidf:
    print(""{} - {}"".format(k[0], k[1])) 
</code></pre>

<p><strong>Sample output:</strong></p>

<pre><code>features  tfidf
Needless - 0.515
horrid - 0.501
worse - 0.312
watched - 0.275
TV - 0.272
say - 0.202
watch - 0.199
thing - 0.189
much - 0.177
see - 0.164
</code></pre>
",""
"56885830","2019-07-04 10:22:02","3","","56884020","<blockquote>
  <h2>Q: What is the problem in my code?</h2>
</blockquote>

<p>Well, most probably the issue comes not from the code, but from the ""hidden"" processing, that appears, once <strong><code>n_jobs</code></strong> directs ( and <code>joblib</code> internally orchestrates ) to prepare that many exact copies of the main process, so as to let them work independently one of each other ( effectively thus escaping from GIL-locking and mapping the multiple process-flows onto physical hardware resources )</p>

<p>This step is responsible for making copies of all pythonic objects and was known to use <strong><code>Pickle</code></strong> for doing this. The <code>Pickle</code> module was known for its historical principal limitations on what can be pickled and what cannot.</p>

<p>The error message confirms this:</p>

<blockquote>
  <p><code>TypeError: self.c_map cannot be converted to a Python object for pickling</code></p>
</blockquote>

<p>One may try a trick to supply Mike McKearns <strong><code>dill</code></strong> module instead of <code>Pickle</code> and test, if your ""problematic"" python objects will get pickled with this module without throwing this error.</p>

<p><strong><code>dill</code></strong> has the same API signatures, so a pure <code>import dill as pickle</code> may help with leaving all the other code the same.</p>

<p>I had the same problems, with large models to get distributed into and back from multiple processes and the <code>dill</code> was a way to go. Also the performance has increased.</p>

<blockquote>
  <h2>Bonus: <code>dill</code> allows to save / restore the full python interpreter state!</h2>
</blockquote>

<p>This was a cool side-effect of finding <code>dill</code>, once <code>import dill as pickle</code> was done, <code>pickle.dump_session( &lt;aFile&gt; )</code> will save ones complete state-full copy of the python interpreter session. This can be restored, if needed ( post-crash restores, trained trained and optimised ML-model state-fully saved / restored, incremental learning ML-model state-fully saved and re-distributed for remote restores for the deployed user-bases, etc. )</p>
",""
"56872225","2019-07-03 14:21:25","3","","56870824","<p>You can convert a list of dictionaries into a dataframe by using the pandas DataFrame class directly.</p>

<pre class=""lang-py prettyprint-override""><code>import pandas as pd

a = [{""0"": 0}, {""1"": 1}]
df = pd.DataFrame(a)
</code></pre>

<p>To apply this to your problem, all you have to do is turn <code>mydict</code> into a list of dictionaries instead of a dictionary of dictionaries.</p>
",""
"56852257","2019-07-02 12:15:53","1","","56841777","<p>You can analyze the whole document and then just find the noun chunk before each reference, either by token position or character offset. The token offset of the last token in a noun chunk is <code>noun_chunk[-1].i</code> and the character offset of the start of the last token is <code>noun_chunk[-1].idx</code>. (Check that the analysis isn't affected by the presence of the reference strings; your example <code>(1)</code>-style references seem to be analyzed as appositives, which is fine.) </p>

<p>If the analysis is affected by the reference strings, remove them from the document while keeping track of their character offsets, analyze the whole document, and then find the noun chunks preceding the saved positions.</p>
",""
"56759895","2019-06-25 18:16:38","0","","56754251","<p>The <code>n_threads</code> argument has been deprecated in newer versions of spacy and doesn't do anything. See the note here: <a href=""https://spacy.io/api/language#pipe"" rel=""nofollow noreferrer"">https://spacy.io/api/language#pipe</a></p>

<p>Here's their example code for doing this with multi-processing instead:</p>

<p><a href=""https://spacy.io/usage/examples#multi-processing"" rel=""nofollow noreferrer"">https://spacy.io/usage/examples#multi-processing</a></p>
",""
"56733926","2019-06-24 09:45:48","0","","56728218","<p>This is a great question actually! I'd say your instinct is definitely right: If all you need is a <code>Doc</code> object in a given state and with given annotations, always create it manually wherever possible. And unless you're <em>explicitly</em> testing a statistical model, avoid loading it in your unit tests. It makes the tests slow, and it introduces too much unnecessary variance. This is also very much in line with the philosophy of unit testing: you want to be writing independent tests for <em>one thing at a time</em> (not one thing plus a bunch of third-party library code plus a statistical model).</p>
<p>Some general tips and ideas:</p>
<ul>
<li>If possible, always construct a <code>Doc</code> manually. Avoid loading models or <code>Language</code> subclasses.</li>
<li>Unless your application or test specifically needs the <code>doc.text</code>, you do not <em>have to</em> set the <code>spaces</code>. In fact, I leave this out in about 80% of the tests I write, because it really only becomes relevant when you're putting the tokens back together.</li>
<li>If you need to create a lot of <code>Doc</code> objects in your test suite, you could consider using a utility function, similar to the <a href=""https://github.com/explosion/spaCy/blob/f22704621ef5d136e00a47068288bf55f666716d/spacy/tests/util.py#L29"" rel=""nofollow noreferrer""><code>get_doc</code> helper</a> we use in the spaCy test suite. (That function also shows you how the individual annotations are set manually, in case you need it.)</li>
<li>Use (session-scoped) fixtures for the shared objects, like the <code>Vocab</code>. Depending on what you're testing, you might want to explicitly use the <code>English</code> vocab. In the spaCy test suite, we do this by setting up an <a href=""https://github.com/explosion/spaCy/blob/d4196a62f198f0ec32239b238f32421bbb6eb942/spacy/tests/conftest.py#L118"" rel=""nofollow noreferrer""><code>en_vocab</code> fixture</a> in the <code>conftest.py</code>.</li>
<li>Instead of setting the <code>doc.ents</code> to a list of tuples, you can also make it a list of <code>Span</code> objects. This looks a bit more straightforward, is easier to read, and in spaCy v2.1+, you can also pass a string as a label:</li>
</ul>
<pre class=""lang-py prettyprint-override""><code>def test_entities(en_vocab):
    doc = Doc(en_vocab, words=[&quot;Hello&quot;, &quot;world&quot;])
    doc.ents = [Span(doc, 0, 1, label=&quot;ORG&quot;)]
    assert doc.ents[0].text == &quot;Hello&quot;
</code></pre>
<ul>
<li>If you do need to test a model (e.g. in the test suite that makes sure that your custom models load and run as expected) or a language class like <code>English</code>, put them in a session-scoped fixture. This means that they'll only be loaded once per session instead of once per test. Language classes are lazy-loaded and may also take some time to load, depending on the data they contain. So you only want to do this once.</li>
</ul>
<pre class=""lang-py prettyprint-override""><code># Note: You probably don't have to do any of this, unless you're testing your
# own custom models or language classes.

@pytest.fixture(scope=&quot;session&quot;)
def en_core_web_sm():
    return spacy.load(&quot;en_core_web_sm&quot;)

@pytest.fixture(scope=&quot;session&quot;)
def en_lang_class():
    lang_cls = spacy.util.get_lang_class(&quot;en&quot;)
    return lang_cls()

def test(en_lang_class):
    doc = en_lang_class(&quot;Hello world&quot;)
</code></pre>
",""
"56726890","2019-06-23 18:48:46","3","","56721753","<p>The error says that <code>TfidfVectorizer</code> expects a string as its input - not a list of string. It does all the tokenization by itself (but you can plug in a custom tokenizer within <code>TfidfVectorizer</code>, if you want). </p>

<p>So I would try a simpler pipeline, without the first line (with <code>nltk.tokenize..</code>). But I cannot be 100% sure, because you did not provide any examples of actual input data that causes the error.</p>
",""
"56689469","2019-06-20 15:37:25","0","","56689311","<p>You can add tf-idf matrices and it still be somewhat relevant, howewer, the vectors will be not normalized afterwards and normalization of them is by itself not easier than vectorization. For clustering data based on similarity score(which can be computed with tfidf) its highly recommended to work with normalized vectors. Also, keep in mind that if you really want to combine those two into one with just addition, their vocabularies have to be the same, otherwise it will make no sence(or even the dimentions would be different). Also the problem is with nature of tf-idf if  some term was in many documents in one dataset and in few in other, his added tf-idf score would probably not be as good and relevant as calculated from skratch. So my best suggestion to you is indeed as you said </p>

<blockquote>
  <p>recalculating the TF-IDF score again</p>
</blockquote>

<p>For 5000 titles (or 10000 if combined idk) it still won't take more than some reasonable time (depending from your machine, on mine around 5-10 mins) and then saving this matrix to not go over the calculations again will do you a great favour.</p>
",""
"56687231","2019-06-20 13:37:17","0","","56687161","<p>Have you tried using tfidf? It creates a weighted matrix providing greater weight to the more semantically meaningful words of each text. It compares the individual text( in this case a tweet) to all of the texts (all of the tweets). It is much more helpful than using raw term counts for classification and other tasks. <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"" rel=""nofollow noreferrer"">https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html</a></p>
",""
"56635121","2019-06-17 16:09:12","1","","56527814","<p>It is to note the python library stanfordnlp is not just a python wrapper for StanfordCoreNLP. </p>

<h2>1. Difference StanfordNLP / CoreNLP</h2>

<p>As said on the <a href=""https://github.com/stanfordnlp/stanfordnlp"" rel=""noreferrer"">stanfordnlp Github repo</a>:</p>

<blockquote>
  <p>The Stanford NLP Group's official Python NLP library. It contains
  packages for running our latest fully neural pipeline from the CoNLL
  2018 Shared Task and for accessing the Java Stanford CoreNLP server.</p>
</blockquote>

<p>Stanfordnlp contains a new set of neural networks models, trained on the CONLL 2018 shared task. The online parser is based on the CoreNLP 3.9.2 java library. Those are two different pipelines and sets of models, as explained <a href=""https://github.com/stanfordnlp/stanfordnlp/issues/67"" rel=""noreferrer"">here</a>. </p>

<p>Your code only accesses their neural pipeline trained on CONLL 2018 data. This explains the differences you saw compared to the online version. Those are basically two different models.</p>

<p>What adds to the confusion I believe is that both repositories belong to the user named stanfordnlp (which is the team name). Don't be fooled between the java stanfordnlp/CoreNLP and the python stanfordnlp/stanfordnlp. </p>

<p>Concerning your 'neg' issue, it seems that in the python libabry stanfordnlp, they decided to consider the negation with an 'advmod' annotation altogether. At least that is what I ran into for a few example sentences.</p>

<h2>2. Using CoreNLP via stanfordnlp package</h2>

<p>However, you can still get access to the CoreNLP through the stanfordnlp package. It requires a few more steps, though. Citing the Github repo,</p>

<blockquote>
  <p>There are a few initial setup steps.</p>
  
  <ul>
  <li>Download Stanford CoreNLP and models for the language you wish to    use. <a href=""https://stanfordnlp.github.io/CoreNLP/"" rel=""noreferrer"">(you can download CoreNLP and the language models here)</a></li>
  <li>Put the model jars in the distribution folder</li>
  <li>Tell the python code where Stanford CoreNLP is located: export CORENLP_HOME=/path/to/stanford-corenlp-full-2018-10-05</li>
  </ul>
</blockquote>

<p>Once that is done, you can start a client, with code that can be found in the <a href=""https://github.com/stanfordnlp/stanfordnlp/blob/master/demo/corenlp.py"" rel=""noreferrer"">demo</a>  :</p>

<pre><code>from stanfordnlp.server import CoreNLPClient 

with CoreNLPClient(annotators=['tokenize','ssplit','pos','depparse'], timeout=60000, memory='16G') as client:
    # submit the request to the server
    ann = client.annotate(text)

    # get the first sentence
    sentence = ann.sentence[0]

    # get the dependency parse of the first sentence
    print('---')
    print('dependency parse of first sentence')
    dependency_parse = sentence.basicDependencies
    print(dependency_parse)

    #get the tokens of the first sentence
    #note that 1 token is 1 node in the parse tree, nodes start at 1
    print('---')
    print('Tokens of first sentence')
    for token in sentence.token :
        print(token)
</code></pre>

<p>Your sentence will therefore be parsed if you specify the 'depparse' annotator (as well as the prerequisite annotators tokenize, ssplit, and pos). 
Reading the demo, it feels that we can only access basicDependencies. I have not managed to make Enhanced++ dependencies work via stanfordnlp.</p>

<p>But the negations will still appear if you use basicDependencies !</p>

<p>Here is the output I obtained using stanfordnlp and your example sentence. It is a DependencyGraph object, not pretty, but it is unfortunately always the case when we use the very deep CoreNLP tools. You will see that between nodes 4 and 5 ('not' and 'born'), there is and edge 'neg'. </p>

<pre><code>node {
  sentenceIndex: 0
  index: 1
}
node {
  sentenceIndex: 0
  index: 2
}
node {
  sentenceIndex: 0
  index: 3
}
node {
  sentenceIndex: 0
  index: 4
}
node {
  sentenceIndex: 0
  index: 5
}
node {
  sentenceIndex: 0
  index: 6
}
node {
  sentenceIndex: 0
  index: 7
}
node {
  sentenceIndex: 0
  index: 8
}
edge {
  source: 2
  target: 1
  dep: ""compound""
  isExtra: false
  sourceCopy: 0
  targetCopy: 0
  language: UniversalEnglish
}
edge {
  source: 5
  target: 2
  dep: ""nsubjpass""
  isExtra: false
  sourceCopy: 0
  targetCopy: 0
  language: UniversalEnglish
}
edge {
  source: 5
  target: 3
  dep: ""auxpass""
  isExtra: false
  sourceCopy: 0
  targetCopy: 0
  language: UniversalEnglish
}
edge {
  source: 5
  target: 4
  dep: ""neg""
  isExtra: false
  sourceCopy: 0
  targetCopy: 0
  language: UniversalEnglish
}
edge {
  source: 5
  target: 7
  dep: ""nmod""
  isExtra: false
  sourceCopy: 0
  targetCopy: 0
  language: UniversalEnglish
}
edge {
  source: 5
  target: 8
  dep: ""punct""
  isExtra: false
  sourceCopy: 0
  targetCopy: 0
  language: UniversalEnglish
}
edge {
  source: 7
  target: 6
  dep: ""case""
  isExtra: false
  sourceCopy: 0
  targetCopy: 0
  language: UniversalEnglish
}
root: 5

---
Tokens of first sentence
word: ""Barack""
pos: ""NNP""
value: ""Barack""
before: """"
after: "" ""
originalText: ""Barack""
beginChar: 0
endChar: 6
tokenBeginIndex: 0
tokenEndIndex: 1
hasXmlContext: false
isNewline: false

word: ""Obama""
pos: ""NNP""
value: ""Obama""
before: "" ""
after: "" ""
originalText: ""Obama""
beginChar: 7
endChar: 12
tokenBeginIndex: 1
tokenEndIndex: 2
hasXmlContext: false
isNewline: false

word: ""was""
pos: ""VBD""
value: ""was""
before: "" ""
after: "" ""
originalText: ""was""
beginChar: 13
endChar: 16
tokenBeginIndex: 2
tokenEndIndex: 3
hasXmlContext: false
isNewline: false

word: ""not""
pos: ""RB""
value: ""not""
before: "" ""
after: "" ""
originalText: ""not""
beginChar: 17
endChar: 20
tokenBeginIndex: 3
tokenEndIndex: 4
hasXmlContext: false
isNewline: false

word: ""born""
pos: ""VBN""
value: ""born""
before: "" ""
after: "" ""
originalText: ""born""
beginChar: 21
endChar: 25
tokenBeginIndex: 4
tokenEndIndex: 5
hasXmlContext: false
isNewline: false

word: ""in""
pos: ""IN""
value: ""in""
before: "" ""
after: "" ""
originalText: ""in""
beginChar: 26
endChar: 28
tokenBeginIndex: 5
tokenEndIndex: 6
hasXmlContext: false
isNewline: false

word: ""Hawaii""
pos: ""NNP""
value: ""Hawaii""
before: "" ""
after: """"
originalText: ""Hawaii""
beginChar: 29
endChar: 35
tokenBeginIndex: 6
tokenEndIndex: 7
hasXmlContext: false
isNewline: false

word: "".""
pos: "".""
value: "".""
before: """"
after: """"
originalText: "".""
beginChar: 35
endChar: 36
tokenBeginIndex: 7
tokenEndIndex: 8
hasXmlContext: false
isNewline: false
</code></pre>

<h2>2. Using CoreNLP via NLTK package</h2>

<p>I will not go into details on this one, but there is also a solution to access the CoreNLP server via the NLTK library , if all else fails. It does output the negations, but requires a little more work to start the servers.
Details on <a href=""https://github.com/nltk/nltk/wiki/Stanford-CoreNLP-API-in-NLTK"" rel=""noreferrer"">this page</a></p>

<h2>EDIT</h2>

<p>I figured I could also share with you the code to get the DependencyGraph into a nice list of 'dependency, argument1, argument2' in a shape similar to what stanfordnlp outputs.</p>

<pre><code>from stanfordnlp.server import CoreNLPClient

text = ""Barack Obama was not born in Hawaii.""

# set up the client
with CoreNLPClient(annotators=['tokenize','ssplit','pos','depparse'], timeout=60000, memory='16G') as client:
    # submit the request to the server
    ann = client.annotate(text)

    # get the first sentence
    sentence = ann.sentence[0]

    # get the dependency parse of the first sentence
    dependency_parse = sentence.basicDependencies

    #print(dir(sentence.token[0])) #to find all the attributes and methods of a Token object
    #print(dir(dependency_parse)) #to find all the attributes and methods of a DependencyGraph object
    #print(dir(dependency_parse.edge))

    #get a dictionary associating each token/node with its label
    token_dict = {}
    for i in range(0, len(sentence.token)) :
        token_dict[sentence.token[i].tokenEndIndex] = sentence.token[i].word

    #get a list of the dependencies with the words they connect
    list_dep=[]
    for i in range(0, len(dependency_parse.edge)):

        source_node = dependency_parse.edge[i].source
        source_name = token_dict[source_node]

        target_node = dependency_parse.edge[i].target
        target_name = token_dict[target_node]

        dep = dependency_parse.edge[i].dep

        list_dep.append((dep, 
            str(source_node)+'-'+source_name, 
            str(target_node)+'-'+target_name))
    print(list_dep)
</code></pre>

<p>It ouputs the following </p>

<pre><code>[('compound', '2-Obama', '1-Barack'), ('nsubjpass', '5-born', '2-Obama'), ('auxpass', '5-born', '3-was'), ('neg', '5-born', '4-not'), ('nmod', '5-born', '7-Hawaii'), ('punct', '5-born', '8-.'), ('case', '7-Hawaii', '6-in')]
</code></pre>
",""
"56621767","2019-06-16 18:58:02","1","","56621702","<p>One approach might be then to negate those with <code>do</code> or <code>should</code> followed by any chars and a must positive lookahead with <code>i need</code>: </p>

<pre><code>^(?=.*i need.*)(?!.*should.*i need.*|.*do.*i need.*).*$
</code></pre>

<h3><a href=""https://regex101.com/r/BIGF3j/1"" rel=""nofollow noreferrer"">Demo</a></h3>
",""
"56592530","2019-06-14 06:23:24","1","","56592351","<p>I'm not 100% sure I understand, but if you're looking to get all the entries in a list after a specific entry, the easiest way would be to do:</p>

<pre><code>foundthing=False
result = []
for i in list:
    if foundthing:
        result.append(i)
    if i == ""Thing I'm Looking For"":
        foundthing = True
</code></pre>

<p>Adding this to your code results in:</p>

<pre><code>import nltk
from nltk.corpus.reader import TaggedCorpusReader
reader = TaggedCorpusReader('cookbook', r'.*\.pos')
train_sents=reader.tagged_sents()
tags = []
foundit=False
for sent in train_sents:
    #i change the line here
    for (word,tag) in nltk.bigrams(sent):
        if foundit: #If the entry is after 'DTDEF'
            tags.append(foundit) #Add it to the resulting list of tags.
        if tag[1]=='DTDEF': #If the entry is 'DTDEF'
            foundit=True #Set the 'After DTDEF' flag.

fd = nltk.FreqDist(tags)
fd.tabulate()
</code></pre>

<p>Hope this helps.</p>
",""
"56560157","2019-06-12 10:37:46","0","","56556837","<p>There might be some confusion what a <em>tokenizer</em> does respectively what <em>tokenization</em> is. Tokenization splits a string into smaller entities such as words or single characters. Therefore, these are also referred  to as <em>tokens</em>. <a href=""https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization"" rel=""nofollow noreferrer"">Wikipedia</a> provides a nice example: </p>

<p><code>The quick brown fox jumps over the lazy dog</code> becomes:</p>

<pre><code>&lt;sentence&gt;
  &lt;word&gt;The&lt;/word&gt;
  &lt;word&gt;quick&lt;/word&gt;
  ...
  &lt;word&gt;dog&lt;/word&gt;
&lt;/sentence&gt;
</code></pre>

<p>Lemmatization (<em>grouping together the inflected forms of a word</em> -> <a href=""https://en.wikipedia.org/wiki/Lemmatisation"" rel=""nofollow noreferrer"">link</a>) or stemming (<em>process of reducing inflected (or sometimes derived) words to their word stem</em> -> <a href=""https://en.wikipedia.org/wiki/Stemming"" rel=""nofollow noreferrer"">link</a>) is something you do during preprocessing. Tokenization can be a part of a preprocessing process before or after (or both) lemmatization and stemming.</p>

<p>Anyhow, Keras is no framework for fully fletched text-preprocessing. Hence, you feed already cleaned, lemmatized etc. data into Keras. <strong><em>Regarding your first question: No, Keras does not provide such functionallity like lemmatization or stemming.</em></strong></p>

<p>What Keras understands under <em>Text preprocessing</em> like <a href=""https://keras.io/preprocessing/text/"" rel=""nofollow noreferrer"">here in the docs</a> is the functionallity to prepare data in order to be fed to a Keras-model (like a Sequential model). This is for example why the <a href=""https://keras.io/preprocessing/text/#Tokenizer"" rel=""nofollow noreferrer"">Keras-Tokenizer</a> does this:</p>

<blockquote>
  <p>This class allows to vectorize a text corpus, by turning each text
  into either a sequence of integers (each integer being the index of a
  token in a dictionary) or into a vector where the coefficient for each
  token could be binary, based on word count, based on tf-idf...</p>
</blockquote>

<p>By for example vectorizing your input strings and transforming them into numeric data you can feed them as input to a, in case of Keras, neural network. </p>

<p>What <code>text_to_sequence</code> means can be extracted from this: <em>[...]sequence of integers (each integer being the index of a token in a dictionary)[...]</em>.
This means that your former strings can afterwards be a sequence (e.g. array) of numeric integers instead of actual words.</p>

<p>Regarding this you should also take a look on what Keras Sequential models are (e.g. <a href=""https://keras.io/getting-started/sequential-model-guide/"" rel=""nofollow noreferrer"">here</a>) since they take seuqences as input.</p>

<p>Additionally, <code>text_to_word_sequence()</code> (<a href=""https://keras.io/preprocessing/text/#text_to_word_sequence"" rel=""nofollow noreferrer"">docs</a>) also provides such tokenization, but does not vectorize your data into numeric vectors and returns an array of your tokenized strings.</p>

<blockquote>
  <p>Converts a text to a sequence of words (or tokens).</p>
</blockquote>
",""
"56427280","2019-06-03 12:21:13","1","","56425454","<p>Alright, I have figured it out myself by now. In case anyone is ever stumbling upon that problem too, here is my solution:
ibanChunk, as it is called in my case, is a list of tuples, but a Chunk turned out to be a tree, not a tuple, so I used that as my advantage, here is my code:</p>

<pre class=""lang-py prettyprint-override""><code>for elem in ibanChunk:
    if isinstance(elem, nltk.Tree):
        ibanString = """"
        for (text, tag) in elem:
            ibanString += text
        chunkList.append(ibanString)
</code></pre>

<p>And there you have the text of all chunks in a list as Strings.</p>
",""
"56415481","2019-06-02 13:27:00","0","","56415464","<p>In your particular case, if the sentences are unrelated, call each sentence a ""document"".</p>

<p>In some more detail, TF means a term is frequent in the current sample (to avoid the term ""document""). DF indicates that a term is frequent in every sample. The quotient TF/DF, then, returns a high number for terms which are rare in the entire collection -- suggesting they are significant -- and a low number for terms which are common.</p>
",""
"56398033","2019-05-31 15:38:13","3","","56392852","<p>I believe you need to use one-hot vectoring for the 'location' feature. 
One-hot vectors for the given data would be,</p>

<p>London - 100</p>

<p>Manchester - 010</p>

<p>Edinburg - 001</p>

<p>Vector length is the number of cities you have in there. Note that each bit here would be a feature.
Categorical data is usually converted to one-hot vectors before feeding to a machine learning algorithm.</p>

<p>Once this is done you can concat the whole row into a 1D array and then feed that to the classifier.</p>
",""
"56316787","2019-05-26 19:38:31","0","","56315645","<p>In spaCy, tokenizer checks for exceptions before splitting text. 
You need to add an exception to tokenizer to treat your symbols as full tokens. </p>

<p>Your code should look like this:</p>

<pre class=""lang-py prettyprint-override""><code>import spacy
from spacy.attrs import ORTH, LEMMA

sent = ""&lt;sos&gt; Hello There! &lt;eos&gt;""

nlp = spacy.load('en_core_web_sm')

nlp.tokenizer.add_special_case('&lt;sos&gt;', [{ORTH: ""&lt;sos&gt;""}])
nlp.tokenizer.add_special_case('&lt;eos&gt;', [{ORTH: ""&lt;eos&gt;""}])

for token in nlp(sent):
    print(token.text)
</code></pre>

<p>You can read more about it here:
<a href=""https://spacy.io/api/tokenizer#add_special_case"" rel=""noreferrer"">https://spacy.io/api/tokenizer#add_special_case</a></p>
",""
"56304583","2019-05-25 11:52:55","0","","56304480","<p>As I understand you, you are trying to build a Hidden Markov Model, using frequencies of n-grams (word tupels of length n). Maybe just try out a more efficiently searchable data structure, for example a nested dictionary. It could be of the form </p>

<pre><code>{ID_word1:{ID_word1:x1,... ID_wordk:y1}, ...ID_wordk:{ID_word1:xn, ...ID_wordk:yn}}.
</code></pre>

<p>This would mean that you only have k**2 dictionary entries for tuples of 2 words (google uses up to 5 for automatic translation) where k is the cardinality of V, your (finite) vocabulary. This should boost your performance, since you do not have to search a growing list of tuples. x and y are representative for the occurrence counts, which you should increment when encountering a tuple. (Never use in-built function count()!)</p>
",""
"56278823","2019-05-23 15:44:52","0","","56267537","<p>Not sure how to address your questions but I'd recommend you carefully read the documentation for the Stanford CoreNLP: <a href=""https://nlp.stanford.edu/software/lex-parser.shtml"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/software/lex-parser.shtml</a></p>

<p>Within the package there are several grammatical and dependency parsers that you can use. Just looking at the grammatical parses, there is an option to retrieve k-best parses and if you process dependencies on them you will most likely get different dependencies for each.</p>

<p>This has to do both with inaccuracies in the parser and ambiguities in natural language.</p>
",""
"56260103","2019-05-22 15:17:59","0","","42798439","<p>According to the founders of LSA, <a href=""http://www.scholarpedia.org/article/Latent_semantic_analysis"" rel=""nofollow noreferrer"">stemming is not necessary</a>. Though, I think there is general disagreement in the literature about this. I have read a few papers where stemming was found to improve results for a given information retrieval task.</p>

<p>Generally, there is <a href=""https://mimno.infosci.cornell.edu/papers/schofield_tacl_2016.pdf"" rel=""nofollow noreferrer"">recent research</a> that shows stemming does not help in topic modeling and may even hurt topic coherence.</p>
",""
"56224318","2019-05-20 15:47:57","1","","56221883","<p>From what I see here, the quoted work is not using a Naive Bayes classifier at all; the approach is different to what you're suggesting.</p>

<p>The proposed approach there is to train individual n-gram based language models for <em>each</em> dialect to be classified. To classify which dialect a given input is in, the input text is scored with each language model. The lower the perplexity according to an LM, the higher the probability. Therefore, if the LM trained on dialect A assigns lower perplexity (i.e. higher probability) to an input than dialect B does, it is more likely that the input text is in dialect A.</p>

<blockquote>
  <p>Perplexity is the inverse probability of some text normalized by the number of words (<a href=""https://web.stanford.edu/class/cs124/lec/languagemodeling.pdf"" rel=""nofollow noreferrer"">source</a>).</p>
</blockquote>

<p>For a sentence W,<br>
<code>Perplexity(W) = P(W)^(-1/N)</code>, where <code>N</code> is the number of words in the sentence, and <code>P(W)</code> is the probability of W according to an LM. </p>

<p>Therefore, the probability, and hence the perplexity, of the input according to each language model is computed, and these are compared to choose the most likely dialect.</p>
",""
"56217275","2019-05-20 08:46:12","0","","56198930","<p>From what I've seen so far, SpaCy is not super-great at doing what you want it to do. Instead, I am using a 3rd party library called <a href=""https://pypi.org/project/pyinflect/"" rel=""nofollow noreferrer"">pyinflect</a>, which is intended to be used as an extension to SpaCy.</p>

<p>While it isn't perfect, I think it will work better than your current approach.</p>

<p>I'm also considering another 3rd-party library called <a href=""https://pypi.org/project/inflect/"" rel=""nofollow noreferrer"">inflect</a>, which might be worth checking out, as well.</p>
",""
"56186497","2019-05-17 12:30:49","4","","56181929","<p>I think that you have 2 errors there.
First you should fix your path - add "".""</p>

<p>from:</p>

<pre><code>output_path = Path(""/images/dependency_plot.svg"")
</code></pre>

<p>to:</p>

<pre><code>output_path = Path(""./images/dependency_plot.svg"")
</code></pre>

<p>The second error is in this line</p>

<pre><code>svg = displacy.render(sentence_nlp, style=""dep"", jupyter=True)
</code></pre>

<p>I think you need to remove <code>jupyter=True</code> to be able to write it in the svg file. Otherwise you will be given error like <code>TypeError: write() argument must be str, not None</code></p>

<p>This works for me:</p>

<pre><code>import spacy
from spacy import displacy
from pathlib import Path

nlp = spacy.load('en_core_web_sm', parse=True, tag=True, entity=True)

sentence_nlp = nlp(""John go home to your family"")
svg = displacy.render(sentence_nlp, style=""dep"")

output_path = Path(""./images/dependency_plot.svg"") # you can keep there only ""dependency_plot.svg"" if you want to save it in the same folder where you run the script 
output_path.open(""w"", encoding=""utf-8"").write(svg)
</code></pre>
",""
"56030163","2019-05-07 20:39:06","1","","56024637","<p>Firstly, a lemma is not a ""root"" word as you thought it to be. It's just a form that exist in the dictionary and for English in NLTK <code>WordNetLemmatizer</code> the dictionary is WordNet and as long as the dictionary entry is in WordNet it is a lemma, there are entries for ""political"" and ""politics"", so they're valid lemma:</p>

<pre><code>from itertools import chain
print(set(chain(*[ss.lemma_names() for ss in wn.synsets('political')])))
print(set(chain(*[ss.lemma_names() for ss in wn.synsets('politics')])))
</code></pre>

<p>[out]:</p>

<pre><code>{'political'}
{'political_sympathies', 'political_relation', 'government', 'politics', 'political_science'}
</code></pre>

<p>Maybe there are other tools out there that can do that, but I'll try this as a first.</p>

<p>First, stem all lemma names and group the lemmas with the same stem:</p>

<pre><code>from collections import defaultdict

from wn import WordNet
from nltk.stem import PorterStemmer

porter = PorterStemmer()
wn = WordNet()

x = defaultdict(set)
i = 0
for lemma_name in wn.all_lemma_names():
    if lemma_name:
        x[porter.stem(lemma_name)].add(lemma_name)
        i += 1
</code></pre>

<p><strong>Note:</strong> <code>pip install -U wn</code></p>

<p>Then as a sanity check, we check that the no. of lemmas > no. of groups:</p>

<pre><code>print(len(x.keys()), i)
</code></pre>

<p>[out]:</p>

<pre><code>(128442, 147306)
</code></pre>

<p>Then we can take a look at the groupings:</p>

<pre><code>for k in sorted(x):
    if len(x[k]) &gt; 1:
        print(k, x[k])
</code></pre>

<p>It seems to do what we need to group the words together with their ""root word"", e.g. </p>

<pre><code>poke {'poke', 'poking'}
polar {'polarize', 'polarity', 'polarization', 'polar'}
polaris {'polarisation', 'polarise'}
pole_jump {'pole_jumping', 'pole_jumper', 'pole_jump'}
pole_vault {'pole_vaulter', 'pole_vault', 'pole_vaulting'}
poleax {'poleaxe', 'poleax'}
polem {'polemically', 'polemics', 'polemic', 'polemical', 'polemize'}
police_st {'police_state', 'police_station'}
polish {'polished', 'polisher', 'polish', 'polishing'}
polit {'politics', 'politic', 'politeness', 'polite', 'politically', 'politely', 'political'}
poll {'poll', 'polls'}
</code></pre>

<p>But if we look closer there is some confusion:</p>

<pre><code>polit {'politics', 'politic', 'politeness', 'polite', 'politically', 'politely', 'political'}
</code></pre>

<p>So I would suggest the next step is </p>

<p><strong>to loop through the groupings again and run some semantics and check the ""relatedness"" of the words and split the words that might not be related</strong>, maybe try something like Universal Sentence Encoder, e.g. <a href=""https://colab.research.google.com/drive/1BM-eKdFb2G2zXqNt3dHgVm4gH8PaPJOq"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1BM-eKdFb2G2zXqNt3dHgVm4gH8PaPJOq</a> (might not be a trivial task)</p>

<p>Or do <strong>some manual work and reorder the groupings</strong>. (The heavy lifting of the work is already done by the porter stemmer in the grouping, now it's time to do some human work)</p>

<p>Then you'll have to somehow find the root among each group of words (i.e. prototype/label for the cluster).</p>

<p>Finally using the resource of groups of words you've created, you can not ""find the root word. </p>
",""
"55993906","2019-05-05 16:08:10","3","","55984480","<p>Just create a dictionary and replace the tags, as you considered doing. The nltk's universal tagset support is provided by the module <code>nltk/tag/mapping.py</code>. It relies on a set of mapping files, which you will find in <code>NLTK_DATA/taggers/universal_tagset</code>. For example, in <code>en-brown.map</code> you'll find lines like this, which map a whole bunch of tags to <code>PRT</code>, <code>ABX</code> to <code>DET</code>, and so on:</p>

<pre><code>ABL     PRT
ABN     PRT
ABN-HL  PRT
ABN-NC  PRT
ABN-TL  PRT
ABX     DET
AP      ADJ
</code></pre>

<p>These files are read into a dictionary that is used for the translation. By creating a mapping file in the same format you could use the nltk's functions to perform the translation, but honestly if your task is simply to produce a corpus in the Universal format, I would just do the translation by hand. But not through ""search-replace"": Work with the tuples provided by the nltk's corpus readers, and just replace the POS tags by direct lookup in your mapping dictionary.</p>

<p>Let's assume you know how to persuade an nltk <code>TaggedCorpusReader</code> to read your corpus, and you now have an <code>stts_corpus</code> reader object with methods <code>tagged_words()</code>, <code>tagged_sents()</code>, etc. You also need the mapping dictionary, whose keys are STTS tags and values are universal tags; if <code>ABL</code> was an STTS tag, <code>mapping_dict[""ABL""]</code> should return the value <code>PRT</code>. Your remapping then goes something like this:</p>

<pre><code>for filename in stts_corpus.fileids():
    with open(""new_dir/""+filename, ""w"") as output:
        for word, tag in stts_corpus.tagged_words():
            output.write(word+""/""+mapping_dict[tag]+"" "")
        output.write(""\n"")
</code></pre>

<p>And that's really all there is to it, unless you want to add luxuries like breaking the text into lines.</p>
",""
"55955202","2019-05-02 14:59:55","2","","55955027","<p><code>list.remove</code> is extremely slow for this purpose - it searches the entire list for the given value each time, and then deletes it. It has to effectively iterate through the entire list for each element that is removed, causing quadratic runtime.</p>

<p>A better solution here would be the following list expression:</p>

<pre><code>sents = [
    sent for sent in sents
    if test_lang(sent[0]) == 'AR' and not re.search(p, sent)
]
</code></pre>

<p>This filters the list in linear time.</p>

<p>(I would guess that the first filter has to operate on a very long list and discards most of it? While the second one receives a much smaller list and doesn't have to remove very much? This would explain why the first one is so much slower.)</p>
",""
"55953565","2019-05-02 13:27:33","0","","55864933","<p>The issue was analysed by the spaCy team and they've come up with a solution.
Here's the fix : <a href=""https://github.com/explosion/spaCy/pull/3646"" rel=""nofollow noreferrer"">https://github.com/explosion/spaCy/pull/3646</a></p>

<p>Basically, when the lemmatization rules were applied, a set was used to return a lemma. Since a set has no ordering, the returned lemma could change in between python session.</p>

<hr>

<p>For example in my case, for the noun ""leaves"", the potential lemmas were ""leave"" and ""leaf"". Without ordering, the result was random - it could be ""leave"" or ""leaf"".</p>
",""
"55776787","2019-04-20 19:03:58","2","","55776532","<p>Try this. The example takes you through putting the data into the correct format so that you are able to create a dataframe. You need to create a list containing lists of your data. This data must be uniformly organised. Then you can create your dataframe. Comment if you need more help</p>

<pre><code>from textblob import TextBlob as blob
import pandas as pd
from string import punctuation

def remove_punctuation(text):
    return ''.join(c for c in text if c not in punctuation)

data = []

text = '''
He an thing rapid these after going drawn or. 
Timed she his law the spoil round defer. 
In surprise concerns informed betrayed he learning is ye. 
Ignorant formerly so ye blessing. He as spoke avoid given downs money on we. 
Of properly carriage shutters ye as wandered up repeated moreover. 
Inquietude attachment if ye an solicitude to. 
Remaining so continued concealed as knowledge happiness. 
Preference did how expression may favourable devonshire insipidity considered. 
An length design regret an hardly barton mr figure.
Those an equal point no years do. Depend warmth fat but her but played. 
Shy and subjects wondered trifling pleasant. 
Prudent cordial comfort do no on colonel as assured chicken. 
Smart mrs day which begin. Snug do sold mr it if such. 
Terminated uncommonly at at estimating. 
Man behaviour met moonlight extremity acuteness direction. '''

text = remove_punctuation(text)
text = text.replace('\n', '')

text = blob(text).parse()
text = text.split(' ')

for tagged_word in text:

    t_word = tagged_word.split('/')
    data.append([t_word[0], t_word[1], t_word[2], t_word[3]])

df = pd.DataFrame(data, columns = ['Words', 'POS', 'Parse Chunker', 'Deep Parsing'] )

</code></pre>

<p>Result</p>

<pre><code>Out[18]: 
          Words   POS Parse Chunker Deep Parsing
0            He   PRP          B-NP            O
1            an    DT          I-NP            O
2         thing    NN          I-NP            O
3         rapid    JJ        B-ADJP            O
4         these    DT             O            O
5         after    IN          B-PP        B-PNP
6         going   VBG          B-VP        I-PNP
7         drawn   VBN          I-VP        I-PNP
8            or    CC             O            O
9         Timed   NNP          B-NP            O
10          she   PRP          I-NP            O
11          his  PRP$          I-NP            O
12          law    NN          I-NP            O
13          the    DT             O            O
14        spoil    VB          B-VP            O
15        round    NN          B-NP            O
16        defer    VB          B-VP            O
17           In    IN          B-PP        B-PNP
18     surprise    NN          B-NP        I-PNP
19     concerns   NNS          I-NP        I-PNP
20     informed   VBN          B-VP        I-PNP
21     betrayed   VBN          I-VP        I-PNP
22           he   PRP          B-NP        I-PNP
23     learning   VBG          B-VP        I-PNP
24           is   VBZ          I-VP            O
25           ye   PRP          B-NP            O
26     Ignorant   NNP          I-NP            O
27     formerly    RB          I-NP            O
28           so    RB          I-NP            O
29           ye   PRP          I-NP            O
..          ...   ...           ...          ...
105          no    DT             O            O
106          on    IN          B-PP        B-PNP
107     colonel    NN          B-NP        I-PNP
108          as    IN          B-PP        B-PNP
109     assured   VBN          B-VP        I-PNP
110     chicken    NN          B-NP        I-PNP
111       Smart   NNP          I-NP        I-PNP
112         mrs   NNS          I-NP        I-PNP
113         day    NN          I-NP        I-PNP
114       which   WDT             O            O
115       begin    VB          B-VP            O
116        Snug   NNP          B-NP            O
117          do   VBP          B-VP            O
118        sold   VBN          I-VP            O
119          mr    NN          B-NP            O
120          it   PRP          I-NP            O
121          if    IN          B-PP        B-PNP
122        such    JJ          B-NP        I-PNP
123  Terminated   NNP          I-NP        I-PNP
124  uncommonly    RB        B-ADVP            O
125          at    IN          B-PP        B-PNP
126          at    IN          I-PP        I-PNP
127  estimating   VBG          B-VP        I-PNP
128         Man    NN          B-NP        I-PNP
129   behaviour    NN          I-NP        I-PNP
130         met   VBD          B-VP            O
131   moonlight    NN          B-NP            O
132   extremity    NN          I-NP            O
133   acuteness    NN          I-NP            O
134   direction    NN          I-NP            O

[135 rows x 4 columns]
</code></pre>
",""
"55605413","2019-04-10 05:40:35","0","","55528385","<p>As I didn't get any meaningful help, I just came up with a temporary solution of my own. Find the sentence with the entity tag ""DATE"", and remove the stop words.</p>

<p>For example, the sentence 'The ideal candidate must have at least 6 years of experience in the field.' becomes into the following:</p>

<p>'ideal candidate 6 years experience field'</p>

<p>Obviously I can refine this further, but for now this will do.</p>
",""
"55495341","2019-04-03 12:34:18","0","","55482342","<p>Given a certain pandas df you can stem the contents by applying a stemming function on the whole df after tokenizing the words.</p>
<p>For this, I exemplarily used the snowball stemmer from nltk.</p>
<pre><code>from nltk.stem.snowball import SnowballStemmer
englishStemmer=SnowballStemmer(&quot;english&quot;) #define stemming dict
</code></pre>
<p>And this tokenizer:</p>
<pre><code>from nltk.tokenize import WhitespaceTokenizer as w_tokenizer
</code></pre>
<p>Define your function:</p>
<pre><code>def stemm_texts(text):
    return [englishStemmer.stem(w) for w in w_tokenizer.tokenize(str(text))]
</code></pre>
<p>Apply the function on your df:</p>
<pre><code>df = df.apply(lambda y: y.map(stemm_texts, na_action='ignore'))
</code></pre>
<p>Note that I additionally added the NaN ignore part.</p>
<p>You might want to detokenize again:</p>
<pre><code>from nltk.tokenize.treebank import TreebankWordDetokenizer

detokenizer = TreebankWordDetokenizer()
df = df.apply(lambda y: y.map(detokenizer.detokenize, na_action='ignore'))
</code></pre>
",""
"55475013","2019-04-02 12:39:14","4","","55474457","<p>You can use the apply method, which as the name suggests will apply the given function to every row of the dataframe or series. This will return a series, which you can add as a new column to your dataframe</p>

<pre><code>df['Proper Nouns'] = df['POS_Description'].apply(
    lambda row: [i[0] for i in row if i[1] == 'NNP'])
</code></pre>

<p>I am assuming the POS_Description dtype to be a list of tuples. </p>
",""
"55362465","2019-03-26 16:55:41","0","","55352301","<p><a href=""https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer.build_tokenizer"" rel=""nofollow noreferrer""><code>build_tokenizer()</code></a> would exactly serve this purpose. </p>

<p>Try this!</p>

<pre><code>tokenizer = lambda docs: [vectorizer.build_tokenizer()(doc) for doc in docs]

tokenizer(corpus)
</code></pre>

<p>Output:</p>

<pre><code>[['This', 'is', 'the', 'first', 'document'],
 ['This', 'document', 'is', 'the', 'second', 'document'],
 ['And', 'this', 'is', 'the', 'third', 'one'],
 ['Is', 'this', 'the', 'first', 'document']]
</code></pre>

<p>‚ÄãOne liner solution would be </p>

<pre><code>list(map(vectorizer.build_tokenizer(),corpus))
</code></pre>
",""
"55313236","2019-03-23 11:26:50","3","","55307452","<p>You can easily find the noun chunk that contains the token you've identified by checking if the token is in one of the noun chunk spans:</p>

<pre><code>doc = nlp(""Autonomous cars and magic wands shift insurance liability toward manufacturers"")
interesting_token = doc[7] # or however you identify the token you want
for noun_chunk in doc.noun_chunks:
    if interesting_token in noun_chunk:
        print(noun_chunk)
</code></pre>

<p>The output is not correct with en_core_web_sm and spacy 2.0.18 because <code>shift</code> isn't identified as a verb, so you get:</p>

<blockquote>
  <p>magic wands shift insurance liability</p>
</blockquote>

<p>With en_core_web_md, it's correct:</p>

<blockquote>
  <p>insurance liability</p>
</blockquote>

<p>(It makes sense to include examples with real ambiguities in the documentation because that's a realistic scenario (<a href=""https://spacy.io/usage/linguistic-features#noun-chunks"" rel=""nofollow noreferrer"">https://spacy.io/usage/linguistic-features#noun-chunks</a>), but it's confusing for new users if they're ambiguous enough that the analysis is unstable across versions/models.)</p>
",""
"55192428","2019-03-16 00:50:26","0","","55141126","<p>The lemmatization does in fact work for Norwegian as it's specified in the <a href=""https://spacy.io/usage/adding-languages#lemmatizer"" rel=""nofollow noreferrer"">docs</a>: all forms in <a href=""https://github.com/explosion/spaCy/blob/master/spacy/lang/nb/lemmatizer/lookup.py"" rel=""nofollow noreferrer"">lookup.py</a> are lemmatized. Try for instance <code>doc = nlp(u'ei')</code> and you'll see that the lemma of <code>ei</code> is <code>en</code>.</p>

<p>Now, the file you are referring to, <code>verbs_wordforms.py</code>, documents exceptions in case the part-of-speech (POS) tag is a verb. However, the blank model <code>Norwegian()</code> does not have a POS tagger and so that particular exception for <code>heter</code> is never triggered.</p>

<p>So the solution is either to use a model which has a POS tagger, or to add your specific exceptions to <code>lookup.py</code>. You'll see for instance that if you'd add there the line <code>'heter': 'hete',</code> that your blank model would find <code>hete</code> as lemma for <code>heter</code>.</p>

<p>Finally, note that there's been a lot of <a href=""https://github.com/explosion/spaCy/issues/3082"" rel=""nofollow noreferrer"">work and discussion</a> about publishing a pre-trained Norwegian model in spaCy - but it looks like that is still a bit of a work in progress.</p>
",""
"55176765","2019-03-15 06:26:27","0","","55174358","<p>Longtime elasticsearch use TF/IDF algorithm to find similarity in queries. But number versions ago is changed to BM25 as more efficient. You can read the information in <a href=""https://www.elastic.co/guide/en/elasticsearch/reference/current/similarity.html"" rel=""nofollow noreferrer"">the documentation</a>. <a href=""https://rebeccabilbro.github.io/intro-doc-similarity-with-elasticsearch/"" rel=""nofollow noreferrer"">And good article explains what is elastic search and how to the similarity in ES</a>.</p>

<p>You can also write a custom algorithm to elasticsearch. <a href=""http://stefansavev.com/blog/custom-similarity-for-elasticsearch/"" rel=""nofollow noreferrer"">Here a good article about how to do</a>.</p>
",""
"55118103","2019-03-12 09:30:21","0","","55090520","<p>TIGER only contains newspaper text, so there aren't a lot of non-3rd person verbs. A statistical model is not going to be able to learn much about verb endings it's barely seen.</p>

<p>Things that could help:</p>

<ul>
<li><p>Choose a better tagger. The one you mentioned has a somewhat limited set of features, especially in terms of prefixes and suffixes. I'm not familiar with all the options in NLTK (there may be some that are equally good), but as an alternative I'd suggest trying marmot for tagging, plus lemming for lemmatization, from <a href=""http://cistern.cis.lmu.de"" rel=""nofollow noreferrer"">http://cistern.cis.lmu.de</a>, which are relatively fast and easy to use. There are also plenty of newer tagging approaches that may be a little better, but it's hard to tell how they compare because many recent evaluations are based on the UD German corpus, which unfortunately has relatively low quality annotation.</p></li>
<li><p>Taggers rely on context, so when you add some new training data it helps to add whole sentences, or at least whole phrases.</p></li>
<li><p>Even a large manually-annotated corpus isn't going to have coverage of plenty of word forms, so lexical resources are very helpful for lemmatization. I'd take a look a <a href=""https://github.com/rsennrich/zmorge"" rel=""nofollow noreferrer"">Zmorge</a>, a morphological analyzer based on data from German wiktionary. If your main goal is lemmatization, I'd recommend starting with something like Zmorge and backing off to statistical models for ambiguous or unseen words.</p></li>
</ul>
",""
"55096235","2019-03-11 06:23:42","1","","55094637","<p>At this time there is not Python support for constituency parses which is what you want.  This is just returning the dependency parses (a different type of parse).</p>

<p>You can use <code>stanfordnlp</code> to communicate with a Java server and get constituency parses in that manner.</p>

<p>There is example code here for accessing the constituency parses:</p>

<p><a href=""https://stanfordnlp.github.io/stanfordnlp/corenlp_client.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/stanfordnlp/corenlp_client.html</a></p>
",""
"55030408","2019-03-06 19:01:51","4","","55030231","<p>Use a <a href=""https://docs.python.org/2/library/collections.html#collections.defaultdict"" rel=""nofollow noreferrer"">defaultdict</a>:</p>

<pre><code>from collections import defaultdict

List = ["", -&gt; ','"", "". -&gt; '!'"", "". -&gt; '.'"", "". -&gt; '?'"", ""CC -&gt; 'but'"", ""CD -&gt; 'hundred'"",
      ""CD -&gt; 'one'"", ""DT -&gt; 'the'"", ""EX -&gt; 'There'"",""IN -&gt; 'as'"", ""IN -&gt; 'because'"",
      ""IN -&gt; 'if'"", ""IN -&gt; 'in'"", ""JJ -&gt; 'Sure'"", 'MD -&gt; ""\'ll""', ""MD -&gt; 'ca'"",
      ""MD -&gt; 'can'"", ""MD -&gt; 'will'"", ""MD -&gt; 'would'"", ""NN -&gt; 'Applause'"",
      ""NN -&gt; 'anybody'"", ""NN -&gt; 'doubt'"", ""NNP -&gt; 'Syria'"",
      ""NNS -&gt; 'Generals'"", ""NNS -&gt; 'people'"", ""NNS -&gt; 'states'"",  ""PRP -&gt; 'it'"",
      ""PRP$ -&gt; 'our'"",  ""RB -&gt; 'there'"", ""RBR -&gt; 'more'"", ""RP -&gt; 'out'"", ""TO -&gt; 'to'"",
      ""UH -&gt; 'Oh'"", ""UH -&gt; 'Wow'"", ""VB -&gt; 'stop'"", ""VB -&gt; 'want'"", ""VBD -&gt; 'knew'"",
      ""VBD -&gt; 'was'"", ""VBG -&gt; 'allowing'"", ""VBG -&gt; 'doing'"", ""VBG -&gt; 'going'"",
      ""VBN -&gt; 'called'"", ""VBP -&gt; 'take'"", 'VBZ -&gt; ""\'s""', ""VBZ -&gt; 'is'"", 
      ""WDT -&gt; 'that'"", ""WP -&gt; 'what'""]

data = defaultdict(set)
for key, value in (_.split('-&gt;') for _ in List):
  d[key.strip()].add(value.strip().replace(""'"", '').replace('""', ''))
print(dict(data))
</code></pre>

<p>This results in:</p>

<pre><code>{',': {','}, '.': {'.', '!', '?'}, 'CC': {'but'}, 'CD': {'hundred','one'}, 'DT': {'the'}, 'EX': {'There'}, 'IN': {'in', 'because', 'as', 'if'}, 'JJ': {'Sure'}, 'MD': {'will', 'ca', 'll', 'can', 'would'}, 'NN': {'Applause', 'doubt', 'anybody'}, 'NNP': {'Syria'}, 'NNS': {'Generals', 'states', 'people'}, 'PRP': {'it'}, 'PRP$': {'our'}, 'RB': {'there'}, 'RBR': {'more'}, 'RP': {'out'}, 'TO': {'to'}, 'UH': {'Wow', 'Oh'}, 'VB': {'want', 'stop'}, 'VBD': {'was', 'knew'}, 'VBG': {'going', 'allowing', 'doing'}, 'VBN': {'called'}, 'VBP': {'take'}, 'VBZ': {'s', 'is'}, 'WDT': {'that'}, 'WP': {'what'}}
</code></pre>
",""
"55022104","2019-03-06 11:32:36","0","","54847574","<p>You should try to train embeding on your own corpus. There are many package: gensim, glove.
You can use embeding from BERT without retraining on your own corpus.</p>

<p>You should know that the probability distribution on different corpus is always different. For example, the count of 'basketball' in posts about food is very different from the count of the term in news about sport, so the gap of word embeding of 'basketball' in those corpus is huge.</p>
",""
"55018475","2019-03-06 08:20:13","0","","55017747","<p>Try using <code>utf-8</code> encding:</p>

<pre><code>with open(""new.txt"", ""w"", encoding='utf-8') as output: ## creates new file but empty
        output.write(str(values))
</code></pre>

<p>You can also use <code>io</code> for python 2 backward compatibility:</p>

<pre><code>import io
with io.open(""new.txt"", ""w"", encoding='utf-8') as output: ## creates new file but empty
    output.write(str(values))
</code></pre>
",""
"54874271","2019-02-25 20:36:44","1","","54874069","<p>I don't think a stemming is what you want to do here. Stemmers are language specific and are based on the common inflectional morphological patterns in that language. For example, in English, you have the infinitival forms of verbs (e.g., ""to walk"") which becomes inflected for tense, aspect, &amp; person/number: I walk vs. She walks (walk+s), I walk vs. walked (walk+ed), also walk+ing, etc. Stemmers codify these stochastic distributions into ""rules"" that are then applied on a ""word"" to change into its stem. In other words, an off-the-shelf stemmer does not exist for your opcodes.</p>

<p>You have two possible solutions: (1) create a dictionary or (2) write your own stemmer. If you don't have too many variants to map, it is probably quickest to just create a custom dictionary where you use all your word variants as keys and the lemma/stem/canonical-form is the value. </p>

<pre><code>addi -&gt; add
addf -&gt; add
multi -&gt; mult
multf -&gt; mult
</code></pre>

<p>If your potential mappings are too numerous to do by hand, then you could write a custom regex stemmer to do the mapping and conversion. Here is how you might do it in R. The following function takes an input word and tries to match it to a pattern representing all the variants of a stem, for all the <code>n</code> stems in your collection. It returns a 1 x <code>n</code> data.frame with 1 indicating presence or 0 indicating absence of variant match.</p>

<pre><code>#' Return word's stem data.frame with each column indicating presence (1) or 
#' absence (0) of stem in that word.
map_to_stem_df &lt;- function(word) {
  ## named list of patterns to match
  stem_regex &lt;- c(add = ""^add[if]$"", 
                  mult = ""^mult[if]$"")

  ## iterate across the stem names
  res &lt;- lapply(names(stem_regex), function(stem) {

    pat &lt;- stem_regex[stem]
    ## if pattern matches word, then 1 else 0
    if (grepl(pattern = pat, x = word))  {
      pat_match &lt;- 1
    } else {
      pat_match &lt;- 0  
    }
    ## create 1x1 data.frame for stem
    df &lt;- data.frame(pat_match) 
    names(df) &lt;- stem
    return(df)
  })
  ## bind all cols into single row data.frame 1 x length(stem_regex) &amp; return
  data.frame(res)

}

map_to_stem_df(""addi"")
#  add mult
#    1    0

map_to_stem_df(""additional"")
# add mult
#   0    0
</code></pre>
",""
"54788211","2019-02-20 14:04:03","0","","54784287","<p>If you look at the source code of WordNetLemmatizer</p>

<pre><code>def lemmatize(self, word, pos=NOUN):
    lemmas = wordnet._morphy(word, pos)
    return min(lemmas, key=len) if lemmas else word
</code></pre>

<p><code>wordnet._morphy</code> returns <code>['us', 'u']</code></p>

<p><code>min(lemmas, key=len)</code> returns the shortest word which is <code>u</code></p>

<p><code>wordnet._morphy</code> uses a rule for nouns which replaces ending <code>""s""</code> with <code>""""</code>.</p>

<p>Here is the list of substitutions</p>

<p><code>[('s', ''),
 ('ses', 's'),
 ('ves', 'f'),
 ('xes', 'x'),
 ('zes', 'z'),
 ('ches', 'ch'),
 ('shes', 'sh'),
 ('men', 'man'),
 ('ies', 'y')]
</code></p>

<p>I don't see a very clean way out.</p>

<p>1) You may write a special rule for excluding all-upper-case words.</p>

<p>2) Or you may add a line <code>us us</code></p>

<p>to the file <code>nltk_data/corpora/wordnet/noun.exc</code></p>

<p>3) You may write your own function to select the longest word (which might be wrong for other words)</p>

<pre><code>from nltk.corpus.reader.wordnet import NOUN
from nltk.corpus import wordnet
def lemmatize(word, pos=NOUN):
    lemmas = wordnet._morphy(word, pos)
    return max(lemmas, key=len) if lemmas else word
</code></pre>
",""
"54748136","2019-02-18 13:12:53","1","","54745482","<p><a href=""https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"" rel=""nofollow noreferrer"">TfidfVectorizer</a> is used on raw documents, while
<a href=""https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html"" rel=""nofollow noreferrer"">TfidfTransformer</a> is used on an existing count matrix, such as one returned by <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"" rel=""nofollow noreferrer"">CountVectorizer</a></p>
",""
"54737936","2019-02-17 21:38:32","0","","54735204","<p><code>nltk.parse.generate.generate</code> does not produce random sentences. It returns an iterator which produces each possible sentence exactly once until the requested number of sentences are generated. The maximum derivation depth can be restricted, but the generation is depth-first; it does not order the sentences by derivation depth.</p>

<p>You can find the <a href=""https://www.nltk.org/_modules/nltk/parse/generate.html"" rel=""nofollow noreferrer"">source code here</a>; it's not difficult to see what it is doing.</p>

<p>So it is entirely deterministic, and never repeats itself. If you want a (potentially infinite) stream of randomly selected sentences, you will have to write your own generator.</p>
",""
"54706978","2019-02-15 10:10:31","1","","54072496","<p>I managed to solve the problem now. 
Here comes the solution for those who might have an interest in it.</p>

<p>In short, the trick was to pick the right columns from the numpy arrays in <code>get_predict_proba_fn_of_class</code>. While I had five independent classification scores that do not add up to one, I had to add the negative scores for every label's classification scores in a new column (e.g. for 0.67 I added 1-0.67) and then pick the original and new column.</p>

<pre><code>from lime.lime_text import LimeTextExplainer, TextDomainMapper
print(encoder.classes_)
##https://lime-ml.readthedocs.io/en/latest/lime.html#module-lime.lime_text


def flatten_predict(i):
    global model   
    # catch single string input and convert to list
    if i.__class__ != list:
        i = [i]
    # list for predictions
    predStorage = []
    # loop through input list and predict
    for textInput in i:
        textInput = preprocess(textInput)
        textInput = make_predictable(textInput)
        pred = model.predict(textInput)
        pred = np.append(pred, 1-pred, axis=1)
        predStorage.extend(pred)

    return np.asarray(predStorage)


def get_predict_proba_fn_of_class(strings):
    def rewrapped_predict(strings): 
        pred = flatten_predict(strings)
        index = np.where(pred==label)[1][0]
        preds = pred[:, index::5]
        return preds

    return rewrapped_predict


string=""Der Arzt wei√ü, was er tut und hat mir alles genau erkl√§rt.""
print(""Simple model prediction:"", model.predict(make_predictable(preprocess(string))))

labels_to_explain = flatten_predict(string)
print(""labels_to_explain:"", labels_to_explain)

explanation_for_label = {}
for index, label in enumerate(labels_to_explain[0]):
    if index &lt; (len(labels_to_explain[0])/2):
        actual_classes = [encoder.classes_[index], 'None']
        explainer = LimeTextExplainer(class_names=actual_classes)
        wrapped = get_predict_proba_fn_of_class(string)  # function returns function!
        explanation_for_label[label] = explainer.explain_instance(string, wrapped)
        explanation_for_label[label].show_in_notebook()
</code></pre>
",""
"54661633","2019-02-13 02:37:34","1","","54636433","<p><code>Tf</code> method can give importance to common words more than necessary rather use <code>Tfidf</code> method which gives importance to words that are rare and unique in the particular document in the dataset.</p>

<p>Also before selecting Kbest rather train on the whole set of features and then use feature importance to get the best features.</p>

<p>You can also try using <code>Tree Classifiers</code> or <code>XGB</code> ones to better model but <code>SVC</code> is also very good classifier.</p>

<p>Try using <code>Naive Bayes</code> as the minimum standard of <code>f1 score</code> and try improving your results on other classifiers with the help of <code>grid search</code>.</p>
",""
"54661544","2019-02-13 02:25:14","4","","54660886","<p>The problem is you are using <code>fit_transform</code> here which make the <code>TfidfTransform()</code> fit on the <code>test data</code> and then transform it.</p>

<p>Rather use <code>transform</code> method on it.</p>

<p>Also, you should use <code>TfidfVectorizer</code></p>

<p>In my opinion the code should be:</p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
tfidf_transformer = TfidfVectorizer()
# X_counts = count_vect.fit_transform(train_v['doc_text']) 
X_tfidf = tfidf_transformer.fit_transform(train_v['doc_text']) 
x_train_tfidf, x_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(X_tfidf, label_vs, test_size=0.33, random_state=9000)
sgd = SGDClassifier(loss='hinge', penalty='l2', random_state=42, max_iter=25, tol=None, fit_intercept=True, alpha = 0.000009  )

# X_counts_test = count_vect.fit_transform(test_v['doc_text']) 
X_tfidf_test = tfidf_transformer.transform(test_v['doc_text']) 
predictions_test = clf.predict(X_tfidf_test)

</code></pre>

<p>Also, why are you using <code>count_vect</code> I think it has no usability here and in <code>train_test_split</code> you are using <code>X_tfidf_r</code> which is not mentioned anywhere.</p>
",""
"54636721","2019-02-11 18:17:44","0","","54628104","<p>A parse tree is not generated for q2 because the sentence is not in the language of the grammar, i.e., the goal symbol <code>S</code> does not derive the sentence.</p>

<p>You should write the sentence down on paper and then manually try to construct a parse tree for it. You'll find that it can't be done, and the specific ways in which it fails to be possible should suggest ways that the grammar needs to change in order to make it possible.</p>

<p>For example, here's one problem (not the only one): in the grammar, NNP derives both 'Big' and 'Data' (and nothing else derives either of them), so (roughly speaking) the sentence begins with <code>NNP NNP</code>, and yet <code>S</code> is incapable of deriving a form that starts with <code>NNP NNP</code>.</p>
",""
"54618624","2019-02-10 16:41:41","2","","54613100","<p>Pyparsing makes quick work of nested expression parsing.</p>

<pre><code>import pyparsing as pp

LPAR, RPAR = map(pp.Suppress, ""()"")
expr = pp.Forward()
label = pp.Word(pp.alphas.upper()+'-') | ""''"" | ""``"" | "".""
word = pp.Literal(""."") | ""''"" | ""``"" | pp.Word(pp.printables, excludeChars=""()"")

expr &lt;&lt;= LPAR + label + (word | pp.OneOrMore(expr)) + RPAR

sample = """"""
(TOP (S (NP (DT The)
            (NNP Fulton)
            (NNP County)
            (NNP Grand)
            (NNP Jury))
        (VP (VBD said)
            (NP (NNP Friday))
            (SBAR (-NONE- 0)
                  (S (NP (DT an)
                         (NN investigation)
                         (PP (IN of)
                             (NP (NP (NNP Atlanta))
                                 (POS 's)
                                 (JJ recent)
                                 (JJ primary)
                                 (NN election))))
                     (VP (VBD produced)
                         (NP (`` ``)
                             (DT no)
                             (NN evidence)
                             ('' '')
                             (SBAR (IN that)
                                   (S (NP (DT any)
                                          (NNS irregularities))
                                      (VP (VBD took)
                                          (NP (NN place)))))))))))
     (. .))
""""""

result = pp.OneOrMore(expr).parseString(sample)
print(' '.join(result))
</code></pre>

<p>Prints:</p>

<pre><code>TOP S NP DT The NNP Fulton NNP County NNP Grand NNP Jury VP VBD said NP NNP Friday SBAR -NONE- 0 S NP DT an NN investigation PP IN of NP NP NNP Atlanta POS 's JJ recent JJ primary NN election VP VBD produced NP `` `` DT no NN evidence '' '' SBAR IN that S NP DT any NNS irregularities VP VBD took NP NN place . .
</code></pre>

<p>Normally, parsers like this will use <code>pp.Group(expr)</code> to retain the grouping of the nested elements. But in your case, since you eventually want a flat list anyway, we just leave that out - pyparsing's default behavior is to just return a flat list of the matched strings.</p>
",""
"54434465","2019-01-30 06:25:34","3","","54429050","<p>You can use <code>predict</code> to get clusters. And then use <code>numpy</code> to get all the documents from a specific cluster</p>

<pre><code>clusters = model.fit_predict(X_train)

clusterX = np.where(clusters==0) 

indices = X_train[clusterX]
</code></pre>

<p>So now <code>indices</code> will have all the indices of documents from that cluster</p>
",""
"54382451","2019-01-26 20:29:20","0","","53612938","<p>You are reading the ""wrong"" attribute. <code>word.pos</code> returns the PoS tag id, not the PoS tag string. To do what you want, just replace <code>word.pos</code> with <code>word.pos_</code>.</p>

<p>The following code will work fine:</p>

<pre><code>import spacy
from spacy.lang.fr.examples import sentences
nlp = spacy.load('en')
mystring = "" I am missing my lovely family a lot.""
exuu = nlp(mystring)
for word in exuu: 
  print(word.text, word.pos_)
</code></pre>
",""
"54325344","2019-01-23 10:40:33","3","","54305070","<p>You are not providing a lot of details, so my answer is going to be similarly general: You original model is making a wrong prediction. Then lime is making a linear approximation of the model. Because of the approximative nature of the linear model, this is not exactly as the original model and deviates from the original model. In your case the original model gives a wrong prediction and the deviation of the linear approximation is - by chance - in the direction of the right answer, so that you get - by chance - the right answer from the approximation although the original model was wrong.</p>
",""
"54054933","2019-01-05 18:22:57","0","","54028477","<p>Input DataFrame:</p>

<pre><code>df
    a   b
0   3   0
1   0   3
2   4   5
</code></pre>

<p>First, find <code>idf</code> of all words,</p>

<pre><code>idf_list = []
for col in list(df.columns):
    total_count = df[col].nonzero()[0][1]
    idf = np.log(len(df) / total_count)
    idf_list.append(round(idf, 3))
</code></pre>

<p>Now, find <code>tf-idf</code> and update the dataframe, </p>

<pre><code>for row in range(len(df)):
    total_doc_words = sum(df.iloc[row].values)
    for col in range(len(df.columns)):
        tf = df.iloc[row, col] / total_doc_words
        df.iloc[row, col] = tf * idf_list[col]
</code></pre>

<p>Output:</p>

<pre><code>df
       a    b
0   0.405   0.000
1   0.000   0.405
2   0.180   0.225
</code></pre>
",""
"53999185","2019-01-01 21:44:31","2","","50598129","<p>The default value of <code>smartirs</code> is None, but if you follow the code, it is equal to <strong>ntc</strong>. </p>

<p><em>But how?</em></p>

<p>First, when you call <code>model = TfidfModel(corpus)</code>, it calculates IDF of the corpus with a function called <code>wglobal</code> which explained in docs as:</p>

<p><code>wglobal</code> is function for global weighting, the default value is <a href=""https://radimrehurek.com/gensim/models/tfidfmodel.html#gensim.models.tfidfmodel.df2idf"" rel=""nofollow noreferrer""><code>df2idf()</code></a>. <code>df2idf</code> is a function that computes IDF for a term with the given document frequency. The default arguman and formula for <code>df2idf</code> is:</p>

<pre><code>df2idf(docfreq, totaldocs, log_base=2.0, add=0.0)
</code></pre>

<p><a href=""https://i.sstatic.net/6EjBv.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/6EjBv.png"" alt=""df2idf formula""></a></p>

<p>which implemented as:</p>

<pre><code>idfs = add + np.log(float(totaldocs) / docfreq) / np.log(log_base)
</code></pre>

<p>One of the smartirs is determined:  document frequency weighting is inverse-document-frequency or <strong><code>idf</code></strong>.</p>

<hr>

<p><code>wlocals</code> by default is <a href=""https://radimrehurek.com/gensim/utils.html#gensim.utils.identity"" rel=""nofollow noreferrer""><code>identity</code></a> function. Term frequency of the corpus passed through the identify function which nothing happened, and the corpus itself return. Hence, another parameter of smartirs, term frequency weighing, is natural or <strong><code>n</code></strong>. Now that we have term frequency and inverse-document-frequency we can compute tfidf:</p>

<p><a href=""https://i.sstatic.net/INxLU.gif"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/INxLU.gif"" alt=""tfidf formula""></a></p>

<hr>

<p><a href=""https://radimrehurek.com/gensim/models/tfidfmodel.html#gensim.models.tfidfmodel.smartirs_normalize"" rel=""nofollow noreferrer""><code>normalize</code></a> by default is true that means after computing TfIDF it normalizes the tfidf vectors. The normalization is done with <code>l2-norm</code> (Euclidean unit norm) which means our last smartirs is cosine or <strong><code>c</code></strong>. This part implemented as:</p>

<pre><code># vec(term_id, value) is tfidf result
length = 1.0 * math.sqrt(sum(val ** 2 for _, val in vec))
normalize_by_length = [(termid, val / length) for termid, val in vec]
</code></pre>

<hr>

<p>When you call <a href=""https://radimrehurek.com/gensim/models/tfidfmodel.html#gensim.models.tfidfmodel.TfidfModel.__getitem__"" rel=""nofollow noreferrer""><code>model[corpus]</code></a> or <a href=""https://radimrehurek.com/gensim/models/tfidfmodel.html#gensim.models.tfidfmodel.TfidfModel.__getitem__"" rel=""nofollow noreferrer""><code>model.__getitem__()</code></a> the following things happen:</p>

<p><code>__getitem__</code> has a <code>eps</code> argument which is a threshold value that will remove all entries that have tfidf-value less than <code>eps</code>. By default, this value is 1e-12. As a result, when you print the vectors only some of them appeared.</p>
",""
"53975238","2018-12-30 04:16:44","2","","53972614","<p>Remember that <a href=""http://hackage.haskell.org/package/transformers-0.5.5.0/docs/Control-Monad-Trans-Reader.html#t:ReaderT"" rel=""nofollow noreferrer""><code>ReaderT r m a</code></a> is a <code>newtype</code> wrapper for <code>r -&gt; m a</code>. Specifically, <code>MonadIO m =&gt; ReaderT (IO a) m b</code> is equivalent to <code>MonadIO m =&gt; IO a -&gt; m b</code>. So let me rephrase your question:</p>

<blockquote>
  <p>Can you convert <code>MonadIO m =&gt; IO a -&gt; m b</code> to <code>MonadIO m =&gt; m a -&gt; m b</code>?</p>
</blockquote>

<p>The answer is <strong>no</strong>, because the <code>IO a</code> appears as an <em>input</em> to the function type. (Sometimes you'll see people say ""in negative position"", which roughly means the same as ""input"".) The important thing here is that converting function inputs works in the opposite direction to converting function outputs.</p>

<hr>

<p>Let's take a step back and think about a more general case. If you have a function <code>a -&gt; b</code> and you want to convert its output to get a function <code>a -&gt; c</code>, you need to be able to convert <code>b</code>s into <code>c</code>s. If you can give me a function which converts <code>b</code>s into <code>c</code>s, I can apply that to values after they come out of the <code>a -&gt; b</code> function.</p>

<pre><code>convertOutput :: (b -&gt; c)  -- the converter function
              -&gt; (a -&gt; b)  -- the function to convert
              -&gt; (a -&gt; c)  -- the resulting converted function
convertOutput f g = \x -&gt; f (g x)
</code></pre>

<p><code>convertOutput</code> is better known as <code>(.)</code>.</p>

<p>Converting a function's input works the opposite way. If you want to convert a function <code>b -&gt; a</code> into a function <code>c -&gt; a</code>, you have to convert <code>c</code>s into <code>b</code>s. If you can give me a function which converts <code>c</code>s into <code>b</code>s, I can apply that to values before they go in to the <code>b -&gt; a</code> function.</p>

<pre><code>convertInput :: (c -&gt; b)  -- the converter function
             -&gt; (b -&gt; a)  -- the function to convert
             -&gt; (c -&gt; a)  -- the resulting converted function
convertInput f g = \x -&gt; g (f x)
</code></pre>

<p>(Occasionally you'll hear the words <em>covariance</em> and <em>contravariance</em> in connection with the idea of converting types. They refer to the idea that that converter functions can go in one of two directions. Functions are covariant in their output parameters and contravariant in their inputs.)</p>

<hr>

<p>Back to the question,</p>

<blockquote>
  <p>Can you convert <code>MonadIO m =&gt; IO a -&gt; m b</code> to <code>MonadIO m =&gt; m a -&gt; m b</code>?</p>
</blockquote>

<p>Hopefully you can see that this question is really asking for a way to turn an <code>m a</code> into <code>IO a</code>. (You have to turn the <code>m a</code> into an <code>IO a</code> in order to feed it to the original function.) <code>MonadIO</code> contains a single method, <code>liftIO :: IO a -&gt; m a</code>, which embeds an <code>IO</code> computation into a ""bigger"" monad which may contain other effects, but that's quite the opposite of what we need. There's no going the other way.</p>

<p>Nor should there be. <code>m a</code> here is a monadic computation which may perform all manner of unknown effects. You can't turn an arbitrary monadic value into an <code>IO</code> without knowing what the effects are. And many (most) monadic effects don't have a straightforward translation into an <code>IO</code> computation; running a <code>State</code> computation, for example, requires a starting value for the state.</p>
",""
"53939660","2018-12-27 04:13:08","1","","53939191","<p>The answer to the question as asked is <code>join :: IO (IO ()) -&gt; IO ()</code>. But the answer to the question I think you <em>should</em> have asked is <code>liftIO :: IO () -&gt; ReaderT (IO String) IO ()</code>. Like this:</p>

<pre><code>userInput :: MonadIO m =&gt; ReaderT (IO String) m String
userInput = ask &gt;&gt;= liftIO -- this liftIO eliminates your need for join

echo :: MonadIO m =&gt; ReaderT (IO String) m ()
echo = userInput &gt;&gt;= liftIO . putStrLn -- this liftIO is just so you can use putStrLn in ReaderT

main :: IO ()
main = runReaderT echo getLine
</code></pre>

<p>Building monadic actions that return monadic actions, and then manually combining the inner actions, is in <em>most</em> cases ignoring the whole point of monad transformers. Instead of having two layers of monadic actions, you should have a single layer which has a transformer version of the outer action on top of the inner action -- that is, instead of working with <code>ReaderT r Foo (IO a)</code> actions, which require manual binds for both the <code>ReaderT r Foo</code> layer and the <code>IO</code> layer, you should be working with <code>ReaderT r (FooT IO) a</code> actions, where just one binding handles the reader, foo, and IO effects at once.</p>
",""
"53934116","2018-12-26 15:43:26","7","","53926860","<p>In this case, what you want should be possible, because a <code>QueryEvent</code> or <code>UpdateEvent</code> is a <code>Method</code>, and a <code>Method</code> is <code>Typeable</code>. <code>Typeable</code> lets us use functions from <code>Data.Typeable</code> to inspect what specific type we have at runtime, which we can't really normally do.</p>

<p>Here's a small, self-contained example that doesn't directly use <code>acid-state</code> but begins to illustrate the idea:</p>

<pre><code>{-# LANGUAGE ViewPatterns #-}
{-# LANGUAGE PatternSynonyms #-}
</code></pre>

<p>These aren't strictly necessary, but make it possible to make nicer syntax for matching on <code>Event</code>s.</p>

<pre><code>import Data.Typeable
</code></pre>

<p>We need functions from this module to access the run-time typing information.</p>

<pre><code>data GetVersion = GetVersion
data GetUser = GetUser String
class Typeable a =&gt; QueryEvent a where
instance QueryEvent GetVersion where
instance QueryEvent GetUser where
</code></pre>

<p>A simplified set of types/classes to emulate what <code>acid-state</code> should produce.</p>

<pre><code>pattern IsEvent p &lt;- (cast -&gt; Just p)
</code></pre>

<p>This ""pattern synonym"" makes it so that we can write <code>IsEvent p</code> on the LHS of a pattern match and have it work the same way as if we had written <code>(cast -&gt; Just p)</code>. This latter is a ""view pattern"" which essentially runs the function <code>cast</code> on the input and then pattern matches it against <code>Just p</code>. <code>cast</code> is a function defined in <code>Data.Typeable</code>: <code>cast :: forall a b. (Typeable a, Typeable b) =&gt; a -&gt; Maybe b</code>.  This means that if we write, for example, <code>(cast -&gt; Just GetVersion)</code>, what happens is that <code>cast</code> tries to convert the argument into a value of type <code>GetVersion</code>, which is then pattern-matched against the value-level <code>GetVersion</code> symbol; if the conversion fails (implying that the event is something else), <code>cast</code> returns <code>Nothing</code>, so this pattern doesn't match.  This lets us write:</p>

<pre><code>exampleFunction :: QueryEvent a =&gt; a -&gt; String
exampleFunction (IsEvent GetVersion) = ""get version""
exampleFunction (IsEvent (GetUser a)) = ""get user "" ++ a
</code></pre>

<p>This then works:</p>

<pre><code>Œª&gt; exampleFunction GetVersion
""get version""
Œª&gt; exampleFunction (GetUser ""foo"")
""get user foo""
</code></pre>

<hr>

<p>Your situation is a bit more complicated, since the (type of) the RHS of the function depends on the type of the input.  We will need some more extensions for this:</p>

<pre><code>{-# LANGUAGE GADTs #-}               -- For type equality
{-# LANGUAGE TypeOperators #-}       -- For type equality
{-# LANGUAGE TypeFamilies #-}        -- For EventResult
{-# LANGUAGE ScopedTypeVariables #-} -- For writing castWithWitness
{-# LANGUAGE TypeApplications #-}    -- For convenience
</code></pre>

<p>We can also add <code>EventResult</code> to our dummy simple <code>QueryEvent</code>:</p>

<pre><code>class Typeable a =&gt; QueryEvent a where
  type EventResult a
instance QueryEvent GetVersion where
  type EventResult GetVersion = Int
instance QueryEvent GetUser where
  type EventResult GetUser = String
</code></pre>

<p>Instead of using <code>cast</code>, we can use</p>

<pre><code>castWithWitness :: forall b a. (Typeable a, Typeable b)
                =&gt; a -&gt; Maybe (b :~: a, b)
castWithWitness x = case eqT @a @b of
                      Nothing -&gt; Nothing
                      Just Refl -&gt; Just (Refl, x)
</code></pre>

<p>The <code>@a</code> and <code>@b</code> are using <code>TypeApplications</code> to apply <code>eqT</code> to the types that <code>castWithWitness</code> was applied to, which are bound via <code>ScopedTypeVariables</code> using the <code>forall</code> in the type signature. <code>castWithWitness</code> is like <code>cast</code>, but in addition to the ""casted"" variable, it returns a proof that the passed-in types are the same.  Unfortunately, this makes it a bit harder to use: the <code>IsEvent</code> pattern synonym can't be used, and the relevant type needs to be passed in directly:</p>

<pre><code>exampleFunction :: forall a. QueryEvent a =&gt; a -&gt; EventResult a
exampleFunction (castWithWitness @GetVersion -&gt; Just (Refl, GetVersion)) = 1
exampleFunction (castWithWitness @GetUser -&gt; Just (Refl, GetUser n)) = n
</code></pre>

<p>This works, because in each case, after matching on <code>Refl</code>, GHC knows on the RHS of the function what <code>a</code> is and can reduce the <code>EventResult</code> type family.</p>
",""
"53880176","2018-12-21 06:30:36","1","","53877017","<p>You are performing the <code>TfidfVectorizer</code> on whole data before <code>train_test_split</code> which may be a reason for increased performance due to ""data leakage"". Since the <code>TfidfVectorizer</code> is learning the vocabulary on your whole data, it is:</p>

<ul>
<li>including words in vocabulary that are not present in train and only present in test (<code>out-of-bag</code> words)</li>
<li>adjusting the <code>tf-idf</code> scores based on data from test words also</li>
</ul>

<p>Try the following:</p>

<pre><code>tweets_train, tweets_test, y_train, y_test = train_test_split(reviews['text'].tolist(), 
                                                  reviews['airline_sentiment'], 
                                                  test_size=0.3, 
                                                  random_state=42)

X_train = v.fit_transform(tweets_train)
X_test = v.transform(tweets_test)
</code></pre>

<p>And then check the performance.</p>

<p><strong>Note</strong>: This may not be the only reason for the performance. Or maybe the dataset is such that simple tf-idf works well for it.</p>
",""
"53877209","2018-12-20 23:02:10","1","","53876024","<p>As Alexis King pointed out in comments, it is standard practice to separate parsing from evaluation.</p>

<p>However, to address the current question, it is possible here to evaluate while parsing in an idiomatic way. The key point is the following: lexical scoping without any context-sensitive rules only ever requires a <code>Reader</code> monad, for scope/type checking and evaluation as well. The reason is in the ""lexical"" property: purely nested scopes have no side effects on other branches of scope structure, hence there should be nothing to be carried around in a state. So it's best to just get rid of the <code>State</code>. </p>

<p>The interesting part is <code>letStarExpr</code>. There, we cannot use <code>many</code> anymore, because it doesn't allow us to handle the newly bound names on each key-value pair. Instead, we can write a custom version of <code>many</code> which uses <code>local</code> to bind a new name on each recursive step. In the code example I just inline this function using <code>fix</code>. </p>

<p>Another note: <code>lift</code> should not be commonly used with <code>mtl</code>; the point of <code>mtl</code> is to eliminate most lifts. The <code>megaparsec</code> exports are already generalized over <code>MonadParsec</code>. Below is a code example with <code>megaparsec</code> 7.0.4, I did the mentioned changes and a few further stylistic ones.</p>

<pre><code>import Control.Monad.Reader
import Data.Map as Map
import Data.Void

import Text.Megaparsec
import qualified Text.Megaparsec.Char as Char
import qualified Text.Megaparsec.Char.Lexer as Lexer

type Env    = Map String Double
type Parser = ReaderT Env (Parsec Void String)

spaceConsumer :: Parser ()
spaceConsumer = Lexer.space Char.space1
                            (Lexer.skipLineComment "";"")
                            (Lexer.skipBlockComment ""#|"" ""|#"")

lexeme = Lexer.lexeme spaceConsumer
symbol = Lexer.symbol spaceConsumer
char   = lexeme . Char.char

parens :: Parser a -&gt; Parser a
parens = between (char '(') (char ')')

num :: Parser Double
num = lexeme $ Lexer.signed (pure ()) Lexer.float

identifier :: Parser String
identifier = try $ lexeme (some Char.letterChar)

keyValuePair :: Parser (String, Double)
keyValuePair = parens ((,) &lt;$&gt; identifier &lt;*&gt; num)

expr :: Parser Double
expr = num &lt;|&gt; var &lt;|&gt; parens (arithExpr &lt;|&gt; letStarExpr)

var :: Parser Double
var = do
  env  &lt;- ask
  name &lt;- identifier
  maybe mzero pure (Map.lookup name env)

arithExpr :: Parser Double
arithExpr =
      (((sum &lt;$ char '+') &lt;|&gt; (product &lt;$ char '*')) &lt;?&gt; ""operation"")
  &lt;*&gt; many (expr &lt;?&gt; ""argument"")

letStarExpr :: Parser Double
letStarExpr = do
  symbol ""let*""
  char '('
  fix $ \go -&gt;
        (char ')' *&gt; expr)
    &lt;|&gt; do {(x, n) &lt;- keyValuePair; local (insert x n) go}

main :: IO ()
main = do
    parseTest (runReaderT expr (fromList [(""x"", 1)]))
              ""(+ (let* ((x 666.0)) x) x)""
    parseTest (runReaderT expr (fromList [(""x"", 1)]))
              ""(+ (let* ((x 666.0)) x) (let* ((w 0.0)) x))""
</code></pre>
",""
"53841530","2018-12-18 21:43:31","0","","53841061","<p>Convert the  problem_definition_stopwords to a string and pass to nltk.sent_tokenize if you are trying to tokenize and get the POS with pos_tag.</p>
",""
"53809540","2018-12-17 05:43:02","0","","53565401","<p>Consider the below sentence :</p>

<blockquote>
  <p>Sands had already begun to trickle into the bottom.</p>
  
  <p>Tree: (ROOT   (S
      (NP (NNP Sands))
      (VP (VBD had)
        (ADVP (RB already))
        (VP (VBN begun)
          (S
            (VP (TO to)
              (VP (VB trickle)
                (PP (IN into)
                  (NP (DT the) (NN bottom))))))))
      (. .)))</p>
  
  <p>dependency parser: [nsubj(begun-4, Sands-1), nsubj:xsubj(trickle-6,
  Sands-1), aux(begun-4, had-2), advmod(begun-4, already-3),
  root(ROOT-0, begun-4), mark(trickle-6, to-5), xcomp(begun-4,
  trickle-6), case(bottom-9, into-7), det(bottom-9, the-8),
  nmod:into(trickle-6, bottom-9), punct(begun-4, .-10)]</p>
</blockquote>

<p>There can be two reasons why dependency parser fails.</p>

<p>1)Here the word ""Sands"" is a Proper noun plural(NNPS) but the POS tagger  output gives NNP which is proper noun, so there is an error in the tagger which in turn propagates to the dependency parser as it uses POS to generate dependencies"".
To handle this case you can train the POS tagger with the sentences it fails on.</p>

<p>2)The context of the sentence may be completely new to dependency parser, as most of the parsers like spacy , stanford , nltk etc are trained ML models so in  order to handle this case you can train the dependency parser separately with new sentences.</p>

<p>you can refer this link to understand how to train POS tagger and Dependency parser:
<a href=""https://spacy.io/usage/training#section-tagger-parser"" rel=""nofollow noreferrer"">https://spacy.io/usage/training#section-tagger-parser</a></p>

<p>Hope it answers your questions.</p>
",""
"53771730","2018-12-13 23:53:36","0","","53753614","<p>The SciKit Learn library has a fairly simple cosine metric. While I agree the library is large and can seem overwhelming you can dip into small parts.</p>

<p>I'm not exactly sure what you are trying to achieve by comparing things in the way you suggest, but if you are trying to get the cosine similarity between documents <em>represented by keywords</em> in a corpus, you first need (as Marmikshah points out) to have a vector representation of the docs in keyword terms (dimensions).</p>

<p>e.g. </p>

<pre><code>import logging
import numpy
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

logging.basicConfig(level=logging.DEBUG,
                    filename='test.log', filemode='w')

dataset = ['the cat sat on the mat',
          'the rat sat in the hat',
          'the hat sat on the bat']


vectorizer = TfidfVectorizer()
X_tfidf = vectorizer.fit_transform(dataset)

# ...you say you are already at this point here...

sims = cosine_similarity(X_tfidf, X_tfidf)
rank = list(reversed(numpy.argsort(sims[0])))

logging.debug(""\nTdidf: \n%s"" % X_tfidf.toarray())
logging.debug(""\nSims: \n%s"", sims)
logging.debug(""\nRank: \n%s"", rank)
</code></pre>

<p>Normally e.g. in a search, you'd first vectorise the corpus in advance, then you vectorise the search query and get the sims of its representation:</p>

<pre><code>Y_tfidf = vectorizer.fit_transform(search_query)
sims = cosine_similarity(Y_tfidf, X_tfidf)
</code></pre>

<p>Then rank and pick/present the top documents.</p>

<p>I modified this X,Y to cross reference documents within the corpus instead above as X, X.</p>
",""
"53762797","2018-12-13 13:19:51","0","","53762436","<p>You can do:</p>

<pre><code>words = [[('icosmos', 'JJ'), ('cosmology', 'NN'), ('calculator', 'NN'), ('with', 'IN'), ('graph', 'JJ')], [('generation', 'NN'), ('the', 'DT'), ('expanding', 'VBG'), ('universe', 'JJ')], [('american', 'JJ'), ('institute', 'NN')]]

new_list = []
for i in words:
    temp = [j[0] for j in i if j[1].startswith(""NN"")]
    new_list.append(temp)

print(new_list)
</code></pre>

<p>Output</p>

<pre><code>[['cosmology', 'calculator'], ['generation'], ['institute']]
</code></pre>
",""
"53760227","2018-12-13 10:53:42","0","","53755893","<p>This is because things like <code>ents</code> or <code>chunks</code> are Spans i.e. collections of tokens. Hence you need to iterate over their individual tokens to get their attributes like <code>tag</code> or <code>tag_</code></p>

<pre><code>&gt;&gt;&gt; doc = nlp(u'Mr. Best flew to New York on Saturday morning.')
&gt;&gt;&gt; [(X.text, X.tag_) for X in doc.ents]
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""&lt;stdin&gt;"", line 1, in &lt;listcomp&gt;
AttributeError: 'spacy.tokens.span.Span' object has no attribute 'tag_'
&gt;&gt;&gt; [(X.text, X.tag_) for Y in doc.ents for X in Y]
[('Best', 'NNP'), ('New', 'NNP'), ('York', 'NNP'), ('Saturday', 'NNP'), ('morning', 'NN')]
</code></pre>
",""
"53752907","2018-12-12 23:22:23","1","","53746225","<p>You can use the TrueCaseAnnotator to fix case issues:</p>

<p><a href=""https://stanfordnlp.github.io/CoreNLP/truecase.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/CoreNLP/truecase.html</a></p>

<p>In general you probably just want to use TokensRegex and write rules patterns to handle these templates.  More info here:</p>

<p><a href=""https://stanfordnlp.github.io/CoreNLP/tokensregex.html"" rel=""nofollow noreferrer"">https://stanfordnlp.github.io/CoreNLP/tokensregex.html</a></p>
",""
"53739970","2018-12-12 09:33:50","0","","53737431","<p>Try using <a href=""https://spacy.io/"" rel=""nofollow noreferrer"">spacy</a>
It supports 34+ languages and it has a lemmatizer.</p>
",""
"53703473","2018-12-10 10:13:00","7","","53692117","<p>Try this-</p>

<pre><code>word_list=[]
for i in range(len(unfiltered_tokens)):
    word_list.append([])
for i in range(len(unfiltered_tokens)): 
    for word in unfiltered_tokens[i]:
        if word[1:].isalpha():
            word_list[i].append(word[1:]) 
</code></pre>

<p>then after do </p>

<pre><code>tagged_tokens=[]
for token in word_list:
    tagged_tokens.append(nltk.pos_tag(token))
</code></pre>

<p>You will get your desired results! Hope this helped.</p>
",""
"53583415","2018-12-02 18:41:27","1","","53582922","<p>Right here is your problem: For each sentence, you read <em>the entire corpus</em> with the <code>words()</code> method. No wonder it's taking a long time.</p>

<pre><code>for sent in self.sents():
    print(time.time())
    counts['sents'] += 1

    for word in self.words():
        counts['words'] += 1
        tokens[word] += 1
</code></pre>

<p>In fact a sentence is already tokenized into words, so this is what you meant:</p>

<pre><code>for sent in self.sents():
    print(time.time())
    counts['sents'] += 1

    for word in sent:
        counts['words'] += 1
        tokens[word] += 1
</code></pre>
",""
"53465438","2018-11-25 07:15:13","0","","23117979","<p>I think that research has made a lot of advances in that area and now the distance between the meaning of sentences can be calculated via several methods thanks to the development of word vectors and transformers:</p>

<ol>
<li><p><strong>Google universal sentence encoder (USE)</strong>: <a href=""https://tfhub.dev/google/universal-sentence-encoder/2"" rel=""nofollow noreferrer"">https://tfhub.dev/google/universal-sentence-encoder/2</a></p></li>
<li><p><strong>Infersent</strong> by facebook: <a href=""https://github.com/facebookresearch/InferSent"" rel=""nofollow noreferrer"">https://github.com/facebookresearch/InferSent</a></p></li>
<li><p><strong>Averaging the word vectors</strong> (with cosine similarity).</p></li>
<li><p><strong>Spacy</strong> also provide a similarity between two sentences based on word vectors: <a href=""https://spacy.io/usage/spacy-101"" rel=""nofollow noreferrer"">https://spacy.io/usage/spacy-101</a></p></li>
<li><p><strong>ELMo</strong>: <a href=""https://github.com/HIT-SCIR/ELMoForManyLangs"" rel=""nofollow noreferrer"">https://github.com/HIT-SCIR/ELMoForManyLangs</a></p></li>
<li><p><strong>Bert</strong>: <a href=""https://github.com/google-research/bert"" rel=""nofollow noreferrer"">https://github.com/google-research/bert</a></p></li>
<li><p><strong>ALBERT</strong>: <a href=""https://github.com/google-research/ALBERT"" rel=""nofollow noreferrer"">https://github.com/google-research/ALBERT</a></p></li>
<li><p><strong>RoBERTa</strong>: <a href=""https://ai.facebook.com/blog/roberta-an-optimized-method-for-pretraining-self-supervised-nlp-systems/"" rel=""nofollow noreferrer"">https://ai.facebook.com/blog/roberta-an-optimized-method-for-pretraining-self-supervised-nlp-systems/</a></p></li>
<li><p><strong>XLNET</strong>: <a href=""https://github.com/zihangdai/xlnet"" rel=""nofollow noreferrer"">https://github.com/zihangdai/xlnet</a></p></li>
<li><p><strong>ELECTRA</strong>: <a href=""https://github.com/google-research/electra"" rel=""nofollow noreferrer"">https://github.com/google-research/electra</a></p></li>
</ol>

<p>etc</p>
",""
"53456278","2018-11-24 07:56:51","6","","53453944","<h1>Abstract</h1>

<p>Use <code>Either</code> for normal <em>success/error</em> APIs. It's defined in the <em>base</em> library, so it doesn't push other dependencies on a consumer. Also, this is one of the most basic Haskell types, so 'everyone' understands how it works.</p>

<p>Only use <code>ExceptT</code> if you specifically need to combine <code>Either</code> with another monad (such as, for example <code>IO</code>). This type is defined in the <em>transformers</em> library, so pushes an extra dependency on consumers. Additionally, monad transformers is a more advanced feature of Haskell, so you can't expect everyone to understand how to use it.</p>

<h1>Speculation on reasons</h1>

<p>I wasn't around when those decisions were made, but <a href=""https://www.reddit.com/r/haskell/comments/3ded39/why_cant_we_have_an_eithert_in_transformers/"" rel=""noreferrer"">it seems that there are various historical reasons for the confusion</a>. Haskell is an <em>old</em> language (older than Java!), so even though efforts have been made to streamline it and rectify old mistakes, some still remain. As far as I can tell, the <code>Either</code>/<code>ExceptT</code> confusion is one of those situations.</p>

<p>I'm <em>speculating</em> that <code>Either</code> is older than the concept of monad transformers, so I imagine that the type <code>Either</code> was introduced to the <em>base</em> library early in the history of Haskell.</p>

<p>The same thing seems to be the case with <code>Maybe</code>.</p>

<p>Other monads, likes e.g. <em>Reader</em> and <em>State</em> seem to have been introduced (or at least 'retconned') together with their monad transformers. For example, <code>Reader</code> is just a <em>special case</em> of <code>ReaderT</code>, where the 'other' <code>Monad</code> is <code>Identity</code>:</p>

<pre><code>type Reader r = ReaderT r Identity
</code></pre>

<p>The same goes for <code>StateT</code>:</p>

<pre><code>type State s = StateT s Identity
</code></pre>

<p>That's the general pattern for many of the monads defined in the <em>transformers</em> library. <code>ExceptT</code> just follows the pattern by defining <code>Except</code> as the special case of <code>ExceptT</code>.</p>

<p>There are exceptions to that pattern. For example, <code>MaybeT</code> doesn't define <code>Maybe</code> as a special case. Again, I believe that this is for historical reasons; <code>Maybe</code> was probably around long before anyone started work on the <em>transformers</em> library.</p>

<p>The story about <code>Either</code> seems even more convoluted. As far as I can tell, there <em>was</em>, originally, an <code>EitherT</code> monad transformer, but apparently (I forget the details) there was something wrong with the way that it behaved (it probably broke some laws), so it was replaced with another transformer called <code>ErrorT</code>, which again turned out to be wrong. Third time's the charm, I suppose, so <code>ExceptT</code> was introduced.</p>

<p>The <code>Control.Monad.Trans.Except</code> module follows the pattern of most other monad transformers by defining the 'uneffectful' special case using a type alias:</p>

<pre><code>type Except e = ExceptT e Identity
</code></pre>

<p>I suppose it does that because it can, but it may be unfortunate, because it's confusing. There's definitely prior art that suggests that a monad transformer doesn't have to follow that pattern (e.g. <code>MaybeT</code>), so I think it would have been better if the module hadn't done that, but it does, and that's where we are.</p>

<p>I would essentially ignore the <code>Except</code> type and use <code>Either</code> instead, but use <code>ExceptT</code> if a transformer is required.</p>
",""
"53402088","2018-11-20 21:48:38","7","","53327804","<p>Would something like the following work for you? I recognize there are some tweaks that need to be made to make this fully useful (checking for upper/lower case; it will also return the word ahead in the sentence rather than the one behind if there is a tie) but hopefully it is useful enough to get you started:</p>

<pre><code>import nltk
from nltk.tokenize import MWETokenizer

def smart_tokenizer(sentence, target_phrase):
    """"""
    Tokenize a sentence using a full target phrase.
    """"""
    tokenizer = MWETokenizer()
    target_tuple = tuple(target_phrase.split())
    tokenizer.add_mwe(target_tuple)
    token_sentence = nltk.pos_tag(tokenizer.tokenize(sentence.split()))

    # The MWETokenizer puts underscores to replace spaces, for some reason
    # So just identify what the phrase has been converted to
    temp_phrase = target_phrase.replace(' ', '_')
    target_index = [i for i, y in enumerate(token_sentence) if y[0] == temp_phrase]
    if len(target_index) == 0:
        return None, None
    else:
        return token_sentence, target_index[0]


def search(text_tag, tokenized_sentence, target_index):
    """"""
    Search for a part of speech (POS) nearest a target phrase of interest.
    """"""
    for i, entry in enumerate(tokenized_sentence):
        # entry[0] is the word; entry[1] is the POS
        ahead = target_index + i
        behind = target_index - i
        try:
            if (tokenized_sentence[ahead][1]) == text_tag:
                return tokenized_sentence[ahead][0]
        except IndexError:
            try:
                if (tokenized_sentence[behind][1]) == text_tag:
                    return tokenized_sentence[behind][0]
            except IndexError:
                continue

x, i = smart_tokenizer(sentence='My problem was with DELL Customer Service',
                       target_phrase='DELL Customer Service')
print(search('NN', x, i))

y, j = smart_tokenizer(sentence=""Good for everyday computing and web browsing."",
                       target_phrase=""everyday computing"")
print(search('NN', y, j))
</code></pre>

<p><strong>Edit:</strong> I made some changes to address the issue of using an arbitrary length target phrase, as you can see in the <code>smart_tokenizer</code> function. The key there is the <code>nltk.tokenize.MWETokenizer</code> class (for more info see: <a href=""https://stackoverflow.com/questions/5532363"">Python: Tokenizing with phrases</a>). Hopefully this helps. As an aside, I would challenge the idea that <code>spaCy</code> is <em>necessarily</em> more elegant - at some point, someone has to write the code to get the work done. This will either that will be the <code>spaCy</code> devs, or you as you roll your own solution. Their API is rather complicated so I'll leave that exercise to you.</p>
",""
"53371297","2018-11-19 09:04:21","0","","53370715","<p>You are using <code>CountVectorizer</code> which requires an iterable of strings. Something like:</p>

<pre><code>datas = ['First sentence', 
         'Second sentence', ...
          ...
         'Yet another sentence']
</code></pre>

<p>But your data is a list of lists, which is why the error occurs. You need to make the inner lists as strings for the CountVectorizer to work. You can do this:</p>

<pre><code>datas = [' '.join(map(str, x)) for x in datas]
</code></pre>

<p>This will result in <code>datas</code> like this:</p>

<pre><code>['1 2 4 6 7', '2 3', '5 6 8 3 5', '2', '93 23 4 5 11 3 5 2']
</code></pre>

<p>Now this form is consumable by <code>CountVectorizer</code>. But even then you will not get proper results, because of the default <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"" rel=""nofollow noreferrer""><code>token_pattern</code> in CountVectorizer</a>:</p>

<blockquote>
  <p><strong>token_pattern</strong> : ‚Äô(?u)\b\w\w+\b‚Äô</p>
  
  <p>string Regular expression denoting what constitutes a
  ‚Äútoken‚Äù, only used if analyzer == 'word'. The default regexp select
  tokens of <strong>2 or more alphanumeric characters</strong> (punctuation is completely
  ignored and always treated as a token separator)</p>
</blockquote>

<p>In order for it to consider your numbers as words, you will need to change it so that it can accept single letters as words by doing this:</p>

<pre><code>vectorizer = CountVectorizer(token_pattern=r""(?u)\b\w+\b"")
</code></pre>

<p>Then it should work. But now your numbers are changed into strings</p>
",""
"53347340","2018-11-17 01:19:33","0","","53258578","<p>When you say ""whether this approach could work"", I presume you mean does merging all the relevant documents into one and vectorising present a valid model. I would guess it depends what you are going to try to do with that model.</p>

<p>I'm not much of a mathematician, but I imagine that this is like averaging the scores for all your documents into one vector space, so you have lost some of the shape of the space the original vector space occupied by the individual relevant documents. So you have tried to make a ""master"" or ""prototype"" document which is mean to represent a topic? </p>

<p>If you are then going to do something like similarity matching with test documents, or classification by distance comparison then you may have lost some of the subtlety of the original documents' vectorisation. There may be more facets to the overall topic than the averages represent.</p>

<p>More specifically, imagine your original ""relevant corpus"" has two clusters of documents because there are actually two main sub-topics represented by different groups of important features. Later while doing classification, test documents could match either of those clusters individually - again because they are close to one of the two sub-topics. By averaging the whole ""relevant corpus"" in this case you would end up with a single document that was half-way between both of these clusters, but not accurately representing either. Therefore the test presentations might not match at all - depending on the classification technique.</p>

<p>I think it's hard to say without trialling it on proper specific corpuses.</p>

<p>Regardless of the validity, below is how it could be implemented. </p>

<p>Note you can also use the TfidfVectorizer to combine the vectorising and Tfidf'ing steps in one. The results are not always the exactly same, but they are in this case.</p>

<p>Also, you say normalise the documents - typically you might normalise the a vector representation before feeding into a classification algorithm which requires a normalised distribution (like SVM). However I think TFIDF naturally normalises so it doesn't appear to have any further effect (I may be wrong here).</p>

<pre><code>import logging
from sklearn import preprocessing
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer

CORPUS = ['this is a relevant document',
          'this one is a relevant text too']
ran_CORPUS = ['the sky is blue',
              'my cat has a furry tail']

doc_CORPUS = ' '.join([str(x) for x in CORPUS])
ran_CORPUS.append(doc_CORPUS)

count_vect = CountVectorizer()
X_counts = count_vect.fit_transform(ran_CORPUS)
tfidf_transformer = TfidfTransformer()
X_tfidf = tfidf_transformer.fit_transform(X_counts)

logging.debug(""\nCount + TdidfTransform \n%s"" % X_tfidf.toarray())

# or do it in one pass with TfidfVectorizer
vectorizer = TfidfVectorizer()
X_tfidf = vectorizer.fit_transform(ran_CORPUS)

logging.debug(""\nTdidfVectoriser \n%s"" % X_tfidf.toarray())

# normalising doesn't achieve much as tfidf is already normalised.
normalizer = preprocessing.Normalizer() 
X_tfidf = normalizer.transform(X_tfidf)
logging.debug(""\nNormalised:\n%s"" % X_tfidf.toarray())


Count + TdidfTransform 
[[0.52863461 0.         0.         0.         0.         0.40204024
  0.         0.         0.         0.52863461 0.         0.
  0.52863461 0.         0.        ]
 [0.         0.4472136  0.         0.4472136  0.4472136  0.
  0.4472136  0.         0.         0.         0.4472136  0.
  0.         0.         0.        ]
 [0.         0.         0.2643173  0.         0.         0.40204024
  0.         0.2643173  0.52863461 0.         0.         0.2643173
  0.         0.52863461 0.2643173 ]]

TdidfVectoriser 
[[0.52863461 0.         0.         0.         0.         0.40204024
  0.         0.         0.         0.52863461 0.         0.
  0.52863461 0.         0.        ]
 [0.         0.4472136  0.         0.4472136  0.4472136  0.
  0.4472136  0.         0.         0.         0.4472136  0.
  0.         0.         0.        ]
 [0.         0.         0.2643173  0.         0.         0.40204024
  0.         0.2643173  0.52863461 0.         0.         0.2643173
  0.         0.52863461 0.2643173 ]]

Normalised:
[[0.52863461 0.         0.         0.         0.         0.40204024
  0.         0.         0.         0.52863461 0.         0.
  0.52863461 0.         0.        ]
 [0.         0.4472136  0.         0.4472136  0.4472136  0.
  0.4472136  0.         0.         0.         0.4472136  0.
  0.         0.         0.        ]
 [0.         0.         0.2643173  0.         0.         0.40204024
  0.         0.2643173  0.52863461 0.         0.         0.2643173
  0.         0.52863461 0.2643173 ]]
</code></pre>
",""
"53340042","2018-11-16 14:43:45","0","","53246564","<p>I have the beginnings of an answer to this, but it's not a simple or straightforward ""do it like this"" recipe.</p>

<p>The method I am attempting to work out in more detail amounts to articulating a number of constraints on well-formed sentences.  There is no way you can enumerate all the possible kinds of noise in a corpus, but you can remove certain kinds of noise with certain kinds of filters, many of them ideally simple to understand and implement.</p>

<p>For example:</p>

<ul>
<li>Discard samples with a very low entropy</li>
<li>Discard samples with characters or character sequences outside of the normal repertoire of English</li>
<li>Discard samples with many repeated words</li>
<li>Discard samples with many finite verbs

<ul>
<li>I cooked up the last two just looking at your single example, but of course, it's impossible to tell whether this will work in the general case without access to more samples, or your entire corpus.</li>
</ul></li>
</ul>

<p>A prototype of this method was <a href=""http://www.lrec-conf.org/proceedings/lrec2016/index.html"" rel=""nofollow noreferrer"">published in the LREC 2016 proceedings</a> (helpfully, <a href=""http://www.lrec-conf.org/proceedings/lrec2016/index.html"" rel=""nofollow noreferrer"">the proceedings</a> are published under a lenient <a href=""https://creativecommons.org/licenses/by-nc/4.0/"" rel=""nofollow noreferrer"">CC BY-NC 4.0</a> license): <a href=""http://www.lrec-conf.org/proceedings/lrec2016/summaries/214.html"" rel=""nofollow noreferrer"">abstract</a> but the submissions were restricted to a maximum of four pages, so the article is by necessity a very brief overview.  My actual materials and scripts are on Github:  <a href=""https://github.com/rcv2/rcv2r1"" rel=""nofollow noreferrer"">https://github.com/rcv2/rcv2r1</a>; but the corpus I used is not redistributable, so there's a piece missing.</p>

<p><a href=""https://github.com/rcv2/rcv2r1/blob/master/appendices.pdf"" rel=""nofollow noreferrer"">Appendix A</a> outlines a brief catalog of proposed constraints.</p>
",""
"53176447","2018-11-06 16:52:53","0","","53173109","<p>As a general rule, fit your vectorizer on the entire corpus of texts to calculate the vocabulary and then transform all text to vectors afterwards.</p>

<p>You are fitting the vectorizer two times, so the second call to <code>fit_transform</code> overwrites the first and updates the vocabulary accordingly. Try fitting on both text fields first to calculate the vocabulary over the whole corpus, and then transform each text field, like this:</p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
import scipy as sp

vectorizer = TfidfVectorizer()
vectorizer.fit(df.message.append(df.text))
X = sp.sparse.hstack( (vectorizer.transform(df.message),
                 df[['feature_1', 'feature_2']].values, vectorizer.transform(df.text)),
                 format='csr')

X_columns = vectorizer.get_feature_names() + df[['feature_1', 'feature_2']].columns.tolist()
</code></pre>

<p>This gives me:</p>

<pre><code>X_columns
Out[51]: ['and', 'another', 'extra', 'is', 'more', 'text', 'the', 'this', 'feature_1', 'feature_2']
</code></pre>

<p>Is that what you're after?</p>
",""
"53166845","2018-11-06 06:39:00","1","","53155057","<p>I suspect the tool you're looking for is <a href=""https://nlp.stanford.edu/software/tregex.html"" rel=""nofollow noreferrer"">Tregex</a>, described in more detail in the power point <a href=""https://nlp.stanford.edu/software/tregex/The_Wonderful_World_of_Tregex.ppt/"" rel=""nofollow noreferrer"">here</a> or the <a href=""https://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/trees/tregex/TregexPattern.html"" rel=""nofollow noreferrer"">Javadoc</a> of the class itself.</p>

<p>In your case, I believe the pattern you're looking for is simply <code>S</code>. So, something like:</p>

<pre><code>tregex.sh ‚ÄúS‚Äù &lt;path_to_file&gt;
</code></pre>

<p>where the file is a Penn Treebank formatted tree -- that is, something like <code>(ROOT (S (NP (NNS dogs)) (VP (VB chase) (NP (NNS cats)))))</code>.</p>

<p>As an aside: I believe the fragment ""<em>, but</em>"" is not actually a sentence, as you've hightlighted in the figure. Rather, the node you've highlighted subsumes the whole sentence ""<em>Richard is working with CoreNLP, but does not really understand what he is doing</em>"". Tregex would then print out this whole sentence as one of the matches. Similarly, ""<em>does not really understand what</em>"" is not a sentence unless it subsumes the entire SBAR: ""<em>does not understand what he is doing</em>"". </p>

<p>If you want just the ""leaf"" sentences (i.e., a sentence that's not subsumed by another sentence), you can try a pattern more like:</p>

<pre><code>S !&gt;&gt; S
</code></pre>

<hr>

<p>Note: I haven't tested the patterns -- use at your own risk!</p>
",""
"53144394","2018-11-04 19:10:13","9","","53144310","<p>If you only want JJ*N you could do something like this:</p>

<pre><code>import re

text = '''paper NN
parallel NN
programming VBG
practical JJ
Greg NNP
Wilson NNP
intended VBD
scientist NN
interested JJ
'''

pattern = re.compile('\w+? JJ\n\w+ NN.?', re.DOTALL)

result = pattern.findall(text)
print(result)
</code></pre>

<p><strong>Output</strong></p>

<pre><code>['practical JJ\nGreg NNP']
</code></pre>

<p><strong>Explanation</strong></p>

<p>The pattern <code>'\w+? JJ\n\w+ NN.?'</code> matches a group of letters <code>\w+</code>, followed by a space followed by JJ followed by a <code>\n</code> followed by another group of letters followed by something with <code>NN</code> prefix. Note that I included both words because I think it might be useful for your purposes.</p>

<p><strong>UPDATE</strong></p>

<p>If you want zero or more adjectives <code>JJ*</code> followed by one or more nouns <code>NN+</code> you could do something like this:</p>

<pre><code>import re

text = '''paper NN
parallel NN
programming VBG
practical JJ
Greg NNP
Wilson NNP
intended VBD
scientist NN
interested JJ
'''

pattern = re.compile('(\w+? JJ\n)*(\w+ NN\w?)+', re.DOTALL)

result = pattern.finditer(text)
for element in result:
    print(element.group())
    print('----')
</code></pre>

<p><strong>Output</strong></p>

<pre><code>paper NN
----
parallel NN
----
practical JJ
Greg NNP
----
Wilson NNP
----
scientist NN
----
</code></pre>
",""
"53074505","2018-10-31 00:02:19","4","","53047808","<p>My output is produced using the <code>stanford-spanish-corenlp-2017-06-09-models.jar</code> which can be downloaded here: <a href=""https://nlp.stanford.edu/software/lex-parser.shtml#Download"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/software/lex-parser.shtml#Download</a></p>

<p>For some reason, using newer versions of the <code>models.jar</code> file create different results. </p>

<p>Make sure and put the Spanish <code>.jar</code> into the folder with the rest of Stanford Core NLP (I used the latest <code>2018-10-05</code>).</p>

<p>Then, when you start the Stanford Core NLP server, make sure and start it in Spanish:</p>

<pre><code> java -mx3g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -serverProperties StanfordCoreNLP-spanish.properties -port 9000 -timeout 15000
</code></pre>

<p>Note that the Spanish instance of the <code>CoreNLPTagger</code> uses a different tag set which is detailed on the <a href=""https://nlp.stanford.edu/software/spanish-faq.shtml"" rel=""nofollow noreferrer"">Spanish FAQ</a> page.</p>

<pre><code>from nltk.parse.corenlp import CoreNLPParser

parser = CoreNLPParser(url='http://localhost:9000')

parsed = parser.raw_parse('si idioma no es elegido entonces elegir espa√±ol por defecto.')

for node in parsed:
    print(node)
</code></pre>

<p>Example output below:</p>

<pre><code>(ROOT
  (sentence
    (S
      (S
        (conj (cs si))
        (sn (grup.nom (nc0s000 idioma)))
        (neg (rn no))
        (grup.verb (vsip000 es)))
      (S (participi (aq0000 elegido))))
    (S
      (sadv (grup.adv (rg entonces)))
      (S
        (infinitiu (vmn0000 elegir))
        (s.a (grup.a (aq0000 espa√±ol)))
        (sp (prep (sp000 por)) (sn (grup.nom (nc0s000 defecto))))))
    (fp .)))
</code></pre>
",""
"53067743","2018-10-30 15:26:56","0","","53066951","<p><code>tfidf.indices</code> are just indexes for feature names in TfidfVectorizer.
Getting words by this indexes from the sentence is a mistake. </p>

<p>You should get columns names for your df as <code>TfidfVec.get_feature_names()</code></p>

<p><a href=""https://i.sstatic.net/mtmP6.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/mtmP6.png"" alt=""enter image description here""></a></p>
",""
"53046888","2018-10-29 13:46:54","0","","53046724","<p>Your problem is with empty strings, namely <code>''</code> so you can use:</p>

<pre><code>tagged = nltk.pos_tag([i for i in sample_list if i])
</code></pre>
",""
"52999917","2018-10-26 00:43:24","0","","52981868","<p>Typically NER relies on preprocessing such as part-of-speech tagging (named entities are typically nouns), so not having this basic information makes the task more difficult and therefore more prone to error. There will be certain patterns that you could look for, such as the one you suggest (although what do you do with sentence-initial named entities?). You could add certain regular expression patterns with prepositions, e.g. (Title_case_token)+ of (the)? (Title_case_token)+ would match ""Leader of the Free World"", ""Prime Minister of the United Kindom"", and ""Alexander the Great"". You might also want to consider patterns to match acronyms such as ""S.N.C.F."", ""IBM"", ""UN"", etc. A first step is probably to look for some lexical resources (i.e. word lists) like country names, first names, etc., and build from there.</p>

<p>You could use <a href=""http://spacy.io/"" rel=""nofollow noreferrer"">spaCy</a> (Python) or <a href=""https://nlp.stanford.edu/software/tokensregex.html"" rel=""nofollow noreferrer"">TokensRegex</a> (Java) to do token-based matching (and not use the linguistic features they add to the tokens).</p>
",""
"52808635","2018-10-15 01:27:11","3","","52808220","<p>If I understand correctly, what you would like to achieve now is to have a <code>dict</code> mapping the lowercase form of each word to its most frequent POS tag. In <code>stats</code> you have how many times each POS tag of each word has appeared in the training data, stored in a <code>Counter</code>.</p>

<p>The line <code>max(stats, key=stats.get)</code> is where you're doing it wrong. <code>stats.get(word)</code> returns the <code>Counter</code> related to word <code>word</code>, and <code>Counter</code>s are not comparable in Python 3 (they are, however, in Python 2, but it doesn't really make sense). What's more is that, even if <code>Counter</code>s are comparable, the <code>max</code> function would just return the word with the maximum <code>Counter</code>, which is not what you want.</p>

<p>What we need to do is to use the <code>most_common()</code> method of <code>Counter</code>s. For each word <code>word</code>, <code>get()</code> its <code>Counter</code> (let's name it <code>c</code>) and call <code>c.most_common(1)[0][0]</code> to get its most frequent POS tag. The reason we need the subscripts <code>[0][0]</code> is that <code>most_common(k)</code> returns a list of top-<code>k</code> frequent items, and for each such item it returns a tuple containing the item itself, and its frequency. So the code would look like this:</p>

<pre><code>pos_tags = {word: stats[word].most_common(1)[0][0] for word in stats}
</code></pre>

<p>And <code>pos_tags</code> is the mapping you desired. All you need to do is to finish the rest of your code (that applies this POS tagging method on other files).</p>
",""
"52791289","2018-10-13 09:01:24","1","","52787562","<p>Your <code>penn2wordNet</code> function assigns the noun POS tag to ""us"" although <code>pos_tag(['us'])</code> returns <code>[('us', 'PRP')]</code>. This makes <code>WordNetLemmatizer</code> to treat ""us"" as a noun. You have to add an additional condition to handle personal pronouns.</p>
",""
"52732615","2018-10-10 04:25:08","0","","52659260","<p>I have found this code which helped me to achieve the results i want.    </p>

<pre><code>for subtree in chunked.subtrees(filter=lambda t: t.label() == 'Verb'):
            verblist.append("" "".join([a for (a,b) in subtree.leaves()]))
</code></pre>
",""
"52625609","2018-10-03 11:01:30","0","","52625388","<p>You need to set <code>pos</code> tag parameter from <code>lemmatize</code> as VERB. By default it is NOUN.
So it considers everything as NOUN even if you pass the VERB.</p>

<pre><code>import nltk
words = ['drink', 'drinking']
WNlemma = nltk.WordNetLemmatizer()
refined_list = [WNlemma.lemmatize(t, pos='v') for t in words]
print(refined_list)
</code></pre>

<p>Output: </p>

<pre><code>['drink', 'drink']
</code></pre>
",""
"52549732","2018-09-28 06:40:02","8","","52549113","<p>You need to have a separate <code>collections.Counter</code> for each word. This code uses  <code>defaultdict</code> to create a dictionary of counters, without checking every word to see if it is known.</p>

<pre><code>from collections import Counter, defaultdict

counts = defaultdict(Counter)
for row in file:           # read one line into `row`
    if not row.strip():
        continue           # ignore empty lines
    pos, word, tag = row.split()
    counts[word.lower()][tag] += 1
</code></pre>

<p>That's it, you can now check the most common tag of any word:</p>

<pre><code>print(counts[""food""].most_common(1))
# Prints [(""NN"", 3)] or whatever
</code></pre>
",""
"52537610","2018-09-27 13:00:19","1","","52519433","<p>What you want to do is referred to as <a href=""https://en.wikipedia.org/wiki/Named-entity_recognition"" rel=""nofollow noreferrer"">Named Entity Recognition</a>. </p>

<p>In Python there is a popular library called <a href=""https://spacy.io/api/entityrecognizer"" rel=""nofollow noreferrer"">SpaCy</a> that can be used for that. The standard models are able to detect <a href=""https://spacy.io/api/annotation#named-entities"" rel=""nofollow noreferrer"">18 different entity types</a> which is a fairly good amount. </p>

<p>Persons and company names should be extracted easily, while whole addresses and the industry might be more difficult. Maybe you would have to train your own model on these entity types. SpaCy also provides an API for training your own models. 
Please note, that you need quite a lot of training data to have decent results. Start with 1000 examples per entity type and see if it's sufficient for your needs. POS can be used as a feature. </p>

<p>If your data is unstructured, this is probably one of most suited approaches. If you have more structured data, you could maybe take advantage of that.</p>
",""
"52495132","2018-09-25 09:43:59","0","","52481933","<h1>TL;DR</h1>

<pre><code>from nltk.parse.corenlp import GenericCoreNLPParser

class CoreNLPParser(GenericCoreNLPParser):
    _OUTPUT_FORMAT = 'penn'
    parser_annotator = 'parse'

    def make_tree(self, result):
        return Tree.fromstring(result['parse'])

    def tag_sents(self, sentences, properties=None):
        """"""
        Tag multiple sentences.

        Takes multiple sentences as a list where each sentence is a list of
        tokens.

        :param sentences: Input sentences to tag
        :type sentences: list(list(str))
        :rtype: list(list(tuple(str, str))
        """"""
        # Converting list(list(str)) -&gt; list(str)
        sentences = (' '.join(words) for words in sentences)
        if properties == None:
            properties = {'tokenize.whitespace':'true'}
        return [sentences[0] for sentences in self.raw_tag_sents(sentences, properties)]

    def tag(self, sentence, properties=None):
        """"""
        Tag a list of tokens.

        :rtype: list(tuple(str, str))

        &gt;&gt;&gt; parser = CoreNLPParser(url='http://localhost:9000', tagtype='ner')
        &gt;&gt;&gt; tokens = 'Rami Eid is studying at Stony Brook University in NY'.split()
        &gt;&gt;&gt; parser.tag(tokens)
        [('Rami', 'PERSON'), ('Eid', 'PERSON'), ('is', 'O'), ('studying', 'O'), ('at', 'O'), ('Stony', 'ORGANIZATION'),
        ('Brook', 'ORGANIZATION'), ('University', 'ORGANIZATION'), ('in', 'O'), ('NY', 'O')]

        &gt;&gt;&gt; parser = CoreNLPParser(url='http://localhost:9000', tagtype='pos')
        &gt;&gt;&gt; tokens = ""What is the airspeed of an unladen swallow ?"".split()
        &gt;&gt;&gt; parser.tag(tokens)
        [('What', 'WP'), ('is', 'VBZ'), ('the', 'DT'),
        ('airspeed', 'NN'), ('of', 'IN'), ('an', 'DT'),
        ('unladen', 'JJ'), ('swallow', 'VB'), ('?', '.')]
        """"""
        return self.tag_sents([sentence], properties)[0]

    def raw_tag_sents(self, sentences, properties=None):
        """"""
        Tag multiple sentences.

        Takes multiple sentences as a list where each sentence is a string.

        :param sentences: Input sentences to tag
        :type sentences: list(str)
        :rtype: list(list(list(tuple(str, str)))
        """"""
        default_properties = {'ssplit.isOneSentence': 'true',
                              'annotators': 'tokenize,ssplit,' }

        default_properties.update(properties or {})

        # Supports only 'pos' or 'ner' tags.
        assert self.tagtype in ['pos', 'ner']
        default_properties['annotators'] += self.tagtype
        for sentence in sentences:
            tagged_data = self.api_call(sentence, properties=default_properties)
            yield [[(token['word'], token[self.tagtype]) for token in tagged_sentence['tokens']]
                    for tagged_sentence in tagged_data['sentences']]

pos_tagger = CoreNLPParser(url='http://localhost:9000', tagtype='pos')
sent = ['My', 'birthday', 'is', 'on', '09-12-2050']
print(pos_tagger.tag(sent))
</code></pre>

<p>[out]:</p>

<pre><code>[('My', 'PRP$'), ('birthday', 'NN'), ('is', 'VBZ'), ('on', 'IN'), ('09-12-2050', 'CD')]
</code></pre>

<hr>

<h1>In Long</h1>

<p>See </p>

<ul>
<li><a href=""https://stackoverflow.com/questions/52250268/why-do-corenlp-ner-tagger-and-ner-tagger-join-the-separated-numbers-together"">Why do CoreNLP ner tagger and ner tagger join the separated numbers together?</a></li>
<li><a href=""https://github.com/nltk/nltk/issues/2112"" rel=""nofollow noreferrer"">https://github.com/nltk/nltk/issues/2112</a></li>
</ul>
",""
"52413130","2018-09-19 19:39:28","0","","52409500","<p>All these issues are related to <a href=""https://lucene.apache.org/solr/guide/6_6/filter-descriptions.html"" rel=""nofollow noreferrer"">how you process the incoming text for those fields</a>. You'll have to create a filter chain for the field - and possibly use multiple fields for different use cases and prioritize those using <code>qf</code> - that processes the input values to do what you want.</p>

<p>Your first case can be solved by using a PatternReplaceFilter to remove any apostrophes - depending on your use case and tokenizer you might want to use the CharFilter version, as it processes the text before it's split into multiple tokens.</p>

<p>Your second case is a straight forward synonym filter or a WordDelimiterFilter, where you expand JPMorgan to ""JP Morgan"", or use the WordDelimiterFilter to expand case changes into separate tokens. That'll also allow you to search for <code>JP</code> and get <code>JPMorgan</code> related entries. These might have different effects on score, use <code>debugQuery=true</code> to see exactly how each term in your query contributes to the score.</p>

<p>The third case is in general the same as the second case. You'll have to create a decent synonym word list for the terms used, and this is usually something you build as you get feedback from your users, from existing dictionaries and from domain knowledge. There's also the option of preprocessing text using NLP, or in this case, something as primitive as indexing the initials of any capitalized words after each other could help.</p>
",""
"52396249","2018-09-19 00:21:23","0","","52393591","<h1>TL;DR</h1>

<p>It's an XY problem of a lemmatizer failing to meet your expectation, when the lemmatizer you're using is to solved a different problem. </p>

<hr>

<h1>In Long</h1>

<p><strong>Q: What is a lemma?</strong> </p>

<blockquote>
  <p>Lemmatisation (or lemmatization) in linguistics is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form. - <a href=""https://en.wikipedia.org/wiki/Lemmatisation"" rel=""noreferrer"">Wikipedia</a></p>
</blockquote>

<p><strong>Q: What is the ""dictionary form""?</strong></p>

<p>NLTK is using the <code>morphy</code> algorithm which is using WordNet as the basis of ""dictionary forms""</p>

<p>See also <a href=""https://stackoverflow.com/questions/43795249/how-does-spacy-lemmatizer-works"">How does spacy lemmatizer works?</a>. Note SpaCy has additional hacks put in to handle more irregular words. </p>

<p><strong>Q: Why <code>moisture -&gt; moisture</code> and <code>moisturizing -&gt; moisturizing</code>?</strong> </p>

<p>Because there are synset (sort of ""dictionary form"") for ""moisture"" and ""moisturizing""</p>

<pre><code>&gt;&gt;&gt; from nltk.corpus import wordnet as wn

&gt;&gt;&gt; wn.synsets('moisture')
[Synset('moisture.n.01')]
&gt;&gt;&gt; wn.synsets('moisture')[0].definition()
'wetness caused by water'

&gt;&gt;&gt; wn.synsets('moisturizing')
[Synset('humidify.v.01')]
&gt;&gt;&gt; wn.synsets('moisturizing')[0].definition()
'make (more) humid'
</code></pre>

<p><strong>Q: How could I get <code>moisture -&gt; moist</code>?</strong></p>

<p>Not really useful. But maybe try a stemmer (but don't expect too much of it)</p>

<pre><code>&gt;&gt;&gt; from nltk.stem import PorterStemmer

&gt;&gt;&gt; porter = PorterStemmer()
&gt;&gt;&gt; porter.stem(""moisture"")
'moistur'

&gt;&gt;&gt; porter.stem(""moisturizing"")
'moistur'
</code></pre>

<p><strong>Q: Then how do I get <code>moisuturizing/moisuture -&gt; moist</code>?!!</strong> </p>

<p>There's no well-founded way to do that. But before even trying to do that, what is the eventual purpose of doing <code>moisuturizing/moisuture -&gt; moist</code>. </p>

<p>Is it really necessary to do that? </p>

<p>If you really want, you can try word vectors and try to look for most similar words but there's a whole other world of caveats that comes with word vectors. </p>

<p><strong>Q: Wait a minute but <code>heard -&gt; heard</code> is ridiculous?!</strong></p>

<p>Yeah, the POS tagger isn't tagging the heard correctly. Most probably because the sentence is not a proper sentence, so the POS tags are wrong for the words in the sentence:</p>

<pre><code>&gt;&gt;&gt; from nltk import word_tokenize, pos_tag
&gt;&gt;&gt; sent
'The laughs you two heard were triggered by memories of his own high j-flying moist moisture moisturize moisturizing.'

&gt;&gt;&gt; pos_tag(word_tokenize(sent))
[('The', 'DT'), ('laughs', 'NNS'), ('you', 'PRP'), ('two', 'CD'), ('heard', 'NNS'), ('were', 'VBD'), ('triggered', 'VBN'), ('by', 'IN'), ('memories', 'NNS'), ('of', 'IN'), ('his', 'PRP$'), ('own', 'JJ'), ('high', 'JJ'), ('j-flying', 'NN'), ('moist', 'NN'), ('moisture', 'NN'), ('moisturize', 'VB'), ('moisturizing', 'NN'), ('.', '.')]
</code></pre>

<p>We see that <code>heard</code> is tagged as <code>NNS</code> (a noun). If we lemmatized it as a verb:</p>

<pre><code>&gt;&gt;&gt; from nltk.stem import WordNetLemmatizer
&gt;&gt;&gt; wnl = WordNetLemmatizer()
&gt;&gt;&gt; wnl.lemmatize('heard', pos='v')
'hear'
</code></pre>

<p><strong>Q: Then how do I get a correct POS tag?!</strong> </p>

<p>Probably with SpaCy, you get <code>('heard', 'VERB')</code>:</p>

<pre><code>&gt;&gt;&gt; import spacy
&gt;&gt;&gt; nlp = spacy.load('en_core_web_sm')
&gt;&gt;&gt; sent
'The laughs you two heard were triggered by memories of his own high j-flying moist moisture moisturize moisturizing.'
&gt;&gt;&gt; doc = nlp(sent)
&gt;&gt;&gt; [(word.text, word.pos_) for word in doc]
[('The', 'DET'), ('laughs', 'VERB'), ('you', 'PRON'), ('two', 'NUM'), ('heard', 'VERB'), ('were', 'VERB'), ('triggered', 'VERB'), ('by', 'ADP'), ('memories', 'NOUN'), ('of', 'ADP'), ('his', 'ADJ'), ('own', 'ADJ'), ('high', 'ADJ'), ('j', 'NOUN'), ('-', 'PUNCT'), ('flying', 'VERB'), ('moist', 'NOUN'), ('moisture', 'NOUN'), ('moisturize', 'NOUN'), ('moisturizing', 'NOUN'), ('.', 'PUNCT')]
</code></pre>

<p>But note, in this case, SpaCy got <code>('moisturize', 'NOUN')</code> and NLTK got <code>('moisturize', 'VB')</code>.</p>

<p><strong>Q: But can't I get <code>moisturize -&gt; moist</code> with SpaCy?</strong></p>

<p>Lets not go back to the start where we define what is a lemma. In short:</p>

<pre><code>&gt;&gt;&gt; import spacy
&gt;&gt;&gt; nlp = spacy.load('en_core_web_sm')
&gt;&gt;&gt; sent
'The laughs you two heard were triggered by memories of his own high j-flying moist moisture moisturize moisturizing.'
&gt;&gt;&gt; doc = nlp(sent)
&gt;&gt;&gt; [word.lemma_ for word in doc]
['the', 'laugh', '-PRON-', 'two', 'hear', 'be', 'trigger', 'by', 'memory', 'of', '-PRON-', 'own', 'high', 'j', '-', 'fly', 'moist', 'moisture', 'moisturize', 'moisturizing', '.']
</code></pre>

<p>See also <a href=""https://stackoverflow.com/questions/43795249/how-does-spacy-lemmatizer-works"">How does spacy lemmatizer works?</a> (again)</p>

<p><strong>Q: Okay, fine. I can't get <code>moisturize -&gt; moist</code>... And POS tag is not perfect for <code>heard -&gt; hear</code>. But why can't I get <code>j-flying -&gt; fly</code>?</strong></p>

<p>Back to the question of <em>why do you need to convert <code>j-flying -&gt; fly</code></em>, there are counter examples of why you wouldn't want to separate something that looks like a compound. </p>

<p>For example:</p>

<ul>
<li>Should <code>Classical-sounding</code> go to <code>sound</code>?</li>
<li>Should <code>X-fitting</code> go to <code>fit</code>?</li>
<li>Should <code>crash-landing</code> go to <code>landing</code>?</li>
</ul>

<p>Depends on what's the ultimate purpose of your application, converting a token to your desired form may or may not be necessary. </p>

<p><strong>Q: Then what is a good way to extract meaningful words?</strong></p>

<p>I sound like a broken record but it depends on what's your ultimate goal? </p>

<p>If you goal is really to understand the meaning of words then you have to ask yourself the question, <strong>""What is the meaning of meaning?""</strong></p>

<p>Does individual word has a meaning out of its context? Or would it have the sum of meanings from all the possible context it could occur in. </p>

<p>Au currant, the state-of-art basically treats all meanings as an array of floats and comparisons between array of floats are what give meaning its meaning. But is that really meaning or just an means to an end? (Pun intended).</p>

<p><strong>Q: Why am I get more questions than answers?</strong></p>

<p>Welcome to the world of computational linguistics which has its roots from philosophy (like computer science). Natural language processing is commonly known as the application of computational linguistics</p>

<hr>

<h1>Food for thought</h1>

<p>Q: <strong>Is a lemmatizer better than a stemmer?</strong></p>

<p>A: No definite answer. (c.f. <a href=""https://stackoverflow.com/questions/17317418/stemmers-vs-lemmatizers"">Stemmers vs Lemmatizers</a>)</p>
",""
"52395728","2018-09-18 23:07:04","0","","51664147","<p><a href=""https://www.youtube.com/watch?v=UDg2AGRGjLQ"" rel=""nofollow noreferrer"">From this video with a Google employee</a>, auto-generated / machine translated versions of webpages can count against your site as duplicate content. If you append the machine translated version with some text of your own you might be able to get around this 'Yes, it's duplicated content' flag, but we can't know how much original text needs to be added to a translation in order for the Google robots to flag the page as original content instead of duplicated content. </p>

<p>Your best bet would be to have an actual human translate the whole web page or you could have a human translator augment or modify a machine-translated version of your webpage so  that human-edited translation of your website is sufficiently different (what 'sufficiently' is we don't know) from the machine translated version.</p>
",""
"52318918","2018-09-13 17:31:15","4","","52315632","<p>The purpose of Lemmatisation is to group together different inflected forms of a word, called lemma. For example, a lemmatiser should map gone, going and went into go. Thus we have to lemmatize each word separately. </p>

<pre><code>from nltk.stem import PorterStemmer, WordNetLemmatizer

sent = 'The laughs you two heard were triggered by memories of his own high j-flying exits for moving beasts'
sent_tokenized = sent.split("" "")
lemmatizer = WordNetLemmatizer()
words = [lemmatizer.lemmatize(word) for word in sent_tokenized]
</code></pre>
",""
"52247460","2018-09-09 18:29:44","0","","51609143","<blockquote>
  <p>i need to pickle a trained tagger and to train and combin Ngram taggers but i don't understand what pickle means or do</p>
</blockquote>

<p>As per this part of your question, Pickle is a library in Python that allows to dump and load binary data on/from your hard drive, related to any python object of your choosing.</p>

<p>Info here: <a href=""https://docs.python.org/3/library/pickle.html"" rel=""nofollow noreferrer"">https://docs.python.org/3/library/pickle.html</a></p>

<p>What you were suggested to do is however to take a pre-trained tagger, which would likely belong to another language, and add the ngrams extracted from the tagged corpora in Malagasy that you have built. If you have a sufficiently large corpus of tagged documents in your own language, however, it might be more useful for yourself and for the NLP community to develop a tagger specific for Malagasy. After a quick research I could not find any on the internet, and it would thus be useful to prepare one.</p>
",""
"52142282","2018-09-03 01:35:38","1","","52140526","<p>You're right. Most stemmers only stem suffixes. In fact the original paper from Martin Porter is titled:</p>

<blockquote>
  <p>Porter, M. ""An algorithm for suffix stripping."" Program 14.3 (1980): 130-137.</p>
</blockquote>

<p>And possibly the only stemmers that has prefix stemming in NLTK are the arabic stemmers:</p>

<ul>
<li><a href=""https://github.com/nltk/nltk/blob/develop/nltk/stem/arlstem.py#L115"" rel=""noreferrer"">https://github.com/nltk/nltk/blob/develop/nltk/stem/arlstem.py#L115</a></li>
<li><a href=""https://github.com/nltk/nltk/blob/develop/nltk/stem/snowball.py#L372"" rel=""noreferrer"">https://github.com/nltk/nltk/blob/develop/nltk/stem/snowball.py#L372</a></li>
</ul>

<p>But if we take a look at this <a href=""https://github.com/nltk/nltk/blob/develop/nltk/stem/util.py"" rel=""noreferrer""><code>prefix_replace</code></a> function, 
it simply removes the old prefix and substitute it with the new prefix. </p>

<pre><code>def prefix_replace(original, old, new):
    """"""
     Replaces the old prefix of the original string by a new suffix
    :param original: string
    :param old: string
    :param new: string
    :return: string
    """"""
    return new + original[len(old):]
</code></pre>

<p>But we can do better!</p>

<p>First, <strong>do you have a fixed list of prefix and substitutions for the language you need to process?</strong></p>

<p>Lets go with the (unfortunately) de facto language, English, and do some linguistics work to find out prefixes in English:</p>

<p><a href=""https://dictionary.cambridge.org/grammar/british-grammar/word-formation/prefixes"" rel=""noreferrer"">https://dictionary.cambridge.org/grammar/british-grammar/word-formation/prefixes</a></p>

<p>Without much work, you can write a prefix stemming function before the suffix stemming from NLTK, e.g. </p>

<pre><code>import re
from nltk.stem import PorterStemmer

# From https://dictionary.cambridge.org/grammar/british-grammar/word-formation/prefixes
english_prefixes = {
""anti"": """",    # e.g. anti-goverment, anti-racist, anti-war
""auto"": """",    # e.g. autobiography, automobile
""de"": """",      # e.g. de-classify, decontaminate, demotivate
""dis"": """",     # e.g. disagree, displeasure, disqualify
""down"": """",    # e.g. downgrade, downhearted
""extra"": """",   # e.g. extraordinary, extraterrestrial
""hyper"": """",   # e.g. hyperactive, hypertension
""il"": """",     # e.g. illegal
""im"": """",     # e.g. impossible
""in"": """",     # e.g. insecure
""ir"": """",     # e.g. irregular
""inter"": """",  # e.g. interactive, international
""mega"": """",   # e.g. megabyte, mega-deal, megaton
""mid"": """",    # e.g. midday, midnight, mid-October
""mis"": """",    # e.g. misaligned, mislead, misspelt
""non"": """",    # e.g. non-payment, non-smoking
""over"": """",  # e.g. overcook, overcharge, overrate
""out"": """",    # e.g. outdo, out-perform, outrun
""post"": """",   # e.g. post-election, post-warn
""pre"": """",    # e.g. prehistoric, pre-war
""pro"": """",    # e.g. pro-communist, pro-democracy
""re"": """",     # e.g. reconsider, redo, rewrite
""semi"": """",   # e.g. semicircle, semi-retired
""sub"": """",    # e.g. submarine, sub-Saharan
""super"": """",   # e.g. super-hero, supermodel
""tele"": """",    # e.g. television, telephathic
""trans"": """",   # e.g. transatlantic, transfer
""ultra"": """",   # e.g. ultra-compact, ultrasound
""un"": """",      # e.g. under-cook, underestimate
""up"": """",      # e.g. upgrade, uphill
}

porter = PorterStemmer()

def stem_prefix(word, prefixes):
    for prefix in sorted(prefixes, key=len, reverse=True):
        # Use subn to track the no. of substitution made.
        # Allow dash in between prefix and root. 
        word, nsub = re.subn(""{}[\-]?"".format(prefix), """", word)
        if nsub &gt; 0:
            return word

def porter_english_plus(word, prefixes=english_prefixes):
    return porter.stem(stem_prefix(word, prefixes))


word = ""extraordinary""
porter_english_plus(word)
</code></pre>

<hr>

<p><strong>Now that we have a simplistic prefix stemmer could we do better?</strong> </p>

<pre><code># E.g. this is not satisfactory:
&gt;&gt;&gt; porter_english_plus(""united"")
""ited""
</code></pre>

<p>What if we check if the prefix stemmed words appears in certain list before stemming it? </p>

<pre><code>import re

from nltk.corpus import words
from nltk.corpus import wordnet as wn
from nltk.stem import PorterStemmer

# From https://dictionary.cambridge.org/grammar/british-grammar/word-formation/prefixes
english_prefixes = {
""anti"": """",    # e.g. anti-goverment, anti-racist, anti-war
""auto"": """",    # e.g. autobiography, automobile
""de"": """",      # e.g. de-classify, decontaminate, demotivate
""dis"": """",     # e.g. disagree, displeasure, disqualify
""down"": """",    # e.g. downgrade, downhearted
""extra"": """",   # e.g. extraordinary, extraterrestrial
""hyper"": """",   # e.g. hyperactive, hypertension
""il"": """",     # e.g. illegal
""im"": """",     # e.g. impossible
""in"": """",     # e.g. insecure
""ir"": """",     # e.g. irregular
""inter"": """",  # e.g. interactive, international
""mega"": """",   # e.g. megabyte, mega-deal, megaton
""mid"": """",    # e.g. midday, midnight, mid-October
""mis"": """",    # e.g. misaligned, mislead, misspelt
""non"": """",    # e.g. non-payment, non-smoking
""over"": """",  # e.g. overcook, overcharge, overrate
""out"": """",    # e.g. outdo, out-perform, outrun
""post"": """",   # e.g. post-election, post-warn
""pre"": """",    # e.g. prehistoric, pre-war
""pro"": """",    # e.g. pro-communist, pro-democracy
""re"": """",     # e.g. reconsider, redo, rewrite
""semi"": """",   # e.g. semicircle, semi-retired
""sub"": """",    # e.g. submarine, sub-Saharan
""super"": """",   # e.g. super-hero, supermodel
""tele"": """",    # e.g. television, telephathic
""trans"": """",   # e.g. transatlantic, transfer
""ultra"": """",   # e.g. ultra-compact, ultrasound
""un"": """",      # e.g. under-cook, underestimate
""up"": """",      # e.g. upgrade, uphill
}

porter = PorterStemmer()

whitelist = list(wn.words()) + words.words()

def stem_prefix(word, prefixes, roots):
    original_word = word
    for prefix in sorted(prefixes, key=len, reverse=True):
        # Use subn to track the no. of substitution made.
        # Allow dash in between prefix and root. 
        word, nsub = re.subn(""{}[\-]?"".format(prefix), """", word)
        if nsub &gt; 0 and word in roots:
            return word
    return original_word

def porter_english_plus(word, prefixes=english_prefixes):
    return porter.stem(stem_prefix(word, prefixes, whitelist))
</code></pre>

<p>We resolve the issue of not stemming away the prefix, causing senseless root, e.g. </p>

<pre><code>&gt;&gt;&gt; stem_prefix(""united"", english_prefixes, whitelist)
""united""
</code></pre>

<p>But the porter stem would have still make remove the suffix, <code>-ed</code>, which may/may not be the desired output that one would require, esp. when the goal is to retain linguistically sound units in the data: </p>

<pre><code>&gt;&gt;&gt; porter_english_plus(""united"")
""unit""
</code></pre>

<p>So, depending on the task, it's sometimes more beneficial to use a lemma more than a stemmer. </p>

<p>See also:</p>

<ul>
<li><a href=""https://stackoverflow.com/questions/44449284/nltk-words-corpus-does-not-contain-okay"">nltk words corpus does not contain &quot;okay&quot;?</a></li>
<li><a href=""https://stackoverflow.com/questions/17317418/stemmers-vs-lemmatizers"">Stemmers vs Lemmatizers</a></li>
<li><a href=""https://stackoverflow.com/questions/43795249/how-does-spacy-lemmatizer-works"">How does spacy lemmatizer works?</a></li>
</ul>
",""
"52055648","2018-08-28 10:26:19","0","","52039155","<p>Even though API is not that great, you can make <code>langdetect</code> restrict itself only to languages that you are actually working with. For example:</p>

<pre><code>from langdetect.detector_factory import DetectorFactory, PROFILES_DIRECTORY
import os

def get_factory_for(langs):
    df = DetectorFactory()
    profiles = []
    for lang in ['en', 'ru', 'pl']:
        with open(os.path.join(PROFILES_DIRECTORY, lang), 'r', encoding='utf-8') as f:
            profiles.append(f.read())
    df.load_json_profile(profiles)

    def _detect_langs(text):
        d = df.create()
        d.append(text)
        return d.get_probabilities()

    def _detect(text):
        d = df.create()
        d.append(text)
        return d.detect()

    df.detect_langs = _detect_langs
    df.detect = _detect
    return df
</code></pre>

<p>While unrestricted <code>langdetect</code> seems to think <code>""today""</code> is Somali, if you only have English, Russian and Polish you can now do this:</p>

<pre><code>df = get_factory_for(['en', 'ru', 'pl'])
df.detect('today')         # 'en'
df.detect_langs('today')   # [en:0.9999988994459187]
</code></pre>

<p>It will still miss a lot (<code>""snow""</code> is apparently Polish), but it will still drastically cut down on your error rate.</p>
",""
"51978364","2018-08-23 04:33:27","1","","51943811","<p>Lemmatization converts each token (aka <code>form</code>) in the sentence into its lemma form (aka <code>type</code>):</p>

<pre><code>&gt;&gt;&gt; from nltk import word_tokenize
&gt;&gt;&gt; from pywsd.utils import lemmatize_sentence

&gt;&gt;&gt; text = ['This is a corpus with multiple sentences.', 'This was the second sentence running.', 'For some reasons, there is a need to second foo bar ran.']

&gt;&gt;&gt; lemmatize_sentence(text[0]) # Lemmatized sentence example.
['this', 'be', 'a', 'corpus', 'with', 'multiple', 'sentence', '.']
&gt;&gt;&gt; word_tokenize(text[0]) # Tokenized sentence example. 
['This', 'is', 'a', 'corpus', 'with', 'multiple', 'sentences', '.']
&gt;&gt;&gt; word_tokenize(text[0].lower()) # Lowercased and tokenized sentence example.
['this', 'is', 'a', 'corpus', 'with', 'multiple', 'sentences', '.']
</code></pre>

<p>If we lemmatize the sentence, each token should receive the corresponding lemma form, so the no. of ""words"" remains the same whether it's the <code>form</code> or the <code>type</code>:</p>

<pre><code>&gt;&gt;&gt; num_tokens = sum([len(word_tokenize(sent.lower())) for sent in text])
&gt;&gt;&gt; num_lemmas = sum([len(lemmatize_sentence(sent)) for sent in text])
&gt;&gt;&gt; num_tokens, num_lemmas
(29, 29)


&gt;&gt;&gt; [lemmatize_sentence(sent) for sent in text] # lemmatized sentences
[['this', 'be', 'a', 'corpus', 'with', 'multiple', 'sentence', '.'], ['this', 'be', 'the', 'second', 'sentence', 'running', '.'], ['for', 'some', 'reason', ',', 'there', 'be', 'a', 'need', 'to', 'second', 'foo', 'bar', 'ran', '.']]

&gt;&gt;&gt; [word_tokenize(sent.lower()) for sent in text] # tokenized sentences
[['this', 'is', 'a', 'corpus', 'with', 'multiple', 'sentences', '.'], ['this', 'was', 'the', 'second', 'sentence', 'running', '.'], ['for', 'some', 'reasons', ',', 'there', 'is', 'a', 'need', 'to', 'second', 'foo', 'bar', 'ran', '.']]
</code></pre>

<p>The ""compression"" per-se would refer to the number of <strong>unique</strong> tokens represented in the whole corpus after you've lemmatized the sentences, e.g.</p>

<pre><code>&gt;&gt;&gt; lemma_vocab = set(chain(*[lemmatize_sentence(sent) for sent in text]))
&gt;&gt;&gt; token_vocab = set(chain(*[word_tokenize(sent.lower()) for sent in text]))
&gt;&gt;&gt; len(lemma_vocab), len(token_vocab)
(21, 23)

&gt;&gt;&gt; lemma_vocab
{'the', 'this', 'to', 'reason', 'for', 'second', 'a', 'running', 'some', 'sentence', 'be', 'foo', 'ran', 'with', '.', 'need', 'multiple', 'bar', 'corpus', 'there', ','}
&gt;&gt;&gt; token_vocab
{'the', 'this', 'to', 'for', 'sentences', 'a', 'second', 'running', 'some', 'is', 'sentence', 'foo', 'reasons', 'with', 'ran', '.', 'need', 'multiple', 'bar', 'corpus', 'there', 'was', ','}
</code></pre>

<hr>

<p><strong>Note:</strong> Lemmatization is a pre-processing step. But it should <strong>not</strong> overwrite your original corpus with the lemmatize forms.</p>
",""
"51942139","2018-08-21 05:30:52","1","","51905788","<p>First see <a href=""https://www.python-course.eu/python3_inheritance.php"" rel=""nofollow noreferrer"">https://www.python-course.eu/python3_inheritance.php</a></p>

<p>Create a file <code>mytools.py</code></p>

<pre><code>import itertools, re
from nltk.stem import StemmerI

class MyStemmer(StemmerI):
    def stem(self, token):
        itoken = token[:-3] if token.endswith('bai') else token
        for r in ((""tha"", ""ta""), (""i"", ""e"")):
            token = token.replace(*r)
            token = re.sub(r'(\w)\1+',r'\1', token)
        return ''.join(i for i, _ in itertools.groupby(token))
</code></pre>

<p>Usage:</p>

<pre><code>&gt;&gt;&gt; from mystemmer import MyStemmer
&gt;&gt;&gt; s = MyStemmer()
&gt;&gt;&gt; s.stem('savithabai')
'savetabae'
</code></pre>
",""
"51910110","2018-08-18 15:29:09","0","","51904251","<p>Make sure that <code>comprehensive_ngrams</code> is a list of unique words. I.e.: </p>

<pre><code>assert len(set(comprehensive_ngrams)) == len(comprehensive_ngrams)
</code></pre>
",""
"51895442","2018-08-17 12:25:59","0","","51895244","<p>It does not matter which one you choose.
The bulk of the computation is likely done by the tokenizer, not by the for loop in the presented code.
Moreover the two examples do the same, except one of them has fewer <strong>explicit</strong> variables, but still the data needs to be stored somewhere.</p>

<p>Usually, algorithmic speedups come from clever elimination of loop iterations, e.g. in sorting algorithms speedups may come from avoiding value comparisons that will not result in a change to the order of elements (ones that don't advance the sorting). Here the number of loop iterations is the same in both cases.</p>
",""
"51838247","2018-08-14 09:32:24","0","","51836500","<p>Each value in the third column is a directed edge in the dependency tree. For example:</p>

<ul>
<li>The head/governor of ""quick"" is token 4: ""fox"", and ""quick"" is a modifier of ""fox"" (amod)</li>
<li>The head/governor of ""fox"" is token 5: ""jumps"" and ""fox"" is the subject of ""jumps"" (nsubj)</li>
</ul>

<p>The value 0 is reserved for the root of the tree, usually the main verb of the sentence.</p>
",""
"51810575","2018-08-12 16:10:48","0","","51786224","<p>yeah, found how to do that through <a href=""https://stackoverflow.com/questions/39340907/converting-output-of-dependency-parsing-to-tree"">this question</a>, but it is not showing root attribute, that's the only issue now </p>

<pre><code>  dependencyParser = stanford.StanfordDependencyParser(model_path=""edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz"")
result = dependencyParser.raw_parse(noiseLessInput)
dep = result.__next__()
for triple in dep.triples():
 print(triple[1], ""("", triple[0][0], "", "", triple[2][0], "")"")
</code></pre>
",""
"51800137","2018-08-11 13:30:07","0","","43191782","<p>Yes, instead of passing FileInputStream to a dictionary, you can create your own implementation of InputStream, say DatabaseSourceInputStream and use it instead.</p>
",""
"51788385","2018-08-10 14:10:36","1","","51787997","<p>To convert the <code>training_set</code> to a scikit-usable form, you just need to do</p>

<pre><code>from sklearn.feature_extraction import DictVectorizer
vectorizer = DictVectorizer()

X_train, y_train = list(zip(*training_set))
X_train = vectorizer.fit_transform(X_train)

X_test, y_test = list(zip(*testing_set))
X_test = vectorizer.transform(X_test)
</code></pre>

<p>After that you can easily call </p>

<pre><code>clf = RandomForestClassifier()
clf.fit(X_train, y_train)

accuracy = clf.score(X_test, y_test)
print(""RandFor accuracy:"", (accuracy) * 100)
</code></pre>
",""
"51776190","2018-08-09 21:46:43","0","","51768414","<pre><code>from nltk.corpus import wordnet as wn

it_lemmas = wn.lemmas(""problema"", lang=""ita"")

for i in range(len(it_lemmas)):
    hypernyms = it_lemmas[i].synset().hypernyms()

    for i in range(len(hypernyms)):
        syn = hypernyms[i].lemmas(lang=""ita"")
        print (syn)
</code></pre>
",""
"51772983","2018-08-09 17:39:57","1","","51772605","<p>There are several issues with your question.</p>

<p>For starters, you don't actually <em>fit</em> the pipeline, hence the error. Looking more closely in the <a href=""http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#building-a-pipeline"" rel=""nofollow noreferrer"">linked tutorial</a>, you'll see that there is a step <code>text_clf.fit</code> (where <code>text_clf</code> is indeed the pipeline).</p>

<p>Second, you don't use the <em>notion</em> of the pipeline correctly, which is exactly to fit end-to-end the whole stuff; instead, you fit the individual components of it one by one... If you check again the tutorial, you'll see that the code for the <em>pipeline fit</em>:</p>



<pre class=""lang-py prettyprint-override""><code>text_clf.fit(twenty_train.data, twenty_train.target)  
</code></pre>

<p>uses the data in their initial form, <em>not</em> their intermediate transformations, as you do; the point of the tutorial is to demonstrate how the individual transformations can be wrapped-up in (and replaced by) a pipeline, <em>not</em> to use the pipeline on top of these transformations...</p>

<p>Third, you should avoid naming variables as <code>fit</code> - this is a reserved keyword; and similarly, we don't use  CV to abbreviate Count Vectorizer (in ML lingo, CV stands for cross validation).</p>

<p>That said, here is the correct way for using your pipeline:</p>

<pre class=""lang-py prettyprint-override""><code>from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline

traindf, testdf = train_test_split(nlp_df, stratify=nlp_df['class'])

x_train = traindf['text']
x_test = traindf['text']
y_train = traindf['class']
y_test = testdf['class']

text_clf = Pipeline([('vect', CountVectorizer(stop_words='english')),
                    ('tfidf', TfidfTransformer()),
                    ('clf', MultinomialNB()),
                     ])

text_clf.fit(x_train, y_train) 

predicted = text_clf.predict(x_test)
</code></pre>

<p>As you can see, the purpose of the pipelines is to make things <em>simpler</em> (compared to using the components one by one sequentially), not to complicate them further...</p>
",""
"51760403","2018-08-09 06:43:06","1","","51760307","<p>I think you need this:</p>

<pre><code>myword = list[0][0]
mypos = list[0][1]
</code></pre>

<p>Output:</p>

<pre><code>good
JJ
</code></pre>
",""
"51744556","2018-08-08 10:36:11","0","","48621246","<p>In short. YES. If you sanitaze your input so you are only left with letters and other characters from different languages then space sounds like a logic answer.</p>
",""
"51709530","2018-08-06 14:06:17","0","","47599575","<p><code>RegexParser</code> only results in <strong>non-overlapping</strong> chunks. I reached the following solution using NLTK's <strong><code>bigrams</code></strong>.
First, I modified the <code>grammar</code> to match any 2 or more consecutive nouns. Then I create bigrams from the result.</p>

<h1>The code:</h1>

<pre><code>import nltk

grammar = ""CONSEC_NOUNS: {&lt;NN&gt;{2,}}"" # match 2 or more nouns
cp = nltk.RegexpParser(grammar)
result = cp.parse([('APPLE', 'NN'), ('BANANA', 'NN'), ('GRAPE', 'NN'), ('PEAR', 'NN'), ('GO', 'VB'), 
                        ('ORANGE', 'NN'), ('STRAWBERRY', 'NN'), ('MELON', 'NN')])

leaves = [chunk.leaves() for chunk in result if ((type(chunk) == nltk.tree.Tree) and chunk.label()=='CONSEC_NOUNS')]
noun_bigram_groups = [list(nltk.bigrams([w for w, t in leaf])) for leaf in leaves]

extract = [' '.join(nouns) for group in noun_bigram_groups for nouns in group]

print(extract)
</code></pre>

<h1>The output is:</h1>

<blockquote>
  <p><code>['APPLE BANANA', 'BANANA GRAPE', 'GRAPE PEAR', 'ORANGE STRAWBERRY', 'STRAWBERRY MELON']</code></p>
</blockquote>
",""
"51658792","2018-08-02 16:58:38","3","","51658153","<p>Each token has a number of attributes, you can iterate through the doc to access them. </p>

<p>For example: <code>[token.lemma_ for token in doc]</code></p>

<p>If you want to reconstruct the sentence you could use: <code>' '.join([token.lemma_ for token in doc])</code></p>

<p>For a full list of token attributes see: <a href=""https://spacy.io/api/token#attributes"" rel=""noreferrer"">https://spacy.io/api/token#attributes</a></p>
",""
"51520376","2018-07-25 13:38:39","0","","51513261","<p>You need to first calculate corpus frequency for each term, for your case for each word and keep them in a <em>frequency dictionary</em>. Let's say cherry happens to occur 78 times in your corpus <strong>cheery --> 78</strong> you need to keep. Then sort your frequency dictionary descending by frequency values, then keep first N pairs.</p>

<p>Then, for your enumeration you may keep a dictionary as an index. For instance, <strong>cherry --> term2</strong> for <em>index dictionary</em>. </p>

<p>Now, an incidence matrix needed to be prepared. It will be vectors of documents, like this:</p>

<pre><code>doc_id   term1 term2 term3 .... termN
doc1       35     0    23         1
doc2        0     0    13         2
   .        .     .     .         .
docM        3     1     2         0
</code></pre>

<p>Each document(text, title, sentence) in your corpus needs to have an id or index as well as listed above. It is time to create a vector for a document. Iterate through your documents and get terms by tokenizing them, you have tokens per document. Iterate through tokens, check if next token exists in your <em>frequency dictionary</em>. If true, update your zero vector by using your <em>index dictionary</em> and <em>frequency dictionary</em>. </p>

<p>Let's say doc5 has cherry and we have it in our first N popular terms. Get its frequency (it was 78) and index (it was term5). Now update zero vector of doc5:</p>

<pre><code>doc_id   term1 term2 term3 .... termN
doc1       35     0    23         1
doc2        0     0    13         2
   .        .     .     .         .
doc5        0    78     0         0 (under process)
</code></pre>

<p>You need to do this for each token against all popular terms for every document in your corpus.</p>

<p>At the end you will end up with a NxM matrix, which contains vectors of M documents in your corpus. </p>

<p>I can suggest you to look at IR-Book. <a href=""https://nlp.stanford.edu/IR-book/information-retrieval-book.html"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/IR-book/information-retrieval-book.html</a></p>

<p>You may think using a tf-idf based matrix instead of corpus frequency-based term incidence matrix as they propose as well. </p>

<p>Hope this post helps,</p>

<p>Cheers</p>
",""
"51416062","2018-07-19 06:47:32","3","","51415694","<p>Here is a simple approach, I'm assuming input as a word and its POS tag.</p>

<pre><code>word = Input(...)
pos = Input(...)
emb = Embedding( ... ) (word)
layer = Concatenate()([emb, pos])
outputs = .... # your processing
model = Model(inputs=[word,pos], outputs=outputs)
</code></pre>
",""
"51390766","2018-07-17 21:57:51","0","","51252914","<p>So my solution was found using <a href=""https://stackoverflow.com/questions/32835291/how-to-find-the-shortest-dependency-path-between-two-words-in-python?rq=1"">that post</a></p>
<p>There is an answer dedicated to spaCy</p>
<p>My implementation for <strong>finding the dependency path between two words in a given sentence</strong>:</p>
<pre><code>import networkx as nx
import spacy
enter code here
doc = nlp(&quot;Ships carrying equipment for US troops are already waiting off the Turkish coast&quot;)
    
def shortest_dependency_path(doc, e1=None, e2=None):
    edges = []
    for token in doc:
        for child in token.children:
            edges.append(('{0}'.format(token),
                          '{0}'.format(child)))
    graph = nx.Graph(edges)
    try:
        shortest_path = nx.shortest_path(graph, source=e1, target=e2)
    except nx.NetworkXNoPath:
        shortest_path = []
    return shortest_path

print(shortest_dependency_path(doc,'Ships','troops'))
</code></pre>
<p>Output:</p>
<pre><code>['Ships', 'carrying', 'for', 'troops']
</code></pre>
<p>What it actually does is to first build a non-oriented graph for the sentence where words are the nodes and dependencies between words are the edges and then find the shortest path between two nodes</p>
<p>For my needs, I just then check for each word if it's on the dependency path (shortest path) generated</p>
",""
"51371428","2018-07-16 23:50:16","0","","51371356","<p>First, have a look at the data that your classifiers are seeing. Measure the correlation between features and the class (Pearson correlation is fine) and check if you have irrelavant features. For example, the word <em>patient</em> is not usually considered a stopword, but in a medical database, it will most likely be one.</p>

<p>Also consider using more complex features, like bigrams or trigrams, or even adding word embeddings (e.g., take a pretrained model such as word2vec or GloVe, and then take the average text vector).</p>

<p>N.B.: These days text classification is mostly done with neural networks and word embeddings. That said, your dataset isn't very big, so it may not be worth it to change methods (or maybe you don't want to, for some reason).</p>
",""
"51338217","2018-07-14 11:50:22","3","","51337884","<ul>
<li>Your regex just has one mistake, you forgot to escape <code>.</code> at the end, since <code>.</code> means match any char except line break. Also <code>(.+?)</code> is non greedy, therefore it matches one char and <code>.</code> after that matches one more char.</li>
</ul>

<p>Below code should work,</p>

<pre><code>def extract_topic(message):
message = re.search('Write short notes on the anatomy of the (.+?)\.', message)
if message:
    return message.group(1)
</code></pre>
",""
"51318333","2018-07-13 05:44:06","4","","51316438","<p>The German Snowball stemmer <a href=""http://snowball.tartarus.org/algorithms/german/stemmer.html"" rel=""nofollow noreferrer"">follows a three step process</a>:</p>

<ol>
<li>Remove <code>ern</code>, <code>em</code>, <code>er</code>, <code>en</code>, <code>es</code>, <code>e</code>, <code>s</code> suffixes</li>
<li>Remove <code>est</code>, <code>en</code>, <code>er</code>, <code>st</code> suffixes</li>
<li>Remove <code>isch</code>, <code>lich</code>, <code>heit</code>, <code>keit</code>, <code>end</code>, <code>ung</code>, <code>ig</code>, <code>ik</code> suffixes</li>
</ol>

<p>Not knowing a lot about German grammar, it seems that <code>in</code> would belong to the same class as the step 3 suffixes (these are referred to as ""derivational suffixes"" in the NLTK source). It would seem that adding <code>in</code> to this list of suffixes should force the Snowball stemmer to remove it but there are two problems.</p>

<p>The first problem is that from your examples I see that <code>in</code> becomes <code>inn</code> when followed by <code>en</code>. This could be worked around by adding both <code>in</code> and <code>inn</code> to the list of step 3 suffixes, but that doesn't solve the second problem.</p>

<p>Looking at the <a href=""http://www.nltk.org/_modules/nltk/stem/snowball.html"" rel=""nofollow noreferrer""><code>GermanStemmer.stem()</code> source</a>, each step will only remove a single suffix. Thus, if there is more than one derivational suffix (i.e. <code>in</code> plus any of the suffixes listed above], only the one will be removed.</p>

<p>In such cases (and I don't know enough about German to know if this can actually happen), you'd need to manually edit <code>GermanStemmer.stem()</code> to add a fourth ""<code>in</code> removal"" step. This would also allow finer control in the case of plurals. But honestly, at that point it's probably better to just ad hoc remove <code>in</code> by wrapping your <code>GermanStemmer.stem()</code> call. For example:</p>

<pre><code>from nltk.stem.snowball import GermanStemmer

def stem_german(word):
    plural = word.endswith(""en"") #for deciding if we are looking for ""in"" or ""inn""
    stemmed_word = GermanStemmer().stem(word)

    feminine_suffix = ""in"" if not plural else ""inn""
    if stemmed_word.endswith(feminine_suffix):
        stemmed_word = stemmed_word[:-len(feminine_suffix)]

    return stemmed_word
</code></pre>

<p>--Edit--</p>

<p>If you wanted to add <code>in</code> to one of the Snowball Stemmer steps, you can do so using:</p>

<pre><code>#Using nltk.stem.snowball.SnowballStemmer
stemmer = SnowballStemmer(""german"")
stemmer.stemmer._GermanStemmer__step3_suffixes += (""in"",) #add ""in"" to the step 3 suffixes

#Using nltk.stem.snowball.GermanStemmer
stemmer = GermanStemmer()
stemmer._GermanStemmer__step3_suffixes += (""in"",)
</code></pre>

<p>Note the comma after <code>""in""</code>. This code will not work without it. You can also replace the <code>3</code> with whichever step you wish to modify. I'm not entirely sure why it's <code>_GermanStemmer__step3_suffixes</code> and not just <code>__step3_suffixes</code> but I've verified that this code works on Python 3.6.4 and NLTK 3.2.5.</p>

<p>I would <strong>not</strong> recommend this approach, though, as it will not properly deal with <code>innen</code>. Also, since each step removes a maximum of one suffix, it will not properly deal with words like <code>Lehrerinnen</code> which have <code>en</code>, <code>in</code>, and <code>er</code> (step 3 doesn't check for <code>er</code>). I think your best bet is to just copy and paste the entirety of <code>GermanStemmer</code> (found in the source code link above. Use <code>ctrl+f</code>) and add a step 2.5 to <code>stem()</code> that checks for and removes <code>in/inn</code>.</p>
",""
"51259828","2018-07-10 07:50:30","8","","51259007","<p>First of all, setting the compact flag in Displacy will reduce the size of tree shown. </p>

<pre><code>options = {'compact': True} 
svg = displacy.render(doc, style='dep',options=options)
</code></pre>

<p>But only this won't work for large paragraphs. What I'll suggest is, instead of viewing the dependency parse of the whole paragraph, break the paragraph into sentences first. Then parse each sentence and view them. You can save the parse trees of each sentence as a SVG file and then see them one by one. Here is the code for saving SVG:</p>

<pre><code>svg = displacy.render(doc, style='dep',options=options)
f = open('sample.svg', 'w')
f.write(svg)
f.close()
</code></pre>

<p>Alternatively, you can save the whole parse tree of the paragraph as SVG and open it in a browser. Then you can easily view it with zoom and scroll. </p>
",""
"51239567","2018-07-09 06:46:52","0","","51239434","<p><code>nltk.pos_tag</code> works on a list or list-like thing as an argument, and tags <em>each element</em> of that. So in your second example, it splits each string (i.e., each word) into letters, just like it split the sentence into letters in the first example. It works when you pass in the whole list you got from splitting the sentence:</p>

<pre><code>&gt;&gt;&gt; nltk.pos_tag(sentence.split("" ""))
[('I', 'PRP'), ('am', 'VBP'), ('a', 'DT'), ('good', 'JJ'), ('boy', 'NN')]
</code></pre>

<p><a href=""https://www.nltk.org/book/ch05.html"" rel=""nofollow noreferrer"">Per documentation</a>, you usually pass in what NLTK's tokenization returned (which is a list of words/tokens).</p>
",""
"51189990","2018-07-05 11:17:50","1","","51189483","<p>So there was another answer here which used a <code>defaultdict</code>. Mine goes a bit further and uses the resultant format I gave in the comments and works in linear time.</p>

<pre><code>list_2 = ['AAA\tADJ\tUK', 'AAA\tN\tUK', 'AAA\tN\tES', 'B\tADV\tUK', 'BB\tADV\tUK']

import collections

d = collections.defaultdict(lambda: collections.defaultdict(list))

for line in list_2:
    word, wordtype, lang = line.split('\t')
    d[word][lang].append(wordtype)
</code></pre>

<p><code>d</code> is </p>

<pre><code>defaultdict(&lt;function __main__.&lt;lambda&gt;&gt;,
            {'AAA': defaultdict(list, {'ES': ['N'], 'UK': ['ADJ', 'N']}),
             'B': defaultdict(list, {'UK': ['ADV']}),
             'BB': defaultdict(list, {'UK': ['ADV']})})
</code></pre>

<p>We can convert into a standard dict like so:</p>

<pre><code>{k: dict(v) for k, v in d.items()}

# {'AAA': {'ES': ['N'], 'UK': ['ADJ', 'N']},
#  'B': {'UK': ['ADV']},
#  'BB': {'UK': ['ADV']}}
</code></pre>

<p>We can access a word/lang combo simply by doing</p>

<pre><code>d['AAA']['UK']
# --&gt; ['ADJ', 'N']
</code></pre>
",""
"51107197","2018-06-29 18:02:27","1","","51097463","<p>Stemmers stem words not sentences, so tokenize the sentence and stem the tokens individually.</p>

<pre><code>&gt;&gt;&gt; from nltk import word_tokenize
&gt;&gt;&gt; from nltk.stem import SnowballStemmer

&gt;&gt;&gt; fr = SnowballStemmer('french')

&gt;&gt;&gt; sent = ""pommes, noisettes dor√©es &amp; moelleuses, la bo√Æte de 350g""
&gt;&gt;&gt; word_tokenize(sent)
['pommes', ',', 'noisettes', 'dor√©es', '&amp;', 'moelleuses', ',', 'la', 'bo√Æte', 'de', '350g']

&gt;&gt;&gt; [fr.stem(word) for word in word_tokenize(sent)]
['pomm', ',', 'noiset', 'dor', '&amp;', 'moelleux', ',', 'la', 'bo√Æt', 'de', '350g']

&gt;&gt;&gt; ' '.join([fr.stem(word) for word in word_tokenize(sent)])
'pomm , noiset dor &amp; moelleux , la bo√Æt de 350g'
</code></pre>
",""
"51011519","2018-06-24 15:50:03","6","","51011474","<p>This may not be the most efficient solution, but it is easy to understand and maintain, and I frequently use it myself. I use the Counter and Pandas:</p>

<pre><code>import pandas as pd
from collections import Counter
</code></pre>

<p>Apply the counter to each document and construct a term-frequency matrix:</p>

<pre><code>df = pd.DataFrame(list(map(Counter, doc_clean)))
</code></pre>

<p>Some fields in the matrix are undefined. They correspond to the words that do not occur in particular document. Count the occurrences:</p>

<pre><code>counts = df.notnull().sum()
</code></pre>

<p>Now, select the words that do not occur often enough:</p>

<pre><code>rare_words = counts[counts &lt; 0.05 * len(doc_clean)].index.tolist()
</code></pre>
",""
"50972645","2018-06-21 15:44:11","0","","50972571","<p>this should be easy:</p>

<pre><code>In [15]: data = [('about', 'IN'),('above', 'JJ'),('account', 'NN'),('address', 'NN'),('after', 'IN')]

In [16]: df = pd.DataFrame(data, columns=['Word', 'POS'])

In [17]: df
Out[17]:
      Word POS
0    about  IN
1    above  JJ
2  account  NN
3  address  NN
4    after  IN
</code></pre>
",""
"50945216","2018-06-20 09:49:19","6","","50941438","<p>I don't know if I Understood your problem correctly. NLTK subtree is just normal Python List. So you can carry out normal list operations here as well.Try this code snippet instead of for loop part in your code.</p>

<pre><code>for subtree in chunked.subtrees(filter=lambda t: t.label() == 'Name'):
    full_name = []
    for word, pos in subtree:
        full_name.append(word)
        st = "" "".join(full_name)  # iterate till the variable catches full name as tokenizer segments words.
        if st in male:
            subtree[:] = [(""Male"",pos)]  # replacing the subtree with our own value
        elif st in female:
            subtree[:] = [(""Female"",pos)]
</code></pre>

<p>output:</p>

<pre><code>&gt; (S (Name male/NNP) arrives/VBZ in/IN (Name Port/NNP Royal/NNP) in/IN (Name Jamaica/NNP) to/TO commandeer/VB a/DT ship/NN ./. Despite/IN rescuing/VBG (Name female/NNP) ,/, the/DT daughter/NN of/IN (Name male/NNP) ,/, from/IN drowning/VBG ,/, he/PRP is/VBZ jailed/VB for/IN piracy/NN./.)
</code></pre>
",""
"50935353","2018-06-19 19:04:05","2","","50918092","<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np

cluster = ['Line a baking pan with a sheet of parchment paper.',
            'Line the cake pan with parchment paper.',
            'Line the bottom with parchment paper.',
            'Line a baking pan with parchment paper.']

corpus = ['Add vinegar and sugar.',
          'Remove pan from heat and let stand 5 minutes.',
          'Line the pan with parchment paper.']

# Train tfidf on cluster
tfidf = TfidfVectorizer()
tfidf_cluster = tfidf.fit_transform(cluster)

# Tranform the corpus using the trained tfidf
tfidf_corpus = tfidf.transform(corpus)

# Cosine similarity
cos_similarity = np.dot(tfidf_corpus, tfidf_cluster.T).A
avg_similarity = np.mean(cos_similarity, axis=1)

cos_similarity
Out[271]: 
array([[0.        , 0.        , 0.        , 0.        ],
       [0.31452723, 0.36145869, 0.        , 0.43855558],
       [0.50673521, 0.8242027 , 0.7139548 , 0.70655744]])

avg_similarity
Out[272]: array([0.        , 0.27863537, 0.68786254])
</code></pre>
",""
"50906599","2018-06-18 09:41:00","2","","50906210","<p>Note that you are printing a sparse matrix so the output looks different compared to printing a standard dense matrix. See below the main components:</p>

<ul>
<li>The tuple represents: <code>(document_id, token_id)</code></li>
<li>The value following the tuple represents the tf-idf score of a given token in a given document</li>
<li>The tuples that are not there have a tf-idf score of 0</li>
</ul>

<p>If you want to find what token the <code>token_id</code> corresponds to, check the <code>get_feature_names</code> method.</p>
",""
"50816168","2018-06-12 11:34:59","1","","50814296","<p>In Python you can use <strong>expressions</strong> in a list comprehension but not <strong>statements</strong>. You may want to look at the <a href=""https://stackoverflow.com/questions/4728073/what-is-the-difference-between-an-expression-and-a-statement-in-python"">diffence between expressions and statements</a> in Python. </p>

<p>As of your question in the comment on how to compute <code>((# of matching nodes) * (1 if the relationship doesn't match, 2.5 if it does))</code>, which is the numerator of the <strong>SABK</strong> similarity function of the paper in your question, you can do it using a generator and the <code>sum</code> function: </p>

<pre><code>theta = 2.5
sim = sum((((d1[0] == d2[0]) + (d1[2] == d2[2])) * (theta if d1[1] == d2[1] else 1) for d1, d2 in product(deps1, deps2)))
</code></pre>

<p>Or, if you want to separate the code for the function of the similarity per sentence, which improves the readability of the code:</p>

<pre><code>def sim_per_sentence(d1, d2):
    matching_nodes = (d1[0] == d2[0]) + (d2[0] == d2[0])
    relation_sim = theta if d1[1] == d2[1] else 1
    return matching_nodes * relation_sim

sim = sum((sim_per_sentence(d1, d2) for d1, d2 in product(deps1, deps2)))
</code></pre>

<p>Remark that it may be way more efficient to use a <a href=""https://stackoverflow.com/questions/47789/generator-expressions-vs-list-comprehension"">generator expression instead of a list comprehension</a> if you have many elements in <code>deps1</code> and <code>deps2</code>, as the individual results of each iteration do not need to be stored in memory. </p>
",""
"50751811","2018-06-08 00:29:03","0","","50742516","<p>A spaCy <code>Doc</code> object also lets you iterate over the <code>doc.sents</code>, which are <a href=""https://spacy.io/api/span"" rel=""noreferrer""><code>Span</code> objects</a> of the individual sentence. To get a span's start and end index in the parent document you can look at the <code>start</code> and <code>end</code> attribute. So if you iterate over the sentences and subtract the sentence start index from the <code>token.i</code>, you get the token's relative index within the sentence:</p>

<pre><code>for sent in doc.sents:
    for token in sent:
        print(token.text, token.i - sent.start)
</code></pre>

<p>The default sentence segmentation uses the dependency parse, which is usually more accurate. However, you can also plug in a rule-based or entirely custom solution (<a href=""https://spacy.io/usage/linguistic-features#section-sbd"" rel=""noreferrer"">see here</a> for details).</p>
",""
"50722360","2018-06-06 14:00:50","0","","50711654","<p>A function that converts a dependency tree to a sequence of transitions is called an oracle. It is a necessary component of a statistical parser.
The transitions you described (shift, reduce-l, reduce-r)¬π are those of the <em>arc-standard</em> transition system (not the <em>arc-eager</em> system, which is: shift, left-arc, right-arc, reduce).</p>

<p>Pseudo-code for an arc-standard oracle:</p>

<pre><code>stack = [] # stack[0] is the top of the stack
buffer = [w1, w2, ..., wn]

while len(buffer) &gt; 0 or len(stack) != 1:
    if stack[0] is the head of stack[1]:
        apply left-arc
    if stack[1] is the head of stack[0]:
        if there is a token in the buffer whose head is stack[0] 
            # (i.e not all children of stack[1] have been attached to it)
            apply shift
        else:
            apply right-arc
</code></pre>

<p>These <a href=""http://demo.clab.cs.cmu.edu/fa2015-11711/images/b/b1/TbparsingSmallCorrection.pdf"" rel=""nofollow noreferrer"">slides</a> present the two parsing algorithms and their oracles.</p>

<p>¬πReduce-left, reduce-right are often named right-arc and left-arc in the context of dependency parsing.</p>
",""
"50689970","2018-06-04 23:13:28","0","","50685343","<p><strong>TL;DR</strong>:</p>

<pre><code>pip3 install -U pywsd
</code></pre>

<p>Then:</p>

<pre><code>&gt;&gt;&gt; from pywsd.utils import lemmatize_sentence

&gt;&gt;&gt; text = 'i like cars'
&gt;&gt;&gt; lemmatize_sentence(text)
['i', 'like', 'car']
&gt;&gt;&gt; lemmatize_sentence(text, keepWordPOS=True)
(['i', 'like', 'cars'], ['i', 'like', 'car'], ['n', 'v', 'n'])

&gt;&gt;&gt; text = 'The cat likes cars'
&gt;&gt;&gt; lemmatize_sentence(text, keepWordPOS=True)
(['The', 'cat', 'likes', 'cars'], ['the', 'cat', 'like', 'car'], [None, 'n', 'v', 'n'])

&gt;&gt;&gt; text = 'The lazy brown fox jumps, and the cat likes cars.'
&gt;&gt;&gt; lemmatize_sentence(text)
['the', 'lazy', 'brown', 'fox', 'jump', ',', 'and', 'the', 'cat', 'like', 'car', '.']
</code></pre>

<hr>

<p>Otherwise, take a look at how the function in <code>pywsd</code>:</p>

<ul>
<li>Tokenize the string</li>
<li>Uses the POS tagger and maps to WordNet POS tagset</li>
<li>Attempts to stem</li>
<li>Finally calling the lemmatizer with the POS and/or stems</li>
</ul>

<p>See <a href=""https://github.com/alvations/pywsd/blob/master/pywsd/utils.py#L129"" rel=""noreferrer"">https://github.com/alvations/pywsd/blob/master/pywsd/utils.py#L129</a></p>
",""
"50533439","2018-05-25 16:28:17","3","","50533070","<p><code>words.words()</code> returns a list, which takes <code>O(n)</code> time for checking whether a word is present in the list or not. To optimize the time complexity, you can take create the set out of this list which offers the constant time search.<br>
Second optimization is that <code>remove()</code> method on list takes <code>O(n)</code> time. You can maintain a separate list to remove that overhead. To know more about the complexity of various operations, you can refer to <a href=""https://www.ics.uci.edu/~pattis/ICS-33/lectures/complexitypython.txt"" rel=""nofollow noreferrer"">https://www.ics.uci.edu/~pattis/ICS-33/lectures/complexitypython.txt</a></p>

<pre><code>set_of_words = set(words.words())

def check_for_word(sentence):
    s = sentence.split(' ')
    return ' '.join(w for word in s if word in set_of_words)
</code></pre>
",""
"50508249","2018-05-24 11:29:23","6","","50508049","<p><strong>For Stemming:</strong></p>

<p>NLTK has <a href=""http://www.nltk.org/howto/stem.html"" rel=""noreferrer"">Porter Stemmer</a> which is widely used. </p>

<p>For Russian, someone seems to have used <a href=""https://stackoverflow.com/questions/45696028/snowballstemmer-for-russian-words-list"">Snowball Stemmer</a>. </p>

<p><strong>For Lemmatization:</strong></p>

<p>I prefer <a href=""https://spacy.io/api/lemmatizer"" rel=""noreferrer"">SpaCy for lemmatization</a>. </p>

<p>For Russian, someone has been working on this <a href=""https://github.com/explosion/spaCy/issues/1275"" rel=""noreferrer"">here</a>. </p>

<p>Another lemmatizer for Russian text can be found <a href=""https://github.com/alexeyknorre/Lemmatizer"" rel=""noreferrer"">here.</a></p>
",""
"50401662","2018-05-17 23:08:15","0","","50379985","<p>Firstly, as already pointed out by @lurker in the comments, always use <code>phrase/2</code> or <code>phrase/3</code> when calling DCGs. Secondly, as pointed out by @TomasBy and @WillBeason, your DCG is describing a list containing the atoms <code>x</code>, <code>y</code> and <code>z</code>, separated by commas. So, to test if xz is an actual sentence (I assume that's what <code>s</code> stands for) according to your grammar, you would query:</p>

<pre><code>?- phrase(s,[x,z]).
true ;
false.
</code></pre>

<p>Indeed it is. Now let's take a look at the most general query, that is, ask <em>Which sentences are there?</em>:</p>

<pre><code>?- phrase(s,S).
ERROR: Out of local stack
</code></pre>

<p>That didn't go too well. The reason for that is the order of the DCG rules: calling <code>s//0</code> leads to the first rule of <code>e//0</code> being called, that recursively calls <code>e//0</code> again, that is to say, the first rule of <code>e//0</code> and so the loop continues until Prolog runs out of stack. So let's alter the ordering of the rules by putting the non-recursive rule first... </p>

<pre><code>s --&gt; e.

e --&gt; [z].          % &lt;- moved here from last position
e --&gt; [x], e.
e --&gt; [y], e.
</code></pre>

<p>... and retry the query:</p>

<pre><code>?- phrase(s,S).
S = [z] ;
S = [x, z] ;
S = [x, x, z] ;
S = [x, x, x, z] ;
.
.
.
</code></pre>

<p>Now we're getting actual solutions. So the ordering of the DCG rules <strong>does matter</strong>. However, the listing of the answers is unfair, since the last rule of <code>e//0</code>, the one dealing with <code>y</code>, is actually never called. That can be remedied by prefixing a goal <code>length/2</code>. The query...</p>

<pre><code>?- length(S,_).
S = [] ;
S = [_G3671] ;
S = [_G3671, _G3674] ;
S = [_G3671, _G3674, _G3677] ;
.
.
.
</code></pre>

<p>...yields lists with all possible lengths, so prefixing it to the DCG call, will make Prolog look for all solutions of length 0  before moving on to length 1 before moving on to length 2 and so on...</p>

<pre><code>?- length(S,_), phrase(s,S).
S = [z] ;                    % &lt;- solutions of length 1 from here
S = [x, z] ;                 % &lt;- solutions of length 2 from here
S = [y, z] ;
S = [x, x, z] ;              % &lt;- solutions of length 3 from here
S = [x, y, z] ;
S = [y, x, z] ;
S = [y, y, z] ;
S = [x, x, x, z] ;           % &lt;- solutions of length 4 from here
.
.
.
</code></pre>

<p>So your grammar is actually producing sentences of arbitrary length, as it should be. Moving on to your seven sentence example, if your application requires limiting the length of the lists, you can do that by prefixing a goal <code>between/3</code> to your query...</p>

<pre><code>?- between(1,3,N), length(S,N), phrase(s,S).
N = 1,
S = [z] ;
N = 2,
S = [x, z] ;
N = 2,
S = [y, z] ;
N = 3,
S = [x, x, z] ;
N = 3,
S = [x, y, z] ;
N = 3,
S = [y, x, z] ;
N = 3,
S = [y, y, z] ;
false.
</code></pre>

<p>... that will now yield all seven sentences consisting of at most 3 words. Note that your example { xz , <strong>xy</strong> , xyz , <strong>xyxz</strong> , z , <strong>xxyz</strong> , <em>xyz</em> } is not exactly the set of sentences described by your grammar. The element <strong>xy</strong> is not a sentence at all according to the grammar rules. The sentences <strong>xyxz</strong> and <strong>xxyz</strong> are specified by your grammar but require a maximum length of at least four words, which will yield sixteen answers. And the last of the seven sentences, <em>xyz</em>, appears twice in your example, unless you meant the query...</p>

<pre><code>?- phrase(s,[X,y,z]).
X = x ;
X = y ;
false.
</code></pre>

<p>... that yields to two sentences, the first of which still constitutes a duplicate.</p>

<p>Finally, if you really desperately need to get atoms as an answer, you can alter the DCG to put the codes corresponding to <code>x</code>, <code>y</code> and <code>z</code> into the list instead of the actual atoms. Then you can use <code>atom_codes/2</code> to get the sentences as a single atom instead of a list of words:</p>

<pre><code>s --&gt; e.

e --&gt; [0'z].      % 0'z denotes the code corresponding to z
e --&gt; [0'x], e.   % 0'x denotes the code corresponding to x
e --&gt; [0'y], e.   % 0'y denotes the code corresponding to y

?- between(1,3,N), length(S,N), phrase(s,S), atom_codes(A,S).
N = 1,
S = [122],
A = z ;
N = 2,
S = [120, 122],
A = xz ;
N = 2,
S = [121, 122],
A = yz ;
N = 3,
S = [120, 120, 122],
A = xxz ;
N = 3,
S = [120, 121, 122],
A = xyz ;
N = 3,
S = [121, 120, 122],
A = yxz ;
N = 3,
S = [121, 121, 122],
A = yyz ;
false.
</code></pre>
",""
"50351368","2018-05-15 13:22:37","0","","50298074","<p>These are two distinct problems for PCFG:</p>

<ul>
<li><strong>recognition</strong>: does the sentence belong to the language generated by the CFG? (output: yes or no)</li>
<li><strong>parsing</strong>: what is the highest scoring tree for this sentence? (output: parse tree)</li>
</ul>

<p>The CKY algorithm video linked in the question only deals with the recognition problem. To perform the parsing problem simultaneously,
we need to
(i) maintain the score of each parsing item and
(ii) keep track of the hierarchical relationships (e.g. if we use the rule S -> NP VP: we must keep track of which NP and which VP are used to predict S).</p>

<p>Notations:</p>

<ul>
<li>A <strong>parsing item</strong> <code>[X, i, j]: s</code> means that there is a node labelled X spanning token <em>i</em> (included) to <em>j</em> (excluded) with score <em>s</em>.
The score is the log probability of the subtree rooted in <code>X</code>.</li>
<li>The sentence is a sequence of words <code>w_1 w_2 ... w_n</code>.</li>
</ul>

<p>At an abstract level, PCFG parsing can be formulated as a set of deduction rules:</p>

<ol>
<li><p>Scan Rule (read tokens)</p>

<pre><code>____________________________{precondition: X -&gt; w_i is a grammar rule
[X, i, i+1]: log p(X -&gt; w_i)
</code></pre>

<p>Gloss: if there is a rule <code>X -&gt; w_i</code> in the grammar, then we can add the item <code>[X, i, i+1]</code> in the chart.</p></li>
<li><p>Complete Rule (create a new constituent bottom up)</p>

<pre><code>[X, i, k]: s1     [Y, k, j]: s2
_____________________________________{precondition: Z -&gt; X Y is a grammar rule
[Z, i, j]: s1 + s2 + log p(Z -&gt; X, Y)
</code></pre>

<p>Gloss: if there are 2 parsing items <code>[X, i, k]</code> and <code>[Y, k, j]</code> in the chart, and a rule <code>Z -&gt; X Y</code> in the grammar, then we can add <code>[Z, i, j]</code> to the chart.</p></li>
</ol>

<p>The goal of weighted parsing is to deduce the parsing item <code>[S, 0, n]:s</code> (<code>S</code>: axiom, <code>n</code>: length of sentence) with the highest score <code>s</code>.</p>

<p><strong>Pseudo code for whole algorithm</strong></p>

<pre><code># The chart stores parsing items and their scores
chart[beginning(int)][end(int)][NonTerminal][score(float)]

# the backtrack table is used to recover the parse tree at the end
backtrack[beginning][end][NonTerminal][item_left, item_right]

# insert a new item in the chart
# for a given span (i, j) and nonterminal X, we only need to
# keep the single best scoring item.
def insert(X, i, j, score, Y, Z, k):
    if X not in chart[i][j] or chart[i][j][X] &lt; score             
        chart[i][j][X] &lt;- score
        backtrack[i][j][X] &lt;- (Y, i, k), (Z, k, j)

n &lt;- length of sentence

for i in range(0, n):
    # apply scan rule
    insert(X, i, i+1, log p(X -&gt; w_i)) for each grammar rule X -&gt; w_i

for span_length in range(2, n):
    for beginning in range(0, n - span_length):
        end &lt;- beginning + span_length

        for k in range(beginning+1, end -1):
            # apply completion rules
            for each grammar rule X -&gt; Y Z such that 
                * Y is in chart[beginning][k]
                * Z is in chart[k][end]

                score_left &lt;- chart[beginning][k][Y]
                score_right &lt;- chart[k][end][Z]

                insert(X, beginning, end, log p(X -&gt; Y Z) + score_left + score_right)

if there is S (axiom) in chart[0][n], then parsing is successful
    the score (log probability) of the best tree is chart[0][n][S]
    the best tree can be recovered recursively by following pointers from backtrack[0][n][S]
else:
    parsing failure, the sentence does not belong to the language
    generated by the grammar
</code></pre>

<p>Some notes:</p>

<ul>
<li>Time complexity is O(n^3 ‚ãÖ |G|) where |G| is the size of the grammar.</li>
<li>The algorithm assumes that the grammar is in <a href=""https://en.wikipedia.org/wiki/Chomsky_normal_form"" rel=""nofollow noreferrer"">Chomsky Normal Form</a></li>
</ul>
",""
"50271356","2018-05-10 10:59:12","2","","50260935","<p>You can use foreach to get all annotation contained in a sentence and store in feature map based on their majorType or kind. </p>

<pre><code>Imports: {
import static gate.Utils.*;
}
Phase:usingJAVAinRHS  
Input:  Lookup Sentence  
Options: control = appelt
Rule: javainRHS1  
(  
{Sentence contains {Lookup.majorType == Team}}  
)  
:team  
--&gt;  
{  
    gate.AnnotationSet team = (gate.AnnotationSet)bindings.get(""team""); 
    gate.FeatureMap features = Factory.newFeatureMap(); 
    for(Annotation annotation:team.inDocumentOrder())  
    {
        if(annotation.getType() == ""Lookup""){
            features.put(annotation.getFeatures().get(""majorType""),stringFor(doc,annotation));
        }
        else{
            features.put(annotation.getType(), stringFor(doc,annotation));
        }
    }
    features.put(""rule"",""javainRHS1"");  
    outputAS.add(team.firstNode(), team.lastNode(), ""Team"",features); 
}  
</code></pre>
",""
"50232063","2018-05-08 11:03:39","3","","50229769","<blockquote>
  <p>However I can imagine if we use bag-of-words based on letter trigrams there'll easily be different words sharing common patterns so it seems difficult to recover the information of which words are in the document by such representation.</p>
</blockquote>

<p>That's correct, because the model does not explicitly aim to learn the posterior probabilities by using the information from the words. Rather it uses the information from the tri-grams.</p>

<blockquote>
  <p>How was this issue solved? or it doesn't really matter to the query/title experiment in the paper?</p>
</blockquote>

<p>This issue can be solved by adding a CNN/LSTM layer to represent a higher (close to words) abstraction from the trigram inputs. The research reported in <a href=""https://dl.acm.org/citation.cfm?doid=2766462.2767702"" rel=""nofollow noreferrer"">this paper</a> employs a CNN on top of trigram inputs as shown below.</p>

<p><a href=""https://i.sstatic.net/4QcO9.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/4QcO9.png"" alt=""enter image description here""></a></p>
",""
"50193264","2018-05-05 19:27:15","0","","49342658","<p><strong>Adequacy:</strong> How much of the source information is preserved?</p>

<p><strong>Fluency:</strong> How good is the generated target language quality?</p>

<p><strong>For Machine Translation task,</strong></p>

<p>English(S-V-O): A dog chased a cat</p>

<p>Reference translation in Hindi would look like:</p>

<p>Hindi(S-O-V): A dog a cat chased</p>

<p>When 1-gram precision is high, correct word-to-word translations will be high but order of those words might not be correct in translated sentence. But still, majority of the source information is preserved.</p>

<p>High 1-gram, low N-gram(2-gram) precision value example: chased dog cat</p>

<p>When N-gram precision is high, order of those words will be preserved to some extent and you would get a fluent sentence in Hindi.</p>

<p>High N-gram(2-gram) precision example: A dog cat chased</p>
",""
"50191902","2018-05-05 16:55:13","3","","50191231","<p>There's a method <code>[most_similar()][1]</code> that will report the words of the closest vectors, by cosine-similarity in the model's coordinates, to a given word. For example:</p>

<pre><code>similars = loaded_w2v_model.most_similar('bright')
</code></pre>

<p>However, Word2vec won't find strictly synonyms ‚Äì just words that were contextually-related in its training-corpus. These are often synonym-like, but also can be similar in other ways ‚Äì such as used in the same topical domains, or able to replace each other functionally. (In that last respect, sometimes the highly-similar word-vectors are for <em>antonyms</em>, because words like 'hot' and 'cold' appear in the same places, referring the the same aspect of something.)</p>

<p>Plain word2vec also doesn't deal with polysemy (that a token like 'bright' is both a word for 'well-lit' and a word for 'smart') well. So the list of most-similar words for 'bright' will include a mix from its alternate senses.</p>
",""
"50158249","2018-05-03 15:03:06","1","","50155188","<p>Thats because lemmatization is done before stop word removal. And then the lemmatized stopwords are not found in the stopwords set provided by <code>stopwords.words('spanish')</code>.</p>

<p>For complete working order of CountVectorizer, please refer to <a href=""https://stackoverflow.com/a/49775000/3374996"">my other answer here</a>. Its about TfidfVectorizer but the order is same. In that answer, step 3 is the lemmatization and step 4 is stopword removal.</p>

<p>So now to remove the stopwords, you have two options:</p>

<p>1) You lemmatize the stopwords set itself, and then pass it to <code>stop_words</code> param in CountVectorizer.</p>

<pre><code>my_stop_words = [lemma(t) for t in stopwords.words('spanish')]
vectorizer = CountVectorizer(stop_words=my_stop_words, 
                             tokenizer=LemmaTokenizer())
</code></pre>

<p>2) Include the stop word removal in the <code>LemmaTokenizer</code> itself.</p>

<pre><code>class LemmaTokenizer(object):
    def __call__(self, text):
        return [lemma(t) for t in word_tokenize(text) if t not in stopwords.words('spanish')]
</code></pre>

<p>Try these and comment if not working.</p>
",""
"50039385","2018-04-26 09:25:05","0","","50039310","<h1>In Short</h1>

<p>No, Wordnet lemmatizer in NLTK is only for English.</p>

<h1>In Long</h1>

<p>If we look at <a href=""https://github.com/nltk/nltk/blob/develop/nltk/stem/wordnet.py#L15"" rel=""noreferrer"">https://github.com/nltk/nltk/blob/develop/nltk/stem/wordnet.py#L15</a></p>

<pre><code>class WordNetLemmatizer(object):

    def __init__(self):
        pass

    def lemmatize(self, word, pos=NOUN):
        lemmas = wordnet._morphy(word, pos)
        return min(lemmas, key=len) if lemmas else word

    def __repr__(self):
        return '&lt;WordNetLemmatizer&gt;' 
</code></pre>

<p>It's based on the <code>_morphy()</code> function at <a href=""https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/wordnet.py#L1764"" rel=""noreferrer"">https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/wordnet.py#L1764</a> which applies several <a href=""https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/wordnet.py#L1799"" rel=""noreferrer"">English specific substitutions</a></p>

<pre><code>    MORPHOLOGICAL_SUBSTITUTIONS = {
    NOUN: [('s', ''), ('ses', 's'), ('ves', 'f'), ('xes', 'x'),
           ('zes', 'z'), ('ches', 'ch'), ('shes', 'sh'),
           ('men', 'man'), ('ies', 'y')],
    VERB: [('s', ''), ('ies', 'y'), ('es', 'e'), ('es', ''),
           ('ed', 'e'), ('ed', ''), ('ing', 'e'), ('ing', '')],
    ADJ: [('er', ''), ('est', ''), ('er', 'e'), ('est', 'e')],
    ADV: []}

MORPHOLOGICAL_SUBSTITUTIONS[ADJ_SAT] = MORPHOLOGICAL_SUBSTITUTIONS[ADJ]
</code></pre>
",""
"49966693","2018-04-22 13:54:03","1","","49959230","<p>If you already wrote a context-free grammar, you can use the ChartParser of NLTK to parse any input, as described here: <a href=""http://www.nltk.org/book/ch08.html"" rel=""nofollow noreferrer"">http://www.nltk.org/book/ch08.html</a> </p>

<p>I think, however, that a hand written grammar will not be robust enough to deal with the huge amount of variations your users could write. These have gone out of fashion decades ago due to their poor performance and in constituency parsing one rather uses treebanks to generate grammars.</p>

<p>Depending on what exactly you want to archive, I suggest you also take a look at a dependency parser e.g. from <a href=""https://spacy.io/usage/linguistic-features#navigating"" rel=""nofollow noreferrer"">spaCy</a>. They are faster and allow you to easily navigate from the predicate of the sentence to its subject and objects.</p>
",""
"49946748","2018-04-20 17:16:59","0","","49944599","<p>When using conda, version 0.3.4 of textacy is installed. This version does not have the the vectorizer. Instead install it through the PyPi project.</p>

<p><a href=""https://pypi.org/project/textacy/"" rel=""nofollow noreferrer"">https://pypi.org/project/textacy/</a></p>

<p>to check if you have the vectorizer you can do the following:</p>

<pre><code>In [1]: import textacy

In [2]: dir(textacy)
Out[2]:
['Corpus',
'Doc',
'TextStats',
'TopicModel',
'Vectorizer',
'__builtins__',
'__cached__',
'__doc__',
'__file__',
'__loader__',
'__name__',
'__package__',
'__path__',
'__spec__',
'__version__',
'about',
'absolute_import',
'cache',
'compat',
'constants',
'corpus',
'data_dir',
'doc',
'extract',
'io',
'load_spacy',
'logger',
'logging',
'network',
'os',
'preprocess',
'preprocess_text',
'spacy_utils',
'text_stats',
'text_utils',
'tm',
'utils',
'viz',
'vsm']
</code></pre>
",""
"49935903","2018-04-20 07:03:35","1","","49929066","<p>Lack of labeled data means you cannot apply any semantic classification method using word vectors, which would be the optimal solution to your problem. An alternative however could be to construct the document frequencies of your token n-grams and assume importance based on some smoothed variant of idf (i.e. words that tend to appear often in descriptions probably carry some semantic weight). You can then inspect your sorted-by-idf list of words and handpick(/erase) words that you deem important(/unimportant). The results won't be perfect, but it's a clean and simple solution given your lack of training data. </p>
",""
"49891951","2018-04-18 05:39:18","0","","49891820","<p>you probably got a null in one or more rows of that column.</p>

<p>fill it with empty string like:</p>

<pre><code>df['CONTENT'] = df['CONTENT'].fillna('')
</code></pre>
",""
"49876334","2018-04-17 10:59:41","2","","49871737","<p>A basic approach for the sentiment analysis, involves making a vocabulary out of training corpus and use it for making feature vectors for your data. A vocabulary as large as few hundred thousand words is quite common and nothing to worry about. The main challenge in this approach is actually opposite of what you are thinking. You should find ways of increasing the size of your vocabulary rather than decreasing it.</p>

<p>You can try to enhance the vocabulary by using other sentiment analysis lexicons like <a href=""http://nmis.isti.cnr.it/sebastiani/Publications/LREC10.pdf"" rel=""nofollow noreferrer"">SentiWordNet</a> too.</p>

<p>As far as implementation of your approach is concerned you can build a scikit pipeline envolving <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"" rel=""nofollow noreferrer"">CountVectorize</a> to build vocabulary and feature vector. One advantage of using CountVectorize() for building vocabulary is that it uses a sparse matrix for building the vector which handles your concern of large size. Then using <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"" rel=""nofollow noreferrer"">TfIdf Vectorizer</a> for calculating term frequencies and inverse term frequencies, and then finally a model for training.</p>

<p>Consider adding some more features to your vector apart from pure bag of words. Be sure to perform a <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"" rel=""nofollow noreferrer"">GridSearch</a> on your model and preprocessing stages to fine-tune the parameters for best accuracy. I recently did a similar project for sentiment analysis of stocktwits data. I used a Naive Bayes classifier and got an accuracy of 72%. Naive Bayes proved to be better than even some deep learning models like RNN/DNN classifiers. The model selection, though independent of your question, is an integral part of building your project so keep tweaking it till you get good results. Check out my <a href=""https://github.com/penguin2048/StockIt"" rel=""nofollow noreferrer"">project</a> if you want some insights on my implementation.   </p>

<p>Be mindful of following points while doing your project:</p>

<ul>
<li>Some researchers believe that stop words actually add meaning to sentiment so I would recommend not removing them during the preprocessing phase. See <a href=""http://www.lrec-conf.org/proceedings/lrec2014/pdf/292_Paper.pdf"" rel=""nofollow noreferrer"">this paper</a></li>
<li>Always use domain knowledge while doing sentiment analysis. A negative sentiment in one domain like ""<em>predictable</em> movie"" can be positive in other like ""<em>predictable</em> share market"".</li>
<li>Don't remove words on your own (on the basis of frequency as you mentioned in the question) from the vocabulary. TfIdf normalization is meant for this purpose only. </li>
</ul>

<p>The field of sentiment analysis is filled with numerous researches and exciting new techniques. I would recommend you to read some papers like <a href=""https://www.sciencedirect.com/science/article/pii/S0306457315000242"" rel=""nofollow noreferrer"">this</a> by pioneers in this field.</p>
",""
"49784853","2018-04-11 21:40:30","0","","49780473","<p>If you add <code>print</code> statements to show the intermediate steps, you can see where the problem is introduced:</p>

<pre><code>sentence_splitter_regex = ""(?&lt;!Mr|Ms)(?&lt;!Mrs)[.!?]""
dialogue_sentences_list = re.split(sentence_splitter_regex, dialogue_sentences)
print(""dialogue sentences:"", dialogue_sentences_list)
other_sentences_list = re.split(sentence_splitter_regex, other_sentences)
print(""other sentences:"", other_sentences_list)

sentences_in_paragraph  = list(map(lambda x: ""‚Äú"" + x.strip() + ""‚Äù"", dialogue_sentences_list)) 
sentences_in_paragraph += list(map(lambda x: x.strip(), other_sentences_list))
</code></pre>

<pre class=""lang-none prettyprint-override""><code>dialogue sentences ['Dirty, Mr. Jones', ' Look at my shoes', ' Not a speck on them', '']
other sentences ['    This is a non-dialogue sentence', '']
</code></pre>

<p>The <code>re.split</code> is leaving an empty element at the end. You can fix this by processing the result using a <code>for</code> comprehension with an <code>if</code> clause to not include empty strings:</p>

<pre><code>[sentence for sentence in sentences_with_whitespace if sentence.strip() != '']
</code></pre>

<p>You should put this code inside a new function <code>split_sentences_into_list</code> to keep your code organized. It also makes sense to move the <code>.strip()</code> processing from <code>get_all_sentences</code> into this function, by changing the first part of the <code>for</code> comprehension to <code>sentence.strip()</code>.</p>

<pre><code>import re

def split_sentences_into_list(sentences_string):
    sentence_splitter_regex = ""(?&lt;!Mr|Ms)(?&lt;!Mrs)[.!?]""
    sentences_with_whitespace = re.split(sentence_splitter_regex, sentences_string)
    return [sentence.strip() for sentence in sentences_with_whitespace if sentence.strip() != '']

def get_all_sentences(corpus):
    sentences_in_paragraph = []

    dialogue = False
    dialogue_sentences = """"
    other_sentences = """"

    example_paragraph = ""‚ÄúDirty, Mr. Jones? Look at my shoes! Not a speck on them.‚Äù    This is a non-dialogue sentence!""

    example_paragraph = example_paragraph.replace(""\n"", """") # remove newline

    for character in example_paragraph:
        if character == ""‚Äú"":
            dialogue = True
            continue
        if character == ""‚Äù"":
            dialogue = False
            continue

        if dialogue:
            dialogue_sentences += character
        else:
            other_sentences += character

    dialogue_sentences_list = split_sentences_into_list(dialogue_sentences)
    other_sentences_list = split_sentences_into_list(other_sentences)

    sentences_in_paragraph  = list(map(lambda x: ""‚Äú"" + x + ""‚Äù"", dialogue_sentences_list)) 
    sentences_in_paragraph += other_sentences_list

    print(sentences_in_paragraph)

get_all_sentences(None)
</code></pre>

<p>This has the expected output:</p>

<pre><code>['‚ÄúDirty, Mr. Jones‚Äù', '‚ÄúLook at my shoes‚Äù', '‚ÄúNot a speck on them‚Äù', 'This is a non-dialogue sentence']
</code></pre>

<p>By the way, standard Python style is to use <code>for</code> comprehensions instead of <code>map</code> and <code>lambda</code> when possible. It would make your code shorter in this case:</p>

<pre><code># from
sentences_in_paragraph  = list(map(lambda x: ""‚Äú"" + x + ""‚Äù"", dialogue_sentences_list)) 
# to
sentences_in_paragraph  = [""‚Äú"" + x + ""‚Äù"" for x in dialogue_sentences_list]
</code></pre>
",""
"49584275","2018-03-31 04:33:53","3","","49564176","<p>Take a look at <a href=""https://stackoverflow.com/questions/47769818/why-is-my-nltk-function-slow-when-processing-the-dataframe"">Why is my NLTK function slow when processing the DataFrame?</a>, there's no need to iterate through all rows multiple times if you don't need intermediate steps. </p>

<p>With <code>ne_chunk</code> and solution from </p>

<ul>
<li><p><a href=""https://stackoverflow.com/questions/31836058/nltk-named-entity-recognition-to-a-python-list"">NLTK Named Entity recognition to a Python list</a> and</p></li>
<li><p><a href=""https://stackoverflow.com/questions/48660547/how-can-i-extract-gpelocation-using-nltk-ne-chunk"">How can I extract GPE(location) using NLTK ne_chunk?</a></p></li>
</ul>

<p>[code]:</p>

<pre><code>from nltk import word_tokenize, pos_tag, ne_chunk
from nltk import RegexpParser
from nltk import Tree
import pandas as pd

def get_continuous_chunks(text, chunk_func=ne_chunk):
    chunked = chunk_func(pos_tag(word_tokenize(text)))
    continuous_chunk = []
    current_chunk = []

    for subtree in chunked:
        if type(subtree) == Tree:
            current_chunk.append("" "".join([token for token, pos in subtree.leaves()]))
        elif current_chunk:
            named_entity = "" "".join(current_chunk)
            if named_entity not in continuous_chunk:
                continuous_chunk.append(named_entity)
                current_chunk = []
        else:
            continue

    return continuous_chunk

df = pd.DataFrame({'text':['This is a foo, bar sentence with New York city.', 
                           'Another bar foo Washington DC thingy with Bruce Wayne.']})

df['text'].apply(lambda sent: get_continuous_chunks((sent)))
</code></pre>

<p>[out]:</p>

<pre><code>0                   [New York]
1    [Washington, Bruce Wayne]
Name: text, dtype: object
</code></pre>

<p>To use the custom <code>RegexpParser</code> :</p>

<pre><code>from nltk import word_tokenize, pos_tag, ne_chunk
from nltk import RegexpParser
from nltk import Tree
import pandas as pd

# Defining a grammar &amp; Parser
NP = ""NP: {(&lt;V\w+&gt;|&lt;NN\w?&gt;)+.*&lt;NN\w?&gt;}""
chunker = RegexpParser(NP)

def get_continuous_chunks(text, chunk_func=ne_chunk):
    chunked = chunk_func(pos_tag(word_tokenize(text)))
    continuous_chunk = []
    current_chunk = []

    for subtree in chunked:
        if type(subtree) == Tree:
            current_chunk.append("" "".join([token for token, pos in subtree.leaves()]))
        elif current_chunk:
            named_entity = "" "".join(current_chunk)
            if named_entity not in continuous_chunk:
                continuous_chunk.append(named_entity)
                current_chunk = []
        else:
            continue

    return continuous_chunk


df = pd.DataFrame({'text':['This is a foo, bar sentence with New York city.', 
                           'Another bar foo Washington DC thingy with Bruce Wayne.']})


df['text'].apply(lambda sent: get_continuous_chunks(sent, chunker.parse))
</code></pre>

<p>[out]:</p>

<pre><code>0                  [bar sentence, New York city]
1    [bar foo Washington DC thingy, Bruce Wayne]
Name: text, dtype: object
</code></pre>
",""
"49341812","2018-03-17 20:53:59","11","","49341740","<p>Initialise a <code>WordNetLemmatizer</code> object, and lemmatize each word in your lines. You can perform inplace file I/O using the <code>fileinput</code> module.</p>

<pre><code># https://stackoverflow.com/a/5463419/4909087
import fileinput

lemmatizer = WordNetLemmatizer()
for line in fileinput.input('1865-Lincoln.txt', inplace=True, backup='.bak'):
    line = ' '.join(
        [lemmatizer.lemmatize(w) for w in line.rstrip().split()]
    )
    # overwrites current `line` in file
    print(line)
</code></pre>

<p><code>fileinput.input</code> redirects stdout to the open file when it is in use.</p>
",""
"49325971","2018-03-16 16:56:33","1","","49240058","<p>Start with building trees:</p>

<pre><code>from nltk import tree
treeData_rules = []

# Extract the CFG rules (productions) for the sentence
for item in treeData:
    for production in item.productions():
    treeData_rules.append(production)
treeData_rules
</code></pre>

<p>Then you can extract Probabilistic-CFG (PCFG) like this:</p>

<pre><code>from nltk import induce_pcfg

S = Nonterminal('S')
grammar_PCFG = induce_pcfg(S, treeData_rules)
print(grammar_PCFG)
</code></pre>
",""
"49244132","2018-03-12 20:45:18","0","","49132482","<p>The problem of make a ""list of POS constraints"" lies in the fact that those constrants will mainly depends on discourse domain. </p>

<p>I think you can face it from a n-gram approach. You can make POS tagging over a specific corpus (wikipedia articles for certain topic for example) then generate 2-grams or 3-grams (using grams of words) and calculate their frequencies, so you will get the most/less frequent POS combinantions. Finally, you can think about those POS combinations which not even appeared in the frecuency list, such sequences may be called ""part of speech constraints"".</p>
",""
"49218753","2018-03-11 09:58:21","13","","49218417","<p>One way to do it would be like this:</p>

<pre><code>import nltk

def pos_count(text, pos_list):
    sents = nltk.tokenize.sent_tokenize(text)
    words = (nltk.word_tokenize(sent) for sent in sents)
    tagged = nltk.pos_tag_sents(words, tagset='universal')
    tags = [tag[1] for sent in tagged for tag in sent]
    counts = nltk.FreqDist(tag for tag in tags if tag in pos_list)
    return counts
</code></pre>

<p>It's all very well explained in the <a href=""http://www.nltk.org/book/ch05.html"" rel=""nofollow noreferrer"">nltk book</a>. Test:</p>

<pre><code>In [3]: emma = nltk.corpus.gutenberg.raw('austen-emma.txt')

In [4]: pos_count(emma, ['DET', 'NOUN'])
Out[4]: FreqDist({'DET': 14352, 'NOUN': 32029})
</code></pre>

<p><strong>EDIT</strong>: it's a good idea to use <code>FreqDist</code> when you need to count things such as part of speech tags. I don't think it's very clever to have a function return a plain list with results, in principle how would you know which number represent which tag?</p>

<p>A possible (imho bad) solution is to return a sorted list of <code>FreqDist.values()</code>. This way the results are sorted in accordance with alphabetic order of the tag names. If you really want this replace <code>return counts</code> with <code>return [item[1] for item in sorted(counts.items())]</code>in the definition of the function above.</p>
",""
"49218581","2018-03-11 09:33:41","6","","49216816","<p>The parser you are instantiating contains no word vectors. Check <a href=""https://spacy.io/models/"" rel=""nofollow noreferrer"">https://spacy.io/models/</a> for an overview of models.</p>
",""
"49094807","2018-03-04 11:20:15","3","","49094311","<p>A complete example here - </p>

<pre><code>import nltk
from nltk.corpus import wordnet
from difflib import get_close_matches as gcm
from itertools import chain
from nltk.stem.porter import *

texts = [ "" apples are good. My teeth will fall out."",
          "" roses are red. cars are great to have""]

lmtzr = nltk.WordNetLemmatizer()
stemmer = PorterStemmer()

for text in texts:
    tokens = nltk.word_tokenize(text) # should sent tokenize it first
    token_lemma = [ lmtzr.lemmatize(token) for token in tokens ] # take your pick here between lemmatizer and wordnet synset.
    wn_lemma = [ gcm(word, list(set(list(chain(*[i.lemma_names() for i in wordnet.synsets(word)]))))) for word in tokens ]
    #print(wn_lemma) # works for unconventional words like 'teeth' --&gt; tooth. You might want to take a closer look
    tokens_final = [ stemmer.stem(tokens[i]) if len(tokens[i]) &gt; len(token_lemma[i]) else token_lemma[i] for i in range(len(tokens)) ]
    print(tokens_final)
</code></pre>

<p><strong>Output</strong></p>

<pre><code>['appl', 'are', 'good', '.', 'My', 'teeth', 'will', 'fall', 'out', '.']
['rose', 'are', 'red', '.', 'car', 'are', 'great', 'to', 'have']
</code></pre>

<p><strong>Explanation</strong></p>

<p>Notice <code>stemmer.stem(tokens[i]) if len(tokens[i]) &gt; len(token_lemma[i]) else token_lemma[i]</code> this is where the magic happens. If the lemmatized word is a subset of the main word, then the word gets stemmed, otherwise it just remains lemmatized.</p>

<p><strong>Note</strong></p>

<p>The lemmatization that you are attempting has some edge cases. <code>WordnetLemmatizer</code> is not smart enough to handle exceptional cases like 'teeth' --> 'tooth'. In those cases you would want to take a look at <code>Wordnet.synset</code> which might come in handy. </p>

<p>I have included a small case in the comments for your investigation.</p>
",""
"49091739","2018-03-04 03:40:48","0","","49090878","<p>This is a lot of code to go through to even check if the implementation is
correct, mainly because I don't know the terminology (like what exactly is a
vowel group) of the algorithm. I've looked up and google returns me a lot of
research papers (for which I can only see the abstract) for syllabification of
different languages, so I'm not sure if the code is correct at all.</p>

<p>But I have a few suggestions that might make your code faster:</p>

<ol>
<li><p>Move all you <code>strlen(word)</code> out of the <code>for</code>-loop conditions. Save the length
in a variable and use that variable instead. So from</p>

<pre><code>for (i = 0; i &lt; strlen(word); i++)
</code></pre>

<p>to</p>

<pre><code>size_t len = strlen(word);
for(i = 0; i &lt; len; i++)
</code></pre></li>
<li><p>Don't use <code>strchr</code> for checking if a character is a vowel. I'd use a lookup
table for this:</p>

<pre><code>// as global variable
char vowels[256];

int main(void)
{
    vowels['a'] = 1;
    vowels['e'] = 1;
    vowels['i'] = 1;
    vowels['o'] = 1;
    vowels['u'] = 1;
    ...
}
</code></pre>

<p>and when you want to check if a character is a vowel:</p>

<pre><code>// 0x20 | c make c a lower case character
if(vowel[0x20 | word[i]])
    syl_length++;
    i++;
    if (vowel_group) continue;
    vowel_group = 1;
}
</code></pre></li>
</ol>

<p>The first suggestion might give you a small performance increase, compilers are
pretty clever and might optimize that anyway. The second suggestion might give
you more performance, because it's just a look up. On the worst case
<code>strchr</code> will have to go through the whole <code>""aeiou""</code> array many times.<sup>1</sup></p>

<p>I also suggest that you profile your code. See <a href=""https://stackoverflow.com/questions/4141307/c-and-c-source-code-profiling-tools/4141345"">this</a> and <a href=""https://stackoverflow.com/questions/1794816/recommendations-for-c-profilers"">this</a>.</p>

<hr>

<p><strong>fotenotes</strong></p>

<p><sup>1</sup>I've made a very crud program that compares the runtime of the
suggestion. I added a few extra bits of code in the hope that the compiler
doesn't  optimize the functions to aggressively.</p>

<pre><code>#include &lt;stdio.h&gt;
#include &lt;string.h&gt;
#include &lt;time.h&gt;


int test1(time_t t)
{
    char text[] = ""The lazy dog is very lazy"";
    for(size_t i = 0; i &lt; strlen(text); ++i)
        t += text[i];

    return t;
}

int test2(time_t t)
{
    char text[] = ""The lazy dog is very lazy"";
    size_t len = strlen(text);
    for(size_t i = 0; i &lt; len; ++i)
        t += text[i];

    return t;
}

#define VOWELS ""aeiou""
char vowels[256];

int test3(time_t t)
{
    char text[] = ""The lazy dog is very lazy"";
    size_t len = strlen(text);
    for(size_t i = 0; i &lt; len; ++i)
    {
        if (strchr(VOWELS, text[i]))
            t += text[i];
        t += text[i];
    }

    return t;
}

int test4(time_t t)
{
    char text[] = ""The lazy dog is very lazy"";
    size_t len = strlen(text);
    for(size_t i = 0; i &lt; len; ++i)
    {
        if(vowels[0x20 | text[i]])
            t += text[i];
        t += text[i];
    }

    return t;
}

int main(void)
{
    vowels['a'] = 1;
    vowels['e'] = 1;
    vowels['i'] = 1;
    vowels['o'] = 1;
    vowels['u'] = 1;
    long times = 50000000;

    long tmp = 0;

    clock_t t1 = 0, t2 = 0, t3 = 0, t4 = 0;

    for(long i = 0; i &lt; times; ++i)
    {
        clock_t start,end;
        time_t t = time(NULL);

        start = clock();
        tmp += test1(t);
        end = clock();

        t1 += end - start;
        //t1 += ((double) (end - start)) / CLOCKS_PER_SEC;

        start = clock();
        tmp += test2(t);
        end = clock();

        t2 += end - start;

        start = clock();
        tmp += test3(t);
        end = clock();

        t3 += end - start;

        start = clock();
        tmp += test4(t);
        end = clock();

        t4 += end - start;
    }

    printf(""t1: %lf %s\n"", ((double) t1) / CLOCKS_PER_SEC, t1 &lt; t2 ? ""wins"":""loses"");
    printf(""t2: %lf %s\n"", ((double) t2) / CLOCKS_PER_SEC, t2 &lt; t1 ? ""wins"":""loses"");
    printf(""t3: %lf %s\n"", ((double) t3) / CLOCKS_PER_SEC, t3 &lt; t4 ? ""wins"":""loses"");
    printf(""t4: %lf %s\n"", ((double) t4) / CLOCKS_PER_SEC, t4 &lt; t3 ? ""wins"":""loses"");
    printf(""tmp: %ld\n"", tmp);


    return 0;
}
</code></pre>

<p>The results are:</p>

<pre class=""lang-none prettyprint-override""><code>$ gcc b.c -ob -Wall -O0
$ ./b 
t1: 10.866770 loses
t2: 7.588057 wins
t3: 10.801546 loses
t4: 8.366050 wins

$ gcc b.c -ob -Wall -O1
$ ./b
t1: 7.409297 loses
t2: 7.082418 wins
t3: 11.415080 loses
t4: 7.847086 wins

$ gcc b.c -ob -Wall -O2
$ ./b
t1: 6.292438 loses
t2: 5.855348 wins
t3: 9.306874 loses
t4: 6.584076 wins

$ gcc b.c -ob -Wall -O3
$ ./b
t1: 6.317390 loses
t2: 5.922087 wins
t3: 9.436450 loses
t4: 6.722685 wins
</code></pre>
",""
"49072831","2018-03-02 16:00:33","0","","49058275","<p>Before trying to match the words, you might want to do a little of pre-processing. So ""has"" or ""haven't"" end up ""transformed"" to ""have"". </p>

<p>I recommend you take a look at both stemming or lemmatizing: </p>

<p>NLTK's Wordnet Lemmatizer (one of my favorites): <a href=""http://www.nltk.org/_modules/nltk/stem/wordnet.html"" rel=""nofollow noreferrer"">http://www.nltk.org/_modules/nltk/stem/wordnet.html</a></p>

<p>NLTK's stemmers: <a href=""http://www.nltk.org/howto/stem.html"" rel=""nofollow noreferrer"">http://www.nltk.org/howto/stem.html</a></p>

<p>Note: for the lemmatizer to work well with verbs, you have to specify that they are in fact verbs. </p>

<pre><code>nltk.stem.WordNetLemmatizer().lemmatize('having', 'v')
</code></pre>

<p>Hope this helps! </p>
",""
"48984283","2018-02-26 08:32:30","0","","48984071","<p>The problem is not the stopwords, there are no stopwords by default. Problem is that the sentences in your test case is too short (1 character). </p>

<p><a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer"" rel=""noreferrer"">By default <code>tfidfVectorizer</code> uses <code>r'(?u)\b\w\w+\b'</code> to tokenize</a> given corpora of sentences into list of words. Which doesn't work with single character strings. </p>

<pre><code>sklearn.feature_extraction.text.TfidfVectorizer(... token_pattern=‚Äô(?u)\b\w\w+\b‚Äô, ...)
</code></pre>

<p>You can use your own regex, give a tokenizer as constructor argument (in that case, given tokenizer overrides the regex). Or use a longer, more realistic test case.</p>
",""
"48898103","2018-02-21 04:18:47","0","","48897910","<p>Something like this:</p>

<pre><code>noun_phrases_list = [[' '.join(leaf[0] for leaf in tree.leaves()) 
                      for tree in cp.parse(sent).subtrees() 
                      if tree.label()=='NP'] 
                      for sent in sentences]
#[['construction', 'need', 'fire inspection', 'fire', 'resistant materials', 
#  'peace', 'mind', 'the one'], 
# ['party sellers', 'Skylite']]
</code></pre>
",""
"48896922","2018-02-21 01:38:08","2","","48896682","<p>Absolutely. </p>

<p>1) You can selectively download corpora like described at <a href=""https://stackoverflow.com/questions/5843817/programmatically-install-nltk-corpora-models-i-e-without-the-gui-downloader"">Programmatically install NLTK corpora / models, i.e. without the GUI downloader?</a> For example, </p>

<p><code>python -m nltk.downloader &lt;your package you would like to download&gt;</code></p>

<p>2) or using the GUI with instructions at <a href=""http://www.nltk.org/data.html"" rel=""nofollow noreferrer"">http://www.nltk.org/data.html</a></p>

<p>Which basically amounts to doing the following and command line</p>

<pre><code>python3
import nltk
nltk.download()
</code></pre>
",""
"48872610","2018-02-19 19:02:58","15","","48872564","<p>No big deal, just not calling the correct method.</p>

<p>Try: <code>nltk.metrics.scores.precision(reference, test)</code></p>

<p><a href=""http://www.nltk.org/api/nltk.metrics.html"" rel=""nofollow noreferrer"">http://www.nltk.org/api/nltk.metrics.html</a></p>

<p>ctrl+f for ""precision"" will get you to documentation</p>

<p>Corrected code:
<code>
print('pos precision:', nltk.metrics.scores.precision(refsets['pos'],testsets['pos']))
print('pos recall:', nltk.metrics.scores.recall(refsets['pos'],testsets['pos']))
</code></p>
",""
"48872512","2018-02-19 18:55:23","3","","48872023","<p>
I'm guessing you're simply trying to get an output of parts of speech that are not ""adverbs""?</p>

<p>Using parentheses results in passing the print function a <a href=""https://www.python-course.eu/python3_list_comprehension.php"" rel=""nofollow noreferrer"">generator comprehension</a>. Try something like this if you just want the output all at once (generator in list comprehension):  </p>

<pre class=""lang-python prettyprint-override""><code>print([s for s in abc if s[1] != 'ADV'])
</code></pre>

<p>Note: You can also achieve the same output without using print().</p>

<p>Also, fyi: <a href=""https://pythonprogramming.net/natural-language-toolkit-nltk-part-speech-tagging/"" rel=""nofollow noreferrer"">Last I checked</a> ""ADV"" doesn't correspond to a pos tag. If you're looking to eliminate adverbs, then I think the correct pos tag adverb types are ""RB"", ""RBR"", and ""RBS"".</p>

<p>Updated the answer, based on Alexis's response below. He is correct, the explanation wasn't complete. Pasting his feedback from comments: </p>

<blockquote>
  <p>There's generators, and there's list comprehensions. print(s for s
  ...) passes print a generator; the version with square brackets uses
  the generator in a list comprehension, to make a list.  </p>
</blockquote>

<p>(please also upvote alexis's comment)</p>
",""
"48794891","2018-02-14 19:35:41","0","","48789043","<p>PyStemmer does not say that it works with python 3.6 in its documentation but it actually does. Install the proper Visual Studio C++ Build compatible with python 3.6 which you can find here:
<a href=""http://landinghub.visualstudio.com/visual-cpp-build-tools"" rel=""nofollow noreferrer"">http://landinghub.visualstudio.com/visual-cpp-build-tools</a></p>

<p>And then try <code>pip install pystemmer</code></p>

<p>If that doesn't work then make sure you install manually exactly as it says here: <a href=""https://github.com/snowballstem/pystemmer"" rel=""nofollow noreferrer"">https://github.com/snowballstem/pystemmer</a></p>
",""
"48713323","2018-02-09 20:05:16","1","","48657359","<p>One of the things you could try is to cut varies parts of the unresolvable input sentence off and deduct a certain confidence score, then try to match it with existing grammar. if the confidence level drop below a certain threshold you would simply return a message about not being able to parse and drop this sentence. While in the same time you could log these sentences and come up with new grammars.</p>

<p>This is what I did for one of the personal assistant projects, trying to give an answer to all unseen or even invalid input is virtually impossible.</p>
",""
"48679657","2018-02-08 07:15:25","0","","48671265","<h1>TL;DR</h1>

<pre><code>from nltk import pos_tag, word_tokenize
from nltk.stem import WordNetLemmatizer

wnl = WordNetLemmatizer()

def penn2morphy(penntag):
    """""" Converts Penn Treebank tags to WordNet. """"""
    morphy_tag = {'NN':'n', 'JJ':'a',
                  'VB':'v', 'RB':'r'}
    try:
        return morphy_tag[penntag[:2]]
    except:
        return 'n' 

def lemmatize_sent(text): 
    # Text input is string, returns lowercased strings.
    return [wnl.lemmatize(word.lower(), pos=penn2morphy(tag)) 
            for word, tag in pos_tag(word_tokenize(text))]
</code></pre>

<p>To lemmatize a dataframe column of string.</p>

<pre><code>df['lemmas'] = df['text'].apply(lemmatize_sent)
</code></pre>

<h1>In Long</h1>

<p>Read <a href=""https://www.kaggle.com/alvations/basic-nlp-with-nltk"" rel=""nofollow noreferrer"">https://www.kaggle.com/alvations/basic-nlp-with-nltk</a></p>
",""
"48635902","2018-02-06 05:14:07","0","","48620621","<p>Yes, use the <code>nltk.grammar.FeatureGrammar.fromstring()</code> function, e.g.</p>

<pre><code>from nltk import grammar, parse
from nltk.parse.generate import generate

# If person is always 3rd, we can skip the PERSON feature.
g = """"""
S[SEM=(?np + WHERE + ?vp)] -&gt; NP[SEM=?np] VP[SEM=?vp]
VP[SEM=(?v + ?pp)] -&gt; IV[SEM=?v] PP[SEM=?pp]
VP[SEM=(?v + ?ap)] -&gt; IV[SEM=?v] AP[SEM=?ap]
NP[SEM=(?det + ?n)] -&gt; Det[SEM=?det] N[SEM=?n]
PP[SEM=(?p + ?np)] -&gt; P[SEM=?p] NP[SEM=?np]
AP[SEM=?pp] -&gt; A[SEM=?a] PP[SEM=?pp]
NP[SEM='Country=""greece""'] -&gt; 'Greece'
NP[SEM='Country=""china""'] -&gt; 'China'
Det[SEM='SELECT'] -&gt; 'Which' | 'What'
N[SEM='City FROM city_table'] -&gt; 'cities'
IV[SEM=''] -&gt; 'are'
A[SEM=''] -&gt; 'located'
P[SEM=''] -&gt; 'in'
""""""

grammar =  grammar.FeatureGrammar.fromstring(g)

for sent in generate(grammar, n=30):
    print(sent)
</code></pre>

<p>[out]:</p>

<pre><code>['Which', 'cities', 'are', 'in', 'Which', 'cities']
['Which', 'cities', 'are', 'in', 'What', 'cities']
['Which', 'cities', 'are', 'in', 'Greece']
['Which', 'cities', 'are', 'in', 'China']
['Which', 'cities', 'are', 'located', 'in', 'Which', 'cities']
['Which', 'cities', 'are', 'located', 'in', 'What', 'cities']
['Which', 'cities', 'are', 'located', 'in', 'Greece']
['Which', 'cities', 'are', 'located', 'in', 'China']
['What', 'cities', 'are', 'in', 'Which', 'cities']
['What', 'cities', 'are', 'in', 'What', 'cities']
['What', 'cities', 'are', 'in', 'Greece']
['What', 'cities', 'are', 'in', 'China']
['What', 'cities', 'are', 'located', 'in', 'Which', 'cities']
['What', 'cities', 'are', 'located', 'in', 'What', 'cities']
['What', 'cities', 'are', 'located', 'in', 'Greece']
['What', 'cities', 'are', 'located', 'in', 'China']
['Greece', 'are', 'in', 'Which', 'cities']
['Greece', 'are', 'in', 'What', 'cities']
['Greece', 'are', 'in', 'Greece']
['Greece', 'are', 'in', 'China']
['Greece', 'are', 'located', 'in', 'Which', 'cities']
['Greece', 'are', 'located', 'in', 'What', 'cities']
['Greece', 'are', 'located', 'in', 'Greece']
['Greece', 'are', 'located', 'in', 'China']
['China', 'are', 'in', 'Which', 'cities']
['China', 'are', 'in', 'What', 'cities']
['China', 'are', 'in', 'Greece']
['China', 'are', 'in', 'China']
['China', 'are', 'located', 'in', 'Which', 'cities']
['China', 'are', 'located', 'in', 'What', 'cities']
</code></pre>
",""
"48618732","2018-02-05 08:53:32","0","","48611488","<p>I recently read this paper: <a href=""https://arxiv.org/abs/1708.05801"" rel=""nofollow noreferrer"">Semantic Relatedness of Words and Phrases</a></p>

<p>Table 1 on p.3. shows how they used a weighting scheme. They then use the total weighted connections to decide if two words are related.</p>

<p>As far as I am aware, there is no ready-made function in nltk to do this.</p>
",""
"48574480","2018-02-02 01:39:20","0","","48572405","<p>See answers below:</p>

<p>1) Dependency Parsing</p>

<p>2) Yes, several - start with information docs for the Spacy library</p>

<p>3) You would need more than several sentences, easier to use an established library</p>
",""
"48532948","2018-01-31 01:09:24","1","","48532723","<p>I guess the problem lies in <code>reticulate</code> not being able to read customized Python objects, which is common, so you have to pass Python objects as close as native Python types between R and Python interfaces. </p>

<p>There's a way to change the output format of <code>ne_chunks</code> to string (<a href=""https://stackoverflow.com/questions/34395127/stanford-nlp-parse-tree-format"">bracketed parse format</a>), using <code>Tree.pformat()</code>:</p>

<pre><code>&gt;&gt;&gt; from nltk import word_tokenize, pos_tag, ne_chunk
&gt;&gt;&gt; sent = ""Christopher is having a difficult time parsing NLTK Trees in R.""
&gt;&gt;&gt; ne_chunk(pos_tag(word_tokenize(sent)))
Tree('S', [Tree('GPE', [('Christopher', 'NNP')]), ('is', 'VBZ'), ('having', 'VBG'), ('a', 'DT'), ('difficult', 'JJ'), ('time', 'NN'), ('parsing', 'VBG'), Tree('ORGANIZATION', [('NLTK', 'NNP'), ('Trees', 'NNP')]), ('in', 'IN'), Tree('GPE', [('R', 'NNP')]), ('.', '.')])
&gt;&gt;&gt; ne_chunk(pos_tag(word_tokenize(sent))).pformat()
'(S\n  (GPE Christopher/NNP)\n  is/VBZ\n  having/VBG\n  a/DT\n  difficult/JJ\n  time/NN\n  parsing/VBG\n  (ORGANIZATION NLTK/NNP Trees/NNP)\n  in/IN\n  (GPE R/NNP)\n  ./.)'
</code></pre>

<p>And to read it back in, use <code>Tree.fromstring()</code>:</p>

<pre><code>&gt;&gt;&gt; tree_str = ne_chunk(pos_tag(word_tokenize(sent))).pformat()
&gt;&gt;&gt; from nltk import Tree
&gt;&gt;&gt; Tree.fromstring(tree_str)
Tree('S', [Tree('GPE', ['Christopher/NNP']), 'is/VBZ', 'having/VBG', 'a/DT', 'difficult/JJ', 'time/NN', 'parsing/VBG', Tree('ORGANIZATION', ['NLTK/NNP', 'Trees/NNP']), 'in/IN', Tree('GPE', ['R/NNP']), './.'])
</code></pre>

<p>So I would guess doing this in R might work:</p>

<pre><code>text &lt;- ""Christopher is having a difficult time parsing NLTK Trees in R.""
ne_tagged_tokens &lt;- ne_chunk(pos_tag(word_tokenize(tagged_tokens)))$pformat()
print(ne_tagged_tokens)
</code></pre>

<p>But reading the strings back into R objects shouldn't be possible since it couldn't handle non-native Python Tree objects so the <code>some_func &lt;- function(...{nltk$some_func(...)})</code> won't work with <code>Tree</code> since it's not a function. </p>

<hr>

<p>If you want to manipulate the output of <code>ne_chunk</code> Tree objects into a list of named entities, then you would have to do something like this in Python: <a href=""https://stackoverflow.com/questions/31836058/nltk-named-entity-recognition-to-a-python-list"">NLTK Named Entity recognition to a Python list</a> </p>

<p>Then again, if you are requiring so many functions in Python that you don't really want to recode or use other R libraries, why aren't you writing in Python instead of sticking to R. </p>
",""
"48529556","2018-01-30 19:58:54","0","","48529212","<p>@haifzhan's solution will get you there for use cases of one word per tag.  If you however need more than one word per tag, here's another solution:</p>

<pre><code>sentence = ""word1 word2 word3 word2 word1 word4 word5 word1""
tags = {'tag1': ['word1'], 'tag2': ['word4', 'word2'], 'tag3': ['word3']} # Set a dictionary of lists based on tags

final_sentence = ' '.join([word for word in sentence.split() if word not in tags.get('tag2')])

# Output:
final_sentence
'word1 word3 word1 word5 word1'
</code></pre>

<p>If your words are not delimited by space though you'll need to approach this a different way, maybe like this:</p>

<pre><code>for word in tags.get('tag2'):
    sentence = sentence.replace(word,'')
</code></pre>
",""
"48459406","2018-01-26 10:18:32","3","","48431173","<p>You can just use TfidfVectorizer with use_idf=True (default value) and then extract with idf_.</p>
<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer

my_data = [&quot;hello how are you&quot;, &quot;hello who are you&quot;, &quot;i am not you&quot;]

tf = TfidfVectorizer(use_idf=True)
tf.fit_transform(my_data)

idf = tf.idf_ 
</code></pre>
<p>[BONUS] if you want to get the idf value for a particular word:</p>
<pre><code># If you want to get the idf value for a particular word, here &quot;hello&quot;    
tf.idf_[tf.vocabulary_[&quot;hello&quot;]]
</code></pre>
",""
"48341103","2018-01-19 12:26:37","4","","48340974","<p>The error message and traceback points you to the source of the problem:</p>

<blockquote>
  <p>in preprocessing(text) 12 #lemmatization 13 lmtzr=WordNetLemmatizer()
  ---> 14 tokens=[lmtzr.lemmatize(word for word in tokens)] 15 preprocessed_text=' '.join(tokens) 16 return preprocessed_text</p>
  
  <p>~\Anaconda3\lib\site-packages\nltk\stem\wordnet.py in lemmatize(self,
  word, pos) 38 39 def lemmatize(self, word, pos=NOUN):</p>
</blockquote>

<p>Obviously, from the function's signature (<code>word</code>, not <code>words</code>) and the error (""has no attribute 'endswith'"" - <code>endswith()</code> is actually a <code>str</code> method), <code>lemmatize()</code> expects a single word, but here:</p>

<pre><code>tokens=[lmtzr.lemmatize(word for word in tokens)]
</code></pre>

<p>you are passing a generator expression.</p>

<p>What you want is:</p>

<pre><code>tokens = [lmtzr.lemmatize(word) for word in tokens]
</code></pre>

<p>NB : you mentions:</p>

<blockquote>
  <p>I believe the error is coming from the source code for
  nltk.corpus.reader.wordnet</p>
</blockquote>

<p>The error is indeed <em>raised</em> in this package, but it ""is coming from"" (in the sense of ""caused by"") your code passing the wrong argument ;)</p>

<p>Hope this will help you debug this kind of problems by yourself next time. </p>
",""
"48217191","2018-01-11 23:00:56","1","","48186483","<p>First of all, Python and especially multi-line strings are indent dependant. Make sure you have no preceding spaces inside the string (as they will be treated as characters) and make sure the patterns (brackets) align visually.</p>

<p>Moreover I think you might want to have <code>&lt;ADJ&gt;&lt;NN&gt;+</code> as your second pattern. <code>+</code> means 1 or more, whereas <code>*</code> means 0 or more.</p>

<p>I hope this solves the issue.</p>

<pre><code>#!/usr/bin/env python
import nltk

PATTERN = r""""""
NP: {&lt;NN&gt;&lt;NN&gt;+}
    {&lt;ADJ&gt;&lt;NN&gt;+}
""""""

sentence = [('the', 'DT'), ('little', 'ADJ'), ('yellow', 'ADJ'),
            ('shepherd', 'NN'), ('dog', 'NN'), ('barked', 'VBD'), ('at', 'IN'),
            ('the', 'DT'), ('silly', 'ADJ'), ('cat', 'NN')]

cp = nltk.RegexpParser(PATTERN)
print(cp.parse(sentence))
</code></pre>

<p>Result:</p>

<pre><code>(S
  the/DT
  little/ADJ
  yellow/ADJ
  (NP shepherd/NN dog/NN)
  barked/VBD
  at/IN
  the/DT
  (NP silly/ADJ cat/NN))
</code></pre>

<p>Reference: <a href=""http://www.nltk.org/book/ch07.html"" rel=""nofollow noreferrer"">http://www.nltk.org/book/ch07.html</a></p>
",""
"48187129","2018-01-10 12:06:10","2","","48169545","<p>You can run Spacy's processing pipeline against already tokenised text. You need to understand, though, that the underlying statistical models have been trained on a reference corpus that has been tokenised using some strategy and if your tokenisation strategy is significantly different, you may expect some performance degradation.</p>

<p>Here's how to go about it using Spacy 2.0.5 and Python 3. If using Python 2, you may need to use unicode literals.</p>

<pre><code>import spacy; nlp = spacy.load('en_core_web_sm')
# spaces is a list of boolean values indicating if subsequent tokens
# are followed by any whitespace
# so, create a Spacy document with your tokenisation
doc = spacy.tokens.doc.Doc(
    nlp.vocab, words=['nuts', 'itch'], spaces=[True, False])
# run the standard pipeline against it
for name, proc in nlp.pipeline:
    doc = proc(doc)
</code></pre>
",""
"48109959","2018-01-05 08:39:25","1","","48109690","<p>One ""hack"" is to convert the feature values into a ""one-hot"" vector of booleans, e.g.</p>

<p>Instead of this:</p>

<pre><code>NP[TYPE=[name,organisation,location]]
</code></pre>

<p>you can do this:</p>

<pre><code>NP[NAME='1',ORG='1',LOC='1']
</code></pre>

<p>Another e.g., instead of this:</p>

<pre><code>NP[TYPE=[name,organisation]
</code></pre>

<p>you can do this with underspecification:</p>

<pre><code>NP[NAME='1',ORG='1']
</code></pre>

<p>or if you like to overspecify:</p>

<pre><code>NP[NAME='1',ORG='1', LOC='0']
</code></pre>

<h1>EDITED</h1>

<p>You can also use the +/- syntax in the feature structure, e.g.</p>

<pre><code>NP[+NAME, -ORG, +LOC]
</code></pre>

<p>For more details, see <a href=""http://www.nltk.org/book/ch09.html#code-slashcfg"" rel=""nofollow noreferrer"">http://www.nltk.org/book/ch09.html#code-slashcfg</a></p>
",""
"48055937","2018-01-02 05:23:35","2","","48054677","<p>Lets solve the easy part of the question first.</p>

<h1>Q2. There is no space between the two words in the sentence.</h1>

<p>You're close when it comes to the printing =)</p>

<p>The problem lies in how you're using the <a href=""https://docs.python.org/3.6/library/stdtypes.html#str.join"" rel=""nofollow noreferrer""><code>str.join</code></a> function. </p>

<pre><code>&gt;&gt;&gt; list_of_str = ['a', 'b', 'c']
&gt;&gt;&gt; ''.join(list_of_str)
'abc'
&gt;&gt;&gt; ' '.join(list_of_str)
'a b c'
&gt;&gt;&gt; '|'.join(list_of_str)
'a|b|c'
</code></pre>

<hr>

<h1>Q1. The agreement is not working out, am having sentences that does not respect the agreement</h1>

<h3>First warning sign</h3>

<p>To produce feature structure grammar with agreement, there should be a rule that contains something like <code>D[AGR=?a] N[AGR=?a]</code> on the right-hand-side (RHS), e.g.</p>

<pre><code>NP -&gt; D[AGR=?a] N[AGR=?a] 
</code></pre>

<p>With that missing there's no real agreement rule in the grammar, see <a href=""http://www.nltk.org/howto/featgram.html"" rel=""nofollow noreferrer"">http://www.nltk.org/howto/featgram.html</a></p>

<h3>Now comes the gotcha!</h3>

<p>If we look at the <code>nltk.parse.generate</code> code carefully, it's merely yielding all possible combinations of the terminals and it seems like it's not caring about the feature structures: <a href=""https://github.com/nltk/nltk/blob/develop/nltk/parse/generate.py"" rel=""nofollow noreferrer"">https://github.com/nltk/nltk/blob/develop/nltk/parse/generate.py</a> </p>

<p>(I think that's a bug not a feature so raising an issue to the NLTK repository would be good)</p>

<p>So if we do this, it'll print all combinations of possible terminals (without caring for the agreement):</p>

<pre><code>from nltk import grammar, parse
from nltk.parse.generate import generate

# If person is always 3rd, we can skip the PERSON feature.
g = """"""
DP -&gt; D[AGR=?a] N[AGR=?a] 
N[AGR=[NUM='sg', GND='m']] -&gt; 'garcon'
N[AGR=[NUM='sg', GND='f']] -&gt; 'fille'
D[AGR=[NUM='sg', GND='m']] -&gt; 'un'
D[AGR=[NUM='sg', GND='f']] -&gt; 'une'

""""""

grammar =  grammar.FeatureGrammar.fromstring(g)

print(list(generate(grammar, n=30)))
</code></pre>

<p>[out]:</p>

<pre><code>[['un', 'garcon'], ['un', 'fille'], ['une', 'garcon'], ['une', 'fille']]
</code></pre>

<h3>But if we try to parse valid and invalid sentences, the agreement rule kicks in:</h3>

<pre><code>from nltk import grammar, parse
from nltk.parse.generate import generate

g = """"""
DP -&gt; D[AGR=?a] N[AGR=?a] 
N[AGR=[NUM='sg', GND='m']] -&gt; 'garcon'
N[AGR=[NUM='sg', GND='f']] -&gt; 'fille'
D[AGR=[NUM='sg', GND='m']] -&gt; 'un'
D[AGR=[NUM='sg', GND='f']] -&gt; 'une'

""""""

grammar =  grammar.FeatureGrammar.fromstring(g)

parser = parse.FeatureEarleyChartParser(grammar)

trees = parser.parse('une garcon'.split()) # Invalid sentence.
print (""Parses for 'une garcon':"", list(trees)) 

trees = parser.parse('un garcon'.split()) # Valid sentence.
print (""Parses for 'un garcon':"", list(trees)) 
</code></pre>

<p>[out]:</p>

<pre><code>Parses for 'une garcon': []
Parses for 'un garcon': [Tree(DP[], [Tree(D[AGR=[GND='m', NUM='sg']], ['un']), Tree(N[AGR=[GND='m', NUM='sg']], ['garcon'])])]
</code></pre>

<h3>To achieve the agreement rule at generation, one possible solution would be to parse each generated production and keep the parse-able ones, e.g.</h3>

<pre><code>from nltk import grammar, parse
from nltk.parse.generate import generate

g = """"""
DP -&gt; D[AGR=?a] N[AGR=?a] 
N[AGR=[NUM='sg', GND='m']] -&gt; 'garcon'
N[AGR=[NUM='sg', GND='f']] -&gt; 'fille'
D[AGR=[NUM='sg', GND='m']] -&gt; 'un'
D[AGR=[NUM='sg', GND='f']] -&gt; 'une'

""""""

grammar =  grammar.FeatureGrammar.fromstring(g)
parser = parse.FeatureEarleyChartParser(grammar)

for tokens in list(generate(grammar, n=30)):
    parsed_tokens = parser.parse(tokens)
    try: 
        first_parse = next(parsed_tokens) # Check if there's a valid parse.
        print(' '.join(first_parse.leaves()))
    except StopIteration:
        continue
</code></pre>

<p>[out]:</p>

<pre><code>un garcon
une fille
</code></pre>

<hr>

<h3>I guess goal is to produce the last 2nd column of:</h3>

<p><a href=""https://i.sstatic.net/4t1SJ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/4t1SJ.png"" alt=""enter image description here""></a></p>

<h3>Without the prepositions:</h3>

<pre><code>from nltk import grammar, parse
from nltk.parse.generate import generate

g = """"""
DP -&gt; D[AGR=?a] N[AGR=?a] 

N[AGR=[NUM='sg', GND='m']] -&gt; 'garcon'
N[AGR=[NUM='sg', GND='f']] -&gt; 'fille'

N[AGR=[NUM='pl', GND='m']] -&gt; 'garcons'
N[AGR=[NUM='pl', GND='f']] -&gt; 'filles'

D[AGR=[NUM='sg', GND='m']] -&gt; 'un'
D[AGR=[NUM='sg', GND='f']] -&gt; 'une'

D[AGR=[NUM='sg', GND='m']] -&gt; 'le'
D[AGR=[NUM='sg', GND='f']] -&gt; 'la'

D[AGR=[NUM='pl', GND='m']] -&gt; 'les'
D[AGR=[NUM='pl', GND='f']] -&gt; 'les'


""""""

grammar =  grammar.FeatureGrammar.fromstring(g)
parser = parse.FeatureEarleyChartParser(grammar)

valid_productions = set()

for tokens in list(generate(grammar, n=30)):
    parsed_tokens = parser.parse(tokens)
    try: 
        first_parse = next(parsed_tokens) # Check if there's a valid parse.
        valid_productions.add(' '.join(first_parse.leaves()))
    except StopIteration:
        continue

for np in sorted(valid_productions):
    print(np)
</code></pre>

<p>[out]:</p>

<pre><code>la fille
le garcon
les filles
les garcons
un garcon
une fille
</code></pre>

<h3>Now to include the preposition</h3>

<p>The TOP (aka START) of the grammar has to have more than one branch, currently the <code>DP -&gt; D[AGR=?a] N[AGR=?a]</code> rule is at the TOP, to allow for a <code>PP</code> construction, we've to something like <code>PHRASE -&gt; DP | PP</code> and make the <code>PHRASE</code> non-terminal the new TOP, e.g.</p>

<pre><code>from nltk import grammar, parse
from nltk.parse.generate import generate

g = """"""

PHRASE -&gt; DP | PP 

DP -&gt; D[AGR=?a] N[AGR=?a] 
PP -&gt; P[AGR=?a] N[AGR=?a] 

P[AGR=[NUM='sg', GND='m']] -&gt; 'du' | 'au'

N[AGR=[NUM='sg', GND='m']] -&gt; 'garcon'
N[AGR=[NUM='sg', GND='f']] -&gt; 'fille'

N[AGR=[NUM='pl', GND='m']] -&gt; 'garcons'
N[AGR=[NUM='pl', GND='f']] -&gt; 'filles'

D[AGR=[NUM='sg', GND='m']] -&gt; 'un'
D[AGR=[NUM='sg', GND='f']] -&gt; 'une'

D[AGR=[NUM='sg', GND='m']] -&gt; 'le'
D[AGR=[NUM='sg', GND='f']] -&gt; 'la'

D[AGR=[NUM='pl', GND='m']] -&gt; 'les'
D[AGR=[NUM='pl', GND='f']] -&gt; 'les'

""""""

french_grammar =  grammar.FeatureGrammar.fromstring(g)
parser = parse.FeatureEarleyChartParser(french_grammar)

valid_productions = set()

for tokens in list(generate(french_grammar, n=100)):
    parsed_tokens = parser.parse(tokens)
    try: 
        first_parse = next(parsed_tokens) # Check if there's a valid parse.
        valid_productions.add(' '.join(first_parse.leaves()))
    except StopIteration:
        continue

for np in sorted(valid_productions):
    print(np)
</code></pre>

<p>[out]:</p>

<pre><code>au garcon
du garcon
la fille
le garcon
les filles
les garcons
un garcon
une fille
</code></pre>

<hr>

<h3>To get everything in the table:</h3>

<pre><code>from nltk import grammar, parse
from nltk.parse.generate import generate

g = """"""

PHRASE -&gt; DP | PP 

DP -&gt; D[AGR=?a] N[AGR=?a] 
PP -&gt; P[AGR=[GND='m', NUM='sg']] N[AGR=[GND='m', NUM='sg']]
PP -&gt; P[AGR=[GND='f', NUM='sg']] D[AGR=[GND='f', NUM='sg', DEF='d']] N[AGR=[GND='f', NUM='sg']]
PP -&gt; P[AGR=[GND=?a, NUM='pl']] N[AGR=[GND=?a, NUM='pl']]


P[AGR=[NUM='sg', GND='m']] -&gt; 'du' | 'au'
P[AGR=[NUM='sg', GND='f']] -&gt; 'de' | '√†'
P[AGR=[NUM='pl']] -&gt; 'des' | 'aux'


N[AGR=[NUM='sg', GND='m']] -&gt; 'garcon'
N[AGR=[NUM='sg', GND='f']] -&gt; 'fille'

N[AGR=[NUM='pl', GND='m']] -&gt; 'garcons'
N[AGR=[NUM='pl', GND='f']] -&gt; 'filles'

D[AGR=[NUM='sg', GND='m', DEF='i']] -&gt; 'un'
D[AGR=[NUM='sg', GND='f', DEF='i']] -&gt; 'une'

D[AGR=[NUM='sg', GND='m', DEF='d']] -&gt; 'le'
D[AGR=[NUM='sg', GND='f', DEF='d']] -&gt; 'la'

D[AGR=[NUM='pl', GND='m']] -&gt; 'les'
D[AGR=[NUM='pl', GND='f']] -&gt; 'les'



""""""

french_grammar =  grammar.FeatureGrammar.fromstring(g)
parser = parse.FeatureEarleyChartParser(french_grammar)

valid_productions = set()

for tokens in list(generate(french_grammar, n=100000)):
    parsed_tokens = parser.parse(tokens)
    try: 
        first_parse = next(parsed_tokens) # Check if there's a valid parse.
        valid_productions.add(' '.join(first_parse.leaves()))
    except StopIteration:
        continue

for np in sorted(valid_productions):
    print(np)
</code></pre>

<p>[out]:</p>

<pre><code>au garcon
aux filles
aux garcons
de la fille
des filles
des garcons
du garcon
la fille
le garcon
les filles
les garcons
un garcon
une fille
√† la fille
</code></pre>

<hr>

<h3>Beyond the table</h3>

<p>It's also possible to produce <code>de|a un(e) garcon|fille</code>, i.e.</p>

<ul>
<li>de un garcon</li>
<li>de une fille</li>
<li>a un garcon</li>
<li>a une fille </li>
</ul>

<p>But I'm not sure whether they're valid French phrases, but if they are you can underspecify the feminin singular PP rule and remove the <code>DEF</code> feature:</p>

<pre><code>PP -&gt; P[AGR=[GND='f', NUM='sg']] D[AGR=[GND='f', NUM='sg', DEF='d']] N[AGR=[GND='f', NUM='sg']]
</code></pre>

<p>to:</p>

<pre><code>PP -&gt; P[AGR=[GND='f', NUM='sg']] D[AGR=[GND='f', NUM='sg']] N[AGR=[GND='f', NUM='sg']]
</code></pre>

<p>and then add an additional rule to produce male singular indefinite PP</p>

<pre><code>PP -&gt; P[AGR=[GND='f', NUM='sg']] D[AGR=[GND='m', NUM='sg', DEF='i']] N[AGR=[GND='m', NUM='sg']]
</code></pre>

<h3>TL;DR</h3>

<pre><code>from nltk import grammar, parse
from nltk.parse.generate import generate

g = """"""

PHRASE -&gt; DP | PP 

DP -&gt; D[AGR=?a] N[AGR=?a] 
PP -&gt; P[AGR=[GND='m', NUM='sg']] N[AGR=[GND='m', NUM='sg']]
PP -&gt; P[AGR=[GND='f', NUM='sg']] D[AGR=[GND='f', NUM='sg']] N[AGR=[GND='f', NUM='sg']]
PP -&gt; P[AGR=[GND='f', NUM='sg']] D[AGR=[GND='m', NUM='sg', DEF='i']] N[AGR=[GND='m', NUM='sg']]
PP -&gt; P[AGR=[GND=?a, NUM='pl']] N[AGR=[GND=?a, NUM='pl']]


P[AGR=[NUM='sg', GND='m']] -&gt; 'du' | 'au'
P[AGR=[NUM='sg', GND='f']] -&gt; 'de' | '√†'
P[AGR=[NUM='pl']] -&gt; 'des' | 'aux'


N[AGR=[NUM='sg', GND='m']] -&gt; 'garcon'
N[AGR=[NUM='sg', GND='f']] -&gt; 'fille'

N[AGR=[NUM='pl', GND='m']] -&gt; 'garcons'
N[AGR=[NUM='pl', GND='f']] -&gt; 'filles'

D[AGR=[NUM='sg', GND='m', DEF='i']] -&gt; 'un'
D[AGR=[NUM='sg', GND='f', DEF='i']] -&gt; 'une'

D[AGR=[NUM='sg', GND='m', DEF='d']] -&gt; 'le'
D[AGR=[NUM='sg', GND='f', DEF='d']] -&gt; 'la'

D[AGR=[NUM='pl', GND='m']] -&gt; 'les'
D[AGR=[NUM='pl', GND='f']] -&gt; 'les'



""""""

french_grammar =  grammar.FeatureGrammar.fromstring(g)
parser = parse.FeatureEarleyChartParser(french_grammar)

valid_productions = set()

for tokens in list(generate(french_grammar, n=100000)):
    parsed_tokens = parser.parse(tokens)
    try: 
        first_parse = next(parsed_tokens) # Check if there's a valid parse.
        valid_productions.add(' '.join(first_parse.leaves()))
    except StopIteration:
        continue

for np in sorted(valid_productions):
    print(np)
</code></pre>

<p>[out]:</p>

<pre><code>au garcon
aux filles
aux garcons
de la fille
de un garcon
de une fille
des filles
des garcons
du garcon
la fille
le garcon
les filles
les garcons
un garcon
une fille
√† la fille
√† un garcon
√† une fille
</code></pre>

<p></p>
",""
"48048786","2018-01-01 10:12:35","5","","48048297","<p>The reason to place the embedding matrix on a CPU is that <code>tf.nn.embedding_lookup</code> <a href=""https://github.com/tensorflow/tensorflow/issues/2502"" rel=""nofollow noreferrer"">isn't supported</a> on a GPU yet:</p>

<blockquote>
  <p>So, given the basic word2vec example being bound to CPU (#514), we can
  see that <code>tf.nn.embedding_lookup</code> doesn't work on GPU. Therefore, ops
  that use <code>embedding_lookup</code> internally doesn't support GPU either (for
  example, <code>nce_loss</code>).</p>
</blockquote>

<p>This means that GPU placement of <code>embedding</code> variable will only lead to unnecessary transfer of data from the main memory to the GPU memory and vice versa. Hence, it would be more efficient to explicitly place the variable on a CPU.</p>
",""
"47984304","2017-12-26 22:20:31","0","","47633449","<p>I think it would help answer your question by clarifying some common NLP tasks.</p>

<p><strong>Lemmatization</strong> is the process of finding the canonical word given different inflections of the word. For example, run, runs, ran and running are forms of the same lexeme: run. If you were to lemmatize <em>run</em>, <em>runs</em>, and <em>ran</em> the output would be <em>run</em>. In your example sentence, note how it lemmatizes <em>means</em> to <em>mean</em>.</p>

<p>Given that, it doesn't sound like the task you want to perform is lemmatization. It may help to solidify this idea with a silly counterexample: what are the different inflections of a hypothetical lemma ""pm"": pming, pmed, pms? None of those are actual words.</p>

<p>It sounds like your task may be closer to <strong>Named Entity Recognition</strong> (NER), which you could also do in spaCy. To iterate through the detected entities in a parsed document, you can use the <code>.ents</code> attribute, as follows:</p>

<pre><code>&gt;&gt;&gt; for ent in doc.ents:
...     print(ent, ent.label_)
</code></pre>

<p>With the sentence you've given, spacy (v. 2.0.5) doesn't detect any entities. If you replace ""PM"" with ""P.M."" it will detect that as an entity, but as a GPE.</p>

<p>The best thing to do depends on your task, but if you want your desired classification of the ""PM"" entity, I'd look at <a href=""https://spacy.io/usage/linguistic-features#setting-entities"" rel=""nofollow noreferrer"">setting entity annotations</a>. If you want to pull out every mention of ""PM"" from a big corpus of documents, <a href=""https://spacy.io/usage/processing-pipelines#component-example2"" rel=""nofollow noreferrer"">use the matcher in a pipeline</a>.</p>
",""
"47878904","2017-12-19 01:44:28","0","","47813330","<p>You are already getting the subtrees: A subtree contains everything below its root, so the output you show is correctly retrieved as the ""subtree"" below the top-level <code>S</code>. Your will then output the subtree dominating ""testing before they can be prescribed"", and finally the lower-most <code>S</code>, dominating ""they can be prescribed"".</p>

<p>Incidentally, you can get the <code>S</code> subtrees directly by specifying a <a href=""http://www.nltk.org/api/nltk.html#nltk.tree.Tree.subtrees"" rel=""nofollow noreferrer"">filter</a>:</p>

<pre><code>for sub_tree in tree_parse[0].subtrees(lambda t: t.label() == ""S""):
    print(sub_tree)
</code></pre>
",""
"47878305","2017-12-19 00:14:50","2","","47872303","<p>As I understood your question, you want to create a function that reads the output numpy array and a certain value (threshold) in order to return two things:</p>

<ul>
<li>how many docs are bigger than or equal the given threshold</li>
<li>the names of these docs.</li>
</ul>

<p>So, here I've made the following function which takes three arguments:</p>

<ul>
<li>the output numpy array from <code>cos_similarity()</code> function.</li>
<li>list of document names.</li>
<li>a certain number (threshold).</li>
</ul>

<p>And here it's:</p>

<pre><code>def get_docs(arr, docs_names, threshold):
    output_tuples = []
    for row in range(len(arr)):
        lst = [row+1+idx for idx, num in \
                  enumerate(arr[row, row+1:]) if num &gt;= threshold]
        for item in lst:
            output_tuples.append( (docs_names[row], docs_names[item]) )

    return len(output_tuples), output_tuples
</code></pre>

<p>Let's see it in action:</p>

<pre><code>&gt;&gt;&gt; docs_names = [""doc1"", ""doc2"", ""doc3"", ""doc4""]
&gt;&gt;&gt; arr = cos_similarity(documents)
&gt;&gt;&gt; arr
array([[ 1.        ,  0.1459739 ,  0.03613371,  0.76357693],
   [ 0.1459739 ,  1.        ,  0.11459266,  0.19117117],
   [ 0.03613371,  0.11459266,  1.        ,  0.04732164],
   [ 0.76357693,  0.19117117,  0.04732164,  1.        ]])
&gt;&gt;&gt; threshold = 0.5   
&gt;&gt;&gt; get_docs(arr, docs_names, threshold)
(1, [('doc1', 'doc4')])
&gt;&gt;&gt; get_docs(arr, docs_names, 1)
(0, [])
&gt;&gt;&gt; get_docs(lst, docs_names, 0.13)
(3, [('doc1', 'doc2'), ('doc1', 'doc4'), ('doc2', 'doc4')])
</code></pre>

<p>Let's see how this function works:</p>

<ul>
<li>first, I iterate over every row of the numpy array.</li>
<li>Second, I iterate over every item in the row whose index is bigger than the row's index. So, we are iterating in a traingular shape like so:
<a href=""https://i.sstatic.net/fqjPh.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/fqjPh.png"" alt=""""></a>
and that's because each pair of documents is mentioned twice in the whole array. We can see that the two values <code>arr[0][1]</code> and <code>arr[1][0]</code> are the same. You also should notice that the diagonal items arn't included because we knew for sure that they are 1 as evey document is very similar to itself :).</li>
<li>Finally, we get the items whose values are bigger than or equal the given threshold, and return their indices. These indices are used later to get the documents names.</li>
</ul>
",""
"47793230","2017-12-13 12:37:41","2","","47793039","<p>By default, TfidfVectorizer converts words to lowercase.Use this line:</p>

<pre><code>  tf = TfidfVectorizer(analyzer='word',lowercase=False, ngram_range=(1, 4), min_df=0, stop_words=stop_words)  
</code></pre>

<p>and it should work.  Use this link for ref. <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"" rel=""nofollow noreferrer"">TfidfVectorizer</a></p>
",""
"47788736","2017-12-13 08:41:14","2","","47769818","<p>Your original <code>nlkt()</code> loops through each row 3 times.</p>

<pre><code>def nlkt(val):
    val=repr(val)
    clean_txt = [word for word in val.split() if word.lower() not in stopwords.words('english')]
    nopunc = [char for char in str(clean_txt) if char not in string.punctuation]
    nonum = [char for char in nopunc if not char.isdigit()]
    words_string = ''.join(nonum)
    return words_string
</code></pre>

<p>Also, each time you're calling <code>nlkt()</code>, you're re-initializing these again and again. </p>

<ul>
<li><code>stopwords.words('english')</code></li>
<li><code>string.punctuation</code></li>
</ul>

<p>These should be global.</p>

<pre><code>stoplist = stopwords.words('english') + list(string.punctuation)
</code></pre>

<p>Going through things line by line:</p>

<pre><code>val=repr(val)
</code></pre>

<p>I'm not sure why you need to do this. But you could easy cast a column to a <code>str</code> type. This should be done outside of your preprocessing function. </p>

<p>Hopefully this is self-explanatory:</p>

<pre><code>&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; df = pd.DataFrame([[0, 1, 2], [2, 'xyz', 4], [5, 'abc', 'def']])
&gt;&gt;&gt; df
   0    1    2
0  0    1    2
1  2  xyz    4
2  5  abc  def
&gt;&gt;&gt; df[1]
0      1
1    xyz
2    abc
Name: 1, dtype: object
&gt;&gt;&gt; df[1].astype(str)
0      1
1    xyz
2    abc
Name: 1, dtype: object
&gt;&gt;&gt; list(df[1])
[1, 'xyz', 'abc']
&gt;&gt;&gt; list(df[1].astype(str))
['1', 'xyz', 'abc']
</code></pre>

<p>Now going to the next line:</p>

<pre><code>clean_txt = [word for word in val.split() if word.lower() not in stopwords.words('english')]
</code></pre>

<p>Using <code>str.split()</code> is awkward, you should use a proper tokenizer. Otherwise, your punctuations might be stuck with the preceding word, e.g.</p>

<pre><code>&gt;&gt;&gt; from nltk.corpus import stopwords
&gt;&gt;&gt; from nltk import word_tokenize
&gt;&gt;&gt; import string
&gt;&gt;&gt; stoplist = stopwords.words('english') + list(string.punctuation)
&gt;&gt;&gt; stoplist = set(stoplist)

&gt;&gt;&gt; text = 'This is foo, bar and doh.'

&gt;&gt;&gt; [word for word in text.split() if word.lower() not in stoplist]
['foo,', 'bar', 'doh.']

&gt;&gt;&gt; [word for word in word_tokenize(text) if word.lower() not in stoplist]
['foo', 'bar', 'doh']
</code></pre>

<p>Also checking for <code>.isdigit()</code> should be checked together:</p>

<pre><code>&gt;&gt;&gt; text = 'This is foo, bar, 234, 567 and doh.'
&gt;&gt;&gt; [word for word in word_tokenize(text) if word.lower() not in stoplist and not word.isdigit()]
['foo', 'bar', 'doh']
</code></pre>

<p>Putting it all together your <code>nlkt()</code> should look like this:</p>

<pre><code>def preprocess(text):
    return [word for word in word_tokenize(text) if word.lower() not in stoplist and not word.isdigit()]
</code></pre>

<p>And you can use the <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.apply.html"" rel=""noreferrer""><code>DataFrame.apply</code></a>:</p>

<pre><code>data['Anylize_Text'].apply(preprocess)
</code></pre>
",""
"47781492","2017-12-12 21:03:48","1","","47778403","<p>According to the documentation of scikit-learn, <code>fit()</code> is used in order to</p>
<blockquote>
<p>Learn vocabulary and idf from training set.</p>
</blockquote>
<p>On the other hand, <code>fit_transform()</code> is used in order to</p>
<blockquote>
<p>Learn vocabulary and idf, return term-document matrix.</p>
</blockquote>
<p>while <code>transform()</code></p>
<blockquote>
<p>Transforms documents to document-term matrix.</p>
</blockquote>
<p>On the training set you need to apply both <code>fit()</code> and <code>transform()</code> (or just <code>fit_transform()</code> that essentially joins both operations) however, on the testing set you only need to <code>transform()</code> the testing instances (i.e. the documents).</p>
<p>Remember that training sets are used for learning purposes (learning is achieved through <code>fit()</code>) while testing set is used in order to evaluate whether the trained model can generalise well to new <strong>unseen</strong> data points.</p>
<hr />
<p><strong>For more details you can refer to the article <a href=""https://medium.com/geekculture/fit-vs-transform-vs-fit-transform-in-python-scikit-learn-2623d5a691e3"" rel=""noreferrer"">fit() vs transform() vs fit_transform()</a></strong></p>
",""
"47728457","2017-12-09 12:10:46","0","","47727078","<p>Averaging (possibly weighted) of word embeddings makes sense, though depending on the main algorithm and the training data this sentence representation may not be the best. The intuition is the following: </p>

<ul>
<li>You might want to handle sentences of different length, hence the averaging (better than plain sum).</li>
<li>Some words in a sentence are usually much more valuable than others. TF-IDF is the simplest measure of the word value. Note that the scale of the result doesn't change.</li>
</ul>

<p>See also <a href=""http://aclweb.org/anthology/P/P16/P16-1089.pdf"" rel=""noreferrer"">this paper by Kenter et al</a>. There is a <a href=""http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/"" rel=""noreferrer"">nice post</a> that performs the comparison of these two approaches in different algorithms, and concludes that none is significantly better than the other: some algorithms favor simple averaging, some algorithms perform better with TF-IDF weighting.</p>
",""
"47726883","2017-12-09 08:45:52","0","","47726833","<p>What, so you want to take a stem and map it to a list of possible words in a dictionary that stem back to it?</p>

<p>This is difficult because the stemming process is lossy and because it's not a 1:1 transformation.</p>

<p>That said, in some cases like <code>environ -&gt; {environment, environments, environmental}</code> and <code>educ -&gt; {educate, educational, education, educated, educating}</code> you can get by with a trie structure where you do a prefix lookup. Things get more interesting for stems like <code>happi</code> which has to map back to <code>happy</code></p>

<p>In the general case, you would have to start with a dictionary and then produce an inverted index by stemming each word and mapping the stem back to the source word in the index. Using the inverted index you can then look up matches given a stem.</p>

<p>Hope this helps..</p>
",""
"47684578","2017-12-06 22:30:40","0","","47683077","<p>You can check this sample code from the <a href=""http://www.nltk.org/book/ch08.html#chap-parse"" rel=""nofollow noreferrer"">NLTK book, chapter 8</a>. </p>

<pre><code>import nltk

grammar1 = nltk.CFG.fromstring(""""""
S -&gt; NP VP
VP -&gt; V NP | V NP PP
PP -&gt; P NP
V -&gt; ""saw"" | ""ate"" | ""walked""
NP -&gt; ""John"" | ""Mary"" | ""Bob"" | Det N | Det N PP
Det -&gt; ""a"" | ""an"" | ""the"" | ""my""
N -&gt; ""man"" | ""dog"" | ""cat"" | ""telescope"" | ""park""
P -&gt; ""in"" | ""on"" | ""by"" | ""with""
"""""")

sent = ""Mary saw Bob"".split()
rd_parser = nltk.RecursiveDescentParser(grammar1)
for tree in rd_parser.parse(sent):
    print(tree)
</code></pre>

<p>output:</p>

<p><code>(S (NP Mary) (VP (V saw) (NP Bob)))</code></p>
",""
"47627822","2017-12-04 06:42:14","0","","47624347","<p>The input for <code>train()</code> and <code>evaluate()</code> functions of the <code>PerceptronTagger</code>  requires a list of list of tuples, where each inner list is a list each tuple is a pair of string.</p>

<hr>

<p>Given <code>train.txt</code> and <code>test.txt</code>:</p>

<pre><code>$ cat train.txt 
This foo
is  foo
a   foo
sentence    bar
.   .

That    foo
is  foo
another foo
sentence    bar
in  foo
conll   bar
format  bar
.   .

$ cat test.txt 
What    foo
is  foo
this    foo
sentence    bar
?   ?

How foo
about   foo
that    foo
sentence    bar
?   ?
</code></pre>

<p>Read the files in CoNLL format into list of tuples. </p>

<pre><code># Using https://github.com/alvations/lazyme
&gt;&gt;&gt; from lazyme import per_section
&gt;&gt;&gt; tagged_train_sentences = [[tuple(token.split('\t')) for token in sent] for sent in per_section(open('train.txt'))]

# Or otherwise

&gt;&gt;&gt; def per_section(it, is_delimiter=lambda x: x.isspace()):
...     """"""
...     From http://stackoverflow.com/a/25226944/610569
...     """"""
...     ret = []
...     for line in it:
...         if is_delimiter(line):
...             if ret:
...                 yield ret  # OR  ''.join(ret)
...                 ret = []
...         else:
...             ret.append(line.rstrip())  # OR  ret.append(line)
...     if ret:
...         yield ret
... 
&gt;&gt;&gt; 
&gt;&gt;&gt; tagged_test_sentences = [[tuple(token.split('\t')) for token in sent] for sent in per_section(open('test.txt'))]
&gt;&gt;&gt; tagged_test_sentences
[[('What', 'foo'), ('is', 'foo'), ('this', 'foo'), ('sentence', 'bar'), ('?', '?')], [('How', 'foo'), ('about', 'foo'), ('that', 'foo'), ('sentence', 'bar'), ('?', '?')]]
</code></pre>

<p>Now you can train/evaluate the tagger:</p>

<pre><code>&gt;&gt;&gt; from lazyme import per_section
&gt;&gt;&gt; tagged_train_sentences = [[tuple(token.split('\t')) for token in sent] for sent in per_section(open('train.txt'))]
&gt;&gt;&gt; from nltk.tag.perceptron import PerceptronTagger
&gt;&gt;&gt; pct = PerceptronTagger(load=False)
&gt;&gt;&gt; pct.train(tagged_train_sentences)
&gt;&gt;&gt; pct.tag('Where do I find a foo bar sentence ?'.split())
[('Where', 'foo'), ('do', 'foo'), ('I', '.'), ('find', 'foo'), ('a', 'foo'), ('foo', 'bar'), ('bar', 'foo'), ('sentence', 'bar'), ('?', '.')]
&gt;&gt;&gt; tagged_test_sentences = [[tuple(token.split('\t')) for token in sent] for sent in per_section(open('test.txt'))]
&gt;&gt;&gt; pct.evaluate(tagged_test_sentences)
0.8
</code></pre>
",""
"47605779","2017-12-02 07:50:48","4","","47605377","<h1>Dependencies:</h1>
<pre><code>pip install autocorrect
</code></pre>
<h1>Code (search.py) :</h1>
<pre><code>from autocorrect import spell
def lcs(X, Y):
    mat = []
    for i in range(0,len(X)):
        row = []
        for j in range(0,len(Y)):
            if X[i] == Y[j]:
                if i == 0 or j == 0:
                    row.append(1)
                else:
                    val = 1 + int( mat[i-1][j-1] )
                    row.append(val)
            else:
                row.append(0)
        mat.append(row)
    new_mat = []
    for r in  mat:
        r.sort()
        r.reverse()
        new_mat.append(r)
    lcs = 0
    for r in new_mat:
        if lcs &lt; r[0]:
            lcs = r[0]
    return lcs
def spellCorrect(string):
    words = string.split(&quot; &quot;)
    correctWords = []
    for i in words:
        correctWords.append(spell(i))
    return &quot; &quot;.join(correctWords)
def semanticSearch(searchString, searchSentencesList):
    result = None
    searchString = spellCorrect(searchString)
    bestScore = 0
    for i in searchSentencesList:
        score = lcs(searchString, i)
        if score &gt; bestScore:
            bestScore = score
            result = i
    return result


result = semanticSearch(&quot;fox jump over dog&quot;, [&quot;The quick brown fox jumped over the lazy dog&quot;, &quot;This is one more string which contains fox bron&quot;])
print result
</code></pre>
",""
"47574977","2017-11-30 13:45:36","3","","47566940","<p>The problem is that the types in <code>pipes</code> aren't instances of <code>MonadLog</code>.  In <code>testApp</code>, you've declared</p>

<pre><code>(MonadLog (WithSeverity Doc) m) =&gt; m ()
</code></pre>

<p>So we are in an instance of <code>MonadLog</code>.  Comparatively, for <code>readInputMessage</code>, you've declared</p>

<pre><code>(MonadLog (WithSeverity Doc) m) =&gt; Producer Text m ()
</code></pre>

<p>So the type <code>m</code> is an instance of <code>MonadLog</code>, but we're not <em>in</em> type <code>m ()</code>.  We're in type <code>Producer Text m ()</code>.  Using <code>lift</code> then takes us into the <code>m</code> monad, which is what you've observed.</p>

<p>The solution is to make the Pipes types members of <code>MonadLog</code> when it's inner monad is.  If you're willing to put up with orphan instances, you can write something similar to the code below.</p>

<pre><code>instance (MonadLog m) =&gt; MonadLog Proxy a' a b' b m r  where
    askLogger = lift askLogger
    localLogger f = lift  . localLogger x
</code></pre>

<p>This <em>should</em> allow you to use <code>MonadLog</code> in any of the Pipes types as long as the inner monad is also a <code>MonadLog</code>.</p>
",""
"47524257","2017-11-28 04:55:39","2","","47523112","<p>Stop words in spaCy are just a set of strings which set a flag on the lexemes, the context-independent entries in the vocabulary (<a href=""https://github.com/explosion/spaCy/blob/master/spacy/lang/en/stop_words.py"" rel=""noreferrer"">see here</a> for the English stop list). The flag simply checks whether <code>text in STOP_WORDS</code>, which is why ""something"" returns <code>True</code> for <code>is_stop</code>, and ""somethings"" doesn't.</p>

<p>However, what you <em>can</em> do is check if the token's lemma or lowercase form is part of the stop list, which is available via <code>nlp.Defaults.stop_words</code> (i.e. the defaults of the language you're using):</p>

<pre><code>def extended_is_stop(token):
    stop_words = nlp.Defaults.stop_words
    return token.is_stop or token.lower_ in stop_words or token.lemma_ in stop_words
</code></pre>

<p>If you're using spaCy v2.0 and want to solve this even more elegantly, you could also implement your own <code>is_stop</code> function via a <a href=""https://spacy.io/usage/processing-pipelines#custom-components-attributes"" rel=""noreferrer"">custom <code>Token</code> attribute extension</a>. You can choose any name for your attribute and it will become available via <code>token._.</code>, for example <code>token._.is_stop</code>:</p>

<pre><code>from spacy.tokens import Token
from spacy.lang.en.stop_words import STOP_WORDS  # import stop words from language data

stop_words_getter = lambda token: token.is_stop or token.lower_ in STOP_WORDS or token.lemma_ in STOP_WORDS
Token.set_extension('is_stop', getter=stop_words_getter)  # set attribute with getter

nlp = spacy.load('en')
doc = nlp(""something Something somethings"")
assert doc[0]._.is_stop  # this was a stop word before, and still is
assert doc[1]._.is_stop  # this is now also a stop word, because its lowercase form is
assert doc[2]._.is_stop  # this is now also a stop word, because its lemma is
</code></pre>
",""
"47501128","2017-11-26 21:19:24","0","","47239639","<p>In WordNet, you have senses which are very close. For instance, for snow you have this two senses.</p>
<blockquote>
<p>S: (n) snow, snowfall (precipitation falling from clouds in the form of ice crystals)</p>
<p>S: (n) snow (a layer of snowflakes (white crystals of frozen water) covering the ground)</p>
</blockquote>
<p>In other words, while it falls from sky it is the first meaning and when it reaches the ground, it changes sense.</p>
<p>In fine grained, these are two different senses while in coarse grained, it will be considered as the same sense.</p>
",""
"47496945","2017-11-26 14:00:19","3","","47495044","<p>I recommend using modern join syntax, which may well fix the problem:</p>

<pre><code>SELECT
  c1.source, c1.word,  c2.word, c3.word,  c4.word, c4.lemma, c4.pos, c5.word, c6.word, c7.word, c8.word, c9.word, c9.lemma, c9.pos, c10.word, c11.word
FROM orderedflatcorpus AS c1
JOIN orderedflatcorpus AS c2 ON c1.id + 1 = c2.id
JOIN orderedflatcorpus AS c3 ON c1.id + 2 = c3.id 
JOIN orderedflatcorpus AS c4 ON c1.id + 3 = c4.id
JOIN orderedflatcorpus AS c5 ON c1.id + 4 = c5.id
JOIN orderedflatcorpus AS c6 ON c1.id + 5 = c6.id
JOIN orderedflatcorpus AS c7 ON c1.id + 6 = c7.id
JOIN orderedflatcorpus AS c8 ON c1.id + 7 = c8.id
JOIN orderedflatcorpus AS c9 ON c1.id + 8 = c9.id
JOIN orderedflatcorpus AS c10 ON c1.id + 9 = c10.id
JOIN orderedflatcorpus AS c11 ON c1.id + 10 = c11.id
WHERE c4.pos LIKE 'v%'
AND c5.pos = 'appge'
AND c6.lemma = 'way'
AND c7.pos LIKE 'i%'
AND c8.word = 'the'
AND c9.pos LIKE 'n%'
</code></pre>

<p>Notes:</p>

<ul>
<li>redundant <code>LIKE</code>s removed</li>
<li><code>ORDER BY</code> removed, because it's very expensive. CSV (like table rows) don't need ordering to be valid. If you absolutely need ordering, use command line tools to order it after the execution of the query.</li>
</ul>
",""
"47464016","2017-11-23 22:30:55","0","","47458616","<p><code>pos_tag</code> should assigned to a field <code>TEXT(stored= True, analyzer=pos_analyzer)</code> not to the <code>pos_analyser</code> directly. </p>

<p>So in <code>schema</code> you should have:</p>

<pre><code>schema = Schema(id=ID(stored=True, unique=True), stem_text=TEXT(stored= True, analyzer=my_analyzer), post_tag=TEXT(stored= True, analyzer=pos_analyzer))
</code></pre>
",""
"47422332","2017-11-21 20:41:54","0","","47419335","<p>If you're sending a corpus through the command-line interface, e.g.</p>

<pre><code>xzcat corpus.sme.xz | sed 's/$/ ./' | apertium -f html-noent sme-nob &gt; translated.nob.mt
</code></pre>

<p>then you can try simply </p>

<pre><code>xzcat corpus.sme.xz | paste - translated.nob.mt
</code></pre>

<p>afterwards to get the input next to the output. That's assuming you want to split things on newlines. The <code>sed</code> is there to ensure words aren't moved across newlines (rules tend not to move across sentence boundaries).</p>

<p>This will be fast, but it's a bit hacky and there are many edge cases. </p>

<hr>

<p>If you want more control, one way would be to install the <a href=""http://wiki.apertium.org/wiki/APy"" rel=""noreferrer"">JSON API</a> locally and send one request at a time.</p>

<p>If you've got a recent Debian/Ubuntu (or are using one of the <a href=""http://wiki.apertium.org/wiki/Prerequisites_for_Debian"" rel=""noreferrer"">apertium repos</a>), you can get the API with</p>

<pre><code>sudo apt install apertium-apy
sudo systemctl start apertium-apy   # start it right now
sudo systemctl enable apertium-apy  # let it start on next boot
</code></pre>

<p>And then you can translate like this:</p>

<pre><code>$ echo 'Jeg liker ikke ansjos' | curl --data-urlencode 'q@-' 'localhost:2737/translate?langpair=nob|nno'
{""responseDetails"": null, ""responseData"": {""translatedText"": ""Eg likar ikkje ansjos""}, ""responseStatus"": 200}
</code></pre>

<p>(or from Javascript with standard ajax requests, some docs at <a href=""http://wiki.apertium.org/wiki/Apertium-apy/Debian"" rel=""noreferrer"">http://wiki.apertium.org/wiki/Apertium-apy/Debian</a> and <a href=""http://wiki.apertium.org/wiki/Apertium-apy#Usage"" rel=""noreferrer"">http://wiki.apertium.org/wiki/Apertium-apy#Usage</a> )</p>

<p>Note that apertium-apy by default serves the pairs that are in /usr/share/apertium/modes; if you start it manually (instead of through systemctl) you can point it at a different path.</p>

<hr>

<p>If you want to produce the JSON format you had in your example, the easiest way would be to use <code>jq</code> (<code>sudo apt install jq</code>), e.g.</p>

<pre><code>$ orig=""Jeg liker ikke ansjos""
$ echo ""$orig"" \
  | curl -Ss --data-urlencode 'q@-' 'localhost:2737/translate?langpair=nob|nno' \
  | jq ""{phrase: {original:\""$orig\"", translated:.responseData.translatedText }}""
{
  ""phrase"": {
    ""original"": ""Jeg liker ikke ansjos"",
    ""translated"": ""Eg likar ikkje ansjos""
  }
}
</code></pre>

<p>or on a corpus:</p>

<pre><code>xzcat corpus.nob.xz | while read -r orig; do 
  echo ""$orig"" \
    | curl -Ss --data-urlencode 'q@-' 'localhost:2737/translate?langpair=nob|nno' \
    | jq ""{phrase: {original:\""$orig\"", translated:.responseData.translatedText}}"";
done
</code></pre>

<p>(A simple test of 500 lines showed this taking 23.7s wall clock time while the <code>paste</code> version took 5.5s.)</p>
",""
"47400532","2017-11-20 20:31:38","7","","47400302","<p>When you execute the following line:</p>

<pre><code>df['vect_message'] = vectorizer.fit_transform(df['message_encoding'])
</code></pre>

<p>Pandas treats the result of <code>vectorizer.fit_transform()</code> as a <strong>scalar</strong> object.
As a result you will have the same sparse matrix in every row in the <code>vect_message</code> column:</p>

<pre><code>In [74]: df.loc[0, 'vect_message']
Out[74]:
&lt;3x4 sparse matrix of type '&lt;class 'numpy.float64'&gt;'
        with 4 stored elements in Compressed Sparse Row format&gt;

In [75]: df.loc[0, 'vect_message'].A
Out[75]:
array([[ 0.        ,  1.        ,  0.        ,  0.        ],
       [ 0.        ,  0.        ,  0.70710678,  0.70710678],
       [ 1.        ,  0.        ,  0.        ,  0.        ]])

In [76]: df.loc[1, 'vect_message'].A
Out[76]:
array([[ 0.        ,  1.        ,  0.        ,  0.        ],
       [ 0.        ,  0.        ,  0.70710678,  0.70710678],
       [ 1.        ,  0.        ,  0.        ,  0.        ]])

In [77]: df.loc[2, 'vect_message'].A
Out[77]:
array([[ 0.        ,  1.        ,  0.        ,  0.        ],
       [ 0.        ,  0.        ,  0.70710678,  0.70710678],
       [ 1.        ,  0.        ,  0.        ,  0.        ]])
</code></pre>

<p>Basically the same is happening when we do <code>df['new_col'] = 0</code> - we will have a column of <code>zeros</code></p>

<p><strong>Workaround:</strong></p>

<pre><code>X = vectorizer.fit_transform(df['message_encoding'])
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
</code></pre>

<p>PS IMO it doesn't make much sense to save (well to try to save) 2D sparse matrix (result of <code>vectorizer.fit_transform()</code> call in Pandas column (Series) - 1D structure)</p>
",""
"47268265","2017-11-13 15:47:58","5","","47266183","<p>You can check the presence of certain words like <code>storage</code>, <code>computing</code> and <code>service</code> by using <code>grepl()</code>. This way, you can check for the presence of a given word in <code>df</code>:</p>

<pre><code>fams   &lt;- c(""storage"", ""computing"", ""service"")
family &lt;- rep(""emtpy_fam"", length(df))

for(fam in fams){
  family[grepl(fam, Keywords)] &lt;- fam
}
cbind(df, family)
#      Keywords                      family     
# [1,] ""cloud storage""               ""storage""  
# [2,] ""cloud computing""             ""computing""
---
#[13,] ""cloud computing services""    ""service""  
#[14,] ""benefits of cloud computing"" ""computing""
</code></pre>

<p>There are certainly nicer ways of doing this, though</p>

<hr>

<p><strong>Edit:</strong> Nicer way to do it, using the <code>stringr</code> package</p>

<pre><code>library(stringr)
family &lt;- str_extract(df, pattern=""storage|computing|service"")
cbind(df, family)
</code></pre>

<hr>

<p><strong>Edit2:</strong> I see your latest edit, indicating that you are looking for non pre-specified family descriptions. The first method I think of in such a case is <a href=""https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation"" rel=""nofollow noreferrer"">Latent Dirichlet Allocation</a> (LDA - not to be confused with Linear Discriminant Analysis, though). </p>

<p>LDA analyzes a corpus of documents and identifies latent topics as a distribution of words (found like <code>terms(lda.output)</code> below) and identifies which document belongs to which topic (found like <code>topic(lda.output)</code> below):</p>

<pre><code>library(topicmodels)
library(tm)

# Preliminary textmining
corpus &lt;- Corpus(VectorSource(df))
corpus &lt;- tm_map(corpus, removeWords, ""cloud"")
corpus &lt;- tm_map(corpus, removePunctuation)
corpus &lt;- tm_map(corpus, stemDocument)

# Term Frequency matrix
TF &lt;- DocumentTermMatrix(corpus, control = list(weighting = weightTf))

lda.output &lt;- LDA(TF, k=3)
terms(lda.output)
# Topic 1  Topic 2  Topic 3 
# ""servic"" ""comput"" ""storag""

cbind(df, terms(lda.output)[topics(lda.output)])
#            df                                    
#Topic 3 ""cloud storage""               ""storag""
#Topic 2 ""cloud computing""             ""comput""
#Topic 3 ""google cloud storage""        ""storag""
#Topic 1 ""cloud services""              ""servic""
#Topic 3 ""free cloud storage""          ""storag""
#Topic 2 ""what is cloud computing""     ""comput""
#Topic 3 ""best cloud storage""          ""storag""
#Topic 1 ""cloud computing definition""  ""servic""
#Topic 1 ""amazon cloud services""       ""servic""
#Topic 3 ""cloud service providers""     ""storag""
#Topic 2 ""google cloud services""       ""comput""
#Topic 2 ""google cloud computing""      ""comput""
#Topic 1 ""cloud computing services""    ""servic""
#Topic 2 ""benefits of cloud computing"" ""comput""
</code></pre>

<p>Final note: If you wish to get <code>""computing""</code> instead of <code>""comput""</code> etc., you should change the stemming part in the text-mining. You can also leave this out, but then <code>""service""</code> and <code>""services""</code> will not be recognised as the same word. You could, however, manually replace <code>""service""</code> with <code>""services""</code> or vice versa.</p>
",""
"47219540","2017-11-10 09:30:08","0","","46214001","<p>In this case need to move forward comma out of <code>{}</code>: </p>

<pre><code>grammar = r""""""
  CHUNK: &lt;,&gt;{&lt;VBN&gt;&lt;,&gt;}
""""""
</code></pre>

<p>In this case system will do exactly what I need.</p>
",""
"47038874","2017-10-31 15:19:56","1","","47022246","<p>If you want to suppress this warning just use the following code, before importing <code>gensim</code>:</p>

<pre><code>import warnings
warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')
import gensim
</code></pre>
",""
"47030495","2017-10-31 08:02:13","5","","47029595","<p>Word2Vec is an algorithm that generates vectors for words, that tend to be similar for similar words. It does not do sentences on its own.</p>

<p>You have more or less the following options:</p>

<ul>
<li>Create a sentence vector</li>
<li>Compare similarity of word vectors within two sentences</li>
</ul>

<h1>Create a sentence vector</h1>

<p>You could build sentence, paragraph or document vectors. There are different approaches to that. You could for example combine the word2vec of of the individual words. If you just want a solution you could go for gensim's doc2vec: <a href=""https://radimrehurek.com/gensim/models/doc2vec.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/models/doc2vec.html</a></p>

<p>Other than that there are methods like concatenating all the word vectors (of a fixed length).</p>

<p>Similar questions:
<a href=""https://stackoverflow.com/questions/22129943/how-to-calculate-the-sentence-similarity-using-word2vec-model-of-gensim-with-pyt"">How to calculate the sentence similarity using word2vec model of gensim with python</a></p>

<h1>Compare similarity of word vectors within two sentences</h1>

<p>One such approach is Movers Distance: <a href=""https://stackoverflow.com/questions/44380199/pairwise-earth-mover-distance-across-all-documents-word2vec-representations"">Pairwise Earth Mover Distance across all documents (word2vec representations)</a></p>

<p>This seems like a good, but expensive approach.</p>

<p><strong>Update</strong>: You've updated your question since to mention that you are using ""SIF Embeddings"" (instead of word2vec): <a href=""https://openreview.net/forum?id=SyK00v5xx"" rel=""nofollow noreferrer"">https://openreview.net/forum?id=SyK00v5xx</a></p>
",""
"47007462","2017-10-30 02:45:33","2","","46986560","<p>When you did</p>

<pre><code>from nltk.corpus import stopwords
</code></pre>

<p><code>stopwords</code> is the variable that's pointing to the <code>CorpusReader</code> object in <code>nltk</code>.</p>

<p>The actual stopwords (i.e. a list of stopwords) you're looking for is instantiated when you do:</p>

<pre><code>stop_words = set(stopwords.words(""english""))
</code></pre>

<p>So when checking whether a word in your list of tokens is a stopwords, you should do:</p>

<pre><code>from nltk.corpus import stopwords
stop_words = set(stopwords.words(""english""))
for w in tokenized_sent:
    if w not in stop_words:
        pass # Do something.
</code></pre>

<p>To avoid confusion, I usually name the actual list of stopwords as <code>stoplist</code>:</p>

<pre><code>from nltk.corpus import stopwords
stoplist = set(stopwords.words(""english""))
for w in tokenized_sent:
    if w not in stoplist:
        pass # Do something.
</code></pre>
",""
"46792893","2017-10-17 14:43:24","1","","46792667","<p>You can use regular expressions:</p>

<pre><code>import re
s = '{(Entertainment (Adult), S), (Performing Arts, S), (Comedy Club, S), ($, S), (Comedy, P), (18+, S), (Plays &amp; Shows, P)}'
final_data = [re.split("",\s+"", i) for i in re.findall(""\((.*?)\)"", s)]
final_data = [[re.sub(""\(|\)"", '', b) for b in i] for i in final_data]
new_final_data = set(map(tuple, final_data))
</code></pre>

<p>Output:</p>

<pre><code>set([('Entertainment (Adult)', 'S'), ('Performing Arts', 'S'), ('Comedy Club', 'S'), ('$', 'S'), ('Comedy', 'P'), ('18+', 'S')])
</code></pre>
",""
"46786449","2017-10-17 09:07:39","0","","46637044","<p>If my comment is what you are looking for, then below is the answer:</p>

<pre><code>grammar = """"""
            NP: 
                {&lt;JJ&gt;*&lt;NN.?&gt;*&lt;V.|IN&gt;*&lt;NN.?&gt;*}""""""
</code></pre>
",""
"46779568","2017-10-16 21:43:59","1","","46779116","<p>No, your current approach does not work, because you must pass <em>one word at a time</em> to the lemmatizer/stemmer, otherwise, those functions won't know to interpret your string as a sentence (they expect words).</p>

<pre><code>import re

__stop_words = set(nltk.corpus.stopwords.words('english'))

def clean(tweet):
    cleaned_tweet = re.sub(r'([^\w\s]|\d)+', '', tweets.lower())
    return ' '.join([lemmatizer.lemmatize(i, 'v') 
                for i in cleaned_tweet.split() if i not in __stop_words])
</code></pre>

<p>Alternatively, you can use a <code>PorterStemmer</code>, which does the same thing as lemmatisation, but without context.</p>

<pre><code>from nltk.stem.porter import PorterStemmer  
stemmer = PorterStemmer() 
</code></pre>

<p>And, call the stemmer like this:</p>

<pre><code>stemmer.stem(i)
</code></pre>
",""
"46746232","2017-10-14 15:33:15","3","","46745871","<p>The computation seems to be blocked on <code>putStrLn $ show r</code>, i.e. outside the <code>interruptable</code> function. Note that <code>stateFib</code> doesn't force the result, so the <code>async</code> exits almost immediately. The whole work is delayed until <code>putStrLn</code> tries to print the result. Try to force the computation earlier:</p>

<pre><code>stateFib :: (MonadState St m, MonadIO m) =&gt; Integer -&gt; m Integer
stateFib n = do
  let f = slowFib n
  modify $ \st -&gt; st{x=f}
  f `seq` return f
</code></pre>
",""
"46729923","2017-10-13 12:26:00","0","","46713629","<p>This questions is essentially a question about model evaluation metrics. In this case, our model is a POS tagger, specifically the <code>UnigramTagger</code></p>

<h1>Quantifying</h1>

<p>You want to know ""<code>how well</code>"" your tagger is doing. This is a <code>qualitative</code> question, so we have some general <code>quantitative</code> metrics to help define what ""<code>how well</code>"" means. Basically, we have standard metrics to give us this information. They are usually <code>accuracy</code>, <code>precision</code>, <code>recall</code> and <code>f1-score</code>.</p>

<h1>Evaluating</h1>

<p>First off, we would need some data that is marked up with <code>POS tags</code>, then we can test. This is usually referred to as a <code>train/test</code> split, since some of the data we use for training the POS tagger, and some is used for testing or <code>evaluating</code> it's performance.</p>

<p>Since POS tagging is traditionally a <code>supervised learning</code> question, we need some sentences with POS tags to train and test with. </p>

<p>In practice, people label a bunch of sentences then split them to make a <code>test</code> and <code>train</code> set. The <a href=""http://www.nltk.org/book/ch05.html#separating-the-training-and-testing-data"" rel=""noreferrer"">NLTK book</a> explains this well, Let's try it out.</p>

<pre><code>from nltk import UnigramTagger
from nltk.corpus import brown
# we'll use the brown corpus with universal tagset for readability
tagged_sentences = brown.tagged_sents(categories=""news"", tagset=""universal"")

# let's keep 20% of the data for testing, and 80 for training
i = int(len(tagged_sentences)*0.2)
train_sentences = tagged_sentences[i:]
test_sentences = tagged_sentences[:i]

# let's train the tagger with out train sentences
unigram_tagger = UnigramTagger(train_sentences)
# now let's evaluate with out test sentences
# default evaluation metric for nltk taggers is accuracy
accuracy = unigram_tagger.evaluate(test_sentences)

print(""Accuracy:"", accuracy)
Accuracy: 0.8630364649525858
</code></pre>

<p>Now, <code>accuracy</code> is an OK metric for knowing ""<code>how many you got right</code>"", but there are other metrics that give us more detail, such as <code>precision</code>, <code>recall</code> and <code>f1-score</code>. We can use <code>sklearn</code>'s <code>classification_report</code> to give us a good overview of the results. </p>

<pre><code>tagged_test_sentences = unigram_tagger.tag_sents([[token for token,tag in sent] for sent in test_sentences])
gold = [str(tag) for sentence in test_sentences for token,tag in sentence]
pred = [str(tag) for sentence in tagged_test_sentences for token,tag in sentence]
from sklearn import metrics
print(metrics.classification_report(gold, pred))

             precision    recall  f1-score   support

          .       1.00      1.00      1.00      2107
        ADJ       0.89      0.79      0.84      1341
        ADP       0.97      0.92      0.94      2621
        ADV       0.93      0.79      0.86       573
       CONJ       1.00      1.00      1.00       453
        DET       1.00      0.99      1.00      2456
       NOUN       0.96      0.76      0.85      6265
        NUM       0.99      0.85      0.92       379
       None       0.00      0.00      0.00         0
       PRON       1.00      0.96      0.98       502
        PRT       0.69      0.96      0.80       481
       VERB       0.96      0.83      0.89      3274
          X       0.10      0.17      0.12         6

avg / total       0.96      0.86      0.91     20458
</code></pre>

<p>Now we have some ideas and values we can look at to quantify our taggers, but I am sure you are thinking, ""<code>That's all well and good, but how well does it perform on random sentences?</code>""</p>

<p>Simply put, it is what was mentioned in other answers, unless you have your own POS tagged data on sentences we want to test, we will never know for sure!</p>
",""
"46709591","2017-10-12 12:26:53","0","","46612949","<p>Training a model with a huge corpus will surely take a very long time because of a large number of weights involved.  Suppose your word vectors have 300 components and your vocabulary size is 10,000. The size of weight matrix would be 300*10000 = 3 million!</p>

<p>To build a model for huge datasets I would recommend you to first preprocess the dataset. Following preprocessing steps can be applied:</p>

<ul>
<li>Removing stop words.</li>
<li>Treating word pairs or phrases as single words, like new york as new_york, etc.</li>
<li>Subsampling frequent words to decrease the number of training examples.</li>
<li>Modifying the optimization objective with a technique they called ‚ÄúNegative Sampling‚Äù, which causes each training sample to update only a small percentage of the model‚Äôs weights. </li>
</ul>

<p>The above tasks were also done in official word2vec implementation released by Google. Gensim provides very beautiful high-level APIs to perform most of above tasks. Also, have a look at this <a href=""https://rare-technologies.com/word2vec-in-python-part-two-optimizing/"" rel=""nofollow noreferrer"">blog</a> for further optimizing techniques.</p>

<p>One more thing that can be done is instead of training your own model use the already trained <a href=""https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing"" rel=""nofollow noreferrer"">word2vec mode</a>l released by Google It‚Äôs 1.5GB and includes word vectors for a vocabulary of 3 million words and phrases that they trained on roughly 100 billion words from a Google News dataset. </p>
",""
"46688653","2017-10-11 12:51:06","3","","46687065","<p>The behaviour that you're seeing and that you don't want is ""stemming"". If you don't want that, you have to use a different dictionary with to_tsvector. The ""simple"" dictionary doesn't do stemming, so it should fit your use case.</p>

<pre><code>select * 
from to_tsvector('simple', 'This is my favourite game. I enjoy everything about it.');
</code></pre>

<p>results in the following output</p>

<blockquote>
  <p>'about':9 'enjoy':7 'everything':8 'favourite':4 'game':5 'i':6 'is':2 'it':10 'my':3 'this':1</p>
</blockquote>

<p>If you still want to remove stop words, you have to define your own dictionary as far as I can see. See the example below, though you might want to read up on the <a href=""https://www.postgresql.org/docs/current/static/textsearch-dictionaries.html"" rel=""nofollow noreferrer"">documentation</a> to make sure this does exactly what you want.</p>

<pre><code>CREATE TEXT SEARCH DICTIONARY only_stop_words (
    Template = pg_catalog.simple,
    Stopwords = english
);
CREATE TEXT SEARCH CONFIGURATION public.only_stop_words ( COPY = pg_catalog.simple );
ALTER TEXT SEARCH CONFIGURATION public.only_stop_words ALTER MAPPING FOR asciiword WITH only_stop_words;
select * 
from to_tsvector('only_stop_words', 'The This is my favourite game. I enjoy everything about it.');
</code></pre>

<blockquote>
  <p>'enjoy':8 'everything':9 'favourite':5 'game':6</p>
</blockquote>
",""
"46591832","2017-10-05 17:45:53","1","","46580932","<p>Try increasing the <code>ngram_range</code> in <code>TfidfVectorizer</code>:</p>

<pre><code>tfidf = TfidfVectorizer(vocabulary = myvocabulary, stop_words = 'english', ngram_range=(1,2))
</code></pre>

<p><strong>Edit:</strong> The output of <code>TfidfVectorizer</code> is the TF-IDF matrix in sparse format (or actually the transpose of it in the format you seek). You can print out its contents e.g. like this:</p>

<pre><code>feature_names = tfidf.get_feature_names()
corpus_index = [n for n in corpus]
rows, cols = tfs.nonzero()
for row, col in zip(rows, cols):
    print((feature_names[col], corpus_index[row]), tfs[row, col])
</code></pre>

<p>which should yield</p>

<pre><code>('biscuit pudding', 1) 0.646128915046
('chocolates', 1) 0.763228291628
('chocolates', 2) 0.508542320378
('tim tam', 2) 0.861036995944
('chocolates', 3) 0.508542320378
('fresh milk', 3) 0.861036995944
</code></pre>

<p>If the matrix is not large, it might be easier to examine it in dense form. <code>Pandas</code> makes this very convenient:</p>

<pre><code>import pandas as pd
df = pd.DataFrame(tfs.T.todense(), index=feature_names, columns=corpus_index)
print(df)
</code></pre>

<p>This results in</p>

<pre><code>                        1         2         3
tim tam          0.000000  0.861037  0.000000
jam              0.000000  0.000000  0.000000
fresh milk       0.000000  0.000000  0.861037
chocolates       0.763228  0.508542  0.508542
biscuit pudding  0.646129  0.000000  0.000000
</code></pre>
",""
"46520077","2017-10-02 05:45:44","0","","46519084","<p><strong>TL;DR</strong></p>

<p>On the terminal:</p>

<pre><code>python -m nltk.downloader averaged_perceptron_tagger
</code></pre>

<p>or in Python</p>

<pre><code>import nltk
nltk.download('averaged_perceptron_tagger')
</code></pre>

<hr>

<h1>In Long</h1>

<p>Firstly, please update your NLTK version to version 3.2.5, on the command line (Use <code>sudo</code> if necessary):</p>

<pre><code>pip install -U nltk
</code></pre>

<p>Now you can try using the <code>pos_tag</code> function again and you should see a more helpful error message:</p>

<pre><code>&gt;&gt;&gt; from nltk import pos_tag
&gt;&gt;&gt; pos_tag(['foo', 'bar'])
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/usr/local/lib/python2.7/site-packages/nltk/tag/__init__.py"", line 133, in pos_tag
    tagger = _get_tagger(lang)
  File ""/usr/local/lib/python2.7/site-packages/nltk/tag/__init__.py"", line 97, in _get_tagger
    tagger = PerceptronTagger()
  File ""/usr/local/lib/python2.7/site-packages/nltk/tag/perceptron.py"", line 140, in __init__
    AP_MODEL_LOC = 'file:'+str(find('taggers/averaged_perceptron_tagger/'+PICKLE))
  File ""/usr/local/lib/python2.7/site-packages/nltk/data.py"", line 673, in find
    raise LookupError(resource_not_found)
LookupError: 
**********************************************************************
  Resource averaged_perceptron_tagger not found.
  Please use the NLTK Downloader to obtain the resource:

  &gt;&gt;&gt; import nltk
  &gt;&gt;&gt; nltk.download('averaged_perceptron_tagger')

  Searched in:
    - '/Users/alvas/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/local/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/local/lib/nltk_data'

**********************************************************************
</code></pre>

<hr>

<p>Note that the <code>punkt</code> resource is used for <code>word_tokenize()</code> but the <code>pos_tag()</code> function requires the <code>averaged_perceptron_tagger</code> model.</p>

<p>So, on the terminal, do:</p>

<pre><code>python -m nltk.downloader averaged_perceptron_tagger
</code></pre>

<p>or in Python</p>

<pre><code>import nltk
nltk.download('averaged_perceptron_tagger')
</code></pre>
",""
"46454392","2017-09-27 18:00:33","1","","46449047","<p>A similar question has been answered <a href=""https://stackoverflow.com/questions/46122591/a-lemmatizing-function-using-a-hash-dictionary-does-not-work-with-tm-package-in/46124729#46124729"">here</a>, but since that question's title (and accepted answer) do not make the obvious link, I will show you how this applies to your question specifically.  I'll also provide additional detail below to implement your own basic stemmer using wildcards for the suffixes.</p>

<h3>Manually mapping stems to inflected forms</h3>

<p>The simplest way to do this is by using a custom dictionary where the keys are your stems, and the values are the inflected forms.  You can then use <code>tokens_lookup()</code> with the <code>exclusive = FALSE, capkeys = FALSE</code> options to convert the inflected terms into their stems.</p>

<p>Note that I have modified your example a little to simplify it, and to correct what I think were mistakes.</p>

<pre><code>library(""quanteda"")
packageVersion(""quanteda"")
[1] ‚Äò0.99.9‚Äô

# no need for the data.frame() call
myText &lt;- c(""ala ma kotka"", ""kasia ma pieska"")  
toks &lt;- tokens(myText, 
               remove_numbers = TRUE, remove_punct = TRUE,
               remove_symbols = TRUE, remove_hyphens = TRUE)

Origin &lt;- c(""kot"", ""kot"", ""pies"", ""pies"")
Word &lt;- c(""kotek"", ""kotka"", ""piesek"", ""pieska"")
</code></pre>

<p>Then we create the dictionary, as follows.  As of quanteda v0.99.9, values with the same keys are merged, so you could have a list mapping multiple, different inflected forms to the same keys.  Here, I had to add new values since the inflected forms in your original <code>Word</code> vector were not found in the <code>myText</code> example.</p>

<pre><code>temp_list &lt;- as.list(Word) 
names(temp_list) &lt;- Origin
(stem_dict &lt;- dictionary(temp_list))
## Dictionary object with 2 key entries.
## - [kot]:
##   - kotek, kotka
## - [pies]:
##   - piesek, pieska    
</code></pre>

<p>Then <code>tokens_lookup()</code> does its magic.</p>

<pre><code>tokens_lookup(toks, dictionary = stem_dict, exclusive = FALSE, capkeys = FALSE)
## tokens from 2 documents.
## text1 :
## [1] ""ala"" ""ma""  ""kot""
## 
## text2 :
## [1] ""kasia"" ""ma""    ""pies"" 
</code></pre>

<h3>Wildcarding all stems from common roots</h3>

<p>An alternative is to implement your own stemmer using the ""glob"" wildcarding to represent all suffixes for your <code>Origin</code> vector, which (here, at least) produces the same results:</p>

<pre><code>temp_list &lt;- lapply(unique(Origin), paste0, ""*"")
names(temp_list) &lt;- unique(Origin)
(stem_dict2 &lt;- dictionary(temp_list))
# Dictionary object with 2 key entries.
# - [kot]:
#   - kot*
# - [pies]:
#   - pies*

tokens_lookup(toks, dictionary = stem_dict, exclusive = FALSE, capkeys = FALSE)
## tokens from 2 documents.
## text1 :
## [1] ""ala"" ""ma""  ""kot""
## 
## text2 :
## [1] ""kasia"" ""ma""    ""pies"" 
</code></pre>
",""
"46445752","2017-09-27 10:39:45","1","","46444656","<h1>TL;DR</h1>

<p>Yes.</p>

<hr>

<h1>In Long</h1>

<p>BLEU score measures n-grams and its agnostic to languages but its dependent on the fact the language sentences can be split into tokens. So yes, it can compare Chinese/Japanese... </p>

<p>Note the caveats of using BLEU score at sentence level. BLEU was never created with sentence level comparison in mind, here's a nice discussion: <a href=""https://github.com/nltk/nltk/issues/1838"" rel=""noreferrer"">https://github.com/nltk/nltk/issues/1838</a></p>

<p>Most probably, you'll see the warning when you have really short sentences, e.g. </p>

<pre><code>&gt;&gt;&gt; from nltk.translate import bleu
&gt;&gt;&gt; ref = 'Êàë ÊòØ Â•Ω ‰∫∫'.split()
&gt;&gt;&gt; hyp = 'Êàë ÊòØ ÂñÑËâØÁöÑ ‰∫∫'.split()
&gt;&gt;&gt; bleu([ref], hyp)
/usr/local/lib/python2.7/site-packages/nltk/translate/bleu_score.py:490: UserWarning: 
Corpus/Sentence contains 0 counts of 3-gram overlaps.
BLEU scores might be undesirable; use SmoothingFunction().
  warnings.warn(_msg)
0.7071067811865475
</code></pre>

<p>You can use the smoothing functions in <a href=""https://github.com/alvations/nltk/blob/develop/nltk/translate/bleu_score.py#L425"" rel=""noreferrer"">https://github.com/alvations/nltk/blob/develop/nltk/translate/bleu_score.py#L425</a> to overcome short sentences. </p>

<pre><code>&gt;&gt;&gt; from nltk.translate.bleu_score import SmoothingFunction
&gt;&gt;&gt; smoothie = SmoothingFunction().method4
&gt;&gt;&gt; bleu([ref], hyp, smoothing_function=smoothie)
0.2866227639866161
</code></pre>
",""
"46387065","2017-09-24 05:55:14","0","","46368720","<p>Expanding on @whs2k's answer, the square bracket syntax is used to form a transformation wrapper around the corpus, forming a kind of lazy processing pipeline.</p>

<p>I didn't get it until I read the note in this tutorial: <a href=""https://radimrehurek.com/gensim/tut2.html"" rel=""nofollow noreferrer"">https://radimrehurek.com/gensim/tut2.html</a></p>

<blockquote>
  <p>Calling model[corpus] only creates a wrapper around the old corpus
  document stream ‚Äì actual conversions are done on-the-fly, during
  document iteration. We cannot convert the entire corpus at the time of
  calling corpus_transformed = model[corpus], because that would mean
  storing the result in main memory, and that contradicts gensim‚Äôs
  objective of memory-indepedence. If you will be iterating over the
  transformed corpus_transformed multiple times, and the transformation
  is costly, serialize the resulting corpus to disk first and continue
  using that.</p>
</blockquote>

<p>But I still don't feel I fully understand the underlying Python list magic.</p>
",""
"46205866","2017-09-13 19:56:39","0","","46186238","<p>Elaborating on @Daniel Wagner's answer...  The way parsers are normally built with Parsec, you start with low-level parsers that parse specific characters (e.g., a plus sign or a digit), and you build parsers on top of them using combinators (like a <code>many1</code> combinator that turns a parser that reads a single digit into a parser that reads one or more digits, or a monadic parse that parsers ""one or more digits"" followed by a ""plus sign"" followed by ""one or more digits"").</p>

<p>However, each parser, whether it's a low-level digit parser or a higher-level ""addition expression"" parser, is intended to be applied directly to the same input stream.</p>

<p>What you don't <em>typically</em> do is write a parser that gobbles a chunk of the input stream to produce, say, a <code>String</code> and another parser that parses that <em><code>String</code></em> (instead of the original input stream) and try to combine them.  This is the kind of ""vertical composition"" that isn't directly supported by Parsec and looks unnatural and non-monadic.</p>

<p>As pointed out in the comments, there are <em>some</em> situations where vertical composition is the cleanest overall approach (like when you have one language embedded within the components or expressions of another language), but it's not the usual approach taken by a Parsec parser.</p>

<p>The bottom line in your application is that a <code>cell</code> parser that produces only a <code>String</code> is too specialized to be useful.  A more useful Parsec framework for CSV files would be:</p>

<pre><code>import Text.Parsec
import Text.Parsec.String

-- | `csv cell` parses a CSV file each of whose elements is parsed by `cell`
csv :: Parser a -&gt; Parser [[a]]
csv cell = many (row cell)

-- | `row cell` parses a newline-terminated row of comma separated
--   `cell`-expressions
row :: Parser a -&gt; Parser [a]
row cell = sepBy cell (char ',') &lt;* char '\n'
</code></pre>

<p>Now, you can write a custom cell parser that parses positive integers:</p>

<pre><code>customCell :: Parser Int
customCell = read &lt;$&gt; many1 digit
</code></pre>

<p>and parse CSV files:</p>

<pre><code>&gt; parse (csv customCell) """" ""1,2,3\n4,5,6\n""
Right [[1,2,3],[4,5,6]]
&gt;
</code></pre>

<p>Here, instead of having a <code>cell</code> subparser that explicitly parses a comma-delimited cell into a string to be fed to a different parser, the ""cell"" is an implicit context in which a supplied cell parser is called to parse the underlying input stream at the appropriate point where one would expect a comma-delimited cell in the middle of a row in the middle of the input stream.</p>
",""
"46089409","2017-09-07 06:34:46","1","","46084574","<p><strong>TL;DR</strong></p>
<p>Use <a href=""https://github.com/mjpost/sacrebleu"" rel=""nofollow noreferrer"">https://github.com/mjpost/sacrebleu</a> when evaluating Machine Translation systems.</p>
<p><strong>In Short</strong></p>
<p>No, the BLEU in NLTK isn't the exactly the same as the <code>mteval-13a.perl</code>.</p>
<p>But it can get really close, see <a href=""https://github.com/nltk/nltk/issues/1330#issuecomment-256237324"" rel=""nofollow noreferrer"">https://github.com/nltk/nltk/issues/1330#issuecomment-256237324</a></p>
<blockquote>
<p><code>nltk.translate.corpus_bleu</code> corresponds to <code>mteval-13a.pl</code> up to the 4th order of ngram with some floating point discrepancies</p>
</blockquote>
<p>The details of the comparison and the dataset used can be downloaded from <a href=""https://github.com/nltk/nltk_data/blob/gh-pages/packages/models/wmt15_eval.zip"" rel=""nofollow noreferrer"">https://github.com/nltk/nltk_data/blob/gh-pages/packages/models/wmt15_eval.zip</a> or:</p>
<pre><code>import nltk
nltk.download('wmt15_eval')
</code></pre>
<p>The major differences:</p>
<p><a href=""https://i.sstatic.net/mI82Y.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/mI82Y.jpg"" alt=""enter image description here"" /></a></p>
<hr />
<p><strong>In Long</strong></p>
<p>There are several difference between <code>mteval-13a.pl</code> and <code>nltk.translate.corpus_bleu</code>:</p>
<ul>
<li><p>The first difference is the fact that <strong><code>mteval-13a.pl</code> comes with its own NIST tokenizer</strong> while the NLTK version of BLEU is the implementation of the metric and <strong>assumes that input is pre-tokenized</strong>.</p>
<ul>
<li>BTW, this <a href=""https://github.com/nltk/nltk/pull/1779"" rel=""nofollow noreferrer"">ongoing PR</a> will bridge the gap between NLTK and NIST tokenizers</li>
</ul>
</li>
<li><p>The other major difference is that <code>mteval-13a.pl</code> expects the input to be in <code>.sgm</code> format while NLTK BLEU takes in python list of lists of strings, see the <a href=""https://github.com/nltk/nltk_data/blob/gh-pages/packages/models/wmt15_eval.zip"" rel=""nofollow noreferrer"">README.txt in the zipball here for more information of how to convert textfile to SGM</a>.</p>
</li>
<li><p><code>mteval-13a.pl</code> expects an ngram order of at least 1-4. If the minimum ngram order for the sentence/corpus is less than 4, it will return a 0 probability which is a <code>math.log(float('-inf'))</code>. To emulate this behavior, NLTK has a put an <code>_emulate_multibleu</code> flag:</p>
<ul>
<li>See <a href=""https://github.com/nltk/nltk/blob/develop/nltk/translate/bleu_score.py#L477"" rel=""nofollow noreferrer"">https://github.com/nltk/nltk/blob/develop/nltk/translate/bleu_score.py#L477</a></li>
</ul>
</li>
<li><p><code>mteval-13a.pl</code> is able to generate NIST scores while NLTK doesn't have NIST score implementation (at least not yet)</p>
<ul>
<li>NIST score in NLTK is <a href=""https://github.com/nltk/nltk/pull/1779"" rel=""nofollow noreferrer"">upcoming in this PR</a></li>
</ul>
</li>
</ul>
<p>Other than the differences, NLTK BLEU scores packed in more features:</p>
<ul>
<li><p>to handle fringe cases that the original BLEU (Papineni, ‚Äé2002) overlooked</p>
<ul>
<li>See <a href=""https://github.com/nltk/nltk/pull/1383"" rel=""nofollow noreferrer"">https://github.com/nltk/nltk/pull/1383</a></li>
</ul>
</li>
<li><p>Also to handle fringe cases where the largest order of Ngram is &lt; 4, the uniform weights of the individual ngram precision will be reweighted such that the mass of the weights sums to 1.0</p>
<ul>
<li>See <a href=""https://github.com/nltk/nltk/blob/develop/nltk/translate/bleu_score.py#L175"" rel=""nofollow noreferrer"">https://github.com/nltk/nltk/blob/develop/nltk/translate/bleu_score.py#L175</a></li>
</ul>
</li>
<li><p>while <a href=""https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/mteval-v13a.pl#L834"" rel=""nofollow noreferrer"">NIST has a smoothing method</a> for geometric sequence smoothing, <a href=""https://github.com/nltk/nltk/blob/develop/nltk/translate/bleu_score.py#L512"" rel=""nofollow noreferrer"">NLTK has an equivalent object with the same smoothing method</a> and even more smoothing methods to handle sentence level BLEU from <a href=""http://www.statmt.org/wmt14/pdf/W14-3346.pdf"" rel=""nofollow noreferrer"">Chen and Collin, 2014</a></p>
</li>
</ul>
<p>Lastly to validate the features added in NLTK's version of BLEU, a regression test is added to accounts for them, see <a href=""https://github.com/nltk/nltk/blob/develop/nltk/test/unit/translate/test_bleu.py"" rel=""nofollow noreferrer"">https://github.com/nltk/nltk/blob/develop/nltk/test/unit/translate/test_bleu.py</a></p>
",""
"45995912","2017-09-01 08:15:49","3","","45981339","<p>Here is a quick conceptual prototype of your grammar, using pyparsing. I could not tell from your question what the contents of the <code>N</code>, <code>V</code>, and <code>DET</code> lists could be, so I just arbitrarily chose words composed of 'n's and 'v's, and the literal 'det'. You can replace the <code>&lt;&lt;=</code> assignments with the correct expressions for your grammar, but this parser and the sample string should show that your grammar is at least feasible. (If you edit your question to show what <code>N</code>, <code>V</code>, and <code>DET</code> are lists <em>of</em>, I can update this answer with less arbitrary expressions and sample. Also including a sample string to be parsed would be useful.)</p>

<p>I also added some grouping so that you could see how the structure of the grammar is reflected in the structure of the results. You can leave this in or removve it and the parser will still work.</p>

<pre><code>import pyparsing as pp

v = pp.Forward()
n = pp.Forward()
det = pp.Forward()

V = pp.Group(pp.OneOrMore(v))
N = pp.Group(pp.OneOrMore(n))
DET = pp.Group(pp.OneOrMore(det))

VP = pp.Group(V + N)
NP = pp.Group(DET + N)
S = NP + VP

# replace these with something meaningful
v &lt;&lt;= pp.Word('v')
n &lt;&lt;= pp.Word('n')
det &lt;&lt;= pp.Literal('det')

sample = 'det det nn nn nn nn vv vv vv nn nn nn nn'

parsed = S.parseString(sample)
print(parsed.asList())
</code></pre>

<p>Prints:</p>

<pre><code>[[['det', 'det'], ['nn', 'nn', 'nn', 'nn']], 
 [['vv', 'vv', 'vv'], ['nn', 'nn', 'nn', 'nn']]]
</code></pre>

<p>EDIT:</p>

<p>I guessed the ""NP"" and ""VP"" are ""noun phrase"" and ""verb phrase"", but I don't know what ""DET"" could be. Still, I made up a less abstract example. I also expanded the lists to accept more grammatical forms of lists of nouns and verbs, with connecting 'and's and commas.</p>

<pre><code>import pyparsing as pp

v = pp.Forward()
n = pp.Forward()
det = pp.Forward()

def collectionOf(expr):
    '''
    Compose a collection expression for a base expression that matches
        expr
        expr and expr
        expr, expr, expr, and expr
    '''
    AND = pp.Literal('and')
    OR = pp.Literal('or')
    COMMA = pp.Suppress(',')
    return expr + pp.Optional(
            pp.Optional(pp.OneOrMore(COMMA + expr) + COMMA) + (AND | OR) + expr)

V = pp.Group(collectionOf(v))('V')
N = pp.Group(collectionOf(n))('N')
DET = pp.Group(pp.OneOrMore(det))('DET')

VP = pp.Group(V + N)('VP')
NP = pp.Group(DET + N)('NP')
S = pp.Group(NP + VP)('S')

# replace these with something meaningful
v &lt;&lt;= pp.Combine(pp.oneOf('chase love hate like eat drink') + pp.Optional(pp.Literal('s')))
n &lt;&lt;= pp.Optional(pp.oneOf('the a my your our his her their')) + pp.oneOf(""dog cat horse rabbit squirrel food water"")
det &lt;&lt;= pp.Optional(pp.oneOf('why how when where')) +pp.oneOf( 'do does did')

samples = '''
    when does the dog eat the food
    does the dog like the cat
    do the horse, cat, and dog like or hate their food
    do the horse and dog love the cat
    why did the dog chase the squirrel
'''
S.runTests(samples)
</code></pre>

<p>Prints:</p>

<pre><code>when does the dog eat the food
[[[['when', 'does'], ['the', 'dog']], [['eat'], ['the', 'food']]]]
- S: [[['when', 'does'], ['the', 'dog']], [['eat'], ['the', 'food']]]
  - NP: [['when', 'does'], ['the', 'dog']]
    - DET: ['when', 'does']
    - N: ['the', 'dog']
  - VP: [['eat'], ['the', 'food']]
    - N: ['the', 'food']
    - V: ['eat']


does the dog like the cat
[[[['does'], ['the', 'dog']], [['like'], ['the', 'cat']]]]
- S: [[['does'], ['the', 'dog']], [['like'], ['the', 'cat']]]
  - NP: [['does'], ['the', 'dog']]
    - DET: ['does']
    - N: ['the', 'dog']
  - VP: [['like'], ['the', 'cat']]
    - N: ['the', 'cat']
    - V: ['like']


do the horse, cat, and dog like or hate their food
[[[['do'], ['the', 'horse', 'cat', 'and', 'dog']], [['like', 'or', 'hate'], ['their', 'food']]]]
- S: [[['do'], ['the', 'horse', 'cat', 'and', 'dog']], [['like', 'or', 'hate'], ['their', 'food']]]
  - NP: [['do'], ['the', 'horse', 'cat', 'and', 'dog']]
    - DET: ['do']
    - N: ['the', 'horse', 'cat', 'and', 'dog']
  - VP: [['like', 'or', 'hate'], ['their', 'food']]
    - N: ['their', 'food']
    - V: ['like', 'or', 'hate']


do the horse and dog love the cat
[[[['do'], ['the', 'horse', 'and', 'dog']], [['love'], ['the', 'cat']]]]
- S: [[['do'], ['the', 'horse', 'and', 'dog']], [['love'], ['the', 'cat']]]
  - NP: [['do'], ['the', 'horse', 'and', 'dog']]
    - DET: ['do']
    - N: ['the', 'horse', 'and', 'dog']
  - VP: [['love'], ['the', 'cat']]
    - N: ['the', 'cat']
    - V: ['love']


why did the dog chase the squirrel
[[[['why', 'did'], ['the', 'dog']], [['chase'], ['the', 'squirrel']]]]
- S: [[['why', 'did'], ['the', 'dog']], [['chase'], ['the', 'squirrel']]]
  - NP: [['why', 'did'], ['the', 'dog']]
    - DET: ['why', 'did']
    - N: ['the', 'dog']
  - VP: [['chase'], ['the', 'squirrel']]
    - N: ['the', 'squirrel']
    - V: ['chase']
</code></pre>
",""
"45971705","2017-08-31 00:58:42","0","","45932370","<p>When I run this command:</p>

<pre><code>java -Xmx8g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,parse -file example.txt -outputFormat json
</code></pre>

<p>With your example text in the file <code>example.txt</code>, I see <code>compound</code> as the relationship between both of those words for both types of dependencies.</p>

<p>I also tried this with the <code>simple API</code> and got the same results.</p>

<p>You can see what <code>simple</code> produces with this code:</p>

<pre><code>package edu.stanford.nlp.examples;

import edu.stanford.nlp.semgraph.SemanticGraphFactory;
import edu.stanford.nlp.simple.*;

import java.util.*;

public class SimpleDepParserExample {

  public static void main(String[] args) {
    Sentence sent = new Sentence(""...example text..."");
    Properties props = new Properties();
    // use sent.dependencyGraph() or sent.dependencyGraph(props, SemanticGraphFactory.Mode.ENHANCED) to see enhanced dependencies
    System.out.println(sent.dependencyGraph(props, SemanticGraphFactory.Mode.BASIC));
  }

}
</code></pre>

<p>I don't know anything about any Scala interfaces for Stanford CoreNLP.  I should also note my results are using the latest code from GitHub, though I presume Stanford CoreNLP 3.8.0 would also produce similar results.  If you are using an older version of Stanford CoreNLP that could be a potential cause of the error.</p>

<p>But running this example in various ways using Java I don't see the issue you are encountering.</p>
",""
"45951891","2017-08-30 04:24:15","4","","45951825","<p>So I will solve a example to loop into list of such type, you can try the same with yours.</p>

<pre><code>    a=[[1,2,3],[4,5,6],[7,8,9]]
    for x in a:
        print(x[0])
   Output looks like:
   1
   4
   7
</code></pre>
",""
"45920982","2017-08-28 14:07:42","6","","45919639","<p>The first obvious point of improvement I see here is that the entire <code>get_wordnet_pos</code> function should be reducible to a dictionary lookup:</p>

<pre><code>def get_wordnet_pos(treebank_tag):
    if treebank_tag.startswith('J'):
        return wordnet.ADJ
    elif treebank_tag.startswith('V'):
        return wordnet.VERB
    elif treebank_tag.startswith('N'):
        return wordnet.NOUN
    elif treebank_tag.startswith('R'):
        return wordnet.ADV
    else:
        return wordnet.NOUN
</code></pre>

<p>Instead of this, initialise a <code>defaultdict</code> from the <code>collections</code> package:</p>

<pre><code>import collections 
get_wordnet_pos = collections.defaultdict(lambda: wordnet.NOUN)
get_wordnet_pos.update({'J' : wordnet.ADJ,  
                        'V' : wordnet.VERB, 
                        'N' : wordnet.NOUN, 
                        'R' : wordnet.ADV })
</code></pre>

<p>You will then access the lookup like this:</p>

<pre><code>get_wordnet_pos[w[1][0]]
</code></pre>

<p>Next, you could consider pre-compiling your regex pattern if it is to be used in multiple places. The speedup you get isn't as much, but it all matters.</p>

<pre><code>pattern = re.compile('[^a-zA-Z]')
</code></pre>

<p>Inside your function, you'd call:</p>

<pre><code>pattern.sub(' ', text)
</code></pre>

<p>OTOH, if you are aware of where your text is coming from and have some idea of what you might and might not see, you can pre-compile a list of characters and instead use <code>str.translate</code>, which is much much faster than a clunky regex based substitution:</p>

<pre><code>tab = str.maketrans(dict.fromkeys(""1234567890!@#$%^&amp;*()_+-={}[]|\'\"":;,&lt;.&gt;/?\\~`"", '')) # pre-compiled use once substitution table (keep this outside the function)

text = 'hello., hi! lol, what\'s up'
new_text = text.translate(tab) # this would run inside your function

print(new_text)

'hello hi lol whats up'
</code></pre>

<p>Furthermore, I'd say <code>word_tokenize</code> is overkill - what you do is get rid of special characters anyway, so you lose all the benefits of <code>word_tokenize</code>, which really makes a difference with punctuation and the like. You could just choose to fall back on <code>text.split()</code>.</p>

<p>Finally, skip the <code>clean = "" "".join(tokensLemmatized)</code> step. Just return the list, and then call <code>df.applymap("" "".join)</code> in the final step.</p>

<p>I leave the benchmarking to you.</p>
",""
"45870635","2017-08-24 20:49:08","2","","45832040","<p>It is a bit hard to understand your problem without an example and the corresponding outputs, but it might be this:</p>

<p>Assuming that <code>text</code> is a string, <code>text_clean</code> will be a list of lists of strings, where every string represents a word. After the part-of-speech tagging, <code>POS_tag_text_clean</code> will therefore be a list of lists of tuples, each tuple containing a word and its tag.</p>

<p>If I'm right, then your last loop actually loops over items from your dataframe instead of words, as the name of the variable suggests. If an item has only one word (which is not so unlikely, since you filter a lot in <code>clean()</code>), your call to <code>word[1]</code> will fail with an error similar to the one you report.</p>

<p>Instead, try this code:</p>

<pre><code>words = []
for item in POS_tag_text_clean:
   words_in_item = []
   for word in item:
      if word[1] !='VBD' and word[1] !='VBN':
         words_in_item .append(word[0])
   words.append(words_in_item)
</code></pre>
",""
"45823963","2017-08-22 17:46:41","3","","45823199","<p>The biggest issue probably is that you are computing tfidf for every pair of documents (document here merely meaning your unit of text - this could be a tweet, a sentence, a scientific paper, or a book). Also, you shouldn't cook up your own similarity measure if it already exists. Finally, <code>sklearn</code> has a <code>pairwise_distance</code> routine that does what you want and is optimized. Putting this all together, here is a sample script:</p>

<pre><code>import requests
import nltk, string
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import pairwise_distances

url = 'https://gist.githubusercontent.com/WalkerHarrison/940c005aa23386a69282f373f6160221/raw/6537d999b9e39d62df3784d2d847d4a6b2602876/sample.txt'
sample = requests.get(url).text.split('\n\n') # I'm splitting the document by ""paragraphs"" since it is unclear what you actually want

stemmer = nltk.stem.porter.PorterStemmer()
remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)

def stem_tokens(tokens):
    return [stemmer.stem(item) for item in tokens]

def normalize(text):
    return stem_tokens(nltk.word_tokenize(text.lower().translate(remove_punctuation_map)))

vectorizer = TfidfVectorizer(tokenizer=normalize, stop_words='english')


doc_vectors = vectorizer.fit_transform(sample)
distances = pairwise_distances(doc_vectors, metric='cosine')


row_idx = list(enumerate(distances.argmax(axis=1)))
sorted_pairs = sorted(row_idx, key= lambda x: distances[x[0], x[1]], reverse=True)

# most similar documents:

i1, i2 = sorted_pairs[0] # index of most similar pair

print(sample[i1])
print(""==""*50)
print(sample[i2])
</code></pre>

<p>There were 99 documents in my <code>sample</code> list, and this ran pretty much instantaneously after the download was complete. Also, the output:</p>

<blockquote>
  <p>Art party taxidermy locavore 3 wolf moon occupy. Tote bag twee tacos
  listicle, butcher single-origin coffee raclette gentrify raw denim
  helvetica kale chips shaman williamsburg man braid. Poke normcore lomo
  health goth waistcoat kogi. Af next level banh mi, deep v locavore
  asymmetrical snackwave chillwave. Subway tile viral flexitarian pok
  pok vegan, cardigan health goth venmo artisan. Iceland next level twee
  adaptogen, dreamcatcher paleo lyft. Selfies shoreditch microdosing
  vape, knausgaard hot chicken pitchfork typewriter polaroid lyft
  skateboard ethical distillery. Farm-to-table blue bottle yr artisan
  wolf try-hard vegan paleo knausgaard deep v salvia ugh offal
  snackwave. Succulents taxidermy cornhole wayfarers butcher, street art
  polaroid jean shorts williamsburg la croix tumblr raw denim. Hot
  chicken health goth taiyaki truffaut pop-up iceland shoreditch
  fingerstache.</p>
</blockquote>

<p>====================================================================================================</p>

<blockquote>
  <p>Organic microdosing keytar thundercats chambray, cray raclette. Seitan
  irony raclette chia, cornhole YOLO stumptown. Gluten-free palo santo
  beard chia. Whatever bushwick stumptown seitan cred quinoa. Small
  batch selfies portland, cardigan you probably haven't heard of them
  shabby chic yr four dollar toast flexitarian palo santo beard offal
  migas. Kinfolk pour-over glossier, hammock poutine pinterest coloring
  book kitsch adaptogen wayfarers +1 tattooed lomo yuccie vice. Plaid
  fixie portland, letterpress knausgaard sartorial live-edge. Austin
  adaptogen YOLO cloud bread wayfarers cliche hammock banjo. Sustainable
  organic air plant mustache.</p>
</blockquote>
",""
"45822930","2017-08-22 16:36:11","3","","45679331","<p>Using the email package, we can read in the .eml files. Then, use the <code>BytesParser</code> library to parse the file. Finally, use a <code>plain</code> preference (for plain text) with the <code>get_body()</code> method, and <code>get_content()</code> method to get the raw text of the email.</p>

<pre><code>import email
from email import policy
from email.parser import BytesParser
import glob
file_list = glob.glob('*.eml') # returns list of files
with open(file_list[2], 'rb') as fp:  # select a specific email file from the list
    msg = BytesParser(policy=policy.default).parse(fp)
text = msg.get_body(preferencelist=('plain')).get_content()
print(text)  # print the email content
&gt;&gt;&gt; ""Hi,
&gt;&gt;&gt; This is an email
&gt;&gt;&gt; Regards,
&gt;&gt;&gt; Mister. E""
</code></pre>

<p>Granted, this is a simplified example - no mention of HTML or attachments. But it gets done essentially what the question asks and what I want to do.</p>

<p>Here is how you would iterate over several emails and save each as  a plain text file:</p>

<pre><code>file_list = glob.glob('*.eml') # returns list of files
for file in file_list:
    with open(file, 'rb') as fp:
        msg = BytesParser(policy=policy.default).parse(fp)
        fnm = os.path.splitext(file)[0] + '.txt'
        txt = msg.get_body(preferencelist=('plain')).get_content()
        with open(fnm, 'w') as f:
            print('Filename:', txt, file = f) 
</code></pre>
",""
"45814875","2017-08-22 10:13:20","2","","45780602","<blockquote>
  <p>Since ""a"" appears in both documents, it has an inverse-document-frequency value of 0</p>
</blockquote>

<p>This is where you have made an error in using inverse document frequency (idf). Idf is meant to be computed over a <em>large</em> collection of documents (not just across two documents), the purpose being to be able to predict the importance of term overlaps in document pairs.</p>

<p>You would expect that common terms, such as 'the', 'a' etc. overlap across all document pairs. Should that be having any contribution to your similarity score? - No.</p>

<p>That is precisely the reason why the vector components are multiplied by the idf factor - just to dampen or boost a particular term overlap (a component of the form a_i*b_i being added to the numerator in the cosine-sim sum).</p>

<p>Now consider you have a collection on computer science journals. Do you believe that an overlap of terms such as 'computer' and 'science' across a document pair is considered to be important? - No.
And this will indeed happen because the idf of these terms would be considerably low in this collection.</p>

<p>What do you think will happen if you extend the collection to scientific articles of any discipline? In that collection, the idf value of the word 'computer' will no longer be low. And that makes sense because in this general collection, you would like to think that two documents are similar enough if they are on the same topic - computer science.</p>
",""
"45763880","2017-08-18 19:34:05","1","","45763803","<p>Going by <a href=""https://groups.google.com/forum/#!topic/nltk-dev/Ii5xtTSD87E"" rel=""nofollow noreferrer"">this</a> link, you might want to parse those rules first using <code>nltk.parse_cfg</code>:</p>

<pre><code>rules = nltk.data.load('grammars/large_grammars/atis.cfg', 'text')
grammar = nltk.parse_cfg(rules)
parser =  nltk.parse.BottomUpChartParser(parsed_grammar)
</code></pre>
",""
"45696131","2017-08-15 15:28:08","0","","45696028","<p>Your variable <code>l</code> is not pre-defined, causing the name error. See my last two lines for fix.</p>

<pre><code>&gt;&gt;&gt; from nltk.stem.snowball import SnowballStemmer
&gt;&gt;&gt; stemmer = SnowballStemmer(""russian"") 
&gt;&gt;&gt; my_words = ['–í–∞—Å–∏–ª–∏–π', '–ì–µ–Ω–Ω–∞–¥–∏–π', '–í–∏—Ç–∞–ª–∏–π']
&gt;&gt;&gt; l=[stemmer.stem(word) for word in l]
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
NameError: name 'l' is not defined
&gt;&gt;&gt; l=[stemmer.stem(word) for word in my_words]
&gt;&gt;&gt; l
['–≤–∞—Å–∏–ª', '–≥–µ–Ω–Ω–∞–¥', '–≤–∏—Ç–∞–ª']
</code></pre>
",""
"45690666","2017-08-15 10:02:39","0","","45690619","<p>Use <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"" rel=""noreferrer"">ngram_range</a> parameter:</p>

<pre><code>vect = tf(min_df=60, stop_words='english', ngram_range=(1,2))
</code></pre>

<p>or (depending on your goals):</p>

<pre><code>vect = tf(min_df=60, stop_words='english', ngram_range=(2,2))
</code></pre>
",""
"45681220","2017-08-14 18:56:26","0","","45681070","<p>Your code has one mistake. <code>l</code> is a list of words, not sentences. You have to do this:</p>

<pre><code>l = [stem(word) for word in l]
</code></pre>

<p>For example:</p>

<pre><code>&gt;&gt;&gt; l = ['gone', 'done', 'crawled', 'laughed', 'cried']
&gt;&gt;&gt; [stem(word) for word in l]
['gone', 'done', 'crawl', 'laugh', 'cri']
</code></pre>
",""
"45677892","2017-08-14 15:23:58","2","","45677519","<p>I wrote this simple version only with <code>python</code>'s standard library,  for educational reasons.</p>

<p>Production code should use <code>spacy</code> and <code>pandas</code></p>

<pre><code>import collections
from operator import itemgetter as at
with open(""input.csv"",'r') as f:
    data = [l.split(',', 2) for l in f.readlines()]
spaced = lambda t: (t[0][0],' '.join(map(at(1), t))) if t[0][0]==t[1][0] else []
unigrams = [(i,w) for i, d in data for w in d.split()]
bigrams = filter(any, map(spaced, zip(unigrams, unigrams[1:] )))
trigrams = filter(any, map(spaced, zip(unigrams, unigrams[1:], unigrams[2:])))
with open(""output.csv"", 'w') as f:
    for ngram in [unigrams, bigrams, trigrams]:
        counts = collections.Counter(ngram)
        for t,count in counts.items():
            f.write(""{i},{w},{c}\n"".format(c=count, i=t[0], w=t[1]))
</code></pre>
",""
"45619532","2017-08-10 16:57:44","4","","45618562","<p>Another option is to create a named vector from your pattern and replacement vectors instead of a data frame, and then use <code>str_replace_all</code> directly, like this:</p>

<pre><code>library(stringr)

vec &lt;- c(""having many items"", ""has an apple"", ""items"")

lem &lt;- c(""item"", ""have"")
names(lem) &lt;- c(""(items)|(item)"", ""(has)|(have)|(having)|(had)"")

str_replace_all(vec, lem)

## ""have many item"" ""have an apple""  ""item""
</code></pre>
",""
"45591295","2017-08-09 12:54:38","1","","45590278","<p>Turning a base form such as a lemma into a situation-appropriate form is called <a href=""https://en.wikipedia.org/wiki/Realization_%28linguistics%29"" rel=""noreferrer"">realization</a> (or ""surface realization""). Example from Wikipedia: </p>

<pre><code>NPPhraseSpec subject = nlgFactory.createNounPhrase(""the"", ""woman"");
subject.setPlural(true);
SPhraseSpec sentence = nlgFactory.createClause(subject, ""smoke"");
sentence.setFeature(Feature.NEGATED, true);
System.out.println(realiser.realiseSentence(sentence));
// output: ""The women do not smoke.""
</code></pre>

<p>Libraries for this are not as frequently used as lemmatizers, which generally means you have fewer options and are less likely to find a well developed library. The Wikipedia example is in Java because the most popular library supporting this is <a href=""https://github.com/simplenlg/simplenlg"" rel=""noreferrer"">SimpleNLG</a>. </p>

<p>A quick search found <a href=""https://github.com/mapado/pynlg"" rel=""noreferrer"">pynlg</a>, though it doesn't seem actively maintained. Alternately you can use SimpleNLG via an HTTP JSON interface provided by the Python library <a href=""https://github.com/mnestis/nlgserv"" rel=""noreferrer"">nlgserv</a>.</p>
",""
"45574050","2017-08-08 16:58:27","0","","45572313","<p>My guess would be that your features are not standardized, meaning that some columns in <code>dtm</code> contain distributions centered around a higher mean than others. The sort you use to extract cluster-associated features will therefore wrongly favor these features.</p>

<p>A common practice to avoid such problems is to <a href=""https://en.wikipedia.org/wiki/Feature_scaling#Standardization"" rel=""nofollow noreferrer"">standardize</a> your features to <code>zero mean</code> and <code>unit variance</code> like this:</p>

<pre><code>dtm_standardized = (dtm - dtm.mean(axis=0)) / dtm.std(axis=0)
</code></pre>

<p>or like this:</p>

<pre><code>dtm_standardized = sklearn.preprocessing.scale(dtm)
</code></pre>
",""
"45562087","2017-08-08 07:36:50","0","","45431399","<p>[Disclaimer: I know next to nothing about alignment and have not used fast_align.]</p>

<p>Yes.</p>

<p>You can prove this to yourself and also plot the accuracy/scale curve by removing data from your dataset to try it at at even lower scale.</p>

<p>That said, 1000 is already absurdly low, for these purposes 1000 ‚âà‚âà 0, and I would not expect it to work.</p>

<p>More ideal would be to try 10K, 100K and 1M.  More comparable to others' results would be some standard corpus, eg Wikipedia or data from the research workshops.</p>

<p>Adding data very different than the data that is important to you can have mixed results, but in this case more data can hardly hurt.  We could be more helpful with suggestions if you mention a specific domain, dataset or goal.</p>
",""
"45555676","2017-08-07 21:03:48","3","","45403390","<p>I'll try to answer your question, knowing that I don't know a lot about italian!</p>

<p>1) As far as I know, the main responsibility for removing apostrophe is the tokenizer, and as such the <code>nltk</code> italian tokenizer seems to have failed.</p>

<p>3) A simple thing you can do about it is call the <code>replace</code> method (although you probably will have to use the <code>re</code> package for more complicated pattern), an example:</p>

<pre><code>word_tokenized_no_punct_no_sw_no_apostrophe = [x.split(""'"") for x in word_tokenized_no_punct_no_sw]
word_tokenized_no_punct_no_sw_no_apostrophe = [y for x in word_tokenized_no_punct_no_sw_no_apostrophe for y in x]
</code></pre>

<p>It yields:</p>

<pre><code>['ieri', 'andato', 'due', 'supermercati', 'oggi', 'volevo', 'andare', 'all', 'ippodromo', 'stasera', 'mangio', 'pizza', 'verdure']
</code></pre>

<p>2) An alternative to pattern would be <code>treetagger</code>, granted it is not the easiest install of all (you need the <a href=""https://pypi.python.org/pypi/treetaggerwrapper/2.1.1"" rel=""noreferrer"">python package</a> and the <a href=""http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/"" rel=""noreferrer"">tool itself</a>, however after this part it works on windows and Linux).</p>

<p>A simple example with your example above:</p>

<pre><code>import treetaggerwrapper 
from pprint import pprint

it_string = ""Ieri sono andato in due supermercati. Oggi volevo andare all'ippodromo. Stasera mangio la pizza con le verdure.""
tagger = treetaggerwrapper.TreeTagger(TAGLANG=""it"")
tags = tagger.tag_text(it_string)
pprint(treetaggerwrapper.make_tags(tags))
</code></pre>

<p>The <code>pprint</code> yields:</p>

<pre><code>[Tag(word=u'Ieri', pos=u'ADV', lemma=u'ieri'),
 Tag(word=u'sono', pos=u'VER:pres', lemma=u'essere'),
 Tag(word=u'andato', pos=u'VER:pper', lemma=u'andare'),
 Tag(word=u'in', pos=u'PRE', lemma=u'in'),
 Tag(word=u'due', pos=u'ADJ', lemma=u'due'),
 Tag(word=u'supermercati', pos=u'NOM', lemma=u'supermercato'),
 Tag(word=u'.', pos=u'SENT', lemma=u'.'),
 Tag(word=u'Oggi', pos=u'ADV', lemma=u'oggi'),
 Tag(word=u'volevo', pos=u'VER:impf', lemma=u'volere'),
 Tag(word=u'andare', pos=u'VER:infi', lemma=u'andare'),
 Tag(word=u""all'"", pos=u'PRE:det', lemma=u'al'),
 Tag(word=u'ippodromo', pos=u'NOM', lemma=u'ippodromo'),
 Tag(word=u'.', pos=u'SENT', lemma=u'.'),
 Tag(word=u'Stasera', pos=u'ADV', lemma=u'stasera'),
 Tag(word=u'mangio', pos=u'VER:pres', lemma=u'mangiare'),
 Tag(word=u'la', pos=u'DET:def', lemma=u'il'),
 Tag(word=u'pizza', pos=u'NOM', lemma=u'pizza'),
 Tag(word=u'con', pos=u'PRE', lemma=u'con'),
 Tag(word=u'le', pos=u'DET:def', lemma=u'il'),
 Tag(word=u'verdure', pos=u'NOM', lemma=u'verdura'),
 Tag(word=u'.', pos=u'SENT', lemma=u'.')]
</code></pre>

<p>It also tokenized pretty nicely the <code>all'ippodromo</code> to <code>al</code> and <code>ippodromo</code> (which is hopefully correct) under the hood before lemmatizing. Now we just need to apply the removal of stop words and punctuation and it will be fine.</p>

<p><a href=""http://treetaggerwrapper.readthedocs.io/en/latest/"" rel=""noreferrer"">The doc for installing the TreeTaggerWrapper</a> library for python</p>
",""
"45522017","2017-08-05 12:40:28","1","","45520228","<p>When you do </p>

<pre><code>&gt;&gt;&gt; result = NPChunker.parse(pos_tag(sentence))
&gt;&gt;&gt; result
Tree('S', [Tree('NP', [('criminal', 'JJ'), ('lawyer', 'NN')]), Tree('NP', [('new', 'JJ'), ('york', 'NN')])])
</code></pre>

<p>you are seeing a <em>string representation of the data structure in memory</em>.</p>

<p>When you type <code>result</code> at the interpreter prompt, what you get is the same as what you get if you type <code>repr(result)</code> at the interpreter prompt. It appears that you have saved this string representation in a file. That is unfortunate because this representation is not acceptable to <code>Tree.fromstring()</code>. </p>

<p>To save an acceptable version to a file you need to write out the <code>str()</code> (not the <code>repr()</code>) of the tree. You can see the difference here:</p>

<pre><code>&gt;&gt;&gt; result
Tree('S', [Tree('NP', [('criminal', 'JJ'), ('lawyer', 'NN')]), Tree('NP', [('new', 'JJ'), ('york', 'NN')])])
&gt;&gt;&gt; str(result)
'(S (NP criminal/JJ lawyer/NN) (NP new/JJ york/NN))'
</code></pre>

<p><code>Tree.fromstring()</code> is expecting the second of these formats.</p>

<p>To verify that this will do what you want:</p>

<pre><code>&gt;&gt;&gt; result2 = nltk.Tree.fromstring(str(result))
&gt;&gt;&gt; result2
Tree('S', [Tree('NP', ['criminal/JJ', 'lawyer/NN']), Tree('NP', ['new/JJ', 'york/NN'])])
</code></pre>

<p>But that is for the future. You need to repair the file you have. Do the following:</p>

<pre><code>&gt;&gt;&gt; from nltk import Tree
&gt;&gt;&gt; input_string = ""Tree('S', [Tree('NP', [('criminal', 'JJ'), ('lawyer', 'NN')]), Tree('NP', [('new', 'JJ'), ('york', 'NN')])])""
</code></pre>

<p>I'm doing an inline assignment here, but of course you will be reading <code>input_string</code> from a text file.</p>

<pre><code>&gt;&gt;&gt; parsed_tree = eval(input_string)
&gt;&gt;&gt; type(parsed_tree)
&lt;class 'nltk.tree.Tree'&gt;
&gt;&gt;&gt; str(parsed_tree)
'(S (NP criminal/JJ lawyer/NN) (NP new/JJ york/NN))'
</code></pre>

<p>This solution is suitable for use as a one-time emergency repair for your file. <em>Don't</em>  do it as a regular procedure. </p>
",""
"45409680","2017-07-31 07:21:46","0","","45353866","<p><strong>answer provided by Klaus Krippendorff</strong></p>

<p>I do not know the NLTK implementation of alpha. It does not seem to be wrong from what you reproduced.</p>

<p>To clarify, Œ± is not based on the interval metric difference. The interval metric difference functions is only one of many versions. It responds to meaningful algebraic differences, absent in nominal categories. </p>

<p>Incidentally, when you have binary data all metric differences should
produce the same results as just two values are either same or different.</p>

<p>Let me focus on the two numerical examples you gave of 2 coders coding 4 units. The coincidence matrix (which tabulates the sum of all possible pairs of valued within units) sums to n=8 not 10 in your calculations. They look like:</p>

<p><a href=""https://i.sstatic.net/Q6bjK.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/Q6bjK.png"" alt=""enter image description here""></a></p>

<p>Yes, as the variance converges to zero so does alpha. In your 1st example there is virtually no variance and the only deviation from uniformity is a  disagreement. The data cannot possible be relied upon for
computing correlations, testing statistical hypotheses, providing information about phenomena of interest to answer research questions. etc. If the annotations were without variation whatsoever, the reliability data would not be able to assure you whether coders were asleep, decided to code everything alike so as to achieve 100% agreement, the instrument they used was broken. Data need variations.</p>

<p>In the 2nd example you do have a larger variance. Whether you calculate alpha with the nominal or the interval metric, the reliabilities have to be higher.</p>
",""
"45240277","2017-07-21 14:30:22","0","","45232671","<p><code>TfidfVectorizer</code> has a <code>vocabulary_</code> attribute which is very useful for what you want. This attribute is dictionary with words as keys and the corresponding column index that word has as value.</p>

<p>For the below example I want the inverse of that dictionary for that I use a dictionary comprehension. </p>

<pre><code>tfidf_vec = TfidfVectorizer()
transformed = tfidf_vec.fit_transform(raw_documents=['this is a quick example','just to show off'])
index_value={i[1]:i[0] for i in tfidf_vec.vocabulary_.items()}
</code></pre>

<p><code>index_value</code> will be used as a lookup table further on.</p>

<p><code>fit_transform</code> returns a Compressed Sparse Row format matrix. The attributes which are useful for what you want to achieve are <code>indices</code> and <code>data</code>. <code>indices</code> returns all the indices that actually contain data and <code>data</code> returns all the data in those indices.</p>

<p>Looping over the returned <code>transformed</code> sparse matrix as follows.</p>

<pre><code>fully_indexed = []
for row in transformed:
    fully_indexed.append({index_value[column]:value for (column,value) in zip(row.indices,row.data)})
</code></pre>

<p>returns a list of dictionaries with the following contents.</p>

<pre><code>[{'example': 0.5, 'is': 0.5, 'quick': 0.5, 'this': 0.5},
 {'just': 0.5, 'off': 0.5, 'show': 0.5, 'to': 0.5}]
</code></pre>

<p>Please note that doing it this way only returns words that have a non zero value for a specific document. Looking at the first document in my example there is no <code>'just', 0.0</code> key value pair in the dictionary. If you want to include those you need to tweak the final dictionary comprehension a bit. </p>

<p>Like so</p>

<pre><code>fully_indexed = []
transformed = np.array(transformed.todense())
for row in transformed:
    fully_indexed.append({index_value[column]:value for (column,value) in enumerate(row)})
</code></pre>

<p>We create a dense version of the matrix as a numpy array loop over each row of the numpy array enumerate the contents and then fill the list of dictionaries. 
Doing it this way results in output that also includes all words that were not present in a document.</p>

<pre><code>[{'example': 0.5,'is': 0.5,'just': 0.0,'off': 0.0,'quick': 0.5,'show': 0.0,'this': 0.5,'to': 0.0},
 {'example': 0.0,'is': 0.0,'just': 0.5,'off': 0.5,'quick': 0.0,'show': 0.5,'this': 0.0,'to': 0.5}]
</code></pre>

<p>You can then add the dictionaries to your dataframe. </p>

<pre><code>df['tf_idf'] = fully_indexed
</code></pre>
",""
"45202286","2017-07-19 22:48:26","2","","45202126","<p>This simple function should return all the nouns in a sentence. Remember that nltk.pos_tag requires the sentence to be tokenized, and the value returned by nltk.pos_tag is an array of tuples.</p>

<pre><code>import nltk
from nltk.tokenize import word_tokenize as wt

NOUN_CODES = ['NN', 'NNP'] # input any codes you want here

def check_nouns(sentence):
    tokenized = wt(sentence)
    tags = nltk.pos_tag(tokenized)
    return [i[0] for i in tags if i[1] in NOUN_CODES]
</code></pre>

<p>In your code, I'm assuming that what_person_said_l_wt is tokenized. In this case, I'd modify it in the following way (with better formatting and keeping in mind the indentation):</p>

<pre><code> NOUN_CODES = ['NN','NNP','NNS','NNPS']

 def Command_Noun_Check(what_person_said_l,what_person_said_l_wt):
                        Command_Noun_Result = nltk.pos_tag(what_person_said_l_wt)
                        print (Command_Noun_Result)
                        for x in Command_Noun_Result:

                            if x[1] in NOUN_CODES:

                                print (""Noun Found"")
                                return True
                            else:
                                return False
</code></pre>
",""
"45180214","2017-07-19 03:00:20","1","","45179185","<p><code>string</code> is immutable so, it is not good practice to update string every time if the string is long. The <a href=""https://waymoot.org/home/python_string/"" rel=""nofollow noreferrer"">link here</a> explains various ways to concatenate string and shows performance analysis. And since, the iteration is done only once, it is good to choose <code>generator expression</code> over <code>list comprehension</code>. For details you can look into <a href=""https://stackoverflow.com/questions/47789/generator-expressions-vs-list-comprehension"">discussion here </a>. Instead in this case, using <code>generator expression</code> with <code>join</code> can be helpful:</p>

<p>Using <code>my_text</code> for long string: <code>len(my_text) -&gt; 444399</code> </p>

<p>Using <code>timeit</code> to compare:</p>

<pre><code>%%timeit
tokenAux=""""
textAux=""""
tokens = nltk.word_tokenize(my_text)
for token in tokens:
    tokenAux = token
    tokenAux = stemmer.stem(token)    
    textAux = textAux + "" ""+ tokenAux
</code></pre>

<p>Result:</p>

<pre><code>1 loop, best of 3: 6.23 s per loop
</code></pre>

<p>Using <code>generator expression</code> with <code>join</code>:</p>

<pre><code>%%timeit 
' '.join(stemmer.stem(token) for token in nltk.word_tokenize(my_text))
</code></pre>

<p>Result:</p>

<pre><code>1 loop, best of 3: 2.93 s per loop
</code></pre>
",""
"45153343","2017-07-17 20:43:01","0","","44946739","<p>It seems like the model can take a batch of inputs, which is set to 20 in all cases by default. You should be able to feed a larger batch of sentences to one test model to get the output for all of them without having to create multiple models instances. This probably involves some experimenting with the reader, which you are already familiar with.</p>
",""
"45033388","2017-07-11 11:33:16","2","","44522536","<p>As a computer scientist, you are definitely looking in the right direction to tackle this linguistic issue ;). Stemming is usually quite a bit more simplistic, and used for Information Retrieval tasks in an attempt to decrease the lexicon size, but usually not sufficient for more sophisticated linguistic analysis. Lemmatisation partly overlaps with the use case for stemming, but includes rewriting for example verb inflections all to the same root form (lemma), and also differentiating ""work"" as a noun and ""work"" as a verb (although this depends a bit on the implementation and quality of the lemmatiser). For this, it usually needs a bit more information (like POS-tags, syntax trees), hence takes considerably longer, rendering it less suitable for IR tasks, typically dealing with larger amounts of data.</p>

<p>In addition to GermaNet (didn't know it was aborted, but never really tried it, because it is free, but you have to sign an agreement to get access to it), there is SpaCy which you could have a look at: <a href=""https://spacy.io/docs/usage/"" rel=""noreferrer"">https://spacy.io/docs/usage/</a></p>

<p>Very easy to install and use. See install instructions on the website, then download the German stuff using: </p>

<pre><code>python -m spacy download de
</code></pre>

<p>then:</p>

<pre><code>&gt;&gt;&gt; import spacy
&gt;&gt;&gt; nlp = spacy.load('de')
&gt;&gt;&gt; doc = nlp('Wir suchen ein Beispiel')
&gt;&gt;&gt; for token in doc:
...     print(token, token.lemma, token.lemma_)
... 
Wir 521 wir
suchen 1162 suchen
ein 486 ein
Beispiel 809 Beispiel
&gt;&gt;&gt; doc = nlp('Er sucht ein Beispiel')
&gt;&gt;&gt; for token in doc:
...     print(token, token.lemma, token.lemma_)
... 
Er 513 er
sucht 1901 sucht
ein 486 ein
Beispiel 809 Beispiel
</code></pre>

<p>As you can see, unfortunately it doesn't do a very good job on your specific example (suchen), and I'm not sure what the number represents (i.e. must be the lemma id, but not sure what other information can be obtained from this), but maybe you can give it a go and see if it helps you.</p>
",""
"44974256","2017-07-07 15:13:35","1","","44972641","<p>There is most likely an error in your <code>clean_doc</code> function. The 'tokenizer' argument should be a function that takes a string as input and returns a list of tokens.</p>
",""
"44942068","2017-07-06 07:12:17","0","","44941604","<p>You can use WordNet which is built into <code>nltk</code>. For more information check out this question here: <a href=""https://stackoverflow.com/questions/26222484/determining-hypernym-or-hyponym-using-wordnet-nltk"">Determining Hypernym or Hyponym using wordnet nltk</a></p>

<p>It's important to understand however that identification of hypernyms will only be as good as the training data allows. There is no magical system for identifying groupings of similar objects other than to train using tagged data.</p>
",""
"44875134","2017-07-02 20:56:18","0","","44873156","<p>This does not make a lot of sense to me. Perplexity is calculated as <code>2^entropy</code>. And the entropy is from 0 to 1. So your results which are &lt; 1 do not make sense.</p>

<p>I would suggest you to take a look at how your model calculate the perplexity because I suspect there might be an error.</p>
",""
"44808234","2017-06-28 16:40:10","4","","44807639","<p>Python interpreter clearly told You:</p>

<pre><code>AttributeError: 'tuple' object has no attribute 'endswith'
</code></pre>

<p><code>tokensPOS</code> is an array of tuples, so You can't pass its elements directly to <code>lemmatize()</code> method (look at code of class <code>WordNetLemmatizer</code> <a href=""http://www.nltk.org/_modules/nltk/stem/wordnet.html"" rel=""nofollow noreferrer"">here</a>). Only string type object have method <code>endswith()</code>, so You need to pass first element of every tuple from <code>tokenPOS</code>, just like that:</p>

<pre><code>lemmatizedWords = []
for w in tokensPOS:
    lemmatizedWords.append(WordNetLemmatizer().lemmatize(w[0]))   
</code></pre>

<p>Method <code>lemmatize()</code> uses <code>wordnet.NOUN</code> as a default POS. Unfortunately Wordnet uses different tags than other nltk corpora, so You have to manually translate them (as in the link You provided) and use proper tag as a second parameter to <code>lemmatize()</code>. Full script, with method <code>get_wordnet_pos()</code> from <a href=""https://stackoverflow.com/a/15590384/4088548"">this answer</a>:</p>

<pre><code>from nltk.corpus import wordnet
from nltk.stem import WordNetLemmatizer
from nltk import pos_tag
from nltk.tokenize import word_tokenize

def get_wordnet_pos(treebank_tag):

    if treebank_tag.startswith('J'):
        return wordnet.ADJ
    elif treebank_tag.startswith('V'):
        return wordnet.VERB
    elif treebank_tag.startswith('N'):
        return wordnet.NOUN
    elif treebank_tag.startswith('R'):
        return wordnet.ADV
    else:
        return ''

string = 'dogs runs fast'

tokens = word_tokenize(string)
tokensPOS = pos_tag(tokens)
print(tokensPOS)

lemmatizedWords = []
for w in tokensPOS:
    lemmatizedWords.append(WordNetLemmatizer().lemmatize(w[0],get_wordnet_pos(w[1])))

print(lemmatizedWords)
</code></pre>
",""
"44752871","2017-06-26 03:47:22","0","","44752571","<p><strong>TL;DR</strong></p>

<pre><code>result1 = dataset['Content'].apply(lambda x: [lemmatizer.lemmatize(y) for y in x.split()])
</code></pre>

<p>Lemmatizer takes in any string as an input. </p>

<p>If <code>dataset['Content']</code> columns are strings, iterating through a string would be iterating through the characters not ""words"", e.g.</p>

<pre><code>&gt;&gt;&gt; from nltk.stem import WordNetLemmatizer
&gt;&gt;&gt; wnl = WordNetLemmatizer()
&gt;&gt;&gt; x = 'this is a foo bar sentence, that is of type str'
&gt;&gt;&gt; [wnl.lemmatize(ch) for ch in x]
['t', 'h', 'i', 's', ' ', 'i', 's', ' ', 'a', ' ', 'f', 'o', 'o', ' ', 'b', 'a', 'r', ' ', 's', 'e', 'n', 't', 'e', 'n', 'c', 'e', ',', ' ', 't', 'h', 'a', 't', ' ', 'i', 's', ' ', 'o', 'f', ' ', 't', 'y', 'p', 'e', ' ', 's', 't', 'r']
</code></pre>

<p>So you would have to first word tokenize your sentence string, e.g.:</p>

<pre><code>&gt;&gt;&gt; from nltk import word_tokenize
&gt;&gt;&gt; [wnl.lemmatize(word) for word in x.split()]
['this', 'is', 'a', 'foo', 'bar', 'sentence,', 'that', 'is', 'of', 'type', 'str']
&gt;&gt;&gt; [wnl.lemmatize(ch) for ch in word_tokenize(x)]
['this', 'is', 'a', 'foo', 'bar', 'sentence', ',', 'that', 'is', 'of', 'type', 'str']
</code></pre>

<p>another e.g.</p>

<pre><code>&gt;&gt;&gt; from nltk import word_tokenize
&gt;&gt;&gt; x = 'the geese ran through the parks'
&gt;&gt;&gt; [wnl.lemmatize(word) for word in x.split()]
['the', u'goose', 'ran', 'through', 'the', u'park']
&gt;&gt;&gt; [wnl.lemmatize(ch) for ch in word_tokenize(x)]
['the', u'goose', 'ran', 'through', 'the', u'park']
</code></pre>

<p>But to get a more accurate lemmatization, you should get the sentence word tokenized and pos-tagged, see <a href=""https://github.com/alvations/earthy/blob/master/FAQ.md#how-to-use-default-nltk-functions-in-earthy"" rel=""nofollow noreferrer"">https://github.com/alvations/earthy/blob/master/FAQ.md#how-to-use-default-nltk-functions-in-earthy</a></p>
",""
"44730440","2017-06-23 21:54:45","0","","44714142","<p>This task is called coreference resolution. In order to parse complex cases like the one you mention, you'd need to use a coreference resolution system, most of which (free/OOS) are developed in Java. There are several ways to easily use them from Python. One of the most well-know is this Standford CoreNLP wrapper: <a href=""https://github.com/dasmith/stanford-corenlp-python"" rel=""noreferrer"">https://github.com/dasmith/stanford-corenlp-python</a></p>
",""
"44651648","2017-06-20 11:19:36","0","","41937898","<p>There is no such dependency parsing for a paragraph but you can use Stanford coreference resolution on the sentences and extract particular dependencies.</p>
",""
"44610184","2017-06-17 23:00:24","1","","44602346","<p>The bigger question is how much training data you have. There's a lot of interesting work, but the reason that the deep neural network approaches tend to use QA ranking tasks is because those tasks typically have hundreds of thousands or millions of training examples. </p>

<p>When you have shorter queries, i.e. title or web queries, you will possibly need even more data to learn, because less of the network will be exercised by each training instance. It is possible, but the method you choose should be based on the training data you have available, rather than the size of your queries, in general.</p>

<ul>
<li>[0-50 queries] -> Hand-tuned, time-tested model such as Query Likelihood, BM25, (or if you want better results, ngram models such as SDM) (if you want more recall, pseudo-relevance-feedback models such as RM3).</li>
<li>[50-1000 queries] -> Linear or Tree-based learning-to-rank methods</li>
<li>[1000-millions] -> Deep approach, or possibly still learning-to-rank. I'm not sure any of the deep papers have truly dominated a state-of-the-art gradient-boosted-regression-tree setup.</li>
</ul>

<p>A recent paper by one of my labmates used <a href=""http://hamedz.ir/blog/how-to-train-deep-learning-models-when-you-dont-have-enough-training-data/"" rel=""nofollow noreferrer"">pseudo-labels from BM25 to bootstrap a DNN</a>. They got good results (better than BM25), but they literally had to be Google (training-time-wise) to pull it off.</p>
",""
"44601800","2017-06-17 07:08:12","0","","43546510","<ul>
<li><p>With good train data, word2vec must have better performance. (I got good results from it)</p></li>
<li><p>You must have large amount of data for good model. The best way is using pre-trained data if you are working on English. There are good models in <a href=""https://github.com/3Top/word2vec-api"" rel=""nofollow noreferrer"">this link</a> you can use. Google News pre-trained model is working perfect as I know.</p></li>
<li><p>It is common to use Average of words in part of text like sentence. The better way can be Weighted Average like tf-idf weighting average. Also there is a hot research on semantic textual similarity you can follow it from it's <a href=""http://ixa2.si.ehu.es/stswiki/index.php/Main_Page"" rel=""nofollow noreferrer"">Wiki Page</a></p></li>
</ul>
",""
"44588364","2017-06-16 11:39:00","1","","44584671","<p>You should have tried to debug your code. If (after necessary imports) you had just tried <code>print(stemmer.stem(""challenges""))</code>, you would have seen that the stemming <em>does</em> work (the above will print ""challeng""). Your problem is a small oversight: You collect the stems in <code>final</code>, but you print <code>tokens_pos</code>. So the ""solution"" is this:</p>

<pre><code>writer.writerow(final)
</code></pre>
",""
"44489957","2017-06-12 00:35:58","0","","44489768","<p>Your problem is with the <code>FreqDist</code> -- you haven't yet gotten around to creating the default tagger. Since you're just trying to count tags, feed the tags to the <code>FreqDist</code> like this:</p>

<pre><code>tagF = FreqDist(tag for word, tag in EC.tagged_words())
</code></pre>

<p>(Note that <code>tagged_words()</code> returns a flat sequence, not a list of lists.)
You can then continue with the nltk tutorial to build your default tagger.</p>
",""
"44489427","2017-06-11 22:45:54","6","","44489357","<p>Yes, there is another alternative to w2v: <a href=""https://nlp.stanford.edu/projects/glove/"" rel=""nofollow noreferrer"">GloVe</a>.</p>

<p>GloVe stands for <em>Global Vector Embeddings</em>. 
As someone who has used this technique before to good effect, I would recommend GloVe. </p>

<p>GloVe optimally trains neural word embeddings not just by looking at local windows but considering a much larger width (30+ size), thereby embedding a much deeper level of semantics to the embedding.</p>

<p>With glove, it is easy to model relationships such as: <code>X[man] - X[woman] = X[king] - X[queen]</code>, where these are all vectors.</p>

<p><a href=""https://i.sstatic.net/AcZzd.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/AcZzd.png"" alt=""enter image description here""></a></p>

<p>Credits: GloVe GitHub page (linked below).</p>

<p>You can train your own GloVe embeddings, or you may use their retrained models available. Even for specific domains, the general models seem to work reasonably well, although you would get a lot more out of your models if you trained them yourself. Please look at the GitHub page for instructions on how to train your own models. It is very easy.</p>

<p>Additional reading:</p>

<ul>
<li><a href=""https://nlp.stanford.edu/pubs/glove.pdf"" rel=""nofollow noreferrer"">GloVe: Global Vectors for Word Representation</a></li>
<li><a href=""https://github.com/stanfordnlp/GloVe"" rel=""nofollow noreferrer"">GloVe repository</a> </li>
</ul>
",""
"44475198","2017-06-10 15:44:19","0","","44468300","<p>The NLTK doesn't come with pre-built resources for French. I recommend using the <a href=""https://nlp.stanford.edu/software/tagger.shtml"" rel=""nofollow noreferrer"">Stanford tagger</a>, which comes with a trained French model. <a href=""https://github.com/cmchurch/nltk_french/blob/master/french-nltk.py"" rel=""nofollow noreferrer"">This code</a> shows how you might set up the nltk for use with Stanford's French POS tagger. Note that the code is outdated (and for Python 2), but you could use it as a starting point.</p>

<p>Alternately, the NLTK makes it very easy to train your own POS tagger on a tagged corpus, and save it for later use. If you have access to a (sufficiently large) French corpus, you can follow the instructions in <a href=""http://www.nltk.org/book/ch05.html#sec-n-gram-tagging"" rel=""nofollow noreferrer"">the nltk book</a> and simply use your corpus in place of the Brown corpus. You're unlikely to match the performance of the Stanford tagger (unless you can train a tagger for your specific domain), but you won't have to install anything.</p>
",""
"44390328","2017-06-06 12:42:23","6","","44390198","<p>Seems like you have the variables mixed up, it should be </p>

<pre><code>for word in w_tokenize:
  print WordNetLemmatizer().lemmatize(word,'v')
</code></pre>
",""
"44383998","2017-06-06 07:35:44","0","","44382254","<p>Yes. The simplest way is not to use a tagger, but simply load up one or more corpora and collect the set of all tags for the word you are interested in. If you're interested in more than one word, it's simplest to collect the tags for all words in the corpus, then look up anything you want. I'll add frequency counts, just because I can. For example, using the Brown corpus and the simple ""universal"" tagset:</p>

<pre><code>&gt;&gt;&gt; wordtags = nltk.ConditionalFreqDist((w.lower(), t) 
        for w, t in nltk.corpus.brown.tagged_words(tagset=""universal""))
&gt;&gt;&gt; wordtags[""report""]
FreqDist({'NOUN': 135, 'VERB': 39})
&gt;&gt;&gt; list(wordtags[""kind""])
['ADJ', 'NOUN']
</code></pre>
",""
"44362491","2017-06-05 05:52:34","4","","44361787","<p>See </p>

<ul>
<li><a href=""https://stackoverflow.com/questions/22333392/stemming-some-plurals-with-wordnet-lemmatizer-doesnt-work"">Stemming some plurals with wordnet lemmatizer doesn&#39;t work</a>  </li>
<li><a href=""https://stackoverflow.com/questions/22999273/python-nltk-lemmatization-of-the-word-further-with-wordnet"">Python NLTK Lemmatization of the word &#39;further&#39; with wordnet</a></li>
</ul>

<p>For most non-standard English word, WordNet Lemmatizer is not going to help much in getting the correct lemma, try a stemmer:</p>

<pre><code>&gt;&gt;&gt; from nltk.stem import PorterStemmer
&gt;&gt;&gt; porter = PorterStemmer()
&gt;&gt;&gt; porter.stem('surahs')
u'surah'
</code></pre>

<hr>

<p>Also, try the <code>lemmatize_sent</code> in <a href=""https://github.com/alvations/earthy"" rel=""nofollow noreferrer""><code>earthy</code></a> (an <code>nltk</code> wrapper, ""shameless plug""):</p>

<pre><code>&gt;&gt;&gt; from earthy.nltk_wrappers import lemmatize_sent
&gt;&gt;&gt; sentence = ""Then bring ten surahs like it that have been invented and call upon for assistance whomever you can besides Allah if you should be truthful""
&gt;&gt;&gt; lemmatize_sent(sentence)
[('Then', 'Then', 'RB'), ('bring', 'bring', 'VBG'), ('ten', 'ten', 'RP'), ('surahs', 'surahs', 'NNS'), ('like', 'like', 'IN'), ('it', 'it', 'PRP'), ('that', 'that', 'WDT'), ('have', 'have', 'VBP'), ('been', u'be', 'VBN'), ('invented', u'invent', 'VBN'), ('and', 'and', 'CC'), ('call', 'call', 'VB'), ('upon', 'upon', 'NN'), ('for', 'for', 'IN'), ('assistance', 'assistance', 'NN'), ('whomever', 'whomever', 'NN'), ('you', 'you', 'PRP'), ('can', 'can', 'MD'), ('besides', 'besides', 'VB'), ('Allah', 'Allah', 'NNP'), ('if', 'if', 'IN'), ('you', 'you', 'PRP'), ('should', 'should', 'MD'), ('be', 'be', 'VB'), ('truthful', 'truthful', 'JJ')]

&gt;&gt;&gt; words, lemmas, tags = zip(*lemmatize_sent(sentence))
&gt;&gt;&gt; lemmas
('Then', 'bring', 'ten', 'surahs', 'like', 'it', 'that', 'have', u'be', u'invent', 'and', 'call', 'upon', 'for', 'assistance', 'whomever', 'you', 'can', 'besides', 'Allah', 'if', 'you', 'should', 'be', 'truthful')

&gt;&gt;&gt; from earthy.nltk_wrappers import pywsd_lemmatize
&gt;&gt;&gt; pywsd_lemmatize('surahs')
'surahs'

&gt;&gt;&gt; from earthy.nltk_wrappers import porter_stem
&gt;&gt;&gt; porter_stem('surahs')
u'surah'
</code></pre>
",""
"44360829","2017-06-05 02:16:56","0","","44360774","<p>You can change the <em>token_pattern</em> parameter from <code>(?u)\\b\\w\\w+\\b</code> (default) to <code>(?u)\\b\\w\\w*\\b</code>; The default matches token that has two or more word characters (in case you are not familiar with regex, <code>+</code> means one or more, so <code>\\w\\w+</code> matches word with two or more word characters; <code>*</code> on the other hand means zero or more, <code>\\w\\w*</code> will thus match word with one or more characters):</p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
tf = TfidfVectorizer(stop_words = None, token_pattern='(?u)\\b\\w\\w*\\b')
‚Äã
words_list = [""Ê≠Ø"",""„Åå"",""Áóõ„ÅÑ""]
tfidf_matrix =  tf.fit_transform(words_list)
feature_names = tf.get_feature_names() 
print(feature_names)
# ['„Åå', 'Ê≠Ø', 'Áóõ„ÅÑ']
</code></pre>
",""
"44295728","2017-05-31 22:36:23","7","","44279889","<p>Maybe something like this could work:</p>

<pre><code>def iter_sentences(fn):
    with open(fn, 'r') as f:
         reader = csv.reader(f,delimiter='\t')
         sentence = []
         for row in reader:
             if not row:
                # Ignore blank lines.
                continue
             if row[0] == '1' and sentence:
                 # A new sentence started.
                 yield sentence
                 sentence = []
             sentence.append(row)
         # Last sentence.
         if sentence:
             yield sentence

def iter_triples(fn):
    for sentence in iter_sentences(fn):
        # Get all subjects and objects.
        subjects = [tok for tok in sentence if tok[-1] == 'su']
        objects = [tok for tok in sentence if tok[-1] == 'obj1']
        # Now try to map them: find pairs with a head in the same position.
        for obj in objects:
            for subj in subjects:
                # row[-2] is the position of the head.
                if subj[-2] == obj[-2]:
                    # Matching subj-obj pair found.
                    # Now get the verb (the head of both subj and obj).
                    # Its position is given in the second-to-last column.
                    position = int(subj[-2])
                    # Subtract 1, as the positions start counting at 1.
                    verb = sentence[position-1]
                    yield subj, verb, obj

for subj, verb, obj in iter_triples('romanfragment_frogged.tsv'):
    # Only print the surface forms.
    print(subj[1], verb[1], obj[1])
</code></pre>

<p>Quick explanation:
<code>iter_sentences</code> iterates over sentences.
Each sentence is a nested list:
It's a list of tokens, and each token is a list itself (containing the row number, surface form, lemma, POS, dependency etc.).
The <code>iter_triples</code> function iterates over triples ‚Äπsubject, verb, object‚Ä∫.
Each element of these triples represents a token (ie. a list, again).</p>

<p>The last three lines of code are just an example of how to use the <code>iter_triples</code> function.
I don't know how much and which information you need from each triple...</p>
",""
"44233155","2017-05-29 00:07:33","0","","44229467","<p>From the <a href=""https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/__init__.py#L32"" rel=""nofollow noreferrer"">docs</a>: </p>

<pre><code>Corpus reader functions are named based on the type of information they return.  
Some common examples, and their return types, are:
- words(): list of str
- sents(): list of (list of str)
- paras(): list of (list of (list of str))
- tagged_words(): list of (str,str) tuple
- tagged_sents(): list of (list of (str,str))
- tagged_paras(): list of (list of (list of (str,str)))
- chunked_sents(): list of (Tree w/ (str,str) leaves)
- parsed_sents(): list of (Tree with str leaves)
- parsed_paras(): list of (list of (Tree with str leaves))
- xml(): A single xml ElementTree
- raw(): unprocessed corpus contents


&gt;&gt;&gt; from nltk.corpus import brown

&gt;&gt;&gt; brown.tagged_words()
[(u'The', u'AT'), (u'Fulton', u'NP-TL'), ...]

&gt;&gt;&gt; len(brown.tagged_words()) # no. of words in the corpus.
1161192


&gt;&gt;&gt; len(brown.tagged_sents()) # no. of sentence in the corpus.
57340

# Loop through the sentences and counts the words per sentence.
&gt;&gt;&gt; sum(len(sent) for sent in brown.tagged_sents()) # no. of words in the corpus.
1161192
</code></pre>
",""
"44232589","2017-05-28 22:25:47","0","","44226139","<p>You can use <a href=""https://docs.scipy.org/doc/scipy-0.19.0/reference/generated/scipy.sparse.hstack.html"" rel=""nofollow noreferrer""><code>hstack</code></a> to merge the two sparse matrices, without having to convert to dense format.</p>

<pre><code>from scipy.sparse import hstack

hstack([X_train_counts, X_train_custom])
</code></pre>
",""
"44199476","2017-05-26 10:40:08","0","","44198863","<p>Is short, it is not possible: see <a href=""https://github.com/chartbeat-labs/textacy/pull/16"" rel=""nofollow noreferrer"">this official page</a>.</p>

<p>However the merge request contains the code of the modified version described in the page, therefore one can recreate the functionality, despite it's less performing than using a <code>SpaCy</code>'s <code>Matcher</code> (see <a href=""https://github.com/spacy-io/spaCy/blob/master/spacy/matcher.pyx"" rel=""nofollow noreferrer"">code</a> and <a href=""https://github.com/spacy-io/spaCy/blob/master/examples/matcher_example.py"" rel=""nofollow noreferrer"">example</a> -- though I have no idea how to reimplement my problem using a <code>Matcher</code>).</p>

<p>If you want to go down this lane anyway, you have to change the line:</p>

<pre><code>words.extend(map(lambda x: re.sub(r'\W', '', x), keyword_map[w]))
</code></pre>

<p>with the following:</p>

<pre><code>words.extend(keyword_map[w])
</code></pre>

<p>otherwise every symbol (like <code>,</code> and <code>;</code> in my case) will be stripped off.</p>
",""
"44176465","2017-05-25 09:02:21","5","","44175810","<p>Apache OpenNLP is actively developed. Take a look at the commit history [1], there are commits done almost everyday by different contributors and they cut four releases this years (1.7.0, 1.7.1, 1.7.2, and just recently 1.8.0).</p>

<p>OpenNLP is licensed under company friendly Apache License 2.0, compared to CoreNLP which is licensed under GPL which is difficult to use in commercial software (e.g. software being distributed must be released under GPL as well), but they are selling commercial licenses.</p>

<p>OpenNLP is developed mostly by companies which run it in their production systems, where CoreNLP is made by a researchers at Stanford.</p>

<p>CoreNLP has a quite a few dependencies which are pulled into your project, where OpenNLP has zero dependencies.</p>

<p>OpenNLP can support you with the following tasks:</p>

<ul>
<li>Sentence Detection</li>
<li>Tokenization</li>
<li>Chunking</li>
<li>Named Entity Recognition</li>
<li>Pos Tagging</li>
<li>Parsing</li>
<li>Stemming</li>
<li>Language Model</li>
<li>Lemmatization</li>
<li>Document classification</li>
</ul>

<p>OpenNLP is highly customizable, easy to train on user data, has support for training on many publicly available corpora and features built-in evaluation to measure performance of every component.</p>

<p>CoreNLP supports these tasks:</p>

<ul>
<li>Sentence Detection</li>
<li>Tokenization</li>
<li>Named Entity Recognition</li>
<li>Pos Tagging</li>
<li>Parsing (also dependency parsing)</li>
<li>Sentiment</li>
<li>Coreference</li>
<li>Lemmatization</li>
<li>Relation Extraction</li>
</ul>

<p>[1] <a href=""https://github.com/apache/opennlp/commits/master"" rel=""noreferrer"">https://github.com/apache/opennlp/commits/master</a></p>
",""
"44105678","2017-05-22 06:05:54","1","","43841467","<p>See <a href=""https://github.com/kpu/kenlm/blob/master/python/kenlm.pyx#L182"" rel=""noreferrer"">https://github.com/kpu/kenlm/blob/master/python/kenlm.pyx#L182</a></p>

<pre><code>import kenlm

model=kenlm.Model(""something.arpa"") 
per=model.perplexity(""your text sentance"")

print(per)
</code></pre>
",""
"44004216","2017-05-16 14:24:53","0","","44004104","<p>You can just use this list comprehension:</p>

<pre><code>[[j[0] for j in i if j[-1]==""NNP""] for i in data]
</code></pre>

<p>Output:</p>

<pre><code>[['User', 'Coala', 'VWR'], ['Arfter', 'COALA', 'Category', 'S9901', 'Dummy'], [], [], ['POETcatalog'], ['Vendor', 'VWR', 'COALA'], [], ['COALA'], ['User', 'Universitaet', 'Regensburg', 'Scout', 'P17', 'YESRMCDMUSER01', 'Merck', 'KGaA'], ['Please']]
</code></pre>
",""
"43973705","2017-05-15 07:30:10","0","","43943372","<p>What you are asking for, belongs to the domain of <em>Confidence Estimation</em>, nowadays (within the Machine Translation (MT) community) better known as <em>Quality Estimation</em>, i.e. ""assigning a score to MT output without access to a reference translation"". </p>

<p>For MT evaluation (using BLEU, NIST or METEOR) you need:</p>

<ol>
<li>A hypothesis translation (MT output)</li>
<li>A reference translation (from a test set)</li>
</ol>

<p>In your case (real-time translation), you do not have (2). So you will have to estimate the performance of your system, based on features of your source sentence and your hypothesis translation, and on the knowledge you have about the MT process.</p>

<p>A baseline system with 17 features is described in:</p>

<ul>
<li>Specia, L., Turchi, M., Cancedda, N., Dymetman, M., &amp; Cristianini, N. (2009b). Estimating the sentence level quality of machine translation systems. 13th Conference of the European Association for Machine Translation, (pp. 28-37)</li>
<li>Which you can find <a href=""http://www.mt-archive.info/EAMT-2009-Specia.pdf"" rel=""nofollow noreferrer"">here</a></li>
</ul>

<p>Quality Estimation is an active research topic. The most recent advances can be followed on the websites of the WMT Conferences. Look for the Quality Estimation shared tasks, for example <a href=""http://www.statmt.org/wmt17/quality-estimation-task.html"" rel=""nofollow noreferrer"">http://www.statmt.org/wmt17/quality-estimation-task.html</a></p>
",""
"43969608","2017-05-14 23:06:54","0","","43966848","<p>Jus count</p>

<pre><code>&gt;&gt;&gt; from nltk import word_tokenize, pos_tag
&gt;&gt;&gt; s = ""this is my problem , i need help for a function like this one ""
&gt;&gt;&gt; sum(1 for word, pos in pos_tag(word_tokenize(s)) if pos.startswith('NN'))
4
</code></pre>

<p>With nouns</p>

<pre><code>&gt;&gt;&gt; from nltk import word_tokenize, pos_tag
&gt;&gt;&gt; s = ""this is my problem , i need help for a function like this one ""
&gt;&gt;&gt; sum(1 for word, pos in pos_tag(word_tokenize(s)) if pos.startswith('NN'))
4
&gt;&gt;&gt; nouns = [word for word, pos in pos_tag(word_tokenize(s)) if pos.startswith('NN')]
&gt;&gt;&gt; len(nouns)
4
&gt;&gt;&gt; nouns
['problem', 'help', 'function', 'one']
</code></pre>

<p>With nouns and pos</p>

<pre><code>&gt;&gt;&gt; from nltk import word_tokenize, pos_tag
&gt;&gt;&gt; s = ""this is my problem , i need help for a function like this one ""
&gt;&gt;&gt; nouns = [(word,pos) for word, pos in pos_tag(word_tokenize(s)) if pos.startswith('NN')]
&gt;&gt;&gt; nouns
[('problem', 'NN'), ('help', 'NN'), ('function', 'NN'), ('one', 'NN')]
</code></pre>
",""
"43960593","2017-05-14 05:09:37","0","","43959815","<p>The input for senna.tag_sents is list of list of strings, which can be achieved through <code>[word_tokenize(sent) for sent in sents]</code></p>

<pre><code>&gt;&gt;&gt; from nltk import word_tokenize
&gt;&gt;&gt; from nltk.tag import SennaTagger
&gt;&gt;&gt; senna = SennaTagger('/home/alvas/senna/')
&gt;&gt;&gt; sents = [""All the banks are closed"", ""Today is Sunday""]

&gt;&gt;&gt; tokenized_sents = [word_tokenize(sent) for sent in sents]
&gt;&gt;&gt; senna.tag_sents(tokenized_sents)
[[('All', u'PDT'), ('the', u'DT'), ('banks', u'NNS'), ('are', u'VBP'), ('closed', u'VBN')], [('Today', u'NN'), ('is', u'VBZ'), ('Sunday', u'NNP')]]
</code></pre>

<p>Or use <code>map</code> if you don't want to materialize <code>tokenized_sents</code> before tagging:</p>

<pre><code>&gt;&gt;&gt; tokenized_sents = map(word_tokenize, sents)
&gt;&gt;&gt; senna.tag_sents(tokenized_sents)
[[('All', u'PDT'), ('the', u'DT'), ('banks', u'NNS'), ('are', u'VBP'), ('closed', u'VBN')], [('Today', u'NN'), ('is', u'VBZ'), ('Sunday', u'NNP')]]
</code></pre>
",""
"43942707","2017-05-12 16:43:01","0","","43795249","<p>Let's start with the class definition: <a href=""https://github.com/explosion/spaCy/blob/develop/spacy/lemmatizer.py"" rel=""noreferrer"">https://github.com/explosion/spaCy/blob/develop/spacy/lemmatizer.py</a> </p>

<h1>Class</h1>

<p>It starts off with initializing 3 variables:</p>

<pre><code>class Lemmatizer(object):
    @classmethod
    def load(cls, path, index=None, exc=None, rules=None):
        return cls(index or {}, exc or {}, rules or {})

    def __init__(self, index, exceptions, rules):
        self.index = index
        self.exc = exceptions
        self.rules = rules
</code></pre>

<p>Now, looking at the <code>self.exc</code> for english, we see that it points to <a href=""https://github.com/explosion/spaCy/tree/develop/spacy/lang/en/lemmatizer/__init__.py"" rel=""noreferrer"">https://github.com/explosion/spaCy/tree/develop/spacy/lang/en/lemmatizer/<strong>init</strong>.py</a> where it's loading files from the directory <a href=""https://github.com/explosion/spaCy/tree/master/spacy/en/lemmatizer"" rel=""noreferrer"">https://github.com/explosion/spaCy/tree/master/spacy/en/lemmatizer</a></p>

<h1>Why don't Spacy just read a file?</h1>

<p>Most probably because declaring the string in-code is faster that streaming strings through I/O. </p>

<hr>

<h1>Where does these index, exceptions and rules come from?</h1>

<p>Looking at it closely, they all seem to come from the original Princeton WordNet <a href=""https://wordnet.princeton.edu/man/wndb.5WN.html"" rel=""noreferrer"">https://wordnet.princeton.edu/man/wndb.5WN.html</a> </p>

<p><strong>Rules</strong></p>

<p>Looking at it even closer, the rules on <a href=""https://github.com/explosion/spaCy/tree/develop/spacy/lang/en/lemmatizer/_lemma_rules.py"" rel=""noreferrer"">https://github.com/explosion/spaCy/tree/develop/spacy/lang/en/lemmatizer/_lemma_rules.py</a> is similar to the <code>_morphy</code> rules from <code>nltk</code> <a href=""https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/wordnet.py#L1749"" rel=""noreferrer"">https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/wordnet.py#L1749</a></p>

<p>And these rules originally comes from the <code>Morphy</code> software <a href=""https://wordnet.princeton.edu/man/morphy.7WN.html"" rel=""noreferrer"">https://wordnet.princeton.edu/man/morphy.7WN.html</a> </p>

<p>Additionally, <code>spacy</code> had included some punctuation rules that isn't from Princeton Morphy: </p>

<pre><code>PUNCT_RULES = [
    [""‚Äú"", ""\""""],
    [""‚Äù"", ""\""""],
    [""\u2018"", ""'""],
    [""\u2019"", ""'""]
]
</code></pre>

<p><strong>Exceptions</strong></p>

<p>As for the exceptions, they were stored in the <code>*_irreg.py</code> files in <code>spacy</code>, and they look like they also come from the Princeton Wordnet.</p>

<p>It is evident if we look at some mirror of the original WordNet <code>.exc</code> (exclusion) files (e.g. <a href=""https://github.com/extjwnl/extjwnl-data-wn21/blob/master/src/main/resources/net/sf/extjwnl/data/wordnet/wn21/adj.exc"" rel=""noreferrer"">https://github.com/extjwnl/extjwnl-data-wn21/blob/master/src/main/resources/net/sf/extjwnl/data/wordnet/wn21/adj.exc</a>) and if you download the <code>wordnet</code> package from <code>nltk</code>, we see that it's the same list:</p>

<pre><code>alvas@ubi:~/nltk_data/corpora/wordnet$ ls
adj.exc       cntlist.rev  data.noun  index.adv    index.verb  noun.exc
adv.exc       data.adj     data.verb  index.noun   lexnames    README
citation.bib  data.adv     index.adj  index.sense  LICENSE     verb.exc
alvas@ubi:~/nltk_data/corpora/wordnet$ wc -l adj.exc 
1490 adj.exc
</code></pre>

<p><strong>Index</strong></p>

<p>If we look at the <code>spacy</code> lemmatizer's <code>index</code>, we see that it also comes from Wordnet, e.g. <a href=""https://github.com/explosion/spaCy/tree/develop/spacy/lang/en/lemmatizer/_adjectives.py"" rel=""noreferrer"">https://github.com/explosion/spaCy/tree/develop/spacy/lang/en/lemmatizer/_adjectives.py</a> and the re-distributed copy of wordnet in <code>nltk</code>:</p>

<pre><code>alvas@ubi:~/nltk_data/corpora/wordnet$ head -n40 data.adj 

  1 This software and database is being provided to you, the LICENSEE, by  
  2 Princeton University under the following license.  By obtaining, using  
  3 and/or copying this software and database, you agree that you have  
  4 read, understood, and will comply with these terms and conditions.:  
  5   
  6 Permission to use, copy, modify and distribute this software and  
  7 database and its documentation for any purpose and without fee or  
  8 royalty is hereby granted, provided that you agree to comply with  
  9 the following copyright notice and statements, including the disclaimer,  
  10 and that the same appear on ALL copies of the software, database and  
  11 documentation, including modifications that you make for internal  
  12 use or for distribution.  
  13   
  14 WordNet 3.0 Copyright 2006 by Princeton University.  All rights reserved.  
  15   
  16 THIS SOFTWARE AND DATABASE IS PROVIDED ""AS IS"" AND PRINCETON  
  17 UNIVERSITY MAKES NO REPRESENTATIONS OR WARRANTIES, EXPRESS OR  
  18 IMPLIED.  BY WAY OF EXAMPLE, BUT NOT LIMITATION, PRINCETON  
  19 UNIVERSITY MAKES NO REPRESENTATIONS OR WARRANTIES OF MERCHANT-  
  20 ABILITY OR FITNESS FOR ANY PARTICULAR PURPOSE OR THAT THE USE  
  21 OF THE LICENSED SOFTWARE, DATABASE OR DOCUMENTATION WILL NOT  
  22 INFRINGE ANY THIRD PARTY PATENTS, COPYRIGHTS, TRADEMARKS OR  
  23 OTHER RIGHTS.  
  24   
  25 The name of Princeton University or Princeton may not be used in  
  26 advertising or publicity pertaining to distribution of the software  
  27 and/or database.  Title to copyright in this software, database and  
  28 any associated documentation shall at all times remain with  
  29 Princeton University and LICENSEE agrees to preserve same.  
00001740 00 a 01 able 0 005 = 05200169 n 0000 = 05616246 n 0000 + 05616246 n 0101 + 05200169 n 0101 ! 00002098 a 0101 | (usually followed by `to') having the necessary means or skill or know-how or authority to do something; ""able to swim""; ""she was able to program her computer""; ""we were at last able to buy a car""; ""able to get a grant for the project""  
00002098 00 a 01 unable 0 002 = 05200169 n 0000 ! 00001740 a 0101 | (usually followed by `to') not having the necessary means or skill or know-how; ""unable to get to town without a car""; ""unable to obtain funds""  
00002312 00 a 02 abaxial 0 dorsal 4 002 ;c 06037666 n 0000 ! 00002527 a 0101 | facing away from the axis of an organ or organism; ""the abaxial surface of a leaf is the underside or side facing away from the stem""  
00002527 00 a 02 adaxial 0 ventral 4 002 ;c 06037666 n 0000 ! 00002312 a 0101 | nearest to or facing toward the axis of an organ or organism; ""the upper side of a leaf is known as the adaxial surface""  
00002730 00 a 01 acroscopic 0 002 ;c 06066555 n 0000 ! 00002843 a 0101 | facing or on the side toward the apex  
00002843 00 a 01 basiscopic 0 002 ;c 06066555 n 0000 ! 00002730 a 0101 | facing or on the side toward the base  
00002956 00 a 02 abducent 0 abducting 0 002 ;c 06080522 n 0000 ! 00003131 a 0101 | especially of muscles; drawing away from the midline of the body or from an adjacent part  
00003131 00 a 03 adducent 0 adductive 0 adducting 0 003 ;c 06080522 n 0000 + 01449236 v 0201 ! 00002956 a 0101 | especially of muscles; bringing together or drawing toward the midline of the body or toward an adjacent part  
00003356 00 a 01 nascent 0 005 + 07320302 n 0103 ! 00003939 a 0101 &amp; 00003553 a 0000 &amp; 00003700 a 0000 &amp; 00003829 a 0000 |  being born or beginning; ""the nascent chicks""; ""a nascent insurgency""   
00003553 00 s 02 emergent 0 emerging 0 003 &amp; 00003356 a 0000 + 02625016 v 0102 + 00050693 n 0101 | coming into existence; ""an emergent republic""  
00003700 00 s 01 dissilient 0 002 &amp; 00003356 a 0000 + 07434782 n 0101 | bursting open with force, as do some ripe seed vessels  
</code></pre>

<hr>

<p>On the basis that the dictionary, exceptions and rules that <code>spacy</code> lemmatizer uses is largely from Princeton WordNet and their Morphy software, we can move on to see the actual implementation of how <code>spacy</code> applies the rules using the index and exceptions.</p>

<p>We go back to the <a href=""https://github.com/explosion/spaCy/blob/develop/spacy/lemmatizer.py"" rel=""noreferrer"">https://github.com/explosion/spaCy/blob/develop/spacy/lemmatizer.py</a></p>

<p>The main action comes from the function rather than the <code>Lemmatizer</code> class:</p>

<pre><code>def lemmatize(string, index, exceptions, rules):
    string = string.lower()
    forms = []
    # TODO: Is this correct? See discussion in Issue #435.
    #if string in index:
    #    forms.append(string)
    forms.extend(exceptions.get(string, []))
    oov_forms = []
    for old, new in rules:
        if string.endswith(old):
            form = string[:len(string) - len(old)] + new
            if not form:
                pass
            elif form in index or not form.isalpha():
                forms.append(form)
            else:
                oov_forms.append(form)
    if not forms:
        forms.extend(oov_forms)
    if not forms:
        forms.append(string)
    return set(forms)
</code></pre>

<h1>Why is the <code>lemmatize</code> method outside of the <code>Lemmatizer</code> class?</h1>

<p>That I'm not exactly sure but perhaps, it's to ensure that the lemmatization function can be called outside of a class instance but given that <a href=""https://stackoverflow.com/questions/136097/what-is-the-difference-between-staticmethod-and-classmethod-in-python""><code>@staticmethod</code> and <code>@classmethod</code></a> exist perhaps there are other considerations as to why the function and class has been decoupled </p>

<h1>Morphy vs Spacy</h1>

<p>Comparing <code>spacy</code> lemmatize() function against the <a href=""https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/wordnet.py#L1710"" rel=""noreferrer""><code>morphy()</code></a> function in nltk (which originally comes from <a href=""http://blog.osteele.com/2004/04/pywordnet-20/"" rel=""noreferrer"">http://blog.osteele.com/2004/04/pywordnet-20/</a> created more than a decade ago), <code>morphy()</code>, the main processes in Oliver Steele's Python port of the WordNet morphy are:</p>

<ol>
<li>Check the exception lists</li>
<li>Apply rules once to the input to get y1, y2, y3, etc.</li>
<li>Return all that are in the database (and check the original too)</li>
<li>If there are no matches, keep applying rules until we find a match</li>
<li>Return an empty list if we can't find anything </li>
</ol>

<p>For <code>spacy</code>, possibly, it's still under development, given the <code>TODO</code> at line <a href=""https://github.com/explosion/spaCy/blob/develop/spacy/lemmatizer.py#L76"" rel=""noreferrer"">https://github.com/explosion/spaCy/blob/develop/spacy/lemmatizer.py#L76</a></p>

<p>But the general process seems to be:</p>

<ol>
<li>Look for the exceptions, get them if the lemma from the exception list if the word is in it.</li>
<li>Apply the rules </li>
<li>Save the ones that are in the index lists</li>
<li>If there are no lemma from step 1-3, then just keep track of the Out-of-vocabulary words (OOV) and also append the original string to the lemma forms</li>
<li>Return the lemma forms</li>
</ol>

<p>In terms of OOV handling, spacy returns the original string if no lemmatized form is found, in that respect, the <code>nltk</code> implementation of <code>morphy</code> does the same,e.g. </p>

<pre><code>&gt;&gt;&gt; from nltk.stem import WordNetLemmatizer
&gt;&gt;&gt; wnl = WordNetLemmatizer()
&gt;&gt;&gt; wnl.lemmatize('alvations')
'alvations'
</code></pre>

<h1>Checking for infinitive before lemmatization</h1>

<p>Possibly another point of difference is how <code>morphy</code> and <code>spacy</code> decides what POS to assign to the word. In that respect, <a href=""https://github.com/explosion/spaCy/blob/master/spacy/lemmatizer.py#L35"" rel=""noreferrer""><code>spacy</code> puts some linguistics rule in the <code>Lemmatizer()</code> to decide whether a word is the base form and skips the lemmatization entirely if the word is already in the infinitive form (is_base_form())</a>, this will save quite a bit if lemmatization was to be done for all words in the corpus and quite a chunk of it are infinitives (already the lemma form).</p>

<p>But that's possible in <code>spacy</code> because it allowed the lemmatizer to access the POS that's tied closely to some morphological rules. While for <code>morphy</code> although it's possible to figure out some morphology using the fine-grained PTB POS tags, it still takes some effort to sort them out to know which forms are infinitive.</p>

<p>Generalment, the 3 primary signals of morphology features needs to be teased out in the POS tag:</p>

<ul>
<li>person</li>
<li>number</li>
<li>gender</li>
</ul>

<hr>

<h1>Updated</h1>

<p>SpaCy did make changes to their lemmatizer after the initial answer (12 May 17). I think the purpose was to make the lemmatization faster without look-ups and rules processing. </p>

<p>So they pre-lemmatize words and leave them in a lookup hash-table to make the retrieval O(1) for words that they have pre-lemmatized <a href=""https://github.com/explosion/spaCy/blob/master/spacy/lang/en/lemmatizer/lookup.py"" rel=""noreferrer"">https://github.com/explosion/spaCy/blob/master/spacy/lang/en/lemmatizer/lookup.py</a> </p>

<p>Also, in efforts to unify the lemmatizers across languages, the lemmatizer is now located at <a href=""https://github.com/explosion/spaCy/blob/develop/spacy/lemmatizer.py#L92"" rel=""noreferrer"">https://github.com/explosion/spaCy/blob/develop/spacy/lemmatizer.py#L92</a> </p>

<p>But the underlying lemmatization steps discussed above is still relevant to the current spacy version (<code>4d2d7d586608ddc0bcb2857fb3c2d0d4c151ebfc</code>)</p>

<hr>

<h1>Epilogue</h1>

<p>I guess now that we know it works with linguistics rules and all, the other question is <strong>""are there any non rule-based methods for lemmatization?""</strong></p>

<p>But before even answering the question before, ""What exactly is a lemma?"" might the better question to ask. </p>
",""
"43881704","2017-05-10 00:15:12","2","","43881605","<p>The <code>ast.parse</code> function seems to do almost exactly what you want:</p>

<pre><code>ast.dump(ast.parse(""""""And(                        
                        AND(AND(xyz,wxy),xzy),
                        XOR(x1,y1, AND(xy,zz),OR(x3,y3))  
                        )"""""").body[0].value)
</code></pre>

<blockquote>
  <p><code>Call(func=Name(id='And', ctx=Load()), args=[Call(func=Name(id='AND', ctx=Load()), args=[Call(func=Name(id='AND', ctx=Load()), args=[Name(id='xyz', ctx=Load()), Name(id='wxy', ctx=Load())], keywords=[], starargs=None, kwargs=None), Name(id='xzy', ctx=Load())], keywords=[], starargs=None, kwargs=None), Call(func=Name(id='XOR', ctx=Load()), args=[Name(id='x1', ctx=Load()), Name(id='y1', ctx=Load()), Call(func=Name(id='AND', ctx=Load()), args=[Name(id='xy', ctx=Load()), Name(id='zz', ctx=Load())], keywords=[], starargs=None, kwargs=None), Call(func=Name(id='OR', ctx=Load()), args=[Name(id='x3', ctx=Load()), Name(id='y3', ctx=Load())], keywords=[], starargs=None, kwargs=None)], keywords=[], starargs=None, kwargs=None)], keywords=[], starargs=None, kwargs=None)</code></p>
</blockquote>

<p>(the <code>.body[0].value</code> removes two pointless layers of abstraction, and the <code>.dump</code> is just for output.</p>

<p>Here is the code that then does the transformations you requested on the output:</p>

<pre><code>class Filterer(ast.NodeTransformer):
    def visit_Call(self, node):
            name=node.func.id
            if name == ""OR"" and len(node.args) == 3:
                    return None
            elif name == ""XOR"":
                    args = [ast.Name(""x3"",ast.Load()),
                            ast.Name(""y3"",ast.Load())]
                    func = ast.Name(""OR"",ast.Load())
                    node.args.append(ast.Call(func, args, [], None, None))
            return self.generic_visit(node)
</code></pre>

<p>and here is the code that prints the result in your format with the exception of whitespace: (Python doesn't have a builtin in its <code>ast</code> module for this): </p>

<pre><code>class Printer(ast.NodeVisitor):
    def visit_Call(self, node):
            self.visit(node.func)
            print(""("",end="""")
            comma = False
            for arg in node.args:
                    if comma:
                            print("","",end="""")
                    comma=True
                    self.visit(arg)
            print("")"",end="""")
    def visit_Name(self, node):
            print(node.id,end="""")
</code></pre>

<p>Thus, the final code would be:</p>

<p><code>Printer().visit(Filterer().visit(ast.parse(queryString)))</code></p>
",""
"43880304","2017-05-09 21:40:30","6","","42212423","<p>I'll post my solution since there are no other answers. Let's say we are in the following scenario:</p>

<pre><code>import gensim
from gensim import models
from gensim import corpora
from gensim import similarities
from nltk.tokenize import word_tokenize
import pandas as pd

# routines:
text = ""I work on natural language processing and I want to figure out how does gensim work""
text2 = ""I love computer science and I code in Python""
dat = pd.Series([text,text2])
dat = dat.apply(lambda x: str(x).lower()) 
dat = dat.apply(lambda x: word_tokenize(x))


dictionary = corpora.Dictionary(dat)
corpus = [dictionary.doc2bow(doc) for doc in dat]
tfidf = models.TfidfModel(corpus)
corpus_tfidf = tfidf[corpus]


#Query:
query_text = ""I love icecream and gensim""
query_text = query_text.lower()
query_text = word_tokenize(query_text)
vec_bow = dictionary.doc2bow(query_text)
vec_tfidf = tfidf[vec_bow]
</code></pre>

<p>if we look at:</p>

<pre><code>print(vec_bow)
[(0, 1), (7, 1), (12, 1), (15, 1)]
</code></pre>

<p>and:</p>

<pre><code>print(tfidf[vec_bow])
[(12, 0.7071067811865475), (15, 0.7071067811865475)]
</code></pre>

<p>FYI id and doc:</p>

<pre><code>print(dictionary.items())

[(0, u'and'),
 (1, u'on'),
 (8, u'processing'),
 (3, u'natural'),
 (4, u'figure'),
 (5, u'language'),
 (9, u'how'),
 (7, u'i'),
 (14, u'code'),
 (19, u'in'),
 (2, u'work'),
 (16, u'python'),
 (6, u'to'),
 (10, u'does'),
 (11, u'want'),
 (17, u'science'),
 (15, u'love'),
 (18, u'computer'),
 (12, u'gensim'),
 (13, u'out')]
</code></pre>

<p>Looks like the query only picked up existing terms and using pre-calculated weights to give you the tfidf score. So my workaround is to rebuild the model weekly or daily since it is fast to do so.</p>
",""
"43876491","2017-05-09 17:39:42","0","","43876234","<p>Sounds like you're looking for ""shallow parsing"", or ""chunking"". A chunker might just identify NPs in your text, or just NPs and VPs, etc.  I don't believe the nltk provides a ready to use one, but it's pretty easy to train your own. <a href=""http://www.nltk.org/book/ch07.html#sec-chunking"" rel=""nofollow noreferrer"">Chapter 7</a> of the nltk book provides detailed instructions for how to create or train various types of chunkers. The chunks can even be nested if you want a bit of hierarchical structure.</p>
",""
"43778861","2017-05-04 09:22:50","1","","43768099","<p>If that's really your tagging code, you are tagging each ten-word sentence ten times before you go to the next one. Understand how your tools work before you complain that they are too slow.</p>

<p>You can get a further speedup by calling <code>pos_tag_sents()</code> on your full list of word-tokenized sentences, instead of launching it separately for each sentence (even just once).</p>

<pre><code>tagged_sents = nltk.pos_tag_sents(tokenized_sentences)
</code></pre>
",""
"43719014","2017-05-01 12:20:15","0","","43526587","<p>Since, there's no definitive answer yet, I'm getting the dataframe with all the rows (25 rows of result as in the example above) and inner-joining/merging it with a dataframe that has the list of duplicate pairs (i.e. the 5 rows of output that I need). That way, the resulting dataframe has the similarity scores for the duplicate document pairs. 
This is a temporary solution. If anyone can come up with a cleaner solution, I'll accept that as the answer, if it works.</p>
",""
"43714754","2017-05-01 05:36:53","2","","43596745","<p>The comments in the code should explain everything. Let me know if anything is under specified, and needs more comments. </p>

<p>In short I leverage regex to find the '=' delimiter lines to subdivide the entire text into subsections, then handle each type of sections separately for clarity sake ( so you can tell how I am handling each case). </p>

<p>Side note: I am using the word 'attendee' and 'author' interchangeably.</p>

<p>EDIT: Updated the code to sort based on the '[x]' pattern found right next to the attendee/author in the presentation/QA section. Also changed the pretty print part since pprint does not handle OrderedDict very well. </p>

<p>To strip any additional whitespace including <code>\n</code> anywhere in the string, simply do <code>str.strip()</code>. if you specifically need to strip only <code>\n</code>, then just do <code>str.strip('\n')</code>. </p>

<p>I have modified the code to strip any whitespace in the talks.</p>

<pre><code>import json
import re
from collections import OrderedDict
from pprint import pprint


# Subdivides a collection of lines based on the delimiting regular expression.
# &gt;&gt;&gt; example_string =' =============================
#                       asdfasdfasdf
#                       sdfasdfdfsdfsdf
#                       =============================
#                       asdfsdfasdfasd
#                       =============================
# &gt;&gt;&gt; subdivide(example_string, ""^=+"")
# &gt;&gt;&gt; ['asdfasdfasdf\nsdfasdfdfsdfsdf\n', 'asdfsdfasdfasd\n']
def subdivide(lines, regex):
    equ_pattern = re.compile(regex, re.MULTILINE)
    sections = equ_pattern.split(lines)
    sections = [section.strip('\n') for section in sections]
    return sections


# for processing sections with dashes in them, returns the heading of the section along with
# a dictionary where each key is the subsection's header, and each value is the text in the subsection.
def process_dashed_sections(section):

    subsections = subdivide(section, ""^-+"")
    heading = subsections[0]  # header of the section.
    d = {key: value for key, value in zip(subsections[1::2], subsections[2::2])}
    index_pattern = re.compile(""\[(.+)\]"", re.MULTILINE)

    # sort the dictionary by first capturing the pattern '[x]' and extracting 'x' number.
    # Then this is passed as a compare function to 'sorted' to sort based on 'x'.
    def cmp(d):
        mat = index_pattern.findall(d[0])
        if mat:
            print(mat[0])
            return int(mat[0])
        # There are issues when dealing with subsections containing '-'s but not containing '[x]' pattern.
        # This is just to deal with that small issue.
        else:
            return 0

    o_d = OrderedDict(sorted(d.items(), key=cmp))
    return heading, o_d


# this is to rename the keys of 'd' dictionary to the proper names present in the attendees.
# it searches for the best match for the key in the 'attendees' list, and replaces the corresponding key.
# &gt;&gt;&gt; d = {'mr. man   ceo of company   [1]' : ' This is talk a' ,
#  ...     'ms. woman  ceo of company    [2]' : ' This is talk b'}
# &gt;&gt;&gt; l = ['mr. man', 'ms. woman']
# &gt;&gt;&gt; new_d = assign_attendee(d, l)
# new_d = {'mr. man': 'This is talk a', 'ms. woman': 'This is talk b'}
def assign_attendee(d, attendees):
    new_d = OrderedDict()
    for key, value in d.items():
        a = [a for a in attendees if a in key]
        if len(a) == 1:
            # to strip out any additional whitespace anywhere in the text including '\n'.
            new_d[a[0]] = value.strip()
        elif len(a) == 0:
            # to strip out any additional whitespace anywhere in the text including '\n'.
            new_d[key] = value.strip()
    return new_d


if __name__ == '__main__':
    with open('input.txt', 'r') as input:
        lines = input.read()

        # regex pattern for matching headers of each section
        header_pattern = re.compile(""^.*[^\n]"", re.MULTILINE)

        # regex pattern for matching the sections that contains
        # the list of attendee's (those that start with asterisks )
        ppl_pattern = re.compile(""^(\s+\*)(.+)(\s.*)"", re.MULTILINE)

        # regex pattern for matching sections with subsections in them.
        dash_pattern = re.compile(""^-+"", re.MULTILINE)

        ppl_d = dict()
        talks_d = dict()

        # Step1. Divide the the entire document into sections using the '=' divider
        sections = subdivide(lines, ""^=+"")
        header = []
        print(sections)
        # Step2. Handle each section like a switch case
        for section in sections:

            # Handle headers
            if len(section.split('\n')) == 1:  # likely to match only a header (assuming )
                header = header_pattern.match(section).string

            # Handle attendees/authors
            elif ppl_pattern.match(section):
                ppls = ppl_pattern.findall(section)
                d = {key.strip(): value.strip() for (_, key, value) in ppls}
                ppl_d.update(d)

                # assuming that if the previous section was detected as a header, then this section will relate
                # to that header
                if header:
                    talks_d.update({header: ppl_d})

            # Handle subsections
            elif dash_pattern.findall(section):
                heading, d = process_dashed_sections(section)

                talks_d.update({heading: d})

            # Else its just some random text.
            else:

                # assuming that if the previous section was detected as a header, then this section will relate
                # to that header
                if header:
                    talks_d.update({header: section})

        #pprint(talks_d)
        # To assign the talks material to the appropriate attendee/author. Still works if no match found.
        for key, value in talks_d.items():
            talks_d[key] = assign_attendee(value, ppl_d.keys())

        # ordered dict does not pretty print using 'pprint'. So a small hack to make use of json output to pretty print.
        print(json.dumps(talks_d, indent=4))
</code></pre>
",""
"43658840","2017-04-27 13:09:05","1","","43656078","<p>You could use WordNet hypernyms (synsets with a more general meaning). My first idea would be to go from the current synset upwards (using <code>synset.hypernyms()</code>) and keep checking whether I find the ""sound"" synset. When I hit the root (which has no hypernyms, i.e. <code>synset.hypernyms()</code> returns an empty list), I would stop.</p>

<p>Now for your two examples, this produces the following sequences of synsets:</p>

<pre><code>Sentence:The van rolled along the highway .
Word synset:Synset('scroll.n.02')
[Synset('manuscript.n.02')]
[Synset('autograph.n.01')]
[Synset('writing.n.02')]
[Synset('written_communication.n.01')]
[Synset('communication.n.02')]
[Synset('abstraction.n.06')]
[Synset('entity.n.01')]

Sentence:The thunder rolled and the lightning striked .
Word synset:Synset('paradiddle.n.01')
[Synset('sound.n.04')]
[Synset('happening.n.01')]
[Synset('event.n.01')]
[Synset('psychological_feature.n.01')]
[Synset('abstraction.n.06')]
[Synset('entity.n.01')]
</code></pre>

<p>So one of the synsets you might want to look for is <code>sound.n.04</code>. But there could be others, I think you could play with other examples and try to come up with a list.</p>
",""
"43611313","2017-04-25 12:56:23","1","","43608448","<p>Check the following approach: </p>

<ul>
<li>Build the regex to remove irrelevant words from the results based on the English nltk stopword list</li>
<li>Build the regex to split the text with using the <code>dict_columns</code> keys </li>
<li>After splitting, zip the resulting list into a tuple list</li>
<li>Remove the irrelevant words from the values and strip the whitespace</li>
</ul>

<p>Here is the code I have come so far:</p>

<pre><code>import nltk, re
s = ""the status is processing and product subtypes are HL year 30 ARM and applicant name is Ryan""
dict_columns = {'status': 4, 'product subtypes': 29, 'applicant name': 69}
dict_values = {'processing': 14, 'hl': 50, 'year': 53, '30': 58, 'arm': 61, 'ryan': 87}
# Build the regex to remove irrelevant words from the results
rx_stopwords = r""\b(?:{})\b"".format(""|"".join([x for x in nltk.corpus.stopwords.words(""English"")]))
# Build the regex to split the text with using the dict_columns keys
rx_split = r""\b({})\b"".format(""|"".join([x for x in dict_columns]))
chunks = re.split(rx_split, s)
# After splitting, zip the resulting list into a tuple list
it = iter(chunks[1:])
lst = list(zip(it, it))
# Remove the irrelevant words from the values and trim them (this can be further enhanced
res = [(x, re.sub(rx_stopwords, """", y).strip()) for x, y in lst]
# =&gt;
#   [('status', 'processing'), ('product subtypes', 'HL year 30 ARM'), ('applicant name', 'Ryan')]
# It can be cast to a dictionary
dict(res)
# =&gt; 
#   {'product subtypes': 'HL year 30 ARM', 'status': 'processing', 'applicant name': 'Ryan'}
</code></pre>
",""
"43597699","2017-04-24 21:04:24","0","","43596005","<p>You may extend <a href=""https://stackoverflow.com/a/35109078/3832970"">@ndn's solution</a> to achieve what you need. Note that <code>$before_regexes</code> contain a list of known abbreviations, add those that are present in your corpora. I added <code>qtd</code>  there.</p>

<p>Then, note that <code>$before_regexes</code> and <code>$after_regexes</code> are paired. I added <code>'/(?:[‚Äù‚Äô""\'¬ª])\s*\Z/u'</code> / <code>'/\A(?:\(\p{L})/u'</code> pair and marked it as a non-sentence boundary (with the first <code>false</code>  in the <code>$is_sentence_boundary</code> array. The regex pair means: find a quotation mark (<code>‚Äù‚Äô""'¬ª</code>), 0+ whitespaces, and then followed with <code>(</code> (with <code>\(</code>) and any Unicode letter (<code>\p{L}</code>), then there should be no split.</p>

<pre><code>function sentence_split($text) {
    $before_regexes = array('/(?:[‚Äù‚Äô""\'¬ª])\s*\Z/u',
        '/(?:(?:[\'\""‚Äû][\.!?‚Ä¶][\'\""‚Äù]\s)|(?:[^\.]\s[A-Z]\.\s)|(?:\b(?:St|Gen|Hon|Prof|Dr|Mr|Ms|Mrs|[JS]r|Col|Maj|Brig|Sgt|Capt|Cmnd|Sen|Rev|Rep|Revd)\.\s)|(?:\b(?:St|Gen|Hon|Prof|Dr|Mr|Ms|Mrs|[JS]r|Col|Maj|Brig|Sgt|Capt|Cmnd|Sen|Rev|Rep|Revd)\.\s[A-Z]\.\s)|(?:\bApr\.\s)|(?:\bAug\.\s)|(?:\bBros\.\s)|(?:\bCo\.\s)|(?:\bCorp\.\s)|(?:\bDec\.\s)|(?:\bDist\.\s)|(?:\bFeb\.\s)|(?:\bInc\.\s)|(?:\bJan\.\s)|(?:\bJul\.\s)|(?:\bJun\.\s)|(?:\bMar\.\s)|(?:\bNov\.\s)|(?:\bOct\.\s)|(?:\bPh\.?D\.\s)|(?:\bSept?\.\s)|(?:\b\p{Lu}\.\p{Lu}\.\s)|(?:\b\p{Lu}\.\s\p{Lu}\.\s)|(?:\bcf\.\s)|(?:\be\.g\.\s)|(?:\besp\.\s)|(?:\bet\b\s\bal\.\s)|(?:\bvs\.\s)|(?:\p{Ps}[!?]+\p{Pe} ))\Z/su',
        '/(?:(?:[\.\s]\p{L}{1,2}\.\s))\Z/su',
        '/(?:(?:[\[\(]*\.\.\.[\]\)]* ))\Z/su',
        '/(?:(?:\b(?:pp|[Vv]iz|i\.?\s*e|[Vvol]|[Rr]col|maj|Lt|[Ff]ig|[Ff]igs|[Vv]iz|[Vv]ols|[Aa]pprox|[Ii]ncl|Pres|[Dd]ept|min|max|[Gg]ovt|lb|ft|c\.?\s*f|vs|qtd)\.\s))\Z/su',
        '/(?:(?:\b[Ee]tc\.\s))\Z/su',
        '/(?:(?:[\.!?‚Ä¶]+\p{Pe} )|(?:[\[\(]*‚Ä¶[\]\)]* ))\Z/su',
        '/(?:(?:\b\p{L}\.))\Z/su',
        '/(?:(?:\b\p{L}\.\s))\Z/su',
        '/(?:(?:\b[Ff]igs?\.\s)|(?:\b[nN]o\.\s))\Z/su',
        '/(?:(?:[\""‚Äù\']\s*))\Z/su',
        '/(?:(?:[\.!?‚Ä¶][\x{00BB}\x{2019}\x{201D}\x{203A}\""\'\p{Pe}\x{0002}]*\s)|(?:\r?\n))\Z/su',
        '/(?:(?:[\.!?‚Ä¶][\'\""\x{00BB}\x{2019}\x{201D}\x{203A}\p{Pe}\x{0002}]*))\Z/su',
        '/(?:(?:\s\p{L}[\.!?‚Ä¶]\s))\Z/su');
    $after_regexes = array('/\A(?:\(\p{L})/u',
        '/\A(?:)/su',
        '/\A(?:[\p{N}\p{Ll}])/su',
        '/\A(?:[^\p{Lu}])/su',
        '/\A(?:[^\p{Lu}]|I)/su',
        '/\A(?:[^p{Lu}])/su',
        '/\A(?:\p{Ll})/su',
        '/\A(?:\p{L}\.)/su',
        '/\A(?:\p{L}\.\s)/su',
        '/\A(?:\p{N})/su',
        '/\A(?:\s*\p{Ll})/su',
        '/\A(?:)/su',
        '/\A(?:\p{Lu}[^\p{Lu}])/su',
        '/\A(?:\p{Lu}\p{Ll})/su');
    $is_sentence_boundary = array(false, false, false, false, false, false, false, false, false, false, false, true, true, true);
    $count = 13;

    $sentences = array();
    $sentence = '';
    $before = '';
    $after = substr($text, 0, 10);
    $text = substr($text, 10);

    while($text != '') {
        for($i = 0; $i &lt; $count; $i++) {
            if(preg_match($before_regexes[$i], $before) &amp;&amp; preg_match($after_regexes[$i], $after)) {
                if($is_sentence_boundary[$i]) {
                    array_push($sentences, $sentence);
                    $sentence = '';
                }
                break;
            }
        }

        $first_from_text = $text[0];
        $text = substr($text, 1);
        $first_from_after = $after[0];
        $after = substr($after, 1);
        $before .= $first_from_after;
        $sentence .= $first_from_after;
        $after .= $first_from_text;
    }

    if($sentence != '' &amp;&amp; $after != '') {
        array_push($sentences, $sentence.$after);
    }

    return $sentences;
}
</code></pre>

<p>See the <a href=""http://ideone.com/wO0Fra"" rel=""nofollow noreferrer"">PHP demo</a>.</p>
",""
"43565143","2017-04-22 22:03:02","0","","43558145","<p>You still need to do a lot of tensor operations on the graph to predict something. So GPU still provides performance improvement for inference. Take a look at this <a href=""https://www.nvidia.com/content/tegra/embedded-systems/pdf/jetson_tx1_whitepaper.pdf"" rel=""nofollow noreferrer"">nvidia paper</a>, they have not tested their stuff on TF, but it is still relevant:</p>

<blockquote>
  <p>Our results show that GPUs provide state-of-the-art inference
  performance and energy efficiency, making them the platform of choice
  for anyone wanting to deploy a trained neural network in the field. In
  particular, the Titan X delivers between 5.3 and 6.7 times higher
  performance than the 16-core Xeon E5 CPU while achieving 3.6 to 4.4
  times higher energy efficiency.</p>
</blockquote>

<p>Regarding how to deploy your model, take a look at <a href=""https://tensorflow.github.io/serving/"" rel=""nofollow noreferrer"">TF serving</a></p>
",""
"43510219","2017-04-20 04:29:30","1","","43444261","<p>Please split string:</p>

<pre><code>distance = word2vec_model.wmdistance(""string 1"".split(), ""string 2"".split())
&gt;&gt;&gt; 0.4114476676950455
</code></pre>

<p>Arguments need to be list of strings.</p>
",""
"43473999","2017-04-18 13:44:59","0","","43450538","<p>You are creating a new file for each run of the for loop. Create the file outside the for loop.</p>
",""
"43428146","2017-04-15 15:57:00","2","","43427133","<p>Let the corpus reader take care of the segmentation. If the trees are in Treebank format, this might work by itself:</p>

<pre><code>from nltk.corpus import BracketParseCorpusReader

reader = BracketParseCorpusReader(""path/to/corpus"", r"".*\.tree"")
for sent in reader.parsed_sents():
    print(sent)
</code></pre>

<p>If this doesn't match your tree format, read <a href=""http://www.nltk.org/api/nltk.corpus.reader.html#nltk.corpus.reader.bracket_parse.BracketParseCorpusReader"" rel=""nofollow noreferrer"">the documentation</a> for the options that customize the input.</p>
",""
"43191644","2017-04-03 18:18:19","3","","43191522","<p>You need to use the <code>vectorizer</code>'s <code>vocabulary_</code> attribute, which is a mapping of terms to feature indices.</p>

<pre><code>&gt;&gt;&gt; from sklearn.feature_extraction.text import TfidfVectorizer
&gt;&gt;&gt; docs = []
&gt;&gt;&gt; docs.append(""this is sentence number one"")
&gt;&gt;&gt; docs.append(""this is sentence number two"")
&gt;&gt;&gt; vectorizer = TfidfVectorizer(norm='l2',min_df=0, use_idf=True, smooth_idf=True, stop_words='english', sublinear_tf=True)
&gt;&gt;&gt; x = vectorizer.fit_transform(docs)
&gt;&gt;&gt; x.todense()
matrix([[ 0.70710678,  0.70710678],
        [ 0.70710678,  0.70710678]])
&gt;&gt;&gt; vectorizer.vocabulary_['sentence']
1
&gt;&gt;&gt; c = vectorizer.vocabulary_['sentence']
&gt;&gt;&gt; x[:,c]
&lt;2x1 sparse matrix of type '&lt;class 'numpy.float64'&gt;'
    with 2 stored elements in Compressed Sparse Row format&gt;
&gt;&gt;&gt; x[:,c].todense()
matrix([[ 0.70710678],
        [ 0.70710678]])
</code></pre>
",""
"43171638","2017-04-02 17:59:45","0","","43171573","<p><a href=""http://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec"" rel=""nofollow noreferrer"">From the gensim documentation:</a></p>

<blockquote>
  <p><code>sg</code> defines the training algorithm. By default (<code>sg=0</code>), CBOW is used. Otherwise (<code>sg=1</code>), skip-gram is employed.</p>
</blockquote>
",""
"43151568","2017-04-01 00:48:32","5","","43149878","<p>The transformers package (currently at version 0.5.4.0) implements <code>MonadTrans</code> using <code>liftM</code>:</p>

<pre><code>lift :: Monad m =&gt; m a -&gt; MaybeT m a
lift = MaybeT . liftM Just
</code></pre>

<p>where <code>liftM</code> is a combinator defined as</p>

<pre><code>liftM :: Monad m =&gt; (a -&gt; b) -&gt; m a -&gt; m b
liftM f m = m &gt;&gt;= \a -&gt; return (f a)
</code></pre>

<p>Furthermore, <code>return</code> is defined for <code>MaybeT</code> as</p>

<pre><code>return :: Monad m =&gt; a -&gt; MaybeT m a
return a = lift . return
</code></pre>

<p>Reduce the definition:</p>

<pre><code>return :: Monad m =&gt; a -&gt; MaybeT m a
return a = MaybeT (return a &gt;&gt;= \a -&gt; return (Just a))
</code></pre>

<p>where the two inner <code>return</code> are instantiated at type <code>m</code>.</p>

<p>One call to <code>return @(MaybeT m)</code> calls <code>return @m</code> twice, hence the exponential behavior you observe for a tower of <code>MaybeT</code>.</p>

<p>This is fixable by using <code>fmap</code> instead of <code>liftM</code>, but this is backwards incompatible, when <code>Functor</code> was not a superclass of <code>Monad</code>.</p>

<p><strong>EDIT:</strong> Other transformers do not have this issue, as <code>return</code> is not defined using <code>lift</code>, which provides an even better fix.</p>

<pre><code>return = MaybeT . return . Just
</code></pre>

<hr>

<p>Here is a more minimal test case:</p>

<pre><code>{-# LANGUAGE RankNTypes, ScopedTypeVariables #-}
import Control.Monad.Trans.Maybe
import System.Environment

f :: forall m proxy. Monad m =&gt; proxy m -&gt; Int -&gt; ()
f _ 0 = (return () :: m ()) `seq` ()
f _ n = f (undefined :: proxy (MaybeT m)) (n - 1)

main = do
  n : _ &lt;- getArgs
  f (undefined :: proxy []) (read n) `seq` return ()
</code></pre>

<p>Output</p>

<pre><code>&gt; for i in {18..21} ; time ./b $i
./b $i  0.35s user 0.04s system 99% cpu 0.390 total
./b $i  0.71s user 0.07s system 99% cpu 0.782 total
./b $i  1.38s user 0.18s system 99% cpu 1.565 total
./b $i  2.82s user 0.32s system 100% cpu 3.139 total
</code></pre>
",""
"43150816","2017-03-31 22:59:02","0","","43150019","<p>Using regex with <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.findall.html"" rel=""noreferrer""><code>.str.findall</code></a> and <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.join.html"" rel=""noreferrer""><code>.str.join</code></a> appears to be fastest:</p>

<pre><code>df['text'].str.findall('\w{4,}').str.join(' ')
</code></pre>

<p><strong>Timings</strong></p>

<p>Using the following setup:</p>

<pre><code>df = pd.DataFrame({'text':[""The quick brown fox"", ""jumped over the lazy dog"", ""foo bar baz"", 'words exceeding desired length']})
df = pd.concat([df]*10**4, ignore_index=True)

def pir2(df):
    t = df.text.str.split(expand=True).stack()
    return t.loc[t.str.len() &gt;= 4].groupby(level=0).apply(' '.join)
</code></pre>

<p>I get the following timings:</p>

<pre><code>%timeit df['text'].str.findall('\w{4,}').str.join(' ')
10 loops, best of 3: 44.8 ms per loop

%timeit df.text.apply(lambda i: ' '.join(filter(lambda j: len(j) &gt; 3, i.split())))
10 loops, best of 3: 79.3 ms per loop

%timeit df['text'].str.split().map(lambda sl: "" "".join(s for s in sl if len(s) &gt; 3))
10 loops, best of 3: 87.2 ms per loop

%timeit pir2(df)
1 loop, best of 3: 2.87 s per loop
</code></pre>
",""
"43137081","2017-03-31 09:23:46","0","","43136202","<p>You are using the default token_pattern, which only selects tokens of 2 or more char.</p>

<blockquote>
  <p>token_pattern : </p>
  
  <p>‚Äútoken‚Äù, only used if analyzer == 'word'. The default regexp selects
  tokens of 2 or more alphanumeric characters (punctuation is completely
  ignored and always treated as a token separator)</p>
</blockquote>

<p>If you define a new token_pattern, you will get the 'a' character, e.g.:</p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
sent=[""This is a sample"", ""This is another example""]
tf = TfidfVectorizer(token_pattern=u'(?u)\\b\\w+\\b')
tfidf_matrix =  tf.fit_transform(sent)
print tfidf_matrix.toarray()
tf.vocabulary_
</code></pre>

<p>[[ 0.57615236  0.          0.          0.40993715  0.57615236  0.40993715]
 [ 0.          0.57615236  0.57615236  0.40993715  0.          0.40993715]]</p>

<pre><code>tf.vocabulary_
</code></pre>

<p>{u'a': 0, u'sample': 4, u'another': 1, u'this': 5, u'is': 3, u'example': 2}</p>
",""
"43078178","2017-03-28 19:21:52","2","","43074949","<p>I'm not sure exactly what the question is here, but I assume you're asking how to load the binary into your Python app? You can use <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">gensim</a> for example which has built-in tools to decode the binary:</p>

<pre><code>from gensim.models.keyedvectors import KeyedVectors
model = KeyedVectors.load_word2vec_format('google_news.bin', binary=True)
print(model['the']) # this prints the 300D vector for the word 'the'
</code></pre>

<p><strong>EDIT</strong></p>

<p>I feel your question is more about binary files in general? This does not seem related to word2vec specifically. Anyways, in a word2vec binary file each line is a pair of word and weights in binary format. First the word is decoded into a string by looping the characters until it meets the binary character for ""space"". Then the rest is decoded from binary into floats. We know the number of floats since word2vec binary files have a header, such as ""3000000 300"", which tells us there are 3m words, each word is a 300D vector.</p>

<p>A binary file is organized as a series of bytes, each 8 bits. Read more about binary on the <a href=""https://en.wikipedia.org/wiki/Binary_file"" rel=""nofollow noreferrer"">wiki page</a>. </p>

<p>The number 0.0056 in decimal format, becomes in binary:</p>

<pre><code>00111011 10110111 10000000 00110100
</code></pre>

<p>So here there are 4 bytes that make up a float. How do we know this? Because we assume the binary encodes 32 bit float.</p>

<p>What if the binary file represents 64 bit precision floats? Then the decimal 0.0056 in binary becomes:</p>

<pre><code>00111111 01110110 11110000 00000110 10001101 10111000 10111010 11000111
</code></pre>

<p>Yes, twice the length because twice the precision. So when we decode the word2vec file, if the weights are 300d, and 64 bit encoding, then there should be 8 bytes to represent each number. So a word embedding would have 300*64=19,200 binary digits in each line of the file. Get it?</p>

<p>You can google ""how binary digits"" work, millions of examples.</p>
",""
"43038238","2017-03-27 04:57:57","0","","43037697","<p>I got the answer to this question. Instead of using <code>fromstring</code> method, generate PCFG object by passing a list of <code>nltk.ProbabilisticProduction</code> objects and an <code>nltk.Nonterminal</code> object as below:  </p>

<pre><code>from nltk import ProbabilisticProduction 
from nltk.grammar import PCFG
from nltk import  Nonterminal as NT

g = ProbabilisticProduction(NT('TS'), [NT('.'), NT('NT6')], prob=1)

# Adding a terminal production
g = ProbabilisticProduction(NT('NT6'), ['terminal'], prob = 1)

start = NT('Q0')  # Q0 is the start symbol for my grammar
PCFG(start, [g]) # Takes a list of ProbabilisticProductions
</code></pre>
",""
"43033034","2017-03-26 18:45:31","2","","43022047","<p>I'm not actually sure they're represented explicitly anywhere in the code. The tagger simply outputs them as Strings rather than any sort of fixed enum, and the output space is inferred directly from the training data. The advantage of this being that you can train the exact same model on arbitrary tag sets. And of course the disadvantage you've just run into. :)</p>

<p>However, for English, the tag set should be the Penn Treebank tag set: <a href=""https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"" rel=""nofollow noreferrer"">https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html</a></p>
",""
"43028769","2017-03-26 12:34:43","1","","43026129","<p>Have a look at the <a href=""https://github.com/trinker/textstem#lemmatizing"" rel=""noreferrer""><strong>textstem</strong> package</a> I maintain:</p>

<pre><code>if (!require(""pacman"")) install.packages(""pacman"")
pacman::p_load(textstem)

lemmatize_words(c(""ran"",""run"",""running""))
###[1] ""run"" ""run"" ""run""
</code></pre>

<p>Note that if you actually have strings and not word vectors you may want the <code>lemmatize_strings</code> function instead.</p>
",""
"42966837","2017-03-23 03:39:48","1","","42966067","<p>Always write the CFG grammar in bite-size, see <a href=""https://stackoverflow.com/questions/20983494/python-and-nltk-how-to-analyze-sentence-grammar"">Python and NLTK: How to analyze sentence grammar?</a></p>

<p><strong>Let's try to handle <code>describe your work</code> first.</strong></p>

<pre><code>import nltk

your_grammar = nltk.CFG.fromstring(""""""
S -&gt; V NP
V -&gt; 'describe' | 'present'
NP -&gt; PRP N 
PRP -&gt; 'your' 
N -&gt; 'work'
"""""")

parser = nltk.ChartParser(your_grammar)
sent = 'describe your work'.split()
print (list(parser.parse(sent)))
</code></pre>

<p>[out]:</p>

<pre><code>[Tree('S', [Tree('V', ['describe']), Tree('NP', [Tree('PRP', ['your']), Tree('N', ['work'])])])]
</code></pre>

<p><strong>Now let's try <code>describe every step of your work</code>:</strong></p>

<pre><code>import nltk

your_grammar = nltk.CFG.fromstring(""""""
S -&gt; V NP
V -&gt; 'describe' | 'present'
NP -&gt; PRP N | DT N PP 
PRP -&gt; 'your' 
N -&gt; 'work' | 'step'
PP -&gt; P NP
P -&gt; 'of' 
DT -&gt; 'every'
"""""")

parser = nltk.ChartParser(your_grammar)
sent = 'describe every step of your work'.split()
print (list(parser.parse(sent)))
</code></pre>

<p>[out]:</p>

<pre><code>[Tree('S', [Tree('V', ['describe']), Tree('NP', [Tree('DT', ['every']), Tree('N', ['step']), Tree('PP', [Tree('P', ['of']), Tree('NP', [Tree('PRP', ['your']), Tree('N', ['work'])])])])])]
</code></pre>

<p><strong>Now let's try <code>present final results in a Word Document</code>:</strong></p>

<pre><code>import nltk

your_grammar = nltk.CFG.fromstring(""""""
S -&gt; V NP
V -&gt; 'describe' | 'present'
NP -&gt; PRP N | DT N PP | DT N | ADJ N PP
PRP -&gt; 'your' 
N -&gt; 'work' | 'step' | 'results' | 'Word_Document'
PP -&gt; P NP
P -&gt; 'of' | 'in'
DT -&gt; 'every' | 'a'
ADJ -&gt; 'final'
"""""")

parser = nltk.ChartParser(your_grammar)
#sent = 'describe every step of your work'.split()
sent = 'present final results in a Word_Document'.split()
print (list(parser.parse(sent)))
</code></pre>

<p>[out]:</p>

<pre><code>[Tree('S', [Tree('V', ['present']), Tree('NP', [Tree('ADJ', ['final']), Tree('N', ['results']), Tree('PP', [Tree('P', ['in']), Tree('NP', [Tree('DT', ['a']), Tree('N', ['Word_Document'])])])])])]
</code></pre>

<p><strong>Now, let's add <code>NP -&gt; DT NP</code> for <code>present all final results in a Word Document</code>:</strong></p>

<pre><code>import nltk

your_grammar = nltk.CFG.fromstring(""""""
S -&gt; V NP
V -&gt; 'describe' | 'present'
NP -&gt; PRP N | DT N PP | DT N | ADJ N PP | DT NP
PRP -&gt; 'your'
N -&gt; 'work' | 'step' | 'results' | 'Word_Document'
PP -&gt; P NP
P -&gt; 'of' | 'in'
DT -&gt; 'every' | 'a' | 'all'
ADJ -&gt; 'final'
"""""")

parser = nltk.ChartParser(your_grammar)
#sent = 'describe every step of your work'.split()
sent = 'present all final results in a Word_Document'.split()
print (list(parser.parse(sent)))
</code></pre>

<p>[out]:</p>

<pre><code>[Tree('S', [Tree('V', ['present']), Tree('NP', [Tree('DT', ['all']), Tree('NP', [Tree('ADJ', ['final']), Tree('N', ['results']), Tree('PP', [Tree('P', ['in']), Tree('NP', [Tree('DT', ['a']), Tree('N', ['Word_Document'])])])])])])]
</code></pre>

<p><strong>Now let's go for the conjunctions for <code>present all intermediate and final results in a Word_Document</code>:</strong></p>

<pre><code>import nltk

your_grammar = nltk.CFG.fromstring(""""""
S -&gt; V NP
V -&gt; 'describe' | 'present'
NP -&gt; PRP N | DT N PP | DT N | ADJ N PP | DT NP
PRP -&gt; 'your'
N -&gt; 'work' | 'step' | 'results' | 'Word_Document'
PP -&gt; P NP
P -&gt; 'of' | 'in'
DT -&gt; 'every' | 'a' | 'all'
ADJ -&gt; 'final' | 'intermediate' | ADJ CONJ ADJ
CONJ -&gt; 'and'
"""""")

parser = nltk.ChartParser(your_grammar)
#sent = 'describe every step of your work'.split()
sent = 'present all intermediate and final results in a Word_Document'.split()
print (list(parser.parse(sent)))
</code></pre>

<p>[out]:</p>

<pre><code>[Tree('S', [Tree('V', ['present']), Tree('NP', [Tree('DT', ['all']), Tree('NP', [Tree('ADJ', [Tree('ADJ', ['intermediate']), Tree('CONJ', ['and']), Tree('ADJ', ['final'])]), Tree('N', ['results']), Tree('PP', [Tree('P', ['in']), Tree('NP', [Tree('DT', ['a']), Tree('N', ['Word_Document'])])])])])])]
</code></pre>

<p>But that only give you one reading <code>present all [(intermediate and final) (results) (in a Word_Document)]</code>. For ambiguous results, I'll leave it to your imagination ;P</p>

<p><strong>Now let's move on and concatenate the <code>S -&gt; S CONJ S</code> for <code>describe your work and present all intermediate and final results in a Word_Document</code>:</strong></p>

<pre><code>import nltk

your_grammar = nltk.CFG.fromstring(""""""
S -&gt; V NP | S CONJ S
V -&gt; 'describe' | 'present'
NP -&gt; PRP N | DT N PP | DT N | ADJ N PP | DT NP
PRP -&gt; 'your' 
N -&gt; 'work' | 'step' | 'results' | 'Word_Document'
PP -&gt; P NP
P -&gt; 'of' | 'in'
DT -&gt; 'every' | 'a' | 'all'
ADJ -&gt; 'final' | 'intermediate' | ADJ CONJ ADJ
CONJ -&gt; 'and'
"""""")

parser = nltk.ChartParser(your_grammar)
sent1 = 'describe every step of your work'
sent2 = 'present all intermediate and final results in a Word_Document'
sent = ' and '.join([sent1, sent2]).split()
print (list(parser.parse(sent)))
</code></pre>

<p>[out]:</p>

<pre><code>[Tree('S', [Tree('S', [Tree('V', ['describe']), Tree('NP', [Tree('DT', ['every']), Tree('N', ['step']), Tree('PP', [Tree('P', ['of']), Tree('NP', [Tree('PRP', ['your']), Tree('N', ['work'])])])])]), Tree('CONJ', ['and']), Tree('S', [Tree('V', ['present']), Tree('NP', [Tree('DT', ['all']), Tree('NP', [Tree('ADJ', [Tree('ADJ', ['intermediate']), Tree('CONJ', ['and']), Tree('ADJ', ['final'])]), Tree('N', ['results']), Tree('PP', [Tree('P', ['in']), Tree('NP', [Tree('DT', ['a']), Tree('N', ['Word_Document'])])])])])])])]
</code></pre>

<p>There're surely other ways to write the CFG grammar to suit your sentence and this is just one of the many ways. But in general, write the CFG grammar in bitesize.</p>
",""
"42933250","2017-03-21 16:42:04","0","","42933127","<p>The nltk's CFG is a teaching tool, not a serious NLP framework. It does not provide support for any kind of open-vocabulary items (which will inevitably result in unknown words, including unknown numbers). Your best option is to pre-process your input, e.g. replace all numerals with a fixed number (or a symbol like <code>#NUM#</code>) that is recognized by your grammar. Then if you need the original numerals, you can post-process the parser results and put them back in.</p>
",""
"42868773","2017-03-17 23:49:14","0","","42865623","<p><strong>In short:</strong></p>

<pre><code>' '.join([word + '/' + pos for word, pos in tagged_sent]
</code></pre>

<p><strong>In long:</strong></p>

<p>I think you're overthinking about using string functions to concat the strings, it's really not that expensive.</p>

<pre><code>import time
from nltk.corpus import brown

tagged_corpus = brown.tagged_sents()

start = time.time()

with open('output.txt', 'w') as fout:
    for i, sent in enumerate(tagged_corpus):
        print(' '.join([word + '/' + pos for word, pos in sent]), end='\n', file=fout)

end = time.time() - start
print (i, end)
</code></pre>

<p>It took 2.955 seconds on my laptop for all 57339 sentences from the brown corpus.</p>

<p>[out]:</p>

<pre><code>$ head -n1 output.txt 
The/AT Fulton/NP-TL County/NN-TL Grand/JJ-TL Jury/NN-TL said/VBD Friday/NR an/AT investigation/NN of/IN Atlanta's/NP$ recent/JJ primary/NN election/NN produced/VBD ``/`` no/AT evidence/NN ''/'' that/CS any/DTI irregularities/NNS took/VBD place/NN ./.
</code></pre>

<p>But using string to concatenate the word and POS can cause trouble later on when you need to read your tagged output, e.g.</p>

<pre><code>&gt;&gt;&gt; from nltk import pos_tag
&gt;&gt;&gt; tagged_sent = pos_tag('cat / dog'.split())
&gt;&gt;&gt; tagged_sent_str = ' '.join([word + '/' + pos for word, pos in tagged_sent])
&gt;&gt;&gt; tagged_sent_str
'cat/NN //CD dog/NN'
&gt;&gt;&gt; [tuple(wordpos.split('/')) for wordpos in tagged_sent_str.split()]
[('cat', 'NN'), ('', '', 'CD'), ('dog', 'NN')]
</code></pre>

<p>If you want to saved the tagged output and then read it later, it's better to use <code>pickle</code> to save the tagged_output, e.g. </p>

<pre><code>&gt;&gt;&gt; import pickle
&gt;&gt;&gt; tagged_sent = pos_tag('cat / dog'.split())
&gt;&gt;&gt; with open('tagged_sent.pkl', 'wb') as fout:
...     pickle.dump(tagged_sent, fout)
... 
&gt;&gt;&gt; tagged_sent = None
&gt;&gt;&gt; tagged_sent
&gt;&gt;&gt; with open('tagged_sent.pkl', 'rb') as fin:
...     tagged_sent = pickle.load(fin)
... 
&gt;&gt;&gt; tagged_sent
[('cat', 'NN'), ('/', 'CD'), ('dog', 'NN')]
</code></pre>
",""
"42841898","2017-03-16 18:16:22","2","","42835852","<p>Part of speech is important for lemmatisation to work, as words which have different meanings depending on part of speech. And using this information, lemmatization will return the base form or lemma. So, it would be better if POS Tagging implementation is done first.</p>

<p>The main idea behind lemmatisation is to group different inflected forms of a word into one. For example, <em>go, going, gone</em> and <em>went</em> will become just one - <em>go</em>. But to derive this, lemmatisation would have to know the context of a word - whether the word is a noun or verb etc. </p>

<p>So, the lemmatisation function can take the word and the part of speech as input and return the lemma after processing the information.</p>
",""
"42638413","2017-03-07 01:07:11","2","","42638034","<p>You can use <a href=""http://polyglot.readthedocs.io/en/latest/Detection.html"" rel=""nofollow noreferrer"">ployglot</a> package's language detection feature.</p>

<pre><code>&gt;&gt;&gt; from polyglot.detect import Detector
&gt;&gt;&gt; print(Detector('today is a good day.').language)
name: English     code: en       confidence:  95.0 read bytes:  1792
</code></pre>
",""
"42589502","2017-03-03 22:25:25","0","","42513179","<p>I don't think it matters.  The code should just load all of the data in, it's just for convenience if you have it split into multiple files.  Also, you can specify different input formats for different files, but this is not going to affect the final model.</p>
",""
"42575018","2017-03-03 09:23:13","2","","42570593","<p>You get an error because you pass <code>cp.parse()</code> a list of sentences, not a list of tagged tokens. You don't show where <code>concat</code> comes from, but clearly (as @lenz commented) it concatenates the sentences into a single list of words. To do the same in the second case, you'd need <code>cp.parse(concat(tag2))</code>.</p>

<p>However, this is incorrect unless you have a very unusual grammar. Parsers work on one sentence at a time, so you should keep your sentences separate, not concatenate them together. Either iterate over your lists of sentences and parse each one, or parse all the tagged sentences at once with <code>cp.parse_sents(tag2)</code>. The same applies to the self-tagged treebank sentences, which should have been tagged and parsed like this:</p>

<pre><code>tbss = treebank.sents()[200:220]
tag1 = nltk.pos_tag_sents(tbss)
parsed1 = cp.parse_sents(tag1)
for sent in parsed1:
    print(sent)
</code></pre>
",""
"42452319","2017-02-25 05:27:32","0","","42452205","<p>No need to run a loop, indeed. At least not an explicit loop. A list comprehension will work just fine.</p>

<p>Assuming you use Porter stemmer <code>ps</code>:</p>

<pre><code>df['stems'] = df['tokenized'].apply(lambda words: 
                                    [ps.stem(word) for word in words])
</code></pre>
",""
"42442303","2017-02-24 15:25:08","0","","42441257","<p>Ok i found the solution for first problem. You don't need XPOSTAG, duplicating UPOSTAG will allow training. my problem was that no word or punctuation mark, ""?"" in the question, can be left blank.it has to be pos tagged and must be made dependent on the root. It solved my issues.</p>

<p>In case of the second question the answer is ambiguous. There is no valid one to one relationship between UPOSTAG and XPOSTAG as it is language dependent. Any table using the Penn Tree Bank tags will work. But will need post-processing for accuracy.</p>
",""
"42384138","2017-02-22 06:24:29","0","","42382446","<p>Yes, you are correct. You can consider weight CKY is equivalent to Viterbi for parsing. You can see the lecture on Viterbi and Statistical parsing with PCFG from <a href=""http://www.cs.virginia.edu/~kc2wc/teaching/NLP16/syllabus.html"" rel=""nofollow noreferrer"">here</a>. However, Viterbi algorithm can be used to find the most likely sequence of hidden states and probabilistic CYK algorithm is specifically designed for tagging/parsing.</p>
",""
"42303030","2017-02-17 16:36:40","1","","42282254","<p>If your corpus is as simple as you show, and you don't really need to create sentences, you can just compute the unigrams.  If it is more complex, run a form of topic modeling.  Topic modeling will return the words common across the corpus.  You will need to have your corpus in a set of documents.  In your case each 'document' could be a sentence.  A good topic modeling algorithm is called ""Latent Dirichlet Allocation"" (LDA).  </p>

<p>For a technical paper on LDA see <a href=""http://ai.stanford.edu/~ang/papers/nips01-lda.pdf"" rel=""nofollow noreferrer"">Latent Dirichlet Allocation</a>.</p>

<p>For an article with sample Python code using the gensim library see <a href=""https://radimrehurek.com/gensim/wiki.html"" rel=""nofollow noreferrer"">Experiments on the English Wikipedia</a>.  </p>

<p>The following article and sample code by Jordan Barber, <a href=""https://rstudio-pubs-static.s3.amazonaws.com/79360_850b2a69980c4488b1db95987a24867a.html"" rel=""nofollow noreferrer"">Latent Dirichlet Allocation (LDA) with Python</a>, uses NLTK to create a corpus and gensim for LDA.  This code is more adaptable to other applications than the Wikipedia code.</p>
",""
"42246116","2017-02-15 10:01:39","0","","32740104","<p>I was unable to find something like this so I set out to create it. The result is called <a href=""https://github.com/spro/nalgene"" rel=""nofollow noreferrer"">Nalgene - the natural language generation language</a>. The syntax is fairly simple but also powerful enough to support recursive phrases, synonyms, captured values, and dependencies.</p>

<pre><code>%
    $person.name went to the $place to %action

%action
    ~buy a new $item
    ~sell @posessive($person.gender) $item

~buy
    buy
    purchase

$place
    store
    market

...
</code></pre>

<p>It outputs generated sentences alongside a tree representation (the primary purpose is to generate training data for ML systems).</p>

<pre><code>&gt; jill went to the store to return her toothbrush

( %
    ( $person.name
        jill )
    ( $place
        store )
    ( %action
        ( $item
            toothbrush ) ) )
</code></pre>

<p>If you are still looking a year later, stop by and open an issue, and let me know what you seek in a dream NLG language.</p>
",""
"42227319","2017-02-14 13:24:05","1","","42220764","<p>Yes, it returns you a <code>tf</code> - term frequency (you had both term frequency for this field, and ttf - which is total term frequency, e.g. sum of all tf's across all fields) and <code>df</code> - document frequency (you also had it in the response). You need to decide which tf-idf you want to calculate across only your field, or all fields. To compute tf-idf you need to do the following:</p>

<pre><code>tf-idf = tf * idf
</code></pre>

<p>where </p>

<pre><code>idf = log (N / df)
</code></pre>

<p>and <code>N = doc_count</code> from your response. Elasticsearch do not provide implementation for calculating tf-idf, so you need to do it by yourself.</p>
",""
"42215940","2017-02-14 00:35:00","1","","42194446","<p>Just in case people find this later, I was able to set up my rules to pass in correct lambda values for coordinated structures: </p>

<pre><code>NP[SEM=&lt;?conj(?p, ?q)&gt;] -&gt; N[SEM=?p] CONJ[SEM=?conj] N[SEM=?q]
CONJ[SEM=&lt;\Q \P \x.(Q(x) | P(x))&gt;] -&gt; 'or'
</code></pre>

<p>This should work for both NPs and VPs with lambdas.</p>
",""
"42207736","2017-02-13 15:31:39","0","","42206539","<p>(Horribly hacky) 
For your example strings and tags:</p>

<pre><code>s = ('The', 'DT'), ('stocks', 'NNS'), ('show', 'VBP'), ('67', 'CD'), ('%', 'NN'), ('rise', 'NN'), (',', ','), ('last', 'JJ'), ('year', 'NN'), ('it', 'PRP'), ('was', 'VBD'), ('12', 'CD'), ('%', 'NN'), ('fall', 'NN')
a = (('67', 'CD'), ('%', 'NN'), ('rise', 'NN'))
c = 'The stocks show 67% rise, last year it was 12% fall'
</code></pre>

<p>Edit: As a list comprehension:</p>

<pre><code>&gt;&gt;&gt;c[min((c.index(i[0]) for i in a)):max((c.index(i[0]) for i in a)) + [len(i[0]) for i in a][-1]]
&gt;&gt;&gt;'67% rise'
</code></pre>

<p>Find the position that each word occurs within your input sentence. Record the length of each word.</p>

<p>Check what position your desired part of speech tags have within your sample sentence.
(Edit: Removed if as was unnecessary)</p>

<pre><code>position=[]
lengths=[]

for wordtag in a:
    print wordtag,c.index(i[0]),wordtag[0],len(wordtag[0])
    position.append(c.index(wordtag[0]))
    lengths.append(len(wordtag[0]))

&gt; ('67', 'CD') 16 67 2
&gt; ('%', 'NN') 18 % 1
&gt; ('rise', 'NN') 20 rise 4

print position
print lengths

&gt; [16, 18, 20]
&gt; [2, 1, 4]
</code></pre>

<p>Slice your input sentence according to the minimum and maximum position of your desired tags. You add a <code>lengths[-1]</code> to add the length of the word <code>rise</code>.</p>

<pre><code>valid = c[min(position):max(position) + lengths[-1]]
print valid

&gt; [16, 18, 20]
&gt; [2, 1, 4]

&gt; 67% rise
</code></pre>

<p>You can then generalise this for any list of sentences and part of speech tags.</p>
",""
"42207722","2017-02-13 15:30:48","2","","42160954","<p>At this moment there are no French POS-taggers implemented in R.</p>
",""
"42206289","2017-02-13 14:20:24","0","","42204887","<p>Your discovery is correct.  On this <a href=""https://cloud.google.com/natural-language/docs/"" rel=""nofollow noreferrer"">page</a> it states ""The Cloud Natural Language API currently supports English, Spanish, and Japanese for sentiment analysis, entity analysis, and syntax analysis.""</p>

<p>The ""Google Cloud Natural Language API"" web page states under multi-lingual support: ""<em>Combine the API with our Google Cloud Speech API and extract insights from audio conversations</em>"".  The languages supported by the ""Google Cloud Speech API"" service are listed <a href=""https://cloud.google.com/speech/docs/languages"" rel=""nofollow noreferrer"">here</a>.  Russian, Polish, and Italian are supported.</p>

<p>Apparently you can use the Speech API with eighty-nine languages but only use the Natural Language API on three languages!</p>
",""
"42089742","2017-02-07 12:18:55","5","","42089517","<p>Probably the most quick and dirty:</p>

<pre><code>import nltk
words = set(nltk.corpus.words.words())
for old in 'sittest walkest liest risest'.split():
    new = old[:-2]
    while new and new not in words:
        new = new[:-1]
    print(old, new)
</code></pre>

<p>Output:</p>

<pre><code>sittest sit
walkest walk
liest lie
risest rise
</code></pre>

<p>UPDATE. A slightly less quick and dirty (works e.g. for <code>rotest</code> ‚Üí verb <code>rot</code>, not noun <code>rote</code>):</p>

<pre><code>from nltk.corpus import wordnet as wn
for old in 'sittest walkest liest risest rotest'.split():
    new = old[:-2]
    while new and not wn.synsets(new, pos='v'):
        new = new[:-1]
    print(old, new)
</code></pre>

<p>Output:</p>

<pre><code>sittest sit
walkest walk
liest lie
risest rise
rotest rot
</code></pre>
",""
"42081746","2017-02-07 04:29:31","2","","41674573","<p><strong>Input</strong></p>

<pre><code>$ cat test.csv 
ID,Task,label,Text
1,Collect Information,no response,cozily married practical athletics Mr. Brown flat
2,New Credit,no response,active married expensive soccer Mr. Chang flat
3,Collect Information,response,healthy single expensive badminton Mrs. Green flat
4,Collect Information,response,cozily married practical soccer Mr. Brown hierachical
5,Collect Information,response,cozily single practical badminton Mr. Brown flat
</code></pre>

<p><strong>TL;DR</strong></p>

<pre><code>&gt;&gt;&gt; from nltk import word_tokenize, pos_tag, pos_tag_sents
&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; df = pd.read_csv('test.csv', sep=',')
&gt;&gt;&gt; df['Text']
0    cozily married practical athletics Mr. Brown flat
1       active married expensive soccer Mr. Chang flat
2    healthy single expensive badminton Mrs. Green ...
3    cozily married practical soccer Mr. Brown hier...
4     cozily single practical badminton Mr. Brown flat
Name: Text, dtype: object
&gt;&gt;&gt; texts = df['Text'].tolist()
&gt;&gt;&gt; tagged_texts = pos_tag_sents(map(word_tokenize, texts))
&gt;&gt;&gt; tagged_texts
[[('cozily', 'RB'), ('married', 'JJ'), ('practical', 'JJ'), ('athletics', 'NNS'), ('Mr.', 'NNP'), ('Brown', 'NNP'), ('flat', 'JJ')], [('active', 'JJ'), ('married', 'VBD'), ('expensive', 'JJ'), ('soccer', 'NN'), ('Mr.', 'NNP'), ('Chang', 'NNP'), ('flat', 'JJ')], [('healthy', 'JJ'), ('single', 'JJ'), ('expensive', 'JJ'), ('badminton', 'NN'), ('Mrs.', 'NNP'), ('Green', 'NNP'), ('flat', 'JJ')], [('cozily', 'RB'), ('married', 'JJ'), ('practical', 'JJ'), ('soccer', 'NN'), ('Mr.', 'NNP'), ('Brown', 'NNP'), ('hierachical', 'JJ')], [('cozily', 'RB'), ('single', 'JJ'), ('practical', 'JJ'), ('badminton', 'NN'), ('Mr.', 'NNP'), ('Brown', 'NNP'), ('flat', 'JJ')]]

&gt;&gt;&gt; df['POS'] = tagged_texts
&gt;&gt;&gt; df
   ID                 Task        label  \
0   1  Collect Information  no response   
1   2           New Credit  no response   
2   3  Collect Information     response   
3   4  Collect Information     response   
4   5  Collect Information     response   

                                                Text  \
0  cozily married practical athletics Mr. Brown flat   
1     active married expensive soccer Mr. Chang flat   
2  healthy single expensive badminton Mrs. Green ...   
3  cozily married practical soccer Mr. Brown hier...   
4   cozily single practical badminton Mr. Brown flat   

                                                 POS  
0  [(cozily, RB), (married, JJ), (practical, JJ),...  
1  [(active, JJ), (married, VBD), (expensive, JJ)...  
2  [(healthy, JJ), (single, JJ), (expensive, JJ),...  
3  [(cozily, RB), (married, JJ), (practical, JJ),...  
4  [(cozily, RB), (single, JJ), (practical, JJ), ... 
</code></pre>

<hr>

<p><strong>In Long:</strong></p>

<p>First, you can extract the <code>Text</code> column to a list of string:</p>

<pre><code>texts = df['Text'].tolist()
</code></pre>

<p>Then you can apply the <code>word_tokenize</code> function:</p>

<pre><code>map(word_tokenize, texts)
</code></pre>

<p>Note that, @Boud's suggested is almost the same, using <code>df.apply</code>:</p>

<pre><code>df['Text'].apply(word_tokenize)
</code></pre>

<p>Then you dump the tokenized text into a list of list of string:</p>

<pre><code>df['Text'].apply(word_tokenize).tolist()
</code></pre>

<p>Then you can use <code>pos_tag_sents</code>:</p>

<pre><code>pos_tag_sents( df['Text'].apply(word_tokenize).tolist() )
</code></pre>

<p>Then you add the column back to the DataFrame:</p>

<pre><code>df['POS'] = pos_tag_sents( df['Text'].apply(word_tokenize).tolist() )
</code></pre>
",""
"42028084","2017-02-03 15:51:25","3","","42027252","<p>You might want to check out Unicode's CLDR (Common Locale Data Repository):
<a href=""http://cldr.unicode.org/"" rel=""nofollow noreferrer"">http://cldr.unicode.org/</a></p>

<p>It has lists of territories and languages that might be useful as you could map them together using their shared standard ISO 639 codes (en, de, fr etc).</p>

<p>Here's a useful JSON repository:  </p>

<p><a href=""https://github.com/unicode-cldr/cldr-localenames-full/tree/master/main/en"" rel=""nofollow noreferrer"">https://github.com/unicode-cldr/cldr-localenames-full/tree/master/main/en</a></p>

<p>Check out the <em>territories.json</em> and <em>languages.json</em> files there. </p>
",""
"41794714","2017-01-22 18:36:44","2","","41793842","<p><strong>1. I still don't get what n.01 means and why it's necessary.</strong></p>

<p>from <a href=""http://www.nltk.org/howto/wordnet.html"" rel=""noreferrer"">here</a> and the <a href=""https://github.com/nltk/nltk/blob/37fe28ef475f1ec33e6280412da92722e9719c68/nltk/corpus/reader/wordnet.py#L207"" rel=""noreferrer"">source of nltk</a> shows that the result is <code>""WORD.PART-OF-SPEECH.SENSE-NUMBER""</code></p>

<p>quoting the source:</p>

<pre><code>Create a Lemma from a ""&lt;word&gt;.&lt;pos&gt;.&lt;number&gt;.&lt;lemma&gt;"" string where:
&lt;word&gt; is the morphological stem identifying the synset
&lt;pos&gt; is one of the module attributes ADJ, ADJ_SAT, ADV, NOUN or VERB
&lt;number&gt; is the sense number, counting from 0.
&lt;lemma&gt; is the morphological form of interest
</code></pre>

<p><em>n</em> means Noun, I also suggest reading about <a href=""http://wordnetweb.princeton.edu"" rel=""noreferrer"">wordnet dataset</a>.</p>

<p><strong>2. there is a way to visually show the computed path between 2 terms?</strong></p>

<p>please look at the <a href=""http://www.nltk.org/howto/wordnet.html"" rel=""noreferrer"">nltk wordnet docs</a> on <strong>similarity</strong> section. you have several choices for path algorithms there (you can try mixing several).</p>

<p>few examples from nltk docs:</p>

<pre><code>from nltk.corpus import wordnet as wn
dog = wn.synset('dog.n.01')
cat = wn.synset('cat.n.01')

print(dog.path_similarity(cat))
print(dog.lch_similarity(cat))
print(dog.wup_similarity(cat))
</code></pre>

<p>for the visualization you can build a distance matrix <code>M[i,j]</code> where:</p>

<p><code>M[i,j] = word_similarity(i, j)</code></p>

<p>and use the following <a href=""https://stackoverflow.com/questions/13513455/drawing-a-graph-or-a-network-from-a-distance-matrix"">stackoverflow answer</a> to draw the visualization.</p>

<p><strong>3. Which other nltk semantic metric could I use?</strong></p>

<p>As mentioned above, there are several ways to calculate the word similarities. I also suggest looking into <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""noreferrer"">gensim</a>. I used its word2vec implementation for word similarities and it worked well for me.</p>

<p>if you need any help choosing algorithms please provide more info about the problem you are facing.</p>

<h2>Update:</h2>

<p>More info about word <code>sense number</code> meaning can be found <a href=""http://%20wordnet.princeton.edu/man/wndb.5WN.html#sect4"" rel=""noreferrer"">here</a>:</p>

<blockquote>
  <p>Senses in WordNet are generally ordered from most to least frequently used, with the <strong>most common sense numbered 1</strong>...</p>
</blockquote>

<p>the problem is that ""dog"" is ambiguous and you must choose the right meaning for it.</p>

<p>you might choose the first sense as naive approach or find your own algorithm for choosing the right meaning depend on your application or research.</p>

<p>to get all available definitions (called <em>synsets</em> on wordnet docs) of a word from wordnet you could simply call <code>wn.synsets(word)</code>.</p>

<p>I encourage you to dig into the metadata contained inside these synset for each definition. </p>

<p>the code below shows a simple example to get this metadata and prints it nicely.</p>

<pre><code>from nltk.corpus import wordnet as wn

dog_synsets = wn.synsets('dog')

for i, syn in enumerate(dog_synsets):
    print('%d. %s' % (i, syn.name()))
    print('alternative names (lemmas): ""%s""' % '"", ""'.join(syn.lemma_names()))
    print('definition: ""%s""' % syn.definition())
    if syn.examples():
        print('example usage: ""%s""' % '"", ""'.join(syn.examples()))
    print('\n')
</code></pre>

<p>code output:</p>

<pre><code>0. dog.n.01
alternative names (lemmas): ""dog"", ""domestic_dog"", ""Canis_familiaris""
definition: ""a member of the genus Canis (probably descended from the common wolf) that has been domesticated by man since prehistoric times; occurs in many breeds""
example usage: ""the dog barked all night""


1. frump.n.01
alternative names (lemmas): ""frump"", ""dog""
definition: ""a dull unattractive unpleasant girl or woman""
example usage: ""she got a reputation as a frump"", ""she's a real dog""


2. dog.n.03
alternative names (lemmas): ""dog""
definition: ""informal term for a man""
example usage: ""you lucky dog""


3. cad.n.01
alternative names (lemmas): ""cad"", ""bounder"", ""blackguard"", ""dog"", ""hound"", ""heel""
definition: ""someone who is morally reprehensible""
example usage: ""you dirty dog""


4. frank.n.02
alternative names (lemmas): ""frank"", ""frankfurter"", ""hotdog"", ""hot_dog"", ""dog"", ""wiener"", ""wienerwurst"", ""weenie""
definition: ""a smooth-textured sausage of minced beef or pork usually smoked; often served on a bread roll""


5. pawl.n.01
alternative names (lemmas): ""pawl"", ""detent"", ""click"", ""dog""
definition: ""a hinged catch that fits into a notch of a ratchet to move a wheel forward or prevent it from moving backward""


6. andiron.n.01
alternative names (lemmas): ""andiron"", ""firedog"", ""dog"", ""dog-iron""
definition: ""metal supports for logs in a fireplace""
example usage: ""the andirons were too hot to touch""


7. chase.v.01
alternative names (lemmas): ""chase"", ""chase_after"", ""trail"", ""tail"", ""tag"", ""give_chase"", ""dog"", ""go_after"", ""track""
definition: ""go after with the intent to catch""
example usage: ""The policeman chased the mugger down the alley"", ""the dog chased the rabbit""
</code></pre>
",""
"41749579","2017-01-19 18:56:24","0","","41749471","<p><code>Document</code> in the <code>tf-idf</code> context can typically be thought of as a <code>bag of words</code>. In a <code>vector space model</code> each word is a dimension in a very high-dimensional space, where the magnitude of an word vector is the number of occurrences of the word (term) in the document. A <code>Document-Term</code> matrix represents a matrix where the rows represent documents and the columns represent the terms, with each cell in the matrix representing # occurrences of the word in the document. Hope it's clear.</p>
",""
"41708013","2017-01-17 22:17:51","2","","41707679","<p>You should provide a tokenizer for the Japanese</p>

<pre><code>vectorizer = TfidfVectorizer(ngram_range=(1,1), tokenizer=jap_tokenizer)
</code></pre>

<p>where <code>jap_tokenizer</code> is either a function you create or one like <a href=""https://jprocessing.readthedocs.io/en/latest/"" rel=""noreferrer"">this</a>. </p>
",""
"41587352","2017-01-11 09:42:57","5","","41575278","<p>I think the concept of <a href=""https://en.wikipedia.org/wiki/Feature_structure"" rel=""nofollow noreferrer"">feature structures</a> is what you're looking for; the sharing of arguments you show in your example is a special case of the more general feature structure unification approach.</p>

<p>I'm not aware of feature structure extensions for BNF specifically, but there are reasonably accepted notations for adding them to other grammar formalisms. The documentation for NLTK (a Python natural language processing library) has <a href=""http://www.nltk.org/book/ch09.html#code-germancfg"" rel=""nofollow noreferrer"">an example here</a> of the notation they use. Here are some of their rules that correspond roughly to the first few productions from your example:</p>

<pre><code>S -&gt; NP[CASE=nom, AGR=?a] VP[AGR=?a]
VP[AGR=?a] -&gt; TV[OBJCASE=?c, AGR=?a] NP[CASE=?c]
NP[CASE=?c, AGR=?a] -&gt; Det[CASE=?c, AGR=?a] N[CASE=?c, AGR=?a]
</code></pre>

<p>The <code>?x</code> is their notation for logic variables. That entire chapter of the NLTK manual contains a general desciption of feature structures and includes some literature references.</p>
",""
"41575916","2017-01-10 18:33:01","0","","41520580","<p>This API actually is doing what you are exactly asking for (Clustering sentences + giving key-words):
<a href=""http://www.rxnlp.com/api-reference/cluster-sentences-api-reference/"" rel=""nofollow noreferrer"">http://www.rxnlp.com/api-reference/cluster-sentences-api-reference/</a></p>

<p>Unfortunately the algorithm used for clustering and the for generating the key-words is not available.</p>

<p>Hope this helps.</p>
",""
"41526359","2017-01-07 20:45:19","2","","41517595","<p>This is an NLTK bug specific to NLTK version 3.2.2, for which I am to blame. It was introduced by PR <a href=""https://github.com/nltk/nltk/pull/1261"" rel=""noreferrer"">https://github.com/nltk/nltk/pull/1261</a> which rewrote the Porter stemmer.</p>

<p>I wrote <a href=""https://github.com/nltk/nltk/pull/1582"" rel=""noreferrer"">a fix</a> which went out in NLTK 3.2.3. If you're on version 3.2.2 and want the fix, just upgrade - e.g. by running</p>

<pre><code>pip install -U nltk
</code></pre>
",""
"41522613","2017-01-07 14:42:25","3","","41522476","<p>If you look at <a href=""http://www.nltk.org/_modules/nltk/parse/stanford.html"" rel=""nofollow noreferrer"">the NLTK classes for the Stanford parser</a>, you can see that the the <code>raw_parse_sents()</code> method doesn't send the <code>-outputFormat wordsAndTags</code> option that you want, and instead sends <code>-outputFormat Penn</code>.
If you derive your own class from <code>StanfordParser</code>, you could override this method and specify the <code>wordsAndTags</code> format.</p>

<pre><code>from nltk.parse.stanford import StanfordParser

class MyParser(StanfordParser):

        def raw_parse_sents(self, sentences, verbose=False):
        """"""
        Use StanfordParser to parse multiple sentences. Takes multiple sentences as a
        list of strings.
        Each sentence will be automatically tokenized and tagged by the Stanford Parser.
        The output format is `wordsAndTags`.

        :param sentences: Input sentences to parse
        :type sentences: list(str)
        :rtype: iter(iter(Tree))
        """"""
        cmd = [
            self._MAIN_CLASS,
            '-model', self.model_path,
            '-sentences', 'newline',
            '-outputFormat', 'wordsAndTags',
        ]
        return self._parse_trees_output(self._execute(cmd, '\n'.join(sentences), verbose))
</code></pre>
",""
"41437935","2017-01-03 06:50:32","1","","41431572","<p>The second approach you explained will work. But there are better ways to solve this kind of problem.
At first you should know a little bit about language models and leave the vector space model.
In the second step based on your kind of problem that is similar to expert finding/profiling you should learn a baseline language model framework to implement a solution.
You can implement <a href=""https://staff.fnwi.uva.nl/m.derijke/Publications/Files/ipm2008.pdf"" rel=""nofollow noreferrer"">A language modeling framework for expert finding</a> with a little changes so that the formulas can be adapted to your problem.
Also reading <a href=""http://vbn.aau.dk/ws/files/176790804/jasist_2013_expertise_profiling.pdf"" rel=""nofollow noreferrer"">On the assessment of expertise profiles</a> will give you a better understanding of expert profiling with the framework above. 
you can find some good ideas, resources and projects on expert finding/profiling at <a href=""http://krisztianbalog.com"" rel=""nofollow noreferrer"">Balog's blog</a>. </p>
",""
"41346520","2016-12-27 13:55:48","1","","41315082","<p>I'm (also) not really sure what you are after. Perhaps you should try to structure your idea/requirements a bit more (in your head/on paper) before trying to put it into code. 
Based on your description and code, I can think of two possible figures that you're after, which can be obtained in the following way:</p>

<pre><code>from collections import defaultdict

tagged_text = [[('A', 'DT'), ('hairy', 'NNS'), ('dog', 'NN')], [('The', 'DT'), ('mischevious', 'NNS'), ('elephant', 'NN')]]

d = defaultdict(int)
t = 0
for sentence in tagged_text:
    for tupl in sentence:
        tag = tupl[1]
        d[tag] += 1
        t += 1

for tag in d:
    print(""Likelihood that %s appears in a sentence: %s"" % (tag, str(float(d[tag] / len(tagged_text)))))
    print(""Likelihood of %s appearing in complete corpus: %s"" % (tag, str(float(d[tag] / t))))
</code></pre>

<p>Resulting in </p>

<pre><code>Likelihood that NN appears in a sentence: 1.0
Likelihood of NN in complete corpus: 0.3333333333333333
Likelihood that NNS appears in a sentence: 1.0
Likelihood of NNS in complete corpus: 0.3333333333333333
Likelihood that DT appears in a sentence: 1.0
Likelihood of DT in complete corpus: 0.3333333333333333
</code></pre>

<p>All three tags appear in both sentences, hence likelihood of it appearing in a sentence is 1. All three tags both appear twice (on a total of six), hence a likelihood of 1/3 for them to appear (not regarding sentence distribution).
But then again, not sure if this is what you're after.</p>
",""
"41301818","2016-12-23 12:47:41","0","","40126849","<p>The paper you cite is using quantifiers somewhat informally, the same way it uses predicates. If you already have a lambda calculus, you can, at least on paper, extend it with any set of formal symbols you want, including quantifiers from FOL. In that case, the quantifiers are something added to the calculus, not part of it.</p>

<p>Quantifiers can be defined in <em>typed</em> lambda calculus, however. In simply typed settings, you have basic function types, but these can be generalized to universal quantifier and <a href=""https://en.wikipedia.org/wiki/Dependent_type"" rel=""nofollow noreferrer"">dependent function/Pi types</a>. In that case, the lambda expressions represent proofs of implications and universal quantifications, meaning they are built in semantic interpretations that you can give to lambda expressions. Existential quantifiers can then be defined as;</p>

<p>‚àÉ a : t . P a := ‚àÄQ . (‚àÄ a : t . P a ‚Üí Q) ‚Üí Q</p>

<p>Which has the same data as the ordinary product type.</p>
",""
"41202547","2016-12-17 20:02:00","0","","33988038","<p>Since no other answer has been posted for a long time, I'm accepting the slightly edited initial script as the best fix for my problem.</p>

<p>While looking into it another approach better that ignoring errors would be <a href=""https://stackoverflow.com/questions/3194516/replace-special-characters-with-ascii-equivalent"">normalizing</a> first.</p>
",""
"40971112","2016-12-05 09:35:37","1","","40957598","<p>For some reason, WordNet indexes <code>antonymy</code> relations at the Lemma level instead of the Synset (see <a href=""http://wordnetweb.princeton.edu/perl/webwn?o2=&amp;o0=1&amp;o8=1&amp;o1=1&amp;o7=&amp;o5=&amp;o9=&amp;o6=&amp;o3=&amp;o4=&amp;s=good&amp;i=8&amp;h=00001000000000000000000000000000#c"" rel=""nofollow noreferrer"">http://wordnetweb.princeton.edu/perl/webwn?o2=&amp;o0=1&amp;o8=1&amp;o1=1&amp;o7=&amp;o5=&amp;o9=&amp;o6=&amp;o3=&amp;o4=&amp;s=good&amp;i=8&amp;h=00001000000000000000000000000000#c</a>), so  the question is whether <code>Synsets</code> and <code>Lemmas</code> have many-to-many or one-to-one relations. </p>

<hr>

<p>In the case of ambiguous words, one word many meaning, we have a one-to-many relation between String-to-<code>Synset</code>, e.g.</p>

<pre><code>&gt;&gt;&gt; wn.synsets('dog')
[Synset('dog.n.01'), Synset('frump.n.01'), Synset('dog.n.03'), Synset('cad.n.01'), Synset('frank.n.02'), Synset('pawl.n.01'), Synset('andiron.n.01'), Synset('chase.v.01')]
</code></pre>

<p>In the case of one meaning/concept, multiple representation, we have a one-to-many relation between <code>Synset</code>-to-String (where String refers to Lemma names):</p>

<pre><code>&gt;&gt;&gt; dog = wn.synset('dog.n.1')
&gt;&gt;&gt; dog.definition()
u'a member of the genus Canis (probably descended from the common wolf) that has been domesticated by man since prehistoric times; occurs in many breeds'
&gt;&gt;&gt; dog.lemma_names()
[u'dog', u'domestic_dog', u'Canis_familiaris']
</code></pre>

<p><strong>Note:</strong> up till now, we are comparing the relationships between String and <code>Synsets</code> not <code>Lemmas</code> and <code>Synsets</code>.</p>

<hr>

<p>The ""cute"" thing is that <code>Lemma</code> and String has a one-to-one relationship:</p>

<pre><code>&gt;&gt;&gt; wn.synsets('dog')
[Synset('dog.n.01'), Synset('frump.n.01'), Synset('dog.n.03'), Synset('cad.n.01'), Synset('frank.n.02'), Synset('pawl.n.01'), Synset('andiron.n.01'), Synset('chase.v.01')]
&gt;&gt;&gt; wn.synsets('dog')[0]
Synset('dog.n.01')
&gt;&gt;&gt; wn.synsets('dog')[0].definition()
u'a member of the genus Canis (probably descended from the common wolf) that has been domesticated by man since prehistoric times; occurs in many breeds'
&gt;&gt;&gt; wn.synsets('dog')[0].lemmas()
[Lemma('dog.n.01.dog'), Lemma('dog.n.01.domestic_dog'), Lemma('dog.n.01.Canis_familiaris')]
&gt;&gt;&gt; wn.synsets('dog')[0].lemmas()[0]
Lemma('dog.n.01.dog')
&gt;&gt;&gt; wn.synsets('dog')[0].lemmas()[0].name()
u'dog'
</code></pre>

<p>The <code>_name</code> property of a <code>Lemma</code> object returns a unicode string, not a list. From the code points: <a href=""https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/wordnet.py#L202"" rel=""nofollow noreferrer"">https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/wordnet.py#L202</a> and <a href=""https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/wordnet.py#L444"" rel=""nofollow noreferrer"">https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/wordnet.py#L444</a></p>

<p>And it seems like the Lemma has a one-to-one relation with Synset. From docstring at <a href=""https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/wordnet.py#L220"" rel=""nofollow noreferrer"">https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/wordnet.py#L220</a>:</p>

<blockquote>
  <p>Lemma attributes, accessible via methods with the same name::</p>
  
  <ul>
  <li>name: The canonical name of this lemma.</li>
  <li>synset: The synset that this lemma belongs to.</li>
  <li>syntactic_marker: For adjectives, the WordNet string identifying the
    syntactic position relative modified noun. See:
    <a href=""http://wordnet.princeton.edu/man/wninput.5WN.html#sect10"" rel=""nofollow noreferrer"">http://wordnet.princeton.edu/man/wninput.5WN.html#sect10</a>
    For all other parts of speech, this attribute is None.</li>
  <li>count: The frequency of this lemma in wordnet.</li>
  </ul>
</blockquote>

<p>So we can do this and somehow know that each <code>Lemma</code> object is only going to return us 1 synset:</p>

<pre><code>&gt;&gt;&gt; wn.synsets('dog')[0].lemmas()[0]
Lemma('dog.n.01.dog')
&gt;&gt;&gt; wn.synsets('dog')[0].lemmas()[0].synset()
Synset('dog.n.01')
</code></pre>

<hr>

<p>Assuming that you are trying to do some sentiment analysis and you need the antonyms of every adjective in WordNet, you can easily do this to accept the Synsets of the antonyms:</p>

<pre><code>&gt;&gt;&gt; from nltk.corpus import wordnet as wn
&gt;&gt;&gt; all_adj_in_wn = wn.all_synsets(pos='a')
&gt;&gt;&gt; def get_antonyms(ss):
...     return set(chain(*[[a.synset() for a in l.antonyms()] for l in ss.lemmas()]))
...
&gt;&gt;&gt; for ss in all_adj_in_wn:
...     print ss, ':', get_antonyms(ss)
... 
Synset('unable.a.01') : set([Synset('unable.a.01')])
</code></pre>
",""
"40952822","2016-12-03 21:45:36","0","","40948117","<p>Represent each document as a <code>bag of words</code> and use <code>tf-idf</code> weight to represent a word in a particular document. Then compute cosine similarity with all <code>n</code> documents. Sum all similarity values and then normalize (divide the final sim value by <code>n</code>). It should give you a reasonable similarity between the <code>n</code> documents and your target document.</p>

<p>You can also consider <a href=""https://en.wikipedia.org/wiki/Mutual_information"" rel=""nofollow noreferrer"">mutual information</a> (<a href=""http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mutual_info_score.html"" rel=""nofollow noreferrer"">sklearn.metrics.mutual_info_score</a>), <a href=""https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence"" rel=""nofollow noreferrer"">KL-divergence</a> to measure similarity/difference between two documents. Note that if you want to use them, you need to represent documents as a probability distribution. To compute probability of a term in a document, you can simply use the following formula:</p>

<pre><code>Probability(w) = TF(w) / TTF(w)
</code></pre>

<p>where,</p>

<pre><code>TF(w) = term frequency of word, w in a document, d
TTF(w) = total term frequency of word, w [sum of tf in all documents]
</code></pre>

<p>I believe any one of them will give you reasonable idea about similarity/dissimilarity between the <code>n</code> documents and your target document.</p>
",""
"40929432","2016-12-02 10:02:39","0","","40788494","<p>You can use the method described in this paper for computing the similarity of two sentences: <a href=""https://www.aaai.org/Papers/AAAI/2006/AAAI06-123.pdf"" rel=""nofollow noreferrer"">Corpus-based and Knowledge-based Measures
of Text Semantic Similarity</a></p>

<p>You can remove words until the similarity with the original sentence drops significantly (this is an interesting problem in itself). </p>

<p>You can also check this simplified version of the similarity algorithm here: <a href=""http://nlpforhackers.io/wordnet-sentence-similarity/"" rel=""nofollow noreferrer"">Wordnet Sentence Similarity</a></p>
",""
"40863527","2016-11-29 10:30:34","3","","40853150","<p>The wordnet interface to NLTK uses english by default. When making a call to <code>synsets()</code>, an additional <code>lang</code> argument can be used to specify the language you are interested in. When used with the available languages in <code>wn.lang()</code> it seems to work appropriately.</p>

<p>Here are a few examples in greek, spanish and italian.</p>

<pre><code>&gt;&gt;&gt; from nltk.corpus import wordnet as wn
&gt;&gt;&gt; wn.synsets('ŒµŒΩŒøœáŒªœé', lang='ell')
[Synset('irritate.v.02'), Synset('harass.v.01'), Synset('tease.v.01')]
&gt;&gt;&gt; wn.synsets('molestar', lang='spa')
[Synset('interrupt.v.02'), Synset('tease.v.02'), Synset('disturb.v.01'), Synset('faze.v.01'), Synset('annoy.v.01'), Synset('tease.v.01'), Synset('chafe.v.01'), Synset('trouble.v.02')]
&gt;&gt;&gt; wn.synsets('annoiare', lang='ita')
[Synset('tire.v.02'), Synset('tire.v.01'), Synset('bore.v.01')]
</code></pre>

<p><a href=""https://stackoverflow.com/questions/39569307/how-to-change-nltk-default-wordnet-language-to-zsm"">a previous similar post</a>, uses a wrapper class to enforce a default language, which may be good for your use. </p>
",""
"40863079","2016-11-29 10:08:47","8","","40860220","<p>A POS Tagger is traditionally based on a probability distribution of words over a corpus. Therefore extending use-case to a new body of text will usually yield higher error rates, since the distribution of words is different. </p>

<p>Other models are not strictly a Probability Distribution, such as Neural Networks, and need to be trained but the same logic holds true for both. </p>

<p>For example, if I make a POS tagger for <code>Shakespeare</code> texts by using tagged sentences from <code>Hamlet</code> to define my probability distribution, then try to POS tag <code>Biomedical</code> texts, it probably won't perform well. </p>

<p>Therefore, to increase accuracy, <em>you should train with a body of text that is similar to your specific domain.</em> </p>

<p>The current best performing POS tagger in NLTK is the Perceptron Tagger which is the default and uses a pre-trained model. Here is how you would train your own model to increase accuracy. </p>

<pre><code>import nltk,math
# get data to train and test
tagged_sentences = [sentence for sentence in nltk.corpus.brown.tagged_sents(categories='news',tagset='universal')]
# hold out 20% for testing, get index for 20% split
split_idx = math.floor(len(tagged_sentences)*0.2)
# testing sentences are words only, list(list(word))
testing_sentences = [[word for word,_ in test_sent] for test_sent in tagged_sentences[0:split_idx]]
# training sentences words and tags, list(list(word,tag))
training_sentences = tagged_sentences[split_idx:] 
# create instance of perceptron POS tagger
perceptron_tagger = nltk.tag.perceptron.PerceptronTagger(load=False)
perceptron_tagger.train(training_sentences)
pos_tagged_sentences = [perceptron_tagger.tag([word for word,_ in test_sentence]) for test_sentence in testing_sentences]
</code></pre>

<p>after <code>perceptron_tagger.train()</code> finishes on the <code>training_sentences</code>, you can use <code>perceptron_tagger.tag()</code> to get <code>pos_tagged_sentences</code> which are more useful to your domain and produce a much higher accuracy. </p>

<p>If done right, they will produce high-accuracy results. From <a href=""https://natemccoy.github.io/2016/10/27/evaluatingnltktaggerstutorial.html"" rel=""nofollow noreferrer"">my basic tests</a>, they show the following results:</p>

<pre><code>Metrics for &lt;nltk.tag.perceptron.PerceptronTagger object at 0x7f34904d1748&gt;
 Accuracy : 0.965636914654
 Precision: 0.965271747376
 Recall   : 0.965636914654
 F1-Score : 0.965368188021
</code></pre>
",""
"40783390","2016-11-24 10:07:04","5","","40766816","<p>I suggest running <em>terminology extraction</em> first, together with their frequencies. Note that stemming can also be applied to the extracted terms to avoid noise in during the subsequent <a href=""https://en.wikipedia.org/wiki/Cosine_similarity"" rel=""nofollow noreferrer"">cosine similarity</a> calculation. See <a href=""https://stackoverflow.com/questions/17447045/java-library-for-keywords-extraction-from-input-text"">Java library for keywords extraction from input text</a> SO thread for more help and ideas on that.</p>

<p>Then, as you yourself mention, for each of those terms, you will have to compute the TF-IDF values, get the vectors and compute the cosine similarity.</p>

<p>When calculating TF-IDF, mind that <code>1 + log(N/n)</code> (<em>N</em> standing for the total number of corpora and <code>n</code> standing for the number of corpora that include the term) formula is better since it avoids the issue when TF is not 0 and IDF turns out equal to 0.</p>
",""
"40650581","2016-11-17 09:03:55","5","","40542523","<p><strong>TL;DR</strong>:</p>

<pre><code>&gt;&gt;&gt; import nltk
&gt;&gt;&gt; hypothesis = ['This', 'is', 'cat'] 
&gt;&gt;&gt; reference = ['This', 'is', 'a', 'cat']
&gt;&gt;&gt; references = [reference] # list of references for 1 sentence.
&gt;&gt;&gt; list_of_references = [references] # list of references for all sentences in corpus.
&gt;&gt;&gt; list_of_hypotheses = [hypothesis] # list of hypotheses that corresponds to list of references.
&gt;&gt;&gt; nltk.translate.bleu_score.corpus_bleu(list_of_references, list_of_hypotheses)
0.6025286104785453
&gt;&gt;&gt; nltk.translate.bleu_score.sentence_bleu(references, hypothesis)
0.6025286104785453
</code></pre>

<p>(Note: You have to pull the latest version of NLTK on the <code>develop</code> branch in order to get a stable version of the BLEU score implementation)</p>

<hr>

<p><strong>In Long</strong>:</p>

<p>Actually, if there's only one reference and one hypothesis in your whole corpus, both <code>corpus_bleu()</code> and <code>sentence_bleu()</code> should return the same value as shown in the example above.</p>

<p>In the code, we see that <a href=""https://github.com/nltk/nltk/blob/develop/nltk/translate/bleu_score.py#L26"" rel=""noreferrer""><code>sentence_bleu</code> is actually a duck-type of <code>corpus_bleu</code></a>:</p>

<pre><code>def sentence_bleu(references, hypothesis, weights=(0.25, 0.25, 0.25, 0.25),
                  smoothing_function=None):
    return corpus_bleu([references], [hypothesis], weights, smoothing_function)
</code></pre>

<p>And if we look at the parameters for <code>sentence_bleu</code>:</p>

<pre><code> def sentence_bleu(references, hypothesis, weights=(0.25, 0.25, 0.25, 0.25),
                      smoothing_function=None):
    """"""""
    :param references: reference sentences
    :type references: list(list(str))
    :param hypothesis: a hypothesis sentence
    :type hypothesis: list(str)
    :param weights: weights for unigrams, bigrams, trigrams and so on
    :type weights: list(float)
    :return: The sentence-level BLEU score.
    :rtype: float
    """"""
</code></pre>

<p>The input for <code>sentence_bleu</code>'s references is a <code>list(list(str))</code>.</p>

<p>So if you have a sentence string, e.g. <code>""This is a cat""</code>, you have to tokenized it to get a list of strings, <code>[""This"", ""is"", ""a"", ""cat""]</code> and since it allows for multiple references, it has to be a list of list of string, e.g. if you have a second reference, ""This is a feline"", your input to <code>sentence_bleu()</code> would be:</p>

<pre><code>references = [ [""This"", ""is"", ""a"", ""cat""], [""This"", ""is"", ""a"", ""feline""] ]
hypothesis = [""This"", ""is"", ""cat""]
sentence_bleu(references, hypothesis)
</code></pre>

<p>When it comes to <code>corpus_bleu()</code> list_of_references parameter, it's basically <a href=""https://github.com/nltk/nltk/blob/develop/nltk/translate/bleu_score.py#L82"" rel=""noreferrer"">a list of whatever the <code>sentence_bleu()</code> takes as references</a>:</p>

<pre><code>def corpus_bleu(list_of_references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25),
                smoothing_function=None):
    """"""
    :param references: a corpus of lists of reference sentences, w.r.t. hypotheses
    :type references: list(list(list(str)))
    :param hypotheses: a list of hypothesis sentences
    :type hypotheses: list(list(str))
    :param weights: weights for unigrams, bigrams, trigrams and so on
    :type weights: list(float)
    :return: The corpus-level BLEU score.
    :rtype: float
    """"""
</code></pre>

<p>Other than look at the doctest within the <a href=""https://github.com/nltk/nltk/blob/develop/nltk/translate/bleu_score.py"" rel=""noreferrer""><code>nltk/translate/bleu_score.py</code></a>, you can also take a look at the unittest at <a href=""https://github.com/nltk/nltk/blob/develop/nltk/test/unit/translate/test_bleu.py"" rel=""noreferrer""><code>nltk/test/unit/translate/test_bleu_score.py</code></a> to see how to use each of the component within the <code>bleu_score.py</code>.</p>

<p>By the way, since the <code>sentence_bleu</code> is imported as <code>bleu</code> in the (<code>nltk.translate.__init__.py</code>](<a href=""https://github.com/nltk/nltk/blob/develop/nltk/translate/__init__.py#L21"" rel=""noreferrer"">https://github.com/nltk/nltk/blob/develop/nltk/translate/<strong>init</strong>.py#L21</a>), using </p>

<pre><code>from nltk.translate import bleu 
</code></pre>

<p>would be the same as:</p>

<pre><code>from nltk.translate.bleu_score import sentence_bleu
</code></pre>

<p>and in code:</p>

<pre><code>&gt;&gt;&gt; from nltk.translate import bleu
&gt;&gt;&gt; from nltk.translate.bleu_score import sentence_bleu
&gt;&gt;&gt; from nltk.translate.bleu_score import corpus_bleu
&gt;&gt;&gt; bleu == sentence_bleu
True
&gt;&gt;&gt; bleu == corpus_bleu
False
</code></pre>
",""
"40643027","2016-11-16 21:51:28","1","","40640242","<p><code>searchString</code> is working because it skips over text that doesn't exactly match the grammar. <code>parseString</code> is much more particular, requiring a complete grammar match, beginning right with the first character of the input string. In your case, the grammar is a little difficult to determine, since it is auto-generated based on the NLTK analysis of the input sentence (an interesting approach, btw). If you just print the grammar itself, it <em>may</em> give you some insights into what strings it is looking for. For instance, I'm guessing NLTK will interpret 'Failure' in your first example as a noun, yet none of your 3 expressions in your grammar starts with a noun - therefore, <code>parseString</code> will fail. </p>

<p>You'll probably need to do a lot more internal printing of noun, adjective, and verb lists  based on what NLTK finds, and then see how that maps to your generated grammar.</p>

<p>You can also try to combine the results of multiple matches in the sentence using Python's sum() builtin:</p>

<pre><code>grammar =  action(""action"") | Group(location)(""location"") | Group(speed)(""speed"")

#parsed = grammar.parseString(sentence)
parsed = sum(grammar.searchString(sentence))
print(parsed.dump())
</code></pre>
",""
"40633734","2016-11-16 13:45:05","6","","40631146","<p><a href=""http://numba.pydata.org/"" rel=""nofollow noreferrer"">Numba</a> will be a good solution for this.  As I think you know, it does not support Pandas DataFrames, but it is built around NumPy arrays.  This is not a problem--you can easily and quickly convert your DataFrame to a 2D array and pass that to a Numba function (which will be pretty much the code you already have, just decorated with <code>@njit</code> at the top).</p>

<p>Also note that instead of a dict-of-dicts for the results, you can use one triangle of a square matrix to store them:</p>

<pre><code>     doc1 doc2 doc3
doc1  NAN  NAN  NAN
doc2  ...  NAN  NAN
doc3  ...  ...  NAN
</code></pre>

<hr>

<p>Edit: You've now implemented it using Numba, but got only a 2.5x speedup.  I ran some experiments and found a big win:</p>

<pre><code>In [66]: x = np.random.random((1000,1000))

In [67]: y = np.array(x, order='F')

In [68]: %timeit similarity_jit(x)
1 loop, best of 3: 13.7 s per loop

In [69]: %timeit similarity_jit(y)
1 loop, best of 3: 433 ms per loop
</code></pre>

<p>That is, your algorithm will be much, much faster if you operate on contiguous chunks of data, due to caching.  Since the kernel of your algorithm is <code>numpy.dot(m[:,i], m[:,j])</code>, and <code>m[:,i]</code> takes one column, you are better off orienting your data in <a href=""https://en.wikipedia.org/wiki/Row-_and_column-major_order"" rel=""nofollow noreferrer"">""Fortran order""</a> (column-major order) first, so that <code>m[:,i]</code> gives one contiguous array (because the array is laid out ""transposed"" in memory).</p>
",""
"40579321","2016-11-13 22:05:00","0","","40568856","<p>Ok, I googled more and I found out how to get these tags. 
First one have to do some preprocessing, to be sure that file will get tokenized (in my case it was about removing some stuff left off after conversion from pdf to txt). </p>

<p>Then these file has to be tokenized into sentences, then each sentence into word array, and that can be tagged by nltk tagger. With that lemmatization can be done, and then stemming added on top of it.</p>

<pre><code>from nltk.tokenize import sent_tokenize, word_tokenize
# use sent_tokenize to split text into sentences, and word_tokenize to
# to split sentences into words
from nltk.tag import pos_tag
# use this to generate array of tuples (word, tag)
# it can be then translated into wordnet tag as in
# [this response][1]. 
from nltk.stem.wordnet import WordNetLemmatizer
from stemming.porter2 import stem

# code from response mentioned above
def get_wordnet_pos(treebank_tag):
    if treebank_tag.startswith('J'):
        return wordnet.ADJ
    elif treebank_tag.startswith('V'):
        return wordnet.VERB
    elif treebank_tag.startswith('N'):
        return wordnet.NOUN
    elif treebank_tag.startswith('R'):
        return wordnet.ADV
    else:
        return ''    


with open(myInput, 'r') as f:
    data = f.read()
    sentences = sent_tokenize(data)
    ignoreTypes = ['TO', 'CD', '.', 'LS', ''] # my choice
    lmtzr = WordNetLemmatizer()
    for sent in sentences:
        words = word_tokenize(sentence)
        tags = pos_tag(words)
        for (word, type) in tags:
            if type in ignoreTypes:
                continue
            tag = get_wordnet_pos(type)
            if tag == '':
                continue
            lema = lmtzr.lemmatize(word, tag)
            stemW = stem(lema)
</code></pre>

<p>And at this point I get stemmed word <code>stemW</code> which I can then write to file, and use these to count tfidf vectors per document.</p>
",""
"40514554","2016-11-09 19:27:44","1","","40513544","<p>No they're not different they're all the same.</p>

<pre><code>from nltk.stem import WordNetLemmatizer as lm1
from nltk import WordNetLemmatizer as lm2
from nltk.stem.wordnet import WordNetLemmatizer as lm3

lm1 == lm2 
&gt;&gt;&gt; True


lm2 == lm3 
&gt;&gt;&gt; True


lm1 == lm3 
&gt;&gt;&gt; True
</code></pre>

<p>As corrected by <a href=""https://stackoverflow.com/users/2883245/erip"">erip</a> why this is happening is because : </p>

<p>That Class(<code>WordNetLemmatizer</code>) is origanlly written in <a href=""https://github.com/nltk/nltk/blob/develop/nltk/stem/wordnet.py#L15"" rel=""nofollow noreferrer"">nltk.stem.wordnet</a> so you can do <code>from nltk.stem.wordnet import WordNetLemmatizer as lm3</code></p>

<p>Which is also import in nltk <a href=""https://github.com/nltk/nltk/blob/develop/nltk/__init__.py#L137"" rel=""nofollow noreferrer"">__init__.py file</a> so you can do <code>from nltk import WordNetLemmatizer as lm2
</code></p>

<p>And is also imported in <a href=""https://github.com/nltk/nltk/blob/develop/nltk/stem/__init__.py#L30"" rel=""nofollow noreferrer"">__init__.py nltk.stem</a> module so you can do <code>from nltk.stem import WordNetLemmatizer as lm1</code></p>
",""
"40508641","2016-11-09 13:52:46","0","","37960667","<p>The nltk3 brill trainer api (I wrote it) does handle training on sequences of tokens described with multidimensional
features, as your data is an example of. However, the practical limits may be severe. The number of possible templates in multidimensional learning
increases drastically, and the  current nltk implementation of the brill trainer trades memory
for speed, similar to Ramshaw and Marcus 1994, ""Exploring the statistical derivation of transformation-rule sequences..."". 
Memory consumption may be HUGE and
it is very easy to give the system more data and/or templates than
it can handle. A useful strategy is to rank
templates according to how often they produce good rules (see
print_template_statistics() in the example below). 
Usually, you can discard the lowest-scoring fraction (say 50-90%)
with little or no loss in performance and a major decrease in training time. </p>

<p>Another or additional possibility is to use the nltk
implementation of Brill's original algorithm, which has very different memory-speed tradeoffs; it does no indexing and so will use much less memory. It uses some optimizations and is actually rather quick in finding the very best rules, but is generally extremely slow towards end of training when there are many competing, low-scoring candidates. Sometimes you don't need those, anyway. For some reason this implementation seems to have been omitted from newer nltks, but here is the source (I just tested it) <a href=""http://www.nltk.org/_modules/nltk/tag/brill_trainer_orig.html"" rel=""nofollow noreferrer"">http://www.nltk.org/_modules/nltk/tag/brill_trainer_orig.html</a>.</p>

<p>There are other algorithms with other tradeoffs, and
in particular the fast memory-efficient indexing algorithms of Florian and Ngai 2000
(<a href=""http://www.aclweb.org/anthology/N/N01/N01-1006.pdf"" rel=""nofollow noreferrer"">http://www.aclweb.org/anthology/N/N01/N01-1006.pdf</a>) and
probabilistic rule sampling of Samuel 1998
(<a href=""https://www.aaai.org/Papers/FLAIRS/1998/FLAIRS98-045.pdf"" rel=""nofollow noreferrer"">https://www.aaai.org/Papers/FLAIRS/1998/FLAIRS98-045.pdf</a>) would be a useful additions. Also, as you noticed, the documentation is not complete and too much focused on part-of-speech tagging, and it is not clear how to generalize from it. Fixing the docs is (also) on the todo list. </p>

<p>However, the interest for generalized (non-POS-tagging) tbl in nltk has been rather limited (the totally unsuited api of nltk2 was untouched for 10 years), so don't hold your breath. If you get impatient, you may wish to check out more dedicated alternatives,
in particular mutbl and fntbl (google them, I only have reputation for two links).</p>

<p>Anyway, here is a quick sketch for nltk:</p>

<p>First, a hardcoded convention in nltk is that tagged sequences ('tags' meaning any label
you would like to assign to your data, not necessarily part-of-speech) are represented
as sequences of pairs, [(token1, tag1), (token2, tag2), ...]. The tags are strings; in
many basic applications, so are the tokens. For instance, the tokens may be words
and the strings their POS, as in</p>

<pre><code>[('And', 'CC'), ('now', 'RB'), ('for', 'IN'), ('something', 'NN'), ('completely', 'RB'), ('different', 'JJ')]
</code></pre>

<p>(As an aside, this sequence-of-token-tag-pairs convention is pervasive in nltk and
its documentation, but it should arguably  be better expressed as named tuples
rather than pairs, so that instead of saying</p>

<pre><code>[token for (token, _tag) in tagged_sequence]
</code></pre>

<p>you could say for instance</p>

<pre><code>[x.token for x in tagged_sequence]
</code></pre>

<p>The first case fails on non-pairs, but the second exploits duck typing so
that tagged_sequence could be any sequence of user-defined instances, as long as
they have an attribute ""token"".)</p>

<p>Now, you could well have a richer representation of what a token is at your
disposal. An existing tagger interface (nltk.tag.api.FeaturesetTaggerI) expects
each token as a featureset rather than a string, which is a dictionary that maps
feature names to feature values for each item in the sequence.</p>

<p>A tagged sequence may then look like</p>

<pre><code>[({'word': 'Pierre', 'tag': 'NNP', 'iob': 'B-NP'}, 'NNP'),
 ({'word': 'Vinken', 'tag': 'NNP', 'iob': 'I-NP'}, 'NNP'),
 ({'word': ',',      'tag': ',',   'iob': 'O'   }, ','),
 ...
]
</code></pre>

<p>There are other possibilities (though with less support in the rest of nltk).
For instance, you could have a named tuple for each token, or a user-defined
class which allows you to add any amount of dynamic calculation to
attribute access (perhaps using @property to offer a consistent interface).</p>

<p>The brill tagger doesn't need to know what view you currently provide
on your tokens. However, it does require you to provide an initial tagger
which can take sequences of tokens-in-your-representation to sequences of
tags. You cannot use the existing taggers in nltk.tag.sequential directly,
since they expect [(word, tag), ...]. But you may still be able to
exploit them. The example below uses this strategy (in MyInitialTagger), and the token-as-featureset-dictionary view.</p>

<pre><code>from __future__ import division, print_function, unicode_literals

import sys

from nltk import tbl, untag
from nltk.tag.brill_trainer import BrillTaggerTrainer
# or: 
# from nltk.tag.brill_trainer_orig import BrillTaggerTrainer
# 100 templates and a tiny 500 sentences (11700 
# tokens) produce 420000 rules and uses a 
# whopping 1.3GB of memory on my system;
# brill_trainer_orig is much slower, but uses 0.43GB

from nltk.corpus import treebank_chunk
from nltk.chunk.util import tree2conlltags
from nltk.tag import DefaultTagger


def get_templates():
    wds10 = [[Word([0])],
             [Word([-1])],
             [Word([1])],
             [Word([-1]), Word([0])],
             [Word([0]), Word([1])],
             [Word([-1]), Word([1])],
             [Word([-2]), Word([-1])],
             [Word([1]), Word([2])],
             [Word([-1,-2,-3])],
             [Word([1,2,3])]]

    pos10 = [[POS([0])],
             [POS([-1])],
             [POS([1])],
             [POS([-1]), POS([0])],
             [POS([0]), POS([1])],
             [POS([-1]), POS([1])],
             [POS([-2]), POS([-1])],
             [POS([1]), POS([2])],
             [POS([-1, -2, -3])],
             [POS([1, 2, 3])]]

    iobs5 = [[IOB([0])],
             [IOB([-1]), IOB([0])],
             [IOB([0]), IOB([1])],
             [IOB([-2]), IOB([-1])],
             [IOB([1]), IOB([2])]]


    # the 5 * (10+10) = 100 3-feature templates 
    # of Ramshaw and Marcus
    templates = [tbl.Template(*wdspos+iob) 
        for wdspos in wds10+pos10 for iob in iobs5]
    # Footnote:
    # any template-generating functions in new code 
    # (as opposed to recreating templates from earlier
    # experiments like Ramshaw and Marcus) might 
    # also consider the mass generating Feature.expand()
    # and Template.expand(). See the docs, or for 
    # some examples the original pull request at
    # https://github.com/nltk/nltk/pull/549 
    # (""Feature- and Template-generating factory functions"")

    return templates

def build_multifeature_corpus():
    # The true value of the target fields is unknown in testing, 
    # and, of course, templates must not refer to it in training.
    # But we may wish to keep it for reference (here, truepos).

    def tuple2dict_featureset(sent, tagnames=(""word"", ""truepos"", ""iob"")):
        return (dict(zip(tagnames, t)) for t in sent)

    def tag_tokens(tokens):
        return [(t, t[""truepos""]) for t in tokens]
    # connlltagged_sents :: [[(word,tag,iob)]]
    connlltagged_sents = (tree2conlltags(sent) 
        for sent in treebank_chunk.chunked_sents())
    conlltagged_tokenses = (tuple2dict_featureset(sent) 
        for sent in connlltagged_sents)
    conlltagged_sequences = (tag_tokens(sent) 
        for sent in conlltagged_tokenses)
    return conlltagged_sequences

class Word(tbl.Feature):
    @staticmethod
    def extract_property(tokens, index):
        return tokens[index][0][""word""]

class IOB(tbl.Feature):
    @staticmethod
    def extract_property(tokens, index):
        return tokens[index][0][""iob""]

class POS(tbl.Feature):
    @staticmethod
    def extract_property(tokens, index):
        return tokens[index][1]


class MyInitialTagger(DefaultTagger):
    def choose_tag(self, tokens, index, history):
        tokens_ = [t[""word""] for t in tokens]
        return super().choose_tag(tokens_, index, history)


def main(argv):
    templates = get_templates()
    trainon = 100

    corpus = list(build_multifeature_corpus())
    train, test = corpus[:trainon], corpus[trainon:]

    print(train[0], ""\n"")

    initial_tagger = MyInitialTagger('NN')
    print(initial_tagger.tag(untag(train[0])), ""\n"")

    trainer = BrillTaggerTrainer(initial_tagger, templates, trace=3)
    tagger = trainer.train(train)

    taggedtest = tagger.tag_sents([untag(t) for t in test])
    print(test[0])
    print(initial_tagger.tag(untag(test[0])))
    print(taggedtest[0])
    print()

    tagger.print_template_statistics()

if __name__ == '__main__':
    sys.exit(main(sys.argv))
</code></pre>

<p>The setup above builds a POS tagger. If you instead wish to target another attribute, say to build an IOB tagger, you need a couple of small changes 
so that the target attribute (which you can think of as read-write) 
is accessed from the 'tag' position in your corpus  [(token, tag), ...]
and any other attributes (which you can think of as read-only)
are accessed from the 'token' position. For instance:</p>

<p>1) construct your corpus [(token,tag), (token,tag), ...] for IOB tagging</p>

<pre><code>def build_multifeature_corpus():
    ...

    def tuple2dict_featureset(sent, tagnames=(""word"", ""pos"", ""trueiob"")):
        return (dict(zip(tagnames, t)) for t in sent)

    def tag_tokens(tokens):
        return [(t, t[""trueiob""]) for t in tokens]
    ...
</code></pre>

<p>2) change the initial tagger accordingly</p>

<pre><code>...
initial_tagger = MyInitialTagger('O')
...
</code></pre>

<p>3) modify the feature-extracting class definitions</p>

<pre><code>class POS(tbl.Feature):
    @staticmethod
    def extract_property(tokens, index):
        return tokens[index][0][""pos""]

class IOB(tbl.Feature):
    @staticmethod
    def extract_property(tokens, index):
        return tokens[index][1]
</code></pre>
",""
"40498962","2016-11-09 01:11:02","2","","40480839","<p>Firstly, to chunk NEs with <code>ne_chunk</code>, the idiom would look something like this </p>

<pre><code>&gt;&gt;&gt; from nltk import ne_chunk, pos_tag, word_tokenize
&gt;&gt;&gt; text = ""Tom is the cofounder of Microsoft""
&gt;&gt;&gt; chunked = ne_chunk(pos_tag(word_tokenize(text)))
&gt;&gt;&gt; chunked
Tree('S', [Tree('PERSON', [('Tom', 'NNP')]), ('is', 'VBZ'), ('the', 'DT'), ('cofounder', 'NN'), ('of', 'IN'), Tree('ORGANIZATION', [('Microsoft', 'NNP')])])
</code></pre>

<p>(see also <a href=""https://stackoverflow.com/a/31838373/610569"">https://stackoverflow.com/a/31838373/610569</a>)</p>

<p>Next let's look at the <a href=""https://github.com/nltk/nltk/blob/develop/nltk/sem/relextract.py#L176"" rel=""noreferrer""><code>extract_rels</code> function</a>.</p>

<pre><code>def extract_rels(subjclass, objclass, doc, corpus='ace', pattern=None, window=10):
    """"""
    Filter the output of ``semi_rel2reldict`` according to specified NE classes and a filler pattern.
    The parameters ``subjclass`` and ``objclass`` can be used to restrict the
    Named Entities to particular types (any of 'LOCATION', 'ORGANIZATION',
    'PERSON', 'DURATION', 'DATE', 'CARDINAL', 'PERCENT', 'MONEY', 'MEASURE').
    """"""
</code></pre>

<p>When you evoke this function:</p>

<pre><code>extract_rels('PER', 'GPE', sent, corpus='ace', pattern=OF, window=10)
</code></pre>

<p>It performs 4 processes sequentially.</p>

<h1>1. It checks whether your <code>subjclass</code> and <code>objclass</code>are valid</h1>

<p>i.e. <a href=""https://github.com/nltk/nltk/blob/develop/nltk/sem/relextract.py#L202"" rel=""noreferrer"">https://github.com/nltk/nltk/blob/develop/nltk/sem/relextract.py#L202</a> :</p>

<pre><code>if subjclass and subjclass not in NE_CLASSES[corpus]:
    if _expand(subjclass) in NE_CLASSES[corpus]:
        subjclass = _expand(subjclass)
    else:
        raise ValueError(""your value for the subject type has not been recognized: %s"" % subjclass)
if objclass and objclass not in NE_CLASSES[corpus]:
    if _expand(objclass) in NE_CLASSES[corpus]:
        objclass = _expand(objclass)
    else:
        raise ValueError(""your value for the object type has not been recognized: %s"" % objclass)
</code></pre>

<h1>2. It extracts ""pairs"" from your NE tagged inputs:</h1>

<pre><code>if corpus == 'ace' or corpus == 'conll2002':
    pairs = tree2semi_rel(doc)
elif corpus == 'ieer':
    pairs = tree2semi_rel(doc.text) + tree2semi_rel(doc.headline)
else:
    raise ValueError(""corpus type not recognized"")
</code></pre>

<p>Now let's see given your input sentence <code>Tom is the cofounder of Microsoft</code>, what does <code>tree2semi_rel()</code> returns:</p>

<pre><code>&gt;&gt;&gt; from nltk.sem.relextract import tree2semi_rel, semi_rel2reldict
&gt;&gt;&gt; from nltk import word_tokenize, pos_tag, ne_chunk
&gt;&gt;&gt; text = ""Tom is the cofounder of Microsoft""
&gt;&gt;&gt; chunked = ne_chunk(pos_tag(word_tokenize(text)))
&gt;&gt;&gt; tree2semi_rel(chunked)
[[[], Tree('PERSON', [('Tom', 'NNP')])], [[('is', 'VBZ'), ('the', 'DT'), ('cofounder', 'NN'), ('of', 'IN')], Tree('ORGANIZATION', [('Microsoft', 'NNP')])]]
</code></pre>

<p>So it returns a list of 2 lists, the first inner list consist of a blank list and the <code>Tree</code> that contains the ""PERSON"" tag.</p>

<pre><code>[[], Tree('PERSON', [('Tom', 'NNP')])] 
</code></pre>

<p>The second list consist of the phrase <code>is the cofounder of</code> and the <code>Tree</code> that contains ""ORGANIZATION"".</p>

<p>Let's move on.</p>

<h1>3. <code>extract_rel</code> then tries to change the pairs to some sort of relation dictionary</h1>

<pre><code>reldicts = semi_rel2reldict(pairs)
</code></pre>

<p>If we look what the <code>semi_rel2reldict</code> function returns with your example sentence, we see that this is where the empty list gets returns:</p>

<pre><code>&gt;&gt;&gt; tree2semi_rel(chunked)
[[[], Tree('PERSON', [('Tom', 'NNP')])], [[('is', 'VBZ'), ('the', 'DT'), ('cofounder', 'NN'), ('of', 'IN')], Tree('ORGANIZATION', [('Microsoft', 'NNP')])]]
&gt;&gt;&gt; semi_rel2reldict(tree2semi_rel(chunked))
[]
</code></pre>

<p>So let's look into the code of <code>semi_rel2reldict</code> <a href=""https://github.com/nltk/nltk/blob/develop/nltk/sem/relextract.py#L144"" rel=""noreferrer"">https://github.com/nltk/nltk/blob/develop/nltk/sem/relextract.py#L144</a>:</p>

<pre><code>def semi_rel2reldict(pairs, window=5, trace=False):
    """"""
    Converts the pairs generated by ``tree2semi_rel`` into a 'reldict': a dictionary which
    stores information about the subject and object NEs plus the filler between them.
    Additionally, a left and right context of length =&lt; window are captured (within
    a given input sentence).
    :param pairs: a pair of list(str) and ``Tree``, as generated by
    :param window: a threshold for the number of items to include in the left and right context
    :type window: int
    :return: 'relation' dictionaries whose keys are 'lcon', 'subjclass', 'subjtext', 'subjsym', 'filler', objclass', objtext', 'objsym' and 'rcon'
    :rtype: list(defaultdict)
    """"""
    result = []
    while len(pairs) &gt; 2:
        reldict = defaultdict(str)
        reldict['lcon'] = _join(pairs[0][0][-window:])
        reldict['subjclass'] = pairs[0][1].label()
        reldict['subjtext'] = _join(pairs[0][1].leaves())
        reldict['subjsym'] = list2sym(pairs[0][1].leaves())
        reldict['filler'] = _join(pairs[1][0])
        reldict['untagged_filler'] = _join(pairs[1][0], untag=True)
        reldict['objclass'] = pairs[1][1].label()
        reldict['objtext'] = _join(pairs[1][1].leaves())
        reldict['objsym'] = list2sym(pairs[1][1].leaves())
        reldict['rcon'] = _join(pairs[2][0][:window])
        if trace:
            print(""(%s(%s, %s)"" % (reldict['untagged_filler'], reldict['subjclass'], reldict['objclass']))
        result.append(reldict)
        pairs = pairs[1:]
    return result
</code></pre>

<p>The first thing that <code>semi_rel2reldict()</code> does is to check where there are more than 2 elements the output from <code>tree2semi_rel()</code>, which your example sentence doesn't:</p>

<pre><code>&gt;&gt;&gt; tree2semi_rel(chunked)
[[[], Tree('PERSON', [('Tom', 'NNP')])], [[('is', 'VBZ'), ('the', 'DT'), ('cofounder', 'NN'), ('of', 'IN')], Tree('ORGANIZATION', [('Microsoft', 'NNP')])]]
&gt;&gt;&gt; len(tree2semi_rel(chunked))
2
&gt;&gt;&gt; len(tree2semi_rel(chunked)) &gt; 2
False
</code></pre>

<p>Ah ha, that's why the <code>extract_rel</code> is returning nothing.</p>

<p><strong>Now comes the question of how to make <code>extract_rel()</code> return something even with 2 elements from <code>tree2semi_rel()</code>?</strong> Is that even possible?</p>

<p>Let's try a different sentence:</p>

<pre><code>&gt;&gt;&gt; text = ""Tom is the cofounder of Microsoft and now he is the founder of Marcohard""
&gt;&gt;&gt; chunked = ne_chunk(pos_tag(word_tokenize(text)))
&gt;&gt;&gt; chunked
Tree('S', [Tree('PERSON', [('Tom', 'NNP')]), ('is', 'VBZ'), ('the', 'DT'), ('cofounder', 'NN'), ('of', 'IN'), Tree('ORGANIZATION', [('Microsoft', 'NNP')]), ('and', 'CC'), ('now', 'RB'), ('he', 'PRP'), ('is', 'VBZ'), ('the', 'DT'), ('founder', 'NN'), ('of', 'IN'), Tree('PERSON', [('Marcohard', 'NNP')])])
&gt;&gt;&gt; tree2semi_rel(chunked)
[[[], Tree('PERSON', [('Tom', 'NNP')])], [[('is', 'VBZ'), ('the', 'DT'), ('cofounder', 'NN'), ('of', 'IN')], Tree('ORGANIZATION', [('Microsoft', 'NNP')])], [[('and', 'CC'), ('now', 'RB'), ('he', 'PRP'), ('is', 'VBZ'), ('the', 'DT'), ('founder', 'NN'), ('of', 'IN')], Tree('PERSON', [('Marcohard', 'NNP')])]]
&gt;&gt;&gt; len(tree2semi_rel(chunked)) &gt; 2
True
&gt;&gt;&gt; semi_rel2reldict(tree2semi_rel(chunked))
[defaultdict(&lt;type 'str'&gt;, {'lcon': '', 'untagged_filler': 'is the cofounder of', 'filler': 'is/VBZ the/DT cofounder/NN of/IN', 'objsym': 'microsoft', 'objclass': 'ORGANIZATION', 'objtext': 'Microsoft/NNP', 'subjsym': 'tom', 'subjclass': 'PERSON', 'rcon': 'and/CC now/RB he/PRP is/VBZ the/DT', 'subjtext': 'Tom/NNP'})]
</code></pre>

<p><strong>But that only confirms that <code>extract_rel</code> can't extract when <code>tree2semi_rel</code> returns pairs of &lt; 2. What happens if we remove that condition of <code>while len(pairs) &gt; 2</code>?</strong></p>

<p>Why can't we do <code>while len(pairs) &gt; 1</code>?</p>

<p>If we look closer into the code, we see the last line of populating the reldict, <a href=""https://github.com/nltk/nltk/blob/develop/nltk/sem/relextract.py#L169"" rel=""noreferrer"">https://github.com/nltk/nltk/blob/develop/nltk/sem/relextract.py#L169</a>:</p>

<pre><code>reldict['rcon'] = _join(pairs[2][0][:window])
</code></pre>

<p>It tries to access a 3rd element of the <code>pairs</code> and if the length of the <code>pairs</code> is 2, you'll get an <code>IndexError</code>.</p>

<p><strong>So what happens if we remove that <code>rcon</code> key and simply change it to <code>while len(pairs) &gt;= 2</code>?</strong></p>

<p>To do that we have to override the <code>semi_rel2redict()</code> function:</p>

<pre><code>&gt;&gt;&gt; from nltk.sem.relextract import _join, list2sym
&gt;&gt;&gt; from collections import defaultdict
&gt;&gt;&gt; def semi_rel2reldict(pairs, window=5, trace=False):
...     """"""
...     Converts the pairs generated by ``tree2semi_rel`` into a 'reldict': a dictionary which
...     stores information about the subject and object NEs plus the filler between them.
...     Additionally, a left and right context of length =&lt; window are captured (within
...     a given input sentence).
...     :param pairs: a pair of list(str) and ``Tree``, as generated by
...     :param window: a threshold for the number of items to include in the left and right context
...     :type window: int
...     :return: 'relation' dictionaries whose keys are 'lcon', 'subjclass', 'subjtext', 'subjsym', 'filler', objclass', objtext', 'objsym' and 'rcon'
...     :rtype: list(defaultdict)
...     """"""
...     result = []
...     while len(pairs) &gt;= 2:
...         reldict = defaultdict(str)
...         reldict['lcon'] = _join(pairs[0][0][-window:])
...         reldict['subjclass'] = pairs[0][1].label()
...         reldict['subjtext'] = _join(pairs[0][1].leaves())
...         reldict['subjsym'] = list2sym(pairs[0][1].leaves())
...         reldict['filler'] = _join(pairs[1][0])
...         reldict['untagged_filler'] = _join(pairs[1][0], untag=True)
...         reldict['objclass'] = pairs[1][1].label()
...         reldict['objtext'] = _join(pairs[1][1].leaves())
...         reldict['objsym'] = list2sym(pairs[1][1].leaves())
...         reldict['rcon'] = []
...         if trace:
...             print(""(%s(%s, %s)"" % (reldict['untagged_filler'], reldict['subjclass'], reldict['objclass']))
...         result.append(reldict)
...         pairs = pairs[1:]
...     return result
... 
&gt;&gt;&gt; text = ""Tom is the cofounder of Microsoft""
&gt;&gt;&gt; chunked = ne_chunk(pos_tag(word_tokenize(text)))
&gt;&gt;&gt; tree2semi_rel(chunked)
[[[], Tree('PERSON', [('Tom', 'NNP')])], [[('is', 'VBZ'), ('the', 'DT'), ('cofounder', 'NN'), ('of', 'IN')], Tree('ORGANIZATION', [('Microsoft', 'NNP')])]]
&gt;&gt;&gt; semi_rel2reldict(tree2semi_rel(chunked))
[defaultdict(&lt;type 'str'&gt;, {'lcon': '', 'untagged_filler': 'is the cofounder of', 'filler': 'is/VBZ the/DT cofounder/NN of/IN', 'objsym': 'microsoft', 'objclass': 'ORGANIZATION', 'objtext': 'Microsoft/NNP', 'subjsym': 'tom', 'subjclass': 'PERSON', 'rcon': [], 'subjtext': 'Tom/NNP'})]
</code></pre>

<p>Ah! It works but there's still a 4th step in <code>extract_rels()</code>.</p>

<h1>4. It performs a filter of the reldict given the regex you have provided to the <code>pattern</code> parameter, <a href=""https://github.com/nltk/nltk/blob/develop/nltk/sem/relextract.py#L222"" rel=""noreferrer"">https://github.com/nltk/nltk/blob/develop/nltk/sem/relextract.py#L222</a>:</h1>

<pre><code>relfilter = lambda x: (x['subjclass'] == subjclass and
                       len(x['filler'].split()) &lt;= window and
                       pattern.match(x['filler']) and
                       x['objclass'] == objclass)
</code></pre>

<p>Now let's try it with the hacked version of <code>semi_rel2reldict</code>:</p>

<pre><code>&gt;&gt;&gt; text = ""Tom is the cofounder of Microsoft""
&gt;&gt;&gt; chunked = ne_chunk(pos_tag(word_tokenize(text)))
&gt;&gt;&gt; tree2semi_rel(chunked)
[[[], Tree('PERSON', [('Tom', 'NNP')])], [[('is', 'VBZ'), ('the', 'DT'), ('cofounder', 'NN'), ('of', 'IN')], Tree('ORGANIZATION', [('Microsoft', 'NNP')])]]
&gt;&gt;&gt; semi_rel2reldict(tree2semi_rel(chunked))
[defaultdict(&lt;type 'str'&gt;, {'lcon': '', 'untagged_filler': 'is the cofounder of', 'filler': 'is/VBZ the/DT cofounder/NN of/IN', 'objsym': 'microsoft', 'objclass': 'ORGANIZATION', 'objtext': 'Microsoft/NNP', 'subjsym': 'tom', 'subjclass': 'PERSON', 'rcon': [], 'subjtext': 'Tom/NNP'})]
&gt;&gt;&gt; 
&gt;&gt;&gt; pattern = re.compile(r'.*\bof\b.*')
&gt;&gt;&gt; reldicts = semi_rel2reldict(tree2semi_rel(chunked))
&gt;&gt;&gt; relfilter = lambda x: (x['subjclass'] == subjclass and
...                            len(x['filler'].split()) &lt;= window and
...                            pattern.match(x['filler']) and
...                            x['objclass'] == objclass)
&gt;&gt;&gt; relfilter
&lt;function &lt;lambda&gt; at 0x112e591b8&gt;
&gt;&gt;&gt; subjclass = 'PERSON'
&gt;&gt;&gt; objclass = 'ORGANIZATION'
&gt;&gt;&gt; window = 5
&gt;&gt;&gt; list(filter(relfilter, reldicts))
[defaultdict(&lt;type 'str'&gt;, {'lcon': '', 'untagged_filler': 'is the cofounder of', 'filler': 'is/VBZ the/DT cofounder/NN of/IN', 'objsym': 'microsoft', 'objclass': 'ORGANIZATION', 'objtext': 'Microsoft/NNP', 'subjsym': 'tom', 'subjclass': 'PERSON', 'rcon': [], 'subjtext': 'Tom/NNP'})]
</code></pre>

<p>It works! Now let's see it in tuple form:</p>

<pre><code>&gt;&gt;&gt; from nltk.sem.relextract import rtuple
&gt;&gt;&gt; rels = list(filter(relfilter, reldicts))
&gt;&gt;&gt; for rel in rels:
...     print rtuple(rel)
... 
[PER: 'Tom/NNP'] 'is/VBZ the/DT cofounder/NN of/IN' [ORG: 'Microsoft/NNP']
</code></pre>
",""
"40460500","2016-11-07 08:11:12","1","","40460401","<p>You could train <a href=""https://code.google.com/archive/p/word2vec/"" rel=""nofollow noreferrer"">word2vec</a> on the product titles. Resulting code would look something like this when using <a href=""https://github.com/danielfrg/word2vec"" rel=""nofollow noreferrer"">the Python word2vec wrapper</a> and slightly different but similar when using <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""nofollow noreferrer"">Gensim's model.word2vec</a>:</p>

<pre><code>indexes, metrics = model.cosine(normalized_phrase)
model.generate_response(indexes, metrics)
</code></pre>

<p>The generated response will be the title vectors sorted by descending cosine similarity.</p>
",""
"40452582","2016-11-06 17:54:53","1","","40452180","<p>An idea...</p>

<p>You can split the lines by the '->' delimiter and trim spaces:</p>

<pre><code>line_items = [x.strip() for x in line.split('-&gt;')]

# Splits 'VP -&gt; V PP' into ['VP', 'V PP']
</code></pre>

<p>Then you can look up your input <code>factor</code> in the second item of this array and return the first item with something like this:</p>

<pre><code>for line in grammarFile:
    line_items = [x.strip() for x in line.split('-&gt;')]
    if factor == line_items[1]:
        return line_items[0:1]
</code></pre>

<p>I am not sure what grammarFile is exactly (bytes? strings?) but something like this could work.</p>

<p>I hope this helps.</p>
",""
"40423640","2016-11-04 13:15:01","0","","40423151","<p>The purpose of a Lucene analyzer is to take character sequences from a source (provided by a parsing procedure) and produce a token stream according to the intended analysis. The EnglishAnalyzer performs stemming by default, yielding tokens that are not necessarily valid English words. In that case, a token ""repli"" may be the output for multiple English words holding the same semantics: ""reply"", ""replied"", ""replying"", ""replies"".</p>

<p>Therefore, a short answer would be ""it doesn't matter, words were conveniently stemmed by the analyzer"". If this is not the intended behaviour, you should be able to make a custom analyzer using a different stemmer (or removing it altogether). See the <a href=""http://lucene.apache.org/core/6_2_1/analyzers-common/index.html"" rel=""nofollow noreferrer"">documentation on analyzers</a> for additional information and a list of stemmers made available.</p>
",""
"40359768","2016-11-01 12:08:24","1","","40357411","<p>You are mixing apples and oranges. Only your first two expansions are valid nltk <code>RegexpParser</code> rules, so you get an error on the third. Convert the rest to the same format: Change the separator from <code>-&gt;</code> to <code>:</code>, then write the expansions as <a href=""http://www.nltk.org/api/nltk.chunk.html#nltk.chunk.regexp.RegexpParser"" rel=""nofollow noreferrer""><code>RegexpParser</code></a> expressions. Note that you are working with a chunker, not a hierarchical parser. (See the above documentation, and also all of <a href=""http://www.nltk.org/book/ch07.html"" rel=""nofollow noreferrer"">Chapter 7</a> of the NLTK book.)</p>
",""
"40328877","2016-10-30 12:08:58","0","","40328503","<p>You can use <strong>sub</strong> method (module re):</p>

<pre><code>import re

def replace_dbquote(render):
    return '[OPEN]' + render.group(0).replace('""', '') + '[CLOSE]'

string = '""Death to the traitors!"" cried the exasperated burghers. ""Go along with you"", growled the officer.'
parser = re.sub('""[^""]*""', replace_dbquote, string)

print(parser)
</code></pre>

<p><a href=""https://docs.python.org/3.5/library/re.html#re.sub"" rel=""noreferrer"">https://docs.python.org/3.5/library/re.html#re.sub</a></p>
",""
"40243925","2016-10-25 15:28:08","0","","38545726","<p>I can't help you to mimic the grammar-checking abilities of AbiWord using Python bindings, but I can at least help you to build it and check out its functionalities.</p>

<h1>Building with MS Visual Studio (32-bit architecture)</h1>

<p>I'd normally say that ""the best method to achieve this"" is to build the Link Grammar library and Python bindings on a Linux machine following the extensive instructions in their <a href=""https://raw.githubusercontent.com/opencog/link-grammar/master/README"" rel=""nofollow"">readme file</a>. However, judging by your <a href=""https://stackoverflow.com/questions/38545726/how-to-use-the-link-grammar-parser-as-a-grammar-checker#comment-64831003"">comment above</a>, Linux may not be an option, and it seems you want to stick to using Visual Studio over using e.g. <a href=""https://www.cygwin.com/"" rel=""nofollow"">Cygwin</a>.</p>

<h2>Dependencies</h2>

<h3>Regex</h3>

<p>As stated in the readme, the Link Grammar library depends on some form of POSIX-compliant regex library &mdash; on Linux, this is baked-in. However, in Windows, you get to (or rather have to) choose an implementation of the library to use. Luckily, version 2.7 of <a href=""http://gnuwin32.sourceforge.net/packages/regex.htm#download"" rel=""nofollow"">the port provided by GnuWin</a> played nicely with the Visual Studio solution/project files provided by Link Grammar 5.3.11 (found under <code>%LINK_GRAMMAR%\msvc14</code>).</p>

<p>However, you have to ensure that the Visual Studio build macro <code>GNUREGEX_DIR</code> points to the directory you unpacked the regex library to (e.g. <code>D:\Program Files (x86)\GnuWin32</code>). Note, however, that these build macros are <em>not</em> the same as Windows environment variables: Despite setting an environment variable under Windows 10 called <code>GNUREGEX_DIR</code>, Visual Studio did <em>not</em> make use of this variable until I changed the definition of the build macros in the Link Grammar project files, namely, in <code>%LINK_GRAMMAR%\msvc14\Local.props</code> the line:</p>

<pre><code>&lt;GNUREGEX_DIR&gt;$(HOMEDRIVE)$(HOMEPATH)\Libraries\gnuregex&lt;/GNUREGEX_DIR&gt;
</code></pre>

<p>to</p>

<pre><code>&lt;GNUREGEX_DIR&gt;$(GNUREGEX_DIR)&lt;/GNUREGEX_DIR&gt;
</code></pre>

<h3>SWIG</h3>

<p>In order to create Python bindings, you need to have <a href=""http://www.swig.org/"" rel=""nofollow"">SWIG</a> on your system. However, in order for the build defined by the Visual Studio project <code>Python2.vcxproj</code> to find the SWIG executable, you need to add the respective directory to the Windows path, e.g. <code>D:\Program Files (x86)\swigwin-3.0.10</code>.</p>

<p>Just as with the regex library, you need to configure the VS project in order to be able to locate your Python directory, e.g. change <code>&lt;PYTHON2&gt;C:\Python27&lt;/PYTHON2&gt;</code> in <code>Local.props</code> to <code>&lt;PYTHON2&gt;$(PYTHON2)&lt;/PYTHON2&gt;</code> if you have a corresponding environment variable set.</p>

<h2>Building</h2>

<p>Once all the above libraries can be found by Visual Studio, the build process is pretty painless: Just build the project <code>Python2</code>, and if you have the VS solution file open (<code>LinkGrammar.sln</code>), it should automatically build the projects <code>LinkGrammar</code> and <code>LinkGrammarExe</code>, which it depends on.</p>

<h2>Resolving shared libraries</h2>

<p>After building the executable, you still need to ensure that the regex shared library (DLL) can be found: In order to do this, the the directory containing the required library (in this case, <code>regex2.dll</code>) should be on your path. It is probably easiest to add the directory to your global path, e.g. <code>%GNUREGEX_DIR%\bin""</code> in the case of using the GnuWin library mentioned above with the environment variable <code>GNUREGEX_DIR</code> pointing to it.</p>

<h2>Running with Python</h2>

<p>Now that you have tested that Windows executable does run and the Python bindings have been built, you can then import them into a Python script. In order to ensure they are correctly imported and SWIG has correctly located the appropriate DLLs, the Link Grammar readme mentions running the executable script <code>make-check.py</code> to load and run your script using Link Grammar:</p>

<pre><code>make-check [PYTHON_FLAG] PYTHON_OUTDIR [script.py] [ARGUMENTS]
</code></pre>

<p>where <code>OUTDIR</code> is the directory to which your Python bindings were written, e.g. <code>Win32\Debug\Python2</code>. Unfortunately, however, despite that this file is mentioned in the readme for version 5.3.11, it is, in fact, not present in <a href=""http://www.abisource.com/downloads/link-grammar/5.3.11/link-grammar-5.3.11.tar.gz"" rel=""nofollow"">the ""stable"" version 5.3.11 distributable</a> &mdash; despite that there is <a href=""https://github.com/opencog/link-grammar/blob/master/msvc14/make-check.py"" rel=""nofollow"">a version of it in the GitHub master repository</a>. You can, however, simply get that one file from the Git repository and then use it in the <code>msvc14</code> directory of your 5.3.11 distributable. As stated above, however, this script requires that <code>regex2.dll</code> be on the Windows path: If it hasn't been added to the global path, you will have to add it to the path accessible to the Python executable when running the script.</p>

<h1>C API vs. Python API</h1>

<p>I haven't used the Link Grammar parser much myself and so can't help you there, but you can still get an idea how to use them by looking at the C code for the project <code>LinkGrammarExe</code>. You can start by looking at the main function in <code>link-parser\link-parser.c</code>:</p>

<pre><code>sent = sentence_create(input_string, dict);

...

num_linkages = sentence_parse(sent, opts);
</code></pre>

<p>In the simple CLI program built by the VS project, it simply checks <code>num_linkages</code> and, if the value thereof is <code>0</code>, it displays <code>No complete linkages found</code>, which a user can interpret as meaning that the sentence is ungrammatical. This behavior can of course be tweaked to accept lower-scoring parses, find the word(s) which don't fit, etc., and so you can explore the functionalities using the C API first. Later, if you really want to use the Python bindings, the Python methods are named similarly to their C counterparts &mdash; see the file <code>clinkgrammar.py</code>:</p>

<pre><code>def sentence_parse(sent, opts):
    return _clinkgrammar.sentence_parse(sent, opts)
    sentence_parse = _clinkgrammar.sentence_parse
</code></pre>
",""
"40167806","2016-10-21 03:18:36","1","","40167612","<p>First your list is a result of not well tokenized text so i tokenized them again
then search <code>pos</code> of all words to find nouns which pos contains NN :</p>

<pre><code>&gt;&gt;&gt; text=' '.join(wordlist).lower()
&gt;&gt;&gt; tokens = nltk.word_tokenize(text)
&gt;&gt;&gt; tags = nltk.pos_tag(tokens)
&gt;&gt;&gt; nouns = [word for word,pos in tags if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS')
]
&gt;&gt;&gt; nouns
['country', 'drinks', 'people', 'Embassy', 'elections']
</code></pre>
",""
"40105858","2016-10-18 10:31:51","1","","40101873","<p>The first column contains the tuples <code>(ind_document, ind_word)</code> where <code>ind_document</code> is the index of your document (in your case a <code>string</code>) contained in your data set, and <code>ind_word</code> the index of the word in the dictionary of words generated by the <code>TfidfVectorizer</code> object.</p>
<p>The second column contains the TF-IDF value of your given <code>word</code> (the word corresponding to <code>(ind_document, ind_word)</code>.</p>
<hr />
<p><strong>UPDATE</strong></p>
<p>If you look closer to the implementation of <code>TfidfVectorizer </code><a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"" rel=""nofollow noreferrer"">here</a>, you can see that there is a parameter called <code>norm</code>. By <strong>default</strong> this parameter is set to <code>l2</code> which is the L2-norm used to normalize the data obtained.</p>
<p>If you don't want to normalize your data and compare it to the results obtained manually <strong>change this parameter</strong> to <code>norm = None</code></p>
",""
"40095843","2016-10-17 21:20:58","2","","40094400","<p>This situation happens often when the true sentences are relatively rare in the data. </p>

<p>1) Get a corpus of sentences that resemble what you will be classifying in the end. The corpus will contain both true and false sentences. Label them as false or non-fact check. We are assuming they are all false even though we know this is not the case. You want the ratio of true/false data created to be approximately its actual distribution if at all possible. So if 10% are true in real data then your assumed false cases are 90% or 9,000 for you 1,000 trues. If you don't know the distribution then just make it 10x or more. </p>

<p>2) Train logistic regression classifier aka maximum entropy on the data with cross validation. Keep track of the high scoring false positives on the held out data.</p>

<p>3) Re-annotate false positives down to what ever score makes sense for possibly being true positives. This will hopefully clean your assumed false data. </p>

<p>4) Keep running this process until you are no longer improving the classifier. </p>

<p>5) To get your ""fact check words"" then make sure your feature extractor is feeding words to your classifier and look for those that are positively associated with the true category--any decent logistic regression classifier should provide the feature weights in some way. I use LingPipe which certainly does. </p>

<p>6) I don't see how PoS (Part of Speech) helps with this problem. </p>

<p>This approach will fail to find true instances that are very different from the training data but it can work none the less.</p>

<p>Breck</p>
",""
"40002540","2016-10-12 15:28:55","0","","29182611","<p>Malt parser works based on a transition system and 2 or three stacks. At each step a transition is predicted using liblinear or libsvm. The input to these models is composed of what is in the stacks and the current state of the machine. So making a decision at one step affects the rest of the possible decisions. To compute the probability of a tree would require to compute the aggregated probabilites of all trees (so that they sum up to 1), which is infeasable. You could compute a trust score of a tree, I guess, or of a particular arc, but it would be a trust score, not a probability. And afaik maltparser doesn't offer this out of the box. You would have to alter the source code, but it is do-able I think</p>
",""
"39930029","2016-10-08 07:50:26","0","","39928277","<p>You could always use a regular expressions.
The regex <code>(\S+)\s(\S+)\s\bwas born on\b\s(\S+)\s(\S+),\s(\S+)</code> will match and return data from specifically the string format above.</p>

<p>Here's it in action: <a href=""https://regex101.com/r/W2ykKS/1"" rel=""nofollow"">https://regex101.com/r/W2ykKS/1</a></p>

<p>Regex in python:</p>

<pre><code>import re

regex = r""(\S+)\s(\S+)\s\bwas born on\b\s(\S+)\s(\S+),\s(\S+)""
test_str = ""James Smith was born on November 17, 1948""

matches = re.search(regex, test_str)

# group 0 in a regex is the input string

print(matches.group(1)) # James
print(matches.group(2)) # Smith
print(matches.group(3)) # November
print(matches.group(4)) # 17
print(matches.group(5)) # 1948
</code></pre>
",""
"39369045","2016-09-07 11:45:30","0","","39355994","<p>Null region means empty region, no letters. You missed the examples in the <a href=""http://snowball.tartarus.org/texts/r1r2.html"" rel=""nofollow"">documentation page</a>:</p>

<blockquote>
  <p>Below, R1 and R2 are shown for a number of English words,</p>

<pre><code>b   e   a   u   t   i   f   u   l
                  |&lt;-------------&gt;|    R1
                          |&lt;-----&gt;|    R2
</code></pre>
  
  <p>Letter t is the first non-vowel following a vowel in beautiful, so R1
  is iful. In iful, the letter f is the first non-vowel following a
  vowel, so R2 is ul. </p>

<pre><code>b   e   a   u   t   y
                  |&lt;-&gt;|    R1
                    -&gt;|&lt;-  R2 
</code></pre>
  
  <p>In beauty, the last letter y is classed as a vowel. Again, letter t is the first non-vowel following a
  vowel, so R1 is just the last letter, y. R1 contains no non-vowel, so
  R2 is the null region at the end of the word. </p>

<pre><code>b   e   a   u
            -&gt;|&lt;-  R1
            -&gt;|&lt;-  R2
</code></pre>
</blockquote>
",""
"39355909","2016-09-06 18:47:38","0","","39353348","<p>The problem is that in your loop, <code>row</code> is a pandas <code>Series</code> rather than a list. You can access the list of words by writing <code>row[0]</code> instead:</p>

<pre><code>&gt;&gt;&gt; for  index, row in df.iterrows():
&gt;&gt;&gt;     noun.append([word for word,pos in pos_tag(row[0]) if pos == 'NN'])
&gt;&gt;&gt; print(noun)
[['day'], ['night']]
</code></pre>

<p>Here you're getting a list of lists, with each list containing the nouns from one sentence. If you really want a flat list (as in the sample result in your question), write <code>noun.extend(...)</code> instead of <code>noun.append</code>. </p>
",""
"39349493","2016-09-06 12:52:30","1","","39349400","<p>The bracket means you have lists in each cell of the data frame. If you are sure there is only one element at most in each list, you can use <code>str</code> on the noun column and extract the first element:</p>

<pre><code>df['noun'] = df.noun.str[0]

df
#    pos    noun
#0  noun    noun
#1  Alice   Alice
#2  good    NaN
#3  well    NaN
#4  city    city
</code></pre>
",""
"39342032","2016-09-06 06:33:38","5","","39340907","<p>You can traverse over <code>dep.triples()</code> and get your desired output.</p>

<p><strong>Code:</strong></p>

<pre><code>for triple in dep.triples():
    print triple[1],""("",triple[0][0],"", "",triple[2][0],"")""
</code></pre>

<p><strong>Output:</strong></p>

<pre><code>nsubj ( shot ,  I )
dobj ( shot ,  elephant )
det ( elephant ,  an )
nmod ( shot ,  sleep )
case ( sleep ,  in )
nmod:poss ( sleep ,  my )
</code></pre>

<p>For more information you can check : <a href=""http://www.nltk.org/_modules/nltk/parse/dependencygraph.html"" rel=""nofollow"">NLTK Dependencygraph</a> methods <code>triples()</code>, <code>to_dot()</code> and <code>dep.tree().draw()</code></p>

<p>Edit - </p>

<p>The output of <code>dep.to_dot()</code> is</p>

<pre><code>digraph G{
edge [dir=forward]
node [shape=plaintext]

0 [label=""0 (None)""]
0 -&gt; 2 [label=""root""]
1 [label=""1 (I)""]
2 [label=""2 (shot)""]
2 -&gt; 4 [label=""dobj""]
2 -&gt; 7 [label=""nmod""]
2 -&gt; 1 [label=""nsubj""]
3 [label=""3 (an)""]
4 [label=""4 (elephant)""]
4 -&gt; 3 [label=""det""]
5 [label=""5 (in)""]
6 [label=""6 (my)""]
7 [label=""7 (sleep)""]
7 -&gt; 5 [label=""case""]
7 -&gt; 6 [label=""nmod:poss""]
}
</code></pre>
",""
"39321548","2016-09-04 21:13:31","0","","39321495","<p>You can actually pass <code>Series</code> object to the <code>pos_tag()</code> method directly:</p>

<pre><code>s = df['pos']
tagged_sent = pos_tag(s)  # or pos_tag(s.tolist())
print(tagged_sent)
</code></pre>

<p>Prints:</p>

<pre><code>[('noun', 'JJ'), ('Alice', 'NNP'), ('good', 'JJ'), ('well', 'RB'), ('city', 'NN')]
</code></pre>
",""
"39320379","2016-09-04 18:50:54","5","","39320015","<p>You can use <code>Tree.subtrees()</code>. For more information check <a href=""http://www.nltk.org/_modules/nltk/tree.html"" rel=""noreferrer"">NLTK Tree Class</a>.</p>

<p><strong>Code:</strong></p>

<pre><code>from nltk import Tree

parse_str = ""(ROOT (S (NP (PRP You)) (VP (MD could) (VP (VB say) (SBAR (IN that) (S (NP (PRP they)) (ADVP (RB regularly)) (VP (VB catch) (NP (NP (DT a) (NN shower)) (, ,) (SBAR (WHNP (WDT which)) (S (VP (VBZ adds) (PP (TO to) (NP (NP (PRP$ their) (NN exhilaration)) (CC and) (NP (FW joie) (FW de) (FW vivre))))))))))))) (. .)))""
#parse_str = ""(ROOT (S (SBAR (IN Though) (S (NP (PRP he)) (VP (VBD was) (ADJP (RB very) (JJ rich))))) (, ,) (NP (PRP he)) (VP (VBD was) (ADVP (RB still)) (ADJP (RB very) (JJ unhappy))) (. .)))""

t = Tree.fromstring(parse_str)
#print t

subtexts = []
for subtree in t.subtrees():
    if subtree.label()==""S"" or subtree.label()==""SBAR"":
        #print subtree.leaves()
        subtexts.append(' '.join(subtree.leaves()))
#print subtexts

presubtexts = subtexts[:]       # ADDED IN EDIT for leftover check

for i in reversed(range(len(subtexts)-1)):
    subtexts[i] = subtexts[i][0:subtexts[i].index(subtexts[i+1])]

for text in subtexts:
    print text

# ADDED IN EDIT - Not sure for generalized cases
leftover = presubtexts[0][presubtexts[0].index(presubtexts[1])+len(presubtexts[1]):]
print leftover
</code></pre>

<p><strong>Output:</strong></p>

<pre><code>You could say 
that 
they regularly catch a shower , 
which 
adds to their exhilaration and joie de vivre
 .
</code></pre>
",""
"39303494","2016-09-03 05:15:46","8","","39302880","<p>Use SnowballStemmer:</p>

<pre><code>&gt;&gt;&gt; from nltk.stem.snowball import SnowballStemmer
&gt;&gt;&gt; stemmer = SnowballStemmer(""english"")
&gt;&gt;&gt; print(stemmer.stem(""generalized""))
general
&gt;&gt;&gt; print(stemmer.stem(""generalization""))
general
</code></pre>

<blockquote>
  <p>Note: Lemmatisation is closely related to stemming. The difference is
  that a stemmer operates on a single word without knowledge of the
  context, and therefore cannot discriminate between words which have
  different meanings depending on part of speech.</p>
</blockquote>

<p>A general issue I have seen with lemmatizers is that it identifies even bigger words as <em>lemma</em>s.</p>

<p>Example: 
In WordNet Lemmatizer(checked in NLTK), </p>

<ul>
<li>Genralized => Generalize</li>
<li>Generalization => Generalization </li>
<li>Generalizations => Generalization</li>
</ul>

<p>POS tag was not given as input in the above cases, so it was always considered <em>noun</em>.</p>
",""
"39210557","2016-08-29 16:05:03","3","","39210008","<p>You forgot to use <code>export</code> in the command line before calling your python script. I.e. </p>

<pre><code>alvas@ubi:~$ export STANFORDTOOLSDIR=$HOME
alvas@ubi:~$ export CLASSPATH=$STANFORDTOOLSDIR/stanford-postagger-full-2015-12-09/stanford-postagger.jar
alvas@ubi:~$ export STANFORD_MODELS=$STANFORDTOOLSDIR/stanford-postagger-full-2015-12-09/models
alvas@ubi:~$ python
</code></pre>

<p>For more details see <a href=""https://gist.github.com/alvations/e1df0ba227e542955a8a"" rel=""nofollow noreferrer"">https://gist.github.com/alvations/e1df0ba227e542955a8a</a></p>

<hr>

<p>Similar problems includes:</p>

<ul>
<li><a href=""https://stackoverflow.com/questions/34037094/setting-nltk-with-stanford-nlp-both-stanfordnertagger-and-stanfordpostagger-fo"">Setting NLTK with Stanford NLP (both StanfordNERTagger and StanfordPOSTagger) for Spanish</a></li>
<li><a href=""https://stackoverflow.com/questions/22930328/error-using-stanford-pos-tagger-in-nltk-python"">Error using Stanford POS Tagger in NLTK Python</a></li>
<li><a href=""https://stackoverflow.com/questions/34692987/cant-make-stanford-pos-tagger-working-in-nltk"">Can&#39;t make Stanford POS tagger working in nltk</a></li>
<li><a href=""https://stackoverflow.com/questions/7344916/trouble-importing-stanford-pos-tagger-into-nltk"">trouble importing stanford pos tagger into nltk</a></li>
<li><a href=""https://stackoverflow.com/questions/13883277/stanford-parser-and-nltk"">Stanford Parser and NLTK</a></li>
</ul>
",""
"39190391","2016-08-28 10:35:06","1","","38045290","<p>In general:</p>

<p><strong>Bleu measures precision</strong>: how much the words (and/or n-grams) in the <em>machine generated summaries</em> appeared in the human reference summaries.</p>

<p><strong>Rouge measures recall</strong>: how much the words (and/or n-grams) in the <em>human reference summaries</em> appeared in the machine generated summaries.</p>

<p>Naturally - these results are complementing, as is often the case in precision vs recall. If you have many words from the system results appearing in the human references you will have high Bleu, and if you have many words from the human references appearing in the system results you will have high Rouge.</p>

<p>In your case it would appear that sys1 has a higher Rouge than sys2 since the results in sys1 consistently had more words from the human references appear in them than the results from sys2. However, since your Bleu score showed that sys1 has lower recall than sys2, this would suggest that not so many words from your sys1 results appeared in the human references, in respect to sys2.</p>

<p>This could happen for example if your sys1 is outputting results which contain words from the references (upping the Rouge), but also many words which the references didn't include (lowering the Bleu). sys2, as it seems, is giving results for which most words outputted do appear in the human references (upping the Blue), but also missing many words from its results which do appear in the human references.</p>

<p>BTW, there's something called <strong>brevity penalty</strong>, which is quite important and has already been added to standard Bleu implementations. It penalizes system results which are <em>shorter</em> than the general length of a reference (read more about it <a href=""http://www1.cs.columbia.edu/nlp/sgd/bleu.pdf"" rel=""noreferrer"">here</a>). This complements the n-gram metric behavior which in effect penalizes longer than reference results, since the denominator grows the longer the system result is.</p>

<p>You could also implement something similar for Rouge, but this time penalizing system results which are <em>longer</em> than the general reference length, which would otherwise enable them to obtain artificially higher Rouge scores (since the longer the result, the higher the chance you would hit some word appearing in the references). In Rouge we divide by the length of the human references, so we would need an additional penalty for longer system results which could artificially raise their Rouge score.</p>

<p>Finally, you could use the <strong>F1 measure</strong> to make the metrics work together: 
F1 = 2 * (Bleu * Rouge) / (Bleu + Rouge)</p>
",""
"39170704","2016-08-26 16:21:11","0","","39167671","<p>You have to set a backoff tagger when using *gramTagger so that if the specific ngram is not seen in the training data, it will backoff to a tagger trained on a lower order ngram. See ""Combining Taggers"" section in <a href=""http://www.nltk.org/book/ch05.html"" rel=""nofollow noreferrer"">http://www.nltk.org/book/ch05.html</a></p>

<pre><code>&gt;&gt;&gt; from nltk import DefaultTagger, UnigramTagger, BigramTagger
&gt;&gt;&gt; from nltk.corpus import brown
&gt;&gt;&gt; text = brown.tagged_sents(categories='news')[:500]
&gt;&gt;&gt; t0 = DefaultTagger('NN')
&gt;&gt;&gt; t1 = UnigramTagger(text, backoff=t0)
&gt;&gt;&gt; t2 = BigramTagger(text, backoff=t1)

&gt;&gt;&gt; test_sent = brown.sents()[502]
&gt;&gt;&gt; test_sent
[u'Noting', u'that', u'Plainfield', u'last', u'year', u'had', u'lost', u'the', u'Mack', u'Truck', u'Co.', u'plant', u',', u'he', u'said', u'industry', u'will', u'not', u'come', u'into', u'this', u'state', u'until', u'there', u'is', u'tax', u'reform', u'.']
&gt;&gt;&gt; t2.tag(test_sent)
[(u'Noting', u'VBG'), (u'that', u'CS'), (u'Plainfield', u'NP-HL'), (u'last', u'AP'), (u'year', u'NN'), (u'had', u'HVD'), (u'lost', u'VBD'), (u'the', u'AT'), (u'Mack', 'NN'), (u'Truck', 'NN'), (u'Co.', u'NN-TL'), (u'plant', 'NN'), (u',', u','), (u'he', u'PPS'), (u'said', u'VBD'), (u'industry', 'NN'), (u'will', u'MD'), (u'not', u'*'), (u'come', u'VB'), (u'into', u'IN'), (u'this', u'DT'), (u'state', u'NN'), (u'until', 'NN'), (u'there', u'EX'), (u'is', u'BEZ'), (u'tax', 'NN'), (u'reform', 'NN'), (u'.', u'.')]
</code></pre>

<p>And to show that it works with your example in the question ;P</p>

<pre><code>&gt;&gt;&gt; test_sent = brown.sents()[2]
&gt;&gt;&gt; test_sent
[u'The', u'September-October', u'term', u'jury', u'had', u'been', u'charged', u'by', u'Fulton', u'Superior', u'Court', u'Judge', u'Durwood', u'Pye', u'to', u'investigate', u'reports', u'of', u'possible', u'``', u'irregularities', u""''"", u'in', u'the', u'hard-fought', u'primary', u'which', u'was', u'won', u'by', u'Mayor-nominate', u'Ivan', u'Allen', u'Jr.', u'.']
&gt;&gt;&gt; t2.tag(test_sent)
[(u'The', u'AT'), (u'September-October', u'NP'), (u'term', 'NN'), (u'jury', u'NN'), (u'had', u'HVD'), (u'been', u'BEN'), (u'charged', u'VBN'), (u'by', u'IN'), (u'Fulton', u'NP-TL'), (u'Superior', u'JJ-TL'), (u'Court', u'NN-TL'), (u'Judge', u'NN-TL'), (u'Durwood', u'NP'), (u'Pye', u'NP'), (u'to', u'TO'), (u'investigate', u'VB'), (u'reports', u'NNS'), (u'of', u'IN'), (u'possible', u'JJ'), (u'``', u'``'), (u'irregularities', u'NNS'), (u""''"", u""''""), (u'in', u'IN'), (u'the', u'AT'), (u'hard-fought', u'JJ'), (u'primary', 'NN'), (u'which', u'WDT'), (u'was', u'BEDZ'), (u'won', u'VBN'), (u'by', u'IN'), (u'Mayor-nominate', u'NN-TL'), (u'Ivan', u'NP'), (u'Allen', u'NP'), (u'Jr.', u'NP'), (u'.', u'.')]
</code></pre>

<p>At some point, you might realize that <a href=""https://stackoverflow.com/questions/30821188/python-nltk-pos-tag-not-returning-the-correct-part-of-speech-tag"">Python NLTK pos_tag not returning the correct part-of-speech tag</a> </p>
",""
"39104215","2016-08-23 14:39:59","0","","39104030","<p>At this moment, writing your own customised diagnostics and repairs is indeed the best solution. However, in the next release of FCG, a library of integrated diagnostics and repairs will be included. The one for unknown words will more or less look like the following:</p>

<p>Diagnostic for detecting unknown words (run after the creation of every node)</p>

<pre><code>(defmethod diagnose ((diagnostic diagnose-unknown-words) (node cip-node)
                 &amp;key &amp;allow-other-keys)
""Diagnose that the fully expanded structure contains untreated strings""
(when (fully-expanded? node)
(let ((strings-in-root (get-strings (assoc 'root
                                           (left-pole-structure
                                            (car-resulting-cfs (cipn-car node)))))))
  (when strings-in-root
    (let ((problem (make-instance 'unknown-words)))
      (set-data problem 'strings strings-in-root)
      problem)))))
</code></pre>

<p>Repair for adding a new lexical construction (very generic of course, you need to customize it to your own grammar):</p>

<pre><code>(defmethod repair ((repair add-lexical-cxn)
               (problem unknown-words)
               (node cip-node)
               &amp;key &amp;allow-other-keys)
""Repair by making a new lexical construction for the first untreated string""
(let ((uw (first (get-data problem 'strings))))
(multiple-value-bind (cxn-set lex-cxn)
    (eval `(def-fcg-cxn ,(make-symbol (upcase (string-append uw ""-cxn"")))
                        ((?word-unit
                          (args (?ref))
                          (syn-cat (lex-class ?lex-class))
                          (sem-cat (sem-class ?sem-class)))
                         &lt;-
                         (?word-unit
                          (HASH meaning ((,(intern (upcase uw)) ?ref)))
                          --
                          (HASH form ((string ?word-unit ,uw)))))
                        :cxn-inventory ,(copy-object (original-cxn-set (construction-inventory node)))
                        :cxn-set lex))
  (declare (ignore cxn-set))
  (make-instance 'fix
                 :repair repair
                 :problem problem
                 :restart-data lex-cxn))))
</code></pre>
",""
"39049856","2016-08-20 02:20:01","1","","38987138","<p>You can use <strong>Tree.productions()</strong> method to get CFG rules from Tree.</p>

<p><strong>Example:</strong></p>

<pre><code>from nltk import Tree

t = Tree.fromstring(""(S (VP (VB open) (NP (DT the) (NN door))) (. .))"")
print t.productions()
</code></pre>

<p><strong>Output:</strong></p>

<pre><code>[S -&gt; VP ., VP -&gt; VB NP, VB -&gt; 'open', NP -&gt; DT NN, DT -&gt; 'the', 
 NN -&gt; 'door', . -&gt; '.']
</code></pre>

<p>For more information check - <a href=""http://www.nltk.org/api/nltk.html#nltk.tree.Tree.productions"" rel=""noreferrer"">NLTK Tree Productions</a></p>
",""
"38913596","2016-08-12 08:41:00","3","","38903589","<p>The default GATE POS tagger uses rules and lexicons, which means it has to be rewritten entirely for a new language.</p>

<p>It may be easier to try with the <a href=""http://nlp.stanford.edu/software/tagger.shtml"" rel=""nofollow"">Stanford POS tagger</a> (compatible with GATE) which uses a statistical model. However you need significant training data which may not be available in your language. Check the link for details.</p>
",""
"38819496","2016-08-07 23:26:37","0","","38819371","<p><strong>TL;DR</strong>:</p>

<p>Without setting environmental variable, use the keywords arguments, <code>model_filename</code> and <code>path_to_jar</code></p>

<pre><code>from nltk.tag import StanfordNERTagger

stanford_ner_dir = '/home/alvas/stanford-ner/'
eng_model_filename= stanford_ner_dir + 'classifiers/english.all.3class.distsim.crf.ser.gz'
my_path_to_jar= stanford_ner_dir + 'stanford-ner.jar'

st = StanfordNERTagger(model_filename=eng_model_filename, path_to_jar=my_path_to_jar) 
st.tag('Rami Eid is studying at Stony Brook University in NY'.split())
</code></pre>

<hr>

<p><strong>In long:</strong> </p>

<p>See <a href=""https://stackoverflow.com/a/34112695/610569"">https://stackoverflow.com/a/34112695/610569</a></p>
",""
"38768326","2016-08-04 13:11:12","0","","38768052","<p>Your error is in <code>analyzer = super(StemmedCountVectorizer, self.build_analyzer())</code> here you are calling the function <code>build_analyzer</code> before the super call, which cause a infinite recursive loop. Change it for <code>analyzer = super(StemmedCountVectorizer, self).build_analyzer()</code></p>
",""
"38713635","2016-08-02 06:55:29","1","","38687056","<p>The issue was that I was modifying the \grammars\book_grammars\sql0.fcfg. When I saved it as separate file and loaded grammar from there, the problem got solved.</p>

<p>Don't know why it happened, but it resolved the issue.</p>
",""
"38625869","2016-07-28 01:47:31","3","","38614738","<p>You are using <strong>training_set[traincv[0]:traincv[len(traincv)-1]]</strong> which means range from traincv[0] to traincv[len(traincv)-1]</p>

<p>In your case traincv[0] and testcv[0] will be always near to 0 and traincv[len(traincv)-1] and testcv[len(testcv)-1] will be near to 1499. So you are using almost same data for training and testing while doing N-Fold Validation.</p>

<p>Here, you actually need to use subset indexes which are there in traincv and testcv.</p>

<pre><code>import numpy as np
training_set = np.array(training_set)
for traincv, testcv in cv:
    classifier = LogisticRegression_classifier.train(training_set[traincv])
    print ('CV_accuracy:', nltk.classify.util.accuracy(classifier, training_set[testcv]
</code></pre>
",""
"38571865","2016-07-25 15:30:47","3","","38571004","<p>You can use <a href=""http://www.nltk.org/howto/tree.html"" rel=""nofollow"">nltk.tree</a> module from <a href=""http://www.nltk.org/"" rel=""nofollow"">NLTK</a>.</p>

<pre><code>from nltk.tree import *

def traverse(t):
    try:
        # Replace Labels
        if t.label() == ""DT"":
            t.set_label(""DET"")
        elif t.label() == ""VBZ"":
            t.set_label(""VT"")   
    except AttributeError:
        return

    for child in t:
        traverse(child)

output_tree= ""(ROOT (S (NP (DT Every) (NN cat)) (VP (VBZ loves) (NP (DT a) (NN dog)))))""
tree = ParentedTree.fromstring(output_tree)

# Remove ROOT Element
if tree.label() == ""ROOT"":  
    tree = tree[0]

traverse(tree)
print tree  
# (S (NP (DET Every) (NN cat)) (VP (VT loves) (NP (DET a) (NN dog))))
</code></pre>
",""
"38562767","2016-07-25 08:19:51","3","","38502341","<p>The <strong>appelt</strong> priorities work only for the same regions of text (e.g. earlier match wins and longer match wins). Text consumed by a previous rule cannot be matched by a later rule...</p>

<p>From <a href=""https://gate.ac.uk/userguide/sec:jape:priority"" rel=""nofollow"">the documentation</a>:</p>

<blockquote>
  <p>With the <strong>appelt</strong> style, only one rule can be Ô¨Åred for the <strong>same region
  of text</strong>, according to a set of priority rules. Priority operates in
  the following way.</p>
  
  <ol>
  <li>From all the rules that match a <strong>region of the document starting at
  some point X</strong>, the one which matches the longest region is Ô¨Åred. </li>
  <li>If
  more than one rule matches <strong>the same region</strong>, the one with the highest
  priority is Ô¨Åred </li>
  <li>If there is more than one rule with the same
  priority, the one deÔ¨Åned earlier in the grammar is Ô¨Åred.</li>
  </ol>
</blockquote>

<p>...</p>

<blockquote>
  <p>Note also that depending on the control style, <strong>Ô¨Åring a rule may
  ‚Äòconsume‚Äô that part of the text, making it unavailable to be matched
  by other rules</strong>. This can be a problem for example if one rule uses
  context to make it more speciÔ¨Åc, and that context is then missed by
  later rules, having been consumed due to use of for example the
  ‚ÄòBrill‚Äô control style.</p>
</blockquote>

<hr>

<hr>

<p>The rule <code>TableRow</code> can win as longer with following modification, note that I added the <code>:tableRow</code> label, which does not include the leading number token.</p>

<pre><code>(
 ({Token.kind == number})?
 (
  ({Lookup.majorType == ""keyword""})
  ({Token.kind == punctuation})[0,4]
  ({Lookup.majorType == ""unit""})
 ):tableRow
)
</code></pre>
",""
"38551147","2016-07-24 10:41:34","8","","38541644","<p>First you can classify all test values and store predicted outcomes and gold results in a list.</p>

<p>Then, you can use <strong>nltk.ConfusionMatrix</strong>.</p>

<pre><code>test_result = []
gold_result = []

for i in range(len(testing_set)):
    test_result.append(classifier.classify(testing_set[i][0]))
    gold_result.append(testing_set[i][1])
</code></pre>

<p>Now, You can calculate different metrics.</p>

<pre><code>CM = nltk.ConfusionMatrix(gold_result, test_result)
print(CM)

print(""Naive Bayes Algo accuracy percent:""+str((nltk.classify.accuracy(classifier, testing_set))*100)+""\n"")

labels = {'pos', 'neg'}

from collections import Counter
TP, FN, FP = Counter(), Counter(), Counter()
for i in labels:
    for j in labels:
        if i == j:
            TP[i] += int(CM[i,j])
        else:
            FN[i] += int(CM[i,j])
            FP[j] += int(CM[i,j])

print(""label\tprecision\trecall\tf_measure"")
for label in sorted(labels):
    precision, recall = 0, 0
    if TP[label] == 0:
        f_measure = 0
    else:
        precision = float(TP[label]) / (TP[label]+FP[label])
        recall = float(TP[label]) / (TP[label]+FN[label])
        f_measure = float(2) * (precision * recall) / (precision + recall)
    print(label+""\t""+str(precision)+""\t""+str(recall)+""\t""+str(f_measure))
</code></pre>

<p>You can check - how to calculate <strong>precision and recall</strong> <a href=""https://en.wikipedia.org/wiki/Precision_and_recall#Definition_.28classification_context.29"" rel=""noreferrer"">here</a>.</p>

<p>You can also use : <strong>sklearn.metrics</strong> for these calculations using gold_result and test_result values.</p>

<pre><code>from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix   

print '\nClasification report:\n', classification_report(gold_result, test_result)
print '\nConfussion matrix:\n',confusion_matrix(gold_result, test_result)    
</code></pre>
",""
"38298563","2016-07-11 03:04:32","0","","35741627","<p>This happened because your PC doesn't have enough RAM.
When you train your large corpus, it requires a lot of memory.
Install more RAM, then you can get it done.</p>
",""
"38295964","2016-07-10 19:57:49","1","","38295454","<p>Phrase-structure trees contain more information than dependency trees and therefore you cannot deterministically convert dependency trees to phrase-structure trees.</p>

<p>But if you are using CoreNLP to parse the sentences, take a look at the <a href=""http://stanfordnlp.github.io/CoreNLP/parse.html"" rel=""nofollow""><code>parse</code></a> annotator. Unlike the dependency parser, this parser also outputs phrase-structure trees, so you can use this annotator to directly parse your sentences to phrase-structure trees. </p>
",""
"38292371","2016-07-10 13:26:16","2","","38226864","<p>You need to add &lt;\$>? in your grammar.</p>

<pre><code>chunkGram = r""""""chunk: {&lt;DT&gt;+&lt;NN.*&gt;+&lt;NN.*&gt;?|&lt;\$&gt;?&lt;CD&gt;+&lt;NN&gt;?|&lt;NNP&gt;?}""""""
</code></pre>

<p><strong>Code :</strong></p>

<pre><code>import nltk
from nltk.tokenize import PunktSentenceTokenizer

text = '80% of $300,000 Each Human Resource/IT Department'
train_text = text
sample_text = text
custom_sent_tokenizer = PunktSentenceTokenizer(train_text)
tokenized = custom_sent_tokenizer.tokenize(sample_text)

for i in tokenized:
    words = nltk.word_tokenize(i)
    tagged = nltk.pos_tag(words)

chunkGram = r""""""chunk: {&lt;DT&gt;+&lt;NN.*&gt;+&lt;NN.*&gt;?|&lt;\$&gt;?&lt;CD&gt;+&lt;NN&gt;?|&lt;NNP&gt;?}""""""

chunkParser = nltk.RegexpParser(chunkGram)
chunked = chunkParser.parse(tagged)

print(chunked)
</code></pre>

<p><strong>Output :</strong> </p>

<pre><code>(S
  (chunk 80/CD %/NN)
  of/IN
  (chunk $/$ 300,000/CD)
  (chunk Each/DT Human/NNP Resource/IT/NNP Department/NNP))
</code></pre>
",""
"38275718","2016-07-08 21:35:37","0","","38034266","<p>If you're a Python beginner and all you want to use is the part-of-speech (POS) tags, SyntaxNet is probably overkill. <a href=""https://spacy.io/"" rel=""nofollow"">SpaCy</a> is easy to use and quite accurate.</p>
",""
"38237004","2016-07-07 03:10:09","0","","38105014","<p>I had to once work on solving the opposite problem, i.e. generating questions out of sentences from Wikipedia articles.</p>

<p>I used the Stanford Parser to generate parse trees out of all possible sentences in my training dataset.  </p>

<p>e.g. </p>

<ol>
<li>Go to <a href=""http://nlp.stanford.edu:8080/parser/index.jsp"" rel=""nofollow"">http://nlp.stanford.edu:8080/parser/index.jsp</a></li>
<li>Enter ""The color of the car is red."" and click ""Parse"".</li>
<li>Then look at the Parse section of the response. The first layer of that sentence is <strong>NP VP</strong> (noun phrase followed by a verb phrase).<br>
The second layer is <strong>NP PP VBZ ADJP</strong>.</li>
</ol>

<p>I basically collected these patterns across 1000s of sentences, sorted them how common each patter was, and then used figured out how to best modify this parse tree to convert into each sentence in a different Wh-question (What, Who, When, Where, Why, etc)</p>

<p>You could you easily do something very similar. Study the parse trees of all of your training data, and figure out what patterns you could extract to get your work done. In many cases, just replacing the Wh word from the question with the answer would give you a valid albeit somewhat awkwardly phrases sentence.
e.g. ""Red is the color of the car.""</p>

<p>In the case of questions like ""Are you a man?"" (i.e. primary verb is something like 'are', 'can', 'should', etc), swapping the first 2 words usually does the trick - ""You are a man?""  </p>
",""
"38232020","2016-07-06 19:13:38","0","","37403563","<p>I was able to do this by creating a <code>Source</code> for each article URL. (disclaimer: not a python developer)</p>

<pre><code>import newspaper

urls = [
  'http://www.baltimorenews.net/index.php/sid/234363921',
  'http://www.baltimorenews.net/index.php/sid/234323971',
  'http://www.atlantanews.net/index.php/sid/234323891',
  'http://www.wpbf.com/news/funeral-held-for-gabby-desouza/33874572',  
]

class SingleSource(newspaper.Source):
    def __init__(self, articleURL):
        super(StubSource, self).__init__(""http://localhost"")
        self.articles = [newspaper.Article(url=url)]

sources = [SingleSource(articleURL=u) for u in urls]

newspaper.news_pool.set(sources)
newspaper.news_pool.join()

for s in sources:
  print s.articles[0].html
</code></pre>
",""
"38227611","2016-07-06 15:17:48","2","","37994012","<p>Depending on the source language you are dealing with, FreeLing does provide a parse tree (e.g. for Spanish, English, Catalan, Portuguese...)</p>

<p>If parsing in your language is not supported by FreeLing, you can add it just by writting a grammar.  FreeLing includes a CKY parser which will apply your grammar and give you the parse tree.</p>

<p>In this way, you could achieve step 2 ""building my own parse tree from the set of tags"".</p>

<p>Regarding the transfer, I am not sure the best strategy is reordering on the fly.  Probably is better to have the whole tree and perform the transfer aftwerwards.</p>

<p>If your goal is rule-based translation, you can have a look to the open-source translation platform <a href=""https://www.apertium.org/"" rel=""nofollow"">https://www.apertium.org/</a></p>
",""
"38020540","2016-06-24 19:11:05","5","","38019823","<p>If you have a few cores to spare, try using the <code>multiprocessing</code> library:</p>

<pre><code>from nltk import WordNetLemmatizer
from multiprocessing import Pool

def lemmed(text, cores=6): # tweak cores as needed
    with Pool(processes=cores) as pool:
        wnl = WordNetLemmatizer()
        result = pool.map(wnl.lemmatize, text)
    return result


sample_text = ['tests', 'friends', 'hello'] * (10 ** 6)

lemmed_text = lemmed(sample_text)

assert len(sample_text) == len(lemmed_text) == (10 ** 6) * 3

print(lemmed_text[:3])
# =&gt; ['test', 'friend', 'hello']
</code></pre>
",""
"37986772","2016-06-23 08:56:29","0","","37982632","<p>I think WordNet is good for this:
<a href=""https://wordnet.princeton.edu/"" rel=""nofollow"">https://wordnet.princeton.edu/</a></p>

<p>Originally wordnet is english ontology describing english word in english, showing synonims, definition etc. but there are a lot of other language wordnets projects as well as multilingual wordnets. Below interesting links:
<a href=""http://globalwordnet.org/wordnets-in-the-world/"" rel=""nofollow"">http://globalwordnet.org/wordnets-in-the-world/</a>
<a href=""http://www.certifiedchinesetranslation.com/openaccess/WordNet/"" rel=""nofollow"">http://www.certifiedchinesetranslation.com/openaccess/WordNet/</a></p>

<p>There is a big dictionary project leveraging from wordnets too:
<a href=""http://babelnet.org/about"" rel=""nofollow"">http://babelnet.org/about</a></p>
",""
"37929474","2016-06-20 18:34:02","0","","35524183","<p>In the paper <a href=""https://pdfs.semanticscholar.org/5b27/7a3f9a9f5250939f334e282d1393971722a9.pdf"" rel=""nofollow noreferrer"">&quot;Practical Earley Parsing&quot;</a> by John Aycock and R. Nigel Horspool, the authors propose the following as a way of handling nullable nonterminals:</p>
<blockquote>
<p>If [A‚Üí ... ‚Ä¢B ..., j] is in S<sub>i</sub>, add [B‚Üí ‚Ä¢ a, i]
to S<sub>i</sub> for all rules B¬†‚Üí¬†a.
<b>If B is nullable, also add [A ‚Üí ... B ‚Ä¢ ..., j] to S<sub>i</sub></b>.</p>
</blockquote>
<p>(Emphasis in original.) So in your example, in the prediction of <strong>A‚Üí ‚Ä¢ B B</strong> the following rules would be produced:</p>
<b>
(1) B &rarr; &#8226; <br>
(2) A &rarr; B &#8226; B <br>
(3) A &rarr; B B &#8226; <br>
</b>
<p>The key is this happens in the prediction phase. During the prediction phase if the 'post dot' symbol is nullable (both directly and through transference) then move the dot right and add that rule as well.</p>
<p>So basically:</p>
<p><strong>A ‚Üí ‚Ä¢ B B</strong> produces (<strong>B ‚Üí ‚Ä¢</strong> and <strong>A  ‚Üí B ‚Ä¢ B</strong>) each being queued and processed <br>
<strong>A ‚Üí B ‚Ä¢ B</strong> produces (<strong>A ‚Üí B B ‚Ä¢</strong>) which is queued and processed
</p>
",""
"37794246","2016-06-13 15:56:21","1","","37782569","<p>Make a <code>FreqDist</code> that counts only the nouns that follow the word ""the"". </p>

<p>The Brown corpus has a very rich tagset, so let's simplify things by asking for the simplified ""universal"" tagset. All nouns are now tagged <code>""NOUN""</code>.</p>

<pre><code>&gt;&gt;&gt; noundist = nltk.FreqDist(w2 for ((w1, t1), (w2, t2)) in 
            nltk.bigrams(brown.tagged_words(tagset=""universal""))
            if w1.lower() == ""the"" and t2 == ""NOUN"")
&gt;&gt;&gt; noundist.most_common(10)
[('world', 346), ('time', 250), ('way', 236), ('end', 206), ('fact', 194), ('state', 190), 
('man', 176), ('door', 172), ('house', 152), ('city', 127)]
</code></pre>

<p>The comprehension unpacks the two <code>word, tag</code> tuples that form the bigram: <code>(w1, t1), (w2, t2)</code>; checks that the first word (lowercased) is ""the"" and the second is tagged ""NOUN""; and if so, passes the second word (so, <code>w2</code> only) to be counted by the <code>FreqDist</code>.</p>
",""
"37767636","2016-06-11 19:15:06","0","","34145785","<p>Finally, found the answer, use</p>

<pre><code>-outputFormatOptions includePunctuationDependencies
</code></pre>

<p>Have contacted Stanford parser and corenlp support long time ago, no response at all</p>
",""
"37739991","2016-06-10 04:34:57","0","","37672070","<p>Instead of this:</p>

<pre><code>var p = parser.Parse(Parse.ParseParse(""...""));
</code></pre>

<p>I simply needed to use this:</p>

<pre><code>var p = ParserTool.ParseLine(""..."", parser, 1);
</code></pre>
",""
"37738709","2016-06-10 01:42:40","6","","37738333","<p>You need to recursively build the tree, but you need to distinguish between terminals and non-terminals. Btw. your parse sequence seems wrong. I hacked this up:</p>

<pre><code>def build_tree(parse):
  assert(parse)
  rule_head = parse[0][0]
  rule_body = grammar[rule_head][parse[0][1]]
  tree_body = []
  rest = parse[1:]
  for r in rule_body:
    if non_term(r):
        (subtree,rest) = build_tree(rest)
        tree_body.append(subtree)
    else:
        tree_body.append(r)

  return (tree(rule_head,tree_body), rest)
</code></pre>
",""
"37712453","2016-06-08 20:38:03","0","","37701305","<p>You can also easily add to existing stop word lists. E.g. use the one in the NLTK toolkit:</p>

<pre><code>from nltk.corpus import stopwords
</code></pre>

<p>and then add whatever you think is missing:</p>

<pre><code>stopwords = stopwords.words('english')+[""based"", ""regarding""]
</code></pre>

<p>The original NLTK list is described <a href=""https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/stopwords.zip"" rel=""nofollow"">here</a>.</p>
",""
"37640182","2016-06-05 09:04:04","1","","37640057","<p>One way to do this would be to turn your function into a generator.</p>

<pre><code>def randm(text):
      text = text.split("" "")
      for i in itertools.permutations(text): 
                yield "" "".join(i)
</code></pre>

<p>Then all you have to do is</p>

<pre><code>for word in randm(text):
    engrammar.grammar_cache(word)
</code></pre>

<p>You can read more about generators <a href=""https://wiki.python.org/moin/Generators"" rel=""nofollow"">here</a>.</p>

<p>If you don't want to use generators, you can always return a list from your function and then iterate through that list.</p>

<pre><code>def randm(text):
      words = []
      text = text.split("" "")
      for i in itertools.permutations(text): 
                words.append("" "".join(i))
      return words

words = randm(text)
for word in words:
    engrammar.grammar_cache(word)
</code></pre>
",""
"37636274","2016-06-05 00:20:58","0","","37622518","<p>If this is the only thing you want to change, the simplest solution is to just post-process the tagged text:</p>

<pre><code>for sentence in tagged_sentences:
    for n, (word,tag) in enumerate(sentence):
        if word == 'and/or':
            sentence[n] = (word, ""CC"")
</code></pre>

<p>But if your question is the first step to ""improving"" the NLTK's tagger, you should take the long view and think about how you could build or install a better tagger. Take a look at the many links included in <a href=""https://stackoverflow.com/a/30823202/699305"">this answer</a>.</p>
",""
"37612211","2016-06-03 10:40:36","6","","37611061","<p>Finally I found it inside <code>spaCy</code>'s source code: <a href=""https://github.com/explosion/spaCy/blob/master/spacy/glossary.py"" rel=""noreferrer"">glossary.py</a>. And this <a href=""https://web.archive.org/web/20190206204307/https://www.clips.uantwerpen.be/pages/mbsp-tags"" rel=""noreferrer"">link</a> explains the meaning of different tags.</p>
",""
"37525550","2016-05-30 12:21:17","0","","37525012","<p>There is a header file missing in your code files. The link you provided above, contains a source file <code>src/common.h</code> , just add <code>#include&lt;string.h&gt;</code> in this file. With this addition, it should work. </p>
",""
"37505799","2016-05-29 03:12:20","0","","37449729","<p>The input file for this demo should be one string representation of a tree per line.  This example prints out the subtrees of the first tree.</p>

<p>The Stanford CoreNLP class of interest is Tree.  </p>

<pre><code>import edu.stanford.nlp.trees.*;

import java.io.BufferedReader;
import java.io.FileInputStream;
import java.io.InputStreamReader;
import java.io.*;

public class TreeLoadExample {

    public static void printSubTrees(Tree t) {
        if (t.isLeaf())
            return;
        System.out.println(t);
        for (Tree subTree : t.children()) {
            printSubTrees(subTree);
        }
    }


    public static void main(String[] args) throws IOException, FileNotFoundException,
            UnsupportedEncodingException {
        TreeFactory tf = new LabeledScoredTreeFactory();
        Reader r = new BufferedReader(new InputStreamReader(new FileInputStream(args[0]), ""UTF-8""));
        TreeReader tr = new PennTreeReader(r, tf);
        Tree t = tr.readTree();
        printSubTrees(t);
    }
}
</code></pre>
",""
"37444102","2016-05-25 17:35:33","8","","37443138","<p>You have to apply the stemming on each word and store it into the ""stemmed"" column.</p>

<pre><code>df['stemmed'] = df['unstemmed'].apply(lambda x: [stemmer.stem(y) for y in x]) # Stem every word.
df = df.drop(columns=['unstemmed']) # Get rid of the unstemmed column.
df # Print dataframe.

+----+--------------------------------------------------------------+
|    | stemmed                                                      |
|----+--------------------------------------------------------------|
|  0 | ['program', 'program', 'with', 'program', 'languag']         |
|  1 | ['my', 'code', 'is', 'work', 'so', 'there', 'must',          |   
|    |  'be', 'a', 'bug', 'in', 'the', 'interpret']                 |
+----+--------------------------------------------------------------+
</code></pre>
",""
"37358498","2016-05-21 03:14:37","1","","37317749","<p><strong>CoNLL-X treebanks</strong></p>

<p>You can get the training data for Danish, Dutch, Portuguese, and Swedish available for free <a href=""http://ilk.uvt.nl/conll/post_task_data.html"" rel=""nofollow"">here</a>. For other languages, you'll probably need to license a treebank from LDC, unfortunately (details for many languages on that page).</p>

<p><a href=""http://universaldependencies.org/"" rel=""nofollow"">Universal Dependencies</a> are in CoNLL-U format, which can usually be converted to CoNLL-X format with some work.</p>

<p>Lastly, there's a large list of treebanks and their availability on <a href=""https://en.wikipedia.org/wiki/Treebank#Syntactic_treebanks"" rel=""nofollow"">this page</a>. You should be able to convert many of the dependency treebanks in this list into CoNLL-X format if they're not already in that format.</p>

<p><strong>Training the Stanford Neural Net Dependency parser</strong></p>

<p>From <a href=""http://nlp.stanford.edu/software/nndep.shtml"" rel=""nofollow"">this page</a>: The embedding file is optional, but the treebank is not. The best treebank and embedding files to use depend on which language and type of text you'd like to parse. Ideally, you would train on as much data as possible in the domain/genre that you're trying to parse.</p>
",""
"37312115","2016-05-19 00:32:24","5","","37311984","<p><code>dateutil.parser.parse</code> accepts a <code>default</code> parameter which you can use to specify a reference date:</p>

<pre><code>import datetime as DT
import dateutil.parser as DP

today = DT.datetime(2016, 4, 13)
for text in ('today', 'tomorrow', 'this Sunday', 'Wednesday next week', 
             'next week Wednesday', 
             'next thursday', 'next tuesday in June', '11/28',
             'Concert this Saturday'
             ""lunch with Andrew @ Mon Mar 7, 2016"",
             'meeting on Tuesday, 3/29'):
    dp_date = DP.parse(text, default=today, fuzzy=True)
    print('{:35} --&gt; {}'.format(text, dp_date))
</code></pre>

<p>yields</p>

<pre><code>today                               --&gt; 2016-04-13 00:00:00
tomorrow                            --&gt; 2016-04-13 00:00:00  should be 2016-04-14
this Sunday                         --&gt; 2016-04-17 00:00:00
Wednesday next week                 --&gt; 2016-04-13 00:00:00
next week Wednesday                 --&gt; 2016-04-13 00:00:00
next thursday                       --&gt; 2016-04-14 00:00:00
next tuesday in June                --&gt; 2016-06-14 00:00:00  should be 2016-06-07
11/28                               --&gt; 2016-11-28 00:00:00
Concert this Saturday               --&gt; 2016-04-16 00:00:00
lunch with Andrew @ Mon Mar 7, 2016 --&gt; 2016-03-07 00:00:00
meeting on Tuesday, 3/29            --&gt; 2016-03-29 00:00:00
</code></pre>

<p>Note, however, that not all phrases are parsed correctly.</p>
",""
"37151286","2016-05-11 00:50:58","1","","37130722","<p>You are doing quite a bit of extra work here as you are running the parser once through CoreNLP and then again by calling <code>lp.apply(words)</code>.</p>

<p>The easiest way of getting a dependency tree/graph with punctuation marks is by using the CoreNLP option <code>parse.keepPunct</code> as following.</p>

<pre><code>Annotation document = new Annotation(text);
Properties props = new Properties();
props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, parse"");
props.setProperty(""ssplit.newlineIsSentenceBreak"", ""always"");
props.setProperty(""ssplit.eolonly"", ""true"");
props.setProperty(""pos.model"", modelPath1);
props.setProperty(""parse.model"", modelPath);
props.setProperty(""parse.keepPunct"", ""true"");

StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

pipeline.annotate(document);

for (CoreMap sentence : sentences) {
   //Pick whichever representation you want
   SemanticGraph basicDeps = sentence.get(SemanticGraphCoreAnnotations.BasicDependenciesAnnotation.class);
   SemanticGraph collapsed = sentence.get(SemanticGraphCoreAnnotations.CollapsedDependenciesAnnotation.class);
   SemanticGraph ccProcessed = sentence.get(SemanticGraphCoreAnnotations.CollapsedCCProcessedDependenciesAnnotation.class);
}
</code></pre>

<p>The sentence annotation object stores the dependency trees/graphs as a <code>SemanticGraph</code>. If you want a list of <code>TypedDependency</code> objects, use the method <code>typedDependencies()</code>. For example,</p>

<pre><code>List&lt;TypedDependency&gt; dependencies = basicDeps.typedDependencies();
</code></pre>
",""
"37133213","2016-05-10 08:35:49","0","","37133079","<p>I recommend Stanford parser. It supports a powerful library for Java coding. You should check this site for more information about Stanford parser <code>http://stanfordnlp.github.io/CoreNLP/</code>. You also can run some demos at <a href=""http://nlp.stanford.edu:8080/corenlp/"" rel=""nofollow"">http://nlp.stanford.edu:8080/corenlp/</a></p>
",""
"37106299","2016-05-09 00:32:13","0","","37104242","<p>These are now part of the <a href=""http://universaldependencies.org/"" rel=""nofollow"">Universal Dependencies project</a> -- see this <a href=""http://universaldependencies.org/u/pos/index.html"" rel=""nofollow"">page</a> for detailed information about the universal part-of-speech tags.</p>
",""
"37065916","2016-05-06 06:39:39","0","","37056797","<p>For singularity and plurality you can always use '<a href=""http://www.clips.ua.ac.be/pages/pattern-en"" rel=""nofollow noreferrer"">pattern-en</a>' library.</p>
",""
"36972265","2016-05-01 21:38:26","3","","36966019","<p>TF-IDF is done in multiple steps by Scikit Learn's TfidfVectorizer, which in fact uses TfidfTransformer and inherits CountVectorizer.</p>

<p>Let me summarize the steps it does to make it more straightforward:</p>

<ol>
<li>tfs are calculated by CountVectorizer's fit_transform()</li>
<li>idfs are calculated by TfidfTransformer's fit()</li>
<li>tfidfs are calculated by TfidfTransformer's transform()</li>
</ol>

<p>You can check the source code <a href=""https://github.com/scikit-learn/scikit-learn/blob/51a765a/sklearn/feature_extraction/text.py#L1052"" rel=""noreferrer"">here</a>.</p>

<p>Back to your example. Here is the calculation that is done for the tfidf weight for the 5th term of the vocabulary, 1st document (X_mat[0,4]):</p>

<p>First, the tf for 'string', in the 1st document:</p>

<pre><code>tf = 1
</code></pre>

<p>Second, the idf for 'string', with smoothing enabled (default behavior):</p>

<pre><code>df = 2
N = 4
idf = ln(N + 1 / df + 1) + 1 = ln (5 / 3) + 1 = 1.5108256238
</code></pre>

<p>And finally, the tfidf weight for (document 0, feature 4):</p>

<pre><code>tfidf(0,4) = tf * idf = 1 * 1.5108256238 = 1.5108256238
</code></pre>

<p>I noticed you choose not to normalize the tfidf matrix. Keep in mind normalizing the tfidf matrix is a common and usually recommended approach, since most models will require the feature matrix (or design matrix) to be normalized.</p>

<p>TfidfVectorizer will L-2 normalize the output matrix by default, as a final step of the calculation. Having it normalized means it will have only weights between 0 and 1.</p>
",""
"36902434","2016-04-27 23:03:50","0","","36901948","<p>The runtime of CYK depends on the length of the longest production rule, since the algorithm considers all possible ways of decomposing a string into k parts for a production of length k. This means that the runtime per phase is O(n<sup>k</sup>), where k is the length of the longest production. Since there are O(n) phases, the runtime of CYK on a grammar with maximum production length k is O(n<sup>k+1</sup>).</p>

<p>CYK will work correctly on grammars that aren't in CNF, but the runtime may not end up being cubic in the length of the string. The CNF requirement just forces k = 2 and therefore guarantees an O(n<sup>3</sup>) overall runtime.</p>
",""
"36854352","2016-04-26 02:37:44","0","","36739843","<p>my environment is MAC.</p>

<p>The problem is the java_path, should be:</p>

<pre><code>java_path = ""/your/Java/jdk/home
</code></pre>
",""
"36802718","2016-04-22 20:23:41","1","","36768459","<p>The architecture of Persistent is a little circuitous and underdocumented:</p>

<ul>
<li><p><code>withSqlPool</code> takes a builder. The builder is able to build a <code>SqlBackend</code> out of any ""logging function"" (basically the internal type that MonadLogger uses). The function then creates a <em>resource pool</em> of <code>SqlBackend</code>s, for you to acquire and release and use. This is the continuation argument <code>Pool SqlBackend -&gt; m a</code> you pass in. In return, <code>withSqlPool</code> promises to give you back a bunch of side effects, typed as <code>(MonadIO m, MonadBaseControl IO m, MonadLogger m) =&gt; m a</code>.</p></li>
<li><p><code>runSqlPool</code>, on the other hand, takes a <code>MonadBaseControl IO m =&gt; ReaderT SqlBackend m a</code> and a <code>Pool SqlBackend</code> and returns <code>m a</code>. We can infer from this that it basically acquires a <code>SqlBackend</code> from the resource pool, uses it to construct and run a SQL query, and then returns <code>MonadBaseControl IO =&gt; m a</code>. Indeed, its documentation is ""Get a connection from the pool, run the given action, and then return the connection to the pool.""</p></li>
</ul>

<p>Though named similarly, they do two very different things. The first function constructs the resource pool and the second function uses it. Most Persistent SQL code will have this shape:</p>

<pre><code>withSqlPool (\logFunc -&gt; do
                conn &lt;- makeConnection connectionString
                return SqlBackend { ... , connLogFunc = logFunc })
            numberOfOpenConnections
            (\pool -&gt; do
              runSqlPool (insertBy myData) pool
              runSqlPool (anotherTransaction moreData) pool)
</code></pre>

<p>In fact, if you're using <code>persistent-postgresql</code>, the above is simply the expanded form of</p>

<pre><code>withPostgresqlPool connectionString
                   numberOfOpenConnections
                   (\pool -&gt; do
                     runSqlPool (insertBy myData) pool
                     runSqlPool (anotherTransaction moreData) pool)
</code></pre>

<p>But wait! We can't quite execute this as an <code>IO</code> action yet. <code>MonadIO m, MonadBaseControl IO m, MonadLogger m</code> are our constraints and it's that third one that we have to discharge:</p>

<pre><code>main :: IO ()
main =
  runStdoutLoggingT $
    withPostgresqlPool connectionString
                       numberOfOpenConnections
                       (\pool -&gt; do
                         runSqlPool (insertBy myData) pool
                         runSqlPool (anotherTransaction moreData) pool
                         return ())
</code></pre>

<p>When the third constraints disappears, we're able to unify <code>IO ()</code> with <code>(MonadIO m, MonadBaseControl IO m) =&gt; m ()</code> by realizing <code>m ~ IO</code>.</p>

<p>It's now, at this stage, that we're able to insert our <code>filterLogger</code> ‚Äì right before the constraint is discharged with <code>runStdoutLoggingT</code>:</p>

<pre><code>main :: IO ()
main =
  runStdoutLoggingT . filterLogger (\_ _ -&gt; False) $
    withPostgresqlPool connectionString
                       numberOfOpenConnections
                       (\pool -&gt; do
                         runSqlPool (insertBy myData) pool
                         runSqlPool (anotherTransaction moreData) pool
                         return ())
</code></pre>

<p>Overall, a mess created by bad naming and the underwhelmingly documented <code>Database.Persist.Sql</code> module.</p>

<p><strong>Let's underline the point: <code>runSqlPool</code> simply inherits the logging behavior from the <code>MonadLogger</code> constraint generated by <code>withSqlPool</code>. It is only at the <code>withSqlPool</code> level that we're able to insert the desired <code>filterLogger</code> call.</strong></p>


",""
"36532481","2016-04-10 16:37:33","0","","36519254","<p>I tried reproducing your example with the inaugural texts.  Using a reproducible example from the package data, your code worked for me:</p>

<pre><code>twitter.docs &lt;- corpus(data_corpus_inaugural[1:5])
stemmed_no_stops &lt;- twitter.docs %&gt;%
    tokens(remove_punct = TRUE, remove_twitter = TRUE) %&gt;%
    tokens_tolower() %&gt;%
    tokens_remove(stopwords(""english"")) %&gt;%
    tokens_wordstem()
lapply(stemmed_no_stops, head)
## $`1789-Washington`
## [1] ""fellow-citizen"" ""senat""          ""hous""           ""repres""         ""among""         
## [6] ""vicissitud""    
## 
## $`1793-Washington`
## [1] ""fellow""  ""citizen"" ""call""    ""upon""    ""voic""    ""countri""
## 
## $`1797-Adams`
## [1] ""first""   ""perceiv"" ""earli""   ""time""    ""middl""   ""cours""  
## 
## $`1801-Jefferson`
## [1] ""friend""   ""fellow""   ""citizen""  ""call""     ""upon""     ""undertak""
## 
## $`1805-Jefferson`
## [1] ""proceed""   ""fellow""    ""citizen""   ""qualif""    ""constitut"" ""requir""  

twitter.semantic &lt;- stemmed_no_stops %&gt;%
    tokens_skipgrams(n = 2, skip = 0:2) %&gt;%
    dfm %&gt;%
    dfm_trim(min_count = 5, min_doc = 2)
twitter.semantic[1:5, 1:4]
# Document-feature matrix of: 5 documents, 4 features.
# 5 x 4 sparse Matrix of class ""dfmSparse""
#                  features
# docs              fellow_citizen let_u unit_state foreign_nation
#   1789-Washington              2     0          2              0
#   1793-Washington              1     0          0              0
#   1797-Adams                   0     0          3              5
#   1801-Jefferson               5     5          0              0
#   1805-Jefferson               8     2          1              1
</code></pre>
",""
"36513178","2016-04-09 05:38:00","1","","36512113","<p>First setup Stanford tools and NLTK correctly, e.g. in Linux:</p>

<pre><code>alvas@ubi:~$ cd
alvas@ubi:~$ wget http://nlp.stanford.edu/software/stanford-parser-full-2015-12-09.zip
alvas@ubi:~$ unzip stanford-parser-full-2015-12-09.zip
alvas@ubi:~$ ls stanford-parser-full-2015-12-09
bin                        ejml-0.23.jar          lexparser-gui.sh              LICENSE.txt       README_dependencies.txt  StanfordDependenciesManual.pdf
build.xml                  ejml-0.23-src.zip      lexparser_lang.def            Makefile          README.txt               stanford-parser-3.6.0-javadoc.jar
conf                       lexparser.bat          lexparser-lang.sh             ParserDemo2.java  ShiftReduceDemo.java     stanford-parser-3.6.0-models.jar
data                       lexparser-gui.bat      lexparser-lang-train-test.sh  ParserDemo.java   slf4j-api.jar            stanford-parser-3.6.0-sources.jar
DependencyParserDemo.java  lexparser-gui.command  lexparser.sh                  pom.xml           slf4j-simple.jar         stanford-parser.jar
alvas@ubi:~$ export STANFORDTOOLSDIR=$HOME
alvas@ubi:~$ export CLASSPATH=$STANFORDTOOLSDIR/stanford-parser-full-2015-12-09/stanford-parser.jar:$STANFORDTOOLSDIR/stanford-parser-full-2015-12-09/stanford-parser-3.6.0-models.jar
</code></pre>

<p>(See <a href=""https://gist.github.com/alvations/e1df0ba227e542955a8a"" rel=""nofollow"">https://gist.github.com/alvations/e1df0ba227e542955a8a</a> for more details and see <a href=""https://gist.github.com/alvations/0ed8641d7d2e1941b9f9"" rel=""nofollow"">https://gist.github.com/alvations/0ed8641d7d2e1941b9f9</a> for windows instructions)</p>

<p>Then, use <a href=""http://www.nltk.org/_modules/nltk/tokenize/punkt.html"" rel=""nofollow"">Kiss and Strunk (2006)</a> to sentence tokenize the text into a list of strings, where each item in the list is a sentence.</p>

<pre><code>&gt;&gt;&gt; from nltk import sent_tokenize, word_tokenize
&gt;&gt;&gt; sentences = 'This is the first sentnece. This is the second. And this is the third'
&gt;&gt;&gt; sent_tokenize(sentences)
['This is the first sentence.', 'This is the second.', 'And this is the third']
</code></pre>

<p>Then feed the document stream into the stanford parser:</p>

<pre><code>&gt;&gt;&gt; list(list(parsed_sent) for parsed_sent in parser.raw_parse_sents(sent_tokenze(sentences)))
[[Tree('ROOT', [Tree('S', [Tree('NP', [Tree('DT', ['This'])]), Tree('VP', [Tree('VBZ', ['is']), Tree('NP', [Tree('DT', ['the']), Tree('JJ', ['first']), Tree('NN', ['sentence'])])]), Tree('.', ['.'])])])], [Tree('ROOT', [Tree('S', [Tree('NP', [Tree('DT', ['This'])]), Tree('VP', [Tree('VBZ', ['is']), Tree('NP', [Tree('DT', ['the']), Tree('NN', ['second'])])]), Tree('.', ['.'])])])], [Tree('ROOT', [Tree('S', [Tree('CC', ['And']), Tree('NP', [Tree('DT', ['this'])]), Tree('VP', [Tree('VBZ', ['is']), Tree('NP', [Tree('DT', ['the']), Tree('JJ', ['third'])])])])])]]
</code></pre>
",""
"36234096","2016-03-26 10:14:24","0","","36230641","<p>If there is a data set of irregular English verbs in a package, this task would be easy. I just do not know any packages with such data, so I chose to create my own database by scraping. I am not sure if this website covers all irregular words. If necessary, you want to search better websites to create your own database. Once you have your database, You can engage in your task.</p>

<p>First, I used <code>stemDocument()</code> and clean up present forms with -s. Then, I collected past forms in <code>words</code> (i.e., <code>past</code>), infinitive forms of the past forms (i.e., <code>inf1</code>),identified the order of the past forms in <code>temp</code>. I further identified the positions of the past forms in <code>temp</code>. I finally replaced the sat forms with their infinitive forms. I repeated the same procedure for past participles.</p>

<pre><code>library(tm)
library(rvest)
library(dplyr)
library(splitstackshape)


### Create a database
x &lt;- read_html(""http://www.englishpage.com/irregularverbs/irregularverbs.html"")

x %&gt;%
html_table(header = TRUE) %&gt;%
bind_rows %&gt;%
rename(Past = `Simple Past`, PP = `Past Participle`) %&gt;%
filter(!Infinitive %in% LETTERS) %&gt;%
cSplit(splitCols = c(""Past"", ""PP""),
       sep = "" / "", direction = ""long"") %&gt;%
filter(complete.cases(.)) %&gt;%
mutate_each(funs(gsub(pattern = ""\\s\\(.*\\)$|\\s\\[\\?\\]"",
                      replacement = """",
                      x = .))) -&gt; mydic

### Work on the task

words &lt;- c(""said"", ""drawn"", ""say"", ""says"", ""make"", ""made"", ""done"")

### says to say
temp &lt;- stemDocument(words)

### past forms become present form
### Collect past forms
past &lt;- mydic$Past[which(mydic$Past %in% temp)]

### Collect infinitive forms of past forms
inf1 &lt;- mydic$Infinitive[which(mydic$Past %in% temp)]

### Identify the order of past forms in temp
ind &lt;- match(temp, past)
ind &lt;- ind[is.na(ind) == FALSE]

### Where are the past forms in temp?
position &lt;- which(temp %in% past)

temp[position] &lt;- inf1[ind]

### Check
temp
#[1] ""say""   ""drawn"" ""say""   ""say""   ""make""  ""make""  ""done"" 


### PP forms to infinitive forms (same as past forms)

pp &lt;- mydic$PP[which(mydic$PP %in% temp)]
inf2 &lt;- mydic$Infinitive[which(mydic$PP %in% temp)]
ind &lt;- match(temp, pp)
ind &lt;- ind[is.na(ind) == FALSE]
position &lt;- which(temp %in% pp)
temp[position] &lt;- inf2[ind]

### Check
temp
#[1] ""say""  ""draw"" ""say""  ""say""  ""make"" ""make"" ""do"" 
</code></pre>
",""
"36125057","2016-03-21 07:14:42","0","","36124329","<p>The real problem will be, that if your words are that sparse a learned classifier will not generalise to the real world data. However, there are several solutions to it</p>

<p>1.) Use more data. This is kind-of a no-brainer. However, you can not only add labeled data you can also use unlabelled data in a <strong>semi-supervised learning</strong></p>

<p>2.) Use more data (part b). You can look into the <strong>transfer learning</strong> setting. There you build a classifier on a large data set with similar characteristics. This might be twitter streams and then adapt this classifier to your domain</p>

<p>3.) Get your processing pipeline right. Your problem might origin from a suboptimal processing pipeline. Are you doing <strong>stemming</strong>? In the email the word <em>steming</em> should be mapped onto <em>stem</em>. This can be pushed even further by using synonym matching with a dictionary.</p>
",""
"36120762","2016-03-20 23:08:56","1","","36109717","<p>You're not doing anything wrong. You're of course welcome to decide for yourself how much to trust any tool, but I suspect you'll see similar issues with any parser trained empirically/statistically. As to your issues:</p>

<ul>
<li>Periods are treated like any other token in model building, so, yes, they can influence the parse chosen.</li>
<li>There are indeed a lot of ambiguities in English (as there are in all other human languages), and the question of whether to interpret forms ending in <em>ing</em> as verbs, nouns (verbal nouns or gerunds), or adjectives is a common one. The parser does not always get it right.</li>
<li><p>In terms of particular bad choices it made, often they reflect usage/domain mismatches between the parser training data and the sentences you are trying. The training data is predominantly news articles ‚Äì last millennium news articles for that matter ‚Äì although we do mix in some other data and occasionally add to it. So:</p>

<ul>
<li>The use of <em>flagging</em> as a verb, common in modern internet developer use, doesn't occur at all in the training data, so it not surprisingly tends to choose JJ for <em>flagging</em>, since that's the analysis of the only cases in the training data.</li>
<li>In news articles <em>drinking</em> is just more commonly a noun, with discussions of <em>underage drinking</em>, <em>coffee drinking</em>, <em>drinking and driving</em>, etc.</li>
</ul></li>
</ul>
",""
"36041915","2016-03-16 16:46:33","0","","36041738","<p>You can do it with <code>CountVectorizer</code>, which scans the document as text and converts into a term-document matrix, and using <code>TfidfTrasnformer</code> on the matrix.</p>

<p>These two steps can also be combined and done together with the <code>TfidfVectorizer</code>.</p>

<p>These are in the <code>sklearn.feature_extraction.text</code> module [<a href=""http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.text"" rel=""nofollow"">link</a>].</p>

<p>Both processes will return the same sparse matrix representation, on which I presume you will probably do SVD transform by <code>TruncatedSVD</code> to get a smaller dense matrix.</p>

<p>You can also of course do it yourself, which requires keeping two maps, one for each document, and one overall, where you count the terms. That is how they operate under the hood.</p>

<p><a href=""http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"" rel=""nofollow"">This page</a> has some nice examples.</p>
",""
"36031245","2016-03-16 09:17:02","3","","36031018","<p>You've passed <code>tagged_sents</code> positionally, so it is being used as the <code>feature_detector</code> argument. You should construct the tagger like this:</p>

<pre><code>tagger = nltk.ClassifierBasedTagger(train=tagged_sents)
</code></pre>

<p>See <a href=""http://www.nltk.org/api/nltk.tag.html#nltk.tag.sequential.ClassifierBasedTagger"" rel=""nofollow"">http://www.nltk.org/api/nltk.tag.html#nltk.tag.sequential.ClassifierBasedTagger</a></p>
",""
"36030688","2016-03-16 08:51:00","0","","36030428","<p><a href=""http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/"" rel=""nofollow"">TreeTagger</a> is a fast easy-to-install well-documented decison-tree based tagger with support for many languages (and yeah, it's built by a German) and a <a href=""https://perso.limsi.fr/pointal/doku.php?id=dev:treetaggerwrapper"" rel=""nofollow"">python wrapper</a>.</p>
",""
"35964803","2016-03-12 23:54:20","0","","35963350","<p>From the demo:</p>

<pre><code>&gt;&gt;&gt; from nltk import CFG
&gt;&gt;&gt; grammar = CFG.fromstring(""""""
... S -&gt; NP VP
... PP -&gt; P NP
... NP -&gt; Det N | NP PP
... VP -&gt; V NP | VP PP
... Det -&gt; 'a' | 'the'
... N -&gt; 'dog' | 'cat'
... V -&gt; 'chased' | 'sat'
... P -&gt; 'on' | 'in'
... """""")
</code></pre>

<p>The grammar for writing the grammar from string should work as such:</p>

<ul>
<li>Each line is a rule that makes up of a the left-hand-side (LHS) and right-hand-side (RHS), where </li>
<li>Only one non-terminal can be on the LHS of the arrow <code>-&gt;</code></li>
<li>RHS can be made up of a combinations of one or more non-terminals and/or terminals. </li>
<li>Terminals strings needs to be enclosed between quotation marks</li>
<li>Non-terminal symbols on the RHS are to be separated by spaces. </li>
<li>Each non-terminal results (LHS) can be made up of one or more RHS combinations and each combination is delimited by the pip symbol <code>|</code></li>
<li>It is CFG's convention to use capitalized letters for non-terminals but it's not necessary. </li>
</ul>

<p>Also, see <a href=""https://en.wikipedia.org/wiki/Terminal_and_nonterminal_symbols"" rel=""nofollow"">https://en.wikipedia.org/wiki/Terminal_and_nonterminal_symbols</a> and <a href=""https://en.wikipedia.org/wiki/Context-free_grammar"" rel=""nofollow"">https://en.wikipedia.org/wiki/Context-free_grammar</a></p>
",""
"35930323","2016-03-11 00:56:17","1","","35915075","<p>You can rewrite the grammar as suggested by alexis, this means several list of terms (nouns, verbs,...) for a specific place in each sentence.</p>

<p>But you can also apply a post-filtering strategy (don't have to touch grammar) :</p>

<ul>
<li>generate all possible sentences with your grammar, even sentences with words occuring twice or more</li>
<li>apply a filter that removes all sentences with words occuring twice or more</li>
</ul>

<p>Here is the filter you can apply :</p>

<pre><code>from collections import Counter
f=lambda sent:False if Counter(sent.split("" "")).most_common(1)[0][1] &gt; 1 else True

f(""creation video software"") # return True, good sentence
f(""creation creation creation"") # return False, bad sentence
f(""creation software creation"") # return False, bad sentence
</code></pre>
",""
"35923989","2016-03-10 18:03:19","0","","35882925","<p>Read a file like this:</p>

<pre><code>f = open('your-file.txt', 'rU') # U is for Unicode
raw = f.read()
tokens = nltk.word_tokenize(raw)
</code></pre>

<p>Once you have a tokenized text you can proceed in tagging it, for example:</p>

<pre><code>def_tagger = nltk.DefaultTagger('NN')
def_tagger.tag(tokens)
</code></pre>

<p>And this will (as an example) tag every token as NN. To evaluate it you'll need to manually assign a tag to each word and then:</p>

<pre><code>def_tagger.evaluate(you_manual_tagged_sents)
</code></pre>

<p>This will return a number between 0 (very bad) and 1 (perfect match).</p>
",""
"35902494","2016-03-09 21:00:07","3","","35836907","<h1>EDITED</h1>

<p>This issue has been resolved from NLTK v3.2.1. Upgrading your NLTK version would resolve the issue, e.g. <code>pip install -U nltk</code>.</p>

<hr>

<p>I faced the same issue and the error encountered was as follows;</p>

<pre><code>Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""C:\Python27\lib\site-packages\nltk-3.2-py2.7.egg\nltk\tag\__init__.py"", line 110, in pos_tag
tagger = PerceptronTagger()
  File ""C:\Python27\lib\site-packages\nltk-3.2-py2.7.egg\nltk\tag\perceptron.py"", line 141, in __init__
self.load(AP_MODEL_LOC)
  File ""C:\Python27\lib\site-packages\nltk-3.2-py2.7.egg\nltk\tag\perceptron.py"", line 209, in load
self.model.weights, self.tagdict, self.classes = load(loc)
  File ""C:\Python27\lib\site-packages\nltk-3.2-py2.7.egg\nltk\data.py"", line 801, in load
opened_resource = _open(resource_url)
  File ""C:\Python27\lib\site-packages\nltk-3.2-py2.7.egg\nltk\data.py"", line 924, in _open
return urlopen(resource_url)
  File ""C:\Python27\lib\urllib2.py"", line 126, in urlopen
return _opener.open(url, data, timeout)
  File ""C:\Python27\lib\urllib2.py"", line 391, in open
response = self._open(req, data)
  File ""C:\Python27\lib\urllib2.py"", line 414, in _open
'unknown_open', req)
  File ""C:\Python27\lib\urllib2.py"", line 369, in _call_chain
result = func(*args)
  File ""C:\Python27\lib\urllib2.py"", line 1206, in unknown_open
raise URLError('unknown url type: %s' % type)
urllib2.URLError: &lt;urlopen error unknown url type: c&gt;
</code></pre>

<p>The URLError that you mentioned was due to a bug in the perceptron.py file within the NLTK library for Windows. 
In my machine, the file is at this location </p>

<pre><code>C:\Python27\Lib\site-packages\nltk-3.2-py2.7.egg\nltk\tag\perceptron.py
</code></pre>

<p>(Basically look at an equivalent location within yours wherever you have the Python27 folder)</p>

<p>The bug was basically in the code to find the corresponding location for the averaged_perceptron_tagger within your machine. One can have a look at the line 801 and 924 mentioned in the data.py file regarding this.</p>

<p>I think the NLTK developer community recently fixed this bug in the code. Have a look at this commit made to their code a few days back.</p>

<p><a href=""https://github.com/nltk/nltk/commit/d3de14e58215beebdccc7b76c044109f6197d1d9#diff-26b258372e0d13c2543de8dbb1841252"" rel=""nofollow"">https://github.com/nltk/nltk/commit/d3de14e58215beebdccc7b76c044109f6197d1d9#diff-26b258372e0d13c2543de8dbb1841252</a></p>

<p>The snippet where the change was made is as follows;</p>

<pre><code>self.tagdict = {}
self.classes = set()
    if load:
        AP_MODEL_LOC = 'file:'+str(find('taggers/averaged_perceptron_tagger/'+PICKLE))
          self.load(AP_MODEL_LOC)
        # Initially it was:AP_MODEL_LOC = str(find('taggers/averaged_perceptron_tagger/'+PICKLE)) 

def tag(self, tokens):
</code></pre>

<p>Updating the file to the most recent commit worked for me and was able to use the nltk.pos_tag command. I believe this would resolve your problem as well (assuming you have everything else set up).</p>
",""
"35871479","2016-03-08 15:31:03","0","","35870282","<p>You need to convert the tag from the pos_tagger to one of the four ""syntactic categories"" that wordnet recognizes, then pass that to the lemmatizer as the word_pos. </p>

<p>From <a href=""http://wordnet.princeton.edu/man/wndb.5WN.html"" rel=""nofollow"" title=""the docs"">the docs</a>:</p>

<blockquote>
  <p>Syntactic category: n for noun files, v for verb files, a for adjective files, r for adverb files.</p>
</blockquote>
",""
"35693506","2016-02-29 06:51:13","0","","35690892","<p>Try:</p>

<pre><code>&gt;&gt;&gt; from nltk.stem import snowball
&gt;&gt;&gt; stemmer = snowball.EnglishStemmer()
&gt;&gt;&gt; stemmer.stem('thee')
u'thee'
&gt;&gt;&gt; dir(stemmer)
['_EnglishStemmer__double_consonants', '_EnglishStemmer__li_ending', '_EnglishStemmer__special_words', '_EnglishStemmer__step0_suffixes', '_EnglishStemmer__step1a_suffixes', '_EnglishStemmer__step1b_suffixes', '_EnglishStemmer__step2_suffixes', '_EnglishStemmer__step3_suffixes', '_EnglishStemmer__step4_suffixes', '_EnglishStemmer__step5_suffixes', '_EnglishStemmer__vowels', '__class__', '__delattr__', '__dict__', '__doc__', '__format__', '__getattribute__', '__hash__', '__init__', '__module__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', '__weakref__', '_r1r2_standard', '_rv_standard', 'stem', 'stopwords', 'unicode_repr']
&gt;&gt;&gt; stemmer._EnglishStemmer__special_words
{u'exceeds': u'exceed', u'inning': u'inning', u'exceed': u'exceed', u'exceeding': u'exceed', u'succeeds': u'succeed', u'succeeded': u'succeed', u'skis': u'ski', u'gently': u'gentl', u'singly': u'singl', u'cannings': u'canning', u'early': u'earli', u'earring': u'earring', u'bias': u'bias', u'tying': u'tie', u'exceeded': u'exceed', u'news': u'news', u'herring': u'herring', u'proceeds': u'proceed', u'succeeding': u'succeed', u'innings': u'inning', u'proceeded': u'proceed', u'proceed': u'proceed', u'dying': u'die', u'outing': u'outing', u'sky': u'sky', u'andes': u'andes', u'idly': u'idl', u'outings': u'outing', u'ugly': u'ugli', u'only': u'onli', u'proceeding': u'proceed', u'lying': u'lie', u'howe': u'howe', u'atlas': u'atlas', u'earrings': u'earring', u'cosmos': u'cosmos', u'canning': u'canning', u'succeed': u'succeed', u'herrings': u'herring', u'skies': u'sky'}
&gt;&gt;&gt; stemmer._EnglishStemmer__special_words['thee'] = 'thou'
&gt;&gt;&gt; stemmer.stem('thee')
'thou'
</code></pre>

<p>And:</p>

<pre><code>&gt;&gt;&gt; stemmer._EnglishStemmer__step0_suffixes
(u""'s'"", u""'s"", u""'"")
&gt;&gt;&gt; stemmer._EnglishStemmer__step1a_suffixes
(u'sses', u'ied', u'ies', u'us', u'ss', u's')
&gt;&gt;&gt; stemmer._EnglishStemmer__step1b_suffixes
(u'eedly', u'ingly', u'edly', u'eed', u'ing', u'ed')
&gt;&gt;&gt; stemmer._EnglishStemmer__step2_suffixes
(u'ization', u'ational', u'fulness', u'ousness', u'iveness', u'tional', u'biliti', u'lessli', u'entli', u'ation', u'alism', u'aliti', u'ousli', u'iviti', u'fulli', u'enci', u'anci', u'abli', u'izer', u'ator', u'alli', u'bli', u'ogi', u'li')
&gt;&gt;&gt; stemmer._EnglishStemmer__step3_suffixes
(u'ational', u'tional', u'alize', u'icate', u'iciti', u'ative', u'ical', u'ness', u'ful')
&gt;&gt;&gt; stemmer._EnglishStemmer__step4_suffixes
(u'ement', u'ance', u'ence', u'able', u'ible', u'ment', u'ant', u'ent', u'ism', u'ate', u'iti', u'ous', u'ive', u'ize', u'ion', u'al', u'er', u'ic')
&gt;&gt;&gt; stemmer._EnglishStemmer__step5_suffixes
(u'e', u'l')
</code></pre>

<p>Note that the step suffixes are tuples and are immutable so you can't append or add to them like the special words, you would have to ""copy"" and cast to list and append to it, then overwrite it, e.g.:</p>

<pre><code>&gt;&gt;&gt; from nltk.stem import snowball
&gt;&gt;&gt; stemmer = snowball.EnglishStemmer()
&gt;&gt;&gt; stemmer._EnglishStemmer__step1b_suffixes
[u'eedly', u'ingly', u'edly', u'eed', u'ing', u'ed', 'eth']
&gt;&gt;&gt; step1b = stemmer._EnglishStemmer__step1b_suffixes 
&gt;&gt;&gt; stemmer._EnglishStemmer__step1b_suffixes = list(step1b) + ['eth']
&gt;&gt;&gt; stemmer.stem('loveth')
u'love'
</code></pre>
",""
"35617031","2016-02-25 02:18:51","5","","35482943","<p>Both sum(ps) and sum(pt) are the total probability mass of _s and _t <strong>over the support of s</strong> (by ""support of s"" I mean all words that appear in _s, regardless of the words that appear in _t).
This means that </p>

<ol>
<li>sum(ps)==1, since the for-loop sums over all words in _s.</li>
<li>sum(pt) &lt;= 1, where equality will hold if the support of t is a subset of the support of s (that is, if all words in _t appear in _s). Also, sum(pt) might be close to 0 if the overlap between words in _s and _t is small. Specifically, if the intersection of _s and _t is the empty set, then sum(pt) == epsilon*len(_s).</li>
</ol>

<p>So, I don't think there's a problem with the code.</p>

<p>Also, contrary to the title of the question, kldiv() does not compute the symmetric KL-divergence, but the KL-divergence between _s and a smoothed version of _t.</p>
",""
"35614291","2016-02-24 22:14:58","1","","34320024","<p>First of all, if you have a known dictionary you will get the fastest solution with something like a <a href=""https://en.wikipedia.org/wiki/Levenshtein_automaton"" rel=""nofollow"">Levenshtein Automata</a>, which will solve this in linear time to get <strong>all</strong> candidates. You can't beat this with a general purpose implementation.</p>

<p>With that said, this implementation of levenshtein distance is a few times faster than yours.</p>

<pre><code>function distance(s, t) {
    if (s === t) {
        return 0;
    }
    var n = s.length, m = t.length;
    if (n === 0 || m === 0) {
        return n + m;
    }
    var x = 0, y, py, a, b, c, d, e, f, k;

    var p = new Array(n);

    for (y = 0; y &lt; n;) {
        p[y] = ++y;
    }

    for (; (x + 3) &lt; m; x += 4) {
        var tx0 = t.charCodeAt(x);
        var tx1 = t.charCodeAt(x + 1);
        var tx2 = t.charCodeAt(x + 2);
        var tx3 = t.charCodeAt(x + 3);
        a = x;
        b = x + 1;
        c = x + 2;
        d = x + 3;
        e = x + 4;
        for (y = 0; y &lt; n; y++) {
            k = s.charCodeAt(y);
            py = p[y];
            if (py &lt; a || b &lt; a) {
                a = (py &gt; b ? b + 1 : py + 1);
            }
            else {
                if (tx0 !== k) {
                    a++;
                }
            }

            if (a &lt; b || c &lt; b) {
                b = (a &gt; c ? c + 1 : a + 1);
            }
            else {
                if (tx1 !== k) {
                    b++;
                }
            }

            if (b &lt; c || d &lt; c) {
                c = (b &gt; d ? d + 1 : b + 1);
            }
            else {
                if (tx2 !== k) {
                    c++;
                }
            }

            if (c &lt; d || e &lt; d) {
                d = (c &gt; e ? e + 1 : c + 1);
            }
            else {
                if (tx3 !== k) {
                    d++;
                }
            }

            p[y] = e = d;
            d = c;
            c = b;
            b = a;
            a = py;
        }
    }

    for (; x &lt; m;) {
        tx0 = t.charCodeAt(x);
        a = x;
        b = ++x;
        for (y = 0; y &lt; n; y++) {
            py = p[y];
            if (py &lt; a || b &lt; a) {
                b = (py &gt; b ? b + 1 : py + 1);
            }
            else {
                if (tx0 !== s.charCodeAt(y)) {
                    b = a + 1;
                }
                else {
                    b = a;
                }
            }
            p[y] = b;
            a = py;
        }
        f = b;
    }

    return f;
}
</code></pre>

<p>I would also not use <code>map</code> in <code>getLeastEditDistance</code>, it is very slow. Just use a normal loop. Also <code>Math.min</code> with many arguments is not very performant.</p>
",""
"35476018","2016-02-18 08:11:21","4","","35455089","<p>Parsing an ANTLR grammar is not difficult. I have done this as part of my <a href=""https://github.com/mysql/mysql-workbench/blob/master/library/parsers/code-completion/mysql-code-completion.cpp#L249"" rel=""nofollow noreferrer"">code completion implementation in MySQL Workbench</a>.</p>

<p>You need the ANTLR meta grammar and generate a parser from it. Then use that to load you own grammar into a structure you can use to generate the matrix from.</p>

<p>As a head start you could use <a href=""https://github.com/mysql/mysql-workbench/tree/master/backend/wbpublic/sqlide/grammar-parser"" rel=""nofollow noreferrer"">the ANTLR grammar parser I have created</a>, but that is for a C/C++ target, not Java. So, you have to get the ANTLR3.g file from the ANTLR homepage and create your own parser from it.</p>
",""
"35465203","2016-02-17 18:51:13","1","","35458896","<p>Here's a cute function from <a href=""https://github.com/alvations/pywsd/blob/master/pywsd/utils.py#L92"" rel=""nofollow""><code>pywsd</code></a>:</p>

<pre><code>from nltk.corpus import wordnet as wn

def penn2morphy(penntag, returnNone=False):
    morphy_tag = {'NN':wn.NOUN, 'JJ':wn.ADJ,
                  'VB':wn.VERB, 'RB':wn.ADV}
    try:
        return morphy_tag[penntag[:2]]
    except:
        return None if returnNone else ''
</code></pre>
",""
"35255992","2016-02-07 16:28:36","0","","35176283","<p>You can not induce any sort of grammatical structure from bigram probabilities.* You could use bigrams to build a language model, though, which may serve some of the same purposes as a parser.</p>

<p><sub>* Actually, you might be able to build a very rudimentary chunking algorithm by starting a new chunk whenever bigram probabiity drops below a certain threshold. But this is nowhere near the accuracy / granularity that is going to be useful for most tasks.</sub></p>
",""
"35255930","2016-02-07 16:24:47","0","","35168281","<p>As I understand your question, you are asking how to estimate the parameters of a PCFG model from data.</p>

<p>In short, it's easy to make empirical production-rule probability estimates when you have ground-truth parses in your training data. If you want to estimate the probability that <code>S -&gt; NP VP</code>, this is something like <code>Count(S -&gt; NP VP) / Count(S -&gt; *)</code>, where <code>*</code> is any possible subtree.</p>

<p>You can find a much more formal statement in lots of places on the web (search for ""PCFG estimation"" or ""PCFG learning""). Here's a nice one from Michael Collins' lecture notes: <a href=""http://www.cs.columbia.edu/~mcollins/courses/nlp2011/notes/pcfgs.pdf#page=9"" rel=""nofollow"">http://www.cs.columbia.edu/~mcollins/courses/nlp2011/notes/pcfgs.pdf#page=9</a></p>
",""
"35111764","2016-01-31 08:20:13","1","","35029648","<p>Adding <code>LoggingT</code> into the base of the stack works right away, but judging from</p>

<blockquote>
  <p>No instance for (MonadLogger (FreeT CraftDSL IO))
    arising from the 'deriving' clause of a data type declaration</p>
</blockquote>

<p>you want to log stuff in your DSL do-blocks. For this we need to make <code>FreeT</code> an instance of <code>MonadLogger</code>:</p>

<pre><code>instance (MonadLogger m, Functor f) =&gt; MonadLogger (FreeT f m) where
    monadLoggerLog loc source level msg = lift $ monadLoggerLog loc source level msg
</code></pre>

<p>Thanks to <code>LoggingT</code> already be an instance of <code>MonadLogger</code>, the <code>(MonadLogger m) =&gt; MonadLogger (FreeT f m)</code> constraint holds for your <code>Craft</code> type.</p>

<p>This code compiled for me, but since you didn't provide a minimal testcase i'm not sure if it really works.</p>
",""
"35069076","2016-01-28 18:20:06","15","","19994396","<p>I was also looking for a solution to this and couldn't find any, so a friend and I built a tool to do this. I thought I would come back and share incase others found it helpful.</p>

<p><a href=""https://github.com/akoumjian/datefinder"" rel=""noreferrer"">datefinder -- find and extract dates inside text</a></p>

<p>Here's an example:</p>

<pre><code>import datefinder

string_with_dates = '''
    Central design committee session Tuesday 10/22 6:30 pm
    Th 9/19 LAB: Serial encoding (Section 2.2)
    There will be another one on December 15th for those who are unable to make it today.
    Workbook 3 (Minimum Wage): due Wednesday 9/18 11:59pm
    He will be flying in Sept. 15th.
    We expect to deliver this between late 2021 and early 2022.
'''

matches = datefinder.find_dates(string_with_dates)
for match in matches:
    print(match)
</code></pre>
",""
"35041502","2016-01-27 15:31:46","0","","35032659","<p>It's necessary to tokenize your text before lemmatizing.</p>

<p>Without tokenization:</p>

<pre><code>&gt;&gt;&gt; from nltk import word_tokenize
&gt;&gt;&gt; from nltk.stem import WordNetLemmatizer
&gt;&gt;&gt; wnl = WordNetLemmatizer()

&gt;&gt;&gt; [wnl.lemmatize(i) for i in ""the woman's going home"".split()]
['the', ""woman's"", 'going', 'home']
&gt;&gt;&gt; [wnl.lemmatize(i) for i in ""the women's home is in London"".split()]
['the', ""women's"", 'home', 'is', 'in', 'London']
</code></pre>

<p>With tokenization:</p>

<pre><code>&gt;&gt;&gt; [wnl.lemmatize(i) for i in word_tokenize(""the woman's going home"")]
['the', 'woman', ""'s"", 'going', 'home']
&gt;&gt;&gt; [wnl.lemmatize(i) for i in word_tokenize(""the women's home is in London"")]
['the', u'woman', ""'s"", 'home', 'is', 'in', 'London']
</code></pre>
",""
"35000615","2016-01-25 19:22:22","3","","34989721","<p>SENNA uses the CoNLL format. You can read about it here: <a href=""http://universaldependencies.github.io/docs/format.html"" rel=""nofollow noreferrer"">http://universaldependencies.github.io/docs/format.html</a></p>

<p>It's rather common and there are plenty of converters around.</p>

<p>As for the prefixes they mean: S- singleton expressions and B- begin I- intermediate E- end of a multi word expression.</p>

<p>Then there is the output of the semantic role labeling. Look for more information on SRL as this gets a little more complex. Notice there are two columns, one for the verb go and one for the verb eat. Usually A0 is the subject and A1 the direct object (again, oversimplified). AM is the argument modifier and -LOC is a location (it could be other adverbs). PNC seems to refer to the surrogate noun phrase acting as object of the verb go. Don't remember from the top of my head. Examples here verbs.colorado.edu/propbank/framesets-english/go-v.html 
As for the parse tree, it's bracketed and also a common notation loosely inspired by Lisp. The * indicates the label of the current token. I found this useful: <a href=""https://math.stackexchange.com/questions/588230/how-to-convert-parentheses-notation-for-trees-into-an-actual-tree-drawing"">https://math.stackexchange.com/questions/588230/how-to-convert-parentheses-notation-for-trees-into-an-actual-tree-drawing</a></p>
",""
"34971823","2016-01-24 03:18:03","5","","34968716","<p>Once again, no model is perfect (see <a href=""https://stackoverflow.com/questions/30821188/python-nltk-pos-tag-not-returning-the-correct-part-of-speech-tag"">Python NLTK pos_tag not returning the correct part-of-speech tag</a>) ;P</p>

<p>You can try a ""more accurate"" parser, using the <code>NeuralDependencyParser</code>.</p>

<p>First setup the parser properly with the correct environment variables (see <a href=""https://stackoverflow.com/questions/13883277/stanford-parser-and-nltk/34112695#34112695"">Stanford Parser and NLTK</a> and <a href=""https://gist.github.com/alvations/e1df0ba227e542955a8a"" rel=""nofollow noreferrer"">https://gist.github.com/alvations/e1df0ba227e542955a8a</a>), then:</p>

<pre><code>&gt;&gt;&gt; from nltk.internals import find_jars_within_path
&gt;&gt;&gt; from nltk.parse.stanford import StanfordNeuralDependencyParser
&gt;&gt;&gt; parser = StanfordNeuralDependencyParser(model_path=""edu/stanford/nlp/models/parser/nndep/english_UD.gz"")
&gt;&gt;&gt; stanford_dir = parser._classpath[0].rpartition('/')[0]
&gt;&gt;&gt; slf4j_jar = stanford_dir + '/slf4j-api.jar'
&gt;&gt;&gt; parser._classpath = list(parser._classpath) + [slf4j_jar]
&gt;&gt;&gt; parser.java_options = '-mx5000m'
&gt;&gt;&gt; sent = ""John sees Bill""
&gt;&gt;&gt; [parse.tree() for parse in parser.raw_parse(sent)]
[Tree('sees', ['John', 'Bill'])]
</code></pre>

<p>Do note that the <code>NeuralDependencyParser</code> only produces the dependency trees:</p>

<p><a href=""https://i.sstatic.net/FkENU.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/FkENU.png"" alt=""enter image description here""></a></p>
",""
"34831393","2016-01-16 19:51:49","0","","34831167","<p>Per the <a href=""http://www.nltk.org/api/nltk.corpus.reader.html?highlight=wordnet#nltk.corpus.reader.wordnet.Lemma.synset"" rel=""noreferrer"">NLTK docs</a>, a <code>&lt;lemma&gt;.&lt;pos&gt;.&lt;number&gt;</code> Synset string is composed of the following parts:</p>

<blockquote>
  <ul>
  <li><code>&lt;lemma&gt;</code> is the word‚Äôs morphological stem </li>
  <li><code>&lt;pos&gt;</code> is one of the module attributes ADJ, ADJ_SAT, ADV, NOUN or VERB </li>
  <li><code>&lt;number&gt;</code> is the sense number, counting from 0</li>
  </ul>
</blockquote>

<p>Thus, the <code>&lt;pos&gt;</code> is the part of speech. 
According to <a href=""http://wordnet.princeton.edu/man/wndb.5WN.html"" rel=""noreferrer"">the wordnet man page</a>, the part of speech character has the following meaning:</p>

<pre><code>n    NOUN
v    VERB
a    ADJECTIVE
s    ADJECTIVE SATELLITE
r    ADVERB 
</code></pre>

<p>The <code>&lt;number&gt;</code> is used to disambiguate word meanings.</p>
",""
"34742540","2016-01-12 11:32:59","8","","34738782","<p>First install the <code>nltk_cli</code> as per the instructions: <a href=""https://github.com/alvations/nltk_cli"" rel=""nofollow"">https://github.com/alvations/nltk_cli</a></p>

<p>Then, here's a secret function in <code>nltk_cli</code>, maybe you'll find it useful:</p>

<pre><code>alvas@ubi:~/git/nltk_cli$ cat infile.txt 
something like how writer pro or phraseology works would be really cool .
more options like the syntax editor would be nice
alvas@ubi:~/git/nltk_cli$ python senna.py --chunk2 VP+ADJP infile.txt 
would be    really cool
would be    nice
</code></pre>

<p>To illustrate other possible usage:</p>

<pre><code>alvas@ubi:~/git/nltk_cli$ python senna.py --chunk2 VP+VP infile.txt 
!!! NO CHUNK of VP+VP in this sentence !!!
!!! NO CHUNK of VP+VP in this sentence !!!
alvas@ubi:~/git/nltk_cli$ python senna.py --chunk2 NP+VP infile.txt 
how writer pro or phraseology works would be
the syntax editor   would be
alvas@ubi:~/git/nltk_cli$ python senna.py --chunk2 VP+NP infile.txt 
!!! NO CHUNK of VP+NP in this sentence !!!
!!! NO CHUNK of VP+NP in this sentence !!!
</code></pre>

<p>Then if you want to check if the phrase in sentence and output True/False, simply read and iterate through the outputs from <code>nltk_cli</code> and check with <code>if-else</code> conditions.</p>
",""
"34677248","2016-01-08 12:32:52","7","","34672986","<p>Assuming you want to check literally for ""would"" followed by ""be"", followed by some adjective, you can do this:</p>

<pre><code>def would_be(tagged):
    return any(['would', 'be', 'JJ'] == [tagged[i][0], tagged[i+1][0], tagged[i+2][1]] for i in xrange(len(tagged) - 2))
</code></pre>

<p>The input is a POS tagged sentence (list of tuples, as per NLTK).</p>

<p>It checks if there are any three elements in the list such that ""would"" is next to ""be"" and ""be"" is next to a word tagged as an adjective ('JJ'). It will return <code>True</code> as soon as this ""pattern"" is matched.</p>

<p>You can do something very similar for the second type of sentence:</p>

<pre><code>def am_able_to(tagged):
    return any(['am', 'able', 'to', 'VB'] == [tagged[i][0], tagged[i+1][0], tagged[i+2][0], tagged[i+3][1]] for i in xrange(len(tagged) - 3))
</code></pre>

<p>Here's a driver for the program:</p>

<pre><code>s1 = [('This', 'DT'), ('feature', 'NN'), ('would', 'MD'), ('be', 'VB'), ('nice', 'JJ'), ('to', 'TO'), ('have', 'VB')]
s2 = [('I', 'PRP'), ('am', 'VBP'), ('able', 'JJ'), ('to', 'TO'), ('delete', 'VB'), ('the', 'DT'), ('group', 'NN'), ('functionality', 'NN')]

def would_be(tagged):
   return any(['would', 'be', 'JJ'] == [tagged[i][0], tagged[i+1][0], tagged[i+2][1]] for i in xrange(len(tagged) - 2))

def am_able_to(tagged):
    return any(['am', 'able', 'to', 'VB'] == [tagged[i][0], tagged[i+1][0], tagged[i+2][0], tagged[i+3][1]] for i in xrange(len(tagged) - 3))

sent1 = ' '.join(s[0] for s in s1)
sent2 = ' '.join(s[0] for s in s2)

print(""Is '{1}' of type 'would be' + adj? {0}"".format(would_be(s1), sent1))
print(""Is '{1}' of type 'am able to' + verb? {0}"".format(am_able_to(s1), sent1))

print(""Is '{1}' of type 'would be' + adj? {0}"".format(would_be(s2), sent2))
print(""Is '{1}' of type 'am able to' + verb? {0}"".format(am_able_to(s2), sent2))
</code></pre>

<p>This correctly outputs:</p>

<pre><code>Is 'This feature would be nice to have' of type 'would be' + adj? True
Is 'This feature would be nice to have' of type 'am able to' + verb? False
Is 'I am able to delete the group functionality' of type 'would be' + adj? False
Is 'I am able to delete the group functionality' of type 'am able to' + verb? True
</code></pre>

<p>If you'd like to generalize this, you can change whether you're checking the literal words or their POS tag. </p>
",""
"34590315","2016-01-04 11:39:12","0","","34587293","<p>To find the dependency head of sentence, simply look for nodes that whose <code>head</code> values points to the <code>root</code> node. In <code>NLTK</code> API to <a href=""https://github.com/nltk/nltk/blob/develop/nltk/parse/dependencygraph.py"" rel=""nofollow"">DependencyGraph</a>, you can easily look for the node that its head points to the 1st index of the dictionary. </p>

<p>Do note that in dependency parsing unlike typical chomsky normal form / CFG parse trees there might be more than one head to the dependency parse.</p>

<p>But since you're casting the dependency output into a Tree structure, you can do the following:</p>

<pre><code>tree_head = next(n for n in p_tree.node_values() if n['head'] == 1)
</code></pre>

<p>But do note that linguistically, the head in the sentence<code>Download and share this tool</code> should be <code>Download</code> <strong>and</strong> <code>share</code>. But computationally a tree is hierarchical and a normal-form tree would have <code>ROOT-&gt;Download-&gt;and-&gt;share</code> but some parsers might produce this tree too: <code>ROOT-&gt;and-&gt;Download;share</code></p>
",""
"34559915","2016-01-01 20:48:17","1","","34557078","<p>It seems like you've caught a bug in NLTK implementations! This <code>try-except</code> is wrong at <a href=""https://github.com/alvations/nltk/blob/develop/nltk/translate/bleu_score.py#L76"" rel=""nofollow noreferrer"">https://github.com/alvations/nltk/blob/develop/nltk/translate/bleu_score.py#L76</a></p>

<hr>

<p><strong>In Long:</strong></p>

<p>Firstly, let's go through what the <code>p_n</code> in BLEU score means:</p>

<p><a href=""https://i.sstatic.net/DMEui.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/DMEui.png"" alt=""enter image description here""></a></p>

<p><strong>Note that</strong>:</p>

<ul>
<li>the Papineni formula is based on a corpus-level BLEU score and the native implementation is using a sentence-level BLEU score (the bleeding edge version of NLTK contains an implementation that follows the Papineni paper to calculate corpus level BLEU).</li>
<li>in multi-reference BLEU, the  <code>Count_match(ngram)</code> is based on the reference with a higher count (see <a href=""https://github.com/alvations/nltk/blob/develop/nltk/translate/bleu_score.py#L270"" rel=""nofollow noreferrer"">https://github.com/alvations/nltk/blob/develop/nltk/translate/bleu_score.py#L270</a>).</li>
</ul>

<p>So the default BLEU score uses <code>n=4</code> which includes unigrams to 4grams. For each ngrams, lets' calculate the <code>p_n</code>:</p>

<pre><code>&gt;&gt;&gt; from collections import Counter
&gt;&gt;&gt; from nltk import ngrams
&gt;&gt;&gt; hyp = u""Èâ¥‰∫é ÁæéÂõΩ ÈõÜ ÁªèÊµé ‰∏é Ë¥∏Êòì ÊúÄÂ§ß ÂõΩ‰∫é ‰∏ÄË∫´ Ôºå ‰∏äËø∞ Âõ†Á¥† Áõ¥Êé• ÂΩ±Âìç ÁùÄ ‰∏ñÁïå Ë¥∏Êòì „ÄÇ"".split()
&gt;&gt;&gt; ref1 = u""Ëøô‰∫õ Áõ¥Êé• ÂΩ±Âìç ÂÖ®ÁêÉ Ë¥∏Êòì Âíå ÁæéÂõΩ ÊòØ ‰∏ñÁïå ‰∏ä ÊúÄÂ§ß ÁöÑ Âçï‰∏Ä ÁöÑ ÁªèÊµé Âíå Ë¥∏ÊòìÂïÜ „ÄÇ"".split()
&gt;&gt;&gt; ref2 = u""Ëøô‰∫õ Áõ¥Êé• ÂΩ±Âìç ÂÖ®ÁêÉ Ë¥∏Êòì Âíå ÁæéÂõΩ ÊòØ ‰∏ñÁïå ‰∏ä ÊúÄÂ§ß ÁöÑ Âçï‰∏Ä ÁöÑ ÁªèÊµé Âíå Ë¥∏ÊòìÂïÜ „ÄÇ"".split()
# Calculate p_1, p_2, p_3 and p_4
&gt;&gt;&gt; from nltk.translate.bleu_score import _modified_precision
&gt;&gt;&gt; p_1 = _modified_precision([ref1, ref2], hyp, 1)
&gt;&gt;&gt; p_2 = _modified_precision([ref1, ref2], hyp, 2)
&gt;&gt;&gt; p_3 = _modified_precision([ref1, ref2], hyp, 3)
&gt;&gt;&gt; p_4 = _modified_precision([ref1, ref2], hyp, 4)
&gt;&gt;&gt; p_1, p_2, p_3, p_4
(Fraction(4, 9), Fraction(1, 17), Fraction(0, 1), Fraction(0, 1))
</code></pre>

<p>Note the latest version of <code>_modified_precision</code> in BLEU score since this <a href=""https://github.com/nltk/nltk/pull/1229"" rel=""nofollow noreferrer"">https://github.com/nltk/nltk/pull/1229</a> has been using <code>Fraction</code> instead of <code>float</code> outputs. So now, we can clearly see the numerator and the denominator.</p>

<p>So let's now verify the outputs from the <code>_modified_precision</code> for unigram. In the hypothesis the bold words occurs in the references:</p>

<ul>
<li>Èâ¥‰∫é <strong>ÁæéÂõΩ</strong> ÈõÜ <strong>ÁªèÊµé</strong> ‰∏é <strong>Ë¥∏Êòì</strong> <strong>ÊúÄÂ§ß</strong> ÂõΩ‰∫é ‰∏ÄË∫´ Ôºå ‰∏äËø∞ Âõ†Á¥† <strong>Áõ¥Êé•</strong> <strong>ÂΩ±Âìç</strong> ÁùÄ <strong>‰∏ñÁïå</strong> <strong>Ë¥∏Êòì</strong> <strong>„ÄÇ</strong></li>
</ul>

<p>There are 9 tokens overlapping with 1 of the 9 is a duplicate that occurs twice.</p>

<pre><code>&gt;&gt;&gt; from collections import Counter
&gt;&gt;&gt; ref1_unigram_counts = Counter(ngrams(ref1, 1))
&gt;&gt;&gt; ref2_unigram_counts = Counter(ngrams(ref2, 1))
&gt;&gt;&gt; hyp_unigram_counts = Counter(ngrams(hyp,1))
&gt;&gt;&gt; for overlaps in set(hyp_unigram_counts.keys()).intersection(ref1_unigram_counts.keys()):
...     print "" "".join(overlaps)
... 
ÁæéÂõΩ
Áõ¥Êé•
ÁªèÊµé
ÂΩ±Âìç
„ÄÇ
ÊúÄÂ§ß
‰∏ñÁïå
Ë¥∏Êòì
&gt;&gt;&gt; overlap_counts = Counter({ng:hyp_unigram_counts[ng] for ng in set(hyp_unigram_counts.keys()).intersection(ref1_unigram_counts.keys())})
&gt;&gt;&gt; overlap_counts
Counter({(u'\u8d38\u6613',): 2, (u'\u7f8e\u56fd',): 1, (u'\u76f4\u63a5',): 1, (u'\u7ecf\u6d4e',): 1, (u'\u5f71\u54cd',): 1, (u'\u3002',): 1, (u'\u6700\u5927',): 1, (u'\u4e16\u754c',): 1})
</code></pre>

<p>Now let's check how many times these overlapping words occurs in the references. Taking the value of the ""combined"" counters from the different references as our numerator for the <code>p_1</code> formula. And if the same word occurs in both references, take the maximum count.</p>

<pre><code>&gt;&gt;&gt; overlap_counts_in_ref1 = Counter({ng:ref1_unigram_counts[ng] for ng in set(hyp_unigram_counts.keys()).intersection(ref1_unigram_counts.keys())})
&gt;&gt;&gt; overlap_counts_in_ref2 = Counter({ng:ref2_unigram_counts[ng] for ng in set(hyp_unigram_counts.keys()).intersection(ref1_unigram_counts.keys())})
&gt;&gt;&gt; overlap_counts_in_ref1
Counter({(u'\u7f8e\u56fd',): 1, (u'\u76f4\u63a5',): 1, (u'\u7ecf\u6d4e',): 1, (u'\u5f71\u54cd',): 1, (u'\u3002',): 1, (u'\u6700\u5927',): 1, (u'\u4e16\u754c',): 1, (u'\u8d38\u6613',): 1})
&gt;&gt;&gt; overlap_counts_in_ref2
Counter({(u'\u7f8e\u56fd',): 1, (u'\u76f4\u63a5',): 1, (u'\u7ecf\u6d4e',): 1, (u'\u5f71\u54cd',): 1, (u'\u3002',): 1, (u'\u6700\u5927',): 1, (u'\u4e16\u754c',): 1, (u'\u8d38\u6613',): 1})
&gt;&gt;&gt; overlap_counts_in_ref1_ref2 = Counter()
&gt;&gt;&gt; numerator = overlap_counts_in_ref1_ref2
&gt;&gt;&gt; 
&gt;&gt;&gt; for c in [overlap_counts_in_ref1, overlap_counts_in_ref2]:
...     for k in c:
...             numerator[k] = max(numerator.get(k,0), c[k])
... 
&gt;&gt;&gt; numerator
Counter({(u'\u7f8e\u56fd',): 1, (u'\u76f4\u63a5',): 1, (u'\u7ecf\u6d4e',): 1, (u'\u5f71\u54cd',): 1, (u'\u3002',): 1, (u'\u6700\u5927',): 1, (u'\u4e16\u754c',): 1, (u'\u8d38\u6613',): 1})
&gt;&gt;&gt; sum(numerator.values())
8
</code></pre>

<p>Now for the denominator, it's simply the no. of unigrams that appears in the hypothesis:</p>

<pre><code>&gt;&gt;&gt; hyp_unigram_counts
Counter({(u'\u8d38\u6613',): 2, (u'\u4e0e',): 1, (u'\u7f8e\u56fd',): 1, (u'\u56fd\u4e8e',): 1, (u'\u7740',): 1, (u'\u7ecf\u6d4e',): 1, (u'\u5f71\u54cd',): 1, (u'\u56e0\u7d20',): 1, (u'\u4e16\u754c',): 1, (u'\u3002',): 1, (u'\u4e00\u8eab',): 1, (u'\u6700\u5927',): 1, (u'\u9274\u4e8e',): 1, (u'\u4e0a\u8ff0',): 1, (u'\u96c6',): 1, (u'\u76f4\u63a5',): 1, (u'\uff0c',): 1})
&gt;&gt;&gt; sum(hyp_unigram_counts.values())
18
</code></pre>

<p>So the resulting fraction is <code>8/18 -&gt; 4/9</code> and our <code>_modified_precision</code> function checks out. </p>

<p>Now lets get to the full BLEU formula:</p>

<p><a href=""https://i.sstatic.net/SBC40.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/SBC40.png"" alt=""enter image description here""></a></p>

<p>From the formula let's consider only the exponential of the summation for now, i.e <code>exp(...)</code>. It can be also simplified as the sum of the logarithm of the various <code>p_n</code> as we calculated previously, i.e. <code>sum(log(p_n))</code>. And that is how it is implemented in NLTK, see <a href=""https://github.com/alvations/nltk/blob/develop/nltk/translate/bleu_score.py#L79"" rel=""nofollow noreferrer"">https://github.com/alvations/nltk/blob/develop/nltk/translate/bleu_score.py#L79</a></p>

<p>Ignoring the BP for now, let's consider summing the <code>p_n</code> and taking their respective weights into consideration:</p>

<pre><code>&gt;&gt;&gt; from fractions import Fraction
&gt;&gt;&gt; from math import log
&gt;&gt;&gt; log(Fraction(4, 9))
-0.8109302162163288
&gt;&gt;&gt; log(Fraction(1, 17))
-2.833213344056216
&gt;&gt;&gt; log(Fraction(0, 1))
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
ValueError: math domain error
</code></pre>

<p>Ah ha! That's where the error appears and the sum of the logs would have returned a <code>ValueError</code> when putting them through <code>math.fsum()</code>.</p>

<pre><code>&gt;&gt;&gt; try:
...     sum(log(pi) for pi in (Fraction(4, 9), Fraction(1, 17), Fraction(0, 1), Fraction(0, 1)))
... except ValueError:
...     0
... 
0
</code></pre>

<p>To correct the implementation, <code>the try-except</code> should have been:</p>

<pre><code>s = []
# Calculates the overall modified precision for all ngrams.
# by summing the the product of the weights and the respective log *p_n*
for w, p_n in zip(weights, p_ns)):
    try:
        s.append(w * math.log(p_n))
    except ValueError:
        # some p_ns is 0
        s.append(0)
 return sum(s)
</code></pre>

<p><strong>References:</strong></p>

<p>The formulas comes from <a href=""http://lotus.kuee.kyoto-u.ac.jp/WAT/papers/submissions/W15/W15-5009.pdf"" rel=""nofollow noreferrer"">http://lotus.kuee.kyoto-u.ac.jp/WAT/papers/submissions/W15/W15-5009.pdf</a> that describes some sensitivity issues with BLEU.</p>
",""
"34478716","2015-12-27 08:13:40","4","","34477757","<p><strong>TL;DR</strong></p>

<pre><code>&gt;&gt;&gt; from nltk.corpus import brown
&gt;&gt;&gt; {word for word, pos in brown.tagged_words() if pos.startswith('NN')}
</code></pre>

<hr>

<p><strong>In Longer</strong></p>

<p>Iterate through the <code>.tagged_words()</code> function and that will return a list of <code>('word', 'POS')</code> tuples:</p>

<pre><code>&gt;&gt;&gt; from nltk.corpus import brown
&gt;&gt;&gt; brown.tagged_words()
[(u'The', u'AT'), (u'Fulton', u'NP-TL'), ...]
</code></pre>

<p>Please read this chapter to know how NLTK corpora API works: <a href=""http://www.nltk.org/book/ch02.html"" rel=""nofollow"">http://www.nltk.org/book/ch02.html</a></p>

<p>Then, do a list comprehension over it and save a set (i.e. unique list) of the words that are tagged with the noun tags, e.g. <code>NN, NNS, NNP, etc.</code>. </p>

<pre><code>&gt;&gt;&gt; {word for word, pos in brown.tagged_words() if pos.startswith('NN')}
</code></pre>

<p>Note that the output <em>might not</em> be what you expect because words that are POS tagged with syntactic and <strong>syntactic noun is not necessary a semantic argument/entity</strong>. </p>

<hr>

<p>Also, I don't think that the words you've extracted are correct. Double checking the list:</p>

<pre><code>&gt;&gt;&gt; nouns = {word for word, pos in brown.tagged_words() if pos.startswith('NN')} 
&gt;&gt;&gt; 'rather' in nouns
False
&gt;&gt;&gt; 'such' in nouns
False
&gt;&gt;&gt; 'Quite' in nouns
False
&gt;&gt;&gt; 'quite' in nouns
False
&gt;&gt;&gt; 'Such' in nouns
False
</code></pre>

<p>The output to the list comprehension: <a href=""http://pastebin.com/bJaPdpUk"" rel=""nofollow"">http://pastebin.com/bJaPdpUk</a></p>

<hr>

<p><strong>Why <code>random.choice(nn)</code> fails when <code>nn</code> is a set?</strong></p>

<p>The input to <code>random.choice()</code> is a sequence (see <a href=""https://docs.python.org/2/library/random.html#random.choice"" rel=""nofollow"">https://docs.python.org/2/library/random.html#random.choice</a>). </p>

<blockquote>
  <p><strong>random.choice(seq)</strong></p>
  
  <p>Return a random element from the non-empty sequence
  seq. If seq is empty, raises IndexError.</p>
</blockquote>

<p>And python sequence types in python are </p>

<ul>
<li><code>str, unicode, list, tuple, bytearray, buffer, xrange</code> in Python 2.x (see <a href=""https://docs.python.org/2/library/stdtypes.html#sequence-types-str-unicode-list-tuple-bytearray-buffer-xrange"" rel=""nofollow"">https://docs.python.org/2/library/stdtypes.html#sequence-types-str-unicode-list-tuple-bytearray-buffer-xrange</a>).</li>
<li><code>list, tuple, range</code> in Python 3.x (see <a href=""https://docs.python.org/3.6/library/stdtypes.html#sequence-types-list-tuple-range"" rel=""nofollow"">https://docs.python.org/3.6/library/stdtypes.html#sequence-types-list-tuple-range</a>)</li>
<li>(binary sequence types) <code>bytes, bytearray, memoryview</code> in Python 3.x </li>
<li>(text string sequence) <code>str</code> in Python 3.x</li>
</ul>

<p>Since <code>set</code> isn't a sequence, you will get the <code>IndexError</code>.</p>
",""
"34458164","2015-12-24 21:44:27","4","","34439208","<p>Firstly, see your other question to setup Stanford CoreNLP to be called from command-line or python: <a href=""https://stackoverflow.com/questions/34455749/nltk-how-to-prevent-stemming-of-proper-nouns"">nltk : How to prevent stemming of proper nouns</a>.</p>

<p>For the proper cased sentence we see that the NER works properly:</p>

<pre><code>&gt;&gt;&gt; from corenlp import StanfordCoreNLP
&gt;&gt;&gt; nlp = StanfordCoreNLP('http://localhost:9000')
&gt;&gt;&gt; text = ('John Donk works POI Jones wants meet Xyz Corp measuring POI short term performance metrics. '
... 'john donk works poi jones wants meet xyz corp measuring poi short term performance metrics')
&gt;&gt;&gt; output = nlp.annotate(text, properties={'annotators': 'tokenize,ssplit,pos,ner',  'outputFormat': 'json'})
&gt;&gt;&gt; annotated_sent0 = output['sentences'][0]
&gt;&gt;&gt; annotated_sent1 = output['sentences'][1]
&gt;&gt;&gt; for token in annotated_sent0['tokens']:
...     print token['word'], token['lemma'], token['pos'], token['ner']
... 
John John NNP PERSON
Donk Donk NNP PERSON
works work VBZ O
POI POI NNP ORGANIZATION
Jones Jones NNP ORGANIZATION
wants want VBZ O
meet meet VB O
Xyz Xyz NNP ORGANIZATION
Corp Corp NNP ORGANIZATION
measuring measure VBG O
POI poi NN O
short short JJ O
term term NN O
performance performance NN O
metrics metric NNS O
. . . O
</code></pre>

<p>And for the lowered cased sentence, you will not get <code>NNP</code> for POS tag nor any NER tag:</p>

<pre><code>&gt;&gt;&gt; for token in annotated_sent1['tokens']:
...     print token['word'], token['lemma'], token['pos'], token['ner']
... 
john john NN O
donk donk JJ O
works work NNS O
poi poi VBP O
jones jone NNS O
wants want VBZ O
meet meet VB O
xyz xyz NN O
corp corp NN O
measuring measure VBG O
poi poi NN O
short short JJ O
term term NN O
performance performance NN O
metrics metric NNS O
</code></pre>

<p>So the question to your question should be:</p>

<ul>
<li><strong>What is the ultimate aim of your NLP application?</strong></li>
<li><strong>Why is your input lower-cased? Was it your doing or how the data was provided?</strong></li>
</ul>

<p>And after answering those questions, you can move on to decide what you really want to do with the NER tags, i.e.</p>

<ul>
<li><p><strong>If the input is lower-cased and it's because of how you structured your NLP tool chain</strong>, then</p>

<ul>
<li><strong>DO NOT do that!!!</strong> Perform the NER on the normal text without distortions you've created. It's because the NER was trained on normal text so it won't really work out of the context of normal text.</li>
<li>Also try to not mix it NLP tools from different suites they will usually not play nice, especially at the end of your NLP tool chain</li>
</ul></li>
<li><p><strong>If the input is lower-cased because that's how the original data was</strong>, then: </p>

<ul>
<li>Annotate a small portion of the data, or find annotated data that was lowercased and then retrain a model.</li>
<li>Work around it and train a truecaser with normal text then apply the truecasing model to the lower-cased text. See <a href=""https://www.cs.cmu.edu/~llita/papers/lita.truecasing-acl2003.pdf"" rel=""nofollow noreferrer"">https://www.cs.cmu.edu/~llita/papers/lita.truecasing-acl2003.pdf</a></li>
</ul></li>
<li><p><strong>If the input has erroneous casing, e.g. `Some big Some Small but not all are Proper Noun</strong>, then</p>

<ul>
<li>Try the truecasing solution too.</li>
</ul></li>
</ul>
",""
"34376149","2015-12-19 22:41:57","0","","34375657","<p>Answering the stated question: by <a href=""https://en.wikipedia.org/wiki/Precision_and_recall"" rel=""nofollow"">definition</a>, precision is computed by the first formula: TP/(TP+FP).</p>

<p>However, it doesn't mean that you have to use this formula, i.e. precision measure. There are many other measures, look at the table on <a href=""https://en.wikipedia.org/wiki/Confusion_matrix"" rel=""nofollow"">this wiki page</a> and choose the one most suited for your task. </p>

<p>For example, <a href=""https://en.wikipedia.org/wiki/Positive_likelihood_ratio"" rel=""nofollow"">positive likelihood ratio</a> seems to be the most similar to your second formula.</p>
",""
"34361516","2015-12-18 18:06:31","4","","34351978","<p>Close but minor changes to your regex will get you your desired output. When you want to get a wildcard using <code>RegexpParser</code> grammar, you should use <code>.*</code> instead of <code>*</code>, e.g. <code>VB.*</code> instead of <code>VB*</code>:</p>

<pre><code>&gt;&gt;&gt; from nltk import word_tokenize, pos_tag, RegexpParser
&gt;&gt;&gt; text = ""This has allowed the device to start, and I then see glitches which is not nice.""
&gt;&gt;&gt; tagged_text = pos_tag(word_tokenize(text))    
&gt;&gt;&gt; g = r""""""
... VP: {&lt;VB.*&gt;&lt;DT&gt;&lt;NN.*&gt;}
... """"""
&gt;&gt;&gt; p = RegexpParser(g); p.parse(tagged_text)
Tree('S', [('This', 'DT'), ('has', 'VBZ'), Tree('VP', [('allowed', 'VBN'), ('the', 'DT'), ('device', 'NN')]), ('to', 'TO'), ('start', 'VB'), (',', ','), ('and', 'CC'), ('I', 'PRP'), ('then', 'RB'), ('see', 'VBP'), ('glitches', 'NNS'), ('which', 'WDT'), ('is', 'VBZ'), ('not', 'RB'), ('nice', 'JJ'), ('.', '.')])
</code></pre>

<p>Note that you're catching the <code>Tree(AdvP, [('then', 'RB'), ('see', 'VB')])</code>,  because the tags are exactly <code>RB</code> and <code>VB</code>. So the wildcard in your grammar (i.e. `""""""AdvP: {}"""""") in this scenario is ignored.</p>

<p>Also, if it's two different types of phrases, it's more advisable to use 2 labels not one. And (i think) end of string after wildcard is sort of redundant, so it's better to:</p>

<pre><code>g = r""""""
VP:{&lt;VB.*&gt;&lt;DT&gt;&lt;NN.*&gt;} 
AdvP: {&lt;RB.*&gt;&lt;VB.*|JJ.*|NN.*&gt;}
""""""
</code></pre>
",""
"34255998","2015-12-13 20:31:58","3","","34242030","<p>That's an interesting question. The NLTK implements mapping to the Universal tagset only for a fixed collection of corpora, with the help of the fixed maps you found in <code>nltk_data/taggers/universal_tagset/</code>. Except for a few special cases (which include treating the brown corpus as if it was named <code>en-brown</code>), the rule is to look for a mapping file that has the same name as the tagset used for your corpus. In your case, the tagset is set to ""unknown"", which is why you see that message.</p>

<p>Now, are you sure ""the mapping for Spanish"", i.e. the map <code>es-cast3lb.map</code>, actually matches the tagset for your corpus? I certainly wouldn't just assume it does, since any project can create their own tagset and rules for use. <strong>If this is the same tagset your corpus uses,</strong> your problem has an easy solution:</p>

<ul>
<li><p>When you initialize your corpus reader, e.g. <code>cess_esp</code>, add the option <code>tagset=""es-cast3lb""</code> to the constructor. If necessary, e.g. for corpora already loaded by the NLTK with <code>tagset=""unknown""</code>, you can override the tagset after initialization like this: </p>

<pre><code>cess_esp._tagset = ""es-cast3lb""
</code></pre></li>
</ul>

<p>This tells the corpus reader what tagset is used in the corpus. After that, specifying <code>tagset=""universal""</code> should cause the selected mapping to be applied.</p>

<p>If <strong>this tagset is not actually suited</strong> to your corpus, your first job is to study the documentation of the tagset for your corpus, and create an appropriate mapping to the Universal tagset; as you've probably seen, the format is pretty trivial. You can then put your mapping in operation by dropping it in <code>nltk_data/taggers/universal_tagset</code>. Adding your own resources to the <code>nltk_data</code> area is decidedly a hack, but if you get this far, I recommend you contribute your tagset map to the nltk... which will resolve the hack after the fact.</p>

<p><strong>Edit:</strong> So (per the comments) it's the right tagset, but only the 1-2 letter POS tags are in the mapping dictionary (the rest of the tag presumably describes the features of inflected words). Here's a quick way to extend the mapping dictionary on the fly, so that you can see the universal tags:</p>

<pre><code>import nltk
from nltk.corpus import cess_esp
cess_esp._tagset = ""es-cast3lb""

nltk.tag.mapping._load_universal_map(""es-cast3lb"")  # initialize; normally loaded on demand
mapdict = nltk.tag.mapping._MAPPINGS[""es-cast3lb""][""universal""] # shortcut to the map

alltags = set(t for w, t in cess_esp.tagged_words())
for tag in alltags:
    if len(tag) &lt;= 2:   # These are complete
        continue
    mapdict[tag] = mapdict[tag[:2]]
</code></pre>

<p>This discards the agreement information. If you'd rather decorate the ""universal"" tags with it, just set <code>mapdict[tag]</code> to <code>mapdict[tag[:2]]+""-""+tag[2:]</code>.</p>

<p>I'd save this dictionary to a file as described above, so that you don't have to recompute the mapping every time you load your corpus.</p>
",""
"34249648","2015-12-13 09:26:32","4","","34249579","<p>You have to split the text first. You're currently parsing the literal text you posted with quotes and everything. This is evident by this part of the parsing result: <code>(""'"", 'POS')</code></p>

<p>To do that you seem to be able to use <code>ast.literal_eval</code> on each line. Note that an apostrophe (in a word like ""don't"") will ruin the formatting and you'll have to handle the apostrophes yourself with something like <code>line = line[1:-1]</code>:</p>

<pre><code>import ast
from nltk.parse.stanford import StanfordDependencyParser
dependency_parser = StanfordDependencyParser(  model_path=""edu\stanford\lp\models\lexparser\englishPCFG.ser.gz"")

with open('sample.txt',encoding=""latin-1"") as f:
    lines = [ast.litral_eval(line) for line in f.readlines()]

for line in lines:
    parsed_lines = dependency_parser.raw_parse(line)

# now parsed_lines should contain the parsed lines from the file
</code></pre>
",""
"34236148","2015-12-12 04:07:32","6","","34232047","<p>Check out <code>read_csv</code> from the <code>pandas</code> library. Here is the documentation: <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html"" rel=""nofollow"">http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html</a></p>

<p>You can install pandas by running <code>pip install pandas</code> at the command line. Then loading the csv and selecting that column should be as easy as the below:</p>

<pre><code>data = pd.read_csv(path_to_csv)
docs = data['col1']

tfs = tfidf.fit_transform(docs)
</code></pre>
",""
"34236002","2015-12-12 03:44:43","8","","34232190","<p>You have to do a little bit of a song and dance to get the matrices as numpy arrays instead, but this should do what you're looking for:</p>

<pre><code>feature_array = np.array(tfidf.get_feature_names())
tfidf_sorting = np.argsort(response.toarray()).flatten()[::-1]

n = 3
top_n = feature_array[tfidf_sorting][:n]
</code></pre>

<p>This gives me:</p>

<pre><code>array([u'fruit', u'travellers', u'jupiter'], 
  dtype='&lt;U13')
</code></pre>

<p>The <code>argsort</code> call is really the useful one, <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html"" rel=""noreferrer"">here are the docs for it</a>. We have to do <code>[::-1]</code> because <code>argsort</code> only supports sorting small to large. We call <code>flatten</code> to reduce the dimensions to 1d so that the sorted indices can be used to index the 1d feature array. Note that including the call to <code>flatten</code> will only work if you're testing one document at at time.</p>

<p>Also, on another note, did you mean something like <code>tfs = tfidf.fit_transform(t.split(""\n\n""))</code>? Otherwise, each term in the multiline string is being treated as a ""document"". Using <code>\n\n</code> instead means that we are actually looking at 4 documents (one for each line), which makes more sense when you think about tfidf.</p>
",""
"34231849","2015-12-11 20:17:00","2","","34198237","<h2>Linguistically</h2>

<p>What you're looking out for when you look for triplets that contains a <code>JJ</code> and an <code>NN</code> is usually a Noun phrase <code>NP</code> in a context-free grammar.</p>

<p>In dependency grammar, what you're looking for is a triplet that contains the the JJ and NN POS tags in the <strong>arguments</strong>. Most specifically, when you're for a constituent / branch that contains an adjectival modified Noun. From the <code>StanfordDepdencyParser</code> output, you need to look for the <strong>predicate</strong> <code>amod</code>. (If you're confused with what's explained above it is advisable to read up on Dependency grammar before proceeding, see <a href=""https://en.wikipedia.org/wiki/Dependency_grammar"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Dependency_grammar</a>.</p>

<p>Note that the parser outputs the triplets, <code>(arg1, pred, arg2)</code>, where the argument 2 (<code>arg2</code>) depends on argument 1 (<code>arg1</code>) through the predicate (<code>pred</code>) relation; i.e. <code>arg1</code> governs <code>arg2</code> (see, <a href=""https://en.wikipedia.org/wiki/Government_(linguistics)"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Government_(linguistics)</a>)</p>

<hr>

<h2>Pythonically</h2>

<p>Now to the code part of the answer. You want to iterate through a list of tuples (i.e. triplets) so the easiest solution is to specifically assign variables to the tuples as you iterate, then check for the conditions you need see <a href=""https://stackoverflow.com/questions/2191699/find-an-element-in-a-list-of-tuples"">Find an element in a list of tuples</a></p>

<pre><code>&gt;&gt;&gt; x = [(('university', 'NN'), 'nsubj', ('bit', 'NN')), (('university', 'NN'), 'cop', ('is', 'VBZ')), (('university', 'NN'), 'amod', ('good', 'JJ'))]
&gt;&gt;&gt; for arg1, pred, arg2 in x:
...     word1, pos1 = arg1
...     word2, pos2 = arg2
...     if pos1.startswith('NN') and pos2.startswith('JJ') and pred == 'amod':
...             print ((arg1, pred, arg2))
... 
(('university', 'NN'), 'amod', ('good', 'JJ'))
</code></pre>
",""
"34093919","2015-12-04 17:18:32","0","","34090734","<p>Firstly, let's take a look at the POS tags that NLTK gives:</p>

<pre><code>&gt;&gt;&gt; from nltk import pos_tag
&gt;&gt;&gt; sent = 'The pizza was awesome and brilliant'.split()
&gt;&gt;&gt; pos_tag(sent)
[('The', 'DT'), ('pizza', 'NN'), ('was', 'VBD'), ('awesome', 'JJ'), ('and', 'CC'), ('brilliant', 'JJ')]
&gt;&gt;&gt; sent = 'The pizza was good but pasta was bad'.split()
&gt;&gt;&gt; pos_tag(sent)
[('The', 'DT'), ('pizza', 'NN'), ('was', 'VBD'), ('good', 'JJ'), ('but', 'CC'), ('pasta', 'NN'), ('was', 'VBD'), ('bad', 'JJ')]
</code></pre>

<p>(Note: The above are the outputs from NLTK v3.1 <code>pos_tag</code>, older version might differ)</p>

<p>What you want to capture is essentially:</p>

<ul>
<li>NN VBD JJ CC JJ</li>
<li>NN VBD JJ</li>
</ul>

<p>So let's catch them with these patterns:</p>

<pre><code>&gt;&gt;&gt; from nltk import RegexpParser
&gt;&gt;&gt; sent1 = ['The', 'pizza', 'was', 'awesome', 'and', 'brilliant']
&gt;&gt;&gt; sent2 = ['The', 'pizza', 'was', 'good', 'but', 'pasta', 'was', 'bad']
&gt;&gt;&gt; patterns = """"""
... P: {&lt;NN&gt;&lt;VBD&gt;&lt;JJ&gt;&lt;CC&gt;&lt;JJ&gt;}
... {&lt;NN&gt;&lt;VBD&gt;&lt;JJ&gt;}
... """"""
&gt;&gt;&gt; PChunker = RegexpParser(patterns)
&gt;&gt;&gt; PChunker.parse(pos_tag(sent1))
Tree('S', [('The', 'DT'), Tree('P', [('pizza', 'NN'), ('was', 'VBD'), ('awesome', 'JJ'), ('and', 'CC'), ('brilliant', 'JJ')])])
&gt;&gt;&gt; PChunker.parse(pos_tag(sent2))
Tree('S', [('The', 'DT'), Tree('P', [('pizza', 'NN'), ('was', 'VBD'), ('good', 'JJ')]), ('but', 'CC'), Tree('P', [('pasta', 'NN'), ('was', 'VBD'), ('bad', 'JJ')])])
</code></pre>

<hr>

<p><strong>So that's ""cheating"" by hardcoding!!!</strong></p>

<p>Let's go back to the POS patterns:</p>

<ul>
<li>NN VBD JJ CC JJ</li>
<li>NN VBD JJ</li>
</ul>

<p>Can be simplified to:</p>

<ul>
<li>NN VBD JJ (CC JJ)</li>
</ul>

<p>So you can use the optional operators in the regex, e.g.:</p>

<pre><code>&gt;&gt;&gt; patterns = """"""
... P: {&lt;NN&gt;&lt;VBD&gt;&lt;JJ&gt;(&lt;CC&gt;&lt;JJ&gt;)?}
... """"""
&gt;&gt;&gt; PChunker = RegexpParser(patterns)
&gt;&gt;&gt; PChunker.parse(pos_tag(sent1))
Tree('S', [('The', 'DT'), Tree('P', [('pizza', 'NN'), ('was', 'VBD'), ('awesome', 'JJ'), ('and', 'CC'), ('brilliant', 'JJ')])])
&gt;&gt;&gt; PChunker.parse(pos_tag(sent2))
Tree('S', [('The', 'DT'), Tree('P', [('pizza', 'NN'), ('was', 'VBD'), ('good', 'JJ')]), ('but', 'CC'), Tree('P', [('pasta', 'NN'), ('was', 'VBD'), ('bad', 'JJ')])])
</code></pre>

<hr>

<p>Most probably you're using the old tagger, that's why your patterns are different but I guess you see how you could capture the phrases you need using the example above.</p>

<p>The steps are:</p>

<ul>
<li>First, check what is the POS patterns using the <code>pos_tag</code></li>
<li>Then generalize patterns and simplify them</li>
<li>Then put them into the <code>RegexpParser</code></li>
</ul>
",""
"33906606","2015-11-25 00:42:58","0","","27728001","<p>I think you got right way. What you need is just a lexicon to determine whether a given word is a known word or unknown word. <a href=""http://rdrpostagger.sourceforge.net/"" rel=""nofollow"">RDRPOSTagger</a> provides a piece of code to compute tagging accuracies for known words and unknown words. See the function <code>computeAccuracies(lexicon, goldCorpus, taggedCorpus)</code> in the <code>Eval.py</code> module in the <code>Utility</code> package. </p>

<p>You might want to look at <a href=""http://arxiv.org/abs/1412.4021"" rel=""nofollow"">this paper</a> which presents tagging results (for known words and unknown words) of 3 POS and morphological taggers on 13 languages including Bulgarian, Czech, Dutch, English, French, German, Hindi, Italian, Portuguese, Spanish, Swedish, Thai and Vietnamese.</p>
",""
"33816257","2015-11-19 23:26:04","3","","33815401","<pre><code>def extract_np(psent):
  for subtree in psent.subtrees():
    if subtree.label() == 'NP':
      yield ' '.join(word for word, tag in subtree.leaves())


cp = nltk.RegexpParser(grammar)
parsed_sent = cp.parse(tagged_sent)
for npstr in extract_np(parsed_sent):
    print (npstr)
</code></pre>
",""
"33804870","2015-11-19 13:09:10","0","","33784448","<p>It sounds like you want to process many phrase pairs quickly, and the API provided here is not serviceable.</p>

<p>Your options for avoiding the pain of the network are:</p>

<ol>
<li><strong>Use an alternate API that's available in batch.</strong> If there was a call that accepted many pairs of phrases and output many similarity scores at once, this would allow you to go faster -- but as long as their API embodies a function call of <code>(lhs, rhs) -&gt; score</code> you're going to be limited by how fast you can call the function. </li>
</ol>

<p>There's a <a href=""https://stackoverflow.com/questions/16205020"">related question</a> that was closed as being off-topic, but which mentions <a href=""http://api.cortical.io/"" rel=""nofollow noreferrer"">cortical.io</a> as an API that provides a ""bulk"" compare.</p>

<ol start=""2"">
<li><strong>Ask for the source to run it yourself.</strong> Reach out to the organization hosting the API and ask if they can make their source code available (publicly or just to you).</li>
<li><strong>Implement their method or something similar yourself.</strong></li>
</ol>

<p>To help with 3., I've provided some resources below.</p>

<p>Poking around their <a href=""http://swoogle.umbc.edu/SimService/"" rel=""nofollow noreferrer"">website</a>, and the <a href=""http://ebiquity.umbc.edu/papers/"" rel=""nofollow noreferrer"">group's publication page</a>, I found this publication which may be interesting.</p>

<p>Abhay L. Kashyap et al., <a href=""http://ebiquity.umbc.edu/get/a/publication/777.pdf"" rel=""nofollow noreferrer"">""Robust Semantic Text Similarity Using LSA, Machine Learning and Linguistic Resources""</a>, Language Resources and Evaluation, January 2016, 73 downloads.</p>

<p>For something that's easier to implement, and at least competitive in performance, I would recommend looking at word vector approaches to similarity, like <a href=""http://nlp.stanford.edu/projects/glove/"" rel=""nofollow noreferrer"">Stanford's GloVe</a> or <a href=""https://code.google.com/p/word2vec/"" rel=""nofollow noreferrer"">Google's word2vec</a> (you might have to retrain to get phrases of the size you want, or you can play tricks with averaging or adding vectors to represent phrases).</p>
",""
"33766792","2015-11-17 20:54:19","4","","33705555","<p>Since you tagged this <code>nltk</code>, let's use the NLTK's tree parser to process your trees. We'll read in each tree, then simply print out the leaves. Done.</p>

<pre><code>&gt;&gt;&gt; text =""(CLAUSE (NP Jack/NNP) (VP stayed/VBD) (NP in/IN London/NNP))""
&gt;&gt;&gt; tree = nltk.Tree.fromstring(text, read_leaf=lambda x: x.split(""/"")[0])
&gt;&gt;&gt; print(tree.leaves())

['Jack', 'stayed', 'in', 'London']
</code></pre>

<p>The lambda form splits each <code>word/tag</code> pair and discards the tag, keeping just the word.</p>

<h3>Multiple trees</h3>

<p>I know, you're going to ask me how to process a whole file's worth of such trees, and some of them take more than one line. That's the job of the NLTK's <code>BracketParseCorpusReader</code>, but it expects terminals to be in the form <code>(POS word)</code> instead of <code>word/POS</code>. I won't bother doing it that way, since it's even easier to trick <code>Tree.fromstring()</code> into reading all your trees as if they're branches of a single tree:</p>

<pre><code>allmytext = """"""
(CLAUSE (NP Jack/NNP) (VP loved/VBD) (NP Peter/NNP))
(CLAUSE (NP Jack/NNP) (VP stayed/VBD) (NP in/IN London/NNP))
(CLAUSE (NP Tom/NNP) (VP is/VBZ) (NP in/IN Kolkata/NNP))
""""""
wrapped = ""(ROOT ""+ allmytext + "" )""  # Add a ""root"" node at the top
trees = nltk.Tree.fromstring(wrapped, read_leaf=lambda x: x.split(""/"")[0])
for tree in trees:
    print(tree.leaves())
</code></pre>

<p>As you see, the only difference is we added <code>""(ROOT ""</code> and <code>"" )""</code> around the file contents, and used a for-loop to generate the output. The loop gives us the children of the top node, i.e. the actual trees.</p>
",""
"33745331","2015-11-16 21:53:11","0","","33705923","<p>You should visit recursively the cells of your table and unfold them in the same way you did for the S node until everything is a terminal (so you don't have anything else to unfold). In your example, you first go to cell [0][2]; this is a terminal, you don't have to do anything. Next you go to [2][5], this is a non-terminal made by [2][3] and [3][5]. You visit [2][3], it's a terminal. [3][5] is a non-terminal, made by two terminals. You are done. Here is a demo in Python:</p>

<pre><code>class Node:
    '''Think this as a cell in your table'''
    def __init__(self, left, right, type, word):
        self.left = left
        self.right = right
        self.type = type
        self.word = word

# Declare terminals
t1 = Node(None,None,'MOD','can')
t2 = Node(None,None,'PRON','you')
t3 = Node(None,None,'VERB', 'eat')
t4 = Node(None,None,'DET', 'a')
t5 = Node(None,None,'NOUN','flower')

# Declare non-terminals
nt1 = Node(t1,t2, 'NP', None)
nt2 = Node(t4,t5, 'NP', None)
nt3 = Node(t3,nt2,'VP', None)
nt4 = Node(nt1,nt3,'S', None)

def unfold(node):
    # Check for a terminal
    if node.left == None and node.right == None:
        return node.word+""_""+node.type

    return ""[""+unfold(node.left)+"" ""+unfold(node.right)+""]_""+node.type

print unfold(nt4)
</code></pre>

<p>And the output:</p>

<pre><code>[[can_MOD you_PRON]_NP [eat_VERB [a_DET flower_NOUN]_NP]_VP]_S
</code></pre>
",""
"33720145","2015-11-15 13:20:44","0","","33176328","<p>How is IDF calculated?</p>

<p>If you have something like <code>IDF = log_e(# of documents / # of documents with term)</code> you could do <code>log_e(# of documents +1 / 1)</code>. i.e. treat the document as a new document in the corpus.</p>
",""
"33677051","2015-11-12 16:58:33","4","","33676526","<p>Use <code>pos_tag_sents</code> for tagging multiple sentences:</p>

<pre><code>&gt;&gt;&gt; import time
&gt;&gt;&gt; from nltk.corpus import brown
&gt;&gt;&gt; from nltk import pos_tag
&gt;&gt;&gt; from nltk import pos_tag_sents
&gt;&gt;&gt; sents = brown.sents()[:10]
&gt;&gt;&gt; start = time.time(); pos_tag(sents[0]); print time.time() - start
0.934092998505
&gt;&gt;&gt; start = time.time(); [pos_tag(s) for s in sents]; print time.time() - start
9.5061340332
&gt;&gt;&gt; start = time.time(); pos_tag_sents(sents); print time.time() - start 
0.939551115036
</code></pre>
",""
"33611944","2015-11-09 15:01:02","0","","33611766","<p>You can check with <code>wordnet.synsets(token)</code>. Be sure to deal with punctuation also, then just check if it's in the list. Here's an example:</p>

<pre><code>from nltk.tokenize import WordPunctTokenizer
from nltk.corpus import wordnet

my_list_of_strings = []  # populate list before using

wpt = WordPunctTokenizer()
only_recognized_words = []

for s in my_list_of_strings:
    tokens = wpt.tokenize(s)
    if tokens:  # check if empty string
        for t in tokens:
            if wordnet.synsets(t):
                only_recognized_words.append(t)  # only keep recognized words
</code></pre>

<p>But you should really create some custom logic for handling Twitter data, particular handling hash tags, @replies, usernames, links, retweets, etc. There are plenty of papers with strategies to glean from.</p>
",""
"33600258","2015-11-08 23:24:55","2","","33594721","<p><strong>In short</strong>:</p>

<p>It's sort of a strange case of exception. </p>

<p>There's also a case where <code>I saw the log the into half.</code> where ""saw"" is a present tense verb. </p>

<p>See @nschneid solution to use more fine-grain tags in the issue raised: <a href=""https://github.com/nltk/nltk/issues/1196"" rel=""noreferrer"">https://github.com/nltk/nltk/issues/1196</a></p>

<hr>

<p><strong>In long</strong>:</p>

<p>If we take a look at how we call the WordNet lemmatizer in NLTK:</p>

<pre><code>&gt;&gt;&gt; from nltk.stem import WordNetLemmatizer
&gt;&gt;&gt; wnl = WordNetLemmatizer()
&gt;&gt;&gt; wnl.lemmatize('saw', pos='v')
'saw'
&gt;&gt;&gt; wnl.lemmatize('saw')
'saw'
</code></pre>

<p>Specifying the POS tag seems redundant. Let's take a look at the lemmatizer code itself:</p>

<pre><code>class WordNetLemmatizer(object):
    def __init__(self):
        pass

    def lemmatize(self, word, pos=NOUN):
        lemmas = wordnet._morphy(word, pos)
        return min(lemmas, key=len) if lemmas else word
</code></pre>

<p>What it does is it relies on the <code>_moprhy</code> property of the wordnet corpus to return possible lemmas.</p>

<p>If we thread through the <code>nltk.corpus.wordnet</code> code, we see the <code>_morphy()</code> code at  <a href=""https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/wordnet.py#L1679"" rel=""noreferrer"">https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/wordnet.py#L1679</a> </p>

<p>The first few lines of the function reads the exception file from wordnet's <code>verb.exc</code>, i.e. <a href=""https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/wordnet.py#L1687"" rel=""noreferrer"">https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/wordnet.py#L1687</a></p>

<p>So if we do an ad-hoc search of the exception outside of the lemmatizer function, we do see that <code>'saw' -&gt; 'see'</code>:</p>

<pre><code>&gt;&gt;&gt; from nltk.corpus import wordnet as wn
&gt;&gt;&gt; exceptions = wn._exception_map['v']
&gt;&gt;&gt; exceptions['saw']
[u'see']
</code></pre>

<p>So if we call the <code>_morphy()</code> function outside of the lemmatizer:</p>

<pre><code>&gt;&gt;&gt; from nltk.corpus import wordnet as wn
&gt;&gt;&gt; exceptions = wn._exception_map['v']
&gt;&gt;&gt; wn._morphy('saw', 'v')
['saw', u'see']
</code></pre>

<p>Let's go back to the return line of the <code>WordNetLemmatizer.lemmatize()</code> code, we see <code>return min(lemmas, key=len) if lemmas else word</code>:</p>

<pre><code>def lemmatize(self, word, pos=NOUN):
    lemmas = wordnet._morphy(word, pos)
    return min(lemmas, key=len) if lemmas else word
</code></pre>

<p>So that means the function will return the output from <code>wn._morphy()</code> with the minimum length. But in this case both saw and see has the same length so the first on the list returned by <code>wn._morphy()</code> will be the returned, i.e. <code>saw</code>.</p>

<p>Effectively, the <code>WordNetLemmatizer.lemmatize()</code> is doing this:</p>

<pre><code>&gt;&gt;&gt; from nltk.corpus import wordnet as wn
&gt;&gt;&gt; wn._morphy('saw', 'v')
['saw', u'see']
&gt;&gt;&gt; min(wn._morphy('saw', 'v'), key=len)
'saw'
</code></pre>

<p>So the question is:</p>

<ul>
<li><strong>How can I avoid this ""bug"" in NLTK?</strong></li>
<li><strong>How can one fix this ""bug"" in NLTK?</strong></li>
</ul>

<p>But note that it's not exactly a ""bug"" but a ""feature"" to represent other possible lemmas of a surface word (although that word in that specific context is rare, e.g. <code>I saw the log into half</code>.</p>

<hr>

<p><strong>How can I avoid this ""bug"" in NLTK?</strong></p>

<p>To avoid this ""bug"" in NLTK, use <code>nltk.wordnet._morphy()</code> instead of <code>nltk.stem.WordNetLemmatizer.lemmatize()</code> that way you will always get a list of possible lemmas instead of the lemma that is filtered by length. To lemmatize:</p>

<pre><code>&gt;&gt;&gt; from nltk.corpus import wordnet as wn
&gt;&gt;&gt; exceptions = wn._exception_map['v']
&gt;&gt;&gt; wn._morphy('saw', pos='v')
['saw', 'see']
</code></pre>

<p>More choice is better than a wrong choice.</p>

<hr>

<p><strong>How to fix this ""bug"" in NLTK?</strong></p>

<p>Other than the <code>min(lemmas, key=len)</code> being sub-optimal, the <code>_morphy()</code> function is a little inconsistent when dealing with exceptions because of rare meaning in the plural words that might be a lemma by itself, e.g. using <code>teeth</code> to refer to dentures, see <a href=""http://wordnetweb.princeton.edu/perl/webwn?s=teeth"" rel=""noreferrer"">http://wordnetweb.princeton.edu/perl/webwn?s=teeth</a></p>

<pre><code>&gt;&gt;&gt; wn._morphy('teeth', 'n')
['teeth', u'tooth']
&gt;&gt;&gt; wn._morphy('goose', 'n')
['goose']
&gt;&gt;&gt; wn._morphy('geese', 'n')
[u'goose']
</code></pre>

<p>So the error in lemma choices must have been introduced in the <code>nltk.wordnet._morphy()</code> function after the exception list. One quick hack is to immediately return the first instance of the exception list if the input surface word occurs in the exception list, e.g.:</p>

<pre><code>from nltk.corpus import wordnet as wn
def _morphy(word, pos):
    exceptions = wn._exception_map[pos]
    if word in exceptions:
        return exceptions[word]

    # Else, continue the rest of the _morphy code.
</code></pre>
",""
"33330620","2015-10-25 14:09:48","1","","33326704","<p>In the current scikit-learn release, your code results in the following warning:</p>

<pre><code>DeprecationWarning: Direct support for sequence of sequences multilabel
    representation will be unavailable from version 0.17. Use
    sklearn.preprocessing.MultiLabelBinarizer to convert to a label
    indicator representation.
</code></pre>

<p>Following this advice, you can use <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MultiLabelBinarizer.html"" rel=""noreferrer""><code>sklearn.preprocessing.MultiLabelBinarizer</code></a> to convert this multilabel class to a form accepted by <code>f1_score</code>. For example:</p>

<pre><code>from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.metrics import f1_score

y_true = [[1,2,3]]
y_pred = [[1,2,3]]

m = MultiLabelBinarizer().fit(y_true)

f1_score(m.transform(y_true),
         m.transform(y_pred),
         average='macro')
# 1.0
</code></pre>
",""
"33330487","2015-10-25 13:56:32","1","","33326810","<blockquote>
  <p>First, if there is any reference that justifies the usage of weighted-F1, I am just curios in which cases I should use weighted-F1.</p>
</blockquote>

<p>I don't have any references, but if you're interested in multi-label classification where you care about precision/recall of <strong>all</strong> classes, then the weighted f1-score is appropriate. If you have binary classification where you just care about the positive samples, then it is probably not appropriate.</p>

<blockquote>
  <p>Second, I heard that weighted-F1 is deprecated, is it true?</p>
</blockquote>

<p>No, weighted-F1 itself is not being deprecated. Only some aspects of the function interface were deprecated, back in v0.16, and then only to make it more explicit in previously ambiguous situations. (Historical discussion <a href=""https://github.com/scikit-learn/scikit-learn/pull/2679"" rel=""noreferrer"">on github</a> or check out <a href=""https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/metrics/classification.py"" rel=""noreferrer"">the source code</a> and search the page for ""deprecated"" to find details.)</p>

<blockquote>
  <p>Third, how actually weighted-F1 is being calculated?</p>
</blockquote>

<p>From the documentation of <code>f1_score</code>:</p>

<pre><code>``'weighted'``:
  Calculate metrics for each label, and find their average, weighted
  by support (the number of true instances for each label). This
  alters 'macro' to account for label imbalance; it can result in an
  F-score that is not between precision and recall.
</code></pre>

<p>So the average is weighted by the <em>support</em>, which is the number of samples with a given label. Because your example data above does not include the support, it is impossible to compute the weighted f1 score from the information you listed.</p>
",""
"33291533","2015-10-22 22:02:44","2","","33232303","<p>You can check out <a href=""https://code.google.com/p/foma/"" rel=""nofollow"">FOMA</a> which is a C library (It is also available as standalone executable for Windows). It is based on <a href=""https://en.wikipedia.org/wiki/Kimmo_Koskenniemi"" rel=""nofollow"">Kimmo Koskenniemi</a>'s computational model that utilizes finite-state transducers. It is the open source version of <a href=""http://www.fsmbook.com/"" rel=""nofollow"">xfst</a>. You can see a quick crash course <a href=""https://code.google.com/p/foma/wiki/GettingStarted"" rel=""nofollow"">here</a>. </p>

<p>It is very easy to use foma. <a href=""https://github.com/yurap/spanish-morphology"" rel=""nofollow"">This repo</a> on github could serve as a sample (Check out the <code>spanish.lexc</code> and <code>spanish.foma</code> files). If you fire up foma and put the two scripts in the same directory, you can load the file and test the morphological realizer:</p>

<p>foma[0]: source spanish.foma<br>
Opening file 'spanish.foma'.<br>
defined Word: 1.6 kB. 2 states, 64 arcs, Cyclic.<br>
defined Cleanup: 276 bytes. 1 state, 2 arcs, Cyclic.<br>
Root...5, A...2, N...2, V1...65, V2...65, V3...65<br>
Building lexicon...<br>
Determinizing...<br>
Minimizing...<br>
Done!<br>
7.9 kB. 289 states, 441 arcs, 199 paths.<br>
defined Lexicon: 7.9 kB. 289 states, 441 arcs, 199 paths. 
9.2 kB. 290 states, 505 arcs, Cyclic.                     </p>

<p>Now the good thing about FOMA is that it is two-ways. It can realize and analyze morphological forms at the same time. If you apply <em>up</em> it dissects forms, but if you apply <em>down</em> it acts as a realizer:</p>

<pre><code>foma[1]: up                       
apply up&gt; leo                     
leo+N+Sg                          
leo+A+Sg                          
leir+V+3C+PresenteIndicativo+1P+Sg
leer+V+2C+PresenteIndicativo+1P+Sg
lear+V+1C+PresenteIndicativo+1P+Sg
</code></pre>

<p>In the case of to-be, here's an example of how to use the transducer as a realizer:</p>

<pre><code>foma[1]: down
apply down&gt; estar+V+1C+PresenteIndicativo+3P+Sg
esta
</code></pre>

<p>Remember that you define the tags yourself at the start of the lexc script, so you can easily change or augment the existing script in that repo. If you actually read through the documentation, you'll quickly get the hang of it. It's very convenient and easy to use. Good luck!</p>
",""
"33271991","2015-10-22 01:27:36","0","","33242858","<p>I studied <a href=""https://en.wikipedia.org/wiki/Latent_semantic_indexing"" rel=""nofollow noreferrer"">LSI</a> and topic modeling almost a year ago, so what I say should be taken as merely a pointer to give you a general idea of where to look.  </p>

<p>There are many different ways to do this with varying degrees of success. This is a hard problem in the realm of <a href=""https://en.wikipedia.org/wiki/Information_retrieval"" rel=""nofollow noreferrer"">information retrieval</a>. You can search for <a href=""https://en.wikipedia.org/wiki/Topic_model"" rel=""nofollow noreferrer"">topic modeling</a> to learn about different options and state of the art. </p>

<p>You definitely need some preprocessing and normalization if the words could appear in different forms. How about NLTK and one of its stemmers:</p>

<pre><code>&gt;&gt;&gt; from nltk.stem.lancaster import LancasterStemmer
&gt;&gt;&gt; st = LancasterStemmer()
&gt;&gt;&gt; st.stem('applied')
'apply'
&gt;&gt;&gt; st.stem('applies')
'apply'
</code></pre>

<p>You have a lexicon of terms that I am going to call <em>terms</em> and also a bunch of <em>documents</em>. I am going to explore a very basic technique to rank documents with regards to the terms. There are a gazillion more sophisticated ways you can read about, but I think this might be enough if you are not looking for something too sophisticated and rigorous. </p>

<p>This is called a vector space IR model. Terms and documents are both converted to vectors in a k-dimensional space. For that we have to construct a <em>term-by-document matrix</em>. This is a sample matrix in which the numbers represent frequencies of the terms in documents:</p>

<p><a href=""https://i.sstatic.net/RlewF.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/RlewF.jpg"" alt=""enter image description here""></a></p>

<p>So far we have a 3x4 matrix using which each document can be expressed by a 3-dimensional array (each column). But as the number of terms increase, these arrays become too large and increasingly sparse. Also, there are many words such as <code>I</code> or <code>and</code> that occur in most of the documents without adding much semantic content. So you might want to disregard these types of words. For the problem of largeness and sparseness, you can use a mathematical technique called <a href=""https://en.wikipedia.org/wiki/Singular_value_decomposition"" rel=""nofollow noreferrer"">SVD</a> that scales down the matrix while preserving most of the information it contains. </p>

<p>Also, the numbers we used on the above chart were raw counts. Another technique would be to use Boolean values: 1 for presence and 0 zero for lack of a term in a document. But these assume that words have equal semantic weights. In reality, rarer words have more <em>weight</em> than common ones. So, a good way to edit the initial matrix would be to use ranking functions like <a href=""https://en.wikipedia.org/wiki/Tf%E2%80%93idf"" rel=""nofollow noreferrer"">tf-id</a> to assign relative weights to each term. If by now we have applied SVD to our weighted term-by-document matrix, we can construct the k-dimensional query vectors, which are simply an array of the term weights. If our query contained multiple instances of the same term, the product of the frequency and the term weight would have been used. </p>

<p>What we need to do from there is somewhat straightforward. We compare the query vectors with document vectors by analyzing their <a href=""https://en.wikipedia.org/wiki/Cosine_similarity"" rel=""nofollow noreferrer"">cosine similarities</a> and that would be the basis for the ranking of the documents relative to the queries. </p>
",""
"33236991","2015-10-20 12:42:26","1","","33236269","<p>One way to fix this is to add the word <code>'coding'</code> to <code>wordnet._exception_map</code>:</p>

<pre><code>import nltk.stem as stem
import nltk.corpus as corpus
wordnet = corpus.wordnet
wordnet._exception_map['v']['coding'] = ['code']
wnl = stem.WordNetLemmatizer()   

print(wnl.lemmatize('coding', 'v'))
# code
</code></pre>

<p>Note that attributes which start with a single underscore are considered private -- i.e. they are not part of the public interface. So modifying <code>wordnet._exception_map</code> as above is not guaranteed to work in future versions of nltk. (The above works with NLTK version 3.0.0. It was found by looking at the source code for <a href=""https://github.com/nltk/nltk/blob/develop/nltk/stem/wordnet.py#L39"" rel=""nofollow""><code>WordNetLemmatizer.lemmatize</code></a> and <a href=""https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/wordnet.py#L1679"" rel=""nofollow""><code>wordnet._morphy</code></a>.)</p>

<p>Another way to fix the problem is to modify <code>nltk_data/corpora/wordnet/verb.exc</code>. The contents of the file looks like:</p>

<pre><code>cockneyfied cockneyfy
codded cod
codding cod
codified codify
cogged cog
cogging cog
</code></pre>

<p>if you add </p>

<pre><code>coding code
</code></pre>

<p>then this exception is added to <code>wordnet._exception_map</code> automatically for you.</p>

<p>The third option, less hacky then the previous two, is to convince the developers of Wordnet to add <code>coding code</code> to <code>nltk_data/copora/wordnet/verb.exc</code>.</p>
",""
"33115514","2015-10-14 02:19:35","0","","33115343","<p>By default, the tfidf rows are L2 normalized. <a href=""https://github.com/scikit-learn/scikit-learn/blob/a95203b/sklearn/feature_extraction/text.py#L1024"" rel=""nofollow"">Here</a> is the critical line in the source code.</p>

<pre><code>if self.norm:
        X = normalize(X, norm=self.norm, copy=False)
</code></pre>

<p><code>normalize()</code> comes from the sklearn.preprocessing module, where it indicates that it normalizes the rows by default. <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html#sklearn.preprocessing.normalize"" rel=""nofollow"">Here</a> is the link to the <code>normalize()</code> docs.</p>
",""
"33086422","2015-10-12 16:56:08","1","","33072971","<p>Your problem is this rule: <code>S -&gt; S U P | P U P</code></p>

<p>By allowing S to begin with an instance of S, you allow this infinite recursion:</p>

<pre><code>S -&gt; S U P
S -&gt; (S U P) U P
S -&gt; ((S U P) U P) U P
S -&gt; (((S U P) U P) U P) U P
</code></pre>

<p>This is called left recursion, and it is caused by a symbol expanding to itself, in this case S expanding to S.</p>

<p>From the <a href=""http://www.nltk.org/book_1ed/ch08.html#backtracks_index_term"" rel=""nofollow"">NLTK book, chapter 8</a>:</p>

<blockquote>
  <p>Recursive descent parsing has three key shortcomings. <strong>First,
  left-recursive productions like NP -> NP PP send it into an infinite
  loop.</strong></p>
</blockquote>

<p><strong>A solution</strong></p>

<p>Luckily, you can simply change the parser you use to one that does not share the left-recursive Achilles heel. Simple change this: </p>

<pre><code>rd_parser = nltk.RecursiveDescentParser(groucho_grammar)
</code></pre>

<p>to this:</p>

<pre><code>rd_parser = nltk.parse.chart.BottomUpLeftCornerChartParser(groucho_grammar)
</code></pre>

<p>This way you make use of the left-recursive-resistant <a href=""http://www.nltk.org/api/nltk.parse.html#nltk.parse.chart.BottomUpLeftCornerChartParser"" rel=""nofollow"">BottomUpLeftCornerChartParser</a></p>

<p><strong>Further reading</strong></p>

<p>The left-recursive problem is well-known in automata theory. There are ways to make your grammar non-recursive, as explained in these links:</p>

<ol>
<li><a href=""http://www.cs.engr.uky.edu/~lewis/essays/compilers/rec-des.html"" rel=""nofollow"">http://www.cs.engr.uky.edu/~lewis/essays/compilers/rec-des.html</a></li>
<li><a href=""http://www.umsl.edu/~kjs9rc/CS4890/presentation.pdf"" rel=""nofollow"">http://www.umsl.edu/~kjs9rc/CS4890/presentation.pdf</a></li>
<li><a href=""http://research.microsoft.com/pubs/68869/naacl2k-proc-rev.pdf"" rel=""nofollow"">http://research.microsoft.com/pubs/68869/naacl2k-proc-rev.pdf</a></li>
</ol>
",""
"33019535","2015-10-08 15:04:50","0","","33015326","<p>The MaltParser API in NLTK just had <a href=""https://github.com/nltk/nltk/pull/944"" rel=""nofollow noreferrer"">a patch</a> that fixes and stabilizes the problems that it used to have:</p>

<ul>
<li><a href=""https://stackoverflow.com/questions/14009330/how-to-use-malt-parser-in-python-nltk"">How to use malt parser in python nltk</a></li>
<li><a href=""https://stackoverflow.com/questions/20091698/malt-parser-throwing-class-not-found-exception"">Malt Parser throwing class not found exception</a></li>
<li><a href=""https://stackoverflow.com/questions/29513187/maltparser-not-working-in-python-nltk"">MaltParser Not Working in Python NLTK</a></li>
</ul>

<p>Here's an example of how to use MaltParser API in NLTK:</p>

<pre><code># Upgrade your NLTK.
alvas@ubi:~$ cd ~
alvas@ubi:~$ pip install -U nltk

# Get the latest MaltParser and model
alvas@ubi:~$ wget http://maltparser.org/dist/maltparser-1.8.1.zip
alvas@ubi:~$ unzip maltparser-1.8.1.zip 
alvas@ubi:~$ wget http://www.maltparser.org/mco/english_parser/engmalt.poly-1.7.mco

# In python, now you can do this:
alvas@ubi:~$ python
&gt;&gt;&gt; from nltk.parse.malt import MaltParser
&gt;&gt;&gt; mp = MaltParser('/home/alvas/maltparser-1.8.1', '/home/alvas/engmalt.poly-1.7.mco')
&gt;&gt;&gt; sent1 = 'I shot an elephant in my pajamas .'.split()
&gt;&gt;&gt; print(mp.parse_one(sent1).tree())
(shot I (elephant an (in (pajamas my))) .)
</code></pre>

<p>(See <a href=""https://github.com/nltk/nltk/blob/develop/nltk/parse/malt.py#L284"" rel=""nofollow noreferrer"">here</a> for more demo code or <a href=""https://github.com/nltk/nltk/blob/f4c16c2f9c46cc42c9b68ae746832b622581c6b5/nltk/parse/malt.py#L354"" rel=""nofollow noreferrer"">here</a> for a more elaborated demo code)</p>

<hr>

<p>Note that you can also use the export features and you can escape the usage of full path when initializing the <code>MaltParser</code> object. But you have to still tell the object what is the name of the parser directory and model filename to look for, e.g.</p>

<pre><code>alvas@ubi:~$ export MALT_PARSER='/home/$UID/maltparser-1.8.1/'
alvas@ubi:~$ export MALT_MODEL='/home/$UID/engmalt.poly-1.7.mco' 
alvas@ubi:~$ python
&gt;&gt;&gt; from nltk.parse.malt import MaltParser
&gt;&gt;&gt; mp = MaltParser('maltparser-1.8.1', 'engmalt.poly-1.7.mco')
&gt;&gt;&gt; sent1 = 'I shot an elephant in my pajamas .'.split()
&gt;&gt;&gt; print(mp.parse_one(sent1).tree())
(shot I (elephant an (in (pajamas my))) .)
</code></pre>
",""
"32981825","2015-10-07 00:22:48","0","","32981188","<p>The last sentence in your question indicates that you're aware of the <code>universal</code> tagset: It only has about 10 POS tags, because they need to be broad enough for other tagsets to be mapped <em>to</em> them. The Penn Treebank tagset has a many-to-many relationship to Brown, so no (reliable) automatic mapping is possible. </p>

<p>What you can do is use one of the corpora that are already tagged with the Penn Treebank tagset. The NLTK's sample of the <code>treebank</code> corpus is only 1/10th the size of Brown (100,000 words), but it might be enough for your purposes. </p>

<p>Alternately, you can simplify the Brown corpus yourself: If you only keep the first part of compound tags like <code>VBN-TL-HL</code> or <code>PPS+HVD</code>, the 472 complex tags are reduced to 71. If that's still too many, inspect the definitions and manually collapse it further, e.g. by merging <code>NN</code> and <code>NNS</code> (singular and plural).</p>
",""
"32973431","2015-10-06 15:08:58","5","","32973119","<p>It doesn't make any difference that how many time you used <code>iter</code> on an iterable object you can simply convert it to list by calling <code>list</code> function. </p>

<pre><code>&gt;&gt;&gt; l =[6, 3, 5, 1, 4, 2]
&gt;&gt;&gt; list(iter(iter(iter(iter(l)))))
[6, 3, 5, 1, 4, 2]
</code></pre>

<p>But if you just want to get the fist item you don't need to use <code>list</code> function you can simply use <code>next</code> method on an iterator or <code>next()</code> built-in function (in python 3.X you can just use built-in function <code>next()</code>) to get the forst item :</p>

<pre><code>&gt;&gt;&gt; iter(iter(l)).next()
6
&gt;&gt;&gt; iter(iter(iter(l))).next()
6
&gt;&gt;&gt; iter(iter(iter(iter(l)))).next()
6
</code></pre>

<p>Now about your problem if you didn't get a <code>list</code> after calling <code>list</code> after calling it surely it's not an iterator it would be another object type that you need to get its items based on how its  <a href=""https://docs.python.org/3/reference/datamodel.html#object.%5F%5Fgetitem%5F%5F"" rel=""nofollow""><code>__getitem__</code></a> method has been implemented.</p>

<p>Based on your edit <code>t1</code> is a list iterator object and doesn't has attribute <code>treepositions</code> you can loop over it's items and then call those <code>treepositions</code> attribute:</p>

<pre><code>s1 = parsedSentence[0]
t1 = Tree.convert(s1)
positions = [item.treepositions() for item in t1]
</code></pre>
",""
"32959872","2015-10-06 00:22:29","2","","32957895","<p>The lemmatizer requires the correct POS tag to be accurate, if you use the default settings of the <code>WordNetLemmatizer.lemmatize()</code>, the default tag is noun, see <a href=""https://github.com/nltk/nltk/blob/develop/nltk/stem/wordnet.py#L39"" rel=""noreferrer"">https://github.com/nltk/nltk/blob/develop/nltk/stem/wordnet.py#L39</a></p>

<p>To resolve the problem, always POS-tag your data before lemmatizing, e.g.</p>

<pre><code>&gt;&gt;&gt; from nltk.stem import WordNetLemmatizer
&gt;&gt;&gt; from nltk import pos_tag, word_tokenize
&gt;&gt;&gt; wnl = WordNetLemmatizer()
&gt;&gt;&gt; sent = 'This is a foo bar sentence'
&gt;&gt;&gt; pos_tag(word_tokenize(sent))
[('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('foo', 'NN'), ('bar', 'NN'), ('sentence', 'NN')]
&gt;&gt;&gt; for word, tag in pos_tag(word_tokenize(sent)):
...     wntag = tag[0].lower()
...     wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None
...     if not wntag:
...             lemma = word
...     else:
...             lemma = wnl.lemmatize(word, wntag)
...     print lemma
... 
This
be
a
foo
bar
sentence
</code></pre>

<p>Note that 'is -> be', i.e.</p>

<pre><code>&gt;&gt;&gt; wnl.lemmatize('is')
'is'
&gt;&gt;&gt; wnl.lemmatize('is', 'v')
u'be'
</code></pre>

<p>To answer the question with words from your examples:</p>

<pre><code>&gt;&gt;&gt; sent = 'These sentences involves some horsing around'
&gt;&gt;&gt; for word, tag in pos_tag(word_tokenize(sent)):
...     wntag = tag[0].lower()
...     wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None
...     lemma = wnl.lemmatize(word, wntag) if wntag else word
...     print lemma
... 
These
sentence
involve
some
horse
around
</code></pre>

<p>Note that there are some quirks with WordNetLemmatizer: </p>

<ul>
<li><a href=""https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python"">wordnet lemmatization and pos tagging in python</a> </li>
<li><a href=""https://stackoverflow.com/questions/22999273/python-nltk-lemmatization-of-the-word-further-with-wordnet"">Python NLTK Lemmatization of the word &#39;further&#39; with wordnet</a></li>
</ul>

<p>Also NLTK's default POS tagger is under-going some major changes to improve accuracy:</p>

<ul>
<li><a href=""https://stackoverflow.com/questions/30821188/python-nltk-pos-tag-not-returning-the-correct-part-of-speech-tag"">Python NLTK pos_tag not returning the correct part-of-speech tag</a></li>
<li><a href=""https://github.com/nltk/nltk/issues/1110"" rel=""noreferrer"">https://github.com/nltk/nltk/issues/1110</a></li>
<li><a href=""https://github.com/nltk/nltk/pull/1143"" rel=""noreferrer"">https://github.com/nltk/nltk/pull/1143</a></li>
</ul>

<p>And for an out-of-the-box / off-the-shelf solution to lemmatizer, you can take a look at <a href=""https://github.com/alvations/pywsd"" rel=""noreferrer"">https://github.com/alvations/pywsd</a> and how I've made some  try-excepts to catch words that are not in WordNet, see <a href=""https://github.com/alvations/pywsd/blob/master/pywsd/utils.py#L66"" rel=""noreferrer"">https://github.com/alvations/pywsd/blob/master/pywsd/utils.py#L66</a></p>
",""
"32956680","2015-10-05 19:45:46","0","","32939186","<p>I suggest ""Speech and Language Processing"" by Daniel Jurafsky &amp; James H. Martin. The last chapters are about Information Extraction and Summarization.</p>

<p>As for your question about the text size, it depends. From my experience, Information Extraction works better with short sentences. However, you will need a large data set to train your system in recognizing relevant patterns.</p>
",""
"32758921","2015-09-24 10:22:41","2","","32751688","<p>Quote:</p>

<blockquote>
  <p>but my corpus is way too large for the only solution there to be
  practical.</p>
</blockquote>

<p><strong>Topic Modelling!</strong></p>

<p>Document classification is a really hot topic at the moment in our research group and other NLP groups. Our primary focus is probabilistic topic modelling. Topic models are an array of algorithms with the aim is to discover the hidden thematic structure in large archives of documents for classification. What is exciting is that there is a lot of room for innovation, invention and just general improvements. Plenty of stuff to work on such as ensembles, hybrids and other statistical techniques.</p>

<p>The <em>Stanford Natural Language Processing Group</em> has a free open source tool for prototyping topic models called the <a href=""http://nlp.stanford.edu/software/tmt/tmt-0.4/"" rel=""nofollow"">Stanford Topic Modelling Toolbox</a>. I suggest you check it out.</p>

<p><strong>A starting point (Maybe?)</strong></p>

<ul>
<li><a href=""https://www.cs.princeton.edu/~blei/papers/Blei2011.pdf"" rel=""nofollow"">Introduction to Probabilistic Topic Models</a></li>
<li><a href=""http://thesai.org/Downloads/Volume6No1/Paper_21-A_Survey_of_Topic_Modeling_in_Text_Mining.pdf"" rel=""nofollow"">A survey of Topic Modelling in Text Mining</a></li>
<li><a href=""http://dl.acm.org/citation.cfm?id=944937"" rel=""nofollow"">Latent dirichlet allocation</a></li>
<li><a href=""http://dl.acm.org/citation.cfm?id=1036902"" rel=""nofollow"">The author-topic model for authors and documents</a></li>
<li><a href=""https://www.coursera.org/course/textanalytics"" rel=""nofollow"">Text Mining &amp; Analytics</a> (MOOC Videos: Week 4) </li>
</ul>
",""
"32754997","2015-09-24 06:45:10","0","","24456056","<p>Possibly, something go wrong with how you call the NLTK functions or you're using an old version of NLTK. </p>

<p>I'm getting the same results for NLTK as what you've shown in segeval:</p>

<pre><code>&gt;&gt;&gt; from nltk.metrics.segmentation import pk
&gt;&gt;&gt; hyp = '0100100000'
&gt;&gt;&gt; ref = '0101000000'
&gt;&gt;&gt; pk(hyp, ref, 2)
0.2222222222222222
&gt;&gt;&gt; hyp = '111111'
&gt;&gt;&gt; ref = '100100'
&gt;&gt;&gt; pk(hyp, ref, 2)
0.4
</code></pre>

<p>My nltk version:</p>

<pre><code>&gt;&gt;&gt; nltk.__version__
'3.0.5'
</code></pre>

<p>Do this:</p>

<pre><code>$ pip install -U nltk
</code></pre>
",""
"32749119","2015-09-23 20:43:22","1","","32748859","<p>If you have sentences both ending with ""."" and "". "", you can try regex: </p>

<pre><code>import re

text = ""your text here. i.e. something.""
sentences = re.split(r'(?&lt;!\w\.\w.)(?&lt;![A-Z][a-z]\.)(?&lt;=\.|\?)\s', text)
</code></pre>

<p>source: <a href=""https://stackoverflow.com/questions/25735644/python-regex-for-splitting-text-into-sentences-sentence-tokenizing"">Python - RegEx for splitting text into sentences (sentence-tokenizing)</a></p>
",""
"32414670","2015-09-05 15:19:58","0","","32414333","<p><strong>S-expressions in Common Lisp</strong></p>

<p>In Common Lisp s-expressions characters like <code>,</code>, <code>.</code> and others are a part of the default syntax.</p>

<p>If you want symbols with arbitrary names in Lisp s-expressions, you have to escape them. Either use a backslash to escape single characters or use a pair of vertical bars to escape multiple characters:</p>

<pre><code>CL-USER 2 &gt; (loop for symbol in '(\, \. | a , b , c .|)
                  do (describe symbol))

\, is a SYMBOL
NAME          "",""
VALUE         #&lt;unbound value&gt;
FUNCTION      #&lt;unbound function&gt;
PLIST         NIL
PACKAGE       #&lt;The COMMON-LISP-USER package, 76/256 internal, 0/4 external&gt;

\. is a SYMBOL
NAME          "".""
VALUE         #&lt;unbound value&gt;
FUNCTION      #&lt;unbound function&gt;
PLIST         NIL
PACKAGE       #&lt;The COMMON-LISP-USER package, 76/256 internal, 0/4 external&gt;

| a , b , c .| is a SYMBOL
NAME          "" a , b , c .""
VALUE         #&lt;unbound value&gt;
FUNCTION      #&lt;unbound function&gt;
PLIST         NIL
PACKAGE       #&lt;The COMMON-LISP-USER package, 76/256 internal, 0/4 external&gt;
NIL
</code></pre>

<p><strong>Tokenizing / Parsing</strong></p>

<p>If you want to deal with other input formats and not s-expressions, you might want to tokenize / parse the input yourself.</p>

<p>Primitive example:</p>

<pre><code>CL-USER 11 &gt; (mapcar (lambda (string)
                       (intern string ""CL-USER""))
                     (split-sequence "" "" ""S --&gt; S , CC S .""))
(S --&gt; S \, CC S \.)
</code></pre>
",""
"32399618","2015-09-04 13:41:19","2","","32399299","<pre><code>from itertools import islice

for sub in l:
    for a, b, c in zip(islice(sub, 0, None), islice(sub, 1, None), islice(sub, 2, None)):
        if all((a[-1] == ""JJ"", b[-1] == ""CC"", c[-1] == ""JJ"")):
            print(""{} {} {}"".format(a[0], b[0], c[0]))
</code></pre>

<p>Which outputs <code>sad and unhappy</code>,  it does not include <code>'great and fun'</code> as that does not match the pattern <code>JJ-CC-JJ</code>.</p>

<p>Or just using enumerate and a generator:</p>

<pre><code>l = [[('reviewtext', 'IN'), ('this', 'DT'), ('movie', 'NN'), ('was', 'VBD'), ('great', 'JJ'), ('and', 'CC'),
      ('fun', 'NN'), ('i', 'PRP'), ('really', 'RB'), ('enjoyed', 'VBD'), ('this', 'DT'), ('awesome', 'NN'),
      ('movie', 'NN')],
     [('reviewtext', 'IN'), ('it', 'PRP'), ('was', 'VBD'), ('fun', 'VBN'), ('but', 'CC'), ('long', 'RB')],
     [('reviewtext', 'IN'), ('i', 'PRP'), ('loved', 'VBD'), ('the', 'DT'), ('new', 'JJ'), ('movie', 'NN'), ('my', 'PRP$'), ('brother', 'NN'), ('got', 'VBD'), ('sad', 'JJ'), ('and', 'CC'), ('unhappy', 'JJ'), ('at', 'IN'), ('the', 'DT'), ('end', 'NN')]]

def match(l,p1,p2,p3):
    for sub in l:
        # avoid index error and catch last three elements
        end = len(sub) - 1
        for ind, (a, b) in enumerate(sub, 1):
            if ind == end:
                break
            if b == p1 and sub[ind][1] == p2 and sub[ind + 1][1] == p3:
                yield (""{} {} {}"".format(a, sub[ind][0], sub[ind + 1][0]))

print(list(match(l,""JJ"",""CC"",""JJ"")))        
</code></pre>

<p>Output (based on example):</p>

<pre><code>['sad and unhappy']
</code></pre>
",""
"32397158","2015-09-04 11:37:39","0","","32333312","<pre><code>def extract_chunks(tagged_sent, chunk_type):
    grp1, grp2, chunk_type = [], [], ""-"" + chunk_type
    for ind, (s, tp) in enumerate(tagged_sent):
        if tp.endswith(chunk_type):
            if not tp.startswith(""B""):
                grp2.append(str(ind))
                grp1.append(s)
            else:
                if grp1:
                    yield "" "".join(grp1), ""-"".join(grp2)
                grp1, grp2 = [s], [str(ind)]
    yield "" "".join(grp1), ""-"".join(grp2)
</code></pre>

<p>Output:</p>

<pre><code>In [2]: l = [('The', 'B-NP'), ('Mitsubishi', 'I-NP'), ('Electric', 'I-NP'), ('Company', 'I-NP'), ('Managing', 'B-NP'),
   ...:                ('Director', 'I-NP'), ('ate', 'B-VP'), ('ramen', 'B-NP')]

In [3]: list(extract_chunks(l, ""NP""))
Out[3]: 
[('The Mitsubishi Electric Company', '0-1-2-3'),
 ('Managing Director', '4-5'),
 ('ramen', '7')]

In [4]: l = [('What', 'B-NP'), ('is', 'B-VP'), ('the', 'B-NP'), ('airspeed', 'I-NP'), ('of', 'B-PP'), ('an', 'B-NP'), ('unladen', 'I-NP'), ('swallow', 'I-NP'), ('?', 'O')]

In [5]: list(extract_chunks(l, ""NP""))
Out[5]: [('What', '0'), ('the airspeed', '2-3'), ('an unladen swallow', '5-6-7')]
</code></pre>
",""
"32396587","2015-09-04 11:06:49","0","","32390030","<p>The question is really, how do I enumerate the possible inputs?  This is one of those rare situations where really ""the only real limit is your imagination"".  But a simple approach is to just catalog the possible permutations, and assign a number to each.</p>

<p>With a riduculously small lexicon, you could assign a number to each possible permutation of the words in the lexicon.  If your vocabulary is {'bag', 'of', 'words'} you could assign the numbers</p>

<pre><code>1 bag of words
2 bag words of
3 of bag words
4 of words bag
5 words bag of
6 words of bag
</code></pre>

<p>and perhaps a few more to cater for an empty slot.  But for a large dictionary, clearly this isn't feasible.  Perhaps if you instead assign a number identifier to each word:</p>

<pre><code>1 bag
2 of
3 words
</code></pre>

<p>then you can do something like <em>(1 * 100) + (2 * 10) + (1 * 3)</em> to obtain the number 123 for the permutation ""bag of words"".  Or if you want to emphasize the context, maybe assign binary features, and apply a multiplier to the central word:</p>

<pre><code>001 bag
010 of
100 words
</code></pre>

<p>would obtain <em>(1 * 001) + (1000 * 010) + (1 * 100)</em> = 010101 = 21 for the head word 'of' surrounded by the leading context 'bag' and the trailing context 'words'.</p>

<p>What makes sense depends on your application.  It's easy to come up with niche applications where it might make sense to count the number of occurrences of the letter <em>b</em> or whatever, and simply directly use the metric you are interested in as the identifier.</p>
",""
"32264508","2015-08-28 06:19:22","0","","32264398","<p>Try splitting the string into each word, and check what type of word it is:</p>

<pre><code>string = 'anarchism/NOUN originated/VERB as/ADP a/DET term/NOUN of/ADP abuse/NOUN first/ADV used/VERB against/ADP early/ADJ working/NOUN class/NOUN radicals/NOUN'
string = string.split(' ')
temp = ''
for a in string:
    if '/NOUN' in a:
        temp += a + ' '
    else:
        temp += 'NIL '
string = temp
print(string)
</code></pre>
",""
"32263235","2015-08-28 04:13:05","1","","32253125","<p>The intuitive thing to do would be exactly what you tried: multiply each <em>tf</em> value by the number of words in the sentence you're examining. However, I think the key observation here is that each row has been normalized by its euclidean length. So multiplying each row by the number of words in that sentence is at best <em>approximating</em> the denormalized row, which is why you get weird values. AFAIK, you can't denormalize the tf*idf matrix without knowing the norms of each of the original rows ahead of time. This is primarily because there are an infinite number of vectors that can be mapped to any one normalized vector. So without the norms, you can't retrieve the correct magnitude of the original vector. <a href=""https://stackoverflow.com/questions/26912479/denormalize-vector"">See this answer</a> for more details about what I mean. </p>

<p>That being said, I think there's a workaround in our case. We can at least retrieve the normalized ratios of the term counts in each sentence, i.e., sun appears twice as much as shiny. I found that normalizing each row so that the sum of the tf values is 1 and <em>then</em> multiplying those values by the length of the <em>stopword-filtered</em> sentences seems to retrieve the original word counts.</p>

<p>To demonstrate:</p>

<pre><code>sentences = (""The sun is shiny i like the sun"",""I have been exposed to sun"")
vect = TfidfVectorizer(stop_words=""english"",lowercase=False)
mat = vect.fit_transform(sentences).toarray()
q = mat / vect.idf_
sums = np.ones((q.shape[0], 1))
lens = np.ones((q.shape[0], 1))
for ix in xrange(q.shape[0]):
    sums[ix] = np.sum(q[ix,:])
    lens[ix] = len([x for x in sentences[ix].split() if unicode(x) in vect.get_feature_names()]) #have to filter out stopwords
sum_to_1 = q / sums
tf = sum_to_1 * lens
print tf
</code></pre>

<p>yields:</p>

<pre><code>[[ 1.  0.  1.  1.  2.]
 [ 0.  1.  0.  0.  1.]]
</code></pre>

<p>I tried this with a few more complicated sentences and it seems to work alright. Let me know if I missed anything.</p>
",""
"32225775","2015-08-26 11:44:51","3","","32224227","<p>There are other third-party models that you can load in <code>NLTK</code>. Take a look at <a href=""https://stackoverflow.com/questions/30821188/python-nltk-pos-tag-not-returning-the-correct-part-of-speech-tag"">Python NLTK pos_tag not returning the correct part-of-speech tag</a></p>

<hr>

<p>To answer the question with some hacks, you can trick the POS tagger by adding a pronoun so that the verb gets a subject, e.g.</p>

<pre><code>&gt;&gt;&gt; from nltk import pos_tag
&gt;&gt;&gt; sent1 = 'get me now'.split()
&gt;&gt;&gt; sent2 = 'run fast'.split()
&gt;&gt;&gt; pos_tag(['He'] + sent1)
[('He', 'PRP'), ('get', 'VBD'), ('me', 'PRP'), ('now', 'RB')]
&gt;&gt;&gt; pos_tag(['He'] + sent1)[1:]
[('get', 'VBD'), ('me', 'PRP'), ('now', 'RB')]
</code></pre>

<p>To functionalize the answer:</p>

<pre><code>&gt;&gt;&gt; from nltk import pos_tag
&gt;&gt;&gt; sent1 = 'get me now'.split()
&gt;&gt;&gt; sent2 = 'run fast'.split()
&gt;&gt;&gt; def imperative_pos_tag(sent):
...     return pos_tag(['He']+sent)[1:]
... 
&gt;&gt;&gt; imperative_pos_tag(sent1)
[('get', 'VBD'), ('me', 'PRP'), ('now', 'RB')]
&gt;&gt;&gt; imperative_pos_tag(sent2)
[('run', 'VBP'), ('fast', 'RB')]
</code></pre>

<hr>

<p>If you want all verbs in your imperative to receive base form VB tag:</p>

<pre><code>&gt;&gt;&gt; from nltk import pos_tag
&gt;&gt;&gt; sent1 = 'get me now'.split()
&gt;&gt;&gt; sent2 = 'run fast'.split()
&gt;&gt;&gt; def imperative_pos_tag(sent):
...     return [(word, tag[:2]) if tag.startswith('VB') else (word,tag) for word, tag in pos_tag(['He']+sent)[1:]]
... 
&gt;&gt;&gt; imperative_pos_tag(sent1)
[('get', 'VB'), ('me', 'PRP'), ('now', 'RB')]
&gt;&gt;&gt; imperative_pos_tag(sent2)
[('run', 'VB'), ('fast', 'RB')]
</code></pre>
",""
"32106813","2015-08-19 22:42:12","0","","32106090","<p>Tag <code>tag()</code> function expects a list of tokens as input.
Since you give it a string as input, this string gets interpreted as a list.
Turning a string into a list gives you a list of characters:</p>

<pre><code>&gt;&gt;&gt; list(""abc"")
['a', 'b', 'c']
</code></pre>

<p>All you need to do is turn your string into a list of tokens before tagging. For example with <a href=""http://www.nltk.org/api/nltk.tokenize.html"" rel=""nofollow"">nltk</a> or simply by splitting at whitespaces:</p>

<pre><code>&gt;&gt;&gt; import nltk
&gt;&gt;&gt; nltk.word_tokenize(""Hi I am Harry Potter."")
['Hi', 'I', 'am', 'Harry', 'Potter', '.']
&gt;&gt;&gt; ""Hi I am Harry Potter."".split(' ')
['Hi', 'I', 'am', 'Harry', 'Potter.']
</code></pre>

<p>Adding tokenization in the tagging gives the following result:</p>

<pre><code>print(tagger.tag(nltk.word_tokenize(""Hi I am Harry Potter."")))
[('Hi', 'NN'), ('I', 'PPSS'), ('am', 'VB'), ('Harry', 'NN'), ('Potter', 'NN'), ('.', '.')]
</code></pre>
",""
"31867703","2015-08-07 00:08:15","0","","31823347","<p>I don't see anything in the utils.  Here is some sample code to help:</p>

<pre><code>import java.io.*;
import java.util.*;
import edu.stanford.nlp.io.*;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.trees.*;
import edu.stanford.nlp.semgraph.*;
import edu.stanford.nlp.trees.TreeCoreAnnotations.*; 
import edu.stanford.nlp.util.*;


public class NGramPositionExample {


    public static List&lt;List&lt;String&gt;&gt; getNGramsPositions(List&lt;String&gt; items, int minSize, int maxSize) {
        List&lt;List&lt;String&gt;&gt; ngrams = new ArrayList&lt;List&lt;String&gt;&gt;();
    int listSize = items.size();
    for (int i = 0; i &lt; listSize; ++i) {
        for (int ngramSize = minSize; ngramSize &lt;= maxSize; ++ngramSize) {
        if (i + ngramSize &lt;= listSize) {
            List&lt;String&gt; ngram = new ArrayList&lt;String&gt;();
            for (int j = i; j &lt; i + ngramSize; ++j) {
            ngram.add(items.get(j));
            }
                    ngram.add(Integer.toString(i));
            ngrams.add(ngram);
        }
        }
    }
    return ngrams;
    }


        public static void main (String[] args) throws IOException {
            String testString = ""I have the best car"";
            List&lt;String&gt; tokens = Arrays.asList(testString.split("" ""));
            List&lt;List&lt;String&gt;&gt; ngramsAndPositions = getNGramsPositions(tokens,1,2);
            for (List&lt;String&gt; np : ngramsAndPositions) {
                System.out.println(Arrays.toString(np.toArray()));
            }
        }
}
</code></pre>

<p>You can just cut and paste that utility method.</p>

<p>This might be a useful functionality to add, so I will put this on our list of things to work on.</p>
",""
"31867346","2015-08-06 23:21:47","3","","31844602","<p>One way Stanford CoreNLP could help you is its TokensRegex functionality.  With this tool you can write explicit patterns and then tag them in your input text.  Then your code can react based on the presence of certain patterns.</p>

<p>Here are some links with more info:</p>

<p><a href=""http://nlp.stanford.edu/software/tokensregex.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/tokensregex.shtml</a></p>

<p><a href=""http://nlp.stanford.edu/software/regexner/"" rel=""nofollow"">http://nlp.stanford.edu/software/regexner/</a></p>

<p>I would recommend identifying common expressions that you want to handle that deserve a clear response, and build up so you get decent coverage of what users input.</p>

<p>For instance:</p>

<pre><code>Let us (start|begin).
(Start|begin) the (program|software)
I'm ready to (start|begin)
etc...
</code></pre>

<p>Obviously you could combine these rules and make them increasingly complicated.  But I think a straight forward approach would be to think of the various ways one might express they want to begin and then capture that with rules.</p>
",""
"31849259","2015-08-06 07:20:42","0","","31848056","<p>You should read the definition again, it says :</p>

<blockquote>
  <p>R1 is the region after the first non-vowel <strong><em>following</em></strong> a vowel.</p>
</blockquote>

<p>Not: <strong><em>followed by</em></strong> a vowel.</p>

<p>In <code>sprinkled</code>, the first non-vowel following a vowel is <code>n</code>, so the region after is <code>kled</code>.</p>

<p>The same for <code>eucharist</code>, the first non-vowel following a vowel is <code>c</code>,  so the region after is <code>harist</code>.</p>
",""
"31843581","2015-08-05 22:01:10","1","","31843524","<p>It's a little tricky but maybe this will work:</p>

<ol>
<li>Translate the sentence and the ambiguous word</li>
<li>Use lesk on the English version of the sentence</li>
</ol>

<p>Try:</p>

<pre><code>alvas@ubi:~$ wget -O translate.sh http://pastebin.com/raw.php?i=aHgFzmMU
--2015-08-05 23:32:46--  http://pastebin.com/raw.php?i=aHgFzmMU
Resolving pastebin.com (pastebin.com)... 190.93.241.15, 190.93.240.15, 141.101.112.16, ...
Connecting to pastebin.com (pastebin.com)|190.93.241.15|:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: unspecified [text/plain]
Saving to: ‚Äòtranslate.sh‚Äô

    [ &lt;=&gt;                                                                                                                            ] 212         --.-K/s   in 0s      

2015-08-05 23:32:47 (9.99 MB/s) - ‚Äòtranslate.sh‚Äô saved [212]

alvas@ubi:~$ python
Python 2.7.6 (default, Jun 22 2015, 17:58:13) 
[GCC 4.8.2] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; import os
&gt;&gt;&gt; import nltk
&gt;&gt;&gt; from nltk.corpus import wordnet as wn
&gt;&gt;&gt; text = 'ŸÑÿØŸäŸá ŸäŸàÿØÿπ ÿßŸÑŸÖÿßŸÑ ŸÅŸä ÿßŸÑÿ®ŸÜŸÉ'
&gt;&gt;&gt; cmd = 'echo ""{}"" | bash translate.sh'.format(text)
&gt;&gt;&gt; translation = os.popen(cmd).read()
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   193    0    40  100   153     21     83  0:00:01  0:00:01 --:--:--    83
&gt;&gt;&gt; translation
'He has deposited the money in the bank. '
&gt;&gt;&gt; ambiguous = u'ÿ£ŸéŸàŸíÿØŸéÿπŸé'
&gt;&gt;&gt; wn.synsets(ambiguous, lang='arb')
[Synset('entrust.v.02'), Synset('deposit.v.02'), Synset('commit.v.03'), Synset('entrust.v.01'), Synset('consign.v.02')]
&gt;&gt;&gt; nltk.wsd.lesk(translation_stems, '', synsets=wn.synsets(ambiguous,lang='arb'))
Synset('entrust.v.02')
</code></pre>

<p>But as you can see, there are many limitations:</p>

<ul>
<li>Access to an MT system is not always easy  (The above bash script using IBM API that will not last forever, it came from <a href=""https://github.com/Rich-Edwards/fsharpwatson/blob/master/Command%20Line%20CURL%20Scripts"" rel=""nofollow"">https://github.com/Rich-Edwards/fsharpwatson/blob/master/Command%20Line%20CURL%20Scripts</a>)</li>
<li>Machine Translation will never be 100% accurate</li>
<li>Looking for the correct lemma in the Open Multilingual WordNet is not as easy as shown in the example, there's inflection and other morphemic variants to a stem.</li>
<li>WordNet is never complete, especially when it's the not English.</li>
<li>WSD is not 100% as human expected (Even between humans we vary our ""senses"", in the example above, some might say the WSD is right, some say it's better to use <code>Synset('deposit.v.02')</code>)</li>
</ul>
",""
"31728189","2015-07-30 15:38:01","1","","31723623","<p>The short answer is that a standard NLP library or toolkit is unlikely to solve this problem. Like Stanford NLP, most libraries will only provide a mapping from <code>word --&gt; lemma</code>. Note that this is a many-to-one function, i.e., the inverse function is not well-defined in a word space. It is, however, a well defined function from the space of words to the space of sets of words (i.e., it's a one-to-many mapping in word-space).</p>

<p>Without some form of explicit mapping being maintained, it is impossible to <em>generate</em> all the variants from a given lemma. This is a theoretical impossibility because lemmatization is a lossy, one-way function.</p>

<p>You can, however, generate a mapping of <code>lemma --&gt; set-of-words</code> without much coding (and definitely without coding a new algorithm):</p>

<pre><code>// Java
Map&lt;String, Set&lt;String&gt;&gt; inverseLemmaMap = new HashMap&lt;&gt;();

// Guava
Multimap&lt;String, String&gt; inverseLemmaMap = HashMultimap.create();
</code></pre>

<p>Then, as you annotate your corpus using Stanford NLP, you can obtain the lemma and its corresponding token, and populate the above map (or multimap). This way, after a single pass over your dataset, you will have the required inverse lemmatization.</p>

<p>Note that this will be restricted to the corpus/dataset you are using, and not all words in the English language will be included.</p>

<p>Another note is that people often think that an inflection is uniquely determined by the part of speech. This is incorrect:</p>

<pre><code>String s = ""My running was beginning to hurt me. I was running all day.""
</code></pre>

<p>The first instance of <code>running</code> is tagged <code>NN</code>, while the second instance is the present continuous tense of the verb, tagged <code>VBG</code>. This is what I meant by ""lossy, one-way function"" earlier in my answer.</p>
",""
"31727984","2015-07-30 15:28:41","0","","31689621","<p>Maybe I'm overlooking things, but is this what you're after?</p>

<pre><code>import nltk
s = '(ROOT (S (NP (NNP Europe)) (VP (VBZ is) (PP (IN in) (NP (DT the) (JJ same) (NNS trends)))) (. .)))'
tree = nltk.tree.Tree.fromstring(s)
def traverse_tree(tree):
    # print(""tree:"", tree)
    for subtree in tree:
        if type(subtree) == nltk.tree.Tree:
            traverse_tree(subtree)
traverse_tree(tree)
</code></pre>

<p>It traverses your tree depth-first.</p>
",""
"31696079","2015-07-29 09:12:26","0","","31684393","<p>The documentation for the rules is <a href=""http://www.nltk.org/api/nltk.tbl.html#nltk.tbl.rule.Rule"" rel=""noreferrer"">here.</a>
<code>016</code> would be the the <code>templateid</code>, i.e. the template that was used to create the rule.
You can also get a description for the rule:</p>

<pre><code>q = Rule('016', 'CS', 'QL', [(Word([1, 2, 3]),'as')])
q.format('verbose')
'CS -&gt; QL if the Word of words i+1...i+3 is ""as""'
</code></pre>

<p>In this case it is actually the words that come <em>after</em> the target word. (Indicated by <code>i+1...</code>)</p>
",""
"31421056","2015-07-15 03:34:57","6","","31400985","<p>You can use the <code>-tokenizerFactory</code> and <code>-tokenizerOptions</code> flags to control tokenization.  The ""Tagging and Testing from the command line"" section of the <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/tagger/maxent/MaxentTagger.html"" rel=""nofollow"">javadoc for MaxentTagger</a> has a complete list of available options.</p>

<p>I believe the following command will do what you want:</p>

<pre><code>java -mx10000m -cp 'stanford-postagger.jar:' \
  edu.stanford.nlp.tagger.maxent.MaxentTagger \
  -model models/french.tagger \
  -tokenizerFactory 'edu.stanford.nlp.international.french.process.FrenchTokenizer$FrenchTokenizerFactory' \
  -sentenceDelimiter newline
</code></pre>
",""
"31393224","2015-07-13 20:51:38","2","","31349851","<p>There seems to be some problems with the tags in NLTK brown corpus that tags <code>NNPS</code> as <code>NPS</code> (Possibly the NLTK tagset is an updated/outdated tags that is different from <a href=""https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"" rel=""nofollow"">https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html</a>)</p>

<p>Here's an example of <code>plural proper nouns</code>:</p>

<pre><code>&gt;&gt;&gt; from nltk.corpus import brown
&gt;&gt;&gt; for sent in brown.tagged_sents():
...     if any(pos for word, pos in sent if pos == 'NPS'):
...             print sent
...             break
... 
[(u'Georgia', u'NP'), (u'Republicans', u'NPS'), (u'are', u'BER'), (u'getting', u'VBG'), (u'strong', u'JJ'), (u'encouragement', u'NN'), (u'to', u'TO'), (u'enter', u'VB'), (u'a', u'AT'), (u'candidate', u'NN'), (u'in', u'IN'), (u'the', u'AT'), (u'1962', u'CD'), (u""governor's"", u'NN$'), (u'race', u'NN'), (u',', u','), (u'a', u'AT'), (u'top', u'JJS'), (u'official', u'NN'), (u'said', u'VBD'), (u'Wednesday', u'NR'), (u'.', u'.')]
</code></pre>

<p>But if you tag with <code>nltk.pos_tag</code>, you'll get <code>NNPS</code>:</p>

<pre><code>&gt;&gt;&gt; for sent in brown.tagged_sents():
...     if any(pos for word, pos in sent if pos == 'NPS'):
...             print "" "".join([word for word, pos in sent])
...             break
... 
Georgia Republicans are getting strong encouragement to enter a candidate in the 1962 governor's race , a top official said Wednesday .
&gt;&gt;&gt; from nltk import pos_tag
&gt;&gt;&gt; pos_tag(""Georgia Republicans are getting strong encouragement to enter a candidate in the 1962 governor's race , a top official said Wednesday ."".split())
[('Georgia', 'NNP'), ('Republicans', 'NNPS'), ('are', 'VBP'), ('getting', 'VBG'), ('strong', 'JJ'), ('encouragement', 'NN'), ('to', 'TO'), ('enter', 'VB'), ('a', 'DT'), ('candidate', 'NN'), ('in', 'IN'), ('the', 'DT'), ('1962', 'CD'), (""governor's"", 'NNS'), ('race', 'NN'), (',', ','), ('a', 'DT'), ('top', 'JJ'), ('official', 'NN'), ('said', 'VBD'), ('Wednesday', 'NNP'), ('.', '.')]
</code></pre>
",""
"31332326","2015-07-10 04:13:48","2","","31327126","<p><code>BasicStats</code> on it's own won't do much for you.  About all it does is hold values for you, it doesn't have any of the intelligence to <em>acquire</em> that information.</p>

<p><code>BasicStats</code> is intended to be used by the <code>Similarity</code> implementation, which generates all the information to put into it.  The methods it uses to do this in the <code>SimilarityBase</code> are protected, but we can make use of the code there.  To populate the <code>BasicStats</code>, you'll also need a <code>CollectionStatistics</code> and a <code>TermStatistics</code>, but really all you'll need to get those is the <code>Term</code> you are interested in, and an <code>IndexReader</code>:</p>

<pre><code>public static BasicStats getBasicStats(IndexReader indexReader, Term myTerm, float queryBoost) throws IOException {
    String fieldName = myTerm.field();

    CollectionStatistics collectionStats = new CollectionStatistics(
            ""field"",
            indexReader.maxDoc(),
            indexReader.getDocCount(fieldName),
            indexReader.getSumTotalTermFreq(fieldName),
            indexReader.getSumDocFreq(fieldName)
            );

    TermStatistics termStats = new TermStatistics(
            myTerm.bytes(),
            indexReader.docFreq(myTerm),
            indexReader.totalTermFreq(myTerm)
            );

    BasicStats myStats = new BasicStats(fieldName, queryBoost);
    assert collectionStats.sumTotalTermFreq() == -1 || collectionStats.sumTotalTermFreq() &gt;= termStats.totalTermFreq();
    long numberOfDocuments = collectionStats.maxDoc();

    long docFreq = termStats.docFreq();
    long totalTermFreq = termStats.totalTermFreq();

    if (totalTermFreq == -1) {
      totalTermFreq = docFreq;
    }

    final long numberOfFieldTokens;
    final float avgFieldLength;

    long sumTotalTermFreq = collectionStats.sumTotalTermFreq();

    if (sumTotalTermFreq &lt;= 0) {
        numberOfFieldTokens = docFreq;
        avgFieldLength = 1;
    } else {
        numberOfFieldTokens = sumTotalTermFreq;
        avgFieldLength = (float)numberOfFieldTokens / numberOfDocuments;
    }

    myStats.setNumberOfDocuments(numberOfDocuments);
    myStats.setNumberOfFieldTokens(numberOfFieldTokens);
    myStats.setAvgFieldLength(avgFieldLength);
    myStats.setDocFreq(docFreq);
    myStats.setTotalTermFreq(totalTermFreq);

    return myStats;
}
</code></pre>

<p>If all you are after is one or two specific figures (that is, a call or two to <code>IndexReader</code>), this is probably overkill, but there it is.</p>
",""
"31233978","2015-07-05 19:07:07","1","","31223082","<pre><code>&gt;&gt;&gt; from nltk.corpus import wordnet as wn
&gt;&gt;&gt; ss = wn.synset('dog.n.01')
&gt;&gt;&gt; offset = str(ss.offset()).zfill(8) + '-' + ss.pos()
&gt;&gt;&gt; offset
u'02084071-n'
</code></pre>
",""
"31221130","2015-07-04 13:25:14","0","","31220234","<p>How about changing</p>

<pre><code>    print tagged
</code></pre>

<p>to</p>

<pre><code>    print [(word, tag) for word, tag in tagged if tag in ('NN', 'VB')]
</code></pre>
",""
"31075605","2015-06-26 14:20:05","0","","31057029","<p>It looks like the <a href=""http://snowball.tartarus.org/texts/earlyenglish.html"" rel=""nofollow"">rule changes are minimal</a> for that.</p>

<p>So, it might be possible to copy/modify the <a href=""https://github.com/apache/lucene-solr/blob/lucene_solr_4_10_1/lucene/analysis/common/src/java/org/apache/lucene/analysis/en/PorterStemmer.java#L269"" rel=""nofollow"">PorterStemmer</a> class and related Factories/Filters.</p>

<p>Or it might be possible to add those specific rules as Regular expression filter before Porter.</p>
",""
"31023937","2015-06-24 10:17:48","2","","31023099","<p>NLTK has a <code>tree</code> module. You can use it to parse the representation you get out of Stanford (see <a href=""https://stackoverflow.com/questions/28674417/how-to-read-constituency-based-parse-tree/28674667#28674667"">this related question</a>). Then you can use <code>nltk.tree.draw</code> to display it.</p>
",""
"30997328","2015-06-23 08:14:03","4","","30995232","<p>There might be more elegant ways to obtain the result, but this one should work:</p>

<pre><code>q &lt;- strsplit(unlist(tagged_str[1]),'/NN')
q &lt;- tail(strsplit(unlist(q[1]),"" "")[[1]],1)
#&gt; q
#[1] ""sentence""
</code></pre>

<p>Hope this helps.</p>
",""
"30888733","2015-06-17 10:20:10","1","","30816692","<p>I wrote this for python. I believe you can read it as a pseudocode. <strike>I will edit the post for Java later</strike>. I added the Java implementation later.</p>

<pre class=""lang-py prettyprint-override""><code>import re

# grammar repository 
grammar_repo = []

s = ""( S ( NP-SBJ ( PRP I  )  )  ( INODE@S ( VP ( VBP have  )  ( NP ( DT a  )  ( INODE@NP ( NN savings  )  ( NN account  )  )  )  )  ( . . )  )  )""
# clean the string (make sure there is no double space in it)
s = s.replace(""   "", "" "").replace(""  "", "" "")

# find inner parenthesis (no-need-to-parse-grammar) or (lhs rhs) format
simple_grammars = re.findall(""\([^\(\)]*\)"", s)
# repeat until you cannot find any ( lhs rhs ) grammar
while len(simple_grammars) &gt; 0:
    # add all simple grammar into grammar repository
    # replace them with their head
    for simple_grammar in simple_grammars:
        grammar = simple_grammar.split("" "")
        # '(' == grammar[0] and ')' == grammar[-1]
        lhs = grammar[1]
        rhs = grammar[2:-1]
        grammar_repo.append((lhs, rhs))

        s = s.replace(simple_grammar, lhs)

    simple_grammars = re.findall(""\([^\(\)]*\)"", s)
</code></pre>

<p>In short, start from simplest grammar you can find and replace them with their left-hand-side and continue. e.g. find <code>(PRP I)</code> save it, then replace it with <code>PRP</code>. repeat until you find all the syntax.</p>

<p><strong>Update:</strong>
The Java implementation is a bit different but it's the same idea. The full code is here: <a href=""http://ideone.com/0eE8bd"" rel=""nofollow"">http://ideone.com/0eE8bd</a></p>

<pre class=""lang-java prettyprint-override""><code>PrintStream ps = new PrintStream(System.out);
ArrayList grammarRepo = new ArrayList();
String item, grammar;
String str = ""( S ( NP-SBJ ( PRP I  )  )  ( INODE@S ( VP ( VBP have  )  ( NP ( DT a  )  ( INODE@NP ( NN savings  )  ( NN account  )  )  )  )  ( . . )  )  )"";
// cleanup double spaces
while (str.contains(""  "")){
    str = str.replaceAll(""  "", "" "");
}
// find no parenthesis zones!
Matcher m = Pattern.compile(""\\([^\\(\\)]*\\)"").matcher(str);

// loop until nothing left:
while (m.find()) {
    item = m.group();
    // make the grammar:
    grammar = item.substring(1, item.length()-1).trim().replaceFirst("" "", "" -&gt; "");

    if (!grammarRepo.contains(grammar)) {
        grammarRepo.add(grammar);
        ps.print(grammar + ""\n"");
    }

    str = str.replace(item, grammar.split("" -&gt; "")[0]);
    m = Pattern.compile(""\\([^\\(\\)]*\\)"").matcher(str);
}
</code></pre>

<p>the output:</p>

<pre><code>PRP -&gt; I
NP-SBJ -&gt; PRP
VBP -&gt; have
DT -&gt; a
NN -&gt; savings
NN -&gt; account
INODE@NP -&gt; NN NN
NP -&gt; DT INODE@NP
VP -&gt; VBP NP
. -&gt; .
INODE@S -&gt; VP .
S -&gt; NP-SBJ INODE@S
</code></pre>
",""
"30823202","2015-06-13 20:24:08","8","","30821188","<p><strong>In short</strong>:</p>

<blockquote>
  <p>NLTK is not perfect. In fact, no model is perfect.</p>
</blockquote>

<p><strong>Note:</strong></p>

<p>As of NLTK version 3.1, default <code>pos_tag</code> function is no longer the <a href=""https://stackoverflow.com/questions/31386224/what-created-maxent-treebank-pos-tagger-english-pickle"">old MaxEnt English pickle</a>. </p>

<p>It is now the <strong>perceptron tagger</strong> from <a href=""https://github.com/nltk/nltk/blob/develop/nltk/tag/perceptron.py"" rel=""noreferrer"">@Honnibal's implementation</a>, see <a href=""https://github.com/nltk/nltk/blob/develop/nltk/tag/__init__.py#L87"" rel=""noreferrer""><code>nltk.tag.pos_tag</code></a></p>

<pre><code>&gt;&gt;&gt; import inspect
&gt;&gt;&gt; print inspect.getsource(pos_tag)
def pos_tag(tokens, tagset=None):
    tagger = PerceptronTagger()
    return _pos_tag(tokens, tagset, tagger) 
</code></pre>

<p>Still it's better but not perfect:</p>

<pre><code>&gt;&gt;&gt; from nltk import pos_tag
&gt;&gt;&gt; pos_tag(""The quick brown fox jumps over the lazy dog"".split())
[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]
</code></pre>

<p>At some point, if someone wants <code>TL;DR</code> solutions, see <a href=""https://github.com/alvations/nltk_cli"" rel=""noreferrer"">https://github.com/alvations/nltk_cli</a></p>

<hr>

<p><strong>In long</strong>:</p>

<p><strong>Try using other tagger (see <a href=""https://github.com/nltk/nltk/tree/develop/nltk/tag"" rel=""noreferrer"">https://github.com/nltk/nltk/tree/develop/nltk/tag</a>) , e.g.</strong>:</p>

<ul>
<li>HunPos</li>
<li>Stanford POS</li>
<li>Senna</li>
</ul>

<p><strong>Using default MaxEnt POS tagger from NLTK, i.e. <code>nltk.pos_tag</code></strong>:</p>

<pre><code>&gt;&gt;&gt; from nltk import word_tokenize, pos_tag
&gt;&gt;&gt; text = ""The quick brown fox jumps over the lazy dog""
&gt;&gt;&gt; pos_tag(word_tokenize(text))
[('The', 'DT'), ('quick', 'NN'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'NNS'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'NN'), ('dog', 'NN')]
</code></pre>

<p><strong>Using Stanford POS tagger</strong>:</p>

<pre><code>$ cd ~
$ wget http://nlp.stanford.edu/software/stanford-postagger-2015-04-20.zip
$ unzip stanford-postagger-2015-04-20.zip
$ mv stanford-postagger-2015-04-20 stanford-postagger
$ python
&gt;&gt;&gt; from os.path import expanduser
&gt;&gt;&gt; home = expanduser(""~"")
&gt;&gt;&gt; from nltk.tag.stanford import POSTagger
&gt;&gt;&gt; _path_to_model = home + '/stanford-postagger/models/english-bidirectional-distsim.tagger'
&gt;&gt;&gt; _path_to_jar = home + '/stanford-postagger/stanford-postagger.jar'
&gt;&gt;&gt; st = POSTagger(path_to_model=_path_to_model, path_to_jar=_path_to_jar)
&gt;&gt;&gt; text = ""The quick brown fox jumps over the lazy dog""
&gt;&gt;&gt; st.tag(text.split())
[(u'The', u'DT'), (u'quick', u'JJ'), (u'brown', u'JJ'), (u'fox', u'NN'), (u'jumps', u'VBZ'), (u'over', u'IN'), (u'the', u'DT'), (u'lazy', u'JJ'), (u'dog', u'NN')]
</code></pre>

<p><strong>Using HunPOS</strong> (NOTE: the default encoding is ISO-8859-1 not UTF8):</p>

<pre><code>$ cd ~
$ wget https://hunpos.googlecode.com/files/hunpos-1.0-linux.tgz
$ tar zxvf hunpos-1.0-linux.tgz
$ wget https://hunpos.googlecode.com/files/en_wsj.model.gz
$ gzip -d en_wsj.model.gz 
$ mv en_wsj.model hunpos-1.0-linux/
$ python
&gt;&gt;&gt; from os.path import expanduser
&gt;&gt;&gt; home = expanduser(""~"")
&gt;&gt;&gt; from nltk.tag.hunpos import HunposTagger
&gt;&gt;&gt; _path_to_bin = home + '/hunpos-1.0-linux/hunpos-tag'
&gt;&gt;&gt; _path_to_model = home + '/hunpos-1.0-linux/en_wsj.model'
&gt;&gt;&gt; ht = HunposTagger(path_to_model=_path_to_model, path_to_bin=_path_to_bin)
&gt;&gt;&gt; text = ""The quick brown fox jumps over the lazy dog""
&gt;&gt;&gt; ht.tag(text.split())
[('The', 'DT'), ('quick', 'JJ'), ('brown', 'JJ'), ('fox', 'NN'), ('jumps', 'NNS'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]
</code></pre>

<p><strong>Using Senna</strong> (Make sure you've the latest version of NLTK, there were some changes made to the API):</p>

<pre><code>$ cd ~
$ wget http://ronan.collobert.com/senna/senna-v3.0.tgz
$ tar zxvf senna-v3.0.tgz
$ python
&gt;&gt;&gt; from os.path import expanduser
&gt;&gt;&gt; home = expanduser(""~"")
&gt;&gt;&gt; from nltk.tag.senna import SennaTagger
&gt;&gt;&gt; st = SennaTagger(home+'/senna')
&gt;&gt;&gt; text = ""The quick brown fox jumps over the lazy dog""
&gt;&gt;&gt; st.tag(text.split())
[('The', u'DT'), ('quick', u'JJ'), ('brown', u'JJ'), ('fox', u'NN'), ('jumps', u'VBZ'), ('over', u'IN'), ('the', u'DT'), ('lazy', u'JJ'), ('dog', u'NN')]
</code></pre>

<hr>

<p><strong>Or try building a better POS tagger</strong>:</p>

<ul>
<li>Ngram Tagger: <a href=""http://streamhacker.com/2008/11/03/part-of-speech-tagging-with-nltk-part-1/"" rel=""noreferrer"">http://streamhacker.com/2008/11/03/part-of-speech-tagging-with-nltk-part-1/</a></li>
<li>Affix/Regex Tagger: <a href=""http://streamhacker.com/2008/11/10/part-of-speech-tagging-with-nltk-part-2/"" rel=""noreferrer"">http://streamhacker.com/2008/11/10/part-of-speech-tagging-with-nltk-part-2/</a> </li>
<li>Build Your Own Brill (Read the code it's a pretty fun tagger, <a href=""http://www.nltk.org/_modules/nltk/tag/brill.html"" rel=""noreferrer"">http://www.nltk.org/_modules/nltk/tag/brill.html</a>), see <a href=""http://streamhacker.com/2008/12/03/part-of-speech-tagging-with-nltk-part-3/"" rel=""noreferrer"">http://streamhacker.com/2008/12/03/part-of-speech-tagging-with-nltk-part-3/</a></li>
<li>Perceptron Tagger: <a href=""https://honnibal.wordpress.com/2013/09/11/a-good-part-of-speechpos-tagger-in-about-200-lines-of-python/"" rel=""noreferrer"">https://honnibal.wordpress.com/2013/09/11/a-good-part-of-speechpos-tagger-in-about-200-lines-of-python/</a></li>
<li>LDA Tagger: <a href=""http://scm.io/blog/hack/2015/02/lda-intentions/"" rel=""noreferrer"">http://scm.io/blog/hack/2015/02/lda-intentions/</a></li>
</ul>

<hr>

<p><strong>Complains about <code>pos_tag</code> accuracy on stackoverflow include</strong>:</p>

<ul>
<li><a href=""https://stackoverflow.com/questions/13529945/pos-tagging-nltk-thinks-noun-is-adjective"">POS tagging - NLTK thinks noun is adjective</a></li>
<li><a href=""https://stackoverflow.com/questions/21786257/python-nltk-pos-tagger-not-behaving-as-expected"">python NLTK POS tagger not behaving as expected</a></li>
<li><a href=""https://stackoverflow.com/questions/8146748/how-to-obtain-better-results-using-nltk-pos-tag"">How to obtain better results using NLTK pos tag</a></li>
<li><a href=""https://stackoverflow.com/questions/8365557/pos-tag-in-nltk-does-not-tag-sentences-correctly"">pos_tag in NLTK does not tag sentences correctly</a></li>
</ul>

<p><strong>Issues about NLTK HunPos include</strong>:</p>

<ul>
<li><a href=""https://stackoverflow.com/questions/5088448/how-to-i-tag-textfiles-with-hunpos-in-nltk"">How do I tag textfiles with hunpos in nltk?</a> </li>
<li><a href=""https://stackoverflow.com/questions/5091389/does-anyone-know-how-to-configure-the-hunpos-wrapper-class-on-nltk"">Does anyone know how to configure the hunpos wrapper class on nltk?</a></li>
</ul>

<p><strong>Issues with NLTK and Stanford POS tagger include</strong>:</p>

<ul>
<li><a href=""https://stackoverflow.com/questions/7344916/trouble-importing-stanford-pos-tagger-into-nltk"">trouble importing stanford pos tagger into nltk</a></li>
<li><a href=""https://stackoverflow.com/questions/27116495/java-command-fails-in-nltk-stanford-pos-tagger"">Java Command Fails in NLTK Stanford POS Tagger</a></li>
<li><a href=""https://stackoverflow.com/questions/22930328/error-using-stanford-pos-tagger-in-nltk-python"">Error using Stanford POS Tagger in NLTK Python</a></li>
<li><a href=""https://stackoverflow.com/questions/23322674/how-to-improve-speed-with-stanford-nlp-tagger-and-nltk"">How to improve speed with Stanford NLP Tagger and NLTK</a></li>
<li><a href=""https://stackoverflow.com/questions/27171298/nltk-stanford-pos-tagger-error-java-command-failed"">Nltk stanford pos tagger error : Java command failed</a></li>
<li><a href=""https://stackoverflow.com/questions/8555312/instantiating-and-using-stanfordtagger-within-nltk"">Instantiating and using StanfordTagger within NLTK</a></li>
<li><a href=""https://stackoverflow.com/questions/26647253/running-stanford-pos-tagger-in-nltk-leads-to-not-a-valid-win32-application-on"">Running Stanford POS tagger in NLTK leads to &quot;not a valid Win32 application&quot; on Windows</a></li>
</ul>
",""
"30748053","2015-06-10 05:48:57","3","","30746460","<p>Classification report must be straightforward - a report of P/R/F-Measure for each element in your test data. In Multiclass problems, it is not a good idea to read Precision/Recall and F-Measure over the whole data any imbalance would make you feel you've reached better results. That's where such reports help.</p>

<p>Coming to confusion matrix, it is much detailed representation of what's going on with your labels. So there were 71 points in the first class (label 0). Out of these, your model was successful in identifying 54 of those correctly in label 0, but 17 were marked as label 4. Similarly look at second row. There were 43 points in class 1, but 36 of them were marked correctly. Your classifier predicted 1 in class 3 and 6 in class 4.</p>

<p><img src=""https://i.sstatic.net/0yLu8.png"" alt=""enter image description here""></p>

<p>Now you can see the pattern this follows. An ideal classifiers with 100% accuracy would produce a pure diagonal matrix which would have all the points predicted in their correct class. </p>

<p>Coming to Recall/Precision. They are some of the mostly used measures in evaluating how good your system works. Now you had 71 points in first class (call it 0 class). Out of them your classifier was able to get 54 elements correctly. That's your recall. 54/71 = 0.76. Now look only at first column in the table. There is one cell with entry 54, rest all are zeros. This means your classifier marked 54 points in class 0, and all 54 of them were actually in class 0. This is precision. 54/54 = 1. Look at column marked 4. In this column, there are elements scattered in all the five rows. 367 of them were marked correctly. Rest all are incorrect. So that reduces your precision.</p>

<p>F Measure is harmonic mean of Precision and Recall. 
Be sure you read details about these. <a href=""https://en.wikipedia.org/wiki/Precision_and_recall"" rel=""noreferrer"">https://en.wikipedia.org/wiki/Precision_and_recall</a></p>
",""
"30739570","2015-06-09 18:05:19","1","","30722624","<p><a href=""http://nlp.stanford.edu/software/parser-faq.shtml#y"" rel=""nofollow"">This FAQ answer</a> explains the difference in a long paragraph. Relevant parts are quoted below:</p>

<blockquote>
  <p><strong>Can you explain the different parsers?</strong></p>
  
  <p>This answer is specific to English. It mostly applies to other languages although some components are missing in some languages. The file <code>englishPCFG.ser.gz</code> comprises just an unlexicalized PCFG grammar. It is basically the parser described in the ACL 2003 Accurate Unlexicalized Parsing paper.</p>
  
  <p>‚Ä¶ The file <code>englishFactored.ser.gz</code> contains two grammars and leads the system to run three parsers. It first runs a (simpler) PCFG parser and then an untyped dependency parser, and then runs a third parser which finds the parse with the best joint score across the two other parsers via a product model. This is described in the NIPS Fast Exact Inference paper.</p>
  
  <p>‚Ä¶ For English, although the grammars and parsing methods differ, the average quality of <code>englishPCFG.ser.gz</code> and <code>englishFactored.ser.gz</code> is similar, and so many people opt for the faster <code>englishPCFG.ser.gz</code>, though <code>englishFactored.ser.gz</code> sometimes does better because it does include lexicalization. For other languages, the factored models are considerably better than the PCFG models, and are what people generally use.</p>
</blockquote>

<p>There are links to the papers referenced on <a href=""http://nlp.stanford.edu/software/lex-parser.shtml#Citing"" rel=""nofollow"">the main parser page</a>.</p>
",""
"30710603","2015-06-08 13:36:07","2","","30676448","<p>First of all, make sure that you have lines in your file then with no worries you can read it line-by-line (<a href=""https://stackoverflow.com/a/12039261/2991872"">discussed here</a>):</p>

<pre><code>with open('my100GBfile.txt') as corpus:
    for line in corpus:
        sequence = preprocess(line)
        extract_n_grams(sequence)
</code></pre>

<p>Let's assume that your corpus doesn't need any special treatment. I guess you can find a suitable treatment for your text, I only want it to be chucked into desirable tokens:</p>

<pre><code>def preprocess(string):
    # do what ever preprocessing that it needs to be done
    # e.g. convert to lowercase: string = string.lower()
    # return the sequence of tokens
    return string.split()
</code></pre>

<p>I don't know what do you want to do with n-grams. Lets assume that you want to count them as a language model which fits in your memory (it usually does, but I'm not sure about 4- and 5-grams). The easy way is to use off the shelf <code>nltk</code> library:</p>

<pre><code>from nltk.util import ngrams

lm = {n:dict() for n in range(1,6)}
def extract_n_grams(sequence):
    for n in range(1,6):
        ngram = ngrams(sentence, n)
        # now you have an n-gram you can do what ever you want
        # yield ngram
        # you can count them for your language model?
        for item in ngram:
            lm[n][item] = lm[n].get(item, 0) + 1
</code></pre>
",""
"30600977","2015-06-02 15:58:51","2","","30460713","<p>As I see in your code samples, you don't call <code>tree()</code> in this line</p>

<pre><code>&gt;&gt;&gt; print(next(next(mp.parse_sents([sent,sent2])))) 
</code></pre>

<p>while you do call <code>tree()</code> in all cases with <code>parse_one()</code>.</p>

<p>Otherwise I don't see the reason why it could happen: <code>parse_one()</code> method of <code>ParserI</code> isn't overridden in <code>MaltParser</code> and everything it does is simply calling <code>parse_sents()</code> of <code>MaltParser</code>, see <a href=""https://github.com/nltk/nltk/blob/develop/nltk/parse/api.py"" rel=""noreferrer"">the code</a>.</p>

<p><strong>Upd:</strong> <a href=""https://github.com/nltk/nltk/blob/develop/nltk/parse/api.py#L45"" rel=""noreferrer"">The line you're talking about</a> isn't called, because <code>parse_sents()</code> is overridden in <code>MaltParser</code> and is directly called. </p>

<p>The only guess I have now is that java lib maltparser doesn't work correctly with input file containing several sentences (I mean <a href=""https://github.com/nltk/nltk/blob/develop/nltk/parse/malt.py#L149"" rel=""noreferrer"">this block</a> - where java is run). Maybe original malt parser has changed the format and now it is not <code>'\n\n'</code>. 
Unfortunately, I can't run this code by myself, because <a href=""http://maltparser.org/"" rel=""noreferrer"">maltparser.org</a> is down for the second day. I checked that the input file has expected format (sentences are separated by double endline), so it is very unlikely that python wrapper merges sentences.</p>
",""
"30526740","2015-05-29 10:03:33","2","","30323409","<p>The Brill part of NLTK has been redesigned in NLTK 3. So all those classes you try to import do not exist any more.<br>
See <a href=""https://github.com/nltk/nltk/pull/549"" rel=""nofollow"">https://github.com/nltk/nltk/pull/549</a></p>

<p>I'm looking for an example how to use the Brill Tagger but till now, I didn't find anything.</p>
",""
"30458575","2015-05-26 12:26:47","5","","30458511","<p>For each suffix in the given list you can check if the given word ends with any of the given suffixes, if yes the remove the suffix, else return the word.</p>

<pre><code>suffixes = ['ing']
def stem(word):
    for suff in suffixes:
        if word.endswith(suff):
            return word[:-len(suff)]

    return word

print(stem ('having'))
&gt;&gt;&gt; hav
</code></pre>
",""
"30414283","2015-05-23 15:13:08","4","","30413885","<p>Yes, the standard PCFG parser (the one that is run by default without any other options specified) will choke on this sort of long nonsense data. You might have better luck using the <a href=""http://nlp.stanford.edu/software/srparser.shtml"" rel=""nofollow"">shift-reduce constituency parser</a>, which is substantially faster than the PCFG and nearly as accurate.</p>
",""
"30284782","2015-05-17 08:15:48","0","","30283217","<p>Read up on unicode string processing in python. There is the type <code>str</code> but there is also a type <code>unicode</code>.</p>

<p>I suggest to:</p>

<ol>
<li><p>decode each line immediately after reading, to narrow down incorrect characters in your input data (real data contains errors)</p></li>
<li><p>work with <code>unicode</code> and <code>u"" ""</code> strings everywhere.</p></li>
</ol>
",""
"30275443","2015-05-16 12:05:39","10","","30275162","<p>That would work as a first approximation.</p>

<p>The problem with fixed word lists for language detection, though, is that real texts (and especially short ones) may not provide enough hits in your list. A more reliable approach would collect parts of other language features (like statistics of letter n-grams that reflect morphology and orthography), not only full words. </p>

<p>Besides, for some text you may get unexpected results anyway. Consider the following phrase:</p>

<blockquote>
  <p>Schwarzenegger in Kindergarten Cop.</p>
</blockquote>

<p>For any reader it would be clear that the language here is English. But what tells you that? It is the ""in"" that makes the phrase unmistakably English. So there are approaches based on short functional words that are assigned much higher weight.</p>

<p>So if you are serious about your project, it would be a good idea to research the area a bit. By the way, why not using one of the existing libraries for language detection? Try <a href=""https://stackoverflow.com/search?q=java%20language%20detection"">this search</a> first. There are also memory considerations (word lists/hash maps can become quite big). But as a quick solution that would work.</p>
",""
"30270898","2015-05-16 01:26:58","0","","30266239","<p>Looks like this has all the info: <a href=""http://www.unixuser.org/~euske/doc/postag/"" rel=""nofollow"">http://www.unixuser.org/~euske/doc/postag/</a>, depending on the output selected for mecab.</p>
",""
"30253046","2015-05-15 06:37:14","1","","30250726","<p>There are a few opportunities for speedup, but my first concern is <em>vector</em>. Where is it initialized? In the code posted, it gets n^2 entries and sorted n times! That seems unintentional. Should it be cleared? Should final be outside the loop?</p>

<p>final=sorted(vector, key=lambda vector: vector[2],reverse = True)</p>

<p>is functional, but has ugly scoping, better is:</p>

<p>final=sorted(vector, key=lambda entry: entry[2], reverse=True)</p>

<p>In general, to solve timing issues consider using a <a href=""https://docs.python.org/2/library/profile.html"" rel=""nofollow"">profiler</a>. </p>
",""
"30249900","2015-05-15 01:01:17","3","","30249815","<p>You can create a set of your stop_words, then for every word in your text see if it is in the set. </p>

<p>Actually it looks like you are already using a set. Though I don't know why you are sorting it. </p>
",""
"30248238","2015-05-14 22:04:44","0","","30219780","<p>Yes, this is possible, but a bit tricky and there is no out of the box feature that can do this, so you will have to write some code. The basic idea is to replace the <code>tokenize</code>, <code>ssplit</code> and <code>pos</code> annotators (and in case you also have trees the <code>parse</code> annotator) with your code that loads these annotations from your annotated files.</p>

<p>On a very high level you have to do the following:</p>

<ul>
<li>Load your trees with <code>MemoryTreebank</code></li>
<li>Loop through all the trees and for each tree create a sentence <code>CoreMap</code> to which you add
<ul>
<li>a <code>TokensAnnotation</code></li>
<li>a <code>TreeAnnotation</code> and the <code>SemanticGraphCoreAnnotations</code></li>
</ul></li>
<li>Create an <code>Annotation</code> object with a list containing the <code>CoreMap</code> objects for all sentences</li>
<li>Run the StanfordCoreNLP pipeline with the <code>annotators</code> option set to <code>lemma,ner,dcoref</code> and the option <code>enforceRequirements</code> set to <code>false</code>.</li>
</ul>

<p>Take a look at the individual annotators to see how to add the required annotations. E.g. there is a method in <a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/pipeline/ParserAnnotatorUtils.java#L29"" rel=""nofollow"">ParserAnnotatorUtils</a> that adds the <code>SemanticGraphCoreAnnotations</code>. </p>
",""
"30211095","2015-05-13 09:42:26","1","","30210494","<p>Lemmatization crucially depends on the part of speech of the token. Only tokens with the same part of speech are mapped to the same lemma. </p>

<p>In the sentence ""This is confusing"", <code>confusing</code> is analyzed as an adjective, and therefore it is lemmatized to <code>confusing</code>. In the sentence ""I was confusing you with someone else"", by contrast, <code>confusing</code> is analyzed as a verb, and is lemmatized to <code>confuse</code>. </p>

<p>If you want tokens with different parts of speech to be mapped to the same lemma, you can use a stemming algorithm such as <a href=""http://tartarus.org/martin/PorterStemmer/java.txt"" rel=""nofollow"">Porter Stemming</a>, which you can simply call on each token.</p>
",""
"30126677","2015-05-08 14:47:38","2","","30004939","<p>There is no subject or VP in the relevant structure. Try writing a rule for <code>and</code> when used to conjoin sentences. What is it? <em>Sentence1 and Sentence2</em> gets the interpretation</p>

<pre><code>&lt;Sentence1&gt; &amp; &lt;Sentence2&gt;
</code></pre>

<p>I.e., take the meanings of the two sentences and conjoin them with <code>&amp;</code>. In the notation of this library, it would be something like this:</p>

<pre><code>S[SEM=&lt;?p &amp; ?q&gt;] -&gt; S[SEM=?p] 'and' S[SEM=?q] 
</code></pre>

<p>I don't use this module and I can't test the example, so you might have to tweak the syntax. But I hope you get the idea.</p>

<p>PS. This kind of semantics usually assumes binary trees, so the interpretation would go in two steps like this:</p>

<pre><code>S1 (and S2)
</code></pre>

<p>This means that you would interpret <code>and</code> as something that takes a sentence (i.e., S2) and gives you a conjunction function, which will combine with another sentence. But since the examples you link to include a three-place expansion (for VP), it doesn't seem to be necessary.</p>

<p>As for further reading (since you ask for ""links""), I would recommend a simple introduction to formal semantics for natural language; e.g., the book by Heim and Kratzer.</p>
",""
"29924010","2015-04-28 15:48:18","3","","29905761","<p>If you are coding in Java, check out getNgrams* functions in the StringUtils class in CoreNLP.</p>

<p>You can also use CollectionUtils.getNgrams (which is what StringUtils class uses too)</p>
",""
"29896275","2015-04-27 12:48:52","0","","29721510","<p>Here's one solution that works for your example, but may not work for other tree structures that you might encounter:</p>

<pre><code>from itertools import groupby
from nltk.tree import Tree

def get_pairs(t, cat):
    pairs = sorted(_get_pairs(t, cat), key=lambda x:x[0])
    for is_none, _adjacents in groupby(pairs, lambda x:x[1] is None):
        if is_none:
            continue
        adjacents = list(_adjacents)
        for (_, p1), (_, p2) in zip(adjacents, adjacents[1:]):
            yield p1, p2

def _get_pairs(t, cat, path=(), idx=(), has_root_cat=False):
    if isinstance(t, str):
        if has_root_cat:
            yield idx, path[:-1] + ((path[-1], t,),)
        else:
            yield idx, None
        return
    for i, ch in enumerate(t):
        found_cat = has_root_cat or t.node == cat
        new_path = path + (t.node,) if found_cat else path
        new_idx = idx + (i,)
        get_pairs_children = _get_pairs(ch, cat, new_path, new_idx, found_cat)
        for pair in get_pairs_children:
            yield pair
</code></pre>

<p>Running</p>

<pre><code>t = Tree.parse(""(S (S (S (S (X (PRO pro))) (X (V v))) (X (ADJ adj))) (X (N n)))"")
print list(get_pairs(t, ""X""))
</code></pre>

<p>gives the output:</p>

<pre><code>[(('X', ('PRO', 'pro')), ('X', ('V', 'v'))),
 (('X', ('V', 'v')), ('X', ('ADJ', 'adj'))),
 (('X', ('ADJ', 'adj')), ('X', ('N', 'n')))]
</code></pre>
",""
"29854869","2015-04-24 18:25:46","2","","29848095","<p>This is a problem that I have worked in past but I did it for different domain. You could start with an online source which gives list of companies and their abbreviations, scrape them and store them in some  format (like hashmap). Now you can use the abbreviations to find a substring match with both original and abbr. word with some threshold (lets say 90%).</p>

<p>Specific to your case you can start scraping this site <a href=""http://www.abbreviations.com/acronyms/FIRMS"" rel=""nofollow"">http://www.abbreviations.com/acronyms/FIRMS</a> using JSOUP. This has a very rich source of company abbreviations. If this list doesnt suffice, you would have to look for some other sources. Hope this helps.</p>
",""
"29807502","2015-04-22 19:52:21","2","","29807175","<p>If you set the property:</p>

<pre><code>tokenize.whitespace = true
</code></pre>

<p>then the CoreNLP pipeline will tokenize on whitespace rather than the default PTB tokenization. You may also want to set:</p>

<pre><code>ssplit.eolonly = true
</code></pre>

<p>so that you only split sentences on newline characters.</p>
",""
"29776406","2015-04-21 15:18:35","2","","29755910","<p>The NERClassifier* is word level, that is, it labels words, not phrases. Given that, the classifier seems to be performing fine. If you want, you can hyphenate words that form phrases. So in your labeled examples and in your test examples, you would make ""Land Cruiser"" to ""Land_Cruiser"".</p>
",""
"29737079","2015-04-19 23:23:50","0","","29733476","<p><code>nltk.wsd.lesk</code> does not return score, it returns the predicted <code>Synset</code>:</p>

<pre><code>&gt;&gt;&gt; from nltk.corpus import wordnet as wn
&gt;&gt;&gt; from nltk.corpus import sentiwordnet as swn
&gt;&gt;&gt; from nltk import word_tokenize
&gt;&gt;&gt; from nltk.wsd import lesk
&gt;&gt;&gt; sent = word_tokenize(""He should be happy"".lower())
&gt;&gt;&gt; lesk(sent, 'be', 'v')
Synset('equal.v.01')
</code></pre>

<p><code>lesk</code> is not perfect, it should only be used as a baseline system for WSD.</p>

<p>Although this is nice:</p>

<pre><code>&gt;&gt;&gt; ss = str(lesk(sent, 'be', 'v'))
&gt;&gt;&gt; re.findall(""'([^']*)'"",ss)
['equal.v.01']
</code></pre>

<p>There's a simpler to get the synset identifier:</p>

<pre><code>&gt;&gt;&gt; lesk(sent, 'be', 'v').name()
u'equal.v.01'
</code></pre>

<p>Then you can do:</p>

<pre><code>&gt;&gt;&gt; swn.senti_synset(lesk(sent, 'be', 'v').name())
SentiSynset('equal.v.01')
</code></pre>

<p>To convert <code>POS tag</code> to <code>WN POS</code>, you can simply try: <a href=""https://stackoverflow.com/questions/24975499/converting-pos-tags-from-textblob-into-wordnet-compatible-inputs"">Converting POS tags from TextBlob into Wordnet compatible inputs</a></p>
",""
"29683827","2015-04-16 19:05:40","0","","29674389","<p>The parser output can be different depending on whether you run it on a part-of-speech tagged sentence or not.</p>

<p>See the <a href=""http://nlp.stanford.edu/software/parser-faq.shtml#corenlpdiff"" rel=""nofollow"">Parser FAQ</a> for more information.</p>
",""
"29680029","2015-04-16 15:56:40","3","","29676112","<p>Please make sure you're using the newest version of Goslate and if not, try updating it from its <a href=""https://bitbucket.org/zhuoqiang/goslate/"" rel=""nofollow"">repository</a>. There were some changes in the API this month, and they are implemented in Goslate already. I've just checked it in my app, it does work flawlessly.<br>
In order to update Goslate, you may need to install <a href=""http://mercurial.selenic.com/downloads"" rel=""nofollow"">Mercurial</a> version controlling system.<br>
After installing Mercurial, clone the repo with this command:</p>

<pre><code>hg clone https://bitbucket.org/zhuoqiang/goslate
</code></pre>

<p>and then update it from time to time like this (you need to be <em>in</em> the repository folder for this to work):</p>

<pre><code>hg pull -u
</code></pre>
",""
"29629542","2015-04-14 14:06:52","0","","28859234","<p><p> You can use <code>tnt</code> tagger for training and then testing with you own data.</p>

<pre><code>word_to_be_tagged = u""‡§™‡•Ç‡§∞‡•ç‡§£ ‡§™‡•ç‡§∞‡§§‡§ø‡§¨‡§Ç‡§ß ‡§π‡§ü‡§æ‡§ì : ‡§á‡§∞‡§æ‡§ï""

word_to_be_tagged_next = u""‡§Æ‡•à‡§Ç ‡§¨‡§π‡•Å‡§§ ‡§π‡•à‡§∞‡§æ‡§® ‡§π‡•Ç‡§Å""

from nltk.corpus import indian

train_data = indian.tagged_sents('hindi.pos')[:300] //used for training 
test_data = indian.tagged_sents('hindi.pos')[301:] //used for testing 

print train_data
[[(u'\u092a\u0942\u0930\u094d\u0923', u'JJ'), (u'\u092a\u094d\u0930\u0924\u093f\u092c\u0902\u0927', u'NN'), (u'\u0939\u091f\u093e\u0913', u'VFM'), (u':', u'SYM'), (u'\u0907\u0930\u093e\u0915', u'NNP')], [(u'\u0938\u0902\u092f\u0941\u0915\u094d\u0924', u'NNC'), (u'\u0930\u093e\u0937\u094d\u091f\u094d\u0930', u'NN'), (u'\u0964', u'SYM')], ...]

print hindi_sents[0][0][0]
‡§™‡•Ç‡§∞‡•ç‡§£

print hindi_sents[0][0][1]
JJ

from nltk.tag import tnt
tnt_pos_tagger = tnt.TnT()
tnt_pos_tagger.train(train_data)
tnt_pos_tagger.evaluate(test_data)
0.6599664991624791

tnt_pos_tagger.tag(nltk.word_tokenize(word_to_be_tagged))
[(u'\u092a\u0942\u0930\u094d\u0923', u'JJ'),
 (u'\u092a\u094d\u0930\u0924\u093f\u092c\u0902\u0927', u'NN'),
 (u'\u0939\u091f\u093e\u0913', u'VFM'),
 (u':', u'SYM'),
 (u'\u0907\u0930\u093e\u0915', u'NNP')]

tnt_pos_tagger.tag(nltk.word_tokenize(word_to_be_tagged_next))
[(u'\u092e\u0948\u0902', u'PRP'),
 (u'\u092c\u0939\u0941\u0924', u'INTF'),
 (u'\u0939\u0948\u0930\u093e\u0928', 'Unk'),
 (u'\u0939\u0942\u0901', 'Unk')]
</code></pre>
",""
"29593497","2015-04-12 19:10:19","1","","29575034","<p>I don't know of any such wrapper at the moment, and there are no plans at Stanford to build one. (Maybe the NLTK developers would be up for the challenge?)</p>
",""
"29572279","2015-04-10 23:40:34","3","","29571927","<p>Yes, you can use WordNet. For example, you can search among hypernyms of the current word (e.g. <code>study</code>) for your category word (e.g. <code>education</code> or <code>sport</code>). There are <a href=""http://lyle.smu.edu/~tspell/jaws/index.html"" rel=""nofollow noreferrer"">JAWS</a>, <a href=""http://sourceforge.net/projects/jwordnet"" rel=""nofollow noreferrer"">JWNL</a>, and other libraries, see <a href=""https://stackoverflow.com/questions/5976537/wordnet-similarity-in-java-jaws-jwnl-or-java-wnsimilarity"">related question</a>.</p>

<p>Alternatively, you can compute similarity between candidate words and category words - e.g. by using <a href=""https://code.google.com/p/ws4j/"" rel=""nofollow noreferrer"">ws4j</a> or <a href=""http://deeptutor2.memphis.edu/Semilar-Web/public/semilar-api.html"" rel=""nofollow noreferrer"">Semilar</a>.</p>
",""
"29570477","2015-04-10 20:51:22","2","","29570207","<p>The NLTK TextCollection class has a method for computing the tf-idf of terms. The documentation is <a href=""http://www.nltk.org/api/nltk.html#nltk.text.TextCollection"">here</a>, and the source is <a href=""http://www.nltk.org/_modules/nltk/text.html"">here</a>. However, it says ""may be slow to load"", so using scikit-learn may be preferable.</p>
",""
"29570406","2015-04-10 20:46:58","3","","29562798","<p>One possible solution would be to train a classifier such as <a href=""http://en.wikipedia.org/wiki/Naive_Bayes_classifier"" rel=""nofollow"">Naive Bayes</a> on the tweets that a user has voted on. You can take a look at the documentation of <a href=""http://scikit-learn.org/"" rel=""nofollow"">scikit-learn</a>, a Python library, which explains how you can easily <a href=""http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"" rel=""nofollow"">preprocess your text and train such a classifier</a>.  </p>
",""
"29510341","2015-04-08 09:05:40","4","","29495556","<p>From Stanford CoreNLP, you can also use TokensRegex to match a pattern in a list of tokens: <a href=""http://nlp.stanford.edu/software/tokensregex.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/tokensregex.shtml</a></p>

<p>For example, your two patterns would be something like:</p>

<p>[{tag:NN}] [{word:is}] [{tag:JJ}]</p>

<p>[{tag:JJ}] [{tag:NN}]</p>

<p>(Side note, but NP is not a POS tag. Likely, really, what you want is [{tag:/N.*/}] and [{lemma:be}] to catch a broader range of cases).</p>
",""
"29451660","2015-04-04 21:37:26","0","","29439545","<p>Take a look at <a href=""https://stackoverflow.com/questions/29013941/relevance-of-document-to-multiple-keywords/29380252#29380252"">this</a> question on stack overflow, it's along the same lines.</p>

<p>You're going to want to tokenise your paragraphs and input, your output could simply be every passage that contains a word in your query, or the results could ranked using a <a href=""http://en.wikipedia.org/wiki/Vector_space_model"" rel=""nofollow noreferrer"">vector space model</a>.</p>
",""
"29397935","2015-04-01 18:17:23","5","","29397708","<p><a href=""http://www.nltk.org/api/nltk.tag.html#nltk.tag.pos_tag"" rel=""noreferrer""><code>nltk.tag.pos_tag</code></a> accepts a list of tokens, separate and tags its elements. Therefore you need to put your words in an iterable like list: </p>

<pre><code>&gt;&gt;&gt; nltk.tag.pos_tag(['going'])
[('going', 'VBG')]
</code></pre>
",""
"29333217","2015-03-29 18:38:35","2","","29332851","<p>The tags that you see are not a result of the chunks but the POS tagging that happens before chunking. It's the Penn Treebank tagset, see <a href=""https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"" rel=""noreferrer"">https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html</a></p>

<pre><code>&gt;&gt;&gt; from nltk import word_tokenize, pos_tag, ne_chunk
&gt;&gt;&gt; sent = ""This is a Foo Bar sentence.""
# POS tag.
&gt;&gt;&gt; nltk.pos_tag(word_tokenize(sent))
[('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('Foo', 'NNP'), ('Bar', 'NNP'), ('sentence', 'NN'), ('.', '.')]
&gt;&gt;&gt; tagged_sent = nltk.pos_tag(word_tokenize(sent))
# Chunk.
&gt;&gt;&gt; ne_chunk(tagged_sent)
Tree('S', [('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), Tree('ORGANIZATION', [('Foo', 'NNP'), ('Bar', 'NNP')]), ('sentence', 'NN'), ('.', '.')])
</code></pre>

<p>To get the chunks look for subtrees within the chunked outputs. From the above output, the <code>Tree('ORGANIZATION', [('Foo', 'NNP'), ('Bar', 'NNP')])</code> indicates the chunk.</p>

<p>This tutorial site is pretty helpful to explain the chunking process in NLTK: <a href=""https://web.archive.org/web/20150412115803/http://www.eecis.udel.edu:80/~trnka/CISC889-11S/lectures/dongqing-chunking.pdf"" rel=""noreferrer"">http://www.eecis.udel.edu/~trnka/CISC889-11S/lectures/dongqing-chunking.pdf</a>. </p>

<p>For official documentation, see <a href=""http://www.nltk.org/howto/chunk.html"" rel=""noreferrer"">http://www.nltk.org/howto/chunk.html</a></p>
",""
"29333165","2015-03-29 18:34:59","3","","29213886","<p>First obtain a <a href=""http://www-nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/semgraph/SemanticGraph.html"" rel=""nofollow""><code>SemanticGraph</code></a> object (e.g. by retrieving the value of a <code>BasicDependenciesAnnotation</code> in the CoreNLP pipeline, or by parsing directly with Stanford Parser). I can elaborate on this more if necessary.</p>

<p>The <code>SemanticGraph</code> provides a simple edge iterable for processing independent graph edges. (See the <a href=""http://www-nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/semgraph/SemanticGraphEdge.html"" rel=""nofollow""><code>SemanticGraphEdge</code></a> class. Also, note that <code>SemanticGraphEdge.getRelation</code> returns a <a href=""http://www-nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/trees/GrammaticalRelation.html"" rel=""nofollow""><code>GrammaticalRelation</code></a> instance.)</p>

<pre><code>SemanticGraph sg = ....
for (SemanticGraphEdge edge : sg.getEdgesIterable()) {
  int headIndex = edge.getGovernor().index();
  int depIndex = edge.getDependent().index();
  String label = edge.getRelation().getShortName();

  System.out.printf(""%d %d %s%n"", headIndex, depIndex, label);
}
</code></pre>
",""
"29327002","2015-03-29 08:13:27","0","","29302518","<p>Pipes separates the nodes in the chart and spaces separates individual words from a multiword expression. The multiword expression would create a single tree with two items in the list.</p>

<pre><code>team -&gt; 'indian' | 'australian' | 'england' | 'sri' 'lankan'
</code></pre>

<p>[out]:</p>

<pre><code>[Tree('root', [Tree('who_player', [Tree('which', ['which']), Tree('team', ['sri', 'lankan']), Tree('player', ['player'])]), Tree('has', ['has']), Tree('the', ['the']), Tree('most', ['most']), Tree('runs', ['runs'])])]
</code></pre>
",""
"29275814","2015-03-26 10:11:06","0","","29230623","<p>Answer (thanks to frankov): Look at the demo functions here: <a href=""http://www.nltk.org/_modules/nltk/parse/dependencygraph.html"" rel=""nofollow"">http://www.nltk.org/_modules/nltk/parse/dependencygraph.html</a></p>

<p>Covert to conll format then do something like</p>

<pre><code>dg = DependencyGraph(conll_data1)
tree = dg.tree()
tree.pprint()
</code></pre>
",""
"29219800","2015-03-23 20:32:04","0","","29151329","<p>The Stanford Arabic segmenter can't do true lemmatization. However, it is possible to train a new model to do something like stemming:</p>

<ul>
<li>ÿ™ŸÉÿ™ÿ®ŸàŸÜ ‚Üê ÿ™+ ŸÉÿ™ÿ® +ŸàŸÜ</li>
<li>Ÿäÿ™ÿµŸÑ ‚Üê Ÿä+ ÿ™ÿµŸÑ</li>
</ul>

<p>If it is very important that the output is real Arabic lemmas (""ÿ™ÿµŸÑ"" is not a true lemma), you might be better off with a tool like MADAMIRA (<a href=""http://nlp.ldeo.columbia.edu/madamira/"">http://nlp.ldeo.columbia.edu/madamira/</a>).</p>

<p><em>Elaboration:</em> The Stanford Arabic segmenter produces its output character-by-character using only these operations (implemented in <code>edu.stanford.nlp.international.arabic.process.IOBUtils</code>):</p>

<ul>
<li>Split a word between two characters</li>
<li>Transform lil- (ŸÑŸÑŸÄ) into li+ al- (ŸÑ+ ÿßŸÑŸÄ)</li>
<li>Transform ta (ÿ™) or ha (Ÿá) into ta marbuta (ÿ©)</li>
<li>Transform ya (Ÿä) or alif (ÿß) into alif maqsura (Ÿâ)</li>
<li>Transform alif maqsura (Ÿâ) into ya (Ÿä)</li>
</ul>

<p>So lemmatizing Ÿäÿ™ÿµŸÑ to Ÿä+ ÿßÿ™ÿµŸÑ would require implementing an extra rule, i.e., to insert an alif after ya or ta. Lemmatization of certain irregular forms would be completely impossible (for example, ŸÜÿ≥ÿßÿ° ‚Üê ÿßŸÖÿ±ÿ£ÿ©).</p>

<p>The version of the Stanford segmenter available for download also only breaks off pronouns and particles:</p>

<p>Ÿàÿ≥ŸäŸÉÿ™ÿ¥ŸÅŸàŸÜŸá ‚Üê Ÿà+ ÿ≥+ ŸäŸÉÿ™ÿ¥ŸÅŸàŸÜ +Ÿá</p>

<p>However, if you have access to the LDC Arabic Treebank or a similarly rich source of Arabic text with morphological segmentation annotated, it is possible to train your own model to remove all morphological affixes, which is closer to lemmatization:</p>

<p>Ÿàÿ≥ŸäŸÉÿ™ÿ¥ŸÅŸàŸÜŸá ‚Üê Ÿà+ ÿ≥+ Ÿä+ ŸÉÿ™ÿ¥ŸÅ +ŸàŸÜ +Ÿá</p>

<p>Note that ""ŸÉÿ™ÿ¥ŸÅ"" is not a real Arabic word, but the segmenter should at least consistently produce ""ŸÉÿ™ÿ¥ŸÅ"" for ÿ™ŸÉÿ™ÿ¥ŸÅŸäŸÜ ,ÿ£ŸÉÿ™ÿ¥ŸÅ ,ŸäŸÉÿ™ÿ¥ŸÅ, etc. If this is acceptable, you would need to change the ATB preprocessing script to instead use the morphological segmentation annotations. You could do this by replacing the script called <code>parse_integrated</code> with a modified version like this: <a href=""https://gist.github.com/futurulus/38307d98992e7fdeec0d"">https://gist.github.com/futurulus/38307d98992e7fdeec0d</a></p>

<p>Then follow the instructions for ""TRAINING THE SEGMENTER"" in the README.</p>
",""
"29146108","2015-03-19 13:37:45","0","","29142230","<p>See: <a href=""http://www.nltk.org/_modules/nltk/tag.html"" rel=""nofollow"">http://www.nltk.org/_modules/nltk/tag.html</a></p>

<p>In particular:</p>

<pre><code>&gt;&gt;&gt; from nltk.corpus import brown
&gt;&gt;&gt; from nltk.tag import UnigramTagger
&gt;&gt;&gt; tagger = UnigramTagger(brown.tagged_sents(categories='news')[:500])
&gt;&gt;&gt; sent = ['Mitchell', 'decried', 'the', 'high', 'rate', 'of', 'unemployment']
&gt;&gt;&gt; for word, tag in tagger.tag(sent):
...     print(word, '-&gt;', tag)
Mitchell -&gt; NP
decried -&gt; None
the -&gt; AT
high -&gt; JJ
rate -&gt; NN
of -&gt; IN
unemployment -&gt; None
</code></pre>

<p>The idea of the UnigramTagger is that it always assigns the tag that was most prominent for that particular word in the training corpus. Or (just above the piece of code in the docs:</p>

<blockquote>
  <p>This package defines several taggers, which take a token list (typically a
  sentence), assign a tag to each token, and return the resulting list of
  tagged tokens.  Most of the taggers are built automatically based on a
  training corpus.  For example, the unigram tagger tags each word <em>w</em>
  by checking what the most frequent tag for <em>w</em> was in a training corpus:</p>
</blockquote>

<p>Not sure if there is a built-in way to view all tags that can be assigned to a particular word. Moreover; this may theoretically be as long as the total number of tags identified, as it depends on context.
If you want to get an idea; what I would do is just tag your whole vocabulary and print out your vocabulary with all different tags assigned in that particular corpus. </p>
",""
"29132972","2015-03-18 21:23:58","4","","29131332","<p>A job for recursion. This should work.</p>

<pre><code>S -&gt; NP VP 
VP -&gt; V NP | V NP Conj VP
NP -&gt; Det N | PN | Pro
PN -&gt; 'David' 
Det -&gt; 'the'
N -&gt; 'man' 
V -&gt; 'saw' | 'helped' 
Pro -&gt; 'him'
Conj -&gt; 'and'
</code></pre>
",""
"29091900","2015-03-17 05:27:45","3","","29057037","<p>This is the normal form for nouns:</p>

<pre><code>N[NUM=sg, SEM=&lt;\x.race(x)&gt;] -&gt; 'race'
</code></pre>

<p>So, for example, you might have:</p>

<pre><code>Det[NUM=sg, SEM=&lt;\P.\Q.exists x.(P(x) &amp; Q(x))&gt;] -&gt; 'a'
NP[NUM=?num, SEM=&lt;?det(?n)&gt;] -&gt; Det[NUM=?num, SEM=?det] N[NUM=?num, SEM=?n]
</code></pre>

<p>So that ""a race"" is: </p>

<pre><code>\P.\Q.exists x.(P(x) &amp; Q(x))(\x.race(x)) = \Q.exists x.(\y.race(y)(x) &amp; Q(x)))
                                           \Q.exists x.(race(x) &amp; Q(x)))
</code></pre>

<p>And if you have:</p>

<pre><code>TV[NUM=sg, SEM=&lt;\X.\y.X(\x.run(y,x))&gt;] -&gt; 'run'
VP[NUM=?num, SEM=&lt;?tv(?obj)&gt;] -&gt; TV[NUM=?num, SEM=?tv] NP[NUM=?num, SEM=?obj]
</code></pre>

<p>So ""run a race"" is:</p>

<pre><code>\X.\y.X(\z.run(y,z))(\Q.exists x.(race(x) &amp; Q(x))))
                                = \y.(\Q.exists x.(race(x) &amp; Q(x)))(\z.run(y,z)))
                                = \y.exists x.(race(x) &amp; \z.run(y,z)(x)))
                                = \y.exists x.(race(x) &amp; run(y,x)))
</code></pre>

<p>And then you'd have:</p>

<pre><code>NP[NUM=sg, SEM=&lt;\P.P(I)&gt;] -&gt; 'I'
S[NUM=?num, SEM=&lt;?subj(?vp)&gt;] -&gt; NP[NUM=?num, SEM=?subj] VP[NUM=?num, SEM=?vp] 
</code></pre>

<p>So ""I run a race"" is:</p>

<pre><code>\P.P(I)(\y.exists x.(race(x) &amp; run(y,x)))) = \y.exists x.(race(x) &amp; run(y,x)))(I)
                                           = exists x.(race(x) &amp; run(I,x)))
</code></pre>

<p>Have a look at <a href=""http://www.nltk.org/book/ch10.html"" rel=""nofollow"">this</a>.</p>
",""
"29026564","2015-03-13 07:01:47","0","","29007478","<p>You can define the dependency with <a href=""http://uima.apache.org/d/uimafit-2.1.0/api/org/apache/uima/fit/descriptor/TypeCapability.html"" rel=""nofollow""><code>@TypeCapability</code></a> like this:</p>

<pre><code>@TypeCapability(inputs = { ""com.myproject.types.MyType"", ... }, outputs = { ... })
public class MyAnnotator extends JCasAnnotator_ImplBase {
    ....
}
</code></pre>

<p>Note that it defines a contract at the annotation level, not the engine level (meaning that any Engine could create <code>com.myproject.types.MyType</code>).</p>

<p>I don't think there are ways to <strong>enforce</strong> it.</p>

<p>I did create some code to check that an Engine is provided with the right required Annotations in the upstream of a pipeline, and prints an error log otherwise (see <a href=""https://github.com/BlueBrain/bluima/blob/8f41d21728fcd12ed4c2feb9578e3487c588a475/modules/bluima_scripting/src/main/java/ch/epfl/bbp/uima/laucher/Pipeline.java"" rel=""nofollow"">Pipeline.checkAndAddCapabilities() and Pipeline.addCapabilities()</a> ). Note however that it will only work if all Engines define their TypeCapabilities, which is often not the case when one uses external Engines/libraries.</p>
",""
"28985629","2015-03-11 11:37:11","1","","28925194","<p>Yes, the method returns <code>None</code> if no word sense was found. You may increase the size of the context. As far as I can see in the methods source code the context sentence must be tokenized.</p>
",""
"28951122","2015-03-09 20:26:23","0","","28938999","<p>Without using Natural Language Toolkit(NLTK) you may use simple Python command 
as follows.</p>

<pre><code>&gt;&gt;&gt; line=""a sentence with a few words""
&gt;&gt;&gt; line.split()
['a', 'sentence', 'with', 'a', 'few', 'words']
&gt;&gt;&gt;
</code></pre>

<p>given in <a href=""https://stackoverflow.com/questions/743806/split-string-into-a-list-in-python"">How to split a string into a list?</a> </p>
",""
"28889482","2015-03-05 22:59:45","0","","28881999","<p>TF in TF-IDF means frequency of a term in a document. In other words, TF-IDF is a measure for both the term and the document. <a href=""https://stackoverflow.com/a/4858932/4588780"">Here</a> is a good illustration of what I mean.</p>

<p>As far as I understand your case, you don't work with any particular document, instead you want to have some integral characteristic for each word over the whole document collection. So, you should use IDF (or simply DF, document frequency), if you want to find something like stop-words. See also for <a href=""https://stackoverflow.com/questions/16927494/how-to-select-stop-words-using-tf-idf-non-english-corpus?rq=1"">related question</a>.</p>
",""
"28884016","2015-03-05 17:29:00","2","","28883234","<p>We use the tag set of the (Penn/LDC/Brandeis/UC Boulder) Chinese Treebank.</p>

<p>See here for details on the tag set: <a href=""http://www.cis.upenn.edu/~chinese/"" rel=""nofollow"">http://www.cis.upenn.edu/~chinese/</a></p>

<p>This was documented in the parser FAQ, but I'll add it to the tagger FAQ.</p>
",""
"28791364","2015-03-01 07:57:22","3","","28791328","<p>As <code>nltk</code> is telling you, it searched for the file <code>help/tagsets/upenn_tagset.pickle</code> in the directories:</p>

<pre><code>- 'C:\\Users\\aarushi/nltk_data'
- 'C:\\nltk_data'
- 'D:\\nltk_data'
- 'E:\\nltk_data'
- 'C:\\Python34\\nltk_data'
- 'C:\\Python34\\lib\\nltk_data'
- 'C:\\Users\\aarushi\\AppData\\Roaming\\nltk_data'
</code></pre>

<p>And could not find it.</p>

<p><strong>Is it there?</strong></p>

<p>If not, use <code>nltk.download()</code> to get it, and make sure it's in one of these directories.</p>
",""
"28785134","2015-02-28 18:13:52","0","","28782336","<p>You can build this using the <a href=""http://www-nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/trees/TreeTransformer.html"" rel=""nofollow""><code>TreeTransformer</code></a> interface. Use a <a href=""http://www-nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/trees/HeadFinder.html"" rel=""nofollow""><code>HeadFinder</code></a> (if you're parsing English, the <a href=""http://www-nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/trees/CollinsHeadFinder.html"" rel=""nofollow""><code>CollinsHeadFinder</code></a>) to retrieve the head word / head constituent at each node.</p>

<p>You can see an example of this kind of work in the <a href=""https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/parser/lexparser/TreeAnnotator.java#L45"" rel=""nofollow""><code>TreeAnnotator</code></a> within the parser.</p>
",""
"28780913","2015-02-28 10:56:23","1","","28774623","<p>I would say, if you employ the above grammar to parse the given sentence, than the parser will return nothing, since no grammar rule matches the initial <code>This</code> to the <code>DT</code> phrase.</p>

<p>You may add the rule: <code>DP -&gt; Det</code></p>
",""
"28674667","2015-02-23 13:14:37","2","","28674417","<p><code>NLTK</code> has a class for reading parse trees: <code>nltk.tree.Tree</code>. The relevant method is called <code>fromstring</code>. You can then iterate its subtrees, leaves, etc...</p>

<p>As an aside: you might want to remove the bit that says <code>sent28:</code> as it confuses the parser (it's also not a part of the sentence). You are not getting a full parse tree, but just a sentence fragment.</p>
",""
"28525687","2015-02-15 11:44:37","0","","28522106","<p>The error comes from how NLTK implements types lambda calculus.</p>

<pre><code> \x.x(\y.some(y)) 
</code></pre>

<p>It expects lowercase letters to have type <code>&lt;e&gt;</code> and uppercase letters to have type <code>&lt;e,t&gt;</code>. That is to say that lowercase letters cannot represent predicates. </p>

<p>The following parses: \X.X(\y.some(y))</p>

<p>As an aside, one represents the concept of ""some"" in ""some X are Y"" with a conjunction as follows:</p>

<pre><code> \X Y.(X(x) &amp; Y(x))
</code></pre>

<p>In words, <em>some X are Y</em> is logically equivalent to <em>there are some items have both X and Y quality</em>.</p>
",""
"28487605","2015-02-12 20:56:47","1","","28469094","<p>As @kindall noted, it's becaus eof the unicode string.</p>

<p>But more specifically, it's because NLTK uses <code>from __future__ import unicode_literals</code> which converts <strong>ALL</strong> strings to unicode by default, see <a href=""https://github.com/nltk/nltk/blob/develop/nltk/stem/porter.py#L87"" rel=""nofollow"">https://github.com/nltk/nltk/blob/develop/nltk/stem/porter.py#L87</a></p>

<p>So let's try an experiment in python 2.x :</p>

<pre><code>$ python
&gt;&gt;&gt; from nltk.stem import PorterStemmer
&gt;&gt;&gt; porter = PorterStemmer()
&gt;&gt;&gt; word = ""analysis""
&gt;&gt;&gt; word
'analysis'
&gt;&gt;&gt; porter.stem(word)
u'analysi'
</code></pre>

<p>We see that suddenly the stemmed word became a unicode.</p>

<p>Then, let's try to import the <code>unicode_literals</code>:</p>

<pre><code>&gt;&gt;&gt; from nltk.stem import PorterStemmer
&gt;&gt;&gt; porter = PorterStemmer()
&gt;&gt;&gt; word = ""analysis""
&gt;&gt;&gt; word
'analysis'
&gt;&gt;&gt; porter.stem(word)
u'analysi'
&gt;&gt;&gt; from __future__ import print_function, unicode_literals
&gt;&gt;&gt; word
'analysis'
&gt;&gt;&gt; word2 = ""analysis""
&gt;&gt;&gt; word2
u'analysis'
</code></pre>

<p>Note that all strings remains still as strings but any string variable that's new after importing unicode_literals will become unicode by default.</p>
",""
"28458759","2015-02-11 15:58:21","0","","26582284","<p>In the current version of nltk_data, they provide two versions of the pickle files: one for Python 2 and one for Python 3. For example, there is one <code>english.pickle</code> at <code>nltk_data/taggers/maxent_treebank_pos_tagger</code> and one at <code>nltk_data/taggers/maxent_treebank_pos_tagger/PY3</code>. The newest nltk handles this automatically by a decorator <a href=""https://github.com/nltk/nltk/blob/develop/nltk/compat.py"" rel=""nofollow""><code>py3_data</code></a>. </p>

<p>In short, if you download the newest nltk_data, but don't have the newest nltk, it may load the wrong pickle file, raising the <code>UnicodeDecodeError</code> exception. </p>

<p>Note: suppose you already have the newest nltk, you may encounter some path error where you can see two ""PY3""'s in the path of the pickle file. This may mean some developers were not aware of the <code>py3_data</code> and have handled the path redundantly. You can remove/revert the redundancy by yourself. See <a href=""https://github.com/nltk/nltk/pull/880"" rel=""nofollow"">this pull request</a> for an example.</p>
",""
"28447478","2015-02-11 05:54:56","3","","28439522","<p>Stemmer is able to process artificial non-existing words. Would you like them to be returned as elements of a set of all possible words? How do you know that the word doesn't exist and shouldn't be returned?</p>

<p>As an option: find a dictionary of all words and their forms. Find a stem for every of them. Save this projection as a map: ( stem, list of all word forms ). So you'll be able to get the list of all word forms for a given stem.</p>

<p><strong>UPD:</strong>
If you need all possible words including non-existing then I can offer such an algorithm (it's not checked, just a suggestion):</p>

<p><a href=""http://snowball.tartarus.org/algorithms/porter/stemmer.html"" rel=""nofollow noreferrer"">Porter stemming algorithm</a>. We need a reversed version.</p>

<p>If the rule in straight algorithm has a form <code>(m&gt;1) E -&gt;</code> (delete last E) then the reversed rule would be ""fork with E"" which means we need to try alternative ways. E.g., in straight algorithm <code>probate -&gt; probat</code>, in reversed we have two alternatives: <code>probat -&gt; { probat, probate }</code>. Each of these alternatives should be separately processed further. Note that this is a <em>set</em> of alternatives, so we will process only distinct words. Such a rule would have the following form: <code>A -&gt; { , B, C }</code>, which means ""replace ending A in three alternative ways: leave as-is, with B and with C"".</p>

<pre><code>Step 5b: (m&gt;1) *L -&gt; { , +L } // Add L if there's L at the end.
Step 5a: (m&gt;1) -&gt; { , +E }
         (m=1 and not *o) -&gt; { , +E } // *o is a special condition, it's not *O.
Step 4: (m&gt;1) *S or *T -&gt; { , +ION }
        (m&gt;1) -&gt; { , +AL, +ANCE, +ENCE, ..., +IVE, +IZE }
Step 3: (m&gt;0) *AL -&gt; { , +IZE }
        (m&gt;0) *IC -&gt; { , +ATE, +ITI, +AL }
        (m&gt;0) -&gt; { , +ATIVE, +FUL, +NESS }
Step 2: (m&gt;0) *ATE -&gt; { , ATIONAL } // Replace ATE.
        (m&gt;0) *TION -&gt; { , +AL } // Add AL at the end.
        (m&gt;0) *ENCE -&gt; { , ENCI } // Replace ENCE.
        ...
        (m&gt;0) *BLE -&gt; { , BILITI } // Replace BLE.
Step 1c: (*v*) *I -&gt; { , Y } // Replace I.
Step 1b: (m=1 and *oE) -&gt; { , +D, delete last E and add ING } // *o is a special condition.
         (*v*c and not (*L or *S or *Z)) -&gt; { , add last consonant +ED, add last consonant + ING }
         *IZE -&gt; { , IZING, +D }
         (*v*BLE) -&gt; { , +D, delete last E and add ING }
         *ATE -&gt; { , ATING, +D }
         (*v*) -&gt; { , +ED, +ING }
         (m&gt;0) *EE -&gt; { , +D }
Step 1a: *I -&gt; { , +ES }
         *SS -&gt; { , +ES }
         not *S -&gt; { , +S }
</code></pre>

<p>The straight algorithm had to choose first longest rule. The reversed algorithm should use all the rules.</p>

<p>Example (straight):</p>

<pre><code>Input: PLAYING
Step 1a doesn't match.
PLAYING -&gt; PLAY (Step 1b)
PLAY -&gt; PLAI (Step 1c)
m=0, so the steps 2-5 don't match.
Result: PLAI
</code></pre>

<p>Reversed:</p>

<pre><code>Input: PLAI
m=0, so the steps 2-5 are skipped
Step 1c:
PLAI -&gt; { PLAI, PLAY }
Step 1b:
PLAI -&gt; { PLAI, PLAIED, PLAIING }
PLAY -&gt; { PLAY, PLAYED, PLAYING }
Resulting set: { PLAI, PLAIED, PLAIING, PLAY, PLAYED, PLAYING }
Step 1a:
PLAI -&gt; { PLAI, PLAIS, PLAIES }
PLAIED -&gt; { PLAIED, PLAIEDS }
PLAIING -&gt; { PLAIING, PLAIINGS }
PLAY -&gt; { PLAY, PLAYS }
PLAYED -&gt; { PLAYED, PLAYEDS }
PLAYING -&gt; { PLAYING, PLAYINGS }
Resulting set: { PLAI, PLAIS, PLAIES, PLAIED, PLAIEDS, PLAIING, PLAIINGS, PLAY, PLAYS, PLAYED, PLAYEDS, PLAYING, PLAYINGS }
</code></pre>

<p>I've checked all those words at <a href=""https://stackoverflow.com/a/28444811/4540645"">Michael Tontchev</a>'s link. The result for every of them is ""plai"" (note that the site doesn't accept upper-case input).</p>
",""
"28377687","2015-02-07 01:56:03","0","","28341007","<p>You asked for 'creative' approaches - the <em>Cell Closure</em> pruning method might be worth a look. See the series of publications by Brian Roark, Kristy Hollingshead, and Nathan Bodenstab. Papers: <a href=""http://www.aclweb.org/anthology-new/C/C08/C08-1094.pdf"" rel=""nofollow"">1</a> <a href=""http://www.aclweb.org/anthology-new/N/N09/N09-1073.pdf"" rel=""nofollow"">2</a> <a href=""http://www.mitpressjournals.org/doi/abs/10.1162/COLI_a_00109"" rel=""nofollow"">3</a>. The basic intuition is:</p>

<ul>
<li>Each cell in the CYK parse chart 'covers' a certain span (e.g. the first 4 words of the sentence, or words 13-18, etc.)</li>
<li>Some words - particularly in certain contexts - are <em>very</em> unlikely to begin a multi-word syntactic constituent; others are similarly unlikely to end a constituent. For example, the word 'the' almost always precedes a noun phrase, and it's almost inconceivable that it would end a constituent.</li>
<li>If we can train a machine-learned classifier to identify such words with very high precision, we can thereby identify cells which would only participate in parses placing said words in highly improbable syntactic positions. (Note that this classifier might make use of a linear-time POS tagger, or other high-speed preprocessing steps.)</li>
<li>By 'closing' these cells, we can reduce both the the asymptotic and average-case complexities considerably - in theory, from cubic complexity all the way to linear; practically, we can achieve approximately n^1.5 without loss of accuracy.</li>
</ul>

<p>In many cases, this pruning actually <em>increases</em> accuracy slightly vs. an exhaustive search, because the classifier can incorporate information that isn't available to the PCFG. Note that this is a simple, but very effective form of coarse-to-fine pruning, with a single coarse stage (as compared to the 7-stage CTF approach in the Berkeley Parser).</p>

<p>To my knowledge, the Stanford Parser doesn't currently implement this pruning technique; I suspect you'd find it quite effective.</p>

<p><strong>Shameless plug</strong>
The <a href=""http://bubs-parser.googlecode.com/"" rel=""nofollow"">BUBS Parser</a> implements this approach, as well as a few other optimizations, and thus achieves throughput of around 2500-5000 words per second, usually with accuracy at least equal to that I've measured with the Stanford Parser. Obviously, if you're using the rest of the Stanford pipeline, the built-in parser is already well integrated and convenient. But if you need improved speed, BUBS might be worth a look, and it does include some example code to aid in embedding the engine in a larger system.</p>

<p><strong>Memoizing Common Substrings</strong>
Regarding your thoughts on pre-analyzing known noun phrases or other frequently-observed sequences with consistent structure: I did some evaluation of a similar idea a few years ago (in the context of sharing common substructures across a large corpus, when parsing on a massively parallel architecture). The preliminary results weren't encouraging.In the corpora we looked at, there just weren't enough repeated substrings of substantial length to make it worthwhile. And the aforementioned cell closure methods usually make those substrings really cheap to parse anyway.</p>

<p>However, if your target domains involved a lot of repetition, you might come to a different conclusion (maybe it would be effective on legal documents with lots of copy-and-paste boilerplate? Or news stories that are repeated from various sources or re-published with edits?)</p>
",""
"28361181","2015-02-06 08:08:49","2","","28360402","<p>The default NLTK POS tag is trained on English texts and is supposedly for English text processing, see <a href=""http://www.nltk.org/_modules/nltk/tag.html"" rel=""nofollow"">http://www.nltk.org/_modules/nltk/tag.html</a>. The docs:</p>

<pre><code>An off-the-shelf tagger is available.  It uses the Penn Treebank tagset:

    &gt;&gt;&gt; from nltk.tag import pos_tag  # doctest: +SKIP
    &gt;&gt;&gt; from nltk.tokenize import word_tokenize # doctest: +SKIP
    &gt;&gt;&gt; pos_tag(word_tokenize(""John's big idea isn't all that bad."")) # doctest: +SKIP
    [('John', 'NNP'), (""'s"", 'POS'), ('big', 'JJ'), ('idea', 'NN'), ('is',
    'VBZ'), (""n't"", 'RB'), ('all', 'DT'), ('that', 'DT'), ('bad', 'JJ'),
    ('.', '.')]
</code></pre>

<p>And the code for <code>pos_tag</code>:</p>

<pre><code>from nltk.data import load


# Standard treebank POS tagger
_POS_TAGGER = 'taggers/maxent_treebank_pos_tagger/english.pickle'
def pos_tag(tokens):
    """"""
    Use NLTK's currently recommended part of speech tagger to
    tag the given list of tokens.

        &gt;&gt;&gt; from nltk.tag import pos_tag # doctest: +SKIP
        &gt;&gt;&gt; from nltk.tokenize import word_tokenize # doctest: +SKIP
        &gt;&gt;&gt; pos_tag(word_tokenize(""John's big idea isn't all that bad."")) # doctest: +SKIP
        [('John', 'NNP'), (""'s"", 'POS'), ('big', 'JJ'), ('idea', 'NN'), ('is',
        'VBZ'), (""n't"", 'RB'), ('all', 'DT'), ('that', 'DT'), ('bad', 'JJ'),
        ('.', '.')]

    :param tokens: Sequence of tokens to be tagged
    :type tokens: list(str)
    :return: The tagged tokens
    :rtype: list(tuple(str, str))
    """"""
    tagger = load(_POS_TAGGER)
    return tagger.tag(tokens)
</code></pre>

<p>This works for me to get Stanford tools working in python on Ubuntu 14.4.1:</p>

<pre><code>$ cd ~
$ wget http://nlp.stanford.edu/software/stanford-postagger-full-2015-01-29.zip
$ unzip stanford-postagger-full-2015-01-29.zip
$ wget http://nlp.stanford.edu/software/stanford-segmenter-2015-01-29.zip
$ unzip /stanford-segmenter-2015-01-29.zip
$ python
</code></pre>

<p>and then:</p>

<pre><code>from nltk.tag.stanford import POSTagger
path_to_model= '/home/alvas/stanford-postagger-full-2015-01-30/models/arabic.tagger'
path_to_jar = '/home/alvas/stanford-postagger-full-2015-01-30/stanford-postagger-3.5.1.jar'

artagger = POSTagger(path_to_model, path_to_jar, encoding='utf8')
artagger._SEPARATOR = '/'
tagged_sent = artagger.tag(u""ÿ£ŸÜÿß ÿ™ÿ≥ŸÑŸÇ ÿ¥ÿ¨ÿ±ÿ©"")
print(tagged_sent)
</code></pre>

<p>[out]:</p>

<pre><code>$ python3 test.py
[('ÿ£', 'NN'), ('ŸÜ', 'NN'), ('ÿß', 'NN'), ('ÿ™', 'NN'), ('ÿ≥', 'RP'), ('ŸÑ', 'IN'), ('ŸÇ', 'NN'), ('ÿ¥', 'NN'), ('ÿ¨', 'NN'), ('ÿ±', 'NN'), ('ÿ©', 'PRP')]
</code></pre>

<p>If you have java problems when using Stanford POS tagger, see DELPH-IN wiki: <a href=""http://moin.delph-in.net/ZhongPreprocessing"" rel=""nofollow"">http://moin.delph-in.net/ZhongPreprocessing</a></p>
",""
"28334552","2015-02-05 01:08:31","3","","28328372","<p>I was able to recreate the behavior of the passing a tokenizer function over-rides the <code>token_pattern</code> pattern.</p>

<p>Here is a tokenizer that excludes tokens less than 4 characters:</p>

<pre><code>from nltk import word_tokenize
def tokenizer(x):
    return ( w for w in word_tokenize(x) if len(w) &gt;3)
</code></pre>

<p>The good news is passing your own tokenizer doesn't override the ngram parameter.</p>
",""
"28318683","2015-02-04 10:11:56","2","","28214148","<p>Hello you can try package <a href=""http://cran.r-project.org/web/packages/koRpus/index.html""><code>koRpus</code></a> which allow to use <a href=""http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/"">Treetagger</a> :</p>

<pre><code>tagged.results &lt;- treetag(c(""run"", ""ran"", ""running""), treetagger=""manual"", format=""obj"",
                      TT.tknz=FALSE , lang=""en"",
                      TT.options=list(path=""./TreeTagger"", preset=""en""))
tagged.results@TT.res

##     token tag lemma lttr wclass                               desc stop stem
## 1     run  NN   run    3   noun             Noun, singular or mass   NA   NA
## 2     ran VVD   run    3   verb                   Verb, past tense   NA   NA
## 3 running VVG   run    7   verb Verb, gerund or present participle   NA   NA
</code></pre>

<p>See the <code>lemma</code> column for the result you're asking for.</p>
",""
"28132353","2015-01-25 01:44:43","0","","28131821","<p><strong>Short answer</strong>: You have to write your own semantic grammar. There is just no easy way to map a CFG to lambda-calculus. </p>

<p><strong>Longer answer</strong>:</p>

<p>From CFG, the closest you get to a semantic tree is to specify semantic features for non-terminals and the SQL CFG example is a good guide to how to go about doing that, see section 1.1. from <a href=""http://www.nltk.org/book/ch10.html"" rel=""nofollow"">http://www.nltk.org/book/ch10.html</a>:</p>

<pre><code>&gt;&gt;&gt; nltk.data.show_cfg('grammars/book_grammars/sql0.fcfg')
% start S
S[SEM=(?np + WHERE + ?vp)] -&gt; NP[SEM=?np] VP[SEM=?vp]
VP[SEM=(?v + ?pp)] -&gt; IV[SEM=?v] PP[SEM=?pp]
VP[SEM=(?v + ?ap)] -&gt; IV[SEM=?v] AP[SEM=?ap]
NP[SEM=(?det + ?n)] -&gt; Det[SEM=?det] N[SEM=?n]
PP[SEM=(?p + ?np)] -&gt; P[SEM=?p] NP[SEM=?np]
AP[SEM=?pp] -&gt; A[SEM=?a] PP[SEM=?pp]
NP[SEM='Country=""greece""'] -&gt; 'Greece'
NP[SEM='Country=""china""'] -&gt; 'China'
Det[SEM='SELECT'] -&gt; 'Which' | 'What'
N[SEM='City FROM city_table'] -&gt; 'cities'
IV[SEM=''] -&gt; 'are'
A[SEM=''] -&gt; 'located'
P[SEM=''] -&gt; 'in'
</code></pre>

<p>Mapping CFG to lambda-calculus and vice versa or learning both from text is a still a research worthy work, so there's no clear way to do it for now. </p>

<p>See <a href=""http://dl.acm.org/citation.cfm?id=2052254"" rel=""nofollow"">http://dl.acm.org/citation.cfm?id=2052254</a> and
<a href=""http://www.computer.org/csdl/proceedings/ictai/2008/3440/02/3440b135-abs.html"" rel=""nofollow"">http://www.computer.org/csdl/proceedings/ictai/2008/3440/02/3440b135-abs.html</a></p>
",""
"28083013","2015-01-22 07:05:15","0","","28072775","<p>WordNet is an excellent tool, and I think you are on the right track. The relation that you are looking for is a <em>hyponym/hypernym</em> relation: the noun <em>horse</em> as a hyponym of <em>animal</em>, and, conversely, <em>animal</em> is a hypernym of <em>horse</em>. WordNet does provide data to evaluate whether two nouns are in this relationship.</p>

<p>Speaking of WordNet, you will probably find all animals in the <code>noun.animal</code> file. This may make your particular problem simpler.</p>

<p>To go from <em>duckling</em> to <em>duck</em>, you would navigate WordNet's <em>sister term</em> relation, which gives a collection of related words. I am not sure if you would get false positives from that, but probably there will be some. <em>Duck</em> and <em>duckling</em> are also listed in a <em>derivationally-related</em> relationship, but <em>lion</em> and <em>cub</em> are not. This might be a moot point, since both <em>duckling</em> and <em>cub</em> are, in some word senses, are animals.</p>

<p>You must, however, tag parts of speech, and take only nouns into account, otherwise you would get false positives when the sentence uses verbs <em>to horse around</em> and <em>to duck</em> (jerk down). Part-of-speech (POS) tagging is a whole problem in itself, and you probably want to look at some of existing libraries that do it. Most successful use a statistical approach, but the results are pretty robust, although might be not 100% correct.</p>

<p>Also, you will inevitably get other type false positives, from noun homonymy. For example, a <em>horse</em> may refer to a piece of gymnastics equipment, which is obviously not an animal. <em>Duck</em> can also refer to a type of fabric. Without deeper context you will not likely be able to resolve such a homonymy. But without a full general intelligence that would understand the text completely, this problem is rather not exactly solvable.</p>
",""
"27997729","2015-01-17 08:41:22","0","","27975767","<p>See <a href=""http://goo.gl/zog68k"">http://goo.gl/zog68k</a>, <code>nltk.sem.logic.Expression</code> is:</p>

<blockquote>
  <p>""""""This is the base abstract object for all logical expressions""""""</p>
</blockquote>

<p>There are many types of logical expressions implemented in <code>nltk</code>. See line 1124, the <code>ApplicationExpression</code> is:</p>

<blockquote>
  <p>This class is used to represent two related types of logical expressions. </p>
  
  <p>The first is a Predicate Expression, such as ""P(x,y)"".  A predicate expression is comprised of a <code>FunctionVariableExpression</code> or
      <code>ConstantExpression</code> as the predicate and a list of Expressions as the arguments.</p>
  
  <p>The second is a an application of one expression to another, such as
      ""(\x.dog(x))(fido)"".</p>
  
  <p>The reason Predicate Expressions are treated as Application Expressions is
      that the Variable Expression predicate of the expression may be replaced
      with another Expression, such as a LambdaExpression, which would mean that
      the Predicate should be thought of as being applied to the arguments.</p>
  
  <p>The logical expression reader will always curry arguments in a application expression.
      So, ""\x y.see(x,y)(john,mary)"" will be represented internally as
      ""((\x y.(see(x))(y))(john))(mary)"".  This simplifies the internals since
      there will always be exactly one argument in an application.</p>
  
  <p>The str() method will usually print the curried forms of application
      expressions.  The one exception is when the the application expression is
      really a predicate expression (ie, underlying function is an
      <code>AbstractVariableExpression</code>).  This means that the example from above
      will be returned as ""(\x y.see(x,y)(john))(mary)"".</p>
</blockquote>

<p>I'm not exactly an expert in formal logics but your code above is trying to declare a logical function variable x:</p>

<pre><code>&gt;&gt;&gt; from nltk.sem.logic import *
&gt;&gt;&gt; lexpr = Expression.fromstring
&gt;&gt;&gt; zero = lexpr(r'\F x.x')
&gt;&gt;&gt; succ = lexpr(r'\N F x.F(N(F,x))')
&gt;&gt;&gt; v1 = ApplicationExpression(succ, zero).simplify()
&gt;&gt;&gt; v1
&lt;LambdaExpression \F x.F(x)&gt;
&gt;&gt;&gt; print v1
\F x.F(x)
</code></pre>

<p>For a crash course, see <a href=""http://theory.stanford.edu/~arbrad/slides/cs156/lec2-4.pdf"">http://theory.stanford.edu/~arbrad/slides/cs156/lec2-4.pdf</a> and a nltk crash course to lambda expressions, see <a href=""http://www.cs.utsa.edu/~bylander/cs5233/nltk-intro.pdf"">http://www.cs.utsa.edu/~bylander/cs5233/nltk-intro.pdf</a></p>
",""
"27910192","2015-01-12 20:25:51","3","","25614279","<p>Why would it match <em>asset</em>?  That's not a verb, and as such shouldn't have that suffix attached to it.</p>

<p>The problems that languages aren't perfectly regular.  The solution that we've used in the Asturian spell checker at SoftAstur is to keep track a list of verbs that form certain suffixes one way or another, and have a script construct the <code>.dic</code> file based on the lists we've kept.  </p>

<p>So for English, you'd define two separate affixes<sup>1</sup>: </p>

<pre><code>SFX Gs Y 3
SFX Gs e ing [^eoy]e
SFX Gs 0 ing [eoy]e
SFX Gs 0 ing [^e]

SFX Gd Y 9
SFX 0 bing [^aeiou][aeiou]b
SFX 0 king [^aeiou][aeiou]c
SFX 0 ding [^aeiou][aeiou]d
SFX 0 ling [^aeiou][aeiou]l   # for British English
SFX 0 ming [^aeiou][aeiou]m
SFX 0 ning [^aeiou][aeiou]n
SFX 0 ping [^aeiou][aeiou]p
SFX 0 ring [^aeiou][aeiou]r
SFX 0 ting [^aeiou][aeiou]t
</code></pre>

<p>There are still other irregulars like <em>singeing</em> (to contrast with <em>singing</em>) that are uncommon enough they are probably best coded as separate.  So your dictionary file then would like the following more or less:</p>

<pre><code>admit/Gd    --&gt; admitting
bake/Gs     --&gt; baking
commit/Gd   --&gt; committed
free/Gs     --&gt; freeing
dye/Gs      --&gt; dyeing
inherit/Gs  --&gt; inherited
picnic/Gd   --&gt; picnicking
target/Gs   --&gt; targetting
tiptoe/Gs   --&gt; tiptoeing
travel/Gs   --&gt; traveling  (if American English)
travel/Gd   --&gt; travelling (if British English)
refer/Gd    --&gt; referring
sing/Gs     --&gt; singing
singe
singing
sob/Gd      --&gt; sobbing
smile/Gs    --&gt; smiling
stop/Gd     --&gt; stopping
tap/Gd      --&gt; tapping
visit/Gs    --&gt; visiting
</code></pre>

<hr>

<p><sup>1. I prefer two-letter tags as they can be easier to read if you have a word with lots of tags, such that <code>Gd</code> = <em>gerund doubled</em> and <code>Gs</code> = <em>gerund single</em> or similar.  Probably not a problem for English, but it definitely is for other languages.  If you don't have a lot of affixes, you might just go with <code>g</code> (no doubling) and <code>G</code> (doubling). </sup></p>
",""
"27873105","2015-01-10 05:16:08","0","","27869416","<p><strong>In short</strong>: yes it is possible but you will get through a heck of pain, it's easier to rewrite your CFG using the <code>atis.cfg</code> as a base then read the new CFG textfile. It's easier than to reassign each new terminal to the correct non-terminal to map them</p>

<hr>

<p><strong>In long</strong>, see the following</p>

<p>First let's look at what a CFG grammar in NLTK is and what it contains:</p>

<pre><code>&gt;&gt;&gt; import nltk
&gt;&gt;&gt; g = nltk.data.load('grammars/large_grammars/atis.cfg')
&gt;&gt;&gt; dir(g)
['__class__', '__delattr__', '__dict__', '__doc__', '__format__', '__getattribute__', '__hash__', '__init__', '__module__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', '__weakref__', '_all_unary_are_lexical', '_calculate_grammar_forms', '_calculate_indexes', '_calculate_leftcorners', '_categories', '_empty_index', '_immediate_leftcorner_categories', '_immediate_leftcorner_words', '_is_lexical', '_is_nonlexical', '_leftcorner_parents', '_leftcorner_words', '_leftcorners', '_lexical_index', '_lhs_index', '_max_len', '_min_len', '_productions', '_rhs_index', '_start', 'check_coverage', 'fromstring', 'is_binarised', 'is_chomsky_normal_form', 'is_flexible_chomsky_normal_form', 'is_leftcorner', 'is_lexical', 'is_nonempty', 'is_nonlexical', 'leftcorner_parents', 'leftcorners', 'max_len', 'min_len', 'productions', 'start', 'unicode_repr']
</code></pre>

<p>For more details, see <a href=""https://github.com/nltk/nltk/blob/develop/nltk/grammar.py#L421"" rel=""noreferrer"">https://github.com/nltk/nltk/blob/develop/nltk/grammar.py#L421</a></p>

<p>Seems like the terminals and non-terminals are of <code>Production</code> type, see <a href=""https://github.com/nltk/nltk/blob/develop/nltk/grammar.py#L236"" rel=""noreferrer"">https://github.com/nltk/nltk/blob/develop/nltk/grammar.py#L236</a>, i.e.</p>

<blockquote>
  <p>A grammar production.  Each production maps a single symbol
      on the ""left-hand side"" to a sequence of symbols on the
      ""right-hand side"".  (In the case of context-free productions,
      the left-hand side must be a <code>Nonterminal</code>, and the right-hand
      side is a sequence of terminals and <code>Nonterminals</code>.)
      ""terminals"" can be any immutable hashable object that is
      not a <code>Nonterminal</code>.  Typically, terminals are strings
      representing words, such as <code>""dog""</code> or <code>""under""</code>.</p>
</blockquote>

<p>So let's take a look at how the grammar stores the productions:</p>

<pre><code>&gt;&gt;&gt; type(g._productions)
&lt;type 'list'&gt;
&gt;&gt;&gt; g._productions[-1]
zero -&gt; 'zero'
&gt;&gt;&gt; type(g._productions[-1])
&lt;class 'nltk.grammar.Production'&gt;
</code></pre>

<p>So now, it seems like we could just create the <code>nltk.grammar.Production</code> objects and append them to the <code>grammar._productions</code>.</p>

<p>Let's try with the original grammar:</p>

<pre><code>&gt;&gt;&gt; import nltk
&gt;&gt;&gt; original_grammar = nltk.data.load('grammars/large_grammars/atis.cfg')
&gt;&gt;&gt; original_parser = ChartParser(original_grammar)
&gt;&gt;&gt; sent = ['show', 'me', 'northwest', 'flights', 'to', 'detroit', '.']
&gt;&gt;&gt; for i in original_parser.parse(sent):
...     print i
...     break
... 
(SIGMA
  (IMPR_VB
    (VERB_VB (show show))
    (NP_PPO
      (pt_pron_ppo me)
      (NAPPOS_NP (NOUN_NP (northwest northwest))))
    (NP_NNS (NOUN_NNS (pt207 flights)) (PREP_IN (to to)))
    (AVPNP_NP (NOUN_NP (detroit detroit)))
    (pt_char_per .)))
</code></pre>

<p>The original grammar doesn't have the terminal <code>singapore</code>:</p>

<pre><code>&gt;&gt;&gt; sent = ['show', 'me', 'northwest', 'flights', 'to', 'singapore', '.']
&gt;&gt;&gt; for i in original_parser.parse(sent):
...     print i
...     break
... 
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/usr/local/lib/python2.7/dist-packages/nltk/parse/api.py"", line 49, in parse
    return iter(self.parse_all(sent))
  File ""/usr/local/lib/python2.7/dist-packages/nltk/parse/chart.py"", line 1350, in parse_all
    chart = self.chart_parse(tokens)
  File ""/usr/local/lib/python2.7/dist-packages/nltk/parse/chart.py"", line 1309, in chart_parse
    self._grammar.check_coverage(tokens)
  File ""/usr/local/lib/python2.7/dist-packages/nltk/grammar.py"", line 631, in check_coverage
    ""input words: %r."" % missing)
ValueError: Grammar does not cover some of the input words: u""'singapore'"".
</code></pre>

<p>Before we try to add <code>singapore</code> into the grammar, let's see how <code>detroit</code> is stored in the grammar:</p>

<pre><code>&gt;&gt;&gt; original_grammar._rhs_index['detroit']
[detroit -&gt; 'detroit']
&gt;&gt;&gt; type(original_grammar._rhs_index['detroit'])
&lt;type 'list'&gt;
&gt;&gt;&gt; type(original_grammar._rhs_index['detroit'][0])
&lt;class 'nltk.grammar.Production'&gt;
&gt;&gt;&gt; original_grammar._rhs_index['detroit'][0]._lhs
detroit
&gt;&gt;&gt; original_grammar._rhs_index['detroit'][0]._rhs
(u'detroit',)
&gt;&gt;&gt; type(original_grammar._rhs_index['detroit'][0]._lhs)
&lt;class 'nltk.grammar.Nonterminal'&gt;
&gt;&gt;&gt; type(original_grammar._rhs_index['detroit'][0]._rhs)
&lt;type 'tuple'&gt;
&gt;&gt;&gt; original_grammar._rhs_index[original_grammar._rhs_index['detroit'][0]._lhs]
[NOUN_NP -&gt; detroit, NOUN_NP -&gt; detroit minneapolis toronto]
</code></pre>

<p>So now we can try to recreate the same <code>Production</code> object for <code>singapore</code>:</p>

<pre><code># First let's create Non-terminal for singapore.
&gt;&gt;&gt; nltk.grammar.Nonterminal('singapore')
singapore
&gt;&gt;&gt; lhs = nltk.grammar.Nonterminal('singapore')
&gt;&gt;&gt; rhs = [u'singapore']
# Now we can create the Production for singapore.
&gt;&gt;&gt; singapore_production = nltk.grammar.Production(lhs, rhs)
# Now let's try to add this Production the grammar's list of production
&gt;&gt;&gt; new_grammar = nltk.data.load('grammars/large_grammars/atis.cfg')
&gt;&gt;&gt; new_grammar._productions.append(singapore_production)
</code></pre>

<p>But it's still not working but cause giving the terminals itself don't really help in relating it to the rest of the CFG and hence singapore is still not parse-able:</p>

<pre><code>&gt;&gt;&gt; new_grammar = nltk.data.load('grammars/large_grammars/atis.cfg')
&gt;&gt;&gt; new_grammar._productions.append(singapore_production)
&gt;&gt;&gt; new_parser = ChartParser(new_grammar)
&gt;&gt;&gt; sent = ['show', 'me', 'northwest', 'flights', 'to', 'singapore', '.']
&gt;&gt;&gt; new_parser.parse(sent)
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/usr/local/lib/python2.7/dist-packages/nltk/parse/api.py"", line 49, in parse
    return iter(self.parse_all(sent))
  File ""/usr/local/lib/python2.7/dist-packages/nltk/parse/chart.py"", line 1350, in parse_all
    chart = self.chart_parse(tokens)
  File ""/usr/local/lib/python2.7/dist-packages/nltk/parse/chart.py"", line 1309, in chart_parse
    self._grammar.check_coverage(tokens)
  File ""/usr/local/lib/python2.7/dist-packages/nltk/grammar.py"", line 631, in check_coverage
    ""input words: %r."" % missing)
ValueError: Grammar does not cover some of the input words: u""'singapore'"".
</code></pre>

<p>From the following we know that singapore is like detroit, and detroit leads to this Left-handside LHS <code>NOUN_NP -&gt; detroit</code>:</p>

<pre><code>&gt;&gt;&gt; original_grammar._rhs_index[original_grammar._rhs_index['detroit'][0]._lhs]
[NOUN_NP -&gt; detroit, NOUN_NP -&gt; detroit minneapolis toronto]
</code></pre>

<p>So what we will need to do is to either add another production for singapore that leads to <code>NOUN_NP</code> nonterminals or append our singapore LHS to the NOUN_NP nonterminals right handside:</p>

<pre><code>&gt;&gt;&gt; lhs = nltk.grammar.Nonterminal('singapore')
&gt;&gt;&gt; rhs = [u'singapore']
&gt;&gt;&gt; singapore_production = nltk.grammar.Production(lhs, rhs)
&gt;&gt;&gt; new_grammar._productions.append(singapore_production)
</code></pre>

<p>Now let's add the new production for <code>NOUN_NP -&gt; singapore</code>:</p>

<pre><code>lhs2 = nltk.grammar.Nonterminal('NOUN_NP')
new_grammar._productions.append(nltk.grammar.Production(lhs2, [lhs]))
</code></pre>

<p>And now we should expect our parser to work:</p>

<pre><code>sent = ['show', 'me', 'northwest', 'flights', 'to', 'singapore', '.']
print new_grammar.productions()[2091]
print new_grammar.productions()[-1]
new_parser = nltk.ChartParser(new_grammar)
for i in new_parser.parse(sent):
    print i
</code></pre>

<p>[out]:</p>

<pre><code>Traceback (most recent call last):
  File ""test.py"", line 31, in &lt;module&gt;
    for i in new_parser.parse(sent):
  File ""/usr/local/lib/python2.7/dist-packages/nltk/parse/api.py"", line 49, in parse
    return iter(self.parse_all(sent))
  File ""/usr/local/lib/python2.7/dist-packages/nltk/parse/chart.py"", line 1350, in parse_all
    chart = self.chart_parse(tokens)
  File ""/usr/local/lib/python2.7/dist-packages/nltk/parse/chart.py"", line 1309, in chart_parse
    self._grammar.check_coverage(tokens)
  File ""/usr/local/lib/python2.7/dist-packages/nltk/grammar.py"", line 631, in check_coverage
    ""input words: %r."" % missing)
ValueError: Grammar does not cover some of the input words: u""'singapore'"".
</code></pre>

<p>But it looks like the grammar is still not recognizing the new terminals and nonterminals we've added, so let's try a hack and output our new grammar into string and create a newer grammar from the output string:</p>

<pre><code>import nltk

lhs = nltk.grammar.Nonterminal('singapore')
rhs = [u'singapore']
singapore_production = nltk.grammar.Production(lhs, rhs)
new_grammar = nltk.data.load('grammars/large_grammars/atis.cfg')
new_grammar._productions.append(singapore_production)    
lhs2 = nltk.grammar.Nonterminal('NOUN_NP')
new_grammar._productions.append(nltk.grammar.Production(lhs2, [lhs]))

# Create newer grammar from new_grammar's string
newer_grammar =  nltk.grammar.CFG.fromstring(str(new_grammar).split('\n')[1:])
# Reassign new_grammar's string to newer_grammar !!!
newer_grammar._start = new_grammar.start()
newer_grammar
sent = ['show', 'me', 'northwest', 'flights', 'to', 'singapore', '.']
print newer_grammar.productions()[2091]
print newer_grammar.productions()[-1]
newer_parser = nltk.ChartParser(newer_grammar)
for i in newer_parser.parse(sent):
    print i
    break
</code></pre>

<p>[out]:</p>

<pre><code>(SIGMA
  (IMPR_VB
    (VERB_VB (show show))
    (NP_PPO
      (pt_pron_ppo me)
      (NAPPOS_NP (NOUN_NP (northwest northwest))))
    (NP_NNS (NOUN_NNS (pt207 flights)) (PREP_IN (to to)))
    (AVPNP_NP (NOUN_NP (singapore singapore)))
    (pt_char_per .)))
</code></pre>
",""
"27801491","2015-01-06 15:11:15","4","","27801219","<p>The approach is good, in the last method you must mock the repo object and the repo response. In example try this code:</p>

<pre><code>public function test_whenSubmittedWithGoodData() {
        $formData = array(
            'name' =&gt; 'existing name',
        );

       $om = $this-&gt;getMockBuilder('Doctrine\Common\Persistence\ObjectManager')-&gt;getMock();

        $repoMock= $this-&gt;getMock('Doctrine\ORM\EntityRepository', array(), array(), '', false);


         $om
            -&gt;expects($this-&gt;atLeastOnce())
            -&gt;method('getRepository')
            -&gt;withAnyParameters()
            -&gt;will($this-&gt;returnValue($repoMock));


        $repoMock
            -&gt;expects($this-&gt;atLeastOnce())
            -&gt;method('findOneBy')
            -&gt;withAnyParameters()
            -&gt;will($this-&gt;returnValue($mockedBook));

        $type = new ItemType($om);
        $form = $this-&gt;factory-&gt;create($type);

        $form-&gt;submit($formData);
    }
</code></pre>
",""
"27774208","2015-01-05 06:03:17","1","","27773802","<p><strong>Edit:</strong>  I stand corrected, the OP is asking for word by word replacement rather than simple string replace ('become' -> 'become' rather than 'becoyou').  I guess a dict version might look like this, using the regex split method found on the comments of the accepted answer to  <a href=""https://stackoverflow.com/questions/367155/splitting-a-string-into-words-and-punctuation"">Splitting a string into words and punctuation</a>:</p>

<pre><code>import re

def clean_split(string_input):
    """""" 
    Split a string into its component tokens and return as list
    Treat spaces and punctuations, including in-word apostrophes as separate tokens

    &gt;&gt;&gt; clean_split(""it's a good day today!"")
    [""it"", ""'"", ""s"", "" "", ""a"", "" "", ""good"", "" "", ""day"", "" "", ""today"", ""!""]
    """"""
    return re.findall(r""[\w]+|[^\w]"", string_input)

with open('replace.txt' , 'r') as myr:
    replacements = dict(tuple(line.split()) for line in myr)

with open('mytext.txt', 'r') as myf:
    for line in myf:
        print ''.join(replacements.get(word, word) for word in clean_split(line)),
</code></pre>

<p>I am not competent to reason well about <code>re</code> efficiency, if someone points out glaring inefficiencies I would be most grateful.</p>

<p><strong>Edit 2:</strong> OK I was inserting spaces between words and punctuation, now <em>that's</em> fixed by treating spaces as tokens and doing a <code>''.join()</code> instead of a <code>' '.join()</code></p>
",""
"27768214","2015-01-04 17:21:59","1","","27766869","<p>I'm not really sure if this is what you are looking for because I don't know if the order of the patterns is always the same in rule1, rule2 and rule3, but try this:</p>

<pre><code>def function():

   sent_pos = [('increasing', 'VBG'), ('of', 'IN'), ('mutation', 'NN')]
   rule1 = [('', 'VBG'), ('', 'IN'), ('', 'NN')]
   rule2 = [('', 'NN'), ('', 'IN'), ('', 'NN')]
   rule3 = [('', 'ADJ'), ('', 'IN'), ('', 'NN')]

   rules = [rule1, rule2, rule3]

   for element in rules:
        counter = 0
        for i in range(len(sent_pos)):
            if sent_pos[i][1] == element[i][1]:
                counter += 1
        if counter == len(sent_pos):
            return True
   return False
</code></pre>
",""
"27768211","2015-01-04 17:21:45","6","","27676164","<p><code>nltk.Tree</code> is actually a subclass of the Python <code>list</code>, so you can access the children of any node <code>c</code> by <code>c[0]</code>, <code>c[1]</code>, <code>c[2]</code>, etc. Note that NLTK trees are not explicitly binary by design, so your notion of ""left"" and ""right"" might have to be enforced somewhere in a contract.</p>

<p>Assuming the tree is binary, you can access the left child of a node with <code>c[0]</code>, and the right with <code>c[1]</code>. For your second task:</p>

<blockquote>
  <p>But what I want to do is to extract the left-hand side of a production and gather right-hand side of all productions with the same left-hand side.</p>
</blockquote>

<p>If I understand correctly, you can traverse the tree and build up a <code>dict</code> as you go, where the keys are left-hand sides and the values are lists of possible right-hand productions. I'm not sure if <code>nltk.Tree</code> objects are hashable / immutable (if not, they wouldn't be usable as <code>dict</code> keys), but you could use the string form of the <code>Tree</code> objects as keys in any case.</p>
",""
"27659535","2014-12-26 16:35:15","2","","27659179","<p>A stem as returned by Porter Stemmer is not necessarily the base form of a verb, or a valid word at all. If you're looking for that, you need to look for a lemmatizer instead.</p>
",""
"27652972","2014-12-26 04:57:26","1","","27649735","<pre><code>VP-&gt;@V_N NP     [0.05]
VP-&gt;@V_N PP     [0.1]
@V_N-&gt;Verb NP   [1.0]
</code></pre>

<p>Because you need to get the same probability for the result by applying both the original and the converted set of rules.</p>
",""
"27522658","2014-12-17 09:49:44","0","","27522015","<p>In your example <code>st</code> is a str (list of bytes). To do that it was encoded in some form (utf8 by the looks), but think of it as a list of bytes, and you need to know how it was encoded in order to decode it (though utf8 is always generally a good first punt).</p>

<pre><code>&gt;&gt;&gt; st = ""they don‚Äôt serve sushi""
&gt;&gt;&gt; st
'they don\xe2\x80\x99t serve sushi'
&gt;&gt;&gt; type(st)
&lt;type 'str'&gt;

&gt;&gt;&gt; st.encode('utf8')
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 8: ordinal not in range(128)
</code></pre>

<p>So <code>st.encode</code> is non-sensical here. It's already encoded (as utf8 by the interpreter by the looks of things). For some mad reason, in python2 <code>str.encode</code> will first <code>decode</code> into a unicode and then <code>encode</code> back to a str. It chooses to decode as ascii by default, but your data is encoded as utf8. So the error you're seeing is in the decode step of your encode operation! It's looking at that list of bytes <code>e2,80,99</code> and saying - 'hmmm, those aren't real ascii characters'.</p>

<p>Let's start with unicode data instead (notice the u):</p>

<pre><code>&gt;&gt;&gt; st = u""they don‚Äôt serve sushi""
&gt;&gt;&gt; st
u'they don\u2019t serve sushi'
&gt;&gt;&gt; type(st)
&lt;type 'unicode'&gt;
&gt;&gt;&gt; st.encode('utf8')
'they don\xe2\x80\x99t serve sushi'
</code></pre>

<p>Really, all this is python2's fault. Python3 won't let you get away with these shenanigans of thinking of unicode and str as the same thing.</p>

<p>The rule of thumb is; always work with unicode within your code. Only encode/decode when you're getting data in and out of the system, and generally, encode as utf8 unless you have some other specific requirement.</p>

<p>In python2 you can ensure that <code>'data'</code> in your code is automatically unicode <code>u'data'</code></p>

<pre><code>from __future__ import unicode_literals

&gt;&gt;&gt; st = ""they don‚Äôt serve sushi""
&gt;&gt;&gt; st
u'they don\u2019t serve sushi'
&gt;&gt;&gt; type(st)
&lt;type 'unicode'&gt;
</code></pre>
",""
"27486368","2014-12-15 14:32:55","2","","27475658","<p>These are two separate subject areas but they do overlap in some places. Because documents, regardless of their format are made up of heterogeneous syntax and semantics, the goal is to represent information that is understandable to a machine and not just a human being. This is a common goal of the Semantic Web and Natural Language Processing.</p>
<p><strong>Semantic Web</strong></p>
<p>The semantic web is based on two fundamental ideas:</p>
<ul>
<li>Associating <strong>meta-information</strong> with Internet-based resources. Metadata is pieces of information about other data which can be added explicitly or implicitly.</li>
<li>The ability to reason about the meta-information. For example, a machine should be able to recognize that a picture of balloon is not an animal, even if it is shaped like one. This idea of reasoning and inference on textual data is still very experimental, however, showing considerable success. There is a range of techniques to query such information such as SPARQL, Machine Learning (a pre-annotated corpus), and other statistical techniques.</li>
</ul>
<p>The use of ontologies is becoming evermore important in this domain. Description Logic provides the mathematical foundation for knowledge representation systems and can be used to reason with the information.</p>
<p><strong>Natural Language Processing</strong></p>
<p>Whereas Natural Language Processing is an important and ongoing research area in theoretical computer science and artificial intelligence, it can look beyond the web and process anything from text in PDFs to speaking to your phone. Wikipedia has highlighted some of the key areas which I will expand on:</p>
<ol>
<li>Automatic (Abstractive &amp; Extractive) Summarisation</li>
<li>Coreference Resolution</li>
<li>Discourse Analysis</li>
<li>Language Translation</li>
<li>Morphological Segmentation</li>
<li>Named Entity Recognition</li>
<li>Natural Language Generation</li>
<li>Optical Character Recognition</li>
<li>Parsing</li>
<li>Question Answering</li>
<li>Relationship (Semantics) Extraction</li>
<li>Speech Segmentation</li>
<li>Speech &amp; Voice Recognition</li>
<li>Topic Segmentation</li>
<li>Word Sense Disambiguation (WSD)</li>
<li>Information Retrieval</li>
<li>Information Extraction</li>
<li>Text Simplification</li>
<li>Spelling Correction</li>
</ol>
<p>Both subject areas have been heavily researched into the syntactics of language, both research fields aim to understand language, notably text. However, in recent times the use of semantics has had a lot of time and investment put into it. But in essence, how to represent relationships in text and miscellaneous structures is a top priority of both fields of thought.</p>
<p><strong>Conclusion</strong></p>
<p>Semantic Web is mostly annotated with RDF, OWL, etc., whereas NLP really focuses on freeform Text.</p>
",""
"27468778","2014-12-14 11:19:53","0","","27467967","<p>This is a rough idea of what you want:</p>

<pre><code>l = ""i find my problem a bit hard to explain in English"".split()
n = 2
ln = len(l)
chnk = ln / n
srt = sorted(l, key=str.lower) # use str.lower as the key or uppercase will come before lower
chunks = (srt[i:chnk+i] for i in xrange(0, len(srt), chnk))

In [4]: l = ""i find my problem a bit hard to explain in English"".split()
In [5]: n = 2    
In [6]: ln = len(l)
In [7]: chnk = ln / n    
In [8]: srt = sorted(l, key=str.lower)
In [9]: chunks = (srt[i:chnk+i] for i in xrange(0, len(srt), chnk))    
In [10]:     
In [10]: for chunk in chunks:
   ....:         print(chunk)
   ....:     
['a', 'bit', 'English', 'explain', 'find']
['hard', 'i', 'in', 'my', 'problem']
['to']
</code></pre>

<p>Obviously you will have to handle the case when n chunks does not divide evenly into the length of your list of words.</p>
",""
"27326275","2014-12-05 23:21:22","0","","27311579","<p>There is a <a href=""https://stackoverflow.com/questions/5034772/how-safe-is-recursion-in-python"">How safe is recursion in Python?</a> which explains why recursion is a bad idea in Python. In other languages it's easier.</p>

<p>Luckily you can often convert recursion to a loop, as discussed on <a href=""https://softwareengineering.stackexchange.com/questions/182314/recursion-or-while-loops"">programmers.stackexchange</a>.</p>

<p>You can find recursion-free DTW example on <a href=""http://en.wikipedia.org/wiki/Dynamic_time_warping"" rel=""nofollow noreferrer"">Wikipedia</a>:</p>

<pre><code>int DTWDistance(s: array [1..n], t: array [1..m]) {
    DTW := array [0..n, 0..m]

    for i := 1 to n
        DTW[i, 0] := infinity
    for i := 1 to m
        DTW[0, i] := infinity
    DTW[0, 0] := 0

    for i := 1 to n
        for j := 1 to m
            cost:= d(s[i], t[j])
            DTW[i, j] := cost + minimum(DTW[i-1, j  ],    // insertion
                                        DTW[i  , j-1],    // deletion
                                        DTW[i-1, j-1])    // match

    return DTW[n, m]
}
</code></pre>
",""
"27294819","2014-12-04 13:05:31","0","","25741209","<p>Firstly, as a side note: What you're trying to do isn't typically called stemming or lemmatiziation.</p>

<p>Your first issue would be mapping the token observed (e.g. <em>—Å–æ–±–∞—á–∫–∞</em>) to its normalised form (e.g. <em>—Å–æ–±–∞–∫–∞</em>)-- Naively, this could be done by creating a <a href=""https://lucene.apache.org/core/4_10_2/analyzers-common/org/apache/lucene/analysis/synonym/SynonymFilter.html"" rel=""nofollow""><code>SynonymFilter</code></a> which uses a <a href=""https://lucene.apache.org/core/4_10_2/analyzers-common/org/apache/lucene/analysis/synonym/SynonymMap.html"" rel=""nofollow""><code>SynonymMap</code></a> mapping dimunitive forms to their canonical forms. However, you'll likely run into problems with any natural language because not all derivations are unambiguous: For example, in German, <em>M√§del</em> ('girl'/'lass') could be a diminutive form of <em>Magd</em> (an archaic word meaning 'young woman'/'maid') or of <em>Made</em> ('maggot').</p>

<p>One way of disambiguating these two forms would be to calculate the probability of each canonical form appearing in the given context (e.g. the history of the preceding <em>n</em> tokens) and then replacing the dimunitive form with the most probable canonical form (using a custom-made <code>TokenFilter</code> to do so)-- See e.g. <a href=""https://en.wikipedia.org/wiki/Word-sense_disambiguation"" rel=""nofollow"">the Wikipedia entry for word-sense disambiguation</a> for different approaches.</p>
",""
"27229930","2014-12-01 13:55:18","0","","27227111","<p>Why don't you try it out?</p>

<p>Here's an example:</p>

<pre><code>&gt;&gt;&gt; from nltk.stem import PorterStemmer
&gt;&gt;&gt; from nltk import word_tokenize, pos_tag
&gt;&gt;&gt; sent = ""This is a messed up sentence from the president's Orama and it's going to be sooo good, you're gonna laugh.""
</code></pre>

<p>This is the outcome of tokenizing.</p>

<pre><code>&gt;&gt;&gt; [word for word in word_tokenize(sent)]
['This', 'is', 'a', 'messed', 'up', 'sentence', 'from', 'the', 'president', ""'s"", 'Orama', 'and', 'it', ""'s"", 'going', 'to', 'be', 'sooo', 'good', ',', 'you', ""'re"", 'gon', 'na', 'laugh', '.']
</code></pre>

<p>This is the outcome of tokenize -> stem</p>

<pre><code>&gt;&gt;&gt; porter = PorterStemmer()
&gt;&gt;&gt; [porter.stem(word) for word in word_tokenize(sent)]
[u'Thi', u'is', u'a', u'mess', u'up', u'sentenc', u'from', u'the', u'presid', u""'s"", u'Orama', u'and', u'it', u""'s"", u'go', u'to', u'be', u'sooo', u'good', u',', u'you', u""'re"", u'gon', u'na', u'laugh', u'.']
</code></pre>

<p>This is the outcome of tokenize -> stem -> POS tag</p>

<pre><code>&gt;&gt;&gt; pos_tag([porter.stem(word) for word in word_tokenize(sent)])
[(u'Thi', 'NNP'), (u'is', 'VBZ'), (u'a', 'DT'), (u'mess', 'NN'), (u'up', 'RP'), (u'sentenc', 'NN'), (u'from', 'IN'), (u'the', 'DT'), (u'presid', 'JJ'), (u""'s"", 'POS'), (u'Orama', 'NNP'), (u'and', 'CC'), (u'it', 'PRP'), (u""'s"", 'VBZ'), (u'go', 'RB'), (u'to', 'TO'), (u'be', 'VB'), (u'sooo', 'RB'), (u'good', 'JJ'), (u',', ','), (u'you', 'PRP'), (u""'re"", 'VBP'), (u'gon', 'JJ'), (u'na', 'NN'), (u'laugh', 'IN'), (u'.', '.')]
</code></pre>

<p>This is the outcome of tokenize -> POS tag</p>

<pre><code>&gt;&gt;&gt; pos_tag([word for word in word_tokenize(sent)])
[('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('messed', 'VBN'), ('up', 'RP'), ('sentence', 'NN'), ('from', 'IN'), ('the', 'DT'), ('president', 'NN'), (""'s"", 'POS'), ('Orama', 'NNP'), ('and', 'CC'), ('it', 'PRP'), (""'s"", 'VBZ'), ('going', 'VBG'), ('to', 'TO'), ('be', 'VB'), ('sooo', 'RB'), ('good', 'JJ'), (',', ','), ('you', 'PRP'), (""'re"", 'VBP'), ('gon', 'JJ'), ('na', 'NN'), ('laugh', 'IN'), ('.', '.')]
</code></pre>

<p><strong>So what's the right way?</strong></p>
",""
"27175149","2014-11-27 16:28:20","2","","26689133","<p>For me it looks normal, it takes around 2 or 3 seconds to init. The <code>.tagger</code> is the same as the <code>.model</code>. </p>

<p>You can see it in the sample models provided by the Stanford Tagger :</p>

<pre><code>model = wsj-0-18-bidirectional-nodistsim.tagger
</code></pre>
",""
"27090896","2014-11-23 15:53:48","7","","27089678","<p>tf-idf  (it's not capitalized, fyi) is a good choice of weight. Your intuition is correct here. However, you don't compute tf-idf on your training set alone. Why? You need to really understand what the tf and idf mean:</p>

<p>tf (term frequency) is a statistic that indicates whether a term appears in the document being evaluated. The simplest way to calculate it would simply be a boolean value, i.e. 1 if the term is in the document.</p>

<p>idf (inverse document frequency), on the other hand, measures how likely a term appears in a random document. It's most often calculated as the log of (N/number of document matches).</p>

<p>Now, tf is calculated for each of the document your IR system will be indexing over (if you don't have the access to do this, then you have a much bigger and insurmountable problem, since an IR without a source of truth is an oxymoron). Ideally, idf is calculated over your entire data set (i.e. all the documents you are indexing), but if this is prohibitively expensive, then you can random sample your population to create a smaller data set, or use a training set such as the Brown corpus.</p>
",""
"26990838","2014-11-18 09:32:38","0","","26988686","<p>For grammatical dependencies (nsubj, poss...), you can read the official manual: <a href=""http://nlp.stanford.edu/software/dependencies_manual.pdf"">http://nlp.stanford.edu/software/dependencies_manual.pdf</a></p>

<p>Tags like NN, VBZ... are part-of-speech tags. You can find info about them here: <a href=""https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"">https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html</a>, or by googling ""part-of-speech tags penn treebank""</p>
",""
"26966351","2014-11-17 05:50:20","0","","26962725","<p>There is a CoNLL formatting option for CoreNLP output, but unfortunately it doesn't match what MaltParser expects. (Confusingly, there are several different common CoNLL data formats, for the different competition years..)</p>

<p>If you run CoreNLP from the command line with the option <code>-outputFormat conll</code>, you'll get output in the following TSV format (example output at end of answer):</p>

<pre><code>INDEX    WORD    LEMMA    POS    NER    DEPHEAD    DEPREL
</code></pre>

<p>MaltParser expects a bit different format, but you can customize the data input / output format. Try putting this content in <code>maltparser/appdata/dataformat/myconll.xml</code>:</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;dataformat name=""myconll"" reader=""tab"" writer=""tab""&gt;
    &lt;column name=""ID"" category=""INPUT"" type=""INTEGER""/&gt;
    &lt;column name=""FORM"" category=""INPUT"" type=""STRING""/&gt;
    &lt;column name=""LEMMA"" category=""INPUT"" type=""STRING""/&gt;
    &lt;column name=""POSTAG"" category=""INPUT"" type=""STRING""/&gt;
    &lt;column name=""NER"" category=""IGNORE"" type=""STRING""/&gt;
    &lt;column name=""HEAD"" category=""HEAD"" type=""INTEGER""/&gt;
    &lt;column name=""DEPREL"" category=""DEPENDENCY_EDGE_LABEL"" type=""STRING""/&gt;
&lt;/dataformat&gt;
</code></pre>

<p>Then add to your MaltParser config file (find an example config in <code>maltparser/examples/optionexample.xml</code>):</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;experiment&gt;
    &lt;optioncontainer&gt;
...
        &lt;optiongroup groupname=""input""&gt;
            &lt;option name=""format"" value=""myconll""/&gt;
        &lt;/optiongroup&gt;
    &lt;/optioncontainer&gt;
...
&lt;/experiment&gt;
</code></pre>

<p>Then you should be able to provide CoreNLP CoNLL output as training data to MaltParser.</p>

<p>Untested, but if the MaltParser docs are honest, this should work. Sources:</p>

<ul>
<li><a href=""http://www.maltparser.org/userguide.html#inout"">MaltParser user guide: I/O</a></li>
<li><a href=""http://www.maltparser.org/optiondesc.html#input-format"">MaltParser option documentation</a></li>
</ul>

<hr>

<p>Example CoreNLP CoNLL output (I only used annotators <code>tokenize,ssplit,pos</code>):</p>

<pre><code>$ echo ""This is a test."" | java edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos -outputFormat conll 2&gt;/dev/null

1   This    this    DT  _   _   _
2   is  be  VBZ _   _   _
3   a   a   DT  _   _   _
4   test    test    NN  _   _   _
5   .   .   .   _   _   _
</code></pre>
",""
"26922032","2014-11-14 02:39:33","7","","26921862","<p><code>VP|&lt;NP-PP&gt;</code> is <strong><em>one</em></strong> nonterminal symbol. The vertical bar does not mean multiple options in the traditional sense. Rather, NLTK puts it there to indicate where the rule is derived from, i.e. ""this new nonterminal symbol was derived from the combination of VP and NP-PP."" It is a new production rule NLTK has created to convert your grammar into Chomsky Normal Form.</p>

<p>Take a look at the productions of the tree, pre-CNF:</p>

<pre><code>ROOT -&gt; S
S -&gt; NP VP
NP -&gt; DT NNS
DT -&gt; 'the'
NNS -&gt; 'kids'
VP -&gt; VBD NP PP ***
VBD -&gt; 'opened'
NP -&gt; DT NN
DT -&gt; 'the'
NN -&gt; 'box'
PP -&gt; IN NP
IN -&gt; 'on'
NP -&gt; DT NN
DT -&gt; 'the'
NN -&gt; 'floor'
</code></pre>

<p>Specifically, look at the rule <code>VP -&gt; VBD NP PP</code>, which is NOT in CNF (There must be exactly two nonterminal symbols on the RHS of any production rule)</p>

<p>The two rules (7): <code>VP|&lt;NP-PP&gt; -&gt; NP PP</code> and (8): <code>VP -&gt; VBD VP|&lt;NP-PP&gt;</code> in your question are functionally equivalent to the more general rule <code>VP -&gt; VBD NP PP</code>.</p>

<p>When <code>VP</code> is detected, rule application results in:</p>

<p><code>VBD VP|&lt;NP-PP&gt;</code></p>

<p>And, <code>VP|&lt;NP-PP&gt;</code> is the LHS of the production rule created, which results in:</p>

<p><code>VBD NP PP</code></p>

<p>Specifically, if you isolate the rule itself, you can take a look at the specific symbol (which is indeed singular):</p>

<pre><code>&gt;&gt;&gt; tree.chomsky_normal_form()
&gt;&gt;&gt; prod = tree.productions()
&gt;&gt;&gt; x = prod[7]  # VP|&lt;NP-PP&gt; -&gt; NP PP
&gt;&gt;&gt; x.lhs().symbol()  # Singular!
u'VP|&lt;NP-PP&gt;'
</code></pre>
",""
"26921132","2014-11-14 00:42:18","2","","26921111","<p>A production rule with multiple options on the left-hand side of the production is no longer a Context Free Grammar - there must be only one nonterminal on the LHS of every rule.</p>

<p>Well, it doesn't really make any sense, in the first place, if you can split the rule</p>

<pre><code>VP | &lt;VBP-SBAR&gt; -&gt; VBP SBAR
</code></pre>

<p>into two rules</p>

<pre><code>VP -&gt; VBP SBAR
&lt;VBP-SBAR&gt; -&gt; VBP SBAR
</code></pre>
",""
"26868180","2014-11-11 15:19:30","0","","26868077","<p>I also tried to install that but got the same error. This is because library has compiled with JDK 1.8 but u may try to run it with a lower version than 1.8. To solve this </p>

<ol>
<li><p>download the latest jdk version from <a href=""http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html"" rel=""nofollow"">here</a></p></li>
<li><p>go to </p></li>
</ol>

<blockquote>
  <p>C:\Program Files\NetBeans 7.x\etc\netbeans.conf</p>
</blockquote>

<p>and Change the following line to point it where your java installation is :</p>

<blockquote>
  <p>netbeans_jdkhome=""C:\Program Files\Java\jdk1.8xxxxx""</p>
</blockquote>

<p>You may need Administrator privileges to edit netbeans.conf.Then Try to run the program.</p>
",""
"26786851","2014-11-06 18:34:02","0","","26780092","<p>CoreNLP outputs ""Collapsed dependencies preserving a tree structure"" (section 4.4 of the manual) from my experience. I think it's the same thing here (e.g. prep_with is a collapsed dependency of prep(saw, with))</p>
",""
"26729837","2014-11-04 07:32:28","0","","26729556","<p>Use the below code to remove <strong>nonascii</strong> from your corpus:</p>

<pre><code>ip=open(nonascii.txt,'r')
#Edit should be in w mode
op=open(ascii.txt,'w')
for line in ip:
        line=line.strip().decode(""ascii"",""ignore"").encode(""ascii"")
        if line=="""":continue
        op.write(line)
ip.close()
op.close()
</code></pre>
",""
"26654565","2014-10-30 13:39:50","0","","24990527","<p>You can use Petrov's universal tag set. The universal tag set is 12 in number and increases the POS tagging efficiency drastically. You can refer <a href=""http://www.petrovi.de/data/lrec.pdf"" rel=""nofollow"">Universal POS tagset</a> You can also download the code and the mappings for few taggers at <a href=""http://code.google.com/p/universal-pos-tags/downloads/list"" rel=""nofollow"">POS mapping</a></p>
",""
"26651130","2014-10-30 10:55:05","0","","26648986","<p>I discovered by myself. The problem is that nlkt send to java this format:</p>

<p><code>1   This    _   DT  DT  _   0   a   _   _</code></p>

<p>and return: <code>1   This    _   DT  DT  _   2   SUBJ   _   _</code></p>

<p>But in java the format is a little different, the last 2 <code>_</code> has to be removed. With that, it'll work!</p>

<p>input: <code>1   This    _   DT  DT  _</code></p>

<p>return: <code>1   This    _   DT  DT  _   2   SUBJ   _   _</code></p>

<p>I hope this help others.</p>
",""
"26465228","2014-10-20 12:01:58","1","","26450647","<p>I think the problem is with the append, try the following:</p>

<pre><code>...
X = tfidf.transform(freq_term_vector)
...
X_test = tfidf.transform(freq_term_vector_test)
clf.predict(X_test)
</code></pre>
",""
"26367962","2014-10-14 18:38:40","2","","26352041","<p>I had to rewrite:</p>

<pre><code>if hasattr(t, 'node') and t.node:
</code></pre>

<p>To:</p>

<pre><code>if hasattr(t, 'label'):
</code></pre>
",""
"26365936","2014-10-14 16:42:15","0","","26365791","<p>I'm not sure if there's a way to do this from within NLTK, but <code>collections.Counter</code> certainly has a way:</p>

<pre><code>import collections

pos_list = nltk.pos_tag(list)
pos_counts = collections.Counter((subl[1] for subl in pos_list))
print ""the five most common tags are"", pos_counts.most_common(5)
</code></pre>
",""
"25980612","2014-09-22 18:15:45","3","","25973351","<p>What you are seeking to do is to identify the independent clauses in a <a href=""http://grammar.about.com/od/c/g/compoundsent.htm"" rel=""nofollow noreferrer"">compound sentence</a>. A compound sentence is a sentence with at least two independent clauses joined by a <a href=""http://grammar.about.com/od/c/g/coordconjterm.htm"" rel=""nofollow noreferrer"">coordinating conjunction</a>. There is no readily available tool for this, but you can identify compound sentences with a high degree of precision by using <a href=""http://en.wikipedia.org/wiki/Parse_tree#Constituency-based_parse_trees"" rel=""nofollow noreferrer"">constituency parse trees</a>.</p>

<p>Be wary, though. Sligh grammatical mistakes can yield a very wrong parse tree! For example, if you use the Berkeley parser (demo page: <a href=""http://tomato.banatao.berkeley.edu:8080/parser/parser.html"" rel=""nofollow noreferrer"">http://tomato.banatao.berkeley.edu:8080/parser/parser.html</a>) on your first example, the parse tree is not what you would expect, but correct it to ""John is <em>an</em> actor and his father ... "", and you can see the parse tree neatly divided into the structure <code>S CC S</code>:</p>

<p><img src=""https://i.sstatic.net/2RS1l.png"" alt=""The Berkeley Parser&#39;s output on the first sentence""></p>

<p>Now, you simply take each sentence-label <code>S</code> as an independent clause!</p>

<p>Questions are not handled well, I am afraid, as you can check with your second example.</p>
",""
"25969784","2014-09-22 08:37:46","1","","25965417","<p>Take a look at the tutorial, it's a little outdated but the idea is there: <a href=""http://www.nltk.org/book/ch08.html"" rel=""nofollow noreferrer"">http://www.nltk.org/book/ch08.html</a></p>

<p>Then take a look at this question and answer: <a href=""https://stackoverflow.com/questions/15003136/cfg-using-pos-tags-in-nltk"">CFG using POS tags in NLTK</a></p>

<p>Lastly, here's an example:</p>

<pre><code>from nltk import parse_cfg, ChartParser

grammar_string = """"""
S -&gt; NP VP
NP -&gt; DT NN | NNP
VP -&gt; VB NP | VBS
VBS -&gt; 'sleeps'
VB -&gt; 'loves' | 'sleeps_with'
NNP -&gt; 'John' | 'Mary'
""""""

grammar = parse_cfg(grammar_string)
sentence = 'John loves Mary'.split()
parser = ChartParser(grammar)
print parser.parse(sentence)
</code></pre>

<p>[out]:</p>

<pre><code>(S (NP (NNP John)) (VP (VB loves) (NP (NNP Mary))))
</code></pre>
",""
"25899118","2014-09-17 19:48:23","5","","25893690","<p>Have you heard about <a href=""http://jeffreykegler.github.io/Ocean-of-Awareness-blog/individual/2011/11/what-is-the-marpa-algorithm.html"" rel=""nofollow"">Marpa</a>? </p>

<p>It's basically Earley + Aycock&amp;Horspool + Leo + author's (Jeffrey Kegler's) improvements. </p>

<p>You might be interested in <a href=""http://jeffreykegler.github.io/Ocean-of-Awareness-blog/metapages/annotated.html#THEORY"" rel=""nofollow"">Theory</a> section of the author's blog and <a href=""https://docs.google.com/file/d/0B9_mR_M2zOc4Ni1zSW5IYzk3TGc/edit?usp=sharing"" rel=""nofollow"">the author's paper about Marpa</a>. </p>

<p>Hope this helps.</p>
",""
"25841723","2014-09-15 06:05:43","0","","25817177","<p>After you have split the data into 60% and 40% You can do the following. This will require new tools, and perhaps not NLTK.  </p>

<pre><code>import random
import nltk
import pandas
import csv
import numpy as np
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn import metrics
from operator import itemgetter
from sklearn.metrics import classification_report
train_setT = [tuple(x) for x in train_set]
test_setT = [tuple(x) for x in test_set]


train_set = np.array([''.join(el[0]) for el in train_setT])
test_set = np.array([''.join(el[0]) for el in test_setT])

y_train = np.array([el[1] for el in train_setT])
y_test = np.array([el[1] for el in test_setT])

vectorizer = TfidfVectorizer(min_df=2,ngram_range=(1, 2), strip_accents='unicode', norm='l2')

X_train = vectorizer.fit_transform(train_set)
X_test = vectorizer.transform(test_set)

nb_classifier = MultinomialNB().fit(X_train, y_train)

y_nb_predicted = nb_classifier.predict(X_test)


print metrics.confusion_matrix(y_test, y_nb_predicted)
print classification_report(y_test, y_nb_predicted)
</code></pre>
",""
"25833720","2014-09-14 13:24:50","2","","25833693","<p>Don't use <code>if word in counts.keys()</code>
If you do that, you end up looking sequentially through the keys, which is what <code>dict</code> is supposed to avoid.</p>

<p>Just put <code>if word in counts</code>.</p>

<p>Or use a <code>defaultdict</code>.
<a href=""https://docs.python.org/2/library/collections.html#collections.defaultdict"">https://docs.python.org/2/library/collections.html#collections.defaultdict</a></p>
",""
"25791984","2014-09-11 15:52:09","0","","25789104","<p>What you describe is called <em>coreference resolution</em> as for the former problem (what does <em>his</em> refers to? John!) and <em>relation extraction</em> as for the latter (that is, <code>job(John, actor)</code>, <code>job(Abraham,doctor)</code>, and <code>father(John,Abraham)</code>).</p>

<p>There are tons of studies on these subjects. Hopefully, ACL Anthology is here to help :</p>

<ul>
<li><a href=""http://www.google.com/cse?cx=011664571474657673452%3A4w9swzkcxiy&amp;cof=FORID%3A0&amp;q=coreference#gsc.tab=0&amp;gsc.q=coreference&amp;gsc.page=1"" rel=""nofollow"">coreference resolution</a></li>
<li><a href=""http://www.google.com/cse?cx=011664571474657673452%3A4w9swzkcxiy&amp;cof=FORID%3A0&amp;q=relation%20extraction#gsc.tab=0&amp;gsc.q=relation%20extraction&amp;gsc.page=1"" rel=""nofollow"">relation extraction</a></li>
</ul>
",""
"25624092","2014-09-02 13:02:07","5","","25623171","<p>There isn't an ""and"" operator like that as such but you could do it with a set of <code>contains</code> checks:</p>

<pre><code>Rule: OrgTitlePer
({Sentence contains {Organization},
  Sentence contains {JobTitle},
  Sentence contains {Person}}):sent
--&gt;
:sent.Interesting = {}
</code></pre>

<p>When you have several constraints within the same set of braces that involve the same annotation type on the left (<code>Sentence</code> in this case) then all the constraints must be satisfied simultaneously <em>by the same annotation</em>.</p>
",""
"25577479","2014-08-29 23:26:35","0","","25575668","<p>I think that you need to have a look to this influential NLP library from Stanford University.</p>

<p><a href=""http://nlp.stanford.edu/software/tagger.shtml"" rel=""nofollow"">POS tagging Java library</a> </p>
",""
"25350663","2014-08-17 15:28:38","0","","25293346","<p>OK- the indentation of the code was critical, but I eliminated the messing append loops  and used Lamba instead:</p>

<pre><code>filtered_words = stopwords.words('english')
 tokenized_docs_no_stopwords = []

tokenized_docs_no_stopwords = filter(lambda x: x not in filtered_words,       
tokenized_docs_no_irishstopwords)
</code></pre>
",""
"25340657","2014-08-16 13:32:00","1","","25330079","<p>There's a training set and testing set from the chunking shared task of the CoNLL-2000 conference here:</p>

<p><a href=""http://www.cnts.ua.ac.be/conll2000/chunking/"" rel=""noreferrer"">http://www.cnts.ua.ac.be/conll2000/chunking/</a></p>

<p>Others have used this to train part-of-speech taggers:</p>

<p><a href=""https://code.google.com/p/miralium/wiki/PosTaggerTutorial"" rel=""noreferrer"">https://code.google.com/p/miralium/wiki/PosTaggerTutorial</a></p>
",""
"25275238","2014-08-12 22:37:46","0","","25269369","<p>What you might be looking for is an online classifier, which can be partially trained without keeping all of the training data in memory, and from my quick glance this isn't anything NLTK seems to offer. Instead, I would recommend one of the classifiers in scikit-learn which has the <code>partial_fit</code>¬†method implemented, <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html"" rel=""nofollow"">like this Naive Bayes classifier for multimodial models</a>.</p>
",""
"25168689","2014-08-06 19:34:26","6","","25145552","<p>Gensim has an efficient <a href=""http://radimrehurek.com/gensim/intro.html"" rel=""noreferrer"">tf-idf model</a> and does not need to have everything in memory at once.</p>

<p>Your corpus simply needs to be an iterable, so it does not need to have the whole corpus in memory at a time.</p>

<p>The <a href=""https://github.com/piskvorky/gensim/blob/develop/gensim/scripts/make_wikicorpus.py"" rel=""noreferrer"">make_wiki script</a> runs over Wikipedia in about 50m on a laptop according to the comments.</p>
",""
"25109586","2014-08-03 22:08:59","3","","25106997","<p>I hope this helps:</p>

<pre><code>from nltk.corpus import brown
from nltk import UnigramTagger as ut

# Let's just take the first 100 sentences.
sents = brown.tagged_sents()[:1000]
num_sents = len(sents)
k = 10
foldsize = num_sents/10

fold_accurracies = []

for i in range(10):
    # Locate the test set in the fold.
    test = sents[i*foldsize:i*foldsize+foldsize]
    # Use the rest of the sent not in test for training.
    train = sents[:i*foldsize] + sents[i*foldsize+foldsize:]
    # Trains a unigram tagger with the train data.
    tagger = ut(train)
    # Evaluate the accuracy using the test data.
    accuracy = tagger.evaluate(test)
    print ""Fold"", i 
    print 'from sent', i*foldsize, 'to', i*foldsize+foldsize
    print 'accuracy =', accuracy 
    print
    fold_accurracies.append(accuracy)

print 'average accuracy =', sum(fold_accurracies)/k
</code></pre>

<p>[out]:</p>

<pre><code>Fold 0
from sent 0 to 100
accuracy = 0.785714285714

Fold 1
from sent 100 to 200
accuracy = 0.745431364216

Fold 2
from sent 200 to 300
accuracy = 0.749628896586

Fold 3
from sent 300 to 400
accuracy = 0.743798291989

Fold 4
from sent 400 to 500
accuracy = 0.803448275862

Fold 5
from sent 500 to 600
accuracy = 0.779836277467

Fold 6
from sent 600 to 700
accuracy = 0.772676371781

Fold 7
from sent 700 to 800
accuracy = 0.755679184052

Fold 8
from sent 800 to 900
accuracy = 0.706402915148

Fold 9
from sent 900 to 1000
accuracy = 0.774622079707

average accuracy = 0.761723794252
</code></pre>
",""
"25109406","2014-08-03 21:44:10","3","","25108053","<p><code>NLTK</code> default pos tagger <code>pos_tag</code> is a MaxEnt tagger, see line 82 from <a href=""https://github.com/nltk/nltk/blob/develop/nltk/tag/__init__.py"" rel=""nofollow"">https://github.com/nltk/nltk/blob/develop/nltk/tag/<strong>init</strong>.py</a></p>

<pre><code>from nltk.corpus import brown
from nltk.data import load

sents = brown.tagged_sents()
# test on last 10% of brown corpus.
numtest = len(sents) / 10
testsents = sents[numtest:]

_POS_TAGGER = 'taggers/maxent_treebank_pos_tagger/english.pickle'

tagger = load(_POS_TAGGER)

print tagger.evaluate(testsents)
</code></pre>

<p>[out]:</p>
",""
"25005857","2014-07-28 23:20:30","1","","24975573","<p>nltk.RegexpParser <strong>can</strong> process custom tags.</p>

<p>Here is how you can modify your code to work:</p>

<pre><code># Import the RegexpParser
from nltk.chunk import RegexpParser

# Define your custom tagged data. 
tags = [(u'greatest', 'P'), (u'internet', 'NN'), (u'ever', 'A'), 
(u',', ','), (u'and', 'CC'), (u'its', 'PRP$'), (u'being', 'VBG'), 
(u'slow', 'N'), (u'as', 'IN'), (u'hell', 'NN')]

# Define your custom grammar (modified to be a valid regex).
grammar = """""" CHUNK: {&lt;A&gt;*&lt;P&gt;+} """"""

# Create an instance of your custom parser.
custom_tag_parser = RegexpParser(grammar)

# Parse!
custom_tag_parser.parse(tags)
</code></pre>

<p>This is the result you would get for your test data:</p>

<pre><code>Tree('S', [Tree('CHUNK', [(u'greatest', 'P')]), (u'internet', 'NN'), (u'ever', 'A'), (u',', ','), (u'and', 'CC'), (u'its', 'PRP$'), (u'being', 'VBG'), (u'slow', 'N'), (u'as', 'IN'), (u'hell', 'NN')])
</code></pre>
",""
"24741909","2014-07-14 17:05:55","3","","24741750","<p>How about </p>

<pre><code>instance Monad m =&gt; MonadEither l r (E.EitherT l m)
</code></pre>

<p>That is, it should be <code>l</code> instead of <code>r</code>.</p>

<p>However once you've done this you'll come up across a separate error.  The root cause is that there's no point to <code>right</code>; it's just <code>return</code>.  This means you need to get rid of the <code>r</code> parameter to the class.</p>

<pre><code>class Monad m =&gt; MonadEither l m where
    left  :: l -&gt; m a
</code></pre>

<p>Your instance declaration should then become</p>

<pre><code>instance Monad m =&gt; MonadEither l (E.EitherT l m)
</code></pre>

<p>(You also may want to look at the <a href=""http://hackage.haskell.org/package/mtl-1.1.0.2/docs/Control-Monad-Error-Class.html"" rel=""nofollow""><code>MonadError</code> class</a> since this is essentially what you are replicating.)</p>
",""
"24648116","2014-07-09 07:50:10","2","","24647400","<p>Python implementations of the Porter, Porter2, Paice-Husk, and Lovins stemming algorithms for English are available in the <a href=""https://pypi.python.org/pypi/stemming/1.0"" rel=""nofollow noreferrer"">stemming package</a></p>
",""
"24542218","2014-07-02 22:40:22","0","","24534699","<p>Use python wrapper for CoreNLP: 
<a href=""https://github.com/kedz/corenlp"" rel=""nofollow"">https://github.com/kedz/corenlp</a></p>
",""
"24383610","2014-06-24 09:55:43","0","","24382900","<p>Although the GATE data model allows arbitrary names for annotation types and features, the JAPE language can only handle types and features whose names are (more or less) valid Java identifiers - upper- and lower case letters, numbers and underscores, and not starting with a number (technically hyphens will work in some situations too, but not all). So it's best to stick to these restrictions if at all possible.</p>

<p>If you can't change the upstream component to generate an annotation type without the space the you may be able to use the renaming feature of the annotation set transfer PR to fix it post hoc.</p>
",""
"24369801","2014-06-23 15:31:42","0","","24363013","<p>Considering a second order HMM, Maximum Likelihood Estimate gives:</p>

<pre><code>P(SomeTag | &lt;BOS&gt;,&lt;BOS&gt;) = count(&lt;BOS&gt;,&lt;BOS&gt;,SomeTag) / count(&lt;BOS&gt;,&lt;BOS&gt;)
</code></pre>

<p>It corresponds to your second proposal: </p>

<blockquote>
  <p>(number of sentences beginning with one tag)/(all sentences)</p>
</blockquote>
",""
"24337089","2014-06-20 23:33:45","4","","24337031","<p>Word Sense Disambiguation (WSD) is the task in disambiguating a word given a context sentence/document. In the case, of a two token phrase, the context is basically the other token.</p>

<p>You can try out different WSD software and here's a list: <a href=""https://stackoverflow.com/questions/4613773/anyone-know-of-some-good-word-sense-disambiguation-software"">Anyone know of some good Word Sense Disambiguation software?</a></p>

<p>I'll give you an example using <code>pywsd</code> (<a href=""https://github.com/alvations/pywsd"" rel=""nofollow noreferrer"">https://github.com/alvations/pywsd</a>):</p>

<pre><code>$ wget https://github.com/alvations/pywsd/archive/master.zip
$ unzip master.zip
$ cd pywsd-master
$ python
Python 2.7.5+ (default, Feb 27 2014, 19:37:08) 
[GCC 4.8.1] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; from lesk import simple_lesk
# disambiguating the word 'bass' given the context 'bass music'
&gt;&gt;&gt; simple_lesk('bass music', 'bass') 
Synset('bass.n.07')
&gt;&gt;&gt; disambiguated = simple_lesk('bass music', 'bass')
&gt;&gt;&gt; disambiguated.definition
&lt;bound method Synset.definition of Synset('bass.n.07')&gt;
&gt;&gt;&gt; disambiguated.definition()
u'the member with the lowest range of a family of musical instruments
</code></pre>

<p>Alternatively, you can use a new module in <code>NLTK</code> (<a href=""https://github.com/nltk/nltk/blob/develop/nltk/wsd.py"" rel=""nofollow noreferrer"">https://github.com/nltk/nltk/blob/develop/nltk/wsd.py</a>), given that you have the bleeding edge version:</p>

<pre><code>from nltk.wsd import lesk
disambiguated = lesk(context_sentence=""bass music"", ambiguous_word=""bass"")
print disambiguated.definition()
</code></pre>

<p>(Disclaimer: I wrote both <code>pywsd</code> and the <code>lesk</code> module in <code>NLTK</code>)</p>
",""
"24316054","2014-06-19 20:55:41","0","","16325390","<p>It would be very difficult to do cleanly. The base parser classes rely on exact matches or the production RHS to pop content, so it would require subclassing and rewriting large parts of the parser class. I attempted it a while ago with the feature grammar class and gave up.</p>

<p>What I did instead is more of a hack, but basically, I extract the regex matches from the text first, and add them to the grammar as productions. It will be very slow if you are using a large grammar since it needs to recompute the grammar and parser for every call.</p>

<pre><code>import re

import nltk
from nltk.grammar import Nonterminal, Production, ContextFreeGrammar

grammar = nltk.parse_cfg (""""""
S -&gt; TEXT
TEXT -&gt; WORD | WORD TEXT | NUMBER | NUMBER TEXT
"""""")

productions = grammar.productions()

def literal_production(key, rhs):
    """""" Return a production &lt;key&gt; -&gt; n 

    :param key: symbol for lhs:
    :param rhs: string literal:
    """"""
    lhs = Nonterminal(key)
    return Production(lhs, [rhs])

def parse(text):
    """""" Parse some text.
""""""

    # extract new words and numbers
    words = set([match.group(0) for match in re.finditer(r""[a-zA-Z]+"", text)])
    numbers = set([match.group(0) for match in re.finditer(r""\d+"", text)])

    # Make a local copy of productions
    lproductions = list(productions)

    # Add a production for every words and number
    lproductions.extend([literal_production(""WORD"", word) for word in words])
    lproductions.extend([literal_production(""NUMBER"", number) for number in numbers])

    # Make a local copy of the grammar with extra productions
    lgrammar = ContextFreeGrammar(grammar.start(), lproductions)

    # Load grammar into a parser
    parser = nltk.RecursiveDescentParser(lgrammar)

    tokens = text.split()

    return parser.parse(tokens)

print parse(""foo hello world 123 foo"")
</code></pre>

<p>Here's more background where this was discussed on the nltk-users group on google groups: <a href=""https://groups.google.com/d/topic/nltk-users/4nC6J7DJcOc/discussion"" rel=""nofollow"">https://groups.google.com/d/topic/nltk-users/4nC6J7DJcOc/discussion</a></p>
",""
"24305819","2014-06-19 11:45:51","0","","24298677","<p>It seems like a textbook case of <a href=""http://en.wikipedia.org/wiki/Locality-sensitive_hashing"" rel=""nofollow noreferrer"">Locality Sensitive Hashing</a>.
Check out <a href=""https://stackoverflow.com/questions/12952729/how-to-understand-locality-sensitive-hashing"">this SO thread</a></p>
",""
"24106239","2014-06-08 12:38:46","0","","24105887","<p>This is not a bug, its <a href=""https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/feature_extraction/text.py"" rel=""noreferrer"">a feature</a></p>

<pre><code># log1p instead of log makes sure terms with zero idf don't get
# suppressed entirely
idf = np.log(float(n_samples) / df) + 1.0
</code></pre>

<p>This <code>+1</code> (as mentioned in the comment) is used to make idf normalizator <strong>weaker</strong>, otherwise, elements which occur in all the documents are completely removed (they have idf=0 so whole tfidf=0)</p>
",""
"23885178","2014-05-27 09:02:31","4","","23877375","<p>I think it really matters about what you want to solve with this. It depends on the task. </p>

<p>Essentially by lemmatization, you make the input space sparser, which can help if you don't have enough training data. </p>

<p>But since Word2Vec is fairly big, if you have big enough training data, lemmatization shouldn't gain you much. </p>

<p>Something more interesting is, how to do tokenization with respect to the existing diction of words-vectors inside the W2V (or anything else).  Like ""Good muffins cost $3.88\nin New York."" needs to be tokenized to ['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New York.'] Then you can replace it with its vectors from W2V. The challenge is that some tokenizers my tokenize ""New York"" as ['New' 'York'], which doesn't make much sense. (For example, NLTK is making this mistake <a href=""https://nltk.googlecode.com/svn/trunk/doc/howto/tokenize.html"" rel=""noreferrer"">https://nltk.googlecode.com/svn/trunk/doc/howto/tokenize.html</a>) This is a problem when you have many multi-word phrases.</p>
",""
"23836216","2014-05-23 18:33:13","0","","23813611","<p>This error is basically an out of memory error. It likely occurs because there are long stretches of text with no sentence terminating punctuation (periods, question marks), and so it has been and is trying to parse a huge list of words that it regards as a single sentence.</p>

<p>The parser in general tries to continue after a parse failure, but can't in this case because it both failed to create data structures for parsing a longer sentence and then failed to recreate the data structures it was using previously. So, you need to do something.</p>

<p>Choices are:</p>

<ul>
<li>Indicate sentence/short document boundaries yourself. This does <strong>not</strong> require loading the parser many times (and you should avoid that). From the command-line you can put each sentence in a file and give the parser many documents to parse and ask it to save them in different files (See the <code>-writeOutputFiles</code> option).</li>
<li>Alternatively (and perhaps better) you can do this keeping everything in one file by either making the sentences one per line, or using simple XML/SGML style tags surrounding each sentence and then to use the <code>-sentences newline</code> or <code>-parseInside ELEMENT</code>.</li>
<li>Or you can just avoid this problem by specifying a maximum sentence length. Longer things that are not sentence divided will be skipped. (This is great for runtime too!)  You can do this with <code>-maxLength 80</code>.</li>
<li>If you are writing your own program, you could catch this Exception and try to resume. But it will only be successful if sufficient memory is available, unless you take the steps in the earlier bullet points.</li>
</ul>
",""
"23735101","2014-05-19 10:13:19","2","","23657885","<p>To my knowledge none of the dictionaries used by MeCab (IPA, Jumandic, or Unidic) includes romaji transcription of words. And actually there is no need for that:</p>

<ol>
<li><p>There exist different transcription schemes (e.g. Hepburn, kunrei, 99 siki);</p></li>
<li><p>Information on the pronunciation of lexical units is already available (e.g. „Éâ„É¢).</p></li>
</ol>

<p>You have to write your own transcription routine... or look for an existing katakana-romaji transcription module (compatible with your transcription scheme)...</p>
",""
"23527690","2014-05-07 20:25:35","0","","23506732","<p>In short, no, you cannot do this sort of semantics with NLTK. And using Wordnet will simply not work because most sentences contain words that are not in the database. The current way to approximate sentential semantics involves distributional techniques (word space models).</p>

<p>If you are a python programmer, scikit-learn and Gensim give you the functionality you want by means of Latent Semantic Analysis (LSA, LSI) and Latent Dirichlet Allocation (LDA). See the answers to <a href=""https://stackoverflow.com/questions/6593030/what-are-some-good-ways-of-estimating-approximate-semantic-similarity-between"">this previous question</a>. In Java, I would suggest you to try the excellent <a href=""https://github.com/fozziethebeat/S-Space/"" rel=""nofollow noreferrer"">S-Space package</a>.</p>

<p>However, most models will give you a strictly word-based representation. Combining the semantics of words into larger structures is much more difficult, unless you assume that phrases and sentences are bags-of-words (and thus, missing the difference between e.g. <em>Mary loves Kate</em> and <em>Kate loves Mary</em>.</p>
",""
"23328299","2014-04-27 20:00:13","1","","23318769","<p>First let's look at the different stemmers/lemmatizer that <code>NLTK</code> has:</p>

<pre><code>&gt;&gt;&gt; from nltk import stem
&gt;&gt;&gt; lancaster = stem.lancaster.LancasterStemmer()
&gt;&gt;&gt; porter = stem.porter.PorterStemmer()
&gt;&gt;&gt; snowball = stem.snowball.EnglishStemmer()
&gt;&gt;&gt; wnl = stem.wordnet.WordNetLemmatizer()
&gt;&gt;&gt; word = ""cowardly""
&gt;&gt;&gt; lancaster.stem(word)
'coward'
&gt;&gt;&gt; porter.stem(word)
u'cowardli'
&gt;&gt;&gt; snowball.stem(word)
u'coward'
&gt;&gt;&gt; wnl.stem(word)
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
AttributeError: 'WordNetLemmatizer' object has no attribute 'stem'
&gt;&gt;&gt; wnl.lemmatize(word)
'cowardly'
</code></pre>

<p>Note: WordNetLemmatizer is not a stemmer, thus it outputs the lemmatize of <code>cowardly</code> and in this case it is the same word.</p>

<p>Seems like Porter stemmer is the only one that changes <code>cowardly -&gt; cowardli</code>, let's look at the code to see why it happens, see <a href=""http://www.nltk.org/_modules/nltk/stem/porter.html#PorterStemmer"" rel=""noreferrer"">http://www.nltk.org/_modules/nltk/stem/porter.html#PorterStemmer</a>.</p>

<p>It seems like this is the part that is the <code>ly -&gt; li</code>:</p>

<pre><code>def _step1c(self, word):
    """"""step1c() turns terminal y to i when there is another vowel in the stem.
    --NEW--: This has been modified from the original Porter algorithm so that y-&gt;i
    is only done when y is preceded by a consonant, but not if the stem
    is only a single consonant, i.e.

       (*c and not c) Y -&gt; I

    So 'happy' -&gt; 'happi', but
      'enjoy' -&gt; 'enjoy'  etc

    This is a much better rule. Formerly 'enjoy'-&gt;'enjoi' and 'enjoyment'-&gt;
    'enjoy'. Step 1c is perhaps done too soon; but with this modification that
    no longer really matters.

    Also, the removal of the vowelinstem(z) condition means that 'spy', 'fly',
    'try' ... stem to 'spi', 'fli', 'tri' and conflate with 'spied', 'tried',
    'flies' ...
    """"""
    if word[-1] == 'y' and len(word) &gt; 2 and self._cons(word, len(word) - 2):
        return word[:-1] + 'i'
    else:
        return word
</code></pre>
",""
"23320561","2014-04-27 07:24:48","1","","23319311","<p>You've got a large array argument in this function:</p>

<pre><code>func stringInArray(a string, list [214]string) bool{
    for _, b := range list{
        if b == a{
            return true;
        }
    }
    return false
}
</code></pre>

<p>The array of stopwords gets copied each time you call this function.</p>

<p>Mostly in Go, you should uses slices rather than arrays most of the time. Change the definition of this to be <code>list []string</code> and define <code>stopWords</code> as a slice rather than an array:</p>

<pre><code>stopWords := []string{
    ""and"", ""or"", ...
}
</code></pre>

<p>Probably an even better approach would be to build a map of the stopWords:</p>

<pre><code>isStopWord := map[string]bool{}
for _, sw := range stopWords {
    isStopWord[sw] = true
}
</code></pre>

<p>and then you can check if a word is a stopword quickly:</p>

<pre><code>if isStopWord[word] { ... }
</code></pre>
",""
"23230720","2014-04-22 21:39:45","0","","23230140","<p>This grammar is not LL(1) just because it has a left recursion on it. When you will try to build the parser table, you will came up with a conflict.</p>

<p><a href=""http://en.wikipedia.org/wiki/LL_parser"" rel=""nofollow"">http://en.wikipedia.org/wiki/LL_parser</a></p>
",""
"23181701","2014-04-20 11:55:41","0","","22930328","<p>Note : Just posting it as answer to help in case others face this issue in future </p>

<p>I finally found out what I did wrong.. it turned out to be a blunder.</p>

<p>Tagger file name  is not 'english-bidirectional-distim.tagger'
but 'english-bidirectional-distsim.tagger'.</p>
",""
"23099837","2014-04-16 04:12:14","8","","23086961","<p>Lemmatization should ideally return a <em>canonical form</em> (known as 'lemma' or 'headword') of a group of words. This canonical form, however, is not always what we intuitively expect. For example, you expect ""learning"" to be yield the lemma ""learn"". But the noun ""learning"" has the lemma ""learning"", while only the present continuous verb ""learning"" has the lemma ""learn"". In case of ambiguity, the lemmatizer should depend on information from the part-of-speech tag.</p>

<p>Well, that explains <em>machine learning</em>, but what about <em>big, bigger and biggest</em>?</p>

<p>Lemmatization depends on morphological analysis. The Stanford Morphology-class computes the base form of English words, by removing just inflections (not derivational morphology). That is, it only does noun plurals, pronoun case, and verb endings, and not things like comparative adjectives or derived nominals. It is based on a finite-state transducer implemented by John Carroll et al., written in flex. I couldn't find the original version, but a Java version seems to be <a href=""https://github.com/knowitall/morpha"" rel=""nofollow"">available here</a>.</p>

<p>That is why <em>biggest</em> will not yield <em>big</em>.</p>

<p>The WordNet lexical database resolves to the correct lemma, though. I have usually used WordNet for lemmatization tasks, and have found no major issues so far. Two other well known tools that handle your example correctly are</p>

<ol>
<li><a href=""http://cst.dk/online/lemmatiser/uk/"" rel=""nofollow"">CST Lemmatizer</a></li>
<li><a href=""http://morphadorner.northwestern.edu/morphadorner/lemmatizer/example/"" rel=""nofollow"">MorphAdorner</a></li>
</ol>
",""
"23005526","2014-04-11 06:54:22","3","","22999273","<p><code>WordNetLemmatizer</code> uses the <code>._morphy</code> function to access its a word's lemma; from <a href=""http://www.nltk.org/_modules/nltk/stem/wordnet.html"" rel=""nofollow"">http://www.nltk.org/_modules/nltk/stem/wordnet.html</a> and returns the possible lemmas with the minimum length.</p>

<pre><code>def lemmatize(self, word, pos=NOUN):
    lemmas = wordnet._morphy(word, pos)
    return min(lemmas, key=len) if lemmas else word
</code></pre>

<p>And the <code>._morphy</code> function apply rules iteratively to get a lemma; the rules keep reducing the length of the word and substituting the affixes with the <code>MORPHOLOGICAL_SUBSTITUTIONS</code>. then it sees whether there are other words that are shorter but the same as the reduced word:</p>

<pre><code>def _morphy(self, form, pos):
    # from jordanbg:
    # Given an original string x
    # 1. Apply rules once to the input to get y1, y2, y3, etc.
    # 2. Return all that are in the database
    # 3. If there are no matches, keep applying rules until you either
    #    find a match or you can't go any further

    exceptions = self._exception_map[pos]
    substitutions = self.MORPHOLOGICAL_SUBSTITUTIONS[pos]

    def apply_rules(forms):
        return [form[:-len(old)] + new
                for form in forms
                for old, new in substitutions
                if form.endswith(old)]

    def filter_forms(forms):
        result = []
        seen = set()
        for form in forms:
            if form in self._lemma_pos_offset_map:
                if pos in self._lemma_pos_offset_map[form]:
                    if form not in seen:
                        result.append(form)
                        seen.add(form)
        return result

    # 0. Check the exception lists
    if form in exceptions:
        return filter_forms([form] + exceptions[form])

    # 1. Apply rules once to the input to get y1, y2, y3, etc.
    forms = apply_rules([form])

    # 2. Return all that are in the database (and check the original too)
    results = filter_forms([form] + forms)
    if results:
        return results

    # 3. If there are no matches, keep applying rules until we find a match
    while forms:
        forms = apply_rules(forms)
        results = filter_forms(forms)
        if results:
            return results

    # Return an empty list if we can't find anything
    return []
</code></pre>

<p>However if the word is in the list of exceptions, it will return a fixed value kept in the <code>exceptions</code>, see <code>_load_exception_map</code> in <a href=""http://www.nltk.org/_modules/nltk/corpus/reader/wordnet.html"" rel=""nofollow"">http://www.nltk.org/_modules/nltk/corpus/reader/wordnet.html</a>:</p>

<pre><code>def _load_exception_map(self):
    # load the exception file data into memory
    for pos, suffix in self._FILEMAP.items():
        self._exception_map[pos] = {}
        for line in self.open('%s.exc' % suffix):
            terms = line.split()
            self._exception_map[pos][terms[0]] = terms[1:]
    self._exception_map[ADJ_SAT] = self._exception_map[ADJ]
</code></pre>

<p>Going back to your example, <code>worse</code> -> <code>bad</code> and <code>further</code> -> <code>far</code> CANNOT be achieved from the rules, thus it has to be from the exception list. Since it's an exception list, there are bound to be inconsistencies. </p>

<p>The exception list are kept in <code>~/nltk_data/corpora/wordnet/adv.exc</code> and <code>~/nltk_data/corpora/wordnet/adv.exc</code>.</p>

<p>From <code>adv.exc</code>:</p>

<pre><code>best well
better well
deeper deeply
farther far
further far
harder hard
hardest hard
</code></pre>

<p>From <code>adj.exc</code>:</p>

<pre><code>...
worldliest worldly
wormier wormy
wormiest wormy
worse bad
worst bad
worthier worthy
worthiest worthy
wrier wry
...
</code></pre>
",""
"22999045","2014-04-10 21:08:14","1","","22905919","<p>Yes it helps to mention you're working with German :)</p>

<p>A regex-based sentence detector with list of abbreviations can be found in <a href=""http://gate.ac.uk/"" rel=""nofollow noreferrer"">GATE</a>. It uses the three files located <a href=""http://sourceforge.net/p/gate/code/HEAD/tree/gate/trunk/plugins/ANNIE/resources/regex-splitter/"" rel=""nofollow noreferrer"">here</a>. The regular expressions are pretty simple:</p>

<pre><code>//more than 2 new lines
(?:[\u00A0\u2007\u202F\p{javaWhitespace}&amp;&amp;[^\n\r]])*(\n\r|\r\n|\n|\r)(?:(?:[\u00A0\u2007\u202F\p{javaWhitespace}&amp;&amp;[^\n\r]])*\1)+

//between 1 and 3 full stops
\.{1,3}""?

//up to 4 ! or ? in sequence
(!|\?){1,4}""?
</code></pre>

<p>The code that uses these 3 files can be found <a href=""http://sourceforge.net/p/gate/code/HEAD/tree/gate/trunk/src/main/gate/creole/splitter/RegexSentenceSplitter.java"" rel=""nofollow noreferrer"">here</a>.</p>

<p>I would enhance the regular expressions with what which could be found on the web, like <a href=""https://stackoverflow.com/questions/5620514/split-text-file-at-sentence-boundary"">this one</a>.</p>

<p>Then I would think of all the German translations of the words in the GATE list. If that's not enough, I would go through a few of these abbreviation lists: <a href=""http://www.retrobibliothek.de/retrobib/meyers/abkuerzungen.html"" rel=""nofollow noreferrer"">1</a>, <a href=""http://de.wikipedia.org/wiki/Portal:Abk%C3%BCrzungen/Gebr%C3%A4uchliche_Abk%C3%BCrzungen"" rel=""nofollow noreferrer"">2</a>, and create the list on my own.</p>

<p>EDIT:</p>

<p>If performance is so important, I wouldn't use the whole GATE for a sentence splitter - it would take time and memory to switch to their documents, create annotations, then parse them back, etc. </p>

<p>I think the best way for you is to get the code from RegexSentenceSplitter class (the <a href=""http://sourceforge.net/p/gate/code/HEAD/tree/gate/trunk/src/main/gate/creole/splitter/RegexSentenceSplitter.java"" rel=""nofollow noreferrer"">link above</a>) and adjust it to your context. </p>

<p>I think the code is too long to paste here. You should see the execute() method. In general it finds all matches for internal, external and blocking regular expressions, then iterates and uses only those internal and external, which don't overlap with any of the blocking.</p>

<p>Here are some fragments you should look at/reuse:</p>

<ul>
<li><p>How the files are parsed</p>

<pre><code>// for each line
if(patternString.length() &gt; 0) patternString.append(""|"");
patternString.append(""(?:"" + line + "")"");

//...
return Pattern.compile(patternString.toString());
</code></pre></li>
<li><p>In the execute method, how the blocking splits are filled:</p>

<pre><code>Matcher nonSplitMatcher = nonSplitsPattern.matcher(docText);
//store all non split locations in a list of pairs
List&lt;int[]&gt; nonSplits = new LinkedList&lt;int[]&gt;();
while(nonSplitMatcher.find()){
   nonSplits.add(new int[]{nonSplitMatcher.start(), nonSplitMatcher.end()});
}
</code></pre></li>
</ul>

<p>Also check the <strong>veto</strong> method which ""Checks whether a possible match is being vetoed by a non split match. A possible match is vetoed if it any nay overlap with a veto region.""</p>

<p>Hope this helps.</p>
",""
"22994954","2014-04-10 17:27:57","2","","22993796","<p>So here is a way to do it in R, using the Northwestern University lemmatizer, <a href=""http://morphadorner.northwestern.edu/morphadorner/lemmatizer/example/"" rel=""nofollow"">MorphAdorner</a>.</p>

<pre><code>lemmatize &lt;- function(wordlist) {
  get.lemma &lt;- function(word, url) {
    response &lt;- GET(url,query=list(spelling=word,standardize="""",
                                   wordClass="""",wordClass2="""",
                                   corpusConfig=""ncf"",    # Nineteenth Century Fiction
                                   media=""xml""))
    content &lt;- content(response,type=""text"")
    xml     &lt;- xmlInternalTreeParse(content)
    return(xmlValue(xml[""//lemma""][[1]]))    
  }
  require(httr)
  require(XML)
  url &lt;- ""http://devadorner.northwestern.edu/maserver/lemmatizer""
  return(sapply(wordlist,get.lemma,url=url))
}

words &lt;- c(""is"",""am"",""was"",""are"")
lemmatize(words)
#   is   am  was  are 
# ""be"" ""be"" ""be"" ""be"" 
</code></pre>

<p>As I suspect you are aware, correct lemmatization requires knowledge of the word class (part of speech), contextually correct spelling, and also depends upon which corpus is being used.</p>
",""
"22863065","2014-04-04 12:38:51","0","","22854710","<p>Please spend more time on reading about POS tagging and spell checking. Two are <strong>VERY</strong> different things. Answer to your question is <strong>NO</strong></p>
",""
"22850033","2014-04-03 22:16:02","5","","22849919","<p>It seems like your double loop could be improved:</p>

<pre><code>for word in mycorp.words(categories=""Blogs""):
    for token in my_wordlist.words():
        if token == word:
            counter[token]+=1
</code></pre>

<p>This would be much faster as:</p>

<pre><code>words = set(my_wordlist.words()) # call once, make set for fast check
for word in mycorp.words(categories=""Blogs""):
    if word in words:
        counter[word] += 1
</code></pre>

<p>This takes you from doing <code>len(my_wordlist.words()) * len(mycorp.words(...))</code> operations to closer to <code>len(my_wordlist.words()) + len(mycorp.words(...))</code> operations, as building the set is <code>O(n)</code> and checking whether a word is in the set is <code>O(1)</code> on average. </p>

<p>You can also build the <code>Counter</code> direct from an iterable, as Two-Bit Alchemist points out:</p>

<pre><code>counter = Counter(word for word in mycorp.words(categories=""Blogs"") 
                  if word in words)
</code></pre>
",""
"22671887","2014-03-26 20:04:28","2","","22507623","<p><strong>Freeling</strong> is written in C++ too, although most people just use their binaries to run the tools: <a href=""http://devel.cpl.upc.edu/freeling/downloads?order=time&amp;desc=1"" rel=""noreferrer"">http://devel.cpl.upc.edu/freeling/downloads?order=time&amp;desc=1</a></p>

<p>Try something like <a href=""https://github.com/clab/dynet"" rel=""noreferrer"">DyNet</a>, it's a generic neural net framework but most of its processes are focusing on NLP because the maintainers are creators of the NLP community.</p>

<p>Or perhaps <a href=""https://github.com/marian-nmt/marian"" rel=""noreferrer"">Marian-NMT</a>, it was designed for sequence-to-sequence model machine translation but potentially many NLP tasks can be structured as a sequence-to-sequence task.</p>

<hr>

<h1>Outdated</h1>

<p>Maybe you can try <strong>Ellogon</strong> <a href=""http://www.ellogon.org/"" rel=""noreferrer"">http://www.ellogon.org/</a> , they have GUI support and also C/C++ API for NLP too.</p>
",""
"22544071","2014-03-20 20:13:52","1","","22544012","<p>Use a list comprehension. </p>

<p>To get an array of the names:</p>

<pre><code>names = [name for name, tag in person]
</code></pre>

<p>To output a string in the format you give:</p>

<pre><code># Python 2 (print is a statement)
print ""\n"".join([name for name, tag in person])

# Python 3 (print is a function)
print(""\n"".join([name for name, tag in person]))
</code></pre>

<p>This is really a basic Python data structure question - it's not specific to NLTK. You might find an introductory guide like <a href=""http://docs.python.org/2/tutorial/introduction.html"" rel=""nofollow"">An informal guide to Python</a> useful.</p>
",""
"22358166","2014-03-12 16:40:21","0","","22333392","<p>I found the solution! 
I checked the files in wordnet.py the folder /usr/local/lib/python2.6/dist-packages/nltk/corpus/reader and i noticed that the function _morphy(self,form,pos) returns a list containing stemmed words.
So i tried to test _morphy :</p>

<pre><code>import nltk
from nltk.corpus import wordnet as wn
from nltk.stem.wordnet import WordNetLemmatizer

words_raw = ""men teeth books""
words = nltk.word_tokenize(words_raw)
for word in words:
        print wn._morphy(word, wn.NOUN)
</code></pre>

<p>This program prints [men,man], [teeth,tooth] and [book]!</p>

<p>the explanation of why lmtzr.lemmatize () prints only the first element of the list, perhaps it can be found in the function lemmatize, contained in the file 'wordnet.py' which is in the folder /usr/local/lib/python2.6/dist-packages/nltk/stem.</p>

<pre><code>def lemmatize(self, word, pos=NOUN):
    lemmas = wordnet._morphy(word, pos)
    return min(lemmas, key=len) if lemmas else word
</code></pre>

<p>I assume that it returns only the shorter word contained in the word list, and if the two words are of equal length it returns the first one; for example 'men' or 'teeth'rather than 'man' and 'tooth'</p>
",""
"22220624","2014-03-06 09:47:00","1","","22213418","<p>I can think of 2 options:</p>

<ol>
<li>(easy) Let the tagger do its magic and then overwrite its output. If you know <code>st316</code> must be tagged as X and Stanford failed to tag it as such, change the tag of <code>st316</code> to X. The disadvantage of this approach is that the tagger is not able to use that information to better tag the rest of the sentence.</li>
<li>(harder) <a href=""http://nlp.stanford.edu/software/pos-tagger-faq.shtml#train"" rel=""nofollow"">Retrain</a> the PoS tagger, adding the extra information you have to its training data. This way it will actually learn from the information you provide and will be able to make use of it. The drawback is you will need to obtain some training data and (depending on how much data you get) it may take a while to train a new model.</li>
</ol>

<p>If you go with option 2, you need to format your data as follows:</p>

<pre>
An_DT avocet_NN is_VBZ a_DT small_JJ ,_, cute_JJ bird_NN ._.
I_PRP am_VBP st316_NNP ._.
I_PRP am_VBP st316_NNP ._.
I_PRP am_VBP st316_NNP ._.
I_PRP am_VBP st316_NNP ._.
I_PRP am_VBP st316_NNP ._.
</pre>

<p>The first line is taken from the Stanford FAQ. The rest is your extra knowledge. Note the one extra sentence is repeated. This is in order to add pseudo-counts to that observation. Informally, if you only included <code>st316_NNP</code> once in the training data chances are the tagger will think it is noise/error and ignore it. Repeating is is like saying ""Yes, I am sure, I know what I'm doing, learn from that data"". Depending on how much data you have, you will need anywhere between 5 and 50 repetitions to ensure the tagger learns properly.</p>
",""
"22158711","2014-03-03 22:18:39","0","","22156698","<p>The Stanford NLP pipeline contains the class <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/parser/lexparser/ExhaustiveDependencyParser.html"" rel=""nofollow"">ExhaustiveDependencyParser</a> as well as the interface <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/parser/ViterbiParser.html"" rel=""nofollow"">ViterbiParser</a>, both of which have methods from which you can get a <code>Tree</code> object. This, in turn, has a <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/trees/Tree.html#depth%28edu.stanford.nlp.trees.Tree%29"" rel=""nofollow""><code>depth(Tree)</code></a> method. Using this method with the root node as argument should suffice.</p>
",""
"22146747","2014-03-03 12:24:56","3","","22144324","<p>Try <code>pattern</code> library from CLIPS, they have support for German, English, Spanish, French and Italian. Just what you needed: <a href=""http://www.clips.ua.ac.be/pattern"" rel=""noreferrer"">http://www.clips.ua.ac.be/pattern</a></p>

<p>Unfortunately it only works with Python 2, no support for Python3 provided yet.</p>
",""
"21988533","2014-02-24 12:59:09","0","","21883108","<p>Some attempts with some profiling. I thought using generators could improve the speed here. But the improvement was not noticeable compared to a slight modification of the original. But if you don't need the full list at the same time, the generator functions should be faster.</p>

<pre><code>import timeit
from itertools import tee, izip, islice

def isplit(source, sep):
    sepsize = len(sep)
    start = 0
    while True:
        idx = source.find(sep, start)
        if idx == -1:
            yield source[start:]
            return
        yield source[start:idx]
        start = idx + sepsize

def pairwise(iterable, n=2):
    return izip(*(islice(it, pos, None) for pos, it in enumerate(tee(iterable, n))))

def zipngram(text, n=2):
    return zip(*[text.split()[i:] for i in range(n)])

def zipngram2(text, n=2):
    words = text.split()
    return pairwise(words, n)


def zipngram3(text, n=2):
    words = text.split()
    return zip(*[words[i:] for i in range(n)])

def zipngram4(text, n=2):
    words = isplit(text, ' ')
    return pairwise(words, n)


s = ""Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.""
s = s * 10 ** 3

res = []
for n in range(15):

    a = timeit.timeit('zipngram(s, n)', 'from __main__ import zipngram, s, n', number=100)
    b = timeit.timeit('list(zipngram2(s, n))', 'from __main__ import zipngram2, s, n', number=100)
    c = timeit.timeit('zipngram3(s, n)', 'from __main__ import zipngram3, s, n', number=100)
    d = timeit.timeit('list(zipngram4(s, n))', 'from __main__ import zipngram4, s, n', number=100)

    res.append((a, b, c, d))

a, b, c, d = zip(*res)

import matplotlib.pyplot as plt

plt.plot(a, label=""zipngram"")
plt.plot(b, label=""zipngram2"")
plt.plot(c, label=""zipngram3"")
plt.plot(d, label=""zipngram4"")
plt.legend(loc=0)
plt.show()
</code></pre>

<p>For this test data, zipngram2 and zipngram3 seems to be the fastest by a good margin.</p>

<p><img src=""https://i.sstatic.net/QlRE0.png"" alt=""enter image description here""></p>
",""
"21911062","2014-02-20 14:42:40","0","","21909165","<p>You will first need the postagger.jar file from stanford and also train your own tagger. BUT the <code>hazm</code> dev has kindly uploaded the resource directory that you will need here: <a href=""http://dl.dropboxusercontent.com/u/90405495/resources.zip"">http://dl.dropboxusercontent.com/u/90405495/resources.zip</a></p>

<p>You will need to unzip and save the folder to the directory where you're running your script. </p>

<p>For example:</p>

<pre><code>$ mkdir testdir
$ wget https://github.com/sobhe/hazm/archive/master.zip
$ unzip master.zip -d testdir
$ cd testdir
$ mv hazm-master/hazm/ .
$ wget http://dl.dropboxusercontent.com/u/90405495/resources.zip
$ unzip resources.zip -d .
$ python
Python 2.7.5+ (default, Sep 19 2013, 13:48:49) 
[GCC 4.8.1] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; import hazm
&gt;&gt;&gt; tagger = hazm.POSTagger()
&gt;&gt;&gt; tagger.tag(hazm.word_tokenize(u'ŸÖÿß ÿ®ÿ≥€åÿßÿ± ⁄©ÿ™ÿßÿ® ŸÖ€å‚ÄåÿÆŸàÿßŸÜ€åŸÖ'))
[(u'\u0645\u0627', u'PR'), (u'\u0628\u0633\u06cc\u0627\u0631', u'ADV'), (u'\u06a9\u062a\u0627\u0628', u'N'), (u'\u0645\u06cc\u200c\u062e\u0648\u0627\u0646\u06cc\u0645', u'V')]
</code></pre>
",""
"21882522","2014-02-19 13:51:47","2","","21882460","<p>You can extract only the tags you want with a list comprehension, e.g.:</p>

<pre><code>&gt;&gt;&gt; tags = nltk.pos_tag(tokens)
&gt;&gt;&gt; dt_tags = [t for t in tags if t[1] == ""DT""]
&gt;&gt;&gt; dt_tags
[('a', 'DT')]
</code></pre>
",""
"21874616","2014-02-19 08:14:22","0","","21871374","<h1>SharpNLP</h1>

<p>The best tool for natural language processing implemented in c# is <a href=""http://sharpnlp.codeplex.com/"" rel=""noreferrer"">SharpNLP</a>. </p>

<p>SharpNLP is a C# port of the Java OpenNLP tools, plus additional code to facilitate natural language processing.</p>

<h1>Embedding IronPython and NLTK</h1>

<p>Python provides a package <a href=""http://www.nltk.org/"" rel=""noreferrer"">NLTK</a> (Natural Language Toolkit) used widely by many computational linguists, NLP researchers.</p>

<p>One can try to embed <a href=""http://ironpython.net/"" rel=""noreferrer"">IronPython</a> under C# and run NLTK from there.</p>

<p>You can check the following <a href=""http://blog.samibadawi.com/2010/03/open-source-nlp-in-c-35-using-nltk.html"" rel=""noreferrer"">link</a> on how to do it.</p>
",""
"21818229","2014-02-16 23:25:58","0","","21818128","<p>One standard way to determine similarity of two articles is create a <a href=""http://en.wikipedia.org/wiki/Language_model"" rel=""nofollow"">language model</a> for each of them, and then find the similarity between them.</p>

<p>The language model is usually a probability function, assuming the article was created by a model that randomly selects tokens (words/bigrams/.../ngrams).</p>

<p>The simplest language model is for unigrams (words): <code>P(word|d) = #occurances(w,d)/|d|</code> (the number of times the word appeared in the document, relative to the total length of the document). Smoothing techniques are often used to prevent words having zero probability to appear.</p>

<p>After you have a language model, all you have to do is compare the two models. One way to do it is <a href=""http://en.wikipedia.org/wiki/Cosine_similarity"" rel=""nofollow"">cosine similarity</a> or <a href=""http://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence"" rel=""nofollow"">Jensen-Shannon similarity</a>.
<br>This gives you an absolute score of similarity of two articles. This can be combined with many other methods, like your suggestion to compare dates.</p>
",""
"21816269","2014-02-16 20:39:42","0","","21815475","<p>The weight of a term (as assigned by an SVM classifier) may or may not be directly proportional to the relevance of that term to a particular class. This depends on the kernel of the classifier as well as the regularization used. SVM does <em>NOT</em> assign weights to terms that best characterize a single document.</p>

<p>Term-frequency (tf) and inverse document frequency (idf) are used to encode the value of a term in a document vector. This is independent of the SVM classifier. </p>
",""
"21759006","2014-02-13 15:51:26","0","","15687440","<p>I have implemented a formula substitution algorithm on top of the Stanford tagger and it works quite nice. The way to go is, as abecadel has written, to replace every formula with a unique but new word, I used a combination of a word and a hash 'formula-duwkziah'.</p>
",""
"21725416","2014-02-12 10:37:21","0","","21718632","<p>It would be trivial to write yourself. As you said, NLTK can give you all pronouns, and you can easily write a function to return a detailed morphological analysis by simple string matching. Just pass the things tagged as pronouns to said function.</p>

<p>Pronouns are so-called ""closed class"" words, i.e. there's a finite (and small) amount of them. There is no way (in the English language) to generate <em>new</em> pronouns. Nouns, on the other hand, are ""open class"" i.e. you can can generate new nouns (compound nouns, such as ""beaver teeth"" or what have you.)</p>

<p>So you just use something like <code>f(x) = if x == ""I"" || x == ""me"" || x == ""my"" then return ""1st. person singular""</code> ... or some such. Obviously you would not want to return a string, but something more informative, such as an object with fields <code>number</code> and <code>person</code>, and possibly <code>possessive</code> or even <code>case</code>.</p>

<p>One more thing: since NLTK is a statistical tagger, it will sometimes tag as pronouns words that simply aren't. In this case you should be prepared to return an error condition.</p>
",""
"21559513","2014-02-04 17:38:23","1","","21548667","<p>Maluuba's Natural Language API may be helpful for you <a href=""http://dev.maluuba.com/"" rel=""nofollow"">http://dev.maluuba.com/</a></p>

<p>It extracts important keywords and intents from sentences and returns them in an easily read format.</p>

<p>For the phrase ""23 july cinema in Paris"" the response is:</p>

<pre><code>{
    entities: {
        dateRange: [
            {
                start: ""2014-07-23"",
                end: ""2014-07-24""
            }
        ],
        location: [
            ""paris""
        ],
        theatre: [
            ""cinema""
        ]
    },
    action: ""ENTERTAINMENT_MOVIE"",
    category: ""ENTERTAINMENT""
}
</code></pre>

<p>It would be up to you to connect this to a movie data provider that has films for Paris.</p>
",""
"21388258","2014-01-27 18:03:29","0","","21387489","<p>Depending on the internal data structure of the algorithm and how the data can be processed (incrementally or all in memory) it will need more memory or not. So the memory you will need depends on the algorithm.</p>

<p>So sparse representation is easier for you because it is compact, but, as fas as I know, the algorithm will need the same amount of memory to create the model from the same dataset. The input should format be transparent to the algorithm.</p>
",""
"21371453","2014-01-27 01:06:57","3","","21371416","<p>Why don't you download the StackOverflow data dumps and write a program to filter the tags?</p>
<p>They just have been released on archive.org, see <a href=""https://blog.stackoverflow.com/2014/01/stack-exchange-cc-data-now-hosted-by-the-internet-archive/"">here</a></p>
<p>Of course, it would not include all terms and there would be some false positives, but I assume this is about as close as you will get.</p>
",""
"21361619","2014-01-26 09:04:55","0","","21357881","<p>CoreNLP out-of-the-box will be restricted to types they mention : PERSON, LOCATION, ORGANIZATION, MISC, DATE, TIME, MONEY, NUMBER. No, you won't be able to recognize other entities just by assuming it could ""intuitively"" do it :)</p>

<p>In practice, you'll have to choose, either:</p>

<ol>
<li>Find another NER systems that tags those types</li>
<li>Address this tagging task using knowledge-based / unsupervised approaches.</li>
<li>Search for extra resources (corpora) that contain types you want recognize, and re-train a supervised NER system (CoreNLP or other)</li>
<li>Build (and possibly annotate) your own resources - then you'll have to define an annotation scheme, rules, etc. - quite an interesting part of the work!</li>
</ol>

<p>Indeed, unless you find an existing system that fulfills your needs, some effort will be required! Unsupervised approaches may help you bootstrapping a system, so as to see if you need to find / annotate a dedicated corpus. In the latter case, it would be better to separate data as train/dev/test parts, so as to be able to assess how much the resulting system performs on unseen data.</p>
",""
"21295143","2014-01-22 22:13:41","6","","21294694","<p>I do not know the arabic language, it may be specific in many aspects, my answer regards english.</p>

<blockquote>
  <p>Um I thought that a stemmer/lemmatizer was always used before text classifications, why does he say that it degrades the results?</p>
</blockquote>

<p>No it is not, in entirely depends on the <strong>task</strong>. If you want to extract some general concept of the text, then stemming/lematization is a good step. But in analysis of short chunks, where each word is valuable, stemming simply destroys its meaning. In particular - in sentiment analysis stemming may destroy the sentiment of the word. </p>
",""
"21222127","2014-01-19 20:18:48","0","","21208568","<p>Whether you use parse trees or not, you will need to use a <a href=""http://en.wikipedia.org/wiki/Markov_model"" rel=""nofollow noreferrer"">Markov process</a> to check validity. The features can be word sequences, part-of-speech tag sequences, parse tree segments (i.e. production rules and their extensions), etc. For these, you would use a <a href=""http://en.wikipedia.org/wiki/Tokenization"" rel=""nofollow noreferrer"">tokenizer</a>, a <a href=""http://en.wikipedia.org/wiki/Part-of-speech_tagging"" rel=""nofollow noreferrer"">POS tagger</a> and a <a href=""http://en.wikipedia.org/wiki/Parsing#Computational_methods"" rel=""nofollow noreferrer"">natural language parser</a>, respectively.</p>

<p>The validity check will also be a probabilistic score, not an absolute truth. All (or almost all) natural language parsers are statistical. Which means they require training data. These parsers use <a href=""http://en.wikipedia.org/wiki/PCFG"" rel=""nofollow noreferrer"">context-free grammars</a> or <a href=""http://en.wikipedia.org/wiki/Mildly_context-sensitive_language"" rel=""nofollow noreferrer"">mildly context-sensitive grammars</a> such as CCG or TAG, which are among the best computational approximations of natural language grammars.</p>

<p>Essentially, the model will tell you <em>how likely is it for a feature to appear in a valid sentence after a certain sequence of features has already been seen</em>. That is, it will allow you to compute probabilities of the form <code>P(""at""|""am working"")</code> and <code>P(""at""|""home am"")</code>. The former should have a higher probability than the latter. You will need to experimentally determine how high a probability should be in order for a sentence to be considered as valid.</p>

<p>As <a href=""https://stackoverflow.com/users/388827/qqilihq"">qqlihq</a> commented, these are under the broad definition of <a href=""http://en.wikipedia.org/wiki/Language_model"" rel=""nofollow noreferrer"">language models</a>. For sentence validity, however, you will usually not need to measure perplexity. The conditional probability measures should suffice.</p>
",""
"21170792","2014-01-16 19:14:07","3","","21170349","<p>There's a couple of ways to read a directory of textfiles. </p>

<p>Let's try the native python way first, from the terminal/console/command prompt:</p>

<pre><code>~$ mkdir ~/testcorpora
~$ cd ~/testcorpora/
~/testcorpora$ ls
~/testcorpora$ echo 'this is a foo foo bar bar.\n bar foo, dah dah.' &gt; somefoobar.txt
~/testcorpora$ echo 'what are you talking about?' &gt; talkingabout.txt
~/testcorpora$ ls
somefoobar.txt  talkingabout.txt
~/testcorpora$ cd ..
~$ python
&gt;&gt;&gt; import os
&gt;&gt;&gt; from nltk.tokenize import word_tokenize
&gt;&gt;&gt; from nltk.tag import pos_tag
&gt;&gt;&gt; corpus_directory = 'testcorpora/'
&gt;&gt;&gt; for infile in os.listdir(corpus_directory):
...     with open(corpus_directory+infile, 'r') as fin:
...             pos_tag(word_tokenize(fin.read()))
... 
[('what', 'WP'), ('are', 'VBP'), ('you', 'PRP'), ('talking', 'VBG'), ('about', 'IN'), ('?', '.')]
[('this', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('foo', 'NN'), ('foo', 'NN'), ('bar', 'NN'), ('bar.\\n', 'NN'), ('bar', 'NN'), ('foo', 'NN'), (',', ','), ('dah', 'NN'), ('dah', 'NN'), ('.', '.')]
</code></pre>

<p>The other solution is using <code>PlaintextCorpusReader</code> in NLTK, then run <code>word_tokenize</code> and <code>pos_tag</code> on the corpus see <a href=""https://stackoverflow.com/questions/4951751/creating-a-new-corpus-with-nltk"">Creating a new corpus with NLTK</a>:</p>

<pre><code>&gt;&gt;&gt; from nltk.corpus.reader.plaintext import PlaintextCorpusReader
&gt;&gt;&gt; from nltk.tag import pos_tag
&gt;&gt;&gt; corpusdir = 'testcorpora/'
&gt;&gt;&gt; newcorpus = PlaintextCorpusReader(corpusdir,'.*')
&gt;&gt;&gt; dir(newcorpus)
['CorpusView', '__class__', '__delattr__', '__dict__', '__doc__', '__format__', '__getattribute__', '__hash__', '__init__', '__module__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_encoding', '_fileids', '_get_root', '_para_block_reader', '_read_para_block', '_read_sent_block', '_read_word_block', '_root', '_sent_tokenizer', '_tag_mapping_function', '_word_tokenizer', 'abspath', 'abspaths', 'encoding', 'fileids', 'open', 'paras', 'raw', 'readme', 'root', 'sents', 'words']
# POS tagging all the words in all text files at the same time.
&gt;&gt;&gt; newcorpus.words()
['this', 'is', 'a', 'foo', 'foo', 'bar', 'bar', '.\\', ...]
&gt;&gt;&gt; pos_tag(newcorpus.words())
[('this', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('foo', 'NN'), ('foo', 'NN'), ('bar', 'NN'), ('bar', 'NN'), ('.\\', ':'), ('n', 'NN'), ('bar', 'NN'), ('foo', 'NN'), (',', ','), ('dah', 'NN'), ('dah', 'NN'), ('.', '.'), ('what', 'WP'), ('are', 'VBP'), ('you', 'PRP'), ('talking', 'VBG'), ('about', 'IN'), ('?', '.')]
</code></pre>
",""
"21009773","2014-01-09 00:56:57","0","","20985604","<p>Here is how it is done with completely manual creation of the <code>List</code> discussed in <a href=""http://nlp.stanford.edu/downloads/parser-faq.shtml#f"" rel=""nofollow"">the FAQ</a>:</p>

<pre><code>String[] sent3 = { ""It"", ""can"", ""can"", ""it"", ""."" };
// Parser gets second ""can"" wrong without help (parsing it as modal MD)
String[] tag3 = { ""PRP"", ""MD"", ""VB"", ""PRP"", ""."" };                                                 
List&lt;TaggedWord&gt; sentence3 = new ArrayList&lt;TaggedWord&gt;();
for (int i = 0; i &lt; sent3.length; i++) {
  sentence3.add(new TaggedWord(sent3[i], tag3[i]));
}
Tree parse = lp.parse(sentence3);
parse.pennPrint();
</code></pre>
",""
"20726559","2013-12-22 05:30:23","3","","20726214","<p>Note that as Edward says, it would generally a lot simpler to put the <code>ErrorT</code> at the top of the stack, not the bottom.</p>

<p>This can change the semantics of the stack, at least for more complicated transformers than <code>ReaderT</code> - e.g. if you have <code>StateT</code> in the stack, then with <code>ErrorT</code> at the bottom changes to the state will be rolled back when there's an error, whereas with <code>ErrorT</code> at the top, changes to the state will be kept when there's an error.</p>

<p>If you do really need it at the bottom, then something like this passes the type checker:</p>

<pre><code>import Control.Monad.Error
import Control.Monad.Morph
import System.IO

toOuter :: MFunctor t =&gt; t (ErrorT String IO) a -&gt; t IO a
toOuter = hoist runErrorTWithPrint

runErrorTWithPrint :: ErrorT String IO a -&gt; IO a
runErrorTWithPrint m = do
   res &lt;- runErrorT m
   case res of
       Left err -&gt; do
           hPutStrLn stderr err
           fail err
       Right v -&gt; return v
</code></pre>

<p>Note that it calls <code>fail</code> when the inner computation fails, which isn't what your code above does.</p>

<p>The main reason is that to use <code>hoist</code> we need to provide a function of type <code>forall a . ErrorT String IO a -&gt; IO a</code> - i.e. to handle any kind of value, not just <code>()</code>. This is because the depending on the rest of the monad stack might mean that the actual return type when you get to the <code>ErrorT</code> is different to the return type you started with.</p>

<p>In the failure case, we don't have a value of type <code>a</code> so one option is to fail.</p>

<p>In your original code you also loop infinitely in <code>outer</code>, which this doesn't do.</p>
",""
"20455545","2013-12-08 16:10:06","0","","20332762","<p>I was unable to find a tagged corpus to use with NLTK.  If you require a pre-tagged corpus you may be out of luck with NLTK.  There is an open issue ticket for this very issue, but there has been no progress (<a href=""https://github.com/nltk/nltk/issues/137"" rel=""nofollow"">Reading Negra Corpus Files</a>)</p>

<p>You could tag your own corpus using the <a href=""http://nltk-trainer.readthedocs.org/en/latest/"" rel=""nofollow"">NLTK Trainer</a> and the <a href=""http://www.coli.uni-saarland.de/projects/sfb378/negra-corpus/negra-corpus.html"" rel=""nofollow"">Negra Corpus</a>.  It would require knowledge of german grammar but no coding.  See demonstration of the <a href=""http://text-processing.com/demo/tag/"" rel=""nofollow"">NLTK-Trainer</a>.  </p>
",""
"20332548","2013-12-02 16:05:13","2","","17325554","<p>There is a very detailed explanation of the difference between precision and accuracy on wikipedia (see <a href=""https://en.wikipedia.org/wiki/Accuracy_and_precision"" rel=""noreferrer"">https://en.wikipedia.org/wiki/Accuracy_and_precision</a>), in brief:</p>

<pre><code>accuracy = (tp + tn) / (tp + tn + fp + fn)
precision = tp / tp + fp
</code></pre>

<p>Back to NLTK, there is a module call <a href=""http://nltk.googlecode.com/svn-/trunk/doc/api/nltk.chunk.util-pysrc.html#ChunkScore"" rel=""noreferrer"">ChunkScore</a> that computes the <code>accuracy</code>, <code>precision</code> and <code>recall</code> of your system. And here's the funny part the way NLTK calculates the <code>tp,fp,tn,fn</code> for <code>accuracy</code> and <code>precision</code>, it does at different granularity. </p>

<p>For <strong>accuracy</strong>, NLTK calculates the total number of tokens (<strong>NOT CHUNKS!!</strong>) that are guessed correctly with the POS tags and IOB tags, then divided by the total number of tokens in the gold sentence.</p>

<pre><code>accuracy = num_tokens_correct / total_num_tokens_from_gold
</code></pre>

<p>For <strong>precision</strong> and <strong>recall</strong>, NLTK calculates the:</p>

<ul>
<li><code>True Positives</code> by counting the number of chunks (<strong>NOT TOKENS!!!</strong>) that are guessed correctly</li>
<li><code>False Positives</code> by counting the number of chunks (<strong>NOT TOKENS!!!</strong>) that are guessed but they are wrong.</li>
<li><code>True Negatives</code> by counting the number of chunks (<strong>NOT TOKENS!!!</strong>) that are not guessed by the system.</li>
</ul>

<p>And then calculates the precision and recall as such:</p>

<pre><code>precision = tp / fp + tp
recall = tp / fn + tp
</code></pre>

<p>To prove the above points, try this script:</p>

<pre><code>from nltk.chunk import *
from nltk.chunk.util import *
from nltk.chunk.regexp import *
from nltk import Tree
from nltk.tag import pos_tag

# Let's say we give it a rule that says anything with a [DT NN] is an NP
chunk_rule = ChunkRule(""&lt;DT&gt;?&lt;NN.*&gt;"", ""DT+NN* or NN* chunk"")
chunk_parser = RegexpChunkParser([chunk_rule], chunk_node='NP')

# Let's say our test sentence is:
# ""The cat sat on the mat the big dog chewed.""
gold = tagstr2tree(""[ The/DT cat/NN ] sat/VBD on/IN [ the/DT mat/NN ] [ the/DT big/JJ dog/NN ] chewed/VBD ./."")

# We POS tag the sentence and then chunk with our rule-based chunker.
test = pos_tag('The cat sat on the mat the big dog chewed .'.split())
chunked = chunk_parser.parse(test)

# Then we calculate the score.
chunkscore = ChunkScore()
chunkscore.score(gold, chunked)
chunkscore._updateMeasures()

# Our rule-based chunker says these are chunks.
chunkscore.guessed()

# Total number of tokens from test sentence. i.e.
# The/DT , cat/NN , on/IN , sat/VBD, the/DT , mat/NN , 
# the/DT , big/JJ , dog/NN , chewed/VBD , ./.
total = chunkscore._tags_total
# Number of tokens that are guessed correctly, i.e.
# The/DT , cat/NN , on/IN , the/DT , mat/NN , chewed/VBD , ./.
correct = chunkscore._tags_correct
print ""Is correct/total == accuracy ?"", chunkscore.accuracy() == (correct/total)
print correct, '/', total, '=', chunkscore.accuracy()
print ""##############""

print ""Correct chunk(s):"" # i.e. True Positive.
correct_chunks = set(chunkscore.correct()).intersection(set(chunkscore.guessed()))
##print correct_chunks
print ""Number of correct chunks = tp = "", len(correct_chunks)
assert len(correct_chunks) == chunkscore._tp_num
print

print ""Missed chunk(s):"" # i.e. False Negative.
##print chunkscore.missed()
print ""Number of missed chunks = fn = "", len(chunkscore.missed())
assert len(chunkscore.missed()) == chunkscore._fn_num
print 

print ""Wrongly guessed chunk(s):"" # i.e. False positive.
wrong_chunks = set(chunkscore.guessed()).difference(set(chunkscore.correct()))
##print wrong_chunks
print ""Number of wrong chunks = fp ="", len(wrong_chunks)
print chunkscore._fp_num
assert len(wrong_chunks) == chunkscore._fp_num
print 

print ""Recall = "", ""tp/fn+tp ="", len(correct_chunks), '/', len(correct_chunks)+len(chunkscore.missed()),'=', chunkscore.recall()

print ""Precision ="", ""tp/fp+tp ="", len(correct_chunks), '/', len(correct_chunks)+len(wrong_chunks), '=', chunkscore.precision()
</code></pre>
",""
"20084516","2013-11-19 23:37:18","0","","20075754","<p>Your <code>xml</code> data (or <code>html</code>) is not well formed.</p>

<p>Assuming an input file with following <code>xml</code> data well formed:</p>

<pre><code>&lt;root&gt;&lt;a&gt;annotated &lt;b&gt;piece&lt;/b&gt;&lt;/a&gt; of &lt;c&gt;text&lt;/c&gt;&lt;/root&gt;
</code></pre>

<p>you can use a <code>sax</code> parser to <code>append</code> and <code>pop</code> tags at start and end element events:</p>

<pre><code>from xml.sax import make_parser
from xml.sax.handler import ContentHandler
import sys 

class Xml2PseudoJson(ContentHandler):

    def __init__(self):
        self.tags = []
        self.chars = []
        self.json = []

    def startElement(self, tag, attrs):
        d = {''.join(self.chars): self.tags[:]}
        self.json.append(d)
        self.tags.append(tag)
        self.chars = []

    def endElement(self, tag):
        d = {''.join(self.chars): self.tags[:]}
        self.chars = []
        self.tags.pop()
        self.json.append(d)

    def characters(self, content):
        self.chars.append(content)

    def endDocument(self):
        print(list(filter(lambda x: '' not in x, self.json)))

parser = make_parser()
handler = Xml2PseudoJson()
parser.setContentHandler(handler)
parser.parse(open(sys.argv[1]))
</code></pre>

<p>Run it like:</p>

<pre><code>python3 script.py xmlfile
</code></pre>

<p>That yields:</p>

<pre><code>[
    {'annotated ': ['root', 'a']}, 
    {'piece': ['root', 'a', 'b']}, 
    {' of ': ['root']}, 
    {'text': ['root', 'c']}
]
</code></pre>
",""
"19960755","2013-11-13 17:45:03","2","","19957656","<p><code>echo cmd/tree-tagger-french-utf8</code> will print the string <code>cmd/tree-tagger-french-utf8</code> and you're piping that to a par file. That cannot work, you need to pipe a file to a command, like in your second example but using <code>cat filename</code> (instead of <code>echo 'Bonjour'</code>) if you want to feed a file to the TreeTagger.</p>
",""
"19689616","2013-10-30 17:18:36","2","","19689557","<p>You can style it like any other document if you include the reference to the CSS file:</p>

<pre><code>&lt;?xml-stylesheet href=""my-style.css""?&gt;
... rest of document here...
</code></pre>

<p><a href=""http://www.w3.org/Style/styling-XML.en.html"" rel=""nofollow"">Taken from the w3 documentation.</a></p>

<p><strong>EDIT</strong></p>

<p>If you wish to style by attribute, CSS has this capability:</p>

<pre><code>*[type=""NNS""] {
    color:red;
}
</code></pre>

<p>or if you wanted to be specific to the tag as well:</p>

<pre><code>w[type=""NNS""] {
    color:red;
}
</code></pre>

<p><a href=""https://developer.mozilla.org/en-US/docs/Web/CSS/Attribute_selectors"" rel=""nofollow"">Here is a bit more documentation on CSS using attribute selectors.</a></p>
",""
"19659698","2013-10-29 13:33:23","1","","18416561","<p>I found a very simple way to do POS tagging in Scala</p>

<p><strong>Step 1</strong></p>

<p>Download stanford tagger version 3.2.0 form the link below</p>

<p><a href=""http://nlp.stanford.edu/software/stanford-postagger-2013-06-20.zip"" rel=""nofollow"">http://nlp.stanford.edu/software/stanford-postagger-2013-06-20.zip</a></p>

<p><strong>Step 2</strong></p>

<p>Add <strong>stanford-postagger</strong> jar present in the folder to your project and also place the <strong>english-left3words-distsim.tagger</strong> file present in the models folder in your project</p>

<p>Then, with the code below you can pos tag a sentence in Scala</p>

<pre><code>              val tagger = new MaxentTagger(
                ""english-left3words-distsim.tagger"")
              val art_con = ""My name is Rahul""
              val tagged = tagger.tagString(art_con)
              println(tagged)
</code></pre>

<p><strong>Output:</strong> My_PRP$ name_NN is_VBZ Rahul_NNP</p>
",""
"19512810","2013-10-22 08:31:44","3","","19495967","<p>You can get tense information from the various penn tags:</p>

<pre><code>27. VB  Verb, base form
28. VBD Verb, past tense
29. VBG Verb, gerund or present participle
30. VBN Verb, past participle
31. VBP Verb, non-3rd person singular present
32. VBZ Verb, 3rd person singular present
</code></pre>

<p>About the active/passive aspect, you can use typed dependencies included in Stanford Core NLP.</p>

<ol>
<li>If the sentence is in active voice, a 'nsubj' dependecy should exist.</li>
<li>If the sentence is in passive voice a 'nsubjpass' dependency should
exist</li>
</ol>

<p>Hope this helps.</p>
",""
"19402157","2013-10-16 11:25:39","0","","19326278","<pre><code>docs = []

for subtree in result.subtrees(filter=lambda t: t.node == 'Proper'):
    docs.append("" "".join([a for (a,b) in subtree.leaves()]))

print docs
</code></pre>

<p>This should do the trick.</p>
",""
"19272635","2013-10-09 12:54:24","3","","19270759","<p>I think you are mixing two different concepts here. </p>

<ol>
<li><p>Cosine similarity measures the angle between two different vectors in a Euclidean space, independently of how the weights have been calculated.</p></li>
<li><p>TF-IDF decides, for each term in a document and a given collection, the weights for each one of the components of a vector that can be used for cosine similarity (among other things).</p></li>
</ol>

<p>I hope this helps.</p>
",""
"19262710","2013-10-09 03:44:34","1","","19262597","<p>There are three principal reasons.</p>

<p>First, as Gabe says - people have figured out through trial and error that programming in things that are close to English sentences only forces programmers to type more useless cruft. (And yes, COBOL was explicitly designed to read more ""naturally"".)</p>

<p>To a programmer,</p>

<pre><code>windows++
</code></pre>

<p>is more readable than</p>

<pre><code>You should now increment the number of windows by one.
</code></pre>

<p>For example, Tetris is a rather easy game to code. I would be terribly surprised if you managed to make an English explanation that is detailed enough for a computer (remember, computers are dumb, so you have to spell it all out) in less pages than a short novel.</p>

<p>The second reason is that the range of things a computer knows how to do is rather small, so the number of language constructs that are needed for that is also limited. In contrast, natural languages need to be able to express the entirety of human experience, which does require many language constructs to pull off. For example, ""According to his wife, John would have caught the fish yesterday if it hadn't rained"" is not expressible in C - and does not need to be.</p>

<p>And third is, indeed, ambiguity, as you yourself note. There are a lot of places where a software error is simply not permissible. People do enough bugs in unambiguous languages; allowing ambiguity would be a disaster waiting to happen. And on the same subject, we are still unable to parse human language sufficiently well - state of the art parsers still have unacceptably high error rates.</p>
",""
"19000710","2013-09-25 09:09:44","1","","18984722","<p>You might need to specially specify them as terminal notes, for e.g. :</p>

<pre><code>&gt;&gt;&gt; import nltk
&gt;&gt;&gt; grammar = nltk.parse_cfg(""""""
... S -&gt; NP VP
... VP -&gt; V PUNCT
... PUNCT -&gt; '.'
... V -&gt; 'eat'
... NP -&gt; 'I'
... """""")
&gt;&gt;&gt; 
&gt;&gt;&gt; sentence = ""I eat ."".split()
&gt;&gt;&gt; cp = nltk.ChartParser(grammar)
&gt;&gt;&gt; for tree in cp.nbest_parse(sentence):
...     print tree
... 
(S (NP I) (VP (V eat) (PUNCT .)))
</code></pre>
",""
"18949018","2013-09-22 21:38:32","1","","18948712","<p>When it comes to the CONLL format, i presume you mean the CONLL2000 chunking task format as such:</p>

<pre><code>   He        PRP  B-NP
   reckons   VBZ  B-VP
   the       DT   B-NP
   current   JJ   I-NP
   account   NN   I-NP
   deficit   NN   I-NP
   will      MD   B-VP
   narrow    VB   I-VP
   to        TO   B-PP
   only      RB   B-NP
   #         #    I-NP
   1.8       CD   I-NP
   billion   CD   I-NP
   in        IN   B-PP
   September NNP  B-NP
   .         .    O
</code></pre>

<p>There are three columns in the CONLL chunking task format:</p>

<ol>
<li><code>token</code> (i.e. word)</li>
<li><code>POS</code> tag</li>
<li><code>BIO</code> (begin, inside, outside) of chunk/phrase tag</li>
</ol>

<p>Sadly, if you use the stanford MaxEnt tagger, it <strong>only give you the <code>token</code> and <code>POS</code> information but has no <code>BIO</code> chunk information</strong>. </p>

<pre><code>java -cp stanford-postagger.jar edu.stanford.nlp.tagger.maxent.MaxentTagger -model models/left3words-wsj-0-18.tagger -textFile short.txt -outputFormat tsv 2&gt; /dev/null
</code></pre>

<p>Using the above command the Stanford POS tagger already give you the tab separated format, just that it's without the 3rd column (see <a href=""http://nlp.stanford.edu/software/pos-tagger-faq.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/pos-tagger-faq.shtml</a>):</p>

<pre><code>   He        PRP
   reckons   VBZ
   the       DT
   ...
</code></pre>

<p><strong>To get the <code>BIO</code> colum, you would require either</strong>: </p>

<ul>
<li>a <strong>statistical chunker</strong> or</li>
<li>a <strong>full parser</strong></li>
</ul>

<p>see <a href=""http://www-nlp.stanford.edu/links/statnlp.html"" rel=""nofollow"">http://www-nlp.stanford.edu/links/statnlp.html</a> for a list of chunker/parser, if you want to stick with stanford tools, i suggest the stanford parser but it gives you the bracketed parse format, which you have to do some post-processing to get it into CONLL2000 format, see <a href=""http://nlp.stanford.edu/software/lex-parser.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/lex-parser.shtml</a></p>
",""
"18871643","2013-09-18 11:57:15","0","","18871249","<p>In general NLP this problem is a subset of the <a href=""http://en.wikipedia.org/wiki/Named-entity_recognition"" rel=""nofollow"">NER (Named Entity Recognition)</a> problem. It can be approached in at least two ways:</p>

<ul>
<li>rule based system - so you build simple rules, like the dictinary check-up, regexps for some Mr., Phd. prefixes etc. and base your extraction on those</li>
<li>machine learning based systems - you characterize each word token with some lexical and statistical properties and train some classifier (like for example <a href=""http://acl.ldc.upenn.edu/acl2002/MAIN/pdfs/Main036.pdf"" rel=""nofollow"">HMM</a> or <a href=""http://nlp.stanford.edu/software/CRF-NER.shtml"" rel=""nofollow"">CRF</a>) to detect whether the particular word (token) is a first or last name.</li>
</ul>
",""
"18855602","2013-09-17 16:56:07","0","","18840537","<p><a href=""https://stackoverflow.com/questions/18391602/what-does-generate-do-when-using-nltk-in-python"">NLTK's generate() function</a> may be what you're looking for.</p>
<p>From <a href=""http://nltk.org/api/nltk.html?highlight=generate#nltk.text.Text.generate"" rel=""nofollow noreferrer"">the docs</a>:</p>
<blockquote>
<p><code>generate(length=100)</code></p>
<p>Print random text, generated using a
trigram language model.</p>
<p>Parameters:</p>
<p>length (int) ‚Äì The length of text to generate (default=100)</p>
</blockquote>
",""
"18706698","2013-09-09 20:43:02","2","","18705778","<p>Take a look at the explanation at the <a href=""http://www.nltk.org/howto/wordnet.html"" rel=""noreferrer"">NLTK howto for wordnet.</a></p>

<p>Specifically, the *_ic notation is <em>information content</em>. </p>

<blockquote>
  <p>synset1.res_similarity(synset2, ic): Resnik Similarity: Return a score
  denoting how similar two word senses are, based on the Information
  Content (IC) of the Least Common Subsumer (most specific ancestor
  node). Note that for any similarity measure that uses information
  content, the result is dependent on the corpus used to generate the
  information content and the specifics of how the information content
  was created.</p>
</blockquote>

<p>A bit more info on <em>information content</em> from <a href=""http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.59.2199"" rel=""noreferrer"">here</a>:</p>

<blockquote>
  <p>The conventional way of measuring the IC of word senses is to combine
  knowledge of their hierarchical structure from an ontology like
  WordNet with statistics on their actual usage in text as derived from
  a large corpus</p>
</blockquote>
",""
"18540406","2013-08-30 19:43:54","0","","16251708","<p><strong>I found my error.</strong></p>

<p>I didn't have the latest version of openopt, funcdesigner etc.</p>

<p>I uninstalled Openopt - first from the software center and then using pip to uninstall openopt, funcdesigner, DerApproximator and SpaceFuncs. (easy_install pip and run the commands:""sudo pip uninstall SpaceFuncs"" etc. <a href=""https://stackoverflow.com/questions/1231688/how-do-i-remove-packages-installed-with-pythons-easy-install"">pip installation and use</a>) </p>

<p>I then reinstalled everything with easyinstall and now it works.</p>
",""
"18527584","2013-08-30 07:45:05","0","","18496925","<p>Since you have a lot of training data (I assume you have a lot of pairs title + structured json specification), I would try to train a <a href=""http://en.wikipedia.org/wiki/Named-entity_recognition"" rel=""noreferrer"">Named Entity Recognizer</a>. </p>

<p>For example, you can train the <a href=""http://nlp.stanford.edu/software/CRF-NER.shtml"" rel=""noreferrer"">Stanford NER</a>. See this <a href=""http://nlp.stanford.edu/software/crf-faq.shtml#a"" rel=""noreferrer"">FAQ entry</a> explaining how to do it. Obviously, you will have to fiddle with the parameters as product titles are not exactly sentences. </p>

<p>You will need to prepare the training data but that should not be that hard. You need two columns, word and answer and you can add the the tag column (but I am not sure what the accuracy of standard POS taggerwould be as it is rather non-typical text). I would simply extract the value of the answer column from the associated json specification, there will be some ambiguity, but I think it will be rare enough so you can ignore it.</p>
",""
"18496893","2013-08-28 19:43:15","0","","18470873","<p>It's a bug I guess. When you do <code>wn.synset('delayed.a.0')</code> the first two lines in the method are:</p>

<pre><code>lemma, pos, synset_index_str = name.lower().rsplit('.', 2)
synset_index = int(synset_index_str) - 1
</code></pre>

<p>So in this case the value of <code>synset_index</code> is <code>-1</code> which is a valid index in python. And it won't fail when looking up in the array of synsets whose <code>lemma</code> is <code>delayed</code> and <code>pos</code> is <code>a</code>.</p>

<p>With this behavior you can do tricky things like:</p>

<pre><code>&gt;&gt;&gt; wn.synset('delay.v.-1')
Synset('stay.v.06')
</code></pre>
",""
"18474612","2013-08-27 20:16:15","2","","18473958","<p>As far as I know, ""zone hashing"" is not a well established concept in the NLP as a discipline. It is just a simple concept used in some algorithms (related to NLP). The only one I know, which uses it is a <code>Sphinx</code> search server, and here, ""zone hashing"" is simply ""hashing of objects called zones"", where ""zone"" is described as follows:</p>

<blockquote>
  <p>Zones can be formally defined as follows. Everything between an
  opening and a matching closing tag is called a span, and the aggregate
  of all spans corresponding sharing the same tag name is called a zone.
  For instance, everything between the occurrences of &lt; H1 > and &lt; /H1 > in
  the document field belongs to H1 zone.</p>
  
  <p>Zone indexing, enabled by index_zones directive, is an optional
  extension of the HTML stripper. So it will also require that the
  stripper is enabled (with html_strip = 1). The value of the
  index_zones should be a comma-separated list of those tag names and
  wildcards (ending with a star) that should be indexed as zones.</p>
  
  <p>Zones can nest and overlap arbitrarily. The only requirement is that
  every opening tag has a matching tag. You can also have an arbitrary
  number of both zones (as in unique zone names, such as H1) and spans
  (all the occurrences of those H1 tags) in a document. Once indexed,
  zones can then be used for matching with the ZONE operator, see
  Section 5.3, ‚ÄúExtended query syntax‚Äù.</p>
</blockquote>

<p>And hashing of these structures is used in the traditional sense to speed up search and lookup. I am not aware of any ""deeper"" meaning.</p>

<blockquote>
  <p>Perhaps it refers to locality sensitive hashing?</p>
</blockquote>

<p>Locality sensitive hashing is a probabilistic method for multi dimensional data, I do not see any deeper connections to the zone hashing then fact that both use hash functions.</p>
",""
"18386728","2013-08-22 17:06:03","2","","18366071","<p>You could download <a href=""http://languagetool.org/"" rel=""nofollow noreferrer"">LanguageTool</a> (Disclaimer: I'm the maintainer), which comes with a binary file <code>english.dict</code>. <a href=""http://wiki.languagetool.org/developing-a-tagger-dictionary"" rel=""nofollow noreferrer"">The LanguageTool Wiki</a> describes how to dump that file as a text file:</p>

<pre><code>java -jar morfologik-tools-1.6.0-standalone.jar fsa_dump -x -d english.dict
</code></pre>

<p>For <code>run</code>, the file will contain this:</p>

<pre><code>ran run VBD
run run NN
run run VB
run run VBN
run run VBP
running run VBG
runs run NNS
runs run VBZ
</code></pre>

<p>The first column is the inflected form, the second is the base form, and the third is the part-of-speech tag according to the (slightly extended) <a href=""https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"" rel=""nofollow noreferrer"">Penn Treebank tagset</a>.</p>
",""
"18337677","2013-08-20 14:30:06","0","","18332234","<p>As far as I know, it is a <a href=""http://en.wikipedia.org/wiki/CYK_algorithm"" rel=""nofollow"">CYK parser</a> (see <a href=""http://nlp.stanford.edu/~manning/papers/unlexicalized-parsing.pdf"" rel=""nofollow"">here</a>, Section 1), i.e. a bottom up parser.</p>
",""
"18140605","2013-08-09 05:36:41","0","","16523067","<p>I am using Stanford parser to extract entities like name ,location,organization.</p>

<p>Here is my code:</p>

<pre><code>public class stanfrdIntro {

    public static void main(String[] args) throws IOException, SAXException,
  {

        String serializedClassifier = ""classifiers/english.all.3class.distsim.crf.ser.gz"";


        AbstractSequenceClassifier&lt;CoreLabel&gt; classifier = CRFClassifier
                .getClassifierNoExceptions(serializedClassifier);

       String s1 = ""Good afternoon Rahul Kulhari, how are you today?"";

       s1 = s1.replaceAll(""\\s+"", "" "");
       String  t=classifier.classifyWithInlineXML(s1);
    System.out.println(Arrays.toString(getTagValues(t).toArray()));

    }
       private static final Pattern TAG_REGEX = Pattern.compile(""&lt;PERSON&gt;(.+?)&lt;/PERSON&gt;"");

private static Set&lt;String&gt; getTagValues(final String str) {
    final Set&lt;String&gt; tagValues = new HashSet&lt;String&gt;();
    //final Set&lt;String&gt; tagValues = new TreeSet();
    final Matcher matcher = TAG_REGEX.matcher(str);
    while (matcher.find()) {
        tagValues.add(matcher.group(1));
    }

    return tagValues;
}
</code></pre>

<p>This might help you but i am extracting only entities.</p>
",""
"18140504","2013-08-09 05:25:53","0","","18140415","<p>My code might help you.</p>

<p>Here is my Code</p>

<pre><code>public class stanfrdIntro {

    public static void main(String[] args) throws IOException, SAXException,
    TikaException {

        String serializedClassifier = ""classifiers/english.all.3class.distsim.crf.ser.gz"";


        AbstractSequenceClassifier&lt;CoreLabel&gt; classifier = CRFClassifier
                .getClassifierNoExceptions(serializedClassifier);

        if (args.length &gt; 1) {
         String fileContents = IOUtils.slurpFile(args[1]);
        List&lt;List&lt;CoreLabel&gt;&gt; out = classifier.classify(fileContents);
        for (List&lt;CoreLabel&gt; sentence : out) {
        for (CoreLabel word : sentence) {
        System.out.print(word.word() + '/' +
         word.get(CoreAnnotations.AnswerAnnotation.class) + ' ');
         }
         System.out.println();
        }
 out = classifier.classifyFile(args[1]);
        for (List&lt;CoreLabel&gt; sentence : out) {
        for (CoreLabel word : sentence) {
        System.out.print(word.word() + '/' +
        word.get(CoreAnnotations.AnswerAnnotation.class) + ' ');
        }
         System.out.println();
         }

         } else {
        String s1 = ""Good afternoon Rahul Kulhari, how are you today?"";
         String s2 =
         ""I go to school at Stanford University, which is located in California."";
        stanfrdIntro si = new stanfrdIntro();
        String s1 = si.contentEx();
        s1 = s1.replaceAll(""\\s+"", "" "");
        System.out.println(s1);
        String  t=classifier.classifyWithInlineXML(s1);
        System.out.println(Arrays.toString(getTagValues(t).toArray()));

        XPath xpath = XPathFactory.newInstance().newXPath();
        XPathExpression expr = xpath.compile(""//PERSON/text()"");
        Object value = expr.evaluate(doc, XPathConstants.STRING);
        System.out.println(t);
        System.out.println(classifier.classifyToString(s1));
        Set&lt;String&gt; s=classifier.;
        for(String s13: s)
        {
            System.out.println(s13);
        }
        System.out.println(classifier.classifyWithInlineXML(s1));
        System.out.println(classifier.classifyToString(s1, ""xml"", true));
         int i=0;
         for (List&lt;CoreLabel&gt; lcl : classifier.classify(s2)) {
        for (CoreLabel cl : lcl) {
         System.out.println(i++ + "":"");
         System.out.println(cl);
 }
 }
 }
    }


}
</code></pre>
",""
"18138696","2013-08-09 01:08:12","0","","18138238","<p>You've got mainly two options (that i know of, theres probably a ton more).</p>

<p>You've got PHPs <a href=""http://php.net/manual/en/book.intl.php"" rel=""nofollow""><code>intl</code></a> stuff, or you can use the <a href=""https://developers.google.com/translate/"" rel=""nofollow"">Google Translate API</a></p>
",""
"17753906","2013-07-19 19:20:26","3","","17695611","<p>Maybe you're looking for <code>CFG.fromstring()</code> (formerly <code>parse_cfg()</code>)?</p>

<p>From <a href=""http://nltk.sourceforge.net/doc/en/ch07.html#formalizing-context-free-grammars"" rel=""nofollow noreferrer"">Chapter 7</a> of the NLTK book (updated to NLTK 3.0):</p>

<pre><code>&gt; grammar = nltk.CFG.fromstring(""""""
 S -&gt; NP VP
 VP -&gt; V NP | V NP PP
 V -&gt; ""saw"" | ""ate""
 NP -&gt; ""John"" | ""Mary"" | ""Bob"" | Det N | Det N PP
 Det -&gt; ""a"" | ""an"" | ""the"" | ""my""
 N -&gt; ""dog"" | ""cat"" | ""cookie"" | ""park""
 PP -&gt; P NP
 P -&gt; ""in"" | ""on"" | ""by"" | ""with""
 """""")

&gt; sent = ""Mary saw Bob"".split()
&gt; rd_parser = nltk.RecursiveDescentParser(grammar)
&gt; for p in rd_parser.parse(sent):
      print p
(S (NP Mary) (VP (V saw) (NP Bob)))
</code></pre>
",""
"17687095","2013-07-16 21:12:41","0","","17684186","<p>Lemmatization does not (and should not) return ""acknowledge"" for ""acknowledgement"". The former is a verb, while the latter is a noun. Porter's stemming algorithm, on the other hand, simply uses a fixed set of rules. So, your only way there is to change the rules at source. (NOT the right way to fix your problem).</p>

<p>What you are looking for is the derivationally related form of ""acknowledgement"", and for this, your best source is WordNet. You can check this <a href=""http://wordnetweb.princeton.edu/perl/webwn?o2=&amp;o0=1&amp;o8=1&amp;o1=1&amp;o7=&amp;o5=&amp;o9=&amp;o6=&amp;o3=&amp;o4=&amp;s=acknowledgement&amp;i=3&amp;h=10000#c"" rel=""noreferrer"" title=""here"">online on WordNet</a>.</p>

<p>There are quite a few WordNet-based libraries that you can use for this (e.g. in <a href=""http://sourceforge.net/projects/jwordnet/"" rel=""noreferrer"">JWNL</a> in Java). In Python, NLTK should be able to get the derivationally related form you saw online:</p>

<pre><code>from nltk.corpus import wordnet as wn

acknowledgment_synset = wn.synset('acknowledgement.n.01')
acknowledgment_lemma = acknowledgment_synset.lemmas[1]

print(acknowledgment_lemma.derivationally_related_forms())
# [Lemma('admit.v.01.acknowledge'), Lemma('acknowledge.v.06.acknowledge')]
</code></pre>
",""
"17453157","2013-07-03 16:18:21","10","","17447045","<p>Here is a possible solution using <a href=""http://lucene.apache.org/"" rel=""noreferrer"">Apache Lucene</a>. I didn't use the last version but the <a href=""http://archive.apache.org/dist/lucene/java/3.6.2/"" rel=""noreferrer"">3.6.2 one</a>, since this is the one I know the best. Besides the <code>/lucene-core-x.x.x.jar</code>, don't forget to add the <code>/contrib/analyzers/common/lucene-analyzers-x.x.x.jar</code> from the downloaded archive to your project: it contains the language-specific analyzers (especially the English one in your case).</p>

<p>Note that this will <em>only</em> find the frequencies of the input text words based on their respective stem. Comparing these frequencies with the English language statistics shall be done afterwards (<a href=""https://stackoverflow.com/a/7248868/1225328"">this answer</a> may help by the way).</p>

<hr>

<h1>The data model</h1>

<p>One keyword for one stem. Different words may have the same stem, hence the <code>terms</code> set. The keyword frequency is incremented every time a new term is found (even if it has been already found - a set automatically removes duplicates).</p>

<pre class=""lang-java prettyprint-override""><code>public class Keyword implements Comparable&lt;Keyword&gt; {

  private final String stem;
  private final Set&lt;String&gt; terms = new HashSet&lt;String&gt;();
  private int frequency = 0;

  public Keyword(String stem) {
    this.stem = stem;
  }

  public void add(String term) {
    terms.add(term);
    frequency++;
  }

  @Override
  public int compareTo(Keyword o) {
    // descending order
    return Integer.valueOf(o.frequency).compareTo(frequency);
  }

  @Override
  public boolean equals(Object obj) {
    if (this == obj) {
      return true;
    } else if (!(obj instanceof Keyword)) {
      return false;
    } else {
      return stem.equals(((Keyword) obj).stem);
    }
  }

  @Override
  public int hashCode() {
    return Arrays.hashCode(new Object[] { stem });
  }

  public String getStem() {
    return stem;
  }

  public Set&lt;String&gt; getTerms() {
    return terms;
  }

  public int getFrequency() {
    return frequency;
  }

}
</code></pre>

<hr>

<h1>Utilities</h1>

<p>To stem a word:</p>

<pre class=""lang-java prettyprint-override""><code>public static String stem(String term) throws IOException {

  TokenStream tokenStream = null;
  try {

    // tokenize
    tokenStream = new ClassicTokenizer(Version.LUCENE_36, new StringReader(term));
    // stem
    tokenStream = new PorterStemFilter(tokenStream);

    // add each token in a set, so that duplicates are removed
    Set&lt;String&gt; stems = new HashSet&lt;String&gt;();
    CharTermAttribute token = tokenStream.getAttribute(CharTermAttribute.class);
    tokenStream.reset();
    while (tokenStream.incrementToken()) {
      stems.add(token.toString());
    }

    // if no stem or 2+ stems have been found, return null
    if (stems.size() != 1) {
      return null;
    }
    String stem = stems.iterator().next();
    // if the stem has non-alphanumerical chars, return null
    if (!stem.matches(""[a-zA-Z0-9-]+"")) {
      return null;
    }

    return stem;

  } finally {
    if (tokenStream != null) {
      tokenStream.close();
    }
  }

}
</code></pre>

<p>To search into a collection (will be used by the list of potential keywords):</p>

<pre class=""lang-java prettyprint-override""><code>public static &lt;T&gt; T find(Collection&lt;T&gt; collection, T example) {
  for (T element : collection) {
    if (element.equals(example)) {
      return element;
    }
  }
  collection.add(example);
  return example;
}
</code></pre>

<hr>

<h1>Core</h1>

<p>Here is the main input method:</p>

<pre class=""lang-java prettyprint-override""><code>public static List&lt;Keyword&gt; guessFromString(String input) throws IOException {

  TokenStream tokenStream = null;
  try {

    // hack to keep dashed words (e.g. ""non-specific"" rather than ""non"" and ""specific"")
    input = input.replaceAll(""-+"", ""-0"");
    // replace any punctuation char but apostrophes and dashes by a space
    input = input.replaceAll(""[\\p{Punct}&amp;&amp;[^'-]]+"", "" "");
    // replace most common english contractions
    input = input.replaceAll(""(?:'(?:[tdsm]|[vr]e|ll))+\\b"", """");

    // tokenize input
    tokenStream = new ClassicTokenizer(Version.LUCENE_36, new StringReader(input));
    // to lowercase
    tokenStream = new LowerCaseFilter(Version.LUCENE_36, tokenStream);
    // remove dots from acronyms (and ""'s"" but already done manually above)
    tokenStream = new ClassicFilter(tokenStream);
    // convert any char to ASCII
    tokenStream = new ASCIIFoldingFilter(tokenStream);
    // remove english stop words
    tokenStream = new StopFilter(Version.LUCENE_36, tokenStream, EnglishAnalyzer.getDefaultStopSet());

    List&lt;Keyword&gt; keywords = new LinkedList&lt;Keyword&gt;();
    CharTermAttribute token = tokenStream.getAttribute(CharTermAttribute.class);
    tokenStream.reset();
    while (tokenStream.incrementToken()) {
      String term = token.toString();
      // stem each term
      String stem = stem(term);
      if (stem != null) {
        // create the keyword or get the existing one if any
        Keyword keyword = find(keywords, new Keyword(stem.replaceAll(""-0"", ""-"")));
        // add its corresponding initial token
        keyword.add(term.replaceAll(""-0"", ""-""));
      }
    }

    // reverse sort by frequency
    Collections.sort(keywords);

    return keywords;

  } finally {
    if (tokenStream != null) {
      tokenStream.close();
    }
  }

}
</code></pre>

<hr>

<h1>Example</h1>

<p>Using the <code>guessFromString</code> method on the <a href=""https://en.wikipedia.org/wiki/Java_(programming_language)"" rel=""noreferrer"">Java wikipedia article introduction part</a>, here are the first 10 most frequent keywords (i.e. stems) that were found:</p>

<pre class=""lang-none prettyprint-override""><code>java         x12    [java]
compil       x5     [compiled, compiler, compilers]
sun          x5     [sun]
develop      x4     [developed, developers]
languag      x3     [languages, language]
implement    x3     [implementation, implementations]
applic       x3     [application, applications]
run          x3     [run]
origin       x3     [originally, original]
gnu          x3     [gnu]
</code></pre>

<p>Iterate over the output list to know which were the <em>original found words</em> for each stem by getting the <code>terms</code> sets (displayed between brackets <code>[...]</code> in the above example).</p>

<hr>

<h1>What's next</h1>

<p>Compare the <em>stem frequency / frequencies sum</em> ratios with the English language statistics ones, and keep me in the loop if your managed it: I could be quite interested too <code>:)</code></p>
",""
"17425786","2013-07-02 12:34:36","0","","17408543","<p>I guess I found a way to do it. For those who were having the same problem, I recommend you to download the source code, build it and call it in a way different from what is described in NLTK docs. As it weren't trivial for me, I'm putting it here step-by-step:</p>

<p>Under Unix:</p>

<p>1) Download <a href=""http://subversion.apache.org/packages.html"" rel=""nofollow"">Subversion</a> SVN if you don't have it and check out the project source code:</p>

<pre><code>svn checkout http://hunpos.googlecode.com/svn/trunk/ hunpos-read-only
</code></pre>

<p>This will create a <code>trunk</code> directory where you checked out.</p>

<p>2) Then, to be able to successfully build it, you might need <code>ocamlbuild</code> for automatic compiling of Objective Caml. <code>sudo apt-get install ocaml-nox</code> should handle this.</p>

<p>3) <code>cd</code> to the <code>trunk</code> directory (where you downloaded Hunpos source code) and do</p>

<pre><code>./build.sh build
</code></pre>

<p>4) At this point, you shall have a binary file <code>tagger.native</code> in your <code>trunk</code> directory. Put the whole <code>trunk</code> directory in your <code>/usr/local/bin</code> (you may need to do it as super user).</p>

<p>5) Download the <code>en_wsj.model.gz</code> file <a href=""https://code.google.com/p/hunpos/downloads/list"" rel=""nofollow"">here</a>, unzip it and put the <code>en_wsj.model</code> binary also in <code>usr/local/bin</code>.</p>

<p>6) Finally, in your python script, you may create an instance of <code>HunposTagger</code> class passing the paths to both files you have created previously, something very close to:</p>

<pre><code>&gt;&gt;&gt; from nltk.tag.hunpos import HunposTagger
&gt;&gt;&gt; ht = HunposTagger(path_to_model='/usr/local/bin/en_wsj.model', \
                      path_to_bin=  '/usr/local/bin/trunk/tagger.native')
&gt;&gt;&gt; ht.tag('I want to go to San Francisco next year'.split())
[('I', 'PRP'), ('want', 'VBP'), ('to', 'TO'), ('go', 'VB'), ('to', 'TO'),
 ('San', 'NNP'), ('Francisco', 'NNP'), ('next', 'JJ'), ('year', 'NN')]
&gt;&gt;&gt; ht.close()
</code></pre>

<p>(Don't forget to close... if you don't like to close, you may use the <code>with</code> statement as well)</p>

<p>7) If you still have some trouble, try to set an environmental variable <code>HUNPOS</code> to <code>/usr/local/bin/trunk</code>. To do this, you may add the following line to your <code>~/.bashrc</code> (or <code>~/.bash_profile</code> in MacOS):</p>

<pre><code>export HUNPOS=/usr/local/bin/trunk
</code></pre>

<p>and restart your terminal.</p>

<p>That worked for me, but if someone has a better, shorter or simpler way to set this up, please I'd love to hear :)</p>
",""
"17320458","2013-06-26 12:46:55","2","","17317418","<p><strong>Q1: &quot;[..] are English stemmers any useful at all today? Since we have a plethora of lemmatization tools for English&quot;</strong></p>
<p>Yes. <strong>Stemmers are much simpler, smaller, and usually faster than lemmatizers,</strong> and for many applications, their results are <strong>good enough</strong>. Using a lemmatizer for that is a waste of resources. Consider, for example, dimensionality reduction in Information Retrieval. You replace all <em>drive/driving</em> with <em>driv</em> in both the searched documents and the query. You do not care if it is <em>drive</em> or <em>driv</em> or <em>x17a$</em> as long as it clusters inflectionally related words together.</p>
<p><strong>Q2: &quot;[..]how should we move on to build robust lemmatizers that can take on nounify, verbify, adjectify, and adverbify preprocesses?</strong></p>
<p>What is your <strong>definition of a lemma,</strong> does it include derivation (<em>drive</em> - <em>driver</em>) or only inflection (<em>drive</em> - <em>drives</em> - <em>drove</em>)? Does it take into account semantics?</p>
<p>If you want to include <strong>derivation</strong> (which most people would say includes verbing nouns etc.) then keep in mind that derivation is far more <strong>irregular</strong> than inflection. There are many idiosyncracies, gaps, etc. Do you really want for <em>to change</em> (<em>change trains</em>) and <em>change</em> (as coins) to have the same lemma? If not, where do you draw the boundary? How about <em>nerve</em> - <em>unnerve</em>, <em>earth</em> -- <em>unearth</em> - <em>earthling</em>, ...  It really depends on the application.</p>
<p>If you take into account <strong>semantics</strong> (<em>bank</em> would be labeled as <em>bank-money</em> or <em>bank-river</em> depending on context), how deep do you go (do you distinguish <em>bank-institution</em> from <em>bank-building</em>)? Some apps may not care about this at all, some might want to distinguish basic semantics, and some might want it fined-grained.</p>
<p><strong>Q3: &quot;How could the lemmatization task be easily scaled to other languages that have similar morphological structures as English?&quot;</strong></p>
<p>What do you mean by &quot;similar morphological structures as English&quot;? English has very little inflectional morphology. There are good lemmatizers for languages of other morphological types (truly inflectional, agglutinative, template, ...).</p>
<p>With a possible exception of agglutinative languages, I would argue that a <strong>lookup table</strong> (say a compressed trie) is the best solution. (Possibly with some backup rules for unknown words such as proper names). The lookup is followed by some kind of disambiguation (ranging from trivial - take the first one, or take the first one consistent with the words POS tag, to much more sophisticated). The more sophisticated disambiguations are usually supervised stochastical algorithms (e.g. <a href=""http://www.cis.uni-muenchen.de/%7Eschmid/tools/TreeTagger/"" rel=""nofollow noreferrer"">TreeTagger</a> or <a href=""http://perso.limsi.fr/Individu/jacquemi/FASTR/"" rel=""nofollow noreferrer"">Faster</a>), although a combination of machine learning and manually created rules has been done too (see e.g. <a href=""http://acl.ldc.upenn.edu/P/P01/P01-1035.pdf"" rel=""nofollow noreferrer"">this</a>).</p>
<p>Obviously, for most languages, you do not want to create the lookup table by
hand, but instead, generate it from a description of the morphology of
that language. For inflectional languages, you can go the engineering
way of Hajic for Czech or Mikheev for Russian, or, if you are daring,
you use two-level morphology. Or you can do something in between,
such as <a href=""http://ufal.mff.cuni.cz/%7Ehana/bib/hana-2008-wp-morph.pdf"" rel=""nofollow noreferrer"">Hana</a> (myself) (Note that these are all full
morphological analyzers that include lemmatization as one of their features). Or you can learn
the lemmatizer in an unsupervised manner a la <a href=""http://dx.doi.org/10.3115/1075218.1075245"" rel=""nofollow noreferrer"">Yarowsky and
Wicentowski</a>, possibly with manual post-processing, correcting the
most frequent words.</p>
<p>There are way too many options and it really all depends on what you want to do with the results.</p>
",""
"17316106","2013-06-26 09:18:35","1","","17314506","<p>Tokenization is the identification of <strong>linguistically meaningful units</strong> (LMU) from the surface text.</p>
<blockquote>
<p><strong>Chinese</strong>: Â¶ÇÊûúÊÇ®Âú®Êñ∞Âä†Âù°Âè™ËÉΩÂâçÂæÄ‰∏ÄÈó¥Â§úÈó¥Â®±‰πêÂú∫ÊâÄÔºåZoukÂøÖÁÑ∂ÊòØÊÇ®ÁöÑ‰∏ç‰∫å‰πãÈÄâ„ÄÇ</p>
<p><strong>English</strong>: If you only have time for one club in Singapore, then it simply has to be Zouk.</p>
<p><strong>Indonesian</strong>: Jika Anda hanya memiliki waktu untuk satu klub di Singapura, pergilah ke Zouk.</p>
<p><strong>Japanese</strong>: „Ç∑„É≥„Ç¨„Éù„Éº„É´„Åß‰∏Ä„Å§„Åó„Åã„ÇØ„É©„Éñ„Å´Ë°å„ÅèÊôÇÈñì„Åå„Å™„Åã„Å£„Åü„Å®„Åó„Åü„Çâ„ÄÅ„Åì„ÅÆ„Ç∫„Éº„ÇØ„Å´Ë°å„Åè„Åπ„Åç„Åß„Åô„ÄÇ</p>
<p><strong>Korean</strong>: Ïã±Í∞ÄÌè¨Î•¥ÏóêÏÑú ÌÅ¥ÎüΩ Ìïú Íµ∞Îç∞Î∞ñÏóê Í∞àÏãúÍ∞ÑÏù¥ ÏóÜÎã§Î©¥, ZoukÎ•º ÏÑ†ÌÉùÌïòÏÑ∏Ïöî.</p>
<p><strong>Vietnamese</strong>: N·∫øu b·∫°n ch·ªâ c√≥ th·ªùi gian gh√© thƒÉm m·ªôt c√¢u l·∫°c b·ªô ·ªü Singapore th√¨ h√£y ƒë·∫øn Zouk.</p>
<p>Text Source: <a href=""http://aclweb.org/anthology/Y/Y11/Y11-1038.pdf"" rel=""noreferrer"">http://aclweb.org/anthology/Y/Y11/Y11-1038.pdf</a></p>
</blockquote>
<p>The tokenized version of the parallel text above should look like this:</p>
<p><img src=""https://i.sstatic.net/ThMvO.jpg"" alt=""enter image description here"" /></p>
<p>For <strong>English</strong>, it's simple because each LMU is <em><strong>delimited/separated by whitespaces</strong></em>. However in other languages, it might not be the case. For most romanized languages, such as <strong>Indonesian</strong>, they have the same whitespace delimiter that can easily identify a LMU.</p>
<p>However, sometimes an LMU is a combination of two &quot;words&quot; separated by spaces. E.g. in the <strong>Vietnamese</strong> sentence above, you have to read <code>th·ªùi_gian</code> (it means <em>time</em> in English) as one token and not 2 tokens. Separating the two words into 2 tokens yields <strong>no LMU</strong> (e.g. <a href=""http://vdict.com/th%E1%BB%9Di,2,0,0.html"" rel=""noreferrer"">http://vdict.com/th%E1%BB%9Di,2,0,0.html</a>) or <strong>wrong LMU(s)</strong> (e.g. <a href=""http://vdict.com/gian,2,0,0.html"" rel=""noreferrer"">http://vdict.com/gian,2,0,0.html</a>). Hence a proper Vietnamese tokenizer would output <code>th·ªùi_gian</code> as one token rather than <code>th·ªùi</code> and <code>gian</code>.</p>
<p>For some other languages, their orthographies might have no spaces to delimit &quot;words&quot; or &quot;tokens&quot;, e.g. Chinese, Japanese and sometimes Korean. In that case, tokenization is necessary for computer to identify LMU. Often there are morphemes/inflections attached to an LMU, so sometimes a <code>morphological analyzer</code> is more useful than a tokenizer in Natural Language Processing.</p>
",""
"17267945","2013-06-24 04:16:09","0","","17259970","<p>Spaceghost is correct, you need to provide a reference back to an actual <code>NgramTagger</code> object as the <code>backoff</code> argument and not just an <code>int</code>. Simply using a number as backoff is meaningless - when creating a new tagger, it has no idea where to look for the previously created tagger with a smaller relative context.</p>

<p>This is why you get the <code>AttributeError: 'int' object has no attribute '_taggers'</code>. NLTK is looking for an object of a class inheriting from <a href=""http://nltk.googlecode.com/svn/trunk/doc/api/nltk.tag.sequential-pysrc.html#SequentialBackoffTagger"" rel=""nofollow""><code>SequentialBackoffTagger</code></a>.</p>

<p>Based on your <code>range(3)</code>, I'm going to guess you actually wanted a <em>trigram tagger</em> with backoff to a <em>bigram tagger</em>, with backoff to a <em>unigram tagger</em>.</p>

<p>You can try something like,</p>

<pre><code>from nltk.corpus import brown
from nltk import NgramTagger

trains = brown.tagged_sents(categories=""news"")
tagger = None         # None here is okay since it's the default argument anyway
for n in range(1,4):  # start at unigrams (1) up to and including trigrams (3)
    tagger = NgramTagger(n, trains, backoff=tagger)
</code></pre>

<p><strong>NOTE:</strong> No need to import nltk multiple times.</p>

<pre><code>&gt;&gt;&gt; tagger.tag('hi how are you'.split())
[('hi', None), ('how', 'WRB'), ('are', 'BER'), ('you', 'PPSS')]
</code></pre>

<p>Notice, we get <code>None</code> for the POS of words like ""hi"" since it doesn't occur in the given corpus (Brown's news category). You can set a default tagger if you want by initially setting <code>tagger</code> (before the for-loop) like,</p>

<pre><code>from nltk import DefaultTagger
tagger = DefaultTagger('NN')
</code></pre>
",""
"17259249","2013-06-23 09:12:42","1","","17251156","<p>A simple solution that will get you a long way is to use the dependency parser, which is included with Stanford CoreNLP. The algorithm goes like this:</p>

<ol>
<li>PoS tag and Dependency parse your sentence</li>
<li>Decide which of the nouns you are interested in. If you are dealing with product reviews, an easy way of doing this is to match all nouns in the text against a list of known product names.</li>
<li>Look for <code>amod</code> relations in the output of the dependency parser that include the noun you are interested in.</li>
</ol>

<p>Example using the <a href=""http://nlp.stanford.edu:8080/parser/index.jsp"" rel=""noreferrer"">online Stanford demo</a>:</p>

<p>Input:</p>

<pre><code>I own a tall glass and just bought a big red car.
</code></pre>

<p><code>amod</code> dependencies:</p>

<pre><code>amod(glass-5, tall-4)
amod(car-12, big-10)
amod(car-12, red-11)
</code></pre>

<p>Suppose the reviews are about cars. The last two dependencies contain the target noun <code>car</code>, and the adjectives you are looking for are therefore <code>big</code> and <code>red</code>.</p>

<p>Warning: this is a <strong>high-precision</strong> search algorithm rather than high recall. Your list of keywords will never be exhaustive, so you are likely to miss some of the adjectives. Also, the parser is not perfect and will sometimes make mistakes. Moreover, the <code>amod</code> relation is one of many way an adjective can describe a noun. For example, <code>""The car is red""</code> parses as</p>

<pre><code>det(car-2, The-1)
nsubj(red-4, car-2)
nsubj(black-6, car-2)
cop(red-4, is-3)
root(ROOT-0, red-4)
conj_and(red-4, black-6)
</code></pre>

<p>As you can see, there isn't an <code>amod</code> relations here, just a copula and a conjunction. You could try and craft more complex rules trying to extract the fact that the <code>car is red</code> and <code>car is black</code>. Whether you want to do that is up to up. In its current form, when this algorithm returns an adjective, you can be reasonably confident it is indeed describing the noun. This, in my opinion, is a good characteristic, but it all depends on your use case.</p>

<hr>

<p>Edit after comment by OP:</p>

<p>Yes, <code>I bought a new car.</code> and <code>It is awesome.</code> are two separate sentences and will be parsed separately. This problem is known as <a href=""http://en.wikipedia.org/wiki/Coreference#Coreference_resolution"" rel=""noreferrer"">coreference (anaphora) resolution</a>. It turns out Stanford also supports this- see <a href=""http://nlp.stanford.edu/software/dcoref.shtml"" rel=""noreferrer"">their webpage</a>. There is also <a href=""http://www.ark.cs.cmu.edu/ARKref/"" rel=""noreferrer"">a system by CMU</a>, which is also in Java. I haven't used either of these systems, but the latter has a very helpful online demo. Putting the above two sentences in, I get</p>

<pre><code>[I] bought [a new car]2 .
[It]2 is awesome .
</code></pre>
",""
"17247934","2013-06-22 06:20:52","0","","17247874","<p><strong>Information extraction</strong> </p>

<p>(IE) is the task of automatically extracting structured information from unstructured and/or semi-structured machine-readable documents. In most of the cases this activity concerns processing human language texts by means of natural language processing (NLP). Recent activities in multimedia document processing like automatic annotation and content extraction out of images/audio/video could be seen as information extraction.</p>

<p><strong>Text Mining</strong> </p>

<p>is the activity of obtaining information resources relevant to an information need from a collection of information resources. Searches can be based on metadata or on full-text indexing.</p>

<p>Text mining is vast area as compared to information retrieval. Typical text mining tasks include document classification, document clustering, building ontology, sentiment analysis, document summarization, Information extraction etc. 
Where as information retrieval typically deals with crawling, parsing and indexing document, retrieving documents.</p>

<p><a href=""https://cs.stackexchange.com/questions/7181/relation-and-difference-between-information-retrieval-and-information-extraction"">Source</a></p>
",""
"17205066","2013-06-20 04:04:01","0","","17176362","<p>I found a rudimentary solution to 2. I noticed that on <a href=""http://www.maltparser.org/userguide.html#api"" rel=""nofollow"">http://www.maltparser.org/userguide.html#api</a> it directs one to a listing of example files. I took this snippet out of one of those files:</p>

<pre><code>/**
* @author Johan Hall
 */
public static void main(String[] args) {
    try {
        MaltParserService service =  new MaltParserService();
        // Inititalize the parser model 'model0' and sets the working directory to '.' and sets the logging file to 'parser.log'
        service.initializeParserModel(""-c model0 -m parse -w . -lfi parser.log"");

        // Creates an array of tokens, which contains the Swedish sentence 'Grundavdraget upph√∂r allts√• vid en taxerad inkomst p√• 52500 kr.'
        // in the CoNLL data format.
        String[] tokens = new String[11];
        tokens[0] = ""1\tGrundavdraget\t_\tN\tNN\tDD|SS"";
        tokens[1] = ""2\tupph√∂r\t_\tV\tVV\tPS|SM"";
        tokens[2] = ""3\tallts√•\t_\tAB\tAB\tKS"";
        tokens[3] = ""4\tvid\t_\tPR\tPR\t_"";
        tokens[4] = ""5\ten\t_\tN\tEN\t_"";
        tokens[5] = ""6\ttaxerad\t_\tP\tTP\tPA"";
        tokens[6] = ""7\tinkomst\t_\tN\tNN\t_"";
        tokens[7] = ""8\tp√•\t_\tPR\tPR\t_"";
        tokens[8] = ""9\t52500\t_\tR\tRO\t_"";
        tokens[9] = ""10\tkr\t_\tN\tNN\t_"";
        tokens[10] = ""11\t.\t_\tP\tIP\t_"";
        // Parses the Swedish sentence above
        DependencyStructure graph = service.parse(tokens);
        // Outputs the dependency graph created by MaltParser.
        System.out.println(graph);
        // Terminates the parser model
        service.terminateParserModel();
    } catch (MaltChainedException e) {
        System.err.println(""MaltParser exception: "" + e.getMessage());
    }
}
</code></pre>
",""
"17187057","2013-06-19 08:58:42","0","","17186824","<p>Google provides the <em>Custom Search API</em> which you can use to search Google from code.</p>

<p>You'll need to create a google account if you don't already have one, create a custom search engine, generate an API key, and then use that as part of your request. You can get the results as JSON or as an Atom XML.</p>

<p>The documentation for this is available at <a href=""https://developers.google.com/custom-search/v1/overview"" rel=""nofollow"">https://developers.google.com/custom-search/v1/overview</a></p>

<p>There's no java library, as such, so to actually call the API, you'll need to either use <code>java.net.URL</code> and <code>java.net.URLConnection</code>, or a REST client library like the one that <a href=""https://jersey.java.net/"" rel=""nofollow"">jersey</a> ships with.</p>
",""
"17165343","2013-06-18 09:32:12","15","","17160097","<p>The code that I am using for similarity library is as follows :</p>

<pre><code>final SentenceSimilarityAssessor s=new SentenceSimilarityAssessor();
s.getSearchEngineHungarianSentenceSimilarity(s1, s2, SimilarityConstants.GOOGLE, SimilarityConstants.NGD_MEASURE, SimilarityConstants.TURNEY_SCORE_1);
</code></pre>

<p>You can try this.</p>
",""
"17131507","2013-06-16 08:35:54","0","","17093322","<p>This is a problem is word segmentation, and an efficient dynamic programming solution exists. <a href=""http://thenoisychannel.com/2011/08/08/retiring-a-great-interview-problem/"" rel=""nofollow"">This</a> page discusses how you could implement it. I have also answered this question on SO before, but I can't find a link to the answer. Please feel free to edit my post if you do.</p>
",""
"17117425","2013-06-14 21:18:25","0","","16181419","<p>I've used the lemmatizer like this</p>
<pre class=""lang-py prettyprint-override""><code>from nltk.stem.wordnet import WordNetLemmatizer # to download corpora: python -m    nltk.downloader all
lmtzr = WordNetLemmatizer() # create a lemmatizer object
lemma = lmtzr.lemmatize('cats')
</code></pre>
<p>It is not slow at all on my machine. There is no need to connect to the web to do this.</p>
",""
"17076741","2013-06-12 22:54:42","2","","17076635","<p>This should solve your problem, presuming you have correct regex in variable 'phrase'</p>

<pre><code>import re

# compile regex
regex = re.compile('[0-9]+')

# open the files
with open('Corpus.txt','r') as inputFile:
    with open('OutputLineNumbers', 'w') as outputLineNumbers:
        # loop through each line in corpus
        for line_i, line in enumerate(inputFile, 1):
            # check if we have a regex match
            if regex.search( line ):
                # if so, write it the output file
                outputLineNumbers.write( ""%d\n"" % line_i )
</code></pre>
",""
"17016149","2013-06-10 02:28:27","1","","17015658","<p>Mahout is easy to use and install. All you need is JDK environment and maven.
<a href=""http://nivirao.blogspot.com/2012/04/installing-apache-mahout-on-ubuntu.html"" rel=""nofollow"">how to install mahout</a></p>

<p>Also you could use hadoop with mahout, which is not a must (you could run mahout locally without hadoop). However you could find this <a href=""http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/"" rel=""nofollow"">blog</a> helpful for install hadoop.</p>
",""
"17013989","2013-06-09 20:45:36","0","","17013370","<p>I wouldn't recommend using regular expressions here. It's definitely not as intuitive as just iterating over each line after being split on whitespace, possibly rearranging the list, and finally joining. You can try something like this,</p>

<pre><code>reordered_corpus = open('reordered_corpus.txt', 'w')
with open('corpus.txt', 'r') as corpus:
    for phrase in corpus:
        phrase = phrase.split()                 # split on whitespace
        vb_index = rp_index = -1                # variables for the indices
        for i, word_pos in enumerate(phrase):
            pos = word_pos.split('_')[1]        # POS at index 1 splitting on _
            if pos == 'VB' or pos == 'VBZ':     # can add more verb POS tags
                vb_index = i
            elif vb_index &gt;= 0 and pos == 'RP': # or more particle POS tags
                rp_index = i
                break                           # found both so can stop
        if vb_index &gt;= 0 and rp_index &gt;= 0:     # do any rearranging
            phrase = phrase[:vb_index+1] + [phrase[rp_index]] + \
                     phrase[vb_index+1:rp_index] + phrase[rp_index+1:]
        reordered_corpus.write(' '.join(word_pos for word_pos in phrase)+'\n')
reordered_corpus.close()
</code></pre>

<p>Using this code, if <code>corpus.txt</code> reads,</p>

<pre class=""lang-none prettyprint-override""><code>you_PRP mean_VBP we_PRP should_MD kick_VB them_PRP out_RP ._.
don_VB 't_NNP take_VB it_PRP off_RP until_IN I_PRP say_VBP so_RB ._.
please_VB help_VB the_DT man_NN out_RP ._.
shut_VBZ it_PRP down_RP !_.
</code></pre>

<p>after running, <code>reordered_corpus.txt</code> will be,</p>

<pre class=""lang-none prettyprint-override""><code>you_PRP mean_VBP we_PRP should_MD kick_VB out_RP them_PRP ._.
don_VB 't_NNP take_VB off_RP it_PRP until_IN I_PRP say_VBP so_RB ._.
please_VB help_VB out_RP the_DT man_NN ._.
shut_VBZ down_RP it_PRP !_.
</code></pre>
",""
"16836828","2013-05-30 12:55:16","1","","16835372","<p>You need to split the text into words for the stemmer to work. Currently, the variable <code>text</code> contains the whole file as one big string. The loop <code>for plural in text:</code> assigns each character in <code>text</code> to <code>plural</code>.</p>

<p>Try <code>for plural in text.split():</code> instead.</p>

<p><strong>[EDIT]</strong> To get the output in the format you want, you need to read the file line by line instead of reading it all at once:</p>

<pre><code>def stemming_text_1():
    with open('test.txt', 'r') as f:
        for line in f:
            print line
            singles = []

            stemmer = PorterStemmer() #problem from HERE
            for plural in line.split():
                singles.append(stemmer.stem(plural))
            print ' '.join(singles)
</code></pre>
",""
"16735015","2013-05-24 12:29:49","0","","16734074","<p>Use <code>indexOf</code>, then search backwards and forwards for the separator of the paragraph. Might be <code>&lt;p&gt;</code> or <code>\n</code>.</p>

<pre><code>public static String findParagraph(String source, String searchText, String paragraphSeparator)
{
    final int locationOfSearchTerm = source.indexOf(searchText);
    if (locationOfSearchTerm == -1) return null;

    int paragraphEnd = source.indexOf(paragraphSeparator, locationOfSearchTerm + searchText.length);

    //if we didn't find an end of a paragraph, we want to go the end
    if (paragraphEnd == -1) paragraphEnd = searchText.length;

    int paragraphStart = source.lastIndexOf(paragraphSeparator, locationOfSearchTerm);

    //if we didn't find a start of a paragraph, we want to go the beginning
    if (paragraphStart == -1) paragraphStart = 0;

    return searchText.subString(paragraphStart, paragraphEnd - 1);
}
</code></pre>
",""
"16721618","2013-05-23 18:46:51","1","","16717161","<p>Ruby on Rails is a framework, not a language. It's written in the Ruby language. You can get the source for Ruby at <a href=""http://www.ruby-lang.org/en/downloads/"" rel=""nofollow"">http://www.ruby-lang.org/en/downloads/</a>. It uses lex and yacc for parsing; as far as I know, that code's contained in parse.y.</p>
",""
"16556737","2013-05-15 04:13:53","3","","16556598","<h1>Soapbox</h1>
<p>We could craft a regex to match your specific case, but given this is HTML parsing and that your use case hints that any number of tags could be in there, you'd be best off using the DOM or using a product like <a href=""http://htmlagilitypack.codeplex.com/"" rel=""nofollow noreferrer"">HTML Agility (free)</a></p>
<h1>However</h1>
<p>If you're just looking to pull out the inner text and not interested in retaining any of the tag data, you could use this regex and replace all matches with a null</p>
<p><code>(&lt;[^&gt;]*&gt;)</code></p>
<p><img src=""https://i.sstatic.net/HEqFL.png"" alt=""enter image description here"" />
<img src=""https://i.sstatic.net/I5m8w.png"" alt=""enter image description here"" /></p>
<h1>Retain sentence as is including subtags</h1>
<ul>
<li><p><code>((?:&lt;p(?:\s[^&gt;]*)?&gt;).*?&lt;/p&gt;)</code> - retain the paragraph tags and entire sentence, but not any data outside the paragraph</p>
</li>
<li><p><code>(?:&lt;p(?:\s[^&gt;]*)?&gt;)(.*?)(?:&lt;/p&gt;)</code> - retain just the paragraph innertext including all subtags, and store sentence into group 1</p>
</li>
<li><p><code>(&lt;p(?:\s[^&gt;]*)?&gt;)(.*?)(&lt;/p&gt;)</code> - capture open and close paragraph tags and the innertext including any sub tags</p>
</li>
</ul>
<p>Granted these are PowerShell examples, the regex and replace function should be similar</p>
<pre><code>$string = '&lt;img&gt; not this stuff either&lt;/img&gt;&lt;p class=SuperCoolStuff&gt;This is a sample of a &lt;a href=&quot;#&quot;&gt;link&lt;/a&gt; getting chewed up.&lt;/p&gt;&lt;a&gt; other stuff&lt;/a&gt;'

Write-Host &quot;replace p tags with a new span tag&quot;
$string -replace '(?:&lt;p(?:\s[^&gt;]*)?&gt;)(.*?)(?:&lt;/p&gt;)', '&lt;span class=sentence&gt;$1&lt;/span&gt;'

Write-Host
Write-Host &quot;insert p tag's inner text into a span new span tag and return the entire thing including the p tags&quot;
$string -replace '(&lt;p(?:\s[^&gt;]*)?&gt;)(.*?)(&lt;/p&gt;)', '$1&lt;span class=sentence&gt;$2&lt;/span&gt;$3'
</code></pre>
<p>Yields</p>
<pre><code>replace p tags with a new span tag
&lt;img&gt; not this stuff either&lt;/img&gt;&lt;span class=sentence&gt;This is a sample of a &lt;a href=&quot;#&quot;&gt;link&lt;/a&gt; getting chewed up.&lt;/span
&gt;&lt;a&gt; other stuff&lt;/a&gt;

insert p tag's inner text into a span new span tag and return the entire thing including the p tags
&lt;img&gt; not this stuff either&lt;/img&gt;&lt;p class=SuperCoolStuff&gt;&lt;span class=sentence&gt;This is a sample of a &lt;a href=&quot;#&quot;&gt;link&lt;/a&gt; 
getting chewed up.&lt;/span&gt;&lt;/p&gt;&lt;a&gt; other stuff&lt;/a&gt;
</code></pre>
",""
"16391584","2013-05-06 03:27:25","0","","16381218","<p>I ended up finding out that these correspond to the senses in WordNet 1.7, which is pretty archaic (doesn't seem easily installable on Mac OS X or Ubuntu 11.04). </p>

<p>There are no online versions of WordNet 1.7 that I could find.</p>

<p>This site also has some useful information about these three corpora. For example, it says that the six senses of <code>interest</code> were taken from the  Longman English Dictionary Online (circa 2001). See <a href=""http://www.d.umn.edu/~tpederse/data.html"" rel=""nofollow"">here</a></p>

<p>It describes the source of HARD as WordNet 1.7.</p>

<p>Ultimately, I ended up manually mapping the definitions to those in WordNet 3.0. If you're interested, here's the dictionary. Note, however, that I'm not an expert on linguistics, and they're not exact</p>

<pre><code># A map of SENSEVAL senses to WordNet 3.0 senses.
# SENSEVAL-2 uses WordNet 1.7, which is no longer installable on most modern
# machines and is not the version that the NLTK comes with.
# As a consequence, we have to manually map the following
# senses to their equivalent(s).
SV_SENSE_MAP = {
    ""HARD1"": [""difficult.a.01""],    # not easy, requiring great physical or mental
    ""HARD2"": [""hard.a.02"",          # dispassionate
              ""difficult.a.01""],
    ""HARD3"": [""hard.a.03""],         # resisting weight or pressure
    ""interest_1"": [""interest.n.01""], # readiness to give attention
    ""interest_2"": [""interest.n.03""], # quality of causing attention to be given to
    ""interest_3"": [""pastime.n.01""],  # activity, etc. that one gives attention to
    ""interest_4"": [""sake.n.01""],     # advantage, advancement or favor
    ""interest_5"": [""interest.n.05""], # a share in a company or business
    ""interest_6"": [""interest.n.04""], # money paid for the use of money
    ""cord"": [""line.n.18""],          # something (as a cord or rope) that is long and thin and flexible
    ""formation"": [""line.n.01"",""line.n.03""], # a formation of people or things one beside another
    ""text"": [""line.n.05""],                 # text consisting of a row of words written across a page or computer screen
    ""phone"": [""telephone_line.n.02""],   # a telephone connection
    ""product"": [""line.n.22""],       # a particular kind of product or merchandise
    ""division"": [""line.n.29""],      # a conceptual separation or distinction
    ""SERVE12"": [""serve.v.02""],       # do duty or hold offices; serve in a specific function
    ""SERVE10"": [""serve.v.06""], # provide (usually but not necessarily food)
    ""SERVE2"": [""serve.v.01""],       # serve a purpose, role, or function
    ""SERVE6"": [""service.v.01""]      # be used by; as of a utility
}
</code></pre>
",""
"16208956","2013-04-25 07:38:02","0","","16026881","<p>I've finally ended up writing my own helper function to get the spans out my original string:</p>

<pre><code>public HashMap&lt;Integer, TokenSpan&gt; getTokenSpans(String text, Tree parse)
{
    List&lt;String&gt; tokens = new ArrayList&lt;String&gt;();
    traverse(tokens, parse, parse.getChildrenAsList());
    return extractTokenSpans(text, tokens);
}

private void traverse(List&lt;String&gt; tokens, Tree parse, List&lt;Tree&gt; children)
{
    if(children == null)
        return;
    for(Tree child:children)
    {
        if(child.isLeaf())
        {
            tokens.add(child.value());
        }
        traverse(tokens, parse, child.getChildrenAsList());         
    }
}

private HashMap&lt;Integer, TokenSpan&gt; extractTokenSpans(String text, List&lt;String&gt; tokens)
{
    HashMap&lt;Integer, TokenSpan&gt; result = new HashMap&lt;Integer, TokenSpan&gt;();
    int spanStart, spanEnd;

    int actCharIndex = 0;
    int actTokenIndex = 0;
    char actChar;
    while(actCharIndex &lt; text.length())
    {
        actChar = text.charAt(actCharIndex);
        if(actChar == ' ')
        {
            actCharIndex++;
        }
        else
        {
            spanStart = actCharIndex;
            String actToken = tokens.get(actTokenIndex);
            int tokenCharIndex = 0;
            while(tokenCharIndex &lt; actToken.length() &amp;&amp; text.charAt(actCharIndex) == actToken.charAt(tokenCharIndex))
            {
                tokenCharIndex++;
                actCharIndex++;
            }

            if(tokenCharIndex != actToken.length())
            {
                //TODO: throw exception
            }
            actTokenIndex++;
            spanEnd = actCharIndex;
            result.put(actTokenIndex, new TokenSpan(spanStart, spanEnd));
        }
    }
    return result;
}
</code></pre>

<p>Then I will call </p>

<pre><code> getTokenSpans(originalString, parse)
</code></pre>

<p>So I get a map, which can map every token to its corresponding token span.
It's not an elegant solution, but at least it works.</p>
",""
"16093199","2013-04-18 21:14:31","1","","16074238","<p>I finally realized that tokenization/segmentation is not included in this pos tagger. It appears the words must be space delimited before feeding them to the tagger. For those interested in maximum entropy word segmentation of Chinese, there is a separate package available here:</p>

<p><a href=""http://nlp.stanford.edu/software/segmenter.shtml"">http://nlp.stanford.edu/software/segmenter.shtml</a></p>

<p>Thanks everyone.</p>
",""
"16064369","2013-04-17 15:38:36","6","","16062511","<p>Without more context explaining what you are trying to accomplish, there is absolutely no reason to remove stop words. Most applications where you need POS tags need them for all of the input text, including the stop words.</p>
",""
"15938300","2013-04-10 23:31:15","1","","15931765","<p>See the <a href=""http://bulba.sdsu.edu/jeanette/thesis/PennTags.html"" rel=""nofollow"">Penn Treebank Tagset</a> and the <a href=""ftp://ftp.cis.upenn.edu/pub/treebank/doc/manual/root.ps.gz"" rel=""nofollow"">treebank annotation guidelines</a></p>
",""
"15931170","2013-04-10 16:16:35","1","","15916143","<p>Grammar checking is an active area of NLP research, so there isn't a 100% answer (maybe not even an 80% answer) at this time. The simplest approach (or at least a reasonable baseline) would be an n-gram language model (normalizing LM probabilities for utterance length and setting a heuristic threshold for 'grammatical' or 'ungrammatical'.</p>

<p>You could use Google's n-gram corpus, or train your own on in-domain data. You might be able to do that with NLTK; you definitely could with LingPipe, the SRI Language Modeling Toolkit, or OpenGRM.</p>

<p>That said, an n-gram model won't perform all that well. If it meets your needs, great, but if you want to do better, you'll have to train a machine-learning classifier. A grammaticality classifier would generally use features from syntactic and/or semantic processing (e.g. POS-tags, dependency and constituency parses, etc.) You might look at some of the work from Joel Tetrault and the team he worked with at ETS, or Jennifer Foster and her team at Dublin.</p>

<p>Sorry there isn't an easy and straightforward answer...</p>
",""
"15902853","2013-04-09 13:09:46","0","","15827947","<p>When you start talking about paragraphs or documents, often times rather than word senses you are really interested in topics. You could use a collection of word senses assigned to the words in a paragraph to help you figure out the topic, so the problems are connected, although it's probably better to think in terms of topics rather than senses (which are mostly associated with words or perhaps phrases).</p>

<p>Good luck,
Ted</p>
",""
"15867989","2013-04-07 21:22:29","0","","15867808","<p>As per comment. </p>

<p>please check:
<a href=""https://stackoverflow.com/questions/1441562/detect-language-from-string-in-php"">Detect language from string in PHP</a></p>

<p>or:</p>

<p><a href=""http://wiki.apache.org/solr/LanguageDetection"" rel=""nofollow noreferrer"">http://wiki.apache.org/solr/LanguageDetection</a></p>

<p>Solr can give you language with probability (for example this sentence is 90% English or 10% Turkish)</p>
",""
"15800786","2013-04-04 00:52:41","0","","15727144","<p>The main first piece of advice is to use the <code>wsj-0-18-left3words-distsim.tagger</code> (or probably better, the <code>english-left3words-distsim.tagger</code> in recent versions, for general text), rather than the <code>wsj-0-18-bidirectional-distsim.tagger</code>. While the tagging performance of the bidirectional tagger is <em>fractionally</em> better, it is about 6 times slower and uses about twice as much memory. A figure FWIW: on a 2012 MacBook Pro, when given enough text to ""warm up"" the <code>left3words</code> tagger will tag text at about 35000 words per second.</p>

<p>The other piece of advice on memory use is that if you have a large amount of text, make sure you pass it to <code>tagString()</code> in reasonable-sized chunks, not all as one huge String, since that whole String will be tokenized at once, adding to the memory requirements.</p>
",""
"15770184","2013-04-02 17:14:19","0","","15768680","<p>From the documentation of RegexpParser:</p>

<blockquote>
  <p>The patterns of a clause are executed in order.  An earlier
    pattern may introduce a chunk boundary that prevents a later
    pattern from executing.  Sometimes an individual pattern will
    match on multiple, overlapping extents of the input.  As with
    regular expression substitution more generally, the chunker will
    identify the first match possible, then continue looking for matches
    after this one has ended.</p>
  
  <p>The clauses of a grammar are also executed in order.  A cascaded
    chunk parser is one having more than one clause.  The maximum depth
    of a parse tree created by this chunk parser is the same as the
    number of clauses in the grammar.</p>
</blockquote>

<p>That is, each clause/pattern is executed <em>once</em>. Thus you'll run into trouble as soon as you need the output of a later clause to be matched by an earlier one.</p>

<p>A practical example is the way something that could be a complete sentence on its own can be used as a clause in a larger sentence:</p>

<blockquote>
  <p>The cat purred.</p>
  
  <p>He heard that the cat purred.</p>
  
  <p>She saw that he heard that the cat purred.</p>
</blockquote>

<p>As we can read from the documentation above, when you construct a RegexpParser you're setting an arbitrary limit for the ""depth"" of this sort of sentence. There is no ""recursion limit"" for context-free grammars.</p>

<p>The documentation mentions that you can use looping to mitigate this somewhat -- if you run through a suitable grammar two or three or four times, you can get a deeper parse. You can add external logic to loop your grammar many times, or until nothing more can be parsed.</p>

<p>However, as the documentation also notes, the basic approach of this parser is still ""greedy"". It proceeds like this for a fixed or variable number of steps:</p>

<ul>
<li>Do as much chunking as you can in one step.</li>
<li>Use the output of the last step as the input of the next step, and repeat.</li>
</ul>

<p>This is na√Øve because if an early step makes a mistake, this will ruin the whole parse.</p>

<p>Think of a ""garden path sentence"":</p>

<blockquote>
  <p>The horse raced past the barn fell.</p>
</blockquote>

<p>And a similar string but an entirely different sentence:</p>

<blockquote>
  <p>The horse raced past the barn.</p>
</blockquote>

<p>It will likely be hard to construct a RegexpParser that will parse both of these sentences, because the approach relies on the initial chunking being correct. Correct initial chunking for one will probably be incorrect initial chunking for the other, yet you can't know ""which sentence you're in"" until you're at a late level in the parsing logic.</p>

<p>For instance, if ""the barn fell"" is chunked together early on, the parse will fail.</p>

<p>You can add external logic to backtrack when you end up with a ""poor"" parse, to see if you can find a better one. However, I think you'll find that at that point, more of the important parts of the parsing algorithm are in your external logic, instead of in RegexpParser.</p>
",""
"15651748","2013-03-27 04:08:20","0","","15625509","<p>Firstly, you have to tackle the problem of getting words into lemmas and then into Synsets, i.e. how can you identify a synset from a word?</p>

<pre><code>word =&gt; lemma =&gt; lemma.pos.sense =&gt; synset    
Waiters =&gt; waiter =&gt; 'waiter.n.01' =&gt; wn.Synset('waiter.n.01')
</code></pre>

<p>So let's say you have already deal with the above problem and arrived at the right most representation of <code>waiter</code>, then you can continue to compare synsets. Do note that, a word can have many synsets</p>

<pre><code>from nltk.corpus import wordnet as wn
waiter = wn.Synset('waiter.n.01')
employee = wn.Synset('employee.n.01')

all_hyponyms_of_waiter = list(set([w.replace(""_"","" "") for s in waiter.closure(lambda s:s.hyponyms()) for w in s.lemma_names]))
all_hyponyms_of_employee = list(set([w.replace(""_"","" "") for s in employee.closure(lambda s:s.hyponyms()) for w in s.lemma_names]))

if 'waiter' in all_hyponyms_of_employee:
  print 'employee more general than waiter'
elif 'employee' in all_hyponyms_of_waiter:
  print 'waiter more general than employee'
else:
  print ""The SUMO ontology used in wordnet just doesn't have employee or waiter under the same tree""
</code></pre>
",""
"15624344","2013-03-25 20:41:10","3","","15594626","<p>I hope the pseudocode I present below proves helpful to you. If I find time, I'd also write some code for you.</p>

<p>This problem can be tackled by following the steps below:</p>

<ol>
<li><p>Create a dictionary of all the common sentence patterns in the English language. For example, <strong>Subject + Verb</strong> is an English pattern and all the sentences like <code>I sleep</code>, <code>Dog barked</code> and <code>Ship will arrive</code> match the <strong>S-V pattern</strong>. You can find a list of the most common english patterns <a href=""http://englishpatterns.com/community/viewtopic.php?f=7&amp;t=1693"" rel=""nofollow"">here</a>. Please note that for some time you may need to keep revising this dictionary to enhance the accuracy of your program.</p></li>
<li><p>Try to fit the input sentence in one of the patterns in the dictionary you created above, for example, if the input sentence is <code>Snakes, unlike elephants, are venomous.</code>, then your code must be able to find a match with the pattern: <code>Subject</code><strong>, unlike</strong> <code>AnotherSubject</code>, <code>Verb</code> <code>Object</code> or <strong>S-,unlike-S`-, -V-O</strong>. To successfully perform this step, you may need to write code that's good at spotting <strong>Structure Markers</strong> like the word <strong>unlike</strong>, in this example sentence.</p></li>
<li><p>When you have found a match for your input sentence in your pattern dictionary, you can easily assign a tag to each word in the sentence. For example, in our sentence, the word <code>Snakes</code> would be tagged as a <strong>subject</strong>, just like the word <code>elephants</code>, the word <code>are</code> would be tagged as a <strong>verb</strong> and finally the word <code>venomous</code> would be tagged as an <strong>object</strong>.</p></li>
<li><p>Once you have assigned a unique tag to each of the words in your sentence, you can go lookup the word in the appropriate text files that you already have and determine whether or not your sentence is valid.</p></li>
<li><p>If your sentence doesn't match any sentence pattern, then you have two options:</p>

<p>a) Add the pattern of this unrecognized sentence in your pattern dictionary if it is a valid English sentence.</p>

<p>b) Or, discard the input sentence as an invalid English sentence.</p></li>
</ol>

<p>Things like what you're trying to achieve are best solved using machine learning techniques so that the system can <strong>learn</strong> any new patterns. So, you may want to include a trainer system that would add a new pattern to your pattern dictionary whenever it finds a valid English sentence not matching any of the existing patterns. I haven't thought much about how this can be done, but for now, you may manually revise your Sentence Pattern dictionary.</p>

<p>I'd be glad to hear your opinion about this pseudocode and would be available to brainstorm it further.</p>
",""
"15590384","2013-03-23 18:15:42","7","","15586721","<p>First of all, you can use <code>nltk.pos_tag()</code> directly without training it. 
The function will load a pretrained tagger from a file. You can see the file name 
with <code>nltk.tag._POS_TAGGER</code>:</p>

<pre><code>nltk.tag._POS_TAGGER
&gt;&gt;&gt; 'taggers/maxent_treebank_pos_tagger/english.pickle' 
</code></pre>

<p>As it was trained with the Treebank corpus, it also uses the <a href=""https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"" rel=""noreferrer"">Treebank tag set</a>.</p>

<p>The following function would map the treebank tags to WordNet part of speech names: </p>

<pre><code>from nltk.corpus import wordnet

def get_wordnet_pos(treebank_tag):

    if treebank_tag.startswith('J'):
        return wordnet.ADJ
    elif treebank_tag.startswith('V'):
        return wordnet.VERB
    elif treebank_tag.startswith('N'):
        return wordnet.NOUN
    elif treebank_tag.startswith('R'):
        return wordnet.ADV
    else:
        return ''
</code></pre>

<p>You can then use the return value with the lemmatizer:</p>

<pre><code>from nltk.stem.wordnet import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
lemmatizer.lemmatize('going', wordnet.VERB)
&gt;&gt;&gt; 'go'
</code></pre>

<p>Check the return value before passing it to the Lemmatizer because an empty string would give a <code>KeyError</code>. </p>
",""
"15508322","2013-03-19 19:06:40","0","","15503388","<p>I think there are two problems: first, the scripts should have ""-utf8"" in their name, e.g. <code>cmd/tagger-chunker-german-utf8</code>, because you downloaded the UTF-8 data. Second, tagging and chunking requires a data file each. See the homepage which has a section ""Parameter files for PC"" and ""Chunker parameter files for PC"" - download the files from both sections, then re-execute <code>install-tagger.sh</code>.</p>
",""
"15449643","2013-03-16 13:06:29","2","","15431139","<p>The <code>Tree</code> class has a <a href=""http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/trees/Tree.html#score%28%29"" rel=""nofollow"">score</a> method you can call to get the score of the sentence.</p>

<pre><code>double score = parse.score();
</code></pre>
",""
"15403585","2013-03-14 07:35:48","0","","15036048","<p>IDF is obtained by dividing the total number of documents by the number of documents containing the term, and then taking the logarithm of that quotient. In your case, all the documents has term0, so IDF for term0 is log(1), equal to 0. So in your doc-term matrix, the column for term0 is all zeros. </p>

<p>A term that appears in all documents has zero weight, it carries absolutely no information. </p>
",""
"15321784","2013-03-10 11:38:11","1","","15320894","<p>You need to have <strong>numpy</strong> installed:</p>

<pre><code>from nltk.classify.maxent import (MaxentClassifier,... 
</code></pre>

<p>is behind </p>

<pre><code>import numpy 
</code></pre>

<p>you should install numpy and be able to do import numpy without error.</p>
",""
"15169584","2013-03-02 01:22:09","0","","15169472","<p>Start by caching things:</p>

<pre><code># Move these outside of the class declaration or make them class variables

stopwords = set(stopwords.words('english'))
grammar = ""NP: {}""
cp = nltk.RegexpParser(grammar)
</code></pre>

<p>This can be sped up a little as well:</p>

<pre><code>from itertools import ifilterfalse

...

keywords_without_stopwords = ifilterfalse(stopwords.__contains__, keywords)

return list(keywords_without_stopwords + set(tags))  # Can you cache `set(tags`)?
</code></pre>

<p>I'd also take a look at Flask-Cache in order to memoize and cache functions and views as much as possible.</p>
",""
"15154234","2013-03-01 09:17:10","0","","14802442","<p>Here is tri_gram tagger which is backed off by bi-gram (which is backed off by uni-gram) and the primary back-off tragger being the regex tragger. So, the last tagging here will be left to regex if any of the other tagger fails to tag it on the basis of rules defined here. Hope this helps you to build your own regex tagger of your rules. </p>

<pre><code>   from nltk.corpus import brown
   import sys
   from nltk import pos_tag
   from nltk.tokenize import word_tokenize
   import nltk
   from nltk import ne_chunk
   def tri_gram():
   ##Trigram tagger done by training data from brown corpus 
    b_t_sents=brown.tagged_sents(categories='news')

   ##Making n-gram tagger using Turing backoff
   default_tagger = nltk.RegexpTagger(
            [(r'^-?[0-9]+(.[0-9]+)?$', 'CD'),   # cardinal numbers
         (r'(The|the|A|a|An|an)$', 'AT'),   # articles
         (r'.*able$', 'JJ'),                # adjectives
         (r'.*ness$', 'NN'),                # nouns formed from adjectives  
         (r'.*ly$', 'RB'),                  # adverbs
         (r'.*s$', 'NNS'),                  # plural nouns  
         (r'.*ing$', 'VBG'),                # gerunds   
         (r'.*ed$', 'VBD'),                 # past tense verbs
         (r'.*', 'NN')                      # nouns (default)
        ])
    u_gram_tag=nltk.UnigramTagger(b_t_sents,backoff=default_tagger) 
    b_gram_tag=nltk.BigramTagger(b_t_sents,backoff=u_gram_tag)
    t_gram_tag=nltk.TrigramTagger(b_t_sents,backoff=b_gram_tag)

    ##pos of given text
    f_read=open(sys.argv[1],'r')
    given_text=f_read.read();
    segmented_lines=nltk.sent_tokenize(given_text) 
    for text in segmented_lines:
        words=word_tokenize(text)
        sent = t_gram_tag.tag(words)
        print ne_chunk(sent)
tri_gram()
</code></pre>
",""
"14935235","2013-02-18 11:28:58","0","","14933345","<p>Did you try googling? The top hits I get either contain stopword lists or are stack overflow posts that link to said lists:</p>

<ul>
<li><a href=""https://stackoverflow.com/questions/1218335/stop-words-list-for-english"">here</a></li>
<li><a href=""http://www.ranks.nl/resources/stopwords.html"" rel=""nofollow noreferrer"">and here</a></li>
<li><a href=""http://dev.mysql.com/doc/refman/5.1/en/fulltext-stopwords.html"" rel=""nofollow noreferrer"">and here</a></li>
</ul>
",""
"14831206","2013-02-12 11:13:20","4","","14822609","<p>Use a dictionary which maps words to numeric ids. If your vocabulary has 10,000 items in it, your dictionary maps each word to a number in the range 0-9999 and every word is represented as a binary vector of length 10,000 where only one element is active: that corresponding to the word's id in the dictionary.</p>

<p>If you want extra features besides word ids, you can just tack these on to the end of the feature vector: that is, you can make features 10,000+ be the capitalisation feature, the previous tag feature (will need binary coding as above) etc.</p>

<p>As a final point, POS tagging is an instance of a <a href=""http://en.wikipedia.org/wiki/Structured_prediction"">structured prediction</a> problem, rather than a series of independent classifications. If this becomes more than an academic exercise, you'll want to move to the <a href=""http://www.cs.columbia.edu/~mcollins/courses/6998-2012/lectures/lec5.1.pdf"">structured perceptron</a>, or another structured learning method like a CRF or struct-SVM.</p>

<p>EDIT: a simple example</p>

<p>Imagine I have a five word vocabulary, {the,cat,sat,on,mat}, and a reduced tagset {DET,N,V,PREP}. My sentence is thus:</p>

<p>(The,DET) (cat,N) (sat,V) (on,PREP) (the,DET) (mat,N).</p>

<p>Now I want a feature vector for each word, from which I would like to be able to predict the tag. I am going to use features 0-4 as my word id indicator functions, so that feature 0 corresponds to 'the', feature 1 to 'cat' and so on. This gives me the following feature vectors (with the intended 'class' or tag assignment following the ->):</p>

<pre><code>[1 0 0 0 0] -&gt; DET
[0 1 0 0 0] -&gt; N
[0 0 0 0 0] -&gt; V
...
</code></pre>

<p>I could treat these as instances and apply my learning algorithm of choice to this task, <em>however</em>, word ID functions alone won't buy me much. I decide I want to incorporate some HMM-like intuition into my classifications, so I also add feature functions which indicate what the <em>previous</em> tag was. So I use features 5-8 as indicators for this, with 5 corresponding to DET, 6 to N, and so on. Now I have the following:</p>

<pre><code>[1 0 0 0 0 0 0 0 0] -&gt; DET (because this is the first word there's no previous tag)
[0 1 0 0 0 1 0 0 0] -&gt; N
[0 0 0 0 0 0 1 0 0] -&gt; V
</code></pre>

<p>Now I can keep adding features to my heart's content, using for example feature 9 to indicate whether the word is capitalised or not, feature 10 might be whether the word matches a list of known proper nouns, etc etc. If you read a little about structured prediction tasks and methods, you should see why using a model customised for this task (easiest is an HMM, but I'd want to progress to a CRF/Structured Perceptron/StructSVM for state of the art performance) is superior to treating this as a bunch of independent decisions.</p>
",""
"14769695","2013-02-08 09:44:54","0","","14760902","<p><a href=""http://www.cse.unsw.edu.au/~chak/haskell/ctk/"" rel=""nofollow"">CTK</a> is an equivalent of parsec but for lexing. It supports adding new combinators dynamically.</p>
",""
"14548480","2013-01-27 14:42:22","0","","14540630","<p>As Steve mentioned in the comment, the best answer (and the ML-style way) is to try !</p>

<p>That being said, I'd start with binary features. The goal of your ML model like SVM is to determine the ""weight"" of these features, so if it is efficient, you don't have to try to set this weight in advance (with TFIDF or other).</p>
",""
"14544057","2013-01-27 03:13:41","0","","14539609","<p>It seems that the newer version available in GitHub works seamlessly.</p>

<p>In the 2.0.4 code the <code>i += 4</code> line is probably a bug.</p>

<p>In order to get NLTK working, download the source code from GitHub and <code>python setup.py install</code> it.</p>

<p>Be sure to set <code>CANDCHOME</code> variable to the <code>bin/</code> dir of your <code>candc</code> and <code>boxer</code> tools, and the models at the previous folder (the path should be <code>$CANDCHOME/../models</code>).</p>
",""
"14533498","2013-01-26 03:17:25","0","","14529782","<p>Here try this:</p>

<pre><code>import nltk
pattern = [(r'(March)$','MAR')]
tagger = nltk.RegexpTagger(pattern)
print tagger.tag(nltk.word_tokenize('He was born in March 1991'))
</code></pre>

<p>You have to tokenize the words.</p>

<p>This is the output I get:</p>

<pre><code>[('He', None), ('was', None), ('born', None), ('in', None), ('March', 'MAR'), ('1991', None)]
</code></pre>
",""
"14529735","2013-01-25 20:20:39","13","","14529633","<p>There are a few things going on here:</p>

<ul>
<li>You call <code>test_lang()</code> on every letter in the string twice, this is probably the main reason this is slow.</li>
<li>Concatenating strings in Python isn't very efficient, you should instead use a list or generator and then use <a href=""http://docs.python.org/2/library/stdtypes.html#str.join"" rel=""nofollow""><code>str.join()</code></a> (most likely, <code>''.join()</code>).</li>
</ul>

<p>Here is the approach I would take, using <a href=""http://docs.python.org/2/library/itertools.html#itertools.groupby"" rel=""nofollow""><code>itertools.groupby()</code></a>:</p>

<pre><code>from itertools import groupby
def keyfunc(letter):
    return (test_lang(letter), letter in punc)

cleaned = ' '.join(''.join(g) for k, g in groupby(letters, keyfunc))
</code></pre>

<p>This will group the letters into consecutive letters of the same language and whether or not they are punctuation, then <code>''.join(g)</code> converts each group back into a string, then <code>' '.join()</code> combines these strings adding a space between each string.</p>

<p>Also, as noted in comments by DSM, make sure that <code>punc</code> is a set.</p>
",""
"14510125","2013-01-24 20:27:42","0","","14510028","<p>It looks like nltk will give you a list of words and their parts of speech.  Since you are only interested in nouns? <a href=""https://nltk.googlecode.com/svn/trunk/doc/howto/tag.html"" rel=""nofollow"">this</a> will provide you with them</p>

<pre><code>&gt;&gt;&gt; from nltk import pos_tag, word_tokenize
&gt;&gt;&gt; pos_tag(word_tokenize(""John's big idea isn't all that bad."")) 
[('John', 'NNP'), (""'s"", 'POS'), ('big', 'JJ'), ('idea', 'NN'), ('is',
'VBZ'), (""n't"", 'RB'), ('all', 'DT'), ('that', 'DT'), ('bad', 'JJ'),
('.', '.')]
</code></pre>
",""
"14508096","2013-01-24 18:22:45","3","","14506969","<p>It seems that the saved word tokenizer requires <a href=""http://www.numpy.org/"" rel=""noreferrer"">numpy</a>. You'll need to <a href=""http://sourceforge.net/projects/numpy/files/NumPy/"" rel=""noreferrer"">install it</a>.</p>
",""
"14490891","2013-01-23 22:51:20","0","","14473017","<p>I posted to stanford parser mailing list and I received an answer from John Bauer (thanks, John !) </p>

<p><em>John Bauer 
2:09 PM (39 minutes ago)
to me, parser-user 
Unfortunately, you would need to start training from the beginning.  <strong>There is no way to extend a current parser model.</strong>
That feature is on ""the list"", but it's somewhere near the back, so don't hold your breath...
John</em></p>
",""
"14429212","2013-01-20 20:53:44","4","","14428756","<p>The reason you can't just use the <code>left</code> and <code>hoistEither</code> functions directly is that unlike <code>StateT</code> and <code>ReaderT</code> from the <a href=""http://hackage.haskell.org/package/mtl"" rel=""noreferrer""><code>mtl</code></a> package, the <a href=""http://hackage.haskell.org/package/either"" rel=""noreferrer""><code>either</code></a> package doesn't provide a typeclass similar to <code>MonadReader</code> or <code>MonadState</code>.</p>

<p>The aforementioned typeclasses take care of lifting in the monad stack transparently, but for <code>EitherT</code>, you have to do the lifting yourself (or write a <code>MonadEither</code> typeclass similar to <code>MonadReader</code> et al).</p>

<pre><code>faultyFunction :: String -&gt; Stuff String
faultyFunction s = do
  when (s == ""left"") $ Stuff $¬†lift $¬†lift $¬†left ""breaking out""
  return ""right""
</code></pre>

<p>First you need to apply the <code>Stuff</code> wrapper, then <code>lift</code> over the <code>ReaderT</code> transformer and then <code>lift</code> again over the <code>StateT</code> transformer.</p>

<p>You probably want to write utility functions for yourself such as</p>

<pre><code>stuffLeft :: T.Text -&gt; Stuff a
stuffLeft = Stuff . lift . lift . left
</code></pre>

<p>Then you can simply use it like this:</p>

<pre><code>faultyFunction :: String -&gt; Stuff String
faultyFunction s = do
  when (s == ""left"") $ stuffLeft ""breaking out""
  return ""right""
</code></pre>

<p>Alternatively, you could use <a href=""http://hackage.haskell.org/packages/archive/mtl/2.1.2/doc/html/Control-Monad-Error.html"" rel=""noreferrer""><code>Control.Monad.Error</code></a> from <code>mtl</code>, if you define an <code>Error</code> instance for <code>Text</code>.</p>

<pre><code>instance Error T.Text where
  strMsg = T.pack
</code></pre>

<p>Now you can change the definition of <code>Stuff</code> implement <code>left</code> and <code>hoistEither</code> like this:</p>

<pre><code>newtype Stuff a = Stuff {
  runStuff :: (ReaderT StuffConfig (StateT StuffState (ErrorT T.Text IO))) a
} deriving (Monad, Functor, Applicative,
            MonadIO,
            MonadReader StuffConfig,
            MonadState StuffState,
            MonadError T.Text
            )

left :: T.Text -&gt; Stuff a
left = throwError

hoistEither :: Either T.Text a -&gt; Stuff a
hoistEither = Stuff . lift . lift . ErrorT . return
</code></pre>

<p>With this your original <code>faultyFunction</code> type-checks without any manual lifting.</p>

<p>You can also write generic implementations for <code>left</code> and <code>hoistEither</code> which work for any instance of <code>MonadError</code> (using <code>either</code> from <code>Data.Either</code>):</p>

<pre><code>left :: MonadError e m =&gt; e -&gt; m a
left = throwError

hoistEither :: MonadError e m =&gt; Either e a -&gt; m a
hoistEither = either throwError return
</code></pre>
",""