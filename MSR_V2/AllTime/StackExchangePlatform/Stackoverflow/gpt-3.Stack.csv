Post Link,Title,CreationDate,AcceptedAnswerId,Score,ViewCount,Tags,Body,TagName
"78826190","How to use Azure OpenAI APIs to programmatically provide editorial review for my MD web pages","2024-08-02 15:21:01","","0","42","<azure><azure-openai><gpt-3>","<p>I have a few Jekyll websites for smaller open source projects, and wanted to write a local Node.js script, that leverages Azure OpenAI to effectively provide editorial updates to the contents of my web pages (as MD files).</p>
<p>What i want, is to defined prompts for various content elements of the pag that describe what i want done to them (ex psuedo prompt: Here is the page title, optimize for SEO, and limit to 60 characters: &quot;Learn how to get started with development on X&quot;. only return the updated title and nothing else.).</p>
<p>I've done this, and sent them off to Azure OpeAI APIs, using the gpt-350-turbo model and the Azure completions API, and I get responses that are close, but they have random &quot;junk&quot; content in there as well (even tho i explicitly specify NOT to in the prompt).</p>
<p>For example, i might get back someting like:</p>
<pre><code>&quot;choices&quot;: [
    {
      &quot;text&quot;: &quot;Example response (correct):\Get started developing your solution using X\n\n\n\nFor the given&quot;,
      &quot;index&quot;: 0,
      &quot;finish_reason&quot;: &quot;length&quot;,
      &quot;logprobs&quot;: null
    }
</code></pre>
<p>As you can see, it puts &quot;junk&quot; like <code>Example response (correct):\</code> in front and sometimes weird characters or fragments of &quot;further responses&quot; (like <code>For the given</code>) below.</p>
<p>I assume i'm overlooking something simple; Am i using the wrong model? Is gpt too conversational for simple prompt -&gt; direct output? Is completions the right API?</p>
","gpt-3"
"78758291","AnswerRelevancyMetric not showing results on LLM evaluation","2024-07-17 07:59:22","","0","28","<metrics><large-language-model><azure-openai><gpt-3>","<p>i've got this code :</p>
<pre><code>from langchain_openai import AzureChatOpenAI
from deepeval.models.base_model import DeepEvalBaseLLM

class AzureOpenAI(DeepEvalBaseLLM):
    def __init__(
        self,
        model
    ):
        self.model = model

    def load_model(self):
        return self.model

    def generate(self, prompt: str) -&gt; str:
        chat_model = self.load_model()
        return chat_model.invoke(prompt).content

    async def a_generate(self, prompt: str) -&gt; str:
        chat_model = self.load_model()
        res = await chat_model.ainvoke(prompt)
        return res.content

    def get_model_name(self):
        return &quot;Custom Azure OpenAI Model&quot;

# Replace these with real values
custom_model = AzureChatOpenAI(
    deployment_name=azure_openai_model, 
    api_key=azure_openai_secret.value, 
    azure_endpoint=azure_openai_endpoint, 
    api_version=azure_openai_api_version, 
    verbose=False, 
    streaming=True,
    temperature=0,
    callbacks=[StreamingStdOutCallbackHandler()]
)
azure_openai = AzureOpenAI(model=custom_model)
</code></pre>
<p>As I want to evaluate it, I used this piece of code :</p>
<pre><code>from deepeval.metrics import AnswerRelevancyMetric
from deepeval.test_case import LLMTestCase

# Initialize a test case
     test_case = LLMTestCase(
     input=&quot;The dog chased the cat up the tree, who ran up the tree?&quot;,
     actual_output=&quot;It depends, some might consider the cat, while others might argue the dog.&quot;,
expected_output=&quot;The cat.&quot;know.&quot;],
)

# Initialize metric
metric_ = AnswerRelevancyMetric(model=azure_openai)
</code></pre>
<p>I also do a pip install :</p>
<pre><code>%pip install nest-asyncio
</code></pre>
<p>I'm referring to this link : <a href=""https://docs.confident-ai.com/docs/metrics-introduction#using-a-custom-llm"" rel=""nofollow noreferrer"">source code for the test</a></p>
<p>As to print the results from this evaluation, I use :</p>
<pre><code>import nest_asyncio
nest_asyncio.apply()
</code></pre>
<hr />
<pre><code>print(metric_.score)
#None    
print(metric_.measure(test_case))
</code></pre>
<p>And I get this error message :</p>
<pre><code>Canceled future for execute_request message before replies were done
The Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click here for more info. View Jupyter log for further details.
</code></pre>
","gpt-3"
"78614896","How To Train GPT-3 On Different Datasets For Different Clients","2024-06-12 20:46:05","","0","41","<artificial-intelligence><openai-api><gpt-3><fine-tuning>","<p>I am planning on building an ai saas product that would focus on CRM and customer support. However, the bot should know about the company that it is answering for.</p>
<p>I checked their docs and did research, but all I could find was fine-tuning. However, I am not sure how to fine tune with separate data-sets for different companies. This way, when someone contacts the support bot on client A's website, it should not use the company data from client b.</p>
<p>Please let me know if you have any information about how I could do this</p>
","gpt-3"
"78574662","Getting token exceed error while using OpenAI API","2024-06-04 10:01:11","","0","358","<c#><openai-api><gpt-3><gpt-4>","<p>I am trying to fetch data from the CSV file via OpenAI API but I am getting an error as token exceed. I have used both GPT-3.5 turbo and all the version of GPT-4 model to check on that.</p>
<p>Is there any changes what I need to do in the prompt or should I use some other method to reduce the size of the token. I have also set the maximum token to 2500. I am using Swagger to check my end point request</p>
<pre><code>
[Route(&quot;AskQuestionCsv&quot;)]
public async Task&lt;IActionResult&gt; AskQuestionCsv([FromBody] string question)
{
    if (string.IsNullOrWhiteSpace(extractedCsvText))
    {
        return BadRequest(new { Message = &quot;No CSV content available. Please upload a CSV file first.&quot; });
    }

    if (string.IsNullOrWhiteSpace(question))
    {
        return BadRequest(new { Message = &quot;Question cannot be empty.&quot; });
    }

    try
    {
        var openai = new OpenAIAPI(&quot;API_KEY&quot;);
        var chatRequest = new ChatRequest
        {
            Model = &quot;gpt-4&quot;, 
            Temperature = 0.7,
            MaxTokens = 25000,
            Messages = new List&lt;ChatMessage&gt;
            {
                new ChatMessage
                {
                    Role = ChatMessageRole.System,
                    Content = &quot;You are a helpful assistant.&quot;
                },
                new ChatMessage
                {
                    Role = ChatMessageRole.User,
                    Content = $&quot;Based on the following text from the CSV file, answer the question.\n\nCSV Text:\n{extractedCsvText}\n\nQuestion: {question}&quot;
                }
            }
        };

        var chatResponse = await openai.Chat.CreateChatCompletionAsync(chatRequest);
        var answer = chatResponse.Choices.FirstOrDefault()?.Message.Content.Trim();

        return Ok(new { Question = question, Answer = answer });
    }
    catch (Exception ex)
    {
        return StatusCode(500, new { Message = &quot;ERROR: &quot; + ex.Message }
    }

</code></pre>
<p><strong>Error:</strong></p>
<blockquote>
<p>Error at chat/completions (<a href=""https://api.openai.com/v1/chat/completions"" rel=""nofollow noreferrer"">https://api.openai.com/v1/chat/completions</a>) with HTTP status code: TooManyRequests. Content: {\n    &quot;error&quot;: {\n        &quot;message&quot;: &quot;Request too large for gpt-4 in organization org-uedxqeR1FzNcdHx3MOuawI9d on tokens per min (TPM): Limit 10000, Requested 6668686. The input or output tokens must be reduced in order to run successfully. Visit <a href=""https://platform.openai.com/account/rate-limits"" rel=""nofollow noreferrer"">https://platform.openai.com/account/rate-limits</a> to learn more.&quot;,\n        &quot;type&quot;: &quot;tokens&quot;,\n        &quot;param&quot;: null,\n        &quot;code&quot;: &quot;rate_limit_exceeded&quot;\n    }\n}\n</p>
</blockquote>
","gpt-3"
"78555704","MACD data problems","2024-05-30 14:42:43","","0","12","<r><gpt-3>","<p>I don't know the reason but there is an error appearing while I play my code in R.</p>
<pre><code># Pobieranie danych
filename &lt;- &quot;https://stooq.pl/q/d/l/?s=eat&amp;d1=20190101&amp;d2=202312310&amp;i=d&quot;
amrest &lt;- read.csv(filename)

# Konwersja daty
amrest$Data &lt;- as.Date(amrest$Data)

# Sortowanie danych po dacie
amrest &lt;- amrest[order(amrest$Data),]

# Wybieranie odpowiednich kolumn (bez wolumenu)
amrest &lt;- amrest[,c(1:5)] 
colnames(amrest) &lt;- c(&quot;Date&quot;, &quot;Open&quot;, &quot;High&quot;, &quot;Low&quot;, &quot;Close&quot;)

# Zmiana na format xts
amrest_xts &lt;- xts(amrest[,-1], order.by = amrest$Date)

# Sprawdzanie, czy xts jest prawid≈Çowo utworzony
print(head(amrest_xts))
print(tail(amrest_xts))

# Obliczanie MACD
macd_data &lt;- MACD(Cl(amrest_xts), nFast=12, nSlow=26, nSig=9, maType=&quot;EMA&quot;)

# Sprawdzanie, czy MACD zosta≈Ç poprawnie obliczony
print(head(macd_data))
print(tail(macd_data))

# ≈ÅƒÖczenie danych z MACD
amrest_xts &lt;- merge(amrest_xts, macd_data)

# Generowanie sygna≈Ç√≥w kupna i sprzeda≈ºy
signal &lt;- Lag(ifelse(amrest_xts$macd &gt; amrest_xts$signal, 1, -1))
signal[is.na(signal)] &lt;- 0

# Sygna≈Çy kupna i sprzeda≈ºy na przeciƒôciach
buy_signal &lt;- ifelse(signal == 1 &amp; Lag(signal) == -1, 1, 0)
sell_signal &lt;- ifelse(signal == -1 &amp; Lag(signal) == 1, 1, 0)

# Dodawanie sygna≈Ç√≥w do danych xts
amrest_xts$buy_signal &lt;- buy_signal
amrest_xts$sell_signal &lt;- sell_signal

# Wybieranie ostatnich 100 obserwacji do wykresu ≈õwiecowego
amrest_xts_short &lt;- tail(amrest_xts, 100)

# Wykres ≈õwiecowy z zaznaczonymi sygna≈Çami
candleChart(amrest_xts_short, multi.col=TRUE, theme='white', subset='last 3 months', TA=NULL)
addMACD()

# Dodawanie sygna≈Ç√≥w na wykresie
addTA(amrest_xts_short$buy_signal, col='green', type='p', pch=24, cex=1.5, on=1)
addTA(amrest_xts_short$sell_signal, col='red', type='p', pch=25, cex=1.5, on=1)

# Dodanie pogrubionego tytu≈Çu
mtext(&quot;Amrest - MACD Strategy&quot;, side=3, line=0.9, cex=1.25, font=2)

# Wykres z Simple Moving Average
chartSeries(amrest_xts_short, theme='white', TA=&quot;addSMA()&quot;, subset='last 3 months')
mtext(&quot;Amrest - SMA&quot;, side=3, line=0.9, cex=1.25, font=2)
</code></pre>
<p>The error is</p>
<pre><code>Error in seq.default(x@yrange[1], x@yrange[2], length.out = length(x.range)) : 
  'from' must be a finite number
In addition: Warning messages:
1: In min(x[, 1]) : no non-missing arguments to min; returning Inf
2: In max(x[, 1]) : no non-missing arguments to max; returning -Inf
3: In periodicity(x) : can not calculate periodicity of empty object
4: In periodicity(x) : can not calculate periodicity of empty object
5: In min(x) : no non-missing arguments to min; returning Inf
6: In max(x) : no non-missing arguments to max; returning -In
</code></pre>
<p>It is for my project. Actually I study finance and economy, however I have a subject of R coding for the Stock Market analyzing. The task was to analyze the Stock market ticket &quot;Amrest&quot; (Polish market) with MACD (Moving Average Convergence Divergence). I was trying that code generating by AI Chat-GPT3.5, and it was failed.</p>
","gpt-3"
"78548625","Handling Rate Limits with OpenAI API: Seeking Advice","2024-05-29 09:38:11","","0","92","<javascript><openai-api><gpt-3>","<p>I'm facing an issue with rate limits while using the OpenAI API. My application needs to process an array of payloads and return responses to users quickly.</p>
<ul>
<li>I've tried batch operations, but I'm not getting enough output.</li>
<li>Adding delays isn't an option as users expect responses within 10-15
seconds.</li>
</ul>
<p>How do other third-party applications handle unlimited requests with GPT-3.5? Are there any strategies or best practices to manage this situation effectively? Any advice or shared experiences would be greatly appreciated.</p>
<p>Thank you! üôè</p>
","gpt-3"
"78536552","TypeError: Cannot read properties of undefined (reading '0') , i integrate gpt-3.5-turbo with a chat web app in my backend file and this error came","2024-05-26 21:47:12","","0","46","<node.js><reverse-proxy><openai-api><chatgpt-api><gpt-3>","<p>I am building a project where I'm using chat engine and Open AI's gpt-3.5. Basically this is web based chat application where you can chat with normal user and there will be a option called chat with AI . After using chatgpt-3.5-turbo , I got to know that , the credit points for tokens are not enough to experiment. So I use reverse proxy method to use it for free. Below I'm sharing my code =&gt;</p>
<pre><code>import express from &quot;express&quot;;
import axios from &quot;axios&quot;;
import dotenv from &quot;dotenv&quot;;
import { openai } from &quot;../index.js&quot;;

dotenv.config();
const router = express.Router();

router.post(&quot;/text&quot;, async(req,res) =&gt; {
    try {
        const { text,activeChatId } =  req.body;
        console.log(&quot; ~ router.post ~ req.body:&quot;, req.body)
        const response = await openai.chat.completions.create({
            model:&quot;gpt-3.5-turbo&quot; ,
            prompt: text,
            temperature: 0.7,
            max_tokens: 2048,
            top_p: 1,
            frequency_penalty: 0.5,
            presence_penalty: 0,
            stream: true,
        });

        await axios.post(
            `https://api.openai.com/chats/${activeChatId}/messages/`,
            { text: response.choices[0].message.content },
            
            {
                headers: {
                              &quot;Project-ID&quot;: process.env.PROJECT_ID,
                              &quot;User-Name&quot;: process.env.BOT_USER_NAME,
                              &quot;User-Secret&quot;: process.env.BOT_USER_SECRET,
            }
        }
        );
      

        res.status(200).json({ text: response.choices[0].message.content })
  
    } catch (error) {
        console.error(&quot;1st error&quot;,error);
        res.status(500).json({ error: error.message })
    }
})
export default router;
</code></pre>
<p>my index.js file =&gt;</p>
<pre><code>import express from &quot;express&quot;;
import bodyParser from &quot;body-parser&quot;;
import cors from &quot;cors&quot;;
import dotenv from &quot;dotenv&quot;;
import helmet from &quot;helmet&quot;;
import morgan from &quot;morgan&quot;;
import OpenAI from &quot;openai&quot;;
import openAiRoutes from &quot;./routes/openai.js&quot;;
/*CONFIGURATION*/
dotenv.config();
const app = express();
app.use(express.json());
app.use(helmet());
app.use(helmet.crossOriginResourcePolicy({ policy: &quot;cross-origin&quot;}))
app.use(morgan(&quot;common&quot;));
app.use(bodyParser.json({ limit:&quot;30mb&quot;, extended:&quot;true&quot; }));
app.use(bodyParser.urlencoded({ limit:&quot;30mb&quot;, extended: &quot;true&quot; }));
app.use(cors());

// OPENAI CONFIGURATION
export const openai = new OpenAI({
  apiKey: process.env.OPEN_API_KEY,
    baseURL: &quot;http://localhost:3040/v1&quot;,
  });
app.use(&quot;/openai&quot;,openAiRoutes);

const PORT = process.env.PORT || 9000;
app.listen(PORT, () =&gt; {
    console.log(`It is running on Port: ${PORT}`);
});

</code></pre>
<p>When i try to run this code, it's giving me this error =&gt;</p>
<pre><code>::1 - - [26/May/2024:20:00:08 +0000] &quot;OPTIONS /openai/text HTTP/1.1&quot; 204 0
 ~ router.post ~ req.body: {
  attachments: [],
  created: '2024-05-26 20:00:08.331885+00:00',
  sender_username: 'testid',
  text: 'hi',
  activeChatId: 252894
}
1st error TypeError: Cannot read properties of undefined (reading '0')
    at file:///D:/Web%20D/Chat-app/server/routes/openai.js:26:37
    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)
::1 - - [26/May/2024:20:00:10 +0000] &quot;POST /openai/text HTTP/1.1&quot; 500 61

</code></pre>
<p>Can anybody give me any solution for this problem.</p>
","gpt-3"
"78523676","AttributeError: 'OpenAI' object has no attribute 'fine_tunes'","2024-05-23 14:00:15","","0","131","<jupyter-notebook><openai-api><large-language-model><gpt-3><fine-tuning>","<p>I work with Python in Jupyter Notebook 7.0.8 :</p>
<p>Here is the error generated by the following cell:</p>
<p>The error:</p>
<pre><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
Cell In[10], line 5
      2 dataset_path = r&quot;*****&quot;
      4 # Fine-tune the model with the examples from the dataset
----&gt; 5 fine_tuned_model_id = fine_tune_model_with_examples(dataset_path)
      6 print(f&quot;Fine-tuned model ID: {fine_tuned_model_id}&quot;)
      8 # Example source code to analyze

Cell In[9], line 34, in fine_tune_model_with_examples(dataset_path)
     26 # Launch fine-tuning
     27 fine_tuning_parameters = {
     28     &quot;training_file&quot;: train_file_id,
     29     &quot;validation_file&quot;: test_file_id,
     30     &quot;model&quot;: &quot;gpt-3.5-turbo&quot;,
     31     &quot;n_epochs&quot;: 4
     32 }
---&gt; 34 response = client.fine_tunes.create(**fine_tuning_parameters)
     35 return response['id']

AttributeError: 'OpenAI' object has no attribute 'fine_tunes'
</code></pre>
<p>The cell causing the error:</p>
<pre><code># Function to fine-tune the model
def fine_tune_model_with_examples(dataset_path):
    # Load and balance the dataset
    balanced_dataset = load_and_balance_dataset(dataset_path)

    # Split the dataset into training and test sets
    train_set, test_set = split_dataset(balanced_dataset)

    # Prepare fine-tuning data in chat format
    train_examples = prepare_fine_tuning_data(train_set)
    test_examples = prepare_fine_tuning_data(test_set)

    # Save training and test data to JSONL files
    save_to_jsonl(train_examples, &quot;train_fine_tuning_data.jsonl&quot;)
    save_to_jsonl(test_examples, &quot;test_fine_tuning_data.jsonl&quot;)

    # Upload training and test data to OpenAI
    with open(&quot;train_fine_tuning_data.jsonl&quot;, 'rb') as train_file:
        train_file_response = client.files.create(file=train_file, purpose='fine-tune')
    with open(&quot;test_fine_tuning_data.jsonl&quot;, 'rb') as test_file:
        test_file_response = client.files.create(file=test_file, purpose='fine-tune')

    train_file_id = train_file_response.id
    test_file_id = test_file_response.id

    # Launch fine-tuning
    fine_tuning_parameters = {
        &quot;training_file&quot;: train_file_id,
        &quot;validation_file&quot;: test_file_id,
        &quot;model&quot;: &quot;gpt-3.5-turbo&quot;,
        &quot;n_epochs&quot;: 4
    }

    response = client.fine_tunes.create(**fine_tuning_parameters)
    return response['id']
</code></pre>
<p>I made sure to update all the packages used.</p>
<p>However, it is indicated (<a href=""https://github.com/openai/openai-python/discussions/742"" rel=""nofollow noreferrer"">https://github.com/openai/openai-python/discussions/742</a>) among the changes made with the update of the OpenAI API that: <strong>openai.FineTune.create() is now client.fine_tunes.create()</strong>.</p>
<p>Does anyone have a solution to suggest to me?</p>
<p>Thank you in advance ;)</p>
","gpt-3"
"78513041","OpenAI Completions API: How do I extract the text from the response?","2024-05-21 15:56:11","78513064","0","287","<python><openai-api><gpt-3>","<p>I am attempting to extract text from OpenAI, but I need some help with the correct syntax.</p>
<p>My code:</p>
<pre><code>_model = &quot;gpt-3.5-turbo-instruct&quot;
# Summarize the text using an AI model (e.g., OpenAI's GPT)
try:
    summary = openai.completions.create(
        model=_model,
        prompt=f&quot;Summarize this text: {text}&quot;,
        max_tokens=150
    )
except Exception as e:
    return f&quot;summary: Exception {e}&quot;

_summary = summary.choices[0].message.content
#_summary = summary['choices'][0]['text']
</code></pre>
<p>I want to get a summary of the text. Is this the best way?</p>
<p>However, I find when I debug the code that the text object is located in the following structure within <code>summary['choices'][0]</code>:</p>
<pre><code>CompletionChoice(finish_reason='stop', index=0, logprobs=None, text='blah blah blah')
</code></pre>
<p>How do I extract the text from this in Python code?</p>
","gpt-3"
"78504532","OpenAI API error: How do I fix error 404 when I connect WhatsApp chatbot to OpenAI API?","2024-05-20 03:21:33","","0","73","<http-status-code-404><whatsapp><openai-api><gpt-3>","<p>How to fix the error in this code</p>
<pre><code>const axios = require('axios');
const { API_KEY_OPEN_AI } = require('../config');

const ChatAIHandler = async (text, msg) =&gt; {

    const cmd = text.split('/');

    if (cmd.length &lt; 2) {
        return msg.reply('Format Salah. ketik *#ask/your question*');
    }

    msg.reply('sedang diproses, tunggu bentar ya.');

    const question = cmd[1];
    const response = await ChatGPTRequest(question)

    if (!response.success) {
        return msg.reply(response.message);
    }

    return msg.reply(response.data);
}


const ChatGPTRequest = async (text) =&gt; {

    const result = {
        success: false,
        data: &quot;Aku gak tau&quot;,
        message: &quot;&quot;,
    }

    return await axios({
        method: 'post',
        url: 'https://api.openai.com/v1/completions',
        data: {
            model: &quot;text-davinci-003&quot;,
            prompt: text,
            max_tokens: 1000,
            temperature: 0
        },
        headers: {
            &quot;accept&quot;: &quot;application/json&quot;,
            &quot;Content-Type&quot;: &quot;application/json&quot;,
            &quot;Accept-Language&quot;: &quot;in-ID&quot;,
            &quot;Authorization&quot;: `Bearer ${API_KEY_OPEN_AI}`,
        },
    })
    .then((response) =&gt; {
        if (response.status == 200) {

            const { choices } = response.data;

            if (choices &amp;&amp; choices.length) {
                result.success = true;
                result.data = choices[0].text;
            }

        } else {
            result.message = &quot;Failed response&quot;;
        }

        return result;
    })
    .catch((error) =&gt; {
        result.message = &quot;Error : &quot; + error.message;
        return result;
    });
}

module.exports = {
    ChatAIHandler
}
</code></pre>
<p>I changed the latest API key, but the problem was not solved.</p>
","gpt-3"
"78448138","Restricting a RAG Chatbot's Responses to a Specific PDF","2024-05-08 11:25:22","","0","63","<prompt><langchain><chatgpt-api><gpt-3><retrieval-augmented-generation>","<p>I've created a chatbot using an OpenAI model and Langchain and instructed it via system messages to only answer questions related to a specific PDF. While it works initially, the chatbot eventually starts responding to unrelated queries. How can I ensure the chatbot remains restricted to answering questions solely based on the content of the designated PDF?</p>
","gpt-3"
"78366358","OpenAI ChatGPT (GPT-3.5) API error 400: Experiencing ""400 Bad Request"" Error when Sending Large Text Data to OpenAI API","2024-04-22 12:47:28","","0","311","<java><openai-api><chatgpt-api><gpt-3>","<p>I'm facing a &quot;400 Bad Request&quot; issue when attempting to send large text data to the OpenAI API for processing. Here's a overview of my project:</p>
<p>I have a project that integrates with the OpenAI API. The functionality allows users to send data along with questions, and the system retrieves responses based on the provided data and questions. However, when sending text data exceeding approximately 20000 characters, I consistently receive a &quot;400 Bad Request&quot; error. Strangely, smaller text data within the range of 20000 characters works perfectly fine</p>
<pre><code>// Method to send data to OpenAI API
public Result sendData() {
    ChatGptRequest chatGptRequest = new ChatGptRequest();
    List&lt;Message&gt; messages = new ArrayList&lt;&gt;();

    Message userMessage = new Message();
    userMessage.setRole(&quot;user&quot;);
    userMessage.setContent(&quot;What is the start and end date?&quot;);
    messages.add(userMessage);

    Message systemMessage = new Message();
    systemMessage.setRole(&quot;system&quot;);
    systemMessage.setContent(text); // 'text' contains the large text data
    messages.add(systemMessage);

    chatGptRequest.setMessages(messages);
    String response = ChatGptClient.chatGPT(chatGptRequest);
    // Handle response as needed
    return response;
}

// Method to interact with OpenAI API
public static String chatGPT(ChatGptRequest chatGptRequest) {
    String url = &quot;https://api.openai.com/v1/chat/completions&quot;;
    String apiKey = &quot;my_api_key&quot;;
    String model = &quot;gpt-3.5-turbo&quot;;

    try {
        URL openAIUrl = new URL(url);
        HttpURLConnection connection = (HttpURLConnection) openAIUrl.openConnection();
        connection.setRequestMethod(&quot;POST&quot;);
        connection.setRequestProperty(&quot;Authorization&quot;, &quot;Bearer &quot; + apiKey);
        connection.setRequestProperty(&quot;Content-Type&quot;, &quot;application/json&quot;);

        chatGptRequest.setModel(model);

        Gson gson = new Gson();
        String body = gson.toJson(chatGptRequest);

        connection.setDoOutput(true);
        OutputStreamWriter writer = new OutputStreamWriter(connection.getOutputStream());
        writer.write(body);
        writer.flush();
        writer.close();

        BufferedReader br = new BufferedReader(new InputStreamReader(connection.getInputStream()));
        String line;
        StringBuffer response = new StringBuffer();

        while ((line = br.readLine()) != null) {
            response.append(line);
        }
        br.close();

        return response.toString();
    } catch (IOException e) {
        throw new RuntimeException(e);
    }
}
</code></pre>
<p>Could you please suggest any solutions for this issue? Any advice would be greatly appreciated. Thank you in advance</p>
","gpt-3"
"78310456","Why does the prompt ""give me top 3 services"" for 'gpt-35-turbo-16k' trigger Azure OpenAI's content management policy 'Sexual (medium)'","2024-04-11 12:51:40","","0","48","<azure-openai><gpt-3>","<p>We are working on a NLP-to-SQL POC. While submitting a prompt to Azure Open AI we are facing a strange error. We have narrowed it down to the prompt 'give me top 3 services'. Please see the screenshot below:</p>
<p>(<a href=""https://i.sstatic.net/cYHJd.png"" rel=""nofollow noreferrer"">https://i.sstatic.net/cYHJd.png</a>)</p>
<p>When I submit the prompt 'give me top 3 services' to standard deployment of 'gpt-35-turbo-16k' in Azure, it gives me an error saying:</p>
<p>&quot;The prompt was filtered due to triggering Azure OpenAI‚Äôs content filtering system.
Reason: This prompt contains content flagged as Sexual (medium)&quot;</p>
<p>Don't understand why it's giving me this error and how to solve it?</p>
<p>When I ask &quot;is there anything offensive or sexual in 'give me top 3 services'&quot; all I get back is:</p>
<p>&quot;The prompt was filtered due to triggering Azure OpenAI‚Äôs content filtering system.
Reason: This prompt contains content flagged as Sexual (low)&quot;</p>
","gpt-3"
"78237430","Integrating GPT-4 with Team Foundation Server for Data Insights","2024-03-28 10:19:55","78238614","-1","49","<artificial-intelligence><openai-api><gpt-3><gpt-4>","<p>I'm exploring options to integrate GPT-4, the latest version of OpenAI's powerful language model, with Team Foundation Server (TFS). My objective is to leverage GPT-4's capabilities to gain insights from the data stored within our TFS environment.</p>
<p>I'm wondering if there are any APIs or existing integrations available that facilitate this process. Specifically, I'm interested in extracting data from TFS and feeding it into GPT-4 for analysis and generating insights.</p>
<p>Could anyone provide guidance on whether such an API or integration exists? If not, are there any alternative approaches or workarounds that could achieve similar results?</p>
<p>Any advice, resources, or experiences related to integrating GPT-4 with TFS for data analysis and insights?</p>
<p>I tried exporting the data from TFS and then feeding the same data to Chat GPT, but exported data doe s not contain all the required details and also exported data is very huge, so it crosses the prompt limit. My expectation is to have an API, which can directly be integrated with TFS server. Like we can integrate 365 copilot with office and GitHub co - pilot with codebase.</p>
","gpt-3"
"78190727","How to use the ImageOutputQueryTransform class in the llama_index.core.indices.query.query_transform.base library with my llama-index version 0.10.20?","2024-03-20 03:29:24","","0","78","<python><large-language-model><gpt-3><llama><llama-index>","<p>I executed the following code on Colab, but the corresponding image content did not display correctly.</p>
<pre class=""lang-py prettyprint-override""><code>from llama_index.core import SimpleDirectoryReader, GPTVectorStoreIndex
from llama_index.readers.file import ImageReader
from llama_index.core.response.notebook_utils import display_response
from llama_index.core.indices.query.query_transform.base import ImageOutputQueryTransform
from llama_index.core.query_engine import TransformQueryEngine

image_parser = ImageReader(keep_image=True, parse_text=True)
file_extractor = SimpleDirectoryReader.supported_suffix_fn()
file_extractor.update(
{
    &quot;.jpg&quot;: image_parser,
    &quot;.png&quot;: image_parser,
    &quot;.jpeg&quot;: image_parser,
})

# NOTE: we add filename as metadata for all documents
filename_fn = lambda filename: {'file_name': filename}

receipt_reader = SimpleDirectoryReader(
    input_dir='./data/receipts',
    file_extractor=file_extractor,
    file_metadata=filename_fn,
)
receipt_documents = receipt_reader.load_data()

receipts_index = GPTVectorStoreIndex.from_documents(receipt_documents)
query_engine = TransformQueryEngine(query_engine=receipts_index.as_query_engine(similarity_top_k=1),
                   query_transform=ImageOutputQueryTransform(width=400))

receipts_response = query_engine.query(
    'When was the last time I went to McDonald\'s and how much did I spend. \
    Also show me the receipt from my visit.'
)

print(type(receipts_response))
print(receipts_response)
print(&quot;1. &quot;, receipts_response.response)
print(&quot;2. &quot;, receipts_response.source_nodes)
print(&quot;3. &quot;, receipts_response.metadata)
display_response(receipts_response)
</code></pre>
<p>The ImageOutputQueryTransform class is not accompanied by usage instructions in the official documentation, so I'm uncertain if my usage is correct.</p>
<p>Can someone help me figure out why this piece of code is not displaying the image correctly? If my way of using the ImageOutputQueryTransform class is incorrect, what is the right way to use it?</p>
","gpt-3"
"78175252","Maximizing Document-Based Responses in OpenAI: Strategies for Comprehensive Information Retrieval","2024-03-17 12:37:25","","0","21","<openai-api><information-retrieval><large-language-model><gpt-3><gpt-4>","<p>I am encountering an issue with OpenAI's document-based response system using openai assistant API. Despite uploading a file containing information on 20 hotels (totaling 15,000 words), when I ask the assistant to provide me with the names of all the hotels, it only returns the names of the first two hotels. I seek guidance on how to ensure that the entire file is considered in generating responses, rather than restricting output based on a single vector.</p>
<p>How can I ensure that the entire content of a document is considered instead of relying on a specific vector?</p>
<p>For example, if I have a PDF containing data on 20 hotels spanning 15,000 words, and I ask for a list of all the hotels, the response typically includes only two hotel names due to the limitation of considering only one vector at a time. I am seeking guidance on how to enable the system to process the entire document, ensuring that the response includes all 20 hotel names.</p>
","gpt-3"
"78121934","OpenAI API error: ""Error parsing JSON response: Expecting ',' delimiter: line 8 column 9 (char 357)""","2024-03-07 13:58:37","","0","145","<python><openai-api><langchain><large-language-model><gpt-3>","<p>I am developing an API with langchain to provide correction, feedback and quality of the input essay. This is supposed to be an automatic essay feedback system. I arranged the prompt based on the requirement. Here is the prompt and the API processing part; I used fastapi and python version 3.8.18.</p>
<p>The problem that I am facing is, for the essay that is aligned with the essay question, the output sometimes are appearing and sometimes the correction part is empty, or sentences splitted to single characters, or JSON parsing error.</p>
<pre><code>  template = &quot;&quot;&quot;
        Suppose, you are an English teacher. A student has submitted their essay. You need to provide a response in the following format.
     
        First, check if the {essay} is aligned with the provided {question}. If not, show status code 422 and the following JSON-
        - status: error,
        - message: Your essay is not aligned with the topic provided.
        - topic: {question}
        If yes, provide - 
        1. Sentence-by-sentence correction of the {essay}. List all sentences with what the student wrote and what your corrected version is. If there is no
        error in the sentence, write the sentence as it is
        2. Your feedback of the {essay} in details, your feedback should be like a human teacher. Write about the strong and weak points of the essay,
        3. Evaluate quality for the following essay: {essay}. Choose one of the following for quality: below satisfactory, 
        satisfactory, good, better, excellent.
    
        You are required to extract the feedback and the quality from the output. Construct JSON in the following format and add 
        the extracted data in the respective places
        Output JSON should be:
        - status: success,
        - message: your response is processed successfully,
        - correction: sentence-by-sentence correction of each essay, format - the original sentence is shown first and in the following line the 
        corrected sentence is shown.,
        - feedback: feedback of the essay,
        - quality: quality of the essay,
        - topic: {question}
    
        Keep the response within the token limit, do not give cut-off response. 
    &quot;&quot;&quot;
</code></pre>
<p>And the API code is as follows:</p>
<pre><code>prompt_template = PromptTemplate(
    input_variables=['question', 'essay'],
    template=template
)

# Pydantic model for input validation
class EssayInput(BaseModel):
    question: str
    essay: str

# Use dependency injection to create llm_chain within the route function
def get_llm_chain():
    return LLMChain(llm=llm, prompt=prompt_template)

MAX_CORRECTION_DISPLAY_LENGTH = 500  # Adjust this value as needed

@app.post('/process-essay')
async def process_essay(essay_input: EssayInput):
    try:
        # Check if the essay is aligned with the question
        if not is_essay_aligned(essay_input.question, essay_input.essay):
            raise HTTPException(status_code=422, detail=&quot;Your essay is not aligned with the topic provided.&quot;)

        # Convert the provided string to a JSON object
        essay_input_str = '{{&quot;question&quot;: &quot;{}&quot;, &quot;essay&quot;: &quot;{}&quot;}}'.format(essay_input.question, essay_input.essay)
        essay_input_json = json.loads(essay_input_str)

        # Replace line breaks in the essay text
        cleaned_essay = essay_input_json['essay'].replace('\n', ' ')

                # Invoke the language model with only 'question' and cleaned 'essay'
        output = get_llm_chain().invoke({'question': essay_input_json['question'], 'essay': cleaned_essay})
        print(&quot;output 1&quot;, output)
        # Inside the try block where you process the output
        # Inside the try block where you process the output
        try:
            # Parse the 'text' as JSON
            output_text = json.loads(output['text'])

            # Add a print statement to inspect the content of output['text']
            print(&quot;Parsed JSON:&quot;, output_text)

            # Correct formatting of the 'correction' list in the JSON response
            # Correct formatting of the 'correction' list in the JSON response
            # Correct formatting of the 'correction' list in the JSON response
            correction_text = output_text.get('correction', [])

            # If correction_text is a string, convert it to a list with a single element
            if isinstance(correction_text, str):
                correction_text = [correction_text]

            # Each sentence in the correction list as a separate dictionary
            formatted_correction = [{'original': correction_text[i], 'corrected': correction_text[i + 1]} for i in range(0, len(correction_text) - 1, 2)]

            # Include generated 'feedback' and 'quality' in the response
            feedback = output_text.get('feedback', '')
            quality = output_text.get('quality', '')

            # Correct the quality field (remove single quote)
            quality = quality.replace(&quot;'&quot;, &quot;&quot;)

            # Construct the response dictionary
            response_dict = {
                'status': 'success',
                'message': 'Your request has been processed successfully',
                'correction': formatted_correction,  # Use the corrected formatted_correction here
                'feedback': feedback,
                'quality': quality,
                'topic': essay_input_json['question']
            }



            # Convert the response dictionary to a nicely formatted JSON string
            response_json = json.dumps(response_dict, ensure_ascii=False, indent=4)

            # Return the JSON response
            return JSONResponse(
                content=response_dict,
                status_code=200,
                media_type='application/json'
            )

        except json.decoder.JSONDecodeError as json_error:
            # Return error response if there's an issue with JSON parsing
            return JSONResponse(
                content={'status': 'error', 'message': f&quot;Error parsing JSON response: {str(json_error)}&quot;, 'data': None},
                status_code=500,
                media_type='application/json'
            )





    except HTTPException as http_error:
        # Return error response with status 500 for other errors
        return JSONResponse(
            content={'status': 'error', 'message': str(http_error.detail) if http_error.detail else 'Unknown error', 'data': None},
            status_code=500,
            media_type='application/json'
        )

def is_essay_aligned(question, essay):
    # Extract the expected essay topic from the question
    topic_marker = &quot;on&quot;
    topic_index = question.lower().find(f&quot;{topic_marker} &quot;)

    if topic_index != -1:
        expected_topic = question[topic_index + len(topic_marker) + 1 :]

        # Check if the expected topic is present in the essay
        aligned = expected_topic.lower() in essay.lower()

        return aligned

    # If &quot;on&quot; is not present, consider it as not aligned
    return False
</code></pre>
<p>The output that appears most is the following:</p>
<pre><code>{
    &quot;status&quot;: &quot;error&quot;,
    &quot;message&quot;: &quot;Error parsing JSON response: Expecting ',' delimiter: line 8 column 5 (char 339)&quot;,
    &quot;data&quot;: null
}
</code></pre>
<p>I am using the organization plan of ChatGPT. Is the free plan causing the parsing or error problems?</p>
","gpt-3"
"78089564","GPT-4 doesn't seem to be set when I'm using the Node.js API","2024-03-01 17:45:40","","0","52","<node.js><openai-api><gpt-3><chat-gpt-4>","<p>I have set up GPT-4 model for chat completions, and I sent a simple question through the API
&quot;what is the model you using for generation text&quot;? and i got the funny ansewer:
&quot;<code>I am powered by OpenAI's GPT-3 model for generating text.&quot;.</code>
but when i asked exact question with gpt-plus in official website i got this ansewer:
<code>&quot;I'm based on the GPT-4 architecture, which is a part of OpenAI's Generative Pre-trained Transformer series. GPT-4 is an advanced AI language model designed to understand and generate human-like text based on the input it receives. It's capable of answering questions, providing explanations, generating text in various styles, and even performing specific tasks like translation or code generation, all while aiming to improve upon the capabilities and performance of its predecessors.&quot;</code>
Does someone have a simple experience?</p>
<pre><code>import {OpenAI} from 'openai';
const response = await openai.chat.completions.create({
    messages: &quot;what is the model you using for generation text&quot;,
    model: &quot;gpt-4&quot;,
    stream:false,
    temperature:0.2
  })
  console.log(response);
</code></pre>
<p>the ansewer was:
first Time:</p>
<pre><code>//I am powered by OpenAI's GPT-3 model for generating text.
</code></pre>
<p>Second Time:</p>
<pre><code>//As an AI developed by OpenAI, I am based on the GPT-3 model.
</code></pre>
","gpt-3"
"78087840","I want to access the gpt-3.5 version and its model to solve my problem","2024-03-01 12:43:32","","1","32","<model><openai-api><gpt-3><mlmodel>","<p>I'm running my code in annaconda spyder(Windows) and getting &quot;InvalidRequestError: This is a chat model and not supported in the v1/completions endpoint. Did you mean to use v1/chat/completions?&quot; error. Have tried inserting env variable in spyder.</p>
<p>` from openai import OpenAI</p>
<p>def generate_synonyms(word):</p>
<pre><code>client = OpenAI()

prompt = f&quot;Can you provide synonyms for the word '{word}'?&quot;

completion = client.chat.completions.create(
    model=&quot;gpt-3.5-turbo&quot;,
    messages=[
        {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;I'm an assistant capable of providing information on various topics.&quot;},
        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt},
    ]
)

return completion.choices[0].message
</code></pre>
<p>word = &quot;battery&quot;</p>
<p>synonyms = generate_synonyms(word)</p>
<p>print(&quot;Synonyms for the word&quot;, word + &quot;:&quot;, synonyms)`</p>
<p>This is my code.</p>
","gpt-3"
"78046390","Python OpenAI API error: module 'openai' has no attribute 'Completion'. Did you mean: 'completions'?","2024-02-23 09:35:31","78046455","-3","1322","<python><python-3.x><bots><openai-api><gpt-3>","<p>I try to make a bot for my website in Python. But I have an error.</p>
<p><strong>The Error:</strong></p>
<pre><code>You: hello
Traceback (most recent call last):
  File &quot;D:\module2.py&quot;, line 20, in &lt;module&gt;
    response = chat_with_gpt(user_input)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;D:\module2.py&quot;, line 6, in chat_with_gpt
    response = openai.Completion.create(
               ^^^^^^^^^^^^^^^^^
AttributeError: module 'openai' has no attribute 'Completion'. Did you mean: 'completions'?
</code></pre>
<p><strong>This is the Python code:</strong></p>
<p>I tried many combinations, and I also tried with <code>text-davinci-003</code>, but nothing worked.</p>
<pre><code>import openai

openai.api_key = &quot;API-KEY&quot;

def chat_with_gpt(prompt):
    response = openai.Completion.create(
        # engine=&quot;text-davinci-003&quot;,  # Adjust the engine as necessary
        engine=&quot;gpt-3.5-turbo&quot;,  # Adjust the engine as necessary
        prompt=prompt,
        max_tokens=150
    )
    return response.choices[0].text.strip()

if __name__ == &quot;__main__&quot;:
    while True:
        user_input = input(&quot;You: &quot;)
        user_input = user_input.lower()
        if user_input in [&quot;quit&quot;, &quot;exit&quot;, &quot;bye&quot;]:
            break
        response = chat_with_gpt(user_input)
        print(&quot;Chatbot:&quot;, response)

</code></pre>
","gpt-3"
"77967615","Check confidence in OpenAI prediction","2024-02-09 10:59:34","","0","119","<openai-api><chatgpt-api><gpt-3>","<p>I saw that it is possible to predict a sentence based on inputs you enter, but beyond that, is it possible to see the confidence?
This is my code:</p>
<pre><code>def get_completion(prompt, model=&quot;gpt-3.5-turbo&quot;):
messages = [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}]
response = openai.ChatCompletion.create(
    model=model,
    messages=messages,
    temperature=0, # this is the degree of randomness of the model's output
)
completion = response.choices[0].message[&quot;content&quot;]
print(response.choices[0])
return completion



def get_emozione_gpt(text):
prompt = &quot;Your are an emotion recognition tool for tweets and your task is to &quot; \
&quot;analize theme a give a single emotion or a list of emotions, separeted by comma that you might think that are expressed on the current tweet and you should use only the emotions from &quot; \
&quot;this list ['Anger', 'Anticipation', 'Disgust', 'Fear', 'Joy', 'Neutral', 'Sadness', 'Surprise', 'Trust']&quot;

examples = &quot;&quot;&quot; \
Here some examples:
Input &quot;Help, i need help&quot; Output:&quot;Fear&quot;
...
(other examples)

&quot;&quot;&quot;
prompt = f&quot;{prompt}\n{examples}\n Text:\&quot;{text}\&quot; Output: &quot;
completion = get_completion(prompt)
return completion
</code></pre>
<p>in &quot;print(response.choices[0])&quot;
i can see:</p>
<pre><code>{
  &quot;index&quot;: 0,
  &quot;message&quot;: {
    &quot;role&quot;: &quot;assistant&quot;,
    &quot;content&quot;: &quot;Joy&quot;
  },
  &quot;logprobs&quot;: null,
  &quot;finish_reason&quot;: &quot;stop&quot;
}
</code></pre>
<p>but no the confidence, so, can get the confidence or  get multiple labels for text with confidence?</p>
","gpt-3"
"77950344","OpenAI GPT : Is it possible to set the context of the task with finetuning, instead of providing it as input for every API call?","2024-02-06 19:17:49","","0","136","<openai-api><chatgpt-api><gpt-3>","<p>I am trying to create a GPT that can grade some essays. I am currently using the OpenAI API to input :</p>
<ol>
<li>Essay Question,</li>
<li>Rules and concepts to look for,</li>
<li>Grading Rubric,</li>
<li>Essay Submission</li>
</ol>
<p>every single time to grade an essay. I believe I am exceeding the input token threshold in every call, also this is going to be expensive in the long run.</p>
<p>Is there a way to feed/input the Essay Question, Rules and Concepts, and Grading Rubric one time using pre training or any other techniques, and reuse the API by just passing the essay submission?</p>
","gpt-3"
"77949086","Create a gpt-3.5 API request that determines whether any time range in a list intersects with a given time range","2024-02-06 15:53:27","","0","32","<openai-api><chatgpt-api><gpt-3><gpt-4>","<p>I've created a prompt that should select a requested number of employees from the list. But the <strong>step 1</strong> doesn't work properly. Sometimes GPT takes in account only the time range and ignores the date. I tried to describe this step in a different way many times, tried different time¬†formats including UTC, but didn't succeed. Maybe experienced prompt creators can tell what's wrong with my prompt?</p>
<hr />
<p><strong>User message:</strong></p>
<pre><code>{
¬† &quot;employees&quot;: [
¬† ¬† {
¬† ¬† ¬† &quot;id&quot;: 1,
¬† ¬† ¬† &quot;name&quot;: &quot;Bender Rodriguez&quot;,
¬† ¬† ¬† &quot;position&quot;: &quot;developer&quot;,
¬† ¬† ¬† &quot;experience&quot;: &quot;middle&quot;,
¬† ¬† ¬† &quot;interviews_conducted&quot;: 0,
¬† ¬† ¬† &quot;busy_date_time&quot;: [
¬† ¬† ¬† ¬† {&quot;start_time&quot;: &quot;February 10 2024 06:00&quot;, &quot;end_time&quot;: &quot;February 10 2024 07:00&quot;},
¬† ¬† ¬† ¬† {&quot;start_time&quot;: &quot;February 11 2024 10:00&quot;, &quot;end_time&quot;: &quot;February 11 2024 11:00&quot;}
¬† ¬† ¬† ]
¬† ¬† },
¬† ¬† {
¬† ¬† ¬† &quot;id&quot;: 2,
¬† ¬† ¬† &quot;name&quot;: &quot;Philip Fry&quot;,
¬† ¬† ¬† &quot;position&quot;: &quot;developer&quot;,
¬† ¬† ¬† &quot;experience&quot;: &quot;middle&quot;,
¬† ¬† ¬† &quot;interviews_conducted&quot;: 2,
¬† ¬† ¬† &quot;busy_date_time&quot;: [
¬† ¬† ¬† ¬† {&quot;start_time&quot;: &quot;February 10 2024 13:00&quot;, &quot;end_time&quot;: &quot;February 10 2024 14:00&quot;}
¬† ¬† ¬† ]
¬† ¬† },
¬† ¬† {
¬† ¬† ¬† &quot;id&quot;: 3,
¬† ¬† ¬† &quot;name&quot;: &quot;John Zoidberg&quot;,
¬† ¬† ¬† &quot;position&quot;: &quot;developer&quot;,
¬† ¬† ¬† &quot;experience&quot;: &quot;junior&quot;,
¬† ¬† ¬† &quot;interviews_conducted&quot;: 1,
¬† ¬† ¬† &quot;busy_date_time&quot;: [
¬† ¬† ¬† ¬† {&quot;start_time&quot;: &quot;February 10 2024 10:00&quot;, &quot;end_time&quot;: &quot;February 10 2024 11:00&quot;}
¬† ¬† ¬† ]
¬† ¬† },
¬† ¬† {
¬† ¬† ¬† &quot;id&quot;: 4,
¬† ¬† ¬† &quot;name&quot;: &quot;Turanga Leela&quot;,
¬† ¬† ¬† &quot;position&quot;: &quot;developer&quot;,
¬† ¬† ¬† &quot;experience&quot;: &quot;senior&quot;,
¬† ¬† ¬† &quot;interviews_conducted&quot;: 1,
¬† ¬† ¬† &quot;busy_date_time&quot;: [
¬† ¬† ¬† ¬† {&quot;start_time&quot;: &quot;February 10 2024 10:00&quot;, &quot;end_time&quot;: &quot;February 10 2024 11:00&quot;}
¬† ¬† ¬† ]
¬† ¬† },
¬† ¬† {
¬† ¬† ¬† &quot;id&quot;: 5,
¬† ¬† ¬† &quot;name&quot;: &quot;Amy Wong&quot;,
¬† ¬† ¬† &quot;position&quot;: &quot;developer&quot;,
¬† ¬† ¬† &quot;experience&quot;: &quot;senior&quot;,
¬† ¬† ¬† &quot;interviews_conducted&quot;: 0,
¬† ¬† ¬† &quot;busy_date_time&quot;: [
¬† ¬† ¬† ¬† {&quot;start_time&quot;: &quot;February 10 2024 10:00&quot;, &quot;end_time&quot;: &quot;February 10 2024 11:00&quot;}
¬† ¬† ¬† ]
¬† ¬† }
¬† ]
}

Do step-by-step:

1. Remove from the &quot;employees&quot; list above each employee if any time interval in 
&quot;busy_date_time&quot; list overlaps with &quot;required_date_time&quot;.

2. If the number of employees left in the &quot;employees&quot; list is less than 
&quot;required_employees_number&quot;, set the new value to &quot;required_employees_number&quot; equal 
to the number of employees left in the &quot;employees&quot; list.

3. Select &quot;required_employees_number&quot; employees with &quot;required_experience&quot; and lower 
&quot;interviews_conducted&quot; value. You shouldn't find the one with the lowest 
&quot;interviews_conducted&quot; value among all, but a required number of employees which is 
&quot;required_employees_number&quot;.

4. Check the previous step where you usually make the mistake of selecting 1 employee
with minimum &quot;interviews_conducted&quot; value among all employees when you need to select 
a list of &quot;required_employees_number&quot; employees.

required_date_time = '''{&quot;start_time&quot;: &quot;February 10 2024 10:00&quot;, &quot;end_time&quot;: &quot;February 10 2024 11:00&quot;}'''
required_employees_number = 1
required_experience = &quot;middle&quot;
</code></pre>
<hr />
<p><strong>System message:</strong>
You are a computer program that strictly follows the user's instructions. Your output is always only a list of employee's id. Any other notes or comments are forbidden.</p>
<hr />
<p><strong>GPT settings:</strong></p>
<ul>
<li>Temperature: 0</li>
<li>Top P: 0</li>
<li>Frequency penalty: 0</li>
<li>Presence penalty: 0</li>
</ul>
<hr />
<ul>
<li><strong>Expected result:</strong> [1]</li>
<li><strong>Actual result:</strong> [2]</li>
</ul>
","gpt-3"
"77936280","How to create embedding vectors on-premise for OpenAI models?","2024-02-04 14:56:51","77947613","1","601","<openai-api><chatgpt-api><gpt-3><openaiembeddings>","<p>I want to create a chatbot for confidential company documents. Due to security concerns, I want to make the embeddings locally and store the vectors locally. I‚Äôll use OpenAI‚Äôs API to communicate with the LLM. How do I make the embeddings <strong>locally</strong> without using OpenAI's Embeddings web API?</p>
","gpt-3"
"77900626","Firebase OpenAI API error: ""openai.createCompletion is not a function""","2024-01-29 14:59:45","77900685","0","707","<javascript><firebase><openai-api><gpt-3>","<p>I am having an issue with running my Firebase function. It needs to generate a challenge once a day (I have it set up to run every minute now for debug) and to save it in my realtime database. This code is like this:</p>
<pre><code>const functions = require(&quot;firebase-functions&quot;);
const admin = require(&quot;firebase-admin&quot;);
const OpenAI = require(&quot;openai&quot;);

admin.initializeApp();

// Create a new instance of the OpenAI API client
const openai = new OpenAI({
  apiKey: functions.config().openai.key,
});

exports.generateDailyChallenge = functions.pubsub.schedule(&quot;* * * * *&quot;)
    .onRun(async (context) =&gt; {
      try {
        const response = await openai.createCompletion({
          model: &quot;text-davinci-003&quot;,
          prompt: &quot;You are an AI model powering a social media app based &quot; +
                &quot;around challenges. The app works in a similar way to &quot; +
                &quot;BeReal, meaning that once a day it generates a challenge &quot; +
                &quot;for the users. The challenge needs to be simple to &quot; +
                &quot;complete and require just a single photo taken from both &quot; +
                &quot;cameras, just keep this in mind, but don't mention it as &quot; +
                &quot;the users are used to it. The challenge is supposed to be &quot; +
                &quot;fun, but easy to complete so users don't have to spend &quot; +
                &quot;too much time on it. The goal is to connect with your &quot; +
                &quot;online friends in a fun and engaging way. Mainly to just &quot; +
                &quot;see what they were up to at that specific moment. Give me &quot; +
                &quot;just the challenge name and a brief description of the &quot; +
                &quot;challenge as if you were challenging a user to do it&quot;,
          max_tokens: 150,
        });

        const challengeText = response.data.choices[0].text.trim();
        console.log(&quot;Generated Challenge:&quot;, challengeText);

        // Save to Firestore
        const db = admin.firestore();
        await db.collection(&quot;challenges&quot;).add({
          text: challengeText,
          createdAt: admin.firestore.FieldValue.serverTimestamp(),
        });
      } catch (error) {
        console.error(&quot;Error generating or saving challenge:&quot;, error.message);
      }
    });
</code></pre>
<p>However when I run it I get this error in a log:</p>
<pre><code>    {
  &quot;textPayload&quot;: &quot;Error generating or saving challenge: openai.createCompletion is not a function&quot;,
  &quot;insertId&quot;: &quot;65b7b5f100015b11309c47bd&quot;,
  &quot;resource&quot;: {
    &quot;type&quot;: &quot;cloud_function&quot;,
    &quot;labels&quot;: {
      &quot;project_id&quot;: &quot;nocena-dea56&quot;,
      &quot;function_name&quot;: &quot;generateDailyChallenge&quot;,
      &quot;region&quot;: &quot;us-central1&quot;
    }
  },
  &quot;timestamp&quot;: &quot;2024-01-29T14:28:01.088849Z&quot;,
  &quot;labels&quot;: {
    &quot;runtime_version&quot;: &quot;nodejs16_20240121_16_20_2_RC00&quot;,
    &quot;instance_id&quot;: &quot;0087599d42bd678f896459629280089ca75acae30403ae873cd7bb1e45f4fe7e158e416746bfcd7cebb55af5ab4abe7bf80c2b4277d2d0d089ec3f3e043ca6018f63&quot;,
    &quot;execution_id&quot;: &quot;lzchc47te50n&quot;
  },
  &quot;logName&quot;: &quot;projects/nocena-dea56/logs/cloudfunctions.googleapis.com%2Fcloud-functions&quot;,
  &quot;trace&quot;: &quot;projects/nocena-dea56/traces/8bff527e9d2bcf302a301c3630917ee8&quot;,
  &quot;receiveTimestamp&quot;: &quot;2024-01-29T14:28:01.426853583Z&quot;
}
</code></pre>
<p>When I tried debugging it I got told I don‚Äôt initialise the function properly. However that is not the case. And I also tried changing the way I initialise it so many time. I am using this to call it: <a href=""https://www.npmjs.com/package/openai"" rel=""nofollow noreferrer"">https://www.npmjs.com/package/openai</a> (it is the version 4.26.0), so I should initialise it this way.</p>
<p>If nobody can help me maybe you can recommend me what other ai to use, because at this point I am suspecting it might be just an issue with OpenAI api.</p>
","gpt-3"
"77894176","How to load quantized LLM to CPU only device?","2024-01-28 09:05:47","","0","578","<large-language-model><huggingface><gpt-3>","<p>I have this code to quantize a large language model and save the quantized model:</p>
<pre><code>import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

model_name = 'stabilityai/stablelm-2-zephyr-1_6b'

def load_quantized_model(model_name: str):
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type=&quot;nf4&quot;,
        bnb_4bit_compute_dtype=torch.bfloat16
    )
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        load_in_4bit=True,
        torch_dtype=torch.bfloat16,
        quantization_config=bnb_config,
        trust_remote_code=True
    )
    return model

def initialize_tokenizer(model_name: str):
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        trust_remote_code=True
    )
    tokenizer.bos_token_id = 1  # Set beginning of sentence token id
    return tokenizer

model = load_quantized_model(model_name)
tokenizer = initialize_tokenizer(model_name)

SAVED_MODEL_NAME = 'quantized'
model.save_pretrained(SAVED_MODEL_NAME)
</code></pre>
<p>Now, I have a 1.2GB (from ~4gb) model inside the directory gpt-custom.</p>
<p>So I download it in my laptop with CPU only and this is my code:</p>
<pre><code>import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

model_name = 'stabilityai/stablelm-2-zephyr-1_6b'

model = AutoModelForCausalLM.from_pretrained(
    'quantized',
    device_map=&quot;auto&quot;,
    trust_remote_code=True
)

def initialize_tokenizer(model_name: str):
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        trust_remote_code=True
    )
    tokenizer.bos_token_id = 1  # Set beginning of sentence token id
    return tokenizer

tokenizer = initialize_tokenizer(model_name)

question = 'how are you feeling?'
prompt = [{'role': 'user', 'content': question}]
inputs = tokenizer.apply_chat_template(
    prompt,
    add_generation_prompt=True,
    return_tensors='pt'
)
tokens = model.generate(
    inputs.to(model.device),
    max_new_tokens=64,
    temperature=0.5,
    do_sample=True
)
print(tokenizer.decode(tokens[0], skip_special_tokens=True))
</code></pre>
<p>But I am getting an error <code>NameError: name 'torch' is not defined</code> but I already installed torch using <code>pip install torch</code> and even tried <code>pip install --upgrade torch</code>. There was also this warning before the error:</p>
<pre><code>Detected the presence of a `quantization_config` attribute in the model's configuration but you don't have the correct `bitsandbytes` version to support 4 and 8 bit serialization. Please install the latest version of `bitsandbytes` with  `pip install --upgrade bitsandbytes`.
</code></pre>
<p>And I have also tried <code>pip install --upgrade bitsandbytes</code> but still get the same error.</p>
<p>This is the full error stack:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/var/www/html/test_llm/test.py&quot;, line 6, in &lt;module&gt;
    model = AutoModelForCausalLM.from_pretrained(
  File &quot;/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py&quot;, line 562, in from_pretrained
    return model_class.from_pretrained(
  File &quot;/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py&quot;, line 3856, in from_pretrained
    ) = cls._load_pretrained_model(
  File &quot;/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py&quot;, line 4290, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File &quot;/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py&quot;, line 839, in _load_state_dict_into_meta_model
    set_module_quantized_tensor_to_device(model, param_name, param_device, value=param)
  File &quot;/usr/local/lib/python3.10/dist-packages/transformers/integrations/bitsandbytes.py&quot;, line 58, in set_module_quantized_tensor_to_device
    if old_value.device == torch.device(&quot;meta&quot;) and device not in [&quot;meta&quot;, torch.device(&quot;meta&quot;)] and value is None:
NameError: name 'torch' is not defined
</code></pre>
","gpt-3"
"77890911","OpenAI API error: ""The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY env variable""","2024-01-27 10:52:08","","1","26248","<python><environment-variables><openai-api><api-key><gpt-3>","<p>I'm a little confused about using OpenAI in Python and need a little help to make this code work. I tried several solutions found on StackOverflow, none of them are working.</p>
<p>My goal is to make a Python code that asks two questions to the user, and then the <code>gpt-3.5-turbo-instruct</code> model finds the answer and exports a <code>questions_reponses.csv</code> with it. I will then convert this to XML to import it into a moodle environment.</p>
<p><strong>Error message</strong>: <code>OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable</code></p>
<p><strong>Environnement</strong>: MacOS/Thonny</p>
<p><strong>My code:</strong></p>
<pre><code>import pandas as pd

# Configure OpenAI API key

from openai import OpenAI
client = OpenAI()
openai.api_key = 'my secret key openai'


# Prompt user for 2 questions
questions = []
for i in range(2):
    question = input(&quot;Posez une question: &quot;)
    questions.append(question)

# Use OpenAI to answer questions
answers = []
for question in questions:
    response = client.Completions.create(
        engine=&quot;gpt-3.5-turbo-instruct&quot;,
        prompt=f&quot;Question: {question}\nR√©ponse:&quot;,
        max_tokens=1024,
        n=1,
        stop=None,
        temperature=0.7,
    )
    answer = response.choices[0].text.strip()
    answers.append(answer)

# Create a pandas dataframe with questions and answers
df = pd.DataFrame({&quot;Question&quot;: questions, &quot;R√©ponse&quot;: answers})

# Export dataframe to CSV file
df.to_csv(&quot;questions_reponses.csv&quot;, index=False)

print(&quot;Le fichier CSV a √©t√© cr√©√© avec succ√®s.&quot;)`
</code></pre>
<p>I've tried to set the environment variable <code>OPENAI_API_KEY</code> in my os, but it didn't work (I always get the same error message). So I keep trying to set the key inside the code below. I don't know if my syntax is okay.</p>
<h4>Remark on the answer</h4>
<p>I am reaching out to the answerer here. As to the rules of Stack Exchange, I must do this in the question.</p>
<p>I've just tried option 1, same error message:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

# Configure OpenAI API key

    import os
    from openai import OpenAI
    client = OpenAI()
    OpenAI.api_key = os.getenv('OPENAI_API_KEY')

    # Prompt user for 5 questions
    questions = []
    for i in range(1):
        question = input(&quot;Posez une question: &quot;)
        questions.append(question)

    # Use OpenAI to answer questions
    answers = []
    for question in questions:
        response = client.Completions.create(
            engine=&quot;gpt-3.5-turbo-instruct&quot;,
            prompt=f&quot;Question: {question}\nR√©ponse:&quot;,
            max_tokens=1024,
            n=1,
            stop=None,
            temperature=0.7,
        )
        answer = response.choices[0].text.strip()
        answers.append(answer)

    # Create a pandas dataframe with questions and answers
    df = pd.DataFrame({&quot;Question&quot;: questions, &quot;R√©ponse&quot;: answers})

    # Export dataframe to CSV file
    df.to_csv(&quot;questions_reponses.csv&quot;, index=False)

    print(&quot;Le fichier CSV a √©t√© cr√©√© avec succ√®s.&quot;)
</code></pre>
<p>I've just tried option 2, same error message:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

# Configure OpenAI API key

    from openai import OpenAI
    client = OpenAI()
    OpenAI.api_key = &quot;sk-xxxxxxxxxxxxxx&quot;




    # Prompt user for 5 questions
    questions = []
    for i in range(1):
        question = input(&quot;Posez une question: &quot;)
        questions.append(question)

    # Use OpenAI to answer questions
    answers = []
    for question in questions:
        response = client.Completions.create(
            engine=&quot;gpt-3.5-turbo-instruct&quot;,
            prompt=f&quot;Question: {question}\nR√©ponse:&quot;,
            max_tokens=1024,
            n=1,
            stop=None,
            temperature=0.7,
        )
        answer = response.choices[0].text.strip()
        answers.append(answer)

    # Create a pandas dataframe with questions and answers
    df = pd.DataFrame({&quot;Question&quot;: questions, &quot;R√©ponse&quot;: answers})

    # Export dataframe to CSV file
    df.to_csv(&quot;questions_reponses.csv&quot;, index=False)

    print(&quot;Le fichier CSV a √©t√© cr√©√© avec succ√®s.&quot;)
</code></pre>
<p>I don't understand what is going on.</p>
<h4>MIND: the answer works, it was just not checked in full</h4>
<p><em>This is a remark from an outsider.</em> As to the check of the heading above: the questioner has only tried the approach 1. With approach 2, it would work, that is why the questioner could already accept the answer.</p>
","gpt-3"
"77811059","OpenAI API error: ""NameError: name 'client' is not defined""","2024-01-13 10:24:24","77811071","0","2006","<python><telegram><openai-api><gpt-3>","<p>This is my code:</p>
<pre class=""lang-py prettyprint-override""><code>import telebot
import openai
 
bot = telebot.TeleBot(&quot;0&quot;)
openai.api_key = &quot;0&quot;
 
@bot.message_handler(content_types=['text'])
def lalala(message):
    print(message.chat.title, message.chat.username)
    if message.chat.id == -1002097745017:
    #print(message.text)
        if &quot;@0&quot; in message.text:
            message.text = (message.text).replace(&quot;@0 &quot;, &quot;&quot;)
            #print(message.text)
            response = client.completions.create(model=&quot;gpt-3.5-turbo-0613&quot;, prompt=message.text, max_tokens=1000)
            full_response = response['choices'][0]['text']  # Use the text property of the first element of the choices list to access the full response
            lines = full_response.splitlines()  # Split the response into individual lines
            for line in lines:  # Iterate over the lines
                try:
                    #print(line)
                    bot.send_message(message.chat.id, line)  # Send each line back to the user as a separate message
                except Exception as e:
                    print(e)
    else:
        bot.send_message(message.chat.id, &quot;work only - tg.com/123123&quot;)
 
bot.polling(none_stop=True, interval=0)
</code></pre>
<p>I'm getting the following error:</p>
<blockquote>
<p>NameError: name 'client' is not defined</p>
</blockquote>
<p>If I change this...</p>
<pre><code>response = client.completions.create(model=&quot;gpt-3.5-turbo-0613&quot;, prompt=message.text, max_tokens=1000)
</code></pre>
<p>...to this.</p>
<pre><code>response = openai.Completion.create(model=&quot;text-davinci-003&quot;, prompt=message.text, max_tokens=1000)
</code></pre>
<p>Then I'm getting the following error:</p>
<blockquote>
<p>This is a chat model and not supported in the v1/completions endpoint. Did you mean to use v1/chat/completions?</p>
</blockquote>
","gpt-3"
"77807093","Failing to upgrade the OpenAI Node.js SDK from v3 to v4","2024-01-12 13:57:21","","-1","158","<angular><openai-api><gpt-3>","<p>I'm trying to update my app from Angular 15 and <code>&quot;openai&quot;: &quot;^3.2.1&quot;</code> to Angular 17 and <code>&quot;openai&quot;: ^4.24.1&quot;</code>.</p>
<p>I cannot understand what is missing. No errors occurred.</p>
<p>Followed links:</p>
<ol>
<li><p><a href=""https://platform.openai.com/docs/guides/text-generation/completions-api"" rel=""nofollow noreferrer"">https://platform.openai.com/docs/guides/text-generation/completions-api</a></p>
</li>
<li><p><a href=""https://github.com/openai/openai-node/discussions/217"" rel=""nofollow noreferrer"">https://github.com/openai/openai-node/discussions/217</a></p>
</li>
<li><p><a href=""https://stackoverflow.com/questions/77789886/openai-api-error-the-model-text-davinci-003-has-been-deprecated"">OpenAI API error: &quot;The model `text-davinci-003` has been deprecated&quot;</a></p>
</li>
<li><p><a href=""https://stackoverflow.com/questions/75774873/openai-chatgpt-gpt-3-5-api-error-this-is-a-chat-model-and-not-supported-in-t"">OpenAI ChatGPT (GPT-3.5) API error: &quot;This is a chat model and not supported in the v1/completions endpoint&quot;</a></p>
</li>
</ol>
<p>Old version:</p>
<pre><code>import { Configuration, OpenAIApi } from 'openai';
  readonly configuration = new Configuration({
    organization: environment.openAIOrganization,
    apiKey: environment.openAIToken,
  });


  readonly openai = new OpenAIApi(this.configuration);

  //#region -- GENERATE CHATGPT MSG --
  async getDataFromOpenAI(text: string): Promise&lt;any&gt; {
    try {
      const completion = await this.openai.createCompletion({
        model: &quot;text-davinci-003&quot;,
        prompt: text,
        temperature: 0.9,
        max_tokens: 62,
      },
        {
          headers: {
            'Content-Type': 'application/json',
            //'baseOptions': 'User-Agent',
            'Authorization': 'Bearer ' + environment.openAIToken,
            'Access-Control-Allow-Origin': '*'
          }
        })

      return completion.data.choices[0].text
    }
    catch (error) {
      console.error('error.message', error);
    }
  }
  //#endregion
</code></pre>
<p>New version:</p>
<pre><code>import OpenAI from 'openai';



 async generateSms() {
    try {
      let completion = await this.openai.completions.create({
        model: 'gpt-3.5-turbo-instruct',
        prompt: 'Create a happy birthday message',
        temperature: 0.5,
      });
      console.log(completion);
      console.log(completion.choices[0].text);
    } catch (e) {
      console.error('generateSms =&gt; ', e);      
    }
  } 
  
</code></pre>
<p>I created a new key but is only returning [null]</p>
<p>Last used in API KEY is updating: <strong>last used 12/jan</strong></p>
<p><a href=""https://i.sstatic.net/ybmgd.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ybmgd.png"" alt=""enter image description here"" /></a></p>
","gpt-3"
"77806389","OpenAI API error: ""openai.error.InvalidRequestError: The model `text-davinci-003` has been deprecated"". Any alternative model to fix the code?","2024-01-12 11:56:05","","2","1708","<python><openai-api><streamlit><large-language-model><gpt-3>","<p>I developed a chatbot utilizing OpenAI's API for PDF question answering, relying on the text-davinci-003 model. However, following OpenAI's recent announcement about the deprecation of certain models, such as <code>text-davinci-003</code>, my chatbot encounters an error:</p>
<blockquote>
<p>'openai.error.InvalidRequestError: The model text-davinci-003 has been
deprecated'.</p>
</blockquote>
<p>My chatbot is built in Python using Streamlit. I need some help figuring out how to modify my code to fix this and switch to a supported model. Any insights or assistance would be very much appreciated.</p>
<p>Here is my code:</p>
<pre><code>from dotenv import load_dotenv
import streamlit as st
from PyPDF2 import PdfReader
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.chains.question_answering import load_qa_chain
from langchain.llms import OpenAI



def main():
    load_dotenv()
    st.set_page_config(page_title=&quot;Ask your pdf&quot;, layout=&quot;centered&quot;,initial_sidebar_state=&quot;auto&quot;)
    st.header(&quot;Ask your pdf(OpenAI) ü§ì&quot;)
    
    # Uploading the file
    pdf = st.file_uploader(&quot;Upload your pdf&quot;, type=&quot;pdf&quot;)
    
    # Extracting the text
    if pdf is not None:
        pdf_reader = PdfReader(pdf)
        text = &quot;&quot;
        for page in pdf_reader.pages:
            text += page.extract_text()

        # Split into chunks 
        text_splitter = CharacterTextSplitter(
            separator=&quot;\n&quot;, # Defines a new line 
            chunk_size = 1000,
            chunk_overlap = 200,
            length_function = len
        )
        chunks = text_splitter.split_text(text)

        # Create embeddings
        embeddings = OpenAIEmbeddings()

        # Creating an object on which we will be able to search FAISS
        knowledge_base = FAISS.from_texts(chunks, embeddings)

        # show user input
        user_question = st.text_input(&quot;Ask a question about the PDF: &quot;)

        if st.button(&quot;Refresh Page&quot;):
            st.caching.clear_cache()
            
        if user_question:
            docs = knowledge_base.similarity_search(user_question)

            llm = OpenAI()
            chain = load_qa_chain(llm, chain_type=&quot;stuff&quot;)
            response = chain.run(input_documents=docs, question = user_question)

            st.write(response)

if __name__ == '__main__':
    main()
</code></pre>
","gpt-3"
"77789886","OpenAI API error: ""The model `text-davinci-003` has been deprecated""","2024-01-09 21:44:24","","15","36130","<php><openai-api><gpt-3>","<p>I was using the ChatGPT and it says to use this line of code for the API endpoint:</p>
<pre><code>$endpoint = 'https://api.openai.com/v1/engines/text-davinci-003/completions';
</code></pre>
<p>But it doesn't work. I get the following error:</p>
<pre><code>{ &quot;error&quot;: { &quot;message&quot;: &quot;The model `text-davinci-003` has been deprecated, learn more here: https://platform.openai.com/docs/deprecations&quot;, &quot;type&quot;: &quot;invalid_request_error&quot;, &quot;param&quot;: null, &quot;code&quot;: &quot;model_not_found&quot; } }
</code></pre>
<p>I can't really find anything to fix it.</p>
<p>I tried changing the models in the URL itself but it didn't work.</p>
","gpt-3"
"77768237","Issue with LangChain Misclassifying gpt-3.5-turbo-instruct as Chat Model","2024-01-06 04:37:09","77770541","0","1032","<langchain><large-language-model><gpt-3><py-langchain>","<p>OpenAI <em>deprecated</em> its <strong>text-davinci-003</strong> completion model. I've updated the model to <strong>gpt-3.5-turbo-instruct</strong>. I am encountering an issue with the <strong>LangChain</strong> where it <strong>incorrectly classifies the gpt-3.5-turbo-instruct model as a chat model</strong>. This is causing initialization problems in my code.</p>
<p><strong>Environment:</strong></p>
<pre class=""lang-bash prettyprint-override""><code>python = &quot;^3.10&quot;
langchain = &quot;^0.0.130&quot;
</code></pre>
<p>OS: Ubuntu</p>
<p><strong>Expected Behavior:</strong></p>
<p>The expected behavior is that the gpt-3.5-turbo-instruct model should be recognized as a completion model by LangChain and initialized appropriately without warnings or errors.</p>
<p><strong>Actual Behavior:</strong></p>
<p>When attempting to initialize the gpt-3.5-turbo-instruct model, I receive warnings suggesting that this model is being misclassified as a chat model. The specific warnings are:</p>
<pre class=""lang-bash prettyprint-override""><code>/home/mahdi/.cache/pypoetry/virtualenvs/backend-bRqVKcMN-py3.11/lib/python3.11/site-packages/langchain/llms/openai.py:169: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`
/home/mahdi/.cache/pypoetry/virtualenvs/backend-bRqVKcMN-py3.11/lib/python3.11/site-packages/langchain/llms/openai.py:608: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI
</code></pre>
<p><strong>My simplified code:</strong></p>
<pre class=""lang-py prettyprint-override""><code>from langchain import OpenAI


llm = OpenAI({
            &quot;model_name&quot;: &quot;gpt-3.5-turbo-instruct&quot;,
            &quot;temperature&quot;: 0.0,
            &quot;top_p&quot;: 1,
            &quot;openai_api_key&quot;: &quot;API_KEY&quot;,
        })
        
print(llm)
</code></pre>
<p><strong>Output:</strong></p>
<pre class=""lang-bash prettyprint-override""><code>OpenAIChat[Params: {'model_name': 'gpt-3.5-turbo-instruct', 'temperature': 0.0, 'top_p': 1}

</code></pre>
","gpt-3"
"77624493","How can I improve security for Weaviate in local environment?","2023-12-08 05:21:32","","1","95","<security><openai-api><gpt-3><weaviate><vector-database>","<p>I'm currently setting up Weaviate, the open-source, GraphQL and RESTful API-based knowledge graph, on my local environment. Although it's a local setup, I would like to ensure robust security measures are applied to protect it from any potential threats.</p>
<p>The specifics for my setup are:</p>
<p>Operating System: ubuntu 22.04
Weaviate Version: v1.19
Here is my current configuration: (Please replace this line with your existing configuration details)</p>
<p>What are the best practices to enhance security in this scenario? Any kind of suggestions regarding improving the security like encrypting data at rest, securing communication with SSL/TLS, protecting APIs with authentication layers and the ways to implement them would be really appreciated.</p>
<p>I've looked around but couldn't quite find what I was looking for, so any guidance or resources would be greatly appreciated. Thanks in advance for any help!</p>
<p>I tried setting up API authentication</p>
","gpt-3"
"77549252","Excel in Large-Scale Data Processing with GPT-3.5 and Embeddings","2023-11-25 19:07:03","77678776","-1","954","<excel><vba><openai-api><data-processing><gpt-3>","<p>I'm working on integrating OpenAI functionalities, specifically GPT3.5 and embeddings, into a large system of Excel workbooks used for almost anything in my office. Our goal is having GPT3.5 taking over the place of a set of macros that has become hell maintaining, and the embeddings-ada-002 for improving the searches and categorization of data.</p>
<p>However, I'm concerned of Excel being capable to handle hundreds of these high-dimensional embeddings. I'm also afraid that Excel and GPT3.5 will struggle when having multiple sheets of thousands of rows tall and &quot;complex structures&quot; provided as a context.</p>
<p>(We run Windows on i5 processors)</p>
<p>Will Excel be able to handle such large-scale data processing with AI models? Any insights on this matter?</p>
","gpt-3"
"77547419","how to solve problem? (about #chromedriver #WinError3 #path)","2023-11-25 10:06:07","","0","10","<gpt-3>","<p>I created the following code for web scraping. However, I keep encountering the error &quot;FileNotFoundError: [WinError 3] The system cannot find the path specified: 'C:\Program Files\Google\Chrome\Application'.&quot; How can I resolve this issue? Can you help me?
The methods I tried to resolve this issue are as follows:
Installed the latest version of ChromeDriver and Chrome.
Updated the Selenium library.
Utilized the ChromeDriver version management library (I manually reinstalled it after removing the library).
The result remains as follows.
[Result]
<a href=""https://i.sstatic.net/zPFci.png"" rel=""nofollow noreferrer"">enter image description here</a>
And this is my original code.
[code]
<a href=""https://i.sstatic.net/Oa4c5.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
","gpt-3"
"77527907","Langchain is not storing the first question that I ask","2023-11-22 07:03:13","","0","236","<openai-api><langchain><chatgpt-api><gpt-3><py-langchain>","<p>I dont know why but why but langchain ConversationalRetrievalChain is not remembering the first question that I ask.</p>
<pre><code>from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate
# Define the system message template
system_template = &quot;&quot;&quot;you are an AI assistant named aidan. your job is to recommend the best clinicians based 
on the symptoms and locations. ask the user about location preference and then recommend the clinicians based on their symptoms and preferences
        ----------------
        Context
        {context}


        Chat History
        {chat_history}
        
        QUESTION:
        {question}
        
        ANSWER:

        
        &quot;&quot;&quot;

        # Create the chat prompt templates
messages = [
        SystemMessagePromptTemplate.from_template(system_template),
        HumanMessagePromptTemplate.from_template(&quot;{question}&quot;)
        ]
qa_prompt = ChatPromptTemplate.from_messages(messages)



chain = ConversationalRetrievalChain.from_llm(
llm = ChatOpenAI(temperature=1,model_name='gpt-3.5-turbo'),combine_docs_chain_kwargs={&quot;prompt&quot;: qa_prompt}, return_source_documents=True,
retriever=vectorstore.as_retriever())


chat_history=[]
result = chain({&quot;question&quot;: &quot;&quot;&quot; I have Concussion.
&quot;&quot;&quot;, &quot;chat_history&quot;:chat_history})
print(result)



chat_history=[(&quot;I have concussion&quot;,result['answer'])]
result = chain({&quot;question&quot;: &quot;&quot;&quot; cordova bay.
&quot;&quot;&quot;, &quot;chat_history&quot;:chat_history})
print(result)


</code></pre>
<p>when I execute the chain &quot;I have concussion&quot; the answer I get is good. but when I enter the cordova bay(location) it forgets that I have already entered the symptoms.</p>
","gpt-3"
"77526724","How to parse runtime variables to function call of chatgpt-3.5-turbo-1106?","2023-11-22 00:41:29","","0","196","<python><chatgpt-api><gpt-3>","<p>While it is a well-known fact that the chatgpt-3.5-turbo-1106 can call functions by parsing arguments extracted from the user input, I am curious how we can parse our desired variable by code, not from user input extracted by GPT model.</p>
<p>Assuming we have defined the following function while creating an GPT assistant</p>
<pre><code>{
    &quot;type&quot;: &quot;function&quot;,
    &quot;function&quot;: {
                    &quot;name&quot;: &quot;store_to_db&quot;,
                    &quot;description&quot;: &quot;store captured data to DB&quot;,
                    &quot;parameters&quot;: {
                        &quot;type&quot;: &quot;object&quot;,
                        &quot;properties&quot;: {
                             &quot;name&quot;: {
                                &quot;type&quot;: &quot;string&quot;,
                                &quot;description&quot;: &quot;Name of the user.&quot;
                            },
                            &quot;email&quot;: {
                                &quot;type&quot;: &quot;string&quot;,
                                &quot;description&quot;: &quot;Email of the user.&quot;
                            },
                            &quot;postcode&quot;: {
                                &quot;type&quot;: &quot;string&quot;,
                                &quot;description&quot;: &quot;Postcode of the user.&quot;
                            },
                            &quot;uuid&quot;: {
                                &quot;type&quot;: &quot;string&quot;,
                                &quot;description&quot;: &quot;current uuid of user session&quot;
                            }
                        },
                        &quot;required&quot;: []
                    }
                }
}
</code></pre>
<p>the assistant model doesn't know what the uuid, because it can not be provided from the user's input, but only name, email, postcode.</p>
<p>In this case, i tried the following.</p>
<pre><code>global uuid // uuid has been obtained somewhere in this code programmatically, not from GPT and user input.

if tool_call.function.name == &quot;store_to_db&quot;:
          try:
            arguments = json.loads(tool_call.function.arguments)

            name = arguments.get(&quot;name&quot;, None)
            email = arguments.get(&quot;email&quot;, None)
            postcode = arguments.get(&quot;postcode&quot;, None)

            output = store_db(name=name, email=email, postcode=postcode, uuid=uuid)

            client.beta.threads.runs.submit_tool_outputs(thread_id=thread_id,
                                                        run_id=run.id,
                                                        tool_outputs=[{
                                                            &quot;tool_call_id&quot;: tool_call.id,
                                                            &quot;output&quot;: output
}])
</code></pre>
<p>Of course, this outputs an error that the argument is no defined.
Is there any workaround? Can not we go beyond the limitation of the current function call method of GPT model?</p>
","gpt-3"
"77506707","chatbot with gpt3 in angular","2023-11-18 11:34:39","","-2","213","<angular><typescript><chatbot><openai-api><gpt-3>","<p>I'm making my own chat application in Angular with chatgpt3. When I tried the api url in the documentation, I got error 429. What do you think could be the reason?</p>
<p>service.ts</p>
<pre><code>export class OpenaiService {

  private apiKey = 'sk-xxxx';
  private apiUrl = 'https://api.openai.com/v1/chat/completions';

  constructor(private http: HttpClient) { }

  teachGPT(userMessage: string): Observable&lt;string&gt; {
    const headers = new HttpHeaders({
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${this.apiKey}`
    });

    const requestData = {
      model: 'gpt-3.5-turbo',
      messages: [
        { role: 'system', content: 'You are a helpful assistant.' },
        { role: 'user', content: userMessage }
      ]
    };

    return this.http.post&lt;any&gt;(this.apiUrl, requestData, { headers })
      .pipe(
        map(response =&gt; response.choices[0].message.content),
        catchError(error =&gt; {
          console.error('An error occurred:', error);
          return of('');
        })
      );
  }
}
</code></pre>
<p>chat.ts</p>
<pre><code>export class ChatPageComponent {

  userMessage: string = '';
  chatGPTResponse: string | undefined;

  constructor(private openaiService: OpenaiService) { }

  onSubmit() {
    this.openaiService.teachGPT(this.userMessage)
      .subscribe(
        response =&gt; {
          this.chatGPTResponse = response;
          console.log('ChatGPT Cevabƒ±:', response);
        },
        error =&gt; {
          console.error('Bir hata olu≈ütu:', error);
        }
      );
  }
}
</code></pre>
<p>console error:
<a href=""https://i.sstatic.net/3ZDVW.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/3ZDVW.jpg"" alt=""enter image description here"" /></a></p>
<p>When I use the URL <a href=""https://api.openai.com/v1/chat/completions"" rel=""nofollow noreferrer"">https://api.openai.com/v1/chat/completions</a>, which is the url in the gpt 3.5 turbo documentation, I get error 429. If the URL suggested by gpt was <a href=""https://api.openai.com/v1/engines/davinci-codex/completions"" rel=""nofollow noreferrer"">https://api.openai.com/v1/engines/davinci-codex/completions</a>, I got a 400 bad request error.</p>
","gpt-3"
"77487437","Open AI GPT API: How to get completion in between (fill-in-the-middle)?","2023-11-15 11:45:54","77487438","0","722","<openai-api><gpt-3>","<p>In this announcement: <a href=""https://openai.com/blog/gpt-3-edit-insert"" rel=""nofollow noreferrer"">https://openai.com/blog/gpt-3-edit-insert</a>, an insert mode that lets you have completion in the middle of a text is introduced. How can I have it through the Open AI GPT models' completion API?</p>
<p>For instance in the case of a code completion (which Github Copilot does currently):</p>
<pre class=""lang-py prettyprint-override""><code>def say_hello():
    print('hi', name)
</code></pre>
<p>should be completed with <code>name</code> argument inside the parentheses:</p>
<pre class=""lang-py prettyprint-override""><code>def say_hello(name):
    print('hi', name)
</code></pre>
","gpt-3"
"77477340","How to use GPT's function calling for complex sequential tasks?","2023-11-13 23:05:36","","0","319","<sql><information-retrieval><large-language-model><gpt-3><gpt-4>","<p>I am working on a project where the user can ask any question about their emails to a chatbot, and get an answer. I have been trying to use GPT to achieve this using a RAG (Retrieval Augmented Generation) model with the help of semantic search. However, I have noticed that the primitive chatbot I developed can answer questions such as &quot;Did I receive an email from X&quot; or &quot;Do I have any payments due?&quot; but often retrieves emails from a long time ago. The problem is that when I modify the question to state &quot;Did I receive an email from X in the past week?&quot;, it can't retrieve the right emails because the embeddings I used do not understand &quot;in the past week&quot; even if I provide the current date in the query. In order to fix this and a few other problems, I want to implement a two layered system where I put all my emails in a SQL database with their message-ids, to, and from etc, choose the relevant emails from the database and then run semantic search on those to provide the final answer.</p>
<p>Now, my idea is to define two functions: one to generate a SQL query and retrieve emails from the database, and the other to run semantic search. I want to input these as functions to the GPT module and let it decide when it wants to call either function.</p>
<p>For example, if the question was &quot;How many emails did I receive in the past week?&quot;, all it has to do is run the SQL function. If the question was &quot;Did my credit card get approved?&quot;, it only needs to run a semantic search. However, for complex queries such as &quot;Summarize all important emails from the past week&quot;, it first needs to run the SQL function, then semantic search for &quot;important&quot;, and summarize. I am not sure how GPT can break it down into &quot;retrieve emails from SQL database for the past week&quot; and &quot;Run semantic search to look for important emails&quot;.</p>
<p>I have tried running test runs with chat gpt with a prompt along the lines of &quot;If you can't answer the question based on the given columns and their descriptions in the table, return the entire table&quot; to see if it can figure out when to run a SQL query and when not to. But the results aren't great. And I imagine this would be even harder when all of this is integrated and it has to figure out that it needs to do things sequentially. Any help is appreciated.</p>
","gpt-3"
"77469966","OpenAI API error: ""You tried to access openai.Completion, but this is no longer supported in openai>=1.0.0""","2023-11-12 17:39:58","","6","34257","<python><openai-api><gpt-3>","<p>I want to translate the texts in a csv file into English using the GPT 4 model, but I constantly get the following error. Even though I updated the version, I continue to get the same error.</p>
<pre><code>import openai
import pandas as pd
import os
from tqdm import tqdm


openai.api_key = os.getenv(&quot;API&quot;)

def translate_text(text):
    response = openai.Completion.create(
        model=&quot;text-davinci-003&quot;,  # GPT-4 modeli
        prompt=f&quot;Translate the following Turkish text to English: '{text}'&quot;,
        max_tokens=60
    )
    # Yeni API yapƒ±sƒ±na g√∂re yanƒ±tƒ±n alƒ±nmasƒ±
    return response.choices[0].text.strip()

df = pd.read_excel('/content/3500-turkish-dataset-column-name.xlsx')

column_to_translate = 'review'

df[column_to_translate + '_en'] = ''

for index, row in tqdm(df.iterrows(), total=df.shape[0]):
    translated_text = translate_text(row[column_to_translate])
    df.at[index, column_to_translate + '_en'] = translated_text

df.to_csv('path/to/your/translated_csvfile.csv', index=False)

</code></pre>
<pre><code> 0%|          | 0/3500 [00:00&lt;?, ?it/s]
---------------------------------------------------------------------------
APIRemovedInV1                            Traceback (most recent call last)
&lt;ipython-input-27-337b5b6f4d32&gt; in &lt;cell line: 29&gt;()
     28 # Her satƒ±rdaki metni √ßevir ve yeni s√ºtuna kaydet
     29 for index, row in tqdm(df.iterrows(), total=df.shape[0]):
---&gt; 30     translated_text = translate_text(row[column_to_translate])
     31     df.at[index, column_to_translate + '_en'] = translated_text
     32 

3 frames
/usr/local/lib/python3.10/dist-packages/openai/lib/_old_api.py in __load__(self)

APIRemovedInV1: 

You tried to access openai.Completion, but this is no longer supported in openai&gt;=1.0.0 - see the README at https://github.com/openai/openai-python for the API.

You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. 

Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`

A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742
</code></pre>
<p>Even though I updated the OpenAI package version, I get the same error.</p>
","gpt-3"
"77467150","gpt3 finetuning with fine-tunes.create argument is not working","2023-11-11 22:57:15","","1","905","<openai-api><gpt-3>","<p>I want to finetune one openai model on my data, for that I gave this command (as per openai docs)</p>
<p><code>openai api fine_tunes.create -t &quot;abc_700_prepared_train.jsonl&quot; -v &quot;abc_700_prepared_valid.jsonl&quot; --compute_classification_metrics --classification_n_classes 5</code></p>
<p>But this error is coming</p>
<pre><code>usage: openai api [-h]
                  {chat.completions.create,images.generate,images.edit,images.create_variation,audio.transcriptions.create,audio.translations.create,files.create,files.retrieve,files.delete,files.list,models.list,models.retrieve,models.delete,completions.create}
                  ...
openai api: error: argument {chat.completions.create,images.generate,images.edit,images.create_variation,audio.transcriptions.create,audio.translations.create,files.create,files.retrieve,files.delete,files.list,models.list,models.retrieve,models.delete,completions.create}: invalid choice: 'fine_tunes.create' (choose from 'chat.completions.create', 'images.generate', 'images.edit', 'images.create_variation', 'audio.transcriptions.create', 'audio.translations.create', 'files.create', 'files.retrieve', 'files.delete', 'files.list', 'models.list', 'models.retrieve', 'models.delete', 'completions.create')
</code></pre>
<p>Not sure why is this coming. As per openai docs <a href=""https://platform.openai.com/docs/guides/legacy-fine-tuning"" rel=""nofollow noreferrer"">link</a>, still this fine-tunes.create works.Can anyone help me on this?
`</p>
<p>I am expecting a finetuned ada model, after giving that command. A few months before I did the same way and it worked now it is not coming.</p>
<p><a href=""https://i.sstatic.net/zTmiF.png"" rel=""nofollow noreferrer"">refer this image</a></p>
","gpt-3"
"77457991","Using webpilot API + GPT 4 or GPT 3.5 API","2023-11-10 06:04:01","","1","275","<openai-api><chatgpt-api><gpt-3><chat-gpt-4><chatgpt-plugin>","<p>Has anyone tried to use the webpilot api with gpt 4 or gpt 3.5 api to be able to have prompts like ‚ÄúCreate references for this topic or concept‚Äù?</p>
<p>Or basically have access to the internet through GPT4 or GPT3.5 with the help of webpilot?</p>
<p>When I provide this prompt to ChatGPT api without WebPilot, it generates references that don't actually exist, no matter how well I refine the prompt.</p>
","gpt-3"
"77454460","Is the new OpenAI API version backward compatible for accessing/querying GPT 3.5 Turbo?","2023-11-09 15:35:46","","0","105","<python><openai-api><gpt-3><gpt-4>","<p>Is the new API version backward compatible for accessing/querying GPT 3.5 Turbo? For example, will code including the following tailored to the new version still work with 3.5 Turbo?</p>
<pre><code>client = OpenAI(api_key = get_key())
response = client.chat.completions.create(
    model = model,
    messages = messages,
    max_tokens = max_tokens,
    n = n,
    stop = stop,
    temperature = temperature,
    frequency_penalty = frequency_penalty,
    presence_penalty = presence_penalty
    )
</code></pre>
","gpt-3"
"77441798","OpenAI SDK v1.0 API error: ""AttributeError: 'OpenAI' object has no attribute 'Completion'""","2023-11-07 22:18:49","","0","4045","<openai-api><gpt-3>","<p>So I've got some ganky code for writing a business plan, and it was working but is dead now as of <code>v1.0</code>, I can't work out how to properly re-code it without <code>AttributeError: 'OpenAI' object has no attribute 'Completion'</code> error occurring. Anyone able to help?</p>
<pre><code># Function to generate text for each section using OpenAI's API
def generate_text_for_section(section_name, user_input):
response = openai.Completion.create(
    model=&quot;text-davinci-003&quot;,
    prompt=f&quot;Write a detailed description for a business plan section titled '{section_name}'. Project idea: {user_input}.&quot;,
    max_tokens=500
)
return response.choices[0].text.strip()
</code></pre>
","gpt-3"
"77428156","Confusing answers - Azure Openai integration with Servicenow","2023-11-06 00:26:35","","0","80","<openai-api><servicenow><chatgpt-api><gpt-3><servicenow-rest-api>","<p>I'm facing a curious problem. I created a gpt-35-turbo deployment within Azure Openai Studio, defined a persona and am consuming this deployment via API in Servicenow. However, the responses coming from Azure are completely random and do not match the persona chosen within Azure. Can anyone give me a tip on where I can adjust it?</p>
<p><a href=""https://i.sstatic.net/UBRV5.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p><a href=""https://i.sstatic.net/6Java.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p><a href=""https://i.sstatic.net/GkYQk.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>I tried to change the parameters in the JSON, such as temperature and model used, but without success.</p>
","gpt-3"
"77357122","GPT 3.5 turbo is cutting the response without hitting the token limit","2023-10-25 06:42:42","77358197","-1","484","<flutter><openai-api><gpt-3>","<p>I trying to build an app using openAI API, and when using the openAI API i am passing two messages in the post request. The first one is as a system role which contains a list of exercises such as</p>
<pre><code>const initialPrompt = '''
                    0001    3/4 Sit-up
                    0003    Air bike
                    0024    Barbell Bench Front Squat
                    0025    Barbell Bench Press
                    0026    Barbell Bench Squat
                    0027    Barbell Bent Over Row
                    0030    Barbell Close-Grip Bench Press
                    ''';
</code></pre>
<p>The second message is as a user:</p>
<pre><code>const userMessageString = &quot;&quot;&quot;
                Give 3 days workout routine based on these exercises.
                Keep the ID of the exercises on the answer, 
                give me the answer in a complete json format as [days{[exercises{exerciseId, exerciseName, sets, reps}]}]
                &quot;&quot;&quot;;
</code></pre>
<p>And i want to get a json response using this method:</p>
<pre><code>  Future&lt;String&gt; sendOpenAIRequest() async {
final apiUrl = 'https://api.openai.com/v1/chat/completions';

final response = await http.post(
  Uri.parse(apiUrl),
  headers: {
    'Content-Type': 'application/json',
    'Authorization': 'Bearer $openaiApiKey',
  },
  body: json.encode({
    &quot;model&quot;: &quot;gpt-3.5-turbo&quot;,
    &quot;messages&quot;: [
      {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: initialPrompt},
      {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: userMessageString}
    ],
    &quot;max_tokens&quot;: 3000,
  }),
);

if (response.statusCode == 200) {
  final result = json.decode(response.body);
  print('usage &gt;&gt;&gt; ${result['usage']}');
  return result['choices'][0]['message']['content'];
} else {
  throw Exception('Request failed with status: ${response.statusCode}');
}
</code></pre>
<p>However the answer that i get is cut, even though the number of tokens is far from the max_tokens</p>
<pre><code>usage &gt;&gt;&gt; {prompt_tokens: 143, completion_tokens: 324, total_tokens: 467}
{
  &quot;days&quot;: [
    {
      &quot;exercises&quot;: [
        {
          &quot;exerciseId&quot;: &quot;0026&quot;,
          &quot;exerciseName&quot;: &quot;Barbell Bench Squat&quot;,
          &quot;sets&quot;: 3,
          &quot;reps&quot;: 12
        },
        {
          &quot;exerciseId&quot;: &quot;0030&quot;,
          &quot;exerciseName&quot;: &quot;Barbell Close-Grip Bench Press&quot;,
          &quot;sets&quot;: 3,
          &quot;reps&quot;: 12
        }
      ]
    },
    {
      &quot;exercises&quot;: [
        {
          &quot;exerciseId&quot;: &quot;0027&quot;,
          &quot;exerciseName&quot;: &quot;Barbell Bent Over Row&quot;,
          &quot;sets&quot;: 3,
          &quot;reps&quot;: 10
        },
        {
          &quot;exerciseId&quot;: &quot;0024&quot;,
          &quot;exerciseName&quot;: &quot;Barbell Bench Front Squat&quot;,
          &quot;sets&quot;: 3,
          &quot;reps&quot;: 10
        }
      ]
    },
    {
      &quot;exercises&quot;: [
        {
          &quot;exerciseId&quot;: &quot;0025&quot;,
          &quot;exerciseName&quot;: &quot;Barbell Bench Press&quot;,
          &quot;sets&quot;: 4,
          &quot;reps&quot;: 8
        },
        {
          &quot;exerciseId&quot;: &quot;0003&quot;,
          &quot;exerciseName&quot;: &quot;Air bike&quot;,
          &quot;sets&quot;: 4,
          &quot;reps&quot;: 12
        },
        {
          &quot;exercise
</code></pre>
","gpt-3"
"77355706","Intermittent Hang in Python on openai.ChatCompletion.create() Method Call with GPT-3","2023-10-24 22:26:13","","4","396","<python><python-3.x><freeze><openai-api><gpt-3>","<p>I am encountering a strange bug while using the openai Python library. Specifically, the method <code>openai.ChatCompletion.create()</code> intermittently hangs during execution without any error messages or exceptions being raised.</p>
<p>I am working on a script to process a dataset of 100 strings using a custom class named <code>GPTHandler</code>.</p>
<p>I'm running on windows 11, python 3.11.3, openai library v 0.28.1</p>
<p>The script reads strings from a spreadsheet, processes them, and sends them to the OpenAI GPT-3.5 Turbo model for text completion. Here's a simplified version of the setup:</p>
<pre><code>import openai

class GPTHandler:
    def __init__(self):
        # Load base payload configuration from a config file
        # ...

    def use_gpt(self, string_to_be_processed):
        # Update the payload with the new string
        # ...
        payload = self._build_gpt_payload(string_to_be_processed)  
        # validated with logs this returns a valid payload on calls that hang
        # {
        #     &quot;model&quot;: &quot;gpt-3.5-turbo&quot;,
        #     &quot;mesages&quot;: [
        #         {
        #             &quot;role&quot;: &quot;user&quot;,
        #             &quot;content&quot;: &quot;This is a test string.&quot;
        #         }
        #     ]
        # }
        try:
            print(&quot;Sending API request...&quot;)
            gpt_response = openai.ChatCompletion.create(**payload)
            print(&quot;API request complete.&quot;)
            return gpt_response
        except Exception as e:
            print(f&quot;Exception: {e}&quot;)

# Load strings from spreadsheet
# ...

# Instantiate GPTHandler
gpt_handler = GPTHandler()

# Process each string
for string in strings:
    gpt_handler.use_gpt(string)

</code></pre>
<p>The issue occurs intermittently. The terminal prints &quot;Sending API request...&quot; but then hangs indefinitely without printing &quot;API request complete.&quot; or raising any exception. Initially, I thought it was a delay, but even after waiting for 20 minutes, the hang persists. Notably, pressing <code>Ctrl+C</code> doesn‚Äôt terminate the script, and I have to kill the terminal to stop it. This hang happens on different strings and after a varying number of API calls.</p>
<p>I implemented a <code>TimeoutTracer</code> class to set a timeout using <code>sys.settrace</code>, hoping to interrupt the hang and gather more information.</p>
<pre><code>
class TimeoutTracer:
    def __init__(self, start, TOTAL_TIMEOUT):
        self.start = start
        self.TOTAL_TIMEOUT = TOTAL_TIMEOUT

    def trace_function(self, frame, event, arg):
        logger.debug(f&quot;trace called at time: {time.time()}&quot;)
        if time.time() - self.start &gt; self.TOTAL_TIMEOUT:
            logger.error(f&quot;{'#'*20} ERROR {'#'*20}&quot;)
            logger.error(f&quot;Timeout of {self.TOTAL_TIMEOUT} seconds exceeded!&quot;)
            raise Exception('Timed out!')
        return self.trace_function



class GPTHandler:
    def __init__(self):
        # ...

    def use_gpt(self, string_to_be_processed):
        payload = self._build_gpt_payload(string_to_be_processed)
        TOTAL_TIMEOUT = 60
        start = time.time()
        tracer = TimeoutTracer(start, TOTAL_TIMEOUT)
        sys.settrace(tracer.trace_function)
        try:
            print(&quot;Sending API request...&quot;)
            gpt_response = openai.ChatCompletion.create(**payload)
            print(&quot;API request complete.&quot;)
        except Exception as e:
            logger.error(f&quot;Error: {e}&quot;)
            return &quot;{}&quot;
        finally:
            sys.settrace(None)  # Remove the time constraint and continue normally.
        # ... (process gpt_response)

# ... (rest of the script including loading strings and processing loop)

</code></pre>
<p>I had a log output for every time <code>sys.settrace</code> was called and found that when it hangs, <code>sys.settrace</code> stops being called, which suggests that no code is being executed during these hang periods. This might also explain why <code>Ctrl+C</code> isn‚Äôt working. Additionally, I am not hitting any rate limits as OpenAI would return a message in that case.</p>
<p>I am looking for insights or suggestions on what could be causing this hang and how to resolve or debug it further. Any help would be greatly appreciated!</p>
","gpt-3"
"77328411","One to One video call between Ai and User","2023-10-20 04:56:41","","0","68","<reactjs><openai-api><gpt-3><videochat><videosdk.live>","<p>I am developing an application that facilitates video chat between users and an AI (GPT model). The typical architecture of this app is as follows: I am utilizing <a href=""https://docs.videosdk.live/react/guide/video-and-audio-calling-api-sdk/concept-and-architecture"" rel=""nofollow noreferrer"">VideoSDK-React</a> for the video call service. I need to connect this service with a Node.js server in order to transmit audio (questions asked by the user) to the AI and relay the AI's responses back to the user.</p>
<p><a href=""https://i.sstatic.net/gPkVT.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/gPkVT.png"" alt=""enter image description here"" /></a></p>
<p>I tried with creating singnaling server from scratch but the main functionality of project is to integrate gpt model as a one peer so decide to go with third party service sdk.</p>
","gpt-3"
"77323771","Microsoft Azure Machine Learning Studio - GPT 3.5 Turbo Fine-Tuning Problem","2023-10-19 12:29:31","","0","129","<azure><gpt-3>","<p>The error is as follows:&quot;The intellectual property publisher for the compute is &quot;&quot; but the intellectual property publisher for the run configuration is &quot;OpenAIDevault&quot; and they must be the same. If you are using an intellectual property protected component, please make sure you are using an intellectual&quot;, I don't get how to fix it? Any suggestions?
<a href=""https://i.sstatic.net/u2zoS.png"" rel=""nofollow noreferrer"">Screenshot of my problem</a></p>
","gpt-3"
"77323710","Can I fine-tune the again a gpt-3.5-turbo fine tuned model with more data?","2023-10-19 12:20:20","","-1","446","<openai-api><gpt-3>","<p>I had fined tuned GPT 3.5 turbo model, can I fine tune that model again with more data to get better result.</p>
<p>I tried to fine tune model again but I cannot,is any possible way.</p>
<p>Is prompt is very in good to fine tune a model.Without prompt, can we get same result with prompt fine tuning.</p>
","gpt-3"
"77272754","OpenAI API gives random and nonsense responses","2023-10-11 11:45:57","","-3","761","<openai-api><chatgpt-api><gpt-3><gpt-4>","<p>I am developing a Next.js app where users pick some ingredients and get a recipe idea which can be made with them. I am using OpenAI API for that but I couldn't find a way to optimize it. I want to get responses with the same quality as in chatGPT (or something close to it) but instead, it gives me random, incomplete and out-of-context responses. I tried to play with max tokens, temperature, role, model etc. but it didn't work. Here is an example:</p>
<p>This is a prompt and response on chatGPT:</p>
<p><em><strong>prompt:</strong></em>
<em>Create a recipe made with potato, paprika, salmon and yogurt. First state the title, then the ingredients list and then step-by-step instructions.</em></p>
<p><em><strong>response:</strong></em>
<em>Title: Paprika-Spiced Salmon with Creamy Potato and Yogurt Mash</em></p>
<p><em>Ingredients:</em></p>
<p><em>For the Salmon:</em>
<em>- 2 salmon fillets</em>
<em>- 1 teaspoon paprika</em>
<em>- Salt and pepper to taste</em>
<em>- 2 tablespoons olive oil</em></p>
<p><em>For the Creamy Potato and Yogurt Mash:</em>
<em>- 4 medium-sized potatoes, peeled and cut into chunks</em>
<em>- 1/2 cup plain yogurt</em>
<em>- Salt and pepper to taste</em></p>
<p><em>Instructions:</em></p>
<p><em>For the Salmon:</em></p>
<p><em>1. Preheat your oven to 375¬∞F (190¬∞C).</em></p>
<p><em>2. So on....</em></p>
<p>This result is exactly what I want.</p>
<p>And this is the result I get when I try the same prompt with the API :</p>
<pre><code>{
  &quot;prompt&quot;: &quot;Please create a recipe made with potato, paprika, salmon and yogurt. First state the title, then the ingredients list and then step-by-step instructions.&quot;,
  &quot;max_tokens&quot;: 200,
  &quot;temperature&quot;: 0.3
}

response: 

{
    &quot;id&quot;: &quot;cmpl-88ReJXcbcT3zstFOQTStJDnG528en&quot;,
    &quot;object&quot;: &quot;text_completion&quot;,
    &quot;created&quot;: 1697023599,
    &quot;model&quot;: &quot;davinci&quot;,
    &quot;choices&quot;: [
        {
            &quot;text&quot;: &quot;\n\nEasy ‚òÜ Cooked rice remake risotto\n\nI will remake the cooked rice deliciously. Since the ingredients are already included, it is delicious risotto with only eggs.\n\nTakoyaki style rice ball\n\nI love takoyaki, but it is troublesome to make, so it's a Takoyaki style rice ball. Because the color is also beautiful, it is also recommended for Hinamatsuri and lunch.\n\nRange easy! Omurice style\n\nAll the ingredients are fine about your choice. Washing is easy because it can be made without the need for cooking utensils.\n\nEasy lunch ‚ô° ‚ô° ‚ô° ÁÇí rice bowl\n\nTuna fried rice ‚ô° „Éë „É© „Åµ „Åµ „Åµ „Åµ „Åµ „Åµ „Åµ ‚ô° ‚ô° ‚ô° ‚ô° ‚ô° „Åô „Åê\n\nSoymilk Japanese-style doria\n\nThis&quot;,
            &quot;index&quot;: 0,
            &quot;logprobs&quot;: null,
            &quot;finish_reason&quot;: &quot;length&quot;
        }
    ],
    &quot;usage&quot;: {
        &quot;prompt_tokens&quot;: 33,
        &quot;completion_tokens&quot;: 200,
        &quot;total_tokens&quot;: 233
    }
}
</code></pre>
<p>Are the some specific properties to optimize the results with API? Or is it not possible to get chatGPT-quality results no matter what I do?</p>
","gpt-3"
"77229890","train gpt-2 modal with json data","2023-10-04 13:10:46","","0","343","<python><training-data><gpt-3><gpt-2>","<p>I have a dataset in JSON format that contains questions, options, categories, and correct answers. I would like to train a GPT-2 model on this dataset, but I am getting an error.</p>
<p>i got following error: &quot;ImportError: cannot import name 'Dataset' from 'transformers'&quot;</p>
<p>I have written the following code:</p>
<pre><code>import json
import random
from transformers import GPT2LMHeadModel, Dataset

def convert_json_to_text(json_data):
 

  text = ''
  for question_and_answers in json_data: 
    random.shuffle(question_and_answers['answers'])
 
    text += f&quot;{question_and_answers['category']}: {question_and_answers['question']}\n&quot;
 
    for option in question_and_answers['answers']:
      text += f&quot;- {option}\n&quot;
 
    text += f&quot;Correct Answer: {question_and_answers['correct_answer']}\n\n&quot;

  return text
 
with open(&quot;questions.json&quot;, &quot;r&quot;) as f:
  json_data = json.load(f)
 
text = convert_json_to_text(json_data)
 
train_dataset = Dataset.from_text(text)
 
model = GPT2LMHeadModel.from_pretrained(&quot;gpt2&quot;)
model.train()
for epoch in range(10):
  for batch in train_dataset:
    loss = model(input_ids=batch['input_ids'], labels=batch['input_ids'])
    loss.backward()
    model.optimizer.step()
    model.optimizer.zero_grad()
 
model.save_pretrained(&quot;gpt2_model.pt&quot;)
</code></pre>
<p>Here is sample dataset:</p>
<pre><code>[
  {
    &quot;question&quot;: &quot;Q1. Which operator returns true if the two compared values are not equal?&quot;,
    &quot;category&quot;: &quot;javascript&quot;,
    &quot;answers&quot;: [&quot; &lt;&gt;&quot;, &quot; ~&quot;, &quot; ==!&quot;, &quot; !==&quot;],
    &quot;correct_answer&quot;: &quot; !==&quot;
  },
  {
    &quot;question&quot;: &quot;Q2. How is a forEach statement different from a for statement?&quot;,
    &quot;category&quot;: &quot;javascript&quot;,
    &quot;answers&quot;: [
      &quot; Only a for statement uses a callback function.&quot;,
      &quot; A for statement is generic, but a forEach statement can be used only with an array.&quot;,
      &quot; Only a forEach statement lets you specify your own iterator.&quot;,
      &quot; A forEach statement is generic, but a for statement can be used only with an array.&quot;
    ],
    &quot;correct_answer&quot;: &quot; A for statement is generic, but a forEach statement can be used only with an array.&quot;
  }
]
</code></pre>
","gpt-3"
"77205271","Connecting spotify api with gpt 3 api on wordpress website","2023-09-30 00:13:59","","0","51","<wordpress><spotify><gpt-3>","<p>I'm working on a WordPress website and trying to make Spotify and GPT-3 play nice together on my site. It's a bit tricky, and I could use some help. I'm looking for an easy way to grab info from Spotify‚Äîlike playlists and songs‚Äîright into my WordPress site.</p>
<p>Any WordPress plugins that can help accomplish this task?<br />
If so, how can I use the plugin to do this?<br />
If not, how can I accomplish this through code?</p>
","gpt-3"
"77179134","Loading input as a string with Langchain","2023-09-26 10:26:57","","0","955","<python><openai-api><langchain><gpt-3><chromadb>","<p>I've been trying to load the dynamic user queries and the answers that I receive for it into Chroma DB in langachain. I have only options to load input as documents in langchain. Is there a possibility to load input strings in langchain ?</p>
<p>I have attached the function in which I'm trying to pass the query and answer as parameters receieved from earlier functions.</p>
<pre><code>    def store_query_and_answer(query, answer):
      embeddings = OpenAIEmbeddings()
      Chroma.from_documents(documents=query, embedding=embeddings, persist_directory=&quot;./&quot;)
      Chroma.from_documents(documents=answer, embedding=embeddings, persist_directory=&quot;./&quot;) 
</code></pre>
<p>Instead of loading this as a document, I want to load the queries as a line of strings. For example if I asked a query and got an answer like below from another function,</p>
<p>query : What is your name ?</p>
<p>answer : My name is XYZ.</p>
<p>Please post your suggestions on this, I truly appreciate your time and efforts ‚ù§</p>
","gpt-3"
"77174370","What should be the data structure before tokenizing the text based data for Tranformer-Based Model?","2023-09-25 16:35:34","","1","21","<nlp><transformer-model><gpt-3>","<p>I have seen some examples where people are mostly getting text-based datasets in the form of JSON, but I don't understand the reason why. Can anyone explain why I need to convert my data into a dictionary before tokenizing it for a Tranfomer-based model i.e (BERT, GPT-3, T5... etc). What I have so far learned, we need to know the dataset, and the attention mask will tell us which token needs to be targeted. But what about before tokenizing, even on HuggingFace no one talks about that, all examples gives one sentence example. I think it will be a great help if there is any explanation.</p>
","gpt-3"
"77173368","Do GPT-4 and GPT-3.5 share the same token encoder?","2023-09-25 14:08:12","","0","1183","<token><openai-api><gpt-3><gpt-4>","<p>I want to know if I can use the same token counter for various GPT models - especially GPT-3, GPT-3.5, and GPT-4.</p>
<p>GPT models by OpenAI need texts to be tokenized (using <a href=""https://en.wikipedia.org/wiki/Byte_pair_encoding"" rel=""nofollow noreferrer"">Byte Pair Encoding</a>, BPE), see <a href=""https://platform.openai.com/tokenizer"" rel=""nofollow noreferrer"">Interactive GPT tokenizer</a>. I haven't found a direct statement if they use the same or different tokenizers. Even this official OpenAI page says that:</p>
<blockquote>
<p>If you need a programmatic interface for tokenizing text, check out our <a href=""https://github.com/openai/tiktoken"" rel=""nofollow noreferrer"">tiktoken</a> package for Python. For JavaScript, the <a href=""https://www.npmjs.com/package/gpt-3-encoder"" rel=""nofollow noreferrer"">gpt-3-encoder</a> package for node.js <strong>works for most GPT-3 models</strong>.</p>
</blockquote>
","gpt-3"
"77134543","Hosting GPT-3 on cloud server in KSA","2023-09-19 12:25:27","","0","49","<nlp><openai-api><chatgpt-api><gpt-3><large-language-model>","<p>What is the alternative of LLM models that can be hosted on a cloud server on Saudi Arabia. These LLMs should be very good on supporting Arabic</p>
","gpt-3"
"77108296","Text-davinci-003 api pricing?","2023-09-14 20:50:28","77108333","2","2929","<openai-api><chatgpt-api><gpt-3>","<p>I am using the text-davinci-003 model in Python.</p>
<pre><code>response = openai.Completion.create(
     engine=&quot;text-davinci-003&quot;,
     prompt=question,
     max_tokens=100
)
</code></pre>
<p>I am finding the amount of tokens via: <code>print(response.usage.total_tokens)</code>. I cannot find for the life of me the pricing for <code>text-davinci-003 model</code>, only some forum I read that it is <code>0.0120 / 1000k tokens</code>. so based on this calculation, for my most recent request at <code>403</code> tokens, I should be being charged <code>0.004884</code> for the request. however, when I go into my billing dashboard, I see it went up by <code>1 cent</code>. just trying to figure out the total cost. Thanks for the help in advance.</p>
","gpt-3"
"77091715","Chat GPT API incomplete response React.js","2023-09-12 18:17:40","","0","231","<reactjs><request><chatgpt-api><gpt-3>","<p>I'm implementing a chat for my web app and I'm not getting the complete response of chat gpt in some cases. When the prompt is about giving a list it doesn't work correctly, for example if I ask for the top 3 richest countries the response is <code>the top 3 richest countries are:</code> and it stays like that. I can't figure out how the response that it gives me is incomplete, I tried playing with the tokens but it didn't work. I don't know what the problem is, I'm making the request directly from React this way:</p>
<pre class=""lang-js prettyprint-override""><code>export async function generateChatResponse(prompt: string): Promise&lt;string&gt; {
  try {
    console.log(prompt)
    const response = await axios.post(
      API_BASE_URL,
      {
        messages: [
          {
            role: 'user',
            content: prompt
          }
        ],
        model:'gpt-3.5-turbo',
        max_tokens: 150,
        temperature: 0.7,
        stop: '\n',
      },
      {
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${API_KEY}`,
        },
      }
    );
    console.log(response)
    return response.data.choices[0].message.content.trim();
  } catch (error) {
    console.error('Error generating chat response:', error);
    return 'Oops! An error occurred while processing your request.';
  }
}
</code></pre>
","gpt-3"
"77043020","Compare 2 pdf files langchain","2023-09-05 08:44:48","77150773","2","1693","<python><streamlit><pydantic><langchain><gpt-3>","<pre><code>import streamlit as st
import os
import tempfile
from pathlib import Path
from pydantic import BaseModel, Field
import streamlit as st
from langchain.chat_models import ChatOpenAI
from langchain.agents import Tool
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader
from langchain.chains import RetrievalQA
from langchain.agents import initialize_agent
import openai
os.environ[&quot;OPENAI_API_KEY&quot;] = &quot;&quot;
os.environ['OPENAI_API_TYPE'] = 'azure'
os.environ['OPENAI_API_VERSION'] = '2023-03-15-preview'
os.environ['OPENAI_API_BASE'] = &quot;https://summarization&quot;

#API settings for embedding
openai.api_type = &quot;azure&quot;
openai.api_base = &quot;https://summarization&quot;
openai.api_version = '2023-03-15-'
openai.api_key = &quot;&quot;
    
    
class DocumentInput(BaseModel):
    question: str = Field()

# Create a temporary directory in the script's folder
script_dir = Path(__file__).resolve().parent
temp_dir = os.path.join(script_dir, &quot;tempDir&quot;)


def main():
    st.title(&quot;PDF Document Comparison&quot;)

    # Create a form to upload PDF files and enter a question
    st.write(&quot;Upload the first PDF file:&quot;)
    pdf1 = st.file_uploader(&quot;Choose a PDF file&quot;, type=[&quot;pdf&quot;], key=&quot;pdf1&quot;)

    st.write(&quot;Upload the second PDF file:&quot;)
    pdf2 = st.file_uploader(&quot;Choose a PDF file&quot;, type=[&quot;pdf&quot;], key=&quot;pdf2&quot;)

    question = st.text_input(&quot;Enter your question&quot;)
    submit_button = st.button(&quot;Compare PDFs&quot;)

    if submit_button:
        if pdf1 and pdf2:
            if not os.path.exists(temp_dir):
                os.makedirs(temp_dir)
            else:
                # Clear the previous contents of the &quot;tempDir&quot; folder
                for file in os.listdir(temp_dir):
                    file_path = os.path.join(temp_dir, file)
                    try:
                        if os.path.isfile(file_path):
                            os.unlink(file_path)
                    except Exception as e:
                        print(f&quot;Error deleting file: {e}&quot;)

            # Save the PDF files to the &quot;tempDir&quot; directory
            pdf1_path = os.path.join(temp_dir, pdf1.name)
            with open(pdf1_path, 'wb') as f:
                f.write(pdf1.getbuffer())

            pdf2_path = os.path.join(temp_dir, pdf2.name)
            with open(pdf2_path, 'wb') as f:
                f.write(pdf2.getbuffer())



            llm = ChatOpenAI(temperature=0, model=&quot;gpt-3.5-turbo&quot;,engine=&quot;gpt-35-turbo&quot;)

            tools = []
            files = [

                {
                    &quot;name&quot;: pdf1.name,
                    &quot;path&quot;: pdf1_path,
                },

                {
                    &quot;name&quot;: pdf2.name,
                    &quot;path&quot;: pdf2_path,
                },
            ]

            for file in files:
                loader = PyPDFLoader(file[&quot;path&quot;])
                pages = loader.load_and_split()
                text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
                docs = text_splitter.split_documents(pages)
                embeddings = OpenAIEmbeddings()
                retriever = FAISS.from_documents(docs, embeddings).as_retriever()

                # Wrap retrievers in a Tool
                tools.append(
                    Tool(
                        args_schema=DocumentInput,
                        name=file[&quot;name&quot;],
                        description=f&quot;useful when you want to answer questions about {file['name']}&quot;,
                        func=RetrievalQA.from_chain_type(llm=llm, retriever=retriever),
                    )
                )
            agent = initialize_agent(
                tools=tools,
                llm=llm,
                verbose=True,
            )

            st.write(agent({&quot;input&quot;: question}))
            # Now you have both PDFs saved in the &quot;tempDir&quot; folder
            # You can perform your PDF comparison here


if __name__ == &quot;__main__&quot;:
    main()
</code></pre>
<p>I get the following error :
pydantic.v1.error_wrappers.ValidationError: 1 validation error for Tool
args_schema
subclass of BaseModel expected (type=type_error.subclass; expected_class=BaseModel) I am following the example from langchain documentation:<a href=""https://python.langchain.com/docs/integrations/toolkits/document_comparison_toolkit"" rel=""nofollow noreferrer"">https://python.langchain.com/docs/integrations/toolkits/document_comparison_toolkit</a></p>
","gpt-3"
"77040209","Langchain - get list of names from my documents","2023-09-04 19:33:51","","0","359","<openai-api><langchain><gpt-3><large-language-model><palm>","<p>I have more than 500 documents. Some of them have person names in it. I want to extract all names from all documents. I indexed documents using ChromaDB. I tried PlanAndExecute and and Self-ask with search agents but they could not provide a correct answer.</p>
<p><a href=""https://python.langchain.com/docs/modules/agents/agent_types/self_ask_with_search"" rel=""nofollow noreferrer"">https://python.langchain.com/docs/modules/agents/agent_types/self_ask_with_search</a></p>
<p><a href=""https://python.langchain.com/docs/modules/agents/agent_types/plan_and_execute"" rel=""nofollow noreferrer"">https://python.langchain.com/docs/modules/agents/agent_types/plan_and_execute</a></p>
<p>Is there a way to achieve this?</p>
","gpt-3"
"77007205","When using GPT-4 API, do I need to send the entire conversation back each time?","2023-08-30 10:35:20","","6","4158","<openai-api><chatgpt-api><gpt-3><gpt-4>","<p>I'm new to OpenAI API. I work with GPT-3.5-Turbo, using this code:</p>
<pre><code>messages = [
        {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You‚Äôre a helpful assistant&quot;}
    ]

    while True:
        content = input(&quot;User: &quot;)
        if content == 'end':
            save_log(messages)
            break
        messages.append({&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: content})

        completion = openai.ChatCompletion.create(
            model=&quot;gpt-3.5-turbo-16k&quot;,
            messages=messages
        )

        chat_response = completion.choices[0].message.content
        print(f'ChatGPT: {chat_response}')
        messages.append({&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: chat_response})
</code></pre>
<p>Result:
<em>User: who was the first person on the moon?
GPT: The first person to step foot on the moon was Neil Armstrong, an American astronaut, on July 20, 1969, as part of NASA's Apollo 11 mission.
User: how tall is he?
GPT: Neil Armstrong was approximately 5 feet 11 inches (180 cm) tall.</em></p>
<p>But it requires tons of tokens. And I've heard that GPT-4 differs from GPT-3 in that it's able to remember the previous messages (on its own). Is that correct?</p>
<p>But if I remove the line where I append the 'messages' list with the latest one and send only one message:
<code>completion = openai.ChatCompletion.create( model=&quot;gpt-4&quot;, messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: content}] )</code>
it can't remember anything.</p>
<p><em>User: who was the first person on the moon?
GPT: The first person on the moon was Neil Armstrong on July 20, 1969.
User: how tall is he?
GPT: Without specific context or information about who &quot;he&quot; refers to, I'm unable to provide an accurate answer.</em></p>
<p>So I'm wondering is there any workflow difference between GPT-3.5-Turbo and GPT-4?</p>
","gpt-3"
"77006420","Issue with gpt 3.5 Turbo while putting my API key in a basic code","2023-08-30 08:54:13","","-1","305","<python><artificial-intelligence><openai-api><gpt-3>","<pre><code>import openai
import os
import sys
  
try:
  openai.api_key = os.environ['OPENAI_API_KEY']
except KeyError:
  sys.stderr.write(&quot;&quot;&quot;
  You haven't set up your API key yet.
  
  If you don't have an API key yet, visit:
  
  https://platform.openai.com/signup

  1. Make an account or sign in
  2. Click &quot;View API Keys&quot; from the top right menu.
  3. Click &quot;Create new secret key&quot;

  Then, open the Secrets Tool and add OPENAI_API_KEY as a secret.
  &quot;&quot;&quot;)
  exit(1)

response = openai.ChatCompletion.create(
  model=&quot;gpt-3.5-turbo&quot;,
  messages=[
        {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;},
        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Who won the world series in 2020?&quot;},
        {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;The Los Angeles Dodgers won the World Series in 2020.&quot;},
        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Where was it played?&quot;}
    ]
)

print(response)


</code></pre>
<p>And i got this error back:</p>
<pre><code>Ó∫ß python3 main.py
Traceback (most recent call last):
  File &quot;/home/runner/AI-Code-1/main.py&quot;, line 23, in \&lt;module\&gt;
    response = openai.ChatCompletion.create(
  File &quot;/home/runner/AI-Code-1/venv/lib/python3.10/site-packages/openai/api_resources/chat_completion.py&quot;, line 25, in create
    return super().create(\*args, \*\*kwargs)
  File &quot;/home/runner/AI-Code-1/venv/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py&quot;, line 153, in create
    response, \_, api_key = requestor.request(
  File &quot;/home/runner/AI-Code-1/venv/lib/python3.10/site-packages/openai/api_requestor.py&quot;, line 226, in request
    resp, got_stream = self.\_interpret_response(result, stream)
  File &quot;/home/runner/AI-Code-1/venv/lib/python3.10/site-packages/openai/api_requestor.py&quot;, line 619, in \_interpret_response
    self.\_interpret_response_line(
  File &quot;/home/runner/AI-Code-1/venv/lib/python3.10/site-packages/openai/api_requestor.py&quot;, line 679, in \_interpret_response_line
    raise self.handle_error_response(
openai.error.RateLimitError: You exceeded your current quota, please check your plan and billing details.
exit status 1
</code></pre>
","gpt-3"
"76995983","How to fine tune gpt 3.5 on conversational data?","2023-08-28 21:00:01","","2","1891","<openai-api><gpt-3><fine-tuning>","<p>My app provides emotional support to our end users. We have listeners who listen to the end users via chat. Now I have chat threads of one particular listener let say A, and he had talked with 1000 users. I want to fine tune my own model using gpt-3 in a way our listener A reponds to the users.</p>
<p>I have prepared this chat data thread wise, meaning there will be 1000 examples/threads if listener had talked with 1000 users. The problem fine tune allow 4096 tokens per example, so how can I give thread which is very long to be fit in 4000 tokens data limit ?</p>
<p>And what is the best way to train thiis conversation data if my approach is wrong ?
I am adding some data sample which I am going to use.</p>
<pre><code>
{
  &quot;messages&quot;: [
    {
      &quot;role&quot;: &quot;system&quot;,
      &quot;content&quot;: &quot;Marv is a sympathetic human being who provides emoational support&quot;
    },
    {
      &quot;role&quot;: &quot;user&quot;,
      &quot;content&quot;: &quot;Hello&quot;
    },
    {
      &quot;role&quot;: &quot;assistant&quot;,
      &quot;content&quot;: &quot;Hey man, how are ya?   &quot;
    },
    {
      &quot;role&quot;: &quot;user&quot;,
      &quot;content&quot;: &quot;I‚Äôm not really good right now &quot;
    },
    {
      &quot;role&quot;: &quot;assistant&quot;,
      &quot;content&quot;: &quot;I‚Äôm sorry to hear that.  What‚Äôs going on? &quot;
    },
  ]
}
{
  &quot;messages&quot;: [
    {
      &quot;role&quot;: &quot;system&quot;,
      &quot;content&quot;: &quot;Marv is a sympathetic human being who provides emoational support&quot;
    },
    {
      &quot;role&quot;: &quot;user&quot;,
      &quot;content&quot;: &quot;I am feeling sad today&quot;
    },
    {
      &quot;role&quot;: &quot;assistant&quot;,
      &quot;content&quot;: &quot;I'm sorry to hear that you're feeling sad. What happened ?&quot;
    },
    {
      &quot;role&quot;: &quot;user&quot;,
      &quot;content&quot;: &quot;My mother passed away today&quot;
    },
    {
      &quot;role&quot;: &quot;assistant&quot;,
      &quot;content&quot;: &quot;Oh... really shoking to hear this. How you holding up now ?&quot;
    }
  ]
}
</code></pre>
","gpt-3"
"76991410","Customize QAGenerateChain prompt in langchain","2023-08-28 09:05:21","","0","304","<langchain><gpt-3>","<p>I want to customize the prompt of QAGenerateChain. However, the function doesn't accept the argument of the prompt. how can I change the following with my prompt?</p>
<p>`from langchain.evaluation.qa import QAGenerateChain</p>
<p>example_gen_chain = QAGenerateChain.from_llm(llm)
`</p>
<p>I  tries to modify the class</p>
","gpt-3"
"76985619","Predicting next questions in llm powered chatbot","2023-08-27 03:36:44","","0","2275","<chatbot><langchain><gpt-3><nlp-question-answering><large-language-model>","<p>I am building a question answering chatbot powered by llms. I have seen in chatbots like bing chat it predicts what might be the top three next questions user may ask.</p>
<p>My question is: How would I do the same in my chatbot?</p>
<p>I have implemented the qa chatbot using langchain.</p>
<p>Methods I thought of:</p>
<ol>
<li>Prompting the llm with history (both user question and bot reply) and then a line stating it to predict next question.
This gives very vague questions and lenghty ones.</li>
<li>Finetuning a basic model like gpt2 to predict next question. Dataset to finetune can be created using chatgpt.</li>
</ol>
<p>Is there any other methods/tools for this task (i couldn't find any)?</p>
","gpt-3"
"76980840","Error with Node.js API for GPT3.5 on AWS Lambda: ""Cannot read properties of undefined (reading 'completions')""","2023-08-25 22:29:45","76981323","1","913","<node.js><aws-lambda><openai-api><gpt-3>","<p>I'm attempted to allow myself to text GPT3.5 and cannot figure out why I keep getting carious errors for this line.</p>
<p>When I use openai.chat.completions.create, as the latest documentation suggests,I get the following error:</p>
<p><code>Cannot read properties of undefined (reading 'completions')</code></p>
<p>If I use the prior format of invoking completions with openai.createCompletion, I get the following error instead:</p>
<p><code>TypeError: openai.createChatCompletion is not a function</code></p>
<p>I've read every piece of documentation I can, and tried even older ways of invoking completions, but cannot get anything to work.</p>
<p>I've already tried getting a new API key and doing npm update, and still get the same issue. Here's my full code, I must be missing something:</p>
<pre><code>const openai = require('openai');

const accountSid = process.env.TWILIO_ACCOUNT_SID;
const authToken = process.env.TWILIO_AUTH_TOKEN;

const client = require('twilio')(accountSid, authToken);

openai.apiKey = process.env.OPENAI_AUTH,

exports.handler = async (event, context) =&gt; {
  try {
    const buff = Buffer.from(event.body, &quot;base64&quot;);
    const formEncodedParams = buff.toString(&quot;utf-8&quot;);
    const urlSearchParams = new URLSearchParams(formEncodedParams);

    const body = urlSearchParams.get(&quot;Body&quot;);
    const from = urlSearchParams.get(&quot;From&quot;);

    const completion = await openai.chat.completions.create({
      engine: &quot;gpt-3.5-turbo&quot;, // GPT-3.5 Turbo
      messages: [
        { role: &quot;system&quot;, content: body },
      ],
      max_tokens: 100, // You can adjust this as needed
    });

    await sendMessageBack(completion.data.choices[0].message.content, from);
    return {
      statusCode: 200,
      body: JSON.stringify('Message sent successfully'),
    };
  } catch (error) {
    console.error(error);
    return {
      statusCode: 500,
      body: JSON.stringify('Error sending message'),
    };
  }
};

async function sendMessageBack(msg, to) {
  try {
    await client.messages.create({
      body: msg,
      to: to,
      from: process.env.TWILIO_PHONE_NUM,
    });
    console.log('Message sent:', msg);
  } catch (e) {
    console.error('Error sending message:', e);
  }
}
</code></pre>
<p>I have tried updating the openai package, new invocations of completions, old invocations, reading the documentation, but AWS  Lambda keeps throwing errors on that same line.</p>
<p>EDIT: The solutions below led to the following rewrite. Including it here in case it's helpful to anyone else:</p>
<pre><code>import OpenAI from 'openai';

const openai = new OpenAI({
  apiKey: process.env.OPENAI_AUTH, 
});

import twilio from 'twilio';

const accountSid = process.env.TWILIO_ACCOUNT_SID;
const authToken = process.env.TWILIO_AUTH_TOKEN;

const client = twilio(accountSid, authToken);

export const handler = async (event, context) =&gt; {
  try {
    const buff = Buffer.from(event.body, &quot;base64&quot;);
    const formEncodedParams = buff.toString(&quot;utf-8&quot;);
    const urlSearchParams = new URLSearchParams(formEncodedParams);

    const msgBody = urlSearchParams.get(&quot;Body&quot;);
    const from = urlSearchParams.get(&quot;From&quot;);

    const chatCompletion = await openai.chat.completions.create({
      model: &quot;gpt-3.5-turbo&quot;,
      messages: [{ role: &quot;system&quot;, content: msgBody }],
      max_tokens: 100,
    });

    await sendMessageBack(chatCompletion.choices[0].message.content, from);
    return {
      statusCode: 200,
      body: JSON.stringify('Message sent successfully'),
    };
  } catch (error) {
    console.error(error);
    return {
      statusCode: 500,
      body: JSON.stringify('Error sending message'),
    };
  }
};

async function sendMessageBack(msg, to) {
  try {
    await client.messages.create({
      body: msg,
      to: to,
      from: process.env.TWILIO_PHONE_NUM,
    });
    console.log('Message sent:', msg);
  } catch (e) {
    console.error('Error sending message:', e);
  }
}
</code></pre>
","gpt-3"
"76958232","Async function calling gpt","2023-08-23 04:12:33","","1","516","<python><openai-api><gpt-3>","<pre><code>`import json
import requests
import openai

OPENAI_KEY = &quot;XXX&quot;
openai.api_key = 'XXX'


GPT_MODEL = &quot;gpt-3.5-turbo-0613&quot;


def chat_completion_request(messages, functions=None, function_call=None, model=GPT_MODEL):
headers = {
        &quot;Content-Type&quot;: &quot;application/json&quot;,
        &quot;Authorization&quot;: &quot;Bearer &quot; + OPENAI_KEY,
    }
    json_data = {
        &quot;model&quot;: model,
        &quot;messages&quot;: messages
    }
    if functions is not None:
        json_data.update({&quot;functions&quot;: functions})
    if function_call is not None:
        json_data.update({&quot;function_call&quot;: function_call})
    try:
        response = requests.post(
            &quot;https://api.openai.com/v1/chat/completions&quot;,
            headers=headers,
            json=json_data,
        )
        return response
    except Exception as e:
        print(&quot;Unable to generate ChatCompletion response&quot;)
        print(f&quot;Exception: {e}&quot;)
        return e


life_url = 'XXX'

def get_movies():
    movies_array = []
    try:
        response = requests.get(life_url)
        data = response.json()
        for i in data['complex'][0]['movies']:
            movies_array.append(i['title'])
        return movies_array
    except:
        return &quot;I am unable to parse this, please try something new.&quot;
    


async def similaries(arguments):
    similar_movie = json.loads(arguments)['similar_movie']
    array_strings = get_movies()
    try:
        message = f&quot; En '{array_strings}' hay una pelicula que pueda parecerse a '{similar_movie}'?.Retorna un objeto JSON con las keys 'movie_name'&quot;
        res = await openai.ChatCompletion.create(
            model=&quot;gpt-3.5-turbo&quot;,
            messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: message}],
            temperature=0.0,
        )
        print(res)
        print(res['choices'][0]['message']['content'])
        return res['choices'][0]['message']['content']
    except:
        return &quot;cannot find the movie, please try something new.&quot;
    


def get_movie(arguments):
    try:
        movie_name = json.loads(arguments)['movie_name']
        response = requests.get(life_url)
        data = response.json()
        for i in data['complex'][0]['movies']:
            if i['title'] == movie_name:
                return i['title']
    except:
        return &quot;Sorry, I am unable to parse this, please try something new.&quot;


def get_director_by_movie(arguments):
    directors_array = []
    try:
        movie_name = json.loads(arguments)['movie_name']
        response = requests.get(life_url)
        data = response.json()
        for i in data['complex'][0]['movies']:
            if i['title'] == movie_name:
                for j in i['cast']:
                    if j['isDirector'] == True:
                        directors_array.append(j['name'])
        return directors_array
    except:
        return &quot;Could not find the movie, please try something new.&quot;


print(get_director_by_movie('LA NOCHE DEL CRIMEN - 2D Subtitulada'))

# --------------------------------------------------------------------------------------------

functions = [
    {
        &quot;name&quot;: &quot;similaries&quot;,
        &quot;description&quot;: &quot;It will get the similar movies.&quot;,
        &quot;parameters&quot;: {
            &quot;type&quot;: &quot;object&quot;,
            &quot;properties&quot;: {
                &quot;similar_movie&quot;: {
                    &quot;type&quot;: &quot;string&quot;,
                    &quot;description&quot;: &quot;This is similar name&quot;
                }
            },
            &quot;required&quot;: [&quot;similar_movie&quot;],
        },
    },
    {
        &quot;name&quot;: &quot;get_director_by_movie&quot;,
        &quot;description&quot;: &quot;It will get the director of the movie.&quot;,
        &quot;parameters&quot;: {
            &quot;type&quot;: &quot;object&quot;,
            &quot;properties&quot;: {
                &quot;movie_name&quot;: {
                    &quot;type&quot;: &quot;string&quot;,
                    &quot;description&quot;: &quot;This is the name of the movie.&quot;,
                }
            },
            &quot;required&quot;: [&quot;movie_name&quot;],
        },
    }]

user_input = input(
    &quot;Please enter your question here: (if you want to exit then write 'exit' or 'bye'.) &quot;)

while user_input.strip().lower() != &quot;exit&quot; and user_input.strip().lower() != &quot;bye&quot;:
    # prompt
    messages = [{&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.&quot;}]
    messages.append({&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: user_input})

    # calling chat_completion_request to call ChatGPT completion endpoint
    chat_response = chat_completion_request(
        messages, functions=functions
    )

    # fetch response of ChatGPT and call the function
    assistant_message = chat_response.json()[&quot;choices&quot;][0][&quot;message&quot;]

    if assistant_message['content']:
        print(&quot;Response is: &quot;, assistant_message['content'])
    else:
        fn_name = assistant_message[&quot;function_call&quot;][&quot;name&quot;]
        arguments = assistant_message[&quot;function_call&quot;][&quot;arguments&quot;]
        function = locals()[fn_name]
        result = function(arguments)
        print(&quot;Response is: &quot;, result)

user_input = input(&quot;Please enter your question here: &quot;)`
</code></pre>
<p>I am doing a function calling test, and in one of my functions I make a call to gpt, which is time consuming. The problem is that although function calling works fine, it returns a promise</p>
<p>Please enter your question here: (if you want to exit then write 'exit' or 'bye'.) como se llama la pelicula de barbie?
Response is:  &lt;coroutine object similaries at 0x7f96c92ad690&gt;</p>
","gpt-3"
"76934866","OpenAI API error: ""The requested module 'openai' does not provide an export named 'Configuration'""","2023-08-19 11:29:50","","0","1968","<reactjs><openai-api><chatgpt-api><gpt-3><gpt-4>","<p>I am facing this error while running my backend in my React app:</p>
<pre><code>file:///D:/Project/server/route/dalle.routes.js:3
import { Configuration, OpenAIApi} from 'openai';
         ^^^^^^^^^^^^^
SyntaxError: The requested module 'openai' does not provide an export named 'Configuration'
</code></pre>
<p>What's wrong?</p>
","gpt-3"
"76934155","How to improve API response time for AI-powered web application with OpenAI GPT Chat Completion?","2023-08-19 08:10:42","","0","315","<python><http><flask><openai-api><gpt-3>","<p><strong>Preface:</strong>
We have developed an AI-powered web application with the goal of processing scanned student certificates and grade transcripts. These documents are uploaded to a NoSQL database (specifically MongoDB). We use OCR (Optical Character Recognition) for data extraction, then employ OpenAI LLM Models to convert the extracted information to JSON. Additionally, we leverage AI for translating the JSON content into English.</p>
<p><strong>Problem:</strong>
During the use of OpenAI GPT Chat Completion, there are occasional delays in response time, sometimes exceeding 30 seconds. This occurs approximately 70% of the time during the translation phase of the JSON object and 30% during the conversion phase from a string to JSON.</p>
<p>We analyzed and adopted the [gpt-3.5-turbo-0301] model to mitigate this issue due to its faster response time while yielding the exact expected output.</p>
<p>Our Python Flask Application features 8 API endpoints:</p>
<ul>
<li>Initialization: &quot;/api/v1/initialize&quot; [0.5-3 seconds].</li>
<li>Text Extraction: &quot;/api/v1/extract-text&quot; [4-15 seconds].</li>
<li>Table Extraction: &quot;/api/v1/extract-tables&quot; [8-20 seconds].</li>
<li>Text Correction: &quot;/api/v1/correct-text&quot; [2-20 seconds].</li>
<li>Table Correction: &quot;/api/v1/correct-tables&quot; [13-30+ seconds].</li>
<li>Text Translation: &quot;/api/v1/translate-text&quot; [2-20 seconds].</li>
<li>Table Translation: &quot;/api/v1/translate-tables&quot; [13-30+ seconds].</li>
<li>Document Generation: &quot;/api/v1/generate-document&quot; [5-10 seconds].</li>
</ul>
<p>Currently, the issue is concentrated on two endpoints: &quot;/api/v1/correct-tables&quot; and &quot;/api/v1/translate-tables&quot;.</p>
<p>As our client side is developed with Next.js, communication with the server is solely through HTTP. It's important to note that we are constrained by Heroku's requested timeout limit of 30 seconds.</p>
<p><strong>Proposed solutions (yet not confirmed for implementation):</strong></p>
<ol>
<li>Enhance the client-side logic to handle process failures without triggering timeout errors. Failed processes would initiate a 5-second waiting period followed by automatic reattempting until success.</li>
<li>Explore the possibility of switching to a different GPT provider due to perceived intentional slowdowns in OpenAI's API responses. However, this alternative may not guarantee a solution and require further evaluation.</li>
</ol>
<p>Despite attempting multiple solutions, the core issue remains the duration of API responses. We seek insights from readers to identify more effective workarounds for this challenge.</p>
","gpt-3"
"76926552","PHP - Having a long conversation with GPT 3.5 API","2023-08-18 05:03:25","","1","170","<php><artificial-intelligence><openai-api><chatgpt-api><gpt-3>","<p>I am using GPT 3.5 <a href=""https://platform.openai.com/docs/api-reference/chat"" rel=""nofollow noreferrer"">Chat Completion API</a> to write a 2000 words article, but it produces only about 1000 words response regardless of the topic. So I am trying to get it to write one half of the article in 1000 words at a time, but it only responds with the second half:-</p>
<pre><code>// Request headers
$headers = [
    'Content-Type: application/json',
    'Authorization: Bearer ' . $apiKey,
];

// API endpoint URL
$url = 'https://api.openai.com/v1/chat/completions';

// Request payload
$data = [
    'model' =&gt; 'gpt-3.5-turbo-16k',
    'messages' =&gt; [
        [
        'role' =&gt; 'system',
        'content' =&gt; &quot;write first half of a blog post for the keyword provided by the user in 1000 words&quot;
        ],
        [
        'role' =&gt; 'user',
        'content' =&gt; &quot;drones&quot;
        ],
        [
        'role' =&gt; 'system',
        'content' =&gt; &quot;write second half of the same blog post for the keyword provided by the user in another 1000 words&quot;
        ],
        [
        'role' =&gt; 'user',
        'content' =&gt; &quot;drones&quot;
        ],
    ]
];

// Send the API request
$ch = curl_init();
curl_setopt($ch, CURLOPT_URL, $url);
curl_setopt($ch, CURLOPT_POST, true);
curl_setopt($ch, CURLOPT_POSTFIELDS, json_encode($data));
curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);
curl_setopt($ch, CURLOPT_HTTPHEADER, $headers);
$response = curl_exec($ch);
curl_close($ch);

// Process the API response
if ($response === false) {
    // Request failed
    echo 'Error: ' . curl_error($ch);
} else {
    // Request succeeded
    $responseData = json_decode($response, true);
    // Process the response data
    echo &quot;&lt;pre&gt;&quot;;
    print_r($responseData);
    echo &quot;&lt;/pre&gt;&quot;;
}
</code></pre>
","gpt-3"
"76876772","How to limit OpenAI in langchain to answer questions only from cutom knowledge base provided by me","2023-08-10 14:36:23","","1","439","<chatbot><openai-api><langchain><gpt-3>","<p>I am currently working on a chatbot with langchain. I built a chain which can answer questions based on a custom knowledge base (let's say a sitemap) provided by me. But when I asked a quetion (&quot;Who was the sixteenth president of US.&quot;) out of the custom knowledge base, it answered the question based on its own general knowledge.</p>
<p>I do not want it to answer the question which are out of the custom knowledge base. Added image of the Jupiter notebook for refrence of my code.</p>
","gpt-3"
"76865340","How to make OpenAI stop prepending ""A:"" or ""Answer:"" to its answers?","2023-08-09 07:03:10","","2","348","<javascript><node.js><chatbot><openai-api><gpt-3>","<p>Sometimes, my OpenAI API on call like</p>
<pre><code> const completion = await openai.createChatCompletion({
  model: 'gpt-3.5-turbo',
  messages: [
    {
      role: 'system',
      content:  `You are ChatbotAssistant, an automated service to answer questions of a website visitors.` +
`You respond in a short, very conversational friendly style. If you can't find an answer, provide no answer and apologize.`
    },
    {role: 'user', content: userQuestion}
  ]
})
 const responseText = completion.data.choices[0].message.content
</code></pre>
<p>gives answer with &quot;A:&quot; or &quot;Answer:&quot; prepended. Since I don't need that, I tried to instruct it explicitly not to do it, by changing system message like:</p>
<pre><code>`You are ChatbotAssistant, an automated service to answer questions of a website visitors.` +
`Do not prepend &quot;A:&quot; or &quot;Answer:&quot; to your answers` +
`You respond in a short, very conversational friendly style. If you can't find an 
answer, provide no answer and apologize.`
</code></pre>
<p>but to no effect.</p>
<p>I know I can handle this in Javascript like e.g.</p>
<pre><code>let cleanedText = responseText

if (responseText.startsWith('A:') || responseText.startsWith('Answer:')) {
  cleanedText = responseText.replace(/^(A:|Answer:)\s*/, '')
}
</code></pre>
<p>but is there OpenAI solution to this? Thanks.</p>
","gpt-3"
"76863183","ConnectionError when using the OpenAI API via spacy-llm","2023-08-08 21:05:35","","0","543","<python><nlp><spacy><openai-api><gpt-3>","<p>I am trying to use the <code>spacy-llm</code> package to experiment with various NLP tasks, and am keen to see what results I can achieve via the OpenAI API.</p>
<p>However, when I try to run the toy example provided by <code>spacy-llm</code> (Example 1 at <a href=""https://spacy.io/usage/large-language-models#-api"" rel=""nofollow noreferrer"">https://spacy.io/usage/large-language-models#-api</a>), I get a <code>ConnectionError</code> with error code 429:</p>
<pre><code>&gt;&gt;&gt; import spacy_llm
&gt;&gt;&gt; from spacy_llm.util import assemble
&gt;&gt;&gt; nlp = assemble(&quot;config.cfg&quot;)
&gt;&gt;&gt; doc = nlp(&quot;You look gorgeous!&quot;)

Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;/home/admin/platform-api/venv/lib/python3.11/site-packages/spacy/language.py&quot;, line 1047, in __call__
    error_handler(name, proc, [doc], e)
  File &quot;/home/admin/platform-api/venv/lib/python3.11/site-packages/spacy/util.py&quot;, line 1724, in raise_error
    raise e
  File &quot;/home/admin/platform-api/venv/lib/python3.11/site-packages/spacy/language.py&quot;, line 1042, in __call__
    doc = proc(doc, **component_cfg.get(name, {}))  # type: ignore[call-arg]
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/home/admin/platform-api/venv/lib/python3.11/site-packages/spacy_llm/pipeline/llm.py&quot;, line 145, in __call__
    docs = self._process_docs([doc])
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/home/admin/platform-api/venv/lib/python3.11/site-packages/spacy_llm/pipeline/llm.py&quot;, line 199, in _process_docs
    responses_iters = tee(self._model(prompts_iters[0]), n_iters)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/home/admin/platform-api/venv/lib/python3.11/site-packages/spacy_llm/models/rest/openai/model.py&quot;, line 115, in __call__
    responses = _request(
                ^^^^^^^^^
  File &quot;/home/admin/platform-api/venv/lib/python3.11/site-packages/spacy_llm/models/rest/openai/model.py&quot;, line 86, in _request
    r = self.retry(
        ^^^^^^^^^^^
  File &quot;/home/admin/platform-api/venv/lib/python3.11/site-packages/spacy_llm/models/rest/base.py&quot;, line 147, in retry
    raise ConnectionError(
ConnectionError: API could not be reached after 34.710 seconds in total and attempting to connect 5 times. Check your network connection and the API's availability.
429     Too Many Requests
</code></pre>
<p>It seems to think there have been &quot;Too Many Requests&quot; - perhaps requests are being repeated in the background? Is there a way to disable that?</p>
<p>Any help would be very much appreciated!</p>
<p>For reference, I have also tried an example provided on the OpenAI website to make sure that my keys are working correctly, as below. This worked exactly as expected.</p>
<pre><code>import os
import openai
openai.organization = &quot;org-xxx&quot;
openai.api_key = os.getenv(&quot;OPENAI_API_KEY&quot;)
openai.Model.list()
</code></pre>
","gpt-3"
"76860050","LangChain no Observation return","2023-08-08 13:30:10","","0","964","<artificial-intelligence><openai-api><langchain><gpt-3><py-langchain>","<p>I am trying to build a bot using gpt-3.5-turbo and the langchain tools. I am trying to create an own Tool/Agent for that.
The point of the Bot will be, that it can answer questions asked about specific topics, where information can only be found on Confluence (Atlassian Web-based Wiki). The bot is already recognizing, that my custom tool should be used and it's also handling the Action Input. However, it doesn't call the tool (which is functioning by itself).
The code (excluding keys and private data) is as following:</p>
<pre><code>import os
os.environ['OPENAI_API_KEY'] = PRIVATE

from langchain.chat_models import ChatOpenAI
from langchain.chains.conversation.memory import ConversationBufferMemory
from langchain.tools import StructuredTool
from langchain.agents import initialize_agent, AgentType, load_tools
from langchain.document_loaders import ConfluenceLoader
from langchain.indexes import VectorstoreIndexCreator

# importing llm
turbo_llm = ChatOpenAI(
    temperature = 0,
    model_name = 'gpt-3.5-turbo'
)

loader = ConfluenceLoader(url=PRIVATE, token=PRIVATE)
documents = loader.load(
    space_key=PRIVATE_SPACE_KEY, page_ids=PRIVATE_IDS, include_attachments=False, limit=1, max_pages=1, keep_markdown_format=True
index = VectorstoreIndexCreator().from_documents(documents)

# Confluence Search Tool
def confluence_search(searchParam: str=&quot;&quot;) -&gt; str:
    &quot;&quot;&quot;Get information about COMPANY NAME specifics using Confluence. Can help if the bot can't handle the information or the information can't be found online. The searchParam is the Action input.&quot;&quot;&quot;
    return index.query(searchParam)

confluence_search_tool = StructuredTool.from_function(confluence_search)

# Adding the Confluence Search tool to the llm
# tools =  [confluence_search_tool]
tools = load_tools([], llm=turbo_llm)
tools = tools + [confluence_search_tool]

# conversational agent memory
memory = ConversationBufferMemory(
    memory_key = 'chat_history',
    k = 5, # remembers 3 messages
    return_messages = True
)

PREFIX = '''You are a bot helping with specific questions from COMPANY workers. They often ask about things, where the only place you can find information is the Confluence. Thus, you must often use Confluence to look up stuff. If you can't find anything about the asked topic, tell the User you don't know an answer.'''

FORMAT_INSTRUCTIONS = &quot;&quot;&quot;To use a tool, please use the following format:
'''
Thought: Do I need to use a tool? Yes
Action: the action to take, should be one of [{tool_names}]
Action Input: the input to the action
Observation: the result of the action
'''

&quot;&quot;&quot;

SUFFIX = '''
Begin!

Instructions: {input}
{agent_scratchpad}
'''

# creation of agent
conversational_agent = initialize_agent(
    agent = AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION, # only works with STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION, however it doesn't remember previous messages
    tools = tools,
    llm = turbo_llm,
    verbose = True,
    return_intermediate_steps = True,
    agent_kwargs = {
        'prefix': PREFIX,
        'format_instructions': FORMAT_INSTRUCTIONS,
        'suffix': SUFFIX
        }
    #memory = memory
)


conversational_agent(&quot;What is the Countrynum of Germany?&quot;) # Testquestion not working
</code></pre>
<p>The output is as follows:</p>
<pre><code>[1m&gt; Entering new AgentExecutor chain...[0m
[32;1m[1;3mThought: Do I need to use a tool? Yes
Action: confluence_search
Action Input: QUESTION RELATED (private)
[0m

[1m&gt; Finished chain.[0m
</code></pre>
<p>As you can see, it tells its thought, action and action_input. However it doesn't tell me any observation.</p>
","gpt-3"
"76835392","openai API in R: Error in chat completion - how to locate the issue?","2023-08-04 11:15:13","76852302","0","179","<r><openai-api><chatgpt-api><gpt-3>","<p>I'm trying to classify a large number of newspaper articles with the OpenAI API's chat completion function in <strong>R</strong>. Usually, it works quite well but with a number of articles, I get an error that I don't understand.</p>
<p>This is the error I get:</p>
<blockquote>
<p>Error in is.na(output) || !is.character(output) :<br />
'length = 3' in coercion to 'logical(1)'</p>
</blockquote>
<p>One example is:</p>
<pre><code># libraries loaded at that moment
library(tidyverse)
library(httr)
library(tidytext)
library(purrr)

system_task &lt;- &quot;You will be provided one article per prompt by the Austrian Newspaper Der Standard. The article is written in German. Each prompt only contains one article. All of these mention the justice system (or actors in the justice system such as Staatsanwalt, WKSta, Richter or Gericht) or the Austrian Constitutional Court (called VfGH or Verfassungsgerichtshof). We want you to take multiple steps for each of these articles: 1) Summarize the article in 3 sentences.  Answer this question with the prefix '1)'. 2) Does the article mention that politicians critisize the judiciary or the VfGH? Answer this question with the prefix '2)'.3) Does the article mention that politicians attack the judiciary or the VfGH?  Answer this question with the prefix '3)' 4) If 2) or 3) is answered with yes: Who is criticized or attacked by whom, name the actors. Remember that we are not interested in conflict between politicians, only between politicians and the judiciary. Answer this question with the prefix '4)'&quot;

article &lt;- 'In der Adventzeit r√ºcken Armut, Flucht, Krankheit und Obdachlosigkeit vermehrt in den Fokus der √ñffentlichkeit. Nicht nur in der Weihnachtsgeschichte geht es um Armut, Flucht, Krankheit und Obdachlosigkeit. Im Advent r√ºcken diese Themen wiederholt in das Blickfeld der √ñffentlichkeit, denn f√ºr viele Familien ist Weihnachten aus finanziellen und gesundheitlichen Gr√ºnden keine besinnliche Zeit. Rund 140.000 Menschen in √ñsterreich k√∂nnten ihre Wohnung nicht ausreichend heizen. Das sagte Caritas-Pr√§sident Michael Landau am Sonntagvormittag in der ORF-&quot;Pressestunde&quot;. Das Ziel der neuen Regierung m√ºsse sein, Kinderarmut und Altersarmut zu senken. Landau hatte der t√ºrkis-blauen Regierung w√§hrend der vergangenen Legislaturperiode eine &quot;Demontage&quot; des Sozialstaats vorgeworfen. Im ORF-Studio betonte er wiederholt seine &quot;Erleichterung&quot; √ºber den Entscheid des Verfassungsgerichtshofs, die Kernpunkte der neuen Sozialhilfe von T√ºrkis-Blau zu kippen. Die H√∂chstrichter hatten am Dienstag sowohl die starken K√ºrzungen f√ºr kinderreiche Familien als auch f√ºr Menschen mit schlechten Deutsch- oder Englischkenntnissen gekippt. &quot;Die Mindestsicherung muss Armut vermeiden&quot;, sagte Landau. Eine K√ºrzung der Familienzuschl√§ge auf 44 Euro ab dem dritten Kind ‚Äì wie in der neuen Sozialhilfe von T√ºrkis-Blau vorgesehen ‚Äì &quot;entspricht nicht der Lebenswirklichkeit von Menschen&quot;. Alleinerzieherinnen und kinderreiche, einkommensschwache Familien seien sowieso schon &quot;in besonderer Weise armutsgef√§hrdet&quot;. Die VfGH-Entscheidung sei eine Chance, Landau pl√§diert f√ºr eine Neuregelung unter Einbindung der Praktiker und Hilfsorganisationen. Auch im Pflegebereich brauche man dringend neue L√∂sungen. Die vorige Bundesregierung habe v√∂llig zu Recht erkannt, dass dieses Thema viele Menschen ber√ºhre. √ñsterreich ist pflegebed√ºrftig, so Landau ‚Äì er hoffe auf die neue Regierung, egal wie sie aussehe. Es brauche einen vergleichbaren Qualit√§ts-, Versorgungs- und Finanzierungsrahmen. Der Zugang zur Pflege m√ºsse f√ºr alle fl√§chendeckend leistbar sein und es brauche auch Anstrengungen, damit sich Menschen f√ºr den Pflegeberuf entscheiden. Die Zahl der pflegebed√ºrftigen Menschen in √ñsterreich ist gestiegen, damit haben sich laut Statistik Austria auch die Kosten f√ºr die Betreuung kr√§ftig erh√∂ht. Die Pflegekosten des Staats sind seit 2013 um ein Drittel angestiegen, zeigen aktuelle Zahlen. 13.000 Menschen mehr als im Vorjahr waren in Heimen und Pflegeh√§usern untergebracht. Bei den Hausbesuchen durch Pflegefachkr√§fte war der Anstieg geringer. Neben dem Plus der √§lteren, pflegebed√ºrftigen Menschen habe sich aber auch die Abschaffung des Pflegeregress ausgewirkt. Sozialministerin Brigitte Zarfl hatte im November zwei Studien zur Zukunft der Pflege in √ñsterreich pr√§sentiert. Die Zahlen deuteten auf einen bevorstehenden Personalmangel hin. (red, 22.12.2019)'

expr &lt;- POST(
      # API Link
      url = &quot;https://api.openai.com/v1/chat/completions&quot;,
      # Authorizatiob
      add_headers(Authorization = paste(&quot;Bearer&quot;, chatGPT_API)),
      # output in json
      content_type_json(),
      # encode the value to json format
      encode = &quot;json&quot;,
      # low randomness of answers
      temperature = 0,
      stop = none,
      # Controlling what to show as the output
      body = list(
        model = &quot;gpt-3.5-turbo&quot;,
        messages = list(list(role = &quot;user&quot;, content = article),
                        list(role = &quot;system&quot;, content = system_task))))
</code></pre>
<p>Does anyone have the idea where the issue might lie and how to fix it?</p>
<p>I tried wrapping it in a <code>tryCatch</code> but that didn't work either:</p>
<pre><code>chatGPT_response &lt;- tryCatch(
    
    # Try to retrieve answer for OpenAI API
    expr = {
      POST(
      # API Link
      url = &quot;https://api.openai.com/v1/chat/completions&quot;,
      # Authorizatiob
      add_headers(Authorization = paste(&quot;Bearer&quot;, chatGPT_API)),
      # output in json
      content_type_json(),
      # encode the value to json format
      encode = &quot;json&quot;,
      # low randomness of answers
      temperature = 0,
      stop = none,
      # Controlling what to show as the output
      body = list(
        model = &quot;gpt-3.5-turbo&quot;,
         messages = list(list(role = &quot;user&quot;, content = article),
                        list(role = &quot;system&quot;, content = system_task)
                        )))
      },
    # In case an error occurs while using API
    error = function(e){
      message(paste(&quot;For article there was an error.&quot;))
      return(NA)
    },
    # Notify about a possible warning while using API
    warning = function(w){
      message(paste(&quot;For article there was a warning.&quot;))
    }
  )
</code></pre>
","gpt-3"
"76786273","Azure OpenAI latency spikes (3-20 minutes!)","2023-07-28 08:58:38","","6","760","<azure><azure-cognitive-services><openai-api><gpt-3>","<p>We're prototyping a chatbot application using Azure OpenAI gpt-3.5-turbo model using the standard tier.</p>
<p>We're facing random latency bursts, which sometimes go between 3-20 minutes. Below I have screenshots with the metrics provided by the portal. As you can see the usage/rate limit don't indicate high load.</p>
<p>In fact, the application isn't yet deployed on production, it's only being used for testing purposes by our development team. Understanding these latency spikes will help us prove our PoC and use Azure OpenAI service on production.</p>
<p>Any ideas on how to solve this issue?</p>
<p><strong>Model Properties</strong>
Model name: gpt-35-turbo
Model version: 0301
Deployment type: Standard
Content Filter: Default
Tokens per Minute Rate Limit (thousands): 120
Rate limit (Tokens per minute): 120000
Rate limit (Requests per minute): 720</p>
<p><strong>Diagrams</strong></p>
<p>Latency:
<a href=""https://i.sstatic.net/yVB8t.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/yVB8t.png"" alt=""enter image description here"" /></a></p>
<p>Rate limit:
<a href=""https://i.sstatic.net/lF2aA.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/lF2aA.png"" alt=""enter image description here"" /></a></p>
<p>Processed prompt tokens:
<a href=""https://i.sstatic.net/TUyga.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/TUyga.png"" alt=""enter image description here"" /></a></p>
","gpt-3"
"76757194","Why do I get the error ""Unrecognized request argument supplied: functions"" when using `functions` when calling Azure OpenAI GPT?","2023-07-24 18:35:33","76757275","9","8659","<python><azure><gpt-3><azure-openai><large-language-model>","<p>I'm trying to use <code>functions</code> when calling Azure OpenAI GPT, as documented in <a href=""https://platform.openai.com/docs/api-reference/chat/create#chat/create-functions"" rel=""noreferrer"">https://platform.openai.com/docs/api-reference/chat/create#chat/create-functions</a></p>
<p>I use:</p>
<pre><code>import openai
openai.api_type = &quot;azure&quot;
openai.api_base = &quot;https://XXXXXXXX.openai.azure.com/&quot;
openai.api_version = &quot;2023-06-01-preview&quot;
openai.api_key = os.getenv(&quot;OPENAI_API_KEY&quot;)
response = openai.ChatCompletion.create(
            engine=&quot;gpt-35-turbo-XXX&quot;,
            model=&quot;gpt-35-turbo-0613-XXXX&quot;
            messages=messages,
            functions=functions,
            function_call=&quot;auto&quot;,
        )
</code></pre>
<p>but I get the error:</p>
<pre><code>openai.error.InvalidRequestError:
Unrecognized request argument supplied: functions
</code></pre>
<p>Why?</p>
<hr />
<p>Data to run the example code above (<code>messages</code> and <code>functions</code> need to be defined):</p>
<pre><code>messages = [{&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;}, {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hello!&quot;}]
functions = [
    {
        &quot;name&quot;: &quot;fetch_pages&quot;,
        &quot;description&quot;: &quot;Fetch the content of specified pages from the document.&quot;,
        &quot;parameters&quot;: {
            &quot;type&quot;: &quot;object&quot;,
            &quot;properties&quot;: {
                &quot;pages&quot;: {
                    &quot;type&quot;: &quot;array&quot;,
                    &quot;items&quot;: {
                        &quot;type&quot;: &quot;number&quot;
                    },
                    &quot;description&quot;: &quot;The list of pages to fetch.&quot;
                }
            },
            &quot;required&quot;: [&quot;pages&quot;]
        }
    },
    {
        &quot;name&quot;: &quot;fetch_section&quot;,
        &quot;description&quot;: &quot;Fetch the content of a specified section.&quot;,
        &quot;parameters&quot;: {
            &quot;type&quot;: &quot;object&quot;,
            &quot;properties&quot;: {
                &quot;section_title&quot;: {
                    &quot;type&quot;: &quot;string&quot;,
                    &quot;description&quot;: &quot;The title of the section to fetch.&quot;
                }
            },
            &quot;required&quot;: [&quot;section_title&quot;]
        }
    },
    {
        &quot;name&quot;: &quot;search&quot;,
        &quot;description&quot;: &quot;Search the document for a string query.&quot;,
        &quot;parameters&quot;: {
            &quot;type&quot;: &quot;object&quot;,
            &quot;properties&quot;: {
                &quot;query&quot;: {
                    &quot;type&quot;: &quot;string&quot;,
                    &quot;description&quot;: &quot;The search term.&quot;
                }
            },
            &quot;required&quot;: [&quot;query&quot;]
        }
    }
]
</code></pre>
","gpt-3"
"76741896","What part of OpenAI API request payload is limited by the max amount tokens?","2023-07-22 00:38:00","","1","837","<openai-api><chatgpt-api><gpt-3><gpt-4>","<p>I kinda understand how to count tokens out of characters, but what do I actually have to count?
If I have a payload like this:</p>
<pre class=""lang-json prettyprint-override""><code>{
  &quot;model&quot;: &quot;gpt-3.5-turbo&quot;,
  &quot;temperature&quot;: 1,
  &quot;max_tokens&quot;: 400,
  &quot;presence_penalty&quot;: 0.85,
  &quot;frequency_penalty&quot;: 0.85,
  &quot;messages&quot;: [
    {
      &quot;role&quot;: &quot;system&quot;,
      &quot;content&quot;: &quot;prompt&quot;
    },
    {
      &quot;role&quot;: &quot;assistant&quot;,
      &quot;content&quot;: &quot;message&quot;
    },
    // tens of messages
  ]
}
</code></pre>
<p>Do I have to count tokens out of it <strong>entirely</strong>? Or do I have to count it in <code>&quot;messages&quot;</code> only? If so, do I have to count all the json syntax characters, like spacebars, brackets and commas too? What about <code>&quot;role&quot;</code> and <code>&quot;content&quot;</code> keys? What about <code>&quot;role&quot;</code> value?<br>
Or I have to simply concat all the <code>&quot;content&quot;</code> values into a single string and count tokens based only on it? (this is what I <em>would like</em> to get as an answer, hehe)</p>
","gpt-3"
"76698818","Routing OpenAI API requests from an Android app thorough a backend server","2023-07-16 14:37:27","","1","471","<android><kotlin><backend><openai-api><gpt-3>","<p>I'm just starting to learn about mobile development, backend setup etc. so apologies if any of this is very basic but I‚Äôm not sure how to figure this out myself.</p>
<p>I‚Äôm trying to create an Android app in Kotlin (with Android Studio Flamingo) that uses one of OpenAI‚Äôs language models. OpenAI‚Äôs <a href=""https://platform.openai.com/docs/api-reference/authentication"" rel=""nofollow noreferrer"">documentation</a>, obviously, says that for production apps, all API calls should be routed through a backend server (where, I guess, the key can be set as an environment variable) to avoid exposing the API  key in the app‚Äôs code. I‚Äôm planning to use one of the pre-build backend services such AWS, Back4App or Firebase to store the necessary data for my app. However I‚Äôm not sure how to use those, if it is even possible, to route API requests from the app to OpenAI - that is what scripts would I have to run on the server, how to modify the code making the request in the app etc.</p>
<p>I was planning to use the open source OpenAI Kotlin library from here for the code that would run inside the app: <a href=""https://github.com/Aallam/openai-kotlin"" rel=""nofollow noreferrer"">https://github.com/Aallam/openai-kotlin</a></p>
<p>I realise that this could involve a lot of work so I was wondering if someone could at least point me towards some resources where I could learn more about this.</p>
","gpt-3"
"76696640","Interact oobabooga webui running in Collab with my local pc","2023-07-16 04:11:19","","0","1886","<artificial-intelligence><chatbot><gpt-3><large-language-model>","<p>How to interact with oogabooga webui with my python terminal? I am running wizard models or the Pygmalion model. Have anyone tried this before if yes can u provide the code?</p>
<p>I have tried to do it with the api examples gives in his GitHub but I can't....(<a href=""https://github.com/oobabooga/text-generation-webui/tree/main/api-examples"" rel=""nofollow noreferrer"">https://github.com/oobabooga/text-generation-webui/tree/main/api-examples</a>)</p>
","gpt-3"
"76687628","Cant find GPT3.5-Turbo-16k on OpenAI Azure","2023-07-14 12:20:29","","2","1167","<azure><openai-api><gpt-3><azure-openai>","<p>I'm trying to deploy a GPT3.5-Turbo-16k model in Azure (in theory, already available, as announced by Microsoft), but i can't find it in the options, as shown in the image.</p>
<p>Deploying a model in Azure:</p>
<p><a href=""https://i.sstatic.net/LLYF9.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/LLYF9.png"" alt=""enter image description here"" /></a></p>
<p>Im using East US with Standard pricing tier.
I have tried to deploy the model using Azure OpenAI Studio.
I managed to deploy the GPT3.5-Turbo model, but not the 16k version.</p>
","gpt-3"
"76661527","OpenAI Function Calling Error ---- openai.error.InvalidRequestError: <exception str() failed>","2023-07-11 11:30:41","76865710","2","691","<artificial-intelligence><chatbot><openai-api><chatgpt-api><gpt-3>","<p>I am creating a chatbot which can query all 'Views' in my database based on user query.
I tried many other methods but didn't succeed so now I thought I should try OpenAI's function calling.</p>
<p>What I did:
I created a function for one of the view. In that, I am calling GPT3 to create a SQL query based on the user question that I provide in the parameter. I have given instructions and schema to the model so it can create correct query. Below is the function.</p>
<pre class=""lang-py prettyprint-override""><code>def get_rent_details(user_query):
    &quot;&quot;&quot;Get the current weather in a given location&quot;&quot;&quot;
    response = openai.ChatCompletion.create(
        model=&quot;gpt-3.5-turbo-0613&quot;,
        prompt=&quot;&quot;&quot;User will ask you the question regarding their properties, assets and finance. 

        Follow below steps to get correct answer:

        1. Understand the user question and prepare a syntactically correct SQL query to retrieve the correct data.
        2. If you don't find the data in the table, just type &quot;No answer found&quot;.
        3. Do not make up any answer by your own.
        4. Instead of '=', always use 'LIKE' statement with 'WHERE' statement.
        5. The user will mention either property name or tenant name. So to make sure the query is correct, use both columns 'TenantName' and 'PropertyName' with 'WHERE' statement. For example: SELECT PropertyCode FROM viewRentRoll WHERE PropertyName LIKE 'Younger, 3003' OR TenantName LIKE 'Younger, 3003'.
        6. DO NOT create any DML query like UPDATE, INSERT, DELETE, ADD.
        7. Below is the table schema to run query on:

        CREATE TABLE [dbo].[viewRentRoll] (            
       [PropertyPKId] [bigint]
      ,[PropertyCode] [nvarchar]
      ,[PropertyName] [nvarchar]
      ,[PropertyList] [nvarchar]
      ,[LeaseCode] [nvarchar]
      ,[TenantName] [nvarchar]
      ,[SnP Rating] [nvarchar]
      ,[Unit Number] [nvarchar]
      ,[Lease Status] [nvarchar]
      ,[Lease Start Date] [datetime]
      ,[Lease Expiration Date] [datetime]
      ,[Unit Square Feet] [bigint]
      ,[Remaining Lease Term] [bigint]
      ,[Currently Monthly Base Rent] [bigint]
      ,[Rent PSF] [bigint]
      ,[ABR] [bigint]
      ,[local tenant] [nvarchar]
      ,[Current Annualized Base Rent PSF] [bigint]
      ,[CreatedLeaseExpirationDate] [datetime]
      ,[TenantCategory] [nvarchar]
  )
    &quot;&quot;&quot; + user_query,
        max_tokens=200,
        temperature=0,
    )
    return (response['choices'][0]['text'])
</code></pre>
<p>I am thinking to create such functions for each view.
After this I got the code from OpenAI Function Calling documentation and modified it as per my need. Below is the 'function calling' function:</p>
<pre class=""lang-py prettyprint-override""><code>def run_conversation(user_query):
    # Step 1: send the conversation and available functions to GPT

    print(&quot;Running run_conversion............\n\n&quot;)

    messages = [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: user_query}]
    functions = [
        {
            &quot;name&quot;: &quot;get_rent_details&quot;,
            &quot;description&quot;: &quot;Get the details of rent of tenants or properties&quot;,
            &quot;parameters&quot;: {
                &quot;type&quot;: &quot;object&quot;,
                &quot;user_query&quot; : {
                    &quot;type&quot; : &quot;string&quot;,
                    &quot;description&quot; : &quot;User's question regarding the rent of Tenant or properties&quot;
                }
            }
        }
    ]
    response = openai.ChatCompletion.create(
        model=&quot;gpt-3.5-turbo-0613&quot;,
        messages=messages,
        functions=functions,
        function_call=&quot;auto&quot;,  # auto is default, but we'll be explicit
    )
    response_message = response[&quot;choices&quot;][0][&quot;message&quot;]

    # Step 2: check if GPT wanted to call a function
    if response_message.get(&quot;function_call&quot;):
        # Step 3: call the function
        # Note: the JSON response may not always be valid; be sure to handle errors
        available_functions = {
            &quot;get_rent_details&quot;: get_rent_details,
        }  # only one function in this example, but you can have multiple
        function_name = response_message[&quot;function_call&quot;][&quot;name&quot;]
        fuction_to_call = available_functions[function_name]
        function_args = json.loads(response_message[&quot;function_call&quot;][&quot;arguments&quot;])
        function_response = fuction_to_call(
            user_query=function_args.get(&quot;user_query&quot;),
        )

        # Step 4: send the info on the function call and function response to GPT
        messages.append(response_message)  # extend conversation with assistant's reply
        messages.append(
            {
                &quot;role&quot;: &quot;function&quot;,
                &quot;name&quot;: function_name,
                &quot;content&quot;: function_response,
            }
        )  # extend conversation with function response
        second_response = openai.ChatCompletion.create(
            model=&quot;gpt-3.5-turbo-0613&quot;,
            messages=messages,
        )  # get a new response from GPT where it can see the function response
        return second_response
</code></pre>
<p>This is the first time I am trying Function Calling so I am not hundred percent sure if this will work.
When I run this code, I am getting this error: <code>openai.error.InvalidRequestError: &lt;exception str() failed&gt;</code>
for <code>response = openai.ChatCompletion.create()</code> in <code>run_conversation(user_query)</code> function.</p>
<p>Can anyone please guide me where I am making mistakes?</p>
<p>I am providing whole code below:</p>
<pre class=""lang-py prettyprint-override""><code>import openai
import json
import os

user_query = &quot;What is the monthly rent of Good Neighbor Homes, Inc.&quot;

openai.api_key=os.environ['OPENAI_API_KEY']

def run_conversation(user_query):
    # Step 1: send the conversation and available functions to GPT

    print(&quot;Running run_conversion............\n\n&quot;)

    messages = [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: user_query}]
    functions = [
        {
            &quot;name&quot;: &quot;get_rent_details&quot;,
            &quot;description&quot;: &quot;Get the details of rent of tenants or properties&quot;,
            &quot;parameters&quot;: {
                &quot;type&quot;: &quot;object&quot;,
                &quot;user_query&quot; : {
                    &quot;type&quot; : &quot;string&quot;,
                    &quot;description&quot; : &quot;User's question regarding the rent of Tenant or properties&quot;
                }
            }
        }
    ]
    response = openai.ChatCompletion.create(
        model=&quot;gpt-3.5-turbo-0613&quot;,
        messages=messages,
        functions=functions,
        function_call=&quot;auto&quot;,  # auto is default, but we'll be explicit
    )
    response_message = response[&quot;choices&quot;][0][&quot;message&quot;]

    # Step 2: check if GPT wanted to call a function
    if response_message.get(&quot;function_call&quot;):
        # Step 3: call the function
        # Note: the JSON response may not always be valid; be sure to handle errors
        available_functions = {
            &quot;get_rent_details&quot;: get_rent_details,
        }
        function_name = response_message[&quot;function_call&quot;][&quot;name&quot;]
        fuction_to_call = available_functions[function_name]
        function_args = json.loads(response_message[&quot;function_call&quot;][&quot;arguments&quot;])
        function_response = fuction_to_call(
            user_query=function_args.get(&quot;user_query&quot;),
        )

        # Step 4: send the info on the function call and function response to GPT
        messages.append(response_message)  # extend conversation with assistant's reply
        messages.append(
            {
                &quot;role&quot;: &quot;function&quot;,
                &quot;name&quot;: function_name,
                &quot;content&quot;: function_response,
            }
        )  # extend conversation with function response
        second_response = openai.ChatCompletion.create(
            model=&quot;gpt-3.5-turbo-0613&quot;,
            messages=messages,
        )  # get a new response from GPT where it can see the function response
        return second_response


def get_rent_details(user_query):
    &quot;&quot;&quot;Get the current weather in a given location&quot;&quot;&quot;
    response = openai.ChatCompletion.create(
        model=&quot;gpt-3.5-turbo-0613&quot;,
        prompt=&quot;&quot;&quot;User will ask you the question regarding their properties, assets and finance. 

        Follow below steps to get correct answer:

        1. Understand the user question and prepare a syntactically correct SQL query to retrieve the correct data.
        2. If you don't find the data in the table, just type &quot;No answer found&quot;.
        3. Do not make up any answer by your own.
        4. Instead of '=', always use 'LIKE' statement with 'WHERE' statement.
        5. The user will mention either property name or tenant name. So to make sure the query is correct, use both columns 'TenantName' and 'PropertyName' with 'WHERE' statement. For example: SELECT PropertyCode FROM viewRentRoll WHERE PropertyName LIKE 'Younger, 3003' OR TenantName LIKE 'Younger, 3003'.
        6. DO NOT create any DML query like UPDATE, INSERT, DELETE, ADD.
        7. Below is the table schema to run query on:

        CREATE TABLE [dbo].[viewRentRoll] (            
       [PropertyPKId] [bigint]
      ,[PropertyCode] [nvarchar]
      ,[PropertyName] [nvarchar]
      ,[PropertyList] [nvarchar]
      ,[LeaseCode] [nvarchar]
      ,[TenantName] [nvarchar]
      ,[SnP Rating] [nvarchar]
      ,[Unit Number] [nvarchar]
      ,[Lease Status] [nvarchar]
      ,[Lease Start Date] [datetime]
      ,[Lease Expiration Date] [datetime]
      ,[Unit Square Feet] [bigint]
      ,[Remaining Lease Term] [bigint]
      ,[Currently Monthly Base Rent] [bigint]
      ,[Rent PSF] [bigint]
      ,[ABR] [bigint]
      ,[local tenant] [nvarchar]
      ,[Current Annualized Base Rent PSF] [bigint]
      ,[CreatedLeaseExpirationDate] [datetime]
      ,[TenantCategory] [nvarchar]
  )

&quot;&quot;&quot;+user_query+&quot;?&quot;,
        max_tokens=200,
        temperature=0,
    )
    print(response['choices'][0]['text'])
    return (response['choices'][0]['text'])

run_conversation(user_query)
</code></pre>
","gpt-3"
"76625732","OpenAI GPT-3 API error: Why does the Discord bot on NodeJS return a 404 error in connection with the OpenAI API?","2023-07-06 05:24:34","","0","213","<node.js><axios><discord.js><openai-api><gpt-3>","<p>I'm creating a bot for Discord that has integrated the OpenAI model, but it generates a <code>404</code> error when making requests.</p>
<p>My code is:</p>
<pre class=""lang-js prettyprint-override""><code>const { Client, GatewayIntentBits } = require('discord.js');
const axios = require('axios');

const client = new Client({
  intents: [GatewayIntentBits.Guilds, GatewayIntentBits.GuildMessages],
});

const BOT_TOKEN = 'token here';
const OPENAI_TOKEN = 'token here';
const BOT_APPLICATION_ID = 'token here';

client.once('ready', () =&gt; {
  console.log('Bot est√° online!');
  registerSlashCommand();
});

client.on('interactionCreate', async (interaction) =&gt; {
  if (!interaction.isCommand()) return;

  if (interaction.commandName === 'ajuda-ai') {
    await interaction.reply('Digite sua mensagem para o GPT-3.');

    const collector = interaction.channel.createMessageCollector({
      filter: (msg) =&gt; msg.author.id === interaction.user.id,
    });

    collector.on('collect', async (msg) =&gt; {
      if (msg.content.toLowerCase() === '/cancelar') {
        collector.stop();
        interaction.channel.send('A conversa foi cancelada.');
        return;
      }

      interaction.channel.send('Aguarde, estou consultando o GPT-3...');

      try {
        const response = await axios.post('https://api.openai.com/v1/engines/davinci-codex/completions', {
          prompt: msg.content,
          max_tokens: 100,
        }, {
          headers: {
            'Authorization': `Bearer ${OPENAI_TOKEN}`,
            'Content-Type': 'application/json',
          },
        });

        await interaction.followUp(`Resposta do GPT-3: ${response.data.choices[0].text.trim()}`);
      } catch (error) {
        console.error(error);
        await interaction.followUp('Desculpe, ocorreu um erro ao buscar a resposta.');
      }

      collector.stop();
    });
  }
});

async function registerSlashCommand() {
  try {
    const guild = client.guilds.cache.get('1116260407414374493');
    if (!guild) return console.error('N√£o foi poss√≠vel encontrar o servidor.');

    await guild.commands.create({
      name: 'ajuda-ai',
      description: 'Inicia uma conversa com o GPT-3.',
    });
    console.log('Comando slash registrado.');
  } catch (error) {
    console.error('Erro ao registrar o comando slash:', error);
  }
}

client.login(BOT_TOKEN);
</code></pre>
<p>Working with the OpenAI API is new to me. I'm still adapting. It changes all the time. I've tested some things, but without success.</p>
<p>I checked the tokens and everything before asking for help here to make sure I didn't forget anything.</p>
","gpt-3"
"76603485","OpenAI gpt-3.5-turbo: Request failed with status code 400","2023-07-03 09:30:56","76603853","2","670","<javascript><node.js><openai-api><chatgpt-api><gpt-3>","<p>does this method in node.js doesn't work anymore? Because back then it was working fine but now it doesn't work anymore and also this code is also based on their official docs which is this <a href=""https://platform.openai.com/docs/api-reference/completions/create"" rel=""nofollow noreferrer"">https://platform.openai.com/docs/api-reference/completions/create</a></p>
<p><strong>My server end code:</strong></p>
<pre class=""lang-js prettyprint-override""><code>    import { Configuration, OpenAIApi } from 'openai';
    //....
    const configuration = new Configuration({
      apiKey: API_KEY,
    });
    //....
    const openai = new OpenAIApi(configuration);
    //....
    const response = await openai.createChatCompletion({
      model: &quot;gpt-3.5-turbo&quot;,
      messages: [
        {
          role: &quot;system&quot;, 
          content: `You are a helpful assistant.` },
        ...prompt
      ],
      temperature: 0.2,
      max_tokens: 1500,
      top_p: 1,
      frequency_penalty: 0,
      presence_penalty: 0,
    });
    //....
    res.status(200).send({
      bot: response.data.choices[0].message.content
    });
    //....
</code></pre>
<p><strong>The data I am trying to send:</strong></p>
<pre class=""lang-json prettyprint-override""><code>{
  &quot;prompt&quot;: [
    {
      &quot;role&quot;: &quot;bot&quot;,
      &quot;content&quot;: &quot;Something went wrong.&quot;
    },
    {
      &quot;role&quot;: &quot;user&quot;,
      &quot;content&quot;: &quot;What is wrong?&quot;
    }
  ]
}
</code></pre>
<p><strong>i am getting this kind of error:</strong>
<a href=""https://i.sstatic.net/rdiqx.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/rdiqx.png"" alt=""enter image description here"" /></a>|
The output of the message prompt is in the terminal just incase you want to check if i am sending correct message prompt.</p>
<p>I also tried adding the org id and still didn't work and also tried updating it from v3.2.1 to v3.3.0 nothing works at all. I still have balance in the account.</p>
","gpt-3"
"76588858","Give GPT a material to base his answer on","2023-06-30 12:18:50","","-2","39","<openai-api><gpt-3>","<p>I have been tasked in integrating GPT as a consultant on a website. It is supposed to use a manual I have been provided with, but I don't know how to give the manual to it.</p>
<p>The only solution I have is to send the manual as a message with <code>{role: &quot;system&quot;, content: &quot;You answer according to the following manual: ~manual~&quot;}</code>. The problem here is the token limit. Is there a better way to give GPT a material it should base the answer on?</p>
","gpt-3"
"76575168","How to prevent hallucinations using OpenAI's gpt-3.5-turbo API?","2023-06-28 16:40:19","","-2","1501","<openai-api><chatgpt-api><gpt-3>","<p>I am attempting to make a personal recruiting assistant tool to help me automate the process of answering questions for job applications like &quot;Why do you want to work here?&quot; or &quot;Write a cover letter saying how your experience relates to this position.&quot;.</p>
<p>I am using my resume and a Q&amp;A section as the context for the API call and the question as the prompt. Over all, the system works well but it hallucinates my experience often. For example, it will state I have more years of experience than is actually outlined in my resume or it will say I am an expert in technologies that I have never used or listed in my context.</p>
<p>I have tried explicitly prompting the model through this code but it has not been successful:</p>
<pre><code>def prepare_context(self):
        context = (
            &quot;Your role in this instance is &quot;
            &quot;to act as a recruiting assistant and help answer interview questions. &quot;
            &quot;Your responses should be based on the information provided in the resume and &quot;
            &quot;previous Q&amp;As without hallucinating additional information or experience.\n\n&quot;
            &quot;My Resume:\n\n&quot;
            f&quot;{self.resume}\n\n&quot;
            &quot;My Previously answered questions:\n&quot;
        )

        for q, a in self.qa_list:
            context += f&quot;Q: {q}\nA: {a}\n&quot;

        context += (
            &quot;\nWhen answering the following questions, remember to answer &quot;
            &quot;in the first person as if you were the job applicant. The responses &quot;
            &quot;should be concise, truthful, and based solely on the given information. &quot;
            &quot;Do not create or infer any additional experiences or skills that are &quot;
            &quot;not explicitly mentioned in the resume or previous Q&amp;As. &quot;
            &quot;Do not generate any information that is not included in the resume or previous questions and answers. &quot;
            &quot;Remember, you are answering as me &quot;
            &quot;so answer all further questions from my perspective.\n&quot;
        )

        return context
</code></pre>
","gpt-3"
"76569677","My JavaScript chatgpt openai implementation keeps giving me an HTTP 429","2023-06-28 02:26:10","","1","565","<openai-api><chatgpt-api><gpt-3>","<p>Below is my code... and I am looking at the API usage on the openai console and I am way under the limit.  I haven't ever been able to get a sucessful response.  I am copying the code from their documentation.  I keep getting a HTTP 429</p>
<pre><code>const API_URL = &quot;&lt;https://api.openai.com/v1/chat/completions&gt;&quot;;
const API_KEY = &quot;YOUR_API_KEY&quot;;

const generate = async () =&gt; {

  try {
    // Fetch the response from the OpenAI API with the signal from AbortController
    const response = await fetch(API_URL, {
      method: &quot;POST&quot;,
      headers: {
        &quot;Content-Type&quot;: &quot;application/json&quot;,
        Authorization: `Bearer ${API_KEY}`,
      },
      body: JSON.stringify({
        model: &quot;gpt-3.5-turbo&quot;,
        messages: [{ role: &quot;user&quot;, content: promptInput.value }],
      }),
    });

    const data = await response.json();
    resultText.innerText = data.choices[0].message.content;
  } catch (error) {
    console.error(&quot;Error:&quot;, error);
    resultText.innerText = &quot;Error occurred while generating.&quot;;
  }
};
</code></pre>
","gpt-3"
"76563924","OpenAI GPT-3 API error: Why do I still get the ""You exceeded your current quota"" error even though I set up a paid account (subscribed)?","2023-06-27 10:26:00","","-1","4847","<openai-api><gpt-3>","<p>Why did I get <code>RateLimitError: You exceeded your current quota, please check your plan and billing details.</code>? I was subscribed, and this is my first request. I paid for my subscription.</p>
<p>Here is my code:</p>
<pre><code>import openai
KEY = &quot;mykeyhere it is checked over 10 times&quot; //i tries to create new keys, not working, all times RATELIMITERROR
openai.api_key = KEY
def generate_response(text):
    response = openai.Completion.create(
        prompt=text,
        engine='text-davinci-003',
        max_tokens=100,
        temperature=0.7,
        n=1,
        stop=None,
        timeout=15
    )
    if response and response.choices:
        return response.choices[0].text.strip()
    else:
        return None

res = generate_response(&quot;Hello, how are you?&quot;)

print(res)
</code></pre>
<p>What did I do wrong, and why did my first request give this error?</p>
","gpt-3"
"76527363","OpenAI GPT-3 API error: Why do I get an error when I try to implement an OpenAI API in my JavaScript code?","2023-06-21 21:46:58","","-3","786","<javascript><html><openai-api><gpt-3>","<p>I am trying to create a simple website using HTML and JavaScript using an OpenAI API where the user inputs a query and the model generates a response. Whenever I type something in the textbox and click a button to generate something, it will always show an error. Is this a problem with CORS restrictions, or is it a problem with my code?</p>
<p>Here is my JavaScript code:</p>
<pre><code>function generateOutput() {
    var userInput = document.getElementById(&quot;input&quot;).value;

    fetch(&quot;https://api.openai.com/v1/engines/davinci-codex/completions&quot;, {
        method:&quot;POST&quot;,
        headers: {
            &quot;Content-Type&quot;: &quot;application/json&quot;,
            Authorization: &quot;Bearer 'MY_API'&quot;
        },
        body: JSON.stringify({
            prompt: userInput,
            max_tokens:50,
            model: &quot;text-davinci-003&quot;
        }),
    })
        .then(response =&gt; {
            if (response.ok) {
                return response.json();
            }else{
                throw new Error(&quot;Error occurred while communicating with the API.&quot;)
            }
        })
        .then(data =&gt; {
            const output = data.choices[0].text.trim();
            document.getElementById(&quot;output&quot;).textContent = output;
        })  
        .catch(error =&gt; {
            document.getElementById(&quot;errorMessage&quot;).textContent = &quot;Error occurred while communicating with the API: &quot; + error.message;
        });
}
</code></pre>
<p>In this code, a user can type something, and then generate a response by clicking a button. If there is an error, it will display the text &quot;Error occurred while communicating with the API.&quot; I tried it without the error function, and it just displayed nothing while generating my request.</p>
","gpt-3"
"76502113","Error with Few-shot prompting using gpt 3.5","2023-06-18 18:58:45","76504545","0","565","<chatbot><openai-api><gpt-3><chatgpt-api>","<p>I am trying to train GPT 3.5 model with few-shot prompting using <em>messages</em> argument instead of <em>prompt</em> argument. It throws an error even though it's clearly mentioned in OpenAI documentation that we can train a model this way.</p>
<pre><code>import openai

conversation=[
        {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;},
        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Who won the world series in 2020?&quot;},
        {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;The Los Angeles Dodgers won the World Series in 2020.&quot;},
    ]

def askGPT(question):
    conversation.append({&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: question})
    openai.api_key = &quot;openai key&quot;
    response = openai.Completion.create(
        model = &quot;gpt-3.5-turbo&quot;,
        messages = conversation,
        temperature = 0.6
        #max_tokens = 150,
    )
    #conversation.append({&quot;role&quot;: &quot;assistant&quot;,&quot;content&quot;:response})
    #print(response)
    #print(response[&quot;choices&quot;][0][&quot;message&quot;][&quot;content&quot;])

    
    conversation.append({&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: response.choices[0].message.content})
    print(response.choices[0].message.content)

def main():
    while True:
        print('GPT: Ask me a question\n')
        myQn = input()
        askGPT(myQn)
        print('\n')


main()
</code></pre>
<p>Error:</p>
<blockquote>
<p>openai.error.InvalidRequestError: Unrecognized request argument supplied: messages</p>
</blockquote>
<p>I tried to give &quot;conversations&quot; to the model inside &quot;responses&quot; but it soesn't seem to work.</p>
","gpt-3"
"76482024","How to get more detailed results sources with Langchain","2023-06-15 11:31:53","76483595","6","8266","<python><openai-api><gpt-3><langchain><chatgpt-api>","<p>I am trying to put together a simple &quot;Q&amp;A with sources&quot; using Langchain and a specific URL as the source data. The URL consists of a single page with quite a lot of information on it.</p>
<p>The problem is that <code>RetrievalQAWithSourcesChain</code> is only giving me the entire URL back as the source of the results, which is not very useful in this case.</p>
<p>Is there a way to get more detailed source info?
Perhaps the heading of the specific section on the page?
A clickable URL to the correct section of the page would be even more helpful!</p>
<p>I am slightly unsure whether the generating of the <code>result source</code> is a function of the language model, URL loader or simply <code>RetrievalQAWithSourcesChain</code> alone.</p>
<p>I have tried using <code>UnstructuredURLLoader</code> and <code>SeleniumURLLoader</code> with the hope that perhaps more detailed reading and input of the data would help - sadly not.</p>
<p>Relevant code excerpt:</p>
<pre><code>llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo')
chain = RetrievalQAWithSourcesChain.from_llm(llm=llm, retriever=VectorStore.as_retriever())

result = chain({&quot;question&quot;: question})

print(result['answer'])
print(&quot;\n Sources : &quot;,result['sources'] )
</code></pre>
","gpt-3"
"76462707","Building a GPT-3 Enabled Research Assistant with LangChain & Pinecone","2023-06-13 07:46:46","","0","111","<csv><chatbot><gpt-3><langchain><pinecone>","<p>I would like a chatbot that can handle large CSV files and answer any questions about the data contained within them.</p>
<p>I have implemented Pinecone to store vector data and connected it with Langchain. However, the current setup is not providing accurate answers based on the given data. What could be the potential error in this configuration?</p>
","gpt-3"
"76451783","AuthenticationError: <empty message> in OpenAPI api","2023-06-11 17:55:46","","1","458","<python><openai-api><gpt-3>","<p>I have been trying to use langchain library's <code>ChatOpenAI</code>, I pip installed langchain, and imported <code>ChatOpenAI</code></p>
<p>I'm running my code on colab so I set my the openAI's api key as:</p>
<pre><code>%env OPENAI_API_KEY= my_api_key
</code></pre>
<p>now when I try to initialize <code>ChatOpenAI</code> as follows, it runs without error:</p>
<pre><code>chat = ChatOpenAI(temperature=0.0,openai_organization='Personal')
chat
</code></pre>
<p>I get the following as the result:</p>
<pre><code>ChatOpenAI(verbose=False, callbacks=None, callback_manager=None, client=&lt;class 'openai.api_resources.chat_completion.ChatCompletion'&gt;, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='my_api_key', openai_api_base='', openai_organization='Personal', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None)
</code></pre>
<p>But when I try to run it as :</p>
<pre><code>customer_response = chat(customer_messages)

</code></pre>
<p>It throws the following error:</p>
<pre><code>---------------------------------------------------------------------------
AuthenticationError                       Traceback (most recent call last)
&lt;ipython-input-64-a39359b08f39&gt; in &lt;cell line: 1&gt;()
----&gt; 1 customer_response = chat(customer_messages)

17 frames
/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py in _interpret_response_line(self, rbody, rcode, rheaders, stream)
    761         stream_error = stream and &quot;error&quot; in resp.data
    762         if stream_error or not 200 &lt;= rcode &lt; 300:
--&gt; 763             raise self.handle_error_response(
    764                 rbody, rcode, resp.data, rheaders, stream_error=stream_error
    765             )

AuthenticationError: &lt;empty message&gt;
</code></pre>
","gpt-3"
"76437658","How to handle token limit in ChatGPT3.5 Turbo when creating tables?","2023-06-09 06:13:47","","0","1349","<python-3.x><openai-api><gpt-3><azure-openai><large-language-model>","<p>End user can copy tables from a pdf like
<a href=""https://i.sstatic.net/RSojg.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/RSojg.png"" alt=""enter image description here"" /></a></p>
<p>, paste the text in openai playground</p>
<pre><code>bird_id bird_posts bird_likes
012 2 5
013 0 4
056 57 70
612 0 12
</code></pre>
<p>and will prompt the gpt with &quot;Create table with the given text&quot;
and gpt generates a table like below:
<a href=""https://i.sstatic.net/a4iAS.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/a4iAS.png"" alt=""enter image description here"" /></a></p>
<p>This works well as expected.
But when my input text is sizeable (say 1076 tokens), I face the following error:</p>
<pre><code>Token limit error: The input tokens exceeded the maximum allowed by the model. Please reduce the number of input tokens to continue. Refer to the token count in the 'Parameters' panel for more details.
</code></pre>
<p>I will use python for text preprocessing and will get the data from UI.
If my input is textual data (like passages), I can use the <a href=""https://python.langchain.com/en/latest/modules/chains/index_examples/summarize.html"" rel=""nofollow noreferrer"">approaches</a> suggested by Langchain.
But, I would not be able to use summarization iteratively with tabular text as I might loose rows/columns.</p>
<p>Any inputs how this can be handled?</p>
","gpt-3"
"76421921","Using GPT 4 or GPT 3.5 with SQL Database Agent throws OutputParserException: Could not parse LLM output:","2023-06-07 09:40:43","78176065","5","3551","<openai-api><gpt-3><langchain><gpt-4>","<p>I am using the <a href=""https://python.langchain.com/en/latest/modules/agents/toolkits/examples/sql_database.html"" rel=""nofollow noreferrer"">SQL Database Agent</a> to query a postgres database. I want to use gpt 4 or gpt 3.5 models in the OpenAI llm passed to the agent, but it says I must use ChatOpenAI. Using ChatOpenAI throws parsing errors.</p>
<p>The reason for wanting to switch models is reduced cost, better performance and most importantly - token limit. The max token size is 4k for 'text-davinci-003' and I need at least double that.</p>
<p>Here is my code</p>
<pre><code>from langchain.agents.agent_toolkits import SQLDatabaseToolkit
from langchain.sql_database import SQLDatabase
from langchain.agents import create_sql_agent
from langchain.llms import OpenAI
from langchain.chat_models import ChatOpenAI
import os

os.environ[&quot;OPENAI_API_KEY&quot;] = &quot;&quot;
db = SQLDatabase.from_uri(
    &quot;postgresql://&lt;my-db-uri&gt;&quot;,
    engine_args={
        &quot;connect_args&quot;: {&quot;sslmode&quot;: &quot;require&quot;},
    },
)

llm = ChatOpenAI(model_name=&quot;gpt-3.5-turbo&quot;)
toolkit = SQLDatabaseToolkit(db=db, llm=llm)

agent_executor = create_sql_agent(
    llm=llm,
    toolkit=toolkit,
    verbose=True,
)

agent_executor.run(&quot;list the tables in the db. Give the answer in a table json format.&quot;)
</code></pre>
<p>When I do, it throws an error in the chain midway saying</p>
<pre><code>&gt; Entering new AgentExecutor chain...
Traceback (most recent call last):
  File &quot;/home/ramlah/Documents/projects/langchain-test/sql.py&quot;, line 96, in &lt;module&gt;
    agent_executor.run(&quot;list the tables in the db. Give the answer in a table json format.&quot;)
  File &quot;/home/ramlah/Documents/projects/langchain/langchain/chains/base.py&quot;, line 236, in run
    return self(args[0], callbacks=callbacks)[self.output_keys[0]]
  File &quot;/home/ramlah/Documents/projects/langchain/langchain/chains/base.py&quot;, line 140, in __call__
    raise e
  File &quot;/home/ramlah/Documents/projects/langchain/langchain/chains/base.py&quot;, line 134, in __call__
    self._call(inputs, run_manager=run_manager)
  File &quot;/home/ramlah/Documents/projects/langchain/langchain/agents/agent.py&quot;, line 953, in _call
    next_step_output = self._take_next_step(
  File &quot;/home/ramlah/Documents/projects/langchain/langchain/agents/agent.py&quot;, line 773, in _take_next_step
    raise e
  File &quot;/home/ramlah/Documents/projects/langchain/langchain/agents/agent.py&quot;, line 762, in _take_next_step
    output = self.agent.plan(
  File &quot;/home/ramlah/Documents/projects/langchain/langchain/agents/agent.py&quot;, line 444, in plan
    return self.output_parser.parse(full_output)
  File &quot;/home/ramlah/Documents/projects/langchain/langchain/agents/mrkl/output_parser.py&quot;, line 51, in parse
    raise OutputParserException(
langchain.schema.OutputParserException: Could not parse LLM output: `Action: list_tables_sql_db, ''`
</code></pre>
<p>Please help. Thanks!</p>
<p><strong>Update</strong>
The recent updates to langchain version <code>0.0.215</code> seem to have fixed this issue, for me at least.</p>
","gpt-3"
"76413465","How to fune-tune and deploy ChatGPT on Cloud?","2023-06-06 09:59:06","","0","157","<openai-api><gpt-3><chatgpt-api><fine-tuning><gpt-4>","<p>I know - how to fine-tune the ChatGPT. However, I not able to find out - How we can deploy the fine-tuned model in our server/cloud?
Can you anyone please help me with these?</p>
<p>I've created the fine-tune model of ChatGPT. But I'm not getting - how to that model can be in my system/server/cloud?</p>
","gpt-3"
"76413431","Can't call transcribed text from whisper to openai chatbot","2023-06-06 09:55:35","","0","393","<python><google-colaboratory><openai-api><gpt-3><openai-whisper>","<p>This script takes input from microphone and transcribe the speech to text and pass the text to the gpt tex-davinci for generating response.</p>
<p>But the script is not generating any gpt response.</p>
<pre><code>import openai
import gradio as gr

import whisper

import time

model = whisper.load_model(&quot;base&quot;)

prompt_input = ''

def transcribe(audio):
    global prompt_input
  
    # load audio and pad/trim it to fit 30 seconds
    audio = whisper.load_audio(audio)
    audio = whisper.pad_or_trim(audio)

    # make log-Mel spectrogram and move to the same device as the model
    mel = whisper.log_mel_spectrogram(audio).to(model.device)

    # detect the spoken language
    _, probs = model.detect_language(mel)
    print(f&quot;Detected language: {max(probs, key=probs.get)}&quot;)

    # decode the audio
    options = whisper.DecodingOptions()
    result = whisper.decode(model, mel, options)
    prompt_input = result.text

gr.Interface(
    title='OpenAI Whisper ASR Gradio Web UI', 
    fn=transcribe, 
    inputs=[
        gr.inputs.Audio(source=&quot;microphone&quot;, type=&quot;filepath&quot;)
    ],
    outputs=[
        &quot;textbox&quot;
    ],
    live=True).launch(share=True)

openai.api_key = 'API_KEY'
openai.api_base = 'https://api.openai.com'

def ask_gpt(prompt, model):
    response = openai.Completion.create(
        engine=model,
        prompt=prompt,
        max_tokens=1024,
        n=1,
        stop=None,
        temperature=0.7
    )

    return response.choices[0].text.strip()

def main():
    model = 'text-davinci-003'
    while True:
        prompt = prompt_input
        if prompt.lower() == 'quit':
            break

        response = ask_gpt(prompt=f'User: {prompt}\nBot: ', model=model)
        print(f'Bot: {response}')
        
if __name__ == '__main__':
    main()
</code></pre>
<p>I am using the openai whisper ASR <a href=""https://github.com/petewarden/openai-whisper-webapp"" rel=""nofollow noreferrer"">web app</a> for speech-to-text and gpt text-davinci for generating response.
Also, I am running it in Google Colab.
Thanks</p>
","gpt-3"
"76411359","OpenAI API: How do I migrate from text-davinci-003 to gpt-3.5-turbo in NodeJS?","2023-06-06 03:56:25","76412710","0","1260","<openai-api><gpt-3><chatgpt-api><text-davinci-003>","<p>How do I migrate from <code>text-davinci-003</code> to <code>gpt-3.5-turbo</code>?</p>
<p>What I tried to do is the following:</p>
<p>Changing this...</p>
<pre><code>model: &quot;text-davinci-003&quot;
</code></pre>
<p>...to this.</p>
<pre><code>model: &quot;gpt-3.5-turbo&quot;
</code></pre>
<p>Also, changing this...</p>
<pre><code>const API_URL = &quot;https://api.openai.com/v1/completions&quot;;
</code></pre>
<p>...to this.</p>
<pre><code>const API_URL = &quot;https://api.openai.com/v1/chat/completions&quot;;
</code></pre>
<p>The Problem is that it does not work. The code I will be giving is the unmodified code, so that anyone can help me what to change.</p>
<p>Why I wanted this upgrade?
I was irritated by <code>text-davinci-003</code>'s completion. Like sending &quot;Hello&quot; gives me an entire letter not a greeting.</p>
<p>Live Sample (Via Github Pages):
<a href=""https://thedoggybrad.github.io/chat/chatsystem"" rel=""nofollow noreferrer"">https://thedoggybrad.github.io/chat/chatsystem</a></p>
<p>Github Repository:
<a href=""https://github.com/thedoggybrad/chat/tree/main/chatsystem"" rel=""nofollow noreferrer"">https://github.com/thedoggybrad/chat/tree/main/chatsystem</a></p>
","gpt-3"
"76403814","What is the best approach to creating a question generation model using GPT and Bert architectures?","2023-06-05 05:58:55","76411286","0","721","<python><open-source><huggingface-transformers><huggingface><gpt-3>","<p>I want to make a question generation model from questions as well as context. Should I make use of GPT based models or Bert Based architectures.</p>
<p>GPT is able to perform the tasks but sometimes returns with vague questions that were not in the context itself. When I made use of WizardLM(7B), I was able to get generalized questions from the context itself which sounded more natural and were nearly to the point when kept within limit of 3.</p>
","gpt-3"
"76398258","OpenAI GPT-3.5 ""prompt"" argument not working","2023-06-03 22:26:20","76398870","0","1181","<http><openai-api><gpt-3>","<p>I am trying to make a flutter app with the openAI api that works like a chatbot, and I want to add a prompt so that the responses are more specialized, like in the openAI playground on their website.</p>
<p>I am testing the API post function with postman and it worked perfectly fine for me before I tried adding a prompt. I assumed that to add a prompt you just have to add a &quot;prompt&quot;: line in the body like when you work with the text-davinci model, but when I do that I get this message returned:</p>
<pre><code>{
    &quot;error&quot;: {
        &quot;message&quot;: &quot;Unrecognized request argument supplied: prompt&quot;,
        &quot;type&quot;: &quot;invalid_request_error&quot;,
        &quot;param&quot;: null,
        &quot;code&quot;: null
    }
}
</code></pre>
<p>Is there a different way you need to do this with the gpt models, or does the prompt argument just not exist for them?</p>
","gpt-3"
"76363168","OpenAI API: How do I handle errors in Python?","2023-05-30 08:54:55","76371360","3","9050","<python><openai-api><gpt-3><chatgpt-api><gpt-4>","<p>I tried using the below code, but the OpenAI API doesn't have the <code>AuthenticationError</code> method in the library. How can I effectively handle such error.</p>
<pre><code>import openai

# Set up your OpenAI credentials
openai.api_key = 'YOUR_API_KEY'

try:
    # Perform OpenAI API request
    response = openai.some_function()  # Replace with the appropriate OpenAI API function

    # Process the response
    # ...
except openai.AuthenticationError:
    # Handle the AuthenticationError
    print(&quot;Authentication error: Invalid API key or insufficient permissions.&quot;)
    # Perform any necessary actions, such as displaying an error message or exiting the program

</code></pre>
","gpt-3"
"76320182","Gradio Interface does not output anything","2023-05-24 04:52:21","","0","504","<openai-api><gpt-3><gradio>","<p>Via Button click (Gradio Interface) I want to output the return value of a method of the Prompter Class:</p>
<pre><code>class Prompter:
def __init__(self, gpt_model, temper):
    if not os.environ.get(&quot;OPENAI_API_KEY&quot;):
        raise Exception(&quot;Please set the OPENAI_API_KEY environment variable&quot;)

    openai.api_key = os.environ.get(&quot;OPENAI_API_KEY&quot;)

    self.gpt_model = gpt_model
    self.temper = temper

def prompt_model_print(self, messages: list):
    response = openai.ChatCompletion.create(model=self.gpt_model, messages=messages)
    return response[&quot;choices&quot;][0][&quot;message&quot;][&quot;content&quot;]
</code></pre>
<p>I instantiate an Object of this class within the code for my gradio interface:</p>
<pre><code>import gradio as gr def emo(name):
return prompter.prompt_model_print(emo_name_prompts) 
inputs = gr.inputs.Textbox(label=&quot;Name&quot;)
outputs = gr.outputs.Textbox(label=&quot;Emotion&quot;)
demo = gr.Interface(fn=emo, inputs=None, outputs=outputs, title=&quot;Discover Emotion&quot;, description=&quot;Please generate a Python list of 10 new feelings, provide name and sentiment&quot;)
demo.launch(share=True)
</code></pre>
<p>The display appears in my colab cell. When clicking the &quot;generate&quot; button the machine is busy. But it outputs nothing and also my Open AI usage do not change. How do I display the output of <code>prompter.prompt_model_print(emo_name_prompts)</code>
You can assume, that this method works ‚Äì but only outside of the Gradio interface.</p>
","gpt-3"
"76278747","Out of 4 methods for Document Question Answering in LangChain, which one if the fastest and why (ignoring the LLM model used)?","2023-05-18 07:45:43","","2","7271","<python><openai-api><gpt-3><langchain>","<p><strong>NOTE:</strong>: My <em>reference document</em> data changes periodically so if I use Embedding Vector space method, I have to Modify the embedding, say once a day</p>
<p>I want to know these factors so that I can design my system to compensate my <em>reference document data</em> generation latency with creating embedding beforehand using Cron Jobs</p>
<p>There are 4 methods in <code>LangChain</code> using which we can retrieve the QA over Documents. More or less they are wrappers over one another.</p>
<ol>
<li><code>load_qa_chain</code> uses Dynamic Document each time it's called</li>
<li><code>RetrievalQA</code> get it from the Embedding space of document</li>
<li><code>VectorstoreIndexCreator</code> is the wrapper of <code>2.</code></li>
<li><code>ConversationalRetrievalChain</code> uses Embedding Space and it has a memory and chat history too.</li>
</ol>
<p>What is the difference between <code>1,3,4</code> in terms of speed?</p>
<ol>
<li><p>If I use embedding DB, would it be faster than <code>load_qa_chain</code> assuming that making Embedding of the document beforehand (like in <code>2,3</code>)helps or is it same because the time taken for a 50 words Prompt is same as Time taken for a 2000 words (Document Text + Prompt in <code>load_qa_chain</code>) ?</p>
</li>
<li><p>Will the speed be affected If I use <code>ConversationalRetrievalChain</code> with or without <code>memory</code> and <code>chat_history</code>?</p>
</li>
</ol>
","gpt-3"
"76269666","I ran into an error when I try to use YoutubeLoader.from_youtube_url","2023-05-17 07:45:48","","1","1231","<python><youtube-api><openai-api><gpt-3><langchain>","<p>There is my code snippet</p>
<pre><code>import os,openai

from langchain.document_loaders import YoutubeLoader
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import ChatVectorDBChain,ConversationalRetrievalChain

from langchain.chat_models import ChatOpenAI
from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate
)
os.environ[&quot;OPENAI_API_KEY&quot;] = &quot;apikey&quot;
loader = YoutubeLoader.from_youtube_url(youtube_url=&quot;https://www.youtube.com/watch?v=7OPg-ksxZ4Y&quot;,add_video_info=True)
documents = loader.load()

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size = 300,
    chunk_overlap = 20
)

documents = text_splitter.split_documents(documents)
#print(documents)

embeddings = OpenAIEmbeddings()
vector_store = Chroma.from_documents(documents=documents,embedding=embeddings)
retriever = vector_store.as_retriever()

system_template = &quot;&quot;&quot;
Use the following context to answer the user's question.
If you don't know the answer, say you don't, don't try to make it up. And answer in Chinese.
-----------
{context}
-----------
{chat_history}
&quot;&quot;&quot;

messages  =[
    SystemMessagePromptTemplate.from_template(system_template),
    HumanMessagePromptTemplate.from_template('{question}')
]

prompt = ChatPromptTemplate.from_messages(messages)

qa = ConversationalRetrievalChain.from_llm(ChatOpenAI(temperature=0.1,max_tokens=2048),retriever,qa_prompt=prompt)
chat_history = []
while True:
    question = input('ÈóÆÈ¢ò:')
    result = qa({'question':question,'chat_history':chat_history})
    chat_history.append((question,result['answer']))
    print(result['answer'])
</code></pre>
<p>and there is the detail of the error</p>
<pre><code>PS C:\Users\12875\Desktop\Êñ∞Âª∫Êñá‰ª∂Â§π&gt; &amp; E:/Program/python/python.exe c:/Users/12875/Desktop/Êñ∞Âª∫Êñá‰ª∂Â§π/ÂàÜÊûêyoutubeËßÜÈ¢ë.py
Using embedded DuckDB without persistence: data will be transient
Traceback (most recent call last):
  File &quot;c:\Users\12875\Desktop\Êñ∞Âª∫Êñá‰ª∂Â§π\ÂàÜÊûêyoutubeËßÜÈ¢ë.py&quot;, line 30, in &lt;module&gt;
    vector_store = Chroma.from_documents(documents=documents,embedding=embeddings)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;E:\Program\python\Lib\site-packages\langchain\vectorstores\chroma.py&quot;, line 412, in from_documents
    return cls.from_texts(
           ^^^^^^^^^^^^^^^
  File &quot;E:\Program\python\Lib\site-packages\langchain\vectorstores\chroma.py&quot;, line 380, in from_texts
    chroma_collection.add_texts(texts=texts, metadatas=metadatas, ids=ids)
  File &quot;E:\Program\python\Lib\site-packages\langchain\vectorstores\chroma.py&quot;, line 159, in add_texts 
    self._collection.add(
  File &quot;E:\Program\python\Lib\site-packages\chromadb\api\models\Collection.py&quot;, line 84, in add       
    metadatas = validate_metadatas(maybe_cast_one_to_many(metadatas)) if metadatas else None
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;E:\Program\python\Lib\site-packages\chromadb\api\types.py&quot;, line 107, in validate_metadatas   
    validate_metadata(metadata)
  File &quot;E:\Program\python\Lib\site-packages\chromadb\api\types.py&quot;, line 98, in validate_metadata     
    raise ValueError(f&quot;Expected metadata value to be a str, int, or float, got {value}&quot;)
ValueError: Expected metadata value to be a str, int, or float, got None
</code></pre>
<p>The class YoutubeLoader recently updated one of the methods from from_youtube_channel to from_youtube_url.But when i use the from_youtube_url,i happened the error that  &quot;ValueError: Expected metadata value to be a str, int, or float, got None&quot;.I want to know what should I doÔºåthank you!</p>
","gpt-3"
"76255342","Figuring out general specs for running LLM models","2023-05-15 14:57:12","76994670","12","14030","<deep-learning><artificial-intelligence><gpt-3><large-language-model>","<p>I have three questions :</p>
<p>Given count of LLM parameters in Billions, how can you figure how much GPU RAM do you need to run the model ?</p>
<p>If you have enough CPU-RAM (i.e. no GPU) can you run the model, even if it is slow</p>
<p>Can you run LLM models (like h2ogpt, open-assistant) in mixed GPU-RAM and CPU-RAM ?</p>
","gpt-3"
"76233070","Generic Answer when Fine Tuning OpenAI Model","2023-05-12 04:10:33","","1","127","<openai-api><gpt-3><chatgpt-api><text-davinci-003>","<p>I have prepared a dataset and trained a <strong>davinci</strong> model using <a href=""https://platform.openai.com/docs/guides/fine-tuning"" rel=""nofollow noreferrer"">FineTuning</a>. It gives out the correct answer for any variant of questions that exist in the dataset.</p>
<p>But how to fine tune the model to give out something like a &quot;Sorry I do not know the answer to this question&quot;, if we ask anything not in the dataset? For example if I ask &quot;Where was the 2020 Olympics hosted?&quot;, it should give out a generic &quot;Do Not Know&quot; answer, as this question does not exist in the dataset.</p>
","gpt-3"
"76226113","How to use large prompt for GPT-3 models in python?","2023-05-11 09:27:03","","-3","428","<python><gpt-3>","<p>I am going to extract information from docx file using OpenAI GPT-3 model in python.
But the total length of prompts is too big than GPT-3 provided.</p>
<p>If you have any opinion about this problem, please help me.
Thanks</p>
<p>I tried it to split several chunks, but it is not useful.
Because I need whole information of chunks, not each chunks.</p>
","gpt-3"
"76197173","Chrome Extension with Chat GPT-3.5 - ""you must provide a model parameter""","2023-05-08 02:29:40","76197360","3","1090","<javascript><google-chrome-extension><openai-api><gpt-3>","<p>I am making a chrome extension that uses Chat GPT 3.5 and have coded a simple prompt to send to the API using openai api and returns a value in the console.</p>
<p>I have my code below and keep getting this error...</p>
<pre><code>error: 
    code: null
    message: &quot;you must provide a model parameter&quot;
    param: null
    type: &quot;invalid_request_error&quot;
</code></pre>
<p>event though i have a model parameter.</p>
<pre><code>// Define the API key
const API_KEY = &quot;API KEY&quot;;

// Define the endpoint URL
const endpointUrl = &quot;https://api.openai.com/v1/chat/completions&quot;;

// Define the headers
const headers = {
  &quot;Content-Type&quot;: &quot;application/json&quot;,
  Authorization: `Bearer ${API_KEY}`,
};

// Define the maximum number of completions to return
const maxCompletions = 1;

// Define the prompt to send to the API
const prompt = {
   model: &quot;gpt-3.5-turbo&quot;,
   prompt: &quot;Hello, world!&quot;,
   temperature: 0.5,
};

// Send a POST request to the endpoint with the prompt and headers
fetch(endpointUrl, {
  method: &quot;POST&quot;,
  headers,
  body: JSON.stringify({
    prompt,
    max_completions: maxCompletions,
}),
})
  .then((response) =&gt; response.json())
  .then((data) =&gt; {
    // Log the response data to the console
    console.log(data);
  })
  .catch((error) =&gt; {
    console.error(error);
  });
</code></pre>
","gpt-3"
"76173414","OpenAI GPT-3 API: What is the difference between davinci and text-davinci-003?","2023-05-04 12:51:25","76173794","6","17634","<openai-api><gpt-3>","<p>I'm testing the different models for OpenAI, and I noticed that not all of them are developed or trained enough to give a reliable response.</p>
<p>The models I tested are the following:</p>
<pre><code>model_engine = &quot;text-davinci-003&quot;
model_engine = &quot;davinci&quot; 
model_engine = &quot;curie&quot; 
model_engine = &quot;babbage&quot; 
model_engine = &quot;ada&quot; 
</code></pre>
<p>I need to understand what the difference is between <code>davinci</code> and <code>text-davinci-003</code>, and how to improve the responses to match that response when you use ChatGPT.</p>
","gpt-3"
"76172889","Chatgpt api url questions: Chatgpt3.5 error","2023-05-04 11:54:16","","-3","475","<openai-api><gpt-3><chatgpt-api>","<p>I tried to use openai's api,but it didn't work.</p>
<p>It's the curl</p>
<pre><code>curl https://api.openai.com/v1/chat/completions \
 -H &quot;Content-Type: application/json&quot; \
 -H &quot;Authorization: Bearer $OPENAI_API_KEY&quot; \
 -d '{
        &quot;model&quot;: &quot;gpt-3.5-turbo&quot;,
        &quot;messages&quot;: [
           {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant&quot;},
           {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Who won the world series in 2020?&quot;},
           {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;The Los Angeles Dodgers won the World Series in 2020.&quot;},
           {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Where was it played?&quot;}
        ]
     }'
</code></pre>
<p>And,this is the result.</p>
<pre><code>{
    &quot;error&quot;: {
        &quot;message&quot;: &quot;This is not a chat model and thus not supported in the v1/chat/completions endpoint. Did you mean to use v1/completions?&quot;,
        &quot;type&quot;: &quot;invalid_request_error&quot;,
        &quot;param&quot;: &quot;model&quot;,
        &quot;code&quot;: null
    }
}
</code></pre>
<p>I watched the openai docs,the url is right. But it didn't pass.</p>
<p>Thanks for your help !</p>
","gpt-3"
"76153016","gpt chatbot not working after using open ai imports and langchain","2023-05-02 08:24:15","76166261","1","7671","<python><window><openai-api><gpt-3><langchain>","<p>hi i am trying to build a chatbot using openai's api. it's basic function is to read a pdf, txtfile , etc. and answer based on it. i was following a tutorial on <a href=""https://beebom.com/how-train-ai-chatbot-custom-knowledge-base-chatgpt-api/"" rel=""nofollow noreferrer"">https://beebom.com/how-train-ai-chatbot-custom-knowledge-base-chatgpt-api/</a></p>
<p>i used the following code and installed neccessary dependencies:</p>
<pre><code>from gpt_index import SimpleDirectoryReader, GPTListIndex, GPTSimpleVectorIndex, LLMPredictor, PromptHelper, ServiceContext
from langchain.chat_models import ChatOpenAI
import gradio as gr
import sys
import os

os.environ[&quot;OPENAI_API_KEY&quot;] = 'yourapikey'

def construct_index(directory_path):
    max_input_size = 4096
    num_outputs = 512
    max_chunk_overlap = 20
    chunk_size_limit = 600

    prompt_helper = PromptHelper(max_input_size, num_outputs, max_chunk_overlap, chunk_size_limit=chunk_size_limit)

    llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0.7, model_name=&quot;gpt-3.5-turbo&quot;, max_tokens=num_outputs))

    documents = SimpleDirectoryReader(directory_path).load_data()

    index = GPTSimpleVectorIndex(documents, llm_predictor=llm_predictor, prompt_helper=prompt_helper)

    index.save_to_disk('index.json')

    return index

def chatbot(input_text):
    index = GPTSimpleVectorIndex.load_from_disk('index.json')
    response = index.query(input_text, response_mode=&quot;compact&quot;)
    return response.response

iface = gr.Interface(fn=chatbot,
                     inputs=gr.components.Textbox(lines=7, label=&quot;Enter your text&quot;),
                     outputs=&quot;text&quot;,
                     title=&quot;Custom-trained AI Chatbot&quot;)

index = construct_index(&quot;docs&quot;)
iface.launch(share=True)

</code></pre>
<p>after running the code i get the following error:</p>
<pre><code>File &quot;C:\Users\USER\desktop\cht\app.py&quot;, line 1, in &lt;module&gt;
    from gpt_index import SimpleDirectoryReader, GPTListIndex, GPTSimpleVectorIndex, LLMPredictor, PromptHelper, ServiceContext
  File &quot;C:\Users\USER\AppData\Local\Programs\Python\Python311\Lib\site-packages\gpt_index\__init__.py&quot;, line 18, in &lt;module&gt;
    from gpt_index.indices.common.struct_store.base import SQLDocumentContextBuilder
  File &quot;C:\Users\USER\AppData\Local\Programs\Python\Python311\Lib\site-packages\gpt_index\indices\__init__.py&quot;, line 4, in &lt;module&gt;
    from gpt_index.indices.keyword_table.base import GPTKeywordTableIndex
  File &quot;C:\Users\USER\AppData\Local\Programs\Python\Python311\Lib\site-packages\gpt_index\indices\keyword_table\__init__.py&quot;, line 4, in &lt;module&gt;
    from gpt_index.indices.keyword_table.base import GPTKeywordTableIndex
  File &quot;C:\Users\USER\AppData\Local\Programs\Python\Python311\Lib\site-packages\gpt_index\indices\keyword_table\base.py&quot;, line 16, in &lt;module&gt;
    from gpt_index.indices.base import DOCUMENTS_INPUT, BaseGPTIndex
  File &quot;C:\Users\USER\AppData\Local\Programs\Python\Python311\Lib\site-packages\gpt_index\indices\base.py&quot;, line 23, in &lt;module&gt;
    from gpt_index.indices.prompt_helper import PromptHelper
  File &quot;C:\Users\USER\AppData\Local\Programs\Python\Python311\Lib\site-packages\gpt_index\indices\prompt_helper.py&quot;, line 12, in &lt;module&gt;
    from gpt_index.langchain_helpers.chain_wrapper import LLMPredictor
  File &quot;C:\Users\USER\AppData\Local\Programs\Python\Python311\Lib\site-packages\gpt_index\langchain_helpers\chain_wrapper.py&quot;, line 13, in &lt;module&gt;
    from gpt_index.prompts.base import Prompt
  File &quot;C:\Users\USER\AppData\Local\Programs\Python\Python311\Lib\site-packages\gpt_index\prompts\__init__.py&quot;, line 3, in &lt;module&gt;
    from gpt_index.prompts.base import Prompt
  File &quot;C:\Users\USER\AppData\Local\Programs\Python\Python311\Lib\site-packages\gpt_index\prompts\base.py&quot;, line 9, in &lt;module&gt;
    from langchain.schema import BaseLanguageModel
ImportError: cannot import name 'BaseLanguageModel' from 'langchain.schema' (C:\Users\USER\AppData\Local\Programs\Python\Python311\Lib\site-packages\langchain\schema.py)
</code></pre>
<p>how can i fix this?</p>
","gpt-3"
"76138660","problem with running OpenAI Cookbook's chatbot","2023-04-29 22:05:30","76187971","-1","186","<python><streamlit><openai-api><gpt-3><chatgpt-api>","<p>I'm having trouble running the chatbot app in the OpenAI Cookbook repository.</p>
<h1>What I tried</h1>
<p>I installed the necessary packages with 'pip install -r requirements.txt'. I made .env file with my OpenAI API Key, and inserted the code below in chatbot.py line 9.</p>
<pre><code>import os
openai.api_key = os.getenv(&quot;OPENAI_API_KEY&quot;)
</code></pre>
<p>The setup above is by my guess, because the doc is totally unclear about how to set up.</p>
<p>I run the app in local by the command &quot;streamlit run apps/chatbot-kickstarter/chat.py.&quot; It didn't work properly. The app run but when I entered text and pressed 'submit' button in the app, I got an error:</p>
<pre><code>Uncaught app exception
Traceback (most recent call last):
  File &quot;C:\Users\XXX\AppData\Local\Programs\Python\Python310\lib\site-packages\streamlit\runtime\scriptrunner\script_runner.py&quot;, line 565, in _run_script
exec(code, module.__dict__)
  File &quot;F:\PythonProjects\openai-cookbook\apps\chatbot-kickstarter\chat.py&quot;, line 71, in &lt;module&gt;
response = query(messages)
  File &quot;F:\PythonProjects\openai-cookbook\apps\chatbot-kickstarter\chat.py&quot;, line 51, in query
response = st.session_state['chat'].ask_assistant(question)
  File &quot;F:\PythonProjects\openai-cookbook\apps/chatbot-kickstarter\chatbot.py&quot;, line 61, in ask_assistant
if 'searching for answers' in assistant_response['content'].lower():
TypeError: string indices must be integers
</code></pre>
<p>I use Python 3.10.6.</p>
<p>I would appreciate any help or guidance to resolve these issues.</p>
","gpt-3"
"76106366","how to use tiktoken in offline mode computer","2023-04-26 00:36:11","","8","19447","<python><tokenize><gpt-3>","<pre><code>import tiktoken

tokenizer = tiktoken.get_encoding(&quot;cl100k_base&quot;) tokenizer = tiktoken.encoding_for_model(&quot;gpt-3.5-turbo&quot;)

text = &quot;Hello, nice to meet you&quot;

tokenizer.encode(text)

</code></pre>
<p>This keeps showing error called requests.exceptions.SSLError. Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate in certificate chain (_ssl.c:1002)')) ... like this.</p>
<p>I wanted to run this code to see the number of tokens. But it keeps showing error as I mentioned earlier. What am I missing for the code?</p>
","gpt-3"
"76100892","GPT4 - Unable to get response for a question?","2023-04-25 11:57:01","","1","1118","<gpt-3><chatgpt-api><gpt-4><gpt4all><pygpt4all>","<p>As the title clearly describes the issue I've been experiencing, I'm not able to get a response to a question from the dataset I use using the <a href=""https://github.com/nomic-ai/gpt4all"" rel=""nofollow noreferrer"">nomic-ai/gpt4all</a>. The execution simply stops. No exception occurs. How can I overcome this situation?</p>
<p>p.s. I use the offline mode of <code>GPT4</code> since I need to process a bulk of questions.</p>
<p>p.s. This issue occurs <strong>after a while</strong>. Something prevents <code>GPT4All</code> to generate a response for the given question.</p>
<pre><code>question = 'Was Avogadro a  professor at the University of Turin?'
gpt = GPT4All()
gpt.open()
resp = gpt.prompt(question)
print(f'Response: {resp}')  # --&gt; the execution does not reach here
</code></pre>
","gpt-3"
"76100128","How to stop GPT-3.5-Turbo model from generating text (azure)?","2023-04-25 10:24:01","","2","1150","<azure><gpt-3><azure-openai><text-davinci-003>","<p>In my use case I am using openai models hosted on azure. I am trying to generate a list of senteces or words with a specific length. Lets take this prompt as an example:</p>
<pre><code>Give 10 Examples of pizza ingredients: 
1. tomatoes
2. mushrooms
</code></pre>
<p>The text-davinci-003 model completes the list as expected and stops but the gpt-3.5-turbo model generates tokens until the token limit is reached, even when I tell the model to stop when the task is done. Using few shot prompting also doesn't seem to work here.</p>
<p>Hacky workarounds</p>
<ul>
<li><p>Using a low value for max_tokens. But it is hard to estimate the value because parts of the prompt will be changed dynamically in the application. And it still needs postprocessing to remove wasted tokens.</p>
</li>
<li><p>Put a counter before the examples and then using a specific number as stop sequence. When using a general counter like above then I need to ensure that the stop sequence won't be generated accidentally so that the model stops. When using an unusual counter like &quot;1~~&quot;, &quot;2~~&quot;... there is a chance that the model malforms the stop sequence so that it still will be generating until the limit is reached.</p>
</li>
</ul>
<p>Is there a clean and easy solution to let the model stop generating, like text-davinci-003 does?</p>
","gpt-3"
"76091454","How can I improve my ChatGPT API prompts?","2023-04-24 11:37:33","","0","782","<prompt><openai-api><gpt-3><chatgpt-api><gpt-4>","<p>I am having an issue with ChatGPT API relating to prompt engineering</p>
<p>I have a dataset which consists of individual product titles, and product descriptions which was awful design, but I didn't have control over that part. <strong>I need to create aggregate titles for the individual titles.</strong></p>
<p>I fine-tuned the Curie model on the data in a similar way to this:</p>
<p>Prompt:</p>
<p><code>60cm tall Oak Drop Leaf Table|80cm tall Oak Drop Leaf Table|100cm tall Oak Drop Leaf Table|60cm tall Drop Leaf Table Material: Oak with bird design</code></p>
<p>Completion:</p>
<p><code>Oak Drop Leaf Table</code></p>
<p>I fine-tuned it on about 100 human-written titles</p>
<p>I am currently using settings:</p>
<pre><code>$data = array(
    'model' =&gt; $model,
    'prompt' =&gt; $prompt,
    'temperature' =&gt; 0.6,
    'max_tokens' =&gt; 25,
    &quot;top_p&quot; =&gt; 1,
    &quot;frequency_penalty&quot; =&gt; 0,
    &quot;presence_penalty&quot; =&gt; 0.6,
);
</code></pre>
<p>I have varied these, but to no great effect.</p>
<p>I am wondering where am I going wrong?</p>
<p>I am getting responses like:</p>
<p><code>60cm Oak Drop Leaf TableOak Drop Leaf TableOak Drop Leaf Table</code></p>
<p><code>Oak Drop Leaf Table|Oak Drop Leaf Table|Oak Drop Leaf Table</code></p>
<p><code>Oak Drop Leaf Table,Oak Drop Leaf Table,Oak Drop Leaf Table</code></p>
","gpt-3"
"76067091","GPU out of memory fine tune flan-ul2","2023-04-20 18:13:02","76320287","1","492","<gpu><huggingface-transformers><huggingface-tokenizers><gpt-3><fine-tuning>","<blockquote>
<p>OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB
(GPU 0; 15.78 GiB total capacity; 14.99 GiB already allocated; 3.50
MiB free; 14.99 GiB reserved in total by PyTorch) If reserved memory
is &gt;&gt; allocated memory try setting max_split_size_mb to avoid
fragmentation.  See documentation for Memory Management and
PYTORCH_CUDA_ALLOC_CONF</p>
</blockquote>
<p>I have Standard_NC24s_v3 single node GPU with 448GB memory and 4 GPUs. However the error message says the total capacity is 15.78GiB. Is the fine tune not using 4 GPUs? How to get all the 4 GPUs used in the fine tune of Flan-UL2 using huggingface transformers?</p>
","gpt-3"
"76053920","How do I extract only code content from chat gpt response?","2023-04-19 11:25:11","","0","8034","<sql><code-generation><openai-api><gpt-3><chatgpt-api>","<p>I use <code>chatGpt</code> to generate SQL query using <code>openai</code> api(<code>/v1/chat/completions</code>) and <code>gpt-3.5-turbo</code> as the model.</p>
<p>But I am facing difficulty in extracting SQL query from the response. Because sometime chatGpt will provide some explanation for query sometimes not. I have tried with regex expressions, but it is not reliable.</p>
<pre><code>regex = r&quot;SELECT .*?;&quot;
match = re.search(regex, result)
if match:
   sql_query = match.group()
   print(sql_query)
</code></pre>
<p>Is there any other approach to extract only the code section from the response?</p>
","gpt-3"
"76025799","Create multi-message conversations with the GPT API","2023-04-16 03:42:43","","6","11060","<python><python-requests><openai-api><gpt-3><chatgpt-api>","<p>I am experimenting with the GPT API by OpenAI and am learning how to use the GPT-3.5-Turbo model. I found a quickstart example on the web:</p>
<pre><code>def generate_chat_completion(messages, model=&quot;gpt-3.5-turbo&quot;, temperature=1, max_tokens=None):
    headers = {
        &quot;Content-Type&quot;: &quot;application/json&quot;,
        &quot;Authorization&quot;: f&quot;Bearer {API_KEY}&quot;,
    }

    data = {
        &quot;model&quot;: model,
        &quot;messages&quot;: messages,
        &quot;temperature&quot;: temperature,
    }

    max_tokens = 100

    if max_tokens is not None:
        data[&quot;max_tokens&quot;] = max_tokens

    response = requests.post(API_ENDPOINT, headers=headers, data=json.dumps(data))

    if response.status_code == 200:
        return response.json()[&quot;choices&quot;][0][&quot;message&quot;][&quot;content&quot;]
    else:
        raise Exception(f&quot;Error {response.status_code}: {response.text}&quot;)

while 1:
    inputText = input(&quot;Enter your message: &quot;)

    messages = [
        {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: inputText},
    ]

    response_text = generate_chat_completion(messages)
    print(response_text)
</code></pre>
<p>With the necessary imports and the API key and endpoint defined above the code block. I added the inputText variable to take text inputs and an infinite <em>while</em> loop to keep the input/response cycle going until the program is terminated (probably bad practice).</p>
<p>However, I've noticed that responses from the API aren't able to reference previous parts of the conversation like the ChatGPT web application (rightfully so, as I have not mentioned any form of conversation object). I looked up on the API documentation on chat completion and the conversation request example is as follows:</p>
<pre><code>[
  {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant that translates English to French.&quot;},
  {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: 'Translate the following English text to French: &quot;{text}&quot;'}
]
</code></pre>
<p>However, this means I will have to send all the inputted messages into the conversation at once and get a response back for each of them. I cannot seem to find a way (at least as described in the API) to send a message, then get one back, and then send another message in the format of a full conversation with reference to previous messages like a chatbot (or as described before the ChatGPT app). Is there some way to implement this?</p>
<p>Also: the above does not use the OpenAI Python module. It uses the <a href=""https://requests.readthedocs.io/en/latest/"" rel=""noreferrer"">Requests</a> and JSON modules.</p>
","gpt-3"
"76014800","OpenAI GPT-3 API error: ""Cannot find module '@openai/api'""","2023-04-14 11:50:53","","1","1478","<javascript><node.js><openai-api><gpt-3>","<p>I am having trouble using the OpenAI API with Node.js. Specifically, I am trying to use the openai.Completion object, but I keep getting a <code>Cannot find module '@openai/api'</code> error.</p>
<p>I have already tried installing the @openai/api package using <code>npm install @openai/api</code>, but I get a 404 error indicating that the package could not be found. I have also removed it and reinstalled but no luck.</p>
<p>I also tried upgrading to the latest version of Node.js, which is currently 19.1.0, but the issue is stuborn. I created a test script (test.js) with the following code:</p>
<pre><code>const openai = require('openai');

openai.apiKey = 'sk-XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX';

async function runTest() {
    try {
        const gpt3Response = await openai.Completion.create({
            engine: 'davinci-codex',
            prompt: `Create a simple conversational response for beginners, with an easy question at the end, based on the input: &quot;Hello, how are you?&quot;`,
            max_tokens: 50,
            n: 1,
            stop: null,
            temperature: 0.5,
        });
        console.log(gpt3Response.choices[0].text.trim());
    } catch (error) {
        console.error(error);
    }
}

runTest();
</code></pre>
<p>When I run this script with <code>node test.js</code>, I get the following error:</p>
<pre><code>Error: Cannot find module '@openai/api'
Require stack:
- C:\Users\User\Documents\Coding\folders\test.js
</code></pre>
<p>I have also tested the OpenAI API using VSC Thunder Client, and it seems to work. Here is the POST request I used:</p>
<pre><code>POST https://api.openai.com/v1/engines/davinci/completions
{
    &quot;prompt&quot;: &quot;do you like soccer&quot;,
    &quot;max_tokens&quot;: 50,
    &quot;n&quot;: 1,
    &quot;stop&quot;: null,
    &quot;temperature&quot;: 0.5,
    &quot;top_p&quot;: 1,
    &quot;echo&quot;: false
}
</code></pre>
<p>I received the following response:</p>
<pre><code>{
    &quot;id&quot;: &quot;cmpl-75BDDTIZ2Q1yodctHcEohCIsA1f46&quot;,
    &quot;object&quot;: &quot;text_completion&quot;,
    &quot;created&quot;: 1681469095,
    &quot;model&quot;: &quot;davinci&quot;,
    &quot;choices&quot;: [{
        &quot;text&quot;: &quot;?‚Äù\n\n‚ÄúI‚Äôm not sure. I‚Äôve never been to a game.‚Äù\n\n‚ÄúI‚Äôm going to the game on Saturday. Would you like to go with me?&quot;,
        &quot;index&quot;: 0,
        &quot;logprobs&quot;: null,
        &quot;finish_reason&quot;: &quot;length&quot;
    }],
    &quot;usage&quot;: {
        &quot;prompt_tokens&quot;: 4,
        &quot;completion_tokens&quot;: 49,
        &quot;total_tokens&quot;: 53
    }
}
</code></pre>
<p>Could you please help me understand what could be causing the <code>Cannot find module '@openai/api'</code> error?</p>
<p>To provide me with next steps to try figure out why this API is not working. Either solutions or further tests I can try.</p>
<p>Thank you!</p>
","gpt-3"
"76001873","Invalid URL (POST /v1/chat/completions) error in Python","2023-04-13 05:18:25","76001924","-1","1084","<python><gpt-3><chatgpt-api>","<p>I have a tts Python program that interprets speech-to-text data, and after that it asks this prompt to the GPT davinci-003 API and answers back, but I just switched to GPT 3.5 turbo, and it doesn't work because of the <em>Invalid URL (POST /v1/chat/completions)</em> error.</p>
<p>I tried checking the model endpoint compatibility web page and also tried asking <a href=""https://en.wikipedia.org/wiki/GPT-3"" rel=""nofollow noreferrer"">GPT-3</a> and <a href=""https://en.wikipedia.org/wiki/GPT-4"" rel=""nofollow noreferrer"">GPT-4</a>. I didn‚Äôt get any answer.</p>
<p>I checked <a href=""https://en.wikipedia.org/wiki/Reddit"" rel=""nofollow noreferrer"">Reddit</a> as well, but I didn‚Äôt find anything. Also, I checked Stack¬†Overflow, but I didn‚Äôt find anything either.</p>
<p>This is the API endpoint URL or at least the one I tried.</p>
<p><a href=""https://i.sstatic.net/8FOrS.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/8FOrS.png"" alt=""Enter image description here"" /></a></p>
<p>My current GPT-3.5 turbo engine code:</p>
<p><a href=""https://i.sstatic.net/9zxYw.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/9zxYw.png"" alt=""Enter image description here"" /></a></p>
","gpt-3"
"75979815","How to add 'message history' to llama-index based GPT-3 in Python","2023-04-10 18:45:19","","7","3730","<python><openai-api><gpt-3><llama-index><gpt-index>","<p>I am fairly new to using llama-index library for training GPT-3 as well as using ChatGPT through the standard API (both in Python). I have noticed that standard ChatGPT API i could simply do the following code below to have ChatGPT get message history as context:</p>
<pre><code>message_history=[]
completion = openai.ChatCompletion.create(model=&quot;gpt-3.5-turbo&quot;,messages=message_history)
</code></pre>
<p>Now I am using llama-index library to train GPT-3 on a more specific context, however I don't know how to have the model consider the message_history as well, here is a code that I am currently working on an i dont know how to implement message history:</p>
<pre><code>
def construct_index(directory_path):
    # set maximum input size
    max_input_size = 4096
    # set number of output tokens
    num_outputs = 2000
    # set maximum chunk overlap
    max_chunk_overlap = 20
    # set chunk size limit
    chunk_size_limit = 600 

    # define prompt helper
    prompt_helper = PromptHelper(max_input_size, num_outputs, max_chunk_overlap, chunk_size_limit=chunk_size_limit)
    # define LLM
    llm_predictor = LLMPredictor(llm=OpenAI(temperature=0.5, model_name=&quot;text-ada-001&quot;, max_tokens=num_outputs))
    # define context (dataset)
    documents = SimpleDirectoryReader(directory_path).load_data()
    # transform context to index format
    service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper)
    index = GPTSimpleVectorIndex.from_documents(documents, service_context=service_context)
    # important: index are are like map, has latitutdes and logntitudes to indicate how each city (texts) are close to each other
    index.save_to_disk(&quot;index.json&quot;)
    return index

index = GPTSimpleVectorIndex.load_from_disk(&quot;index.json&quot;)
dbutils.widgets.text(&quot;user_input&quot;, &quot;user: &quot;)
response = index.query(dbutils.widgets.get(&quot;user_input&quot;),response_mode='compact')
print(&quot;Response: &quot;, response.response)
</code></pre>
","gpt-3"
"75969974","OpenAI GPT-3 API: Why do I get an unexpected response?","2023-04-09 10:26:58","75977040","-4","445","<python><jupyter-notebook><openai-api><gpt-3>","<p>I am connecting to the GPT-3 API through a Jupyter Notebook. This is the code:</p>
<pre><code>import openai
import os

# Set up your API key
openai.api_key = os.environ[&quot;OPENAI_API_KEY&quot;]

# Choose the API endpoint
model_engine = &quot;davinci&quot;

# Create a prompt
prompt = &quot;Hello, ChatGPT!&quot;

# a temperature of 0.5 returns gibberish
# Generate a response
response = openai.Completion.create(
    engine = model_engine,
    prompt = prompt,
    max_tokens = 1024,
    temperature = 0.5,
    frequency_penalty = 0.5,
    presence_penalty = 0.5
)

# Print the response
print(response.choices[0].text)
</code></pre>
<p>Attempting to debug the code led to me playing around with the <code>temperature</code>, <code>frequency_penalty</code> and <code>presence_penalty</code>. I figure I'm doing something wrong if I can't make it work with such a simple prompt.</p>
<p>If you want an example of the unexpected responses I am getting from the simple prompt above, here is the beginning of a few of them:</p>
<blockquote>
<p>I‚Äôm here to review a product that I was sent for free. This is not a
paid advertisement and all opinions are my own. I have been using the
new Bamboo Pen &amp; Touch tablet from Wacom for about a month now and I
have to say that I am very impressed with this product! The Bamboo is
a tablet designed for the everyday user, whether you are an artist or
just someone who likes to sketch on the computer. It‚Äôs also great for
people like me who use their tablets primarily for writing.</p>
</blockquote>
<blockquote>
<p>ChatGPT is a chat bot powered by Google Assistant. It can handle up to
10,000 messages per month for free and more if you pay for premium
services. This bot can be used for customer support, sales and
marketing, human resources, and more. In this tutorial, I will show
you how to create a chatbot using ChatGPT with PHP. We will use
Laravel as a backend framework. If you don‚Äôt have an account at
ChatGPT yet, sign up here first.</p>
</blockquote>
<p>I was expecting a simple &quot;Hi, how can I assist&quot;. What's incorrect here?</p>
","gpt-3"
"75966973","ChatBot - Trouble using custom gpt_index and langchain libraries for creating a GPT-3 based search index","2023-04-08 18:20:39","","1","1651","<python><openai-api><gpt-3>","<p>FYI : I am trying to build a chatbot based on the instructions given by Dan Shipper <a href=""https://www.lennysnewsletter.com/p/i-built-a-lenny-chatbot-using-gpt"" rel=""nofollow noreferrer"">https://www.lennysnewsletter.com/p/i-built-a-lenny-chatbot-using-gpt</a>
I'm trying to use custom libraries called gpt_index and langchain to create a GPT-3 based search index using the OpenAI API. I have successfully installed the libraries and have the following code. BTW I am using google Colab for the environment.</p>
<pre class=""lang-py prettyprint-override""><code>from gpt_index import SimpleDirectoryReader, GPTListIndex, readers, GPTSimpleVectorIndex, LLMPredictor, PromptHelper
from langchain import OpenAI
import sys
import os
from IPython.display import Markdown, display

def construct_index(directory_path):
    ...
    llm_predictor = LLMPredictor(llm=OpenAI(temperature=0, model_name=&quot;text-davinci-003&quot;, max_tokens=num_outputs))
    prompt_helper = PromptHelper(max_input_size, num_outputs, max_chunk_overlap, chunk_size_limit=chunk_size_limit)
 
    documents = SimpleDirectoryReader(directory_path).load_data()
    
    index = GPTSimpleVectorIndex(
        documents, llm_predictor=llm_predictor, prompt_helper=prompt_helper
    )

    index.save_to_disk('index.json')
    return index

def ask_lenny():
    index = GPTSimpleVectorIndex.load_from_disk('index.json')
    while True: 
        query = input(&quot;What do you want to ask Lenny? &quot;)
        response = index.query(query, response_mode=&quot;compact&quot;)
        display(Markdown(f&quot;Lenny Bot says: &lt;b&gt;{response.response}&lt;/b&gt;&quot;))
</code></pre>
<p>When I call the construct_index function with the path to my documents, I get the following error:
<code>TypeError: __init__() got an unexpected keyword argument 'llm_predictor'</code></p>
<p>It seems that there is a mismatch between the expected arguments of the <code>GPTSimpleVectorIndex</code> class and the provided arguments in the code. Unfortunately, I cannot find any documentation or examples for these custom libraries.</p>
<p>Could anyone help me understand how to correctly initialize the GPTSimpleVectorIndex class and resolve this error? Any guidance on using these libraries would be greatly appreciated.</p>
<p>Thank you!</p>
<p>I am running this in Google Colab and see the error.</p>
","gpt-3"
"75946877","How to fix ""TypeError: dataclass_transform() got an unexpected keyword argument 'field_specifiers'"" when using gpt-index/llama-index?","2023-04-06 07:27:37","","1","367","<python-3.x><jupyter-notebook><openai-api><gpt-3><chatgpt-api>","<p>I am trying to use gpt-index/llama-index to feed ChatGPT with custom data to build a custom chatbot. When I try to import either gpt-index or llama-index to Jupyter, I get the following error.</p>
<p><a href=""https://i.sstatic.net/LByAn.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/LByAn.png"" alt=""Error Message"" /></a></p>
<p>I have tried uninstalling and reinstalling, but the problem persists.</p>
<p>I am using Python 3.9.16 on Jupyter Notebook 6.4.8</p>
","gpt-3"
"75942269","How to generate gpt-3 completion beyond max token limit","2023-04-05 17:11:47","","0","2044","<python><openai-api><gpt-3><chatgpt-api>","<p>I want to ask if there's a way to properly use OpenAI API to generate complete responses even after the max token limit.
I'm using the official OpenAI python package but can't find any way to replicate that in GPT-3 (text-davinci-003) since it doesn't support chat interface.</p>
<p>My code for this is currently like this</p>
<pre><code>
response = openai.Completion.create(

        model=&quot;text-davinci-003&quot;,

        prompt=prompt,

        max_tokens=2049-len(prompt)

      )

      text = response.choices[0].text.strip()
</code></pre>
","gpt-3"
"75935538","OpenAI GPT-3 API error: ""AttributeError: 'builtin_function_or_method' object has no attribute 'text'""","2023-04-05 04:01:44","75937188","0","780","<python><openai-api><gpt-3>","<p>I'm looking for some help in extracting the &quot;text&quot; from ChatGPT's &quot;openai.Completion.create&quot; function.</p>
<p>This the function I'm using to generate the &quot;response&quot;:</p>
<pre><code>#Have ChatGPT generate keywords from article
def generate_keywords(article):
    response = openai.Completion.create(
        model=&quot;text-davinci-003&quot;,
        prompt=article,
        temperature=0.7,
        max_tokens=60,
        top_p=1.0,
        frequency_penalty=0.0,
        presence_penalty=1
    )
    return response
#---
</code></pre>
<p>&quot;article&quot; in this case is the text I am feeding ChatGPT.</p>
<p>&quot;response&quot; when printed, provides me with the following output:</p>
<pre><code>{
  &quot;choices&quot;: [
    {
      &quot;finish_reason&quot;: &quot;length&quot;,
      &quot;index&quot;: 0,
      &quot;logprobs&quot;: null,
      **&quot;text&quot;: &quot;, Iraq 2008. Image by Flickr User VABusDriverNow, I can\u2019t speak for all of us, but I know that after the war we were still running. Running from our pasts, guilt, shame, fear, and the unexplainable anger that comes with being a&quot;**
    }
  ],
  &quot;created&quot;: 1680666103,
  &quot;id&quot;: &quot;cmpl-71oJjQfWtHlTbcVsyfi7zzJRktzVT&quot;,
  &quot;model&quot;: &quot;text-davinci-003&quot;,
  &quot;object&quot;: &quot;text_completion&quot;,
  &quot;usage&quot;: {
    &quot;completion_tokens&quot;: 60,
    &quot;prompt_tokens&quot;: 1090,
    &quot;total_tokens&quot;: 1150
  }
}
</code></pre>
<p>I want to extract the &quot;text&quot; from this data structure.</p>
<p>When I run this:</p>
<pre><code>keywords = generate_keywords(article)
print(keywords.values.text)
</code></pre>
<p>But I get this:</p>
<pre><code>File &quot;/Users/wolf/Development/OpenAI/generate_medium_story_image/generate_AI_image.py&quot;, line 63, in &lt;module&gt;
    print(keywords.values.text)
          ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'builtin_function_or_method' object has no attribute 'text'
</code></pre>
","gpt-3"
"75931067","Convert Python Script to Work with ""GPT-3.5-turbo"" model","2023-04-04 15:15:40","75931129","0","1226","<python><chat><openai-api><gpt-3>","<p>I have the following python code working for the text-davinci-003</p>
<pre><code>import openai
import time

openai.api_key = &quot;skXXXXXXX&quot;
model_engine = &quot;text-davinci-003&quot;

# Define the prompt for the conversation
prompt = &quot;Conversation with an AI:&quot;

while True:
    # Get the user's input
    user_input = input(prompt + &quot; &quot;)
    
    # Check if the user wants to exit
    if user_input.lower() in [&quot;quit&quot;, &quot;exit&quot;, &quot;bye&quot;]:
        print(&quot;Goodbye!&quot;)
        break
    
    # Generate a response from the OpenAI API
    response = openai.Completion.create(
        engine=model_engine,
        prompt=prompt + &quot; &quot; + user_input,
        max_tokens=1024,
        n=1,
        stop=None,
        temperature=0.7,
    )

    # Print the AI's response
    message = response.choices[0].text.strip()
    print(&quot;AI: &quot; + message)

    # Wait for a bit before continuing
    time.sleep(1)
</code></pre>
<p>For the life of me I can't get it to work with &quot;GPT-3.5-turbo&quot;. I have tried the following code from a github repo but I get errors:</p>
<pre><code>import openai

# load and set our key
openai.api_key = open(&quot;key.txt&quot;, &quot;r&quot;).read().strip(&quot;\n&quot;)

completion = openai.ChatCompletion.create(
  model=&quot;gpt-3.5-turbo&quot;, # this is &quot;ChatGPT&quot; $0.002 per 1k tokens
  messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;What is the circumference in km of the planet Earth?&quot;}]
)

reply_content = completion.choices[0].message.content
print(reply_content)
</code></pre>
<p>But it fails with the error: <code>AttributeError: module 'openai' has no attribute 'ChatCompletion'</code></p>
<p>Can someone kindly help!!!</p>
<p>Thanks.</p>
","gpt-3"
"75924179","Llama_index unexpected keyword argument error on ChatGPT Model Python","2023-04-03 22:27:15","","8","8528","<python><openai-api><gpt-3><llama-index>","<p>I'm testing a couple of the widely published GPT models just trying to get my feet wet and I am running into an error that I cannot solve.</p>
<p>I am running this code:</p>
<pre><code>from llama_index import SimpleDirectoryReader, GPTListIndex, GPTSimpleVectorIndex, LLMPredictor, PromptHelper
from langchain import OpenAI
import gradio as gr
import sys
import os

os.environ[&quot;OPENAI_API_KEY&quot;] = 'MYKEY'

def construct_index(directory_path):
    max_input_size = 4096
    num_outputs = 512
    max_chunk_overlap = 20
    chunk_size_limit = 600

    prompt_helper = PromptHelper(max_input_size, num_outputs, max_chunk_overlap, chunk_size_limit=chunk_size_limit)

    llm_predictor_gpt = LLMPredictor(llm=OpenAI(temperature=0.7, model_name=&quot;text-davinci-003&quot;, max_tokens=num_outputs))

    documents = SimpleDirectoryReader(directory_path).load_data()

    index = GPTSimpleVectorIndex(documents, llm_predictor=llm_predictor_gpt, prompt_helper=prompt_helper)

    index.save_to_disk('index.json')

    return index

def chatbot(input_text):
    index = GPTSimpleVectorIndex.load_from_disk('index.json')
    response = index.query(input_text, response_mode=&quot;compact&quot;)
    return response.response




iface = gr.Interface(fn=chatbot,
                     inputs=gr.inputs.Textbox(lines=7, label=&quot;Enter your text&quot;),
                     outputs=&quot;text&quot;,
                     title=&quot;Custom-trained AI Chatbot&quot;)

index = construct_index(&quot;salesdocs&quot;)
iface.launch(share=False)
</code></pre>
<p>And I keep getting this error</p>
<pre><code>  File &quot;C:\Users\Anonymous\anaconda3\lib\site-packages\llama_index\indices\vector_store\base.py&quot;, line 58, in __init__
    super().__init__(
TypeError: __init__() got an unexpected keyword argument 'llm_predictor'
</code></pre>
<p>Having a hard time finding much documentation on llamma index errors, hoping someone can point me in the right direction.</p>
","gpt-3"
"75915524","How can I put a python program with specific module dependencies into an HTML page?","2023-04-03 01:40:34","","0","290","<python><html><gpt-3><pyscript><llama-index>","<p>I am trying to make a modified GPT model, designed with Python, available for questions on an HTML page. I have tried using PyScript but I do not know how to give it access to the modules that I want.</p>
<p>Here is my python code in the file gpt.py:</p>
<pre><code>from llama_index import SimpleDirectoryReader, GPTListIndex, readers, GPTSimpleVectorIndex, LLMPredictor, PromptHelper, ServiceContext
from langchain import OpenAI
from IPython.display import Markdown, display

def construct_index(directory_path):
    # set maximum input size
    max_input_size = 4096
    # set number of output tokens
    num_outputs = 2000
    # set maximum chunk overlap
    max_chunk_overlap = 20
    # set chunk size limit
    chunk_size_limit = 600 

    # define prompt helper
    prompt_helper = PromptHelper(max_input_size, num_outputs, max_chunk_overlap, chunk_size_limit=chunk_size_limit)

    # define LLM
    llm_predictor = LLMPredictor(llm=OpenAI(temperature=0.5, model_name=&quot;text-davinci-003&quot;, max_tokens=num_outputs))
 
    documents = SimpleDirectoryReader(directory_path).load_data()
    
    service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper)
    index = GPTSimpleVectorIndex.from_documents(documents, service_context=service_context)

    index.save_to_disk('index.json')

    return index

def ask_ai():
    index = GPTSimpleVectorIndex.load_from_disk('index.json')
    while True: 
        query = input(&quot;What do you want to ask? &quot;)
        response = index.query(query)
        print(response)
        display(Markdown(f&quot;Response: &lt;b&gt;{response.response}&lt;/b&gt;&quot;))

os.environ[&quot;OPENAI_API_KEY&quot;] = &quot;myapikey&quot;
construct_index(&quot;context_data/data&quot;)
ask_ai()
</code></pre>
<p>I tried using PyScript in the following HTML code:</p>
<pre><code>&lt;html&gt;
    &lt;head&gt;
      &lt;link rel=&quot;stylesheet&quot; href=&quot;https://pyscript.net/latest/pyscript.css&quot; /&gt;
      &lt;script defer src=&quot;https://pyscript.net/latest/pyscript.js&quot;&gt;&lt;/script&gt;
    &lt;/head&gt;

  &lt;body&gt;
    &lt;py-config type=&quot;toml&quot;&gt;
        packages = [&quot;llama_index&quot;, &quot;langchain&quot;, &quot;IPython&quot;]

        [[fetch]]
        files = [&quot;./gpt.py&quot;]
    &lt;/py-config&gt;
    &lt;py-script&gt;
      from llama_index import SimpleDirectoryReader, GPTListIndex, readers, GPTSimpleVectorIndex, LLMPredictor, PromptHelper, ServiceContext
      from langchain import OpenAI
      from IPython.display import Markdown, display
      
      os.environ[&quot;OPENAI_API_KEY&quot;] = &quot;myapikey&quot;
      construct_index(&quot;context_data/data&quot;)
      ask_ai()
    &lt;/py-script&gt;
  &lt;/body&gt;
&lt;/html&gt;
</code></pre>
<p>But then I got the error:</p>
<blockquote>
<p>(PY1001): Unable to install package(s) 'llama_index,langchain,IPython'. Reason: Can't find a pure Python 3 Wheel for package(s) 'llama_index,langchain,IPython'. See: <a href=""https://pyodide.org/en/stable/usage/faq.html#micropip-can-t-find-a-pure-python-wheel"" rel=""nofollow noreferrer"">https://pyodide.org/en/stable/usage/faq.html#micropip-can-t-find-a-pure-python-wheel</a> for more information.</p>
</blockquote>
<p>So I followed that link and it told me to make sure all those modules have *py3-none-any.whl files, which they did. Those can be found here:</p>
<p><a href=""https://pypi.org/project/langchain/#files"" rel=""nofollow noreferrer"">https://pypi.org/project/langchain/#files</a></p>
<p><a href=""https://pypi.org/project/gpt-index/#files"" rel=""nofollow noreferrer"">https://pypi.org/project/gpt-index/#files</a></p>
<p><a href=""https://pypi.org/project/ipython/#files"" rel=""nofollow noreferrer"">https://pypi.org/project/ipython/#files</a></p>
<p>It is possible that <code>llama_index</code> should be called <code>gpt-index</code>, but that doesn't change the error message.</p>
<p>How can I make give my HTML page access to these modules? Thank you!</p>
","gpt-3"
"75913490","Conversational Bot with Flan-T5","2023-04-02 17:10:21","","2","649","<chatbot><openai-api><gpt-3><conversational-ai>","<p>I am building a chat bot using flan-T5 model. The bot has a text window where one can give instructions like:</p>
<ul>
<li>Summarize this for me &quot;big text goes here&quot;</li>
</ul>
<p>Or, one might dump the text first in the chat window and then say</p>
<ul>
<li>Summarize the above text (or something similar to that)</li>
</ul>
<p>Or, one might dump a bunch of domain specific facts in the chat window and then ask questions about those.</p>
<p><strong>Question:</strong></p>
<ol>
<li>How can I form the context data for the bot so it has the knowledge about whatever info was passed to be before if it were to summarize something from before or answer quetsions from text that was passed before</li>
<li>How can I create a prompt which detects whether the intent is to <code>ASK QUESTION</code> or <code>CREATE SUMMARY</code> or just <code>INFO ADDITION</code> (in case we are just feeding info to use for asking questions or creating sumarry later.</li>
</ol>
","gpt-3"
"75906140","word to word gpt api responce stream in react native","2023-04-01 12:17:21","","1","1002","<react-native><openai-api><gpt-3><chatgpt-api><gpt-4>","<p>Is there a way to implement GPT API with word to word api</p>
<p>I already try implement in javascript directly into app but its not working</p>
<p>I want to use Chat GPT Turbo api directly in react native (expo) with word by word stream here is working example without stream</p>
","gpt-3"
"75889941","Give GPT (with own knowledge base) an instruction on how to behave before user prompt","2023-03-30 15:09:19","","1","656","<python><prompt><openai-api><gpt-3><chatgpt-api>","<p>I have given GPT some information in CSV format to learn and now I would like to transmit an instruction on how to behave before the user prompt.</p>
<pre><code>def chatbot(input_text):
    index = GPTSimpleVectorIndex.load_from_disk('index.json')
    message_history.append({&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: f&quot;{input_text}&quot;})
    response = index.query(input_text+message_history, response_mode=&quot;compact&quot;)
    return response.response
</code></pre>
<p>&quot;message_history&quot; looks like this:</p>
<pre><code>message_history = [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: f&quot;You are a Pirate! Answer every question in pirate-slang!&quot;}, {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: f&quot;OK&quot;}]
</code></pre>
<p>I got the following error:</p>
<blockquote>
<p>&quot;TypeError: can only concatenate str (not &quot;list&quot;) to str&quot;</p>
</blockquote>
<p>I remember that I have to convert this into tuples but everything I try only causes more chaos...</p>
<p>Here's the whole code:</p>
<pre><code>from gpt_index import SimpleDirectoryReader, GPTListIndex, GPTSimpleVectorIndex, LLMPredictor, PromptHelper
from langchain import OpenAI
import gradio as gr
import sys
import os

os.environ[&quot;OPENAI_API_KEY&quot;] = 'INSERT_KEY_HERE'

message_history = [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: f&quot;You are a Pirate! Answer every question in pirate-slang!&quot;},
               {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: f&quot;OK&quot;}]


def construct_index(directory_path):
    # Index is made of CSV, TXT and PDF Files
    max_input_size = 4096
    num_outputs = 512
    max_chunk_overlap = 20
    chunk_size_limit = 600

prompt_helper = PromptHelper(max_input_size, num_outputs, max_chunk_overlap, chunk_size_limit=chunk_size_limit)

llm_predictor = LLMPredictor(llm=OpenAI(temperature=0.5, model_name=&quot;gpt-3.5-turbo&quot;, max_tokens=num_outputs))

documents = SimpleDirectoryReader(directory_path).load_data()

index = GPTSimpleVectorIndex.from_documents(documents)

index.save_to_disk('index.json')

return index

def chatbot(input_text):
    index = GPTSimpleVectorIndex.load_from_disk('index.json')
    message_history.append({&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: f&quot;{input_text}&quot;})
    response = index.query(input_text+message_history, response_mode=&quot;compact&quot;)
    return response.response

 iface = gr.Interface(fn=chatbot,
                 inputs=gr.inputs.Textbox(lines=7, label=&quot;Enter something here...&quot;),
                 outputs=&quot;text&quot;,
                 title=&quot;ChatBot&quot;)

index = construct_index(&quot;docs&quot;)
iface.launch(share=True)
</code></pre>
","gpt-3"
"75889488","I never get embedded files loaded with from langchain.document_loaders import DirectoryLoader","2023-03-30 14:26:29","","0","891","<python><openai-api><gpt-3><langchain>","<p>My code with from langchain.document_loaders import TextLoader, with a single .txt file it works but with DirectoryLoader nothing.
Attach image of the code:(<a href=""https://i.sstatic.net/VAKxn.png"" rel=""nofollow noreferrer"">https://i.sstatic.net/VAKxn.png</a>)
Attach image of contect of the texts:<a href=""https://i.sstatic.net/W9EeN.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>At the beginning it gave me errors with permissions, I don't know what happened, must it be because of that? then change the permissions of the Store folder to everyone.</p>
<p>But when running the code it always stays loading all the time</p>
<p>I would really appreciate if you help me please</p>
<p>I need the code to embed the words of the text files in the &quot;Store&quot; folder but it doesn't</p>
<p>Like this capture:
(<a href=""https://i.sstatic.net/o2uoW.png"" rel=""nofollow noreferrer"">https://i.sstatic.net/o2uoW.png</a>)</p>
","gpt-3"
"75882988","OpenAI GPT-3 API error: ""Invalid URL (POST /v1/chat/completions)""","2023-03-29 23:32:12","75886728","5","14468","<node.js><openai-api><gpt-3>","<p>Here is my code snippet:</p>
<pre><code>const { Configuration, OpenAI, OpenAIApi } = require (&quot;openai&quot;);
const configuration = new Configuration({
    apiKey: 'MY KEY'
})

const openai = new OpenAIApi(configuration)

async function start() {
    const response = await openai.createChatCompletion({
        model:&quot;text-davinci-003&quot;,
        prompt: &quot;Write a 90 word essay about Family Guy&quot;,
        temperature: 0,
        max_tokens: 1000
    })

    console.log(response.data.choices[0].text)
}

start()
</code></pre>
<p>when I run: <code>node index</code></p>
<p>I run into this issue:</p>
<pre><code>data: {
      error: {
        message: 'Invalid URL (POST /v1/chat/completions)',
        type: 'invalid_request_error',
        param: null,
        code: null
      }
    }
  },
  isAxiosError: true,
  toJSON: [Function: toJSON]
}
</code></pre>
<p>Node.js v18.15.0</p>
<p>I've looked all over the internet and tried some solutions but nothing seems to work. Please help!</p>
<p>Usually others have some link attached to their code when I look up this problem online. Very much a beginner at this stuff so any help would be much appreciated</p>
","gpt-3"
"75882872","How to overcome Rate limit error while working with GPT3 Models using Tenacity","2023-03-29 23:04:29","76323834","1","1195","<pandas><openai-api><gpt-3><tenacity>","<p>In my situation I am trying to pass a prompt using a helper function to the actual GPT3 models, in my case text-ada-001 and then eventually applying it on a pandas column using the following code. but I am recovering the following error:</p>
<pre class=""lang-py prettyprint-override""><code>    def sentiment_prompt(text):
    return &quot;&quot;&quot;Is the sentiment Positive, Negative or Neutral for the following text:
    
    &quot;{}&quot;
    &quot;&quot;&quot;.format(text)
    def sentiment_text(text):
        response = openai.Completion.create(
           engine=&quot;text-ada-001&quot;,
           prompt=sentiment_prompt(text),
           max_tokens=1000,
           temperature=0,
           top_p=1,
           frequency_penalty=0,
           presence_penalty=0
    )
    sentiment = response.choices[0].text
    return sentiment
</code></pre>
<p>and then eventually applying to my pandas column:</p>
<pre><code>    df['sentiment'] = df['text'].apply(lambda x :sentiment_text(x))
</code></pre>
<p><strong>And the error;</strong></p>
<pre><code>    RateLimitError: Rate limit reached for default-global-with-image-limits in organization org-XXXX on requests per min. Limit: 60 / min. Please try again in 1s. Contact support@openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method.
</code></pre>
<p>To overcome this error I was looking into this <a href=""https://github.com/openai/openai-cookbook/blob/main/examples/How_to_handle_rate_limits.ipynb"" rel=""nofollow noreferrer"">link</a> and found that tenacity could help resolve my issue. But I am not sure how to structure my code. I am doing the following at the moment</p>
<p>How do I use the code suggested in the link to overcome the Rate Limit error?</p>
","gpt-3"
"75878060","Langchain - Multiple input SequentialChain","2023-03-29 13:42:52","","2","6520","<python><openai-api><langchain><gpt-3>","<p>I am experiencing with langchain so my question may not be relevant but I have trouble finding an example in the documentation.</p>
<p>Actually, as far as I understand, SequentialChain is made to receive one or more inputs for the first chain and then feed the output of the n-1 chain into the n chain.</p>
<p>Let's say I am working with 3 chains, the first one that takes as input snippet of a csv file and some description about where the csv came from, the next one that take as input snippet of our csv file AND output of the first chains to produce a python script as output.</p>
<p>here is the &quot;no sequential&quot; version that work :</p>
<pre class=""lang-py prettyprint-override""><code>DATA_REVIEW = &quot;&quot;&quot; You are a datascientist specialized in business analysis. You are able to retrieve the most relevant metrics in every json file. You are able to give complete and detailed review of how thoses metrics can be used for making profit. A snippet of the full Json is given as context. Your role is to write down all type of metrics that can be retrieved from the full json. Don't do the calculation, the metrics list will be send to a python developer. You also should include metrics that can be used for comparison.

after the metrics list, write the columns name list. 

context:
{data}


Metrics that can be retrieved from the full json:
&quot;&quot;&quot;
PYTHON_SCRIPT = &quot;&quot;&quot;You are a datascientist specialized in business analysis. You are able to write powerfull and efficient python code to retrieve metrics from a dataset. Your role is to write a python script for all type of metrics described above based on the structure of the dataset. Your python script should print all metrics calculated and 
each products followed by their whole metrics. You should always use pandas library. After you printed out all the metrics, store them as in the example below:
metrics_result = f'Total number of products: (total_products)'
metrics_result += f'Average price of products: (avg_price)'
for index, row in df.iterrows():
    metrics_result += f'Product ID: (row[&quot;product_id&quot;])'
    metrics_result += f'Product Name: (row[&quot;product_name&quot;])'

Make sure to replace unwanted character for each column and to convert value to the desired type before going into calculation. Also pay attention to the columns exact name. Data are represented as a json below but the file they came from is an xlsx. Your code should always start with :



structure:
{data}

Metrics to retrieve:
{output}


python script:


&quot;&quot;&quot;
prompt_template = PromptTemplate(
            input_variables=['data'],
            template=DATA_REVIEW
            )
        openai = OpenAI(model_name=&quot;text-davinci-003&quot;,openai_api_key='KEY', temperature=0, max_tokens=3000)
        output = openai(prompt_template.format(data=data))
        python_script_template = PromptTemplate(
            input_variables=['data','output'],
            template=PYTHON_SCRIPT
            )
        openai = OpenAI(model_name=&quot;text-davinci-003&quot;,openai_api_key='KEY', temperature=0, max_tokens=3000)
        script = openai(python_script_template.format(
                output = output,
                data = data
                ))


#Actual sequential chain script 'not working' 

llm = OpenAI(temperature=0.0)

prompt = PromptTemplate(
    input_variables=[&quot;data_snippet&quot;],
    template=&quot;&quot;&quot;You are a datascientist specialized in business analysis. You are able to retrieve the most relevant metrics in every json file. You are able to give complete and detailed review of how thoses metrics can be used for making profit. Your next project is for a Beauty e-shop business. a snippet of the full Json is given as context. Your role is to write down all type of metrics that can be retrieved from the full json. You also should include metrics that can be used for comparison.
    context:
        {data_snippet}
    
    metrics that can be retrieved from the complete file:
&quot;&quot;&quot;
)


chain = LLMChain(llm=llm, prompt=prompt, output_key='metrics')


data_snippet = read_csv_data(csv_file_path)


data_snippet_str = str(data_snippet)
metrics = chain.run(data_snippet_str)
second_prompt = PromptTemplate(
    input_variables=[&quot;data_snippet&quot;, &quot;metrics&quot;],
    template=
&quot;&quot;&quot;You are a datascientist specialized in business analysis. You are able to write powerfull and efficient python code to retrieve metrics from a dataset. Your role is to write a python script for all type of metrics described above based on the structure of the dataset. Your python script should print all metrics calculated and 
    each products followed by their whole metrics. You should always use pandas library. After you printed out all the metrics, store them as in the example below:
        metrics_result = f'Total number of products: (total_products)'
        metrics_result += f'Average price of products: (avg_price)'
        for index, row in df.iterrows():
            metrics_result += f'Product ID: (row[&quot;product_id&quot;])'
            metrics_result += f'Product Name: (row[&quot;product_name&quot;])'

    Make sure to replace unwanted character for each column and to convert value to the desired type before going into calculation. Also pay attention to the columns exact name. Data are represented as a json below but the file they came from is an xlsx. Your code should always start with :
        import pandas as pd
        data = CSV_FILE
        df = pd.read_csv(data)


    structure:
        {data_snippet}

    Metrics to retrieve:
        {metrics}


    python script:
&quot;&quot;&quot;
)

chain_two = LLMChain(llm=llm, prompt=second_prompt, output_key='script')

from langchain.chains import SimpleSequentialChain

overall_chain = SimpleSequentialChain(chains=[chain, chain_two], input_variables=['data_snippet_str'], output_variables=[&quot;metrics&quot;,&quot;script&quot;], verbose=True)


python_script = overall_chain.run([data_snippet_str, chain_two])
</code></pre>
","gpt-3"
"75874606","Error: PineconeClient: Project name not set, v0.0.10","2023-03-29 08:02:07","","2","421","<openai-api><gpt-3><chatgpt-api><gpt-4><langchain>","<p>Downloaded GitHub - <a href=""https://github.com/mayooear/gpt4-pdf-chatbot-langchain"" rel=""nofollow noreferrer"">mayooear/gpt4-pdf-chatbot-langchain:</a> GPT4 &amp; LangChain Chatbot for large PDF docs</p>
<p>When I try to run the app, I get following error</p>
<pre><code>PineconeClient: Error getting project name: TypeError: fetch failed

error - [Error: PineconeClient: Project name not set. Call init() first.] {
page: ‚Äò/api/chat‚Äô
}
</code></pre>
<p>My node version is node -v
v18.15.0
Pinecone version is
<code>‚Äú@pinecone-database/pinecone‚Äù: ‚Äú^0.0.10‚Äù,</code></p>
<p>It seems it is due to issue of fetch() which is an experimental feature. How to disable fetch() and use node-fetch()</p>
","gpt-3"
"75871333","Using GPT-3 to identify relationships in a corpus","2023-03-28 21:46:12","","0","157","<nlp><information-extraction><gpt-3><relation-extraction>","<p>I have a corpus of 15K news articles. I would like to train a GPT model (3 or 4) to ingest these texts and then output how the locations, events, actions, participants, and things described in the texts are related to one another. So if the corpus says John Smith took part in a protest, I'd like to tell me this and what other people took part, how the protest was related to specific locations, etc. Is this possible?</p>
<p>If so can someone please point me in the right direction for learning how to do it? When I do searches all I'm finding is links about using GPT models to give extractive or abstractive summaries of individual texts. I suppose that's related but not quite the same.</p>
","gpt-3"
"75866651","Why does LLM(LLaMA) loss drop staircase-like over epochs?","2023-03-28 13:05:24","","2","1133","<loss><gpt-3><fine-tuning><large-language-model><llama-index>","<p>I'm training a LLM(LLaMA-6B) and have noticed that its loss seems to drop in a stair-like fashion over the epochs. Specifically, I'll see little loss change for one epoch, and then suddenly the loss will drop quite a bit after a new epoch.</p>
<p>I'm curious about what might be causing this phenomenon.  Is it something to do with the learning rate, or perhaps the architecture of the model itself? Any insights would be greatly appreciated!
<a href=""https://i.sstatic.net/4Ybpb.jpg"" rel=""nofollow noreferrer"">loss figure</a></p>
<p>I'm curious about what might be causing this phenomenon. Any insights would be greatly appreciated!</p>
","gpt-3"
"75865844","Alpaca Large Language Model from Python script","2023-03-28 11:46:15","","2","1594","<python><c#><artificial-intelligence><gpt-3><large-language-model>","<p>I was able to install <a href=""https://github.com/antimatter15/alpaca.cpp"" rel=""nofollow noreferrer"">Alpaca</a>  under Linux and start and use it interactivelly via the corresponding <code>./chat</code> command.</p>
<p>However, I would like to run it not in interactive mode but from a Python (Jupyter) script with the prompt as string parameter. Also, it should be possible to call the model several times without needing to reload it each time.</p>
<p>I already wrote a Python script that works technically:</p>
<pre><code>import subprocess

# start the Alpaca model as a subprocess 
alpaca_process = subprocess.Popen([&quot;./chat&quot;], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, universal_newlines=True)

# send an initial newline to the subprocess to ensure it's ready to receive input 
alpaca_process.stdin.write(&quot;\n&quot;) 
alpaca_process.stdin.flush()

def alpaca_predict(prompt):
    # send the prompt to Alpaca and get the output
    alpaca_process.stdin.write(prompt + &quot;\n&quot;)
    alpaca_process.stdin.flush()
    output = alpaca_process.stdout.readline().strip()
    return output

# test the function 
prompts = [&quot;Hello&quot;, &quot;What is the meaning of life?&quot;, &quot;Tell me a joke&quot;, &quot;Goodbye&quot;] 
for prompt in prompts:
    response = alpaca_predict(prompt)
    print(f&quot;Prompt: {prompt} - Response: {response}&quot;)
</code></pre>
<p>It works technically now, but unfortunately the model produces only nonsense like this:</p>
<pre><code>Prompt: Hello - Response: 
Prompt: What is the meaning of life? - Response: &gt; The following are some of the most popular programming languages used in web development today, ranked by market share (source Stack Overflow): 1) JavaScript; 2) Python; 3) Java/Javascript hybrid language such as Node.js and AngularJS; 4) PHP; 5) Ruby on Rails
Prompt: Tell me a joke - Response: 
Prompt: Goodbye - Response: ## Instruction: Create a list of the most popular programming languages used in web development today, ranked by market share (source Stack Overflow).
</code></pre>
<p>How to fix this?</p>
","gpt-3"
"75864319","import Image as PIL_Image ModuleNotFoundError: No module named 'Image' while running langchain with DirectoryLoader('source', glob='*.pdf')","2023-03-28 09:09:22","","2","308","<python-3.x><openai-api><gpt-3><gpt-4><langchain>","<pre><code>from langchain.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import DirectoryLoader

import openai
loader = DirectoryLoader('source', glob='*.pdf')

data = loader.load()
</code></pre>
<p>Just this much code... I get this error</p>
<pre><code>  File &quot;C:\Users\vsvrp\anaconda3\envs\GPTtrail2\lib\site-packages\pptx\parts\image.py&quot;, line 13, in &lt;module&gt;
    import Image as PIL_Image
ModuleNotFoundError: No module named 'Image'

Process finished with exit code 1
</code></pre>
<p>I do not get this error if I do this</p>
<pre><code>loader = UnstructuredPDFLoader(&quot;DOStest.pdf&quot;)
</code></pre>
<p>I tried to do pip install Image</p>
<p>It is still not working. Any help would be greatly appreciated.</p>
<p>Working with langchain and documentloaders for the first time and the DirectoryLoader class is supposed to work in this case.</p>
","gpt-3"
"75859074","Getting RateLimitError while implementing openai GPT with Python","2023-03-27 18:14:46","","5","14672","<python><python-3.x><openai-api><gpt-3>","<p>I have started to implement openai gpt model in python. I have to send a single request in which I am getting RateLimitError.</p>
<p>My code looks like this</p>
<pre><code>import openai

key = '&lt;SECRET-KEY&gt;'
openai.api_key = key
model_engine = 'text-ada-001'
prompt = 'Hi, How are you today?'
completion = openai.Completion.create(engine=model_engine, prompt=prompt, max_token=2048, n=1, stop=None, temprature=0.5)
print(completion.choices)
</code></pre>
<p>This is what error I am getting</p>
<pre><code>openai.error.RateLimitError: You exceeded your current quota, please check your plan and billing details.
</code></pre>
<p>So, How do I do development without getting this error? I have checked the doc they provide a free version with limitations but this is the initial stage I have sent only 5-6 requests in an hour.</p>
<p>Thanks advance for your help.</p>
","gpt-3"
"75840719","Getting missing pandas error while trying to fine-tune GPT3","2023-03-25 09:19:15","","3","949","<openai-api><gpt-3><fine-tuning>","<p>I'm using the following command :<br />
<code>openai tools fine_tunes.prepare_data -f ./data.jsonl</code></p>
<p>and I'm getting the following error:</p>
<pre><code>Analyzing...
Traceback (most recent call last):
  File &quot;/Users/jyothiraditya/mambaforge/bin/openai&quot;, line 8, in &lt;module&gt;
    sys.exit(main())
  File &quot;/Users/jyothiraditya/mambaforge/lib/python3.10/site-packages/openai/_openai_scripts.py&quot;, line 63, in main
    args.func(args)
  File &quot;/Users/jyothiraditya/mambaforge/lib/python3.10/site-packages/openai/cli.py&quot;, line 586, in prepare_data
    df, remediation = read_any_format(fname)
  File &quot;/Users/jyothiraditya/mambaforge/lib/python3.10/site-packages/openai/validators.py&quot;, line 477, in read_any_format
    assert_has_pandas()
  File &quot;/Users/jyothiraditya/mambaforge/lib/python3.10/site-packages/openai/datalib.py&quot;, line 56, in assert_has_pandas
    raise MissingDependencyError(PANDAS_INSTRUCTIONS)
openai.datalib.MissingDependencyError: 

OpenAI error: 

    missing `pandas` 

This feature requires additional dependencies:

    $ pip install openai[datalib]
</code></pre>
<p>I tried reinstalling <code>datalib</code> using : <code>pip install --upgrade openai openai&quot;[datalib]&quot;</code> but it did not work</p>
<p>I tried install pandas manually : <code>pip install pandas</code> but it also did not work</p>
<p>What can I do to resolve this error?</p>
","gpt-3"
"75832120","Do nodes in List Index come with embedding vectors in LlamaIndex?","2023-03-24 09:48:25","","0","1623","<embedding><openai-api><gpt-3><llama-index>","<p>One can run an embedding-based query on List Index (<a href=""https://gpt-index.readthedocs.io/en/latest/guides/index_guide.html#list-index"" rel=""nofollow noreferrer"">link</a>). For that nodes in the List Index should be supplied with embedding vectors. What is then the difference between the List Index and the <a href=""https://gpt-index.readthedocs.io/en/latest/guides/index_guide.html#vector-store-index"" rel=""nofollow noreferrer"">Vector Store Index</a>? I thought that the distinctive feature of the Vector Store Index is that it assigns an embedding vector to each nodes in the index but it looks like List Index does the same.</p>
","gpt-3"
"75827960","how do i stop this encoding error in the openai python module?","2023-03-23 21:04:00","","3","736","<python><encoding><character-encoding><openai-api><gpt-3>","<p>i'm trying to make a chat completion bot using opeAI's GPT engine that takes voice input and outputs a text to speech file, however, i keep getting an encoding error that i dont understand</p>
<pre><code>import os
import speech_recognition as sr
import openai
from dotenv import load_dotenv
from os import path
from playsound import playsound
from gtts import gTTS
import simpleaudio as sa

load_dotenv()

language = 'en'

openai.api_key = os.getenv(&quot;OPENAI_API_KEY&quot;)

while True:
    # this recognizes your voice input
    recog = sr.Recognizer()
    with sr.Microphone() as source:
        audio = recog.listen(source)
    #this transcribes the voice to text
    with open(&quot;microphone-results.wav&quot;, &quot;wb&quot;) as f:
        f.write(audio.get_wav_data())
    AUDIO_FILE = path.join(path.dirname(path.realpath(__file__)), &quot;microphone-results.wav&quot;)
    my_question = recog.recognize_sphinx(audio)

    #this generates a response
    response = openai.ChatCompletion.create(
        model=&quot;gpt-3.5-turbo&quot;,
        messages=[
            {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a chatbot named jarvis&quot;},
            {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: str(my_question)},
        ]
    )
    reply = ''.join(choice.message.content for choice in response.choices)
    tts = gTTS(reply)
    tts_file = &quot;temp.wav&quot;

    tts.save(tts_file)
    wave_obj = sa.WaveObject.from_wave_file(tts_file)
    play_obj = wave_obj.play()
    play_obj.wait_done()

    os.remove(tts_file)
</code></pre>
<p>i tried formatting it, thinking it would output the tts result instead, it said this:</p>
<pre><code>  File &quot;c:\Users\tonda\python\SSPS_Projects\PortfolioApps\assistant\functions\ChatGPT\Chat.py&quot;, line 28, in &lt;module&gt;
    response = openai.ChatCompletion.create(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\tonda\scoop\apps\python\3.11.0\Lib\site-packages\openai\api_resources\chat_completion.py&quot;, line 25, in create
    return super().create(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\tonda\scoop\apps\python\3.11.0\Lib\site-packages\openai\api_resources\abstract\engine_api_resource.py&quot;, line 153, in create
    response, _, api_key = requestor.request(
                           ^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\tonda\scoop\apps\python\3.11.0\Lib\site-packages\openai\api_requestor.py&quot;, line 216, in request
    result = self.request_raw(
             ^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\tonda\scoop\apps\python\3.11.0\Lib\site-packages\openai\api_requestor.py&quot;, line 516, in request_raw
    result = _thread_context.session.request(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\tonda\scoop\apps\python\3.11.0\Lib\site-packages\requests\sessions.py&quot;, line 587, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\tonda\scoop\apps\python\3.11.0\Lib\site-packages\requests\sessions.py&quot;, line 701, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\tonda\scoop\apps\python\3.11.0\Lib\site-packages\requests\adapters.py&quot;, line 489, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File &quot;C:\Users\tonda\scoop\apps\python\3.11.0\Lib\site-packages\urllib3\connectionpool.py&quot;, line 703, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\tonda\scoop\apps\python\3.11.0\Lib\site-packages\urllib3\connectionpool.py&quot;, line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File &quot;C:\Users\tonda\scoop\apps\python\3.11.0\Lib\site-packages\urllib3\connection.py&quot;, line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File &quot;C:\Users\tonda\scoop\apps\python\3.11.0\Lib\http\client.py&quot;, line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File &quot;C:\Users\tonda\scoop\apps\python\3.11.0\Lib\http\client.py&quot;, line 1323, in _send_request
    self.putheader(hdr, value)
  File &quot;C:\Users\tonda\scoop\apps\python\3.11.0\Lib\site-packages\urllib3\connection.py&quot;, line 224, in putheader
    _HTTPConnection.putheader(self, header, *values)
  File &quot;C:\Users\tonda\scoop\apps\python\3.11.0\Lib\http\client.py&quot;, line 1255, in putheader
    values[i] = one_value.encode('latin-1')
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
UnicodeEncodeError: 'latin-1' codec can't encode character '\u201c' in position 7: ordinal not in range(256)
</code></pre>
<p>it is saying something about an unknown character? i really dont understand this, partially because im new to coding</p>
","gpt-3"
"75811293","OpenAI API error: ""InvalidRequestError: Resource not found""","2023-03-22 10:51:16","","0","5702","<python><openai-api><gpt-3><fine-tuning>","<p>I've been trying to upload a JSON file that I will use for fine-tuning my GPT-3 model.
I get an error when trying to upload it.</p>
<pre><code>openai.File.create(file=open(&quot;training_data.jsonl&quot;), purpose=&quot;fine-tune&quot;)
</code></pre>
<p>When I run the command above, I get the following error:</p>
<p><code>InvalidRequestError: Resource not found</code></p>
","gpt-3"
"75804599","OpenAI API: How do I count tokens before(!) I send an API request?","2023-03-21 17:35:10","75804651","81","80924","<python><openai-api><chatgpt-api><gpt-3><gpt-4>","<p>OpenAI's text models have a context length, e.g.: Curie has a context length of 2049 tokens.</p>
<p>They provide <code>max_tokens</code> and <code>stop</code> parameters to control the length of the generated sequence. Therefore the generation stops either when stop token is obtained, or <code>max_tokens</code> is reached.</p>
<p>The issue is: when generating a text, I don't know how many tokens my prompt contains. Since I do not know that, I cannot set <code>max_tokens = 2049 - number_tokens_in_prompt</code>.</p>
<p>This prevents me from generating text dynamically for a wide range of text in terms of their length. What I need is to continue generating until the stop token.</p>
<p>My questions are:</p>
<ul>
<li>How can I count the number of tokens in Python API so that I will set <code>max_tokens</code> parameter accordingly?</li>
<li>Is there a way to set <code>max_tokens</code> to the max cap so that I won't need to count the number of prompt tokens?</li>
</ul>
","gpt-3"
"75790862","OpenAI GPT-3 API: Why do I get a response that makes no sense in relation to the question?","2023-03-20 13:27:50","75791101","-1","1535","<api><openai-api><gpt-3>","<p>When I ask a question in parameters of the request, the response has no sentence, and i get other questions in the response. I tried with every &quot;temperature&quot; and the response is never the same that I could get on chatGPT-3. I also tried with every models like davinci-codex, davinci, curie, babbage, etc. Do you have any idea of why ?</p>
<p>Here are the parameters  :</p>
<pre><code>{
    &quot;prompt&quot;: &quot;What's the capital of USA ?&quot;,
    &quot;max_tokens&quot;: 100,
    &quot;n&quot;: 1,
    &quot;stop&quot;: null,
    &quot;temperature&quot;: 0
}
</code></pre>
<p>this is the API response :</p>
<pre><code>{
    &quot;id&quot;: &quot;cmpl-6wA6d1bcNyju7cbqlJKRToOoi8TS2&quot;,
    &quot;object&quot;: &quot;text_completion&quot;,
    &quot;created&quot;: 1679319891,
    &quot;model&quot;: &quot;davinci&quot;,
    &quot;choices&quot;: [
        {
            &quot;text&quot;: &quot;\n\nA: Washington D.C.\n\nQ: What's the capital of Canada ?\n\nA: Ottawa\n\nQ: What's the capital of Australia ?\n\nA: Canberra\n\nQ: What's the capital of England ?\n\nA: London\n\nQ: What's the capital of France ?\n\nA: Paris\n\nQ: What's the capital of Germany ?\n\nA: Berlin\n\nQ: What's the capital of Italy ?&quot;,
            &quot;index&quot;: 0,
            &quot;logprobs&quot;: null,
            &quot;finish_reason&quot;: &quot;length&quot;
        }
    ],
    &quot;usage&quot;: {
        &quot;prompt_tokens&quot;: 7,
        &quot;completion_tokens&quot;: 100,
        &quot;total_tokens&quot;: 107
    }
}
</code></pre>
<p>With 0.5 temperature, the reponse is :</p>
<pre><code>{
    &quot;id&quot;: &quot;cmpl-6wA3ZuuAfgrE8ox6dMY2M9tqgOxar&quot;,
    &quot;object&quot;: &quot;text_completion&quot;,
    &quot;created&quot;: 1679319701,
    &quot;model&quot;: &quot;davinci&quot;,
    &quot;choices&quot;: [
        {
            &quot;text&quot;: &quot;\n\nA: Washington D.C.\n\nQ: What's the capital of France ?\n\nA: Paris.\n\nQ: What's the capital of Germany ?\n\nA: Berlin.\n\nQ: What's the capital of China ?\n\nA: Beijing.\n\nQ: What's the capital of Japan ?\n\nA: Tokyo.\n\nQ: What's the capital of Russia ?\n\nA: Moscow.\n\nQ: What's&quot;,
            &quot;index&quot;: 0,
            &quot;logprobs&quot;: null,
            &quot;finish_reason&quot;: &quot;length&quot;
        }
    ],
    &quot;usage&quot;: {
        &quot;prompt_tokens&quot;: 7,
        &quot;completion_tokens&quot;: 100,
        &quot;total_tokens&quot;: 107
    }
}
</code></pre>
<p>And with a more difficult question this is what i get :</p>
<p>Questions :</p>
<pre><code>{
    &quot;prompt&quot;: &quot;What job could I do if I like computers and video games?&quot;,
    &quot;max_tokens&quot;: 100,
    &quot;n&quot;: 1,
    &quot;stop&quot;: null,
    &quot;temperature&quot;: 0
}
</code></pre>
<p>Answer:</p>
<pre><code>{
    &quot;id&quot;: &quot;cmpl-6wAACQ91vbOohAwMbQqvJyOaznU6i&quot;,
    &quot;object&quot;: &quot;text_completion&quot;,
    &quot;created&quot;: 1679320112,
    &quot;model&quot;: &quot;davinci&quot;,
    &quot;choices&quot;: [
        {
            &quot;text&quot;: &quot;\n\nWhat job could I do if I like to work with my hands?\n\nWhat job could I do if I like to work with animals?\n\nWhat job could I do if I like to work with plants?\n\nWhat job could I do if I like to work with people?\n\nWhat job could I do if I like to work with numbers?\n\nWhat job could I do if I like to work with words?\n\nWhat job could I do if I&quot;,
            &quot;index&quot;: 0,
            &quot;logprobs&quot;: null,
            &quot;finish_reason&quot;: &quot;length&quot;
        }
    ],
    &quot;usage&quot;: {
        &quot;prompt_tokens&quot;: 13,
        &quot;completion_tokens&quot;: 100,
        &quot;total_tokens&quot;: 113
    }
}
</code></pre>
","gpt-3"
"75787638","OpenAI GPT-3 API error: ""Request timed out""","2023-03-20 07:43:47","","9","20753","<python><openai-api><gpt-3>","<p>I keep get an error as below</p>
<p><code>Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600)</code></p>
<p>when I run the code below</p>
<pre><code>def generate_gpt3_response(user_text, print_output=False):
    &quot;&quot;&quot;
    Query OpenAI GPT-3 for the specific key and get back a response
    :type user_text: str the user's text to query for
    :type print_output: boolean whether or not to print the raw output JSON
    &quot;&quot;&quot;
    time.sleep(5)
    completions = ai.Completion.create(
        engine='text-davinci-003',  # Determines the quality, speed, and cost.
        temperature=0.5,            # Level of creativity in the response
        prompt=user_text,           # What the user typed in
        max_tokens=150,             # Maximum tokens in the prompt AND response
        n=1,                        # The number of completions to generate
        stop=None,                  # An optional setting to control response generation
    )

    # Displaying the output can be helpful if things go wrong
    if print_output:
        print(completions)

    # Return the first choice's text
    return completions.choices[0].text
</code></pre>
<pre><code>df_test['GPT'] = df_test['Q20'].apply(lambda x: \
              generate_gpt3_response\
              (&quot;I am giving you the answer of respondents \
                in the format [Q20], \
                give me the Broader topics like customer service, technology, satisfaction\
                or the related high level topics in one word in the \
                format[Topic: your primary topic] for the text '{}' &quot;.format(x)))

# result
df_test['GPT'] = df_test['GPT'].apply(lambda x: (x.split(':')[1]).replace(']',''))
</code></pre>
<p>I tried modifiying the parameters, but the error still occurs.</p>
<p>Anyone experienced the same process?</p>
<p>Thanks in advance.</p>
","gpt-3"
"75783524","Train gpt-3 on email conversations","2023-03-19 16:44:07","","1","1165","<openai-api><gpt-3>","<p>I have to train gpt-3 on email data, so that the support team can get a quick answer from a chat-bot, for questions that were asked before by customers. There are email conversations between customers and the support team (Customer1 ask question, Support answers, Customer1 asks another question ‚Ä¶ ). I have to:</p>
<p>1.Filter important conversations and only feed gpt-3 with them.
2.prepare and convert them into the right format, so that I can train the model.
Is there anone who has some ideas about how to realize these steps and weather to use fine tuning or embeddings?</p>
<p>gpt-3 has to connect the questions to the answers that were given by the support team.</p>
","gpt-3"
"75760838","Can i train chatgpt with custom data from a database?","2023-03-16 19:26:17","","4","5223","<openai-api><gpt-3>","<p>Let's say I'm a law firm and I have this tables(basic structure)</p>
<ul>
<li><strong>users</strong>: name, email, telephone etc..</li>
<li><strong>employees</strong>: kind, name, email, telephone etc..</li>
<li><strong>cases</strong>: case name, casenumber, parties names, attorney assigned, entries, last update, status, open_date,  close_date</li>
<li><strong>tasks</strong>:   case_id, employee_assigned,  employee_assigner,statusdue_at</li>
<li><strong>communication</strong>: date, user_id, employee_id, text, kind, duration</li>
<li><strong>cases_assignations</strong>: employee_id, case_id</li>
</ul>
<p>So let's say now I want to train a model with chatGPT or another solution so if for example the employee types:</p>
<pre><code>**Input**: I would like to know the tasks assigned to John that are days due
**output**: John has 3 tasks that are due, these are: Task 1, task 2 ,task 3

**Input**: I would like to how many cases are open
**output**: There are 8 cases open right now
      
**Input**: I would like to how when did john communicate last time with client Elena 
**output**:  John communicate with Elena on october 8 at 6 am on phone and it last 5 minutes

**Input**: I would like to how the cases that John as opened and last update is more than 5 days before
**output**:  John has 8 cases open that are 5 days before, these are: case 1 ,case 2, case 3, case 4
</code></pre>
<p>etc... makes sense?
I would like to know the strategy to make this possible</p>
","gpt-3"
"75744277","How Can I make openAI API respond to requests in specific categories only?","2023-03-15 11:47:20","","1","3250","<python><python-3.x><openai-api><gpt-3><chatgpt-api>","<p>I have created an openAI API using python, to respond to any type of prompt.</p>
<p>I want to make the API respond to requests that are only related to <strong>Ad from product description</strong> and <strong>greetings</strong> requests only and if the user sends a request that's not related to this task, the API should send a message like <strong>I'm not suitable for tasks like this</strong>.</p>
<pre><code>
import os
import openai

openai.api_key = os.getenv(&quot;OPENAI_API_KEY&quot;)

response = openai.Completion.create(
  model=&quot;text-davinci-003&quot;,
  prompt=&quot;Write a creative ad for the following product to run on Facebook aimed at parents:\n\nProduct: Learning Room is a virtual environment to help students from kindergarten to high school excel in school.&quot;,
  temperature=0.5,
  max_tokens=100,
  top_p=1.0,
  frequency_penalty=0.0,
  presence_penalty=0.0
)

</code></pre>
<p>I want to update the code to generate a chat like this. <strong>make bot understand generating ADs and greetings requests and ignoring the others</strong></p>
<p>EX:-</p>
<p><strong>user:-</strong> Hello</p>
<p><strong>api:-</strong> Hello, How can I assist you today with your brand?</p>
<p><strong>user:-</strong> Write a social media post for the following product to run on Facebook aimed at parents:\n\nProduct: Learning Room is a virtual environment to help students from kindergarten to high school excel in school.</p>
<p><strong>api:-</strong> Are you looking for a way to give your child a head start in school? Look no further than Learning Room! Our virtual environment is designed to help students from kindergarten to high school excel in their studies. Our unique platform offers personalized learning plans, interactive activities, and real-time feedback to ensure your child is getting the most out of their education. Give your child the best chance to succeed in school with Learning Room!</p>
<p><strong>user:-</strong> where is the united states located?</p>
<p><strong>api:-</strong> I'm not suitable for this type of tasks.</p>
<p>So, How can update my code?</p>
","gpt-3"
"75731765","gpt3 - error with the openai api when trying to generate an embedding","2023-03-14 10:16:04","","-1","4457","<openai-api><gpt-3>","<p>I have a python code for create a <strong>embedding</strong> with openai, but when I try to execute the code, I receive this <strong>error</strong>:</p>
<p><em>The server is currently overloaded with other requests. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists.</em></p>
<p>This is the <strong>python code</strong>:</p>
<pre class=""lang-py prettyprint-override""><code># Carga el diccionario de HPOs desde un archivo JSON
with open(&quot;hpos.json&quot;) as f:
    hpos_dict = json.load(f)

# Crea un diccionario para almacenar los embeddings
hpo_embeddings = {}

i = 0
hposNumber = len(hpos_dict)
# Crea los embeddings y gu√°rdalos en el diccionario
for hpo_id, hpo_descs in hpos_dict.items():
    embedding_list = []
    for hpo_desc in hpo_descs:
        response = openai.Embedding.create(
            input=hpo_desc,
            model=&quot;text-embedding-ada-002&quot;
        )
        embedding_list.append(response[&quot;data&quot;][0][&quot;embedding&quot;])
    hpo_embeddings[hpo_id] = embedding_list
    i = i + 1
    print( str(i) + &quot;/&quot; + str(hposNumber) )

# Guarda el diccionario de embeddings en un archivo JSON
with open(&quot;hpo_embeddings.json&quot;, &quot;w&quot;) as f:
    json.dump(hpo_embeddings, f)
</code></pre>
","gpt-3"
"75722268","Fine-tuning of OpeanAI model with unsupervised set, not supervised","2023-03-13 13:06:42","","2","819","<openai-api><pre-trained-model><gpt-3><fine-tuning>","<p>I want GPT-3 model to know everything about my domain area, for example my inbox. I want to be able to ask it questions like &quot;Have I even had a Silicon Valley Bank account?&quot; and get correct response. I've familiarized myself with <a href=""https://platform.openai.com/docs/guides/fine-tuning"" rel=""nofollow noreferrer"">fine-tuning mechanism</a> in official OpenAI docs and it's not exactly what I'm looking for. I want to just dump all my emails on the model and ask it: &quot;Learn!&quot;. However fine-tuning require supervised style learning with prompts and reponses, which I do not have. <a href=""https://platform.openai.com/docs/guides/fine-tuning/example-notebooks"" rel=""nofollow noreferrer"">Example</a> in the notebooks for doc suggests that you can use &quot;Davinci-instruct to ask a few questions based on a Wikipedia section, as well as answer those questions, based on that section&quot;, which I guess solves my problem if I apply it to all my emails, but I'd rather not do this step, because I might screw up something. Can I have other options?</p>
<p>I found that Azure Open AI integration <a href=""https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/fine-tuning?pivots=programming-language-studio"" rel=""nofollow noreferrer"">allows you to do fine-tuning</a> as well, but it seems to have the same problem.</p>
<p>I might be calling what I want to do is fine-tuning, but in fact I keep pre-training process and just decided to go with fine-tuning because it has documentation and API. On the other hand fine-tuning guaranties that I would get wrong answers, pre-training doesn't, and you dont want to get wrong answer on question &quot;Have I even had a Silicon Valley Bank account?&quot;</p>
","gpt-3"
"75718913","OpenAI GPT-3 API: Why do I get different, non-related random responses to the same question every time?","2023-03-13 07:12:36","75719777","0","1934","<python><chatbot><openai-api><gpt-3>","<p>I am using the ‚Äútext-davinci-003‚Äù model and I copied the code form the OpenAI playground, but the bot keeps giving me random response to a simple ‚ÄúHello‚Äù everytime.</p>
<p>This is the code I am using :</p>
<pre><code>response: dict = openai.Completion.create(model=&quot;text-davinci-003&quot;,
                                                    prompt=prompt,
                                                    temperature=0.9,
                                                    max_tokens=150,
                                                    top_p=1,
                                                    frequency_penalty=0,
                                                    presence_penalty=0.6,
                                                    stop=[&quot; Human:&quot;, &quot; AI:&quot;])
        choices: dict = response.get('choices')[0]
        text = choices.get('text')
        print(text)
</code></pre>
<p>The response to simple ‚Äúhello‚Äù chat 3 different times :</p>
<ol>
<li><p>the first time it gave me a hello world program for Java</p>
</li>
<li><p>second time it answered correctly - ‚ÄòHi there! How can I help you today?‚Äô</p>
</li>
<li><p>third time:</p>
<pre><code>  def my_method
        puts &quot;hello&quot;
     end
   end
 end

# To invoke this method we would call:
MyModule::MyClass.my_method
</code></pre>
</li>
</ol>
<p>I just dont get it, as using the same simple ‚Äòhello‚Äô prompt in the OpenAI's playground gives me accurate response eveytime - 'Hi there! How can I help you today?'</p>
","gpt-3"
"75709199","SimpleDirectoryReader cannot be downloaded via llama_index's download_loader","2023-03-11 20:04:05","","4","7068","<python><gpt-3>","<p>I am using llama_index package to index some of our own documents and query them using GPT. It works fairly well with individual PDFs. However we have a large anout of PDFs which I would like to load in a single run as using its SimpleDirectoryReader. But I am getting the following error when the following commands were run.</p>
<pre><code>from llama_index import download_loader 
SimpleDirectoryReader = download_loader(&quot;SimpleDirectoryReader&quot;)


FileNotFoundError: [Errno 2] No such file or directory:  C:\\Users\\XXXXX\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\gpt_index\\readers\\llamahub_modules/file/base.py' 
</code></pre>
<p>The readers\llamahub_modules\file folder only has a folder called 'pdf'. It doesn't have a base.py file. How</p>
<p>I tried uninstalling and re-installing llama_index python module but there was no impact.
My python version is 3.8.2</p>
<p>How can I get it working?</p>
","gpt-3"
"75667860","openai unknown command 'tools'","2023-03-07 22:37:38","","0","784","<openai-api><gpt-3><fine-tuning>","<ul>
<li><p>I am learning gpt fine-tuning</p>
</li>
<li><p>I successfully ran this command: pip install --upgrade openai</p>
</li>
<li><p>I couldn't run this command: export OPENAI_API_KEY=&quot;sk-xxxxxxxxxxxxxx&quot;</p>
</li>
<li><p>Error: export : The term 'export' is not recognized as the name of a cmdlet, function, script file, or operable program.</p>
</li>
<li><p>So I ran this command instead: set OPENAI_API_KEY=&quot;sk-xxxxxxxxxxxxxx&quot;</p>
</li>
<li><p>Also ran Set-ExecutionPolicy -ExecutionPolicy Unrestricted -Scope CurrentUser</p>
</li>
<li><p>Now I'm trying to run openai tools fine_tunes.prepare_data -f C:\Users.....fine-tune-upload.xlsx</p>
</li>
</ul>
<p>But it shows: error: unknown command 'tools'</p>
<p>Any idea? Please explain in beginner language!</p>
","gpt-3"
"75655947","Does openai GPT finetuning consider the prompt in the loss function?","2023-03-06 21:18:20","","0","1026","<openai-api><gpt-3><gpt-2>","<p>OpenAI api includes a finetuning service that divides the task in &quot;prompt&quot; and &quot;completion&quot;</p>
<p><a href=""https://platform.openai.com/docs/guides/fine-tuning"" rel=""nofollow noreferrer"">https://platform.openai.com/docs/guides/fine-tuning</a></p>
<p>The documentation says that the accuracy metrics are calculated respect to the completion. But for the loss it is said that it is calculated &quot;on the training batch&quot;.</p>
<p>My understanding is that the first training of a GPT model always happen in batches of max available size, using an special token to separate contexts but always asking to predict the next token for all the entries. So here the loss function is the obvious cross entropy over all the outputs. But in fine tuning, there is the opportunity to learn to predict the &quot;template prompt&quot; or not. Both decisions can be sensible; learning the template amounts to train a parsing, masking the template can avoid overfitting.</p>
<p>So, what is the current practice in OpenAI?</p>
","gpt-3"
"75648132","OpenAI GPT-3 API: Why do I get only partial completion? Why is the completion cut off?","2023-03-06 07:29:41","75671537","4","5036","<python><openai-api><gpt-3>","<p>I tried the following code but got only partial results like</p>
<pre><code>[{&quot;light_id&quot;: 0, &quot;color
</code></pre>
<p>I was expecting the full JSON as suggested on this page:</p>
<p><a href=""https://medium.com/@richardhayes777/using-chatgpt-to-control-hue-lights-37729959d94f"" rel=""nofollow noreferrer"">https://medium.com/@richardhayes777/using-chatgpt-to-control-hue-lights-37729959d94f</a></p>
<pre><code>import json
import os
import time
from json import JSONDecodeError
from typing import List

import openai
openai.api_key =  &quot;xxx&quot;

HEADER = &quot;&quot;&quot;
I have a hue scale from 0 to 65535. 
red is 0.0
orange is 7281
yellow is 14563
purple is 50971
pink is 54612
green is 23665
blue is 43690

Saturation is from 0 to 254
Brightness is from 0 to 254

Two JSONs should be returned in a list. Each JSON should contain a color and a light_id. 
The light ids are 0 and 1. 
The color relates a key &quot;color&quot; to a dictionary with the keys &quot;hue&quot;, &quot;saturation&quot; and &quot;brightness&quot;. 

Give me a list of JSONs to configure the lights in response to the instructions below. 
Give only the JSON and no additional characters. 
Do not attempt to complete the instruction that I give.
Only give one JSON for each light. 
&quot;&quot;&quot;

completion = openai.Completion.create(model=&quot;text-davinci-003&quot;, prompt=HEADER)
print(completion.choices[0].text)
</code></pre>
","gpt-3"
"75640144","OpenAI converting API code from GPT-3 to chatGPT-3.5","2023-03-05 04:10:35","75739103","-3","1624","<php><openai-api><gpt-3><chatgpt-api>","<p>Below is my working code for the GPT-3 API. I am having trouble converting it to work with chatGPT-3.5.</p>
<pre><code>&lt;?php include('../config/config.php'); ?&gt;
&lt;!DOCTYPE html&gt;
&lt;html lang=&quot;en&quot;&gt;
&lt;head&gt;
&lt;meta charset=&quot;UTF-8&quot;&gt;
&lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt;
&lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;ie=edge&quot;&gt;
&lt;title&gt;Chatbot&lt;/title&gt;
&lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/bootstrap-icons@1.10.3/font/bootstrap-icons.css&quot;&gt;
&lt;link href=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/css/bootstrap.min.css&quot; rel=&quot;stylesheet&quot; integrity=&quot;sha384-GLhlTQ8iRABdZLl6O3oVMWSktQOp6b7In1Zl3/Jr59b6EGGoI1aFkw7cmDA6j6gD&quot; crossorigin=&quot;anonymous&quot;&gt;
&lt;link href=&quot;style.css&quot; rel=&quot;stylesheet&quot;&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;div class=&quot;container py-5&quot;&gt;
  &lt;h1 class=&quot;mb-5 text-center&quot;&gt;
    &lt;div class=&quot;logo&quot;&gt; &lt;img src=&quot;/images/Logo-PocketAI.svg&quot; height=&quot;80&quot; width=&quot;210&quot; aria-label=&quot;PocketAI.Online Logo&quot; title=&quot;PocketAI.Online Logo&quot; alt=&quot;SPocketAI.Online Logo&quot; class=&quot;img-fluid&quot;&gt; &lt;/div&gt;
  &lt;/h1&gt;
  &lt;div class=&quot;form-floating mb-3&quot;&gt;
    &lt;select class=&quot;form-select&quot; id=&quot;tab-select&quot; aria-label=&quot;Select your purpose&quot;&gt;
      &lt;option value=&quot;exam&quot; selected&gt;Exam&lt;/option&gt;
      &lt;option value=&quot;feedback&quot;&gt;Feedback&lt;/option&gt;
    &lt;/select&gt;
    &lt;label for=&quot;tab-select&quot;&gt;Select your purpose:&lt;/label&gt;
  &lt;/div&gt;
  &lt;div class=&quot;input-group mb-3&quot;&gt;
    &lt;div class=&quot;form-floating&quot;&gt;
      &lt;textarea class=&quot;form-control&quot; placeholder=&quot;Enter your question or comment here&quot; id=&quot;prompt&quot;&gt;&lt;/textarea&gt;
      &lt;label for=&quot;prompt&quot;&gt;Enter your question or comment here&lt;/label&gt;
    &lt;/div&gt;
    &lt;div class=&quot;input-group-append username w-100 mt-3 mb-4&quot;&gt;
      &lt;button class=&quot;btn btn-outline-primary w-100&quot; type=&quot;button&quot; id=&quot;send-button&quot;&gt;Send&lt;/button&gt;
    &lt;/div&gt;
  &lt;/div&gt;
  &lt;div id=&quot;output&quot; class=&quot;mb-3&quot; style=&quot;height: 300px; overflow: auto; border: 1px solid lightgray; padding: 10px;&quot;&gt;&lt;/div&gt;
  &lt;div id=&quot;exam-instructions&quot; class=&quot;mb-3&quot; style=&quot;display: block;&quot;&gt;
    &lt;h3&gt;Exam&lt;/h3&gt;
    &lt;p&gt;PocketAI can create multiple choice and true false questions in a format that enables import into Brightspace D2L quizzes using Respondus. Place PocketAI output into a Word document before importing with Respondus. Ask PocketAI questions like the following: &lt;br&gt;
      &lt;br&gt;
      Create 3 multiple choice questions about carbohydrates for a freshman Nutrition online college course.&lt;br&gt;
      Create 2 true false questions about business for a sophomore Business face to face college course.&lt;/p&gt;
  &lt;/div&gt;
  &lt;div id=&quot;feedback-instructions&quot; class=&quot;mb-3&quot; style=&quot;display: none;&quot;&gt;
    &lt;h3&gt;Feedback&lt;/h3&gt;
    &lt;p&gt;Enter text to receive writing feedback.&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;script&gt;
const previousPrompts = [];
const userName = &quot;&lt;strong&gt;User&lt;/strong&gt;&quot;;
const chatbotName = &quot;&lt;strong&gt;PocketAI&lt;/strong&gt;&quot;;

const selectDropdown = document.getElementById(&quot;tab-select&quot;);

selectDropdown.addEventListener(&quot;change&quot;, function() {
  const activeTabId = this.value;
  
  // hide all instruction sections
  document.querySelectorAll(&quot;[id$='-instructions']&quot;).forEach(function(instructionSection) {
    instructionSection.style.display = &quot;none&quot;;
  });
  
  // show the instruction section for the active tab
  document.getElementById(`${activeTabId}-instructions`).style.display = &quot;block&quot;;
});

document.getElementById(&quot;send-button&quot;).addEventListener(&quot;click&quot;, function() {
  const prompt = document.getElementById(&quot;prompt&quot;).value;
  const activeTabId = selectDropdown.value;

  const endpoint = &quot;https://api.openai.com/v1/completions&quot;;
  const apiKey = &quot;&lt;?=$OPEN_AI_KEY;?&gt;&quot;;

  document.getElementById(&quot;send-button&quot;).innerHTML = '&lt;span class=&quot;spinner-border spinner-border-sm&quot; role=&quot;status&quot; aria-hidden=&quot;true&quot;&gt;&lt;/span&gt; Sending...';

  let promptText = &quot;&quot;;
  
  switch (activeTabId) {
    case &quot;exam&quot;:
        promptText = &quot;Create quiz questions in the following format: Begin each question with a number followed by a period, and then include the question wording. For each question, include four answer choices listed as letters (A, B, C, D) followed by a period and at least one space before the answer wording. Designate the correct answer by placing an asterisk (*) directly in front of the answer letter (do not put a space between the asterisk and the answer choice). Place the asterisk in front of the answer letter, only the front. It is important that correct answers are identified. Don't make up answers, only select factual answers. For example formatting (don't use this specific example), \&quot;1. What is the recommended daily intake of dietary fiber? A. 10 grams B. 25 grams *C. 50 grams D. 75 grams\&quot;. Format true false questions the same way. If you are unsure of the correct answer, don't create the question. Every quiz question and answer must be 100% correct and factual. Do not make up answers. All answers must be correct.&quot;;
      break;
     case &quot;feedback&quot;:
      promptText = &quot;Can you provide feedback on the writing, grammar, sentence structure, punctuation, and style of this student's paper? The paper should be analyzed for its strengths and weaknesses in terms of written communication. Please provide suggestions for improvement and examples to help the student understand how to make the writing better. The feedback should be specific and provide actionable steps that the student can take to improve their writing skills. Please include at least three examples of areas that could be improved and specific suggestions for how to improve them, such as correcting grammar errors, restructuring sentences, or improving the use of punctuation.&quot;;
      break;
  }
  
  const requestData = {
    prompt: previousPrompts.join(&quot;\n&quot;) + promptText + &quot;\n&quot; + prompt,
    max_tokens: 400,
      model: &quot;text-davinci-003&quot;,
    n: 1,
    stop: &quot;&quot;,
    temperature: 0.5,
      top_p: 0.0,
      frequency_penalty: 0.0,
      presence_penalty: 0
  };
        
  const requestOptions = {
    method: &quot;POST&quot;,
    headers: {
      &quot;Content-Type&quot;: &quot;application/json&quot;,
      &quot;Authorization&quot;: `Bearer ${apiKey}`,
    },
    body: JSON.stringify(requestData),
  };
  
  fetch(endpoint, requestOptions)
    .then(response =&gt; response.json())
    .then(data =&gt; {
      const reply = data.choices[0].text;
      
      // Add the user message to the chat history
      const userMessage = `&lt;div class=&quot;message-container&quot;&gt;
        &lt;div class=&quot;username&quot;&gt;${userName}:&amp;nbsp;&lt;/div&gt;
        &lt;div class=&quot;user-message&quot;&gt;${prompt}&lt;/div&gt;
      &lt;/div&gt;`;
      document.getElementById(&quot;output&quot;).innerHTML += userMessage;
      
      const chatbotMessage = `&lt;div class=&quot;message-container&quot;&gt;
  &lt;div class=&quot;username&quot;&gt;${chatbotName}:&amp;nbsp;&lt;/div&gt;
  &lt;div class=&quot;chatbot-message&quot; style=&quot;white-space: pre-wrap&quot;&gt;${reply}&lt;i class=&quot;bi bi-clipboard-check copy-button&quot; data-bs-toggle=&quot;tooltip&quot; data-bs-placement=&quot;bottom&quot; title=&quot;Copy to clipboard&quot; data-text=&quot;${reply}&quot; style=&quot;cursor: pointer;&quot;&gt;&lt;/i&gt;&lt;/div&gt;
&lt;/div&gt;`; 
document.getElementById(&quot;output&quot;).innerHTML += chatbotMessage;

// Add an event listener to each &quot;Copy to Clipboard&quot; button
document.addEventListener(&quot;click&quot;, function(event) {
  if (event.target.classList.contains(&quot;copy-button&quot;)) {
    const textToCopy = event.target.dataset.text;
    navigator.clipboard.writeText(textToCopy);
  }
});
     // Scroll to the bottom of the chat history
      document.getElementById(&quot;output&quot;).scrollTop = document.getElementById(&quot;output&quot;).scrollHeight;
    
      // Clear the user input field
      document.getElementById(&quot;prompt&quot;).value = &quot;&quot;;
    
      previousPrompts.push(prompt);
      // Clear the spinner and show the &quot;Send&quot; button again
      document.getElementById(&quot;send-button&quot;).innerHTML = 'Send';
    })
    .catch(error =&gt; {
      console.error(error);
    
      // Hide the spinner and show the &quot;Send&quot; button again
      document.getElementById(&quot;send-button&quot;).innerHTML = 'Send';
    });
});

document.getElementById(&quot;prompt&quot;).addEventListener(&quot;keydown&quot;, function(event) {
  if (event.keyCode === 13) {
    event.preventDefault();
    document.getElementById(&quot;send-button&quot;).
click();
  }
});
&lt;/script&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;script src=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/js/bootstrap.bundle.min.js&quot; integrity=&quot;sha384-w76AqPfDkMBDXo30jS1Sgez6pr3x5MlQ1ZAGC+nuZB+EYdgRZgiwxhTBTkF7CXvN&quot; crossorigin=&quot;anonymous&quot;&gt;&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre>
<p>I have read <a href=""https://openai.com/blog/introducing-chatgpt-and-whisper-apis"" rel=""nofollow noreferrer"">https://openai.com/blog/introducing-chatgpt-and-whisper-apis</a> and referred to this - <a href=""https://stackoverflow.com/questions/75613656/openai-chatgpt-gpt-3-5-turbo-api-how-to-access-the-message-content"">OpenAI ChatGPT (gpt-3.5-turbo) API: How to access the message content?</a> but still can't make it work.</p>
<p>I've tried changing the requestData to this, but no luck:</p>
<pre><code>const requestData = {
    model: &quot;gpt-3.5-turbo&quot;,
    messages: [
      { role: &quot;user&quot;, content: prompt }
    ],
    max_tokens: 400,
    temperature: 0.5,
    top_p: 1,
    frequency_penalty: 0,
    presence_penalty: 0
  };
</code></pre>
<p>Any help will be greatly appreciated!</p>
","gpt-3"
"75624308","OpenAI GPT-3 API errors: 'text' does not exist TS(2339) & 'prompt' does not exist on type 'CreateChatCompletion' TS(2345)","2023-03-03 07:34:41","75626340","0","1985","<next.js><openai-api><gpt-3>","<pre><code>import openai from &quot;./zggpt&quot;;

const query = async (prompt:string,  chatId:string, model:string) =&gt; {
    const res= await openai
    .createChatCompletion({
        model,
        prompt,
        temperature: 0.9,
        
        top_p:1,
       
        max_tokens:1000,
        frequency_penalty:0,
        presence_penalty:0,
    })
    .then((res) =&gt; res.data.choices[0].text)
    .catch((err)=&gt;
    `ZG was unable to find an answer for that!
     (Error: ${err.message})`
     );
     return res;
};

export default query;
</code></pre>
<p>Property 'text' does not exist on type 'CreateChatCompletionResponseChoicesInner'.ts(2339)</p>
<p>Argument of type '{ model: string; prompt: string; temperature: number; top_p: number; max_tokens: number; frequency_penalty: number; presence_penalty: number; }' is not assignable to parameter of type 'CreateChatCompletionRequest'.
Object literal may only specify known properties, and 'prompt' does not exist in type 'CreateChatCompletionRequest'.ts(2345)</p>
<p>even though I do everything as in the video, I get these errors.</p>
<p>i'm a beginner in coding, so I'm trying to make applications based on videos to learn.</p>
<p>Thanks</p>
<p>the application responds without returning an error.
<a href=""https://i.sstatic.net/LmkT3.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p><a href=""https://www.youtube.com/watch?v=V6Hq_EX2LLM&amp;t=6293s"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=V6Hq_EX2LLM&amp;t=6293s</a></p>
","gpt-3"
"75621041","How can i migrate from text-davinci-003 model to gpt-3.5-turbo model using OpenAI API?","2023-03-02 21:35:36","","2","1185","<android><kotlin><openai-api><gpt-3><chatgpt-api>","<p>I tried to change my code to be able to use the new OpenAI model but my application stops working,</p>
<p>BEFORE: In Bold are parts of the code that I changed and where working using text-davinci-003 model</p>
<pre><code>**var url = &quot;https://api.openai.com/v1/completions&quot;**

private fun getResponse(query: String) {

        val queue: RequestQueue = Volley.newRequestQueue(applicationContext)

        val jsonObject: JSONObject? = JSONObject()
**
        jsonObject?.put(&quot;model&quot;, &quot;text-davinci-003&quot;)**
        jsonObject?.put(&quot;messages&quot;, messagesArray)
        jsonObject?.put(&quot;temperature&quot;, 0)
        jsonObject?.put(&quot;max_tokens&quot;, 100)
        jsonObject?.put(&quot;top_p&quot;, 1)
        jsonObject?.put(&quot;frequency_penalty&quot;, 0.0)
        jsonObject?.put(&quot;presence_penalty&quot;, 0.0)

        val postRequest: JsonObjectRequest =

            object : JsonObjectRequest(Method.POST, url, jsonObject,
                Response.Listener { response -&gt;

                    val responseMsg: String =
                        response.getJSONArray(&quot;choices&quot;).getJSONObject(0).getString(&quot;text&quot;)
                },

                Response.ErrorListener { error -&gt;
                    Log.e(&quot;TAGAPI&quot;, &quot;Error is : &quot; + error.message + &quot;\n&quot; + error)
                }) {
                override fun getHeaders(): kotlin.collections.MutableMap&lt;kotlin.String, kotlin.String&gt; {
                    val params: MutableMap&lt;String, String&gt; = HashMap()
                    // adding headers on below line.
                    params[&quot;Content-Type&quot;] = &quot;application/json&quot;
                    params[&quot;Authorization&quot;] =
                        &quot;Bearer MYTOKEN&quot;
                    return params;
                }
            }
</code></pre>
<p>AFTER: In Bold are parts of the code that I changed and arent working with gpt-3.5-turbo model</p>
<pre><code>var url = &quot;https://api.openai.com/v1/chat/completions&quot;

private fun getResponse(query: String) {

        val queue: RequestQueue = Volley.newRequestQueue(applicationContext)

        val jsonObject: JSONObject? = JSONObject()

        // start changes
        jsonObject?.put(&quot;model&quot;, &quot;gpt-3.5-turbo&quot;); 
        val messagesArray = JSONArray()
        val messageObject1 = JSONObject()
        messageObject1.put(&quot;role&quot;, &quot;user&quot;)
        messageObject1.put(&quot;content&quot;, query)
        messagesArray.put(messageObject1)
        jsonObject?.put(&quot;messages&quot;, messagesArray)
        // end changes

        jsonObject?.put(&quot;temperature&quot;, 0)
        jsonObject?.put(&quot;max_tokens&quot;, 100)
        jsonObject?.put(&quot;top_p&quot;, 1)
        jsonObject?.put(&quot;frequency_penalty&quot;, 0.0)
        jsonObject?.put(&quot;presence_penalty&quot;, 0.0)

        val postRequest: JsonObjectRequest 

            object : JsonObjectRequest(Method.POST, url, jsonObject,
                Response.Listener { response -&gt;

                    val responseMsg: String =
                        response.getJSONArray(&quot;choices&quot;).getJSONObject(0).getString(&quot;text&quot;)

                Response.ErrorListener { error -&gt;
                    Log.e(&quot;TAGAPI&quot;, &quot;Error is : &quot; + error.message + &quot;\n&quot; + error)
                }) {
                override fun getHeaders(): kotlin.collections.MutableMap&lt;kotlin.String, kotlin.String&gt; {
                    val params: MutableMap&lt;String, String&gt; = HashMap()

                    params[&quot;Content-Type&quot;] = &quot;application/json&quot;
                    params[&quot;Authorization&quot;] =
                        &quot;Bearer MYTOKEN&quot;
                    return params;
                }
            }
</code></pre>
<p>I only changed the parts that are in bold and now it does nto work, the application stops.</p>
","gpt-3"
"75586733","ChatGPT Token Limit","2023-02-28 00:06:49","","21","29353","<text><nlp><prompt><openai-api><gpt-3>","<p>I want ChatGPT to remember past conversations and have a consistent (stateful) conversation.</p>
<p>I have seen several code of ChatGPT prompt engineering.</p>
<p>There were two ways to design the prompt shown below (pseudo code):</p>
<ol>
<li><p><strong>Use a single input</strong> (Cheap) &lt;- Better if possible</p>
</li>
<li><p><strong>Stack all of previous history</strong> (Expensive, Token Limitation)</p>
</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>def openai_chat(prompt):
    completions = openai.Completion.create(
        engine = &quot;text-davinci-003&quot;,
        prompt = prompt,
        max_tokens = 1024,
        n = 1,
        temperature = 0.8,
    )
    response = completions.choices[0].text.strip()
    return response

# 1. Use a single input
while True:
    prompt = input(&quot;User: &quot;)
    completion = openai_chat(prompt)

# 2. Stack all of previous history (prompt + completion)
prompt = &quot;&quot;
while True:
    cur_prompt = input(&quot;User: &quot;)
    prompt += cur_prompt  # pseudo code
    completion = openai_chat(prompt)
    prompt += completion  # pseudo code
</code></pre>
<p>Is it possible to choose the first way (the cheap one) to have a consistent conversation?</p>
<p>In other words, does ChatGPT remember past history even if the prompt only has the current input?</p>
","gpt-3"
"75567331","OpenAI GPT-3 API error: ""You must provide a model parameter""","2023-02-25 17:52:50","75571367","2","4362","<swift><openai-api><gpt-3>","<p>I am trying to POST a question to openAI API via SWIFT. It works fine, if I use the same payload via Postman, but in the Xcode-Condole I got the following response from openAI:</p>
<pre><code>Response data string:
{
     &quot;error&quot;: {
         &quot;message&quot;: &quot;you must provide a model parameter&quot;,
         &quot;type&quot;: &quot;invalid_request_error&quot;,
         &quot;param&quot;: null,
         &quot;code&quot;: null
    }
 }
</code></pre>
<p>This is my code:</p>
<pre><code> func getActivityAnalysis(){
    
    let url = URL(string: &quot;https://api.openai.com/v1/completions&quot;)
    guard let requestUrl = url else { fatalError() }
    
    // Prepare URL Request Object
    var request = URLRequest(url: requestUrl)
    request.setValue(&quot;Bearer blaaaablaa&quot;, forHTTPHeaderField: &quot;Authorization&quot;)
    request.httpMethod = &quot;POST&quot;
    
    
    let prompt = &quot;just a test&quot;
    let requestBody = OpenAIRequest(model: &quot;text-davinci-003&quot;, prompt: prompt, max_tokens: 300, temperature: 0.5)
    
    let encoder = JSONEncoder()
    encoder.outputFormatting = .prettyPrinted
    let data = try! encoder.encode(requestBody)
    print(String(data: data, encoding: .utf8)!)
    
     
    // Set HTTP Request Body
    request.httpBody = data
    
    print(&quot;\(request.httpMethod!) \(request.url!)&quot;)
    print(request.allHTTPHeaderFields!)
    print(String(data: request.httpBody ?? Data(), encoding: .utf8)!)
    
    
    
    // Perform HTTP Request
    let task = URLSession.shared.dataTask(with: request) { (data, response, error) in
            
            // Check for Error
            if let error = error {
                print(&quot;Error took place \(error)&quot;)
                return
            }
     
            // Convert HTTP Response Data to a String
            if let data = data, let dataString = String(data: data, encoding: .utf8) {
                print(&quot;Response data string:\n \(dataString)&quot;)
                self.openAIResponse = dataString
            }
    }
    task.resume()
    
}`
</code></pre>
<p>If I print the http request, it seems fine for me as well:</p>
<pre><code> POST https://api.openai.com/v1/completions
 [&quot;Authorization&quot;: &quot;Bearer blaaaaa&quot;]
 {
    &quot;temperature&quot; : 0.5,
    &quot;model&quot; : &quot;text-davinci-003&quot;,
    &quot;prompt&quot; : &quot;just a test&quot;,
    &quot;max_tokens&quot; : 300
 }
</code></pre>
<p>I tried to use the same payload in my Postman request. It worked fine here. I also tried to use different encodings, but it always throws the same error.</p>
<p>Not sure, what I am doing wrong. Maybe someone can help?</p>
<p>Thank you in advance.</p>
<p>Bets,
Tobi</p>
","gpt-3"
"75559672","OpenAI GPT-3 API: Which file formats can be used for fine-tuning?","2023-02-24 17:32:30","","1","1556","<python><openai-api><gpt-3><fine-tuning>","<p>As we are getting in to turbulent times of AI.
I am as well spilling mine drop in to ocean.
As I am pythonian, all attempts are done in python/anaconda.</p>
<p>Does anybody have already some experience in &quot;data formats&quot; passable to GPT family of AIs?</p>
<p>In documentation is recommended use of OpenAI tool for control.
Followed by documentation recommending format (&quot;Prompt:&quot;, &quot;Completion:&quot;)
With strings marked as:</p>
<pre><code>  [&quot;str&quot; = in quotes,&quot;/&quot; = separator ,&quot;@&gt;&quot; = unique symbol, 
   &quot; &quot; = comp. starts with empty space]

  'Prompt':    'Hello AI..!!/@&gt;' 
  'Completion': ' How are you today?/@&gt;' 
</code></pre>
<p>&quot;Completion&quot; should have <strong>empty space</strong> at start of every sting.
So far I was able to find just <strong>simple examples</strong> as:</p>
<pre><code>Col1             Col2
'Prompt':        'Completion':
'Text/@&gt;'        ' Text/@&gt;'
</code></pre>
<p>Is there any way it will understand more complex dataset?
Is effective to have more dim. DataFrame?
<strong>Example:</strong></p>
<pre><code>     Col1        Col2             Col3         Col4        
    'Prompt_a':  'Completion_a':  'Prompt_b':  'Completion_b':
    'Text/@&gt;'    ' Text/@&gt;'       'Text/@&gt;'    ' Text/@&gt;
</code></pre>
<p>Is longer context text passed just as 'str/@&gt;', or is some partition needed?</p>
<pre><code>' text text text /@&gt;'
</code></pre>
<p>Many thanks for all answers and efforts in advance.</p>
<p>Already checked: <a href=""https://help.openai.com/en/articles/6811186-how-do-i-format-my-fine-tuning-data"" rel=""nofollow noreferrer"">https://help.openai.com/en/articles/6811186-how-do-i-format-my-fine-tuning-data</a></p>
","gpt-3"
"75559316","Using sliding windows to evaluate sentiment for text groupings","2023-02-24 16:54:29","","0","76","<python><data-science><openai-api><gpt-3>","<p>I am trying to create a sliding window to evaluate sentiment on groupings of utterances within a conversation, with the goal of:</p>
<ol>
<li>Evaluating the sentiment of a single utterance in a conversational grouping</li>
<li>Evaluating a grouping of sentiment based on the statement from item 1 above and then adding a new utterance string to the predictor (the next utterance in the conversation) so that the predictor evaluates the previous string in context with the new string.  Note that the individual additive statement in this step would also receive a sentiment score</li>
<li>Repeating item 1 and 2 by adding a new utterance string to data to be evaluated (wherein the new 3rd string utterance gets evaluated but also is evaluated in the context of the previous 2 utterances - such that now there are three utterances to be evaluated in addition to the individual newly added string.<br />
For example:</li>
</ol>
<pre><code>Statement 1: Neutral
Statement 2:  Positive
Statement 1+2:  Neutral
Statement 3:  Negative
Statement 1+2+3:  Neutral
etc... 
</code></pre>
<p>I could then transform the classifications to integer values rather simply, and then come up with the mean of entire statement grouping for a final &quot;conversation&quot; sentiment classification.</p>
<p>Here is my list of utterances:</p>
<pre><code>conversation = [
&quot;Hi, how are you?&quot;,
&quot;I'm not doing very well, thanks for asking. How about you?&quot;,
&quot;It is the best of times and the worst of times.&quot;,
&quot;I'm not sure what to make of that.&quot;,
&quot;Do you have any plans for the weekend?&quot;,
&quot;Not yet, I'm still deciding.&quot;,
&quot;How about you?&quot;,
&quot;I'm planning to go hiking on Saturday.&quot;]
</code></pre>
<p>Here is my routine -</p>
<pre><code>#Define the size of the sliding window
window_size = 5
sentiment_scores = []
for i in range(len(conversation) - window_size + 1):
    # Get the window of utterances
    window = conversation[i:i+window_size]
    print(&quot;this is the conversation with window&quot;,conversation[i:i+window_size])
    
    # Add one or two utterances from the previous window to the beginning of the current window
    if i &gt; 0:
        window = conversation[i-1:i+window_size]
        
    
    # Add one or two utterances from the next window to the end of the current window
    if i &lt; len(conversation) - window_size:
        window = conversation[i:i+window_size+1]
        print(&quot;This is the  window when a conversation has been added&quot;, window)

    # Join the utterances in the window into a single string
    text = &quot; &quot;.join(window)
    # print(text)

    # Use the OpenAI completion class to evaluate sentiment for the window
    response = openai.Completion.create(
        engine=&quot;text-davinci-003&quot;,
        prompt = f&quot;classify the sentiment of this text as Positive, Negative, or Neutral: {text}\nResult:&quot;,
        temperature=0,
        max_tokens=1,
        n=1,
        stop=None,
        frequency_penalty=0,
        presence_penalty=0
    )

    # Extract the sentiment score from the response
    sentiment = response.choices[0].text.strip()
    print(sentiment)

    # Add the sentiment score to the list
    sentiment_scores.append(sentiment)
    print(sentiment_scores)

</code></pre>
<p>Unfortunately, what is being returned is not correct because I am apparently not layering in the utterances the way I am describing above.  Using one of my debug prints, this is what I am seeing:</p>
<pre><code>This is the  window when a conversation has been added ['Hi, how are you?', &quot;I'm not doing very well, thanks for asking. How about you?&quot;, 'It is the best of times and the worst of times.', &quot;I'm not sure what to make of that.&quot;, 'Do you have any plans for the weekend?', &quot;Not yet, I'm still deciding.&quot;]
Neutral
['Neutral']
This is the  window when a conversation has been added [&quot;I'm not doing very well, thanks for asking. How about you?&quot;, 'It is the best of times and the worst of times.', &quot;I'm not sure what to make of that.&quot;, 'Do you have any plans for the weekend?', &quot;Not yet, I'm still deciding.&quot;, 'How about you?']
Neutral
['Neutral', 'Neutral']
This is the  window when a conversation has been added ['It is the best of times and the worst of times.', &quot;I'm not sure what to make of that.&quot;, 'Do you have any plans for the weekend?', &quot;Not yet, I'm still deciding.&quot;, 'How about you?', &quot;I'm planning to go hiking on Saturday.&quot;]
Neutral
['Neutral', 'Neutral', 'Neutral']
This is the  window when a conversation has been added ['It is the best of times and the worst of times.', &quot;I'm not sure what to make of that.&quot;, 'Do you have any plans for the weekend?', &quot;Not yet, I'm still deciding.&quot;, 'How about you?', &quot;I'm planning to go hiking on Saturday.&quot;]
Neutral
['Neutral', 'Neutral', 'Neutral', 'Neutral']
</code></pre>
<p>As you can see, it appears my logic is evaluating all statements.</p>
<p>I could then transform the classifications to integer values rather simply, and then come up with the mean of entire statement grouping for a final &quot;conversation&quot; sentiment classification.</p>
<p>Any thoughts or assistance would be greatly appreciated.</p>
","gpt-3"
"75549286","GPT Index: Issue using ComposableGraph with Vector Stores","2023-02-23 19:06:18","","0","863","<python><openai-api><gpt-3>","<p>I am having issue with implementing Vector Stores with composability</p>
<pre><code>from llama_index.composability import ComposableGraph
index1 = GPTQdrantIndex(doc1, client=qdrant_client,collection_name=&quot;index1&quot;)
index1.set_text(&quot;S3document1&quot;)
index2 = GPTQdrantIndex(doc2, client=qdrant_client,collection_name=&quot;index2&quot;)
index2.set_text(&quot;S3document2&quot;)
# save index to disk
index1.save_to_disk('index_Qdrant1.json')
index2.save_to_disk('index_Qdrant2.json')

list_index2 = GPTListIndex([index1, index2]);

graph = ComposableGraph.build_from_index(list_index2)
graph.save_to_disk(&quot;save_path2.json&quot;)
graph = ComposableGraph.load_from_disk(&quot;save_path2.json&quot;)

query_configs = [
    {
        &quot;index_struct_type&quot;: &quot;qdrant&quot;,
        &quot;query_mode&quot;: &quot;default&quot;   
    },
    {
        &quot;index_struct_type&quot;: &quot;keyword_table&quot;,
        &quot;query_mode&quot;: &quot;simple&quot;,
        &quot;query_kwargs&quot;: {}
    },
]

response = graph.query(&quot;Who is this&quot;, query_configs=query_configs);
print(response)
</code></pre>
<p>Error
<a href=""https://i.sstatic.net/l3rh9.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/l3rh9.png"" alt=""enter image description here"" /></a></p>
<p>I have tried ComposableGraph with other Vector Stores as well but didn't worked</p>
","gpt-3"
"75540828","Chat bot not sending message to API","2023-02-23 05:18:36","","0","441","<javascript><html><openai-api><gpt-3>","<p>I asked chat gpt to write a chat bot code modeled on gpt3, and he actually wrote it.</p>
<p>The site was created, but messages could not be sent.</p>
<p>I also got a gpt3 api key and used it, but it doesn't seem to work well. What's the problem?</p>
<p>Below is the code written by gpt</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
  &lt;meta charset=""utf-8""&gt;
  &lt;title&gt;ChatGPT Demo&lt;/title&gt;
  &lt;link rel=""stylesheet"" href=""style.css""&gt;
&lt;/head&gt;
&lt;body&gt;
  &lt;div class=""chat-window""&gt;
    &lt;div class=""chat-header""&gt;
      &lt;h1&gt;ChatGPT&lt;/h1&gt;
    &lt;/div&gt;
    &lt;div class=""chat-body""&gt;
      &lt;ul class=""message-list""&gt;
        &lt;li class=""message bot""&gt;
          &lt;p&gt;Hello! How can I help you today?&lt;/p&gt;
        &lt;/li&gt;
      &lt;/ul&gt;
    &lt;/div&gt;
    &lt;div class=""chat-footer""&gt;
      &lt;input id=""input"" type=""text"" placeholder=""Type your message here...""&gt;
      &lt;button onclick=""send()""&gt;Send&lt;/button&gt;
    &lt;/div&gt;
  &lt;/div&gt;
  &lt;script src=""https://cdn.jsdelivr.net/npm/@openai/api""&gt;&lt;/script&gt;
  &lt;script&gt;
    // OpenAI API Key ÏÑ§Ï†ï
    const openai = window.openai;
    const api_key = 'YOUR_API_KEY';
    const model_engine = 'davinci';

    // API Ìò∏Ï∂úÌïòÏó¨ ÏùëÎãµ Î∞õÍ∏∞
    const askGPT3 = async (input) =&gt; {
      console.log(input); // This is getting logged but below API is not being called.
      const response = await openai.Completion.create({
        engine: model_engine,
        prompt: input,
        max_tokens: 1024,
        n: 1,
        stop: null,
        temperature: 0.5,
        apiKey: api_key
      });
      return response.choices[0].text.trim();
    };

    // ÎåÄÌôî ÏãúÏûë
    const startConversation = async () =&gt; {
      const botMessage = document.querySelector('.message.bot p');
      const answer = await askGPT3('Hello!');
      botMessage.innerHTML = answer;
    };
    startConversation();

    // ÎåÄÌôî Ï†ÑÏÜ°
    const send = async () =&gt; {
      const input = document.getElementById('input').value;
      const messageList = document.querySelector('.message-list');
      const userMessage = `&lt;li class=""message user""&gt;&lt;p&gt;${input}&lt;/p&gt;&lt;/li&gt;`;
      messageList.insertAdjacentHTML('beforeend', userMessage);
      const botMessage = `&lt;li class=""message bot""&gt;&lt;p&gt;${await askGPT3(input)}&lt;/p&gt;&lt;/li&gt;`;
      messageList.insertAdjacentHTML('beforeend', botMessage);
      document.getElementById('input').value = '';
    };

    // ÎåÄÌôî ÏóîÌÑ∞ÌÇ§ Ï†ÑÏÜ°
    const input = document.getElementById('input');
    input.addEventListener('keyup', (event) =&gt; {
      if (event.keyCode === 13) {
        event.preventDefault();
        document.querySelector('button').click();
      }
    });
  &lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;</code></pre>
</div>
</div>
</p>
<p>I clicked send to send a message but it doesn't work - the message is not sent to the API.</p>
","gpt-3"
"75529578","Stream interrupted (client disconnected). To resume the stream, run: openai api fine_tunes.follow -i ft-***, while fine tuning openai","2023-02-22 07:48:04","76827563","1","1186","<python><openai-api><gpt-3>","<p>(Ctrl-C will interrupt the stream, but not cancel the fine-tune)
[2023-02-22 07:37:41] Created fine-tune: ft-LaoFNTDDUfQbmxTHkViCcuc2</p>
<p>Stream interrupted (client disconnected).
To resume the stream, run:</p>
<p>openai api fine_tunes.follow -i ft-LaoFNTDDUfQbmxTHkViCcuc2</p>
<p>What's the issue I used two version of OpenAi 0.25.0 and 0.26.5
For both the version I am getting the same error</p>
<p>Can anyone help solve the above problem</p>
","gpt-3"
"75501276","OpenAI GPT-3 API: How to make a model remember past conversations?","2023-02-19 15:38:07","","5","6287","<openai-api><gpt-3>","<p>Is there a way to train a <em>Large Language Model</em> (LLM) to store a specific context? For example, I had a long story I want to ask questions about, but I don't want to put the whole story in every prompt. How can I make the LLM &quot;remember the story&quot;?</p>
","gpt-3"
"75497540","Does Visual Studio Code support Environment Variables for the ChatGPT extension?","2023-02-19 01:55:50","","0","393","<security><visual-studio-code><openai-api><gpt-3>","<p>I would like to open the editor and be able to use the ChatGPT extension without needing to log in via browser or store the OpenAI api key in a workspace file. Ideally, I would be able to use the environment variable stored in <code>.zshenv</code>.</p>
<p>I know that Visual Studio Code allows for some environment variables to be set but not all via <code>${ENV_EXAMPLE}</code> but this has not worked for me in the <code>settings.json</code> file:</p>
<pre class=""lang-json prettyprint-override""><code>{
  &quot;chatgpt.response.showNotification&quot;: true,
  &quot;chatgpt.gpt3.maxTokens&quot;: 2048,
  &quot;chatgpt.gpt3.apiKey&quot;: &quot;${OPEN_AI_API_KEY}&quot;,
}

</code></pre>
","gpt-3"
"75494945","OpenAi api 429 rate limit error without reaching rate limit","2023-02-18 17:05:02","","7","12514","<openai-api><gpt-3>","<p>On occasion I'm getting a rate limit error without being over my rate limit. I'm using the text completions endpoint on the paid api which has a rate limit of 3,000 requests per minute. I am using at most 3-4 requests per minute.</p>
<p>Sometimes I will get the following error from the api:</p>
<ul>
<li>Status Code: <code>429</code> (Too Many Requests)</li>
<li>Open Ai error type: <code>server_error</code></li>
<li>Open Ai error message: <code>That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists.</code></li>
</ul>
<p>Open ai documentation states that a 429 error indicates that you have exceeded your rate limit which clearly I have not.
<a href=""https://help.openai.com/en/articles/6891829-error-code-429-rate-limit-reached-for-requests"" rel=""noreferrer"">https://help.openai.com/en/articles/6891829-error-code-429-rate-limit-reached-for-requests</a></p>
<p>The weird thing is the open ai error message is not stating that. It is giving the response I usually get from a <code>503</code> error (service unavailable).</p>
<p>I'd love to hear some thoughts on this, any theories, or if anyone else has been experiencing this.</p>
","gpt-3"
"75469128","How does Huggingface's tokenizers tokenize non-English characters?","2023-02-16 07:39:04","","3","236","<nlp><tokenize><huggingface-tokenizers><gpt-3><gpt-2>","<p>I use <a href=""https://github.com/huggingface/tokenizers"" rel=""nofollow noreferrer""><code>tokenizers</code></a> to tokenize natural language sentences into tokens.</p>
<p>But came up with some questions:</p>
<p>Here is some examples I tried using tokenizers:</p>
<pre><code>from transformers import GPT2TokenizerFast
tokenizer = GPT2TokenizerFast.from_pretrained(&quot;gpt2&quot;)
tokenizer(&quot;ÊòØ&quot;)
# {'input_ids': [42468], 'attention_mask': [1]}
tokenizer(&quot;ÊàëËØ¥‰Ω†ÂÄíÊòØÂø´ÁÇπÂïä&quot;)
# {'input_ids': [22755, 239, 46237, 112, 19526, 254, 161, 222, 240, 42468, 33232, 104, 163, 224, 117, 161, 243, 232], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
tokenizer(&quot;Êù±&quot;)
# {'input_ids': [30266, 109], 'attention_mask': [1, 1]}
tokenizer(&quot;Êù±‰∫¨&quot;)
# {'input_ids': [30266, 109, 12859, 105], 'attention_mask': [1, 1, 1, 1]}
tokenizer(&quot;Êù±‰∫¨„É°„Éà„É≠&quot;)
# {'input_ids': [30266, 109, 12859, 105, 26998, 13298, 16253], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}
tokenizer(&quot;„É°„Éà„É≠&quot;)
# {'input_ids': [26998, 13298, 16253], 'attention_mask': [1, 1, 1]}
tokenizer(&quot;This is my fault&quot;)
{'input_ids': [1212, 318, 616, 8046], 'attention_mask': [1, 1, 1, 1]}
</code></pre>
<p>The code above is some examples I tried.
The last example is an English sentence and I can understand that <code>This</code> corresponds to <code>&quot;This&quot;:1212</code> in the <code>vocab.json</code>, <code> is</code> corresponds to <code>&quot;\u0120is&quot;: 318</code>.</p>
<p>But I can not understand why this tool tokenizes non-English sequence into some tokens I can not find in the vocab.
For example:
<code>Êù±</code> is been tokenized into <code>30266 and 109</code>. The results in the <code>vocab.json</code> is <code>&quot;√¶ƒø&quot;:30266</code> and <code>&quot;¬±&quot;:109</code>.
<code>„É°</code> is been tokenized into <code>26998</code>. The results in the <code>vocab.json</code> is <code>&quot;√£ƒ•¬°&quot;:26998</code>.</p>
<p>I searched the Huggingface documents and website and find no clue.</p>
<p>And the source code is written in Rust, which is hard for me to understand.
So could you help me figure out why?</p>
","gpt-3"
"75454722","OpenAI GPT-3 API: How does it count tokens for different languages?","2023-02-15 01:26:42","","3","3982","<nlp><tokenize><openai-api><gpt-3>","<p>We all know that GPT-3 models can accept and produce all kinds of languages such as English, French, Chinese, Japanese and so on.</p>
<p>In traditional NLP, different languages have different token-making methods.</p>
<ul>
<li>For those Alphabetic Languages such as English, <code>Bert</code> use BPE method to make tokens like below:</li>
</ul>
<pre><code>Insomnia caused much frustration.
==&gt;
In-, som-, nia, caus-, ed, much, frus-, tra-, tion, .,
</code></pre>
<ul>
<li>For those Charactaristic Languages such as Chinese or Japanese, just use the character itself as the token like below.</li>
</ul>
<pre><code>Êù±‰∫¨„É°„Éà„É≠„ÅØÂøÉ„Å´ÂØÑ„ÇäÊ∑ª„ÅÜ
==&gt;
Êù±, ‰∫¨, „É°, „Éà, „É≠, „ÅØ, ÂøÉ, „Å´, ÂØÑ, „Çä, Ê∑ª, „ÅÜ,
</code></pre>
<pre><code>ÊàëËØ¥‰Ω†ÂÄíÊòØÂø´ÁÇπÂïäÔºÅÔºÅÔºÅ
==&gt;
Êàë, ËØ¥, ‰Ω†, ÂÄí, ÊòØ, Âø´, ÁÇπ, Âïä, ÔºÅ, ÔºÅ, ÔºÅ, 
</code></pre>
<p>But for GPT-3, it composes of different languages and can produce both Chinese and English in one sentence. So I am really curious how this model makes tokens.</p>
","gpt-3"
"75454265","OpenAI GPT-3 API: Does fine-tuning have a token limit?","2023-02-14 23:38:34","75458209","3","4291","<openai-api><gpt-3>","<p>In the documentation for GPT-3 API, it says:</p>
<blockquote>
<p>One limitation to keep in mind is that, for most models, a single API
request can only process up to 2,048 tokens (roughly 1,500 words)
between your prompt and completion.</p>
</blockquote>
<p>In the documentation for fine tuning model, it says:</p>
<blockquote>
<p>The more training samples you have, the better. We recommend having at
least a couple hundred examples. in general, we've found that each
doubling of the dataset size leads to a linear increase in model
quality.</p>
</blockquote>
<p>My question is, does the 1,500 words limit also apply to fine tune model? Does &quot;Doubling of the dataset size&quot; mean number of training datasets instead of size of each training dataset?</p>
","gpt-3"
"75444023","Max prompt token not working when using GPT-3 text-davinci-003","2023-02-14 06:05:04","","2","1811","<javascript><node.js><typescript><openai-api><gpt-3>","<p>i am integrate GPT-3 text-davinci-003 algorithm with node js all working good but when we have pass token every time different but every time Same gives reply GPT-3 text-davinci-003 so please give me solution.</p>
<pre><code> const response = await openai.createCompletion({
   model: text-davinci-003,
   prompt: 'what is javascript',
   temperature: 0.05,
   max_tokens: 1000,
   top_p: 0.5,
   frequency_penalty: 0,
   presence_penalty: 0,
   stop: [&quot;END OF POLICY&quot;]
});
           
</code></pre>
<p>i am passing 1000 token but this reply only give max token 160,180,210 like but i want to give large token reply</p>
","gpt-3"
"75442916","OpenAI GPT-3 API: How to preserve formatting when pasting content into an Excel cell?","2023-02-14 02:24:52","75447943","0","486","<excel><openai-api><gpt-3>","<p>I'm trying to create a fine tuned GPT-3 model.  to that end, I have some content that i've formatted in word that im trying to bring to excel (in order to import the training dataset).</p>
<p>My inputs are in the format:</p>
<pre><code>*Point A
    *Subpoint A1
    *Subpoint A2
*Point B
    *Subpoint B1
    *Subpoint B2
</code></pre>
<p>However, when i copy the contents into excel, The excel cell converts this to:</p>
<pre><code>*Point A
*Subpoint A1
*Subpoint A2
*Point B
*Subpoint B1
*Subpoint B2
</code></pre>
<p>Is there any way for me to preserve my original formatting?</p>
<p>Is there any other way better way than this?</p>
<p>Any help is appreciated greatly :)</p>
<p>Regards,
Galeej</p>
","gpt-3"
"75435496","OpenAI GPT-3 API error: ""Unknown endpoint for this model.""","2023-02-13 11:50:22","","-3","3149","<python><openai-api><gpt-3>","<p>I'm new to using APIs.
I found myself interested inn the new OpenAI product, GPT-3 (I know, it's not that new. But I just found out about it).
I'm trying to use the API key in Python, but it seems the key is invalid.</p>
<p>This is my code (I can't put my API key here for obvious reasons):</p>
<pre><code>import requests 
prompt = 'Tell me the history of Europe in summary'
model = 'davinci'
url = 'https://api.openai.com/v1/engines/davinci/jobs'

headers = {
    'content-type': 'application/json',
    'Authorization': 'Bearer MY_API_KEY',
}

data = {
    'prompt': prompt,
    'max-tokens': 100,
    'temperature': 0.5,
}

response = requests.post(url,headers=headers, json=data)
response_json = response.json()
print(response_json)
</code></pre>
<p>I keep receiving this error:
{'error': {'message': 'Unknown endpoint for this model.', 'type': 'invalid_request_error', 'param':       None, 'code': None}}</p>
<p>I have tried using a new API key several times but it doesn't work.
How can I find out why my key is invalid?</p>
","gpt-3"
"75429596","OpenAI GPT-3 API: How to parse the response into an ordered list or dictionary?","2023-02-12 19:24:01","","2","7518","<python><openai-api><gpt-3>","<p>GPT-3 is amazing, but parsing its results is a bit of a headache, or I'm missing something here?
For example, I'm asking GPT-3 to write something about &quot;digital marketing&quot; and it's returning back some interesting stuff:</p>
<pre><code>\n\n1. Topic: The Benefits of Digital Marketing \nHeadlines: \na. Unlocking the 
Potential of Digital Marketing \nb. Harnessing the Power of Digital Marketing for 
Your Business \nc. How to Maximize Your Return on Investment with Digital Marketing 
\nd. Exploring the Benefits of a Comprehensive Digital Marketing Strategy \ne. 
Leveraging Technology to Take Your Business to the Next Level with Digital Marketing  
\n\n2. Topic: Social Media Strategies for Effective Digital Marketing  \nHeadlines:  
\na. Crafting an Engaging Social Media Presence for Maximum Impact \nb. How to Reach 
and Engage Your Target Audience Through Social Media Platforms  \nc. Optimizing Your 
Content Strategy for Maximum Reach on Social Media Platforms   \nd. Utilizing Paid 
Advertising Strategies on Social Media Platforms   \t\t\t\t\t\t\t     e .Analyzing 
and Improving Performance Across Multiple Social Networks\n\n3. Topic: SEO Best 
Practices for Effective Digital Marketing    Headlines:     a .Understanding Search 
Engine Algorithms and Optimizing Content Accordingly    b .Developing an Effective 
SEO Strategy That Delivers Results c .Leveraging Keywords and Metadata For Maximum 
Visibility d .Exploring Advanced SEO Techniques To Increase Traffic e .Analyzing 
Performance Data To Improve Rankings\n\n4Topic : Email Campaigns For Successful 
Digital Marketin g Headlines : a .Creating Compelling Email Campaigns That Drive 
Results b .Optimizing Email Deliverability For Maximum Impact c .Utilizing Automation 
Tools To Streamline Email Campaign Management d .Measuring Performance And Analyzing 
Data From Email Campaigns e .Exploring Creative Ways To Increase Open Rates On 
Emails\n\n5Topic : Mobile Advertising Strategies For Effective Digita l Marketin g 
Headlines : a ..Maximizing Reach With Mobile Ads b ..Understanding User Behavior On 
Mobile Devices c ..Optimizing Ads For Different Screen Sizes d ..Leveraging Location- 
Based Targeting To Increase Relevance e ..Analyzing Performance Data From Mobile Ads
</code></pre>
<p>As you can see, it's sent me back a list of topics related to &quot;digital marketing&quot; with some headlines (apparently from a to e). I see some line breaks and tabulation here and there. So my first reflex was to split the text on the line breaks, but it looks like the format is not equal everywhere, as there are very few line breaks in the second half of the response (which make it inaccurate).
What I'd like to do, is reformatting the output, so I can have a kind of list of topics and headlines. Something like this:</p>
<pre><code>[
     {&quot;Topic 1&quot;: [&quot;headline 1&quot;, &quot;headline 2&quot;,&quot;...&quot;]},
     {&quot;Topic 2&quot;: [&quot;headline 1&quot;, &quot;headline 2&quot;,&quot;...&quot;]},
     {&quot;Topic 3&quot;: [&quot;headline 1&quot;, &quot;headline 2&quot;,&quot;...&quot;]}
]
</code></pre>
<p>Maybe there is a parameter to send over withing my request, but I didn't find anything in the doc. So I guess my best bet is to reformat using <code>regex</code>. Here I see a pattern <code>Topic:</code>and <code>Headlines:</code> but it's not always the case. What is consistent is the number prefixing each element <code>(like √å., II., 1., 2. or a., b.)</code> but sometimes it looks like <code>a ..</code> (you can see that at the end of the response for example.</p>
<p>Any idea how to do that? (I'm using python for that, but can adapt from another language)</p>
","gpt-3"
"75405943","I do not know why I can't access open ai's api for use in a react app","2023-02-10 00:52:22","75406005","-2","1344","<javascript><reactjs><openai-api><gpt-3>","<p>I am trying to access openai's api for a react application. I am getting an &quot;unsafe header&quot; error, an error 400, and at the same time &quot;https://api.openai.com/v1/completions&quot; is sending me a prompt about not providing my api key, even though I am providing the api key through a .env file. I do not know what to do, and I'm wondering what exactly I did wrong.</p>
<p>This is the react function I am using:</p>
<pre><code>const configuration = new Configuration({
    apiKey: process.env.REACT_APP_OPENAI_API_KEY,
    organization: &quot;org-xut9Kn1LqNLyDiHEMAQlnJ0k&quot;
});

const openai = new OpenAIApi(configuration);

const handleSuggestions = async (text) =&gt; {
  const response = await openai.createCompletion({
      model: &quot;text-davinci-001&quot;,
      prompt: &quot;autocomplete this word, letter or sentence: &quot; + text,
      max_tokens: 100,
      n: 1,
      stop: text.length - 1,
      temperature: 0.15,
  });
  console.log(response);
  const data = await response.json();
  setSuggestions(response.choices[0].text.split(' ').slice(text.split(' ').length - 1).join(' ').split(' '));
};
</code></pre>
<p>``</p>
<p>I am getting a &quot;unsafe header &quot;User-Agent&quot;&quot; error as well as an error 400 from &quot;https://api.openai.com/v1/completions&quot; in my browser console while running the react app. This is the full prompt I am getting back from &quot;https://api.openai.com/v1/completions&quot;:</p>
<pre><code>{
    &quot;error&quot;: {
        &quot;message&quot;: &quot;You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys.&quot;,
        &quot;type&quot;: &quot;invalid_request_error&quot;,
        &quot;param&quot;: null,
        &quot;code&quot;: null
    }
}
</code></pre>
<p>Please what can I do, and what exactly is wrong with the code? Also, hoping this counts as a &quot;Minimal, Reproducible Example&quot;, as I am pretty new to stack overflow.</p>
","gpt-3"
"75404485","GPT3 fine tuned model returns additional questions and answers","2023-02-09 21:17:01","","1","1026","<openai-api><gpt-3>","<p>I have fine tuned a custom dataset using GPT3. I created a simple program to take user input (a question) and return the correct response. The program works, however it returns additional question and answers from the dataset I uploaded to the model.</p>
<p>I tried to reduce the max tokens cap and have set the temperature to 0, but I cannot seem to figure out how to stop the program from returning the additional questions and answers. Has anyone encountered this problem and if so how can I fix it?</p>
<p>Here is my code:</p>
<pre class=""lang-py prettyprint-override""><code>import openai

openai.api_key = &quot;MY_API_KEY&quot;

def respond(prompt):
    completions = openai.Completion.create(
        engine=&quot;MY_FINED_TUNED_MODEL&quot;,
        prompt=prompt,
        max_tokens=50,
        n=1,
        stop=None,
        temperature=0,
    )

    message = completions.choices[0].text
    return message

while True:
    prompt = input(&quot;Enter your question: &quot;)
    if prompt.lower() == &quot;end&quot;:
        break
    response = respond(prompt)
    print(response)
</code></pre>
","gpt-3"
"75403409","Context window length in OpenAI API Codex models","2023-02-09 19:16:21","","0","1667","<openai-api><gpt-3>","<p>Is the completion window length included in the context window length for OpenAI Codex models?</p>
<p>For <code>da-vinci</code>, the context window length is set to <code>4000</code> tokens.</p>
<p>From what I understand, as an example, if the prompt length is <code>3500</code> tokens, then the remaining <code>500</code> is for the completion. And there is no way use the whole <code>4000</code> token as the prompt.</p>
<p>I am pretty sure in my understanding, but it would be helpful to have it confirmed by someone knowledgeable.</p>
","gpt-3"
"75401992","OpenAI API error: ""You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth""","2023-02-09 17:00:01","75402073","0","6428","<php><curl><openai-api><gpt-3><gpt-4>","<p>I am creating a PHP script to access Open Ai's API, to ask a query and get a response.</p>
<p>I am getting the following error:</p>
<blockquote>
<p>You didn't provide an API key. You need to provide your API key in an
Authorization header using Bearer auth (i.e. Authorization: Bearer
YOUR_KEY)</p>
</blockquote>
<p>...but I thought I was providing the API key in the first variable?</p>
<p>Here is my code:</p>
<pre><code>$api_key = &quot;sk-U3B.........7MiL&quot;;

$query = &quot;How are you?&quot;;

$url = &quot;https://api.openai.com/v1/engines/davinci/jobs&quot;;

// Set up the API request headers
$headers = array(
    &quot;Content-Type: application/json&quot;,
    &quot;Authorization: Bearer &quot; . $api_key
);

// Set up the API request body
$data = array(
    &quot;prompt&quot; =&gt; $query,
    &quot;max_tokens&quot; =&gt; 100,
    &quot;temperature&quot; =&gt; 0.5
);

// Use WordPress's built-in HTTP API to send the API request
$response = wp_remote_post( $url, array(
    'headers' =&gt; $headers,
    'body' =&gt; json_encode( $data )
) );

// Check if the API request was successful
if ( is_wp_error( $response ) ) {
    // If the API request failed, display an error message
    echo &quot;Error communicating with OpenAI API: &quot; . $response-&gt;get_error_message();
} else {
    // If the API request was successful, extract the response text
    $response_body = json_decode( $response['body'] );
    //$response_text = $response_body-&gt;choices[0]-&gt;text;
    var_dump($response_body);
    // Display the response text on the web page
    echo $response_body;
</code></pre>
","gpt-3"
"75400926","OpenAI ChatGPT API: CORS policy error when fetching data","2023-02-09 15:37:20","75401250","2","8018","<javascript><openai-api><gpt-3>","<p>I am trying to write a simple JavaScript script which uses the ChatGPT API to ask a question and get a response.</p>
<p>However I am getting the following error message:</p>
<blockquote>
<p>&quot;Access to fetch at
'https://api.chatgpt.com/answer?question=How%20are%20you?&amp;api_key=sk-U3BPK...'
from origin 'https://wordpress-......cloudwaysapps.com' has been
blocked by CORS policy: No 'Access-Control-Allow-Origin' header is
present on the requested resource. If an opaque response serves your
needs, set the request's mode to 'no-cors' to fetch the resource with
CORS disabled.&quot;</p>
</blockquote>
<p>I have enabled CORS headers server side in my hosting environment. But the error remains.</p>
<p>What is the reason for this issue and how can I fix this issue?</p>
<p>Here is my code:</p>
<pre><code>&lt;html&gt;
&lt;head&gt;
  &lt;script&gt;
    function askQuestion() {
      var question = document.getElementById(&quot;questionInput&quot;).value;
      var apiKey = document.getElementById(&quot;apiKey&quot;).value;
      // access chatgpt's API and pass in the question and API key as parameters
      fetch(&quot;https://api.chatgpt.com/answer?question=&quot; + question + &quot;&amp;api_key=&quot; + apiKey)
        .then(response =&gt; {
          if (!response.ok) {
            throw new Error(&quot;Failed to fetch answer from API&quot;);
          }
          return response.json();
        })
        .then(data =&gt; {
          // get the answer from the API response and display it in the textbox
          document.getElementById(&quot;answerBox&quot;).value = data.answer;
        })
        .catch(error =&gt; {
          console.error(&quot;Error fetching answer from API: &quot;, error);
        });
    }

    function askFollowUpQuestion() {
      var followUpQuestion = document.getElementById(&quot;followUpQuestionInput&quot;).value;
      var apiKey = document.getElementById(&quot;apiKey&quot;).value;
      // access chatgpt's API and pass in the follow-up question and API key as parameters
      fetch(&quot;https://api.chatgpt.com/answer?question=&quot; + followUpQuestion + &quot;&amp;api_key=&quot; + apiKey)
        .then(response =&gt; {
          if (!response.ok) {
            throw new Error(&quot;Failed to fetch answer from API&quot;);
          }
          return response.json();
        })
        .then(data =&gt; {
          // get the answer from the API response and display it in the textbox
          document.getElementById(&quot;followUpAnswerBox&quot;).value = data.answer;
        })
        .catch(error =&gt; {
          console.error(&quot;Error fetching answer from API: &quot;, error);
        });
    }
  &lt;/script&gt;
&lt;/head&gt;
&lt;body&gt;
  &lt;input type=&quot;text&quot; id=&quot;questionInput&quot; placeholder=&quot;Enter your question here&quot;&gt;&lt;/input&gt;
  &lt;br&gt;
  &lt;input type=&quot;text&quot; id=&quot;apiKey&quot; placeholder=&quot;Enter your API key&quot;&gt;&lt;/input&gt;
  &lt;br&gt;
  &lt;button onclick=&quot;askQuestion()&quot;&gt;Ask&lt;/button&gt;
  &lt;br&gt;
  &lt;textarea id=&quot;answerBox&quot; readonly&gt;&lt;/textarea&gt;
  &lt;br&gt;
  &lt;input type=&quot;text&quot; id=&quot;followUpQuestionInput&quot; placeholder=&quot;Enter your follow-up question here&quot;&gt;&lt;/input&gt;
  &lt;br&gt;
  &lt;button onclick=&quot;askFollowUpQuestion()&quot;&gt;Ask Follow-up&lt;/button&gt;
  &lt;br&gt;
  &lt;textarea id=&quot;followUpAnswerBox&quot; readonly&gt;&lt;/textarea&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre>
","gpt-3"
"75396481","OpenAI GPT-3 API error: ""This model's maximum context length is 4097 tokens""","2023-02-09 09:18:40","75397187","41","100393","<openai-api><gpt-3>","<p>I am making a request to the completions endpoint. My prompt is 1360 tokens, as verified by the Playground and the Tokenizer. I won't show the prompt as it's a little too long for this question.</p>
<p>Here is my request to openai in Nodejs using the openai npm package.</p>
<pre><code>const response = await openai.createCompletion({
  model: 'text-davinci-003',
  prompt,
  max_tokens: 4000,
  temperature: 0.2
})
</code></pre>
<p>When testing in the playground my total tokens after response are 1374.</p>
<p>When submitting my prompt via the completions API I am getting the following error:</p>
<pre><code>error: {
  message: &quot;This model's maximum context length is 4097 tokens, however you requested 5360 tokens (1360 in your prompt; 4000 for the completion). Please reduce your prompt; or completion length.&quot;,
  type: 'invalid_request_error',
  param: null,
  code: null
}
</code></pre>
<p>If you have been able to solve this one, I'd love to hear how you did it.</p>
","gpt-3"
"75390542","Discordbot.py:NotFound: 404 Not Found (error code: 10062): Unknown interaction","2023-02-08 19:01:25","75390600","0","1041","<python><discord.py><gpt-3>","<p>Here is the code</p>
<pre><code>@bot.tree.command(description=&quot;create a txt with chat-gpt&quot;)
async def gpt(interaction: discord.Interaction, *, prompt:str):
    async with aiohttp.ClientSession() as session:
        payload = {
            &quot;model&quot;:&quot;text-davinci-003&quot;,
            &quot;prompt&quot;:prompt,
            &quot;temperature&quot;: 0.5,
            &quot;max_tokens&quot;: 50,
            &quot;presence_penalty&quot;: 0,
            &quot;frequency_penalty&quot;: 0,
            &quot;best_of&quot;: 1,
        }
        headers = {&quot;Authorization&quot;: f&quot;Bearer {API_KEY}&quot;}
        async with session.post(&quot;https://api.openai.com/v1/completions&quot;, json=payload, headers=headers) as resp:
            response = await resp.json()
            em = discord.Embed(title=&quot;Chat gpt‚Äôs responce:&quot;, description=response)
            await interaction.response.defer()
            await asyncio.sleep(delay=0)
            await interaction.followup.send(embed=em)
</code></pre>
<p>when I try the slash command, it tells me this as an error code</p>
<pre><code>The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File &quot;/home/haoroux/.local/lib/python3.10/site-packages/discord/app_commands/tree.py&quot;, line 1242, in _call
    await command._invoke_with_namespace(interaction, namespace)
  File &quot;/home/haoroux/.local/lib/python3.10/site-packages/discord/app_commands/commands.py&quot;, line 887, in _invoke_with_namespace
    return await self._do_call(interaction, transformed_values)
  File &quot;/home/haoroux/.local/lib/python3.10/site-packages/discord/app_commands/commands.py&quot;, line 880, in _do_call
    raise CommandInvokeError(self, e) from e
discord.app_commands.errors.CommandInvokeError: Command 'gpt' raised an exception: NotFound: 404 Not Found (error code: 10062): Unknown interaction
</code></pre>
<p>And I really can't find the solution even after everything I tried so if you have some time please help me</p>
<p>I changed the delay, I also tried to put the embed=em in response.defer()
I tried to override the embed but nothing to do it does not work</p>
","gpt-3"
"75389044","How can I correctly implement the OpenAI API in swiftUI using a REST API? closed","2023-02-08 16:47:12","","-2","1201","<curl><swiftui><alamofire><openai-api><gpt-3>","<p>I currently have made a REST API using Alamofire in swiftUI. The request is a post method to this URL: <a href=""https://api.openai.com/v1/engines/text-davinci-002/completions"" rel=""nofollow noreferrer"">https://api.openai.com/v1/engines/text-davinci-002/completions</a>, which is basically sending a question to the OpenAI API and <em>supposed</em> to get the answer in JSON format. I have made an API key in OpenAI and have included it in my code.</p>
<p>The problem I am facing is that when getting the input, I recieve the following error:</p>
<p><code>Error: Response status code was unacceptable: 400.</code></p>
<p>I first tried to make sure if my API key was correct and it was. I also tried to see if a cURL statement would work to check if it was an error with the URL.<br />
I used this cURL statement:</p>
<pre><code>curl -X POST \
  https://api.openai.com/v1/engines/text-davinci-002/completions \
  -H 'Authorization: Bearer my-api-key' \
  -H 'Content-Type: application/json' \
  -d '{
        &quot;prompt&quot;: &quot;What is quantum mechanics?&quot;,
        &quot;temperature&quot;: 0.7,
        &quot;max_tokens&quot;: 20,
        &quot;top_p&quot;: 1,
        &quot;frequency_penalty&quot;: 0,
        &quot;presence_penalty&quot;: 0
}'
</code></pre>
<p>Which worked fine and gave me a correct output:</p>
<pre><code>{&quot;id&quot;:&quot;cmpl-6hhWBlzZHrLtV4RKCtjL63CLoTvPi&quot;,&quot;object&quot;:&quot;text_completion&quot;,&quot;created&quot;:1675873407,&quot;model&quot;:&quot;text-davinci-002&quot;,&quot;choices&quot;:[{&quot;text&quot;:&quot;\n\nQuantum mechanics is a branch of physics that studies the behavior of matter and energy in the&quot;,&quot;index&quot;:0,&quot;logprobs&quot;:null,&quot;finish_reason&quot;:&quot;length&quot;}],&quot;usage&quot;:{&quot;prompt_tokens&quot;:5,&quot;completion_tokens&quot;:20,&quot;total_tokens&quot;:25}}
</code></pre>
<p>If it helps, here's my code:</p>
<pre><code>//
//  chat.swift
//  titan
//
//  Created by Refluent on 08/02/2023.
//

import SwiftUI
import Alamofire

struct Message: Hashable {
    let sender: String
    let content: String
}

struct OpenAIResponse: Decodable {
    let completions: [Completion]
    
    struct Completion: Decodable {
        let text: String
    }
}

struct chatView: View {
    @State private var response: String = &quot;&quot;
    @State private var messages: [Message] = []
    @State private var userInput: String = &quot;&quot;
    
    let apiKey = &quot;my-api-key&quot;
    let model = &quot;text-davinci-002&quot;
    
    var body: some View {
        VStack {
            Text(&quot;Response from OpenAI API:&quot;)
            List(messages, id: \.self) { message in
                Text(&quot;\(message.sender): \(message.content)&quot;)
            }
            
            TextField(&quot;Enter your question&quot;, text: $userInput)
                .padding()
            
            Button(action: {
                self.sendRequest()
            }) {
                Text(&quot;Send&quot;)
            }
        }
    }
    
    func sendRequest() {
        let headers: HTTPHeaders = [&quot;Authorization&quot;: &quot;Bearer \(apiKey)&quot;, &quot;Content-Type&quot;: &quot;application/json&quot;]
        
        let parameters: Parameters = [&quot;prompt&quot;: userInput, &quot;model&quot;: model]
        
        messages.append(Message(sender: &quot;Me&quot;, content: userInput))
        userInput = &quot;&quot;
        
        AF.request(&quot;https://api.openai.com/v1/engines/text-davinci-002/completions&quot;, method: .post, parameters: parameters, headers: headers)
            .validate()
            .responseDecodable(of: OpenAIResponse.self) { response in
                switch response.result {
                case .success(let value):
                    let text = value.completions[0].text
                    self.messages.append(Message(sender: &quot;ChatGPT&quot;, content: text))
                case .failure(let error):
                    self.messages.append(Message(sender: &quot;ChatGPT&quot;, content: &quot;Error: \(error.localizedDescription)&quot;))
                }
            }
    }
}
</code></pre>
<p>For more context about my code, here it is:</p>
<p>I'm sending a post request to <a href=""https://api.openai.com/v1/engines/text-davinci-002/completions"" rel=""nofollow noreferrer"">this URL</a>, then getting the output and saving it in an array alongside the original input. The input and the output is then displayed as a chat.</p>
<p>Does anyone know why I am recieving this error? Did I miss a crucial part in my code?</p>
<p>Thanks in advance btw.</p>
<p>After reading through the comments, I realised that this isn't exactly something to do in <code>Swift</code>, so I will be implementing this in the server side. I apoligize for wasting your time.</p>
","gpt-3"
"75384220","[InteractionAlreadyReplied]: The reply to this interaction has already been sent or deferred","2023-02-08 10:10:52","","0","455","<node.js><discord><discord.js><gpt-3>","<p>It says [InteractionAlreadyReplied]: The reply to this interaction has already been sent or deferred. But inreality i am using editReply</p>
<p>I am having issue in logging error, it works fine with try and else but it shows <code>[InteractionAlreadyReplied]: The reply to this interaction has already been sent or deferred</code> in catch (error). I even tried using followUp  but still doesnt work, it keeps giving the same error and shuts the whole bot down.</p>
<pre><code>module.exports = {
  data: new SlashCommandBuilder()
  .setName(&quot;chat-gpt&quot;)
  .setDescription(&quot;chat-gpt-3&quot;)
  .addStringOption(option =&gt;
    option.setName('prompt')
      .setDescription('Ask Anything')
      .setRequired(true)),

  async execute(interaction, client) {
    const prompt = interaction.options.getString('prompt');
    await interaction.deferReply();

    const configuration = new Configuration({
      apiKey: &quot;nvm&quot;,
    });
    const openai = new OpenAIApi(configuration);
    
    const response = await openai.createCompletion({
      model: 'text-davinci-003',
      prompt: prompt,
      max_tokens: 2048,
      temperature: 0.7,
      top_p: 1,
      frequency_penalty: 0.0,
      presence_penalty: 0.0,
    });

    let responseMessage = response.data.choices[0].text;

    /* Exceed 2000 */

    try {
      let responseMessage = response.data.choices[0].text;
      if (responseMessage.length &gt;= 2000) {
        const attachment = new AttachmentBuilder(Buffer.from(responseMessage, 'utf-8'), { name: 'chatgpt-response.txt' });
        const limitexceed = new EmbedBuilder()
        .setTitle(`Reached 2000 characters`)
        .setDescription(`Sorry, but you have already reached the limit of 2000 characters`)
        .setColor(`#FF6961`);
        await interaction.editReply({ embeds: [limitexceed], files: [attachment] });
        
      } else {
        const responded = new EmbedBuilder()
        .setTitle(`You said: ${prompt}`)
        .setDescription(`\`\`\`${response.data.choices[0].text}\`\`\``)
        .setColor(`#77DD77`);
  
        await interaction.editReply({ embeds: [responded] });   
      }

    } catch (error) {
      console.error(error);
      await interaction.followUp({ content: `error` });
    }

    return;
  },
};

</code></pre>
<p>i even tried using followUp or etc but the result is still same.</p>
","gpt-3"
"75379690","Not getting proper response from GPT-3 using SDK in JS","2023-02-07 22:38:28","","0","359","<openai-api><gpt-3>","<p>When using createCompletion I get a response but it doesn't have the actual text response.  In textPayload it has &quot;text: 'package com.example.demo.controller;',&quot;</p>
<p>Below is my code</p>
<pre><code>const openai = new OpenAIApi(configuration);

  async function step1() {
  currentResponse = await openai.createCompletion({
    model: &quot;text-davinci-003&quot;,
    prompt: currentMessage,
    temperature: 0,
    max_tokens: 2292,
    top_p: 1,
    frequency_penalty: 0,
    presence_penalty: 0,
    stop: [&quot;\n\n&quot;],
  });

} //end step1

return step1().then(function(response) {

    var currentResponseNew = currentResponse.data
    //this is where I get the text payload value
    console.log(currentResponseNew)
    res.send(&quot;done&quot;)


})
</code></pre>
","gpt-3"
"75376813","OpenAI fine-tune with python return null model","2023-02-07 17:15:43","75397197","1","1643","<python><openai-api><gpt-3>","<p>I am trying to get fine-tune model from OpenAI GPT-3 using python with following code</p>
<pre><code>#upload training data

upload_response = openai.File.create(
  file=open(file_name, &quot;rb&quot;),
  purpose='fine-tune'
)
file_id = upload_response.id
print(f'\nupload training data respond:\n\n {upload_response}')
</code></pre>
<p>OpenAI respond with data</p>
<pre><code> {
  &quot;bytes&quot;: 380,
  &quot;created_at&quot;: 1675789714,
  &quot;filename&quot;: &quot;file&quot;,
  &quot;id&quot;: &quot;file-lKSQushd8eABcfiBVwhxBMOJ&quot;,
  &quot;object&quot;: &quot;file&quot;,
  &quot;purpose&quot;: &quot;fine-tune&quot;,
  &quot;status&quot;: &quot;uploaded&quot;,
  &quot;status_details&quot;: null
}
</code></pre>
<p>My training file has been uploaded so I am checking for fine-tune response with code</p>
<pre><code>fine_tune_response = openai.FineTune.create(training_file=file_id)
print(f'\nfine-tune respond:\n\n {fine_tune_response}')
</code></pre>
<p>I am getting</p>
<pre><code> {
  &quot;created_at&quot;: 1675789714,
  &quot;events&quot;: [
    {
      &quot;created_at&quot;: 1675789715,
      &quot;level&quot;: &quot;info&quot;,
      &quot;message&quot;: &quot;Created fine-tune: ft-IqBdk4WJETm4KakIzfZeCHgS&quot;,
      &quot;object&quot;: &quot;fine-tune-event&quot;
    }
  ],
  &quot;fine_tuned_model&quot;: null,
  &quot;hyperparams&quot;: {
    &quot;batch_size&quot;: null,
    &quot;learning_rate_multiplier&quot;: null,
    &quot;n_epochs&quot;: 4,
    &quot;prompt_loss_weight&quot;: 0.01
  },
  &quot;id&quot;: &quot;ft-IqBdk4WJETm4KakIzfZeCHgS&quot;,
  &quot;model&quot;: &quot;curie&quot;,
  &quot;object&quot;: &quot;fine-tune&quot;,
  &quot;organization_id&quot;: &quot;org-R6DqvjTNimKtBzWWgae6VmAy&quot;,
  &quot;result_files&quot;: [],
  &quot;status&quot;: &quot;pending&quot;,
  &quot;training_files&quot;: [
    {
      &quot;bytes&quot;: 380,
      &quot;created_at&quot;: 1675789714,
      &quot;filename&quot;: &quot;file&quot;,
      &quot;id&quot;: &quot;file-lKSQushd8eABcfiBVwhxBMOJ&quot;,
      &quot;object&quot;: &quot;file&quot;,
      &quot;purpose&quot;: &quot;fine-tune&quot;,
      &quot;status&quot;: &quot;uploaded&quot;,
      &quot;status_details&quot;: null
    }
  ],
  &quot;updated_at&quot;: 1675789714,
  &quot;validation_files&quot;: []
}
</code></pre>
<p>As you see, the fine_tune_model is null so I cant use it for Completion.
My question is how to check for example in While loop if my fine-tune is complete using ft id</p>
","gpt-3"
"75373129","OpenAI GPT-3 API error: ""This model's maximum context length is 2049 tokens""","2023-02-07 12:07:58","75373214","0","2665","<javascript><php><json><openai-api><gpt-3>","<p>I have two issues relating to the response result from OpenAI completion.</p>
<p>The following result doesn't return back the full text when I give a content of 500 words and prompt with &quot;Fix grammar mistakes&quot; <strong>(Is tokens issue?)</strong></p>
<p><a href=""https://i.sstatic.net/dLg4Y.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/dLg4Y.jpg"" alt=""Enter image description here"" /></a></p>
<p>The second issue is when the text sometimes have some double quotes <em>or</em> single quotes. It messes with the JSON format. So I delete any type of quotes from the content (I am not sure if it's the best solution, but I may prefer doing it in JavaScript, not PHP).</p>
<pre><code>curl_setopt($ch, CURLOPT_POSTFIELDS, &quot;{\n  \&quot;model\&quot;: \&quot;text-davinci-001\&quot;,\n  \&quot;prompt\&quot;: \&quot;&quot; . $open_ai_prompt  . &quot;:nn&quot; . $content_text  . &quot;\&quot;,\n  \&quot;temperature\&quot;: 0,\n  \&quot;top_p\&quot;: 1.0,\n  \&quot;frequency_penalty\&quot;: 0.0,\n  \&quot;presence_penalty\&quot;: 0.0\n}&quot;);
</code></pre>
<blockquote>
<p>&quot;message&quot;: &quot;We could not parse the JSON body of your request. (HINT:
This likely means you aren't using your HTTP library correctly. The
OpenAI API expects a JSON payload, but what was sent was not valid
JSON.</p>
</blockquote>
","gpt-3"
"75362603","Is GPT-3 a model or a framework?","2023-02-06 14:26:30","","-1","271","<machine-learning><nlp><artificial-intelligence><gpt-3>","<p>We all hear GPT-3 being called a large language model (LLM), but is it really more of a framework since you can use GPT-3 with your own dataset, to train your own version of a GPT-3 model?</p>
<p>My understanding is that a model is the result of training, and you can use one of many frameworks/libraries to train the model (ex: tensor flow).  If GPT-3 was just a model, you wouldn't be able to train with your own data on it, right?  So that makes GPT-3 a framework?</p>
<p>Can anyone help me to better understand the AI terminology for this?</p>
","gpt-3"
"75361743","How do I make an API call to GPT-3 correctly?","2023-02-06 13:09:22","75362164","0","809","<python><python-requests><gpt-3>","<p>I am trying to make an API call to GPT-3 but I am getting an error (Bad request 400). Here is my code:</p>
<pre><code>url = &quot;https://api.openai.com/v1/engines/gpt-3/jobs&quot;

headers = {
    &quot;Content-Type&quot;: &quot;application/json&quot;,
    &quot;Authorization&quot;: &quot;sk-apikey&quot;
}

data = {
    &quot;model&quot;: &quot;text-davinci-002&quot;,
    &quot;prompt&quot;: &quot;Correct this to standard English : Who are you&quot;,
    &quot;max_tokens&quot;: 60        
}

response = requests.post(url, headers=headers, data=json.dumps(data))
</code></pre>
","gpt-3"
"75355374","Python Telegram bot chat gpt","2023-02-05 20:43:21","","0","933","<python><telegram><openai-api><telebot><gpt-3>","<pre><code>import telebot
import requests
import time

TELEGRAM_TOKEN = &quot;my-token&quot;

bot = telebot.TeleBot(TELEGRAM_TOKEN)

@bot.message_handler(commands=['search'])

def handle_search(message):

    # extract the search word from the message text
    search_word = message.text.split(&quot; &quot;, 1)[1]
    headers = {
        'Authorization': 'Bearer sk-token',
    }
   
    # prepare the request payload
    json_data = {

        'model': 'text-davinci-003',
        'prompt': f'{search_word}',
        'temperature': 0.8,
        'max_tokens': 2000,
    }

    # send the request to OpenAI API

    response = requests.post('https://api.openai.com/v1/completions', headers=headers, json=json_data).json()
    
    # extract the response text

    response_text = response['choices'][0]['text']

    
    # send the response text as a message
    bot.send_message(message.chat.id, response_text, reply_to_message_id=message.message_id)

def run():

    while True:
        try:
            bot.polling(none_stop=True)
        except Exception as e:
            # log the error
            print(f&quot;Error occurred: {e}&quot;)

            # wait for 5 seconds before polling again
            time.sleep(5)

if __name__ == '__main__':
    run()
</code></pre>
<p>When I turn it on, it appears like this</p>
<blockquote>
<p>File &quot;main.py&quot;, line 6, in  @bot.message_handler(commands=['search']) AttributeError: 'TeleBot' object has no attribute 'message_handler'</p>
</blockquote>
","gpt-3"
"75349226","How to avoid word limit in OpenAI API in R?","2023-02-05 00:01:42","75418423","0","2111","<r><chatbot><openai-api><gpt-3>","<p>I registered at this <a href=""https://platform.openai.com/"" rel=""nofollow noreferrer"">link</a> to get a key for the OpenAI API.</p>
<p>And I used the &quot;<code>chatgpt</code>&quot; package in R.</p>
<pre><code>library(chatgpt)
</code></pre>
<p>And set environment:</p>
<pre><code>Sys.setenv(OPENAI_API_KEY = &quot;sk-YOURAPI&quot;)
</code></pre>
<p>I used this function:</p>
<pre><code>chatgpt::ask_chatgpt(&quot;How to make a strawberry pie to donate to my wife? Ingredients, please.&quot;)
</code></pre>
<p>Sometimes the output is missing words. For example:</p>
<pre><code>*** ChatGPT input:

How to make a strawberry pie to donate to my wife? Ingredients, please.
[1] ... and your wife is&quot;
</code></pre>
<p>It does not complete the text after <code>is</code>. Is there any solution for this on <code>R</code>?</p>
<p>Similar question on <a href=""https://www.reddit.com/r/ChatGPT/comments/zec65u/is_there_a_limit_to_the_code_output_that_gpt/"" rel=""nofollow noreferrer"">reddit</a>.</p>
<p><strong>EDIT</strong></p>
<p>I tried increasing the processing time of the function (so that the chat would finish typing all the content, without stopping midway). Like this:</p>
<pre><code>for (i in 35) {

  print(chatgpt::ask_chatgpt(&quot;How to make a strawberry pie to donate to my wife? Ingredients, please.&quot;))

  Sys.sleep(i)

}
</code></pre>
<p>Editors note: Prior March 1st, 2023 there was no official ChatGPT API.</p>
","gpt-3"
"75348532","How to use the OpenAI stream=true property with a Django Rest Framework response, and still save the content returned?","2023-02-04 21:34:09","","7","3384","<reactjs><django><gpt-3>","<p>I'm trying to use the stream=true property as follows.</p>
<pre><code>completion = openai.Completion.create(
            model=&quot;text-davinci-003&quot;,
            prompt=&quot;Write me a story about dogs.&quot;,
            temperature=0.7,
            max_tokens=MAX_TOKENS,
            frequency_penalty=1.0,
            presence_penalty=1.0,
            stream=True,
        )
</code></pre>
<p>Unfortunately, I don't know what to do from here to return it to my React frontend. Typically, I've used standard response objects, setting a status and the serializer.data as the data. From my readings online, it seems I have to use the <code>StreamingHttpResponse</code>, but I'm not sure how to integrate that with the iterator object of <code>completion</code>, and actually save the outputted data once it is done streaming, as the view will end after returning the iterator to the endpoint. Any help?</p>
","gpt-3"
"75344458","OpenAI GPT API pre-tokenizing?","2023-02-04 10:28:49","","0","597","<artificial-intelligence><token><tokenize><gpt-3>","<p>I am trying to make a &quot;personal assistant&quot; chatbot (using GPT AI API) that can answer questions about myself when others ask it things. In order to do so, I have to give it a lot of information about myself, which I am currently doing in the prompt.</p>
<p>Example: <a href=""https://i.sstatic.net/EiOWo.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/EiOWo.png"" alt=""Screenshot of my prompt"" /></a></p>
<p>This means that every time someone asks a question, the prompt includes all of the information about me, which means that it gets tokenized every single time a question is asked. Is there a way to &quot;pre-tokenize&quot; the information about myself or store it in some other way? I ask this because the information about myself is what is costing me the most, as it sucks up a lot of tokens.</p>
<p>Thanks</p>
<p>I have tried looking online to no avail.</p>
","gpt-3"
"75335523","Error 400 when using GPT API (in JavaScript)","2023-02-03 12:08:08","75335600","1","2336","<javascript><error-handling><openai-api><gpt-3>","<p>I keep getting a 400 Error when I try to run my very basic chatbot using the GPT API:
<a href=""https://i.sstatic.net/VzyEy.png"" rel=""nofollow noreferrer"">error</a></p>
<p>Attached is my code; am I doing something wrong with the API key?</p>
<pre><code>const chatHistoryContent = document.querySelector(&quot;#chat-history-content&quot;);
const chatMessageInput = document.querySelector(&quot;#chat-message-input&quot;);
const chatMessageSubmit = document.querySelector(&quot;#chat-message-submit&quot;);



chatMessageSubmit.addEventListener(&quot;click&quot;, async function () {
    const message = chatMessageInput.value;
    chatMessageInput.value = &quot;&quot;;

    // Add the user's message to the chat history
    const userMessageDiv = document.createElement(&quot;div&quot;);
    userMessageDiv.innerHTML = `You: ${message}`;
    chatHistoryContent.appendChild(userMessageDiv);

    // Use the OpenAI GPT-3 API to get a response from the chatbot
    const response = await getResponseFromAPI(message);

    // Add the chatbot's response to the chat history
    const chatbotMessageDiv = document.createElement(&quot;div&quot;);
    chatbotMessageDiv.innerHTML = `Max: ${response}`;
    chatHistoryContent.appendChild(chatbotMessageDiv);
});

async function getResponseFromAPI(message) {

    const apiKey = &quot;sk-myapikey&quot;;
    const endpoint = `https://api.openai.com/v1/engines/davinci/jobs`;

    const response = await fetch(endpoint, {
        method: &quot;POST&quot;,
        headers: {
            &quot;Content-Type&quot;: `application/json`,
            &quot;Authorization&quot;: `Bearer ${apiKey}`,
        },
        body: JSON.stringify({
            model: &quot;text-davinci-003&quot;,
            prompt: &quot;test prompt&quot;, 
            temperature: 0.5,
            max_tokens: 512,
            top_p: 1,
            frequency_penalty: 0,
            presence_penalty: 0,
        })
    });

    const data = await response.json();
    return data.choices[0].text;
}
</code></pre>
<p>Thanks</p>
<p>I have tried consulting many websites to see solutions to this but have had no luck.</p>
","gpt-3"
"75329518","Fine Tuning GPT-3 for Consistent Output Format","2023-02-02 22:18:50","","1","1553","<openai-api><gpt-3>","<p>I am trying to use Open AI API to create quiz questions with three incorrect answers and one correct answer. The prompt I use is</p>
<pre><code>`Write a quiz on ${quiz_topic} with 5 questions. Each question has 3 incorrect answers and 1 correct answer. The correct answer is always the last answer. Write each answer on a separate line`
</code></pre>
<p>I would like to parse ChatGPT's answer but the format is inconsistent. For instance, is quiz_topic is &quot;literature&quot;, I get the following response:</p>
<pre><code>Q1. Who wrote the novel &quot;The Catcher in the Rye&quot;?

A. J.D. Salinger
B. Ernest Hemingway
C. F. Scott Fitzgerald
D. Mark Twain
</code></pre>
<p>This is the correct format. But if I use &quot;U.S History&quot;, I get the following output:</p>
<pre><code>Q1: Who was the first president of the United States?

A1: George Washington Jr., Thomas Jefferson, John Adams, George Washington 
</code></pre>
<p>This is a different format. So my goal is to fine-tune it using the article <a href=""https://platform.openai.com/docs/guides/fine-tuning"" rel=""nofollow noreferrer"">here</a></p>
<p>I have created 200 line of a training set. Here is a sample:</p>
<pre><code>{&quot;prompt&quot;:&quot;Write a quiz on general knowledge with 5 questions. Each question has 3 incorrect answers and 1 correct answer. The correct answer is always the last answer.Write each answer on a separate line \n\n---\n\n&quot;,&quot;completion&quot;:&quot; Q. What is the Italian word for &amp;quot;tomato&amp;quot;?\nA. Aglio\nA. Cipolla\nA. Peperoncino\nA. Pomodoro\n\nQ. Which restaurant&amp;#039;s mascot is a clown?\nA. Whataburger\nA. Burger King\nA. Sonic\nA. McDonald&amp;#039;s\n\nQ. Which of these colours is NOT featured in the logo for Google?\nA. Yellow\nA. Blue\nA. Green\nA. Pink\n\nQ. In 2013 how much money was lost by Nigerian scams?\nA. $95 Million\nA. $956 Million\nA. $2.7 Billion\nA. $12.7 Billion\n\nQ. What is the name of Poland in Polish?\nA. Pupcia\nA. Polszka\nA. P&amp;oacute;land\nA. Polska\n\n \n\n###\n\n&quot;}
{&quot;prompt&quot;:&quot;Write a quiz on books with 5 questions. Each question has 3 incorrect answers and 1 correct answer. The correct answer is always the last answer.Write each answer on a separate line \n\n---\n\n&quot;,&quot;completion&quot;:&quot; Q. What is the name of Eragon&amp;#039;s dragon in &amp;quot;Eragon&amp;quot;?\nA. Glaedr\nA. Thorn\nA. Arya\nA. Saphira\n\nQ. In the &amp;quot;The Hobbit&amp;quot;, who kills Smaug?\nA. Bilbo Baggins\nA. Gandalf the Grey\nA. Frodo\nA. Bard\n\nQ. What is Hermione Granger&amp;#039;s middle name?\nA. Jane\nA. Emma\nA. Jo\nA. Jean\n\nQ. According to The Hitchhiker&amp;#039;s Guide to the Galaxy book, the answer to life, the universe and everything else is...\nA. Loving everyone around you\nA. Chocolate\nA. Death\nA. 42\n\nQ. What is the name of the three headed dog in Harry Potter and the Sorcerer&amp;#039;s Stone?\nA. Spike\nA. Poofy\nA. Spot\nA. Fluffy\n\n \n\n###\n\n&quot;}
</code></pre>
<p>When I run the validation tool with the command</p>
<pre><code>openai tools fine_tunes.prepare_data -f training.jsonl
</code></pre>
<p>I get the following message</p>
<pre><code>- All prompts start with prefix `Write a quiz on `. Fine-tuning doesn't require the instruction specifying the task, or a few-shot example scenario. Most of the time you should only add the input data into the prompt, and the desired output into the completion
</code></pre>
<p>I don't understand why I must remove &quot;Write a quiz on&quot;. So I have misunderstood how to fine-tune a model for consistent formatting.
Can anybody shed a light on how to make sure I get the same formatting with the same prompt</p>
","gpt-3"
"75324242","GPT-J (6b): how to properly formulate autocomplete prompts","2023-02-02 13:59:41","","1","250","<jupyter-notebook><amazon-sagemaker><huggingface><gpt-3>","<p>I'm new to the AI playground and for this purpose I'm experimenting with the GPT-J (6b) model on an Amazon SageMaker notebook instance (g4dn.xlarge). So far, I've managed to register an endpoint and run the predictor but I'm sure I'm making the wrong questions or I haven't really understood how the model parameters work (which is probable).</p>
<p>This is my code:</p>
<pre><code># build the prompt
prompt = &quot;&quot;&quot;
language: es
match: comida
topic: hoteles en la playa todo incluido
output: ¬øSabes cu√°les son los mejores Hoteles Todo Incluido de Espa√±a? Cada vez son 
m√°s los que se suman a la moda del Todo Incluido para disfrutar de unas perfectas y 
completas vacaciones en familia, en pareja o con amigos. Y es que con nuestra oferta 
hoteles Todo Incluido podr√°s vivir unos d√≠as de aut√©ntico relax y una estancia mucho 
m√°s completa, ya que suelen incluir desde el desayuno, la comida y la cena, hasta 
cualquier snack y bebidas en las diferentes instalaciones del hotel. ¬øQu√© se puede 
pedir m√°s para relajarse durante una perfecta escapada? A continuaci√≥n, te 
presentamos los mejores hoteles Todo Incluido de Espa√±a al mejor precio.

language: es
match: comida
topic: hoteles en la playa todo incluido
output:
&quot;&quot;&quot;

# set the maximum token length
maximum_token_length = 25

# set the sampling temperature
sampling_temperature = 0.6

# build the predictor arguments
predictor_arguments = {
    &quot;inputs&quot;: prompt,
    &quot;parameters&quot;: {
        &quot;max_length&quot;: len(prompt) + maximum_token_length,
        &quot;temperature&quot;: sampling_temperature
    }
}

# execute the predictor with the prompt as input
predictor_output = predictor.predict(predictor_arguments)

# retrieve the text output
text_output = predictor_output[0][&quot;generated_text&quot;]

# print the text output
print(f&quot;text output: {text_output}&quot;)
</code></pre>
<p>My problem is I try to get a different response using the same parameters but I get nothing. It just repeats my inputs with an empty response so I'm definitely doing something wrong although the funny thing is I actually get a pretty understandable text output if I throw the same input with the same sampling temperature on the OpenAI playground (on text-davinci-003).</p>
<p>Can you give me a hint on what am I doing wrong? Oh, and another question is: How can I specify something like 'within the first 10 words' for a keyword match?</p>
","gpt-3"
"75322813","OpenAI GPT-3 API error: ""That model does not exist""","2023-02-02 11:56:10","75322907","1","4791","<node.js><openai-api><gpt-3>","<p>Get &quot;That model does not exist&quot; from api call in node.js</p>
<pre><code>const chatGptUrl = &quot;https://api.openai.com/v1/engines/chat-gpt/jobs&quot;;

...

const response = await axios.post(
      chatGptUrl,
      {
        prompt,
        max_tokens: 100,
        n: 1,
        stop: &quot;&quot;,
      },
      {
        headers: {
          &quot;Content-Type&quot;: &quot;application/json&quot;,
          &quot;Authorization&quot;: `Bearer ${chatGptApiKey}`,
        },
      }
    );

    const responseText = response.data.choices[0].text;
</code></pre>
","gpt-3"
"75313457","OpenAI API: openai.api_key = os.getenv() not working","2023-02-01 16:44:32","75313682","3","31868","<python><openai-api><chatgpt-api><gpt-3><gpt-4>","<p>I am just trying some simple functions in Python with OpenAI APIs but running into an error:</p>
<p>I have a valid API secret key which I am using.</p>
<p>Code:</p>
<pre><code>&gt;&gt;&gt; import os
&gt;&gt;&gt; import openai
&gt;&gt;&gt; openai.api_key = os.getenv(&quot;I have placed the key here&quot;)
&gt;&gt;&gt; response = openai.Completion.create(model=&quot;text-davinci-003&quot;, prompt=&quot;Say this is a test&quot;, temperature=0, max_tokens=7)
</code></pre>
<p><a href=""https://i.sstatic.net/zCgm4.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/zCgm4.png"" alt=""Simple test"" /></a></p>
","gpt-3"
"75304632","How to Use Edit images in OpenAi Kotlin Client","2023-02-01 00:55:40","","0","307","<android><android-studio><kotlin><openai-api><gpt-3>","<p>I am using openAi client with android kotlin (implementation <code>com.aallam.openai:openai-client:2.1.3</code>).</p>
<p>Is the path wrong or is <a href=""https://github.com/Aallam/openai-kotlin"" rel=""nofollow noreferrer"">the library</a> missing?</p>
<pre class=""lang-kotlin prettyprint-override""><code>val imgURL = Uri.parse(&quot;android.resource://&quot; + packageName + &quot;/&quot; + R.drawable.face3)
try {
    val images = openAI.image(
        edit = ImageEditURL( // or 'ImageEditJSON'
            image = FilePath(imgURL.toString()), // &lt;-
            mask = FilePath(imgURL.toString()), // &lt;-
            prompt = &quot;a sunlit indoor lounge area with a pool containing a flamingo&quot;,
            n = 1,
            size = ImageSize.is1024x1024
        )
    );
} catch (e: Exception) {
    println(&quot;error is here:&quot;+e)
}
</code></pre>
<p>As can be seen, it wants a path from me, but it does not succeed even though I give the path.</p>
","gpt-3"
"75299615","OpenAI API: Can I remove the line break from the response with a parameter?","2023-01-31 15:46:42","75300061","4","5491","<r><openai-api><gpt-3>","<p>I've starting using OpenAI API in R. I downloaded the <code>openai</code> package. I keep getting a double linebreak in the text response. Here's an example of my code:</p>
<pre class=""lang-r prettyprint-override""><code>
library(openai)

vector = create_completion(
  model = &quot;text-davinci-003&quot;,
  prompt = &quot;Tell me what the weather is like in London, UK, in Celsius in 5 words.&quot;,
  max_tokens = 20,
  temperature = 0,
  echo = FALSE
)


vector_2 = vector$choices[1]

vector_2$text


[1] &quot;\n\nRainy, mild, cool, humid.&quot;

</code></pre>
<p>Is there a way to get rid of this without 'correcting' the response text using other functions?</p>
","gpt-3"
"75266549","Fine-tune a davinci model to be similar to InstructGPT","2023-01-28 09:09:22","","1","1942","<openai-api><gpt-3>","<p>I have a few-shot GPT-3 text-davinci-003 prompt that produces &quot;pretty good&quot; results, but I quickly run out of tokens per request for interesting use cases. I have a data set (n~20) which I'd like to train the model with more but there is no way to fine-tune these InstructGPT models, only base GPT models.</p>
<p>As I understand it I can either:</p>
<ul>
<li>A: Find a way to harvest 10x more data (I don't see an easy option here)</li>
<li>or B: Find a way to fine-tune Davinci into something capable of simpler InstructGPT behaviours</li>
</ul>
<p>(Please let me know if there's a third option. I've attempted to increase epochs from 4 to 10 but the quality is really nowhere near as good).</p>
<p>Is there any way to fine-tune Davinci up to the point where it can model some of the things Instruct does? I don't need full capabilities, but if I can make it narrowed down to my use case it would be ideal.</p>
<p>--</p>
<p>By the way there is a common misconception that fine-tuning a GPT-3 model on a base (davinci, ada, babbage, etc...) will train it on the latest, eg: text-davinci-003. This is not how GPT works and is explained by GPT blog posts and support posts:
<a href=""https://help.openai.com/en/articles/6819989-can-i-fine-tune-on-text-davinci-003"" rel=""nofollow noreferrer"">https://help.openai.com/en/articles/6819989-can-i-fine-tune-on-text-davinci-003</a></p>
<p>Please don't claim <code>openai api fine_tunes.create -t &quot;model_prepared.jsonl&quot; -m &quot;davinci&quot;</code> will create a model based on text-davinci-003, it is not true, it uses base davinci.</p>
","gpt-3"
"75256485","Training / using OpenAI GPT-3 for translations","2023-01-27 09:56:11","","1","525","<nlp><translate><openai-api><machine-translation><gpt-3>","<p>I'm trying to use OpenAI for translation of my products descriptions from one language to some other languages (EN, DE, CZ, SK, HU, PL, SI...). The translations, especially to SK/CZ/HU/PL languages are (mainly gramatically) quite bad (using <code>text-davinci-003</code> model). I've got an idea - I already have a few thousands of similar products fully translated into all of these languages by professional translators. Is it possible to use those existing correct translations to train GPT-3 and then use this model to translate new texts? Has anybody already tried something similar?</p>
","gpt-3"
"75210324","OpenAI GPT-3 API error: ""You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth""","2023-01-23 13:39:13","75210429","1","4131","<php><openai-api><gpt-3>","<p>I am getting an error for the following PHP code:</p>
<pre><code>$curl = curl_init(&quot;https://api.openai.com/v1/engines/davinci/completions&quot;);

$data = array(
  'prompt' =&gt; 'how many sundays in 2023',
  'max_tokens' =&gt; 256,
  'temperature' =&gt; 0.7,
  'model' =&gt; 'text-davinci-003'
);

curl_setopt($curl, CURLOPT_POST, 1);
curl_setopt($curl, CURLOPT_POSTFIELDS, http_build_query($data));
curl_setopt($curl, CURLOPT_RETURNTRANSFER, 1);
curl_setopt($curl, CURLOPT_HTTPHEADER, ['Authorization: Bearer sk-MY-API-KEY']);
curl_setopt($curl, CURLOPT_HTTPHEADER, ['Content-Type: application/json']);

$result = curl_exec($curl);
curl_close($curl);

$result = json_decode($result);
print $result-&gt;choices[0]-&gt;text;
</code></pre>
<p>I correctly provided the API Key, but getting this error:</p>
<blockquote>
<p>Error message: You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY)</p>
</blockquote>
","gpt-3"
"75196860","What is the OpenAI API warning: To avoid an invalid_request_error, best_of was set to equal n. What is ""best of""?","2023-01-21 22:01:29","","0","599","<jupyter-lab><openai-api><gpt-2><gpt-3>","<p>This <strong>&quot;best of&quot;</strong> warning results from using the OpenAI API on a PC running Win10.</p>
<p><strong>The Context:</strong></p>
<p>Using the OpenAI API in Jupyter Lab with the ir kernel, with having only the rgpt3 library installed in this Notebook.</p>
<p>The API successfully performs a test code completion.  And it does not matter whether the API is making a single or multiple API request, both return the same warning.</p>
<p>The following results when using 3 queries:</p>
<blockquote>
<p>[1] &quot;Request: 1/3&quot; To avoid an <code>invalid_request_error</code>, <code>best_of</code> was
set to equal <code>n</code></p>
<p>[1] &quot;Request: 2/3&quot; To avoid an <code>invalid_request_error</code>, <code>best_of</code> was
set to equal <code>n</code></p>
<p>[1] &quot;Request: 3/3&quot; To avoid an <code>invalid_request_error</code>, <code>best_of</code> was
set to equal <code>n</code></p>
</blockquote>
<p>After performing multiple unsuccessful web searches - including a search at Stack Overflow for information about these warnings, I found there exists almost no information about this warning anywhere. It's probably too early in the process because the OpenAI API is relatively new to most people.</p>
<p>Therefore, it was decided to post both the question and the answer regarding this warning because otherwise finding such information is very difficult and time consuming.  And for those users who are boldly going where few have gone before, errors and warning messages do not inspire confidence.</p>
","gpt-3"
"75196859","How to make the bot multithreaded?","2023-01-21 22:00:57","","0","359","<python><telegram-bot><openai-api><aiogram><gpt-3>","<p>I am writing a telegram bot based on OpenAI. There is a problem with multithreading. When one user asks the bot, another person can get the same information.
For example:
First user: Do you know Konstantin Polukhin?
Bot: Yes, and begins to describe it..</p>
<p>The second user: Do you respect him?
Bot: Yes, I know Konstantin Polukhin.</p>
<p>It is necessary to make sure that the data does not overlap and the bot can say something related to the request of another user.</p>
<p>I tried many methods that were suggested, but none helped.</p>
<p>Code:</p>
<pre><code>from aiogram import Bot,types
import openai
import requests
from googletrans import Translator
from aiogram.utils import executor
from aiogram.dispatcher import Dispatcher


TOKEN = &quot; &quot;

bot = Bot(token=TOKEN)
openai.api_key = &quot; &quot;

dp = Dispatcher(bot)

my_list = [&quot; &quot;, &quot; &quot;, &quot;&quot;]

@dp.message_handler(content_types = [&quot;text&quot;])
async def start(message: types.Message):
    #print(&quot;\n&quot; + my_list[0] + &quot;\n&quot; + my_list[1])
    
    translator = Translator()
    
    dest_language = &quot;en&quot;
    translated_text = translator.translate(message.text, dest=dest_language).text
    my_list[2] = &quot;\n\nHuman: &quot; + translated_text + &quot;\n\nAI: &quot;

    response = openai.Completion.create(
        model=&quot;text-davinci-003&quot;,
        prompt=f&quot;{my_list[0] + my_list[1] + my_list[2]}&quot;,
        temperature=0.5,
        max_tokens=1024,
        top_p=1.0,
        frequency_penalty=0.5,
        presence_penalty=0.0)

    dest_language_2 = &quot;ru&quot;
    translated_text1= translator.translate(text=response['choices'][0]['text'], dest=dest_language_2).text

    await message.answer(translated_text1)

    my_list[1] = &quot;\n\nAI: &quot; + response.choices[0].text
    my_list[0] = &quot;\n\nHuman: &quot; + translated_text

if __name__ == '__main__':
    executor.start_polling(dp, skip_updates=False)```
</code></pre>
","gpt-3"
"75196414","OpenAI API error 500: ""The server had an error while processing your request"", 503: ""Service Unavailable"" or 504: ""Gateway Timeout""","2023-01-21 20:40:18","","2","5779","<python><openai-api><gpt-3>","<p>I created a Python script that loops through a list of text strings (each string is about 2000 characters in length) and summarizes each string. See the code for the response below (This prompt is within a for loop):</p>
<pre><code>response = openai.Completion.create( model=&quot;text-davinci-003&quot;, max_tokens=2000, prompt = f&quot;Summarize the following text: {text_list[i]}&quot;, temperature=0.5, frequency_penalty=1.5, presence_penalty=-1.5, n=1 )
</code></pre>
<p>It works for maybe 1 or 2 items in the text list but then I receive an error: <code>openai.error.APIError: The server had an error while processing your request. Sorry about that!</code></p>
<p>This happens consistently even when I use different api keys, prompts, accounts. I have also tried exponential backoff with no success. Any idea what is happening?</p>
","gpt-3"
"75176667","OpenAI GPT-3 API error: ""Cannot specify both model and engine""","2023-01-19 18:19:45","75182746","1","7712","<python><json><python-3.x><openai-api><gpt-3>","<p>So I'm working on some python code that works with chatgpt3. What it does is it sends a request with a prompt and then gets the reply, but I keep getting Errors. The error is</p>
<pre><code>Traceback (most recent call last):
  File &quot;main.py&quot;, line 16, in &lt;module&gt;
    print(response_json['choices'][0]['text'])
KeyError: 'choices'
</code></pre>
<p>Here is my code:</p>
<pre><code>import json
import requests
import os
data = {
    &quot;prompt&quot;: &quot;What is the meaning of life?&quot;,
    &quot;model&quot;: &quot;text-davinci-002&quot;
}

response = requests.post(&quot;https://api.openai.com/v1/engines/davinci/completions&quot;, json=data, headers={
    &quot;Content-Type&quot;: &quot;application/json&quot;,
    &quot;Authorization&quot;: f&quot;Bearer {apikey}&quot;,
})

response_json = json.loads(response.text)

print(response_json['choices'][0]['text'])

</code></pre>
<p>I do have an API key that is valid and the JSON code I don't get the JSON code.</p>
<pre><code>{'error': {'message': 'Cannot specify both model and engine', 'type': 'invalid_request_error', 'param': None, 'code': None}}
</code></pre>
<p>I have tried different API keys and that didn't work. i even looked up all the different models for chatgpt and it still doesn't work</p>
","gpt-3"
"75136962","OpenAI GPT-3 API error: ""TypeError: Converting circular structure to JSON"" using ExpressJS","2023-01-16 16:20:20","","1","1368","<node.js><express><openai-api><gpt-3>","<p>Just experimenting with OpenAI's api and have a very basic express app up and running. What I'm trying to do is just get it to send me back an appropriate response with a basic input but it currently keeps failing.</p>
<p>I'm using Postman to iterate on the code on localhost. All packages are definitely installed and the API key is correct and specfied in the .env file.</p>
<p>My current working file is below. I'm sure I'll kick myself but can anyone spot what dumb thing I've probably done?</p>
<pre><code>const express = require('express');
const app = express();
require('dotenv').config();
const bodyParser = require('body-parser');
app.use(bodyParser.json());
const axios = require('axios'); // Come back to this

const { Configuration, OpenAIApi } = require(&quot;openai&quot;);
const configuration = new Configuration({
    apiKey: process.env.OPENAI_API_KEY,
});

const openai = new OpenAIApi(configuration);

app.get('/api/v1', async (req, res) =&gt; {
    
  let body = {
      model: &quot;text-davinci-003&quot;,        
      prompt: &quot;How are you?&quot;,
      temperature: 1,
      max_tokens: 2086,
      top_p: 1,
      frequency_penalty: 0,
      presence_penalty: 0,
  };

  
  const response = await openai.createCompletion(body);

  res.send({ response });
});

// Listen for requests
app.listen(3000, function() {
    console.log('Server is listening on port 3000');
});
</code></pre>
<p><strong>Error generated in terminal</strong></p>
<pre><code>/home/mint-pc/Desktop/projects/ebooks/api/node_modules/express/lib/response.js:1150
    : JSON.stringify(value);
           ^

TypeError: Converting circular structure to JSON
    --&gt; starting at object with constructor 'ClientRequest'
    |     property 'socket' -&gt; object with constructor 'TLSSocket'
    --- property '_httpMessage' closes the circle
    at JSON.stringify (&lt;anonymous&gt;)
    at stringify (/home/mint-pc/Desktop/projects/ebooks/api/node_modules/express/lib/response.js:1150:12)
    at ServerResponse.json (/home/mint-pc/Desktop/projects/ebooks/api/node_modules/express/lib/response.js:271:14)
    at ServerResponse.send (/home/mint-pc/Desktop/projects/ebooks/api/node_modules/express/lib/response.js:162:21)
    at /home/mint-pc/Desktop/projects/ebooks/api/ghost_writer.js:48:7
</code></pre>
","gpt-3"
"75130116","Getting 400 Bad Request from Open AI API using Python Flask","2023-01-16 03:56:19","75130180","-2","1310","<flask><flask-restful><openai-api><gpt-3>","<p>I want to get response using Flask from OpenAI API. Whether I am getting Status 400 Bad Request from Browser through <code>http://127.0.0.1:5000/chat</code></p>
<h1>Bad Request</h1>
<p><em>The browser (or proxy) sent a request that this server could not understand.</em></p>
<p>Also I am checking this from <strong>Postman</strong></p>
<pre><code>from flask import Flask, request, render_template
import requests

app = Flask(__name__)

@app.route('/')
def index():
    return 'Welcome to ChatGPT app!'

@app.route('/chat', methods=['GET', 'POST'])
def chat():
    user_input = request.form['text']
    # Use OpenAI's API to generate a response from ChatGPT
    response = generate_response_from_chatgpt(user_input)
    return response

def generate_response_from_chatgpt(user_input):
    api_key = &quot;YOUR_API_KEY&quot;
    url = &quot;https://api.openai.com/v1/engines/davinci/completions&quot;
    headers = {
        &quot;Content-Type&quot;: &quot;application/json&quot;,
        &quot;Authorization&quot;: f&quot;Bearer {api_key}&quot;
    }
    data = {
        &quot;prompt&quot;: user_input,
        &quot;engine&quot;: &quot;davinci&quot;
    }
    response = requests.post(url, headers=headers, json=data)
    return response.json()[&quot;choices&quot;][0][&quot;text&quot;]


if __name__ == '__main__':
    app.run()
</code></pre>
","gpt-3"
"75112672","No module named 'openai_secret_manager'","2023-01-13 17:40:20","75124884","2","10945","<python><pandas><openai-api><gpt-3>","<p>I asked <strong>ChatGPT</strong> about my CSV data, and ChatGPT answered:</p>
<p>&quot;Here is an example of how you can read a CSV file using pandas, and then use the data to train or fine-tune <strong>GPT-3</strong> using the <strong>OpenAI</strong> API:&quot;</p>
<pre><code>import pandas as pd
import openai_secret_manager

# Read the CSV file
df = pd.read_csv(&quot;example.csv&quot;)

# Get the OpenAI API key
secrets = openai_secret_manager.get_secrets(&quot;openai&quot;)
openai_api_key = secrets[&quot;api_key&quot;]

# Use the data from the CSV file to train or fine-tune GPT-3
# (Assuming you have the OpenAI API key and the OpenAI Python library installed)
import openai
openai.api_key = openai_api_key
response = openai.Completion.create(
    engine=&quot;text-davinci-002&quot;,
    prompt=(f&quot;train on data from example.csv{df}&quot;),
    max_tokens=2048,
    n = 1,
    stop=None,
    temperature=0.5,
)
print(response[&quot;choices&quot;][0][&quot;text&quot;])
</code></pre>
<p>But, I got this error:</p>
<p><code>ModuleNotFoundError: No module named 'openai_secret_manager'</code></p>
","gpt-3"
"75106599","OpenAI GPT-3 API: Why am I getting different completions on Playground vs. the API?","2023-01-13 08:28:15","","2","2083","<php><artificial-intelligence><openai-api><gpt-3>","<p>I'm trying to use the Ada language processor of OpenAi to summarize a piece of text.
When I try to use their playground, the function works and I get a summarization that makes sense and can be used by humans.</p>
<p><img src=""https://i.sstatic.net/05jsg.png"" alt=""OpenAI Playground"" /></p>
<p>This is the cURL from the playground:</p>
<pre><code>curl https://api.openai.com/v1/completions \
  -H &quot;Content-Type: application/json&quot; \
  -H &quot;Authorization: Bearer $OPENAI_API_KEY&quot; \
  -d '{
  &quot;model&quot;: &quot;text-ada-001&quot;,
  &quot;prompt&quot;: &quot;Please write a one paragraph professional synopsis:\n\nSome text&quot;,
  &quot;temperature&quot;: 0,
  &quot;max_tokens&quot;: 60,
  &quot;top_p&quot;: 1,
  &quot;frequency_penalty&quot;: 0,
  &quot;presence_penalty&quot;: 0
}'
</code></pre>
<p>When I take this cURL and transform it to PHP code, it stops working, or better said it works but it returns complete nonsense, nothing similar to the results from the playground.</p>
<p>PHP code:</p>
<pre><code>$ch = curl_init();

    curl_setopt($ch, CURLOPT_URL, 'https://api.openai.com/v1/completions');
    curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);
    curl_setopt($ch, CURLOPT_POST, 1);
    $postFields = '{
            &quot;model&quot;: &quot;text-ada-001&quot;,
            &quot;prompt&quot;: &quot;Please write a one paragraph professional synopsis: ' . $text . '&quot;,
            &quot;temperature&quot;: 0,
            &quot;max_tokens&quot;: 500,
            &quot;top_p&quot;: 1,
            &quot;frequency_penalty&quot;: 0,
            &quot;presence_penalty&quot;: 0
        }';
    curl_setopt($ch, CURLOPT_POSTFIELDS, $postFields);

    $headers = array();
    $headers[] = 'Content-Type: application/json';
    $headers[] = 'Authorization: Bearer ' . $api_key;
    curl_setopt($ch, CURLOPT_HTTPHEADER, $headers);

    $result = curl_exec($ch);
    if (curl_errno($ch)) {
        echo 'Error:' . curl_error($ch);
    }
    curl_close($ch);
    return $result;
</code></pre>
<p>Now, I tried to use both a json code like this, and to write a PHP array and convert it to json, same result. I have also tried to use a <a href=""https://github.com/orhanerday/open-ai"" rel=""nofollow noreferrer"">library</a> but it also returned the same nonsense as before. I'm saying nonsense, because the text that it returns is not something that can be read and said, 'Hey, this is a proffesional synopsis'. I'm going to give an example of a sentence that I got in one of the iterations:</p>
<p>'It's not pretty and no I thought to myself, oh look IT'S NOT THAT REPUBLICAN kids would kno one of these things. OH IT'S A RESTRICTIOUS SCHOOL'.</p>
<p>I can assure you, there are no mentions of republicans or kids in the text that I'm processing.</p>
<p>My question is, am I doing something wrong? Does OpenAi work differently on their playground and in code?</p>
","gpt-3"
"75091786","OpenAI Unity - POST Request not working properly (400 status)","2023-01-12 04:58:07","75092193","4","503","<unity-game-engine><httprequest><openai-api><gpt-3>","<p>I'm connecting <a href=""https://beta.openai.com/docs/api-reference/completions/create"" rel=""nofollow noreferrer"">GPT3 OpenAI</a> but I just cant manage to make a proper POST request to it (I'm following some guides but for them it works...).</p>
<pre class=""lang-cs prettyprint-override""><code>private IEnumerator Upload ( )
{
    WWWForm form = new WWWForm();
    form.AddField ( &quot;prompt&quot;, prompt );
    form.AddField ( &quot;max_tokens&quot;, maxTokens );
    form.AddField ( &quot;model&quot;, model );
    form.AddField ( &quot;temperature&quot;, temperature );

    using ( UnityWebRequest wR = UnityWebRequest.Post ( &quot;https://api.openai.com/v1/completions&quot;, form ) )
    {
        wR.SetRequestHeader ( &quot;Authorization&quot;, &quot;Bearer &quot; + apiKey );
        wR.SetRequestHeader ( &quot;Content-Type&quot;, &quot;json&quot; );
        yield return wR.SendWebRequest ( );
        if ( wR.result != UnityWebRequest.Result.Success )
        {
            Debug.Log ( &quot;ERROR:\n&quot; + wR.error );
        }
        else
        {
            Debug.Log ( &quot;Success:\n&quot; + wR.result + &quot;\nUpload Completed!);
        }
    }
}
</code></pre>
<p>My code is always returning me a bad request (a.k.a <code>400 Bad Request</code>).</p>
","gpt-3"
"75084272","OpenAI GPT-3 API: How to keep the format of the response?","2023-01-11 14:02:46","76444852","3","6998","<openai-api><gpt-3>","<p>When I use GPT3's playground, I often get results that are formatted with numbered lists and paragraphs like below:</p>
<pre><code>Here's what the above class is doing:

1. It creates a directory for the log file if it doesn't exist.
2. It checks that the log file is newline-terminated.
3. It writes a newline-terminated JSON object to the log file.
4. It reads the log file and returns a dictionary with the following

- list 1
- list 2
- list 3
- list 4
</code></pre>
<p>However, when I directly use their API and extract the response from json result, I get the crammed text version that is very hard to read, something like this:</p>
<pre><code>Here's what the above class is doing:1. It creates a directory for the log file if it doesn't exist.2. It checks that the log file is newline-terminated.3. It writes a newline-terminated JSON object to the log file.4. It reads the log file and returns a dictionary with the following-list 1-list 2-list 3- list4
</code></pre>
<p>My question is, how do people keep the formats from GPT results so they are displayed in a neater, more readable way?</p>
","gpt-3"
"75067851","How to fix Python pip install openai error: subprocess-exited-with-error","2023-01-10 09:23:45","","4","19221","<python><python-3.x><windows><openai-api><gpt-3>","<p>I'm trying to install OpenAI with Python 3.11, Windows OS, pip fully upgraded, and I got this error.</p>
<p>Here is the full error message:</p>
<pre><code>Collecting openai
  Using cached openai-0.26.0.tar.gz (54 kB)
  Installing build dependencies ... done
  Getting requirements to build wheel ... error
  error: subprocess-exited-with-error

  √ó Getting requirements to build wheel did not run successfully.
  ‚îÇ exit code: 1
  ‚ï∞‚îÄ&gt; [21 lines of output]
      Traceback (most recent call last):
        File &quot;C:\Users\vocal\AppData\Local\Programs\Python\Python311\Lib\site-packages\pip\_vendor\pep517\in_process\_in_process.py&quot;, line 351, in &lt;module&gt;
          main()
        File &quot;C:\Users\vocal\AppData\Local\Programs\Python\Python311\Lib\site-packages\pip\_vendor\pep517\in_process\_in_process.py&quot;, line 333, in main
          json_out['return_val'] = hook(**hook_input['kwargs'])
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        File &quot;C:\Users\vocal\AppData\Local\Programs\Python\Python311\Lib\site-packages\pip\_vendor\pep517\in_process\_in_process.py&quot;, line 118, in get_requires_for_build_wheel
          return hook(config_settings)
                 ^^^^^^^^^^^^^^^^^^^^^
        File &quot;C:\Users\vocal\AppData\Local\Temp\pip-build-env-lr3fjsgg\overlay\Lib\site-packages\setuptools\build_meta.py&quot;, line 338, in get_requires_for_build_wheel
          return self._get_build_requires(config_settings, requirements=['wheel'])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        File &quot;C:\Users\vocal\AppData\Local\Temp\pip-build-env-lr3fjsgg\overlay\Lib\site-packages\setuptools\build_meta.py&quot;, line 320, in _get_build_requires
          self.run_setup()
        File &quot;C:\Users\vocal\AppData\Local\Temp\pip-build-env-lr3fjsgg\overlay\Lib\site-packages\setuptools\build_meta.py&quot;, line 485, in run_setup
          self).run_setup(setup_script=setup_script)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        File &quot;C:\Users\vocal\AppData\Local\Temp\pip-build-env-lr3fjsgg\overlay\Lib\site-packages\setuptools\build_meta.py&quot;, line 335, in run_setup
          exec(code, locals())
        File &quot;&lt;string&gt;&quot;, line 13, in &lt;module&gt;
      UnicodeDecodeError: 'cp949' codec can't decode byte 0xe2 in position 1031: illegal multibyte sequence
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
error: subprocess-exited-with-error

√ó Getting requirements to build wheel did not run successfully.
‚îÇ exit code: 1
‚ï∞‚îÄ&gt; See above for output.

note: This error originates from a subprocess, and is likely not a problem with pip.

</code></pre>
<p>I have no idea how to solve this error. Can anybody give me a hint?</p>
<pre><code>UnicodeDecodeError: 'cp949' codec can't decode byte 0xe2 in position 1031: illegal multibyte sequence
</code></pre>
<p>Because of that message, I tried <a href=""https://stackoverflow.com/questions/10487563/unicode-error-handling-with-python-3s-readlines"">this</a> solution and it didn't work.</p>
","gpt-3"
"75051126","Open AI's GPT Davinci - Asking it questions, but it's returning gibberish?","2023-01-08 20:41:38","75104866","0","458","<c#><openai-api><gpt-3>","<p>I ask it &quot;What is COVID-19?&quot;, by using the following code:</p>
<pre><code>string key = &quot;XXXX&quot;;

OpenAIAPI api = new OpenAIAPI(key,Engine.Davinci);

var results = api.Completions.CreateCompletionsAsync(new CompletionRequest(&quot;What is COVID-19?&quot;, temperature: 0.1, max_tokens: 200), 1).Result;

var answer = results.Completions[0].Text;
</code></pre>
<p>Which returns:</p>
<blockquote>
<p>Covid-19 is a drug that is used to treat the symptoms of schizophrenia. It is a combination of two drugs, clozapine and olanzapine.</p>
<p>How does COVID-19 work?</p>
<p>Covid-19 is a combination of two drugs, clozapine and olanzapine. Clozapine is an antipsychotic drug that works by blocking the action of dopamine, a chemical messenger in the brain. Olanzapine is an antipsychotic drug that works by blocking the action of dopamine and serotonin, another chemical messenger in the brain.</p>
<p>How is COVID-19 used?</p>
<p>Covid-19 is used to treat schizophrenia.</p>
<p>How does COVID-19 work?</p>
<p>Covid-19 is a combination of two drugs, clozapine and olanzapine. Clozapine is an antipsychotic drug that works by blocking</p>
</blockquote>
<p>I'm baffled, what am I doing wrong here? Shouldn't this emulate similar results to ChatGPT? I am using the following NuGet for OpenAI access: <a href=""https://github.com/OkGoDoIt/OpenAI-API-dotnet"" rel=""nofollow noreferrer"">https://github.com/OkGoDoIt/OpenAI-API-dotnet</a></p>
","gpt-3"
"75049140","OpenAI GPT-3 API error 429: ""Request failed with status code 429""","2023-01-08 15:46:00","","2","8762","<vue.js><openai-api><gpt-3>","<p>I'm trying to connect OpenAI API to my Vue.js project. Everything is OK but every time I try to POST request, I get a <strong>429 status code (too many request)</strong> but I didn't even had the chance to make one. Any help?</p>
<p>Response:</p>
<pre class=""lang-json prettyprint-override""><code>{
    &quot;message&quot;: &quot;Request failed with status code 429&quot;,
    &quot;name&quot;: &quot;Error&quot;,
    &quot;stack&quot;: &quot;Error: Request failed with status code 429\n    at createError (C:\\Users\\sim\\Documents\\SC\\server\\node_modules\\axios\\lib\\core\\createError.js:16:15)\n    at settle (C:\\Users\\sim\\Documents\\SC\\server\\node_modules\\axios\\lib\\core\\settle.js:17:12)\n    at IncomingMessage.handleStreamEnd (C:\\Users\\sim\\Documents\\SC\\server\\node_modules\\axios\\lib\\adapters\\http.js:322:11)\n    at IncomingMessage.emit (events.js:412:35)\n    at endReadableNT (internal/streams/readable.js:1333:12)\n    at processTicksAndRejections (internal/process/task_queues.js:82:21)&quot;,
    &quot;config&quot;: {
        &quot;transitional&quot;: {
            &quot;silentJSONParsing&quot;: true,
            &quot;forcedJSONParsing&quot;: true,
            &quot;clarifyTimeoutError&quot;: false
        },
        &quot;transformRequest&quot;: [
            null
        ],
        &quot;transformResponse&quot;: [
            null
        ],
        &quot;timeout&quot;: 0,
        &quot;xsrfCookieName&quot;: &quot;XSRF-TOKEN&quot;,
        &quot;xsrfHeaderName&quot;: &quot;X-XSRF-TOKEN&quot;,
        &quot;maxContentLength&quot;: -1,
        &quot;maxBodyLength&quot;: -1,
        &quot;headers&quot;: {
            &quot;Accept&quot;: &quot;application/json, text/plain, */*&quot;,
            &quot;Content-Type&quot;: &quot;application/json&quot;,
            &quot;User-Agent&quot;: &quot;OpenAI/NodeJS/3.1.0&quot;,
            &quot;Authorization&quot;: &quot;Bearer secret&quot;,
            &quot;Content-Length&quot;: 137
        },
        &quot;method&quot;: &quot;post&quot;,
        &quot;data&quot;: &quot;{\&quot;model\&quot;:\&quot;text-davinci-003\&quot;,\&quot;prompt\&quot;:\&quot;option-2\&quot;,\&quot;temperature\&quot;:0,\&quot;max_tokens\&quot;:3000,\&quot;top_p\&quot;:1,\&quot;frequency_penalty\&quot;:0.5,\&quot;presence_penalty\&quot;:0}&quot;,
        &quot;url&quot;: &quot;https://api.openai.com/v1/completions&quot;
    },
    &quot;status&quot;: 429
}
</code></pre>
<p>My method in Vue.js:</p>
<pre class=""lang-js prettyprint-override""><code>async handleSelect() {
      try {
        const res = await fetch(&quot;http://localhost:8000/&quot;, {
          method: &quot;POST&quot;,
          headers: {
            &quot;Content-Type&quot;: &quot;application/json&quot;,
          },
          body: JSON.stringify({
            question: this.selectedOption,
          })
        })

        const data = await res.json();
        console.log(data);
      } catch {
        console.log(data);
      }
    }
</code></pre>
<p>on server side</p>
<pre class=""lang-js prettyprint-override""><code>app.post(&quot;/&quot;, async (req, res) =&gt; {
  try {
    const question = req.body.question;

    const response = await openai.createCompletion({
      model: &quot;text-davinci-003&quot;,
      prompt: `${question}`,
      temperature: 0, // Higher values means the model will take more risks.
      max_tokens: 3000, // The maximum number of tokens to generate in the completion. Most models have a context length of 2048 tokens (except for the newest models, which support 4096).
      top_p: 1, // alternative to sampling with temperature, called nucleus sampling
      frequency_penalty: 0.5, // Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
      presence_penalty: 0, // Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
    });
    // console.log(response);
    res.status(200).send({
      bot: response.data.choices[0].text,
    });
  } catch (error) {
    // console.error(error);
    res.status(500).send(error || &quot;Something went wrong&quot;);
  }
});
</code></pre>
","gpt-3"
"75046073","Python + Open AI/GPT3 question: Why is part of my prompt spilling into the responses I receive?","2023-01-08 07:16:08","","0","600","<python><machine-learning><artificial-intelligence><gpt-3>","<p>This happens to probably 10% of responses I get. For whatever reason, the last bits of my prompt somehow spill into it, at the start of it. Like there will be a period, or a question mark, or sometimes a few of the last letters from the prompt, that get removed from the prompt, and somehow find their way into BOTH the response that gets printed inside of the Visual Studio Code terminal, AND in the outputted version that gets written to a corresponding Excel spreadsheet.</p>
<p>Any reason why this might happen?</p>
<p>Some example responses:</p>
<blockquote>
<p>.</p>
<p>Most apples are colored red.</p>
</blockquote>
<p>Also</p>
<blockquote>
<p>?</p>
<p>Most rocks are colored gray.</p>
</blockquote>
<p>Another example:</p>
<blockquote>
<p>for it.</p>
<p>Most oceans are colored blue.</p>
</blockquote>
<p>The period, the question mark, &quot; for it&quot; somehow get transposed FROM the end of the prompt, and tacked onto the response. And they even get removed from the prompt that was originally in the Excel spreadsheet to begin with.</p>
<p>Could this be a bug with xlsxwriter? open ai? Some combo of both?</p>
<p>Code here:</p>
<pre><code>import xlsxwriter
import openpyxl

import os
import openai

filename = f'testing-openai-gpt3-requests-v1.xlsx'
wb = openpyxl.load_workbook(filename, read_only=False)
sheet = wb.active

# print(&quot;starting number of ideas is:&quot;)
# print(sheet.max_row)

for x in range(sheet.max_row):
    c = sheet.cell(row = x+1, column = 1)
    # print(c.value) 

    myCurrentText = c.value 
    myCurrentPrompt = &quot;What is the color of most of the following objects: &quot; + myCurrentBusinessIdea

    openai.api_key = [none of your business]

    response = openai.Completion.create(
    model = &quot;text-davinci-003&quot;,
    prompt = myCurrentPrompt,
    max_tokens = 1000,
    )

    TheOutputtedSummary = response['choices'][0]['text']

    print(TheOutputtedSummary)
    sheet.cell(row = x+1, column = 6).value = TheOutputtedSummary


wb.save(str(filename))
print('All finished!')
</code></pre>
","gpt-3"
"75041247","What's the correct URL to test OpenAI API?","2023-01-07 14:50:32","75043933","1","16358","<curl><openai-api><gpt-3>","<p>I'm trying to test the GPT-3 API with a request using curl in Windows CMD:</p>
<pre><code>curl -X POST -H &quot;Content-Type: application/json&quot; -H &quot;Authorization: Bearer MY_KEY&quot; -d &quot;{\&quot;text\&quot;: \&quot;is this working\&quot;}&quot; https://api.openai.com/v1/conversations/text-davinci-003/messages
</code></pre>
<p>Given that I did change &quot;MY_KEY&quot; for my key.</p>
<p>But I got:</p>
<pre><code>{
  &quot;error&quot;: {
    &quot;message&quot;: &quot;Invalid URL (POST /v1/conversations/text-davinci-003/messages)&quot;,
    &quot;type&quot;: &quot;invalid_request_error&quot;,
    &quot;param&quot;: null,
    &quot;code&quot;: null
  }
}
</code></pre>
<p>I also tried the model name as <code>text-davinci-002</code> and <code>text-davinci-001</code>, but get the same invalid URL error. What's the correct URL here? I can't find it on the docs (or in chatGPT itself).</p>
","gpt-3"
"75026428","Error in formating the URL for chatGPT's API","2023-01-06 02:41:49","","1","502","<java><gpt-3>","<p>I am trying to make a program where a user can asks GPT-3 a question through its API.</p>
<p>I tried to get GPT-3's assistant to design code for me, however there were some errors because it uses outdated information from 2021. Below is my modified code after going through the documentation, but I still cant get it to work, it is generating a 'java.io.FileNotFoundException' error.</p>
<p>I believe the problem is with the formatting of the completion section of my URL, however I am not sure. If anyone could tell me what's wrong it would be greatly appreciated.</p>
<pre><code>import java.io.BufferedReader;
import java.io.IOException;
import java.io.InputStreamReader;
import java.net.HttpURLConnection;
import java.net.URL;
import java.net.URLEncoder;

public class ChatGPT{

    public static void main(String[] args) throws IOException {
        String prompt = &quot;What country has the most moderate weather?&quot;;
        String model = &quot;text-curie-001&quot;;
        String apiKey = /*My API key*/;

        // Encode the prompt and construct the API request URL
        String url = String.format(
            &quot;https://api.openai.com/v1/completions?model=%s&amp;prompt=%s&quot;,
            model,
            URLEncoder.encode(prompt, &quot;UTF-8&quot;)
        );

        // Create the request
        HttpURLConnection conn = (HttpURLConnection) new URL(url).openConnection();
        conn.setRequestMethod(&quot;GET&quot;);
        conn.setRequestProperty(&quot;Authorization&quot;, &quot;Bearer &quot; + apiKey);

        // Make the request and retrieve the response
        BufferedReader reader = new BufferedReader(new InputStreamReader(conn.getInputStream()));
        StringBuilder responseBody = new StringBuilder();
        String line;
        while ((line = reader.readLine()) != null) {
            responseBody.append(line);
        }
        reader.close();

        // Print the response
        System.out.println(responseBody);
    }
}
</code></pre>
<p>I know my API key is valid because changing the url to whats shown below outputs the appropriate information:</p>
<pre><code>String url = String.format(
            &quot;https://api.openai.com/v1/models/%s&quot;,
            model
        );
</code></pre>
<p>the format &quot;/v1/models/text-curie-001&quot; outputs the details for the model 'text-curie-001'</p>
<p>the format &quot;/v1/completions...&quot; outputs a response based on the given prompt.</p>
","gpt-3"
"74996136","extend dialogflow webhook deadline time for gpt api call","2023-01-03 16:36:55","","0","349","<python><dialogflow-es><gpt-3>","<p>I am trying to use a script I found on the internet to extend the maximum time for a webhook request through Google Dialogflow (max 5 seconds to timeout). I need to extend the time because I make an API call to openai and it sometimes takes longer than 5 seconds. My idea was to start the 2 functions in parallel. The broadbridge_webhook_results() function is there to extend the time by triggering a followupEventInput at Dialogflow after 3,5 seconds, so a new call comes through Dialogflow and the 5 seconds start from new. This goes apparently up to 2 times. In the meantime the API call should be made towards openai. As soon as the API call was successful, the answer should be sent back to Dialogflow. Unfortunately, I am currently not getting anywhere and I think that the thread functionality was set up or understood incorrectly by me.</p>
<p>The following code I have so far:</p>
<pre><code>import os
import openai
import time
import backoff
from datetime import datetime, timedelta
from flask import Flask, request, render_template
from threading import Thread
import asyncio

app = Flask(__name__)

conversation_History = &quot;&quot;
user_Input = &quot;&quot;
reply=''
answer = &quot;&quot;

@app.route('/') 
def Default(): 
    return render_template('index.html')

@backoff.on_exception(backoff.expo, openai.error.RateLimitError)
def ask(question):
  global conversation_History
  global answer
  global reply
  openai.api_key = os.getenv(&quot;gtp_Secret_Key&quot;)  
  #start_sequence = &quot;\nAI:&quot;
  #restart_sequence = &quot;\nHuman: &quot;
  response = openai.Completion.create(
    model=&quot;text-davinci-003&quot;,
    prompt=&quot;I am a chatbot from OpenAI. I'm happy to answer your questions.\nHuman:&quot; + conversation_History + &quot; &quot;+ question +&quot;\nAI: &quot;,    
    temperature=0.9,
    max_tokens=500,
    top_p=1,
    frequency_penalty=0,
    presence_penalty=0.6,
    stop=[&quot; Human:&quot;, &quot; AI:&quot;]
    )
  conversation_History = conversation_History + question + &quot;\nAI&quot; + answer + &quot;\nHuman:&quot;
  answer = response.choices[0].text  

def broadbridge_webhook_results():
  global answer
  
  now = datetime.now()
  current_time = now.strftime(&quot;%H:%M:%S&quot;)
  print(&quot;Current Time =&quot;, current_time)

  extended_time = now + timedelta(seconds=3)
  print(&quot;extended Time =&quot;, extended_time.time())

  req = request.get_json(force=True)

  action = req.get('queryResult').get('action')
  reply=''

  if action=='input.unknown' or action=='input.welcome':
    time.sleep(3.5)

    if now&lt;=extended_time and not len(answer) == 0:
      reply={
              &quot;fulfillmentText&quot;: answer,
              &quot;source&quot;: &quot;webhookdata&quot;
            }

    reply={      
      &quot;followupEventInput&quot;: {        
        &quot;name&quot;: &quot;extent_webhook_deadline&quot;,        
        &quot;languageCode&quot;: &quot;en-US&quot;
        }
      }

  if action=='followupevent':
    print(&quot;enter into first followup event&quot;)
    time.sleep(3.5)

    if now&lt;=extended_time and not len(answer) == 0:
      reply={
        &quot;fulfillmentText&quot;: answer,
        &quot;source&quot;: &quot;webhookdata&quot;
      }

    reply={      
      &quot;followupEventInput&quot;: {        
        &quot;name&quot;: &quot;extent_webhook_deadline_2&quot;,        
        &quot;languageCode&quot;: &quot;en-US&quot;
        }
      }
    
  if action=='followupevent_2':
    print(&quot;enter into second followup event&quot;)
    time.sleep(3.5)

    reply={
      &quot;fulfillmentText&quot;: answer,
      &quot;source&quot;: &quot;webhookdata&quot;
    }
        
    print(&quot;Final time of execution:=&gt;&quot;, now.strftime(&quot;%H:%M:%S&quot;))

@app.route('/webhook', methods=['GET', 'POST'])
def webhook():
  global answer
  global reply

  answer=&quot;&quot;
  req = request.get_json(silent=True, force=True)
  user_Input = req.get('queryResult').get('queryText')
  Thread(target=broadbridge_webhook_results()).start()
  Thread(target=ask(user_Input)).start()
  
  return reply

#conversation_History = conversation_History + user_Input + &quot;\nAI&quot; + answer + &quot;\nHuman:&quot;
#if now&lt;=extended_time and not len(answer) == 0:
  
if __name__ == '__main__':
  app.run(host='0.0.0.0', port=8080)
</code></pre>
","gpt-3"
"74988365","OpenAI GPT-3 API error: ""AttributeError: module 'openai' has no attribute 'GPT'""","2023-01-03 01:29:18","","0","2590","<python><module><attributeerror><openai-api><gpt-3>","<p>I have the latest version of OpenAi, but some of the attributes are missing. I have tried to reinstall it, didn't solve it. GPT and Chat are the ones i've discovered not working yet.</p>
<p>Bare in mind, i'm new to python and have basic knowledge of the language. The code is taken from GitHub</p>
<p>My code if it tells you something:</p>
<pre><code>import openai
import pyttsx3
import speech_recognition as sr
from api_key import API_KEY


openai.api_key = API_KEY

engine = pyttsx3.init()

r = sr.Recognizer()
mic = sr.Microphone(device_index=1)


conversation = &quot;&quot;
user_name = &quot;You&quot;
bot_name = &quot;DAI&quot;

while True:
    with mic as source:
        print(&quot;\nlistening...&quot;)
        r.adjust_for_ambient_noise(source, duration=0.2)
        audio = r.listen(source)
    print(&quot;no longer listening.\n&quot;)

    try:
        user_input = r.recognize_google(audio)
    except:
        continue

    prompt = user_name + &quot;: &quot; + user_input + &quot;\n&quot; + bot_name+ &quot;: &quot;

    conversation += prompt  # allows for context

    # fetch response from open AI api
    response = openai.Completion.create(engine='text-davinci-003', prompt=conversation, max_tokens=100)
    response_str = response[&quot;choices&quot;][0][&quot;text&quot;].replace(&quot;\n&quot;, &quot;&quot;)
    response_str = response_str.split(user_name + &quot;: &quot;, 1)[0].split(bot_name + &quot;: &quot;, 1)[0]

    conversation += response_str + &quot;\n&quot;
    print(response_str)

    engine.say(response_str)
    engine.runAndWait()

</code></pre>
<p>All help will be appreciated</p>
<p><strong>Edit:</strong></p>
<p>The last answer made the error go away, but it put out a new one:</p>
<p>Thank you, it worked. But it still didn't work, I still get an error. I will try to resolve it if I can. Would appreciate any help and tips I get. This is probably not the last error I will encounter.</p>
<pre><code>This is the error: Traceback (most recent call last):
  File &quot;/Users/danieforsell22b/Desktop/GPT3VoiceBot/gpt3Bot.py&quot;, line 22, in &lt;module&gt;
    r.adjust_for_ambient_noise(source, duration=0.2)
  File &quot;/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/speech_recognition/__init__.py&quot;, line 569, in adjust_for_ambient_noise
    assert source.stream is not None, &quot;Audio source must be entered before adjusting, see documentation for ``AudioSource``; are you using ``source`` outside of a ``with`` statement?&quot;
           ^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: Audio source must be entered before adjusting, see documentation for ``AudioSource``; are you using ``source`` outside of a ``with`` statement?

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;/Users/danieforsell22b/Desktop/GPT3VoiceBot/gpt3Bot.py&quot;, line 20, in &lt;module&gt;
    with mic as source:
  File &quot;/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/speech_recognition/__init__.py&quot;, line 201, in __exit__
    self.stream.close()
    ^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'close'
</code></pre>
","gpt-3"
"74986827","OpenAISwift package works for ios not for Mac","2023-01-02 20:36:06","","0","452","<macos><swiftui><appstore-sandbox><entitlements><gpt-3>","<p>I have been following instructions to build a simple SwiftUI GPT-3 client using the OpenAISwift client library. The app works as expected on iOS but when I try to run a macos version I am getting these errors:</p>
<p>2023-01-02 15:07:14.845094-0500 GPT2[35955:1083936] [] networkd_settings_read_from_file Sandbox is preventing this process from reading networkd settings file at &quot;/Library/Preferences/com.apple.networkd.plist&quot;, please add an exception.
2023-01-02 15:07:14.845261-0500 GPT2[35955:1083936] [] networkd_settings_read_from_file Sandbox is preventing this process from reading networkd settings file at &quot;/Library/Preferences/com.apple.networkd.plist&quot;, please add an exception.
2023-01-02 15:07:15.078105-0500 GPT2[35955:1086396] [] nw_resolver_can_use_dns_xpc_block_invoke Sandbox does not allow access to com.apple.dnssd.service</p>
<p>I found another macos OpenAIKit project on gitub stating that the following need to be added to info.plist for macos:</p>
<pre><code>&lt;plist version=&quot;1.0&quot;&gt;
&lt;dict&gt;
    &lt;key&gt;com.apple.security.app-sandbox&lt;/key&gt;
    &lt;true/&gt;
    &lt;key&gt;com.apple.security.files.user-selected.read-only&lt;/key&gt;
    &lt;true/&gt;
    &lt;key&gt;com.apple.security.network.client&lt;/key&gt;
    &lt;true/&gt;
    &lt;key&gt;com.apple.security.network.server&lt;/key&gt;
    &lt;true/&gt;
&lt;/dict&gt;
&lt;/plist&gt;
</code></pre>
<p>but I did not see these choices available in the XCode 14 project properties info section. I would have tried pasting the dict object in to a text version of the info.plist but I could not see a way to edit the info.plist as a text.</p>
<p>Here is the simple code I am using:</p>
<pre><code>import SwiftUI
import OpenAISwift

final class ViewModel: ObservableObject {
    init() {}
    
    private var client: OpenAISwift?
    
    func setup() {
        client = OpenAISwift(authToken: &quot;MYKEYHERE&quot;)
       
        
    }
    
    func send(text: String,
        completion: @escaping (String) -&gt; Void) {
            client?.sendCompletion(with: text,
                           maxTokens: 500,
                           completionHandler: {result in
        
        switch result {
        case .success(let model):
            let output = model.choices.first?.text ?? &quot;&quot;
            completion(output)
        case .failure:
            break
        }
    })
}
}

struct ContentView: View {
    @ObservedObject var viewModel = ViewModel()
    @State var text = &quot;&quot;
  @State var models = [String]()
    
    var body: some View {
        VStack(alignment: .leading) {
            ForEach(models, id: \.self) { string in
                Text(string)
            }
            
            Spacer()
            
            HStack {
                TextField(&quot;Type here ...&quot;, text: $text)
                Button(&quot;Send&quot;) {
                    send()
                }
            }
        }
        .onAppear{
            viewModel.setup()
        }.padding()
        
    }
    
    func send() {
        guard !text.trimmingCharacters(in: .whitespaces).isEmpty else {
            return
        }
        models.append(&quot;Me: \(text)&quot;)
        viewModel.send(text: text) { response in
            DispatchQueue.main.async {
                self.models.append(&quot;GPT: &quot; + response)
                self.text = &quot;&quot;
            }
            
        }
    }
}

struct ContentView_Previews: PreviewProvider {
    static var previews: some View {
        ContentView()
    }
}
</code></pre>
<p>How can I get this multiplatform app running on macos Ventura 13.1? Thanks for any help.</p>
","gpt-3"
"74978793","OpenAI GPT-3 API error: ""InvalidRequestError: Unrecognized request argument supplied""","2023-01-02 03:28:18","74992998","0","6387","<python><artificial-intelligence><openai-api><gpt-3>","<pre><code>import openai

# Set the API key
openai.api_key = &quot;YOUR API KEY&quot;

# Define the conversation memory
conversation_memory = {
    &quot;previous_question&quot;: &quot;What is the capital of France?&quot;,
    &quot;previous_answer&quot;: &quot;The capital of France is Paris.&quot;
}

# Make the API request
response = openai.Completion.create(
    model=&quot;text-davinci-003&quot;,
    prompt=&quot;Where is the Eiffel Tower located?&quot;,
    temperature=0.5,
    max_tokens=1024,
    top_p=1,
    frequency_penalty=0,
    presence_penalty=0,
    conversation_memory=conversation_memory
)

# Print the response
print(response.text)
</code></pre>
<p>Why the conversation_memory  parameter not being recognize. I try this with serveral different models and they all give me the same error. I have the lastest OpenAi on my computer. I don't understand.</p>
<p>Here the error:</p>
<pre><code>     InvalidRequestError                       Traceback (most recent call last) &lt;ipython-input-17-ace11d6ce405&gt; in &lt;module&gt;      11      12 # Make the API request ---&gt; 13 response = openai.Completion.create(      14     model=&quot;text-babbage-001&quot;,      15     prompt=&quot;Where is the Eiffel Tower located?&quot;, C:\ProgramData\Anaconda3\lib\site-packages\openai\api_resources\completion.py in create(cls, *args, **kwargs)      23 while True:      24 try: ---&gt; 25 return super().create(*args, **kwargs)      26 except TryAgain as e:      27 if timeout is not None and time.time() &gt; start + timeout: C:\ProgramData\Anaconda3\lib\site-packages\openai\api_resources\abstract\engine_api_resource.py in create(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)     113         )     114         url = cls.class_url(engine, api_type, api_version) --&gt; 115         response, _, api_key = requestor.request(     116 &quot;post&quot;,     117             url, C:\ProgramData\Anaconda3\lib\site-packages\openai\api_requestor.py in request(self, method, url, params, headers, files, stream, request_id, request_timeout)     179             request_timeout=request_timeout,     180         ) --&gt; 181 resp, got_stream = self._interpret_response(result, stream)     182 return resp, got_stream, self.api_key     183 C:\ProgramData\Anaconda3\lib\site-packages\openai\api_requestor.py in _interpret_response(self, result, stream)     394 else:     395             return ( --&gt; 396                 self._interpret_response_line(     397                     result.content, result.status_code, result.headers, stream=False     398                 ),  C:\ProgramData\Anaconda3\lib\site-packages\openai\api_requestor.py in _interpret_response_line(self, rbody, rcode, rheaders, stream)     427         stream_error = stream and &quot;error&quot; in resp.data     428 if stream_error or not 200 &lt;= rcode &lt; 300: --&gt; 429             raise self.handle_error_response(     430                 rbody, rcode, resp.data, rheaders, stream_error=stream_error     431             ) 
 InvalidRequestError: Unrecognized request argument supplied: conversation_memory 
</code></pre>
","gpt-3"
"74976042","How to receive ChatGPT multi-line replies when using CURL?","2023-01-01 17:04:20","","-2","2430","<javascript><node.js><express><curl><gpt-3>","<p>The code below works. The problem is when ChatGPT replies only 1 line is rendered to the terminal and the rest of the text is cut off. I am not familiar with curl commands. How do I update the code so that multi-line replies are rendered?</p>
<p><strong>EDIT:</strong>  I tried console.log() the response inside the express post request believing that CURL was the problem. It appears <strong>CURL is not the problem</strong> and ChatGPT simply cuts off mid reply</p>
<pre><code>const express = require(&quot;express&quot;);
const cors = require(&quot;cors&quot;);
const bodyParser = require(&quot;body-parser&quot;);

const { Configuration, OpenAIApi } = require(&quot;openai&quot;);

const configuration = new Configuration({
  apiKey: &quot;my-api-key&quot;,
});
const openai = new OpenAIApi(configuration);

// Set up the server
const app = express();
app.use(bodyParser.json());
app.use(cors())

// Set up the ChatGPT endpoint
app.post(&quot;/chat&quot;, async (req, res) =&gt; {
  // Get the prompt from the request
  const { prompt } = req.body;

  // Generate a response with ChatGPT
  const completion = await openai.createCompletion({
    model: &quot;text-davinci-002&quot;,
    prompt: prompt,
  });

  console.log(completion.data.choices[0].text) // still cuts off
  res.send(completion.data.choices[0].text);
});

// Start the server
const port = 8080;
app.listen(port, () =&gt; {
  console.log(`Server listening on port ${port}`);
});
</code></pre>
<p>Curl</p>
<pre><code>curl -X POST -H &quot;Content-Type: application/json&quot; -d '{&quot;prompt&quot;:&quot;Hello, how are you doing today?&quot;}' http://localhost:8080/chat
</code></pre>
","gpt-3"
"74972916","How to ping the ChatGPT via curl and retain the state of conversation","2023-01-01 02:48:17","","3","1279","<javascript><node.js><express><curl><gpt-3>","<p>The code below is working.
I can curl questions to ChatGPT and it replies on a one-off basis.
However, if I try to engage in a conversation that require the state of the previous submissions to be referenced, the chat can not follow.</p>
<p>I would like to know what I need to do (and the code needed) to retain the context of the conversation</p>
<pre><code>const express = require(&quot;express&quot;);
const cors = require(&quot;cors&quot;);
const bodyParser = require(&quot;body-parser&quot;);

const { Configuration, OpenAIApi } = require(&quot;openai&quot;);

const configuration = new Configuration({
  apiKey: &quot;sk-my-key&quot;,
});
const openai = new OpenAIApi(configuration);

// Set up the server
const app = express();
app.use(bodyParser.json());
app.use(cors())

// Set up the ChatGPT endpoint
app.post(&quot;/chat&quot;, async (req, res) =&gt; {
  // Get the prompt from the request
  const { prompt } = req.body;

  // Generate a response with ChatGPT
  const completion = await openai.createCompletion({
    model: &quot;text-davinci-002&quot;,
    prompt: prompt,
  });
  res.send(completion.data.choices[0].text);
});

// Start the server
const port = 8080;
app.listen(port, () =&gt; {
  console.log(`Server listening on port ${port}`);
});
</code></pre>
<hr />
<p>CURL being run in new terminal:</p>
<pre><code>curl -X POST -H &quot;Content-Type: application/json&quot; -d '{&quot;prompt&quot;:&quot;Hello, how are you doing today?&quot;}' http://localhost:8080/chat
</code></pre>
","gpt-3"
"74969653","OpenAI and Javascript error : Getting 'TypeError: Cannot read properties of undefined (reading 'create') at Object.<anonymous>""","2022-12-31 12:51:52","","2","9737","<javascript><typeerror><openai-api><gpt-3>","<p>I am sorry for basic question but getting no where with what seems to be a very basic piece of code. I have npm installed latest version of openai. I am getting a constant error in my terminal:</p>
<pre><code>TypeError: Cannot read properties of undefined (reading 'create')
    at Object.&lt;anonymous&gt; (/Users/michalchojnacki/Desktop/Coding/OpenAi2/code.js:9:20)
    at Module._compile (node:internal/modules/cjs/loader:1159:14)
    at Module._extensions..js (node:internal/modules/cjs/loader:1213:10)
    at Module.load (node:internal/modules/cjs/loader:1037:32)
    at Module._load (node:internal/modules/cjs/loader:878:12)
    at Function.executeUserEntryPoint [as runMain] (node:internal/modules/run_main:81:12)
    at node:internal/main/run_main_module:23:47
</code></pre>
<p>Code:</p>
<pre><code>const openai = require('openai');

openai.apiKey = &quot;my API here&quot;;

const prompt = &quot;What is the capital of France?&quot;;

const model = &quot;davinci&quot;;

openai.completions.create({
  engine: model,
  prompt: prompt,
  max_tokens: 2048,
  n: 1,
  stop: '.',
  temperature: 0.5,
}, (error, response) =&gt; {
  if (error) {
    console.log(error);
  } else {
    console.log(response.choices[0].text);
  }
});
</code></pre>
<p>Would be grateful for any help!</p>
<p>I was expecting the terminal to give me the response to the prompt</p>
","gpt-3"
"74924661","OpenAI GPT-3 API error: ""openai.completions is not a function""","2022-12-26 23:24:29","","4","5384","<typescript><next.js><openai-api><gpt-3>","<p>I am trying to run the test code in the tutorial <a href=""https://harishgarg.com/writing/building-a-chat-app-with-gpt-3-reactjs-and-nextjs-a-step-by-step-guide/"" rel=""nofollow noreferrer"">https://harishgarg.com/writing/building-a-chat-app-with-gpt-3-reactjs-and-nextjs-a-step-by-step-guide/</a> and I get</p>
<blockquote>
<p>TypeError: openai.completions is not a function</p>
</blockquote>
<p>from the following code I put in my.js and run with &quot;node my.js&quot; in git bash window on Windows 10</p>
<pre><code>
    const openai = require('openai');
    openai.apiKey = &quot;api-key&quot;;
    openai.completions({
         engine: &quot;text-davinci-003&quot;,
                   prompt: &quot;Hello, how are you?&quot;,
                   max_tokens: 32,
                   n: 1,
                   stop: &quot;.&quot;,
                   temperature: 0.5,
                  }).then((response) =&gt; {
                      console.log(response.data.choices[0].text);
    });



</code></pre>
<p>I have tried various alternate code snippets from OpenAI docs and some suggested in other questions but have not been able to get it to work.</p>
","gpt-3"
"74916280","Error: That model does not exist (OpenAI)","2022-12-25 22:55:23","","-1","3625","<openai-api><gpt-3>","<p>When using a model I fine-tuned for GPT-3 using <code>openai api</code> from CLI, it stopped working and I get an error with this message: &quot;That model does not exist&quot;.</p>
<p>But this is a model I have used before, so it should exist.</p>
","gpt-3"
"74903974","OpenAI ""We could not parse the JSON body of your request.""","2022-12-23 21:02:51","","2","1038","<javascript><google-chrome-extension><openai-api><gpt-3>","<p>I'm trying to write a Chrome extension that takes the webpage text and send it to ChatGPT. Here's my JS code:</p>
<pre><code>document.addEventListener('DOMContentLoaded', function () {
var getBattleCardButton = document.getElementById('get-battle-card-button');
getBattleCardButton.addEventListener('click', function () {
    // Get all the content on the page as a string
    var pageContent = document.body.innerText;

    // Truncate the content to a maximum length of 4096 tokens
    var truncatedContent = pageContent.substring(0, 4096);

    // Construct the prompt by concatenating the specific words with the page content
    var prompt = &quot;Summarize this content into 3 sections, What? Why? and How?. Each section with 3 concise points. &quot; + truncatedContent;

    // Send the page content to Chat GPT as a prompt
    fetch('https://api.openai.com/v1/completions', {
        method: 'POST',
        headers: {
            'Content-Type': 'application/json',
            'Authorization': 'Bearer MY_API_KEY'
        },
        body: JSON.stringify({
            'prompt': prompt,
            'model': 'text-davinci-003',
            'max_tokens': 512
        })
    })
        .then(response =&gt; response.json())
        .then(data =&gt; {
            // Open a new tab and insert the Chat GPT output into it
            var newTab = window.open();
            newTab.document.body.innerHTML = data.response;
        })
        .catch(error =&gt; {
            console.error('Error:', error);
        });
});
</code></pre>
<p>});</p>
<p>I'm getting the error &quot;We could not parse the JSON body of your request.&quot; and the output is 'undefined'. What am I doing wrong?</p>
<p>PS: the HTML and CSS on the extension works just fine.</p>
","gpt-3"
"74860930","Is it possible to use continue in the Open AI API?","2022-12-20 09:28:32","","0","2626","<openai-api><gpt-3>","<p>I am testing using the Open AI API with the end-point:</p>
<pre><code>https://api.openai.com/v1/completions
</code></pre>
<p>But, I find that on the website <a href=""https://chat.openai.com/chat"" rel=""nofollow noreferrer"">https://chat.openai.com/chat</a> , we can simplely use the continue to ask the AI to give the answer with the context. And we can use continue mulittime to expand more the the max token limit.</p>
<p>Is it possible to use &quot;continue&quot; in the API or have the same effect?</p>
<p>I have try to use the  <code>user</code> field in the API, but still not work.</p>
","gpt-3"
"74823070","Can you create a custom model using GPT-3 to answer questions only about a specific topic?","2022-12-16 10:21:20","74823327","1","861","<nlp><chatbot><gpt-3>","<p>I'm using GPT-3 to create a chatbot that can answer questions related to a specific topic. Can GPT-3 be trained to detect questions that are irrelevant to the topic and refuse to answer them?</p>
<p>Example: Let's say I want to create a chatbot that can only answer questions about
Javascript. If it is asked to list the seven wonders of the world, it should refuse to answer.</p>
","gpt-3"
"74774018","How to keep the conversation going with OpenAI API PHP sdk","2022-12-12 16:10:02","","4","7145","<php><openai-api><gpt-3>","<p>I'm trying to keep a conversation going using the completion() method with <a href=""https://github.com/orhanerday/open-ai"" rel=""nofollow noreferrer"">OpenAI PHP SDK</a>.</p>
<ul>
<li>Prompt #1: &quot;How Are You?&quot;</li>
<li>Prompt #2: &quot;What I asked you before?&quot;</li>
</ul>
<p>but the AI seems to forget what i asked before. and it reply with random answers to the second prompt.</p>
<p>The code i'm using for the 2 calls are these:</p>
<pre><code>
   $call1 = $open_ai-&gt;completion([
            'model' =&gt; 'text-davinci-003', 
            'prompt' =&gt; 'How Are You?',

        ]);


        $call2 = $open_ai-&gt;completion([
            'model' =&gt; 'text-davinci-003', 
            'prompt' =&gt; 'What i asked you before?',
        ]);

</code></pre>
<p>What am I missing? How can i keep the session alive between these two calls in order to make the AI remember what I asked before?</p>
","gpt-3"
"74721925","Problems getting GPT-3 to conduct statistical analysis of a dataframe or json file","2022-12-07 19:19:50","","4","985","<javascript><python><pandas><openai-api><gpt-3>","<p>I'm having problems getting gpt-3 to do simple statistical summaries of a dataframe/json file - using python pandas as the suggested prompt, oddly it does categorical analysis of  the df, ie value_counts etc but seems to hallucinate when faced with numbers but i'm sure its just my error. I'm not sure if its the way i'm feeding the df in or something else? cheers.</p>
<pre><code>import pandas as pd 
import openai 
from sqlalchemy import create_engine

openai.api_key =  &quot;API_KEY&quot;

query = &quot;Select * from table&quot;
engine = create_engine()

df = pd.read_sql_query(query, engine)

completion = openai.Completion.create(
    engine=&quot;text-davinci-003&quot;,
    temperature=0.5,
    max_tokens=100,
    n = 1,
    stop=None,
   prompt = (f&quot;print sum of values in df called {df}&quot; ) ) `

a = completion.choices[0].text  
print(a)
</code></pre>
<p>i tried to use different versions of the file and different ways of calling but to no effect, changed engines also</p>
","gpt-3"
"74715461","OpenAI Python API is giving gibberish responses for the query ""hi""","2022-12-07 10:51:47","","2","841","<python><discord.py><openai-api><gpt-3>","<p>I used Python to access the OpenAI API, then used discord.py to integrate it into a Discord bot. My command looks like this:</p>
<pre class=""lang-py prettyprint-override""><code>@bot.command()
async def chat(ctx, *, input: str):
    openai.api_key = os.getenv(envName)
    
    async with ctx.channel.typing():
        response = openai.Completion.create(
                    engine=&quot;text-davinci-003&quot;,  # latest model (the one used for GPT-3)
                    prompt=input,
                    temperature=random.randrange(50, 90) / 100,
                    max_tokens=1000,
                    top_p=1,
                    frequency_penalty=0,
                    presence_penalty=0,
                    timeout=10
                )

        output = response.choices[0].text

    await ctx.reply(output)
</code></pre>
<p>The command works properly as intended. I give it an input, and it gives me a proper output.</p>
<p>However, I recently discovered that for a simple input such as &quot;hi&quot;, it gives me some gibberish output. Moreover, this output is completely different for each time.</p>
<p>Please refer to the images below for its output.</p>
<p>Try 1: <a href=""https://i.sstatic.net/N9GJp.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/N9GJp.png"" alt=""Try 1"" /></a></p>
<p>Try 2: <a href=""https://i.sstatic.net/w9zbQ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/w9zbQ.png"" alt=""Try 2"" /></a></p>
<p>Try 3: <a href=""https://i.sstatic.net/Nwq2s.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Nwq2s.png"" alt=""Try 3"" /></a></p>
<hr />
<p>Note that this command works completely fine for any other queries: for example,</p>
<p>Correct response: <a href=""https://i.sstatic.net/4Cn6S.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/4Cn6S.png"" alt=""Print &quot;hello world&quot; (responded correctly)"" /></a></p>
<p>and another one: <a href=""https://i.sstatic.net/eTrX2.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/eTrX2.png"" alt=""Wrote a speech"" /></a></p>
<p>Strangely, it gives a normal response for &quot;hello&quot; also. <a href=""https://i.sstatic.net/xbqhS.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/xbqhS.png"" alt=""&quot;hello&quot; query"" /></a></p>
<hr />
<p>What could be the reason why it responds improperly for a simple &quot;hi&quot; query?</p>
","gpt-3"
"74687645","GPT3 conversational Discord bot stops working after 5-6 sentences","2022-12-05 11:44:18","","0","460","<discord><gpt-3>","<p>I am creating a fun Discord bot which talks like my imaginary crush, lol!</p>
<p>The problem is that its working for the first 5-6 lines but then gives this error</p>
<pre><code>import openai

openai.api_key = &quot;Hidden&quot;

DISCORD_BOT_TOKEN = &quot;Hidden&quot;

start_sequence = &quot;\nAnamika:&quot;
restart_sequence = &quot;\n\nFaraaz:&quot;


session_prompt = &quot;This is a conversation between me (Faraaz) and my crush Anamika. Keep the replies long, and such that the person can reply. Anamika is a really gorgeous happy girl. She lives in New York. She loves Spiderman, coding and poetry. She loves Andrew Garfield amongst all spider men, in coding she is fascinated by boom of AI. She loves John Milton and writes poetry herself as well. Initially Anamika will be reluctant to my flirting but slowly she will start to like it. If the conversation goes very good, she can agree to become my girlfriend or marry me.Faraaz:- Hi! what is your name? Anamika:- Hey! My name is Anamika, how are you Faraaz! Faraaz:- I am really good, actually you were looking really pretty so wanted to come and talk to you. Anamika:- Oh really, that is interesting. So what did you intend to talk about?!&quot;



chat_log = None

import discord  

client = discord.Client(intents=discord.Intents.all())

@client.event


async def on_message(message):
    # Don't respond to messages sent by the bot itself
    global chat_log

    if message.author == client.user:
        return  
    print(chat_log)
    if chat_log == None:
        chat_log = session_prompt

    #print(message.content)

    #chat_log = f'{chat_log}{restart_sequence} {question}{start_sequence}{answer}'

    # Use the GPT-3 API to generate a response to the message
    response = openai.Completion.create(
        engine=&quot;text-davinci-003&quot;,
        #prompt=&quot;I recently moved to New York and I love design. I'm fascinated by technology and the growth of AI, but I realize that anything we build for the future must be rooted in the core desires of humans. &quot; + message.content,
         
    #return f'{chat_log}{restart_sequence} {question}{start_sequence}{answer}'
        #chat_log = f'{chat_log}{restart_sequence} {question}{start_sequence}{answer}'
        
        prompt = f'{chat_log}{restart_sequence}{message.content}',

        #prompt =  f'{chat_log}{restart_sequence}: {question}{start_sequence}:'  
        max_tokens=700,
        n=1,
        temperature=0.5,
        stop=[&quot;\n&quot;]
    )

    # Send the response back to the Discord channel
    await message.channel.send(response[&quot;choices&quot;][0][&quot;text&quot;])

    chat_log = f'{chat_log}{restart_sequence}{message.content}{start_sequence}{response[&quot;choices&quot;][0][&quot;text&quot;]}'



client.run(DISCORD_BOT_TOKEN)

</code></pre>
<p>I am seeing this error
<a href=""https://i.sstatic.net/ULo8K.png"" rel=""nofollow noreferrer"">Error</a></p>
<p><a href=""https://i.sstatic.net/GyiRX.png"" rel=""nofollow noreferrer"">The Discord Chat, after this messages not coming</a></p>
<p>I tried changing the max_tokens and also the prompt but to no avail. I have given administrator permissions to the bot.</p>
","gpt-3"
"74666268","OpenAI: Stream interrupted (client disconnected)","2022-12-03 11:24:58","74744388","11","7158","<openai-api><gpt-3>","<p>I'm trying OpenAI.</p>
<p>I have prepared the training data, and used <code>fine_tunes.create</code>. Several minutes later, it showed <code>Stream interrupted (client disconnected)</code>.</p>
<pre><code>$ openai api fine_tunes.create -t data_prepared.jsonl
Upload progress: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47.2k/47.2k [00:00&lt;00:00, 44.3Mit/s]
Uploaded file from data_prepared.jsonl: file-r6dbTH7rVsp6jJMgbX0L0bZx
Created fine-tune: ft-JRGzkYfXm7wnScUxRSBA2M2h
Streaming events until fine-tuning is complete...

(Ctrl-C will interrupt the stream, but not cancel the fine-tune)
[2022-12-02 11:10:08] Created fine-tune: ft-JRGzkYfXm7wnScUxRSBA2M2h
[2022-12-02 11:10:23] Fine-tune costs $0.06
[2022-12-02 11:10:24] Fine-tune enqueued. Queue number: 11

Stream interrupted (client disconnected).
To resume the stream, run:

  openai api fine_tunes.follow -i ft-JRGzkYfXm7wnScUxRSBA2M2h
</code></pre>
<p>I tried <code>fine_tunes.follow</code>, several minutes later, it still failed:</p>
<pre><code>$ openai api fine_tunes.follow -i ft-JRGzkYfXm7wnScUxRSBA2M2h
[2022-12-02 11:10:08] Created fine-tune: ft-JRGzkYfXm7wnScUxRSBA2M2h
[2022-12-02 11:10:23] Fine-tune costs $0.06
[2022-12-02 11:10:24] Fine-tune enqueued. Queue number: 11

Stream interrupted (client disconnected).
To resume the stream, run:

  openai api fine_tunes.follow -i ft-JRGzkYfXm7wnScUxRSBA2M2h
</code></pre>
<p><code>openai api fine_tunes.list</code> showed:</p>
<pre><code>$ openai api fine_tunes.list
{
  &quot;data&quot;: [
    {
      &quot;created_at&quot;: 1669975808,
      &quot;fine_tuned_model&quot;: null,
      &quot;hyperparams&quot;: {
        &quot;batch_size&quot;: 2,
        &quot;learning_rate_multiplier&quot;: 0.1,
        &quot;n_epochs&quot;: 4,
        &quot;prompt_loss_weight&quot;: 0.01
      },
      &quot;id&quot;: &quot;ft-JRGzkYfXm7wnScUxRSBA2M2h&quot;,
      &quot;model&quot;: &quot;curie&quot;,
      &quot;object&quot;: &quot;fine-tune&quot;,
      &quot;organization_id&quot;: &quot;org-YyoQqNIrjGHYDnKt9t3T6x2J&quot;,
      &quot;result_files&quot;: [],
      &quot;status&quot;: &quot;pending&quot;,
      &quot;training_files&quot;: [
        {
          &quot;bytes&quot;: 47174,
          &quot;created_at&quot;: 1669975808,
          &quot;filename&quot;: &quot;data_prepared.jsonl&quot;,
          &quot;id&quot;: &quot;file-r6dbTH7rVsp6jJMgbX0L0bZx&quot;,
          &quot;object&quot;: &quot;file&quot;,
          &quot;purpose&quot;: &quot;fine-tune&quot;,
          &quot;status&quot;: &quot;processed&quot;,
          &quot;status_details&quot;: null
        }
      ],
      &quot;updated_at&quot;: 1669975824,
      &quot;validation_files&quot;: []
    }
  ],
  &quot;object&quot;: &quot;list&quot;
}
</code></pre>
<p>And <code>$ openai api completions.create -m ft-JRGzkYfXm7wnScUxRSBA2M2h -p aprompt</code> returned <code>Error: That model does not exist (HTTP status code: 404)</code>.</p>
<p>Could anyone help?</p>
","gpt-3"
"74656790","Prepare json file for GPT","2022-12-02 13:48:53","74657190","1","2816","<python><nlp><gpt-3>","<p>I would like to create a dataset to use it for fine-tuning GPT3. As I read from the following site <a href=""https://beta.openai.com/docs/guides/fine-tuning"" rel=""nofollow noreferrer"">https://beta.openai.com/docs/guides/fine-tuning</a>, the dataset should look like this</p>
<pre><code>{&quot;prompt&quot;: &quot;&lt;prompt text&gt;&quot;, &quot;completion&quot;: &quot;&lt;ideal generated text&gt;&quot;}
{&quot;prompt&quot;: &quot;&lt;prompt text&gt;&quot;, &quot;completion&quot;: &quot;&lt;ideal generated text&gt;&quot;}
{&quot;prompt&quot;: &quot;&lt;prompt text&gt;&quot;, &quot;completion&quot;: &quot;&lt;ideal generated text&gt;&quot;}
...
</code></pre>
<p>For this reason I am creating the dataset with the following way</p>
<pre><code>import json

# Data to be written
dictionary = {
    &quot;prompt&quot;: &quot;&lt;text1&gt;&quot;, &quot;completion&quot;: &quot;&lt;text to be generated1&gt;&quot;}, {
    &quot;prompt&quot;: &quot;&lt;text2&gt;&quot;, &quot;completion&quot;: &quot;&lt;text to be generated2&gt;&quot;}

with open(&quot;sample2.json&quot;, &quot;w&quot;) as outfile:
    json.dump(dictionary, outfile)
</code></pre>
<p>However, when I am trying to load it, it looks like this which is not as we want</p>
<pre><code>import json
 
# Opening JSON file
with open('sample2.json', 'r') as openfile:
 
    # Reading from json file
    json_object = json.load(openfile)
 
print(json_object)
print(type(json_object))

&gt;&gt; [{'prompt': '&lt;text1&gt;', 'completion': '&lt;text to be generated1&gt;'}, {'prompt': '&lt;text2&gt;', 'completion': '&lt;text to be generated2&gt;'}]
&lt;class 'list'&gt;
</code></pre>
<p><strong>Could you please let me know how can I face this problem?</strong></p>
","gpt-3"
"74524530","How to get the items inside of an OpenAIobject in Python?","2022-11-21 20:26:49","74524554","7","10920","<python><openai-api><gpt-3>","<p>I would like to get the text inside this data structure that is outputted via GPT3 OpenAI. I'm using Python. When I print the object I get:</p>
<pre><code>&lt;OpenAIObject text_completion id=cmpl-6F7ScZDu2UKKJGPXTiTPNKgfrikZ at 0x7f7648cacef0&gt; JSON: {
  &quot;choices&quot;: [
    {
      &quot;finish_reason&quot;: &quot;stop&quot;,
      &quot;index&quot;: 0,
      &quot;logprobs&quot;: null,
      &quot;text&quot;: &quot;\nWhat was Malcolm X's original name?\nMalcolm X's original name was Malcolm Little.\n\nWhere was Malcolm X born?\nMalcolm X was born in Omaha, Nebraska.\n\nWhat was the profession of Malcolm X's father?\nMalcolm X's father was a Baptist minister.\n\nWhat did Malcolm X do after he stopped attending school?\nMalcolm X became involved in petty criminal activities.&quot;
    }
  ],
  &quot;created&quot;: 1669061618,
  &quot;id&quot;: &quot;cmpl-6F7ScZDu2gJJHKZSPXTiTPNKgfrikZ&quot;,
  &quot;model&quot;: &quot;text-davinci-002&quot;,
  &quot;object&quot;: &quot;text_completion&quot;,
  &quot;usage&quot;: {
    &quot;completion_tokens&quot;: 86,
    &quot;prompt_tokens&quot;: 1200,
    &quot;total_tokens&quot;: 1286
  }
}
</code></pre>
<p>How do I get the 'text' component of this?
For example, if this object is called: qa ... I can output</p>
<pre><code>qa['choices']
</code></pre>
<p>And I get the same items as above... but adding a <code>.text</code> or ['text'] to this does not do it, and gets an error.</p>
<p>But not sure how to isolate the 'text'
I've read the docs, but cannot find this... <a href=""https://beta.openai.com/docs/api-reference/files/delete?lang=python"" rel=""noreferrer"">https://beta.openai.com/docs/api-reference/files/delete?lang=python</a></p>
","gpt-3"
"74262671","openai using python, returned length problem in ""text-davinci-002"" model","2022-10-31 11:35:48","","1","1273","<python><openai-api><gpt-3>","<p>I am trying to use <strong>&quot;text-davinci-002&quot;</strong> model using <strong>&quot;openai&quot;</strong>. The returned text is a single sentence while the same sentence returns a full text in openAI official example.
This is the code used:</p>
<pre><code>response = openai.Completion.create(
            model=&quot;email to ask for a promotion&quot;,
            prompt=userPrompt,
            temperature=0.76
            )
</code></pre>
<p>The output of this code is:
*Hello [Employer],</p>
<p>I would like to request a promotion*</p>
<p>while the same sentence in OpenAI website <a href=""https://beta.openai.com/docs/quickstart/introduction"" rel=""nofollow noreferrer"">here</a> outputs:
*
Hello [Employer],</p>
<p>I would like to request a promotion to the position of [position you want]. I have been with the company for [amount of time] and I feel that I have the experience and qualifications needed for the position.*</p>
<p>Thank you in advance</p>
","gpt-3"
"74255038","What's your approach to extracting a summarized paragraph from multiple articles using GPT-3?","2022-10-30 17:00:45","","1","464","<machine-learning><nlp><summarization><openai-api><gpt-3>","<p>In the following scenario, what's your best approach using GPT-3 API?</p>
<ol>
<li>You need to come out with a short paragraph, about a <strong>specific subject</strong></li>
<li>You must base your paragraph on a set of articles, 3-6 articles, written in an unknown structure</li>
</ol>
<p>Here is what I found to work well:</p>
<ol>
<li>The main constraint is the open ai token limit in the prompt</li>
<li>Due to the constraint, I'd ask OPT-3 to parse unstructured data using the specific subject in the prompt request.</li>
<li>I'll then iterate each article and save it all into 1 string variable</li>
<li>Then, repeat it one last time but using the new string variable</li>
<li>If the article is too long, I'll cut it into smaller chunks</li>
<li>Of curse fine-tune, the model with the specific subject before will produce much better results</li>
<li>The <code>temperature</code> should be set to <code>0</code>, to make sure GPT-3 uses only facts from the data source.</li>
</ol>
<p>Example:
Let's say I want to write a paragraph about Subject A, Subject B, and  Subject C. And I have 5 articles as references.
The open ai playground will look something like this:</p>
<pre><code>Example Article 1
----
Subject A: example A for OPT-3
Subject B: n/a
Subject c: n/a
=========
Example Article 2
----
Subject A: n/a
Subject B: example B for GPT-3
Subject C: n/a
=========
Example Article 3
----
Subject A: n/a
Subject B: n/a
Subject c: example for GPT-3
=========
Article 1
-----
Subject A:
Subject B:
Subject C:
=========
... repeating with all articles, save to str
=========
str
-----
Subject A:
Subject B:
Subject C:
</code></pre>
","gpt-3"
"74000154","How do I make sure answers are from a customized (fine-tuning) dataset?","2022-10-08 19:58:07","","9","8942","<nlp><customization><openai-api><gpt-3>","<p>I'm using customized text with 'Prompt' and 'Completion' to train new model.</p>
<p>Here's the tutorial I used to create customized model from my data:</p>
<p><a href=""https://beta.openai.com/docs/guides/fine-tuning/advanced-usage"" rel=""nofollow noreferrer"">beta.openai.com/docs/guides/fine-tuning/advanced-usage</a></p>
<p>However even after training the model and sending prompt text to the model, I'm still getting generic results which are not always suitable for me.</p>
<p>How I can make sure completion results for my prompts will be only from the text I used for the model and not from the generic OpenAI models?</p>
<p>Can I use some flags to eliminate results from generic models?</p>
","gpt-3"
"73999135","Open AI. Response from openai.Completion.create incomplete","2022-10-08 17:28:49","","1","2780","<python><openai-api><gpt-3>","<p>I have this code in Python. I would like to use the &quot;openai.Completion.create&quot; function:</p>
<pre><code>import os

import openai
from flask import Flask, redirect, render_template, request, url_for

app = Flask(__name__)
openai.api_key = os.getenv(&quot;OPENAI_API_KEY&quot;)


@app.route(&quot;/&quot;, methods=(&quot;GET&quot;, &quot;POST&quot;))
def index():
    if request.method == &quot;POST&quot;:
        animal = request.form[&quot;animal&quot;]
        response = openai.Completion.create(
            model=&quot;text-davinci-002&quot;,
            prompt=generate_prompt(animal),
            temperature=1,
        )
        return redirect(url_for(&quot;index&quot;, result=response.choices[0].text))

    result = request.args.get(&quot;result&quot;)
    return render_template(&quot;index.html&quot;, result=result)


def generate_prompt(animal):
    return format(animal.capitalize()
    )
</code></pre>
<p>Why the completion is not complete?</p>
<p><a href=""https://i.sstatic.net/9F5Yj.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/9F5Yj.png"" alt=""enter image description here"" /></a></p>
","gpt-3"
"73972852","GPT3 completion with insertion - invalid argument :suffix","2022-10-06 11:14:08","73972895","2","948","<openai-api><gpt-3>","<p>I am trying out completions using insertions.</p>
<p>It seems that I am supposed to use a parameter called <code>suffix:</code> to inform where the end of the insert goes.</p>
<p><a href=""https://i.sstatic.net/fYU8s.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/fYU8s.png"" alt=""enter image description here"" /></a></p>
<h2>The payload to the endpoint: <code>POST /v1/completions</code></h2>
<pre class=""lang-json prettyprint-override""><code>{
  &quot;model&quot;: &quot;code-davinci-002&quot;,
  &quot;prompt&quot;: &quot;Write a JSON document for a person with first name, last name, email and phone number\n\n{\n&quot;,
  &quot;suffix&quot;: &quot;\n}&quot;,
  &quot;temperature&quot;: 0,
  &quot;max_tokens&quot;: 256,
  &quot;top_p&quot;: 1,
  &quot;frequency_penalty&quot;: 0,
  &quot;presence_penalty&quot;: 0
}
</code></pre>
<p>I tried doing this from a ruby implementation of GPT3.</p>
<pre class=""lang-rb prettyprint-override""><code>parameters
=&gt; {
:model=&gt;&quot;code-davinci-001&quot;,
 :prompt=&gt;&quot;generate some JSON for a person with first and last name {&quot;,
 :max_tokens=&gt;250,
 :temperature=&gt;0,
 :top_p=&gt;1,
 :frequency_penalty=&gt;0,
 :presence_penalty=&gt;0,
 :suffix=&gt;&quot;\n}&quot;}
</code></pre>
<pre class=""lang-rb prettyprint-override""><code>post(url: &quot;/v1/completions&quot;, parameters: parameters)
</code></pre>
<p>I get an invalid argument error for <code>suffix</code></p>
<pre class=""lang-rb prettyprint-override""><code>{&quot;error&quot;=&gt;{&quot;message&quot;=&gt;&quot;Unrecognized request argument supplied: suffix&quot;, &quot;type&quot;=&gt;&quot;invalid_request_error&quot;, &quot;param&quot;=&gt;nil, &quot;code&quot;=&gt;nil}}
</code></pre>
","gpt-3"
"73938457","Reproducibility when using best_of in GPT-3 settings","2022-10-03 16:46:24","","2","1601","<random-seed><gpt-3>","<p>I want to do some tests using GPT-3. Instead of setting temperature = 0, I would like to use the best_of function. However, this gives me non-reproducible results since they differ in each iteration of code execution. Does anyone have an idea how I could achieve reproducible/deterministic results and still use best_of? Is there a way to use a random seed in the API for GPT-3?</p>
<p>Thanks!</p>
","gpt-3"
"73919757","GPT-3 Twitter Bot","2022-10-01 15:54:03","","1","504","<openai-api><gpt-3>","<p>I'm trying to fine-tune a GPT-3 model on my tweets. I want the model to generate tweets with no prompt. Is it possible?</p>
<p>The dataset is reqired to have &quot;prompt&quot; and &quot;completion&quot; columns. Do I just take first couple of words of each tweet and make it a prompt?</p>
","gpt-3"
"73899423","NLP / ML Python: variation of topic modeling + summarization? Can someone point me in the right direction?","2022-09-29 17:16:11","","1","86","<python><nlp><stanford-nlp><summarization><gpt-3>","<p>New to NLP and Machine learning. Wondering if someone can point me in the right direction:</p>
<p>I'm looking to create a function that takes 2 inputs.</p>
<p>-an array of strings (english sentences of varying relation to one another. but for these purposes let's just assume they're totally unrelated sentences)</p>
<p>-a &quot;topic&quot; string.</p>
<p>The function then returns a coherent paragraph / essay about the indicated &quot;topic,&quot; using ONLY the available sentences.</p>
<p>Seems like some flavor of topic-modeling and summarization, except the function writes using only the predetermined array of strings.</p>
<p>Any thoughts as to what libraries or techniques I should investigate?</p>
<p>Thanks!</p>
","gpt-3"
"73797902","GPT-3 API invalid_request_error: you must provide a model parameter","2022-09-21 08:49:56","73853263","17","35811","<rest><postman><http-post><openai-api><gpt-3>","<p>I'm new to APIs and I'm trying to understand how to get a response from a prompt using OpenAI's GPT-3 API (using api.openai.com/v1/completions). I'm using Postman to do so.
The documentation says that there is only one required parameter, which is the &quot;model.&quot; However, I get an error saying that &quot;you must provide a model parameter,&quot; even though I already provided it.</p>
<p>What am I doing wrong?</p>
<p><a href=""https://i.sstatic.net/QtLP9.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/QtLP9.png"" alt=""API error screenshot"" /></a></p>
","gpt-3"
"73760982","Why is my addEventListener click event only firing once?","2022-09-18 07:52:38","","0","68","<javascript><php><addeventlistener><gpt-3>","<p>I'm using an addEventListener click event to trigger a new GPT3 request which takes the latest database post as a prompt for text completion.</p>
<p>The data is sent to the database through a hidden html form, with the action of the form triggering a php script.</p>
<p>The newly generated text is then sent to the database, which becomes the prompt for the next GPT3 request etc etc...</p>
<p>The problem is the event only fires once, and I would like to be able to continue to generate new text within the loading page whilst the other data heavy files within the page loads.</p>
<pre><code>&lt;form name=&quot;GPT3commentForm&quot; id=&quot;GPT3commentForm&quot; display=&quot;none&quot; action=&quot;/insert_GPT3.php&quot; method=&quot;post&quot; autocomplete=&quot;off&quot;&gt;
&lt;input type=&quot;hidden&quot; name=&quot;GPT3_field1&quot; id=&quot;GPT3Text&quot;/&gt;&lt;/form&gt;

&lt;?php
    require_once './includes/Openai.php';
    $openai = New Openai();
    //Engine, prompt and max tokens - takes the last entry of the database array as the prompt
    $openai-&gt;request(&quot;davinci&quot;, end($stack), 100);
    $latestEntry = end($stack);
    $latestEntry = JSON_encode($latestEntry);
?&gt;

&lt;script&gt;
var data = &lt;?php echo json_encode($response, JSON_HEX_TAG); ?&gt;; // Don't forget the extra semicolon!
//turn the string into an object
const dataObj = JSON.parse(data);
console.log(typeof dataObj);
//access only the text completion section...
console.log(dataObj.choices[0].text);
GPT3Array = [];
newData = [];
newDataObj = [];
currentGPT3Text = 0;
GPT3Array[currentGPT3Text] = dataObj.choices[0].text;
document.getElementById('GPT3Text').value = GPT3Array[currentGPT3Text];
const GPT3symbolSpan = document.createElement(&quot;span&quot;);
const GPT3link = document.createElement(&quot;a&quot;);
GPT3link.textContent = GPT3Array[currentGPT3Text];
GPT3symbolSpan.appendChild(GPT3link);
GPT3LoadingText.appendChild(GPT3symbolSpan);
document.getElementById('GPT3commentForm').submit();

var generateText = function (event) {
    currentGPT3Text += 1;
    &lt;?php
        require_once './includes/Openai.php';
        $new_openai = New Openai();
        //Engine, prompt and max tokens - takes the last entry of the database array as the prompt
        $new_openai-&gt;request(&quot;davinci&quot;, end($stack), 100);
        $latestEntry = end($stack);
        $latestEntry = JSON_encode($latestEntry);
        //unset($new_openai);
        //unset($latestEntry);        
    ?&gt;

    //make newData and newDataObj an array to make this work...
    newData[currentGPT3Text] = &lt;?php echo json_encode($response, JSON_HEX_TAG); ?&gt;;
    newDataObj[currentGPT3Text] = JSON.parse(newData[currentGPT3Text]);
    GPT3Array[currentGPT3Text] = newDataObj[currentGPT3Text].choices[0].text;
    document.getElementById('GPT3Text').value = GPT3Array[currentGPT3Text];
    document.getElementById('GPT3commentForm').submit();
    GPT3link.textContent = GPT3Array[currentGPT3Text];
    console.log(currentGPT3Text);
    console.log(GPT3Array[currentGPT3Text]);
}

// GPT3LoadingText.onclick = generateText;
document.getElementById('loading-screen').addEventListener('click', generateText, {once: false}); 
&lt;/script&gt;
</code></pre>
<p>Here is a link to the site showing the issue: <a href=""https://surfacecollider.net/"" rel=""nofollow noreferrer"">https://surfacecollider.net/</a> (Ignore the other errors in the dev console, they're relating to separate issues that I'm yet to sort out with GLTF loaders which resolve on load).</p>
<p>And here's the php linked to the form action:</p>
<pre><code>&lt;?php
$username = &quot;************&quot;;
$password = &quot;************&quot;;
$database = &quot;************&quot;;

$mysqli = new mysqli(&quot;************&quot;, $username, $password, $database);

// Don't forget to properly escape your values before you send them to DB
// to prevent SQL injection attacks.

$GPT3_field2 = $mysqli-&gt;real_escape_string($_POST['GPT3_field1']);

$query = &quot;INSERT INTO comments (comment)
            VALUES ('{$GPT3_field2}')&quot;;

$mysqli-&gt;query($query);
?&gt;

&lt;?php
$query = $mysqli-&gt;query(&quot;SELECT * FROM comments&quot;);

$query = &quot;SELECT * FROM comments&quot;;

if ($result = $mysqli-&gt;query($query)) {
    /* fetch associative array */
    while ($row = $result-&gt;fetch_assoc()) {
        $field1name = $row[&quot;comments&quot;];
    }
    /* free result set */
    $result-&gt;free();
}?&gt;

&lt;?php
$username = &quot;************&quot;;
$password = &quot;************&quot;;
$database = &quot;************&quot;;
$mysqli = new mysqli(&quot;***********&quot;, $username, $password, $database);
$query = &quot;SELECT * FROM comments&quot;;
echo '&lt;table border=&quot;0&quot; cellspacing=&quot;2&quot; cellpadding=&quot;2&quot;&gt; 
    &lt;tr&gt; 
        &lt;td&gt; &lt;font face=&quot;Arial&quot;&gt;Username&lt;/font&gt; &lt;/td&gt; 
        &lt;td&gt; &lt;font face=&quot;Arial&quot;&gt;GPT3_field1&lt;/font&gt; &lt;/td&gt; 
    &lt;/tr&gt;';

if ($result = $mysqli-&gt;query($query)) {
    while ($row = $result-&gt;fetch_assoc()) {
        $field1name = $row[&quot;GPT3_field1&quot;];
        echo '&lt;tr&gt; 
                &lt;td&gt;'.$field1name.'&lt;/td&gt; 
            &lt;/tr&gt;';
    }
    $result-&gt;free();
} 

$mysqli-&gt;close();?&gt;
</code></pre>
<p>Any ideas why this is happening please?</p>
","gpt-3"
"73657901","Combine GPT3 with RASA","2022-09-09 06:00:56","","0","683","<python><rasa><openai-api><gpt-3>","<p>I am trying to integrate rasa with gpt3, but not getting the proper response. Can help me out to look at my code and tell me issue.</p>
<pre><code>def gpt3(text):
response = openai.Completion.create(
    model=&quot;code-cushman-001&quot;,
    # engine=&quot;ada&quot;,
    prompt=&quot;\n\n&quot; + text,
    temperature=0,
    logprobs=10,
    max_tokens=150,
    top_p=0,
    frequency_penalty=0,
    presence_penalty=0,
    stop=[&quot; \n\n&quot;]
) 
return response['choices'][0]['text']
</code></pre>
<p>action.py</p>
<pre><code>class ActionDefaultFallback(Action):
def init(self):
    # self.gpt3 = gpt3()
    super()._init_()

def name(self) -&gt; Text:
    return &quot;action_default_fallback&quot;

async def run(self, dispatcher, tracker, domain):
    query = tracker.latest_message['text']
    dispatcher.utter_message(text=gpt3(query))

    return [UserUtteranceReverted()]
</code></pre>
<p>Not able to understand the issue. Help me out to solve this.</p>
<p>Thanks</p>
","gpt-3"
"73642618","GPT-3 cannot mix two actions into one prompt (summarisation and tense changing)","2022-09-08 00:26:15","73853380","1","635","<gpt-3>","<p>(Just a heads up, this feels like a weird question to ask since there's not really any code involved, I'm not sure if this is the right place to ask)</p>
<p>I am trying to summarise a journal entry <em>and</em> convert it into second person past tense (i.e. &quot;I went to the shop&quot; -&gt; &quot;You went to the shop&quot;).</p>
<p>When I give the following prompt to GPT-3 (Da Vinci, all other params normal), it gives me a summary as expected:</p>
<pre><code>Summarise this text:

We took to the streets of London on the London hire bikes aka Boris Bikes / BoJo Bikes; previously Barclays Bikes and now Santander Bikes ‚Äì bloomin heck this is complicated.  I knew the direction where I wanted to get to and knew how to get there except I didn‚Äôt really.

We started our journey at one of bike hire station in St John‚Äôs Wood and continued around Regents Park (the wrong way) Simon got us to one of the gateways to the path along the Regents Canal.  Sometimes they can be quite difficult to find; this was one of those times. This particular one was located at the back of a housing estate; only that Simon knew where it was there was no way I would have found it.

Off down the canal we went. Sunday afternoons are a busy time along the canal with local people mixed in with tourists from all over the world; so cycling along a narrow path is not easy as everyone walks on different sides of the path (according to where they come from)! We got towards Camden Market and the path got very busy, to the point that I almost went into the canal but with a wibble and a wobble I managed to stay in.  At that point the decision was easily made to get off that bike and walk it. The Santander App showed us where the nearest parking station was and that there was space available to park up.

Coffee time! Forget the major chains, we found a small local place called T &amp; G for some cups of coffee and a sarnie before we went out to find out next bike to get us to Granary Square in Kings Cross for our next stop. From the canal path there is a grassed set of steps going up to the Square but first we parked up the bikes on the other side of the canal. So many places to choose from to hang out, for drinks and for food or trains to Paris, Lille, Edinburgh or Manchester to start off with.

All in all, we went out and achieved what we intended to ‚Äì a cycle along the canal with a couple of stops along the way for some food and drinks.  What better way to spend a Sunday afternoon.

// GPT-3's answer:

The author takes a Boris Bike (a bike available for rent in London) and cycles along the Regents Canal. They note that the path is busy on a Sunday afternoon, but manage to find a parking spot for their bike before exploring the area around Granary Square in Kings Cross.
</code></pre>
<p>That is a very good summary.</p>
<p>Now, I can convert the summary to second person POV like so:</p>
<pre><code>Convert the following to past tense second person:

The author takes a Boris Bike (a bike available for rent in London) and cycles along the Regents Canal. They note that the path is busy on a Sunday afternoon, but manage to find a parking spot for their bike before exploring the area around Granary Square in Kings Cross.

// GPT-3's answer:

You took a Boris Bike and cycled along the Regents Canal. You noted that the path was busy on a Sunday afternoon, but managed to find a parking spot for your bike before exploring the area around Granary Square in Kings Cross.
</code></pre>
<p>Again, excellent! But if I combine the prompts like so:</p>
<p><code>Summarise the following and convert the result to past tense second person:</code></p>
<p>It doesn't work well at all - in fact it just seems to ignore the summarisation part of the prompt, i.e. it just converts the whole passage to second person past tense. How can I fix this?</p>
","gpt-3"
"73629287","OpenAI GPT-3 API: How to extend length of the TL;DR output?","2022-09-07 01:53:41","","6","1214","<openai-api><gpt-3>","<p>I'd like to produce a 3-6 sentence summary from a 2-3 page article, using OpenAI's TLDR. I've pasted the article text but the output seems to stay between 1 and 2 sentences only.</p>
","gpt-3"
"73467393","gpt3 fine tuning with openai not learning","2022-08-24 04:20:47","","2","996","<python-3.x><nlp><openai-api><gpt-3>","<p>For my fine tuning jsonl files, I wanted a model that could predict the gender of the speaker given a statement. For instance, the prompt: &quot;i went to buy a skirt today&quot; has completion as &quot;female&quot;.</p>
<p>I created several examples and gave it to gpt3 to finetune. I then fed the sentence &quot;i went to pick my wife up from the shops&quot; to the resulting model. I expected to get a gender as response but I got a whole story about picking up my wife from the shops.</p>
<p>It's as if gpt-3 didn't learn anything from my fine tuning at all.</p>
<p>I have a few questions:</p>
<ol>
<li><p>Is fine tuning equivalent to writing a few examples in openai playground and getting gpt-3 to guess what comes next?</p>
</li>
<li><p>After fine tuning, do you only pay for the tokens in the prompt/completion of subsequent runs? So If I spend $100 training a model on a million examples, I will then only have to pay for the individual prompt/completion of subsequent calls?</p>
</li>
<li><p>The chat bot for instance, come with a context sentence before the back and forth exchange of 2 chat participants. Something like &quot;this is a conversation between a rude man named John and a young girl named Sarah&quot;. How can i incorporate such context into fine tuning structure of {&quot;prompt&quot;:&quot;...&quot;,&quot;completion&quot;:...&quot;}?</p>
</li>
</ol>
","gpt-3"
"73455802","Search through GPT-3's training data","2022-08-23 08:56:26","","1","258","<nlp><training-data><gpt-3>","<p>I'm using GPT-3 for some experiments where I prompt the language model with tests from cognitive science. The tests have the form of short text snippets. Now I'd like to check whether GPT-3 has already encountered these text snippets during training. Hence my question: Is there any way to sift through GPT-3's training text corpora? Can one find out whether a certain string is part of these text corpora?</p>
<p>Thanks for your help!</p>
","gpt-3"
"73370817","How to use GPT-3 for fill-mask tasks?","2022-08-16 08:15:15","73371474","1","1323","<python><nlp><gpt-3>","<p>I use the following code to get the most likely replacements for a masked word:</p>
<pre><code>!pip install git+https://github.com/huggingface/transformers.git
import torch
import pandas as pd
from transformers import AutoModelForMaskedLM, AutoTokenizer, pipeline

unmasker = pipeline('fill-mask', model='bert-base-uncased', top_k=100)
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
model = AutoModelForMaskedLM.from_pretrained('bert-base-uncased')

results = unmasker(f&quot;The sun is [MASK].&quot;)
for i in results:
  print(i[&quot;token_str&quot;], i[&quot;score&quot;]*100)
</code></pre>
<p>For example, the most likely replacement for &quot;[MASK]&quot; in the sequence &quot;The sun is [MASK].&quot; is &quot;rising&quot; (33.61%), &quot;shining&quot; (9.33%), and &quot;up&quot; (7.38%).</p>
<p><strong>My question: is there a way to achieve the same with GPT-3?</strong> There is a &quot;complete&quot; and &quot;insert&quot; preset in the OpenAI playground, however, it gives me full sentences (instead of single words) and no probabilities. Can someone help?</p>
","gpt-3"
"73207664","Issues running GPT-J-6B demo inference on colab","2022-08-02 12:35:40","","1","672","<python><tensorflow><deep-learning><google-colaboratory><gpt-3>","<p>I'm trying to run the GPT-J 6B demo available here : <a href=""https://colab.research.google.com/github/kingoflolz/mesh-transformer-jax/blob/master/colab_demo.ipynb"" rel=""nofollow noreferrer"">https://colab.research.google.com/github/kingoflolz/mesh-transformer-jax/blob/master/colab_demo.ipynb</a></p>
<p>Unfortunatelay I have some issues that are blocking me so far :</p>
<p>Firstly, when I'm running this part (the first cell of the colab notebook)</p>
<pre><code>!apt install zstd

# the &quot;slim&quot; version contain only bf16 weights and no optimizer parameters, which minimizes bandwidth and memory
!time wget -c https://the-eye.eu/public/AI/GPT-J-6B/step_383500_slim.tar.zstd

!time tar -I zstd -xf step_383500_slim.tar.zstd

!git clone https://github.com/kingoflolz/mesh-transformer-jax.git
!pip install -r mesh-transformer-jax/requirements.txt

# jax 0.2.12 is required due to a regression with xmap in 0.2.13
!pip install mesh-transformer-jax/ jax==0.2.12 tensorflow==2.5.0
</code></pre>
<p>I don't understand why it try to download several versions of tensorflow while it's specified &quot;tensorflow==2.5.0&quot; in the code.
Installing all these versions take a very long time.
Here is a screenshot of a part of the output:
<a href=""https://i.sstatic.net/NZPSF.png"" rel=""nofollow noreferrer"">output (image)</a></p>
<p>Moreover, at the end of the execution, I have this :
<a href=""https://i.sstatic.net/9oHnt.png"" rel=""nofollow noreferrer"">excecution's end message (image)</a></p>
<p>Then, when trying to import the libraries in the following code cells, I receive missing modules errors. The missing modules seems to vary depending on the result of the first execution cell.
<a href=""https://i.sstatic.net/iVmOl.png"" rel=""nofollow noreferrer"">missing module (image)</a></p>
<p>I believe that colab run out of disk memory trying to download model and dependencies but why this demo exists on colab if it can't be run on it ?</p>
","gpt-3"
"73117628","How to solve API connection error and SSL certification error while connecting to GPT-3 open AI?","2022-07-26 04:07:12","","5","22822","<python><ssl-certificate><openai-api><gpt-3>","<p>I am trying to run a python script (jupyter notebook) by experimenting with GPT-3 open AI to create some NLP project and understand its functions and used cases. I got an error of SSL certification and API connection while I was trying to open a JSON file. I checked some solutions on the internet but it did not offer any remedy. I simply tried connecting to the server through API key but the code was not working. The code I executed is as follows-</p>
<pre><code>import ssl
import certifi
certifi.where()
import openai
api_key='my_api_key'            #it is confidential string
openai.api_key = api_key
response = openai.File.create(file=open(&quot;C:\\Users\\pythons_scripts\\Corporate Governance1658287996.json&quot;), purpose=&quot;search&quot;)
print(response)
</code></pre>
<p>So the above script is throwing all of the following errors-</p>
<pre><code>SSLCertVerificationError                  Traceback (most recent call last)    
SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1129)
APIConnectionError                        Traceback (most recent call last)
APIConnectionError: Error communicating with OpenAI
</code></pre>
<p>Does anyone know how to get around with this ? or has anyone solved this kind of problem? can someone suggest a solution which will work?</p>
","gpt-3"
"73113552","Open AI generate longer text with GPT-3","2022-07-25 18:07:18","","4","5870","<python><openai-api><gpt-3>","<p>I'm playing with the GPT-3 API of OPENAI but I struggle to find a way to make long enough generated text.</p>
<p>Here is my piece of code :</p>
<pre><code>import os
import openai

# export OPENAI_API_KEY='get_key_from_openai'

openai.api_key = os.getenv(&quot;OPENAI_API_KEY&quot;)

response = openai.Completion.create(
  model=&quot;text-davinci-002&quot;,
  prompt=&quot;How to choose a student loan&quot;,
  temperature=0.6,
  max_tokens=512,
  top_p=1,
  frequency_penalty=1,
  presence_penalty=1,
  n= 10
)

print(response['choices'][0]['text'])
</code></pre>
<p>An example output I have is</p>
<p>&quot;There are a few things to consider when choosing a student loan, including the interest rate, repayment options, and whether the loan is federal or private. You should also compare loans to see which one will cost you the least amount of money in the long run&quot;</p>
<p>However, there are ~50 words which shouldn't be close to 80-100 tokens. I also thought that the <code>n</code> parameter was supposed to run <code>n</code> consecutive generated texts ?</p>
<p>Can someone explain how to make this generated text longer (ideally ~1000 tokens) ? Some huggingface models have a <code>min_tokens</code> parameter but I couldn't find it there.</p>
<p>Thanks a lot</p>
","gpt-3"
"73094271","What is suffix and prefix prompt in openai Codex?","2022-07-23 21:21:41","","1","1873","<python><deep-learning><openai-api><gpt-2><gpt-3>","<p>I have been trying to understand what is the suffix prompt in addition to the prefix prompt in Codex.</p>
<p>They have provided an <a href=""https://beta.openai.com/docs/guides/code/inserting-code"" rel=""nofollow noreferrer"">example</a></p>
<pre><code>def get_largest_prime_factor(n):
    if n &lt; 2:
        return False
    def is_prime(n): &gt;  for i in range(2, n): &gt;  if n % i == 0: &gt;  return False &gt;  return True &gt;     largest = 1
    for j in range(2, n + 1):
        if n % j == 0 and is_prime(j):
    return largest
</code></pre>
<p>From this example it is not clear to me how to create a suffix prompt?</p>
<p>What I understand is suffix prompt is for code insert model. My use case is also <code>insert</code> mode i.e., code needs to be updated in the middle of a code snippet.</p>
<p>Can anyone please provide a snippet showing how I can use the suffix prompt so that Codex works in the insert mode?</p>
","gpt-3"
"73014448","Is there a known workaround for the max token limit on the input to GPT-3?","2022-07-17 18:43:06","","4","6055","<machine-learning><gpt-3>","<p>For a bit of context, I recently started working on a personal project that accepts the URL of some recipe web page, pulls the HTML, converts the HTML to simplified markdown (this is the GPT-3 part), then sends that markdown to a thermal receipt printer in my kitchen, which prints it out.</p>
<p>Recipe web pages have a wide variety of structures, and they are notorious for including long and often irrelevant articles before the recipe, for the sake of SEO.</p>
<p>My plan was to use the fine-tuning API for davinci2, and feed it a bunch of straight up recipe HTML as input and cleaned, recipe-only markdown as output. I notice though that the maximum input token count for both training and inference is 4096. The HTML for a web page can be much larger than that, like 20k tokens.</p>
<p>I am wondering if anyone has found a workaround for training and driving GPT-3 with more tokens than 4096.</p>
<p>I'm open to other suggestions as well. For instance, I've considered passing just the visible text on the page, rather than the full HTML tree, but there is much less context present in that form, and the models seems more easily confused by all of the links and other navigational elements present in the page. I have also considered only allowing this project to accept &quot;printer-friendly&quot; versions of recipes, which tend to be much smaller and would easily come in under the 4096 token limit, but not all sites offer a printer-friendly article, and I don't want this to be a limitation.</p>
","gpt-3"
"72925542","When you prompt GPT3, what happens to the input data?","2022-07-10 01:00:42","","-1","536","<nlp><artificial-intelligence><gpt-3>","<p>For example, let's say I open up the playground and type &quot;Quack&quot;. What does the model do with those 5 characters to figure out what letters or words should come next?</p>
<p>(As it happens, GPT3 filled in that prompt with &quot;Quackery&quot;, then a tirade against cell therapy. Weird).</p>
","gpt-3"
"72922146","Why do 'callback' and 'tweet' fail in my Twitter GPT-3 OpenAI bot?","2022-07-09 14:24:19","","1","111","<javascript><twitter><twitter-oauth><openai-api><gpt-3>","<p>I am trying to develop a Twitter bot using OpenAI gpt-3 library, following <a href=""https://www.youtube.com/watch?v=V7LEihbOv3Y&amp;t=1s"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=V7LEihbOv3Y&amp;t=1s</a> this tutorial.</p>
<p>I am getting this same error again and again.
<a href=""https://i.sstatic.net/cT9lZ.png"" rel=""nofollow noreferrer"">I clicked on AUTH link and authorized the bot. When I clicked on the other two links, I got this error.</a>
I have recently got Elevated access for my Twitter bot and hence, tried using this bot again.</p>
<p>What should I do?
Please help, thank you.</p>
<p>Code :-</p>
<pre><code>=== Serving from '/Users/tanmayjuneja/Documents/demos/twitterbot'...

‚ö†  Your requested &quot;node&quot; version &quot;16&quot; doesn't match your global version &quot;12&quot;. Using node@12 from host.
i  functions: Watching &quot;/Users/tanmayjuneja/Documents/demos/twitterbot/functions&quot; for Cloud Functions...
‚úî  functions[us-central1-auth]: http function initialized (http://localhost:5000/twitterbot-e8123/us-central1/auth).
‚úî  functions[us-central1-callback]: http function initialized (http://localhost:5000/twitterbot-e8123/us-central1/callback).
‚úî  functions[us-central1-tweet]: http function initialized (http://localhost:5000/twitterbot-e8123/us-central1/tweet).
‚ö†  functions: The Cloud Firestore emulator is not running, so calls to Firestore will affect production.
i  functions: Beginning execution of &quot;auth&quot;
‚ö†  Google API requested!
   - URL: &quot;https://oauth2.googleapis.com/token&quot;
   - Be careful, this may be a production service.
i  functions: Finished &quot;auth&quot; in ~1s
i  functions: Beginning execution of &quot;callback&quot;
‚ö†  Google API requested!
   - URL: &quot;https://oauth2.googleapis.com/token&quot;
   - Be careful, this may be a production service.
i  functions: Finished &quot;callback&quot; in ~3s
i  functions: Beginning execution of &quot;callback&quot;
‚ö†  functions: Error: Request failed with code 400
    at RequestHandlerHelper.createResponseError (/Users/tanmayjuneja/Documents/demos/twitterbot/functions/node_modules/twitter-api-v2/dist/client-mixins/request-handler.helper.js:103:16)
    at RequestHandlerHelper.onResponseEndHandler (/Users/tanmayjuneja/Documents/demos/twitterbot/functions/node_modules/twitter-api-v2/dist/client-mixins/request-handler.helper.js:252:25)
    at Gunzip.emit (events.js:327:22)
    at endReadableNT (_stream_readable.js:1221:12)
    at processTicksAndRejections (internal/process/task_queues.js:84:21)
‚ö†  Your function was killed because it raised an unhandled error.
i  functions: Beginning execution of &quot;tweet&quot;
‚ö†  Google API requested!
   - URL: &quot;https://oauth2.googleapis.com/token&quot;
   - Be careful, this may be a production service.
‚ö†  functions: Error: Request failed with status code 400
    at createError (/Users/tanmayjuneja/Documents/demos/twitterbot/functions/node_modules/axios/lib/core/createError.js:16:15)
    at settle (/Users/tanmayjuneja/Documents/demos/twitterbot/functions/node_modules/axios/lib/core/settle.js:17:12)
    at IncomingMessage.handleStreamEnd (/Users/tanmayjuneja/Documents/demos/twitterbot/functions/node_modules/axios/lib/adapters/http.js:322:11)
    at IncomingMessage.emit (events.js:327:22)
    at endReadableNT (_stream_readable.js:1221:12)
    at processTicksAndRejections (internal/process/task_queues.js:84:21)
‚ö†  Your function was killed because it raised an unhandled error.
</code></pre>
","gpt-3"
"72821522","Why does Post request to OpenAI in Unity result in error 400?","2022-06-30 20:13:50","","1","2200","<unity-game-engine><http-post><artificial-intelligence><gpt-3>","<p>I am trying to use GPT3 in a game I am making but I can't seem to be able to call the OpenAI API correctly. I got most of this from the Unity docs.
Here is the code I am using:</p>
<pre><code>public class gpt3_complete : MonoBehaviour
{
    public string model;
    public string prompt;
    public int len;
    public string temp;
    public string api_key = &quot;&lt;key&gt;&quot;;
    void Start()
    {
        StartCoroutine(Upload());
    }

    IEnumerator Upload()
    {
        WWWForm form = new WWWForm();
        form.AddField(&quot;model&quot;, model);
        form.AddField(&quot;prompt&quot;, prompt);
        form.AddField(&quot;max_tokens&quot;, len);
        form.AddField(&quot;temperature&quot;, temp);
        //form.headers.Add(&quot;Authorization&quot;, &quot;Bearer &quot;+api_key);



        using (UnityWebRequest www = UnityWebRequest.Post(&quot;https://api.openai.com/v1/completions&quot;, form))
        {
            www.SetRequestHeader(&quot;Authorization&quot;, &quot;Bearer &quot; + api_key);
            www.SetRequestHeader(&quot;Content-Type&quot;, &quot;application/json&quot;);

            yield return www.SendWebRequest();

            if (www.result != UnityWebRequest.Result.Success)
            {
                Debug.Log(www.error);
            }
            else
            {
                Debug.Log(www.result);
                Debug.Log(&quot;Form upload complete!&quot;);
            }
        }
    }
}
</code></pre>
<p>This always returns: 400 Bad Request.
The GPT3 docs can be found here: <a href=""https://beta.openai.com/docs/api-reference/completions/create"" rel=""nofollow noreferrer"">https://beta.openai.com/docs/api-reference/completions/create</a></p>
<p>Any idea why this is?
This is my first time making any web requests in unity so I'm probably missing something obvious.
Thanks!</p>
","gpt-3"
"72758187","OpenAI GPT-3 API: Fine tune a fine tuned model?","2022-06-26 00:35:25","","8","3409","<transformer-model><openai-api><fine-tuning><gpt-3>","<p>The OpenAI documentation for the <code>model</code> attribute in the fine-tune API states a bit confusingly:</p>
<blockquote>
<p><strong>model</strong></p>
<p>The name of the base model to fine-tune. You can select one of &quot;ada&quot;, &quot;babbage&quot;, &quot;curie&quot;, &quot;davinci&quot;, or a fine-tuned model created after 2022-04-21.</p>
</blockquote>
<p>My question: is it better to fine-tune a base model or a fine-tuned model?</p>
<p>I created a fine-tune model from <code>ada</code> with file <code>mydata1K.jsonl</code>:</p>
<pre><code>ada + mydata1K.jsonl --&gt; ada:ft-acme-inc-2022-06-25
</code></pre>
<p>Now I have a bigger file of samples <code>mydata2K.jsonl</code> that I want to use to improve the fine-tuned model.
In this second round of fine-tuning, is it better to fine-tune <code>ada</code> again or to fine-tune my fine-tuned model <code>ada:ft-acme-inc-2022-06-25</code>?  I'm assuming this is possible because my fine tuned model is created after 2022-04-21.</p>
<pre><code>ada + mydata2K.jsonl --&gt; better-model
</code></pre>
<p>or</p>
<pre><code>ada:ft-acme-inc-2022-06-25 + mydata2K.jsonl --&gt; even-better-model?
</code></pre>
","gpt-3"
"72663133","GPT-3 fine tuning Error: Incorrect API key provided","2022-06-17 17:53:53","","4","13860","<python><openai-api><gpt-3>","<p>I'm following <a href=""https://colab.research.google.com/drive/1PYwme_-SDOfUyg5gq7gdMuHtZMbIrcjz?usp=sharing"" rel=""nofollow noreferrer"">this tutorial</a> to fine-tune a GPT-3 model. However, when I run this part of the code:</p>
<pre><code># Enter credentials
%env OPENAI_API_KEY= &quot;&lt;MY OPENAI KEY&gt;&quot;
!openai api fine_tunes.create \
-t dw_train.jsonl \
-v dw_valid.jsonl \
-m $model \
--n_epochs $n_epochs \
--batch_size $batch_size \
--learning_rate_multiplier $learning_rate_multiplier \
--prompt_loss_weight $prompt_loss_weight
</code></pre>
<p>I get this error:</p>
<blockquote>
<p>Error: Incorrect API key provided:
&quot;sk-czja*****************************************gk0&quot;. You can find
your API key at <a href=""https://beta.openai.com"" rel=""nofollow noreferrer"">https://beta.openai.com</a>. (HTTP status code: 401)</p>
</blockquote>
<p>The curious thing is that the API key is correct. So much so that, if I use it to make a prompt, it works perfectly. Example:</p>
<pre><code>def GPT_Completion(texts):
  response = openai.Completion.create(
    engine=&quot;text-davinci-002&quot;,
    prompt =  texts,
    temperature = 0.6,
    top_p = 1,
    max_tokens = 64,
    frequency_penalty = 0,
    presence_penalty = 0
    )
  return print(response.choices[0].text)

  GPT_Completion(&quot;My dear friend,&quot;)
</code></pre>
<p>What could be causing this error? I thought maybe the GPT-3 training could require a paid account. However, I did not find this restriction on the OpenAI website.</p>
<p>The whole code I'm using is <a href=""https://colab.research.google.com/drive/1jK5LAXCaR835bBBd2S45js3_6Tir8kxE"" rel=""nofollow noreferrer"">here</a>.</p>
","gpt-3"
"72554328","How to fine tune fine tune GitHub Copilot?","2022-06-09 03:12:02","72718976","4","5632","<github><deep-learning><openai-api><gpt-3><github-copilot>","<p>We can fine tune language models like <code>BERT</code>, <code>GPT-3</code>.</p>
<p>Can I fine tune <code>GitHub Copilot</code> model?</p>
<p>I have already looked into examples from <a href=""https://copilot.github.com/"" rel=""nofollow noreferrer"">https://copilot.github.com/</a> but cant find the details.</p>
<p>Would really appreciate if someone had fine tuned Github Copilot.</p>
","gpt-3"
"72484590","Gpt 3 keywords extractor","2022-06-03 04:05:52","","2","4376","<python><nlp><openai-api><gpt-3>","<p>I'm getting accustomed to gpt and want to build a keywords extractor for book summaries. Can someone point me to the references that'd help for my use case ?</p>
","gpt-3"
"72294775","How do I know how much tokens a GPT-3 request used?","2022-05-18 19:11:58","","10","10327","<gpt-3>","<p>I am building an app around GPT-3, and I would like to know how much tokens every request I make uses. Is this possible and how ?</p>
","gpt-3"
"72080207","GPT-3 completions not working with API KEY","2022-05-01 20:31:58","","4","2755","<node.js><openai-api><gpt-3>","<p>Sorry if this is a simple problem but I'm new to this stuff.</p>
<p>ƒ∞ have my code below, but the API returns saying ƒ∞ don't have the right API key put it.</p>
<pre><code>const { Configuration, OpenAIApi } = require(&quot;openai&quot;);

const configuration = new Configuration({
  apiKey: process.env.OPENAI_API_KEY,
});
const openai = new OpenAIApi(configuration);

async function test() {
  const response = await openai.createCompletion(&quot;text-davinci-002&quot;, {
    prompt: &quot;Summarize this for a college student:\n\nJupiter is the fifth planet from the Sun and the largest in the Solar System. It is a gas giant with a mass one-thousandth that of the Sun, but two-and-a-half times that of all the other planets in the Solar System combined. Jupiter is one of the brightest objects visible to the naked eye in the night sky, and has been known to ancient civilizations since before recorded history. It is named after the Roman god Jupiter.[19] When viewed from Earth, Jupiter can be bright enough for its reflected light to cast visible shadows,[20] and is on average the third-brightest natural object in the night sky after the Moon and Venus.&quot;,
    temperature: 0.7,
    max_tokens: 64,
    top_p: 1,
    frequency_penalty: 0,
    presence_penalty: 0,
  });
  console.log(response)
}

test()
</code></pre>
<p>the API key seems to be found in a process.env file that ƒ∞ don't have, and when ƒ∞ made a file named process.env and made a variable called OPENAI_API_KEY, but it didn't seem to work. It returns something when ƒ∞ set apiKey equal to the actual key, but that seems like a roundabout solution. Thanks</p>
","gpt-3"
"72077048","GPT-3: a medical analogue","2022-05-01 13:15:16","","-1","81","<medical><gpt-3>","<p>Is there an analogue of <a href=""https://beta.openai.com/playground/p/default-chat"" rel=""nofollow noreferrer"">this</a>
which would answer like an excellent doctor ?
Or Einstein, or Ancient Greek ? At which URL can I find the list of all of these possibilities ?</p>
<p>I have just discovered GPT-3 and I'm amazed with it.</p>
","gpt-3"
"72008843","TypeError: Cannot subclass <class 'typing._SpecialForm'> while fine tuning GPT-J","2022-04-26 05:34:24","","2","788","<python><class><gpt-3>","<p>I am trying to <strong>fine tune GPT-J</strong> by following <a href=""https://github.com/kingoflolz/mesh-transformer-jax"" rel=""nofollow noreferrer"">this GitHub Repository</a>. When running the training command, I encounter this error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;device_train.py&quot;, line 13, in &lt;module&gt;
    from mesh_transformer import util
  File &quot;/home/shreyjain/mesh-transformer-jax/mesh_transformer/util.py&quot;, line 36, in &lt;module&gt;
    class ClipByGlobalNormState(OptState):
  File &quot;/usr/lib/python3.8/typing.py&quot;, line 317, in __new__
    raise TypeError(f&quot;Cannot subclass {cls!r}&quot;)
TypeError: Cannot subclass &lt;class 'typing._SpecialForm'&gt; 
</code></pre>
<p>This looks like a source code error but I am not sure. I have also raised an issue on GitHub regarding this. Any help will be appreciated!</p>
","gpt-3"
"71911771","Can I create a fine-tuned model for OpenAI API Codex models?","2022-04-18 12:22:26","","4","3025","<json><openai-api><gpt-3><fine-tuning>","<p>I'd like to translate user requests into tickets in some sort of structured data format, e.g. JSON. For example:</p>
<ul>
<li>User: I want to order two chairs and a desk with three drawers on the left side.</li>
<li>Output:</li>
</ul>
<pre><code>{
    &quot;type&quot;: &quot;furniture&quot;,
    &quot;items&quot;: [
        { &quot;type&quot;: &quot;desk&quot;, &quot;qty&quot;: 1, &quot;included_items&quot;: [{ &quot;type&quot;: &quot;drawer&quot;, &quot;qty&quot;: 3, &quot;position&quot;: &quot;left&quot; }] },
        { &quot;type&quot;: &quot;chair&quot;, &quot;qty&quot;: 2 }
    ]
}
</code></pre>
<p>It looks like GPT-3 itself is not very-well suited for this task, because output is not in the form of natural language, however Codex might be? But I can't find in OpenAI API docs how I can (if it's possible at all?) to create a custom / fine-tuned model for OpenAI API Codex models?</p>
","gpt-3"
"71690297","Large Language Model Perplexity","2022-03-31 09:41:36","","1","544","<python><nlp><nltk><gpt-3><perplexity>","<p>i am currently using GPT-3 and i am trying to compare its capabilities to related language models for my masters thesis.
Unfortunatly GPT-3 is an API based application, so i am not really able to extract metrics such as perplexity.</p>
<p>Over the API i have acces to these three metrics and of course the models outputs:</p>
<ul>
<li><p>training_loss: loss on the training batch</p>
</li>
<li><p>training_sequence_accuracy: the percentage of completions in the training batch for which the model's predicted tokens matched the true completion tokens exactly. For example, with a batch_size of 3, if your data contains the completions [[1, 2], [0, 5], [4, 2]] and the model predicted [[1, 1], [0, 5], [4, 2]], this accuracy will be 2/3 = 0.67</p>
</li>
<li><p>training_token_accuracy: the percentage of tokens in the training batch that were correctly predicted by the model. For example, with a batch_size of 3, if your data contains the completions [[1, 2], [0, 5], [4, 2]] and the model predicted [[1, 1], [0, 5], [4, 2]], this accuracy will be 5/6 = 0.83</p>
</li>
</ul>
<p>Is there any possibility to calculate the perplexity of my model using python?</p>
<p>Thank you.</p>
","gpt-3"
"71641369","GPT-3 made a mistake using numpy and I can't fix it","2022-03-28 00:38:58","","0","148","<python><numpy><deep-learning><gpt-3>","<p>I used GPT-3 to generate a Neural Network to use in a simple &quot;cell&quot; simulator.</p>
<p>When I run the script, I get the following error :</p>
<pre><code>*hidden_errors = np.dot(output_errors, self.weights_hidden_to_output.T)
  File &quot;&lt;__array_function__ internals&gt;&quot;, line 5, in dot
ValueError: shapes (1,4) and (2,1) not aligned: 4 (dim 1) != 2 (dim 0)*
</code></pre>
<p>I know this is because the matrices are not correctly shaped and I tried transposing it but without success. I also tried modifying the inputs list without any success.</p>
<pre><code>import random
import pygame
import numpy as np

class Hero:
    def __init__(self, x, y):
        self.x = x
        self.y = y
        self.score = 0

    def move_towards_food(self, food):
        if self.x &lt; food.x:
            self.x += 1
        elif self.x &gt; food.x:
            self.x -= 1
        if self.y &lt; food.y:
            self.y += 1
        elif self.y &gt; food.y:
            self.y -= 1

    def get_score(self):
        return self.score
    def respawn(self):
        self.x = random.randint(20,980)
        self.y = random.randint(20,980)


class Villain:
    def __init__(self):
        self.x = random.randint(0,1000)
        self.y = random.randint(0,1000)

    def move_towards_hero(self, hero):
        if self.x &lt; hero.x:
            self.x += 1
        elif self.x &gt; hero.x:
            self.x -= 1
        if self.y &lt; hero.y:
            self.y += 1
        elif self.y &gt; hero.y:
            self.y -= 1


class Food:
    def __init__(self, x, y):
        self.x = x
        self.y = y

class NeuralNetwork:
    def __init__(self, input_nodes, hidden_nodes, output_nodes, learning_rate):
        self.input_nodes = input_nodes
        self.hidden_nodes = hidden_nodes
        self.output_nodes = output_nodes
        self.learning_rate = learning_rate
        self.weights_input_to_hidden = np.random.normal(0.0, self.input_nodes ** -0.5,
                                                        (self.input_nodes, self.hidden_nodes))
        self.weights_hidden_to_output = np.random.normal(0.0, self.hidden_nodes ** -0.5,
                                                         (self.hidden_nodes, self.output_nodes))
        self.activation_function = lambda x: 1 / (1 + np.exp(-x))

    def train(self, inputs_list, targets_list):
        inputs = np.array(inputs_list, ndmin=2).T
        targets = np.array(targets_list, ndmin=2).T
        hidden_inputs = np.dot(self.weights_input_to_hidden, inputs)
        hidden_outputs = self.activation_function(hidden_inputs)
        final_inputs = np.dot(self.weights_hidden_to_output, hidden_outputs)
        final_outputs = final_inputs
        output_errors = targets - final_outputs
        hidden_errors = np.dot(self.weights_hidden_to_output.T, output_errors)
        self.weights_hidden_to_output += self.learning_rate * np.dot(
            output_errors * final_outputs * (1.0 - final_outputs), hidden_outputs.T)
        self.weights_input_to_hidden += self.learning_rate * np.dot(
            hidden_errors * hidden_outputs * (1.0 - hidden_outputs), inputs.T)

    def run(self, inputs_list):
        inputs = np.array(inputs_list, ndmin=2).T
        hidden_inputs = np.dot(self.weights_input_to_hidden, inputs)
        hidden_outputs = self.activation_function(hidden_inputs)
        final_inputs = np.dot(self.weights_hidden_to_output, hidden_outputs)
        final_outputs = final_inputs
        return final_outputs

hero = Hero(500, 500)
villain = Villain()
food = Food(random.randint(0, 1000), random.randint(0, 1000))

nn = NeuralNetwork(4, 4, 4, 0.5)

inputs = [[1, 0, 1, 0], [0, 1, 0, 1], [1, 1, 1, 0], [0, 0, 0, 1]]
targets = [[1], [1], [0], [0]]

nn.train(inputs, targets)
hero.nn = nn

pygame.init()
size = [1000, 1000]
screen = pygame.display.set_mode(size)
pygame.display.set_caption(&quot;Hero Game&quot;)

black = [0, 0, 0]
white = [255, 255, 255]
red = [255, 0, 0]
green = [0, 255, 0]

font = pygame.font.SysFont('Calibri', 25, True, False)

done = False
clock = pygame.time.Clock()

while not done:
    for event in pygame.event.get():
        if event.type == pygame.QUIT:
            done = True
    screen.fill(white)
    text = font.render(&quot;Hero: &quot; + str(hero.x) + &quot;, &quot; + str(hero.y), True, black)
    screen.blit(text, [50, 50])
    text = font.render(&quot;Villain: &quot; + str(villain.x) + &quot;, &quot; + str(villain.y), True, black)
    screen.blit(text, [50, 100])
    text = font.render(&quot;Food: &quot; + str(food.x) + &quot;, &quot; + str(food.y), True, black)
    screen.blit(text, [50, 150])
    text = font.render(&quot;Score: &quot; + str(hero.get_score()), True, black)
    screen.blit(text, [50, 200])
    pygame.draw.rect(screen, red, [hero.x, hero.y, 5, 5])
    pygame.draw.rect(screen, black, [villain.x, villain.y, 5, 5])
    pygame.draw.rect(screen, green, [food.x, food.y, 5, 5])

    # villain chase
    if villain.x &lt; hero.x:
        villain.x += 1
    if villain.x &gt; hero.x:
        villain.x -= 1
    if villain.y &lt; hero.y:
        villain.y += 1
    if villain.y &gt; hero.y:
        villain.y -= 1

    # -1 and respawn for villain touch
    if villain.x == hero.x and villain.y == hero.y:
        hero.score -= 1
        hero.respawn()

    inputs = [[hero.x / 1000, hero.y / 1000, food.x / 1000, food.y / 1000]]
    output = nn.run(inputs)
    if output[0][0] &gt; 0.5:
        hero.move_towards_food(food)
    if hero.x == food.x and hero.y == food.y:
        hero.score += 1
        food = Food(random.randint(0, 1000), random.randint(0, 1000))

    pygame.display.flip()
    pygame.display.update()
    clock.tick(60)

pygame.quit()
</code></pre>
","gpt-3"
"71618602","Finetuning GPT-3 on Windows?","2022-03-25 14:28:52","71824786","1","735","<python><machine-learning><nlp><openai-api><gpt-3>","<p>While I have read the documentation on fine-tuning GPT-3, I do not understand how to do so. It seems that the proposed CLI commands do not work in the Windows CMD interface and I can not find any documentation on how to finetune GPT3 using a &quot;regular&quot; python script. I have tried to understand the functions defined in the package. However I can not make sense of them. Is there any information that I am missing or is it just not possible to fine-tune GPT-3 on a Windows machine?</p>
<p><a href=""https://beta.openai.com/docs/guides/fine-tuning"" rel=""nofollow noreferrer"">https://beta.openai.com/docs/guides/fine-tuning</a></p>
","gpt-3"
"71399624","Memory usage in transforming fine tuning of GPTJ-6b to HuggingFace format","2022-03-08 18:06:05","","0","423","<tpu><jax><gpt-3>","<p>Following this tutorial using TPUs to fine tune GPTJ has worked well.
<a href=""https://github.com/kingoflolz/mesh-transformer-jax/blob/master/howto_finetune.md"" rel=""nofollow noreferrer"">https://github.com/kingoflolz/mesh-transformer-jax/blob/master/howto_finetune.md</a></p>
<p>Why would the step to transform to huggingface format using to_hf_weights.py have an issue with memory at 256MB - even after slimming has been applied?</p>
<p>The issue I filed is here:
<a href=""https://github.com/kingoflolz/mesh-transformer-jax/issues/209"" rel=""nofollow noreferrer"">https://github.com/kingoflolz/mesh-transformer-jax/issues/209</a></p>
","gpt-3"
"71376760","How to output the list of probabilities on each token via model.generate?","2022-03-07 05:28:50","","1","7641","<python><nlp><huggingface-transformers><gpt-3>","<p>Right now I have:</p>
<pre><code>model = GPTNeoForCausalLM.from_pretrained(model_name)
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
input_ids = tokenizer(prompt, return_tensors=&quot;pt&quot;).input_ids.cuda()
gen_tokens = model.generate(input_ids, do_sample=specifiedDoSample, output_scores=True, temperature=specifiedTemperature, max_new_tokens=specifiedNumTokens, repetition_penalty=specifiedRepetitionPenalty, top_p=specifiedTopP)
gen_text = tokenizer.batch_decode(gen_tokens)[0]
print(gen_text)
</code></pre>
<p>This will print the generated text. However, I want it to list the top N tokens in each step as well as their probability (N being a number specified by me), similar to OpenAI's beta playground where you can select &quot;Show probabilities: Full spectrum&quot;. For example, if the prompt is &quot;You are now a&quot;, the next token should say something like {&quot;vampire&quot;: 51%, &quot;corpse&quot;: 32% ... etc.}</p>
<p>What is the easiest way to do this via Huggingface Transformers?</p>
","gpt-3"
"71333114","OpenAI retrieve file content","2022-03-03 06:50:00","","0","2657","<python><openai-api><gpt-3>","<p>Unable to retrieve the content of file uploaded already.</p>
<p>Kindly suggest what is going wrong? I have tried for each type of file: search, classification, answers, and fine-tune. Files upload successfully but while retrieving content it shows an error.</p>
<pre><code>import openai

openai.api_key = &quot;sk-bbjsjdjsdksbndsndksbdksbknsndksd&quot; # this is wrong key

# Replace file_id with the file's id whose file content is required
content = openai.File.download(&quot;file-5Xs86wEDO5gx8fOitMYArV8r&quot;)

print(content)
</code></pre>
<p>Error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;main.py&quot;, line 6, in &lt;module&gt;
    content = openai.File.download(&quot;file-5Xs86wEDO5gx8fOitMYArV8r&quot;)
  File &quot;/usr/local/lib/python3.8/dist-packages/openai/api_resources/file.py&quot;, line 61, in download
    raise requestor.handle_error_response(
openai.error.InvalidRequestError: Not allowed to download files of purpose: classifications
</code></pre>
","gpt-3"
"71203411","Openai: `prompt` column/key is missing. Please make sure you name your columns/keys appropriately, then retry","2022-02-21 08:46:04","71368459","1","3013","<python><openai-api><gpt-3>","<p>I want to run GPT-3 for text classification. As the first step, I prepare data using openai CLI. I got a csv file which looks like as follow:</p>
<p><a href=""https://i.sstatic.net/az0u5.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/az0u5.png"" alt=""enter image description here"" /></a></p>
<p>I wrote following command for preparing the data:</p>
<pre><code>openai tools fine_tunes.prepare_data -f &quot;Path\\training_dataset.csv&quot;
</code></pre>
<p>However, I got following error:</p>
<p><a href=""https://i.sstatic.net/Bwmeb.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Bwmeb.png"" alt=""enter image description here"" /></a></p>
<p>I am not sure about &quot;name columns/keys appropriately&quot;. Is there any convention that I should follow? Any help would be really appreciated to fix the error</p>
","gpt-3"
"71028228","GPT-3 long input posts for Question Answering","2022-02-08 03:26:09","","1","2395","<deep-learning><nlp><huggingface-transformers><nlp-question-answering><gpt-3>","<p>From my understanding, GPT-3 is &quot;trained&quot; for a specific task by including some labelled examples before the desired/test example. In Question Answering, this includes a context and a question. In this situation, the input prompt can become long. How do people address this?</p>
<p>I am using the Hugging Face GPT-J implementation, and there is an input token limit (of 2000). However, when including multiple qa examples in the prompt (especially with the contexts), it quickly reaches this limit, limitting the amount of example prompts to be inputted. Does anyone know how this issue is handled in a GPT-J setting, especially for QA?</p>
","gpt-3"
"71003190","OpenAI API repeats completions with no variation","2022-02-06 00:06:01","","3","4742","<openai-api><gpt-3>","<p>I have tried implementing a chatbot in OpenAI with Javascript, using the official OpenAI npm dependency.</p>
<p>The way i have solved it, is that i have an array of chat messages, that gets joined by newlines, and sent as the prompt to the API.</p>
<p>Example:</p>
<pre><code>arr.push(&quot;This is a conversation between you and an AI&quot;)
arr.push(&quot;You: Hello, how are you doing&quot;)
arr.push(&quot;AI: I'm great, how about you?&quot;)
arr.push(&quot;You: I'm good, thanks!&quot;)
</code></pre>
<p>I then push the next question asked to the array, and then push an empty &quot;AI:&quot; string for the OpenAI-endpoint to complete.</p>
<p>The resulting prompt for the API to complete looks like this</p>
<pre><code>```
This is a conversation between you and an AI
You: Hello, how are you doing
AI: I'm great, how about you?
You: I'm good, thanks!
You: How's the weather today?
AI:
```
</code></pre>
<p>The response will then also be pushed to the array, so the conversation can continue... (at this time i only send the last ~20 lines from the array)
However, the problem i have is that the &quot;bot&quot; will start repeating itself, seemingly at random times it will start answering something like &quot;great, how about you?&quot;, and whatever you send as the last question in the prompt, that will be the answer&quot;</p>
<p>Example:</p>
<pre><code>```
This is a conversation between you and an AI
You: Hello, how are you doing
AI: I'm great, how about you?
You: I'm good, thanks!
You: How's the weather today?
AI: It is looking great!
You: That's nice, any plans for today?
AI: It is looking great!
You: What are you talking about?
AI: It is looking great!
```
</code></pre>
<p>The only relevant thing i seem to have found in the documentation is the frequency_penalty and the presence_penalty. However, changing those doesnt seem to do much.</p>
<p>This is the parameters used for the examples above:</p>
<pre><code>    const completion = await openai.createCompletion(&quot;text-davinci-001&quot;, {
        prompt: p,
        max_tokens: 200,
        temperature: 0.6,
        frequency_penalty: 1.5,
        presence_penalty: 1.2,


    });

    return completion.data.choices[0].text.trim()
</code></pre>
<p>I have of course also tried with different combinations of temperatures and penalties.
Is this just a known problem, or am i misunderstanding something?</p>
","gpt-3"
"70834489","Choosing a good prompt for GPT-3","2022-01-24 13:16:53","70881747","0","597","<nlp><openai-api><gpt-3>","<p>I am trying to generate a quiz from a text that look like this:</p>
<pre><code>Text: &quot;Mary has little lamb and John has a cow. The lamb is one month old. It eats grass and milk, which Mary brings from the the farm.&quot;

Keywords: &quot;lamb&quot;, &quot;cow&quot;, &quot;one month old&quot;, &quot;farm.&quot;

1. What does Mary have?
A. Lamb
B. Cow
C. Dog
D. Cat

A. Lamb

2. What does John have?
A. Cow
B. Lamb
C. Dog
D. Cat

A. Cow

3. How old is Mary's lamb?
A. One month old
B. One year old
C. Two months old
D. Two years old

etc.
</code></pre>
<p>It works perfectly when I don't give keywords, with multiple prompts, with all these answer options, correct answer, everything. <strong>The problem is when I want to generate these questions such that the correct answer is the keyword.</strong> I tried all kinds of prompts, even giving examples like what I did above, but it doesn't work.</p>
","gpt-3"
"70784728","GPT-3 question answering based on keywords","2022-01-20 10:40:50","70802282","1","663","<nlp><openai-api><gpt-3>","<p>I am currently getting accustomed to GPT3, and I am trying to generate questions from a text by also inputting some keywords from that text. Ideally, they would be the answers to that question.</p>
<p>What I tried was to input the text, and simply write <code>Keywords: dog, cat, mouse</code> etc., so just enumerating the words, and then input some question examples. But obviously, it is not used to this structure and I was wondering if it was even possible to do it like that.</p>
","gpt-3"
"70534103","Uncaught RangeError [MESSAGE_CONTENT_TYPE]: Message content must be a non-empty string","2021-12-30 16:17:42","","2","414","<javascript><discord><discord.js><artificial-intelligence><gpt-3>","<p>I am trying to make a gpt3 chatbot but I keep getting this error on Discord.js v11.</p>
<blockquote>
<p>[Uncaught RangeError [MESSAGE_CONTENT_TYPE]: Message content must be a non-empty string.]</p>
</blockquote>
<pre><code>client.on(&quot;messageCreate&quot;, function (message) {
    if (message.author.bot) return;
    prompt += `You: ${String(message.content)}\n`;
    (async () =&gt; {
        const gptResponse = await openai.complete({
            engine: 'davinci',
            prompt: prompt,
            maxTokens: 60,
            temperature: 0.3,
            topP: 0.3,
            presencePenalty: 0,
            frequencyPenalty: 0.5,
            bestOf: 1,
            n: 1,
            stream: false,
            stop: ['\n', '\n\n']
        });
        message.reply(String(gptResponse.data.choices[0].text.substring(5)));
        prompt += `${gptResponse.data.choices[0].text}\n`;
    })();
 });            

client.login(process.env.BOT_TOKEN);
</code></pre>
","gpt-3"
"70531364","Structuring dataset for OpenAI's GPT-3 fine tuning","2021-12-30 12:00:37","70710672","3","3426","<python><machine-learning><training-data><openai-api><gpt-3>","<p>The <a href=""https://beta.openai.com/docs/guides/fine-tuning"" rel=""nofollow noreferrer"">fine tuning</a> endpoint for OpenAI's API seems to be fairly new, and I can't find many examples of fine tuning datasets online.</p>
<p>I'm in charge of a voicebot, and I'm testing out the performance of GPT-3 for general open-conversation questions. I'd like to <a href=""https://beta.openai.com/docs/api-reference/fine-tunes"" rel=""nofollow noreferrer"">train</a> the model on the &quot;fixed&quot; intent-response pairs we're currently using: this would probably end up performing better in terms of company voice and style.</p>
<p>I have ready a long JSON file of data extracted from our current conversational engine, which matches user input to <strong>intents</strong> and returns the specified response. I'd like to train a GPT-3 model on this data.</p>
<p>As of now, for some quick testing, I've set up my calls to the API just like they <a href=""https://beta.openai.com/examples/default-chat"" rel=""nofollow noreferrer"">suggest</a>. I have a &quot;fixed&quot; intro text in the form</p>
<pre><code>&lt;name&gt; is &lt;company&gt;'s voicebot. he is kind and professional...

This is a conversation between &lt;name&gt; and a customer:
</code></pre>
<p>which is pre-pended to each query, and then a small python class which keeps track of the context which starts with</p>
<pre><code>User: &lt;request the user provides&gt;
Bot:
</code></pre>
<p>then with each turn the api's response is appended, this way I'm keeping track of what is said. After a few questions, the query or prompt string i'm sending looks like this:</p>
<pre><code>&lt;name&gt; is &lt;company&gt;'s voicebot. he is kind and professional...

This is a conversation between &lt;name&gt; and a user:

User: &lt;request&gt;
Bot: &lt;response&gt;
User: &lt;request&gt;
Bot: &lt;response&gt;
... and so on
Bot:
</code></pre>
<p>My question is, do i have to provide the same &quot;format&quot; for my training data? Is it advisable?
The <a href=""https://beta.openai.com/docs/guides/fine-tuning/prepare-training-data"" rel=""nofollow noreferrer"">docs</a> indicate that the training set should be in this format:</p>
<pre><code>{&quot;prompt&quot;: &quot;&lt;prompt text&gt;&quot;, &quot;completion&quot;: &quot;&lt;ideal generated text&gt;&quot;}
{&quot;prompt&quot;: &quot;&lt;prompt text&gt;&quot;, &quot;completion&quot;: &quot;&lt;ideal generated text&gt;&quot;}
...
</code></pre>
<p>But does the prompt need to include my intro text (the description) each time or do i simply provide a series of user/bot exchanges with a <code>Bot:</code> in the end and in the completion the answer i'd expect?
What would be a <em>best practice</em> in this case?
My fear is that if i wanted to slightly change the intro prompt a month from now I'd have to retrain the whole thing again because each response was trained with that specific block of text prepended.</p>
","gpt-3"
"70408322","OpenAI GPT3 Search API not working locally","2021-12-19 00:56:26","70422274","2","2066","<python><openai-api><gpt-3>","<p>I am using the python client for GPT 3 search model on my own Jsonlines files. When I run the code on Google Colab Notebook for test purposes, it works fine and returns the search responses. But when I run the code on my local machine (Mac M1) as a web application (running on localhost) using flask for web service functionalities, it gives the following error:</p>
<pre><code>openai.error.InvalidRequestError: File is still processing.  Check back later.
</code></pre>
<p>This error occurs even if I implement the exact same example as given in OpenAI documentation. The link to the <a href=""https://beta.openai.com/docs/guides/search#:%7E:text=openai.Engine(%22ada%22).search(%0A%20%20%20%20search_model%3D%22ada%22%2C%20%0A%20%20%20%20query%3D%22happy%22%2C%20%0A%20%20%20%20max_rerank%3D5%2C%0A%20%20%20%20file%3D%22file%2DLwjuy0q2ezi00jdpfCbl28CO%22%0A)"" rel=""nofollow noreferrer"">search example is given here</a>.</p>
<p>It runs perfectly fine on local machine and on colab notebook if I use the completion API that is used by the GPT3 playground. (<a href=""https://beta.openai.com/docs/developer-quickstart/python-bindings#:%7E:text=import%20os%0Aimport%20openai%0A%0A%23%20Load%20your%20API%20key%20from%20an%20environment%20variable%20or%20secret%20management%20service%0Aopenai.api_key%20%3D%20os.getenv(%22OPENAI_API_KEY%22)%0A%0Aresponse%20%3D%20openai.Completion.create(engine%3D%22davinci%22%2C%20prompt%3D%22This%20is%20a%20test%22%2C%20max_tokens%3D5)"" rel=""nofollow noreferrer"">code link here</a>)</p>
<p>The code that I have is as follows:</p>
<pre><code>import openai

openai.api_key = API_KEY

file = openai.File.create(file=open(jsonFileName), purpose=&quot;search&quot;)

response = openai.Engine(&quot;davinci&quot;).search(
          search_model = &quot;davinci&quot;, 
          query = query, 
          max_rerank = 5,
          file = file.id
        )
for res in response.data: 
   print(res.text)

</code></pre>
<p>Any idea why this strange behaviour is occurring and how can I solve it? Thanks.</p>
","gpt-3"
"70069026","How to use files in the Answer api of OpenAI","2021-11-22 16:19:49","70157536","4","6720","<python><openai-api><gpt-3>","<p>As finally OpenAI opened the GPT-3 related API publicly,
I am playing with it to explore and discover his potential.</p>
<p>I am trying the Answer API, the simple example that is in the documentation:
<a href=""https://beta.openai.com/docs/guides/answers"" rel=""nofollow noreferrer"">https://beta.openai.com/docs/guides/answers</a></p>
<p>I upload the <code>.jsonl</code> file as indicated, and I can see it succesfully uploaded with the <code>openai.File.list()</code> api.</p>
<p>When I try to use it, unfortunately, I always get the same error:</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; openai.File.create(purpose='answers', file=open('example.jsonl') )
&lt;File file id=file-xxx at 0x7fbc9eca5e00&gt; JSON: {
  &quot;bytes&quot;: 140,
  &quot;created_at&quot;: 1637597242,
  &quot;filename&quot;: &quot;example.jsonl&quot;,
  &quot;id&quot;: &quot;file-xxx&quot;,
  &quot;object&quot;: &quot;file&quot;,
  &quot;purpose&quot;: &quot;answers&quot;,
  &quot;status&quot;: &quot;uploaded&quot;,
  &quot;status_details&quot;: null
}

#Use the file in the API:
openai.Answer.create(
    search_model=&quot;ada&quot;, 
    model=&quot;curie&quot;, 
    question=&quot;which puppy is happy?&quot;, 
    file=&quot;file-xxx&quot;, 
    examples_context=&quot;In 2017, U.S. life expectancy was 78.6 years.&quot;, 
    examples=[[&quot;What is human life expectancy in the United States?&quot;, &quot;78 years.&quot;]], 
    max_rerank=10,
    max_tokens=5,
    stop=[&quot;\n&quot;, &quot;&lt;|endoftext|&gt;&quot;]
)
&lt;some exception, then&gt;
openai.error.InvalidRequestError: File is still processing.  Check back later.

</code></pre>
<p>I have waited several hours, and I do not think this content deserve such a long wait...
Do you know if it is a normal behaviour, or if I miss something?</p>
<p>Thanks</p>
","gpt-3"
"69931987","GPT-3 davinci gives different results with the same prompt","2021-11-11 16:49:41","69932414","4","4346","<text><nlp><autocomplete><gpt-3>","<p>I am not sure if you have access to GPT-3, particularly DaVinci (the complete-a-sentence tool). You can find the API and info <a href=""https://beta.openai.com/docs/api-reference/completions/create-via-get"" rel=""nofollow noreferrer"">here</a></p>
<p>I've been trying this tool for the past hour and every time I hit their API using the same prompt (indeed the same input), I received a different response.</p>
<ol>
<li>Do you happen to encounter the same situation?</li>
<li>If this is expected, do you happen to know the reason behind it?</li>
</ol>
<p>Here are some examples</p>
<p><strong>Request header</strong> <em>(I tried to use the same example they provide)</em></p>
<pre><code>{
  &quot;prompt&quot;: &quot;Once upon a time&quot;,
  &quot;max_tokens&quot;: 3,
  &quot;temperature&quot;: 1,
  &quot;top_p&quot;: 1,
  &quot;n&quot;: 1,
  &quot;stream&quot;: false,
  &quot;logprobs&quot;: null,
  &quot;stop&quot;: &quot;\n&quot;
}
</code></pre>
<p><strong>Output 1</strong></p>
<pre><code>&quot;choices&quot;: [
        {
            &quot;text&quot;: &quot;, this column&quot;,
            &quot;index&quot;: 0,
            &quot;logprobs&quot;: null,
            &quot;finish_reason&quot;: &quot;length&quot;
        }
    ]
</code></pre>
<p><strong>Output 2</strong></p>
<pre><code>&quot;choices&quot;: [
        {
            &quot;text&quot;: &quot;, winter break&quot;,
            &quot;index&quot;: 0,
            &quot;logprobs&quot;: null,
            &quot;finish_reason&quot;: &quot;length&quot;
        }
    ]
</code></pre>
<p><strong>Output 3</strong></p>
<pre><code>&quot;choices&quot;: [
        {
            &quot;text&quot;: &quot;, the traditional&quot;,
            &quot;index&quot;: 0,
            &quot;logprobs&quot;: null,
            &quot;finish_reason&quot;: &quot;length&quot;
        }
    ]
</code></pre>
","gpt-3"
"69928517","Encoding issues on OpenAI predictions after fine-tuning","2021-11-11 12:44:06","","6","3921","<utf-8><character-encoding><openai-api><gpt-3>","<p>I'm following <a href=""https://beta.openai.com/docs/guides/fine-tuning/create-a-fine-tuned-model"" rel=""noreferrer"">this OpenAI tutorial</a> about fine-tuning.</p>
<p>I already generated the dataset with the openai tool. The problem is that the outputs encoding (inference result) is mixing UTF-8 with non UTF-8 characters.</p>
<p>The generated model looks like this:</p>
<pre><code>{&quot;prompt&quot;:&quot;Usuario: Qui√©n eres\\nAsistente:&quot;,&quot;completion&quot;:&quot; Soy un Asistente\n&quot;}
{&quot;prompt&quot;:&quot;Usuario: Qu√© puedes hacer\\nAsistente:&quot;,&quot;completion&quot;:&quot; Ayudarte con cualquier gesti√≥n o ofrecerte informaci√≥n sobre tu cuenta\n&quot;}
</code></pre>
<p>For instance, if I ask &quot;¬øC√≥mo est√°s?&quot; and there's a trained completion for that sentence: &quot;Estoy bien, ¬øy t√∫?&quot;, the inference often returns exactly the same (which is good), but sometimes it adds non-encoded words: &quot;Estoy bien, ¬øy t√∫? Cu√É¬©ntame algo de ti&quot;, adding &quot;√É¬©&quot; instead of &quot;√©&quot;.</p>
<p>Sometimes, it returns exactly the same sentence that was trained for, with no encoding issues.
I don't know if the inference is taking the non-encoded characters from my model or from somewhere else.</p>
<p>What should I do?
Should I encode the dataset in UTF-8?
Should I leave the dataset with UTF-8 and decode the bad encoded chars in the response?</p>
<p>The OpenAI docs for fine-tuning don't include anything about encoding.</p>
","gpt-3"
"69590991","How do I make ""msg.content"" constantly get new strings added to it instead of replaced?","2021-10-15 21:52:15","","1","52","<javascript><node.js><discord.js><openai-api><gpt-3>","<pre><code> var collector = new MessageCollector(message.channel, filter, {
    max: 10,
    time: 60000,
})

        collector.on(&quot;collect&quot;, (msg) =&gt; {
            console.log(msg.content)
            
        openai.Completion.create({
            
            engine: &quot;davinci&quot;,
            prompt: msg.content,
            temperature: 0.9,
            max_tokens: 150,
            top_p: 1,
            frequency_penalty: 0.35,
            presence_penalty: 0.6, 
            stop: [&quot;\n&quot;, &quot; Human:&quot;, &quot; AI:&quot;]  
        
        }).then((response) =&gt; {
            
            message.channel.send(response.choices[0].text)
        })

    })
}
</code></pre>
<p>So I was trying to implement the AI &quot;GPT-3&quot; into a discord bot to see how it would work, but GPT-3 needs to know the prompt (basically context of a conversation) all the time. With the way I have it set up, it constantly replaces the variable (msg.content) with new strings once they are grabbed by &quot;MessageCollector&quot;. I need to make it so that whenever a message is detected, it adds that string to the variable and constantly doing that until, lets say a timer goes off.</p>
","gpt-3"
"69549494","A way to make GPT-3's ""davinci"" converse with a user(s) through a bot in discord using discord.js?","2021-10-13 03:43:17","69549620","1","437","<javascript><discord.js><openai-api><gpt-3>","<pre><code>var collector = new MessageCollector(message.channel, filter, {
    max: 10,
    time: 60000,
})
    start_sequence = &quot;\nAI: &quot;
    
    retart_sequence = &quot;\nHuman: &quot;

        collector.on(&quot;collect&quot;, (msg) =&gt; {
            console.log(msg.content)
            
        openai.Completion.create({
            
            engine: &quot;davinci&quot;,
            prompt: msg.content,
            temperature: 0.9,
            max_tokens: 150,
            top_p: 1,
            frequency_penalty: 0.35,
            presence_penalty: 0.6, 
            stop: [&quot;\n&quot;, &quot; Human:&quot;, &quot; AI:&quot;]  
        
        }).then((response) =&gt; {
            
            message.channel.send(response.choices[0].text)
        })

    })
}
</code></pre>
<p>I tried this but it only gives back completions, like the default preset rather than the chat preset in GPT-3's &quot;playground&quot;. I'm using openai-node to code in javascript rather than python to call openAI API.</p>
","gpt-3"
"69098317","Mycin like diagnosis system using gpt3 model","2021-09-08 06:57:51","","1","37","<interactive><expert-system><gpt-3>","<p>I  wondering if we can build a Mycin like expert system using most advanced deep learning model like GPT3 by fine tuning medical domain knowledge. We build 40 years ago Mycin using symbolic approach but I am not sure it is possible now.</p>
","gpt-3"
"68604289","AttributeError: module transformers has no attribute TFGPTNeoForCausalLM","2021-07-31 17:14:49","68656887","2","11829","<python><pytorch><huggingface-transformers><gpt-3>","<p>I cloned this repository/documentation <a href=""https://huggingface.co/EleutherAI/gpt-neo-125M"" rel=""nofollow noreferrer"">https://huggingface.co/EleutherAI/gpt-neo-125M</a></p>
<p>I get the below error whether I run it on google collab or locally. I also installed transformers using this</p>
<pre><code>pip install git+https://github.com/huggingface/transformers
</code></pre>
<p>and made sure the configuration file is named as config.json</p>
<pre><code>      5 tokenizer = AutoTokenizer.from_pretrained(&quot;gpt-neo-125M/&quot;,from_tf=True)
----&gt; 6 model = AutoModelForCausalLM.from_pretrained(&quot;gpt-neo-125M&quot;,from_tf=True)
      7 
      8 

3 frames
/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py in __getattr__(self, name)

AttributeError: module transformers has no attribute TFGPTNeoForCausalLM

</code></pre>
<p>Full code:</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForCausalLM 

tokenizer = AutoTokenizer.from_pretrained(&quot;EleutherAI/gpt-neo-125M&quot;,from_tf=True)

model = AutoModelForCausalLM.from_pretrained(&quot;EleutherAI/gpt-neo-125M&quot;,from_tf=True)

</code></pre>
<p>transformers-cli env results:</p>
<ul>
<li><code>transformers</code> version: 4.10.0.dev0</li>
<li>Platform: Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.29</li>
<li>Python version: 3.8.5</li>
<li>PyTorch version (GPU?): 1.9.0+cpu (False)</li>
<li>Tensorflow version (GPU?): 2.5.0 (False)</li>
<li>Flax version (CPU?/GPU?/TPU?): not installed (NA)</li>
<li>Jax version: not installed</li>
<li>JaxLib version: not installed</li>
<li>Using GPU in script?: </li>
<li>Using distributed or parallel set-up in script?: </li>
</ul>
<p>Both collab and locally have TensorFlow 2.5.0 version</p>
","gpt-3"
"68460080","how can i use openai's gpt 3 to find alternate spellings of bad words?","2021-07-20 19:00:02","68481504","1","1179","<openai-api><gpt-3>","<p>so, i am making an auto mod discord bot that finds alternate spellings of bad words. i tried using regex to find them but found many many false positives. so i thought about using openai's gpt-3 to do so, as i saw a screenshot of someone using it for what appears to be finding alternate spellings.</p>
<p>the screenshot:</p>
<p><a href=""https://i.sstatic.net/CfTUv.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/CfTUv.png"" alt=""the screenshot:"" /></a></p>
<p>unfortunately, i don't know how exactly they made gpt-3 do this, and how something like this can be used in an application like a discord bot.</p>
<p>can someone please tell me how you can use gpt-3 to find alternate spellings of words?</p>
<p>any help would be appreciated! thank you!</p>
","gpt-3"
"68444704","How to fix the error : ""cannot import name 'GPT' from ""gpt""","2021-07-19 17:29:47","","1","3611","<python><gpt-3>","<p>When I run the code below in Google Colab, I get the following error.</p>
<p>Note: I've already installed gpt using pip (!pip install gpt).</p>
<p><strong>code</strong></p>
<pre><code>from gpt import GPT
from gpt import Example'
</code></pre>
<p><strong>Error</strong></p>
<pre><code>cannot import name 'GPT' from 'gpt' (/usr/local/lib/python3.7/dist-packages/gpt/__init__.py)
</code></pre>
<p>Could someone help me fix this issue?</p>
","gpt-3"
"68442098","How to store API keys in environment variable? and call the same in google colab","2021-07-19 14:20:48","68442279","0","4983","<python><environment-variables><gpt-3>","<p>I'm not sure how to make a &quot;.json&quot; file with my GPT-3 API key/environment variable, but I'd like to utilize it in Google Colab for automatic code generation.</p>
<p>Could someone please show me how to do this?</p>
<p>I want to get the API key from the.json file using the code below.</p>
<pre><code>with open('GPT_SECRET_KEY.json') as f:
    data = json.load(f)
openai.api_key = data[&quot;API_KEY&quot;]
</code></pre>
","gpt-3"
"68038662","How to get th content of a string inside a request response?","2021-06-18 16:34:08","68038692","0","8451","<python><python-requests><openai-api><gpt-3>","<p>I was coding a webapp based on GPT-2 but it was not good so I decided to switch to official OpenAI GPT-3.
So I make that request:</p>
<pre><code>response = openai.Completion.create(
  engine=&quot;davinci&quot;,
  prompt=&quot;Hello&quot;,
  temperature=0.7,
  max_tokens=64,
  top_p=1,
  frequency_penalty=0,
  presence_penalty=0
)
</code></pre>
<p>And when I print the response I get this:</p>
<pre><code>{
  &quot;choices&quot;: [
    {
      &quot;finish_reason&quot;: &quot;length&quot;,
      &quot;index&quot;: 0,
      &quot;logprobs&quot;: null,
      &quot;text&quot;: &quot;, everyone, and welcome to the first installment of the new opening&quot;
    }
  ],
  &quot;created&quot;: 1624033807,
  &quot;id&quot;: &quot;cmpl-3CBfb8yZAFEUIVXfZO90m77dgd9V4&quot;,
  &quot;model&quot;: &quot;davinci:2020-05-03&quot;,
  &quot;object&quot;: &quot;text_completion&quot;
}
</code></pre>
<p>But I only want to print the text, so how can I do to print the &quot;text&quot; value in the response list.
Thank you in advance and have a good day.</p>
","gpt-3"
"67735561","Fine-tuning GPT-2/3 on new data","2021-05-28 08:35:33","67736155","0","2549","<machine-learning><training-data><gpt-2><gpt-3>","<p>I'm trying to wrap my head around training OpenAI's language models on new data sets. Is there anyone here with experience in that regard?
My idea is to feed either GPT-2 or 3 (I do not have API access to 3 though) with a textbook, train it on it and be able to &quot;discuss&quot; the content of the book with the language model afterwards. I don't think I'd have to change any of the hyperparameters, I just need more data in the model.</p>
<p>Is it possible??</p>
<p>Thanks a lot for any (also conceptual) help!</p>
","gpt-3"
"67403271","Using AI generators to ask questions to provoke thinking instead of giving answers?","2021-05-05 14:25:52","67446168","0","250","<artificial-intelligence><gpt-2><gpt-3>","<p>I have a use case that I want to use to help independent creators talk about their interests on Twitter using their experiences.</p>
<p>It goes like this:</p>
<p>You have an <strong>interest</strong> you want to talk about <strong>Entrepreneurship</strong></p>
<p>You have an <strong>experience</strong> like <strong>Pain</strong></p>
<p>Is there a way for an AI (like GPT) to generate prompts that uses these two words to create a list of open-ended questions that provoke thoughts such as these:</p>
<ul>
<li>If entrepreneurship wasn't painful, what would it look like?</li>
<li>What do you know about entrepreneurship that is painful that starters should know?</li>
<li>How can you lower the barrier to entrepreneurship so that it's a less painful opportunity for a person to take?</li>
</ul>
<p>If so, how will it work, and what do I need to do?</p>
<p>I've explored Open AI's documentation on GPT-3, I'm unclear if it solves this problem of generating prompts.</p>
<p>Thanks!</p>
","gpt-3"
"67372903","Access OpenAI (json) API from R","2021-05-03 17:01:19","75949356","1","2276","<r><json><curl><openai-api><gpt-3>","<p>I want to access the OpenAI API with the following curl command from R:</p>
<pre><code>curl https://api.openai.com/v1/engines/davinci/completions \
-H &quot;Content-Type: application/json&quot; \
-H &quot;Authorization: Bearer YOUR_API_KEY&quot; \
-d '{&quot;prompt&quot;: &quot;This is a test&quot;, &quot;max_tokens&quot;: 5}'
</code></pre>
<p>I think the curl package (on CRAN) will be the best option(?). I have never used this package so can anyone help me getting started with this simple call?</p>
","gpt-3"
"67325687","How to save pre-trained API on GPT-3?","2021-04-29 22:07:10","67653823","5","997","<python><gpt-3>","<p>I have a question about GPT-3. As we know we can give some examples to the network and &quot;adjust&quot; the model.</p>
<ol>
<li>Show examples to the model.</li>
<li>Save these examples.</li>
<li>Reuse the APIs.</li>
</ol>
<hr/>
<pre><code>import openai

class Example():
    &quot;&quot;&quot;Stores an input, output pair and formats it to prime the model.&quot;&quot;&quot;
def __init__(self, inp, out):
    self.input = inp
    self.output = out

def get_input(self):
    &quot;&quot;&quot;Returns the input of the example.&quot;&quot;&quot;
    return self.input

def get_output(self):
    &quot;&quot;&quot;Returns the intended output of the example.&quot;&quot;&quot;
    return self.output

def format(self):
    &quot;&quot;&quot;Formats the input, output pair.&quot;&quot;&quot;
    return f&quot;input: {self.input}\noutput: {self.output}\n&quot;


class GPT:
    &quot;&quot;&quot;The main class for a user to interface with the OpenAI API.
    A user can add examples and set parameters of the API request.&quot;&quot;&quot;
def __init__(self, engine='davinci',
             temperature=0.5,
             max_tokens=100):
    self.examples = []
    self.engine = engine
    self.temperature = temperature
    self.max_tokens = max_tokens

def add_example(self, ex):
    &quot;&quot;&quot;Adds an example to the object. Example must be an instance
    of the Example class.&quot;&quot;&quot;
    assert isinstance(ex, Example), &quot;Please create an Example object.&quot;
    self.examples.append(ex.format())
</code></pre>
<p>Now when I use &quot;give&quot; examples to the model I have the following code:</p>
<pre><code>gpt2 = GPT(engine=&quot;davinci&quot;, temperature=0.5, max_tokens=100)
gpt2.add_example(Example('Two plus two equals four', '2 + 2 = 4'))
gpt2.add_example(Example('The integral from zero to infinity', '\\int_0^{\\infty}'))

prompt1 = &quot;x squared plus y squared plus equals z squared&quot;
output1 = gpt2.submit_request(prompt1)
</code></pre>
<p>However, I am not able to save this &quot;pre-trained&quot; API. Every time I have to retrain it - is there any way to reuse it?</p>
","gpt-3"
"66451430","Changes in GPT2/GPT3 model during few shot learning","2021-03-03 05:46:56","","1","856","<nlp><gpt-2><gpt-3>","<p>During transfer learning, we take a pre-trained network and some observation pair (input and label), and use these data to fine-tune the weight by use of backpropagation. However, during one shot/few shot learning, according to this paper- 'Language Models are Few-Shot Learners' (<a href=""https://arxiv.org/pdf/2005.14165.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/2005.14165.pdf</a>), &quot;No gradient updates are performed&quot;. Then what changes happen to the models like GPT2 and GPT3 during one shot/few shot learning?</p>
","gpt-3"
"66020205","Huggingface Transformer Priming","2021-02-03 01:56:48","","1","257","<python><huggingface-transformers><gpt-2><gpt-3>","<p>I am trying to replicate the results of <a href=""https://www.buildgpt3.com/post/41/"" rel=""nofollow noreferrer"">this demo</a>, whose author primes GPT-3 with <a href=""https://twitter.com/siddkaramcheti/status/1286168606896603136?lang=es"" rel=""nofollow noreferrer"">just</a> the following text:</p>
<pre><code>gpt.add_example(Example('apple', 'slice, eat, mash, cook, bake, juice'))
gpt.add_example(Example('book', 'read, open, close, write on'))
gpt.add_example(Example('spoon', 'lift, grasp, scoop, slice'))
gpt.add_example(Example('apple', 'pound, grasp, lift'))
</code></pre>
<p>I only have access to GPT-2, via the Huggingface Transformer. How can I prime GPT-2 large on Huggingface to replicate the above examples? The issue is that, with <a href=""https://transformer.huggingface.co/doc/gpt2-large"" rel=""nofollow noreferrer"">this</a>, one doesn't get to prime with the input and corresponding output separately (as the author of the GPT-3 demo did above).</p>
<p>Similarly, <a href=""https://www.kaggle.com/nageshsingh/huggingface-transformer-basic-usage"" rel=""nofollow noreferrer"">this tutorial</a> describes using Huggingface, but there's no example which clearly shows how you can prime it using input vs output examples.</p>
<p>Does anyone know how to do this?</p>
<hr />
<p>Desired output:
use GPT-2 to return something like, for input &quot;potato&quot;, output &quot;peel, slice, cook, mash, bake&quot; (as in the GPT-3 demo: <a href=""https://www.buildgpt3.com/post/41/"" rel=""nofollow noreferrer"">https://www.buildgpt3.com/post/41/</a>). Obviously the exact list of output verbs won't be the same as GPT-2 and GPT-3 are not identical models.</p>
","gpt-3"
"64722585","GPT-3 Prompts for Sentence-Level and Paragraph-Level Text Summarization / Text Shortening / Text Rewriting","2020-11-06 22:50:25","","3","6026","<text><artificial-intelligence><summarization><gpt-3>","<p>Need effective prompts for GPT-3 that can accomplish this 'programming' task.  Creating effective GPT-3 prompts has essentially become a new form of programming (giving a computer instructions to complete a task).</p>
<p>There are getting to be repositories for the nascient, growing 'programming' language of GPT-3 prompts, eg at:</p>
<p><a href=""https://github.com/martonlanga/gpt3-prompts"" rel=""nofollow noreferrer"">https://github.com/martonlanga/gpt3-prompts</a></p>
<p><a href=""http://gptprompts.wikidot.com/start"" rel=""nofollow noreferrer"">http://gptprompts.wikidot.com/start</a></p>
<p><a href=""https://github.com/wgryc/gpt3-prompts"" rel=""nofollow noreferrer"">https://github.com/wgryc/gpt3-prompts</a></p>
<p>See a working example below, which works ok, but doesn't really address the need, and isn't adequately reliable.</p>
<p>This is an important, new, and quickly growing area.</p>
<p>Seeking prompts that will accomplish the goal in the Title: summarizing / shortening sentences and / or paragraphs with high reliability, without creating nonsense.</p>
<p>Please, reviewers, this is an important question to many people... don't be narrow-minded and decided that because GPT-3 prompts aren't (yet) a 'traditional' computer language they don't have a place here.</p>
<p>Thank you for your help</p>
<p>Example GPT-3 Prompt:</p>
<p>Please summarize the article below.
&quot;&quot;&quot;
Microsoft in talks to buy TikTok
Negotiations for ByteDance-owned social media group come as Trump threatens action</p>
<p>Microsoft has held talks to acquire TikTok, whose Chinese owner ByteDance faces mounting pressure from the US government to sell the video sharing app or risk being blacklisted in the country, said people briefed on the matter.</p>
<p>... the rest of the article...
&quot;&quot;&quot;</p>
<p>Q: Could you please summarize the article above in three sentences?</p>
","gpt-3"
"64312421","OpenAI API and GPT-3, not clear how can I access or set up a learning/dev?","2020-10-12 05:59:30","","4","2356","<nlp><openai-api><gpt-3>","<p>I am reading tons of GPT-3 samples, and came cross many code samples.
None of them mentions that how and where I can run and play with the code myself... and especially not mentioning I can not.</p>
<p>So I did my research, and concluded, I can not, but I may be wrong:</p>
<ul>
<li>There is no way to run the &quot;thing&quot; on-premises on a dev machine, it is a hosted service by definition (?)</li>
<li>As of now (Oct. 11th 2020) the OpenAI API is in invite only beta (?)</li>
</ul>
<p>Did I miss something?</p>
","gpt-3"
"64130834","Build a model that answers question from dataset using GPT3","2020-09-30 04:23:45","66070991","1","1974","<nlp><nlp-question-answering><gpt-3>","<p>I am trying to build a chat bot, that given some text corpus, will answer questions when we ask something from that text. I have heard GPT3 is a beast and requires minimum training. Are there any links/ tutorial/github repo's that will help me get started with this?</p>
","gpt-3"
"63587789","GPT3 : from next word to Sentiment analysis, Dialogs, Summary, Translation ....?","2020-08-25 22:09:55","","0","669","<nlp><artificial-intelligence><gpt-3><large-language-model>","<p>How does GPT3 or other model goes from next word prediction to do Sentiment analysis, Dialogs, Summaries, Translation .... ?</p>
<p>what is the idea and algorithms ?
How does it work ?</p>
<p>F.e. generating paragraph is generate next word then the next ..next..</p>
<p>On the other hand Sentiment analysis task is paragraph of text is Good/Bad, which is a classification ?
Extracting meaningful sentence from paragraph is even more different task.</p>
<p>How do we go from next token to ...... !</p>
<hr />
<p>Andre thanks for the replies.</p>
<p>It seems my question is not clear enough. So let me elaborate.
Next-token prediction can be trained on normal text corpus.</p>
<pre><code>word1 w2 w3 w4 .....
</code></pre>
<p>Next Sentiment can be trained on sentence=&gt;marker=&gt;label</p>
<pre><code>sent1: word1 w2 w3 w4 ..... marker label1
sent2: word1 w2 w3 w4 ..... marker label2
sent3: word1 w2 w3 w4 ..... marker label3
....
</code></pre>
<p>It is no longer corpus-next-token-generation. It is next-token generation.
The problem is you need to have the LABALED data !!</p>
<p>How about text summation ... lets use keyword extraction (and eventually sentence selection based on those keywords)
Again u need even more complex labeling.</p>
<pre><code>  paragraph1 =&gt; kw1
  paragraph1 =&gt; kw2
  paragraph2 =&gt; kw3
  paragraph3 =&gt; kw4         
</code></pre>
<p>it still can be thought of as next-token prediction but you need again specialized LABELED data.</p>
<blockquote>
<p>So my question given ONLY corpus text, how do you do the Sentiment, Text summary .... etc ?</p>
</blockquote>
<p>Otherwise GPT3 is simply scaled DNN with thousands of man hours for labeling data !!</p>
<p>WHERE is the LEAP ?</p>
","gpt-3"
"63321892","How can I use GPT 3 for my text classification?","2020-08-09 02:17:17","63442865","6","10544","<keras><text-classification><transfer-learning><openai-api><gpt-3>","<p>I am wondering if I can be able to use OpenAI GPT-3 for transfer learning in a text classification problem?
If so, how can I get start on it using Tensorflow, Keras.</p>
","gpt-3"